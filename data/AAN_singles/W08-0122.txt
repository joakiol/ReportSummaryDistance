Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 129?137,Columbus, June 2008. c?2008 Association for Computational LinguisticsDiscourse Level Opinion Relations: An Annotation StudySwapna SomasundaranDept.
of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260swapna@cs.pitt.eduJosef RuppenhoferIntelligent Systems ProgramUniversity of PittsburghPittsburgh, PA 15260josefr@cs.pitt.eduJanyce WiebeDept.
of Computer ScienceUniversity of PittsburghPittsburgh, PA 15260wiebe@cs.pitt.eduAbstractThis work proposes opinion frames as a repre-sentation of discourse-level associations thatarise from related opinion targets and whichare common in task-oriented meeting dialogs.We define the opinion frames and explain theirinterpretation.
Additionally we present anannotation scheme that realizes the opinionframes and via human annotation studies, weshow that these can be reliably identified.1 IntroductionThere has been a great deal of research in recentyears on opinions and subjectivity.
Opinions havebeen investigated at the phrase, sentence, and docu-ment levels.
However, little work has been carriedout at the level of discourse.Consider the following excerpt from a dialogabout designing a remote control for a television (theopinion targets - what the opinions are about - areshown in italics).
(1) D:: And I thought not too edgy and like a box, morekind of hand-held not as computery, yeah, more or-ganic shape I think.
Simple designs, like the last onewe just saw, not too many buttons .
.
.Speaker D expresses an opinion in favor of a de-sign that is simple and organic in shape, and againstan alternative design which is not.
Several individ-ual opinions are expressed in this passage.
The firstis a negative opinion about the design being too edgyand box-like, the next is a positive opinion towarda hand-held design, followed by a negative opin-ion toward a computery shape, and so on.
Whilewe believe that recognizing individual expressionsof opinions, their properties, and components is im-portant, we believe that discourse interpretation isneeded as well.
It is by understanding the passageas a discourse that we see edgy, like a box, com-putery, and many buttons as descriptions of the typeof design D does not prefer, and hand-held, organicshape, and simple designs as descriptions of the typehe does.
These descriptions are not in general syn-onyms/antonyms of one another; for example, thereare hand-held ?computery?
devices and simple de-signs that are edgy.
The unison/opposition amongthe descriptions is due to how they are used in thediscourse.This paper focuses on such relations between thetargets of opinions in discourse.
Specifically, wepropose opinion frames, which consist of two opin-ions which are related by virtue of having unitedor opposed targets.
We believe that recognizingopinion frames will provide more information forNLP applications than recognizing their individualcomponents alone.
Further, if there is uncertaintyabout any one of the components, we believe opin-ion frames are an effective representation incorpo-rating discourse information to make an overall co-herent interpretation (Hobbs, 1979; Hobbs, 1983).To our knowledge, this is the first work to ex-tend a manual annotation scheme to relate opinionsin the discourse.
In this paper, we present opin-ion frames, and motivate their usefulness throughexamples.
Then we provide an annotation schemefor capturing these opinion frames.
Finally we per-form fine-grained annotation studies to measure thehuman reliability in recognizing of these opinionframes.129Opinion frames are presented in Section 2, our an-notation scheme is described in Section 3, the inter-annotator agreement studies are presented in Section4, related work is discussed in Section 5, and conclu-sions are in Section 6.2 Opinion Frames2.1 IntroductionThe components of opinion frames are individualopinions and the relationships between their targets.We address two types of opinions, sentiment andarguing.
Following (Wilson and Wiebe, 2005; So-masundaran et al, 2007), sentiment includes posi-tive and negative evaluations, emotions, and judg-ments, while arguing includes arguing for or againstsomething, and arguing that something should orshould not be done.
In our examples, the lexical an-chors revealing the opinion type (as the words areinterpreted in context) are indicated in bold face.In addition, the text span capturing the target of theopinion (again, as interpreted in context) is indicatedin italics.
(2) D:: .
.
.
this kind of rubbery material, it?s a bit morebouncy, like you said they get chucked around a lot.
Abit more durable and that can also be ergonomic and itkind of feels a bit different from all the other remotecontrols.Speaker D expresses his preference for the rub-bery material for the remote.
He reiterates his opin-ion with a number of positive evaluations like bitmore bouncy, bit more durable, ergonomic andso on.All opinions in this example are related to the oth-ers via opinion frames by virtue of having the sametargets, i.e., the opinions are essentially about thesame things (the rubbery material for the remote).For example, the opinions ergonomic and a bit dif-ferent from all the other remote controls are re-lated in a frame of type SPSPsame, meaning the firstopinion is a S(entiment) with polarity P(ositive); thesecond also is a S(entiment) with polarity P(ositive);and the targets of the opinions are in a same (target)relation.The specific target relations addressed in this pa-per are the relations of either being the same or beingalternatives to one another.
While these are not theonly possible relations, they are not infrequent, andSPSPsame, SNSNsame, APAPsame, ANANsame,SPAPsame, APSPsame, SNANsame, ANSNsame,SPSNalt, SNSPalt, APANalt, ANAPalt,SPANalt, SNAPalt, APSNalt, ANSPaltSPSNsame, SNSPsame, APANsame, ANAPsame,SPANsame, APSNsame, SNAPsame, ANSPsame,SPSPalt, SNSNalt, APAPalt, ANANalt,SPAPalt, SNANalt, APSPalt, ANSNaltTable 1: Opinion Framesthey commonly occur in task-oriented dialogs suchas those in our data.With four opinion type - polarity pairs (SN, SP,AN, AP), for each of two opinion slots, and two pos-sible target relations, we have 4 * 4 * 2 = 32 typesof frame, listed in Table 1.In the remainder of this section, we elaborate fur-ther on the same target relation (in 2.2) the alter-native target relation (in 2.3) and explain a methodby which these relationships can be propagated (in2.4).
Finally, we illustrate the usefulness of opinionframes in discourse interpretation (in 2.5).2.2 Same TargetsOur notion of sameness for targets includes casesof anaphora and ellipses, lexically similar items, aswell as less direct relations such as part-whole, sub-set, inferable, and instance-class.Looking at the opinion frames for Example 2 inmore detail, we separately list the opinions, followedby the relations between targets.Opinion Span - target Span TypeO1 bit more bouncy - it?s [t1] SPO2 bit more durable - ellipsis [t2] SPO3 ergonomic - that [t3] SPO4 a bit different from all the other remote - it [t4] SPTarget - target Relt1 - t2 samet1 - t3 samet3 - t4 sameEllipsis occurs with bit more durable.
[t2] rep-resents the (implicit) target of that opinion, and [t2]has a same relation to [t1], the target of the bit morebouncy opinion.
(Note that the interpretation of thefirst target, [t1], would require anaphora resolutionof its target span with a previous noun phrase, rub-bery material.
)Let us now consider the following passage, inwhich a meeting participant analyzes two leading re-130motes on the market.1(3) D:: These are two leading remote controls at the mo-ment.
You know they?re grey, this one?s got loads ofbuttons, it?s hard to tell from here what they actuallydo, and they don?t look very exciting at all.Opinion Span - target Span RelO1 leading - remote controls [t1] SPO2 grey - they [t2] SNO3 loads of buttons - this one [t3] SNO4 hard to tell - they [t4] SNO5 don?t look very exciting at all - they [t5] SNTarget - target Relt1 - t2 samet2 - t3 samet3 - t4 samet5 - t1 sameTarget [t2] is the set of two leading remotes, and [t3],which is in a same relation with [t2], is one of thoseremotes.
Target [t4], which is also in a same rela-tion with [t3], is an aspect of that remote, namelyits buttons.
Thus, opinion O3 is directly about oneof the remotes, and indirectly about the set of bothremotes.
Similarly, opinion O4 is directly about thebuttons of one of the remotes, and indirectly aboutthat remote itself.2.3 Alternative TargetsThe alt(ernative) target relation arises when multiplechoices are available, and only one can be selected.For example, in the domain of TV remote controls,the set of all shapes are alternatives to one another,since a remote control may have only one shape at atime.
In such scenarios, a positive opinion regardingone choice may imply a negative opinion toward therest of the choices, and vice versa.As an example, let us now consider the follow-ing passage (some intervening utterances have beenremoved for clarity).
(4) C:: .
.
.
shapes should be curved, so round shapes2Nothing square-like.C:: .
.
.
So we shouldn?t have too square corners andthat kind of thing.B:: Yeah okay.
Not the old box look.1In the other examples in this paper, the source (holder) ofthe opinions is the speaker.
The leading opinion in this exampleis an exception: its source is implicit; it is a consensus opinionthat is not necessarily shared by the speaker (i.e., it is a nestedsource (Wiebe et al, 2005)).2In the context of the dialogs, the annotators read the ?soround shapes?
as a summary statement.
Had the ?so?
been inter-preted as Arguing, the round shapes would have been annotatedas a target (and linked to curved).Opinion Span - target Span RelO1 should be - curved [t1] APO2 Nothing - square-like [t2] ANO3 shouldn?t have - square corners [t3] ANO4 too - square corners [t3] SNO5 Not - the old box look [t4] ANO6 the old box look - the old box look [t4] SNTarget - target Relt1 -t2 alternativest2 - t3 samet3 - t4 sameThere is an alt relation between, for example,[t1] and [t2].
Thus, we have an opinion frame be-tween O1 and O2, whose type is APANalt.
Fromthis frame, we understand that a positive opinion isexpressed toward something and a negative opinionis expressed toward its alternative.2.4 Link TransitivityWhen individual targets are linked, they form achain-like structure.
Due to this, a connecting pathmay exist between targets that were not directlylinked by the human annotators.
This path may betraversed to create links between new pairs of tar-gets - which in turn results in new opinion frame re-lations.
For instance, in Example 4, the frame withdirect relation is O1O2 APANalt.
By following thealt link from [t1] to [t2] and the same link from [t2]to [t3], we have an alt link between [t1] and [t3],and the additional frames O1O3 APANalt and O1O4APSNalt.
Repeating this process would finally linkspeaker C?s opinion O1 with B?s opinion O6, yield-ing a APSNalt frame.2.5 InterpretationThis section illustrates two motivations for opinionframes: they may unearth additional informationover and above the individual opinions stated in thetext, and they may contribute toward arriving at acoherent interpretation (Hobbs, 1979; Hobbs, 1983)of the opinions in the discourse.Through opinion frames, opinions regardingsomething not explicitly mentioned in the local con-text and not even lexically related can become rel-evant, providing more information about someone?sopinions.
This is particularly interesting when altrelations are involved, as opinions towards one al-ternative imply opinions of opposite polarity towardthe remaining options.
For instance in Example 4131above, if we consider only the explicitly stated opin-ions, there is only one (positive) opinion about thecurved shape, namely O1.
However, the speaker ex-presses several other opinions which reinforce hispositivity toward the curved shape.
These are infact opinion frames in which the other opinion hasthe opposite polarity as O1 and the target relation isalt (for example frames such as O1O3 APANalt andO1O4 APSNalt).In the dialog, notice that speaker B agrees withC and exhibits his own reinforcing opinions.
Thesewould be similarly linked via targets resulting inframes like O1O6 APSNalt.Turning to our second point, arriving at a coher-ent interpretation obviously involves disambigua-tion.
Suppose that some aspect of an individualopinion, such as polarity, is unclear.
If the discoursesuggests certain opinion frames, this may in turn re-solve the underlying ambiguity.
For instance in Ex-ample 2, we see that out of context, the polarities ofbouncy and different from other remotes are un-clear (bounciness and being different may be neg-ative attributes for another type of object).
How-ever, the polarities of two of the opinions are clear(durable and ergonomic).
There is evidence in thispassage of discourse continuity and same relations,such as the pronouns, the lack of contrastive cuephrases, and so on.
This evidence suggests that thespeaker expresses similar opinions throughout thepassage, making the opinion frame SPSPsame morelikely throughout.
Recognizing the frames would re-solve the polarity ambiguities of bouncy and differ-ent.Example 2 is characterized by opinion frames inwhich the opinions reinforce one other.
Interest-ingly, interplays among different opinion types mayshow the same type of reinforcement.
As we an-alyzed above, Example 4 is characterized by mix-tures of opinion types, polarities, and target rela-tions.
However, the opinions are still unified inthe intention to argue for a particular type of shape.There is evidence in this passage suggesting rein-forcing frames: the negations are applied to targetsthat are alternative to the desired option, and the pas-sage is without contrastive discourse cues.
If weare able to recognize the best overall set of opinionframes for the passage, the polarity ambiguities willbe resolved.On the other hand, evidence for non-reinforcingopinions would suggest other frames, potentially re-sulting in different interpretations of polarity and re-lations among targets.
Such non-reinforcing associ-ations between opinions and often occur when thespeaker is ambivalent or weighing pros and cons.Table 1 lists the frames that occur in reinforcing sce-narios in the top row, and the frames that occur innon-reinforcing scenarios in the bottom row.3 Annotation SchemeOur annotation scheme began with the definitionand basics of the opinion annotation from previ-ous work (Wilson and Wiebe, 2005; Somasundaranet al, 2007).
We then add to it the attributes andcomponents that are necessary to make an OpinionFrame.First, the text span that reveals the opinion expres-sion is identified.
Then, the text spans correspondingto the targets are marked, if there exist any (we alsoallow span-less targets).
Then, the type and polar-ity of the opinion in the context of the discourse ismarked.
Finally the targets that are related (againin the context of the discourse) are linked.
Specif-ically, the components that form the Annotation ofthe frame are as follows:Opinion Span: This is a span of text that revealsthe opinion.Type: This attribute specifies the opinion type as ei-ther Arguing or Sentiment.Polarity: This attribute identifies the valence of anopinion and can be one of: positive, negative,neutral, both, unknown.Target Span: This is a span of text that captureswhat an opinion is about.
This can be a propo-sition or an entity.Target Link: This is an attribute of a target andrecords all the targets in the discourse that thetarget is related to.Link Type: The link between two targets is speci-fied by this attribute as either same or alterna-tive.132In addition to these definitions, our annotation man-ual has guidelines detailing how to deal with gram-matical issues, disfluencies, etc.
Appendix A illus-trates how this annotation scheme is applied to theutterances of Example 4.Links between targets can be followed in eitherdirection to construct chains.
In this work, weconsider target relations to be commutative, i.e.,Link(t1,t2) => Link(t2,t1).
When a newly anno-tated target is similar (or opposed) to a set of tar-gets already participating in same relations, then thesame (or alt) link is made only to one of them - theone that looks most natural.
This is often the onethat is closest.4 Annotation StudiesConstruction of an opinion frame is a stepwise pro-cess where first the text spans revealing the opinionsand their targets are selected, the opinion text spansare classified by type and polarity and finally thetargets are linked via one of the possible relations.We split our annotation process into these 3 intuitivestages and use an evaluation that is most applicablefor the task at that stage.Two annotators (both co-authors on the paper) un-derwent training at each stage, and the annotationmanual was revised after each round of training.
Inorder to prevent errors incurred at earlier stages fromaffecting the evaluation of later stages, the anno-tators produced a consensus version at the end ofeach stage, and used that consensus annotation asthe starting point for the next annotation stage.
Inproducing these consensus files, one annotator firstannotated a document, and the other annotator re-viewed the annotations, making changes if needed.This prevented any discussion between the annota-tors from influencing the tagging task of the nextstage.In the following subsections, we first introducethe data and then present our results for annotationstudies for each stage, ending with discussion.4.1 DataThe data used in this work is the AMI meeting cor-pus (Carletta et al, 2005) which contains multi-modal recordings of group meetings.
We annotatedmeetings from the scenario based meetings, whereGold Exact Lenient SubsetANN-1 53 89 87ANN-2 44 76 74Table 2: Inter-Annotator agreement on Opinion Spansfour participants collaborate to design a new TVremote control in a series of four meetings.
Themeetings represent different project phases, namelyproject kick-off, functional design, conceptual de-sign, and detailed design.
Each meeting has richtranscription and segment (turn/utterance) informa-tion for each speaker.
Each utterance consists ofone or more sentences.
At each agreement stage weused approximately 250 utterances from a meetingfor evaluation.
The annotators also used the audioand video recordings in the annotation of meetings.4.2 Opinion Spans and Target SpansIn this step, the annotators selected text spans andlabeled them as opinion or target We calculated ouragreement for text span retrieval similar to Wiebe etal.
(2005).
This agreement metric corresponds tothe Precision metric in information retrieval, whereannotations from one annotator are considered thegold standard, and the other annotator?s annotationsare evaluated against it.Table 2 shows the inter-annotator agreement (inpercentages).
For the first row, the annotations pro-duced by Annotator-1 (ANN-1) are taken as the goldstandard and, for the second row, the annotationsfrom annotator-2 form the gold standard.
The ?Ex-act?
column reports the agreement when two textspans have to match exactly to be considered cor-rect.
The ?Lenient?
column shows the results ifan overlap relation between the two annotators?
re-trieved spans is also considered to be a hit.
Wiebeet al (2005) use this approach to measure agree-ment for a (somewhat) similar task of subjectivityspan retrieval in the news corpus.
Our agreementnumbers for this column is comparable to theirs.
Fi-nally, the third column, ?Subset?, shows the agree-ment for a more strict constraint, namely, that oneof the spans must be a subset of the other to be con-sidered a match.
Two opinion spans that satisfy thisrelation are ensured to share all the opinion words ofthe smaller span.The numbers indicate that, while the annotators133Gold Exact Lenient SubsetANN-1 54 73 71ANN-2 54 75 74Table 3: Inter-Annotator agreement on Target SpansGold Exact Lenient SubsetANN-1 74 87 87ANN-2 76 90 90Table 4: Inter-Annotator agreement on Targets with Per-fect Opinion spansdo not often retrieve the exact same span, theyreliably retrieve approximate spans.
Interestingly,the agreement numbers between Lenient and Sub-set columns are close.
This implies that, in the casesof inexact matches, the spans retrieved by the twoannotators are still close.
They agree on the opinionwords and differ mostly on the inclusion of func-tion words (e.g.
articles) and observation of syntac-tic boundaries.In similar fashion, Table 3 gives the inter-annotator agreement for target span retrieval.
Ad-ditionally, Table 4 shows the inter-annotator agree-ment for target span retrieval when opinions that donot have an exact match are filtered out.
That is, Ta-ble 4 shows results only for targets of the opinionson which the annotators perfectly agree.
As targetsare annotated with respect to the opinions, this sec-ond evaluation removes any effects of disagreementsin the opinion detection task.
As seen in Table 4, thisimproves the inter-coder agreement.4.3 Opinion Type and PolarityIn this step, the annotators began with the consensusopinion span and target span annotations.
We hy-pothesized that given the opinion expression, deter-mining whether it is Arguing or Sentiment would notbe difficult.
Similarly, we hypothesized that targetinformation would make the polarity labeling taskclearer.As every opinion instance is tagged with a typeType Tagging Polarity TaggingAccuracy 97.8% 98.5%?
0.95 0.952Table 5: Inter-Annotator agreement on Opinion Typesand Polarityand polarity, we use Accuracy and Cohen?s Kappa(?)
metric (Cohen, 1960).
The ?
metric measuresthe inter-annotator agreement above chance agree-ment.
The results, in Table 5, show that ?
both fortype and polarity tagging is very high.
This con-firms our hypothesis that Sentiment and Arguing canbe reliably distinguished once the opinion spans areknown.
Our polarity detection task shows an im-provement in ?
over a similar polarity assignmenttask by Wilson et al (2005) for the news corpus (?of 0.72).
We believe this improvement can partly beattributed to the target information available to ourannotators.4.4 Target LinkingAs an intuitive first step in evaluating target link-ing, we treat target links in the discourse similarly toanaphoric chains and apply methods developed forco-reference resolution (Passonneau, 2004) for ourevaluation.
Passonneau?s method is based on Krip-pendorf?s ?
metric (Krippendorff, 2004) and allowsfor partial matches between anaphoric chains.
In ad-dition to this, we evaluate links identified by bothannotators for the type (same / alternative) labelingtask with the help of the ?
metric.Passonneau (2004) reports that in her co-referencetask on spoken monologs, ?
varies with the diffi-culty of the corpus (from 0.46 to 0.74).
This is truein our case too.
Table 6 shows our agreement forthe four types of meetings in the AMI corpus: thekickoff meeting (a), the functional design (b), theconceptual design (c) and the detailed design (d).Of the meetings, the kickoff meeting (a) we usehas relatively clear discussions.
The conceptual de-sign meeting (c) is the toughest, as as participantsare expressing opinions about a hypothetical (desir-able) remote.
In our detailed design meeting (d),there are two final designs being evaluated.
On an-alyzing the chains from the two annotators, we dis-covered that one annotator had maintained two sepa-rate chains for the two remotes as there is no explicitlinguistic indication (within the 250 utterances) thatthese two are alternatives.
The second annotator, onthe other hand, used the knowledge that the goalof the meeting is to design a single TV remote tolink them as alternatives.
Thus by changing justtwo links in the second annotator?s file to accountfor this, our ?
for this meeting went up from 0.52134Meeting: a b c dTarget linking (?)
0.79 0.74 0.59 0.52Relation Labeling (?)
1 1 0.91 1Table 6: Inter-Annotator agreement on Target relationidentificationto 0.70.
We plan to further explore other evalua-tion methodologies that account for severity of dif-ferences in linking and are more relevant for ourtask.
Nonetheless, the resulting numbers indicatethat there is sufficient information in the discourseto provide for reliable linking of targets.The high ?
for the relation type identificationshows that once the presence of a link is detected,it is not difficult to determine if the targets are simi-lar or alternatives to each other.4.5 DiscussionOur agreement studies help to identify the aspects ofopinion frames that are straightforward, and thosethat need complex reasoning.
Our results indicatethat while the labeling tasks such as opinion type,opinion polarity and target relation type are rel-atively reliable for humans, retrieval of opinionsspans, target spans and target links is more difficult.A common cause of annotation disagreement isdifferent interpretation of the utterance, particularlyin the presence of disfluencies and restarts.
For ex-ample consider the following utterance where a par-ticipant is evaluating the drawing of another partici-pant on the white board.
(5) It?s a baby shark , it looks to me, .
.
.One annotator interpreted this ?it looks to me?
asan arguing for the belief that it was indeed a draw-ing of a baby shark (positive Arguing).
The sec-ond annotator on the other hand looked at it as aneutral viewpoint/evaluation (Sentiment) being ex-pressed regarding the drawing.
Thus even thoughboth annotators felt an opinion is being expressed,they differed on its type and polarity.There are some opinions that are inherently on theborderline of Sentiment and Arguing.
For example,consider the following utterance where there is anappeal to importance:(6) Also important for you all is um the production costmust be maximal twelve Euro and fifty cents.Here, ?also important?
might be taken as an assess-ment of the high value of adhering to the budget (rel-ative to other constraints), or simply as an argumentfor adhering to the budget.One potential source of problems to the target-linking process consists of cases where the sameitem becomes involved in more than one opposition.For instance, in the example below, speaker D ini-tially sets up an alternative between speech recog-nition and buttons as a possible interface for navi-gation.
But later, speaker A re-frames the choice asbetween having speech recognition only and havingboth options.
Connecting up all references to speechrecognition as a target respects the co-reference butit also results in incorrect conclusions: the speechrecognition is an alternative to having both speechrecognition and buttons.
(7) A:: One thing is interesting is talking about speechrecognition in a remote control...D:: ...
So that we don?t need any button on the remotecontrol it would be all based on speech.A:: ...
I think that would not work so well.
You wannahave both options.5 Related WorkEvidence from the surrounding context has beenused previously to determine if the current sentenceshould be subjective/objective (Riloff et al, 2003;Pang and Lee, 2004)) and adjacency pair informa-tion has been used to predict congressional votes(Thomas et al, 2006).
However, these methods donot explicitly model the relations between opinions.Additionally, in our scheme opinions that are notin the immediate context may be allowed to influ-ence the interpretation of a given opinion via targetchains.Polanyi and Zaenen (2006), in their discussion oncontextual valence shifters, have also observed thephenomena described in this work - namely that acentral topic may be divided into subtopics in orderto perform evaluations, and that discourse structurecan influence the overall interpretation of valence.Snyder and Barzilay (2007) combine an agree-ment model based on contrastive RST relations witha local aspect (or target) model to make a more in-formed overall decision for sentiment classification.The contrastive cue indicates a change in the senti-ment polarity.
In our scheme, their aspects wouldbe related as same and their high contrast relationswould result in frames such as SPSNsame, SNSP-same.
Additionally, our frame relations would linksentiments across non-adjacent clauses, and makeconnections via alt target relations.135Considering the discourse relation annotations inthe PDTB (Prasad et al, 2006), there can be align-ment between discourse relations (like contrast) andour opinion frames when the frames represent dom-inant relations between two clauses.
However, whenthe relation between opinions is not the most promi-nent one between two clauses, the discourse relationmay not align with the opinion frames.
And when anopinion frame is between two opinions in the sameclause, there would be no discourse relation counter-part at all.
Further, opinion frames assume particularintentions that are not necessary for the establish-ment of ostensibly similar discourse relations.
Forexample, we may not impose an opinion frame evenif there are contrastive cues.
(Please refer to Ap-pendix B for examples)With regard to meetings, the most closely re-lated work includes the dialog-related annotationschemes for various available corpora of conversa-tion (Dhillon et al (2003) for ICSI MRDA; Car-letta et al (2005) for AMI ) As shown by Soma-sundaran et al (2007), dialog structure informationand opinions are in fact complementary.
We believethat, like discourse relations, dialog information willadditionally help in arriving at an overall coherentinterpretation.6 Conclusion and Future workThis is the first work that extends an opinion annota-tion scheme to relate opinions via target relations.We first introduced the idea of opinion frames asa representation capturing discourse level relationsthat arise from related opinion targets and which arecommon in task-oriented dialogs such as our data.We built an annotation scheme that would capturethese relationships.
Finally, we performed extensiveinter-annotator agreement studies in order to find thereliability of human judgment in recognizing framecomponents.
Our results and analysis provide in-sights into the complexities involved in recognizingdiscourse level relations between opinions.AcknowledgmentsThis research was supported in part by theDepartment of Homeland Security under grantN000140710152.ReferencesJ.
Carletta, S. Ashby, and et al 2005.
The AMI MeetingsCorpus.
In Proceedings of Measuring Behavior Sym-posium on ?Annotating and measuring Meeting Be-havior?.J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Measurement,20:37?46.R.
Dhillon, S. Bhagat, H. Carvey, and E. Shriberg.
2003.Meeting recorder project: Dialog act labeling guide.Technical report, ICSI Tech Report TR-04-002.J.
Hobbs.
1979.
Coherence and coreference.
CognitiveScience, 3:67?90.J.
Hobbs, 1983.
Why is Discourse Coherent?, pages 29?70.
Buske Verlag.K.
Krippendorff.
2004.
Content Analysis: An Introduc-tion to Its Methodology, 2nd Edition.
Sage Publica-tions, Thousand Oaks, California.B.
Pang and L. Lee.
2004.
A sentimental education:Sentiment analysis using subjectivity summarizationbased on minimum cuts.
In ACl 2004.R.
J. Passonneau.
2004.
Computing reliability for coref-erence annotation.
In LREC.L.
Polanyi and A. Zaenen, 2006.
Contextual ValenceShifters, chapter 1.
Computing Attitude and Affect inText: Theory and Applications.
Springer.R.
Prasad, N. Dinesh, A. Lee, A. Joshi, and B. Webber.2006.
Annotating attribution in the Penn DiscourseTreeBank.
In Workshop on Sentiment and Subjectivityin Text.
ACL.E.
Riloff, J. Wiebe, and T. Wilson.
2003.
Learning sub-jective nouns using extraction pattern bootstrapping.In CoNLL 2003.B.
Snyder and R. Barzilay.
2007.
Multiple aspect rank-ing using the good grief algorithm.
In HLT 2007:NAACL.S.
Somasundaran, J. Ruppenhofer, and J. Wiebe.
2007.Detecting arguing and sentiment in meetings.
In SIG-dial Workshop on Discourse and Dialogue 2007.M.
Thomas, B. Pang, and L. Lee.
2006.
Get out the vote:Determining support or opposition from congressionalfloor-debate transcripts.
In EMNLP 2006.J.
Wiebe, T. Wilson, and C Cardie.
2005.
Annotating ex-pressions of opinions and emotions in language.
Lan-guage Resources and Evaluation, pages 164?210.T.
Wilson and J. Wiebe.
2005.
Annotating attributionsand private states.
In Proceedings of ACL Workshopon Frontiers in Corpus Annotation II: Pie in the Sky.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recogniz-ing contextual polarity in phrase-level sentiment anal-ysis.
In HLT-EMNLP 2005.136A Annotation ExampleC:: .
.
.
shapes should be curved, so round shapes.
Nothingsquare-like.C:: .
.
.
So we shouldn?t have too square corners and that kindof thing.B:: Yeah okay.
Not the old box look.Span AttributesO1 should be type=Arguing; Polarity=pos; target=t1t1 curved Link,type=(t2,alt)O2 Nothing type=Arguing; Polarity=neg; target=t2t2 square-like Link,type=(t1,alt),(t3,same)O3 shouldn?t have type=Arguing; Polarity=neg; target=t3O4 too type=Sentiment; Polarity=neg; target=t3t3 square corners Link,type=(t2,same),(t4,same)O5 Not type=Arguing; Polarity=neg; target=t4t4 the old box look Link,type=(t3,same)O6 the old box look type=Sentiment; Polarity=neg; target=t4B Comparison between Opinion Framesand Discourse RelationsOpinion frames can align with discourse relationsbetween clauses only when the frames represent thedominant relation between two clauses (1); but notwhen the opinions occur in the same clause (2); orwhen the relation between opinions is not the mostprominent (3); or when two distinct targets are nei-ther same nor alternatives (4).
(1) Non-reinforcing opinion frame (SNSP-same); Contrast discourse relationD :: And so what I have found and after a lotof work actually I draw for you this schemathat can be maybe too technical for you butis very important for me you know.
(2) Reinforcing opinion frame (SPSPsame); nodiscourse relationThirty four percent said it takes too longto learn to use a remote control, they wantsomething that?s easier to use straight away,more intuitive perhaps.
(3) Reinforcing opinion frame (SPSPsame);Reason discourse relationShe even likes my manga, actually the quoteis: ?I like it, because you like it, honey.?
(source: web)(4) Unrelated opinions; Contrast discourse re-lationA :: Yeah, what I have to say about means.The smart board is okay.
Digital pen is hor-rible.
I dunno if you use it.
But if you wantto download it to your computer, it?s doesn?twork.
No.137
