Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 198?199,Vancouver, October 2005. c?2005 Association for Computational LinguisticsOnline Statistics for a Unification-Based Dialogue ParserMicha Elsner, Mary Swift, James Allen, and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627{melsner,swift,allen,gildea}@cs.rochester.eduAbstractWe describe a method for augmentingunification-based deep parsing with statis-tical methods.
We extend and adapt theBikel parser, which uses head-driven lex-ical statistics, to dialogue.
We show thatour augmented parser produces signifi-cantly fewer constituents than the baselinesystem and achieves comparable brack-eting accuracy, even yielding slight im-provements for longer sentences.1 IntroductionUnification parsers have problems with efficiencyand selecting the best parse.
Lexically-conditionedstatistics as used by Collins (1999) may provide asolution.
They have been used in three ways: asa postprocess for parse selection (Toutanova et al,2005; Riezler et al, 2000; Riezler et al, 2002), apreprocess to find more probable bracketing struc-tures (Swift et al, 2004), and online to rank eachconstituent produced, as in Tsuruoka et al (2004)and this experiment.The TRIPS parser (Allen et al, 1996) is a unifi-cation parser using an HPSG-inspired grammar andhand-tuned weights for each rule.
In our augmentedsystem (Aug-TRIPS), we replaced these weightswith a lexically-conditioned model based on theadaptation of Collins used by Bikel (2002), allowingmore efficiency and (in some cases) better selection.Aug-TRIPS retains the same grammar and lexiconas TRIPS, but uses its statistical model to determinethe order in which unifications are attempted.2 ExperimentsWe tested bracketing accuracy on the Monroe cor-pus (Stent, 2001), which contains collaborativeemergency-management dialogues.
Aug-TRIPS iscomparable to TRIPS in accuracy, but producesfewer constituents (Table 1).
The Bikel parser hasslightly higher precision/recall than either TRIPSor Aug-TRIPS, since it can choose any bracketingstructure regardless of semantic coherence, whilethe TRIPS systems must find a legal pattern of fea-ture unifications.
Aug-TRIPS also has better preci-sion/recall when parsing the longer sentences (Ta-ble 2).
(training=9282) Bikel Aug-TRIPS TRIPSRecall 79.40 76.09 76.77Precision 79.40 77.08 78.20Complete Match 42.00 46.00 65.00% Constit.
Reduction - 36.96 0.00Table 1: Bracketing accuracy for 100 random sen-tences ?
2 words.> 7 Aug-TRIPS > 7 TRIPSRecall 73.25 71.00Precision 74.78 73.44Complete Match 22.50 37.50Table 2: Bracketing accuracy for the 40 sentences >7 words.Since our motivation for unification parsing is toreveal semantics as well as syntax, we next evalu-ated Aug-TRIPS?s production of correct interpreta-tions at the sentence level, which require completecorrectness not only of the bracketing structure butof the sense chosen for each word and the thematic198roles of each argument (Tetreault et al, 2004).For this task, we modified the probability modelto condition on the senses in our lexicon rather thanwords.
For instance, the words ?two thousand dol-lars?
are replaced with the senses ?number number-unit money-unit?.
This allows us to model lexi-cal disambiguation explicitly.
The model generatesone or more senses from each word with probabilityP (sense|word, tag), and then uses sense statisticsrather than word statistics in all other calculations.Similar but more complex models were used in thePCFG-sem model of Toutanova et al (2005) and us-ing WordNet senses in Bikel (2000).We used the Projector dialogues (835 sentences),which concern purchasing video projectors.
In thisdomain, Aug-TRIPS makes about 10% more inter-pretation errors than TRIPS (Table 3), but whenparsing sentences on which TRIPS itself makes er-rors, it can correct about 10% (Table 4).
(training=310) TRIPS Aug-TRIPSCorrect 26 21Incorrect 49 54% Reduction in Constituents 0% 45%Table 3: Sentence-level accuracy on 75 random sen-tences.
(training=396) TRIPS Aug-TRIPSCorrect 0 8Incorrect 54 46% Reduction in Constituents 0% 46%Table 4: Sentence-level accuracy on 54 TRIPS errorsentencesOur parser makes substantially fewer constituentsthan baseline TRIPS at only slightly lower accu-racy.
Tsuruoka et al (2004) achieved a much higherspeedup (30 times) than we did; this is partly due totheir use of the Penn Treebank, which contains muchmore data than our corpora.
In addition, however,their baseline system is a classic HPSG parser withno efficiency features, while our baseline, TRIPS, isdesigned as a real-time dialogue parser which useshand-tuned weights to guide its search and imposesa maximum chart size.Acknowledgements Our thanks to Will DeBeau-mont and four anonymous reviewers.ReferencesJames F. Allen, Bradford W. Miller, Eric K. Ringger, andTeresa Sikorski.
1996.
A robust system for naturalspoken dialogue.
In Proceedings of the 1996 AnnualMeeting of the Association for Computational Linguis-tics (ACL?96).Daniel Bikel.
2000.
A statistical model for parsingand word-sense disambiguation.
In Proceedings ofthe Joint SIGDAT Conference on Empirical Methodsin Natural Language Processing and Very Large Cor-pora, Hong Kong.Daniel Bikel.
2002.
Design of a multi-lingual, parallel-processing statistical parsing engine.
In Human Lan-guage Technology Conference (HLT), San Diego.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Stefan Riezler, Detlef Prescher, Jonas Kuhn, and MarkJohnson.
2000.
Lexicalized stochastic modeling ofconstraint-based grammars using log-linear measuresand EM training.
In Proceedings of the 38th AnnualMeeting of the ACL, Hong Kong.Stefan Riezler, Tracy H. King, Richard Crouch, andJohn T. Maxwell.
2002.
Parsing the Wall Street Jour-nal using a Lexical-Functional Grammar and discrim-inative estimation.
In Proceedings of the 40th AnnualMeeting of the ACL, Philadelphia.Amanda J. Stent.
2001.
Dialogue Systems as Conversa-tional Partners.
Ph.D. thesis, University of Rochester.Mary Swift, James Allen, and Daniel Gildea.
2004.Skeletons in the parser: Using a shallow parser to im-prove deep parsing.
In Proceedings of the 20th In-ternational Conference on Computational Linguistics(COLING-04), Geneva, Switzerland, August.Joel Tetreault, Mary Swift, Preethum Prithviraj, My-roslava Dzikovska, and James Allen.
2004.
Discourseannotation in the Monroe corpus.
In ACL workshop onDiscourse Annotation, Barcelona, Spain, July.Kristina Toutanova, Christopher D. Manning, DanFlickinger, and Stephan Oepen.
2005.
StochasticHPSG parse disambiguation using the Redwoods cor-pus.
Journal of Logic and Computation.Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichi Tsujii.2004.
Towards efficient probabilistic HPSG parsing:Integrating semantic and syntactic preference to guidethe parsing.
In Proceedings of IJCNLP-04 Workshop:Beyond Shallow Analyses- Formalisms and StatisticalModeling for Deep Analyses, Sanya City, China.199
