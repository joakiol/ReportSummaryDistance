Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,pages 86?89, Dublin, Ireland, August 23-29 2014.NTU-MC Toolkit: Annotating a Linguistically Diverse CorpusLiling TanUniversit?t des SaarlandCampus, 66123 Saarbr?cken, Germanyalvations@gmail.comFrancis BondNanyang Technological University14 Nanyang Drive, Singapore 637332bond@ieee.orgAbstractThe NTU-MC Toolkit is a compilation of tools to annotate the Nanyang Technological University- Multilingual Corpus (NTU-MC).
The NTU-MC is a parallel corpora of linguistically diverselanguages (Arabic, English, Indonesian, Japanese, Korean, Mandarin Chinese, Thai and Viet-namese).
The NTU-MC thrives on the mantra of "more data is better data and more annotationis better information".
Other than increasing parallel data from diverse language pairs, annotat-ing the corpus with various layers of information allows corpora linguists to discover linguisticphenomena and provides computational linguists with pre-annotated features for various NLPtasks.
In addition to the agglomeration existing tools into a single python wrapper library, wehave implemented three tools (Mini-segmenter, GaChalign and Indotag) that (i) pro-vides users with varying analysis of the corpus, (ii) improves the state-of-art performance and(iii) reimplements a previously unavailable annotation tool as a free and open tool.
This paperbriefly describes the wrapper classes available in the toolkit and introduces and demonstrates theusage of the Mini-segmenter, GaChalign and Indotag.1 IntroductionThe NTU-MC Toolkit was developed in conjunction with the compilation of the Nanyang TechnologicalUniversity - Multilingual Corpus (NTU-MC) (Tan and Bond, 2012).
It is an agglomeration of existingstate-of-art tools into a single python wrapper library.
The NTU-MC Toolkit provides python wrapperclasses for tokenizers and Part-of-Speech (POS) taggers for the respectively languages:?
Stanford Segmenter and POS taggers (Arabic and Chinese)?
POSTECH POSTAG/K tagger (Korean)?
tinysegmenter and MeCab (Japanese)?
JVnTextPro (Vietnamese)Additionally, we implemented three tools to provide complementary or better annotations, viz.:?
Mini-segmenter (Chinese): Dictionary based Chinese segmenter?
GaChalign (Crosslingual): Gale-Church Sentence-level Aligner with variable parameters?
Indotag (Indonesian): Conditional Random Field (CRF) POS tagger.The following sections of the paper will briefly describe the wrapper classes available in the toolkit (Sec-tion 2) and introduce and demonstrate the usage of the Mini-segmenter (Section 3), GaChalign(Section 4) and Indotag (Section 5).This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/862 Tokenization and POS Tagger WrappersPython wrapper classes were written for (i) Stanford Segmenter and POS tagger (Chang et al., 2008;Toutanova et al., 2003), (ii) POSTECH POSTAG/K tagger (Lee et al., 2002), (iii) tinysegmenterand MeCab (Kudo et al., 2004) and (iv) JVnTextPro (Nguyen et al., 2010).
Although scientificallyuninteresting, it simplifies usage of annotation tools especially for beginner who are new to NaturalLanguage Processing or python programming.
The wrapper classes are also compatible with corporareaders of the Natural Language Toolkit (NLTK).Usage Users can either invoke the wrapper classes programmatically1:$ python?> from ntumc.tk import postech?> sentence = u"??????????????????????????
?TianTian Hainanese Chicken Rice????????
(Maxwell Food Centre)??????
?, ??????????????????????.
"?> postech.postagk(sentence)[(u?????
?, ?NNP?
), (u???
?, ?JKB?
), (u???
?, ?MAG?
), (u????,?XR?
), (u??
?, ?XSA?
), (u??
?, ?ETM?
), (u?????
?, ?NNG?
), ...]or via command line:$ echo "??????????????????????????
?Tian TianHainanese Chicken Rice????????
(Maxwell Food Centre)???????,??????????????????????."
> input.txt$ python ntumc/tk/postech.py input.txt > output.txt3 Mini-segmenterThe mini-segmenter is dictionary based Chinese segmenter that capitalizes on token length asheuristics for Chinese text tokenization.
The tool includes a dictionary of Singaporean Chinese NEscrawled from Wikipedia titles and articles on Singapore.Motivation The mini-segmenter was created to resolve the problem of segmenting localized Chinesewords from the Singaporean variety of Mandarin Chinese in the NTU-MC.
After manual inspection,the Stanford Chinese segmenter2was segmenting the Chinese tokens with the wrong word boundary.For example, the Stanford Chinese word segmenter wrongly tokenized ???
wujielu ?Orchardroad" as ?_??
wu jielu ?black joint-road".
Originally, these topological terms were re-segmentedwith a manually crafted dictionary built using Wikipedia?s Chinese translations of English names ofSingapore places and streets.
Then we found more localized Named Entities (NEs) for person names,organizations and food terms.
Short of building a manually segmented corpus and retraining the Stan-ford segmenter models, a simple dictionary approach to segmentation could resolve out-of-domain issue.Innovation A lightweight lexicon/dictionary based Chinese text segmenter.
The advantage of us-ing a lexicon/dictionary for text segmentation is the ability to localize and scale according to the Chinesevariety or domain.
The mini-segmenter ranks the token boundaries based on sum of the square ofthe tokens?
character length,?nilen(tokeni)2, where n is the number of tokens and len(token) is thecharacter length of each token.
This novel scoring is based on the preference for larger chunks thansmaller chunks in a sentence.Usage The full documentation of the mini-segmenter can be found on https://code.google.com/p/mini-segmenter/1The example sentence in English, ?One of the most famous Hainanese chicken rice stalls in Singapore, Tian TianHainanaese Chiken Rice is located in the Maxwell Food Centre, with long queues forming in front of the stall every day.
"2both Penn Chinese Treebank (ctb) and Peking University (pku) models87Results We evaluate the mini-segmenter output against the Stanford segmenter output with thefish-head-curry.txt from the NTU-MC which was was previously selected at random as a textsample for human annotators to verify the tagger accuracy.
The Stanford segmenter with Stanford POStagger, it achieved 85.94% POS accuracy with 19% mis-segments.
Using the mini-segmenter withthe Stanford POS tagger, it achieved 91.27% POS accuracy with 11.43% mis-segments.4 GaChalignThe GaChalign tool is sentence alignment tool to align sentences given a bitext.
The tool is a modifica-tion of the original Gale-Church algorithm that capitalized on ratio of characters/tokens of two languagesin the bitext to align the sentences (Gale and Church, 1993).Motivation The Gale-Church algorithm had parameters tuned to suit Indo-European languages morespecifically German-English language pairs.
When using state-of-art sentence alignment tool based onGale-Church algorithm to align Chinese, Japanese or Korean texts to their respective English texts, theNTU-MC reported a poor performance in F-measure metrics adheres to standards set by the ARCADEII project (Chiao et al.
2006).
We want to see whether it is possible to improve the algorithm by tunealgorithm using language-pair specific parameters.Innovation We replaced the mean, variance and penalty parameters from the Gale-Church algorithmwith language-pair specific parameters automatically calculated from a non-aligned corpus.Results Our experiment with English-Japanese corpus has shown that (i) simply using the calculatedcharacter mean from the unaligned text improves precision and recall of the algorithm; from 61.0%(default parameters) to 62.0% (language specific) F-scores) and (ii) using language specific penaltiesfurther increased the F-scores to 62.9%.
However, aligning syllabic/logographic language (Japanese) toalphabetic language (English) remains a challenge for Gale-Church algorithm3.5 IndotagThe Indotag is a probabilistic Conditional Random Field (CRF) Bahasa Indonesian Part of Speech(POS) tagger with the specifications recommended by (Pisceldo et al., 2009).
The pre-trained model isbased on the unigram CRF with 2-left and 2-right context features using the Universitas Indonesia?s 1million word corpus compiled under the Pan Asia Networking Localization (PANL10N) project.Motivation To reimplement the Indonesian POS tagger described in Pisceldo et al.
(2010) using freeand open data and licensing it as open source tool.Innovation None or not much.
An open source reimplementation of a Bahasa Indonesian POS tagger.Result The IndoTag achieved 78% accuracy when annotating the the fish-head-curry.txt textsample from the NTU-MC.6 DiscussionWhile English POS tagging reports >97% accuracy (Manning, 2011) and sentence alignments for Indo-European languages performs well at >96% (Gale and Church, 1993; Varga et al., 2007), there is muchroom for improvement with regards to POS tagger accuracy for Asian languages and automatic sen-tence alignments from syllabic/logographic languages to alphabetic ones.
Even though the languages inthe NTU-MC are not considered low-resource languages, the tools to annotate them have limited per-formance.
While the maintainers of the NTU-MC continues to push the performance of the individualtools for these languages, we urge researchers to work on improving NLP tools/application for Asianlanguages.3Detailed evaluation on the GaChalign experiments can be found on https://code.google.com/p/gachalign/887 ConclusionWe have introduced the NTU-MC Toolkit that was compiled to annotated the linguistically diverse NTU-MC.
The toolkit agglomerate existing tools into a single python wrapper library.
The toolkit also imple-mented the novel dictionary-based segmenter (Mini-segmenter) to improve state-of-art performancefor Chinese segmentation, an modified Gale-Church algorithm (GaChalign) to improve sentence align-ments for syllabic-alphabetic language pairs and reimplemented an open source Indotag Bahasa In-donesian POS tagger.AcknowledgementsThis research was partially funded by a joint JSPS/NTU grant on Revealing Meaning through MultipleLanguages and the Erasmus Mundus Action 2 program MULTI of the European Union, grant agreementnumber 2009-5259-5.The research leading to these results has received funding from the People Programme (MarieCurie Actions) of the European Union?s Seventh Framework Programme FP7/2007-2013/ under REAgrant agreement n?317471.ReferencesPi-Chuan Chang, Michel Galley, and Christopher D Manning.
2008.
Optimizing chinese word segmentation formachine translation performance.
In Proceedings of the Third Workshop on Statistical Machine Translation,pages 224?232.
Association for Computational Linguistics.William A. Gale and Kenneth Ward Church.
1993.
A program for aligning sentences in bilingual corpora.
Com-putational Linguistics, 19(1):75?102.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.
2004.
Applying conditional random fields to japanesemorphological analysis.
In EMNLP, pages 230?237.Gary Geunbae Lee, Jeongwon Cha, and Jong-Hyeok Lee.
2002.
Syllable-pattern-based unknown-morphemesegmentation and estimation for hybrid part-of-speech tagging of korean.
Computational Linguistics, 28(1):53?70.Christopher D Manning.
2011.
Part-of-speech tagging from 97% to 100%: is it time for some linguistics?
InComputational Linguistics and Intelligent Text Processing, pages 171?189.
Springer.Cam-Tu Nguyen, Xuan-Hieu Phan, and Thu-Trang Nguyen.
2010.
Jvntextpro: A java-based vietnamese textprocessing tool.
http://jvntextpro.sourceforge.net/.Femphy Pisceldo, Ruli Manurung, and Mirna Adriani.
2009.
Probabilistic part-of-speech tagging for bahasaindonesia.Liling Tan and Francis Bond.
2012.
Building and annotating the linguistically diverse ntu-mc (ntu-multilingualcorpus).
In International Journal of Asian Language Processing, 22(4), page 161?174.Kristina Toutanova, Dan Klein, Christopher D Manning, and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages173?180.
Association for Computational Linguistics.Daniel Varga, Peter Halacsy, AndraS Kornai, Viktor Nagy, Laszlo Nemeth, and Viktor TrOn.
2007.
Parallelcorpora for medium density languages.
Recent Advances in Natural Language Processing IV: Selected Papersfrom RANLP 2005, 292:247.89
