Cross-Cutting AspectsofCross-Language Question Answering SystemsBogdan SacaleanuLanguage Technology GroupDFKI GmbHSaarbr?cken, Germanybogdan@dfki.deG?nter NeumannLanguage Technology GroupDFKI GmbHSaarbr?cken, Germanyneumann@dfki.deAbstractWe describe on-going work in the devel-opment of a cross-language question-answering framework for the open do-main.
An overview of the framework isbeing provided, some details on the im-portant concepts of a flexible frameworkare presented and two cross-cutting as-pects (cross-linguality and credibility) forquestion-answering systems are up fordiscussion.1 IntroductionDifferent projects, different evaluation forums,different tasks, different languages, differentdocument collections, different question types,different answer processing strategies ?
Anyonefamiliar with all these concepts knows the com-plexity and what a daunting prospect of develop-ing a QA-System easily adaptable to ever chang-ing requirements this might be.
We have startedoff with a ?prototype-and-go?
approach, trying tokeep pace with the emergence of new tasks andmanaging the scarcity of your time and of yourresources, to realize later on that what we had isa bunch of prototypes very tuned to their taskrequirements.
Trying to adapt them to new re-quirements seemed often more difficult thenstarting off with a new one.
Therefore we startedlooking for an alternative, which should be moreflexible and should allow us to cover much morerequirements?
variations; in other words we wereconsidering putting together a Question Answer-ing framework.In the rest of the paper we will shortly over-view the components of such a framework andwill describe the relevant aspects of the solutionoffered for each of them, aspects that should ac-count for a large variety of question types,document collections and answer processingtechniques, as well as for several languages.
Wewill continue with a discussion of two issues thatcut across several components of the framework,namely: cross-linguality and answer credibility,and will conclude by shortly naming the domainsof usage for the framework and future work.2 Framework OverviewBased on an existing set of cross-languageQuestion Answering prototypes developed fordifferent requirements, we began by looking forthe commonalities among them.
Following is alist of reusable components that might beconsidered as a starting point in defining a QAframework (see Figure 1).Several components along the work-flow of atypical QA system were identified: a UnitAlignment component in cross-language envi-ronments and a Query Expansion compo-nent for the Question Analysis task; a UnitProcessor and a Query Generator com-ponent for the Information Retrieval task; aMention Chain component for the AnswerExtraction task and a Scoring Strategy forthe Answer Selection task.
(see Figure 1)EACL 2006 Workshop on Multilingual Question Answering - MLQA0615Beside the reusability aspect in context of aQA framework, extensibility is a furtherimportant issue.
We have approached it by con-centrating on a flexible representation format ofthe data being passed around in the framework,namely the representation of the Question-Object, the InformationUnit and the An-swerObject.2.1 Reusability?Reusability is the likelihood a segment ofstructured code can be used again to add newfunctionalities with slight or no modification?.
(Wikipedia)In a cross-language setting of a QA system,which is crossing the language barrier at the userend rather then at the document end, there aretwo approaches for getting the formalized userinformation need (QuestionObject) to thedocuments?
language: either creating it based onthe question translation or analyzing the originalquestion and translating the formalized result.This last approach is convenient when machinereadable dictionaries, part-of-speech taggers andother bilingual lexical resources (i.e.
WordNets)for both languages are available.
For this purposea Unit Alignment Component was designedthat produces an alignment of simple (words)and complex (phrases) syntactic units from thesource to target language.Query Expansion is another componentpresent among the support structures of ourframework.
Backed by lexico-semantic resourcesas EuroWordNet [V98] it can be used for all lan-guages supported by these resource.
For a giveninput word, it can return synonyms, hyponymsand hypernyms according to the following algo-rithm:if (trgtWord_is_not_ambig)return Information;else if (trgtWord_is_ambig){TRANS:1. translate Question2.
do Unit Alignment3.
if (transTrgtWord_is_EWN_aligned)a. if (alignment_is_not_ambig)return Information;b. else if (alignment_is_ambig)save Alignments;goto TRANS;}intersection = intersect_saved_alignments();if (intersection.size == N)         // strict N=1return Information_for_intersection;return NULL;An example for the above algorithm up to thestage 3.a is given in Section 3.1.
This representsthe ideal case, when our input can bedisambiguated using the alignments of a questiontranslation.
But more often it is the case ofadvancing over this stage to 3.b, when theambiguous EuroWordNet algnments are beingsaved and a new translation of the questionthrough other online translation services isattempted.
The idea behind this expansionmethod is that lexical diversity of differenttranslations could narrow down the ambiguity ofa word to a desired level (N).Figure 1: Framework ArchitectureEACL 2006 Workshop on Multilingual Question Answering - MLQA0616To select an initial set of information unitsrelevant to the question, traditional search en-gines are being used to scale down the searchspace.
Two important aspects in this process are:the level of detail at which the indexation unitsare chosen and the granularity of the informationunits (be it document, paragraph, sentence orphrase).
Two subcomponents are important atthis stage: the Unit Processor and theQuery Generator.The Unit Processor covers the above-mentioned aspects: it takes as input anInformationUnit (i.e., a raw text document)and it eitherreduces it to a set of new units (i.e.,sentences), or it annotates the unit at differentlevels (i.e., named entities, grammatical rela-tions).
This way, by chaining different UnitProcessors, you can both reduce the informa-tion unit size and generate new indexing units.The Query Generator relies on an ab-stract description of the processing method toaccordingly generate the IRQuery to make useof the advanced indexation units.
For example,when named entities were annotated during theUnit Processor chaining and used as index-ing units, the Query Generator will adaptthe IRQuery so as to search for an additionalfield (neType) that might reflect the expectedanswer type for the question considered.
Weconsider the Query Generator as being themediator between the question analysis resultQuestionObject (answer type, constraints)and the search engine serving the retrieval com-ponent with information units (documents, sen-tences).
Even more, the Query Generatorrelies on an abstract description of the searchengine, too, and can adapt the IRQuery accord-ing to either a boolean or a ranked search engine.A Mention Chain component in the An-swer Extraction task provides an ease of burdenfor the Selection task by computing answer ref-erence chains.
This is very helpful for factoidquestions on the Web, and not only, where re-dundancy of an answer candidate is a good hintfor its potential selection and credibility.
A men-tion chain contains all answers sharing a com-mon normalized representation, determined ei-ther through the string similarity of the answersonly or by additionally employing context en-tailment measures.The Scoring Strategy component buildson the mathematical graph theory and reducesthe answer candidate scoring issue to a shortestpath problem on lexical graphs.
In most of thecases the answer ?suitability?
could be scaleddown to computing a distance metric for the an-swer and some information in the question (i.e.,keywords, focus).
Both a simple textual distancemeasure and another one based on dependencystructures were implemented on these graphstructures, with slight variations making use ofweight and cost properties for graph edges.Based on available web search API (i.e.,Google, Yahoo) the Answer Validationcomponent computes a total frequency count ofco-occurrence for pairs of question and answer,assuming that the right answer shares more con-texts with the question than any other candidateand that the considered answers are semanticindependent and insensitive with respect to thetimeline preferred by the search engines.2.2 Extensibility?Extensibility is a system design principle wherethe implementation takes into consideration fu-ture growth.
?
The central theme is to providefor change while minimizing impact to existingsystem functions.?
(Wikipedia)Extensibility can be approached by two meth-ods during framework design: through softwaredesign patterns and through a common extensibledata representation format.
While we have usedsome patterns through the development of reus-able components like the chaining aspect in theUnit Processor, the normalization function in theMention Chain and the graph design in the Scor-ing Strategy, we have concentrated more on anextensible representation format for data beingpassed around through the framework: theQuestionObject, the InformationUnitand the AnswerObject.
For this purpose wehave used XML, XML Schema and data bindingmethods (JAXB) to guarantee component life onan evolving data format.
The primary benefit ofallowing extensibility in a format is that it en-ables a format to evolve without requiring centralcontrol of the format.
A secondary benefit is thatit allows the format to stay focused and simpleby pushing specialized cases and complex solu-tions into optionally supported extensions.
W3CXML Schema provides two features that promoteextensibility in XML vocabularies: the wildcardsxs:any and xs:anyAttribute are used toallow the occurrence of elements and attributesfrom specified namespaces into a given format,and the xsi:type attribute that can be placedon an element in an XML instance document tochange its type to a more refined subtype.
ThatEACL 2006 Workshop on Multilingual Question Answering - MLQA0617is, according to user?s need, the data exchangedamong the framework?s components can be ex-tended, without changing the framework func-tionality.3 QA Cross-Cutting AspectsIn a question answering framework there are as-pects that does not directly relate to the core con-cerns, but are needed for a proper design.
More-over, they can hardly be pinned down to a com-ponent, as they cross several core components ofthe system.
We are talking about concepts likecross-linguality and credibility ?
system credibil-ity reflected in answer credibility.3.1 Cross-LingualityThere are three traditional approaches that countfor cross?linguality in context of informationmanagement systems:?
translating the queries into the targetlanguage,?
translating the document collection intothe source language or?
translating the queries and the docu-ments into an intermediate representation(inter?lingua).Two types of translation services are wellknown within this context which are based on?
lexical resources (e.g., dictionaries,aligned wordnets), or?
machine translation (e.g., example?basedtranslation).The only feasible approach when dealing withhuge amounts of data, as is the case for questionanswering systems, is translating the questioninto the language of the document collection andthe related issue of back-translating the answersinto the language of the user.We are using two different methods for re-sponding questions asked in a language differentfrom the one of the answer-bearing documents.Both employ online translation services (Alta-vista, FreeTranslation, etc.)
for crossing the lan-guage barrier, but at different processing steps:before and after formalizing the user informationneed into a QuestionObject.The before?method translates the questionstring in an earlier step, resulting in severalautomatic translated strings, of which the bestone is analyzed by the Question Analysis com-ponent and passed on to the Information Re-trieval component.
This is the strategy we use inan English?German cross-lingual setting.
To bemore precise: the English source question istranslated into several alternative German ques-tions using online MT services.
Each Germanquestion is then parsed with SMES [NP02], ourGerman parser.
The resulting query object is thenweighted according to its linguistic well?formedness and its completeness wrt.
query in-formation (question type, question focus, an-swer?type).The assumption behind this weighting schemeis that ?a translated string is of greater utility forsubsequent processes than another one, if its lin-guistic analysis is more complete or appropri-ate.
?The after?method translates the formal-ized result of the Query Analysis Component byusing the question translations, a language mod-eling tool and a word alignment tool for creatinga mapping of the formal information need fromthe source language into the target language.
Weillustrate this strategy in a German?English set-ting along two lines (using the following Germanquestion as example: In welchem Jahr-zehnt investierten japanischeAutohersteller sehr stark?
):- translations as returned by the on-line MTsystems are being ranked according to alanguage modelIn which decade did Japaneseautomakers invest verystrongly?
(0.7)In which decade did Japanesecar manufacturers investvery strongly?
(0.8)- translations with a satisfactory degree ofresemblance to a natural language utterance (i.e.linguistically well-formedness), given by athreshold on the language model ranking, arealigned based on several filters: back-propagation dictionary filter - based on MRD(machine readable dictionaries), PoS filter -based on statistical part-of-speech taggers, andcognates filter - based on string similaritymeasures (dice coefficient and LCSR (lowestcommon substring ratio)).In: [in:1]welchem: [which:0.5]Jahrzehnt: [decade:1]investierten: [invest:1]EACL 2006 Workshop on Multilingual Question Answering - MLQA0618japanische: [Japanese:0.5]Autohersteller:[car manufacturers:0.8,auto makers:0.1]sehr: [very:1]stark: [strongly:0.5]The evaluation gives evidence that bothstrategies are comparable in results, whereby thelast one is slightly better, due to the fact of notbeing forced to choose a best translation, butworking with and combining all the translationsavailable.
That is, considering and combiningseveral, possible different, translations of thesame question, the chance of detecting atranslation error in an earlier phase of the work?flow becomes higher and avoids errorpropagations through the whole system.The related issue of back-translating is ex-plored by looking for parallel data to the an-swer?s context or metadata, and extracting trans-lation candidates based on their context andstring surface similarity.
For example, in a CLEFsetting, for a German question having as Englishanswer  ?Yasser Arafat?
we have extracted thetime-stamp of the answer?s context (19.07.1994),collected all the data with a time-stamp of07.1994 in the source language, extracted thenamed entities of type PERSON and then aligned?Jassir Arafat?
based on its string surface simi-larity to the initial answer.The translations and their alignment to theoriginal question, according to the above-mentioned after-method, have also a posi-tive side-effect, namely: some of the alignedwords may have several ranked translations.
Asit is the case of the ?Autohersteller?, a wordmight consider the best ranked alignment (?carmanufacturers?)
as its direct translation and theremaining ones as its expanded words.
As such,given a reliable alignment method, cross-linguality can prove supportive even for QueryExpansion.
Moreover, another method of us-age can confirm the added value of cross-linguality for Query Expansion, as de-scribed below.For this task we are using the German and theEnglish wordnets aligned within the EuroWord-Net [V98] lexical resource.
Our goal is to extendthe formalized information need QuestionObject with synonyms for the words that arepresent in the wordnet.Considering the ambiguity of words, a WSDmodule is required as part of the expansion task.For this purpose we are using both the originalquestion and its translations, leveraging the re-duction in ambiguity gained through translation.Our devised pseudo-WSD algorithm worksas following:1. look up every word from the word-translation alignment (see example above) in thelexical resource;2. if the word is not ambiguous (which is, forexample, the case for Japanese) then extendthe Question Object with its synonyms (e.g.,[Japanese, Nipponese]);3. if the word is ambiguous (e.g., invest) then(3a) for every possible reading of it, get itsaligned German correspondent reading (if itexists) and look up that reading in the Germanoriginal question , e.g.,1351398: adorn-clothe-invest (EN)1351223: invest-vest (EN)1400771: empower-endow-endue-gift-indue-invest (EN)1350325: induct-invest-seat (EN)1293271:?
commit-invest-place-put (EN)?
anlegen-investieren (DE)(3b) if an aligned reading is found (e.g., Read-ing-1293271) retain it and add the Englishsynonyms of it to the Question Object, i.e.,expand it with:commit, place, putFollowing the question expansion task, theQuestion Object has been enriched with newwords that are synonyms of the un?ambiguousEnglish words and by synonyms of thoseambiguous words, whose meaning(s) have beenfound in the original German question.
Thus ourexpanded example has gained several moreexpanded words as follows:{Nipponese,commit,place,put}3.2 Answer CredibilityIn the ideal case, a Questions Answering System(QAS) will deliver correct answers and knowsthat they are correct, i.e., it can deliver a proof ofthe correctness of the answers.
However, at leastfor the case of open-domain textual QAapplications, this is out of reach with currenttechnology.
Thus, current QAS can only deliveranswers with certain trustworthyness.
Since, areceiver of an answer usually assumes, that aQAS tries to identify the best answer possible (atEACL 2006 Workshop on Multilingual Question Answering - MLQA0619least for cooperative language games), the QASshould assign a credibility measure to eachselected answer.
The underlying decisions made,can then also be used for explaining, how theanswer?s credibility was determined.We view answer credibility as an additionalprocess to answer extraction and selection thatdetermines the quality of identified answercandidates by checking the plausibility of theanswer sources and context on basis of metainformation.
For example, useful document andweb page information might be:1?
The name of the author of this activity;?
Textual fingerprints of authority, e.g.,?official web page of US government?;?
E-mail address of the contact person forthis activity;?
When was this webpage last updated andlinks were checked (also: is there an regu-lar update);?
The name of the host school or organiza-tion;?
The link structure of the document, e.g.,link IN/OUT density, links to relevantpeople, other authorities, clusters / hierar-chies of authorities;?
Text structure, e.g., textual coherence orstyle;Another important source of meta informationrelates to ontological knowledge, e.g., the consis-tency of contextual information with respect to adomain ontology that defines the scope of theanswer candidates.
By this we mean the follow-ing:?
Given an answer candidate A of a ques-tion Q (e.g., an instance of a concept inquestion) determined by a web-basedQAS.?
Check the textual context of A concern-ing the mentioning of other relevantfacts/concepts, that can be determined viaaccess to an external (domain) ontologyusing relevant terms from Q and A.1For information on the topic of Web Credibility, cf.http://credibility.stanford.edu/ .
This URL links to theWeb Credibility Project of the Stanford PersuasiveTechnology Lab.
Although they do not considercredibility under a strict QA perspective as we do,their work and results are a rich source of inspiration.For example, if the following request is sent to aweb-based QAS: Name IE-systems that arebased on statistical methods.
Assume that forthis question, the QAS identifies a list of namesof IE-systems from textual sources as answercandidates.
Assume further, that the QAS hasaccess to an ontology-based meta-store aboutLanguage Technology terms, cf.
[JU05].
Then,answer credibility checking can use the queryterms IE-system and statistical method for ex-tracting relevant facts from this LT-store, e.g.,names of statistical methods, IE experts, IE sub-tasks or properties of the concept informationextraction.
Next, for each answer candidate,check the textual context of the answer for thementioning of these terms and their mutual rela-tionship.
Then a possible credibility heuristicsmight decide that the more relevant domain-knowledge can be identified in the context of theanswer the higher is the credibility of this an-swer.These examples demonstrate that a wide vari-ety of metadata from different levels can be andshould be exploited for determining the credibil-ity of answers.
Since answer credibility is a newresearch area, it is still unclear which level ofinformation is best suited for which kind of ques-tions and answers.
In order to be able to investi-gate many possible credibility scenarios, we con-sider answer credibility as a complex abstractdata type or QAS credibility model, QAS-CMfor short.
The QAS-CM allows the definition ofdifferent kinds of credibility parameters, whichare related to corresponding meta data and areorthogonal to the component-oriented view ofthe QAS.
The values of the parameters might becomputed and evaluated by different compo-nents, even in parallel.
Thus, the credibility of ananswer is a complex value determined throughcomposition of component-related credibilityvalues.
We are distinguishing two types of meta-data:?
Static metadata: are available directlythrough the textual source of an answercandidate and are represented in form ofannotations, e.g.
HTML/XML tags.?
Dynamic metadata: are computedonline via the components of the QAS,e.g., linguistic entities, semantic relations,textual entailment, text structure and co-herence, topic linkage.EACL 2006 Workshop on Multilingual Question Answering - MLQA0620Following this perspective, we propose thefollowing structure for the QAS-CM (see Figure2).
We distinguish two major subsystems: theQAS-CM itself and the QAS runtime instance.The QAS-CM is further subdivided into threemodules:1.
CM-description2.
Data-Model3.
Process-ModelThe CM-description module defines thecredibility properties in a declarative wayindependently from the specific QAS instanceused.
It defines the name space of the credibilityproperties that are to be implemented and theway they are composed.
We assume that eachindividual credibility property returns a realnumber, i.e., we consider it as a utility function,cf.
[RN03].
Thus, the composition rules definehow a complex value is computed, e.g., bysummation or multiplication.
The CM-description also contains specification about theactual QAS instance, e.g., the type of documentsource or natural language to be processed.
TheCM-description serves as a blueprint for theother two modules, the Data-Model and theProcess-Model.The Data-Model implements credibility prop-erties as decision rules over instantiated metainformation taking into account either static ordynamic meta data.
For example a rule likeentails(<SOURCE>,?officialweb page of <LOCATION>?)
authority:=?Iassigns the weight ?I to the credibility propertywith name authority if the document sourceof the answer candidate has a tag <source>which contains a substring ?official webpage <LOCATION>?, where <LOCATION>contains a location name which corresponds toone mentioned in the Wh-question.
For aquestion like ?Name all German chancellors afterWorld War II.
?, then an answer with a web pagesource, that contains something like <SOURCE>?
official web page of Germany ?</SOURCE> will receive a credibility value ?I.The Process-Model is related to the compo-nents of the QAS that have been used for theprocessing of a question/answer pair.
We assumethat each component can assign a utility measurefor its determined output.
In doing so, a compo-nent should also take into account the size andutility measure of its input and the ambiguitydegree of its output etc.
For example, assumethat the question analysis component (cf.
sec.
3)can recognize the syntactic quality of a parse treeof a Wh-question, as being complete, par-tial or null, cf.
[NS05a].
Then a possibledecision rule for the Process-Model might besyntactic_quality(<QUESTION>, COMPLETE)  QueryAna-lyser:=?I=1in case, the parser of the question analysiscomponent was able to completely parse the Wh-question <QUESTION>.
In a similar way, theweb-validator mentioned in sec.
2.1 can also beintegrated as a credibility function into our QAS-CM using the following decision rule:Figure 2.
Credibility ModelEACL 2006 Workshop on Multilingual Question Answering - MLQA0621validate(Web, <QUESTION>,<CurrAnsw>)  WebValua-tor:=?INote that in case statistical components are usedin QAS, e.g., a stochastic NE-recognizer, thenthe probabilities of the those components can beused for defining the utility values for therepresentative decision rules.
Furthermore note,that each decision rule of the Process-Modelcorresponds to a single component.
Thecompositon of several such rules are defined inthe CM-description module.We are currently implementing QAS-CMfollowing the same object-oriented framework asdescribed in sec.
3, which eases integration of thecomponents of the QAS.
The data and theprocess model are implemented as a set ofproduction rules using the RuleML language, cf.http://www.ruleml.org/.
Currently, we define thevalues of ?I manually.
This is more easier toimplement but more tricky to maintain becauseof potential mutual interdependencies betweenindividual values.
Therefore, in the nextdevelopment cycle of our QAS-CM, we will usea statistical approach for automatically acquiringoptimal parameter settings.
Starting point will bea question/answer/document corpus.
This corpuscan directly be used for training the Data-Model.In the case of the Process-Model, a BayesianNetwork will be dynamically trained followingideas from IE-research, cf.
[PP03].
Since in thiscase we also need output from all major QAScomponents, we are integrating an exhaustivetracing mechanism into our QA framework as abasis for automatically determining initialtraining material.4 ResultsThe presented QA framework has been usedboth in mono-lingual and cross-lingual scenariosfor closed document collections (CLEF collec-tion) and open document collections (WorldWide Web).
In terms of reusability and extensi-bility, the framework allowed for up to twoweeks of work for building fully functional QAsystems for different use scenarios.
In terms oftime performance for systems build upon theframework, the following figures apply: for sys-tems used to query the Web a response time ofup to 20 seconds in mono-lingual settings couldbe measured; those querying the CLEF documentcollection in a mono-lingual setting (Germanonly) registered a latency of up to 3 seconds andfor a cross-lingual setting of up to 15 seconds.The qualitative performance has been measuredonly on closed document collection and the bestresults for 200 questions of the CLEF 2005evaluation campaign in different use scenarios,according to [NS05b], were as follows:Right Wrong InexactDeDe 87 43.5% 100 13DeEn 51 25.5% 141 8EnDe 46 23% 141 12Reference[JU05] B. J?rg and  H. Uszkoreit.
The Ontology-based Architecture of LT World, a ComprehensiveWeb Information System for a Science and Tech-nology Discipline.
Leitbild Informations-kompetenz: Positionen - Praxis - Perspektiven imeurop?ischen Wissensmarkt.
27.
Online Tagung,2005.[NP02]G.
Neumann and J. Piskorski.
A shallow textprocessing core engine.
Computational Intelli-gence, 18(3):451?476, 2002.[NS05a]G.
Neumann and S. Sacaleanu.
Experimentson robust NL-question interpretation and multi-layered document annotation for a cross-languagequestion/answering system.
In Clef 2004, volume3491.
Springer-Verlag LNCS, 2005.
[NS05b] G. Neumann and B. Sacaleanu.
DFKI's LT-lab at the CLEF 2005 Multiple Language QuestionAnsweringTrack.
In Working Notes for the CLEF2005 Workshop, 21-23 September, Vienna, Aus-tria, 2005.
[PP03] L. Peshkin and A. Pfefer.
Bayesian Informa-tion Extraction Network.
In proceedings of IJCAI,2003.
[RN03] S. Russell and P. Norvig.
Artificial Intelli-gence: A Modern Approach.
Prentice-Hall, Engle-wood Cliffs, NJ, 2nd edition, 2003.Vossen, P. (eds) 1998 EuroWordNet: A Multi-lingual Database with Lexical Semantic Networks,Kluwer Academic Publishers, Dordrecht.EACL 2006 Workshop on Multilingual Question Answering - MLQA0622
