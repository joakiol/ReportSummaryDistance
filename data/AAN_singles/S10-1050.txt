Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 226?229,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsECNU: Effective Semantic Relations Classification without ComplicatedFeatures or Multiple External CorporaYuan Chen?, Man Lan?,?, Jian Su?, Zhi Min Zhou?, Yu Xu?
?East China Normal University, Shanghai, PRC.
?Institute for Infocomm Research, Singapore.lanman.sg@gmail.comAbstractThis paper describes our approach to theautomatic identification of semantic rela-tions between nominals in English sen-tences.
The basic idea of our strategyis to develop machine-learning classifierswhich: (1) make use of class-independentfeatures and classifier; (2) make use ofa simple and effective feature set withouthigh computational cost; (3) make no useof external annotated or unannotated cor-pus at all.
At SemEval 2010 Task 8 oursystem achieved an F-measure of 75.43%and a accuracy of 70.22%.1 IntroductionKnowledge extraction of semantic relations be-tween pairs of nominals from English text is oneimportant application both as an end in itself andas an intermediate step in various downstreamNLP applications, such as information extraction,summarization, machine translation, QA etc.
It isalso useful for many auxiliary tasks such as wordsense disambiguation, language modeling, para-phrasing and discourse relation processing.In the past decade, semantic relation classifica-tion has attracted a lot of interest from researchersand a wide variety of relation classificationschemes exist in the literature.
However, mostresearch work is quite different in definition ofrelations and granularities of various applications.That is, there is little agreement on relationinventories.
SemEval 2010 Task 8 (Hendrickxet al, 2008) provides a new standard benchmarkfor semantic relation classification to a widercommunity, where it defines 9 relations includ-ing CAUSE-EFFECT, COMPONENT-WHOLE,CONTENT-CONTAINER, ENTITY-DESTINATION,ENTITY-ORIGIN, INSTRUMENT-AGENCY,MEMBER-COLLECTION, MESSAGE-TOPIC,PRODUCT-PRODUCER, and a tenth pseudo-relation OTHER (where relation is not one of the9 annotated relations).Unlike the previous semantic relation task inSemEval 2007 Task 4, the current evaluation pro-vides neither query pattern for each sentence normanually annotated word sense (in WordNet se-mantic) for each nominals.
Since its initiative isto provide a more realistic real-world applicationdesign that is practical, any classification systemmust be usable without too much effort.
It needsto be easily computable.
So we need to take intoaccount the following special considerations.1.
The extracted features for relation are ex-pected to be easily computable.
That is, thesteps in the feature extraction process are tobe simple and direct for the purpose of reduc-ing errors possibly introduced by many NLPtools.
Furthermore, a unified (global) featureset is set up for all relations rather than foreach relation.2.
Most previous work at SemEval 2007 Task4 leveraged on external theauri or corpora(whether unannotated or annotated) (Davi-dov and Rappoport, 2008), (Costello, 2007),(Beamer et al, 2007) and (Nakov and Hearst,2008) that make the task adaption to differentdomains and languages more difficult, sincethey would not have such manually classifiedor annotated corpus available.
From a practi-cal point of view, our system would make useof less resources.3.
Most previous work at Semeval 2007 Task4 constructed several local classifiers on dif-ferent algorithms or different feature subsets,one for each relation (Hendrickx et al, 2007)and (Davidov and Rappoport, 2008).
Our ap-proach is to build a global classifier for allrelations in practical NLP settings.226Based on the above considerations, the idea ofour system is to make use of external resources asless as possible.
The purpose of this work is two-fold.
First, it provides an overview of our simpleand effective process for this task.
Second, it com-pares different features and classification strate-gies for semantic relation.Section 2 presents the system description.
Sec-tion 3 describes the results and discussions.
Sec-tion 4 concludes this work.2 System Description2.1 Features ExtractionFor each training and test sentence, we reduce theannotated target entities e1 and e2 to single nounsnoun1 and noun2, by keeping their last nouns only,which we assume to be heads.We create a global feature set for all relations.The features extracted are of three types, i.e., lex-ical, morpho-syntactic and semantic.
The featureset consists of the following 6 types of features.Feature set 1: Lemma of target entities e1and e2.
The lemma of the entities annotated inthe given sentence.Feature set 2: Stem and POS of words be-tween e1 and e2.
The stem and POS tag of thewords between two nominals.
First all the wordsbetween two nominals were extracted and then thePorter?s stemming was performed to reduce wordsto their base forms (Porter, 1980).
Meanwhile,OpenNLP postag tool was used to return part-of-speech tagging for each word.Feature set 3: syntactic pattern derived fromsyntactic parser between e1 and e2.
Typically,the verb phrase or preposition phrase which con-tain the nominals are important for relation clas-sification.
Therefore, OpenNLP Parser was per-formed to do full syntactic parsing for each sen-tence.
Then for each nominal, we look for its par-ent node in the syntactic tree until the parent nodeis a verb phrase or preposition phrase.
Then thelabel of this phrase and the verb or preposition ofthis phrase were extracted as the syntactic features.Besides, we also extracted other 3 feature typeswith the aid of WordNet.Feature set 4: WordNet semantic class of e1and e2.
The WordNet semantic class of each an-notated entity in the relation.
If the nominal hastwo and more words, then we examine the seman-tic class of ?w1 w2?
in WordNet.
If no result re-turned from WordNet, we examine the semanticclass of head in the nominal.
Since the cost ofmanually WSD is expensive, the system simplyused the first (most frequent) noun senses for thosewords.Feature set 5: meronym-holonym relationbetween e1 and e2.
The meronym-holonymrelation between nominals.
These informationare quite important for COMPONENT-WHOLE andMEMBER-COLLECTION relations.
WordNet3.0provides meronym and holonym information forsome nouns.
The features are extracted in the fol-lowing steps.
First, for nominal e1, we extract itsholonym from WN and for nominal e2, we extractits Synonyms/Hypernyms.
Then, the system willcheck if there is same word between e1?s holonymand e2?s synonym & hypernym.
The yes or noresult will be a binary feature.
If yes, we also ex-amine the type of this match is ?part of ?
or ?mem-ber of ?
in holonym result.
Then this type is alsoa binary feature.
After that, we exchange the posi-tion of e1 and e2 and perform the same process-ing.
By creating these features, the system canalso take the direction of relations into account.Feature set 6: hyponym-hypernym rela-tion between nominal and the word of ?con-tainer?.
This feature is designed for CONTENT-CONTAINER relation.
For each nominal, WordNetreturns its hypernym set.
Then the system examineif the hypernym set contains the word ?container?.The result leads to a binary feature.2.2 Classifier ConstructionOur system is to build up a global classifier basedon global feature set for all 9 non-Other relations.Generally, for this multi-class task, there are twostrategies for building classifier, which both con-struct classifier on a global feature set.
The firstscheme is to treat this multi-class task as an multi-way classification.
Since each pair of nominalscorresponds to one relation, i.e., single label clas-sification, we build up a 10-way SVM classifier forall 10 relations.
Here, we call it multi-way clas-sification.
That is, the system will construct onesingle global classifier which can classify 10 rela-tions simultaneously in a run.
The second schemeis to split this multi-class task into multiple binaryclassification tasks.
Thus, we build 9 binary SVMclassifiers, one for each non-Other relation.
Notedthat in both strategies the classifiers are built onglobal feature set for all relations.
For the sec-ond multiple binary classification, we also exper-227imented on different prob.
thresholds, i.e., 0.25and 0.5.
Furthermore, in order to reduce errorsand boost performance, we also adopt the major-ity voting strategy to combine different classifiers.3 Results and Discussion3.1 System Configurations and ResultsThe classifiers for all relations were optimizedindependently in a number of 10-fold cross-validation (CV) experiments on the provided train-ing sets.
The feature sets and learning algorithmswhich were found to obtain the highest accuraciesfor each relation were then used when applying theclassifiers to the unseen test data.Table 1 summaries the 7 system configurationswe submitted and their performance on the testdata.Among the above 7 system, SR5 system showsthe best macro-averaged F1 measure.
Table 2 de-scribes the statistics and performance obtained perrelation on the SR5 system.Table 3 shows the performance of these 7 sys-tems on the test data as a function of training setsize.3.2 DiscussionThe first three systems are based on three featuresets, i.e.,F1-F3, with different classification strat-egy.
The next three systems are based on all sixfeature sets with different classification strategy.The last system adopts majority voting scheme onthe results of four systems, i.e., SR1, SR2, SR4and SR5.
Based on the above series of exper-iments and results shown in the above 3 tables,some interesting observations can be found as fol-lows.Obviously, although we did not perform WSDon each nominal and only took the first noun senseas semantic class, WordNet significantly improvedthe performance.
This result is consistent withmany previous work on Semeval 2007 Task 4 andonce again it shows that WordNet is importantfor semantic relation classification.
Specifically,whether for multi-way classification or multiplebinary classification, the systems involved featuresextracted from WordNet performed better than theothers not involved WN, for example, SR4 betterthan SR1 (74.82% vs 60.08%), SR5 better thanSR2 (75.43% vs 72.59%), SR6 better than SR3(72.19% vs 68.50%).Generally, the performance of multiple binaryclassifier is better than multi-way classifier.
Thatmeans, given a global feature set for 9 relations,the performance of 9 binary classifiers is betterthan a 10-way classifier.
Specifically, when F1-F3are involved, SR2 (72.59%) and SR3 (68.50%) areboth better than SR1 (60.08%).
However, whenF1-F6 feature sets are involved, the performanceof SR4 is between that of SR5 and SR6 in terms ofmacro-averaged F1measure.
With respect to ac-curacy measure (Acc), SR4 system performs thebest.Moreover, for multiple binary classification, thethreshold of probability has impact on the perfor-mance.
Generally, the system with prob.
threshold0.25 is better than that with 0.5, for example, SR2better than SR3 (72.59% vs 68.50%), SR5 betterthan SR6 (75.43% vs 72.19%).As an ensemble system, SR7 combines the re-sults of SR1, SR2, SR4 and SR5.
However, thismajority voting strategy has not shown significantimprovements.
The possible reason may be thatthese classifiers come from a family of SVM clas-sifiers and thus the random errors are not signifi-cantly different.Besides, one interesting observation is that SR4system achieved the top 2 performance on TD1data amongst all participating systems.
Thisshows that, even with less training data, SR4 sys-tem achieves good performance.AcknowledgmentsThis work is supported by grants from Na-tional Natural Science Foundation of China(No.60903093), Shanghai Pujiang Talent Program(No.09PJ1404500) and Doctoral Fund of Ministryof Education of China (No.20090076120029).ReferencesI.
Hendrickx, S. N. Kim, Z. Kozareva, P. Nakov, D.?O S?eaghdha, S. Pad?o, M. Pennacchiotti, L. Ro-mano and S. Szpakowicz.
SemEval-2010 Task 8:Multi-Way Classification of Semantic Relations Be-tween Pairs of Nominals.
In Proceedings of the 5thSIGLEX Workshop on Semantic Evaluation, pp.94-99, 2010, Uppsala, Sweden.D.
Davidov and A. Rappoport.
Classification ofSemantic Relationships between Nominals UsingPattern Clusters.
Proceedings of ACL-08: HLT,pp.227-235, 2008.F.
J. Costello.
UCD-FC: Deducing semantic rela-tions using WordNet senses that occur frequently228Run Feature Set Classifier P (%) R (%) F1(%) Acc (%)SR1 F1-F3 multi-way classification 70.69 58.05 60.08 57.05SR2 F1-F3 multiple binary (prob.
threshold =0.25) 74.02 71.61 72.59 67.10SR3 F1-F3 multiple binary (prob.
threshold =0.5) 80.25 60.92 68.50 62.02SR4 F1-F6 multi-way classification 75.72 74.16 74.82 70.52SR5 F1-F6 multiple binary (prob.
threshold =0.25) 75.88 75.29 75.43 70.22SR6 F1-F6 multiple binary (prob.
threshold =0.5) 83.08 64.72 72.19 65.81SR7 F1-F6 majority voting based on SR1, SR2, SR4 and SR5 74.83 75.97 75.21 70.15Table 1: Summary of 7 system configurations and performance on the test data.
Precision, Recall, F1are macro-averaged for system?s performance on 9 non-Other relations and evaluated with directionalitytaken into account.Run Total # P (%) R (%) F1(%) Acc (%)Cause-Effect 328 83.33 86.89 85.07 86.89Component-Whole 312 74.82 65.71 69.97 65.71Content-Container 192 79.19 81.25 80.21 81.25Entity-Destination 292 79.38 86.99 83.01 86.99Entity-Origin 258 81.01 81.01 81.01 81.01Instrument-Agency 156 63.19 58.33 60.67 58.33Member-Collection 233 73.76 83.26 78.23 83.26Message-Topic 261 75.2 73.18 74.17 73.18Product-Producer 231 73.06 61.04 66.51 61.04Other 454 38.56 40.09 39.31 40.09Micro-Average 76.88 76.27 76.57 70.22Macro-Average 75.88 75.29 75.43 70.22Table 2: Performance obtained per relation on SR5 system.
Precision, Recall, F1 are macro-averaged forsystem?s performance on 9 non-Other relations and evaluated with directionality taken into account.Run TD1 TD2 TD3 TD4F1(%) Acc (%) F1(%) Acc (%) F1(%) Acc (%) F1(%) Acc (%)SR1 52.13 49.50 56.58 54.84 58.16 56.16 60.08 57.05SR2 46.24 38.90 47.99 40.45 69.83 64.67 72.59 67.10SR3 39.89 34.56 42.29 36.66 65.47 59.59 68.50 62.02SR4 67.95 63.45 70.58 66.14 72.99 68.94 74.82 70.52SR5 49.32 41.59 50.70 42.77 72.63 67.72 75.43 70.22SR6 42.88 36.99 45.54 39.57 69.87 64.00 72.19 65.81SR7 58.67 52.71 58.87 53.18 72.79 68.09 75.21 70.15Table 3: Performance of these 7 systems on the test data as a function of training set size.
The fourtraining subsets, TD1, TD2, TD3 and TD4, have 1000, 2000, 4000 and 8000 (complete) training samplesrespectively.
F1 is macro-averaged for system?s performance on 9 non-Other relations and evaluatedwith directionality taken into account.in a database of noun-noun compounds.
ACL Se-mEval?07 Workshop, pp.370C373, 2007.B.
Beamer, S. Bhat, B. Chee, A. Fister, A. Rozovskayaand R.Girju.
UIUC: A knowledge-rich approachto identifying semantic relations between nominals.ACL SemEval?07 Workshop, pp.386-389, 2007.I.
Hendrickx, R. Morante, C. Sporleder and A. Bosch.ILK: machine learning of semantic relations withshallow features and almost no data.
ACL Se-mEval?07 Workshop, pp.187C190, 2007.P.
Nakov and M. A. Hearst.
Solving Relational Simi-larity Problems Using the Web as a Corpus.
In Pro-ceedings of ACL, pp.452-460, 2008.M.
Porter.
An algorithm for suffix stripping.
In Pro-gram, vol.
14, no.
3, pp.130-137, 1980.229
