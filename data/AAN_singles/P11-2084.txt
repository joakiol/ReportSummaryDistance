Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 479?484,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsIdentifying Word Translations from Comparable Corpora Using LatentTopic ModelsIvan Vulic?, Wim De Smet and Marie-Francine MoensDepartment of Computer ScienceK.U.
LeuvenCelestijnenlaan 200ALeuven, Belgium{ivan.vulic,wim.desmet,sien.moens}@cs.kuleuven.beAbstractA topic model outputs a set of multinomialdistributions over words for each topic.
Inthis paper, we investigate the value of bilin-gual topic models, i.e., a bilingual LatentDirichlet Allocation model for finding trans-lations of terms in comparable corpora with-out using any linguistic resources.
Experi-ments on a document-aligned English-ItalianWikipedia corpus confirm that the developedmethods which only use knowledge fromword-topic distributions outperform methodsbased on similarity measures in the originalword-document space.
The best results, ob-tained by combining knowledge from word-topic distributions with similarity measures inthe original space, are also reported.1 IntroductionGenerative models for documents such as LatentDirichlet Allocation (LDA) (Blei et al, 2003) arebased upon the idea that latent variables exist whichdetermine how words in documents might be gener-ated.
Fitting a generative model means finding thebest set of those latent variables in order to explainthe observed data.
Within that setting, documentsare observed as mixtures of latent topics, where top-ics are probability distributions over words.Our goal is to model and test the capability ofprobabilistic topic models to identify potential trans-lations from document-aligned text collections.
Arepresentative example of such a comparable textcollection is Wikipedia, where one may observe arti-cles discussing the same topic, but strongly varyingin style, length and even vocabulary, while still shar-ing a certain amount of main concepts (or topics).We try to establish a connection between such latenttopics and an idea known as the distributional hy-pothesis (Harris, 1954) - words with a similar mean-ing are often used in similar contexts.Besides the obvious context of direct co-occurrence, we believe that topic models are an ad-ditional source of knowledge which might be usedto improve results in the quest for translation can-didates extracted without the availability of a trans-lation dictionary and linguistic knowledge.
We de-signed several methods, all derived from the coreidea of using word distributions over topics as anextra source of contextual knowledge.
Two wordsare potential translation candidates if they are oftenpresent in the same cross-lingual topics and not ob-served in other cross-lingual topics.
In other words,a wordw2 from a target language is a potential trans-lation candidate for a word w1 from a source lan-guage, if the distribution of w2 over the target lan-guage topics is similar to the distribution of w1 overthe source language topics.The remainder of this paper is structured as fol-lows.
Section 2 describes related work, focusing onprevious attempts to use topic models to recognizepotential translations.
Section 3 provides a shortsummary of the BiLDA model used in the experi-ments, presents all main ideas behind our work andgives an overview and a theoretical background ofthe methods.
Section 4 evaluates and discusses ini-tial results.
Finally, section 5 proposes several ex-tensions and gives a summary of the current work.4792 Related WorkThe idea to acquire translation candidates basedon comparable and unrelated corpora comes from(Rapp, 1995).
Similar approaches are described in(Diab and Finch, 2000), (Koehn and Knight, 2002)and (Gaussier et al, 2004).
These methods needan initial lexicon of translations, cognates or simi-lar words which are then used to acquire additionaltranslations of the context words.
In contrast, ourmethod does not bootstrap on language pairs thatshare morphology, cognates or similar words.Some attempts of obtaining translations usingcross-lingual topic models have been made in thelast few years, but they are model-dependent and donot provide a general environment to adapt and ap-ply other topic models for the task of finding trans-lation correspondences.
(Ni et al, 2009) have de-signed a probabilistic topic model that fits Wikipediadata, but they did not use their models to obtain po-tential translations.
(Mimno et al, 2009) retrievea list of potential translations simply by selectinga small number N of the most probable words inboth languages and then add the Cartesian productof these sets for every topic to a set of candidatetranslations.
This approach is straightforward, but itdoes not catch the structure of the latent topic spacecompletely.Another model proposed in (Boyd-Graber andBlei, 2009) builds topics as distributions over bilin-gual matchings where matching priors may comefrom different initial evidences such as a machinereadable dictionary, edit distance, or the Point-wise Mutual Information (PMI) statistic scores fromavailable parallel corpora.
The main shortcoming isthat it introduces external knowledge for matchingpriors, suffers from overfitting and uses a restrictedvocabulary.3 MethodologyIn this section we present the topic model we usedin our experiments and outline the formal frameworkwithin which three different approaches for acquir-ing potential word translations were built.3.1 Bilingual LDAThe topic model we use is a bilingual extensionof a standard LDA model, called bilingual LDA(BiLDA), which has been presented in (Ni et al,2009; Mimno et al, 2009; De Smet and Moens,2009).
As the name suggests, it is an extensionof the basic LDA model, taking into account bilin-guality and designed for parallel document pairs.We test its performance on a collection of compara-ble texts which are document-aligned and thereforeshare their topics.
BiLDA takes advantage of thedocument alignment by using a single variable thatcontains the topic distribution ?, that is language-independent by assumption and shared by the pairedbilingual comparable documents.
Topics for eachdocument are sampled from ?, from which the wordsare sampled in conjugation with the vocabulary dis-tribution ?
(for language S) and ?
(for languageT).
Algorithm 3.1 summarizes the generative story,while figure 1 shows the plate model.Algorithm3.1: GENERATIVE STORY FOR BILDA()for each document pair djdo??????????????
?for each word position i ?
djSdo{sample zSji ?Mult(?
)sample wSji ?Mult(?, zSji)for each word position i ?
djTdo{sample zTji ?Mult(?
)sample wTji ?Mult(?, zTji)DN M????
?zSji zTjiwSji wTjiFigure 1: The standard bilingual LDA modelHaving one common ?
for both of the related doc-uments implies parallelism between the texts.
Thisobservation does not completely hold for compara-ble corpora with topically aligned texts.
To train the480model we use Gibbs sampling, similar to the sam-pling method for monolingual LDA, with param-eters ?
and ?
set to 50/K and 0.01 respectively,where K denotes the number of topics.
After thetraining we end up with a set of ?
and ?
word-topicprobability distributions that are used for the calcu-lations of the word associations.If we are given a source vocabulary WS , then thedistribution ?
of sampling a new token as word wi ?WS from a topic zk can be obtained as follows:P (wi|zk) = ?k,i =n(wi)k + ?
?|WS |j=1 n(wj)k +WS?
(1)where, for a word wi and a topic zk, n(wi)k denotesthe total number of times that the topic zk is assignedto the word wi from the vocabulary WS , ?
is a sym-metric Dirichlet prior,?|WS |j=1 n(wj)k is the total num-ber of words assigned to the topic zk, and |WS | isthe total number of distinct words in the vocabulary.The formula for a set of ?
word-topic probabilitydistributions for the target side of a corpus is com-puted in an analogical manner.3.2 Main FrameworkOnce we derive a shared set of topics along withlanguage-specific distributions of words over topics,it is possible to use them for the computation of thesimilarity between words in different languages.3.2.1 KL MethodThe similarity between a source word w1 and a tar-get word w2 is measured by the extent to whichthey share the same topics, i.e., by the extent thattheir conditional topic distributions are similar.
Oneway of expressing similarity is the Kullback-Leibler(KL) divergence, already used in a monolingual set-ting in (Steyvers and Griffiths, 2007).
The simi-larity between two words is based on the similar-ity between ?
(1) and ?
(2), the similarity of con-ditional topic distributions for words w1 and w2,where ?
(1) = P (Z|w1)1 and ?
(2) = P (Z|w2).
Wehave to calculate the probabilities P (zj |wi), whichdescribe a probability that a given word is assignedto a particular topic.
If we apply Bayes?
rule, weget P (Z|w) = P (w|Z)P (Z)P (w) , where P (Z) and P (w)1P (Z|w1) refers to a set of all conditional topic distributionsP (zj |w1)are prior distributions for topics and words respec-tively.
P (Z) is a uniform distribution for the BiLDAmodel, whereas this assumption clearly does nothold for topic models with a non-uniform topic prior.P (w) is given by P (w) = P (w|Z)P (Z).
If theassumption of uniformity for P (Z) holds, we canwrite:P (zj |wi) ?P (wi|zj)Norm?= ?j,iNorm?
(2)for an English word wi, and:P (zj |wi) ?P (wi|zj)Norm?= ?j,iNorm?
(3)for a French word wi, where Norm?
denotes thenormalization factor?Kj=1 P (wi|zj), i.e., the sumof all probabilities ?
(or probabilities ?
forNorm?
)for the currently observed word wi.We can then calculate the KL divergence as fol-lows:KL(?
(1), ?
(2)) ?K?j=1?j,1Norm?log ?j,1/Norm??j,2/Norm?
(4)3.2.2 Cue MethodAn alternative, more straightforward approach(called the Cue method) tries to express similaritybetween two words emphasizing the associative re-lation between two words in a more natural way.
Itmodels the probability P (w2|w1), i.e., the probabil-ity that a target word w2 will be generated as a re-sponse to a cue source word w1.
For the BiLDAmodel we can write:P (w2|w1) =K?j=1P (w2|zj)P (zj |w1)=K?j=1?j,2?j,1Norm?
(5)This conditioning automatically compromises be-tween word frequency and semantic relatedness(Griffiths et al, 2007), since higher frequency wordstend to have higher probabilities across all topics,but the distribution over topics P (zj |w1) ensuresthat semantically related topics dominate the sum.4813.2.3 TI MethodThe last approach borrows an idea from informationretrieval and constructs word vectors over a sharedlatent topic space.
Values within vectors are theTF-ITF (term frequency - inverse topic frequency)scores which are calculated in a completely ana-logical manner as the TF-IDF scores for the orig-inal word-document space (Manning and Schu?tze,1999).
If we are given a source word wi, n(wi)k,S de-notes the number of times the word wi is associatedwith a source topic zk.
Term frequency (TF) of thesource word wi for the source topic zk is given as:TFi,k =n(wi)k,S?wj?WSn(wj)k,S(6)Inverse topical frequency (ITF) measures the gen-eral importance of the source word wi across allsource topics.
Rare words are given a higher im-portance and thus they tend to be more descriptivefor a specific topic.
The inverse topical frequencyfor the source word wi is calculated as2:ITFi = logK1 + |k : n(wi)k,S > 0|(7)The final TF-ITF score for the source wordwi andthe topic zk is given by TF?ITFi,k = TFi,k ?ITFi.We calculate the TF-ITF scores for target words as-sociated with target topics in an analogical man-ner.
Source and target words share the same K-dimensional topical space, where K-dimensionalvectors consisting of the TF-ITF scores are builtfor all words.
The standard cosine similarity met-ric is then used to find the most similar word vectorsfrom the target vocabulary for a source word vec-tor.
We name this method the TI method.
For in-stance, given a source word w1 represented by a K-dimensional vector S1 and a target word w2 repre-sented by a K-dimensional vector T 2, the similaritybetween the two words is calculated as follows:2Stronger association with a topic is modeled by setting ahigher threshold value in n(wi)k,S > threshold, where we havechosen 0.cos(w1, w2) =?Kk=1 S1k ?
T 2k?
?Kk=1 (S1k)2 ??
?Kk=1 (T 2k )2(8)4 Results and DiscussionAs our training corpus, we use the English-ItalianWikipedia corpus of 18, 898 document pairs, whereeach aligned pair discusses the same subject.
In or-der to reduce data sparsity, we keep only lemmatizednoun forms for further analysis.
Our Italian vocabu-lary consists of 7, 160 nouns, while our English vo-cabulary contains 9, 166 nouns.
The subset of the650 most frequent terms was used for testing.
Wehave used the Google Translate tool for evaluations.As our baseline system, we use the cosine similar-ity between Italian word vectors and English wordvectors with TF-IDF scores in the original word-document space (Cos), with aligned documents.Table 1 shows the Precision@1 scores (the per-centage of words where the first word from the listof translations is the correct one) for all three ap-proaches (KL, Cue and TI), for different numberof topics K. Although KL is designed specificallyto measure the similarity of two distributions, its re-sults are significantly below those of the Cue and TI,whose performances are comparable.
Whereas thelatter two methods yield the highest results aroundthe 2, 000 topics mark, the performance of KL in-creases linearly with the number of topics.
This isan undesirable result as good results are computa-tionally hard to get.We have also detected that we are able to boostoverall scores if we combine two methods.
We haveopted for the two best methods (TI+Cue), whereoverall score is calculated by Score =?
?ScoreCue+ScoreTI .3 We also provide the results obtained bylinearly combining (with equal weights) the cosinesimilarity between TF-ITF vectors with that betweenTF-IDF vector (TI+Cos).In a more lenient evaluation setting we employ themean reciprocal rank (MRR) (Voorhees, 1999).
Fora source word w, rankw denotes the rank of its cor-rect translation within the retrieved list of potentialtranslations.
MRR is then defined as follows:3The value of ?
is empirically set to 10482K KL Cue TI TI+Cue TI+Cos200 0.3015 0.1800 0.3169 0.2862 0.5369500 0.2846 0.3338 0.3754 0.4000 0.5308800 0.2969 0.4215 0.4523 0.4877 0.56311200 0.3246 0.5138 0.4969 0.5708 0.59851500 0.3323 0.5123 0.4938 0.5723 0.59081800 0.3569 0.5246 0.5154 0.5985 0.61232000 0.3954 0.5246 0.5385 0.6077 0.60462200 0.4185 0.5323 0.5169 0.5908 0.60152600 0.4292 0.4938 0.5185 0.5662 0.59073000 0.4354 0.4554 0.4923 0.5631 0.59533500 0.4585 0.4492 0.4785 0.5738 0.5785Table 1: Precision@1 scores for the test subset of the IT-EN Wikipedia corpus (baseline precision score: 0.5031)MRR = 1|V |?w?V1rankw(9)where V denotes the set of words used for evalu-ation.
We kept only the top 20 candidates from theranked list.
Table 2 shows the MRR scores for thesame set of experiments.K KL Cue TI TI+Cue TI+Cos200 0.3569 0.2990 0.3868 0.4189 0.5899500 0.3349 0.4331 0.4431 0.4965 0.5808800 0.3490 0.5093 0.5215 0.5733 0.61731200 0.3773 0.5751 0.5618 0.6372 0.65141500 0.3865 0.5756 0.5562 0.6320 0.64351800 0.4169 0.5858 0.5802 0.6581 0.65832000 0.4561 0.5841 0.5914 0.6616 0.65482200 0.4686 0.5898 0.5753 0.6471 0.65232600 0.4763 0.5550 0.5710 0.6268 0.64163000 0.4848 0.5272 0.5572 0.6257 0.64653500 0.5022 0.5199 0.5450 0.6238 0.6310Table 2: MRR scores for the test subset of the IT-ENWikipedia corpus (baseline MRR score: 0.5890)Topic models have the ability to build clusters ofwords which might not always co-occur together inthe same textual units and therefore add extra infor-mation of potential relatedness.
Although we havepresented results for a document-aligned corpus, theframework is completely generic and applicable toother topically related corpora.Again, the KL method has the weakest perfor-mance among the three methods based on the word-topic distributions, while the other two methodsseem very useful when combined together or whencombined with the similarity measure used in theoriginal word-document space.
We believe that theresults are in reality even higher than presented inthe paper, due to errors in the evaluation tool (e.g.,the Italian word raggio is correctly translated as ray,but Google Translate returns radius as the first trans-lation candidate).All proposed methods retrieve lists of semanti-cally related words, where synonymy is not the onlysemantic relation observed.
Such lists provide com-prehensible and useful contextual information in thetarget language for the source word, even when thecorrect translation candidate is missing, as might beseen in table 3.
(1) romanzo (2) paesaggio (3) cavallo(novel) (landscape) (horse)writer tourist horsenovella painting studnovellette landscape horsebackhumorist local hoofnovelist visitor breedessayist hut staminapenchant draftsman luggageformative tourism mareforeword attraction ridingauthor vegetation ponyTable 3: Lists of the top 10 translation candidates, wherethe correct translation is not found (column 1), lies hiddenlower in the list (2), and is retrieved as the first candidate(3); K=2000; TI+Cue.5 ConclusionWe have presented a generic, language-independentframework for mining translations of words fromlatent topic models.
We have proven that topicalknowledge is useful and improves the quality ofword translations.
The quality of translations de-pends only on the quality of a topic model and itsability to find latent relations between words.
Ournext steps involve experiments with other topic mod-els and other corpora, and combining this unsuper-vised approach with other tools for lexicon extrac-tion and synonymy detection from unrelated andcomparable corpora.AcknowledgementsThe research has been carried out in the frame-work of the TermWise Knowledge Platform (IOF-KP/09/001) funded by the Industrial Research FundK.U.
Leuven, Belgium, and the Flemish SBO-IWTproject AMASS++ (SBO-IWT 0060051).483ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Jordan Boyd-Graber and David M. Blei.
2009.
Multilin-gual topic models for unaligned text.
In Proceedingsof the Twenty-Fifth Conference on Uncertainty in Arti-ficial Intelligence, UAI ?09, pages 75?82.Wim De Smet and Marie-Francine Moens.
2009.
Cross-language linking of news stories on the web usinginterlingual topic modelling.
In Proceedings of theCIKM 2009 Workshop on Social Web Search and Min-ing, pages 57?64.Mona T. Diab and Steve Finch.
2000.
A statistical trans-lation model using comparable corpora.
In Proceed-ings of the 2000 Conference on Content-Based Multi-media Information Access (RIAO), pages 1500?1508.E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,Cyril Goutte, and Herve?
De?jean.
2004.
A geometricview on bilingual lexicon extraction from comparablecorpora.
In Proceedings of the 42nd Annual Meetingon Association for Computational Linguistics, pages526?533.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representation.Psychological Review, 114(2):211?244.Zellig S. Harris.
1954.
Distributional structure.
In Word10 (23), pages 146?162.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
In Pro-ceedings of the ACL-02 Workshop on UnsupervisedLexical Acquisition - Volume 9, ULA ?02, pages 9?16.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical Natural Language Process-ing.
MIT Press, Cambridge, MA, USA.David Mimno, Hanna M. Wallach, Jason Naradowsky,David A. Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 880?889.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.2009.
Mining multilingual topics from Wikipedia.
InProceedings of the 18th International World Wide WebConference, pages 1155?1156.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd AnnualMeeting of the Association for Computational Linguis-tics, ACL ?95, pages 320?322.Mark Steyvers and Tom Griffiths.
2007.
Probabilistictopic models.
Handbook of Latent Semantic Analysis,427(7):424?440.Ellen M. Voorhees.
1999.
The TREC-8 question answer-ing track report.
In Proceedings of the Eighth TExtRetrieval Conference (TREC-8).484
