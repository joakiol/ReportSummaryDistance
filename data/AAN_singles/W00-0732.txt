In: Proceedings of CoNLL-2000 and LLL-2000, pages 148-150, Lisbon, Portugal, 2000.Improving Chunking by Means of Lexical-Contextual Informationin Statistical Language ModelsFer ran  P la  and Anton io  Mo l ina  and Nat iv idad  Pr ie toUniversitat Polit~cnica de ValenciaCamf de Vera s/n46020 Val~ncia (Spain){fpla, amolina, nprieto}@dsic.upv.es1 In t roduct ionIn this work, we present a stochastic approachto shallow parsing.
Most of the current ap-proaches to shallow parsing have a commoncharacteristic: they take the sequence of lex-ical tags proposed by a POS tagger as inputfor the chunking process.
Our system producestagging and chunking in a single process usingan Integrated Language Model (ILM) formal-ized as Markov Models.
This model integratesseveral knowledge sources: lexical probabilities,a contextual Language Model (LM) for everychunk, and a contextual LM for the sentences.We have extended the ILM by adding lexical in-formation to the contextual LMs.
We have ap-plied this approach to the CoNLL-2000 sharedtask improving the performance of tile chunker.2 Overv iew o f  the  sys temThe baseline system described in (Pla et al,2000a) uses bigrams, formalized as finite-stateautomata.
It is a transducer composed of twolevels (see Figure 1).
The upper one (Figure la)represents he contextual LM for the sentences.The symbols associated to the states are POStags (Ci) and chunk descriptors (Si).
The lowerone modelizes the different chunks considered(Figure lb).
In this case, the symbols are thePOS tags (Ci) that belong to the correspond-ing chunk (Si).
Next, a regular substitution ofthe lower models into the upper level is made(Figure lc).
In this way, we get a single Inte-grated LM which shows the possible concate-nations of lexical tags and chunks.
Also, eachstate is relabeled with a tuple (Ci, Sj) whereCi E g and Sj E S. g is the POS tag set usedand S = {\[Si, Si\], Si, S0} is the chunk set de-fined.
\[Si and Si\] stand for the initial and thefinal state of chunk whose descriptor is Si.
Thelabel Si is assigned to those states which are in-side Si chunk, and So is assigned to those stateswhich are outside of any chunk.
All the LMsinvolved have been smoothed by using a back-off technique (Katz, 1987).
We have not spec-ified lexical probabilities in every state of thedifferent contextual models.
We assumed thatP(WjI(Ci, Si)) = P(WjlCi ) for every Si E S.Once the integrated transducer has beenmade, the tagging and shallow parsing processconsists of finding the sequence of states of max-imum probability on it for an input sentence.Therefore, this sequence must be compatiblewith the contextual, syntactical and lexical con-straints.
This process can be carried out bydynamic programming using the Viterbi algo-r ithm (Viterbi, 1967), which has been appropri-ately modified to use our models.
From the dy-namic programming trellis, we can obtain themaximum probability path for the input sen-tence through the model, and thus the best se-quence of lexical tags and the best segmentationin chunks, in a single process.3 Spec ia l i zed  Contextua l  LanguageMode lsThe contextual model for the sentences and themodels for chunks (and, therefore, the ILM) canbe modified taking into account certain wordsin the context where they appear.
This spe-cialization us allows to set certain contextualconstraints which modify the contextual LMsand improve the performance of the chunker (asshown below).
This set of words can be definedusing some heuristics uch as: the most frequentwords in the training corpus, the words with ahigher tagging error rate, the words that belongto closed classes (prepositions, pronouns, etc.
),or whatever word chosen following some linguis-148(a )  Contextual LM - .
.
(b )  LM for Chunks (S  l ).
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
(c) lntegratedLM~ I I i IFigure 1: Integrated Language Model for Tagging and Chunking.tic criterion.To do this, we added to the POS tag set theset of structural tags (Wi, Cj) for each special-ized word Wi in all of their possible categoriesCj.
Then, we relabelled the training corpus: ifa word Wi was labelled with the POS tag Cj,we changed Cj for the pair (Wi, Cj).
The learn-ing process of the bigram LMs was carried outfrom this new training data set.The Contextual LMs obtained has some spe-cific states which are related to the specializedwords.
In the basic Language Model (ILM), astate was labelled by (Ci, Sj).
In the specializedILM, a state was specified for a certain word Wk(only if the Wk word belongs to the categoryCi).
In this way, the state is relabelled with thetuple (Wk, Ci, Sj) and only the word Wk can beemitted with a probability equal to 1.4 Experimental WorkWe applied both approaches (ILM and spe-cialized ILM) using the training and test dataof the CoNLL-2000 shared task (http://lcg-www.uia.ac.be/conll2000).
We also evaluatedhow the performance of the chunker varies whenwe modify the specialized word set.
Neverthe-less, the use of our approach on other corpora(including different languages), other lexical tagsets or other kinds of chunks can be done in adirect way.Although our system is able to carry out tag-ging and chunking in a single process, we willnot present agging results for this task, as thePOS tags of the data set used are not supervisedand, therefore, a comparison is not possible.We would like to point out that we have simu-lated a morphological nalyzer for English.
Wehave constructed a tag dictionary with the lex-icon of the training set and the test set used.This dictionary gave us the possible lexical tagsfor each word from the corpus.
In no case, wasthe test used to estimate the lexical probabili-ties.As stated above, several criterion can be cho-sen to define the set of specialized words.
Wehave selected the most frequent words in thetraining data set.
We have not taken into ac-count certain words such as punctuation sym-bols, proper nouns, numbers, etc.
This fact didnot decrease the performance of the chunker andalso reduced the number of states of the contex-tual LMs.
Figure 2 shows how the performanceof the chunker (Fz=I) improves as a function ofthe size of the specialized word set.
The best re-sults were obtained with the set of words whosefrequency in the training corpus was larger than80 (about 470 words).
We obtained similar re-sults when only considering the words of thetraining set belonging to closed classes (that,about, as, if, out, while, whether, for, to, ...).In Table 1 we present he results of chunk-ing with the specialized ILM.
When comparingthese results with the results obtained using thebasic ILM, we observed that, in general, the F-149score was improved for each chunk.
The bestimprovement was observed for SBAR (from 0.37to 79.46), PP (from 88.94 to 95.51) and PRT(38.82 to 66.67).5 Conc lus ionsIn this paper, we have presented a system forTagging and Chunking based on an IntegratedLanguage Model that uses a homogeneous for-malism (finite-state machine) to combine differ-ent knowledge sources.
It is feasible both interms of performance and also in terms of com-putational efficiency.All the models involved are learnt automat-ically from data, so the system is very flexiblewith changes in the reference language, changesin POS tags or changes in the definition ofchunks.Our approach allows us to use any regularmodel which has been previously defined orlearnt.
In previous works, we have used bi-grams (Pla et al, 2000a), and we have com-bined them with other more complex modelswhich had been learnt using grammatical in-ference techniques (Pla et al, 2000b).
In thiswork, we used only bigram models improvedwith lexical-contextual information.The Ff~ score obtained increased from 86.64 to90.14 when we used the specialized ILM.
Never-(I I I I I I I I I50  100  150  L200 250  300  350  400  450  500#SPECIAL IZED WORDStest data precision recallADJPADVPCONJPINTJLSTNPPPPRTSBARVP72.89 %79.65%40.00%lOO.OO%0.o0%90.28%95.89%60.31%82.07%91.53%66.89 %74.13%66.67%100.00%0.00%89.41%95.14%74.53%77.01%91.58%F/3=l69.7676.7950.00100.000.0089.8495.5166.6779.4691.55all 90.63% 89.65% 90.14Table 1: Chunking results using specialized ILM(Accuracy= 93.79%)theless, we believe that the models could be im-proved with a more detailed study of the wordswhose contextual information is really relevantto tagging and chunking.6 AcknowledgmentsThis work has been partially supported by theSpanish Research Project CICYT (TIC97-0671-C02-01/02).Re ferencesS.
M. Katz.
1987.
Estimation of Probabilities fromSparse Data for the Language Model Componentof a Speech Recognizer.
IEEE Transactions onAcoustics, Speech and Signal Processing, 35.F.
Pla, A. Molina, and N. Prieto.
2000a.
Taggingand Chunking with Bigrams.
In Proceedings of theCOLING-2000, Saarbrficken, Germany, August.F.
Pla, A. Molina, and N. Prieto.
2000b.
An Inte-grated Statistical Model for Tagging and Chunk-ing Unrestricted Text.
In Proceedings of the Text,Speech and Dialogue 2000, Brno, Czech Republic,September.A.
J. Viterbi.
1967.
Error Bounds for ConvolutionalCodes and an Asymptotically Optimal DecodingAlgorithm.
IEEE Transactions on InformationTheory, pages 260-269, April.Figure 2: F-score as a function of the numberof specialized words in the ILM150
