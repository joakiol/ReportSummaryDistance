Three  Heads  are  Bet ter  than  OneRobert FrederkingCenter for Machine TranslationCarnegie Mellon UniversityPittsburgh, PA 15213ref@cs.cmu.eduSergei NirenburgComputing Research LaboratoryNew Mexico State UniversityLas Cruces, NM 88003sergei@crl.nmsu.eduAbstractMachine translation (MT) systems do notcurrently achieve optimal quality trans-lation on free text, whatever translationmethod they employ.
Our hypothesis isthat the quality of MT will improve if anMT environment uses output from a vari-ety of MT systems working on the sametext.
In the latest version of the Pan-gloss MT project, we collect the results ofthree translation engines - -  typically, sub-sentential chunks - -  in a chart data struc-ture.
Since the individual MT systems op-erate completely independently, their re-sults may be incomplete, conflicting, or re-dundant.
We use simple scoring heuristicsto estimate the quality of each chunk, andfind the highest-score sequence of chunks(the "best cover").
This paper describesin detail the combining method, present-ing the algorithm and illustrations of itsprogress on one of many actual translationsit has produced.
It uses dynamic program-ming to efficiently compare weighted aver-ages of sets of adjacent scored componenttranslations.
The current system operatesprimarily in a human-aided MT mode.
Thetranslation delivery system and its associ-ated post-editing aide are briefly described,as is an initial evaluation of the usefulnessof this method.
Individual M T engines willbe reported separately and are not, there-fore, described in detail here.951 INTRODUCTIONCurrent MT systems, whatever translation methodthey employ, do not reach an optimal output on freetext.
In part, this is due to the inherent problemsof a particular method - -  for instance, the inabil-ity of statistics-based MT to take into account long-distance dependencies, the difficulty in achieving ex-tremely broad coverage in knowledge-based MT sys-tems, or the reliance of most transfer-oriented MTsystems on similarities in syntactic structures of thesource and the target languages.Our hypothesis i that if an MT environment canuse the best results from a variety of MT systemsworking simultaneously on the same text, the overallquality will improve.
Using this novel approach toMT in the latest version of the Pangloss MT project,we submit an input text to a battery of machinetranslation systems (engines), collect their (possibly,incomplete) results in a joint chart data structureand select the overall best translation using a set ofsimple heuristics.2 INTEGRATINGMULTI-ENGINE OUTPUTIn our experiment we used three MT engines:* a knowledge-based MT (KBMT) system, themainline Pangloss engine (Frederking et al,1993b);?
an example-based MT (EBMT) system (see(Nirenburg et al, 1993; Nirenburg et al,1994b); the original idea is due to Nagao (Na-gao, 1984)); and?
a lexical transfer system, fortified with mor-phological analysis and synthesis modules andrelying on a number of databases - -  amachine-readable dictionary (the Collins Span-ish/English), the lexicons used by the KBMTmodules, a large set of user-generated bilingualglossaries as well as a gazetteer and a list ofproper and organization ames.The outputs from these engines (target languagewords and phrases) are recorded in a chart whosepositions correspond to words in the source languageinput.
As a result of the operation of each of theMT engines, new edges are added to the chart, eachlabeled with the translation of a region of the inputstring and indexed by this region's beginning andend positions.
We will refer to all of these edges ascomponents (as in "components of the translation")for the remainder of this article.
The KBMT andEBMT engines also carry a quality score for eachoutput element.
The KBMT scores are producedbased on whether any questionable heuristics wereused in the source analysis or target generation.
TheEBMT scores are produced using a technique basedon human judgements, as described in (Nirenburg etal., 1994a), submitted.UserTranslator'sWorkStationKnowledge-Based MTExample-Based MTChartManagerLexical transfer MTFigure 1: Structure of a multi-engine MT systemFigure 1 presents a general view of the operationof our multi-engine MT system.
The chart managerselects the overall best cover from the collection ofcandidate partial translations by normalizing eachcomponent's quality score (positive, with larger be-ing better), and then selecting the best combinationof components with the help of the chart walk algo-rithm.Figure 2 illustrates the result of this process onthe example Spanish sentence: Al momento de suventa a Iberia, VIASA contaba con ocho aviones,que ten'an en promedio 13 a~os de vuelo which canbe translated into English as At the moment of itssale to Iberia, VIASA had eight airplanes, which hadon average thirteen years o\[ flight (time).
This is asentence from one of the 1993 ARPA MT evaluationtexts.For each component, he starting and ending po-sitions in the chart, the corresponding source lan-guage words, and alternative translations are shown,as well as the engine and the engine-internal qual-ity scores.
Inspection of these translations howsnumerous problems; for example, at position 12,"aviones" is translated, among other things, as "air-crafts".
It must be remembered that these weregenerated automatically from an on-line dictionary,without any lexical feature marking or other humanintervention.
It is well known that such automaticmethods are at the moment less than perfect, to saythe least.
In our current system, this is not a majorproblem, since the results go through a mandatoryediting step, as described below.2.1 Normal i z ing  the  component  scoresThe chart manager normalizes the internal scoresto make them directly comparable.
In the case ofKBMT and EBMT, the pre-existing scores are mod-ified, while lexical transfer esults are scored basedon the estimated reliability of individual databases,from 0.5 up to 15.
Currently the KBMT scores arereduced by a constant, except for known erroneousoutput, which has its score set to zero.
The internalEBMT scores range from 0 being perfect to 10,000being worthless; but the scores are nonlinear.
So aregion selected by a threshold is converted linearlyinto scores ranging from zero to a normalized max-imum EBMT score.
The normalization levels wereempirically determined in the initial experiment byhaving several individuals judge the comparative av-erage quality of the outputs in an actual translationrun.In every case, the base score produced by the scor-ing functions is currently multiplied by the lengthof the candidate in words, on the assumption thatlonger items are better.
We intend to test a varietyof functions in order to find the right contributionof the length factor.2.2 The  char t  walk a lgor i thmFigure 3 presents the chart walk algorithm usedto produce a single, best, non-overlapping, contigu-ous combination (cover) of the available componenttranslations, assuming correct component qualityscores.
The code is organized as a recursive divide-and-conquer procedure: to calculate the cover of aregion of the input, it is repeatedly split into twoparts, at each possible position.
Each time, the bestpossible cover for each part is recursively found, andthe two scores are combined to give a score for thechart walk containing the two best subwalks.
Thesedifferent splits are then compared with each otherand with components from the chart spanning thewhole region (if any), and the overall best result is96Position InputL R (Spanish)0 1 Almomento2 2 de3 3 su4 4 venta5 5 a6 6 Iberia7 78 8 ~rIASA9 10 contabacon11 11 ocho12 12 avionesOutput E Q(Eng sh)"In a minute" G 10"At once""A moment"of from about Dfor byhis her its Gone's your theirinn sale selling Gmarketing"country inn""small shop"stall boothto a of D 2Iberia G 5G 5VIASA D 2"was rely on" G 10"rely on" "wasCount on""count on" "wasdepending on""depended on"haveeight eighth D 2airplane L 2.5aeroplanesplanesaircraftsairplanesmartins13 13 ,14 14 que15 15 ten\[an16 16 en17 17 promedio18 18 1319 21 afios devuelo22 22hopscotchesG 5who that D 2whom which"were have" G 5"have""were hold" hold"were thinking"thought "wereconsidering"considered "weredeeming" deemed"were coming"camein on onto D 2at byaverage mean G 5middle midpointmid-point13 L 15"years of E 8.8experiencewith space flight""flight activities""of years"Figure 2: Chart walk resultsD 2used.
The terminating step of this recursion is thusgetting components from the chart.To find best walk on a region:i f  there is a stored resul t  for th is  regionthen return i te lsebeginget a l l  pr imit ive components for the regionfor each posit ion p within the regionbeginsplit region into two parts at pfind best walk for first partfind best walk for second partcombine into a componentendfind maximum score over all primitiveand combined componentsstore and return itendFigure 3: Chart walk algorithmWithout dynamic programming, this would have acombinatorial time complexity.
Dynamic program-ming utilizes a large array to store partial results, sothat the best cover of any given subsequence is onlycomputed once; the second time that a recursive callwould compute the same result, it is retrieved fromthe array instead.
This reduces the time complexityto O(n3), and in practice it uses an insignificant partof total processing time.All possible combinations of components are com-pared: this is not a heuristic method, but an efficientexhaustive one.
This is what assures that the cho-sen cover is optimal.
This assumes, in addition tothe scores actually being correct, that the scores arecompositional, in the sense that the combined scorefor a set of components really represents their qualityas a group.
This might not be the case, for example,if gaps or overlaps are allowed in some cases (per-haps where they contain the same words in the samepositions).We calculate the combined score for a sequence ofcomponents as the weighted average of their individ-ual scores.
Weighting by length is necessary so thatthe same components, when combined in a differentorder, produce the same combined scores.
Otherwisethe algorithm can produce inconsistent results.The chart walk algorithm can also be thought of asfilling in the two-dimensional dynamic-programmingarray!.
Figure 4 shows an intermediate point in thefilling of the array.
In this figure, each element (i,j)is initially the best score of any single chart compo-nent covering the input region from word i to wordj.
Dashes indicate that no one component covers ex-1 Note that this array is a different data structure fromthe chart.970123456789101112131415161718192021221 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 2210 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.2 .5  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.2 - -  .83  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.2 .25  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.2 3 .5  7 .3  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.5 10 7 .3  6 .1  5 .9  5 .2  5 .2  4 .8  4 .8  5 .8  5 .7  5 .4  6 .5  6 .22.02 .12 .82 .73 .02 .93 .14 .54 .54 .35 .55 .32 2 .23 .12 .83 .33 .03 .34 .84 .84 .55 .95 .52 .53 .73 .13 ,63 .33 .55 .25 .14 .86 ,35 .95 3 .54 .03 .53 .85 .65 .55 .16 .76 .22 3 .53 ,03 .55 .85 .65 .16 .96 .35 3 .54 .06 .76 ,45 .67 .66 .92 3 .57 .36 .75 .88 .07 ,25 10.
8 .36 .79 .38 .015 10 .7 .310 .8 ,75 3 .58 .87 .12 3 .53 .05 3 .52F igure  4: T r iangu lar  ar ray  fi l led in th rough (8,10) by char t  walk0123456789101112131415161718192021221 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22I0 7 .36 .76 .45 .65 .55 .55 .15 .16 .05 .65 .45 .35 .15 .14 .94 .95 .52 .52 ,23 .13 .63 .33 .53 .73 .53 .74 ,84 .54 .44 .44 .24 .34 .14 .24 .82 3 .54 .03 .53 .84 ,03 .73 .85 .14 .84 .54 .64 .44 .44 .34 .34 .95 5 .04 .04 .24 .44 .04 .15 .55 .14 .84 .84 .64 .64 .44 .55 .15 3 .54 .04 .23 .84 .05 .55 .14 .84 .84 .54 .64 .44 .45 .12 3.54 .03 .53 .85 .65 .14 .84 .84 .54 .54 .34 .45 .15 5 .04 .04 .26 .45 .65 .25 .14 .84 .84 .54 .65 .45 3 .54 .06 .75 .85 .25 .24 .84 .84 .54 .55 .42 3 .57 .36 .05 .35 .24 .74 .84 .54 .55 .55 10 7 .36 .15 .95 ,2524.84 .85 ,82 .02 .12 .82 .732 2 .23 .12 .832 .53 .73 .135 3 .542 3502 .93 .14 .533 .03 .34 .863 .33 .55 .203 .53 .85 .653 .03 .55 .83 .54 .06 .72 3 .5  7.35 10.15F igure  5: F ina l  ar ray  produced  by char t  walk5 .45 .35 .95 .74 .84 .75 .45 .24 .94 .85 .55 .35 .14 .95 .75 .55 .14 .95 .75 .55 .14 .95 .85 .65 .35 .16 .05 .85 .45 .16 .15 .85 .45 .16 .25 .95 .75 .46 .56 .24 .54 .35 .55 .34 .84 .55 .95 .55 .14 .86 .35 .95 .55 .16 .76 .25 .65 .16 .96 .36 .45 .67 .66 .96 .75 .88 .07 .28 .36 .79 .38 .010 .7 .310 .8 .75 3 .58 .87 .12 3 .53 .05 3 .5298actly that region.
(In rows 1 through 7, the arrayhas not yet been operated on, so it still shows its ini-tial state.)
After processing (see rows 9 through 22),each element is the score for the best set of compo-nents covering the input from word i to word j (thebest cover for this substring) ~.
(Only a truncatedscore is shown for each element in the figure, forreadability.
There is also a list of best componentsassociated with each element.)
The array is uppertriangular since the starting position of a componenti must be less than or equal to its ending position j.For any position, the score is calculated based on acombination of scores in the row to its left and in thecolumn below it, versus the previous contents of thearray cell for its position.
So the array must be filledfrom the bottom-up, and left to right.
Intuitively,this is because larger regions must be built up fromsmaller regions within them.For example, to calculate lement (8,10), we com-pute the length-weighted averages of the scores ofthe best walks over the pair of elements (8,8) and(9,10) versus the pair (8,9) and (10,10), and comparethem with the scores of any single chart componentsgoing from 8 to 10 (there were none), and take themaximum.
Referring to Figure 2 again, this corre-sponds to a choice between combining the transla-tions of (8,8) VIASA and (9,10) conlaba con versuscombining the (not shown) translations of (8,9) VI-ASA contaba and (10,10) con.
(This (8,9) elementwas itself previously built up from single word com-ponents.)
Thus, we compare (2 .1  + 10,2) /3  - 7.33with (3 .5 .2+2.1) /3  = 3.0 and select the first, 7.33.The first wins because contaba con has a high scoreas an idiom from the glossary.Figure 5 shows the final array.
When the elementin the top-right corner is produced (5.78), the algo-rithm is finished, and the associated set of compo-nents is the final chart walk result shown in Figure 2.It may seem that the scores should increase to-wards the top-right corner.
This has not generallybeen the case.
While the system produces a num-ber of high-scoring short components, many low-scoring components have to be included to span theentire input.
Since the score is a weighted aver-age, these low-scoring components pull the combinedscore down.
A clear example can be seen at position(18,18), which has a score of 15.
The scores aboveand to its right each average this 15 with a 5, fortotal values of 10.0 (all the lengths happen to be1), and the score continues to decrease with distancefrom this point as one moves towards the final score,which does include the component for (18,18) in thecover.2In the actual implementation, the initial componentsare not present yet in the array, since the presence of anelement indicates that the computation has been carriedout for this position.
They are accessed from the chartdata structure as needed, but are shown here as an aidto understanding.2.3 Reorder ing  componentsThe chart-oriented integration of MT engines doesnot easily support deviations from the linear order ofthe source text elements, as when discontinuous con-stituents translate contiguous trings or in the caseof cross-component substring order differences.
Weuse a language pair-dependent set of postprocess-ing rules to alleviate this (for example, by switchingthe order of adjacent single-word adjective and nouncomponents).3 TRANSLAT ION DEL IVERYSYSTEMResults of multi-engine MT were fed in our exper-iment into a translator's workstation (TWS) (Co-hen et al, 1993), through which a translator ei-ther approved the system's output or modified it.The main option for human interaction in TWS cur-rently is the Component Machine-Aided Translation(CMAT) editor (Frederking et hi., 1993a).
The usersees the original source language text in one editorwindow, and phrases marked by double angle brack-ets in another, each of which is the first translationfrom a candidate chosen by the chart walk.
Menus,function keys and mouse clicks are used to performboth regular and enhanced editing actions.The most important enhancement provided isthe ability to select an alternate translation witha popup menu, and instantly replace the system'sinitially chosen candidate translation string, whichbecomes the first alternative in this menu if it isused again.
The alternate translations are the othertranslations from the chosen component 3.As mentioned above, Figure 2 shows the sets ofcandidates in the best chart walk that are presentedas choices to the human user through the CMATeditor in our example.TEST ING AND EVALUATINGMULT I -ENGINEPERFORMANCEAutomatically assessing the utility of the multi-engine system relative to the engines taken sepa-rately would be a useful development tool.
The bestmethod we could find was counting the number ofkeystrokes in the TWS to convert he outputs of in-dividual engines and the multi-engine configurationto a "canonical" human translation.
A sample teston a passage of 2060 characters from the June 1993evaluation of Pangloss is shown in figure 6.The difference in keystrokes was calculated as fol-lows: one keystroke for deleting a character; two3The CMAT editor may also include translationsfrom other candidates, lower in the menu, if they havethe same boundaries as the chosen candidate and themenu is not too long.99Type of translationht~rnan tester (US GovernmentLevel 2 translator)Word-for-word lookup in MRDslookup in phrasal glossariesKBMTExample-Based MTMulti-engine configurationI Keystrokedifference154218291973188318761716Figure 6: Results of keystroke testand target languages.
It is a weaker approach, butshould go some distance in selecting between other-wise indistinguishable outputs.Another possible direction for future developmentwould be to employ ideas from the area of heuristicsearch, and only run the highest-quality-score en-gine on each unit of source text.
This assumes thatwe can reliably estimate scores in advance (not cur-rently true for the expensive ngines), and that theengines can be run on fragments.
A less ambitiousversion of this idea would be to run the low-scoringengines only where there are gaps in the normallyhigh-scoring engines.keystrokes for inserting a character; three keystrokesfor deleting a word (in an editor with mouse action);three keystrokes plus the number of characters in theword being inserted for inserting a word.
It is clearfrom the above table that the multi-engine config-uration works better than any of our available in-dividual engines, though it still does not reach thequality of a Level 2 translator.It is also clear that using keystrokes as a measureis not very satisfactory.
It would be much better tomake the comparison against he closest member ofa set of equivalent paraphrastic translations, incethere are many "correct" ways of translating a giveninput.
However, this is predicated on the availabilityof a "paraphraser" system, developing which is nota trivial task.5 CURRENT AND FUTUREWORKUltimately, a multi-engine system depends on thequality of each particular engine.
We expect theperformance ofKBMT and EBMT to grow.
We planto use a standard regression mechanism to modifythe scoring system based on feedback from havinghumans elect he best covers for test texts.The current system is human-aided.
We have be-gun an experiment with a fully-automated mode,with the understanding that the quality will drop.The most important effect of this change is thataccurate quality scores become much more impor-tant, since the first choice becomes the only choice.Besides improving the KBMT and EBMT scoringmechanisms, we need to provide finer distinctionsfor the lexical transfer engine's output.
As thedatabases for this are quite large (all together, over400,000 entries), adding scores to individual entriesis, in the short run, prohibitive.
We have not as yetdiscovered any feasible automatic technique for gen-erating such scores.
Instead, we are planning to usean English language model on the output, in a man-ner similar to that done by speech and statisticaltranslation systems (Brown et al, 1990).
Statisti-cally generating such a model is feasible, since it doesnot rely on knowing correspondences between sourceReferencesBrown, P., K. Cocke, S. Della Pietra, V.J.
DellaPietra, F. Jelinek, J.D.
Lafferty, R.L.
Mercerand P.S.
Roossin.
"A statistical approach to Ma-chine Translation", Computational Linguistics 16,pp.79-85, 1990.Cohen, A., Cousseau, P., Frederking, R., Grannes,D., Khanna, S., McNeilly, C., Nirenburg, S., Shell,P., Waeltermann, D. Translator's WorkStationUser Document, Center for Machine Translation,Carnegie Mellon University, 1993.Frederking, R., Grannes, D., Cousseau, P., andNirenburg, S. "An MAT Tool and Its Effective-ness."
In Proceedings of the DARPA Human Lan-guage Technology Workshop, Princeton, N J, 1993.Frederking, R., A. Cohen, P. Cousseau, D. Grannesand S. Nirenburg.
"The Pangloss Mark I MATSystem."
Proceedings of EACL-93, Utrecht, TheNetherlands, 1993.Nagao, M. "A framework of a mechanical translationbetween Japanese and English by analogy princi-ple."
In: A. Elithorn and R. Banerji (eds.)
Artifi-cial and Human Intelligence.
NATO Publications,1984.Nirenburg, S., C. Domashnev and D.J.
Grannes.
"Two Approaches to Matching in Example-BasedMachine Translation."
Proceedings of TMI-93,Kyoto, 1993.Nirenburg, S., S. Beale and C. Domashnev.
"AFull-Text Experiment in Example-Based MachineTranslation."
Submitted to the International Con-ference on New Methods in Language Processing,Manchester, September 1994.Nirenburg, S., S. Beale, C. Domashnev and P.Sheridan.
"Example-Based Machine Translationof Running Text."
In preparation.i00
