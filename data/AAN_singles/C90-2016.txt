Integrating Stress and Intonation into a Concept-to-Speech SystemGeorg DORFFNERErnst BUCHBERGERAustrian Research Institutefor Artificial IntelligenceSchottengasse 3A-1010 Vienna, Austriaand University of Viennaemaih georg%ai-vie.uucp@relay.eu, n t,ernst%ai-vie.uucp@relay.eu.netAbstract: The paper deals with the integration ofintonation algorithms into a concept-to-speechsystem for German 1).
The algorithm forcomputing the stress hierarchy of a sentenceintroduced by Kiparski (1973) and the theory ofsyntactic grouping for intonation patternsdeveloped by Bierwisch (1973) have been studiedextensively, but they have never been implementedin a concept-to-speech system like the onepresented here.
We describe the back end of thisconcept-to-speech system: The surface generatortransfers a hierarchical dependency structure of asentence into a phoneme string by traversing it in arecurs~ve-descent manner.
Surface structuresunfold while generation proceeds,  which meansthat at no point of the process does the fullsyntactic tree structure xist.
As they depend onsyntactic features, both the indices introduced bythe Kiparski (degrees of stress) and the Bierwisch(indexed border markers) formalism have to beinserted by the generator.
This  implies somechanges to the original algorithms, which aredemonstrated in this paper.
The generator hasbeen tested in the domain of an expert system thathelps to debug electronic ircuits.
The synthesizedutterances of the test domain show significantimprovements over monotonous forms of speechproduced by systems not making use of intonationinformation.1.
IntroductionThe goal of the system, a part of which is describedin this paper, was to synthesize speech utterancesstarting from a conceptual representation of theknowledge to be uttered (concept-to-speechsystem).
Compared to speech reproduction, ourapproach is far more flexible.
In contrast totext-to-speech synthesis (Frenkenberger et.al.1988) on the other hand, our approach allows foran easier integration of prosodic elements, assyntactic data such as phrases and treedependencies are directly available.Markus KOMMENDAInstitut f. Nachrichtentechnikund HochfrequenztechnikTechnical University of ViennaGusshausstr.
25/389A-1040 Vienna, Austriaemaih E38901 I@AWITUW01.BITNETAppropriate formalisms for obtaining a basis forstress and pitch information were introduced byKiparski (1973), who proposed an algorithm forcomputing a stress hierarchy for a whole sentence,and Bierwisch (1973), who showed how todetermine pitch variation patterns depending onthe phrasal structure of a sentence.
Like Kiparsky'sstress markers, the boundary indices introduced byBierwisch can be computed from the syntacticstructure of the sentence.In this respect, concept-to-speech ontrasts withtext-to-speech systems: In text-to-speechsynthesis - at least for the German language - it isvirtually impossible to carry out a completesyntactic analysis because of the large number ofambiguities which can only be resolved at thesemantic level.
Thus, the derivation of prosodicinformation in existing text-to-speech systems isbased on a very rudimentary syntactic analysiswhich consists in a purely linear segmentation ofthe input sentences (e.g.
Kulas & Riihl 1982,Zingle 1982, Schnabel 1988, Frenkenberger t al.1988).In concept-to-speech synthesis, on the otherhand, we are in a position to exploit the inherentlyavailable syntactic structure of the given text, sothat we can apply the formalisms described byBierwisch and Kiparsky.Both processes are only theoretically developedand have not been fully implemented in a workingsystem before.
We have integrated these processesinto the surface generator of ourconcept-to-speech system and applied somenecessary changes and adaptations to them.In this paper we concentrate on the computation ofstress and intonation markers, integrated into thesurface generation component.
The readerinterested in the overall structure of the system, anapplication domain and the first phase ofgeneration which starts with concepts and produces1) This work was supported by the Jubiliiumsfonds der Oesterreichischen Nationalbank, as part ofproject no.
2901.1 89the input structure to the surface generator(henceforth 'deep structure') is referred toDorffner, Trost & Buchberger (1988).2.
The Surface GeneratorThe deep structure which forms the input to thesurface generator consists of a hierarchicalstructure of essentially two building blocks:CLAUSEs, which roughly correspond to entiresentences and PHRASEs like NPs, PPs or APs (fig.1).
A PHRASE can be modified by otherPHRASEtype noun, adjhead *lxm*wid .
.
.
.mods <phrase>, <clause> Ifeats .
.
.
.
.
IPHRASE-FEATURESdet def, indef .
.
.
.betont t, nilvorfeld t, nilp ron .
,  t, nil ........case e-zero, ,.,num si,ng, plur, 10 .
.
.
.Fig.
1I_lPHRASEs or CLAUSEs, thus forming ahierarchical structure for complex utterances(Dorffner, Kommenda & Frenkenberger 1988).Surface generation ow works on this hierarchicalstructure of building blocks and transfers it into asurface structure consisting of phonemic stringswhich are subsequently synthesized.
Our generatordiffers from the often encountered two-stepapproach - generate the syntactic tree with lexicalitems as its leaves and morphological and otherfeatures attached to them, then scan all its leavesand synthesize the lexical elements (see e.g.McDonald 1983) - in an important way, forreasons of efficiency and plausibility.
The deepstructure, as introduced above, was designed so asto already correspond to the surface structure ofthe sentence 1), except for aspects of order andfunction words.
In other words, the (unordered)hierarchy of deep structure building blocks isisomorphic (after order has been imposed) to thesyntactic tree structure of the surface sentence.This can be easily achieved in German, whereconstituent order is much less strict than in otherlanguages, such as English.
As a result of thisproperty of German, the position of phrases withina sentence is not tied to their functional role andthus does not have to be reflected in the deepsyntactic structure.
This design of a deep structureas being isomorphic to surface structure implies asimplification in the surface generator, comparedto the two-step approach mentioned above: Thesurface tree does not have to be produced entirelybefore lexical items can be synthesized, but canunfold while the hierarchy of building blocks isscanned recursively.The process of surface generation is as follows: Foreach CLAUSE or PHRASE, a correspondingsurface building block (e.g.
an NP) is generated,depending on their features and lexical heads (fig.2).
Such a building block contains slots for eitherNP-PPprep detLexem <detp>modl head modi<phrase> Lexem <phra/<clause> <clau~Fig.
2pointers to other building blocks or lexical items intheir correct order.
Now each slot can be scannedand synthesized (if it contains a lexical item) orrecursively treated like the other building blocks(Fig.3, Dorffner Kommenda & Frenkenberger1988).DS (CLAUSE)r~ces~- - - -  \] SS .. surface structure/ building block eep structure \[DS .. deep structure/building blockS ~ pass elementP A SS.
S to next componentsurface structure ~ --~n'tlaesisbuilding block \] z, ir~n.me 7e.g.
DETPFig.
3This form of generation process has seriousconsequences on the intended integration ofintonational information: All syntactic information1) Strictly speaking, this differs from a deep structure as defined in Chomsky (1975)90 2is available during the process, but the syntactictree never exists in its entirety.
Furthermore,indices have to be produced (during synthesis oflexical items) before the remainder of the syntacticstructure has unfolded.
At first sight this looks likea major restriction and reduction of availableinformation.
As it turns out, however, theapproaches of Kiparski and Bierwisch can both bemodified so as to fit into this scheme.
Aninteresting side-effect is that synthesis of speech,starting :from deep structures, works in a strictleft-to-right manner, which seems psychologicallyvery plausible.3.
Insertion of Kiparski Stress MarkersKiparski (1973) introduced two rules forcomputing stress markers based on a syntactic tree:(1) (a) ttead stress rule:the first (left-most) node keeps its index,all others are incremented by 1(b) Tail stress rule:the last (right-most) node keeps its index,all others are incremented by 1The algorithm works as follows:(2) -assign the index 1 to each stressable lexicalitem-scan  the tree bottom-up and apply rule (\]a)or ( lb)  to each significant nodeThis algorithm works strictly bottom-up and thusrequires the entire syntactic tree.
As a result, itcannot be integrated into our generator in thisform.
It is, however, possible to rewrite thealgorithm so that it works top-down anddepth-first so as to fit into the generation schemedescribed above.
The new algorithm is thefollowing:O) Introduce a pair of indices and maintain it asfollows while scanning the tree top down.
At theroot, start with the pair (1 1).- at each significant node that has at least twosignificant successor nodes, do the following,given the index pair (n m):- with head stress rule:assign the pair (n m+l) to the first successorassign the pair (n+m 1) to all the others- with tail stress rule:assign the pair (n re+l) to the last successorassign the pair (n+m 1) to all the others- at the leaves of the tree (= lexical entry), withassigned pair (n m):- n is the Kiparski marker for the lexical itemIf one considers the preferred successor (head ortail, depending on the rule) as the winner of therule and all others as losers, algorithm (3) can beinterpreted as follows: The second index of a pair(m) counts how often a node is on the winningside.
All losers have to increment heir marker bythat amount.
Thus, at each decision, the winnerkeeps its marker (n), while the markers of all theothers have to be increased by m (n+m).
As therecan be only one leaf that is on the winning sideeach time, it is ensured that only one lexical itemreceives marker 1.A similar algorithm could be applied to yield thestress pattern within complex words (which arequite numerous in German).
However, as thelexicon of the generator contains morphemes andcomplex lexernes with pointers to each morpheme,a decision about stress within a word can be storedlexically and no algorithmic treatment is necessary.A syllable now receives a Kiparski marker if- it is in a stressable morpheme (lexical feature)- it is marked by the lexical entry of the (possiblycomplex) word AND- algorithm (3) has assigned an index pair to thelexical entryThe so computed marker is inserted into thephonemic string during the morphologic synthesisof the word.4.
Insertion of Bierwisch Boundary IndicesBierwisch (1973) suggests inserting a marker ateach word boundary to express how manysignificant nodes dominate both words involved.His algorithm was designed in a bottom-upfashion.
We show again that it can be formulatedtop-down (as required in our system):(4) Assign an index to each node.
At the root,start with 1.
For each node with index ifor each successor do, left to right:- i f  the successor is a lexical item, synthesize itand append i as boundary marker- if the successor is a significant node, assignindex i+ l- otherwise assign index iwhen all nodes on that level have been processed,- overwrite the index that was written last with iThe problem that a left-to-right process cannotknow whether the following word is on the samelevel in the tree is solved by permitting to overwritea marker already written.5.
Acoustic Realization of Prosodic PatternsStarting from the above stress and boundarymarkers, the prosodic structure of a sentence isderived by applying a phonological rule set.
In3 91particular, some of the previously computedboundaries are deleted, others receive a pausemarker.
Furthermore, the resulting phrases areprovided with an intonation contour, which,according to Bierwisch (1973), is specified in termsof so-called SON values.
In a subsequent phoneticcomponent he phrasal structure and the SONvalues are exploited to generate the acousticcorrelates of the prosodic information, inparticular, the duration of phonetic segments andpauses and the pitch values for all voiced phones.6.
An ExampleAn annotated example shall illustrate the processof generation.
Take the following sentence:Betrgt die Spannung am Kondensator 10 Volt?
(Is the voltage at the capacitor equal to 10 Volts?
)The deep structure of the sentence, which is theinput to the surface generator is depicted in fig.4,CLAUSE \]lxm: betraglcX~e: SePaznenrUo ng evenPHRASE 3lxm: Kondensatorcase: locationFig.4the corresponding syntactic tree, which is unfoldedduring generation, in fig.4a.
Both structures havebeen simplified.sl I, 1,1--.4._1 IBetr~igt NP-PP P-PPI , ;  , i , i l  i , i , i/ I I I Idie Spannung \] 10 VoltNP-PPI / I  I\l\ Iam KondensatorFig.4aEach building block in the dependency structure(to the left) has a feature case which indicates theconceptual role of the element (adapted fromEngel 1982).
e-zero, for example, refers to thenominative phrase or subject of a sentence.
Thestructure to the right consists of the surfacebuilding blocks.
Each slot (drawn as a box)corresponds to a possible position which can befilled with a lexical item or another building block,depending on the features of CLAUSE andPHRASE.
Slots that remain empty are ignoredduring synthesis.
One can see in this example thatthe tree of CLAUSEs and PHRASEs has acorresponding isomorphic tree of S and NP-PPs(there are other surface elements like AP, as well),with the exception that in the former ca,;e there isno order information yet.
This illustrates the abovementioned isomorphism between deep and surfacestructure.Generation starts at the root of the deep structure,the CLAUSE.
A Kiparski pair (1 1) and aBierwisch index 1 are assigned, The correspondingsurface building block, S, is generated, filled withthe lexical item betrdgt (verb) and with the twoPHRASES in their correct position (which can bedetermined by looking at the features and usingsome default heuristics as in Engel 1982).
Thestructure at this point looks like the one in fig.5:S kip: (1 1) bier: 1._-1---4---1/1-?-i 3Betrgigt / ~._ .
.?
.J PHRASE 1 \[PHRASEI lxm: Spannung lxm: Volt |I case: e-zero ca.se: e -se~IPHRASE 2Ixrn: Kondensatorcase: locationFig.
5Note that betrgigt can already be synthesized, eventhough the rest of the syntactic structure has notunfolded yet.
For algorithm (3), actually threenodes in Kiparski's notation are comprised in S:Satz, S and D. Therefore, for (3) the structure hasto be viewed as if it looked like the one in fig.
6.
(3) applied to Satz yields the pair (1+1 1) forbetrgigt and (1 1+1) for S (tail stress).
S has onlyone successor, therefore (3) does not apply.
Itdoes, however, apply to D, where the pairs (1+2 1)and (1 2+1) are computed for the two PHRASEs(tail stress).
The Bierwisch index is simplyincremented by 1 for both PHRASEs.
Thus thestring in the lower left of fig.6 can already bewritten (phonemes are given in an ASCIIrepresentation of IPA notation, stress markers arepreceded by ", boundary indices by #).92 4Satz kip: (1 1) bier: 1S Betr~gtkip: (2 1) Ikip: (3 1) r, ~ .~~4.~:  (1 2) kip: (1 3)bier: 2 / "~ bier: 2 -VH .
.
.
.RASE 1 PHRASE 3\[ lxm: Spannung lxm: Volt\[case: e-zero case: e-seven#0 b$tr '2Egt #1Fig.
6The process now recursively continues bygenerating the left PHRASE (Kiparski pair (3 1),Bierwisch index 2).
As above, a correspondingsurface building block (NP-PP) is generated andfilled with lexical items and the modifyingPHRASE ("am Kondensator").
The structure soproduced is shown in fig.7.NP-PP 1die Spannungkip: (3 2)PHRASE 2lxm: Kondensatorcase: location _ _"-I kip: (3 1)_3 bier: 2kip: (4 1)bier: 3#0 b$tr"2Egt #1 dI #2 Sp"3an=N #2Fig.
7Algorithm (3) is applied once (tail stress) andyields a stress marker 3 for Spannung.
TheBierwisch index is incremented once again for thenested PHRASE 2 (note that the Kiparski pair forthat PHRASE is the same as for a loser although itis behind the 'tail'.
Kiparski, in his original article,did not mention post-head modifiers).
ThisPHRASE will subsequently be generatedaccordingly.
The lower right of fig.
7 shows theresult at this stage.
The determiner die is not astressable item and therefore does not receive astress :marker.
The noun, on the other hand, isprovided with the marker 3.After the final lexical item of PHRASE 2,Kondensator, a boundary marker 3 will be written.Now the last part of (4) comes to bear.
As it is theend of the phrase, it is overwritten by the markerof the dominating phrase (NP-PP 1), 2.
It is alsothe end of NP-.PP 1, so it is finally overwritten bythe marker assigned to S, which is 1.
The output atthis stage is the following:#0 b$tr"2Egt #1 dl #2 Sp"3an=N #2Ham #3 k0nd$ns"4Ator #1After that, PHRASE 3 - the next one attached to S- is generated, in an analogous fashion.7.
Discussion and ConclusionThe experiences with the described generator haveshown thai: synthesis of German utterances in aconcept-to-speech system is possible while bothsynthesizing intonation patterns using syntacticinformation and maintaining the efficient processstructure of the generator designed for the specificsof the German language.
The assumptions underwhich it was applied are a single-sentence systemwithout contextual or pragmatic information.Problems rooted in the lack of such informationhave therefore not been solved.
The speechproduced this way shows considerableimprovement over monotonous versions or versionswhich cannot make full use of syntacticinformation.
Furthermore, the approach can easilybe extended to include additional aspects ofintonation such as emphasis of elements overothers.Despite the success of the system described in thispaper, some limitations have been discovered.
Inthe test domain long sentences with complex andmultiply nested phrases were quite frequent.
Someof them included post-head modifiers such as"rechts unten" (= "to the lower right"), in additonto other modifiers like several adjectives.
Thealgorithm by Bierwisch produced boundarymarkers between the beginning and the end of"rechts unten" that were only slightly greater thanthe surrounding ones.
Synthesis of the utterance,however, revealed that the modifier was spokenwith an unnaturally high pitch and a pause that wastoo short.
\]Manually altering the indices to lowervalues, which would mean that "rechts unten" is aconstituent on sentence level rather than a nounmodifier, lead to better results.
Thus, thetop-down scheme of the algorithm would have tobe broken in this case.Future work will be required to discover otherlimitations and to adapt the process to overcomethem.5 93ReferencesBierwisch M.: Regeln f/)r die Intonationdeutscher S~itze.
In: Studia Grammatica VII,Berlin, 3rd ed., 1973.Chomsky N,: Reflections on language, MIT Press,Cambridge, MA, 1975.Dorffner G., Kommenda M., Frenkenberger S.:Ein OberflSchengenerator zur Erzeugunggeschriebener und gesprochener Sgtze;Austrian Research Institute for ArtificialIntelligence, Vienna, TR 88-10, 1988.Dorffner G., Trost H., Buchberger E.:Generating spoken output for an expert systeminterface, OGAI-Journal 3-4, 36-41, 1988.Engel U.: Syntax der deutschenGegenwartssprache, 2nd ed., Erich Schmidt,Berlin, 1982.Frenkenberger S., Kommenda M., Pounder A.:Automatische Wortklassifizierung undProsodiebestimmung im SprachausgabesystemGRAPHON; ITG-Fachbericht 105, DigitaleSprachverarbeitung, 19 8 8.Kiparsky P.: l)ber den deutschen Akzent.
In:Studia Grammatica VII, Berlin, 3rd ed., 1973.Kulas W., Rfihl H.-W.: Satzzerlegung ffir einSprachausgabesystem mit unbegrenztemWortschatz, Fortschritte der Akustik -FASE/DAGA'82, pp.1017-1019, 1982.McDonald D.: Natural language generation as acomputational problem; in: Brady & Berwick(eds.
): Computational models of discourse,MIT Press, 1983.Schnabel B.: Developpement d'un syst~me desynth~se de l'Allemand a partir du texte, Th~sede doctorat, Universit~ Stendhal, Grenoble,1988.Zingl~ H.: Traitement de la prosodie n Allemanddans un syst~me de synth~se de la parole,Thi~.se d'Etat, Universitd de Strasbourg II,1982.94 6
