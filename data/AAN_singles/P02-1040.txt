BLEU: a Method for Automatic Evaluation of Machine TranslationKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing ZhuIBM T. J. Watson Research CenterYorktown Heights, NY 10598, USA{papineni,roukos,toddward,weijing}@us.ibm.comAbstractHuman evaluations of machine translationare extensive but expensive.
Human eval-uations can take months to finish and in-volve human labor that can not be reused.We propose a method of automatic ma-chine translation evaluation that is quick,inexpensive, and language-independent,that correlates highly with human evalu-ation, and that has little marginal cost perrun.
We present this method as an auto-mated understudy to skilled human judgeswhich substitutes for them when there isneed for quick or frequent evaluations.11 Introduction1.1 RationaleHuman evaluations of machine translation (MT)weigh many aspects of translation, including ade-quacy, fidelity , and fluency of the translation (Hovy,1999; White and O?Connell, 1994).
A compre-hensive catalog of MT evaluation techniques andtheir rich literature is given by Reeder (2001).
Forthe most part, these various human evaluation ap-proaches are quite expensive (Hovy, 1999).
More-over, they can take weeks or months to finish.
This isa big problem because developers of machine trans-lation systems need to monitor the effect of dailychanges to their systems in order to weed out badideas from good ideas.
We believe that MT progressstems from evaluation and that there is a logjam offruitful research ideas waiting to be released from1So we call our method the bilingual evaluation understudy,BLEU.the evaluation bottleneck.
Developers would bene-fit from an inexpensive automatic evaluation that isquick, language-independent, and correlates highlywith human evaluation.
We propose such an evalua-tion method in this paper.1.2 ViewpointHow does one measure translation performance?The closer a machine translation is to a professionalhuman translation, the better it is.
This is the cen-tral idea behind our proposal.
To judge the qualityof a machine translation, one measures its closenessto one or more reference human translations accord-ing to a numerical metric.
Thus, our MT evaluationsystem requires two ingredients:1. a numerical ?translation closeness?
metric2.
a corpus of good quality human reference trans-lationsWe fashion our closeness metric after the highly suc-cessful word error rate metric used by the speechrecognition community, appropriately modified formultiple reference translations and allowing for le-gitimate differences in word choice and word or-der.
The main idea is to use a weighted average ofvariable length phrase matches against the referencetranslations.
This view gives rise to a family of met-rics using various weighting schemes.
We have se-lected a promising baseline metric from this family.In Section 2, we describe the baseline metric indetail.
In Section 3, we evaluate the performance ofBLEU.
In Section 4, we describe a human evaluationexperiment.
In Section 5, we compare our baselinemetric performance with human evaluations.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
311-318.Proceedings of the 40th Annual Meeting of the Association for2 The Baseline BLEU MetricTypically, there are many ?perfect?
translations of agiven source sentence.
These translations may varyin word choice or in word order even when they usethe same words.
And yet humans can clearly dis-tinguish a good translation from a bad one.
For ex-ample, consider these two candidate translations ofa Chinese source sentence:Example 1.Candidate 1: It is a guide to action whichensures that the military always obeysthe commands of the party.Candidate 2: It is to insure the troopsforever hearing the activity guidebookthat party direct.Although they appear to be on the same subject, theydiffer markedly in quality.
For comparison, we pro-vide three reference human translations of the samesentence below.Reference 1: It is a guide to action thatensures that the military will foreverheed Party commands.Reference 2: It is the guiding principlewhich guarantees the military forcesalways being under the command of theParty.Reference 3: It is the practical guide forthe army always to heed the directionsof the party.It is clear that the good translation, Candidate 1,shares many words and phrases with these three ref-erence translations, while Candidate 2 does not.
Wewill shortly quantify this notion of sharing in Sec-tion 2.1.
But first observe that Candidate 1 shares"It is a guide to action" with Reference 1,"which" with Reference 2, "ensures that themilitary" with Reference 1, "always" with Ref-erences 2 and 3, "commands" with Reference 1, andfinally "of the party" with Reference 2 (all ig-noring capitalization).
In contrast, Candidate 2 ex-hibits far fewer matches, and their extent is less.It is clear that a program can rank Candidate 1higher than Candidate 2 simply by comparing n-gram matches between each candidate translationand the reference translations.
Experiments overlarge collections of translations presented in Section5 show that this ranking ability is a general phe-nomenon, and not an artifact of a few toy examples.The primary programming task for a BLEU imple-mentor is to compare n-grams of the candidate withthe n-grams of the reference translation and countthe number of matches.
These matches are position-independent.
The more the matches, the better thecandidate translation is.
For simplicity, we first fo-cus on computing unigram matches.2.1 Modified n-gram precisionThe cornerstone of our metric is the familiar pre-cision measure.
To compute precision, one simplycounts up the number of candidate translation words(unigrams) which occur in any reference translationand then divides by the total number of words inthe candidate translation.
Unfortunately, MT sys-tems can overgenerate ?reasonable?
words, result-ing in improbable, but high-precision, translationslike that of example 2 below.
Intuitively the prob-lem is clear: a reference word should be consideredexhausted after a matching candidate word is iden-tified.
We formalize this intuition as the modifiedunigram precision.
To compute this, one first countsthe maximum number of times a word occurs in anysingle reference translation.
Next, one clips the to-tal count of each candidate word by its maximumreference count,2adds these clipped counts up, anddivides by the total (unclipped) number of candidatewords.Example 2.Candidate: the the the the the the the.Reference 1: The cat is on the mat.Reference 2: There is a cat on the mat.Modified Unigram Precision = 2/7.3In Example 1, Candidate 1 achieves a modifiedunigram precision of 17/18; whereas Candidate2 achieves a modified unigram precision of 8/14.Similarly, the modified unigram precision in Exam-ple 2 is 2/7, even though its standard unigram pre-cision is 7/7.2Countclip = min(Count,Max Re f Count).
In other words,one truncates each word?s count, if necessary, to not exceed thelargest count observed in any single reference for that word.3As a guide to the eye, we have underlined the importantwords for computing modified precision.Modified n-gram precision is computed similarlyfor any n: all candidate n-gram counts and theircorresponding maximum reference counts are col-lected.
The candidate counts are clipped by theircorresponding reference maximum value, summed,and divided by the total number of candidate n-grams.
In Example 1, Candidate 1 achieves a mod-ified bigram precision of 10/17, whereas the lowerquality Candidate 2 achieves a modified bigram pre-cision of 1/13.
In Example 2, the (implausible) can-didate achieves a modified bigram precision of 0.This sort of modified n-gram precision scoring cap-tures two aspects of translation: adequacy and flu-ency.
A translation using the same words (1-grams)as in the references tends to satisfy adequacy.
Thelonger n-gram matches account for fluency.
42.1.1 Modified n-gram precision on blocks oftextHow do we compute modified n-gram precisionon a multi-sentence test set?
Although one typicallyevaluates MT systems on a corpus of entire docu-ments, our basic unit of evaluation is the sentence.A source sentence may translate to many target sen-tences, in which case we abuse terminology and re-fer to the corresponding target sentences as a ?sen-tence.?
We first compute the n-gram matches sen-tence by sentence.
Next, we add the clipped n-gramcounts for all the candidate sentences and divide bythe number of candidate n-grams in the test corpusto compute a modified precision score, pn, for theentire test corpus.pn =?C?
{Candidates}?n-gram?CCountclip(n-gram)?C ??{Candidates}?n-gram?
?C ?Count(n-gram?)
.4BLEU only needs to match human judgment when averagedover a test corpus; scores on individual sentences will often varyfrom human judgments.
For example, a system which producesthe fluent phrase ?East Asian economy?
is penalized heavily onthe longer n-gram precisions if all the references happen to read?economy of East Asia.?
The key to BLEU?s success is thatall systems are treated similarly and multiple human translatorswith different styles are used, so this effect cancels out in com-parisons between systems.2.1.2 Ranking systems using only modifiedn-gram precisionTo verify that modified n-gram precision distin-guishes between very good translations and badtranslations, we computed the modified precisionnumbers on the output of a (good) human transla-tor and a standard (poor) machine translation systemusing 4 reference translations for each of 127 sourcesentences.
The average precision results are shownin Figure 1.Figure 1: Distinguishing Human from Machine   The strong signal differentiating human (high pre-cision) from machine (low precision) is striking.The difference becomes stronger as we go from un-igram precision to 4-gram precision.
It appears thatany single n-gram precision score can distinguishbetween a good translation and a bad translation.To be useful, however, the metric must also reliablydistinguish between translations that do not differ sogreatly in quality.
Furthermore, it must distinguishbetween two human translations of differing quality.This latter requirement ensures the continued valid-ity of the metric as MT approaches human transla-tion quality.To this end, we obtained a human translationby someone lacking native proficiency in both thesource (Chinese) and the target language (English).For comparison, we acquired human translations ofthe same documents by a native English speaker.
Wealso obtained machine translations by three commer-cial systems.
These five ?systems?
?
two humansand three machines ?
are scored against two refer-ence professional human translations.
The averagemodified n-gram precision results are shown in Fig-ure 2.Each of these n-gram statistics implies the sameFigure 2: Machine and Human Translations       ranking: H2 (Human-2) is better than H1 (Human-1), and there is a big drop in quality between H1 andS3 (Machine/System-3).
S3 appears better than S2which in turn appears better than S1.
Remarkably,this is the same rank order assigned to these ?sys-tems?
by human judges, as we discuss later.
Whilethere seems to be ample signal in any single n-gramprecision, it is more robust to combine all these sig-nals into a single number metric.2.1.3 Combining the modified n-gramprecisionsHow should we combine the modified precisionsfor the various n-gram sizes?
A weighted linear av-erage of the modified precisions resulted in encour-aging results for the 5 systems.
However, as can beseen in Figure 2, the modified n-gram precision de-cays roughly exponentially with n: the modified un-igram precision is much larger than the modified bi-gram precision which in turn is much bigger than themodified trigram precision.
A reasonable averag-ing scheme must take this exponential decay into ac-count; a weighted average of the logarithm of modi-fied precisions satisifies this requirement.BLEU uses the average logarithm with uniformweights, which is equivalent to using the geometricmean of the modified n-gram precisions.5 ,6 Experi-mentally, we obtain the best correlation with mono-5The geometric average is harsh if any of the modified pre-cisions vanish, but this should be an extremely rare event in testcorpora of reasonable size (for Nmax ?
4).6Using the geometric average also yields slightly strongercorrelation with human judgments than our best results usingan arithmetic average.lingual human judgments using a maximum n-gramorder of 4, although 3-grams and 5-grams give com-parable results.2.2 Sentence lengthA candidate translation should be neither too longnor too short, and an evaluation metric should en-force this.
To some extent, the n-gram precision al-ready accomplishes this.
N-gram precision penal-izes spurious words in the candidate that do not ap-pear in any of the reference translations.
Addition-ally, modified precision is penalized if a word oc-curs more frequently in a candidate translation thanits maximum reference count.
This rewards usinga word as many times as warranted and penalizesusing a word more times than it occurs in any ofthe references.
However, modified n-gram precisionalone fails to enforce the proper translation length,as is illustrated in the short, absurd example below.Example 3:Candidate: of theReference 1: It is a guide to action thatensures that the military will foreverheed Party commands.Reference 2: It is the guiding principlewhich guarantees the military forcesalways being under the command of theParty.Reference 3: It is the practical guide forthe army always to heed the directionsof the party.Because this candidate is so short compared tothe proper length, one expects to find inflated pre-cisions: the modified unigram precision is 2/2, andthe modified bigram precision is 1/1.2.2.1 The trouble with recallTraditionally, precision has been paired withrecall to overcome such length-related problems.However, BLEU considers multiple reference trans-lations, each of which may use a different wordchoice to translate the same source word.
Further-more, a good candidate translation will only use (re-call) one of these possible choices, but not all.
In-deed, recalling all choices leads to a bad translation.Here is an example.Example 4:Candidate 1: I always invariably perpetu-ally do.Candidate 2: I always do.Reference 1: I always do.Reference 2: I invariably do.Reference 3: I perpetually do.The first candidate recalls more words from thereferences, but is obviously a poorer translation thanthe second candidate.
Thus, na?
?ve recall computedover the set of all reference words is not a goodmeasure.
Admittedly, one could align the refer-ence translations to discover synonymous words andcompute recall on concepts rather than words.
But,given that reference translations vary in length anddiffer in word order and syntax, such a computationis complicated.2.2.2 Sentence brevity penaltyCandidate translations longer than their refer-ences are already penalized by the modified n-gramprecision measure: there is no need to penalize themagain.
Consequently, we introduce a multiplicativebrevity penalty factor.
With this brevity penalty inplace, a high-scoring candidate translation must nowmatch the reference translations in length, in wordchoice, and in word order.
Note that neither thisbrevity penalty nor the modified n-gram precisionlength effect directly considers the source length; in-stead, they consider the range of reference transla-tion lengths in the target language.We wish to make the brevity penalty 1.0 when thecandidate?s length is the same as any reference trans-lation?s length.
For example, if there are three ref-erences with lengths 12, 15, and 17 words and thecandidate translation is a terse 12 words, we wantthe brevity penalty to be 1.
We call the closest refer-ence sentence length the ?best match length.
?One consideration remains: if we computed thebrevity penalty sentence by sentence and averagedthe penalties, then length deviations on short sen-tences would be punished harshly.
Instead, we com-pute the brevity penalty over the entire corpus to al-low some freedom at the sentence level.
We firstcompute the test corpus?
effective reference length,r, by summing the best match lengths for each can-didate sentence in the corpus.
We choose the brevitypenalty to be a decaying exponential in r/c, where cis the total length of the candidate translation corpus.2.3 BLEU detailsWe take the geometric mean of the test corpus?modified precision scores and then multiply the re-sult by an exponential brevity penalty factor.
Cur-rently, case folding is the only text normalizationperformed before computing the precision.We first compute the geometric average of themodified n-gram precisions, pn, using n-grams up tolength N and positive weights wn summing to one.Next, let c be the length of the candidate transla-tion and r be the effective reference corpus length.We compute the brevity penalty BP,BP ={1 if c > re(1?r/c) if c ?
r .Then,BLEU= BP ?
exp(N?n=1wn log pn).The ranking behavior is more immediately apparentin the log domain,log BLEU = min(1?
rc,0)+N?n=1wn log pn.In our baseline, we use N = 4 and uniform weightswn = 1/N.3 The BLEU EvaluationThe BLEU metric ranges from 0 to 1.
Few transla-tions will attain a score of 1 unless they are identi-cal to a reference translation.
For this reason, evena human translator will not necessarily score 1.
Itis important to note that the more reference trans-lations per sentence there are, the higher the scoreis.
Thus, one must be cautious making even ?rough?comparisons on evaluations with different numbersof reference translations: on a test corpus of about500 sentences (40 general news stories), a humantranslator scored 0.3468 against four references andscored 0.2571 against two references.
Table 1 showsthe BLEU scores of the 5 systems against two refer-ences on this test corpus.The MT systems S2 and S3 are very close in thismetric.
Hence, several questions arise:Table 1: BLEU on 500 sentencesS1 S2 S3 H1 H20.0527 0.0829 0.0930 0.1934 0.2571Table 2: Paired t-statistics on 20 blocksS1 S2 S3 H1 H2Mean 0.051 0.081 0.090 0.192 0.256StdDev 0.017 0.025 0.020 0.030 0.039t ?
6 3.4 24 11?
Is the difference in BLEU metric reliable??
What is the variance of the BLEU score??
If we were to pick another random set of 500sentences, would we still judge S3 to be betterthan S2?To answer these questions, we divided the test cor-pus into 20 blocks of 25 sentences each, and com-puted the BLEU metric on these blocks individually.We thus have 20 samples of the BLEU metric foreach system.
We computed the means, variances,and paired t-statistics which are displayed in Table2.
The t-statistic compares each system with its leftneighbor in the table.
For example, t = 6 for the pairS1 and S2.Note that the numbers in Table 1 are the BLEUmetric on an aggregate of 500 sentences, but themeans in Table 2 are averages of the BLEU metricon aggregates of 25 sentences.
As expected, thesetwo sets of results are close for each system and dif-fer only by small finite block size effects.
Since apaired t-statistic of 1.7 or above is 95% significant,the differences between the systems?
scores are sta-tistically very significant.
The reported variance on25-sentence blocks serves as an upper bound to thevariance of sizeable test sets like the 500 sentencecorpus.How many reference translations do we need?We simulated a single-reference test corpus by ran-domly selecting one of the 4 reference translationsas the single reference for each of the 40 stories.
Inthis way, we ensured a degree of stylistic variation.The systems maintain the same rank order as withmultiple references.
This outcome suggests that wemay use a big test corpus with a single referencetranslation, provided that the translations are not allfrom the same translator.4 The Human EvaluationWe had two groups of human judges.
The firstgroup, called the monolingual group, consisted of 10native speakers of English.
The second group, calledthe bilingual group, consisted of 10 native speakersof Chinese who had lived in the United States forthe past several years.
None of the human judgeswas a professional translator.
The humans judgedour 5 standard systems on a Chinese sentence sub-set extracted at random from our 500 sentence testcorpus.
We paired each source sentence with eachof its 5 translations, for a total of 250 pairs of Chi-nese source and English translations.
We prepared aweb page with these translation pairs randomly or-dered to disperse the five translations of each sourcesentence.
All judges used this same webpage andsaw the sentence pairs in the same order.
They ratedeach translation from 1 (very bad) to 5 (very good).The monolingual group made their judgments basedonly on the translations?
readability and fluency.As must be expected, some judges were more lib-eral than others.
And some sentences were easierto translate than others.
To account for the intrin-sic difference between judges and the sentences, wecompared each judge?s rating for a sentence acrosssystems.
We performed four pairwise t-test compar-isons between adjacent systems as ordered by theiraggregate average score.4.1 Monolingual group pairwise judgmentsFigure 3 shows the mean difference between thescores of two consecutive systems and the 95% con-fidence interval about the mean.
We see that S2 isquite a bit better than S1 (by a mean opinion scoredifference of 0.326 on the 5-point scale), while S3is judged a little better (by 0.114).
Both differencesare significant at the 95% level.7 The human H1 ismuch better than the best system, though a bit worsethan human H2.
This is not surprising given that H1is not a native speaker of either Chinese or English,7The 95% confidence interval comes from t-test, assumingthat the data comes from a T-distribution with N degrees of free-dom.
N varied from 350 to 470 as some judges have skippedsome sentences in their evaluation.
Thus, the distribution isclose to Gaussian.whereas H2 is a native English speaker.
Again, thedifference between the human translators is signifi-cant beyond the 95% level.Figure 3: Monolingual Judgments - pairwise differ-ential comparison              4.2 Bilingual group pairwise judgmentsFigure 4 shows the same results for the bilingualgroup.
They also find that S3 is slightly better thanS2 (at 95% confidence) though they judge that thehuman translations are much closer (indistinguish-able at 95% confidence), suggesting that the bilin-guals tended to focus more on adequacy than on flu-ency.Figure 4: Bilingual Judgments - pairwise differentialcomparison            5 BLEU vs The Human EvaluationFigure 5 shows a linear regression of the monolin-gual group scores as a function of the BLEU scoreover two reference translations for the 5 systems.The high correlation coefficient of 0.99 indicatesthat BLEU tracks human judgment well.
Particularlyinteresting is how well BLEU distinguishes betweenS2 and S3 which are quite close.
Figure 6 showsthe comparable regression results for the bilingualgroup.
The correlation coefficient is 0.96.Figure 5: BLEU predicts Monolingual Judgments   		Figure 6: BLEU predicts Bilingual Judgments   	We now take the worst system as a reference pointand compare the BLEU scores with the human judg-ment scores of the remaining systems relative tothe worst system.
We took the BLEU, monolingualgroup, and bilingual group scores for the 5 systemsand linearly normalized them by their correspond-ing range (the maximum and minimum score acrossthe 5 systems).
The normalized scores are shown inFigure 7.
This figure illustrates the high correlationbetween the BLEU score and the monolingual group.Of particular interest is the accuracy of BLEU?s esti-mate of the small difference between S2 and S3 andthe larger difference between S3 and H1.
The figurealso highlights the relatively large gap between MTsystems and human translators.8 In addition, we sur-mise that the bilingual group was very forgiving injudging H1 relative to H2 because the monolingualgroup found a rather large difference in the fluencyof their translations.Figure 7: BLEU vs Bilingual and Monolingual Judg-ments    6 ConclusionWe believe that BLEU will accelerate the MT R&Dcycle by allowing researchers to rapidly home in oneffective modeling ideas.
Our belief is reinforcedby a recent statistical analysis of BLEU?s correla-tion with human judgment for translation into En-glish from four quite different languages (Arabic,Chinese, French, Spanish) representing 3 differentlanguage families (Papineni et al, 2002)!
BLEU?sstrength is that it correlates highly with human judg-8Crossing this chasm for Chinese-English translation ap-pears to be a significant challenge for the current state-of-the-artsystems.ments by averaging out individual sentence judg-ment errors over a test corpus rather than attemptingto divine the exact human judgment for every sen-tence: quantity leads to quality.Finally, since MT and summarization can both beviewed as natural language generation from a tex-tual context, we believe BLEU could be adapted toevaluating summarization or similar NLG tasks.Acknowledgments This work was partially sup-ported by the Defense Advanced Research ProjectsAgency and monitored by SPAWAR under contractNo.
N66001-99-2-8916.
The views and findingscontained in this material are those of the authorsand do not necessarily reflect the position of pol-icy of the Government and no official endorsementshould be inferred.We gratefully acknowledge comments about thegeometric mean by John Makhoul of BBN and dis-cussions with George Doddington of NIST.
We es-pecially wish to thank our colleagues who servedin the monolingual and bilingual judge pools fortheir perseverance in judging the output of Chinese-English MT systems.ReferencesE.H.
Hovy.
1999.
Toward finely differentiated evaluationmetrics for machine translation.
In Proceedings of theEagles Workshop on Standards and Evaluation, Pisa,Italy.Kishore Papineni, Salim Roukos, Todd Ward, John Hen-derson, and Florence Reeder.
2002.
Corpus-basedcomprehensive and diagnostic MT evaluation: InitialArabic, Chinese, French, and Spanish results.
In Pro-ceedings of Human Language Technology 2002, SanDiego, CA.
To appear.Florence Reeder.
2001.
Additional mt-eval references.Technical report, International Standards for LanguageEngineering, Evaluation Working Group.
http://issco-www.unige.ch/projects/isle/taxonomy2/J.S.
White and T. O?Connell.
1994.
The ARPA MT eval-uation methodologies: evolution, lessons, and futureapproaches.
In Proceedings of the First Conference ofthe Association for Machine Translation in the Ameri-cas, pages 193?205, Columbia, Maryland.
