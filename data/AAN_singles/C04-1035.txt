Classifying Ellipsis in Dialogue: A Machine Learning ApproachRaquel FERNA?NDEZ, Jonathan GINZBURG and Shalom LAPPINDepartment of Computer ScienceKing?s College LondonStrand, London WC2R 2LS, UK{raquel,ginzburg,lappin}@dcs.kcl.ac.ukAbstractThis paper presents a machine learning approachto bare sluice disambiguation in dialogue.
We ex-tract a set of heuristic principles from a corpus-basedsample and formulate them as probabilistic Hornclauses.
We then use the predicates of such clausesto create a set of domain independent features to an-notate an input dataset, and run two different ma-chine learning algorithms: SLIPPER, a rule-basedlearning algorithm, and TiMBL, a memory-basedsystem.
Both learners perform well, yielding simi-lar success rates of approx 90%.
The results showthat the features in terms of which we formulate ourheuristic principles have significant predictive power,and that rules that closely resemble our Horn clausescan be learnt automatically from these features.1 IntroductionThe phenomenon of sluicing?bare wh-phrasesthat exhibit a sentential meaning?constitutesan empirically important construction whichhas been understudied from both theoreticaland computational perspectives.
Most theoret-ical analyses (e.g.
(Ross, 1969; Chung et al,1995)), focus on embedded sluices consideredout of any dialogue context.
They rarely lookat direct sluices?sluices used in queries to re-quest further elucidation of quantified parame-ters (e.g.
(1a)).
With a few isolated exceptions,these analyses also ignore a class of uses we referto (following (Ginzburg and Sag, 2001) (G&S))as reprise sluices.
These are used to requestclarification of reference of a constituent in apartially understood utterance, as in (1b).
(1) a. Cassie: I know someone who?s a good kisser.Catherine: Who?
[KP4, 512]1b.
Sue: You were getting a real panic then.Angela: When?
[KB6, 1888]Our corpus investigation shows that the com-bined set of direct and reprise sluices constitutes1This notation indicates the British National Corpusfile (KP4) and the sluice sentence number (512).more than 75% of all sluices in the British Na-tional Corpus (BNC).
In fact, they make up ap-prox.
33% of all wh-queries in the BNC.In previous work (Ferna?ndez et al, to ap-pear), we implemented G&S?s analysis of di-rect sluices as part of an interpretation modulein a dialogue system.
In this paper we applymachine learning techniques to extract rules forsluice classification in dialogue.In Section 2 we present our corpus study ofclassifying sluices into dialogue types and dis-cuss the methodology we used in this study.Section 3 analyses the distribution patterns weidentify and considers possible explanations forthese patterns.
In Section 4 we identify a num-ber of heuristic principles for classifying eachsluice dialogue type and formulate these prin-ciples as probability weighted Horn clauses.
InSection 5, we then use the predicates of theseclauses as features to annotate our corpus sam-ples of sluices, and run two machine learningalgorithms on these data sets.
The first ma-chine learner used, SLIPPER, extracts opti-mised rules for identifying sluice dialogue typesthat closely resemble our Horn clause principles.The second, TiMBL, uses a memory-based ma-chine learning procedure to classify a sluice bygeneralising over similar environments in whichthe sluice occurs in a training set.
Both algo-rithms performed well, yielding similar successrates of approximately 90%.
This suggests thatthe features in terms of which we formulated ourheuristic principles for classifying sluices werewell motivated, and both learning algorithmsthat we used are well suited to the task of dia-logue act classification for fragments on the ba-sis of these features.
We finally present our con-clusions and future work in Section 6.2 Corpus Study2.1 The CorpusOur corpus-based investigation of bare sluiceshas been performed using the ?10 million worddialogue transcripts of the BNC.
The corpus ofbare sluices has been constructed using SCoRE(Purver, 2001), a tool that allows one to searchthe BNC using regular expressions.The dialogue transcripts of the BNC contain5183 bare sluices (i.e.
5183 sentences consist-ing of just a wh-word).
We distinguish betweenthe following classes of bare sluices: what, who,when, where, why, how and which.
Given thatonly 15 bare which were found, we have alsoconsidered sluices of the form which N. Includ-ing which N, the corpus contains a total of 5343sluices, whose distribution is shown in Table 1.The annotation was performed on two differ-ent samples of sluices extracted from the totalfound in the dialogue transcripts of the BNC.The samples were created by arbitrarily select-ing 50 sluices of each class (15 in the case ofwhich).
The first sample included all instancesof bare how and bare which found, making up atotal of 365 sluices.
The second sample con-tained 50 instances of the remaining classes,making up a total of 300 sluices.what why who where3045 1125 491 350when which N how which107 160 50 15Total: 5343Table 1: Total of sluices in the BNC2.2 The Annotation ProcedureTo classify the sluices in the first sample of oursub-corpus we used the categories described be-low.
The classification was done by 3 expertannotators (the authors) independently.Direct The utterer of the sluice understandsthe antecedent of the sluice without difficulty.The sluice queries for additional informationthat was explicitly or implicitly quantified awayin the previous utterance.
(2) Caroline: I?m leaving this school.Lyne: When?
[KP3, 538]Reprise The utterer of the sluice cannot un-derstand some aspect of the previous utterancewhich the previous (or possibly not directly pre-vious) speaker assumed as presupposed (typi-cally a contextual parameter, except for why,where the relevant ?parameter?
is somethinglike speaker intention or speaker justification).
(3) Geoffrey: What a useless fairy he was.Susan: Who?
[KCT, 1753]Clarification The sluice is used to ask forclarification about the previous utterance as awhole.
(4) June: Only wanted a couple weeks.Ada: What?
[KB1, 3312]Unclear It is difficult to understand whatcontent the sluice conveys, possibly because theinput is too poor to make a decision as to itsresolution, as in the following example:(5) Unknown : <unclear> <pause>Josephine: Why?
[KCN, 5007]After annotating the first sample, we decidedto add a new category to the above set.
Thesluices in the second sample were classified ac-cording to a set of five categories, including thefollowing:Wh-anaphor The antecedent of the sluice isa wh-phrase.
(6) Larna: We?re gonna find poison apple and Iknow where that one is.Charlotte: Where?
[KD1, 2371]2.3 ReliabilityTo evaluate the reliability of the annotation, weuse the kappa coefficient (K) (Carletta, 1996),which measures pairwise agreement between aset of coders making category judgements, cor-recting for expected chance agreement.
2The agreement on the coding of the firstsample of sluices was moderate (K = 52).3There were important differences amongstsluice classes: The lowest agreement was on theannotation for why (K = 29), what (K = 32)and how (K = 32), which suggests that thesecategories are highly ambiguous.
Examina-tion of the coincidence matrices shows that thelargest confusions were between reprise andclarification in the case of what, and be-tween direct and reprise for why and how.On the other hand, the agreement on classi-fying who was substantially higher (K = 71),with some disagreements between direct andreprise.Agreement on the annotation of the 2nd sam-ple was considerably higher although still notentirely convincing (K = 61).
Overall agree-ment was improved in all classes, except for2K = P (A)?P (E)/1?P (E), where P(A) is the pro-portion of actual agreements and P(E) is the proportionof expected agreement by chance, which depends on thenumber of relative frequencies of the categories undertest.
The denominator is the total proportion less theproportion of chance expectation.3All values are shown as percentages.where and who.
Agreement on what improvedslightly (K = 39), and it was substantiallyhigher on why (K = 52), when (K = 62) andwhich N (K = 64).Discussion Although the three coders maybe considered experts, their training and famil-iarity with the data were not equal.
This re-sulted in systematic differences in their anno-tations.
Two of the coders (coder 1 and coder2) had worked more extensively with the BNCdialogue transcripts and, crucially, with the def-inition of the categories to be applied.
Leavingcoder 3 out of the coder pool increases agree-ment very significantly: K = 70 in the firstsample, and K = 71 in the second one.
Theagreement reached by the more expert pair ofcoders was high and stable.
It provides a solidfoundation for the current classification.
It alsoindicates that it is not difficult to increase an-notation agreement by relatively light trainingof coders.3 Results: Distribution PatternsIn this section we report the results obtainedfrom the corpus study described in Section 2.The study shows that the distribution of read-ings is significantly different for each class ofsluice.
Subsection 3.2 outlines a possible expla-nation of such distribution.3.1 Sluice/Interpretation CorrelationsThe distribution of interpretations for each classof sluice is shown in Table 2.
The distributionsare presented as percentages of pairwise agree-ment (i.e.
agreement between pairs of coders),leaving aside the unclear cases.
This allowsus to see the proportion made up by each in-terpretation for each sluice class, together withany correlations between sluice and interpreta-tion.
Distributions are similar over both sam-ples, suggesting that corpus size is large enoughto permit the identification of repeatable pat-terns.Table 2 reveals interesting correlations be-tween sluice classes and preferred interpreta-tions.
The most common interpretation forwhat is clarification, making up 69% in thefirst sample and 66% in the second one.
Whysluices have a tendency to be direct (57%,83%).
The sluices with the highest probabilityof being reprise are who (76%, 95%), which(96%), which N (88%, 80%) and where (75%,69%).
On the other hand, when (67%, 65%) andhow (87%) have a clear preference for directinterpretations.1st Sample 2nd SampleDir Rep Cla Dir Rep Cla Wh-awhat 9 22 69 7 23 66 4why 57 43 0 83 14 0 3who 24 76 0 0 95 0 5where 25 75 0 22 69 0 9when 67 33 0 65 29 0 6which N 12 88 0 20 80 0 0which 4 96 0 ?
?
?
?how 87 8 5 ?
?
?
?Table 2: Distributions as pairwise agr percentages3.2 Explaining the Frequency HierarchyIn order to gain a complete perspective on sluicedistribution in the BNC, it is appropriate tocombine the (averaged) percentages in Table 2with the absolute number of sluices contained inthe BNC (see Table 1), as displayed in Table 3:whatcla 2040 whichNrep 135whydir 775 whendir 90whatrep 670 whodir 70whorep 410 wheredir 70whyrep 345 howdir 45whererep 250 whenrep 35whatdir 240 whichNdir 24Table 3: Sluice Class Frequency - Estim.
TokensFor instance, although more than 70% of whysluices are direct, the absolute number of whysluices that are reprise exceeds the total num-ber of when sluices by almost 3 to 1.
Explicatingthe distribution in Table 3 is important in or-der to be able to understand among other issueswhether we would expect a similar distributionto occur in a Spanish or Mandarin dialogue cor-pus; similarly, whether one would expect thisdistribution to be replicated across different do-mains.
Here we restrict ourselves to sketchingan explanation of a couple of striking patternsexhibited in Table 3.One such pattern is the low frequency of whensluices, particularly by comparison with whatone might expect to be its close cousin?where;indeed the direct/reprise splits are almostmirror images for when v. where.
Another verynotable pattern, alluded to above, is the highfrequency of why sluices.4The when v. where contrast provides one ar-gument against (7), which is probably the null4As we pointed out above, sluices are a commonmeans of asking wh?interrogatives; in the case of why?interrogatives, this is even stronger?close to 50% of allsuch interrogatives in the BNC are sluices.hypothesis w/r to the distribution of reprisesluices:(7) Frequency of antecedent hypothesis:The frequency of a class of reprise sluicesis directly correlated with the frequency ofthe class of its possible antecedents.Clearly locative expressions do not outnum-ber temporal ones and certainly not by theproportion the data in Table 3 would requireto maintain (7).5 (Purver, 2004) provides ad-ditional data related to this?clarification re-quests of all types in the BNC that pertain tonominal antecedents outnumber such CRs thatrelate to verbal antecedents by 40:1, which doesnot correlate with the relative frequency of nom-inal v. verbal antecedents (about 1.3:1).A more refined hypothesis, which at presentwe can only state quite informally, is (8):(8) Ease of grounding of antecedent hy-pothesis: The frequency of a class ofreprise sluices is directly correlated withthe ease with which the class of its possibleantecedents can be grounded (in the senseof (Clark, 1996; Traum, 1994)).This latter hypothesis offers a route towardsexplaining the when v. where contrast.
Thereare two factors at least which make ground-ing a temporal parameter significantly easier onthe whole than grounding a locative parameter.The first factor is that conversationalists typi-cally share a temporal ontology based on a clockand/or calendar.
Although well structured loca-tive ontologies do exist (e.g.
grid points in amap), they are far less likely to be common cur-rency.
The natural ordering of clock/calendar-based ontologies reflected in grammatical de-vices such as sequence of tense is a second fac-tor that favours temporal parameters over loca-tives.From this perspective, the high frequency ofwhy reprises is not surprising.
Such reprisesquery either the justification for an antecedentassertion or the goal of an antecedent query.Speakers usually do not specify these explicitly.In fact, what requires explanation is why such5A rough estimate concerning the BNC can be ex-tracted by counting the words that occur more than 1000times.
Of these approx 35k tokens are locative in natureand could serve as antecedents of where; the correspond-ing number for temporal expressions and when yieldsapprox 80k tokens.
These numbers are derived from afrequency list (Kilgarriff, 1998) of the demographic por-tion of the BNC.reprises do not occur even more frequently thanthey actually do.
To account for this, one hasto appeal to considerations of the importance ofanchoring a contextual parameter.6A detailed explication of the distributionshown in Table 3 requires a detailed model ofdialogue interaction.
We have limited ourselvesto suggesting that the distribution can be expli-cated on the basis of some quite general princi-ples that regulate grounding.4 Heuristics for sluicedisambiguationIn this section we informally describe a setof heuristics for assigning an interpretation tobare sluices.
In subsection 4.2, we show howour heuristics can be formalised as probabilisticsluice typing constraints.4.1 Description of the heuristicsTo maximise accuracy we have restricted our-selves to cases of three-way agreement amongthe three coders when considering the distri-bution patterns from which we obtained ourheuristics.
Looking at these patters we havearrived at the following general principles forresolving bare sluice types.What The most likely interpretation isclarification.
This seems to be the casewhen the antecedent utterance is a fragment, orwhen there is no linguistic antecedent.
Repriseinterpretations also provide a significant propor-tion (about 23%).
If there is a pronoun (match-ing the appropriate semantic constraints) in theantecedent utterance, then the preferred inter-pretation is reprise:(9) Andy: I don?t know how to do it.Nick: What?
Garlic bread?
[KPR, 1763]Why The interpretation of why sluices tendsto be direct.
However, if the antecedent is anon-declarative utterance, or a negative declar-ative, the sluice is likely to be a reprise.
(10) Vicki: Were you buying this erm newspaperlast week by any chance?Frederick: Why?
[KC3, 3388]Who Sluices of this form show a very strongpreference for reprise interpretation.
In themajority of cases, the antecedent is either aproper name (11), or a personal pronoun.6Another factor is the existence of default strategiesfor resolving such parameters, e.g.
assuming that thequestion asked transparently expresses the querier?s pri-mary goal.
(11) Patrick: [...] then I realised that it was FenniteKatherine: Who?
[KCV, 4694]Which/Which N Both sorts of sluices ex-hibit a strong tendency to reprise.
In theoverwhelming majority of reprise cases for bothwhich and which N, the antecedent is a definitedescription like ?the button?
in (12).
(12) Arthur: You press the button.June: Which one?
[KSS, 144]Where The most likely interpretation ofwhere sluices is reprise.
In about 70% ofthe reprise cases, the antecedent of the sluiceis a deictic locative pronoun like ?there?
or?here?.
Direct interpretations are preferredwhen the antecedent utterance is declarativewith no overt spatial location expression.
(13) Pat: You may find something in there actually.Carole: Where?
[KBH, 1817]When If the antecedent utterance is a declar-ative and there is no time-denoting expressionother than tense, the sluice will be interpretedas direct, as in example (14).
On the otherhand, deictic temporal expressions like ?then?trigger reprise interpretations.
(14) Caroline: I?m leaving this school.Lyne: When?
[KP3, 538]How This class of sluice exhibits a very strongtendency to direct (87%).
It appears thatmost of the antecedent utterances contain anaccomplishment verb.
(15) Anthony: I?ve lost the, the whole work itselfArthur: How?
[KP1, 631]4.2 Probabilistic ConstraintsThe problem we are addressing is typing of baresluice tokens in dialogue.
This problem is anal-ogous to part-of-speech tagging, or to dialogueact classification.We formulate our typing constraints as Hornclauses to achieve the most general and declar-ative expression of these conditions.
The an-tecedent of a constraint uses predicates corre-sponding to dialogue relations, syntactic prop-erties, and lexical content.
The predicate of theconsequent represents a sluice typing tag, whichcorresponds to a maximal type in the HPSGgrammar that we used in implementing our di-alogue system.
Note that these constraints can-not be formulated at the level of the lexical en-tries of the wh-words since these distributionsare specific to sluicing and not to non-ellipticalwh-interrogatives.7 As a first example, considerthe following rule:sluice(x), where(x),ant utt(y,x),contains(y,?there?)
?
reprise(x) [.78]This rule states that if x is a sluice constructionwith lexical head where, and its antecedent ut-terance (identified with the latest move in thedialogue) contains the word ?there?, then x is areprise sluice.
Note that, as in a probabilisticcontext-free grammar (Booth, 1969), the rule isassigned a conditional probability.
In the exam-ple above, .78 is the probability that the contextdescribed in the antecedent of the clause pro-duces the interpretation specified in the conse-quent.8The following three rules are concerned withthe disambiguation of why sluice readings.
Thestructure of the rules is the same as before.
Inthis case however, the disambiguation is basedon syntactic and semantic properties of the an-tecedent utterance as a whole (like polarity ormood), instead of focusing on a particular lexi-cal item contained in such utterance.sluice(x), why(x),ant utt(y,x), non decl(y) ?
reprise(x) [.93]sluice(x), why(x),ant utt(y,x), pos decl(y) ?
direct(x) [.95]sluice(x), why(x),ant utt(y,x), neg decl(y) ?
reprise(x) [.40]5 Applying Machine LearningTo evaluate our heuristics, we applied machinelearning techniques to our corpus data.
Ouraim was to evaluate the predictive power of thefeatures observed and to test whether the intu-itive constraints formulated in the form of Hornclause rules could be learnt automatically fromthese features.5.1 SLIPPERWe use a rule-based learning algorithm calledSLIPPER (for Simple Learner with IterativePruning to Produce Error Reduction).
SLIP-PER (Cohen and Singer, 1999) combines the7Thus, whereas Table 2 shows that approx.
70% ofwho-sluices are reprise, this is clearly not the casefor non-elliptical who?interrogatives.
For instance, theKB7 block in the BNC has 33 non-elliptical who?interrogatives.
Of these at most 3 serve as reprise ut-terances.8These probabilities have been extracted manuallyfrom the three-way agreement data.separate-and-conquer approach used by mostrule learners with confidence-rated boosting tocreate a compact rule set.The output of SLIPPER is a weighted ruleset, in which each rule is associated with a con-fidence level.
The rule builder is used to finda rule set that separates each class from the re-maining classes using growing and pruning tech-niques.
To classify an instance x, one computesthe sum of the confidences that cover x: if thesum is greater than zero, the positive class ispredicted.
For each class, the only rule witha negative confidence rating is a single defaultrule, which predicts membership in the remain-ing classes.We decided to use SLIPPER for two mainreasons: (1) it generates transparent, relativelycompact rule sets that can provide interestinginsights into the data, and (2) its if-then rulesclosely resemble our Horn clause constraints.5.2 Experimental SetupTo generate the input data we took all three-way agreement instances plus those instanceswhere there is agreement between coder 1 andcoder 2, leaving out cases classified as unclear.We reclassified 9 instances in the first sample aswh-anaphor, and also included these data.9 Thetotal data set includes 351 datapoints.
Thesewere annotated according to the set of featuresshown in Table 4.sluice type of sluicemood mood of the antecedent utterancepolarity polarity of the antecedent utterancefrag whether the antecedent utterance isa fragmentquant presence of a quantified expressiondeictic presence of a deictic pronounproper n presence of a proper namepro presence of a pronoundef desc presence of a definite descriptionwh presence of a wh wordovert presence of any other potentialantecedent expressionTable 4: FeaturesWe use a total of 11 features.
All features arenominal.
Except for the sluice feature that in-dicates the sluice type, they are all boolean, i.e.they can take as value either yes or no.
Thefeatures mood, polarity and frag refer to syn-tactic and semantic properties of the antecedent9We reclassified those instances that had motivatedthe introduction of the wh-anaphor category for the sec-ond sample.
Given that there were no disagreements in-volving this category, such reclassification was straight-forward.utterance as a whole.
The remaining features,on the other hand, focus on a particular lexicalitem or construction contained in such utter-ance.
They will take yes as a value if this ele-ment or construction exists and, it matches thesemantic restrictions imposed by the sluice type.The feature wh will take a yes value only if thereis a wh-word that is identical to the sluice type.Unknown or irrelevant values are indicated bya question mark.
This allows us to express, forinstance, that the presence of a proper name isirrelevant to determine the interpretation of awhere sluice, while it is crucial when the sluicetype is who.
The feature overt takes no as valuewhen there is no overt antecedent expression.
Ittakes yes when there is an antecedent expres-sion not captured by any other feature, and itis considered irrelevant (question mark value)when there is an antecedent expression definedby another feature.5.3 Accuracy ResultsWe performed a 10-fold cross-validation on thetotal data set, obtaining an average success rateof 90.32%.
Using leave-one-out cross-validationwe obtained an average success rate of 84.05%.For the holdout method, we held over 100 in-stances as a testing data, and used the reminder(251 datapoints) for training.
This yieldeda success rate of 90%.
Recall, precision andf-measure values are reported in Table 5.category recall precision f-measuredirect 96.67 85.29 90.62reprise 88.89 94.12 91.43clarification 83.33 71.44 76.92wh anaphor 80.00 100 88.89Table 5: SLIPPER - ResultsUsing the holdout procedure, SLIPPER gen-erated a set of 23 rules: 4 for direct, 13for reprise, 1 for clarification and 1 forwh-anaphor, plus 4 default rules, one for eachclass.
All features are used except for frag,which indicates that this feature does not play asignificant role in determining the correct read-ing.
The following rules are part of the rule setgenerated by SLIPPER:direct not reprise|clarification|wh anaphor :-overt=no, polarity=pos (+1.06296)reprise not direct|clarification|wh anaphor :-deictic=yes (+3.31703)reprise not direct|clarification|wh anaphor :-mood=non decl, sluice=why (+1.66429)5.4 Comparing SLIPPER and TiMBLAlthough SLIPPER seems to be especially wellsuited for the task at hand, we decided to run adifferent learning algorithm on the same train-ing and testing data sets and compare the re-sults obtained.
For this experiment we usedTiMBL, a memory-based learning algorithm de-veloped at Tilburg University (Daelemans etal., 2003).
As with all memory-based machinelearners, TiMBL stores representations of in-stances from the training set explicitly in mem-ory.
In the prediction phase, the similarity be-tween a new test instance and all examples inmemory is computed using some distance met-ric.
The system will assign the most frequentcategory within the set of most similar exam-ples (the k-nearest neighbours).
As a distancemetric we used information-gain feature weight-ing, which weights each feature according to theamount of information it contributes to the cor-rect class label.The results obtained are very similar to theprevious ones.
TiMBL yields a success rate of89%.
Recall, precision and f-measure values areshown in Table 6.
As expected, the feature thatreceived a lowest weighting was frag.category recall precision f-measuredirect 86.60 86.60 86.6reprise 88.89 90.50 89.68clarification 83.33 71.44 76.92wh anaphor 100 100 100Table 6: TiMBL -Results6 Conclusion and Further WorkIn this paper we have presented a machinelearning approach to bare sluice classificationin dialogue using corpus-based empirical data.From these data, we have extracted a set ofheuristic principles for sluice disambiguationand formulated such principles as probabilityweighted Horn clauses.
We have then usedthe predicates of these clauses as features toannotate an input dataset, and ran two dif-ferent machine learning algorithms: SLIPPER,a rule-based learning algorithm, and TiMBL,a memory-based learning system.
SLIPPERhas the advantage of generating transparentrules that closely resemble our Horn clause con-straints.
Both algorithms, however, performwell, yielding to similar success rates of approx-imately 90%.
This shows that the features weused to formulate our heuristic principles werewell motivated, except perhaps for the featurefrag, which does not seem to have a signifi-cant predictive power.
The two algorithms weused seem to be well suited to the task of sluiceclassification in dialogue on the basis of thesefeatures.In the future we will attempt to constructan automatic procedure for annotating a dia-logue corpus with the features presented here,to which both machine learning algorithms ap-ply.ReferencesT.
Booth.
1969.
Probabilistic representationof formal languages.
In IEEE ConferenceRecord of the 1969 Tenth Annual Symposiumof Switching and Automata Theory.J.
Carletta.
1996.
Assessing agreement on clas-sification tasks: the kappa statistics.
Compu-tational Linguistics, 2(22):249?255.S.
Chung, W. Ladusaw, and J. McCloskey.1995.
Sluicing and logical form.
Natural Lan-guage Semantics, 3:239?282.H.
H. Clark.
1996.
Using Language.
CambridgeUniversity Press, Cambridge.W.
Cohen and Y.
Singer.
1999.
A simple, fast,and effective rule learner.
In Proc.
of the 16thNational Conference on AI.W.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2003.
TiMBL: TilburgMemory Based Learner, Reference Guide.Technical Report ILK-0310, U. of Tilburg.R.
Ferna?ndez, J. Ginzburg, H. Gregory, andS.
Lappin.
(to appear).
SHARDS: Frag-ment resolution in dialogue.
In H. Bunt andR.
Muskens, editors, Computing Meaning,volume 3.
Kluwer.J.
Ginzburg and I.
Sag.
2001.
InterrogativeInvestigations.
CSLI Publications, Stanford,California.A.
Kilgarriff.
1998.
BNC Database andWord Frequency Lists.
www.itri.bton.ac.uk/?Adam.Kilgarriff/ bnc-readme.html.M.
Purver.
2001.
SCoRE: A tool for searchingthe BNC.
Technical Report TR-01-07, Dept.of Computer Science, King?s College London.M.
Purver.
2004.
The Theory and Use of Clari-fication in Dialogue.
Ph.D. thesis, King?s Col-lege, London, forthcoming.J.
Ross.
1969.
Guess who.
In Proc.
of the 5thannual Meeting of the Chicago Linguistics So-ciety, pages 252?286, Chicago.
CLS.D.
Traum.
1994.
A Computational Theory ofGrounding in Natural Language Conversa-tion.
Ph.D. thesis, University of Rochester,Department of Computer Science, Rochester.
