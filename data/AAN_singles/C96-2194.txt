A Gradual Refinement Model for A Robust Thai Morphological AnalyzerKAWTRAKUL Asanee, THUMKANON Chalatip,JAMJANYA Thitima, MUANGYUNNAN Parinee, POOLWAN KritsadaNatural Language Processing Research Laboratory, Department of Computer Engineering Kasetsart University,Paholyothin Rd.,Bangkok 10900, Thailand.email: ak@nontri.ku.ac.thINAGAKI YasuyoshiDepartment of Information Engineering Nagoya University, Chikusa-ku, Nagoya 464, Japan.email : inagaki@inagaki.nuie.nagoya-u.ac.jpAbstractThis work attempts to provide a robust Thaimorphological analyzer which can automaticallyassign the correct part-of-speech tag to the correctword with time and space efficiency.
Instead of usinga corpus based approach which requires a largeamount of training data and validation data, a newsimple hybrid technique which incorporates heuristic,syntactic and semantic knowledge is proposed.
Toimplement his technique, a three-stage approach isadopted to the gradual refinement module.
It consistsof preference based pruning, syntactic based pruningand semantic based pruning.
Each stage willgradually weeds out word boundary ambiguities, tagambiguities and implicit spelling errors.
Frdm theresult of the experiment, the proposed model canwork with time-efficiency and increase the accuracyof word boundary segmentations, POS tagging aswell as implicit spelling error correction.1.
IntroductionOne of the important requirements fordeveloping practical natural language processingsystem is a morphological analyzer that canautomatically assign the correct POS (part-of-speech)tagging to the correct word with time and spaceefficiency.
For non-separated languages such asJapanese, Korea, Chinese and Thai, the more task inmorphological nalyzer is needed, i.e, segmenting aninput sentence into the right words (Nobesawa et.al,1994; Seung-Shik Kang et.al, 1994).
However, thereis another problematic aspect, called implicit spellingerror, that should be solved in morphologicalprocessing level.
The implicit spelling errors arespelling errors which make the other rightmeaningful words., This work attempts to provide arobust morphological analyzer by using a gradualrefinement module for weeding out the manypossible alternatives and/or the erroneous chains ofwords caused by those three non-trivial problems:word boundary ambiguity, POS tagging ambiguityand implicit spelling error.Many researchers have used a corpus basedapproach to POS tagging such as trigram model(Charniak, 1993); feature structure tagger(Kemp,1994), to word segmentation, such as D-bigram (Nobesawa et.al, 1994), to both POS taggingand word segmentation (Nagata, 1994) and tospelling error detection as well as correction (Arakiet.al, 1994; Kawtrakul, et.al, 1995(b)).
Eventhough acorpus based approach exhibits seemingly highaverage accuracy, it requires a large amount oftraining data and validation, data (Franz, 1995).Instead of using a corpus based approach, a newsimple hybrid technique which incorporates heuristic,syntactic and semantic knowledge is proposed toThai morphological analyzer.
It consists of word-boundary preference, syntactic coarse rules andsemantic strength measurement.
To implement histechnique, a three-stage approach is adopted to thegradual refinement module : preference basedpruning, syntactic based pruning and semantic basedpruning.
Each stage will gradually weed out wordboundary ambiguities, tag ambiguities and implicitspelling errors.Our preliminary experiment shows that theproposed model can work with a time-efficiency andincrease the accuracy of word boundary and taggingdisambiguation as well as the implicit spelling errorcorrection.In the following sections, we will begin byreviewing three non-trivial problems of Thaimorphological nalyzer.
An overview of the gradualrefinement module will be given.
We will then showthe algorithm with examples for pruning theerroneous word chains prior to parsing.
Finally, theresults of applying this algorithm will be presented.2.
Three Nontrivial Problems of ThaiMorphological Processing.2.1 Word Boundary AmbiguityLike many other languages such asJapanese, Chinese and Korean, Thai sentences areformed with a sequence of words mostly Withoutexplicit delimiters.
Especially, for Thai and Japanesewritten in Hirakana (Nobesawa,1994), a word is astream of characters.
This causes the problem ofword boundary ambiguity (see Fig.
1).1086stream of characters W 1 W2C1C2C3C4C5 \ [ ~  (2)Figurel.
Two possible grouping characters into words:longest possible segment or shortest possible segmentThere are two possible grouping charactersinto words, i.e, shortest possible segment such as (1)and longest possible segment such as (2) in Fig.1.Each word given by either way of grouping has ameaning.
In our corpus, more than 50% of sentencesinclude word boundary ambiguity.
This causes a lotof alternative chains of words where some aremeaningless.2.2 Tagging AmbiguityThai word can have more than one part ofspeech.
In our corpus, only 2% of sentences arewritten by using one-tagged words.
Accordingly, tagambiguity in Thai causes a large set of tagged wordcombinations.
We found that a sentence with 12words can generate 3027 syntactic patterns of wordchain.
Both word boundary and tag ambiguity alsocreate complexity in syntactic analysis.2.3 Implicit Spelling ErrorSpelling errors in Thai are classified intotwo types (Kawtrakul, 1995 (b)): explicit spellingerror and implicit spelling error.
The former can bedetected easily by using a dictionary-based approach.The latter can not be detected by simply usingdictionary since the error Call lead to words that areunintended, but spelled correctly.
Table I showsthree kinds of spelling errors caused by carelessnessand lack of knowledge.Table 1.
Three types of implicit spelling error.TypeMissingMistypingSwappingCause...... carelessness lack of knowle'dge(t)his---> his free -----> teefa(t) ----> far both ---> boat(n)o -----> on form ----> fromIn Thai, implicit spelling errors can occurmore easily than in English because there are 2distinctive characters on each-keypad.
From theresult of our experiment, 2,286 words can generate6,609 implicit spelling error words where 75.68 % ofthose errors have new syntactic ategories.
This willcause an erroneous pattern of word chain whichincreases a lot of unnecessary job to the parser.Accordingly, Thai morphological nalysis isnot only expected to assign the right tag to the rightword but should correct the implicit spelling errorprior to parsing.3.
An Overview of Thai MorphologicalAnalyzer with a Gradual Refinement ModuleInstead of using a corpus based approachwhich requires a large amount of training data andvalidation data, a new simple hybrid technique whichincorporates heuristic, syntactic and semanticknowledge is proposed to a gradual refinementmodule which gradually weeds out the alternativeand/or the erroneous chains of words caused by thosethree nontrivial problems.
The techniqueisimplemented by using word boundary preference,syntactic coarse rules and semantic dependencystrength measurement.
Fig.2 shows an overview ofthe system.The system consists of four steps:Step 0: This step provides all possible wordgroupings with all possible .tags by using wordformation rules and Lexicon base (Kawtrakul et.al,1995 (a)).
If there is any explicit spelling error, it willbe detected and suggested for correction.
At thisstage a temporary dictionary is created for theremaining steps.Step 1-3 : These steps are preference based pruning,syntactic based pruning and semantic based pruning.Each step will gradually weeds out word boundaryambiguities, tag ambiguities and implicit spellingerrors.Morphological AnalyzerInput ._._) step 0 L f sentence Word Segmenting & \]Explicit Spelling Er ro~Prunning Astep 1PreferencebasedPrunningGradual Relinement MOdule+H "Syntactic based Pruning I'l Semantic based Prunning/Implicit Spelling Error IlImplicit Spelling Error /Correction \]\[Correctioin \[?1 ~  D ic t ionary -Syntact i c~l~ Collection of preferred word segmentation \]the most likely .->sequence oftagged wordsFigure 2.
An overview of Thai morphological Analysis with a gradual refinement module10874.
A Gradua l  Re f inement  Modu le4.1 Preference based PruningFrom the Fig.l, Thai Word Segmentationcan be implemented as follows:case i - only longest segmentation or shortestsegmentation is possible,case ii - both longest segmentation and shortestsegmentation are possible.The former will be processed at this stage bylooking up the preferred words (see Table 2).
Someof them are determined by the cooccurrence word inthe left or right?
For the latter, it will be processed bythe next steps.Table 2.
The Collection of Preferred Words.stream I segmentationof char.
longest shortest preferred L 1 R!
!
I il l l t l l  I ~ l f l - I  1 ~ l - l l l  1 I J l -n l  I * *(much-that)\] more-than (more-than)(electricity)(twist)(twist)IW-~11 IW~11 * *(fire-sky) (electricity)(raft-down\] (raft-down) (push)I I~ i - \ [ l~  I I~ l \ [ l~  {ill,14 El} *(raft-down\] (twist) (leg,hand)Note : * means any word, L1 means a word in the left., RImeans a word in the right.In summary, word boundary preference isused to prune the word chains which consist ofimpossibly occurred or rarely occured wordsegmentation.4.2 Syntactic based Pruning and Implicit SpellingCorrectionAt this stage, the syntactic coarse rules areused for pruning the remaining erroneous wordchains caused by the word boundary ambiguities,tagging ambiguities and/or implicit spelling errors.Syntactic Coarse Rules .
'An example of the syntacticcoarse rules for a set of  two consecutive words(Wi,Wi+l) in Thai grammar is given as follows ?i f  Wi is noun then Wi+ z might be : noun, verb, modifier .....ifWi is verb then Wi+ s might be : noun, postverb, rood .....The POS matrix (PM) given below is used toimplement he finite state automaton model of  thesyntactic oarse rules: where syntactic ategory of  Wiis cati and Wi+l, is catH.Table 3.
The 46 X 46 POS matrix obtained from 20,000sentences corpus.cat i stop noun verbstart 1 1noun 0 1 Iverb 1 1 Imod.
0 0 0postv.
0el.
0 "-0eat i+  1rood.
postv, cl .
.
.
.
.
.
.0 0 0 .....1 0 11 1 0l 0 0Note: start means the beginning of a sentence, stop meansthe end of a sentence.Together with the POS matrix, someconstraints, called flag, are used to change the PMijfrom 0 to 1.
For example :i f  there exist "'verb" before "'modifier" then f lag = 1else f lag : 0According to the above constraint, PMij,where i = modifier and j = postverb, can be changedfrom 0 to 1 if flag equals 1.
Based on POS matrix andconstraints, now, we can use the following definitionto detect he position of  error in the word chains.\ [ - - -T rue  if PMi i : 1cati ' cati+l= "t True if ( PMij = 0) ^  (flag : 1)L_ .Fa lse  if ( PMij = 0) ^  (flag = 0)Consider the following example ?W1 W2 W3 W4{tube-shape container : n, el} {is : v} {on : prep} {table : n}As shown above, Wl has 2 tags : noun andclassifier.
However, "'classifier" will be pruned sinceit violates the syntactic coarse rule that "classifier"could not be an initial word.
The POS matrix is usedto disambiguate word boundary as well.Finally, if there is no word chain which hasall right POS sequences, the erroneous word chain,which has the error marker at the most remoteposition, will be selected and be expected that there isan implicit spelling error.
Then the word generatingfunction will be called for generating a set ofcandidate words to that position and the process willstart pruning at this stage again.4.3 Semantic based Pruning and Implicit SpellingCorrectionSince the syntactic coarse rules only weedout the erroneous POS word chains, some errors still1088remain.
At this stage, the semantic information fromLexicon Matrix (Kawtrakul et.al, 1995 (a)) isaccessed and used to calculate the semanticdependency strength between certain pairs of twowords.
Consider the following example :t spelling errort~q ~Jlfl{ he:pron. }
{bent:verb } { so nmch: modifier }I Iwea~h stron~ strengthAs shown in the above example, there is noPOS chain error, but there exists an implicit spellingerror which can be detected by using the semanticdependency strength.
The word generating functionwill be called for generating a set of candidate wordsfor the two consecutive words that have weakstrength, and the process will return to step 2 forpruning the erroneous POS chains and then goto step3 for calculating the semantic strength again.
Thestrongest strength chain will be selected as the mostlikely sequence of the right words in the right tags.The final solution for the above example ist'll'l ~ ~qn{he:pron.
}{think:verb} {so much: modif ier}5.
Experimentation ResultsWe tested the system on PC-486 DX2 byusing two hundred sentences corpus.
The percentagesof word correctly segmented, tagged and spelled,based on the gradual refinement module and timeefficiency are compared with the results based on astatistical approach to word filtering on small trainingcorpus (Kawtrakul, 1995 (b)) as shown in Table 4.Table 4.
Percentage ofAccuracyApproachCorpus basedWordSegmentation85.2%(word filtering)Linguistic based(the gradual Iretinement model)\]92.5%POS \] implicit speedtagging spelling (lbr onecorrection sentence)76,6% 61.9% msec.- mill.88,7% 76.6% msec.6.
Conclusion and Future WorkThis paper has described a new simpletechnique that performs the disambiguation of wordboundary, POS tagging and implicit spellingcorrection by using local information such as lexiconpreference, a consecutive POS preference andsemantic dependency strength measurement of theassociative words in a sentence.
From theexperimentation results, while a corpus basedapproach has proven to be efficient, the methodseems to be computationally costly and requires alarge amount of training data and validation data.
Forthe proposed model, it can work in time efficient andincrease the accuracy of word boundary and taggingdisambiguation aswell as implicit spelling error.The further directions for this research willconcern with unknown word processing and increasethe accuracy of the gradual refinement method.AcknowledgementsThe work reported in this paper wassupported by the National Research Council ofThailand.
Thanks are also due, to Patcharee Varasai,Supapas Kumtanode, Mukda Suktarajarn for theirlinguistic helps..ReferencesAraki, T., Ikehara, S., Tsukahara, N. and Komatsu,Y., "An Evaluation to Detect and Correct ErroneousCharacters Wrongly Substituted, Deleted andInserted in Japanese and English Sentences UsingMarkov Model", Coling 94, pp.
187-193, 1994.Charniak, E., "StatisticalLanguage Learning", MITPress, 1993.Franz Alexander, "An Exploration of Stochastic Partof Speech Tagging", Proceeding NLPRLS '95, pp.217-222, 1995.Kang, S-S. and Kim, Y.T., "Syllable-Based ModelFor the Korean Morphology", Coling 94, pp.
221-226, 1994.Kawtrakul, A., Kumtanode, S.., "A Lexicon Modelfor Writing Production Assistant System", TheProceeding of the second Symposium on NaturalLanguage Processing, pp.
226-236, 1995 (a).Kawtrakul, A., "A computational Model for WritingProduction Assistant System", The ProceedingNLPRL '95, pp.
119-124, 1995 (b).Kemp, A., "Probabilistic Tagging with FeatureStructures", Coling 94, pp.
161-165, 1994.Nagata, M., "A Stochastic Japanese MorphologicalAnalyzer Using a Forward-DP Backward-A* N-Best Search Algorithm", Coling 94, pp.
201-207,1994.Nobesawa, S., Tsutsumi, J., Nitta, T., Ono, K., Jiang,S.
and Nakamishi, M., "Segmenting a Sentence intoMorphemes using Statistic Information BetweenWords", Coling 94, pp.
227-232, 1994.1089
