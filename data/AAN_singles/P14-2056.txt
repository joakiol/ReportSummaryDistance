Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 339?344,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsPredicting Power Relations between Participantsin Written Dialog from a Single ThreadVinodkumar PrabhakaranDept.
of Computer ScienceColumbia University, New York, NYvinod@cs.columbia.eduOwen RambowCntr.
for Comp.
Learning SystemsColumbia University, New York, NYrambow@ccls.columbia.eduAbstractWe introduce the problem of predictingwho has power over whom in pairs of peo-ple based on a single written dialog.
Wepropose a new set of structural features.We build a supervised learning system topredict the direction of power; our newfeatures significantly improve the resultsover using previously proposed features.1 IntroductionComputationally analyzing the social context inwhich language is used has gathered great interestwithin the NLP community recently.
One of theareas that has generated substantial research is thestudy of how social power relations between peo-ple affect and/or are revealed in their interactionswith one another.
Researchers have proposed sys-tems to detect social power relations between par-ticipants of organizational email threads (Bramsenet al, 2011; Gilbert, 2012; Prabhakaran and Ram-bow, 2013), online forums (Danescu-Niculescu-Mizil et al, 2012; Biran et al, 2012; Danescu-Niculescu-Mizil et al, 2013), chats (Strzalkowskiet al, 2012), and off-line interactions such as pres-idential debates (Prabhakaran et al, 2013; Nguyenet al, 2013).
Automatically identifying power andinfluence from interactions can have many prac-tical applications ranging from law enforcementand intelligence to online marketing.A significant number of these studies are per-formed in the domain of organizational emailwhere there is a well defined notion of power (or-ganizational hierarchy).
Bramsen et al (2011) andGilbert (2012) predict hierarchical power relationsbetween people in the Enron email corpus usinglexical features extracted from all the messagesexchanged between them.
However, their ap-proaches primarily apply to situations where largecollections of messages exchanged between pairsof people are available.
In (Prabhakaran and Ram-bow, 2013), we introduced the problem of detect-ing whether a participant of an email thread haspower over someone else in the thread and estab-lished the importance of dialog structure in thattask.
However, in that work we did not detect overwhom that person has power.In this paper, we introduce a new problem for-mulation.
We predict the hierarchical power rela-tion between pairs of participants in an email in-teraction thread based solely on features extractedfrom that thread.
As a second major contribution,we introduce a new set of features to capture as-pects of participant behavior such as responsive-ness, and we show that these features are signifi-cantly correlated with the direction of power.
Wepresent a fully automatic system for this task ob-taining an accuracy of 73.0%, an improvement of6.9% over 68.3% by a system using only lexicalfeatures.
This best-performing system uses ournew feature set.2 MotivationEarly NLP-based approaches such as Bramsen etal.
(2011) and Gilbert (2012) built systems to pre-dict hierarchical power relations between peoplein the Enron email corpus using lexical featuresfrom all the messages exchanged between them.One limitation of this approach is that it reliessolely on lexical cues and hence works best whenlarge collections of messages exchanged betweenthe pairs of people are available.
For example,Bramsen et al (2011) excluded sender-recipientpairs who exchanged fewer than 500 words fromtheir evaluation set, since they found smaller textsamples are harder to classify.
By taking the mes-sage out of the context of the interaction in whichit was exchanged, they fail to utilize cues from thestructure of interactions, which complements thelexical cues in detecting power relations, as weshowed in (Prabhakaran and Rambow, 2013).339We modeled the problem of detecting power re-lationships differently in (Prabhakaran and Ram-bow, 2013): we predicted whether a participantin an email thread has a certain type of poweror not.
However, in that work we did not pre-dict over whom he/she has that power.
Thismay result in noisy features; consider a thread inwhich participant X has power over participantY , who has power over participant Z .
By ag-gregating features over all messages sent by Y ,features salient to a subordinate-superior interac-tion are incorrectly conflated with those salient tosuperior-subordinate interaction.
Another limita-tion of (Prabhakaran and Rambow, 2013) is thatwe used manual annotations for many of our fea-tures such as dialog acts and overt displays ofpower.
Relying on manual annotations for featureslimited our analysis to a small subset of the Enroncorpus, which has only 18 instances of hierarchi-cal power.
Consequently, our findings with respectto hierarchical power were weak in terms of bothcorrelations of features and system performance.In this paper, we introduce the problem of pre-dicting who has power over whom in pairs of inter-acting participants based on a single thread of in-teractions.
From (Bramsen et al, 2011) we retainthe idea that we want to predict the power relationbetween pairs of people.
But in contrast to theirformulation, we retain the goal from (Prabhakaranand Rambow, 2013) that we want to study com-munication in the context of an interaction, andthat we want to be able to make predictions us-ing only the emails exchanged in a single thread.Like (Prabhakaran and Rambow, 2013), we usefeatures to capture the dialog structure, but we useautomatic taggers to generate them and assume nomanual annotation at all at training or test time.This allows us to use the entire Enron email cor-pus for this study.3 DataIn this work, we use the version of Enron emailcorpus by Yeh and Harnly (2006) which capturesthe thread structure of email exchanges.
The cor-pus contains 36,615 email threads.
We excluded asmall subset of 419 threads that was used for pre-vious manual annotation efforts, part of which wasalso used to train the DA and ODP taggers (Sec-tion 5) that generate features for our system.
Theaverage number of email messages per thread wasaround 3.
We divided the remaining threads intotrain (50%), dev (25%) and test (25%) sets by ran-dom sampling.
We then applied various basic NLPpreprocessing steps such as tokenization, POS tag-ging and lemmatization to the body of email mes-sages.
We use the Enron gold organizational hier-archy released by Agarwal et al (2012) to modelhierarchical power.
Their corpus was manuallybuilt using information from Enron organizationalcharts.
It includes relations of 1,518 employeesand captures dominance relations between 13,724pairs of them.
Theirs is the largest such data setavailable to the best of our knowledge.4 Problem FormulationLet t denote an email thread and Mtdenote theset of all messages in t .
Also, let Ptbe the setof all participants in t , i.e., the union of sendersand recipients (To and CC) of all messages inMt.
We are interested in detecting power rela-tions between pairs of participants who interactwithin a given email thread.
Not every pair of par-ticipants (p1, p2) ?
Pt?
Ptinteract with one an-other within t .
Let IMt(p1, p2) denote the set ofInteraction Messages ?
non-empty messages int in which either p1is the sender and p2is oneof the recipients or vice versa.
We call the set of(p1, p2) such that |IMt(p1, p2)| > 0 the interact-ing participant pairs of t (IPPt).We focus on the manifestations of power in in-teractions between people across different levelsof hierarchy.
For every (p1, p2) ?
IPPt, we querythe set of dominance relations in the gold hierar-chy to determine their hierarchical power relation(HP(p1, p2)).
We exclude pairs that do not existin the gold hierarchy from our analysis and denotethe remaining set of related interacting participantpairs as RIPPt.
We assign HP(p1, p2) to be su-perior if p1dominates p2, and subordinate if p2dominates p1.
Table 1 shows the total number ofpairs in IPPtand RIPPtfrom all the threads inour corpus and across train, dev and test sets.Description Total Train Dev Test# of threads 36,196 18,079 8,973 9,144?t|IPPt| 355,797 174,892 91,898 89,007?t|RIPPt| 15,048 7,510 3,578 3,960Table 1: Data StatisticsRow 1 presents the total number of threads in differentsubsets of the corpus.
Row 2 and 3 present the number ofinteracting participant pairs (IPP ) and related interactingparticipant pairs (RIPP ) in those subsets.340Given a thread t and a pair of participants(p1, p2) ?
RIPPt, we want to automatically de-tect HP(p1, p2).
This problem formulation issimilar to the ones in (Bramsen et al, 2011) and(Gilbert, 2012).
However, the difference is that forus an instance is a pair of participants in a singlethread of interaction (which may or may not in-clude other people), whereas for them an instanceconstitutes all messages exchanged between a pairof people in the entire corpus.
Our formula-tion also differs from (Prabhakaran and Rambow,2013) in that we detect power relations betweenpairs of participants, instead of just whether a par-ticipant had power over anyone in the thread.5 Structural AnalysisIn this section we analyze various features thatcapture the structure of interaction between thepairs of participants in a thread.
Each feature fis extracted with respect to a person p over a ref-erence set of messages M (denoted fpM).
For apair (p1, p2), we extract 4 versions of each fea-ture f : fp1IMt(p1,p2), fp2IMt(p1,p2), fp1Mtand fp2Mt.
Thefirst two capture behavior of the pair among them-selves, while the third and fourth capture theiroverall behavior in the entire thread.
We group ourfeatures into three categories ?
THRNew, THRPRand DIAPR.
THRNewis a set of new features wepropose, while THRPRand DIAPRincorporate fea-tures we proposed in (Prabhakaran and Rambow,2013).
THRNewand THRPRcapture the structureof message exchanges without looking at the con-tent of the emails (e.g., how many emails did a per-son send), while DIAPRcaptures the pragmatics ofthe dialog and requires an analysis of the contentof the emails (e.g., did they issue any requests).THRNew: This is a new set of features we in-troduce in this paper.
It includes the average num-ber of recipients (AvgRecipients) and To recipients(AvgToRecipients) in emails sent by p, the per-centage of emails p received in which he/she wasin the To list (InToList%), boolean features de-noting whether p added or removed people whenresponding to a message (AddPerson and Re-movePerson), average number of replies receivedper message sent by p (ReplyRate) and averagenumber of replies received from the other personof the pair to messages where he/she was a To re-cipient (ReplyRateWithinPair).
ReplyRateWithin-Pair applies only to IMt(p1, p2).THRPR: This feature set includes two meta-data based feature sets ?
positional and verbosity.Positional features include a boolean feature to de-note whether p sent the first message (Initiate),and relative positions of p?s first and last messages(FirstMsgPos and LastMsgPos) in M .
Verbosityfeatures include p?s message count (MsgCount),message ratio (MsgRatio), token count (Token-Count), token ratio (TokenRato) and tokens permessage (TokenPerMsg), all calculated over M .DIAPR: In (Prabhakaran and Rambow, 2013),we used dialog features derived from manual an-notations ?
dialog acts (DA) and overt displaysof power (ODP) ?
to model the structure of inter-actions within the message content.
In this work,we obtain DA and ODP tags on the entire cor-pus using automatic taggers trained on those man-ual annotations.
The DA tagger (Omuya et al,2013) obtained an accuracy of 92%.
The ODPtagger (Prabhakaran et al, 2012) obtained an ac-curacy of 96% and F-measure of 54%.
The DAtagger labels each sentence to be one of the 4dialog acts: Request Action, Request Informa-tion, Inform, and Conventional.
The ODP Tag-ger identifies sentences (mostly requests) that ex-press additional constraints on its response, be-yond those introduced by the dialog act.
We use5 features: ReqAction%, ReqInform%, Inform%,Conventional%, and ODP% to capture the per-centage of sentences in messages sent by p that haseach of these labels.
We also use a feature to cap-ture the number of p?s messages with a request thatdid not get a reply, i.e., dangling requests (Dan-glingReq%), over all messages sent by p.We perform an unpaired two-sample two-tailedStudent?s t-Test comparing mean values of eachfeature for subordinates vs. superiors.
For ouranalysis, a data point is a related interacting pair,and not a message.
Hence, a message with mul-tiple recipients who have a superior/subordinaterelation with the sender will contribute to featuresfor multiple data points.
We limit our analysis tothe related interacting pairs from only our trainset.
Table 2 presents mean values of features forsubordinates and superiors at the interaction level.Thread level versions of these features also ob-tained similar results overall in terms of directionof difference and significance.
We denote threesignificance levels ?
* (p < .05 ), ** (p < .01 ),and *** (p < .001 ).
To control false discoveryrates in multiple testing, we adjusted the p-values(Benjamini and Hochberg, 1995).
We summarize341Feature Name Mean(fsubIMt) Mean(fsupIMt)THRNewAvgRecipients??
?21.14 43.10AvgToRecipients??
?18.19 38.94InToList% 0.82 0.80ReplyRate??
?0.86 1.23ReplyRateWithinPair??
?0.16 0.10AddPerson 0.48 0.47RemovePerson??
?0.41 0.37THRPRInitiate??
?0.45 0.56FirstMsgPos 0.04 0.03LastMsgPos??
?0.15 0.11MsgCount??
?0.64 0.70MsgRatio??
?0.44 0.56TokenCount 91.22 83.26TokenRatio??
?0.45 0.55TokenPerMsg?140.60 120.87DIAPRConventional%??
?0.15 0.17Inform%??
?0.78 0.72ReqAction%??
?0.02 0.04ReqInform%??
?0.05 0.06DanglingReq%??
?0.12 0.15ODP%??
?0.03 0.06Table 2: Student?s t-Test Results of fpIMt.THRNew: new meta-data features; THRPR, DIAPR: meta-dataand dialog-act features from previous studies;* (p < .05 ); ** (p < .01 ); *** (p < .001 )the main findings on the significant features below.1.
Superiors send messages addressed to morepeople (AvgRecipients and AvgToRecipi-ents).
Consequently, they get more replies totheir messages (ReplyRate).
However, con-sidering messages where the other person ofthe pair is addressed in the To list (ReplyRate-WithinPair), subordinates get more replies.2.
Superiors issue more requests (ReqAction%and ReqInform%) and overt displays ofpower (ODP%).
Subordinates issue moreinforms (Inform%) and, surprisingly, havefewer unanswered requests (DanglingReq%).3.
Superiors initiate the interactions more oftenthan subordinates (Initiate).
They also leaveinteractions earlier (LastMsgPos).4.
Superiors send shorter messages (Token-PerMsg).
They also send more messages(MsgCount & MsgRatio) and even contributea higher ratio of tokens in the thread (Token-Ratio) despite sending shorter messages.Finding 1 goes in line with findings from stud-ies analyzing social networks that superiors havehigher connectivity in the networks that they arepart of (Rowe et al, 2007).
Intuitively, those whohave higher connectivity also send emails to largernumber of people, and hence our result.
Since su-periors address more people in their emails, theyalso have a higher chance of getting replies.
Find-ing 2 also aligns with the general intuition abouthow superiors and subordinates behave within in-teractions (e.g., superiors exhibit more overt dis-plays of power than subordinates).Findings 3 & 4 are interesting since they re-veal special characteristics of threads involving hi-erarchically related participants.
In (Prabhakaranand Rambow, 2013), we had found that personswith hierarchical power rarely initiated threadsand contributed less within the threads.
But thatproblem formulation was different ?
we wereidentifying whether a person in a given thread hadhierarchical power over someone else or not.
Thedata points in that formulation included partici-pants from threads that did not have any hierar-chically related people, whereas our current for-mulation do not.
These findings suggest that if aperson starts an email thread, he?s likely not to bethe one who has power, but if a thread includes apair of people who are hierarchically related, thenit is likely to be initiated by the superior and he/shetends to contribute more in such threads.6 Predicting Direction of PowerWe build an SVM-based supervised learning sys-tem that can predict HP(p1, p2) to be either su-perior or subordinate based on the interactionwithin a thread t for any pair of participants(p1, p2) ?
RIPPt.
We deterministically fix theorder of participants in (p1, p2) such that p1is thesender of the first message in IMt(p1, p2).
Weuse the ClearTK (Ogren et al, 2008) wrapper forSVMLight (Joachims, 1999) in our experiments.We use the related interacting participant pairs inthreads from the train set to train our models andoptimize our performance on those from the devset.
We report results obtained on dev and test sets.In our formulation, values of many features areundefined for some instances (e.g., Inform% is un-defined when MsgCount = 0).
Handling of unde-fined values for features in SVM is not straight-forward.
Most SVM implementations assume thevalue of 0 by default in such cases, conflating them342Description AccuracyBaseline (Always Superior) 52.54Baseline (Word Unigrams + Bigrams) 68.56THRNew55.90THRPR54.30DIAPR54.05THRPR+ THRNew61.49DIAPR+ THRPR+ THRNew62.47LEX 70.74LEX + DIAPR+ THRPR67.44LEX + DIAPR+ THRPR+ THRNew68.56BEST (= LEX + THRNew) 73.03BEST (Using p1features only) 72.08BEST (Using IMtfeatures only) 72.11BEST (Using Mtonly) 71.27BEST (No Indicator Variables) 72.44Table 3: Accuracies on feature subsets (dev set).THRNew: new meta-data features; THRPR, DIAPR: meta-dataand dialog-act features from previous studies; LEX: ngrams;BEST: best subset; IMtstands for IMt(p1,p2)with cases where Inform% is truly 0.
In order tomitigate this issue, we use an indicator feature foreach structural feature to denote whether or not itis valid.
Since we use a quadratic kernel, we ex-pect the SVM to pick up the interaction betweeneach feature and its indicator feature.Lexical features have already been shown to bevaluable in predicting power relations (Bramsenet al, 2011; Gilbert, 2012).
We use another fea-ture set LEX to capture word ngrams, POS (partof speech) ngrams and mixed ngrams.
A mixedngram (Prabhakaran et al, 2012) is a special caseof word ngram where words belonging to openclasses are replaced with their POS tags.
We foundthe best setting to be using both unigrams and bi-grams for all three types of ngrams, by tuning inour dev set.
We then performed experiments usingall subsets of {LEX, THRNew, THRPR, DIAPR}.Table 3 presents the results obtained using var-ious feature subsets.
We use a majority classbaseline assigning HP(p1, p2) to be always su-perior, which obtains 52.5% accuracy.
We alsouse a stronger baseline using word unigrams andbigrams as features, which obtained an accuracyof 68.6%.
The performance of the system usingeach structural feature class on its own is verylow.
Combining all three of them improves theaccuracy to 62.5%.
The highest performance ob-tained without using any message content is forTHRPRand THRNew(61.5%).
LEX features byitself obtain a very high accuracy of 70.7%, con-firming the importance of lexical patterns in thistask.
Perplexingly, adding all structural features toLEX reduces the accuracy by around 2.2 percent-age points.
The best performing system (BEST)uses LEX and THRNewfeatures and obtains anaccuracy of 73.0%, a statistically significant im-provement over the LEX-only system (McNemar).We also performed an ablation study to under-stand the importance of different slices of our fea-ture sets.
If we remove all feature versions withrespect to the second person, the accuracy dropsto 72.1%.
This suggests that features about theother person?s behavior also help the predictiontask.
If we remove either the thread level versionsof features or interaction level versions of features,the accuracy again drops, suggesting that both thepair?s behavior among themselves, and their over-all behavior in the thread add value to the predic-tion task.
Removing the indicator feature denot-ing the structural features?
validity also reducesthe performance of the system.We now discuss evaluation on our blind test set.The majority baseline (Always Superior) for ac-curacy is 55.0%.
The word unigrams and bigramsbaseline obtains an accuracy of 68.3%.
The LEXsystem (using other forms of ngrams as well) ob-tains a slightly lower accuracy of 68.1%.
OurBEST system using LEX and THRNewfeaturesobtains an accuracy of 73.0% (coincidentally thesame as on the dev set), an improvement of 6.9%over the system using only lexical features.7 ConclusionWe introduced the problem of predicting who haspower over whom based on a single thread of writ-ten interactions.
We introduced a new set of fea-tures which describe the structure of the dialog.Using this feature set, we obtain an accuracy of73.0% on a blind test.
In future work, we willtackle the problem of three-way classification ofpairs of participants, which will cover cases inwhich they are not in a power relation at all.AcknowledgmentsThis paper is based upon work supported by theDARPA DEFT Program.
The views expressed arethose of the authors and do not reflect the officialpolicy or position of the Department of Defenseor the U.S. Government.
We also thank severalanonymous reviewers for their feedback.343ReferencesApoorv Agarwal, Adinoyi Omuya, Aaron Harnly, andOwen Rambow.
2012.
A Comprehensive GoldStandard for the Enron Organizational Hierarchy.
InProceedings of the 50th Annual Meeting of the ACL(Short Papers), pages 161?165, Jeju Island, Korea,July.
Association for Computational Linguistics.Yoav Benjamini and Yosef Hochberg.
1995.
Control-ling the false discovery rate: a practical and pow-erful approach to multiple testing.
Journal of theRoyal Statistical Society.
Series B (Methodological),pages 289?300.Or Biran, Sara Rosenthal, Jacob Andreas, KathleenMcKeown, and Owen Rambow.
2012.
Detectinginfluencers in written online conversations.
In Pro-ceedings of the Second Workshop on Language inSocial Media, pages 37?45, Montr?eal, Canada, June.Association for Computational Linguistics.Philip Bramsen, Martha Escobar-Molano, Ami Patel,and Rafael Alonso.
2011.
Extracting social powerrelationships from natural language.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 773?782, Portland, Oregon, USA,June.
Association for Computational Linguistics.Cristian Danescu-Niculescu-Mizil, Lillian Lee,Bo Pang, and Jon Kleinberg.
2012.
Echoes ofpower: language effects and power differences insocial interaction.
In Proceedings of the 21st in-ternational conference on World Wide Web, WWW?12, New York, NY, USA.
ACM.Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,Dan Jurafsky, Jure Leskovec, and Christopher Potts.2013.
A computational approach to politeness withapplication to social factors.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages250?259, Sofia, Bulgaria, August.
Association forComputational Linguistics.Eric Gilbert.
2012.
Phrases that signal workplace hier-archy.
In Proceedings of the ACM 2012 conferenceon Computer Supported Cooperative Work, CSCW?12, pages 1037?1046, New York, NY, USA.
ACM.Thorsten Joachims.
1999.
Making Large-Scale SVMLearning Practical.
In Bernhard Sch?olkopf, Christo-pher J.C. Burges, and A. Smola, editors, Advancesin Kernel Methods - Support Vector Learning, Cam-bridge, MA, USA.
MIT Press.Viet-An Nguyen, Jordan Boyd-Graber, Philip Resnik,Deborah A. Cai, Jennifer E. Midberry, and YuanxinWang.
2013.
Modeling topic control to detect in-fluence in conversations using nonparametric topicmodels.
Machine Learning, pages 1?41.Philip V. Ogren, Philipp G. Wetzler, and StevenBethard.
2008.
ClearTK: A UIMA toolkit for sta-tistical natural language processing.
In TowardsEnhanced Interoperability for Large HLT Systems:UIMA for NLP workshop at Language Resourcesand Evaluation Conference (LREC).Adinoyi Omuya, Vinodkumar Prabhakaran, and OwenRambow.
2013.
Improving the quality of minor-ity class identification in dialog act tagging.
In Pro-ceedings of the 2013 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages802?807, Atlanta, Georgia, June.
Association forComputational Linguistics.Vinodkumar Prabhakaran and Owen Rambow.
2013.Written dialog and social power: Manifestations ofdifferent types of power in dialog behavior.
In Pro-ceedings of the IJCNLP, pages 216?224, Nagoya,Japan, October.
Asian Federation of Natural Lan-guage Processing.Vinodkumar Prabhakaran, Owen Rambow, and MonaDiab.
2012.
Predicting Overt Display of Power inWritten Dialogs.
In Human Language Technolo-gies: The 2012 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, Montreal, Canada, June.
Associ-ation for Computational Linguistics.Vinodkumar Prabhakaran, Ajita John, and Dor?ee D.Seligmann.
2013. Who had the upper hand?
rank-ing participants of interactions based on their rela-tive power.
In Proceedings of the IJCNLP, pages365?373, Nagoya, Japan, October.
Asian Federationof Natural Language Processing.Ryan Rowe, German Creamer, Shlomo Hershkop, andSalvatore J. Stolfo.
2007.
Automated social hier-archy detection through email network analysis.
InProceedings of the 9th WebKDD and 1st SNA-KDD2007 workshop on Web Mining and Social NetworkAnal.
ACM.Tomek Strzalkowski, Samira Shaikh, Ting Liu,George Aaron Broadwell, Jenny Stromer-Galley,Sarah Taylor, Umit Boz, Veena Ravishankar, andXiaoai Ren.
2012.
Modeling leadership and influ-ence in multi-party online discourse.
In Proceedingsof COLING, pages 2535?2552, Mumbai, India, De-cember.
The COLING 2012 Organizing Committee.Jen-Yuan Yeh and Aaron Harnly.
2006.
Email threadreassembly using similarity matching.
In CEAS2006 - The Third Conference on Email and Anti-Spam, July 27-28, 2006, Mountain View, California,USA, Mountain View, California, USA, July.344
