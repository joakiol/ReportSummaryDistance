Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 392?402,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsEvent Detection and Co-reference with Minimal SupervisionHaoruo Peng1 and Yangqiu Song2 and Dan Roth11University of Illinois, Urbana-Champaign2Department of Computer Science and Engineering,Hong Kong University of Science and Technology1{hpeng7,danr}@illinois.edu, 2yqsong@cse.ust.hkAbstractAn important aspect of natural language un-derstanding involves recognizing and catego-rizing events and the relations among them.However, these tasks are quite subtle and an-notating training data for machine learningbased approaches is an expensive task, re-sulting in supervised systems that attempt tolearn complex models from small amounts ofdata, which they over-fit.
This paper addressesthis challenge by developing an event detec-tion and co-reference system with minimal su-pervision, in the form of a few event exam-ples.
We view these tasks as semantic similar-ity problems between event mentions or eventmentions and an ontology of types, thus fa-cilitating the use of large amounts of out ofdomain text data.
Notably, our semantic re-latedness function exploits the structure of thetext by making use of a semantic-role-labelingbased representation of an event.We show that our approach to event detectionis competitive with the top supervised meth-ods.
More significantly, we outperform state-of-the-art supervised methods for event co-reference on benchmark data sets, and supportsignificantly better transfer across domains.1 IntroductionNatural language understanding involves, as a keycomponent, the need to understand events men-tioned in texts.
This entails recognizing elementssuch as agents, patients, actions, location and time,among others.
Understanding events also necessi-tates understanding relations among them and, asa minimum, determining whether two snippets oftext represent the same event or not ?
the event co-reference problem.
Events have been studied foryears, but they still remain a key challenge.
Onereason is that the frame-based structure of events ne-cessitates addressing multiple coupled problems thatare not easy to study in isolation.
Perhaps an evenmore fundamental difficulty is that it is not clearwhether our current set of events?
definitions is ade-quate (Hovy et al, 2013).
Thus, given the complex-ity and fundamental difficulties, the current evalua-tion methodology in this area focuses on a limiteddomain of events, e.g.
33 types in ACE 2005 (NIST,2005) and 38 types in TAC KBP (Mitamura et al,2015).
Consequently, this allows researchers to trainsupervised systems that are tailored to these sets ofevents and that overfit the small domain covered inthe annotated data, rather than address the realisticproblem of understanding events in text.In this paper, we pursue an approach to under-standing events that we believe to be more feasi-ble and scalable.
Fundamentally, event detectionis about identifying whether an event in context issemantically related to a set of events of a specifictype; and, event co-reference is about whether twoevent mentions are semantically similar enough toindicate that the author intends to refer to the samething.
Therefore, if we formulate event detectionand co-reference as semantic relatedness problems,we can scale it to deal with a lot more types and, po-tentially, generalize across domains.
Moreover, bydoing so, we facilitate the use of a lot of data that isnot part of the existing annotated event collectionsand not even from the same domain.
The key chal-392Supervised Unsupervised MSEPGuideline !
!
!In-domain Data !
!
7Data Annotation !
7 7Table 1: Comparing requirements of MSEP andother methods.
Supervised methods need all threeresources while MSEP only needs an annotationguideline (as event examples).lenges we need to address are those of how to repre-sent events, and how to model event similarity; bothare difficult partly since events have structure.We present a general event detection and co-reference framework, which essentially requires nolabeled data.
In practice, in order to map an eventmention to an event ontology, as a way to commu-nicate with a user, we just need a few event exam-ples, in plain text, for each type a user wants to ex-tract.
This is a reasonable setting; after all, givingexamples is the easiest way of defining event types,and is also how information needs are defined toannotators - by providing examples in the annota-tion guideline.1 Our approach makes less assump-tions than standard unsupervised methods, whichtypically require a collection of instances and ex-ploit similarities among them to eventually learn amodel.
Here, given event type definitions (in theform of a few examples), we can classify a sin-gle event into a provided ontology and determinewhether two events are co-referent.
In this sense, ourapproach is similar to what has been called datalessclassification (Chang et al, 2008; Song and Roth,2014).
Table 1 summarizes the difference betweenour approach, MSEP (Minimally Supervised EventPipeline)2, and other methods.Our approach builds on two key ideas.
First,to represent event structures, we use the generalpurpose nominal and verbial semantic role label-ing (SRL) representation.
This allows us to de-velop a structured representation of an event.
Sec-ond, we embed event components, while maintain-ing the structure, into multiple semantic spaces, in-1Event examples also serve for disambiguation purposes.For example, using ?U.S.
forces bombed Baghdad.?
to exem-plify an attack type, disambiguates it from a heart attack.2Available at http://cogcomp.cs.illinois.edu/page/downloadview/eventPipeline .Figure 1: An overview of the end-to-end MSEP sys-tem.
?Event Examples?
are the only supervisionhere, which produce ?Example Vectors?.
No train-ing is needed for MSEP.duced at a contextual, topical, and syntactic levels.These semantic representations are induced fromlarge amounts of text in a way that is completely in-dependent of the tasks at hand, and are used to repre-sent both event mentions and event types into whichwe classify our events.
The combination of these se-mantic spaces, along with the structured vector rep-resentation of an event, allow us to directly deter-mine whether a candidate event mention is a validevent or not and, if it is, of which type.
Moreover,with the same representation, we can evaluate eventsimilarities and decide whether two event mentionsare co-referent.
Consequently, the proposed MSEP,can also adapt to new domains without any training.An overview of the system is shown in Figure 1.A few event examples are all the supervision MSEPneeds; even the few decision thresholds needed to beset are determined on these examples, once and forall, and are used for all test cases we evaluate on.We use two benchmark datasets to compare MSEPwith baselines and supervised systems.
We showthat MSEP performs favorably relative to state-of-the-art supervised systems; the co-reference mod-ule, in fact, outperforms supervised approaches onB3 and CEAF metrics.
The superiority of MSEP isalso demonstrated in across domain settings.2 The MSEP System2.1 Structured Vector RepresentationThere is a parallel between event structures and sen-tence structures.
Event triggers are mostly pred-icates of sentences or clauses.
Predicates can besense disambiguated, which roughly corresponds to393Figure 2: Basic event vector representation.
Event vector is the concatenation of vectors corresponding toaction, agentsub, agentobj , location, time and sentence/clause.Figure 3: Augmented event vector representation.
Event vector is the concatenation of vectors corre-sponding to basic event vector representation, agentsub + action, agentobj + action, location + action andtime + action.
Here, ?+?
means that we first put text fragments together and then convert the combined textfragment into an ESA vector.event types.
Event arguments are largely entity men-tions or temporal/spatial arguments.
They serve asspecific roles in events, similarly to SRL argumentsthat are assigned role labels for predicates.We use the Illinois SRL (Punyakanok et al, 2004)tool to pre-process the text.
We evaluate the SRLcoverage on both event triggers and event argu-ments, shown in Table 2.3 For event triggers, weonly focus on recall since we expect the event men-tion detection module to filter out most non-triggerpredicates.
Results show a good coverage of SRLpredicates and arguments on event triggers and argu-ments.
Even though we only get approximate eventarguments, it is easier and more reliable to catego-rize them into five abstract roles, than to determinethe exact role label with respect to event triggers.We identify the five most important and ab-stract event semantic components: action, agentsub,agentobj , location and time.
To map SRL argu-ments to these event arguments, we run through thefollowing procedures: 1) set predicates as actions,and preserve SRL negations for actions, 2) set SRLsubject as agentsub, 3) set SRL object and indirectobject as agentobj , 4) set SRL spatial argument asevent location.
If there is no such SRL label, wethen scan for any NER location label within the sen-tence/clause to which the action belongs.
We setthe location according to NER information if it ex-3We place events in two categories, verb or noun, accordingto the part-of-speech tag of the trigger.
We evaluate verb-SRLon events with verb triggers, nom-SRL on events with noun trig-gers, and the overall performance on all events.
When evaluat-ing, we allow partial overlaps.ACE Precision Recall F1Predicates Verb-SRL ?
93.2 ?over Nom-SRL ?
87.5 ?Triggers All ?
91.9 ?SRL Args Verb-SRL 90.4 85.7 88.0over Nom-SRL 92.5 73.5 81.9Event Args All 90.9 82.3 86.4TAC KBP Precision Recall F1Predicates Verb-SRL ?
90.6 ?over Nom-SRL ?
85.5 ?Triggers All ?
88.1 ?SRL Args Verb-SRL 89.8 83.6 86.6over Nom-SRL 88.2 69.9 78.0Event Args All 89.5 81.0 85.0Table 2: Semantic role labeling coverage.
We eval-uate both ?Predicates over Triggers?
and ?SRL Ar-guments over Event Arguments?.
?All?
stands forthe combination of Verb-SRL and Nom-SRL.
Theevaluation is done on all data.ists.
5) We set the SRL temporal argument as eventtime.
If there is no such SRL label, we then usethe Illinois Temporal Expression Extractor (Zhao etal., 2012) to find the temporal argument within anevent?s sentence/clause.
6) We allow one or moremissing event arguments among agentsub, agentobj ,location or time, but require actions to always exist.Given the above structured information, we con-vert each event component to its correspondingvector representation, discussed in detail in Sec-tion 3.
We then concatenate the vectors of all com-ponents together in a specific order: action, agentsub,agentobj , location, time and sentence/clause.
Wetreat the whole sentence/clause, to which the ?ac-394tion?
belongs, as context, and we append its corre-sponding vector to the event representation.
This ba-sic event vector representation is illustrated in Fig.
2.If there are missing event arguments, we set the cor-responding vector to be ?NIL?
(we set each posi-tion as ?NaN?).
We also augment the event vectorrepresentation by concatenating more text fragmentsto enhance the interactions between the action andother arguments, as shown in Fig.
3.
Essentially, weflatten the event structure to preserve the alignmentof event arguments so that the structured informationcan be reflected in our vector space.2.2 Event Mention DetectionMotivated by the seed-based event trigger labelingtechnique employed in Bronstein et al (2015), weturn to ACE annotation guidelines for event exam-ples described under each event type label.
For in-stance, the ACE-2005 guidelines list the example?Mary Smith joined Foo Corp. in June 1998.?
forlabel ?START-POSITION?.
Altogether, we collect172 event examples from 33 event types (5 each onaverage).4 We can then get vector representationsfor these example events following the proceduresin Sec.
2.1.
We define the event type representa-tion as the numerical average of all vector represen-tations corresponding to example events under thattype.
We use the similarity between an event candi-date with the event type representation to determinewhether the candidate belongs to an event type:S(e1, e2) =vec(e1) ?
vec(e2)?vec(e1)?
?
?vec(e2)?=?a vec(a1) ?
vec(a2)?
?a ?vec(a1)?2 ??
?a ?vec(a2)?2,(1)where e1 is the candidate, e2 the type (vec(e2) iscomputed as average of event examples), a1, a2 arecomponents of e1, e2 respectively.
We use the no-tation vec(?)
for corresponding vectors.
Note thatthere may be missing event arguments (NIL).
Insuch cases, we use the average of all non-NIL sim-ilarity scores for that particular component as thecontributed score.
Formally, we define Spair(a =4See supplementary materials for the full list of examples.NIL) and Ssingle(a = NIL) as follows:Spair(a = NIL) = vec(NIL) ?
vec(a2)= vec(a1) ?
vec(NIL)=?a1,a2 6=NILvec(a1) ?
vec(a2)#|a1, a2 6= NIL| ,Ssingle(a = NIL) =?
?a6=NIL ?vec(a)?2#|a 6= NIL| .Thus, when we encounter missing event arguments,we use Spair(a = NIL) to replace the correspond-ing term in the numerator in S(e1, e2) while usingSsingle(a = NIL) in the denominator.
These aver-age contributed scores are corpus independent, andcan be pre-computed ahead of time.
We use a cut-offthreshold to determine that an event does not belongto any event types, and can thus be eliminated.
Thisthreshold is set by tuning only on the set of eventexamples, which is corpus independent.52.3 Event Co-referenceSimilar to the mention-pair model in entity co-reference (Ng and Cardie, 2002; Bengtson and Roth,2008; Stoyanov et al, 2010), we use cosine sim-ilarities computed from pairs of event mentions:S(e1, e2) (as in Eq.
(1)).Before applying the co-reference model, we firstuse external knowledge bases to identify conflictevents.
We use the Illinois Wikification (Chengand Roth, 2013) tool to link event arguments toWikipedia pages.
Using the Wikipedia IDs, we mapevent arguments to Freebase entries.
We view thetop-level Freebase type as the event argument type.An event argument can contain multiple wikified en-tities, leading to multiple Wikipedia pages and thusa set of Freebase types.
We also augment the argu-ment type set with NER labels: PER (person) andORG (organization).
We add either of the NER la-bels if we detect such a named entity.For each pair of events, we check event argumentsagentsub and agentobj respectively.
If none of thetypes for the aligned event arguments match, thispair is determined to be in conflict.
If the event ar-gument is missing, we deem it compatible with anytype.
In this procedure, we generate a set of eventpairs Setconflict that will not get co-reference links.5See Sec.
4.4 for details.395Given the event mention similarity as well as theconflicts, we perform event co-reference inferencevia a left-linking greedy algorithm, i.e.
co-referencedecisions are made on each event from left to right,one at a time.
Without loss of generality, for eventek+1,?k ?
1, we first choose a linkable event to itsleft with the highest event-pair similarity:ep = arg maxe?
{e1,e2,...,ek}e6?SetconflictS(e, ek+1).We make co-reference links when S(ep, ek+1) ishigher than a cut-off threshold, which is also tunedonly on event examples ahead of time.
Otherwise,event ek+1 is not similar enough to any of its an-tecedents, and we make it the start of a new cluster.3 Vector RepresentationsWe experiment with different methods to con-vert event components into vector representations.Specifically, we use Explicit Semantic Analysis(ESA), Brown Cluster (BC), Word2Vec (W2V) andDependency-Based Word Embedding (DEP) respec-tively to convert text into vectors.
We then concate-nate all components of an event together to form astructured vector representation.Explicit Semantic Analysis ESA uses Wikipediaas an external knowledge base to generate con-cepts for a given fragment of text (Gabrilovich andMarkovitch, 2009).
ESA first represents a given textfragment as a TF-IDF vector, then uses an invertedindex for each word to search the Wikipedia corpus.The text fragment representation is thus a weightedcombination of the concept vectors corresponding toits words.
We use the same setting as in Chang et al(2008) to filter out pages with fewer than 100 wordsand those containing fewer than 5 hyperlinks.
Tobalance between the effectiveness of ESA represen-tations and its cost, we use the 200 concepts with thehighest weights.
Thus, we convert each text frag-ment to a very sparse vector of millions of dimen-sions (but we just store 200 non-zero values).Brown Cluster BC was proposed by Brown et al(1992) as a way to support abstraction in NLP tasks,measuring words?
distributional similarities.
Thismethod generates a hierarchical tree of word clus-ters by evaluating the word co-occurrence based ona n-gram model.
Then, paths traced from root toleaves can be used as word representations.
We usethe implementation by Song and Roth (2014), gen-erated over the latest Wikipedia dump.
We set themaximum tree depth to 20, and use a combinationof path prefixes of length 4,6 and 10 as our BC rep-resentation.
Thus, we convert each word to a vectorof 24 + 26 + 210 = 1104 dimensions.Word2Vec We use the skip-gram tool by Mikolov etal.
(2013) over the latest Wikipedia dump, resultingin word vectors of dimensionality 200.Dependency-Based Embedding DEP is the gener-alization of the skip-gram model with negative sam-pling to include arbitrary contexts.
In particular, itdeals with dependency-based contexts, and producesmarkedly different embeddings.
DEP exhibits morefunctional similarity than the original skip-gram em-beddings (Levy and Goldberg, 2014).
We directlyuse the released 300-dimension word embeddings6.Note that it is straightforward text-vector conver-sion for ESA.
But for BC, W2V and DEP, we firstremove stop words from the text and then average,element-wise, all remaining word vectors to producethe resulting vector representation of the text frag-ment.4 Experiments4.1 DatasetsACE The ACE-2005 English corpus (NIST, 2005)contains fine-grained event annotations, includingevent trigger, argument, entity, and time-stamp an-notations.
We select 40 documents from newswirearticles for event detection evaluation and the restfor training (same as Chen et al (2015)).
We do 10-fold cross-validation for event co-reference.TAC-KBP The TAC-KBP-2015 corpus is annotatedwith event nuggets that fall into 38 types and co-reference relations between events.
7 We use thetrain/test data split provided by the official TAC-6https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings7The event ontology of TAC-KBP (based on ERE annota-tion) is almost the same to that of ACE.
To adapt our sys-tem to the TAC-KBP corpus, we use all ACE event seeds of?Contact.Phone-Write?
for ?Contact.Correspondence?
and sep-arate ACE event seeds of ?Movement.Transport?
into ?Move-ment.TransportPerson?
and ?Movement.TransportArtifact?
bymanual checking.
So, we use exactly the same set of event seedsfor TAC-KBP with only these two changes.396#Doc #Sent.
#Men.
#ClusterACE(All) 599 15,494 5,268 4,046ACE(Test) 40 672 289 222TAC-KBP(All) 360 15,824 12,976 7,415TAC-KBP(Test) 202 8,851 6,438 3,779Table 3: Statistics for the ACE and TAC-KBP cor-pora.
#Sent.
is the number of sentences, #Men.is the number of event mentions, and #Cluster isthe number of event clusters (including singletons).Note that the proposed MSEP does not need anytraining data.2015 Event Nugget Evaluation Task.Statistics for the ACE and TAC-KBP corpora isshown in Table 3.
Note that the training set andcross-validation is only for competing supervisedmethods.
For MSEP, we only need to run on eachcorpus once for testing.4.2 Compared SystemsFor event detection, we compare with DM-CNN (Chen et al, 2015), the state-of-art super-vised event detection system.
We also implementanother supervised model, named supervised struc-tured event detection SSED system following thework of Sammons et al (2015).
The system utilizesrich semantic features and applies a trigger identifi-cation classifier on every SRL predicate to determinethe event type.
For event co-reference, Joint (Chenet al, 2009) is an early work based on super-vised learning.
We also report HDP-Coref resultsas an unsupervised baseline (Bejan and Harabagiu,2010), which utilizes nonparametric Bayesian mod-els.
Moreover, we create another unsupervised eventco-reference baseline (Type+SharedMen): we treatevents of the same type which share at least oneco-referent entity (inside event arguments) as co-referred.
On TAC-KBP corpus, we report resultsfrom the top ranking system of the TAC-2015 EventNugget Evaluation Task as TAC-TOP.We name our event mention detection modulein MSEP similarity-based event mention detec-tion MSEP-EMD system.
For event co-reference,the proposed similarity based co-reference detec-tion MSEP-Coref method has a number of varia-tions depending on the modular text-vector conver-sion method (ESA, BC, W2V, DEP), whether weuse augmented ESA vector representation (AUG)8,and whether we use knowledge during co-referenceinference (KNOW).
We also develop a super-vised event co-reference system following the workof Sammons et al (2015), namely SupervisedBase.We also add additional event vector representa-tions9 as features to this supervised system and getSupervisedExtend.4.3 Evaluation MetricsFor event detection, we use standard precision, re-call and F1 metrics.
For event co-reference, wecompare all systems using standard F1 metrics:MUC (Vilain et al, 1995), B3 (Bagga and Baldwin,1998), Entity-based CEAF (CEAFe) (Luo, 2005)and BLANC (Recasens and Hovy, 2011).
We usethe average scores (AVG) of these four metrics asthe main comparison metric.104.4 Results for Event detectionThe performance comparison for event detection ispresented in Table 4.
On both ACE and TAC-KBP, parameters of SSED are tuned on a develop-ment set (20% of randomly sampled training doc-uments).
The cut-off threshold for MSEP-EMDis tuned on the 172 event examples ahead of timeby optimizing the F1 score on the event seed ex-amples.
Note that different text-vector conversionmethods lead to different cut-off thresholds, but theyremain fixed for all the test corpus.
Results showthat SSED achieves state-of-the-art performance.Though MSEP-EMD?s performance is below thebest supervised system, it is very competitive.
Notethat both SSED and MSEP-EMD use SRL predi-cates as input and thus can further improve with abetter SRL module.4.5 Results for Event Co-referenceThe performance of different systems for event co-reference based on gold event triggers is shown inTable 5.
The co-reference cut-off threshold is tunedby optimizing the CoNLL average score on ten se-8It is only designed for ESA because the ESA vector fortwo concatenated text fragments is different from the sum of theESA vectors of individual text fragments, unlike other methods.9We add the best event vector representation empirically.10We use the latest scorer (v1.7) provided by TAC-2015Event Nugget Evaluation for all metrics.397ACE (Test Data) Precision Recall F1SpanDMCNN 80.4 67.7 73.5SSED 76.6 71.5 74.0MSEP-EMD 75.6 69.8 72.6Span+TypeDMCNN 75.6 63.6 69.1SSED 71.3 66.5 68.8MSEP-EMD 70.4 65.0 67.6TAC-KBP (Test Data) Precision Recall F1SpanSSED 77.2 55.9 64.8TAC-TOP ?
?
65.3MSEP-EMD 76.5 54.5 63.5Span+TypeSSED 69.9 48.8 57.5TAC-TOP ?
?
58.4MSEP-EMD 69.2 47.8 56.6Table 4: Event detection (trigger identification)results.
?Span?/?Type?
means span/type match re-spectively.lected ACE documents.
The threshold is then fixed,thus we do not change it when evaluating on theTAC-KBP corpus.
As we do cross-validation onACE, we exclude these ten documents from test atall times.11 Results show that the proposed MSEPevent co-reference system significantly outperformsbaselines and achieves the same level of perfor-mance of supervised methods (82.9 v.s.
83.3 onACE and 73.8 v.s.
74.4 on TAC-KBP).
MSEPachieves better results on B3 and CEAFe than su-pervised methods.
Note that supervised methodsusually generate millions of features (2.5M on ACEand 1.8M on TAC-KBP for SupervisedBase).
In con-trast, MSEP only has several thousands of non-zerodimensions in event representations.
This meansthat our structured vector representations, throughderived without explicit annotations, are far moreexpressive than traditional features.
When we addthe event vector representation (augmented ESA) asfeatures in SupervisedExtend, we improve the overallperformance by more than 1 point.
When tested in-dividually, DEP performs the best among the fourtext-vector conversion methods while BC performsthe worst.
A likely reason is that BC has too few di-11We regard this tuning procedure as ?independent?
and?ahead of time?
because of the following reasons: 1) We couldhave used as threshold-tuning co-reference examples a fewnews documents from other sources; we just use ACE doc-uments as a data source for simplicity.
2) We believe thatthe threshold only depends on event representation (the model)rather than data.
3) Tuning a single decision threshold is muchcheaper than tuning a whole set of model parameters.mensions while DEP constructs the longest vector.However, the results show that our augmented ESArepresentation (Fig.
2) achieves even better results.When we use knowledge to detect conflictingevents during inference, the system further im-proves.
Note that event arguments for the proposedMSEP are predicted by SRL.
We show that replac-ing them with gold event arguments, only slightlyimproves the overall performance, indicating thatSRL arguments are robust enough for the event co-reference task.4.6 End-to-End Event Co-reference ResultsTable 6 shows the performance comparison for end-to-end event co-reference.
We use both SSED andMSEP-EMD as event detection modules and weevaluate on standard co-reference metrics.
Resultson TAC-KBP show that ?SSED+SupervisedExtend?achieves similar performance to the TAC top rankingsystem while the proposed MSEP event co-referencemodule helps to outperform supervised methods onB3 and CEAFe metrics.4.7 Domain Transfer EvaluationTo demonstrate the superiority of the adaptation ca-pabilities of the proposed MSEP system, we test itsperformance on new domains and compare with thesupervised system.
TAC-KBP corpus contains twogenres: newswire (NW) and discussion forum (DF),and they have roughly equal number of documents.When trained on NW and tested on DF, supervisedmethods encounter out-of-domain situations.
How-ever, the MSEP system can adapt well.12 Table 7shows that MSEP outperforms supervised methodsin out-of-domain situations for both tasks.
The dif-ferences are statistically significant with p < 0.05.5 Related WorkEvent detection has been studied mainly in thenewswire domain as the task of detecting event trig-gers and determining event types and arguments.Most earlier work has taken a pipeline approachwhere local classifiers identify triggers first, andthen arguments (Ji and Grishman, 2008; Liao and12Note that the supervised method needs to be re-trained andits parameters re-tuned while MSEP does not need training andits cut-off threshold is fixed ahead of time using event examples.398ACE (Cross-Validation) MUC B3 CEAFe BLANC AVGSupervisedGraph ?
?
84.5 ?
?Joint 74.8 92.2 87.0 ?
?SupervisedBase 73.6 91.6 85.9 82.2 83.3SupervisedExtend 74.9 92.8 87.1 83.8 84.7Unsupervised Type+SharedMen 59.1 83.2 76.0 72.9 72.8HDP-Coref ?
83.8 76.7 ?
?MSEPMSEP-CorefESA 65.9 91.5 85.3 81.8 81.1MSEP-CorefBC 65.0 89.8 83.7 80.9 79.9MSEP-CorefW2V 65.1 90.1 83.6 81.5 80.1MSEP-CorefDEP 65.9 92.3 85.6 81.5 81.3MSEP-CorefESA+AUG 67.4 92.6 86.0 82.6 82.2MSEP-CorefESA+AUG+KNOW 68.0 92.9 87.4 83.2 82.9MSEP-CorefESA+AUG+KNOW (GA) 68.8 92.5 87.7 83.4 83.1TAC-KBP (Test Data) MUC B3 CEAFe BLANC AVGSupervisedTAC-TOP ?
?
?
?
75.7SupervisedBase 63.8 83.8 75.8 74.0 74.4SupervisedExtend 65.3 84.7 76.8 75.1 75.5Unsupervised Type+SharedMen 56.4 77.5 69.6 68.7 68.1MSEPMSEP-CorefESA 57.7 83.9 76.9 72.9 72.9MSEP-CorefBC 56.9 81.8 76.2 71.7 71.7MSEP-CorefW2V 57.2 82.1 75.9 72.3 71.9MSEP-CorefDEP 58.2 83.3 76.7 72.8 72.8MSEP-CorefESA+AUG 59.0 84.5 77.3 72.5 73.3MSEP-CorefESA+AUG+KNOW 59.9 84.9 77.3 73.1 73.8MSEP-CorefESA+AUG+KNOW (GA) 60.5 84.0 77.7 73.5 73.9Table 5: Event Co-reference Results on Gold Event Triggers.
?MSEP-CorefESA,BC,W2V,DEP?
are varia-tions of the proposed MSEP event co-reference system using ESA, Brown Cluster, Word2Vec and Depen-dency Embedding representations respectively.
?MSEP-CorefESA+AUG?
uses augmented ESA event vec-tor representation and ?MSEP-CorefESA+AUG+KNOW?
applies knowledge to detect conflicting events.
(GA)means that we use gold event arguments instead of approximated ones from SRL.Grishman, 2010; Hong et al, 2011; Huang andRiloff, 2012a; Huang and Riloff, 2012b).
Li etal.
(2013) presented a structured perceptron modelto detect triggers and arguments jointly.
Attemptshave also been made to use a Distributional Seman-tic Model (DSM) to represent events (Goyal et al,2013).
A shortcoming of DSMs is that they ignorethe structure within the context, thus reducing thedistribution to a bag of words.
In our work, we pre-serve event structure via structured vector represen-tations constructed from event components.Event co-reference is much less studied in com-parison to the large body of work on entity co-reference.
Our work follows the event co-referencedefinition in Hovy et al (2013).
All previous workon event co-reference except Cybulska and Vossen(2012) deals only with full co-reference.
Earlyworks (Humphreys et al, 1997; Bagga and Baldwin,1999) performed event co-reference on scenario spe-cific events.
Both Naughton (2009) and Elkhlifi andFaiz (2009) worked on sentence-level co-reference,which is closer to the definition of Danlos and Gaiffe(2003).
Pradhan et al (2007) dealt with both entityand event coreference by taking a three-layer ap-proach.
Chen and Ji (2009) proposed a clusteringalgorithm using a maximum entropy model with arange of features.
Bejan and Harabagiu (2010) builta class of nonparametric Bayesian models using a(potentially infinite) number of features to resolveboth within and cross document event co-reference.Lee et al (2012) formed a system with determinis-tic layers to make co-reference decisions iterativelywhile jointly resolving entity and event co-reference.More recently, Hovy et al (2013) presented an un-supervised model to capture semantic relations andco-reference resolution, but they did not show quan-titatively how well their system performed in each ofthese two cases.
Huang et al (2016) also considered399ACE (Cross-Validation) MUC B3 CEAFe BLANC AVGSSED + SupervisedExtend 47.1 59.9 58.7 44.4 52.5SSED + MSEP-CorefESA+AUG+KNOW 42.1 60.3 59.0 44.1 51.4MSEP-EMD + MSEP-CorefESA+AUG+KNOW 40.2 58.6 57.4 43.8 50.0TAC-KBP (Test Data) MUC B3 CEAFe BLANC AVGTAC-TOP ?
?
?
?
39.1SSED + SupervisedExtend 34.9 44.2 39.6 37.1 39.0SSED + MSEP-CorefESA+AUG+KNOW 33.1 44.6 39.7 36.8 38.5MSEP-EMD + MSEP-CorefESA+AUG+KNOW 30.2 43.9 38.7 35.7 37.1Table 6: Event Co-reference End-To-End Results.Train Test MSEP SupervisedEvent Detection Span+Type F1In Domain NW NW 58.5 63.7Out of Domain DF NW 55.1 54.8In Domain DF DF 57.9 62.6Out of Domain NW DF 52.8 52.3Event Co-reference AVG F1In Domain NW NW 73.2 73.6Out of Domain DF NW 71.0 70.1In Domain DF DF 68.6 68.9Out of Domain NW DF 67.9 67.0Table 7: Domain Transfer Results.
We con-duct the evaluation on TAC-KBP corpus with thesplit of newswire (NW) and discussion form (DF)documents.
Here, we choose MSEP-EMD andMSEP-CorefESA+AUG+KNOW as the MSEP approachfor event detection and co-reference respectively.We use SSED and SupervisedBase as the supervisedmodules for comparison.
For event detection, wecompare F1 scores of span plus type match while wereport the average F1 scores for event co-reference.the problem of event clustering.
They representedevent structures based on AMR (Abstract MeaningRepresentation) and distributional semantics, andfurther generated event schemas composing eventtriggers and argument roles.
Recently, TAC has or-ganized Event Nugget Detection and Co-referenceEvaluations, resulting in interesting works, some ofwhich contributed to our comparisons (Liu et al,2015; Mitamura et al, 2015; Hsi et al, 2015; Sam-mons et al, 2015).6 ConclusionThis paper proposes a novel event detection andco-reference approach with minimal supervision,addressing some of the key issues slowing downprogress in research on events, including the dif-ficulty to annotate events and their relations.
Atthe heart of our approach is the design of struc-tured vector representations for events which, as weshow, supports a good level of generalization withinand across domains.
The resulting approach outper-forms state-of-art supervised methods on some ofthe key metrics, and adapts significantly better toa new domain.
One of the key research directionsis to extend this unsupervised approach to a rangeof other relations among events, including temporaland causality relations, as is (Do et al, 2011; Do etal., 2012).AcknowledgmentsThe authors would like to thank Eric Horn forcomments that helped to improve this work.
Thismaterial is based on research sponsored by theUS Defense Advanced Research Projects Agency(DARPA) under agreements FA8750-13-2-000 andHR0011-15-2-0025.
The U.S. Government is autho-rized to reproduce and distribute reprints for Gov-ernmental purposes notwithstanding any copyrightnotation thereon.
The views and conclusions con-tained herein are those of the authors and shouldnot be interpreted as necessarily representing the of-ficial policies or endorsements, either expressed orimplied, of DARPA or the U.S. Government.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In MUC-7.A.
Bagga and B. Baldwin.
1999.
Cross-document eventcoreference: Annotations, experiments, and observa-tions.
In Proceedings of the Workshop on Coreferenceand its Applications.C.
A. Bejan and S. Harabagiu.
2010.
Unsupervised event400coreference resolution with rich linguistic features.
InACL.E.
Bengtson and D. Roth.
2008.
Understanding the valueof features for coreference resolution.
In EMNLP.O.
Bronstein, I. Dagan, Q. Li, H. Ji, and A. Frank.
2015.Seed-based event trigger labeling: How far can eventdescriptions get us?
In ACL.P.
Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-cer.
1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics.M.
Chang, L. Ratinov, D. Roth, and V. Srikumar.
2008.Importance of semantic represenation: Dataless clas-sification.
In AAAI.Z.
Chen and H. Ji.
2009.
Graph-based event coreferenceresolution.
In Proceedings of the Workshop on Graph-based Methods for Natural Language Processing.Z.
Chen, H. Ji, and R. Haralick.
2009.
A pairwise eventcoreference model, feature impact and evaluation forevent coreference resolution.
In Proceedings of theWorkshop on Events in Emerging Text Types.Y.
Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao.
2015.Event extraction via dynamic multi-pooling convolu-tional neural networks.
In ACL.X.
Cheng and D. Roth.
2013.
Relational inference forwikification.
In EMNLP.A.
Cybulska and P. Vossen.
2012.
Using semantic rela-tions to solve event coreference in text.
In Proceedingsof the Workshop on Semantic relations.L.
Danlos and B. Gaiffe.
2003.
Event coreference anddiscourse relations.
Philosophical Studies Series.Q.
Do, Y. S. Chan, and D. Roth.
2011.
Minimally super-vised event causality extraction.
In EMNLP.Q.
Do, W. Lu, and D. Roth.
2012.
Joint inference forevent timeline construction.
In EMNLP.A.
Elkhlifi and R. Faiz.
2009.
Automatic annotation ap-proach of events in news articles.
International Jour-nal of Computing & Information Sciences.Evgeniy Gabrilovich and Shaul Markovitch.
2009.Wikipedia-based semantic interpretation for naturallanguage processing.
J. Artif.
Int.
Res., 34(1):443?498, March.K.
Goyal, S. K. Jauhar, H. Li, M. Sachan, S. Srivastava,and E. Hovy.
2013.
A structured distributional seman-tic model for event co-reference.
In ACL.Y.
Hong, J. Zhang, B. Ma, J. Yao, G. Zhou, and Q. Zhu.2011.
Using cross-entity inference to improve eventextraction.
In ACL.E.
Hovy, T.o Mitamura, F. Verdejo, J. Araki, andA.
Philpot.
2013.
Events are not simple: Identity,non-identity, and quasi-identity.
In NAACL-HLT.A.
Hsi, J. Carbonell, and Y. Yang.
2015.
Modeling eventextraction via multilingual data sources.
In TAC.R.
Huang and E. Riloff.
2012a.
Bootstrapped training ofevent extraction classifiers.
In EACL.R.
Huang and E. Riloff.
2012b.
Modeling textual cohe-sion for event extraction.
In AAAI.L.
Huang, T. Cassidy, X. Feng, H. Ji, C. R. Voss, J. Han,and A. Sil.
2016.
Liberal event extraction and eventschema induction.
In ACL.K.
Humphreys, R. Gaizauskas, and S. Azzam.
1997.Event coreference for information extraction.
In Pro-ceedings of Workshop on Operational Factors in Prac-tical, Robust Anaphora Resolution for UnrestrictedTexts.H.
Ji and R. Grishman.
2008.
Refining event extractionthrough cross-document inference.
In ACL.H.
Lee, M. Recasens, A. Chang, M. Surdeanu, and D. Ju-rafsky.
2012.
Joint entity and event coreference reso-lution across documents.
In EMNLP.O.
Levy and Y. Goldberg.
2014.
Dependencybased wordembeddings.
In ACL.Q.
Li, H. Ji, and L. Huang.
2013.
Joint event extractionvia structured prediction with global features.
In ACL.S.
Liao and R. Grishman.
2010.
Using document levelcross-event inference to improve event extraction.
InACL.Z.
Liu, T. Mitamura, and E. Hovy.
2015.
Evaluation al-gorithms for event nugget detection: A pilot study.
InProceedings of the Workshop on Events at the NAACL-HLT.X.
Luo.
2005.
On coreference resolution performancemetrics.
In EMNLP.T.
Mikolov, W. Yih, and G. Zweig.
2013.
Linguisticregularities in continuous space word representations.In NAACL.T.
Mitamura, Y. Yamakawa, S. Holm, Z.
Song, A. Bies,S.
Kulick, and S. Strassel.
2015.
Event nugget an-notation: Processes and issues.
In Proceedings of theWorkshop on Events at NAACL-HLT.M.
Naughton.
2009.
Sentence Level Event Detection andCoreference Resolution.
Ph.D. thesis, National Uni-versity of Ireland, Dublin.V.
Ng and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In ACL.NIST.
2005.
The ACE evaluation plan.S.
Pradhan, L. Ramshaw, R. Weischedel, J. MacBride,and L. Micciulla.
2007.
Unrestricted coreference:Identifying entities and events in ontonotes.
In ICSC.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Semantic role labeling via integer linear programminginference.
In COLING.M.
Recasens and E. Hovy.
2011.
Blanc: Implement-ing the rand index for coreference evaluation.
NaturalLanguage Engineering, 17(04):485?510.401M.
Sammons, H. Peng, Y.
Song, S. Upadhyay, C.-T. Tsai,P.
Reddy, S. Roy, and D. Roth.
2015.
Illinois ccg tac2015 event nugget, entity discovery and linking, andslot filler validation systems.
In TAC.Y.
Song and D. Roth.
2014.
On dataless hierarchical textclassification.
In AAAI.V.
Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,and D. Hysom.
2010.
Coreference resolution withreconcile.
In ACL.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In Proceedings of the 6th conferenceon Message understanding.R.
Zhao, Q.
Do, and D. Roth.
2012.
A robust shallowtemporal reasoning system.
In North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies (NAACL-HLT Demo).402
