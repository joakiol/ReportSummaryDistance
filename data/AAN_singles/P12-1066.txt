Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 629?637,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSelective Sharing for Multilingual Dependency ParsingTahira NaseemCSAIL, MITtahira@csail.mit.eduRegina BarzilayCSAIL, MITregina@csail.mit.eduAmir GlobersonHebrew Universitygamir@cs.huji.ac.ilAbstractWe present a novel algorithm for multilin-gual dependency parsing that uses annotationsfrom a diverse set of source languages to parsea new unannotated language.
Our motiva-tion is to broaden the advantages of multilin-gual learning to languages that exhibit signif-icant differences from existing resource-richlanguages.
The algorithm learns which as-pects of the source languages are relevant forthe target language and ties model parame-ters accordingly.
The model factorizes theprocess of generating a dependency tree intotwo steps: selection of syntactic dependentsand their ordering.
Being largely language-universal, the selection component is learnedin a supervised fashion from all the traininglanguages.
In contrast, the ordering decisionsare only influenced by languages with simi-lar properties.
We systematically model thiscross-lingual sharing using typological fea-tures.
In our experiments, the model con-sistently outperforms a state-of-the-art multi-lingual parser.
The largest improvement isachieved on the non Indo-European languagesyielding a gain of 14.4%.11 IntroductionCurrent top performing parsing algorithms rely onthe availability of annotated data for learning thesyntactic structure of a language.
Standard ap-proaches for extending these techniques to resource-lean languages either use parallel corpora or rely on1The source code for the work presented in this paper isavailable at http://groups.csail.mit.edu/rbg/code/unidep/annotated trees from other source languages.
Thesetechniques have been shown to work well for lan-guage families with many annotated resources (suchas Indo-European languages).
Unfortunately, formany languages there are no available parallel cor-pora or annotated resources in related languages.For such languages the only remaining option is toresort to unsupervised approaches, which are knownto produce highly inaccurate results.In this paper, we present a new multilingual al-gorithm for dependency parsing.
In contrast to pre-vious approaches, this algorithm can learn depen-dency structures using annotations from a diverseset of source languages, even if this set is not re-lated to the target language.
In our selective shar-ing approach, the algorithm learns which aspects ofthe source languages are relevant for the target lan-guage and ties model parameters accordingly.
Thisapproach is rooted in linguistic theory that charac-terizes the connection between languages at variouslevels of sharing.
Some syntactic properties are uni-versal across languages.
For instance, nouns take ad-jectives and determiners as dependents, but not ad-verbs.
However, the order of these dependents withrespect to the parent is influenced by the typologicalfeatures of each language.To implement this intuition, we factorize genera-tion of a dependency tree into two processes: selec-tion of syntactic dependents and their ordering.
Thefirst component models the distribution of depen-dents for each part-of-speech tag, abstracting overtheir order.
Being largely language-universal, thisdistribution can be learned in a supervised fashionfrom all the training languages.
On the other hand,629ordering of dependents varies greatly across lan-guages and therefore should only be influenced bylanguages with similar properties.
Furthermore, thissimilarity has to be expressed at the level of depen-dency types ?
i.e., two languages may share noun-adposition ordering, but differ in noun-determinerordering.
To systematically model this cross-lingualsharing, we rely on typological features that reflectordering preferences of a given language.
In addi-tion to the known typological features, our parsingmodel embeds latent features that can capture cross-lingual structural similarities.While the approach described so far supports aseamless transfer of shared information, it does notaccount for syntactic properties of the target lan-guage unseen in the training languages.
For in-stance, in the CoNLL data, Arabic is the only lan-guage with the VSO ordering.
To handle such cases,our approach augments cross-lingual sharing withunsupervised learning on the target languages.We evaluated our selective sharing model on 17languages from 10 language families.
On this di-verse set, our model consistently outperforms state-of-the-art multilingual dependency parsers.
Per-formance gain, averaged over all the languages, is5.9% when compared to the highest baseline.
Ourmodel achieves the most significant gains on non-Indo-European languages, where we see a 14.4%improvement.
We also demonstrate that in the ab-sence of observed typological information, a set ofautomatically induced latent features can effectivelywork as a proxy for typology.2 Related WorkTraditionally, parallel corpora have been a main-stay of multilingual parsing (Wu, 1997; Kuhn, 2004;Smith and Smith, 2004; Hwa et al, 2005; Xi andHwa, 2005; Burkett and Klein, 2008; Snyder et al,2009).
However, recent work in multilingual pars-ing has demonstrated the feasibility of transfer in theabsence of parallel data.
As a main source of guid-ance, these methods rely on the commonalities in de-pendency structure across languages.
For instance,Naseem et al (2010) explicitly encode these similar-ities in the form of universal rules which guide gram-mar induction in the target language.
An alterna-tive approach is to directly employ a non-lexicalizedparser trained on one language to process a targetlanguage (Zeman and Resnik, 2008; McDonald etal., 2011; S?gaard, 2011).
Since many unlexicalizeddependencies are preserved across languages, theseapproaches are shown to be effective for relatedlanguages.
For instance, when applied to the lan-guage pairs within the Indo-European family, suchparsers outperform unsupervised monolingual tech-niques by a significant margin.The challenge, however, is to enable dependencytransfer for target languages that exhibit structuraldifferences from source languages.
In such cases,the extent of multilingual transfer is determined bythe relation between source and target languages.Berg-Kirkpatrick and Klein (2010) define such a re-lation in terms of phylogenetic trees, and use thisdistance to selectively tie the parameters of mono-lingual syntactic models.
Cohen et al (2011) do notuse a predefined linguistic hierarchy of language re-lations, but instead learn the contribution of sourcelanguages to the training mixture based on the like-lihood of the target language.
S?gaard (2011)proposes a different measure of language related-ness based on perplexity between POS sequencesof source and target languages.
Using this measure,he selects a subset of training source sentences thatare closer to the target language.
While all of theabove techniques demonstrate gains from modelinglanguage relatedness, they still underperform whenthe source and target languages are unrelated.Our model differs from the above approaches inits emphasis on the selective information sharingdriven by language relatedness.
This is further com-bined with monolingual unsupervised learning.
Asour evaluation demonstrates, this layered approachbroadens the advantages of multilingual learning tolanguages that exhibit significant differences fromthe languages in the training mix.3 Linguistic MotivationLanguage-Independent Dependency PropertiesDespite significant syntactic differences, human lan-guages exhibit striking similarity in dependency pat-terns.
For a given part-of-speech tag, the set of tagsthat can occur as its dependents is largely consistentacross languages.
For instance, adverbs and nounsare likely to be dependents of verbs, while adjectives630are not.
Thus, these patterns can be freely trans-ferred across languages.Shared Dependency Properties Unlike dependentselection, the ordering of dependents in a sentencediffers greatly across languages.
In fact, cross-lingual syntactic variations are primarily expressedin different ordering of dependents (Harris, 1968;Greenberg, 1963).
Fortunately, the dimensions ofthese variations have been extensively studied in lin-guistics and are documented in the form of typo-logical features (Comrie, 1989; Haspelmath et al,2005).
For instance, most languages are either dom-inantly prepositional like English or post-positionallike Urdu.
Moreover, a language may be close to dif-ferent languages for different dependency types.
Forinstance, Portuguese is a prepositional language likeEnglish, but the order of its noun-adjective depen-dency is different from English and matches that ofArabic.
Therefore, we seek a model that can expressparameter sharing at the level of dependency typesand can benefit from known language relations.Language-specific Dependency Variations Notevery aspect of syntactic structure is shared acrosslanguages.
This is particularly true given a limitednumber of supervised source languages; it is quitelikely that a target language will have previously un-seen syntactic phenomena.
In such a scenario, theraw text in the target language might be the onlysource of information about its unique aspects.4 ModelWe propose a probabilistic model for generatingdependency trees that facilitates parameter sharingacross languages.
We assume a setup where de-pendency tree annotations are available for a set ofsource languages and we want to use these annota-tions to infer a parser for a target language.
Syn-tactic trees for the target language are not availableduring training.
We also assume that both sourceand target languages are annotated with a coarseparts-of-speech tagset which is shared across lan-guages.
Such tagsets are commonly used in multilin-gual parsing (Zeman and Resnik, 2008; McDonaldet al, 2011; S?gaard, 2011; Naseem et al, 2010).The key feature of our model is a two-tier ap-proach that separates the selection of dependentsfrom their ordering:1.
Selection Component: Determines the depen-dent tags given the parent tag.2.
Ordering Component: Determines the positionof each dependent tag with respect to its parent(right or left) and the order within the right andleft dependents.This factorization constitutes a departure fromtraditional parsing models where these decisions aretightly coupled.
By separating the two, the modelis able to support different degrees of cross-lingualsharing on each level.For the selection component, a reasonable ap-proximation is to assume that it is the same for alllanguages.
This is the approach we take here.As mentioned in Section 3, the ordering of depen-dents is largely determined by the typological fea-tures of the language.
We assume that we have aset of such features for every language l, and denotethis feature vector by vl.
We also experiment with avariant of our model where typological features arenot observed.
Instead, the model captures structuralvariations across languages by means of a small setof binary latent features.
The values of these fea-tures are language dependent.
We denote the set oflatent features for language l by bl.Finally, based on the well known fact that longdistance dependencies are less likely (Eisner andSmith, 2010), we bias our model towards short de-pendencies.
This is done by imposing a corpus-levelsoft constraint on dependency lengths using the pos-terior regularization framework (Grac?a et al, 2007).4.1 Generative ProcessOur model generates dependency trees one fragmentat a time.
A fragment is defined as a subtree com-prising the immediate dependents of any node in thetree.
The process recursively generates fragmentsin a head outwards manner, where the distributionover fragments depends on the head tag.
If the gen-erated fragment is not empty then the process con-tinues for each child tag in the fragment, drawingnew fragments from the distribution associated withthe tag.
The process stops when there are no morenon-empty fragments.A fragment with head node h is generated in lan-guage l via the following stages:631h{N,A,N, V,D}h{N,N, V } {A,D}hN NV D A(a) (b) (c)Figure 1: The steps of the generative process for a fragment with head h. In step (a), the unordered set of dependentsis chosen.
In step (b) they are partitioned into left and right unordered sets.
Finally, each set is ordered in step (c).?
Generate the set of dependents of h via a distri-bution Psel(S|h).
Here S is an unordered set ofPOS tags.
Note that this part is universal (i.e.,it does not depend on the language l).?
For each element in S decide whether it shouldgo to the right or left of h as follows: for everya ?
S, draw its direction from the distributionPord(d|a, h, l), where d ?
{R,L}.
This resultsin two unordered sets SR, SL, the right and leftdependents of h. This part does depend on thelanguage l, since the relative ordering of depen-dents is not likely to be universal.?
Order the sets SR, SL.
For simplicity, we as-sume that the order is drawn uniformly fromall the possible unique permutations over SRand SL.
We denote the number of such uniquepermutations of SR by n(SR).2 Thus the prob-ability of each permutation of SR is 1n(SR)3.Figure 1 illustrates the generative process.
The firststep constitutes the selection component and the lasttwo steps constitute the ordering component.
Giventhis generation scheme, the probability P (D) ofgenerating a given fragment D with head h will be:Psel({D}|h)?a?DPord(dD(a)|a, h, l)1n(DR)n(DL)(1)Where we use the following notations:?
DR, DL denote the parts of the fragment thatare to the left and right of h.2This number depends on the count of each distinct tag inSR.
For example if SR = {N,N,N} then n(SR) = 1.
IfSR = {N,D, V } then n(SR) = 3!.3We acknowledge that assuming a uniform distribution overthe permutations of the right and left dependents is linguisticallycounterintuitive.
However, it simplifies the model by greatlyreducing the number of parameters to learn.?
{D} is the unordered set of tags in D.?
dD(a) is the position (either R or L) of the de-pendent a w.r.t.
the head of D.In what follows we discuss the parameterizationsof the different distributions.4.1.1 Selection ComponentThe selection component draws an unordered setof tags S given the head tag h. We assume that theprocess is carried out in two steps.
First the numberof dependents n is drawn from a distribution:Psize(n|h) = ?size(n|h) (2)where ?size(n|h) is a parameter for each value ofn and h. We restrict the maximum value of n tofour, since this is a reasonable bound on the totalnumber of dependents for a single parent node ina tree.
These parameters are non-negative and sat-isfy?n ?size(n|h) = 1.
In other words, the sizeis drawn from a categorical distribution that is fullyparameterized.Next, given the size n, a set S with |S| = n isdrawn according to the following log-linear model:Pset(S|h, n) =1Zset(h, n)e?Si?S?sel(Si|h)Zset(h, n) =?S:|S|=ne?Si?S?sel(Si|h)In the above, Si is the ith POS tag in the unorderedset S, and ?sel(Si|h) are parameters.
Thus, large val-ues of ?sel(Si|h) indicate that POS Si is more likelyto appear in the subset with parent POS h.Combining the above two steps we have the fol-lowing distribution for selecting a set S of size n:Psel(S|h) = Psize(n|h)Pset(S|h, n) .
(3)632ID Feature Description Values81A Order of Subject, Object and Verb SVO, SOV, VSO, VOS, OVS, OSV85A Order of Adposition and Noun Postpositions, Prepositions, Inpositions86A Order of Genitive and Noun Genitive-Noun, Noun-Genitive87A Order of Adjective and Noun Adjective-Noun, Noun-Adjective88A Order of Demonstrative and Noun Demonstrative-Noun, Noun-Demonstrative89A Order of Numeral and Noun Numeral-Noun, Noun-NumeralTable 1: The set of typological features that we use in our model.
For each feature, the first column gives the ID ofthe feature as used in WALS, the second column describes the feature and the last column enumerates the allowablevalues for the feature.
Besides these values, each feature can also have a value of ?No dominant order?.4.1.2 Ordering ComponentThe ordering component consists of distributionsPord(d|a, h, l) that determine whether tag a will bemapped to the left or right of the head tag h. Wemodel it using the following log-linear model:Pord(d|a, h, l) =1Zord(a, h, l)eword?g(d,a,h,vl)Zord(a, h, l) =?d?
{R,L}eword?g(d,a,h,vl)Note that in the above equations the orderingcomponent depends on the known typological fea-tures vl.
In the setup when typological features arenot known, vl is replaced with the latent orderingfeature set bl.The feature vector g contains indicator featuresfor combinations of a, h, d and individual featuresvli (i.e., the ith typological features for language l).4.2 Typological FeaturesThe typological features we use are a subset oforder-related typological features from ?The WorldAtlas of Language Structure?
(Haspelmath et al,2005).
We include only those features whose val-ues are available for all the languages in our dataset.Table 1 summarizes the set of features that we use.Note that we do not explicitly specify the correspon-dence between these features and the model param-eters.
Instead, we leave it for the model to learn thiscorrespondence automatically.4.3 Dependency Length ConstraintTo incorporate the intuition that long distance de-pendencies are less likely, we impose a posteriorconstraint on dependency length.
In particular, weuse the Posterior Regularization (PR) framework ofGrac?a et al (2007).
The PR framework incorporatesconstraints by adding a penalty term to the standardlikelihood objective.
This term penalizes the dis-tance of the model posterior from a set Q, whereQ contains all the posterior distributions that satisfythe constraints.
In our case the constraint is that theexpected dependency length is less than or equal toa pre-specified threshold value b.
If we denote thelatent dependency trees by z and the observed sen-tences by x thenQ = {q(z|x) : Eq[f(x, z)] ?
b} (4)where f(x, z) computes the sum of the lengths of alldependencies in z with respect to the linear order ofx.
We measure the length of a dependency relationby counting the number of tokens between the headand its modifier.
The PR objective penalizes the KL-divergence of the model posterior from the set Q:L?
(x)?KL (Q ?
p?
(z|x))where ?
denotes the model parameters and the firstterm is the log-likelihood of the data.
This objectivecan be optimized using a modified version of the EMalgorithm (Grac?a et al, 2007).5 Parameter LearningOur model is parameterized by the parameters ?sel,?size and word.
We learn these by maximizing thelikelihood of the training data.
As is standard, weadd `2 regularization on the parameters and tune iton source languages.
The likelihood is marginalizedover all latent variables.
These are:?
For sentences in the target language: all pos-sible derivations that result in the observedPOS tag sequences.
The derivations includethe choice of unordered sets size n, the un-ordered sets themselves S, their left/right al-633locations and the orderings within the left andright branches.?
For all languages: all possible values of the la-tent features bl.4Since we are learning with latent variables, we usethe EM algorithm to monotonically improve thelikelihood.
At each E step, the posterior over latentvariables is calculated using the current model.
Atthe M step this posterior is used to maximize thelikelihood over the fully observed data.
To com-pensate for the differences in the amount of trainingdata, the counts from each language are normalizedbefore computing the likelihood.The M step involves finding maximum likelihoodparameters for log-linear models in Equations 3 and4.
This is done via standard gradient based search;in particular, we use the method of BFGS.We now briefly discuss how to calculate the pos-terior probabilities.
For estimating the word param-eters we require marginals of the type P (bli|Dl;wt)where Dl are the sentences in language l, bli is theith latent feature for the language l and wt are theparameter values at iteration t. Consider doing thisfor a source language l. Since the parses are known,we only need to marginalize over the other latentfeatures.
This can be done in a straightforward man-ner by using our probabilistic model.
The complex-ity is exponential in the number of latent features,since we need to marginalize over all features otherthan bli.
This is feasible in our case, since we use arelatively small number of such features.When performing unsupervised learning for thetarget language, we need to marginalize over possi-ble derivations.
Specifically, for the M step, we needprobabilities of the form P (a modifies h|Dl;wt).These can be calculated using a variant of the insideoutside algorithm.
The exact version of this algo-rithm would be exponential in the number of depen-dents due to the 1n(Sr) term in the permutation factor.Although it is possible to run this exact algorithm inour case, where the number of dependents is limitedto 4, we use an approximation that works well inpractice: instead of 1n(Sr) we use1|Sr|!.
In this casethe runtime is no longer exponential in the numberof children, so inference is much faster.4This corresponds to the case when typological features arenot known.Finally, given the trained parameters we generateparses in the target language by calculating the max-imum a posteriori derivation.
This is done using avariant of the CKY algorithm.6 Experimental SetupDatasets and Evaluation We test the effectivenessof our approach on 17 languages: Arabic, Basque,Bulgarian, Catalan, Chinese, Czech, Dutch, English,German, Greek, Hungarian, Italian, Japanese, Por-tuguese, Spanish, Swedish and Turkish.
We useddatasets distributed for the 2006 and 2007 CoNLLShared Tasks (Buchholz and Marsi, 2006; Nivreet al, 2007).
Each dataset provides manually an-notated dependency trees and POS tags.
To en-able crosslingual sharing, we map the gold part-of-speech tags in each corpus to a common coarsetagset (Zeman and Resnik, 2008; S?gaard, 2011;McDonald et al, 2011; Naseem et al, 2010).
Thecoarse tagset consists of 11 tags: noun, verb, ad-jective, adverb, pronoun, determiner, adposition, nu-meral, conjunction, particle, punctuation mark, andX (a catch-all tag).
Among several available fine-to-coarse mapping schemes, we employ the one ofNaseem et al (2010) that yields consistently betterperformance for our method and the baselines thanthe mapping proposed by Petrov et al (2011).As the evaluation metric, we use directed depen-dency accuracy.
Following standard evaluation prac-tices, we do not evaluate on punctuation.
For boththe baselines and our model we evaluate on all sen-tences of length 50 or less ignoring punctuation.Training Regime Our model typically convergesquickly and does not require more than 50 iterationsof EM.
When the model involves latent typologicalvariables, the initialization of these variables can im-pact the final performance.
As a selection criterionfor initialization, we consider the performance of thefinal model averaged over the supervised source lan-guages.
We perform ten random restarts and selectthe best according to this criterion.
Likewise, thethreshold value b for the PR constraint on the depen-dency length is tuned on the source languages, usingaverage test set accuracy as the selection criterion.Baselines We compare against the state-of-the-artmultilingual dependency parsers that do not use par-allel corpora for training.
All the systems were eval-634uated using the same fine-to-coarse tagset mapping.The first baseline, Transfer, uses direct transfer of adiscriminative parser trained on all the source lan-guages (McDonald et al, 2011).
This simple base-line achieves surprisingly good results, within lessthan 3% difference from a parser trained using par-allel data.
In the second baseline (Mixture), pa-rameters of the target language are estimated as aweighted mixture of the parameters learned from an-notated source languages (Cohen et al, 2011).
Theunderlying parsing model is the dependency modelwith valance (DMV) (Klein and Manning, 2004).Originally, the baseline methods were evaluated ondifferent sets of languages using a different tag map-ping.
Therefore, we obtained new results for thesemethods in our setup.
For the Transfer baseline,for each target language we trained the model onall other languages in our dataset.
For the Mixturebaseline, we trained the model on the same four lan-guages used in the original paper ?
English, Ger-man, Czech and Italian.
When measuring the per-formance on these languages, we selected anotherset of four languages with a similar level of diver-sity.57 ResultsTable 2 summarizes the performance for differentconfigurations of our model and the baselines.Comparison against Baselines On average, theselective sharing model outperforms both base-lines, yielding 8.9% gain over the weighted mixturemodel (Cohen et al, 2011) and 5.9% gain over thedirect transfer method (McDonald et al, 2011).
Ourmodel outperforms the weighted mixture model on15 of the 17 languages and the transfer method on12 of the 17 languages.
Most of the gains are ob-tained on non-Indo-European languages, that havelittle similarity with the source languages.
For thisset, the average gain over the transfer baseline is14.4%.
With some languages, such as Japanese,achieving gains of as much as 30%.On Indo-European languages, the model perfor-mance is almost equivalent to that of the best per-forming baseline.
To explain this result we con-5We also experimented with a version of the Cohen et al(2011) model trained on all the source languages.
This setupresulted in decreased performance.
For this reason, we chose totrain the model on the four languages.sider the performance of the supervised version ofour model which constitutes an upper bound on theperformance.
The average accuracy of our super-vised model on these languages is 66.8%, comparedto the 76.3% of the unlexicalized MST parser.
SinceIndo-European languages are overrepresented in ourdataset, a target language from this family is likelyto exhibit more similarity to the training data.
Whensuch similarity is substantial, the transfer baselinewill benefit from the power of a context-rich dis-criminative parser.A similar trait can be seen by comparing the per-formance of our model to an oracle version of ourmodel which selects the optimal source languagefor a given target language (column 7).
Overall,our method performs similarly to this oracle variant.However, the gain for non Indo-European languagesis 1.9% vs -1.3% for Indo-European languages.Analysis of Model Properties We first test ourhypothesis about the universal nature of the depen-dent selection.
We compare the performance ofour model (column 6) against a variant (column 8)where this component is trained from annotations onthe target language.
The performance of the two isvery close ?
1.8%, supporting the above hypothesis.To assess the contribution of other layers of selec-tive sharing, we first explore the role of typologicalfeatures in learning the ordering component.
Whenthe model does not have access to observed typo-logical features, and does not use latent ones (col-umn 4), the accuracy drops by 2.6%6.
For somelanguages (e.g., Turkish) the decrease is very pro-nounced.
Latent typological features (column 5) donot yield the same gain as observed ones, but they doimprove the performance of the typology-free modelby 1.4%.Next, we show the importance of using raw tar-get language data in training the model.
Whenthe model has to make all the ordering decisionsbased on meta-linguistic features without accountfor unique properties of the target languages, theperformance decreases by 0.9% (see column 3).To assess the relative difficulty of learning theordering and selection components, we considermodel variants where each of these components is6In this setup, the ordering component is trained in an unsu-pervised fashion on the target language.635Baselines Selective Sharing ModelMixture Transfer (D-,To) (D+) (D+,Tl) (D+,To) Best Pair Sup.
Sel.
Sup.
Ord.
MLECatalan 64.9 69.5 71.9 66.1 66.7 71.8 74.8 70.2 73.2 72.1Italian 61.9 68.3 68.0 65.5 64.2 65.6 68.3 65.1 70.7 72.3Portuguese 72.9 75.8 76.2 72.3 76.0 73.5 76.4 77.4 77.6 79.6Spanish 57.2 65.9 62.3 58.5 59.4 62.1 63.4 61.5 62.6 65.3Dutch 50.1 53.9 56.2 56.1 55.8 55.9 57.8 56.3 58.6 58.0English 45.9 47.0 47.6 48.5 48.1 48.6 44.4 46.3 60.0 62.7German 54.5 56.4 54.0 53.5 54.3 53.7 54.8 52.4 56.2 58.0Swedish 56.4 63.6 52.0 61.4 60.6 61.5 63.5 67.9 67.1 73.0Bulgarian 67.7 64.0 67.6 63.5 63.9 66.8 66.1 66.2 69.5 71.0Czech 39.6 40.3 43.9 44.7 45.4 44.6 47.5 53.2 51.2 58.9Arabic 44.8 40.7 57.2 58.8 60.3 58.9 57.6 62.9 61.9 64.2Basque 32.8 32.4 39.7 40.1 39.8 47.6 42.0 46.2 47.9 51.6Chinese 46.7 49.3 59.9 52.2 52.0 51.2 65.4 62.3 65.5 73.5Greek 56.8 60.4 61.9 67.5 67.3 67.4 60.6 67.2 69.0 70.5Hungarian 46.8 54.3 56.9 58.4 58.8 58.5 57.0 57.4 62.0 61.6Japanese 33.5 34.7 62.3 56.8 61.4 64.0 54.8 63.4 69.7 75.6Turkish 28.3 34.3 59.1 43.6 57.8 59.2 56.9 66.6 59.5 67.6Average 50.6 53.6 58.6 56.9 58.3 59.5 59.5 61.3 63.7 66.8Table 2: Directed dependency accuracy of different variants of our selective sharing model and the baselines.
Thefirst section of the table (column 1 and 2) shows the accuracy of the weighted mixture baseline (Cohen et al, 2011)(Mixture) and the multi-source transfer baseline (McDonald et al, 2011) (Transfer).
The middle section shows theperformance of our model in different settings.
D?
indicates the presence/absence of raw target language data duringtraining.
To indicates the use of observed typological features for all languages and Tl indicates the use of latenttypological features for all languages.
The last section shows results of our model with different levels of oraclesupervision: a.
(Best Pair) Model parameters are borrowed from the best source language based on the accuracy onthe target language b.
(Sup.
Sel.)
Selection component is trained using MLE estimates from target language c.
(Sup.Ord.)
Ordering component is trained using MLE estimates from the target language d. (MLE) All model parametersare trained on the target language in a supervised fashion.
The horizontal partitions separate language families.
Thefirst three families are sub-divisions of the Indo-European language family.trained using annotations in the target language.
Asshown in columns 8 and 9, these two variants out-perform the original model, achieving 61.3% for su-pervised selection and 63.7% for supervised order-ing.
Comparing these numbers to the accuracy ofthe original model (column 6) demonstrates the dif-ficulty inherent in learning the ordering information.This finding is expected given that ordering involvesselective sharing from multiple languages.Overall, the performance gap between the selec-tive sharing model and its monolingual supervisedcounterpart is 7.3%.
In contrast, the unsupervisedmonolingual variant of our model achieves a mea-ger 26%.7 This demonstrates that our model can ef-fectively learn relevant aspects of syntactic structurefrom a diverse set of languages.7This performance is comparable to other generative modelssuch as DMV (Klein and Manning, 2004).8 ConclusionsWe present a novel algorithm for multilingual de-pendency parsing that uses annotations from a di-verse set of source languages to parse a new unan-notated language.
Overall, our model consistentlyoutperforms the multi-source transfer based depen-dency parser of McDonald et al (2011).
Our ex-periments demonstrate that the model is particularlyeffective in processing languages that exhibit signif-icant differences from the training languages.AcknowledgmentsThe authors acknowledge the support of the NSF(IIS-0835445), the MURI program (W911NF-10-1-0533), the DARPA BOLT program, and the ISF(1789/11).
We thank Tommi Jaakkola, Ryan Mc-Donald and the members of the MIT NLP group fortheir comments.636ReferencesTaylor Berg-Kirkpatrick and Dan Klein.
2010.
Phyloge-netic grammar induction.
In ACL, pages 1288?1297.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL, pages 149?164.David Burkett and Dan Klein.
2008.
Two languages arebetter than one (for syntactic parsing).
In Proceedingsof EMNLP, pages 877?886.Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011.Unsupervised structure prediction with non-parallelmultilingual guidance.
In EMNLP, pages 50?61.Bernard Comrie.
1989.
Language Universals and Lin-guistic Typology: Syntax and Morphology.
Oxford:Blackwell.Jason Eisner and Noah A. Smith.
2010.
Favor short de-pendencies: Parsing with soft and hard constraints ondependency length.
In Trends in Parsing Technology:Dependency Parsing, Domain Adaptation, and DeepParsing, pages 121?150.Joa?o Grac?a, Kuzman Ganchev, and Ben Taskar.
2007.Expectation maximization and posterior constraints.In Advances in NIPS, pages 569?576.Joseph H Greenberg.
1963.
Some universals of languagewith special reference to the order of meaningful ele-ments.
In Joseph H Greenberg, editor, Universals ofLanguage, pages 73?113.
MIT Press.Z.S.
Harris.
1968.
Mathematical structures of language.Wiley.Martin Haspelmath, Matthew S. Dryer, David Gil, andBernard Comrie, editors.
2005.
The World Atlas ofLanguage Structures.
Oxford University Press.R.
Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-lak.
2005.
Bootstrapping parsers via syntactic projec-tion across parallel texts.
Journal of Natural LanguageEngineering, 11(3):311?325.Dan Klein and Christopher Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL,pages 478?485.Jonas Kuhn.
2004.
Experiments in parallel-text basedgrammar induction.
In Proceedings of the ACL, pages470?477.Ryan T. McDonald, Slav Petrov, and Keith Hall.
2011.Multi-source transfer of delexicalized dependencyparsers.
In EMNLP, pages 62?72.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowledgeto guide grammar induction.
In EMNLP, pages 1234?1244.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 915?932.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A universal part-of-speech tagset.
In ArXiv, April.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: Using English toparse Korean.
In Proceeding of EMNLP, pages 49?56.Benjamin Snyder, Tahira Naseem, and Regina Barzilay.2009.
Unsupervised multilingual grammar induction.In Proceedings of ACL/AFNLP, pages 73?81.Anders S?gaard.
2011.
Data point selection for cross-language adaptation of dependency parsers.
In ACL(Short Papers), pages 682?686.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Chenhai Xi and Rebecca Hwa.
2005.
A backoff modelfor bootstrapping resources for non-english languages.In Proceedings of EMNLP, pages 851 ?
858.Daniel Zeman and Philip Resnik.
2008.
Cross-languageparser adaptation between related languages.
In Pro-ceedings of the IJCNLP-08 Workshop on NLP for LessPrivileged Languages, pages 35?42, January.637
