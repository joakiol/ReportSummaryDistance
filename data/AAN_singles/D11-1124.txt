Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1342?1351,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsSummarize What You Are Interested In:An Optimization Framework for Interactive Personalized SummarizationRui YanDepartment of ComputerScience and Technology,Peking University,Beijing 100871, Chinar.yan@pku.edu.cnJian-Yun NieDe?partement d?informatiqueet de recherche ope?rationnelle,Universite?
de Montre?al,Montre?al, H3C 3J7 Que?bec, Canadanie@iro.umontreal.caXiaoming LiDepartment of ComputerScience and Technology,Peking University,Beijing 100871, Chinalxm@pku.edu.cnAbstractMost traditional summarization methods treattheir outputs as static and plain texts, whichfail to capture user interests during summa-rization because the generated summaries arethe same for different users.
However, usershave individual preferences on a particularsource document collection and obviously auniversal summary for all users might not al-ways be satisfactory.
Hence we investigatean important and challenging problem in sum-mary generation, i.e., Interactive PersonalizedSummarization (IPS), which generates sum-maries in an interactive and personalized man-ner.
Given the source documents, IPS capturesuser interests by enabling interactive clicksand incorporates personalization by model-ing captured reader preference.
We developexperimental systems to compare 5 rival al-gorithms on 4 instinctively different datasetswhich amount to 5197 documents.
Evalua-tion results in ROUGE metrics indicate thecomparable performance between IPS and thebest competing system but IPS produces sum-maries with much more user satisfaction ac-cording to evaluator ratings.
Besides, lowROUGE consistency among these user pre-ferred summaries indicates the existence ofpersonalization.1 IntroductionIn the era of information explosion, people need newinformation to update their knowledge whilst infor-mation on Web is updating extremely fast.
Multi-document summarization has been proposed to ad-dress such dilemma by producing a summary de-livering the majority of information content from adocument set, and hence is a necessity.Traditional summarization methods play an im-portant role with the exponential document growthon the Web.
However, for the readers, the impact ofhuman interests has seldom been considered.
Tra-ditional summarization utilizes the same methodol-ogy to generate the same summary no matter who isreading.
However, users may have bias on what theyprefer to read due to their potential interests: theyneed personalization.
Therefore, traditional summa-rization methods are to some extent insufficient.Topic biased summarization tries for personaliza-tion by pre-defining human interests as several gen-eral categories, such as health or science.
Readersare required to select their possible interests beforesummary generation so that the chosen topic haspriority during summarization.
Unfortunately, suchtopic biased summarization is not sufficient for tworeasons: (1) interests cannot usually be accuratelypre-defined by ambiguous topic categories and (2)user interests cannot always be foreknown.
Oftenusers do not really know what general ideas or detailinformation they are interested in until they read thesummaries.
Therefore, more flexible interactionsare required to establish personalization.Due to all the insufficiencies of existed sum-marization approaches, we introduce a new multi-document summarization task of Interactive Person-alized Summarization (IPS) and a novel solution forthe task.
Taking a document collection as input, thesystem outputs a summary aligned both with sourcecorpus and with user personalization, which is cap-tured by flexible human?system interactions.
We1342build an experimental system on 4 real datasets toverify the effectiveness of our methods comparedwith 4 rivals.
The contribution of IPS is manifoldby addressing following challenges:?
The 1st challenge for IPS is to integrate userinterests into traditional summary components.
Wemeasure the utilities of these components and com-bine them.
We formulate the task into a balancedoptimization framework via iterative substitution togenerate summaries with maximum overall utilities.?
The 2nd challenge is to capture user inter-ests through interaction.
We develop an interactivemechanism of ?click?
and ?examine?
between read-ers and summaries and address sparse data by ?clicksmoothing?
under the scenario of few user clicks.We start by reviewing previous works.
In Section3 we provide IPS overview, describe user interac-tion and optimize component combination with per-sonalization.
We conduct empirical evaluation anddemonstrate the experimental system in Section 4.Finally we draw conclusions in Section 5.2 Related WorkMulti-Document Summarization (MDS) has drawnmuch attention in recent years and gained emphasisin conferences such as ACL, EMNLP and SIGIR,etc.
General MDS can either be extractive or ab-stractive.
The former assigns salient scores to se-mantic units (e.g.
sentences, paragraphs) of the doc-uments indicating their importance and then extractstop ranked ones, while the latter demands informa-tion fusion(e.g.
sentence compression and reformu-lation).
Here we focus on extractive summarization.Centroid-based method is one of the most popularextractive summarization method.
MEAD (Radevet al, 2004) and NeATS (Lin and Hovy, 2002) aresuch implementations, using position and term fre-quency, etc.
MMR (Goldstein et al, 1999) algorithmis used to remove redundancy.
Most recently, thegraph-based ranking methods have been proposed torank sentences or passages based on the ?votes?
or?recommendations?
between each other.
The graph-based methods first construct a graph representingthe sentence relationships at different granularitiesand then evaluate the saliency score of the sentencesbased on the graph.
TextRank (Mihalcea and Tarau,2005) and LexPageRank (Erkan and Radev, 2004)use algorithms similar to PageRank and HITS tocompute sentence importance.
Wan et al improvethe graph-ranking algorithm by differentiating intra-document and inter-document links between sen-tences (2007b) and incorporate cluster informationin the graph model to evaluate sentences (2008).To date, topics (or themes, clusters) in documentshave been discovered and used for sentence selec-tion for topic biased summarization (Wan and Yang,2008; Gong and Liu, 2001).
Wan et al haveproposed a manifold-ranking method to make uni-form use of sentence-to-sentence and sentence-to-topic relationships to generate topic biased sum-maries (2007a).
Leuski et al in (2003) pre-defineseveral topic concepts, assuming users will foreseetheir interested topics and then generate the topicbiased summary.
However, such assumption is notquite reasonable because user interests may not beforecasted, or pre-defined accurately as we have ex-plained in last section.The above algorithms are usually traditional ex-tensions of generic summarizers.
They do not in-volve interactive mechanisms to capture reader in-terests, nor do they utilize user preference for per-sonalization in summarization.
Wan et al in (2008)have proposed a summarization biased to neighbor-ing reading context through anchor texts.
How-ever, such scenario does not apply to contexts with-out human-edited anchor texts like Wikipedia theyhave used.
Our approach can naturally and simulta-neously take into account traditional summary ele-ments and user interests and combine both in opti-mization under a wider practical scenario.3 Interactive Personalized SummarizationPersonalization based on user preference can becaptured via various alternative ways, such as eye-tracking or mouse-tracking instruments used in (Guoand Agichtein, 2010).
In this study, we utilize inter-active user clicks/examinations for personalization.Unlike traditional summarization, IPS supportshuman?system interaction by clicking into the sum-mary sentences and examining source contexts.
Theimplicit feedback of user clicks indicates what theyare interested in and the system collects preferenceinformation to update summaries if readers wish to.We obtain an associated tuple <q, c> between a1343clicked sentence q and the examined contexts c.As q has close semantic coherence with neigh-boring contexts due to consistency in human naturallanguage, we consider a window of sentences cen-tered at the clicked sentence q as c, which is a bag ofsentences.
The window size k is a parameter to set.However, click data is often sparse: users are notlikely to click more than 1/10 of total summary sen-tences within a single generation.
We amplify thesetiny hints of user interest by click smoothing.We change the flat summary structure into a hi-erarchical organization by extracting important se-mantic units (denoted as u) and establishing link-age between them.
If the clicked sentence q con-tains u, we diffuse the click impact to the correlatedunits, which makes a single click perform as multi-ple clicks and the sparse data is smoothed.Problem FormulationInput: Given the sentence collection D decom-posed by documents, D = {s1, s2, .
.
.
, s|D|} andthe clicked sentence record Q = {q1, q2, .
.
.
}, wegenerate summaries in sentences.
A user click isassociated with a tuple <q, (u), c> where the exis-tence of u depends on whether q contains u. Thecollection of semantic units is denoted as M ={u1, u2, .
.
.
, u|M |}.Output: A summary S as a set of sentences{s1, s2, .
.
.
, s|S|} and S ?
D according to the pre-specified compression rate ?
(0 < ?
< 1).After the overview and formulation of IPS prob-lem, we move on to the major components of UserInteraction and Personalized Summarization.3.1 User InteractionHypertexify Summaries.
We hypertexify the sum-mary structure by establishing linkage between se-mantic units.
There are several possible formats forsemantic units, such as words or n-grams, etc.
Assingle words are proved to be not illustrative of se-mantic meanings (Zhao et al, 2011) and n-grams arerigid in length, we choose to extract semantic unitsat a phrase granularity.
Among all phrases fromsource texts, some are of higher importance to at-tract user interests, such as hot concepts or popu-lar event names.
We utilize the toolkit provided by(Zhao et al, 2011) based on graph proximity LDA(Blei et al, 2003) to extract key phrases and theircorresponding topic.
A topic T is represented by{(u1, pi(u1, T )), (u2, pi(u2, T )), .
.
.
}where pi(u, T )is the probability of u belonging to topic T .
We in-vert the topic-unit representation in Table 1, whereeach u is represented as a topic vector.
The corre-lation corr(.)
between ui, uj is measured by cosinesimilarity sim(.)
on topic distribution vector ~ui, ~uj .corr(ui, uj) = simtopic(~ui, ~uj) (1)Table 1: Inverted representation of topic-unit vector.~u1 pi(u1, T1) pi(u1, T2) .
.
.
pi(u1, Tn)~u2 pi(u2, T1) pi(u2, T2) .
.
.
pi(u2, Tn)... ... ... ... ...~u|M | pi(u|M |, T1) pi(u|M |, T2) .
.
.
pi(u|M |, Tn)When the summary is hypertexified by establishedlinkage, users click into the generated summary toexamine what they are interested in.
A single clickon one sentence become multiple clicks via clicksmoothing when the indicative function I(u|q) = 1.I(u|q) ={1 q contains u;0 otherwise.
(2)The click smoothing brings pseudo clicks q?
asso-ciated with u?
and contexts c?.
The entire user feed-back texts A from q can be written as:A(q) = I(u|q)|M |?j=1corr(u?, u)(u?+?
?c?)+?
?c (3)where ?
is the weight tradeoff between u and asso-ciated contexts c. If I(u|q) = 0, only the examinedcontext c is feedbacked for user preference; other-wise, correlative contexts with u are taken into con-sideration, which is a process of impact diffusion.3.2 Personalized SummarizationTraditional summarization involves two essential re-quirements: (1) coverage: the summary shouldkeep alignment with the source collection, which isproved to be significant (Li et al, 2009).
(2) di-versity: according to MMR principle (Goldstein etal., 1999) and its applications (Wan et al, 2007b;Wan and Yang, 2008), a good summary should beconcise and contain as few redundant sentences aspossible, i.e., two sentences providing similar infor-mation should not both present.
According to our1344investigation, we observe that a well generated sum-mary should properly consider a key component of(3) user interests, which captures user preference tosummarize what they are interested in.All above requirements involve a measurementof similarity between two word distributions ?1and ?2.
Cosine, Kullback-Leibler divergence DKLand Jensen Shannon divergence DJS are all ableto measure the similarity, but (Louis and Nenkova,2009) indicate the superiority of DJS in summa-rization task.
We also introduce a pair of decreas-ing/increasing logistic functions, L1(x) = 1/(1 +ex) and L2(x) = ex/(1 + ex), to map the diver-gence into interval [0,1].
V is the vocabulary setand tf denotes the term frequency for word w.DJS(?1||?2) =12[DKL(?1||?2)+DKL(?2||?1)]whereDKL(?1||?2) =?k?Vp(w|?1)logp(w|?1)p(w|?2)wherep(w|?)
= tf(w,?)?w?
tf(w?,?
).Modeling Interest for User Utility.
Given a gener-ated summary S, users tend to scrutinize texts rele-vant to their interests.
Texts related to user implicitfeedback are collected as A = ?|Q|i=1A(qi).
Intu-itively, the smaller distance between the word distri-bution of final summary (?S) and the word distri-bution of user preference (?A), the higher utility ofuser interests Uuser(S) will be, i.e.,Uuser(S) = L1(DJS(?S ||?A)).
(4)We model the utility of traditional summarizationUtrad(S) using a linear interpolation controlled byparameter ?
between utility from coverage Uc(S)and utility Ud(S) from diversity:Utrad(S) = Uc(S) + ?
?
Ud(S).
(5)Coverage Utility.
The summary should share acloser word distribution with the source collection(Allan et al, 2001; Li et al, 2009).
A good summaryfocuses on minimizing the loss of main informationfrom the whole collection D. Utility from coverageUc(S) is defined as follows and for coverage utility,smaller divergence is desired.Uc(S) = L1(DJS(?S ||?D)).
(6)Diversity Utility.
Diversity measures the noveltydegree of any sentence s compared with all othersentences within S, i.e., the distances between allother sentences and itself.
Diversity utility Ud(S) isan average novelty score for all sentences in S. Fordiversity utility, larger distance is desired, and hencewe use the increasing function L2 as follows:Ud(S) =1|S|?s?SL2(DJS(?s||?(S?s))).
(7)3.3 Balanced Optimization FrameworkA well generated summary S should be sufficientlyaligned with the original source corpus, and alsobe optimized given the user interests.
The utilityof an individual summary U(S) is evaluated by theweighted combination of these components, con-trolled by parameter ?
for balanced weights.U(S) = Utrad(S) + ?
?
Uuser(S) (8)Given the sentence setD and the compression rate?, there are ?
?|D| out of |D| possibilities to generateS.
The IPS task is to predict the optimized sentencesubset of S?
from the space of all combinations.
Theobjective function is as follows:S?
= argmaxSU(S).
(9)As U(S) is measured based on preferred interestsfrom user interaction within a generation in our sys-tem, we extract S iteratively to approximate S?, i.e,maximize U(S) based on the user feedbacks fromthe interaction sessions.
Each session is an iteration.We use a similar framework as we have proposed in(Yan et al, 2011).During every session, the top ranked sentences arestrong candidates for the summary to generate andthe rank methodology is based on the metrics U(.
).The algorithm tends to highly rank sentences whichare with both coverage utility and interest utility, andare diversified in balance: we rank each sentence saccording to U(s) under such metrics.Consider S(n?1) generated in the (n-1)-th sessionwhich consists of top ?|D| ranked sentences, as well1345as the top ?|D| ranked sentences in the n-th iteration(denoted by O(n)), they have an intersection set ofZ(n) = Sn?1?On.
There is a substitutable sentenceset X (n) = S(n?1) ?Z(n) and a new candidate sen-tence set Y(n) = O(n) ?
Z(n).
We substitute x(n)sentences with y(n), where x(n) ?
X (n) and y(n)?
Y(n).
During every iteration, our goal is to find asubstitutive pair <x,y> for S:<x,y> : X ?
Y ?
R.To measure the performance of such a substitu-tion, a discriminant utility gain function ?Ux,y?U (n)x(n),y(n) = U(S(n))?
U(S(n?1))= U((S(n?1) ?
x(n)) ?
y(n))?
U(S(n?1))(10)is employed to quantify the penalty.
Therefore, wepredict the substitutive pair by maximizing the gainfunction ?Ux,y over the state set R, with a size of?Yk=0AkXCkY , where <x,y>?
R. Finally the ob-jective function of Equation (9) changes into maxi-mization of utility gain by substitute x?
with y?
duringeach iteration:< x?, y?
>= argmaxx?X ,y?Y?Ux,y.
(11)Note that the objectives of interest utility opti-mization and traditional utility optimization are notalways the same because the word distributions inthese texts are usually different.
The substitutivepair <x,y> may perform well based on the userpreference component while not on the traditionalsummary part and vice versa.
There is a tradeoffbetween both user optimization and traditional opti-mization and hence we need to balance them by ?.The objective Equation (11) is actually to maxi-mize ?U(S) from all possible substitutive pairs be-tween two iteration sessions to generate S. The al-gorithm is shown in Algorithm 1.
The threshold  isset at 0.001 in this study.4 Experiments and Evaluation4.1 DatasetsIPS can be tested on any document set but a tinycorpus to summarize may not cover abundant effec-tive interests to attract user clicks indicating theirAlgorithm 1 Regenerative Optimization1: Input: D, , ?2: for all s ?
D do3: calculate Utrad(s)4: end for5: S ?
top ?|D| ranked sentences6: while new generation=TRUE do7: collect clicks and update utility from U ?
to U8: if |U(S)?
U ?
(S)| >  then9: for all s ?
D do10: calculate U(s)11: end for12: O ?
top ?|D| ranked sentences by U(s)13: Z ?
S ?
O14: X ?
S ?Z , Y ?
O ?Z15: for all <x,y> pair where x ?
X ,y ?
Ydo16: ?Ux,y = U((S ?
x) ?
y)?
U(S)17: end for18: < x?, y?
>= argmax ?Ux,y19: S ?
(S ?
x?)
?
y?20: end if21: end whilepreference.
Besides, the scenario of small corpus isnot quite practical for the exponential growing web.Therefore, we test IPS on large real world datasets.We build 4 news story sets which consist of docu-ments and reference summaries to evaluate our pro-posed framework empirically.
We downloaded 5197news articles from 10 selected sources.
As shown inTable 2, three of the sources are in UK, one of themis in China and the rest are in US.
We choose thembecause many of these websites provide handcraftedsummaries for their special reports, which serve asreference summaries.
These events belong to differ-ent categories of Rule of Interpretation (ROI) (Ku-maran and Allan, 2004).
Statistics are in Table 3.4.2 Experimental System Setups?
Preprocessing.
Given a collection of documents,we first decompose them into sentences.
Stop-wordsare removed and words stemming is performed.Then the word distributions can be calculated.?
User Interface Design.
Users are required tospecify the overall compression rate ?
and the sys-tem extracts ?|D| sentences according to user utility1346Figure 1: A demonstration system for Interactive Personalized Summarization when compression rate ?
is specified(e.g.
5%).
For convenience of browsing, we number the selected sentences (see in part 3).
Extracted semantic units,such as ?drilling mud?, are in bold and underlined format (see in part 1).
When the user clicks a sentence (part 4), theclicked sentence ID is kept in the click record (part 2).
Mis-clicked records revocation can be operated by clickingthe deletion icon ?X?
(see in part 3).
Once a sentence is clicked, user can track the sentence into the popup sourcedocument to examine the contexts.
The selected sentences are highlighted in the source documents (see in part 5).Table 2: News sources of 4 datasetsNews Sources Nation News Sources NationBBC UK Fox News USXinhua China MSNBC USCNN US Guardian UKABC US New York Times USReuters UK Washington Post USTable 3: Detailed basic information of 4 datasets.News Subjects #size #docs #RS Avg.L1.Influenza A 115026 2557 5 832.BP Oil Spill 63021 1468 6 763.Haiti Earthquake 12073 247 2 324.Jackson Death 37819 925 3 64#size: total sentence counts; #RS: the number of reference summaries;Avg.L: average length of reference summary measured in sentences.and traditional utility.
User utility is obtained frominteraction.
The system keeps the clicked sentencerecords and calculates the user feedback by Equa-tion (3) during every session.
Consider sometimesusers click into the summary due to confusion ormis-operations, but not their real interests.
The sys-tem supports click records revocation.
More detailsof the user interface is demonstrated in Figure 1.4.3 Evaluation MetricsWe include both subjective evaluation from 3 evalu-ators based on their personalized interests and pref-erence, and the objective evaluation based on thewidely used ROUGE metrics (Lin and Hovy, 2003).Evaluator JudgmentsEvaluators are requested to express an opinionover all summaries based on the sentences whichthey deem to be important for the news.
In generala summary can be rated in a 5-point scale, where?1?
for ?terrible?, ?2?
for ?bad?, ?3?
for ?normal?,?4?
for ?good?
and ?5?
for ?excellent?.
Evaluatorsare allowed to judge at any scores between 1 and 5,e.g.
a score of ?3.3?
is adopted when the evaluatorfeels difficult to decide whether ?3?
or ?4?
is more1347appropriate but with preference towards ?3?.ROUGE EvaluationThe DUC usually officially employs ROUGEmeasures for summarization evaluation, which mea-sures summarization quality by counting overlap-ping units such as the N-gram, word sequences, andword pairs between the candidate summary and thereference summary.
We use ROUGE-N as follows:ROUGE-N =?S?{RefSum}?N-gram?SCountmatch(N-gram)?S?
{RefSum}?N-gram?SCount (N-gram)whereN stands for the length of the N-gram and N-gram?RefSum denotes the N-grams in the referencesummaries while N-gram?CandSum denotes the N-grams in the candidate summaries.
Countmatch(N-gram) is the maximum number of N-gram in thecandidate summary and in the set of reference sum-maries.
Count(N-gram) is the number of N-grams inthe reference summaries or candidate summary.According to (Lin and Hovy, 2003), among allsub-metrics in ROUGE, ROUGE-N (N=1, 2) is rela-tively simple and works well.
In this paper, we eval-uate our experiments using all methods provided bythe ROUGE package (version 1.55) and only reportROUGE-1, since the conclusions drawn from differ-ent methods are quite similar.
Intuitively, the higherthe ROUGE scores, the similar two summaries are.4.4 Algorithms for ComparisonWe implement the following widely used multi-document summarization algorithms as the baselinesystems, which are all designed for traditional sum-marization without user interaction.
For fairness weconduct the same preprocessing for all algorithms.Random: The method selects sentences ran-domly for each document collection.Centroid: The method applies MEAD algorithm(Radev et al, 2004) to extract sentences according tothe following parameters: centroid value, positionalvalue, and first-sentence overlap.GMDS: The Graph-based MDS proposed by(Wan and Yang, 2008) first constructs a sentenceconnectivity graph based on cosine similarity andthen selects important sentences based on the con-cept of eigenvector centrality.IPSini: The initial generated summary from IPSmerely models coverage and diversity utility, whichis similar to the previous work described in (Allan etal., 2001) with different goals and frameworks.IPS: Our proposed algorithms with personaliza-tion component to capture interest by user feed-backs.
IPS generates summaries via iterative sen-tence substitutions within user interactive sessions.RefSum: As we have used multiple referencesummaries from websites, we not only provideROUGE evaluations of the competing systems butalso of the reference summaries against each other,which provides a good indicator of not only theupper bound ROUGE score that any system couldachieve, but also human inconsistency among refer-ence summaries, indicating personalization.4.5 Overall Performance ComparisonWe take the average ROUGE-1 performance and hu-man ratings on all sets.
The overall results are shownin Figure 2 and details are listed in Tables 4?6.Figure 2: Overall performance on 6 datasets.From the results, we have following observations:?
Random has the worst performance as expected,both in ROUGE-1 scores and human judgements.?
The ROUGE-1 and human ratings of Centroidand GMDS are better than those of Random.
This ismainly because the Centroid based algorithm takesinto account positional value and first-sentence over-lap, which facilitates main aspects summarizationand PageRank-based GMDS ranks the sentence us-ing eigenvector centrality which implicitly accountsfor information subsumption among all sentences.?
In general, the GMDS system slightly outper-forms Centroid system in ROUGE-1, but the humanjudgements of GMDS and Centroid are of no signifi-cant difference.
This is probably due to the difficulty1348Table 4: Overall performance comparison on Influenza A.ROI?
category: Science.Systems R-1 95%-conf.
H-1 H-2 H-3RefSum 0.491 0.44958 3.5 3.0 3.9Random 0.257 0.75694 1.2 1.0 1.0Centroid 0.331 0.45073 2.5 3.0 3.5GMDS 0.364 0.33269 3.0 2.7 3.5IPSini 0.302 0.21213 2.0 2.5 2.5IPS 0.337 0.46757 4.8 4.5 4.5Table 5: Overall performance comparison on BP OilLeak.
ROI category: Accidents.Systems R-1 95%-conf.
H-1 H-2 H-3RefSum 0.517 0.48618 4.0 3.3 3.9Random 0.262 0.64406 1.5 1.0 1.5Centroid 0.369 0.34743 3.2 3.0 3.5GMDS 0.389 0.43877 3.5 3.0 3.9IPSini 0.327 0.53722 3.0 2.5 3.0IPS 0.372 0.35681 4.8 4.5 4.5Table 6: Overall performance comparison on Haiti Earth-quake.
ROI category: Disasters.Systems R-1 95%-conf.
H-1 H-2 H-3RefSum 0.528 0.30450 3.8 4.0 4.0Random 0.266 0.75694 1.5 1.5 1.8Centroid 0.362 0.43045 3.6 3.0 4.0GMDS 0.380 0.33694 3.9 3.5 4.0IPSini 0.331 0.34120 2.8 2.5 3.0IPS 0.391 0.40069 5.0 4.7 5.0Table 7: Overall performance comparison on MichaelJackson Death.
ROI category: Legal Cases.Systems R-1 95%-conf.
H-1 H-2 H-3RefSum 0.482 0.47052 3.5 3.5 4.0Random 0.232 0.52426 1.2 1.0 1.5Centroid 0.320 0.21045 3.0 2.5 2.7GMDS 0.341 0.30070 3.5 3.3 3.9IPSini 0.287 0.48526 2.5 2.0 2.2IPS 0.324 0.36897 5.0 4.5 4.8?ROI: news categorization defined by Linguistic Data Consortium.Available at http://www.ldc.upenn.edu/projects/tdt4/annotationof human judgements on comparable summaries.?
The results of ROUGE-1 and ratings for IPSiniare better than Random but worse than Centroid andGMDS.
The reason in this case may be that IPSinidoes not capture sufficient attributes: coverage anddiversity are merely fundamental requirements.?
Traditional summarization considers sentenceselection based on corpus only, and hence neglectsTable 8: Ratings consistency between evaluators: mean?
standard deviation over the 4 datasets.RefSum Evaluator 1 Evaluator 2 Evaluator 3Evaluator 1 0.35?0.09 0.30?0.33Evaluator 2 0.50?0.14Random Evaluator 1 Evaluator 2 Evaluator 3Evaluator 1 0.23?0.04 0.20?0.02Evaluator 2 0.33?0.06Centroid Evaluator 1 Evaluator 2 Evaluator 3Evaluator 1 0.45?0.03 0.50?0.12Evaluator 2 0.55?0.11GMDS Evaluator 1 Evaluator 2 Evaluator 3Evaluator 1 0.35?0.02 0.35?0.03Evaluator 2 0.70?0.03IPSini Evaluator 1 Evaluator 2 Evaluator 3Evaluator 1 0.45?0.01 0.25?0.04Evaluator 2 0.30?0.06IPS Evaluator 1 Evaluator 2 Evaluator 3Evaluator 1 0.35?0.01 0.18?0.02Evaluator 2 0.28?0.04user interests.
Many sentences are extracted due toarbitrary assumption of reader preference, which re-sults in a low user satisfaction.
Human judgementsunder our proposed IPS framework greatly outper-form baselines, indicating that the appropriate useof human interests for summarization are beneficial.The ROUGE-1 performance for IPS is not as idealas that of GMDS.
This situation may result from thedivergence between user interests and general infor-mation provided by mass media propaganda, whichagain motivates the need for personalization.Although the high disparities between differenthuman evaluators have been observed in (Gong andLiu, 2001), we still examine the consistency among3 evaluators and their preferred summaries to provethe motivation of personalization in our work.4.6 Consistency Analysis for PersonalizationThe low ROUGE-1 scores of RefSum indicate theinconsistency among reference summaries.
We con-duct personalization analysis from two perspectives:(1) human rating consistency and (2) content consis-tency among human supervised summaries.We calculate the mean and variance of rating vari-ations among evaluator judgements, listed in Table1349Table 9: Content consistency among evaluators super-vised summaries.Evaluator 1 Evaluator 2 Evaluator 3Evaluator 1 0.273 0.398Evaluator 2 0.289 0.257Evaluator 3 0.407 0.235RefSum 0.365 0.302 0.3948.
We see that for Random the average rating vari-ation is 0.25, for IPS is 0.27, for IPSini is 0.33, forRefSum is 0.38, for GMDS is 0.47 and for Centroidis the highest, 0.50.
Such phenomenon indicatesfor poor generated summaries, such as Random orIPSini, humans have consensus, but for normal sum-maries without personalized interests, they are likelyto have disparities, surprisingly, even for RefSum.General summaries provided by mass media satisfypart of audiences, but obviously not all of them.The high rating consistency of IPS indicates peo-ple tend to favor summaries generated according totheir interests.
We next examine content consistencyof these summaries with high rating consistency.As shown in Table 9, although highly scored,these human supervised summaries still have lowcontent consistency (especially Evaluator 2).
Thelow content consistency between RefSum and su-pervised summaries shows reader have individualpersonalization.
Note that the inconsistency amongevaluators is larger than that between RefSum andsupervised summaries, indicating interests take ahigh proportion in evaluator supervised summaries.4.7 Parameter Settings?
controls coverage/diversity tradeoff.
We tune ?
onIPSini and apply the optimal ?
directly in IPS.
Ac-cording to the statistics in (Yan et al, 2010), the se-mantic coherent context is about 7 sentences.
There-fore, we empirically choose k=3 for the examinedcontext window.
The number of topics is set atn=50.
We assign an equal weight (?
= 1) to seman-tic units and examined contexts according to analog-ical research of summarization from implicit feed-backs via clickthrough data (Sun et al, 2005).?
is the key parameter in IPS approach, control-ling the weight of user utility during the process ofinteractive personalized summarization.Through Figure 3, we see that when ?
is smallFigure 3: ?
v.s.
human ratings and ROUGE scores.(?
?
[0.01, 0.1]), both human judgements andROUGE evaluation scores have little difference.When ?
?
[0.1, 1], ROUGE scores increase signifi-cantly but human satisfaction shows little response.?
?
[1, 10] brings large user utility enhancement be-cause user may find what they are interested in butROUGE scores start to decay.
When ?
?
[10, 100],ROUGE scores drop much because the emphasizeduser interests may guide the generated summariesdivergent away from the original corpus.In Figure 4 we examine how ?
attracts user clicksand regeneration counts until satisfaction.
As the re-sult indicates, both counts increase as ?
increases.When ?
is small (from 0.01 to 0.1), readers findno more interesting aspects through clicks and re-generations and stop due to the bad user experience.As ?
increases, the system mines more relevant sen-tences according to personalized interests and henceattracts user clicks and intention to regenerate.Figure 4: ?
v.s.
click counts and regeneration counts.13505 ConclusionWe present an important and novel summariza-tion problem, Interactive Personalized Summariza-tion (IPS), which generates summaries based onhuman?system interaction for ?interests?
and per-sonalization.
We formally formulate IPS as a combi-nation of user utility and traditional summary utility,such as coverage and diversity.
We implement a sys-tem under such framework for experiments on realweb datasets to compare all approaches.
Throughour experiments we notice that user personalizationof interests plays an important role in summary gen-eration, which largely increase human ratings due touser satisfaction.
Besides, our experiments indicatethe inconsistency between user preferred summariesand reference summaries measured by ROUGE, andhence prove the effectiveness of personalization.AcknowledgmentsThis work was partially supported by HGJ 2010Grant 2011ZX01042-001-001 and NSFC with GrantNo.61073082, 60933004.
Rui Yan was supported bythe MediaTek Fellowship.ReferencesJames Allan, Rahul Gupta, and Vikas Khandelwal.
2001.Temporal summaries of new topics.
In Proceedings ofthe 24th annual international SIGIR?01, pages 10?18.D.M.
Blei, A.Y.
Ng, and M.I.
Jordan.
2003.
Latentdirichlet alocation.
The Journal of Machine LearningResearch, 3:993?1022.G.
Erkan and D.R.
Radev.
2004.
Lexpagerank: Prestigein multi-document text summarization.
In Proceed-ings of EMNLP?04, volume 4.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, andJaime Carbonell.
1999.
Summarizing text documents:sentence selection and evaluation metrics.
In Proceed-ings of SIGIR?99, pages 121?128.Yihong Gong and Xin Liu.
2001.
Generic text sum-marization using relevance measure and latent seman-tic analysis.
In Proceedings of the 24th internationalACM SIGIR conference, SIGIR ?01, pages 19?25.Q.
Guo and E. Agichtein.
2010.
Ready to buy or justbrowsing?
: detecting web searcher goals from inter-action data.
In Proceeding of the 33rd internationalACM SIGIR conference, SIGIR?10, pages 130?137.Giridhar Kumaran and James Allan.
2004.
Text clas-sification and named entities for new event detection.In Proceedings of the 27th annual international ACMSIGIR?04, pages 297?304.Anton Leuski, Chin-Yew Lin, and Eduard Hovy.
2003.ineats: interactive multi-document summarization.
InProceedings of ACL?03, pages 125?128.Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zha,and Yong Yu.
2009.
Enhancing diversity, cover-age and balance for summarization through structurelearning.
In Proceedings of WWW?09, pages 71?80.Chin-Yew Lin and Eduard Hovy.
2002.
From singleto multi-document summarization: a prototype systemand its evaluation.
In Proceedings of ACL?02, pages457?464.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proceedings of NAACL?03, pages 71?78.Annie Louis and Ani Nenkova.
2009.
Automaticallyevaluating content selection in summarization withouthuman models.
In EMNLP?09, pages 306?314.R.
Mihalcea and P. Tarau.
2005.
A language indepen-dent algorithm for single and multiple document sum-marization.
In Proceedings of IJCNLP, volume 5.D.R.
Radev, H. Jing, and M. Sty.
2004.
Centroid-basedsummarization of multiple documents.
InformationProcessing and Management, 40(6):919?938.Jian-Tao Sun, Dou Shen, Hua-Jun Zeng, Qiang Yang,Yuchang Lu, and Zheng Chen.
2005.
Web-page sum-marization using clickthrough data.
In Proceedings ofSIGIR?05, pages 194?201.Stephen Wan and Ce?cile Paris.
2008.
In-browser sum-marisation: generating elaborative summaries biasedtowards the reading context.
In ACL-HLT?08, pages129?132.Xiaojun Wan and Jianwu Yang.
2008.
Multi-documentsummarization using cluster-based link analysis.
InProceedings of SIGIR?08, pages 299?306.X.
Wan, J. Yang, and J. Xiao.
2007a.
Manifold-rankingbased topic-focused multi-document summarization.In Proceedings of IJCAI, volume 7, pages 2903?2908.X.
Wan, J. Yang, and J. Xiao.
2007b.
Single documentsummarization with document expansion.
In Proceed-ings of the 22nd AAAI?07, pages 931?936.Rui Yan, Yu Li, Yan Zhang, and Xiaoming Li.
2010.Event recognition from news webpages through latentingredients extraction.
In AIRS?10, pages 490?501.Rui Yan, Xiaojun Wan, Jahna Otterbacher, Liang Kong,Xiaoming Li, and Yan Zhang.
2011.
Evolution-ary timeline summarization: a balanced optimizationframework via iterative substitution.
In Proceedingsof the 34th annual international ACM SIGIR?11.Xin Zhao, Jing Jiang, Jing He, Yang Song, PalakornAchanauparp, Ee-Peng Lim, and Xiaoming Li.
2011.Topical Keyphrase Extraction from Twitter.
In Pro-ceedings of ACL-HLT?11.1351
