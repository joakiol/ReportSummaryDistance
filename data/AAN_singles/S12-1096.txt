First Joint Conference on Lexical and Computational Semantics (*SEM), pages 648?654,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsStanford: Probabilistic Edit Distance Metrics for STSMengqiu Wang and Daniel Cer?Computer Science DepartmentStanford UniversityStanford, CA 94305 USA{mengqiu,danielcer}@cs.stanford.eduAbstractThis paper describes Stanford University?ssubmission to SemEval 2012 Semantic Tex-tual Similarity (STS) shared evaluation task.Our proposed metric computes probabilisticedit distance as predictions of semantic sim-ilarity.
We learn weighted edit distance ina probabilistic finite state machine (pFSM)model, where state transitions correspond toedit operations.
While standard edit dis-tance models cannot capture long-distanceword swapping or cross alignments, we rectifythese shortcomings using a novel pushdownautomaton extension of the pFSM model.
Ourmodels are trained in a regression framework,and can easily incorporate a rich set of lin-guistic features.
The performance of our editdistance based models is contrasted with anadaptation of the Stanford textual entailmentsystem to the STS task.
Our results show thatthe most advanced edit distance model, pPDA,outperforms our entailment system on all butone of the genres included in the STS task.1 IntroductionWe describe a probabilistic edit distance based met-ric, which was originally designed for evaluatingmachine translation quality, for computing seman-tic textual similarity (STS).
This metric modelsweighted edit distance in a probabilistic finite statemachine (pFSM), where state transitions correspondto edit operations.
The weights of the edit op-erations are automatically learned in a regressionframework.
One of the major contributions of this?
Daniel Cer is one of the organizers for the STS task.
TheSTS test set data was not used in any way for the developmentor training of the systems described in this paper.paper is a novel extension of the pFSM model into aprobabilistic Pushdown Automaton (pPDA), whichenhances traditional edit-distance models with theability to model phrase shift and word swapping.Furthermore, we give a new log-linear parameteri-zation to the pFSM model, which allows it to easilyincorporate rich linguistic features.
We contrast theperformance of our probabilistic edit distance metricwith an adaptation of the Stanford textual entailmentsystem to the STS task.2 pFSMs for Semantic Textual SimilarityWe start off by framing the problem of semantic tex-tual similarity in terms of weighted edit distance cal-culated using probabilistic finite state machines (pF-SMs).
A FSM defines a language by accepting astring of input tokens in the language, and reject-ing those that are not.
A probabilistic FSM definesthe probability that a string is in a language, extend-ing on the concept of a FSM.
Commonly used mod-els such as HMMs, n-gram models, Markov Chainsand probabilistic finite state transducers all fall inthe broad family of pFSMs (Knight and Al-Onaizan,1998; Eisner, 2002; Kumar and Byrne, 2003; Vi-dal et al, 2005).
Unlike all the other applicationsof FSMs where tokens in the language are words, inour language tokens are edit operations.
A string oftokens that our FSM accepts is an edit sequence thattransforms one side of the sentence pair (denoted ass1) into the other side (s2).Our pFSM has a unique start and stop state, andone state per edit operation (i.e., Insert, Delete, Sub-stitution).
The probability of an edit sequence e isgenerated by the model is the product of the statetransition probabilities in the pFSM, formally de-648Figure 1: This diagram illustrates an example sentence pair from the statistical machine translation subtask of STS.The three rows below are the best state transition (edit) sequences that transforms REF to SYS, according to the basicpFSM model, the extended pPDA model, and pPDA model with synonym and paraphrase linguistic features.
Thecorresponding alignments generated by the models (pFSM, pPDA, pPDA+f ) are shown with different styled lines,with later models in the order generating strictly more alignments than earlier ones.
The gold human evaluation scoreis 6.5, and model predictions are: pPDA+f 5.5, pPDA 4.3, pFSM 3.1.scribed as:w(e | s1,s2) = ?|e|i=1 exp ?
?
f(ei?1,ei,s1,s2)Z(1)We featurize each of the state changes with a log-linear parameterization; f is a set of binary featurefunctions defined over pairs of neighboring states(by the Markov assumption) and the input sentences,and ?
are the associated feature weights; Z is a parti-tion function.
In this basic pFSM model, the featurefunctions are simply identity functions that emit thecurrent state, and the state transition sequence of theprevious state and the current state.The feature weights are then automaticallylearned by training a global regression model wherethe human judgment score for each sentence pair isthe regression target (y?).
Since the ?gold?
edit se-quence are not given at training or prediction time,we treat the edit sequences as hidden variables andsum over them in our model.
We introduce a newregression variable y ?
R which is the log-sum ofthe unnormalized weights (Eqn.
(1)) of all edit se-quences, formally expressed as:y = log?e??e?|e?
|?i=1exp ?
?
f(ei?1,ei,s1,s2) (2)e?
is the set of all possible alignments.
The sumover an exponential number of edit sequences in e?is solved efficiently using a forward-backward styledynamic program.
Any edit sequence that does notlead to a complete transformation of the sentencepair has a probability of zero in our model.
Ourregression target then seeks to minimize the leastsquares error with respect to y?, plus a L2-norm regu-larizer term parameterized by ?
:??
= min?
{?s1i ,s2i[y?i ?
(y|s1i |+ |s2i |+?
)]2 +???
?2}(3)The |s1i |+ |s2i | is a length normalization term forthe ith training instance, and ?
is a scaling con-stant whose value is to be learned.
At test time,y/(|s1|+ |s2|) + ?
is computed as the predictedscore.We replaced the standard substitution edit oper-ation with three new operations: Sword for sameword substitution, Slemma for same lemma substitu-tion, and Spunc for same punctuation substitution.
Inother words, all but the three matching-based substi-tutions are disallowed.
The start state can transitioninto any of the edit states with a constant unit cost,and each edit state can transition into any other editstate if and only if the edit operation involved is validat the current edit position (e.g., the model cannottransition into Delete state if it is already at the end649of s1; similarly it cannot transition into Slemma unlessthe lemma of the two words under edit in s1 and s2match).
When the end of both sentences are reached,the model transitions into the stop state and endsthe edit sequence.
The first row in Figure 1 start-ing with pFSM shows a state transition sequence foran example sentence pair.
1 There exists a one-to-one correspondence between substitution edits andword alignments.
Therefore this example state tran-sition sequence correctly generates an alignment forthe word 43 and people.2.1 pPDA ExtensionA shortcoming of edit distance models is that theycannot handle long-distance word swapping ?
apervasive phenomenon found in most natural lan-guages.
2 Edit operations in standard edit distancemodels need to obey strict incremental order intheir edit position, in order to admit efficient dy-namic programming solutions.
The same limitationis shared by our pFSM model, where the Markovassumption is made based on the incremental or-der of edit positions.
Although there is no knownsolution to the general problem of computing editdistance where long-distance swapping is permit-ted (Dombb et al, 2010), approximate algorithms doexist.
We present a simple but novel extension of thepFSM model to a probabilistic pushdown automa-ton (pPDA), to capture non-nested word swappingwithin limited distance, which covers a majority ofword swapping in observed in real data (Wu, 2010).A pPDA, in its simplest form, is a pFSM whereeach control state is equipped with a stack (Esparzaand Kucera, 2005).
The addition of stacks for eachtransition state endows the machine with memory,extending its expressiveness beyond that of context-free formalisms.
By construction, at any stage in anormal edit sequence, the pPDA model can ?jump?forward within a fixed distance (controlled by a maxdistance parameter) to a new edit position on eitherside of the sentence pair, and start a new edit subse-quence from there.
Assuming the jump was made on1It is safe to ignore the second and third row in Figure 1 fornow, their explanations are forthcoming in Section 2.1.2The edit distance algorithm described in Cormen etal.
(2001) can only handle adjacent word swapping (transpo-sition), but not long-distance swapping.the s2 side, 3 the machine remembers its current editposition in s2 as Jstart , and the destination positionon s2 after the jump as Jlanding.We constrain our model so that the only edit op-erations that are allowed immediately following a?jump?
are from the set of substitution operations(e.g., Sword).
And after at least one substitutionhas been made, the device can now ?jump?
back toJstart , remembering the current edit position as Jend .Another constraint here is that after the backward?jump?, all edit operations are permitted except forDelete, which cannot take place until at least onesubstitution has been made.
When the edit sequenceadvances to position Jlanding, the only operation al-lowed at that point is another ?jump?
forward opera-tion to position Jend , at which point we also clear allmemory about jump positions and reset.An intuitive explanation is that when pPDAmakes the first forward jump, a gap is left in s2 thathas not been edited yet.
It remembers where it leftoff, and comes back to it after some substitutionshave been made to complete the edit sequence.
Thesecond row in Figure 1 (starting with pPDA) illus-trates an edit sequence in a pPDA model that in-volves three ?jump?
operations, which are annotatedand indexed by number 1-3 in the example.
?Jump1?
creates an un-edited gap between word 43 andwestern, after two substitutions, the model makes?jump 2?
to go back and edit the gap.
The only editpermitted immediately after ?jump 2?
is deleting thecomma in s1, since inserting the word 43 in s2 beforeany substitution is disallowed.
Once the gap is com-pleted, the model resumes at position Jend by making?jump 3?, and completes the jump sequence.The ?jumps?
allowed the model to align wordssuch as western India, in addition to the alignmentsof 43 people found by the pFSM.
In practice, wefound that our extension gives a big boost to modelperformance (cf.
Section 4), with only a modest in-crease in computation time.
43Recall that we transform s1 into s2, and thus on the s2 side,we can only insert but not delete.
The argument applies equallyto the case where the jump was made on the other side.4The length of the longest edit sequence with jumps onlyincreased by 0.5 ?max(|s1|, |s2|) in the worst case, and by andlarge swapping is rare in comparison to basic edits.650Figure 2: Stanford Entailment Recognizer: The pipelined approach used by the Stanford entailment recognizer toanalyze sentence pairs and determine whether or not an entailment relationship is present.
The entailment recognizerfirst obtains dependency parses for both the passage and the hypothesis.
These parses are then aligned based uponlexical and structural similarity between the two dependency graphs.
From the aligned graphs, features are extractedthat suggest the presence or absence of an entailment relationship.
Figure courtesy of (Pado et al, 2009).2.2 Parameter EstimationSince the least squares operator preserves convexity,and the inner log-sum-exponential function is con-vex, the resulting objective function is also convex.For parameter learning, we used the limited mem-ory quasi-newton method (Liu and Nocedal, 1989)to find the optimal feature weights and scaling con-stant for the objective.
We initialized ?
= 0?, ?
= 0,and ?
= 5.
We also threw away features occurringfewer than five times in training corpus.
Gradientcalculation was similar to other pFSM models, suchas HMMs, we omitted the details here, for brevity.2.3 Rich Linguistic FeaturesWe add new substitution operations beyond thoseintroduced in Section 2, to capture synonyms andparaphrase in the sentence pair.
Synonym rela-tions are defined according to WordNet (Miller etal., 1990), and paraphrase matches are given by alookup table.
To better take advantage of paraphraseinformation at the multi-word phrase level, we ex-tended our substitution operations to match longerphrases by adding one-to-many and many-to-manybigram block substitutions.
In our experiments onmachine translation evaluation task, which our met-ric was originally developed for, we found that mostof the gain came from unigrams and bigrams, withlittle to no additional gains from trigrams.
There-fore, we limited our experiments to bigram pFSMand pPDA models, and pruned the paraphrase tableadopted from TERplus 5 to unigrams and bigrams,resulting in 2.5 million paraphrase pairs.
Trained onall available training data, the resulting pPDA modelhas a total of 218 features.2.4 Model ConfigurationWe evaluate both the pFSM and pPDA models withthe addition of rich linguistic features, as describedin the previous section.
For pPDA model, the jumpdistance is set to five.
For each model, we experi-mented with two different training schemes.
In the5Available from www.umiacs.umd.edu/~snover/terp.651HYP: Virus was infected.REF: No one was infected by the virus.no entailment no entailmentHYP: The virus did not infect anybody.REF: No one was infected by the virus.entailment entailmentFigure 3: Semantic similarity as determined by mutual textual entailment.
Figure courtesy of (Pado et al, 2009).first scheme, we train a separate model for each sec-tion of the training dataset (i.e., MSRpar, MSRvid,and SMTeuroparl), and use that model to test ontheir respective test set.
For the two unseen testsets (SMTnews and OnWN), we used a joint modeltrained on all of the available training data.
We re-fer to this scheme as Indi henceforth.
In the secondscheme, we used the joint model trained on all train-ing data to make preditions for all test sets (we referto this scheme as All).
Our official submission con-tains two runs ?
pFSM with scheme Indi, and pPDAwith scheme All.3 Textual Entailment for STSWe contrast the performance of the probabilistic editdistance metrics with an adaptation of the StanfordEntailment Recognizer to the STS task.
In this sec-tion, we review the textual entailment task, the op-eration of the Stanford Entailment Recognizer, anddescribe how we adapted our entailment system tothe STS task.3.1 Recognizing Textual EntailmentThe Recognizing Textual Entailment (RTE) task(Dagan et al, 2005) involves determining whetherthe meaning of one text can be inferred from an-other.
The text providing the ground truth for theevaluation is known as the passage while the textbeing tested for entailment is known as the the hy-pothesis.
A passage entails a hypothesis if a casualspeaker would consider the inference to be correct.This intentionally side-steps strict logical entailmentand implicitly brings in all of the world knowledgespeakers use to interpret language.The STS task and RTE differ in two significantways.
First, the RTE task is one directional.
If ahypothesis sentence is implied by a passage, the in-verse does not necessarily hold (e.g., ?John is out-side in the snow without a coat.?
casually implies?John is cold?, but not vice versa).
Second, the RTEtask forces systems to make a boolean choice aboutentailment, rather than the graded scale of semanticrelatedness implied by STS.3.2 Textual Entailment System DescriptionShown in Figure 2, the Stanford entailment sys-tem uses a linguistically rich multi-stage annotationpipeline.
Incoming sentence pairs are first depen-dency parsed.
The dependency parse trees are thentransformed into semantic graphs containing addi-tional annotations such as named entities and coref-erence.
The two semantic graphs are then alignedbased upon structural overlap and lexical semanticsimilarity using a variety of word similarity metricsbased on WordNet, vector space distributional sim-ilarity as calculated by InfoMap, and a specializedmodule for matching ordinal values.
The systemthen supplies the aligned semantic graphs as input toa number of feature producing modules.
Some mod-ules produce gross aggregate scores, such as return-ing the alignment quality between the two sentences.Others look for specific phenomena that suggest thepresence or absence of an entailment relationship,such as a match or mismatch in polarity (e.g., ?died?vs.
?didn?t die?
), tense, quantification, and argumentstructure.
The resulting features are then passed onto a down stream classifier to predict whether or notan entailment relationship exists.3.3 Adapting RTE to STSIn order to adapt our entailment recognition sys-tem to STS, we follow the same approach Padoet al (2009) used to successfully adapt the entail-ment system to machine translation evaluation.
Asshown in Figure 3, for each pair of sentences pre-sented to the system, we run the entailment systemin both directions and extract features that describewhether the first sentence entails the second and viceversa for the opposite direction.
This setup effec-tively treats the STS task as a bidirectional variantof the RTE task.
The extracted bidirectional entail-ment features are then passed on to a support vec-652Models All MSRpar MSRvid SMTeuro OnWn SMTnewspFSMIndi 0.6354(38) 0.3795 0.5350 0.4377 - -pFSMAll 0.3727 0.3769 0.4569 0.4256 0.6052 0.4164pPDAIndi 0.6808 0.4244 0.5051 0.4554 - -pPDAAll 0.4229(77) 0.4409 0.4698 0.4558 0.6468 0.4769Entailment 0.5589(55) 0.4374 0.8037 0.3533 0.3077 0.3235Table 1: Absolute score prediction results on STS12 test set.
Numbers in this table are Pearson correlation scores.
Bestresult on each test set is highlighted in bold.
Numbers in All column that has superscript are the official submissions.Their relative rank among 89 systems in shown in parentheses.tor machine regression (SVR) model, which predictsthe STS score for the sentence pair.
As in Pado etal.
(2009), we augment the bidirectional entailmentfeatures with sentence level BLEU scores, in orderto improve robustness over noisy non-grammaticaldata.
We trained the SVR model using libSVM overall of the sentence pairs in the STS training set.
Themodel uses a Gaussian kernel with ?
= 0.125, anSVR ?-loss of 0.25, and margin violation cost, C, of2.0.
These hyperparameters were selected by crossvalidation over the training set.4 ResultsFrom Table 1, we can see that the pPDA modelperformed better than the pFSM model on all testsets except the MSRvid section.
This result clearlydemonstrates the power of the pPDA extensionin modeling long-distance word swapping.
TheMSRvid test set has the shortest overall sentencelength (13, versus 35 forMSRpar), and therefore it isnot too surprising that long distance word swappingdid not help much here.
Furthermore, the pPDAmodel shows a much more pronounced performancegain than pFSM when tested on unseen datasets(OnWn and SMTnews), suggesting that the pPDAmodel is more robust across domain.
A second ob-servation is that the Indi training scheme seems towork better than the All approach, which shows hav-ing more training data does not compensate the dif-ferent characteristics of each training portion.
Ourbest metric on all test set is the pPDAIndi model,with a Pearson?s correlation score of 0.6808.
Ifinterpolated into the official submitted runs rank-ing, it would be placed at the 22nd place among89 runs.
Among the three official runs submittedto the shared task (pPDAAll, pFSMIndi and En-tailment), pFSMIndi performs the best, placed at38th place among 89 runs.
Since our metrics wereoriginally designed for statistical machine transla-tion (MT) evaluation, we found that on the unseenSMTNews test set, which consists of news conversa-tion sentence pairs from the MT domain, our pPDAmodel placed at a much higher position (13 among89 runs).In comparison to results on MT evaluationtask (Wang and Manning, 2012), we found that thepPDA and pFSM models work less well on STS.Whereas in MT evaluation it is common to haveaccess to thousands of training examples, there isan order of magnitude less available training datain STS.
Therefore, learning hundreds of feature pa-rameters in our models from such few examples arelikely to be ill-posed.Overall, the RTE system did not perform as wellas the regression based models except for MSRviddomain , which has the shortest overall sentencelength.
Our qualitative evaluation suggests thatMSRvid domain seems to exhibit the least degree oflexical divergence between the sentence pairs, thusmaking this task easier than other domains (the me-dian score of all 89 official systems for MSRvidis 0.7538, while the median for MSRpar and SM-Teuroparl is 0.5128 and 0.4437, respectively).
Therelative rank of RTE for MSRvid is 21 among 89,whereas the pFSM and pPDA systems ranked 80 and83, respectively.
The low performance of pFSM andpPDA on this task significantly affected the rankingof these two systems on the ALL evaluation measure.We do not have a clear explanation why RTE systemthrives on this easier task while pPDA and pFSMsuffers.
In the future, we aim to gain a better under-standing of the characteristics of the two differentsystems, and explore combination techniques.6535 ConclusionWe describe a metric for computing sentence levelsemantic textual similarity, which is based on aprobabilistic finite state machine model that com-putes weighted edit distance.
Our model admits arich set of linguistic features, and can be trained tolearn feature weights automatically by optimizinga regression objective.
A novel pushdown automa-ton extension was also presented for capturing long-distance word swapping.
Our models outperformedStanford textual entailment system on all but one ofthe genres on the STS task.AcknowledgementsWe gratefully acknowledge the support of theDefense Advanced Research Projects Agency(DARPA) Machine Reading Program under AirForce Research Laboratory (AFRL) prime contractno.
FA8750-09-C-0181 and the support of theDARPA Broad Operational Language Translation(BOLT) program through IBM.
Any opinions, find-ings, and conclusion or recommendations expressedin this material are those of the author(s) and do notnecessarily reflect the view of the DARPA, AFRL,or the US government.ReferencesT.
H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.2001.
Introduction to Algorithms, Second Edition.MIT Press.I.
Dagan, O. Glickman, and B. Magnini.
2005.
ThePASCAL recognising textual entailment challenge.
InProceedings of the PASCAL Challenges Workshop onRecognising Textual Entailment.Y.
Dombb, O. Lipsky, B. Porat, E. Porat, and A. Tsur.2010.
The approximate swap and mismatch edit dis-tance.
Theoretical Computer Science, 411(43).J.
Eisner.
2002.
Parameter estimation for probabilisticfinite-state transducers.
In Proceedings of ACL.J.
Esparza and A. Kucera.
2005.
Quantitative analysisof probabilistic pushdown automata: Expectations andvariances.
In Proceedings of the 20th Annual IEEESymposium on Logic in Computer Science.K.
Knight and Y. Al-Onaizan.
1998.
Translation withfinite-state devices.
In Proceedings of AMTA.S.
Kumar and W. Byrne.
2003.
A weighted finite statetransducer implementation of the alignment templatemodel for statistical machine translation.
In Proceed-ings of HLT/NAACL.D.
C. Liu and J. Nocedal.
1989.
On the limited mem-ory BFGS method for large scale optimization.
Math.Programming, 45:503?528.G.
A. Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.
J. Miller.
1990.
WordNet: an on-line lexicaldatabase.
International Journal of Lexicography, 3(4).S.
Pado, D. Cer, M. Galley, D. Jurafsky, and C. Man-ning.
2009.
Measuring machine translation quality assemantic equivalence: A metric based on entailmentfeatures.
Machine Translation, 23:181?193.E.
Vidal, F. Thollard, C. de la Higuera, F. Casacuberta,and R. C. Carrasco.
2005.
Probabilistic finite-statemachines part I. IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 27(7):1013?1025.M.
Wang and C. Manning.
2012.
SPEDE: Probabilisticedit distance metrics for sentence level MT evaluation.In Proceedings of WMT.D.
Wu, 2010.
CRC Handbook of Natural Language Pro-cessing, chapter How to Select an Answer String?,pages 367?408.
CRC Press.654
