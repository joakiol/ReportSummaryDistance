Cross-Document Coreference on a Large Scale CorpusAbstractIn this paper, we will compare and evaluate theeffectiveness of different statistical methods in thetask of cross-document coreference resolution.
Wecreated entity models for different test sets andcompare the following disambiguation andclustering techniques to cluster the entity models inorder to create coreference chains:Incremental Vector SpaceKL-DivergenceAgglomerative Vector Space1.
IntroductionCoreference analysis refers to the process ofdetermining whether or not two mentions of entitiesrefer to the same person (Kibble and Deemter, 2000).For example, consider the following short passage oftext:John Smith was appointed chair of the committee.Because of his past experience, Mr. Smith seemed theperfect choice.
His good friend John, however, was notconsidered.Coreference analysis attempts to decide whetherJohn Smith and Mr. Smith refer to the same person, andwhether John is also the same person.
The task is oftenextended to include references such as his or even hisgood friend, though we do not explore that extension inthis study.Addressing this problem is important to supportsystems such as those that search for, extract, andprocess mentions of ?people of interest?
in news ortranscripts (BBN 2001), or for other informationorganization tasks that might benefit from preciseknowledge of how names occur, such as TopicDetection and Tracking (Allan 2002).Cross-document coreference analysis pushes the taskinto considering whether mentions of a name indifferent documents are the same.
The problembecomes more complex because documents might comefrom different sources, will probably have differentauthors and different writing conventions andstyles(Bagga and Baldwin,1998), and may even be indifferent languages.There has been little published work on cross-document coreference analysis and that has generallybeen evaluated on a small corpus of documents.
Amajor contribution of this work is to develop asubstantially larger (more than two orders of magnitude)corpus for evaluation.
We show that the previousapproach is effective but that a variation on it,agglomerative vector space, provides improved andmuch more stable results.We begin in Section 2 by describing how cross-document coreference analysis is evaluated.
We sketchprior work in Section 3 and describe our two evaluationcorpora in Section 4.
Section 5 discusses the threealgorithms that we explore for this task and then Section6 describes our experimental results on both corpora.
InSection 7 we provide some additional analysis thatattempts to explain some surprising results.
Weconclude in Section 8 with a description of our plans forfuture work.2.
EvaluationGiven a collection of named entities fromdocuments, the coreferencing task is to put them intoequivalence classes, where every mention in the sameclass refers to the same entity (person, location,organization, and so on).
The classes are referred to as?coreference chains?
because the entities are chainedtogether.To evaluate the coreference chains emitted by asystem, we need truth data: the chains of entities that areactually referring to the same person.
Evaluation thenproceeds by comparing the true chains to the system?shypothesized chains.We use the B-CUBED scoring algorithm (Bagga andBaldwin 1998) because it is the one used in thepublished research.
The algorithm works as follows.For each entity mention e in the evaluation set, wefirst locate the truth chain TC that contains that mention(it can be in only one truth chain) and the system?shypothesized chain HC that contains it (again, there canChung Heong Gooi and James AllanCenter for Intelligent Information RetrievalDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003{cgooi,allan}@cs.umass.edube only one hypothesis chain).
We then compute aprecision and recall score for those two chains.Precision is the proportion of mentions in HC that arealso in TC and recall is the proportion of mentions in TCthat are also in HC.
If the chains match perfectly, recalland precision will both be one.
If the hypothesis chaincontains only the single mention e, then its precisionwill be one, and its recall will be 1/|TC|, the inverse ofthe size of the truth chain.
Note that it is not possible tohave a precision or recall of zero since entity e is alwaysin common between the two chains.
Ourimplementation of the B-CUBED algorithm is usedspecifically to evaluate an existing set of coreferencechains and does not utilize any smoothing to handlesystem output which contains no entities.Overall precision and recall values are determined byaveraging the individual values over all mentions in theevaluation set.
These are the primary evaluationmeasures for cross-document coreference analysis.3.
Related WorkTIPSTER Phase III first identified cross-documentcoreference as an area for research since it is a centraltool to drive the process of producing summaries frommultiple documents and for information fusion (Baggaand Baldwin, 1998).
The Sixth Message UnderstandingConference (MUC-6) identified cross-documentcoreference as a potential task but it was not includedbecause it was considered to be too difficult (Bagga andBaldwin, 1998).ISOQuest?s NetOwl and IBM?s Textract attemptedto determine whether multiple named entities refer tothe same entity but neither had the ability to distinguishdifferent entities with the same name.
Entity detectionand tracking looks at the same tasks as cross documentcoreferencing.Much of the work in this study is based on that byBagga and Baldwin (1998), where they presented asuccessful cross-document coreference resolutionalgorithm to resolve ambiguities between people havingthe same name using the vector space model.
We haveimplemented a simplified version of their algorithm thatachieves roughly equivalent accuracy, but will showthat the algorithm does not work as well when translatedto a substantially larger corpus of documents.There has been significant work recently in theinformation extraction community on a problem knownas Entity Detection and Tracking within the AutomaticContent Extraction (ACE) evaluations (NIST 2003).That work includes an optional sub-task referred toalternately as either Entity Tracking or Entity MentionDetection.
The goal is to pull together all mentions ofthe same entity across multiple documents.
This task isa small and optional part of the complete ACEevaluation and results from it do not appear to bepublished.4.
CorporaTo evaluate the effectiveness of our varioustechniques for cross document coreference, we use thesame ?John Smith?
corpus created by Bagga andBaldwin (1998).
In addition, we created a larger, richerand highly ambiguous corpus that we call the ?Person-xcorpus.
?4.1 John Smith CorpusBagga and Baldwin tested their coreferencealgorithm against a set of 197 articles from 1996 and1997 editions of the New York Times, all of which referto ?John Smith?.
All articles either contain the nameJohn Smith or some variation with a middle name orinitial.
There are 35 different John Smiths mentioned inthe articles.
Of these, 24 refer to a unique John Smithentity which is not mentioned in the other 173 articles(197 minus 24).We present results on this corpus for comparisonwith past work, to show that our approximation of thosealgorithms is approximately as effective as the originals.The corpus also permits us to show how our additionalalgorithms compare on that data.
However, ourprimarily evaluation corpus is the larger corpus that wenow discuss.4.2 Person-x CorpusSince the task of annotating documents is timeconsuming, we used a different technique to create alarge test set with different entities of the same name.The technique used to construct this corpus is similar tothe well known technique of creating artificial sensetagged corpora.
Artificial sense tagged corpora is usedto evaluate word sense disambiguation algorithms and iscreated by adding ambiguity to a corpus.
Similarly, weconsider the task of coreferencing multiple occurrencesof ?John Smith?
to be similar to coreferencing multipleoccurrences of ?person-x?, where the occurrences of?person-x?
are a disguise of multiple named entitiessuch as ?George Bush?
and ?Saddam Hussein?.
Thisapproach simplifies the task of looking for a largecollection of ?John Smith?
articles and obtaining theactual coreference links between the many ?JohnSmith?
entities.
It also allows us to create a vastlylarger corpus of documents mentioning the ?sameperson.
?We first obtained from 10,000 to 50,000 uniquedocuments from the TREC 1, 2 and 3 volumes using theInquery search engine from UMass Amherst for each ofthe following subjects: art, business, education,government, healthcare, movies, music, politics,Number ofoccurrencesPercentageof entities1 46.662 18.783 9.034 4.555 1.866 1.167 0.838 0.469 or more 16.67Table 1: Breakdown of distribution by number ofoccurrences within the Person X corpus.religion, science and sports.
Then, we ran thedocuments through Identifinder, a named entityextraction system developed by BBN, to tag the namedentities in the documents.Next, we selected one person entity randomly fromeach document.
Since Identifinder occasionally tags anentity incorrectly, we manually went through eachselection to filter out entities that were not people?snames.
We also manually filter out cases where thetagged entity is only one word (e.g., John, Alex, etc.
).We replaced the occurrences of the selected entity ineach document with ?person-x?
as follows:In the late 1970s, the company hired producers<enamex type="person">jon peters</enamex>  and<enamex type="person">peter guber </enamex>  tostart a studio from scratch.In the late 1970s, the company hired producers<enamex type="person">jon peters</enamex>  and<enamex type="person"> person-x </enamex>  to starta studio from scratch.We also replaced all additional occurrences of thesame name and names that matched (except for amiddle initial or name) in that document with ?person-x?.
For example, in the case above, other occurrencesof Peter Guber or names such as Peter X. Guber wouldbe replaced by ?person-x?.We now have a large set of documents containing areference to ?Person X?
and we know for eachdocument ?which?
person entity it is referring to.
Weactually verified that names of the same name were thesame entity, though with the large number of entities,the task was potentially overwhelming.
However, sincethe entities are categorized according to domain (by thequery that found the document), determining the actualcoreference links becomes significantly easier.
In anarticle discussing sports, the multiple occurrences of thename ?Michael Chang?
are most likely to be pointing tothe tennis player?and the same tennis player.These mappings from ?Person X?
to their true nameswill serve as our evaluation/true coreference chains.Since we know the name that ?Person X?
replaced, weassume that if those names are identical, they refer tothe same person.
So all references to ?Person X?
thatcorrespond to, say, ?Bill Clinton,?
will be put into thesame coreference chain.We manually removed documents whose Person Xentity pointed to a different person than the person in itscorresponding chain.
Consider the scenario where wehave four documents, three of which contains Person Xentities pointing to John Smith (president of GeneralElectric Corporation) and the other pointing to JohnSmith (the character in Pocahontas).
The last JohnSmith document will be removed from the chain and theentire corpus.
The final Person X corpus contains34,404 unique documents.
Hence, there are 34,404?Person X?s in the corpus and they point to 14,767different actual entities.
15.24% of the entities occur inmore than one domain subject.Table 1 displays the distribution of entities versustheir occurrences in our corpus.
Slightly over 46% ofentities occur only once in the collection of 34,404entities.
That compares to about 12% in the John Smithcorpus.
Of the total of 315,415 unique entities thatIdentifinder recognized in the entire corpus, just under49% occurred precisely once, so our sample appears tobe representative of the larger corpus even if it does notrepresent how ?John Smith?
appears.A potential shortcoming that was noted is thatvariation of names such as ?Bob Matthews?
versus?Robert Matthews?
may have been missed during theconstruction of this corpus.
However, this problem didnot show up in a set entities randomly sampled foranalysis.5.
MethodologyIn all cases, we represented the mention of an entity(i.e., an occurrence of ?John Smith?
or ?Person-x?depending on the corpus used) by the words around alloccurrences of the entity in a document.
Based onexploratory work on training data, we choose a windowof 55 words centered on each mention, merged those,and called the result a ?snippet.?
(In many cases thesnippet incorporates text from only a single occurrenceof the entity, but there are documents that contain 2-3?person-x?
instances, and those are merged together.
)We then employ three different methods for comparingsnippets to determine whether their correspondingmentions are or are not to the same entity.
In theremainder of this section, we describe the threemethods: incremental vector space, KL divergence, andagglomerative vector space.5.1.
Incremental vector spaceOur intent with the incremental vector space modelis to approximate the work reported by Bagga andBaldwin (1998).
Their system takes as input properlyformatted documents and uses the University ofPennsylvania?s CAMP system to perform within-document coreference resolution, doing more carefulwork to find additional mentions of the entity in thedocument.
It then extracts all sentences that are relevantfor each entity of interest based on the within-documentcoreference chains produced by CAMP.
The sentencesextracted form a summary that represents the entity (incontrast to our 55-word snippets).
The system thencomputes the similarity of that summary with each ofthe other summaries using the vector space model.
If thesimilarity computed is above a predefined threshold,then the two summaries are considered to be coreferent.Each of the summaries was stored as a vector ofterms.
The similarity between two summaries S1 and S2is computed as by the cosine of the angle between theircorresponding vectors.
Terms are weighted by a tf-idfweight as tf?log(N/df), where tf is the number of timesthat a term occurs in the summary, N is the total numberof documents in the collection, and df is the number ofdocuments that contain the term.Because we did not have the same within-documentcoreference tools, we opted for a simpler variation onBagga and Baldwin?s approach.
In our implementation,we represent snippets (combined 55-word sections oftext) as vectors and use this model to represent eachentity.
We calculated term weights and similarity in thesame way, however.
The only difference is the textused to represent each mention.For both cases, the system operates incrementally onthe list of entities as follows.
We first create onecoreference chain containing a single entity mention(one vector).
We then take the next entity vector andcompare it against the entity in the link.
If the twovectors have a similarity above a pre-defined threshold,then they are regarded to be referring to the same entityand the latter will be added into the same chain.Otherwise, a new coreference link is created for theentity.We continue creating links using this incrementalapproach until all of the entities have been clustered intoa chain.
At each step, a new entity is compared againstall existing coreference chains and is added into thechain with the highest average similarity if it is abovethe predefined threshold.
Our implementation differsfrom that of Bagga and Baldwin in the following ways:Bagga and Baldwin use a single-link technique tocompare an entity with the entities in a coreferencechain.
This means they include an entity into a chain assoon as they find one pair-wise entity to entitycomparison that is above the predefined threshold.
Weutilize an average-link comparison and compared anentity to each other entity in a coreference chain, thenused the average similarity to determine if the entityshould be included into the chain.They utilized the CAMP system developed by theUniversity of Pennsylvania to resolve within documentcoreferencing and extract a summary for each entity.
Inour system, we simply extract the snippets for eachentity and do not depend on within documentcoreferencing of an entity.5.2 KL DivergenceThe second technique that we implemented for entitydisambiguation was based on Kullback-LeiblerDivergence.
For this technique, we represent thesnippets in the form of a probability distribution ofwords, creating a so-called entity language model (Allanand Raghavan, 2002).
The KL divergence is a classicmeasure of the ?distance?
between two probabilitydistributions.
The more dissimilar the distributions are,the higher the KL divergence.
It is given by theequation:?=x xrxqxqrqD )()(log)()||(where x ranges over the entire vocabulary.
The smallerthe distance calculated by KL divergence, the moresimilar a document is with another.
If the distance is 0,then the two distributions are identical.
To deal withzero probabilities, we need some type of smoothing, andwe chose to use the asymmetric skew divergence,mixing one distribution with the other as determined bya ?
(Lee,2001): D(r || ?q + (1 ?
?
)r)Skew divergence best approximates KL divergencewhen the parameter ?
is set to a value close to 1.
In ourexperiments, we let ?=0.9We used the incremental approach of Section 5.1,but with probability distributions.
Each of thedistributions created (from a snippet) was evaluatedagainst the distributions for existing coreference chains.Smaller distances computed through skew divergenceindicate that the entity is similar to the entities in thechain.
If the distance computed is smaller than apredefined threshold, then the new entity is added intothe coreference chain and the probabilistic distributionof the coreference chain?s model is updated accordingly.We start with one entity in one coreference chain andcontinue comparing, inserting, and creating coreferencechains until all of the entities have been resolved.Note that the KL divergence approach is modeleddirectly after the incremental vector space approach.The difference is that the vector is replaced by aprobability distribution and the comparison usesdivergence rather than cosine similarity.5.3 Agglomerative vector spaceIn our explorations with the previous algorithm, wenoticed that if early coreference chains containedmisplaced entities, those entities attracted other entitieswith high similarity and ?polluted?
the coreferencechain with entities that are not part of the truth chain.We therefore switched to an agglomerative approachthat builds up the clusters in a way that is orderindependent.
This approach is typically known asbottom-up agglomerative clustering.
It is also done inthe vector space model, so we again represent thesnippets by vectors.We first create a coreference chain containing oneentity for every entity to be resolved.
For eachcoreference chain, we then find its nearest neighbor bycomputing the similarity of the chain against all otherchains using the technique described above in Section5.1.
If the highest similarity computed is above apredefined threshold, then we merge those two chainstogether.
If any merging was performed in this iteration,we repeat the whole process of looking for the mostsimilar pair and merging then in the next iteration.
Wecontinue this until no more merging is done?i.e., thehighest similarity is below the threshold.The only difference between this approach and thatin the previous section is that the agglomerativetechnique requires more comparisons and takes moretime.
On the other hand, it minimizes problems causedby a single spurious match and it is order independent.6.
Experiments and ResultsTo evaluate our various techniques for the task ofcross-document coreferencing, we used the two testcorpora mentioned in Section 4 and the threecoreference approaches described in Section 5.
Thecoreference chains are then evaluated using the B-CUBED algorithm to measure precision and recall asdescribed in Section 2.
We present the results bycorpus.6.1 John Smith Corpus ResultsOur main goal for the John Smith corpus is to demon-strate that we have successfully approximated thealgorithm of Bagga and Baldwin (1998).
Figure 1shows how recall and precision trade off against eachother as the decision threshold (should a name be putinto a chain) varies in the incremental vector spaceapproach.
This graph is nearly identical to the tradeoffcurves shown by Bagga and Baldwin, so we believe ourvariation on their approach is sufficiently accurate todraw conclusions.
A key point to note about the graphis that although there is an excellent recall/precisiontradeoff point, the results are not stable around thatthreshold.
If the threshold is shifted slightly higher,recall plummets; if it is lowered slightly, precision dropsoff rapidly.Figure 2 provides an alternative view of the sameinformation, and overlays the other algorithms on it.
Inthis case we show a recall/precision tradeoff curve.Again, in all cases the tradeoff drops off rapidly, thoughthe agglomerative vector space approach takes longer tofall from high accuracy.Figure 3 provides another comparison of the threeapproaches by highlighting how the F-measure varies01020304050607080901000  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1PercentageThresholdprecisionrecallF-MeasureFigure 1: Results of cross-document coreferencingon the John Smith corpus using the incrementalvector space approach.203040506070809010020  30  40  50  60  70  80  90  100PrecisionRecallIncremental VSKL DivergenceAgglomerative VSBreck and BaggaFigure 2: Recall and precision tradeoff of threealgorithms on the John Smith Corpus.
Results fromBaldwin and Bagga (1998) are estimated andoverlaid onto the graph.with the threshold.
Note that the agglomerative vectorspace approach has the highest measure and has asubstantially less ?pointed?
curve: it is much lesssensitive to threshold selection and therefore morestable.The agglomerative vector space achieved a peak Fmeasure of 88.2% in comparison to the incrementalapproach that peaked at 84.3% (comparable to Baggaand Baldwin?s reported 84.6%).
We also created asingle-link version of our incremental algorithm.
Itachieved a peak F measure of only 81.4%, showing theadvance of average link (when compared to ourapproach) and the advantage of using within-documentcoreference to find related sentences (when compared totheir work).6.2 Person-x ResultsWe next evaluated the same three algorithms on themuch larger Person X corpus.
The recall/precision graphin Figure 4, when compared to that in Figure 2, clearlydemonstrates that the larger corpus has made the taskmuch harder.
However, the agglomerative vector spaceapproach has been impacted the least and maintainsexcellent performance.Figure 5 shows the F-measure graph.
Incomparison to Figure 3, all of the techniques are lesssensitive to threshold selection, but the two vector spaceapproaches are less sensitive than the KL divergenceapproach.
It is unclear why this is, though may reflectproblems with using the skewed divergence forsmoothing.7.
Further ExplorationWe conducted additional analysis to explore theissues surrounding cross-document coreferencing.
Weran experiments with the John Smith corpus to explorethe question of the effectiveness of a model based on theamount of text used to represent an entity.7.1 Window size and recall/precisionAllan and Raghavan (2002) showed that the size of thesnippets correlates positively with the ?clarity?
(non-ambiguity) of the model.
As the size of the snippetincreases, the ambiguity of the model increases,presumably because it is more likely to includeextraneous information from the surrounding text.In our experiment with the John Smith corpus, weused the incremental vector space approach with athreshold of 0.1 and evaluated precision/recall usingvarious window sizes for the snippets.
Figure 6 showsthe variation.
We discovered that the F-Measure peaksat 84.3% with a window size of 55 words.
This is thewindow size that we used for all of our otherexperiments.4050607080901000  10  20  30  40  50  60  70  80  90  100PrecisionRecallIncremental VSKL DivergenceAgglomerative VSFigure 4: Recall and precision tradeoff for theperson-x corpus.01020304050607080901000  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1F-MeasureThresholdBreck and BaldwinIncremental VSKL DivergenceAgglomerative VSFigure 3: Comparison of F-Measure on the JohnSmith Corpus.01020304050607080901000  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1F-MeasureThresholdIncremental VSKL DivergenceAgglomerative VSFigure 5: Comparison of F-Measure on the Person-xCorpus.7.2 Domain-specific sub-corporaThe person-x corpus may appear to be biased due tothe manner of its construction.
Since the documentswere selected by subject, one may argue that the task ofclustering entities will be much easier if the entities areclearly from different genres.
However, if this is true,then it may account for about 85% of the entities in theperson-x corpus that occur only in one domain subject.We hypothesized that coreferencing entities in the samegenre domain can be considered to be harder in termsof achieving high precision because the consistency ofthe contents between documents in the same genredomain makes it significantly harder to create a uniquemodel for each entity to aid the task of distinguishingone entity from another.In order to see how our techniques measure upagainst this, we reevaluated the effectiveness of ourmethods of cross-document coreference resolution on amodified version of the person-x corpus.
We clusteredthe documents into their original genre domain (recallthat they were created using simple informationretrieval queries).
Then, we evaluated theprecision/recall for each of the clusters and averagedthe results to obtain a final precision/recall score.
Thiseliminates the potential bias that clustering the entitiesbecomes easier if the entities are clearly from differentgenres.
Hypothetically, it also makes the task of cross-document coreferencing more challenging than inreality when performed on actual corpora that is notclustered according to genre.
Table 2 shows thebreakdown of documents and entities in each genre.The results of the experiments show that clusteringdocuments by their domain specific attributes such asdomain genre will hurt cross-document coreferencing.The highest F-Measure that was achieved withagglomerative vector space dropped 6% to 77% andincremental dropped a similar 5%.
The KL divergenceapproach, on the other hand, showed a modest increaseof 3% to 77%, equaling the agglomerative approach.The reason for this may be because KL divergencerelies more on the global property of the corpus and thisapproach is more effective when the nearest neighborcomputation is degraded by the consistency of the worddistributions between documents in the same genredomain.7.3 Runtime comparison.An important observation in our comparison amongthe algorithms is running time.
While we have shownthat the agglomerative vector space approach producedthe best results in our experiments, it is also importantto  note  that  it  was  noticeably  slower.
The  estimatedrunning time for the agglomerative vector spaceexperiment  on  the  large  corpus was  approximately  3times longer than that of the incremental vector spaceand KL-Divergence.
The runtimes of incrementalapproaches are linear whereas the runtime of ouragglomerative vector space approach is O(n?
).Is the improvement in our results worth thedifference in runtime?
The noticeable run timedifference in our experiment is caused by the need tocluster a large number of Person-x entities (34,404entities).
In reality, it would be rare to find such a largenumber of entities across documents with the samename.
In the analysis of our reasonably large corpus,less than 16% of entities occur more than 10 times.
Ifthe mean size of entities to be disambiguated isrelatively small, then there will not be a significantdegrade in runtime on the agglomerative approach.
Thus,GenreNumber ofDocumentsNumber of person-x entitiesArt      3346       1455Business        315         182Education      6177       2351Government      3374         945Healthcare        914         405Movies        677       2292Music        976         366Politics      4298         949Religion      2699       1030Science      7211       2783Sports      4417       2009Table 2: Breakdown of document and entitydistribution in the domain subject specific clusters.505560657075808590951000  10  20  30  40  50  60  70  80  90  100PercentageWindow SizePrecisionRecallF-MeasureFigure 6: Relationship between the window size of thesnippet and recall/precision on the John Smith corpus.our conclusion is that the tradeoff between coreferencequality versus runtime in our agglomerative approach isdefinitely worthwhile if the number of same-namedentities to be disambiguated is relatively small.8.
Conclusion and Future WorkWe were able to compare and contrast our resultsdirectly with previous work of Bagga and Baldwin byusing the same corpus and evaluation technique.
Inorder to perform a careful excursion into the limitedwork on cross document coreferencing, we deployeddifferent information retrieval techniques for entitydisambiguation and clustering.
In our experiments, wehave shown that the agglomerative vector spaceclustering algorithm consistently yields better precisionand recall throughout most of the tests.
It outperformsthe incremental vector space disambiguation model andis much more stable with respect to the decisionthreshold.
Both vector space approaches outperformKL divergence except when the entities to be clusteredbelong to the same genre.We are pleased that our snippet approach workedwell on the task of cross document coreferencing sinceit was easier than running a within documentcoreference analyzer first.
It was also interesting todiscover that previous techniques that worked well on asmaller corpus did not show the same promising recalland precision tradeoff on a larger corpus.We are interested in continuing these evaluations intwo ways.
First, colleagues of ours are working on amore realistic corpus that is not just large but alsocontains a much richer set of marked up entities.
Welook forward to trying out techniques on that data whenit is available.
Second, we intend to extend our work toinclude new comparison and clustering approaches.
Itappears that sentence-based snippets and within-document coreference information may provide a smallgain.
And the subject information has apparently valuein some cases, so we hope to determine how to use theinformation more broadly.AcknowledgementsThe John Smith corpus was provided by Breck Baldwin,and we are grateful to him for digging through hisarchives to find the data.
This work was supported inpart by the Center for Intelligent Information Retrieval,in part by SPAWARSYSCEN-SD grant numberN66001-02-1-8903 and in part by Advanced Researchand Development Activity under contract numberMDA904-01-C-0984.
Any opinions, findings andconclusions or recommendations expressed in thismaterial are the author(s) and do not necessarily reflectthose of the sponsor.ReferencesAllan, James, ed.
Topic Detection and Tracking: Event-based Information Organization, Kluwer AcademicPublishers, 2002.Allan, James and Raghavan, Hema.
Entity Models:Construction and Application.
Center for IntelligentInformation Retrieval, Department of ComputerScience, University of Massachusetts, 2002.Bagga, Amit and Breck Baldwin.
Algorithms forScoring Coreference Chains.
In Proceedings of theLinguistic Coreference Workshop at The FirstInternational Conference on Language Resources andEvaluation (LREC'98), pp563-566, 1998.Bagga, Amit and Breck Baldwin.
Entity-Based Cross-Document Coreferencing Using the Vector SpaceModel.
Proceedings of the 36th Annual Meeting ofthe Association for Computational Linguistics andthe 17th International Conference on ComputationalLinguistics (COLING-ACL'98),pp.79-85, 1998.Bagga, Amit and Breck Baldwin.
Coreference as theFoundations for Link Analysis Over Free TextDatabases.
In Proceedings of the COLING-ACL'98Content Visualization and IntermediaRepresentations Workshop (CVIR'98), pp.
19-24,1998.Bagga, Amit, and Biermann, Alan.
A Methodology forCross-Document Coreference.
In Proceedings of theFifth Joint Conference on Information Sciences(JCIS 2000), pp.
207-210, 2000.Bagga, Amit and Baldwin, Breck.
How MuchProcessing Is Required for Cross-DocumentCoreference?
In Proceedings of the LinguisticCoreference Workshop (LREC'98), pp.
563-566,1998.BBN Technologies.
Rough ?n?
Ready?
: Audio Index-ing for Meetings and News.
http://www.bbn.com/-speech/roughnready.html (2001)Kibble, Rodger and Kees, van Deemter.
CoreferenceAnnotation: Whither?
Proceedings of LREC2000,Athens, pp.
1281-1286, 2000.Lee, Lillian.
On the Effectiveness of the SkewDivergence for Statistical Language Analysis,Technical Report, Department of Computer Science,Cornell University, 2001.
