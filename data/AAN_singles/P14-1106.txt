Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1123?1133,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Unified Model for Soft Linguistic Reordering Constraintsin Statistical Machine TranslationJunhui Li?Yuval Marton?Philip Resnik?Hal Daum?e III?
?UMIACS, University of Maryland, College Park, MD{lijunhui, resnik, hal}@umiacs.umd.edu?Microsoft Corp., City Center Plaza, Bellevue, WAyumarton@microsoft.comAbstractThis paper explores a simple and effec-tive unified framework for incorporatingsoft linguistic reordering constraints into ahierarchical phrase-based translation sys-tem: 1) a syntactic reordering modelthat explores reorderings for context freegrammar rules; and 2) a semantic re-ordering model that focuses on the re-ordering of predicate-argument structures.We develop novel features based on bothmodels and use them as soft constraintsto guide the translation process.
Ex-periments on Chinese-English translationshow that the reordering approach can sig-nificantly improve a state-of-the-art hier-archical phrase-based translation system.However, the gain achieved by the seman-tic reordering model is limited in the pres-ence of the syntactic reordering model,and we therefore provide a detailed analy-sis of the behavior differences between thetwo.1 IntroductionReordering models in statistical machine transla-tion (SMT) model the word order difference whentranslating from one language to another.
Thepopular distortion or lexicalized reordering mod-els in phrase-based SMT make good local pre-dictions by focusing on reordering on word level,while the synchronous context free grammars inhierarchical phrase-based (HPB) translation mod-els are capable of handling non-local reorderingon the translation phrase level.
However, reorder-ing, especially without any help of external knowl-edge, remains a great challenge because an ac-curate reordering is usually beyond these wordlevel or translation phrase level reordering mod-els?
ability.
In addition, often these translationmodels fail to respect linguistically-motivated syn-tax and semantics.
As a result, they tend to pro-duce translations containing both syntactic and se-mantic reordering confusions.
In this paper ourgoal is to take advantage of syntactic and seman-tic parsing to improve translation quality.
Ratherthan introducing reordering models on either theword level or the translation phrase level, we pro-pose a unified approach to modeling reordering onthe linguistic unit level, e.g., syntactic constituentsand semantic roles.
The reordering unit falls intomultiple granularities, from single words to morecomplex constituents and semantic roles, and of-ten crosses translation phrases.
To show the ef-fectiveness of our reordering models, we integrateboth syntactic constituent reordering models andsemantic role reordering models into a state-of-the-art HPB system (Chiang, 2007; Dyer et al,2010).
We further contrast it with a stronger base-line, already including fine-grained soft syntac-tic constraint features (Marton and Resnik, 2008;Chiang et al, 2008).
The general ideas, however,are applicable to other translation models, e.g.,phrase-based model, as well.Our syntactic constituent reordering model con-siders context free grammar (CFG) rules in thesource language and predicts the reordering oftheir elements on the target side, using word align-ment information.
Due to the fact that a con-stituent, especially a long one, usually maps intomultiple discontinuous blocks in the target lan-guage, there is more than one way to describe themonotonicity or swapping patterns; we thereforedesign two reordering models: one is based on theleftmost aligned target word and the other basedon the rightmost target word.While recently there has also been some encour-aging work on incorporating semantic structure(or, more specifically, predicate-argument struc-ture: PAS) reordering in SMT, it is still an openquestion whether semantic structure reordering1123strongly overlaps with syntactic structure reorder-ing, since the semantic structure is closely tied tosyntax.
To this end, we employ the same reorder-ing framework as syntactic constituent reorderingand focus on semantic roles in a PAS.
We then an-alyze the differences between the syntactic and se-mantic features.The contributions of this paper include the fol-lowing:?
We introduce novel soft reordering con-straints, using syntactic constituents or se-mantic roles, composed over word alignmentinformation in translation rules used duringdecoding time;?
We introduce a unified framework to incor-porate syntactic and semantic reordering con-straints;?
We provide a detailed analysis providing in-sight into why the semantic reordering modelis significantly less effective when syntacticreordering features are also present.The rest of the paper is organized as follows.Section 2 provides an overview of HPB transla-tion model.
Section 3 describes the details of ourunified reordering models.
Section 4 gives our ex-perimental results and Section 5 discusses the be-havior difference between syntactic constituent re-ordering and semantic role reordering.
Section 6reviews related work and, finally Section 7 con-cludes the paper.2 HPB Translation Model: an OverviewIn HPB models (Chiang, 2007), synchronous rulestake the formX ?
?
?, ?,?
?, whereX is the non-terminal symbol, ?
and ?
are strings of lexicalitems and non-terminals in the source and targetside, respectively, and ?
indicates the one-to-onecorrespondence between non-terminals in ?
and ?.Each such rule is associated with a set of transla-tion model features {?i}, such as phrase transla-tion probability p (?
| ?)
and its inverse p (?
| ?
),the lexical translation probability plex(?
| ?)
andits inverse plex(?
| ?
), and a rule penalty that af-fects preference for longer or shorter derivations.Two other widely used features are a target lan-guage model feature and a target word penalty.Given a derivation d, its translation log-probability is estimated as:logP (d) ??i?i?i(d)(1)?PAS?A0?(NP)?
TMP?(NP)?
Pre?(VBD)?
A1?(NP)?Applicants??????yesterday??????filled?????
?the?forms?Figure 1: Example of predicate-argument struc-ture.where ?iis the corresponding weight of feature ?i.See (Chiang, 2007) for more details.3 Unified Linguistic Reordering ModelsAs mentioned earlier, the linguistic reordering unitis the syntactic constituent for syntactic reorder-ing, and the semantic role for semantic reordering.The syntactic reordering model takes a CFG rule(e.g., VP ?
VP PP PP) and models the reorder-ing of the constituents on the left hand side by ex-amining their translation or visit order accordingto the target language.
For the semantic reorder-ing model, it takes a PAS and models its reorder-ing on the target side.
Figure 1 shows an exampleof a PAS where the predicate (Pre) has two corearguments (A0 and A1) and one adjunct (TMP).Note that we refer all core arguments, adjuncts,and predicates as semantic roles; thus we say thePAS in Figure 1 has 4 roles.
According to the an-notation principles in (Chinese) PropBank (Palmeret al, 2005; Xue and Palmer, 2009), all the rolesin a PAS map to a corresponding constituent in theparse tree, and these constituents (e.g., NPs andVBD in Figure 1) do not overlap with each other.Next, we use a CFG rule to describe our syn-tactic reordering model.
Treating the two formsof reorderings in a unified way, the semantic re-ordering model is obtainable by regarding a PASas a CFG rule and considering a semantic role as aconstituent.Because the translation of a source constituentmight result in multiple discontinuous blocks,there can be several ways to describe or groupthe reordering patterns.
Therefore, we designtwo general constituent reordering sub-models.One is based on the leftmost aligned word (left-most reordering model) and the other is based onthe rightmost aligned word (rightmost reorderingmodel), as follows.
Figure 2 shows the model-ing steps for the leftmost reordering model.
Fig-ure 2(a) is an example of a CFG rule in the source1124?XP?XP1?
XP2?
XP3?
XP4?f3??f4?
f5??
f6??f7?
f8?...?...?...?...?????e2?????e3????e4????e5????e6????e7????e8????e9????
XP1?
XP2?
XP3?
XP4?e2?
e3?
e5?(a)?a?CFG?rule?and?its?alignment?
(b)?leftmost?aligned?target?words?XP1?
XP2?
XP3?
XP4?1?
4?
2?
3?
XP1?
XP2?
XP3?
XP4?DM?
DS?
M?(c)?visit?order?
(d)?reordering?types?Figure 2: Modeling process illustration for leftmost reordering model.parse tree and its word alignment links to the targetlanguage.
Note that constituent XP4, which coversword f8, has no alignment.
Then for each XPi, wefind the leftmost target word which is aligned to asource word covered by XPi.
Figure 2(b) showsthat the leftmost target words for XP1, XP2, andXP3are e2, e5, and e3, respectively, while XP4has no aligned target word.
Then we get visitorder V = {vi} for {XPi} in the transformationfrom Figure 2(b) to Figure 2(c), with the follow-ing strategies for special cases:?
if the first constituent XP1is unaligned, weadd a NULL word at the beginning of the tar-get side and link XP1to the NULL word;?
if a constituent XPi(i > 1) is unaligned, weadd a link to the target word which is alignedto XPi?1, e.g., XP4will be linked to e3; and?
if k constituents XPm1.
.
.XPmk(m1<.
.
.
< mk) are linked to the same target word,then vmi= vmi+1?
1, e.g., since XP3andXP4are both linked to e3, then v3= v4?
1.Finally Figure 2(d) converts the visit order V ={v1, .
.
.
vn} into a sequence of leftmost reorderingtypes LRT = {lrt1, .
.
.
, lrtn?1}.
For every twoadjacent constituents XPiand XPi+1with corre-sponding visit order viand vi+1, their reorderingcould be one of the following:?
Monotone (M) if vi+1= vi+ 1;?
Discontinuous Monotone (DM) if vi+1> vi+ 1;?
Swap (S) if vi+1= vi?
1;?
Discontinuous Swap (DS) if vi+1< vi?
1.Up to this point, we have generated a se-quence of leftmost reordering types LRT ={lrt1, .
.
.
, lrtn?1} for a given CFG rule cfg:XP ?
XP1.
.
.XPn.
The leftmost reorderingmodel takes the following form:scorelrt(cfg) = Pl(lrt1, .
.
.
, lrtn?1| ?
(cfg))(2)where ?
(cfg) indicates the surrounding context ofthe CFG.
By assuming that any two reorderingtypes in LRT = {lrt1, .
.
.
, lrtn?1} are indepen-dent of each other, we reformulate Eq.
2 into:scorelrt(cfg) =n?1?i=1Pl(lrti| ?
(cfg))(3)Similarly, the sequence of rightmost reorderingtypes RRT can be decided for a CFG rule XP ?XP1.
.
.XPn.Accordingly, for a PAS pas: PAS ?
R1.
.
.Rn,we can obtain its sequences of leftmost and right-most reordering types by using the same way de-scribed above.3.1 Probability EstimationIn order to predict either the leftmost or right-most reordering type for two adjacent constituents,we use a maximum entropy classifier to esti-mate the probability of the reordering type rt ?
{M,DM,S,DS} as follows:P (rt | ?
(cfg)) =exp (?k?kfk(rt, ?
(cfg)))?rt?exp (?k?kfi(rt?, ?
(cfg)))(4)where fkare binary features, ?kare the weights ofthese features.
Most of our features fkare syntax-based.
For XPiand XPi+1in cfg, the features1125#Index Featurecf1 L(XPi) & L(XPi+1) & L(XP)cf2for each XPj(j < i)L(XPi) & L(XPi+1) & L(XP) & L(XPj)cf3for each XPj(j > i+ 1)L(XPi) & L(XPi+1) & L(XP) & L(XPj)cf4 L(XPi) & L(XPi+1) & P(XPi)cf5 L(XPi) & L(XPi+1) &H(XPi)cf6 L(XPi) & L(XPi+1) & P(XPi+1)cf7 L(XPi) & L(XPi+1) &H(XPi+1)cf8 L(XPi) & L(XPi+1) & S(XPi)cf9 L(XPi) & L(XPi+1) & S(XPi+1)cf10 L(XPi) & L(XP)cf11 L(XPi+1) & L(XP)Table 1: Features adopted in the syntactic leftmostand rightmost reordering models.
L (XP) returnsthe syntactic category of XP, e.g., NP, VP, PP etc.
;H (XP) returns the head word of XP; P (XP) re-turns the POS tagger of the head word; S (XP)returns the translation status of XP on the targetlanguage: un.
if it is untranslated; cont.
if it isa continuous block; and discont.
if it maps intomultiple discontinuous blocks.are aimed to examine which of them should betranslated first.
Therefore, most features share twocommon components: the syntactic categories ofXPiand XPi+1.
Table 1 shows the features used insyntactic leftmost and rightmost reordering mod-els.
Note that we use the same features for both.Although the semantic reordering model isstructured in precisely the same way, we use dif-ferent feature sets to predict the reordering be-tween two semantic roles.
Given the two adjacentroles Riand Ri+1in a PAS pas, Table 2 shows thefeatures that are used in the semantic leftmost andrightmost reordering models.3.2 Integrating into the HPB ModelFor models with syntactic reordering, we add twonew features (i.e., one for the leftmost reorder-ing model and the other for the rightmost reorder-ing model) into the log-linear translation model inEq.
1.
Unlike the conventional phrase and lexi-cal translation features, whose values are phrasepair-determined and thus can be calculated offline,the value of the reordering features can only beobtained during decoding time, and requires wordalignment information as well.
Before we presentthe algorithm integrating the reordering models,we define the following functions by assumingXPiand XPi+1are the constituent pair of interestin CFG rule cfg, H is the translation hypothesisand a is its word alignment:#Index Featurerf1R(Ri) &R(Ri+1) & P(pas)R(Ri) &R(Ri+1)rf2for each Rj(j < i)R(Ri) &R(Ri+1) &R(Rj) & P(pas)R(Ri) &R(Ri+1) &R(Rj)rf3for each Rj(j > i+ 1)R(Ri) &R(Ri+1) &R(Rj) & P(pas)R(Ri) &R(Ri+1) &R(Rj)rf4 R(Ri) &R(Ri+1) & P(Ri)rf5 R(Ri) &R(Ri+1) &H(Ri)rf6 R(Ri) &R(Ri+1) & L(Ri)rf7 R(Ri) &R(Ri+1) & P(Ri+1)rf8 R(Ri) &R(Ri+1) &H(Ri+1)rf9 R(Ri) &R(Ri+1) & L(Ri+1)rf10 R(Ri) &R(Ri+1) & S(Ri)rf11 R(Ri) &R(Ri+1) & S(Ri+1)rf12R(Ri) & P(pas)R(Ri)rf13R(Ri+1) & P(pas)R(Ri+1)Table 2: Features adopted in the semantic leftmostand rightmost reordering models.
P (pas) returnsthe predicate content of pas;R (R) returns the roletype of R, e.g., Pred, A0, TMP, etc.
For featuresrf1, rf2, rf3, rf12 and rf13, we include another ver-sion which excludes the predicate content P(pas)for reasons of sparsity.?
F1(w1, w2, XP): returns true if constituent XP iswithin the span from word w1to w2; otherwise returnsfalse.?
F2(H, cfg, XPi, XPi+1) returns true if the reorderingof the pair ?XPi, XPi+1?
in rule cfg has not been calcu-lated yet; otherwise returns false.?
F3(H, a, XPi, XPi+1) returns the leftmost and right-most reordering types for the constituent pair ?XPi,XPi+1?, given alignment a, according to Section 3.?
F4(rt, cfg, XPi, XPi+1) returns the probability ofleftmost reordering type rt for the constituent pair?XPi, XPi+1?
in rule cfg.?
F5(rt, cfg, XPi, XPi+1) returns the probability ofrightmost reordering type rt for the constituent pair?XPi, XPi+1?
in rule cfg.Algorithm 1 integrates the syntactic leftmostand rightmost reordering models into a CKY-styledecoder whenever a new hypothesis is generated.Given a hypothesis H with its alignment a, it tra-verses all CFG rules in the parse tree and sees iftwo adjacent constituents are conditioned to trig-ger the reordering models (lines 2-4).
For eachpair of constituents, it first extracts its leftmost andrightmost reordering types (line 6) and then getstheir respective probabilities returned by the max-imum entropy classifiers defined in Section 3.11126Algorithm 1: Integrating the syntactic reordering modelsinto a CKY-style decoderInput: Sentence f in the source languageParse tree t of fAll CFG rules {cfg} in tHypothesis H spanning from word w1to w2Alignment a of HOutput: Log-Probabilities of the syntactic leftmostand rightmost reordering models1.
set l prob = rprob = 0.02. foreach cfg in {cfg}3. foreach pair XPiand XPi+1in cfg4.
if F1(w1, w2, XPi) = false orF1(w1, w2, XPi+1) = false orF2(H, cfg, XPi, XPi+1) = false5.
continue6.
(l type, r type) = F3(H, a, XPi, XPi+1)7. l prob += logF4(l type, cfg,XPi,XPi+1)8. r prob += logF5(r type, cfg,XPi,XPi+1)9. return (l prob, r prob)(lines 7-8).
Then the algorithm returns two log-probabilities of the syntactic reordering models.Note that Function F1returns true if hypothesisH fully covers, or fully contains, constituentXPi,regardless of the reordering type of XPi.
Do notconfuse any parsing tag XPiwith the namelessvariables Xiin Hiero or cdec rules.For the semantic reordering models, we alsoadd two new features into the log-linear transla-tion model.
To get the two semantic reorderingmodel feature values, we simply use Algorithm 1and its associated functions from F1to F5replac-ing a CFG rule cfg with a PAS pas, and a con-stituent XPiwith a semantic role Ri.
Algorithm 1therefore permits a unified treatment of syntacticand PAS-based reordering, even though it is ex-pressed in terms of syntactic reordering here forease of presentation.4 ExperimentsWe have presented our unified approach to in-corporating syntactic and semantic soft reorder-ing constraints in an HPB system.
In this section,we test its effectiveness in Chinese-English trans-lation.4.1 Experimental SettingsFor training we use 1.6M sentence pairs of thenon-UN and non-HK Hansards portions of NISTMT training corpora, segmented with the Stan-ford segmenter (Tseng et al, 2005).
The En-glish data is lowercased, tokenized and alignedwith GIZA++ (Och and Ney, 2000) to obtain bidi-rectional alignments, which are symmetrized us-ing the grow-diag-final-and method (Koehn et al,2003).
We train a 4-gram LM on the Englishside of the corpus with 600M additional wordsfrom non-NYT and non-LAT, randomly selectedportions of the Gigaword v4 corpus, using modi-fied Kneser-Ney smoothing (Chen and Goodman,1996).
We use the HPB decoder cdec (Dyer etal., 2010), with Mr. Mira (Eidelman et al, 2013),which is a k-best variant of MIRA (Chiang et al,2008), to tune the parameters of the system.We use NIST MT 06 dataset (1664 sentencepairs) for tuning, and NIST MT 03, 05, and 08datasets (919, 1082, and 1357 sentence pairs, re-spectively) for evaluation.1We use BLEU (Pap-ineni et al, 2002) for both tuning and evaluation.To obtain syntactic parse trees and semanticroles on the tuning and test datasets, we firstparse the source sentences with the BerkeleyParser (Petrov and Klein, 2007), trained on theChinese Treebank 7.0 (Xue et al, 2005).
Wethen pass the parses to a Chinese semantic rolelabeler (Li et al, 2010), trained on the ChinesePropBank 3.0 (Xue and Palmer, 2009), to anno-tate semantic roles for all verbal predicates (part-of-speech tag VV, VE, or VC).Our basic baseline system employs 19 basicfeatures: a language model feature, 7 transla-tion model features, word penalty, unknown wordpenalty, the glue rule, date, number and 6 pass-through features.
Our stronger baseline employs,in addition, the fine-grained syntactic soft con-straint features of Marton and Resnik (2008), here-after MR08.
The syntactic soft constraint featuresinclude both MR08 exact-matching and cross-boundary constraints (denoted XP= and XP+).Since the syntactic parses of the tuning and testdata contain 29 types of constituent labels and 35types of POS tags, we have 29 types of XP+ fea-tures and 64 types of XP= features.4.2 Model TrainingTo train the syntactic and semantic reorderingmodels, we use a gold alignment dataset.2It con-tains 7,870 sentences with 191,364 Chinese wordsand 261,399 English words.
We first run syn-1http://www.itl.nist.gov/iad/mig//tests/mt2This dataset includes LDC2006E86, and newswireparts of LDC2012T16, LDC2012T20, LDC2012T24, andLDC2013T05.
Indeed, the reordering models can also betrained on the MT training data with its automatic alignment.However, our preliminary experiments showed that the re-ordering models trained on gold alignment yielded higher im-provement.1127ReorderingTypeSyntactic Semanticl-m r-m l-m r-mM 73.5 80.6 63.8 67.9DM 3.9 3.3 14.0 12.0S 19.5 13.2 13.1 10.7DS 3.2 3.0 9.1 9.5#instance 199,234 66,757Table 3: Reordering type distribution over the re-ordering model?s training data.
Hereafter, l-m andr-m are for leftmost and rightmost, respectively.tactic parsing and semantic role labeling on theChinese sentences, then train the models by us-ing MaxEnt toolkit with L1 regularizer (Tsuruokaet al, 2009).3Table 3 shows the reordering typedistribution over the training data.
Interestingly,about 17% of the syntactic instances and 16% ofthe semantic instances differ in their leftmost andrightmost reordering types, indicating that the left-most/rightmost distinction is informative.
We alsosee that the number of semantic instances is about1/3 of that of syntactic instances, but the entropyof the semantic reordering classes is higher, indi-cating the reordering of semantic roles is harderthan that of syntactic constituents.A deeper examination of the reordering model?straining data reveals that some constituent pairsand semantic role pairs have a preference for aspecific reordering type (monotone or swap).
Inorder to understand how well the MR08 systemrespects their reordering preference, we use thegold alignment dataset LDC2006E86, in whichthe source sentences are from the Chinese Tree-bank, and thus both the gold parse trees and goldpredicate-argument structures are available.
Ta-ble 4 presents examples comparing the reorderingdistribution between gold alignment and the out-put of the MR08 system.
For example, the firstrow shows that based on the gold alignment, for?PP,VP?, 16% are in monotone and 76% are inswap reordering.
However, our MR08 system out-puts 46% of them in monotone and and 50% inswap reordering.
Hence, the reordering accuracyfor ?PP,VP?
is 54%.
Table 4 also shows that thesemantic reordering between core arguments andpredicates (e.g., ?Pred,A1?, ?A0,Pred?)
has a lessambiguous pattern than that between adjuncts andother roles (e.g., ?LOC,Pred?, ?A0,TMP?
), indicat-ing the higher reordering flexibility of adjuncts.3http://www.logos.ic.i.u-tokyo.ac.jp/?tsuruoka/maxent/Const.
PairGold MR08 outputM S M S acc.PP VP 16 76 46 50 54NP LC 26 74 58 42 50DNP NP 24 72 78 19 39CP NP 26 67 84 10 33NP DEG 39 61 31 69 66... ... ...all 81 13 79 14 80Role PairGold MR08 outputM S M S acc.Pred A1 84 6 82 9 72A0 Pred 82 11 79 8 75LOC Pred 17 30 36 25 49A0 TMP 35 25 61 6 45TMP Pred 30 22 49 19 43... ... ...all 63 13 73 9 64Table 4: Examples of the reordering distribution(%) of gold alignment and the MR08 system out-put.
For simplicity, we only focus on (M)onotoneand (S)wap based on leftmost reordering.4.3 Translation Experiment ResultsOur first group of experiments investigateswhether the syntactic reordering models are ableto improve translation quality in terms of BLEU.To this end, we respectively add our syntactic re-ordering models into both the baseline and MR08systems.
The effect is shown in the rows of ?+ syn-reorder?
in Table 5.
From the table, we have thefollowing two observations.?
Although the HPB model is capable ofhandling non-local phrase reordering usingsynchronous context free grammars, bothour syntactic leftmost reordering model andrightmost model are still able to achieve im-provement over both the baseline and MR08.This suggests that our syntactic reorderingfeatures interact well with the MR08 syntac-tic soft constraints: the XP+ and XP= fea-tures focus on a single constituent each, whileour reordering features focus on a pair of con-stituents each.?
There is no clear indication of whether theleftmost reordering model works better thanthe other.
In addition, integrating both theleftmost and rightmost reordering models haslimited improvement over a single reorderingmodel.Our second group of experiments is to vali-date the semantic reordering models.
Results are1128SystemTuning TestMT06 MT03 MT05 MT08 Avg.Baseline 34.1 36.1 32.3 27.4 31.9+syn-reorderl-m 35.2 36.9?
33.6?
28.4?
33.0r-m 35.2 37.2?
33.7?
28.6?
33.2both 35.6 37.1?
33.6?
28.8?
33.1+sem-reorderl-m 34.4 36.7?
33.0?
27.8?
32.5r-m 34.5 36.7?
33.1?
27.8?
32.5both 34.5 37.0?
33.6?
27.7?
32.8+syn+sem 35.5 37.3?
33.7?
29.0?
33.3MR08 35.6 37.4 34.2 28.7 33.4+syn-reorderl-m 36.0 38.2?
35.0?
29.2?
34.1r-m 36.0 38.1?
34.8?
29.2?
34.0both 35.9 38.2?
35.3?
29.5?
34.3+sem-reorderl-m 35.8 37.6?
34.7?
28.7 33.7r-m 35.8 37.4 34.5?
28.8 33.6both 35.8 37.6?
34.7?
28.8 33.7+syn+sem 36.1 38.4?
35.2?
29.5?
34.4Table 5: System performance in BLEU scores.?/?
: significant over baseline or MR08 at 0.01/ 0.05, respectively, as tested by bootstrap re-sampling (Koehn, 2004)shown in the rows of ?+ sem-reorder?
in Table 5.Here we observe:?
The semantic reordering models also achievesignificant gain of 0.8 BLEU on average overthe baseline system, demonstrating the ef-fectiveness of PAS-based reordering.
How-ever, the gain diminishes to 0.3 BLEU on theMR08 system.?
The syntactic reordering models outperformthe semantic reordering models on both thebaseline and MR08 systems.Finally, we integrate both the syntactic and se-mantic reordering models into the final system.The two models collectively achieve a gain of upto 1.4 BLEU over the baseline and 1.0 BLEU overMR08 on average, which is shown in the rows of?+syn+sem?
in Table 5.5 DiscussionThe trend of the results, summarized as perfor-mance gain over the baseline and MR08 systemsaveraged over all test sets, is presented in Table 6.The syntactic reordering models outperform thesemantic reordering models, and the gain achievedby the semantic reordering models is limited in thepresence of the MR08 syntactic features.
In thissection, we look at MR08 system and the systemsimproving it to explore the behavior differencesbetween the two reordering models.Coverage analysis: Our statistics show thatsyntactic reordering features (either leftmost orSystem Baseline MR08+syn-reorder 1.2 0.9+sem-reorder 0.8 0.3+ both 1.4 1.0Table 6: Performance gain in BLEU over baselineand MR08 systems averaged over all test sets.rightmost) are called 24 times per sentence on av-erage.
This is compared to only 9 times per sen-tence for semantic reordering features.
This is notsurprising since the semantic reordering featuresare exclusively attached to predicates, and the spanset of the semantic roles is a strict subset of thespan set of the syntactic constituents; only 22% ofsyntactic constituents are semantic roles.
On aver-age, a sentences has 4 PASs and each PAS contains3 semantic roles.
Of all the semantic role pairs,44% are in the same CFG rules, indicating that thispart of semantic reordering has overlap with syn-tactic reordering.
Therefore, the PAS model hasfewer opportunities to influence reordering.Reordering accuracy analysis: The reorderingtype distribution on the reordering model trainingdata in Table 3 suggests that semantic reorderingis more difficult than syntactic reordering.
To val-idate this conjecture on our translation test data,we compare the reordering performance amongthe MR08 system, the improved systems and themaximum entropy classifiers.
For the test set, wehave four reference translations.
We run GIZA++on the data combination of our translation train-ing data and test data to get the alignment for thetest data and each reference translation.
Once wehave the (semi-)gold alignment, we compute thegold reordering types between two adjacent syn-tactic constituents or semantic roles.
Then weevaluate the automatic reordering outputs gener-ated from both our translation systems and max-imum entropy classifiers.
Table 7 shows the ac-curacy averaged over the four gold reordering sets(the four reference translations).
It shows that 1)as expected, our classifiers do worse on the hardersemantic reordering prediction than syntactic re-ordering prediction; 2) thanks to the high accu-racy obtained by the maxent classifiers, integrat-ing either the syntactic or the semantic reorder-ing constraints results in better reordering perfor-mance from both syntactic and semantic perspec-tives; 3) in terms of the mutual impact, the syn-tactic reordering models help improving seman-tic reordering more than the semantic reordering1129SystemSyntactic Semanticl-m r-m l-m r-mMR08 75.0 78.0 66.3 68.5+syn-reorder 78.4 80.9 69.0 70.2+sem-reorder 76.0 78.8 70.7 72.7+both 78.6 81.7 70.6 72.1Maxent Classifier 80.7 85.6 70.9 73.5Table 7: Reordering accuracy on four gold sets.SystemSyntactic Semanticl-m r-m l-m r-m+syn-reorder 1.2 1.2 - -+sem-reorder - - 0.7 0.9+both 1.2 1.0 0.5 0.4Table 8: Reordering feature weights.models help improving syntactic reordering; and4) the rightmost models have a learnability advan-tage over the leftmost models, achieving higheraccuracy across the board.Feature weight analysis: Table 8 shows thesyntactic and semantic reordering feature weights.It shows that the semantic feature weights de-crease in the presence of the syntactic features, in-dicating that the decoder learns to trust semanticfeatures less in the presence of the more accuratesyntactic features.
This is consistent with our ob-servation that semantic reordering is harder thansyntactic reordering, as seen in Tables 3 and 7.Potential improvement analysis: Table 7 alsoshows that our current maximum entropy classi-fiers have room for improvement, especially forsemantic reordering.
In order to explore the errorpropagation from the classifiers themselves andexplore the upper bound for improvement from thereordering models, we perform an ?oracle?
study,letting the classifiers be aware of the ?gold?
re-ordering type between two syntactic constituentsor two semantic roles, and returning a higher prob-ability for the gold reordering type and a smallerone for the others (i.e., we set 0.9 for the goldSystem MT 03 MT 05 MT 08 Avg.Non-OracleMR08 37.4 34.2 28.7 33.4+syn-reorder38.2 35.3 29.5 34.3+sem-reorder37.6 34.7 28.8 33.7+ both 38.4 35.2 29.5 34.4Oracle+syn-reorder39.2 35.9 29.6 34.9+sem-reorder37.9 34.8 28.9 33.9+ both 39.1 36.0 29.8 35.0Table 9: Performance (BLEU score) comparisonbetween non-oracle and oracle experiments.reordering type, and let the other non-gold threetypes share 0.1).
Again, to get the gold reorder-ing type, we run GIZA++ to get the alignment fortuning/test source sentences and each of four ref-erence translations.
We report the averaged per-formance by using the gold reordering type ex-tracted from the four reference translations.
Ta-ble 9 compares the performance between the non-oracle and oracle settings.
We clearly see that us-ing gold syntactic reordering types significantlyimproves the performance (e.g., 34.9 vs. 33.4 onaverage) and there is still some room for improve-ment by building a better maximum entropy clas-sifiers (e.g., 34.9 vs. 34.3).
To our surprise, how-ever, the improvement achieved by gold semanticreordering types is still small (e.g., 33.9 vs. 33.4),suggesting that the potential improvement of se-mantic reordering models is much more limited.And we again see that the improvement achievedby semantic reordering models is limited in thepresence of the syntactic reordering models.6 Related WorkSyntax-based reordering: Some previous workpre-ordered words in the source sentences, so thatthe word order of source and target sentences issimilar.
The reordering rules were either manu-ally designed (Collins et al, 2005; Wang et al,2007; Xu et al, 2009; Lee et al, 2010) or auto-matically learned (Xia and McCord, 2004; Gen-zel, 2010; Visweswariah et al, 2010; Khalilovand Sima?an, 2011; Lerner and Petrov, 2013), us-ing syntactic parses.
Li et al (2007) focused onfinding the n-best pre-ordered source sentences bypredicting the reordering of sibling constituents,while Yang et al (2012) obtained word order byusing a reranking approach to reposition nodes insyntactic parse trees.
Both are close to our work;however, our model generates reordering featuresthat are integrated into the log-linear translationmodel during decoding.Another approach in previous work added softconstraints as weighted features in the SMT de-coder to reward good reorderings and penalize badones.
Marton and Resnik (2008) employed softsyntactic constraints with weighted binary featuresand no MaxEnt model.
They did not explicitlytarget reordering (beyond applying constraints onHPB rules).
Although employing linguisticallymotivated labels in SCFG is capable of captur-ing constituent reorderings (Chiang, 2010; Mylon-1130akis and Sima?an, 2011), the rules are sparser thanSCFG with nameless non-terminals (i.e., Xs) andsoft constraints.
Ge (2010) presented a syntax-driven maximum entropy reordering model thatpredicted the source word translation order.
Gaoet al (2011) employed dependency trees to predictthe translation order of a word and its head word.Huang et al (2013) predicted the translation orderof two source words.4Our work, which shares thisapproach, differs from their work primarily in thatour syntactic reordering models are based on theconstituent level, rather than the word level.Semantics-based reordering: Semantics-based reordering has also seen an increasein activity recently.
In the pre-ordering ap-proach, Wu et al (2011) automatically learnedpre-ordering rules from PAS.
In the soft con-straint or reordering model approach, Liu andGildea (2010) modeled the reordering/deletionof source-side semantic roles in a tree-to-stringtranslation model.
Xiong et al (2012) and Li etal.
(2013) predicted the translation order betweeneither two arguments or an argument and itspredicate.
Instead of decomposing a PAS intoindividual units, Zhai et al (2013) constructeda classifier for each source side PAS.
Finally inthe post-processing approach category, Wu andFung (2009) performed semantic role labelingon translation output and reordered arguments tomaximize the cross-lingual match of the semanticframes between the source sentence and the targettranslation.
To our knowledge, their semanticreordering models were PAS-specific.
In contrast,our model is universal and can be easily adoptedto model the reordering of other linguistic units(e.g., syntactic constituents).
Moreover, wehave studied the effectiveness of the semanticreordering model in different scenarios.Non-syntax-based reorderings in HPB: Re-cently we have also seen work on lexicalized re-ordering models without syntactic information inHPB (Setiawan et al, 2009; Huck et al, 2013;Nguyen and Vogel, 2013).
The non-syntax-based reordering approach models the reorder-ing of translation words/phrases while the syntax-based approach models the reordering of syn-tactic constituents.
Although there are overlapsbetween translation phrases and syntactic con-stituents, it is reasonable to think that the two re-4Note that they obtained the translation order of sourceword pairs by predicting the reordering of adjacent con-stituents, which was quite close to our work.ordering approaches can work together well andeven complement each other, as the linguistic pat-terns they capture differ substantially.
Setiawanet al (2013) modeled the orientation decisionsbetween anchors and two neighboring multi-unitchunks which might cross phrase or rule bound-aries.
Last, we also note that recent work on non-syntax-based reorderings in (flat) phrase-basedmodels (Cherry, 2013; Feng et al, 2013) can alsobe potentially adopted to hpb models.7 Conclusion and Future WorkIn this paper, we have presented a unified reorder-ing framework to incorporate soft linguistic con-straints (of syntactic or semantic nature) into theHPB translation model.
The syntactic reorderingmodels take CFG rules and model their reorderingon the target side, while the semantic reorderingmodels work with PAS.
Experiments on Chinese-English translation show that the reordering ap-proach can significantly improve a state-of-the-arthierarchical phrase-based translation system.
Wehave also discussed the differences between thetwo linguistic reordering models.There are many directions in which this workcan be continued.
First, the syntactic reorderingmodel can be extended to model reordering amongconstituents that cross CFG rules.
Second, al-though we do not see obvious gain from the se-mantic reordering model when the syntactic modelis adopted, it might be beneficial to further jointlyconsider the two reordering models, focusing onwhere each one does well.
Third, to better exam-ine the overlap or synergy between our approachand the non-syntax-based reordering approach, wewill conduct direct comparisons and combinationswith the latter.AcknowledgmentsThis research was supported in part by theBOLT program of the Defense Advanced Re-search Projects Agency, Contract No.
HR0012-12-C-0015.
Any opinions, findings, conclusionsor recommendations expressed in this paper arethose of the authors and do not necessarily re-flect the view of DARPA.
The authors would liketo thank three anonymous reviewers for providinghelpful comments, and also acknowledge Ke Wu,Vladimir Eidelman, Hua He, Doug Oard, YueningHu, Jordan Boyd-Graber, and Jyothi Vinjumur foruseful discussions.1131ReferencesStanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of ACL 1996, pages 310?318.Colin Cherry.
2013.
Improved reordering for phrase-based translation using sparse features.
In Proceed-ings of HLT-NAACL 2013, pages 22?31.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of EMNLP2008, pages 224?233.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of ACL 2010,pages 1443?1452.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of ACL 2005, pages531?540.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of ACL 2010 System Demonstra-tions, pages 7?12.Vladimir Eidelman, Ke Wu, Ferhan Ture, PhilipResnik, and Jimmy Lin.
2013.
Mr. mira: Open-source large-margin structured learning on mapre-duce.
In Proceedings of ACL 2013 System Demon-strations, pages 199?204.Minwei Feng, Jan-Thorsten Peter, and Hermann Ney.2013.
Advancements in reordering models for sta-tistical machine translation.
In Proceedings of ACL2013, pages 322?332.Yang Gao, Philipp Koehn, and Alexandra Birch.
2011.Soft dependency constraints for reordering in hier-archical phrase-based translation.
In Proceedings ofEMNLP 2011, pages 857?868.Niyu Ge.
2010.
A direct syntax-driven reorderingmodel for phrase-based machine translation.
In Pro-ceedings of HLT-NAACL 2010, pages 849?857.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion.
In Proceedings of COLING 2010, pages 376?384.Zhongqiang Huang, Jacob Devlin, and Rabih Zbib.2013.
Factored soft source syntactic constraints forhierarchical machine translation.
In Proceedings ofEMNLP 2013, pages 556?566.Matthias Huck, Joern Wuebker, Felix Rietig, and Her-mann Ney.
2013.
A phrase orientation model forhierarchical machine translation.
In Proceedings ofWMT 2013, pages 452?463.Maxim Khalilov and Khalil Sima?an.
2011.
Context-sensitive syntactic source-reordering by statisticaltransduction.
In Proceedings of IJCNLP 2011,pages 38?46.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedingsof HLT-NAACL 2003, pages 48?54.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofEMNLP 2004, pages 388?395.Young-Suk Lee, Bing Zhao, and Xiaoqian Luo.2010.
Constituent reordering and syntax models forEnglish-to-Japanese statistical machine translation.In Proceedings of COLING 2010, pages 626?634.Uri Lerner and Slav Petrov.
2013.
Source-side clas-sifier preordering for machine translation.
In Pro-ceedings of EMNLP 2013, pages 513?523.Chi-Ho Li, Minghui Li, Dongdong Zhang, Mu Li,Ming Zhou, and Yi Guan.
2007.
A probabilisticapproach to syntax-based reordering for statisticalmachine translation.
In Proceedings of ACL 2007,pages 720?727.Junhui Li, Guodong Zhou, and Hwee Tou Ng.
2010.Joint syntactic and semantic parsing of Chinese.
InProceedings of ACL 2010, pages 1108?1117.Junhui Li, Philip Resnik, and Hal Daum?e III.
2013.Modeling syntactic and semantic structures in hier-archical phrase-based translation.
In Proceedings ofHLT-NAACL 2013, pages 540?549.Ding Liu and Daniel Gildea.
2010.
Semantic rolefeatures for machine translation.
In Proceedings ofCOLING 2010, pages 716?724.Yuval Marton and Philip Resnik.
2008.
Soft syntac-tic constraints for hierarchical phrased-based trans-lation.
In Proceedings of ACL-HLT 2008, pages1003?1011.Markos Mylonakis and Khalil Sima?an.
2011.
Learn-ing hierarchical translation structure with linguisticannotations.
In Proceedings of ACL 2011, pages642?652.ThuyLinh Nguyen and Stephan Vogel.
2013.
Integrat-ing phrase-based reordering features into a chart-based decoder for machine translation.
In Proceed-ings of ACL 2013, pages 1587?1596.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of ACL2000, pages 440?447.1132Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedingsof ACL 2002, pages 311?318.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL 2007, pages 404?411.Hendra Setiawan, Min Yen Kan, Haizhou Li, and PhilipResnik.
2009.
Topological ordering of functionwords in hierarchical phrase-based translation.
InProceedings of ACL-IJCNLP 2009, pages 324?332.Hendra Setiawan, Bowen Zhou, Bing Xiang, and LibinShen.
2013.
Two-neighbor orientation model withcross-boundary global contexts.
In Proceedings ofACL 2013, pages 1264?1274.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, pages168?171.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent trainingfor l1-regularized log-linear models with cumula-tive penalty.
In Proceedings of ACL-IJCNLP 2009,pages 477?485.Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,Vijil Chenthamarakshan, and Nandakishore Kamb-hatla.
2010.
Syntax based reordering with automat-ically derived rules for improved statistical machinetranslation.
In Proceedings of COLING 2010, pages1119?1127.Chao Wang, Michael Collins, and Philipp Koehn.2007.
Chinese syntactic reordering for statisticalmachine translation.
In Proceedings of EMNLP2007, pages 737?745.Dekai Wu and Pascale Fung.
2009.
Semantic roles forsmt: A hybrid two-pass model.
In Proceedings ofHLT-NAACL 2009: short papers, pages 13?16.Xianchao Wu, Katsuhito Sudoh, Kevin Duh, HajimeTsukada, and Masaaki Nagata.
2011.
Extractingpre-ordering rules from predicate-argument struc-tures.
In Proceedings of IJCNLP 2011, pages 29?37.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proceedings of COLING 2004, pages508?514.Deyi Xiong, Min Zhang, and Haizhou Li.
2012.
Mod-eling the translation of predicate-argument structurefor smt.
In Proceedings of ACL 2012, pages 902?911.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improvesmt for subject-object-verb languages.
In Proceed-ings of HLT-NAACL 2009, pages 245?253.Nianwen Xue and Martha Palmer.
2009.
Adding se-mantic roles to the Chinese Treebank.
Natural Lan-guage Engineering, 15(1):143?172.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese Treebank: Phrasestructure annotation of a large corpus.
Natural Lan-guage Engineering, 11(2):207?238.Nan Yang, Mu Li, Dongdong Zhang, and Nenghai Yu.2012.
A ranking-based approach to word reorderingfor statistical machine translation.
In Proceedings ofACL 2012, pages 912?920.Feifei Zhai, Jiajun Zhang, Yu Zhou, and ChengqingZong.
2013.
Handling ambiguities of bilingualpredicate-argument structures for statistical machinetranslation.
In Proceedings of ACL 2013, pages1127?1136.1133
