Bagging and Boost ing a Treebank ParserJohn C. HendersonThe MITRE Corporat ion202 Bur l ington  RoadBedford ,  MA 01730jhndrsn@mit re .o rgEric Br i l lM ic roso f t  Research1 M ic roso f t  WayRedmond,  WA 98052br i l l@microso f t .
comAbstractBagging and boosting, two effective machine learn-ing techniques, are applied to natural anguage pars-ing.
Experiments using these techniques with atrainable statistical parser are described.
The bestresulting system provides roughly as large of a gainin F-measure as doubling the corpus size.
Erroranalysis of the result of the boosting technique re-veals some inconsistent annotations in the PennTreebank, suggesting a semi-automatic method forfinding inconsistent treebank annotations.1 Int roduct ionHenderson and Brill (1999) showed that independenthuman research efforts produce parsers that can becombined for an overall boost in accuracy.
Findingan ensemble of parsers designed to complement eachother is clearly desirable.
The parsers would needto be the result of a unified research effort, though,in which the errors made by one parser are targetedwith priority by the developer of another parser.A set of five parsers which each achieve only 40%exact sentence accuracy would be extremely valu-able if they made errors in such a way that at leasttwo of the five were correct on any given sentence(and the others abstained or were wrong in differentways).
100% sentence accuracy could be achievedby selecting the hypothesis that was proposed bythe two parsers that agreed completely.In this paper, the task of automatically creatingcomplementary parsers is separated from the task ofcreating a single parser.
This facilitates tudy of theensemble creation techniques in isolation.
The resultis a method for increasing parsing performance bycreating an ensemble of parsers, each produced fromdata using the same parser induction algorithm.2 Bagging and Pars ing2.1 BackgroundThe work of Efron and Tibshirani (1993) enabledBreiman's refinement and application of their tech-niques for machine learning (Breiman, 1996).
Histechnique is called bagging, short for "bootstrap ag-gregating".
In brief, bootstrap techniques and bag-ging in particular educe the systematic biases manyestimation techniques introduce by aggregating es-timates made from randomly drawn representativeresamplings of those datasets.Bagging attempts to find a set of classifiers whichare consistent with the training data, different fromeach other, and distributed such that the aggregatesample distribution approaches the distribution ofsamples in the training set.Algorithm: Bagging Predictors(Breiman, 1996) (1)Given: training set  = {(yi ,x~), i  E {1. .
.m}}drawn from the set A of possible training sets whereYi is the label for example x~, classification i ductionalgorithm q2 : A --* ?
with classification algorithmCe ?
and ?
:  X- -~Y.1.
Create k bootstrap replicates o f / :  by samplingm items from E with replacement.
Call themL1.
.
.Lk .2.
For each j e {1. .
.k},  Let Cj = ~(?
j )  be theclassifier induced using Lj as the training set.3.
If Y is a discrete set, then for each x~ observedin the test set, yi = mode(?j (x i ) .
.
.
Cj(x~)).
y~is the value predicted by the most predictors,the majority vote.2.2 Bagging for ParsingAn algorithm that applies the technique of baggingto parsing is given in Algorithm 2.
Previous work oncombining independent parsers is leveraged to pro-duce the combined parser.
The rest of the algorithmis a straightforward transformation of bagging forclassifiers.
Exploratory work in this vein was de-scribed by HajiC et al (1999).Algorithm: Bagging A Parser (2)Given: A corpus (again as a funct ion)C :S?T ~ N,S is the set of possible sentences, and T is the setof trees, with size m = \[C\] = ~s, t  C(s, t) and parserinduction algorithm g.1.
Draw k bootstrap replicates C1 ... Ck of C eachcontaining m samples of (s,t) pairs randomly34picked from the domain of C according to thedistribution D(s , t )  = C(s,t)/\]C\].
Each boot-strap replicate is a bag of samples, where eachsample in a bag is drawn randomly with replace-ment from the bag corresponding to C.2.
Create parser f~ = g(Ci) for each i.3.
Given a novel sentence 8test E Ctest ,  combinethe collection of hypotheses ti = fi(Stest) us-ing the unweighted constituent voting schemeof Henderson and Brill (1999).2.3 Exper imentThe training set for these experiments was sections01-21 of the Penn Treebank (Marcus et al, 1993).The test set was section 23.
The parser inductionalgorithm used in all of the experiments in this pa-per was a distribution of Collins's model 2 parser(Collins, 1997).
All comparisons made below referto results we obtained using Collins's parser.The results for bagging are shown in Figure 2 andTable 1.
The row of figures are (from left-to-right)training set F-measure ~,test set F-measure, percentperfectly parsed sentences in training set, and per-cent perfectly parsed sentences in test set.
An en-semble of bags was produced one bag at a time.
Inthe table, the In i t ia l  row shows the performanceachieved when the ensemble contained only one bag,F inal(X)  shows the performance when the ensem-ble contained X bags, BestF gives the performanceof the ensemble size that gave the best F-measurescore.
Tra inBestF and TestBestF give the test setperformance for the ensemble size that performedthe best on the training and test sets, respectively.On the training set al of the accuracy measuresare improved over the original parser, and on thetest set there is clear improvement in precision andrecall.
The improvement on exact sentence accuracyfor the test set is significant, but only marginally so.The overall gain achieved on the test set by bag-ging was 0.8 units of F-measure, but because theentire corpus is not used in each bag the initial per-formance is approximately 0.2 units below the bestpreviously reported result.
The net gain using thistechnique is 0.6 units of F-measure.3 Boosting3.1 BackgroundThe AdaBoost algorithm was presented by Fre-und and Schapire in 1996 (Freund and Schapire,1996; Freund and Schapire, 1997) and has become awidely-known successful method in machine learn-ing.
The AdaBoost algorithm imposes one con-straint on its underlying learner: it may abstain frommaking predictions about labels of some samples,1This is the balanced version ofF-measure, where precisionand recall are weighted equally.but it must consistently be able to get more than50?-/o accuracy on the samples for which it commitsto a decision.
That accuracy is measured accord-ing to the distribution describing the importance ofsamples that it is given.
The learner must be ableto get more correct samples than incorrect samplesby mass of importance on those that it labels.
Thisstatement of the restriction comes from Schapire andSinger's study (1998).
It is called the weak learningcriterion.Schapire and Singer (1998) extended AdaBoost bydescribing how to choose the hypothesis mixing co-efficients in certain circumstances and how to incor-porate a general notion of confidence scores.
Theyalso provided a better characterization f its theo-retical performance.
The version of AdaBoost usedin their work is shown in Algorithm 3, as it is theversion that most amenable to parsing.Algorithm: AdaBoost(F reund and Schapire, 1997") (3)Given: Training set /: as in bagging, except yi E{-1,  1 } is the label for example xi.
Initial uniformdistribution D1 (i) = 1/m.
Number of iterations, T.Counter t = 1. tI,, ?~, and ?
are as in Bagging.1.
Create Lt by randomly choosing with replace-ment m samples from L: using distribution Dt.2.
Classifier induction: Ct ~- ~(Lt)3.
Choose at E IR.4.
Adjust and normalize the distribution.
Zt is anormalization coefficient.1D, + , ( i) = -~- Dt ( i ) exp(-c~tYiCt( xi ) )5.
Increment t. Quit if t > T.6.
Repeat from step 1.7.
The final hypothesis i~)boost(:g) ~- sign Z ~t?,(x)tThe value of at should generally be chosen to min-imizeZ Dt (i) exp(-a~ Yi Ct (x,))iin order to minimize the expected per-sample train-ing error of the ensemble, which Schapire and Singershow can be concisely expressed by I-\] Zt.
They alsogive several examples for how to pick an appropriatea, and selection generally depends on the possibleoutputs of the underlying learner.Boosting has been used in a few NLP systems.Haruno et al (1998) used boosting to produce moreaccurate classifiers which were embedded as control35Set Instance P R F Gain Exact GainTraining Original Parser 96.25 96.31 96.28 NA 64.7 NAInitial 93.61 93.63 93.62 0.00 55.5 0.0BestF(15) 96.16 95.86 96.01 2.39 62.1 6.6Final(15) 96.16 95.86 96.01 2.39 62.1 6.6Test Original Parser 88.73 88.54 88.63 NA 34.9 NAInitial 88.43 88.34 88.38 0.00 33.3 0.0TrainBestF(15) 89.54 88.80 89.17 0.79 34.6 1.3TestBestF(13) 89.55 88.84 89.19 0.81 34.7 1.4Final(15) 89.54 88.80 89.17 0.79 34.6 1.3Table 1: Bagging the Treebankmechanisms of a parser for Japanese.
The creatorsof AdaBoost used it to perform text classification(Schapire and Singer, 2000).
Abney et al (1999)performed part-of-speech tagging and prepositionalphrase attachment using AdaBoost as a core compo-nent.
They found they could achieve accuracies onboth tasks that were competitive with the state ofthe art.
As a side effect, they found that inspectingthe samples that were consistently given the mostweight during boosting revealed some faulty anno-tations in the corpus.
In all of these systems, Ad-aBoost has been used as a traditional classificationsystem.3.2 Boosting for ParsingOur goal is to recast boosting for parsing while con-sidering a parsing system as the embedded learner.The formulation is given in Algorithm 4.
The in-tuition behind the additive form is that the weightplaced on a sentence should be the sum of the weightwe would like to place on its constituents.
Theweight on constituents that are predicted incorrectlyare adjusted by a factor of 1 in contrast o a factorof ~ for those that are predicted incorrectly.Algorithm: Boosting A Parser (4)Given corpus C with size m = IC I = ~s.~C(s, t )and parser induction algorithm g. Initial uniformdistribution Dl(i) = 1/m.
Number of iterations, T.Counter t = 1.1.
Create Ct by randomly choosing with replace-ment m samples from C using distribution Dr.2.
Create parser ft ~ g(Ct).3.
Choose at E R (described below).4.
Adjust and normalize the distribution.
Zt isa normalization coefficient.
For all i, let parsetree ~-~' -- ft(s,).
Let ~(T,c) be a function indi-cating that c is in parse tree r, and ITI is thenumber of constituents in tree T. T(s) is the setof constituents that are found in the referenceor hypothesized annotation for s.Dt+l ( i )  :1 - ,cET(s i )5.
Increment .
Quit if t > T.6.
Repeat from step 1.7.
The final hypothesis is computed by combin-ing the individual constituents.
Each parser Ctin the ensemble gets a vote with weight at forthe constituents they predict.
Precisely thoseconstituents with weight strictly larger than1 ~--~t at are put into the final hypothesis.A potential constituent can be considered correctif it is predicted in the hypothesis and it exists inthe reference, or it is not predicted and it is not inthe reference.
Potential constituents that do not ap-pear in the hypothesis or the reference should notmake a big contribution to the accuracy computa-tion.
There are many such potential constituents,and if we were maximizing a function that treatedgetting them incorrect the same as getting a con-stituent that appears in the reference correct, wewould most likely decide not to predict any con-stituents.Our model of constituent accuracy is thus sim-ple.
Each prediction correctly made over T(s) will begiven equal weight.
That is, correctly hypothesizinga constituent in the reference will give us one point,but a precision or recall error will cause us to missone point.
Constituent accuracy is then a/(a+b+c),where a is the number of constituents correctly hy-pothesized, b is the number of precision errors and cis the number of recall errors.In Equation 1, a computation of aca as describedis shown.Otca =D( i )i c6T(si)D( i )i cCT(s i )Boosting algorithms were developed that at-tempted to maximize F-measure, precision, and re-call by varying the computation of a, giving resultstoo numerous to include here.
The algorithm givenhere performed the best of the lot, but was onlymarginally better for some metrics.
(1:36Set Instance P R F Gain Exact GainTraining Original Parser 96.25 96.31 96.28 NA 64.7 NAInitial 93.54 93.61 93.58 0.00 54.8 0.0BestF(15) 96.21 95.79 96.00 2.42 57.3 2.5Final(15) 96.21 95.79 96.00 2.42 57.3 2.5Test Original Parser 88.73 88.54 88.63 NA 34.9 NAInitial 88.05 88.09 88.07 0.00 33.3 0.0TrainBestF(15) 89.37 88.32 88.84 0.77 33.0 -0.3TestBestF(14) 89.39 88.41 88.90 0.83 33.4 0.1Final(15) 89.37 88.32 88.84 0.77 33.0 -0.3Table 2: Boosting the Treebank3.3 ExperimentThe experimental results for boosting are shown inFigure 3 and Table 2.
There is a large plateau inperformance from iterations 5 through 12.
Becauseof their low accuracy and high degree of specializa-tion, the parsers produced in these iterations hadlittle weight during voting and had little effect onthe cumulative decision making.As in the bagging experiment, it appears thatthere would be more precision and recall gain tobe had by creating a larger ensemble.
In both thebagging and boosting experiments ime and resourceconstraints dictated our ensemble size.In the table we see that the boosting algorithmequaled bagging's test set gains in precision and re-call.
The In i t ia l  performance for boosting waslower, though.
We cannot explain this, and expectit is due to unfortunate resampling of the data dur-ing the first iteration of boosting.
Exact sentenceaccuracy, though, was not significantly improved onthe test set.Overall, we prefer bagging to boosting for thisproblem when raw performance is the goal.
Thereare side effects of boosting that are useful in otherrespects, though, which we explore in Section 4.2.3.3.1 Weak Learning Criterion ViolationsIt was hypothesized in the course of investigating thefailures of the boosting algorithm that the parser in-duction system did not satisfy the weak learning cri-terion.
It was noted that the distribution of boostingweights were more skewed in later iterations.
Inspec-tion of the sentences that were getting much massplaced upon them revealed that their weight was be-ing boosted in every iteration.
The hypothesis wasthat the parser was simply unable to learn them.39832 parsers were built to test this, one for eachsentence in the training set.
Each of these parserswas trained on only a single sentence 2 and evaluatedon the same sentence.
It was discovered that a full4764 (11.2%) of these sentences could not be parsedcompletely correctly by the parsing system.2The sentence was replicated 10 times to avoid threshold-ing effects in the learner.3.3.2 Corpus TrimmingIn order to evaluate how well boosting worked witha learner that better satisfied the weak learning cri-terion, the boosting experiment was run again onthe Treebank minus the troublesome sentences de-scribed above.
The results are in Table 3.
Thisdataset produces a larger gain in comparison to theresults using the entire Treebank.
The initial ac-curacy, however, is lower.
We hypothesize that theboosting algorithm did perform better here, but theparser induction system was learning useful informa-tion in those sentences that it could not memorize(e.g.
lexical information) that was successfully ap-plied to the test set.In this manner we managed to clean our dataset othe point that the parser could learn each sentencein isolation.
The corpus-makers cannot necessarilybe blamed for the sentences that could not be mem-orized.
All that can be said about those sentencesis that for better or worse, the parser's model wouldnot accommodate hem.4 Corpus Analys is4.1 Noisy Corpus: Empirical InvestigationTo acquire experimental evidence of noisy data, dis-tributions that were used during boosting the sta-ble corpus were inspected.
The distribution was ex-pected to be skewed if there was noise in the data, orbe uniform with slight fluctuations if it fit the datawell.We see how the boosting weight distributionchanges in Figure 1.
The individual curves are in-dexed by boosting iteration in the key of the figure.This training run used a corpus of 5000 sentences.The sentences are ranked by the weight they aregiven in the distribution, and sorted in decreasing or-der by weight along the x-axis.
The distribution wassmoothed by putting samples into equal weight bins,and reporting the average mass of samples in the binas the y-coordinate.
Each curve on this graph cor-responds to a boosting iteration.
We used 1000 binsfor this graph, and a log scale on the x-axis.
Sincethere were 5000 samples, all samples initially had ay-value of 0.0002.37Set Instance P R F Gain Exact GainTraining Original Parser 96.25 96.31 96.28 NA 64.7 NAInitial 94.60 94.68 94.64 0.00 62.2 0.0BestF(8) 97.38 97.00 97.19 2.55 63.1 0.9Final(15) 97.00 96.17 96.58 1.94 55.0 -7.2Test Original Parser 88.73 88.54 88.63 NA 34.9 NAInitial 87.43 87.21 87.32 0.00 32.6 0.0TrainBestF(8) 89.12 87.62 88.36 1.04 32.8 0.2TestBestF(6) 89.07 87.77 88.42 1.10 32.9 0.4Final(15) 89.18 87.19 88.18 0.86 31.7 -0.8Table 3: Boosting the Stable Corpus0.050.0450.0400350.03~' o.o2s I0,020.0150.010.0050, , .
.
.
, i " .2 .
.
.
.
.
.
.3 .
.
.
.
.
.
.4 ?5 .
.
.
.
.6 .
.
.
.7 .
.
.
.
.
.
.
.8 .
.
.
.
.
.
.9 .
.
.
.
.
.
.
.
.1 0 - -11  .
.
.
.
.
.
.iFigure 1: Weight Change During BoostingNotice first that the left endpoints of the linesmove from bottom to top in order of boosting it-eration.
The distribution becomes monotonicallymore skewed as boosting progresses.
Secondly wesee by the last iteration that most of the weight isconcentrated on less than 100 samples.
This graphshows behavior consistent with noise in the corpuson which the boosting algorithm is focusing.4.2 T reebank  InconsistenciesThere are sentences in the corpus that can be learnedby the parser induction algorithm in isolation butnot in concert because they contain conflicting in-formation.
Finding these sentences leads to a betterunderstanding of the quality of our corpus, and givesan idea for where improvements in annotation qual-ity can be made.
Abney et al (1999) showed asimilar corpus analysis technique for part of speechtagging and prepositional phrase tagging, but forparsing we must remove errors introduced by theparser as we did in Section 3.3.2 before questioningthe corpus quality.
A particular class of errors, in-consistencies, can then be investigated.
Inconsistentannotations are those that appear plausible in iso-lation, but which conflict with annotation decisionsmade elsewhere in the corpus.In Figure 5 we show a set of trees selected fromwithin the top 100 most heavily weighted trees atthe end of 15 iterations of boosting the stable cor-pus.Collins's parser induction system is able to learnto produce any one of these structures in isolation,but the presence of conflicting information in differ-ent sentences prevents it from achieving 100% accu-racy on the set.5 Training Corpus Size EffectsWe suspect our best parser diversification techniquesgives performance gain approximately equal to dou-bling the size of the training set.
While this cannotbe directly tested without hiring more annotators,an expected performance bound for a larger train-ing set can be produced by extrapolating from howwell the parser performs using smaller training sets.There are two characteristics of training curves forlarge corpora that can provide such a bound: train-ing curves generally increase monotonically in theabsence of over-training, and their first derivativesgenerally decrease monotonically.Set Sentences P R50100500100050001000020000398325010050010005000100002000039832F Exact67.57 32.15 43.57 5.469,03 56.23 61.98 8,578,12 75.46 76.77 18,281.36 80.70 81.03 22.987.28 87.09 87.19 34.189.74 89.56 89.65 41.092.42 92.40 92.41 50.396.25 96.31 96.28 64.768.13 32.24 43.76 4.769.90 54.19 61.05 7.878.72 75.33 76.99 19.181.61 80.68 81.14 22.286.03 85.43 85.73 28.687.29 86.81 87.05 30.887.99 87.87 87.93 32.788.73 88.54 88.63 34.9Table 4: Effects of Varying Training Corpus SizeThe training curves we present in Figure 4 and Ta-ble 4 suggest hat roughly doubling the corpus size38in the range of interest (between 10000 and 40000sentences) gives a test set F-measure gain of approx-imately 0.70.Bagging achieved significant gains of approxi-mately 0.60 over the best reported previous F-measure without adding any new data.
In this re-spect, these techniques how promise for makingperformance gains on large corpora without addingmore data or new parsers.6 Conc lus ionWe have shown two methods, bagging and boosting,for automatically creating ensembles of parsers thatproduce better parses than any individual in the en-semble.
Neither of the algorithms exploit any spe-cialized knowledge of the underlying parser induc-tion algorithm, and the data used in creating theensembles has been restricted to a single commontraining set to avoid issues of training data quantityaffecting the outcome.Our best bagging system performed consistentlywell on all metrics, including exact sentence accu-racy.
It resulted in a statistically significant F-measure gain of 0.6 over the performance of the base-line parser.
That baseline system is the best knownTreebank parser.
This gain compares favorably witha bound on potential gain from increasing the corpussize.Even though it is computationally expensive tocreate and evaluate a small (15-30) ensemble ofparsers, the cost is far outweighed by the opportu-nity cost of hiring humans to annotate 40000 moresentences.
The economic basis for using ensemblemethods will continue to improve with the increasingvalue (performance p r price) of modern hardware.Our boosting system, although dominated by thebagging system, also performed significantly betterthan the best previously known individual parsingresult.
We have shown how to exploit the distri-bution created as a side-effect of the boosting al-gorithm to uncover inconsistencies in the trainingcorpus.
A semi-automated technique for doing thisas well as examples from the Treebank that are in-consistently annotated were presented.
Perhaps thebiggest advantage ofthis technique is that it requiresno a priori notion of how the inconsistencies can becharacterized.7 AcknowledgmentsWe would like to thank Michael Collins for enablingall of this research by providing us with his parserand helpful comments.This work was funded by NSF grant IRI-9502312.The views expressed in this paper are those of theauthors and do not necessarily reflect the views ofthe MITRE Corporation.
This work was done whileboth authors were at Johns Hopkins University.Re ferencesSteven Abney, Robert E. Schapire, and YoramSinger.
1999.
Boosting applied to tagging and PPattachment.
In Proceedings of the Joint SIGDATConference on Empirical Methods in Natural Lan-guage Processing and Very Large Corpora, pages38-45, College Park, Maryland.L.
Breiman.
1996.
Bagging predictors.
In MachineLearning, volume 24, pages 123-140.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings ofthe Annual Meeting of the Association for Com-putational Linguistics, volume 35, Madrid.B.
Efron and R. Tibshirani.
1993.
An Introductionto the Bootstrap.
Chapman and Hall.Y.
Freund and R.E.
Schapire.
1996.
Experimentswith a new boosting algorithm.
In Proceedings ofthe International Conference on Machine Learn-ing.Y.
Freund and R.E.
Schapire.
1997.
A decision-theoretic generalization f on-line learning and anapplication to boosting.
Journal of Computer andSystems Sciences, 55(1):119-139, Aug.Jan Haji~, E. Brill, M. Collins, B. Hladka, D. Jones,C.
Kuo, L. Ramshaw, O. Schwartz, C. Tillmann,and D. Zeman.
1999.
Core natural languageprocessing technology applicable to multiple lan-guages.
Prague Bulletin of Mathematical Linguis-tics, 70.Masahiko Haruno, Satoshi Shirai, and YoshifumiOoyama.
1998.
Using decision trees to constructa practical parser.
In Proceedings of the 36thAnnual Meeting of the Association for Compu-tational Linguistics and 17th International Con-ference on Computational Linguistics, volume 1,pages 505-511, Montreal, Canada.John C. Henderson and Eric Brill.
1999.
Exploitingdiversity in natural language processing: Combin-ing parsers.
In Proceedings of the Fourth Confer-ence on Empirical Methods in Natural LanguageProcessing, College Park, Maryland.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building a largeannotated corpus of english: The Penn Treebank.Computational Linguistics, 19(2):313-330.Robert E. Schapire and Yoram Singer.
1998.
Im-proved boosting algorithms using confidence-ratedpredictions.
In Proceedings of the Eleventh An-nual Conference on Computational Learning The-ory, pages 80-91.Robert E. Schapire and Yoram Singer.
2000.
Boos-texter: A boosting-based system for text catego-rization.
Machine Learning, 39(2/3):1-34, May.To appear.
"4Q 39\j Jii ?/'...\]~'.,..ANb~.=.O OOb~..=b~40, , z _~/~ ~j~_.
~g0O41
