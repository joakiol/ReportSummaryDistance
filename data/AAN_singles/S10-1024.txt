Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 117?122,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsUSPwlv and WLVusp: Combining Dictionaries and ContextualInformation for Cross-Lingual Lexical SubstitutionWilker AzizUniversity of S?ao PauloS?ao Carlos, SP, Brazilwilker.aziz@usp.brLucia SpeciaUniversity of WolverhamptonWolverhampton, UKl.specia@wlv.ac.ukAbstractWe describe two systems participatingin Semeval-2010?s Cross-Lingual LexicalSubstitution task: USPwlv and WLVusp.Both systems are based on two main com-ponents: (i) a dictionary to provide a num-ber of possible translations for each sourceword, and (ii) a contextual model to selectthe best translation according to the con-text where the source word occurs.
Thesecomponents and the way they are inte-grated are different in the two systems:they exploit corpus-based and linguisticresources, and supervised and unsuper-vised learning methods.
Among the 14participants in the subtask to identify thebest translation, our systems were ranked2nd and 4th in terms of recall, 3rd and 4thin terms of precision.
Both systems out-performed the baselines in all subtasks ac-cording to all metrics used.1 IntroductionThe goal of the Cross-Lingual Lexical Substitu-tion task in Semeval-2010 (Mihalcea et al, 2010)is to find the best (best subtask) Spanish transla-tion or the 10-best (oot subtask) translations for100 different English source words depending ontheir context of occurrence.
Source words includenouns, adjectives, adverbs and verbs.
1, 000 oc-currences of such words are given along with ashort context (a sentence).This task resembles that of Word Sense Dis-ambiguation (WSD) within Machine Translation(MT).
A few approaches have recently been pro-posed using standard WSD features to learn mod-els using translations instead of senses (Specia etal., 2007; Carpuat and Wu, 2007; Chan and Ng,2007).
In such approaches, the global WSD scoreis added as a feature to statistical MT systems,along with additional features, to help the systemon its choice for the best translation of a sourceword or phrase.We exploit contextual information in alternativeways to standard WSD features and supervised ap-proaches.
Our two systems - USPwlv and WLVusp - use two main components: (i) a list of pos-sible translations for the source word regardless ofits context; and (ii) a contextual model that rankssuch translations for each occurrence of the sourceword given its context.While these components constitute the core ofmost WSD systems, the way they are created andintegrated in our systems differs from standard ap-proaches.
Our systems do not require a modelto disambiguate / translate each particular sourceword, but instead use general models.
We experi-mented with both corpus-based and standard dic-tionaries, and different learning methodologies torank the candidate translations.
Our main goal wasto maximize the accuracy of the system in choos-ing the best translation.WLVusp is a very simple system based es-sentially on (i) a Statistical Machine Translation(SMT) system trained using a large parallel cor-pus to generate the n-best translations for eachoccurrence of the source words and (ii) a stan-dard English-Spanish dictionary to filter out noisytranslations and provide additional translations incase the SMT system was not able to produce alarge enough number of legitimate translations,particularly for the oot subtask.USPwlv uses a dictionary built from a large par-allel corpus using inter-language information the-ory metrics and an online-learning supervised al-gorithm to rank the options from the dictionary.The ranking is based on global and local contex-tual features, such as the mutual information be-tween the translation and the words in the sourcecontext, which are trained using human annotationon the trial dataset.1172 Resources2.1 Parallel corpusThe English-Spanish part of Europarl (Koehn,2005), a parallel corpus from the European Par-liament proceedings, was used as a source of sen-tence level aligned data.
The nearly 1.7M sentencepairs of English-Spanish translations, as providedby the Fourth Workshop on Machine Translation(WMT091), sum up to approximately 48M tokensin each language.
Europarl was used both to trainthe SMT system and to generate dictionaries basedon inter-language mutual information.2.2 DictionariesThe dictionary used by WLVusp was extracted us-ing the free online service Word Reference2, whichprovides two dictionaries: Espasa Concise andPocket Oxford Spanish Dictionary.
Regular ex-pressions were used to extract the content of thewebpages, keeping only the translations of thewords or phrasal expressions, and the outcomewas manually revised.
The manual revision wasnecessary to remove translations of long idiomaticexpressions which were only defined through ex-amples, for example, for the verb check: ?wechecked up and found out he was lying ?
hicimosaveriguaciones y comprobamos que ment??a?.
Theresulting dictionary contains a number of open do-main (single or multi-word) translations for eachof the 100 source words.
This number varies from3 to 91, with an average of 12.87 translations perword.
For example:?
yet.r = todav?
?a, a?un, ya, hasta ahora, sin em-bargo?
paper.n = art?
?culo, papel, envoltorio, diario,peri?odico, trabajo, ponencia, examen, parte,documento, libroAny other dictionary can in principle be used toproduce the list of translations, possibly withoutmanual intervention.
More comprehensive dictio-naries could result in better results, particularlythose with explicit information about the frequen-cies of different translations.
Automatic metricsbased on parallel corpus to learn the dictionary canalso be used, but we would expect the accuracy ofthe system to drop in that case.1http://www.statmt.org/wmt09/translation-task.html2http://www.wordreference.com/The process to generate the corpus-based dic-tionary for USPwlv is described in Section 4.2.3 Pre-processing techniquesThe Europarl parallel corpus was tokenized andlowercased using standard tools provided by theWMT09 competition.
Additionally, the sentencesthat were longer than 100 tokens after tokenizationwere discarded.Since the task specifies that translations shouldbe given in their basic forms, and also in order todecrease the sparsity due to the rich morphologyof Spanish, the parallel corpus was lemmatized us-ing TreeTagger (Schmid, 2006), a freely availablepart-of-speech (POS) tagger and lemmatizer.
Twodifferent versions of the parallel corpus were builtusing both lemmatized words and their POS tags:Lemma Words are represented by their lemma-tized form.
In case of ambiguity, the originalform was kept, in order to avoid incorrect choices.Words that could not be lemmatized were also keptas in their original form.Lemma.pos Words are represented by their lem-matized form followed by their POS tags.
POStags representing content words are generalizedinto four groups: verbs, nouns, adjectives and ad-verbs.
When the system could not identify a POStag, a dummy tag was used.The same techniques were used to pre-processthe trial and test data.2.4 Training samplesThe trial data available for this task was used as atraining set for the USPwlv system, which uses asupervised learning algorithm to learn the weightsof a number of global features.
For the 300 oc-currences of 30 words in the trial data, the ex-pected lexical substitutions were given by the taskorganizers, and therefore the feature weights couldbe optimized in a way to make the system resultin good translations.
These sentences were pre-processed in the same way the parallel corpus.3 WLVusp systemThis system is based on a combination of theStatistical Machine Translation (SMT) frame-work using the English-Spanish Europarl dataand an English-Spanish dictionary built semi-automatically (Section 2.2).
The parallel corpus118was lowercased, tokenized and lemmatized (Sec-tion 2.3) and then used to train the standard SMTsystem Moses (Koehn et al, 2007) and translatethe trial/test sentences, producing the 1000-besttranslations for each input sentence.Moses produces its own dictionary from theparallel corpus by using a word alignment tooland heuristics to build parallel phrases of up toseven source words and their corresponding targetwords, to which are assigned translation probabil-ities using frequency counts in the corpus.
Thismethodology provides some very localized con-textual information, which can help guiding thesystem towards choosing a correct translation.
Ad-ditional contextual information is used by the lan-guage model component in Moses, which con-siders how likely the sentence translation is inthe Spanish language (with a 5-gram languagemodel).Using the phrase alignment information, thetranslation of each occurrence of a source wordis identified in the output of Moses.
Since thephrase translations are learned using the Europarlcorpus, some translations are very specific to thatdomain.
Moreover, translations can be very noisy,given that the process is unsupervised.
We there-fore filter the translations given by Moses to keeponly those also given as possible Spanish trans-lations according to the semi-automatically builtEnglish-Spanish dictionary (Section 2.2).
This isa general-domain dictionary, but it is less likely tocontain noise.For best results, only the top translation pro-duced by Moses is considered.
If the actual trans-lation does not belong to the dictionary, the firsttranslation in that dictionary is used.
Althoughthere is no information about the order of thetranslations in the dictionaries used, by looking atthe translations provided, we believe that the firsttranslation is in general one of the most frequent.For oot results, the alternative translations pro-vided by the 1000-best translations are consid-ered.
In cases where fewer than 10 translationsare found, we extract the remaining ones from thehandcrafted dictionary following their given orderuntil 10 translations (when available) are found,without repetition.WLVusp system therefore combines contextualinformation as provided by Moses (via its phrasesand language model) and general translation infor-mation as provided by a dictionary.4 USPwlv SystemFor each source word occurring in the context ofa specific sentence, this system uses a linear com-bination of features to rank the options from anautomatically built English-Spanish dictionary.For the best subtask, the translation ranked firstis chosen, while for the oot subtask, the 10 bestranked translations are used without repetition.The building of the dictionary, the features usedand the learning scheme are described in what fol-lows.Dictionary Building The dictionary building isbased on the concept of inter-language Mutual In-formation (MI) (Raybaud et al, 2009).
It consistsin detecting which words in a source-languagesentence trigger the appearance of other words inits target-language translation.
The inter-languageMI in Equation 3 can be defined for pairs of source(s) and target (t) words by observing their occur-rences at the sentence level in a parallel, sentencealigned corpus.
Both simple (Equation 1) andjoint distributions (Equation 2) were built basedon the English-Spanish Europarl corpus using itsLemma.pos version (Section 2.3).pl(x) =countl(x)Total(1)pen,es(s, t) =fen,es(s, t)Total(2)MI(s, t) = pen,es(s, t)log(pen,es(s, t)pen(s)pes(t))(3)AvgMI(tj) =?li=1w(|i?
j|)MI(si, tj)?li=1w(|i?
j|)(4)In the equations, countl(x) is the number of sen-tences in which the word x appear in a corpus ofl-language texts; counten,es(s, t) is the number ofsentences in which source and target words co-occur in the parallel corpus; and Total is the to-tal number of sentences in the corpus of the lan-guage(s) under consideration.
The distributionspenand pesare monolingual and can been ex-tracted from any monolingual corpus.To prevent discontinuities in Equation 3, weused a smoothing technique to avoid null proba-bilities.
We assume that any monolingual eventoccurs at least once and the joint distribution issmoothed by a Guo?s factor ?
= 0.1 (Guo et al,2004):pen,es(s, t)?pen,es(s, t) + ?pen(s)pes(t)1 + ?119For each English source word, a list of Span-ish translations was produced and ranked accord-ing to inter-language MI.
From the resulting list,the 50-best translations constrained by the POS ofthe original English word were selected.Features The inter-language MI is a featurewhich indicates the global suitability of translat-ing a source token s into a target one t. However,inter-language MI is not able to provide local con-textual information, since it does not take into ac-count the source context sentence c. The followingfeatures were defined to achieve such capability:Weighted Average MI (aMI) consists in averag-ing the inter-language MI between the targetword tjand every source word s in the con-text sentence c (Raybaud et al, 2009).
TheMI component is scaled in a way that longrange dependencies are considered less im-portant, as shown in Equation 4.
The scalingfactor w(?)
is assigned 1 for verbs, nouns, ad-jectives and adverbs up to five positions fromthe source word, and 0 otherwise.
This fea-ture gives an idea of how well the elements ina window centered in the source word head(sj) align to the target word tj, representingthe suitability of tjtranslating sjin the givencontext.Modified Weighted Average MI (mMI) takesthe average MI as previously defined, exceptthat the source word head is not taken intoaccount.
In other words, the scaling functionin Equation 4 equals 0 also when |i?
j| = 0.It gives an idea of how well the source wordsalign to the target word tjwithout the stronginfluence of its source translation sj.
Thisshould provide less biased information to thelearning.Best from WLVusp (B) consists in a flag that in-dicates whether a candidate t is taken as thebest ranked option according to the WLVuspsystem.
The goal is to exploit the informa-tion from the SMT system and handcrafteddictionary used by that system.10-best from WLVusp (T) this feature is a flagwhich indicates whether a candidate t wasamong the 10 best ranked translations pro-vided by the WLVusp system.Online Learning In order to train a binary rank-ing system based on the trial dataset as our train-ing set, we used the online passive-aggressive al-gorithm MIRA (Crammer et al, 2006).
MIRA issaid to be passive-aggressive because it updatesthe parameters only when a misprediction is de-tected.
At training time, for each sentence a setof pairs of candidate translations is retrieved.
Foreach of these pairs, the rank given by the systemwith the current parameters is compared to the cor-rect rankh(?).
A loss function loss(?)
controls theupdates attributing non 0 values only for mispre-dictions.
In our implementation, it equals 1 forany mistake made by the model.Each element of the kind (c, s, t) = (sourcecontext sentence, source head, translation can-didate) is assigned a feature vector f(c, s, t) =?MI, aMI,mMI,B, T ?, which is modeled by avector of parameters w ?
R5.The binary ranking is defined as the task of find-ing the best parameters w which maximize thenumber of successful predictions.
A successfulprediction happens when the system is able to ranktwo translation candidates as expected.
For do-ing so, we define an oriented pair x = (a, b) ofcandidate translations of s in the context of c anda feature vector F (x) = f(c, s, a) ?
f(c, s, b).signal(w?F (x)) is the orientation the model givesto x, that is, whether the system believes a is bet-ter than b or vice versa.
Based on whether or notthat orientation is the same as that of the reference3, the algorithm takes the decision between updat-ing or not the parameters.
When an update occurs,it is the one that results in the minimal changes inthe parameters leading to correct labeling x, thatis, guaranteeing that after the update the systemwill rank (a, b) correctly.
Algorithm 1 presentsthe general method, as proposed in (Crammer etal., 2006).In the case of this binary ranking, the minimiza-tion problem has an analytic solution well definedas long as f(c, s, a) 6= f(c, s, b) and rankh(a) 6=rankh(b), otherwise signal(w ?
F (x)) or the hu-man label would not be defined, respectively.These conditions have an impact on the content ofPairs(c), the set of training points built upon thesystem outputs for c, which can only contain pairsof differently ranked translations.The learning scheme was initialized with a uni-3Given s in the context of c and (a, b) a pair of candidatetranslations of s, the reference produces 1 if rankh(a) >rankh(b) and ?1 if rankh(b) > rankh(a).120Algorithm 1 MIRA1: for c ?
Training Set do2: for x = (a, b) ?
Pairs(c) do3: y?
?
signal(w ?
F (x))4: z ?
correct label(x)5: w = argmaxu12||w ?
u||26: s.t.
u ?
F (x) ?
loss(y?, z)7: v ?
v + w8: T ?
T + 19: end for10: end for11: return1Tvform vector.
The average parameters after N = 5iterations over the training set was taken.5 Results5.1 Official resultsTables 1 and 2 show the main results obtained byour two systems in the official competition.
Wecontrast our systems?
results against the best base-line provided by the organizers, DIC, which con-siders translations from a dictionary and frequencyinformation from WordNet, and show the relativeposition of the system among the 14 participants.The metrics are defined in (Mihalcea et al, 2010).Subtask Metric Baseline WLVusp PositionBestR 24.34 25.27 4thP 24.34 25.27 3rdMode R 50.34 52.81 3rdMode P 50.34 52.81 4thOOTR 44.04 48.48 6thP 44.04 48.48 6thMode R 73.53 77.91 5thMode P 73.53 77.91 5thTable 1: Official results for WLVusp on the test set, com-pared to the highest baseline, DICT.
P = precision, R = recall.The last column shows the relative position of the system.Subtask Metric Baseline USPwlv PositionBestR 24.34 26.81 2ndP 24.34 26.81 3rdMode R 50.34 58.85 1stMode P 50.34 58.85 2ndOOTR 44.04 47.60 8thP 44.04 47.60 8thMode R 73.53 79.84 3rdMode P 73.53 79.84 3rdTable 2: Official results for USPwlv on the test set, com-pared to the highest baseline, DICT.
The last column showsthe relative position of the system.In the oot subtask, the original systems wereable to output the mode translation approximately80% of the times.
From those translations, nearly50% were actually considered as best options ac-cording to human annotators.
It is worth noticingthat we focused on the best subtask.
Therefore,for the oot subtask we did not exploit the fact thattranslations could be repeated to form the set of 10best translations.
For certain source words, our re-sulting set of translations is smaller than 10.
Forexample, in the WLVusp system, whenever theset of alternative translations identified in Moses?top 1000-best list did not contain 10 legitimatetranslations, that is, 10 translations also found inthe handcrafted dictionary, we simply copied othertranslations from that dictionary to amount 10 dif-ferent translations.
If they did not sum to 10 be-cause the list of translations in the dictionary wastoo short, we left the set as it was.
As a result, 58%of the 1000 test cases had fewer than 10 transla-tions, many of them with as few as two or threetranslations.
In fact, the list of oot results for thecomplete test set resulted in only 1, 950 transla-tions, when there could be 10, 000 (1, 000 test caseoccurrences ?
10 translations).
In the next sectionwe describe some additional experiments to takethis issue into account.5.2 Additional resultsAfter receiving the gold-standard data, we com-puted the scores for a number of variations of ourtwo systems.
For example, we checked whetherthe performance of USPwlv is too dependent onthe handcrafted dictionary, via the features B andT.
Table 3 presents the performance of two varia-tions of USPwlv: MI-aMI-mMI was trained with-out the two contextual flag features which dependon WLVusp.
MI-B-T was trained without the mu-tual information contextual features.
The variationMI-aMI-mMI of USPwlv performs well even inthe absence of the features coming from WLVusp,although the scores are lower.
These results showthe effectiveness of the learning scheme, sinceUSPwlv achieves better performance by combin-ing these feature variations, as compared to theirindividual performance.To provide an intuition on the contributionof the two different components in the systemWLVusp, we checked the proportion of times atranslation was provided by each of the compo-nents.
In the best subtask, 48% of the translationscame from Moses, while the remaining 52% pro-121Subtask Metric Baseline MI-aMI-mMI MI-B-TBestR 24.34 22.59 20.50P 24.34 22.59 20.50Mode R 50.34 50.21 44.01Mode P 50.34 50.21 44.01OOTR 39.65 47.60 32.75P 44.04 39.65 32.75Mode R 73.53 74.19 56.70Mode P 73.53 74.19 56.70Table 3: Comparing between variations of the systemUSPwlv on the test set and the highest baseline, DICT.
Thevariations are different sources of contextual knowledge: MI(MI?aMI?mMI) and the WLVusp (MI?B?T) system.vided by Moses were not found in the dictionary.In those cases, the first translation in the dictio-nary was used.
In the oot subtask, only 12% (246)of the translations came from Moses, while the re-maining (1, 704) came from the dictionary.
Thiscan be explained by the little variation in the n-best lists produced by Moses: most of the varia-tions account for word-order, punctuation, etc.Finally, we performed additional experiments inorder to exploit the possibility of replicating wellranked translations for the oot subtask.
Table 4presents the results of some strategies arbitrarilychosen for such replications.
For example, in thecolums labelled ?5?
we show the scores for re-peating (once) the 5 top translations.
Notice thatprecision and recall increase as we take fewer toptranslation and repeat them more times.
In termsof mode metrics, by reducing the number of dis-tinct translations from 10 to 5, USPwlv still out-performs (marginally) the baseline.
In general, thenew systems outperform the baseline and our pre-vious results (see Table 1 and 2) in terms of pre-cision and recall.
However, according to the othermode metrics, they are below our official systems.System Metric 5 4 3 2WLVuspR 69.09 88.36 105.32 122.29P 69.09 88.36 105.32 122.29Mode R 68.27 63.05 63.05 52.47Mode P 68.27 63.05 63.05 52.47USPwlvR 73.50 94.78 102.96 129.09P 73.50 94.78 102.96 129.09Mode R 73.77 68.27 62.62 57.40Mode P 73.77 68.27 62.62 57.40Table 4: Comparison between different strategies for dupli-cating answers in the task oot.
The systems output a numberof distinct guesses and through arbitrarily schemes replicatethem in order to complete a list of 10 translations.6 Discussion and future workWe have presented two systems combining con-textual information and a pre-defined set of trans-lations for cross-lingual lexical substitution.
Bothsystems performed particularly well in the bestsubtask.
A handcrafted dictionary has shown to beessential for the WLVusp system and also helpfulfor the USPwlv system, which uses an additionaldictionary automatically build from a parallel cor-pus.
We plan to investigate how such systems canbe improved by enhancing the corpus-based re-sources to further minimize the dependency on thehandcrafted dictionary.ReferencesMarine Carpuat and Dekai Wu.
2007.
Improving sta-tistical machine translation using word sense disam-biguation.
In Joint Conference on Empirical Meth-ods in Natural Language Processing and Computa-tional Natural Language Learning, pages 61?72.Yee Seng Chan and Hwee Tou Ng.
2007.
Word sensedisambiguation improves statistical machine transla-tion.
In 45th Annual Meeting of the Association forComputational Linguistics, pages 33?40.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-agressive algorithms.
Jornal of MachineLearning Research, 7:551?585.Gang Guo, Chao Huang, Hui Jiang, and Ren-HuaWang.
2004.
A comparative study on various con-fidence measures in large vocabulary speech recog-nition.
In International Symposium on Chinese Spo-ken Language Processing, pages 9?12.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.2010.
Semeval-2010 task 2: Cross-lingual lexicalsubstitution.
In SemEval-2010: 5th InternationalWorkshop on Semantic Evaluations.Sylvain Raybaud, Caroline Lavecchia, David Langlois,and Kamel Smaili.
2009.
Word- and sentence-level confidence measures for machine translation.In 13th Annual Conference of the European Associ-ation for Machine Translation, pages 104?111.Helmut Schmid.
2006.
Probabilistic part-of-speechtagging using decision trees.
In International Con-ference on New Methods in Natural Language Pro-cessing, pages 44?49.Lucia Specia, Mark Stevenson, and Maria das Grac?asVolpe Nunes.
2007.
Learning expressive models forword sense disambiguation.
In 45th Annual Meet-ing of the Association for Computational Linguis-tics, pages 41?148.122
