Proceedings of the Second Workshop on Metaphor in NLP, pages 27?32,Baltimore, MD, USA, 26 June 2014.c?2014 Association for Computational LinguisticsMulti-dimensional abstractness in cross-domain mappingsJonathan DunnDepartment of Computer Science / Illinois Institute of Technologyjonathan.edwin.dunn@gmail.comAbstractMetaphor is a cognitive process thatshapes abstract target concepts by map-ping them to concrete source concepts.Thus, many computational approaches tometaphor make reference, directly or in-directly, to the abstractness of words andconcepts.
The property of abstractness,however, remains theoretically and empir-ically unexplored.
This paper implementsa multi-dimensional definition of abstract-ness and tests the usefulness of each di-mension for detecting cross-domain map-pings.1 IntroductionThe idea of metaphor as cross-domain map-ping goes back, at least, to Black (1954), whomade explicit an earlier implicit view that lin-guistic metaphors depend upon non-linguistic(i.e., conceptual) connections between networksof concepts.
Black?s premises were later em-ployed to represent groups of related linguisticmetaphoric expressions using non-linguistic con-ceptual metaphors (for example, Reddy, 1979,and Lakoff & Johnson, 1980).
Inherent in thisapproach to representing metaphor is the ideathat metaphor is, at its core, a matter of cross-domain mapping (e.g., Lakoff, 1993); in otherwords, metaphor is a cognitive process that buildsor maps connections between networks of con-cepts.
The study of cognitive metaphor processeshas largely focused on content-specific representa-tions of such mappings within a number of contentdomains, such as TIME and IDEAS.
Thus, a cross-domain mapping may be represented as somethinglike ARGUMENT IS WAR.Computational approaches to metaphor, how-ever, have represented cross-domain mappingsusing higher-level properties like abstractness(Gandy, et al., 2013; Assaf, et al., 2013; Tsvetkov,et al., 2013; Turney, et al., 2011), semanticsimilarity (Li & Sporleder, 2010; Sporleder &Li, 2010), domain membership (Dunn, 2013a,2013b), word clusters that represent semantic sim-ilarity (Shutova, et al.
2013; Shutova & Sun,2013), and selectional preferences (Wilks, 1978;Mason, 2004).
Most of these approaches relyon some concept of abstractness, whether directly(e.g., in terms of abstractness ratings) or indi-rectly (e.g., in terms of clusters containing abstractwords).
Further, these approaches have viewed ab-stractness as a one-dimensional scale between ab-stract and concrete concepts, with metaphor cre-ating mappings from concrete source concepts toabstract target concepts.Although both theoretical and computationaltreatments of metaphor depend upon the conceptof abstractness, little has been done to either de-fine or operationalize the notion.
To fill this gap,this paper puts forward a multi-dimensional def-inition of abstractness and implements it in orderto test the usefulness of the dimensions of abstract-ness for detecting cross-domain mappings.2 Multi-dimensional abstractnessThis approach recognizes four dimensions of ab-stractness: Domain of the Referent, Domain ofthe Sense, Fact-Status, and Function-Status, eachof which has a range of values from more ab-stract to less abstract, as shown in Table 1.
Do-main refers to top-level categories in a hierarchi-cal ontology as in, for example, ontological se-mantics (Nirenburg & Raskin, 2004), which usesfour top-level domains: PHYSICAL, MENTAL, SO-CIAL, ABSTRACT.
Each concept belongs within acertain domain so that, at the highest level, cross-domain mappings can be represented as mappingsbetween, for example, a PHYSICAL concept andan ABSTRACT concept.
This dimension corre-sponds most with the traditional one-dimensional27approach to abstractness.Here we divide domain membership into twotypes: (i) Domain of the Sense and (ii) Domainof the Referent.
The idea is that a concept mayrefer to an object in one domain but define prop-erties of that concept relative to another domain.For example, the concept teacher refers to a PHYS-ICAL object, a human who has physical proper-ties.
At the same time, the concept teacher is de-fined or distinguished from other humans in termsof SOCIAL properties, such as being focused onthe education of students.
Thus, the referent ofthe concept is within the PHYSICAL domain butits sense is within the SOCIAL domain.
This isalso true, for example, of countries (e.g., Mexico)which refer to a PHYSICAL location but also to aSOCIAL entity, the government and people whoreside in that physical location.
It is importantto distinguish sense and reference when searchingfor cross-domain mappings because many con-cepts inherently map between different domains inthis way (and yet are not considered metaphoric).Within both types of Domain, ABSTRACT is thecategory with the highest abstractness and PHYSI-CAL with the least abstractness.Fact-Status is an ontological property as op-posed to a domain within a hierarchical ontol-ogy.
It represents the metaphysical property ofa concept?s dependence on human consciousness(Searle, 1995).
In other words, PHYSICAL-FACTSare those, like rocks and trees, which exist in theexternal world independent of human perceptions.NON-INTENTIONAL facts are involuntary humanperceptions such as pain or fear.
INTENTIONALfacts are voluntary products of individual humanconsciousness such as ideas and opinions.
COL-LECTIVE facts are products of the consciousnessof groups of humans, such as laws and govern-ments.
Thus, all categories except for PHYSICAL-FACTS are dependent on human consciousness.NON-INTENTIONAL and INTENTIONAL facts de-pend only on individuals, and in this sense are lessabstract than COLLECTIVE facts, which exist onlyif a group of humans agrees to recognize their ex-istence.
This dimension of abstractness measureshow dependent on human consciousness and howsocially-constructed a concept is, with COLLEC-TIVE facts being more socially-constructed (andthus more society-dependent) than the others.The final dimension of abstractness is Function-Status, which reflects how embedded function in-Property ValueDomain of the Referent AbstractMentalSocialPhysicalDomain of the Sense AbstractMentalSocialPhysicalFact-Status CollectiveIntentionalNon-IntentionalPhysicalFunction InstitutionalPhysical-UseNon-AgentiveNoneEvent-Status ObjectStateProcessAnimacy HumanAnimateInanimateUndefinedTable 1: Concept properties and values.formation is in the sense of a concept.
Function in-formation is human-dependent, being present onlyas assumed by humans; thus, this dimension isalso related to how human-centric a particular con-cept is.
Many concepts have no function informa-tion embedded in them, for example rock or tree,and these are the least human-dependent.
Someconcepts have NON-AGENTIVE functions, some-times called NATURAL functions; for example, thefunction of a heart is to pump blood.
Some con-cepts have PHYSICAL-USE functions, in which theembedded function is a reflection of how humansuse a physical object; for example, the functionof a hammer is to drive nails.
Finally, manyconcepts have embedded within them INSTITU-TIONAL functions, those which perform a socialfunction only insofar as a group of individualsagree that the social function is performed.
Forexample, a group of individuals may declare thatcertain taxes will be collected on income; but ifothers do not consent to the performance of thatfunction then it is not performed (e.g., if the grouphad no legal authority to do so).
Thus, INSTITU-TIONAL functions have the highest abstractness.28In addition to these dimensions of abstract-ness, two properties are added in order to testhow they interact with these dimensions of ab-stractness: Event-Status, distinguishing OBJECTSfrom STATES and PROCESSES, and Animacy, dis-tinguishing HUMANS from ANIMATE non-humansand INANIMATE objects.3 ImplementationThe system has two main steps: first, the inputtext is mapped to concepts in the Suggested Up-per Merged Ontology (Niles & Pease, 2001); sec-ond, features based on the ontological propertiesof these concepts are used to represent the inputsentences as a feature vector.
The text is processedusing Apache OpenNLP for tokenization, namedentity recognition, and part of speech tagging.Morpha (Minnen, et al., 2001) is used for lemma-tization.
At this point word sense disambiguationis performed using SenseRelate (Pedersen & Kol-hatkar, 2009), mapping the lexical words to thecorresponding WordNet senses.
These WordNetsenses are first mapped to SynSets and then to con-cepts in the SUMO ontology, using existing map-pings (Niles & Pease, 2003).
Thus, the input tothe second part of the system is the set of SUMOconcepts which are pointed to by the input text.The properties of these concepts are contained ina separate knowledge-base developed for this sys-tem and available from the author.
Each conceptin SUMO has a value for each of the concept prop-erties.
This value is fixed and is the same across allinstances of that concept.
Thus, SenseRelate dis-ambiguates input text into WordNet synsets whichare mapped onto SUMO concepts, at which pointthe mapping from concepts to concept propertiesis fixed.Feature Type NumberRelative value frequency 23Main value / concepts 6Other values / concepts 6Number of value types 6Total 41Table 2: Concept properties and values.The concept properties discussed above areused to create a total of 41 features as shown in Ta-ble 2: First, 23 features contain the total number ofinstances of each possible value for the propertiesin each sentence relative to the number of conceptspresent.
Second, 6 features contain the relative fre-quency of the most common values of a property(the ?main?
value) and 6 features the relative fre-quency of all the other values (the ?other?
value).Third, 6 features contain the number of types ofproperty values present in a sentence relative to thenumber of possible types.4 Evaluation of the FeaturesWe evaluated these features in a binary classifi-cation task using the VU Amsterdam MetaphorCorpus (Steen, et al., 2010), which consists of200,000 words from the British National Corpusdivided into four genres (academic, news, fiction,and spoken; the spoken genre was not evaluated)and annotated by five linguists.
Metaphoricallyused prepositions have been untagged, as haveambiguously metaphoric sentences.
Non-sentencefragments have been removed (e.g., titles and by-lines), along with very short sentences (e.g., ?Hesaid.?
).The first step was to evaluate the featuresindividually for their usefulness in detectingmetaphoric language, allowing us to ask theoreti-cal questions about which dimensions of abstract-ness are most related to metaphor.
The Classifier-SubSetEval feature in Weka (Hall, et al., 2009)was used with the logistic regression classifieron the full corpus with 10 fold cross-validation.Three different search algorithms were used to en-sure that the best possible combination of vari-ables was found: the Greedy Stepwise, Linear For-ward Selection, and Rank Search algorithms.
Thefinal feature rating was computed by taking the re-verse ranking given by the GreedyStepwise search(e.g., the top ranked feature out of 41 is given a41) and adding the number of folds for which thatfeature was selected by the other two algorithms.Table 3 below shows the top variables, arrangedby score.An interesting finding from this selection pro-cess is that each of the concept properties made thelist of the top 16 features in the form of the Prop-erty: Other feature.
In other words, the number ofminority values for each property is useful for de-tecting cross-domain mappings.
Next, each of thevalues for the Function property was a top feature,while only two of the Domain-Sense and one ofthe Domain-Referent properties made the list.
Theproperties of Animacy and Fact are represented bythe number of types present in the utterance, and29Property Feature ScoreFunction Other Values 45.5Fact-Status Other Values 41Animacy Types 39.1Fact-Status Collective 37.8Event-Status Other Values 31Function Non-Agentive 30.6Animacy Other Values 29.8Function Physical-Use 29.8Fact-Status Types 28.3Domain-Sense Abstract 27.1Domain-Sense Other Values 25.1Domain-Sense Mental 22.1Domain-Referent Social 21.8Function None 20.5Function Institutional 17.1Domain-Referent Other Values 12.8Table 3: Top features.Fact is also significant for the number of conceptswith the Collective value.
These are interesting,and unexpected, findings, because the most im-portant properties for detecting metaphor are notthe traditional Domain-defined notions of abstract-ness, either Sense or Referent, but rather thosenotions of abstractness which are tied to a con-cept?s degree of dependence on human conscious-ness and degree of being socially-constructed.Using these top 16 variables, a binary classifi-cation task was performed on the entire VU Am-sterdam Corpus, prepared as described above, us-ing the logistic regression algorithm with 10 foldcross-validation, giving the results shown belowin Table 4.
These results show that while the fullset of 41 features performs slightly better than theselect set of the top 16, the performance gain isfairly small.
For example, the F-measure on thefull corpus raises from 0.629, using only the top16 variables, to 0.649 using the full set of 41 vari-ables.
Thus, a similar performance is achievedmuch more efficiently (at least, in terms of theevaluation of the feature vectors; the top 16 vari-ables still require many of the other variables in or-der to be computed).
More importantly, this showsthat the different dimensions of abstractness canbe used to detect cross-domain mappings, licens-ing the inference that each of these operationaliza-tions of abstractness represents an important andindependent property of cross-domain mappings.Var.
Corpus Prec.
Recall F1Select Full 0.655 0.629 0.629All Full 0.672 0.691 0.649Select Academic 0.655 0.682 0.600All Academic 0.639 0.676 0.626Select Fiction 0.595 0.597 0.592All Fiction 0.642 0.642 0.642Select News 0.749 0.813 0.743All News 0.738 0.808 0.746Table 4: Results of evaluation.5 Relation between the dimensions ofabstractnessIn order to determine the relationship betweenthese dimensions of abstractness, to be sure thatthey are not measuring only a single scale, prin-cipal components analysis was used to determinehow many distinct groups are formed by the prop-erties and their values.
The written subset of theAmerican and Canadian portions of the Interna-tional Corpus of English, consisting of 44,189 sen-tences, was used for this task.
The corpus wasnot annotated for metaphor; rather, the purposeis to find the relation between the features acrossboth metaphor and non-metaphor, using the directoblimin rotation method.# Main Features CL.
Vari.1 Domain-Sense: Types .834 18.7%Domain-Ref.
: Types .816Event-Status: Types .8082 Fact-Status: Main .778 14.2%Fact-Status: Physical .7743 Domain-Sense: Physical .509 11.1%Domain-Ref.
: Physical .548Event-Status: Object .4514 Fact-Status: Intentional .990 10.6%Fact-Status: Collective .990Fact-Status: Other .9135 Domain-Sense: Abstract .997 6.6%Domain-Ref.
: Abstract .9976 Domain-Sense: Main .851 5.8%Domain-Ref.
: Main .7737 Function: Physical-Use .876 4.4%8 Event-Status: Process .574 3.6%9 Animacy: Main .800 2.9%10 Function: Non-Agentive .958 2.4%Table 5: Grouped features.30This procedure identified 10 components witheigenvalues above 1 containing unique highestvalue features, accounting for a cumulative 83.2%of the variance.
These components are shown inTable 5 along with the component loadings of themain features for each component and the amountof variance which the component explains.
Allfeatures with component loadings within 0.100 ofthe top feature are shown.These components show two important results:First, the division of the Domain property intoSense and Referent is not necessary because thetwo are always contained in the same compo-nents; in other words, these really constitute asingle-dimension of abstractness.
Second, Do-main, Function, and Fact-Status are not containedin the same components, but rather remain distin-guishable dimensions of abstractness.The important point of this analysis of the rela-tions between features is that, even for those sys-tems which do not represent abstractness in thisway (e.g., systems which use numeric scales in-stead of nominal attributes), the dimensions of ab-stractness used here do represent independent fac-tors.
In other words, there is more than one dimen-sion of abstractness.
Domain membership, whichcorresponds most closely to the traditional one-dimensional view, refers essentially to how con-crete or physical a concept is.
Thus, love is moreabstract than grass, but no distinction is possiblebetween love and war.
Fact-Status refers to howdependent on human consciousness a concept is.PHYSICAL concepts do not depend upon humansin order to exist.
Thus, PHYSICAL concepts will berepresented with the same degree of abstractnessby both the Domain and Fact-Status properties.However, Fact-Status adds distinctions betweenabstract concepts.
For example, ideas are notphysical, but laws are both non-physical and de-pend upon complex social agreements.
Function-Status refers to how much of the definition of aconcept is dependent upon Function informationwhich is, ultimately, only present in human un-derstandings of the concept.
This dimension addsdistinctions between even physical concepts.
Forexample, canes are just as physical as sticks, butcane embeds function information, that the objectis used to help a human to walk, and this functioninformation is dependent upon human conscious-ness.
These two additional and distinguishable di-mensions of abstractness, then, operationalize howdependent a concept is on human consciousnessand how socially-constructed it is.Using the traditional one-dimensional approachto abstractness, not all metaphors have abstract tar-get concepts.
For example, in the metaphoric ex-pressions ?My car drinks gasoline?
and ?My sur-geon is a butcher,?
the concepts CAR and SUR-GEON are both PHYSICAL concepts in terms ofDomain, and thus not abstract.
And yet these con-cepts are the targets of metaphors.
However, theconcept DRINKING, according to this system, hasan INTENTIONAL Fact-Status, because it is an ac-tion which is performed purposefully, and thus isan action which only sentient beings can perform.It is more abstract, then, than a verb like uses,which would not be metaphoric.
The second ex-ample, however, cannot be explained in this way,as both SURGEON and BUTCHER would have thesame concept properties (they are not included inthe knowledge-base; both map to HUMAN).
Thisphrase occurs only twice in the 450+ million wordCorpus of Contemporary American English, how-ever, and represents a rare exception to the rule.6 ConclusionsThis paper has examined the notion of abstract-ness, an essential component of many theoreti-cal and computational approaches to the cross-domain mappings which create metaphoric lan-guage.
There are two important findings: First,of the four posited dimensions of abstractness,three were shown to be both (1) members ofseparate components and (2) useful for detectingmetaphoric mappings.
These three dimensions,Domain Membership, Fact-Status, and Function-Status, are different and distinguishable ways ofdefining and operationalizing the key notion ofabstractness.
Second, and perhaps more impor-tantly, the Fact-Status and Function-Status di-mensions of abstractness, which are not directlypresent in the traditional one-dimensional viewof abstractness, were shown to be the most use-ful for detecting metaphoric mappings.
Al-though more evidence is needed, this suggeststhat cross-domain mappings are mappings fromless socially-constructed source concepts to moresocially-constructed target concepts and from lessconsciousness-dependent source concepts to moreconsciousness-dependent target concepts.
Thismulti-dimensional approach thus provides a moreprecise definition of abstractness.31ReferencesAssaf, D., Neuman, Y., Cohen, Y., Argamon, S.,Howard, N., Last, M., Koppel, M. 2013.
Why ?darkthoughts?
aren?t really dark: A novel algorithm formetaphor identification.
2013 IEEE Symposium onComputational Intelligence, Cognitive Algorithms,Mind, and Brain: 60?65.
Institute of Electrical andElectronics Engineers.Black, M. 1954.
Metaphor.
Proceedings of the Aris-totelian Society, New Series, 55: 273-294.Dunn, J.
2013a.
Evaluating the premises and results offour metaphor identification systems.
Proceedingsof the 14th International Conference on Computa-tional Linguistics and Intelligent Text Processing,Volume I: 471-486.
Berlin, Heidelberg: Springer-Verlag.Dunn, J.
2013b.
What metaphor identification systemscan tell us about metaphor-in-language.
Proceed-ings of the First Workshop on Metaphor in NLP: 1-10.
Association for Computational Linguistics.Gandy, L., Allan, N., Atallah, M., Frieder, O., Howard,N., Kanareykin, S., Argamon, S. 2013.
AutomaticIdentification of Conceptual Metaphors With Lim-ited Knowledge.
Proceedings of the 27th Confer-ence on Artificial Intelligence: 328?334.
Associa-tion for the Advancement of Artificial Intelligence.Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reute-mann, P., Witten, I. H. 2009.
The WEKA data min-ing software.
ACM SIGKDD Explorations Newslet-ter, 11(1): 10.Lakoff, G. 1993.
The contemporary theory ofmetaphor.
Metaphor and thought, 2nd edition: 202-251.
Cambridge, UK: Cambridge Univ Press.Lakoff, G., Johnson, M. 1980.
Metaphors we live by.Chicago: University Of Chicago Press.Li, L., Sporleder, C. 2010a.
Linguistic Cues for Distin-guishing Literal and Non-literal Usages.
Proceed-ings of the 23rd International Conference on Com-putational Linguistics: Posters: 683-691.
Associa-tion for Computational Linguistics.Li, L., Sporleder, C. 2010b.
Using Gaussian Mix-ture Models to Detect Figurative Language in Con-text.
Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics: 297?300.
Association for Computational Linguistics.Mason, Z.
2004.
CorMet: A Computational, Corpus-Based Conventional Metaphor Extraction System.Computational Linguistics, 30(1), 23-44.Minnen, G., Carroll, J., Pearce, D. 2001.
Appliedmorphological processing of English.
Natural Lan-guage Engineering, 7(3), 207-223.Niles, I., Pease, A.
2001.
Towards a standard upperontology.
Proceedings of the International Confer-ence on Formal Ontology in Information Systems:2-9.
Association for Computing Machinery.Niles, I., Pease, A.
2003.
Linking lexicons and on-tologies: Mapping WordNet to the Suggested UpperMerged Ontology.
Proceedings of the 2003 Inter-national Conference on Information and KnowledgeEngineering: 412-416.
World Congress in Com-puter Science, Computer Engineering, and AppliedComputing.Nirenburg, S., Raskin, V. 2004.
Ontological Seman-tics.
Cambridge, MA: MIT Press.Pedersen, T., Kolhatkar, V. 2009.WordNet::SenseRelate::AllWords?A broad cover-age word sense tagger that maximimizes semanticrelatedness.
Proceedings of Human LanguageTechnologies: The 2009 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics, Companion Volume:Demonstration Session: 17-20.
Association forComputational Linguistics.Reddy, M. 1979.
The conduit metaphor: A caseof frame conflict in our language about language.Metaphor and Thought, 1st edition: 284-310.
Cam-bridge, UK: Cambridge Univ Press.Searle, J.
1995.
The construction of social reality.
NewYork: The Free Press.Shutova, E., Sun, L. 2013.
Unsupervised MetaphorIdentification using Hierarchical Graph Factoriza-tion Clustering.
Proceedings of Human LanguageTechnologies: The 2013 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics: 978-988.
Associationfor Computational Linguistics.Shutova, E., Teufel, S., Korhonen, A.
2013.
Statis-tical Metaphor Processing.
Computational Linguis-tics, 39(2), 301-353.Steen, G. J., Dorst, A. G., Herrmann, J.
B., Kaal, A. A.,Krennmayr, T. 2010.
Metaphor in usage.
CognitiveLinguistics, 21(4), 765-796.Tsvetkov, Y., Mukomel, E., Gershman, A.
2013.Cross-Lingual Metaphor Detection Using CommonSemantic Features.
Proceedings of the First Work-shop on Metaphor in NLP: 45-51.
Association forComputational Linguistics.Turney, P. D., Neuman, Y., Assaf, D., Cohen, Y.2011.
Literal and Metaphorical Sense Identifica-tion Through Concrete and Abstract Context.
Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing: 680-690.
Associ-ation for Computational Linguistics.Wilks, Y.
1978.
Making preferences more active.
Ar-tificial Intelligence, 11(3), 197-223.32
