Feature Selection and Feature Extract ion for TextCategorizationDavid D. LewisCenter for Information and Language StudiesUniversity of ChicagoChicago, IL 60637ABSTRACTThe effect of selecting varying numbers and kinds of fea-tures for use in predicting category membership was inves-tigated on the Reuters and MUC-3 text categorization datasets.
Good categorization performance was achieved usinga statistical classifier and a proportional assignment strat-egy.
The optimal feature set size for word-based indexingwas found to be surprisingly low (10 to 15 features) despitethe large training sets.
The extraction of new text featuresby syntactic analysis and feature clustering was investigatedon the Reuters data set.
Syntactic indexing phrases, clus-ters of these phrases, and clusters of words were all found toprovide less effective representations than individual words.1.
IntroductionText categorization-the automated assigning of naturallanguage texts to  predefined categories based on theircontent-is a task of increasing importance.
Its appli-cations include indexing texts to support document re-trieval [I], extracting data from texts [2], and aiding hu-mans in these tasks.The indexing language used to represent texts influenceshow easily and effectively a text categorization systemcan be built, whether the system is built by human engi-neering, statistical training, or a combination of the two.The simplest indexing languages are formed by treatingeach word as a feature.
However, words have properties,such as synonymy and polysemy, that make them a lessthan ideal indexing language.
These have motivated at-tempts to  use more complex feature extraction methodsin text retrieval and text categorization tasks.If a syntactic parse of text is available, then featurescan be defined by the presence of two or more words inparticular syntactic relationships.
We call such a fea-ture a syntactic indexing phrase.
Another strategy is touse cluster analysis or other statistical methods to  de-tect closely related features.
Groups of such features canthen, for instance, be replaced by a single feature corre-sponding to their logical or numeric sum.
This strategyis referred to as term clustering.Syntactic phrase indexing and term clustering have op-posite effects on the properties of a text representation,which led us t o  investigate combining the two techniques[3].
However, the small size of standard text retrievaltest collections, and the variety of approaches availablefor query interpretation, made it difficult to  study purelyrepresentational issues in text retrieval experiments.
Inthis paper we examine indexing language properties us-ing two text categorization data  sets.
We obtain muchclearer results, as well as producing a new text catego-rization method capable of handing multiple, overlap-ping categories.2.
Data Sets and TasksOur first data  set was a set of 21,450 Reuters newswirestories from the year 1987 [4].
These stories have beenmanually indexed using 135 financial topic categories,to  support document routing and retrieval.
Particularcare was taken in assigning categories [I].
All storiesdated April 7, 1987 and earlier went into a set of 14,704training documents, and all stories from April 8, 1987 orlater went into a test set of 6,746 documents.The second data set consisted of 1,500 documents fromthe US.
Foreign Broadcast Information Service (FBIS)that had previously been used in the MUC-3 evaluationof natural language processing systems [2].
The docu-ments are mostly translations from Spanish to  English,and include newspaper stories, transcripts of broadcasts,communiques, and other material.The MUC-3 task required extracting simulated databaserecords ("templates") describing terrorist incidents fromthese texts.
Eight of the template slots had a limitednumber of possible fillers, so a simplification of the MUC-3 task is to  view filling these slots as text categorization.There were 88 combinations of these 8 slots and legalfillers for the slots, and each was treated as a binary cat-egory.
Other text categorization tasks can be defined forthe MUC-3 data (see Riloff and Lehnert in this volume).We used for our test set the 200 official MUC-3 test doc-uments, plus the first 100 training documents (DEV-MUC3-0001 through DEV-MUC3-0100).
Templates forthese 300 documents were encoded by the MUC-3 orga-nizers.
We used the other 1,200 MUC-3 training docu-ments (encoded by 16 different MUC-3 sites) as our cat-egorization training documents, Category assignmentsshould be quite consistent on our test set, but less so onour training set.3.
Categorization MethodThe statistical model used in our experiments was pro-posed by Fuhr [5] for probabilistic text retrieval, butthe adaptation to  text categorization is straightforward.Figure 1 shows the formula used.
The model allows thepossibility that the values of the binary features for adocument is not known with certainty, though that as-pect of the model was not used in our experiments.3.1.
Binary CategorizationIn order to  compare text categorization output with anexisting manual categorization we must replace probabil-ity estimates with explicit binary category assignments.Previous work on statistical text categorization has oftenignored this step, or has not dealt with the case wheredocuments can have zero, one, or multiple correct cate-gories.Given accurate estimates of P(Cj  = 11 Dm), decision the-ory tells us that the optimal strategy, assuming all errorshave equal cost, is to  set a single threshold p and assignCj to  a document exactly when P(Cj  = llD,) >= p [6].However, as is common in probabilistic models for textclassification tasks, the formula in Figure 1 makes as-sumptions about the independence of probabilities whichdo not hold for textual data.
The result is that the esti-mates of P (C j  = llDm) can be quite inaccurate, as wellas inconsistent across categories and documents.We investigated several strategies for dealing with thisproblem and settled on proportional assignment [4].Each category is assigned to its top scoring documentson the test set in a designated multiple of the percentageof documents it was assigned to on the training corpus.Proportional assignment is not very satisfactory from atheoretical standpoint, since the probabilistic model issupposed to already take into account the prior prob-ability of a category.
In tests the method was foundto perform well as a standard decision tree inductionmethod, however, so it is at least a plausible strategy.We are continuing to investigate other approaches.3.2.
Feature SelectionA primary concern of ours was to  examine the effectof feature set size on text categorization effectiveness.All potential features were ranked for each category byexpected mutual information [7] between assignment of0 WORDS-DF2: Starts with all words tokenized byparts.
Capitalization and syntactic class ignored.Stopwords discarded based on syntactic tags.
To-kens consisting solely of digits and punctuation re-moved.
Words occurring in fewer than 2 trainingdocuments removed.
Total terms: 22,791.0 WC-MUTINFO-135: Starts with WORDS-DF2,and discards words occurring in fewer than 5 ormore than 1029 (7%) training documents.
RNNclustering used 135 metafeatures with value equalto mutual information between presence of the wordand presence of a manual indexing category.
Resultis 1,442 clusters and 8,506 singlets, for a total of9,948 terms.0 PHRASE-DF2: Starts with all simple noun phrasesbracketed by parts.
Stopwords removed fromphrases based on tags.
Single word phrases dis-carded.
Numbers replaced with the token NUM-BER.
Phrases occurring in fewer than 2 trainingdocuments removed.
Total terms: 32,521.0 PC-W-GIVEN-C-44: Starts with PHRASE-DF2.Phrases occurring in fewer than 5 training docu-ments removed.
RNN clustering uses 44 metafea-tures with value equal t o  our estimate of P ( W  =1IC = 1) for phrase W and category C.  Result is1,883 clusters and 1,852 singlets, for a total of 3,735terms.Figure 2: Summary of indexing languages used with theReuters data set.that feature and assignment of that category.
The top kfeatures for each category were chosen as its feature set,and different values of k were investigated.4.
Indexing LanguagesWe investigated phrasal and term clustering methodsonly on the Reuters collection, since the smaller amountof text made the MUC-3 corpus less appropriate for clus-tering experiments.
For the MUC-3 data set a single in-dexing language consisting of 8,876 binary features wastested, corresponding to all words occurring in 2 or moretraining documents.
The original MUC-3 text was allcapitalized.
Stop words were not removed.For the Reuters data we adopted a conservative approachto syntactic phrase indexing.
The phrasal indexing lan-guage consisted only of simple noun phrases, i.e.
headnouns and their immediate premodifiers.
Phrases wereformed using parts, a stochastic syntactic class taggerand simple noun phrase bracketing program [8].
WordsWe estimate P (C j  = l lDm) by:P (C j  = 1) x ( p(wi= llcj = 1) x P(Wi = IlDm) + P(Wi = OlCj = 1) x P(Wi = OjD,) i P(Wj = 1) P(Wi = 0)Explanation:P(Cj  = llD,) is the probability that category Cj  is assigned to document Dm.
Estimating this probability isthe goal of the categorization procedure.
The index j ranges over categories to be assigned.P (C j  = 1) is the prior probability that category Cj  is assigned to a document, in the absence of any informationabout the contents of the particular document.P(Wj = 1) is the prior probability that feature Wi is present in a randomly selected document.
P(Wi = 0) =1 - P(Wi = 1).
The index i ranges over the set of predictor features for category q.P(Wi = l jCj = 1) is the probability that feature Wj is assigned t o  a document given that we know category Cjis assigned to that document.
P(Wi = OICj = 1) is 1 - P(Wi = lICj = 1).P(Wi = l lDm) is the probability that feature Wi is assigned to document Dm.All probabilities were estimated from the training corpus using the "add one" adjustment (the Jeffreys prior).Figure 1: Probabilistic model used for text categorization.that were tagged as function words were removed from 5 .
Evaluationphrases, and all items tagged as numbers were replaced The effectiveness measures used were recall (number ofwith the NUMBER' We wed the parts seg- categories correctly assigned divided by the total num-mentation to  define the set of words indexed on.
ber of categories that should be assigned) and precisionReciprocal nearest neighbor clustering was used for clus- (number ofcategorie~ correctly assigned divided by totaltering features.
An RNN cluster consists of two items, number of categories assigned).each of which is the nearest neighbor of the other ac-cording to the similarity metric in use.
Therefore, notall items are clustered.
If this stringent clustering strat-egy does not bring together closely related features, itis unlikely that any clustering method using the samemetafeatures would do so.Clustering features requires defining a set of metafea-tures on which the similarity of the features will bejudged.
We experimented with forming clusters fromwords under three metafeature definitions, and fromphrases under eight metafeature definitions 141.
Metafea-tures were based on presence or absence of features indocuments, or on the strength of association of featureswith categories of documents.
In all cases, similaritybetween metafeature vectors was measured using the co-sine correlation.
The sets of clusters formed were exam-ined by the author, and categorization experiments wererun with the three sets of word clusters and with thetwo sets of phrase clusters that appeared best.
Figure 2summarizes the properties of the most effective versionof each representation type used in the experiments onthe Reuters data.For a set of k categories and d documents a total ofn = kd categorization decisions are made.
We used mi-croaveraging, which considers all kd decisions as a singlegroup, t o  compute average effectiveness 191.
The propor-tionality parameter in our categorization method wasvaried to  show the possible tradeoffs between recall andprecision.
As a single summary figure for recall precisioncurves we took the breakeven point, i.e.
the highest value(interpolated) at which recall and precision are equal.6.
ResultsWe first looked a t  effectiveness of proportional assign-ment with word-based indexing languages.
Figure 3shows results for the best feature set sizes found: 10features on Reuters and 15 features on MUC-3.
Abreakeven point of 0.65 on Reuters and 0.48 on MUC-3 isreached.
For comparison, the operational AIR/X systemuses both rule-based and statistical techniques to achievea microaveraged breakeven point of approximately 0.65in indexing a physics database [lo].The CONSTRUE rule-based text categorization systemachieves a microaveraged breakeven of around 0.90 onPrec..8.6.4.20l i l l IReuters 0MUC-30 .2 .4 .6 .8 1RecallFigure 3: Microaveraged recall and precision on Reuters(w/ 10 features) and MUC-3 (w/ 15 features) test sets.a different, and possibly easier, testset drawn from theReuters data \[1\].
This level of performance, the resultof a 9.5 person-year effort, is an admirable target forlearning-based systems to shoot for.Comparison with published results on MUC-3 are diffi-cult, since we simplified the complex MUC-3 task.
How-ever, in earlier experiments using the official MUC-3 test-set and scoring, proportional assignment achieved per-formance toward but within the low end of official MUC-3 scores achieved by a variety of NLP methods.
Thisis despite being limited in most cases to 50% the scoreachievable by methods that attempted cross-referencingIn\].6.1.
Feature  Select ionFigure 4 summarizes our data on feature set size.
Weshow the breakeven point reached for categorization runswith various size sets of words, again on both the Reutersand MUC-3 data sets.
The results exhibit the classicpeak associated with the "curse of dimensionality."
Thesurprise is the small number of features found to be op-timal.
With 14,704 and 1,300 training examples, peaksof 10 and 15 features respectively are smaller than onewould expect based on sample size considerations.Overfitting, i.e.
training a model on accidental as wellas systematic relationships between feature values and.8.7.6.5B .E.
.4.3(.2.1O I. , , , , , , , \ [  .
, , , , .
, , \ [  , , , , .
, .
.Figure 4: Microaveraged breakeven points for featuresets of words on Reuters and MUC-3 test sets, and onReuters training set.category membership, was one possible villain \[6\].
Wechecked for overfitting directly by testing the inducedclassifiers on the training set.
The thicker line in Figure 4shows the effectiveness of the Reuters classifers whentested on the 14,704 stories used to train them.
Surpris-ingly, effectiveness reaches a peak not much higher thanthat achieved on the unseen test set, and even dropsoff when a very large feature set is used.
Apparently ourprobabilistic model is sufficiently constrained that, whileoverfitting occurs, its effects are limited3Another possible explanation for the decrease in effec-tiveness with increasing feature set size is that the as-sumptions of the probabilistic model are increasingly vi-olated.
Fuhr's model assumes that the probability ofobserving a word in a document is independent of theprobability of observing any other word in the document,both for documents in general and for documents knownto belong to particular categories.
The number of oppor-tunities for groups of dependent features to be selectedas predictor features for the same category increases asthe feature set size grows.Finally, since features with a higher value on expectedmutual information are selected first, we intuitively ex-pect features with lower ratings, and thus appearing onlyin the larger feature sets, to simply be worse features.This intuition is curiously hard to justify.
Any featurehas some set of conditional and uncondltionM probabili-ties and, if the assumptions of the statistical model hold,1We have  not  yet  made this test on the  MUC-3  ds ta  set.215Prec..8.6.4.20 I I I0 .2 .4 .6RecallI Iwordsword clusters 0phrases ?clusters \[.8 1Figure 5: Microaveraged recall and precision on Reuterstest set for WORDS-DF2 words (10 features), WC-MUTINFO-135 word clusters (10 features), PHRASE-DF2 phrases (180 features), and PC-W-GIVEN-C-44phrase clusters (90 features).will be used in an appropriate fashion.
It may be thatthe inevitable errors in estimating probabilities from asample are more harmful when a feature is less stronglyassociated with a category.6.2.
Feature  Ext rac t ionThe best results we obtained for each of the four ba-sic representations on the Reuters test set are shown inFigure 5.
Individual terms in a phrasal representationhave, on the average, a lower frequency of appearancethan terms in a word-based representation.
So, not sur-prisingly, effectiveness of a phrasal representation peaksat a much higher feature set size (around 180 features)than that of a word-based representation (see Figure 6).More phrases are needed simply to make any distinc-tions among documents.
Maximum effectiveness of thephrasal representation is also substantially lower thanthat of the word-based representation.
Low frequencyand high degree of synonymy outweigh the advantagesphrases have in lower ambiguity.Disappointingly, as shown in Figure 5, term cluster-ing did not significantly improve the quality of eithera word-based or phrasal representation.
Figure 7 showssome representative PC-W-GIVEN-C-44 phrase clusters..8.7.6.51B.E.
.4.3.2'.101i i i i , v l l  I i i i I l l l l  I I I I I l l l l 'wordsphrases 0, i , , , i , I  , i , , , i , *  I I I I I I l l10 100 1000DimensionalityFigure 6: Microaveraged breakeven point for varioussized feature sets of words and phrases on Reuters testset.
(The various abbreviations and other oddities in thephrases were present in the original text.)
Many of therelationships captured in the clusters appear to be acci-dental rather than the systematic semantic relationshipshoped for.Why did phrase clustering fail?
In earlier work on theCACM collection \[3\], we identified lack of training dataas a primary impediment to high quality cluster forma-tion.
The Reuters corpus provided approximately 1.5million phrase occurrences, a factor of 25 more thanCACM.
Still, it remains the case that the amount of datawas insufficient to measure the distributional properties'8 investors ervice inc, < amo >NUMBER accounts, slate regulatorsNUMBER elections, NUMBER enginesfederal reserve chairman paul volcker,private consumptionadditional NUMBER dlrs, america >canadian bonds, cme boarddenmark NUMBER, equivalent pricefund government-approved equity investments,fuji bank ledits share price, new venturenew policy, representative officessame-store sales, santa rosaFigure 7: Some representative PC-W-GIVEN-C-44 clus-ters.216of many phrases encountered.The definition of metafeatures i  a key issue to recon-sider.
Our original reasoning was that, since phraseshave low frequency, we should use metafeatures corre-sponding to bodies of text large enough that we couldexpect cooccurrences of phrases within them.
The poorquality of the clusters formed suggests that this ap-proach is not effective.
The use of such coarse-grainedmetafeatures simply gives many opportunities for acci-dental cooccurrences to arise, without providing a suffi-cient constraint on the relationship between phrases (orwords).
The fact that clusters captured few high qualitysemantic relationships, even when an extremely conser-vative clustering method was used, suggests that usingother clustering methods with the same metafeature def-initions is not likely to be effective.Finally, while phrases are less ambiguous than words,they are not all good content indicators.
Even restrict-ing phrase formation to simple noun phrases we see asubstantial number of poor content indicators, and theimpact of these are compounded when they are clusteredwith better content indicators.7.
Future WorkA great deal of research remains in developing text cat-egorization methods.
New approaches to setting appro-priate category thresholds, estimating probabilities, andselecting features need to be investigated.
For practicalsystems, combinations of knowledge-based and statisti-cal approaches are likely to be the best strategy.On the text representation side, we continue to believethat forming groups of syntactic indexing phrases is aneffective route to better indexing languages.
We be-lieve the key will be supplementing statistical evidenceof phrase similarity with evidence from thesauri andother knowledge sources, along with using metafeatureswhich provide tighter constraints on meaning.
Cluster-ing of words and phrases based on syntactic ontext isa promising approach (see Strzalkowski in this volume).Pruning out of low quality phrases is also likely to beimportant.8.
SummaryWe have shown a statistical classifier trained on manu-ally categorized documents to achieve quite effective per-formance in assigning multiple, overlapping categories todocuments.
We have also shown, via studying text cate-gorization effectiveness, a variety of properties of index-ing languages that are difficult or impossible to measuredirectly in text retrieval experiments, uch as effects offeature set size and performance of phrasal representa-tions in isolation from word-based representations.Like text categorization, text retrieval is a text classifi-cation task.
The results shown here for text categoriza-tion, in particular the ineffectiveness of term clusteringwith coarse-grained metafeatures, are likely to hold fortext retrieval as well, though further experimentation isnecessary.9.
AcknowledgmentsThanks to Bruce Croft, Paul Utgoff, Abe Bookstein andMarc Ringuette for helpful discussions.
This researchwas supported at U Mass Amherst by grant AFOSR-90-0110, and at the U Chicago by Ameritech.
Manythanks to Phil Hayes, Carnegie Group, and Reuters formaking available the Reuters data, and to Ken Churchand AT&T for making available parts.References1.
Hayes, P. and Weinstein, S. CONSTRUE/TIS: a systemfor content-based indexing of a database of news stories.In IAAIogO, 1990.2.
Sundheim, B., ed.
Proceedings ofthe Third Message Un-derstanding Evaluation and Conference, Morgan Kauf-mann, Los Altos, CA, May 1991.3.
Lewis, D. and Croft, W. Term clustering of syntacticphrases.
In ACM SIGIR-90, pp.
385-404, 1990.4.
Lewis, D. Representation a d Learning in InformationRetrieval.
PhD thesis, Computer Science Dept.
; Univ.of Mass.
; Amherst, MA, 1992.
Technical Report 91-93.5.
Fuhr, N. Models for retrieval with probabilistic index-ing.
Information Processing and Management, 25(1):55-72, 1989.6.
Duda, It.
and Hart, P. Pattern Classification and SceneAnalysis.
Wiley-Interscience, New York, 1973.7.
Hamming, It.
Coding and Information Theory.Prentice-Hall, Englewood Cliffs, N J, 1980.8.
Church, K. A stochastic parts program and noun phraseparser for unrestricted text.
In Second Conference onApplied NLP, pp.
136-143, 1988.9.
Lewis, D. Evaluating text categorization.
In Speech andNatural Language Workshop, p. 312-318, Feb. 1991.10.
Fuhr, N., et ai.
AIR/X--a rule-based multistage index-ing system for large subject fields.
In RIAO 91, pp.606-623, 1991.11.
Lewis, D. Data extraction as text categorization: An ex-periment with the MUC-3 corpus.
In Proceedings MUC-3, May 1991.217
