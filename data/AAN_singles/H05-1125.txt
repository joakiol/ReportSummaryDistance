Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 995?1002, Vancouver, October 2005. c?2005 Association for Computational LinguisticsThe Vocal Joystick: A Voice-Based Human-Computer Interface forIndividuals with Motor Impairments?Jeff A.
Bilmes?, Xiao Li?, Jonathan Malkin?, Kelley Kilanski?, Richard Wright?,Katrin Kirchhoff?, Amarnag Subramanya?, Susumu Harada?, James A.Landay?, Patricia Dowden?, Howard Chizeck??Dept.
of Electrical Engineering?Dept.
of Computer Science & Eng.?Dept.
of Linguistics?Dept.
of Speech & Hearing ScienceUniversity of WashingtonSeattle, WAAbstractWe present a novel voice-based human-computer interface designed to enable in-dividuals with motor impairments to usevocal parameters for continuous controltasks.
Since discrete spoken commandsare ill-suited to such tasks, our interfaceexploits a large set of continuous acoustic-phonetic parameters like pitch, loudness,vowel quality, etc.
Their selection is opti-mized with respect to automatic recogniz-ability, communication bandwidth, learn-ability, suitability, and ease of use.
Pa-rameters are extracted in real time, trans-formed via adaptation and acceleration,and converted into continuous control sig-nals.
This paper describes the basic en-gine, prototype applications (in particu-lar, voice-based web browsing and a con-trolled trajectory-following task), and ini-tial user studies confirming the feasibilityof this technology.1 IntroductionMany existing human-computer interfaces (e.g.,mouse and keyboard, touch screens, pen tablets,etc.)
are ill-suited to individuals with motorimpairments.
Specialized (and often expensive)human-computer interfaces that have been devel-oped specifically for this target group include sipand puff switches, head mice, eye-gaze devices, chinjoysticks and tongue switches.
While many indi-viduals with motor impairments have complete use?This material is based on work supported by the NationalScience Foundation under grant IIS-0326382.of their vocal system, these devices make little useof it.
Sip and puff switches, for example, have lowcommunication bandwidth, making it impossible toachieve more complex control tasks.Natural spoken language is often regarded asthe obvious choice for a human-computer inter-face.
However, despite significant research effortsin automatic speech recognition (ASR) (Huang etal., 2001), existing ASR systems are still not suf-ficiently robust to a wide variety of speaking condi-tions, noise, accented speakers, etc.
ASR-based in-terfaces are therefore often abandoned by users aftera short initial trial period.
In addition, natural speechis optimal for communication between humans butsub-optimal for manipulating computers, windows-icons-mouse-pointer (WIMP) interfaces, or otherelectro-mechanical devices (such as a prosthetic ro-botic arm).
Standard spoken language commandsare useful for discrete but not for continuous op-erations.
For example, in order to move a cursorfrom the bottom-left to the upper-right of a screen,the user might have to repeatedly utter ?up?
and?right?
or ?stop?
and ?go?
after setting an initial tra-jectory and rate, which is quite inefficient.
For thesereasons, we are developing alternative and reusablevoice-based assistive technology termed the ?VocalJoystick?
(VJ).2 The Vocal JoystickThe VJ approach has three main characteristics:1) Continuous control parameters: Unlike standardspeech recognition, the VJ engine exploits continu-ous vocal characteristics that go beyond simple se-quences of discrete speech sounds (such as syllablesor words) and include e.g., pitch, vowel quality, andloudness, which are then mapped to continuous con-995trol parameters.2) Discrete vocal commands: Unlike natural speech,the VJ discrete input language is based on a pre-designed set of sounds.
These sounds are selectedwith respect to acoustic discriminability (maximiz-ing recognizer accuracy), pronounceability (reduc-ing potential vocal strain), mnemonic characteris-tics (reducing cognitive load), robustness to environ-mental noise, and application appropriateness.3) Reusable infrastructure: Our goal is not to createa single application but to provide a modular librarythat can be incorporated by developers into a varietyof applications that can be controlled by voice.
TheVJ technology is not meant to replace standard ASRbut to enhance and be compatible with it.2.1 Vocal CharacteristicsThree continuous vocal characteristics are extractedby the VJ engine: energy, pitch, and vowel qual-ity, yielding four specifiable continuous degrees offreedom.
The first of these, localized acoustic en-ergy, is used for voice activity detection.
In addi-tion, it is normalized relative to the current voweldetected (see Section 3.3), and is used by our cur-rent VJ-WIMP application (Section 4) to control thevelocity of cursor movements.
For example, a loudvoice causes a faster movement than does a quietvoice.
The second parameter, pitch, is also extractedbut is currently not mapped to any control dimensionin the VJ-WIMP application but will be in the future.The third parameter is vowel quality.
Unlike conso-nants, which are characterized by a greater degree ofconstriction in the vocal tract, vowels have much in-herent signal energy and are therefore well-suited toenvironments where both high accuracy and noise-robustness are crucial.
Vowels can be characterizedusing a 2-D space parameterized by F1 and F2, thefirst and second vocal-tract formants (resonant fre-quencies).
We initially experimented with directlyextracting F1/F2 and using them for directly spec-ifying 2-D continuous control.
While we have notruled out the use of F1/F2 in the future, we haveso far found that even the best F1/F2 detection al-gorithms available are not yet accurate enough forprecise real-time specification of movement.
There-fore, we classify vowels directly and map them ontothe 2-D vowel space characterized by degree of con-striction (i.e., tongue height) and tongue body posi-tion (Figure 1).
In our VJ-WIMP application, we useDegreeof Constriction Front Central BackHighMidLowTongue Body Position[iy ] [ix ] [uw ][ey] [ax ] [ow ][ae ] [a] [aa ]Figure 1: Vowel configurations as a function of theirdominant articulatory configurations.the four corners of this chart to map to the 4 princi-ple directions of up, down, left, and right as shownin Figure 2 (note that the two figures are flipped androtated with respect to each other).
We have fourdifferent VJ systems running: A) a 4-class systemallowing only the specification of the 4 principle di-rections; B) a 5-class system that also includes thephone [ax] to act as a carrier when wishing to varyonly pitch and loudness; C) a 8-class system that in-cludes the four diagonal directions; and D) a 9-classsystem that includes all phones and directions.
Mostof the discussion in this paper refers to the 4-classsystem.A fourth vocal characteristic is also extractedby the VJ engine, namely discrete sounds.
Thesesounds may correspond to button presses as on amouse or joystick.
The choice of sounds dependson the application and are chosen according to char-acteristic 2 above.3 The VJ EngineOur system-level design goals are modularity, lowlatency, and maximal computational efficiency.
Forthis reason, we share common signal processingoperations in multiple signal extraction modules,which yields real-time performance but leaves con-siderable computational headroom for the back-endapplications being driven by the VJ engine.Figure 3 shows the VJ engine architecture havingthree modules: signal processing, pattern recogni-tion, and motion control.3.1 Signal ProcessingThe goal of the signal processing module is to ex-tract low-level acoustic features that can be used in996[iy ][ix ][uw ][ey][ow ][ae ][a][aa ][ax ]Figure 2: Vowel-direction mapping: vowels corre-sponding to directions.AcousticWaveform FeatureExtractionFeatures:EnergyNCCFF1/F2MFCCSignalProcessingEnergyVowelClassificationPatternRecognitionPitchTrackingDiscrete SoundRecognitionMotionParameters:xy-directions,Speed,Acceleration,Motion ControlSpaceTransformationMotionComputerInterfaceDriver AdaptationFigure 3: System organizationestimating the vocal characteristics.
The features weuse are energy, normalized cross-correlation coeffi-cients (NCCC), formant estimates, Mel-frequencycepstral coefficients (MFCCs), and formant esti-mates.
To extract features, the speech signal is PCMsampled at a rate of Fs =16,000Hz.
Energy is mea-sured on a frame-by-frame basis with a frame sizeof 25ms and a frame step of 10ms.
Pitch is ex-tracted with a frame size of 40ms and a frame step of10ms.
Multiple pattern recognition tasks may sharethe same acoustic features: for example, energy andNCCCs are used for pitch tracking, and energy andMFCCs can be used in vowel classification and dis-crete sound recognition.
Therefore, it is more ef-ficient to decouple feature extraction from patternrecognition, as is shown in Figure 3.3.2 Pattern RecognitionThe pattern recognition module uses the acousticfeatures to extract desired parameters.
The estima-tion and classification system must simultaneouslyperform energy computation (available from the in-put), pitch tracking, vowel classification, and dis-crete sound recognition.Many state-of-the-art pitch trackers are based ondynamic programming (DP).
This, however, oftenrequires the meticulous design of local DP cost func-tions.
The forms of these cost functions are usu-ally empirically determined and/or their parametersare tuned by algorithms such as gradient descent(D.Talkin, 1995).
Since different languages and ap-plications may follow very different pitch transitionpatterns, the cost functions optimized for certain lan-guages and applications may not be the most appro-priate for others.
Our VJ system utilizes a graphi-cal model mechanism to automatically optimize theparameters of these cost functions, and has beenshown to yield state-of-the-art performance (X.Li etal., 2004; J.Malkin et al, 2005).For frame-by-frame vowel classification, our de-sign constraints are the need for extremely low la-tency and low computational cost.
Probability es-timates for vowel classes thus need to be obtainedas soon as possible after the vowel has been utteredor after any small change in voice quality has oc-curred.
It is well known that models of vowel clas-sification that incorporate temporal dynamics suchas hidden Markov models (HMMs) can be quite ac-curate.
However, the frame-by-frame latency re-quirements of VJ make HMMs unsuitable for vowelclassification since HMMs estimate the likelihoodof a model based on the entire utterance.
An alter-native is to utilize causal ?HMM-filtering?, whichcomputes likelihoods at every frame based on allframes seen so far.
We have empirically found,however, that slightly non-causal and quite local-ized estimates of the vowel category probabilityis sufficient to achieve user satisfaction.
Specifi-cally, we obtain probability estimates of the formp(Vt|Xt??
, .
.
.
, Xt+?
), where V is a vowel class,and Xt??
, .
.
.
, Xt+?
are feature frames within alength 2?
+ 1 window of features centered at timet.
After several empirical trials, we decided onneural networks for vowel classification because ofthe availability of efficient discriminative training al-gorithms and their computational simplicity.
Specif-ically we use a simple 2-layer multi-layer percep-tron (Bishop, 1995) whose input layer consists of26 ?
7 = 182 nodes, where 26 is the dimension ofXt, the MFCC feature vector, and 2?
+ 1 = 7 is the997number of consecutive frames, and that has 50 hid-den nodes (the numbers 7 and 50 were determinedempirically).
The output layer has 4 output nodesrepresenting 4 vowel probabilities.
During training,the network is optimized to minimize the Kullback-Leibler (K-L) divergence between the output and thetrue label distribution, thus achieving the aforemen-tioned probabilistic interpretation.The VJ engine needs not only to detect that theuser is specifying a vowel (for continuous control)but also a consonant-vowel-consonant (CVC) pat-tern (for discrete control) quickly and with a lowprobability of confusion (a VJ system also uses Cand CV patterns for discrete commands).
Requir-ing an initial consonant will phonetically distinguishthese sounds from the pure vowel segments usedfor continuous control ?
the VJ system constantlymonitors for changes that indicate the beginning ofone of the discrete control commands.
The vowelwithin the CV and CVC patterns, moreover, can helpprevent background noise from being mis-classifiedas a discrete sound.
Lastly, each such pattern cur-rently requires an ending silence, so that the nextcommand (a new discrete sound or continuous con-trol vowel) can be accurately initiated.
In all cases, asimple threshold-based rejection mechanism is usedto reduce false positives.To recognize the discrete control signals, HMMsare employed since, as in standard speech recogni-tion, time warping is necessary to normalize for dif-ferent signal durations corresponding to the sameclass.
Specifically, we embed phone HMMs into?word?
(C, CV, or CVC) HMMs.
In this way, itis possible to train phone models using a trainingset that covers all possible phones, and then con-struct an application-specific discrete command vo-cabulary without retraining by recombining existingphone HMMs into new word HMMs.
Therefore,each VJ-driven application can have its own appro-priate discrete sound set.3.3 Motion Control: Direction and VelocityThe VJ motion control module receives several pat-tern recognition parameters and processes them toproduce output more appropriate for determining 2-D movement in the VJ-WIMP application.Initial experiments suggested that using pitch toaffect cursor velocity (Igarashi and Hughes, 2001)would be heavily constrained by an individual?s vo-cal range.
Giving priority to a more universal user-independent VJ system, we instead focused on rela-tive energy.
Our observation that users often becamequiet when trying to move small amounts confirmedenergy as a natural choice.
Drastically different in-trinsic average energy levels for each vowel, how-ever, meant that comparing all sounds to a global av-erage energy would create a large vowel-dependentbias.
To overcome this, we distribute the energy perframe among the different vowels, in proportion tothe probabilities output by the neural network, andtrack the average energy for each vowel indepen-dently.
By splitting the power in this way, there isno effect when probabilities are close to 1, and wesmooth out changes during vowel transitions whenprobabilities are more evenly distributed.There are many possible options for determiningvelocity (a vector capturing both direction and speedmagnitude) and ?acceleration?
(a function determin-ing how the control-to-display ratio changes basedon input parameters), and the different schemes havea large impact on user satisfaction.
Unlike a standardmouse cursor, where the mapping is from 2-D handmovement to a 2-D screen, the VJ system maps fromvocal-tract articulatory movement to a 2-D screen,and the transformation is not as straightforward.
Allvalues are for the current time frame t unless indi-cated otherwise.
First, a raw direction value is cal-culated for each axis j ?
{x, y} asdj =?ipi ?
?vi, ej?
(1)in which pi = p(Vt = i|Xt??,...,t+? )
is the proba-bility for vowel i at time t, vi is a unit vector in thedirection of vowel i, ej is the unit-length positive di-rectional basis vector along the j axis, and ?v, e?
isthe projection of vector v onto unit vector e. To de-termine movement speed, we first calculate a scalarfor each axis j assj =?imax[0, gi(pi ?
f(E?i))]?
|?vi, ej?|where E is the energy in the current frame, ?i is theaverage energy for vowel i, and f(?)
and gi(?)
arefunctions used for energy normalization and percep-tual scaling (such as logs and/or cube-roots).
Thistherefore allocates frame energy to direction basedon the vowel probabilities.
Lastly, we calculate thevelocity for axis j at the current frame asVj = ?
?
s?j ?
exp(?sj).
(2)998where ?
represents the overall system sensitivity andthe other values (?
and ?)
are warping constants, al-lowing the user to control the shape of the accelera-tion curve.
Typically only one of ?
and ?
is nonzero.Setting both to zero results in constant-speed move-ment along each axis, while ?
= 1 and ?
= 0gives a linear mapping that will scale motion withenergy but have no acceleration.
The current user-independent system uses ?
= 0.6, ?
= 1.0 and sets?
= 0.
Lastly, the final velocity along axis j is Vjdj .Future publications will report on systematic evalu-ations of different f(?)
and gi(?)
functions.3.4 Motion Control: User AdaptationSince vowel quality is used for continuous control,inaccuracies can arise due to speaker variability ow-ing to different speech loudness levels, vocal tractlengths, etc.
Moreover, a vowel class articulated byone user might partially overlap in acoustic spacewith a different vowel class from another user.
Thisimposes limitations on a purely user-independentvowel classifier.
Differences in speaker loudnessalone could cause significant unpredictability.
Tomitigate these problems, we have designed an adap-tation procedure where each user is asked to pro-nounce four pre-defined vowel sounds, each last-ing 2-3 seconds, at the beginning of a VJ ses-sion.
We have investigated several novel adaptationstrategies utilizing both neural networks and supportvector machines (SVM).
The fundamental idea be-hind them both is that an initial speaker-independenttransformation of the space is learned using train-ing data, and is represented by the first layer of aneural network.
Adaptation data then is used totransform various parameters of the classifier (e.g.,all or sub-portions of the neural network, or the para-meters of the SVM).
Further details of some of thesenovel adaptation strategies appear in (X.Li et al,2005), and the remainder will appear in forthcom-ing publications.
Also, the average energy values ofeach vowel for each user are recorded and used tonormalize the speed control rate mentioned above.Preliminary evaluations on the data so far collectedshow very good results, with adaptation reducing thevowel classification error rate by 18% for the 4-classcase, and 35% for the 8-class case.
Moreover, infor-mal studies have shown that users greatly prefer theVJ system after adaptation than before.4 Applications and VideosOur overall intent is for VJ to interface with a va-riety of applications, and our primary applicationso far has been to drive a standard WIMP interfacewith VJ controls, what we call the VJ-WIMP ap-plication.
The current VJ version allows left but-ton clicks (press and release, using the consonant[k]) as well as left button toggles (using consonant[ch]) to allow dragging.
Since WIMP interfacesare so general, this allows us to indirectly controla plethora of different applications.
Video demon-strations are available at the URL: http://ssli.ee.washington.edu/vj.One of our key VJ applications is vocal webbrowsing.
The video (dated 6/2005) shows exam-ples of two web browsing tasks, one as an exam-ple of navigating the New York Times web site, theother using Google Maps to select and zoom in on atarget area.
Section 5 describes a preliminary evalu-ation on these tasks.
We have also started using theVJ engine to control video games (third video ex-ample), have interfaced VJ with the Dasher system(Ward et al, 2000) (we call it the ?Vocal Dasher?
),and have also used VJ for figure drawing.Several additional direct VJ-applications havealso been developed.
Specifically, we have directlyinterfaced the VJ system into a simple blocks worldenvironment, where more precise object movementis possible than via the mouse driver.
Specifically,this environment can draw arbitrary trajectories, andcan precisely measure user fidelity when moving anobject along a trajectory.
Fidelity depends both onpositional accuracy and task duration.
One use ofthis environment shows the spatial direction corre-sponding to vocal effort (useful for training, forthvideo example).
Another shows a simple roboticarm being controlled by VJ.
We plan to use thisenvironment to perform formal and precise user-performance studies in future work.5 Preliminary User StudyWe conducted a preliminary user study1 to evaluatethe feasibility of VJ and to obtain feedback regard-ing specific difficulties in using the VJ-WIMP sys-tem.
While this study is not accurate in that: 1) itdoes not yet involve the intended target population1The user study presented here used an earlier version of VJthan the current improved one described in the preceding pages.999of individuals with motor impairments, and: 2) theusers had only a small amount of time to practice andbecome adept at using VJ, the study is still indica-tive of the VJ approach?s overall viability as a novelvoice-based human-computer interface method.
Thestudy quantitatively compares VJ performance witha standard desktop mouse, and provides qualitativemeasurement of the user?s perception of the system.5.1 Experiment SetupWe recruited seven participants ranging from age 22to 26, none of whom had any motor impairment.Of the seven participants, two were female and fivewere male.
All of them were graduate students inComputer Science, although none of them had pre-viously heard of or used VJ.
Four of the participantswere native English speakers; the other three had anAsian language as their mother tongue.We used a Dell Inspiron 9100 laptop with a 3.2GHz Intel Pentium IV processor running the FedoraCore 2 operating system, with a 1280 x 800 24-bitcolor display.
The laptop was equipped with an ex-ternal Microsoft IntelliMouse connected through theUSB port which was used for all of the tasks in-volving the mouse.
A head-mounted Amanda NC-61 microphone was used as the audio input device,while the audio feedback from the laptop was outputthrough the laptop speakers.
The Firefox browserwas used for all of the tasks, with the browser screenmaximized such that the only portion of the screenwhich was not displaying the contents of the webpage was the top navigation toolbar which was 30pixels high.5.2 Quantitative and Qualitative EvaluationAt the beginning of the quantitative evaluation, eachparticipant was given a brief description of the VJoperations and was shown a demonstration of thesystem by a practiced experimenter.
The participantwas then guided through an adaptation process dur-ing which she/he was asked to pronounce the fourdirectional vowels (Section 3.4).
After adaptation,the participant was given several minutes to practiceusing a simple target clicking application.
The quan-titative portion of our evaluation followed a within-participant design.
We exposed each participant totwo experimental conditions which we refer to asinput modalities: the mouse and the VJ.
Each par-ticipant completed two tasks on each modality, withone trial per task.The first task was a link navigation task (Link),in which the participants were asked to start from aspecific web page and follow a particular set of linksto reach a destination.
Before the trial, the experi-menter demonstrated the specified sequence of linksto the participant by using the mouse and clicking atthe appropriate links.
The participant was also pro-vided with a sheet of paper for their reference thatlisted the sequence of links that would lead them tothe target.
The web site we used was a ComputerScience Department student guide and the task in-volved following six links with the space betweeneach successive link including both horizontal andvertical components.The second task was map navigation (Map), inwhich the participant was asked to navigate an on-line map application from a starting view (showingthe entire USA) to get to a view showing a partic-ular campus.
The size of the map was 400x400pixels, and the set of available navigation controlssurrounding the map included ten discrete zoomlevel buttons, eight directional panning arrows, anda click inside the map causing the map to be centeredand zoomed in by one level.
Before the trial, a prac-ticed experimenter demonstrated how to locate thecampus map starting from the USA view to ensurethey were familiar with the geography.For each task, the participants performed one trialusing the mouse, and one trial using a 4-class VJ.The trials were presented to the participants in acounterbalanced order.
We recorded the completiontime for each trial, as well as the number of falsepositives (system interprets a click when the userdid not make a click sound), missed recognitions(the user makes a click sound but the system fails torecognize it as a click), and user errors (wheneverthe user clicks on an incorrect link).
The recordedtrial times include the time used by all of the aboveerrors including recovery time.After the completion of the quantitative evalu-ation, the participants were given a questionnairewhich consisted of 14 questions related to the partic-ipants?
perception of their experience using VJ suchas the degree of satisfaction, frustration, and embar-rassment.
The answers were encoded on a 7-pointLikert scale.
We also included a space where theparticipants could write in any comments, and an in-10000102030405060708090100Link MapTask typeTaskcompletiontime(seconds)MouseVocal JoystickFigure 4: Task complement times02468101214161820M,KoreaM,NortheastM,MidwestM,NortheastF,Mid-AtlanticF,ChinaM,ChinaParticipant (Gender, Origin)Number of missedrecognitionsLinkMapFigure 5: Missed recognitions by participantformal post-experiment interview was performed tosolicit further feedback.5.3 ResultsFigure 4 shows the task completion times for Linkand Map tasks, Figure 5 shows the breakdown ofclick errors by individual participants, Figure 6shows the average number of false positive andmissed recognition errors for each of the tasks.There was no instance of user error in any trial.
Fig-ure 7 shows the median of the responses to each ofthe fourteen questionnaire questions (error bars ineach plot show ?
standard error).
In our measure-ment of the task completion times, we consideredthe VJ?s recognition error rate as a fixed factor, andthus did not subtract the time spent during those er-rors from the task completion time.There were several other interesting observationsthat were made throughout the study.
We noticedthat the participants who had the least trouble withmissed recognitions for the clicking sound were ei-012345678910Link MapTask typeNumber of errorsFalse positiveMissed RecognitionFigure 6: Average number of click errors per task1.02.03.04.05.06.07.0Easyto learnEasyto useDiff icult tocontrolFrustrating FunTiringEmbarrassingIntuitiveErrorproneSelf-consciousSelf-consciousnessdecreasedVowel sounds distinguishableMap harder thansearchMotionmatchedintentionStronglyagreeStronglydisagreeFigure 7: Questionnaire resultsther female or with an Asian language background,as shown in Figure 5.
Our hypothesis regarding thebetter performance by female participants is that theoriginal click sound was trained on one of our fe-male researcher?s voice.
We plan also in future workto determine how the characteristics of different na-tive language speakers influence VJ, and ultimatelyto correct for any bias.All but one user explicitly expressed their confu-sion in distinguishing between the [ae] and [aa] vow-els.
Four of the seven participants independentlystated that their performance would probably havebeen better if they had been able to practice longer,and did not attribute their perceived suboptimal per-formance to the quality of the VJ?s recognition sys-tem.
Several participants reported that they felt theirvocal cords were strained due to having to produce aloud sound in order to get the cursor to move at thedesired speed.
We suspect this is due either to ana-log gain problems or to their adapted voice being tooloud, and therefore the system calibrating the nor-mal speed to correspond to the loud voice.
We havesince removed this problem by adjusting our adapta-1001tion strategy to express preference for a quiet voice.In summary, the results from our study suggestthat users without any prior experience were ableto perform basic mouse based tasks using the VocalJoystick system with relative slowdown of four tonine times compared to a conventional mouse.
Weanticipate that future planned improvements in thealgorithms underlying the VJ engine (to improve ac-curacy, user-independence, adaptation, and speed)will further increase the VJ system?s viability, andcombined with practice could improve VJ enough sothat it becomes a reasonable alternative compared toa standard mouse?s performance.6 Related WorkRelated voice-based interface studies include(Igarashi and Hughes, 2001; Olwal and Feiner,2005).
Igarashi & Hughes presented a system wherenon-verbal voice features control a mouse system ?their system requires a command-like discrete soundto determine direction before initiating a movementcommand, where pitch is used to control veloc-ity.
We have empirically found an energy-basedmapping for velocity (as used in our VJ system)both more reliable (no pitch-tracking errors) andintuitive.
Olwal & Feiner?s system moves the mouseonly after recognizing entire words.
de Mauro?s?voice mouse?
http://www.dii.unisi.it/?maggini/research/voice mouse.htmlfocuses on continuous cursor movements similarto the VJ scenario; however, the voice mouseonly starts moving after the vocalization has beencompleted leading to long latencies, and it is noteasily portable to other applications.
Lastly, thecommercial dictation program Dragon by ScanSoftincludes MouseGridTM(Dra, 2004) which allowsdiscrete vocal commands to recursively 9-partitionthe screen, thus achieving log-command access to aparticular screen point.
A VJ system, by contrast,uses continuous aspects of the voice, has changelatency (about 60ms) not much greater than reactiontime, and allows the user to make instantaneousdirectional change using one?s voice (e.g., a usercan draw a ?U?
shape in one breath).7 ConclusionsWe have presented new voice-based assistive tech-nology for continuous control tasks and havedemonstrated an initial system implementation ofthis concept.
An initial user study using a groupof individuals from the non-target population con-firmed the feasibility of this technology.
We plannext to further improve our system by evaluating anumber of novel pattern classification techniques toincrease accuracy and user-independence, and to in-troduce additional vocal characteristics (possibilitiesinclude vibrato, degree of nasality, rate of changeof any of the above as an independent parameter)to increase the available simultaneous degrees offreedom controllable via the voice.
Moreover, weplan to develop algorithms to decouple unintendeduser correlations of these parameters, and to furtheradvance both our adaptation and acceleration algo-rithms.ReferencesC.
Bishop.
1995.
Neural Networks for Pattern Recogni-tion.
Clarendon Press, Oxford.2004.
Dragon naturally speaking, MousegridTM, Scan-Soft Inc.D.Talkin.
1995.
A robust algorithm for pitch track-ing (RAPT).
In W.B.Kleign and K.K.Paliwal, editors,Speech Coding and Synthesis, pp.
495?515, Amster-dam.
Elsevier Science.X.
Huang, A. Acero, and H.-W. Hon.
2001.
Spoken Lan-guage Processing: A Guide to Theory, Algorithm, andSystem Development.
Prentice Hall.T.
Igarashi and J. F. Hughes.
2001.
Voice as sound: Us-ing non-verbal voice input for interactive control.
InACM UIST 2001, November.J.Malkin, X.Li, and J.Bilmes.
2005.
A graphical modelfor formant tracking.
In Proc.
IEEE Intl.
Conf.
onAcoustics, Speech, and Signal Processing.A.
Olwal and S. Feiner.
2005.
Interaction techniques us-ing prosodic feature of speech and audio localization.In Proceedings of the 10th International Conferenceon Intelligent User Interfaces, pp.
284?286.D.
Ward, A. F. Blackwell, and D. C. MacKay.
2000.Dasher - a data entry interface using continuous ges-tures and language models.
In ACM UIST 2000.X.Li, J.Malkin, and J.Bilmes.
2004.
A graphical modelapproach to pitch tracking.
In Proc.
Int.
Conf.
on Spo-ken Language Processing.X.Li, J.Bilmes, and J.Malkin.
2005.
Maximum mar-gin learning and adaptation of MLP classifers.
In 9thEuropean Conference on Speech Communication andTechnology (Eurospeech?05), Lisbon, Portugal, Sep-tember.1002
