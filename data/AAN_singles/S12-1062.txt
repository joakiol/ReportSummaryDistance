First Joint Conference on Lexical and Computational Semantics (*SEM), pages 454?460,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUNED: Evaluating Text Similarity Measures without Human Assessments?Enrique Amigo?
?
Julio Gonzalo ?
Jesu?s Gime?nez ?
Felisa Verdejo??
UNED, Madrid{enrique,julio,felisa}@lsi.uned.es?
Google, Dublinjesgim@gmail.comAbstractThis paper describes the participation ofUNED NLP group in the SEMEVAL 2012 Se-mantic Textual Similarity task.
Our contribu-tion consists of an unsupervised method, Het-erogeneity Based Ranking (HBR), to combinesimilarity measures.
Our runs focus on com-bining standard similarity measures for Ma-chine Translation.
The Pearson correlationachieved is outperformed by other systems,due to the limitation of MT evaluation mea-sures in the context of this task.
However,the combination of system outputs that partici-pated in the campaign produces three interest-ing results: (i) Combining all systems withoutconsidering any kind of human assessmentsachieve a similar performance than the bestpeers in all test corpora, (ii) combining the 40less reliable peers in the evaluation campaignachieves similar results; and (iii) the correla-tion between peers and HBR predicts, with a0.94 correlation, the performance of measuresaccording to human assessments.1 IntroductionImagine that we are interested in developing com-putable measures that estimate the semantic simi-larity between two sentences.
This is the focus ofthe STS workshop in which this paper is presented.In order to optimize the approaches, the organizers?This work has been partially funded by the Madrid gov-ernment, grant MA2VICMR (S-2009/TIC- 1542), the Span-ish government, grant Holopedia (TIN2010-21128-C02-01) andthe European Community?s Seventh Framework Programme(FP7/ 2007-2013) under grant agreement nr.
288024 (LiMo-SINe project).provide a training corpus with human assessments.The participants must improve their approaches andselect three runs to participate.
Unfortunately, wecan not ensure that systems will behave similarlyin both the training and test corpora.
For instance,some Pearson correlations between system achieve-ments across test corpora in this competition are:0.61 (MSRpar-MSRvid), 0.34 (MSRvid-SMTeur),or 0.49 (MSRpar-SMTeur).
Therefore, we cannotexpect a high correlation between the system per-formance in a specific corpus and the test corporaemployed in the competition.Now, imagine that we have a magic box that,given a set of similarity measures, is able to pre-dict which measures will obtain the highest corre-lation with human assessments without actually re-quiring those assessments.
For instance, supposethat putting all system outputs in the magic box, weobtain a 0.94 Pearson correlation between the pre-diction and the system achievements according tohuman assessments, as in Figure 1.
The horizontalaxis represents the magic box ouput, and the verticalaxis represents the achievement in the competition.Each dot represents one system.
In this case, wecould decide which system or system combinationto employ for a certain test set.Is there something like this magic box?
Theanswer is yes.
Indeed, what Figure 1 shows isthe results of an unsupervised method to combinemeasures, the Heterogeneity Based Ranking (HBR).This method is grounded on a generalization of theheterogeneity property of text evaluation measuresproposed in (Amigo?
et al, 2011), which states thatthe more a set of measures is heterogeneous, the454Figure 1: Correspondence between the magic box infor-mation and the (unknown) correlation with human assess-ments, considering all runs in the evaluation campaign.more a score increase according to all the mea-sures is reliable.
In brief, the HBR method consistsof computing the heterogeneity of the set of mea-sures (systems) for which a similarity instance (pairof texts) improves each of the rest of similarity in-stances in comparison.
The result is that HBR tendsto achieve a similar or higher correlation with humanassessments than the single measures.
In order toselect the most appropriate single measure, we canmeta-evaluate measures in terms of correlation withHBR, which is what the previous figure showed.We participated in the STS evaluation campaignemploying HBR over automatic text evaluation mea-sures (e.g.
ROUGE (Lin, 2004)), which are not actu-ally designed for this specific problem.
For this rea-son our results were suboptimal.
However, accord-ing to our experiments this method seem highly use-ful for combining and evaluating current systems.In this paper, we describe the HBR method and wepresent experiments employing the rest of partici-pant methods as similarity measures.2 Definitions2.1 Similarity measuresIn (Amigo?
et al, 2011) a novel definition of sim-ilarity is proposed in the context of automatic textevaluation measures.
Here we extend the definitionfor text similarity problems in general.Being ?
the universe of texts d, we assume thata similarity measure, is a function x : ?2 ?
?< such that there exists a decomposition functionf : ?
??
{e1..en} (e.g., words or other linguis-tic units or relationships) satisfying the followingconstraints; (i) maximum similarity is achieved onlywhen the text decomposition resembles exactly theother text; (ii) adding one element from the secondtext increases the similarity; and (iii) removing oneelement that does not appear in the second text alsoincreases the similarity.f(d1) = f(d2)?
x(d1, d2) = 1(f(d1) = f(d?1) ?
{e ?
f(d2) \ f(d1)})?
x(d?1, d2) > x(d1, d2)(f(d1) = f(d?1)?
{e ?
f(d1) \ f(d2)})?
x(d?1, d2) > x(d1, d2)According to this definition, a random function,or the inverse of a similarity function (e.g.
1x(d1d2) ),do not satisfy the similarity constraints, and there-fore cannot be considered as similarity measures.However, this definition covers any kind of overlap-ping or precision/recall measure over words, syntac-tic structures or semantic units, which is the case ofmost systems here.Our definition assumes that measures are granu-lated: they decompose text in a certain amount ofelements (e.g.
words, grammatical tags, etc.)
whichare the basic representation and comparison units toestimate textual similarity.2.2 HeterogeneityHeterogeneity (Amigo?
et al, 2011) represents towhat extent a set of measures differ from each other.Let us refer to a pair of texts i = (i1, i2) with acertain degree of similarity to be computed as a sim-ilarity instance.
Then we estimate the Heterogene-ity H(X ) of a set of similarity measures X as theprobability over similarity instances i = (i1, i2) andj = (j1, j2) between distinct texts, that there existtwo measures in X that contradict each other.
For-mally:H(X ) ?
Pi1 6=i2j1 6=j2(?x, x?
?
X|x(i) > x(j) ?
x?
(j) < x?
(i))where x(i) stands for the similarity, according tomeasure x, between the texts i1, i2.4553 Proposal: Heterogeneity-BasedSimilarity RankingThe heterogeneity property of text evaluation mea-sures (in fact, text similarity measures to human ref-erences) introduced in (Amigo?
et al, 2011) statesthat the quality difference between two texts is lowerbounded by the heterogeneity of the set of evalua-tion measures that corroborate the quality increase.Based on this, we define the Heterogeneity Principlewhich is applied to text similarity in general as: theprobability of a real similarity increase between ran-dom text pairs is correlated with the Heterogeneityof the set of measures that corroborate this increase:P (h(i) ?
h(j)) ?
H({x|x(i) ?
x(j)})where h(i) is the similarity between i1, i2 accord-ing to human assessments (gold standard).
In addi-tion, the probability is maximal if the heterogeneityis maximal:H({x|x(i) ?
x(j)}) = 1?
P (h(i) ?
h(j)) = 1The first part is derived from the fact that increas-ing Heterogeneity requires additional diverse mea-sures corroborating the similarity increase.
The di-rect relationship is the result of assuming that a sim-ilarity increase according to any aspect is always apositive evidence of true similarity.
In other words,a positive match between two texts according to anyfeature can never be a negative evidence of similar-ity.As for the second part, if the heterogeneity of ameasure set is maximal, then the condition of theheterogeneity definition holds for any pair of dis-tinct documents (i1 6= i2 and j1 6= j2).
Given thatall measures corroborate the similarity increase, theheterogeneity condition does not hold.
Then, thecompared texts in (i1, i2) are not different.
There-fore, we can ensure that P (h(i) ?
h(j)) = 1.The proposal in this paper consists of rank-ing similarity instances by estimating, for each in-stance i, the average probability of its texts (i1, i2)being closer to each other than texts in a differentinstance j:R(i) = Avgj(P (h(i) ?
h(j)))Applying the heterogeneity principle we can esti-mate this as:HBRX (i) = Avgj(H({x|x(i) ?
x(j)}))We refer to this ranking function as the Heterogene-ity Based Ranking (HBR).
It satisfies three crucialproperties for a measure combining function:1.
HBR is independent from measure scales andit does not require relative weighting schemesbetween measures.
Formally, being f any strictgrowing function:HBRx1..xn(i) = HBRx1..f(xn)(i)2.
HBR is not sensitive to redundant measures:HBRx1..xn(i) = HBRx1..xn,xn(i)3.
Given a large enough set of similarityinstances, HBR is not sensitive to non-informative measures.
Being xr a randomfunction such that P (xr(i) > xr(j)) = 12 ,then:HBRx1..xn(i) ?
HBRx1..xn,xr(i)The first two properties are trivially satisfied: the?
operator in H and the score comparisons are not af-fected by redundant measures nor their scales prop-erties.
Regarding the third property, the heterogene-ity of a set of measures plus a random function xris:H(X ?
{xr}) ?Pi1 6=i2j1 6=j2(?x, x?
?
X ?
{xr}|x(i) > x(j) ?
x?
(j) < x?
(i)) =H(X ) + (1?H(X )) ?12=H(X ) + 12That is, the heterogeneity grows proportionallywhen including a random function.
Assuming thatthe random function corroborates the similarity in-crease in a half of cases, the result is a proportionalrelationship between HBR and HBR with the addi-tional measure.
Note that we need to assume a largeenough amount of data to avoid random effects.4564 Official RunsWe have applied the HBR method with excellentresults in different tasks such as Machine Transla-tion and Summarization evaluation measures, Infor-mation Retrieval and Document Clustering.
How-ever, we had not previously applied our method tosemantic similarity.
Therefore, we decided to ap-ply directly automatic evaluation measures for Ma-chine Translation as single similarity measures to becombined by means of HBR.
We have used 64 auto-matic evaluation measures provided by the ASIYAToolkit (Gime?nez and Ma`rquez, 2010)1.
This set in-cludes measures operating at different linguistic lev-els (lexical, syntactic, and semantic) and includes allpopular measures (BLEU, NIST, GTM, METEOR,ROUGE, etc.)
The similarity formal constraints inthis set of measures is preserved by considering lex-ical overlap when the target linguistic elements (i.e.named entities) do not appear in the texts.We participated with three runs.
The first one con-sisted of selecting the best measure according to hu-man assessments in the training corpus.
It was theINIST measure (Doddington, 2002).
The second runconsisted of selecting the best 34 measures in thetraining corpus and combining them with HBR, andthe last run consisted of combining all evaluationmeasures with HBR.
The heterogeneity of measureswas computed over 1000 samples of similarity in-stance pairs (pairs of sentences pairs) extracted fromthe five test sets.
Similarity instances were rankedover each test set independently.In essence, the main contribution of these runs isto corroborate that Machine Translation evaluationmeasures are not enough to solve this task.
Our runsappear at the Mean Rank positions 42, 28 and 77.Apart of this, our results corroborate our main hy-pothesis: without considering human assessment orany kind of supervised tunning, combining the mea-sures with HBR resembles the best measure (INIST)in the combined measure set.
However, when in-cluding all measures the evaluation result decreases(rank 77).
The reason is that some Machine Trans-lation evaluation measures do not represent a posi-tive evidence of semantic similarity in this corpus.Therefore, the HBR assumptions are not satisfiedand the final correlation achieved is lower.
In sum-1http://www.lsi.upc.edu/ nlp/Asiyamary, our approach is suitable if we can ensure thatall measures (systems) combined are at least a posi-tive (high or low) evidence of semantic similarity.But let us focus on the HBR behavior when com-bining participant measures, which are specificallydesigned to address this problem.5 Experiment with Participant Systems5.1 Combining System OutputsWe can confirm empirically in the official resultsthat all participants runs are positive evidence of se-mantic similarity.
That is, they achieve a correlationwith human assessments higher than 0.
Therefore,the conditions to apply HBR are satisfied.
Our goalnow is to resemble the best performance without ac-cessing human assessments neither from the trainingnor the test corpora.
Figure 2 illustrates the Pear-son correlation (averaged across test sets) achievedby single measures (participants) and all peers com-bined in an unsupervised manner by HBR (blackcolumn).
As the figure shows, HBR results are com-parable with the best systems appearing in the ninthposition.
In addition, Figure 4 shows the differencesover particular test sets between HBR and the bestsystem.
The figure shows that there are not con-sistent differences between these approaches acrosstest beds.The next question is why HBR is not able to im-prove the best system.
Our intuition is that, in thistest set, average quality systems do not contributewith additional information.
That is, the similarityaspects that the average quality systems are able tocapture are also captured by the best system.However, the best system within the combined setis not a theoretical upper bound for HBR.
We canprove it with the following experiment.
We applyHBR considering only the 40 less predictive systemsin the set (the rest of measures are not consideredwhen computing HBR).
Then we compare the re-sults of HBR regarding the considered single sys-tems.
As Figure 3 shows, HBR improves substan-tially all single systems achieving the same resultthan when combining all systems (0.61).
The rea-son is that all these systems are positive evidencesbut they consider partial similarity aspects.
But themost important issue here is that combining the 40less predictive systems in the evaluation campaign457Figure 2: Measures (runs) and HBR sorted by average correlation with human assessments.Figure 3: 40 less predictive measures (runs) and HBRsorted by average correlation with human assessments.is enough to achieve high final scores.
This meansthat the drawback of these measures as a whole isnot what information is employed but how this in-formation is scaled and combined.
This drawback issolved by the HBR approach.In summary, the main conclusion that we can ex-tract from these results is that, in the absence of hu-man assessments, HBR ensures a high performancewithout the risk derived from employing potentiallybiased training corpora or measures based on partialsimilarity aspects.6 An Unsupervised Meta-evaluationMethodBut HBR has an important drawback: its computa-tional cost, which isO(n4 ?m), being n the numberFigure 4: Average correlation with human assessmentsfor the best runs and HBR.of texts involved in the computation and m the num-ber of measures.
The reason is that computing H isquadratic with the number of texts, and the methodrequires to compute H for every pair of texts.
Inaddition, HBR does not improve the best systems.However, HBR can be employed as an unsuper-vised evaluation method.
For this, it is enough tocompute the Pearson correlation between runs andHBR.
This is what Figure 1 showed at the beginningof this article.
For each dot (participant run), thehorizontal axis represent the correlation with HBR(magic box) and the vertical axis represent the cor-relation with human assessments.
This graph has aPearson correlation of 0.94 between both variables.In other words, without accessing human assess-ments, this method is able to predict the quality of458Figure 5: Predicting the quality of measures over a single test set.textual similarity system with a 0.94 of accuracy inthis test bed.In this point, we have two options for optimiz-ing systems.
First, we can optimize measures ac-cording to the results achieved in an annotated train-ing corpus.
The other option consists of consideringthe correlation with HBR in the test corpus.
In or-der to compare both approaches we have developedthe following experiment.
Given a test corpus t, wecompute the correlation between system scores in tversus a training corpus t?.
This approach emulatesthe scenario of training systems over a (training) setand evaluating over a different (test) set.
We alsocompute the correlation between system scores inall corpora vs. the scores in t. Finally, we computethe correlation between system scores in t and ourpredictor in t (which is the correlation system/HBRacross similarity instances in t).
This approach em-ulates the use of HBR as unsupervised optimizationmethod.Figure 5 shows the results.
The horizontal axisrepresents the test set t. The black columns rep-resent the prediction over HBR in the correspond-ing test set.
The grey columns represent the predic-tion by using the average correlation across test sets.The light grey columns represents the prediction us-ing the correlation with humans in other single testset.
Given that there are five test sets, the figure in-cludes four grey columns for each test set.
The fig-ure clearly shows the superiority of HBR as measurequality predictor, even when it does not employ hu-man assessments.7 ConclusionsThe Heterogeneity Based Ranking provides a mech-anism to combine similarity measures (systems)without considering human assessments.
Interest-ingly, the combined measure always improves orachieves similar results than the best single measurein the set.
The main drawback is its computationalcost.
However, the correlation between single mea-sures and HBR predicts with a high confidence theaccuracy of measures regarding human assessments.Therefore, HBR is a very useful tool when optimiz-ing systems, specially when a representative trainingcorpus is not available.
In addition, our results shedsome light on the contribution of measures to thetask.
According to our experiments, the less reliablemeasures as a whole can produce reliable results ifthey are combined according to HBR.The HBR software is available athttp://nlp.uned.es/?enrique/ReferencesEnrique Amigo?, Julio Gonzalo, Jesus Gimenez, and Fe-lisa Verdejo.
2011.
Corroborating text evaluation re-sults with heterogeneous measures.
In Proceedings ofthe 2011 Conference on Empirical Methods in Natu-ral Language Processing, pages 455?466, Edinburgh,Scotland, UK., July.
Association for ComputationalLinguistics.George Doddington.
2002.
Automatic Evaluationof Machine Translation Quality Using N-gram Co-Occurrence Statistics.
In Proceedings of the 2nd Inter-459national Conference on Human Language Technology,pages 138?145.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010.
Asiya:An Open Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathemat-ical Linguistics, (94):77?86.Chin-Yew Lin.
2004.
Rouge: A Package for Auto-matic Evaluation of Summaries.
In Marie-FrancineMoens and Stan Szpakowicz, editors, Text Summariza-tion Branches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associa-tion for Computational Linguistics.460
