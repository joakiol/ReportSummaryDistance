Multi-Engine Machine Translation with Voted Language ModelTadashi NomotoNational Institute of Japanese Literature1-16-10 Yutaka ShinagawaTokyo 142-8585 Japannomoto@acm.orgAbstractThe paper describes a particular approach to multi-engine machine translation (MEMT), where wemake use of voted language models to selectivelycombine translation outputs from multiple off-the-shelf MT systems.
Experiments are done usinglarge corpora from three distinct domains.
Thestudy found that the use of voted language modelsleads to an improved performance of MEMT sys-tems.1 IntroductionAs the Internet grows, an increasing number ofcommercial MT systems are getting on line readyto serve anyone anywhere on the earth.
An inter-esting question we might ponder is whether it is notpossible to aggregate the vast number of MT sys-tems available on the Internet into one super MTwhich surpasses in performance any of those MTsthat comprise the system.
And this is what wewill be concerned with in the paper, with somewhatwatered-down settings.People in the speech community pursued the ideaof combining off-the-shelf ASRs (automatic speechrecognizers) into a super ASR for some time, andfound that the idea works (Fiscus, 1997; Schwenkand Gauvain, 2000; Utsuro et al, 2003).
In IR (in-formation retrieval), we find some efforts going (un-der the name of distributed IR or meta-search) to se-lectively fuse outputs from multiple search engineson the Internet (Callan et al, 2003).
So it would becurious to see whether we could do the same withMTs.Now back in machine translation, we do findsome work addressing such concern: Frederkingand Nirenburg (1994) develop a multi-engine MTor MEMT architecture which operates by com-bining outputs from three different engines basedon the knowledge it has about inner workings ofeach of the component engines.
Brown and Fred-erking (1995) is a continuation of Frederking andNirenburg (1994) with an addition of a ngram-based mechanism for a candidate selection.
Nomoto(2003), however, explores a different line of re-search whose goal is to combine black box MTs us-ing statistical confidence models.
Similar efforts arealso found in Akiba et al (2002).The present paper builds on the prior work byNomoto (2003).
We start by reviewing his ap-proach, and go on to demonstrate that it could be im-proved by capitalizing on dependence of the MEMTmodel there on language model.
Throughout thepaper, we refer to commercial black box MT sys-tems as OTS (off-the-shelf) systems, or more sim-ply, OTSs.2 Confidence ModelsWe take it here that the business of MEMT is aboutchoosing among translation outputs from multipleMT systems, whether black box or not, for each in-put text.
Therefore the question we want to addressis, how do we go about choosing among MT outputsso that we end up with a best one?What we propose to do is to use some confidencemodels for translations generated by OTSs, and letthem decide which one we should pick.
We essen-tially work along the lines of Nomoto (2003).
Wereview below some of the models proposed there,together with some motivation behind them.Confidence models he proposes come in two va-rieties: Fluency based model (FLM) and Alignmentbased model (ALM), which is actually an extensionof FLM.
Now suppose we have an English sentencee and its Japanese translation j generated by someOTS.
(One note here: throughout the paper we workon English to Japanese translation.)
FLM dictatesthat the quality of j as a translation of e be deter-mined by:FLM(e, j) = logPl(j) (1)Pl(j) is the probability of j under a particular lan-guage model (LM) l.1 What FLM says is that thequality of a translation essentially depends on its loglikelihood (or fluency) and has nothing to do withwhat it is a translation of.ALM extends FLM to include some informationon fidelity.
That is, it pays some attention to howfaithful a translation is to its source text.
ALM doesthis by using alignment models from the statisticalmachine translation literature (Brown et al, 1993).Here is what ALM looks like.ALM(e, j) = logPl(j)Q(e | j)Q(e | j) is the probability estimated using IBMModel 1.
ALM takes into account the fluency ofa translation output (given by Pl(j)) and the degreeof association between e and j (given by Q(e | j)),which are in fact two features generally agreed inthe MT literature to be most relevant for assessingthe quality of translations (White, 2001).One problem with FLM and ALM is that they failto take into account the reliability of an OTS sys-tem.
As Nomoto (2003) argues, it is reasonable tobelieve that some MT systems could inherently bemore prone to error and outputs they produce tendto be of less quality than those from other systems,no matter what the outputs?
fluency or translationprobability may be.
ALM and FLM work solelyon statistical information that can be gathered fromsource and target sentences, dismissing any opera-tional bias that an OTS might have on a particulartask.Nomoto (2003) responds to the problem by intro-ducing a particular regression model known as Sup-port Vector regression (SVR), which enables him toexploit bias in performance of OTSs.
What SVRis intended to do is to modify confidence scoresFLM and ALM produce for MT outputs in such away that they may more accurately reflect their in-dependent evaluation involving human translationsor judgments.
SVR is a multi-dimensional regres-sor, and works pretty much like its enormously pop-ular counterpart, Support Vector classification, ex-cept that we are going to work with real numbers fortarget values and construct the margin, using Vap-nik?s ?-insensitive loss function (Scho?lkopf et al,1998).1Note that Pl(j) = P (l)Qmi P (wi | wi?2, wi?1, l) wherej = w1 ?
?
?wm.
Assume a uniform prior for l.SVR looks something like this.h(~x) = ~w ?
~x+ b,with input data ~x = (x1, .
.
.
, xm) and the corre-sponding weights ~w = (w1, .
.
.
, wm).
?x ?
y?
de-notes the inner product of x and y.
~x could be a setof features associated with e and j. Parameters ~wand b are something determined by SVR.It is straightforward to extend the ALM and FLMwith SVR, which merely consists of plugging in ei-ther model as an input variable in the regressor.
Thiswould give us the following two SVR models withm = 1.Regressive FLM (rFLM)h(FLM(e, j)) = w1 ?
FLM(e, j) + bRegressive ALM (rALM)h(ALM(e, j)) = w1 ?ALM(e, j) + bNotice that h(?)
here is supposed to relate FLM orALM to some independent evaluation metric suchas BLEU (Papineni et al, 2002), not the log likeli-hood of a translation.With confidence models in place, define a MEMTmodel ?
by:?
(e, J, l) = arg maxj?J(?
(e, j | l))Here e represents a source sentence, J a set of trans-lations for e generated by OTSs, and ?
denotes someconfidence model under an LM l. Throughout therest of the paper, we let FLM?
and ALM?
denoteMEMT systems based on FLM and ALM, respec-tively, and similarly for others.3 Notes on EvaluationWe assume here that the MEMT works on asentence-by-sentence basis.
That is, it takes as in-put a source sentence, gets it translated by severalOTSs, and picks up the best among translations itgets.
Now a problem with using BLEU in this setupis that translations often end up with zero becausemodel translations they refer to do not contain n-grams of a particular length.2 This would make im-possible a comparison and selection among possibletranslations.2In their validity study of BLEU, Reeder and White (2003)finds that its correlation with human judgments increases withthe corpus size, and warns that to get a reliable score for BLEU,one should run it on a corpus of at least 4,000 words.
Also Tateet al (2003) reports about some correlation between BLEU andtask based judgments.One way out of this, Nomoto (2003) suggests,is to back off to a somewhat imprecise yet robustmetric for evaluating translations, which he calls m-precision.3 The idea of m-precision helps definewhat an optimal MEMT should look like.
Imaginea system which operates by choosing, among can-didates, a translation that gives a best m-precision.We would reasonably expect the system to outper-form any of its component OTSs.
Indeed Nomoto(2003) demonstrates empirically that it is the case.Moreover, since rFLM?
and rALM?
work on a sen-tence, not on a block of them, what h(?)
relates to isnot BLEU, but m-precision.Hogan and Frederking (1998) introduces a newkind of yardstick for measuring the effectivenessof MEMT systems.
The rationale for this is thatit is often the case that the efficacy of MEMT sys-tems does not translate into performance of outputsthat they generate.
We recall that with BLEU, onemeasures performance of translations, not how of-ten a given MEMT system picks the best translationamong candidates.
The problem is, even if a MEMTis right about its choices more often than a best com-ponent engine, BLEU may not show it.
This happensbecause a best translation may not always get a highscore in BLEU.
Indeed, differences in BLEU amongcandidate translations could be very small.Now what Hogan and Frederking (1998) suggestis the following.d(?m) =?Ni ?
(?m(e),max{?e1 ?
?
?
?eM })Nwhere ?
(i, j) is the Kronecker delta function, whichgives 1 if i = j and 0 otherwise.
Here ?m rep-resents some MEMT system, ?m(e) denotes a par-ticular translation ?m chooses for sentence e, i.e.,?m(e) = ?
(e, J, l).
?e1 .
.
.
?eM ?
J denotes a setof candidate translations.
max here gives a transla-tion with the highest score in m-precision.
N is thenumber of source sentences.
?(?)
says that you get1 if a particular translation the MEMT chooses for agiven sentences happens to rank highest among can-3For a reference translation r and a machine-generatedtranslation t, m-precision is defined as:m-precision =NXiPv?Sit C(v, r)Pv?Sit C(v, t),which is nothing more than Papineni et al (2002)?s modifiedn-gram precision applied to a pair of a single reference and theassociated translation.
Sit here denotes a set of i-grams in t,v an i-gram.
C(v, t) indicates the count of v in t. Nomoto(2003) finds that m-precision strongly correlates with BLEU,which justifies the use of m-precision as a replacement of BLEUat the sentence level.didates.
d(?m) gives the average ratio of the times?m hits a right translation.
Let us call d(?m) HFaccuracy (HFA) for the rest of the paper.4 LM perplexity and MEMT performanceNow the question we are interested in asking iswhether the choice of LM really matters.
That is,does a particular choice of LM gives a better per-forming FLM?
or ALM?
than something else, andif it does, do we have a systematic way of choosingone LM over another?Let us start with the first question.
As a way ofshedding some light on the issue, we ran FLM?
andALM?
using a variety of LMs, derived from variousdomains with varying amount of training data.
Weworked with 24 LMs from various genres, with vo-cabulary of size ranging from somewhere near 10Kto 20K in words (see below and also Appendix Afor details on train sets).
LMs here are trigram basedand created using an open source speech recognitiontool called JULIUS.4Now train data for LMs are collected from fivecorpora, which we refer to as CPC, EJP, PAT, LIT,NIKMAI for the sake of convenience.
CPC is ahuge set of semi-automatically aligned pairs of En-glish and Japanese texts from a Japanese news pa-per which contains as many as 150,000 sentences(Utiyama and Isahara, 2002), EJP represents a rel-atively small parallel corpus of English/Japanesephrases (totaling 15,187) for letter writing in busi-ness (Takubo and Hashimoto, 1999), PAT is a bilin-gual corpus of 336,971 abstracts from Japanesepatents filed in 1995, with associated translationsin English (a.k.a NTCIR-3 PATENT).5 LIT contains100 Japanese literary works from the early 20th cen-tury, and NIKMAI 1,536,191 sentences compiledfrom several Japanese news paper sources.
BothLIT and NIKMAI are monolingual.Fig.1 gives a plot of HF accuracy by perplexityfor FLM?
?s on test sets pulled out of PAT, EJP andCPC.6 Each dot there represents an FLM?
with aparticular LM plugged into it.
The HFA of eachFLM?
in Fig.1 represents a 10-fold cross validatedHFA score, namely an HFA averaged over evenly-4http://julius.sourceforge.jp5A bibliographic note.
NTCIR-3 PATENT: NII Test Col-lection for Information Retrieval Systems distributed throughNational Institute of Informatics (www.nii.ac.jp).6A test set from EJP and CPC each contains 7,500 bilingualsentences, that from PAT contains 4,600 bilingual abstracts (ap-proximately 9,200 sentences).
None of them overlaps with theremaining part of the corresponding data set.
Relevant LMs arebuilt on Japanese data drawn from the data sets.
We took carenot to train LMs on test sets.
(See Section 6 for further details.)??????????????
????
??
?
?
?LM PerplexityHF Accuracy500 1000 1500 20000.550.650.75PAT????
??
???????????????
?LM PerplexityHF Accuracy500 1000 15000.380.400.420.44 CPC???
??
????????????????
?LM PerplexityHF Accuracy500 1000 1500 20000.280.320.360.40 EJPFigure 1: HF accuracy-by-perplexity plots for FLM?
with four OTSs, Ai, Lo, At, Ib, on PAT (left), CPC(center) and EJP (right).
Dots represent FLM?
?s with various LMs .split 10 blocks of a test set.
The perplexity is thatof Pl(j) averaged over blocks, with a particular LMplugged in for l (see Equation 1).We can see there an apparent tendency for an LMwith lower perplexity to give rise to an FLM?
withhigher HFA, indicating that the choice of LM doesindeed influence the performance of FLM?.
Whichis somewhat surprising given that the perplexity ofa machine generated translation should be indepen-dent of how similar it is to a model translation,which dictates the HFA.7Now let us turn to the question of whether thereis any systematic way of choosing an LM so thatit gives rise to a FLM?
with high HFA.
Since weare working with multiple OTS systems here, weget multiple outputs for a source text.
Our ideais to let them vote for an LM to plug into FLM?or for that matter, any other forms of MEMT dis-cussed earlier.
Note that we could take an alternateapproach of letting a model (or human) translation(associated with a source text) pick an LM by alone.An obvious problem with this approach, however,is that a mandatory reference to model translationswould compromise the robustness of the approach.We would want the LM to work for MEMT regard-less of whether model translations are available.
Soour concern here is more with choosing an LM inthe absence of model translations, to which we willreturn below.5 Voting Language ModelWe consider here a simple voting scheme a` laROVER (Fiscus, 1997; Schwenk and Gauvain,2000; Utsuro et al, 2003), which works by picking7Recall that the HFA does not represent the confidence scoresuch as one given by FLM (Equation 1), but the average ratioof the times that an MEMT based on FLM picks a translationwith the best m-precision.Table 1: A MEMT algorithm implementing V-by-M. S represents a set of OTS systems, L a set oflanguage models.
?
is some confidence model such(r)FLM or (r)ALM.
V-by-M chooses a most-voted-for LM among those in L, given the set J of trans-lations for e.MEMT(e,S,L)beginJ = {j | j is a translation of e generated by s ?
S.}l = V-by-M(J, L)jk = arg maxj?J(?
(e, j | l))return jkendup an LM voted for by the majority.
More specif-ically, for each output translation for a given input,we first pick up an LM which gives it the smallestperplexity, and out of those LMs, one picked by themajority of translations will be plugged into MEMT.We call the selection scheme voting-by-majority orsimply V-by-M.
The V-by-M scheme is motivatedby the results in Fig.1, where perplexity is found tobe a reasonably good predictor of HFA.Formally, we could put the V-by-M scheme asfollows.
For each of the translation outputs je1 .
.
.
jenassociated with a given input sentence e, we want tofind some LM M from a set L of LMs such that:Mi = arg minm?LPP (jei | m),where PP (j | m) is the perplexity of j under m.Now assume M1 .
.
.Mn are such LMs for je1 .
.
.
jen.Then we pick up an M with the largest frequencyand plug it into ?
such as FLM.8Suppose, for instance, that Ma, Mb, Ma and Mcare lowest perplexity LMs found for translationsje1 ,je2 ,je3 and je4 , respectively.
Then we choose Maas an LM most voted for, because it gets two votesfrom je1 and je3 , meaning that Ma is nominated asan LM with lowest perplexity by je1 and je3 , whileMb and Mc each collect only one vote.
In case ofties, we randomly choose one of the LMs with thelargest count of votes.6 Experiment Setup and ProcedureLet us describe the setup of experiments we haveconducted.
The goal here is to learn how the V-by-M affects the overall MEMT performance.
Fortest sets, we carry over those from the perplexityexperiments (see Footnote 6, Section 4), which arederived from CPC, EJP, and PAT.
(Call them tCPC,tEJP, and tPAT hereafter.
)In experiments, we begin by splitting a test setinto equal-sized blocks, each containing 500 sen-tences for tEJP and tCPC, and 100 abstracts (ap-proximately 200 sentences) for tPAT.9 We had thetotal of 15 blocks for tCPC and tEJP, and 46 blocksfor tPAT.
We leave one for evaluation and use therest for training alignment models, i.e., Q(e | j),SV regressors and some inside-data LMs.
(Againwe took care not to inadvertently train LMs on testsets.)
We send a test block to OTSs Ai, Lo, At, andIb, for translation and combine their outputs usingthe V-by-M scheme, which may or may not be cou-pled with regression SVMs.
Recall that the MEMToperates on a sentence by sentence basis.
So whathappens here is that for each of the sentences in ablock, the MEMT works the four MT systems toget translations and picks one that produces the bestscore under ?.We evaluate the MEMT performance by run-ning HFA and BLEU on MEMT selected translationsblock by block,10 and giving average performanceover the blocks.
Table 1 provides algorithmic de-tails on how the MEMT actually operates.8It is worth noting that the voted language model readilylends itself to a mixture model: P (j) =Pm?M ?mP (j | m)where ?m = 1 if m is most voted for and 0 otherwise.9tCPC had the average of 15,478 words per block, whereastEJP had about 11,964 words on the average in each block.With tPAT, however, the average per block word length grewto 16,150.10We evaluate performance by block, because of some re-ports in the MT literature that warn that BLEU behaves errati-cally on a small set of sentences (Reeder and White, 2003).
Seealso Section 3 and Footnote 2 for the relevant discussion.Table 2: HF accuracy of MEMT models with V-by-M.Model tCPC tEJP tPAT avg.rFLM?
0.4230 0.4510 0.8066 0.5602rALM?
0.4194 0.4346 0.8093 0.5544FLM?
0.4277 0.4452 0.7342 0.5357ALM?
0.4453 0.4485 0.7702 0.5547Table 3: HF accuracy of MEMT models with ran-domly chosen LMs.
Note how FLM?
and ALM?drop in performance.Model tCPC tEJP tPAT avg.rFLM?
0.4207 0.4186 0.8011 0.5468rALM?
0.4194 0.4321 0.8095 0.5537FLM?
0.4126 0.3520 0.6350 0.4665ALM?
0.4362 0.3597 0.6878 0.49467 Results and DiscussionNow let us see what we found from the experiments.We ran the MEMT on a test set with (r)FLM or(r)ALM embedded in it.
Recall that our goal hereis to find how the V-by-M affects performance ofMEMT on tCPC, tEJP, and tPAT.First, we look at whether the V-by-M affects inany way, the HFA of the MEMT, and if it does, thenhow much.
Table 2 and Table 3 give summaries ofresults on HFA versus V-by-M. Table 2 shows howthings are with V-by-M on, and Table 3 shows whathappens to HFA when we turn off V-by-M, that is,when we randomly choose an LM from the same setthat the V-by-M chooses from.
The results indicatea clear drop in performance of FLM?
and ALM?when one chooses an LM randomly.11Curiously, however, rFLM?
and rALM?
are af-fected less.
They remain roughly at the same levelof HFA over Table 2 and Table 3.
What this means11Another interesting question to ask at this point is, howdoes one huge LM trained across domains compare to the V-by-M here?
By definition of perplexity, the increase in size ofthe training data leads to an increase in perplexity of the LM.So if general observations in Fig.1 hold, then we would expectthe ?one-huge-LM?
approach to perform poorly compared tothe V-by-M, which is indeed demonstrated by the followingresults.
HFLM?
below denotes a FLM?
based on a compositeLM trained over CPC, LIT, PAT, NIKMAI, and EJP.
The testingprocedure is same as that described in Sec.6Model tCPC tEJP tPAT avg.HFLM?
(HFA) 0.4182 0.4081 0.6927 0.5063HFLM?
(BLEU) 0.1710 0.2619 0.1874 0.2067Table 4: Performance in BLEU of MEMT modelswith V-by-M.Model tCPC tEJP tPAT avg.rFLM?
0.1743 0.2861 0.1954 0.2186rALM?
0.1735 0.2869 0.1954 0.2186FLM?
0.1736 0.2677 0.1907 0.2107ALM?
0.1763 0.2622 0.1934 0.2106Table 5: Performance in BLEU of MEMT modelswith randomly chosen LMs.Model tCPC tEJP tPAT avg.rFLM?
0.1738 0.2717 0.1950 0.2135rALM?
0.1735 0.2863 0.1954 0.2184FLM?
0.1710 0.2301 0.1827 0.1946ALM?
0.1745 0.2286 0.1871 0.1967is that there is some discrepancy in the effective-ness of V-by-M between the fluency based and re-gression based models.
We have no explanation forthe cause of the discrepancy at this time, though wemay suspect that in learning, as long as there is somepattern to exploit in m-precision and the probabilityestimates of test sentences, how accurate those esti-mates are may not matter much.Table 4 and Table 5 give results in BLEU.12 Theresults tend to replicate what we found with HFA.rFLM?
and rALM?
keep the edge over FLM?and ALM?
whether or not V-by-M is brought intoaction.
The differences in performance betweenrFLM?
and rALM?
with or without the V-by-Mscheme are rather negligible.
However, if we turnto FLM?
and ALM?, the effects of the V-by-M areclearly visible.
FLM?
scores 0.2107 when coupledwith the V-by-M.
However, when disengaged, thescore slips to 0.1946.
The same holds for ALM?.Table 6: HF accuracy of OTS systemsModel tCPC tEJP tPAT avg.Ai 0.2363 0.4319 0.0921 0.2534Lo 0.1718 0.2124 0.0504 0.1449At 0.4211 0.1681 0.8037 0.4643Ib 0.1707 0.1876 0.0537 0.1373OPM 1.0000 1.0000 1.0000 1.000012The measurements in BLEU here take into account up totrigrams.Table 7: Performance of OTS systems in BLEU.Model tCPC tEJP tPAT avg.Ai 0.1495 0.2874 0.1385 0.1918Lo 0.1440 0.1711 0.1402 0.1518At 0.1738 0.1518 0.1959 0.1738Ib 0.1385 0.1589 0.1409 0.1461OPM 0.2111 0.3308 0.1995 0.2471Leaving the issue of MEMT models momentar-ily, let us see how the OTS systems Ai, Lo, At, andIb are doing on tCPC, tEJP, and tPAT.
Note that thewhole business of MEMT would collapse if it slipsbehind any of the OTS systems that compose it.Table 6 and Table 7 show performance of thefour OTS systems plus OPM, by HFA and by BLEU.OPM here denotes an oracle MEMT which operatesby choosing in hindsight a translation that gives thebest score in m-precision, among those producedby OTSs.
It serves as a practical upper bound forMEMT while OTSs serve as baselines.First, let us look at Table 6 and compare it to Ta-ble 2.
A good news is that most of the OTS sys-tems do not even come close to the MEMT mod-els.
At, a best performing OTS system, gets 0.4643on the average, which is about 20% less than thatscored by rFLM?.
Turning to BLEU, we find againin Table 7 that a best performing system among theOTSs, i.e., Ai, is outperformed by FLM?, ALM?and all their varieties (Table 4).
Also something ofnote here is that on tPAT, (r)FLM?
and (r)ALM?in Table 4, which operate by the V-by-M scheme,score somewhere from 0.1907 to 0.1954 in BLEU,coming close to OPM, which scores 0.1995 on tPAT(Table 7).It is interesting to note, incidentally, that there issome discrepancy between BLEU and HFA in per-formance of the OTSs: A top performing OTS inTable 6, namely At, achieves the average HFA of0.4643, but scores only 0.1738 for BLEU (Table 7),which is worse than what Ai gets.
Apparently,high HFA does not always mean a high BLEU score.Why?
The reason is that a best MT output neednot mark a high BLEU score.
Notice that ?best?
heremeans the best among translations by the OTSs.
Itcould happen that a poor translation still gets chosenas best, because other translations are far worse.To return to the discussion of (r)FLM?
and(r)ALM?, an obvious fact about their behavior isthat regressor based systems rFLM?
and rALM?,whether V-by-M enabled or not, surpass in per-formance their less sophisticated counterparts (seeTable 8: HF accuracy of MEMTs with perturbed SVregressor in the V-by-M scheme.Model tCPC tEJP tPAT avg.rFLM?
0.4230 0.4353 0.6712 0.5098rALM?
0.4195 0.4302 0.5582 0.4693FLM?
0.4277 0.4452 0.7342 0.5357ALM?
0.4453 0.4485 0.7702 0.5547Table 9: Performance in BLEU of MEMTs with per-turbed SV regressor in the V-by-M scheme.Model tCPC tEJP tPAT avg.rFLM?
0.1743 0.2823 0.1835 0.2134rALM?
0.1736 0.2843 0.1696 0.2092FLM?
0.1736 0.2677 0.1907 0.2107ALM?
0.1763 0.2622 0.1934 0.2106Table 2,4 and also Table 3,5).
Regression allowsthe MEMT models to correct themselves for somedomain-specific bias of the OTS systems.
But thedownside of using regression to capitalize on theirbias is that you may need to be careful about datayou train a regressor on.Here is what we mean.
We ran experiments usingSVM regressors trained on a set of data randomlysampled from tCPC, tEJP, and tPAT.
(In contrast,rFLM?
and rALM?
in earlier experiments had a re-gressor trained separately on each data set.)
Theyall operated in the V-by-M mode.
The results areshown in Table 8 and Table 9.
What we find thereis that with regressors trained on perturbed data,both rFLM?
and rALM?
are not performing as wellas before; in fact they even fall behind FLM?
andALM?
in HFA and their performance in BLEU turnsout to be just about as good as FLM?
and ALM?.So regression may backfire when trained on wrongdata.8 ConclusionLet us summarize what we have done and learnedfrom the work.
We started with a finding that thechoice of language model could affect performanceof MEMT models of which it is part.
The V-by-Mwas introduced as a way of responding to the prob-lem of how to choose among LMs so that we getthe best MEMT.
We have shown that the V-by-Mscheme is indeed up to the task, predicting a rightLM most of the time.
Also worth mentioning is thatthe MEMT models here, when coupled with V-by-M, are all found to surpass component OTS systemsby a respectable margin (cf., Tables 4, 7 for BLEU,2, 6 for HFA).Regressive MEMTs such as rFLM?
and rALM?,are found to be not affected as much by the choiceof LM as their non-regressive counterparts.
We sus-pect this happens because they have access to ex-tra information on the quality of translation derivedfrom human judgments or translations, which maycloud effects of LMs on them.
But we also pointedout that regressive models work well only when theyare trained on right data; if you train them acrossdifferent sources of varying genres, they could fail.An interesting question that remains to be ad-dressed is how we might deal with translations froma novel domain.
One possible approach would beto use a dynamic language model which adapts it-self for a new domain by re-training itself on datasampled from the Web (Berger and Miller, 1998).ReferencesYasuhiro Akiba, Taro Watanabe, and EiichiroSumita.
2002.
Using language and translationmodels to select the best among outputs frommultiple mt systems.
In Proceedings of the 19thInternational Conference on Computational Lin-guistics (COLING 2002), Taipei.Adam Berger and Robert Miller.
1998.
Just-in-time language modelling.
In Proceedings ofICASSP98.Ralf Brown and Robert Frederking.
1995.
Ap-plying statistical English language modelling tosymbolic machine translation.
In Proceedings ofthe Sixth International Conference on Theoreticaland Methodological Issues in Machine Transla-tion (TMI?95), pages 221?239, Leuven, Belgium,July.Peter F. Brown, Stephen A. Della Pietra, Vin-cent J.Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine transla-tion: Parameter estimation.
Computational Lin-guistics, 19(2):263?311, June.Jamie Callan, Fabio Crestani, Henrik Nottelmann,Pietro Pala, and Xia Mang Shou.
2003.
Re-source selection and data fusion in multimediadistributed digital libaries.
In Proceedings of the26th Annual International ACM/SIGIR Confer-ence on Research and Development in Informa-tion Retrieval.
ACM.Jonathan G. Fiscus.
1997.
A post-processing sys-tem to yield reduced word error rates: Recogniseroutput voting error reduction (ROVER).
In Proc.IEEE ASRU Workshop, pages 347?352, SantaBarbara.Rober Frederking and Sergei Nirenburg.
1994.Three heads are better than one.
In Proceed-ings of the Fourth Conference on Applied NaturalLanguage Processing, Stuttgart.Christopher Hogan and Robert E. Frederking.
1998.An evaluation of the multi-engine MT architec-ture.
In Proceedings of the Third Conference ofthe Association for Machine Translation in theAmericas (AMTA ?98), pages 113?123, Berlin,October.
Springer-Verlag.
Lecture Notes in Ar-tificial Intelligence 1529.Tadashi Nomoto.
2003.
Predictive models of per-formance in multi-engine machine translation.
InProceedings of Machine Translation Summit IX,New Orleans, September.
IAMT.Kishore Papineni, Salim Roukos, Todd Ward, andWei ing Zhu.
2002.
BLEU: a method for auto-matic evaluation of machine translation.
In Pro-ceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics, pages311?318, July.Florence Reeder and John White.
2003.
Granular-ity in MT evaluation.
In MT Summit Workshop onMachine Translation Evaluation: Towards Sys-tematizing MT Evaluation, pages 37?42, New Or-leans.
AMTA.Bernhard Scho?lkopf, Chirstpher J. C. Burges, andAlexander J. Smola, editors.
1998.
Advances inKernel Methods: Support Vector Learning.
TheMIT Press.Holger Schwenk and Jean-Luc Gauvain.
2000.Combining multiple speech recognizers usingvoting and language model information.
In Pro-ceedings of the IEEE International Conferenceon Speech and Language Proceesing (ICSLP),volume 2, pages 915?918, Beijin, October.
IEEE.Kohei Takubo and Mitsunori Hashimoto.
1999.A Dictionary of English Business Letter Ex-pressions.
Published in CDROM.
Nihon KeizaiShinbun Sha.Calandra Tate, Sooyon Lee, and Clare R. Voss.2003.
Task-based MT evaluation: Tackling soft-ware, experimental design, & statistical models.In MT Summit Workshop on Machine TranslationEvaluation: Towards Systematizing MT Evalua-tion, pages 43?50.
AMTA.Masao Utiyama and Hitoshi Isahara.
2002.
Align-ment of japanese-english news articles and sen-tences.
In IPSJ Proceedings 2002-NL-151, pages15?22.
In Japanese.Takehito Utsuro, Yasuhiro Kodama, TomohiroWatanabe, Hiromitsu Nishizaki, and Seiichi Nak-agawa.
2003.
Confidence of agreement amongmultiple LVCSR models and model combinationby svm.
In Proceedings of the 28th IEEE Interna-Table 10: Language models in MEMTModels Train Size Voc.
Genrepaj98j102t 1,020K 20K PATpaj96j5t 50K 20K PATpaj96j3t 30K 20K PATpaj98j5t 50K 20K PATpaj96j102t 1,020K 20K PATpaj98j3t 30K 20K PATpaj98j1t 10K 14K PATpaj1t 10K 14K PATpaj98j5k 5K 10K PATpaj5k 5K 10K PATlit8t 80K 20K LITlit5t 50K 20K LITlit3t 30K 20K LITlit5k 5K 13K LITlit1t 10K 13K LITnikmai154t 1,540K 20K NWSnikmai5t 50K 20K NWScrl14t 40K 20K NWScrl5t 50K 20K NWSnikmai3t 30K 20K NWSnikmai1t 10K 17K NWSnikmai5k 5K 12K NWScrl3t 30K 20K NWSejp8k 8K 8K BIZtional Conference on Acoustics, Speech and Sig-nal Processing, pages 16?19.
IEEE, April.John White.
2001.
Predicting intelligibility from fi-delity in MT evaluation.
In Proceedings of theworkshop ?MT Evaluation: Who did What toWhom?, pages 35?37.AppendixA Language ModelsTable 10 lists language models used in the votingbased MEMTs discussed in the paper.
They aremore or less arbitrarily built from parts of the co-pora CPC, EJP, NIKMAI, EJP, and LIT.
?Train size?indicates the number of sentences, given in kilo,in a corpus on which a particular model is trained.Under ?Voc(abulary)?
is listed the number of typewords for each LM (also given in kilo).
Noticethe difference in the way the train set and vocabu-lary are measured.
?Genre?
indicates the genre ofa trainig data used for a given LM: PAT stands forpatents (from PAT), LIT literary texts (from LIT),NWS news articles (from CPC and NIKMAI), andBIZ business related texts (from EJP).
