Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 81?84,Paris, October 2009. c?2009 Association for Computational LinguisticsAnalysis of Discourse Structure with Syntactic Dependencies and Data-Driven Shift-Reduce ParsingKenji Sagae USC Institute for Creative Technologies Marina del Rey, CA 90292 USA sagae@ict.usc.eduAbstract We present an efficient approach for dis-course parsing within and across sen-tences, where the unit of processing is an entire document, and not a single sen-tence.
We apply shift-reduce algorithms for dependency and constituent parsing to determine syntactic dependencies for the sentences in a document, and subse-quently a Rhetorical Structure Theory (RST) tree for the entire document.
Our results show that our linear-time shift-reduce framework achieves high accu-racy and a large improvement in effi-ciency compared to a state-of-the-art ap-proach based on chart parsing with dy-namic programming.
1 Introduction Transition-based dependency parsing using shift-reduce algorithms is now in wide use for de-pendency parsing, where the goal is to determine the syntactic structure of sentences.
State-of-the-art results have been achieved for syntactic analysis in a variety of languages (Bucholz and Marsi, 2006).
In contrast to graph-based ap-proaches, which use edge-factoring to allow for global optimization of parameters over entire tree structures using dynamic programming or maxi-mum spanning tree algorithms (McDonald et al, 2005) transition-based models are usually opti-mized at the level of individual shift-reduce ac-tions, and can be used to drive parsers that pro-duce competitive accuracy using greedy search strategies in linear time.
Recent research in data-driven shift-reduce parsing has shown that the basic algorithms used for determining dependency trees  (Nivre, 2004) can be extended to produce constituent structures (Sagae and Lavie, 2005), and more general de-pendency graphs, where words can be linked to more than one head (Henderson et al, 2008; Sa-gae and Tsujii, 2008).
A remarkably similar parsing approach, which predates the current wave of interest in data-driven shift-reduce pars-ing sparked by Yamada and Matsumoto (2003) and Nivre and Scholz (2004), was proposed by Marcu (1999) for data-driven discourse parsing, where the goal is to determine the rhetorical structure of a document, including relationships that span multiple sentences.
The linear-time shift-reduce framework is particularly well suited for discourse parsing, since the length of the in-put string depends on document length, not sen-tence length, making cubic run-time chart pars-ing algorithms often impractical.
Soricut and Marcu (2003) presented an ap-proach to discourse parsing that relied on syntac-tic information produced by the Charniak (2000) parser, and used a standard bottom-up chart pars-ing algorithm with dynamic programming to determine discourse structure.
Their approach greatly improved on the accuracy of Marcu?s shift-reduce approach, showing the value of us-ing syntactic information in discourse analysis, but recovered only discourse relations within sentences.
We present an efficient approach to discourse parsing using syntactic information, inspired by Marcu?s application of a shift-reduce algorithm for discourse analysis with Rhetorical Structure Theory (RST), and Soricut and Marcu?s use of syntactic structure to help determine discourse structure.
Our transition-based discourse parsing framework combines elements from Nivre (2004)?s approach to dependency parsing, and Sagae and Lavie (2005)?s approach to constituent parsing.
Our results improve on accuracy over existing approaches for data-driven RST parsing, while also improving on speed over Soricut and Marcu?s chart parsing approach, which produces state-of-the-art results for RST discourse rela-tions within sentences.812 Discourse analysis with the RST Dis-course Treebank The discourse parsing approach presented here is based on the formalization of Rhetorical Struc-ture Theory (RST) (Mann and Thompson, 1988) used in the RST Discourse Treebank (Carlson et al, 2003).
In this scheme, the discourse structure of a document is represented as a tree, where the leaves are contiguous spans of text, called ele-mentary discourse units, or EDUs.
Each node in the tree corresponds to a contiguous span of text formed by concatenation of the spans corre-sponding to the node?s children, and represents a rhetorical relation (attribution, enablement, elaboration, consequence, etc.)
between these text segments.
In addition, each node is marked as a nucleus or as a satellite, depending on whether its text span represents an essential unit of information, or a supporting or background unit of information, respectively.
While the no-tions of nucleus and satellite are in some ways analogous to head and dependent in syntactic dependencies, RST allows for multi-nuclear rela-tions, where two nodes marked as nucleus can be linked into one node.
Our parsing framework includes three compo-nents: (1) syntactic dependency parsing, where standard techniques for sentence-level parsing are applied; (2) discourse segmentation, which uses syntactic and lexical information to segment text into EDUs; and (3) discourse parsing, which produces a discourse structure tree from a string of EDUs, also benefiting from syntactic informa-tion.
In contrast to the approach of Soricut and Marcu (2003), which also includes syntactic parsing, discourse segmentation and discourse parsing, our approach assumes that the unit of processing for discourse parsing is an entire document, and that discourse relations may exist within sentences as well as across sentences, while Soricut and Marcu?s processes one sen-tence at a time, independently, finding only dis-course relations within individual sentences.
Parsing entire documents at a time is made pos-sible in our approach through the use of linear-time transition-based parsing.
An additional mi-nor difference is that in our approach syntactic information is represented using dependencies, while Soricut and Marcu used constituent trees.
2.1 Syntactic parsing and discourse seg-mentation Assuming the document has been segmented into sentences, a task for which there are approacheswith very high accuracy (Gillick, 2009), we start by finding the dependency structure for each sen-tence.
This includes part-of-speech (POS) tag-ging using a CRF tagger trained on the Wall Street Journal portion of the Penn Treebank, and transition-based dependency parsing using the shift-reduce arc-standard algorithm (Nivre, 2004) trained with the averaged perceptron (Collins, 2002).
The dependency parser is also trained with the WSJ Penn Treebank, converted to de-pendencies using the head percolation rules of Yamada and Matsumoto (2003).
Discourse segmentation is performed as a bi-nary classification task on each word, where the decision is whether or not to insert an EDU boundary between the word and the next word.
In a sentence of length n, containing the words w1, w2 ?
wn, we perform one classification per word, in order.
For word wi, the binary choice is whether to insert an EDU boundary between wi and wi+1.
The EDUs are then the words between EDU boundaries (assuming boundaries exist in the beginning and end of each sentence).
The features used for classification are: the current word, its POS tag, its dependency label, and the direction to its head (whether the head appears before or after the word); the previous two words, their POS tags and dependency la-bels; the next two words, their POS tags and de-pendency labels; the direction from the previous word to its head; the leftmost dependent to the right of the current word, and its POS tag; the rightmost dependent to the left of the current word, and its POS tag; whether the head of the current word is between the previous EDU boundary and the current word; whether the head of the next word is between the previous EDU boundary and the current word.
In addition, we used templates that combine these features (in pairs or triples).
Classification was done with the averaged perceptron.
2.2 Transition-based discourse parsing RST trees can be represented in a similar way as constituent trees in the Penn Treebank, with a few differences: the trees represent entire docu-ments, instead of single sentences; the leaves of the trees are EDUs consisting of one or more contiguous words; and the node labels contain nucleus/satellite status, and possibly the name of a discourse relation.
Once the document has been segmented into a sequence of EDUs, we use a transition-based constituent parsing ap-proach (Sagae and Lavie, 2005) to build an RST tree for the document.82Sagae and Lavie?s constituent parsing algo-rithm uses a stack that holds subtrees, and con-sumes the input string (in our case, a sequence of EDUs) from left to right, using four types of ac-tions: (1) shift, which removes the next token from the input string, and pushes a subtree con-taining exactly that token onto the stack; (2) re-duce-unary-LABEL, which pops the stack, and push onto it a new subtree where a node with label LABEL dominates the subtree that was popped (3) reduce-left-LABEL, and (4) reduce-right-LABEL, which each pops two items from the stack, and pushes onto it a new subtree with root LABEL, which has as right child the subtree previously on top of the stack, and as left child the subtree previously immediately below the top of the stack.
The difference between reduce-left and reduce-right is whether the head of the new subtree comes from the left or right child.
The algorithm assumes trees are lexicalized, and in our use of the algorithm for discourse parsing, heads are entire EDUs, and not single words.
Our process for lexicalization of discourse trees, which is required for the parsing algorithm to function properly, is a simple percolation of ?head EDUs,?
performed in the same way as lexical heads can be assigned in Penn Treebank-style trees using a head percolation table (Collins, 1999).
To determine head EDUs, we use the nucleus/satellite status of nodes, as fol-lows: for each node, the leftmost child with nu-cleus status is the head; if no child is a nucleus, the leftmost satellite is the head.
Most nodes have exactly two children, one nucleus and one satellite.
The parsing algorithm deals only with binary trees.
We use the same binarization trans-form as Sagae and Lavie, converting the trees in the training set to binary trees prior to training the parser, and converting the binary trees pro-duced by the parser at run-time into n-ary trees.
As with the dependency parser and discourse segmenter, learning is performed using the aver-aged perceptron.
We use similar features as Sa-gae and Lavie, with one main difference: since there is usually no single head-word associated with each node, but a EDU that contains a se-quence of words, we use the dependency struc-ture of the EDU to determine what lexical fea-tures and POS tags should be used as features associated with each RST tree node.
In place of the head-word and POS tag of the top four items on the stack, and the next four items in the input, we use subsets of the words and POS tags in the EDUs for each of those items.
The subset of words (and POS tags) that represent an EDUcontain the first two and last words in the EDU, and each word in the EDU whose head is outside of the EDU.
In the vast majority of EDUs, this subset of words with heads outside the EDU (the EDU head set) contains a single word.
In addi-tion, we extract these features for the top three (not four) items on the stack, and the next three (not four) words in the input.
For the top two items on the stack, in addition to subsets of words and POS tags described above, we also take the words and POS tags for the leftmost and rightmost children of each word in the EDU head set.
Finally, we use feature templates that com-bine these and other individual features from Sa-gae and Lavie, who used a polynomial kernel and had no need for such templates (at the cost of increased time for both training and running).
3 Results To test our discourse parsing approach, we used the standard training and testing sections of the RST Discourse Treebank and the compacted 18-label set described by Carlson et al (2003).
We used approximately 5% of the standard training set as a development set.
Our part-of-speech tagger and syntactic parser were not trained using the standard splits of the Penn Treebank for those tasks, since there are documents in the RST Discourse Treebank test section that are included in the usual training sets for POS taggers and parsers.
The POS tagger and syntactic parser were then trained on sec-tions 2 to 21 of the WSJ Penn Treebank, exclud-ing the specific documents used in the test sec-tion of the RST Discourse Treebank.
Table 1 shows the precision, recall and f-score of our discourse segmentation approach on the test set, compared to that of Soricut and Marcu (2003) and Marcu (1999).
In all cases, results were obtained with automatically produced syn-tactic structures.
We also include the total time required for syntactic parsing (required in ourPrec.
Recall F-score Time Marcu99  83.3 77.1 80.1 - S&M03 83.5 82.7 83.1 361s this work 87.4 86.0 86.7 40s  Table 1: Precision, recall, f-score and time for discourse segmenters, tested on the RST Discourse Treebank.
Time includes syntactic parsing, Charniak (2000) for S&M03, and our implemetation of Nivre arc-standard for our segmenter.83segmentation approach and Soricut and Marcu?s) and segmentation.
For comparison with previous results, we include only segmentation within sen-tences (if all discourse boundaries are counted, including sentence boundaries, our f-score is 92.9).
Using our discourse segmentation and transi-tion-based discourse parsing approach, we obtain 42.9 precision and 46.2 recall (44.5 f-score) for all discourse structures in the test set.
Table 2 shows f-score of labeled bracketing for discourse relations within sentences only, for comparison with previously published results.
We note that human performance on this task has f-score 77.0.
While our f-score is still far below that of hu-man performance, we have achieved a large gain in speed of processing compared to a state-of-the-art approach.
4 Conclusion We have presented an approach to discourse analysis based on transition-based algorithms for dependency and constituent trees.
Dependency parsing is used to determine the syntactic struc-ture of text, which is then used in discourse seg-mentation and parsing.
A simple discriminative approach to segmentation results in an overall improvement in discourse parsing f-score, and the use of a linear-time algorithm results in an a large improvement in speed over a state-of-the-art approach.
Acknowledgments The work described here has been sponsored by the U.S. Army Research, Development, and En-gineering Command (RDECOM).
Statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred.References Buchholz, S. and Marsi, E. 2006.
CoNLL-X shared task on multilingual dependency parsing.
In Proc.
of CoNLL 2006 Shared Task.
Carlson, L., Marcu, D., and Okurowski, M. E. 2003.
Building a discourse-tagged corpus in the frame-work of Rhetorical Structure Theory.
In J. van Kuppevelt and R. W. Smith, editors, Current and New Directions in Discourse and Dialogue.
Klu-wer Academic Publishers.
Charniak, E. 2000.
A maximum-entropy-inspired parser.
In Proc.
of NAACL.
Collins, M. 1999.
Head-driven statistical models for natural language processing.
PhD dissertation, University of Pennsylvania.
Collins, M. 2002.
Discriminative Training Methods for Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In Proc.
of EMNLP.
Philadelphia, PA. Gillick, D. 2009.
Sentence Boundary Detection and the Problem with the U.S.
In Proc.
of the NAACL HLT: Short Papers.
Boulder, Colorado.
Henderson, J., Merlo, P., Musillo, G., Titov, I.
2008.
A Latent Variable Model of Synchronous Parsing for Syntactic and Semantic Dependencies.
In Proc.
of CoNLL 2008 Shared Task, Manchester, UK.
Mann, W. C. and Thompson, S. A.
1988.
Rhetorical Structure Theory: toward a functional theory of text organization.
Text, 8(3):243-281.
Marcu, D. 1999.
A decision-based approach to rhe-torical parsing.
In Proc.
of the Annual Meeting of the Association for Computational Linguistics.
McDonald, R., Pereira, F., Ribarov, K., and Hajic, J.
2005.
Non-projective dependency parsing using spanning tree algorithms.
In Proc.
of HLT/EMNLP.
Nivre, J.
2004.
Incrementality in Deterministic De-pendency Parsing.
In Incremental Parsing: Bring-ing Engineering and Cognition Together (work-shop at ACL-2004).
Barcelona, Spain.
Nivre, J. and Scholz, M. 2004.
Deterministic Depend-ency Parsing of English Text.
In Proc.
of COLING.
Sagae, K. and Lavie, A.
2005.
A classifier-based parser with linear run-time complexity.
In Proc.
of IWPT.
Sagae, K. and Tsujii, J.
2008.
Shift-reduce depend-ency DAG parsing.
In Proc.
of COLING.
Soricut, R. and Marcu, D. 2003.
Sentence level dis-course parsing using syntactic and lexical informa-tion.
In Proc.
of NAACL.
Edmonton, Canada.
Yamada, H. and Matsumoto, Y.
2003.
Statistical de-pendency analysis with support vector machines.
In Proc.
of IWPT.F-score Time Marcu99  37.2 - S&M03 49.0 481s this work 52.9 69s human 77.0 -  Table 2: F-score for bracketing of RST dis-course trees on the test set of the RST Dis-course Treebank, and total time (syntactic parsing, segmentation and discourse parsing) required to parse the test set (S&M03 and our approach were run on the same hardware).84
