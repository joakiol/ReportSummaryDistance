Improved Cross-Language Retrievalusing Backoff TranslationPhilip Resnik,1;2 Douglas Oard,2;3 and Gina Levow2Department of Linguistics,1Institute for Advanced Computer Studies,2College of Information Studies,3University of MarylandCollege Park, MD 20742fresnik,ginag@umiacs.umd.edu, oard@glue.umd.eduABSTRACTThe limited coverage of available translation lexicons can pose a se-rious challenge in some cross-language information retrieval appli-cations.
We present two techniques for combining evidence fromdictionary-based and corpus-based translation lexicons, and showthat backoff translation outperforms a technique based on merginglexicons.1.
INTRODUCTIONThe effectiveness of a broad class of cross-language informationretrieval (CLIR) techniques that are based on term-by-term transla-tion depends on the coverage and accuracy of the available trans-lation lexicon(s).
Two types of translation lexicons are commonlyused, one based on translation knowledge extracted from bilingualdictionaries [1] and the other based on translation knowledge ex-tracted from bilingual corpora [8].
Dictionaries provide reliable ev-idence, but often lack translation preference information.
Corpora,by contrast, are often a better source for translations of slang or newlycoined terms, but the statistical analysis through which the trans-lations are extracted sometimes produces erroneous results.
In thispaper we explore the question of how best to combine evidencefromthese two sources.2.
TRANSLATION LEXICONSOur term-by-term translation technique (described below) requiresa translation lexicon (henceforth tralex) in which each word f is as-sociated with a ranked set fe1; e2; : : : eng of translations.
We usedtwo translation lexicons in our experiments.2.1 WebDict TralexWe downloadeda freely available, manually constructedEnglish-French term list from the Web1 and inverted it to French-English1http://www.freedict.com.format.
Since the WebDict translations appear in no particular or-der, we ranked the eibased on target language unigram statisticscalculated over a large comparable corpus, the English portion ofthe Cross-LanguageEvaluation Forum (CLEF) collection, smoothedwith statistics from the Brown corpus, a balanced corpus coveringmany genres of English.
All single-word translations are ordered bydecreasing unigram frequency, followed by all multi-word transla-tions, and finally by any single-word entries not found in either cor-pus.
This ordering has the effect of minimizing the effect of infre-quent words in non-standard usages or of misspellings that some-times appear in bilingual term lists.2.2 STRAND TralexOur second lexical resource is a translation lexicon obtained fullyautomatically via analysisof parallel French-Englishdocuments fromthe Web.
A collection of 3,378 document pairs was obtained usingSTRAND, our technique for mining the Web for bilingual text [7].These document pairs were aligned internally, using their HTMLmarkup, to produce 63,094 aligned text ?chunks?
ranging in lengthfrom 2 to 30 words, 8 words on average per chunk, for a total of500K words per side.
Viterbi word-alignments for these pairedchunks were obtained using the GIZA implementation of the IBMstatistical translation models.2 An ordered set of translation pairswas obtained by treating each alignment link between words as aco-occurrence and scoring each word pair according to the likeli-hood ratio [2].
We then rank the translation alternatives in order ofdecreasing likelihood ratio score.3.
CLIR EXPERIMENTSRanked tralexes are particularly well suited to a simple rankedterm-by-term translation approach.
In our experiments, we use top-2 balanced document translation, in which we produce exactly twoEnglish terms for each French term.
For terms with no known trans-lation, the untranslated French term is generated twice (often appro-priate for proper names).
For French terms with one translation, thattranslation is generated twice.
For French terms with two or moretranslations, we generate the first two translations in the tralex.
Thusbalanced translation has the effect of introducing a uniform weight-ing over the top n translations for each term (here n = 2).Benefits of the approachinclude simplicity and modularity ?
no-tice that a lexicon containing ranked translations is the only require-ment, and in particular that there is no need for access to the in-ternals of the IR system or to the document collection in order to2http://www.clsp.jhu.edu/ws99/projects/mt/perform computations on term frequencies or weights.
In addition,the approach is an effective one: in previous experiments we havefound that this balancedtranslation strategy significantly outperformsthe usual (unbalanced) technique of including all known translations [3].We have also investigated the relationship between balanced trans-lation and Pirkola?s structured query formulation method [6].For our experiments we used the CLEF-2000 French documentcollection (approximately 21 million words from articles in Le Monde).Differences in use of diacritics, case, and punctuation can inhibitmatching between tralex entries and document terms, so we normal-ize the tralex and the documents by converting characters to low-ercase and removing all diacritic marks and punctuation.
We thentranslate the documents using the process described above, indexthe translated documentswith the Inquery information retrieval sys-tem, and perform retrieval using ?long?
queries formulated by group-ing all terms in the title, narrative, and description fields of eachEnglish topic description using Inquery?s #sum operator.
We reportmean average precision on the 34 topics for which relevant Frenchdocumentsexist, basedon the relevancejudgments provided by CLEF.We evaluated several strategies for using the WebDict and STRANDtralexes.3.1 WebDict TralexSince a tralex may contain an eclectic mix of root forms and mor-phological variants, we use a four-stage backoff strategy to maxi-mize coverage while limiting spurious translations:1.
Match the surface form of a document term to surface formsof French terms in the tralex.2.
Match the stem of a document term to surface forms of Frenchterms in the tralex.3.
Match the surface form of a document term to stems of Frenchterms in the tralex.4.
Match the stem of a document term to stems of French terms inthe tralex.We used unsupervisedinduction of stemming rules basedon the Frenchcollection to build the stemmer [5].
The process terminates as soonas a match is found at any stage, and the known translations for thatmatch are generated.
The process may produce an inappropriatemorphological variant for a correct English translation, so we usedInquery?s English kstem stemmer at indexing time to minimize theeffect of that factor on retrieval effectiveness.3.2 STRAND TralexOne limitation of a statistically derived tralex is that any term hassome probability of aligning with any other term.
Merely sortingtranslation alternatives in order of decreasing likelihood ratio willthus find some translation alternatives for every French term that ap-peared at least once in the set of parallel Web pages.
In order to limitthe introduction of spurious translations, we included only transla-tion pairs with at least N co-occurrences in the set used to build thetralex.
We performed runs with N = 1; 2; 3, using the four-stagebackoff strategy described above.3.3 WebDict Merging using STRANDWhen two sources of evidence with different characteristics areavailable, a combination-of-evidence strategy can sometimes out-perform either source alone.
Our initial experiments indicated thatthe WebDict tralex was the better of the two (see below), so we adopteda reranking strategy in which the WebDict tralex was refined ac-cording a voting strategy to which both the original WebDict andSTRAND tralex rankings contributed.Condition MAPSTRAND (N = 1) 0.2320STRAND (N = 2) 0.2440STRAND (N = 3) 0.2499Merging 0.2892WebDict 0.2919Backoff 0.3282Table 1: Mean Average Precision (MAP), averaged over 34 top-icsFor each French term that appeared in both tralexes, we gave thetop-ranked translation in each tralex a score of 100, the next a scoreof 99, and so on.
We then summed the WebDict and STRAND scoresfor each translation, reranked the WebDict translations based on thatsum, and then appendedany STRAND-only translations for that Frenchterm.
Thus, although both sourcesof evidence were weighted equallyin the voting, STRAND-only evidence received lower precedencein the merged ranking.
For French terms that appeared in only onetralex, we included those entries unchangedin the merged tralex.
Inthis experiment run we used a threshold of N = 1, and applied thefour-stage backoff strategy described above to the merged resource.3.4 WebDict Backoff to STRANDA possibleweaknessof our merging strategy is that inflected formsare more common in our STRAND tralex, while root forms are morecommon in our WebDict tralex.
STRAND tralex entries that werecopied unchangedinto the merged tralex thus often matched in step1 of the four-stage backoff strategy, preventing WebDict contribu-tions from being used.
With the WebDict tralex outperforming theSTRAND tralex, this factor could hurt our results.
As an alterna-tive to merging, therefore, we also tried a simple backoff strategy inwhich we used the original WebDict tralex with the four-stage back-off strategy described above, to which we added a fifth stage in theevent that fewer than two WebDict tralex matches were found:5.
Match the surface form of a document term to surface formsof French terms in the STRAND tralex.We used a threshold of N = 2 for this experiment run.4.
RESULTSTable 1 summarizes our results.
Increasing thresholds seem tobe helpful with the STRAND tralex, although the differences werenot found to be statistically significant by a paired two-tailed t-testwith p < 0:05.
Merging the tralexes provided no improvementover using the WebDict tralex alone, but our backoff strategy pro-duced a statistically significant 12% improvement in mean averageprecision (at p < 0:01) over the next best tralex (WebDict alone).As Figure 1 shows, the improvement is remarkably consistent, withonly four of the 34 topics adverselyaffected and only one topic show-ing a substantial negative impact.Breaking down the backoff results by stage (Table 2), we findthat the majority of query-to-document hits are obtained in the firststage, i.e.
matches of the term?s surface form in the document to atranslation of the surface form in the dictionary.
However, the back-off process improves by-token coverage of terms in documents by8%, and gives a 3% relative improvement in retrieval results; it alsocontributed additional translations to the top-2 set in approximately30% of the cases, leading to the statistically significant 12% relativeimprovement in mean averageprecision as compared to the baselineusing WebDict alone with 4-stage backoff.Figure 1: WebDict-to-tralex backoff vs. WebDict alone, byqueryStage (forms) Lexicon matches1 (surface-surface) 70.38%2 (stem-surface) 3.18%3 (surface-stem) 0.46%4 (stem-stem) 0.98%5 (STRAND) 8.34%No match found 16.66%Table 2: Term matches in 5-stage backoff5.
CONCLUSIONSThere are many ways of combining evidence from multiple trans-lation lexicons.
We use tralexes similar to those usedby Nie et al [4],but our work differs in our use of balanced translation and a back-off translation strategy (which produces a stronger baseline for ourWebDict tralex), and in our comparisonof merging and backoff trans-lation strategies for combining resources.
In future work we plan toexplore other combinations of merging and backoff and other merg-ing strategies, including post-retrieval merging of the ranked lists.In addition, parallel corpora can be exploited for more than justthe extraction of a non-contextualized translation lexicon.
We arecurrently engagedin work on lexical selection methods that take ad-vantage of contextual information, in the context of our research onmachine translation, and we expect that CLIR results will be im-proved by contextually-informed scoring of term translations.6.
ACKNOWLEDGMENTSThis research was supported in part by Department of Defensecontract MDA90496C1250 and TIDES DARPA/ITO CooperativeAgreement N660010028910,7.
REFERENCES[1] L. Ballesteros and W. B. Croft.
Resolving ambiguity forcross-language retrieval.
In W. B. Croft, A. Moffat, and C. V.Rijsbergen, editors, Proceedings of the 21st AnnualInternational ACM SIGIR Conference on Research andDevelopment in Information Retrieval, pages 64?71.
ACMPress, Aug.
1998.
[2] T. Dunning.
Accurate methods for the statistics of surprise andcoincidence.
Computational Linguistics, 19(1):61?74, March1993.
[3] G.-A.
Levow and D. W. Oard.
Translingual topic trackingwith PRISE.
In Working Notes of the Third Topic Detectionand Tracking Workshop, Feb.
2000.
[4] J.-Y.
Nie, M. Simard, P. Isabelle, and R. Durand.Cross-language information retrieval based on parallel textsand automatic mining of parallel texts from the web.
InM.
Hearst, F. Gey, and R. Tong, editors, Proceedings of the22nd Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval, pages74?81, Aug.
1999.
[5] D. W. Oard, G.-A.
Levow, and C. I. Cabezas.
CLEFexperiments at Maryland: Statistical stemming and backofftranslation.
In C. Peters, editor, Proceedings of the FirstCross-Language Evaluation Forum.
2001.
To appear.http://www.glue.umd.edu/oard/research.html.
[6] D. W. Oard and J. Wang.
NTCIR-2 ECIR experiments atMaryland: Comparing structured queries and balancedtranslation.
In Second National Institute of Informatics (NII)Test Collection Information Retrieval (NTCIR) workshop.forthcoming.
[7] P. Resnik.
Mining the Web for bilingual text.
In 37th AnnualMeeting of the Association for Computational Linguistics(ACL?99), College Park, Maryland, June 1999.
[8] P. Sheridan and J. P. Ballerini.
Experiments in multilingualinformation retrieval using the SPIDER system.
InProceedings of the 19th Annual International ACM SIGIRConference on Research and Development in InformationRetrieval, Aug. 1996.
