Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 715?719,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsBetter Automatic Treebank Conversion Using A Feature-Based ApproachMuhua Zhu Jingbo Zhu Minghan HuNatural Language Processing Lab.Northeastern University, Chinazhumuhua@gmail.comzhujingbo@mail.neu.edu.cn huminghan@ise.neu.edu.cnAbstractFor the task of automatic treebank conversion,this paper presents a feature-based approachwhich encodes bracketing structures in a tree-bank into features to guide the conversion ofthis treebank to a different standard.
Exper-iments on two Chinese treebanks show thatour approach improves conversion accuracyby 1.31% over a strong baseline.1 IntroductionIn the field of syntactic parsing, research efforts havebeen put onto the task of automatic conversion ofa treebank (source treebank) to fit a different stan-dard which is exhibited by another treebank (tar-get treebank).
Treebank conversion is desirable pri-marily because source-style and target-style annota-tions exist for non-overlapping text samples so that alarger target-style treebank can be obtained throughsuch conversion.
Hereafter, source and target tree-banks are named as heterogenous treebanks due totheir different annotation standards.
In this paper,we focus on the scenario of conversion betweenphrase-structure heterogeneous treebanks (Wang etal., 1994; Zhu and Zhu, 2010).Due to the availability of annotation in a sourcetreebank, it is natural to use such annotation toguide treebank conversion.
The motivating idea isillustrated in Fig.
1 which depicts a sentence anno-tated with standards of Tsinghua Chinese Treebank(TCT) (Zhou, 1996) and Penn Chinese Treebank(CTB) (Xue et al, 2002), respectively.
Supposethat the conversion is in the direction from the TCT-style parse (left side) to the CTB-style parse (rightside).
The constituents vp:[?/will??/surrender],dj:[??/enemy?/will?
?/surrender], and np:[??/intelligence?
?/experts] in the TCT-style parsestrongly suggest a resulting CTB-style parse alsobracket the words as constituents.
Zhu andZhu (2010) show the effectiveness of using brack-eting structures in a source treebank (source-sidebracketing structures in short) as parsing constraintsduring the decoding phase of a target treebank-basedparser.However, using source-side bracketing structuresas parsing constraints is problematic in some cases.As illustrated in the shadow part of Fig.
1, the TCT-style parse takes ???/deems?
as the right bound-ary of a constituent while in the CTB-style parse,????
is the left boundary of a constituent.
Ac-cording to the criteria used in Zhu and Zhu (2010),any CTB-style constituents with ????
being theleft boundary are thought to be inconsistent with thebracketing structure of the TCT-style parse and willbe pruned.
However, if we prune such ?inconsistent?constituents, the correct conversion result (right sideof Fig.
1) has no chance to be generated.The problem comes from binary distinctions usedin the approach of Zhu and Zhu (2010).
With bi-nary distinctions, constituents generated by a targettreebank-based parser are judged to be either con-sistent or inconsistent with source-side bracketingstructures.
That approach prunes inconsistent con-stituents which instead might be correct conversionresults1.
In this paper, we insist on using source-side bracketing structures as guiding information.Meanwhile, we aim to avoid using binary distinc-tions.
To achieve such a goal, we propose to use afeature-based approach to treebank conversion andto encode source-side bracketing structures as a set1To show how severe this problem might be, Section 3.1presents statistics on inconsistence between TCT and CTB.715zjdjnpn??n??v??,,djn??vpd?v??IPNPNN??NN??VPVV??PU,IPNPNN??VPAD?VV????
??
??
, ??
?
?
?qingbao zhuanjia renwei , diren jiang touxiangintelligence experts deem , enemy will surrenderFigure 1: An example sentence with TCT-style annotation (left) and CTB-style annotation (right).of features.
The advantage is that inconsistent con-stituents can be scored with a function based on thefeatures rather than ruled out as impossible.To test the efficacy of our approach, we conductexperiments on conversion from TCT to CTB.
Theresults show that our approach achieves a 1.31% ab-solute improvement in conversion accuracy over theapproach used in Zhu and Zhu (2010).2 Our Approach2.1 Generic System ArchitectureTo conduct treebank conversion, our approach, over-all speaking, proceeds in the following steps.Step 1: Build a parser (named source parser) on asource treebank, and use it to parse sentencesin the training data of a target treebank.Step 2: Build a parser on pairs of golden target-style and auto-assigned (in Step 1) source-styleparses in the training data of the target tree-bank.
Such a parser is named heterogeneousparser since it incorporates information derivedfrom both source and target treebanks, whichfollow different annotation standards.Step 3: In the testing phase, the heterogeneousparser takes golden source-style parses as inputand conducts treebank conversion.
This will beexplained in detail in Section 2.2.To instantiate the generic framework describedabove, we need to decide the following three factors:(1) a parsing model for building a source parser, (2)a parsing model for building a heterogeneous parser,and (3) features for building a heterogeneous parser.In principle, any off-the-shelf parsers can be usedto build a source parser, so we focus only on thelatter two factors.
To build a heterogeneous parser,we use feature-based parsing algorithms in order toeasily incorporate features that encode source-sidebracketing structures.
Theoretically, any feature-based approaches are applicable, such as Finkel etal.
(2008) and Tsuruoka et al (2009).
In this pa-per, we use the shift-reduce parsing algorithm for itssimplicity and competitive performance.2.2 Shift-Reduce-Based Heterogeneous ParserThe heterogeneous parser used in this paper is basedon the shift-reduce parsing algorithm described inSagae and Lavie (2006a) and Wang et al (2006).Shift-reduce parsing is a state transition process,where a state is defined to be a tuple ?S,Q?.
Here, Sis a stack containing partial parses, and Q is a queuecontaining word-POS pairs to be processed.
At eachstate transition, a shift-reduce parser either shifts thetop item of Q onto S, or reduces the top one (or two)items on S.A shift-reduce-based heterogeneous parser pro-ceeds similarly as the standard shift-reduce parsingalgorithm.
In the training phase, each target-styleparse tree in the training data is transformed intoa binary tree (Charniak et al, 1998) and then de-composed into a (golden) action-state sequence.
Aclassifier can be trained on the set of action-states,716where each state is represented as a feature vector.In the testing phase, the trained classifier is usedto choose actions for state transition.
Moreover,beam search strategies can be used to expand thesearch space of a shift-reduce-based heterogeneousparser (Sagae and Lavie, 2006a).
To incorporate in-formation on source-side bracketing structures, inboth training and testing phases, feature vectors rep-resenting states ?S,Q?
are augmented with featuresthat bridge the current state and the correspondingsource-style parse.2.3 FeaturesThis section describes the feature functions used tobuild a heterogeneous parser on the training dataof a target treebank.
The features can be dividedinto two groups.
The first group of features arederived solely from target-style parse trees so theyare referred to as target side features.
This groupof features are completely identical to those used inSagae and Lavie (2006a).In addition, we have features extracted jointlyfrom target-style and source-style parse trees.
Thesefeatures are generated by consulting a source-styleparse (referred to as ts) while we decompose atarget-style parse into an action-state sequence.Here, si denote the ith item from the top of thestack, and qi denote the ith item from the frontend of the queue.
We refer to these features asheterogeneous features.Constituent features Fc(si, ts)This feature schema covers three feature functions:Fc(s1, ts), Fc(s2, ts), and Fc(s1 ?
s2, ts), whichdecide whether partial parses on stack S correspondto a constituent in the source-style parse ts.
That is,Fc(si, ts)=+ if si has a bracketing match (ignoringgrammar labels) with any constituent in ts.
s1?s2represents a concatenation of spans of s1 and s2.Relation feature Fr(Ns(s1), Ns(s2))We first position the lowest node Ns(si) in ts,which dominates the span of si.
Then a featurefunction Fr(Ns(s1), Ns(s2)) is defined to indicatethe relationship of Ns(s1) and Ns(s2).
If Ns(s1)is identical to or a sibling of Ns(s2), we sayFr(Ns(s1), Ns(s2)) =+.Features Bridging Source and Target ParsesFc(s1, ts)=?Fc(s2, ts)=+Fc(s1?s2, ts)=+Fr(Ns(s1), Ns(s2))=?Ff (RF (s1), q1)=?Fp(RF (s1), q1)= ?v ?
dj ?
zj ?,?Table 1: An example of new features.
Suppose we areconsidering the sentence depicted in Fig.
1.Frontier-words feature Ff (RF (s1), q1)A feature function which decides whether the rightfrontier word of s1 and q1 are in the same basephrase in ts.
Here, a base phrase is defined to beany phrase which dominates no other phrases.Path feature Fp(RF (s1), q1)Syntactic path features are widely used in the litera-ture of semantic role labeling (Gildea and Jurafsky,2002) to encode information of both structures andgrammar labels.
We define a string-valued featurefunction Fp(RF (s1), q1) which connects the rightfrontier word of s1 to q1 in ts.To better understand the above feature func-tions, we re-examine the example depicted inFig.
1.
Suppose that we use a shift-reduce-basedheterogeneous parser to convert the TCT-style parseto the CTB-style parse and that stack S currentlycontains two partial parses: s2:[NP (NN??)
(NN??)]
and s1: (VV ??).
In such a state, we cansee that spans of both s2 and s1 ?s2 correspond toconstituents in ts but that of s1 does not.
Moreover,Ns(s1) is dj and Ns(s2) is np, so Ns(s1) andNs(s2) are neither identical nor sisters in ts.
Thevalues of these features are collected in Table 1.3 Experiments3.1 Data Preparation and Performance MetricIn the experiments, we use two heterogeneous tree-banks: CTB 5.1 and the TCT corpus released bythe CIPS-SIGHAN-2010 syntactic parsing competi-tion2.
We actually only use the training data of thesetwo corpora, that is, articles 001-270 and 400-1151(18,100 sentences, 493,869 words) of CTB 5.1 and2http://www.cipsc.org.cn/clp2010/task2 en.htm717the training data (17,529 sentences, 481,061 words)of TCT.To evaluate conversion accuracy, we use thesame test set (named Sample-TCT) as in Zhu andZhu (2010), which is a set of 150 sentences withmanually assigned CTB-style and TCT-style parsetrees.
In Sample-TCT, 6.19% (215/3473) CTB-style constituents are inconsistent with respect to theTCT standard and 8.87% (231/2602) TCT-style con-stituents are inconsistent with respect to the CTBstandard.For all experiments, bracketing F1 is used as theperformance metric, provided by EVALB 3.3.2 Implementation IssuesTo implement a heterogeneous parser, we first builda Berkeley parser (Petrov et al, 2006) on the TCTtraining data and then use it to assign TCT-styleparses to sentences in the CTB training data.
Onthe ?updated?
CTB training data, we build two shift-reduce-based heterogeneous parsers by using max-imum entropy classification model, without/withbeam search.
Hereafter, the two heterogeneousparsers are referred to as Basic-SR and Beam-SR, re-spectively.In the testing phase, Basic-SR and Beam-SR con-vert TCT-style parse trees in Sample-TCT to theCTB standard.
The conversion results are evalu-ated against corresponding CTB-style parse trees inSample-TCT.
Before conducting treebank conver-sion, we apply the POS adaptation method proposedin Jiang et al (2009) to convert TCT-style POS tagsin the input to the CTB standard.
The POS conver-sion accuracy is 96.2% on Sample-TCT.3.3 ResultsTable 2 shows the results achieved by Basic-SR andBeam-SR with heterogeneous features being addedincrementally.
Here, baseline represents the systemswhich use only target side features.
From the tablewe can see that heterogeneous features improve con-version accuracy significantly.
Specifically, addingthe constituent (Fc) features to Basic-SR (Beam-SR) achieves a 2.79% (3%) improvement, addingthe relation (Fr) and frontier-word (Ff ) featuresyields a 0.79% (0.98%) improvement, and adding3http://nlp.cs.nyu.edu/evalbSystem Features <= 40 words UnlimitedBasic-SR baseline 83.34 80.33+Fc 85.89 83.12+Fr, +Ff 85.47 83.91+Fp 86.01 84.05Beam-SR baseline 84.40 81.27+Fc 86.30 84.27+Fr, + Ff 87.00 85.25+Fp 87.27 85.38Table 2: Adding new features to baselines improve tree-bank conversion accuracy significantly on Sample-TCT.the path (Fp) feature achieves a 0.14% (0.13%) im-provement.
The path feature is not so effective asexpected, although it manages to achieve improve-ments.
One possible reason lies on the data sparse-ness problem incurred by this feature.Since we use the same training and testing dataas in Zhu and Zhu (2010), we can compare ourapproach directly with the informed decoding ap-proach used in that work.
We find that Basic-SRachieves very close conversion results (84.05% vs.84.07%) and Beam-SR even outperforms the in-formed decoding approach (85.38% vs. 84.07%)with a 1.31% absolute improvement.4 Related WorkFor phrase-structure treebank conversion, Wang etal.
(1994) suggest to use source-side bracketingstructures to select conversion results from k-bestlists.
The approach is quite generic in the sense thatit can be used for conversion between treebanks ofdifferent grammar formalisms, such as from a de-pendency treebank to a constituency treebank (Niuet al, 2009).
However, it suffers from limitedvariations in k-best lists (Huang, 2008).
Zhu andZhu (2010) propose to incorporate bracketing struc-tures as parsing constraints in the decoding phase ofa CKY-style parser.
Their approach shows signifi-cant improvements over Wang et al (1994).
How-ever, it suffers from binary distinctions (consistentor inconsistent), as discussed in Section 1.The approach in this paper is reminiscent ofco-training (Blum and Mitchell, 1998; Sagae andLavie, 2006b) and up-training (Petrov et al, 2010).Moreover, it coincides with the stacking methodused for dependency parser combination (Martins718et al, 2008; Nivre and McDonald, 2008), thePred method for domain adaptation (Daume?
III andMarcu, 2006), and the method for annotation adap-tation of word segmentation and POS tagging (Jianget al, 2009).
As one of the most related works,Jiang and Liu (2009) present a similar approach toconversion between dependency treebanks.
In con-trast to Jiang and Liu (2009), the task studied in thispaper, phrase-structure treebank conversion, is rel-atively complicated and more efforts should be putinto feature engineering.5 ConclusionTo avoid binary distinctions used in previous ap-proaches to automatic treebank conversion, we pro-posed in this paper a feature-based approach.
Exper-iments on two Chinese treebanks showed that ourapproach outperformed the baseline system (Zhuand Zhu, 2010) by 1.31%.AcknowledgmentsWe thank Kenji Sagae for helpful discussions on theimplementation of shift-reduce parser and the threeanonymous reviewers for comments.
This work wassupported in part by the National Science Founda-tion of China (60873091; 61073140), SpecializedResearch Fund for the Doctoral Program of HigherEducation (20100042110031), the Fundamental Re-search Funds for the Central Universities and Nat-ural Science Foundation of Liaoning Province ofChina.ReferencesAvrim Blum and Tom Mitchell.
1998.
Combining La-beled and Unlabeled Data with Co-Training.
In Pro-ceedings of COLT 1998.Eugene Charniak, Sharon Goldwater, and Mark Johnson.1998.
Edge-Based Best-First Chart Parsing.
In Pro-ceedings of the Six Workshop on Very Large Corpora,pages 127-133.Hal Daume?
III and Daniel Marcu.
2006.
Domain Adap-tation for Statistical Classifiers.
Journal of ArtificalIntelligence Research, 26:101-166.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, Feature-Based ConditionalRandom Fileds Parsing.
In Proceedings of ACL 2008,pages 959-967.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic La-beling for Semantic Roles.
Computational Linguis-tics, 28(3):245-288.Liang Huang.
2008.
Forest Reranking: DiscriminativeParsing with Non-local Features.
In Proceedings ofACL, pages 824-831.Wenbin Jiang, Liang Huang, and Qun Liu.
2009.
Au-tomatic Adaptation of Annotation Standards: ChineseWord Segmentation and POS Tagging - A Case Study.In Proceedings of ACL 2009, pages 522-530.Wenbin Jiang and Qun Liu.
2009.
Automatic Adapta-tion of Annotation Standards for Dependency Parsing?
Using Projected Treebank As Source Corpus.
InProceedings of IWPT 2009, pages 25-28.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stack Dependency Parsers.
InProceedings of EMNLP 2008, pages 157-166.Zheng-Yu Niu, Haifeng Wang, and Hua Wu.
2009.
Ex-ploiting Heterogeneous Treebanks for Parsing.
In Pro-ceedings of ACL 2009, pages 46-54.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing Graph-Based and Transition-Based DependencyParsers.
In Proceedings of ACL 2008, pages 950-958.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and In-terpretable Tree Annotation.
In Proceedings of ACL2006, pages 433-440.Slav Petrov, Pi-Chuan Chang, Michael Ringgaard, andHiyan Alshawi.
2010.
Uptraining for Accurate Deter-ministic Question Parsing.
In Proceedings of EMNLP2010, pages 705-713.Kenji Sagae and Alon Lavie.
2006.
A Best-First Prob-abilistic Shift-Reduce Parser.
In Proceedings of ACL-COLING 2006, pages 691-698.Kenji Sagae and Alon Lavie.
2006.
Parser Combinationby Reparsing.
In Proceedings of NAACL 2006, pages129-132.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Anani-adou.
2009.
Fast Full Parsing by Linear-Chain Condi-tional Random Fields.
In Proceedings of EACL 2009,pages 790-798.Jong-Nae Wang, Jing-Shin Chang, and Keh-Yih Su.1994.
An Automatic Treebank Conversion Algorithmfor Corpus Sharing.
In Proceedings of ACL 1994,pages 248-254.Mengqiu Wang, Kenji Sagae, and Teruk Mitamura.
2006.A Fast, Deterministic Parser for Chinese.
In Proceed-ings of ACL-COLING 2006, pages 425-432.Nianwen Xue, Fu dong Chiou, and Martha Palmer.
2002.Building a Large-Scale Annotated Chinese Corpus.
InProceedings of COLING 2002, pages 1-8.Qiang Zhou.
1996.
Phrase Bracketing and Annotation onChinese Language Corpus (in Chinese).
Ph.D. thesis,Peking University.Muhua Zhu, and Jingbo Zhu.
2010.
Automatic TreebankConversion via Informed Decoding.
In Porceedings ofCOLING 2010, pages 1541-1549.719
