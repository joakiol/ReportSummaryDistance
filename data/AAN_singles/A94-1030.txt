IMPROVING CHINESE TOKENIZATION WITH LINGUISTICFILTERS ON STATISTICAL LEXICAL ACQUISITIONDekai Wu Pascale FungDepartment ofComputer Science Computer Science DepartmentUniversity of Science & Technology (HKUST) Columbia UniversityClear Water Bay, Hong Kong New York, NY 10027dekai@cs, ust.
hk pascale@cs, columbia, eduAbstractThe first step in Chinese NLP is to tokenize or segment char-acter sequences into words, since the text contains no worddelimiters.
Recent heavy activity in this area has shownthe biggest stumbling block to be words that are absentfrom the lexicon, since successful tokenizers to date havebeen based on dictionary lookup (e.g., Chang &Chen 1993;Chiang et al 1992; L inet  al.
1993; Wu & Tseng 1993;Sproat et al 1994).We present empirical evidence for four points concern-ing tokenization of Chinese text: (I) More rigorous "blind"evaluation methodology is needed to avoid inflated accuracymeasurements; we introduce the nk-blind method.
(2) Theextent of the unknown-word problem is far more serious thangenerally thought, when tokenizing unrestricted texts in re-alistic domains.
(3) Statistical lexical acquisition is a prac-tical means to greatly improve tokenization accuracy withunknown words, reducing error rates as much as 32.0%.
(4)When augmenting the lexicon, linguistic onstraints can pro-vide simple inexpensive filters yielding significantly betterprecision, reducing error rates as much as 49.4%.HOW TO HANDLE DOUBLE STANDARDSCurrent evaluation practice favors overly optimistic accuracyestimates.
Because partially-tokenized words are usuallyevaluated as being correctly tokenized, failures to tokenizeunknown words can be overlooke d .
For example, what makes~l~JJ~ (yufin zhh j~n, a charity) a single word when iBJ~llJJ andare both legitimate words.'?
One answer is that translat-ing the partially-tokenized segments individually can yield"assistance gold" or "aid currency", instead of the unques-tionably correct "charity" or "charity fund".
Another answeris that a speech synthesizer should never pause between thetwo segments; otherwise ~g)J is taken as a verb and ~i~ as asurname, changing the meaning to "help Gold".
A blind eval-uation paradigm is needed that accommodates disagreementbetween human judges, yet does not bias the judges to acceptthe computer's output oo generously.We have devised a procedure called nk-blind that uses nblind judges' standards.
The n judges each hand-segmentthe test sentences independently, before the algorithm is run.Then, the algorithm's output is compared against he judges';for each segment produced by the algorithm, the segment isconsidered to be a correct oken if at least k of the n judgesagree.
Thus, more than one segmentation may be consideredcorrect if we set k such that k _< \[~J.
If k is set to 1, it issufficient for any judge to sanction a segment.
If k = n, allthe judges must agree.
Under the n/c-blind method aprecisionrate can be given under any chosen (n, k) setting.The experiments below were conducted with 100 pairs ofsentences from the corpus containing between 2,000 and 2,600words, sampled randomly with replacement.
All results re-ported in Figure 1 give the precision rates for n = 8 judgeswith all values of k between 1 and n. Note the tendency ofhigher values of k to reduce precision estimates.
The widevariance with different k (between 30% and 90%) underscoresthe importance of more rigorous evaluation methodology.EXPERIMENT ITokenizing independently derived test data.
The unknownword problem is now widely recognized, but we believe itsseverity is still greatly underestimated.
As an "acid test", wetokenized a corpus that was derived completely independentlyof the dictionary that our tokenizer is based on.
We used astatistical dictionary-based tokenizer designed to be represen-tative of current okenizing approaches, which chooses thesegmentation that maximizes the product of the individualwords' probabilities.
The baseline dictionary used by the tok-enizer is the BDC dictionary (BDC 1992), containing 89,346unique orthographic forms.
The text, drawn from the HKUSTEnglish-Chinese Parallel Bilingual Corpus (Wu 1994), con-sists of transcripts from the parliamentary proceedings of theHong Kong Legislative Council.
Thus, the text can be ex-pected to contain many references to subjects outside the do-mains under consideration by our dictionary's lexicographersin Taiwan.
Regional usage differences are also to be expected.The results (see Figure 1) show accuracy rates far below the90-99% range which is typically reported.
Visual inspectionof tokenized output showed that an overwhelming majority ofthe errors arose from missing dictionary entries.
Tokeniza-tion performance on realistic unrestricted text is still seriouslycompromised.EXPERIMENT IITokenization with statistical lexicon augmentation.
To al-leviate the unknown word problem, we next experimentedwith augmenting the tokenizer's dictionary using CXtract,a statistical tool that finds morpheme sequences likely to beChinese words (Fung & Wu 1994).
In the earlier work wefound CXtract to be a good generator f previously unknownlexical entries, so overall token recall was expected to im-prove.
However, it was not clear whether the gain wouldoutweigh errors introduced by the illegitimate l xical entriesthat CXtract also produces.The training corpus consisted of approximately 2 millionChinese characters drawn from the Chinese half of our bilin-gual corpus.
The unsupervised training procedure is describedin detail in Fung & Wu (1994).
The training suggested 6,650candidate l xical entries.
Of these, 2,040 were already present1RCI 180in the dictionary, leaving 4,610 previously unknown new en-tries.The same tokenization experiment was then run, using theaugmented ictionary instead.
The results shown in Fig-ure 1 bear out our hypothesis that augmenting the lexiconwith CXtract's statistically generated lexical entries wouldimprove the overall precision, reducing error ates as much as32.0% for k = 2.EXPERIMENT IIIMorphosyntactic filters for lexicon candidates.
CXtractproduces excellent recall but we wished to improve precisionfurther.
Ideally, the false candidates should be rejected bysome automatic means, without eliminating valid lexical en-tries.
To this end, we investigated a set of 34 simple filtersbased on linguistic principles.
Space precludes a full listing;selected filters are discussed below.Our filters can be extremely inexpensive because CXtract'sstatistical criteria are already tuned for high precision.
Thefiltering process first segments the candidate using the orig-inal dictionary, to identify the component words.
It thenapplies morphological nd syntactic onstraints o eliminate(a) sequences that should remain multiple segments and (b) ill-formed sequences.Morphological constraints.
The morphologically-based fil-ters reject a hypothesized lexical entry if it matches any fil-tering pattern.
The particular characters in these filters areusually classified either as morphological ffixes, or as indi-vidual words.
We reject any sequence with the affix on thewrong end (the special case of the genitive fl",j (de) is consid-ered below).
Because morphemes such as the plural marker~ (m6n) or the instance marker -3k (d)  are suffixes, we caneliminate candidate sequences that begin with them.
Simi-larly, we can reject sequences that end with the ordinal prefix(di) or the preverbial durative ~ (z/d).Filtering characters cannot be used if they are polysemousor homonymous and can participate in legitimate words inother uses.
For example, the durative ~i~ (zhe) is not a goodfilter because the same character (with varying pronuncia-tions) can be used to mean "apply", "trick", or "touch", amongothers.Any candidate l xical entry is filtered if it contains the gen-itive/associative ~ (de).
This includes, for example, both ill-formed boundary-crossing patterns like ~ j~ (de w6i xitin,danger of), and phrases like ~:~\ ]~ (xiang gang de qifmtti, Hong Kong's future) which should properly be segmented~:h~ fl'-,J ~J~,.
In addition, because the compounding processdoes not involve two double-character words as frequently asother patterns, uch sequences were rejected.Closed-class yntactic onstraints.
The closed-class filtersoperate on two distinct principles.
Sequences ending withstrongly prenominal or preverbial words are rejected, as aresequences beginning with postnominals and postverbials.
Amajority of the filtering patterns match correct syntactic units,including prepositional, conjunctive, modal, adverbial, andverb phrases.
The rationale for rejecting such sequences ithat these closed-class words do not satisfy the criteria forbeing bound into compounds, and just co-occur with somesequences by chance because of their high frequency.Results.
The same tokenization experiment was run us-ing the filtered augmented dictionary.
The filters left 5,506candidate l xical entries out of the original 6,650, of which3,467 were previously unknown.
Figure 1 shows significantlyimproved precision in every measurement except for a veryslight drop with k = 8, with an error rate reduction of 49.4%at k : 2.
Thus any loss in token recall due to the filters isoutweighed by the gain in precision.
This may be taken asindirect evidence that the loss in recall is not large.CONCLUSIONWe have introduced a blind evaluation method that accom-modates multiple standards and gives some indication of howwell algorithms' outputs match human preferences.We have demonstrated that pure statistically-based l xicalacquisition on the same corpus being tokenized can signif-icantly reduce error rates due to unknown words.
We alsodemonstrated mpirically the effectiveness of simple mor-phosyntactic filters in improving the precision of a hybridstatistical/linguistic method for generating new lexical en-tries.
Using linguistic knowledge to construct filters ratherthan generators has the advantage that applicability conditionsdo not need to be closely checked, since the training corpuspresumably already adheres to any applicability conditions.1009080706050403020m" l -base l ine"  - - - -: ......... "n -augmented"  - .
.
.
.
.......... \ ] "~ ............. = " I l l - f i l te red"  ..a ....-~- - -~ ...... ?
... ....... , , , .
..Q.-----, '.o.i .
i i2 3 4 5 6 7 8Figure 1.
Comparison of nk-Blind Precision PercentagesREFERENCESBDC.
1992.
The BDC Chinese-English electronic dictionary (version 2.0).
BehaviorDesign Corporation.CHANG, CHAO-HUANG & CHENG-DER CHEN.
1993.
HMM-based part-of-speechtaggingfor Chinese corpora.
In Proceedings ofthe Workshop on Very Large Corpora,40-47, Columbus, Ohio.CHIANG, TUNG-HUI, JING-SHIN CHANG, MING-YU LIN, & KEH-YIH SU.
1992.
Statis-tical models for word segmentation a d unknown resolution.
In Proceedings ofROCLING-92, 121-146.FUNG, PASCALE & DEKAI WU.
1994.
Statistical augmentation f a Chinese machine-readable dictionary.
In Proceedings ofthe Second Annual Workshop on VeryLarge Corpora, 69-85, Kyoto.LIN, MING-YU, TUNG-HUI CHIANG, & KEH-YIH Su.
1993.
A preliminary study onunknown word problem in Chinese word segmentation.
In Proceedings ofROCLING-93, 119-141.SPROAT, RICHARD, CHILIN SHIH, WILLIAM GALE, & NANCY CHANG.
1994.
A stochasticword segmentation algorithm for a Mandarin text-to-speech system.
In Pro-ceedings of the 32nd Annual Conference of the Association for ComputationalLinguistics, 66-72, Las Cruces, New Mexico.Wu, DEKAi.
1994.
Aligning a parallel English-Chinese corpus tatistically with lexicalcriteria.
In Proceedings ofthe 32nd Annual Conference ofthe Association forComputationalLinguistics, 80-87, Las Cruces, New Mexico.Wu, ZIMIN  GWYNETH TSENG.
1993.
Chinese text segmentation for text retrieval:Achievements and problems.
Journal of The American Society for InformationScience, 44(9):532-542.181
