R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
177 ?
187, 2005.?
Springer-Verlag Berlin Heidelberg 2005Linguistically-Motivated Grammar Extraction,Generalization and AdaptationYu-Ming Hsieh, Duen-Chi Yang, and Keh-Jiann ChenInstitute of Information Science, Academia Sinica, Taipei{morris, ydc, kchen}@iis.sinica.edu.twAbstract.
In order to obtain a high precision and high coverage grammar, weproposed a model to measure grammar coverage and designed a PCFG parser tomeasure efficiency of the grammar.
To generalize grammars, a grammar binari-zation method was proposed to increase the coverage of a probabilistic context-free grammar.
In the mean time linguistically-motivated feature constraintswere added into grammar rules to maintain precision of the grammar.
The gen-eralized grammar increases grammar coverage from 93% to 99% and bracket-ing F-score from 87% to 91% in parsing Chinese sentences.
To cope with errorpropagations due to word segmentation and part-of-speech tagging errors, wealso proposed a grammar blending method to adapt to such errors.
The blendedgrammar can reduce about 20~30% of parsing errors due to error assignment ofpos made by a word segmentation system.Keywords: Grammar Coverage, Ambiguity, Sentence Parsing, GrammarExtraction.1   IntroductionTreebanks provide instances of phrasal structures and their statistical distributions.However none of treebanks provide sufficient amount of samples which cover all typesof phrasal structures, in particular, for the languages without inflectional markers, suchas Chinese.
It results that grammars directly extracted from treebanks suffer low cover-age rate and low precision [7].
However arbitrarily generalizing applicable rule patternsmay cause over-generation and increase ambiguities.
It may not improve parsing per-formance [7].
Therefore a new approach of grammar binarization was proposed in thispaper.
The binarized grammars were derived from probabilistic context-free grammars(PCFG) by rule binarization.
The approach was motivated by the linguistic fact thatadjuncts could be arbitrarily occurred or not occurred in a phrase.
The binarized gram-mars have better coverage than the original grammars directly extracted from treebank.However they also suffer problems of over-generation and structure-ambiguity.
Con-temporary grammar formalisms, such as GPSG, LFG, HPSG, take phrase structure rulesas backbone for phrase structure representation and adding feature constraints to elimi-nate illegal or non-logical structures.
In order to achieve higher coverage, the backbonegrammar rules (syntactic grammar) are allowed to be over-generation and the featureconstraints (semantic grammar for world knowledge) eliminate superfluous structures178 Y.-M. Hsieh, D.-C. Yang, and K.-J.
Chenand increase the precision of grammar representation.
Recently, probabilistic prefer-ences for grammar rules were incorporated to resolve structure-ambiguities and hadgreat improvements on parsing performances [2, 6, 10].
Regarding feature constrains, itwas shown that contexture information of categories of neighboring nodes, mothernodes, or head words are useful for improving grammar precision and parsing perform-ances [1, 2, 7, 10, 12].
However tradeoffs between grammar coverage and grammarprecision are always inevitable.
Excessive grammatical constraints will reduce grammarcoverage and hence reduce parsing performances.
On the other hand, loosely con-strained grammars cause structure-ambiguities and also reduce parsing performances.
Inthis paper, we consider grammar optimization in particular for Chinese language.
Lin-guistically-motivated feature constraints were added to the grammar rules and evaluatedto maintain both grammar coverage and precision.
In section 2, the experimental envi-ronments were introduced.
Grammar generalization and specialization methods werediscussed in section 3.
Grammars adapting to pos-tagging errors were discussed in sec-tion 4.
Conclusions and future researches were stated in the last section.2   Research EnvironmentsThe complete research environment, as shown in the figure 1, comprises of the fol-lowing five modules and functions.a) Word segmentation module: identify words including out-of-vocabulary wordand provide their syntactic categories.b) Grammar construction module: extract and derive (perform rule generalization,specialization and adaptation processes) probabilistic grammars from tree-banks.c) PCFG parser: parse input sentences.d) Evaluation module: evaluate performances of parsers and grammars.e) Semantic role assignment module: resolve semantic relations for constituents.Fig.
1.
The system diagram of CKIP parsing environmentLinguistically-Motivated Grammar Extraction, Generalization and Adaptation 1792.1   Grammar Extraction ModuleGrammars are extracted from Sinica Treebank [4, 5].
Sinica Treebank version 2.0contains 38,944 tree-structures and 230,979 words.
It provides instances of phrasalstructures and their statistical distributions.
In Sinica Treebank, each sentence is anno-tated with its syntactic structure and semantic roles for constituents in a dependencyframework.
Figure 2 is an example.e.g.
?
?
??
?
?.Ta  jiao  Li-si  jian  qiu.
?He asked Lisi to pick up the ball.?Tree-structure:S(agent:NP(Head:Nh:?)|Head:VF:?|goal:NP(Head:Nb:??)|theme:VP(Head:VC:?|goal:NP(Head:Na:?)))Fig.
2.
A sample tree-structureSince the Treebank cannot provide sufficient amount of samples which cover alltypes of phrasal structures, it results that grammars directly extracted from treebankssuffer low coverage rate [5].
Therefore grammar generalization and specializationprocesses are carried out to obtain grammars with better coverage and precision.
Thedetail processes will be discussed in section 3.2.2   PCFG Parser and Grammar Performance EvaluationThe probabilistic context-free parsing strategies were used as our parsing model [2, 6,8].
Calculating probabilities of rules from a treebank is straightforward and we usemaximum likelihood estimation to estimate the rule probabilities, as in [2].
The parseradopts an Earley?s Algorithm [8].
It is a top-down left-to-right algorithm.
The resultsof binary structures will be normalized into a regular phrase structures by removingintermediate nodes, if used grammars are binarized grammars.
Grammar efficiencywill be evaluated according to its parsing performance.2.3   Experiments and Performance EvaluationThree sets of testing data were used in our performance evaluation.
Their basic statis-tics are shown in Table 1.
Each set of testing data represents easy, hard and moderaterespectively.Table 1.
Three sets of testing data were used in our experimentsTesting data Sources hardness# of shortsentence(1-5 words)# of normalsentences(6-10 words)# of longsentences(>11 words)TotalsentencesSinica Balanced corpus moderate 612 385 124 1,121Sinorama Magazine harder 428 424 104 956Textbook Elementary school easy 1,159 566 25 1,750180 Y.-M. Hsieh, D.-C. Yang, and K.-J.
ChenThe following parser and grammar performance evaluation indicators were used inour experiments:z LP(Labeled Precision)parser by the labeled phrases of #parser by the labeled phrasescorrect  of #LP =z LR(Labeled Recall)data  testing thein phrases of #parser by the labeled phrasescorrect  of #LR =z LF(Labeled F-measure)LR  LP2* LR * LPLF+=z BP(Bracketed Precision)parser by the made brackets of pairs of #parser by the madecorrectly  brackets of pairs of #BP =z BR(Bracketed Recall)data  testing theof standard gold  thein brackets of pairs of #parser by the madecorrectly  brackets of pairs of #BR =z BF(Bracketed F-measure)BR  BP2* BR * BPBF+=Additional indicators regarding coverage of grammars?z RC-Type?type coverage of rulesdata  testingin  typesrule of #rulesgrammar  anddata   testingboth in  typesrules of #Type-RC =z RC-Token?token coverage of rulesdata  testingin  tokensrule of #rulesgrammar  anddata   testingboth in  tokensrules of #Token-RC =The token coverage of a set of rules is the ceiling of parsing algorithm to achieve.Tradeoff effects between grammar coverage and parsing F-score can be examined foreach set of rules.3   Grammar Generalization and SpecializationBy using above mentioned research environment, we intend to find out most effec-tive grammar generalization method and specialization features for Chinese lan-guage.
To extend an existing or extracted grammar, there are several different ap-proaches.
A na?ve approach is to generalize a fine-grained rule to a coarse-grainedrule.
The approach does not generate new patterns.
Only the applicable patterns foreach word were increased.
However it was shown that arbitrarily increasing theapplicable rule patterns does increase the coverage rates of grammars, but degradeparsing performance [5].
A better approach is to generalizing and specializing rulesunder linguistically-motivated way.Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 1813.1   Binary Grammar Generation, Generalization, and SpecializationThe length of a phrase in Treebank is variable and usually long phrases suffer fromlow probability.
Therefore most PCFG approaches adopt the binary equivalencegrammar, such as Chomsky normal form (CNF).
For instance, a grammar rule of S?NP Pp Adv V can be replaced by the set of equivalent rules of {S?Np R0, R0?PpR1, R1?Adv V}.
The binarization method proposed in our system is different fromCNF.
It generalizes the original grammar to broader coverage.
For instance, the aboverule after performing right-association binarization 1  will produce following threebinary rules {S?Np S?, S?
?Pp S?, S?
?Adv V}.
It results that constituents (adjunctsand arguments) can be occurred or not occurred at almost any place in the phrase.
Itpartially fulfilled the linguistic fact that adjuncts in a phrase are arbitrarily occurred.However it also violated the fact that arguments do not arbitrarily occur.
Experimentalresults of the Sinica testing data showed that the grammar token coverage increasedfrom 92.8% to 99.4%, but the labeling F-score dropped from 82.43% to 82.11% [7].Therefore feature constraints were added into binary rules to limit over-generationcaused by recursively adding constituents into intermediate-phrase types, such as S?
atabove example.Feature attached rules will look like following:S?-left:Adv-head:V?
Adv V;S?-left:Pp-head:V?Pp S?-left:Adv-head:V;The intermediated node S?-left:Pp-head:V says that it is a partial S structure with left-most constituent Pp and a phrasal head V. Here the leftmost feature constraints linearorder of constituents and the head feature implies that the structure patterns are headword dependent.
Both constraints are linguistically plausible.
Another advantage ofthe feature-constraint binary grammar is that in addition to rule probability it is easyto implement association strength of modifier word and head word to evaluate plausi-bility of derived structures.3.2   Feature Constraints for Reducing Ambiguities of Generalized GrammarsAdding feature constraints into grammar rules attempts to increase precision of gram-mar representation.
However the side-effect is that it also reduces grammar coverage.Therefore grammar design is balanced between its precision and coverage.
We arelooking for a grammar with highest coverage and precision.
The tradeoff depends onthe ambiguity resolution power of adopted parser.
If the ambiguity resolution powerof adopted parser is strong and robust, the grammar coverage might be more impor-tant than grammar precision.
On the other hand a weak parser had better to usegrammars with more feature constraints.
In our experiments, we consider grammarssuited for PCFG parsing.
The follows are some of the most important linguistically-motivated features which have been tested.1The reason for using right-association binarization instead of left-association or head-firstassociation binarization is that our parsing process is from left to right.
It turns out that pars-ing speed of right associated grammars is much faster than left-associated grammars for left-to-right parsing.182 Y.-M. Hsieh, D.-C. Yang, and K.-J.
ChenHead (Head feature): Pos of phrasal head will propagate to all intermediate nodeswithin the constituent.Example:S(NP(Head:Nh:?)|S?-VF(Head:VF:?|S?-VF(NP(Head:Nb:??
)|VP(Head:VC:?| NP(Head:Na:?
)))))Linguistic motivations: Constrain sub-categorization frame.Left (Leftmost feature): The pos of the leftmost constitute will propagate one?level toits intermediate mother-node only.Example:S(NP(Head:Nh:?)|S?-Head:VF(Head:VF:?|S?-NP(NP(Head:Nb:??
)|VP(Head:VC:?| NP(Head:Na:?
)))))Linguistic motivation: Constraint linear order of constituents.Mother (Mother-node): The pos of mother-node assigns to all daughter nodes.Example:S(NP-S(Head:Nh:?)|S?(Head:VF:?|S?(NP-S(Head:Nb:??
)|VP-S(Head:VC:?| NP-VP(Head:Na: ?
)))))Linguistic motivation: Constraint syntactic structures for daughter nodes.Head0/1 (Existence of phrasal head): If phrasal head exists in intermediate node, thenodes will be marked with feature 1; otherwise 0.Example:S(NP(Head:Nh:?
)|S?-1(Head:VF:?
|S?-0(NP(Head:Nb:??
)|VP(Head:VC:?| NP(Head:Na: ?
)))))Linguistic motivation: Enforce unique phrasal head in each phrase.Table 2.
Performance evaluations for different features(a)Binary rules without features (b)Binary+LeftSinica Snorama Textbook Sinica Sinorama TextbookRC-Type 95.632 94.026 94.479 95.074 93.823 94.464RC-Token 99.422 99.139 99.417 99.012 98.756 99.179LP 81.51 77.45 84.42 86.27 80.28 86.67LR 82.73 77.03 85.09 86.18 80.00 87.23LF 82.11 77.24 84.75 86.22 80.14 86.94BP 87.73 85.31 89.66 90.43 86.71 90.84BR 89.16 84.91 90.52 90.46 86.41 91.57BF 88.44 85.11 90.09 90.45 86.56 91.20(c)Binary+Head (d)Binary+MotherSinica Snorama Textbook Sinica Sinorama TextbookRC-Type 94.595 93.474 94.480 94.737 94.082 92.985RC-Token 98.919 98.740 99.215 98.919 98.628 98.857LP 83.68 77.96 85.52 81.87 78.00 83.77LR 83.75 77.83 86.10 82.83 76.95 84.58LF 83.71 77.90 85.81 82.35 77.47 84.17BP 89.49 85.29 90.17 87.85 85.44 88.47BR 89.59 85.15 90.91 88.84 84.66 89.57BF 89.54 85.22 90.54 88.34 85.05 89.01Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 183Each set of feature constraint added grammar is tested and evaluated.
Table 2shows the experimental results.
Since all features have their own linguistic motiva-tions, the result feature constrained grammars maintain high coverage and have im-proving grammar precision.
Therefore each feature more or less improves the parsingperformance and the feature of leftmost daughter node, which constrains the linearorder of constituents, is the most effective feature.
The Left-constraint-added gram-mar reduces grammar token-coverage very little and significantly increases label andbracket f-scores.It is shown that all linguistically-motivated features are more or less effective.
Theleftmost constitute feature, which constraints linear order of constituents, is the mosteffective feature.
The mother-node feature is the least effective feature, since syntacticstructures do not vary too much for each phrase type while playing different gram-matical functions in Chinese.Table 3.
Performances of grammars with different feature combinations(a) Binary+Left+Head1/0 (b) Binary+Left+HeadSinica Sinorama Textbook Sinica Sinorama TextbookRC-Type 94.887 93.745 94.381 92.879 91.853 92.324RC-Token 98.975 98.740 99.167 98.173 98.022 98.608LF 86.54 79.81 87.68 86.00 79.53 86.86BF 90.69 86.16 91.39 90.10 86.06 90.91LF-1 86.71 79.98 87.73 86.76 79.86 87.16BF-1 90.86 86.34 91.45 90.89 86.42 91.22Table 4.
Performances of the grammar with most feature constraintsBinary+Left+Head+Mother+Head1/0Sinica Sinorama TextbookRC-Type 90.709 90.460 90.538RC-Token 96.906 96.698 97.643LF 86.75 78.38 86.19BF 90.54 85.20 90.07LF-1 88.56 79.55 87.84BF-1 92.44 86.46 91.80Since all the above features are effective, we like to see the results of multi-featurecombinations.
Many different feature combinations were tested.
The experimentalresults show that none of the feature combinations outperform the binary grammarswith Left and Head1/0 features, even the grammar combining all features, as shown inthe Table 3 and 4.
Here LF-1 and BF-1 measure the label and bracket f-scores only onthe sentences with parsing results (i.e.
sentences failed of producing parsing resultsare ignored).
The results show that grammar with all feature constraints has better LF-1 and BF-1 scores, since the grammar has higher precision.
However the total per-formances, i.e.
Lf and BF scores, are not better than the simpler grammar with feature184 Y.-M. Hsieh, D.-C. Yang, and K.-J.
Chenconstraints of Left and Head1/0, since the higher precision grammar losses slight edgeon the grammar coverage.
The result clearly shows that tradeoffs do exist betweengrammar precision and coverage.
It also suggests that if a feature constraint can im-prove grammar precision a lot but also reduce grammar coverage a lot, it is better totreat such feature constraints as a soft constraint instead of hard constraint.
Probabilis-tic preference for such feature parameters will be a possible implementation of softconstraint.3.3   DiscussionsFeature constraints impose additional constraints between constituents for phrasestructures.
However different feature constraints serve for different functions andhave different feature assignment principles.
Some features serve for local constraints,such as Left, Head, and Head0/1.
Those features are only assigned at local intermedi-ate nodes.
Some features are designed for external effect such as Mother Feature,which is assigned to phrase nodes and their daughter intermediate nodes.
For in-stances, NP structures for subject usually are different from NP structures for objectin English sentences [10].
NP attached with Mother-feature can make the difference.NPS rules and NPVP rules will be derived each respectively from subject NP and ob-ject NP structures.
However such difference seems not very significant in Chinese.Therefore feature selection and assignment should be linguistically-motivated asshown in our experiments.In conclusion, linguistically-motivated features have better effects on parsing per-formances than arbitrarily selected features, since they increase grammar precision,but only reduce grammar coverage slightly.
The feature of leftmost daughter, whichconstraints linear order of constituents, is the most effective feature for parsing.
Othersub-categorization related features, such as mother node and head features, do notcontribute parsing F-scores very much.
Such features might be useful for purpose ofsentence generation instead of parsing.4   Adapt to Pos Errors Due to Automatic Pos TaggingPerfect testing data was used for the above experiments without considering wordsegmentation and pos tagging errors.
However in real life word segmentation and postagging errors will degenerate parsing performances.
The real parsing performancesof accepting input from automatic word segmentation and pos tagging system areshown in the Table 5.Table 5.
Parsing performances of inputs produced by the automatic word segmentation andpos taggingBinary+Left+Head1/0Sinica Sinorama TextbookLF 76.18 64.53 73.61BF 84.01 75.95 84.28Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 185The na?ve approach to overcome the pos tagging errors was to delay some of theambiguous pos resolution for words with lower confidence tagging scores and leaveparser to resolve the ambiguous pos until parsing stage.
The tagging confidence ofeach word is measured by the following value.Confidence value=)c(P)c(P)c(Pw,2w,1w,1+, where P(c1,w) and P(c2,w) are probabilitiesassigned by the tagging model for the best candidate c1,w and the second best candi-date c2,w.The experimental results, Table 6, show that delaying ambiguous pos resolutiondoes not improve parsing performances, since pos ambiguities increase structure am-biguities and the parser is not robust enough to select the best tagging sequence.
Thehigher confidence values mean that more words with lower confidence tagging willleave ambiguous pos tags and the results show the worse performances.
Charniak et al[3] experimented with using multiple tags per word as input to a treebank parser, andcame to a similar conclusion.Table 6.
Parsing performances for different confidence level of pos ambiguitiesConfidence value=0.5Sinica Sinorama TextbookLF 75.92 64.14 74.66BF 83.48 75.22 83.65Confidence value=0.8Sinica Sinorama TextbookLF 75.37 63.17 73.76BF 83.32 74.50 83.33Confidence value=1.0Sinica Sinorama TextbookLF 74.12 61.25 69.44BF 82.57 73.17 81.174.1   Blending GrammarsA new approach of grammar blending method was proposed to cope with pos taggingerrors.
The idea is to blend the original grammar with a newly extracted grammarderived from the Treebank in which pos categories are tagged by the automatic postagger.
The blended grammars contain the original rules and the extended rules due topos tagging errors.
A 5-fold cross-validation was applied on the testing data to tunethe blending weight between the original grammar and the error-adapted grammar.The experimental results show that the blended grammar of weights 8:2 between theoriginal grammar and error-adapted grammar achieves the best results.
It reducesabout 20%~30% parsing errors due to pos tagging errors, shown in the Table 7.
Thepure error-adapted grammar, i.e.
0:10 blending weight, does not improve the parsingperformance very much186 Y.-M. Hsieh, D.-C. Yang, and K.-J.
ChenTable 7.
Performances of the blended grammarsError-adapted grammar i.e.blending weight (0:10)Blending weight 8:2Sinica Sinirama Textbook Sinica Sinirama TextbookLF 75.99 66.16 71.92 78.04 66.49 74.69BF 85.65 77.89 85.04 86.06 77.82 85.915   Conclusion and Future ResearchesIn order to obtain a high precision and high coverage grammar, we proposed a modelto measure grammar coverage and designed a PCFG parser to measure efficiency ofthe grammar.
Grammar binarization method was proposed to generalize rules and toincrease the coverage of context-free grammars.
Linguistically-motivated featureconstraints were added into grammar rules to maintain grammar rule precision.
It isshown that the feature of leftmost daughter, which constraints linear order of constitu-ents, is the most effective feature.
Other sub-categorization related features, such asmother node and head features, do not contribute parsing F-scores very much.
Suchfeatures might be very useful for purpose of sentence generation instead of parsing.The best performed feature constraint binarized grammar increases the grammar cov-erage of the original grammar from 93% to 99% and bracketing F-score from 87% to91% in parsing moderate hard testing data.
To cope with error propagations due toword segmentation and part-of-speech tagging errors, a grammar blending methodwas proposed to adapt to such errors.
The blended grammar can reduce about 20~30%of parsing errors due to error assignment of a pos tagging system.In the future, we will study more effective way to resolve structure ambiguities.
Inparticular, consider the tradeoff effect between grammar coverage and precision.
Thebalance between soft constraints and hard constraints will be focus of our future re-searches.
In addition to rule probability, word association probability will be anotherpreference measure to resolve structure ambiguity, in particular for conjunctivestructures.AcknowledgementThis research was supported in part by National Science Council under a Center Ex-cellence Grant NSC 93-2752-E-001-001-PAE and National Digital Archives ProgramGrant NSC93-2422-H-001-0004.References1.
E. Charniak, and G. Carroll, ?Context-sensitive statistics for improved grammatical lan-guage models.?
In Proceedings of the 12th National Conference on Artificial Intelligence,AAAI Press, pp.
742-747, Seattle, WA, 1994,2.
E. Charniak, ?Treebank grammars.?
In Proceedings of the Thirteenth National Conferenceon Artificial Intelligence, pp.
1031-1036.
AAAI Press/MIT Press, 1996.Linguistically-Motivated Grammar Extraction, Generalization and Adaptation 1873.
E. Charniak, and G. Carroll, J. Adcock, A. Cassanda, Y. Gotoh, J. Katz, M. Littman, J.Mccann, "Taggers for Parsers", Artificial Intelligence, vol.
85, num.
1-2, 1996.4.
Feng-Yi Chen, Pi-Fang Tsai, Keh-Jiann Chen, and Huang, Chu-Ren, ?Sinica Treebank.
?Computational Linguistics and Chinese Language Processing, 4(2):87-103, 2000.5.
Keh-Jiann Chen and, Yu-Ming Hsieh, ?Chinese Treebanks and Grammar Extraction.?
theFirst International Joint Conference on Natural Language Processing (IJCNLP-04), March2004.6.
Michael Collins, ?Head-Driven Statistical Models for Natural Language parsing.?
Ph.D.thesis, Univ.
of Pennsylvania, 1999.7.
Yu-Ming Hsieh, Duen-Chi Yang and Keh-Jiann Chen, ?Grammar extraction, generaliza-tion and specialization.
( in Chinese)?Proceedings of ROCLING 2004.8.
Christopher D. Manning and Hinrich Schutze, ?Foundations of Statistical Natural Lan-guage Processing.?
the MIT Press, Cambridge, Massachusetts, 1999.9.
Mark Johnson, ?PCFG models of linguistic tree representations.?
Computational Linguis-tics, Vol.24, pp.613-632, 1998.10.
Dan Klein and Christopher D. Manning, ?Accurate Unlexicalized Parsing.?
Proceeding ofthe 4lst Annual Meeting of the Association for Computational Linguistics, pp.
423-430,July 2003.11.
Honglin Sun and Daniel Jurafsky, ?Shallow Semantic Parsing of Chinese.?
Proceedings ofNAACL 2004.12.
12.Hao Zhang, Qun Liu, Kevin Zhang, Gang Zou and Shuo Bai, ?Statistical ChineseParser ICTPROP.?
Technology Report, Institute of Computing Technology, 2003.
