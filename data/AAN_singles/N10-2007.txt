Proceedings of the NAACL HLT 2010: Demonstration Session, pages 25?28,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAn Interactive Tool for Supporting Error Analysis for Text MiningElijah MayfieldLanguage Technologies InstituteCarnegie Mellon University5000 Forbes AvePittsburgh, PA 15216, USAelijah@cmu.eduCarolyn Penstein-Rose?Language Technologies InstituteCarnegie Mellon University5000 Forbes AvePittsburgh, PA 15216, USAcprose@cs.cmu.eduAbstractThis demo abstract presents an interactive toolfor supporting error analysis for text mining,which is situated within the SummarizationIntegrated Development Environment (SIDE).This freely downloadable tool was designedbased on repeated experience teaching textmining over a number of years, and has beensuccessfully tested in that context as a tool forstudents to use in conjunction with machinelearning projects.1 IntroductionIn the past decade, more and more work in thelanguage technologies community has shifted fromwork on formal, rule-based methods to work involv-ing some form of text categorization or text miningtechnology.
At the same time, use of this technologyhas expanded; where it was once accessible only tothose within studying core language technologies,it is now almost ubiquitous.
Papers involving textmining can currently be found even in core socialscience and humanities conferences.The authors of this demonstration are involvedin regular teaching of an applied machine learningcourse, which attracts students from virtually everyfield, including a variety of computer science relatedfields, the humanities and social sciences, and thearts.
In five years of teaching this course, what hasemerged is the finding that the hardest skill to impartto students is the ability to do a good error analysis.In response to this issue, the interactive error analy-sis tool presented here was designed, developed, andsuccessfully tested with students.In the remainder of this demo abstract, we offer anoverview of the development environment that pro-vides the context for this work.
We then describeon a conceptual level the error analysis process thatthe tool seeks to support.
Next, we step through theprocess of conducting an error analysis with the in-terface.
We conclude with some directions for ourcontinued work, based on observation of students?use of this interface.2 Overview of SIDEThe interactive error analysis interface is situatedwithin an integrated development environment forbuilding summarization systems.
Note that theSIDE (Kang et al, 2008) software and comprehen-sive user?s manual are freely available for down-load1.
We will first discuss the design of SIDE froma theoretical standpoint, and then explore the detailsof practical implementation.2.1 Design GoalsSIDE was designed with the idea that documents,whether they are logs of chat discussions, sets ofposts to a discussion board, or notes taken in acourse, can be considered relatively unstructured.Nevertheless, when one thinks about their interpre-tation of a document, or how they would use the in-formation found within a document, then a structureemerges.
For example, an argument written in a pa-per often begins with a thesis statement, followed bysupporting points, and finally a conclusion.
A reader1SIDE and its documentation are downloadable fromhttp://www.cs.cmu.edu/?cprose/SIDE.html25can identify with this structure even if there is noth-ing in the layout of the text that indicates that certainsentences within the argument have a different sta-tus from the others.
Subtle cues in the language canbe used to identify those distinct roles that sentencesmight play.Conceptually, then, the use of SIDE proceeds intwo main parts.
The first part is to construct filtersthat can impose that structure on the texts to be sum-marized, to identify the role a sentence is playingin a document; and the second part is constructingspecifications of summaries that refer to that struc-ture, such as subsets of extracted text or data visu-alizations.
This demo is primarily concerned withsupporting error analysis for text mining.
Thus, thefirst of these two stages will be the primary focus.This approach to summarization was inspired bythe process described in (Teufel and Moens, 2002).That work focused on the summarization of scien-tific articles to describe a new work in a way whichrhetorically situates that work?s contribution withinthe context of related prior work.
This is done byfirst overlaying structure onto the documents to besummarized, categorizing the sentences they containinto one of a number of rhetorical functions.
Oncethis structure is imposed, using the information itprovides was shown to increase the quality of gener-ated summaries.2.2 Building Text Mining Models with SIDEThis demo assumes the user has already interactedwith the SIDE text mining interface for model build-ing, including feature extraction and machine learn-ing, to set up a model.
Defining this in SIDE terms,to train the system and create a model, the user firsthas to define a filter.
Filters are trained using ma-chine learning technology.
Two customization op-tions are available to analysts in this process.The first and possibly most important is the set ofcustomization options that affect the design of theattribute space.
The standard attribute space is setup with one attribute per unique feature - the valuecorresponds to the number of times that feature oc-curs in a text.
Options include unigrams, bigrams,part-of-speech bigrams, stemming, and stopword re-moval.The next step is the selection of the machinelearning algorithm that will be used.
Dozens of op-tions are made available through the Weka toolkit(Witten and Frank, 2005), although some are morecommonly used than others.
The three options thatare most recommended to analysts beginning workwith machine learning are Na?
?ve Bayes (a prob-abilistic model), SMO (Weka?s implementation ofSupport Vector Machines), and J48, which is oneof many Weka implementations of a Decision Treelearner.
SMO is considered state-of-the-art for textclassification, so we expect that analysts will fre-quently find that to be the best choice.As this error analysis tool is built within SIDE, wefocus on applications to text mining.
However, thistool can also be used on non-text data sets, so long asthey are first preprocessed through SIDE.
The detailsof our error analysis approach are not specific to anyindividual task or machine learning algorithm.3 High Level View of Error AnalysisIn an insightful usage of applied machine learning, apractitioner will design an approach that takes intoaccount what is known about the structure of thedata that is being modeled.
However, typically, thatknowledge is incomplete, and there is thus a goodchance that the decisions that are made along theway are suboptimal.
When the approach is evalu-ated, it is possible to determine based on the pro-portion and types of errors whether the performanceis acceptable for the application or not.
If it is not,then the practitioner should engage in an error analy-sis process to determine what is malfunctioning andwhat could be done to better model the structure inthe data.In well-known machine learning toolkits such asWeka, some information is available about what er-rors are being made.
Predictions can be printed out,to allow a researcher to identify how a document isbeing classified.
One common format for summariz-ing these predictions is a confusion matrix, usuallyprinted in a format like:a b <-- classified as67 19 | a = PT42 70 | b = DRThis lists, for example, that 19 text segments wereclassified as type DR but were actually type PT.While this gives a rough view of what errors are26Figure 1: The error analysis interface with key function-ality locations highlighted.appearing, it gives no indication of why the errorsare being made.
This is where a more extensive er-ror analysis is necessary.
Two common ways to ap-proach this question are top down, which starts witha learned model, and bottom up, which starts withthe confusion matrix from that model?s performanceestimate.
In the first case, the model is examinedto find the attributes that are treated as most impor-tant.
These are the attributes that have the great-est influence on the predictions made by the learnedmodel, and thus these attributes provide a good start-ing point.
In the second case, the bottom-up case,one first examines the confusion matrix to identifylarge off-diagonal cells, which represent commonconfusions.
The error analysis for any error cell isthen the process of determining relations betweenthree sets of text segments2 related to that cell.Within the ?classified as DR but actually PT?
cell,for instance, error analysis would require findingwhat makes these examples most different from ex-amples correctly classified as PT, and what makesthese examples most similar to those correctly clas-sified as DR.
This can be done by identifying at-tributes that mostly strongly differentiate the firsttwo sets, and attributes most similar between the firstand third sets.
An ideal approach would combinethese two directions.4 Error Analysis ProcessVisitors to this demo will have the opportunity to ex-periment with the error analysis interface.
It will beset up with multiple data sets and previously trainedtext mining models.
These models can first be exam-ined from the model building window, which con-tains information such as:?
Global feature collection, listing all featuresthat were included in the trained model.?
Cross-validation statistics, including varianceand kappa statistics, the confusion matrix andother general information.?
Weights or other appropriate information forthe text mining model that was trained.By moving to the error analysis interface, the usercan explore a model more deeply.
The first step isto select a model to examine.
By default, all textsegments that were evaluated in cross-validation dis-play in a scrolling list in the bottom right corner ofthe window.
Each row contains the text within a seg-ment, and the associated feature vector.
Users willfirst be asked to examine this data to understand themagnitude of the error analysis challenge.Clicking on a cell in the confusion matrix (at thetop of the screen) will fill the scrolling list at the bot-tom left corner of the screen with the classified seg-ments that fall in that cell.
A comparison chooserdropdown menu gives three analysis options - full,horizontal, and vertical.
By default, full comparison2Our interface assumes that the input text has been seg-mented already; depending on the task involved, these segmentsmay correspond to a sentence, a paragraph, or even an entiredocument.27is selected, and shows all text segments used in train-ing.
The two additional modes of comparison allowsome insight into what features are most representa-tive of the subset of segments in that cell, comparedto the correct predictions aligned with that cell (ei-ther vertically or horizontally within the confusionmatrix).
By switching to horizontal comparison, thescrolling list on the right changes to display only textsegments that fall in the cell which is along the con-fusion matrix diagonal and horizontal to the selectedcell.
Switching to vertical comparison changes thislist to display segments categorized in the cell whichis along the diagonal and vertically aligned with theselected error cell.Once a comparison method is selected, there isa feature highlighting dropdown menu which is ofuse.
The contents in this menu are sorted by degreeof difference between the segments in the two listsat the bottom of the screen.
This means, for a hor-izontal comparison, that features at the top of thislist are the most different between the two cells (thisdifference is displayed in the menu).
We computethis difference by the difference in expected (aver-age) value for that feature between the two sets.
In avertical comparison, features are ranked by similar-ity, instead of difference.
Once a feature is selectedfrom this menu, two significant changes are made.The first is that a second confusion matrix appears,giving the confusion matrix values (mean and stan-dard deviation) for the highlighted feature.
The sec-ond is that the two segment lists are sorted accordingto the feature being highlighted.User interface design elements were important inthis design process.
One option available to users isthe ability to ?hide empty features,?
which removesfeatures which did not occur at all in one or both ofthe sets being studied.
This allows the user to fo-cus on features which are most likely to be causinga significant change in a classifier?s performance.
Itis also clear that the number of different subsets ofclassified segments can become very confusing, es-pecially when comparing various types of error inone session.
To combat this, the labels on the listsand menus will change to reflect some of this infor-mation.
For instance, the left-hand panel gives thepredicted and actual labels of the segments you havehighlighted, while the right-hand panel is labelledwith the name of the category of correct predictionyou are comparing against.
The feature highlightingdropdown menu also changes to reflect similar in-formation about the type of comparison being made.5 Future DirectionsThis error analysis tool has been used in the textmining unit for an Applied Machine Learning coursewith approximately 30 students.
In contrast to pre-vious semesters where the tool was not availableto support error analysis, the instructor noticed thatmany more students were able to begin surpassingshallow observations, instead forming hypothesesabout where the weaknesses in a model are, andwhat might be done to improve performance.Based on our observations, however, the erroranalysis support could still be improved by directingusers towards features that not only point to differ-ences and similarities between different subsets ofinstances, but also to more information about howfeatures are being used in the trained model.
Thiscan be implemented either in algorithm-specificways (such as displaying the weight of features inan SVM model) or in more generalizable formats,for instance, through information gain.
Investigatinghow to score these general aspects, and presentingthis information in an intuitive way, are directionsfor our continued development of this tool.AcknowledgementsThis research was supported by NSF Grant DRL-0835426.ReferencesMoonyoung Kang, Sourish Chaudhuri, Mahesh Joshi,and Carolyn Penstein-Rose?
2008.
SIDE: The Summa-rization Integrated Development Environment.
Pro-ceedings of the Association for Computational Lin-guistics, Demo Abstracts.Simone Teufel and Marc Moens 2002.
SummarizingScientific Articles: Experiments with Relevance andRhetorical Status.
Computational Linguistics, Vol.
28,No.
1.Ian Witten and Eibe Frank 2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques, secondedition.
Elsevier: San Fransisco.28
