Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 585?594,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Semiparametric Model for Bayesian Reader IdentificationAhmed Abdelwahab1 and Reinhold Kliegl2 and Niels Landwehr11 Department of Computer Science, Universita?t PotsdamAugust-Bebel-Stra?e 89, 14482 Potsdam, Germany{ahmed.abdelwahab, niels.landwehr}@uni-potsdam.de2 Department of Psychology, Universita?t PotsdamKarl-Liebknecht-Stra?e 24/25, 14476 Potsdam OT/Golmkliegl@uni-potsdam.deAbstractWe study the problem of identifying individu-als based on their characteristic gaze patternsduring reading of arbitrary text.
The motiva-tion for this problem is an unobtrusive biomet-ric setting in which a user is observed duringaccess to a document, but no specific chal-lenge protocol requiring the user?s time and at-tention is carried out.
Existing models of indi-vidual differences in gaze control during read-ing are either based on simple aggregate fea-tures of eye movements, or rely on paramet-ric density models to describe, for instance,saccade amplitudes or word fixation durations.We develop flexible semiparametric models ofeye movements during reading in which den-sities are inferred under a Gaussian processprior centered at a parametric distribution fam-ily that is expected to approximate the true dis-tribution well.
An empirical study on read-ing data from 251 individuals shows signifi-cant improvements over the state of the art.1 IntroductionEye-movement patterns during skilled reading con-sist of brief fixations of individual words in atext that are interleaved with quick eye movementscalled saccades that change the point of fixation toanother word.
Eye movements are driven both bylow-level visual cues and high-level linguistic andcognitive processes related to text understanding; asa reflection of the interplay between vision, cog-nition, and motor control during reading they arefrequently studied in cognitive psychology (Klieglet al, 2006; Rayner, 1998).
Computational mod-els (Engbert et al, 2005; Reichle et al, 1998) as wellas models based on machine learning (Matties andS?gaard, 2013; Hara et al, 2012) have been devel-oped to study how gaze patterns arise based on textcontent and structure, facilitating the understandingof human reading processes.A central observation in these and earlier psycho-logical studies (Huey, 1908; Dixon, 1951) is that eyemovement patterns strongly differ between individu-als.
Holland et al (2012) and Landwehr et al (2014)have developed models of individual differences ineye movement patterns during reading, and studiedthese models in a biometric problem setting wherean individual has to be identified based on observingher eye movement patterns while reading arbitrarytext.
Using eye movements during reading as a bio-metric feature has the advantage that it suffices toobserve a user during a routine access to a deviceor document, without requiring the user to react toa specific challenge protocol.
If the observed eyemovement sequence is unlikely to be generated byan authorized individual, access can be terminated oran additional verification requested.
This is in con-trast to approaches where biometric identification isbased on eye movements in response to an artificialvisual stimulus, for example a moving (Kasprowskiand Ober, 2004; Komogortsev et al, 2010; Rigas etal., 2012b; Zhang and Juhola, 2012) or fixed (Bed-narik et al, 2005) dot on a computer screen, or aspecific image stimulus (Rigas et al, 2012a).The model studied by Holland & Komogort-sev (2012) uses aggregate features (such as averagefixation duration) of the observed eye movements.Landwehr et al (2014) showed that readers can beidentified more accurately with a model that cap-tures aspects of individual-specific distributions over585eye movements, such as the distribution over fixa-tion durations or saccade amplitudes for word refix-ations, regressions, or next-word movements.
Someof these distributions need to be estimated from veryfew observations; a key challenge is thus to designmodels that are flexible enough to capture character-istic differences between readers yet robust to sparsedata.
Landwehr et al (2014) used a fully paramet-ric approach where all densities are assumed to be inthe gamma family; gamma distributions were shownto approximate the true distribution of interest wellfor most cases (see Figure 1).
This model is robustto sparse data, but might not be flexible enough tocapture all differences between readers.The model we study in this paper follows ideasdeveloped by Landwehr et al (2014), but em-ploys more flexible semiparametric density models.Specifically, we place a Gaussian process prior overdensities that concentrates probability mass on den-sities that are close to the gamma family.
Givendata, a posterior distribution over densities is de-rived.
If data is sparse, the posterior will still besharply peaked around distributions in the gammafamily, reducing the effective capacity of the modeland minimizing overfitting.
However, given enoughevidence in the data, the model will also deviatefrom the gamma-centered prior?depending on thekernel function chosen for the GP prior, any densityfunction can in principle be represented.
Integratingover the space of densities weighted by the posterioryields a marginal likelihood for novel observationsfrom which predictions are inferred.
We empiricallystudy this model in the same setting as studied byLandwehr et al (2014), but using an order of mag-nitude more individuals.
Identification error is re-duced by more than a factor of three compared tothe state of the art.The rest of the paper is organized as follows.After defining the problem setting in Section 2,Section 3 presents the semiparametric probabilis-tic model.
Section 4 discusses inference, Section 5presents an empirical study on reader identification.2 Problem SettingAssume R different readers, indexed by r ?
{1, .
.
.
, R}, and letX = {X1, .
.
.
,Xn} denote a setof texts.
Each r ?
R generates a set of eye move-ment patterns S(r) = {S(r)1 , .
.
.
,S(r)n } on X , byS(r)i ?
p(S|Xi, r,?
)where p(S|Xi, r,?)
is a reader-specific distributionover eye movement patterns given a text Xi.
Here,r is a variable indicating the reader generating thesequence, and ?
is a true but unknown model thatdefines all reader-specific distributions.
We assumethat ?
can be broken down into reader-specific mod-els, ?
= (?1, .
.
.
,?k), such that the distributionp(S|Xi, r,?)
= p(S|Xi,?r) (1)is defined by the partial model ?r.
We aggregate theobservations of all readers on the training data into avariable S(1:R) = (S(1), .
.
.
,S(R)).We follow a Bayesian approach, defining a priorp(?)
over the joint model that factorizes into priorsover reader-specific models, p(?)
= ?Rr=1 p(?r).At test time, we observe novel eye movementpatterns S?
= {S?1, .
.
.
, S?m} on a novel set oftexts X?
= {X?1, .
.
.
, X?m} generated by an unknownreader r ?
R. We assume a uniform prior overreaders, that is, each r ?
R is equally likely to beobserved at test time.
The goal is to infer the mostlikely reader to have generated the novel eye move-ment patterns.
In a Bayesian setting, this means in-ferring the most likely reader given the training ob-servations (X ,S(1:R)) and test observation (X?
, S?):r?
= arg maxr?R p(r|X?
, S?,X ,S(1:R)).
(2)We can rewrite Equation 2 tor?
= arg maxr?R p(S?|r, X?
,X ,S(1:R)) (3)= arg maxr?R?p(S?|r, X?
,?
)p(?|X ,S(1:R))d?= arg maxr?R?p(S?|X?
,?r)p(?r|X ,S(r))d?r (4)wherep(S?|X?
,?r) =m?i=1p(S?i|X?i,?r) (5)p(?r|X ,S(r)) ?
p(?r)n?i=1p(S(r)i |Xi,?r).
(6)586In Equation 3 we exploit that readers are uniformlychosen at test time, and in Equation 4 we exploitthe factorization p(?)
= ?Rr=1 p(?r) of the prior,which together with Equation 1 entails a factoriza-tion p(?|X ,S(1:R)) = ?Rr=1 p(?r|X ,S(r)) of theposterior.
Note that Equation 4 states that at testtime we predict the reader r for which the marginallikelihood (that is, after integrating out the reader-specific model ?r) of the test observations is high-est.
The next section discusses the reader-specificmodels p(S|X,?r) and prior distributions p(?r).3 Probabilistic ModelThe probabilistic model we employ follows the gen-eral structure proposed by Landwehr et al (2014),but employs semiparametric density models and al-lows for fully Bayesian inference.
To reduce nota-tional clutter, let ?
?
{?1, .
.
.
,?R} denote a par-ticular reader-specific model, and let X ?
X de-note a text.
An eye movement pattern is a sequenceS = ((s1, d1), .
.
.
, (sT , dT )) of gaze fixations, con-sisting of a fixation position st (position in text thatwas fixated) and duration dt ?
R (length of fixationin milliseconds).
In our experiments, individual sen-tences are presented in a single line on screen, thuswe only model a horizontal gaze position st ?
R.We model p(S|X,?)
as a dynamic process that suc-cessively generates fixation positions st and dura-tions dt in S, reflecting how a reader generates a se-quence of saccades in response to a text stimulus X:p(S|X,?)
= p(s1, d1|X,?
)T?t=2p(st, dt|st?1,X,?
),where p(st, dt|st?1,X,?)
models the generation ofthe next fixation position and duration given the oldfixation position st?1.
In the psychological litera-ture, four different saccade types are distinguished:a reader can refixate the current word (refixation),fixate the next word in the text (next word move-ment), move the fixation to a word after the nextword, that is, skip one or more words (forward skip),or regress to fixate a word occurring earlier in thetext (regression), see, e.g., Heister et al (2012).We observe empirically that for each saccade type,there is a characteristic distribution over saccade am-plitudes and fixation durations, and that both ap-proximately follow gamma distributions?see Fig-?20 ?10 0 10 2000.10.20.30.40.5DensityAmplitudeRefixationEmpirical DistributionSemiparametric FitGamma Fit?20 ?10 0 10 2000.050.10.150.20.25DensityAmplitudeNext Word MoveEmpirical DistributionSemiparametric FitGamma Fit?20 ?10 0 10 2000.050.10.150.2DensityAmplitudeForward SkipEmpirical DistributionSemiparametric FitGamma Fit?20 ?10 0 10 2000.050.10.150.20.250.30.35DensityAmplitudeRegressionEmpirical DistributionSemiparametric FitGamma FitFigure 1: Empirical distributions of saccade amplitudes intraining data for first individual, with fitted Gamma distribu-tions and semiparametric distribution fits.ure 1.
We therefore model p(st, dt|st?1,X,?)
us-ing a mixture over distributions for the four differentsaccade types.
At each time t, the model first drawsa saccade type ut ?
{1, 2, 3, 4}, and then draws asaccade amplitude at and fixation duration dt fromtype-specific distributions p(a|ut, st?1,X,?)
andp(d|ut,?).
More formally,ut ?
p(u|pi) (7)at ?
p(a|ut, st?1,X,?)
(8)dt ?
p(d|ut, ?
), (9)where ?
= (pi,?, ?)
is decomposed into compo-nents pi, ?, and ?.
Afterwards, the model updatesthe fixation position according to st = st?1 + at,concluding the definition of p(st, dt|st?1,X,?
).Figure 2 shows a slice in the dynamical model.The distribution p(u|pi) over saccade types(Equation 7) is multinomial with parameter vectorpi ?
R4.
The distributions over amplitudes and du-rations (Equations 8 and 9) are modeled semipara-metrically as discussed in the following subsections.3.1 Model of Saccade AmplitudesWe first discuss the amplitude modelp(a|ut, st?1,X,?)
(Equation 8).
We first de-fine a distribution p(a|ut,?)
over amplitudes forsaccade type ut, and subsequently discuss condi-tioning on the text X and old fixation position st?1,587tu 1tu ?ta 1ta ?td 1td ?t 1tXts 1ts ???
?Figure 2: Plate notation of of a slice in the dynamic model.leading to p(a|ut, st?1,X,?).
We definep(a|ut = 1,?)
={?
?1(a) : a > 0(1?
?)?
?1(?a) : a ?
0(10)where ?
is a mixture weight and ?1, ?
?1 are densitiesdefining the distribution over positive and negativeamplitudes for the saccade type refixation, andp(a|ut = 2,?)
= ?2(a) (11)p(a|ut = 3,?)
= ?3(a) (12)p(a|ut = 4,?)
= ?4(?a) (13)where ?2(a), ?3(a), and ?4(a) are densities defin-ing the distribution over amplitudes for the remain-ing saccade types.
Finally, the distributionp(s1|X,?)
= ?0(s1) (14)over the initial fixation position is given by anotherdensity function ?0.
The variables ?, ?0, ?1, ?
?1,?2, ?3, and ?4 are aggregated into model compo-nent ?.
For resolving the most likely reader at testtime (Equation 4), densities in ?
will be integratedout under a prior based on Gaussian processes (Sec-tion 3.3) using MCMC inference (Section 4).Given the old fixation position st?1, the text X,and the chosen saccade type ut, the amplitude isconstrained to fall within a specific interval.
For in-stance, for a refixation the amplitude has to be cho-sen such that the novel fixation position lies withinthe beginning and the end of the currently fixatedword; a regression implies an amplitude that is neg-ative and makes the novel fixation position lie be-fore the beginning of the currently fixated word.These constraints imposed by the text structure de-fine the conditional distribution p(a|ut, st?1,X,?
).More formally, p(a|ut, st?1,X,?)
is the distribu-tion p(a|ut,?)
conditioned on a ?
[l, r], that is,p(a|ut, st?1,X,?)
= p(a|a ?
[l, r], ut,?
),where l and r are the minimum and maximum am-plitude consistent with the constraints.
Recall thatfor a distribution over a continuous variable x givenby density ?
(x), the distribution over x conditionedon x ?
[l, r] is given by the truncated density?
(x|x ?
[l, r]) ={ ?(x)?
rl ?(x?)dx?
: x ?
[l, r]0 : x /?
[l, r].
(15)We derive p(a|ut, st?1,X,?)
by truncating the dis-tributions given by Equations 10 to 13 to the min-imum and maximum amplitude consistent with thecurrent fixation position st?1 and text X.
Let w?l(w?r ) denote the position of the left-most (right-most) character of the currently fixated word, andlet w+l , w+r denote these positions for the next wordin X.
Let furthermore l?
= w?l ?
st?1, r?
= w?r ?st?1, l+ = w+l ?
st?1, and r+ = w+r ?
st?1.
Thenp(a|ut = 1, st?1,X,?)
={?
?1(a|a ?
[0, r?])
: a > 0(1?
?)?
?1(?a|a ?
[l?, 0]) : a ?
0(16)p(a|ut = 2, st?1,X,?)
=?2(a|a?
[l+, r+]) (17)p(a|ut = 3, st?1,X,?)
=?3(a|a?
(r+,?))
(18)p(a|ut = 4, st?1,X,?)
=?4(?a|a?
(?
?, l?
))(19)defines the appropriately truncated distributions.3.2 Model of Fixation DurationsThe model for fixation durations (Equation 9) is sim-ilarly specified by saccade type-specific densities,p(d|ut = u, ?)
= ?u(d) for u ?
{1, 2, 3, 4} (20)and a density for the initial fixation durationsp(d1|X, ?)
= ?0(d1) (21)where ?0, ..., ?4 are aggregated into model compo-nent ?.
Unlike saccade amplitude, the fixation du-ration is not constrained by the text structure andaccordingly densities are not truncated.
This con-cludes the definition of the model p(S|X,?
).5883.3 Prior DistributionsThe prior distribution over the entire model ?
fac-torizes over the model components asp(?|?, ?, ?)
= (22)p(pi|?)p(?|?)p(??1|?)4?i=0p(?i|?)4?i=0p(?i|?
)where p(pi) = Dir(pi|?)
is a symmetric Dirich-let prior and p(?)
= Beta(?|?)
is a Beta prior.The key challenge is to develop appropriate pri-ors for the densities defining saccade amplitude(p(??1|?
), p(?i|?))
and fixation duration (p(?i|?))distributions.
Empirically, we observe that ampli-tude and duration distributions tend to be close togamma distributions?see the example in Figure 1.Our goal is to exploit the prior knowledge thatdistributions tend to be closely approximated bygamma distributions, but allow the model to devi-ate from the gamma assumption in case there isenough evidence in the data.
To this end, we de-fine a prior over densities that concentrates probabil-ity mass around the gamma family.
For all densitiesf ?
{?
?1, ?0, ..., ?4, ?0, ..., ?4}, we employ identicalprior distributions p(f |?).
Intuitively, the prior isgiven by first drawing a density function from thegamma family and then drawing the final densityfrom a Gaussian process (with covariance function?)
centered at this function.
More formally, letG(x|?)
= exp(?Tu(x))?exp(?Tu(x?))dx?
(23)denote the gamma distribution in exponential familyform, with sufficient statistics u(x) = (log(x), x)Tand parameters ?
= (?1, ?2).
Let p(?)
denote aprior over the gamma parameters, and definep(f |?)
=?p(?
)p(f |?, ?)d?
(24)where p(f |?, ?)
is given by drawingg ?
GP(0, ?)
(25)from a Gaussian process prior GP(0, ?)
with meanzero and covariance function ?, and lettingf(x) = exp(?Tu(x) + g(x))?exp(?Tu(x?)
+ g(x?))dx?
.
(26)Note that decreasing the variance of the Gaussianprocess means regularizing g(x) towards zero, andtherefore Equation 26 towards Equation 23.
Thisconcludes the specification of the prior p(?|?, ?, ?
).The density model defined by Equations 24 to 26draws on ideas from the large body of literatureon GP-based density estimation, for example byAdams et al (2009), Leonard (1978), or Tokdar etal.
(2010), and semiparametric density estimation,e.g.
as discussed by Yang (2009), Lenk (2003) orHjort & Glad (1995).
However, note that existingdensity estimation approaches are not applicable off-the-shelf as in our domain distributions are truncateddifferently at each observation due to constraints thatarise from the way eye movements interact with thetext structure (Equations 16 to 19).4 InferenceTo solve Equation 4, we need to integrate for eachr ?
R over the reader-specific model ?r.
To reducenotational clutter, let ?
?
{?1, .
.
.
,?R} denote areader-specific model, and let S ?
{S(1), .
.
.
,S(R)}denote the eye movement observations of that readeron the training texts X .
We approximate?p(S?|X?
,?
)p(?|X ,S)d?
?
1KK?k=1p(S?|X?
,?
(k))by a sample ?
(1), .
.
.
,?
(K) of models drawn by?
(k) ?
p(?|X ,S, ?, ?, ?
),where p(?|X ,S, ?, ?, ?)
is the posterior as given byEquation 6 but with the dependence on the prior hy-perparameters ?, ?, ?
made explicit.
Note that withX and S, all saccade types ut are observed.
Togetherwith the factorizing prior (Equation 22), this meansthat the posterior factorizes according top(?|X ,S, ?, ?, ?)
= p(pi|X ,S, ?
)p(?|X ,S, ?)?
p(?
?1|X ,S, ?
)4?i=0p(?i|X ,S, ?
)4?i=0p(?i|X ,S, ?
)as is easily seen from the graphical model in Fig-ure 2.
Obtaining samples pi(k) ?
p(pi|X ,S)and ?
(k) ?
p(?|X ,S) is straightforward becausetheir prior distributions are conjugate to the likeli-hood terms.
Let now f ?
{?
?1, ?0, ..., ?4, ?0, ..., ?4}589denote a particular density in the model.
Theposterior p(f |X ,S, ?)
is proportional to the priorp(f |?)
(Equation 24) multiplied by the likeli-hood of all observations that are generated bythis density, that is, that are generated accord-ing to Equation 14, 16, 17, 18, 19, 20, or 21.Let y = (y1, .
.
.
, y|y|)T ?
R|y| denote the vector ofall observations generated from density f , and letl = (l1, .
.
.
, l|l|)T ?
R|l|, r = (r1, .
.
.
, r|r|)T ?
R|r|denote the corresponding left and right boundariesof the truncation intervals (again see Equations 14to 21), where for densities that are not truncated wetake li = 0 and ri =?
throughout.
Then the likeli-hood of the observations generated from f isp(y|f, l, r) =|y|?i=1f(yi|yi ?
[li, ri]) (27)and the posterior over f is given byp(f |X ,S, ?)
?
p(f |?
)p(y|f, l, r).
(28)Note that y, l and r are observable from X , S.We obtain samples from the posterior given byEquation 28 from a Metropolis-Hastings samplerthat explores the space of densities f : R ?
R,generating density samples f (1), ..., f (K).
A densityf is given by a combination of gamma parameters?
?
R2 and function g : R ?
R; specifically, f isobtained by multiplying the gamma distribution withparameters ?
by exp(g) and normalizing appropri-ately (Equation 26).
During sampling, we explicitlyrepresent a density sample f (k) by its gamma param-eters ?
(k) and function g(k).
The proposal distribu-tion of the Metropolis-Hastings sampler isq(?
(k+1), g(k+1)|?
(k), g(k)) =p(g(k+1)|?
)N (?(k+1)|?
(k), ?2I)where p(g(k+1)|?)
is the probability of g(k+1) ac-cording to the GP prior GP(0, ?)
(Equation 25),and N (?(k+1)|?
(k), ?2I) is a symmetric proposalthat randomly perturbs the old state ?
(k) accord-ing to a Gaussian.
In every iteration k a proposal?
?, g?
?
q(?, g|?
(k), g(k)) is drawn based on theold state (?
(k), g(k)).
The acceptance probability isA(?
?, g?|?
(k), g(k)) = min(1, Q) withQ =q(?
(k), g(k)|?
?, g?)p(??)p(g?|?
)p(y|f?, l, r)q(?
?, g?|?
(k), g(k))p(?(k))p(g(k)|?
), p(y|f (k), l, r) .Here, p(??)
is the prior probability of gamma pa-rameters ??
(Section 3.3) and p(y|f?, l, r) is givenby Equation 27 where f?
is obtained from ?
?, g?according to Equation 26.To compute the likelihood terms p(y|f (k), l, r)(Equation 27) and also to compute the likelihoodof test data under a model (Equation 5), the den-sity f : R ?
R needs to be evaluated.
Accord-ing to Equation 26, f is represented by parame-ter vector ?
together with the nonparametric func-tion g : R ?
R. As usual when working withdistributions over functions in a Gaussian processframework, the function g only needs to be repre-sented at those points for which we need to evalu-ate it.
Clearly, this includes all observations of sac-cade amplitudes and fixation durations observed inthe training and test set.
However, we also needto evaluate the normalizer in Equation 26, and (forf ?
{?1, ?
?1, ?2, ?3, ?4}) the additional normalizerrequired when truncating the distribution (see Equa-tion 15).
As these integrals are one-dimensional,they can be solved relatively accurately using nu-merical integration; we use 2-point Newton-Cotesquadrature.
Newton-Cotes integration requires theevaluation (and thus representation) of g at an addi-tional set of equally spaced supporting points.When the set of test observations S?, X?
is large,the need to evaluate p(S?|X?
,?
(k)) for all ?k and alltest observations leads to computational challenges.In our experiments, we use a heuristic to reducecomputational load.
While generating samples, den-sities are only represented at the training observa-tions and the supporting points needed for Newton-Cotes integration.
We then estimate the mean of theposterior by ??
= 1K?Kk=1 ?
(k), and approximate1K?Kk=1 p(S?|X?
,?
(k)) ?
p(S?|X?
, ??).
To evaluatep(S?|X?
, ??
), we infer the approximate value of thedensity ??
at a test observation by linearly interpo-lating based on the available density values at thetraining observations and supporting points.5 Empirical StudyWe conduct a large-scale study of biometric iden-tification performance using the same setup as dis-cussed by Landwehr et al (2014) but a much largerset of individuals (251 rather than 20).Eye movement records for 251 individuals are5900 0.2 0.4 0.6 0.8 100.10.20.30.40.50.60.70.80.91Fraction of test data usedAccuracySemiparametric Landwehr et al Landwehr et al (TA)0 50 100 150 200 25000.10.20.30.40.50.60.70.80.91Number of individuals RAccuracyLandwehr et al (T) Holland & K. (unweighted) Holland & K. (weighted)Figure 3: Multiclass accuracy over number of test observations (left) and number of individuals R (right) with standard errors.Method AccuracySemiparametric 0.9502 ?
0.0130Semiparametric (TD) 0.8853 ?
0.0142Semiparametric (TA) 0.7717 ?
0.0361Landwehr et al 0.8319 ?
0.0218Landwehr et al (TA) 0.5964 ?
0.0262Landwehr et al (T) 0.2749 ?
0.0369Holland & K. (unweighted) 0.6988 ?
0.0241Holland & K. (weighted) 0.4566 ?
0.0220Table 1: Multiclass identification accuracy ?
standard error.obtained from an EyeLink II system with a 500-Hz sampling rate (SR Research, Ongoode, Ontario,Canada) while reading sentences from the PotsdamSentence Corpus (Kliegl et al, 2006).
There are 144sentences in the corpus, which we split into equallysized sets of training and test sentences.
Individu-als read between 100 and 144 sentences, the training(testing) observations for one individual are the ob-servations on those sentences in the training (testing)set of sentences that the individual has read.
Resultsare averaged over 10 random train-test splits.
Eachsentence is shown as a single line on the screen.We study the semiparametric model discussed inSection 3 with MCMC inference as presented inSection 4 (denoted Semiparametric1).
We employ asquared exponential covariance function ?
(x, x?)
=?
exp(??x?x?
?22?2), where the multiplicative con-stant ?
is tuned on the training data by cross-1An implementation is available at github.com/abdelwahab/SemiparametricIdentificationvalidation and the bandwidth ?
is set to the av-erage distance between points in the training data.The Beta and Dirichlet parameters ?
and ?
areset to one (Laplace smoothing), the prior p(?
)for the Gamma parameters is uninformative.
Weuse backoff-smoothing as discussed by Landwehret al (2014).
We initialize the sampler with themaximum-likelihood Gamma fit and perform 10000sampling iterations, 5000 of which are burn-in it-erations.
As a baseline, we study the model byLandwehr et al (2014) (Landwehr et al) and sim-plified versions proposed by them that only use sac-cade type and amplitude (Landwehr et al (TA) ) orsaccade type (Landwehr et al (T) ).
We also studythe weighted and unweighted version of the feature-based model of Holland & Komogortsev (2012) witha feature set adapted to the Potsdam Sentence Cor-pus data as described in Landwehr et al (2014).We note that there are two recent extensions of thefeature-based model (by Rigas et al (2016) and Ab-dulin & Komogortsev (2015)) that are unfortunatelynot applicable in our empirical setting but mightyield improved results in other scenarios.
Rigas etal.
(2016) study a model that is focused on repre-senting reader-specific differences in saccadic vigorand acceleration, which are both derived from thedynamics of saccadic velocity.
In the preprocesseddata set that we use, saccadic velocities are not avail-able, therefore we do not make use of velocities inour model and cannot easily compare against theirmodel.
Abdulin & Komogortsev (2015) study amodel that is based on features that relate eye move-5910 0.02 0.04 0.06 0.08 0.1?0.0100.010.020.030.040.050.06False rejectFalse acceptSemiparametricLandwehr et alLandwehr et al (TA)Landwehr et al (T)Holland & K. (unweighted)Holland & K. (weighted)Figure 4: False-accept over false-reject rate when varying ?
.ments to the 2D text structure, that is, to the waywords are arranged into lines in a text.
As in ourempirical study each sentence is presented as a sin-gle line on screen, this 2D structure does not ex-ist.
Moreover, Abdulin & Komogortsev (2015) onlyreport accuracy improvements for their method ina setting where individuals have to be identified inthe future based on data collected in the past (agingtest), which is not the focus of our study.We first study multiclass identification accuracy.All test observations of one particular individualconstitute one test example; the task is to infer theindividual that has generated these test observations.Multiclass identification accuracy is the fraction ofcases in which the correct individual is identified.Table 1 shows multiclass identification accuracy forall methods, including variants of Semiparametricdiscussed below.
We observe that Semiparametricoutperforms Landwehr et al, reducing the error bymore than a factor of three.
Consistent with resultsreported in Landwehr et al (2014), Holland & K.(unweighted) is less accurate than Landwehr et al,but more accurate than the simplified variants.
Wenext study how the amount of data available at testtime?that is, the amount of time we can observe areader before having to make a decision?influencesaccuracy.
Figure 3 (left) shows identification accu-racy as a function of the fraction of test data avail-able, obtained by randomly removing a fraction ofsentences from the test set.
We observe that iden-tification accuracy steadily improves with more testobservations for all methods.
Figure 3 (right) showsidentification accuracy when varying the number Rof individuals that need to be distinguished.
We ran-domly draw a subset of R individuals from the set0 0.02 0.04 0.06 0.08 0.1?0.0100.010.020.030.040.050.06False rejectFalse acceptSemiparametricLandwehr et alHolland & K. (unweighted)Figure 5: False-accept over false-reject rate when using 40%(dotted), 60% (dashed-dotted), 80% (dashed), and 100% (solid)of test observations, for selected subset of methods.Method Area under curveSemiparametric 0.0000119Semiparametric (TD) 0.0000821Semiparametric (TA) 0.0001833Landwehr et al 0.0001743Landwehr et al (TA) 0.0010371Landwehr et al (T) 0.0017040Holland & K. (unweighted) 0.0027853Holland & K. (weighted) 0.0039978Table 2: Area under the curve in binary classification setting.of 251 individuals, and perform identification basedon only these individuals.
Results are averaged over10 such random draws.
As expected, accuracy im-proves if fewer individuals need to be distinguished.We next study a binary setting in which for eachindividual and each set of test observations a deci-sion has to be made whether or not the test observa-tions have been generated by that individual.
Thissetting more closely matches typical use cases forthe deployment of a biometric system.
Let X?
de-note the text being read at test time, and let S?
de-note the observed eye movement sequences.
Ourmodel infers for each reader r ?
R the marginallikelihood p(S?|r, X?
,X ,S(1:R)) of the eye move-ment observations under the reader-specific model(Equation 3).
The binary decision is made bydividing this marginal likelihood by the averagemarginal likelihood assigned to the observations byall reader-specific models, and comparing the resultto a threshold ?
.
Figure 4 shows the fraction of falseaccepts as a function of false rejects as the thresh-old ?
is varied, averaged over all individuals.
TheLandwehr et al model and variants also assign a5920 0.2 0.4 0.6 0.8 100.20.40.60.81Fraction of test data usedAccuracyFigure 6: Multiclass accuracy over number of test observationswith standard errors for Semiparametric variants.reader-specific likelihood to novel test observations;we compute the same statistics again by normaliz-ing the likelihood and comparing to a threshold ?
.Finally, Holland & K. (unweighted) and Holland& K. (weighted) compute a similarity measure foreach combination of individual and set of test ob-servations, which we normalize and threshold anal-ogously.
We observe that Semiparametric accom-plishes a false-reject rate of below 1% at virtuallyno false accepts; Landwehr et al and variants tendto perform better than Holland & K. (unweighted)and Holland & K. (weighted) .
Table 2 shows theerror under the curve for the experiment shown inFigure 4, as well as for variants of Semiparametricdiscussed below.We finally study the contribution of the individualmodel components for saccade type, saccade am-plitude, and fixation duration (see Figure 2) by re-moving the corresponding model components, as inLandwehr et al (2014).
By Semiparametric (TD)we denote a variant of Semiparametric in which thevariable at and the corresponding distribution is re-moved, that is, only the distribution over the sac-cade type and duration is modeled.
Semiparamet-ric (TA) denotes a variant in which the variabledt and the corresponding distribution is removed.Figure 6 shows identification accuracy as a func-tion of the fraction of test data available for modelvariants Semiparametric (TD) and Semiparametric(TA) in comparison to Semiparametric; results forthese variants are also included in Table 1.
Figure 7shows the fraction of false accepts as a function of0 0.02 0.04 0.06 0.08 0.1?0.0100.010.020.030.040.050.06False rejectFalse acceptSemiparametric FullSemiparametric (TD)Semiparametric (TA)Figure 7: False-accept over false-reject rate when varying ?
forthe Semiparametric variants.false rejects in the binary classification setting dis-cussed above for these two model variants; Table 2includes area under the curve results for the experi-ment shown in Figure 7.
We observe that accuracyis substantially reduced when removing any modelcomponent.
Note that if both the amplitude and du-ration components of the model are removed, it be-comes identical to the model Landwehr et al (T) .Training the joint model for all 251 individualstakes 46 hours on a single eight-core CPU (IntelXeon E5520, 2.27GHz); predicting the most likelyindividual to have generated a set of 72 test sen-tences takes less than 2 seconds.6 ConclusionsWe have studied the problem of identifying read-ers unobtrusively during reading of arbitrary text.For fitting reader-specific distributions, we employ aBayesian semiparametric approach that infers den-sities under a Gaussian process prior centered at thegamma family of distributions, striking a balance be-tween robustness to sparse data and modeling flex-ibility.
In an empirical study with 251 individuals,the model was shown to reduce identification er-ror by more than a factor of three compared to ear-lier approaches to reader identification proposed byLandwehr et al (2014) and Holland & Komogort-sev (2012).AcknowledgementsWe gratefully acknowledge support from theGerman Research Foundation (DFG), grantLA 3270/1-1.593ReferencesEvgeniy Abdulin and Oleg Komogortsev.
2015.
Per-son verification via eye movement-driven text readingmodel.
In Proceedings of the Sixth International Con-ference on Biometrics: Theory, Applications and Sys-tems.Ryan P. Adams, Iain Murray, and David J.C. MaxKay.2009.
Gaussian process density sampler.
In Proceed-ings of the 21st Annual Conference on Neural Infor-mation Processing Systems.Roman Bednarik, Tomi Kinnunen, Andrei Mihaila, andPasi Fra?nti.
2005.
Eye-movements as a biometric.
InProceedings of the 14th Scandinavian Conference onImage Analysis.W.
Robert Dixon.
1951.
Studies in the psychology ofreading.
In W. S. Morse, P. A. Ballantine, and W. R.Dixon, editors, Univ.
of Michigan Monographs in Ed-ucation No.
4.
Univ.
of Michigan Press.Ralf Engbert, Antje Nuthmann, Eike M. Richter, andReinhold Kliegl.
2005.
SWIFT: A dynamical modelof saccade generation during reading.
PsychologicalReview, 112(4):777?813.Tadayoshi Hara, Daichi Mochihashi, Yoshino Kano, andAkiko Aizawa.
2012.
Predicting word fixations in textwith a CRF model for capturing general reading strate-gies among readers.
In Proceedings of the First Work-shop on Eye-Tracking and Natural Language Process-ing.Julian Heister, Kay-Michael Wu?rzner, and ReinholdKliegl.
2012.
Analysing large datasets of eye move-ments during reading.
In James S. Adelman, editor,Visual word recognition.
Vol.
2: Meaning and context,individuals and development, pages 102?130.Nils L. Hjort and Ingrid K. Glad.
1995.
Nonparametricdensity estimation with a parametric start.
The Annalsof Statistics, 23(3):882?904.Corey Holland and Oleg V. Komogortsev.
2012.
Biomet-ric identification via eye movement scanpaths in read-ing.
In Proceedings of the 2011 International JointConference on Biometrics.Edmund B. Huey.
1908.
The psychology and pedagogyof reading.
Cambridge, Mass.
: MIT Press.Pawel Kasprowski and Jozef Ober.
2004.
Eye move-ments in biometrics.
In Proceedings of the 2004 Inter-national Biometric Authentication Workshop.Reinhold Kliegl, Antje Nuthmann, and Ralf Engbert.2006.
Tracking the mind during reading: The influ-ence of past, present, and future words on fixation du-rations.
Journal of Experimental Psychology: Gen-eral, 135(1):12?35.Oleg V. Komogortsev, Sampath Jayarathna, Cecilia R.Aragon, and Mechehoul Mahmoud.
2010.
Biomet-ric identification via an oculomotor plant mathemati-cal model.
In Proceedings of the 2010 Symposium onEye-Tracking Research & Applications.Niels Landwehr, Sebastian Arzt, Tobias Scheffer, andReinhold Kliegl.
2014.
A model of individual differ-ences in gaze control during reading.
In Proceedingsof the 2014 Conference on Empirical Methods on Nat-ural Language Processing.Peter J. Lenk.
2003.
Bayesian semiparametric den-sity estimation and model verification using a logistic-Gaussian process.
Journal of Computational andGraphical Statistics, 12(3):548?565.Tom Leonard.
1978.
Density estimation, stochastic pro-cesses and prior information.
Journal of the Royal Sta-tistical Society, 40(2):113?146.Franz Matties and Anders S?gaard.
2013.
With blinkerson: robust prediction of eye movements across readers.In Proceedings of the 2013 Conference on EmpiricalNatural Language Processing.Keith Rayner.
1998.
Eye movements in reading and in-formation processing: 20 years of research.
Psycho-logical Bulletin, 124(3):372?422.Erik D. Reichle, Alexander Pollatsek, Donald L. Fisher,and Keith Rayner.
1998.
Toward a model of eyemovement control in reading.
Psychological Review,105(1):125?157.Ioannis Rigas, George Economou, and Spiros Fotopou-los.
2012a.
Biometric identification based on the eyemovements and graph matching techniques.
PatternRecognition Letters, 33(6).Ioannis Rigas, George Economou, and Spiros Fotopou-los.
2012b.
Human eye movements as a trait for bio-metrical identification.
In Proceedings of the IEEE 5thInternational Conference on Biometrics: Theory, Ap-plications and Systems.Ioannis Rigas, Oleg Komogortsev, and Reza Shadmehr.2016.
Biometric recognition via eye movements: Sac-cadic vigor and acceleration cues.
ACM Transactionon Applied Perception, 13(2):1?21.Surya T. Tokdar, Yu M. Zhuy, and Jayanta K. Ghoshz.2010.
Bayesian density regression with logistic gaus-sian process and subspace projection.
Bayesian Anal-ysis, 5(2):319?344.Ying Yang.
2009.
Penalized semiparametric density es-timation.
Statistics and Computing, 19(1):355?366.Youming Zhang and Martti Juhola.
2012.
On biomet-ric verification of a user by means of eye movementdata mining.
In Proceedings of the 2nd InternationalConference on Advances in Information Mining andManagement.594
