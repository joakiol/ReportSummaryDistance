Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 287?296,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSemi-Supervised Learning of Sequence Models with the Method of MomentsZita Marinho?]
Andre?
F. T.
Martins???
Shay B. Cohen?
Noah A.
Smith?
?Instituto de Sistemas e Robo?tica, Instituto Superior Te?cnico, 1049-001 Lisboa, Portugal?Instituto de Telecomunicac?o?es, Instituto Superior Te?cnico, 1049-001 Lisboa, Portugal]School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA?Unbabel Lda, Rua Visconde de Santare?m, 67-B, 1000-286 Lisboa, Portugal?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal?School of Informatics, University of Edinburgh, Edinburgh EH8 9AB, UK?Computer Science & Engineering, University of Washington, Seattle, WA 98195, USAzmarinho@cmu.edu, andre.martins@unbabel.com,scohen@inf.ed.ac.uk, nasmith@cs.washington.eduAbstractWe propose a fast and scalable method forsemi-supervised learning of sequence models,based on anchor words and moment matching.Our method can handle hidden Markov mod-els with feature-based log-linear emissions.Unlike other semi-supervised methods, no de-coding passes are necessary on the unlabeleddata and no graph needs to be constructed?only one pass is necessary to collect momentstatistics.
The model parameters are estimatedby solving a small quadratic program for eachfeature.
Experiments on part-of-speech (POS)tagging for Twitter and for a low-resource lan-guage (Malagasy) show that our method canlearn from very few annotated sentences.1 IntroductionStatistical learning of NLP models is often lim-ited by the scarcity of annotated data.
Weakly su-pervised methods have been proposed as an alter-native to laborious manual annotation, combininglarge amounts of unlabeled data with limited re-sources, such as tag dictionaries or small annotateddatasets (Merialdo, 1994; Smith and Eisner, 2005;Garrette et al, 2013).
Unfortunately, most semi-supervised learning algorithms for the structuredproblems found in NLP are computationally expen-sive, requiring multiple decoding passes through theunlabeled data, or expensive similarity graphs.
Morescalable learning algorithms are in demand.In this paper, we propose a moment-matchingmethod for semi-supervised learning of sequencemodels.
Spectral learning and moment-matchingapproaches have recently proved a viable alternativeto expectation-maximization (EM) for unsupervisedlearning (Hsu et al, 2012; Balle and Mohri, 2012;Bailly et al, 2013), supervised learning with latentvariables (Cohen and Collins, 2014; Quattoni et al,2014; Stratos et al, 2013) and topic modeling (Aroraet al, 2013; Nguyen et al, 2015).
These methodshave learnability guarantees, do not suffer from lo-cal optima, and are computationally less demanding.Unlike spectral methods, ours does not require anorthogonal decomposition of any matrix or tensor.Instead, it considers a more restricted form of super-vision: words that have unambiguous annotations,so-called anchor words (Arora et al, 2013).
Ratherthan identifying anchor words from unlabeled data(Stratos et al, 2016), we extract them from a smalllabeled dataset or from a dictionary.
Given the an-chor words, the estimation of the model parameterscan be made efficient by collecting moment statisticsfrom unlabeled data, then solving a small quadraticprogram for each word.Our contributions are as follows:?
We adapt anchor methods to semi-supervisedlearning of generative sequence models.?
We show how our method can also handle log-linear feature-based emissions.?
We apply this model to POS tagging.
Our ex-periments on the Twitter dataset introduced byGimpel et al (2011) and on the dataset in-troduced by Garrette et al (2013) for Mala-gasy, a low-resource language, show that ourmethod does particularly well with very little la-beled data, outperforming semi-supervised EMand self-training.2872 Sequence LabelingIn this paper, we address the problem of sequencelabeling.
Let x1:L = ?x1, .
.
.
, xL?
be a sequence ofL input observations (for example, words in a sen-tence).
The goal is to predict a sequence of labelsh1:L = ?h1, .
.
.
, hL?, where each hi is a label for theobservation xi (for example, the word?s POS tag).We start by describing two generative sequencemodels: hidden Markov models (HMMs, ?2.1), andtheir generalization with emission features (?2.2).Later, we propose a weakly-supervised method forestimating these models?
parameters (?3?
?4) basedonly on observed statistics of words and contexts.2.1 Hidden Markov ModelsWe define random variables X := ?X1, .
.
.
, XL?andH := ?H1, .
.
.
,HL?, corresponding to observa-tions and labels, respectively.
Each Xi is a randomvariable over a set X (the vocabulary), and each Hiranges over H (a finite set of ?states?
or ?labels?
).We denote the vocabulary size by V = |X |, and thenumber of labels by K = |H|.
A first-order HMMhas the following generative scheme:p(X = x1:L,H = h1:L) := (1)L?`=1p(X`=x` | H`=h`)L?`=0p(H`+1=h`+1 | H`=h`),where we have defined h0 = START and hL+1 =STOP.
We adopt the following notation for the pa-rameters:?
The emission matrix O ?
RV?K , defined asOx,h := p(X` = x | H` = h), ?h ?
H, x ?
X .?
The transition matrix T ?
R(K+2)?
(K+2), de-fined as Th,h?
:= p(H`+1 = h | H` = h?
), forevery h, h?
?
H ?
{START, STOP}.
This matrixsatisfies T>1 = 1.1Throughout the rest of the paper we will adoptX ?
X` and H ?
H` to simplify notation, when-ever the index ` is clear from the context.
Underthis generative process, predicting the most proba-ble label sequence h1:L given observations x1:L is1That is, it satisfies ?Kh=1 p(H`+1 = h | H` = h?)
+p(H`+1 = STOP | H` = h?)
= 1; and also ?Kh=1 p(H1 =h | H0 = START) = 1.accomplished with the Viterbi algorithm inO(LK2)time.If labeled data are available, the model param-eters O and T can be estimated with the maxi-mum likelihood principle, which boils down to asimple counting of events and normalization.
Ifwe only have unlabeled data, the traditional ap-proach is the expectation-maximization (EM) algo-rithm, which alternately decodes the unlabeled ex-amples and updates the model parameters, requiringmultiple passes over the data.
The same algorithmcan be used in semi-supervised learning when la-beled and unlabeled data are combined, by initial-izing the model parameters with the supervised esti-mates and interpolating the estimates in the M-step.2.2 Feature-Based Hidden Markov ModelsSequence models with log-linear emissions havebeen considered by Smith and Eisner (2005), in adiscriminative setting, and by Berg-Kirkpatrick etal.
(2010), as generative models for POS induc-tion.
Feature-based HMMs (FHMMs) define a fea-ture function for words, ?
(X) ?
RW , which can bediscrete or continuous.
This allows, for example, toindicate whether an observation, corresponding to aword, starts with an uppercase letter, contains digitsor has specific affixes.
More generally, it helps withthe treatment of out-of-vocabulary words.
The emis-sion probabilities are modeled as K conditional dis-tributions parametrized by a log-linear model, wherethe ?h ?
RW represent feature weights:p(X = x | H = h) := exp(?>h?(x))/Z(?h).
(2)Above, Z(?h) := ?x?
?X exp(?>h?(x?))
is a nor-malization factor.
We will show in ?4 how ourmoment-based semi-supervised method can also beused to learn the feature weights ?h.3 Semi-Supervised Learning via MomentsWe now describe our moment-based semi-supervised learning method for HMMs.
Through-out, we assume the availability of a small labeleddataset DL and a large unlabeled dataset DU .The full roadmap of our method is shown as Al-gorithm 1.
Key to our method is the decompositionof a context-word moment matrix Q ?
RC?V ,which counts co-occurrences of words and contexts,288Algorithm 1 Semi-Supervised Learning of HMMs withMomentsInput: Labeled dataset DL, unlabeled dataset DUOutput: Estimates of emissions O and transitions T1: Estimate context-word moments Q?
from DU (Eq.
5)2: for each label h ?
H do3: Extract set of anchor words A(h) from DL (?3.2)4: end for5: Estimate context-label moments R?
from anchors andDU (Eq.
12)6: for each word w ?
[V ] do7: Solve the QP in Eq.
14 to obtain ?w from Q?, R?8: end for9: Estimate emissions O from ?
via Eq.
1510: Estimate transitions T from DL11: Return ?O,T?Figure 1: HMM, context (green) conditionally indepen-dent of present (red) w` given state h`.and will be formally defined in ?3.1.
Such co-occurrence matrices are often collected in NLP, forvarious problems, ranging from dimensionality re-duction of documents using latent semantic index-ing (Deerwester et al, 1990; Landauer et al, 1998),distributional semantics (Schu?tze, 1998; Levy et al,2015) and word embedding generation (Dhillon etal., 2015; Osborne et al, 2016).
We can build such amoment matrix entirely from the unlabeled dataDU .The same unlabeled data is used to build an estimateof a context-label moment matrix R ?
RC?K , asexplained in ?3.3.
This is done by first identifyingwords that are unambiguously associated with eachlabel h, called anchor words, with the aid of a fewlabeled data; this is outlined in ?3.2.
Finally, givenempirical estimates of Q and R, we estimate theemission matrix O by solving a small optimizationproblem independently per word (?3.4).
The transi-tion matrix T is obtained directly from the labeleddataset DL by maximizing the likelihood.3.1 Moments of Contexts and WordsTo formalize the notion of ?context,?
we introducethe shorthand Z` := ?X1:(`?1),X(`+1):L?.
Impor-tantly, the HMM in Eq.
1 entails the following con-ditional independence assumption: X` is condition-ally independent of the surrounding contextZ` giventhe hidden state H`.
This is illustrated in Figure 1,using POS tagging as an example task.We introduce a vector of context features?
(Z`) ?
RC , which may look arbitrarily withinthe context Z` (left or right), but not at X` itself.These features could be ?one-hot?
representationsor other reduced-dimensionality embeddings (as de-scribed later in ?5).
Consider the word w ?
X aninstance of X ?
X`.
A pivotal matrix in our formu-lation is the matrix Q ?
RC?V , defined as:Qc,w := E[?c(Z) | X = w].
(3)Expectations here are taken with respect to the prob-abilistic model in Eq.
1 that generates the data.
Thefollowing quantities will also be necessary:qc := E[?c(Z)], pw := p(X = w).
(4)Since all the variables in Eqs.
3?4 are observed, wecan easily obtain empirical estimates by taking ex-pectations over the unlabeled data:Q?c,w =?x,z?DU ?c(z)1(x = w)?x,z?DU 1(x = w), (5)q?c =?x,z?DU ?c(z)/|DU |, (6)p?w =?x,z?DU 1(x = w)/|DU |.
(7)where we take 1(x = w) to be the indicator for wordw.
Note that, under our modeling assumptions, Qdecomposes in terms of its hidden states:E[?c(Z) | X = w] = (8)?h?Hp(H = h | X = w)E[?c(Z) | H = h]The reason why this holds is that, as stated above,Zand X are conditionally independent given H .3.2 Anchor WordsFollowing Arora et al (2013) and Cohen and Collins(2014), we identify anchor words whose hidden289state is assumed to be deterministic, regardless ofcontext.
In this work, we generalize this notion tomore than one anchor word per label, for improvedcontext estimates.
This allows for more flexibleforms of anchors with weak supervision.
For eachstate h ?
H, let its set of anchor words beA(h)= {w ?
X : p(H = h | X = w) = 1} (9)={w ?
X : Ow,h>0 ?Ow,h?=0, ?h?
6=h}.That is, A(h) is the set of unambiguous words thatalways take the label h. This can be estimated fromthe labeled dataset DL by collecting the most fre-quent unambiguous words for each label.Algorithms for identifying A(h) from unlabeleddata alone were proposed by Arora et al (2013) andZhou et al (2014), with application to topic models.Our work differs in which we do not aim to discoveranchor words from pure unlabeled data, but ratherexploit the fact that small amounts of labeled dataare commonly available in many NLP tasks?betteranchors can be extracted easily from such small la-beled datasets.
In ?5 we give a more detailed de-scription of the selection process.3.3 Moments of Contexts and LabelsWe define the matrix R ?
RC?K as follows:Rc,h := E[?c(Z) | H = h].
(10)Since the expectation in Eq.
10 is conditioned on the(unobserved) label h, we cannot directly estimate itusing moments of observed variables, as we do forQ.
However, if we have identified sets of anchorwords for each label h ?
H, we have:E[?c(Z) | X ?
A(h)] ==?h?E[?c(Z) | H = h?]
p(H = h?
| X ?
A(h))?
??
?=1(h?=h)= Rc,h.
(11)Therefore, given the set of anchor words A(h), thehth column of R can be estimated in a single passover the unlabeled data, as follows:R?c,h =?x,z?DU ?c(z)1(x ?
A(h))?x,z?DU 1(x ?
A(h))(12)3.4 Emission DistributionsWe can now put all the ingredients above togetherto estimate the emission probability matrix O. Theprocedure we propose here is computationally veryefficient, since only one pass is required over the un-labeled data, to collect the co-occurrence statistics Q?and R?.
The emissions will be estimated from thesemoments by solving a small problem independentlyfor each word.
Unlike EM and self-training, no de-coding is necessary, only counting and normalizing;and unlike label propagation methods, there is re-quirement to build a graph with the unlabeled data.The crux of our method is the decomposition inEq.
8, which is combined with the one-to-one cor-respondence between labels h and anchor wordsA(h).
We can rewrite Eq.
8 as:Qc,w =?hRc,h p(H = h | X = w).
(13)In matrix notation, we have Q = R?, where ?
?RK?V is defined as ?h,w := p(H = h | X = w).If we had infinite unlabeled data, our moment es-timates Q?
and R?
would be perfect and we couldsolve the system of equations in Eq.
13 to obtain?
exactly.
Since we have finite data, we resort toa least squares solution.
This corresponds to solv-ing a simple quadratic program (QP) per word, in-dependent from all the other words, as follows.
De-note by qw := E[?
(Z) | X = w] ?
RC and by?w := p(H = ?
| X = w) ?
RK the wth columnsof Q and ?, respectively.
We estimate the latter dis-tribution following Arora et al (2013):?
?w = arg min?w?qw ?R?w?22s.t.
1>?w = 1, ?w ?
0.
(14)Note that this QP is very small?it has only Kvariables?hence, we can solve it very quickly (1.7ms on average, in Gurobi, with K = 12).Given the probability tables for p(H = h | X =w), we can estimate the emission probabilities O bydirect application of Bayes rule:O?w,h =p(H = h | X = w)?
p(X = w)p(H = h) (15)= ?
?w,c ?Eq.
7????p?w?w?
?
?w?,c ?
p?w?.
(16)290These parameters are guaranteed to lie in the prob-ability simplex, avoiding the need of heuristics fordealing with ?negative?
and ?unnormalized?
prob-abilities required by prior work in spectral learn-ing (Cohen et al, 2013).3.5 Transition DistributionsIt remains to estimate the transition matrix T. Forthe problems tackled in this paper, the number oflabels K is small, compared to the vocabulary sizeV .
The transition matrix has only O(K2) degrees offreedom, and we found it effective to estimate it us-ing the labeled sequences in DL alone, without anyrefinement.
This was done by smoothed maximumlikelihood estimation on the labeled data, whichboils down to counting occurrences of consecutivelabels, applying add-one smoothing to avoid zeroprobabilities for unobserved transitions, and normal-izing.For problems with numerous labels, a possible al-ternative is the composite likelihood method (Cha-ganty and Liang, 2014).
Given O?, the maximizationof the composite log-likelihood function leads to aconvex optimization problem that can be efficientlyoptimized with an EM algorithm.
A similar proce-dure was carried out by Cohen and Collins (2014).24 Feature-Based EmissionsNext, we extend our method to estimate the param-eters of the FHMM in ?2.2.
Other than contextualfeatures ?
(Z) ?
RC , we also assume a featureencoding function for words, ?
(X) ?
RW .
Ourframework, illustrated in Algorithm 2, allows forboth discrete and continuous word and context fea-tures.
Lines 2?5 are the same as in Algorithm 1,replacing word occurrences with expected values ofword features (we redefine Q and ?
to cope withfeatures instead of words).
The main differencewith respect to Algorithm 1 is that we do not es-timate emission probabilities; rather, we first esti-mate the mean parameters (feature expectationsE[?
(X) | H = h]), by solving one QP for each2In preliminary experiments, the compositional likelihoodmethod was not competitive with estimating the transition ma-trices directly from the labeled data, on the datasets describedin ?6; results are omitted due to lack of space.
However, thismay be a viable alternative if there is no labeled data and theanchors are extracted from gazetteers or a dictionary.Algorithm 2 Semi-Supervised Learning of Feature-Based HMMs with MomentsInput: Labeled dataset DL, unlabeled dataset DUOutput: Emission log-linear parameters ?
and transi-tions T1: Estimate context-word moments Q?
from DU(Eq.
20)2: for each label h ?
H do3: Extract set of anchor words A(h) from DL (?3.2)4: end for5: Estimate context-label moments R?
from the anchorsand DU (Eq.
12)6: for each word feature j ?
[W ] do7: Solve the QP in Eq.
22 to obtain ?j from Q?, R?8: end for9: for each label h ?
H do10: Estimate the mean parameters ?h from ?
(Eq.
24)11: Estimate the canonical parameters ?h from ?h bysolving Eq.
2512: end for13: Estimate transitions T from DL14: Return ?
?,T?emission feature; and then we solve a convex op-timization problem, for each label h, to recoverthe log-linear weights over emission features (calledcanonical parameters).4.1 Estimation of Mean ParametersFirst of all, we replace word probabilities by expec-tations over word features.
We redefine the matrix?
?
RK?W as follows:?h,j :=p(H = h)?
E[?j(X) | H = h]E[?j(X)] .
(17)Note that, with one-hot word features, we haveE[?w(X) | H = h] = P (X = w | H = h),E[?w(X)] = p(X = w), and therefore ?h,w =p(H = h | X = w), so this can be regarded as ageneralization of the framework in ?3.4.Second, we redefine the context-word momentmatrix Q as the following matrix in RC?W :Qc,j = E [?c(Z)?
?j(X)]/E[?j(X)].
(18)Again, note that we recover the previous Q if we useone-hot word features.
We then have the followinggeneralization of Eq.
13:E [?c(Z)?
?j(X)]/E[?j(X)] = (19)?h E [?c(Z) | H = h] P (H=h)E[?j(X)|H=h]E[?j(X)] ,291or, in matrix notation, Q = R?.Again, matrices Q and R can be estimated fromdata by collecting empirical feature expectationsover unlabeled sequences of observations.
For Ruse Eq.
12 with no change; for Q replace Eq.
5 byQ?c,j =?x,z?DU ?c(z)?j(x)/?x,z?DU ?j(x).
(20)Let qj ?
RC and ?j ?
RK be columns of Q?
and ??,respectively.
Note that we must have1>?j =?hP (H=h)E[?j(X)|H=h]E[?j(X)] = 1, (21)since E[?j(X)] = ?h P (H =h)E [?j(X) | H = h].
We rewrite the QP tominimize the squared difference for each dimensionj independently:?
?j = arg min?j?
?qj ?R?j?
?22 s.t.
1>?j = 1.
(22)Note that, if ?
(x) ?
0 for all x ?
X , then we musthave ?j ?
0, and therefore we may impose this in-equality as an additional constraint.Let ??
?
RK be the vector of state probabilities,with entries ?
?h := p(H = h) for h ?
H. This vec-tor can also be recovered from the unlabeled datasetand the set of anchors, by solving another QP thataggregates information for all words:??
= arg min???q?
?R??
?22 s.t.
1>??
= 1, ??
?
0.
(23)where q?
:= E?[?
(Z)] ?
RC is the vector whoseentries are defined in Eq.
6.Let ?h := E[?
(X) | H = h] ?
RW be themean parameters of the distribution for each state h.These parameters are computed by solving W inde-pendent QPs (Eq.
22), yielding the matrix ?
definedin Eq.
17, and then applying the formula:?h,j = ?j,h ?
E[?j(X)]/ ?
?h, (24)with ?
?h = p(H = h) estimated as in Eq.
23.4.2 Estimation of Canonical ParametersTo compute a mapping from mean parameters ?hto canonical parameters ?h, we use the well-knownFenchel-Legendre duality between the entropy andthe log-partition function (Wainwright and Jordan,2008).
Namely, we need to solve the following con-vex optimization problem:?
?h = arg max?h?>h?h ?
logZ(?h) + ?
?h?, (25)where  is a regularization constant.3 In practice,this regularization is important, since it prevents ?hfrom growing unbounded whenever ?h falls outsidethe marginal polytope of possible mean parameters.We solve Eq.
25 with the limited-memory BFGS al-gorithm (Liu and Nocedal, 1989).5 Method ImprovementsIn this section we detail three improvements to ourmoment-based method that had a practical impact.Supervised Regularization.
We add a supervisedpenalty term to Eq.
22 to keep the label posteriors ?jclose to the label posteriors estimated in the labeledset, ?
?j , for every feature j ?
[W ].
The regularizedleast-squares problem becomes:min?j(1?
?
)?qj ?R?j?2 + ???j??
?j?2s.t.
1>?j = 1.
(26)CCA Projections.
A one-hot feature representa-tion of words and contexts has the disadvantage thatit grows with the vocabulary size, making the mo-ment matrix Q too sparse.
The number of contex-tual features and words can grow rapidly on largetext corpora.
Similarly to Cohen and Collins (2014)and Dhillon et al (2015), we use canonical correla-tion analysis (CCA) to reduce the dimension of thesevectors.
We use CCA to form low-dimensional pro-jection matrices for features of words PW ?
RW?Dand features of contexts PC ?
RC?D, with Dmin{W,C}.
We use these projections on the origi-nal feature vectors and replace the these vectors withtheir projections.Selecting Anchors.
We collect counts of eachword-label pair, and select up to 500 anchors withhigh conditional probability on the anchoring statep?
(h | w).
We tuned the probability threshold to3As shown by Xiaojin Zhu (1999) and Yasemin Altun(2006), this regularization is equivalent, in the dual, to a ?soft?constraint ?E?h [?
(X) | H = h]?
?h?2 ?
, as opposed to astrict equality.292select the anchors on the validation set, using stepsof 0.1 in the unit interval, and making sure that alltags have at least one anchor.
We also considereda frequency threshold, constraining anchors to oc-cur more than 500 times in the unlabeled corpus,and four times in the labeled corpus.
Note thatpast work used a single anchor word per state (i.e.,|A(h)| = 1).
We found that much better results areobtained when |A(h)|  1, as choosing more an-chors increases the number of samples used to esti-mate the context-label moment matrix R?, reducingnoise.6 ExperimentsWe evaluated our method on two tasks: POS taggingof Twitter text (in English), and POS tagging for alow-resource language (Malagasy).
For all the ex-periments, we used the universal POS tagset (Petrovet al, 2012), which consists of K = 12 tags.We compared our method against supervised base-lines (HMM and FHMM), which use the labeleddata only, and two semi-supervised baselines thatexploit the unlabeled data: self-training and EM.For the Twitter experiments, we also evaluated astacked architecture in which we derived featuresfrom our model?s predictions to improve a state-of-the-art POS tagger (MEMM).46.1 Twitter POS TaggingFor the Twitter experiment, we used the Oct27dataset of Gimpel et al (2011), with the providedpartitions (1,000 tweets for training and 328 for val-idation), and tested on the Daily547 dataset (547tweets).
Anchor words were selected from the train-ing partition as described in ?5.
We used 2.7Munlabeled tweets (O?Connor et al, 2010) to trainthe semi-supervised methods, filtering the Englishtweets as in Lui and Baldwin (2012), tokenizingthem as in Owoputi et al (2013), and normalizingat-mentions, URLs, and emoticons.We used as word features ?
(X) the word iself,as well as binary features for capitalization, titles,and digits (Berg-Kirkpatrick et al, 2010), the wordshape, and the Unicode class of each character.
Sim-ilarly to Owoputi et al (2013), we also used suf-fixes and prefixes (up to length 3), and Twitter-4http://www.ark.cs.cmu.edu/TweetNLP/0.70.750.80.850.90.950 100 200 300 400 500 600 700 800 900 1000Tagging accuracy (0/1 loss)Labeled sequencesanchor FHMM ?=0 FHMManchor FHMM ?=1 HMMFigure 2: POS tagging accuracy in the Twitter data versusthe number of labeled training sequences.specific features: whether the word starts with @,#, or http://.
As contextual features ?
(Z), we de-rive analogous features for the preceding and fol-lowing words, before reducing dimensionality withCCA.
We collect feature expectations for words andcontexts that occur more than 20 times in the un-labeled corpus.
We tuned hyperparameters on thedevelopment set: the supervised interpolation co-efficient in Eq.
26, ?
?
{0, 0.1, .
.
.
, 1.0}, and,for all systems, the regularization coefficient  ?
{0.0001, 0.001, 0.01, 0.1, 1, 10}.
(Underlines indi-cate selected values.)
The former controls howmuch we rely on the supervised vs. unsupervised es-timates.
For ?
= 1.0 we used supervised estimatesonly for words that occur in the labeled corpus, allthe remaining words rely solely on unsupervised es-timates.Varying supervision.
Figure 2 compares thelearning curves of our anchor-word method for theFHMM with the supervised baselines.
We showthe performance of the anchor methods without in-terpolation (?
= 0), and with supervised interpo-lation coefficient (?
= 1).
When the amount ofsupervision is small, our method with and withoutinterpolation outperforms all the supervised base-lines.
This improvement is gradually attenuatedwhen more labeled sequences are used, with the su-pervised FHMM catching up when the full labeleddataset is used.
The best model ?
= 1.0 relies onsupervised estimates for words that occur in the la-beled corpus, and on anchor estimates for words thatoccur only in the unlabeled corpus.
The unregular-293HMM FHMMModels / #sequences 150 1000 150 1000Supervised baselineHMM 71.7 81.1 81.8 89.1Semi-supervised baselinesEM 77.2 83.1 81.8 89.1self-training 78.2 86.1 83.4 89.4Anchor Modelsanchors, ?
= 0.0 83.0 85.5 84.1 86.7anchors, ?
= 1.0 84.3 88.0 85.3 89.1Table 1: Tagging accuracies on Twitter.
Shown arethe supervised and semi-supervised baselines, and ourmoment-based method, trained with 150 training labeledsequences, and the full labeled corpus (1000 sequences).ized model ?
= 0.0 relies solely on unsupervisedestimates given the set of anchors.Semi-supervised comparison.
Next, we compareour method to two other semi-supervised baselines,using both HMMs and FHMMs: EM and self-training.
EM requires decoding and counting inmultiple passes over the full unlabeled corpus.
Weinitialized the parameters with the supervised esti-mates, and selected the iteration with the best ac-curacy on the development set.5 The self-trainingbaseline uses the supervised system to tag the unla-beled data, and then retrains on all the data.Results are shown in Table 1.
We observe that,for small amounts of labeled data (150 tweets), ourmethod outperforms all the supervised and semi-supervised baselines, yielding accuracies 6.1 pointsabove the best semi-supervised baseline for a simpleHMM, and 1.9 points above for the FHMM.
Withmore labeled data (1,000 instances), our method out-performs all the baselines for the HMM, but not withthe more sophisticated FHMM, in which our accura-cies are 0.3 points below the self-training method.6These results suggest that our method is more effec-tive when the amount of labeled data is small.5The FHMM with EM did not perform better than the su-pervised baseline, so we consider the initial value as the bestaccuracy under this model.6According to a word-level paired Kolmogorov-Smirnovtest, for the FHMM with 1,000 tweets, the self-training methodoutperforms the other methods with statistical significance atp < 0.01; and for the FHMM with 150 tweets the anchor-basedand self-training methods outperform the other baselines withthe same p-value.
Our best HMM outperforms the other base-lines at a significance level of p < 0.01 for 150 and 1000 se-quences.150 1000MEMM (same+clusters) 89.57 93.36MEMM (same+clusters+posteriors) 91.14 93.18MEMM (all+clusters) 91.55 94.17MEMM (all+clusters+posteriors) 92.06 94.11Table 2: Tagging accuracy for the MEMM POS tagger ofOwoputi et al (2013) with additional features from ourmodel?s posteriors.Stacking features.
We also evaluated a stacked ar-chitecture in which we use our model?s predictionsas an additional feature to improve the state-of-the-art Twitter POS tagger of Owoputi et al (2013).
Thissystem is based on a semi-supervised discriminativemodel with Brown cluster features (Brown et al,1992).
We provide results using their full set of fea-tures (all), and using the same set of features in ouranchor model (same).
We compare tagging accuracyon a model with these features plus Brown clusters(+clusters) against a model that also incorporatesthe posteriors from the anchor method as an addi-tional feature in the MEMM (+clusters+posteriors).The results in Table 2 show that using our model?sposteriors are beneficial in the small labeled case,but not if the entire labeled data is used.Runtime comparison.
The training time of an-chor FHMM is 3.8h (hours), for self-training HMM10.3h, for EM HMM 14.9h and for Twitter MEMM(all+clusters) 42h.
As such, the anchor method ismuch more efficient than all the baselines becauseit requires a single pass over the corpus to collectthe moment statistics, followed by the QPs, with-out the need to decode the unlabeled data.
EM andthe Brown clustering method (the latter used to ex-tract features for the Twitter MEMM) require severalpasses over the data; and the self-training method in-volves decoding the full unlabeled corpus, which isexpensive when the corpus is large.
Our analysisadds to previous evidence that spectral methods aremore scalable than learning algorithms that requireinference (Parikh et al, 2012; Cohen et al, 2013).6.2 Malagasy POS TaggingFor the Malagasy experiment, we used the small la-beled dataset from Garrette et al (2013), which con-sists of 176 sentences and 4,230 tokens.
We alsomake use of their tag dictionaries with 2,773 types294Models Accuraciessupervised FHMM 90.5EM FHMM 90.5self-training FHMM 88.7anchors FHMM (token), ?=1.0 89.4anchors FHMM (type+token), ?=1.0 90.9Table 3: Tagging accuracies for the Malagasy dataset.and 23 tags, and their unlabeled data (43.6K se-quences, 777K tokens).
We converted all the orig-inal POS tags to universal tags using the mappingproposed in Garrette et al (2013).Table 3 compares our method with semi-supervised EM and self-training, for the FHMM.Wetested two supervision settings: token only, andtype+token annotations, analogous to Garrette et al(2013).
The anchor method outperformed the base-lines when both type and token annotations wereused to build the set of anchor words.77 ConclusionWe proposed an efficient semi-supervised sequencelabeling method using a generative log-linear model.We use contextual information from a set of an-chor observations to disambiguate state, and builda weakly supervised method from this set.
Ourmethod outperforms other supervised and semi-supervised methods, with small supervision in POS-tagging for Malagasy, a scarcely annotated lan-guage, and for Twitter.
Our anchor method is mostcompetitive for learning with large amounts of un-labeled data, under weak supervision, while trainingan order of magnitude faster than any of the base-lines.AcknowledgmentsSupport for this research was provided byFundac?a?o para a Cie?ncia e Tecnologia (FCT)through the CMU Portugal Program under grantSFRH/BD/52015/2012.
This work has also beenpartially supported by the European Union underH2020 project SUMMA, grant 688139, and by7Note that the accuracies are not directly comparable to Gar-rette et al (2013), who use a different tag set.
However, oursupervised baseline trained on those tags is already superior tothe best semi-supervised system in Garrette et al (2013), as weget 86.9% against the 81.2% reported in Garrette et al (2013)using their tagset.FCT, through contracts UID/EEA/50008/2013,through the LearnBig project (PTDC/EEI-SII/7092/2014), and the GoLocal project (grantCMUPERI/TIC/0046/2014).ReferencesSanjeev Arora, Rong Ge, Yoni Halpern, David Mimno,David Sontag Ankur Moitra, Yichen Wu, and MichaelZhu.
2013.
A practical algorithm for topic model-ing with provable guarantees.
In Proc.
of InternationalConference of Machine Learning.Raphae?l Bailly, Xavier Carreras, Franco M. Luque, andAriadna Quattoni.
2013.
Unsupervised spectral learn-ing of WCFG as low-rank matrix completion.
In Proc.of Empirical Methods in Natural Language Process-ing, pages 624?635.Borja Balle and Mehryar Mohri.
2012.
Spectral learningof general weighted automata via constrained matrixcompletion.
In Advances in Neural Information Pro-cessing Systems, pages 2168?2176.Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?,John DeNero, and Dan Klein.
2010.
Painless unsu-pervised learning with features.
In Human LanguageTechnologies: Conference of the North American As-sociation of Computational Linguistics.Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.Arun T. Chaganty and Percy Liang.
2014.
Estimatinglatent-variable graphical models using moments andlikelihoods.
In Proc.
of International Conference onMachine Learning.Shay B. Cohen and Michael Collins.
2014.
A provablycorrect learning algorithm for latent-variable PCFGs.In Proc.
of Association for Computational Linguistics.Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.Foster, and Lyle Ungar.
2013.
Experiments withspectral learning of latent-variable PCFGs.
In Proc.of North American Association of Computational Lin-guistics.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science, 41(6):391?407.Paramveer S. Dhillon, Dean P. Foster, and Lyle H. Ungar.2015.
Eigenwords: Spectral word embeddings.
Jour-nal of Machine Learning Research, 16:3035?3078.Dan Garrette, Jason Mielens, and Jason Baldridge.
2013.Real-world semi-supervised learning of POS-taggers295for low-resource languages.
In Proc.
of Associationfor Computational Linguistics.Gimpel, Schneider, O?Connor, Das, Mills, Eisenstein,Heilman, Yogatama, Flanigan, and Smith.
2011.
Part-of-speech tagging for twitter: Annotation, features,and experiments.
In Proc.
of Association of Compu-tational Linguistics.Daniel Hsu, Sham M. Kakade, and Tong Zhang.
2012.A spectral algorithm for learning hidden markov mod-els.
Journal of Computer and System Sciences,78(5):1460?1480.Thomas K. Landauer, Peter W. Foltz, and Darrell La-ham.
1998.
An introduction to latent semantic analy-sis.
Discourse Processes 25, pages 259?284.Omer Levy, Yoav Goldberg, and Ido Dagan.
2015.
Im-proving distributional similarity with lessons learnedfrom word embeddings.
Transactions of the Associa-tion for Computational Linguistics, 3:211?225.Dong Liu and Jorge Nocedal.
1989.
On the limited mem-ory bfgs method for large scale optimization.
Mathe-matical Programming, 45:503?528.Marco Lui and Timothy Baldwin.
2012. langid.py:An off-the-shelf language identification tool.
In Proc.of Association of Computational Linguistics SystemDemonstrations, pages 25?30.Bernard Merialdo.
1994.
Tagging english text witha probabilistic model.
Computational Linguistics,20(2):155?171.Thang Nguyen, Jordan Boyd-Graber, Jeff Lund, KevinSeppi, and Eric Ringger.
2015.
Is your anchor go-ing up or down?
Fast and accurate supervised topicmodels.
In Proc.
of North American Association forComputational Linguistics.Brendan O?Connor, Michel Krieger, and David Ahn.2010.
TweetMotif: Exploratory search and topic sum-marization for Twitter.
In Proc.
of AAAI Conferenceon Weblogs and Social Media.Dominique Osborne, Shashi Narayan, and Shay B. Co-hen.
2016.
Encoding prior knowledge with eigenwordembeddings.
Transactions of the Association of Com-putational Linguistics.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A Smith.
2013.Improved part-of-speech tagging for online conversa-tional text with word clusters.
In Proc.
of North Amer-ican Association for Computational Linguistics.Ankur P. Parikh, Lee Song, Mariya Ishteva, GabiTeodoru, and Eric P. Xing.
2012.
A spectral algo-rithm for latent junction trees.
In Proc.
of Uncertaintyin Artificial Intelligence.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proc.
of Interna-tional Conference on Language Resources and Evalu-ation (LREC).Ariadna Quattoni, Borja Balle, Xavier Carreras, andAmir Globerson.
2014.
Spectral regularization formax-margin sequence tagging.
In Proc.
of Interna-tional Conference of Machine Learning, pages 1710?1718.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Noah A. Smith and Jason Eisner.
2005.
Contrastive esti-mation: Training log-linear models on unlabeled data.In Proc.
of Association for Computational Linguistics,pages 354?362.Karl Stratos, Alexander M. Rush, Shay B. Cohen, andMichael Collins.
2013.
Spectral learning of refine-ment hmms.
In Proc.
of Computational Natural Lan-guage Learning.Karl Stratos, Michael Collins, and Daniel J. Hsu.
2016.Unsupervised part-of-speech tagging with anchor hid-den markov models.
Transactions of the Associationfor Computational Linguistics, 4:245?257.Martin J. Wainwright and Michael I. Jordan.
2008.Graphical models, exponential families, and varia-tional inference.
Foundations and Trends in MachineLearning, 1(2):1?305.Roni Rosenfeld Xiaojin Zhu, Stanley F. Chen.
1999.Linguistic features for whole sentence maximum en-tropy language models.
In European Conference onSpeech Communication and Technology.Alexander J. Smola Yasemin Altun.
2006.
Unifyingdivergence minimization and statistical inference viaconvex duality.
In Proc.
of Conference on LearningTheory.Tianyi Zhou, Jeff A. Bilmes, and Carlos Guestrin.
2014.Divide-and-conquer learning by anchoring a conicalhull.
In Advances in Neural Information ProcessingSystems.296
