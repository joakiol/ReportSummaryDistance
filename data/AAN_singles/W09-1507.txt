Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 42?44,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTowards Agile and Test-Driven Development in NLP ApplicationsJana Z. Sukkarieh Jyoti KamalEducational Testing Service Educational Testing ServiceRosedale Road  Rosedale RoadPrinceton, NJ 08541, USA Princeton, NJ 08541, USAJsukkarieh@ets.org Jkamal@ets.orgAbstractc-rater?
is the Educational Testing Service technol-ogy for automatic content scoring for short free-textresponses.
In this paper, we contend that an Agileand test-driven development environment optimizesthe development of an NLP-based technology.1 Introductionc-rater (Leacock and Chodorow, 2003) is the Edu-cational Testing Service technology for the auto-matic content scoring of short free-text responsesfor items whose rubrics are concept-based.
Thismeans that a set of concepts or main points are pre-specified in the rubric (see the example in Table 1).We view c-rater?s task as a textual entailmentproblem that involves the detection of whether astudent?s answer entails a particular concept (withthe additional challenge that the students?
data con-tains misspellings and grammatical errors).
Oursolution depends on a combination of rule-basedand statistically-based NLP modules (Sukkariehand Blackmore, 2009).
In addition to databases, aJBOSS server (www.jboss.org), and two user inter-faces, c-rater consists of 10 modules?eight ofwhich are Natural Language Processing (NLP)modules.
Figure 1 depicts the system?s architec-ture.
The c-rater engine is where all the linguisticprocessing and concept detection takes place.
Sec-tion 2 lists some of the major problems we facewhile developing such a complex NLP-based ap-plication and how our adoption of Agile and test-driven development is helping us.Example Item (Full Credit 2)Figures are givenPrompt:The figures show three poly-gons.
Is the polygon in Figure 1an octagon, hexagon, or paral-lelogram?
Explain your answer.Concepts or main/key points:C1: The polygon/it is a quadri-lateral with two sets of par-allel sides OR the oppositesides are of equal length ORopposite angles are equalC2: The polygon/it has four/4sidesScoring rules:2 points for C1 (only if C2 is not present)1 point for C1 and C2Otherwise 0Table 1.
Example item for c-rater scoringFigure 1. c-rater?s System Architecture2 Major Concerns and Solutions2.1 CommunicationIn the past, the implementation of each modulewas done in isolation and communication amongteam members was lacking.
When a team member42encountered a problem, it was only then that s/hewould be aware of some logic or data structurechanges by another member.
This is not necessar-ily an NLP-specific problem, however due to theparticularly frequent modifications in NLP-basedapplications (see Section 2.2), communication ismore challenging and updates are even more cru-cial.
The adoption of Scrum within Agile(Augustine, 2005) has improved communicationtremendously.
Although both the task backlog andthe choice of tasks within each sprint is done bythe product owner, throughout the sprint the plan-ning, requirement analysis, design, coding, andtesting is performed by all of the team members.This has been effecting in decreasing the numberof logic design errors.2.2 Planning and Frequent ModificationVery frequent modifications and re-prioritizing are,to a great extent, due to the nature of NL input andconstant re-specification, extension, and customi-zation of NLP modules.
This could also be due tochanges in business requirements, e.g.
to tailor theneeds of the application to a particular client?sneeds.
Further, this could be a response to emerg-ing research, following a sudden intuition or per-forming a heuristic approach.
Agile takes care ofall these issues.
It allows the development to adaptto changes more quickly and retract/replace the lastfeature-based enhancement(s) when the needarises.
It allows for incorporating research time andexperimental studies into the task backlog; hencethe various sprints.
The nature of the Agile envi-ronment allows us also to add tasks driven by thebusiness needs and consider them highest in value.2.3 Metrics for Functionality and ProgressMetrics for functionality includes measuring pro-gress, comparing one version to another and moni-toring the effect of frequent modifications.
Thisparticularly proves challenging due to the nature ofc-rater?s tasks and the NLP modules.
In most soft-ware, the business value is a working product.
In c-rater, it is not only about producing a score butproducing one for the ?right?
reasons and not dueto errors in the linguistic features obtained.Until recently, comparing versions meant compar-ing holistic scores without a sense of the effect ofparticular changes.
Evaluating the effect of achange often meant hand-checking hundreds andhundreds of cases.
To improve monitoring, wehave designed an engine test suite (each is a pair<model-sentence, answer> where model-sentenceis a variant of a concept) and introduced automatedtesting.
The suite is categorized according to thelinguistic phenomenon of interest (e.g., passive,ergative, negation, appositive, parser output, co-reference output).
Some categories follow the phe-nomena in Vanderwende and Dolan (2006).
SomeRTE data was transformed for engine tests.
Thisproduced a finer-grained view of the NLP modulesperformance, decreased the amount of hand-checking, and increased our confidence about the?correctness?
of our scores.2.4 Maintenance and DebuggingUntil very recently maintaining and debuggingthe system was very challenging.
We faced manyissues including the unsystematic scattering ofcommon data structures, making it hard to managedependencies; long functions making it difficult totrack bugs; and late integration or lack of regularupdates causing, at times, the system to crash ornot compile.
Although this may not be deemedNLP-specific, the need to modify NLP modulesmore frequently than anticipated has made this par-ticularly challenging.
To face this challenge, weintroduced unit tests (UT) and continuous integra-tion.
We usually select some representative or?typical?
NL input for certain phenomena, createan expected output, create a failed UT, and make itpass.
An additional challenge is that since stu-dents?
responses are noisy, sometimes choosing?typical?
text is hard.
Ideally, unit tests are sup-posed to be written before or at the same time asthe code; we were able to do that for approxi-mately 40% of the code.
The rest of the unit testingwas being written after the code was written.
Forlegacy code, we have covered around 10-20% ofthe code.In conclusion, we strongly believe like Degerstedtand J?nsson (2006), Agile and Test-Driven Devel-opment form a most-suitable environment forbuilding NLP-based applications.AcknowledgmentsSpecial thanks to Kenneth Willian, and Rene Law-less.43ReferencesAugustine, S. Managing Agile Projects.
2005.
Publishedby Prentice Hall Professional Technical Reference.ISBN 0131240714, 9780131240711.
229 pages.Degerstedt, L. and J?nsson, A.
2006.
LINTest, A devel-opment tool for testing dialogue systems.
In: Pro-ceedings of the 9th International Conference onSpoken Language Processing (Interspeech/ICSLP),Pittsburgh, USA, pp.
489-492.Leacock, C. and Chodorow, M. 2003.
C-rater: Auto-mated Scoring of Short-Answer Question.
Journal ofComputers and Humanities.
pp.
389-405.Sukkarieh, J.
Z., & Blackmore, J.
To appear.
c-rater:Automatic Content Scoring for Short ConstructedResponses.
To appear in the Proceedings of the 22ndInternational Conference for the Florida Artificial In-telligence Research Society, Florida, USA, May2009.Vanderwende, L. and Dolan, W. B.
2006.
What SyntaxCan Contribute in the Entailment Task.
J. Quinonero-Candela et al (eds.).
Machine Learning Challenges,Lecture notes in computer science, pp.
205-216.Springer Berlin/Heidelberg.44
