Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 310?313,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsAttention and Interaction Control ina Human-Human-Computer Dialogue SettingGabriel SkantzeDept.
of Speech Music and HearingKTH, Stockholm, Swedengabriel@speech.kth.seJoakim GustafsonDept.
of Speech Music and HearingKTH, Stockholm, Swedenjocke@speech.kth.seAbstractThis paper presents a simple, yet effectivemodel for managing attention and interactioncontrol in multimodal spoken dialogue sys-tems.
The model allows the user to switch at-tention between the system and other hu-mans, and the system to stop and resumespeaking.
An evaluation in a tutoring settingshows that the user?s attention can be effec-tively monitored using head pose tracking,and that this is a more reliable method thanusing push-to-talk.1 IntroductionMost spoken dialogue systems are based on theassumption that there is a clear beginning andending of the dialogue, during which the userpays attention to the system constantly.
However,as the use of dialogue systems is extended tosettings where several humans are involved, orwhere the user needs to attend to other thingsduring the dialogue, this assumption is obviouslytoo simplistic (Bohus & Horvitz, 2009).
When itcomes to interaction, a strict turn-taking protocolis often assumed, where user and system wait fortheir turn and deliver their contributions in wholeutterance-sized chunks.
If system utterances areinterrupted, they are treated as either fullydelivered or basically unsaid.This paper presents a simple, yet effectivemodel for managing attention and interactioncontrol in multimodal (face-to-face) spoken dia-logue systems, which avoids these simplifyingassumptions.
We also present an evaluation in atutoring setting where we explore the use of headtracking for monitoring user attention, and com-pare it with a more traditional method: push-to-talk.2 Monitoring user attentionIn multi-party dialogue settings, gaze has beenidentified as an effective cue to help disambi-guate the addressee of a spoken utterance(Vertegaal et al, 2001).
When it comes to hu-man-machine interaction, Maglio et al (2000)showed that users tend to look at speech-controlled devices when talking to them, even ifthey do not have the manifestation of an embo-died agent.
Bakx et al (2003) investigated theuse of head pose for identifying the addressee ina multi-party interaction between two humansand an information kiosk.
The results indicatethat head pose should be combined with acousticand linguistic features such as utterances length.Facial orientation in combination with speech-related features was investigated by Katzenmaieret al (2004) in a human-human-robot interaction,confirming that a combination of cues was mosteffective.
A common finding in these studies isthat if a user does not look at the system whiletalking he is most likely not addressing it.
How-ever, when the user looks at the system whilespeaking, there is a considerable probability thatshe is actually addressing a bystander.3 The MonAMI ReminderThis study is part of the 6th framework IP projectMonAMI1.
The goal of the MonAMI project is todevelop and evaluate services for elderly anddisabled people.
Based on interviews with poten-tial users in the target group, we have developedthe MonAMI Reminder, a multimodal spokendialogue system which can assist elderly and dis-abled people in organising and initiating theirdaily activities (Beskow et al, 2009).
The dia-logue system uses Google Calendar as a back-bone to answer questions about events.
However,1 http://www.monami.info/310it can also take the initiative and give remindersto the user.The MonAMI Reminder is based on the HIG-GINS platform (Skantze, 2007).
The architectureis shown in Figure 1.
A microphone and a cam-era are used for system input (speech recognitionand head tracking), and a speaker and a displayare used for system output (an animated talkinghead).
This is pretty much a standard dialoguesystem architecture, with some exceptions.
Di-alogue management is split into a DiscourseModeller and an Action Manager, which consultsthe discourse model and decides what to do next.There is also an Attention and Interaction Con-troller (AIC), which will be discussed next.Figure 1.
The system architecture in the MonAMIReminder.4 Attention and interaction modelThe purpose of the AIC is to act as a low levelmonitor and controller of the system?s speakingand attentional behaviour.
The AIC uses a state-based model to track the attentional and interac-tional state of the user and the system, shown inFigure 2.
The states shown in the boxes can beregarded as the combined state of the system(columns) and the user (rows)2.
Depending onthe combined state, events from input and outputcomponents will have different effects.
As can beseen in the figure, some combination of statescannot be realised, such as the system and userspeaking at the same time (if the user speakswhile the system is speaking, it will automati-cally change to the state INTERRUPTED).
Ofcourse, the user might speak while the system isspeaking without the system detecting this, but2 This is somewhat similar to the ?engagement state?
usedin Bohus & Horvitz (2009).the model should be regarded from the system?sperspective, not from an observer.The user?s attention is monitored using a cam-era and an off-the-shelf head tracking software.As the user starts to look at the system, the statechanges from NONATTENTIVE to ATTENTIVE.When the user starts to speak, a UserStartSpeakevent from the ASR will trigger a change to theLISTENING state.
The Action Manager mightthen trigger a SystemResponse event (togetherwith what should be said), causing a change intothe SPEAKING state.
Now, if the user would lookaway while the system is speaking, the systemwould enter the HOLDING state ?
the systemwould pause and then resume when the userlooks back.
If the user starts to speak while thesystem is speaking, the controller will enter theINTERRUPTED state.
The Action Manager mightthen either decide to answer the new request,resume speaking (e.g., if there was just a back-channel or the confidence was too low), or abortspeaking (e.g., if the user told the system to shutup).There is also a CALLING state, in which thesystem might try to grab the user?s attention.This is very important for the current applicationwhen the system needs to remind the user aboutsomething.4.1 Incremental multimodal speechsynthesisThe speech synthesiser used must be capable ofreporting the timestamp of each word in thesynthesised string.
These are two reasons for this.First, it must be possible to resume speakingafter returning from the states INTERRUPTED andHOLDING.
Second, the AIC is responsible forreporting what has actually been said by thesystem back to the Discourse Modeller forcontinuous self monitoring (there is a directfeedback loop as can be seen in Figure 1).
Thisway, the Discourse Modeller may relate what thesystem says to what the user says on a highresolution time scale (which is necessary forhandling phenomena such as backchannels, asdiscussed in Skantze & Schlangen, 2009).Currently, the system may pause and resumespeaking at any word boundary and there is nospecific prosodic modelling of these events.
Thesynthesis of interrupted speech is something thatwe will need to improve.GALATEA:Discourse ModellerASRPICKERING:Semantic ParsingMultimodal SpeechSynthesisUtteranceGenerationGoogleCalendarActionManagerAttention and Inter-action ControllerDisplay Microphone CameraHeadTrackerSpeaker311An animated talking head is shown on a display,synchronised with the synthesised speech(Beskow, 2003).
The head is making small con-tinuous movements (recorded from real humanhead movements), giving it a more life-like ap-pearance.
The head pose and facial gestures aretriggered by the different states and events in theAIC, as can be seen in Figure 3.
Thus, when theuser approaches the system and starts to look at it,the system will look up, giving a clear signal thatit is now attending to the user and ready to listen.5 EvaluationIn the evaluation, we not only wanted to checkwhether the AIC model worked, but also to un-derstand whether user attention could be effec-tively modelled using head tracking.
Similarly toOh et al (2002), we wanted to compare ?look-to-talk?
with ?push-to-talk?.
To do this, we used ahuman-human-computer dialogue setting, wherea tutor was explaining the system to a subject(shown in Figure 4).
Thus, the subject needed tofrequently switch between speaking to the tutorand the system.
A second version of the systemwas also implemented where the head trackerwas not used, but where the subject insteadpushed a button to switch between the attentionalstates (a sort-of push-to-talk).
The tutor first ex-plained both versions of the system to the subjectand let her try both.
The tutor gave the subjectshints on how to express themselves, but avoidedto remind them about how to control the atten-tion of the system, as this was what we wanted totest.
After the introduction, the tutor gave thesubject a task where both of them were supposedto find a suitable slot in their calendars to plan adinner or lunch together.
The tutor used a papercalendar, while the subject used the MonAMIReminder.
At the end of the experiment, the tutorinterviewed the subject about her experience ofusing the system.
7 subjects (4 women and 3 men)were used in the evaluation, 3 lab members and 4elderly persons in the target group (recruited bythe Swedish Handicap Institute).There was no clear consensus on which ver-sion of the system was the best.
Most subjectsliked the head tracking version better when itworked but were frustrated when the headtracker occasionally failed.
They reported that acombined version would perhaps be the best,where head pose could be the main method forhandling attention, but where a button or a verbalcall for attention could be used as a fall-back.When looking at the interaction from an objec-tive point of view, however, the head trackingNonAttentiveAttentive SpeakingListeningUserStartLookSystemInitiativeSystemResponseSystemStopSpeakUserStopLookHoldingTimeoutInterruptedUserStartSpeak SystemIgnore (resume)SystemResponse (restart)SystemIgnoreCallingPausingSpeakingAttendingNot attendingAttendingSpeakingNot attendingSystemInitiativeUserStartLookSystemResponseUserStartSpeakUserStopLookUserStartLook (resume)SystemStopSpeakSystemUserSystemAbortSpeakFigure 2.
The attention and interaction model.
Dashed lines indicate events coming from input modules.
Solidlines indicate events from output modules.
Note that some events and transitions are not shown in the figure.NonAttentive Attentive Listening SystemIgnoreFigure 3.
Examples of facial animations triggered bythe different states and events shown in Figure 2.312version was clearly more successful in terms ofnumber of misdirected utterances.
When talkingto the system, the subjects always looked at thesystem in the head tracking condition and neverforgot to activate it in the push-to-talk condition.However, on average 24.8% of all utterancesaddressed to the tutor in the push-to-talk condi-tion were picked up by the system, since the userhad forgotten to deactivate it.
The number of ut-terances addressed to the tutor while looking atthe system in the head tracking condition wassignificantly lower, only 5.1% on average (pairedt-test; p<0.05).These findings partly contradict findings fromprevious studies, where head pose has not beenthat successful as a sole indicator when the useris looking at the system, as discussed in section 2above.
One explanation for this might be that thesubjects were explicitly instructed about how thesystem worked.
Another explanation is the clearfeedback (and entrainment) that the agent?s headpose provided.Two of the elderly subjects had no previouscomputer experience.
During pre-interviews theyreported that they were intimidated by com-puters, and that they got nervous just thinkingabout having to operate them.
However, afteronly a short tutorial session with the spoken in-terface, they were able to navigate through acomputerized calendar in order to find twoempty slots.
We think that having a human tutorthat guides the user through their first interac-tions with this kind of system is very important.One of the tutor?s tasks is to explain why the sys-tem fails to understand out-of-vocabulary ex-pressions.
By doing this, the users?
trust in thesystem is increased and they become less con-fused and frustrated.
We are confident that moni-toring and modelling the user?s attention is a keycomponent of spoken dialogue systems that areto be used in tutoring settings.AcknowledgementsThis research is supported by MonAMI, an IntegratedProject under the European Commission?s 6th Frame-work Program (IP-035147), and the Swedish researchcouncil project GENDIAL (VR #2007-6431).ReferencesBakx, I., van Turnhout, K., & Terken, J.
(2003).
Fa-cial orientation during multi-party interaction withinformation kiosks.
In Proceedings of the Interact2003.Beskow, J., Edlund, J., Granstr?m, B., Gustafson, J.,Skantze, G., & Tobiasson, H. (2009).
TheMonAMI Reminder: a spoken dialogue system forface-to-face interaction.
In Proceedings of Inter-speech 2009.Beskow, J.
(2003).
Talking heads - Models and appli-cations for multimodal speech synthesis.
Doctoraldissertation, KTH, Department of Speech, Musicand Hearing, Stockholm, Sweden.Bohus, D., & Horvitz, E. (2009).
Open-World Dialog:Challenges, Directions, and Prototype.
In Proceed-ings of IJCAI'2009 Workshop on Knowledge andReasoning in Practical Dialogue Systems.
Pasade-na, CA.Katzenmaier, M., Stiefelhagen, R., Schultz, T., Rogi-na, I., & Waibel, A.
(2004).
Identifying the Ad-dressee in Human-Human-Robot Interactionsbased on Head Pose and Speech.
In Proceedings ofICMI 2004.Maglio, P. P., Matlock, T., Campbell, C. S., Zhai, S.,& Smith, B.
A.
(2000).
Gaze and speech in atten-tive user interfaces.
In Proceedings of ICMI 2000.Oh, A., Fox, H., Van Kleek, M., Adler, A., Gajos, K.,Morency, L-P., & Darrell, T. (2002).
EvaluatingLook-to-Talk: A Gaze-Aware Interface in a Col-laborative Environment.
In Proceedings of CHI2002.Skantze, G., & Schlangen, D. (2009).
Incrementaldialogue processing in a micro-domain.
In Pro-ceedings of EACL-09.
Athens, Greece.Skantze, G. (2007).
Error Handling in Spoken Dia-logue Systems ?
Managing Uncertainty, Groundingand Miscommunication.
Doctoral dissertation,KTH, Department of Speech, Music and Hearing,Stockholm, Sweden.Vertegaal, R., Slagter, R., van der Veer, G., & Nijholt,A.
(2001).
Eye gaze patterns in conversations:there is more to conversational agents than meetsthe eyes.
In Proceedings of ACM Conf.
on HumanFactors in Computing Systems.Figure 4.
The human-human-computer dialogue set-ting used in the evaluation.
The tutor is sitting on theleft side and the subject on the right side313
