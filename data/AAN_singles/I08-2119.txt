A Re-examination of Dependency Path Kernels for Relation ExtractionMengqiu WangComputer Science DepartmentStanford Universitymengqiu@cs.stanford.eduAbstractExtracting semantic relations between enti-ties from natural language text is an impor-tant step towards automatic knowledge ex-traction from large text collections and theWeb.
The state-of-the-art approach to rela-tion extraction employs Support Vector Ma-chines (SVM) and kernel methods for classi-fication.
Despite the diversity of kernels andthe near exhaustive trial-and-error on ker-nel combination, there lacks a clear under-standing of how these kernels relate to eachother and why some are superior than oth-ers.
In this paper, we provide an analysis ofthe relative strength and weakness of severalkernels through systematic experimentation.We show that relation extraction can bene-fit from increasing the feature space throughconvolution kernel and introducing bias to-wards more syntactically meaningful featurespace.
Based on our analysis, we proposea new convolution dependency path kernelthat combines the above two benefits.
Ourexperimental results on the standard ACE2003 datasets demonstrate that our new ker-nel gives consistent and significantly betterperformance than baseline methods, obtain-ing very competitive results to the state-of-the-art performance.1 IntroductionThere exists a large body of knowledge embedded inunstructured natural language text on the Web.
Thesheer volume and heterogeneity of such knowledgerenders traditional rule-based and manually-craftedknowledge extraction systems unsuitable.
Thus itcalls for methods that automatically extract knowl-edge from natural language text.
An important steptowards automatic knowledge discovery is to extractsemantic relations between entities.Two types of collections are commonly studiedfor relation extraction.
The first type is annotatednewswire text made available by programs such asMessage Understanding Conferences (MUC) andAutomatic Content Extraction (ACE).
The types ofentities that are of interest to these programs includeperson, organization, facilities, location and GPE(Geo-political entities).
Given entities in a docu-ment, the relation extraction task is to identify ex-plicit semantic relationship such as Located-In andCitizen-Of between pairs of entities.
For example, inthe sentence ?The funeral was scheduled for Thurs-day in Paris at the Saint-Germain-des-Pres Church?,the organization Saint-Germain-des-Pres Church is?Located-In?
GPE Paris.
The second type of collec-tion that has been widely studied is biomedical liter-ature (Bunescu and Mooney, 2005b; Giuliano et al,2006; McDonald et al, 2005b), promoted by evalu-ation programs such as BioCreAtIvE and JNLPBA2004.
In this particular domain, studies often focuson specific entities such as genes and proteins.
Andthe kinds of relations to extract are usually gene-to-protein interactions.The predominant approach to relation extractiontreats the task as a multi-class classification prob-lem, in which different relation types form differ-ent output classes.
Early work employed a diverserange of features in a linear classifier (commonlyreferred to as ?feature-based?
approaches), includ-ing lexical features, syntactic parse features, de-pendency features and semantic features (Jiang andZhai, 2007; Kambhatla, 2004; Zhou et al, 2005).These approaches were hindered by drawbacks suchas limited feature space and excessive feature en-gineering.
Kernel methods (Cortes and Vapnik,1995; Cristianini and Shawe-Taylor, 2000) on theother hand can explore a much larger feature spacevery efficiently.
Recent studies on relation extrac-tion have shown that by combining kernels withSupport-vector Machines (SVM), one can obtain re-sults superior to feature-based methods (Bunescu841and Mooney, 2005b; Bunescu and Mooney, 2005a;Culotta and Sorensen, 2004; Cumby and Roth,2003; Zelenko et al, 2003; Zhang et al, 2006a;Zhang et al, 2006b; Zhao and Grishman, 2005).Despite the large number of recently proposedkernels and their reported success, there lacks a clearunderstanding of their relative strength and weak-ness.
In this study, we provide a systematic com-parison and analysis of three such kernels ?
sub-sequence kernel (Bunescu and Mooney, 2005b), de-pendency tree kernel (Culotta and Sorensen, 2004)and dependency path kernel (Bunescu and Mooney,2005a).
We replicated these kernels and conductedexperiments on the standard ACE 2003 newswiretext evaluation set.
We show that whereas some ker-nels are less effective than others, they exhibit prop-erties that are complementary to each other.
In par-ticular, We found that relation extraction can benefitfrom increasing the feature space through convolu-tion kernel and introducing bias towards more syn-tactically meaningful feature space.Drawn from our analysis, we further proposea new convolution dependency path kernel whichcombines the benefits of the subsequence kernel andshortest path dependency kernel.
Comparing to theprevious kernels, our new kernel gives consistentand significantly better performance than all threeprevious kernels that we look at.2 Related WorkStatistical methods for relation extraction can beroughly categorized into two categories: feature-based and kernel-based.Feature-based methods (Jiang and Zhai, 2007;Kambhatla, 2004; Zhou et al, 2005) use pre-definedfeature sets to extract features to train classifica-tion models.
Zhou et al (2005) manually crafteda wide range of features drawn from sources suchas lexical, syntactic and semantic analyses.
Com-bined with SVM, they reported the best results atthe time on ACE corpus.
Kambhatla (2004) tooka similar approach but used multivariate logistic re-gression (Kambhatla, 2004).
Jiang & Zhai (2007)gave a systematic examination of the efficacy of un-igram, bigram and trigram features drawn from dif-ferent representations ?
surface text, constituencyparse tree and dependency parse tree.One drawback of these feature-based methods isthat the feature space that can be explored is oftenlimited.
On the other hand, kernel-based methodsoffer efficient solutions that allow us to explore amuch larger (often exponential, or in some cases, in-finite) feature space in polynomial time, without theneed to explicitly represent the features.Lodhi et al (2002) described a convolution stringkernel, which measures the similarity between twostrings by recursively computing matching of allpossible subsequences of the two strings.
Bunescu& Mooney (2005b) generalized the string kernel towork with vectors of objects occurred in relation ex-traction.
In a later work also done by Bunescu &Mooney (2005a), they proposed a kernel that com-putes similarities between nodes on the shortest de-pendency paths that connect the entities.
Their ker-nel assigns no-match to paths that are of differentlength.
And for paths that are of the same length, itsimply computes the product of the similarity scoreof node pairs at each index.
The dependency treekernel proposed by Zelenko et al (2003) was alsoinspired by the string kernel of Lodhi et al (2002).Their kernel walks down the parse trees from theroot and computes a similarity score for childrennodes at each depth level using the same subse-quence algorithm as the string kernel.
Culotta &Sorensen (2004) worked on the same idea but ap-plied it to dependency parse trees.
Prior to these twotree kernels, Collins & Duffy (2001) proposed a con-volution tree kernel for natural language tasks.
Theirkernel has since been applied to relation extractionby Zhang et al (2006a).
The tree kernel consid-ers matching of all subtrees that share the sameproduction rule at the root of the subtree.
Zhanget al (2006a) showed results that are significantlybetter than the previous two dependency tree ker-nels.
They obtained further improvements in theirlater paper (2006b) by composing the tree kernelwith a simple entity kernel and raising the compos-ite kernel to polynomial degree 2.
Another study onkernel composition is the work by Zhao & Grish-man (2005).It is worth noting that although there exist stan-dard evaluation datasets such as ACE 2003 and2004, many of the aforementioned work report re-sults on non-standard datasets or splits, making itdifficult to directly compare the performance.
We842feel that there is a sense of increasing confusiondown this line of research.
Although partly due tothe lack of compatibility in evaluation results, webelieve it is more due to the lack of understanding inthe relative strength and weakness of these kernels.Therefore we focus on analyzing and understandingthe pros and cons of different kernels, through sys-tematic comparison and experimentation.3 Kernel Methods for Relation ExtractionIn this Section we first give a very brief introduc-tion to kernel methods.
We then present the al-gorithms behind three kernels that we are particu-larly interested in: subsequence kernel (Bunescu andMooney, 2005b), dependency tree kernel (Culottaand Sorensen, 2004) and shortest path dependencykernel (Bunescu and Mooney, 2005a).3.1 SVM and KernelsSupport-Vector Machines (Cortes and Vapnik, 1995;Cristianini and Shawe-Taylor, 2000) learn to findhyperplanes that separate the positive and negativedata points so that the margin between the support-vector points and the hyperplane is maximized.
Thedual formulation of the optimization problem in-volves only computing the dot product of featurevectors.
This is equivalent to mapping the datapoints into a high dimensional space.
And the sepa-rating plane learnt in the high dimensional space cangive non-linear decision boundaries.
The dot prod-uct of data points can be computed using a kernelfunction K(X,Y ) = ??
(X), ?
(Y )?
for any map-ping function.
A valid kernel function satisfies cer-tain properties: it is symmetric and the Gram matrixG formed by K(X,Y ) is positive semi-definite.3.2 Subsequence KernelThe subsequence kernel introduced in (Bunescuand Mooney, 2005b) is a generalization of thestring kernel first introduced by Lodhi et al (2002).The feature space of the original string kernel?stringkernel is defined as ?stringkernel = ?char,where ?char is simply a set of characters.
Bunescu& Mooney (2005a) re-defined the feature space tobe ?x = ?1??2??
?
??
?k, where ?1,?2, ?
?
?
,?kcan be some arbitray disjoint feature spaces, such asthe set of words, part-of-speech (POS) tags, etc.
Wecan measure the number of common features sharedby two feature vectors x, y ?
?x using functionc(x, y).
Let s, t be two sequences over the featureset ?x, we use |s| to denote the length of s. Thus scan be written out as s1 ?
?
?
s|s|.
We use s[i : j] todenote a continuous subsequence si ?
?
?
sj of s. Leti = (i1, ?
?
?
, i|i|) be a sequence of |i| indices in s,we define the length of the index sequence i to bel(i) = i|i| ?
i1 + 1.
Similarly we have index se-quence j in t of length l(j).Let ??
= ?1 ?
?2 ?
?
?
?
?
?k be the set of allpossible features.
A sequence u ?
???
is a subse-quence of feature vector sequence s if there exists asequence of |u| indices i, such that uk ?
sik , ?k ?
{1, ?
?
?
, |u|}.
Follow the notions in (Bunescu andMooney, 2005b; Cumby and Roth, 2003), we useu ?
s[i] as a shorthand for the above component-wise ???
relationship.
Now we can define the kernelfunction Kn(s, t) to be the total number of weightedcommon subsequence of length n between the twosequeneces s and t.Kn(s, t) =?u??n?
?i:u?s[i]?j:u?t[j]?l(i)+l(j) (1)where ?
is a decaying factor ?
1, penalizing long,sparse subsequence.
We can re-write this kernelfunction asKn(s, t) =?i:|i|=n?j:|j|=nn?k=1c(sik , tjk)?l(i)+l(j) (2)(Bunescu and Mooney, 2005b) showed that us-ing the recursive dynamic programming algorithmfrom (Cumby and Roth, 2003), the kernel Kn(s, t)can be computed in O(kn|s||t |) time.3.3 From Subsequence to Tree KernelsWe will use an example to illustrate the relation be-tween the dependency tree kernels proposed by (Cu-lotta and Sorensen, 2004; Zelenko et al, 2003) andthe subsequence kernel we introduced above.
Con-sider two instances of the ?Located-In?
relations?his actions in Brcko?
and ?his recent arrival in Bei-jing?.
The dependency parse trees of these two sen-tences are shown below.843actionsNNSNOUNhisPRPPERSONinINBrckoNNPNOUNLOCATIONarrivalNNNOUNhisPRPPERSONrecentADJinINBeijingNNPNOUNLOCATIONThe entities in these two relations are the pro-noun mentions of ?his?, and two locations ?Br-cko?
and ?Beijing?, all shown in italic.
The de-pendency tree kernel visits nodes in the two treesstarting from the root.
And at each depth level, ittakes nodes that are at that level and form two se-quences of nodes.
For example, in the example in-stances, nodes at one level below the root formsvectors s=?
{his, PRP, PERSON},{in, IN}?
andt=?
{his,PRP,PERSON},{recent, ADJ},{in, IN}?.
Itthen makes use of the subsequence kernel in theprevious section to compute the total number ofweighted subsequences between these two vectors.The kernel returns the sum of subsequence match-ing scores at each depth level as the final score.3.4 Shortest Path Dependency KernelThe shortest path dependency kernel proposed byBunescu & Mooney (2005a) also works with depen-dency parse trees.
Reuse our example in the previ-ous section, the shortest dependency path betweenentity his and Brcko in the first sentence is s=?
{his,PRP, PERSON}, {actions, NNS, NOUN}, {in, IN},{Brcko, NNP, NOUN, LOCATION}?
; and the pathbetween his and Beijing in the second sentence ist=?
{his, PRP, PERSON}, {arrival, NN, NOUN},{in, IN}, {Beijing, NNP, NOUN, LOCATION}?.Since most dependency parser output connectedtrees, finding the shortest path between two nodesis trivial.
Once the two paths are found, the kernelsimply computes the product of the number of com-mon features between a pair of nodes at each indexalong the path.
If the two paths have different num-ber of nodes, the kernel assigns 0 (no-match) to thepair.
Formally, the kernel is defined as:K(s, t) ={0, if |s| 6= |t|?ni=1 c(si, ti), if |s| = |t|(3)5-fold CV on ACE 2003kernel method Precision Recall F1subsequence 0.703 0.389 0.546dependency tree 0.681 0.290 0.485shortest path 0.747 0.376 0.562Table 1: Results of different kernels on ACE 2003training set using 5-fold cross-validation.4 Experiments and AnalysisWe implemented the above three kernels and con-ducted a set of experiments to compare these ker-nels.
By minimizing divergence in our experimentsetup and implementation for these kernels, we hopeto reveal intrinsic properties of different kernels.4.1 Experiment setupWe conducted experiments using the ACE 2003standard evaluation set.
Training set of this collec-tion contains 674 doc and 9683 relations.
The testset contains 97 doc and 1386 relations.
5 entity types(Person, Organization, Location, Facilities and Geo-political Entities) and 5 top-level relation types (At,Near, Part-of, Role and Social) are manually anno-tated in this collection.
Since no development set isgiven, we report results in this section only on thetraining set, using 5-fold cross-validation, and de-fer the comparison of results on the test set till Sec-tion 6.
Corpus preprocessing is done as the follow-ing: sentence segmentation was performed using thetool from CCG group at UIUC 1; words are then to-kenized and tagged with part-of-speech using MX-POST (Ratnaparkhi, 1996) and dependency parsingis performed using MSTParser (McDonald et al,2005a).
We used the SVM-light (Joachims, 2002)toolkit and augmented it with our custom kernels.SVM parameters are chosen using cross-validation(C=2.4), and the decaying factor in all kernels areuniformally set to be 0.75.
We report precision (P),recall (R) and F-measure (F) on the training (5-foldcross-validation) and test set.4.2 Comparison of KernelsIn table 1 we listed results of the above three kernelson the training set using 5-fold cross-validation.
A1http://l2r.cs.uiuc.edu/?cogcomp/atool.php?tkey=SS844first glimpse of the results tells us that the shortestpath kernel performs the best in terms of F-measure,while the dependency tree kernel did the worst.
Theperformance of subsequence kernel is not as goodas the dependency path kernel, but the difference issmall.
In particular, the subsequence kernel gave thebest recall, whereas the dependency path kernel gavethe highest precision.To understand why shortest path kernel performsbetter than the subsequence kernel, let us review thedefinition of these two kernels.
The subsequencekernel considers all subsequences of feature vectorsequences that are formed by all words occurred in-between two entities in a sentence; while the shortestpath kernel only considers feature vector sequencesformed by words that are connected through a de-pendency path.
In general, the sequences consid-ered in the dependency path kernel are more com-pact than the sequences used in the subsequence ker-nel.
Actually, in most cases the dependency path se-quence is indeed one particular subsequence of theentire subsequence used in subsequence kernel.
Ar-guably, this particular subsequence is the one thatcaptures the most important syntactic information.Although the feature spaces of the dependency pathkernels are not subsets of the subsequence kernel,we can clearly see that we get higher precisionsby introducing bias towards the syntactically moremeaningful feature space.However, the dependency path kernel is fairlyrigid and imposes many hard constraints such as re-quiring the two paths to have exactly the same num-ber of nodes.
This restriction is counter-intuitive.
Toillustrate this, let us reconsider the example given inSection 3.
In that example, it is obviously the casethat the two instances of relations have very similardependency path connecting the entities.
However,the second path is one node longer than the first path,and therefore the dependency path kernel will de-clare no match for them.
The subsequence kernel, onthe other hand, considers subsequence matching andtherefore inherently incorporates a notion of fuzzymatching.
Furthermore, we have observed from thetraining data that many short word sequences carrystrong relational information; hence only part of theentire dependency path is truly meaningful in mostcases.
It also helps to understand why subsequencekernel has better recall than dependency path kernel.ACE 2003 test setkernel method Precision Recall F1subsequence 0.673 0.499 0.586dependency tree 0.621 0.362 0.492shortest path 0.691 0.462 0.577convolution dep.
path 0.725 0.541 0.633(Zhang et al, 2006b) 0.773 0.656 0.709Table 2: Results on the ACE 2003 test set.
We ref-erence the best-reported score (in italic) on this testset, given by (Zhang et al, 2006b)The disappointing performance of the depen-dency tree kernel can also be explained by our anal-ysis.
Although the dependency tree kernel performssubsequence matching for nodes at each depth level,it is unclear what the relative syntactic or semanticrelation is among sibling nodes in the dependencytree.
The sequence formed by sibling nodes is farless intuitive from a linguistic point of view than thesequence formed by nodes on a dependency path.To summarize the above results, we found that de-pendency path kernel benefits from a reduction infeature space by using syntactic dependency infor-mation.
But the subsequence kernel has an edge inrecall by allowing fuzzy matching and expanding thefeature space into convolution space.
We will showin the following section that these two benefits arecomplementary and can be combined to give betterperformance.5 Combining the Benefits ?
A New KernelIt is a natural extension to combine the two bene-fits that we have identified in the previous section.The idea is simple: we want to allow subsequencematching in order to gain more flexibility and there-fore higher recall, but constrain the sequence fromwhich to deduce subsequences to be the dependencypath sequence.
We call the combined kernel a ?con-volution dependency path kernel?.6 Final Test ResultsWe obtained the final results on the test set of theACE 2003 collection, using the same experimentalsetting as above.
The results are listed in Table 2.From the table we can see that the performances ofthe previous three kernels hold up qualitatively on845the test set as cross-validation on training set.
Thereis one exception that the shortest path kernel?s F-measure score is no longer better than the subse-quence kernel on the test set, but the difference issmall.
And our new convolution dependency pathkernel beats all above three kernels in precision, re-call and F-measure, suggesting that our analysis isaccurate and the benefits we outlined are truly com-plementary.Comparing to the best reported results on thesame test set from (Zhang et al, 2006b), our scoresare not as high, but the results are quite competitive,given our minimum efforts on tuning kernel param-eters and trying out kernel combinations.7 ConclusionWe re-examined three existing kernel methods forrelation extraction.
We conducted experiments onthe standard ACE 2003 evaluation set and showedthat whereas some kernels are less effective thanothers, they exhibit properties that are complemen-tary to each other.
In particular, we found that rela-tion extraction can benefit from increasing the fea-ture space through convolution kernel and introduc-ing bias towards more syntactically meaningful fea-ture space.
Drawn from our analysis, we proposeda new convolution dependency path kernel whichcombines the benefits of the subsequence kernel andshortest path dependency kernel.
Comparing withprevious kernels, our new kernel consistently andsignificantly outperforms all three previous kernels,suggesting that our analyses of the previously pro-posed kernels are correct.ReferencesR.
C. Bunescu and R. J. Mooney.
2005a.
A shortest pathdependency kernel for relation extraction.
In Proceed-ings of HLT/EMNLP.R.
C. Bunescu and R. J. Mooney.
2005b.
Subsequencekernels for relation extraction.
In Proceedings ofNIPS.M.
Collins and N. Duffy.
2001.
Convolution kernels fornatural language.
In Proceedings of NIPS.C.
Cortes and V. Vapnik.
1995.
Support-vector networks.Machine Learning, 20(3):273?297.N.
Cristianini and J. Shawe-Taylor.
2000.
An Introduc-tion to Support-vector Machines.
Cambridge Univer-sity Press.A.
Culotta and J. Sorensen.
2004.
Dependency tree ker-nels for relation extraction.
In Proceedings of ACL.C.
M. Cumby and D. Roth.
2003.
On kernel methods forrelation learning.
In Proceedings of ICML.C.
Giuliano, A. Lavelli, and L. Romano.
2006.
Ex-ploiting shallow linguistic information for relation ex-traction from biomedical literature.
In Proceedings ofEACL.J.
Jiang and C. Zhai.
2007.
A systematic exploration ofthe feature space for relation extraction.
In Proceed-ings of NAACL-HLT.T.
Joachims.
2002.
Learning to Classify Text Using Sup-port Vector Machines.
Ph.D. thesis, Universit?
?at Dort-mund.N.
Kambhatla.
2004.
Combining lexical, syntactic andsemantic features with maximum entropy models forextracting relations.
In Proceedings of ACL.H.
Lodhi, C Saunders, J. Shawe-Taylor, N. Cristianini,and C Watkins.
2002.
Text classification using stringkernels.
JMLR, 2:419?444.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
On-line large-margin training of dependency parsers.
InProceedings of ACL.R.
McDonald, F. Pereira, S. Kulick, S. Winters, Y. Jin,and P. White.
2005b.
Simple algorithms for complexrelation extraction with applications to biomedical ie.In Proceedings of ACL.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of EMNLP.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernelmethods for relation extraction.
JMLR, 3:1083?1106.M.
Zhang, J. Zhang, and J. Su.
2006a.
Exploring syntac-tic features for relation extraction using a convolutiontree kernel.
In Proceedings of NAACL-HLT.M.
Zhang, J. Zhang, J. Su, and G. Zhou.
2006b.
A com-posite kernel to extract relations between entities withboth flat and structured features.
In Proceedings ofACL.S.
Zhao and R. Grishman.
2005.
Extraction relationswith integrated information using kernel methods.
InProceedings of ACL.G.
Zhou, S. Jian, J. Zhang, and M. Zhang.
2005.
Ex-ploring various knowledge in relation extraction.
InProceedings of ACL.846
