MULTI -PARAGRAPH SEGMENTATIONEXPOSITORY TEXTMart i  A.
Hears tComputer  Science Division, 571 Evans HallUniversity of California, BerkeleyBerkeley, CA 94720andXerox Palo Alto Research Centermart i  @cs.
berkeley, eduOFAbst rac tThis paper describes TextTiling, an algorithm for parti-tioning expository texts into coherent multi-paragraphdiscourse units which reflect the subtopic structure ofthe texts.
The algorithm uses domain-independent l x-ical frequency and distribution information to recog-nize the interactions of multiple simultaneous themes.Two fully-implemented versions of the algorithm are de-scribed and shown to produce segmentation that corre-sponds well to human judgments of the major subtopicboundaries of thirteen lengthy texts.INTRODUCTIONThe structure of expository texts can be characterizedas a sequence ofsubtopical discussions that occur in thecontext of a few main topic discussions.
For example, apopular science text called Stargazers, whose main topicis the existence of life on earth and other planets, can bedescribed as consisting of the following subdiscussions(numbers indicate paragraph numbers):1-3 Intro - the search for life in space4-5 The moon's chemical composition6-8 How early proximity of the moon shaped it9-12 How the moon helped life evolve on earth13 Improbability of the earth-moon system14-16 Binary/trinary star systems make life un-likely17-18 The low probability of non-binary/trinarysystems19-20 Properties of our sun that facilitate life21 SummarySubtopic structure is sometimes marked in techni-cal texts by headings and subheadings which divide thetext into coherent segments; Brown & Yule (1983:140)state that this kind of division is one of the most basicin discourse.
However, many expository texts consist oflong sequences of paragraphs with very little structuraldemarcation.
This paper presents fully-implemented al-gorithms that use lexical cohesion relations to partitionexpository texts into multi-paragraph segments hat re-flect their subtopic structure.
Because the model of dis-course structure is one in which text is partitioned intocontiguous, nonoverlapping blocks, I call the generalapproach TextTiling.
The ultimate goal is to not onlyidentify the extents of the subtopical units, but to labeltheir contents as well.
This paper focusses only on thediscovery of subtopic structure, leaving determinationof subtopic ontent o future work.Most discourse segmentation work is done at a finergranularity than that suggested here.
However, forlengthy written expository texts, multi-paragraph seg-mentation has many potential uses, including the im-provement ofcomputational t sks that make use of dis-tributional information.
For example, disambiguationalgorithms that train on arbitrary-size text windows,e.g., Yarowsky (1992) and Gale et ai.
(1992b), and al-gorithms that use lexical co-occurrence todetermine se-mantic relatedness, e.g., Schfitze (1993), might benefitfrom using windows with motivated boundaries instead.Information retrieval algorithms can use subtopicstructuring to return meaningful portions of a text ifparagraphs are too short and sections are too long(or are not present).
Motivated segments can also beused as a more meaningful nit for indexing long texts.Salton et al (1993), working with encyclopedia text,find that comparing a query against sections and thenparagraphs i more successful than comparing againstfull documents alone.
I have used the results of Text-Tiling in a new paradigm for information access on full-text documents (Hearst 1994).The next section describes the discourse model thatmotivates the approach.
This is followed by a descrip-tion of two algorithms for subtopic structuring thatmake use only of lexical cohesion relations, the evalua-tion of these algorithms, and a summary and discussion9of future work.THE DISCOURSE MODELMany discourse models assume a hierarchical segmen-tation model, e.g., attentional/intentional structure(Crosz & Sidner 1986) and Rhetorical Structure Theory(Mann ~ Thompson 1987).
Although many aspects ofdiscourse analysis require such a model, I choose to castexpository text into a linear sequence of segments, bothfor computational simplicity and because such a struc-ture is sufficient for the coarse-grained tasks of interesthere.
1-2_ "- -- -- Chainedn l  m i-~__ -- _ / ' -  RingedmMonolithPleeewiseFigure 1: Skorochod'ko's text structure types.
Nodescorrespond to sentences and edges between odes indi-cate strong term overlap between the sentences.Skorochod'ko (1972) suggests discovering a text'sstructure by dividing it up into sentences and seeinghow much word overlap appears among the sentences.The overlap forms a kind of intra-structure; fully con-nected graphs might indicate dense discussions of atopic, while long spindly chains of connectivity mightindicate a sequential account (see Figure 1).
The cen-tral idea is that of defining the structure of a text as afunction of the connectivity patterns of the terms thatcomprise it.
This is in contrast with segmenting guidedprimarily by fine-grained iscourse cues such as registerchange, focus shift, and cue words.
From a computa-tional viewpoint, deducing textual topic structure fromlexical connectivity alone is appealing, both because itis easy to compute, and also because discourse cues aresometimes misleading with respect o the topic struc-ture (Brown & Yule 1983)(?3).1 Additionally, (Passonneau & Litman 1993) concede thedifficulty of eliciting hierarchical intentional structure withany degree of consistency from their human judges.The topology most of interest o this work is the finalone in the diagram, the Piecewise Monolithic Structure,since it represents sequences of densely interrelated is-cussions linked together, one after another.
This topol-ogy maps nicely onto that of viewing documents as asequence of densely interrelated subtopical discussions,one following another.
This assumption, as will be seen,is not always valid, but is nevertheless quite useful.This theoretical stance bears a close resemblance toChafe's notion of The Flow Model of discourse (Chafe1979), in description of which he writes (pp 179-180):Our data .
.
,  suggest hat as a speaker moves fromfocus to focus (or from thought o thought) thereare certain points at which there may be a more orless radical change in space, time, character config-uration, event structure, or, even, world .
.
.
.
Atpoints where all of these change in a maximal way,an episode boundary is strongly present.
But oftenone or another will change considerably while oth-ers will change less radically, and all kinds of var-ied interactions between these several factors arepossible.
2Although Chafe's work concerns narrative text, thesame kind of observation applies to expository text.The TextTiling algorithms are designed to recognizeepisode boundaries by determining where thematiccomponents like those listed by Chafe change in a max-imal way.Many researchers have studied the patterns of occur-rence of characters, setting, time, and the other the-matic factors that Chafe mentions, usually in the con-text of narrative.
In contrast, I attempt o determinewhere a relatively large set of active themes changessimultaneously, regardless of the type of thematic fac-tor.
This is especially important in expository text inwhich the subject matter tends to structure the dis-course more so than characters, setting, etc.
For ex-ample, in the Stargazers text, a discussion of continen-tal movement, shoreline acreage, and habitability givesway to a discussion of binary and unary star systems.This is not so much a change in setting or characteras a change in subject matter.
Therefore, to recognizewhere the subtopic hanges occur, I make use of lexicalcohesion relations (Halliday & Hasan 1976) in a mannersimilar to that suggested by Skorochod'ko.Morris and Hirst's pioneering work on computing dis-course structure from lexical relations (Morris & Hirst1991), (Morris 1988) is a precursor to the work reportedon here.
Influenced by Halliday & Hasan's (1976) the-ory of lexical coherence, Morris developed an algorithmthat finds chains of related terms via a comprehensivethesaurus (Roget's Fourth Edition).
3 For example, the2Interestingly, Chafe arrived at the Flow Model afterworking extensively with, and then becoming dissatisfiedwith, a hierarchical model of paragraph structure like thatof Longacre (1979).3The algorithm is executed by hand since the thesaurus\ ]0Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1 i i i  1 1 1 1 1 1 1I I  1 1 1 1 11 1 11111 22 111112 1 1 1 11 1111 111 1 1 11 1 1 11 1 1 1 1 11 12 1 12 1 1 1 11 11 1 1 21 11111 1 11 1 11 1 11 1 1 11 1 11 1 11 1 1 1 1 1 111 1 113 1111 1 1 22 21 21 21 11 11 1 121121121 1 1 1 1 111 11 1 1 11 11 1 114 form8 scientist5 space II25 star 15 binary4 trinary8 astronomer 17 orb i t  16 pull16 planet 17 galaxy 14 lunar19 l i fe  1 127 moon3 move7 continent3 shoreline6 time3 water6 say3 species1 1Sentence: 05 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 2: Distribution of selected terms from the Stargazer text, with a single digit frequency per sentence number(blanks indicate a frequency of zero).words residential and apartment both index the samethesaural category and can thus be considered to bein a coherence relation with one another.
The chainsare used to structure texts according to the atten-tional/intentional theory of discourse structure (Grosz& Sidner 1986), and the extent of the chains correspondto the extent of a segment.
The algorithm also incorpo-rates the notion of "chain returns" - repetition of termsafter a long hiatus - to close off an intention that spansover a digression.Since the Morris & Hirst (1991) algorithm attemptsto discover attentional/intentional structure, their goalsare different than those of TextTiling.
Specifically, thediscourse structure they attempt o discover is hierar-chical and more fine-grained than that discussed here.Thus their model is not set up to take advantage ofthe fact that multiple simultaneous chains might occurover the same intention.
Furthermore, chains tend tooverlap one another extensively in long texts.
Figure 2shows the distribution, by sentence number, of selectedterms from the Stargazers text.
The first two termshave fairly uniform distribution and so should not beexpected to provide much information about the di-visions of the discussion.
The next two terms occurmainly at the beginning and the end of the text, whileterms binary through planet have considerable overlapis not generally available online.from sentences 58 to 78.
There is a somewhat well-demarked cluster of terms between sentences 35 and 50,corresponding to the grouping together of paragraphs10, 11, and 12 by human judges who have read the text.From the diagram it is evident that simply lookingfor chains of repeated terms is not sufficient for deter-mining subtopic breaks.
Even combining terms that areclosely related semantically into single chains is insuf-ficient, since often several different themes are activein the same segment.
For example, sentences 37 - 51contain dense interaction among the terms move, con-tinent, shoreline, time, species, and life, and all but thelatter occur only in this region.
However, it is the casethat the interlinked terms of sentences 57 - 71 (space,star, binary, trinary, astronomer, orbit ) are closely re-lated semantically, assuming the appropriate senses ofthe terms have been determined.ALGORITHMS FOR D ISCOVERINGSUBTOPIC  STRUCTUREMany researchers (e.g., Halliday ~z Hasan (1976), Tan-hen (1989), Walker (1991)) have noted that term rep-etition is a strong cohesion indicator.
I have found inthis work that term repetition alone is a very useful in-dicator of subtopic structure, when analyzed in termsof multiple simultaneous information threads.
This sec-tion describes two algorithms for discovering subtopic11structure using term repetition as a lexical cohesion in-dicator.The first method compares, for a given window size,each pair of adjacent blocks of text according to howsimilar they are lexically.
This method assumes that themore similar two blocks of text are, the more likely it isthat the current subtopic continues, and, conversely, iftwo adjacent blocks of text are dissimilar, this implies achange in subtopic flow.
The second method, an exten-sion of Morris & Hirst's (1991) approach, keeps trackof active chains of repeated terms, where membershipin a chain is determined by location in the text.
Themethod determines subtopic flow by recording where inthe discourse the bulk of one set of chains ends and anew set of chains begins.The core algorithm has three main parts:1.
Tokenization2.
Similarity Determination3.
Boundary IdentificationTokenization refers to the division of the input textinto individual exical units.
For both versions of thealgorithm, the text is subdivided into psuedosentencesof a pre-defined size w (a parameter of the algorithm)rather than actual syntactically-determined s ntences,thus circumventing normalization problems.
For thepurposes of the rest of the discussion these groupings oftokens will be referred to as token-sequences.
In prac-tice, setting w to 20 tokens per token-sequence worksbest for many texts.
The morphologically-analyzed to-ken is stored in a table along with a record of the token-sequence number it occurred in, and how frequently itappeared in the token-sequence.
A record is also kept ofthe locations of the paragraph breaks within the text.Closed-class and other very frequent words are elimi-nated from the analysis.After tokenization, the next step is the comparisonof adjacent pairs of blocks of token-sequences for over-all lexical similarity.
Another important parameter forthe algorithm is the blocksize: the number of token-sequences that are grouped together into a block to becompared against an adjacent group of token-sequences.This value, labeled k, varies slightly from text to text;as a heuristic it is the average paragraph length (intoken-sequences).
In practice, a value of k = 6 workswell for many texts.
Actual paragraphs are not usedbecause their lengths can be highly irregular, leadingto unbalanced comparisons.Similarity values are computed for every token-sequence gap number; that is, a score is assigned totoken-sequence gap i corresponding to how similar thetoken-sequences from token-sequence i -  k through i areto the token-sequences from i + 1 to i + k + 1.
Note thatthis moving window approach means that each token-sequence appears in k * 2 similarity computations.Similarity between blocks is calculated by a cosinemeasure: given two text blocks bl and bz, each with ktoken-sequences,/E t 2 n ~JJt,bx Et=l ~/)2 t,b~where t ranges over all the terms that have been reg-istered during the tokenization step, and wt,b~ is theweight assigned to term t in block /)I- In this versionof the algorithm, the weights on the terms are simplytheir frequency within the block .4 Thus if the similarityscore between two blocks is high, then the blocks havemany terms in common.
This formula yields a scorebetween 0 and 1, inclusive.These scores can be plotted, token-sequence numberagainst similarity score.
However, since similarity ismeasured between blocks bl and b2, where bl spanstoken-sequences i - k through i and b2 spans i + 1 toi + k + 1, the measurement's z-axis coordinate falls be-tween token-sequences i and i + 1.
Rather than plot-ting a token-sequence number on the x-axis, we plottoken-sequence gap number i.
The plot is smoothedwith average smoothing; in practice one round of aver-age smoothing with a window size of three works bestfor most texts.Boundaries are determined by changes in the se-quence of similarity scores.
The token-sequence gapnumbers are ordered according to how steeply the slopesof the plot are to either side of the token-sequence gap,rather than by their absolute similarity score.
For agiven token-sequence gap i, the algorithm looks at thescores of the token-sequence gaps to the left of i as longare their values are increasing.
When the values to theleft peak out, the difference between the score at thepeak and the score at i is recorded.
The same proce-dure takes place with the token-sequence gaps to theright of i; their scores are examined as long as theycontinue to rise.
The relative height of the peak to theright of i is added to the relative height of the peak tothe left.
(A gap occurring at a peak will have a scoreof zero since neither of its neighbors is higher than it.
)These new scores, called depth scores, corresponding tohow sharp a change occurs on both sides of the token-sequence gap, are then sorted.
Segment boundaries areassigned to the token-sequence gaps with the largestcorresponding scores, adjusted as necessary to corre-spond to true paragraph breaks.
A proviso check isdone that prevents assignment of very close adjacentsegment boundaries.
Currently there must be at leastthree intervening token-sequences between boundaries.This helps control for the fact that many texts havespurious header information and single-sentence para-graphs.The algorithm must determine how many segmentsto assigned to a document, since every paragraph is a4Earlier work weighted the terms according to their fre-quency times their inverse document frequency.
In thesemore recent experiments, imple term frequencies seem towork better.12I1i:ilIOm u  i m m mimmml~mlmmm m ~ Iml l lm l l lm Im Immm~ml~m l l l l l l m m m ~ m  min im m m m l l m l m l m  l~mmm i m ~ m m l l D ~ m l  m~lm~mmm i m m ~ l l m  ~ l m1 .11 l lmlE l l l l l E i l lmB.
.
.
.
.
.
I  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
j '  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
: : : : : .
.
.
.
: .
.
: .
: .
.
.
: .
.
: : :7  .
.
.
.
.
.?
s ~ ~ s e 7 ?
o 1o  11 1~ ?~  1~ , ?
l a  ?7  18  ?
?
~o& 2o ~0' ..' 2o --' 7o' . '
~ , .oFigure 3: Judgments of seven readers on the Stargazer text.
Internal numbers indicate location of gaps betweenparagraphs; x-axis indicates token-sequence gap number, y-axis indicates judge number, a break in a horizontal lineindicates ajudge-specified segment break.o .eo .
l L0 .4o .
io ,1?
014  ~s  i l a|i oI I i / !I o  *o  ~ To  ~ looFigure 4: Results of the block similarity algorithm on the Stargazer text.
Internal numbers indicate paragraphnumbers, x-axis indicates token-sequence gap number, y-axis indicates imilarity between blocks centered at thecorresponding token-sequence gap.
Vertical lines indicate boundaries chosen by the algorithm; for example, theleftmost vertical ine represents a boundary after paragraph 3.
Note how these align with the boundary gaps ofFigure 3 above.potential segment boundary.
Any attempt o make anabsolute cutoff is problematic since there would needto be some correspondence to the document style andlength.
A cutoff based on a particular valley depth issimilarly problematic?I have devised a method for determining the numberof boundaries to assign that scales with the size of thedocument and is sensitive to the patterns of similarityscores that it produces: the cutoff is a function of theaverage and standard eviations of the depth scores forthe text under analysis?
Currently a boundary is drawnonly if the depth score exceeds g - ?r/2.EVALUATIONOne way to evaluate these segmentation algorithms ito compare against judgments made by human readers,another is to compare the algorithms against exts pre-marked by authors, and a third way is to see how wellthe results improve a computational task.
This sectioncompares the algorithm against reader judgments, inceauthor markups are fallible and are usually applied totext types that this algorithm is not designed for, andHearst (1994) shows how to use TextTiles in a task(although it does not show whether or not the resultsof the algorithms used here are better than some otheralgorithm with similar goals).Reader  JudgmentsJudgments were obtained from seven readers for eachof thirteen magazine articles which satisfied the lengthcriteria (between 1800 and 2500 words) 5 and whichcontained little structural demarkation.
The judgesSOne longer text of 2932 words was used since readerjudgments had been obtained for it from an earlier ex-periment.
Judges were technical researchers.
Two textshad three or four short headers which were removed forconsistency.13were asked simply to mark the paragraph boundariesat which the topic changed; they were not given moreexplicit instructions about the granularity of the seg-mentation.Figure 3 shows the boundaries marked by sevenjudges on the Stargazers text.
This format helps il-lustrate the general trends made by the judges andalso helps show where and how often they disagree.For instance, all but one judge marked a boundary be-tween paragraphs 2 and 3.
The dissenting judge didmark a boundary after 3, as did two of the concurringjudges.
The next three major boundaries occur afterparagraphs 5, 9, 12, and 13.
There is some contentionin the later paragraphs; three readers marked both 16and 18, two marked 18 alone, and two marked 17 alone.The outline in the Introduction gives an idea of whateach segment is about.Passonneau & Litman (1993) discuss at length con-siderations about evaluating segmentation algorithmsaccording to reader judgment information.
As Figure 3shows, agreement among judges is imperfect, but trendscan be discerned.
In Passonneau & Litman's (1993)data, if 4 or more out of 7 judges mark a boundary, thesegmentation is found to be significant using a variationof the Q-test (Cochran 1950).
My data showed similarresults.
However, it isn't clear how useful this signifi-cance information is, since a simple majority does notprovide overwhelming proof about the objective real-ity of the subtopic break.
Since readers often disagreeabout where to draw a boundary marking for a topicshift, one can only use the general trends as a basisfrom which to compare different algorithms.
Since thegoals of TextTiling are better served by algorithms thatproduce more rather than fewer boundaries, I set thecutoff for "true" boundaries to three rather than fourjudges per paragraph.
6 The remaining aps are consid-ered nonboundaries.Resu l tsFigure 4 shows a plot of the results of applying the blockcomparison algorithm to the Stargazer text.
When thelowermost portion of a valley is not located at a para-graph gap, the judgment is moved to the nearest para-graph gap.
7 For the most part, the regions of strongsimilarity correspond to the regions of strong agree-ment among the readers.
(The results for this text werefifth highest out of the 13 test texts.)
Note however,that the similarity information around paragraph 12 isweak.
This paragraph briefly summarizes the contentsof the previous three paragraphs; much of the terminol-6Paragraphs of three or fewer sentences were combinedwith their neighbor if that neighbor was deemed to follow at"true" boundary, as in paragraphs 2 and 3 of the Stargazerstext.rThis might be explained in part by (Stark 1988) whoshows that readers disagree measurably about where toplace paragraph boundaries when presented with texts withthose boundaries removed.ogy that occurred in all of them reappears in this onelocation (in the spirit of a Grosz ~; Sidner (1986) "pop"operation).
Thus it displays low similarity both to itselfand to its neighbors.
This is an example of a breakdowncaused by the assumptions about the subtopic struc-ture.
It is possible that an additional pass through thetext could be used to find structure of this kind.The final paragraph is a summary of the entire text;the algorithm recognizes the change in terminologyfrom the preceding paragraphs and marks a boundary;only two of the readers chose to differentiate the sum-mary; for this reason the algorithm is judged to havemade an error even though this sectioning decision isreasonable.
This illustrates the inherent fallibility oftesting against reader judgments, although in part thisis because the judges were given loose constraints.Following the advice of Gale et al (1992a), I comparethe Mgorithm against both upper and lower bounds.The upper bound in this case is the reader judgmentdata.
The lower bound is a baseline algorithm that isa simple, reasonable approach to the problem that canbe automated.
A simple way to segment he texts isto place boundaries randomly in the document, con-straining the number of boundaries to equal that of theaverage number of paragraph gaps assigned by judges.In the test data, boundaries are placed in about 41% ofthe paragraph gaps.
A program was written that placesa boundary at each potential gap 41% of the time (us-ing a random number generator), and run 10,000 timesfor each text, and the average of the scores of these runswas found.
These scores appear in Table 1 (results at33% are also shown for comparison purposes).The algorithms are evaluated according to how manytrue boundaries they select out of the total selected(precision) and how many true boundaries are found outof the total possible (recall) (Salton 1988).
The recallmeasure implicitly signals the number of missed bound-aries (false negatives, or deletion errors); the number offalse positives, or insertion errors, is indicated explic-itly.In many cases the algorithms are almost correct butoff by one paragraph, especially in the texts that the al-gorithm performs poorly on.
When the block similarityalgorithm is allowed to be off by one paragraph, there isdramatic improvement in the scores for the texts thatlower part of Table 2, yielding an overall precision of83% and recall of 78%.
As in Figure 4, it is often thecase that where the algorithm is incorrect, e.g., para-graph gap 11, the overall blocking is very close to whatthe judges intended.Table 1 shows that both the blocking algorithm andthe chaining algorithm are sandwiched between the up-per and lower bounds.
Table 2 shows some of theseresults in more detail.
The block similarity algorithmseems to work slightly better than the chaining algo-rithm, although the difference may not prove significantover the long run.
Furthermore, in both versions of thealgorithm, changes to the parameters of the algorithm14Baseline 33%Baseline 41%ChainsBlocksJudgesPrecision Recall.44 .08 .37 .04'.43 .08 .42 .03.64 .17 .58 .17.66 .18 .61 .13.81 .06 .71 .06Table 1: Precision and Recall values for 13 test texts.perturbs the resulting boundary markings.
This is anundesirable property and perhaps could be remediedwith some kind of information-theoretic formulation ofthe problem.SUMMARY AND FUTURE WORKThis paper has described algorithms for the segmen-tation of expository texts into discourse units that re-flect the subtopic structure of expository text.
I haveintroduced the notion of the recognition of multiple si-multaneous themes, which bears some resemblance to.Chafe's Flow Model of discourse and Skorochod'ko'stext structure types.
The algorithms are fully imple-mented: term repetition alone, without use of thesauralrelations, knowledge bases, or inference mechanisms,works well for many of the experimental texts.
Thestructure it obtains is coarse-grained but generally re-flects human judgment data.Earlier work (Hearst 1993) incorporated thesauralinformation into the algorithms; surprisingly the lat-est experiments find that this information degrades theperformance.
This could very well be due to problemswith the algorithm used.
A simple algorithm that justposits relations among terms that are a small distanceapart according to WordNet (Miller et al 1990) or Ro-get's 1911 thesaurus (from Project Gutenberg), mod-eled after Morris and Hirst's heuristics, might work bet-ter.
Therefore I do not feel the issue is closed, and in-stead consider successful grouping of related words asfuture work.
As another possible alternative Kozima(1993) has suggested using a (computationally expen-sive) semantic similarity metric to find similarity amongterms within a small window of text (5 to 7 words).This work does not incorporate the notion of multi-ple simultaneous themes but instead just tries to findbreaks in semantic similarity among a small numberof terms.
A good strategy may be to substitute thiskind of similarity information for term repetition in al-gorithms like those described here.
Another possibilitywould be to use semantic similarity information as com-puted in Schiitze (1993), Resnik (1993), or Dagan et ai.
(1993).The use of discourse cues for detection of segmentboundaries and other discourse purposes has been ex-tensively researched, although predominantly on spo-ken text (see Hirschberg & Litman (1993) for a sum-mary of six research groups' treatments of 64 cuewords).
It is possible that incorporation of such in-formation may provide a relatively simple way improvethe cases where the algorithm is off by one paragraph.AcknowledgmentsThis paper has benefited from the comments of GraemeHirst, Jan Pedersen, Penni Sibun, and Jeff Siskind.
Iwould like to thank Anne Fontaine for her interest andhelp in the early stages of this work, and Robert Wilen-sky for supporting this line of research.
This work wassponsored in part by the Advanced Research ProjectsAgency under Grant No.
MDA972-92-J-1029 with theCorporation for National Research Initiatives (CNRI),and by the Xerox Palo Alto Research Center.Re ferencesBROWN, GILLIAN, ~ GEORGE YULE.
1983.
Discourse Anal-ysis.
Cambridge Textbooks in Linguistics Series.
Cam-bridge University Press.CHAFE, WALLACE L. 1979.
The flow of thought and theflow of language.
In Syntax and Semantics: Discourseand Syntax, ed.
by Talmy Giv6n, volume 12, 159-182.Academic Press.COCrmAN, W. G. 1950.
The comparison of percentages inmatched samples.
Biometrika 37.256-266.DAGAN, IDO, SHAUL MARCUS, & SHAUL MARKOVITCH.1993.
Contextual word similarity and estimation fromsparse data.
In Proceedings of the 31th Annual Meet-ing of the Association for Computational Linguistics,164-171.GALE, WILLIAM A., KENNETH W. CHURCH, &: DAVIDYAROWSKY.
1992a.
Estimating upper and lowerbounds on the performance ofword-sense disambigua-tion programs.
In Proceedings of the 30th Meeting ofthe Association for Computational Linguistics, 249-256.- - ,  , & .
1992b.
A method for disambiguatingword senses in a large corpus.
Computers and the Hu-manities 5-6.415-439.GRosz, BARBARA J., &: CANDACE L. SIDNER.
1986.
Atten-tion, intention, and the structure of discourse.
Compu-tational Linguistics 12.172-204.HALLIDAY, M. A. K., & R. HASAN.
1976.
Cohesion inEnglish.
London: Longman.HEARST, MARTI A.
1993.
TextTiling: A quantitative ap-proach to discourse segmentation.
Technical ReportSequoia 93/24, Computer Science Department, Univer-sity of California, Berkeley., 1994.
Context and Structure in Automated Full-TextInformation Access.
University of California at Berke-ley dissertation.
(Computer Science Division TechnicalReport).HIRSCHBERG, JULIA, ~: DIANE LITMAN.
1993.
Empiricalstudies on the disambiguation of cue phrases.
Compu-tational Linguistics 19.501-530.15Text1 92 93 94 125 86 87 98 89 910 811 912 913 10Total Baseline 41% (avg) Blocks Chains Judges (avg)Possible Prec Rec C I Prec Rec C I Prec Rec C Rec C I.44 .44 4 5.50 .44 4 4.40 .44 4 6.63 .42 5 3.43 .38 3 4.40 .38 3 9.36 .44 4 7.43 .38 3 4.36 .44 4 7.50 .38 3 3.36 .44 4 ?.44 .44 4 5.36 .40 4 71.0 .78 7 0.88 .78 7 1.78 .78 7 2.86 .50 6 1.70 .75 6 2.60 .75 6 3.60 .56 5 3.50 .63 5 4.50 .44 4 3.50 .50 4 3.50 .44 4 4.50 .56 5 5.30 .50 5 91.0 .78 7.75 .33 3.56 .56 5.56 .42 5.86 .75 6.42 .63 5.40 .44 4.67 .75 6.60 .33 3.63 .63 5.71 .56 5.54 .78 7.60 .60 6I Prec0 .781 .884 .754 .911 .868 .756 .753 .862 .753 .862 .756 .864 .78.78 7 2.78 7 1.67 6 2.83 10 1.75 6 1.75 6 2.67 6 2.75 6 1.67 6 2.75 6 1.67 6 2.67 6 1.70 7 2Table 2: Scores by text, showing precision and recall.
(C) indicates the number of correctly placed boundaries, (I)indicates the number of inserted boundaries.
The number of deleted boundaries can be determined by subtracting(C) from Total Possible.KOZIMA, HIDEKI.
1993.
Text segmentation based on similar-ity between words.
In Proceedings of the 31th AnnualMeeting of the Association for Computational Linguis-tics, 286-288, Columbus, OH.LONGACRE, R. E. 1979.
The paragraph as a grammaticalunit.
In Syntax and Semantics: Discourse and Syntax,ed.
by Talmy Givdn, volume 12, 115-134.
AcademicPress.MANN, WILLIAM C., & SANDRA A. THOMPSON.
1987.Rhetorical structure theory: A theory of text organi-zation.
Technical Report ISI/RS 87-190, ISI.MILLER, GEORGE A., RICHARD BECKWITH, CHRISTIANEFELLBAUM, DEREK GROSS, ~ KATHERINE J. MILLER.1990.
Introduction to WordNet: An on-line lexicaldatabase.
Journal of Lexicography 3.235-244.MoPmIS, JANE.
1988.
Lexical cohesion, the thesaurus,and the structure of text.
Technical Report CSRI-219, Computer Systems Research Institute, Universityof Toronto.- - ,  ~z GRAEME HIRST.
1991.
Lexical cohesion computedby thesaural relations as an indicator of the structureof text.
Computational Linguistics 17.21-48.PASSONNEAU, REBECCA J., ~z DIANE J. LITMAN.
1993.Intention-based segmentation: Human reliability andcorrelation with linguistic cues.
In Proceedings of the31st Annual Meeting of the Association for Computa-tional Linguistics, 148-155.RESNIK, PHILIP, 1993.
Selection and Information: A Class-Based Approach to Lexical Relationships.
University ofPennsylvania dissertation.
(Institute for Research inCognitive Science report IRCS-93-42).SALTON, GERARD.
1988.
Automatic text processing : thetransformation, analysis, and retrieval of informationby computer.
Reading, MA: Addison-Wesley.- - ,  JAMES ALLAN, ~ CHRIS BUCKLEY.
1993.
Ap-proaches to passage retrieval in full text informationsystems.
In Proceedings of the 16th Annual Inter-national ACM/SIGIR Conference, 49-58, Pittsburgh,PA.SCHUTZE, HINRICH.
1993.
Word space.
In Advancesin Neural Information Processing Systems 5, ed.
byStephen J. Hanson, Jack D. Cowan, & C. Lee Giles.San Mateo CA: Morgan Kaufmann.SKOROCHOD'KO, E.F. 1972.
Adaptive method of automaticabstracting and indexing.
In Information Processing71: Proceedings of the IFIP Congress 71, ed.
by C.V.Freiman, 1179-1182.
North-Holland Publishing Com-pany.STARK, HEATHER.
1988.
What do paragraph markers do?Discourse Processes 11.275-304.TANNEN, DEBORAH.
1989.
Talking Voices: Repetition, dia-logue, and imagery in conversational discourse.
Studiesin Interactional Sociolinguistics 6.
Cambridge Univer-sity Press.WALKER, MARILYN.
1991.
Redundancy in collaborative dia-logue.
In AAAI  Fall Symposium on Discourse Structurein Natural Language Understanding and Generation,ed.
by Julia Hirschberg, Diane Litman, Kathy McCoy,& Candy Sidner, Pacific Grove, CA.YAROWSKY, DAVID.
1992.
Word sense disambiguation us-ing statistical models of Roget's categories trained onlarge corpora.
In Proceedings of the Fourteenth Interna-tional Conference on Computational Linguistics, 454-460, Nantes, France.16
