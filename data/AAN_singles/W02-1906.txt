Passage Selection to Improve Question AnsweringFernando LLopisDepartamento de Lenguajes ySistemas Inform?ticosAlicante (Spain) 03800llopis@dlsi.ua.esJos?
Luis VicedoDepartamento de Lenguajes ySistemas Inform?ticosAlicante (Spain) 03800vicedo@dlsi.ua.esAntonio Ferr?ndezDepartamento de Lenguajes ySistemas Inform?ticosAlicante (Spain) 03800antonio@dlsi.ua.esAbstractOpen-Domain Question Answering systems(QA) performs the task of detecting textfragments in a collection of documents thatcontain the response to user?s queries.These systems use high complexity tools thatreduce its applicability to the treatment ofsmall amounts of text.
Consequently, whenworking on large document collections, QAsystems apply Information Retrieval (IR)techniques to reduce drastically textcollections to a tractable quantity ofrelevant text.
In this paper, we propose anovel Passage Retrieval (PR) model thatperforms this task with better performancefor QA purposes than current best IRsystems1 IntroductionInformation Retrieval (IR) systems receive asinput a user?s query, and they have to return aset of documents sorted by their relevance to thequery.
There are different techniques to carryout the document extraction process, but most ofthem are based on pattern matching modules thatdepend on the number of times that a query termappear in each document, as well as theimportance or discrimination value of each termin the document collection.
Question Answering(QA) systems try to improve the outputgenerated by IR systems by means of returningjust small pieces of text that are supposed tocontain the response.
Usually, QA systemscombine IR and Natural Language Processing(NLP) techniques to perform their task.
Thiscombination allows text understanding until aminimum level that permits a precise answerdetection and extraction.
Nevertheless, sinceNLP techniques are computationally expensive,QA systems need to reduce the amount of textwhere these techniques have to be applied.
Inthis way, they usually work on the output of IRsystems [10] that select the most relevantdocuments to the query by supposing that theywill contain the answer required.
Most appliedIR systems are mainly based on three models:the cosine model [15], the pivoted cosine model1[17], and the probabilistic model (OKAPI [18]).Moreover, IR systems usually employ queryexpansion techniques that frequently improvetheir precision.
These techniques can be basedon thesaurus [21] or on the incorporation of themost frequent terms in the top M relevantdocuments [7].Currently, several Passage Retrieval (PR)systems have also been proposed for this task[2][5][8][9].
PR systems deal with fragments oftext in order to determine the relevance of adocument to a query, as well as to detectdocument extracts that are likely to contain theexpected answer (instead of full documents).Although PR systems apply IR-based techniquesto perform their work, they have revealed to bemore effective than IR systems for QA tasks.In this paper, we are analysing the importance ofthe IR-n PR system for QA n [11] as it was usedin last TREC-10 Conference [19].
The followingsection briefly presents the backgrounds in IR,PR and QA.
Section 3 shows the architecture ofIR-n.
Section 4 presents the evaluationaccomplished and finally, section 5 detailsconclusions and work in progress.1 It is a modification of the cosine model.
It tries toreduce the problem of the preference for biggerdocuments.2 Backgrounds in Question Answering andPassage Retrieval2.1 Information Retrieval and Passage RetrievalGiven a question, an IR system sorts thedocuments by its relevance to the query.
Itcomputes the similarity between each documentand the question by taking into account thefrequency of each query term in the document.This fact usually produces that biggerdocuments are preferred.
A possible alternativeto IR models is based on obtaining the similarityin accordance with the relevance of the passagescontained in the document.
This new approach,called Passage Retrieval (PR), has severaladvantages.
When used for document retrieval,as the relevance of a document will depend onthe relevance of the passages it contains, thismeasure will not be affected by the length of thefull document.
Moreover, these techniques allowto detect high relevant information embedded ina long document obtaining, this way, betterperformance than IR approaches [2][9].
On theother hand, when applied for QA tasks, PRsystems allow reducing the amount of text to beprocessed with costly NLP tools by returningpassages instead of whole documents.Two classifications can be accomplished in PR.The first one is in accordance with the way ofdividing the documents into passages.
Thesecond one is in accordance with the moment inwhich the passage segmentation is carried out.With reference to the first one, PR communitygenerally agrees with the classification proposedin [2], where the author distinguishes betweendiscourse models, semantic models, and windowmodels.
The first one uses the structuralproperties of the documents, such as sentencesor paragraphs [13][16] in order to define thepassages.
The second one divides eachdocument into semantic pieces according to thedifferent topics in the document [5].
The last oneuses windows of a fixed size (usually a numberof terms) to determine passage boundaries [2][8].At first glance, we could think that discourse-based models would be the most effective, inretrieval terms, since they use the structure ofthe document itself.
However, this modelgreatest problem relies on detecting passageboundaries since it depends on the writing styleof the author of each document.
On the otherhand, window models have as main advantagethat they are simpler to accomplish, since thepassages have a previously known size, whereasthe remaining models have to bear in mind thevariable size of each passage.
Nevertheless,discourse-based and semantic models have themain advantage that they return full informationunits of the document, which is quite importantif these units are used as input by otherapplications such as QA.According to the second classification, we candistinguish between approaches that segmentdocuments into passages for indexing purposes,and those that perform segmentation after thequery is posed.
The first one allows a quickercalculation; nevertheless, the second one allowsdifferent segmentation models in accordancewith the kind of query.The passage extraction model that we propose(IR-n) allows us to benefit from the advantagesof discourse-based models since self-containedinformation units of text, such as sentences, areused for building passages.
Moreover, anothernovel proposal in our PR system is the relevancemeasure which, unlike other discourse-basedmodels, is not based on the number of passageterms, but on a fixed number of passagesentences.
This fact allows a simpler calculationof this measure unlike other discourse-based orsemantic models.
Although each passage ismade up by a fixed number of sentences, weconsider that our proposal differs from thewindow models since our passages do not have afixed size (i.e.
a fixed number of words) sincewe use sentences with a variable size.Furthermore, IR-n document segmentation intopassages is accomplished after the query isposed, which allows us to determine the numberof sentences to be considered in accordance withthe kind of the query.2.2 Question AnsweringOpen domain QA systems are defined as toolscapable of extracting the answer to user queriesdirectly from unrestricted domain documents.
Orat least, systems that can extract text snippetsfrom texts, from whose content it are possible toinfer the answer to a specific question.
In bothcases, these systems try to reduce the amount oftime users spend to locate a concreteinformation.Interest in QA systems is quite recent.
We hadlittle information about this kind of systems untilthe ?First Question Answering Track?
was heldin TREC-8 Conference.
This track tries tobenefit from large-scale evaluation that waspreviously carried out on IR systems, inprevious TREC conferences.If a QA system wants to successfully obtain auser?s request, it needs to understand both textsand questions to a minimum level.
From alinguistic perspective, ?understanding?
means tocarry out many of the typical steps on naturallanguage analysis: lexical, syntactic andsemantic.
This analysis takes much more timethan the statistical analysis that is usually carriedout in IR.
Besides, as QA systems have tomanage with as much text as done for IR tasks,and the user needs the answer in a limitedinterval of time, it is nearly mandatory that first,an IR system processes the query and second,the QA process continues with its output.
In thisway, the time of analysis is highly decreased.The analysis of current best systems [3] [4] [14][6] allows identifying main QA sub-componentswhere document retrieval is accomplished byusing IR technology:?
Question Analysis.?
Document Retrieval.?
Passage Selection.?
Answer Extraction.3 IR-n overviewIn this section, we describe the architecture ofthe proposed PR system, namely IR-n, focusingon its three main modules: indexing, passageretrieval and query expansion.3.1 Indexing moduleThe main aim of this module is to generate thedictionaries that contain all the requiredinformation for the passage retrieval module.
Itrequires the following information for eachterm:?
The number of documents that containthis term.?
For each document:?
The number of times this termappears in the document.?
The position of each term in thedocument represented as the numberof sentence it appears in.As term, we consider the stem produced by thePorter stemmer on those words that do notappear in a list of stop-words, list that is similarto those generally used for IR.
On the otherhand, query terms are also extracted in the sameway, that is to say, we only consider the stems ofquery words that do not appear in the stop-wordslist.3.2 Passage retrieval moduleThis module extracts the passages according toits similarity with the user?s query.
The schemein this process is the following:1.
Query terms are sorted according to thenumber of documents they appear in.
Terms thatappear in fewer documents are processed firstly.2.
The documents that contain any query termare selected.3.
The following similarity measure is calculatedfor each passage p (contained in the selecteddocuments) with the query q:Similarity_measure(p, q) = ?
??
qpt tq,tp, W?WWp,t = loge( fp,t + 1).Wq, t= loge( fq,t + 1) ?
idfidf  = loge( N / ft + 1)Where fp,t  is the number of times that the term tappears in the passage p. fq,t  represents thenumber of times that the term t appears in thequery q. N is the number of documents in thecollection and ft is refers to the number ofdocuments that contain the term t.4.
Only the most relevant passage of eachdocument is selected for retrieval.5.
The selected passages are sorted by theirsimilarity measure.6.
Passages are associated with the documentthey pertain and they are presented in a rankedlist form.As we can notice, the similarity measure issimilar to the cosine measure presented in [15].The only difference is that the size of eachpassage (the number of terms) is not used tonormalise the results.
This proposal performsnormalization according to the fixed number ofsentences per passage.
This difference makes thecalculation simpler than other discourse-basedPR or IR systems.
Another important detail toremark is that we are using N as the number ofdocuments in the collection, instead of thenumber of passages according to theconsiderations presented in [9].As it has been commented, our PR system usesvariable-sized passages that are based on a fixednumber of sentences (with different number ofterms per passage).
The passages overlap eachother, that is to say, if a passage contains Nsentences, the first passage will be formed by thesentences from 1 to N, the second one from 2 toN+1, and so on.
We decided to overlap just onesentence according to the experiments andresults presented in [12].
This work studied theoptimum number of overlapping sentences ineach passage for retrieval purposes concluding,that best results were obtained when only oneoverlapping sentence was used.
Regarding to theoptimum number (N) of sentences per passageconsidered in this paper, it will beexperimentally obtained.4 EvaluationThis section presents the experiments developedfor training and evaluating our approach.
Theexperiments have been run on the TREC-9 QATrack question set and document collections.4.1 Data collectionTREC-9 question test set is made up by 682questions with answers included in thedocument collection.
The document set consistsof 978,952 documents from the TIPSTER andTREC following collections: AP Newswire,Wall Street Journal, San Jose Mercury News,Financial Times, Los Angeles Times, ForeignBroadcast Information Service.4.2 TrainingTraining experiments had two objectives.
Theywere designed (1) to calculate the optimumnumber of sentences (N) that define passagelength and (2) to test two different possible waysof applying our method.First training experiment consists of working onthe output of one of the current best performingIR systems (the ATT system).
This experimentre-sorts its output (the first 1,000 rankeddocuments) by using IR-n. Second experimentconsists of using our proposal as the main IRsystem, that is, indexing the whole collectionsby means of IR-n. For each experiment, adifferent number of sentences per passage weretested: 5, 10, 15 and 20 sentences.
The relevanceof each returned document was measured bymeans of the tool provided by TRECorganization that allows us to determine if apassage contains the right answer.
The twoexperiments are summed up in Figure 1.ATTIR-systemDocumentsQuestionsIR-n systemQA system1000 morerelevantdocuments200 morerelevantpassagesDocumentsIR-n system200 more relevantpassagesAnswersFigure 1.
Training ExperimentsThese experiments were performed using onlythe first 100 questions included in the datacollection.
Table 1 shows training results forpassages of 5, 10, 15 and 20 sentences usingboth approaches.
This results measure thenumber of questions whose correct answer wasincluded into the top n retrieved passages (ordocuments) for the training question set.
Thefirst experiment (IR-n Ref) uses IR-n on the1,000 documents returned by ATT system whilethe second one (IR-n) applies passage retrievaloverall collections.As we can see, IR-n Ref and IR-n test obtainsimilar results although using our approach tore-rank the output of a good IR system presentsa slight better performance than applying IR-noverall document collection.
Regarding to thenumber of sentences to be taken into account todefine passage length, we can observe that bestresults are obtained with passages of 20sentences.
In this case, both tests improvesignificantly the performance of ATT-system.
Itranges from 12 (IR-n Ref) and 10 (IR-n) pointson a passage length of 20 sentences (for only thefirst 5 documents retrieved) to 8 and 7 pointswhen the first 200 documents are taken intoaccount respectively.AnswerincludedAt5docsAt10docsAt20docsAt30docsAt50docsAt100docsAt200docsIR-n Ref.5 Sent 57 66 78 83 85 88 9310 Sent 63 76 80 89 93 96 9715 Sent 70 78 83 89 94 95 9620 Sent 74 83 87 91 93 96 97IR-n5 Sent 55 63 75 80 84 89 9010 Sent 60 73 78 87 92 95 9715 Sent 70 76 82 87 93 95 9520 Sent 72 80 86 90 92 96 96ATT system62 69 77 82 83 87 89Table 1.
Number of questions rightly answered(training set of 100 questions).4.3 ExperimentIn order to evaluate our proposal we decided tocompare the quality of the information retrievedby our approaches with the ranked list retrievedby the ATT IR system.
For this evaluation, the682 questions included in the data collectionwere processed and the number N of sentencesper passage was set to 20.
Table 2 shows theresults of this evaluation experiment.
This tableshows the percentage of questions whose answercan be found into the first n documents returnedby the ATT IR system and the best n passagesreturned by IR-n Ref and IR-n respectively.These results are also presented in Figure 2These data confirm training results.
In this case,both approaches perform better than ATTsystem and improvements range form 6 to 12points for 20 sentences passage length.AnswerIncludedATTsystem IR-n Ref IR-nAt 5 docs 64.90% 74.59% 72.21%At 10 docs 70.33% 82.73% 80.37%At 20 docs 75.91% 87.37% 86.35%At 30 docs 79.14% 89.96% 89.31%At 50 docs 83.70% 91.62% 91.52%At 100 docs 87.37% 94.56% 95.55%At 200 docs 90.01% 96.03% 95.92%Table 2.
ATT-system versus IR-n systems.60657075808590951005 10 20 30 50 100 200Number of documents%QuestionsIR-n Ref IR-n ATT systemFigure 2.
Comparative of ATT-system andexperiments with IR-n (Passages of 20 sentences)5 Conclusions and future worksIn this paper, we have analysed the improvementobtained by our passage retrieval system, calledIR-n, with reference to a high-performance IRsystem (ATT) regarding to is application for QAtasks.
This improvement has been evaluated onthe TREC-9 QA track data set.
The achievedimprovements are twofold: First, our approachobtains a better precision by retrieving morepassages that contain the answer to users?queries than ATT system does.
Second, sinceour approach returns passages (instead ofdocuments), it significantly reduces the amountof text to be processed with costly techniques bythe QA system.
The related experiments showthat the optimal passage length for this task is 20when passages are made up by a fixed numberof sentences.
Moreover, we have tested twodifferent ways of applying our model.
As wehave seen, IR-n presents similar results when itworks on the output of an IR system, than whenit works on the whole collections.
Nevertheless,in both cases, benefits range from 6 to 12 pointswith reference to ATT system depending on thenumber of first documents or passages retrievedto be processed for QA tasks.As future work, in order to improve our systemprecision, we intend to obtain the optimum sizeof passages in accordance with the kind ofquestion.
Besides, we need to investigate theeffects of query expansion techniques on IR-nsystem.
Furthermore, we are also trying toimprove the relationship between IR-n and thefollowing QA system, in order to detect theminimum number of passages to extract for eachquery without affecting QA performance.References[1] Bertoldi, N and Federico, M. ITC-irst at CLEF-2001 ,Working Notes for the Clef 2001 Darmstdt, Germany ,pp  41-44[2] Callan, J. Passage-Level Evidence in DocumentRetrieval.
In Proceedings of the 17 th Annual ACMSIGIR Conference on Research and Development inInformation Retrieval, Dublin, Ireland   1994, pp.
302-310.
[3] Clarke, C.; Cormack, g, Kisman, D and Lynam, T.Question Answering by Passage Selection(MultitextExperiments for TREC-9) Proceedings of the TenthText REtrieval Conference, TREC-9.
Gaithersburg ,USA 2000, pp 673-683[4] Harabagiu, S.; Moldovan, D.; Pasca, M.; Mihalcea, R.;Surdeanu, M.; Bunescu, R.; G?rju, R.; Rus, V. andMorarescu, P. FALCON: Boosting Knowledge forAnswer Engines.
In Nineth Text REtrieval Conference,Gaithersburg  USA 2000.pp 479-[5] Hearst, M. and Plaunt, C. Subtopic structuring for full-length document access.
Proceedings of the SixteenthAnnual International ACM SIGIR Conference onResearch and Development in Information Retrieval,Pittsburgh, PA USA 1993 , pp 59-68[6] Ittycheriah, A.; Franz, M.; Zu, W. and Ratnaparkhi, A.IBM's Statistical Question Answering System.
InNineth Text REtrieval Conference, Gaithersburg  USA2000., pp 231-236[7] J. Xu and W. Croft.
Query expansion using local andglobal document analysis.
In Proceedings of the 19thAnnual International ACM SIGIR, Zurich,Switzerland,  1996 pp 4?11, 18?22.
[8] Kaskiel, M. and  Zobel, J.
Passage Retrieval RevisitedSIGIR '97: Proceedings of the 20th AnnualInternational ACM  Philadelphia, PA 1997, USA, pp27-31[9] KaszKiel, M. and  Zobel, J.
Effective Ranking withArbitrary Passages.
Journal of the American Societyfor Information Science, Vol 52, No.
4, February2001, pp 344-364.
[10] Litkowski, k, Syntactic Clues and Lexical Resourcesin Question-Answering In Nineth Text REtrievalConference,  Gaithersburg  USA  2000  pp177-188[11] Llopis,  F. and  Vicedo, J.  Ir-n system, a passageretrieval system  at CLEF 2001  Working Notes forthe Clef 2001 Darmstdt, Germany  2001, pp  115-120 .To appear in Lecture Notes in Computer Science[12] Llopis,  F.; Ferr?ndez, and  Vicedo, J.   TextSegmentation for efficient Information Retrieval ThirdInternational Conference onIntelligent Text Processing andComputational Linguistics.
Mexico 2002 To appear inLecture Notes in Computer Science[13] Namba, I Fujitsu Laboratories TREC9 Report.Proceedings of the Nineth Text REtrieval Conference,TREC-9.
Gaithersburg,USA.2000, pp 203-208[14] Prager, J.; Brown, E.; Radev, D. and Czuba, K. OneSearch Engine or Two for QuestionAnswering.
InNineth Text REtrieval Conference,Gaithersburg,USA.
2000.
[15] Salton G.  Automatic Text Processing: TheTransformation, Analysis, and Retrieval ofInformation by Computer, Addison WesleyPublishing, New York.
1989[16] Salton, G.;  Allan, J. Buckley Approaches to passageretrieval in full text information systems.
In RKorfhage, E Rasmussen & P Willet (Eds.
)Prodeedings of the 16 th annual international ACM-SIGIR conference on research and development ininformation retrieval.
Pittsburgh PA USA , pp 49-58[17] Singhal, A.; Buckley, C. and  Mitra, M. Pivoteddocument length normalization.
Proceedings of the19th annual international ACM- 1996.
[18] Venner, G. and Walker, S. Okapi '84: `Best match'system.
Microcomputer networking in libraries II.Vine, 48,1983, pp 22-26.
[19] Vicedo, J.; Ferrandez, A and Llopis, F. University ofAlicante al TREC-10.
In Tenth Text REtrievalConference, Gaithersburg,USA.
2001[20] Vicedo, J.; Ferrandez, A; A semantic approach toQuestion Answering systems.
In Nineth Text REtrievalConference, 2000  pp 440-444.
[21] Y. Jing and W. B. Croft.
An association thesaurus forinformation retrieval.
In RIAO 94 ConferenceProceedings, , New York, 1994. pp 146--160
