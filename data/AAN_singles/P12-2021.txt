Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 105?109,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsRobust Conversion of CCG Derivations to Phrase Structure TreesJonathan K. Kummerfeld?
Dan Klein?
James R.
Curran?
?Computer Science Division ?
e-lab, School of ITUniversity of California, Berkeley University of SydneyBerkeley, CA 94720, USA Sydney, NSW 2006, Australia{jkk,klein}@cs.berkeley.edu james@it.usyd.edu.auAbstractWe propose an improved, bottom-up methodfor converting CCG derivations into PTB-stylephrase structure trees.
In contrast with pastwork (Clark and Curran, 2009), which usedsimple transductions on category pairs, our ap-proach uses richer transductions attached tosingle categories.
Our conversion preservesmore sentences under round-trip conversion(51.1% vs. 39.6%) and is more robust.
In par-ticular, unlike past methods, ours does not re-quire ad-hoc rules over non-local features, andso can be easily integrated into a parser.1 IntroductionConverting the Penn Treebank (PTB, Marcus et al,1993) to other formalisms, such as HPSG (Miyaoet al, 2004), LFG (Cahill et al, 2008), LTAG (Xia,1999), and CCG (Hockenmaier, 2003), is a com-plex process that renders linguistic phenomena informalism-specific ways.
Tools for reversing theseconversions are desirable for downstream parser useand parser comparison.
However, reversing conver-sions is difficult, as corpus conversions may lose in-formation or smooth over PTB inconsistencies.Clark and Curran (2009) developed a CCG to PTBconversion that treats the CCG derivation as a phrasestructure tree and applies hand-crafted rules to ev-ery pair of categories that combine in the derivation.Because their approach does not exploit the gener-alisations inherent in the CCG formalism, they mustresort to ad-hoc rules over non-local features of theCCG constituents being combined (when a fixed pairof CCG categories correspond to multiple PTB struc-tures).
Even with such rules, they correctly convertonly 39.6% of gold CCGbank derivations.Our conversion assigns a set of bracket instruc-tions to each word based on its CCG category, thenfollows the CCG derivation, applying and combin-ing instructions at each combinatory step to build aphrase structure tree.
This requires specific instruc-tions for each category (not all pairs), and genericoperations for each combinator.
We cover all cate-gories in the development set and correctly convert51.1% of sentences.
Unlike Clark and Curran?s ap-proach, we require no rules that consider non-localfeatures of constituents, which enables the possibil-ity of simple integration with a CKY-based parser.The most common errors our approach makes in-volve nodes for clauses and rare spans such as QPs,NXs, and NACs.
Many of these errors are inconsis-tencies in the original PTB annotations that are notrecoverable.
These issues make evaluating parseroutput difficult, but our method does enable an im-proved comparison of CCG and PTB parsers.2 BackgroundThere has been extensive work on converting parseroutput for evaluation, e.g.
Lin (1998) and Briscoe etal.
(2002) proposed using underlying dependenciesfor evaluation.
There has also been work on conver-sion to phrase structure, from dependencies (Xia andPalmer, 2001; Xia et al, 2009) and from lexicalisedformalisms, e.g.
HPSG (Matsuzaki and Tsujii, 2008)and TAG (Chiang, 2000; Sarkar, 2001).
Our focus ison CCG to PTB conversion (Clark and Curran, 2009).2.1 Combinatory Categorial Grammar (CCG)The lower half of Figure 1 shows a CCG derivation(Steedman, 2000) in which each word is assigned acategory, and combinatory rules are applied to ad-jacent categories until only one remains.
Categories105JJ NNSPRP$ NN DT NNNP NPVBD SNP VPSItalian magistrates labeled his death a suicideN /N N ((S [dcl ]\NP)/NP)/NP NP [nb]/N N NP [nb]/N N> > >N NP NP>NP (S [dcl ]\NP)/NP>S [dcl ]\NP<S [dcl ]Figure 1: A crossing constituents example: his .
.
.
suicide(PTB) crosses labeled .
.
.
death (CCGbank).Categories SchemaN create an NP((S [dcl ]\NP)/NP)/NP create a VPN /N + N place left under rightNP [nb]/N + N place left under right((S [dcl ]\NP)/NP)/NP + NP place right under left(S [dcl ]\NP)/NP + NP place right under leftNP + S [dcl ]\NP place both under STable 1: Example C&C-CONV lexical and rule schemas.can be atomic, e.g.
the N assigned to magistrates,or complex functions of the form result / arg, whereresult and arg are categories and the slash indicatesthe argument?s directionality.
Combinators definehow adjacent categories can combine.
Figure 1 usesfunction application, where a complex category con-sumes an adjacent argument to form its result, e.g.S [dcl ]\NP combines with the NP to its left to forman S [dcl ].
More powerful combinators allow cate-gories to combine with greater flexibility.We cannot form a PTB tree by simply relabelingthe categories in a CCG derivation because the map-ping to phrase labels is many-to-many, CCG deriva-tions contain extra brackets due to binarisation, andthere are cases where the constituents in the PTB treeand the CCG derivation cross (e.g.
in Figure 1).2.2 Clark and Curran (2009)Clark and Curran (2009), hereafter C&C-CONV, as-sign a schema to each leaf (lexical category) and rule(pair of combining categories) in the CCG derivation.The PTB tree is constructed from the CCG bottom-up, creating leaves with lexical schemas, then merg-ing/adding sub-trees using rule schemas at each step.The schemas for Figure 1 are shown in Table 1.These apply to create NPs over magistrates, death,and suicide, and a VP over labeled, and then com-bine the trees by placing one under the other at eachstep, and finally create an S node at the root.C&C-CONV has sparsity problems, requiringschemas for all valid pairs of categories ?
at aminimum, the 2853 unique category combinationsfound in CCGbank.
Clark and Curran (2009) createschemas for only 776 of these, handling the remain-der with approximate catch-all rules.C&C-CONV only specifies one simple schema foreach rule (pair of categories).
This appears reason-able at first, but frequently causes problems, e.g.
:(N /N )/(N /N ) + N /N?more than?
+ ?30?
(1)?relatively?
+ ?small?
(2)Here either a QP bracket (1) or an ADJP bracket(2) should be created.
Since both examples involvethe same rule schema, C&C-CONV would incorrectlyprocess them in the same way.
To combat the mostglaring errors, C&C-CONV manipulates the PTB treewith ad-hoc rules based on non-local features overthe CCG nodes being combined ?
an approach thatcannot be easily integrated into a parser.These disadvantages are a consequence of failingto exploit the generalisations that CCG combinatorsdefine.
We return to this example below to show howour approach handles both cases correctly.3 Our ApproachOur conversion assigns a set of instructions to eachlexical category and defines generic operations foreach combinator that combine instructions.
Figure 2shows a typical instruction, which specifies the nodeto create and where to place the PTB trees associatedwith the two categories combining.
More complexoperations are shown in Table 2.
Categories withmultiple arguments are assigned one instruction perargument, e.g.
labeled has three.
These are appliedone at a time, as each combinatory step occurs.For the example from the previous section we be-gin by assigning the instructions shown in Table 3.Some of these can apply immediately as they do notinvolve an argument, e.g.
magistrates has (NP f).One of the more complex cases in the example isItalian, which is assigned (NP f {a}).
This createsa new bracket, inserts the functor?s tree, and flattensand inserts the argument?s tree, producing:(NP (JJ Italian) (NNS magistrates))106((S\NP)/NP)/NP NPf a(S\NP)/NPf aVPFigure 2: An example function application.
Top row:CCG rule.
Bottom row: applying instruction (VP f a).Symbol Meaning Example(X f a) Add an X bracket around (VP f a)functor and argument{ } Flatten enclosed node (N f {a})X* Use same label as arg.
(S* f {a})or default to Xfi Place subtrees (PP f0 (S f1..k a))Table 2: Types of operations in instructions.For the complete example the final tree is almostcorrect but omits the S bracket around the final twoNPs.
To fix our example we could have modified ourinstructions to use the final symbol in Table 2.
Thesubscripts indicate which subtrees to place where.However, for this particular construction the PTB an-notations are inconsistent, and so we cannot recoverwithout introducing more errors elsewhere.For combinators other than function application,we combine the instructions in various ways.
Ad-ditionally, we vary the instructions assigned basedon the POS tag in 32 cases, and for the word not,to recover distinctions not captured by CCGbankcategories alone.
In 52 cases the later instruc-tions depend on the structure of the argument beingpicked up.
We have sixteen special cases for non-combinatory binary rules and twelve special casesfor non-combinatory unary rules.Our approach naturally handles our QP vs. ADJPexample because the two cases have different lexicalcategories: ((N /N )/(N /N ))\(S [adj ]\NP) on thanand (N /N )/(N /N ) on relatively.
This lexical dif-ference means we can assign different instructions tocorrectly recover the QP and ADJP nodes, whereasC&C-CONV applies the same schema in both casesas the categories combining are the same.4 EvaluationUsing sections 00-21 of the treebanks, we hand-crafted instructions for 527 lexical categories, a pro-cess that took under 100 hours, and includes all thecategories used by the C&C parser.
There are 647further categories and 35 non-combinatory binaryrules in sections 00-21 that we did not annotate.
ForCategory Instruction setN (NP f)N /N1 (NP f {a})NP [nb]/N1 (NP f {a})((S [dcl ]\NP3 )/NP2 )/NP1 (VP f a)(VP {f} a)(S a f)Table 3: Instruction sets for the categories in Figure 1.System Data P R F Sent.00 (all) 95.37 93.67 94.51 39.6C&C 00 (len ?
40) 95.85 94.39 95.12 42.1CONV 23 (all) 95.33 93.95 94.64 39.723 (len ?
40) 95.44 94.04 94.73 41.900 (all) 96.69 96.58 96.63 51.1This 00 (len ?
40) 96.98 96.77 96.87 53.6Work 23 (all) 96.49 96.11 96.30 51.423 (len ?
40) 96.57 96.21 96.39 53.8Table 4: PARSEVAL Precision, Recall, F-Score, and exactsentence match for converted gold CCG derivations.unannotated categories, we use the instructions ofthe result category with an added instruction.Table 4 compares our approach with C&C-CONVon gold CCG derivations.
The results shown are asreported by EVALB (Abney et al, 1991) using theCollins (1997) parameters.
Our approach leads to in-creases on all metrics of at least 1.1%, and increasesexact sentence match by over 11% (both absolute).Many of the remaining errors relate to missingand extra clause nodes and a range of rare structures,such as QPs, NACs, and NXs.
The only other promi-nent errors are single word spans, e.g.
extra or miss-ing ADVPs.
Many of these errors are unrecover-able from CCGbank, either because inconsistenciesin the PTB have been smoothed over or because theyare genuine but rare constructions that were lost.4.1 Parser ComparisonWhen we convert the output of a CCG parser, the PTBtrees that are produced will contain errors created byour conversion as well as by the parser.
In this sec-tion we are interested in comparing parsers, so weneed to factor out errors created by our conversion.One way to do this is to calculate a projected score(PROJ), as the parser result over the oracle result, butthis is a very rough approximation.
Another way isto evaluate only on the 51% of sentences for whichour conversion from gold CCG derivations is perfect(CLEAN).
However, even on this set our conversion1070204060801000 20 40 60 80 100ConvertedC&C,EVALBConverted Gold, EVALB0204060801000 20 40 60 80 100NativeC&C,ldepsConverted Gold, EVALBFigure 3: For each sentence in the treebank, we plotthe converted parser output against gold conversion (left),and the original parser evaluation against gold conversion(right).
Left: Most points lie below the diagonal, indicat-ing that the quality of converted parser output (y) is upperbounded by the quality of conversion on gold parses (x).Right: No clear correlation is present, indicating that theset of sentences that are converted best (on the far right),are not necessarily easy to parse.introduces errors, as the parser output may containcategories that are harder to convert.Parser F-scores are generally higher on CLEAN,which could mean that this set is easier to parse, or itcould mean that these sentences don?t contain anno-tation inconsistencies, and so the parsers aren?t in-correct for returning the true parse (as opposed tothe one in the PTB).
To test this distinction we lookfor correlation between conversion quality and parsedifficulty on another metric.
In particular, Figure 3(right) shows CCG labeled dependency performancefor the C&C parser vs. CCGbank conversion PARSE-VAL scores.
The lack of a strong correlation, and thespread on the line x = 100, supports the theory thatthese sentences are not necessarily easier to parse,but rather have fewer annotation inconsistencies.In the left plot, the y-axis is PARSEVAL on con-verted C&C parser output.
Conversion quality essen-tially bounds the performance of the parser.
The fewpoints above the diagonal are mostly short sentenceson which the C&C parser uses categories that leadto one extra correct node.
The main constructionson which parse errors occur, e.g.
PP attachment, arerarely converted incorrectly, and so we expect thenumber of errors to be cumulative.
Some sentencesare higher in the right plot than the left because thereare distinctions in CCG that are not always present inthe PTB, e.g.
the argument-adjunct distinction.Table 5 presents F-scores for three PTB parsersand three CCG parsers (with their output convertedby our method).
One interesting comparison is be-tween the PTB parser of Petrov and Klein (2007) andSentences CLEAN ALL PROJConverted gold CCGCCGbank 100.0 96.3 ?Converted CCGClark and Curran (2007) 90.9 85.5 88.8Fowler and Penn (2010) 90.9 86.0 89.3Auli and Lopez (2011) 91.7 86.2 89.5Native PTBKlein and Manning (2003) 89.8 85.8 ?Petrov and Klein (2007) 93.6 90.1 ?Charniak and Johnson (2005) 94.8 91.5 ?Table 5: F-scores on section 23 for PTB parsers andCCG parsers with their output converted by our method.CLEAN is only on sentences that are converted perfectlyfrom gold CCG (51%).
ALL is over all sentences.
PROJ isa projected F-score (ALL result / CCGbank ALL result).the CCG parser of Fowler and Penn (2010), whichuse the same underlying parser.
The performancegap is partly due to structures in the PTB that are notrecoverable from CCGbank, but probably also indi-cates that the split-merge model is less effective inCCG, which has far more symbols than the PTB.It is difficult to make conclusive claims aboutthe performance of the parsers.
As shown earlier,CLEAN does not completely factor out the errors in-troduced by our conversion, as the parser output maybe more difficult to convert, and the calculation ofPROJ only roughly factors out the errors.
However,the results do suggest that the performance of theCCG parsers is approaching that of the Petrov parser.5 ConclusionBy exploiting the generalised combinators of theCCG formalism, we have developed a new methodof converting CCG derivations into PTB-style trees.Our system, which is publicly available1 , is moreeffective than previous work, increasing exact sen-tence match by more than 11% (absolute), and canbe directly integrated with a CCG parser.AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful suggestions.
This research wassupported by a General Sir John Monash Fellow-ship, the Office of Naval Research under MURIGrant No.
N000140911081, ARC Discovery grantDP1097291, and the Capital Markets CRC.1http://code.google.com/p/berkeley-ccg2pst/108ReferencesS.
Abney, S. Flickenger, C. Gdaniec, C. Grishman,P.
Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-vans, M. Liberman, M. Marcus, S. Roukos, B. San-torini, and T. Strzalkowski.
1991.
Procedure for quan-titatively comparing the syntactic coverage of englishgrammars.
In Proceedings of the workshop on Speechand Natural Language, pages 306?311.Michael Auli and Adam Lopez.
2011.
A comparison ofloopy belief propagation and dual decomposition forintegrated ccg supertagging and parsing.
In Proceed-ings of ACL, pages 470?480.Ted Briscoe, John Carroll, Jonathan Graham, and AnnCopestake.
2002.
Relational evaluation schemes.
InProceedings of the Beyond PARSEVAL Workshop atLREC, pages 4?8.Aoife Cahill, Michael Burke, Ruth O?Donovan, StefanRiezler, Josef van Genabith, and Andy Way.
2008.Wide-coverage deep statistical parsing using auto-matic dependency structure annotation.
Computa-tional Linguistics, 34(1):81?124.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of ACL, pages 173?180.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProceedings of ACL, pages 456?463.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark and James R. Curran.
2009.
Comparingthe accuracy of CCG and penn treebank parsers.
InProceedings of ACL, pages 53?56.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of ACL,pages 16?23.Timothy A. D. Fowler and Gerald Penn.
2010.
Accu-rate context-free parsing with combinatory categorialgrammar.
In Proceedings of ACL, pages 335?344.Julia Hockenmaier.
2003.
Data and models for statis-tical parsing with Combinatory Categorial Grammar.Ph.D.
thesis, School of Informatics, The University ofEdinburgh.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL, pages423?430.Dekang Lin.
1998.
A dependency-based method forevaluating broad-coverage parsers.
Natural LanguageEngineering, 4(2):97?114.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: the penn treebank.
Computational Lin-guistics, 19(2):313?330.Takuya Matsuzaki and Jun?ichi Tsujii.
2008.
Com-parative parser performance analysis across grammarframeworks through automatic tree conversion usingsynchronous grammars.
In Proceedings of Coling,pages 545?552.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.2004.
Corpus-oriented grammar development for ac-quiring a head-driven phrase structure grammar fromthe penn treebank.
In Proceedings of IJCNLP, pages684?693.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACL,pages 404?411.Anoop Sarkar.
2001.
Applying co-training methods tostatistical parsing.
In Proceedings of NAACL, pages1?8.Mark Steedman.
2000.
The Syntactic Process.
MITPress.Fei Xia and Martha Palmer.
2001.
Converting depen-dency structures to phrase structures.
In Proceedingsof HLT, pages 1?5.Fei Xia, Owen Rambow, Rajesh Bhatt, Martha Palmer,and Dipti Misra Sharma.
2009.
Towards a multi-representational treebank.
In Proceedings of the 7thInternational Workshop on Treebanks and LinguisticTheories, pages 159?170.Fei Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Proceedings of the Natural Lan-guage Processing Pacific Rim Symposium, pages 398?403.109
