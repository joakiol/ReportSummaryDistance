Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 1?5, New York City, June 2006. c?2006 Association for Computational LinguisticsA Mission for Computational Natural Language LearningWalter DaelemansCNTS Language Technology GroupUniversity of AntwerpBelgiumwalter.daelemans@ua.ac.beAbstractIn this presentation, I will look back at10 years of CoNLL conferences and thestate of the art of machine learning of lan-guage that is evident from this decade ofresearch.
My conclusion, intended to pro-voke discussion, will be that we currentlylack a clear motivation or ?mission?
tosurvive as a discipline.
I will suggest thata new mission for the field could be foundin a renewed interest for theoretical work(which learning algorithms have a biasthat matches the properties of language?,what is the psycholinguistic relevance oflearner design issues?
), in more sophis-ticated comparative methodology, and insolving the problem of transfer, reusabil-ity, and adaptation of learned knowledge.1 IntroductionWhen looking at ten years of CoNLL conferences,it is clear that the impact and the size of the con-ference has enormously grown over time.
The tech-nical papers you will find in this proceedings noware comparable in quality and impact to those ofother distinguished conferences like the Conferenceon Empirical Methods in Natural Language Pro-cessing or even the main conferences of ACL, EACLand NAACL themselves.
An important factor inthe success of CoNLL has been the continued se-ries of shared tasks (notice we don?t use terms likechallenges or competitions) that has produced a use-ful set of benchmarks for comparing learning meth-ods, and that has gained wide interest in the field.It should also be noted, however, that the successof the conferences is inversely proportional withthe degree to which the original topics which mo-tivated the conference are present in the programme.Originally, the people driving CoNLL wanted it tobe promiscuous (i) in the selection of partners (wewanted to associate with Machine Learning, Lin-guistics and Cognitive Science conferences as wellas with Computational Linguistics conferences) and(ii) in the range of topics to be presented.
We wantedto encourage linguistically and psycholinguisticallyrelevant machine learning work, and biologically in-spired and innovative symbolic learning methods,and present this work alongside the statistical andlearning approaches that were at that time only start-ing to gradually become the mainstream in Compu-tational Linguistics.
It has turned out differently,and we should reflect on whether we have becometoo much of a mainstream computational linguisticsconference ourselves, a back-off for the good papersthat haven?t made it in EMNLP or ACL because ofthe crazy rejection rates there (with EMNLP in itsturn a back-off for good papers that haven?t madeit in ACL).
Some of the work targeted by CoNLLhas found a forum in meetings like the workshop onPsycho-computational models of human languageacquisition, the International Colloquium on Gram-matical Inference, the workshop on Morphologicaland Phonological Learning etc.
We should ask our-selves why we don?t have this type of work morein CoNLL.
In the first part of the presentation Iwill sketch very briefly the history of SIGNLL and1CoNLL and try to initiate some discussion on whata conference on Computational Language Learningshould be doing in 2007 and after.2 State of the Art in ComputationalNatural Language LearningThe second part of my presentation will be a dis-cussion of the state of the art as it can be found inCoNLL (and EMNLP and the ACL conferences).The field can be divided into theoretical, method-ological, and engineering work.
There has beenprogress in theory and methodology, but perhapsnot sufficiently.
I will argue that most progress hasbeen made in engineering with most often incre-mental progress on specific tasks as a result ratherthan increased understanding of how language canbe learned from data.Machine Learning of Natural Language (MLNL),or Computational Natural Language Learning(CoNLL) is a research area lying in the intersec-tion of computational linguistics and machine learn-ing.
I would suggest that Statistical Natural Lan-guage Processing (SNLP) should be treated as partof MLNL, or perhaps even as a synonym.
Symbolicmachine learning methods belong to the same partof the ontology as statistical methods, but have dif-ferent solutions for specific problems.
E.g., Induc-tive Logic Programming allows elegant addition ofbackground knowledge, memory-based learning hasimplicit similarity-based smoothing, etc.There is no need here to explain the success ofinductive methods in Computational Linguistics andwhy we are all such avid users of the technology:availability of data, fast production of systems withgood accuracy, robustness and coverage, cheaperthan linguistic labor.
There is also no need hereto explain that many of these arguments in favor oflearning in NLP are bogus.
Getting statistical andmachine learning systems to work involves design,optimization, and smoothing issues that are some-thing of a black art.
For many problems, gettingsufficient annotated data is expensive and difficult,our annotators don?t sufficiently agree, our trainedsystems are not really that good.
My favorite exam-ple for the latter is part of speech tagging, which isconsidered a solved problem, but still has error ratesof 20-30% for the ambiguities that count, like verb-noun ambiguity.
We are doing better than hand-crafted linguistic knowledge-based approaches butfrom the point of view of the goal of robust lan-guage understanding unfortunately not that signifi-cantly better.
Twice better than very bad is not nec-essarily any good.
We also implicitly redefined thegoals of the field of Computational Linguistics, for-getting for example about quantification, modality,tense, inference and a large number of other sen-tence and discourse semantics issues which do notfit the default classification-based supervised learn-ing framework very well or for which we don?t haveannotated data readily available.
As a final irony,one of the reasons why learning methods have be-come so prevalent in NLP is their success in speechrecognition.
Yet, there too, this success is relative;the goal of spontaneous speaker-independent recog-nition is still far away.2.1 TheoryThere has been a lot of progress recently in theoret-ical machine learning(Vapnik, 1995; Jordan, 1999).Statistical Learning Theory and progress in Graph-ical Models theory have provided us with a well-defined framework in which we can relate differ-ent approaches like kernel methods, Naive Bayes,Markov models, maximum entropy approaches (lo-gistic regression), perceptrons and CRFs.
Insightinto the differences between generative and discrim-inative learning approaches has clarified the rela-tions between different learning algorithms consid-erably.However, this work does not tell us somethinggeneral about machine learning of language.
The-oretical issues that should be studied in MLNL arefor example which classes of learning algorithms arebest suited for which type of language processingtask, what the need for training data is for a giventask, which information sources are necessary andsufficient for learning a particular language process-ing task, etc.
These fundamental questions all re-late to learning algorithm bias issues.
Learning isa search process in a hypothesis space.
Heuristiclimitations on the search process and restrictions onthe representations allowed for input and hypothe-sis representations together define this bias.
There isnot a lot of work on matching properties of learningalgorithms with properties of language processing2tasks, or more specifically on how the bias of partic-ular (families of) learning algorithms relates to thehypothesis spaces of particular (types of) languageprocessing tasks.As an example of such a unifying approach,(Roth, 2000) shows that several different algorithms(memory-based learning, tbl, snow, decision lists,various statistical learners, ...) use the same typeof knowledge representation, a linear representationover a feature space based on a transformation of theoriginal instance space.
However, the only relationto language here is rather negative with the claimthat this bias is not sufficient for learning higherlevel language processing tasks.As another example of this type of work,Memory-Based Learning (MBL) (Daelemans andvan den Bosch, 2005), with its implicit similarity-based smoothing, storage of all training evidence,and uniform modeling of regularities, subregulari-ties and exceptions has been proposed as having theright bias for language processing tasks.
Languageprocessing tasks are mostly governed by Zipfiandistributions and high disjunctivity which makes itdifficult to make a principled distinction betweennoise and exceptions, which would put eager learn-ing methods (i.e.
most learning methods apart fromMBL and kernel methods) at a disadvantage.More theoretical work in this area should make itpossible to relate machine learner bias to propertiesof language processing tasks in a more fine-grainedway, providing more insight into both language andlearning.
An avenue that has remained largely unex-plored in this respect is the use of artificial data emu-lating properties of language processing tasks, mak-ing possible a much more fine-grained study of theinfluence of learner bias.
However, research in thisarea will not be able to ignore the ?no free lunch?theorem (Wolpert and Macready, 1995).
Referringback to the problem of induction (Hume, 1710) thistheorem can be interpreted that no inductive algo-rithm is universally better than any other; general-ization performance of any inductive algorithm iszero when averaged over a uniform distribution ofall possible classification problems (i.e.
assuminga random universe).
This means that the only wayto test hypotheses about bias and necessary infor-mation sources in language learning is to performempirical research, making a reliable experimentalmethodology necessary.2.2 MethodologyEither to investigate the role of different informationsources in learning a task, or to investigate whetherthe bias of some learning algorithm fits the proper-ties of natural language processing tasks better thanalternative learning algorithms, comparative experi-ments are necessary.
As an example of the latter, wemay be interested in investigating whether part-of-speech tagging improves the accuracy of a Bayesiantext classification system or not.
As an example ofthe former, we may be interested to know whethera relational learner is better suited than a propo-sitional learner to learn semantic function associa-tion.
This can be achieved by comparing the accu-racy of the learner with and without the informationsource or different learners on the same task.
Crucialfor objectively comparing algorithm bias and rele-vance of information sources is a methodology toreliably measure differences and compute their sta-tistical significance.
A detailed methodology hasbeen developed for this involving approaches likek-fold cross-validation to estimate classifier quality(in terms of measures derived from a confusion ma-trix like accuracy, precision, recall, F-score, ROC,AUC, etc.
), as well as statistical techniques like Mc-Nemar and paired cross-validation t-tests for deter-mining the statistical significance of differences be-tween algorithms or between presence or absence ofinformation sources.
This methodology is generallyaccepted and used both in machine learning and inmost work in inductive NLP.CoNLL has contributed a lot to this compara-tive work by producing a successful series of sharedtasks, which has provided to the community a richset of benchmark language processing tasks.
Othercompetitive research evaluations like senseval, thePASCAL challenges and the NIST competitionshave similarly tuned the field toward comparativelearning experiments.
In a typical comparative ma-chine learning experiment, two or more algorithmsare compared for a fixed sample selection, featureselection, feature representation, and (default) al-gorithm parameter setting over a number of trials(cross-validation), and if the measured differencesare statistically significant, conclusions are drawnabout which algorithm is better suited to the problem3being studied and why (mostly in terms of algorithmbias).
Sometimes different sample sizes are used toprovide a learning curve, and sometimes parametersof (some of the) algorithms are optimized on train-ing data, or heuristic feature selection is attempted,but this is exceptional rather than common practicein comparative experiments.Yet everyone knows that many factors potentiallyplay a role in the outcome of a (comparative) ma-chine learning experiment: the data used (the sam-ple selection and the sample size), the informationsources used (the features selected) and their repre-sentation (e.g.
as nominal or binary features), theclass representation (error coding, binarization ofclasses), and the algorithm parameter settings (mostML algorithms have various parameters that can betuned).
Moreover,all these factors are known to in-teract.
E.g., (Banko and Brill, 2001) demonstratedthat for confusion set disambiguation, a prototypi-cal disambiguation in context problem, the amountof data used dominates the effect of the bias of thelearning method employed.
The effect of trainingdata size on relevance of POS-tag information on topof lexical information in relation finding was studiedin (van den Bosch and Buchholz, 2001).
The pos-itive effect of POS-tags disappears with sufficientdata.
In (Daelemans et al, 2003) it is shown thatthe joined optimization of feature selection and algo-rithm parameter optimization significantly improvesaccuracy compared to sequential optimization.
Re-sults from comparative experiments may thereforenot be reliable.
I will suggest an approach to im-prove methodology to improve reliability.2.3 EngineeringWhereas comparative machine learning work canpotentially provide useful theoretical insights and re-sults, there is a distinct feeling that it also leads toan exaggerated attention for accuracy on the dataset.Given the limited transfer and reusability of learnedmodules when used in different domains, corporaetc., this may not be very relevant.
If a WSJ-trainedstatistical parser looses 20% accuracy on a compa-rable newspaper testcorpus, it doesn?t really mattera lot that system A does 1% better than system B onthe default WSJ-corpus partition.In order to win shared tasks and perform best onsome language processing task, various clever archi-tectural and algorithmic variations have been pro-posed, sometimes with the single goal of gettinghigher accuracy (ensemble methods, classifier com-bination in general, ...), sometimes with the goal ofsolving manual annotation bottlenecks (active learn-ing, co-training, semisupervised methods, ...).This work is extremely valid from the point ofview of computational linguistics researchers look-ing for any old method that can boost performanceand get benchmark natural language processingproblems or applications solved.
But from the pointof view of a SIG on computational natural languagelearning, this work is probably too much theory-independent and doesn?t teach us enough about lan-guage learning.However, engineering work like this can suddenlybecome theoretically important when motivated notby a few percentage decimals more accuracy butrather by (psycho)linguistic plausibility.
For exam-ple, the current trend in combining local classifierswith holistic inference may be a cognitively relevantprinciple rather than a neat engineering trick.3 ConclusionThe field of computational natural language learn-ing is in need of a renewed mission.
In two par-ent fields dominated by good engineering use of ma-chine learning in language processing, and interest-ing developments in computational language learn-ing respectively, our field should focus more on the-ory.
More research should address the question whatwe can learn about language from comparative ma-chine learning experiments, and address or at leastacknowledge methodological problems.4 AcknowledgementsThere are many people that have influenced me,most of my students and colleagues have done soat some point, but I would like to single out DavidPowers and Antal van den Bosch, and thank themfor making this strange field of computational lan-guage learning such an interesting and pleasant play-ground.ReferencesMichele Banko and Eric Brill.
2001.
Mitigating thepaucity-of-data problem: exploring the effect of train-4ing corpus size on classifier performance for natu-ral language processing.
In HLT ?01: Proceedingsof the first international conference on Human lan-guage technology research, pages 1?5, Morristown,NJ, USA.
Association for Computational Linguistics.Walter Daelemans and Antal van den Bosch.
2005.Memory-Based Language Processing.
CambridgeUniversity Press, Cambridge, UK.Walter Daelemans, Ve?ronique Hoste, Fien De Meulder,and Bart Naudts.
2003.
Combined optimization offeature selection and algorithm parameter interactionin machine learning of language.
In Proceedings ofthe 14th European Conference on Machine Learn-ing (ECML-2003), Lecture Notes in Computer Sci-ence 2837, pages 84?95, Cavtat-Dubrovnik, Croatia.Springer-Verlag.D.
Hume.
1710.
A Treatise Concerning the Principles ofHuman Knowledge.M.
I. Jordan.
1999.
Learning in graphical models.
MIT,Cambridge, MA, USA.D.
Roth.
2000.
Learning in natural language: The-ory and algorithmic approaches.
In Proc.
of the An-nual Conference on Computational Natural LanguageLearning (CoNLL), pages 1?6, Lisbon, Portugal.Antal van den Bosch and Sabine Buchholz.
2001.
Shal-low parsing on the basis of words only: a case study.In ACL ?02: Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, pages433?440,Morristown, NJ, USA.
Association for Com-putational Linguistics.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-VerlagNew York, Inc., NewYork, NY, USA.David H. Wolpert and William G. Macready.
1995.
Nofree lunch theorems for search.
Technical Report SFI-TR-95-02-010, Santa Fe, NM.5
