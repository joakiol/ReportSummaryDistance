Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1323?1328,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsMPQA 3.0: An Entity/Event-Level Sentiment CorpusLingjia DengIntelligent Systems ProgramUniversity of Pittsburghlid29@pitt.eduJanyce WiebeIntelligent Systems ProgramDepartment of Computer ScienceUniversity of Pittsburghwiebe@cs.pitt.eduAbstractThis paper presents an annotation scheme foradding entity and event target annotations tothe MPQA corpus, a rich span-annotated opin-ion corpus.
The new corpus promises to bea valuable new resource for developing sys-tems for entity/event-level sentiment analysis.Such systems, in turn, would be valuable inNLP applications such as Automatic QuestionAnswering.
We introduce the idea of entityand event targets (eTargets), describe the an-notation scheme, and present the results of anagreement study.1 IntroductionMuch work in sentiment analysis and opinion min-ing is at the document level (Pang et al, 2002;Turney, 2002).
There is increasing interest inmore fine-grained levels - sentence-level (Yu andHatzivassiloglou, 2003; McDonald et al, 2007),phrase-level (Choi and Cardie, 2008), aspect-level(Hu and Liu, 2004; Titov and McDonald, 2008),etc.
We specifically address sentiments toward en-tities and events (i.e., eTargets) expressed in datasuch as blogs, newswire, and editorials.
A systemthat could recognize sentiments toward entities andevents would be valuable in an application such asAutomatic Question Answering, to support answer-ing questions such as ?Toward whom/what is X neg-ative/positive??
?Who is negative/positive towardX??
(Stoyanov et al, 2005).
Or, to augment anautomatic wikification system (Ratinov et al, 2011)?
in addition to relationships such as spouse andparents, the system could include information aboutwhom or what the subject supports or opposes.
A re-cent NIST evaluation ?
The Knowledge Base Popu-lation (KBP) Sentiment track1?
aims at using cor-pora to collect information regarding sentiments ex-pressed toward or by named entities.Annotated corpora of reviews (e.g., (Hu and Liu,2004; Titov and McDonald, 2008)), widely used inNLP, often include target annotations.
Such targetsare often aspects or features of products or services,and as such are somewhat limited.2Recently, to create the Sentiment Treebank(Socher et al, 2013), researchers crowdsourced an-notations of movie review data and then overlaidthe annotations onto syntax trees.
Thus, the tar-gets are not limited to aspects of products/services.However, annotators were asked to annotate smalland then increasingly larger segments of the sen-tence.
Thus, the annotations are mixed in the de-gree to which context was considered when makingthe judgements.
Previously, we (Deng et al, 2013)annotated a corpus of non-review data with senti-ments toward entities, but only for those that partic-ipate in certain types of events.
In all of the abovecorpora, the only sentiments considered are those ofthe writer, excluding sentiments attributed to otherentities.The MPQA opinion annotated corpus (Wiebe etal., 2005; Wilson, 2007) is entirely span-based, andcontains no eTarget annotations.
However, it pro-vides an infrastructure for sentiment annotation thatis not provided by other sentiment NLP corpora, and1http://www.nist.gov/tac/2014/KBP/Sentiment/index.html2For example, as stated in SemEval-2014: ?We annotateonly aspect terms naming particular aspects.
?1323is much more varied in topic, genre, and publicationsource.
This paper addresses adding eTarget annota-tions to the MPQA corpus; we believe that the resultwill be a valuable new resource for the community.2 From MPQA 2.0 to MPQA 3.0To create MPQA 3.0, entity-target and event-target(eTarget) annotations are added to the MPQA 2.0annotations.3The MPQA annotations consist ofprivate states, which are states of sources holdingattitudes toward targets.
In the MPQA 2.0 anno-tations, the top-level annotations are direct subjec-tive (DS) and objective speech event annotations.DS annotations are for private states, and objec-tive speech event annotations are for objective state-ments attributed to a source.
An important propertyof sources is that they are nested, reflecting the factthat private states and speech events are often em-bedded in one another.As shown in Figure 1, one DS may contain linksto multiple attitude annotations, meaning that all ofthe attitudes share the same nested source.
The atti-tudes differ from one another in their attitude types,polarities, and/or targets.
There are several types ofattitudes included in MPQA 2.0 (Wilson, 2007; So-masundaran et al, 2007), including sentiment andarguing.
This work focuses on sentiments, whichare defined in (Wilson, 2007) as positive and nega-tive evaluations, emotions, and judgements.MPQA 2.0 also contains expressive subjective el-ement (ESE) annotations, which pinpoint specificexpressions used to express subjectivity (Wiebe etal., 2005).
An ESE also has a nested-source an-notation.
Since we focus on sentiments, we onlyconsider ESEs whose polarity is positive or negative(excluding those marked neutral).The target-span annotations in MPQA 2.0 arelinked to from the attitudes.
More than one targetmay be linked to from an attitude, but most atti-tudes have only one target.
The MPQA 2.0 anno-tators identified the main/most important target(s)they perceive in the sentence.
If there is no target,the target-span annotation is ?none?.
However, thereare many other eTargets to be identified.
First, whileESE annotations have nested sources, they do nothave any target annotations.
Second, there are many3Available at http://mpqa.cs.pitt.eduFigure 1: Structure in MPQA 3.0.more targets that may be marked than the major onesidentified in MPQA 2.0.
In Figure 1, the eTargets arewhat we add in MPQA 3.0.
We identify the blue (or-ange) eTargets that are in the span of a blue (orange)target in MPQA 2.0.
We also identify the green eTar-gets that are not in the scope of any target.Since our priority was to add eTargets to senti-ments, no eTargets have yet been added to objectivespeech events, as shown in Figure 1.To create MPQA 3.0, the corpus is first parsed,and potential eTarget annotations are automaticallycreated from the heads of NPs and VPs.
The annota-tors then consider each sentiment attitude and eachpolar ESE, and decide for each which eTargets toadd.
By adding eTargets to the existing annotations,the information in MPQA 2.0 is retained.
Beforepresenting the scheme, we first give some examples.2.1 ExamplesFor each example, a subset of the annotations areshown.
The phrase in blue is an attitude span, thephrase in red is a target span, the tokens in yellow arethe eTargets which are newly annotated in MPQA3.0.
The underlined phrases are ESE spans.
Eachexample is followed by the MPQA structure of theannotations.In Ex(1), a negative attitude is shown, issued thefatwa against.
The source is the Imam.
The targetis the event Rushdie insulting the Prophet.
How-ever, the assertion that the Imam is negative towardthe insult event is within the scope of this article.This is captured by an objective speech event anno-tation (not shown) whose target span includes the in-sult event, and whose source is the writer (w).
Thus,the complete interpretation of this negative attitudeis, according to the writer, the Imam is negative to-1324ward the insult event.
And, the nested source is w,imam.Ex(1) When the Imam issued the fatwaagainst Salman Rushdie for insultingthe Prophet ...DS: issued the fatwanested-source: w, imamattitude: issued the fatwa againstattitude-type: sentiment-negativetarget: Salman...insult...PropheteTarget : Rushdie, insultingWe find two eTargets in the target-span:?Rushdie?
himself plus his act of ?insulting.
?In the same sentence, there is another negative at-titude, insulting, as shown in Ex(2).
The source isSalman Rushdie and the target is the Prophet.
Notethat the span covering this event is the target span ofthe attitude in Ex(1) ?
the private state of Ex(2) isnested in the private state of Ex(1).
Thus, the com-plete interpretation of the negative attitude in Ex(2)is: according to the writer, the Imam is negativetoward Rushdie insulting the Prophet.
The nestedsource is w, Imam, Rushdie.Ex(2) When the Imam issued the fatwaagainst Salman Rushdie for insulting theProphet ...DS: insultingnested-source: w, imam, rushdieattitude: insultingattitude-type: sentiment-negativetarget: the PropheteTarget : ProphetWe add an eTarget for the Prophet, anchored tothe head ?Prophet.?
Interestingly, ?Prophet?
is aneTarget for w,Iman,Rushdie (i.e., Rushdie is nega-tive toward the Prophet), but not for w,Imam (i.e.,the Imam is not negative toward the Prophet).In the following example, the target span is short.Ex(3) He is therefore planning totrigger wars ...DS: (entire sentence)nested-source: wattitude: planning to trigger warsattitude-type: sentiment-negativetarget: HeeTarget : HeeTarget : planning, trigger, wars?He?
is George W. Bush; this article appeared inthe early 2000s.
The writer is negative toward Bushbecause (the writer claims) he is planning to trig-ger wars.
As shown in the example, the MPQA 2.0target span is only ?He,?
for which we do createan eTarget.
But there are three additional eTargets,which are not included in the target span.
The writeris negative toward Bush planning to trigger wars; wemake sense of this by inferring that the writer is neg-ative toward the idea of triggering wars and thus to-ward war itself.Ex(4) Three leading international organ-isations warned jointly Thursday thatthe international fight against terror-ism should not be a pretext for theviolation of human rights.DS: warnednested-source: w, threeintattitude: warnedattitude-type: sentiment-negativetarget: the international ... rightseTarget :be, pretext, violationESE: pretextnested-source: w, threeintpolarity: negativeeTarget : pretextThe viewpoints in the article of Ex(4) are notagainst fighting terrorism (another sentence begins?While we recognize that the threat of terrorism re-quires specific measures ...?)
but against doing soin certain ways.
Here the three organizations areagainst the fight being used as a pretext for civilrights violations.
Thus, ?be?, ?pretext?, and ?vi-olation?
are eTargets, but ?fight?
and ?terrorism?are not.
We mark ?be?
as an eTarget because thesource is negative toward the state of the fight be-ing a pretext for the violation of human rights.
Thismakes sense with the source also being negative to-ward ?pretext?
and ?violation.?
The fact that ?pre-1325text?
is identified as a negative ESE annotation in theMPQA 2.0 supports this as well.There is a difference between ESE and attitudeeTarget annotations.
Since ESE annotations pin-point specific wording used to express subjectivity,ESE eTargets are annotated more narrowly than atti-tude eTargets.
For ESEs, the eTargets are the entitiesand events that are directly evaluated by the expres-sion, while, for attitudes, the eTargets include all en-tities and events toward which the attitude holds (aswe saw in the examples above).
For example:Ex(5) ... because the hard-line wing inthe US administration comprising VicePresident Dick Cheney ...DS: (entire sentence)nested-source: wattitude: hard-lineattitude-type: sentiment-negativetarget: wing in the .. Dick CheneyeTarget : wing, CheneyESE: hard-line wingnested-source: wpolarity: negativeeTarget : wingThe ESE has only one eTarget, ?wing,?
while theattitude has two: ?wing?
and ?Cheney.
?2.2 MPQA 3.0 Annotation SchemeAn eTarget is an entity or event that is the target ofa sentiment (identified in MPQA 2.0 by a sentimentattitude or polar ESE span).
The eTarget annotationis anchored to the head word of the NP or VP thatrefers to the entity or event, and has three slots: id(unique within the document), isNegated (yes or no),and type (entity or event; note that event includesboth states and events).
The isNegated = yes optionis for the case where the eTarget is the negation ofthe event referred to by the head word, for example,when the source is positive toward someone not do-ing something.An attitude has one or more target-span annota-tions in MPQA 2.0.
We provide two slots for the kthtarget annotation.
k-targetSpan shows the kthtar-get span.
k-eTarget-link is to be filled with a list ofids of eTargets whose text anchors are within the kthtarget span.
An additional slot new-eTarget-link is tobe filled with a list of ids of other eTargets.Each eTarget of an ESE has two slots, one for theeTarget id, and one for an attribute, isReferedInSpan(yes, or no).
The value is yes if the eTarget is referredto in the ESE span.3 Agreement StudyWe developed the manual via iterative annotation,discussion, and revision.
Once the manual was de-veloped, we participated in an agreement study.For the formal agreement study, one documentwas randomly selected from each of the four topicsof the OPQA subset (Stoyanov et al, 2005) of theMPQA corpus.
They were not any of the documentsused to develop the manual.
We then independentlyannotated the four documents.
There are 292 eTar-gets in the four documents in total.To evaluate the results, the same agreement mea-sure is used for both attitude and ESE eTargets.Given an attitude or ESE, let set A be the set ofeTargets annotated by annotator X , and set B be theset of eTargets annotated by annotator Y .
Following(Wilson and Wiebe, 2003; Johansson and Moschitti,2013), which treat each set A and B in turn as thegold-standard, we calculate the average F-measure,denoted agr(A,B).
The agr(A,B) is 0.82 on aver-age over the four documents, showing good agree-ment: agr(A,B) = (|A?B|/|B|+ |A?B|/|A|)/2.4 Disagreement AnalysisOne issue is whether an attitude toward an entity orevent is indeed communicated in the sentence.
Con-sider this sentence: ?President Mugabe?s reelectionhas been praised by OAU.?
The OAU is positive to-ward ?reelection,?
which is an eTarget both annota-tors mark.
The question is whether it is also commu-nicated in this sentence that the OAU is also positivetoward ?Mugabe.?
X did not mark Mugabe as aneTarget, whereas Y did.
During the subsequent dis-cussion, X now agrees that it should be marked.
Ingeneral, X was using what we now consider to bea too conservative policy.
Overall, 29% of all dis-agreements are of this type of borderline case.8% of the disagreements arise when there aremultiple attitudes with overlapping spans, the samesource, the same polarity, but different targets and1326intensities4.
When there are new eTargets which arenot in any target span, annotator X splits the neweTargets into different attitudes based on the inten-sity, while annotator Y adds the new eTargets to allthe attitudes regardless of intensity.
Later the an-notators discuss to decide which attitude each neweTarget should be linked to in the final version.31% of the disagreements are caused by negli-gence, meaning an annotator realized, during laterdiscussion, that she should have included an eTargetwhen she saw that the other annotator had includedit.The remaining disagreements are due to annotatormistakes such as filling in the wrong id.5 Current CorpusThe current corpus consists of 70 documents, in-cluding the subset of the documents in MPQA 2.0that come from English-language sources (i.e., thatare not translations) and a subset of the OPQA sub-set in MPQA 2.0.
A subset contains consensus an-notations of X and Y and the rest were annotatedby Y .
The 70 documents have 1,029 ESEs, 1,287attitudes, and 1,213 target spans of attitudes (exclud-ing the target span that are marked as ?none?)
fromMPQA 2.0; they have 4,459 eTargets in total.
Weadded 1,366 eTargets to the ESEs and 1,608 eTar-gets to the target spans.
We added 1,485 eTargetswhich are not in any target span.6 An ExampleIn this section, we present an example from theOPQA subset (Stoyanov et al, 2005) to demonstratehow eTargets could help to automatically answer aquestion.
There are opinion and fact questions foreach document in the OPQA subset.
The sentencebelow is annotated in MPQA 2.0 to answer the ques-tion, ?Is the US Annual Human Rights Report re-ceived with universal approval around the world?
?Here the writer is negative toward the report.It is due to this hegemony, which theUnited States wants to maintain, that itsState Department makes an assessmentof the human rights situation in different4In MPQA 2.0, an attitude is marked with an intensity (low,medium, or high) representing the intensity.countries and prepares a report on theirviolations all over the world.The annotations in MPQA 2.0:S1: ?writer-US, positive, hegemony?S2: ?writer, negative, the United States?ESE1: ?writer, negative, N/A?First, it is possible for a state-of-the-art systemto be trained to recognize the sentiment S1, by themaintaining phrase and syntax information.
But itwould be difficult to find S2.
There is no direct sen-timent modifying the US, nor is there any sentimentor ESE annotation toward maintain or hegemony inMPQA 2.0.
Now, in MPQA 3.0, we add the eTargetof the ESE1, so that it becomes ?writer, negative,hegemony?.
This is a critical step, because the com-plete ESE bridges the two sentiments together.Second, even though we have the two sentimentsand the ESE, there is still a gap between the UnitedStates in the sentence and report in the question.One of the eTargets we add is ?report.?
It is morefeasible for a co-reference system to recognize re-port in both the sentence and the question as thesame thing, than recognizing that the United Statesand report refer to the same concept.Third, in this sentence, according to the newlyadded eTargets, the system knows the writer is nega-tive toward both the United States and State Depart-ment.
When building a knowledge base about thehuman rights report, this reveals that the two entitieshave the same stance toward this topic, even withoutany world knowledge.7 ConclusionThis paper presents an annotation scheme for addingentity and event target annotations to the MPQA cor-pus.
A subset of MPQA has already been annotatedaccording to the new scheme.
We believe that thecorpus will be a valuable new resource for develop-ing entity/event-level sentiment analysis systems tofacilitate NLP applications such as Automatic Ques-tion Answering.Acknowledgements.
This work was supportedin part by DARPA-BAA-12-47 DEFT grant#12475008.
We thank the anonymous reviewers fortheir helpful comments.1327ReferencesYejin Choi and Claire Cardie.
2008.
Learning with com-positional semantics as structural inference for subsen-tential sentiment analysis.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 793?801.
Association for Compu-tational Linguistics.Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013.Benefactive/malefactive event and writer attitude an-notation.
In ACL (2), pages 120?125.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the tenthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 168?177.ACM.Richard Johansson and Alessandro Moschitti.
2013.Relational features in fine-grained opinion analysis.Computational Linguistics, 39(3):473?509.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured mod-els for fine-to-coarse sentiment analysis.
In AnnualMeeting-Association For Computational Linguistics,volume 45, page 432.
Citeseer.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Proceedings of the ACL-02 conference on Empirical methods in natural lan-guage processing-Volume 10, pages 79?86.
Associa-tion for Computational Linguistics.Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to wikipedia.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume1, pages 1375?1384.
Association for ComputationalLinguistics.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP), pages1631?1642.
Citeseer.Swapna Somasundaran, Theresa Wilson, Janyce Wiebe,and Veselin Stoyanov.
2007.
Qa with attitude: Ex-ploiting opinion type analysis for improving questionanswering in on-line discussions and the news.
In In-ternational Conference on Weblogs and Social Media,Boulder, CO.Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.2005.
Multi-Perspective Question Answering us-ing the OpQA corpus.
In Proceedings of the Hu-man Language Technologies Conference/Conferenceon Empirical Methods in Natural Language Process-ing (HLT/EMNLP-2005), pages 923?930, Vancouver,Canada.Ivan Titov and Ryan T McDonald.
2008.
A joint modelof text and aspect ratings for sentiment summarization.In ACL, volume 8, pages 308?316.
Citeseer.Peter D Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th annualmeeting on association for computational linguistics,pages 417?424.
Association for Computational Lin-guistics.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions inlanguage ann.
Language Resources and Evaluation,39(2/3):164?210.Theresa Wilson and Janyce Wiebe.
2003.
Annotatingopinions in the world press.
In Proceedings of the 4thACL SIGdial Workshop on Discourse and Dialogue(SIGdial-03), pages 13?22.Theresa Wilson.
2007.
Fine-grained Subjectivity andSentiment Analysis: Recognizing the Intensity, Polar-ity, and Attitudes of private states.
Ph.D. thesis, Intel-ligent Systems Program, University of Pittsburgh.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of the 2003 conference on Em-pirical methods in natural language processing, pages129?136.
Association for Computational Linguistics.1328
