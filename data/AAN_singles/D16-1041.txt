Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 424?435,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSupervised Distributional Hypernym Discoveryvia Domain AdaptationLuis Espinosa-Anke1, Jose Camacho-Collados2, Claudio Delli Bovi2 and Horacio Saggion11Department of Information and Communication Technologies, Universitat Pompeu Fabra2Department of Computer Science, Sapienza University of Rome1{luis.espinosa,horacio.saggion}@upf.edu2{collados,dellibovi}@di.uniroma1.itAbstractLexical taxonomies are graph-like hierarchi-cal structures that provide a formal representa-tion of knowledge.
Most knowledge graphs todate rely on is-a (hypernymic) relations as thebackbone of their semantic structure.
In thispaper, we propose a supervised distributionalframework for hypernym discovery which op-erates at the sense level, enabling large-scaleautomatic acquisition of disambiguated tax-onomies.
By exploiting semantic regularitiesbetween hyponyms and hypernyms in embed-dings spaces, and integrating a domain clus-tering algorithm, our model becomes sensi-tive to the target data.
We evaluate severalconfigurations of our approach, training withinformation derived from a manually createdknowledge base, along with hypernymic rela-tions obtained from Open Information Extrac-tion systems.
The integration of both sourcesof knowledge yields the best overall results ac-cording to both automatic and manual evalua-tion on ten different domains.1 IntroductionLexical taxonomies (taxonomies henceforth) aregraph-like hierarchical structures where terms arenodes, and are typically organized over a predefinedmerging or splitting criterion (Hwang et al, 2012).By embedding cues about how we perceive con-cepts, and how these concepts generalize in a do-main of knowledge, these resources bear a capacityfor generalization that lies at the core of human cog-nition (Yu et al, 2015) and have become key in Nat-ural Language Processing (NLP) tasks where infer-ence and reasoning have proved to be essential.
Infact, taxonomies have enabled a remarkable numberof novel NLP techniques, e.g.
the contribution ofWordNet (Miller, 1995) to lexical semantics (Pile-hvar et al, 2013; Yu and Dredze, 2014) as wellas various tasks, from word sense disambiguation(Agirre et al, 2014) to information retrieval (Vare-las et al, 2005), question answering (Harabagiu etal., 2003) and textual entailment (Glickman et al,2005).
To date, the application of taxonomies inNLP has consisted mainly of, on one hand, formallyrepresenting a domain of knowledge (e.g.
Food),and, on the other hand, constituting the semanticbackbone of large-scale knowledge repositories suchas ontologies or Knowledge Bases (KBs).In domain knowledge formalization, prominentwork has made use of the web (Kozareva and Hovy,2010), lexico-syntactic patterns (Navigli and Ve-lardi, 2010), syntactic evidence (Luu Anh et al,2014), graph-based algorithms (Fountain and Lap-ata, 2012; Velardi et al, 2013; Bansal et al, 2014) orpopularity of web sources (Luu Anh et al, 2015).
Asfor enabling large-scale knowledge repositories, thistask often tackles the additional problem of disam-biguating word senses and entity mentions.
Notableapproaches of this kind include Yago (Suchanek etal., 2007), WikiTaxonomy (Ponzetto and Strube,2008), and the Wikipedia Bitaxonomy (Flati et al,2014).
In addition, while not being taxonomy learn-ing systems per se, semi-supervised systems for In-formation Extraction such as NELL (Carlson et al,2010) rely crucially on taxonomized concepts andtheir relations within their learning process.Taxonomy learning is roughly based on a two-step process, namely is-a (hypernymic) relation de-424tection, and graph induction.
The hypernym detec-tion phase has gathered much interest not only fortaxonomy learning but also for lexical semantics.
Ithas been addressed by means of pattern-based meth-ods1 (Hearst, 1992; Snow et al, 2004; Kozarevaand Hovy, 2010; Carlson et al, 2010; Boella andDi Caro, 2013; Espinosa-Anke et al, 2016), clus-tering (Yang and Callan, 2009) and graph-based ap-proaches (Fountain and Lapata, 2012; Velardi etal., 2013).
Moreover, work stemming from dis-tributional semantics introduced notions of linguis-tic regularities found in vector representations suchas word embeddings (Mikolov et al, 2013d).
Inthis area, supervised approaches, arguably the mostpopular nowadays, learn a feature vector betweenterm-hypernym vector pairs and train classifiers topredict hypernymic relations.
These pairs may berepresented either as a concatenation of both vec-tors (Baroni et al, 2012), difference (Roller et al,2014), dot-product (Mikolov et al, 2013c), or in-cluding additional linguistic information for LSTM-based learning (Shwartz et al, 2016).In this paper we propose TAXOEMBED2, a hy-pernym detection algorithm based on sense em-beddings, which can be easily applied to the con-struction of lexical taxonomies.
It is designed todiscover hypernymic relations by exploiting lineartransformations in embedding spaces (Mikolov etal., 2013b) and, unlike previous approaches, lever-ages this intuition to learn a specific semantically-aware transformation matrix for each domain ofknowledge.
Our best configuration (ranking firstin two thirds of the experiments conducted) consid-ers two training sources: (1) Manually curated pairsfrom Wikidata (Vrandec?ic?
and Kr?tzsch, 2014); and(2) Hypernymy relations from a KB which inte-grates several Open Information Extraction (OIE)systems (Delli Bovi et al, 2015a).
Since our methoduses a very large semantic network as referencesense inventory, we are able to perform jointly hy-pernym extraction and disambiguation, from which1The terminology is not entirely unified in this respect.
Inaddition to pattern-based (Fountain and Lapata, 2012; Bansal etal., 2014; Yu et al, 2015), other terms like path-based (Shwartzet al, 2016) or rule-based (Navigli and Velardi, 2010) are alsoused.2Data and source code available from the following link:www.taln.upf.edu/taxoembed.expanding existing ontologies becomes a trivial task.Compared to word-level taxonomy learning, TAXO-EMBED results in more refined and unambiguoushypernymic relations at the sense level, with a directapplication in tasks such as semantic search.
Eval-uation (both manual and automatic) shows that wecan effectively replicate the Wikidata is-a branch,and capture previously unseen relations in other ref-erence taxonomies (YAGO or WIBI).2 Related WorkPattern-based methods for hypernym identificationexploit the joint co-ocurrence of term and hyper-nym in text corpora.
Building up on Hearst?s pat-terns (Hearst, 1992), these approaches have focusedon, for instance, exploiting templates for harvestingcandidate instances which are ranked via mutual in-formation (Etzioni et al, 2005), training a classi-fier with WordNet hypernymic relations combinedwith syntactic dependencies (Snow et al, 2006),or applying a doubly-anchored method (Kozarevaand Hovy, 2010), which queries the web with twosemantically related terms for collecting domain-specific corpora.
Syntactic information is also usedfor supervised definition and hypernym extraction(Navigli and Velardi, 2010; Boella and Di Caro,2013), or together with Wikipedia-specific heuris-tics (Flati et al, 2014).
One of the main drawbacksof these methods is that they require both term andhypernym to co-occur in text within a certain win-dow, which strongly hinders their recall.
Higher re-call can be achieved thanks to distributional meth-ods, as they do not have co-occurrence requirements.In addition, they can be tailored to cover any num-ber of predefined semantic relations such as co-hyponymy or meronymy (Baroni and Lenci, 2011),but also cause-effect or entity-origin (Hendrickx etal., 2009).
However, they are often more impreciseand seem to perform best in discovering broader se-mantic relations (Shwartz et al, 2016).One way to surmount the issue of generality wasproposed by Fu et al (2014), who explored the pos-sibility to learn a hypernymic transformation matrixover a word embeddings space.
As shown empiri-cally in Fu et al?s original work, the hypernymic re-lation that holds for the pair (dragonfly, insect) dif-fers from the one of e.g.
(carpenter, man).
Prior to425training, their system addresses this discrepancy viak-means clustering using a held-out development setfor tuning.The previously described methods for hypernymand taxonomy learning operate inherently at the sur-face level.
This is partly due to the way evaluationis conducted, which is often limited to very spe-cific domains with no integrative potential (e.g.
tax-onomies in food, science or equipment fromBordea et al (2015)), or restricted to lists of wordpairs.
Hence, a drawback of surface-level taxonomylearning, apart from ambiguity issues, is that theyrequire additional and error-prone steps to identifysemantic clusters (Fu et al, 2014).Alternatively, recent advances in OIE basedon disambiguation and deeper semantic analysis(Nakashole et al, 2012; Grycner and Weikum, 2014;Delli Bovi et al, 2015b) have shown their potentialto construct taxonomized disambiguated resourcesboth at node and at relation level.
However, in ad-dition to their inherently broader scope, OIE ap-proaches are designed to achieve high coverage, andhence they tend to produce noisier data compared totaxonomy learning systems.In our sense-based approach, instead, not onlydo we leverage an unambiguous vector representa-tion for hypernym discovery, but we also take ad-vantage of a domain-wise clustering strategy to di-rectly obtain specific term-hypernym training pairs,thereby substantially refining this step.
Additionally,we exploit the complementary knowledge of OIEsystems by incorporating high-confidence relationtriples drawn from OIE-derived resources, yieldingthe best average configuration as evaluated on tendifferent domains of knowledge.3 PreliminariesTAXOEMBED leverages the vast amounts of train-ing data available from structured and unstructuredknowledge resources, along with the mappingamong these resources and a state-of-the-art vectorrepresentation of word senses.BabelNet3 (Navigli and Ponzetto, 2012) con-stitutes our sense inventory, as it is currently thelargest single multilingual repository of named en-3http://babelnet.orgtities and concepts, integrating various resourcessuch as WordNet, Wikipedia or Wikidata.
As inWordNet, BabelNet is structured in synsets.
Eachsynset is composed of a set of words (lexicaliza-tions or senses) representing the same meaning.
Forinstance, the synset referring to the members of abusiness organization is represented by the set ofsenses firm, house, business firm.
BabelNet containsaround 14M synsets in total.
We exploit BabelNet4as (1) A repository for the manually-curated hyper-nymic relations included in Wikidata; (2) A seman-tic pivot of the integration of several OIE systemsinto one single resource, namely KB-UNIFY; and(3) A sense inventory for the SENSEMBED vectorrepresentations.
In the following we provide furtherdetails about each of these resources.3.1 Training DataWikidata5 (Vrandec?ic?
and Kr?tzsch, 2014) is adocument-oriented semantic database operatedby the Wikimedia Foundation with the goal ofproviding a common source of data that can beused by other Wikimedia projects.
Our initialtraining set W consists of the hypernym branchof Wikidata, specifically the version included inBabelNet.
Each term-hypernym ?
W is in fact apair of BabelNet synsets, e.g.
the synset for Apple(with the company sense), and the concept company.KB-UNIFY6 (Delli Bovi et al, 2015a) (KB-U) isa knowledge-based approach, based on BabelNet,for integrating the output of different OIE systemsinto a single unified and disambiguated knowledgerepository.
The unification algorithm takes as inputa set K of OIE-derived resources, each of which ismodeled as a set of ?entity, relation, entity?
triples,and comprises two subsequent stages: in the firstdisambiguation stage, each KB in K is linked to thesense inventory of BabelNet by disambiguating itsrelation argument pairs; in the following alignmentstage, equivalent relations across different KB in Kare merged together.
As a result, KB-U generatesa KB of triples where arguments are linked tothe corresponding BabelNet synsets, and relationsare replaced by relation synsets of semantically4We use BabelNet 3.0 release version in our experiments.5https://www.wikidata.org6http://lcl.uniroma1.it/kb-unify426similar OIE-derived relation patterns.
The originalexperimental setup of KB-UNIFY included NELL(Carlson et al, 2010) as one of its input resources:since NELL features its own manually-built taxo-nomic structure and relation type inventory (henceits own is-a relation type), we identified the relationsynset containing NELL?s is-a7 and then drew fromthe unified KB all the corresponding triples, whichwe denote as K. These triples constitute, similarlyas in the previous case, a set of term-hypernympairs automatically extracted from OIE-derivedresources, with a disambiguation confidence ofabove 0.9 according to the disambiguation strategydescribed in the original paper.Initially, |W| = 5,301,867 and |K| = 1,358,949.3.2 Sense vectorsSENSEMBED (Iacobacci et al, 2015)8 constitutesthe sense embeddings space that we use for train-ing our hypernym detection algorithm.
Vectors inthe SENSEMBED space, denoted as S, are latentcontinuous representations of word senses based onthe Word2Vec architecture (Mikolov et al, 2013a),which was applied on a disambiguated Wikipediacorpus.
Each vector ~v ?
S represents a BabelNetsense, i.e.
a synset alng with one of its lexical-izations (e.g.
album_chart_bn:00002488n).
Thisdiffers from unsupervised approaches (Huang et al,2012; Tian et al, 2014; Neelakantan et al, 2014)that learn sense representations from text corporaonly and are not mapped to any lexical resource, lim-iting their application in our task.4 MethodologyOur approach can be summarized as follows.
First,we take advantage of a clustering algorithm for allo-cating each BabelNet synset of the training set intoa domain cluster C (Section 4.1).
Then, we expandthe training set by exploiting the different lexical-izations available for each BabelNet synset (Section4.2).
Finally, we learn a cluster-wise linear pro-jection (a hypernym transformation matrix) over allpairs (term-hypernym) of the expanded training set(Section 4.3).7represented by the relation generalizations.8http://lcl.uniroma1.it/sensembed4.1 Domain ClusteringFu et al (2014) induced semantic clusters via k-means, where k was tuned on a development set.Instead, we aim at learning a function sensitive toa predefined knowledge domain, under the assump-tion that vectors clustered with this criterion arelikely to exhibit similar semantic properties (e.g.similarity).
First, we allocate each synset into itsmost representative domain, which is achieved byexploiting the set of thirty four domains availablein the Wikipedia featured articles page9.
Warfare,transport, or music are some of these domains.In the Wikipedia featured articles page each domainis composed of 128 Wikipedia pages on average.Then, in order to expand the set of concepts as-sociated with each domain, we leverage NASARI10(Camacho-Collados et al, 2015), a distributional ap-proach that has been used to construct explicit vectorrepresentations of BabelNet synsets.Our goal is to associate BabelNet synsets with do-mains.
To this end, we follow Camacho-Colladoset al (2016) and build a lexical vector for eachWikipedia domain by concatenating all Wikipediapages representing the given domain into a singletext.
Finally, given a BabelNet synset b, we calculatethe similarity between its corresponding NASARIlexical vector and all the domain vectors, selectingthe domain leading to the highest similarity score:d?
(b) = maxd?DWO(~d,~b) (1)whereD is the set of all thirty-three domains, ~d is thevector of the domain d ?
D, ~b is the vector of theBabelNet synset b, and WO refers to the WeightedOverlap comparison measure (Pilehvar et al, 2013),which is defined as follows:WO(~v1, ~v2) =????
?w?O(rankw, ~v1 + rankw, ~v2)?1?|O|i=1(2i)?1(2)where rankw,~vi is the rank of the word w in the vec-tor ~vi according to its weight, and O is the set ofoverlapping words between the two vectors.
In orderto have a highly reliable set of domain labels, those9https://en.wikipedia.org/wiki/Wikipedia:Featured_articles10http://lcl.uniroma1.it/nasari427synsets whose maximum similarity score is belowa certain threshold are not annotated with any do-main.
We fixed the threshold to 0.35, which pro-vided a fine balance between precision (estimated inaround 85%) and recall in our development set.
Byfollowing this approach almost 2 million synsets arelabelled with a domain.4.2 Training Data ExpansionPrior to training our model, we benefit from thefact that a given BabelNet synset may be associ-ated with a fixed number of lexicalizations or senses,i.e.
different ways of referring to the same con-cept, to expand our set of training pairs.
For in-stance, the synset b associated with the concept mu-sic_album is represented by the set of lexicalizationsLb = {album, music_album .
.
.
album_project}.We take advantage of this synset representation toexpand each term-hypernym synset pair.
For eachterm-hypernym pair, both concepts are expanded totheir given lexicalizations and thus, each synset pairterm-hypernym in the training data is expanded to aset of |Lt|.|Lh| sense training pairs.This expansion step results in much larger setsW?
and K?, where |W?| = 18,291,330 and |K?| =15,362,268.
Specifically, they are 3 and 11 timesbigger than the original training sets described inSection 3.1.
These numbers are higher than those re-ported in recent approaches for hypernym detection,which exploited Chinese semantic thesauri alongwith manual validation of hypernym pairs (Fu et al,2014) (obtaining a total of 1,391 instances), or pairsfrom knowledge resources such as Wikidata, Yago,WordNet and DBpedia (Shwartz et al, 2016), wherethe maximum reported split for training data (70%)amounted to 49,475 pairs.4.3 Learning a Hypernym Detection MatrixThe gist of our approach lies on the property of cur-rent semantic vector space models to capture rela-tions between vectors, in our case hypernymy.
Thiscan be found even in disjoint spaces, where thisproperty has been exploited for machine translation(Mikolov et al, 2013b) or language normalization(Tan et al, 2015).
For our purposes, however, in-stead of learning a global linear transformation func-tion in two spaces over a broad relation like hyper-nymy, we learn a function sensitive to a given do-main of knowledge.
Thus, our training data becomesrestricted to those term-hypernym BabelNet sensepairs (xd, yd) ?
Cd?Cd, where Cd is the cluster ofBabelNet synsets labelled with the domain d.For each domain-wise expanded training set T d,we construct a hyponym matrix Xd = [~xd1 .
.
.
~xdn]and a hypernym matrix Yd = [~ydi .
.
.
~ydn], which arecomposed by the corresponding SENSEMBED vec-tors of the training pairs (xdi , ydi)?
Cd ?
Cd, 0 ?i ?
n.Under the intuition that there exists a matrix ?
sothat ~yd = ?~xd, we learn a transformation matrix foreach domain cluster Cd by minimizing:min?C|T d|?i=1?
?C~xdi ?
~ydi ?2 (3)Then, for any unseen term xd, we obtain a rankedlist of the most likely hypernyms of its lexicalizationvectors ~xjd, using as measure cosine similarity:argmax~v?S~v ?
?C ~xjd||~v||||?C ~xjd||(4)At this point, we have associated with each sensevector a ranked list of candidate hypernym vectors.However, in the (frequent) cases in which one synsethas more than one lexicalization, we need to con-dense the results into one single list of candidates,which we achieve with a simple ranking function?(?
), which we compute as ?
(~v) = cos(~v,?C~xd)rank(~v) ,where rank(~v) is the rank of ~v according to its co-sine similarity with ?C~xd.The above operations allow us to cast the hyper-nym detection task as a ranking problem.
This isalso particularly interesting to enable a flexible eval-uation framework where we can combine highly de-manding metrics for the quality of the candidategiven at a certain rank, as well as other measureswhich consider the rank of the first valid retrievedcandidate.5 EvaluationThe performance of TAXOEMBED is evaluated byconducting several experiments, both automatic andmanual.
Specifically, we assess its ability to re-turn valid hypernyms for a given unseen term with428a held-out evaluation dataset of 250 Wikidata term-hypernym pairs (Section 5.1).
In addition, we as-sess the extent to which TAXOEMBED is able to cor-rectly identify hypernyms outside of Wikidata (Sec-tion 5.2).5.1 Experiment 1: Automatic Evaluation5.1.1 Experimental settingFor each domain, we retain 5k, 10k, 15k, 20k and25k Wikidata term-hypernym training pairs for dif-ferent experiments, and evaluate on 250 test pairsfor each of the 10 domains.
Moreover, we aim atimproving TAXOEMBED by including 1k and 25kextra OIE-derived training pairs per domain (gen-erating two more systems, namely 25k+Kd1k and25k+Kd25k).
These OIE-derived instances are thosecontained in KB-U (see Section 3.1).
Moreover, inorder to quantify the empirically grounded intuitionof the need to train a cluster-wise transformation ma-trix (Fu et al, 2014), we also introduce an additionalconfiguration at 25k (25k+Kr50k), where we include50k additional pairs randomly from KB-U, and twomore settings with only random pairs coming fromWikidata (100krwd) and KB-U (100k+rkbu).We also include a distributional supervised base-line11 based on word analogies (Mikolov et al,2013a), computed as follows.
First, we calculate thedifference vector of each training SENSEMBED vec-tor pair (~xd,~yd) of a given domain d. Then, we aver-age all the difference vectors of all training pairs toobtain a global vector ~Vd for the domain d. Finally,given a test term t we calculate the closest vector ofthe sum of the corresponding term vector and ~Vd:t?
= argmax~v?S ~Vd + ~t (5)This baseline has shown to capture different se-mantic relations and to improve as training data in-creases (Mikolov et al, 2013a).Evaluation metrics.
We computed, for each do-main and for the above configurations, the follow-ing metrics: Mean Reciprocal Rank (MRR), MeanAverage Precision (MAP), and R-Precision (R-P).These measures provide insights on different aspectsof the outcome of the task, e.g.
how often valid hy-pernyms were retrieved in the first positions of the11Using the 25k domain-filtered expanded Wikidata pairs astraining set.rank (MRR), and if there were more than one validhypernym, whether this set was correctly retrieved,(MAP and R-P)12.5.1.2 Results and discussionWe summarize the main outcome of our experi-ments in Table 1.
Results suggest that the perfor-mance of TAXOEMBED increases as training dataexpands.
This is consistent with the findings shownin Mikolov et al (2013b), who showed a substantialimprovement in accuracy in the machine translationtask by gradually increasing the training set.
Ad-ditionally, the improvement of TAXOEMBED overthe baseline is consistent across most evaluation do-main clusters and metrics, with domain-filtered datafrom KB-U contributing to the learning process inabout two thirds of the evaluated configurations.These are very encouraging results considering thenoisy nature of OIE systems, and that the resourcewe obtained from KB-U is the result of error-pronesteps such as Word Sense Disambiguation and En-tity Linking, as well as relation clustering.As far as the individual domains are concerned,the biology domain seems to be easier to modelthan the rest, likely due to the fact that fauna andflora are areas where hierarchical division of speciesis a field of study in itself, which traces back to Aris-totelian times (Mayr, 1982), and therefore has beenconstantly refined over the years.
Also, it is no-table how well the 100krwd configuration performson this domain.
This is the only domain in whichtraining with no semantic awareness gives good re-sults.
We argue that this is highly likely due tothe fact that a vast amount of synsets are allocatedinto the biology cluster (60% of them, and upto 80% in hypernym position).
This produces theso-called lexical memorization phenomenon (Levyet al, 2015), as the system memorizes prototypicalbiology-related hypernyms like taxon as valid hy-pernyms for many concepts.
This contrasts with thelower presence of other domains, e.g.
5% in media,4% in music, or 2% in transport.Another remarkable case involves theeducation and media domains, which ex-perience the highest improvement when trainingwith KB-U (5 and 6 MRR points, respectively).12See Bian et al (2008) for an in-depth analysis of these met-rics.429art biology education geography healthTrain MRR MAP R-P MRR MAP R-P MRR MAP R-P MRR MAP R-P MRR MAP R-P5k 0.12 0.12 0.12 0.63 0.63 0.59 0.00 0.00 0.00 0.08 0.07 0.07 0.08 0.08 0.0715k 0.21 0.20 0.18 0.84 0.72 0.79 0.22 0.22 0.21 0.15 0.14 0.14 0.08 0.07 0.0725k 0.29 0.27 0.26 0.84 0.83 0.81 0.33 0.32 0.30 0.23 0.22 0.21 0.09 0.09 0.0825k+Kd1k 0.29 0.28 0.26 0.84 0.80 0.79 0.32 0.29 0.27 0.22 0.22 0.21 0.09 0.09 0.0825k+Kd25k 0.26 0.24 0.22 0.70 0.63 0.56 0.38 0.36 0.33 0.15 0.13 0.12 0.11 0.11 0.1025k+Kr50k 0.28 0.26 0.24 0.82 0.77 0.72 0.36 0.33 0.30 0.17 0.16 0.16 0.12 0.11 0.10100krwd 0.00 0.00 0.00 0.84 0.81 0.77 0.00 0.00 0.00 0.01 0.01 0.01 0.07 0.06 0.06100krkbu 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.12 0.12 0.11Baseline 0.13 0.12 0.10 0.58 0.57 0.57 0.10 0.10 0.09 0.12 0.09 0.05 0.07 0.13 0.14media music physics transport warfareTrain MRR MAP R-P MRR MAP R-P MRR MAP R-P MRR MAP R-P MRR MAP R-P5k 0.28 0.28 0.27 0.10 0.10 0.09 0.01 0.01 0.00 0.01 0.01 0.01 0.01 0.01 0.0115k 0.14 0.13 0.12 0.08 0.07 0.07 0.36 0.35 0.34 0.25 0.23 0.21 0.01 0.01 0.0125k 0.46 0.45 0.43 0.30 0.28 0.26 0.41 0.40 0.38 0.46 0.43 0.39 0.05 0.05 0.0425k+Kd1k 0.43 0.42 0.41 0.32 0.30 0.28 0.39 0.38 0.37 0.47 0.44 0.40 0.04 0.04 0.0125k+Kd25k 0.52 0.51 0.49 0.26 0.25 0.23 0.37 0.36 0.34 0.48 0.45 0.41 0.04 0.03 0.0325k+Kr50k 0.46 0.45 0.43 0.29 0.28 0.25 0.31 0.30 0.29 0.52 0.49 0.46 0.05 0.04 0.04100krwd 0.00 0.00 0.00 0.00 0.00 0.00 0.01 0.01 0.01 0.01 0.02 0.02 0.02 0.02 0.01100krkbu 0.08 0.07 0.07 0.01 0.01 0.00 0.00 0.00 0.00 0.10 0.10 0.10 0.00 0.00 0.00Baseline 0.57 0.43 0.52 0.03 0.03 0.03 0.05 0.04 0.04 0.29 0.25 0.21 0.04 0.04 0.04Table 1: Overview of the performance of TAXOEMBED using different training data samples.One of the main sources for is-a relations in KB-Uis NELL, which contains a vast amount of relationtriples between North American academic entities(professors, sports teams, alumni, donators; aswell as media celebrities).
Many of these entitiesare missing in Wikidata, and relations amongthem encoded in NELL are likely to be correctbecause in most cases these are unambiguousentities which occur in the same communicativecontexts.
For example, leveraging KB-U we wereable to include the pair (university_of_north_wales,four_year_college), which is absent in Wikidata.
Infact, many high quality is-a pairs like this can befound in KB-U for these two domains.We also computed P@k (number of valid hyper-nyms on the first k returned candidates), where kranges from 1 to 5.
Numbers are on the line of the re-sults shown in Table 1 and therefore are not providedin detail.
The main trend we found is showcased inFigure 1, which shows an illustrative example fromthe transport domain.
As we can see, all val-ues of k exhibit a similar performance curve, with agradual increase of performance as the training setbecomes larger.Figure 1: P@k scores for the transport domain.False positives.
We complement this experimentwith a manual evaluation of theoretical false posi-tives.
Our intuition is that due to the nature of thetask, some domains may be more flexible in allow-430ing two terms to encode an is-a relation, while othersmay be more restrictive.
We asked human judges tomanually validate a sample of 200 wrong pairs fromour best run in each domain, and estimated precisionover them.
As expected, hard science domains likephysics obtain very low results (about 1% preci-sion).
In contrast, other domains like education(12% precision), or transport (16% precision),probably due to their multidisciplinary nature, allowmore valid hypernyms for a given term than what iscurrently encoded in Wikidata.5.2 Experiment 2: Extra-CoverageIn this experiment we evaluate the performance ofTAXOEMBED on instances not included in Wiki-data.
We describe the experimental setting in Sec-tion 5.2.1 and present the results in Section 5.2.2.5.2.1 Experimental settingFor this experiment we use two configurations ofTAXOEMBED: the first one includes 25k domain-wise expanded training pairs (TaxE25k), whereas thesecond one adds 1k pairs from KB-U (TaxE25k+Kd).We randomly extract 200 test BabelNet synsets (20per domain) whose hypernyms are missing in Wiki-data.
We compare against a number of taxon-omy learning and Information Extraction systems,namely Yago (Suchanek et al, 2007), WiBi (Flatiet al, 2014) and DefIE (Delli Bovi et al, 2015b).Yago and WiBi are used as upper bounds due to thenature of their hypernymic relations.
They includea great number of manually-encoded taxonomies(e.g.
exploiting WordNet and Wikipedia categories).Yago derives its taxonomic relations from an au-tomatic mapping between WordNet and Wikipediacategories.
WiBi, on the other hand, exploits, amonga number of different Wikipedia-specific heuristics,categories and the syntactic structure of the intro-ductory sentence of Wikipedia pages.
Finally, DefIEis an automaic OIE system relying on the syntacticstructure of pre-disambiguated definitions13.
Threeannotators manually evaluated the validity of the hy-pernyms extracted by each system (one per test in-stance).13For this experiment, we included DefIE?s is-a relationsonly.5.2.2 Results and discussionTable 2 shows the results of TAXOEMBED and allcomparison systems.
As expected, Yago and WiBiachieve the best overall results.
However, TAXOEM-BED, based solely on distributional information, per-formed competitively in detecting new hypernymswhen compared to DefIE, improving its recall inmost domains, and even surpassing Yago in techni-cal areas like biology or health.
However, ourmodel does not perform particularly well on mediaand physics.
In most domains our model is ableto discover novel hypernym relations that are notcaptured by any other system (e.g.
therapy for ra-diation treatment planning in the health domainor decoration for molding in the art domain)14.In fact, the overlap between our approach and theremaining systems is actually quite small (on aver-age less than 25% with all of them on the Extra-Coverage experiment).
This is mainly due to the factthat TAXOEMBED only exploits distributional infor-mation and does not make use of predefined syntac-tic heuristics, suggesting that the information it pro-vides and the rule-based comparison systems maybe complementary.
We foresee a potential avenuefocused on combining a supervised distributionalapproach such as TAXOEMBED with syntactically-motivated systems such as Wibi or Yago.
Thiscombination of a distributional system and manualpatterns was already introduced by Shwartz et al(2016) on the hypernym detection task with highlyencouraging results.6 ConclusionWe have presented TAXOEMBED, a supervised tax-onomy learning framework exploiting the propertythat was observed in Fu et al (2014), namelythat there exists, for a given domain-specific ter-minology, a shared linear projection among term-hypernym pairs.
We showed how this can be usedto learn a hypernym transformation matrix for dis-covering novel is-a relations, which are the back-bone of lexical taxonomies.
First, we allocate al-most 2M BabelNet synsets into a predefined domainof knowledge.
Then, we collect training data bothfrom a manually constructed knowledge base (Wiki-14For simplicity, we use the word surface form to refer toBabelNet synsets.431art biology education geography healthP R F P R F P R F P R F P R FTaxE25k 0.45 0.45 0.45 0.40 0.40 0.40 0.60 0.60 0.60 0.35 0.35 0.35 0.45 0.45 0.45TaxE25k+Kd 0.50 0.50 0.50 0.40 0.40 0.40 0.55 0.55 0.55 0.35 0.35 0.35 0.45 0.45 0.45DefIE 0.63 0.35 0.45 0.36 0.20 0.25 0.57 0.20 0.29 0.66 0.40 0.50 0.25 0.15 0.18Yago 0.88 0.75 0.81 0.62 0.25 0.36 0.94 0.80 0.86 0.79 0.75 0.77 0.28 0.10 0.15Wibi 0.70 0.70 0.70 0.58 0.50 0.54 0.94 0.80 0.86 0.75 0.75 0.75 0.66 0.50 0.57media music physics transport warfareP R F P R F P R F P R F P R FTaxE25k 0.10 0.10 0.10 0.45 0.45 0.45 0.15 0.15 0.15 0.35 0.35 0.35 0.25 0.25 0.25TaxE25k+Kd 0.10 0.10 0.10 0.40 0.40 0.40 0.15 0.15 0.15 0.25 0.25 0.25 0.45 0.45 0.45DefIE 0.81 0.45 0.58 0.71 0.50 0.58 0.42 0.15 0.22 0.54 0.30 0.38 0.60 0.30 0.40Yago 0.76 0.65 0.70 0.84 0.55 0.67 0.80 0.40 0.53 0.93 0.70 0.80 0.81 0.65 0.72Wibi 0.90 0.90 0.90 0.89 0.85 0.87 0.68 0.55 0.61 0.87 0.70 0.77 0.66 0.50 0.57Table 2: Precision, recall and F-Measure between TAXOEMBED, two taxonomy learning systems (Yago andWiBi), and a pattern-based approach that performs hypernym extraction (DefIE).data), and from OIE systems.
We substantially ex-pand our initial training set by expanding both termsand hypernyms to all their available senses, and in alast step, to their corresponding disambiguated vec-tor representations.Evaluation shows that the general trend is that ourhypernym matrix improves as we increase trainingdata.
Our best domain-wise configuration combines25k training pairs from Wikidata and additionalpairs from an OIE-derived KB, achieving promis-ing results.
The domains in which the addition ofthe OIE-based information contributed the most areeducation, transport and media.
For in-stance, in the case of education, this may be dueto the over representation of the North American ed-ucational system in IE systems like NELL.
We ac-company this quantitative evaluation with manualassessment of precision of false positives, and ananalysis of the potential coverage comparing it withknowledge taxonomies like Yago or WiBi, and withDefIE, a quasi-OIE system.7 Future WorkFor future work we are planning to apply this strat-egy to learn large-scale semantic relations beyondhypernymy.
This may constitute a first step towardsa global and fully automatic ontology learning sys-tem.
In the context of semantic web, we would liketo include semantic parsers and distant supervisionto our algorithm in order to capture n-ary relationsbetween pairs of concepts to further create and im-prove existing KBs.As mentioned in Section 5.2.2, we are also plan-ning to combine our distributional approach withrule-based heuristics, following the line of work in-troduced by Shwartz et al (2016).
Finally, we seepotential in the domain clustering approach for im-proving graph-based taxonomy learning systems, asit can serve as a weighting measure as to how perti-nent a given set of concepts in a taxonomy are for aspecific domain.AcknowledgmentsThis work is partially funded by the Spanish Min-istry of Economy and Competitiveness under theMaria de Maeztu Units of Excellence Programme(MDM-2015-0502) and under the TUNER project(TIN2015-65308-C5-5-R, MINECO/FEDER, UE).The authors also acknowledge support from Dr. In-ventor (FP7-ICT-2013.8.1611383).
Jos?
Camacho-Collados is supported by a Google Doctoral Fellow-ship in Natural Language Processing.
We wouldalso like to thank Tommaso Pasini for helping us tocompute the Wibi and Yago baselines in our secondexperiment.432ReferencesEneko Agirre, Oier Lopez de Lacalle, and Aitor Soroa.2014.
Random walks for knowledge-based wordsense disambiguation.
Computational Linguistics,40(1):57?84.Mohit Bansal, David Burkett, Gerard De Melo, and DanKlein.
2014.
Structured learning for taxonomy induc-tion with belief propagation.
In ACL (1), pages 1041?1051.Marco Baroni and Alessandro Lenci.
2011.
How weblessed distributional semantic evaluation.
In Pro-ceedings of the GEMS 2011 Workshop on GEometricalModels of Natural Language Semantics, pages 1?10.Association for Computational Linguistics.Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do, andChung-chieh Shan.
2012.
Entailment above the wordlevel in distributional semantics.
In Proceedings ofEACL, pages 23?32.Jiang Bian, Yandong Liu, Eugene Agichtein, andHongyuan Zha.
2008.
Finding the right facts in thecrowd: factoid question answering over social media.In Proceedings of the 17th international conference onWorld Wide Web, pages 467?476.
ACM.Guido Boella and Luigi Di Caro.
2013.
Super-vised learning of syntactic contexts for uncoveringdefinitions and extracting hypernym relations in textdatabases.
In Machine learning and knowledge dis-covery in databases, pages 64?79.
Springer.Georgeta Bordea, Paul Buitelaar, Stefano Faralli, andRoberto Navigli.
2015.
Semeval-2015 task 17: Tax-onomy extraction evaluation (texeval).
In Proceedingsof the SemEval workshop.Jos?
Camacho-Collados, Mohammad Taher Pilehvar, andRoberto Navigli.
2015.
A Unified Multilingual Se-mantic Representation of Concepts.
In Proceedings ofACL, pages 741?751, Beijing, China.Jos?
Camacho-Collados, Mohammad Taher Pilehvar, andRoberto Navigli.
2016.
Nasari: Integrating explicitknowledge and corpus statistics for a multilingual rep-resentation of concepts and entities.
Artificial Intelli-gence, 240:36?64.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an Architecture for Never-Ending Language Learning.
In Proceedings of AAAI,pages 1306?1313.Claudio Delli Bovi, Luis Espinosa Anke, and RobertoNavigli.
2015a.
Knowledge base unification via senseembeddings and disambiguation.
In Proceedings ofEMNLP, pages 726?736, Lisbon, Portugal, September.Association for Computational Linguistics.Claudio Delli Bovi, Luca Telesca, and Roberto Navigli.2015b.
Large-scale information extraction from tex-tual definitions through deep syntactic and semanticanalysis.
TACL, 3:529?543.Luis Espinosa-Anke, Horacio Saggion, Francesco Ron-zano, and Roberto Navigli.
2016.
Extasem!
ex-tending, taxonomizing and semantifying domain ter-minologies.
AAAI.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S Weld, and Alexander Yates.
2005.
Unsuper-vised named-entity extraction from the web: An exper-imental study.
Artificial intelligence, 165(1):91?134.Tiziano Flati, Daniele Vannella, Tommaso Pasini, andRoberto Navigli.
2014.
Two is bigger (and better)than one: the wikipedia bitaxonomy project.
In ACL.Trevor Fountain and Mirella Lapata.
2012.
Taxonomyinduction using hierarchical random graphs.
In Pro-ceedings of NAACL, pages 466?476.
Association forComputational Linguistics.Ruiji Fu, Jiang Guo, Bing Qin, Wanxiang Che, HaifengWang, and Ting Liu.
2014.
Learning semantic hier-archies via word embeddings.
In Proceedings of ACL,volume 1.Oren Glickman, Ido Dagan, and Moshe Koppel.
2005.
Aprobabilistic classification approach for lexical textualentailment.
In Proceedings of the National ConferenceOn Artificial Intelligence, page 1050.Adam Grycner and Gerhard Weikum.
2014.
Harpy: Hy-pernyms and alignment of relational paraphrases.
InProceedings of COLING, pages 2195?2204, Dublin,Ireland.Sanda M Harabagiu, Steven J Maiorano, and Marius APasca.
2003.
Open-domain textual question an-swering techniques.
Natural Language Engineering,9(03):231?267.Marti A Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings of the14th conference on Computational linguistics, pages539?545.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, PreslavNakov, Diarmuid ?
S?aghdha, Sebastian Pad?, MarcoPennacchiotti, Lorenza Romano, and Stan Szpakow-icz.
2009.
Semeval-2010 task 8: Multi-way classifi-cation of semantic relations between pairs of nominals.In Proceedings of SemEval: Recent Achievements andFuture Directions, pages 94?99.Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y. Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of ACL, pages 873?882, Jeju Island,Korea.Sung Ju Hwang, Kristen Grauman, and Fei Sha.
2012.Semantic kernel forests from multiple taxonomies.
In433Advances in Neural Information Processing Systems,pages 1718?1726.Ignacio Iacobacci, Mohammad Taher Pilehvar, andRoberto Navigli.
2015.
SensEmbed: Learning senseembeddings for word and relational similarity.
In Pro-ceedings of ACL, pages 95?105, Beijing, China.Zornitsa Kozareva and Eduard Hovy.
2010.
A semi-supervised method to learn and construct taxonomiesusing the web.
In Proceedings of EMNLP, pages1110?1118.Omer Levy, Steffen Remus, Chris Biemann, Ido Dagan,and Israel Ramat-Gan.
2015.
Do supervised distribu-tional methods really learn lexical inference relations?In NAACL 2015, Denver, Colorado, USA.Tuan Luu Anh, Jung-jae Kim, and See Kiong Ng.
2014.Taxonomy construction using syntactic contextual ev-idence.
In Proceedings of EMNLP, pages 810?819.Tuan Luu Anh, Jung-jae Kim, and See-Kiong Ng.2015.
Incorporating trustiness and collective syn-onym/contrastive evidence into taxonomy construc-tion.
In Proceedings of EMNLP, pages 1013?1022.Ernst Mayr.
1982.
The growth of biological thought:Diversity, evolution, and inheritance.
Harvard Univer-sity Press.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
CoRR, abs/1301.3781.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b.Exploiting similarities among languages for machinetranslation.
arXiv preprint arXiv:1309.4168.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013c.
Distributed represen-tations of words and phrases and their composition-ality.
In Advances in Neural Information ProcessingSystems, pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013d.
Linguistic regularities in continuous spaceword representations.
In HLT-NAACL, pages 746?751.George A Miller.
1995.
Wordnet: a lexical database forenglish.
Communications of the ACM, 38(11):39?41.Ndapandula Nakashole, Gerhard Weikum, and Fabian M.Suchanek.
2012.
PATTY: A Taxonomy of Rela-tional Patterns with Semantic Types.
In Proceedingsof EMNLP-CoNLL, pages 1135?1145.Roberto Navigli and Simone Paolo Ponzetto.
2012.
Ba-belNet: The automatic construction, evaluation andapplication of a wide-coverage multilingual semanticnetwork.
Artificial Intelligence, 193:217?250.Roberto Navigli and Paola Velardi.
2010.
Learningword-class lattices for definition and hypernym extrac-tion.
In ACL, pages 1318?1327.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In Proceedings of EMNLP,pages 1059?1069, Doha, Qatar.Mohammad Taher Pilehvar, David Jurgens, and RobertoNavigli.
2013.
Align, Disambiguate and Walk: aUnified Approach for Measuring Semantic Similarity.In Proceedings of ACL, pages 1341?1351, Sofia, Bul-garia.Simone Paolo Ponzetto and Michael Strube.
2008.
Wik-itaxonomy: A large scale knowledge resource.
InECAI, volume 178, pages 751?752.Stephen Roller, Katrin Erk, and Gemma Boleda.
2014.Inclusive yet selective: Supervised distributional hy-pernymy detection.
In Proceedings of COLING 2014,Dublin, Ireland.Vered Shwartz, Yoav Goldberg, and Ido Dagan.
2016.Improving hypernymy detection with an integratedpath-based and distributional method.
arXiv preprintarXiv:1603.06076.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2004.Learning syntactic patterns for automatic hypernymdiscovery.
Advances in Neural Information Process-ing Systems 17.Rion Snow, Daniel Jurafsky, and Andrew Y Ng.
2006.Semantic taxonomy induction from heterogenous evi-dence.
In Proceedings of COLING/ACL 2006, pages801?808.Fabian M Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A core of semantic knowledge.In WWW, pages 697?706.
ACM.L.
Tan, H. Zhang, C. Clarke, and M. Smucker.
2015.Lexical comparison between wikipedia and twittercorpora by using word embeddings.
In Proceedingsof ACL (2), pages 657?661, Beijing, China, July.Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,Enhong Chen, and Tie-Yan Liu.
2014.
A probabilisticmodel for learning multi-prototype word embeddings.In COLING, pages 151?160.Giannis Varelas, Epimenidis Voutsakis, ParaskeviRaftopoulou, Euripides GM Petrakis, and Evange-los E Milios.
2005.
Semantic similarity methods inwordnet and their application to information retrievalon the web.
In Proceedings of the 7th annual ACMinternational workshop on Web information and datamanagement, pages 10?16.
ACM.Paola Velardi, Stefano Faralli, and Roberto Navigli.2013.
OntoLearn Reloaded: A graph-based algorithmfor taxonomy induction.
Computational Linguistics,39(3):665?707.Denny Vrandec?ic?
and Markus Kr?tzsch.
2014.
Wiki-data: a free collaborative knowledgebase.
Communi-cations of the ACM, 57(10):78?85.434Hui Yang and Jamie Callan.
2009.
A metric-basedframework for automatic taxonomy induction.
In Pro-ceedings of ACL/IJCNLP, pages 271?279.
Associationfor Computational Linguistics.Mo Yu and Mark Dredze.
2014.
Improving lexical em-beddings with semantic knowledge.
In ACL (2), pages545?550.Zheng Yu, Haixun Wang, Xuemin Lin, and Min Wang.2015.
Learning term embeddings for hypernymy iden-tification.
In Proceedings of IJCAI, pages 1390?1397.435
