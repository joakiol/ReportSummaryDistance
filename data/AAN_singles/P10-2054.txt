Proceedings of the ACL 2010 Conference Short Papers, pages 291?295,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAn Entity-Level Approach to Information ExtractionAria HaghighiUC Berkeley, CS Divisionaria42@cs.berkeley.eduDan KleinUC Berkeley, CS Divisionklein@cs.berkeley.eduAbstractWe present a generative model oftemplate-filling in which coreferenceresolution and role assignment are jointlydetermined.
Underlying template rolesfirst generate abstract entities, which inturn generate concrete textual mentions.On the standard corporate acquisitionsdataset, joint resolution in our entity-levelmodel reduces error over a mention-leveldiscriminative approach by up to 20%.1 IntroductionTemplate-filling information extraction (IE) sys-tems must merge information across multiple sen-tences to identify all role fillers of interest.
Forinstance, in the MUC4 terrorism event extrac-tion task, the entity filling the individual perpetra-tor role often occurs multiple times, variously asproper, nominal, or pronominal mentions.
How-ever, most template-filling systems (Freitag andMcCallum, 2000; Patwardhan and Riloff, 2007)assign roles to individual textual mentions usingonly local context as evidence, leaving aggrega-tion for post-processing.
While prior work hasacknowledged that coreference resolution and dis-course analysis are integral to accurate role identi-fication, to our knowledge no model has been pro-posed which jointly models these phenomena.In this work, we describe an entity-centered ap-proach to template-filling IE problems.
Our modeljointly merges surface mentions into underlyingentities (coreference resolution) and assigns rolesto those discovered entities.
In the generative pro-cess proposed here, document entities are gener-ated for each template role, along with a set ofnon-template entities.
These entities then generatementions in a process sensitive to both lexical andstructural properties of the mention.
Our modeloutperforms a discriminative mention-level base-line.
Moreover, since our model is generative, it[S CSR] has said that [S it] has sold [S its]  [B oilinterests] held in  [A Delhi Fund].
[P Esso Inc.] did notdisclose how much [P they] paid for [A Dehli].
(a)(b)DocumentEsso Inc.PURCHASERACQUIREDDelhi FundOil and GasBUSINESSCSR LimitedSELLERTemplateFigure 1: Example of the corporate acquisitions role-fillingtask.
In (a), an example template specifying the entities play-ing each domain role.
In (b), an example document withcoreferent mentions sharing the same role label.
Note thatpronoun mentions provide direct clues to entity roles.can naturally incorporate unannotated data, whichfurther increases accuracy.2 Problem SettingFigure 1(a) shows an example template-fillingtask from the corporate acquisitions domain (Fre-itag, 1998).1 We have a template of K roles(PURCHASER, AMOUNT, etc.)
and we must iden-tify which entity (if any) fills each role (CSR Lim-ited, etc.).
Often such problems are modeled at themention level, directly labeling individual men-tions as in Figure 1(b).
Indeed, in this data set,the mention-level perspective is evident in the goldannotations, which ignore pronominal references.However, roles in this domain appear in several lo-cations throughout the document, with pronominalmentions often carrying the critical informationfor template filling.
Therefore, Section 3 presentsa model in which entities are explicitly modeled,naturally merging information across all mentiontypes and explicitly representing latent structurevery much like the entity-level template structurefrom Figure 1(a).1In Freitag (1998), some of these fields are split in two todistinguish a full versus abbreviated name, but we ignore thisdistinction.
Also we ignore the status field as it doesn?t applyto entities and its meaning is not consistent.291R1 R2 RKZ1 Z2 ZnM1 M2 Mn...........DocumentRole Entity ParametersMentions?RolePriorsE1 E2M3Z3 ...........EKOtherEntities.... ....Other Entity Parameters....
....Entity Indicators1[1: 0.02,0:0.015,2: 0.01,...]MOD-APPOS[company: 0.02,firm:0.015,group: 0.01,...][1: 0.19,2:0.14,0: 0.08,...]HEAD-NAM[Inc.: 0.02,Corp.:0.015,Ltd.
: 0.01,...][2: 0.18,3:0.12,1: 0.09,...]GOV-NSUBJfr?rr[bought: 0.02,obtained:0.015,acquired: 0.01,...]Purchaser RoleRoleEntitesCaliforniaMOD-PREPMOD-NN search, giantcompanyHEAD-NOMHEAD-NAMLrrGoogle, GOOGPurchaser EntityGOV-NSUBJ boughtHEAD-NAM GooglewrPurchaser MentionFigure 2: Graphical model depiction of our generative model described in Section 3.
Sample values are illustrated for keyparameters and latent variables.3 ModelWe describe our generative model for a document,which has many similarities to the coreference-only model of Haghighi and Klein (2010), butwhich integrally models template role-fillers.
Webriefly describe the key abstractions of our model.Mentions: A mention is an observed textualreference to a latent real-world entity.
Mentionsare associated with nodes in a parse tree and aretypically realized as NPs.
There are three ba-sic forms of mentions: proper (NAM), nominal(NOM), and pronominal (PRO).
Each mention Mis represented as collection of key-value pairs.The keys are called properties and the values arewords.
The set of properties utilized here, de-noted R, are the same as in Haghighi and Klein(2010) and consist of the mention head, its depen-dencies, and its governor.
See Figure 2 for a con-crete example.
Mention types are trivially deter-mined from mention head POS tag.
All mentionproperties and their values are observed.Entities: An entity is a specific individual orobject in the world.
Entities are always latent intext.
Where a mention has a single word for eachproperty, an entity has a list of signature words.Formally, entities are mappings from propertiesr ?
R to lists Lr of ?canonical?
words which thatentity uses for that property.Roles: The elements we have described so farare standard in many coreference systems.
Ourmodel performs role-filling by assuming that eachentity is drawn from an underlying role.
Theseroles include theK template roles as well as ?junk?roles to represent entities which do not fill a tem-plate role (see Section 5.2).
Each role R is rep-resented as a mapping between properties r andpairs of multinomials (?r, fr).
?r is a unigram dis-tribution of words for property r that are seman-tically licensed for the role (e.g., being the sub-ject of ?acquired?
for the ACQUIRED role).
fr is a?fertility?
distribution over the integers that char-acterizes entity list lengths.
Together, these distri-butions control the lists Lr for entities which in-stantiate the role.We first present a broad sketch of our model?scomponents and then detail each in a subsequentsection.
We temporarily assume that all men-tions belong to a template role-filling entity; welift this restriction in Section 5.2.
First, a se-mantic component generates a sequence of enti-ties E = (E1, .
.
.
, EK), where each Ei is gen-erated from a corresponding role Ri.
We useR = (R1, .
.
.
, RK) to denote the vector of tem-plate role parameters.
Note that this work assumesthat there is a one-to-one mapping between entitiesand roles; in particular, at most one entity can filleach role.
This assumption is appropriate for thedomain considered here.Once entities have been generated, a dis-course component generates which entities will beevoked in each of the n mention positions.
Werepresent these choices using entity indicators de-noted by Z = (Z1, .
.
.
, Zn).
This component uti-lizes a learned global prior ?
over roles.
The Zi in-292dicators take values in 1, .
.
.
,K indicating the en-tity number (and thereby the role) underlying theith mention position.
Finally, a mention genera-tion component renders each mention conditionedon the underlying entity and role.
Formally:P (E,Z,M|R, ?)
=(K?i=1P (Ei|Ri))[Semantic, Sec.
3.1]?
?n?j=1P (Zj |Z<j , ?)??
[Discourse, Sec.
3.2]?
?n?j=1P (Mj |EZj , RZj )??
[Mention, Sec.
3.3]3.1 Semantic ComponentEach role R generates an entity E as follows: foreach mention property r, a word list, Lr, is drawnby first generating a list length from the corre-sponding fr distribution in R.2 This list is thenpopulated by an independent draw from R?s uni-gram distribution ?r.
Formally, for each r ?
R, anentity word list is drawn according to,3P (Lr|R) = P (len(Lr)|fr)?w?LrP (w|?r)3.2 Discourse ComponentThe discourse component draws the entity indica-tor Zj for the jth mention according to,P (Zj |Z<j , ?)
={P (Zj |?
), if non-pronominal?j?
1[Zj = Zj?
]P (j?|j), o.w.When the jth mention is non-pronominal, we drawZj from ?, a global prior over the K roles.
WhenMj is a pronoun, we first draw an antecedent men-tion position j?, such that j?
< j, and then we setZj = Zj?
.
The antecedent position is selected ac-cording to the distribution,P (j?|j) ?
exp{?
?TREEDIST(j?, j)}where TREEDIST(j?,j) represents the tree distancebetween the parse nodes forMj andMj?
.4 Mass is2There is one exception: the sizes of the proper and nom-inal head property lists are jointly generated, but their wordlists are still independently populated.3While, in principle, this process can yield word lists withduplicate words, we constrain the model during inference tonot allow that to occur.4Sentence parse trees are merged into a right-branchingdocument parse tree.
This allows us to extend tree distance tointer-sentence nodes.restricted to antecedent mention positions j?
whichoccur earlier in the same sentence or in the previ-ous sentence.53.3 Mention GenerationOnce the entity indicator has been drawn, we gen-erate words associated with mention conditionedon the underlying entity E and role R. For eachmention property r associated with the mention,a word w is drawn utilizing E?s word list Lr aswell as the multinomials (fr, ?r) from roleR.
Theword w is drawn according to,P (w|E,R)=(1?
?r)1 [w ?
Lr]len(Lr)+ ?rP (w|?r)For each property r, there is a hyper-parameter ?rwhich interpolates between selecting a word uni-formly from the entity list Lr and drawing fromthe underlying role distribution ?r.
Intuitively, asmall ?r indicates that an entity prefers to re-use asmall number of words for property r. This is typi-cally the case for proper and nominal heads as wellas modifiers.
At the other extreme, setting ?r to 1indicates the property isn?t particular to the entityitself, but rather always drawn from the underly-ing role distribution.
We set ?r to 1 for pronounheads as well as for the governor properties.4 Learning and InferenceSince we will make use of unannotated data (seeSection 5), we utilize a variational EM algorithmto learn parameters R and ?.
The E-Step re-quires the posterior P (E,Z|R,M, ?
), which isintractable to compute exactly.
We approximateit using a surrogate variational distribution of thefollowing factored form:Q(E,Z) =(K?i=1qi(Ei))??n?j=1rj(Zj)?
?Each rj(Zj) is a distribution over the entity in-dicator for mention Mj , which approximates thetrue posterior of Zj .
Similarly, qi(Ei) approxi-mates the posterior over entity Ei which is asso-ciated with role Ri.
As is standard, we iterativelyupdate each component distribution to minimizeKL-divergence, fixing all other distributions:qi ?
argminqiKL(Q(E,Z)|P (E,Z|M,R, ?)?
exp{EQ/qi lnP (E,Z|M,R, ?
))}5The sole parameter ?
is fixed at 0.1.293Ment Acc.
Ent.
Acc.INDEP 60.0 43.7JOINT 64.6 54.2JOINT+PRO 68.2 57.8Table 1: Results on corporate acquisition tasks with givenrole mention boundaries.
We report mention role accuracyand entity role accuracy (correctly labeling all entity men-tions).For example, the update for a non-pronominalentity indicator component rj(?)
is given by:6ln rj(z) ?
EQ/rj lnP (E,Z,M|R, ?)?
Eqz ln (P (z|?
)P (Mj |Ez, Rz))= lnP (z|?)
+ Eqz lnP (Mj |Ez, Rz)A similar update is performed on pronominal en-tity indicator distributions, which we omit here forspace.
The update for variational entity distribu-tion is given by:ln qi(ei) ?
EQ/qi lnP (E,Z,M|R, ?)?
E{rj} ln?
?P (ei|Ri)?j:Zj=iP (Mj |ei, Ri)?
?= lnP (ei|Ri) +?jrj(i) lnP (Mj |ei, Ri)It is intractable to enumerate all possible entitiesei (each consisting of several sets of words).
Weinstead limit the support of qi(ei) to several sam-pled entities.
We obtain entity samples by sam-pling mention entity indicators according to rj .For a given sample, we assume that Ei consistsof the non-pronominal head words and modifiersof mentions such that Zj has sampled value i.During the E-Step, we perform 5 iterations ofupdating each variational factor, which results inan approximate posterior distribution.
Using ex-pectations from this approximate posterior, our M-Step is relatively straightforward.
The role param-eters Ri are computed from the qi(ei) and rj(z)distributions, and the global role prior ?
from thenon-pronominal components of rj(z).5 ExperimentsWe present results on the corporate acquisitionstask, which consists of 600 annotated documentssplit into a 300/300 train/test split.
We use 50training documents as a development set.
In all6For simplicity of exposition, we omit terms where Mj isan antecedent to a pronoun.documents, proper and (usually) nominal men-tions are annotated with roles, while pronouns arenot.
We preprocess each document identically toHaghighi and Klein (2010): we sentence-segmentusing the OpenNLP toolkit, parse sentences withthe Berkeley Parser (Petrov et al, 2006), and ex-tract mention properties from parse trees and theStanford Dependency Extractor (de Marneffe etal., 2006).5.1 Gold Role BoundariesWe first consider the simplified task where rolemention boundaries are given.
We map each la-beled token span in training and test data to a parsetree node that shares the same head.
In this set-ting, the role-filling task is a collective classifica-tion problem, since we know each mention is fill-ing some role.As our baseline, INDEP, we built a maxi-mum entropy model which independently classi-fies each mention?s role.
It uses features as similaras possible to the generative model (and more), in-cluding the head word, typed dependencies of thehead, various tree features, governing word, andseveral conjunctions of these features as well ascoarser versions of lexicalized features.
This sys-tem yields 60.0 mention labeling accuracy (see Ta-ble 1).
The primary difficulty in classification isthe disambiguation amongst the acquired, seller,and purchaser roles, which have similar internalstructure, and differ primarily in their semanticcontexts.
Our entity-centered model, JOINT in Ta-ble 1, has no latent variables at training time in thissetting, since each role maps to a unique entity.This model yields 64.6, outperforming INDEP.7During development, we noted that often themost direct evidence of the role of an entity wasassociated with pronoun usage (see the first ?it?in Figure 1).
Training our model with pronominalmentions, whose roles are latent variables at train-ing time, improves accuracy to 68.2.85.2 Full TaskWe now consider the more difficult setting whererole mention boundaries are not provided at testtime.
In this setting, we automatically extractmentions from a parse tree using a heuristic ap-7We use the mode of the variational posteriors rj(Zj) tomake predictions (see Section 4).8While this approach incorrectly assumes that all pro-nouns have antecedents amongst our given mentions, this didnot appear to degrade performance.294ROLE ID OVERALLP R F1 P R F1INDEP 79.0 65.5 71.6 48.6 40.3 44.0JOINT+PRO 80.3 69.2 74.3 53.4 46.4 49.7BEST 80.1 70.1 74.8 57.3 49.2 52.9Table 2: Results on corporate acquisitions data where men-tion boundaries are not provided.
Systems must determinewhich mentions are template role-fillers as well as label them.ROLE ID only evaluates the binary decision of whether amention is a template role-filler or not.
OVERALL includescorrectly labeling mentions.
Our BEST system, see Sec-tion 5, adds extra unannotated data to our JOINT+PRO sys-tem.proach.
Our mention extraction procedure yields95% recall over annotated role mentions and 45%precision.9 Using extracted mentions as input, ourtask is to label some subset of the mentions withtemplate roles.
Since systems can label mentionsas non-role bearing, only recall is critical to men-tion extraction.
To adapt INDEP to this setting, wefirst use a binary classifier trained to distinguishrole-bearing mentions.
The baseline then classi-fies mentions which pass this first phase as before.We add ?junk?
roles to our model to flexibly modelentities that do not correspond to annotated tem-plate roles.
During training, extracted mentionswhich are not matched in the labeled data haveposteriors which are constrained to be amongst the?junk?
roles.We first evaluate role identification (ROLE ID inTable 2), the task of identifying mentions whichplay some role in the template.
The binary clas-sifier for INDEP yields 71.6 F1.
Our JOINT+PROsystem yields 74.3.
On the task of identifying andcorrectly labeling role mentions, our model out-performs INDEP as well (OVERALL in Table 2).
Asour model is generative, it is straightforward to uti-lize totally unannotated data.
We added 700 fullyunannotated documents from the mergers and ac-quisitions portion of the Reuters 21857 corpus.Training JOINT+PRO on this data as well as ouroriginal training data yields the best performance(BEST in Table 2).10To our knowledge, the best previously pub-lished results on this dataset are from Siefkes(2008), who report 45.9 weighted F1.
Our BESTsystem evaluated in their slightly stricter wayyields 51.1.9Following Patwardhan and Riloff (2009), we match ex-tracted mentions to labeled spans if the head of the mentionmatches the labeled span.10We scaled expected counts from the unlabeled data sothat they did not overwhelm those from our (partially) labeleddata.6 ConclusionWe have presented a joint generative model ofcoreference resolution and role-filling informationextraction.
This model makes role decisions atthe entity, rather than at the mention level.
Thisapproach naturally aggregates information acrossmultiple mentions, incorporates unannotated data,and yields strong performance.Acknowledgements: This project is funded inpart by the Office of Naval Research under MURIGrant No.
N000140911081.ReferencesM.
C. de Marneffe, B. Maccartney, and C. D. Man-ning.
2006.
Generating typed dependency parsesfrom phrase structure parses.
In LREC.Dayne Freitag and Andrew McCallum.
2000.
Infor-mation extraction with hmm structures learned bystochastic optimization.
In Association for the Ad-vancement of Artificial Intelligence (AAAI).Dayne Freitag.
1998.
Machine learning for informa-tion extraction in informal domains.A.
Haghighi and D. Klein.
2010.
Coreference resolu-tion in a modular, entity-centered model.
In NorthAmerican Association of Computational Linguistics(NAACL).P.
Liang and D. Klein.
2007.
Structured Bayesian non-parametric models with variational inference (tuto-rial).
In Association for Computational Linguistics(ACL).S.
Patwardhan and E. Riloff.
2007.
Effective infor-mation extraction with semantic affinity patterns andrelevant regions.
In Joint Conference on EmpiricalMethods in Natural Language Processing.S.
Patwardhan and E Riloff.
2009.
A unified model ofphrasal and sentential evidence for information ex-traction.
In Empirical Methods in Natural LanguageProcessing (EMNLP).Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433?440,Sydney, Australia, July.
Association for Computa-tional Linguistics.Christian Siefkes.
2008.
An Incrementally Train-able Statistical Approach to Information Extraction:Based on Token Classification and Rich ContextModel.
VDM Verlag, Saarbru?cken, Germany, Ger-many.295
