Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 143?148,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsAutomatic Generation of Challenging DistractorsUsing Context-Sensitive Inference RulesTorsten ZeschLanguage Technology LabUniversity of Duisburg-Essentorsten.zesch@uni-due.deOren MelamudComputer Science DepartmentBar-Ilan Universitymelamuo@cs.biu.ac.ilAbstractAutomatically generating challenging dis-tractors for multiple-choice gap-fill itemsis still an unsolved problem.
We proposeto employ context-sensitive lexical infer-ence rules in order to generate distractorsthat are semantically similar to the gap tar-get word in some sense, but not in the par-ticular sense induced by the gap-fill con-text.
We hypothesize that such distrac-tors should be particularly hard to distin-guish from the correct answer.
We focuson verbs as they are especially difficult tomaster for language learners and find thatour approach is quite effective.
In our testset of 20 items, our proposed method de-creases the number of invalid distractors in90% of the cases, and fully eliminates allof them in 65%.
Further analysis on thatdataset does not support our hypothesis re-garding item difficulty as measured by av-erage error rate of language learners.
Weconjecture that this may be due to limita-tions in our evaluation setting, which weplan to address in future work.1 IntroductionMultiple-choice gap-fill items as illustrated in Fig-ure 1 are frequently used for both testing lan-guage proficiency and as a learning device.
Eachitem consists of a carrier sentence that providesthe context to a target word.
The target word isblanked and presented as one possible gap-fill an-swer together with a certain number (usually 3)of distractors.
Given a desired target word, car-rier sentences containing it can be automaticallyselected from a corpus.
Some methods even selectonly sentences where the target word is used in acertain sense (Liu et al., 2005).
Then, the mainproblem is to pick challenging distractors that areFigure 1: Multiple-choice gap-fill item.reasonably hard to distinguish from the correct an-swer (i.e.
the target word) on one hand, yet cannotbe considered as correct answers on the other.In this paper we propose to generate distrac-tors that are semantically similar to the gap tar-get word in some sense, but not in the particu-lar sense induced by the gap-fill context, therebymaking them difficult to distinguish from the tar-get word.
For example, the distractor gain in Fig-ure 1 is semantically similar to acquire, but is notappropriate in the particular context of purchasingcompanies, and therefore has high distractive po-tential.
On the other hand, the distractor purchaseis a correct answer in this context and is thereforean invalid distractor.
To generate challenging dis-tractors, we utilize context-sensitive lexical infer-ence rules that can discriminate between appropri-ate substitutes of a target word given its contextand other inappropriate substitutes.In the next section, we give an overview of pre-vious work in order to place our contribution intocontext.2 Previous WorkThe process of finding good distractors involvestwo steps: Candidate Selection controls the diffi-culty of the items, while Reliability Checking en-sures that the items remain solvable, i.e.
it ensures143that there is only one correct answer.
We notethat this work is focused on single-word distrac-tors rather than phrases (Gates et al., 2011), andonly on target isolated carrier sentences rather thanlonger texts as in (Mostow and Jang, 2012).2.1 Candidates SelectionIn some settings the set of possible distractors isknown in advance, e.g.
the set of English prepo-sitions in preposition exercises (Lee and Seneff,2007) or a confusion set with previously knownerrors like {two, too, to}.
Sakaguchi et al.
(2013)use data from the Lang-8 platform (a corpus ofmanually annotated errors1) in order to determinetypical learner errors and use them as distractors.However, in the common setting only the targetword is known and the set of distractors needs tobe automatically generated.Randomly selecting distractors is a valid strat-egy (Mostow and Jang, 2012), but it is only suit-able for the most beginner learners.
More ad-vanced learners can easily rule out distractors thatdo not fit grammatically or are too unrelated se-mantically.
Thus, more advanced approaches usu-ally employ basic strategies, such as choosing dis-tractors with the same part-of-speech tag as thetarget word, or distractors with a corpus frequencycomparable to the target word (Hoshino and Naka-gawa, 2007) (based on the assumption that corpusfrequency roughly correlates with word difficulty).Pino and Eskenazi (2009) use distractors that aremorphologically, orthographically, or phoneticallysimilar (e.g.
bread ?
beard).Another approach used in previous works tomake distractors more challenging is utilizing the-sauri (Sumita et al., 2005; Smith and Avinesh,2010) or taxonomies (Hoshino and Nakagawa,2007; Mitkov et al., 2009) to select words that aresemantically similar to the target word.
In addi-tion to the target word, some approaches also con-sider the semantic relatedness of distractors withthe whole carrier sentence or paragraph (Pino etal., 2008; Agarwal and Mannem, 2011; Mostowand Jang, 2012), i.e.
they pick distractors that arefrom the same domain as the target word.Generally, selecting more challenging distrac-tors usually means making them more similar tothe target word.
As this increases the probabilitythat a distractor might actually be another correctanswer, we need a more sophisticated approach for1http://cl.naist.jp/nldata/lang-8/checking the reliability of the distractor set.2.2 Reliability CheckingIn order to make sure that there is only one correctanswer to a gap-fill item, there needs to be a wayto decide for each distractor whether it fits into thecontext of the carrier sentence or not.
In thosecases, where we have a limited list of potential tar-get words and distractors, e.g.
in preposition exer-cises (Lee and Seneff, 2007), a supervised classi-fier can be trained to do this job.
Given enoughtraining data, this approach yields very high preci-sion, but it cannot be easily applied to open wordclasses like nouns or verbs, which are much largerand dynamic in nature.When we do not have a closed list of potentialdistractors at hand, one way to perform reliabil-ity checking is by considering collocations involv-ing the target word (Pino et al., 2008; Smith andAvinesh, 2010).
For example, if the target wordis strong, we can find the collocation strong tea.Then we can use powerful as a distractor becauseit is semantically similar to strong, yet *powerfultea is not a valid collocation.
This approach is ef-fective, but requires strong collocations to discrim-inate between valid and invalid distractors.
There-fore it cannot be used with carrier sentences thatdo not contain strong collocations, such as the sen-tence in Figure 1.Sumita et al.
(2005) apply a simple web searchapproach to judge the reliability of an item.
Theycheck whether the carrier sentence with the targetword replaced by the distractor can be found onthe web.
If such a sentence is found, the distrac-tor is discarded.
We note that the applicability ofthis approach is limited, as finding exact matchesfor such artificial sentences can be unlikely dueto sparseness of natural languages.
Therefore notfinding an exact match does not necessarily ruleout the possibility of an invalid distractor.3 Automatic Generation of ChallengingDistractorsOur goal is to automatically generate distractorsthat are as ?close?
to the target word as possible,yet do not fit the carrier sentence context.
To ac-complish this, our strategy is to first generate a setof distractor candidates, which are semanticallysimilar to the target word.
Then we use context-sensitive lexical inference rules to filter candidatesthat fit the context, and thus cannot be used as dis-144tractors.
In the remainder of this section we de-scribe this procedure in more detail.3.1 Context-Sensitive Inference RulesA lexical inference rule ?LHS ?
RHS?, such as?acquire ?
purchase?, specifies a directional in-ference relation between two words (or terms).
Arule can be applied when its LHS matches a wordin a text T , and then that word is substituted forRHS, yielding the modified text H .
For example,applying the rule above to ?Microsoft acquiredSkype?, yields ?Microsoft purchased Skype?.
If therule is true then the meaning of H is inferred fromthe meaning of T .
A popular way to learn lex-ical inference rules in an unsupervised setting isby using distributional similarity models (Lin andPantel, 2001; Kotlerman et al., 2010).
Under thisapproach, target words are represented as vectorsof context features, and the score of a rule betweentwo target words is based on vector arithmetics.One of the main shortcomings of such rules isthat they are context-insensitive, i.e.
they have asingle score, which is not assessed with respect tothe concrete context T under which they are ap-plied.
However, the appropriateness of an infer-ence rule may in fact depend on this context.
Forexample, ?Microsoft acquire Skype ?
Microsoftpurchase Skype?, is an appropriate application ofthe rule ?acquire ?
purchase?, while ?Childrenacquire skills ?
Children purchase skills?
is not.To address this issue, additional models were in-troduced that compute a different context-sensitivescore per each context T , under which it is applied(Dinu and Lapata, 2010; Melamud et al., 2013).In this work, we use the resource providedby Melamud et al.
(2013), which includes bothcontext-sensitive and context-insensitive rules forover 2,000 frequent verbs.2We use these rules togenerate challenging distractors as we show next.3.2 Distractor Selection & ReliabilityWe start with the following illustrative example tomotivate our approach.
While the words purchaseand acquire are considered to be almost perfectsynonyms in sentences like Microsoft acquiresSkype and Microsoft purchases Skype, this is nottrue for all contexts.
For example, in Childrenacquire skills vs. Children purchase skills, themeaning is clearly not equivalent.
These context-dependent senses, which are particularly typical to2http://www.cs.biu.ac.il/nlp/downloads/wt-rules.htmlFigure 2: Filtering context-insensitive substitu-tions with context-sensitive ones in order to getchallenging distractors.verbs, make it difficult for learners to understandhow to properly use these words.Acquiring such fine-grained sense distinctionskills is a prerequisite for really competent lan-guage usage.
These skills can be trained and testedwith distractors, such as purchase in the exam-ple above.
Therefore, such items are good indi-cators in language proficiency testing, and shouldbe specifically trained when learning a language.To generate such challenging distractors, wefirst use the context-insensitive rules, whose LHSmatches the carrier sentence target word, to createa distractor candidate set as illustrated on the left-hand side of Figure 2.
We include in this set thetop-n inferred words that correspond to the high-est rule scores.
These candidate words are inferredby the target word, but not necessarily in the par-ticular context of the carrier sentence.
Therefore,we expect this set to include both correct answers,which would render the item unreliable, as wellas good distractors that are semantically similar tothe target word in some sense, but not in the par-ticular sense induced by the carrier sentence.Next, we use context-sensitive rules to generatea distractor black-list including the top-m wordsthat are inferred by the target word, but this timetaking the context of the carrier sentence into con-sideration.
In this case, we expect the words inthe list to comprise only the gap-fillers that fit thegiven context as illustrated on the right-hand sideof Figure 2.
Such gap-fillers are correct answersand therefore cannot be used as distractors.
Fi-nally, we subtract the black-list distractors fromthe initial distractor candidate set and expect theremaining candidates to comprise only good dis-tractors.
We consider the candidates in this finalset as our generated distractors.1453.3 Distractor RankingIn case our approach returns a large number ofgood distractors, we should use ranking to selectthe most challenging ones.
A simple strategy isto rely on the corpus frequency of the distractor,where less frequent means more challenging as itwill not be known to the learner.
However, thistends to put a focus on the more obscure wordsof the vocabulary while actually the more frequentwords should be trained more often.
Therefore, inthis work we use the scores that were assigned tothe distractors by the context-insensitive inferencerules.
Accordingly, the more similar a distractor isto the target word, the higher rank it will get (pro-vided that it was not in the distractor black-list).4 Experiments & ResultsIn our experiments we wanted to test two hy-potheses: (i) whether context-sensitive inferencerules are able to reliably distinguish between validand invalid distractors, and (ii) whether the gener-ated distractors are more challenging for languagelearners than randomly chosen ones.We used the Brown corpus (Nelson Francis andKuc?era, 1964) as a source for carrier sentences andselected medium-sized (5-12 tokens long) sen-tences that contain a main verb.
We then manu-ally inspected this set, keeping only well-formedsentences that are understandable by a general au-dience without requiring too much context knowl-edge.
In a production system, this manual pro-cess would be replaced by a sophisticated methodfor obtaining good carrier sentences, but this is be-yond the scope of this paper.
Finally, for this ex-ploratory study, we only used the first 20 selectedsentences from a much larger set of possible car-rier sentences.4.1 ReliabilityOur first goal was to study the effectiveness of ourapproach in generating reliable items, i.e.
itemswhere the target word is the only correct answer.In order to minimize impact of pre-processing andlemmatization, we provided the context-sensitiveinference rules with correctly lemmatized carriersentences and marked the target verbs.
We foundthat we get better results when using a distractorblack-list that is larger than the distractor candi-date set, as this more aggressively filters invaliddistractors.
We used the top-20 distractor black-list and top-10 distractor candidate set, which leadOnly valid distractors 13/20 (65%)Mix of valid and invalid 5/20 (25%)Only invalid distractors 2/20 (10%)Table 1: Reliability of items after filteringto generating on average 3.3 distractors per item.All our generated distractors were checked bytwo native English speakers.
We count a distrac-tor as ?invalid?
if it was ruled out by at least oneannotator.
Table 1 summarizes the results.
Wefound that in 13 of the 20 items (65%) all distrac-tors generated by our approach were valid, whileonly for 2 items all generated distractors were in-valid.
For the remaining 5 items, our approach re-turned a mix of valid and invalid distractors.
Wenote that the unfiltered distractor candidate set al-ways contained invalid distractors and in 90% ofthe items it contained a higher proportion of in-valid distractors than the filtered one.
This sug-gests that the context-sensitive inference rules arequite effective in differentiating between the dif-ferent senses of the verbs.A main source of error are sentences that do notprovide enough context, e.g.
because the subjectis a pronoun.
In She [served] one four-year termon the national committee, it would be acceptableto insert sold in the context of a report on po-litical corruption, but a more precise subject likeBarack Obama would render that reading muchmore unlikely.
Therefore, more emphasis shouldbe put on selecting better carrier sentences.
Se-lecting longer sentences that provide a richer con-text would help to rule out more distractor candi-dates and may also lead to better results when us-ing the context-sensitive inference rules.
However,long sentences are also more difficult for languagelearners, so there will probably be some trade-off.A qualitative analysis of the results shows thatespecially for verbs with clearly distinct senses,our approach yields good results.
For examplein He [played] basketball there while working to-ward a law degree, our method generates the dis-tractors compose and tune which are both relatedto the ?play a musical instrument?
sense.
An-other example is His petition [charged] mentalcruelty, where our method generates among oth-ers the distractors pay and collect that are both re-lated to the ?charge taxes?
reading of the verb.
Theball [floated] downstream is an example where ourmethod did not work well.
It generated the distrac-tors glide and travel which also fit the context and146Group 1 Group 2Control Items 0.24?
0.12 0.20?
0.12Test Items 0.18?
0.17 0.18?
0.15Table 2: Average error rates on our datasetshould thus not be used as distractors.
The verbfloat is different from the previous examples, asall its dominant senses involve some kind of ?float-ing?
even if only metaphorically used.
This resultsin similar senses that are harder to differentiate.4.2 DifficultyNext, we wanted to examine whether our approachleads to more challenging distractors.
For thatpurpose we removed the distractors that our an-notators identified as invalid in the previous step.We then ranked the remaining distractors accord-ing to the scores assigned to them by the context-sensitive inference rules and selected the top-3 dis-tractors.
If our method generated less than 3 dis-tractors, we randomly generated additional dis-tractors from the same frequency range as the tar-get word.We compared our approach with randomly se-lected distractors that are in the same order ofmagnitude with respect to corpus frequency as thedistractors generated by our method.
This way weensure that a possible change in distractor diffi-culty cannot simply be attributed to differences inthe learners?
familiarity with the distractor verbsdue to their corpus frequency.
We note that ran-dom selection repeatedly created invalid distrac-tors that we needed to manually filter out.
Thisshows that better methods for checking the relia-bility of items like in our approach are definitelyrequired.We randomly split 52 participants (all non-natives) into two groups, each assigned with a dif-ferent test version.
Table 2 summarizes the results.For both groups, the first 7 test items were identi-cal and contained only randomly selected distrac-tors.
Average error rate for these items was 0.24(SD 0.12) for the first group, and 0.20 (SD 0.12)for the second group, suggesting that the results ofthe two groups on the remaining items can be com-pared meaningfully.
The first group was testedon the remaining 13 items with randomly selecteddistractors, while the second group got the sameitems but with distractors created by our method.Contrary to our hypothesis, the average errorrate for both groups was equal (0.18, SD1=0.17,SD2=0.15).
One reason might be that the Englishlanguage skills of the participants (mostly com-puter science students or faculty) were rather high,close to the native level, as shown by the low errorrates.
Furthermore, even if the participants weremore challenged by our distractors, they mighthave been able to finally select the right answerwith no measurable effect on error rate.
Thus, infuture work we want measure answer time insteadof average error rate, in order to counter this effect.We also want to re-run the experiment with lowergrade students, who might not have mastered thekind of sense distinctions that our approach is fo-cused on.5 ConclusionsIn this paper we have tackled the task of generatingchallenging distractors for multiple-choice gap-fillitems.
We propose to employ context-sensitivelexical inference rules in order to generate distrac-tors that are semantically similar to the gap targetword in some sense, but not in the particular senseinduced by the gap-fill context.Our results suggest that our approach is quite ef-fective, reducing the number of invalid distractorsin 90% of the cases, and fully eliminating all ofthem in 65%.
We did not find a difference in aver-age error rate between distractors generated withour method and randomly chosen distractors fromthe same corpus frequency range.
We conjecturethat this may be due to limitations in the setup ofour experiment.Thus, in future work we want to re-run the ex-periment with less experienced participants.
Wealso wish to measure answer time in addition toerror rate, as the distractive powers of a gap-fillermight be reflected in longer answer times morethan in higher error rates.AcknowledgementsWe thank all participants of the gap-fill survey,and Emily Jamison and Tristan Miller for theirhelp with the annotation study.
This work waspartially supported by the European Community?sSeventh Framework Programme (FP7/2007-2013)under grant agreement no.
287923 (EXCITE-MENT).147ReferencesManish Agarwal and Prashanth Mannem.
2011.
Au-tomatic Gap-fill Question Generation from TextBooks.
In Proceedings of the Sixth Workshop on In-novative Use of NLP for Building Educational Ap-plications, pages 56?64.Georgiana Dinu and Mirella Lapata.
2010.
Measur-ing Distributional Similarity in Context.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages1162?1172.Donna Gates, Margaret Mckeown, Juliet Bey, ForbesAve, and Ross Hall.
2011.
How to GenerateCloze Questions from Definitions : A Syntactic Ap-proach.
In Proceedings of the AAAI Fall Symposiumon Question Generation, pages 19?22.Ayako Hoshino and Hiroshi Nakagawa.
2007.
As-sisting Cloze Test Making with a Web Application.In Proceedings of the Society for Information Tech-nology and Teacher Education International Confer-ence, pages 2807?
2814.Lili Kotlerman, Ido Dagan, Idan Szpektor, and MaayanZhitomirsky-Geffet.
2010.
Directional Distribu-tional Similarity for Lexical Inference.
Natural Lan-guage Engineering, 16(4):359?389.John Lee and Stephanie Seneff.
2007.
AutomaticGeneration of Cloze Items for Prepositions.
InProceedings of INTERSPEECH, pages 2173?2176,Antwerp, Belgium.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
Discov-ery of Inference Rules from Text.
In Proceedings ofACM SIGKDD Conference on Knowledge Discoveryand Data Mining 2001.Chao-lin Liu, Chun-hung Wang, and Zhao-ming Gao.2005.
Using Lexical Constraints to Enhance theQuality of Computer-Generated Multiple-ChoiceCloze Items.
Computational Linguistics and Chi-nese Language Processing, 10(3):303?328.Oren Melamud, Jonathan Berant, Ido Dagan, JacobGoldberger, and Idan Szpektor.
2013.
A Two LevelModel for Context Sensitive Inference Rules.
InProceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 1331?1340, Sofia, Bulgaria.Ruslan Mitkov, Le An Ha, Andrea Varga, and LuzRello.
2009.
Semantic Similarity of Distractors inMultiple-choice Tests: Extrinsic Evaluation.
In Pro-ceedings of the Workshop on Geometrical Models ofNatural Language Semantics, pages 49?56.Jack Mostow and Hyeju Jang.
2012.
GeneratingDiagnostic Multiple Choice Comprehension ClozeQuestions.
In Proceedings of the Seventh Workshopon Building Educational Applications Using NLP,pages 136?146, Stroudsburg, PA, USA.W.
Nelson Francis and Henry Kuc?era.
1964.
Manualof Information to Accompany a Standard Corpus ofPresent-day Edited American English, for use withDigital Computers.Juan Pino and Maxine Eskenazi.
2009.
Semi-Automatic Generation of Cloze Question Distrac-tors Effect of Students L1.
In SLaTE Workshop onSpeech and Language Technology in Education.Juan Pino, Michael Heilman, and Maxine Eskenazi.2008.
A Selection Strategy to Improve Cloze Ques-tion Quality.
In Proceedings of the Workshop on In-telligent Tutoring Systems for Ill-Defined Domainsat the 9th Internationnal Conference on IntelligentTutoring Systems.Keisuke Sakaguchi, Yuki Arase, and Mamoru Ko-machi.
2013.
Discriminative Approach to Fill-in-the-Blank Quiz Generation for Language Learners.In Proceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 238?242, Sofia, Bulgaria.Simon Smith and P V S Avinesh.
2010.
Gap-fill Testsfor Language Learners: Corpus-Driven Item Gener-ation.
In Proceedings of ICON-2010: 8th Interna-tional Conference on Natural Language Processing.Eiichiro Sumita, Fumiaki Sugaya, and Seiichi Ya-mamoto.
2005.
Measuring Non-native Speakers?Proficiency of English by Using a Test withAutomatically-generated Fill-in-the-blank Ques-tions.
In Proceedings of the second workshop onBuilding Educational Applications Using NLP,EdAppsNLP 05, pages 61?68, Stroudsburg, PA,USA.148
