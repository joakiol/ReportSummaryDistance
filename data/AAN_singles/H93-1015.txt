COMPARATIVE EXPERIMENTS ON LARGE VOCABULARYSPEECH RECOGNITIONRichard Schwartz, Tasos Anastasakos, Francis Kubala, John Makhoul, Long Nguyen, George ZavaliagkosBBN Systems & Technologies70 Fawcett Street.
Cambridge, MA 02138ABSTRACTThis paper describes everal key experiments in large vocabu-lay  speech recognition.
We demonstrate that, counter to ourintuitions, given a fixed amount of training speech, the num-ber of training speakers has little effect on the accuracy.
Weshow how much speech is needed for speaker-independent (SI)recognition in order to achieve the same performance as speaker-dependent (SD) recognition.
We demonstrate that, though theN-Best Paradigm works quite well up to vocabularies of 5,000words, it begins to break down with 20,000 words and long sen-tences.
We compare the performance of two feature preprocess-ing algorithms for microphone independence and we describe anew microphone adaptation algorithm based on selection amongseveral codebook transformations.1.
INTRODUCTIONDuring the past year, the DARPA program has graduated frommedium vocabulary recognition problems like Resource Manage-ment and ATIS into the large vocabulary dictation of Wall StreetJoumal (WSJ) texts.
With this move comes some changes in com-putational requirements and the possibility that the algorithms thatworked best on smaller vocabularies would not be the same onesthat work best on larger vocabularies.
We found that, while therequired computation certainly increased, the programs that wehad developed on the smaller problems still worked efficientlyenough on the larger problems.
However, while the BYBLOSsystem achieved the lowest word error rate obtained by any sitefor recognition of ATIS speech, the error rates for the WSJ testswere the second lowest of the six sites that tested their systems onthis corpus.
The reader will find more details on the evaluationresults in \[1\].In the sections that follow, we will describe the BBN BYBLOSsystem briefly.
Then we enumerate several modifications to theBBN BYBLOS system.
Following this we will describe fourdifferent experiments hat we performed and the results obtained.2.
BYBLOSAll of the experiments that will be described were performedusing the BBN BYBLOS speech recognition system.
This sys-tem introduced an effective strategy for using context-dependentphonetic hidden Markov models (HMM) and demonstrated theirfeasibility for large vocabulary, continuous peech applications\[2\].
Over the years, the core algorithms have been refined withimproved algorithms for estimating robust speech models and us-ing them effectively to search for the most likely sentence.The system can be trained using the pooled speech of manyspeakers or by training separate models for each speaker and thenaveraging the resulting models.The system can be constrained by any finite-state languagemodel, which includes probabilisfic n-gram models as a specialcase.
Nonfinite-state models can also be used in a post processthrough the N-best Paradigm.The BYBLOS speech recognition system uses a multi-passsearch strategy designed to use progressively more detailed mod-els on a correspondingly reduced search space.
It produces anordered list of the N top-scoring hypotheses which is then re-ordered by several detailed knowledge sources.1.
A forward pass with a bigram grammar and discrete HMMmodels saves the top word-ending scores and times \[6\].2.
A fast time-synchronous backward pass produces an initalN-best list using the Word-Dependent N-best algorithm\[5\].3.
Each of the N hypotheses is rescored with cross-word-boundary triphones and semi-continuous density HMMs.4.
The N-best list can be rescored with a trigram grammar (orany other language model).Each utterance is decoded with each gender-dependent model.For each utterance, the N-best list with the highest op-1 hypoth-esis score is chosen.
The top choice in the final list constitutes thespeech recognition results reported below.
This N-best strategy\[3, 4\] permits the use of otherwise computationally prohibitivemodels by greatly reducing the search space to a few (N=20-100)word sequences.
It has enabled us to use cross-word-bonndarytriphone models and trigram language models with ease.During most of the development of the system we used the1000-Word RM cospus \[8\] for testing.
More recently, the systemhas been used for recognizing spontaneous speech from the ATIScorpus, which contains many spontaneous speech effects, such aspartial words, nonspeech sounds, extraneous.noises, false starts,etc.
The vocabulary of the ATIS domain was about twice thatof the RM corpus.
So there were no significant new problemshaving to do with memory and computation.2.1.
Wall Street Journal CorpusThe Wall Street Joumal (WSJ) pilot CSR corpus contains trainingspeech read from processed versions of the Wall Street Journal.The vocabulary is inherently unlimited.
The text of 35M wordsavailable for language modeling contains about 160,000 different75words.
?~e data used for speech recognition training and testwas constrained to come from sentences that contained only the64,000 most frequent words.There are two speech training sets.
One has 600 sentencesfrom each of 12 speakers (6 male and 6 female).
The other hasa total of 7,200 sentences from 84 different speakers.
The totalvocabulary in the training set is about 13,000 words.
There aretwo different standard bigram language models that are typicallyused - oue with 5,000 (SK) words and one with 20,000 (20K)words.
'Hie 5K language models were designed to include aU ofthe words in the 5K test set.
The 20K language models containthe most likely 20K words in the corpus.
As a result, about 2% ofthe words in the test speech are not in this vocabulary.
In addition,there are two variants depending on whether the punctuation isread out loud: verbalized punctuation (VP) and nonverbalizedpunctuation (NVP).Most of the test speech is read.
In addition to test sets for5K-word and 20K-word vocabularies, there is also spontaneousspeech collected from joumalists who were instructed to dictatea newspaper story.3.
IMPROVEMENTS IN ACCURACYIn this section, we describe several modifications that each re-salted in an improvement in accuracy on the WSJ corpus.
Inall cases, we used the same training set (SI-12) and the standardbigram grammars.
The initial word error rate when testing ona SK-word closed-vocabulasy VP language model was 12.0%~Each of these methods is described below.3.1.
Silence DetectionEven though the training speech is read from prompts, there areoften short pauses either due to natural sentential phrasing, read-ing disfiuency, or nmning out of breath on long sentences.
Nat-urally, the orthographic transcription that is provided with eachutterance does not indicate these pauses.
But it would be incor-rect to model the speech as ff there were no pauses.
In particular,phonetic models that take into account acoustic oarticulation be-tween words (cross-word models) do not function properly ff theyare confounded by unmarked pauses between words.We developed a two-stage training process to deal with thisproblem.
First we train HMM models assuming there are nopauses between words.
Then we mark the missing silence lo-cathms automatically by running the recognizer on the trainingdata constrained to the correct word sequence, but allowing op-tional silence between words.
Then we retrain the model usingthe output of the recognizer as corrected transcriptions.We find that this two-stage process increases the gain due tousing cross-word phonetic models.
The word error was reducedby 0.6% which is about a 5% reduction in word error.3.2.
Phonetic DictionaryTwo distinct phonetic dictionaries were supplied for training andtesting purposes, We found the dictionaries for training and test-ing were not consistent.
That is, there were many words thatappeared in both dictionaries, but had different spellings.
Wealso modified the speRings of several words that we judged to bewrong.
However, after correcting all of these mistakes, includingthe inconsistency between the training and testing dictionary, theimprovement was only 0.2%, which is statistically insignificant.One inadequacy of the supplied dictionary was that it did notcontain any schwa phonemes to represent redue~ vowels.
Itdid, on the other hand, distinguish three levels of stress.
Butwe traditionally remove the stress distinction before using thedictionary.
So we translated all of the lowest stress level of theUH and IH phonemes into AX and IX (We will use RandomHouse symbols here).
This resulted in another 0.2% reduction inword error.Another consideration i designing a phonetic dictionary is thetradeoff between the number of parameters and the accuracy ofthe estimates.
Finer phonetic distinctions in the dictionary canresult in improved modeling, but they also increase the need fortraining data.
Lori Lame1 had previously repoRed \[7\] that theerror rate on the RM corpus was reduced when the number ofphonemes was reduced, ignoring some phonetic distinctions.
Inparticular, she suggested replacing some diphthongs, affricates,and syllabic consonants with two-vowel sequences.
She alsosuggested removing some phonetic distinctions.
The fist of substitutions is listed in Table 1 below.OriginalAYOYOWCHIXUNUMULAENewAH-EEAWH-EEAH-OOHT-SHAXAX-NAX-MAX-LEYOO UHZH ZAH AWTable 1: These phonemes were removed by mapping them toother phonemes or sequences.When we made these substitutions, we found that the worderror rate decreased by 0.2% again.
While this change is notsignificant, the size of the system was subtanfially decreased ueto the smaller number of triphone models.Finally, we reinstated the last tluee phonemes in the list, sincewe were uncomfortable with removing too many distinctions.Again, the word error rate was reduced by another 0.2%.While each of the above improvements was miniscul?, the totalimprovement from changes to the phonetic dictionary was 0.8%,which is about a 7% reduction in word error.
At the same time,we now only have a single phonetic dictionary to keep track of,and the system is substantially smaller.3.3.
Weight Optimi~ationAfter making several changes to the system, we reoptimized therelative weights for the acoustic and language models, as weU asthe word and phoneme insertion penalties.
These weights wereoptimized on the development test set automaticaUy using theN-best lists \[4\].
Optimization of these weights reduced the worderror by 0.4%.3.4.
Cepstral Mean RemovalOne of the areas of interest is recognition when the microphonefor the test speech is unknown.
We tried a few different methods76to solve this problem, which will be described in a later section.However, during the course of trying different methods, we foundthat the simplest of all methods, which is to subtract he meancepstmm from every frame's cepstrum vector actually resulted ina very small improvement in recognition accuracy even when themicrophone was the same for training and test.
This resulted ina 0.3% reduction in word error rate.3.5.
3-Way Gender SelectionIt has become a standard technique to model the speech of maleand female speakers eparately, since the speech of males andfemales is so different.
This typically results in a 10% reductionin error relative to using a single speaker-independent model.However, we have found that there are occassional speakers whodo not match one model much better than the other.
In fact,there are some very rare sentences in which the model of thewrong gender is chosen.
Therefore we experimented with usinga third "gender" model that is the simple gender-independentmodel, derived by averaging the male and the female models.During recognition, we find the answer independently using eachof these models and then we choose the answer that has thehighest overall score.
We find that about one out of 10 speakerswill typically score better using the gender-independent modelthan the model for the correct gender.
In addition, with this thirdmodel, we no longer ever see sentences that are misclassitied asbelonging to the wrong gender.
The reduction error associatedwith using a third gender model was 0.4%.3.6.
Improvement SummaryThe methods we used and the corresponding improvements aresummarized in Table 2 below.Improvement Method0.6%0.80.20.20.20.20.40.30.4silence-detectionimprovements o phonetic dictionaryconsistent dictionaryaddition of schwareduced phoneme setless reduced phoneme setAutomatic optimization of weightsRemoving mean cepstrum, and3-way gender selection2.5% Total improvementTable 2: Absolute reduction in word error due to each improve-ment.All the gains shown were additive, resulting in a total of 2.5%reduction in absolute word error, or about a 20% relative change.4.
COMPARATIVE EXPERIMENTSIn this section we describe several controlled experiments com-paring the accuracy when using different training and recognitionscenarios, and different algorithms.4.1.
Effect of Number of Training SpeakersIt has always been assumed that for speaker independent recogni-tion to work well, we must train the system on as many speakersas possible.
We reported in \[9\] that when we trained a speaker-independent system on 600 sentences from each of 12 differentspeakers (a total of 7,200 sentences), the word error rate wasonly slightly higher than when the system was trained on a totalof 3,990 sentences from 109 speakers.
These experiments wereperformed on the 1000-word Resource Management (RM) Cor-pus.
The results were dit~ficult o interpret because the numberof sentences were not exactly the same for both conditions, thedata for the 109 speakers covered a larger variety of phoneticcontexts than the data for the 12 speakers, and the 12 speakerswere carefully selected to cover the various dialectic regions ofthe country (as well as is possible with only 7 male and 5 femalespeakers).For the first time we were able to perform a well-controlledexpefirnent to answer this question on the large vocabulary WSJcorpus.
The amount of training data is the same in both cases.
Inone condition, there are 12 speakers (6 male and 6 female) with600 sentences each.
In the other case, there are 84 speakers with atotal of 7,200 sentences.
In both cases, all of the sentence scriptsare unique.
"nre speakers in both sets were selected randomly,without any effort to cover the general population.
In both cases,we used separate models for male and female speakers.In a second experiment, we repeated another experiment thathad previously been run only on the RM corpus.
Instead ofpooling all of the training data (for one gender) and estimatinga single model, we trained on the speech of each speaker sep-arately, and then combined all of the resulting models simplyby averaging the densities of the resulting models.
We had pre-viously found that this method worked well when each speakerhad a substantial mount of training speech (enough to estimate aspeaker-dependent model), and all of the speakers had the samesentences in their training.
But in this experiment, we also com-puted separate speaker-dependent models for the speakers with50-100 utterances, and each speaker had different sentences.The resdts of these comparisons are shown in Table 3.Training Pooled AveragedSL84 11.2 12.3SL12 11.6 12.0Table 3: Word error rate for few (SI-12) vs many (SL84) speak-ers, and for a single (Pooled) model vs separately trained (Aver-aged) models.
The experiments were run on the 5K VP closed-vocabulary development test set of the WSJ pilot corpus usingthe standard bigram grammar.We found, to our surprise, that there is almost no advantage forhaving more speakers ff the total amount of speech is fixed.
Wealso that the performance when we trained the system separatelyon each of the speakers and averaged the resulting models, wasquite similar to that when we trained jointly on all of the speakerstogether.
This result was particularly surprising for the SI-84case, in which each speaker had very little training data.More recently we ran this experiment again on the 5K NVPclosed-vocabulary development test set with an improved system,and found that the results for a pooled model from 84 speakerswere almost identical to those with an averaged model from 12speakers (10.9% vs 11.3Both of these results have important implications for practicalspeech corpus collection.
There are many advantages for havinga small number of speakers.
We call this paradigm the Sl-fewparadigm as opposed to the SI-many paradigm.
There are also77practicai advantages for being able to train the models for thedifferent speakers eparately.1.
It is much more efficient o collect the data; there are farfewer people to recruit and train.2.
In SI-few training, we get speaker-dependent models for thetraining speakers for free.3.
When new speakers are added to the training data, we justdevelop the models for the new speakers and average theirmodels in with the model for all of the speakers, withouthaving to retrain on all of the speech from scratch.4.
The computation for the average model method is easy toparallelize across several machines.5.
Perhaps the most compelling argument for SI-few trainingis that having speaker-specific models available for each ofthe training speakers allows us to experiment with speakeradaptation techniques that would not be possible otherwise.Our conclusion is that there is little evidence that having a verylarge number of speakers is significantly better than a relativelysmall number of speakers - if the total amount of Raining iskept the same.
Actually, if we equalize the cost of collectingdata under the SI-few and SI-many conditions, then the SI-fewparadigm would likely yield better ecognition performance thanthe SI-many paradigm.4.2.
Speaker-Dependent vs Speaker-IndependentIt is well-known that, for the same amount of training speech,a system trained on many speakers and tested on new speak-ers (i.e.
speaker-independent recognition) results in significantlyworse performance than when the system is trained on the speakerwho will use it.
However, it is important to know what the trade-off is between the amount of speech and whether the system isspeaker-independent or not, since for many applications, it wouldbe practical to collect a substantial mount of speech from eachuser.Below we compare the recognition error rate between SI andSD recognition.
The SI models were trained with 7,200 sen-tences, while the SD were trained with only 600 sentences, each.Two different sets of test speakers were used for the SI model,while for the SD case, the test and training speakers were thesame, but we compare two different test sets from these samespeakers.
These experiments were performed using the 5K-wordN'VP test sets, using the standard bigram language models andalso rescofing using a trigram language model.Training SI-12 SD-1Test (7200) (600)Dev.
Test 10.9 7.9Nov.
92 Eva1 8.7 8.2Table 4: Speaker-dependent vs Speaker-independent trainingAs can be seen, the word error rate for the SI model is onlysomewhat higher than for the SD model, depending on which SItest set is used.
We estimate that, on the average, if the amount oftraining speech for the SI model were 15-20 times that used forthe SD model, then the average word error rate would be aboutthe same.One might mistakenly conclude from the above results thatif there is a large amount of speaker-independent training avail-able, there is no longer any reason to consider speaker-dependentrecognition.
However, it is extremely important o rememberthat these results only hold for the case where all of the speakersare native speakers of English.
We have previously shown \[10\]that when the test speakers are not native speakers, the error rategoes up by an astonishing factor of eight!
In this case, we mustclearly use either a speaker-dependent or speaker-adaptive modelin order to obtain usable performance.
Of course each speakercan use the type of model that is best for him.4.3.
N-Best ParadigmIn 1989 we developed the N-best Paradigm method for combiningknowledge sources mainly as a way to integrate speech recogni-tion with natural language processing.
Since then, we have foundit to be useful for applying other expensive speech knowledgesources as well, such as cross-word models, tied-mixture densi-ties, and trigram language models.
The basic idea is that we firstfind the top N sentence hypotheses using a less expensive model,such as a bigram grarnmar with discrete densities, and within-word context models.
And then we rescore ach of the resultinghypotheses with the more complex models, and finally we pickthe highest scoring sentence as the answer.One might expect hat there would be a severe problem withthis approach if the latter knowledge sources were much morepowerful than those used in the initial N-best pass.
However, wehave found that this is not the case, as long as the initial errorrate is not too high and the sentences are not too long.In tests on the ATIS corpus (class A+D sentences only), weobtained a 40% reduction in word error rate by rescoring the N-best sentence hypotheses with a trigram language model.
In thistest, we used a value of 100 for N. 'Ibis shows that the tfigramlanguage model is much more powerful than the bigram languagemodel used in finding the N-best sentences.
But there were manyutterances for which the correct answer was not found within theN-best hypotheses.
It was important to determine whether thesystem was being hampered by restricting its consideration tothe N-best sentences before using the trigram language model.Therefore, we artificially added the correct sentence to the N-best list before rescoring with the trigram model.
We found thatthe word error only decreased by another 7%.
We must rememberthat in this experiment, he performance with the correct sentenceadded was an optimistic estimate, since we did not add all ofthe other sentence hypotheses that scored worse than the 100thhypothesis, but better than the correct answer.The question is whether this result would hold up when the vo-cabulary is much larger, thereby increasing the word error rate,and the sentences are much longer, thereby increasing the num-ber of possible permutations of word sequences exponentially.In experiments with the 5K-word WSJ sentences with word er-ror rates around 14% during the initial pass, and with averagesentence lengths around 18 words we still found little loss.However, on the 20K-word development test set, we observeda significant loss for trigram rescoring, but not for other less pow-erful knowledge sources.
The experiment was limited to thosesentences that contained only words that were inside the recogni-tion vocabulary.
(It is impossible to correct errors due to wordsthat are outside of the recognition vocabulary.)
This includedabout 80% of the development test set.
The results are shown78below in Table 5 for the actual N-best list and with the correctutterance artificially inserted into the list.Knowledge ActualUsed N-bestInitial N-best 19.5Cross-word rescoring 16.1Trigram rescoring 13.9With CorrectAnswer Added19.515.610.2Table 5: Effect of N-best Paradigm on 20K-word recognitionwith trigram Language model rescoringWhile this result is a lower bound on the error rate, it indicatesthat much of the potential gain for using the trigram languagemodel is being lost due to the correct answer not being included inthe N-best list.
As a result we are modifying the N-best rescoringto alleviate this problem.5.
MICROPHONE INDEPENDENCEDARPA has placed a high priority on microphone independence.That is, if a new user plugs in any microphone (e.g., a lapelmicrophone or a telephone) without informing the system of thechange, the recognition system is expected to work as well as itdoes with the microphone that was used for training.We considered two different ypes of methods to alleviate thisproblem.
The first attempts to use features that are independent ofthe microphone, while the second attempts to adapt he system orthe input to observed ifferences in the incoming signal in orderto make the speech models match better.5.1.
Cepstrum PreprocessingThe RASTA algorithm \[11\] smoothes the cepstral vector with afive-frame averaging window, and also removes the effect of aslowly varying multiplicative filter, by subtracting an estimateof the average cepstrum.
This average is estimated with an ex-ponential filter with a constant of 0.97, which results in a timeconstant of about one third of a second.
The blind deeonvolutionalgorithm estimates the simple mean of each cepstral value overthe utterance, and then subtracts this mean from the value in eachframe.
In both cases, speech frames are not distinguished fromnoise frames.
The processing is applied to all frames equally.
Inaddition, there was no dependence on estimates of SNR.Every test utterance was recorded simultaneously on the samemicrophone used in the training (a high-quality noise-cancellingSennheiser microphone) and on some other microphone whichwas not known, but which ranged from an omni-directionalboom-mounted microphone or table-mounted microphone, a lapelmicrophone, or a speaker-phone.
We present he error rates forthe baseline and for the two preprooessing methods in Table 6below.Preprocessing Sennheiser Alternate-MicMel cepstra vectors 12.0 37.7IRASTA preprocessing 12.5 27.8Cepstral Mean Removal 11.8 27.2Table 6: Comparison of simple preprocessing algorithms.
Theresults were obtained on the 5K-word VP development test set,using the bigram language model.The results show that the word error rates increase by a factorof three when the microphone is changed radically.
The RASTAalgorithm reduced the degradation toa factor of 2.3, while degrad-ing the performance on the Sennlaeiser microphone just slightly.The blind deconvolufion also reduced the degradation, but did notdegrade the performance on the training microphone.
(In fact, itseemed to improve it very slightly, but not significantly.)
Thisshows that the five-frame averaging used in the RASTA algo-rithm is not necessary for this problem, and that the short-termexponential averaging used to estimate the long-term cepstrummight vary too quickly.5.2.
Known Microphone AdaptationWe decided to attack the problem of accomodating an unknownmicrophone by considering another problem that seemed simplerand more generally useful.
It would be very useful to be ableto adapt a system trained on one microphone so that it workswell on another particular microphone.
The microphone wouldnot have been known at the time the HMM training data wascollected, but it is known before it is to be used.
In this case,we can collect a small sample of stereo data with the microphoneused for training and the new microphone simultaneously.
Thenusing the stereo data we can adapt the system to work well onthe new microphone.For microphone adaptation, we assume we have the VQ indexof the cepstmm of the Sennheiser signal, and the cepstrum ofthe alternate microphone.
Given this stereo data, we accumulatethe mean and variance of the cepstra of the alternate microphoneof the frames whose Sennlaeiser data falls into each of the binsof the VQ codebook.
Now, we can use this to define a new setof Gaussians for data that comes from the new microphone.
Thenew Ganssians have means that are shifted relative to the originalmeans, where the shift can be different for each bin.
In addition,the variances are typically wider for the new microphone, dueto some nondeterminisfie differences between the microphones.Thus the distributions typically overlap more, but only to thedegree that they should.
The new set of means and variancesrepresents a codebook transformation that accomodates the newmicrophone.5.3.
Microphone SelectionIn the problem we were trying to solve the test microphone isnot known, and is not even included in any data that we mighthave seen before.
In this case, how can we estimate a codebooktransformation like the one described above?
One technique is toestimate a transformation for many different types of microphonesand then use one of those transformations.We had available stereo training data from several microphonesthat were not used in the test.
We grouped the alternate micro-phones in the training into six broad categories, such as lapel,telephone, omni-directional, directional microphones, and twospecific desk-mounted microphones.
Then, we estimated a trans-formed codebook for each of the microphones using stereo datafrom that microphone and the Sennheiser, being sure that theadaptation data included both male and female speakers.To select which microphone transformation to use, we triedsimply using each of the transformed codebooks in turn, recog-nizing the utterance with each, and then choosing the answer withthe highest score.
Unfortunately, we found that this method didnot work well, because data that really came from the Sennheiser79microphone was often misclassitied as belonging to another mi-crophone.
We believe this was due to the radically differentnature of the Gaussians for the Sennheiser and the alternate mi-crophones.
The alternate microphone Gaussians overlapped muchmore.Instead we developed a much simpler, less costly method toselect among the microphones.
For each of the seven micro-phone types (Senrtheiser plus six alternate types) we estimateda mixture density consisting of eight Gaussians.
Then, given asentence from an unknown microphone, we computed the prob-ability of the data being produced by each of the seven mixturedensities.
The one with the highest likelihood was chosen, andwe then used the transformed codebook corresponding to the cho-sen microphone type.
We found that on development data thismicrophone selection algorithm was correct about 98% of thetime, and had the desirable property that it never misclassifiedthe Sennheiser data.After developing this algorithm, we found that a similar algo-rithm had been developed at CMU \[12\].
There were four differ-ences between the MFCDCN method and our method.
First, wegrouped the several different microphones into six microphonetypes rather than modeling them each separately.
Second, wemodified the covariances as well as the means of each Gaussian,in order to reflect he increased uncertainty in the codebook trans-formation.
Third, we used an independent microphone classifier,rather than depend on the transformed codebook itself to performmicrophone selection.
And fourth, the CMU algorithm used anSNR-dependent transformation, whereas we used only a singletransformation.
The first difference is probably not important.We believe that the second and third differences favor our al-gorithm, and the fourth difference clearly favors the MFCDCNalgorithm.
Further experimentation will be needed to determinethe best combination of algorithm features.We then compared the performance of the baseline system withblind deconvolution and the microphone adaptation algorithm de-scribed above.
Since these experiments were performed afterimprovements described in Section 1, and the test sets and lan-guage models were different he results in Table 7 are not directlycomparable to those in Table 6 above.Preprocessing Sennheiser Altemate-MicMel cepstra vectors 11.6Cepstral Mean Removal 11.3 32.4Microphone Adaptation 11.3 21.3Table 7: Microphone Adaptation vs Mean Removal.
These ex-periments were performed on the 5K-word btVP development testset using a bigram language model.6.
SUMMARYWe have reported on several methods that result in some reductionin word error rate on the 5K-word WSJ test.
In addition, we havedescribed several experiments that answer questions related totraining scenarios, recognition search strategies, and microphoneindependence.
In particular, we verified that there is no reason tocollect speech from a large number of speakers for estimating aspeaker-independent model Rather, the same results can be ob-tained with less effort by collecting the same amount of speechfrom a smaller number of speakers.
We determined that the N-best rescoring paradigm can degrade somewhat when the errorrate is very high and the sentences are very long.
We showed thata simple blind deconvolution preprocessing of the cepstral fea-tures results in a better microphone independence method than themore complicated RASTA method.
And finally, we introduceda new microphone adaptation algorithm that achieves improvedaccuracy by adapting to one of several codebook transformationsderived from several known microphones.AcknowledgementThis work was supported by the Defense Advanced ResearchProjects Agency and monitored by the Office of Naval Researchunder Contract Nos.
N00014-91-C-0115, and N00014-92-C-0035.REFERENCF~\[1\] Pallett, D., Fiscus, J., Fisher, W., and J. Garofolo, "BenchmarkTests for the Spoken Language Program", DARPA Human LanguageTechnology Workshop, Princeton, NJ, March, 1993.\[2\] Chow, Y., M. Dunham, O Kimball, M. Krasner, G.F. Kubala, J.Makhoul, P. Price, S. Roucos, and R. Schwartz (1987) "BYBLOS:The BBN Continuous Speech Recognition System," IEEEICASSP-87, pp.
8%92\[3\] Chow, Y-L. and R.M.
Schwartz, "The N-Best Algorithm: AnEfficient Procedure for Finding Top N Sentence Hypotheses",ICASSP90, Albuquerque, NM $2.12, pp.
81-84.\[4\] Schwartz, R., S. Austin, Kubala, F., and J. Makhoul, "New Usesfor the N-Best Sentence Hypotheses Within the BYBLOS SpeechRecognition System", ICASSP92, San Francisco, CA, pp.
1.1-1.4.\[5\] Schwartz, R. and S. Austin, "A Comparison Of Several ApproximateAlgorithms for Finding Multiple (N-Bes0 Sentence Hypotheses",ICASSP91, Toronto, Canada, pp.
701-704.\[6\] Austin, S., Schwartz, R., and P. Placeway, "The Forward-BackwardSearch Algorithm", ICASSP91, Toronto, Canada, pp.
697-700.\[7\] Larnel, L., Gauvain, J., "Continuous Speech Recognition at LIMSI",DARPA Neural Net Speech Recognition Workshop, September, 1992.\[8\] Price, P., Fisher, W.M., Bernstein, J., and D.S.
Pallett (1988) "TheDARPA 1000-Word Resource Management Database for Continu-ous Speech Recognition," IEEE Int.
Conf.
Acoust., Speech, SignalProcessing, New York, N'Y', April 1988, pp.
651-654.\[9\] Kubala, F., R. Schwartz, C. Barry, "Speaker Adaptation froma Speaker-Independent Training Corpus", IEEE ICASSP-90, Apr.1990, paper $3.3.\[10\] Kubala, F., R. Schwartz, Makhoul, J., "Dialect Normalizationthrough Speaker Adaptation", 1EEE Workshop on Speech Recog-nition Arden House, Harriman, NY, Dec. 1991.\[11\] Hermansky, H., Morgan, N., Bayya, A., Kohn, P., "Compensa-tion for the Effect of the Communication Channel in Auditory-LikeAnalysis of Speech (R.ASTA-PLP), Proc.
of the Second EuropeanConf.
on Speech Comm.
and Tech.
September, 1991.\[12\] Liu, F-H., Stern, R., Huang, X., Acero, A., "Efficient Cepstral Nor-realization for Robust Speech Recognition", DARPA Human Lan-guage Technology Workshop, Princeton, NJ, March, 1993.\[13\] Placeway, P., Schwartz, R., Fung, P., and L. Nguyen, "The Estima-tion of Powerful Language Models from Small and Large Corpora",To be presented at ICASSP93, Minneapolis, MN.80
