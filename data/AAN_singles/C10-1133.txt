Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1182?1190,Beijing, August 2010Near-synonym Lexical Choice in Latent Semantic SpaceTong WangDepartment of Computer ScienceUniversity of Torontotong@cs.toronto.eduGraeme HirstDepartment of Computer ScienceUniversity of Torontogh@cs.toronto.eduAbstractWe explore the near-synonym lexicalchoice problem using a novel representa-tion of near-synonyms and their contextsin the latent semantic space.
In contrast totraditional latent semantic analysis (LSA),our model is built on the lexical levelof co-occurrence, which has been empir-ically proven to be effective in provid-ing higher dimensional information on thesubtle differences among near-synonyms.By employing supervised learning on thelatent features, our system achieves an ac-curacy of 74.5% in a ?fill-in-the-blank?task.
The improvement over the currentstate-of-the-art is statistically significant.We also formalize the notion of subtletythrough its relation to semantic space di-mensionality.
Using this formalizationand our learning models, several of ourintuitions about subtlety, dimensionality,and context are quantified and empiricallytested.1 IntroductionLexical choice is the process of selecting contentwords in language generation.
Consciously ornot, people encounter the task of lexical choiceon a daily basis ?
when speaking, writing, andperhaps even in inner monologues.
Its applica-tion also extends to various domains of naturallanguage processing, including Natural LanguageGeneration (NLG, Inkpen and Hirst 2006), writ-ers?
assistant systems (Inkpen, 2007), and sec-ond language (L2) teaching and learning (Ouyanget al, 2009).In the context of near-synonymy, the processof lexical choice becomes profoundly more com-plicated.
This is partly because of the subtle nu-ances among near-synonyms, which can arguablydiffer along an infinite number of dimensions.Each dimension of variation carries differences instyle, connotation, or even truth conditions intothe discourse in question (Cruse, 1986), all mak-ing the seemingly intuitive problem of ?choosingthe right word for the right context?
far from triv-ial even for native speakers of a language.
Ina widely-adopted ?fill-in-the-blank?
task, wherethe goal was to guess missing words (from a setof near-synonyms) in English sentences, two hu-man judges achieved an accuracy of about 80%(Inkpen, 2007).
The current state-of-the-art accu-racy for an automated system is 69.9% (Islam andInkpen, 2010).When the goal is to make plausible or evenelegant lexical choices that best suit the con-text, the representation of that context becomes akey issue.
We approach this problem in the la-tent semantic space, where transformed local co-occurrence data is capable of implicitly inducingglobal knowledge (Landauer and Dumais, 1997).A latent semantic space is constructed by reduc-ing the dimensionality of co-occurring linguisticunits ?
typically words and documents as in La-tent Semantic Analysis (LSA).
We refer to thislevel of association (LoA) as document LoA here-after.
Although document LoA can benefit topicallevel classification (e.g., as in document retrieval,Deerwester et al 1990), it is not necessarily suit-able for lexical-level tasks which might require in-formation on a more fine-grained level (Edmondsand Hirst, 2002).
Our experimental results show1182noticeable improvement when the co-occurrencematrix is built on a lexical LoA between wordswithin a given context window.One intuitive explanation for this improvementis that the lexical-level co-occurrence might havehelped recover the high-dimensional subtle nu-ances between near-synonyms.
This conjectureis, however, as imprecise as it is intuitive.
Thenotion of subtlety has mostly been used qualita-tively in the literature to describe the level of dif-ficulty involved in near-synonym lexical choice.Hence, we endeavor to formalize the concept ofsubtlety computationally by using our observa-tions regarding the relationship between ?subtle?concepts and their lexical co-occurrence patterns.We introduce related work on near-synonymy,lexical choice, and latent semantic space modelsin the next section.
Section 3 elaborates on lexicaland contextual representations in latent semanticspace.
In Section 4, we formulate near-synonymlexical choice as a learning problem and report oursystem performance.
Section 5 formalizes the no-tion of subtlety and its relation to dimensionalityand context.
Conclusions and future work are pre-sented in Section 6.2 Related Work2.1 Near-Synonymy and NuancesNear-synonymy is a concept better explained byintuition than by definition ?
which it does notseem to have in the existing literature.
We thusborrow Table 1 from Edmonds and Hirst (2002) toillustrate some basic ideas about near-synonymy.Cruse (1986) compared the notion of plesionymyto cognitive synonymy in terms of mutual entail-ment and semantic traits, which, to the best of ourknowledge, is possibly the closest to a textbookaccount of near-synonymy.There has been a substantial amount of inter-est in characterizing the nuances between near-synonyms for a computation-friendly representa-tion of near-synonymy.
DiMarco et al (1993)discovered 38 dimensions for differentiating near-synonyms from dictionary usage notes and cat-egorized them into semantic and stylistic varia-tions.
Stede (1993) focused on the latter and fur-ther decomposed them into seven scalable sub-Table 1: Examples of near-synonyms and dimen-sion of variations (Edmonds and Hirst, 2002).Types of variation ExamplesContinuous, intermittent seep:dripEmphasis enemy:foeDenotational, indirect error:mistakeDenotational, fuzzy woods:forestStylistic, formality pissed:drunk:inebriatedStylistic, force ruin:annihilateExpressed attitude skinny:thin:slim:slenderEmotive daddy:dad:fatherCollocational task:jobSelectional pass away:dieSub-categorization give:donatecategories.
By organizing near-synonym vari-ations into a tree structure, Inkpen and Hirst(2006) combined stylistic and attitudinal varia-tion into one class parallel to denotational differ-ences.
They also incorporated this knowledge ofnear-synonyms into a knowledge base and demon-strated its application in an NLG system.2.2 Lexical Choice EvaluationDue to their symbolic nature, many of the earlystudies were only able to provide ?demo runs?
inNLG systems rather than any empirical evalua-tion.
The study of near-synonym lexical choicehad remained largely qualitative until a ?fill-in-the-blank?
(FITB) task was introduced by Ed-monds (1997).
The task is based on sentences col-lected from the 1987 Wall Street Journal (WSJ)that contain any of a given set of near-synonyms.Each occurrence of the near-synonyms is removedfrom the sentence to create a ?lexical gap?, and thegoal is to guess which one of the near-synonyms isthe missing word.
Presuming that the 1987 WSJauthors have made high-quality lexical choices,the FITB test provides a fairly objective bench-mark for empirical evaluation for near-synonymlexical choice.
The same idea can be applied tovirtually any corpus to provide a fair amount ofgold-standard data at relatively low cost for lexi-cal choice evaluation.The FITB task has since been frequentlyadopted for evaluating the quality of lexical choicesystems on a standard dataset of seven near-synonym sets (as shown in Table 2).
Edmonds1183(1997) constructed a second-order lexical co-occurrence network on a training corpus (the 1989WSJ).
He measured the word-word distance us-ing t-score inversely weighted by both distanceand order of co-occurrence in the network.
Fora sentence in the test data (generated from the1987 WSJ), the candidate near-synonym minimiz-ing the sum of its distance from all other words inthe sentence (word-context distance) was consid-ered the correct answer.
Average accuracy on thestandard seven near-synonym sets was 55.7%.Inkpen (2007) modeled word-word distanceusing Pointwise Mutual Information (PMI) ap-proximated by word counts from querying theWaterloo Multitext System (Clarke et al, 1998).Word-context distance was the sum of PMI scoresbetween a candidate and its neighboring wordswithin a window-size of 10.
An unsuper-vised model using word-context distance directlyachieved an average accuracy of 66.0%, while asupervised method with lexical features added tothe word-context distance further increased theaccuracy to 69.2%.Islam and Inkpen (2010) developed a systemwhich completed a test sentence with possiblecandidates one at a time.
The candidate gener-ating the most probable sentence (measured bya 5-gram language model) was proposed as thecorrect answer.
N-gram counts were collectedfrom Google Web1T Corpus and smoothed withmissing counts, yielding an average accuracy of69.9%.2.3 Lexical Choice Outside theNear-synonymy DomainThe problem of lexical choice also comes in manyflavors outside the near-synonymy domain.
Reiterand Sripada (2002) attributed the variation in lexi-cal choice to cognitive and vocabulary differencesamong individuals.
In their meteorology domaindata, for example, the term by evening was inter-preted as before 00:00 by some forecasters butbefore 18:00 by others.
They claimed that NLGsystems might have to include redundancy in theiroutput to tolerate cognitive differences among in-dividuals.2.4 Latent Semantic Space Models and LoALSA has been widely applied in various fieldssince its introduction by Landauer and Dumais(1997).
In their study, LSA was conducted ondocument LoA on encyclopedic articles and thelatent space vectors were used for solving TOEFLsynonym questions.
Rapp (2008) used LSAon lexical LoA for the same task and achieved92.50% in accuracy in contrast to 64.38% givenby Landauer and Dumais (1997).
This work con-firmed our early postulation that document LoAmight not be tailored for lexical level tasks, whichmight require lower LoAs for more fine-grainedco-occurrence knowledge.
Note, however, thatconfounding factors might also have led to the dif-ference in performance, since the two studies useddifferent weighting schemes and different corporafor the co-occurrence model1.
In Section 3.2 wewill compare models on the two LoAs in a morecontrolled setting to show their difference in thelexical choice task.3 Representing Words and Contexts inLatent Semantic SpaceWe first formalize the FITB task to facili-tate later discussions.
A test sentence t ={w1, .
.
.
,w j?1,si,w j+1, .
.
.
,wm} contains a near-synonym si which belongs to a set of synonymsS = {s1, .
.
.
,sn},1 ?
i ?
n. A FITB test case iscreated by removing si from t, and the context (theincomplete sentence) c = t?
{si} is presented tosubjects with a set of possible choices S to guesswhich of the near-synonyms in S is the missingword.3.1 Constructing the Latent SpaceRepresentationThe first step in LSA is to build a co-occurrencematrix M between words and documents, which isfurther decomposed by Singular Value Decompo-sition (SVD) according to the following equation:Mv?d = Uv?k?k?kV Tk?d1The former used Groliers Academic American Encyclo-pedia with weights divided by word entropy, while the latterused the British National Corpus with weights multiplied byword entropy.1184Here, subscripts denote matrix dimensions, U , ?,and V together create a decomposition of M, v andd are the number of word types and documents,respectively, and k is the number of dimensionsfor the latent semantic space.
A word w is repre-sented by the row in U corresponding to the rowfor w in M. For a context c, we construct a vector cof length v with zeros and ones, each correspond-ing to the presence or absence of a word wi withrespect to c, i.e.,ci ={ 1 if wi ?
c0 otherwiseWe then take this lexical space vector cv?1 as apseudo-document and transform it into a latent se-mantic space vector c?:c?
= ?
?1UT c (1)An important observation is that this represen-tation is equivalent to a weighted centroid of thecontext word vectors: when c is multiplied by?
?1UT in Equation (1), the product is essentiallya weighted sum of the rows in U corresponding tothe context words.
Consequently, simple modifi-cations on the weighting can yield other interest-ing representations of context.
Consider, for ex-ample, the weighting vector wk?1 = (?1, ?
?
?
,?k)Twith?i = 1|2(pgap?
i)?1|where pgap is the position of the ?gap?
in the testsentence.
Multiplying w before ?
?1 in Equation(1) is equivalent to giving the centroid gradient-decaying weights with respect to the distance be-tween a context word and the near-synonym.
Thisis a form of a Hyperspace Analogue to Language(HAL) model, which is sensitive to word order, incontrast to a bag-of-words model.3.2 Dimensionality and Level of AssociationThe number of dimensions k is an importantchoice to make in latent semantic space mod-els.
Due to the lack of any principled guidelinefor doing otherwise, we conducted a brute forcegrid search for a proper k value for each LoA, onthe basis of the performance of the unsupervisedmodel (Section 4.1 below).Figure 1: FITB Performance on different LoAs asa function of the latent space dimensionality.In Figure 1, performance on FITB using thisunsupervised model is plotted against k for doc-ument and lexical LoAs.
Document LoA is verylimited in the available number of dimensions2;higher dimensional knowledge is simply unavail-able from this level of co-occurrence.
In contrast,lexical LoA stands out around k = 550 and peaksaround k = 700.
Although the advantage of lexi-cal LoA in the unsupervised setting is not signif-icant, later we show that lexical LoA nonethelessmakes higher-dimensional information availablefor other learning methods.Note that the scale on the y-axis is stretched tomagnify the trends.
On a zero-to-one scale, theperformance of these unsupervised methods is al-most indistinguishable, indicating that the unsu-pervised model is not capable of using the high-dimensional information made available by lexi-cal LoA.
We will elaborate on this point in Section5.2.2The dimensions for document and lexical LoAs on ourdevelopment corpus are 55,938?500 and 55,938?55,938,respectively.
The difference is measured between v?
d andv?
v (Section 3.1).11854 Learning in the Latent Semantic Space4.1 Unsupervised Vector Space ModelWhen measuring distance between vectors, LSAusually adopts regular vector space model dis-tance functions such as cosine similarity.
With thecontext being a centroid of words (Section 3.1),the FITB task then becomes a k-nearest neighborproblem in the latent space with k = 1 to choosethe best near-synonym for the context:s?
= argmaxsicos(UrowId(v(si),M), c?
)where v(si) is the corresponding row for near-synonym si in M, and rowId(v,M) gives the rownumber of a vector v in a matrix M containing vas a row.In a model with a cosine similarity distancefunction, it is detrimental to use ?
?1 to weight thecontext centroid c?.
This is because elements in ?are the singular values of the co-occurrence matrixalong its diagonal, and the amplitude of a singularvalue (intuitively) corresponds to the significanceof a dimension in the latent space; when the in-verted matrix is used to weight the centroid, it will?misrepresent?
the context by giving more weightto less-significantly co-occurring dimensions andthus sabotage performance.
We thus use ?
insteadof ?
?1 in our experiments.
As shown in Figure1, the best unsupervised performance on the stan-dard FITB dataset is 49.6%, achieved on lexicalLoA at k = 800.4.2 Supervised Learning on the LatentSemantic Space FeaturesIn traditional latent space models, the latent spacevectors have almost invariantly been used in theunsupervised setting discussed above.
Althoughthe number of dimensions has been reduced in thelatent semantic space, the inter-relations betweenthe high-dimension data points may still be com-plex and non-linear; such problems lend them-selves naturally to supervised learning.We therefore formulate the near-synonym lex-ical choice problem as a supervised classificationproblem with latent semantic space features.
Fora test sentence in the FITB task, for example, thecontext is represented as a latent semantic spacevector as discussed in Section 3.1, which is thenpaired with the correct answer (the near-synonymremoved from the sentence) to form one trainingcase.We choose Support Vector Machines (SVMs) asour learning algorithm for their widely acclaimedclassification performance on many tasks as wellas their noticeably better performance on the lex-ical choice task in our pilot study.
Table 2 liststhe supervised model performance on the FITBtask together with results reported by other relatedstudies.
The model is trained on the 1989 WSJand tested on the 1987 WSJ to ensure maximalcomparability with other results.
The optimal kvalue is 415.
Context window size3 around thegap in a test sentence also affects the model per-formance.
In addition to using the words in theoriginal sentence, we also experiment with enlarg-ing the context window to neighboring sentencesand shrinking it to a window frame of n wordson each side of the gap.
Interestingly, when mak-ing the lexical choice, the model tends to favormore-local information ?
a window frame of size5 gives the best accuracy of 74.5% on the test.Based on binomial exact test4 with a 95% confi-dence interval, our result outperforms the currentstate-of-the-art with statistical significance.5 Formalizing Subtlety in the LatentSemantic SpaceIn this section, we formalize the notion of sub-tlety through its relation to dimensionality, anduse the formalization to provide empirical supportfor some of the common intuitions about subtletyand its complexity with respect to dimensionalityand size of context.5.1 Characterizing Subtlety UsingCollocating Differentiator of SubtletyIn language generation, subtlety can be viewed asa subordinate semantic trait in a linguistic realiza-3Note that the context window in this paragraph is im-plemented on FITB test cases, which is different from thecontext size we compare in Section 5.3 for building co-occurrence matrix.4The binomial nature of the outcome of an FITB test case(right or wrong) makes binomial exact test a more suitablesignificance test than the t-test used by Inkpen (2007).1186Table 2: Supervised performance on the seven standard near-synonym sets in the FITB task.
95%Confidence based on Binomial Exact Test.Near-synonymsCo-occur.
SVMs 5-gram SVMs onnetwork & PMI language model latent vectors(Edmonds, 1997) (Inkpen, 2007) (Islam and Inkpen, 2010) (Section 4.2)difficult, hard, tough 47.9% 57.3% 63.2% 61.7%error, mistake, oversight 48.9% 70.8% 78.7% 82.5%job, task, duty 68.9% 86.7% 78.2% 82.4%responsibility, burden, 45.3% 66.7% 72.2% 63.5%obligation, commitmentmaterial, stuff, substance 64.6% 71.0% 70.4% 78.5%give, provide, offer 48.6% 56.1% 55.8% 75.4%settle, resolve 65.9% 75.8% 70.8% 77.9%Average 55.7% 69.2% 69.9% 74.5%Data size 29,835 31,116 31,116 30,30095% confidence 55.1?56.3% 68.7?69.7% 69.3?70.4% 74.0?75.0%tion of an intention5.
A key observation regard-ing subtlety is that it is non-trivial to characterizesubtle differences between two linguistic units bytheir collocating linguistic units.
More interest-ingly, the difficulty in such characterization canbe approximated by the difficulty in finding a thirdlinguistic unit satisfying the following constraints:1.
The unit must collocate closely with at leastone of the two linguistic units under differ-entiation;2.
The unit must be characteristic of the differ-ence between the pair.Such approximation is meaningful in that it trans-forms the abstract characterization into a concretetask of finding this third linguistic unit.
For ex-ample, suppose we want to find out whether thedifference between glass and mug is subtle.
Theapproximation boils the answer down to the dif-ficulty of finding a third word satisfying the twoconstraints, and we may immediately concludethat the difference between the pair is not subtlesince it is relatively easy to find wine as the quali-fying third word, which 1) collocates closely withglass and 2) characterizes the difference between5The same principle applies when we replace ?genera-tion?
with ?understanding?
and ?an intention?
with ?a cogni-tion?.the pair by instantiating one of their major differ-ences ?
the purpose of use.
The same reasoningapplies to concluding non-subtlety for word pairssuch as pen and pencil with sharpener, weatherand climate with forecast, watch and clock withwrist, etc.In contrast, for the pair forest and woods, itmight be easy to find words satisfying one but notboth constraints.
Consequently, the lack of suchqualifying words ?
or at least the relative diffi-culty for finding one ?
makes the difference be-tween this pair more subtle than in the previousexamples.We call a linguistic unit satisfying both con-straints a collocating differentiator of subtlety(CDS).
Notably, the second constraint puts an im-portant difference between CDSs and the conven-tional sense of collocation.
On the lexical level,CDSs are not merely words that collocate morewith one word in a pair than with the other; theyhave to be characteristic of the differences be-tween the pair.
In the example of forest andwoods, one can easily find a word exclusively col-locating with one but not the other ?
such as na-tional forest but not *national woods; however,unlike the CDSs in the previous examples, theword national does not characterize any of the dif-ferences between the pair in size, primitiveness,1187proximity to civilization, or wildness (Edmondsand Hirst, 2002), and consequently fails to satisfythe second constraint.5.2 Relating Subtlety to Latent SpaceDimensionality6As mentioned in Section 4.1, elements of a latentspace vector are in descending order in terms ofco-occurrence significance, i.e., the informationwithin the first few dimensions is obtained frommore closely collocating linguistic units.
Fromthe two constraints in the previous section, it fol-lows that it should be relatively easier to find aCDS for words that can be well distinguished in alower-dimensional sub-space of the latent seman-tic space, and the difference among such wordsshould not be considered subtle.We thus claim that co-occurrence-based infor-mation capable of characterizing subtle differ-ences must then reside in higher dimensions inthe latent space vectors.
Furthermore, our intu-ition on the complexity of subtlety can also beempirically tested by comparing the performanceof supervised and unsupervised models at differ-ent k values.
One of the differences between thetwo types of models is that supervised models arebetter at unraveling the convoluted inter-relationsbetween high-dimensional data points.
Under thisassumption, if we hypothesize that subtlety is acertain form of complex, high-dimensional rela-tion between semantic elements, then the differ-ence in performance between the supervised andunsupervised model should increase as the formerrecovers subtle information in higher dimensions.As shown in Figure 2, performance of bothmodels is positively correlated to the number ofdimensions in the latent semantic space (with cor-relation coefficient ?
= 0.95 for supervised modeland ?
= 0.81 for unsupervised model).
This sug-gests that the lexical choice process is indeed?picking up?
implicit information about subtletyin the higher dimensions of the latent vectors.Meanwhile, the difference between the perfor-mance of the two models correlates strongly to kwith ?
= 0.95.
Significance tests on the ?differ-6In order to keep the test data (1987 WSJ) unseen beforeproducing the results in Table 2, models in this section weretrained on The Brown Corpus and tested on 1988?89 WSJ.Figure 2: Supervised performance increasing fur-ther from unsupervised performance in higher di-mensions.ence of difference?7 between their performancesfurther reveal increasing difference in growth rateof their performance.
Significance is witnessed inboth the F-test and the paired t-test,8 indicatingthat the subtlety-related information in the higherdimensions exhibits complex clustering patternsthat are better recognized by SVMs but beyondthe capability of the KNN model.5.3 Subtlety and the Level of ContextOur previous models on lexical LoA associatedwords within the same sentence to build the co-occurrence matrix.
Lexical LoA also allows usto associate words that co-occur in different lev-els of context (LoC) such as paragraphs or docu-ments.
This gives an approximate measurementof how much context a lexical LoA model usesfor word co-occurrence.
Intuitively, by looking atmore context, higher LoC models should be betterat differentiating more subtle differences.We compare the performance of models withdifferent LoCs in Figure 3.
The sentence LoCmodel constantly out-performs the paragraph LoCmodel after k = 500, indicating that, by inter-model comparison, larger LoC models do notnecessarily perform better on higher dimensions.However, there is a noticeable difference in theoptimal dimensionality for the model perfor-mances.
Sentence LoC performance peaks around7The italicized difference is used in its mathematicalsense as the discrete counterpart of derivative.8F-test: f (1,16) = 9.13, p < 0.01.
Paired t-test: t(8) =4.16 with two-tailed p = 0.0031.
Both conducted on 10 datapoints at k = 50 to 500 with a step of 50.1188Figure 3: LoC in correlation to latent space di-mensionality for optimal model performance.k = 700 ?
much lower than that of paragraphLoC which is around k = 1,100.
Such differ-ence may suggest that, by intra-model compari-son, each model may have its own ?comfort zone?for the degree of subtlety it differentiates; modelson larger LoC are better at differentiating betweenmore subtle nuances, which is in accordance withour intuition.One possible explanation for sentence LoCmodels outperforming paragraph LoC models isthat, although the high-dimensional elements areweighed down by ?
due to their insignificance inthe latent space, their contribution to the outputof distance function is larger in paragraph LoCmodels because the vectors are much denser thanthat in the sentence LoC model; since the unsuper-vised method is incapable of recognizing the clus-tering patterns well in high-dimensional space,the ?amplified?
subtlety information is eventuallytaken as noise by the KNN model.
An interestingextension to this discussion is to see whether a su-pervised model can consistently perform better onhigher LoC in all dimensions.6 Conclusions and Future WorkWe propose a latent semantic space representa-tion of near-synonyms and their contexts, whichallows a thorough investigation of several aspectsof the near-synonym lexical choice problem.
Byemploying supervised learning on the latent spacefeatures, we achieve an accuracy of 74.5% on the?fill-in-the-blank?
task, outperforming the currentstate-of-the-art with statistical significance.In addition, we formalize the notion of subtletyby relating it to the dimensionality of the latent se-mantic space.
Our empirical analysis suggests thatsubtle differences between near-synonyms residein higher dimensions in the latent semantic spacein complex clustering patterns, and that the degreeof subtlety correlates to the level of context for co-occurrence.
Both conclusions are consistent withour intuition.As future work, we will make better use of theeasy customization of the context representationto compare HAL and other models with bag-of-words models.
The correlation between subtletyand dimensionality may lead to many interestingtasks, such as measuring the degree of subtlety forindividual near-synonyms or near-synonym sets.With regard to context representation, it is alsointriguing to explore other dimensionality reduc-tion methods (such as Locality Sensitive Hashingor Random Indexing) and to compare them to theSVD-based model.AcknowledgmentThis study is supported by the Natural Sciencesand Engineering Research Council of Canada(NSERC).
We greatly appreciate the valuable sug-gestions and feedback from all of our anonymousreviewers and from our colleagues Julian Brooke,Frank Rudzicz, and George Dahl.1189ReferencesCharles L. A. Clarke, Gordon Cormack, andChristopher Palmer.
An overview of MultiText.ACM SIGIR Forum, 32(2):14?15, 1998.D.
A. Cruse.
Lexical Semantics.
Cambridge Uni-versity Press, 1986.Scott Deerwester, Susan Dumais, George Furnas,Thomas Landauer, and Richard Harshman.
In-dexing by latent semantic analysis.
Journal ofthe American Society for Information Science,41(6):391?407, 1990.Chrysanne DiMarco, Graeme Hirst, and ManfredStede.
The semantic and stylistic differentiationof synonyms and near-synonyms.
AAAI SpringSymposium on Building Lexicons for MachineTranslation, pages 114?121, 1993.Philip Edmonds.
Choosing the word most typi-cal in context using a lexical co-occurrence net-work.
In Proceedings of the 35th annual meet-ing of the Association for Computational Lin-guistics and Eighth Conference of the EuropeanChapter of the Association for ComputationalLinguistics, pages 507?509, 1997.Philip Edmonds and Graeme Hirst.
Near-synonymy and lexical choice.
ComputationalLinguistics, 28(2):105?144, 2002.Diana Inkpen.
A statistical model for near-synonym choice.
ACM Transactions on Speechand Language Processing, 4(1):1?17, 2007.Diana Inkpen and Graeme Hirst.
Building and us-ing a lexical knowledge-base of near-synonymdifferences.
Computational Linguistics, 32(2):223?262, 2006.Aminul Islam and Diana Inkpen.
Near-synonymchoice using a 5-gram language model.
Re-search in Computing Sciences, 46:41?52, 2010.Thomas Landauer and Susan Dumais.
A solutionto Plato?s problem: the latent semantic analy-sis theory of acquisition, induction, and repre-sentation of knowledge.
Psychological Review,104(2):211?240, 1997.Shixiao Ouyang, Helena Hong Gao, andSoo Ngee Koh.
Developing a computer-facilitated tool for acquiring near-synonymsin Chinese and English.
In Proceedingsof the Eighth International Conference onComputational Semantics, pages 316?319,2009.Reinhard Rapp.
The automatic generation of the-sauri of related words for English, French, Ger-man, and Russian.
International Journal ofSpeech Technology, 11(3):147?156, 2008.Ehud Reiter and Somayajulu Sripada.
Humanvariation and lexical choice.
ComputationalLinguistics, 28(4):545?553, 2002.Manfred Stede.
Lexical choice criteria in lan-guage generation.
In Proceedings of the sixthconference of the European Chapter of the As-sociation for Computational Linguistics, pages454?459, 1993.1190
