Proceedings of NAACL HLT 2007, pages 33?40,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAutomatic Evaluation of Machine Translation Based on Rate ofAccomplishment of Sub-goalsKiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi IsaharaNational Institute of Information and Communications Technology3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan{uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jpAbstractThe quality of a sentence translated by amachine translation (MT) system is dif-ficult to evaluate.
We propose a methodfor automatically evaluating the qualityof each translation.
In general, whentranslating a given sentence, one or moreconditions should be satisfied to maintaina high translation quality.
In English-Japanese translation, for example, prepo-sitions and infinitives must be appropri-ately translated.
We show several proce-dures that enable evaluating the quality ofa translated sentence more appropriatelythan using conventional methods.
Thefirst procedure is constructing a test setwhere the conditions are assigned to eachtest-set sentence in the form of yes/noquestions.
The second procedure is devel-oping a system that determines an answerto each question.
The third procedure iscombining a measure based on the ques-tions and conventional measures.
We alsopresent a method for automatically gener-ating sub-goals in the form of yes/no ques-tions and estimating the rate of accom-plishment of the sub-goals.
Promising re-sults are shown.1 IntroductionIn machine translation (MT) research, appropriatelyevaluating the quality of MT results is an importantissue.
In recent years, many researchers have triedto automatically evaluate the quality of MT and im-prove the performance of automatic MT evaluations(Niessen et al, 2000; Akiba et al, 2001; Papineni etal., 2002; NIST, 2002; Leusch et al, 2003; Turian etal., 2003; Babych and Hartley, 2004; Lin and Och,2004; Banerjee and Lavie, 2005; Gimen?ez et al,2005) because improving the performance of auto-matic MT evaluation is expected to enable us to useand improve MT systems efficiently.
For example,Och reported that the quality of MT results was im-proved by using automatic MT evaluation measuresfor the parameter tuning of an MT system (Och,2003).
This report shows that the quality of MT re-sults improves as the performance of automatic MTevaluation improves.MT systems can be ranked if a set of MT re-sults for each system and their reference translationsare given.
Usually, about 300 or more sentencesare used to automatically rank MT systems (Koehn,2004).
However, the quality of a sentence translatedby an MT system is difficult to evaluate.
For exam-ple, the results of five MTs into Japanese of the sen-tence ?The percentage of stomach cancer among theworkers appears to be the highest for any asbestosworkers.?
are shown in Table 1.
A conventional au-tomatic evaluation method ranks the fifth MT resultfirst although its human subjective evaluation is thelowest.
This is because conventional methods arebased on the similarity between a translated sentenceand its reference translation, and they give the trans-lated sentence a high score when the two sentencesare globally similar to each other in terms of lexicaloverlap.
However, in the case of the above example,33Table 1: Examples of conventional automatic evaluations.Original sentence The percentage of stomach cancer among the workers appears to be the highest for any asbestos work-ers.Reference translation(in Japanese)roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda .System MT results BLEU NIST Fluency Adequacy1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwataroudousha no tame ni demo mottomo ookii youdearu .0.2111 2.1328 2 32 roudousha no aida no igan no paasenteeji wa, arayuru asubesutoroudousha no tame ni mottomo takai youni omowa re masu .0.2572 2.1234 2 33 roudousha no aida no igan no paasenteeji wa donna asubesuto no tameni mo mottomo takai youni mie masu0 1.8094 1 24 roudousha no aida no igan no paasenteeji wa nin?ino ishiwata ni wamottomo takaku mie masu .0 1.5902 1 25 roudousha no naka no igan no wariai wa donna asubesuto ni mo mot-tomo takai youni mieru .0.2692 2.2640 1 2the most important thing to maintain a high trans-lation quality is to correctly translate ?for?
into thetarget language, and it would be difficult to detectthe importance just by comparing an MT result andits reference translations even if the number of ref-erence translations is increased.In general, when translating a given sentence, oneor more conditions should be satisfied to maintain ahigh translation quality.
In this paper, we show thatconstructing a test set where the conditions that aremainly established from a linguistic point of vieware assigned to each test-set sentence in the formof yes/no questions, developing a system that de-termines an answer to each question, and combin-ing a measure based on the questions and conven-tional measures enable the evaluation of the qualityof a translated sentence more appropriately than us-ing conventional methods.
We also present a methodfor automatically generating sub-goals in the form ofyes/no questions and estimating the rate of accom-plishment of the sub-goals.2 Test Set for Evaluating MachineTranslation Quality2.1 Test SetTwo main types of data are used for evaluating MTquality.
One type of data is constructed by arbi-trarily collecting sentence pairs in the source- andtarget-languages, and the other is constructed by in-tensively collecting sentence pairs that include lin-guistic phenomena that are difficult to automaticallytranslate.
Recently, MT evaluation campaigns suchas the International Workshop on Spoken LanguageTranslation 1, NISTMachine Translation Evaluation2, and HTRDP Evaluation 3 were organized to sup-port the improvement of MT techniques.
The dataused in the evaluation campaigns were arbitrarilycollected from newspaper articles or travel conver-sation data for fair evaluation.
They are classifiedas the former type of data mentioned above.
On theother hand, the data provided by NTT (Ikehara et al,1994) and that constructed by JEIDA (Isahara, 1995)are classified as the latter type.
Almost all the datamentioned above consist of only parallel translationsin two languages.
Data with information for evaluat-ing MT results, such as JEIDA?s are rarely found.
Inthis paper, we call data that consist of parallel trans-lations collected for MT evaluation and that the in-formation for MT evaluation is assigned to, a testset.The most characteristic information assigned tothe JEIDA test set is the yes/no question for assess-ing the translation results.
For example, a yes/noquestion such as ?Is ?for?
translated into an expres-sion representing a cause/reason such as ?de???
(inJapanese) is assigned to a test-set sentence.
We canevaluate MT results objectively by answering thequestion.
An example of a test-set sample consist-ing of an ID, a source-language sample sentence, itsreference translation, and a question is as follows.1http://www.slt.atr.jp/IWSLT2006/2http://www.nist.gov/speech/tests/mt/index.htm3http://www.863data.org.cn/34ID 1.1.7.1.3-1Sample sen-tenceThe percentage of stomach can-cer among the workers appearsto be the highest for any asbestosworkers.Referencetranslation(in Japanese)roudousha no igan no wariai wa, asubesuto roudousha no tameni saikou to naru youda .Question Is ?appear to?
translated into anauxiliary verb such as ?youda?
?The questions are classified mainly in terms ofgrammar, and the numbers to the left of the hyphen-ation of each ID such as 1.1.7.1.3 represent the cat-egories of the questions.
For example, the abovequestion is related to catenative verbs.The JEIDA test set consists of two parts, one forthe evaluation of English-Japanese MT and the otherfor that of Japanese-English MT.
We focused on thepart for English-Japanese MT.
This part consists of769 sample sentences, each of which has a yes/noquestion.The 769 sentences were translated by using fivecommercial MT systems to investigate the relation-ship between subjective evaluation based on yes/noquestions and conventional subjective evaluationbased on fluency and adequacy.
The instruction forthe subjective evaluation based on fluency and ad-equacy followed that given in the TIDES specifi-cation (TIDES, 2002).
The subjective evaluationbased on yes/no questions was done by manuallyanswering each question for each translation.
Thesubjective evaluation based on the yes/no questionswas stable; namely, it was almost independent ofthe human subjects in our preliminary investigation.There were only two questions for which the an-swers generated inconsistency in the subjective eval-uation when 1,500 question-answer pairs were ran-domly sampled and evaluated by two human sub-jects.Then, we investigated the correlation between thetwo types of subjective evaluation.
The correlationcoefficients mentioned in this paper are statisticallysignificant at the 1% or less significance level.
TheSpearman rank-order correlation coefficient is usedin this paper.
In the subjective evaluation based onyes/no questions, yes and no were numerically trans-formed into 1 and ?1.
For 3,845 translations ob-tained by using five MT systems, the correlation co-efficients between the subjective evaluations basedon yes/no questions and based on fluency and ade-quacy were 0.48 for fluency and 0.63 for adequacy.These results indicate that the two subjective evalu-ations have relatively strong correlations.
The cor-relation is especially strong between the subjectiveevaluation based on yes/no questions and adequacy.2.2 Expansion of JEIDA Test SetEach sample sentence in the JEIDA test set has onlyone question.
Therefore, in the subjective evalua-tion using the JEIDA test set, translation errors thatdo not involve the pre-assigned question are ignoredeven if they are serious.
Therefore, translations thathave serious errors that are not related to the ques-tion tend to be evaluated as being of high quality.To solve this problem, we expanded the test set byadding new questions about translations with the se-rious errors.Sentences whose average grades were three orless for fluency and adequacy for the translation re-sults of the five MT systems were selected for theexpansion.
Besides them, sentences whose averagegrades were more than three for fluency and ade-quacy for the translation results of the five MT sys-tems were selected when a majority of evaluationresults based on yes/no questions about the transla-tions of the five MT systems were no.
The numberof selected sentences was 150.
The expansion wasmanually performed using the following steps.1.
Serious translation errors are extracted from theMT results.2.
For each extracted error, questions strongly re-lated to the error are searched for in the test set.If related questions are found, the same typesof questions are generated for the selected sen-tence, and the same ID as that of the relatedquestion is assigned to each generated question.Otherwise, questions are newly generated, anda new ID is assigned to each generated ques-tion.3.
Each MT result is evaluated according to eachadded question.Eventually, one or more questions were assigned toeach selected sentence in the test set.
Among the 15035Table 2: Expanded test-set samples.ID 1.1.7.1.3-1Original Sample sentence The percentage of stomach cancer among the workers appears to be the highest for anyasbestos workers.Reference translation(in Japanese)roudousha no igan no wariai wa , asubesuto roudousha no tame ni saikou to naru youda.Question (Q-0) Is ?appear to?
translated into an auxiliary verb such as ?youda?
?ID 1.1.6.1.3-5Expanded Translation error ?For?
is not translated appropriately.Question-1 (Q-1) Is ?for?
translated into an expression representing a cause/reason such as ?.
.
.de?
?ID Additional-1Expanded Translation error Some expressions are not translated.Question-2 (Q-2) Are all English words translated into Japanese?Table 3: Examples of subjective evaluations based on yes/no questions.AnswerSystem MT results Q-0 Q-1 Q-2 Fluency Adequacy1 roudousha no aida no igan no paasenteeji wa , donoyouna ishiwataroudousha no tame ni demo mottomo ookii youdearu .Yes No Yes 2 32 roudousha no aida no igan no paasenteeji wa, arayuru asubesutoroudousha no tame ni mottomo takai youni omowa re masu .Yes Yes Yes 2 33 roudousha no aida no igan no paasenteeji wa donna asubesuto notame ni mo mottomo takai youni mie masuYes No No 1 24 roudousha no aida no igan no paasenteeji wa nin?ino ishiwata niwa mottomo takaku mie masu .Yes No No 1 25 roudousha no naka no igan no wariai wa donna asubesuto ni momottomo takai youni mieru .Yes No No 1 2selected sentences, questions were newly assignedto 103 sentences.
The number of added questionswas 148.
The maximum number of questions addedto a sentence was five.
After expanding the test set,the correlation coefficients between the subjectiveevaluations based on yes/no questions and based onfluency and adequacy increased from 0.48 to 0.51for fluency and from 0.63 to 0.66 for adequacy.
Thedifferences between the correlation coefficients ob-tained before and after the expansion are statisticallysignificant at the 5% or less significance level foradequacy.
These results indicate that the expansionof the test set significantly improves the correlationbetween the subjective evaluations based on yes/noquestions and based on adequacy.
When two ormore questions were assigned to a test-set sentence,the subjective evaluation based on the questions wasdecided by the majority answer.
The majority an-swers, yes and no, were numerically transformedinto 1 and ?1.
Ties between yes and no were trans-formed into 0.
Examples of added questions andthe subjective evaluations based on the questions areshown in Tables 2 and 3.3 Automatic Evaluation of MachineTranslation Based on Rate ofAccomplishment of Sub-goals3.1 A New Measure for Evaluating MachineTranslation QualityThe JEIDA test set was not designed for auto-matic evaluation but for human subjective evalua-tion.
However, a measure for automatic MT evalu-ation that strongly correlates fluency and adequacyis likely to be established because the subjectiveevaluation based on yes/no questions has a rela-tively strong correlation with the subjective evalua-tion based on fluency and adequacy, as mentioned inSection 2.
In this section, we describe a method forautomatically evaluating MT quality by predictingan answer to each yes/no question and using thoseanswers.Hereafter, we assume that each yes/no question isdefined as a sub-goal that a given translation shouldsatisfy and that the sub-goal is accomplished if theanswer to the corresponding yes/no question to thesub-goal is yes.
We also assume that the sub-goalis unaccomplished if the answer is no.
A new eval-uation score, A, is defined based on a multiple lin-36Table 4: Examples of Patterns.Sample sentence She lived there by herself.Question Is ?by herself?
translated as ?hitori de?
?Pattern The answer is yes if the pattern [hitori dake de|hitori kiri de |tandoku de|tanshin de] is included in atranslation.
Otherwise, the answer is no.Sample sentence They speak English in New Zealand.Question The personal pronoun ?they?
is omitted in a translation like ?nyuujiilando de wa eigo wo hanasu?
?Pattern The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation.
Otherwise, theanswer is no.ear regression model as follows using the rate of ac-complishment of the sub-goals and the similaritiesbetween a given translation and its reference trans-lation.
The best-fitted line for the observed data iscalculated by the method of least-squares (Draperand Smith, 1981).A =m?i=1?Si?
Si(1)+n?j=1(?Qj?
Qj+ ?Q?j?
Q?j) + ?Qj={1 : if subgoal is accomplished0 : otherwise (2)Q?j={1 : if subgoal is unaccomplished0 : otherwise (3)Here, the term Qjcorresponds to the rate of accom-plishment of the sub-goal having the i-th ID, and?Qjis a weight for the rate of accomplishment.
Theterm Q?jcorresponds to the rate of unaccomplish-ment of the sub-goal having the i-th ID, and ?Q?jis aweight for the rate of unaccomplishment.
The valuen indicates the number of types of sub-goals.
Theterm ?is constant.The term Siindicates a similarity between a trans-lated sentence and its reference translation, and ?Siis a weight for the similarity.
Many methods for cal-culating the similarity have been proposed (Niessenet al, 2000; Akiba et al, 2001; Papineni et al, 2002;NIST, 2002; Leusch et al, 2003; Turian et al, 2003;Babych and Hartley, 2004; Lin and Och, 2004;Banerjee and Lavie, 2005; Gimen?ez et al, 2005).In our research, 23 scores, namely BLEU (Papineniet al, 2002) with maximum n-gram lengths of 1, 2,3, and 4, NIST (NIST, 2002) with maximum n-gramlengths of 1, 2, 3, 4, and 5, GTM (Turian et al, 2003)with exponents of 1.0, 2.0, and 3.0, METEOR (ex-act) (Banerjee and Lavie, 2005), WER (Niessen etal., 2000), PER (Leusch et al, 2003), and ROUGE(Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and4 variants (LCS, S?, SU?, W-1.2), were used to cal-culate each similarity Si.
Therefore, the value of min Eq.
(1) was 23.
Japanese word segmentation wasperformed by using JUMAN 4 in our experiments.As you can see, the definition of our new measureis based on a combination of an evaluation measurefocusing on local information and that focusing onglobal information.3.2 Automatic Estimation of Rate ofAccomplishment of Sub-goalsThe rate of accomplishment of sub-goals is esti-mated by determining the answer to each questionas yes or no.
This section describes a method basedon simple patterns for determining the answers.An answer to each question is automatically de-termined by checking whether patterns are includedin a translation or not.
The patterns are constructedfor each question.
All of the patterns are expressedin hiragana characters.
Before applying the pat-terns to a given translation, the translation is trans-formed into hiragana characters, and all punctuationis eliminated.
The transformation to hiragana char-acters was performed by using JUMAN in our ex-periments.Test-set sentences, the questions assigned tothem, and the patterns constructed for the questionsare shown in Table 4.
In the patterns, the symbol ?|?represents ?OR?.3.3 Automatic Sub-goal Generation andAutomatic Estimation of Rate ofAccomplishment of Sub-goalsWe found that expressions important for maintain-ing a high translation quality were often commonly4http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman.html37included in the reference translations for each test-set sentence.
We also found that the expression wasalso related to the yes/no question assigned to thetest-set sentence.
Therefore, we automatically gen-erate yes/no questions in the following steps.1.
For each test-set sentence, a set of words com-monly appearing in the reference translationsare extracted.2.
For each combination of n words in the setof words extracted in the first step, skip wordn-grams commonly appearing in the referencetranslations in the same word order are selectedas a set of common skip word n-grams.3.
For each test-set sentence, the sub-goal is de-fined as the yes/no question ?Are all of the com-mon skip word n-grams included in the transla-tion?
?If no common skip word n-grams are found, theyes/no question is not generated.
The answer to theyes/no question is determined to be yes if all of thecommon skip word n-grams are included in a trans-lation.
Otherwise, the answer is determined to beno.This scheme assigns greater weight to importantphrases that should be included in the translation tomaintain a high translation quality.
Our observationis that those important phrases are often commonbetween human translations.
A similar scheme wasproposed by Babych and Hartley (Babych and Hart-ley, 2004) for BLEU.
In their scheme, greater weightis assigned to components that are salient through-out the document.
Therefore, their scheme focuseson global context while our scheme focuses on localcontext.
We believe that the two schemes are com-plementary to each other.4 Experiments and DiscussionIn our experiments, the translation results of threeMT systems and their subjective evaluation resultswere used as a development set for constructing thepatterns described in Section 3.2 and for tuning theparameters ?Si, ?Qj, ?Q?j, and ?in Eq.
(1).
Thetranslations and evaluation results of the remainingtwo MT systems were used as an evaluation set fortesting.In the development set, each test-set sentence hasat least one question, at least one reference transla-tion, three MT results, and subjective evaluation re-sults of the three MT results.
The patterns for deter-mining yes/no answers were manually constructedfor the questions assigned to the 769 test-set sen-tences.
There were 917 questions assigned to them.Among them, the patterns could be constructed for898 questions assigned to 767 test-set sentences.The remaining 19 questions were skipped becausemaking simple patterns as described in Section 3.2was difficult; for example, one of the questionswas ?Is the whole sentence translated into one sen-tence??.
The yes/no answer determination accura-cies obtained by using the patterns are shown in Ta-ble 5.Table 5: Results of yes/no answer determination.Test set AccuracyDevelopment 97.6% (2,629/2,694)Evaluation 82.8% (1,487/1,796)We investigated the correlation between the eval-uation score, A in Eq.
(1) and the subjective eval-uations, fluency and adequacy, for the 769 test-setsentences.
First, to maximize the correlation coeffi-cients between the evaluation score, A, and the hu-man subjective evaluations, fluency and adequacy,the optimal values of ?Si, ?Qj, ?Q?j, and ?inEq.
(1) were investigated using the developmentset within a framework of multiple linear regressionmodeling (Draper and Smith, 1981).
Then, the cor-relation coefficients were investigated by using theoptimal value set.
The results are shown in Table 6,7, and 8.
In these tables, ?Conventional method?
in-dicates the correlation coefficients obtained when Awas calculated by using only similarities Si.
?Con-ventional method (combination)?
is a combinationof existing automatic evaluation methods from theliterature.
?Our method (automatic)?
indicates thecorrelation coefficients obtained when the results ofthe automatic determination of yes/no answers wereused to calculate Qjand Q?jin Eq.
(1).
For the 19questions for which the patterns could not be con-structed, Qjwas set at 0.
?Our method (full au-tomatic)?
indicates the correlation coefficients ob-tained when the results of the automatic sub-goalgeneration and determination of rate of accomplish-38Table 6: Coefficients of correlation between evaluation score A and fluency/adequacy.
(A reference transla-tion is used to calculate Si.
)Method fluency adequacyDevelopment set Evaluation set Development set Evaluation setConventional method (WER) 0.43 0.48 0.42 0.48Conventional method (combination) 0.52 0.51 0.49 0.47Our method (automatic) 0.90?
0.59?
0.89?
0.62?Our method (upper bound) 0.90?
0.62?
0.90?
0.68?Table 7: Coefficients of correlation between evaluation score A and fluency/adequacy.
(Three referencetranslations are used to calculate Si.
)Method fluency adequacyDevelopment set Evaluation set Development set Evaluation setConventional method (WER) 0.47 0.51 0.45 0.51Conventional method (combination) 0.54 0.54 0.51 0.52Our method (automatic) 0.90?
0.60?
0.90?
0.64?Our method (full automatic) 0.85?
0.58 0.84?
0.60?Our method (upper bound) 0.90?
0.62?
0.90?
0.69?Table 8: Coefficients of correlation between evaluation score A and fluency/adequacy.
(Five referencetranslations are used to calculate Si.
)Method fluency adequacyDevelopment set Evaluation set Development set Evaluation setConventional method (WER) 0.49 0.53 0.46 0.53Conventional method (combination) 0.56 0.56 0.52 0.54Our method (automatic) 0.90?
0.60 0.90?
0.63?Our method (full automatic) 0.86?
0.59 0.85?
0.60?Our method (upper bound) 0.91?
0.63?
0.90?
0.69?In these tables, ?
indicates significance at the 5% or less significance level.ment of sub-goals were used to calculate Qjand Q?jin Eq.
(1).
Skip word trigrams, skip word bigrams,and skip word unigrams were used for generatingthe sub-goals according to our preliminary experi-ments.
?Our method (upper bound)?
indicates thecorrelation coefficients obtained when human judg-ments on the questions were used to calculate Qjand Q?j.As shown in Table 6, 7, and 8, our methods signif-icantly outperform the conventional methods fromliterature.
Note that WER outperformed other indi-vidual measures like BLEU and NIST in our exper-iments, and the combination of existing automaticevaluation methods from the literature outperformedindividual lexical similarity measures by themselvesin almost all cases.
The differences between thecorrelation coefficients obtained using our methodand the conventional methods are statistically sig-nificant at the 5% or less significance level for flu-ency and adequacy, even if the number of referencetranslations increases, except in three cases shownin Table 7 and 8.
This indicates that consideringthe rate of accomplishment of sub-goals to automat-ically evaluate the quality of each translation is use-ful, especially when the number of reference trans-lations is small.The differences between the correlation coeffi-cients obtained using two automatic methods are notsignificant.
These results indicate that we can reducethe development cost for constructing sub-goals.However, there are still significant gaps between thecorrelation coefficients obtained using a fully auto-matic method and upper bounds.
These gaps indi-cate that we need further improvement in automaticsub-goal generation and automatic estimation of rateof accomplishment of sub-goals, which is our futurework.Human judgments of adequacy and fluency areknown to be noisy, with varying levels of intercoderagreement.
Recent work has tended to apply cross-judge normalization to address this issue (Blatz etal., 2003).
We would like to evaluate against thenormalized data in the future.395 Conclusion and Future WorkWe demonstrated that the quality of a translated sen-tence can be evaluated more appropriately than byusing conventional methods.
That was demonstratedby constructing a test set where the conditions thatshould be satisfied to maintain a high translationquality are assigned to each test-set sentence in theform of a question, by developing a system that de-termines an answer to each question, and by com-bining a measure based on the questions and con-ventional measures.
We also presented a method forautomatically generating sub-goals in the form ofyes/no questions and estimating the rate of accom-plishment of the sub-goals.
Promising results wereobtained.In the near future, we would like to expand thetest set to improve the upper bound obtained byour method.
We are also planning to expand themethod and improve the accuracy of the automaticsub-goal generation and determination of the rate ofaccomplishment of sub-goals.
The sub-goals of agiven sentence should be generated by consideringthe complexity of the sentence and the alignment in-formation between the original source-language sen-tence and its translation.
Further advanced genera-tion and estimation would give us information aboutthe erroneous parts of MT results and their quality.We believe that future research would allow us todevelop high-quality MT systems by tuning the sys-tem parameters based on the automatic MT evalua-tion measures.AcknowledgmentsThe guideline for expanding the test set is based on that con-structed by the Technical Research Committee of the AAMT(Asia-Pacific Association for Machine Translation) The authorswould like to thank the committee members, especially, Mr.Kentaro Ogura, Ms. Miwako Shimazu, Mr. Tatsuya Sukehiro,Mr.
Masaru Fuji, and Ms. Yoshiko Matsukawa for their coop-eration.
This research is partially supported by special coordi-nation funds for promoting science and technology.ReferencesYasuhiro Akiba, Kenji Imamura, and Eiichiro Sumita.
2001.Using Multiple Edit Distances to Automatically Rank Ma-chine Translation Output.
In Proceedings of the MT SummitVIII, pages 15?20.Bogdan Babych and Anthony Hartley.
2004.
Extending theBLEU MT Evaluation Method with Frequency Weightings.In Proceedings of the 42nd ACL, pages 622?629.Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: An au-tomatic metric for mt evaluation with improved correlationwith human judgments.
In Proceedings of Workshop on In-trinsic and Extrinsic Evaluation Measures for MT and/orSummarization, pages 65?72.John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur,Cyril Goutte, Alex Kulesza, Alberto Sanchis, and NicolaUeffing.
2003.
Confidence Estimation for Machine Trans-lation.
Technical report, Center for Language and SpeechProcessing, Johns Hopkins University.
Summer WorkshopFinal Report.Norman R. Draper and Harry Smith.
1981.
Applied RegressionAnalysis.
2nd edition.
Wiley.Jesus?
Gimen?ez, Enrique Amigo?, and Chiori Hori.
2005.
Ma-chine translation evaluation inside qarla.
In Proceedings ofthe IWSLT?05.Satoru Ikehara, Satoshi Shirai, and Kentaro Ogura.
1994.
Cri-teria for Evaluating the Linguistic Quality of Japanese toEnglish Machine Translations.
Transactions of the JSAI,9(4):569?579.
(in Japanese).Hitoshi Isahara.
1995.
JEIDA?s Test-Sets for Quality Evalua-tion of MT Systems ?
Technical Evaluation from the Devel-oper?s Point of View.Philipp Koehn.
2004.
Statistical Significance Tests for Ma-chine Translation Evaluation.
In Proceedings of the 2004Conference on EMNLP, pages 388?395.Gregor Leusch, Nicola Ueffing, and Hermann Ney.
2003.
ANovel String-to-String Distance Measure with Applicationsto Machine Translation Evaluation.
In Proceedings of theMT Summit IX, pages 240?247.Chin-Yew Lin and Franz Josef Och.
2004.
ORANGE: aMethod for Evaluating Automatic Evaluation Metrics forMachine Translation.
In Proceedings of the 20th COLING,pages 501?507.Chin-Yew Lin.
2004.
ROUGE: A Package for Automatic Eval-uation of Summaries.
In Proceedings of the Workshop onText Summarization Branches Out, pages 74?81.Sonja Niessen, Franz Josef Och, Gregor Leusch, and HermannNey.
2000.
An Evaluation Tool for Machine Translation:Fast Evaluation for MT Research.
In Proceedings of theLREC 2000, pages 39?45.NIST.
2002.
Automatic Evaluation of Machine TranslationQuality Using N-gram Co-Occurrence Statistics.
Technicalreport, NIST.Franz Josef Och.
2003.
Minimum Error Training in StatisticalMachine Translation.
In Proceedings of the 41st ACL, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei jing Zhu.2002.
BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of the 40th ACL, pages311?318.TIDES.
2002.
Linguistic Data Annotation Specifi-cation: Assessment of Fluency and Adequacy inArabic-English and Chinese-English Translations.http://www.ldc.upenn.edu/Projects/TIDES/Translation/TransAssess02.pdf.Joseph P. Turian, Luke Shen, and I. Dan Melamed.
2003.
Eval-uation of Machine Translation and its Evaluation.
In Pro-ceedings of the MT Summit IX, pages 386?393.40
