Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 227?237,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsBayesian Checking for Topic ModelsDavid MimnoDepartment of Computer SciencePrinceton University Princeton, NJ 08540mimno@cs.princeton.eduDavid BleiDepartment of Computer SciencePrinceton University Princeton, NJ 08540blei@cs.princeton.eduAbstractReal document collections do not fit the inde-pendence assumptions asserted by most statis-tical topic models, but how badly do they vi-olate them?
We present a Bayesian methodfor measuring how well a topic model fits acorpus.
Our approach is based on posteriorpredictive checking, a method for diagnosingBayesian models in user-defined ways.
Ourmethod can identify where a topic model fitsthe data, where it falls short, and in which di-rections it might be improved.1 IntroductionProbabilistic topic models are a suite of machinelearning algorithms that decompose a corpus intoa set of topics and represent each document with asubset of those topics.
The inferred topics often cor-respond with the underlying themes of the analyzedcollection, and the topic modeling algorithm orga-nizes the documents according to those themes.Most topic models are evaluated by their predic-tive performance on held out data.
The idea is thattopic models are fit to maximize the likelihood (orposterior probability) of a collection of documents,and so a good model is one that assigns high likeli-hood to a held out set (Blei et al, 2003; Wallach etal., 2009).But this evaluation is not in line with howtopic models are frequently used.
Topic mod-els seem to capture the underlying themes of acollection?indeed the monicker ?topic model?
isretrospective?and so we expect that these themesare useful for exploring, summarizing, and learningabout its documents (Mimno and McCallum, 2007;Chang et al, 2009).
In such exploratory data anal-ysis, however, we are not concerned with the fit toheld out data.In this paper, we develop and study new methodsfor evaluating topic models.
Our methods are basedon posterior predictive checking, which is a modeldiagnosis technique from Bayesian statistics (Rubin,1984; Gelman et al, 1996).
The goal of a posteriorpredictive check (PPC) is to assess the validity of aBayesian model without requiring a specific alterna-tive model.
Given data, we first compute a posteriordistribution over the latent variables.
Then, we esti-mate the probability of the observed data under thedata-generating distribution that is induced by theposterior (the ?posterior predictive distribution?).
Adata set that is unlikely calls the model into ques-tion, and consequently the posterior.
PPCs can showwhere the model fits and doesn?t fit the observations.They can help identify the parts of the posterior thatare worth exploring.The key to a posterior predictive check is the dis-crepancy function.
This is a function of the data thatmeasures a property of the model which is impor-tant to capture.
While the model is often chosenfor computational reasons, the discrepancy functionmight capture aspects of the data that are desirablebut difficult to model.
In this work, we will designa discrepancy function to measure an independenceassumption that is implicit in the modeling assump-tions but is not enforced in the posterior.
We willembed this function in a posterior predictive checkand use it to evaluate and visualize topic models innew ways.227Specifically, we develop discrepancy functionsfor latent Dirichlet alocation (the simplest topicmodel) that measure how well its statistical assump-tions about the topics are matched in the observedcorpus and inferred topics.
LDA assumes that eachobserved word in a corpus is assigned to a topic, andthat the words assigned to the same topic are drawnindependently from the same multinomial distribu-tion (Blei et al, 2003).
For each topic, we mea-sure the whether this assumption holds by comput-ing the mutual information between the words as-signed to that topic and which document each wordappeared in.
If the assumptions hold, these two vari-ables should be independent: low mutual informa-tion indicates that the assumptions hold; high mu-tual information indicates a mismatch to the model-ing assumptions.We embed this discrepancy in a PPC and studyit in several ways.
First, we focus on topics thatmodel their observations well; this helps separateinterpretable topics from noisy topics (and ?boiler-plate?
topics, which exhibit too little noise).
Sec-ond, we focus on individual terms within topics; thishelps display a model applied to a corpus, and under-stand which terms are modeled well.
Third, we re-place the document identity with an external variablethat might plausibly be incorporated into the model(such as time stamp or author).
This helps point themodeler towards the most promising among morecomplicated models, or save the effort in fitting one.Finally, we validate this strategy by simulating datafrom a topic model, and assessing whether the PPC?accepts?
the resulting data.2 Probabilistic Topic ModelingProbabilistic topic models are statistical models oftext that assume that a small number of distributionsover words, called ?topics,?
are used to generate theobserved documents.
One of the simplest topic mod-els is latent Dirichlet alocation (LDA) (Blei et al,2003).
In LDA, a set of K topics describes a cor-pus; each document exhibits the topics with differentproportions.
The words are assumed exchangeablewithin each document; the documents are assumedexchangeable within the corpus.More formally, let ?1, .
.
.
, ?K be K topics, eachof which is a distribution over a fixed vocabulary.For each document, LDA assumes the followinggenerative process1.
Choose topic proportions ?d ?
Dirichlet(?).2.
For each word(a) Choose topic assignment zd,n ?
?.
(b) Choose word wd,n ?
?zd,n .This process articulates the statistical assumptionsbehind LDA: Each document is endowed with itsown set of topic proportions ?d, but the same set oftopics ?1:K governs the whole collection.Notice that the probability of a word is indepen-dent of its document ?d given its topic assignmentzd,n (i.e., wd,n ?
?d | zd,n).
Two documents mighthave different overall probabilities of containing aword from the ?vegetables?
topic; however, all thewords in the collection (regardless of their docu-ments) drawn from that topic will be drawn from thesame multinomial distribution.The central computational problem for LDA isposterior inference.
Given a collection of docu-ments, the problem is to compute the conditionaldistribution of the hidden variables?the topics ?k,topic proportions ?d, and topic assignments zd,n.Researchers have developed many algorithms forapproximating this posterior, including samplingmethods (Griffiths and Steyvers, 2004) (used in thispaper), variational methods (Blei et al, 2003), dis-tributed variants (Asuncion et al, 2008), and onlinealgorithms (Hoffman et al, 2010).3 Checking Topic ModelsOnce approximated, the posterior distribution isused for the task at hand.
Topic models have beenapplied to many tasks, such as classification, predic-tion, collaborative filtering, and others.
We focuson using them as an exploratory tool, where we as-sume that the topic model posterior provides a gooddecomposition of the corpus and that the topics pro-vide good summaries of the corpus contents.But what is meant by ?good??
To answer thisquestion, we turn to Bayesian model checking (Ru-bin, 1981; Gelman et al, 1996).
The goal ofBayesian model checking is to assess whether theobserved data matches the modeling assumptions inthe directions that are important to the analysis.
The228ScoreRank141210864214121086421412108642Topic850weekendBroadwayTimeslistingselectivenoteworthycritics ticketshighlyrecommendeddenotesboothTicketsStreetTKTSTopic628IraqIraqi HusseinBaghdadSaddamShiitegovernmental IraqisSunniKurdishforcescountrymilitarytroopsTopic87Roberts GrantFortWorth BurkeHuntKravis BassKohlbergGraceRothschildBaronBordenTexasWilliam1 2 3 4Topic371TicketsThroughStreetRoadSaturdaysSundaysNewFridaysJerseyHoursFreeTuesdaysMUSEUMThursdaysTHEATERTopic178agencysafetyreportFederalAdministrationproblemsinvestigationSafetyviolationsfederalfailedinspectorreviewdepartmentgeneralTopic750Four FreemanSeasonsDaVinciCode ThomsonWolffLeonardoBrownThreeDanCliffHolyda1 2 3 4Topic760WeekbookWarnersalesListWeekswomanbookstoresdeathindicatesAdvicePutnamOFreportNewTopic632job jobsworkingofficebusinesscareerworkedemployeeshiredbossmanagerfindcorporatehelpexperienceTopic274LeonLevy HessBardLEVYBotsteinAtlas ShelbyPanetta Norman WieseltierHESSDavidAmerada Norma1 2 3 4Figure 1: Visualization of variability within topics.
Nine randomly selected topics from the New York Times withlow (top row), medium (middle row) and high (bottom row) mutual information between words and documents.
They-axis shows term rank within the topic, with size proportional to log probability.
The x-axis represents divergencefrom the multinomial assumption for each word: terms that are uniformly distributed across documents are towardsthe left, while more specialized terms are to the right.
Triangles represent real values, circles represent 20 replicationsof this same plot from the posterior of the model.229intuition is that only when satisfied with the modelshould the modeler use the posterior to learn abouther data.
In complicated Bayesian models, such astopic models, Bayesian model checking can point tothe parts of the posterior that better fit the observeddata set and are more likely to suggest somethingmeaningful about it.In particular, we will develop posterior predictivechecks (PPC) for topic models.
In a PPC, we spec-ify a discrepancy function, which is a function ofthe data that measures an important property that wewant the model to capture.
We then assess whetherthe observed value of the function is similar to val-ues of the function drawn from the posterior, throughthe distribution of the data that it induces.
(This dis-tribution of the data is called the ?posterior predic-tive distribution.?
)An innovation in PPCs is the realized discrepancyfunction (Gelman et al, 1996), which is a functionof the data and any hidden variables that are in themodel.
Realized discrepancies induce a traditionaldiscrepancy by marginalizing out the hidden vari-ables.
But they can also be used to evaluate assump-tions about latent variables in the posterior, espe-cially when combined with techniques like MCMCsampling that provide realizations of them.
In topicmodels, as we will see below, we use a realized dis-crepancy to factor the observations and to check spe-cific components of the model that are discovered bythe posterior.3.1 A realized discrepancy for LDAReturning to LDA, we design a discrepancy func-tion that checks the independence assumption ofwords given their topic assignments.
As we men-tioned above, given the topic assignment z the wordw should be independent of its document ?.
Con-sider a decomposition of a corpus from LDA, whichassigns every observed word wd,n to a topic zd,n.Now restrict attention to all the words assigned to thekth topic and form two random variables: W are thewords assigned to the topic and D are the documentindices of the words assigned to that topic.
If theLDA assumptions hold then knowing W gives noinformation about D because the words are drawnindependently from the topic.We measure this independence with the mutualinformation between W and D:1MI(W,D | k)=?w?dP (w, d | k) log P (w | d, k)P (d | k)P (w | k)P (d | k)=?w?dN(w, d, k)N(k) logN(w, d, k)N(k)N(d, k)N(w, k) .
(1)Where N(w, d, k) is the number of tokens of typew in topic k in document d, with N(w, k) =?dN(w, d, k), N(d, k) =?wN(w, d, k), andN(k) =?w,dN(w, d, k).
This function mea-sures the divergence between the joint distributionover word and document index and the product ofthe marginal distributions.
In the limit of infinitesamples, independent random variables have mutualinformation of zero, but we expect finite samplesto have non-zero values even for truly independentvariables.
Notice that this is a realized discrepancy;it depends on the latent assignments of observedwords to topics.Eq.
1 is defined as a sum over a set of documentsand a set of words.
We can rearrange this summa-tion as a weighted sum of the instantaneous mutualinformation between words and documents:IMI(w,D | k) = H(D|k)?H(D |W = w, k).
(2)This quantity can be understood by considering theper-topic distribution of document labels, p(d|k).This distribution is formed by normalizing thecounts of how many words assigned to topic k ap-peared in each document.
The first term of Eq.
2is the entropy?some topics are evenly distributedacross many documents (high entropy); others areconcentrated in fewer documents (low entropy).The second term conditions this distribution ona particular word type w by normalizing the per-document number of times w appeared in each doc-ument (in topic k).
If this distribution is closeto p(d|k) then H(D|W = w, k) will be close toH(D|k) and IMI(w,D|k) will be low.
If, on theother hand, word w occurs many times in only a fewdocuments, it will have lower entropy over docu-1There are other choices of discrepancies, such as word-word point-wise mutual information scores (Newman et al,2010).230ments than the overall distribution over documentsfor the topic and IMI(w,D|k) will be high.We illustrate this discrepancy in Figure 1, whichshows nine topics trained from the New York Times.2Each row contains randomly selected topics fromlow, middle, and high ranges of MI, respectively.Each triangle represents a word.
Its place on the y-axis is its rank in the topic.
Its place on the x-axisis its IMI(w|k), with more uniformly distributedwords (low IMI) to the left and more specific words(high IMI) to the right.
(For now, ignore the otherpoints in this figure.)
IMI varies between topics, buttends to increase with rank as less frequent wordsappear in fewer documents.The discrepancy captures different kinds of struc-ture in the topics.
The top left topic represents for-mulaic language, language that occurs verbatim inmany documents.
In particular, it models the boil-erplate text ?Here is a selective listing by critics ofThe Times of new or noteworthy...?
Identifying re-peated phrases is a common phenomenon in topicmodels.
Most words show lower than expected IMI,indicating that word use in this topic is less vari-able than data drawn from a multinomial distribu-tion.
The middle-left topic is an example of a goodtopic, according to this discrepancy, which is relatedto Iraqi politics.
The bottom-left topic is an exampleof the opposite extreme from the top-left.
It showsa loosely connected series of proper names with nooverall theme.3.2 Posterior Predictive Checks for LDAIntuitively, the middle row of topics in Figure 1 arethe sort of topics we look for in a model, while thetop and bottom rows contain topics that are less use-ful.
Using a PPC, we can formally measure the dif-ference between these topics.
For each of the realtopics in Figure 1 we regenerated the same figure20 times.
We sampled new words for every tokenfrom the posterior distribution of the topic, and re-calculated the rank and IMI for each word.
These?shadow?
figures are shown as gray circles.
Thedensity of those circles creates a reference distribu-tion indicating the expected IMI values at each rankunder the multinomial assumption.2Details about the corpus and model fitting are in Section4.2.
Similar figures for two other corpora are in the supplement.By themselves, IMI scores give an indication ofthe distribution of a word between documents withina topic: small numbers are better, large numbers in-dicate greater discrepancy.
These scores, however,are based on the specific allocation of words to top-ics.
For example, lower-ranked, less frequent wordswithin a topic tend to have higher IMI scores thanhigher-ranked, more frequent words.
This differencemay be due to greater violation of multinomial as-sumptions, but may also simply be due to smallersample sizes, as the entropy H(D|W = w, k) is es-timated from fewer tokens.
The reference distribu-tions help distinguish between these two cases.In more detail, we generate replications of thedata by considering a Gibbs sampling state.
Thisstate assigns each observed word to a topic.
Wefirst record the number of instances of each term as-signed to each topic, N(w|k).
Then for each wordwd,n in the corpus, we sample a new observed wordwrepd,n where P (w) ?
N(w|zd,n).
(We did not usesmoothing parameters.)
Finally, we recalculate themutual information and instantenous mutual infor-mation for each topic.In the top-left topic, most of the words have muchlower IMI than the word at the same rank in repli-cations, indicating lower than expected variability.The exception is the word Broadway, which is morevariable than expected.
In the middle-left topic,IMI for the words Iraqi and Baghdad occur withinthe expected range.
These words fit the multino-mial assumption: any word assigned to this topicis equally likely to be Iraqi.
Values for the wordsShiite, Sunni, and Kurdish are more specific to par-ticular documents than we expect under the model.In the bottom-left topic, almost all words occur withgreater variability than expected.
This topic com-bines many terms with only coincidental similarity,such as Mets pitcher Grant Roberts and the firmKohlberg Kravis Roberts.Turning to an analysis of the full mutual infor-mation, Figure 2 shows the three left-hand topicsfrom Figure 1: Weekend, Iraq, and Roberts.
Thehistogram represents MI scores for 100 replicationsof the topic, rescaled to have mean zero and unitvariance.
The observed value, also rescaled, andthe mean replicated value (set to zero) are shownwith vertical lines.
The formulaic Weekend topichas significantly lower than expected MI.
The Iraq231Deviancecount051015202530051015202530051015202530Topic850Topic628Topic87?20 0 20 40Figure 2: News: Observed topic scores (vertical lines)relative to replicated scores, rescaled so that replica-tions have zero mean and unit variance.
The Weekendtopic (top) has lower than expected MI.
The Iraq (mid-dle) and Roberts (bottom) topics both have MI greaterthan expected.and Roberts topics have significantly greater thanexpected MI.For most topics the actual discrepancy is outsidethe range of any replicated discrepancies.
In theiroriginal formulation, PPCs prescribe computing atail probability of a replicated discrepancy beinggreater than (or less than) the observed discrepancyunder the posterior predictive distribution.
For ex-ample if an observed value is greater than 70 of 100replicated values, we report a PPC p-value of 0.7.When the observed value is far outside the rangeof any replicated values, as in Figure 2, that tailprobability will be degenerate at 0 or 1.
So, we re-port instead a deviance value, an alternative way ofcomparing an observed value to a reference distri-bution.
We compute the distribution of the repli-cated discrepancies and compute its standard devi-ation.
We then compute how many standard devia-tions the observed discrepancy is from the mean ofthe replicated discrepancies.This score allows us to compare topics.
The ob-served value for the Weekend topic is 31.8 standarddeviations below the mean replicated value, and thushas deviance of -31.8, which is lower than expected.The Iraq topic has deviance of 16.8 and the Robertstopic has deviance of 47.7.
This matches our intu-ition that the former topic is more useful than thelatter.4 Searching for Systematic DeviationsWe demonstrated that the mutual information dis-crepancy function can detect violations of multi-nomial assumptions, in which instances of a termin a given topic are not independently distributedamong documents.
One way to address this lackof fit is to encode document-level extra-multinomialvariance (?burstiness?)
into the model using Dirich-let compound multinomial distributions (Doyle andElkan, 2009).
If there is no pattern to the deviationsfrom multinomial word use across documents, thismethod is the best we can do.In many corpora, however, there are systematicdeviations that can be explained by additional vari-ables.
LDA is the simplest generative topic model,and researchers have developed many variants ofLDA that account for a variety of variables that canbe found or measured with a corpus.
Examples in-clude models that account for time (Blei and Laf-ferty, 2006), books (Mimno and McCallum, 2007),and aspect or perspective (Mei and Zhai, 2006; Linet al, 2008; Paul et al, 2010).
In this section, weshow how we can use the mutual information dis-crepancy function of Equation 1 and PPCs to guideour choice in which topic model to fit.Greater deviance implies that a particular group-ing better explains the variation in word use withina topic.
The discrepancy functions are large whenwords appear more than expected in some groupsand less than expected in others.
We know thatthe individual documents show significantly morevariation than we expect from replications from themodel?s posterior distribution.
If we combine docu-ments randomly in a meaningless grouping, such de-viance should decrease, as differences between doc-uments are ?smoothed out.?
If a grouping of docu-ments shows equal or greater deviation, we can as-sume that that grouping is maintaining the underly-ing structure of the systematic deviation from themultinomial assumption, and that further modelingor visualization using that grouping might be useful.4.1 PPCs for systematic discrepancyThe idea is that the words assigned to a topic shouldbe independent of both document and any other vari-able that might be associated with the document.
Wesimply replace the document index d with another232ScoreRank2015105DocumentsIraqIraqi HusseinBaghdadSaddamShiitegovernmental IraqisSunniKurdishforcescountrymilitarytroopsleaderscity KurdssecuritySadr0.0 0.5 1.0 1.5 2.0 2.5MonthsIraqIraqiHusseinBaghdadSaddamShiitegovernmental IraqisSunniKurdishforcescountrymilitarytroopsleaderscityKurdssecurity Sadr0.0 0.5 1.0 1.5 2.0 2.5DesksIraqIraqiHusseinBaghdadSaddamShiitegovernmentalIraqisSunniKurdishforcescountrymilitarytroopsleaderscityKurdssecuritySadr0.0 0.5 1.0 1.5 2.0 2.5Figure 3: Groupings decrease MI, but values are still larger than expected.
Three ways of grouping words in atopic from the New York Times.
The word leaders varies more between desks than by time, while Sadr varies more bytime than desk.variable g in the discrepancy.
For example, the NewYork Times articles are each associated with a par-ticular news desk and also associated with a timestamp.
If the topic modeling assumptions hold, thewords are independent of both these variables.
If wesee a significant discrepancy relative to a groupingdefined by a metadata feature, this systematic vari-ability suggests that we might want to take that fea-ture into account in the model.Let G be a set of groups and let ?
?
GD bea grouping of D documents.
Let N(w, g, k) =?dN(w, d, k)I?d=g, that is, the number of words oftypew in topic k in documents in group g, and definethe other count variables similarly.
We can now sub-stitute these group-specific counts for the document-specific counts in the discrepancy function in Eq.1.
Note that the previous discrepancy functions areequivalent to a trivial grouping, in which each docu-ment is the only member of its own group.
In the fol-lowing experiments we explore groupings by pub-lished volume, blog, preferred political candidate,and newspaper desk, and evaluate the effect of thosegroupings on the deviation between mean replicatedvalues and observed values of those functions.4.2 Case studiesWe analyze three corpora, each with its own meta-data: the New York Times Annotated Corpus (1987?2007)3, the CMU 2008 political blog corpus (Eisen-stein and Xing, 2010), and speeches from the British3http://www.ldc.upenn.eduHouse of Commons from 1830?1891.4 Descriptivestatistics are presented in Table 1.
The realizationis represented by a single Gibbs sampling state after1000 iterations of Gibbs sampling.Table 1: Statistics for models used as examples.Name Docs Tokens Vocab TopicsNews 1.8M 76M 121k 1000Blogs 13k 2.2M 90k 100Parliament 540k 55M 52k 300New York Times articles.
Figure 3 shows threegroupings of words for the middle-left topic in Fig-ure 1: by document, by month of publication (e.g.May of 2005), and by desk (e.g.
Editorial, Foreign,Financial).
Instantaneous mutual information valuesare significantly smaller for the larger groupings, butthe actual values are still larger than expected underthe model.
We are interested in measuring the de-gree to which word usage varies within topics as afunction of both time and the perspective of the ar-ticle.
For example, we may expect that word choicemay differ between opinion articles, which overtlyreflect an author?s views, and news articles, whichtake a more objective, factual approach.We summarize each grouping by plotting the dis-tribution of deviance scores for all topics.
Resultsfor all 1000 topics grouped by documents, months,and desks are shown in Figure 4.4http://www.hansard-archive.parliament.uk/233DevianceGroupingDocumentsMonthsDesksl lll l l ll ll l ll llll lll llll l ll ll lll lll lll lll lll l l lll l l ll lll l ll l0 100 200 300 400Figure 4: News: Lack of fit correlates best with desks.We calculate the number of standard deviations betweenthe mean replicated discrepancy and the actual discrep-ancy for each topic under three groupings.
Boxes repre-sent typical ranges, points represent outliers.MonthScore0.00000.00050.00100.00150.00000.00050.00100.00150e+002e?044e?046e?040.00000.00050.00100.00150.00200.00250e+002e?044e?046e?048e?04?2e?040e+002e?044e?046e?04KurdishHusseinSunniSadrMalikiShiite1987 1992 1997 2002 2007Figure 5: News: Events change word distributions.Words with the largest MI from a topic on Iraq?s gov-ernment are shown, with individual scores grouped bymonth.Finally, we can analyze how individual words in-teract with groupings like time or desk.
Figure 5breaks down the per-word discrepancy shown in Fig-ure 3 by month, for the words with the largest overalldiscrepancy.
Kurdish is prominent during the GulfWar and the 1996 cruise missile strikes, but is lesssignificant during the Iraq War.
Individuals (Hus-sein, Sadr, and Maliki) move on and off the stage.Political blogs.
The CMU 2008 political blog cor-pus consists of six blogs, three of which supportedBarack Obama and three of which supported JohnMcCain.
This corpus has previously been consid-ered in the context of aspect-based topic models(Ahmed and Xing, 2010) that assign distinct worddistributions to liberal and conservative bloggers.
Itis reasonable to expect that blogs with different po-litical leanings will use measurably different lan-guage to describe the same themes, suggesting thatthere will be systematic deviations from a multino-mial hypothesis of exchangeability of words withintopics.
Indeed, Ahmed and Xing obtained improvedresults with such a model.
Figure 6 shows the dis-tribution of standard deviations from the mean repli-cated value for a set of 150 topics grouped by doc-ument, blog, and preferred candidate.
Deviance isgreatest for blogs, followed by candidates and thendocuments.DevianceGroupingDocumentsBlogsCandidateslllllll lll0 100 200 300 400Figure 6: Blogs: Lack of fit correlates more with blogthan preferred candidate.
Grouping by preferred can-didate has only slightly higher average deviance than bydocuments, but the variance is greater.Grouping by blogs appears to show greater de-viance from mean replicated values than group-ing by candidates, indicating that there is fur-ther structure in word choice beyond a simple lib-eral/conservative split.
Are these results, however,comparable?
It may be that this difference is ex-plained by the fact that there are six blogs and only234two candidates.
To determine whether this particularassignment of documents to blogs is responsible forthe difference in discrepancy functions or whetherany such split would have greater deviance, we com-pared random groupings to the real groupings andrecalculate the PPC.
We generated 10 such group-ings by permuting document blog labels and another10 by permuting document candidate labels, eachtime holding the topics fixed.
The average numberof standard deviations across topics was 6.6 ?
14.4for permuted ?candidates?
compared to 37.9?
39.2for the real corpus, and 10.6 ?
12.9 for permuted?blogs?
compared to 44.4?
29.6 for real blogs.British parliament proceedings.
The parliamentcorpus is divided into 305 volumes, each comprisingabout three weeks of debates, with between 600 and4000 speeches per session.
In addition to volumes,10 Prime Ministers were in office during this period.DevianceGroupingDocumentsVolumesPMsll0 100 200 300 400Figure 7: Parliament: Lack-of-fit correlates with time(publication volume).
Correlation with prime ministersis not significantly better than with volume.Grouping by prime minister shows greater av-erage deviance than grouping by volumes, eventhough there are substantially fewer divisions.
Al-though such results would need to be accompaniedby permutation experiments as in the blog corpus,this methodology may be of interest to historians.In order to provide insight into the nature of tem-poral variation, we can group the terms in the sum-mation in Equation 1 by word and rank the words bytheir contribution to the discrepancy function.
Fig-ure 8 shows the most ?mismatching?
words for atopic with the most probable words ships, vessels,admiralty, iron, ship, navy, consistent with changesin naval technology during the Victorian era (thatis, wooden ships to ?iron clads?).
Words that oc-cur more prominently in the topic (ships, vessels)are also variable, but more consistent across time.VolumeScore0.00000.00050.00100.00150.00000.00050.00100.00150.00000.00050.00100.00150.00000.00050.00100.00150.00000.00050.00100.00150.00000.00050.00100.0015ironturretcladswoodenvesselsships1830 1835 1840 1845 1850 1855 1860 1865 1870 1875 1880 1885 1890Figure 8: Parliament: iron-clads introduced in 1860s.High probability words (ships, vessels) are variable, butshow less concentrated discrepancy than iron, wooden.5 Calibration on Synthetic DataA posterior predictive check asks ?do observationssampled from the learned model look like the origi-nal data??
In the previous sections, we have consid-ered PPCs that explore variability within a topic ona per-word basis, measure discrepancy at the topiclevel, and compare deviance over all topics betweengroupings of documents.
Those results show thatthe PPC detects deviation from multinomial assump-tions when it exists: as expected, variability in wordchoice aligns with known divisions in corpora, forexample by time and author perspective.
We nowconsider the opposite direction.
When documentsare generated from a multinomial topic model, PPCsshould not detect systematic deviation.We must also distinguish between lack of fit dueto model misspecification and lack of fit due to ap-proximate inference.
In this section, we present syn-thetic data experiments where the learned model isprecisely the model used to generate documents.
Weshow that there is significant lack of fit introducedby approximate inference, which can be correctedby considering only parts of the model that are well-estimated.We generated 10 synthetic corpora, each consist-ing of 100,000 100-word documents, drawn from 20235pcount010203040All0.0 0.2 0.4 0.6 0.8 1.0TopDocs0.0 0.2 0.4 0.6 0.8 1.0TopWords0.0 0.2 0.4 0.6 0.8 1.0TopWordsDocs0.0 0.2 0.4 0.6 0.8 1.0Figure 9: Replicating only documents with large allocation in the topic leads to more uniform p-values.
p-valuesfor 200 topics estimated from synthetic data generated from an LDA model are either uniform or skewed towards 1.0.Overly conservative p-values would be clustered around 0.5.topics over a vocabulary of 100 terms.
Hyperpa-rameters for both the document-topic and topic-termDirichlet priors were 0.1 for each dimension.
Wethen trained a topic model with the same hyperpa-rameters and number of topics on each corpus, sav-ing a Gibbs sampling state.We can measure the fit of a PPC by examining thedistribution of empirical p-values, that is, the propor-tion of replications wrep that result in discrepanciesless than the observed value.
p-values should be uni-formly distributed on (0, 1).
Non-uniform p-valuesindicate a lack of calibration.
Unlike real collec-tions, in synthetic corpora the range of discrepan-cies from these replicated collections often includesthe real values, so p-values are meaningful.
A his-togram of p-values for 200 synthetic topics after 100replications is shown in the left panel of Figure 9.PPCs have been criticized for reusing trainingdata for model checking.
For some models, theposterior distribution is too close to the data, so allreplicated values are close to the real value, leadingto p-values clustered around 0.5 (Draper and Krn-jajic, 2006; Bayarri and Castellanos, 2007).
Wetest divergence from a uniform distribution with aKolmogorov-Smirnov test.
Our results indicate thatLDA is not overfitting, but that the distribution is notuniform (KS p < 0.00001).The PPC framework allows us to choose discrep-ancy functions that reflect the relative importanceof subsets of words and documents.
The secondpanel in Figure 9 sums only over the 20 documentswith the largest probability of the topic, the thirdsums over all documents but only over the top 10most probable words, and the fourth sums over onlythe top words and documents.
This test indicatesthat the distribution of p-values for the subset Top-Words is not uniform (KS p < 0.00001), but that auniform distribution is a good fit for TopDocs (KSp = 0.358) and TopWordsDocs (KS p = 0.069).6 ConclusionsWe have developed a Bayesian model checkingmethod for probabilistic topic models.
Conditionedon their topic assignment, the words of the docu-ments are independently and identically distributedby a multinomial distribution.
We developed a real-ized discrepancy function?the mutual informationbetween words and document indices, conditionedon a topic?that checks this assumption.
We em-bedded this function in a posterior predictive check.We demonstrated that we can use this posteriorpredictive check to identify particular topics that fitthe data, and particular topics that misfit the data indifferent ways.
Moreover, our method provides anew way to visualize topic models.We adapted the method to corpora with externalvariables.
In this setting, the PPC provides a way toguide the modeler in searching through more com-plicated models that involve more variables.Finally, on simulated data, we demonstrated thatPPCs with the mutual information discrepancy func-tion can identify model fit and model misfit.AcknowledgmentsDavid M. Blei is supported by ONR 175-6343, NSFCAREER 0745520, AFOSR 09NL202, the Alfred P.Sloan foundation, and a grant from Google.
DavidMimno is supported by a Digital Humanities Re-search grant from Google.
Arthur Spirling and Andy236Eggers suggested the use of the Hansards corpus.ReferencesAmr Ahmed and Eric Xing.
2010.
Staying informed: Su-pervised and semi-supervised multi-view topical anal-ysis of ideological perspective.
In EMNLP.Arthur Asuncion, Padhraic Smyth, and Max Welling.2008.
Asynchronous distributed learning of topicmodels.
In NIPS.M.J.
Bayarri and M.E.
Castellanos.
2007.
Bayesianchecking of the second levels of hierarchical models.Statistical Science, 22(3):322?343.David M. Blei and John D. Lafferty.
2006.
Dynamictopic models.
In ICML.David Blei, Andrew Ng, and Michael Jordan.
2003.
La-tent Dirichlet alocation.
Journal of Machine LearningResearch, 3:993?1022, January.Jonathan Chang, Jordan Boyd-Graber, Chong Wang,Sean Gerrish, and David M. Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In Ad-vances in Neural Information Processing Systems 22,pages 288?296.Gabriel Doyle and Charles Elkan.
2009.
Accounting forburstiness in topic models.
In ICML.David Draper and Milovan Krnjajic.
2006.
Bayesianmodel specification.
Technical report, University ofCalifornia, Santa Cruz.Jacob Eisenstein and Eric Xing.
2010.
The CMU 2008political blog corpus.
Technical report, Carnegie Mel-lon University.A.
Gelman, X.L.
Meng, and H.S.
Stern.
1996. poste-rior predictive assessment of model fitness via realizeddiscrepancies.
Statistica Sinica, 6:733?807.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
PNAS, 101(suppl.
1):5228?5235.Matthew Hoffman, David Blei, and Francis Bach.
2010.Online learning for latent dirichlet alocation.
In NIPS.Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.2008.
A joint topic and perspective model for ideo-logical discourse.
In PKDD.Qiaozhu Mei and ChengXiang Zhai.
2006.
A mixturemodel for contextual text mining.
In KDD.David Mimno and Andrew McCallum.
2007.
Organizingthe OCA: learning faceted subjects from a library ofdigital books.
In JCDL.David Newman, Jey Han Lau, Karl Grieser, and TimothyBaldwin.
2010.
Automatic evaluation of topic coher-ence.
In Human Language Technologies: The AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics.Michael J. Paul, ChengXiang Zhai, and Roxana Girju.2010.
Summarizing contrastive viewpoints in opin-ionated text.
In EMNLP.Donald B. Rubin.
1981.
Estimation in parallel random-ized experiments.
Journal of Educational Statistics,6:377?401.D.
Rubin.
1984.
Bayesianly justifiable and relevant fre-quency calculations for the applied statistician.
TheAnnals of Statistics, 12(4):1151?1172.Hanna Wallach, Iain Murray, Ruslan Salakhutdinov, andDavid Mimno.
2009.
Evaluation methods for topicmodels.
In ICML.237
