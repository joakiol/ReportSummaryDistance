An Analogical Parser for Restricted DomainsDonald HindleAT&T Bell Labs600 Mountain Ave.Murray Hill, NJ 07974ABSTRACTThis note describes the current development of an approachto parsing designed to overcome some of the problems of ex-isting parsers, particularly with respect to their utility a~language models.
The parser combines lexical and grammat-ical constraints into a uniform grammatical representation, isreadily trainable (since the parser output is indistinguishablefrom the grammar input), and uses analogy to guess aboutthe likelihood of constructions outside the grammar.1.
THE PROBLEM WITH PARSERSA parser is a device that provides a description of thesyntactic phrases that make up a sentence.
For a speechunderstanding task such as ATIS, the parser has tworoles.
First, it should provide a description of the phrasesin a sentence so these phrases can be interpreted by asubsequent semantic processor.
The second function isto provide a language model - a model of the likelihoodof a sentence - to constrain the speech recognition task.It is unfortunately the case that existing parsers devel-oped for text fulfill neither of these roles very well.It is useful to begin by reviewing some of the reasonsfor this failure.
We can describe the situation in termsof three general problems that parsers face: the Lexi-cality Problem, the Tail Problem, and the InterpolationProblem.The Lexicality ProblemThe most familiar way to think of a parser is as a de-vice that provides a description of a sentence given somegrammar.
Consider for example a context free gram-mar, where nonterminal categories are rewritten as ter-minals or nonterminals, and terminals are rewritten aswords.
There typically is no way to express the con-straints among individual words.Yet it is clear that much of our knowledge of language hasto do with what words go together.\[2\] Merely knowingthe grammatical rules of the language is not enough topredict which words can go together.
So for example,general English grammatical rules admit premodificationof a noun by another noun or by an adjective.
It ispossible to describe broad semantic onstraints on suchmodification; so for example, early morning is a case ofa time-adjective modifying a time-period, and morningflight is a time-period modifying an event.
Already weare have an explosion of categories in the grammar, sincewe are talking not about nouns and adjectives, but abouta fairly detailed subclassification of semantic types ofnouns and adjectives.But the problem is worse than this.
As Table 1 shows,even this rough characterization f semantic onstraintson modification is insufficient, since the adjective-nouncombination early night does not occur.
This depen-dency of syntactic combinability on particular lexicMitems is repeated across the grammar and lexicon.The lexicality problem has two aspects.
One is represent-ing the information and the other is acquiring it.
Therehas recently been increasing work on both aspects of theproblem.
The approach described in this paper is butone of many possible approaches, designed with an em-phasis on facilitating efficient parsing.The Tail ProblemMost combinations of words never occur in a corpus,but many of these combinations are possible, but simplyhave not been observed yet.
For a grammar (lexicalizedor not) the problem presented by this tail of rare eventsis unavoidable.
The grammar will always undercoverthe language.
The solution to the tail problem involvestraining from text.The Interpolation ProblemWhile it is always useful to push a grammar out the tail,it is inevitable that a grammar will not cover everythingencounted, and that a parser will have to deal with un-foreseen constructions.
This is of course the typical prob-lem in language modeling, and it raises the problem ofestimating the probabilities of structures that have notbeen seen - the Interpolation Problem.
The rules of thegrammar must be extendible to new constructions.
Inthis parser the approach is through analogy, or memory-based reasoning.\[8\]150_flight(s)morning 143afternoon 146evening 51night 27easy_  N33 59725 50412 2150 121Table 1: Pre- and post- modifiers for time nominals in a266k word ATIS sample.2.
THE PARSERThe basic parser data structure is a pointer to a node,the parser's focus of attention.
The basic operation is tocombine the focus of attention with either the precedingor following element to create a new node, updating pre-ceding and following pointers and updating the focus ofattention and then repeat.
If no combination is possible,the focus is moved forward, and thus parsing proceedsfrom the beginning of the sentence to the end.Some consequences of this model: no non-contiguous de-pendencies are picked up by this stage of the parser.
Theidea is that the parser is a reactive component.
Its out-put is not the complete analysis of a sentence, but ratherconsists of a set of fragments that subsequent processingwill glue together.
(cf.
\[1, 3, 4\]).2 .1 .
The  GrammarTrees in the grammar are either terminal or non-terminal.
Terminal trees are a pair of a syntactic featurespecification and a word.
Non-terminals are a pair oftrees, with a specification of which tree is head - thus,this is a binary dependency grammar.t ~ termina l  I (1 t t) I (2 t t)terminal  ~ ( features word)The category of a non-terminal is the category of itshead.The grammar for the parser is expressed as a set of treesthat have lexically specified terminals, each with a fre-quency count.
For example, in the ATIS grammar, thetree corresponding to the phrase book a flight is(1 (V "book") (2 (XI "a")(N "flight")))It occurs 6 times.
The grammar consists of a large set ofsuch partial trees, which encode both the grammaticaland the lexical constraints of the language.Following are examples of two trees that might be in thegrammar for the parser.
(V 1(V 1(V 0 aIVZ)(XPII_O 0 NE))(N 1(N 2(Xl o A)(N 0 LIST))(P 1(P o OF)(N2 2(XQ 0 ALL)(N2 0 AIRFARES)))))(P 1(P 0 FOR)(~2 2(N 0 ROUND-TRIP)(N2 0 TICKETS)))2.2 .
Pars ingThe basic parser operation is to combine subtrees bymatching existing trees in the grammar.
Consider, forexample, parsing the fragment give me a list.Initially, the parser focuses on the first word in the sen-tence, and tries to combine it with preceding and follow-ing nodes.
Since give exists in the grammar as head of atree with me as second element, the match is straightfor-ward, and the node give me is built, directly copying thegrammar.
Nothing in the grammar leads to combininggive me and a, so the parser attention moves forward,and a list is built, again, directly from the grammar.At this point, the parser will is looking at the fragmentsgive me (with head give) and a list (with head list), andis faced again with the question: can these pieces becombined.
Here the answer is not so obvious.2.3.
Smooth ing  by  ana logy .If we could guarantee that all trees that the parser mustconstruct will exist in its grammar of trees, then theparsing procedure would be as described in the preced-ing section.
Of course, we don't predict in advance alltrees the parser might see.
Rather, the parser has agrammar epresenting a subset of the trees it might seealong with a measure of similarity between trees.
Whenthe parser finds no exact way to combine two nodes tomatch a tree that exists in the grammar, it looks for sim-ilar trees that combine.
In particular, it looks at eachof the two potential combined nodes in turn and tries tofind a similar tree that does combine with the observedtree.So in our example, although give me a list does not occur,151give me occurs with a number of similar trees, including:a list of ground transportationa list of the cities servea list of flights from philadelphiaa list of all the flightsa list of all flightsa list of all aircraft typeOne of these trees is selected to be the analog of a list,thus allowing give me to be combined as head with a list.The parser uses a heuristically defined measure of sim-ilarity that depends on: category, root, type , specifier,and distribution.
Obviously, much depends on the sim-ilarity metric used.
The aim here is to combine ourknowledge of language, to determine what in general con-tributes to the similarity of words, with patterns trainedfrom the text.
The details of the current similarity met-ric are largely arbitrary, and ways of training it are beinginvestigated.Notice that this approach finds the closest exemplar, notaverage of behavior.
(cf.
\[7, 81)2 .4 .
D isambiguat ionFor words which are ambiguous among more than onepossible terminal (e.g.
to can be a preposition or aninfinitival marker), the parser must assign a terminaltree.
In this parser, the disambiguation process is part ofthe parsing process.
That is, when the parser is focusingon the word to it selects the tree which best combinesto with a neighboring node.
If that tree has to as, forexample, head of a prepositional phrase, then to is apreposition, and similarly if to is an infinitival marker.Of course, if a word is not attached to any other con-stituent in the course of parsing, this method will notapply.
Disambiguation is still necessary, to allow subse-quent processing.
In such cases, the parser reverts to itsbigram model to make the best guess about the propertree for a word.3.
DEVELOPING A GRAMMARDeveloping a grammar for this parser means collectinga set of trees.
There are 4 distinct sources of grammartrees.Genera l  Engl ish.
The base set of trees for the parseris a set of general trees for the language as a whole, inde-pendent of the domain.
These include standard sentencepatterns as well as trees for the regular expressions oftime, place, quantity, etc.
For the current parser, thesetrees were written by hand (though in this set will overtime be developed partly by hand and partly from text).This set of trees is independent of the domain, and avail-able for any application.
It forms part of a general modelfor English.The remaining three parts of the tree database are allspecific to the particular estricted omain.Domain  Database  Specific.
Trees specific to the sub-domain, derived semi-automatically from the underlyingdatabase.
Included are airline names, flight names andcodes, aircraft names, etc.
This can also include a setof typical sentences for the domain.
In a sense, this setof trees provides information about the content of themessages in the domain, the things one is likely to talkabout.Parsed  Tra in ing Sentences.
hand parsed text fromthe training sentences.
These trees are fairly easy toproduce through an incremental process of: a) parse aset of sentences, b) hand correct them, c) remake theparser, and d) repeat.
About a thousand words an hourcan be analyzed this way.
(Thus for the ATIS task, it iseasy to hand parse the entire training set, though thiswas not done for the experiment reported here.
)Unsuperv ised  Parsed Text.
also from the trainingsentences, but parsed by the existing parser and left un-corrected.
(Note: given an existing database of parsedsentences, these could transformed into trees for theparser grammar.
)Obviously, one aim of this design is to make acquisi-tion of the grammar easy.
Indeed, the parser design isnot English-specific, and in fact a Spanish version of theparser (under an earlier but related design) is currentlybeing updated.4.
THE AT IS  EXPERIMENTFor The ATIS task, a vocabulary was defined consist-ing of 1842 distinct erminal symbols (a superset of theFebruary 91 vocabulary, enhanced by adding words toregularize the grammar, and by distinguishing wordswith features; e.g.
"travel" as a verb is a different ermi-nal from "travel" as a noun).
A grammar was derived,based on 1) a relatively small general English model in-cluding trees for general sentence structure as well astrees for dates, times, numbers, money, and cities, and2) an ATIS specific set of trees covering types of objectsin the database (aircraft, airports, airlines, flight info,ground transportation) and 3) sentences in the trainingset.
In this experiment, approximately 10% of the gram-mar are language general, 10~ are database specific, 50%are supervised parsed trees and 30~ are unsupervised.152The weighting of the various sources of grammar treeshas not arisen here - all trees are weighted equally.
Butin the general case, where there is a pre-existing largegeneral grammar, and a large corpus for unsupervisedtraining, the weighting of grammar trees will become anissue.Given this grammar consisting of 14,000 trees, derivedas described above, the grammar perplexity is 15.9 onthe 138 February 91 test sentences.
This compares to aperplexity of 18.9 for the bigram model (where bigramsare terminals).
The grammar trees derived from the un-supervised parsing of the training sentences improve themodel slightly (from 16.4 to 15.9 perplexity).5.
SENTENCE PROBABIL ITYThe parse of a sentence consists of a sequence of N nodes.By convention, the first and last nodes in the sequence(nl and nN) are instances of the distinguished sentenceboundary node.
If all the words in a sentence are incor-porated by the parser under a single root node, then theoutput will consist of a sequence of three nodes, of whichthe middle one covers the words of the sentence.
But re-member, the parser may emit a sequence of fragments;in the limiting case, the parser will emit one node foreach word.5.1.
The tree grammarThe tree grammar, consists of a set of tree specifications.For each tree ti, the specification records:the  shape  o f t i  -for terminals - the root and categoryfor non-terminals - whether the head ison the left or right what the left andright subtrees are.eount( t i )  - number of times that tl appearsleft_count (ti) - number of times ti appears on the leftin a larger treer ight_count( t i )  -number of times ti appears on theright in a larger treelsubs_for(t l ,  t~) - for tree tj in which ti is the left sub-tree, sum of count(tk) where tk could realizeti in tjrsubs._for(ti,tj) - for tree tj in which ti is the rightsubtree, sum of count(re) where tk could re-alize ti in tjlsubs(t l )  - sum of count of trees tj such that ti couldrealize the left subtree of tj5.2.
probabi l i ty calculationIn the following, rd, ld, re, and lc mean right daughter,left daughter, right corner and left corner respectively.The probability of a sentence s consisting of a sequenceof n nodes (starting with the sentence boundary node,which we call nl)  is:.N-1Pr ( , )  =i=1Pr( bigram( re( n, ), Ic( ni+ l ) ) )?
Pr(not_attached(ni))?
P r (n i+ l  Ire(hi+l))In this formula, the bigram probabilities are calculatedon the terminals (word plus grammatical features), in-terpolating using feature similarity.Pr(not_attaehed(ni)) means the probability that ni isnot attached as the ld of any node.
It is estimated fromcount(n) and left_count(n).Pr(ni+l \[ le(ni+l)), the probability of a node given thatwe have seen its left corner, is derived recursively:Pr(n I lc(n)) = 1.0, if n is a terminal node, since the lcof a terminal node is the node itself; otherwise,, Pr(n lie(n)) = Pr(ld(n) l le(ld(n)))?
1 .0  - Pr(not_attaehed(ld(n)))?
Pr(tree(n) lld(n))?
Pr(rd(n) ltree(n), td(n))In this formula, the first term is the recursion, whichdescends the left edge of the node to the left corner.At each step in the descent, the second term in the for-mula takes account of the probability that the left daugh-ter will be attached to something.The third term is the probability that the tree tree(n)will be the parent given that node le(n) is the left daugh-ter of a node.The fourth term is the probability that node rd(n) willbe the right daughter given that ld(n) is the left daughterand tree(n) is the parent tree corresponding to node n.p robab i l i ty  o f  tree(n) given ld(n) To find thePr(tree(n)\[ ld(n)),  we consider the two cases, depend-153ing on whether there is a substitution for the left_tree ofn:Case: no  le f t_subst l tut ion .
If the left_tree(tree(n))is equal to the tree(ld(n)) (i.e.
if there is no substitu-tion), thenPr(tree(n) l ld(n)) =(1.0 - prob_left_substitution(id(n)))Pr(tree(n) I ld(n), no.left_substitution)The prob_left_substitution(ld(n)) is the probability thatgiven the node ld(n) whose tree is tt, that node will bethe left daughter in a node whose left_tree is is not thesame as tt.
That is, tt will realize the left_tree(n).
Weestimate this probability on the basis of the count(t 0and the left_count(tt).When there is no left_substitution, the probability of theparent tree is estimated irectly from the counts of thetrees that tree(id(n)) can be left_tree of:Pr(tree(n) I Id(n), no_left_substitution) =eount( tree( n ) ) /le ft_count( tree( ld( n ) ) )Case: le f t_subst i tu t ion .
If there is a substitution,thenPr(tree(n) l ld(n)) =prob_le ft_substitution( ld(n ) )Pr(tree(n) I tree(td(n) ), left_substitution)To estimate the Pr(tree(n) \] tree(id(n))) in case2 (where we know there is a substitution for theleft_ptree(n), we reason as follows.
For each tree txs,l,that might substitute for tree(ld(n)), it will substituteonly if tXlelt is observed as a left member of a treethat tree(leftdaughter(n)) is not observed with, and fortxright, tXleyt is the best substitution.
The total of suchtrees is called lsubs(t).By this account,Pr(tree(n) \] tree(id(n) , left_substitution) =eount( tree( n ) ) / lsubs( tree( ld( n ) ) ).The probability of the right daughter, given the leftdaughter and the tree similarly takes into account theprobabilities of substitution.6.
FURTHER WORKWhile current results for this parsing model ook promis-ing, there are several directions of further exploration.In tegrat ion  in Speech  Recogn i t ion .
There are twoobvious ways of incorporating this parser into the speechrecognition task.
First, it can be used to select amonga set of candidate sentences proposed by a recognizer.The second, more interesting, approach is to embed theparser in the recognition process.
Given the parser's lo-calization of information and its deterministic beginning-to-end processing, it can naturally be used to find a lo-cally (where the domain of locality is adjacent rees) op-timal path through an (appropriately sparse) lattice.Deve lopment  o f  Fur ther  P rocess ing .
This parserrests on the assumption, shared in a variety of recentwork from quite different perspectives \[1, 3, 4\], that alevel of underspeeified syntactic description is efficientlyobtainable and is useful.
The current work supports aparticular view of what partial syntactic descriptions areobtainable.
It remains to show that the further process-ing components can be constructed to make these piecesuseful.Imp lementat ion  Detai ls .
A number of decisions inthe implementation of the current parser are arbitrary,and further development demands exploring the opti-mal design.
For example, we need to explore what thesimilarity function should look like, and what functionshould be used for comparing potential attachments.7.
REFERENCES1.
Abney, Steven P. Rapid incremental parsing with repair.Paper presented at Waterloo conference on ElectronicText Research.2.
Church, Kenneth W., William A. Gale, Patrick Hanks,and Donald Hindle.
(to appear).
Using statistics in lex-ical analysis, in Zernik (ed.)
Lexical acquisition: usingon-line resources to build a lexicon.3.
Jacobs, Paul.
1990.
To parse or not to parse: relation-driven text skimming.
In COLING 90, 194-198, Helsinki,Finland.4.
Marcus, Mitchell P. and Donald Hindle.
1990.
Descrip-tion Theory and Intonation Boundaries.
In Gerald Alt-mann (ed.
), Computational and Cognitive Models ofSpeech.
MIT Press.5.
Sadler, Victor.
1989.
Working with analogicalsemantics.Foris: Dordrecht.6.
Parsing strategies with 'lexicalized' grammars: applica-tion to tree adjoining grammars.
In Proceedings fo the12th International Conference on Computational Lin-guistics, COLING88, Budapest, Hungary.7.
Skousen, Royal.
1989.
Analogical modeling of language.Kluwer:Dordrecht.8.
Stanfill, Craig and David Waltz.
1986.
Toward memory-based reasoning.
Communications of the ACM 29.12.154
