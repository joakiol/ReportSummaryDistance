Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 255?265,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLearning Abstract Concept Embeddings from Multi-Modal Data:Since You Probably Can?t See What I MeanFelix HillComputer LaboratoryUniversity of Cambridgefelix.hill@cl.cam.ac.ukAnna KorhonenComputer LaboratoryUniversity of Cambridgeanna.korhonen@cl.cam.ac.ukAbstractModels that acquire semantic represen-tations from both linguistic and percep-tual input are of interest to researchersin NLP because of the obvious parallelswith human language learning.
Perfor-mance advantages of the multi-modal ap-proach over language-only models havebeen clearly established when models arerequired to learn concrete noun concepts.However, such concepts are comparativelyrare in everyday language.
In this work,we present a new means of extendingthe scope of multi-modal models to morecommonly-occurring abstract lexical con-cepts via an approach that learns multi-modal embeddings.
Our architecture out-performs previous approaches in combin-ing input from distinct modalities, andpropagates perceptual information on con-crete concepts to abstract concepts moreeffectively than alternatives.
We discussthe implications of our results both for op-timizing the performance of multi-modalmodels and for theories of abstract con-ceptual representation.1 IntroductionMulti-modal models that learn semantic represen-tations from both language and information aboutthe perceptible properties of concepts were orig-inally motivated by parallels with human wordlearning (Andrews et al., 2009) and evidence thatmany concepts are grounded in perception (Barsa-lou and Wiemer-Hastings, 2005).
The perceptualinformation in such models is generally mined di-rectly from images (Feng and Lapata, 2010; Bruniet al., 2012) or from data collected in psychologi-cal studies (Silberer and Lapata, 2012; Roller andSchulte im Walde, 2013).By exploiting the additional information en-coded in perceptual input, multi-modal modelscan outperform language-only models on a rangeof semantic NLP tasks, including modelling sim-ilarity (Bruni et al., 2014; Kiela et al., 2014) andfree association (Silberer and Lapata, 2012), pre-dicting compositionality (Roller and Schulte imWalde, 2013) and concept categorization (Silbererand Lapata, 2014).
However, to date, these pre-vious approaches to multi-modal concept learningfocus on concrete words such as cat or dog, ratherthan abstract concepts, such as curiosity or loyalty.However, differences between abstract and con-crete processing and representation (Paivio, 1991;Hill et al., 2013; Kiela et al., 2014) suggest thatconclusions about concrete concept learning maynot necessarily hold in the general case.
In this pa-per, we therefore focus on multi-modal models forlearning both abstract and concrete concepts.Although concrete concepts might seem morebasic or fundamental, the vast majority of open-class, meaning-bearing words in everyday lan-guage are in fact abstract.
72% of the noun orverb tokens in the British National Corpus (Leechet al., 1994) are rated by human judges1as moreabstract than the noun war, for instance, a con-cept many would already consider to be quiteabstract.
Moreover, abstract concepts by defi-nition encode higher-level (more general) princi-ples than concrete concepts, which typically re-side naturally in a single semantic category or do-main (Crutch and Warrington, 2005).
It is there-fore likely that abstract representations may provehighly applicable for multi-task, multi-domain ortransfer learning models, which aim to acquire?general-purpose?
conceptual knowledge withoutreference to a specific objective or task (Collobertand Weston, 2008; Mesnil et al., 2012).In a recent paper, Hill et al.
(2014) investigatewhether the multi-modal models cited above are1Contributors to the USF dataset (Nelson et al., 2004).255effective for learning concepts other than concretenouns.
They observe that representations of cer-tain abstract concepts can indeed be enhanced inmulti-modal models by combining perceptual andlinguistic input with an information propagationstep.
Hill et al.
(2014) propose ridge regression asan alternative to the nearest-neighbour averagingproposed by Johns and Jones (2012) for such prop-agation, and show that it is more robust to changesin the type of concept to be learned.
However, bothmethods are somewhat inelegant, in that they learnseparate linguistic and ?pseudo-perceptual?
repre-sentations, which must be combined via a separateinformation combination step.
Moreover, for themajority of abstract concepts, the best performingmulti-modal model employing these techniquesremains less effective than conventional text-onlyrepresentation learning model.Motivated by these observations, we introducean architecture for learning both abstract and con-crete representations that generalizes the skipgrammodel of Mikolov et al.
(2013) from text-based tomulti-modal learning.
Aspects of the model de-sign are influenced by considering the process ofhuman language learning.
The model moderatesthe training input to include more perceptual infor-mation about commonly-occurring concrete con-cepts and less information about rarer concepts.Moreover, it integrates the processes of combin-ing perceptual and linguistic input and propagat-ing information from concrete to abstract conceptsinto a single representation update process basedon back-propagation.We train our model on running-text languageand two sources of perceptual descriptors for con-crete nouns: the ESPGame dataset of annotatedimages (Von Ahn and Dabbish, 2004) and theCSLB set of concept property norms (Devereuxet al., 2013).
We find that our model combines in-formation from the different modalities more ef-fectively than previous methods, resulting in animproved ability to model the USF free associa-tion gold standard (Nelson et al., 2004) for con-crete nouns.
In addition, the architecture propa-gates the extra-linguistic input for concrete nounsto improve representations of abstract conceptsmore effectively than alternative methods.
Whilethis propagation can effectively extend the advan-tage of the multi-modal approach to many moreconcepts than simple concrete nouns, we observethat the benefit of adding perceptual input appearsto decrease as target concepts become more ab-stract.
Indeed, for the most abstract concepts ofall, language-only models still provide the mosteffective learning mechanism.Finally, we investigate the optimum quantityand type of perceptual input for such models.
Be-tween the most concrete concepts, which can beeffectively represented directly in the perceptualmodality, and the most abstract concepts, whichcannot, we identify a set of concepts that cannotbe represented effectively directly in the percep-tual modality, but still benefit from perceptual in-put propagated in the model via concrete concepts.The motivation in designing our model and ex-periments is both practical and theoretical.
Takentogether, the empirical observations we present arepotentially important for optimizing the learningof representations of concrete and abstract con-cepts in multi-modal models.
In addition, they of-fer a degree of insight into the poorly understoodissue of how abstract concepts may be encoded inhuman memory.2 Model DesignBefore describing how our multi-modal architec-ture encodes and integrates perceptual informa-tion, we first describe the underlying corpus-basedrepresentation learning model.Language-only Model Our multi-modal archi-tecture builds on the continuous log-linear skip-gram language model proposed by Mikolov etal.
(2013).
This model learns lexical representa-tions in a similar way to neural-probabilistic lan-guage models (NPLM) but without a non-linearhidden layer, a simplification that facilitates theefficient learning of large vocabularies of denserepresentations, generally referred to as embed-dings (Turian et al., 2010).
Embeddings learnedby the model achieve state-of-the-art performanceon several evaluations including sentence comple-tion and analogy modelling (Mikolov et al., 2013).For each word type w in the vocabulary V , themodel learns both a ?target-embedding?
rw?
Rdand a ?context-embedding?
r?w?
Rdsuch that,given a target word, its ability to predict nearbycontext words is maximized.
The probability ofseeing context word c given target w is defined as:p(c|w) =er?c?rw?v?Ver?v?rw256wnTarget RepresentationScore: p(c|w)Context Representations  Information Sourcewn+2pwn+2wn+1pwn+1wn-1pwn-1wn-2pwn-2LinguisticText8 CorpusPerceptualPESPPCSBLFigure 1: Our multi-modal model architecture.
Light boxes are elements of the original Mikolov etal.
(2013) model.
For target words wnin the domain of P (concrete concepts), the model updates itsrepresentations based on corpus context wordswn?i, then on words pwn?iin perceptual pseudo-sentences.For wnnot in the domain of P (abstract concepts), updates are based solely on the wn?i.The model learns from a set of target-word,context-word pairs, extracted from a corpus ofsentences as follows.
In a given sentence S (oflength N ), for each position n ?
N , each wordwnis treated in turn as a target word.
An inte-ger t(n) is then sampled from a uniform distribu-tion on {1, .
.
.
k}, where k > 0 is a predefinedmaximum context-window parameter.
The pair to-kens {(wn, wn+j) : ?t(n) ?
j ?
t(n), wi?
S}are then appended to the training data.
Thus, tar-get/context training pairs are such that (i) onlywords within a k-window of the target are selectedas context words for that target, and (ii) wordscloser to the target are more likely to be selectedthan those further away.The training objective is then to maximize thesum of the log probabilities T across of all suchexamples from S and across all sentences in thecorpus, where T is defined as follows:T =1NN?n=1?
?t(n)?j?t(n),j 6=0log(p(wn+j|wn))The model free parameters (target-embeddingsand context-embeddings of dimension d for eachword in the corpus with frequency above a certainthreshold f ) are updated according to stochasticgradient descent and backpropation, with learningrate controlled by Adagrad (Duchi et al., 2011).For efficiency, the output layer is encoded as ahierarchical softmax function based on a binaryHuffman tree (Morin and Bengio, 2005).As with other distributional architectures, themodel captures conceptual semantics by exploit-ing the fact that words appearing in similar lin-guistic contexts are likely to have similar mean-ings.
Informally, the model adjusts its embeddingsto increase the ?probability?
of seeing the languagein the training corpus.
Since this probability in-creases with the p(c|w), and the p(c|w) increasewith the dot product r?c?
rw, the updates have theeffect of moving each target-embedding incremen-tally ?closer?
to the context-embeddings of its col-locates.
In the target-embedding space, this resultsin embeddings of concept words that regularly oc-cur in similar contexts moving closer together.Multi-modal Extension We extend the Mikolovet al.
(2013) architecture via a simple means of in-troducing perceptual information that aligns withhuman language learning.
Based on the assump-tion that frequency in domain-general linguisticcorpora correlates with the likelihood of ?experi-encing?
a concept in the world (Bybee and Hop-per, 2001; Chater and Manning, 2006), perceptualinformation is introduced to the model wheneverdesignated concrete concepts are encountered inthe running-text linguistic input.
This has the ef-fect of introducing more perceptual input for com-monly experienced concrete concepts and less in-put for rarer concrete concepts.To implement this process, perceptual informa-tion is extracted from external sources and en-coded in an associative array P, which maps (typ-ically concrete) words w to bags of perceptual fea-tures b(w).
The construction of this array dependson the perceptual information source; the processfor our chosen sources is detailed in Section 2.1.Training our model begins as before on running-text.
When a sentence Smcontaining a word w inthe domain of P is encountered, the model finishestraining on Smand begins learning from a per-ceptual pseudo-sentence?Sm(w).
?Sm(w) is con-structed by alternating the token w with a fea-257?S(crocodile) = Crocodile legs crocodile teeth crocodileteeth crocodile scales crocodile green crocodile.
?S(screwdriver) = Screwdriver handle screwdriver flatscrewdriver long screwdriver handle screwdriver head.Figure 2: Example pseudo-sentences generated byour model.ture sampled at random from b(w) until?Sm(w)is the same length as Sm(see Figure 2).
Becausewe want the ensuing perceptual learning processto focus on how w relates to its perceptual prop-erties (rather than how those properties relate toeach other), we insert multiple instances of w into?Sm(w).
This ensures that the majority of train-ing cases derived from?Sm(w) are instances of (w,feature) rather than (feature, feature) pairs.
Oncetraining on?Sm(w) is complete, the model revertsto the next ?genuine?
(linguistic) sentence Sm+1,and the process continues.
Thus, when a concreteconcept is encountered in the corpus, its embed-ding is first updated based on language (moved in-crementally closer to concepts appearing in sim-ilar linguistic contexts), and then on perception(moved incrementally closer to concepts with thesame or similar perceptual features).For greater flexibility, we introduce a parameter?
reflecting the raw quantity of perceptual infor-mation relative to linguistic input.
When ?
= 2,two pseudo-sentences are generated and insertedfor every corpus occurrence of a token from thedomain of P. For non-integral ?, the number ofsentences inserted is b?c, and a further sentence isadded with probability ??
b?c.In all experiments reported in the following sec-tions we set the window size parameter k = 5 andthe minimum frequency parameter f = 3, whichguarantees that the model learns embeddings forall concepts in our evaluation sets.
While themodel learns both target and context-embeddingsfor each word in the vocabulary, we conduct ourexperiments with the target embeddings only.
Weset the dimension parameter d = 300 as this pro-duces high quality embeddings in the language-only case (Mikolov et al., 2013).2.1 Information SourcesWe construct the associative array of perceptualinformation P from two sources typical of thoseused for multi-modal semantic models.ESPGame Dataset The ESP-Game dataset(ESP) (Von Ahn and Dabbish, 2004) consists of100,000 images, each annotated with a list of lex-ical concepts that appear in that image.For any concept w identified in an ESP im-age, we construct a corresponding bag of featuresb(w).
For each ESP image I that contains w, weappend the other concept tokens identified in I tob(w).
Thus, the more frequently a concept co-occurs with w in images, the more its correspond-ing lexical token occurs in b(w).
The array PESPin this case then consists of the (w,b(w)) pairs.CSLB Property Norms The Centre for Speech,Language and the Brain norms (CSLB) (Devereuxet al., 2013) is a recently-released dataset contain-ing semantic properties for 638 concrete conceptsproduced by human annotators.
The CSLB datasetwas compiled in the same way as the McRae etal.
(2005) property norms used widely in multi-modal models (Silberer and Lapata, 2012; Rollerand Schulte im Walde, 2013); we use CSLB be-cause it contains more concepts.
For each concept,the proportion of the 30 annotators that produceda given feature can also be employed as a measureof the strength of that feature.When encoding the CSLB data in P, we firstmap properties to lexical forms (e.g.
is greenbecomes green).
By directly identifying percep-tual features and linguistic forms in this way,we treat features observed in the perceptual dataas (sub)concepts to be acquired via the samemulti-modal input streams and stored in the samedomain-general memory as the evaluation con-cepts.
This design decision in fact correspondsto a view of cognition that is sometimes disputed(Fodor, 1983).
In future studies we hope to com-pare the present approach to architectures withdomain-specific conceptual memories.For each concept w in CSLB, we then con-struct a feature bag b(w) by appending lexicalforms to b(w) such that the count of each fea-ture word is equal to the strength of that featurefor w. Thus, when features are sampled fromb(w) to create pseudo-sentences (as detailed pre-viously) the probability of a feature word occur-ring in a sentence reflects feature strength.
Thearray PCSLBthen consists of all (w,b(w)) pairs.Linguistic Input The linguistic input to allmodels is the 400m word Text8 Corpus2of2From http://mattmahoney.net/dc/textdata.html258ESPGame CSLBImage 1 Image 2 Crocodile Screwdriverred wreck has 4 legs (7) has handle (28)chihuaua cyan has tail (18) has head (5)eyes man has jaw (7) is long (9)little crash has scales (8) is plastic (18)ear accident has teeth (20) is metal (28)nose street is green (10)small is large (10)Table 1: Concepts identified in images in the ESPGame (left) and features produced for concepts byhuman annotators in the CSLB dataset (with fea-ture strength, max=30).Concept 1 Concept 2 Assoc.abdomen (6.83) stomach (6.04) 0.566throw (4.05) ball (6.08) 0.234hope (1.18) glory (3.53) 0.192egg (5.79) milk (6.66) 0.012Table 2: Example concept pairs (with mean con-creteness rating) and free-association scores fromthe USF dataset.Wikipedia text, split into sentences and with punc-tuation removed.2.2 EvaluationWe evaluate the quality of representations by howwell they reflect free association scores, an em-pirical measure of cognitive conceptual proxim-ity.
The University of South Florida Norms(USF) (Nelson et al., 2004) contain free associa-tion scores for over 40,000 concept pairs, and havebeen widely used in NLP to evaluate semantic rep-resentations (Andrews et al., 2009; Feng and La-pata, 2010; Silberer and Lapata, 2012; Roller andSchulte im Walde, 2013).
Each concept that weextract from the USF database has also been ratedfor conceptual concreteness on a Likert scale of1-7 by at least 10 human annotators.
Followingprevious studies (Huang et al., 2012; Silberer andLapata, 2012), we measure the (Spearman ?)
cor-relation between association scores and the cosinesimilarity of vector representations.We create separate abstract and concrete con-cept lists by ranking the USF concepts accord-ing to concreteness and sampling at random fromthe first and fourth quartiles respectively.
We alsointroduce a complementary noun/verb dichotomy,Concept Type List Pairs Examplesconcrete nouns 541 1418 yacht, cupabstract nouns 100 295 fear, respectall nouns 666 1815 fear, cupconcrete verbs 50 66 kiss, launchabstract verbs 50 127 differ, obeyall verbs 100 221 kiss, obeyTable 3: Details the subsets of USF data used inour evaluations, downloadable from our website.on the intuition that information propagation mayoccur differently from noun to noun or from nounto verb (because of their distinct structural rela-tionships in sentences).
POS-tags are not assignedas part of the USF data, so we draw the noun/verbdistinction based on the majority POS-tag of USFconcepts in the lemmatized British National Cor-pus (Leech et al., 1994).
The abstract/concreteand noun/verb dichotomies yield four distinct con-cept lists.
For consistency, the concrete noun listis filtered so that each concrete noun concept whas a perceptual representation b(w) in both PESPand PCSLB.
For the four resulting concept listsC (concrete/abstract, noun/verb), a correspond-ing set of evaluation pairs {(w1, w2) ?
USF :w1, w2?
C} is extracted (see Table 3 for details).3 Results and DiscussionOur experiments were designed to answer fourquestions, outlined in the following subsec-tions: (1) Which model architectures perform bestat combining information pertinent to multiplemodalities when such information exists explicitly(as common for concrete concepts)?
(2) Whichmodel architectures best propagate perceptual in-formation to concepts for which it does not existexplicitly (as is common for abstract concepts)?
(3) Is it preferable to include all of the perceptualinput that can be obtained from a given source, orto filter this input stream in some way?
(4) Howmuch perceptual vs. linguistic input is optimal forlearning various concept types?3.1 Combining information sourcesTo evaluate our approach as a method of in-formation combination we compared its perfor-mance on the concrete noun evaluation set againstthree alternative methods.
The first alternativeis simple concatenation of these perceptual vec-tors with linguistic vectors embeddings learned259by the Mikolov et al.
(2013) model on the Text8Corpus.
In the second alternative (proposedfor multi-modal models by Silberer and Lapata(2012)), canonical correlation analysis (CCA)(Hardoon et al., 2004) was applied to the vec-tors of both modalities.
CCA yields reduced-dimensionality representations that preserve un-derlying inter-modal correlations, which are thenconcatenated.
The final alternative, proposed byBruni et al.
(2014) involves applying SingularValue Decomposition (SVD) to the matrix of con-catenated multi-modal representations, yieldingsmoothed representations.3When implementing the concatenation, CCAand SVD methods, we first encoded the percep-tual input directly into sparse feature vectors, withcoordinates for each of the 2726 features in CSLBand for each of the 100,000 images in ESP.
Thissparse encoding matches the approach taken bySilberer and Lapata (2012), for CCA and concate-nation, and by Hill et al.
(2014) for the ridge re-gression method of propagation (see below).We compare these alternatives to our proposedmodel with ?
= 1.
In The CSLB and ESP models,all training pseudo-sentences are generated fromthe arrays PCSLBand PESPrespectively.
In themodels classed as CSLB&ESP, a random choicebetween PCSLBand PESPis made every timeperceptual input is included (so that the overallquantity of perceptual information is the same).As shown in Figure 2 (left side), the embed-dings learned by our model achieve a higher cor-relation with the USF data than simple concatena-tion, CCA and SVD regardless of perceptual inputsource.
With the optimal perceptual source (ESPonly), for instance, the correlation is 11% higherthat the next best alternative method, CCA.One possible factor behind this improvementis that, in our model, the learned representationsfully integrate the two modalities, whereas forboth CCA and the concatenation method each rep-resentation feature (whether of reduced dimensionor not) corresponds to a particular modality.
Thisdeeper integration may help our architecture toovercome the challenges inherent in informationcombination such as inter-modality differences ininformation content and representation sparsity.
Itis also important to note that Bruni et al.
(2014) ap-3CCA was implemented using the CCA package inR.
SVD was implemented using SVDLIBC (http://tedlab.mit.edu/?dr/SVDLIBC/), with truncationfactor k = 1024 as per (Bruni et al., 2014).plied their SVD method with comparatively denseperceptual representations extracted from images,whereas our dataset-based perceptual vectors weresparsely-encoded.3.2 Propagating input to abstract conceptsTo test the process of information propagation inour model, we evaluated the learned embeddingsof more abstract concepts.
We compared ourapproach with two recently-proposed alternativemethods for inferring perceptual features when ex-plicit perceptual information is unavailable.Johns and Jones In the method of Johns andJones (2012), pseudo-perceptual representationsfor target concepts without a perceptual repre-sentations (uni-modal concepts) are inferred as aweighted average of the perceptual representationsof concepts that do have such a representation (bi-modal concepts).In the first step of their two-step method, foreach uni-modal concept k, a quasi-perceptual rep-resentation is computed as an average of theperceptual representations of bi-modal concepts,weighted by the proximity between each of theseconcepts and kkp=?c?
?CS(kl, cl)??
cpwhere?C is the set of bi-modal concepts, cpand kpare the perceptual representations for c and k re-spectively, and cland klthe linguistic representa-tions.
The exponent parameter ?
reflects the learn-ing rate.In step two, the initial quasi-perceptual repre-sentations are inferred for a second time, but withthe weighted average calculated over the percep-tual or initial quasi-perceptual representations ofall other words, not just those that were originallybi-modal.
As with Johns and Jones (2012), we setthe learning rate parameter ?
to be 3 in the firststep and 13 in the second.Ridge Regression An alternative, proposed forthe present purpose by Hill et al.
(2014), uses ridgeregression (Myers, 1990).
Ridge regression is avariant of least squares regression in which a reg-ularization term is added to the training objectiveto favor solutions with certain properties.For bi-modal concepts of dimension np, we ap-ply ridge regression to learn nplinear functions260fi: Rnl?
R that map the linguistic represen-tations (of dimension nl) to a particular percep-tual feature i.
These functions are then appliedtogether to map the linguistic representations ofuni-modal concepts to full quasi-perceptual repre-sentations.Following Hill et al.
(2014), we take the Euclid-ian l2norm of the inferred parameter vector as theregularization term.
This ensures that the regres-sion favors lower coefficients and a smoother so-lution function, which should provide better gen-eralization performance than simple linear regres-sion.
The objective for learning the fiis then tominimize?aX ?
Yi?22+ ?a?22where a is the vector of regression coefficients, Xis a matrix of linguistic representations and Yiavector of the perceptual feature i for the set of bi-modal concepts.Comparisons We applied the Johns and Jonesmethod and ridge regression starting from linguis-tic embeddings acquired by the Mikolov et al.
(2013) model on the Text8 Corpus, and concate-nated the resulting pseudo-perceptual and linguis-tic representations.
As with the implementationof our model, the perceptual input for these alter-native models was limited to concrete nouns (i.e.concrete nouns were the only bi-modal conceptsin the models).Figure 3 (right side) shows the propagation per-formance of the three models.
While the corre-lations overall may seem somewhat low, this isa consequence of the difficulty of modelling theUSF data.
In fact, the performance of both thelanguage-only model and our multi-modal exten-sion across the concept types (from 0.18 to 0.36) isequal to or higher than previous models evaluatedon the same data (Feng and Lapata, 2010; Silbererand Lapata, 2012; Silberer et al., 2013).For learning representations of concrete verbs,our approach achieves a 69% increase in perfor-mance over the next best alternative.
The perfor-mance of the model on abstract verbs is marginallyinferior to Johns and Jones?
method.
Neverthe-less, the clear advantage for concrete verbs makesour model the best choice for learning represen-tations of verbs in general, as shown by perfor-mance on the set all verbs, which also includesmixed abstract-concrete pairs.Our model is also marginally inferior to alterna-tive approaches in learning representations of ab-stract nouns.
However, in this case, no methodimproves on the linguistic-only baseline.
It ispossible that perceptual information is simply soremoved from the core semantics of these con-cepts that they are best acquired via the linguis-tic medium alone, regardless of learning mecha-nism.
The moderately inferior performance of ourmethod in such cases is likely caused by its greaterinherent inter-modal dependence compared withmethods that simply concatenate uni-modal rep-resentations.
When the perceptual signal is oflow quality, this greater inter-modal dependenceallows the linguistic signal to be obscured.The trade-off, however, is generally higher-quality representations when the perceptual signalis stronger, exemplified by the fact that our pro-posed approach outperforms alternatives on pairsgenerated from both abstract and concrete nouns(all nouns).
Indeed, the low performance of theJohns and Jones method on all nouns is strik-ing given that: (a) It performs best on abstractnouns (?
= .282), and (b) For concrete nouns itreverts to simple concatenation, which also per-forms comparatively well (?
= .249).
The poorperformance of the Jobns and Jones method onall nouns must therefore derive its comparisonsof mixed abstract-concrete or concrete-abstractpairs.
This suggests that the pseudo-perceptualrepresentations inferred by this method for ab-stract concepts method may not be compatiblewith the directly-encoded perceptual representa-tions of concrete concepts, rendering the compar-ison computation between items of differing con-creteness inaccurate.3.3 Direct representation vs. propagationAlthough property norm datasets such as theCSLB data typically consist of perceptual fea-ture information for concrete nouns only, image-based datasets such as ESP do contain informa-tion on more abstract concepts, which was omit-ted from the previous experiments.
Indeed, im-age banks such as Google Images contain millionsof photographs portraying quite abstract concepts,such as love or war.
On the other hand, encod-ings or descriptions of abstract concepts are gen-erally more subjective and less reliable than thoseof concrete concepts (Wiemer-Hastings and Xu,2005).
We therefore investigated whether or notit is preferable to include this additional informa-tion as model input or to restrict perceptual input2610.2030.220.150.2390.259 0.271 0.2560.3010.249 0.24 0.2310.2960.00.10.20.30.4CSLB ESP CSLB & ESPConcrete nouns ?
information combinationCorrelationCombination MethodVector ConcatenationCCASVDOur Model (?=1)0.2820.265 0.250.070.2360.3640.060.1160.1970.177 0.172 0.175 0.167 0.1750.2250.00.10.20.30.4abstract nouns all nouns concrete verbs abstract verbs all verbsMore abstract concepts ?
information propagation (CSLB & ESPCorrelationPropagation MethodJohns and JonesRidge RegressionOur Model (?=1)Figure 3: The proposed approach compared with other methods of information combination (left) andpropagation.
Dashed lines indicate language-only model baseline.
For brevity we include both perceptualinput sources ESP and CSLB when comparing means of propagation; results with individual informationsources were similar.to concrete nouns as previously.Of our evaluation sets, it was possible to con-struct from ESP (and add to PESP) representa-tions for all of the concrete verbs, and for ap-proximately half of the abstract verbs and abstractnouns.
Figure 4 (top), shows the performance ofa our model trained on all available perceptual in-put versus the model in which the perceptual inputwas restricted to concrete nouns.The results reflect a clear manifestation of theabstract/concrete distinction.
Concrete verbs be-have similarly to concrete nouns, in that they canbe effectively represented directly from perceptualinformation sources.
The information encoded inthese representations is beneficial to the model andincreases performance.
In contrast, constructing?perceptual?
representations of abstract verbs andabstract nouns directly from perceptual informa-tion sources is clearly counter-productive (to theextent that performance also degrades on the com-bined sets all nouns and all verbs).
It appears inthese cases that the perceptual input acts to ob-scure or contradict the otherwise useful signal in-ferred from the corpus.As shown in the previous section, the inclusionof any form of perceptual input inhibits the learn-ing of abstract nouns.
However, this is not the casefor abstract verbs.
Our model learns higher qual-ity representations of abstract verbs if perceptualinput is restricted to concrete nouns than if no per-ceptual input is included at all and when percep-tual input is included for both concrete nouns andabstract verbs.
This supports the idea of a grad-ual scale of concreteness: The most concrete con-cepts can be effectively represented directly in theperceptual modality; somewhat more abstract con-cepts cannot be represented directly in the percep-tual modality, but have representations that are im-proved by propagating perceptual input from con-crete concepts via language; and the most abstractconcepts are best acquired via language alone.3.4 Source and quantity of perceptual inputFor different concept types, we tested the effect ofvarying the proportion of perceptual to linguisticinput (the parameter ?).
Perceptual input was re-stricted to concrete nouns as in Sections 3.1-3.2.As shown in Figure 4, performance on concretenouns improves (albeit to a decreasing degree) as?
increases.
When learning concrete noun rep-resentations, linguistic input is apparently redun-dant if perceptual input is of sufficient quality andquantity.
For the other concept types, in each casethere is an optimal value for ?
in the range .5?2,above which perceptual input obscures the linguis-tic signal and performance degrades.
The prox-imity of these optima to 1 suggests that for op-timal learning, when a concrete concept is experi-enced approximately equal weight should be givento available perceptual and linguistic information.4 ConclusionsMotivated by the notable prevalence of abstractconcepts in everyday language, and their likelyimportance to flexible, general-purpose represen-tation learning, we have investigated how abstractand concrete representations can be acquired bymulti-modal models.
In doing so, we presented asimple and easy-to-implement architecture for ac-quiring semantic representations of both types of2620.10.20.30 1 2 3 4 5?CorrelationConcrete Nouns0.10.20.30 1 2 3 4 5?Abstract Nouns0.10.20.30 1 2 3 4 5?Concrete Verbs0.10.20.30 1 2 3 4 5?Perceptual InputCSLBESPCSLB &ESPText?onlyAbstract Verbs0.267 0.2950.1360.2490.335 0.364 0.3370.1760.0870.166 0.2010.2250.00.10.20.30.4concretenounsabstractnounsall nouns concreteverbs abstract  verbs all verbsConcept TypeCorrelationPerceptual Information Source Direct representation PropagationOur Model ?
= 1Figure 4: Top: Comparing the strategy of directly representing abstract concepts from perceptual in-formation where available (yellow bars) vs. propagating via concrete concepts.
Bottom: The effect ofincreasing ?
on correlation with USF pairs (Spearman ?)
for each concept type.
Horizontal dashed linesindicate language-only model baseline.concept from linguistic and perceptual input.While neuro-probabilistic language modelshave been applied to the problem of multi-modalrepresentation learning previously (Srivastava andSalakhutdinov, 2012; Wu et al., 2013; Silberer andLapata, 2014) our model and experiments developthis work in several important ways.
First, we ad-dress the problem of learning abstract concepts.By isolating concepts of different concretenessand part-of-speech in our evaluation sets, and sep-arating the processes of information combinationand propagation, we demonstrate that the multi-modal approach is indeed effective for some, butperhaps not all, abstract concepts.
In addition, ourmodel introduces a clear parallel with human lan-guage learning.
Perceptual input is introduced pre-cisely when concrete concepts are ?experienced?by the model in the corpus text, much like a lan-guage learner experiencing concrete entities viasensory perception.Taken together, our findings indicate the utilityof distinguishing three concept types when learn-ing representations in the multi-modal setting.Type I Concepts that can be effectively repre-sented directly in the perceptual modality.
Forsuch concepts, generally concrete nouns or con-crete verbs, our proposed approach provides a sim-ple means of combining perceptual and linguisticinput.
The resulting multi-modal representationsare of higher quality than those learned via otherapproaches, resulting in a performance improve-ment of over 10% in modelling free association.Type II Concepts, including abstract verbs, thatcannot be effectively represented directly in theperceptual modality, but whose representationscan be improved by joint learning from linguis-tic input and perceptual information about relatedconcepts.
Our model can effectively propagateperceptual input (exploiting the relations inferredfrom the linguistic input) from Type I concepts toenhance the representations of Type II conceptsabove the language-only baseline.
Because of thefrequency of abstract concepts, such propagationextends the benefit of the multi-modal approach toa far wider range of language than models basedsolely in the concrete domain.Type III Concepts that are more effectivelylearned via language-only models than multi-modal models, such as abstract nouns.
Neither263our proposed approach nor alternative propagationmethods achieve an improvement in representa-tion quality for these concepts over the language-only baseline.
Of course, it is an empirical ques-tion whether a multi-modal approach could everenhance the representation learning of these con-cepts, one with potential implications for cognitivetheories of grounding (a topic of much debate inpsychology (Grafton, 2009; Barsalou, 2010)).Additionally, we investigated the optimum typeand quantity of perceptual input for learning con-cepts of different types.
We showed that too muchperceptual input can result in degraded represen-tations.
For concepts of type I and II, the op-timal quantity resulted from setting ?
= 1; i.e.whenever a concrete concept was encountered, themodel learned from an equal number of language-based and perception-based examples.
While wemake no formal claims here, such observationsmay ultimately provide insight into human lan-guage learning and semantic memory.In future we will address the question ofwhether Type III concepts can ever be enhancedvia multi-modal learning, and investigate multi-modal models that optimally learn concepts ofeach type.
This may involve filtering the percep-tual input stream for concepts according to con-creteness, and possibly more elaborate model ar-chitectures that facilitate distinct representationalframeworks for abstract and concrete concepts.AcknowledgementsThanks to the Royal Society and St John?s Collegefor supporting this research, and to Yoshua Bengioand Diarmuid?O S?eaghdha for helpful discussions.ReferencesMark Andrews, Gabriella Vigliocco, and David Vin-son.
2009.
Integrating experiential and distribu-tional data to learn semantic representations.
Psy-chological Review, 116(3):463.Lawrence W Barsalou and Katja Wiemer-Hastings.2005.
Situating abstract concepts.
Grounding Cog-nition: The Role of Perception and Action in Mem-ory, Language, and Thought, pages 129?163.Lawrence W Barsalou.
2010.
Grounded cognition:past, present, and future.
Topics in Cognitive Sci-ence, 2(4):716?724.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 136?145.
Asso-ciation for Computational Linguistics.Elia Bruni, Nam Khanh Tran, and Marco Baroni.
2014.Multimodal distributional semantics.
Journal of Ar-tificial Intelligence Research, 49:1?47.Joan L Bybee and Paul J Hopper.
2001.
Frequency andthe Emergence of Linguistic Structure, volume 45.John Benjamins Publishing.Nick Chater and Christopher D Manning.
2006.
Prob-abilistic models of language processing and acquisi-tion.
Trends in Cognitive Sciences, 10(7):335?344.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th International Conference onMachine Learning, pages 160?167.
ACM.Sebastian J Crutch and Elizabeth K Warrington.
2005.Abstract and concrete concepts have structurallydifferent representational frameworks.
Brain,128(3):615?627.Barry J Devereux, Lorraine K Tyler, Jeroen Geertzen,and Billi Randall.
2013.
The centre for speech, lan-guage and the brain (cslb) concept property norms.Behavior Research Methods, pages 1?9.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 91?99.
Asso-ciation for Computational Linguistics.Jerry A Fodor.
1983.
The modularity of mind: Anessay on faculty psychology.
MIT press.Scott T Grafton.
2009.
Embodied cognition and thesimulation of action to understand others.
Annals ofthe New York Academy of Sciences, 1156(1):97?117.David R Hardoon, Sandor Szedmak, and John Shawe-Taylor.
2004.
Canonical correlation analysis:An overview with application to learning methods.Neural Computation, 16(12):2639?2664.Felix Hill, Anna Korhonen, and Christian Bentz.2013.
A quantitative empirical analysis of the ab-stract/concrete distinction.
Cognitive Science.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.Multi-modal models for abstract and concrete con-cept semantics.
Transactions of the Association forComputational Linguistics.264Eric H Huang, Richard Socher, Christopher D Man-ning, and Andrew Y Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 873?882.
Asso-ciation for Computational Linguistics.Brendan T Johns and Michael N Jones.
2012.
Per-ceptual inference through global lexical similarity.Topics in Cognitive Science, 4(1):103?120.Douwe Kiela, Felix Hill, Anna Korhonen, and StephenClark.
2014.
Improving multi-modal representa-tions using image dispersion: Why less is sometimesmore.
In Proceedings of the annual meeting of theAssociation for Computational Linguistics.
ACL.Geoffrey Leech, Roger Garside, and Michael Bryant.1994.
Claws4: the tagging of the British NationalCorpus.
In Proceedings of the 15th conferenceon Computational linguistics-Volume 1, pages 622?628.
Association for Computational Linguistics.Ken McRae, George S Cree, Mark S Seidenberg, andChris McNorgan.
2005.
Semantic feature pro-duction norms for a large set of living and nonliv-ing things.
Behavior Research Methods, 37(4):547?559.Gr?egoire Mesnil, Yann Dauphin, Xavier Glorot, SalahRifai, Yoshua Bengio, Ian J Goodfellow, ErickLavoie, Xavier Muller, Guillaume Desjardins, DavidWarde-Farley, et al.
2012.
Unsupervised and trans-fer learning challenge: a deep learning approach.Journal of Machine Learning Research-ProceedingsTrack, 27:97?110.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word repre-sentations in vector space.
In Proceedings of Inter-national Conference of Learning Representations,Scottsdale, Arizona, USA.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the international Workshop on Arti-ficial Intelligence and Statistics, pages 246?252.Raymond H Myers.
1990.
Classical and ModernRegression with Applications, volume 2.
DuxburyPress Belmont, CA.Douglas L Nelson, Cathy L McEvoy, and Thomas ASchreiber.
2004.
The University of South Floridafree association, rhyme, and word fragment norms.Behavior Research Methods, Instruments, & Com-puters, 36(3):402?407.Allan Paivio.
1991.
Dual coding theory: Retrospectand current status.
Canadian Journal of Psychology,45(3):255.Stephen Roller and Sabine Schulte im Walde.
2013.A multimodal LDA model integrating textual, cog-nitive and visual modalities.
In Proceedings of the2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1146?1157, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Carina Silberer and Mirella Lapata.
2012.
Groundedmodels of semantic representation.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 1423?1433.
As-sociation for Computational Linguistics.Carina Silberer and Mirella Lapata.
2014.
Learn-ing grounded meaning representations with autoen-coders.
In Proceedings of the annual meeting of theAssociation for Computational Linguistics.
Associ-ation for Computational Linguistics.Carina Silberer, Vittorio Ferrari, and Mirella Lapata.2013.
Models of semantic representation with vi-sual attributes.
In Proceedings of the 51th AnnualMeeting of the Association for Computational Lin-guistics, Sofia, Bulgaria, August.Nitish Srivastava and Ruslan Salakhutdinov.
2012.Multimodal learning with deep boltzmann ma-chines.
In NIPS, pages 2231?2239.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.Luis Von Ahn and Laura Dabbish.
2004.
Labelingimages with a computer game.
In Proceedings of theSIGCHI conference on human factors in computingsystems, pages 319?326.
ACM.Katja Wiemer-Hastings and Xu Xu.
2005.
Contentdifferences for abstract and concrete concepts.
Cog-nitive Science, 29(5):719?736.Pengcheng Wu, Steven CH Hoi, Hao Xia, Peilin Zhao,Dayong Wang, and Chunyan Miao.
2013.
On-line multimodal deep similarity learning with ap-plication to image retrieval.
In Proceedings of the21st ACM International Conference on Multimedia,pages 153?162.
ACM.265
