Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 9?16Manchester, August 2008A Supervised Algorithm for Verb Disambiguation into VerbNet ClassesOmri Abend1 Roi Reichart2 Ari Rappoport11Institute of Computer Science , 2ICNCHebrew University of Jerusalem{omria01|roiri|arir}@cs.huji.ac.ilAbstractVerbNet (VN) is a major large-scale En-glish verb lexicon.
Mapping verb instancesto their VN classes has been proven use-ful for several NLP tasks.
However, verbsare polysemous with respect to their VNclasses.
We introduce a novel supervisedlearning model for mapping verb instancesto VN classes, using rich syntactic featuresand class membership constraints.
Weevaluate the algorithm in both in-domainand corpus adaptation scenarios.
In bothcases, we use the manually tagged Sem-link WSJ corpus as training data.
For in-domain (testing on Semlink WSJ data), weachieve 95.9% accuracy, 35.1% error re-duction (ER) over a strong baseline.
Foradaptation, we test on the GENIA corpusand achieve 72.4% accuracy with 10.7%ER.
This is the first large-scale experimen-tation with automatic algorithms for thistask.1 IntroductionThe organization of verbs into classes whose mem-bers exhibit similar syntactic and semantic behav-ior has been discussed extensively in the linguisticsliterature (see e.g.
(Levin and Rappaport Hovav,2005; Levin, 1993)).
Such an organization helpsin avoiding lexicon representation redundancy andenables generalizations across similar verbs.
Itcan also be of great practical use, e.g.
in com-pensating NLP statistical models for data sparse-ness.
Indeed, Levin?s seminal work had motivatedc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.much research aimed at automatic discovery ofverb classes (see Section 2).VerbNet (VN) (Kipper et al, 2000; Kipper-Schuler, 2005) is a large scale, publicly availabledomain independent verb lexicon that builds onLevin classes and extends them with new verbs,new classes, and additional information such assemantic roles and selectional restrictions.
VNclasses were proven beneficial for Semantic RoleLabeling (SRL) (Swier and Stevenson, 2005), Se-mantic Parsing (Shi and Mihalcea, 2005) andbuilding conceptual graphs (Hensman and Dun-nion, 2004).
Levin-inspired classes have beenused in several NLP tasks, such as Machine Trans-lation (Dorr, 1997) and Document Classification(Klavans and Kan, 1998).Many applications that use VN need to map verbinstances onto their VN classes.
However, verbsare polysemous with respect to VN classes.
Sem-link (Loper et al, 2007) is a dataset that maps eachverb instance in the WSJ Penn Treebank to its VNclass.
The mapping has been created using a com-bination of automatic and manual methods.
Yi etal.
(2007) have used Semlink to improve SRL.In this paper we present the first large-scale ex-perimentation with a supervised machine learningclassification algorithm for disambiguating verbinstances to their VN classes.
We use rich syntacticfeatures extracted from a treebank-style parse tree,and utilize a learning algorithm capable of impos-ing class membership constraints, thus taking ad-vantage of the nature of our task.
We use Semlinkas the training set.We evaluate our algorithm in both in-domainand corpus adaptation scenarios.
In the former,we test on the WSJ (using Semlink again), ob-taining 95.9% accuracy with 35.1% error reduc-tion (ER) over a strong baseline (most frequent9class) when using a modern statistical parser.
Inthe corpus adaptation scenario, we disambiguateverbs in sentences taken from outside the train-ing domain.
Since the manual annotation of newcorpora is costly, and since VN is designed to bea domain independent resource, adaptation resultsare important to the usability in NLP in practice.We manually annotated 400 sentences from GE-NIA (Kim et al, 2003), a medical domain cor-pus1.
Testing on these, we achieved 72.4% ac-curacy with 10.7% ER.
Our adaptation scenariois complete in the sense that the parser we usewas also trained on a different corpus (WSJ).
Wealso report experiments done using gold-standard(manually created) parses.The most relevant previous works addressingverb instance class classification are (Lapata andBrew, 2004; Li and Brew, 2007; Girju et al, 2005).The former two do not use VerbNet and their ex-periments were narrower than ours, so we can-not compare to their results.
The latter mapped toVN, but used a preliminary highly restricted setupwhere most instances were monosemous.
Forcompleteness, we compared our method to theirs2,achieving similar results.We review related work in Section 2, and dis-cuss the task in Section 3.
Section 4 introduces themodel, Section 5 describes the experimental setup,and Section 6 presents our results.2 Related WorkVerbNet.
VN is a major electronic English verblexicon.
It is organized in a hierarchical struc-ture of classes and sub-classes, each sub-class in-heriting the full characterization of its super-class.VN is built on a refinement of the Levin classes,the intersective Levin classes (Dang et al, 1998),aimed at achieving more coherent classes both se-mantically and syntactically.
VN was also sub-stantially extended (Kipper et al, 2006) using theLevin classes extension proposed in (Korhonenand Briscoe, 2004).
VN today contains 3626 verblemmas (forms), organized in 237 main classeshaving 4991 verb types (we refer to a lemma withan ascribed class as a type).
Of the 3626 lem-mas, 912 are polysemous (i.e., appear in morethan a single class).
VN?s significant coverage ofthe English verb lexicon is demonstrated by the1Our annotations will be made available to the community.2Using the same sentences and instances, obtained fromthe authors.75.5% coverage of VN classes over PropBank3instances (Loper et al, 2007).
Each class con-tains rich semantic information, including seman-tic roles of the arguments augmented with se-lectional restrictions, and possible subcategoriza-tion frames consisting of a syntactic descriptionand semantic predicates with temporal informa-tion.
VN thematic roles are relatively coarse, vs.the situation-specific FrameNet role system or theverb-specific PropBank role system, enabling gen-eralizations across a wide semantic scope.
Swierand Stevenson (2005) and Yi et al (2007) used VNfor SRL.Verb type classification.
Quite a few workshave addressed the issue of verb type classificationand in particular classification to ?Levin inspired?classes (e.g., (Schulte im Walde, 2000; Merlo andStevenson, 2001)).
Such work is not comparableto ours, as it deals with verb type (sense) ratherthan verb token (instance) classification.Verb token classification.
Lapata and Brew(2004) dealt with classification to Levin classes ofpolysemous verbs.
They established a prior fromthe BNC in an unsupervised manner.
They alsoshowed that this prior helps in the training of anaive Bayes classifier employed to distinguish be-tween possible verb classes of a given verb in agiven frame (when the ambiguity is not solved byknowing the frame alone).
Li and Brew (2007) ex-tended this model by proposing a method to trainthe class disambiguator without using hand-taggeddata.
While these papers have good results, theirexperimental setup was rather narrow and usedonly at most 67 polysemous verbs (in 4 frames).VN includes 912 polysemous verbs, of which 695appeared in our in-domain experiments.Girju et al (2005) performed the only previouswork we are aware of that addresses the problem oftoken level verb disambiguation into VN classes.They treated the task as a supervised learning prob-lem, proposing features based on a POS tagger, aChunker and a named entity classifier.
In orderto create the data4, they used a mapping betweenPropbank rolesets and VN classes, and took the in-stances in WSJ sections 15-18,20,21 that were an-notated by Propbank and for which the roleset de-termines the VN class uniquely.
This resulted inmost instances being in fact monosemous.
Their3Propbank (Palmer et al, 2005) is a corpus annotation ofthe WSJ sections of the Penn Treebank with semantic roles ofeach verbal proposition.4Semlink was not available then.10experiment was conducted in a WSJ in-domainscenario, and in a much narrower scope than inthis paper.
They had 870 (39 polysemous) uniqueverb lemmas, compared to 2091 (695 polysemous)in our in-domain scenario.
They did not test theirmodel in an adaptation scenario.
The scope anddifficulty contrast between our setup and theirs aredemonstrated by the large differences in the num-ber of instances and in the percentage of polyse-mous instances: 972/12431 (7.8%) in theirs, com-pared to 49571/84749 (58.5%) in our in-domainscenario (training+test).
We compared our methodto theirs for completeness and achieved similar re-sults.Semlink.
The Semlink project (Yi et al, 2007;Loper et al, 2007) aims to create a mapping ofPropBank, FrameNet (Baker et al, 1998), Word-Net (henceforth WN) and VN to one another, thusallowing these resources to synergize.
In addition,the project includes the most extensive token map-ping of verbs to their VN classes available today.It covers all verbs in the WSJ sections of the PennTreebank within VN coverage (out of 113K verbinstances, 97K have lemmas present in VN).3 Nature of the TaskPolysemy is a major issue in NLP.
Verbs are not anexception, resulting in a single verb form (lemma)appearing in more than a single class.
This pol-ysemy is also present in the original Levin clas-sification, where polysemous classes account formore than 48% of the BNC verb instances (Lapataand Brew, 2004).Given a verb instance whose lemma is withinthe coverage of VN, given the sentence in whichit appears, given a parse tree of this sentence (seebelow), and given the VN resource, our task is toclassify the verb instance to its correct VN class.There are currently 237 possible classes5.
Eachverb has only a few possible classes (no more than10, but only about 2.5 on the average over the poly-semous verbs).
Depending on the application, theparse tree for the sentence may be either a goldstandard parse or a parse tree generated by a parser.We have experimented with both options.The task can be viewed in two complemen-tary ways: per-class and per-verb type.
The per-class perspective takes into consideration the small5We ignore sub-class distinctions.
This is justified since in98.2% of the in-coverage instances in Semlink, knowing theverb and its class suffices for knowing its exact sub-class.number of classes relative to the number of types6.A classifier may gather valuable information for allmembers of a certain VN class, without seeing allof its members in the training data.
From this per-spective the task resembles POS tagging.
In bothtasks there are many dozens (or more) of possiblelabels, while each word has only a small subset ofpossible labels.
Different words may receive thesame label.The per-verb perspective takes into consider-ation the special properties of every verb type.Even the best lexicons necessarily ignore certainidiosyncratic characteristics of the verb when as-signing it to a certain class.
If a verb appearsmany times in the corpus, it is possible to estimateits parameters to a reasonable reliability, and thusto use its specific distributional properties for dis-ambiguation.
Viewed in this manner, the task re-sembles a word sense disambiguation (WSD) task:each verb has a small distinct set of senses (types),and no two different verbs have the same sense.The similarity to WSD suggests that our taskmight be solved by WN sense disambiguation fol-lowed by a mapping from WN to VN.
However,good results are not to be expected, due to themedium quality of today?s WSD algorithms andbecause the mapping between WN and VN is bothincomplete and many-to-many7.
Even for a perfectWN WSD algorithm, the resulting WN synset maynot be mapped to VN at all or may be mapped ontomultiple VN classes.
We experimented with thismethod and obtained results below the MF base-line we used8.The above discussion does not rule out the pos-sibility of obtaining reasonable results through ap-plying a high quality WSD engine followed by aWN to VN mapping.
However, there are muchfewer VN classes than WN classes per verb.
Thismay result in the WSD engine learning many dis-tinctions that are not useful in this context, whichmay in turn jeopardize its performance with re-spect to our task.
Moreover, a word sense maybelong to a single verb only while a VN class con-tains many verbs.
Consequently, the performance6237 classes vs. 4991 types.7In the WN to VN mapping built into VN, 14.69% of thecovered WN synsets were mapped to more than a single VNclass.8We used the publicly available SenseLearner 2.0, the VB-Collocations model.
We chose VN classes containing thelemma in random when a single mapping is not specified.
Weobtained 67.74% accuracy on section 00 of the WSJ, which isless than the MF baseline.
See Sections 5 and 7.11on a certain lemma may benefit from training in-stances of other lemmas.Note that our task is not reducible to VN frameidentification (a non-trivial task given the rich-ness of the information used to define a framein VN).
Although the categorizing criterion forLevin?s classification is the subset of frames theverb may appear in (equivalently, the diathesis al-ternations the verbal proposition may perform),knowing only the frame in which an instance ap-pears does not suffice, as frames are shared amongclasses.4 The Learning ModelAs common in supervised learning models, we en-code the verb instances into feature vectors andthen apply a learning algorithm to induce a clas-sifier.
We first discuss the feature set and then thelearning algorithm.Features.
Our feature set heavily relies on syn-tactic annotation.
Dorr and Jones (1996) showedthat perfect knowledge of the allowable syntacticframes for a verb allows 98% accuracy in type as-signment to Levin classes.
This motivates the en-coding of the syntactic structure of the sentenceas features, since we have no access to all frames,only to the one appearing in the sentence.Since some verbs may appear in the same syn-tactic frame in different VN classes, a model rely-ing on the syntactic frame alone would not be ableto disambiguate instances of these verbs when ap-pearing in those frames.
Hence our features in-clude lexical context words.
The parse tree en-ables us to use words that appear in specific syn-tactic slots rather than in a linear window aroundthe verb.
To this end, we use the head words ofthe neighboring constituents.
The definition of thehead of a constituent is given in (Collins, 1999).Our feature set is comprised of two parallel setsof features.
The first contains features extractedfrom the parse tree and the verb?s lemma as a stan-dalone feature.
In the second set, each feature is aconjunction of a feature from the first set with theverb?s lemma.
By doing so we created a generalfeature space shared by all verbs, and replicationsof it for each and every verb.
This feature selectionstrategy was chosen in view of the two perspec-tives on the task (per-class and per-verb) discussedin Section 3.Our first set of features encodes the verb?s con-text as inferred from the sentence?s parse tree (Fig-First Feature SetThe stemmed head words, POS, parse tree labels,function tags, and ordinals of the verb?s right krsiblings (kris the maximum number of right sib-lings in the corpus.
These are at most 5krdiffer-ent features).The stemmed head words, POS, labels, functiontags and ordinals of the verb?s left klsiblings, asabove.The stemmed head word & POS of the ?secondhead word?
nodes on the left and right (see textfor precise definition).All of the above features employed on the sib-lings of the parent of the verb (only if the verb?sparent is the head constituent of its grandparent)The number of right/left siblings of the verb.The number of right/left siblings of the verb?sparent.The parse tree label of the verb?s parent.The verb?s voice (active or passive).The verb?s lemma.Figure 1: The first set of features in our model.
Allof them are binary.
The final feature set includestwo sets: the set here, and a set obtained by itsconjunction with the verb?s lemma.ure 1).
We attempt to encode both the syntacticframe, by encoding the tree structure, and the ar-gument preferences, by encoding the head wordsof the arguments and their POS.
The restriction onthe verb?s parent being the head constituent of itsgrandparent is done in order to focus on the correctverb in verb series such as ?intend to run?.The 3rd cell in the table makes use of a ?sec-ond head word?
node, defined as follows.
Considera left sibling (right siblings are addressed analo-gously) M of the verb?s node.
Take the node Hin the subtree of M where M ?s head appears.
His a descendent of a node J which is a child ofM .
The ?second head word?
node is J?s sibling onthe right.
For example, in the sentence We went toschool (see Figure 2) the head word of the PP ?toschool?
is ?to?, and the ?second head word?
node is?school?.
The rationale is that ?school?
could be auseful feature for ?went?, in addition to ?to?, whichis highly polysemous (note that it is also a featurefor ?went?, in the 1st and 2nd cells of the table).The voice feature was computed using a simpleheuristic based on the verb?s POS tag (past partici-ple) and presence of auxiliary verbs to its left.12SNPPRPWeVPVBDwentPPTOtoNPNNschoolFigure 2: An example parse tree for the ?secondhead word?
feature.The current set of features does not detect verbparticle constructions.
We leave this for future re-search.Learning Algorithm.
Our learning task can beformulated as follows.
Let xidenote the featurevector of an instance i, and let X denote the spaceof all such feature vectors.
The subset of possi-ble labels for xiis denoted by Ci, and the correctlabel by ci?
Ci.
We denote the label space byS.
Let T be the training set of instances T = {<x1, C1, c1>,< x2, C2, c2>, ..., < xn, Cn, cn>} ?
(X ?
2S?
S)n, where n is the size of thetraining set.
Let < xn+1, Cn+1>?
(X ?
2S) bea new instance.
Our task is to select which of thelabels in Cn+1is its correct label cn+1(xn+1doesnot have to be a previously observed lemma, butits lemma must appear in a VN class).The structure of the task lets us apply a learn-ing algorithm that is especially appropriate for it.What we need is an algorithm that allows us to re-strict the possible labels of each instance, both intraining and in testing.
The sequential model algo-rithm presented by Even-Zohar and Roth (2001)directly supports this requirement.
We use theSNOW learning architecture for multi-class clas-sification (Roth, 1998), which contains an imple-mentation of that algorithm 9.5 Experimental SetupWe used SemLink VN annotations and parse treeson sections 02-21 of the WSJ Penn Treebank fortraining, and section 00 as a development set, asis common in the parsing community.
We per-formed two parallel sets of experiments, one us-ing manually created gold standard parse trees andone using parse trees created by a state-of-the-art9Experiments on development data revealed that for verbsfor which almost all of the training instances are mapped tothe same VN class, it is most beneficial to select that class.Thus, where more than 90% of the training instances of a verbare mapped to the same class, our algorithm mapped the in-stances of the verb to that class regardless of the context.parser (Charniak and Johnson, 2005) (Note thatthis parser does not output function tags).
Theparser was also trained on sections 02-21 and tunedon section 0010.
Consequently, our adaptation sce-nario is a full adaptation situation in which both theparser and the VerbNet training data are not in thetest domain.
Note that generative parser adaptationresults are known to be of much lower quality thanin-domain results (Lease and Charniak, 2005).
Thequality of the discriminative parser we used didindeed decrease in our adaptation scenario (Sec-tion 7).The training data included 71209 VN in-scopeinstances (of them 41753 polysemous) and the de-velopment 3624 instances (2203 polysemous).
An?in-scope?
instance is one that appears in VN andis tagged with a verb POS.
The same trained modelwas used in both the in-domain and adaptation sce-narios, which only differ in their test sets.In-Domain.
Tests were held on sections01,22,23,24 of WSJ PTB.
Test data includes all in-scope instances for which there is a SemLink anno-tation, yielding 13540 instances, 7798 (i.e., 57.6%)of them polysemous.Adaptation.
For the testing we annotated sen-tences from GENIA (Kim et al, 2003) (version3.0.2).
The GENIA corpus is composed of MED-LINE abstracts related to transcription factors inhuman blood cells.
We annotated 400 sentencesfrom the corpus, each including at least one in-scope verb instance.
We took the first 400 sen-tences from the corpus that met that criterion11 .After cleaning some GENIA POS inconsistencies,this amounts to 690 in-scope instances (380 ofthem polysemous).
The tagging was done by twoannotators with an inter-annotator agreement rateof 80.35% and Kappa 67.66%.Baselines.
We used two baselines, random andmost frequent (MF).
The random baseline selectsuniformly and independently one of the possibleclasses of the verb.
The most frequent (MF) base-line selects the most frequent class of the verb inthe training data for verbs seen while training, andselects in random for the unseen ones.
Conse-quently, it obtains a perfect score over the monose-mous verbs.
This baseline is a strong one and iscommon in disambiguation tasks.We repeated all of the setup above in two sce-10For the very few sentences out of coverage for the parser,we used the MF baseline (see below).11Discarding the first 120 sentences, which were used todesign the annotator guidelines.13narios.
In the first (main) scenario, in-scope in-stances were always mapped to VN classes, whilein the second (?other is possible?
(OIP)) scenario,in-scope instances were allowed to be tagged (dur-ing training) and classified (during test) as not be-longing to any existing VN class12.
In all cases,out-of-scope instances (verbs whose lemmas donot appear in VN) were ignored.
For the OIP sce-nario, we used a different ?other?
label for each ofthe lemmas, not a single label shared by them all.6 ResultsTable 1 shows our results.
In addition to the over-all results, we also show results for the polysemousones alone, since the task is trivial for the monose-mous ones.
The results using gold standard parseseffectively set an upper bound on our model?s per-formance, while those using statistical parser out-put demonstrate its current usability.In-Domain.
Results are shown in the WSJ ?WSJ columns of Table 1.
Using gold standardparses (top), we achieve 96.42% accuracy over-all.
Over the polysemous verbs, the accuracy is93.68%.
This translates to an error reduction overthe MF baseline of 43.35% overall and 43.22% forthe polysemous verbs.
In the ?other is possible?scenario (right), we obtained 36.67% error reduc-tion.
Using a state-of-the-art parser (Charniak andJohnson, 2005) (bottom), we experienced somedegradation of the results (as expected), but theyremained significantly above baseline.
We achieve95.9% accuracy overall and 92.77% for the polyse-mous verbs, which translates to about 35.13% and35.04% error reduction respectively.
In the OIPscenario, we obtained 28.95% error reduction.The results of the random baseline for the in-domain scenario are substantially worse than theMF baseline.
On the WSJ the random baselinescored 66.97% (37.51%) accuracy in the main(OIP) scenarios.Adaptation.
Here we test our model?s abilityto generalize across domains.
Since VN is sup-posed to be a domain independent resource, wehope to acquire statistics that are relevant acrossdomains as well and so to enable us to automati-cally map verbs in domains of various genres.
Theresults are shown in the WSJ ?
GENIA columnsof Table 1.
When using gold standard parses, ourmodel scored 73.16%.
This translates to about13.17% ER on GENIA.
We interestingly experi-12i.e., including instances tagged by SemLink as ?none?.enced very little degradation in the results whenmoving to parser output, achieving 72.4% accu-racy which translates to 10.71% error reductionover the MF baseline.
The random baseline on GE-NIA was again worse than MF, obtaining 66.04%accuracy as compared to 69.09% of MF (in the OIPscenario, 39.12% compared to 46.41%).Run-time performance.
Given a parsed cor-pus, our main model trains and runs in no morethan a few minutes for a training set of ?60K in-stances and a test set of ?11K instances, using aPentium 4 CPU 2.40GHz with 1GB main mem-ory.
The bottleneck in tagging large corpora usingour model is thus most likely the running time ofcurrent parsers.7 DiscussionIn this paper we introduced a new statistical modelfor automatically mapping verb instances intoVerbNet classes, and presented the first large-scaleexperiments for this task, for both in-domain andcorpus adaptation scenarios.Using gold standard parse trees, we achieved96.42% accuracy on WSJ test data, showing43.35% error reduction over a strong baseline.For adaptation to the GENIA corpus, we showed13.1% error reduction over the baseline.
A sur-prising result in the context of adaptation is the lit-tle influence of using gold standard parses versususing parser output, especially given the relativelylow performance of today?s parsers in the adapta-tion task (91.4% F-score for the WSJ in-domainscenario compared to 81.24% F-score when pars-ing our GENIA test set).
This is an interesting di-rection for future work.In addition, we conducted some additional pre-liminary experiments in order to shed light onsome aspects of the task.
The experiments reportedbelow were conducted on the development data,given gold standard parse trees.First, motivated by the close connection be-tween WSD and our task (see Section 3), we con-ducted an experiment to test the applicability ofusing a WSD engine.
In addition to the experi-ments listed above, we also attempted to encodethe output of a modern WSD engine (the VBCollo-cations Model of SenseLearner 2.0 (Mihalcea andCsomai, 2005)), both by encoding the synset (ifexists) of the verb instance as a feature, and by en-coding each possible mapped class of the WSDengine output synset as a feature.
There are k14Main Scenario ?Other is Possible?
(OIP) ScenarioWSJ?WSJ WSJ?GENIA WSJ?WSJ WSJ?GENIAMF Model MF Model MF Model MF ModelGold Std Total 93.68 96.42 69.09 73.16 88.6 92.78 46.41 52.46ER 43.35 13.17 36.67 11.29Poly.
88.87 93.68 48.58 55.35 ?
?
?
?ER 43.22 13.17 ?
?Parser Total 93.68 95.9 69.09 72.4 88.6 91.9 46.41 52.46ER 35.13 10.71 28.95 11.29Poly.
88.87 92.77 48.58 55.35 ?
?
?
?ER 35.04 10.72 ?
?Table 1: Accuracy and error reduction (ER) results (in percents) for our model and the MF baseline.Error reduction is computed as MODEL?MF100?MF.
Results are given for the WSJ and GENIA corpora testsets.
The top table is for a model receiving gold standard parses of the test data.
The bottom is for amodel using (Charniak and Johnson, 2005) state-of-the-art parses of the test data.
In the main scenario(left), instances were always mapped to VN classes, while in the OIP one (right) it was possible (duringboth training and test) to map instances as not belonging to any existing class.
For the latter, no resultsare displayed for polysemous verbs, since each verb can be mapped both to ?other?
and to at least oneclass.features if there are k possible classes13.
Therewas no improvement over the previous model.
Apossible reason for this is the performance of theWSD engine (e.g.
56.1% precision on the verbs inSenseval-3 all-words task data).
Naturally, moreresearch is needed to establish better methods ofincorporating WSD information to assist in thistask.Second, we studied the relative usability of classinformation as opposed to verb idiosyncratic infor-mation in the VN disambiguation task.
By mea-suring the accuracy of our model, first given theper-class features (the first set of features exclud-ing the verb?s lemma feature) and second given theper-verb features (the conjunction of the first setwith the verb?s lemma), we tried to address thisquestion.
We obtained 94.82% accuracy for theper-class experiment, and 95.51% for the per-verbexperiment, compared to 95.95% when using bothin the in-domain gold standard scenario.
The MFbaseline scored 92.45% on this development set.These results, which are close in the per-class ex-periment to those of the MF baseline, indicate thatcombining both approaches in the construction ofthe classifier is justified.Third, we studied the importance of having alearning algorithm utilizing the task?s structure(mapping into a large label space where each in-13The mapping is many-to-many and partial.
To overcomethe first issue, given a WN sense of the verb, we encoded allpossible VN classes that correspond to it.
To overcome thesecond, we treated a verb in a certain VN class, for which themapping to WN was available, as one that can be mapped toall WN senses of the verb.stance can be mapped to only a small subspace).Our choice of the algorithm in (Even-Zohar andRoth, 2001) was done in light of this requirement.We conducted an experiment in which we omittedthese per-instance restrictions on the label space,effectively allowing each verb to take every labelin the label space.
We obtained 94.54% accuracy,which translates to 27.68% error reduction, com-pared to 95.95% accuracy (46.36% error reduc-tion) when using the restrictions.
These results in-dicate that although our feature set keeps us sub-stantially above baseline even without the abovealgorithm, using it boosts our results even further.This result is different from the results obtainedin (Girju et al, 2005), where the results of the un-constrained (flat) model were significantly belowbaseline.As noted earlier, the field of instance levelverb classification into Levin-inspired classes is farfrom being exhaustively explored.
We intend tomake our implementation of the model availableto the community, to enable others to engage infurther research on this task.Acknowledgements.
We would like to thank DanRoth, Mark Sammons and Ran Luria for their help.ReferencesCollin F. Baker, Charles J. Fillmore and John B. Lowe,1998.
The Berkeley FrameNet Project.
Proc.
of the36th Meeting of the ACL and the 17th COLING.Eugene Charniak and Mark Johnson, 2005.
Coarse-15to-fine n-best parsing and maxent discriminativereranking.
Proc.
of the 43rd Meeting of the ACL.Michael Collins, 1999.
Head-driven statistical modelsfor natural language parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Hoa Trang Dang, Karin Kipper, Martha Palmer andJoseph Rosenzweig, 1998.
Investigating regularsense extensions based on intersective Levin classes.Proc.
of the 36th Meeting of the ACL and the 17thCOLING.Bonnie J. Dorr, 1997.
Large-Scale Dictionary Con-struction for Foreign Language Tutoring and Inter-lingual Machine Translation.
Machine Translation,12:1-55.Bonnie J. Dorr and Douglas Jones, 1996.
Role of WordSense Disambiguation in Lexical Acquisition: Pre-dicting Semantics from Syntactic Cues.
Proc.
of the16th COLING.Yair Even-Zohar and Dan Roth, 2001.
A SequentialModel for Multi-Class Classification.
Proc.
of the2001 Conference on Empirical Methods in NaturalLanguage Processing.Roxana Girju, Dan Roth and Mark Sammons, 2005.Token-level Disambiguation of VerbNet classes.
TheInterdisciplinary Workshop on Verb Features andVerb Classes.Svetlana Hensman and John Dunnion, 2004.
Automat-ically building conceptual graphs using VerbNet andWordNet.
International Symposium on Informationand Communication Technologies (ISICT).Jin?Dong Kim, Tomoko Ohta, Yuka Teteisi andJun?ichi Tsujii, 2003.
GENIA corpus ?
a seman-tically annotated corpus for bio-textmining.
Bioin-formatics, 19:i180?i182, Oxford U.
Press 2003.Karin Kipper, Hoa Trang Dang and Martha Palmer,2000.
Class-Based Construction of a Verb Lexicon.Proc.
of the 17th National Conference on ArtificialIntelligence.Karin Kipper-Schuler, 2005.
VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.
D. the-sis, University of Pennsylvania.Karin Kipper, Anna Korhonen, Neville Ryant andMartha Palmer, 2006.
Extending VerbNet withNovel Verb Classes.
Proc.
of the 5th InternationalConference on Language Resources and Evaluation.Judith Klavans and Min-Yen Kan, 1998.
Role of verbsin document analysis.
Proc.
of the 36th Meeting ofthe ACL and the 17th International Conference onComputational Linguistics.Anna Korhonen and Ted Briscoe, 2004.
ExtendedLexical-Semantic Classification of English Verbs.Proc.
of the 42nd Meeting of the ACL, Workshop onComputational Lexical Semantics.Mirella Lapata and Chris Brew, 2004.
Verb ClassDisambiguation using Informative Priors.
Compu-tational Linguistics, 30(1):45-73Matthew Lease and Eugene Charniak, 2005.
Towardsa Syntactic Account of Punctuation.
Proc.
of the 2ndInternational Joint Conference on Natural LanguageProcessing.Beth Levin, 1993.
English Verb Classes And Alterna-tions: A Preliminary Investigation.
The Universityof Chicago Press.Beth Levin and Malka Rappaport Hovav, 2005.
Argu-ment Realization.
Cambridge University Press.Juanguo Li and Chris Brew, 2007.
DisambiguatingLevin Verbs Using Untagged Data.
Proc.
of the2007 International Conference on Recent Advancesin Natural Language Processing.Edward Loper, Szu-ting Yi and Martha Palmer, 2007.Combining Lexical Resources: Mapping BetweenPropBank and VerbNet.
Proc.
of the 7th Inter-national Workshop on Computational Linguistics,Tilburg, the Netherlands.Paola Merlo and Suzanne Stevenson.
2001.
AutomaticVerb-Classification Based On Statistical Distribu-tion Of Argument Structure.
Computational Linguis-tics, 27(3):373?408.Rada Mihalcea and Andras Csomai 2005.
Sense-Learner: word sense disambiguation for all wordsin unrestricted text.
Proc.
of the 43rd Meeting of theACL , Poster Session.Martha Palmer, Daniel Gildea and Paul Kingsbury,2005.
The proposition bank: A corpus annotatedwith semantic roles.
Computational Linguistics,31(1).Dan Roth, 1998.
Learning to resolve natural languageambiguities: A unified approach.
Proc.
of the 15thNational Conference on Artificial IntelligenceSabine Schulte im Walde, 2000.
Clustering verbs se-mantically according to their alternation behavior.Proc.
of the 18th COLING.Lei Shi and Rada Mihalcea, 2005.
Putting pieces to-gether: Combining FrameNet, VerbNet and WordNetfor robust semantic parsing.
Proc.
of the Interna-tional Conference on Intelligent Text Processing andComputational Linguistics.Robert S. Swier and Suzanne Stevenson, 2005.
Ex-ploiting a Verb Lexicon in Automatic Semantic RoleLabelling.
Proc.
of the 2005 conference on empiricalmethods in natural language processing.Szu-ting Yi, Edward Loper and Martha Palmer, 2007.Can Semantic Roles Generalize Across Genres?Proc.
of the 2007 conference of the north americanchapter of the association for computational linguis-tics.16
