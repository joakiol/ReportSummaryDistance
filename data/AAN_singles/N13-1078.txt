Proceedings of NAACL-HLT 2013, pages 673?679,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsA Multi-Dimensional Bayesian Approach to Lexical StyleJulian BrookeDepartment of Computer ScienceUniversity of Torontojbrooke@cs.toronto.eduGraeme HirstDepartment of Computer ScienceUniversity of Torontogh@cs.toronto.eduAbstractWe adapt the popular LDA topic model (Bleiet al 2003) to the representation of stylisticlexical information, evaluating our model onthe basis of human-interpretability at the wordand text level.
We show, in particular, that thismodel can be applied to the task of inducingstylistic lexicons, and that a multi-dimensionalapproach is warranted given the correlationsamong stylistic dimensions.1 IntroductionIn language, stylistic variation is a reflection of var-ious contextual factors, including the backgroundsof and relationship between the parties involved.Although in the context of prescriptive linguistics(Strunk and White, 1979), style is often assumed tobe a matter of aesthetics, the stylistic intuitions oflanguage users are inextricably linked to the conven-tions of register and genre (Biber and Conrad, 2009).Intentional or not, stylistic differences play a rolein numerous NLP tasks.
Examples include genreclassification (Kessler et al 1997), author profil-ing (Garera and Yarowsky, 2009; Rosenthal and Mc-Keown, 2011), social relationship classification (Pe-terson et al 2011), sentiment analysis (Wilson et al2005), readability classification (Collins-Thompsonand Callan, 2005), and text generation (Hovy, 1990;Inkpen and Hirst, 2006).
Following the classic workof Biber (1988), computational modeling of stylehas often focused on textual statistics and the fre-quency of function words and syntactic categories.When content words are considered, they are of-ten limited to manually-constructed lists (Argamonet al 2007), or used as individual features for su-pervised classification, which can be confounded bytopic (Petrenz and Webber, 2011) or fail in the faceof lexical variety.
Our interest is models that offerbroad lexical coverage of human-identifiable stylis-tic variation.Research most similar to ours has focused on clas-sifying the lexicon in terms of individual aspects rel-evant to style (e.g.
formality, specificity, readability)(Brooke et al 2010; Pan and Yang, 2010; Kidwellet al 2009) and a large body of research on the in-duction of polarity lexicons, in particular from largecorpora (Turney, 2002; Kaji and Kitsuregawa, 2007;Velikovich et al 2010).
Our work is the first to rep-resent multiple dimensions of style in a single statis-tical model, adapting latent Dirichlet alcation (Bleiet al 2003), a Bayesian ?topic?
model, to our stylis-tic purposes; as such, our approach also follows onrecent interest in the interpretability of topic-modeltopics (Chang et al 2009; Newman et al 2011).We show that our model can be used for acquisitionof stylistic lexicons, and we also evaluate the modelrelative to theories of register variation and the ex-pected stylistic character of particular genres.2 Model2.1 Linguistic foundationsIn English manuals of style and other prescriptivisttexts (Fowler and Fowler, 1906; Gunning, 1952;Follett, 1966; Strunk and White, 1979; Kane, 1983;Hayakawa, 1994), writers are urged to pay atten-tion to various aspects of lexical style, including el-ements such as clarity, familiarity, readability, for-673mality, fanciness, colloquialness, specificity, con-creteness, objectivity, and naturalness; these stylis-tic categories reflect common aesthetic judgmentsabout language.
In descriptive studies of register,some researchers have posited a few fixed styles(Joos, 1961) or a small, discrete set of situationalconstraints which determine style and register (Crys-tal and Davy, 1969; Halliday and Hasan, 1976); bycontrast, the applied approach of Biber (1988) andtheoretical framework of Leckie-Tarry (1995) offer amore continuous interpretation of register variation.In Biber?s approach, functional dimensions suchas Involved vs. Informational, Argumentative vs.Non-argumentative, and Abstract vs. non-Abstractare derived in an unsupervised manner from amixed-genre corpus, with the labels assigned de-pending on where features (a small set of known in-dicators of register) and genres fall on each spec-trum.
The theory of Leckie-Tarry posits a singlemain cline of register with one pole (the oral pole)reflecting a full reliance on the context of the lin-guistic situation, and the other (the literate pole) re-flecting a reliance on cultural knowledge.
The morespecific elements of register are represented as sub-clines which are strongly influenced by this maincline, creating probabilistic relationships betweenrelated dimensions (Birch, 1995).For the present study, we have chosen 3 dimen-sions (6 styles) which are clearly represented in thelexicon, which are discussed often in the relevant lit-erature, and which fit well into the Leckie-Tarry con-ception of related subclines: colloquial vs. literary,concrete vs. abstract, and subjective vs. objective.
Inaddition to a negative correlation between opposingstyles, we also expect a positive correlation betweenstylistic aspects that tend toward the same main pole,situational (i.e.
colloquial, concrete, subjective) orcultural (i.e.
literary, abstract, objective).
These cor-relations can potentially interfere with accurate lex-ical acquisition.2.2 ImplementationOur main model is an adaption of the popular latentDirichlet alcation topic model (Blei et al 2003),with each of the 6 styles corresponding to a topic.Briefly, latent Dirichlet alcation (LDA) is a gener-ative Bayesian model: for each document d, a dis-tribution of topics ?d is drawn from a Dirichlet prior(with parameter ?).
For each topic z, there is a prob-ability distribution ?z1 corresponding to the proba-bility of that topic generating any given word in thevocabulary.
Words in document d are generated byfirst selecting a topic z randomly according to ?d ,and then randomly selecting a word w according to?z.
An extension of LDA, the correlated topic model(CTM) (Blei and Lafferty, 2007), supposes a morecomplex representation of topics: given a matrix ?representing the covariance between topics and ?representing the means, for each document a topicdistribution ?
(analogous to ? )
is drawn from thelogistic normal distribution.
Given a corpus, goodestimates for the relevant parameters can be derivedusing Bayesian inference.For both LDA and CTM we use the originalvariational Bayes implementation of Blei.
Varia-tional Bayes (VB) works by approximating the trueposterior with a simpler distribution, minimizingthe Kullback-Leibler divergence between the twothrough iterative updates of specially-introducedfree variables.
The mathematical and algorithmicdetails are omitted here; see Blei et al(2003; 2007).Our early investigations used an online, batch ver-sion of LDA (Hoffman et al 2010), which is moreappropriate for large corpora because it requiresonly a single iteration over the dataset.
We discov-ered, however, that batch models were markedly in-ferior to more traditional models for our purposesbecause the influence of the initial model diminishestoo quickly; here, we need particular topics in themodel to correspond to particular styles, and we ac-complish this by seeding the model with known in-stances of each style (see Section 3).
Specifically,our initial ?
consists of distributions where the entireprobability mass is divided amongst the seeds foreach corresponding topic, and a full iteration overthe corpus occurs before ?
is updated.
Typically,LDA iterates over the corpus until a convergence re-quirement is met, but in this case this is neither prac-tical (due to the size of our corpus) nor necessarilydesirable; the diminishing effects of the initial seed-ing means that the model may not stabilize, in termsof its likelihood, until after it has shifted away fromour desired stylistic dimensions towards some other1Some versions of LDA smooth this distribution using aDirichlet prior; here, though, we use the original formulationfrom Blei (2003), which does not.674variation in the data.
Therefore, we treat the optimalnumber of iterations as a variable to investigate.The model is trained on a 1 million text por-tion of the 2009 version of the ICWSM Spinn3rdataset (Burton et al 2009), a corpus of blogs wehave previously used for formality lexicon induction(Brooke et al 2010).
Since our method relies on co-occurrence, we followed our earlier work in usingonly texts with at least 100 different word types.
Allwords were tokenized and converted to lower-case,with no further lemmatization.
Following Hoffmanet al(2010), we initialized the ?
of our models to1/k where k is the number of topics.
Otherwise weused the default settings; when they overlap theywere identical for the LDA and CTM models.3 Lexicon InductionOur primary evaluation is based on the stylistic in-duction of held-out seed words.
The words werecollected from various sources by the first authorand further reviewed by the second; we are bothnative speakers of English with significant experi-ence in English linguistics.
Included words had tobe clear, extreme members of their stylistic cate-gory, with little or no ambiguity with respect to theirstyle.
The colloquial seeds consist of English slangterms and acronyms, e.g.
cuz, gig, asshole, lol.
Theliterary seeds were primarily drawn from web siteswhich explain difficult language in texts such as theBible and Lord of the Rings; examples include be-hold, resplendent, amiss, and thine.
The concreteseeds all denote objects and actions strongly rootedin the physical world, e.g.
shove and lamppost, whilethe abstract seeds all involve concepts which requiresignificant human psychological or cultural knowl-edge to grasp, for instance patriotism and noncha-lant.
For our subjective seeds, we used an editedlist of strongly positive and negative terms from amanually-constructed sentiment lexicon (Taboada etal., 2011), e.g.
gorgeous and depraved, and for ourobjective set we selected words from sets of near-synonyms where one was clearly an emotionally-distant alternative, e.g.
residence (for home), jocu-lar (for funny) and communicable (for contagious).We filtered initial lists to 150 of each type, remov-ing words which did not appear in the corpus orwhich occurred in multiple lists.
For evaluation weused stratified 3-fold crossvalidation, averaged over5 different (3-way) splits of the seeds, with the samesplits used for all evaluated conditions.Given two sets of opposing seeds, we follow ourearlier work in evaluating our performance in termsof the number of pairings of seeds from each setwhich have the expected stylistic relationship rel-ative to each other (the guessing baseline is 0.5).Given a word w and two opposing styles (topics) pand n, we place w on the PN dimension according tothe ?
of our trained model as follows:PNw =?pw?
?nw?pw +?nwThe normalization is important because otherwisemore-common words would tend to have higherPN?s, when in fact the opposite is true (rare wordstend to be more stylistically prominent).
We thencalculate pairwise accuracy as the percentage ofpairs ?wp,wn?
(wp ?
Pseeds and wn ?
Nseeds) wherePNwp >PNwn .
However, this metric does not addressthe case where the degree of a word in one stylisticdimension is overestimated because of its status ona parallel dimension.
Two more-holistic alternativesare total accuracy, the percentage of seeds for whichthe highest ?tw is the topic t for which w is a seed(guessing baseline is 0.17), and the average rank ofthe correct t as ordered by ?tw (in the range 1?6,guessing baseline is 3.5); the latter is more forgivingof near misses.We tested a few options which involved straight-forward modifications to model training.
StandardLDA produces all tokens in the document, but whendealing with style rather than topic, the number oftimes a word appears is much less relevant (Brookeet al 2010).
Our binary model assumes an LDAthat generates types, not tokens.2 A key comparison2At the theoretical level, this move is admittedly problem-atic, since our LDA model is thus being trained under the as-sumption that texts with multiple instances of the same type canbe generated, when of course such texts cannot by definition ex-ist.
We might address this by moving to Bayesian models withvery different generative assumptions, e.g.
the spherical topicmodel (Reisinger et al 2010), but these methods involve a sig-nificant increase of computational complexity and we believethat on a practical level there are no real negatives associatedwith directly using a binary representation as input to LDA; infact, we are avoiding what appears to be a much more seriousproblem, burstiness (Doyle and Elkan, 2009), i.e.
the fact that675ModelPairwise Accuracy (%)Total Acc.
(%) Avg.
RankLit/Col Abs/Con Obj/Sub Allguessing baseline 50.0 50.0 50.0 50.0 16.6.
3.50basic LDA (iter 2) 94.3 98.8 93.0 95.4 55.0 1.79binary LDA (iter 2) 96.2 98.9 93.5 96.2 57.7 1.74combo binary LDA (iter 1) 95.4 99.2 93.3 96.0 53.1 1.86binary CTM (iter 1) 96.3 99.0 89.6 95.0 53.0 1.87Table 1: Model performance in lexical induction of seeds.
Bold indicates best in column.here is with a combined LDA model (combo), anamalgamation of three independently trained 2-topicmodels, one for each dimension; this tests our keyhypothesis that training dimensions of style togetheris beneficial.
Finally, we test against the correlatedtopic model (CTM), which offers an explicit repre-sentation of style correlation, but which has donepoorly with respect to interpretability, despite offer-ing better perplexity (Chang et al 2009).The results of the lexicon induction evaluationare in Table 1.
Since the number of optimal iter-ations varies, we report the result from the best ofthe first five iterations, as measured by total accu-racy; the best iteration is shown in parenthesis.
Ingeneral, all the results are high enough?we are re-liably above 90% for the pairwise task, and above50% for the 6-way task?for us to conclude withsome confidence that our model is capturing a sig-nificant amount of stylistic variation.
As predicted,using words as boolean features had a net positivegain, consistent across all of our metrics, though thiseffect was not as marked as we have seen previously.The model with independent training of each dimen-sion (combo) did noticeably worse, supporting ourconclusion that a multidimensional approach is war-ranted here.
Particularly striking is the much largerdrop in overall accuracy as compared to pairwise ac-curacy, which suggests that the combo model is cap-turing the general trends but not distinguishing cor-related styles as well.
However, the most complexmodel, the CTM, actually does slightly worse thanthe combo, which was contrary to our expectationsbut nonetheless consistent with previous work on theinterpretability of topic models.
The performance ofthe full LDA models benefited from a second itera-traditional LDA is influenced too much by multiple instances ofthe same word.tion, but this was not true of combo LDA or CTM,and the performance of all models dropped after thesecond iteration.An analysis of individual errors reveals, unsur-prisingly, that most of the errors occur across styleson the same pole; by far the largest single com-mon misclassification is objective words to abstract.Of the words that consistently show this misclas-sification across the runs, many of them, e.g.
ani-mate, aperture, encircle, and constrain are clearlyerrors (if anything, these words tend towards con-creteness), but in other cases the word in questionis arguably also fairly abstract, e.g.
categorize andpredominant, and might not be labeled an error atall.
Other signs that our model might be doing bet-ter than our total accuracy metric gives it credit for:many of the subjective words that are consistentlymislabeled as literary have an exaggerated, literaryfeel, e.g.
jubilant, grievous, and malevolent.4 Text-level AnalysisOur secondary analysis involved evaluating the ?
?sof our best configuration (based on average pairwiseand total accuracy) on other texts.
After training,we carried out inference on the BNC corpus, aver-aging the resulting ?
?s to see which styles are asso-ciated with which genres.
Appearances of the seedterms for each model were disregarded during thisprocess; only the induced part of the lexicon wasused.
The average differences relative to the meanacross the various stylistic dimensions (as measuredby the probabilities in ? )
are given for a selection ofgenres in Table 2.The most obvious pattern in table 2 is the domi-nance of the medium: all written genres are positivefor our styles on the ?cultural?
pole and negative forstyles on the ?situational?
pole and the opposite is676GenreStylesLiterary Abstract Objective Colloquial Concrete SubjectiveNews +0.67 +0.50 +0.43 ?0.31 ?0.72 ?0.57Religious texts +0.38 +0.38 +0.28 ?0.27 ?0.44 ?0.32Academic +0.18 +0.29 +0.26 ?0.20 ?0.36 ?0.18Fiction +0.31 +0.09 +0.02 ?0.05 ?0.12 ?0.25Meeting ?0.61 ?0.54 ?0.42 +0.35 +0.69 +0.55Courtroom ?0.63 ?0.53 ?0.41 +0.32 +0.69 +0.57Conversation ?0.56 ?0.63 ?0.54 +0.43 +0.80 +0.50Table 2: Average differences from corpus mean of LDA-derived stylistic dimension probabilities for various genres inthe BNC, in hundredths.true for spoken genres.
The magnitude of this ef-fect is more difficult to interpret: though it is clearwhy fiction should sit on the boundary (since it con-tains spoken dialogue), the appearance of news atthe written extreme is odd, though it might be due tothe fact that news blogs are the most prevalent for-mal genre in the training corpus.However, if we ignore magnitude and focus on therelative ratios of the stylistic differences for styleson the same pole, we can identify some individ-ual stylistic effects among genres within the samemedium.
Relative to the other written genres, for in-stance, fiction is, sensibly, more literary and muchless objective, while academic texts are much moreabstract and objective; for the other two written gen-res, the spread is more even, though relative to re-ligious texts, news is more objective.
At the sit-uational pole, fiction also stands out, being muchmore colloquial and concrete than other written gen-res.
Predictably, if we consider again the ratiosacross styles, conversation is the most colloquialgenre here, though the difference is subtle.We carried out a correlation analysis of the LDA-reduced styles of all texts in the BNC and, con-sistent with the genre results in Table 2, found astrong positive correlation for all styles on the samemain pole, averaging 0.83.
The average negativecorrelation between opposing poles is even higher,?0.88.
This supports the Leckie-Tarry formulation.The independence assumptions of the LDA modeldid not prevent strong correlations from forming be-tween these distinct yet clearly interrelated dimen-sions; if anything, the correlations are stronger thanwe would have predicted.5 ConclusionWe have introduced a Bayesian model of stylisticvariation.
Topic models like LDA are often evalu-ated using information-theoretic measures, but ouremphasis has been on interpretibility: at the wordlevel we can use the model to induce stylistic lex-icons which correspond to human judgement, andat the text level we can use it distinguish genres inexpected ways.
Another theme has been to offer ev-idence that indeed a multi-dimensional approach isstrongly warranted: importantly, our results indicatethat separate unidimensional models of style are in-ferior for identifying the core stylistic character ofeach word, and in our secondary analysis we foundstrong correlations among styles attributable to thesituational/cultural dichotomy.
However, an off-the-shelf model that integrates correlation among topicsdid not outperform basic LDA.One advantage of a Bayesian approach is in theflexibility of the model: there are any number ofother interesting possible extensions at both the ?and ?
levels of the model, including alternative ap-proaches to correlation (Li and McCallum, 2006).Beyond Bayesian models, vector space and graphi-cal approaches should be compared.
More work isclearly needed to improve evaluation: some of ourseeds could fall into multiple stylistic categories, soa more detailed annotation would be useful.AcknowledgementsThis work was financially supported by the Natu-ral Sciences and Engineering Research Council ofCanada.677ReferencesShlomo Argamon, Casey Whitelaw, Paul Chase, Sob-han Raj Hota, Navendu Garg, and Shlomo Levitan.2007.
Stylistic text classification using functional lex-ical features.
Journal of the American Society for In-formation Science and Technology, 7:91?109.Douglas Biber and Susan Conrad.
2009.
Register, Genre,and Style.
Cambridge University Press.Douglas Biber.
1988.
Variation Across Speech and Writ-ing.
Cambridge University Press.David Birch.
1995.
Introduction.
In Helen Leckie-Tarry,editor, Language and Context: A Functional Linguis-tic Theory of Register.
Pinter.David M. Blei and John D. Lafferty.
2007.
Correlatedtopic models.
Annals of Applied Statistics, 1(1):17?35.David M. Blei, Andrew Y. Ng, Michael I. Jordan, andJohn Lafferty.
2003.
Latent Dirichlet alcation.Journal of Machine Learning Research, 3:993?1022.Julian Brooke, Tong Wang, and Graeme Hirst.
2010.
Au-tomatic acquisition of lexical formality.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics (COLING ?10), Beijing.Kevin Burton, Akshay Java, and Ian Soboroff.
2009.
TheICWSM 2009 Spinn3r Dataset.
In Proceedings of theThird Annual Conference on Weblogs and Social Me-dia (ICWSM 2009), San Jose, CA.Jonathan Chang, Jordan Boyd-Graber, Sean Gerrish,Chong Wang, and David Blei.
2009.
Reading tealeaves: How humans interpret topic models.
In Pro-ceedings of Neural Information Processing Systems(NIPS ?09).Kevyn Collins-Thompson and Jamie Callan.
2005.Predicting reading difficulty with statistical languagemodels.
Journal of the American Society for Informa-tion Science Technology, 56(13):1448?1462.David Crystal and Derek Davy.
1969.
Investigating En-glish Style.
Indiana University Press.Gabriel Doyle and Charles Elkan.
2009.
Accounting forburstiness in topic models.
In International Confer-ence on Machine Learning (ICML ?09).Wilson Follett.
1966.
Modern American Usage.
Hill &Wang, New York.H.
W. Fowler and F. G. Fowler.
1906.
The King?s En-glish.
Clarendon Press, Oxford, 2nd edition.Nikesh Garera and David Yarowsky.
2009.
Modeling la-tent biographic attributes in conversational genres.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP (ACL-IJCNLP ?09), pages 710?718, Sin-gapore.Robert Gunning.
1952.
The Technique of Clear Writing.McGraw-Hill, New York.M.A.K.
Halliday and Ruqaiya Hasan.
1976.
Cohesion inEnglish.
Longman, London.S.I.
Hayakawa, editor.
1994.
Choose the Right Word.HarperCollins Publishers, second edition.
Revised byEugene Ehrlich.Matthew D. Hoffman, David M. Blei, and Francis R.Bach.
2010.
Online learning for latent Dirichlet allocation.
In Neural Information Processing Systems(NIPS ?10), pages 856?864.Eduard H. Hovy.
1990.
Pragmatics and natural languagegeneration.
Artificial Intelligence, 43:153?197.Diana Inkpen and Graeme Hirst.
2006.
Building andusing a lexical knowledge base of near-synonym dif-ferences.
Computational Linguistics, 32(2):223?262.Martin Joos.
1961.
The Five Clocks.
Harcourt, Braceand World, New York.Nobuhiro Kaji and Masaru Kitsuregawa.
2007.
Build-ing lexicon for sentiment analysis from massive col-lection of HTML documents.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL ?07).Thomas S. Kane.
1983.
The Oxford Guide to Writing.Oxford Univeristy Press.Brett Kessler, Geoffrey Nunberg, and Hinrich Schu?tze.1997.
Automatic detection of text genre.
In Proceed-ings of the 35th Annual Meeting of the Associationfor Computational Linguistics (ACL ?97), pages 32?38, Madrid, Spain.Paul Kidwell, Guy Lebanon, and Kevyn Collins-Thompson.
2009.
Statistical estimation of wordacquisition with application to readability predic-tion.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing(EMNLP?09), pages 900?909, Singapore.Helen Leckie-Tarry.
1995.
Language and Context: AFunctional Linguistic Theory of Register.
Pinter.Wei Li and Andrew McCallum.
2006.
Pachinko alloca-tion: DAG-structured mixture models of topic correla-tions.
In Proceedings of the 23rd International Con-ference on Machine Learning, ICML ?06, pages 577?584.David Newman, Edwin V. Bonilla, and Wray Buntine.2011.
Improving topic coherence with regularizedtopic models.
In Proceedings of Advances in NeuralInformation Processing Systems (NIPS ?11).Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
IEEE Transactions on Knowledgeand Data Engineering, 22(10).Kelly Peterson, Matt Hohensee, and Fei Xia.
2011.Email formality in the workplace: A case study on678the Enron corpus.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics (ACL ?11), Portland, Oregon.Philipp Petrenz and Bonnie Webber.
2011.
Stable clas-sification of text genres.
Computational Linguistics,37(2):385?393, June.J.
Reisinger, A.
Waters, B. Silverthorn, and R. Mooney.2010.
Spherical topic models.
In International Con-ference on Machine Learning (ICML ?10).Sara Rosenthal and Kathleen McKeown.
2011.
Age pre-diction in blogs: A study of style, content, and onlinebehavior in pre- and post-social media generations.
InProceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics (ACL ?11), Port-land, Oregon.William Strunk and E.B.
White.
1979.
The Elements ofStyle.
Macmillan, 3rd edition.Maite Taboada, Julian Brooke, Milan Tofiloski, KimberlyVoll, and Manfred Stede.
2011.
Lexicon-based meth-ods for sentiment analysis.
Computational Linguis-tics, 37(2):267?307.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting on Association for Computational Linguis-tics, ACL ?02, pages 417?424, Philadelphia, Pennsyl-vania.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-nan, and Ryan McDonald.
2010.
The viability of web-derived polarity lexicons.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, HLT ?10, pages 777?785, Los An-geles, California.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In Proceedings of the conferenceon Human Language Technology and Empirical Meth-ods in Natural Language Processing, HLT/EMNLP?05, pages 347?354.679
