A Test Environment for Natural Language Understanding SystemsLi Li, Deborah A. Dahl, Lewis M. Norton, Marcia C. Linebarger, Dongdong ChenUnisys Corporation2476 Swedesford RoadMalvern, PA 19355, U.S.A.{Li.Li, Daborah.Dahl, Lewis.Norton, Marcia.Linebarger, Dong.Chen}@unisys.comAbstractThe Natural Language Understanding EngineTest Environment (ETE) is a GUI software toolthat aids in the development and maintenance oflarge, modular, natural anguage understanding(NLU) systems.
Natural anguage understandingsystems are composed of modules (such as part-of-speech taggers, parsers and semanticanalyzers) which are difficult to test individuallybecause of the complexity of their output datastructures.
Not only are the output datastructures of the internal modules complex, butalso many thousands of test items (messages orsentences) are required to provide a reasonablesample of the linguistic structures of a singlehuman language, even if the language isrestricted to a particular domain.
The ETEassists in the management and analysis of thethousands of complex data structures createdduring natural language processing of a largecorpus using relational database technology in anetwork environment.IntroductionBecause of the complexity of the internal datastructures and the number of test cases involved intesting a natural anguage understanding system,evaluation of testing results by manualcomparison of the internal data structures i verydifficult.
The difficulty of examining NLUsystems in turn greatly increases the difficulty ofdeveloping and extending the coverage of thesesystems, both because as the system increases incoverage and complexity, extensions becomeprogressively harder to assess and because loss ofcoverage of previously working test data becomesharder to detect.The ETE addresses these problems by:1. managing batch input of large numbers of tdstsentences or messages, whether spoken orwritten.2.
storing the NLU system output for a batch runinto a database.3.
automatically comparing multiple levels ofinternal NLU data structures across batchruns of the same data with different engineversions.
These data structures include part-of-speech tags, syntactic analyses, andsemantic analyses.4.
flagging and displaying changed portions ofthese data structures for an analyst's attention.5.
providing access to a variety of databasequery options to allow an analyst to selectinputs of potential interest, for example, thosewhich took an abnormally long time toprocess, or those which contain certain words.6.
providing a means for the analyst o annotateand record the quality of the variousintermediate data structures.7.
providing a basis for quantifying bothregression and improvement in the NLUsystem.1 Testing Natural LanguageUnderstanding SystemsApplication level tests, in which the ability of thesystem to output he correct answer on a set of763system parameterso on n+l n+2~ystemL versionsdataFigure 1: Matrix Comparison Analysisinputs is measured, have been used in naturallanguage processing for a number of years(ATIS-3 (1991), MUC-6 (1995), Harman andVoorhees (1996)).
Although these tests wereoriginally designed for comparing differentsystems, they can also be used to compare theperformance of sequential versions of the samesystem.
These kinds of black-box tests, whileuseful, do not provide insight into the correctnessof the internal NLU data structures since they areonly concerned with the end result, or answerprovided by the system.
They also require theimplementation f a particular application againstwhich to test.
This can be time-consuming andalso can give rise to the concern that the NLUprocessing will become slanted toward theparticular test application as the developersattempt o improve the system's performance onthat application.The Parseval effort (Black (1991)) attempted tocompare parsing performance across systemsusing the Treebank as a basis for comparison.Although Parseval was very useful for comparingparses, it did not enable developers to compareother data structures, such as semanticrepresentations.
In addition, in order toaccommodate many different parsing formalismsfor evaluation, it does not attempt o compareevery aspect of the parses.
Finally, Treebank datais not always available for domains which need tobe tested.King (1996) discusses the general issues in NLUsystem evaluations from a software engineeringpoint of view.
Flickinger et al (1987) describe invery general terms a method for evaluation ofNLU systems in a single application domain(database query) with a number of differentmeasures, such as accuracy of lexical analysis,parsing, semantics, and correctness of query,based on a large collection of annotated Englishsentences.
Neal et al (1992) report on an effort todevelop a more general evaluation tool for NLUsystems.
These approaches either focus onapplication level tests or presuppose theavailability of large annotated test collections~which in fact are very expensive to create andmaintain.
For the purpose of diagnostic evaluationof different versions of the same system, anannotated test corpus is not absolutely necessarybecause defects and regressions of the system canbe discovered from its internal data structures andthe differences between them.2 Matrix Comparison Analysisof NLU SystemsA typical NLU system takes an input of certainform and produces a final as well as a set ofintermediate analyses, for instance, parse treeslrepresented by a variety of data structures rangingfrom list to graph.
These intermediate data can beused as "milestones" to measure the behavior ofthe underlying system and provide clues fordetermining the types and scopes of problems.The intermediate data can be further comparedsystematically to reveal the behavior changes of asystem.
In a synchronic comparison, different testsare conducted for a version of the system bychanging its parameters, uch as the presence orabsence of the lexical server, to determine theimpact of the module to the system.
In adiachronic comparison, tests are conducted fordifferent versions of the system with the sameparameters, to gauge the improvements ofdevelopment effort.
In practice, any two tests canbe compared to determine the effect of certainfactors on the performance of a NLU system.Conceptually, this type of matrix analysis can be764represented in a coordinate system (Figure l) inwhich a test is represented as a point and acomparison between two tests as an arrowheadline connecting the points.
In theory, n-way andsecond order comparisons are possible, but inpractice 2-way first order comparisons are mostuseful.ETE is designed for the Unisys natural anguageengine (NLE), a NL system implemented inQuintus Prolog.
NLE can take as input text(sentences or paragraphs) or nbest speech outputand produce the following intermediate datastructures as Prolog constructs:?
tokens (flat list)?
words (flat list)* part-of-speech tags (fiat list)?
lexical entries (nested attribute-value list)?
parse trees (tree)?
syntactic representation (graph and treederived from graph)?
semantic representation (graph and tree ?derived from graph)?
processing time of different stages of analysesThe trees in this case are lines of text whereparent-child relationships are implied by lineindentations.
A graph is expressed as a Prolog listof terms, in which two terms are linked if theyhave the same (constant) argument in a particularposition.
In addition to these data structures, NLEalso generates a set of diagnostic flags, such asbackup parse (failure to achieve a full-span parse)and incomplete semantic analysis.A special command in NLE can be called toproduce the above data in a predefined format ona given corpus.3 The Engine Test EnvironmentETE is comprised of two components: a commonrelational database that houses the test results anda GUI program that manages and displays the testresources and results.
The central database isstored on a file server PC and shared by theanalysts through ETE in a Windows NT networkenvironment ETE communicates with NLEthrough a TCP/IP socket and the Access databasewith Visual Basic 5.0.
Large and time-consumingbatch runs can be carried out on several machinesand imported into the database simultaneously.Tests conducted on other platforms, such as Unix~can be transferred into the ETE database andanalyzed as well.The key functions of ETE are described below:Manage test resources: ETE provides angraphical interface to manage variousresources needed for tests, including corpora,NLE versions and parameter settings, andconnections to linguistic servers (Norton et al(1998)).
The interface also enforces theconstraints on each test.
For example, twotests with different corpora cannot becompared.Compare various types of analysis data.ETE employs different algorithms to computethe difference between different ypes of dataand display the disparate regions graphicallyThe comparison routines are implemented inProlog except for trees.
Lists comparisons aretrivial in Prolog.
Graph comparison isachieved in two steps.
First, all linkagearguments in graph terms are substituted byvariables such that links are maintained byunification.
Second, set operations are appliedto compute the differences.
Let U(G) denotethe variable substitution of a graph G anddiff(Gx, Gy) the set of different terms betweenGx and Gy, then diff(Gx, Gy) = Gx - U(Gy)and diff(Gy, Gx) = Gy - U(Gx), where (-) isthe Prolog set difference operation.
Under thisdefinition, differences in node ordering andlink labeling of two graphs are discounted incomparison.
For instance, Gx = \[f(a, el),g(el, e2)\], for which U(Gx) = \[f(a, X), g(X,Y)\], is deemed identical to Gy = \[g(e3, e4),f(a, e3)\], where ei are linkage arguments.
It iseasy to see the time complexity of diff isO(mn) for two graphs of size m and n765respectively.
Trees are treated as text files andthe DOS command fc (file comparison) isutilized to compare the differences.
Since fchas several limits, we are consideringreplacing it with a tree matching algorithmthat is more accurate and sensitive tolinguistic structures.?
Present a hierarchical view of batchanalyses.
We base our approach to visualinformation management upon the notion of"overview, filter, detail-on-demand."
For eachtest, ETE displays a diagnostic report and atable of sentence analyses.
The diagnosticreport is an overview to direct an analyst'sattention to the problem areas which comeeither from the system's own diagnostics, orfrom comparisons.
ETE is therefore stilluseful even without every sentence beingannotated.
The sentence analyses tablepresents the intermediate data in their logicalorder and shows on demand the details of eachtype of data.?
Enable access to a variety of databasequery capabilities.
ETE stores all types ofintermediate data as strings in the databaseand provides regular-expression based textsearch for various data.
A unique feature ofETE is in-report query, which enables queryoptions on various reports to allow an analystto quickly zoom in to interesting data based onthe diagnostic information.
Compared withTgrep (1992) which works only on Treebanktrees, ETE provides a more general andpowerful search mechanism for a complexdatabase.?
Provide graphical and contextualinformation for annotation.
Annotation is aproblem because it still takes a human.
ETEoffers flexible and easy access to theintermediate data within and across batchruns.
For instance, when grading a semanticanalysis, the analyst can bring up the lexicaland syntactic analyses of the same sentence,or look at the analyses of the sentence in othertests at the same time, all with a few mouseclicks.
This context information helps analyststo maintain consistency within and betweenthemselves during annotation.Facilitate access to other resources andapplications.
Within ETE, an analyst canexecute other applications, such as MicrosoftExcel (spreadsheet), and interact with otherdatabases, uch as a Problem Database whichtracks linguistic problems and an ApplicationDatabase which records test results forspecific applications, to offer an integrateddevelopment, test and diagnosis environmentfor a complex NLU system.
The integration ofthese databases will provide a foundation toevaluate overall system performance.
Forinstance, it would be possible to determinewhether more accurate semantic analysesincrease the application accuracy.4 Using the Engine TestEnvironmentSo far ETE has been used in the Unisys NLUgroup for the following tasks:?
Analyze and quantify system improvementsand regressions due to modifications to thesystem, such as expanding lexicon, grammarand knowledge base.
In these diachronicanalyses, we use a baseline system andcompare subsequent versions against thebaseline performance, as well as the previousversion.
ETE is used to filter out sentenceswith changed syntactic and semantic analysesso that the analyst can determine the types ofthe changes in the light of other diagnosticinformation.
A new system can becharacterized by percentage of regression andimprovement in accuracy as well as timespeedup.?
Test the effects of new analysis trategies.
Forinstance, ETE has been used to study if oursystem can benefit from a part-of-speechtagger.
With ETE, we were able quantify thesystem's accuracy and speed improvementswith different tagging options easily andquickly on test corpora and modify the systemand the tagger accordingly.Annotate parses and semantic analyses forquality analysis and future reference.
We haveso far used corrective and gradingannotations.
In corrective annotation, theanalyst corrects a wrong analysis, forexample, a part-of-speech tag, with the correctone.
In grading annotation, the analyst assignsproper categories to the analyses.
In the testswe found that both absolute grading (i.e.
aparse is perfect, mediocre or terrible in a test)and relative grading (i.e.
a parse is better,same or worse in a comparison) are veryuseful.The corpora used in these tests are drawn fromvarious domains of English language, rangingfrom single sentence questions to e-mail messages.The performance of ETE on batch tests dependslargely on NLE, which in turn depends on the sizeand complexity of a corpus.
The tests thereforerange from 20 hours to 30 minutes with variouscorpora in a Pentium Pro PC (200 Mhz, 256 MBmemory).
A comparison of two batch test resultsis independent of linguistic analysis and is linearto the size of the corpus.
So far we haveaccumulated 209 MB of test data in the ETEdatabase.
The tests show that ETE is capable ofdealing with large sets of test items (at an averageof 1,000 records per test) in a networkenvironment with fast database access responses.ETE assists analysts to identify problems anddebug the system on large data sets.
Without ETE,it would be difficult, if not impossible, to performtasks of this complexity and scale.
ETE not onlyserves as a software tool for large scale tests of asystem, but also helps to enforce a sound andsystematic development strategy for the NLUsystem.
An issue to be further studied is whetherthe presence of ETE skews the performance ofNLE as they compete for computer resources.ConclusionWe have described ETE, a software tool for NLUsystems and its application in our NL developmentproject.
Even though ETE is tied to the currentNLU system architecture, its core concepts andtechniques, we believe, could be applicable to thetesting of other NLU systems.
ETE is stillundergoing constant improvements, driven both bythe underlying NLU system and by users' requestsfor new features.
The experiments with ETE sofar show that the tool is of great benefit foradvancing Unisys NLU technologyReferencesATIS-3 (1991) Proceedings of the DARPA Speechand Natural Language Workshops, MorganKaufmannBlack E. et al (1991) A Procedure for QuantitativelyComparing the Syntactic Coverage of EnglishGrammars, Proceedings of Speech and NaturalLanguage Workshop, DARPA, pp.
306 - 311Flickinger D., Nerbounne J., Sag I., and Wasow T.(1987) Toward Evaluation of NLP Systems.Hewlett Packard Laboratories, Palo Alto, CaliforniaHarman D.K., Voorhees E.M. (1996) Proceedings ofthe Fifth Text Retrieval Conference (TREC-5),Department ofCommerce and NISTKing Margaret (1996) Evaluating Natural LanguageProcessing Systems.
Communication f ACM, Vol.39, No.
1,  January 1996, pp.
73 - 79MUC-6 (1995) Proceedings of the Sixth MessageUnderstanding Conference, Columbia, Maryland,Morgan KaufmannNeal J., Feit, E.L., Funke D.J., and Montgomery C.A.
(1992) An Evaluation Methodology for NaturalLanguage Processing Systems.
Rome LaboratoryTechnical Report RL-TR-92-308Norton M.L., Dahl D.A., Li Li, Beals K.P.
(1998)Integration of Large-Scale Linguistic Resources ina Natural Language Understanding System.
to bepresented in COLING 98, August 10-14, 1998,Universite de Montreal, Montreal, Quebec, CanadaTgrep Documentation (1992)http://www.ldc.upenn.edu/ldc/online/treebank/README.Iong767
