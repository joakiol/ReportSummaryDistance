Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 89?98,Paris, October 2009. c?2009 Association for Computational LinguisticsWeight pushing and binarization for fixed-grammar parsingMatt Post and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractWe apply the idea of weight pushing(Mohri, 1997) to CKY parsing with fixedcontext-free grammars.
Applied afterrule binarization, weight pushing takes theweight from the original grammar rule andpushes it down across its binarized pieces,allowing the parser to make better prun-ing decisions earlier in the parsing pro-cess.
This process can be viewed as gen-eralizing weight pushing from transduc-ers to hypergraphs.
We examine its ef-fect on parsing efficiency with various bi-narization schemes applied to tree sub-stitution grammars from previous work.We find that weight pushing produces dra-matic improvements in efficiency, espe-cially with small amounts of time and withlarge grammars.1 IntroductionFixed grammar-parsing refers to parsing that em-ploys grammars comprising a finite set of rulesthat is fixed before inference time.
This is incontrast to markovized grammars (Collins, 1999;Charniak, 2000), variants of tree-adjoining gram-mars (Chiang, 2000), or grammars with wildcardrules (Bod, 2001), all of which allow the con-struction and use of rules not seen in the trainingdata.
Fixed grammars must be binarized (eitherexplicitly or implicitly) in order to maintain theO(n3|G|) (n the sentence length, |G| the grammarsize) complexity of algorithms such as the CKYalgorithm.Recently, Song et al (2008) explored differentmethods of binarization of a PCFG read directlyfrom the Penn Treebank (the Treebank PCFG),showing that binarization has a significant effecton both the number of rules and new nontermi-nals introduced, and subsequently on parsing time.This variation occurs because different binariza-tion schemes produce different amounts of sharedrules, which are rules produced during the bina-rization process from more than one rule in theoriginal grammar.
Increasing sharing reduces theamount of state that the parser must explore.
Bina-rization has also been investigated in the context ofparsing-based approaches to machine translation,where it has been shown that paying careful atten-tion to the binarization scheme can produce muchfaster decoders (Zhang et al, 2006; Huang, 2007;DeNero et al, 2009).The choice of binarization scheme will not af-fect parsing results if the parser is permitted to ex-plore the whole search space.
In practice, how-ever, this space is too large, so parsers use prun-ing to discard unlikely hypotheses.
This presentsa problem for bottom-up parsing algorithms be-cause of the way the probability of a rule is dis-tributed among its binarized pieces: The standardapproach is to place all of that probability on thetop-level binarized rule, and to set the probabilitiesof lower binarized pieces to 1.0.
Because theserules are reconstructed from the bottom up, prun-ing procedures do not have a good estimate of thecomplete cost of a rule until the entire original rulehas been reconstructed.
It is preferable to have thisinformation earlier on, especially for larger rules.In this paper we adapt the technique of weightpushing for finite state transducers (Mohri, 1997)to arbitrary binarizations of context-free grammarrules.
Weight pushing takes the probability (or,more generally, the weight) of a rule in the origi-nal grammar and pushes it down across the rule?sbinarized pieces.
This helps the parser make bet-89ter pruning decisions, and to make them earlier inthe bottom-up parsing process.
We investigate thisalgorithm with different binarization schemes andgrammars, and find that it improves the time vs.accuracy tradeoff for parsers roughly proportion-ally to the size of the grammar being binarized.This paper extends the work of Song et al(2008) in three ways.
First, weight pushing fur-ther reduces the amount of time required for pars-ing.
Second, we apply these techniques to TreeSubstitution Grammars (TSGs) learned from theTreebank, which are both larger and more accu-rate than the context-free grammar read directlyfrom the Treebank.1 Third, we examine the inter-action between binarization schemes and the in-exact search heuristic of beam-based and k-bestpruning.2 Weight pushing2.1 BinarizationNot all binarization schemes are equivalent interms of efficiency of representation.
Consider thegrammar in the lefthand column of Figure 1 (rules1 and 2).
If this grammar is right-binarized orleft-binarized, it will produce seven rules, whereasthe optimal binarization (depicted) produces only5 rules due to the fact that two of them are shared.Since the complexity of parsing with CKY is afunction of the grammar size as well as the inputsentence length, and since in practice parsing re-quires significant pruning, having a smaller gram-mar with maximal shared substructure among therules is desirable.We investigate two kinds of binarization in thispaper.
The first is right binarization, in which non-terminal pairs are collapsed beginning from thetwo rightmost children and moving leftward.
Thesecond is a greedy binarization, similar to that ofSchmid (2004), in which the most frequently oc-curring (grammar-wide) nonterminal pair is col-lapsed in turn, according to the algorithm given inFigure 2.Binarization must ensure that the product of theprobabilities of the binarized pieces is the same asthat of the original rule.
The easiest way to dothis is to assign each newly-created binarized rulea probability of 1.0, and give the top-level rule thecomplete probability of the original rule.
In thefollowing subsection, we describe a better way.1The mean rule rank in a Treebank PCFG is 2.14, whilethe mean rank in our sampled TSG is 8.51.
See Table 1.NPa JJ NN NN PP??JJ:NN?:NN??JJ:NN?
NNJJ NN?a:??JJ:NN?:NN?
?aPPNPECBA1RuleNPthe JJ NN NNRule 2??JJ:NN?:NN??JJ:NN?
NNJJ NNNPtheABDFigure 1: A two-rule grammar.
The greedybinarization algorithm produces the binarizationshown, with the shared structure highlighted.
Bi-narized rules A, B, and C are initially assigneda probability of 1.0, while rules D and E are as-signed the original probabilities of rules 2 and 1,respectively.2.2 Weight pushingSpreading the weight of an original rule acrossits binarized pieces is complicated by sharing,because of the constraint that the probability ofshared binarized pieces must be set so that theproduct of their probabilities is the same as theoriginal rule, for each rule the shared piece partici-pates in.
Mohri (1997) introduced weight pushingas a step in the minimization of weighted finite-state transducers (FSTs), which addressed a sim-ilar problem for tasks employing finite-state ma-chinery.
At a high level, weight pushing movesthe weight of a path towards the initial state, sub-ject to the constraint that the weight of each pathin the FST is unchanged.
To do weight pushing,one first computes for each state q in the trans-ducer the shortest distance d(q) to any final state.Let ?
(q, a) be the state transition function, deter-ministically transitioning on input a from state q tostate ?
(q, a).
Pushing adjusts the weight of eachedge w(e) according to the following formula:w?
(e) = d(q)?1 ?
w(e)?
d(?
(q, a)) (1)Mohri (1997, ?3.7) and Mohri and Riley (2001)discuss how these operations can be applied us-ing various semirings; in this paper we use the(max,?)
semiring.
The important observation forour purposes is that pushing can be thought of as asequence of local operations on individual nodes901: function GREEDYBINARIZE(P )2: while RANK(P ) > 2 do3: ?
:= UPDATECOUNTS(P )4: for each rule X ?
x1x2 ?
?
?xr do5: b := argmaxi?(2??
?r) ?
[xi?1, xi]6: l := ?xb?1 : xb?7: add l ?
xb?1xb to P8: replace xb?1xb with l in rule9: function UPDATECOUNTS(P )10: ?
:= {} ?
a dictionary11: for each rule X ?
x1x2 ?
?
?xr ?
P do12: for i ?
(2 ?
?
?
r) do13: ?
[xi?1, xi]++return ?Figure 2: A greedy binarization algorithm.
Therank of a grammar is the rank of its largest rule.Our implementation updates the counts in ?
moreefficiently, but we present it this way for clarity.q, shifting a constant amount of weight d(q)?1from q?s outgoing edges to its incoming edges.Klein and Manning (2003) describe an encod-ing of context-free grammar rule binarization thatpermits weight pushing to be applied.
Their ap-proach, however, works only with left or right bi-narizations whose rules can be encoded as an FST.We propose a form of weight pushing that worksfor arbitrary binarizations.
Weight pushing acrossa grammar can be viewed as generalizing push-ing from weighted transducers to a certain kind ofweighted hypergraph.
To begin, we use the fol-lowing definition of a hypergraph:Definition.
A hypergraph H is a tuple?V,E, F,R?, where V is a set of nodes, E is aset of hyperedges, F ?
V is a set of final nodes,and R is a set of permissible weights on the hy-peredges.
Each hyperedge e ?
E is a triple?T (e), h(e), w(e)?, where h(e) ?
V is its headnode, T (e) is a sequence of tail nodes, and w(e) isits weight.We can arrange the binarized rules of Figure 1into a shared hypergraph forest (Figure 3), withnodes as nonterminals and binarized rules as hy-peredges.
We distinguish between final and non-final nodes and hyperedges.
Nonfinal nodes arethose in V ?F .
Nonfinal hyperdges ENF are thosein {e : h(e) ?
V ?
F}, that is, all hyperedgeswhose head is a nonfinal node.
Because all nodesintroduced by our binarization procedure expanddeterministically, each nonfinal node is the headof no more than one such hyperedge.
Initially, all0.6/1.00.4/0.67?1.0/0.61.0/1.01.0/1.0??JJ:NN?:NN??JJ:NN?
NNJJ NNNPthe?a:??JJ:NN?:NN?
?aPPFigure 3: The binarized rules of Figure 1 arrangedin a shared hypergraph forest.
Each hyperedge islabeled with its weight before/after pushing.nonfinal hyperedges have a probability of 1, and fi-nal hyperedges have a probability equal to the thatof the original unbinarized rule.
Each path throughthe forest exactly identifies a binarization of a rulein the original grammar, and hyperpaths overlapwhere binarized rules are shared.Weight pushing in this hypergraph is similar toweight pushing in a transducer.
We consider eachnonfinal node v in the graph and execute a localoperation that moves weight in some way from theset of edges {e : v ?
T (e)} (v?s outgoing hyper-edges) to the edge eh for which v = h(e) (v?sincoming hyperedge).A critical difference from pushing in trans-ducers is that a node in a hyperpath may beused more than once.
Consider adding the ruleNP?JJ NN JJ NN to the binarized two-rule gram-mar we have been considering.
Greedy binariza-tion could2 binarize it in the following mannerNP ?
?JJ:NN?
?JJ:NN??JJ:NN?
?
JJ NNwhich would yield the hypergraph in Figure 4.
Inorder to maintain hyperpath weights, a pushingprocedure at the ?JJ:NN?
node must pay attentionto the number of times it appears in the set of tailnodes of each outgoing hyperedge.2Depending on the order in which the argmax variable iof Line 5 from the algorithm in Figure 2 is considered.
Thisparticular binarization would not have been produced if thevalues 2 .
.
.
r were tested sequentially.910.6/1.00.3/0.51.0/0.61.0/1.01.0/1.00.1/0.27???JJ:NN?:NN??JJ:NN?
NNJJ NNNPthe?a:??JJ:NN?:NN?
?aPPFigure 4: A hypergraph containing a hyperpathrepresenting a rule using the same binarized piecetwice.
Hyperedge weights are again shown be-fore/after pushing.With these similarities and differences in mind,we can define the local weight pushing procedure.For each nonfinal node v in the hypergraph, wedefine eh as the edge for which h(e) = v (as be-fore), P = {e : v ?
T (e)} (the set of outgo-ing hyperedges), and c(v, T (e)) as the number oftimes v appears in the sequence of tail nodes T (e).The minimum amount of probability available forpushing is thenmax{ c(v,T (e))?w(e) : e ?
P} (2)This amount can then be multiplied into w(eh) anddivided out of each edge e ?
P .
This max is alower bound because we have to ensure that theamount of probability we divide out of the weightof each outgoing hyperedge is at least as large asthat of the maximum weight.While finite state transducers each have aunique equivalent transducer on which no furtherpushing is possible, defined by Equation 1, this isnot the case when operating on hypergraphs.
Inthis generalized setting, the choice of which tailnodes to push weight across can result in differ-ent final solutions.
We must define a strategy forchoosing among sequences of pushing operations,and for this we now turn to a discussion of thespecifics of our algorithm.2.3 AlgorithmWe present two variants.
Maximal pushing, analo-gous to weight pushing in weighted FSTs, pushesthe original rule?s weight down as far as pos-sible.
Analysis of interactions between pruning1: function DIFFUSEWEIGHTS(PBIN ,?
)2: R := bottom-up sort of PBIN3: for each rule r ?
R do4: r.pr := max{ c(r,p)?p.pr : p ?
?
(r)}5: for each rule p ?
?
(r) do6: p.pr := p.pr/r.prc(r,p)Figure 6: Maximal weight pushing algorithm ap-plied to a binarized grammar, PBIN .
?
is a dictio-nary mapping from an internal binary rule to a listof top-level binary rules that it appeared under.and maximal pushing discovered situations wheremaximal pushing resulted in search error (see?4.2).
To address this, we also discuss nthrootpushing, which attempts to distribute the weightmore evenly across its pieces, by taking advantageof the fact that Equation 2 is a lower bound on theamount of probability available for pushing.The algorithm for maximal pushing is listedin Figure 6, and works in the following manner.When binarizing we maintain, for each binarizedpiece, a list of all the original rules that shareit.
We then distribute that original rule?s weightby considering each of these binarized pieces inbottom-up topological order and setting the prob-ability of the piece to the maximum (remaining)probability of these parents.
This amount is thendivided out of each of the parents, and the processcontinues.
See Figure 5 for a depiction of this pro-cess.
Note that, although we defined pushing as alocal operation between adjacent hyperedges, it issafe to move probability mass from the top-leveldirectly to the bottom (as we do here).
Intuitively,we can imagine this as a series of local pushingoperations on all intervening nodes; the end resultis the same.For nthroot pushing, we need to maintain a dic-tionary ?
which records, for each binary piece, therank (number of items on the rule?s righthand side)of the original rule it came from.
This is accom-plished by replacing line 4 in Figure 6 withr.pr := max{ (?
(p)?1)?c(r,p)?p.pr : p ?
?
(r)}Applying weight pushing to a binarized PCFGresults in a grammar that is not a PCFG, be-cause rule probabilities for each lefthand sideno longer sum to one.
However, the tree dis-tribution, as well as the conditional distributionP(tree|string) (which are what matter for parsing)are unchanged.
To show this, we argue fromthe algorithm in Figure 6, demonstrating that, for92step A B C D E0 1.0 1.0 1.0 x y1 max(x, y) ?
?
xmax(x,y) ymax(x,y)2 ?
max(z1,D, z1,E) ?
z1,Dmax(z1,D,z1,E)z1,Dmax(z1,D,z1,E)3 ?
?
max(z2,D, z2,E) z2,Dmax(z2,D,z2,E)z2,Emax(z2,D,z2,E)4 ?
?
?
?
?Figure 5: Stepping through the maximal weight pushing algorithm for the binarized grammar in Figure 1.Rule labels A through E were chosen so that the binarized pieces are sorted in topological order.
A (?
)indicates a rule whose value has not changed from the previous step, and the value zr,c denotes the valuein row r column c.each rule in the original grammar, its probabilityis equal to the product of the probabilities of itspieces in the binarized grammar.
This invariantholds at the start of the algorithm (because theprobability of each original rule was placed en-tirely at the top-level rule, and all other pieces re-ceived a probability of 1.0) and is also true at theend of each iteration of the outer loop.
Considerthis loop.
Each iteration considers a single binarypiece (line 3), determines the amount of probabil-ity to claim from the parents that share it (line 4),and then removes this amount of weight from eachof its parents (lines 5 and 6).
There are two impor-tant considerations.1.
A binarized rule piece may be used more thanonce in the reconstruction of an original rule;this is important because we are assigningprobabilities to binarized rule types, but rulereconstruction makes use of binarized rule to-kens.2.
Multiplying together two probabilities resultsin a lower number: when we shift weight pfrom the parent rule to (n instances of) a bi-narized piece beneath it, we are creating anew set of probabilities pc and pp such thatpnc ?
pp = p, where pc is the weight placed onthe binarized rule type, and pp is the weightwe leave at the parent.
This means that wemust choose pc from the range [p, 1.0].3In light of these considerations, the weight re-moved from each parent rule in line 6 must begreater than or equal to each parent sharing thebinarized rule piece.
To ensure this, line 4 takes3The upper bound of 1.0 is set to avoid assigning a nega-tive weight to a rule.the maximum of the c(r, p)th root of each parent?sprobability, where c(r, p) is the number of timesbinarized rule token r appears in the binarizationof p.Line 4 breaks the invariant, but line 6 restores itfor each parent rule the current piece takes part in.From this it can be seen that weight pushing doesnot change the product of the probabilities of thebinarized pieces for each rule in the grammar, andhence the tree distribution is also unchanged.We note that, although Figures 3 and 4 showonly one final node, any number of final nodes canappear if binarized pieces are shared across differ-ent top-level nonterminals (which our implemen-tation permits and which does indeed occur).3 Experimental setupWe present results from four different grammars:1.
The standard Treebank probabilistic context-free grammar (PCFG).2.
A ?spinal?
tree substitution grammar (TSG),produced by extracting n lexicalized subtreesfrom each length n sentence in the trainingdata.
Each subtree is defined as the sequenceof CFG rules from leaf upward all sharing thesame lexical head, according to the Mager-man head-selection rules (Collins, 1999).
Wedetach the top-level unary rule, and add incounts from the Treebank CFG rules.3.
A ?minimal subset?
TSG, extracted and thenrefined according to the process defined inBod (2001).
For each height h, 2 ?
h ?
14,400,000 subtrees are randomly sampled fromthe trees in the training data, and the counts93rankgrammar # rules median mean maxPCFG 46K 1 2.14 51spinal 190K 3 3.36 51sampled 804K 8 8.51 70minimal 2,566K 10 10.22 62Table 1: Grammar statistics.
A rule?s rank is thenumber of symbols on its right-hand side.grammar unbinarized right greedyPCFG 46K 56K 51Kspinal 190K 309K 235Ksampled 804K 3,296K 1,894Kminimal 2,566K 15,282K 7,981KTable 2: Number of rules in each of the completegrammars before and after binarization.are summed.
From these counts we remove(a) all unlexicalized subtrees of height greaterthan six and (b) all lexicalized subtrees con-taining more than twelve terminals on theirfrontier, and we add all subtrees of height one(i.e., the Treebank PCFG).4.
A sampled TSG produced by inducingderivations on the training data using aDirichlet Process prior (described below).The sampled TSG was produced by inducing aTSG derivation on each of the trees in the train-ing data, from which subtree counts were read di-rectly.
These derivations were induced using acollapsed Gibbs sampler, which sampled from theposterior of a Dirichlet process (DP) defined overthe subtree rewrites of each nonterminal.
The DPdescribes a generative process that prefers smallsubtrees but occasionally produces larger ones;when used for inference, it essentially discoversTSG derivations that contain larger subtrees onlyif they are frequent in the training data, which dis-courages model overfitting.
See Post and Gildea(2009) for more detail.
We ran the sampler for 100iterations with a stop probability of 0.7 and the DPparameter ?
= 100, accumulating subtree countsfrom the derivation state at the end of all the itera-tions, which corresponds to the (100, 0.7,?
100)grammar from that paper.All four grammar were learned from all sen-tences in sections 2 to 21 of the Wall Street Journalportion of the Penn Treebank.
All trees were pre-processed to remove empty nodes and nontermi-NPNPDTaJJ NN NNPPFigure 7: Rule 1 in Figure 1 was produced byflattening this rule from the sampled grammar.nal annotations.
Punctuation was retained.
Statis-tics for these grammars can be found in Table 1.We present results on sentences with no more thanforty words from section 23.Our parser is a Perl implementation of the CKYalgorithm.4 For the larger grammars, memory lim-itations require us to remove from considerationall grammar rules that could not possibly take partin a parse of the current sentence, which we do bymatching the rule?s frontier lexicalization patternagainst the words in the sentence.
All unlexical-ized rules are kept.
This preprocessing time is notincluded in the parsing times reported in the nextsection.For pruning, we group edges into equivalenceclasses according to the following features:?
span (s, t) of the input?
level of binarization (0,1,2+)The level of binarization refers to the height of anonterminal in the subtree created by binarizing aCFG rule (with the exception that the root of thistree has a binarization level of 0).
The namingscheme used to create new nonterminals in line 6of Figure 2 means we can determine this level bycounting the number of left-angle brackets in thenonterminal?s name.
In Figure 1, binarized rulesD and E have level 0, C has level 3, B has level 2,and A has level 1.Within each bin, only the ?
highest-weightitems are kept, where ?
?
(1, 5, 10, 25, 50) is a pa-rameter that we vary during our experiments.
Tiesare broken arbitrarily.
Additionally, we maintain abeam within each bin, and an edge is pruned if itsscore is not within a factor of 10?5 of the highest-scoring edge in the bin.
Pruning takes place whenthe edge is added and then again at the end of each4It is available from http://www.cs.rochester.edu/?post/.94span in the CKY algorithm (but before applyingunary rules).In order to binarize TSG subtrees, we followBod (2001) in first flattening each subtree to adepth-one PCFG rule that shares the subtree?s rootnonterminal and leaves, as depicted in Figure 7.Afterward, this transformation is reversed to pro-duce the parse tree for scoring.
If multiple TSGsubtrees have identical mappings, we take only themost probable one.
Table 2 shows how grammarsize is affected by binarization scheme.We note two differences in our work that ex-plain the large difference between the scores re-ported for the ?minimal subset?
grammar in Bod(2001) and here.
First, we did not implement thesmoothed ?mismatch parsing?, which introducesnew subtrees into the grammar at parsing time byallowing lexical leaves of subtrees to act as wild-cards.
This technique reportedly makes a largedifference in parsing scores (Bod, 2009).
Second,we approximate the most probable parse with thesingle most probable derivation instead of the top1,000 derivations, which Bod also reports as hav-ing a large impact (Bod, 2003, ?4.2).4 ResultsFigure 8 displays search time vs. model score forthe PCFG and the sampled grammar.
Weightpushing has a significant impact on search effi-ciency, particularly for the larger sampled gram-mar.
The spinal and minimal graphs are similar tothe PCFG and sampled graphs, respectively, whichsuggests that the technique is more effective forthe larger grammars.For parsing, we are ultimately interested in ac-curacy as measured by F1 score.5 Figure 9 dis-plays graphs of time vs. accuracy for parses witheach of the grammars, alongside the numericalscores used to generate them.
We begin by notingthat the improved search efficiency from Figure 8carries over to the time vs. accuracy curves forthe PCFG and sampled grammars, as we expect.Once again, we note that the difference is less pro-nounced for the two smaller grammars than for thetwo larger ones.4.1 Model score vs. accuracyThe tables in Figure 9 show that parser accuracyis not always a monotonic function of time; someof the runs exhibited peak performance as early5F1 = 2?P ?RP+R , where P is precision and R recall.-340-338-336-334-332-330-328-326-324-322-3201 5 10 25 50model score(thousands)(greedy,max)(greedy,nthroot)(greedy,none)(right,max)(right,nthroot)(right,none)-370-360-350-340-330-320-310-300-2901 5 10 25 50model score(thousands)mean time per sentence (s)(greedy,max)(greedy,nthroot)(greedy,none)(right,max)(right,nthroot)(right,none)Figure 8: Time vs. model score for the PCFG (top)and the sampled grammar (bottom).
Note that they-axis range differs between plots.as at a bin size of ?
= 10, and then saw dropsin scores when given more time.
We examineda number of instances where the F1 score for asentence was lower at a higher bin setting, andfound that they can be explained as modeling (asopposed to search) errors.
With the PCFG, theseerrors were standard parser difficulties, such as PPattachment, which require more context to resolve.TSG subtrees, which have more context, are ableto correct some of these issues, but introduce a dif-ferent set of problems.
In many situations, largerbin settings permitted erroneous analyses to re-main in the chart, which later led to the parser?sdiscovery of a large TSG fragment.
Because thesefragments often explain a significant portion of thesentence more cheaply than multiple smaller rulesmultiplied together, the parser prefers them.
Moreoften than not, they are useful, but sometimes theyare overfit to the training data, and result in an in-correct analysis despite a higher model score.Interestingly, these dips occur most frequentlyfor the heuristically extracted TSGs (four of six9550556065707580851 5 10 25 50accuracymean time per sentence (s)(greedy,max)(greedy,none)(right,max)(right,none)PCFGrun 1 5 10 25 50 (g,m) 66.44 72.45 72.54 72.54 72.51u (g,n) 65.44 72.21 72.47 72.45 72.47N (g,-) 63.91 71.91 72.48 72.51 72.51 (r,m) 67.30 72.45 72.61 72.47 72.49e (r,n) 64.09 71.78 72.33 72.45 72.47?
(r,-) 61.82 71.00 72.18 72.42 72.4150556065707580851 5 10 25 50accuracymean time per sentence (s)(greedy,max)(greedy,none)(right,max)(right,none)spinalrun 1 5 10 25 50 (g,m) 68.33 78.35 79.21 79.25 79.24u (g,n) 64.67 78.46 79.04 79.07 79.09N (g,-) 61.44 77.73 78.94 79.11 79.20 (r,m) 69.92 79.07 79.18 79.25 79.05e (r,n) 67.76 78.46 79.07 79.04 79.04?
(r,-) 65.27 77.34 78.64 78.94 78.9050556065707580851 5 10 25 50accuracymean time per sentence (s)(greedy,max)(greedy,none)(right,max)(right,none)sampledrun 1 5 10 25 50 (g,m) 63.75 80.65 81.86 82.40 82.41u (g,n) 61.87 79.88 81.35 82.10 82.17N (g,-) 53.88 78.68 80.48 81.72 81.98 (r,m) 72.98 81.66 82.37 82.49 82.40e (r,n) 65.53 79.01 80.81 81.91 82.13?
(r,-) 61.82 77.33 79.72 81.13 81.7050556065707580851 5 10 25 50accuracymean time per sentence (s)(greedy,max)(greedy,none)(right,max)(right,none)minimalrun 1 5 10 25 50 (g,m) 59.75 77.28 77.77 78.47 78.52u (g,n) 57.54 77.12 77.82 78.35 78.36N (g,-) 51.00 75.52 77.21 78.30 78.13 (r,m) 65.29 76.14 77.33 78.34 78.13e (r,n) 61.63 75.08 76.80 77.97 78.31?
(r,-) 59.10 73.42 76.34 77.88 77.91Figure 9: Plots of parsing time vs. accuracy for each of the grammars.
Each plot contains four sets of fivepoints (?
?
(1, 5, 10, 25, 50)), varying the binarization strategy (right (r) or greedy (g)) and the weightpushing technique (maximal (m) or none (-)).
The tables also include data from nthroot (n) pushing.9650556065707580851 5 10 25 50accuracy(right,max)(right,nthroot)(right,none)50556065707580851 5 10 25 50accuracymean time per sentence (s)(greedy,max)(greedy,nthroot)(greedy,none)Figure 10: Time vs. accuracy (F1) for the sampledgrammar, broken down by binarization (right ontop, greedy on bottom).runs for the spinal grammar, and two for the min-imal grammar) and for the PCFG (four), and leastoften for the model-based sampled grammar (justonce).
This may suggest that rules selected by oursampling procedure are less prone to overfitting onthe training data.4.2 PushingFigure 10 compares the nthroot and maximalpushing techniques for both binarizations of thesampled grammar.
We can see from this figurethat there is little difference between the two tech-niques for the greedy binarization and a large dif-ference for the right binarization.
Our original mo-tivation in developing nthroot pushing came as aresult of analysis of certain sentences where max-imal pushing and greedy binarization resulted inthe parser producing a lower model score thanwith right binarization with no pushing.
One suchexample was binarized fragment A from Fig-ure 1; when parsing a particular sentence in thedevelopment set, the correct analysis required therule from Figure 7, but greedy binarization andmaximal pushing resulted in this piece gettingpruned early in the search procedure.
This pruninghappened because maximal pushing allowed toomuch weight to shift down for binarized pieces ofcompeting analyses relative to the correct analy-sis.
Using nthroot pushing solved the search prob-lem in that instance, but in the aggregate it doesnot appear to be helpful in improving parser effi-ciency as much as maximal pushing.
This demon-strates some of the subtle interactions between bi-narization and weight pushing when inexact prun-ing heuristics are applied.4.3 BinarizationSong et al (2008, Table 4) showed that CKY pars-ing efficiency is not a monotonic function of thenumber of constituents produced; that is, enumer-ating fewer edges in the dynamic programmingchart does not always correspond with shorter runtimes.
We see here that efficiency does not al-ways perfectly correlate with grammar size, ei-ther.
For all but the PCFG, right binarizationimproves upon greedy binarization, regardless ofthe pushing technique, despite the fact that theright-binarized grammars are always larger thanthe greedily-binarized ones.Weight pushing and greedy binarization both in-crease parsing efficiency, and the graphs in Fig-ures 8 and 9 suggest that they are somewhat com-plementary.
We also investigated left binarization,but discontinued that exploration because the re-sults were nearly identical to that of right bina-rization.
Another popular binarization approachis head-outward binarization.
Based on the anal-ysis above, we suspect that its performance willfall somewhere among the binarizations presentedhere, and that pushing will improve it as well.
Wehope to investigate this in future work.5 SummaryWeight pushing increases parser efficiency, espe-cially for large grammars.
Most notably, it im-proves parser efficiency for the Gibbs-sampledtree substitution grammar of Post and Gildea(2009).We believe this approach could alo bene-fit syntax-based machine translation.
Zhang etal.
(2006) introduced a synchronous binariza-tion technique that improved decoding efficiencyand accuracy by ensuring that rule binarizationavoided gaps on both the source and target sides97(for rules where this was possible).
Their binariza-tion was designed to share binarized pieces amongrules, but their approach to distributing weight wasthe default (nondiffused) case found in this paperto be least efficient: The entire weight of the orig-inal rule is placed at the top binarized rule and allinternal rules are assigned a probability of 1.0.Finally, we note that the weight pushing algo-rithm described in this paper began with a PCFGand ensured that the tree distribution was notchanged.
However, weight pushing need not belimited to a probabilistic interpretation, but couldbe used to spread weights for grammars with dis-criminatively trained features as well, with neces-sary adjustments to deal with positively and nega-tively weighted rules.Acknowledgments We thank the anonymousreviewers for their helpful comments.
This workwas supported by NSF grants IIS-0546554 andITR-0428020.ReferencesRens Bod.
2001.
What is the minimal set of fragmentsthat achieves maximal parse accuracy?
In Pro-ceedings of the 39th Annual Conference of the As-sociation for Computational Linguistics (ACL-01),Toulouse, France.Rens Bod.
2003.
Do all fragments count?
NaturalLanguage Engineering, 9(4):307?323.Rens Bod.
2009.
Personal communication.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 2000 Meet-ing of the North American chapter of the Associationfor Computational Linguistics (NAACL-00), Seattle,Washington.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProceedings of the 38th Annual Conference of theAssociation for Computational Linguistics (ACL-00), Hong Kong.Michael John Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia.John DeNero, Mohit Bansal, Adam Pauls, and DanKlein.
2009.
Efficient parsing for transducer gram-mars.
In Proceedings of the 2009 Meeting of theNorth American chapter of the Association for Com-putational Linguistics (NAACL-09), Boulder, Col-orado.Liang Huang.
2007.
Binarization, synchronous bi-narization, and target-side binarization.
In NorthAmerican chapter of the Association for Computa-tional Linguistics Workshop on Syntax and Struc-ture in Statistical Translation (NAACL-SSST-07),Rochester, NY.Dan Klein and Christopher D. Manning.
2003.
A*parsing: Fast exact Viterbi parse selection.
In Pro-ceedings of the 2003 Meeting of the North Americanchapter of the Association for Computational Lin-guistics (NAACL-03), Edmonton, Alberta.Mehryar Mohri and Michael Riley.
2001.
A weightpushing algorithm for large vocabulary speechrecognition.
In European Conference on SpeechCommunication and Technology, pages 1603?1606.Mehryar Mohri.
1997.
Finite-state transducers in lan-guage and speech processing.
Computational Lin-guistics, 23(2):269?311.Matt Post and Daniel Gildea.
2009.
Bayesian learningof a tree substitution grammar.
In Proceedings of the47th Annual Meeting of the Association for Compu-tational Linguistics (ACL-09), Suntec, Singapore.Helmut Schmid.
2004.
Efficient parsing of highly am-biguous context-free grammars with bit vectors.
InProceedings of the 20th International Conference onComputational Linguistics (COLING-04), Geneva,Switzerland.Xinying Song, Shilin Ding, and Chin-Yew Lin.
2008.Better binarization for the CKY parsing.
In 2008Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP-08), Honolulu, Hawaii.Hao Zhang, Liang Huang, Daniel Gildea, and KevinKnight.
2006.
Synchronous binarization for ma-chine translation.
In Proceedings of the 2006 Meet-ing of the North American chapter of the Associ-ation for Computational Linguistics (NAACL-06),New York, NY.98
