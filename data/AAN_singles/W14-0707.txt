Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 48?57,Gothenburg, Sweden, April 26, 2014.c?2014 Association for Computational LinguisticsRecognizing Causality in Verb-Noun Pairsvia Noun and Verb SemanticsMehwish Riaz and Roxana GirjuDepartment of Computer Science and Beckman InstituteUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801, USA{mriaz2,girju}@illinois.eduAbstractSeveral supervised approaches have beenproposed for causality identification by re-lying on shallow linguistic features.
How-ever, such features do not lead to improvedperformance.
Therefore, novel sourcesof knowledge are required to achieveprogress on this problem.
In this paper,we propose a model for the recognition ofcausality in verb-noun pairs by employingadditional types of knowledge along withlinguistic features.
In particular, we fo-cus on identifying and employing seman-tic classes of nouns and verbs with hightendency to encode cause or non-cause re-lations.
Our model incorporates the in-formation about these classes to minimizeerrors in predictions made by a basic su-pervised classifier relying merely on shal-low linguistic features.
As compared withthis basic classifier our model achieves14.74% (29.57%) improvement in F-score(accuracy), respectively.1 IntroductionThe automatic detection of causal relations is im-portant for various natural language processing ap-plications such as question answering, text sum-marization, text understanding and event predic-tion.
Causality can be expressed using various nat-ural language constructions (Girju and Moldovan,2002; Chang and Choi, 2006).
Consider the fol-lowing examples where causal relations are en-coded using (1) a verb-verb pair, (2) a noun-nounpair and (3) a verb-noun pair.1.
Five shoppers were killed when a car blew upat an outdoor market.2.
The attack on Kirkuk?s police intelligencecomplex sees further deaths after violencespilled over a nearby shopping mall.3.
At least 1,833 people died in hurricane.Since, the task of automatic recognition ofcausality is quite challenging, researchers haveaddressed this problem by considering specificconstructions.
For example, various modelshave been proposed to identify causation betweenverbs (Bethard and Martin, 2008; Beamer andGirju, 2009; Riaz and Girju, 2010; Do et al., 2011;Riaz and Girju, 2013) and between nouns (Girjuand Moldovan, 2002; Girju, 2003).
Do et al.
(2011) have worked with verb-noun pairs forcausality detection but they focused only on asmall list of predefined nouns representing events.In this paper, we focus on the task of identi-fying causality encoded by verb-noun pairs (ex-ample 3).
We propose a novel model which firstpredicts cause or non-cause relations using a su-pervised classifier and then incorporates additionaltypes of knowledge to reduce errors in predictions.Using a supervised classifier, our model identi-fies causation by employing shallow linguistic fea-tures (e.g., lemmas of verb and noun, words be-tween verb and noun).
Such features have beenused successfully for various NLP tasks (e.g., part-of-speech tagging, named entity recognition, etc.
)but confinement to such features does not helpmuch to achieve performance for identifying cau-sation (Riaz and Girju, 2013).
Therefore, in ourmodel we plug in additional types of knowledgeto obtain better predictions for the current task.For example, we identify the semantic classes ofnouns and verbs with high tendency to encodecause or non-causal relations and use this knowl-edge to achieve better performance.
Specifically,the contributions of this paper are as follows:?
In order to build a supervised classifier, weuse the annotations of FrameNet to generate atraining corpus of verb-noun instances encod-ing cause and non-cause relations.
We proposea set of linguistic features to learn and identifycausal relations.48?
In order to make intelligent predictions, it isimportant for our model to have knowledgeabout the semantic classes of nouns with hightendency to encode causal or non-causal re-lations.
For example, a named entity suchas person, organization or location may havehigh tendency to encode non-causality unless ametonymic reading is associated with it.
In ourapproach, we identify such semantic classes ofnouns by exploiting a named entity recognizer,the annotations of frame elements provided inFrameNet and WordNet.?
Verbs are the important components of lan-guage for expressing events of various types.For example, Pustejovsky et al.
(2003)have classified events into eight semanticclasses: OCCURRENCE, PERCEPTION, RE-PORTING, ASPECTUAL, STATE, I STATE,I ACTION and MODAL.
We argue that thereare some semantic classes in this list with hightendency to encode cause or non-cause rela-tions.
For example, reporting events repre-sented by verbs say, tell, etc., have high ten-dency to just report other events instead of en-coding causality with them.
In our model, weuse such information to reduce errors in predic-tions.?
Each causal relation is characterized by tworoles i.e., cause and its effect.
In example 3above, the noun ?hurricane?
is cause and theverb ?died?
is its effect.
However, a verb-nounpair may not encode causality when a verband a noun represent same event.
For exam-ple, in instance ?Colin Powell presented fur-ther evidence in his presentation.
?, the verb?presented?
and the noun ?presentation?
rep-resent same event of ?presenting?
and thus en-coding non-cause relation with each other.
Inour model, we determine the verb-noun pairsrepresenting same or distinct events to makepredictions accordingly.?
We adopt the framework of Integer Linear Pro-gramming (ILP) (Roth and Yih, 2004; Do et al.,2011) to combine all the above types of knowl-edge for the current task.This paper is organized as follows.
In next sec-tion, we briefly review the previous research donefor identifying causality.
We introduce our modeland evaluation with discussion on results in sec-tion 3 and 4, respectively.
The section 5 of thepaper concludes our current research.2 Related WorkIn computational linguistics, researchers have al-ways shown interest in the task of automaticrecognition of causal relations because success onthis task is critical for various natural languageapplications (Girju, 2003; Chklovski and Pantel,2004; Radinsky and Horvitz, 2013).Following the successful employment of lin-guistic features for various tasks (e.g., part-of-speech tagging, named entity recognition, etc.
),initially NLP researchers proposed approaches re-lying mainly on such features to identify causal-ity (Girju, 2003; Bethard and Martin, 2008;Sporleder and Lascarides, 2008; Pitler andNenkova, 2009; Pitler et al., 2009).
However,researchers have recently shifted their attentionfrom these features and tried to consider othersources of knowledge for extracting causal rela-tions (Beamer and Girju, 2009; Riaz and Girju,2010; Do et al., 2011; Riaz and Girju, 2013).For example, Riaz and Girju (2010) and Do etal.
(2011) have proposed unsupervised metrics forlearning causal dependencies between two events.Do et al.
(2011) have also incorporated minimalsupervision with unsupervised metrics.
For a pairof events (a, b), their model makes the decision ofcause or non-cause relation based on unsupervisedco-occurrence counts and then improves this deci-sion by using minimal supervision from the causaland non-causal discourse markers (e.g., because,although, etc.
).In search of novel and effective types of knowl-edge to identify causation between two verbalevents, Riaz and Girju (2013) have proposed amodel to learn a Knowledge Base (KBc) of verb-verb pairs.
In this knowledge base, the Englishlanguage verb-verb pairs are automatically clas-sified into three categories: (1) Strongly Causal,(2) Ambiguous and (2) Strongly Non-Causal.
TheStrongly Causal and Strongly Non-Causal cate-gories contain verb-verb pairs with highest andleast tendency to encode causality, respectivelyand rest of the verb-verb pairs are considered am-biguous with tendency to encode both types ofrelations.
They claim that this knowledge baseof verb-verb pairs is a rich source of causal as-sociations.
The incorporation of this resourceinto a causality detection model can help identi-fying causality with better performance.
In thisresearch, we also try to go beyond the scope ofshallow linguistic features and identify additional49interesting types of knowledge for the current task.3 Computational Model for IdentifyingCausalityIn this section, we introduce our model for iden-tifying causality encoded by verb-noun pairs.Specifically, we extract all main verbs and nounphrases from a sentence and predict cause or non-cause relation on verb-noun phrase (v-np) pairs.In order to make task easier, we consider onlythose v-np pairs where v (verb) is grammaticallyconnected to np (noun phrase).
We assume that a vand np are grammatically connected if there existsa dependency relation between them in the depen-dency tree.
We apply a dependency parser (Marn-effe et al., 2006) to identify such dependencies.Our model first employs a supervised classifier re-lying on linguistic features to make binary predic-tions (i.e., does a verb-noun phrase pair encode acause or non-cause relation?).
We then incorpo-rate additional types of knowledge on top of thesebinary predictions to improve performance.3.1 Supervised ClassifierIn this section, we propose a basic supervisedclassifier to identify causation encoded by v-nppairs.
To set up this supervised classifier, we needa training corpus of instances of v-np pairs en-coding cause and non-cause relations.
For thispurpose, we employ the annotations of FrameNetproject (Baker et al., 1998) provided for verbs.For example, consider the following annotationfrom FrameNet for the verb ?dying?
with ar-gument ?solvent abuse?
where the pair ?dying-solvent abuse?
encodes causality.A campaign has started to try to cut therising number of children dying [causefrom solvent abuse].To generate a training corpus, we collect anno-tations of verbs from FrameNet s.t.
the annotatedelement (aka.
frame element) is a noun phrase.For example, we get a causal training instance of?dying-solvent abuse?
pair from the above anno-tation.
We assume that if a FrameNet?s annotatedelement contains a verb in it then this may not rep-resent a training instance of v-np pair.
For exam-ple, we do not consider the following annotationin our training corpus where causality is encodedbetween two verbs i.e., ?died-fell?.A fitness fanatic died [causewhen 26stone of weights fell on him as he ex-ercised].After extracting training instances fromFrameNet, we assign them cause (c) and non-cause (?
c) labels.
We manually examined theinventory of labels of FrameNet and use thefollowing scheme to assign the c or ?c to eachtraining instance.
All the annotations of FrameNetwith following labels are considered as causaltraining instances and rest of the annotations areconsidered as non-causal training instances.Purpose, Internal cause, Result, Exter-nal cause, Cause, Reason, Explanation,Required situation, Purpose of Event,Negative consequences, resulting ac-tion, Effect, Cause of shine, Purpose ofGoods, Response action, Enabled situa-tion, Grinding cause, TriggerFor this work, we have acquired 2, 158(65, 777) cause (non-cause) training instancesfrom FrameNet.
Since, the non-cause instancesare very large in number, our supervised modeltends to assign non-cause labels to almost all in-stances.
Therefore, we employ equal number ofcause and non-cause instances for training.
In fu-ture, we plan to extract more annotations fromthe FrameNet and employ more than one humanannotators to assign the labels of cause and non-cause relations to the full inventory of labels ofFrameNet.?
Lexical Features: verb, lemma of verb, nounphrase, lemma of all words of noun phrase,head noun of noun phrase, lemmas of all wordsbetween verb and head noun of noun phrase.?
Semantic Features: We adopted this featurefrom Girju (2003) to capture the semantics ofnouns.
The 9 noun hierarchies of WordNet i.e.,entity, psychological feature, abstraction, state,event, act, group, possession, phenomenon areused as this feature.
Each of these hierarchiesis set to 1 if any sense of the head noun of nounphrase lies in that hierarchy otherwise set to 0.?
Structural Features: This feature is appliedby considering both subject (i.e., sub in np)and object (i.e., obj in np) of a verb.
For ex-ample, for a v-np pair the variable sub in np isset to 1 if the subject of v is contained in np,set to 0 if the subject of v is not contained innp and set to -1 if the subject of v is not avail-able in the instance.
The subject and object ofa verb are its core arguments and may some-time be part of an event represented by a verb.Therefore, these argument may have high ten-dency to encode non-cause relations.50We set up the following integer linear programafter acquiring predictions of c and ?
c labels us-ing our supervised classifier.Z1= max?v-np?I?l?L1x1(v-np, l)P (v-np, l) (1)?l?L1x1(v-np, l) = 1 ?
v-np ?
I (2)x1(v-np, l) ?
{0, 1} ?
v-np ?
I ?l ?
L1(3)Here L1= {c,?c}, I is the set of all instancesof v-np pairs and x1(v-np, l) is the decision vari-able set to 1 only if the label l ?
L1is assignedto v-np.
The Equation 2 constraints that only onelabel out of |L1| choices can be assigned to a v-nppair.
The equation 3 requires x1(v-np, l) to be abinary variable.
Specifically, we try to maximizethe objective function Z1(equation 1) which as-signs the label cause or non-cause to all v-np pairs(i.e., set the variables x1(v-np, l) to 1 or 0 for alll ?
L1and for all v-np pairs in I) dependingon the probabilities of assignment of labels (i.e.,P (v-np, l))1.
These probabilities can be obtainedby running a supervised classification algorithm(e.g., Naive Bayes and Maximum Entropy).
In ourexperiments, we provide results using the follow-ing probabilities acquired with Naive Bayes.P (v-np, c) = 1.0?
?nk=1logP (fk| c)?nk=1?l?
(c,?c)logP (fk| l)P (v-np,?c) = 1.0?
P ((v, np), c) (4)where fkis a feature, n is total number of fea-tures and P (fk| l) is the smoothed probability ofa feature fkgiven the training instances of label l.3.2 Knowledge of Semantic classes of nounsPhilospher Jaegwon Kim (Kim, 1993) (as cited byGirju and Moldovan (2002)) pointed out that theentities which represent either causes or effects areoften events, but also conditions, states, phenom-ena, processes, and sometimes even facts.
There-fore, according to this our model should haveknowledge of the semantic classes of noun phraseswith high tendency to encode cause or non-causerelations.
Considering this type of knowledge, wecan automatically review and correct the wrongpredictions made by our basic supervised classi-fier.1We use the integer linear program solver available athttp://sourceforge.net/projects/lpsolve/We argue that if a noun phrase represents anamed entity then it can have least tendency to en-code causal relations unless there is a metonymicreading associated with it.
For example, con-sider the following cause and non-cause exampleswhere noun phrase is a named entity.4.
Sandy hit Cuba as a Category 3 hurricane.5.
Almost all the weapon sites in Iraq were de-stroyed by the United States.In example 4, Cuba is location and does notencode causality.
However, in example 5 thepair ?destroyed-the United States?
encode causal-ity where a metonymic reading is associated withthe location.
We apply Named Entity Recog-nizer (Finkel et al., 2005) and assume if a nounphrase is identified as a named entity then its cor-responding verb-noun phrase pair encodes non-cause relation.
This constraint can lead to a falsenegative prediction when the metonymic readingis associated with a noun phrase.
In order toavoid as much false negatives as possible, we im-ply the following simple rule i.e., if one of thefollowing cue words appear between a verb and anoun phrase then do not apply the constraint statedabove.by, from, because of, through, forIn our experiments, the above simple rule helpsavoiding some false negatives but in future anysubsequent improvement with a better metonomyresolver (Markert and Nissim, 2009) should im-prove the performance of our model.In addition to named entities, there can be var-ious noun phrases with least tendency to encodecausation.
Consider the following example, where?city?
is a location and does not encode cause-effect relation with the verb ?remained?.Substantially fewer people remained inthe city during the Hurricane Ivan evac-uation.In this work, we identify the semantic classesof noun phrases which do not normally representevents, conditions, states, phenomena, processesand thus have high tendency to encode non-causerelations.
For this purpose, we manually examinethe inventory of labels assigned to noun phrasesin FrameNet (see table 1) and classify these labelsinto two classes (cnp and ?cnp).
Here, the classcnp(?cnp) represents the labels of noun phraseswith high (less) tendency to encode cause-effectrelations.
For example, the label ?Place?
?
?cnp51(see table 1) represents a location and it may haveleast tendency to encode causality if metonymy isnot associated with it.
Using the classification offrame elements in table 1, we obtain the annota-tions of noun phrases from FrameNet and catego-rize these annotations into cnpand ?cnpclasses.On top of the annotations of these two semanticclasses, we build a supervised classifier for pre-dicting cnpor ?cnplabel for the noun phrases.After obtaining predictions, we select all nounphrases lying in class ?cnpand apply the sameconstraint stated above for the named entities.
Weuse the following set of features to set up a super-vised classifier for cnpand ?cnplabels.?
Lexical Features: words of noun phrase, lem-mas of all words of noun phrase, head wordof noun phrase, first two (three) (four) lettersof head noun of noun phrase, last two, (three)(four) letters of head noun of noun phrase.?
Word Class Features: part-of-speech tags ofall words of noun phrase, part-of-speech tag ofhead noun of noun phrase.?
Semantic Features: all (frequent) sense(s) ofhead noun of noun phrase.We have acquired 23,334 (81,279) training in-stances of cnp(?cnp) class, respectively for thiswork.
We also use WordNet to obtain more train-ing instances of these classes.
We follow the ap-proach similar to Girju and Moldovan (2002) andadopt some senses of WordNet (shown in table 1)to acquire training instances of noun phrases.
Forexample, considering the table 1, we assign ?cnplabel to any noun whose all senses in WordNet liein the semantic hierarchy originated by the sense{time period, period of time, period}.
Follow-ing this scheme, we extract instances of nouns andnoun phrases from English GigaWord corpus andassign the labels cnpand ?cnpto them by em-ploying WordNet senses given in table 1.
Girjuand Moldovan (2002) have used similar schemeto rank noun phrases according to their tendencyto encode causation.
In comparison to them, weuse the WordNet senses to increase the size ofour training set of noun phrases obtained usingFrameNet above.
In addition to this, we build aautomatic classifier on the training data obtainedusing labels of FrameNet and WordNet senses toclassify noun phrases of test instances into two se-mantics classes (i.e., cnpand ?cnp).
In our train-ing corpus of there are 2, 214, 68 instances of nounphrases (50% belongs to each of cnpand ?cnpclasses).We incorporate the knowledge of semantics ofnouns in our model by making the following ad-ditions to the integer linear program introduced insection 3.1.Z2= Z1+?np:v-np?I?l?L2x2(np, l)P (np, l) (5)?l?L2x2(np, l) = 1 ?
np : v-np ?
I ?M (6)x2(np, l) ?
{0, 1} ?
np : v-np ?
I ?M (7)?l ?
L2x1(v-np,?c)?
x2(np,?cnp) ?
0 (8)?
np : v-np, ?
v-np ?
I ?MHere L2= {cnp,?cnp} and M is the set ofinstances of those v-np pairs for which we con-sider the possibility of attachment of metonymicreading with np, x2(np, l) is the decision variableset to 1 only if the label l ?
L2is assigned tonp.
The Equation 6 constraints that only one la-bel out of |L2| choices can be assigned to a np.The equation 7 requires x2(np, l) to be a binaryvariable.
The constraint 8 assumes that if an npbelongs to the semantic class ?cnpthen its corre-sponding pair v-np is assigned the label ?c.
Wemaximize the objective function Z2(equation 5)of our integer linear program subject to the con-straints introduced above.
We predict the semanticclass of a noun phrase using the supervised classi-fier for cnpand ?cnpclasses and set the probabil-ities i.e., P (np, l) = 1, P (np, {L2} ?
{l}) = 0 ifthe label l ?
L2is assigned to np.
Again we useNaive Bayes to predict the labels for noun phrases.Also before running this supervised classifier, werun the named entity recognizer and assign ?cnplabels to all noun phrases identified as named en-tities.
For our model, we apply named entity rec-ognizer for seven classes i.e., LOCATION, PER-SON, ORGANIZATION, DATE, TIME, MONEY,PERCENT (Finkel et al., 2005).3.3 Knowledge of Semantic classes of verbsIn this section, we introduce our method to in-corporate the knowledge of semantic classes ofverbs to identify causation.
Verbs are the com-ponents of language for expressing events of var-ious types.
In TimeBank corpus, Pustejovsky etal.
(2003) have introduced eight semantic classesof events i.e., OCCURRENCE, PERCEPTION,REPORTING, ASPECTUAL, STATE, I STATE,I ACTION and MODAL.
According to the defi-nitions of these classes provided by Pustejovsky52SemanticClassFrameNet Labels WordNet SensescnpEvent, Goal, Purpose, Cause, Internalcause, External cause, Result, Means, Rea-son, Phenomena{act, deed, human action, human activity},{phenomenon}, {state}, {psychologicalfeature}, {event}, {causal agent, cause,causal agency}?cnpArtist, Performer, Duration, Time, Place,Distributor, Area, Path, Direction, Sub-region{time period, period of time, period},{measure, quantity, amount}, {group,grouping}, {organization, organisation},{time unit, unit of time}, {clock time, time}Table 1: This table presents some examples of FrameNet labels in cnpand ?cnpclasses.
The full set oflabels in both semantic classes are given in appendix A.
It also presents the WordNet senses of nounslying in cnpand ?cnpclasses.et al.
(2003), the reporting events describe theaction of a person, declare something or narratean event e.g., the reporting events represented byverbs say, tell, etc.
Here, we argue that a reportingevent has the least tendency to encode causationbecause such an event only describes or narratesanother event instead of encoding causality withit.
We assume that the verbs representing report-ing events have least tendency to encode causa-tion and thus their corresponding v-np pairs haveleast tendency to encode causation.
To add thisknowledge to our model, we consider two classesof verbs i.e., cvand ?cvwhere the class cv(?cv)contains the verbs with high (less) tendency to en-code causation.
Using above argument we claimthat all verbs representing reporting events belong?cvclass and verbs representing rest of the typesof events belong to cvclass.
We build a supervisedclassifier which automatically classifies verbs intocvand ?cvclasses.
We extract the instances ofverbal events (i.e., verbs or verbal phrases) fromTimeBank corpus and assign the labels cvand?cvto these instances.
Using these labeled in-stances, we build a supervised classifier by adopt-ing the same set features as introduced in Bethardand Martin (2006) to identify semantic classes ofverbs.
Due to space constraint, we refer the readerto Bethard and Martin (2006) for the details of fea-tures.
Again we use Naive Bayes to take predic-tions of cvand ?cvlabels and their correspondingprobabilities using equation 4.We incorporate the knowledge of semantics ofverbs in our model by making the following addi-tions to the integer linear program.Z3= Z2+?v:v-np?I?l?L3x3(v, l)P (v, l) (9)?l?L3x3(v, l) = 1 ?
v : v-np ?
I (10)x3(v, l) ?
{0, 1} ?
v : v-np ?
I ?l ?
L3(11)x1(v-np,?c)?
x3(v,?cv) >= 0 (12)?
v : v-np, ?
v-np ?
Ix3(v, cv)?
x1(v-np, c) >= 0 (13)?
v : v-np, ?
v-np ?
IHere L3= {cv,?cv}, x3(v, l) is the decisionvariable set to 1 only if the label l ?
L3is as-signed to v. The Equation 10 constraints that onlyone label out of |L3| choices can be assigned toa v. The equation 11 requires x3(v, l) to be a bi-nary variable.
The constraint 12 assumes if a verbv belongs to the class cv(i.e., has least potentialto encode causation) then its corresponding pairv-np encodes non-causality.
The constraint 12 en-forces that if a verb v belongs to the class ?cvthenits corresponding v-np pair is assigned the label?c.
Similarly, the constraint 16 enforces that ifa v-np pair encodes causality then its verb v haspotential to encode causal relation.
We maximizethe objective function Z3subject to the constraintsintroduced above.3.4 Knowledge of Indistinguishable Verb andNounAs introduced earlier, each causal relation is char-acterized by two roles i.e., cause and its effect.
Inorder to encode causal relation, two componentsof an instance of verb-noun phrase pair need torepresent distinct events, processes or phenomena.Employing simple lexical matching, we determineif a verb and a noun phrase represent same eventor not as follows:53?
We use NOMLEX (Macleod et al., 2009) totransform a verb into its corresponding nomi-nalization and use the following text segmentsfor lexical matching.Tv= [Subject] verb [Object]2Tn= Head noun of noun phrase?
We remove stopwords and duplicate wordsfrom Tvand Tnand take lemmas of all words.If the subject or object or both arguments arecontained in noun phrase then we remove thesearguments from Tv.
We determine the proba-bility of a verb (v) and a noun phrase (np) rep-resenting same event as follows.
If head noun(i.e., Tn) lexically matches with any word of Tvthen set P(v ?
np) to 1 and 0 otherwise.We assign non-cause relation if P(v ?
np) = 1.Next, we incorporate the knowledge of indistin-guishable verb and noun in our model using thefollowing additions to our integer linear program.Z4= Z3+?v-np?I?l?L4x4(v-np, l)P (v-np, l) (14)?l?L4x4(v-np, l) = 1 ?
v-np ?
I (15)x4(v-np, l) ?
{0, 1} ?
v-np ?
I, ?l ?
L4x1(v-np,?c)?
x4(v-np,?)
?
0 (16)?
v-np ?
IHere L4= {?, 6?}
where the label ?
(6?)
rep-resents same (distinct) events, x4(v-np, l) is thedecision variable set to 1 only if the label l ?
L4is assigned to v-np.
The Equation 15 constraintsthat only one label out of |L4| choices can be as-signed to a v-np pair.
The equation 16 requiresx4(v-np, l) to be a binary variable.
The con-straint 16 enforces that if a v-np pair belongs tothe class ?
then this pair is assigned the label ?c.We maximize the objective function Z4subject tothe constraints introduced above.4 Evaluation and DiscussionIn this section we present the experiments, evalu-ation procedures, and a discussion on the resultsachieved through our model for the current task.In order to evaluate our model, we generated atest set with instances of form verb-noun phrasewhere the verb is grammatically connected to thenoun phrase in an instance.
For this purpose, we2Following Riaz ang Girju (2010), we assume that thesubject and object of a verb are parts of an event representedby a verb.
Therefore, we use these arguments along with averb for lexical matching with a noun phrase.collected three wiki articles on the topics of Hurri-cane Katrina, Iraq War and Egyptian Revolutionof 2011.
We selected first 100 sentences fromthese articles and applied part-of-speech tagger(Toutanova et al., 2003) and dependency parser(Marneffe et al., 2006) on these sentences.
Usingeach sentence, we extracted all verb-noun phrasepairs where the verb has a dependency relationwith any word of noun phrase.
We manually in-spected all of the extracted instances and removedthose instances in which a word had been wronglyclassified as a verb by the part-of-speech tagger.There are total 1106 instances in our test set.
Weassigned the task of annotation of these instanceswith cause and non-cause relations to a humanannotator.
Using manipulation theory of causal-ity (Woodward, 2008), we adopted the annotationguidelines from Riaz and Girju (2010) which is asfollows: ?Assign cause label to a pair (a, b), if thefollowing two conditions are satisfied: (1), a tem-porally precedes/overlap b in time, (2) while keep-ing as many state of affairs constant as possible,modifying a must entail predictably modifying b.Otherwise assign non-cause label.
?We have 149 (957) cause (non-cause) instancesin our test set3, respectively.
We evaluate the per-formance of our model using F-score and accuracyevaluation measures (see table 2 for results).The results in table 2 reveal that the ba-sic supervised classifier is a naive model andachieves only 27.27% F-score and 46.47% ac-curacy.
The addition of novel types ofknowledge introduced in section 3 (i.e., themodel Basic+SCNM+SCV+IVN) brings 14.74%(29.57%) improvements in F-score (accuracy), re-spectively.
These results show that the knowledgeof semantics of nouns and verbs and the knowl-edge of indistinguishable verb and noun are crit-ical to achieve performance.
The maximum im-provement in results is achieved with the additionof semantic classes of nouns (i.e., Basic+SCNM).The consideration of association of metonymicreadings using model Basic+SCNMhelps us tomaintain recall as compared with SCN!Mandtherefore brings better F-score.One can notice that almost all models sufferfrom low precision which leads to lower F-scores.Although, our model achieves 14.58% increase inprecision over basic supervised classifier, the lackof high precision is still responsible for lower F-3We will make the test set available54Model Basic +SCN!M+SCNM+SCNM+SCV +SCNM+SCV+IVNAccuracy 46.47 75.76 74.41 75.31 76.04Precision 16.69 28.14 29.53 30.47 31.27Recall 74.49 50.66 64.00 64.00 64.00F-score 27.27 39.19 40.42 41.29 42.01Table 2: This table presents results of the basic supervised classifier (i.e., Basic) and the models afterincrementally adding the knowledge of semantic classes of nouns without consideration of metonymicreadings (i.e., + SCN!M), the knowledge of semantic classes of nouns with consideration of metonymicreadings (i.e., + SCNM), the knowledge of semantic classes of verbs (i.e., +SCNM+SCV) and the knowl-edge of indistinguishable verb and noun (i.e., +SCNM+SCV+IVN).score.
The highly skewed distribution of test setwith only 13.47% causal instances results in lotsof false positives.
We manually examined falsepositives to determine the language features whichmay help us reducing more false positives withoutaffecting F-score.
We noticed that the direct ob-jects of the verbs are mostly part of the event rep-resented by the verbs and therefore encodes non-causation with the verbs.
For example, considerfollowing instances:6.
The hurricane surge protection failuresprompted a lawsuit.7.
They provided weather forecasts.In example 6, ?lawsuit?
is the direct object ofthe verb ?prompted?
and is part of the event rep-resented by the verb ?prompt?.
However thereis a cause relation between ?protection failures?and ?prompted?.
Similarly in example 7, the di-rect object ?forecasts?
is part of the ?providing?event and thus the noun phrase ?weather fore-casts?
encode non-cause relation with the verb?provide?.
Therefore, following this observationwe employed the training corpus of cause and non-cause relations (see section 3.1) and learned thestructure of verb-noun phrase pairs encoding non-cause relations most of the time.
We consideredonly those training instances where the subjectand/or object of the verb was available.
For thecurrent purpose, we picked up following four fea-tures (1) sub in np, (2) !sub in np, (3) obj in npand (4) !obj in np.
Just to remind the reader, thefeature sub in np (!sub in np) is set to 1 if thesubject of the verb is (not) contained in the nounphrase np, respectively.
For each of the above fourfeatures, the percentage of cause and entropy ofrelations with that feature are as follows:?
sub in np (%c = 34.72, Entropy = 0.931)?
!sub in np (%c = 59.71, Entropy = 0.972)?
obj in np (%c = 28.89, Entropy = 0.867)?
!obj in np (%c = 55.30, Entropy = 0.991).There are two important observations fromabove scores: (1) verbs mostly encode non-causerelations with their objects and subjects (i.e., high%?c with obj in np and sub in np), (2) amongobj in np and sub in np features, obj in np yieldsleast entropy i.e., there are least chances of encod-ing causality of a verb with its object.Considering the above statistics, we enforce theconstraint on each verb-noun phrase pair that ifthe object of the verb is contained in the nounphrase of the above pair then assigns non-causerelation to that pair.
Using this constraint, we ob-tain 46.61% (80.74%) F-score (accuracy), respec-tively.
This confirms our observation that the ob-ject of a verb is normally part of an event repre-sented by the verb and thus it encodes non-causerelation with the verb.In this research, we have utilized novel typesof knowledge to improve the performance of ourmodel.
In future, we need to consider moreadditional information (e.g., predictions frommetonymy resolver) to achieve further progress.5 ConclusionIn this paper, we have proposed a model for iden-tifying causality in verb-noun pairs by employingthe knowledge of semantic classes of nouns andverbs and the knowledge of indistinguishable nounand verb of an instance along with shallow linguis-tic features.
Our empirical evaluation of modelhas revealed that such novel types of knowledgeare critical to achieve a better performance on thecurrent task.
Following the encouraging resultsachieved by our model, we invite researchers toinvestigate more interesting types of knowledge infuture to make further progress on the task of rec-ognizing causality.55ReferencesCollin F. Baker, Charles J. Fillmore and John B. Lowe.1998.
The Berkeley FrameNet project.
In proceed-ings of the Association for Computational Linguis-tics and International Conference on ComputationalLinguistics (COLING-ACL).Brandon Beamer and Roxana Girju.
2009.
Usinga Bigram Event Model to Predict Causal Poten-tial.
In proceedings of the Conference on Compu-tational Linguistics and intelligent Text Processing(CICLING).Steven Bethard and James H. Martin.
2006.
Identifica-tion of Event Mentions and their Semantic Class.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Steven Bethard and James H. Martin.
2008.
LearningSemantic Links from a Corpus of Parallel Temporaland Causal Relations.
In proceedings of the Associ-ation for Computational Linguistics (ACL).Du-Seong Chang and Key-Sun Choi.
2006.
Incremen-tal cue phrase learning and bootstrapping method forcausality extraction using cue phrase and word pairprobabilities.
Information Processing and Manage-ment, volume 42 issue 3, 662678.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained SemanticVerb Relations.
In proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP).Quang X.
Do, Yee S. Chen and Dan Roth.
2011.
Min-imally Supervised Event Causality Identication.
Inproceedings of the Conference on Empirical Meth-ods in Natural Language Processing (EMNLP).Jenny R. Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Informa-tion into Information Extraction Systems by GibbsSampling.
In Proceedings of the Association forComputational Linguistics (ACL).Roxana Girju.
2003.
Automatic detection of causalrelations for Question Answering.
Association forComputational Linguistics ACL, Workshop on Mul-tilingual Summarization and Question AnsweringMachine Learning and Beyond.Roxana Girju and Dan Moldovan.
2002.
Mining An-swers for Causation Questions.
In American Asso-ciations of Artificial Intelligence (AAAI), 2002 Sym-posium.Jaegwon Kim.
1993.
Causes and Events.
Mackie onCausation.
In Cansation, Oxford Readings in Phi-losophy, ed.
Ernest Sosa, and Michael Tooley, Ox-ford University Press.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, Ruth Reeves.
1998.
NOMLEX: ALexicon of Nominalizations.
In proceedings of EU-RALEX.Katja Markert, Malvina Nissim 2009.
Data and mod-els for metonymy resolution.
Language Resourcesand Evaluation Volume 43 Issue 2, Pages 123?138.Marie-Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.In Proceedings of the International Conference onLanguage Resources and Evaluation (LREC).Emily Pitler, Annie Louis and Ani Nenkova.
2009.Automatic Sense Prediction for Implicit DiscourseRelations in Text.
In proceedings of ACL-IJCNLP.Emily Pitler and Ani Nenkova.
2009.
Using Syntaxto Disambiguate Explicit Discourse Connectives inText.
In proceedings of ACL-IJCNLP.James Pustejovsky, Patrick Hanks, Roser Saur, AndrewSee, Robert Gaizauskas, Andrea Setzer, DragomirRadev, Beth Sundheim, David Day, Lisa Ferro andMarcia Lazo.
2003.
The TIMEBANK Corpus.
InProceedings of Corpus Linguistics.Kira Radinsky and Eric Horvitz.
2013.
Mining theWeb to Predict Future Events.
In proceedings ofsixth ACM international conference on Web searchand data mining, (WSDM).Mehwish Riaz and Roxana Girju.
2010.
Another Lookat Causality: Discovering Scenario-Specific Contin-gency Relationships with No Supervision.
In pro-ceedings of the IEEE 4th International Conferenceon Semantic Computing (ICSC).Mehwish Riaz and Roxana Girju 2013.
Towarda Better Understanding of Causality between Ver-bal Events: Extraction and Analysis of the CausalPower of Verb-Verb Associations.
Proceedings ofthe annual SIGdial Meeting on Discourse and Dia-logue (SIGDIAL).Dan Roth and Wen-tau Yih 2004.
A Linear Program-ming Formulation for Global Inference in NaturalLanguage Tasks.
In Proceedings of the Annual Con-ference on Computational Natural Language Learn-ing (CoNLL).Caroline Sporleder and Alex Lascarides.
2008.
Usingautomatically labelled examples to classify rhetor-ical relations: An assessment.
Journal of NaturalLanguage Engineering Volume 14 Issue 3.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In Proceedings of Human Language Technol-ogy and North American Chapter of the Associationfor Computational Linguistics (HLT-NAACL).James Woodward.
2008.
Causation and Manipulation.Online Encyclopedia of Philosophy.Appendix A. Semantic Classes of NounsThis appendix presents the FrameNet labels we as-sign to cnpand ?cnpclasses (see section 3.2).56Semantic Class FrameNet LabelscnpEvent, Goal, Purpose, Cause, Internal cause, External cause, Result, Means, Reason, Phenomena, Char-acterization, Coordinated event, Final state, Information, Topic, Containing event, Mental content, Ac-tion, Experience, Impactee, Impactor, Message, Question, Circumstances, Desired goal, Explanation,Required situation, Complaint, Content, Activity, Intended goal, Phenomenon, State, Dependent state,Forgery, Purpose of Event, Negative consequences, Inference, Appraisal, Noisy event, Function, Evi-dence, Process, Paradigm, Standard, Old order, Focal occasion, Landmark occasion, resulting action,Victim, Issue, Effect, State of affairs, Cause of shine, Qualification, Undesirable Event, Skill, Precept,Outcome, Norm, Act, State of Affairs, Phenomenon 1, Phenomenon 2, Quality Eventuality, Expression,Intended event, Cognate event, Epistemic stance, Goal conditions, Possession, Support Proposition,Domain of Relevance, Charges, Idea, Initial subevent, Hypothetical event, Scene, Purpose of Goods,Response action, Motivation, Executed, Affliction, Medication, Treatment, Stimulus, Last subevent,Undesirable situation, Sleep state, Initial state, Enabled situation, Grinding cause, Finding, Case, LegalBasis, Role of focal participant, Trigger, Authenticity, World state, Emotion, Emotional state, Evalua-tion, New idea, Production, Performance, Undertaking, Destination event?cnpArtist, Performer, Duration, Time, Place, Distributor, Area, Path, Direction, Sub-region, Creator, Copy,Original, Iteration, Manner, Frequency, Agent, Body part, Depictive, Theme, Subregion, Area, De-gree, Angle, Fixed location, Path shape, Addressee, Entity, Individual 1, Individual 2, Road, Distance,Speaker, Medium, Clothing, Wearer, Bodypart of agent, Locus, Cognizer, Salient entity, Name, Inspec-tor, Ground, Unwanted entity, Location of inspector, Researcher, Population, Searcher, Sought entity,Instrument, Created entity, Components, Forgoer, Desirable, Bad entity, Dodger, Experiencer, Vehicle,Self mover, Speed, Cotheme, Consecutive, Re encoding, Supplier, Individuals, Driver, Complainer,Communicator, Protagonist, Attribute, Final value, Item, Initial value, Difference, Group, Value range,Co participant, Perceiver agentive, Target symbol, Location of perceiver, Location, Expected entity,Focal participant, Time of Event, Variable, Limits, Limit1, Limit2, Point of contact, Goods, Lessee,Lessor, Money, Rate, Unit, Reversive, Perceiver passive, Sound, Sound source, Location of source,Fidelity, Official, Selector, Role, Concessive, New leader, Body, Old leader, Leader, Governed, Resultsize, Size change, Dimension, Initial size, Elapsed time, Interval, Category, Criteria, Text, Final cor-relate, Correlate, Initial correlate, Manipulator, Side 1, Sides, Side 2, Perpetrator, Value 1, Value 2,Actor, Partner 2, Partner 1, Partners, Figure, Resident, Co resident, Student, Subject, Institution, Level,Teacher, Undergoer, Subregion bodypart, Course, Owner, Defendant, Judge, Co abductee, Locationof appearance, Material, Accused, Arraign authority, Hair, Configuration, Emitter, Beam, Amount ofprogress, Evaluee, Patient, Buyer, Seller, Recipient, Relay, Relative location, Connector, Items, Part1, Part 2, Parts, Whole, Name source, Payer, Fine, Executioner, Interlocutor 1, Interlocutor 2, Inter-locutors, Healer, Food, Cook, Container, Heating instrument, Temperature setting, Resource controller,Resource, Donor, Constant location, Carrier, Sender, Co theme, Transport means, Holding location,Rope, Knot, Handle, Containing object, Fastener, Enclosed region, Container portal, Aggregate, Sus-pect, Authorities, Offense, Source of legal authority, Ingestor, Ingestibles, Sleeper, Pieces, Goal area,Period of iterations, Mode of transportation, Produced food, Ingredients, Cognizer agent, Excreter,Excreta, Air, Perceptual source, Escapee, Undesirable location, Evader, Capture, Pursuer, Amount ofdiscussion, Means of communication, Periodicity, Author, Honoree, Reader, Child, Mother, Father,Egg, Flammables, Flame, Kindler, Mass theme, Address, Intermediary, Communication, Location ofcommunicator, Firearm, Indicated entity, Hearer, Sub region, Member, Object, Organization, Guardian,New Status, Arguer, Criterion, Liquid, Impactors, Force, Coparticipant, Holding Location, Legal basis,Precipitation, Quantity, Voice, Duration of endstate, Period of Iterations, Employer, Employee, Task,Position, Compensation, Field, Place of employment, Amount of work, Contract basis, Recipients, HotCold source, Temperature goal, Temperature change, Hot/Cold source, Dryee, Temperature, Traveler,Iterations, Baggage, Deformer, Resistant surface, Fluid, Injured Party, Avenger, Injury, Punishment,Offender, Grinder, Profiled item, Standard item, Profiled attribute, Standard attribute, Extent, Sourceemitter, Emission, Sub source, Item 1, Item 2, Parameter, Form, Chosen, Change agent, Injuring entity,Severity, Substance, Delivery device, Entry path, Wrong, Amends, Grounds, Expressor, Basis, Signs,Manufacturer, Product, Factory, Consumer, Interested party, Performer1, Performer2, Whole patient,Destroyer, Exporting area, Importing area, Accuracy, Time of Eventuality, Indicator, Indicated, Au-dience, Valued entity, Journey, Duration of end state, Killer, Beneficiary, Destination time, Landmarktime, Seat of emotion, Arguers, Arguer1, Arguer2, Company, Asset, Origin, Sound maker, Static object,Themes, Heat source, Following distance, Perceiver, Intended perceiver, Location of expressor, Path ofgaze, Relatives, Final temperature, Particular iteration, Participant 1, LanguageTable 3: This table presents the FrameNet labels we assign to cnpand ?cnpclasses.57
