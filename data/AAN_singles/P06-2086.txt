Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 667?674,Sydney, July 2006. c?2006 Association for Computational LinguisticsURES : an Unsupervised Web Relation Extraction SystemBenjamin RosenfeldComputer Science DepartmentBar-Ilan UniversityRamat-Gan, ISRAELgrurgrur@gmail.comRonen FeldmanComputer Science DepartmentBar-Ilan UniversityRamat-Gan, ISRAELfeldman@cs.biu.ac.ilAbstractMost information extraction systems ei-ther use hand written extraction patternsor use a machine learning algorithm thatis trained on a manually annotated cor-pus.
Both of these approaches requiremassive human effort and hence preventinformation extraction from becomingmore widely applicable.
In this paper wepresent URES (Unsupervised RelationExtraction System), which extracts rela-tions from the Web in a totally unsuper-vised way.
It takes as input thedescriptions of the target relations, whichinclude the names of the predicates, thetypes of their attributes, and several seedinstances of the relations.
Then the sys-tem downloads from the Web a large col-lection of pages that are likely to containinstances of the target relations.
Fromthose pages, utilizing the known seed in-stances, the system learns the relationpatterns, which are then used for extrac-tion.
We present several experiments inwhich we learn patterns and extract in-stances of a set of several common IE re-lations, comparing several patternlearning and filtering setups.
We demon-strate that using simple noun phrase tag-ger is sufficient as a base for accuratepatterns.
However, having a named en-tity recognizer, which is able to recog-nize the types of the relation attributessignificantly, enhances the extractionperformance.
We also compare our ap-proach with KnowItAll?s fixed genericpatterns.1 IntroductionThe most common preprocessing technique fortext mining is information extraction (IE).
It isdefined as the task of extracting knowledge outof textual documents.
In general, IE is dividedinto two main types of extraction tasks ?
Entitytagging and Relation extraction.The main approaches used by most informa-tion extraction systems are the knowledge engi-neering approach and the machine learningapproach.
The knowledge engineering (mostlyrule based) systems traditionally were the topperformers in most IE benchmarks, such asMUC (Chinchor, Hirschman et al 1994), ACEand the KDD CUP (Yeh and Hirschman 2002).Recently though, the machine learning systemsbecame state-of-the-art, especially for simplertagging problems, such as named entity recogni-tion (Bikel, Miller et al 1997), or field extrac-tion (McCallum, Freitag et al 2000).
Thegeneral idea is that a domain expert labels thetarget concepts in a set of documents.
The sys-tem then learns a model of the extraction task,which can be applied to new documents auto-matically.Both of these approaches require massive hu-man effort and hence prevent information extrac-tion from becoming more widely applicable.
Inorder to minimize the huge manual effort in-volved with building information extraction sys-tems, we have designed and developed URES(Unsupervised Relation Extraction System)which learns a set of patterns to extract relationsfrom the web in a totally unsupervised way.
Thesystem takes as input the names of the target re-lations, the types of its arguments, and a smallset of seed instances of the relations.
It then usesa large set of unlabeled documents downloadedfrom the Web in order to build extraction pat-terns.
URES patterns currently have two modesof operation.
One is based upon a generic shal-low parser, able to extract noun phrases and their667heads.
Another mode builds patterns for use byTEG (Rosenfeld, Feldman et al 2004).
TEG is ahybrid rule-based and statistical IE system.
Itutilizes a trained labeled corpus in order to com-plement and enhance the performance of a rela-tively small set of manually-built extractionrules.
When it is used with URES, the relationextraction rules and training data are not builtmanually but are created automatically from theURES-learned patterns.
However, URES doesnot built rules and training data for entity extrac-tion.
For those, we use the grammar and trainingdata we developed separately.It is important to note that URES is not a clas-sic IE system.
Its purpose is to extract as manyas possible different instances of the given rela-tions while maintaining a high precision.
Sincethe goal is to extract instances and not mentions,we are quite willing to miss a particular sentencecontaining an instance of a target relation ?
if theinstance can be found elsewhere.
In contrast, theclassical IE systems extract mentions of entitiesand relations from the input documents.
Thisdifference in goals leads to different ways ofmeasuring the performance of the systems.The rest of the paper is organized as follows:in Section 2 we present the related work.
In Sec-tion 3 we outline the general design principles ofURES and the architecture of the system andthen describe the different components of URESin details while giving examples to the input andoutput of each component.
In Section 4 we pre-sent our experimental evaluation and then wrapup with conclusions and suggestions for futurework.2 Related WorkInformation Extraction (IE) is a sub-field ofNLP, aims at aiding people to sift through largevolume of documents by automatically identify-ing and tagging key entities, facts and eventsmentioned in the text.Over the years, much effort has been investedin developing accurate and efficient IE systems.Some of the systems are rule-based (Fisher, So-derland et al 1995; Soderland 1999), some arestatistical (Bikel, Miller et al 1997; Collins andMiller 1998; Manning and Schutze 1999; Miller,Schwartz et al 1999) and some are based on in-ductive-logic-based (Zelle and Mooney.
1996;Califf and Mooney 1998).
Recent IE researchwith bootstrap learning  (Brin 1998; Riloff andJones 1999; Phillips and Riloff 2002; Thelen andRiloff 2002) or learning from documents taggedas relevant (Riloff 1996; Sudo, Sekine et al2001) has decreased, but not eliminated hand-tagged training.Snowball (Agichtein and Gravano 2000) is anunsupervised system for learning relations fromdocument collections.
The system takes as inputa set of seed examples for each relation, and usesa clustering technique to learn patterns from theseed examples.
It does rely on a full fledgesNamed Entity Recognition system.
Snowballachieved fairly low precision figures (30-50%)on relations such as merger and acquisition onthe same dataset used in our experiments.KnowItAll system is a direct predecessor ofURES.
It is developed at University of Washing-ton by Oren Etzioni and colleagues (Etzioni,Cafarella et al 2005).
KnowItAll is an autono-mous, domain-independent system that extractsfacts from the Web.
The primary focus of thesystem is on extracting entities (unary predi-cates).
The input to KnowItAll is a set of entityclasses to be extracted, such as ?city?, ?scien-tist?, ?movie?, etc., and the output is a list ofentities extracted from the Web.
KnowItAll usesa set of manually-built generic rules, which areinstantiated with the target predicate names, pro-ducing queries, patterns and discriminatorphrases.
The queries are passed to a search en-gine, the suggested pages are downloaded andprocessed with patterns.
Every time a pattern ismatched, the extraction is generated and evalu-ated using Web statistics ?
the number of searchengine hits of the extraction alone and the ex-traction together with discriminator phrases.KnowItAll has also a pattern learning module(PL) that is able to learn patterns for extractingentities.
However, it is unsuitable for learningpatterns for relations.
Hence, for extracting rela-tions KnowItAll currently uses only the generichand written patterns.3 Description of URESThe goal of URES is extracting instances of rela-tions from the Web without human supervision.Accordingly, the input of the system is limited to(reasonably short) definition of the target rela-tions.
The output of the system is a large list ofrelation instances, ordered by confidence.
Thesystem consists of several largely independentcomponents.
The Sentence Gatherer generates(e.g., downloads from the Web) a large set ofsentences that may contain target instances.
ThePattern Learner uses a small number of knownseed instances to learn likely patterns of relation668occurrences.
The Sentence Classifier filters theset of sentences, removing those that are unlikelyto contain instances of the target relations.
TheInstance Extractor extracts the attributes of theinstances from the sentences, and generates theoutput of the system.3.1 Sentence GathererThe Sentence Gatherer is currently implementedin a very simple way.
It gets a set of keywords asinput, and proceeds to download all documentsthat contain one of those keywords.
From thedocuments, it extracts all sentences that containat least one of the keywords.The keywords for a relation are the words thatare indicative of instances of the relation.
Thekeywords are given to the system as part of therelation definition.
Their number is usuallysmall.
For instance, the set of keywords for Ac-quisition in our experiments contains two words?
?acquired?
and ?acquisition?.
Additional key-words (such as ?acquire?, ?purchased?, and?hostile takeover?)
can be added automaticallyby using WordNet (Miller 1995).3.2 Pattern LearnerThe task of the Pattern Learner is to learn thepatterns of occurrence of relation instances.
Thisis an inherently supervised task, because at leastsome occurrences must be known in order to beable to find patterns among them.
Consequently,the input to the Pattern Learner includes a smallset (10-15 instances) of known instances foreach target relation.
Our system assumes that theseeds are a part of the target relation definition.However, the seeds need not be created manu-ally.
Instead, they can be taken from the top-scoring results of a high-precision low-recallunsupervised extraction system, such asKnowItAll.
The seeds for our experiments wereproduced in exactly this way.The Pattern Learner proceeds as follows: first,the gathered sentences that contain the seed in-stances are used to generate the positive andnegative sets.
From those sets the pattern arelearned.
Then, the patterns are post-processedand filtered.
We shall now describe those stepsin detail.Preparing the positive and negative setsThe positive set of a predicate (the terms predi-cate and relation are interchangeable in ourwork) consists of sentences that contain a knowninstance of the predicate, with the instance at-tributes changed to ?<AttrN>?, where N is theattribute index.
For example, assuming there is aseed instance Acquisition(Oracle, PeopleSoft),the sentenceThe Antitrust Division of the U.S. De-partment of Justice evaluated the likelycompetitive effects of Oracle's proposedacquisition of PeopleSoft.will be changed toThe Antitrust Division?
?of <Attr1>'sproposed acquisition of <Attr2>.The positive set of a predicate P is generatedstraightforwardly, using substring search.The negative set of a predicate consists ofsimilarly modified sentences with known falseinstances of the predicate.
We build the negativeset as a union of two subsets.
The first subset isgenerated from the sentences in the positive setby changing the assignment of one or both at-tributes to some other suitable entity.
In the firstmode of operation, when only a shallow parser isavailable, any suitable noun phrase can be as-signed to an attribute.
Continuing the exampleabove, the following sentences will be includedin the negative set:<Attr1> of <Attr2> evaluated the likely?<Attr2> of the U.S. ?
?acquisition of<Attr1>.etc.In the second mode of operation, when theNER is available, only entities of the correcttype get assigned to an attribute.The other subset of the negative set containsall sentences produced in a similar way from thepositive sentences of all other target predicates.We assume without loss of generality that thepredicates that are being extracted simultane-ously are all disjoint.
In addition, the definitionof each predicate indicates whether the predicateis symmetric (like ?merger?)
or antisymmetric(like ?acquisition?).
In the former case, the sen-tences produced by exchanging the attributes inpositive sentences are placed into the positiveset, and in the later case ?
into the negative set ofthe predicate.The following pseudo code shows the processof generating the positive and negative sets indetail:669Let S be the set of gathered sentences.For each predicate PFor each s?S containing a word from Keywords(P)For each known seed P(A1, A2) of the predicate PIf A1 and A2 are each found exactly once inside sFor all entities e1, e2 ?
s, such that e2 ?
e1, andType(e1) = type of Attr1 of P, andType(e2) = type of Attr2 of PLet s' := s  with eN changed to ?<AttrN>?.If e1 = A1 and e2 = A2Add  s'  to the PositiveSet(P).Elseif e1 = A2 and e2 = A1 and symmetric(P)Add s' to the PositiveSet(P).ElseAdd s' to the NegativeSet(P).For each predicate PFor each predicate P2 ?
PFor each sentence s ?
PositiveSet(P2)Put s into the NegativeSet(P).Generating the patternsThe patterns for predicate P are generalizationsof pairs of sentences from the positive set of P.The function Generalize(S1, S2)  is applied toeach pair of sentences S1 and S2 from the positiveset of the predicate.
The function generates apattern that is the best (according to the objectivefunction defined below) generalization of its twoarguments.
The following pseudo code showsthe process of generating the patterns:For each predicate PFor each pair S1, S2 from PositiveSet(P)Let Pattern := Generalize(S1, S2).Add Pattern to PatternsSet(P).The patterns are sequences of tokens, skips(denoted *), limited skips (denoted *?)
and slots.The tokens can match only themselves, the skipsmatch zero or more arbitrary tokens, and slotsmatch instance attributes.
The limited skipsmatch zero or more arbitrary tokens, which mustnot belong to entities of the types equal to thetypes of the predicate attributes.
The General-ize(s1, s2) function takes two patterns (note, thatsentences in the positive and negative sets arepatterns without skips) and generates the least(most specific) common generalization of both.The function does a dynamical programmingsearch for the best match between the two pat-terns (Optimal String Alignment algorithm),with the cost of the match defined as the sum ofcosts of matches for all elements.
We use thefollowing numbers:  two identical elementsmatch at cost 0, a token matches a skip or anempty space at cost 10, a skip matches an emptyspace at cost 2, and different kinds of skip matchat cost 3.
All other combinations have infinitecost.
After the best match is found, it is con-verted into a pattern by copying matched identi-cal elements and adding skips where non-identical elements are matched.
For example,assume the sentences areToward this end, <Attr1> in July acquired<Attr2>Earlier this year, <Attr1> acquired <Attr2>from XAfter the dynamical programming-basedsearch, the following match will be found:Table 1 - Best Match between SentencesToward (cost 10)Earlier   (cost 10)this this (cost 0)end (cost 10)year (cost 10), , (cost 0)<Attr1 > <Attr1 > (cost 0)in  July (cost 20)acquired acquired (cost 0)<Attr2 > <Attr2 > (cost 0)from (cost 10)X (cost 10)at total cost = 80.
The match will be converted tothe pattern (assuming the NER mode, so the onlyentity belonging to the same type as one of theattributes is ?X?):*?
*?
this *?
*?
, <Attr1> *?
acquired <Attr2> *?
*which becomes, after combining adjacent skips,*?
this  *?
,  <Attr1>  *?
acquired  <Attr2>   *Note, that the generalization algorithm allowspatterns with any kind of elements beside skips,such as CapitalWord, Number, CapitalizedSe-quence, etc.
As long as the costs and results ofmatches are properly defined, the Generalizefunction is able to find the best generalization ofany two patterns.
However, in the present workwe stick with the simplest pattern definition asdescribed above.Post-processing, filtering, and scoringThe number of patterns generated at the previousstep is very large.
Post-processing and filteringtries to reduce this number, keeping the mostuseful patterns and removing the too specific andirrelevant ones.First, we remove from patterns all ?stopwords?
surrounded by skips from both sides,670such as the word ?this?
in the last pattern in theprevious subsection.
Such words do not add tothe discriminative power of patterns, and onlyneedlessly reduce the pattern recall.
The list ofstop words includes all functional and verycommon English words, as well as puncuationmarks.
Note, that the stop words are removedonly if they are surrounded by skips, becausewhen they are adjacent to slots or non-stopwords they often convey valuable information.After this step, the pattern above becomes*?
,  <Attr1>  *?
acquired  <Attr2>   *In the next step of filtering, we remove all pat-terns that do not contain relevant words.
Foreach predicate, the list of relevant words isautomatically generated from WordNet by fol-lowing all links to depth at most 2 starting fromthe predicate keywords.
For example, the pattern*   <Attr1>  *  by  <Attr2>   *will be removed, while the pattern*   <Attr1>  *  purchased  <Attr2>  *will be kept, because the word ?purchased?
canbe reached from ?acquisition?
via synonym andderivation links.The final (optional) filtering step removes allpatterns, that contain slots surrounded by skipson both sides, keeping only the patterns, whoseslots are adjacent to tokens or to sentenceboundaries.
Since both the shallow parser andthe NER system that we use are far from perfect,they often place the entity boundaries incor-rectly.
Using only patterns with anchored slotssignificantly improves the precision of the wholesystem.
In our experiments we compare the per-formance of anchored and unanchored patterns.The filtered patterns are then scored by theirperformance on the positive and negative sets.Currently we use a simple scoring method ?
thescore of a pattern is the number of positivematches divided by the number of negativematches plus one:| { : matches } |( )|{ : matches } | 1S PositiveSet Pattern SScore PatternS NegativeSet Pattern S?= ?
+This formula is purely empirical and producesreasonable results.
The threshold is applied tothe set of patterns, and all patterns scoring lessthan the threshold (currently, it is set to 6) arediscarded.3.3 Sentence ClassifierThe task of the Sentence Classifier is to filter outfrom the large pool of sentences produced by theSentence Gatherer the sentences that do not con-tain the target predicate instances.
In the currentversion of our system, this is only done in orderto reduce the number of sentences that need tobe processed by the Slot Extractor.
Therefore, inthis stage we just remove the sentences that donot match any of the regular expressions gener-ated from the patterns.
Regular expressions aregenerated from patterns by replacing slots withskips.3.4 Instance ExtractorThe task of the Instance Extractor is to use thepatterns generated by the Pattern Learner on thesentences that were passed through by the Sen-tence Classifier.
However, the patterns cannot bedirectly matched to the sentences, because thepatterns only define the placeholders for instanceattributes and cannot by themselves extract thevalues of the attributes.We currently have two different ways to solvethis problem ?
using a general-purpose shallowparser, which is able to recognize noun phrasesand their heads, and using an information extrac-tion system called TEG (Rosenfeld, Feldman etal.
2004), together with a trained grammar ableto recognize the entities of the types of thepredicates?
attributes.
We shall briefly describethe two modes of operation.Shallow Parser modeIn the first mode of operation, the predicatesmay define attributes of two different types:ProperName and CommonNP.
We assume thatthe values of the ProperName type are alwaysheads of proper noun phrases.
And the values oftheCommonNP type are simple common nounphrases (with possible proper noun modifiers,e.g.
?the Kodak camera?
).We use a Java-written shallow parser from theOpenNLP (http://opennlp.sourceforge.net/)package.
Each sentence is tokenized, tagged withpart-of-speech, and tagged with noun phraseboundaries.
The pattern matching and extractionis straightforward.TEG modeTEG (Trainable Extraction Grammars)(Rosenfeld, Feldman et al 2004) is general-671purpose hybrid rule-based and statistical IE sys-tem, able to extract entities and relations at thesentence level.
It is adapted to any domain bywriting a suitable set of rules, and training themusing an annotated corpus.
The TEG rule lan-guage is a straightforward extension of a con-text-free grammar syntax.
A complete set ofrules is compiled into a PCFG (ProbabilisticContext Free Grammar), which is then trainedupon the training corpus.Some of the nonterminals inside the TEGgrammar can be marked as target concepts.Wherever such nonterminal occurs in a finalparse of a sentence, TEG generates an outputlabel.
The target concept rules may specify someof their parts as attributes.
Then the concept isconsidered to be a relation, with the values of theattributes determined by the concept parse.
Con-cepts without attributes are entities.For the TEG-based instance extractor we util-ize the NER ruleset of TEG and an internal train-ing corpus called INC, as described in(Rosenfeld, Feldman et al 2004).
The rulesetdefines a grammar with a set of concepts forPerson, Location, and Organization entities.
Inaddition, the grammar defines a generic Noun-Phrase concept, which can be used for capturingthe entities that do not belong to any of the entitytypes above.In order to do the extraction, the patterns gener-ated by the Pattern Learner are converted to theTEG syntax and added to the pre-built NERgrammar.
This produces a grammar, which isable to extract relations.
This grammar is trainedupon the automatically labeled positive set fromthe Pattern Learning.
The resulting trainedmodel is applied to the sets of sentences pro-duced by the Sentence Classifier.4 Experimental EvaluationIn order to evaluate URES, we used five predi-catesAcquisition(BuyerCompany, BoughtCom-pany),Merger(Company1, Company2),CEO_Of(Company, Name),MayorOf(City, Name),InventorOf(InventorName, Invention).Merger is symmetric predicate, in the sense thatthe order of its attributes does not matter.
Acqui-sition is antisymmetric, and the other three aretested as bound in the first attribute.
For thebound predicates, we are only interested in theinstances with particular prespecified values ofthe first attribute.We test both modes of operation ?
using shal-low parser and using TEG.
In the shallow parsermode, the Invention attribute of the InventorOfpredicate is of type CommonNP, and all otherattributes are of type ProperName.
In the TEGmode, the ?Company?
attributes are of type Or-ganization, the ?Name?
attributes are of typePerson, the ?City?
attribute is of type Location,and the ?Invention?
attribute is of type Noun-Phrase.We evaluate our system by running it over alarge set of sentences, counting the number ofextracted instances, and manually checking arandom sample of the instances to estimate pre-cision.
In order to be able to compare our resultswith KnowItAll-produced results, we used theset of sentences collected by the KnowItAll?scrawler as if they were produced by the SentenceGatherer.The set of sentences for the Acquisition andMerger predicates contained around 900,000sentences each.
For the other three predicates,each of the sentences contained one of the 100predefined values for the first attribute.
The val-ues (100 companies for CEO_Of, 100 cities forMayorOf, and 100 inventors for InventorOf) areentities collected by KnowItAll, half of them arefrequent entities (>100,000 hits), and anotherhalf are rare (<10,000 hits).In all of the experiments, we use ten toppredicate instances extracted by KnowItAll forthe relation seeds needed by the Pattern Learner.The results of our experiments are summa-rized in the Table 2.
The table displays the num-ber of extracted instances and estimatedprecision for three different URES setups, andfor the KnowItAll manually built patterns.
Threeresults are shown for each setup and each rela-tion ?
extractions supported by at least one, atleast two, and at least three different sentences,respectively.Several conclusions can be drawn from the re-sults.
First, both modes of URES significantlyoutperform KnowItAll in recall (number of ex-tractions), while maintaining the same level ofprecision or improving it.
This demonstrates util-ity of our pattern learning component.
Second, itis immediately apparent, that using only an-chored patterns significantly improves precisionof NP Tagger-based URES, though at a high costin recall.
The NP tagger-based URES with an-chored patterns performs somewhat worse than672Table 2 - Experimental results.Acquisition CEO_Of InventorOf MayorOf Mergersupport Count Prec Count Prec Count Prec Count Prec Count Prec?
1 10587 0.74 545 0.7 1233 0.84 2815 0.6 25071 0.71?
2 815 0.87 221 0.92 333 0.92 717 0.74 2981 0.8NP TaggerAll patterns?
3 234 0.9 133 0.94 185 0.96 442 0.84 1245 0.88?
1 5803 0.84 447 0.8 1035 0.86 2462 0.65 17107 0.8?
2 465 0.96 186 0.94 284 0.92 652 0.78 2481 0.83NP TaggerAnchoredpatterns ?
3 148 0.98 123 0.96 159 0.96 411 0.88 1084 0.9?
1 8926 0.82 618 0.83 2322 0.65 2434 0.85 15002 0.8?
2 1261 0.94 244 0.94 592 0.85 779 0.93 2932 0.86TEGAll patterns?
3 467 0.98 158 0.98 334 0.88 482 0.98 1443 0.9?
1 2235 0.84 421 0.81 604 0.8 725 0.76 3233 0.82KnowItAll?
2 257 0.98 190 0.98 168 0.92 308 0.92 352 0.92TEG-based URES on all predicates except In-ventorOf, as expected.
For the InventorOf, TEGperforms worse, because of overly simplisticimplementation of the NounPhrase concept in-side the TEG grammar ?
it is defined as a se-quence of zero or more adjectives followed by asequence of nouns.
Such definition often leads toonly part of a correct invention name being ex-tracted.5 Conclusions and Future WorkWe have presented the URES system for autono-mously extracting relations from the Web.URES bypasses the bottleneck created by classicinformation extraction systems that either relieson manually developed extraction patterns or onmanually tagged training corpus.
Instead, thesystem relies upon learning patterns from a largeunlabeled set of sentences downloaded fromWeb.One of the topics we would like to further ex-plore is the complexity of the patterns that welearn.
Currently we use a very simple patternlanguage that just has 4 types of elements, slots,constants and two types of skips.
We want to seeif we can achieve higher precision with morecomplex patterns.
In addition we would like totest URES on n-ary predicates, and to extend thesystem to handle predicates that are allowed tolack some of the attributes.ReferencesAgichtein, E. and L. Gravano (2000).
Snowball: Ex-tracting Relations from Large Plain-Text Collec-tions.
Proceedings of the 5th ACM InternationalConference on Digital Libraries (DL).Bikel, D. M., S. Miller, et al (1997).
Nymble: a high-performance learning name-finder.
Proceedings ofANLP-97: 194-201.Brin, S. (1998).
Extracting Patterns and Relationsfrom the World Wide Web.
WebDB Workshop,EDBT '98.Califf, M. E. and R. J. Mooney (1998).
RelationalLearning of Pattern-Match Rules for InformationExtraction.
Working Notes of AAAI Spring Sym-posium on Applying Machine Learning to    Dis-course Processing.
Menlo Park, CA, AAAI Press:6-11.Chinchor, N., L. Hirschman, et al (1994).
"Evaluat-ing Message Understanding Systems: An Analysisof the Third Message Understanding Conference(MUC-3)."
Computational Linguistics 3(19): 409-449.Collins, M. and S. Miller (1998).
Semantic Taggingusing a Probabilistic Context Free Grammar.
Pro-ceedings of the Sixth Workshop on Very LargeCorpora.Etzioni, O., M. Cafarella, et al (2005).
"Unsupervisednamed-entity extraction from the Web: An ex-perimental study."
Artificial Intelligence.Fisher, D., S. Soderland, et al (1995).
Description ofthe UMass Systems as Used for MUC-6.
6th Mes-sage Understanding Conference: 127-140.673Manning, C. and H. Schutze (1999).
Foundations ofStatistical Natural Language Processing.
Cam-bridge, US, The MIT Press.McCallum, A., D. Freitag, et al (2000).
MaximumEntropy Markov Models for Information Extrac-tion and    Segmentation.
Proc.
17th InternationalConf.
on Machine Learning, Morgan Kaufmann,San Francisco, CA: 591-598.Miller, D., R. Schwartz, et al (1999).
Named entityextraction from broadcast news.
Proceedings ofDARPA Broadcast News Workshop.
Herndon,VA.Miller, G. A.
(1995).
"WordNet: A lexical databasefor English."
CACM 38(11): 39-41.Phillips, W. and E. Riloff (2002).
Exploiting StrongSyntactic Heuristics and Co-Training to LearnSemantic Lexicons.
Conference on EmpiricalMethods in Natural Language Processing(EMNLP 2002).Riloff, E. (1996).
Automatically Generating Extrac-tion Patterns from Untagged Text.
AAAI/IAAI,Vol.
2: 1044-1049.Riloff, E. and R. Jones (1999).
Learning Dictionariesfor Information Extraction by Multi-level Boot-strapping.
Proceedings of the Sixteenth NationalConference on Artificial    Intelligence, The AAAIPress/MIT Press: 1044-1049.Rosenfeld, B., R. Feldman, et al (2004).
TEG: a hy-brid approach to information extraction.
CIKM2004, Arlington, VA.Soderland, S. (1999).
"Learning Information Extrac-tion Rules for Semi-Structured and Free Text.
"Machine Learning 34(1-3): 233-272.Sudo, K., S. Sekine, et al (2001).
Automatic patternacquisition for Japanese information extraction.Human Language Technology Conference(HTL2001).Thelen, M. and E. Riloff (2002).
A BootstrappingMethod for Learning Semantic Lexicons usingExtraction Pattern Contexts.
Conference on Em-pirical Methods in Natural Language Processing(EMNLP 2002).Yeh, A. and L. Hirschman (2002).
"Background andoverview for kdd cup 2002 task 1: Information ex-traction from biomedical articles."
KDD Ex-plorarions 4(2): 87-89.Zelle, J. M. and R. J. Mooney.
(1996).
Learning toparse database queries using inductive logic pro-gramming.
13th National Conference on ArtificialIntelligence (AAAI-96).674
