Applying Extrasentential Context To Maximum Entropy BasedTagging With A Large Semantic And Syntactic TagsetEzra  B lack  and Andrew F inch  and Ru ig iang  ZhangATR Interpreting Telecommunications Laboratories2-2 Hikaridai Seika-cho, Soraku-gunKyoto, Japan 619-02black?it I. atr.
co. jpf inch?itl,  atr.
co. jprzhang?itl,  atr.
co. jpAbstractExperiments are presented which measurethe perplexity reduction derived from incor-porating into the predictive model utilised ina standard tag-n-gram part-of-speech tagger,contextual information from previous entencesof a document.
The tagset employed is theroughly-3000-tag ATR General English Tagset,whose tags are both syntactic and semantic innature.
The kind of extrasentential informa-tion provided to the tagger is semantic, andconsists in the occurrence or non-occurrence,within the past 6 sentences of the documentbeing tagged, of words tagged with particulartags from the tagset, and of boolean combina-tions of such conditions.
In some cases, theseconditions are combined with the requirementthat the word being tagged belong to a partic-ular set of words thought most likely to ben-efit from the extrasentential information theyare being conjoined with.
The baseline modelutilized is a maximum entropy-based tag-n-gram tagging model, embodying a standardtag-n-gram approach to tagging: i.e.
con-straints for tag trigrams, bigrams, and and theword-tag occurrence frequency of the specificword being tagged, form the basis of prediction.Added into to this baseline tagging model is theextrasentential semantic information just indi-cated.
The performance of the tagging modelwith and without the added contextual knowl-edge is contrasted, training from the 850,000-word ATR General English Treebank, and test-ing on the accompanying 53,000-word test tree-bank.
Results are that a significant reduction intestset perplexity is achieved via the added se-mantic extrasentential nformation of the richermodel.
The model with both long-range tagtriggers and more complex linguistic onstraintsachieved a perplexity reduction of 21.4%.1 Int roduct ionIt appears intuitively that information fromearlier sentences in a document ought to helpreduce uncertMnty as to a word's correct part-of-speech tag.
This is especially so for alarge semantic and syntactic tagset such as theroughly-3000-tag ATR General English Tagset(Black et al, 1996; Black et al, 1998).
And infact, (Black et al, 1998) demonstrate a signif-icant "tag trigger-pair" effect.
That is, giventhat certain "triggering" tags have already oc-curred in a document, the probability of oc-currence of specific "triggered" tags is raisedsignificantly--with respect o the unigram tagprobability model.
Table 1, taken from (Blacket al, 1998), provides examples of the tagtrigger-pair effect.Yet, it is one thing to show that extrasenten-tial context yields a gain in information withrespect to a unigram tag probability model.But it is another thing to demonstrate thatextrasentential context supports an improve-ment in perplexity vis-a-vis a part-of-speechtagging model which employs state-of-the-arttechniques: such as, for instance, the tag-ging model of a maximum entropy tag-n-gram-based tagger.The present paper undertakes just such ademonstration.
Both the model underlyinga standard tag-n-gram-based tagger, and thesame model augmented with extrasententialcontextual information, are trMned on the850,000-word ATR General English Treebank(Black et al, 1996), and then tested on the ac-companying 53,000-word test treebank.
Perfor-mance differences are measured, with the resultthat semantic information from previous sen-tences within a document is shown to help sig-nificantly in improving the perplexity of tagging46Triggering Tag Triggered Tag I.e.
Words Like: Trigger Words Like:Utah, Maine, Alaska#1 NPiLOCNM2 J JSYSTEM3 VVDINCHOATIVE4 I IDESPITE5 DD6 PN1PERSON.
, .8 IIATSTANDIN9 I IFROMSTANDIN10 NNUNUMNPISTATENMNP1ORGVVDPROCESSIVECFYETPPHO2LEBUT22MPRICEMPHONE22MZIPNNIMONEYHill, County, Baynational, federalcaused, died, madedespiteany, some, certaineveryone, oneat (sent.-final)from (sent.-final)25%, 12", 9.4m3Party, Councilbegan, happenedyet (conjunction)them(not) only, (not) just$452,983,000, $10,000913-343422314-1698 (zip)profit, price, costTable 1: Selected Tag Trigger-Pairs, ATR General-English Treebankwith the indicated tagset.In what follows, Section 2 provides a basicoverview of the tagging approach used (a max-imum entropy tagging model employing con-straints equivalent to those of the standard hid-den Markov model).
Section 3 discusses andoffers examples of the sorts of extrasententially-based semantic onstraints that were added tothe basic tagging model.
Section 4 describes theexperiments we performed.
Section 5 details ourexperimental results.
Section 6 glances at pro-jected future research, and concludes.2 Tagg ing  Mode l2.1 ME Mode lOur tagging model is a maximum entropy(ME) model of the following form:KP(tlh) = 7 I~ ~ k(h't)p?
(1)k=0where:- t is tag we are predicting;- h is the history (all prior words and tags)of t;- 7 is a normalization coefficient hat en-~L r-TK \]k(h,t) sures: ~t=oTllk=o ak P0 = 1;L is the number of tags in our tag set;- ak is the weight of trigger fk;fk are trigger functions and f~e{0, 1};- P0 is the default agging model (in our case,the uniform distribution, since all of the in-formation in the model is specified usingME constraints).The model we use is similar to that of (Rat-naparkhi, 1996).
Our baseline model shares thefollowing features with this tagging model; wewill call this set of features the basic n-gramtagger constraints:1. w=X&t=T2.
t _ l=X&t=T3.
t-2t-1 = XY  ~: t = Twhere:- w is word whose tag we are predicting;- t is tag we are predicting;- t-1 is tag to the left of tag t;- t-2 is tag to the left of tag t - l ;Our baseline model differs from Ratna-parkhi's in that it does not use any informa-tion about the occurrence of words in the his-tory or their properties (other than in con-straint 1).
Our model exploits the same kindof tag-n-gram information that forms the coreof many successful tagging models, for exam-ple, (Kupiec, 1992), (Merialdo, 1994), (Ratna-parkhi, 1996).
We refer to this type of taggeras a tag-n-gram tagger.2.2 Tr igger  select ionWe use mutual information (MI) to select themost useful trigger pairs (for more details, see(Rosenfeld, 1996)).
That is, we use the follow-ing formula to gauge a feature's usefulness to1-he model:MI( , t)where:= P(,s,t)'tog -?~P(tl~)+ P(s,t)"  P(tls) ~og p(~)+ P(~,t) log+ P(~,/)lo~ P(tl~) c~ p(\[)- t is the tag we are predicting;- s can be any kind of triggering feature.For each of our trigger predictors, s is definedbelow:B igram and t r ig ram tr iggers : s is thepresence of a particular tag as the first tagin the bigram pair, or the presence of twoparticular tags (in a particular order) asthe first two tags of a trigram triple.
Inthis case, t is the presence of a particulartag in the final position in the n-gram.Extrasentential tag t r iggers : 8 is the pres-ence of a particular tag in the extrasenten-tial history.Quest ion  tr iggers : s is the boolean answerto a question.This method has the advantage of findinggood candidates quickly, and the disadvantageof ignoring any duplication of information in thefeatures it selects.
A more principled approachis to select features by actually adding themone-by-one into the ME model (Della Pietra etal., 1997); however, using this approach is verytime-consuming and we decided on the MI ap-proach for the sake of speed.3 The Constra intsTo understand what extrasentential semanticconstraints were added to the base taggingmodel in the current experiments, one needssome familiarity with the ATR General En-glish Tagset.
For detailed presentations, ee(Black et al, 1998; Black et al, 1996).
Anapercu can be gained, however, from Figure1, which shows two sample sentences fromthe ATR Treebank (and originally from aChinese take-out food flier), tagged withrespect to the ATR GenerM English Tagset.Each verb, noun, adjective and adverb in theATR tagset includes a semantic label, chosenfrom 42 noun/adjective/adverb categoriesand 29 verb/verbal categories, some overlapexisting between these category sets.
Propernouns, plus certain adjectives and certainnumerical expressions, are further categorizedvia an additional 35 "proper-noun" categories.These semantic ategories are intended for any"Standard-American-English" text, in anydomain.
Sample categories include: "phys-ical.attribute" (nouns/adjectives/adverbs),"alter" (verbs/verbals), "interpersonal.act"(nouns/adjectives/adverbs/verbs/verbals),"orgname" (proper nouns), and "zipcode"(numericals).
They were developed by theATR grammarian and then proven and refinedvia day-in-day-out tagging for six months atATR by two human "treebankers', then viafour months of tagset-testing-only work atLancaster University (UK) by five treebankers,with daily interactions among treebankers,and between the treebankers and the ATRgrammarian.
The semantic ategorization is,of course, in addition to an extensive syn-tactic classification, involving some 165 basicsyntactic tags.Starting with a basic tag-n-gram taggertrained to tag raw text with respect o the ATRGeneral English Tagset, then, we added con-straints defined in terms of "tag families".
Atag family is the set of all tags sharing a givensemantic ategory.
For instance, the tag fam-ily "MONEY" contains common ouns, propernouns, adjectives, and adverbs, the semanticcomponent of whose tags within the ATR Gen-eral English Tagset, is "money": 500-stock, De-posit, TOLL-FREE, inexpensively, etc.One class of constraints consisted of the pres-ence, within the 6 sentences (from the same doc-ument) 1 preceding the current sentence, of oneor more instances of a given tag family.
Thistype of constraint came in two varieties: ei-ther including, or excluding, the words withinthe sentence of the word being tagged.
Wherethese intrasentential words were included, they1 (Black et al, 1998) determined a 6-sentence windowto be optimal for this task.48(_( Please_RRCONCESSIVE Mention_VVIVERBAL-ACT this_DDl coupon_NNIDOCUMENTwhen_CSWHEN ordering_VVGINTER-ACT0R_CCOR 0NE_MCIWORD FREE_JJMONEY FANTAIL_NNiANIMAL SHRIMPS_NNiF00DFigure h Two ATR Treebank Sentences from Chinese Take-Out Food Flier (Tagged Only - i.e.Parses Not Displayed)consisted of the set of words preceding the wordbeing tagged, within its sentence.A second class of constraints added to the re-quirements of the first class the representation,within the past 6 sentences, of related tag fam-ilies.
Boolean combinations of such events de-fined this group of constraints.
An example isas follows: (a) an instance ither of the tag fam-ily "person" or of the tag family "personal at-tribute"(or both) occurs within the 6 sentencespreceding the current one; or else (b) an in-stance of the  tag family "person" occurs in thecurrent sentence, to the left of the word beingtagged; or, finally, both (a) and (b) occur.A third class of constraints had to do withthe specific word being tagged.
In particular,the word being classified is required to belongto a set of words which have been tagged atleast once, in the training treebank, with sometag from a particular tag family; and which, fur-ther, always shared the same basic syntax in thetraining data.
For instance, consider the words"currency" and "options".
Not only have theyboth been tagged at least once in the train-ing set with some member of the tag family"MONEY" (as well, it happens, as with tagsfrom other tag families); but in addition theyboth occur in the training set only as nouns.Therefore these two words would occur on a listnamed "MONEY nouns", and when an instanceof either of these words is being tagged, the con-straint "MONEY nouns" is satisfied.A fourth and final class of constraints com-bines the first or the second class, above, withthe third class.
E.g.
it is both the case thatsome avatar of the tag family "MONEY" hasoccurred within the last 6 sentences to the left;and that the word being tagged satisfies theconstraint "MONEY nouns".
The advantageof this sort Of composite constraint is that it isfocused, and likely to be'helpful when it doesoccur.
The d\[isadvantage is that it is unlikely tooccur extremely often.
On the other hand, con-straints of the first, second, and third classes,above, are more likely to occur, but less focusedand therefore less obviously helpful.4 The  Exper iments4.1 The Four Mode lsTo evaluate the utility of long-range seman-tic context we performed four separate xper-iments.
All of the models in the experimentsinclude the basic ME tag-n-gram tagger con-straints listed in section 2.
The models used inour experiments are as follows:(1) The first model is a model consisting ONLYof these basic ME tag-n-gram tagger con-straints.
This model represents the base-line model.
(2) The second model consists of the baselinemodel together with constraints represent-ing extrasentential tag triggers.
This ex-periment measures the effect of employingthe triggers pecified in (Black et al, 1998)--i.e.
the presence (or absence) in the pre-vious 6 sentences of each tag in the tagset,in turn- -  to assist a real tagger, as opposedto simply measuring their mutual informa-tion.
In other words, we are measuring thecontribution of this long-range informationover and above a model which uses localtag-n-grams as context, rather than mea-suring the gain over a naive model whichdoes not take context into account, as wasthe case with the mutual information ex-periments in (Black et al, 1998).
(3) The third model consists of the baselinemodel together with the four classes ofmore sophisticated question-based triggersdefined in the previous section.
(4) The fourth model consists of the baselinemodel together with both the long-range49tag trigger constraints and the question-based trigger constraints.\\:(~ chose the model underlying a standardla< n-gram tagger as the baseline because itrepresents a respectable tagging model whichmost readers will be familiar with.
The MEframework was used to build the models sinceil provides a principled manner in which to inte-grate the diverse sources of information neededfor these experiments.4.2 Exper imenta l  ProcedureThe performance of each the tagging models ismeasured on a 53,000-word test treebank hand-labelled to an accuracy of over 97% (Black etal., 1996; Black et al, 1998).
We measure themodel performance in terms of the perplexityof the tag being predicted.
This measurementgives an indication of how useful the featureswe supply could be to an n-gram tagger whenit consults its model to obtain a probablity dis-tribution over the tagset for a particular word.Since our intention is to gauge the usefulnessof long-range context, we measure the perfor-mance improvement with respect to correctly(very accurately) labelled context.
We choseto do this to isolate the effect of the correctmarkup of the history on tagging performance(i.e.
to measure the performance gain in the ab-sence of noise from the tagging process itself).Earlier experiments using predicted tags in thehistory showed that at current levels of taggingaccuracy for this tagset, these predicted tagsyielded very little benefit to a tagging model.However, removing the noise from these tagsshowed clearly that improvement was possiblefrom this information.
As a consequence, wechose to investigate in the absence of noise, sothat we could see the utility of exploiting thehistory when labelled with syntactic/semantictags.The resulting measure is an idealization of acomponent of a real tagging process, and is ameasure of the usefulness of knowing the tags inthe history.
In order to make the comparisonsbetween models fair, we use correctly-labelledhistory in the n-gram components of our mod-els as well as for the long-range triggers.
As aconsequence of this, no search is nescessary.The number of possible triggers is obviouslyvery large and needs to be limited for reasons ofDescription NumberTag set sizeWord vocabulary sizeBigram trigger numberTrigram trigger numberLong history trigger numberQuestion trigger number18373813818520156601575182425Table 2: Vocabulary sizes and number of trig-gers usedpracticability.
The number of triggers used forthese experiments i shown in Table 2.
\[;singthese limits we were able to build each modelin around one week on a 600MHz DEC-alpha.The constraints were selected by mutual infor-mation.
Thus, as an example, the 82425 ques-tion trigger constraints hown in Table 2 repre-sent the 82425 question trigger constraints withthe highest mutual information.The improved iterative scaling technique(Della Pietra et al, 1997) was used to train theparameters in the ME model.5 The  Resu l tsTable 4 shows the perplexity of each of thefour models on the testset.The maximum entropy framework adoptedfor these experiments virtually guarantees thatmodels which utilize more information will per-form as well as or better than models which donot include this extra information.
Therefore,it comes as no surprise that all models improveupon the baseline model, since every model ef-fectively includes the baseline model as a com-ponent.However, despite promising results whenmeasuring mutual information gain (Black etM., 1998), the baseline model combined onlywith extrasentential tag triggers reduced per-plexity by just a modest 7.6% .
The explana-tion for this is that the information these trig-gers provide is already present to some degreein the n-grams of the tagger and is thereforeredundant.In spite of this, when long-range informationis captured using more sophisticated, linguisti-cally meaningful questions generated by an ex-pert grammarian (as in experiment 3), the per-plexity reduction is a more substantial 19.4%.50iQuestion Description MI (bits) #1 Person or personal attribute word in full history2 Word being tagged has taken NN1PERSON in training set3 Person or personal attribute word in remote history4 Person or personal attribute or other related tags in full history5 Person or personal attribute or other related tags in remote history0.0244100.0243550.0242940.0207770.020156Table 3: The 5 triggers for tag NNIPERSON with the highest MIModel Perplexity Perplexity ReductionBaseline n-gram model 2.99 0.0%Baseline + long-range tag triggers 2.76 7.6%Baseline + question-based triggers 2.41 19.4%Baseline + all triggers 2.35 21.4%Table 4: Perplexity of the four models#1234The explanation for this lies in the fact thatthese question-based triggers are much morespecific.
The simple tag-based triggers will beactive much more frequently and often inap-propriately.
The more sophisticated question-based triggers are less of a blunt instrument.As an example, constraints from the fourth class(described in the constraints section of this pa-per) are likely to only be active for words ableto take the particular tag the constraint wasdesigned to apply to.
In effect, tuning the MEconstraints has recovered much ground lost tothe n-grams in the model.The final experiment shows that using all.thetriggers reduces perplexity by 21.4%.
This is amodest improvement over the results obtainedin experiment 3.
This suggests that even thoughthis long-range trigger information is less useful,it is still providing some additional informationto the more sophisticated question-based trig-gers.Table 3 shows the five constraints withthe highest mutual information for the tagNN1PERSON (singular common noun of per-son, e.g.
lawyer, friend, niece).
All five of theseconstraints happen to fall within the twenty-fiveconstraints of any type with the highest mutualinformation with their predicted tags.
WithinTable 3, "full history" refers to the previous 6sentences as well as the previous words in thecurrent sentence, while "remote history" indi-cates only the previous 6 sentences.
A "per-son word" is any word in the tag family "per-son", hence adjectives, adverbs, and both com-mon and proper nouns of person.
Similarly, a"personal attribute word" is any word in thetag family "personal attribute", e.g.
left-wing,liberty, courageously.6 Conc lus ionOur main concern in this paper has beento show that extrasentential nformation canprovide significant assistance to a real tagger.There has been almost no research done in thisarea, possibly due to the fact that, for smallsyntax-only tagsets, very accurate performancecan be obtained labelling the Wall Street Jour-nal corpus using only local context.
In theexperiments presented, we have used a muchmore detailed, semantic and syntactic tagset, onwhich the performance is much lower.
Extrasen-tential semantic information is needed to disam-biguate these tags.
We have observed that thesimple approach of only using the occurrence oftags in the history as features did not signif-icantly improve performance.
However, whenmore sophisticated questions are employed tomine this long-range contextual information, amore significant contribution to performance ismade.
This motivates further research towardfinding more predictive features.
Clearly, thework here has only scratched the surface interms of the kinds of questions that it is possi-ble to ask of the history.
The maximum entropyapproach that we have adopted is extremely ac-commodating in this respect.
It is possible to51go much further in the direction of querying thehistorical tag structure.
For example, we can, ineffect, exploit grammatical relations within pre-vious sentences with an eye to predicting thetags of similarly related words in the currentsentence.
It is also possible to go even furtherand exploit the structure of full parses in thehistory.Re ferencesE.
Black, A. Finch, H. Kashioka.
1998.
Trigger-Pair Predictors in Parsing and Tagging.
InProceedings, 36th Annual Meeting of the As-sociation for Computational Linguistics, 17thAnnual Conference on Computational Lin-guistics, pages 131-137, Montreal.E.
Black, S. Eubank, H. Kashioka, J. Saia.1998.
Reinventing Part-of-Speech Tagging.Journal of .Natural Language Processing(Japan), 5:1.E.
Black, S. Eubank, H. Kashioka, R. Garside,G.
Leech, and D. Magerman.
1996.
Beyondskeleton parsing: producing a comprehensivelarge-scale general-English treebank with fullgrammatical analysis.
In Proceedings of the16th Annual Conference on ComputationalLinguistics, pages 107-112, Copenhagen.S.
Della Pietra, V. Della Pietra, J. Lafferty.1997.
Inducing features of random fields.IEEE Transactions on Pattern Analysis andMachine Intelligence, 19(4):380-393.J.
Kupiec.
1992.
Robust part-of-speech taggingusing a hidden Markov model.
In ComputerSpeech and Language, 6:225-242.R.
Lau, R. Rosenfeld, S. Roukos.
1993.
Trigger-based language models: a maximum entropyapproach.
Proceedings of the InternationalConference on Acoustics, Speech and SignalProcessing, Ih45-48.B.
Merialdo 1994.
Tagging English text witha probabilistic model.
In Computational Lin-guistics, 20(2):155-!72_A.
Ratnaparkhi.
1996.
A Maximum EntropyPart-Of-Speech Tagger.
In Proceedings of theEmpirical Methods in Natural Language Pro-cessing Conference, University of Pennsylva-nia.R.
Rosenfeld.
1996.
A maximum entropyapproach to adaptive statistical languagemodelling.
Computer Speech and Language,10:187-228.52
