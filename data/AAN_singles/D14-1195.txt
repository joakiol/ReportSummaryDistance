Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1828?1833,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsJoint Decoding of Tree Transduction Models for Sentence CompressionJin-ge Yao Xiaojun Wan Jianguo XiaoInstitute of Computer Science and Technology, Peking University, Beijing 100871, ChinaKey Laboratory of Computational Linguistic (Peking University), MOE, China{yaojinge, wanxiaojun, xiaojianguo}@pku.edu.cnAbstractIn this paper, we provide a new method fordecoding tree transduction based sentencecompression models augmented with lan-guage model scores, by jointly decodingtwo components.
In our proposed so-lution, rich local discriminative featurescan be easily integrated without increasingcomputational complexity.
Utilizing anunobvious fact that the resulted two com-ponents can be independently decoded, weconduct efficient joint decoding based ondual decomposition.
Experimental resultsshow that our method outperforms tradi-tional beam search decoding and achievesthe state-of-the-art performance.1 IntroductionSentence compression is the task of generating agrammatical and shorter summary for a long sen-tence while preserving its most important informa-tion.
One specific instantiation is deletion-basedcompression, namely generating a compression bydropping words.
Various approaches have beenproposed to challenge the task of deletion-basedcompression.
Earlier pioneering works (Knightand Marcu, 2000) considered several insightfulapproaches, including noisy-channel based gen-erative models and discriminative decision treemodels.
Structured discriminative compressionmodels (McDonald, 2006) are capable of inte-grating rich features and have been proved effec-tive for this task.
Another powerful paradigm forsentence compression should be mentioned hereis constraints-based compression,including inte-ger linear programming solutions (Clarke and La-pata, 2008) and first-order Markov logic networks(Huang et al., 2012; Yoshikawa et al., 2012).A notable class of methods that explicitly dealwith syntactic structures are tree transductionmodels (Cohn and Lapata, 2007; Cohn and Lap-ata, 2009).
In such models a synchronous gram-mar is extracted from a corpus of parallel syn-tax trees with leaves aligned.
Compressions aregenerated from the grammar with learned weights.Previous works have noticed that local coherenceis usually needed by introducing ngram languagemodel scores, which will make accurate decodingintractable.
Traditional approaches conduct beamsearch to find approximate solutions (Cohn andLapata, 2009).In this paper we propose a joint decoding strat-egy to challenge this decoding task.
We ad-dress the problem as jointly decoding a simpletree transduction model that only considers ruleweights and an ngram compression model.
Al-though either part can be independently solved bydynamic programming, the naive way to integratetwo groups of partial scores into a huge dynamicprogramming chart table is computationally im-practical.
We provide an effective dual decompo-sition solution that utilizes the efficient decodingof both parts.
By integrating rich structured fea-tures that cannot be efficiently involved in normalformulation, results get significantly improved.2 MotivationUnder the tree transduction models, the sentencecompression task is formulated as learning a map-ping from an input source syntax tree to a targettree with reduced number of leaves.
This map-ping is known as a synchronous grammar.
Thesynchronous grammar discussed through out thispaper will be synchronous tree substitution gram-mar (STSG), as in previous studies.In such formulations, sentence compression isfinding the best derivation from a syntax tree thatproduces a simpler target tree, under the currentdefinition of grammar and learned parameters.Each derivation is attached with a score.
For thesake of efficient decoding, the score often decom-1828poses with rules involved in the derivation.
A typ-ical score definition for a derivation y of sourcetree x is in such form (Cohn and Lapata, 2008;Cohn and Lapata, 2009):S(x,y)=?r?ywT?r(x)+logP (ngram(y)) (1)The first term is a weighted sum of features ?r(x)defined on each rule r. It is plausible to introducelocal scores from ngram models.
The second termin the above score definition is added with suchpurpose.Cohn and Lapata (2009) explained that ex-act decoding of Equation 1 is intractable.
Theyproposed a beam search decoding strategy cou-pled with cube-pruning heuristic (Chiang, 2007),which can further improve decoding efficiency atthe cost of largely losing exactness in log probabil-ity calculations.
For efficiency reasons, rich localngram features have not been introduced as well.3 Components of Joint DecodingThe score in Equation 1 consists of two parts: sumof weighted rule features and local ngram scoresretrieved from a language model.
There is an im-plicit fact that either part can be used alone withslight modifications to generate a coarse candidatecompression.
Therefore, we can build a joint de-coding system that consists of these two indepen-dently decodable components.In this section we will refer to these two in-dependent models as the pure tree transductionmodel and the pure ngram compression model,described in Section 3.1 and Section 3.2 respec-tively.
There is a direct generalization of thengram model by introducing rich local features,which results in the structured discriminative mod-els (Section 3.3).3.1 Pure Tree Transduction modelBy merely considering scores from tree transduc-tion rules, i.e.
the first part of Equation 1, we canhave our scores factorized with rules.
Then findingthe best derivation from a STSG grammar can beeasily solved by a dynamic programming processdescribed by Cohn and Lapata (2007).This simplified pure tree transduction modelcan still produce decent compressions if the ruleweights are properly learned during training.3.2 Pure Ngram based CompressionThe pure ngram based model will try to findthe most locally smooth compression, reflectedby having the maximum log probability score ofngrams.To avoid the trivial solution of deleting allwords, we find the target compression with speci-fied length by dynamic programming.Furthermore, we can integrate features otherthan log probabilities.
This is equivalent to using astructured discriminative model with rich featureson ngrams of candidate compressions.3.3 Structured Discriminative ModelThe structured discriminative model proposed byMcDonald (2006) defines rich features on bigramsof possible compressions.
The score is defined asweighted linear combination of those features:f(x, z) =|z|?j=2w ?
f(x, L(zj?1), L(zj)) (2)where the functionL(zk) maps a token zkin com-pression z back to the index of the original sen-tence x. Decoding can still be efficiently done bydynamic programming.With rich local structural information, the struc-tured discriminative model can play a complemen-tary role to the tree transduction model that focusmore on global syntactic structures.4 Joint DecodingFrom now on the remaining issue is jointly de-coding the components.
Either part factorizesover local structures: rules for the tree transduc-tion model and ngrams for the language model orstructured discriminative model.
We may build alarge dynamic programming table to utilize thiskind of locality.
Unfortunately this is computa-tionally impractical.
It is mathematically equiva-lent to perform exact dynamic programming de-coding of Equation 1, which would consumeasymptotically O(SRL2(n?1)V)1time for build-ing the chart (Cohn and Lapata, 2009).
Cohn andLapata (2009) proposed a beam search approxima-tion along with cube-pruning heuristics to reducethe time complexity down to O(SRBV )2.1S, R, L and V denote respectively for the number ofsource tree nodes, the number of rules, size of target lexiconand number of variables involved in each rule.2B denotes the beam width.1829In this work we utilize the efficiency of indepen-dent decoding from the two components respec-tively and then combine their solutions accordingto certain standards.
This naturally results in adual decomposition (Rush et al., 2010) solution.Dual decomposition has been applied in sev-eral natural language processing tasks, includingdependency parsing (Koo et al., 2010), machinetranslation (Chang and Collins, 2011; Rush andCollins, 2011) and information extraction (Re-ichart and Barzilay, 2012).
However, the strengthof this inference strategy has seldom been noticedin researches on language generation tasks.We briefly describe the formulation here.4.1 DescriptionWe denote the pure tree transduction part and thepure ngram part as g(y) and f(z) respectively.Then joint decoding is equivalent to solving:maxy?Y,z?Zg(y) + f(z) (3)s.t.
zkt= ykt, ?k ?
{1, ..., n}, ?t ?
{0, 1},where y denotes a derivation which yields a finalcompression {y1, ...,ym}.
This derivation comesfrom a pure tree transduction model.
z denotes thecompression composed of {z1, ..., zm} from anngram compression model.
Without loss of gener-ality, we consider ykand zkas indicators that takevalue 1 if the k?s token of original sentence hasbeen preserved in the compression and 0 if it hasbeen deleted.
In the constraints of problem 3, yktor zktdenote indicator variables that take value 1if ykor zk= t and 0 otherwise.Let L(u,y, z) be the Lagrangian of (3).
Thenthe dual objective naturally factorizes into twoparts that can be evaluated independently:L(u) = maxy?Y,z?ZL(u,y, z)= maxy?Y,z?Zg(y) + f(z) +?k,tukt(zkt?
ykt)= maxy?Y(g(y)?
?k,tuktykt) +maxz?Z(f(z) +?k,tuktzkt)With this factorization, Algorithm 1 tries tosolve the dual problem minuL(u) by alternativelydecoding each component.This framework is feasible and plausible in thatthe two subproblems (line 3 and line 4 in Algo-rithm 1) can be easily solved with slight modifica-Algorithm 1 Dual Decomposition Joint Decoding1: Initialization: u(0)k= 0, ?k ?
{1, ..., n}2: for i = 1 to MAX ITER do3: y(i)?
argmaxy?Y(g(y)?
?k,tu(i?1)ktykt)4: z(i)?
argmaxz?Z(f(z) +?k,tu(i?1)ktzkt)5: if y(i)kt= z(i)kt?k ?t then6: return (y(i), z(i))7: else8: u(i)kt?
u(i?1)kt?
?i(z(i)kt?
y(i)kt)9: end if10: end fortions on the values of the original dynamic pro-gramming chart.
Joint decoding of a pure treetransduction model and a structured discriminativemodel is almost the same.The asymptotic time complexity of Algorithm 1is O(k(SRV + L2(n?1))), where k denotes thenumber of iterations.
This is a significant re-duction of O(SRL2(n?1)V) by directly solvingthe original problem and is also comparable toO(SRBV ) of conducting beam search decoding.We apply a similar heuristic with Rush andCollins (2012) to set the step size ?i=1t+1, wheret < i is the number of past iterations that increasethe dual value.
This setting decreases the stepsize only when the dual value moves towards thewrong direction.
We limit the maximum iterationnumber to 50 and return the best primal solutiony(i)among all previous iterations for cases that donot converge in reasonable time.5 Experiments5.1 BaselinesThe pure tree transduction model and the discrim-inative model naturally become part of our base-lines for comparison3.
Besides comparing ourmethods against the tree-transduction model withngram scores by beam search decoding, we alsocompare them against the available previous workfrom Galanis and Androutsopoulos (2010).
Thisstate-of-the-art work adopts a two-stage method torerank results generated by a discriminative maxi-mum entropy model.5.2 Data PreparationWe evaluated our methods on two standard cor-pora4, refer to as Written and Spoken respectively.3The pure ngram language model should not be consid-ered here as it requires additional length constraints and ingeneral does not produce competitive results at all merely byitself.4Available at http://jamesclarke.net/research/resources1830We split the datasets according to Table 1.Table 1: Dataset partition (number of sentences)Corpus Training Development TestingWritten 1,014 324 294Spoken 931 83 254All tree transduction models require parallelparse trees with aligned leaves.
We parsed all sen-tences with the Stanford Parser5and aligned sen-tence pairs with minimum edit distance heuristic6.
Syntactic features of the discriminative modelwere also taken from these parse trees.For systems involving ngram scores, we traineda trigram language model on the Reuters Corpus(Volume 1)7with modified Kneser-Ney smooth-ing, using the widely used tool SRILM8.5.3 Model TrainingThe training process of a tree transduction modelfollowed similarly to Cohn and Lapata (2007) us-ing structured SVMs (Tsochantaridis et al., 2005).The structured discriminative models were trainedaccording to McDonald (2006).5.4 Evaluation MetricsWe assessed the compression results by the F1-score of grammatical relations (provided by adependency parser) of generated compressionsagainst the gold-standard compression (Clarke andLapata, 2006).
All systems were controlled to pro-duce similar compression ratios (CR) for fair com-parison.
We also reported manual evaluation on asampled subset of 30 sentences from each dataset.Three unpaid volunteers with self-reported fluencyin English were asked to rate every candidate.
Rat-ings are in the form of 1-5 scores for each com-pression.6 ResultsWe report test set performance of the struc-tured discriminative model, the pure tree transduc-tion (T3), Galanis and Androutsopoulos (2010)?smethod (G&A2010), tree transduction with lan-guage model scores by beam search and the pro-posed joint decoding solutions.5http://nlp.stanford.edu/software/lex-parser.shtml6Ties were broken by always aligning a token in compres-sion to its last appearance in the original sentence.
This maybetter preserve the alignments of full constituents.7http://trec.nist.gov/data/reuters/reuters.html8http://www-speech.sri.com/projects/srilm/Table 2 shows the compression ratios and F-measure of grammatical relations in average foreach dataset.
Table 3 presents averaged human rat-ing results for each dataset.
We carried out pair-wise t-test to examine the statistical significanceof the differences9.
In both datasets joint decod-ing with dual decomposition solution outperformsother systems, especially when structured modelsinvolved.
We can also find certain improvementsof joint modeling with dual decomposition on theoriginal beam search decoding of Equation 1, un-der very close compression ratios.Joint decoding of pure tree transduction and dis-criminative model gives better performance thanthe joint model of tree transduction and languagemodel.
From Table 3 we can see that integrat-ing discriminative model will mostly improve thepreservation of important information rather thangrammaticality.
This is reasonable under the factthat the language model is trained on large scaledata and will often preserve local grammatical co-herence, while the discriminative model is trainedon small but more compression specific corpora.Table 2: Results of automatic evaluation.
(?:sig.
diff.
from T3+LM(DD); *: sig.
diff.
fromT3+Discr.
(DD) for p < 0.01)Written CR(%) GR-F1(%)Discriminative 70.3 52.4?
?G&A2010 71.6 60.2?Pure Tree-Transduction 72.6 52.3?
?T3+LM (Beam Search) 70.4 58.8?T3+LM (Dual Decomp.)
70.7 60.5T3+Discr.
(Dual Decomp.)
71.0 62.3Gold-Standard 71.4 100.0Spoken CR(%) GR-F1(%)Discriminative 69.5 50.6?
?G&A2010 71.7 59.2?Pure Tree-Transduction 73.6 53.8?
?T3+LM (Beam Search) 75.5 59.5?T3+LM (Dual Decomp.)
75.3 61.5T3+Discr.
(Dual Decomp.)
74.9 63.3Gold-Standard 72.4 100.0Table 4 shows some examples of compressedsentences produced by all the systems in compar-ison.
The two groups of outputs are compressionsof one sentence from the Written corpora andthe Spoken corpora respectively.
Ungrammaticalcompressions can be found very often by severalbaselines for different reasons, such as the outputsfrom pure tree transduction and the discriminativemodel in the first group.
The reason behind the9For all multiple comparisons in this paper, significancelevel was adjusted by the Holm-Bonferroni method.1831Table 3: Results of human rating.
(?
: sig.diff.
from T3+LM(DD); *: sig.
diff.
fromT3+Discr.
(DD), for p < 0.01)Written GR.
Imp.
CR(%)Discriminative 3.92??3.46?
?70.6G&A2010 4.11??3.50?
?72.4Pure Tree-Transduction 3.85??3.42?
?70.1T3+LM (Beam Search) 4.22?
?3.69?73.0T3+LM (Dual Decomp.)
4.63 3.98 73.2T3+Discr.
(Dual Decomp.)
4.62 4.25 73.5Gold-Standard 4.89 4.76 72.9Spoken GR.
Imp.
CR(%)Discriminative 3.95??3.62?
?71.2G&A2010 4.09?
?3.96?72.5Pure Tree-Transduction 3.92??3.55?
?71.4T3+LM (Beam Search) 4.20?3.78?75.0T3+LM (Dual Decomp.)
4.35 4.18 74.5T3+Discr.
(Dual Decomp.)
4.47 4.26 74.7Gold-Standard 4.83 4.80 73.1under generation of pure tree transduction is that itmainly deals with global syntactic integrity merelyin terms of the application of synchronous rules.Introducing language model scores will smooththe candidate compressions and avoid many ag-gressive decisions of tree transduction.
Discrim-inative models are good at local decisions withpoor consideration of grammaticality.
We can seethat the joint models have collected their predic-tive power together.
Unfortunately we can stillobserve some redundancy from our outputs in theexamples.
The size of training corpus is not largeenough to provide enough lexicalized information.On the other hand, the time consumption ofthe joint model with dual decomposition decodingin our experiments matched the aforementionedasymptotic analysis.
The training process basedon new decoding method consumes similar timeas beam search with cube-pruning heuristic.7 Conclusion and Future WorkIn this paper we propose a joint decoding schemefor tree transduction based sentence compression.Experimental results suggest that the proposedframework works well.
The overall performancegets further improved under our framework by in-troducing the structured discriminative model.As several recent efforts have focused on ex-tracting large-scale parallel corpus for sentencecompression (Filippova and Altun, 2013), wewould like to study how larger corpora can af-fect tree transduction and our joint decoding so-Table 4: Example outputsOriginal: It was very high for people who took theirfull-time education beyond the age of 18 , and higheramong women than men for all art forms except jazzand art galleries .Discr.
: It was high for people took education higheramong women .
(Galanis and Androutsopoulos, 2010): It was high forpeople who took their education beyond the age of 18 ,and higher among women .Pure T3: It was very high for people who took .T3+LM-BeamSearch: It was very high for people whotook their education beyond the age of 18 , and higheramong women than men .T3+LM-DualDecomp: It was very high for people whotook their education beyond the age of 18 , and higheramong women than men .T3+Discr.
: It was high for people who took educationbeyond the age of 18 , and higher among women thanmen .Gold-Standard: It was very high for people who tookfull-time education beyond 18 , and higher amongwomen for all except jazz and galleries .Original: But they are still continuing to search thearea to try and see if there were , in fact , any furthershooting incidents .Discr.
: they are continuing to search the area to try andsee if there were , further shooting incidents .
(Galanis and Androutsopoulos, 2010): But they are stillcontinuing to search the area to try and see if therewere , in fact , any further shooting incidents .Pure T3: they are continuing to search the area to tryand see if there were any further shooting incidents .T3+LM-BeamSearch: But they are continuing tosearch the area to try and see if there were , in fact ,any further shooting incidents .T3+LM-DualDecomp: But they are continuing tosearch the area to try and see if there were any furthershooting incidents .T3+Discr.
: they are continuing to search the area to tryand see if there were further shooting incidents .Gold-Standard: they are continuing to search the areato see if there were any further incidents .lution.
Meanwhile, We would like to explore onhow other text-rewriting problems can be formu-lated as a joint model and be applicable to similarstrategies described in this work.AcknowledgementsThis work was supported by National Hi-Tech Re-search and Development Program (863 Program)of China (2014AA015102, 2012AA011101) andNational Natural Science Foundation of China(61170166, 61331011).
We also thank the anony-mous reviewers for very helpful comments.The contact author of this paper, according tothe meaning given to this role by Peking Univer-sity, is Xiaojun Wan.1832ReferencesYin-Wen Chang and Michael Collins.
2011.
Exact de-coding of phrase-based translation models throughlagrangian relaxation.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing, pages 26?37, Edinburgh, Scot-land, UK., July.
Association for Computational Lin-guistics.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
computational linguistics, 33(2):201?228.James Clarke and Mirella Lapata.
2006.
Modelsfor sentence compression: A comparison across do-mains, training requirements and evaluation mea-sures.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for Computa-tional Linguistics, pages 377?384.
Association forComputational Linguistics.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression: An integer linearprogramming approach.
Journal of Artificial Intelli-gence Research, 31:273?381.Trevor Cohn and Mirella Lapata.
2007.
Large mar-gin synchronous generation and its application tosentence compression.
In Proceedings of the 2007Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Nat-ural Language Learning (EMNLP-CoNLL), pages73?82, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Trevor Cohn and Mirella Lapata.
2008.
Sentencecompression beyond word deletion.
In Proceedingsof the 22nd International Conference on Computa-tional Linguistics-Volume 1, pages 137?144.
Asso-ciation for Computational Linguistics.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of ArtificialIntelligence Research, 34:637?674.Katja Filippova and Yasemin Altun.
2013.
Overcom-ing the lack of parallel data in sentence compres-sion.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 1481?1491, Seattle, Washington, USA,October.
Association for Computational Linguistics.Dimitrios Galanis and Ion Androutsopoulos.
2010.
Anextractive supervised two-stage method for sentencecompression.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 885?893, Los Angeles, California,June.
Association for Computational Linguistics.Minlie Huang, Xing Shi, Feng Jin, and Xiaoyan Zhu.2012.
Using first-order logic to compress sentences.In AAAI.Kevin Knight and Daniel Marcu.
2000.
Statistics-based summarization-step one: Sentence compres-sion.
In AAAI/IAAI, pages 703?710.Terry Koo, Alexander M. Rush, Michael Collins,Tommi Jaakkola, and David Sontag.
2010.
Dualdecomposition for parsing with non-projective headautomata.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Process-ing, pages 1288?1298, Cambridge, MA, October.Association for Computational Linguistics.Ryan T McDonald.
2006.
Discriminative sentencecompression with soft syntactic evidence.
In EACL.Roi Reichart and Regina Barzilay.
2012.
Multi-eventextraction guided by global constraints.
In Proceed-ings of the 2012 Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, pages 70?79, Montr?eal, Canada, June.
Association for Com-putational Linguistics.Alexander M. Rush and Michael Collins.
2011.
Exactdecoding of syntactic translation models through la-grangian relaxation.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages72?82, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Alexander M Rush and Michael Collins.
2012.
A tuto-rial on dual decomposition and lagrangian relaxationfor inference in natural language processing.
Jour-nal of Artificial Intelligence Research, 45:305?362.Alexander M Rush, David Sontag, Michael Collins,and Tommi Jaakkola.
2010.
On dual decompositionand linear programming relaxations for natural lan-guage processing.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1?11, Cambridge, MA, October.Association for Computational Linguistics.Ioannis Tsochantaridis, Thorsten Joachims, ThomasHofmann, and Yasemin Altun.
2005.
Large mar-gin methods for structured and interdependent out-put variables.
In Journal of Machine Learning Re-search, pages 1453?1484.Katsumasa Yoshikawa, Tsutomu Hirao, Ryu Iida, andManabu Okumura.
2012.
Sentence compressionwith semantic role constraints.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Short Papers-Volume 2, pages349?353.
Association for Computational Linguis-tics.1833
