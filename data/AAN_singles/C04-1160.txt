Computational Cognitive LinguisticsJerome A. FeldmanICSI & UC Berkeley1947 Center StreetBerkeley, CA, 94704jfeldman@icsi.berkeley.eduAbstract1 The talk will describe an ongoing project(modestly named the Neural Theory ofLanguage) that is attempting to modellanguage behavior in a way that is bothneurally plausible and computationallypractical.
The cornerstone of the effort is aformalism called Embodied ConstructionGrammar (ECG).
I will describe theformalism, a robust semantic parser basedon it, and a variety of applications ofmoderate scale.
These include a system forunderstanding the (probabilistic andmetaphorical) implications of  news stories,and the first cognitively plausible model ofhow children learn grammar.2 IntroductionComputational Linguistics is sometimesnarrowly identified with statistical corpus studies,but is much broader.
The learning and use oflanguage are inherently information processingtasks and any systematic description of them mustbe computational.
If we want to understand howhuman brains acquire and exploit language, we aredriven to computational theories and models thatlink neural structure to linguistic behavior.Cognitive Linguistics provides the basicmechanisms for explaining human languageprocessing, but has traditionally been informal andnon-computational.
Recent work suggests that aformally grounded computational cognitvelinguistics can be quite productive.3 The Neural Theory of LanguageFor some sixteen years, an interdisciplinarygroup at ICSI and UC Berkeley has been buildinga theory of language that respects all experimentalfindings and is consistent with biological andcomputational constraints.
The intellectual base forthe project is a synthesis of cognitive linguisticsand structured connectionist modelling.
Thisinvolves work from neurobiology through politicsand philosophy (Lakoff 1999).
An accessibleoverview of the goals and methodology of theproject can be found in (Regier 1996) and furtherinformation is available at:WWW.icsi.berkeley.edu/NTLThe focus of this talk is computational; how canan embodied theory of language support scalablesystems for language learning and use.
The key toscalability in any paradigm is compositionality; ourgoal in modeling language understanding is tosystematically combine the heterogeneousstructures posited in cognitive linguistics to yieldoverall interpretations.
We have identified fourconceptual primitives that appear to capture thesuffice for building scalable languageunderstanding systems: SCHEMA, MAP,(MENTAL) SPACE, and CONSTRUCTION.Schemas are language-independent representationof meanings and constructions are form-meaningpairs.
Constructions are form-meaning mappingsas depicted in Figure 1.
Maps and mental spacesare discussed in (Mok 2004).
The ECG formalismhas, in addition to the usual inheritance hierarchy,an EVOKES relation that makes an outsidestructure accessible to a schema through a localname.As shown in Figure 1, langauge understanding isimplemented as having distinct analysis andsimulation phases.
During analysis, a SemanticSpecification (SemSpec) is created from themeaning poles of the constructions, and isessentially a network of schemas with theappropriate roles filled in.. Unification ofconstructions requires compatability of theirembodied semantic scehemas as well as formmatching.
Crucially, within this network ofschemas are executing schemas (or x-schemasNarayanan 1999), which are models of events.They are active structures for event-basedasynchronous control that can capture bothsequential flow and concurrency.Simulation is a dynamic process which includesexecuting the x-schemas specified in the SemSpecand propagating belief updates in a belief network(Jensen 1996, Narayanan 1999).
This will bediscussed further in Section 5.Figure1.
Overview of the Comprehension Model4 Embodied Construction GrammarThe cornerstone of the effort is the formalismcalled Embodied Construction Grammar (ECG).
Intraditional terms, ECG resembles a unificationgrammar like HPSG or LFG and many of thecomputational insights carry over.
But the centraltask of grammar is taken to be accounting for thefull range of language learning and use rather thanthe specification of acceptable forms.Grammars in ECG are deeply cognitive, withmeaning being expressed in terms of cognitiveprimitives such as image schemas, force dynamics,etc.
The hypothesis is that a modest number ofuniversal primitives will suffice to provide the coremeaning component for the grammar.
Specificknowledge about specialized items, categories andrelations will be captured in the external ontology.As a linguistic formalism, ECG combines theidea of Construction Grammar as form-meaningpairings (Croft 2001, Fillmore & Kay 1999,Goldberg 1995, etc.)
with the embodied semanticsof the Cognitive Linguistics tradition (Fauconnier1997, Lakoff 1999, Langacker 1991, etc.
).Computationally, the central ideas involveprobabilistic relational models (Pfeffer and Koller2000, etc.)
and active knowledge (Bailey 1998,Narayanan 1999, etc.)
along with their reduction tostructured connectionist form and thus to neuralmodels (Shastri 2002).A grammar specification in ECG shouldsimultaneously fill three distinct functions:capturing linguistic insights, specifying theanalysis phase of a computational system, andserving as the high level description of the neuralembodiment of the grammar.
This has beenachieved in some simple cases and serves as amajor constraint in all ECG efforts.The deep semantic construction grammar ofECG also supports a novel style of general robustparsing.
The first phase of analysis is an modifiedchunk parser (Abney 1996).The chunker generates a set of semantic chunksstored in a chart.
The second phase of analysisextracts the smallest number of  chunks that spanthe utterance from the chart, and performssemantic integration.
Their common semanticstructures are merged, and the resulting analysesare ranked according to the semantic densitymetricWithout a complete analysis of an utterance, thesystem must infer exactly how a set of local,partial semantic structures fit together into acoherent, global analysis of the utterance.
Theapproach taken is an abductive one in that itassumes compatible structures are the same andmerges them.
This has been shown to work quitewell in a system for modeling the learning of newconstructions (Bryant 2004, Chang 2004).5 ApplicationsNarayanan (Narayan 1999) has built abiologically plausible model of how suchmetaphorical uses can be understood by mappingto their underlying embodied meaning.
We assumethat people can execute x-schemas with respect tostructures that are not linked to the body, the hereand the now.
In this case, x-schema actions are notcarried out directly, but instead trigger simulationsof what they would do in the imagined situation.This ability to simulate or imagine situations is acore component of human intelligence and iscentral to our model of language.
The systemmodels the physical world as other x-schemas thathave input/output links to the x-schemarepresenting the planned action.In the computational implementation, the spatialmotion domain (source domain) is encoded asconnected x-schemas.
Our model of the sourcedomain is a dynamic system based on inter-x-schema activation,  inhibition and interruption.
Inthe simulation framework, whenever an executingx-schema makes a control transition, it potentiallymodifies state, leading to asynchronous andparallel triggering or inhibition of other x-schemas.The notion of system state as a graph marking isinherently distributed over the network, so theworking memory of an x-schema-based inferencesystem is distributed over the entire set of x-schemas and source domain feature structures.
TheKARMA system has been tested on narrativesfrom the abstract domain  of internationaleconomics.
The implemented model has about 100linked x-schemas, and about 50 metaphor mapsfrom the domains of health and spatial motion.These were developed using a database of 30 2-3phrase fragments from newspaper stories all ofwhich have been successfully interpreted by theprogram.
Results of testing the system have shownthat a surprising variety of fairly subtle andinformative inferences arise from the interaction ofthe metaphoric projection of embodied terms withthe topic specific target domain structure(Narayanan, 1999).
Among the inferences madewere those related to goals (their accomplishment,modification, subsumption, concordance, orthwarting), resources (consumption, production,depletion, level), aspect (temporal structure ofevents) frame-based inferences, perspectivalinferences, and inferences about communicativeintent.The ECG formalisms as well as the analyzerdescribed above play a crucial role in acomputational model of how languagecomprehension may drive the acquisition of earlyphrasal and clausal constructions (Chang, 2004).This model takes ECG as the target representationto be learned from a sequence of utterances incontext.
Learning is usage-based in that utterancesare first analyzed using the existing set ofconstructions, typically resulting in only a partialanalysis that neither provides complete coverage ofthe richer background context nor exploits all thepotential input forms and relations in the utterance.This incomplete analysis prompts the formation ofnew constructions that take up the slack.Constructions can also be formed on the basis ofsimilarity (two constructions can merge into amore general construction) and co-occurrence (twoconstructions can compose into a largerconstruction).Besides specifying the means for forming newECG constructions, the acquisition model providesan overarching computational framework forconverging on an optimal set of constructions,based on a minimum description length principle (Rissanen 1978) that favors compactness indescribing both the grammar and the statisticalproperties of the data.
This framework extendsprevious work in Bayesian model merging forlexical acquisition (Bailey, 1997) and the inductionof context-free grammars (Stolcke 1994) to handlethe relational structures and usage-basedconsiderations madepossible with ECG.Specifically, the criteria employed favorconstructions that have simple descriptions(relative to the available meaning representationsand current set of constructions) and are frequentlyemployed.The model has been applied to learn simpleEnglish motion constructions from a corpus ofchild-directed utterances, paired with situationrepresentations.
The resulting learning trendsreflect cross-linguistic acquisition patterns,including the incremental growth of theconstructional inventory based on experience, theprevalence of early grammatical markers forconceptually basic scenes (Slobin, 1985) and thelearning of lexically specific verb islandconstructions before more abstract grammaticalpatterns (Tomasello, 1992).
For current purposes,the systems described demonstrate the utility of theECG formalism for supporting computationalmodeling and offers a precisely specifiedinstantiation of the simulation-based approach tolanguage.ConclusionFor current purposes, the systems describedabove demonstrate the utility of the ECGformalism for supporting computational modelingand offer a precisely specified instantiation of thesimulation-based approach to language.Cognitive Linguistics has developed manyprofound insights, but these had not beenformalized and made computationally tractable.Recent results like these suggest that aComputational Cognitive Linguistics is bothscientifically productive and a semantic basis for awide range of natural language understandingapplications.6 AcknowledgementsThe NTL project has always been a team effort.Special thanks for this presentation go to JohnBryant, Nancy Chang, Eva Mok, and SriniNarayanan.ReferencesS.
Abney.
1996.
Partial parsing via finite-statecascades.
In Workshop on Robust Parsing, 8thEuropean Summer School in Logic, Language andInformation, pages 8-15, Prague, Czech Republic.D.
Bailey, N. Chang, J. Feldman and S.Naryanan.
1998.
Extending embodied lexicaldevelopment.
In Proceedings of the twentiethannual meeting of the Cognitive Science SocietyCOGSCI-98, Madison.B.
Bergen and N. Chang.
2002.
Embodiedconstruction grammar in simulation-basedlanguage understanding.
Technical Report TR-02-004, ICSI, 2002.
To appear in Oestman and Reid,eds., Construction Grammar(s): Cognitive andCross-Linguistic Dimensions.
John Benjamins.B.
Bergen, N. Chang and S. Narayan.
2004.Simulated Action in an Embodied ConstructionGrammar.
Proc.
26th Cognitive Science SocietyConference.J.
Bryant.
2003.
Constructional analysis.Master?s thesis, UC Berkeley.J.
Bryant.
2004.
Towards Cognitive,Compositional Construction Grammar.
ROMAND2004: RObust Methods in Analysis of Naturallanguage Data, Geneva.G.
Buccino, F. Binkofski, G. R. Fink, L. Fadiga,L.
Fogassi, V. Gallese, R. J. Seitz, K Zilles, G.Rizzolatti and H.-J.
Freund.
2001.
Actionobservation activates premotor and paretal areas ina somatotopic manner: An fMRI study.
EuropeanJournal of Neuroscience, 13:400-404.N.
Chang.
2004a.
Constructing Grammar: Acomputational model of the emergence of earlyconstructions.
PhD thesis, University of Californiaat Berkeley.N.
Chang.
2004b.
Putting Meaning intoGrammar Learning.
In Proc.
Psycho-computational Models of Human LanguageAcquisition.
Geneva.N.
Chang, J. Feldman, R. Porzel and K. Sanders.2002.
Scaling cognitive linguistics: Formalisms forlanguage understanding.
In Proceedings of the 1stInternational Workshop on Scalable NaturalLanguage Understanding, Heidelberg, Germany.N.
Chang and O. Gurevich.
Context-drivenconstruction learning.
Proc.
26th CognitiveScience Society Conference.N.
Chang, S. Narayanan and M. R. L. Petruck.2002.
Putting frames in perspective.
InProceedings of the nineteenth internationalconference on Computational Linguistics(COLING 2002), Taipei.W.
Croft.
2001.
Radical Construction Grammar.Oxford University Press.G.
Fauconnier.
1997.
Mappings in Thought andLanguage.
New York: Cambridge UniversityPress.J.
Feldman.
2002.
The meaning of reference inembodied construction grammar.
TechnicalReport, International Computer Science Institute,ICSI TR-02-11, Berkeley, CA.J.
Feldman.
2004.
From Molecule to Metaphor:A Neural Theory of Language.
ForthcomingJ.
Feldman and S. Narayanan.
2003.
Embodiedmeaning in a neural theory of language.
Brain andLanguage, 89:385-392.C.
Fillmore and P. Kay.
1999.
ConstructionGrammar.
CSLI, Stanford, CA to appear.A.
E. Goldberg.
1995.
Constructions: AConstruction Grammar Approach to ArgumentStructure.
University of Chicago Press.F.
Jensen.
1996.
An Introduction to BayesianNetworks.
Berlin: Springer Verlag.G.
Lakoff and M. Johnson.
1999.
Philosophy inthe flesh: The embodied mind and its challenge towestern thought, 2nded.
202-251.
Cambridge:Cambridge University Press.R.
Langacker.
1991.
Concept, Image, Symbol:The Cognitive Basics of Grammar.
Mouton deGruyter.B.
MacWhinney.
1991.
The CHILDES project:Tools for analyzing talk.
Erlbaum Associates,Mahwah, New Jersey.E.
Mok, J. Bryant, J. Feldman.
2004.
Scaling upto Mental Spaces.
In Proceedings of the SecondInternational Workshop on Scalable NaturalLanguage Understanding, Boston.S.
Narayanan.
1999.
Moving right along: Acomputational model of metaphoric reasoningabout events.
In Proceedings of the nationalConference on Artificial Intelligence AAAI-99.Orlando, FL.S.
Narayanan and S. Harabagiu (2004).
QuestionAnswering Based on Semantic Structures, 21stInternational Conference on ComputationalLinguistics (COLING 2004), Geneva, Switzerland(to appear).F.
Pulvermueller.
2001.
Brain reflections ofwords and their meaning.
Trends in CognitiveScience, 5(12).T.
Regier.
1996.
The Human Semantic Potential.MIT Press, Cambridge, MA.A.
Pfeffer and D. Koller.
Semantics andInference for Rewcursive Probability Models.National Conference on Artificial Intelligence(AAAI), 2000.J.
Rissanen.
1978.
Modeling by shortest datadescription.
Automatica, 14:465-471.L.
Shastri.
2002. episodic memory and cortico-hippocampal interactions.
Trends in CognitiveScience, 6:162-168.D.I.
Slobin.
1985.
Crosslinguistic evidence forthe language-making capacity.
In Slobin, D.I.,editor, Theoretical Issues, volume 2 of T h eCrosslinguistic Study of Language Acquisition,chapter 15.
Lawrence Erlbaum Associates,Mahwah, New Jersey.A.
Stolcke 1994 Bayesian Learning ofProbabilistic Language Models.
Doctoral thesis,UC Berkeley.M.
Tomasello.
1992.
First verbs: A case study ofearly grammatical development.
CambridgeUniversity Press, Cambridge, UK.M.
Tomasello.
2003.
Constructing a Language:A usage-Based Theory of Language Acquisition.Harvard University Press, Cambridge, MA.
