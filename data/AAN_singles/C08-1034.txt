Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 265?272Manchester, August 2008Instance-Based Ontology PopulationExploiting Named-Entity SubstitutionClaudio GiulianoFondazione Bruno KesslerTrento, Italygiuliano@fbk.euAlfio GliozzoLaboratory for Applied OntologyItalian National Research CouncilRome, Italyalfio.gliozzo@cnr.istc.itAbstractWe present an approach to ontology popu-lation based on a lexical substitution tech-nique.
It consists in estimating the plausi-bility of sentences where the named entityto be classified is substituted with the onescontained in the training data, in our case,a partially populated ontology.
Plausibilityis estimated by using Web data, while theclassification algorithm is instance-based.We evaluated our method on two differentontology population tasks.
Experimentsshow that our solution is effective, out-performing existing methods, and it canbe applied to practical ontology populationproblems.1 IntroductionSemantic Web and knowledge management appli-cations require to populate the concepts of theirdomain ontologies with individuals and find theirrelationships from various data sources, includingdatabases and natural language texts.
As the ex-tensional part of an ontology (the ABox) is oftenmanually populated, this activity can be very time-consuming, requiring considerable human effort.The development of automatic techniques for on-tology population is then a crucial research area.Natural language processing techniques are natu-ral candidates to solve this problem as most of thedata contained in the Web and in the companies?intranets is free text.
Information extraction (IE)is commonly employed to (semi-) automate such atask.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.Current state-of-the-art IE systems are mostlybased on general purpose supervised machinelearning techniques (e.g., kernel methods).
How-ever, supervised systems achieve acceptable accu-racy only if they are supplied with a sufficientlylarge amount of training data, usually consisting ofmanually annotated texts.
Consequently, they canbe only used to populate top-level concepts of on-tologies (e.g., people, locations, organizations).
Infact, when the number of subclasses increases thenumber of annotated documents required to findsufficient positive examples for all subclasses be-comes too large to be practical.
As domain ontolo-gies usually contain hundreds of concepts arrangedin deep class/subclass hierarchies, alternative tech-niques have to be found to recognize fine-graineddistinctions (e.g., to categorize people as scientistsand scientists as physicists, mathematicians, biol-ogists, etc.
).In this paper, we present an approach to the clas-sification of named entities into fine-grained onto-logical categories based on a method successfullyemployed in lexical substitution.1In particular, wepredict the fine-grained category of a named en-tity, previously recognized, by simply estimatingthe plausibility of sentences where the entity to beclassified is substituted with the ones contained inthe training data, in our case, a partially populatedontology.In most of the cases, ontologies are partiallypopulated during the development phase and af-ter that the annotation cost is practically negligi-ble, making this method highly attractive in manyapplicative domains.
This allows us to define aninstance-based learning approach for fine-grained1Lexical substitution consists in identifying the mostlikely alternatives (substitutes) of a target word given its con-text (McCarthy, 2002).265entity categorization that exploits the Web to col-lect evidence of the new entities and does not re-quire any labeled text for supervision, only a par-tially populated ontology.
Therefore, it can be usedin different domains and languages to enrich anexisting ontology with new entities extracted fromtexts by a named-entity recognition system and/ordatabases.We evaluated our method on the benchmark pro-posed by Tanev and Magnini (2006) to provide afair comparison with other approaches, and on ageneral purpose ontology of people derived fromWordNet (Fellbaum, 1998) to perform a more ex-tensive evaluation.
Specifically, the experimentswere designed to investigate the effectiveness ofour approach at different levels of generality andwith different amounts of training data.
The resultsshow that it significantly outperforms the base-line methods and, where a comparison is possible,other approaches and achieves a good performancewith a small number of examples per category.
Er-ror analysis shows that most of the misclassifica-tion errors are due to the finer-grained distinctionsbetween instances of the same super-class.2 Lexical Substitutability and OntologyPopulationOur approach is based on the assumption that en-tities that occur in similar contexts belong to thesame concept(s).
This can be seen as a specialcase of the distributional hypothesis, that is, termsthat occur in the same contexts tend to have similarmeanings (Harris, 1954).If our assumption is correct, then given an in-stance in different contexts one can substitute itwith another of the same ontological type (i.e.,of the same category) and probably generate truestatements.
In fact, most of the predicates thatcan be asserted for an instance of a particular cate-gory can also be asserted for other instances of thesame category.
For instance, the sentence ?Ayr-ton Senna is a F1 Legend?
preserves its truthful-ness when Ayrton Senna is replaced with MichaelSchumacher, while it is false when Ayrton Sennais replaced with the MotoGP champion ValentinoRossi.For our purposes, the Web provides a simple andeffective solution to the problem of determiningwhether a statement is true or false.
Due to the highredundancy of the Web, the high frequency of astatement generated by a substitution usually pro-vides sufficient evidence for its truth, allowing usto easily implement an automatic method for fine-grained entity classification.
Following this intu-ition, we developed an ontology population tech-nique adopting pre-classified entities as trainingdata (i.e., a partially populated ontology) to clas-sify new ones.When a new instance has to be classified, wefirst collect snippets containing it from the Web.Then, for each snippet, we substitute the new in-stance with each of the training instances.
Thesnippets play a crucial role in our approach be-cause we expect that they provide the features thatcharacterize the category to which the entity be-longs.
Thus, it is important to collect a sufficientlylarge number of snippets to capture the featuresthat allow a fine-grained classification.To estimate the correctness of each substitution,we calculate a plausibility score using a modifiedversion of the lexical substitution algorithm intro-duced in Giuliano et al (2007), that assigns higherscores to the substitutions that generate highly fre-quent sentences on the Web.
In particular, thistechnique ranks a given list of synonyms accord-ing to a similarity metric based on the occur-rences in the Web 1T 5-gram corpus,2which spec-ify n-grams frequencies in a large Web sample.This technique achieved the state-of-the-art perfor-mance on the English Lexical Substitution task atSemEval 2007 (McCarthy and Navigli, 2007).Finally, on the basis of these plausibility scores,the algorithm assigns the new instance to the cat-egory whose individuals show a closer linguisticbehavior (i.e., they can be substituted generatingplausible statements).3 The IBOP algorithmIn this section, we describe the algorithmic andmathematical details of our approach.
Theinstance-based ontology population (IBOP) algo-rithm is an instance-based supervised machinelearning approach.3The proposed algorithm issummarized as follows:Step 1 For each candidate instance i, we collectthe first N snippets containing i from the Web.For instance, 3 snippets for the candidate instanceAyrton Senna are ?The death of Ayrton Senna at2http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13.3An analogy between instance-based learning methodsand our approach is left to future work.266the 1994 San Marino GP?, ?triple world cham-pion Ayrton Senna?, and ?about F1 legend AyrtonSenna?.Step 2 Then, for each retrieved snippet qk(1 6k 6 N ), we derive a list of hypothesis phrasesby replacing i with each training instance j fromthe given ontology.
For instance, from the snippet?about F1 legend Ayrton Senna?, we derive ?aboutF1 legend Michael Schumacher?
and ?about F1legend Valentino Rossi?, assuming to have the for-mer classified as F1 driver and the latter as Mo-toGP driver.Step 3 For each hypothesis phrase hj, we calcu-late the plausibility score sjusing a variant of thescoring procedure defined in Giuliano et al (2007).In our case, sjis given by the sum of the point-wise mutual information (PMI) of all the n-grams(1 < n 6 5) that contain j divided by the self-information of the right and left contexts.4Divid-ing by the self-information allows us to penalizethe hypotheses that have contexts with a low infor-mation content, such as sequences of stop words.The frequency of the n-grams is estimated fromthe Web 1T 5-gram corpus.
For instance, fromthe hypothesis phrase ?about F1 legend MichaelSchumacher?, we generate and score the followingn-grams: ?legend Michael Schumacher?, ?F1 leg-end Michael Schumacher?, and ?about F1 legendMichael Schumacher?.Step 4 To obtain an overall score scfor the cat-egory c, we sum the scores obtained from eachtraining instance of category c for all snippets, asdefined in Equation 1.sc=NXk=1MXl=1sl, (1)where M is the number of training instances forthe category c.5Step 5 Finally, the instance i is categorized withthat concept having the maximum score:c?=(argmaxcscif sc> ?;?
otherwise.
(2)4The pointwise mutual information is defined as the log ofthe deviation between the observed frequency of a n-gram andthe probability of that n-gram if it were independent and theself-information is a measure of the information content of an-gram (?
log p, where p is the probability of the n-gram).5Experiments using the sum of average or argmax scoreyield worst results.Where a higher value of the parameter ?
increasesprecision but degrades recall.4 BenchmarksFor evaluating the proposed algorithm and com-paring it with other algorithms, we adopted the twobenchmarks described below.4.1 Tanev and Magnini BenchmarkTanev and Magnini (2006) proposed a benchmarkontology that consists of two high-level namedentity categories (i.e., person and location) bothhaving five fine-grained subclasses (i.e., mountain,lake, river, city, and country as subtypes of loca-tion; statesman, writer, athlete, actor, and inventorare subtypes of person).
WordNet and Wikipediawere used as primary data sources for populatingthe evaluation ontology.
In total, the ontology ispopulated with 280 instances which were not am-biguous (with respect to the ontology).
We ex-tracted the training set from WordNet, collecting20 examples per sub-category, of course, not al-ready contained in the test set.4.2 People OntologyThe benchmark described in the previous sectionis clearly a toy problem, and it does not allow usto evaluate the effectiveness of our method, in par-ticular the ability to perform fine-grained classifi-cations.
To address this problem, we developeda larger ontology of people (called People Ontol-ogy), characterized by a complex taxonomy hav-ing multiple layers and containing thousands of in-stances.
This ontology has been extracted fromWordNet, that we adapted to our purpose after are-engineering phase.
In fact, we need a formalspecification of the conceptualizations that are ex-pressed by means of WordNet?s synsets, and, inparticular, we need a clear distinction between in-dividuals and categories, as well as a robust cate-gorization mechanism to assign individuals to gen-eral concepts.This result can be achieved by following the di-rectives defined by Gangemi et al (2003) for On-toWordNet, in which the informal WordNet se-mantics is re-engineered in terms of a descriptionlogic.
We follow an analogous approach.
Firstly,any possible instance in WordNet 1.7.1 has beenidentified by looking for all those synsets contain-ing at least one word starting with a capital letter.The result is a set of instances I .
All the remaining267Figure 1: The taxonomy of the People Ontology extracted from WordNet 1.7.1.
Numbers in brackets arethe total numbers of individuals per category.
Concepts that have less than 40 instances were removed.synsets are then regarded as concepts, collected inthe set C. Then, is a relations between synsets areconverted into one of the following standard OWL-DL constructs:X subclass of Y if X is a Y and X ?
C and Y ?
CX instance of Y if X is a Y and X ?
I and Y ?
CThe formal semantics of both subclass ofand instance of is formally defined in OWL-DL.
subclass of is a transitive relation (i.e.,Xsubclass ofY and Y subclass ofZ impliesXsubclass ofZ) and the instance of relationhas the following property: Xinstance ofY andY subclass ofZ implies Xinstance ofZ.To define the People Ontology, we selectedthe sub-hierarchy of WordNet representing peo-ple, identifying the corresponding top-level synsetX = {person, individual, someone, somebody,mortal, soul}, and collecting all the classes Y suchthat Y is a subclass of X and all the instances Isuch that I is an instance of Y .
We discoveredthat many concepts in the derived hierarchy wereempty or scarcely populated.
As we need a suffi-cient amount data to obtain statistically significantresults, we eliminated the classes that contain lessthan 40 instances from the ontology.
The derivedontology contains 1627 instances structured in 21sub-categories (Figure 1).
Finally, we randomlysplit its individuals into two equally sized subsets.The results reported in the following section wereevaluated using two-fold cross-validation on thesetwo subsets.5 EvaluationIn this section, we present the performance of theIBOP algorithm on the evaluation benchmarks de-scribed in the previous section.5.1 Experimental SettingFor each individual, we collected 100 entity men-tions in their context by querying GoogleTM.
Asmost of them are names of celebrities, the Webprovided sufficient data.6We approached the population task as a stan-dard categorization problem, trying to assign newinstances to the most specific category.
We mea-sured standard precision/recall figures.
In addition,we evaluated the classifier accuracy at the most ab-stract level, by inheriting the predictions from sub-concepts to super-concepts.
For example, whenan instance is assigned to a specific category (e.g.,Musician), it is also (implicitly) assigned to all itssuper-classes (e.g., Artist and Creator).
This op-eration is performed according to the extensionalsemantics of the description logic, as described inthe previous section.
Following this approach, weare able to evaluate the effectiveness of our algo-rithm at any level of generality.
The micro- andmacro-averaged F1have been evaluated by takinginto account both specific and generic classes atthe same time.
In this way, we tend to penalize the6A study of how the number of snippets N would impactthe performance of the IBOP algorithm has been deferred tofuture work.2680.30.40.50.60.70.810  20  30  40  50Micro-F 1Number of examplesFigure 2: Learning curve on the People Ontology.gross misclassification errors (e.g., Biologist vs.Poet), while minor errors (e.g., Poet vs. Drama-tist) are less relevant.
This approach is similar tothe one proposed by Melamed and Resnik (2000)for a similar hierarchical categorization task.5.2 AccuracyTable 1 shows micro- and macro-averaged resultsof the proposed method obtained on the Tanevand Magnini (2006) benchmark and comparesthem with the class-example (Tanev and Magnini,2006), IBLE (Giuliano and Gliozzo, 2007), andclass-word (Cimiano and V?olker, 2005) methods,respectively.
Table 2 shows micro- and macro-averaged results of the proposed method obtainedon the People Ontology and compares them withthe random and most frequent baseline methods.7In both experiments, the IBOP algorithm wastrained on 20 examples per category and settingthe parameter ?
= 0 in Equation 2.For the People Ontology, we performed a dis-aggregated evaluation, whose results are shown inTable 3, while Figure 2 shows the learning curve.The experiment was conducted setting the param-eter ?
= 0.System Micro-F1Macro-F1IBOP 73 71Class-Example 68 62IBLE 57 47Class-Word 42 33Table 1: Comparison of different ontology popula-tion techniques on the Tanev and Magnini (2006)benchmark.7The most frequent category has been estimated on thetraining data.System Micro-F1Macro-F1IBOP 70.1 62.3Random 15.4 15.5Most Frequent 20.7 3.3Table 2: Comparison between the IBOP algorithmand the baseline methods on the People Ontology.Class Prec Recall F1Scientist 84.4 73.3 78.4Physicist 63.0 39.3 48.4Mathematician 25.0 67.5 36.5Chemist 44.2 52.0 47.7Biologist 62.5 13.2 21.7Social scientist 43.1 30.1 35.5Performer 76.5 66.9 71.4Actor 67.5 67.9 67.7Musician 68.1 48.9 56.9Creator 70.6 84.5 76.9Film Maker 52.9 68.7 59.7Artist 72.8 85.5 78.6Painter 74.4 86.1 79.8Musician 68.9 81.6 74.7Comunicator 76.4 83.1 79.6Writer 78.6 76.6 77.6Poet 67.4 61.2 64.1Dramatist 65.0 70.7 67.7Representative 84.8 76.7 80.6Business man 47.2 40.5 43.6Health professional 29.3 25.0 27.0micro 69.6 70.7 70.1macro 62.3 70.7 62.3Table 3: Results for each category of the PeopleOntology.5.3 Confusion MatrixTable 4 shows the confusion matrix for the PeopleOntology task, in which the rows are ground truthclasses and the columns are predictions.
The ex-periment was conducted using 20 training exam-ples per category and setting the parameter ?
=0.
The matrix has been calculated for the finer-grained categories and, then, grouped according totheir top-level concepts.5.4 Precision/Recall TradeoffFigure 3 shows the precision/recall curve for thePeople Ontology task obtained varying the param-eter ?
in Equation 2.
The experiment was con-ducted using 20 training examples per category.5.5 DiscussionThe results obtained are undoubtedly satisfactory.Table 1 shows that our approach outperforms theother three methods on the Tanev and Magnini(2006) benchmark.
Note that the Class-Exampleapproach has been trained on 1194 named enti-269Scientist Performer Creator Communicator Business HealthPhy Mat Che Bio Soc Act Mus Fil Pai Mus Poe Dra Rep man profPhy 68 40 25 3 11 2 0 0 3 1 7 1 7 2 3Mat 3 27 1 0 0 0 0 1 0 0 4 0 2 1 1Che 12 10 53 2 7 3 1 2 2 0 1 0 4 4 1Bio 4 12 13 10 3 3 0 1 5 2 4 1 11 2 5Soc 6 3 4 1 22 4 0 2 2 3 4 1 12 0 9Act 3 1 2 0 0 106 6 20 0 3 2 4 7 1 1Mus 1 1 2 0 0 16 64 5 2 28 2 2 7 0 1Fil 0 0 0 0 0 7 0 46 0 4 1 1 4 3 1Pai 2 1 0 0 1 1 1 2 93 3 1 0 2 1 0Mus 1 0 0 0 0 1 16 2 3 142 1 3 2 1 2Poe 1 2 1 0 1 2 3 3 6 12 93 20 6 1 1Dra 0 2 1 0 0 3 0 2 2 3 9 65 1 2 2Rep 0 6 7 0 3 6 1 0 3 2 5 0 189 1 0Bus 3 3 6 0 0 0 1 1 0 1 2 0 6 17 2Hea 4 0 5 0 3 3 1 0 4 2 2 2 10 0 12Table 4: Confusion matrix for the finer-grained categories grouped according to their top-level conceptsof the People Ontology.00.20.40.60.810  0.2  0.4  0.6  0.8  1PrecisionRecallterminal conceptsall conceptsFigure 3: Precision/recall curve on the People On-tology.ties, almost 60 examples per category, whereas weachieved the same result with around 10 examplesper category.
On the other hand, as Table 2 shows,the IBOP algorithm is effective in populating amore complex ontology and significantly outper-forms the random and most frequent baselines.An important characteristic of the algorithm isthe small number of examples required per cate-gory.
This affects both the prediction accuracy andthe computation time (this is generally a commonproperty of instance-based algorithms).
Therefore,finding an optimal tradeoff between the trainingsize and the performance is crucial.
The learn-ing curve (Figure 2) shows that the algorithm out-performs the baselines with only 1 example percategory and achieves a good accuracy (F1?67%) with only 10 examples per category, whileit reaches a plateau around 20 examples (F1?70%), but leaving a little room for improvement.Table 4 shows that misclassification errors arelargely distributed among categories belonging tothe same super-class (i.e., the blocks on the maindiagonal are more densely populated than others).As expected, the algorithm is much more accuratefor the top-level concepts (i.e., Scientist, Commu-nicator, etc.
), where the category distinctions areclearer, while a further fine-grained classification,in some cases, is even difficult for human anno-tators.
In particular, results are higher for fine-grained categories densely populated and with asmall number of sibling categories (i.e., Painterand Musician).
We have observed that the resultson sparse categories can be made more precise byincreasing the training size, generally at the ex-pense of a lower recall.We tried to maximize precision by varying theparameter ?
in Equation 2, that is, avoiding allassignments where the plausibility score is lowerthan a given threshold.
Figure 3 shows that theprecision can be significantly enhanced (?
90%)at the expense of poor recall (?
20%), while thealgorithm achieves 80% precision at around 50%recall.Finally, we performed some preliminary erroranalysis, investigating the misclassifications in thecategories Scientists and Musicians.
Several errorsare due to lack of information in WordNet, For ex-ample, Leonhard Euler was a mathematician andphysicist, however, in WordNet, he is classified asphysicist, and our system classifies him as math-ematician.
On the other hand, for simplicity, thealgorithm returns a single category per instance,270however, the test set contains many entities that areclassified in more than one category.
For instance,Bertolt Brecht is both poet and dramatist and thesystem classified him as dramatist.
Another inter-esting case is the presence of two categories Musi-cian, one is subclass of Performer and the other ofArtist, in which, for instance, Ringo Starr is a per-former while John Lennon is an artist, while thesystem classified both as performers.6 Related workBrin (1998) defined a methodology to extract in-formation from the Web starting from a small setof seed examples, then alternately learning extrac-tion patterns from seeds, and further seeds frompatterns.
Despite the fact that the evaluation wason relation extraction the method is general andmight be applied to entity extraction and catego-rization.
The approach was further extended byAgichtein and Gravano (2000).
Our approach dif-fers from theirs in that we do not learn patterns.Thus, we do not require ad hoc strategies for gen-erating patterns and estimating their reliability, acrucial issue in these approaches as ?bad?
patternsmay extract wrong seeds instances that in turn maygenerate even more inaccurate patterns in the fol-lowing iteration.Fleischman and Hovy (2002) approached theontology population problem as a supervised clas-sification task.
They compare different machinelearning algorithms, providing instances in theircontext as training examples as well as more globalsemantic information derived from topic signatureand WordNet.Alfonseca and Manandhar (2002) and Cimianoand V?olker (2005) present similar approaches re-lying on the Harris?
distributional hypothesis andthe vector-space model.
They assign a particu-lar instance represented by a certain context vec-tor to the concept corresponding to the most simi-lar vector.
Contexts are represented using lexical-syntactic features.KnowItAll (Etzioni et al, 2005) uses a searchengine and semantic patterns (similar to those de-fined by Hearst (1992)) to classify named entitieson the Web.
The approach uses simple techniquesfrom the ontology learning field to perform extrac-tion and then annotation.
It also is able to performvery simple pattern induction, consisting of look-ing at n words before and n words after the occur-rence of an example in the document.
With pat-tern learning, KnowItAll becomes a bootstrappedlearning system, where rules are used to learn newseeds, which in turn are used to learn new rules.A similar approach is used in C-PANKOW (Cimi-ano et al, 2005).
Compared to KnowItAll andC-PANKOW, our approach does not need hand-crafted patterns as input.
They are implicitly foundby substituting the training instances in the con-texts of the input entities.
Another key differenceis that concepts in the ontology do not need to belexicalized.Tanev and Magnini (2006) proposed a weakly-supervised method that requires as training data alist of terms without context for each category un-der consideration.
Given a generic syntacticallyparsed corpus containing at least each training en-tity twice, the algorithm learns, for each category, afeature vector describing the contexts where thoseentities occur.
Then, it compares the new (un-known) entity with the so obtained feature vec-tors, assigning it to the most similar category.
Eventhough we used a significantly smaller number oftraining instances, we obtained better results ontheir benchmark.More recently, Giuliano and Gliozzo (2007)proposed an unsupervised approach based on lexi-cal entailment, consisting in assigning an entity tothe category whose lexicalization can be replacedwith its occurrences in a corpus preserving themeaning.
A disadvantage is that the concepts in theontology have to be lexicalized, as they are usedas training examples.
Our approach is based on asimilar idea, but with the main difference that aninstance is substituted with other instances ratherthan with their category names.
Considering that,in most of the cases, ontologies are partially popu-lated during the development phase, and hence theannotation cost is marginal, our approach is a re-alistic alternative for practical ontology populationproblems.7 Conclusions and Future WorkWe have described an instance-based algorithmfor automatic fine-grained categorization of namedentities, previously identified by an entity recogni-tion system or already present in a database.
Thismethod is meant to provide an effective solution tothe ontology population problem.
It exploits theWeb or a domain corpus to collect evidence of thenew instances and does not require labeled textsfor supervision, but a partially populated ontology.271The experimental results show that, where a com-parison is possible, our method outperforms previ-ous methods and it can be applied to different do-mains and languages to (semi-) automatically en-rich an existing ontology.Future work will address the definition of a hi-erarchical categorization strategy where instancesare classified in a top-down manner, in order to ef-ficiently populate very large ontologies, since weplan to apply this method to extract structured in-formation from Wikipedia.
Furthermore, we willinvestigate how co-reference resolution might wellbenefit from our ontology classification.
Finally,we plan to exploit the IBOP algorithm for ontol-ogy mapping and multilingual alignment of lexicalresources.AcknowledgmentsClaudio Giuliano is supported by the X-Mediaproject (http://www.x-media-project.org), sponsored by the European Commission aspart of the Information Society Technologies (IST)program under EC grant number IST-FP6-026978.ReferencesAgichtein, Eugene and Luis Gravano.
2000.
Snowball:extracting relations from large plain-text collections.In DL ?00: Proceedings of the fifth ACM conferenceon Digital libraries, pages 85?94, New York, NY,USA.
ACM.Alfonseca, Enrique and Suresh Manandhar.
2002.
Ex-tending a lexical ontology by a combination of dis-tributional semantics signatures.
In EKAW ?02: Pro-ceedings of the 13th International Conference onKnowledge Engineering and Knowledge Manage-ment.
Ontologies and the Semantic Web, pages 1?7,London, UK.
Springer-Verlag.Brin, Sergey.
1998.
Extracting patterns and rela-tions from the world wide web.
In WebDB Work-shop at 6th International Conference on ExtendingDatabase Technology, EDBT?98.Cimiano, Philipp and Johanna V?olker.
2005.
Towardslarge-scale, open-domain and ontology-based namedentity classification.
In Proceedings of RANLP?05,pages 66?
166?172, Borovets, Bulgaria.Cimiano, Philipp, G?unter Ladwig, and Steffen Staab.2005.
Gimme the context: Context-driven automaticsemantic annotation with C-PANKOW.
In Ellis, Al-lan and Tatsuya Hagino, editors, Proceedings of the14th World Wide Web Conference, pages 332 ?
341,Chiba, Japan, MAY.
ACM Press.Etzioni, Oren, Michael Cafarella, Doug Downey,Ana M. Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Un-supervised named-entity extraction from the web:An experimental study.
Artificial Intelligence,165(1):191?134.Fellbaum, Christiane.
1998.
WordNet.
An ElectronicLexical Database.
MIT Press, Cambridge, MA.Fleischman, Michael and Eduard Hovy.
2002.
Finegrained classification of named entities.
In Proceed-ings of the 19th International Conference on Com-putational Linguistics, Taipei, Taiwan.Gangemi, Aldo, Roberto Navigli, and Paola Velardi.2003.
Axiomatizing WordNet glosses in the On-toWordNet project.
In Proocedings of the Workshopon Human Language Technology for the SemanticWeb and Web Services at ISWC 2003, Sanibel Island,Florida.Giuliano, Claudio and Alfio Gliozzo.
2007.
In-stance based lexical entailment for ontology popu-lation.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 248?256.Giuliano, Claudio, Alfio Gliozzo, and Carlo Strappar-ava.
2007.
Fbk-irst: Lexical substitution task ex-ploiting domain and syntagmatic coherence.
In Pro-ceedings of the Fourth International Workshop onSemantic Evaluations (SemEval-2007), pages 145?148, Prague, Czech Republic, June.Harris, Zellig.
1954.
Distributional structure.
WORD,10:146?162.Hearst, Marti A.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe Fourteenth International Conference on Compu-tational Linguistics, Nantes, France, July.McCarthy, Diana and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
InProceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007), pages 48?53, Prague, Czech Republic, June.McCarthy, Diana.
2002.
Lexical substitution as a taskfor WSD evaluation.
In Proceedings of the ACL-02 workshop on Word Sense Disambiguation, pages109?115, Morristown, NJ, USA.Melamed, I. Dan and Philip Resnik.
2000.
Tagger eval-uation given hierarchical tag sets.
Computers andthe Humanities, pages 79?84.Tanev, Hristo and Bernardo Magnini.
2006.
Weaklysupervised approaches for ontology population.
InProceedings of the Eleventh Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL-2006), Trento, Italy.272
