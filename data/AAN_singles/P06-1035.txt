Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 273?280,Sydney, July 2006. c?2006 Association for Computational LinguisticsMeasuring Language Divergence by Intra-Lexical ComparisonT.
Mark EllisonInformaticsUniversity of Edinburghmark@markellison.netSimon KirbyLanguage Evolution and Computation Research UnitPhilosophy, Psychology and Language Sciences,University of Edinburghsimon@ling.ed.ac.ukAbstractThis paper presents a method for build-ing genetic language taxonomies basedon a new approach to comparing lexi-cal forms.
Instead of comparing formscross-linguistically, a matrix of language-internal similarities between forms is cal-culated.
These matrices are then com-pared to give distances between languages.We argue that this coheres better withcurrent thinking in linguistics and psy-cholinguistics.
An implementation of thisapproach, called PHILOLOGICON, is de-scribed, along with its application to Dyenet al?s (1992) ninety-five wordlists fromIndo-European languages.1 IntroductionRecently, there has been burgeoning interest inthe computational construction of genetic lan-guage taxonomies (Dyen et al, 1992; Nerbonneand Heeringa, 1997; Kondrak, 2002; Ringe etal., 2002; Benedetto et al, 2002; McMahonand McMahon, 2003; Gray and Atkinson, 2003;Nakleh et al, 2005).One common approach to building languagetaxonomies is to ascribe language-language dis-tances, and then use a generic algorithm to con-struct a tree which explains these distances asmuch as possible.
Two questions arise with thisapproach.
The first asks what aspects of lan-guages are important in measuring inter-languagedistance.
The second asks how to measure dis-tance given these aspects.A more traditional approach to building lan-guage taxonomies (Dyen et al, 1992) answersthese questions in terms of cognates.
A word inlanguage A is said to be cognate with word in lan-guage B if the forms shared a common ancestorin the parent language of A and B.
In the cognate-counting method, inter-language distance dependson the lexical forms of the languages.
The dis-tance between two languages is a function of thenumber or fraction of these forms which are cog-nate between the two languages1 .
This approachto building language taxonomies is hard to imple-ment in toto because constructing ancestor formsis not easily automatable.More recent approaches, such as Kondrak?s(2002) and Heggarty et als (2005) work on di-alect comparison, take the synchronic word formsthemselves as the language aspect to be compared.Variations on edit distance (see Kessler (2005) fora survey) are then used to evaluate differences be-tween languages for each word, and these differ-ences are aggregated to give a distance betweenlanguages or dialects as a whole.
This approachis largely automatable, although some methods dorequire human intervention.In this paper, we present novel answers to thetwo questions.
The features of language we willcompare are not sets of words or phonologicalforms.
Instead we compare the similarities be-tween forms, expressed as confusion probabilities.The distribution of confusion probabilities in onelanguage is called a lexical metric.
Section 2presents the definition of lexical metrics and somearguments for their being good language represen-tatives for the purposes of comparison.The distance between two languages is the di-vergence their lexical metrics.
In section 3, wedetail two methods for measuring this divergence:1McMahon and McMahon (2003) for an account of tree-inference from the cognate percentages in the Dyen et al(1992) data.273Kullback-Liebler (herafter KL) divergence andRao distance.
The subsequent section (4) de-scribes the application of our approach to automat-ically constructing a taxonomy of Indo-Europeanlanguages from Dyen et al (1992) data.Section 5 suggests how lexical metrics can helpidentify cognates.
The final section (6) presentsour conclusions, and discusses possible future di-rections for this work.Versions of the software and data files describedin the paper will be made available to coincidewith its publication.2 Lexical MetricThe first question posed by the distance-based ap-proach to genetic language taxonomy is: whatshould we compare?In some approaches (Kondrak, 2002; McMahonet al, 2005; Heggarty et al, 2005; Nerbonne andHeeringa, 1997), the answer to this question is thatwe should compare the phonetic or phonologicalrealisations of a particular set of meanings acrossthe range of languages being studied.
There area number of problems with using lexical forms inthis way.Firstly, in order to compare forms from differ-ent languages, we need to embed them in com-mon phonetic space.
This phonetic space providesgranularity, marking two phones as identical ordistinct, and where there is a graded measure ofphonetic distinction it measures this.There is growing doubt in the field of phonol-ogy and phonetics about the meaningfulness of as-suming of a common phonetic space.
Port andLeary (2005) argue convincingly that this assump-tion, while having played a fundamental role inmuch recent linguistic theorising, is neverthelessunfounded.
The degree of difference betweensounds, and consequently, the degree of phoneticdifference between words can only be ascertainedwithin the context of a single language.It may be argued that a common phonetic spacecan be found in either acoustics or degrees of free-dom in the speech articulators.
Language-specificcategorisation of sound, however, often restruc-tures this space, sometimes with distinct soundsbeing treated as homophones.
One example ofthis is the realisation of orthographic rr in Euro-pean Portuguese: it is indifferently realised withan apical or a uvular trill, different sounds made atdistinct points of articulation.If there is no language-independent, commonphonetic space with an equally common similar-ity measure, there can be no principled approachto comparing forms in one language with those ofanother.In contrast, language-specific word-similarity iswell-founded.
A number of psycholinguistic mod-els of spoken word recognition (Luce et al, 1990)are based on the idea of lexical neighbourhoods.When a word is accessed during processing, theother words that are phonemically or orthograph-ically similar are also activated.
This effect canbe detected using experimental paradigms such aspriming.Our approach, therefore, is to abandon thecross-linguistic comparison of phonetic realisa-tions, in favour of language-internal comparisonof forms.
(See also work by Shillcock et al (2001)and Tamariz (2005)).2.1 Confusion probabilitiesOne psychologically well-grounded way of de-scribing the similarity of words is in terms of theirconfusion probabilities.
Two words have highconfusion probability if it is likely that one wordcould be produced or understood when the otherwas intended.
This type of confusion can be mea-sured experimentally by giving subjects words innoisy environments and measuring what they ap-prehend.A less pathological way in which confusionprobability is realised is in coactivation.
If a per-son hears a word, then they more easily and morequickly recognise similar words.
This coactiva-tion occurs because the phonological realisationof words is not completely separate in the mind.Instead, realisations are interdependent with reali-sations of similar words.We propose that confusion probabilities areideal information to constitute the lexical met-ric.
They are language-specific, psychologicallygrounded, can be determined by experiment, andintegrate with existing psycholinguistic models ofword recognition.2.2 NAM and beyondUnfortunately, experimentally determined confu-sion probabilities for a large number of languagesare not available.
Fortunately, models of spokenword recognition allow us to predict these proba-bilities from easily-computable measures of wordsimilarity.274For example, the neighbourhood activationmodel (NAM) (Luce et al, 1990; Luce and Pisoni,1998) predicts confusion probabilities from therelative frequency of words in the neighbourhoodof the target.
Words are in the neighbourhood ofthe target if their Levenstein (1965) edit distancefrom the target is one.
The more frequent the wordis, the greater its likelihood of replacing the target.Bailey and Hahn (2001) argue, however, that theall-or-nothing nature of the lexical neighbourhoodis insufficient.
Instead word similarity is the com-plex function of frequency and phonetic similarityshown in equation (1).
Here A,B,C and D areconstants of the model, u and v are words, and dis a phonetic similarity model.s = (AF (u)2 + BF (u) + C)e?D.d(u,v) (1)We have adapted this model slightly, in line withNAM, taking the similarity s to be the probabil-ity of confusing stimulus v with form u.
Also, asour data usually offers no frequency information,we have adopted the maximum entropy assump-tion, namely, that all relative frequencies are equal.Consequently, the probability of confusion of twowords depends solely on their similarity distance.While this assumption degrades the psychologicalreality of the model, it does not render it useless, asthe similarity measure continues to provide impor-tant distinctions in neighbourhood confusability.We also assume for simplicity, that the constantD has the value 1.With these simplifications, equation (2) showsthe probability of apprehending word w, out ofa set W of possible alternatives, given a stimulusword ws.P (w|ws) = e?d(w,ws)/N(ws) (2)The normalising constant N(s) is the sum of thenon-normalised values for e?d(w,ws) for all wordsw.N(ws) =?w?We?d(u,v)2.3 Scaled edit distancesKidd and Watson (1992) have shown that discrim-inability of frequency and of duration of tones ina tone sequence depends on its length as a pro-portion of the length of the sequence.
Kapatsinski(2006) uses this, with other evidence, to argue thatword recognition edit distances must be scaled byword-length.There are other reasons for coming to the sameconclusion.
The simple Levenstein distance exag-gerates the disparity between long words in com-parison with short words.
A word of consisting of10 symbols, purely by virtue of its length, will onaverage be marked as more different from otherwords than a word of length two.
For example,Levenstein distance between interested and rest issix, the same as the distance between rest and by,even though the latter two have nothing in com-mon.
As a consequence, close phonetic transcrip-tions, which by their very nature are likely to in-volve more symbols per word, will result in largeredit distances than broad phonemic transcriptionsof the same data.To alleviate this problem, we define a new editdistance function d2 which scales Levenstein dis-tances by the average length of the words beingcompared (see equation 3).
Now the distance be-tween interested and rest is 0.86, while that be-tween rest and by is 2.0, reflecting the greater rel-ative difference in the second pair.d2(w2, w1) =2d(w2, w1)|w1|+ |w2|(3)Note that by scaling the raw edit distance withthe average lengths of the words, we are preserv-ing the symmetric property of the distance mea-sure.There are other methods of comparing strings,for example string kernels (Shawe-Taylor andCristianini, 2004), but using Levenstein distancekeeps us coherent with the psycholinguistic ac-counts of word similarity.2.4 Lexical MetricBringing this all together, we can define the lexicalmetric.A lexicon L is a mapping from a set of mean-ings M , such as ?DOG?, ?TO RUN?, ?GREEN?,etc., onto a set F of forms such as /pies/, /biec/,/zielony/.The confusion probability P of m1 for m2 inlexical L is the normalised negative exponentialof the scaled edit-distance of the correspondingforms.
It is worth noting that when frequenciesare assumed to follow the maximum entropy dis-tribution, this connection between confusion prob-abilities and distances (see equation 4) is the sameas that proposed by Shepard (1987).275P (m1|m2;L) =e?d2(L(m1),L(m2))N(m2;L)(4)A lexical metric of L is the mapping LM(L) :M2 ?
[0, 1] which assigns to each pair of mean-ings m1,m2 the probability of confusing m1 form2, scaled by the frequency of m2.LM(L)(m1,m2)= P (L(m1)|L(m2))P (m2)= e?d2(L(m1),L(m2))N(m2;L)|M |where N(m2;L) is the normalising function de-fined in equation (5).N(m2;L) =?m?Me?d2(L(m),L(m2)) (5)Table 1 shows a minimal lexicon consisting onlyof the numbers one to five, and a correspondinglexical metric.
The values in the lexical metric areone two three four fiveone 0.102 0.027 0.023 0.024 0.024two 0.028 0.107 0.024 0.026 0.015three 0.024 0.024 0.107 0.023 0.023four 0.025 0.025 0.022 0.104 0.023five 0.026 0.015 0.023 0.025 0.111Table 1: A lexical metric on a mini-lexicon con-sisting of the numbers one to five.inferred word confusion probabilities.
The matrixis normalised so that the sum of each row is 0.2,ie.
one-fifth for each of the five words, so the totalof the matrix is one.
Note that the diagonal valuesvary because the off-diagonal values in each rowvary, and consequently, so does the normalisationfor the row.3 Language-Language DistanceIn the previous section, we introduced the lexi-cal metric as the key measurable for comparinglanguages.
Since lexical metrics are probabilitydistributions, comparison of metrics means mea-suring the difference between probability distri-butions.
To do this, we use two measures: thesymmetric Kullback-Liebler divergence (Jeffreys,1946) and the Rao distance (Rao, 1949; Atkinsonand Mitchell, 1981; Micchelli and Noakes, 2005)based on Fisher Information (Fisher, 1959).
Thesecan be defined in terms the geometric path fromone distribution to another.3.1 Geometric pathsThe geometric path between two distributions Pand Q is a conditional distribution R with a con-tinuous parameter ?
such that at ?
= 0, the distri-bution is P , and at ?
= 1 it is Q.
This conditionaldistribution is called the geometric because it con-sists of normalised weighted geometric means ofthe two defining distributions (equation 6).R(w?|?)
= P (w?)?Q(w?)1??/k(?
;P,Q) (6)The function k(?
;P,Q) is a normaliser for theconditional distribution, being the sum of theweighted geometric means of values from P andQ (equation 7).
This value is known as theChernoff coefficient or Helliger path (Basseville,1989).
For brevity, the P,Q arguments to k willbe treated as implicit and not expressed in equa-tions.k(?)
=?w?
?W 2P (w?)1??Q(w?)?
(7)3.2 Kullback-Liebler distanceThe first-order (equation 8) differential of the nor-maliser with regard to ?
is of particular interest.k?(?)
=?w?
?W 2log Q(w?
)P (w?
)P (w?)1??Q(w?)?
(8)At ?
= 0, this value is the negative of theKullback-Liebler distance KL(P |Q) of Q with re-gard to P (Basseville, 1989).
At ?
= 1, it is theKullback-Liebler distance KL(Q|P ) of P with re-gard to Q. Jeffreys?
(1946) measure is a symmetri-sation of KL distance, by averaging the commuta-tions (equations 9,10).KL(P,Q) = KL(Q|P ) + KL(P |Q)2 (9)= k?(1)?
k?
(0)2 (10)3.3 Rao distanceRao distance depends on the second-order (equa-tion 11) differential of the normaliser with regardto ?.k??(?)
=?w?
?W 2log2 Q(w?
)P (w?
)P (w?)1??Q(w?)?
(11)Fisher information is defined as in equation (12).FI(P, x) = ??
?2 log P (y|x)?x2 P (y|x)dy (12)276Equation (13) expresses Fisher information alongthe path R from P to Q at point ?
using k and itsfirst two derivatives.FI(R,?)
= k(?)k??(?)?
k?(?)2k(?
)2 (13)The Rao distance r(P,Q) along R can be approxi-mated by the square root of the Fisher informationat the path?s midpoint ?
= 0.5.r(P,Q) =?k(0.5)k??
(0.5) ?
k?
(0.5)2k(0.5)2 (14)3.4 The PHILOLOGICON algorithmBringing these pieces together, the PHILOLOGI-CON algorithm for measuring the divergence be-tween two languages has the following steps:1. determine their joint confusion probabilitymatrices, P and Q,2.
substitute these into equation (7), equation(8) and equation (11) to calculate k(0),k(0.5), k(1), k?
(0.5), and k??(0.5),3.
and put these into equation (10) and equation(14) to calculate the KL and Rao distancesbetween between the languages.4 Indo-EuropeanThe ideal data for reconstructing Indo-Europeanwould be an accurate phonemic transcription ofwords used to express specifically defined mean-ings.
Sadly, this kind of data is not readily avail-able.
However, as a stop-gap measure, we canadopt the data that Dyen et al collected to con-struct a Indo-European taxonomy using the cog-nate method.4.1 Dyen et als dataDyen et al (1992) collected 95 data sets, each pair-ing a meaning from a Swadesh (1952)-like 200-word list with its expression in the correspondinglanguage.
The compilers annotated with data withcognacy relations, as part of their own taxonomicanalysis of Indo-European.There are problems with using Dyen?s data forthe purposes of the current paper.
Firstly, the wordforms collected are not phonetic, phonological oreven full orthographic representations.
As the au-thors state, the forms are expressed in sufficientdetail to allow an interested reader acquainted withthe language in question to identify which word isbeing expressed.Secondly, many meanings offer alternativeforms, presumably corresponding to synonyms.For a human analyst using the cognate approach,this means that a language can participate in two(or more) word-derivation systems.
In preparingthis data for processing, we have consistently cho-sen the first of any alternatives.A further difficulty lies in the fact that many lan-guages are not represented by the full 200 mean-ings.
Consequently, in comparing lexical metricsfrom two data sets, we frequently need to restrictthe metrics to only those meanings expressed inboth the sets.
This means that the KL divergenceor the Rao distance between two languages weremeasured on lexical metrics cropped and rescaledto the meanings common to both data-sets.
Inmost cases, this was still more than 190 words.Despite these mismatches between Dyen et al?sdata and our needs, it provides an testbed for thePHILOLOGICON algorithm.
Our reasoning being,that if successful with this data, the method is rea-sonably reliable.
Data was extracted to language-specific files, and preprocessed to clean up prob-lems such as those described above.
An additionaldata-set was added with random data to act as anoutlier to root the tree.4.2 Processing the dataPHILOLOGICON software was then used to calcu-late the lexical metrics corresponding to the indi-vidual data files and to measure KL divergencesand Rao distances between them.
The programNEIGHBOR from the PHYLIP2 package was usedto construct trees from the results.4.3 The resultsThe tree based on Rao distances is shown in figure1.
The discussion follows this tree except in thosefew cases mentioning differences in the KL tree.The standard against which we measure the suc-cess of our trees is the conservative traditional tax-onomy to be found in the Ethnologue (Grimesand Grimes, 2000).
The fit with this taxonomywas so good that we have labelled the majorbranches with their traditional names: Celtic, Ger-manic, etc.
In fact, in most cases, the branch-internal divisions ?
eg.
Brythonic/Goidelic inCeltic, Western/Eastern/Southern in Slavic, or2See http://evolution.genetics.washington.edu/phylip.html.277Western/Northern in Germanic ?
also accord.Note that PHILOLOGICON even groups Baltic andSlavic together into a super-branch Balto-Slavic.Where languages are clearly out of place incomparison to the traditional taxonomy, these arehighlighted: visually in the tree, and verbally inthe following text.
In almost every case, there areobvious contact phenomena which explain the de-viation from the standard taxonomy.Armenian was grouped with the Indo-Iranianlanguages.
Interestingly, Armenian was at firstthought to be an Iranian language, as it sharesmuch vocabulary with these languages.
The com-mon vocabulary is now thought to be the resultof borrowing, rather than common genetic origin.In the KL tree, Armenian is placed outside of theIndo-Iranian languages, except for Gypsy.
On theother hand, in this tree, Ossetic is placed as anoutlier of the Indian group, while its traditionalclassification (and the Rao distance tree) puts itamong the Iranian languages.
Gypsy is an Indianlanguage, related to Hindi.
It has, however, beensurrounded by European languages for some cen-turies.
The effects of this influence is the likelycause for it being classified as an outlier in theIndo-Iranian family.
A similar situation exists forSlavic: one of the two lists that Dyen et al of-fer for Slovenian is classed as an outlier in Slavic,rather than classifying it with the Southern Slaviclanguages.
The other Slovenian list is classifiedcorrectly with Serbocroatian.
It is possible thatthe significant impact of Italian on Slovenian hasmade it an outlier.
In Germanic, it is English thatis the outlier.
This may be due to the impact of theEnglish creole, Takitaki, on the hierarchy.
Thislanguage is closest to English, but is very distinctfrom the rest of the Germanic languages.
Anothermisclassification also is the result of contact phe-nomena.
According to the Ethnologue, Sardinianis Southern Romance, a separate branch from Ital-ian or from Spanish.
However, its constant contactwith Italian has influenced the language such thatit is classified here with Italian.
We can offer noexplanation for why Wakhi ends up an outlier toall the groups.In conclusion, despite the noisy state of Dyen etal.
?s data (for our purposes), the PHILOLOGICONgenerates a taxonomy close to that constructed us-ing the traditional methods of historical linguis-tics.
Where it deviates, the deviation usuallypoints to identifiable contact between languages.GreekIndo?IranianAlbanianBalto?SlavicGermanicRomanceCelticWakhiGreek DGreek MDGreek MLGreek ModGreek KAfghanWaziriArmenian ListBaluchiPersian ListTadzikOsseticBengaliHindiLahndaPanjabi STGujaratiMarathiKhaskuraNepali ListKashmiriSinghaleseGypsy GkALBANIANAlbanian GAlbanian CAlbanian KAlbanian TopAlbanian TBulgarianBULGARIAN PMACEDONIAN PMacedonianSerbocroatianSERBOCROATIAN PSLOVENIAN PByelorussianBYELORUSSIAN PRussianRUSSIAN PUkrainianUKRAINIAN PCzechCZECH PSlovakSLOVAK PCzech ELusatian LLusatian UPolishPOLISH PSlovenianLatvianLithuanian OLithuanian STAfrikaansDutch ListFlemishFrisianGerman STPenn DutchDanishRiksmalSwedish ListSwedish UpSwedish VLFaroeseIcelandic STEnglish STTakitakiBrazilianPortuguese STSpanishCatalanItalianSardinian LSardinian NLadinFrenchWalloonProvencalFrench Creole CFrench Creole DRumanian ListVlachBreton ListBreton STBreton SEWelsh CWelsh NIrish AIrish BRandomArmenian ModSardinian CFigure 1: Taxonomy of 95 Indo-European datasets and artificial outlier using PHILOLOGICONand PHYLIP2785 Reconstruction and CognacySubsection 3.1 described the construction of geo-metric paths from one lexical metric to another.This section describes how the synthetic lexicalmetric at the midpoint of the path can indicatewhich words are cognate between the two lan-guages.The synthetic lexical metric (equation 15) ap-plies the formula for the geometric path equation(6) to the lexical metrics equation (5) of the lan-guages being compared, at the midpoint ?
= 0.5.R 12(m1,m2) =?P (m1|m2)Q(m1|m2)|M |k(12 )(15)If the words for m1 and m2 in both languages havecommon origins in a parent language, then it isreasonable to expect that their confusion probabil-ities in both languages will be similar.
Of coursedifferent cognate pairs m1,m2 will have differingvalues for R, but the confusion probabilities in Pand Q will be similar, and consequently, the rein-force the variance.If either m1 or m2, or both, is non-cognate, thatis, has been replaced by another arbitrary format some point in the history of either language,then the P and Q for this pair will take indepen-dently varying values.
Consequently, the geomet-ric mean of these values is likely to take a valuemore closely bound to the average, than in thepurely cognate case.Thus rows in the lexical metric with wider dy-namic ranges are likely to correspond to cognatewords.
Rows corresponding to non-cognates arelikely to have smaller dynamic ranges.
The dy-namic range can be measured by taking the Shan-non information of the probabilities in the row.Table 2 shows the most low- and high-information rows from English and Swedish(Dyen et als (1992) data).
At the extremes oflow and high information, the words are invari-ably cognate and non-cognate.
Between these ex-tremes, the division is not so clear cut, due tochance effects in the data.6 Conclusions and Future DirectionsIn this paper, we have presented a distance-based method, called PHILOLOGICON, that con-structs genetic trees on the basis of lexicafrom each language.
The method only com-pares words language-internally, where compari-son seems both psychologically real and reliable,English Swedish 104(h?
h?
)Low Informationwe vi -1.30here her -1.19to sit sitta -1.14to flow flyta -1.04wide vid -0.97:scratch klosa 0.78dirty smutsig 0.79left (hand) vanster 0.84because emedan 0.89High InformationTable 2: Shannon information of confusion dis-tributions in the reconstruction of English andSwedish.
Information levels are shown translatedso that the average is zero.and never cross-linguistically, where comparisonis less well-founded.
It uses measures foundedin information theory to compare the intra-lexicaldifferences.The method successfully, if not perfectly, recre-ated the phylogenetic tree of Indo-European lan-guages on the basis of noisy data.
In further work,we plan to improve both the quantity and the qual-ity of the data.
Since most of the mis-placementson the tree could be accounted for by contact phe-nomena, it is possible that a network-drawing,rather than tree-drawing, analysis would producebetter results.Likewise, we plan to develop the methodfor identifying cognates.
The key improvementneeded is a way to distinguish indeterminate dis-tances in reconstructed lexical metrics from deter-minate but uniform ones.
This may be achieved byretaining information about the distribution of theoriginal values which were combined to form thereconstructed metric.ReferencesC.
Atkinson and A.F.S.
Mitchell.
1981.
Rao?s distancemeasure.
Sankhya?, 4:345?365.Todd M. Bailey and Ulrike Hahn.
2001.
Determinantsof wordlikeness: Phonotactics or lexical neighbor-hoods?
Journal of Memory and Language, 44:568?591.Michle Basseville.
1989.
Distance measures for signalprocessing and pattern recognition.
Signal Process-ing, 18(4):349?369, December.279D.
Benedetto, E. Caglioti, and V. Loreto.
2002.
Lan-guage trees and zipping.
Physical Review Letters,88.Isidore Dyen, Joseph B. Kruskal, and Paul Black.1992.
An indo-european classification: a lexicosta-tistical experiment.
Transactions of the AmericanPhilosophical Society, 82(5).R.A.
Fisher.
1959.
Statistical Methods and ScientificInference.
Oliver and Boyd, London.Russell D. Gray and Quentin D. Atkinson.
2003.Language-tree divergence times support the ana-tolian theory of indo-european origin.
Nature,426:435?439.B.F.
Grimes and J.E.
Grimes, editors.
2000.
Ethno-logue: Languages of the World.
SIL International,14th edition.Paul Heggarty, April McMahon, and Robert McMa-hon, 2005.
Perspectives on Variation, chapter Fromphonetic similarity to dialect classification.
Moutonde Gruyter.H.
Jeffreys.
1946.
An invariant form for the prior prob-ability in estimation problems.
Proc.
Roy.
Soc.
A,186:453?461.Vsevolod Kapatsinski.
2006.
Sound similarity rela-tions in the mental lexicon: Modeling the lexicon asa complex network.
Technical Report 27, IndianaUniversity Speech Research Lab.Brett Kessler.
2005.
Phonetic comparison algo-rithms.
Transactions of the Philological Society,103(2):243?260.Gary R. Kidd and C.S.
Watson.
1992.
The?proportion-of-the-total-duration rule for the dis-crimination of auditory patterns.
Journal of theAcoustic Society of America, 92:3109?3118.Grzegorz Kondrak.
2002.
Algorithms for LanguageReconstruction.
Ph.D. thesis, University of Toronto.V.I.
Levenstein.
1965.
Binary codes capable of cor-recting deletions, insertions and reversals.
DokladyAkademii Nauk SSSR, 163(4):845?848.Paul Luce and D. Pisoni.
1998.
Recognizing spokenwords: The neighborhood activation model.
Earand Hearing, 19:1?36.Paul Luce, D. Pisoni, and S. Goldinger, 1990.
Cog-nitive Models of Speech Perception: Psycholinguis-tic and Computational Perspectives, chapter Simi-larity neighborhoods of spoken words, pages 122?147.
MIT Press, Cambridge, MA.April McMahon and Robert McMahon.
2003.
Find-ing families: quantitative methods in language clas-sification.
Transactions of the Philological Society,101:7?55.April McMahon, Paul Heggarty, Robert McMahon,and Natalia Slaska.
2005.
Swadesh sublists and thebenefits of borrowing: an andean case study.
Trans-actions of the Philological Society, 103(2):147?170.Charles A. Micchelli and Lyle Noakes.
2005.
Rao dis-tances.
Journal of Multivariate Analysis, 92(1):97?115.Luay Nakleh, Tandy Warnow, Don Ringe, andSteven N. Evans.
2005.
A comparison ofphylogenetic reconstruction methods on an iedataset.
Transactions of the Philological Society,103(2):171?192.J.
Nerbonne and W. Heeringa.
1997.
Measuringdialect distance phonetically.
In Proceedings ofSIGPHON-97: 3rd Meeting of the ACL Special In-terest Group in Computational Phonology.B.
Port and A. Leary.
2005.
Against formal phonology.Language, 81(4):927?964.C.R.
Rao.
1949.
On the distance between two popula-tions.
Sankhya?, 9:246?248.D.
Ringe, Tandy Warnow, and A. Taylor.
2002.
Indo-european and computational cladistics.
Transac-tions of the Philological Society, 100(1):59?129.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press.R.N.
Shepard.
1987.
Toward a universal law of gen-eralization for physical science.
Science, 237:1317?1323.Richard C. Shillcock, Simon Kirby, Scott McDonald,and Chris Brew.
2001.
Filled pauses and their statusin the mental lexicon.
In Proceedings of the 2001Conference of Disfluency in Spontaneous Speech,pages 53?56.M.
Swadesh.
1952.
Lexico-statistic dating of prehis-toric ethnic contacts.
Proceedings of the Americanphilosophical society, 96(4).Monica Tamariz.
2005.
Exploring the Adaptive Struc-ture of the Mental Lexicon.
Ph.D. thesis, Universityof Edinburgh.280
