Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 439?444,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsConfidence-Weighted Learning of Factored Discriminative LanguageModelsViet Ha-ThucComputer Science DepartmentThe University of IowaIowa City, IA 52241, USAhviet@cs.uiowa.eduNicola CanceddaXerox Research Centre Europe6, chemin de Maupertuis38240 Meylan, FranceNicola.Cancedda@xrce.xerox.comAbstractLanguage models based on word surfaceforms only are unable to benefit from avail-able linguistic knowledge, and tend to sufferfrom poor estimates for rare features.
We pro-pose an approach to overcome these two lim-itations.
We use factored features that canflexibly capture linguistic regularities, and weadopt confidence-weighted learning, a form ofdiscriminative online learning that can bettertake advantage of a heavy tail of rare features.Finally, we extend the confidence-weightedlearning to deal with label noise in trainingdata, a common case with discriminative lan-guage modeling.1 IntroductionLanguage Models (LMs) are key components inmost statistical machine translation systems, wherethey play a crucial role in promoting output fluency.Standard n-gram generative language modelshave been extended in several ways.
Generativefactored language models (Bilmes and Kirchhoff,2003) represent each token by multiple factors ?such as part-of-speech, lemma and surface form?and capture linguistic patterns in the target languageat the appropriate level of abstraction.
Instead ofestimating likelihood, discriminative language mod-els (Roark et al, 2004; Roark et al, 2007; Li andKhudanpur, 2008) directly model fluency by castingthe task as a binary classification or a ranking prob-lem.
The method we propose combines advantagesof both directions mentioned above.
We use factoredfeatures to capture linguistic patterns and discrim-inative learning for directly modeling fluency.
Wedefine highly overlapping and correlated factoredfeatures, and extend a robust learning algorithm tohandle them and cope with a high rate of label noise.For discriminatively learning language models,we use confidence-weighted learning (Dredze et al,2008), an extension of the perceptron-based on-line learning used in previous work on discrimi-native language models.
Furthermore, we extendconfidence-weighted learning with soft margin tohandle the case where training data labels are noisy,as is typically the case in discriminative languagemodeling.The rest of this paper is organized as follows.
InSection 2, we introduce factored features for dis-criminative language models.
Section 3 presentsconfidence-weighted learning.
Section 4 describesits extension for the case where training data arenoisy.
We present empirical results in Section 5and differentiate our approach from previous onesin Section 6.
Finally, Section 7 presents some con-cluding remarks.2 Factored featuresFactored features are n-gram features where eachcomponent in the n-gram can be characterized bydifferent linguistic dimensions of words such as sur-face, lemma, part of speech (POS).
Each of thesedimensions is conventionally referred to as a factor.An example of a factored feature is ?pick PRONup?, where PRON is the part of speech (POS) tagfor pronouns.
Appropriately weighted, this featurecan capture the fact that in English that pattern is of-ten fluent.
Compared to traditional surface n-gramfeatures like ?pick her up?, ?pick me up?
etc., thefeature ?pick PRON up?
generalizes the pattern bet-ter.
On the other hand, this feature is more precise439POS Extended POSNoun SingNoun, PlurNounPronoun Sing3PPronoun, OtherPronounVerb InfVerb, ProgrVerb, SimplePastVerb,PastPartVerb, Sing3PVerb, OtherVerbTable 1: Extended tagset used for the third factor in theproposed discriminative language model.than the corresponding POS n-gram feature ?VERBPRON PREP?
since the latter also promotes unde-sirable patterns such as ?pick PRON off?
and ?goPRON in?.
So, constructing features with compo-nents from different abstraction levels allows bettercapturing linguistic patterns.In this study, we use tri-gram factored features tolearn a discriminative language model for English,where each token is characterized by three factorsincluding surface, POS, and extended POS.
In thelast factor, some POS tags are further refined (Table1).
In other words, we will use all possible trigramswhere each element is either a surface from, a POS,or an extended POS.3 Confidence-weighted LearningOnline learning algorithms scale well to largedatasets, and are thus well adapted to discrimina-tive language modeling.
On the other hand, theperceptron and Passive Aggressive (PA) algorithms1(Crammer et al, 2006) can be ill-suited for learn-ing tasks where there is a long tail of rare significantfeatures as in the case of language modeling.Motivated by this, we adopt a simplified versionof the CW algorithm of (Dredze et al, 2008).
We in-troduce a score , based on the number of times a fea-ture has been obseerved in training, indicating howconfident the algorithm is in the current estimate wifor the weight of feature i.
Instead of equally chang-ing all feature weights upon a mistake, the algorithmnow changes more aggressively the weights it is lessconfident in.At iteration t, if the algorithm miss-ranks the pairof positive and negative instances (pt, nt), it updatesthe weight vector by solving the optimization in Eq.
(1):1The popular MIRA algorithm is a particular PA algorithm,suitable for the linearly-separable case.wt+1 = argminw12(w ?wt)>?2t (w ?wt)(1)s.t.
w>?t ?
1 (2)where ?t = ?
(pt) ?
?
(nt), ?
(x) is the vector rep-resentation of sentence x in factored feature space,and ?t is a diagonal matrix with confidence scores.The algorithm thus updates weights aggressivelyenough to correctly rank the current pair of instances(i.e.
satisfying the constraint), and preserves asmuch knowledge learned so far as possible (i.e.
min-imizing the weighted difference to wt).
In the spe-cial case when ?t = I this is the update of thePassive-Aggressive algorithm of (Crammer et al,2006).By introducing multiple confidence scores withthe diagonal matrix ?, we take into account thefact that feature weights that the algorithm has moreconfidence in (because it has learned these weightsfrom more training instances) contribute more tothe knowledge the algorithm has accumulated so farthan feature weights it has less confidence in.
Achange in the former is more risky than a changewith the same magnitude on the latter.
So, to avoidover-fitting to the current instance pair (thus gener-alize better to the others), the difference between wand wt is weighted by confidence matrix ?
in theobjective function.To solve the quadratic optimization problem inEq.
(1), we form the corresponding Lagrangian:L(w, ?)
= 12(w?wt)>?2t (w?wt)+?(1?w>?
)(3)where ?
is the Lagrange multiplier corresponding tothe constraint in Eq.
(2).
Setting the partial deriva-tives of L with respect to w to zero, and then settingthe derivative of L with respect to ?
to zero, we get:?
= 1?wt>????1?
?2 (4)Given this, we obtain Algorithm 1 for confidence-weighted passive-aggressive learning (Figure 1).
Inthe algorithm, Pi and Ni are sets of fluent and non-fluent sentences that can be contrasted, e.g.
Pi is aset of fluent translations and Ni is a set of non-fluenttranslations of a same source sentence si.440Algorithm 1 Confidence-weighted Passive-Aggressive algorithm for re-ranking.Input: Tr = {(Pi, Ni), 1 ?
i ?
K}w0 ?
0, t?
0for a predefined number of iterations dofor i from 1 to K dofor all (pj , nj) ?
(Pi ?Ni) do?t ?
?(pj)?
?
(nj)if w>t ?t < 1 then?
?
1?w>t ?t?>t ?
?2t ?twt+1 ?
wt + ??
?2t ?tUpdate ?t?
t + 1return wtThe confidence matrix ?
is updated following theintuition that the more often the algorithm has seena feature, the more confident the weight estimationbecomes.
In our work, we set ?ii to the logarithm ofthe number of times the algorithm has seen featurei, but alternative choices are possible.4 Extension to soft marginIn many practical situations, training data is noisy.This is particularly true for language modeling,where even human experts will argue about whethera given sentence is fluent or not.
Moreover, effectivelanguage models must be trained on large datasets,so the option of requiring extensive human annota-tion is impractical.
Instead, collecting fluency judg-ments is often done by a less expensive and thuseven less reliable manner.
One way is to rank trans-lations in n-best lists by NIST or BLEU scores, thentake the top ones as fluent instances and bottom onesas non-fluent instances.
Nonetheless, neither NISTnor BLEU are designed directly for measuring flu-ency.
For example, a translation could have lowNIST and BLEU scores just because it does not con-vey the same information as the reference, despitebeing perfectly fluent.
Therefore, in our setting it iscrucial to be robust to noise in the training labels.The update rule derived in the previous section al-ways forces the new weights to satisfy the constraint(Corrective updates): mislabeled training instancescould make feature weights change erratically.
Toincrease robustness to noise, we propose a soft mar-gin variant of confidence-weighted learning.
Theoptimization problem becomes:argminw12(w ?wt)>?2t (w ?wt) + C?2 (5)s.t.
w>?t ?
1?
?
(6)where C is a regularization parameter, controllingthe relative importance between the two terms in theobjective function.
Solving the optimization prob-lem, we obtain, for the Lagrange multiplier:?
= 1?wt>?t?>t ?
?2t ?t + 12C(7)Thus, the training algorithm with soft-margins is thesame as Algorithm 1, but using Eq.
7 to update ?instead.5 ExperimentsWe empirically validated our approach in two ways.We first measured the effectiveness of the algorithmsin deciding, given a pair of candidate translationsfor a same source sentence, whether the first candi-date is more fluent than the second.
In a second ex-periment we used the score provided by the trainedDLM as an additional feature in an n-best list re-ranking task and compared algorithms in terms ofimpact on NIST and BLEU.5.1 DatasetThe dataset we use in our study is the Spanish-English one from the shared task of the WMT-2007workshop2.Matrax, a phrase-based statistical machine trans-lation system (Simard et al, 2005), including a tri-gram generative language model with Kneser-Neysmoothing.
We then obtain training data for the dis-criminative language model as follows.
We take arandom subset of the parallel training set containing50,000 sentence pairs.
We use Matrax to generatean n-best list for each source sentence.
We define(Pi, Ni), i = 1 .
.
.
50, 000 as:Pi = {s ?
nbesti|NIST(s) ?
NIST?i ?
1} (8)Ni = {s ?
nbesti|NIST(s) ?
NIST?i ?
3} (9)2http://www.statmt.org/wmt07/441Error rateBaseline model 0.4720Baseline + DLM0 0.4290Baseline + DLM1 0.4183Baseline + DLM2 0.4005Baseline + DLM3 0.3803Table 2: Error rates for fluency ranking.
See article bodyfor an explanation of the experiments.where NIST?i is the highest sentence-level NISTscore achieved in nbesti.
The size of n-best listswas set to 10.
Using this dataset, we trained dis-criminative language models by standard percep-tron, confidence-weighted learning and confidence-weighted learning with soft margin.We then trained the weights of a re-ranker usingeight features (seven from the baseline Matrax plusone from the DLM) using a simple structured per-ceptron algorithm on the development set.For testing, we used the same trained Matraxmodel to generate n-best lists of size 1,000 each foreach source sentence.
Then, we used the trained dis-criminative language model to compute a score foreach translation in the n-best list.
The score is usedwith seven standard Matrax features for re-ranking.Finally, we measure the quality of the translationsre-ranked to the top.In order to obtain the required factors for thetarget-side tokens, we ran the morphological ana-lyzer and POS-tagger integrated in the Xerox Incre-mental Parser (XIP, Ait-Mokhtar et al (2001)) onthe target side of the training corpus used for creat-ing the phrase-table, and extended the phrase-tableformat so as to record, for each token, all its factors.5.2 ResultsIn the first experiment, we measure the quality ofthe re-ranked n-best lists by classification error rate.The error rate is computed as the fraction of pairsfrom a test-set which is ranked correctly accordingto its fluency score (approximated here by the NISTscore).
Results are in Table 2.For the baseline, we use the seven default Ma-trax features, including a generative language modelscore.
DLM* are discriminative language mod-els trained using, respectively, POS features onlyNIST BLEUBaseline model 6.9683 0.2704Baseline + DLM0 6.9804 0.2705Baseline + DLM1 6.9857 0.2709Baseline + DLM2 7.0288 0.2745Baseline + DLM3 7.0815 0.2770Table 3: NIST and BLEU scores upon n-best list re-ranking with the proposed discriminative language mod-els.
(DLM 0) or factored features by standard percep-tron (DLM 1), confidence-weighted learning (DLM2) and confidence-weighted learning with soft mar-gin (DLM 3).
All discriminative language modelsstrongly reduce the error rate compared to the base-line (9.1%, 11.4%, 15.1%, 19.4% relative reduc-tion, respectively).
Recall that the training set forthese discriminative language models is a relativelysmall subset of the one used to train Matrax?s inte-grated generative language model.
Amongst the fourdiscriminative learning algorithms, we see that fac-tored features are slightly better then POS features,confidence-weighted learning is slightly better thanperceptron, and confidence-weighted learning withsoft margin is the best (9.08% and 5.04% better thanperceptron and confidence-weighted learning withhard margin).In the second experiment, we use standard NISTand BLEU scores for evaluation.
Results are in Ta-ble 3.
The relative quality of different methods interms of NIST and BLEU correlates well with er-ror rate.
Again, all three discriminative languagemodels could improve performances over the base-line.
Amongst the three, confidence-weighted learn-ing with soft margin performs best.6 Related WorkThis work is related to several existing directions:generative factored language model, discriminativelanguage models, online passive-aggressive learningand confidence-weighted learning.Generative factored language models are pro-posed by (Bilmes and Kirchhoff, 2003).
In thiswork, factors are used to define alternative back-off paths in case surface-form n-grams are not ob-served a sufficient number of times in the train-442ing corpus.
Unlike ours, this model cannot con-sider simultaneously multiple factored features com-ing from the same token n-gram, thus integrating allpossible available information sources.Discriminative language models have also beenstudied in speech recognition and statistical machinetranslation (Roark et al, 2007; Li and Khudanpur,2008).
An attempt to combine factored features anddiscriminative language modeling is presented in(Mahe?
and Cancedda, 2009).
Unlike us, they com-bine together instances from multiple n-best lists,generally not comparable, in forming positive andnegative instances.
Also, they use an SVM to trainthe DLM, as opposed to the proposed online algo-rithms.Our approach stems from Passive-Aggressive al-gorithms proposed by (Crammer et al, 2006) andthe CW online algorithm proposed by (Dredze etal., 2008).
In the former, Crammer et al proposean online learning algorithm with soft margins tohandle noise in training data.
However, the workdoes not consider the confidence associated with es-timated feature weights.
On the other hand, the CWonline algorithm in the later does not consider thecase where the training data is noisy.While developed independently, our soft-marginextension is closely related to the AROW(project)algorithm of (Crammer et al, 2009; Crammer andLee, 2010).
The cited work models classifiers asnon-correlated Gaussian distributions over weights,while our approach uses point estimates for weightscoupled with confidence scores.
Despite the differ-ent conceptual modeling, though, in practice the al-gorithms are similar, with point estimates playingthe same role as the mean vector, and our (squared)confidence score matrix the same role as the preci-sion (inverse covariance) matrix.
Unlike in the citedwork, however, in our proposal, confidence scoresare updated also upon correct classification of train-ing examples, and not only on mistakes.
The ra-tionale of this is that correctly classifying an exam-ple could also increase the confidence on the currentmodel.
Thus, the update formulas are also differentcompared to the work cited above.7 ConclusionsWe proposed a novel approach to discriminative lan-guage models.
First, we introduced the idea of us-ing factored features in the discriminative languagemodeling framework.
Factored features allow thelanguage model to capture linguistic patterns at mul-tiple levels of abstraction.
Moreover, the discrimi-native framework is appropriate for handling highlyoverlapping features, which is the case of factoredfeatures.
While we did not experiment with this, anatural extension consists in using all n-grams upto a certain order, thus providing back-off featuresand enabling the use of higher-order n-grams.
Sec-ond, for learning factored language models discrim-inatively, we adopt a simple confidence-weightedalgorithm, limiting the problem of poor estimationof weights for rare features.
Finally, we extendedconfidence-weighted learning with soft margins tohandle the case where labels of training data arenoisy.
This is typically the case in discriminativelanguage modeling, where labels are obtained onlyindirectly.Our experiments show that combining all these el-ements is important and achieves significant transla-tion quality improvements already with a weak formof integration: n-best list re-ranking.ReferencesSalah Ait-Mokhtar, Jean-Pierre Chanod, and ClaudeRoux.
2001.
A multi-input dependency parser.
InProceedings of the Seventh International Workshop onParsing Technologies, Beijing, Cina.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Fac-tored language models and generalized parallel back-off.
In Proceedings of HLT/NAACL, Edmonton, Al-berta, Canada.Koby Crammer and Daniel D. Lee.
2010.
Learning viagaussian herding.
In Pre-proceeding of NIPS 2010.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal Of Machine LearningResearch, 7.Koby Crammer, Alex Kulesza, and Mark Dredze.
2009.Adaptive regularization of weight vectors.
In Ad-vances in Neural Processing Information Systems(NIPS 2009).Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classifiers.
In Pro-ceedings of ICML, Helsinki, Finland.443Zhifei Li and Sanjeev Khudanpur.
2008.
Large-scalediscriminative n-gram language models for statisticalmachine translation.
In Proceedings of AMTA.Pierre Mahe?
and Nicola Cancedda.
2009.
Linguisti-cally enriched word-sequence kernels for discrimina-tive language modeling.
In Learning Machine Trans-lation, NIPSWorkshop Series.
MIT Press, Cambridge,Mass.Brian Roark, Murat Saraclar, Michael Collins, and MarkJohnson.
2004.
Discriminative language modelingwith conditional random fields and the perceptron al-gorithm.
In Proceedings of the annual meeting ofthe Association for Computational Linguistics (ACL),Barcelona, Spain.Brian Roark, Murat Saraclar, and Michael Collins.
2007.Discriminative n-gram language modeling.
ComputerSpeech and Language, 21(2).M.
Simard, N. Cancedda, B. Cavestro, M. Dymetman,E.
Gaussier, C. Goutte, and K. Yamada.
2005.
Trans-lating with non-contiguous phrases.
In Associationfor Computational Linguistics, editor, Proceedings ofHuman Language Technology Conference and Con-ference on Empirical Methods in Natural Language,pages 755?762, October.444
