Modeling Local Coherence:An Entity-Based ApproachRegina Barzilay?Massachusetts Institute of TechnologyMirella Lapata?
?University of EdinburghThis article proposes a novel framework for representing and measuring local coherence.
Centralto this approach is the entity-grid representation of discourse, which captures patterns of entitydistribution in a text.
The algorithm introduced in the article automatically abstracts a textinto a set of entity transition sequences and records distributional, syntactic, and referentialinformation about discourse entities.
We re-conceptualize coherence assessment as a learningtask and show that our entity-based representation is well-suited for ranking-based generationand text classification tasks.
Using the proposed representation, we achieve good performance ontext ordering, summary coherence evaluation, and readability assessment.1.
IntroductionA key requirement for any system that produces text is the coherence of its output.Not surprisingly, a variety of coherence theories have been developed over the years(e.g., Mann and Thomson 1988; Grosz et al 1995) and their principles have foundapplication in many symbolic text generation systems (e.g., Scott and de Souza 1990;Kibble and Power 2004).
The ability of these systems to generate high quality text,almost indistinguishable from human writing, makes the incorporation of coherencetheories in robust large-scale systems particularly appealing.
The task is, however,challenging considering that most previous efforts have relied on handcrafted rules,valid only for limited domains, with no guarantee of scalability or portability (Reiterand Dale 2000).
Furthermore, coherence constraints are often embedded in complexrepresentations (e.g., Asher and Lascarides 2003) which are hard to implement in arobust application.This article focuses on local coherence, which captures text relatedness at the levelof sentence-to-sentence transitions.
Local coherence is undoubtedly necessary for globalcoherence and has received considerable attention in computational linguistics (Foltz,Kintsch, and Landauer 1998; Marcu 2000; Lapata 2003; Althaus, Karamanis, and Koller?
Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, 32 VassarStreet, 32-G468 Cambridge, MA 02139.
E-mail: regina@csail.mit.edu.??
School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK.
E-mail: mlap@inf.ed.ac.uk.Submission received: 29 November 2005; revised submission received: 6 March 2007; accepted forpublication: 5 May 2007.?
2008 Association for Computational LinguisticsComputational Linguistics Volume 34, Number 12004; Karamanis et al 2004).
It is also supported bymuch psycholinguistic evidence.
Forinstance, McKoon and Ratcliff (1992) argue that local coherence is the primary source ofinference-making during reading.The key premise of our work is that the distribution of entities in locally coher-ent texts exhibits certain regularities.
This assumption is not arbitrary?some of theseregularities have been recognized in Centering Theory (Grosz, Joshi, and Weinstein1995) and other entity-based theories of discourse (e.g., Givon 1987; Prince 1981).
Thealgorithm introduced in the article automatically abstracts a text into a set of entity tran-sition sequences, a representation that reflects distributional, syntactic, and referentialinformation about discourse entities.We argue that the proposed entity-based representation of discourse allows usto learn the properties of coherent texts from a corpus, without recourse to manualannotation or a predefined knowledge base.
We demonstrate the usefulness of this rep-resentation by testing its predictive power in three applications: text ordering, automaticevaluation of summary coherence, and readability assessment.We formulate the first two problems?text ordering and summary evaluation?asranking problems, and present an efficiently learnable model that ranks alternative ren-derings of the same information based on their degree of local coherence.
Such a mecha-nism is particularly appropriate for generation and summarization systems as they canproduce multiple text realizations of the same underlying content, either by varying pa-rameter values, or by relaxing constraints that control the generation process.
A systemequipped with a ranking mechanism could compare the quality of the candidateoutputs, in much the same way speech recognizers employ language models at thesentence level.In the text-ordering task our algorithm has to select a maximally coherent sen-tence order from a set of candidate permutations.
In the summary evaluation task,we compare the rankings produced by the model against human coherence judgmentselicited for automatically generated summaries.
In both experiments, our method yieldsimprovements over state-of-the-art models.
We also show the benefits of the entity-based representation in a readability assessment task, where the goal is to predict thecomprehension difficulty of a given text.
In contrast to existing systems which focus onintra-sentential features, we explore the contribution of discourse-level features to thistask.
By incorporating coherence features stemming from the proposed entity-basedrepresentation, we improve the performance of a state-of-the-art readability assessmentsystem (Schwarm and Ostendorf 2005).In the following section, we provide an overview of entity-based theories of lo-cal coherence and outline previous work on its computational treatment.
Then, weintroduce our entity-based representation, and define its linguistic properties.
In thesubsequent sections, we present our three evaluation tasks, and report the results of ourexperiments.
Discussion of the results concludes the article.2.
Related WorkOur approach is inspired by entity-based theories of local coherence, and is well-suitedfor developing a coherence metric in the context of a ranking-based text generationsystem.
We first summarize entity-based theories of discourse, and overview previousattempts for translating their underlying principles into computational coherence mod-els.
Next, we describe ranking approaches to natural language generation and focus oncoherence metrics used in current text planners.2Barzilay and Lapata Modeling Local Coherence2.1 Entity-Based Approaches to Local CoherenceLinguistic Modeling.
Entity-based accounts of local coherence have a long traditionwithin the linguistic and cognitive science literature (Kuno 1972; Chafe 1976; Hallidayand Hasan 1976; Karttunen 1976; Clark and Haviland 1977; Prince 1981; Grosz, Joshi,and Weinstein 1995).
A unifying assumption underlying different approaches is thatdiscourse coherence is achieved in view of the way discourse entities are introducedand discussed.
This observation is commonly formalized by devising constraints on thelinguistic realization and distribution of discourse entities in coherent texts.At any point in the discourse, some entities are considered more salient thanothers, and consequently are expected to exhibit different properties.
In CenteringTheory (Grosz, Joshi, and Weinstein 1995; Walker, Joshi, and Prince 1998; Strube andHahn 1999; Poesio et al 2004), salience concerns how entities are realized in an utterance(e.g., whether they are they pronominalized or not).
In other theories, salience is definedin terms of topicality (Chafe 1976; Prince 1978), predictability (Kuno 1972; Halliday andHasan 1976), and cognitive accessibility (Gundel, Hedberg, and Zacharski 1993).
Morerefined accounts expand the notion of salience from a binary distinction to a scalar one;examples include Prince?s (1981) familiarity scale, and Givon?s (1987) and Ariel?s (1988)givenness-continuum.The salience status of an entity is often reflected in its grammatical function andthe linguistic form of its subsequent mentions.
Salient entities are more likely to ap-pear in prominent syntactic positions (such as subject or object), and to be introducedin a main clause.
The linguistic realization of subsequent mentions?in particular,pronominalization?is so tightly linked to salience that in some theories (e.g., Givon1987) it provides the sole basis for defining a salience hierarchy.
The hypothesis is thatthe degree of underspecification in a referring expression indicates the topical status ofits antecedent (e.g., pronouns refer to very salient entities, whereas full NPs refer to lesssalient ones).
In Centering Theory, this phenomenon is captured in the Pronoun Rule,and Givon?s Scale of Topicality and Ariel?s Accessibility Marking Scale propose a gradedhierarchy of underspecification that ranges from zero anaphora to full noun phrases,and includes stressed and unstressed pronouns, demonstratives with modifiers, anddefinite descriptions.Entity-based theories capture coherence by characterizing the distribution of en-tities across discourse utterances, distinguishing between salient entities and the rest.The intuition here is that texts about the same discourse entity are perceived to bemore coherent than texts fraught with abrupt switches from one topic to the next.
Thepatterned distribution of discourse entities is a natural consequence of topic continuityobserved in a coherent text.
Centering Theory formalizes fluctuations in topic continuityin terms of transitions between adjacent utterances.
The transitions are ranked, thatis, texts demonstrating certain types of transitions are deemed more coherent than textswhere such transitions are absent or infrequent.
For example, CONTINUE transitionsrequire that two utterances have at least one entity in common and are preferredover transitions that repeatedly SHIFT from one entity to the other.
Givon?s (1987) andHoey?s (1991) accounts of discourse continuity complement local measurements byconsidering global characteristics of entity distribution, such as the lifetime of an entityin discourse and the referential distance between subsequent mentions.Computational Modeling.
An important practical question is how to translate principlesof these linguistic theories into a robust coherence metric.
A great deal of researchhas been devoted to this issue, primarily in Centering Theory (Miltsakaki and Kukich3Computational Linguistics Volume 34, Number 12000; Hasler 2004; Karamanis et al 2004).
Such translation is challenging in severalrespects: one has to determine ways of combining the effects of various constraints andto instantiate parameters of the theory that are often left underspecified.
Poesio et al(2004) note that even for fundamental concepts of Centering Theory such as ?utterance,??realization,?
and ?ranking,?
multiple?and often contradictory?interpretations havebeen developed over the years, because in the original theory these concepts are notexplicitly fleshed out.
For instance, in some Centering papers, entities are ranked withrespect to their grammatical function (Brennan, Friedman, and Pollard 1987; Walker,Iida, and Cote 1994; Grosz, Joshi, andWeinstein 1995), and in others with respect to theirposition in Prince?s (1981) givenness hierarchy (Strube and Hahn 1999) or their thematicrole (Sidner 1979).
As a result, two ?instantiations?
of the same theory make differentpredictions for the same input.
Poesio et al (2004) explore alternative specificationsproposed in the literature, and demonstrate that the predictive power of the theory ishighly sensitive to its parameter definitions.A common methodology for translating entity-based theories into computationalmodels is to evaluate alternative specifications on manually annotated corpora.
Somestudies aim to find an instantiation of parameters that is most consistent with observabledata (Strube and Hahn 1999; Karamanis et al 2004; Poesio et al 2004).
Other studiesadopt a specific instantiation with the goal of improving the performance of a metric ona task.
For instance, Miltsakaki and Kukich (2000) annotate a corpus of student essayswith entity transition information, and show that the distribution of transitions corre-lates with human grades.
Analogously, Hasler (2004) investigates whether CenteringTheory can be used in evaluating the readability of automatic summaries by annotatinghuman and machine generated extracts with entity transition information.The present work differs from these approaches in goal andmethodology.
Althoughour work builds upon existing linguistic theories, we do not aim to directly implementor refine any of them in particular.
We provide our model with sources of knowledgeidentified as essential by these theories, and leave it to the inference procedure todetermine the parameter values and an optimal way to combine them.
From a designviewpoint, we emphasize automatic computation for both the underlying discourserepresentation and the inference procedure.
Thus, our work is complementary to com-putational models developed onmanually annotated data (Miltsakaki and Kukich 2000;Hasler 2004; Poesio et al 2004).
Automatic, albeit noisy, feature extraction allows usto perform a large scale evaluation of differently instantiated coherence models acrossgenres and applications.2.2 Ranking Approaches in Natural Language GenerationRanking approaches have enjoyed an increasing popularity at all stages in thegeneration pipeline, ranging from text planning to surface realization (Knight andHatzivassiloglou 1995; Langkilde and Knight 1998; Mellish et al 1998; Walker, Rambow,and Rogati 2001; Karamanis 2003; Kibble and Power 2004).
In this framework, anunderlying system produces a potentially large set of candidate outputs, with respectto various text generation rules encoded as hard constraints.
Not all of the resultingalternatives will correspond to well-formed texts, and of those which may be judged ac-ceptable, some will be preferable to others.
The candidate generation phase is followedby an assessment phase in which the candidates are ranked based on a set of desirableproperties encoded in a ranking function.
The top-ranked candidate is selected forpresentation.
A two-stage generate-and-rank architecture circumvents the complexity4Barzilay and Lapata Modeling Local Coherenceof traditional generation systems, where numerous, often conflicting constraints, haveto be encoded during development in order to produce a single high-quality output.Because the focus of our work is on text coherence, we discuss here rank-ing approaches applied to text planning (see Walker et al [2001] and Knight andHatzivassiloglou [1995] for ranking approaches to sentence planning and surface re-alization, respectively).
The goal of text planning is to determine the content of a textby selecting a set of information-bearing units and arranging them into a structure thatyields well-formed output.
Depending on the system, text plans are represented as dis-course trees (Mellish et al 1998) or linear sequences of propositions (Karamanis 2003).Candidate text structures may differ in terms of the selected propositions, the sequencein which facts are presented, the topology of the tree, or the order in which entities areintroduced.
A set of plausible candidates can be created via stochastic search (Mellishet al 1998) or by a symbolic text planner following different text-formation rules (Kibbleand Power 2004).
The best candidate is chosen using an evaluation or ranking functionoften encoding coherence constraints.
Although the type and complexity of constraintsvary greatly across systems, they are commonly inspired by Rhetorical Structure Theoryor entity-based constraints similar to the ones captured by our method.
For instance,the ranking function used by Mellish et al gives preference to plans where consecutivefacts mention the same entities and is sensitive to the syntactic environment in whichthe entity is first introduced (e.g., in a subject or object position).
Karamanis findsthat a ranking function based solely on the principle of continuity achieves competi-tive performance against more sophisticated alternatives when applied to orderingshort descriptions of museum artifacts.1 In other applications, the ranking function ismore complex, integrating rules from Centering Theory along with stylistic constraints(Kibble and Power 2004).A common feature of current implementations is that the specification of the rank-ing function?feature selection and weighting?is performed manually based on theintuition of the system developer.
However, even in a limited domain this task hasproven difficult.
Mellish et al (1998; page 100) note: ?The problem is far too complexand our knowledge of the issues involved so meager that only a token gesture can bemade at this point.?
Moreover, these ranking functions operate over semantically richinput representations that cannot be created automatically without extensive knowl-edge engineering.
The need for manual coding impairs the portability of existing meth-ods for coherence ranking to new applications, most notably to text-to-text generationapplications, such as summarization.In the next section, we present a method for coherence assessment that overcomesthese limitations: We introduce an entity-based representation of discourse that is auto-matically computed from raw text; we argue that the proposed representation revealsentity transition patterns characteristic of coherent texts.
The latter can be easily trans-lated into a large feature space which lends itself naturally to the effective learning of aranking function, without explicit manual involvement.3.
The Coherence ModelIn this section we describe our entity-based representation of discourse.
We explain howit is computed and how entity transition patterns are extracted.
We also discuss how1 Each utterance in the discourse refers to at least one entity in the utterance that precedes it.5Computational Linguistics Volume 34, Number 1these patterns can be encoded as feature vectors appropriate for performing coherence-related ranking and classification tasks.3.1 The Entity-Grid Discourse RepresentationEach text is represented by an entity grid, a two-dimensional array that capturesthe distribution of discourse entities across text sentences.
We follow Miltsakaki andKukich (2000) in assuming that our unit of analysis is the traditional sentence (i.e., amain clause with accompanying subordinate and adjunct clauses).
The rows of thegrid correspond to sentences, and the columns correspond to discourse entities.
Bydiscourse entity we mean a class of coreferent noun phrases (we explain in Section 3.3how coreferent entities are identified).
For each occurrence of a discourse entity in thetext, the corresponding grid cell contains information about its presence or absencein a sequence of sentences.
In addition, for entities present in a given sentence, gridcells contain information about their syntactic role.
Such information can be expressedin many ways (e.g., using constituent labels or thematic role information).
Becausegrammatical relations figure prominently in entity-based theories of local coherence (seeSection 2), they serve as a logical point of departure.
Each grid cell thus corresponds toa string from a set of categories reflecting whether the entity in question is a subject (S),object (O), or neither (X).
Entities absent from a sentence are signaled by gaps (?
).Grammatical role information can be extracted from the output of a broad-coveragedependency parser (Lin 2001; Briscoe and Carroll 2002) or any state-of-the art statisticalparser (Collins 1997; Charniak 2000).
We discuss how this information was computedfor our experiments in Section 3.3.Table 1 illustrates a fragment of an entity grid constructed for the text in Table 2.Because the text contains six sentences, the grid columns are of length six.
Considerfor instance the grid column for the entity trial, [O ?
?
?
?
X].
It records that trial ispresent in sentences 1 and 6 (as O and X, respectively) but is absent from the rest of thesentences.
Also note that the grid in Table 1 takes coreference resolution into account.Even though the same entity appears in different linguistic forms, for example,MicrosoftCorp., Microsoft, and the company, it is mapped to a single entry in the grid (see thecolumn introduced byMicrosoft in Table 1).Table 1A fragment of the entity grid.
Noun phrases are represented by their head nouns.
Grid cellscorrespond to grammatical roles: subjects (S), objects (O), or neither (X).DepartmentTrialMicrosoftEvidenceCompetitorsMarketsProductsBrandsCaseNetscapeSoftwareTacticsGovernmentSuitEarnings1 S O S X O ?
?
?
?
?
?
?
?
?
?
12 ?
?
O ?
?
X S O ?
?
?
?
?
?
?
23 ?
?
S O ?
?
?
?
S O O ?
?
?
?
34 ?
?
S ?
?
?
?
?
?
?
?
S ?
?
?
45 ?
?
?
?
?
?
?
?
?
?
?
?
S O ?
56 ?
X S ?
?
?
?
?
?
?
?
?
?
?
O 66Barzilay and Lapata Modeling Local CoherenceTable 2Summary augmented with syntactic annotations for grid computation.1 [The Justice Department]S is conducting an [anti-trust trial]O against [Microsoft Corp.]Xwith [evidence]X that [the company]S is increasingly attempting to crush [competitors]O.2 [Microsoft]O is accused of trying to forcefully buy into [markets]X where [its ownproducts]S are not competitive enough to unseat [established brands]O.3 [The case]S revolves around [evidence]O of [Microsoft]S aggressively pressuring[Netscape]O into merging [browser software]O.4 [Microsoft]S claims [its tactics]S are commonplace and good economically.5 [The government]S may file [a civil suit]O ruling that [conspiracy]S to curb [competition]Othrough [collusion]X is [a violation of the Sherman Act]O.6 [Microsoft]S continues to show [increased earnings]O despite [the trial]X.When a noun is attested more than once with a different grammatical role in thesame sentence, we default to the role with the highest grammatical ranking: subjects areranked higher than objects, which in turn are ranked higher than the rest.
For example,the entity Microsoft is mentioned twice in Sentence 1 with the grammatical roles x (forMicrosoft Corp.) and s (for the company), but is represented only by s in the grid (seeTables 1 and 2).3.2 Entity Grids as Feature VectorsA fundamental assumption underlying our approach is that the distribution of entitiesin coherent texts exhibits certain regularities reflected in grid topology.
Some of theseregularities are formalized in Centering Theory as constraints on transitions of thelocal focus in adjacent sentences.
Grids of coherent texts are likely to have some densecolumns (i.e., columns with just a few gaps, such as Microsoft in Table 1) and manysparse columns which will consist mostly of gaps (see markets and earnings in Table 1).One would further expect that entities corresponding to dense columns are more oftensubjects or objects.
These characteristics will be less pronounced in low-coherence texts.Inspired by Centering Theory, our analysis revolves around patterns of local entitytransitions.
A local entity transition is a sequence {S,O, X, ?
}n that represents entityoccurrences and their syntactic roles in n adjacent sentences.
Local transitions can beeasily obtained from a grid as continuous subsequences of each column.
Each transitionwill have a certain probability in a given grid.
For instance, the probability of thetransition [S ?]
in the grid from Table 1 is 0.08 (computed as a ratio of its frequency[i.e., six] divided by the total number of transitions of length two [i.e., 75]).
Each textcan thus be viewed as a distribution defined over transition types.We can now go one step further and represent each text by a fixed set of transitionsequences using a standard feature vector notation.
Each grid rendering j of a documentdi corresponds to a feature vector ?
(xij) = (p1(xij), p2(xij), .
.
.
, pm(xij)), where m is thenumber of all predefined entity transitions, and pt(xij) the probability of transition tin grid xij.
This feature vector representation is usefully amenable to machine learningalgorithms (see our experiments in Sections 4?6).
Furthermore, it allows the consid-eration of large numbers of transitions which could potentially uncover novel entitydistribution patterns relevant for coherence assessment or other coherence-related tasks.Note that considerable latitude is available when specifying the transition types tobe included in a feature vector.
These can be all transitions of a given length (e.g., twoor three) or the most frequent transitions within a document collection.
An example of7Computational Linguistics Volume 34, Number 1a feature space with transitions of length two is illustrated in Table 3.
The second row(introduced by d1) is the feature vector representation of the grid in Table 1.3.3 Grid Construction: Linguistic DimensionsOne of the central research issues in developing entity-based models of coherence isdetermining what sources of linguistic knowledge are essential for accurate prediction,and how to encode them succinctly in a discourse representation.
Previous approachestend to agree on the features of entity distribution related to local coherence?thedisagreement lies in the way these features are modeled.Our study of alternative encodings is not a mere duplication of previous ef-forts (Poesio et al 2004) that focus on linguistic aspects of parameterization.
Because weare interested in an automatically constructed model, we have to take into account com-putational and learning issues when considering alternative representations.
Therefore,our exploration of the parameter space is guided by three considerations: the linguisticimportance of a parameter, the accuracy of its automatic computation, and the size of theresulting feature space.
From the linguistic side, we focus on properties of entity distri-bution that are tightly linked to local coherence, and at the same time allow for multipleinterpretations during the encoding process.
Computational considerations prevent usfrom considering discourse representations that cannot be computed reliably by exist-ing tools.
For instance, we could not experiment with the granularity of an utterance?sentence versus clause?because available clause separators introduce substantial noiseinto a grid construction.
Finally, we exclude representations that will explode the size ofthe feature space, thereby increasing the amount of data required for training themodel.Entity Extraction.
The accurate computation of entity classes is key to computing mean-ingful entity grids.
In previous implementations of entity-basedmodels, classes of coref-erent nouns have been extracted manually (Miltsakaki and Kukich 2000; Karamaniset al 2004; Poesio et al 2004), but this is not an option for our model.
An obvioussolution for identifying entity classes is to employ an automatic coreference resolutiontool that determines which noun phrases refer to the same entity in a document.Current approaches recast coreference resolution as a classification task.
A pairof NPs is classified as coreferring or not based on constraints that are learned froman annotated corpus.
A separate clustering mechanism then coordinates the possiblycontradictory pairwise classifications and constructs a partition on the set of NPs.
Inour experiments, we employ Ng and Cardie?s (2002) coreference resolution system.The system decides whether two NPs are coreferent by exploiting a wealth of lexical,grammatical, semantic, and positional features.
It is trained on the MUC (6?7) data setsand yields state-of-the-art performance (70.4 F-measure onMUC-6 and 63.4 onMUC-7).Table 3Example of a feature-vector document representation using all transitions of length two givensyntactic categories S, O, X, and ?.S S S O S X S ?
O S O O O X O ?
X S X O X X X ?
?
S ?
O ?
X ?
?d1 .01 .01 0 .08 .01 0 0 .09 0 0 0 .03 .05 .07 .03 .59d2 .02 .01 .01 .02 0 .07 0 .02 .14 .14 .06 .04 .03 .07 0.1 .36d3 .02 0 0 .03 .09 0 .09 .06 0 0 0 .05 .03 .07 .17 .398Barzilay and Lapata Modeling Local CoherenceAlthough machine learning approaches to coreference resolution have been rea-sonably successful?state-of-the-art coreference tools today reach an F-measure2 of70% when trained on newspaper texts?it is unrealistic to assume that such tools willbe readily available for different domains and languages.
We therefore consider anadditional approach to entity extraction where entity classes are constructed simply byclustering nouns on the basis of their identity.
In other words, each noun in a text cor-responds to a different entity in a grid, and two nouns are considered coreferent only ifthey are identical.
Under this viewMicrosoft Corp. from Table 2 (Sentence 1) correspondsto two entities, Microsoft and Corp., which are in turn distinct from the company.
Thisapproach is only a rough approximation to fully fledged coreference resolution, but itis simple from an implementational perspective and produces consistent results acrossdomains and languages.Grammatical Function.
Several entity-based approaches assert that grammatical functionis indicative of an entity?s prominence in discourse (Hudson, Tanenhaus, and Dell 1986;Kameyama 1986; Brennan, Friedman, and Pollard 1987; Grosz, Joshi, and Weinstein1995).
Most theories discriminate between subject, object, and the remaining grammati-cal roles: subjects are ranked higher than objects, and these are ranked higher than othergrammatical functions.In our framework, we can easily assess the impact of syntactic knowledge bymodifying how transitions are represented in the entity grid.
In syntactically awaregrids, transitions are expressed by four categories: s, o, x and ?, whereas in simplifiedgrids, we only record whether an entity is present (x) or absent (?)
in a sentence.We employ a robust statistical parser (Collins 1997) to determine the constituentstructure for each sentence, fromwhich subjects (s), objects (o), and relations other thansubject or object (x) are identified.
The phrase-structure output of Collins?s parser istransformed into a dependency tree from which grammatical relations are extracted.Passive verbs are recognized using a small set of patterns, and the underlying deepgrammatical role for arguments involved in the passive construction is entered in thegrid (see the grid cell o forMicrosoft, Sentence 2, Table 2).
For more details on the gram-matical relations extraction component we refer the interested reader to Barzilay (2003).Salience.
Centering and other discourse theories conjecture that the way an entity isintroduced and mentioned depends on its global role in a given discourse.
We evaluatethe impact of salience information by considering two types of models: The first modeltreats all entities uniformly, whereas the second one discriminates between transitionsof salient entities and the rest.
We identify salient entities based on their frequency,3 fol-lowing the widely accepted view that frequency of occurrence correlates with discourseprominence (Givon 1987; Ariel 1988; Hoey 1991; Morris and Hirst 1991).To implement a salience-based model, we modify our feature generation proce-dure by computing transition probabilities for each salience group separately, and then2 When evaluating the output of coreference algorithms, performance is typically measured using amodel-theoretic scoring scheme proposed in Vilain et al (1995).
The scoring algorithm computes therecall error by taking each equivalence class S in the gold standard and determining the number ofcoreference links m that would have to be added to the system?s output to place all entities in S intothe same equivalence class produced by the system.
Recall error then is the sum of ms divided by thenumber of links in the gold standard.
Precision error is computed by reversing the roles of the goldstandard and system output.3 The frequency threshold is empirically determined on the development set.
See Section 4.2 for furtherdiscussion.9Computational Linguistics Volume 34, Number 1combining them into a single feature vector.
For n transitions with k salience classes,the feature space will be of size n?
k. While we can easily build a model with multiplesalience classes, we opt for a binary distinction (i.e., k = 2).
This is more in line withtheoretical accounts of salience (Chafe 1976; Grosz, Joshi, and Weinstein 1995) andresults in a moderate feature space for which reliable parameter estimation is possible.Considering a large number of salience classes would unavoidably increase the numberof features.
Parameter estimation in such a space requires a large sample of trainingexamples that is unavailable for most domains and applications.Different classes of models can be defined along the linguistic dimensions just dis-cussed.
Our experiments will consider several models with varying degrees of linguisticcomplexity, while attempting to strike a balance between expressivity of representationand ease of computation.
In the following sections we evaluate their performance onthree tasks: sentence ordering, summary coherence rating, and readability assessment.3.4 LearningEquipped with the feature vector representation introduced herein, we can view co-herence assessment as a machine learning problem.
When considering text generationapplications, it is desirable to rank rather than classify instances: There is often no singlecoherent rendering of a given text but many different possibilities that can be partiallyordered.
It is therefore not surprising that systems often employ scoring functions toselect the most coherent output among alternative renderings (see the discussion inSection 2.2).
In this article we argue that encoding texts as entity transition sequencesconstitutes an appropriate feature set for learning (rather than manually specifying)such a ranking function (see Section 4 for details).
We present two task-based exper-iments that put this hypothesis to the test: information ordering (Experiment 1) andsummary coherence rating (Experiment 2).
Both tasks can be naturally formulated asranking problems; the learner takes as input a set of alternative renderings of thesame document and ranks them based on their degree of local coherence.
Examplesof such renderings are a set of different sentence orderings of the same text and a setof summaries produced by different systems for the same document.
Note that in bothranking experiments we assume that the algorithm is provided with a limited numberof alternatives.
In practice, the space of candidates can be vast, and finding the optimalcandidate may require pairing our ranking algorithmwith a decoder similar to the onesused in machine translation (Germann et al 2004).Although the majority of our experiments fall within the generate-and-rank frame-work previously sketched, nothing prevents the use of our feature vector representationfor conventional classification tasks.
We offer an illustration in Experiment 3, wherefeatures extracted from entity grids are used to enhance the performance of a readabilityassessment system.
Here, the learner takes as input a set of documents labeled withdiscrete classes (e.g., denoting whether a text is difficult or easy to read) and learns tomake predictions for unseen instances (see Section 6 for details on the machine learningparadigm we employ).4.
Experiment 1: Sentence OrderingText structuring algorithms (Lapata 2003; Barzilay and Lee 2004; Karamanis et al 2004)are commonly evaluated by their performance at information-ordering.
The task con-cerns determining a sequence in which to present a pre-selected set of information-10Barzilay and Lapata Modeling Local Coherencebearing items; this is an essential step in concept-to-text generation, multi-documentsummarization, and other text-synthesis problems.
The information bearing items canbe database entries (Karamanis et al 2004), propositions (Mellish et al 1998) or sen-tences (Lapata 2003; Barzilay and Lee 2004).
In sentence ordering, a document is viewedas a bag of sentences and the algorithm?s task is to try to find the ordering whichmaximizes coherence according to some criterion (e.g., the probability of an order).As explained previously, we use our coherence model to rank alternative sentenceorderings instead of trying to find an optimal ordering.
We do not assume that localcoherence is sufficient to uniquely determine a maximally coherent ordering?otherconstraints clearly play a role here.
It is nevertheless a key property of well-formedtext (documents lacking local coherence are naturally globally incoherent), and a modelwhich takes it into account should be able to discriminate coherent from incoher-ent texts.
In our sentence-ordering task we generate random permutations of a testdocument and measure how often a permutation is ranked higher than the originaldocument.
A non-deficient model should prefer the original text more frequently thanits permutations (see Section 4.2 for details).We begin by explaining how a ranking function can be learned for the sentenceordering task.
Next, we give details regarding the corpus used for our experiments,describe the methods used for comparison with our approach, and note the evaluationmetric employed for assessing model performance.
Our results are presented in Sec-tion 4.3.4.1 ModelingOur training set consists of ordered pairs of alternative renderings (xij, xik) of the samedocument di, where xij exhibits a higher degree of coherence than xik (we describe inSection 4.2 how such training instances are obtained).
Without loss of generality, weassume j > k. The goal of the training procedure is to find a parameter vector w thatyields a ?ranking score?
function which minimizes the number of violations of pairwiserankings provided in the training set?
(xij, xik) ?
r?
: w ?
?
(xij) > w ?
?
(xik)where (xij, xik) ?
r?
if xij is ranked higher than xik for the optimal ranking r?
(in thetraining data), and ?
(xij) and ?
(xik) are a mapping onto features representing thecoherence properties of renderings xij and xik.
In our case the features correspond tothe entity transition probabilities introduced in Section 3.2.
Thus, the ideal rankingfunction, represented by the weight vector wwould satisfy the conditionw ?
(?(xij)?
?
(xik)) > 0 ?j, i, k such that j > kThe problem is typically treated as a Support Vector Machine constraint optimizationproblem, and can be solved using the search technique described in Joachims (2002).This approach has been shown to be highly effective in various tasks ranging fromcollaborative filtering (Joachims 2002) to parsing (Toutanova, Markova, and Manning2004).
Other discriminative formulations of the ranking problem are possible (Collins2002; Freund et al 2003); however, we leave this to future work.11Computational Linguistics Volume 34, Number 1Table 4The size of the training and test instances for the Earthquakes and Accidents corpora (measuredby the number of pairs that contain the original order and a random permutation of this order).Training TestingEarthquakes 1,896 2,056Accidents 2,095 2,087Once the ranking function is learned, unseen renderings (xij, xik) of document dican be ranked simply by computing the valuesw??
(xij) andw??
(xik) and sorting themaccordingly.
Here,w?
is the optimized parameter vector resulting from training.4.2 MethodData.
To acquire a large collection for training and testing, we create synthetic data,wherein the candidate set consists of a source document and permutations of its sen-tences.
This framework for data acquisition enables large-scale automatic evaluationand is widely used in assessing ordering algorithms (Karamanis 2003; Lapata 2003;Althaus, Karamanis, and Koller 2004; Barzilay and Lee 2004).
The underlying assump-tion is that the original sentence order in the source document must be coherent, andso we should prefer models that rank it higher than other permutations.
Because wedo not know the relative quality of different permutations, our corpus includes onlypairwise rankings that comprise the original document and one of its permutations.Given k original documents, each with n randomly generated permutations, we obtaink ?
n (trivially) annotated pairwise rankings for training and testing.Using the technique described herein, we collected data4 in two different genres:newspaper articles and accident reports written by government officials.
The first col-lection consists of Associated Press articles from the North American News Corpuson the topic of earthquakes (Earthquakes).
The second includes narratives from theNational Transportation Safety Board?s aviation accident database (Accidents).
Bothcorpora have documents of comparable length?the average number of sentences is 10.4and 11.5, respectively.
For each set, we used 100 source articles with up to 20 randomlygenerated permutations for training.5 A similar methodwas used to obtain the test data.Table 4 shows the size of the training and test corpora used in our experiments.
We heldout 10 documents (i.e., 200 pairwise rankings) from the training data for developmentpurposes.Features and Parameter Settings.
In order to investigate the contribution of linguisticknowledge on model performance we experimented with a variety of grid representa-tions resulting in different parameterizations of the feature space fromwhich our modelis learned.
We focused on three sources of linguistic knowledge?syntax, coreferenceresolution, and salience?which play a prominent role in entity-based analyses of dis-4 The collections are available from http://people.csail.mit.edu/regina/coherence/.5 Short texts may have less than 20 permutations.
The corpus described in the original ACL publication(Barzilay and Lapata 2005) contained a number of duplicate permutations.
These were removed fromthe current version of the corpus.12Barzilay and Lapata Modeling Local Coherencecourse coherence (see Section 3.3 for details).
An additional motivation for our studywas to explore the trade-off between robustness and richness of linguistic annotations.NLP tools are typically trained on human-authored texts, and may deteriorate in per-formance when applied to automatically generated texts with coherence violations.We thus compared a linguistically rich model against models that use more im-poverished representations.
More concretely, our full model (Coreference+Syntax+Salience+) uses coreference resolution, denotes entity transition sequences via gram-matical roles, and differentiates between salient and non-salient entities.
Our less-expressive models (seven in total) use only a subset of these linguistic featuresduring the grid construction process.
We evaluated the effect of syntactic knowl-edge by eliminating the identification of grammatical relations and recording solelywhether an entity is present or absent in a sentence.
This process created a classof four models of the form Coreference[+/?]Syntax?Salience[+/?].
The effect offully fledged coreference resolution was assessed by creating models where entityclasses were constructed simply by clustering nouns on the basis of their identity(Coreference?Syntax[+/?]Salience[+/?]).
Finally, the contribution of salience wasmeasured by comparing the full model which accounts separately for patterns of salientand non-salient entities against models that do not attempt to discriminate betweenthem (Coreference[+/?]Syntax[+/?]Salience?
).We would like to note that in this experiment we apply a coreference resolution toolto the original text and then generate permutations for the pairwise ranking task.
Analternative design is to apply coreference resolution to permuted texts.
Because existingmethods for coreference resolution take into consideration the order of noun phrases ina text, the accuracy of these tools on permuted sentence sequences is close to random.Therefore, we opt to resolve coreference within the original text.
Although this designhas an oracle feel to it, it is not uncommon in practical applications.
For instance, in textgeneration systems, content planners often operate over fully specified semantic rep-resentations, and can thus take advantage of coreference information during sentenceordering.Besides variations in the underlying linguistic representation, our model is alsospecified by two free parameters: the frequency threshold used to identify salient en-tities and the length of the transition sequence.
These parameters were tuned separatelyfor each data set on the corresponding held-out development set.
Optimal salience-based models were obtained for entities with frequency ?2.
The optimal transitionlength was ?3.6In our ordering experiments, we used Joachims?s (2002) SVMlight package for train-ing and testing with all parameters set to their default values.Comparison with State-of-the-Art Methods.
We compared the performance of our algo-rithm against two state-of-the-art models proposed by Foltz, Kintsch, and Landauer(1998) and Barzilay and Lee (2004).
These models rely largely on lexical informationfor assessing document coherence, contrary to our models which are in essence un-lexicalized.
Recall from Section 3 that our approach captures local coherence by mod-eling patterns of entity distribution in discourse, without taking note of their lexicalinstantiations.
In the following we briefly describe the lexicalized models we employedin our comparative study and motivate their selection.6 The models we used in our experiments are available from http://people.csail.mit.edu/regina/coherence/ and http://homepages.inf.ed.ac.uk/mlap/coherence/.13Computational Linguistics Volume 34, Number 1Foltz, Kintsch, and Landauer (1998) model measures coherence as a function ofsemantic relatedness between adjacent sentences.
The underlying intuition here is thatcoherent texts will contain a high number of semantically related words.
Semanticrelatedness is computed automatically using Latent Semantic Analysis (LSA; Landauerand Dumais 1997) from raw text without employing syntactic or other annotations.
Inthis framework, a word?s meaning is captured in a multi-dimensional space by a vectorrepresenting its co-occurrence with neighboring words.
Co-occurrence information iscollected in a frequencymatrix, where each row corresponds to a uniqueword, and eachcolumn represents a given linguistic context (e.g., sentence, document, or paragraph).Foltz, Kintsch, and Landauer?s model use singular value decomposition (SVD; Berry,Dumais, and O?Brien 1994) to reduce the dimensionality of the space.
The transforma-tion renders sparse matrices more informative and can be thought of as a means ofuncovering latent structure in distributional data.
The meaning of a sentence is nextrepresented as a vector by taking the mean of the vectors of its words.
The similaritybetween two sentences is determined by measuring the cosine of their means:sim(S1,S2) = cos(?
( S1),?
( S2))=n?j=1?j( S1)?j( S2)?n?j=1(?j( S1))2?n?j=1(?j( S2))2(1)where ?
(Si) =1|Si|?u?Si u, and u is the vector for word u.
An overall text coherencemeasure can be easily obtained by averaging the cosines for all pairs of adjacent sen-tences Si and Si+1:coherence(T) =n?1?i=1cos(Si,Si+1)n?
1 (2)This model is a good point of comparison for several reasons: (a) it is fully automaticand has relatively few parameters (i.e., the dimensionality of the space and the choice ofsimilarity function), (b) it correlates reliably with human judgments and has been usedto analyze discourse structure, and (c) it models an aspect of local coherence which isorthogonal to ours.
The LSAmodel is lexicalized: coherence amounts to quantifying thedegree of semantic similarity between sentences.
In contrast, our model does not incor-porate any notion of similarity: coherence is encoded in terms of transition sequencesthat are document-specific rather than sentence-specific.Our implementation of the LSA model followed closely Foltz, Kintsch, andLandauer (1998).
We constructed vector-based representations for individual wordsfrom a lemmatized version of the North American News Corpus7 (350 million words)using a term?document matrix.
We used SVD to reduce the semantic space to 100dimensions obtaining thus a space similar to LSA.
We estimated the coherence of a doc-ument using Equations (1) and (2).
A ranking can be trivially inferred by comparing the7 Our selection of this corpus was motivated by two factors: (a) the corpus is large enough to yield areliable semantic space, and (b) it consists of news stories and is therefore similar in style, vocabulary,and content to most of the corpora employed in our coherence experiments.14Barzilay and Lapata Modeling Local Coherencecoherence score assigned to the original document against each of its permutations.
Tiesare resolved randomly.Both LSA and our entity-grid model are local?they model sentence-to-sentencetransitions without being aware of global document structure.
In contrast, the contentmodels developed by Barzilay and Lee (2004) learn to represent more global text prop-erties by capturing topics and the order in which these topics appear in texts from thesame domain.
For instance, a typical earthquake newspaper report contains informationabout the quake?s epicenter, how much it measured, the time it was felt, and whetherthere were any victims or damage.
By encoding constraints on the ordering of thesetopics, content models have a pronounced advantage in modeling document structurebecause they can learn to represent how documents begin and end, but also how thediscourse shifts from one topic to the next.
Like LSA, the content models are lexicalized;however, unlike LSA, they are domain-specific, and would expectedly yield inferiorperformance on out-of-domain texts.Barzilay and Lee (2004) implemented content models using anHMMwherein statescorrespond to distinct topics (for instance, the epicenter of an earthquake or the numberof victims), and state transitions represent the probability of changing from one topicto another, thereby capturing possible topic-presentation orderings within a domain.Topics refer to text spans of varying granularity and length.
Barzilay and Lee usedsentences in their experiments, but clauses or paragraphs would also be possible.Barzilay and Lee (2004) employed their content models to find a high-probabilityordering for a document whose sentences had been randomly shuffled.
Here, we usecontent models for the simpler coherence ranking task.
Given two text permutations,we estimate their likelihood according to their HMMmodel and select the text with thehighest probability.
Because the two candidates contain the same set of sentences, theassumption is that a more probable text corresponds to an ordering that is more typicalfor the domain of interest.In our experiments, we built two content models, one for the Accidents corpus andone for the Earthquake corpus.
Although these models are trained in an unsupervisedfashion, a number of parameters related to the model topology (i.e., number of statesand smoothing parameters) affect their performance.
These parameters were tuned onthe development set and chosen so as to optimize the models?
performance on thepairwise ranking task.Evaluation Metric.
Given a set of pairwise rankings (an original document and one ofits permutations), we measure accuracy as the ratio of correct predictions made by themodel over the size of the test set.
In this setup, random prediction results in an accuracyof 50%.4.3 ResultsImpact of Linguistic Representation.
We first investigate how different types of linguisticknowledge influence our model?s performance.
Table 5 shows the accuracy on the or-dering task when the model is trained on different grid representations.
As can be seen,in both domains, the full model Coreference+Syntax+Salience+ significantly outper-forms a linguistically naive model which simply records the presence (and absence)of entities in discourse (Coreference?Syntax?Salience?).
Moreover, we observe thatlinguistically impoverished models consistently perform worse than their linguisti-cally elaborate counterparts.
We assess whether differences in accuracy are statistically15Computational Linguistics Volume 34, Number 1Table 5Accuracy measured as a fraction of correct pairwise rankings in the test set.
Coreference[+/?
]indicates whether coreference information has been used in the construction of the entity grid.Similarly, Syntax[+/?]
and Salience[+/?]
reflect the use of syntactic and salience information.Diacritics ** (p < .01) and * (p < .05) indicate whether differences in accuracy between the fullmodel (Coreference+Syntax+Salience+) and all other models are significant (using a FisherSign test).Model Earthquakes AccidentsCoreference+Syntax+Salience+ 87.2 90.4Coreference+Syntax+Salience?
88.3 90.1Coreference+Syntax?Salience+ 86.6 88.4?
?Coreference?Syntax+Salience+ 83.0??
89.9Coreference+Syntax?Salience?
86.1 89.2Coreference?Syntax+Salience?
82.3??
88.6?Coreference?Syntax?Salience+ 83.0??
86.5??Coreference?Syntax?Salience?
81.4??
86.0?
?HMM-based Content Models 88.0 75.8?
?Latent Semantic Analysis 81.0??
87.3?
?significant using a Fisher Sign Test.
Specifically, we compare the full model against eachof the less expressive models (see Table 5).Let us first discuss in more detail how the contribution of different knowl-edge sources varies across domains.
On the Earthquakes corpus every model thatdoes not use coreference information (Coreference?Syntax[+/?]Salience[+/?])
per-forms significantly worse than models augmented with coreference (Coreference+Syntax[+/?]Salience[+/?]).
This effect is less pronounced on the Accidents corpus,especially for model Coreference?Syntax+Salience+ whose accuracy drops onlyby 0.5% (the difference between Coreference?Syntax+Salience+ and Coreference+Syntax+Salience+ is not statistically significant).
The same model?s performance de-creases by 4.2% on the Earthquakes corpus.
This variation can be explained by differ-ences in entity realization between the two domains.
In particular, the two corpora varyin the amount of coreference they employ; texts from the Earthquakes corpus containmany examples of referring expressions that our simple identity-based approach cannotpossibly resolve.
Consider for instance the text in Table 6.
Here, the expressions thesame area, the remote region, and site all refer to Menglian county.
In comparison, the textfrom the Accidents corpus contains fewer referring expressions, in fact entities are oftenrepeated verbatim across several sentences, and therefore could be straightforwardlyresolved with a shallow approach (see the pilot, the pilot, the pilot in Table 6).The omission of syntactic information causes a drop in accuracy for models appliedto the Accidents corpus.
This effect is less noticeable on the Earthquakes corpus (com-pare the performance of model Coreference+Syntax?Salience+ on the two corpora).We explain this variation by the substantial difference in the type/token ratio betweenthe two domains?12.1 for Earthquakes versus 5.0 for Accidents.
The low type/tokenratio for Accidents means that most sentences in a text have some words in common.For example, the entities pilot, airplane, and airport appear in multiple sentences in thetext from Table 6.
Because there is so much repetition in this domain, the syntax-freegrids will be relatively similar for both coherent (original) and incoherent texts (permu-tations).
In fact, inspection of the grids from the Accidents corpus reveals that they havemany sequences of the form [X X X], [X ?
?
X], [X X ?
?
], and [?
?
X X] in common,16Barzilay and Lapata Modeling Local CoherenceTable 6Two texts from the Earthquakes and Accidents corpus.
One entity class for each document isshown to demonstrate the difference in referring expressions used in the two corpora.Example Text from EarthquakesA strong earthquake hit the China-Burma border early Wednesday morning, but therewere no reports of deaths, according to China?s Central Seismology Bureau.
The 7.3 quakehit?
??Menglian county at 5:46 am.?
??The same area was struck by a 6.2 temblor early Mondaymorning, the bureau said.
The county is on the China-Burma border, and is a sparsely populated,mountainous region.
The bureau?s XuWei said some buildings sustained damage and there weresome injuries, but he had no further details.
Communication with?
??the remote region is difficult,and satellite phones sent from the neighboring province of Sichuan have not yet reached?
??the site.However, he said the likelihood of deaths was low because residents should have been evacuatedfrom?
??the area following Monday?s quake.Example Text from AccidentsWhen?
??the pilot failed to arrive for his brother?s college graduation, concerned family membersreported that he and his airplane were missing.
A search was initiated, and the Civil Air Patrollocated the airplane on top of Pine Mountain.
According to?
??the pilot ?s flight log, the intendeddestination was Pensacola, FL, with intermediate stops for fuel at Thomson, GA, and Greenville,AL.
Airport personal at Thomson confirmed that the airplane landed about 1630 on 11/6/97.They reported that?
??the pilot purchased 26.5 gallons of 100LL fuel and departed about 1700.Witnesses at the Thomson Airport stated that when he took off, the weather was marginal VFRand deteriorating rapidly.
Witnesses near Pine Mountain stated that the visibility at the time ofthe accident was about 1/4 mile in haze/fog.whereas such sequences are more common in coherent Earthquakes documents andmore sparse in their permutations.
This indicates that syntax-free analysis can suffi-ciently discriminate coherent from incoherent texts in the Earthquakes domain, whilea more refined representation of entity transition types is required for the Accidentsdomain.The contribution of salience is less pronounced in both domains?the differ-ence in performance between the full model (Coreference+Syntax+Salience+) andits salience-agnostic counterpart (Coreference+Syntax+Salience+) is not statisti-cally significant.
Salience-based models do deliver some benefits for linguisticallyimpoverished models?for instance, Coreference?Syntax?Salience+ improves overCoreference?Syntax?Salience?
(p< 0.06) on the Earthquakes corpus.We hypothesizethat the small contribution of salience is related to the way it is currently represented.Addition of this knowledge source to our grid representation, doubles the numberof features that serve as input to the learning algorithm.
In other words, salience-aware models need to learn twice as many parameters as salience-free models, whilehaving access to the same amount of training data.
Achieving any improvement in theseconditions is challenging.Comparison with State-of-the-Art Methods.We next discuss the performance of the HMM-based content models (Barzilay and Lee 2004) and LSA (Foltz, Kintsch, and Landauer1998) in comparison to our model (Coreference+Syntax+Salience+).17Computational Linguistics Volume 34, Number 1First, note that the entity-grid model significantly outperforms LSA on both do-mains (p < .01 using a Sign test, see Table 5).
In contrast to our model, LSA is nei-ther entity-based nor unlexicalized: It measures the degree of semantic overlap acrosssuccessive sentences, without handling discourse entities in a special way (all contentwords in a sentence contribute towards its meaning).
We attribute our model?s superiorperformance, despite the lack of lexicalization, to three factors: (a) the use of moreelaborate linguistic knowledge (coreference and grammatical role information); (b) amore holistic representation of coherence (recall that our entity grids operate over textsrather than individual sentences; furthermore, entity transitions can span more thantwo consecutive sentences, something which is not possible with the LSA model); and(c) exposure to domain relevant texts (the LSA model used in our experiments was notparticularly tuned to the Earthquakes or Accidents corpus).
Our semantic space wascreated from a large news corpus (see Section 4.2) covering a wide variety of topicsand writing styles.
This is necessary for constructing robust vector representations thatare not extremely sparse.
We thus expect the grid models to be more sensitive to thediscourse conventions of the training/test data.The accuracy of the HMM-based content modes is comparable to the grid model onthe Earthquakes corpus (the difference is not statistically significant) but is significantlylower on the Accidents texts (see Table 5).
Although the grid model yields similarperformance on the two domains, content models exhibit high variability.
These resultsare not surprising.
The analysis presented in Barzilay and Lee (2004) shows that theEarthquakes texts are quite formulaic in their structure, following the editorial style ofthe Associated Press.
In contrast, the Accidents texts are more challenging for contentmodels?reports in this set do not undergo centralized editing and therefore exhibitmore variability in lexical choice and style.
The LSA model also significantly outper-forms the content model on the Earthquakes domain (p < .01 using a Sign test).
Being alocal model, LSA is less sensitive to the way documents are structured and is thereforemore likely to deliver consistent performance across domains.The comparison in Table 5 covers a broad spectrum of coherence models.
At oneend of the spectrum is LSA, a lexicalized model of local discourse coherence which isfairly robust and domain independent.
In the middle of the spectrum lies our entity-grid model, which is unlexicalized but linguistically informed and goes beyond sim-ple sentence-to-sentence transitions without, however, fully modeling global discoursestructure.
At the other end of the spectrum are the HMM-based content models, whichare both global and lexicalized.
Our results indicate that these models are complemen-tary and that their combination could yield improved results.
For example, we couldlexicalize our entity grids or supply the content models with local information either inthe style of LSA or as entity transitions.
However, we leave this to future work.Training Requirements.We now examine in more detail the training requirements for theentity-grid models.
Although for our ordering experiments we obtained training datacheaply, this will not generally be the case and some effort will have to be investedin collecting appropriate data with coherence ratings.
We thus address two questions:(1) Howmuch training data is required for achieving satisfactory performance?
(2) Howdomain sensitive are the entity-grid models?
In other words, does their performancedegrade gracefully when applied to out-of-domain texts?Figure 1 shows learning curves for the best performing model (Coreference+Syntax+Salience+) on the Earthquakes and Accidents corpora.
We observe that theamount of data required depends on the domain at hand.
The Accidents texts are morerepetitive and therefore less training data is required to achieve good performance.
The18Barzilay and Lapata Modeling Local CoherenceFigure 1Learning curves for the entity-based model Coreference+Syntax+Salience+ on theEarthquakes and Accidents corpora.learning curve is steeper for the Earthquakes documents.
Irrespective of the domaindifferences, the model reaches good accuracies when half of the data set is used (1,000pairwise rankings).
This is encouraging, because for some applications (e.g., summa-rization) large amounts of training data may be not readily available.Table 7 illustrates the accuracy of the best performing model Coreference+Syntax+Salience+ when trained on the Earthquakes corpus and tested on Accidentstexts and reversely when trained on the Accident corpus and tested on Earthquakesdocuments.
We also illustrate how this model performs when trained and tested ona data set that contains texts from both domains.
For the latter experiment the train-ing data set was created by randomly sampling 50 Earthquakes and 50 Accidentsdocuments.Table 7Accuracy of entity-based model (Coreference+Syntax+Salience+) and HHM-based contentmodel on out-of-domain texts.
Diacritics ** (p < .01) and * (p < .05) indicate whetherperformances on in-domain and out-of-domain data are significantly different using a FisherSign Test.Coreference+Syntax+SalienceTrainTest Earthquakes AccidentsEarthquakes 87.3 67.0?
?Accidents 69.7??
90.4EarthAccid 86.7 88.5?HMM-Based Content ModelsTrainTest Earthquakes AccidentsEarthquakes 88.0 31.7?
?Accidents 60.3??
75.819Computational Linguistics Volume 34, Number 1As can be seen from Table 7, the model?s performance degrades considerably(approximately by 20%) when tested on out-of-domain texts.
On the positive side,the model?s out-of-domain performance is better than chance (i.e., 50%).
Furthermore,once the model is trained on data representative of both domains, it performs almostas well as a model which has been trained exclusively on in-domain texts (see therow EarthAccid in Table 7).
To put these results into context, we also considered thecross-domain performance of the content models.
As Table 7 shows, the decrease inperformance is more dramatic for the content models.
In fact, the model trained onthe Earthquakes domain plummets below the random baseline when applied to theAccidents domain.
These results are expected for content models?the two domainshave little overlap in topics and do not share structural constraints.
Note that the LSAmodel is not sensitive to cross-domain issues.
The semantic space is constructed overmany different domains without taking into account style or writing conventions.The cross-training performance of the entity-based models is somewhat puzzling:these models are not lexicalized, and one would expect that valid entity transitionsare preserved across domains.
Although transition types are not domain-specific, theirdistribution could vary from one domain to another.
To give a simple example, somedomains will have more entities than others (e.g., descriptive texts).
In other words,entity transitions capture not only text coherence properties, but also reflect stylisticand genre-specific discourse properties.
This hypothesis is indirectly confirmed by theobserved differences in the contribution of various linguistic features across the twodomains discussed above.
Cross-domain differences in the distribution and occurrenceof entities have been also observed in other empirical studies of local coherence.
Forinstance, Poesio et al (2004) show differences in transition types between instructionaltexts and descriptions of museum texts.
In Section 6, we show that features derivedfrom the entity grid help determine the readability level for a given text, therebyverifying more directly the hypothesis that the grid representation captures stylisticdiscourse factors.The results presented so far suggest that adapting the proposed model to a newdomain would involve some effort in collecting representative texts with associatedcoherence ratings.
Thankfully, the entity grids are constructed in a fully automaticfashion, without requiring manual annotation.
This contrasts with traditional imple-mentations of Centering Theory that operate over linguistically richer representationsthat are typically hand-coded.5.
Experiment 2: Summary Coherence RatingWe further test the ability of our method to assess coherence by comparing modelinduced rankings against rankings elicited by human judges.
Admittedly, the syntheticdata used in the ordering task only partially approximates coherence violations thathuman readers encounter in machine generated texts.
A representative example ofsuch texts are automatically generated summaries which often contain sentences takenout of context and thus display problems with respect to local coherence (e.g., dan-gling anaphors, thematically unrelated sentences).
A model that exhibits high agree-ment with human judges not only accurately captures the coherence properties ofthe summaries in question, but ultimately holds promise for the automatic evaluationof machine-generated texts.
Existing automatic evaluation measures such as BLEU(Papineni et al 2002) and ROUGE (Lin and Hovy 2003) are not designed for thecoherence assessment task, because they focus on content similarity between systemoutput and reference texts.20Barzilay and Lapata Modeling Local Coherence5.1 ModelingSummary coherence rating can be also formulated as a ranking learning task.
We areassuming that the learner has access to several summaries corresponding to the samedocument or document cluster.
Such summaries can be produced by several systemsthat operate over identical inputs or by a single system (e.g., by varying the compressionlength or by switching on or off individual system modules, for example a sentencecompression or anaphora resolution module).
Similarly to the sentence ordering task,our training data includes pairs of summaries (xij, xik) of the same document(s) di,where xij is more coherent than xik.
An optimal learner should return a ranking r?
thatorders the summaries according to their coherence.
As in Experiment 1 we adopt anoptimization approach and follow the training regime put forward by Joachims (2002).5.2 MethodData.
Our evaluation was based on materials from the Document Understanding Con-ference (DUC 2003), which include multi-document summaries produced by humanwriters and by automatic summarization systems.
In order to learn a ranking, werequire a set of summaries, each of which has been rated in terms of coherence.
Onestumbling block to performing this kind of evaluation is the coherence ratings them-selves, which are not routinely provided byDUC summary evaluators.
In DUC 2003, thequality of automatically generated summaries was assessed along several dimensionsranging from grammatically, to content selection, fluency, and readability.
Coherencewas indirectly evaluated by noting the number of sentences indicating an awkwardtime sequence, suggesting a wrong cause?effect relationship, or being semanticallyincongruent with their neighboring sentences.8 Unfortunately, the observed coherenceviolations were not fine-grained enough to be of use in our rating experiments.
Inthe majority of cases DUC evaluators noted either 0 or 1 violations; however, withoutjudging the coherence of the summary as a whole, we cannot know whether a singleviolation disrupts coherence severely or not.We therefore obtained judgments for automatically generated summaries from hu-man subjects.9 We randomly selected 16 input document clusters and five systems thathad produced summaries for these sets, along with reference summaries composed byhumans.
Coherence ratings were collected during an elicitation study by 177 unpaidvolunteers, all native speakers of English.
The study was conducted remotely over theInternet.
Participants first saw a set of instructions that explained the task, and definedthe notion of coherence using multiple examples.
The summaries were randomized inlists following a Latin square design ensuring that no two summaries in a given listwere generated from the same document cluster.
Participants were asked to use a seven-point-scale to rate how coherent the summaries were without having seen the sourcetexts.
The ratings (approximately 23 per summary) given by our subjects were averagedto provide a rating between 1 and 7 for each summary.The reliability of the collected judgments is crucial for our analysis; we thereforeperformed several tests to validate the quality of the annotations.
First, we measuredhow well humans agree in their coherence assessment.
We employed leave-one-out8 See question 12 in http://duc.nist.gov/duc2003/quality.html.9 The ratings are available from http://homepages.inf.ed.ac.uk/mlap/coherence/.21Computational Linguistics Volume 34, Number 1resampling10 (Weiss and Kulikowski 1991), by correlating the data obtained from eachparticipant with the mean coherence ratings obtained from all other participants.
Theinter-subject agreement was r = .768 (p < .01.)
Second, we examined the effect of differ-ent types of summaries (human- vs.
machine-generated.)
An ANOVA revealed a reliableeffect of summary type: F(1; 15) = 20.38, p < .01 indicating that human summaries areperceived as significantly more coherent than system-generated ones.
Finally, we alsocompared the elicited ratings against the DUC evaluations using correlation analysis.The human judgments were discretized to two classes (i.e., 0 or 1) using entropy-baseddiscretization (Witten and Frank 2000).
We found a moderate correlation between thehuman ratings and DUC coherence violations (r = .41, p < .01).
This is expected giventhat DUC evaluators were using a different scale and and were not explicitly assessingsummary coherence.The summaries used in our rating elicitation study form the basis of a corpusused for the development of our entity-based coherence models.
To increase the sizeof our training and test sets, we augmented the materials used in the elicitation studywith additional DUC summaries generated by humans for the same input sets.
Weassumed that these summaries were maximally coherent.
As mentioned previously, ourparticipants tend to rate human-authored summaries higher than machine-generatedones.
To ensure that we do not tune a model to a particular system, we used the outputsummaries of distinct systems for training and testing.
Our set of training materialscontained 6?
16 summaries (average length 4.8), yielding(62)?
16 = 240 pairwise rank-ings.
Because human summaries often have identical (high) scores, we eliminated pairsof such summaries from the training set.
Consequently, the resulting training corpusconsisted of 144 summaries.
In a similar fashion, we obtained 80 pairwise rankings forthe test set.
Six documents from the training data were used as a development set.Features, Parameter Settings, and Training Requirements.We examine the influence of lin-guistic knowledge on model performance by comparing models with varying degreesof linguistic complexity.
To be able to assess the performance of our models across tasks(e.g., sentence ordering vs. summarization), we experimented with the same modeltypes introduced in the previous experiment (see Section 4.3).
We also investigate thetraining requirements for these models on the summary coherence task.Experiment 1 differs from the present study in the way coreference informationwas obtained.
In Experiment 1, a coreference resolution tool was applied to human-written texts, which are grammatical and coherent.
Here, we apply a coreference toolto automatically generated summaries.
Because many summaries in our corpus arefraught with coherence violations, the performance of a coreference resolution toolis likely to drop.
Unfortunately, resolving coreference in the input documents wouldrequire a multi-document coreference tool, which is currently unavailable to us.As in Experiment 1, the frequency threshold and the length of the transition se-quence were optimized on the development set.
Optimal salience-based models wereobtained for entities with frequency ?2.
The optimal transition length was ?2.
Allmodels were trained and tested using SVMlight (Joachims 2002).Comparison with State-of-the-Art Methods.
Our results were compared to the LSA modelintroduced in Experiment 1 (see Section 4.2 for details).
Unfortunately, we could not10 We cannot apply the commonly used Kappa statistic for measuring agreement because it is appropriatefor nominal scales, whereas our summaries are rated on an ordinal scale.22Barzilay and Lapata Modeling Local Coherenceemploy Barzilay and Lee?s (2004) content models for the summary ranking task.
Beingdomain-dependent, thesemodels require access to domain representative texts for train-ing.
Our summary corpus, however, contains texts frommultiple domains and does notprovide an appropriate sample for reliably training content models.5.3 ResultsImpact of Linguistic Representation.
Our results are summarized in Table 8.
Similarlyto the sentence ordering task, we observe that the linguistically impoverished modelCoreference?Syntax?Salience?
exhibits decreased accuracy when compared againstmodels that operate overmore sophisticated representations.
However, the contributionof individual knowledge sources differs in this task.
For instance, coreference resolu-tion improved model performance in ordering, but it causes a decrease in accuracyin summary evaluation (compare the models Coreference+Syntax+Salience+ andCoreference?Syntax+Salience+ in Tables 5 and 8).
This drop in performance can beattributed to two factors both related to the fact that our summary corpus containsmany machine-generated texts.
First, an automatic coreference resolution tool will beexpected to be less accurate on our corpus, because it was trained on well-formedhuman-authored texts.
Second, automatic summarization systems do not use anaphoricexpressions as often as humans do.
Therefore, a simple entity clustering method is moresuitable for automatic summaries.Both salience and syntactic information contribute to the accuracy of the rankingmodel.
The impact of each of these knowledge sources in isolation is not dramatic?dropping either of them yields some decrease in accuracy, but the difference is not sta-tistically significant.
However, eliminating both salience and syntactic information sig-nificantly decreases performance (compare Coreference?Syntax+Salience+ againstCoreference+Syntax?Salience?
and Coreference?Syntax?Salience?
in Table 8).Figure 2 shows the learning curve for our best model Coreference?Syntax+Salience+.
Although the model performs poorly when trained on a small fraction of thedata, it stabilizes relatively fast (with 80 pairwise rankings), and does not improve afterTable 8Summary ranking accuracy measured as fraction of correct pairwise rankings in the test set.Coreference[+/?]
indicates whether anaphoric information has been used when constructingthe entity grid.
Similarly, Syntax[+/?]
and Salience[+/?]
reflect the use of syntactic andsalience information.
Diacritics ** (p < .01) and * (p < .05) indicate whether Coreference?Syntax+Salience+ is significantly different from all other models (using a Fisher Sign Test).Model AccuracyCoreference+Syntax+Salience+ 80.0Coreference+Syntax+Salience?
75.0Coreference+Syntax?Salience+ 78.8Coreference?Syntax+Salience+ 83.8Coreference+Syntax?Salience?
71.3?Coreference?Syntax+Salience?
78.8Coreference?Syntax?Salience+ 77.5Coreference?Syntax?Salience?
73.8?Latent Semantic Analysis 52.5?
?23Computational Linguistics Volume 34, Number 1Figure 2Learning curve for the entity-based model Coreference?Syntax+Salience+ applied to thesummary ranking task.a certain point.
These results suggest that further improvements to summary rankingare unlikely to come from adding more annotated data.Comparison with the State-of-the-Art.
As in Experiment 1, we compared the best per-forming grid model (Coreference?Syntax+Salience+) against LSA (see Table 8).
Theformermodel significantly outperforms the latter (p < .01) by awidemargin.
LSA is per-haps at a disadvantage here because it has been exposed only to human-authored texts.Machine-generated summaries are markedly distinct from human texts even whenthese are incoherent (as in the case of our ordering experiment).
For example, manualinspection of our summary corpus revealed that low-quality summaries often containrepetitive information.
In such cases, simply knowing about high cross-sentential over-lap is not sufficient to distinguish a repetitive summary from a well-formed one.Furthermore, note that in contrast to the documents in Experiment 1, the summariesbeing ranked here differ in lexical choice.
Some are written by humans (and are thusabstracts), whereas others have been produced by systems following different summa-rization paradigms (some systems perform rewriting whereas others extract sentencesverbatim from the source documents).
This means that LSA may consider a summarycoherent simply because its vocabulary is familiar (i.e., it contains words for whichreliable vectors have been obtained).
Analogously, a summary with a large numberof out-of-vocabulary lexical items will be given low similarity scores, irrespective ofwhether it is coherent or not.
This is not uncommon in summaries with many propernames.
These often do not overlap with the proper names found in the North AmericanNews Corpus used for training the LSA model.
Lexical differences exert much lessinfluence on the entity-grid model which abstracts away from alternative verbalizationsof the same content and captures coherence solely on the basis of grid topology.6.
Experiment 3: Readability AssessmentSo far, our experiments have explored the potential of the proposed discourse repre-sentation for coherence modeling.
We have presented several classes of grid models24Barzilay and Lapata Modeling Local Coherenceachieving good performance in discerning coherent from incoherent texts.
Our experi-ments also reveal a surprising property of grid models: Even though these models arenot lexicalized, they are domain- and style-dependent.
In this section, we investigate indetail this feature of grid models.
Here, we move away from the coherence rating taskand put the entity-grid representation further to the test by examining whether it canbe usefully employed in style classification.
Specifically, we embed our entity grids intoa system that assesses document readability.
The term describes the ease with whicha document can be read and understood.
The quantitative measurement of readabilityhas attracted considerable interest and debate over the last 70 years (see Mitchell [1985]and Chall [1958] for detailed overviews) and has recently benefited from the use of NLPtechnology (Schwarm and Ostendorf 2005).A number of readability formulas have been developed with the primary aim ofassessing whether texts or books are suitable for students at particular grade levelsor ages.
Many readability methods focus on simple approximations of semantic factorsconcerning the words used and syntactic factors concerning the length or structure ofsentences (Gunning 1952; Kincaid et al 1975; Chall and Dale 1995; Stenner 1996; Katzand Bauer 2001).
Despite their widespread applicability in education and technicalwriting (Kincaid et al 1981), readability formulas are often criticized for being toosimplistic; they systematically ignoremany important factors that affect readability suchas discourse coherence and cohesion, layout and formatting, use of illustrations, thenature of the topic, the characteristics of the readers, and so forth.Schwarm and Ostendorf (2005) developed amethod for assessing readability whichaddresses some of the shortcomings of previous approaches.
By recasting readabilityassessment as a classification task, they are able to combine several knowledge sourcesranging from traditional reading level measures, to statistical language models, andsyntactic analysis.
Evaluation results show that their system outperforms two com-monly used reading level measures (the Flesch-Kincaid Grade Level index and Lexile).In the following we build on their approach and examine whether the entity-grid rep-resentation introduced in this article contributes to the readability assessment task.
Theincorporation of coherence-based information in the measurement of text readability is,to our knowledge, novel.6.1 ModelingWe follow Schwarm andOstendorf (2005) in treating readability assessment as a classifi-cation task.
The unit of classification is a single article and the learner?s task is to predictwhether it is easy or difficult to read.
A variety of machine learning techniques areamenable to this problem.
Because our goal was to replicate Schwarm and Ostendorf?ssystem as closely as possible, we followed their choice of support vector machines(SVMs) (Joachims 1998b) for our classification experiments.
Our training sample there-fore consisted of n documents such that(x1, y1), .
.
.
, ( xn, yn) xi ?
N, yi ?
{?1,+1}where xi is a feature vector for the ith document in the training sample and yi its(positive or negative) class label.
In the basic SVM framework, we try to separate thepositive and negative instances by a hyperplane.
This means that there is a weight25Computational Linguistics Volume 34, Number 1Table 9Excerpts from the Britannica readability corpusThe Lemma Valletta in BritannicaAlso spelled Valletta, seaport and capital of Malta, on the northeast coast of the island.
Thenucleus of the city is built on the promontory of Mount Sceberras that runs like a tongue intothe middle of a bay, which it thus divides into two harbours, Grand Harbour to the east andMarsamxett (Marsamuscetto) Harbour to the west.
Built after the Great Siege of Malta in 1565,which checked the advance of Ottoman power in southern Europe, it was named after Jean Parisotde la Valette, grand master of the order of Hospitallers (Knights of St. John of Jerusalem), andbecame the Maltese capital in 1570.
The Hospitallers were driven out by the French in 1798, anda Maltese revolt against the French garrison led to Valletta?s seizure by the British in 1800.The Lemma Valletta in Britannica ElementaryA port city, Valletta is the capital of the island country of Malta in the Mediterranean Sea.
Vallettais located on the eastern coast of the largest island, which is also named Malta.
Valletta lies on apeninsula?a land mass surrounded by water on three sides.
It borders Marsamxett Harbor to thenorth and Grand Harbor to the south.
The eastern end of the city juts out into the Mediterranean.Valletta was planned in the 16th century by the Italian architect Francesco Laparelli.
To maketraveling through Valletta easier, Laparelli designed the city in a grid pattern with straight streetsthat crossed each other and ran the entire width and length of the town.
Valletta was one of thefirst towns to be laid out in this way.vector w and a threshold b, so that all positive training examples are on one side of thehyperplane, while all negative ones lie on the other side.
This is equivalent to requiringyi[(w ?
xi)+ b] > 0Finding the optimal hyperplane is an optimization problem which can be solvedefficiently using the procedure described in Vapnik (1998).
SVMs have been widelyused for many NLP tasks ranging from text classification (Joachims 1998b), to syntacticchunking (Kudo and Matsumoto 2001), and shallow semantic parsing (Pradhan et al2005).6.2 MethodData.
For our experiments we used a corpus collected by Barzilay and Elhadad (2003)from the Encyclopedia Britannica and Britannica Elementary.
The latter is a new versiontargeted at children.
The corpus contains 107 articles from the full version of the encyclo-pedia and their corresponding simplified articles from Britannica Elementary (214 articlesin total).
Although these texts are not explicitly annotated with grade levels, they stillrepresent two broad readability categories, namely, easy and difficult.11 Examples ofthese two categories are given in Table 9.11 The Britannica corpus was also used by Schwarm and Ostendorf (2005); in addition they make use of acorpus compiled from theWeekly Reader, an educational newspaper with documents targeted at gradelevels 2?5.
Unfortunately, this corpus is not publicly available.26Barzilay and Lapata Modeling Local CoherenceFeatures and Parameter Settings.We created two system versions: the first one used solelySchwarm and Ostendorf (2005) features;12 the second one employed a richer featurespace?we added the entity-based representation proposed here to their original featureset.
We will briefly describe the readability-related features used in our systems anddirect the interested reader to Schwarm and Ostendorf for a more detailed discussion.Schwarm and Ostendorf (2005) use three broad classes of features: syntactic, se-mantic, and their combination.
Their syntactic features are average sentence length andfeatures extracted from parse trees computed using Charniak?s (2000) parser.
The latterinclude average parse tree height, average number of NPs, average number of VPs, andaverage number of subordinate clauses (SBARs).
We computed average sentence lengthby measuring the number of tokens per sentence.Their semantic features include the average number of syllables per word, andlanguage model perplexity scores.
A unigram, bigram, and trigram model was esti-mated for each class, and perplexity scores were used to assess their performance ontest data.
Following Schwarm and Ostendorf (2005) we used information gain to selectwords that were good class discriminants.
All remaining words were replaced by theirparts of speech.
The vocabulary thus consisted of 300 words with high informationgain and 36 Penn Treebank part-of-speech tags.
The language models were estimatedusing maximum likelihood estimation and smoothed with Witten-Bell discounting.The language models described in this article were all built using the CMU statisticallanguage modeling toolkit (Clarkson and Rosenfeld 1997).
Our perplexity scores weresix in total (2 classes ?
3 language models).Finally, the Flesch-Kincaid Grade Level score was included as a feature that cap-tures both syntactic and semantic text properties.
The Flesch-Kincaid formula estimatesreadability as a combination of the the average number of syllables per word and theaverage number of words per sentence:0.39(total wordstotal sentences)+ 11.8(total syllablestotal words)?
15.59 (3)We also enriched Schwarm and Ostendorf?s (2005) feature space with coherence-based features.
Each document was represented as a feature vector using the entity tran-sition notation introduced in Section 3.
We experimented with two models that yieldedgood performances in our previous experiments: Coreference+Syntax+Salience+ (seeExperiment 1) and Coreference?Syntax+Salience+ (see Experiment 2).
The transitionlength was ?2 and entities were considered salient if they occurred ?2 times.
As in ourprevious experiments, we compared the entity-based representation against LSA.
Thelatter is a measure of the semantic relatedness across pairs of sentences.
We could notapply the HMM-based content models (Barzilay and Lee 2004) to the readability dataset.
The encyclopedia lemmas are written by different authors and consequently varyconsiderably in structure and vocabulary choice.
Recall that these models are suitablefor more restricted domains and texts that are more formulaic in nature.12 Schwarm and Ostendorf (2005) define out-of-vocabulary (OOV) scores relative to the most commonwords in grade 2, the lowest grade level in their corpus; it was not possible to estimate OOV scores,because we did not have access to grade 2 texts.27Computational Linguistics Volume 34, Number 1Table 10The contribution of coherence-based features to the automatic readability assessment task.Diacritics ** (p < .01) and * (p < .05) indicate whether differences in accuracy betweenSchwarm and Ostendorf and all other models are significant (using a Fisher Sign test).Model AccuracySchwarm & Ostendorf 78.56Schwarm & Ostendorf, Coreference+Syntax+Salience+ 88.79?Schwarm & Ostendorf, Coreference?Syntax+Salience+ 79.49Schwarm & Ostendorf, Latent Semantic Analysis 78.56Coreference+Syntax+Salience+ 50.90?
?Coreference?Syntax+Salience+ 49.55?
?Latent Semantic Analysis 48.58?
?The different systems were trained and tested on the Britannica corpus using five-fold cross-validation.13 The languagemodels were created anew for every fold using thedocuments in the training data.
We use Joachims?
(1998a) SVMlight package for trainingand testing with all parameters set to their default values.Evaluation Metric.
We measure classification accuracy (i.e., the number of classes as-signed correctly by the SVM over the size of the test set).
We report accuracy averagedover folds.
A chance baseline (selecting one class at random) yields an accuracy of 50%.Our training and test sets have the same number of documents for the two readabilitycategories.6.3 ResultsTable 10 summarizes our results on the readability assessment task.
We first com-pared Schwarm and Ostendorf?s (2005) system against a system that incorporatesentity-based coherence features (see rows 3?4 in Table 10).
As can be seen, the sys-tem?s accuracy significantly increases by 10% when the full feature set is included(Coreference+Syntax+Salience+).
Entity-grid features that do not incorporate corefer-ence information (Coreference?Syntax+Salience+) perform numerically better (com-pare row 1 and 3 in Table 10); however, the difference is not statistically significant.The superior performance of the Coreference+Syntax+Salience+ feature set is notentirely unexpected.
Inspection of our corpus revealed that easy and difficult texts differin their distribution of pronouns and coreference chains in general.
Easy texts tend toemploy less coreference and the use of personal pronouns is relatively sparse.
To givea concrete example, the pronoun they is attested 173 times in the difficult corpus andonly 73 in the easy corpus.
This observation suggests that coreference information is agood indicator of the level of reading difficulty and explains why its omission from theentity-based feature space yields inferior performance.13 The data for the experiments reported here can be found at http://homepages.inf.ed.ac.uk/mlap/coherence/.28Barzilay and Lapata Modeling Local CoherenceFurthermore, note that discourse-level information is absent from Schwarm andOstendorf?s (2005) original model.
The latter employs a large number of lexical andsyntactic features which capture sentential differences among documents.
Our entity-based representation supplements their feature space with information spanning two ormore successive sentences.
We thus are able to model stylistic differences in readabilitythat go beyond syntax and lexical choice.
Besides coreference, our feature representa-tion captures important information about the presence and distribution of entities indiscourse.
For example, difficult texts tend to have twice as many entities as easy ones.Consequently, easy and difficult texts are represented by entity transition sequenceswith different probabilities (e.g., the sequences [S S] and [S O] are more probable indifficult texts).
Interestingly, when coherence is quantified using LSA, we observe noimprovement to the classification task.
The LSA scores capture lexical or semantic textproperties similar to those expressed by the Flesch Kincaid index and the perplexityscores (e.g., word repetition).
It is therefore not surprising that their inclusion in thefeature set does not increase performance.We also evaluated the training requirements for the readability system describedherein.
Figure 3 shows the learning curve for Schwarm and Ostendorf?s (2005) modelenhanced with the Coreference+Syntax+Salience+ feature space and on its own.
Ascan be seen, both models perform relatively well when trained on small data sets(e.g., 20?40 documents) and reach peak accuracy with half of the training data.
Theinclusion of discourse-based features consistently increases accuracy irrespective of theamount of training data available.
Figure 3 thus suggests that better feature engineeringis likely to bring further performance improvements on the readability task.Our results indicate that the entity-based text representation introduced here cap-tures aspects of text readability and can be successfully incorporated into a practicalsystem.
Coherence is by no means the sole predictor of readability.
In fact, on its own,it performs poorly on this task as demonstrated when using either LSA or the entity-based feature space without Schwarm and Ostendorf?s (2005) features (see rows 5?7 inTable 10).
Rather, we claim that coherence is one among many factors contributing totext readability and that our entity-grid representation is well-suited for text classifica-tion tasks such as reading level assessment.Figure 3Learning curve for Schwarm and Ostendorf?s (2005) model on its own and enhanced with theCoreference+Syntax+Salience+ feature space.29Computational Linguistics Volume 34, Number 17.
Discussion and ConclusionsIn this article we proposed a novel framework for representing and measuring text co-herence.
Central to this framework is the entity-grid representation of discourse, whichwe argue captures important patterns of sentence transitions.
We re-conceptualize co-herence assessment as a learning task and show that our entity-based representationis well-suited for ranking-based generation and text classification tasks.
Using theproposed representation, we achieve good performance on text ordering, summarycoherence evaluation, and readability assessment.The entity grid is a flexible, yet computationally tractable, representation.
Weinvestigated three important parameters for grid construction: the computation ofcoreferring entity classes, the inclusion of syntactic knowledge, and the influence ofsalience.
All these knowledge sources figure prominently in theories of discourse(see Section 2) and are considered important in determining coherence.
Our resultsempirically validate the importance of salience and syntactic information (expressedby S, O, X, and ?)
for coherence-based models.
The combination of both knowledgesources (Syntax+Salience) yields models with consistently good performance for allour tasks.The benefits of full coreference resolution are less uniform.
This is partly due tomismatches between training and testing conditions.
The system we employ (Ng andCardie 2002) was trained on human-authored newspaper texts.
The corpora we usedin our sentence ordering and readability assessment experiments are somewhat similar(i.e., human-authored narratives), whereas our summary coherence rating experimentemployed machine generated texts.
It is therefore not surprising that coreference reso-lution delivers performance gains on the first two tasks but not on the latter (see Table 5in Section 4 and Table 10 in Section 6.3).
Our results further show that in lieu of anautomatic coreference resolution system, entity classes can be approximated simplyby string matching.
The latter is a good indicator of nominal coreference; it is oftenincluded as a feature in machine learning approaches to coreference resolution (Soon,Ng, and Lim 2001; Ng and Cardie 2002) and is relatively robust (i.e., likely to deliverconsistent results in the face of different domains and genres).It is important to note that, although inspired by entity-based theories of discoursecoherence, our approach is not a direct implementation of any theory in particular.Rather, we sacrifice linguistic faithfulness for automatic computation and breadth ofcoverage.
Despite approximations and unavoidable errors (e.g., in the parser?s output),our results indicate that entity grids are a useful representational framework acrosstasks and text genres.
In agreement with Poesio et al (2004) we find that pronomi-nalization is a good indicator of document coherence.
We also find that coherent textsare characterized by transitions with particular properties which do not hold for alldiscourses.
Contrary to Centering Theory, we remain agnostic to the type of transi-tions that our models capture (e.g., CONTINUE, SHIFT).
We simply record whether anentity is mentioned in the discourse and in what grammatical role.
Our experimentsquantitatively measured the predictive power of various linguistic features for severalcoherence-related tasks.
Crucially, we find that ourmodels are sensitive to the domain athand and the type of texts under consideration (human-authored vs. machine generatedtexts).
This is an unavoidable consequence of the grid representation, which is entity-specific.
Differences in entity distribution indicate not only differences in coherence, butalso in writing conventions and style.
Similar observations have been made in otherwork which is closer in spirit to Centering?s claims (Hasler 2004; Karamanis et al 2004;Poesio et al 2004).30Barzilay and Lapata Modeling Local CoherenceAn important future direction lies in augmenting our entity-based representationwith more fine-grained lexico-semantic knowledge.
One way to achieve this goal is tocluster entities based on their semantic relatedness, thereby creating a grid represen-tation over lexical chains (Morris and Hirst 1991).
An entirely different approach is todevelop fully lexicalized models, akin to traditional language models.
Cache languagemodels (Kuhn and De Mori 1990) seem particularly promising in this context.
Thegranularity of syntactic information is another topic that warrants further investigation.So far we have only considered the contribution of ?core?
grammatical relations tothe grid construction.
Expanding our grammatical categories to modifiers and adjunctsmay provide additional information, in particular when consideringmachine generatedtexts.
We also plan to investigate whether the proposed discourse representation andmodeling approaches generalize across different languages.
For instance the identifi-cation and extraction of entities poses additional challenges in grid construction forChinese where word boundaries are not denoted orthographically (by space).
Similarchallenges arise in German, a language with a large number of inflected forms andproductive derivational processes (e.g., compounding) not indicated by orthography.In the discourse literature, entity-based theories are primarily applied at the levelof local coherence, while relational models, such as Rhetorical Structure Theory (Mannand Thomson 1988; Marcu 2000), are used to model the global structure of discourse.We plan to investigate how to combine the two for improved prediction on both localand global levels, with the ultimate goal of handling longer texts.AcknowledgmentsThe authors acknowledge the support ofthe National Science Foundation (Barzilay;CAREER grant IIS-0448168 and grantIIS-0415865) and EPSRC (Lapata; grantGR/T04540/01).
We are grateful to ClaireCardie and Vincent Ng for providing usthe results of their coreference systemon our data.
Thanks to Eli Barzilay, EugeneCharniak, Michael Elhadad, NoemieElhadad, Nikiforos Karamanis, Frank Keller,Alex Lascarides, Igor Malioutov, SmarandaMuresan, Martin Rinard, Kevin Simler,Caroline Sporleder, Chao Wang, BonnieWebber, and three anonymous reviewersfor helpful comments and suggestions.Any opinions, findings, and conclusions orrecommendations expressed herein are thoseof the authors and do not necessarily reflectthe views of the National Science Foundationor EPSRC.ReferencesAlthaus, Ernst, Nikiforos Karamanis, andAlexander Koller.
2004.
Computing locallycoherent discourses.
In Proceedings of the42nd Annual Meeting of the Association forComputational Linguistics, pages 399?406,Barcelona, Spain.Ariel, Mira.
1988.
Referring and accessibility.Journal of Linguistics, 24:65?87.Asher, Nicholas and Alex Lascarides.
2003.Logics of Conversation.
CambridgeUniversity Press, Cambridge, England.Barzilay, Regina.
2003.
Information Fusionfor Multi-Document Summarization:Praphrasing and Generation.
Ph.D. thesis,Columbia University, New York.Barzilay, Regina and Noemie Elhadad.
2003.Sentence alignment for monolingualcomparable corpora.
In Proceedings of the8th Conference on Empirical Methods inNatural Language Processing, pages 25?32,Sapporo, Japan.Barzilay, Regina and Mirella Lapata.
2005.Modeling local coherence: An entity-basedapproach.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics, pages 141?148, Ann Arbor, MI.Barzilay, Regina and Lillian Lee.
2004.Catching the drift: Probabilistic contentmodels, with applications to generationand summarization.
In Proceedings of the2nd Human Language Technology Conferenceand Annual Meeting of the North AmericanChapter of the Association for ComputationalLinguistics, pages 113?120, Boston, MA.Berry, Michael W., Susan T. Dumais, andGavin W. O?Brien.
1994.
Using linearalgebra for intelligent informationretrieval.
SIAM Review, 37(4):573?595.Brennan, Susan E., Marilyn W. Friedman,and Charles J. Pollard.
1987.
A centeringapproach to pronouns.
In Proceedings of the31Computational Linguistics Volume 34, Number 125th Annual Meeting of the Association forComputational Linguistics, pages 155?162,Palo Alto, CA.Briscoe, Ted and John Carroll.
2002.
Robustaccurate statistical annotation of generaltext.
In Proceedings of the 3rd InternationalConference on Language Resources andEvaluation, pages 1499?1504, Las Palmas,Canary Islands.Chafe, Wallace L. 1976.
Givenness,contrastiveness, definiteness, subjects,topics, and point of view.
In Charles N. Li,editor, Subject and Topic.
Academic Press,New York, pages 25?55.Chall, Jeanne S. 1958.
Readability: AnAppraisal of Research and Application.Number 34 in Bureau of EducationalResearch Monographs.
Ohio StateUniversity Press, Columbus.Chall, Jeanne S. and Edgar Dale.
1995.Readability Revisited: The New Dale-ChallReadability Formula.
Brookline Books,Cambridge, MA.Charniak, Eugene.
2000.
Amaximum-entropy-inspired parser.
InProceedings of the 1st Annual Meeting of theNorth American Chapter of the Association forComputational Linguistics, pages 132?139,Seattle, WA.Clark, Herbert H. and Susan E. Haviland.1977.
Comprehension and the given-newcontract.
In Roy O. Freedle, editor,Discourse Production and Comprehension.Ablex, Norwood, NJ, pages 1?39.Clarkson, Philip and Ronald Rosenfeld.1997.
Statistical language modeling.In Proceedings of ESCA EuroSpeech?97,pages 2707?2710, Rhodes, Greece.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.In Proceedings of the 35th Annual Meetingof the Association for ComputationalLinguistics and 8th Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 16?23,Madrid, Spain.Collins, Michael.
2002.
Discriminativereranking for natural language parsing.In Proceedings of the 17th InternationalConference on Machine Learning,pages 175?182, Palo Alto, CA.Foltz, Peter W., Walter Kintsch, andThomas K. Landauer.
1998.
Textualcoherence using latent semantic analysis.Discourse Processes, 25(2&3):285?307.Freund, Yovav, Raj Iyer, Robert E. Schapire,and Yoram Singer.
2003.
An efficientboosting algorithm for combiningpreferencs.Machine Learning, 4:933?969.Germann, Ulrich, Michael Jahr, KevinKnight, Daniel Marcu, and Kenji Yamada.2004.
Fast and optimal decoding formachine translation.
Artificial Intelligence,154(1?2):127?143.Givon, Talmy.
1987.
Beyond foreground andbackground.
In Russell S. Tomlin, editor,Coherence and Grounding in Discourse.Benjamins, Amsterdam/Philadelphia,pages 175?188.Grosz, Barbara, Aravind K. Joshi, and ScottWeinstein.
1995.
Centering: A frameworkfor modeling the local coherence ofdiscourse.
Computational Linguistics,21(2):203?225.Gundel, Jaenette K., Nancy Hedberg, andRon Zacharski.
1993.
Cognitive statusand the form of referring expressionsin discourse.
Language, 69(2):274?307.Gunning, Robert.
1952.
The Technique of ClearWriting.
McGraw Hill, New York.Halliday, M. A. K. and Ruqaiya Hasan.
1976.Cohesion in English.
Longman, London.Hasler, Laura.
2004.
An investigation intothe use of centering transitions forsummarisation.
In Proceedings of the7th Annual CLUK Research Colloquium,pages 100?107, Birmingham, UK.Hoey, Michael.
1991.
Patterns of Lexis in Text.Oxford University Press, Oxford, England.Hudson, S. B., M. K. Tanenhaus, and G. S.Dell.
1986.
The effect of the discoursecenter on the local coherence of adiscourse.
In Proceedings of the 8th AnnualMeeting of the Cognitive Science Society,pages 96?101, Amherst, MA.Joachims, Thorsten.
1998a.
Makinglarge-scale support vector machinelearning practical.
In Bernard Scho?lkopf,Christopher Burges, and Alexander Smola,editors, Advances in Kernel Methods:Support Vector Machines.
MIT Press,Cambridge, MA.Joachims, Thorsten.
1998b.
Textcategorization with support vectormachines: Learning with many relevantfeatures.
In Proceedings of the EuropeanConference on Machine Learning,pages 137?142, Berlin, Springer.Joachims, Thorsten.
2002.
Optimizing searchengines using clickthrough data.
InProceedings of ACM Conference on KnowledgeDiscovery and Data Mining, pages 133?142,Chicago, IL.Kameyama, Megumi.
1986.
Aproperty-sharing constraint in centering.In Proceedings of the 24th Annual Meeting ofthe Association for Computational Linguistics,pages 200?206, New York.32Barzilay and Lapata Modeling Local CoherenceKaramanis, Nikiforos.
2003.
Entity Coherencefor Descriptive Text Structuring.
Ph.D.thesis, University of Edinburgh,Edinburgh, Scotland.Karamanis, Nikiforos, Massimo Poesio,Chris Mellish, and Jon Oberlander.2004.
Evaluating centering-basedmetrics of coherence for text structuringusing a reliably annotated corpus.
InProceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics,pages 391?398, Barcelona, Spain.Karttunen, Lauri.
1976.
Discourse referents.In James D. McCawley, editor, Syntaxand Semantics: Notes from the LinguisticUnderground, volume 7.
Academic Press,New York, pages 363?386.Katz, Irvin R. and Malcolm I. Bauer.
2001.Sourcefinder: Course preparation vialinguistically targeted web search.
Journalof Educational Technology and Society,4(3):45?49.Kibble, Rodger and Richard Power.
2004.Optimising referential coherence in textgeneration.
Computational Linguistics,30(4):401?416.Kincaid, J. Peter, James Aagard, John O?Hara,and Larry Cottrell.
1981.
Computerreadability editing system.
IEEETransactions on Professional Communication,1(24):34?81.Kincaid, Peter J., Robert P. Fishburne,Richard L. Rodgers, and Brad S. Chissom.1975.
Derivation of new readabilityformulas for Navy enlisted personnel.Research Branch Report 8-75, U.S.Naval Air Station, Memphis, TN.Knight, Kevin and Vasileios Hatzivassiloglou.1995.
Two-level, many-path generation.
InProceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics,pages 252?260, Cambridge, MA.Kudo, Taku and Yuji Matsumoto.
2001.Chunking with support vector machines.In Thorsten Joachims, editor, Proceedingsof the 2nd Annual Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 192?199,Pittsburgh, PA.Kuhn, R. and R. De Mori.
1990.
Acache-based natural language model forspeech recognition.
IEEE Transactions onPAMI, 12(6):570?583.Kuno, Susumu.
1972.
Functional sentenceperspective.
Linguistic Inquiry, 3:269?320.Landauer, Thomas K. and Susan T. Dumais.1997.
A solution to Plato?s problem:The latent semantic analysis theory ofacquisition, induction and representationof knowledge.
Psychological Review,104(2):211?240.Langkilde, Irene and Kevin Knight.
1998.Generation that exploits corpus-basedstatistical knowledge.
In Proceedingsof the 17th International Conference onComputational Linguistics and 36thAnnual Meeting of the Association forComputational Linguistics, pages 704?710,Montre?al, Canada.Lapata, Mirella.
2003.
Probabilistic textstructuring: Experiments with sentenceordering.
In Proceedings of the 41st AnnualMeeting of the Association for ComputationalLinguistics, pages 545?552, Sapporo, Japan.Lin, Chin-Yew and Eduard H. Hovy.
2003.Automatic evaluation of summaries usingn-gram co-occurrence statistics.
InProceedings of the 2nd Human LanguageTechnology Conference and Annual Meeting ofthe North American Chapter of the Associationfor Computational Linguistics, pages 71?78,Boston, MA.Lin, Dekang.
2001.
LaTaT: Language andtext analysis tools.
In Proceedings of the 1stInternational Conference on Human LanguageTechnology Research, pages 222?227,San Francisco, CA.Mann, William C. and Sandra A. Thomson.1988.
Rhetorical structure theory.
Text,8(3):243?281.Marcu, Daniel.
2000.
The Theory and Practiceof Discourse Parsing and Summarization.MIT Press, Cambridge, MA.McKoon, Gail and Roger Ratcliff.
1992.Inference during reading.
PsychologicalReview, 99(3):440?446.Mellish, Chris, Mick O?Donnell, JonOberlander, and Alistair Knott.
1998.Experiments using stochastic searchfor text planning.
In Proceedings of the9th International Workshop on NaturalLanguage Generation, pages 98?107,New Brunswick, NJ.Miltsakaki, Eleni and Karen Kukich.
2000.The role of centering theory?s rough-shiftin the teaching and evaluation of writingskills.
In Proceedings of the 38th AnnualMeeting of the Association for ComputationalLinguistics, pages 408?415, Hong Kong.Mitchell, James V. 1985.
The Ninth MentalMeasurements Yearbook.
University ofNebraska Press, Lincoln.Morris, Jane and Graeme Hirst.
1991.
Lexicalcohesion computed by thesaural relationsas an indicator of the structure of text.Computational Linguistics, 1(17):21?43.Ng, Vincent and Claire Cardie.
2002.Improving machine learning approaches33Computational Linguistics Volume 34, Number 1to coreference resolution.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics, pages 104?111,Philadelphia, PA.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
Bleu: Amethod for automatic evaluation ofmachine translation.
In Proceedings of the40th Annual Meeting of the Association forComputational Linguistics, pages 311?318,Philadelphia, PA.Poesio, Massimo, Rosemary Stevenson,Barbara Di Eugenio, and Janet Hitzeman.2004.
Centering: A parametric theory andits instantiations.
Computational Linguistics,30(3):309?363.Pradhan, Sameer, Kadri Hacioglu, ValerieKrugler, Wayne Ward, James H. Martin,and Dan Jurafsky.
2005.
Support vectorlearning for semantic argumentclassification.Machine Learning,60(1):11?39.Prince, Ellen.
1978.
A comparison of wh-cleftsand it-clefts in discourse.
Language,54:883?906.Prince, Ellen.
1981.
Toward a taxonomy ofgiven-new information.
In Peter Cole,editor, Radical Pragmatics.
Academic Press,New York, pages 223?255.Reiter, Ehud and Robert Dale.
2000.Building Natural-Language GenerationSystems.
Cambridge University Press,Cambridge, England.Schwarm, Sarah E. and Mari Ostendorf.
2005.Reading level assessment using supportvector machines and statistical languagemodels.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics, pages 523?530, Ann Arbor, MI.Scott, Donia and Clarisse Sieckeniusde Souza.
1990.
Getting the message acrossin RST-based text generation.
In RobertDale, Chris Mellish, and Michael Zock,editors, Current Research in NaturalLanguage Generation.
Academic Press,New York, pages 47?73.Sidner, Candace L. 1979.
Towards aComputational Theory of Definite AnaphoraComprehension in English Discourse.
Ph.D.thesis, MIT.Soon, W. M., Hwee Tou Ng, and D. C. Y. Lim.2001.
A machine learning approach tocoreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Stenner, A. Jackson.
1996.
Measuringreading comprehension with thelexile framework.
Presented at theCalifornia Comparability Symposium,Burlingame, CA.Strube, Michael and Udo Hahn.
1999.Functional centering?Groundingreferential coherence in informationstructure.
Computational Linguistics,25(3):309?344.Toutanova, Kristina, Penka Markova, andChristopher D. Manning.
2004.
The leafprojection path view of parse trees:Exploring string kernels for HPSGparse selection.
In Proceedings of theConference on Empirical Methodsin Natural Language Processing,pages 166?173, Barcelona, Spain.Vapnik, Vladimir.
1998.
Statistical LearningTheory.
Wiley, Chichester, UK.Vilain, Marc, John Burger, John Aberdeen,Dennis Connolly, and Lynette Hirschman.1995.
A model-theoretic coreferencescoring scheme.
In Proceedings of the 6thMessage Understanding Conference (MUC-6),pages 45?52, San Francisco, CA.Walker, Marilyn, Masayo Iida, and SharonCote.
1994.
Japanese discourse and theprocess of centering.
ComputationalLinguistics, 20(2):193?232.Walker, Marilyn, Aravind Joshi, andEllen Prince, editors.
1998.
CenteringTheory in Discourse.
Clarendon Press,Oxford, UK.Walker, Marilyn A., Owen Rambow, andMonica Rogati.
2001.
Spot: A trainablesentence planner.
In Proceedings of the2nd Annual Meeting of the North AmericanChapter of the Association for ComputationalLinguistics, pages 17?24, Pittsburgh, PA.Weiss, Sholom M. and Casimir A.Kulikowski.
1991.
Computer Systems thatLearn: Classification and Prediction Methodsfrom, Statistics, Neural Nets, MachineLearning, and Expert Systems.
MorganKaufmann, San Mateo, CA.Witten, Ian H. and Eibe Frank.
2000.
DataMining: Practical Machine Learning Toolsand Techniques with Java Implementations.Morgan Kaufman, San Mateo, CA.34
