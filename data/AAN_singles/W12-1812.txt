NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 23?24,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsA belief tracking challenge task for spoken dialog systemsJason D. WilliamsMicrosoft Research, Redmond, WA 98052 USAjason.williams@microsoft.comAbstractBelief tracking is a promising technique foradding robustness to spoken dialog systems,but current research is fractured across differ-ent teams, techniques, and domains.
This pa-per amplifies past informal discussions (Raux,2011) to call for a belief tracking challengetask, based on the Spoken dialog challengecorpus (Black et al, 2011).
Benefits, limita-tions, evaluation design issues, and next stepsare presented.1 Introduction and backgroundIn dialog systems, belief tracking refers to maintain-ing a distribution over multiple dialog states as a di-alog progresses.
Belief tracking is desirable becauseit provides robustness to errors in speech recogni-tion, which can be quite common.This distribution can be modeled in a varietyof ways, including heuristic scores (Higashinaka etal., 2003), Bayesian networks (Paek and Horvitz,2000; Williams and Young, 2007), and discrimi-native models (Bohus and Rudnicky, 2006).
Tech-niques have been fielded which scale to realisti-cally sized dialog problems and operate in real time(Young et al, 2009; Thomson and Young, 2010;Williams, 2010; Mehta et al, 2010).
In lab settings,belief tracking has been shown to improve overallsystem performance (Young et al, 2009; Thomsonand Young, 2010).Despite this progress, there are still important un-resolved issues.
For example, a deployment withreal callers (Williams, 2011) found that belief track-ing sometimes degraded performance due to modelmis-matches that are difficult to anticipate at train-ing time.
What is lacking is a careful comparisonof methods to determine their relative strengths, interms of generalization, sample efficiency, speed,etc.This position paper argues for a belief trackingchallenge task.
A corpus of labeled dialogs and scor-ing code would be released.
Research teams wouldenter one or more belief tracking algorithms, whichwould be evaluated on a held-out test set.2 CorpusThe Spoken dialog challenge corpus is an attractivecorpus for this challenge.
It consists of phone callsfrom real (not simulated) bus riders with real (notimagined) information needs.
There have been 2rounds of the challenge (2010, and 2011-2012), with3 systems in each round.
The rounds differed inscope and (probably) user population.
A total of 3different teams entered systems, using different dia-log designs, speech recognizers, and audio output.For each system in each round, 500-1500 dialogswere logged.
While it would be ideal if the corpusincluded more complex interactions such as negotia-tions, as a publicly available corpus it is unparalleledin terms of size, realism, and system diversity.There are limitations to a challenge based on thiscorpus: it would not allow comparisons across do-mains, nor for multi-modal or situated dialog.
Theseaspects could be left for a future challenge.
An-other possible objection is that off-line experimentswould not measure end-to-end impact on a real di-alog system; however, we do know that good be-lief tracking improves dialog performance (Young23et al, 2009; Thomson and Young, 2010; Williams,2011), so characterizing and improving belief track-ing seems a logical next step.
Moreover, building anend-to-end dialog system is a daunting task, out ofreach of many research teams without specific fund-ing.
A corpus-based challenge has a much lowerbarrier to entry.3 Evaluation issuesThere are many (not one!)
metrics to evaluate.
Itis crucial to design these in advance and implementthem as computer programs for use during develop-ment.
Specific metrics could draw on the follow-ing core concepts.
Baseline accuracy measures thespeech recognition 1-best ?
i.e., accuracy withoutbelief tracking.
1-best accuracy measures how of-ten the belief tracker?s 1-best hypothesis is correct.Mean reciprocal rank measures the quality of theordering of the belief state, ignoring the probabili-ties used to order; log-likelihood measures the qual-ity of the probabilities.
ROC curves measure the1-best discrimination of the belief tracker at differ-ent false-accept rates, or at the equal error rate.An important question is at which turns to assessthe accuracy of the belief in a slot.
For example, ac-curacy could be measured at every turn; every turnafter a slot is first mentioned; only turns where a slotis mentioned; only turns where a slot appears in thespeech recognition result; and so on.
Depending onthe evaluation metric, it may be necessary to anno-tate dialogs for the user?s goal, which could be doneautomatically or manually.
Another issue is how toautomatically determine whether a belief state valueis correct at the semantic level.A final question is how to divide the corpus into atraining and test set in a way that measures robust-ness to the different conditions.
Perhaps some of thedata from the second round (which has not yet beenreleased) could be held back for evaluation.4 Next stepsThe next step is to form a group of interested re-searchers to work through the issues above, partic-ularly for the preparation of the corpus and evalu-ation methodology.
Once this is documented andagreed, code to perform the evaluation can be de-veloped, and additional labelling (if needed) can bestarted.AcknowledgmentsThanks to Antoine Raux for advocating for this chal-lenge task, and for helpful discussions.
Thanks alsoto Spoken Dialog Challenge organizers Alan Blackand Maxine Eskenazi.ReferencesAW W Black, S Burger, A Conkie, H Hastie, S Keizer,O Lemon, N Merigaud, G Parent, G Schubiner,B Thomson, JD Williams, K Yu, SJ Young, and M Es-kenazi.
2011.
Spoken dialog challenge 2010: Com-parison of live and control test results.
In Proc SIG-dial Workshop on Discourse and Dialogue, Portland,Oregon.D Bohus and AI Rudnicky.
2006.
A ?K hypotheses +other?
belief updating model.
In Proc AAAI Workshopon Statistical and Empirical Approaches for SpokenDialogue Systems, Boston.H Higashinaka, M Nakano, and K Aikawa.
2003.Corpus-based discourse understanding in spoken dia-logue systems.
In Proc ACL, Sapporo.N Mehta, R Gupta, A Raux, D Ramachandran, andS Krawczyk.
2010.
Probabilistic ontology trees forbelief tracking in dialog systems.
In Proc SIGdialWorkshop on Discourse and Dialogue, Tokyo, Japan.T Paek and E Horvitz.
2000.
Conversation as actionunder uncertainty.
In Proc Conf on Uncertainty in Ar-tificial Intelligence (UAI), Stanford, California, pages455?464.A Raux.
2011.
Informal meeting on a belief trackingchallenge at interspeech.B Thomson and SJ Young.
2010.
Bayesian updateof dialogue state: A POMDP framework for spokendialogue systems.
Computer Speech and Language,24(4):562?588.JD Williams and SJ Young.
2007.
Partially observableMarkov decision processes for spoken dialog systems.Computer Speech and Language, 21(2):393?422.JD Williams.
2010.
Incremental Partition Recom-biantion for Efficient Tracking of Multiple DialogueStates.
In ICASSP, Dallas, TX.JD Williams.
2011.
An empirical evaluation of a statis-tical dialog system in public use.
In Proc SIGDIAL,Portland, Oregon, USA.SJ Young, M Gas?ic?, S Keizer, F Mairesse, J Schatzmann,B Thomson, and K Yu.
2009.
The hidden informationstate model: a practical framework for POMDP-basedspoken dialogue management.
Computer Speech andLanguage.24
