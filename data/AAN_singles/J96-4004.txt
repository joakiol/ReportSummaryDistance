A Statistically Emergent Approachfor Language Processing: Application toModeling Context Effects in AmbiguousChinese Word Boundary PerceptionKok-Wee Gan+Hong Kong University of ScienceTechnologyKim-Teng Lua~National University of SingaporeandMartha Palmer+University of PennsylvaniaThis paper proposes that the process of language understanding can be modeled as a collectivephenomenon that emerges from a myriad of microscopic and diverse activities.
The process isanalogous to the crystallization process in chemistry.
The essential features of this model are:asynchronous parallelism; temperature-controlled randomness; and statistically emergent activesymbols.
A computer program that tests this model on the task of capturing the effect of contexton the perception of ambiguous word boundaries in Chinese sentences i presented.
The programadopts a holistic approach in which word identification forms an integral component of sentenceanalysis.
Various types of knowledge, from statistics to linguistics, are seamlessly integratedfor the tasks of word boundary disambiguation aswell as sentential nalysis.
Our experimentalresults howed that the model is able to address the word boundary ambiguity problems effectively.1.
IntroductionThis paper suggests that the language understanding process can be effectively mod-eled as the statistical outcome of a large number of independent activities occurringin parallel.
There is no global controller deciding which processes to run next.
All pro-cessing is done locally by many simple, independent agents that make their decisionsstochastically.
The system is self-organizing, with coherent behavior being a statisti-cally emergent property of the system as a whole.
The model, in a nutshell, simulateslanguage understanding as a crystallization process.
This process consists of a seriesof hierarchical, structure-building activities in which high-level inguistic structuresare formed from their constituents and get properly hooked up to each other as theprocess converges.The essential features of the model are:?
The process of sentence analysis is a series of computational ctivitiesthat determine how various constituents in a sentence can bemeaningfully related.?
Department of Computer Science, Hong Kong University of Science and Technology, Clear Water Bay,Kowloon, Hong Kongt Department of Computer Information Science, University of Pennsylvania, Philadelphia, PA 19104-6389:~ Department of Information Systems & Computer Science, National University of Singapore, LowerKent Ridge Road, Singapore 119260, Republic of Singapore?
1996 Association for Computational LinguisticsComputational Linguistics Volume 22, Number 4?
All computational activities are carried out by a large number ofprocedures known as codelets.?
A linguistic structure is not built by a single codelet.
Rather, it isconstructed by a sequence of codelets.
The execution of this sequence ofcodelets is interleaved with other codelets that are responsible forbuilding other structures.?
The order by which structures are built is not explicitly programmed,but is an emergent outcome of chains of codelets working in anasynchronous parallel mode.?
Computational ctivities are a combination of top-down and bottom-upactivities.?
Computational ctivities are indirectly guided by a semantic network oflinguistic oncepts, which ensures that these activities do not operateindependently of the system's representation f the context of a sentence.?
Decision making is stochastic, with the amount of randomness beingcontrolled by a parameter known as the computational temperature.We have applied our model to the task of capturing the effect of context on theperception of ambiguous word boundaries in Chinese sentences (Gan 1993).
Our ap-- proach differs from existing work on Chinese word segmentation (Liang 1983; Wang,Wang, and Bai 1991; Fan and Tsai 1988; Chang, Chen, and Chen 1991; Chiang et al1992; Sproat and Shih 1990; Wu and Su 1993; Lua and Gan 1994; Lai et al 1992; Sproatet al 1994; Sproat et al 1996) primarily in that our system performs entence inter-pretation, in addition to word boundary identification.
Our system figures out wherethe word boundaries of a sentence are by determining how various constituents ina sentence can be meaningfully related.
The relations the system builds represent i sinterpretation f the sentence.
In the initial stage of a run, the system constructs re-lations between characters of a sentence.
Through a spreading activation mechanism,the system gradually shifts to the construction of words and of relations betweenwords.
Later, the system progresses to identifying and constructing chunks (in otherwords, phrases), and to establishing connections between chunks.
Note that there isno top-level executive that decides the order of these activities.
At any given time, thesystem stochastically selects one action to execute.
Therefore, efforts toward buildingdifferent structures are interleaved, sometimes cooperating and sometimes competing.The system's high-level behavior, therefore, arises from its low-level stochastic actions.We will give a detailed escription of this application i  this paper.
In Section 2, weintroduce the problem of ambiguous Chinese word boundary perception, and follow,in Section 3, with a summary of the current practices in Chinese word identification.
Wedescribe our model in Section 4, showing a sample run of our program in Section 5 toillustrate the behavior of the model.
Finally, some discussions of the model are coveredin Section 6.
In Section 7, we compare our model with others, and explore areas forfuture research in Section 8.2.
Ambiguous Chinese Word Boundary PerceptionA written Chinese sentence consists of a series of evenly spaced Chinese characters.Each character corresponds to one syllable.
A word in Chinese can be made up ofa single character, such as OK f?m 'rice', or it can be a combination of two or more532Gan, Palmer, and Lua A Statistically Emergent Approachcharacters, uch as ~ shu~gu6 'fruit'.
It is possible that the component characters ofa word are fre@, such as ;~ shut and ~ gu6 of the word ;q<~ shu~gu6 'fruit', whichmean 'water' and 'fruit' respectively.
For any two Chinese characters in a sentence,denoted as x and y, if xy cannot be combined together to function as a word, a singleword boundary exists between these two characters.
If x and y can be constituents ofthe same word, yet at the same time may also be free, then word boundary ambiguityexists in these two characters.
If there is a unique word boundary before x and aftery, we refer to the ambiguity existing in xy as a combination ambiguity.
On the otherhand, if there is a word boundary ambiguity between the characters xy and the char-acter that precedes or follows them, say z, and these three characters can be groupedinto either xy z or x yz, then we say that an overlap ambiguity exists.
A sentencethat allows an ambiguous fragment to have multiple word boundaries will end upwith more than one interpretation.
This type of ambiguity is called global ambiguitywith respect o the sentence.
On the other hand, if only one way of segmenting theword boundary of an ambiguous fragment is allowed in a sentence, we call this localambiguity with respect o the sentence.
Global ambiguity can only be resolved withdiscourse knowledge.
An example for each category is shown in (1) to  (4).
2 Through-out this paper, we follow the guidelines on Chinese word segmentation adopted inChina.
3Overlap, Local Ambiguity(1)zh~ w~i zhiyudn gOngzu6 de yali hOn dath i s  CL  4 worker work STRUC 5 pressure very great'This worker faces great pressure in his work.
'The underlined fragment ~12~ yudn gOngzuD in (1) has overlap, local ambiguity.The middle character I gDng can combine with the previous character ~ yudn toform the word H I  yudngong 'worker', leaving the third character functioning as amonosyllabic word ~ zuD 'do'.
The middle character can also combine with the nextcharacter to form the word X2~ gongzuD 'work', leaving the first character alone.The sentence containing this fragment allows only one way of segmenting the wordboundary, which is shown in (1).
The character ~ yudn combines with the characterpreceding it, lI~ zhi, to form the bisyllabic word ~ zhiyudn 'worker', and the twocharacters 32 gong and ~ zuD form a word.Overlap, Global Ambiguity(2)a.w?Smen y~o xuesh~ng hu6 d6 y6u yiyiwe want student live CSC 6 have meaning'We want our students to have a meaningful life.
'1 A free character is one which can occur independently as a word (Li and Thompson 1981).2 The characters underl ined in sentences (1) to (4) are the locations of word boundary ambiguities wewould like to focus on.
This convention will be used throughout in this paper.3 See Contemporary Chinese Language Words Segmentation Standard Used for Information Processing, fifthedition, 1988, published in China.4 CL stands for a CLassifier.5 STRUC stands for the STRUCture word ~ de.6 CSC stands for the Complex Stative Construction word ~ de.533Computational Linguistics Volume 22, Number 4b.w{~men y?~o xuf sh~nghu6 d6 y6u yiyiwe want learn life CSC have meaning'We want to learn how to lead a meaningful life.
'The fragment _~e_~ xud shdng hu6 also has overlap ambiguity, where the middlecharacter can either combine with the first character to form a word, or combine withthe last character to form a word.
The sentence containing this fragment has twoplausible interpretations as shown in (2a) and (2b).
Both alternations: ~ ~ xudshdnghu6 'student live' (2a) and -~ ~41.~ xu~ sh~nghu6 'learn life' are acceptable.Combination, Local Ambiguity(3) ~)~ ~ ~I \ ]  -\]-~j~ ~nr de bidoqing shff~n hudj~you STRUC look very funny'You look very funny.
'In (3), the two characters in the fragment nt'~ shif~n can either function as two au-tonomous words q~ shi 'ten' and ~ f~n 'mark', or they can combine together to functionas a bisyllabic word f f '~ shif~n 'very'.
Given the sentential context of (3), however,only the second alternation is correct.Combination, Global Ambiguity(4)a.
~ ~ ~l~ ~__ \]~w6men dou h~n n~n gubwe all very hard live'We all have a hard life.
'b.wfimen dou h~n ngmgubwe all very sad'We all feel very sad.
'The fragment \]~\]~ ngmgub also has combination ambiguity.
It differs from (3) in that thesentence in which it appears has two plausible interpretations.
Hence, this fragmentcan either be segmented as ~l~ ndm 'hard' and i~ gub 'live' in (4a), or as ~ i~ n~ngub'sad' in (4b).Word boundary ambiguity is a very common phenomenon i written Chinese,due to the fact that a large number of words in modem Chinese are formed fromfree characters (Chao 1957).
The problem also exists in continuous speech recognitionresearch, where correct interpretation f word boundaries in an utterance requires lin-guistic and nonlinguistic information.
However, people have a fascinating ability tofluidly perceive groups of characters as words in one context but break these groupsapart in a different context.
This human capability highlights the fact that there is acontinual interaction between word identification and sentence interpretation.
We aretherefore motivated to study how our statistically emergent model can be used to sim-ulate the interactions between word identification and sentence analysis.
In particular,we want to study how the model (i) handles fragments with local ambiguities, uchas those in sentences (1) and (3), when they appear in different sentential contexts and(ii) handles fragments with global ambiguities, uch as those in sentences (2) and (4),when there is no discourse information.534Gan, Palmer, and Lua A Statistically Emergent Approach3.
Existing ApproachesTraditionally, word identification has been treated as a preprocessing issue, distinctfrom sentence analysis.
We will therefore only discuss current practices in word iden-tification, leaving sentence analysis aside.
Several techniques have been used in wordidentification, ranging from simple pattern matching, to statistical approaches, to rule-based methods.
The most popular pattern-matching method is based on the MaximumMatching heuristics, commonly known as the MM method (Liang 1983; Wang, Wang,and Bai 1991).
This method scans a sentence from left to right.
In each step, the longestmatched substring is selected as a word by dictionary look-up.
For example, in sen-tence (5),(5)fisu~nj~ de f~ming y~y~ zh6ngdacomputer STRUC invention implication profound'The invention of the computer has profound implications.
'the first three characters are identified as the word ~t~J~ fisu~nfi'computer' because itis the longest matched substring found in a word dictionary.
With the same reasoning,the words ~ de 'STRUC', ~ faming 'invention', ~ y~y~ 'implication', and ~zhbngd~ 'profound' are identified.Statistical techniques include the relaxation approach (Fan and Tsai 1988; Chang,Chen, and Chen 1991; Chiang et al 1992), the mutual information approach (Sproatand Shih 1990; Wu and Su 1993; Lua and Gan 1994), and the Markov model (Laiet al 1992).
These approaches make use of co-occurrence frequencies of charactersin a large corpus of written texts to achieve word segmentation without getting intodeep syntactic and semantic analysis.
For example, the relaxation approach uses theusage frequencies of words and the adjacency constraints among words to iterativelyderive the most plausible assignment of characters into word classes.
First, all possi-ble words in a sentence are identified and assigned initial probabilities based on theirusage frequency.
These probabilities are updated iteratively by employing the consis-tency constraints among neighboring words.
Impossible combinations are graduallyfiltered out, leading to the identification of the most likely combination.
The mutualinformation approach is similar to the relaxation approach in principle.
Here, mutualinformation is used to measure how strongly two characters are associated.
The mu-tual information score is derived from the ratio of the co-occurrence frequency of twocharacters to the frequency of each character.
In a sentence, the mutual informationscore for each pair of adjacent characters i  determined.
The pair having the highestscore is grouped together.
The sentence is split into two parts by the two charactersjust grouped.
The same procedure is applied to each part recursively.
Eventually, allword boundaries will be identified.Both the pattern-matching and the statistical approaches are simple and easy toimplement.
It is well known, however, that they perform poorly when presented withambiguous fragments that have alternate word boundaries in different sentential con-texts.
For instance, the fragment -~ shif~n, which is a bisyllabic word in sentence(3a), functions as two separate word, s in sentence (6).
(6)t~ zhr kao d~o shi fenhe only score ASP ten mark'He scores only ten marks.
'535Computational Linguistics Volume 22, Number 4The MM method will regard this fragment as a bisyllabic word nutj " shff~n 'very'regardless of the sentential context in (3a) and (6), since this word is longer than thelengths of the two monosyllabic words n u shf 'ten' and ~ f~n 'mark'.
As a result,this method fails to correctly identify the word boundaries in sentence (6).
Withinstatistical approaches, considering, for example, the mutual information method (Luaand Gan 1994), the same fragment is identified as a bisyllabic word in both sentences(3a) and (6) 7.By checking the structural relationships among words in a sentence, rule-basedapproaches aim to overcome limitations faced by pattern-matching and statistical ap-proaches.
However, many of the rules in existing rule-based systems (Huang 1989;Yao, Zheng, and Wu 1990; Yeh and Lee 1991; He, Xu, and Sun 1991; Chen and Liu1992) are either arbitrary and word-specific, or overly general.
For example,RuleGiven an ambiguous fragment xyz where x, z, xy, and yz are all possible words, ifx can be analyzed as a so-called direction word, segment he fragment as x yz, elsesegment i as xy z (Liang 1990).This syntactic rule works in sentence (7).
(7) ~ ~ -\[ v ~--~t~ ff~ xi?~ shenzihe bend down body'He bends down his body.
'The fragment T :~ xi?~ shen zi in sentence (7) is ambiguous.
As -F xi?~ 'down' is adirection word, the fragment is segmented as -~ ~:j~ xi?l sh@nzi 'down body', whichis as desired.Similarly, this rule will segment he fragment ~\]~lJ~ w?li gu6 r~n as ~ \[~l),, w?~igu6rdn 'out citizen', since ~ w?zi 'out' is also a direction word.
Therefore, when thisfragment appears in sentence (8a),(8)a.
~ ;~ P~I~J~ta sh~ w?~igu6r~nhe COPULA foreigner'He is a foreigner.
'the word boundaries identified will be:t4 sh~ w?d gu6r~nhe COPULA out citizenwhich is incorrect.Examples (7) and (8) illustrate that although syntactic information has been incor-porated in word segmentation, there are still errors.
In contrast, people are extremelyflexible in their perception of word boundaries of ambiguous fragments appearing indifferent sentential contexts.
We believe that the separation of word identification fromthe task of analysis accounts for the difference in performance.
This has motivated usto study how word identification and sentence analysis can be integrated.7 This result is reported inGan (1994).536Gan, Palmer, and Lua A Statistically Emergent Approach4.
The Statistically Emergent ModelThis model is inspired by the work done in the Fluid Analogies Research Group (Hof-stadter 1983; Meredith 1986; Mitchell 1990; French 1992).
There are four main compo-nents in this model.
Namely, (i) the conceptual network, which is a network of nodesand links representing some permanent linguistic oncepts; (ii) the workspace, whichis the working area in which high-level linguistic structures representing the system'scurrent understanding of a sentence are built and modified; (iii) thecoderack, which is a pool of structure-building a ents (codelets) waiting to run; and(iv) the computational temperature, which is an approximate measure of the amountof disorganization in the system's understanding of a sentence.4.1 The Conceptual NetworkThis is a network of nodes and links representing some permanent linguistic oncepts(Figure 1).In the network, anode represents a concept.
For example, the node labeled charac-ter represents he concept of character; the node word represents he concept of word;the node chunk represents he concept of chunk; the nodes character-l, character-2, upto character-n represent the actual characters in a sentence; the affix and affinity nodesrepresent the concepts of relations between characters; the nodes classifier, eflexive ad-jective, structure, etc., represent the concepts of relations between words; the nodesagent, patient, theme, etc., represent the concepts of relations between chunks.A link represents an association between two nodes.
There are four types of links:(i) category-of links, or is-a links, which connect instances to types, for example,the connections from character-I, character-2, up to character-n to the character node;(ii) has-instance links, the converse of category-of links; (iii) has-relation links, whichassociate a node with the relations it contributes, for example, the connection fromthe character node to the affix node represents that the character node contributes tothe character-based relation named as affix; (iv) part-of links, which represent part-ofrelations between two nodes.
The direction of a part-of link, for instance, the link fromthe character node to the word node, is interpreted as 'the character is part of the word'.During a run of the program, nodes become activated when perceived to be rele-vant, and decay when no longer perceived to be relevant.
Nodes also spread activationto their neighbors, and thus concepts closely associated with relevant concepts alsobecome relevant.
The activation levels of nodes can be affected by processes that takeplace in the workspace.
Several nodes in the network (e.g., agent, patient, word, chunk,etc.
), when activated, are able to exert top-down influences on the types of activitiesthat may occur in the workspace in subsequent processing.
The context-dependentactivation of nodes enables the system to dynamically decide what is relevant at agiven point in time, and influences what types of actions the system engages in.4.2 The WorkspaceThe workspace is meant to be the region where the system does the parsing andconstruction required to understand a sentence.
This area can be thought of as corre-sponding to the locus of the creation and modification of mental representations thatoccurs in the mind as one tries to form a coherent understanding of a sentence.
Theconstruction process is done by a large number of processing agents.Figure 2 shows an example of a possible state of the workspace when the systemis processing sentence (9).537Computational Linguistics Volume 22, Number 4* //q patientI predlicate f/---q theme \], ~,'- i J X\~-~ classifieri I i ~  , , ~  / re.~ \ I~ne l .
.
.a~j~c~w: \ / / / ,  structur~\[charact~r_l I ' \ .'..
~ _ _  : \ " f / __C  _, coo~.tion I re , ~ //___i character-2 l \  : /lexical marker ~,~-  -... -... complex \[I~\ , \4 ~ ,.\ ~ stative I \[character-3 I \ \  , \4 ~ \ \ constructionrt \ \  : ~ '~ '  X" "-4 judgmentI ?hara~ter-n l '-"t c.ara~t~r r', \ \\ ,, ,,~1 quantity Ia.ff~ty \ \ \ \  \ "~ i \\\\ d manner \[\\ \ \  ~ degree IL < nds:< r- has-instance & category-of link \ X. direction \]- - - ->  has-relation link \ demonstrative J. .
.
.
?
part-of link question\[Figure 1The conceptual network.
(9) ~ ~.X.
~ T' - -  ~ ~t~ b6nr~n sh~ng le s~n g~ hfiizishe self give birth ASP three CL child'She herself has given birth to three children.
'There are three types of objects that may exist in the workspace: character objects,word objects, and chunk objects.
The Chinese characters in Figure 2 not enclosed byrectangles, namely, the characters _~ s4n and ~I g~, are character objects.
When a fewChinese characters are enclosed by a rectangle, for example ~,/k b6nr~n, it indicatesthat these characters make up a word object.
The constituent characters of the wordstill exist in the workspace but they become less explicit in the figure.
If a groupof characters is enclosed by two rectangles, for example, the character ~1~ sh~ng, itindicates that a chunk object exists, made up of word objects.
In short, the immediateconstituents of a word object are character objects, and those of a chunk object are538Gan, Palmer, and Lua A Statistically Emergent Approachreflexive adjective aspectualaffinityI TaffixFigure 2A possible state of the workspace.Types of relationsNon-linguistic - -LinguisticFigure 3An overview of the types of relations.statisticalbetween character objectsbetween word objectsbetween chunk objectsword objects.
It is possible to have unitary constituency whereby one object is theonly part of another object.
The chunk object ~l~ sh~ng 'give birth' is an example.Each object in the workspace has a list of descriptions not shown in Figure 2.For example, descriptions of character objects include their morphological category(stem/affix) and whether they are bound or unbound.
8 Descriptions of word objectsinclude their categorial information and sense.
Descriptions of chunk objects may alsoinclude these two descriptions, except hat here, these two descriptions are derivedfrom the category and the sense of the word that is the governor.The directed arc connecting two objects in Figure 2 denotes a linguistic relationbetween the objects connected.
We adopt he dependency grammar notation (Tesni6re1959; Mel'~uk 1988) in which the object pointed to by an arrow is the dependent whilethe object where the arrow originates i  the governor.
The undirected arc connectingthe characters ~ hdi and ~ zi in Figure 2 represents a statistical relation, and statisticalrelations are undirected in our representation.An overview of our classification of relations is shown in Figure 3.A list of all types of relations is summarized in Table 1; a detailed exposition canbe found in Gan (1994).In Figure 2, the connection between the word objects ~ ta 'she' and ~.h.
b~nr~n'self' is a reflexive adjective relation, the connection between the word objects ~ sh~ng'give birth' and -j" le 'ASP' is an aspectual relation, and the two arcs connecting thecharacter objects ~ hdi and --~ zi are affix and affinity relations.8 A bound character cannot occur independently as aword.539Computational Linguistics Volume 22, Number 4Table 1A list of all types of relations.Object Type Relation Type ExampleObject 1 Object 2character affinity relation -~-character affix relation ~word classifier relation (~ 'CL' ~f~ 'snake'word reflexive adjective relation ~lJ~\] 'they' 2~\]" 'self'word structure relation i~1 'STRUC' .~J~ 'father'word coordination relation ~11 'and' ~\]~1~I 'Lisi'word adjective relation ~- 'blue' ~ 'sky'word complex stative relation ~ 'STRUC' ~ 'good'word attitude relation ~J~ 'really' ~ 'go'word disposal relation ~\] 'BA' \]~ 'door'word quantity relation ~J~\] 'we' ~ 'all'word manner relation ~ 'able' II~I 'sing'word degree relation ~\[~ 'very' ~\[~ 'nervous'word aspectual relation \]~ 'sleep' ~ 'ASP'word direction relation ~:-~ 'table' _\]~ 'on'word demonstrative r lation ~ 'this' ,,~, 'fish'word interrogative r lation ~-~ 'what' I~  'time'chunk agent relation ~\[~ 'he' ~\]'(6~ T 'broke'chunk patient relation \[~ 'door' ~*  'broke'chunk theme relation ~ 'chant' .~ 'scripture'chunk source relation ~ \[\] 'from China' \ [~  'return'chunk goal relation ~1\]\]~ ~\]~ 'to room' ~ 'get'chunk time relation @~ 'today' ~i~J\]~ 'not well'4.3 The CoderackThe building of linguistic structures (e.g., word and chunk objects, descriptions ofobjects, relations between objects) is carried out by a large number of agents knownas codelets.
These codelets reside in a data structure called the coderack.
A codelet isa piece of code that carries out some small, local task that is part of the process ofbuilding a linguistic structure.
For example, one codelet may check for the possibilityof building an aspectual relation between the words ~4~ sh~ng 'give birth' and -Tle'ASP' of sentence (9).
There are several codelet ypes.
Each type is responsible forbuilding one of the relations hown in Table 1.
In addition, there are word and chunkcodelet ypes, which are responsible for the construction of words and chunks.
Twospecial codelet ypes, namely, breaker and answer, will be explained in Section 5.
Here,we make a distinction between codelets and codelet ype.
The latter is a prewrittenpiece of code while the former are instances of the latter.In the initial stage when the program is presented with a sentence, the defaultcodelets initialized in the coderack are affix and affinity codelets.
They will constructrelations between character objects.
Some default bottom-up word codelets are alsoposted to determine whether monosyllabic words could be constructed from characterobjects.
When the word node in the conceptual network becomes activated by activationspreading from the character node, more top-down word codelets will be posted.
Whenword objects are constructed, nodes denoting relevant relations between words will beactivated.
These nodes in turn cause the posting of codelets that will build relationsbetween word objects.
Again, by activation spreading to the chunk node, codelets540Gan, Palmer, and Lua A Statistically Emergent Approachbuilding chunk objects will be posted, which will further lead to the posting of codeletsthat determine how chunk objects can be related.Note that there is no top-level executive deciding the order in which codeletsare executed.
At any given time, one of the existing codelets is selected to execute.The selection is a stochastic one, and it is a function of the relative urgencies of allexisting codelets.
The urgency of a codelet is a number assigned at the time of itscreation to represent the importance of the task that it is supposed to carry out (thisis an integer between 1 to 7, with 1 as the least urgent and 7 as the most urgent).Many codelets are independent and they run in parallel.
Therefore, efforts towardsbuilding different structures are interleaved, sometimes co-operating and sometimescompeting.
The rate at which a structure is built is a function of the urgencies of itsdedicated codelets.
More promising structures are explored at high speeds and othersat lower speeds.
Almost all codelets make one or more stochastic decisions, and thehigh-level behavior of the program arises from the combination of thousands of thesevery small choices.
In other words, the system's high-level behavior arises from itslow-level stochastic substrate.
To summarize, the macroscopic behavior of the systemis not preprogrammed; the details of how it emerges from the low-level stochasticarchitecture of the system are given in Sections 5.2 and 5.3.4.4 The Computational TemperatureThe computational temperature is an approximate measure of the amount of coherencyin the system's interpretation f a sentence: the value at a given time is a function ofthe amount and quality of linguistic structures that have been built in the workspace.The computational temperature is in turn used to control the amount c~f randomnessin the local action of codelets.
If many good linguistic structures have been built, thetemperature will be low, and the system will make decisions less randomly.
Whenfew good linguistic structures have been found, the temperature will be high, leadingto many more random decisions and hence to more diverse paths being explored bycodelets.
9The notion of temperature used here is similar to that in simulated annealing(Kirkpatrick, Gelatt, and Vecchi 1983).
Both start with a high temperature, allowing allsorts of random steps to be taken, and slowly cool the system down by lowering thetemperature.
However, the decrease in temperature in our system is not necessarilymonotonic.
It varies according to the amount of coherency in the system's interpreta-tion of a sentence.
Thus, our system has an extra degree of flexibility, which allowsuphill steps in temperature; in effect, this means that the system is annealing at themetalevel as well.5.
An ExampleWe will use a sample run of the program on sentence (9) to illustrate many centralfeatures of the model, including the selection of a codelet; the selection of competingalternatives; the interaction between the workspace and the conceptual network; etc.Note that this section would be overwhelmed with details if a step-by-step xplanationwere given.
A detailed trace of the system's execution on this sentence can be foundin Gan (1994), and a short description of the program's behavior can be found in Gan(1993).
Here, only selected snapshots are highlighted.Sentence (9) is an example with local, overlap, and combination ambiguities in the9 "Diverse paths" refers to different ways of analyzing the structure ofa sentence.541Computational Linguistics Volume 22, Number 4Table 2Initial state of the coderack.Codelet Type Urgency (U) Temperature-regulated Urgency QuantityUt = 100 Ut = 0word 2 2 16 14affinity 3 2 81 20affix 3 2 81 8fragment :~:,K~ b~n r~n sh~ng.
Without considering the sentential context, these threecharacters have three possible word boundaries: :~ d~ ~L b~n rfn sh~ng 'CL human givebirth', ~ ,~ ~ b~nr~n sh~ng 'self give birth' or ~ ,~e~_ b~n r~nsh~ng 'CL life'.
Given thesentential context of (9), however, only the second alternative is correct.5.1 Initial SetupWhen the parsing process starts, the program is presented with the sentence.
Thetemperature is clamped at 100 for the first 80 cycles to ensure that diverse pathsare explored initially (the range of the temperature varies between 0 and 100).
Acycle is the execution of one codelet.
The number 80 is decided based on intuitionand trial-and-error; it is not necessarily optimal.
The workspace is initialized withnine character objects, each corresponding to a character of the sentence.
Since theworkspace contains only character objects, the only relevant concepts are: character,affinity, affix, and each character of the sentence.
The corresponding nodes in theconceptual network, namely: character, affinity, affix, ~ ta, ~ b~n, up to ~ zi, are setto full activation.
Fourteen instances of word codelet are posted to the coderack.
Theyare responsible for identifying and constructing monosyllabic words.
Twenty instancesof affinity codelet are also posted to identify and construct affinity relations betweencharacters.
Eight instances of affix codelet are posted to identify and construct affixrelations between characters.
In general, the number of codelets posted is a functionof the length of a sentence.5.2 Selection of a CodeletAmong all codelet instances that exist in the coderack, only one of them is stochas-tically selected to execute ach time.
The choice of which codelet instance to executedepends on three factors: (i) its urgency, (ii) the number of codelet instances in thecoderack that are of the same type as the individual instance, and (iii) the currenttemperature.
At cycle 0, the coderack contains the statistics as shown in Table 2.The temperature-regulated urgency (Lit) is derived in the following way:Ut = U ~120-t)/30 (1)where t denotes the temperature, which ranges between \[0,1001.
This equation is usedto magnify differences in urgency values when the temperature is low.
Conversely, athigh temperatures, it will minimize differences in urgency values.
The idea is to letthe system explore diverse paths when the temperature is high, while always stick toone search path when the temperature is low.At cycle 0 where the temperature is 100, the temperature-regulated urgencies ofthe three codelet ypes are the same.
The probability of selecting an instance of a wordcodelet, an affinity codelet, and an affix codelet is 33.3%, 47.6%, and 19.1% respectively.542Gan, Palmer, and Lua A Statistically Emergent ApproachI IaffinityFigure 4State of the workspace at cycle 17.These probabilities are derived as follows:Uj, t x Qj (2) P'(G)= u  i=1( i,t X Qi)where Qi and Qj are the quantities of codelet ypes Ci and Cj respectively, Ui, t and Uj, tare the urgencies of codelet ypes Ci and Cj at temperature t respectively, and n is thetotal number of codelet ypes.Supposing that the coderack contains the same types of codelets with the samequantities, but the temperature is 0, the probability of selecting an instance of a wordcodelet, an affinity codelet, and an affix codelet becomes 8.99%, 65.01%, and 26.00%respectively.
Therefore, at low temperatures, codelets with high urgency are preferred.5.3 Construction of Linguistic StructuresLinguistic structures include high-level objects (words and chunks) and relations be-tween two objects (see Table 1).
In this run, for example, an affinity relation betweenthe character objects ~: b~n and ),, rdn is constructed by an instance of an affinitycodelet at cycle 17 (Figure 4).An affinity codelet works on any two adjacent character objects to evaluate whetheran affinity relation should be built between these two characters.
The affinity relationis a quantitative measure that reflects how strongly two characters co-occur statis-tically.
It is derived from mutual information (Fano 1961), which is the probabilitythat two characters occur together versus the probability that they are independent.Mathematically, it is:P(a,b)A(a, b) = log 2 P(a)P(b) (3)where A(a, b) is the affinity relation between the character objects a and b, P(a, b) isthe probability that the two character objects co-occur consecutively, P(a) and P(b) arethe probabilities that a and b occur independently.
To derive affinity relations betweencharacters, we have the usage frequencies of 6,768 Chinese characters specified in theGB2312-80 standard, and the usage frequencies of46,520 words derived from a corpus.The total usage frequency of these words is 13,019,814.
(The data was obtained fromLiang Nanyuan, Beijing University of Aeronautics and Astronautics.
)Note that efforts towards building different structures are interleaved, as manycodelets are independent and they run in parallel.
Apart from the initial set of codeletspresent at the onset of processing, new codelets are sometimes created by old codeletsto continue working on a task in progress, and these codelets may in turn create other543Computational Linguistics Volume 22, Number 4codelets, and so on.
The cycle in which a structure is built is not preprogrammed.Rather, it emerges from the statistics of the interaction of all codelets in the coderack.5.4 Selection of Competing StructuresIt may happen that a structure being constructed is in conflict with an existing struc-ture.
In this run, for example, an affinity relation between the characters .?.
r~n andshgng is being considered at cycle 79.
This structure is in conflict with the previouslyconstructed affinity relation between the characters dg b~n and .),.
r~n.
The decisionabout which competing structure should win is decided stochastically as a functionof two factors: (i) the strengths of the competing structures, and (ii) the temperature.The strength of a structure is an approximate measure of how promising the structureis.
It is an integer ranging between 0 and 100, inclusive.
The strengths of differentstructures are derived according to either linguistic knowledge ncoded in the lexiconor certain statistical measures.
Equation (3) is a key factor in deriving the strength ofan affinity relation.
In this run, the strength of the proposed affinity relation betweenthe characters .?.
r~n and ~ sh~ng is 55, while that of the existing affinity relationbetween the characters ~ b~n and ),.
r~n is 56.
These two values are adjusted by thetemperature according to equation (4).St ~ S (120-t)/40 (4)where St is the temperature-regulated strength, S is the original strength, and t isthe temperature.
The effect of equation (4) is similar to equation (1): to maximizedifferences in strength values at low temperatures, and to minimize differences athigh temperatures.
At cycle 79, the temperature is still clamped at 100, and hence thetemperature-regulated strengths of these two competing structures are both 7 (roundedup to the nearest integer).
The decision about which structure should win is thereforea random one, as both have an equal probability of success.
According to equation (4),at low temperatures, it is increasingly difficult for a new structure of lesser strength towin in competition against existing structures of greater strength.
Since the system'sbehavior is more random at high temperatures, it is able to explore diverse paths inthe initial stage when little structure has been built.
When a large number of structuresdeemed to be good have been found, which entails a low temperature, the system willproceed in a more deterministic fashion, always preferring good paths to bad ones.Indeed, in this case, the new affinity relation between the characters .?.
r~n andshgng has won.
Instead of destroying the affinity relation between the charactersb~n and ),.
r~n, this structure is retained, but it becomes dormant in the workspace.5.5 The interaction between the Workspace and the Conceptual NetworkActivated nodes in the conceptual network spread activation to their neighbors, andthus concepts closely related to relevant concepts also become relevant.
In this run,for example, the nodes word and chunk become activated at cycle 80 due to activationspreading from the character node.
Activated nodes influence what tasks the systemwill focus on subsequently through the posting of top-down codelets.
For example, atcycle 80, the activated word node causes the proportion of word codelets to increaseto 93%.
This is an important feature of the system: the context-dependent ac ivationof nodes, which enables the system to dynamically decide what is relevant at a givenpoint in time, and influences what actions to take through the posting of top-downcodelets.544Gan, Palmer, and Lua A Statistically Emergent Approachlaffinitdormant affinity affinity affixFigure 5State of the workspace at cycle 180.5.6 Detection and Resolution of Erroneous StructuresBy the end of cycle 180, the following structures have been built (Figure 5):activesh~ng,~- zi;activeactiverelations: an affinity relation between the characters ,K, r~n and~dd hdi and -~ zi, an affix relation between the characters ~ hdi andword objects: ~zj~ hdizi 'child', ),.~4~ r~nsh~ng 'life', and :~ b~n 'CL';chunk objects: ,K.~_ r~nsh~ng "life', and ~:j~ hdizi 'child';dormant relations: an affinity relation between the characters ~: b~n andr~n.Among them, the word ~ b~n 'CL' is a classifier.
This word has activated theclassifier node in the conceptual network, which in turn causes the posting of classifiercodelets to the coderack.
The responsibility of this type of codelet is to explore thepossibility of establishing a classifier elation between a classifier and an object name.
1?The use of a classifier is in general idiosyncratic.
This type of idiosyncrasy is encodedin the lexicon.
Since ~ b~n cannot be the classifier of the object name ,K.~ r~nsh~ng'life', a special type of codelet known as a breaker codelet is posted to the coderack.The role of a breaker is to identify erroneous linguistic structures, and set them todormant, restoring any dormant competing structure when necessary.At cycle 187, a breaker codelet is executed that examines tructures that are "in-trouble", namely, the words :~ b~n and ),,~4~ r~nsh~ng 'life'.
Since the componentcharacters of the second word can be free, the breaker codelet concludes that this isan erroneous grouping.
The word yk.~4~ r~nsh~ng 'life' is made dormant.
The otherstructures that support he word ,K.~ r~nsh~ng 'life', namely the affinity relation be-tween the characters ,K. r~n and ~ sh~ng and the chunk ,~.~-~ r~nsh~ng 'life', are alsomade dormant.
The competing alternative, the affinity relation between the charactersb~n and ),, rdn, is reactivated.
This snapshot also illustrates an important featureof the system: syntactic analysis can be performed without waiting for the system tocomplete the task of word identification.10 The term object name is borrowed from Meaning-Text linguistics (Mel'~uk 1988).
It refers to words thatcannot have a semantic dependent.
A more formal attempt to define this term can be found inPolgu6re (to appear).545Computational Linguistics Volume 22, Number 4affinityreflexive adjective aspectualaffinivaffixII!IquantityclassifierFigure 6State of the workspace at cycle 373.5.7 The Final StateFigure 6 shows the state of the workspace at the end of cycle 373.For easy reference, sentence (9) is repeated here:(9) ~ :~.K.
~ T G ~I ~x-~ta b~nr~n sh~ng le san g~ h~izishe self give birth ASP three CL child'She herself has given birth to three children.
'The list of structures built are:?
active relations: an affinity relation between the characters ~ b~n and .~r~n, ~ h~i and ~ zi, an affix relation between the characters ~ h~i andzi, a reflexive adjective relation between the words ~ ta 'she' and;4;.~ b~nr~n 'self', a classifier elation between the words ~ g~ 'CU and~ h~izi 'child', a quantity relation between the words ~ san 'three'and ~:~ h~izi 'child', an aspectual relation between the words ~ sh~ng'give birth' and ~ le 'ASP';?
active words: ~ ta 'she', :~:.J~ b~nr~n 'self', ~ sh~ng 'give birth', T le'ASP', -~ san 'three', ~l g~ 'CU, and ~ h~izi 'child';?
active chunks: ~:4:.~ ta b~nr~n 'she herself', P4~ sh~ng 'give birth', and~--.
{~l~x~ san g~ h~izi 'three CL children';?
dormant relations: an affinity relation between the characters K r~n andsh~ng;?
dormant words: d~-I~ r~nsh~ng 'life';?
dormant chunks: ) ,~  r~nsh~ng 'life'.Comparing the above structures with the complete analysis of the sentence inFigure 7 (for simplicity, we have omitted relations between characters in Figure 7),it is observed that the system has not yet constructed the agent and theme relations.They were not identified because the system has come to a stop at cycle 381, afteran instance of answer codelet was executed.
This type of codelet reports on the word546Gan, Palmer, and Lua A Statistically Emergent Approachagent themereflexive adjective aspect4classifierquantityFigure 7A complete analysis of sentence (9).40O3OO2oo1000|affray afk~?
t!II ??
?
?word chtmk refl~ adj.
clas~fier quantity aspecttlalFigure 8A graph of structures constructed against cycle number.boundaries of a sentence.
The system currently adopts a greedy approach and startsposting large numbers of this type of codelet as soon as it has identified a plausibleinterpretation ofthe word boundaries of a sentence.
Hence, although instances of agentand theme codelets were present in the coderack, they were being overwhelmed bythe ubiquitous answer codelets.Figure 8 summarizes the cycle number in which various types of structures wereconstructed during this run.
In this figure we see that affinity relations are built earlierthan words, reflecting the system's preference for words of greater lengths.
The systemmakes use of statistical information (the mutual information scores) to make quick andreliable guesses of the locations of these words.
It can also be observed that overall,there is a gradual shift in the types of operations executed, from being character-centered initially, to word-centered, and then to chunk-centered.
From time to time,however, the construction of different ypes of structures i  interleaved.6.
System Performance and DiscussionsThirty ambiguous fragments that have alternating word boundaries in different sen-tential contexts were presented to the system and the system was able to resolve allthe ambiguities.
The test set covers the four types of word boundary ambiguities de-547Computational Linguistics Volume 22, Number 4scribed in Section 2.
When the sentential contexts of locally ambiguous fragments (boththe overlap and combination type) were varied, our system was able to identify thecorrect word boundaries.
When the system was presented with sentences with globalambiguities, it produced all the plausible alternative word boundaries.
However, atany run of such a sentence, only one alternative is generated.
The system's behavior issimilar to human performance in the goblet/faces recognition problem in perception(Hoffman and Richards 1984).
We cannot see both the goblet and the faces at the sametime, but we are able to switch back and forth between these two interpretations.The frequencies of generating all the alternatives vary from one sentence to another.It is important o note that such frequencies are not meant to indicate some kind of"goodness" measure of alternative word boundary interpretations.
Neither are theymeant o reflect the preferences of a human.
They are merely a reflection of the usagefrequencies of Chinese characters and words in our dictionary.The system's ability to generate different word boundaries for a globally ambigu-ous sentence arises from its stochastic search mechanism, which does not rule outa priori certain possibilities.
This feature enables the system to occasionally discoverless-obvious interpretations of word boundaries.
For example, in addition to the twoapparent ways of aligning the fragment ~,~ yTjrnggu6 as either ~,~ i~ yfjrnggu~'already over' or B ,~  yfjrnggu6 'already go through' in sentences (10a) and (10b),a less-obvious possibility that the system has identified is: ~, ,~ i~ y~jTnggu~ 'alreadyexperience over', where i~ gu6 'over' is the complement of .~ jTng 'experience'.
(10)a.w~ y~j~ng gu~ le xu~sh~ng shfd?liI already over ASP student period'My student days are over.
'b.w~ yr jTnggu6 le xu~sh~ng shid?ziI already go through ASP student period'I have already gone through the period as a student.
'C.w6 y~ j~ng gu?~ le xu~sheng shid?liI already experience over ASP student period'I have already experienced student life.
'The system rarely produces the less-obvious interpretations.
This demonstrates thatits mechanisms are able to strike an effective balance between random search anddeterministic search, imbuing it with both flexibility and robustness.An issue that arises from the nondeterministic feature of the system is: will theword boundaries of a locally ambiguous entence vary at different runs?
To addressthis, weran the program with each sentence 20 times.
We found that for sentences cov-ered by our current set of linguistic descriptions, the system arrived at the same wordboundaries despite different paths being taken at each run.
For linguistic phenom-ena not yet covered, suboptimal solutions may sometimes be generated.
For example,when the program worked on sentence (10), it produced sentence (11) once as theanswer.548Gan, Palmer, and Lua A Statistically Emergent Approach(11)zhonggu6 yz kdif~ h~ sh?mg w~iChina already exploit and yet notkaifa de z~yu~n dOu h~n duoexploit STRUC resource all very many'China has many resources which have either been exploitedor not yet been exploited.
'(12)*zhdnggu6 yr kaifa hd sh?mg w~iChina already exploit and yet notkai f~ de ziyu~n dOu h6n3open distribute STRUC resource all veryduomanyIn this run, the bisyllabic word ~_~ /a//f~ 'develop' has been wrongly identified astwo monosyllabic words ~'\] /a/i 'open' and ~ ft/'distribute'.
To determine the properuse of two juxtaposed predicates, uch as ~J kai 'open' and ~ fa 'distribute' in this case,requires a careful study of serial verb constructions.
It is inevitable that the systemwould make such a mistake as our linguistic descriptions have not yet covered thisphenomenon.In comparison, consider the performance of a strictly statistical approach basedon mutual information (Lua and Gan 1994): the latter wrongly identified the wordboundaries in 11 out of the 30 ambiguous fragments.
For the 6 fragments that appearin globally ambiguous entences, the mutual information approach gave only oneinterpretation f the word boundaries.
In terms of processing speed, the mutual infor-mation approach took an average of 110.4 ms to process one character; our approachtook 1.7 s. 11 The extra time in our approach is spent in parsing sentences.7.
ConclusionIn this paper, we reported on a stochastically emergent model for language processingand described its application to the modeling of context effects in ambiguous Chineseword boundary interpretation.
The model simulates language processing as a collectivephenomenon that emerges from a myriad of microscopic and diverse activities.
Theproposed mechanism, whereby word objects and chunk objects are formed by thehooking up of character objects as the latter are gradually cooled down, is analagousto the crystallization process in chemistry.Our application is distinct from existing work in two main respects:Word identification: We show that the full power of natural languageprocessing can be brought o bear on the issue of word identificationeffectively and seamlessly.
The model is able to resolve ambiguitiesappearing in different sentential contexts.
This is an improvement overstatistical pproaches such as the relaxation method (Fan and Tsai 1988),which generates all possible ways of grouping the characters of asentence into words, and then uses some scoring function to select he11 The mutual information approach was written in Borland C, version 2.0 while the new approach waswritten in Borland C++, version 3.0.
Both ran on a 33 MHz, 386 machine.549Computational Linguistics Volume 22, Number 4best combination.
At the same time, this model eliminates the use of adhoc rules, as syntactic and semantic analysis are interleaved with wordidentification.
This application is diametrically opposed to thereductionist approach of separating word segmentation a d sentenceanalysis into two distinct stages.
We have argued that our approach canavoid the computational problem of combinatorial explosion as thearchitecture has appropriate mechanisms to regulate run-time resourcesdynamically.Sentence analysis: We show that a sentence can be analyzed withoutassuming a presegmented input.
The main feature is that there is nofixed, predetermined order of morphological, syntactic, and semanticanalysis, since the control mechanism is a nondeterministic one.Essentially, the order in which these analyses are carried out isdependent on what has been discovered so far by the system, and thesystem's perception of what is relevant to the task it is currentlyinvestigating.The essential idea of the proposed model is that of stochastically guided conver-gence to what is called a globally optimum state.
This model shares ome features withAPRIL (Annealing Parser for Realistic Input Language) (Sampson, Haigh, and Atwell1989).
APRIL uses simulated annealing to determine the most plausible parse tree ofa sentence.
It begins with an arbitrary tree.
Many local modifications are generatedrandomly.
They are either adopted or rejected according to their effect on a plausibil-ity measure.
Modifications that improve the plausibility measure are always accepted;while unfavorable modifications are rejected only if the loss of merit exceeds a certainthreshold.
The threshold value is generated randomly but its mean value decreasesaccording to some predefined schedule.
This differs from the behavior of the compu-tational temperature in our system, which does not have a monotonically decreasingproperty.
Our system further differs from APRIL in the following aspects: (i) APRILbegins with an arbitrary parse tree whereas our system begins with no parse structure;(ii) APRIUs plausibility measure is defined using statistics collected from a treebankof manually parsed English text while ours is derived from mutual information statis-tics and linguistic constraints; (iii) parse trees in APRIL are immediate-constituencytype while ours are dependency-based.
That is, nodes in our system are either char-acters, words, or chunks.
There are no nonterminal nodes defined with grammaticalcategories.Our model also shares some features with connectionist models, such as fine-grained parallelism, local actions, competition, spreading activation, and statisticallyemergent effects from a large number of small, subcognitive vents.
On the otherhand, the representation f concepts is quite different: they are encoded as atomic,symbolic primitives instead of distributed as weighted connections between odes ina network, which is common in connectionist systems.
Therefore, in terms of the degreeto which concepts are distributed, our representation has a strong symbolic flavor; interms of the extent o which high-level behavior emerges from lower-level processes,ours has a strong subsymbolic orientation.
By providing an account of the languageunderstanding process at such an intermediate level of description, it is hoped that ourresults will provide a guide to connectionists studying how such intermediate-levelstructures can emerge from neurons or cell-assemblies in the brain.550Gan, Palmer, and Lua A Statistically Emergent Approach8.
Future WorkOur application, which handles only thirty sentences at present, has enabled us tofocus on the mechanisms that underlie the process of sentence comprehension, andtheir interactions.
With the progress made in this study, which would not have beenpossible if we had plunged straight into large-scale unrestricted texts, our next concernwould be to address the issue of scalability.
There are two aspects to this issue.The effect of various parameter values chosen for the formulae shown inSection 5 on the operation of the program: These values are set bytrial-and-error.
They are not specifically tailored to our test set.
To finessethese parameters in order to completely weed out unpromising searchpaths is impossible, since decision making in the system is stochastic.
Wetherefore do not anticipate that the setting of the various parametervalues is an issue during scaling up.
The values of the parameters mayaffect he rate of convergence, but they will not affect he accuracy of thesystem in terms of the analysis results.The possibility of generating thousands of codelets as a result of using alarge lexicon: We do not expect such a scenario to occur.
Instead, havinga large lexicon means that the system is able to handle more sentences.The number of codelets pawned to process a sentence is determined bythe number of characters and words in the sentence, and the types ofwords and chunks in the sentence, not by the size of the lexicon.
Inaddition, there are built-in mechanisms to manage the growth ofcodelets.
We have demonstrated in Section 5 how we have made use ofstatistics (the maximum matching heuristics and mutual information) toavoid generating all possible word boundary combinations.
The samplerun in Section 5 has also demonstrated that the program need not finishexecuting all codelets in the coderack before it is allowed to stop, andthat simpler and more clear-cut decisions tend to be made before themore subtle ones.
Furthermore, certain features of the system, namely,the stochastic selection of a codelet by relative urgencies, the use of theconceptual network as a top-down controller, the interactions betweenthe conceptual network and the workspace, enable the system todynamically decide on the number and the types of codelets to begenerated.The real bottleneck when scaling-up is the acquisition of linguistic descriptions,as our current work has limited breadth and depth of coverage.
Therefore, the cur-rent system has less practical value to people working on the word segmentationproblem, where the main concern is to develop algorithms that work for large-scaletext.
However, the proposed model provides a useful architecture for us to study theroot of what people do when they encounter unknown words in text.
This issue ofunknown-word resolution has been the single major problem in the segmentation funrestricted text.
Understanding how higherqevel knowledge is brought o bear onthis issue is essential to the design of an effective solution.
Hence, our next goal is toapply the model to handle the unknown-word problem, including treatments of un-known compounds such as personal names, previously unseen place names, foreignnames in transliteration, and company names.551Computational Linguistics Volume 22, Number 4AcknowledgmentsThroughout the course of this work, wehave benefited from discussions with AlainPolgu~re, Melanie Mitchell, Robert French,Ngai-lai Cheng, Chew Lim Tan, Loke SooHsu, Gee Kim Yeo, Guojin, Zhibiao Wu, andPaul Wu.
We would like to express ourthanks to them.
We are also grateful to thereviewers for their insightful comments andsuggestions.ReferencesChang, Jyun-Sheng, C. D. Chen; and S. D.Chen.
1991.
Chinese word segmentationthrough constraint satisfaction andstatistical optimization (in Chinese).
InProceedings o/ROCLING-IV, R.O.C.Computational Linguistics Conference,pages 147-165.Chao, Yuen-Ren.
1957.
Formal and semanticdiscrepancies between different levels ofChinese structure.
Bulletin o/The Instituteof History and Philosophy, XXVIII: 1-16.Chen, Keh-Jiann and Shing-Huan Liu.
1992.Word identification for Mandarin Chinesesentences.
In Proceedings ofCOLING-92,pages 101-107.Chiang, Tung-Hui, Jing-Shin Chang,Ming-Yu Lin, and Keh-Yih Su.
1992.Statistical models for word segmentationand unknown resolution.
In Proceedings ofROCLING V, R.O.C.
ComputationalLinguistics Conference, pages 121-146.Fan, Charng-Kang and Wen-Hsiang Tsai.1988.
Automatic word identification iChinese sentences by the relaxationtechnique.
Computer Processing of Chineseand Oriental Languages, 4(1): 33-56.Fano, Robert M. 1961.
Transmission fInformation.
MIT Press, Cambridge, MA.French, Robert M. 1992.
Tabletop: AnEmergent, Stochastic Computer Model ofAnalogy-Making.
Ph.D. thesis, Universityof Michigan.Gan, Kok-Wee.
1993.
Integrating wordboundary identification with sentenceunderstanding.
In Proceedings ofthe 31stAnnual Meeting of the Association forComputational Linguistics, pages 301-303.Ohio State University, June.Gan, Kok-Wee.
1994.
Integrating WordBoundary Disambiguation with SentenceUnderstanding.
Ph.D. thesis, Departmentof Information Systems & ComputerScience, National University of Singapore.He, Ke-Kang, Hui Xu, and Bo Sun.
1991.Design principle of expert system forautomatic word segmentation i  writtenChinese (in Chinese).
Journal of ChineseInformation Processing, 5(2): 1-14.Hoffman, Donald D. and Whitman A.Richards.
1984.
Parts of recognition.Cognition, 18: 65-96.Hofstadter, Douglas R. 1983.
Thearchitecture of JUMBO.
In Proceedings ofthe International Machine Learning Workshop,edited by Ryszard Michalski.Huang, Xiang-Xi.
1989.
A produce-testapproach to automatic segmentation fwritten Chinese (in Chinese).
Journal ofChinese Information Processing, 3(4): 42-48.Kirkpatrick, S., C. D. Gelatt Jr., andM.
P. Vecchi.
1983.
Optimization bysimulated annealing.
Science, 220: 671-680.Lai, T. B. Y., S. C. Lun, C. F. Sun, and M. S.Sun.
1992.
A tagging-based first-orderMarkov model approach to automaticword identification for Chinese sentences.In Proceedings ofthe 1992 InternationalConference on Computer Processing of Chinese& Oriental Languages, pages 17-23.Li, Charles N. and Sandra A. Thompson.1981.
Mandarin Chinese: A FunctionalReference Grammar.
University ofCalifornia Press.Liang, Nan-Yuan.
1983.
Automatic wordsegmentation in written Chinese and anautomatic word segmentationsystem----CDWS (in Chinese).
InProceedings ofthe National Chinese LanguageProcessing System.Liang, Nan-Yuan.
1990.
The knowledge ofChinese words segmentation (in Chinese).Journal of Chinese Information Processing,4(2): 29-33.Lua, Kim-Teng and Kok-Wee Gan.
1994.
Anapplication of information theory inChinese word segmentation.
ComputerProcessing of Chinese & Oriental Languages,8(1): 115-123.Mel'~uk, Igor A.
1988.
Dependency S ntax:Theory And Practice.
State University Pressof New York.Meredith, Marsha J.
1986.
Seek-Whence: Amodel of pattern perception.
TechnicalReport 214, Computer ScienceDepartment, Indiana University,Bloomington, IN.Mitchell, Melanie.
1990.
Copycat: A ComputerModel of High-Level Perception andConceptual S ippage in Analogy-Making.Ph.D.
thesis, University of Michigan.Polgu~re, Alain.
To appear.
Meaning-textsemantic networks as a formal language.In Current Issues In Meaning-TextLinguistics, edited by Leo Wanner.552Gan, Palmer, and Lua A Statistically Emergent ApproachSampson, Geoffrey, Robin Haigh, and EricAtwell.
1989.
Natural anguage analysisby stochastic optimization: A progressreport on project APRIL.
Journal ofExperimental nd Theoretical Arti~'cialIntelligence, 1(4): 271-287.Sproat, Richard and Chilin Shih.
1990.
Astatistical method for finding wordboundaries in Chinese text.
ComputerProcessing of Chinese & Oriental Languages,4(4): 336-351.Sproat, Richard, Chilin Shih, William Gale,and Nancy Chang.
1994.
A stochasticfinite-state word-segmentation algorithmfor Chinese.
In Proceedings of the 32ndAnnual Meeting of the Association forComputational Linguistics, pages 66-73.Sproat, Richard, Chilin Shih, William Gale,and Nancy Chang.
1996.
A stochasticfinite-state word-segmentation algorithmfor Chinese.
Computational Linguistics,22(3).Tesni~re, Lucien.
1959.
Eldments de la syntaxestructurale.
Klincksieck, Paris.Wang, Xiaog-Long, Kai-Zhu Wang, andXiao-Hua Bai.
1991.
Separating syllablesand characters into words in naturallanguage understanding (in Chinese).Journal of Chinese Information Processing,5(3): 48-58.Wu, Ming-Wen and Keh-Yih Su.
1993.Corpus-based automatic compoundextraction with mutual information andrelative frequency count.
In Proceedings ofR.O.C.
Computational Linguistics ConferenceVI, pages 207-216.Yao, Tian-Shun Gui-Ping Zhang, andYing-Ming Wu.
1990.
A rule-basedChinese automatic segmenting system (inChinese).
Journal of Chinese InformationProcessing, 4(1): 37-43.Yeh, Ching-Long and Hsi-Jian Lee.
1991.Rule-based word identification forMandarin Chinesesentences--A unification approach.Computer Processing of Chinese & OrientalLanguages, 5(2): 97-118.553
