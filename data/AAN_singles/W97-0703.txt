Using Lexical Chains for Text SummarizationReg ina  Barz i layMathematics and Computer S~nence DeptBen Gunon University m the NegevBeer-Sheva, 84105 Israelregana@cs.bEu ac.
~1Michae l  E lhadadMathemat~s and Computer Saence DeptBen Gunon Umveraty m the NegevBeer-Sheva, 84105 Israelhttp //mr?
cs.bgu ac.xl /  elhadadAbst rac tWe investigate one techmque to  produce a summaryof an original text without requmng zts full seman-ttc interpretation, but instead relying on a model ofthe topic progresston m the text derived from lex-lcal chains We present a new algonthm to com-pute lexlcal chains m a text, merging several robustknowledge sources the WordNet thesaurus, a part-of-speech tagger and shallow parser for the identifi-cation of nominal groups, and a segmentatton algo-r ithm dernved from (Hearst, 1994) Summarizationproceeds m three steps the ongmal text is first seg-mented, lexxcal chmns are constructed, strong chainsare ldsnhfied and ssgnzflcant sentences are extractedfrom the  text We present m tins paper empiricalresults on the tdent~catlon of strong chains and ofslgmfieant sentencesIn t roduct ionSummarization ts the process of condensing a sourcetext into a shorter Version preserving its reformationcontent It can serve several goals - -  from surveyanalysis of a sctenttfic field to qmck mchcatzve noteson the general toplc of a text Producing a quahtyreformative summary of an arbitrary text remamsa challenge winch reqmres full understanding of thetext Indtcattves, lm~artes, winch can be used toqmckly decide whether a text is worth reading, arenaturally easter to produce In tins paper we investi-gate a method for the production of such mdxcatlvesummaries from arintrary text(Jones, 1993) descnbes ummarization as a two-step process (1) Building from the source text asource representatton, (2) Summary generat ion -fonmng summary representation from the sourcerepresentation bmlt m the first step and synthesismgthe output summary textWithin this framework, the relevantquestion iswhat reformation has to be included m the sourcerepresentation m order to create a summary Thereare three types of source text reformation hngms-tlc, domain and commumcatlve Each of these textaspects can be chosen as a barns for source represen-tatlonSummaries can be bmlt on a deep semantic anal=ysis of the source text For example, (McKcown andRadsv, !905)investigate ways to produce a coher-ent summary of several texts describing the sameevent, when a detaded semantic representation fthe source texts m available (m their case, they useMUC-style systems to interpret he source texts)Alternatzvely, early summarisatzonsystems (Luhn, 1968) used only hngumtlc source m-formation The mtmtlon was that the moat frequentwords represent the tmportant concepts of the textIn this approach the source representation was thefrequency table of text words Tins representationabstracts the text into the umon of its words w~thoutconmdermg any connectlon among themIn contrast o these two extreme pcsltlous (usingas a source representation a full semantic representa-tion of the text or reducing l t to  a simple frequencytable), we deal m tins paper wttb the issue of pro-ducmg a summary from an arbitrary text without re-qmrmg zts full understanding, but using wtdely avad-able knowledge sources Our mare goal is thereforeto find a middle ground for source representation,rich enough to braid quality indicative summaries,but easy enough to extract from the source text towork on arbltrary textOver-slmphficatlon can harm the quahty of thesource representation As a trivial illustration, con-sider the following two sequences?
1 "Dr Kenny has sn~ented an anesthetsc maehsneThss devwe controls the rate at wh:ch an ana-esthctsc ss pumped into the blood"2 "Dr Kenny has :nvented an anesthet:c machsneThe Doctor spent wo years on thu research"~Dr Kenny ~ appears once m both sequences andI0IIIIiIIIIII!so does ~nach:n?
~ But sequence 1 ts about the roa-ch:he, and sequence 2 m about the *doctor ~ Tlusexample mchcates that zf the source representationdoes not supply mformatlon about semantically re-lated terms, one cannot capture the %boutnesg' ofthe text, and therefore the summary will not capturethe mare point of the original textThe norton of cohemon, introduced m (Halhdayand Hasan, 1976) captures part of the mtmtmn Co-hereon is a dewce for "sticking together" differentparts of the text Cohesion m achmved through theuse of semantmaUy related terms, reference, lhpsmand conjunctlousAmong these dtfferent means, the most easdy zde-ntfllable and the most frequent type m lemcal cohe-" slon (as discussed m (Hoey, ~ 1991)) Lexlcal cohe-sion is created by usmg semantically related wordsHalhday and Hasan classflled lemcal cohesion intorelteratlon category and collocatlon category Rezt-eratlon can be achmved by repetltlon, synonyms andhyponyms Collocatmn relatzons spectfy the relationbetween words that tend to co-occur m the same lex-zeal contexts (e g ,  "She works as a teacher  m the.School ~)Collocation relations are more problematzc for ld-enttticat~on than rezterat|on, but both of t~hese cat-egones are Identifiable on the surface oi~ the textLextcal cohemon occurs not only between two terms,but among sequences of related words ~ called/ez-~cal chains (Morns and Hlrst, 1991) Lemcal chainsprovide arepresentahon f the lemcal cohemve struc-tare of the text Lemcal chains have also been usedfor mfo~nahon retrieval (Stamnand, 1996) and forcorrection ofmalaproptsms (Htrst and St-Onge, 1997(to appear)) In tlus paper, we mveshgate how lem-cal chmns can be used as a source representation forsummarizationAnother nnportant dunenmon of the lmgumtzc str-ucture of a source ,text m captured under the re-lated not,on of coherence Coherence defines themacro-level semantic structure of a connected Ls-course, while cohesion creates connectedness m anon-structural manner Coherence m represented mterms of coherence relat~ous between text segments,such as cla~orahon, cause and ezplanat|on Someresearchers, e g, (Ono, Kazuo, and Seljl, 1994),use chscourse structure (encoded umng RST (MAnnand Thompson, 1987) as a source representatxon forsummanzatxon) Clearly, thin representation ms ex-presmve enough, the question m whether ~t m com-putable In contrast to lemcal cohemon, coherencem chfl~cult to zdent|fy mthout complete understand-mg of the text and complex reference In ad&tton,there m no prease criteria for clasmficat~on fdiffer-ent relatlous Consider the following example fromHobbs(1978) "John can open the safe He Imowsthe combmahon "(Morns and H~mt, 1991) show that the relationbetween these two sentences can he interpreted asdaborahon or as ezplanahon, depen&ng on %on-text, knowledge and behefs"There m, however, a close connechon between din-course structure and cohemon Related words tendto co-occur mthm a dmcourse umt of the text Socohemon m one of the surface mgns of dmcourse struc-ture and lexlcal chaln~ can' be used to Identify itOther mgns can be used to ldentzfy dmcourse struc-ture as well (connect,yes, paragraph markers, tenseshifts)In thls paper, we investigate the use oflemcalchains as a model of the source text for the pur-pose of producing a summary Obviously, otherpects of the source text need to be integrated m thetext representation to produce quahty summaries,but we want to empmcally investigate how far onecan go exploiting mainly lemcal chains In the restof the paper we first present our algorithm for lex-zeal chain construct,on We then present empmcalresults on the ldentlficatzon ofstrong chains amongthe posmble can&dates produced by our algorithmFinally, we describe how lexlcal chains are used toidentify mgmficant sentences mtlnn the source textand eventually produce a surQmaryA lgor i thm for  Cha in  Comput ingOne of the clnef advantages of lemcal cohesmn mthat zt m an easdy reco~m~able relatmn, enabhnglexlcal chains computation The first computationalmodel for lemcal chains was presented m(Morns andHlrst, 1991) They define lexlcal cohesmn relatzonsm terms of categories, index entries and pointers mRoget's Thesaurus Morns and Hlrst evaluated thattheir relatedness criterion covered over 90% of themtmttve lexzcal relatzons Cham~ are created by tak-ing a new text word and findtng a related chain forit according to relatedness criteria Morns and HLrstintroduce the notion of "actzvated chain ~ and ~chamreturns", to take into account he dmtance betweenoccurrences ofrelated words They also analyze fac-tors contributing to the strength of a chain - -  rep-etltxon, density and length Morns and Hn'st &dnot ~nplement their algorithm, because there wasno machine-readable vermon of Roget's Thesaurusat the tzmeOne of the drawbacks of thelr approach was thatthey chd not reqmre the same word to appear uththe same sense m ~ts &ffexent occurrences for ttto belong to a chain For semantically ambiguous11words, this can lead to confnslous (e g ,  mixing twosenses of taSle as aptece 0f furniture or an array)Note that choosing the appropriate chain for a wordis eqmvalent to dzsamblguatmg tins word m context,which is a well-known d~fl~cult problem m text un-derstandingMore recently, two algorithms for the calculationof lexlcal chains have been presented m Hirst and St-Onge (1995) and Stairmand (1996) Both of thesealgornthms use the WordNet lexlcal database for de-termining relatedness of the words (Miller et a l ,1990) Senses m the WordNet database are repre-sented relatlonally by synonym sets ('synsets') - -which are the sets of all the words sharing a com-mon sense For example two senses of "computer"are represented as {calculator, eckoner, figurer, es-timator, computer) (s e ,  a person who computes)and {computer, data processor, electromc computer,reformation processing system) WordNet containsmore than 118,000 dflferent word forms Words ofthe same category are hnked through semantic rela-tions hke synonymy and hyponymyPolysemous words appear m more than one syn-sets (for example, comptdcr occurs m two synsets)Approxtmately 17% of the words m WordNet arepolysemous But, as noted by Stairmand, this fig-ure is very tmsleadmg "a slguxficant proportion ofWordNet nouns are Latin labels for biological en-titles, which by their nature are monosemons andour experience wtth the news-report texts we haveprocessed ts that approxtmately half of the nounsencountered are polysemous" (Stairmand, 1996)Generally, a procedure for constructing lexlcal ch-ains follows three steps (1) Select a set of can&datewords, (2) For each candldate word, find an appro-priate chain relying on a relatedness cute.on amongmembers of the chains, (3) If  It is found, insert theword m the chain and update It accorchnglyAn example of such a procedure was representedby Hlrst and St-Onge (H&S) In the preprocessorstep, all words that appear as a noun entry m Word-Net are chosen Relatedness of words xs dstermmedm terms of the distance between their occurrencesand the shape of the path connecting them m theWordNet thesaurus Three kinds of relation are de-fined extra-strong (between a word and tts rep-etxt~on), strong (between two words connected bya Wordnet relatxon) and mechum-stroug when thehnk between the synsets of the words is longer thanone (only paths satisfying certain restrictions are ac-cepted as vahd connectxons)The maxtmum distance between related words de-pends on the kind of relatxon for extra-strong rela-ttons, there is not hxmt m &stance, for strong rela-tlons, it is hmlted to a window of seven sentences,and for mechum-strong relations, It is wltinn threesentences backTo find a chain m winch to insert a given can-dtdate word, extra-strong relattons are preferred tostrong-relations and both of them are preferred tomedmm-strong relations If a chain is found, thenthe candtdate word is inserted with the appropriatesense, and the senses of the other words m the receiv-ing chain are updated, so that every word connectedto the new word m the chain relates to Its selectedsenses only If no chaan is found, then a new chain Iscreated and the can&date word ts inserted with allits possible senses m WordNetThe greedy &samblguatzon strategy Implementedm this algorithm has some lmntatlonsdinstrated bythe following exampleMr.
Kenny ~s the person that invented an anaesthehcmachine whsch uses micro-computers to control therate at whsch an anaesthehc ,s pumped into the bloodSuch machines are nothing new But hu device usestwo micro-computers to achseee much closer momtor-mg o/the pump \]eedmg the anaesthehc into the pahentAccor&ug to H&S's algorithm, the chain for theword "Mr" is first created \[ lex "Kr ."
,  sense{mzster, Mr. }\] "Mr" belongs only to one synset,so it is chsamblguated from the beginning The word"person" is related to tins chain m the sense "ahuman be,ng" by a medmm-stroug relation, so thechain now contains two entries\[ lex "Mr'.
", sense {m.ster ,  Mr.)\]\[ lex "person", sense {person, :t.nd~.v~dual,someone, man, morta l ,  huma.u, sou1}\]When the algorithm processes the word "machineD,It relates it to this cham, because "roach:hen mthe first WordNet sense ("an e Oiczent person") isa holonym of apersonn m the chosen sense In otherwords, "machine" and  "person" are related by astrong relation In tins case, "machine" ts disam-blguated m the wrong way, even though after tinsfirst occurrence of "machine", there is strong evi-dence supporting the selechon of xts more commonsense "macro-computer", "demce" and "pemp" allpoint to its correct sense m tins context ~ "any me-chanzcal or electrzcal devzce thaZ performs or assgs~szn the performance"Tins example mdtcates that disamblguatlon can-not be a greedy decision In order to choose the rightsense of the word the 'whole ptcture' of chain distn-butwn m the text must be conmdered We proposeto develop a chaining model according to all possxblealternatives of word senses and then choose the bestone among themLet us dlustrate tins method on the above exam-12IIIIIIIIIIIIIII!II!pie First, a node for the word =Mr" Is created \[ lex"ltr ' ."
,  sense {mister ,  Kr }3 The next candi-date word Is "person" It  has two senses "hamanbesng n (person "1)  and erratum=heal cafegory ofpronouns and verb forms" (person -- 2) The choiceof sense for ~person" sphts the chain world to twodflferent interpretations a shown m Figure 1IFigure I Step Ilpen%}Interpretations 1 and 2We define a component as a list of interpretationsthat are exclusive of each other Component Wordsinfluence each other in the selection of their respec-tive sensesThe next candidate word =anaesthetsc" Is not re-lated to any word m the first component, so we cxe-ate a new component for it with a single lntexpreta-taonThe word "machsne" has 5 senses mach:nei tomachine5 In its first sense; "an e.0ic:ent person",it m related to the senses =person" and =Mr" Ittherefore influences the selection of thexr senses, thus"machine" has to be ~ m the first componentAfter its msertmn the picture of the first componentbecomes the one shown m Figure 2?
But ff we continue the process and insert the wor-ds =micro-compeer', = dcmce n and =pump', the nu-mber of nlternatlve greatly increases The strongestinterpretations are given m Figures 3 and 4Under the assumption that the text Is cohessve,we define the best interpretation as the interpreta-tion with the most connections (edges m the graph)In tins case, the second interpretation at the end ofStep 3 is selected, which predicts the right sense for"machine" We define the score of an interpretationas the sum of its chain scores Chain seore is deter-mined by the number and weight of the relations be-tween chain members Expenlnentally, we fixed theweight of reiteration and synonym to 10, of antonymto 7, and of hyperonym and holonym to 4' Our al-gorithm develops all possible interpretations, main-tainmg each one without self contradiction Whenthe number of possible interpretations is larger thana certain threshold, we prune the weak interpreta-tions according to tins criteria In the end, we selectfrom each component, the strongest interpretation(Mr  .
m |\[pe~ntlZlt mdtvtdt ta l  mmeoae II maclune a } SStep 2: Interpretauon 1tMr,numer}lpenm}{marina% machine s I"Step 2: Interpretation 2(pez -~ ind iv idua l  me,  )(~ehme a m~h,ne  s )Step 2" InterpretaUon 3iMr.n~tef}\[permn}{n,aclane, IStep 2: Interpret=non 4FFtgure 2 Step 2 Interpretations I to 4.
'~.-.
.
?In snmmary, our algorithm differs from H&S's al-gorithm m that It introduces, m addition to the re-latedness criterion for members~p to a chain, a non-greedy dzsainbiguatlon heuristic to select he appro-priate senses of chain membersThe two algonthms differ m two other major as-pects the criterion for the selection of candidatewords and the operative defimhon of a text unitWe choose as candidate words simple nouns andnoun compounds As mentioned above, nouns arethe main contributors to the =aboutness" of a text,and noun synsets dominate m WordNet Both(Stairmand, 1996) and H&S rely only on nouns ascandidate words In our algorithm, we rely on theresults of Brdl's part-of-speech tagging algorithm toidsntlfy nouns, whl\]e H&S do not go through thisstep and only select okens that?
happen to occur asnouns m WordNetIn addition, we extend the set of candidate wordsto include noan compound We first empmcally eval-uated the unportance of noun compounds by takingmto account he noun compounds exphcttly presentm WordNet (some 50,000 entries m WordNet arenoun compounds such as "sea level" or co\].locatlons13(Mr,lms~e?}
('MLczq-~__'~ {PC, rmaro- computer, }t Iperso~Figure 3 Step 3 Interpretation 1Figure 4 Step 3 Interpretation 2such as "digital computeff) However, Enghsh in-cludes a productive system of noun comp0hnds, andm each domain, new noun-compounds and colloca-tions not present m WordNet play a major roleWe addreseed the issue, by usmg a shallow parser(developed by Ido Dagan's team at Bar Ilan Um-verslty) to identify noun-compounds u ing a snnplecharacterization f noun sequences Tins has twomajor benefits (1) it ldentflles Important conceptsm the domain (for example, m a text on "quan-tum computing", the mare token was the noun com-pound ``~uantum computing" winch was not presentm WordNet), (2) it chromates words that occur asmodn~ere as posmble can&dates for chain member-sinp For example, when ` `quantum computing" mselected as a smgle umt, the word ` `?uantum ~ is notselected This Is beneficial because m tins example,the text was not about-"quantum', but more aboutcomputers When a noun compound ~s selected, therelatedness criterion in WordNet ~s used by couslder-mg its head noun only Thus, "quantum computer ~~s related to ` `machine ~ as a ~computer ~The second dflfexence m our algorithm hes mthe operative defuntion we gwe to the notion oftext umt We use as text umts the segments ob-tained from Hearst's algorithm of text segmentation(Hearst, 1994) We braid chains m every segmentaccording to relatedness criteria, and in a secondstage, we merge chains from the dflferent segmentsusing much stronger criteria for connectedness onlytwo chains are merged across a segment boundaryonly if they contain a common word with the samesense Our mira-segment relatedness criterion.is lessstrict members of the same synsets are related, anode and its offspnng m the hyperonym graph arerelated, mbhngs m the hyperonym graph are relatedonly ffthe length of the path m less thana thresholdThe relation between text segmentation a d lex-lcal chain is dehcate, since they are both derivedfrom partially common source of knowledge lexlcal&stnbutlon and repetitions In fact, lexlcal chainscould serve as a barns for an algorithm for segmen-tation We have found empmcally, however, thatHearst's algorithm behaves well on the type of texts?
we checked and that it prowdes effectively a sohdbasLS for lexlcal chains constructionBuilding Summaries UsingLexical ChainsWe now investigate how lexlcal chains can serve asa source representation f the original text to budda summary The next question m how to build.sum-mary representation from tins source representationThe most prevalent dmcourse topic will play animportant role m the summary We first presentthe mtmtlon why lex~cal chains are a good m&catorof the central topic of a text G!ven an approprn-ate measure of strength, we show that picking theconcepts represented by strong lexlcal chains glves abetter mchcatlon of the central toplc of a text thansnnply plckmg the most frequent words m the text(which forms the zero-hypothesis)For example, we show m Appendix a sampletext about Bayeman Network technology There, theconcept of network was represented by the words"network" with 6 occurrences, %ct" with 2, and``system ~ ruth 4 But the summary representa-tion has to reflect that all these words representthe same concept Otherwise, the summary gen-eration stage would extract information separatelyfor each term The chain representation approachavmds completely this problem, because all tl~eseterms occur m the same chain, winch reflects thatthey represent the same conceptAn ad&tlonal argument for the chain representa-tion as opposed to a rumple word frequency modelis the case when a tangle concept is represented by anumber of words, each with relatively low fTequencyIn the same Bayesian Network sample text, the con-cept of "reformat:on" was represented by the words",nformatson" (3), "datum" (2), "Irnowledge" (3),"concept" (1) and "model" 1 In tins text, "mforma.tzon" m a more important concept han "computer"14IiIIIIiIIIIIIIIIIIIwhtfh occurs 4 times Because the "mformatson"chmn combines the number of occurrences of all itsmembers, It can overcome the weight of the singleword "computer"Scor ing ChainsIn order to use leemcal chains as outlined above, onemust first identify the strongest chains among allthose that are produced by our algorithm As isfrequent m summarization, there Is no formal wayto evaluate chain strength (as there m no formalmethod to evaluate a summary quality) We there-fore rely on an empmcal methodology We havedeveloped an envxronment to compute and graph-lcally visuallze lexxcal chains to evaluate xperimen-tally how they capture the mare topics of the textsFigure 5 shows how lemcal chains are visualized tohelp human testers evaluate therr importanceFigure 5 Visual representa~on f lexlcal chRm~We have collected data for a set of 30 textsextracted from popular magazmes (from "TheEconommt" and ` `Scientific American"), all of themare popular science genre For each text, we manu-ally ranked chains m terms of relevance to the maretoplcs We then computed ifferent formal measureson the chmns, including chmn length, ?hstnbutionm the text, text span covered by the chain, density,graph topology (diameter of the graph of the words)and number of repetitious The results on our dataset indicate that only the following parameters aregood predictors of the strength of a chmnLength:  The number of occurrences ofmembers ofthe chainHomogene i ty  index: 1 - the number of distractoccurrences divided by the lengthWe demgned a score function for chains asScore(Chain)  = Length ?
Homogene=tyWhen ranking CbamR according to thexr score, weevaluated that strong chamR are those winch satlsfyour "Strength Criterion"5core(Cha:n) > Auerage(Seores) +2 .
~tandardDeemtson( Scorea)These are prehmmary results but they are con-firmed by our experience on 30 texts analyzed ex-tensively We have expertraenteed wsth d~erent nor-mahzation methods for the score function, but theydo not seem to nnprove the results We plan onextending the empmcal analym m the future andto use formal earmng methods to determine a goodscoring functionThe average number of strong chains selected bythxs selection method was 5 for texts of 1055 wordson average (474 words mmunum, 3198 words mare-mum), when 32 chmnR were originally generated onaverage The strongest chmn of the sample text arerepresented m AppendixExtract ing Significant SentencesOnce strong chains have been selected, the next stepof the summarization algorithm is to extract full sen-tences from the original text based on chain distn- .butlonWe investigated three alternatives for tlus StepHeurist ic  1 For each chain m the summary rep-resentation choose the sentence that contains thefirst appearance ofa chain member m the textThls heuristic produced the following summary forthe text shown in AppendixWhen Mscroaoft Semor Vsce Pressdcnt Steve Ballmerfirst heard h~ company was planning to make a huge m-vestment m an Internet oermec offering mome remewsand local entertainment mformahon m ma3or cstwzsacross the nahon, he trent to Cfiasrman Bdl Gates wlthhu concerns M.crasoft's compehhve advantage, he re-sponded, tvas its exparhse m Bayesian etworksBayessan ettvorks an~ cort~pl~ diagraras that o~gamzethe body of knowledge m any gwen area by mapping outcause and effect relatmnshlpa among key varmbl~ andencoding them ?vsth numbers that repr~ent the eztent otvhsch one varmble ss hkely to a~ect anotherProgrammed into computers, these systems can auto.mahcally generate optimal pred, chon8 or decisions eventohen key pieces of mformahon are mtsslngWhen Mserosoft tn 1993 hired Eric Horustz, David Heck-erman and Jack Brecse, pioneers m the development ofBayesmn systems, colleague8 m the field were surprisedThe problem wxth tins approach m that all wordsm a chain reflect he same concept, but to a &fferentextent For example, m the AI chain, (AppendL~,Chain 3) the token %czence" ts related to the con-cept aA~', but the words ``AF' and ``)~eid" are moresuitable to represent the mare topic ``AI" m the con-text of the text That is, not all chain members aregood representatives of the topic (even though theyall contribute to its meamng)15IWe therefore defined a criterion to evaluate theapproprlateness of a cham member to represent, itschain based on its frequency of occurrence m thechani ~ ~We found experimentally that such words,call them represenfafs~e words, have a frequency mthe chain no less than the average word frequencym the chain For example, m the third chain therepresentative words are "field" and "AI"Heur i s t i c  2 We therefore defined a second heu-ristic based on the notion of representative wordsFor each chain m the summary representation,choose the sentence that contains the first appear-ance of a representative chain member m the textIn this special case this heuristic gives the sameresult as the first oneHeur i s t i c  3 Often, the same topic is dmcussedin a number of places in the text, so its chain isdL~tnbuted across the whole text Still, m some textunit, this global topic is the central topic (focus) ofthe segment We try to identify this umt and extractsentences related to the topic from this segment (orsuccessive segments) onlyWe characterize this text umt as a cluster of suc-cessive segments with high density of chain mere-beers Our tlnrd heuristic Is based on thts approachFor each chain, find the text umt where the chainIs highly concentrated Extract the sentence withthe first chain appearance m tins central umt Con-centratlon m computed as the number of chain mem-bers occurrences m a segment &vlded by the numberof nouns m the segment A chain has high concen-tratton ff its concentrat|on is the mammum of allchains Cluster is grou p of successive segments suchthat every segment contains chain membersNote that m all these three techmques only onesentence is extracted for each chain (regardless ofits strength)For most texts we tested, the first and second tech-niques produce the same results, but when they aredflferent, the output of the second teclmlque Is bet-tex Generally, the second techmque produces thebest summary We checked these methods on our30 texts data set Surprisingly, the tlnrd heuris-tic, winch intuition predicts as the most sophisti-cated, gives the least indicative results TIns maybe due to several factors our criteria for 'cen-trahty' or 'clustering' may be insufficient or, morehkely, the problem seems to be related to the in-teraction with text structure The third heuristicstends to extract sentences from the middle of thetext and to extract several sentences from dmtantplaces m the text for a single chain The completeresults of our experiments are avatlable onohne ath t l ;p : / /~  cs bgu.
ac 3.1/sllmm,a.r~.za?3.on-tesl;L imi ta t ions  and  Future  WorkWe have identified the following maul problems withour.
methodSentence granularity all our methods extractwhole sentences as single umts Ttus has severaldrawbacks long sentences have mgnflicantly n-gher hkehhood to be selected, they also includemany constituents which would not have beenselected on theu own merit The alternativeIs extremely costly it revolves ome parsing ofthe sentences, the extraction of only the centralconstituents from the source text and the regen-eration of a summary text using text generationtechniquesExtracted sentences contain anaphera hnks tothe rest of the text This has been investigatedand observed by (Black, 1994) Several heurls-ties have been proposed m the hterature to ad-dress flus problem (Pmce, 1990), (Patce andHusk, 1991) and (Black, 1994) The strongestseems to be to include together wtth the ex-tracted sentence the one lmme&ately precechngit Unfortunately, when we select he first sen-tence in a segment, he preceding sentence doesnot belong to the paragraph and its insertionhas a detrimental effect on the overall coherenceof the summary A preferable solution wouldbe to replace anaphora wzth theLr referent, butagain fins m an extremely costly solutionOur method oes not provide any way to controlthe length and level of detad of the summaryIn all of the methods, we extract one sentencefor each chain The number of strong chamR re-mmns maU (around 5 or 6 for the texts we havetested, regardless of then length), and the re-mmmng chains would introduce too much nmseto be of interest m ad&ng details The best so-lution seems to be to extract more material forthe strongest chainsThe method presented m thin paper m obviouslypartial mthat  it only considers lemcal chains as asource representation, and ignores any other cluesthat could be gathered from the text Still, ourfirst mformalevaluatlon indicates our results are of aquahty superior to that of summarizers u ually em-ployed m commercial systems such as search systemson the World Wide Web on the texts we investigatedA large-scale evaluation of the method and how sen-sltlve It IS to the quahty of the thesaurus and to itsparameters is under way16IIIIIIIIIIII!
!IIIIBayesian Networks TextWhen 'M~rosoft Semor VICU P~esKhmt Steve BalJm~ fn'st heard has company yvesplanning to m eke a huge mveatment In an Interoet eennca offenng mowe re ins  endIoca| entu~tainment mfonlnntmn mmajor cmea ao~ea the no?tun he went to ChanmunBd\] Gates wth his concern?After ell Bellme~ has bflhom of dollars of lus own money m M~'osoft stock, and~tertumment tsn t exactly the compuny'e strong pointOut Gates dmrmesed such re?alva?runs M~:rosoft's compehUve advantage, he re-sponded, me mt expertem In Bayeamn etw0ckeAsked recently when computers mum fnaagy beipn to understand human speechGates begun dncussmgthe r .ntal  role of Bay?stun' systemsAsk any other software executive about anything Bay~mn and you're hable to set ?blunk ?tareI?
Gates onto ?orneUung?
I?
tim abm4oundmg technology Mmrosoft ?
new secretwenpoN ?Bayeslun netvmrk?
ere complex dmgram?
that organize the body of knowledge m anyIpven area by rnapping out cause-and-effect relat~onshnps among key vamblea endencoding them with numbees that represent he extent o which one ramble n 5kth7to affect another ( )Programmed into comlmt4m~, these ?ystems can autometzcuHy generate optunzd pre-dlct4ons or deastons even when key pieces of Informabon are m~n gWhen Miorozoft in 1993 hired Eric Homtz, David H~:kermun and Jack Brine pro-nears m the devdopment of Bay?stun systems, colleagues m the field were ?mlmsedThe held was still an obscure, lergdy academic enterlmeaBayesian nets prowde an oeararchmggraphtcal framework" that Imngs togetlm'cb-verse elementsof AI and Increase'the range of Ks hkely epphcabon to the real worldsays Michael Jordon profes~mr of bram'and cog?rove menea et the MassachusettsInstitute of TechnoloWMmrosoft ts unquestionably the most aKfFessJve in expkntmg the new epproach Thecumpeny offers ?
'free Web sen?tee that helps customers dtegnose pnntmg problemsruth their cornputae and recommends the qmdoett way to resolve them AnotherWeb se~nca help?
parents diagnose thew chlklren'e health problems ( )Horm~ who with two colleagues founded Knowledse Induutnea to develop tools fordeveloping Ba.~slan systems says he and the others left the eompany tojom MicrosoftIn part because they wanted to see their theoretu:ol work more broadly apphedAlthough the corn pany did important work for the Natmnal Aeronautics and Space Ad-mimstmbon and on medw.al diagnostics Homtz says 'it ?
not Ilk?
your grundmothefvnll use itMiorosoft'?
eehvmea m the held am now helping to build a groundsv.~dl of support forBayesian KleasPeople look up So M~osoft ' says Pearl.
who wrote one of the key early texts on0aye?on networks tn 1988 and has become an unoflu:~d ?pokesrcan for the hdd"They ve Ipven a boost to the whole area"M~'osoft m wodtm s on technques that wdl enable the Bayeamn etworks to ke~rnor update them?dyes automatu:;dly based on new knowledge 8 task that m currentlycumbersomeBayesian Network Text: theStrongest ChainThe Criterion t~ 3 ~8, here are the five strong chasn~OHAIN I Score = 14 0mzcro~oft 10 concern I company 6enterta~tment-~ervlce 1 enterprbe 1ma~ttchu~et t -mats?
ut e 1~HA/,N ?
Score ffi 9 0ba~'e~l&n-~y~tem 2 ~y~tem 2 baye~za~s-net 2network 1 baye~z~n-network 5 weapon 1 :CHAIN 3 Score ---- 7 0m 2 a~ttficzal-mtolhgunce /~field 7 technology 1 t, czence ICHAIN ~ Score ffi 6 Otochmquo 1 b&ye~tsn-techmque I condztzon Idatum 2 model I mformatton 3 area Iknowledge 3~HAIN S Score = 3 0computer  4AcknowledgementsTkts work has been supported by the Israeh Mlmstryof Science We axe grateful to Graeme Ktrst, DragonnrRadev and Claude Bneson for thezr feedback on a previ-ous vermonReferencesBlack, Wflham J 1994 Parsing, lmgmstlc resources andsemantic analysis, for abstzactmg and categorizationHalhday, lqhchael and Ruqatya Hasan 1976 Cohesionin Enghsh Longman, London17Hearst, Marti A 1994 Multi-paragraph segmentationof exposltoZT text In Proceedmgs of the 3~nd AnnualMeetmg of the Assocmhon for Computational Lmguts-hcsHLmt, Graeme and Dared St-Onge 1997 (to appear)Lemca\] chains as representation of context for the de-tection and correction of malapropisms In Chns-tiane Fellbanm, edxtor, WordNet An electmmc lez-tcal database and some of ,ts appheat:ons Cambridge,MA The MIT PressHoey, M 1991 Patterns of Leats m Tezt Oxford Um-vermty Press, OxfordJones,.KarenSpaxck 1993 What mightbe m summary ?Informahon RetrlevalLulm, H P 1968 The automatxc ereatzon of hteratureabstracts In Schultz, edttor, H P Luhn Pioneer oflnformahon Science SpartanMann, W C and S Thompson 1987 RhetoncaJ.
struc-ture theory description and constructions of textstructures In Gerard Kempen, echtor, Natural Lan-guage Generahon New Results m Arhfictal Intellh-gence, Psychology and Lmgutst:cs Maxtmus Nmjhot~Pubhshers, pages 85-96McKeown, Kathleen and Dragonur Radev 1995 Gen-eratmg summaries of multiple news articles In SIGIR95 ProceedingsMdler, George A ,  P,~chard Beckwlth, Chnstiane Fell-bans, Derek Gross, and Kathenne J ~ 1990Introduction to WordNet An on-lme lexxcal databaseInternat,onal Journal of Lazwcographg (special issue),3(4) 23,5,-312Morns, J and G Hzrst 1991 Lexzcal coheszon com-puted by thesanra\] relations as an re&cater of thestructure of the text Computahonal Lmgutsttos,17(1) pp 21.--45Onq, Ken3h Sunuta Ks?no, and Mnke Seql 1994 Ab-stract genezation based on rhetorical structure extruc-tzon In Proceedings of the International Conferenceon Computahonal Lmgutshcs (Cohng 9~), pages 344--348, JapanPa~ce, C D and G D Husk 1991 Towards the au-tomatic recognntlon of anaphonc features m enghshtext The nnpexsonal pronoun "zt ~ Computer Speechand Language, (2) pp 109-132Patce, Chris D 1990 Constructing hterature abstractsby computer techmqu~ and prospects lnformahonProcessmg and Management, 26(1) 171-186Statrmand, Mark A 1996 A Computahonal Analysts ofLexscal Cohesion wtth Apphcattons :n Informatton Re-trievai Ph D thesis, Center for Computational Lm-gmstics, UMIST, Manchester
