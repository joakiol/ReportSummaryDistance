Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 523?534, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsOpen Language Learning for Information ExtractionMausam, Michael Schmitz, Robert Bart, Stephen Soderland, and Oren EtzioniTuring CenterDepartment of Computer Science and EngineeringUniversity of Washington, Seattle{mausam,schmmd,rbart,soderlan,etzioni}@cs.washington.eduAbstractOpen Information Extraction (IE) systems ex-tract relational tuples from text, without re-quiring a pre-specified vocabulary, by iden-tifying relation phrases and associated argu-ments in arbitrary sentences.
However, state-of-the-art Open IE systems such as REVERBand WOE share two important weaknesses ?
(1) they extract only relations that are medi-ated by verbs, and (2) they ignore context,thus extracting tuples that are not asserted asfactual.
This paper presents OLLIE, a sub-stantially improved Open IE system that ad-dresses both these limitations.
First, OLLIEachieves high yield by extracting relations me-diated by nouns, adjectives, and more.
Sec-ond, a context-analysis step increases preci-sion by including contextual information fromthe sentence in the extractions.
OLLIE obtains2.7 times the area under precision-yield curve(AUC) compared to REVERB and 1.9 timesthe AUC of WOEparse.1 IntroductionWhile traditional Information Extraction (IE)(ARPA, 1991; ARPA, 1998) focused on identifyingand extracting specific relations of interest, therehas been great interest in scaling IE to a broaderset of relations and to far larger corpora (Banko etal., 2007; Hoffmann et al2010; Mintz et al2009;Carlson et al2010; Fader et al2011).
However,the requirement of having pre-specified relations ofinterest is a significant obstacle.
Imagine an intel-ligence analyst who recently acquired a terrorist?slaptop or a news reader who wishes to keep abreastof important events.
The substantial endeavor in1.
?After winning the Superbowl, the Saints are nowthe top dogs of the NFL.
?O: (the Saints; win; the Superbowl)2.
?There are plenty of taxis available at Bali airport.
?O: (taxis; be available at; Bali airport)3.
?Microsoft co-founder Bill Gates spoke at ...?O: (Bill Gates; be co-founder of; Microsoft)4.
?Early astronomers believed that the earth is thecenter of the universe.
?R: (the earth; be the center of; the universe)W: (the earth; be; the center of the universe)O: ((the earth; be the center of; the universe)AttributedTo believe; Early astronomers)5.
?If he wins five key states, Romney will be electedPresident.
?R,W: (Romney; will be elected; President)O: ((Romney; will be elected; President)ClausalModifier if; he wins five key states)Figure 1: OLLIE (O) has a wider syntactic range and findsextractions for the first three sentences where REVERB(R) and WOEparse (W) find none.
For sentences #4,5,REVERB and WOEparse have an incorrect extraction byignoring the context that OLLIE explicitly represents.analyzing their corpus is the discovery of importantrelations, which are likely not pre-specified.
OpenIE (Banko et al2007) is the state-of-the-artapproach for such scenarios.However, the state-of-the-art Open IE systems,REVERB (Fader et al2011; Etzioni et al2011)and WOEparse (Wu and Weld, 2010) suffer from twokey drawbacks.
Firstly, they handle a limited sub-set of sentence constructions for expressing relation-ships.
Both extract only relations that are mediatedby verbs, and REVERB further restricts this to a sub-set of verbal patterns.
This misses important infor-mation mediated via other syntactic entities such asnouns and adjectives, as well as a wider range ofverbal structures (examples #1-3 in Figure 1).523Secondly, REVERB and WOEparse perform onlya local analysis of a sentence, so they often extractrelations that are not asserted as factual in the sen-tence (examples #4,5).
This often occurs when therelation is within a belief, attribution, hypotheticalor other conditional context.In this paper we present OLLIE (Open LanguageLearning for Information Extraction), 1 our novelOpen IE system that overcomes the limitations ofprevious Open IE by (1) expanding the syntacticscope of relation phrases to cover a much largernumber of relation expressions, and (2) expand-ing the Open IE representation to allow additionalcontext information such as attribution and clausalmodifiers.
OLLIE extractions obtain a dramaticallyhigher yield at higher or comparable precision rela-tive to existing systems.The outline of the paper is as follows.
First, weprovide background on Open IE and how it relatesto Semantic Role Labeling (SRL).
Section 3 de-scribes the syntactic scope expansion component,which is based on a novel approach that learns openpattern templates.
These are relation-independentdependency parse-tree patterns that are automati-cally learned using a novel bootstrapped training set.Section 4 discusses the context analysis component,which is based on supervised training with linguisticand lexical features.Section 5 compares OLLIE with REVERB andWOEparse on a dataset from three domains: News,Wikipedia, and a Biology textbook.
We find thatOLLIE obtains 2.7 times the area in precision-yieldcurves (AUC) as REVERB and 1.9 times the AUCas WOEparse.
Moreover, for specific relations com-monly mediated by nouns (e.g., ?is the presidentof?)
OLLIE obtains two order of magnitude higheryield.
We also compare OLLIE to a state-of-the-artSRL system (Johansson and Nugues, 2008) on anIE-related end task and find that they both have com-parable performance at argument identification andhave complimentary strengths in sentence analysis.In Section 6 we discuss related work on pattern-based relation extraction.2 BackgroundOpen IE systems extract tuples consisting of argu-ment phrases from the input sentence and a phrase1Available for download at http://openie.cs.washington.edufrom the sentence that expresses a relation betweenthe arguments, in the format (arg1; rel; arg2).
This isdone without a pre-specified set of relations and withno domain-specific knowledge engineering.
Wecompare OLLIE to two state-of-the-art Open IE sys-tems: (1) REVERB (Fader et al2011), whichuses shallow syntactic processing to identify rela-tion phrases that begin with a verb and occur be-tween the argument phrases;2 (2) WOEparse (Wuand Weld, 2010), which uses bootstrapping from en-tries in Wikipedia info-boxes to learn extraction pat-terns in dependency parses.
Like REVERB, therelation phrases begin with verbs, but can handlelong-range dependencies and relation phrases thatdo not come between the arguments.
Unlike RE-VERB, WOE does not include nouns within the re-lation phrases (e.g., cannot represent ?is the presi-dent of?
relation phrase).
Both systems ignore con-text around the extracted relations that may indi-cate whether it is a supposition or conditionally truerather than asserted as factual (see #4-5 in Figure 1).The task of Semantic role labeling is to identifyarguments of verbs in a sentence, and then to clas-sify the arguments by mapping the verb to a se-mantic frame and mapping the argument phrases toroles in that frame, such as agent, patient, instru-ment, or benefactive.
SRL systems can also identifyand classify arguments of relations that are mediatedby nouns when trained on NomBank annotations.Where SRL begins with a verb or noun and thenlooks for arguments that play roles with respect tothat verb or noun, Open IE looks for a phrase that ex-presses a relation between a pair of arguments.
Thatphrase is often more than simply a single verb, suchas the phrase ?plays a role in?, or ?is the CEO of?.3 Relational Extraction in OLLIEFigure 2 illustrates OLLIE?s architecture for learningand applying binary extraction patterns.
First, it usesa set of high precision seed tuples from REVERB tobootstrap a large training set.
Second, it learns openpattern templates over this training set.
Next, OLLIEapplies these pattern templates at extraction time.This section describes these three steps in detail.
Fi-nally, OLLIE analyzes the context around the tuple(Section 4) to add information (attribution, clausalmodifiers) and a confidence function.2Available for download at http://reverb.cs.washington.edu/524ReVerbSeed TuplesTraining DataOpen Pattern Learning BootstrapperPattern TemplatesPattern Matching Context Analysis Sentence Tuples Ext.
TuplesExtractionLearningFigure 2: System architecture: OLLIE begins with seedtuples from REVERB, uses them to build a bootstraptraining set, and learns open pattern templates.
These areapplied to individual sentences at extraction time.3.1 Constructing a Bootstrapping SetOur goal is to automatically create a large trainingset, which encapsulates the multitudes of ways inwhich information is expressed in text.
The key ob-servation is that almost every relation can also be ex-pressed via a REVERB-style verb-based expression.So, bootstrapping sentences based on REVERB?s tu-ples will likely capture all relation expressions.We start with over 110,000 seed tuples ?
these arehigh confidence REVERB extractions from a largeWeb corpus (ClueWeb)3 that are asserted at leasttwice and contain only proper nouns in the argu-ments.
These restrictions reduce ambiguity whilestill covering a broad range of relations.
For ex-ample, a seed tuple may be (Paul Annacone; is thecoach of; Federer) that REVERB extracts from thesentence ?Paul Annacone is the coach of Federer.
?For each seed tuple, we retrieve all sentences in aWeb corpus that contains all content words in thetuple.
We obtain a total of 18 million sentences.For our example, we will retrieve all sentences thatcontain ?Federer?, ?Paul?, ?Annacone?
and some syn-tactic variation of ?coach?.
We may find sentenceslike ?Now coached by Annacone, Federer is win-ning more titles than ever.
?Our bootstrapping hypothesis assumes that allthese sentences express the information of the orig-inal seed tuple.
This hypothesis is not always true.As an example, for a seed tuple (Boyle; is born in;Ireland) we may retrieve a sentence ?Felix G. Whar-ton was born in Donegal, in the northwest of Ireland,a county where the Boyles did their schooling.
?3http://lemurproject.org/clueweb09.php/To reduce bootstrapping errors we enforce addi-tional dependency restrictions on the sentences.
Weonly allow sentences where the content words fromarguments and relation can be linked to each othervia a linear path of size four in the dependency parse.To implement this restriction, we only use the sub-set of content words that are headwords in the parsetree.
In the above sentence ?Ireland?, ?Boyle?
and?born?
connect via a dependency path of length six,and hence this sentence is rejected from the trainingset.
This reduces our set to 4 million (seed tuple,sentence) pairs.In our implementation, we use Malt DependencyParser (Nivre and Nilsson, 2004) for dependencyparsing, since it is fast and hence, easily applica-ble to a large corpus of sentences.
We post-processthe parses using Stanford?s CCprocessed algorithm,which compacts the parse structure for easier extrac-tion (de Marneffe et al2006).We randomly sampled 100 sentences from ourbootstrapping set and found that 90 of them sat-isfy our bootstrapping hypothesis (64 without de-pendency constraints).
We find this quality to be sat-isfactory for our needs of learning general patterns.Bootstrapped data has been previously used togenerate positive training data for IE (Hoffmann etal., 2010; Mintz et al2009).
However, previoussystems retrieved sentences that only matched thetwo arguments, which is error-prone, since multiplerelations can hold between a pair of entities (e.g.,Bill Gates is the CEO of, a co-founder of, and has ahigh stake in Microsoft).Alternatively, researchers have developed sophis-ticated probabilistic models to alleviate the effectof noisy data (Riedel et al2010; Hoffmann et al2011).
In our case, by enforcing that a sentence ad-ditionally contains some syntactic form of the rela-tion content words, our bootstrapping set is naturallymuch cleaner.Moreover, this form of bootstrapping is bettersuited for Open IE?s needs, as we will use this datato generalize to other unseen relations.
Since therelation words in the sentence and seed match, wecan learn general pattern templates that may applyto other relations too.
We discuss this process next.3.2 Open Pattern LearningOLLIE?s next step is to learn general patterns thatencode various ways of expressing relations.
OL-525Extraction Template Open Pattern1.
(arg1; be {rel} {prep}; arg2) {arg1} ?nsubjpass?
{rel:postag=VBN} ?
{prep ?}?
{arg2}2.
(arg1; {rel}; arg2) {arg1} ?nsubj?
{rel:postag=VBD} ?dobj?
{arg2}3.
(arg1; be {rel} by; arg2) {arg1} ?nsubjpass?
{rel:postag=VBN} ?agent?
{arg2}4.
(arg1; be {rel} of; arg2) {rel:postag=NN;type=Person} ?nn?
{arg1} ?nn?
{arg2}5.
(arg1; be {rel} {prep}; arg2) {arg1} ?nsubjpass?
{slot:postag=VBN;lex ?announce|name|choose...}?dobj?
{rel:postag=NN} ?
{prep ?}?
{arg2}Figure 3: Sample open pattern templates.
Notice that some patterns (1-3) are purely syntactic, and others are seman-tic/lexically constrained (in bold font).
A dependency parse that matches pattern #1 is shown in Figure 4.LIE learns open pattern templates ?
a mapping froma dependency path to an open extraction, i.e., onethat identifies both the arguments and the exact(REVERB-style) relation phrase.
Figure 3 gives ex-amples of high-frequency pattern templates learnedby OLLIE.
Note that some of the dependencypaths are completely unlexicalized (#1-3), whereasin other cases some nodes have lexical or semanticrestrictions (#4, 5).Open pattern templates encode the ways inwhich a relation (in the first column) maybe expressed in a sentence (second column).For example, a relation (Godse; kill; Gandhi)may be expressed with a dependency path (#2){Godse}?nsubj?{kill:postag=VBD}?dobj?
{Gandhi}.To learn the pattern templates, we first extract thedependency path connecting the arguments and re-lation words for each seed tuple and the associatedsentence.
We annotate the relation node in the pathwith the exact relation word (as a lexical constraint)and the POS (postag constraint).
We create a re-lation template from the seed tuple by normalizing?is?/?was?/?will be?
to ?be?, and replacing the rela-tion content word with {rel}.4If the dependency path has a node that is not partof the seed tuple, we call it a slot node.
Intuitively,if slot words do not negate the tuple they can beskipped over.
As an example, ?hired?
is a slot wordfor the tuple (Annacone; is the coach of; Federer) inthe sentence ?Federer hired Annacone as a coach?.We associate postag and lexical constraints with theslot node as well.
(see #5 in Figure 3).Next, we perform several syntactic checks oneach candidate pattern.
These checks are the con-straints that we found to hold in very general pat-terns, which we can safely generalize to other un-seen relations.
The checks are: (1) There are no slot4Our current implementation only allows a single relationcontent word; extending to multiple words is straightforward ?the templates will require rel1, rel2,.
.
.nodes in the path.
(2) The relation node is in themiddle of arg1 and arg2.
(3) The preposition edge(if any) in the pattern matches the preposition in therelation.
(4) The path has no nn or amod edges.If the checks hold true we accept it as a purelysyntactic pattern with no lexical constraints.
Oth-ers are semantic/lexical patterns and require furtherconstraints to be reliable as extraction patterns.3.2.1 Purely Syntactic PatternsFor syntactic patterns, we aggressively general-ize to unseen relations and prepositions.
We removeall lexical restrictions from the relation nodes.
Weconvert all preposition edges to an abstract {prep ?}edge.
We also replace the specific prepositions inextraction templates with {prep}.As an example, consider the sentences, ?MichaelWebb appeared on Oprah...?
and ?...when Alexan-der the Great advanced to Babylon.?
and associ-ated seed tuples (Michael Webb; appear on; Oprah)and (Alexander; advance to; Babylon).
Both thesedata points return the same open pattern after gen-eralization: ?
{arg1} ?nsubj?
{rel:postag=VBD}?
{prep ?}?
{arg2}?
with the extraction template(arg1, {rel} {prep}, arg2).
Other examples of syn-tactic pattern templates are #1-3 in Figure 3.3.2.2 Semantic/Lexical PatternsPatterns that do not satisfy the checks are not asgeneral as those that do, but are still important.
Con-structions like ?Microsoft co-founder Bill Gates...?work for some relation words (e.g., founder, CEO,director, president, etc.)
but would not work forother nouns; for instance, from ?Chicago SymphonyOrchestra?
we should not conclude that (Orchestra;is the Symphony of; Chicago).Similarly, we may conclude (Annacone; is thecoach of; Federer) from the sentence ?Federer hiredAnnacone as a coach.
?, but this depends on the se-mantics of the slot word, ?hired?.
If we replaced526?hired?
by ?fired?
or ?considered?
then the extractionwould be false.To enable such patterns we retain the lexical con-straints on the relation words and slot words.5 Wecollect all patterns together based only on the syn-tactic restrictions and convert the lexical constraintinto a list of words with which the pattern was seen.Example #5 in Figure 3 shows one such lexical list.Can we generalize these lexically-annotated pat-terns further?
Our insight is that we can generalizea list of lexical items to other similar words.
Forexample, if we see a list like {CEO, director, presi-dent, founder}, then we should be able to generalizeto ?chairman?
or ?minister?.Several ways to compute semantically similarwords have been suggested in the literature likeWordnet-based, distributional similarity, etc.
(e.g.,(Resnik, 1996; Dagan et al1999; Ritter et al2010)).
For our proof of concept, we use a simpleoverlap metric with two important Wordnet classes?
Person and Location.
We generalize to these typeswhen our list has a high overlap (> 75%) with hy-ponyms of these classes.
If not, we simply retain theoriginal lexical list without generalization.
Example#4 in Figure 3 is a type-generalized pattern.We combine all syntactic and semantic patternsand sort in descending order based on frequency ofoccurrence in the training set.
This imposes a naturalranking on the patterns ?
more frequent patterns arelikely to give higher precision extractions.3.3 Pattern Matching for ExtractionWe now describe how these open patterns are usedto extract binary relations from a new sentence.
Wefirst match the open patterns with the dependencyparse of the sentence and identify the base nodes forarguments and relations.
We then expand these toconvey all the information relevant to the extraction.As an example, consider the sentence: ?I learnedthat the 2012 Sasquatch music festival is scheduledfor May 25th until May 28th.?
Figure 4 illustrates thedependency parse.
To apply pattern #1 from Figure3 we first match arg1 to ?festival?, rel to ?scheduled?and arg2 to ?25th?
with prep ?for?.
However, (festi-val, be scheduled for, 25th) is not a very meaningfulextraction.
We need to expand this further.5For highest precision extractions, we may also need seman-tic constraints on the arguments.
In this work, we increase ouryield by ignoring the argument-type constraints.learned_VBDI_PRP scheduled_VBNthat_IN festival_NN is_VBZ 25th_NNP 28th_NNPthe_DET Sasquatch_NNP music_NN May_NNP_11 May_NNP_14 2012_CDnsubj ccompcomplm nsubjpass auxpass prep_for prep_untildet num nn nn nn nnFigure 4: A sample dependency parse.
The col-ored/greyed nodes represent all words that are extractedfrom the pattern {arg1} ?nsubjpass?
{rel:postag=VBN}?
{prep ?}?
{arg2}.
The extraction is (the 2012Sasquatch Music Festival; is scheduled for; May 25th).For the arguments we expand on amod, nn, det,neg, prep of, num, quantmod edges to build thenoun-phrase.
When the base noun is not a propernoun, we also expand on rcmod, infmod, partmod,ref, prepc of edges, since these are relative clausesthat convey important information.
For relationphrases, we expand on advmod, mod, aux, auxpass,cop, prt edges.
We also include dobj and iobj in thecase that they are not in an argument.
After identi-fying the words in arg/relation we choose their orderas in the original sentence.
For example, these ruleswill result in the extraction (the Sasquatch music fes-tival; be scheduled for; May 25th).3.4 Comparison with WOEparseOLLIE?s algorithm is similar to that of WOEparse?
both systems follow the basic structure of boot-strap learning of patterns based on dependency parsepaths.
However, there are three significant differ-ences.
WOE uses Wikipedia-based bootstrapping,finding a sentence in a Wikipedia article that con-tains the infobox values.
Since WOE does not haveaccess to a seed relation phrase, it heuristically as-signs all intervening words between the argumentsin the parse as the relation phrase.
This often resultsin under-specified or nonsensical relation phrases.For example, from the sentence ?David Miscavigelearned that after Tom Cruise divorced Mimi Rogers,he was pursuing Nicole Kidman.?
WOE?s heuristicswill extract the relation divorced was pursuing be-tween ?Tom Cruise?
and ?Nicole Kidman?.
OLLIE,in contrast, produces well-formed relation phrasesby basing its templates on REVERB relation phrases.Secondly, WOE does not assign semantic/lexicalrestrictions to its patterns, and thus, has lower preci-sion due to aggressive syntactic generalization.
Fi-nally, WOE is designed to have verb-mediated rela-527tion phrases that do not include nouns, thus missingimportant relations such as ?is the president of?.
Inour experiments (see Figure 5) we find WOEparse tohave lower precision and yield than OLLIE.4 Context Analysis in OLLIEWe now turn to the context analysis component,which handles the problem of extractions that are notasserted as factual in the text.
In some cases, OLLIEcan handle this by extending the tuple representationwith an extra field that turns an otherwise incorrecttuple into a correct one.
In other cases, there is no re-liable way to salvage the extraction, and OLLIE canavoid an error by giving the tuple a low confidence.Cases where OLLIE extends the tuple representa-tion include conditional truth and attribution.
Con-sider sentence #4 in Figure 1.
It is not asserting thatthe earth is the center of the universe.
OLLIE addsan AttributedTo field, which makes the final extrac-tion valid (see OLLIE extraction in Figure 1).
Thisfield indicates who said, suggested, believes, hopes,or doubts the information in the main extraction.Another case is when the extraction is only condi-tionally true.
Sentence #5 in Figure 1 does not assertas factual that (Romney; will be elected; President),so it is an incorrect extraction.
However, addinga condition (?if he wins five states?)
can turn thisinto a correct extraction.
We extend OLLIE to havea ClausalModifier field when there is a dependentclause that modifies the main extraction.Our approach for extracting these additional fieldsmakes use of the dependency parse structure.
Wefind that attributions are marked by a ccomp (clausalcomplement) edge.
For example, in the parse of sen-tence #4 there is a ccomp edge between ?believe?and ?center?.
Our algorithm first checks for the pres-ence of a ccomp edge to the relation node.
However,not all ccomp edges are attributions.
We match thecontext verb (e.g., ?believe?)
with a list of commu-nication and cognition verbs from VerbNet (Schuler,2006) to detect attributions.
The context verb and itssubject then populate the AttributedTo field.Similarly, the clausal modifiers are marked by ad-vcl (adverbial clause) edge.
We filter these lexically,and add a ClausalModifier field when the first wordof the clause matches a list of 16 terms created usinga training set: {if, when, although, because, ...}.OLLIE has high precision for AttributedTo andClausalModifier fields, nearly 98% on a develop-ment set, however, these two fields do not cover allthe cases where an extraction is not asserted as fac-tual.
To handle others, we train OLLIE?s confidencefunction to reduce the confidence of an extraction ifits context indicates it is likely to be non-factual.We use a supervised logistic regression classifierfor the confidence function.
Features include thefrequency of the extraction pattern, the presence ofAttributedTo or ClausalModifier fields, and the po-sition of certain words in the extraction?s context,such as function words or the communication andcognition verbs used for the AttributedTo field.
Forexample, one highly predictive feature tests whetheror not the word ?if?
comes before the extractionwhen no ClausalModifier fields are attached.
Ourtraining set was 1000 extractions drawn evenly fromWikipedia, News, and Biology sentences.5 ExperimentsOur experiments evaluate three main questions.
(1)How does OLLIE?s performance compare with exis-ting state-of-the-art open extractors?
(2) What arethe contributions of the different sub-componentswithin OLLIE?
(3) How do OLLIE?s extractions com-pare with semantic role labeling argument identifi-cation?5.1 Comparison of Open IE SystemsSince Open IE is designed to handle a variety ofdomains, we create a dataset of 300 random sen-tences from three sources: News, Wikipedia and Bi-ology textbook.
The News and Wikipedia test setsare a random subset of Wu and Weld?s test set forWOEparse.
We ran three systems, OLLIE, REVERBand WOEparse on this dataset resulting in a total of1,945 extractions from all three systems.
Two an-notators tagged the extractions as correct if the sen-tence asserted or implied that the relation was true.Inter-annotator agreement was 0.96, and we retainedthe subset of extractions on which the two annotatorsagree for further analysis.All systems associate a confidence value with anextraction ?
ranking with these confidence valuesgenerates a precision-yield curve for this dataset.Figure 5 reports the curves for the three systems.We find that OLLIE has a higher performance, ow-ing primarily to its higher yield at comparable preci-5280.50.60.70.80.910  100  200  300  400  500  600OLLIEReVerbWOEYieldPrecisionparseFigure 5: Comparison of different Open IE systems.
OL-LIE achieves substantially larger area under the curvethan other Open IE systems.sion.
OLLIE finds 4.4 times more correct extractionsthan REVERB and 4.8 times more than WOEparse ata precision of about 0.75.
Overall, OLLIE has 2.7times larger area under the curve than REVERB and1.9 times larger than WOEparse.6 We use the Boot-strap test (Cohen, 1995) to find that OLLIE?s betterperformance compared to the two systems is highlystatistically significant.We perform further analysis to understand the rea-sons behind the high yield from OLLIE.
We find that40% of the OLLIE extractions that REVERB missesare due to OLLIE?s use of parsers ?
REVERB missesthose because its shallow syntactic analysis cannotskip over the intervening clauses or prepositionalphrases between the relation phrase and the argu-ments.
About 30% of the additional yield is thoseextractions where the relation is not between its ar-guments (see instance #1 in Figure 1).
The rest aredue to other causes such as OLLIE?s ability to handlerelationships mediated by nouns and adjectives, orREVERB?s shallow syntactic analysis, etc.
In con-trast, OLLIE misses very few extractions returned byREVERB, mostly due to parser errors.We find that WOEparse misses extractions foundby OLLIE for a variety of reasons.
The primarycause is that WOEparse does not include nouns in re-lation phrases.
It also misses some verb-based pat-terns, probably due to training noise.
In other cases,WOEparse misses extractions due to ill-formed rela-tion phrases (as in the example of Section 3.4: ?di-vorced was pursuing?
instead of the correct relation?was pursuing?
).While the bulk of OLLIE?s extractions in our test6Evaluating recall is difficult at this scale ?
however, sinceyield is proportional to recall, the area differences also hold forthe equivalent precision-recall curves.Relation OLLIE REVERB incr.is capital of 8,566 146 59xis president of 21,306 1,970 11xis professor at 8,334 400 21xis scientist of 730 5 146xFigure 6: OLLIE finds many more correct extractions forrelations that are typically expressed by noun phrases ?up to 146 times that of REVERB.
WOEparse outputs noinstances of these, because it does not allow nouns in therelation.
These results are at point of maximum yield(with comparable precisions around 0.66).set were verb-mediated, our intuition suggests thatthere exist many relationships that are most natu-rally expressed via noun phrases.
To demonstratethis effect, we chose four such relations ?
is capi-tal of, is president of, is professor at, and is scientistof.
We ran our systems on 100 million random sen-tences from the ClueWeb corpus.
Figure 6 reportsthe yields of these four relations.7OLLIE found up to 146 times as many extrac-tions for these relations than REVERB.
BecauseWOEparse does not include nouns in relation phrases,it is unable to extract any instance of these relations.We examine a sample of the extractions to verify thatnoun-mediated extractions are the main reason forthis large yield boost over REVERB (73% of OLLIEextractions were noun-mediated).
High-frequencynoun patterns like ?Obama, the president of the US?,?Obama, the US president?, ?US President Obama?far outnumber sentences of the form ?Obama is thepresident of the US?.
These relations are seldom theprimary information in a sentence, and are typicallymentioned in passing in noun phrases that expressthe relation.For some applications, noun-mediated relationsare important, as they associate people with workplaces and job titles.
Overall, we think of the resultsin Figure 6 as a ?best case analysis?
that illustratesthe dramatic increase in yield for certain relations,due to syntactic scope expansion in Open IE.5.2 Analysis of OLLIEWe perform two control experiments to understandthe value of semantic/lexical restrictions in patternlearning and precision boost due to context analysiscomponent.7We multiply the total number of extractions with precisionon a sample for that relation to estimate the yield.52900.20.40.60.810  10  20  30  40  50  60OLLIEOLLIE[Lex]OLLIE[syn]YieldPrecisionFigure 7: Results on the subset of extractions from pat-terns with semantic/lexical restrictions.
Ablation studyon patterns with semantic/lexical restrictions.
These pat-terns without restrictions (OLLIE[syn]) result in low pre-cision.
Type generalization improves yield compared topatterns with only lexical constraints (OLLIE[lex]).Are semantic restrictions important for open pat-tern learning?
How much does type generalizationhelp?
To answer these questions we compare threesystems ?
OLLIE without semantic or lexical restric-tions (OLLIE[syn]), OLLIE with lexical restrictionsbut no type generalization (OLLIE[lex]) and the fullsystem (OLLIE).
We restrict this experiment to thepatterns where OLLIE adds semantic/lexical restric-tions, rather than dilute the result with patterns thatwould be unchanged by these variants.Figure 7 shows the results of this experiment onour dataset from three domains.
As the curvesshow, OLLIE was correct to add lexical/semanticconstraints to these patterns ?
precision is quite lowwithout the restrictions.
This matches our intuition,since these are not completely general patterns andgeneralizing to all unseen relations results in a largenumber of errors.
OLLIE[lex] performs well thoughat lower yield.
The type generalization helps theyield somewhat, without hurting the precision.
Webelieve that a more data-driven type generalizationthat uses distributional similarity (e.g., (Ritter et al2010)) may help much more.
Also, notice that over-all precision numbers are lower, since these are themore difficult relations to extract reliably.
We con-clude that lexical/semantic restrictions are valuablefor good performance of OLLIE.We also compare our full system to a version thatdoes not use the context analysis of Section 4.
Fig-ure 8 compares OLLIE to a version (OLLIE[pat]) thatdoes not add the AttributedTo and ClausalModifierfields, and, instead of context-sensitive confidencefunction, uses the pattern frequency in the training0.50.60.70.80.910  100  200  300  400  500  600OLLIEOLLIE[pat]YieldPrecisionFigure 8: Context analysis increases precision, raising thearea under the curve by 19%.set as a ranking function.
10% of the sentences havean OLLIE extraction with ClausalModifier and 6%have AttributedTo fields.
Adding ClausalModifiercorrects errors for 21% of extractions that have aClausalModifier and does not introduce any new er-rors.
Adding AttributedTo corrects errors for 55%of the extractions with AttributedTo and introducesan error for 3% of the extractions.
Overall, we findthat OLLIE gives a significant boost to precision overOLLIE[pat] and obtains 19% additional AUC.Finally, we analyze the errors made by OLLIE.Unsurprisingly, because of OLLIE?s heavy relianceon the parsers, parser errors account for a large partof OLLIE?s errors (32%).
18% of the errors are dueto aggressive generalization of a pattern to all un-seen relations and 12% due to incorrect applicationof lexically annotated patterns.
About 14% of the er-rors are due to important context missed by OLLIE.Another 12% of the errors are because of the limita-tions of binary representation, which misses impor-tant information that can only be expressed in n-arytuples.We believe that as parsers become more robustOLLIE?s performance will improve even further.
Thepresence of context-related errors suggests that thereis more to investigate in context analysis.
Finally, inthe future we wish to extend the representation toinclude n-ary extractions.5.3 Comparison with SRLOur final evaluation suggests answers to two im-portant questions.
First, how does a state-of-the-artOpen IE system do in terms of absolute recall?
Sec-ond, how do Open IE systems compare against state-of-the-art SRL systems?SRL, as discussed in Section 2, has a very dif-ferent goal ?
analyzing verbs and nouns to identify530their arguments, then mapping the verb or noun toa semantic frame and determining the role that eachargument plays in that frame.
These verbs and nounsneed not make the full relation phrase, although, re-cent work has shown that they may be convertedto Open IE style extractions with additional post-processing (Christensen et al2011).While a direct comparison between OLLIE anda full SRL system is problematic, we can compareperformance of OLLIE and the argument identifica-tion step of an SRL system.
We set each system thefollowing task ?
?based on a sentence, find all noun-pairs that have an asserted relationship.?
This task ispermissive for both systems, as it does not requirefinding an exact relation phrase or argument bound-ary, or determining the argument roles in a relation.We create a gold standard by tagging a random50 sentences of our test set to identify all pairs ofNPs that have an asserted relation.
We only countedrelation expressed by a verb or noun in the text, anddid not include relations expressed simply with ?of?or apostrophe-s. Where a verb mediates between anargument and multiple NPs, we represent this as abinary relation for all pairs of NPs.For example the sentence, ?Macromoleculestranslocated through the phloem include proteinsand various types of RNA that enter the sieve tubesthrough plasmodesmata.?
has five binary relations.arg1: arg2: relation termMacromolecules phloem translocatedMacromolecules proteins includeMacromolecules types of RNA includetypes of RNA sieve tubes entertypes of RNA plasmodesmata enterWe find an average of 4.0 verb-mediated relationsand 0.3 noun-mediated relations per sentence.
Eval-uating OLLIE against this gold standard helps to an-swer the question of absolute recall: what percent-age of binary relations expressed in a sentence canour systems identify.For comparison, we use a state-of-the-art SRLsystem from Lund University (Johansson andNugues, 2008), which is trained on PropBank(Martha and Palmer, 2002) for its verb-frames andNomBank (Meyers et al2004) for noun-frames.The PropBank version of the system won the verycompetitive 2008 CONLL SRL evaluation.We conduct this experiment by manually compar-LUND OLLIE unionVerb relations 0.58 (0.69) 0.49 (0.55) 0.71 (0.83)Noun relations 0.07 (0.33) 0.13 (0.13) 0.20 (0.33)All relations 0.54 (0.67) 0.47 (0.52) 0.67 (0.80)Figure 9: Recall of LUND and OLLIE on binary relations.In parentheses is recall with oracle co-reference.
Bothsystems identify approximately half of all argument pairs,but have lower recall on noun-mediated relations.ing the outputs of LUND and OLLIE against the goldstandard.
For each pair of NPs in the gold standardwe determine whether the systems find a relationwith that pair of NPs as arguments.
Recall is basedon the percentage of NP pairs where the head nounsmatches head nouns of two different arguments in anextraction or semantic frame.
If the argument valueis conjunctive, we count a match against the headnoun of each item in the list.
We also count caseswhere system output would match the gold standard,given perfect co-reference.Figure 9 shows the recall for OLLIE and LUND,with recall based on oracle co-referential matchesin parentheses.
Our analysis shows strong recallfor both systems for verb-mediated relations: LUNDfinding about two thirds of the argument pairs andOLLIE finding over half.
Both systems have lowrecall for noun-mediated relations, with most ofLUND?s recall requiring co-reference.
We observethat a union of the two systems raises recall to0.71 for verb-mediated relations and 0.83 with co-reference, demonstrating that each system is identi-fying argument pairs that the other missed.It is not surprising that OLLIE has recall of ap-proximately 0.5, since it is tuned for high precisionextraction, and avoids less reliable extractions fromconstructions such as reduced relative clauses andgerunds, or from noun-mediated relations with long-range dependencies.
In contrast, SRL is tuned toidentify the argument structure for nearly all verbsand nouns in a sentence.
The missing recall fromSRL is primarily where it does not identify both ar-guments of a binary relation, or where the correctargument is buried in a long argument phrase, but isnot its head noun.It is surprising that LUND, trained on Nom-Bank, identifies so few noun-mediated argumentpairs without co-reference.
An example will makethis clear.
For the sentence, ?Clarcor, a maker ofpackaging and filtration products, said ...?, the tar-531get relation is between Clarcor and the products itmakes.
LUND identifies a frame maker.01 in whichargument A0 has head noun ?maker?
and A1 is a PPheaded by ?products?, missing the actual name of themaker without co-reference post-processing.
OLLIEfinds the extraction (Clarcor; be a maker of; packag-ing and filtration products) where the heads of botharguments matched those of the target.
In anotherexample, LUND identifies ?his?
and ?brother?
as thearguments of the frame brother.01, rather than theactual names of the two brothers.We can draw several conclusions from this exper-iment.
First, nouns, although less frequently mediat-ing relations, are much harder and both systems arefailing significantly on those ?
OLLIE is somewhatbetter.
Two, neither systems dominates the other;in fact, recall is increased significantly by a unionof the two.
Three, and probably most importantly,significant information is still being missed by bothsystems, and more research is warranted.6 Related WorkThere is a long history of bootstrapping and pat-tern learning approaches in traditional informa-tion extraction, e.g., DIPRE (Brin, 1998), Snow-Ball (Agichtein and Gravano, 2000), Espresso (Pan-tel and Pennacchiotti, 2006), PORE (Wang et al2007), SOFIE (Suchanek et al2009), NELL (Carl-son et al2010), and PROSPERA (Nakashole etal., 2011).
All these approaches first bootstrap databased on seed instances of a relation (or seed datafrom existing resources such as Wikipedia) and thenlearn lexical or lexico-POS patterns to create an ex-tractor.
Other approaches have extended these tolearning patterns based on full syntactic analysis ofa sentence (Bunescu and Mooney, 2005; Suchaneket al2006; Zhao and Grishman, 2005).OLLIE has significant differences from the previ-ous work in pattern learning.
First, and most impor-tantly, these previous systems learn an extractor foreach relation of interest, whereas OLLIE is an openextractor.
OLLIE?s strength is its ability to gener-alize from one relation to many other relations thatare expressed in similar forms.
This happens bothvia syntactic generalization and type generalizationof relation words (sections 3.2.1 and 3.2.2).
This ca-pability is essential as many relations in the test setare not even seen in the training set ?
in early exper-iments we found that non-generalized pattern learn-ing (equivalent to traditional IE) had significantlyless yield at a slightly higher precision.Secondly, previous systems begin with seeds thatconsist of a pair of entities, whereas we also in-clude the content words from REVERB relations inour training seeds.
This results in a much higherprecision bootstrapping set and high rule preci-sion while still allowing morphological variants thatcover noun-mediated relations.
A third difference isin the scale of the training ?
REVERB yields millionsof training seeds, where previous systems had ordersof magnitude less.
This enables OLLIE to learn pat-terns with greater coverage.The closest to our work is the pattern learningbased open extractor WOEparse.
Section 3.4 de-tails the differences between the two extractors.
An-other extractor, StatSnowBall (Zhu et al2009), hasan Open IE version, which learns general but shal-low patterns.
Preemptive IE (Shinyama and Sekine,2006) is a paradigm related to Open IE that firstgroups documents based on pairwise vector cluster-ing, then applies additional clustering to group en-tities based on document clusters.
The clusteringsteps make it difficult for it to scale to large corpora.7 ConclusionsOur work describes OLLIE, a novel Open IE ex-tractor that makes two significant advances overthe existing Open IE systems.
First, it expandsthe syntactic scope of Open IE systems by identi-fying relationships mediated by nouns and adjec-tives.
Our experiments found that for some rela-tions this increases the number of correct extrac-tions by two orders of magnitude.
Second, by an-alyzing the context around an extraction, OLLIE isable to identify cases where the relation is not as-serted as factual, but is hypothetical or conditionallytrue.
OLLIE increases precision by reducing con-fidence in those extractions or by associating addi-tional context in the extractions, in the form of at-tribution and clausal modifiers.
Overall, OLLIE ob-tains 1.9 to 2.7 times more area under precision-yield curves compared to existing state-of-the-artopen extractors.
OLLIE is available for download athttp://openie.cs.washington.edu.532AcknowledgmentsThis research was supported in part by NSF grant IIS-0803481,ONR grant N00014-08-1-0431, DARPA contract FA8750-09-C-0179 and the Intelligence Advanced Research Projects Ac-tivity (IARPA) via Air Force Research Laboratory (AFRL) con-tract number FA8650-10-C-7058.
The U.S. Government is au-thorized to reproduce and distribute reprints for Governmen-tal purposes notwithstanding any copyright annotation thereon.The views and conclusions contained herein are those of the au-thors and should not be interpreted as necessarily representingthe official policies or endorsements, either expressed or im-plied, of IARPA, AFRL, or the U.S. Government.
This researchis carried out at the University of Washington?s Turing Center.We thank Fei Wu and Dan Weld for providing WOE?s codeand Anthony Fader for releasing REVERB?s code.
Peter Clark,Alan Ritter, and Luke Zettlemoyer provided valuable feedbackon the research and Dipanjan Das helped us with state-of-the-art SRL systems.
We also thank the anonymous reviewers fortheir comments on an earlier draft.ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Ex-tracting relations from large plain-text collections.
InProcs.
of the Fifth ACM International Conference onDigital Libraries.ARPA.
1991.
Proc.
3rd Message Understanding Conf.Morgan Kaufmann.ARPA.
1998.
Proc.
7th Message Understanding Conf.Morgan Kaufmann.M.
Banko, M. Cafarella, S. Soderland, M. Broadhead,and O. Etzioni.
2007.
Open information extractionfrom the Web.
In Procs.
of IJCAI.S.
Brin.
1998.
Extracting Patterns and Relations from theWorld Wide Web.
In WebDB Workshop at 6th Interna-tional Conference on Extending Database Technology,EDBT?98, pages 172?183, Valencia, Spain.Razvan C. Bunescu and Raymond J. Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In Proc.
of HLT/EMNLP.Andrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010.
Toward an architecture for never-ending language learning.
In Procs.
of AAAI.Janara Christensen, Mausam, Stephen Soderland, andOren Etzioni.
2011.
An analysis of open informa-tion extraction based on semantic role labeling.
InProceedings of the 6th International Conference onKnowledge Capture (K-CAP ?11).Paul R. Cohen.
1995.
Empirical Methods for ArtificialIntelligence.
MIT Press.Ido Dagan, Lillian Lee, and Fernando C. N. Pereira.1999.
Similarity-based models of word cooccurrenceprobabilities.
Machine Learning, 34(1-3):43?69.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Lan-guage Resources and Evaluation (LREC 2006).Oren Etzioni, Anthony Fader, Janara Christensen,Stephen Soderland, and Mausam.
2011.
Open infor-mation extraction: the second generation.
In Proceed-ings of the International Joint Conference on ArtificialIntelligence (IJCAI ?11).Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of EMNLP.Raphael Hoffmann, Congle Zhang, and Daniel S. Weld.2010.
Learning 5000 relational extractors.
In Pro-ceedings of the 48th Annual Meeting of the Associationfor Computational Linguistics, ACL ?10, pages 286?295.Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke S.Zettlemoyer, and Daniel S. Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In ACL, pages 541?550.Richard Johansson and Pierre Nugues.
2008.
The ef-fect of syntactic representation on semantic role label-ing.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (COLING 08),pages 393?400.Paul Kingsbury Martha and Martha Palmer.
2002.
Fromtreebank to propbank.
In Proceedings of the Third In-ternational Conference on Language Resources andEvaluation (LREC 02).A.
Meyers, R. Reeves, C. Macleod, R. Szekely, V. Zielin-ska, B.
Young, and R. Grishman.
2004.
AnnotatingNoun Argument Structure for NomBank.
In Proceed-ings of LREC-2004, Lisbon, Portugal.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In ACL-IJCNLP ?09: Proceedingsof the Joint Conference of the 47th Annual Meeting ofthe ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Volume2, pages 1003?1011.Ndapandula Nakashole, Martin Theobald, and GerhardWeikum.
2011.
Scalable knowledge harvesting withhigh precision and high recall.
In Proceedings of theFourth International Conference on Web Search andWeb Data Mining (WSDM 2011), pages 227?236.Joakim Nivre and Jens Nilsson.
2004.
Memory-baseddependency parsing.
In Proceedings of the Conferenceon Natural Language Learning (CoNLL-04), pages49?56.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging generic patterns for automatically harvest-ing semantic relations.
In Proceedings of 21st Interna-tional Conference on Computational Linguistics and53344th Annual Meeting of the Association for Computa-tional Linguistics (ACL?06).P.
Resnik.
1996.
Selectional constraints: an information-theoretic model and its computational realization.Cognition.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions withoutlabeled text.
In ECML/PKDD (3), pages 148?163.Alan Ritter, Mausam, and Oren Etzioni.
2010.
A latentdirichlet alation method for selectional preferences.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics (ACL ?10).Karin Kipper Schuler.
2006.
VerbNet: A Broad-Coverage, Comprehensive Verb Lexicon.
Ph.D. thesis,University of Pennsylvania.Y.
Shinyama and S. Sekine.
2006.
Preemptive informa-tion extraction using unrestricted relation discovery.In Procs.
of HLT/NAACL.Fabian M. Suchanek, Georgiana Ifrim, and GerhardWeikum.
2006.
Combining linguistic and statisticalanalysis to extract relations from web documents.
InProcs.
of KDD, pages 712?717.Fabian M. Suchanek, Mauro Sozio, and GerhardWeikum.
2009.
Sofie: a self-organizing frameworkfor information extraction.
In Proceedings of WWW,pages 631?640.Gang Wang, Yong Yu, and Haiping Zhu.
2007.
Pore:Positive-only relation extraction from wikipedia text.In Proceedings of 6th International Semantic WebConference and 2nd Asian Semantic Web Conference(ISWC/ASWC?07), pages 580?594.Fei Wu and Daniel S. Weld.
2010.
Open informationextraction using Wikipedia.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics (ACL ?10).Shubin Zhao and Ralph Grishman.
2005.
Extracting re-lations with integrated information using kernel meth-ods.
In Procs.
of ACL.Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, andJi-Rong Wen.
2009.
StatSnowball: a statistical ap-proach to extracting entity relationships.
In WWW?09: Proceedings of the 18th international conferenceon World Wide Web, pages 101?110, New York, NY,USA.
ACM.534
