Proceedings of the TextGraphs-6 Workshop, pages 37?41,Portland, Oregon, USA, 19-24 June 2011. c?2011 Association for Computational LinguisticsGrawlTCQ: Terminology and Corpora Building by Ranking SimultaneouslyTerms, Queries and Documents using Graph Random WalksCle?ment de GrocSyllabsUniv.
Paris SudLIMSI-CNRScdegroc@limsi.frXavier TannierUniv.
Paris SudLIMSI-CNRSxtannier@limsi.frJavier CoutoSyllabsMoDyCo (UMR 7114, CNRS-UPX)jcouto@syllabs.comAbstractIn this paper, we present GrawlTCQ, a newbootstrapping algorithm for building special-ized terminology, corpora and queries, basedon a graph model.
We model links be-tween documents, terms and queries, and usea random walk with restart algorithm to com-pute relevance propagation.
We have evalu-ated GrawlTCQ on an AFP English corpus of57,441 news over 10 categories.
For corporabuilding, GrawlTCQ outperforms the Boot-CaT tool, which is vastly used in the domain.For 1,000 documents retrieved, we improvemean precision by 25%.
GrawlTCQ has alsoshown to be faster and more robust than Boot-CaT over iterations.1 IntroductionSpecialized terminology and corpora are key re-sources in applications such as machine translationor lexicon-based classification, but they are expen-sive to develop because of the manual validation re-quired.
Bootstrapping is a powerful technique forminimizing the cost of building these resources.In this paper, we present GrawlTCQ1, a bootstrap-ping algorithm for building specialized terminol-ogy, corpora and queries: from a small set of user-provided terms, GrawlTCQ builds the resources viaautomated queries to a search engine.
The algorithmrelies on a graph that encodes the three kinds of enti-ties involved in the procedure (terms, documents andqueries) and relations between them.
We model the1GrawlTCQ stands for Graph RAndom WaLk for Terminol-ogy, Corpora and Queries.relevance propagation in our graph by using a ran-dom walk with restart algorithm.We use BootCaT (Baroni and Bernardini, 2004)as our baseline because it is a similar algorithm thathas been vastly used and validated experimentallyin the domain.
We have evaluated GrawlTCQ andBootCaT on an AFP (Agence France Presse) En-glish corpus of 57,441 news over 10 categories.
Re-sults show that, for corpora building, GrawlTCQsignificantly outperforms the BootCaT algorithm.As this is an on-going work, further work is neededto evaluate terminology and query results.The article is structured as follows: in Section 2,we review the related work in terminology and cor-pora construction using bootstrapping techniques, aswell as random walk applications.
In Section 3,we describe GrawlTCQ.
In Section 4, we evaluateGrawlTCQ and compare its results with those pro-vided by BootCaT.
We conclude in Section 5.2 Related WorkSeveral works using bootstrapping techniques havebeen carried out in terminology and corpora cre-ation.
For example, (Ghani et al, 2005) has built mi-nority language corpora from the web.
The Web-as-Corpus WaCky initiative (Baroni et al, 2009; Fer-raresi et al, 2008; Sharoff, 2006) has built very largeweb-derived corpus in various languages.
They usedpreviously mentioned BootCaT tool to do this.
Asthe quality of the results is strongly dependent onthe quality of seed terms and the underlying searchengine, manual filtering is usually mandatory to en-hance performance.
GrawlTCQ uses a graph to au-tomatically filter out erroneous terms and documents37SeedsQueriesCombinationsRanked documentsSearch engineGraph modelFilter and keep N-best..............................---------------------------------------------RankedtermsRankeddocumentsRankedqueriesUserFigure 1: Components of the GrawlTCQ algorithm.and improve the system?s overall performance.
Themanual filtering cost is therefore drastically reduced.Graph modeling and random walks have beenapplied with success to many different domainsof NLP, such as keyword and sentence extraction(Mihalcea and Tarau, 2004), computer-science arti-cles ranking (Nie et al, 2005), web pages ranking(Haveliwala, 2002; Page et al, 1999; Richardsonand Domingos, 2002), WordNet-based word sensedisambiguation (Agirre and Soroa, 2009) and lexicalsemantic relatedness (Hughes and Ramage, 2007),or set expansion (Wang and Cohen, 2007).
In thispaper, we confirm the relevance of this approach toterminology and corpora bootstrapping.3 Ranking simultaneously Terms, Queriesand Documents3.1 The GrawlTCQ bootstrapping algorithmFigure 1 shows the components of the GrawlTCQalgorithm.
Starting from user provided seed terms2,GrawlTCQ iteratively creates queries, finds docu-ments and extracts new terms.
We model this boot-strapping procedure with a graph that keeps all linksbetween documents, terms and queries.
Our hypoth-2These terms may be easily computed from a list of seed urlsor documents, using terminology extraction techniques.Boxoffice Grammys BBCBoxofficeANDGrammysGrammysANDBBCDOC 1 DOC 2Jackson BeatlesAlbumin query(-1)leads to(-1)contains(-1)TermsQueriesDocumentsFigure 2: Sample subgraph using ?boxoffice?, ?Gram-mys?
and ?BBC?
as seed terms.esis is that the information added will increase theprocedure?s robustness and overall performances.The graph model (see figure 2) is built online.
Ascommon terms will occur in many documents andthus have high centrality, they will end with highscores.
In order to avoid this effect, document-term edges are weighted with a TermHood measure(Kageura and Umino, 1996) such as tfidf or log oddsratio.By using a random walk with restart algorithm,also known as personalized PageRank (Haveliwala,2002), terms, queries and documents are weightedglobally and simultaneously.
At the end of each it-eration of GrawlTCQ, a random walk is computedand the resulting stationary distribution is used torank documents and terms3.
If more documents areneeded, then the algorithm executes one more step.Several parameters can be specified by the user,such as the number of seed terms, the number ofterms composing a query, as well as the number ofdocuments retrieved for each query.
In addition, thealgorithm may use the Internet (with search enginesas Google, Yahoo!
or Bing), an Intranet, or both,as data sources.
When using the web as source, spe-cific algorithms must be used to remove HTML boil-erplate (Finn et al, 2001) and filter un-useful docu-ments (duplicates (Broder, 2000), webspam and er-ror pages (Fletcher, 2004)).3As an additional result, we also obtain a ranked list ofqueries.383.2 Graph WalkConsidering a directed graph G = (V,E), the scoreof a vertex Vi is defined asPR(Vi) = (1?
?
)?0 + ??
?j?In(Vi)PR(Vj)|Out(Vj)|where In(Vi) (resp.
Out(Vi)) are Vi predecessors(resp.
successors).
In the original PageRank algo-rithm, a damping factor ?
of 0.85 has been used andthe personalization vector (or teleportation vector)?0 is distributed uniformly over V .
On the contrary,(Richardson and Domingos, 2002) and (Haveliwala,2002) have proposed to personalize the PageRankaccording to a user query or a chosen topic.
Follow-ing previous work (Page et al, 1999; Mihalcea andTarau, 2004), we have fixed the damping factor to0.854 and the convergence threshold to 10?8.As we have different types of edges carrying dif-ferent relations, we slightly modify the PageRankformula, as in (Wang and Cohen, 2007): when walk-ing away from a node, the random surfer first picksrandomly a relation type and then chooses uniformlybetween all edges of the chosen relation type.
Bias-ing the algorithm to insist more on seed terms is alegitimate lead as these nodes represent the strongbase of our model.
We thus use a custom ?0 distri-bution that spreads weights uniformly over the seedterms instead of the whole set of vertices.4 EvaluationEvaluating the proposed method on the web canhardly be done without laborious manual annota-tion.
Moreover, web-based evaluations are not re-producible as search engines index and rankingfunctions change over time.
This is especially aproblem when evaluating the impact of different pa-rameters of our algorithm.
In this article, we havechosen to carry out an objective and reproducibleevaluation based on a stable and annotated documentcollection.The AFP has provided us an English corpus com-posed of 57,441 news documents written betweenJanuary 1st and March 31, 2010.
We have con-sidered the 17 top-level categories from the IPTC4During our experiments, we haven?t observed any signifi-cant change when modifying this parameter.Id Category #docs01 Arts, culture and entertainment 307402 Crime, law and justice 567503 Disaster and accident 460204 Economy, business and finance 1332108 Human interest 130011 Politics 1784812 Religion and belief 149114 Social issue 176415 Sport 1508916 Unrest, conflicts and war 8589Table 1: AFP corpus categories distribution.standard (http://www.iptc.org).
Documents are cat-egorized in one or more of those categories and areannotated with various metadata, such as keywords.As some categories contained too few documents,we have only kept the 10 largest ones (see table 1).The corpus was then indexed using Apache Lucene(http://lucene.apache.org) in order to create a basicsearch engine5.
This setup has several advantages:first, the document collection is stable and quantifi-able.
Documents are clean text written in a journal-istic style.
As they are already annotated, severalautomatic evaluations can be run with different pa-rameters.
Finally, querying the search engine andretrieving documents can be done efficiently.
How-ever, note that, as the document collection is lim-ited, queries might return few or no results (which israrely the case on the web).We have used the BootCaT algorithm as our base-line.
To the best of our knowledge this is the first at-tempt to rigorously evaluate BootCaT performances.We have compared both algorithms in exactly thesame conditions, on a task-based experiment: to re-trieve 50, 100, 300, 500 and 1000 documents foreach category, independently of the number of itera-tions done.To be as close as possible to the original BootCaTalgorithm, we have weighted document-term edgesby log odds ratio.
This measure allows us to dis-tinguish common terms by using a reference back-ground corpus.
In all our experiments, we have usedthe ukWac corpus (Ferraresi et al, 2008), a verylarge web-derived corpus, for this purpose.In order to select initial seed terms we have useddocuments?
metadata.
We have computed the fre-5All normalization features except lower-casing were dis-abled to allow ease of reproducibility.3950 150 250 350 450 550 650 750 850 950Number of documents0.10.20.30.40.50.6MeanPrecision/ RecallGrawlTCQ PrecisionGrawlTCQ RecallBootCaT PrecisionBootCaT Recall1 5 10# iter0100020003000# docsGRBCFigure 3: Mean precision and recall at 50, 100, 300, 500and 1000 documents (inset: Mean number of documents/ number of iterations)quency of occurrences of a keyword in a categoryand have then divided this score by the sum of oc-currences in all other categories.
This strategy leadsto relevant seed terms that are not necessarily ex-clusive to a category.
For instance, selected seedsfor the 4th category are: economics, summary, rate,opec, distress, recession, zain, jal, gold, and spyker.We have fixed a number of parameters for our ex-periments: at each iteration, the top-10 seeds are se-lected (either from the initial set or from newly ex-tracted terms).
Queries are composed of 2 seeds, all45 possible combinations6 are used and a total of 10documents are retrieved for each query.All scores are averaged over the 10 categories.As can be seen in figure 3, GrawlTCQ shows muchmore robustness and outperforms BootCaT by 25%precision at 1000 documents.
Detailed results foreach category are shown in table 2 and confirm therelevance of our approach.
Interestingly, BootCaTand GrawlTCQ have very low precisions for the 14thcategory (Social issue).
Documents found in thiscategory are often ambiguous and both algorithmsfail to extract the domain terminology.
We have alsoplotted the number of documents in function of thenumber of iterations as shown in figure 3 (inset).The curve clearly shows that GrawlTCQ yields more6When running the same experiment with randomly selectedtuples several times, we have found similar results when aver-aging all runs output.CatIdP@50 P@100 P@300 P@500 P@1000GR BC GR BC GR BC GR BC GR BC01 0.58 0.50 0.57 0.30 0.43 0.12 0.35 0.08 0.23 0.0502 0.44 0.60 0.45 0.33 0.46 0.17 0.44 0.10 0.34 0.0703 0.82 0.82 0.99 0.81 0.89 0.41 0.66 0.26 0.54 0.1404 0.86 0.80 0.82 0.85 0.84 0.55 0.78 0.34 0.79 0.1908 0.79 0.79 0.44 0.48 0.23 0.42 0.17 0.40 0.20 0.3911 0.76 0.78 0.79 0.81 0.87 0.71 0.57 0.64 0.57 0.5612 0.46 0.54 0.35 0.27 0.20 0.10 0.17 0.06 0.15 0.0314 0.08 0.24 0.13 0.10 0.06 0.04 0.04 0.02 0.04 0.0215 1.0 1.0 1.0 1.0 0.92 0.78 0.87 0.67 0.81 0.3916 0.82 0.56 0.81 0.49 0.71 0.21 0.72 0.15 0.70 0.13Table 2: Precision at various cutoffs by categorydocuments at a faster rate.
This is due to the seed se-lection process: GrawlTCQ?s queries lead to manydocuments while BootCaT queries often lead to fewor no documents.
Moreover, as we can see in figure3, while fetching more documents faster, the meanprecision of GrawlTCQ is still higher than the Boot-CaT one which shows that selected seeds are, at thesame time, more prolific and more relevant.5 ConclusionIn this paper, we have tackled the problem of ter-minology and corpora bootstrapping.
We have pro-posed GrawlTCQ, an algorithm that relies on agraph model including terms, queries, and docu-ments to track each entity origin.
We have used arandom walk algorithm over our graph in order toglobally and simultaneously compute a ranking foreach entity type.
We have evaluated GrawlTCQ on alarge news dataset and have shown interesting gainover the BootCaT baseline.
We have especially ob-tained better results without any human intervention,reducing radically the cost of manual filtering.
Weare considering several leads for future work.
First,we must evaluate GrawlTCQ for query and termranking.
Then, while preliminary experiments haveshown very promising results on the web, we wouldlike to setup a large scale rigorous evaluation.
Fi-nally, we will conduct further experiments on edgesweighting and seed terms selection strategies.AcknowledgmentsWe would like to thank the AFP for providing usthe annotated news corpus.
This work was par-tially funded by the ANR research project ANR-08-CORD-013.40ReferencesEneko Agirre and Aitor Soroa.
2009.
PersonalizingPageRank for word sense disambiguation.
In Proceed-ings of the 12th Conference of the European Chapterof the Association for Computational Linguistics on -EACL, 2009.Marco Baroni and Silvia Bernardini.
2004.
BootCaT:Bootstrapping Corpora and Terms from the Web.
InProceedings of the LREC 2004 conference.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky Wide Web : A Col-lection of Very Large Linguistically Processed Web-Crawled Corpora.
In Proceedings of the LREC 2009conference, volume 43, pages 209?226.Andrei Z Broder.
2000.
Identifying and Filtering Near-Duplicate Documents.
In Proceedings of the 11th An-nual Symposium on Combinatorial Pattern Matching,pages 1?10, London, UK.
Springer-Verlag.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukwac, a very large web-derived corpus of english.In Proceedings of the 4th Web as Corpus Workshop(WAC-4), pages 47?54.Aidan Finn, Nicholas Kushmerick, and Barry Smyth.2001.
Fact or fiction: Content classification for dig-ital libraries.
In DELOS Workshop: Personalisationand Recommender Systems in Digital Libraries.William H Fletcher.
2004.
Making the Web More Usefulas a Source for Linguistic Corpora.
Corpus Linguisticsin North America, (January 2003):191?205.Rayid Ghani, Rosie Jones, and Dunja Mladenic.
2005.Building Minority Language Corpora by Learning toGenerate Web Search Queries.
Knowl.
Inf.
Syst.,7(1):56?83.Taher H. Haveliwala.
2002.
Topic-sensitive PageRank.Proceedings of the eleventh international conferenceon World Wide Web - WWW ?02, page 517.Thad Hughes and Daniel Ramage.
2007.
Lexical Se-mantic Relatedness with Random Graph Walks.
InProceedings of EMNLP, 2007, pages 581?589.Kyo Kageura and Bin Umino.
1996.
Methods of au-tomatic term recognition: A review.
Terminology,3(2):259?289.Rada Mihalcea and Paul Tarau.
2004.
TextRank bringingorder into text.
In Proceedings of EMNLP, pages 404?411.
Barcelona: ACL.Zaiqing Nie, Yuanzhi Zhang, J.R. Wen, and W.Y.
Ma.2005.
Object-level ranking: Bringing order to webobjects.
In Proceedings of the 14th international con-ference on World Wide Web, pages 567?574.
ACM.Lawrence Page, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1999.
The PageRank Citation Ranking:Bringing Order to the Web.
Technical report, StanfordInfoLab.M.
Richardson and P. Domingos.
2002.
The intelligentsurfer: Probabilistic combination of link and contentinformation in pagerank.
Advances in Neural Infor-mation Processing Systems, 2:1441?1448.Serge Sharoff.
2006.
Creating general-purpose corporausing automated search engine queries.
M. Baroni, S.Bernardini (eds.)
WaCky!
Working papers on the Webas Corpus, Bologna, 2006, pages 63?98.Richard C. Wang and William W. Cohen.
2007.Language-independent set expansion of named enti-ties using the web.
Proceedings of IEEE InternationalConference on Data Mining (ICDM 2007), Omaha,NE, USA.
2007.41
