Proceedings of the 2nd Workshop on Predicting and Improving Text Readability for Target Reader Populations, pages 78?84,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Pilot Study on Readability Prediction with Reading TimeHitoshi Nishikawa, Toshiro Makino and Yoshihiro MatsuoNTT Media Intelligence Laboratories, NTT Corporation1-1 Hikari-no-oka, Yokosuka-shi, Kanagawa, 239-0847 Japan{nishikawa.hitoshimakino.toshiro, matsuo.yoshihiro}@lab.ntt.co.jpAbstractIn this paper we report the results of a pi-lot study of basing readability predictionon training data annotated with readingtime.
Although reading time is known tobe a good metric for predicting readabil-ity, previous work has mainly focused onannotating the training data with subjec-tive readability scores usually on a 1 to5 scale.
Instead of the subjective assess-ments of complexity, we use the more ob-jective measure of reading time.
We createand evaluate a predictor using the binaryclassification problem; the predictor iden-tifies the better of two documents correctlywith 68.55% accuracy.
We also report acomparison of predictors based on readingtime and on readability scores.1 IntroductionSeveral recent studies have attempted to predictthe readability of documents (Pitler and Nenkova,2008; Burstein et al 2010; Nenkova et al 2010;Pitler et al 2010; Tanaka-Ishii et al 2010).
Pre-dicting readability has a very important role in thefield of computational linguistics and natural lan-guage processing:?
Readability prediction can help users retrieveinformation from the Internet.
If the read-ability of documents can be predicted, searchengines can rank the documents according toreadability, allowing users to access the infor-mation they need more easily (Tanaka-Ishii etal., 2010).?
The predicted readability of a document canbe used as an objective function in naturallanguage applications such as machine trans-lation, automatic summarization, and docu-ment simplification.
Machine translation canuse a readability predictor as a part of the ob-jective function to make more fluent transla-tions (Nenkova et al 2010).
The readabil-ity predictor can also be used as a part of asummarizer to generate readable summaries(Pitler et al 2010).
Document simplificationcan help readers understand documents moreeasily by automatically rewriting documentsthat are not easy to read (Zhu et al 2010;Woodsend and Lapata, 2011).
This is pos-sible by paraphrasing the sentences so as tomaximize document readability.?
Readability prediction can be used for educa-tional purposes (Burstein et al 2010).
It canassess human-generated documents automat-ically.Most studies build a predictor that outputs areadability score (generally 1-5 scale) or a clas-sifier or ranker that identifies which of two doc-uments has the better readability.
Using textualcomplexity to rank documents may be adequatefor several applications in the fields of informationretrieval, machine translation, document simplifi-cation, and the assessment of human-written doc-uments.
Approaches based on complexity, how-ever, do not well support document summariza-tion.In the context of automatic summarization,users want concise summaries to understand theimportant information present in the documents asrapidly as possible?
to create summaries that canbe read as quickly as possible, we need a func-tion that can evaluate the quality of the summaryin terms of reading time.78To achieve this goal, in this paper, we show theresults of our pilot study on predicting the readingtime of documents.
Our predictor has two featuresas follows:1.
Our predictor is trained by documents di-rectly annotated with reading time.
Whileprevious work employs subjective assess-ments of complexity, we directly use thereading time to build a predictor.
As a pre-dictor, we adopt Ranking SVM (Joachims,2002).2.
The predictor predicts the reading time with-out recourse to features related to documentlength since our immediate goal is text sum-marization.
A preliminary experiment con-firms that document length is effective forreadability prediction confirming the workby (Pitler and Nenkova, 2008; Pitler et al2010).
Summarization demands that the pre-dictor work well regardless of text length.This is the first report to show that the result oftraining a predictor with data annotated by read-ing time is to improve the quality of automaticreadability prediction.
Furthermore, we reportthe result of the comparison between our read-ing time predictor and a conventional complexity-based predictor.This paper is organized as follows: Section 2describes related work.
Section 3 describes thedata used in the experiments.
Section 4 describesour model.
Section 5 elaborates the features forpredicting document readability based on read-ing time.
Section 6 reports our evaluation exper-iments.
We conclude this paper and show futuredirections in Section 7.2 Related WorkRecent work formulates readability prediction asan instance of a classification, regression, or rank-ing problem.
A document is regarded as a mix-ture of complex features and its readability is pre-dicted by the use of machine learning (Pitler andNenkova, 2008; Pitler et al 2010; Tanaka-Ishii etal., 2010).
Pitler and Nenkova (2008) built a clas-sifier that employs various features extracted froma document and newswire documents annotatedwith a readability score on a 1 to 5 scale.
They in-tegrated complex features by using SVM and iden-tified the better document correctly with 88.88%accuracy.
They reported that the log likelihood ofa document based on its discourse relations, thelog likelihood of a document based on n-gram, theaverage number of verb phrases in sentences, thenumber of words in the document were good in-dicators on which to base readability prediction.Pitler et al (2010) used the same framework topredict the linguistic quality of a summary.
In thefield of automatic summarization, linguistic qual-ity has been assessed manually and hence to auto-mate the assessment is an important research prob-lem (Pitler et al 2010).
A ranker based on Rank-ing SVM has been constructed (Joachims, 2002)and identified the better of two summaries cor-rectly with an accuracy of around 90%.
Tanaka-Ishii et al (2010) also built a ranker to predict therank of documents according to readability.
WhileTanaka-Ishii et alused word-level features for theprediction, Pitler and Nenkova (2008) and Pitleret al (2010) also leveraged sentence-level fea-tures and document-level features.
In this paper,we extend their findings to predict readability.
Weelaborate our feature set in Section 5.
While allof them either classify or rank the documents byassigning a readability score on a 1-5 scale, ourresearch goal is to build a predictor that can alsoestimate the reading time.In the context of multi-document summariza-tion, the linguistic quality of a summary is pre-dicted to order the sentences extracted from theoriginal documents (Barzilay and Lapata, 2005;Lapata, 2006; Barzilay and Lapata, 2008).
Inmulti-document summarization, since sentencesare extracted from the original documents withoutregard for context, they must be ordered in someway to make the summary coherent.
One of themost important features for ordering sentences isthe entity grid suggested by Barzilay and Lapata(2005; 2008).
It captures transitions in the seman-tic roles of the noun phrases in a document, andcan predict the quality of an order of the sentenceswith high accuracy.
It was also used as an im-portant feature in the work by Pitler and Nenkova(2008) and Piter et al (2010) to predict the read-ability of a document.
Burstein et al (2010) usedit for an educational purpose, and used it to predict79the readability of essays.
Lapata (Lapata, 2006)suggested the use of Kendall?s Tau as an indicatorof the quality of a set of sentences in particular or-der; she also reported that self-paced reading timeis a good indicator of quality.
While Lapata fo-cuses on sentence ordering, our research goal is topredict the overall quality of a document in termsof reading time.3 DataTo build a predictor that can estimate the read-ing time of a document, we made a collectionof documents and annotated each with its read-ing time and readability score.
We randomly se-lected 400 articles from Kyoto Text Corpus 4.0 1.The corpus consists of newswire articles writtenin Japanese and annotated with word boundaries,part-of-speech tags and syntactic structures.
Wedeveloped an experimental system that showed ar-ticles for each subject and gathered reading times.Each article was read by 4 subjects.
All subjectsare native speakers of Japanese.Basically, we designed our experiment follow-ing Pitler and Nenkova (2008).
The subjects wereasked to use the system to read the articles.
Theycould read each document without a time limit, theonly requirement being that they were to under-stand the content of the document.
While the sub-jects were reading the article, the reading time wasrecorded by the system.
We didn?t tell the subjectsthat the time was being recorded.To prevent the subjects from only partially read-ing the document and raise the reliability of the re-sults, we made a multiple-choice question for eachdocument; the answer was to be found in the doc-ument.
This was used to weed out unreliable re-sults.After the subjects read the document, they wereasked to answer the question.Finally, the subjects were asked questions re-lated to readability as follows:1.
How well-written is this article?2.
How easy was it to understand?3.
How interesting is this article?Following the work by Pitler and Nenkova(2008), the subjects answered by selecting a value1http://nlp.ist.i.kyoto-u.ac.jp/EN/between 1 and 5, with 5 being the best and 1 be-ing the worst and we used only the answer to thefirst question (How well-written is this article?)
asthe readability score.
We dropped the results inwhich the subjects gave the wrong answer to themultiple-choice question.
Finally, we had 683 tu-ples of documents, reading times, and readabilityscores.4 ModelTo predict the readability of a document accordingto reading time, we use Ranking SVM (Joachims,2002).
A target document is converted to a featurevector as explained in Section 5, then the predictorranks two documents.
The predictor assigns a realnumber to a document as its score; ranking is doneaccording to score.
In this paper, a higher scoremeans better readability, i.e., shorter reading time.5 FeaturesIn this section we elaborate the features used topredict the reading time.
While most of them wereintroduced in previous work, see Section 3, theword level features are introduced here.5.1 Word-level FeaturesCharacter Type (CT)Japanese sentences consist of several types ofcharacters: kanji, hiragana, katakana, and Romanletters.
We use the ratio of the number of kanji tothe number of hiragana as a feature of the docu-ment.Word Familiarity (WF)Amano and Kondo (2007) developed a list ofwords annotated with word familiarity; it indicateshow familiar a word is to Japanese native speakers.The list is the result of a psycholinguistic experi-ment and the familiarity ranges from 1 to 7, with7 being the most familiar and 1 being the least fa-miliar.
We used the average familiarity of wordsin the document as a feature.5.2 Sentence-level FeaturesLanguage Likelihood (LL)Language likelihood based on an n-gram languagemodel is widely used to generate natural sen-tences.
Intuitively, a sentence whose languagelikelihood is high will have good readability.
We80made a trigram language model from 17 years(1991-2007) of Mainichi Shinbun Newspapers byusing SRILM Toolkit.
Since the language modelassigns high probability to shorter documents, wenormalized the probability by the number of wordsin a document.Syntactic Complexity (TH/NB/NC/NP)Schwarm and Ostendorf (2005) suggested thatsyntactic complexity of a sentence can be used asa feature for reading level assessment.
We use thefollowing features as indicators of syntactic com-plexity:?
The height of the syntax tree (TH): we usethe height of the syntax tree as an indicator ofthe syntactic complexity of a sentence.
Com-plex syntactic structures demand that readersmake an effort to interpret them.
We use theaverage, maximum and minimum heights ofsyntax trees in a document as a feature.?
The number of bunsetsu (NB): in Japanesedependency parsing, syntactic relations aredefined between bunsetsu; they are almostthe same as Base-NP (Veenstra, 1998) withpostpositions.
If a sentence has a lot of bun-setsu, it can have a complex syntactic struc-ture.
We use the average, maximum and min-imum number of them as a feature.?
The number of commas (NC): a comma sug-gests a complex syntax structure such as sub-ordinate and coordinate clauses.
We use theaverage, maximum and minimum number ofthem as a feature.?
The number of predicates (NP): intuitively,a sentence can be syntactically complex if ithas a lot of predicates.
We use the average,maximum and minimum number of them asa feature.5.3 Document-level FeaturesDiscourse Relations (DR)Pitler and Nenkova (2008) used discourse rela-tions of the Penn Discourse Treebank (Prasadet al 2008) as a feature.
Since our corpusdoesn?t have human-annotated discourse relationsbetween the sentences, we use the average num-ber of connectives per sentence as a feature.
In-tuitively, the explicit discourse relations indicatedby the connectives will yield better readability.Entity Grid (EG)Along with the previous work (Pitler andNenkova, 2008; Pitler et al 2010), we use entitygrid (Barzilay and Lapata, 2005; Barzilay and La-pata, 2008) as a feature.
We make a vector whoseelement is the transition probability between syn-tactic roles (i.e.
subject, object and other) of thenoun phrases in a document.
Since our corpusconsists of Japanese documents, we use postpo-sitions to recognize the syntactic role of a nounphrase.
Noun phrases with postpositions ?Ha?
and?Ga?
are recognized as subjects.
Noun phraseswith postpositions ?Wo?
and ?Ni?
are recognizedas objects.
Other noun phrases are marked asother.
We combine the entity grid vector to form afinal feature vector for predicting reading time.Lexical Cohesion (LC)Lexical cohesion is one of the strongest featuresfor predicting the linguistic quality of a summary(Pitler et al 2010).
Following their work, weleverage the cosine similarity of adjacent sen-tences as a feature.
To calculate it, we makea word vector by extracting the content words(nouns, verbs and adjectives) from a sentence.
Thefrequency of each word in the sentence is used asthe value of the sentence vector.
We use the aver-age, maximum and minimum cosine similarity ofthe sentences as a feature.6 ExperimentsThis section explains the setting of our experi-ment.
As mentioned above, we adopted RankingSVM as a predictor.
Since we had 683 tuples (doc-uments, reading time and readability scores), wemade 683C2 = 232, 903 pairs of documents forRanking SVM.
Each pair consists of two docu-ments where one has a shorter reading time thanthe other.
The predictor learned which parameterswere better at predicting which document wouldhave the shorter reading time, i.e.
higher score.We performed a 10-fold cross validation on thepairs consisting of the reading time explained inSection 3 and the features explained in Section 5.In order to analyze the contribution of each feature81Features AccuracyALL 68.45TH + EG + LC 68.55Character Type (CT) 52.14Word Familiarity (WF) 51.30Language Likelihood (LL) 50.40Height of Syntax Tree (TH) 61.86Number of Bunsetsu (NB) 51.54Number of Commas (NC) 47.07Number of Predicates (NP) 52.82Discourse Relations (DR) 48.04Entity Grid (EG) 67.74Lexical Cohesion (LC) 61.63Document Length 69.40Baseline 50.00Table 1: Results of proposed reading time predic-tor.to prediction accuracy, we adopted a linear kernel.The range of the value of each feature was normal-ized to lie between -1 and 1.6.1 Classification based on reading timeTable 1 shows the results yielded by the read-ing time predictor.
ALL indicates the accuracyachieved by the classifier with all features ex-plained in Section 5.
At the bottom of Table 1,Baseline shows the accuracy of random classifica-tion.
As shown in Table 1, since the height of syn-tax tree, entity grid and lexical cohesion are goodindicators for the prediction, we combined thesefeatures.
TH + EG + LC indicates that this combi-nation achieves the best performance.As to individual features, most of them couldn?tdistinguish a better document from a worse one.CT, WF and LL show similar performance toBaseline.
The reason why these features failed toclearer identify the better of the pair could be be-cause the documents are newswire articles.
Theratio between kanji and hiragana, CT, is similar inmost of the articles and hence it couldn?t identifythe better document.
Similarly, there isn?t so muchof a difference among the documents in terms ofword familiarity, WF.
The language model used,LL, was not effective against the documents testedbut it is expected that it would useful if the targetdocuments came from different fields.Among the syntactic complexity features, THoffers the best performance.
Since its learnedfeature weight is negative, the result shows thata higher syntax tree causes longer reading time.While TH has shows good performance, NB, NCand NP fail to offer any significant advantage.
Aswith the word-level features, there isn?t so muchof a difference among the documents in terms ofthe values of these features.
This is likely becausemost of the newswire articles are written by ex-perts for a restricted field.Among the document-level features, EG andLC show good performance.
While Pitler andNenkova (2008) have shown that the discourse re-lation feature is strongest at predicting the linguis-tic quality of a document, DR shows poor perfor-mance.
Whereas they modeled the discourse rela-tions by a multinomial distribution using human-annotated labels, DR was simply the number ofconnectives in the document.
A more sophisti-cated approach will be needed to model discourse.EG and LC show the best prediction perfor-mance of the single features, which agrees withprevious work (Pitler and Nenkova, 2008; Pitleret al 2010).
While, as shown above, most of thesentence-level features don?t have good discrimi-native performance, EG and LC work well.
Sincethese features can work well in homogeneous doc-uments like newswire articles, it is reasonable toexpect that they will also work well in heteroge-neous documents from various domains.We also show the classification result achievedwith document length.
Piter and Nenkova (2008)have shown that document length is a strong indi-cator for readability prediction.
We measure docu-ment length by three criteria: the number of char-acters, the number of words and the number ofsentences in the document.
We used these valuesas features and built a predictor.
While the docu-ment length has the strongest classification perfor-mance, the predictor with TH + EG + LC showsequivalent performance.6.2 Classification based on readability scoreWe also report that the result of the classificationbased on the readability score in Table 2.
Alongwith the result of the reading time, we testedALL and TH + EG + LC, and the single features.While DR shows poor classification performancein terms of reading time, it shows the best classi-82Features AccuracyALL 57.25TH + DR + EG + LC 56.51TH + EG + LC 56.50Character Type (CT) 51.96Word Familiarity (WF) 51.50Language Likelihood (LL) 50.68Height of Syntax Tree (TH) 55.77Number of Bunsetsu (NB) 52.99Number of Commas (NC) 51.50Number of Predicates (NP) 52.56Discourse Relations (DR) 58.14Entity Grid (EG) 56.14Lexical Cohesion (LC) 55.77Document Length 56.83Baseline 50.00Table 2: A result of classification based on read-ability score.Cor.
coef.Reading Time 0.822Readability Score 0.445Table 3: Correlation coefficients of the readingtime and readability score between the subjects.We calculated the coefficient for each pair of sub-jects and then averaged them.fication performance as regards readability score.Hence we add the result of TH + DR + EG + LC.It agrees with the findings showed by Pitler andNenkova (2008) in which they have shown dis-course relation is the best feature for predicting thereadability score.In general, the same features used for classifica-tion based on the reading time work well for pre-dicting the readability score.
TH and EG, LC havegood prediction performance.6.3 Variation in reading time vs. variation inreadability scoreWe show the correlation between the subjects interms of the variation in reading time and read-ability score in Table 3.
As shown, the readingtime shows much higher correlation (less varia-tion) than the readability score.
This agrees withthe findings shown by Lapata (2006) in whichthe reading time is a better indicator for read-ability prediction.
Since the readability scorevaries widely among the subjects, training be-comes problematic with lowers predictor perfor-mance.The biggest difference between the predictionof the reading time and readability score is theeffect of feature DR. One hypothesis that couldexplain the difference is that the use of connec-tives works as a strong sign that the document hasa good readability score?it doesn?t necessarilyimply that the document has good readability?for the subjects.
That is, the subjects perceivedthe documents with more connectives as readable,however, those connectives contribute to the read-ing time.
Of course, our feature about discourserelations is just based on their usage frequency andhence more precise modeling could improve per-formance.7 Conclusion and Future WorkThis paper has described our pilot study of read-ability prediction based on reading time.
With au-tomatic summarization in mind, we built a predic-tor that can predict the reading time, and read-ability, of a document.
Our predictor identifiedthe better of two documents with 68.55% accuracywithout using features related to document length.The following findings can be extracted fromthe results described above:?
The time taken to read documents can bepredicted through existing machine learningtechnique and the features extracted fromtraining data annotated with reading time(Pitler and Nenkova, 2008; Pitler et al2010).?
As Lapata (2006) has shown, reading time isa highly effective indicator of readability.
Inour experiment, reading time showed goodagreement among the subjects and hencemore coherent prediction results can be ex-pected.Future work must proceed in many directions:1.
Measuring more precise reading time is oneimportant problem.
One solution is to use aneye tracker; it can measure the reading timemore accurately because it can capture when83the subject finishes reading a document.
Inorder to prepare the data used in this paper,we set questions so as to identify and dropunreliable data.
The eye tracker could allevi-ate this effort.2.
Testing the predictor in another domain isnecessary for creating practical applications.We tested the predictor only in the domainof newswire articles, as described earlier, anddifferent results might be recorded in do-mains other than newswire articles.3.
Improving the accuracy of the predictor isalso important.
There could be other fea-tures associated with readability prediction.We plan to explore other features.4.
Applying the predictor to natural languagegeneration tasks is particularly important.
Weplan to integrate our predictor into a summa-rizer and evaluate its performance.ReferencesShigeaki Amano and Tadahisa Kondo.
2007.
Reliabil-ity of familiarity rating of ordinary japanese wordsfor different years and places.
Behavior ResearchMethods, 39(4):1008?1011.Regina Barzilay and Mirella Lapata.
2005.
Model-ing local coherence: an entity-based approach.
InProceedings of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics (ACL), pages141?148.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Jill Burstein, Joel Tetreault, and Slava Andreyev.
2010.Using entity-based features to model coherence instudent essays.
In Proceedings of Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics (NAACL-HLT), pages681?684.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Proceedings of the ACMConference on Knowledge Discovery and Data Min-ing (KDD), pages 133?142.Mirella Lapata.
2006.
Automatic evaluation of infor-mation ordering: Kendall?s tau.
Computational Lin-guistics, 32(4):471?484.Ani Nenkova, Jieun Chae, Annie Louis, and EmilyPitler.
2010.
Structural features for predicting thelinguistic quality of text: Applications to machinetranslation, automatic summarization and human-authored text.
In Emiel Krahmer and TheunemMariet, editors, Empirical Methods in Natural Lan-guage Generation: Data-oriented Methods and Em-pirical Evaluation, pages 222?241.
Springer.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 186?195.Emily Pitler, Annie Louis, and Ani Nenkova.
2010.Automatic evaluation of linguistic quality in multi-document summarization.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 544?554.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The penn discourse treebank 2.0.
InProceedings of the 6th International Conference onLanguage Resources and Evaluation (LREC).Sarah Schwarm and Mari Ostendorf.
2005.
Readinglevel assessment using support vector machines andstatistical language models.
In Proceedings of the43rd Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 523?530.Kumiko Tanaka-Ishii, Satoshi Tezuka, and Hiroshi Ter-ada.
2010.
Sorting by readability.
ComputationalLinguistics, 36(2):203?227.Jorn Veenstra.
1998.
Fast np chunking using memory-based learning techniques.
In Proceedings of the8th Belgian-Dutch Conference on Machine Learn-ing (Benelearn), pages 71?78.Kristian Woodsend and Mirella Lapata.
2011.
Learn-ing to simplify sentences with quasi-synchronousgrammar and integer programming.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages409?420.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proceedings of the23rd International Conference on ComputationalLinguistics (Coling 2010), pages 1353?1361.84
