Proceedings of the 6th Workshop on Statistical Machine Translation, pages 490?495,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsCEU-UPV English?Spanish system for WMT11Francisco Zamora-Mart??nezD.
F?
?sica, Matema?tica, y Computacio?nUniversidad CEU-Cardenal HerreraAlfara del Patriarca (Valencia), Spainfzamora@dsic.upv.esM.J.
Castro-BledaD.
Sistemas Informa?ticos y Computacio?nUniversitat Polite`cnica de Vale`nciaValencia, Spainmcastro@dsic.upv.esAbstractThis paper describes the system presented forthe English-Spanish translation task by thecollaboration between CEU-UCH and UPVfor 2011 WMT.
A comparison of indepen-dent phrase-based translation models interpo-lation for each available training corpora weretested, giving an improvement of 0.4 BLEUpoints over the baseline.
Output N -best listswere rescored via a target Neural NetworkLanguage Model.
An improvement of oneBLEU point over the baseline was obtainedadding the two features, giving 31.5 BLEUand 57.9 TER for the primary system, com-puted over lowercased and detokenized out-puts.
The system was positioned second in thefinal ranking.1 IntroductionThe goal of Statistical Machine Translation (SMT)is to translate a sentence between two languages.Giving the source language sentence f , it would betranslated to an equivalent target language sentencee.
The most extended formalization is done via log-linear models (Papineni et al, 1998; Och and Ney,2002) as follows:e?
= arg maxeK?k=1?khk(f , e) (1)where hk(f , e) is a score function representing animportant feature for the translation of f into e, Kis the number of models (or features) and ?k arethe weights of the log-linear combination.
Typically,the weights ?k are optimized during the tuning stagewith the use of a development set.SMT systems rely on a bilingual sentence alignedtraining corpus.
These sentences are aligned at theword level (Brown et al, 1993), and after that, dif-ferent hk feature functions are trained.
In some prac-tical cases, the out-of-domain training data is largerthan the in-domain training data.
In these cases thetarget Language Model (LM) is composed of a lin-ear interpolation of independent LMs, one for eachavailable training domain or corpus.
Nevertheless,the training of phrase-based translation models is anopen problem in these cases.Some recent works (Resnik and Smith, 2003; Ya-suda et al, ; Koehn and Schroeder, 2007; Matsoukaset al, 2009; Foster et al, 2010; Sanchis-Trillesand Casacuberta, 2010) related to corpus weight-ing, make use of data selection, data weighting,and translation model adaptation to overcome thisproblem.
In this work, we explore a simple cor-pus weighting technique to interpolate any numberof different phrase tables.
Two different approachesare tested, obtaining similar performance.
On theone hand, a count-based smoothing technique thatapplies a weight to the counting of phrases and lexi-cal links depending on the relevance of each corpus.On the other hand, a linear interpolation of indepen-dent trained phrase tables.Another important feature of this work isthe use of Neural Network Language Models(NN LMs) (Bengio, 2008).
This kind of LMs hasbeen successfully applied in some connectionist ap-proaches to language modeling (Bengio et al, 2003;Castro-Bleda and Prat, 2003; Schwenk et al, 2006;490Schwenk, 2010).
The advantage of these NN LMs isthe projection of words on a continuous space werethe probabilities of n-grams are learned.
A NeuralNetwork (NN) is proposed to learn both the wordprojections and the n-gram probabilities.The presented system combines a standard, state-of-the-art SMT system with a NN LM via log-linearcombination and N -best output re-scoring.
Wechose to participate in the English-Spanish direction.2 Translation modelsA standard phrase-based translation model is com-posed of the following five hk features:?
inverse phrase translation probability p(f |e)?
inverse lexical weighting l(f |e)?
direct phrase translation probability p(e|f)?
direct lexical weighting l(e|f)?
phrase penalty (always e = 2.718).We rely only on the first four features.
Theyare computed from word alignments at the sentencelevel, by counting over the alignments, and using theinverse and direct lexical dictionaries.
Given a pairof phrases, f on the source language and e in the tar-get language, the phrase translation probabilities arecomputed by relative frequency as:p(f |e) =count(f, e)?e?
count(f, e?
)p(e|f) =count(f, e)?f ?
count(f?, e)Given a word f on the source language, and aword e in the target language, the lexical translationdistribution is computed again by relative frequencyas:w(f |e) =count(f, e)?e?
count(f, e?
)w(e|f) =count(f, e)?f ?
count(f?, e)Given the previous lexical translation distribution,two phrase pairs f and e, and a, the word alignmentbetween the source word positions i = 1, .
.
.
, n andthe target word positions j = 1, .
.
.
,m, the inverselexical weighting is computed as:l(f |e) =n?i=11|{j|(i, j) ?
a}|?
(i,j)?aw(fi|ej)and the direct lexical weighting is computed as:l(e|f) =m?j=11|{i|(i, j) ?
a}|?
(i,j)?aw(ej |fi)3 Weighting different translation modelsThe proposed modifications of the phrase-basedtranslation models are similar to (Foster et al, 2010;Matsoukas et al, 2009), but in this case the weight-ing is simpler and focused at the corpus level.
Ifwe have T different training sets, we could define?t as the weight of the set t, for 1 ?
t ?
T .
Theword alignments are computed via Giza++ (Och andNey, 2003) over the concatenation of all the trainingmaterial available for the translation models (in thiscase, Europarl, News-Commentary, and United Na-tions).
After that, we could recompute the lexicaltranslation distribution using the weights informa-tion, and compute the phrase-based translation mod-els taking into account these weights.
The countfunction will be redefined to take into account onlyinformation of the corresponding training set.3.1 Count smoothingThe weight ?t is applied to the count function, inorder to modify the corpus effect on the probabilityof each phrase pair alignment, and each word pairalignment.
First, we modify the lexical translationdistribution in this way:w(f |e) =?t ?tcountt(f, e)?t ?t?e?
countt(f, e?
)w(e|f) =?t ?tcountt(f, e)?t ?t?f ?
countt(f?, e)491having a global lexical translation distribution forthe alignment between words.
Second, we mod-ify the phrase translation probabilities for each di-rection, remaining without modification the lexicalweightings:p(f |e) =?t ?tcountt(f, e)?t ?t?e?
countt(f, e?
)p(e|f) =?t ?tcountt(f, e)?t ?t?f?
countt(f?, e)When some phrase/word count is not found, countis set to zero.3.2 Linear interpolationIn this case, we compute independently the transla-tion models for each training set.
We have T mod-els, one for each set.
The final translation models areobtained by means of a linear interpolation of eachindependent translation model.
If some phrase pairis not found, the translation model is set to have zeroprobability.First, we redefine the lexical translation distribu-tion.
In this case we have w1, w2, .
.
.
, wT lexicaldictionaries:wt(f |e) =countt(f, e)?e?
countt(f, e?
)wt(e|f) =countt(f, e)?f ?
countt(f?, e).Then, we could compute the linear interpolationof phrase translation probabilities as follows:p(f |e) =?t?tcountt(f, e)?e?
countt(f, e?
)p(e|f) =?t?tcountt(f, e)?f?
countt(f?, e)And finally, the inverse lexical weighting is ob-tained as:l(f |e) =?t?tn?i=11|{j|(i, j) ?
a}|?
(i,j)?awt(fi|ej)and the direct lexical weighting:l(e|f) =?t?tm?j=11|{i|(i, j) ?
a}|?
(i,j)?awt(ej |fi).4 Neural Network Language ModelsIn SMT the most useful language models aren-grams (Bahl et al, 1983; Jelinek, 1997; Bahl et al,1983).
They compute the probability of each wordgiven the context of the n?
1 previous words:p(s1 .
.
.
s|S|) ?|S|?i=1p(si|si?n+1 .
.
.
si?1) (2)where S is the sequence of words for which we wantcompute the probability, and si ?
S, from a vocab-ulary ?.A NN LM is a statistical LM which follows equa-tion (2) as n-grams do, but where the probabilitiesthat appear in that expression are estimated with aNN (Bengio et al, 2003; Castro-Bleda and Prat,2003; Schwenk, 2007; Bengio, 2008).
The modelnaturally fits under the probabilistic interpretationof the outputs of the NNs: if a NN, in this case aMLP, is trained as a classifier, the outputs associatedto each class are estimations of the posterior proba-bilities of the defined classes (Bishop, 1995).The training set for a LM is a sequences1s2 .
.
.
s|S| of words from a vocabulary ?.
In orderto train a NN to predict the next word given a historyof length n?1, each input word must be encoded.
Anatural representation is a local encoding followinga ?1-of-|?|?
scheme.
The problem of this encodingfor tasks with large vocabularies (as is typically thecase) is the huge size of the resulting NN.
We havesolved this problem following the ideas of (Bengioet al, 2003; Schwenk, 2007), learning a distributedrepresentation for each word.
Figure 1 illustrates thearchitecture of the feed-forward NN used to estimatethe NN LM.This n-gram NN LM predicts the posterior proba-bility of each word of the vocabulary given the n?1previous words.
A single forward pass of the MLPgives p(?|si?n+1 .
.
.
si?1) for every word ?
?
?.After training the projection layer is replaced by atable look-up.492Figure 1: Architecture of the continuous space NN LMduring training.
The input words are si?n+1, .
.
.
, si?1(in this example, the input words are si?3, si?2, and si?1for a 4-gram).
I , P , H , and O are the input, projection,hidden, and output layer, respectively, of the MLP.The major advantage of the connectionist ap-proach is the automatic smoothing performed bythe neural network estimators.
This smoothing isdone via a continuous space representation of theinput words.
Learning the probability of n-grams,together with their representation in a continuousspace (Bengio et al, 2003), is an appropriate ap-proximation for large vocabulary tasks.
However,one of the drawbacks of such approach is the highcomputational cost entailed whenever the NN LMis computed directly, with no simplification what-soever.
For this reason, the vocabulary size will berestricted in the experiments presented in this work.5 ExperimentsThe baseline SMT system is built with the open-source SMT toolkit Moses (Koehn et al, 2007), inits standard setup.
The decoder includes a log-linearmodel comprising a phrase-based translation model,a language model, a lexicalized distortion modeland word and phrase penalties.
The weights of thelog-linear interpolation were optimized by means ofMERT (Och, 2003), using the News-Commentarytest set of the 2008 shared task as a development set.The phrase-based translation model uses the con-Table 1: Spanish corpora statistics.
NC stands forNews-Commentary and UN for United Nations, while|?| stands for vocabulary size, and M /K for mil-lions/thousands of elements.
All numbers are computedwith tokenized and lowercased data.Set # Lines # Words |?|NC v6 159K 4.44M 80KNews-Shuffled 9.17M 269M 596KEuroparl v6 1.94M 55M 177KUN 6.22M 214M 579KTotal 21.93M 678M 1.03MTable 2: Weights of different combination of phrase-based translation models.System Europarl NC UNSmooth1 0.35 0.35 0.30Smooth2 0.40 0.40 0.20Smooth3 0.15 0.80 0.05Linear 0.35 0.35 0.30catenation of News-Commentary, United Nations,and Europarl corpora, to estimate the four transla-tion model features.The baseline LM was a regular n-gram LM withKneser-Ney smoothing (Kneser and Ney, 1995) andinterpolation by means of the SRILM toolkit (Stol-cke, 2002).
Specifically, we trained a 6-gram LMon United Nations, a 5-gram on Europarl and News-Shuffled, and a 4-gram on News-Commentary.
Oncethese LMs had been built, they were interpolatedso as to maximize the perplexity of the News-Commentary test set of the 2009 shared task.
The fi-nal model was pruned out using a threshold of 10?8.This was done so according to preliminary research.Three different weights for the count smooth-ing technique described in section 3.1 were tested.For the interpolation model of section 3.2, we se-lect the weights minimizing the perplexity of thecorresponding three LMs (Europarl, NC, and UN)over the News2008 set.
Table 2 summarizes theseweights.NN LM was trained with all the corpora describedin Table 1, using a weighted replacement algorithmto modify the impact of each corpus in the trainingalgorithm.
The weights were the same that for thestandard LM.
In order to reduce the complexity of493the model, the input vocabulary of the NN LM wasrestricted using only words that appears more than10 times in the corpora.
The vocabulary is formedby the 107 607 more frequent words, with two addi-tional inputs: one to represent the words out of thisvocabulary, and another for the begin-of-sentencecue.
The output of the NN LM was restricted muchmore, using only a shortlist (Schwenk, 2007) of the10K more frequent words, plus the end-of-sentencecue.
The rest of words are collected by an additionaloutput in the neural network.
When the probabilityof an out-of-shortlist word is required, its probabilityis computed multiplying this additional output acti-vation by the unigram probability distribution of ev-ery out-of-shortlist word.
This implies that 10.7% ofthe running words of the News2009 set, and 11.1%of the running words of the News2011 official testset, will be considered as out-of-shortlist words forthe NN LM.A 6-gram NN LM was trained for this task, basedin previous works (Zamora-Mart?
?nez and Sanchis-Trilles, 2010).
Four NN LMs with different valuesfor the projection of each word (128, 192, 256, 320)were linearly combined for the final NN LM.
EachNN LM had 320 units in the hidden layer.
The com-bination weights were computed maximizing theperplexity over the News2009 set.
The training pro-cedure was conducted by means of the stochasticback-propagation algorithm with weight decay, witha replacement of 300K training samples and 200Kvalidation samples in each training epoch, select-ing the random sample using a different distributionweight for each corpus.
The validation set was theNews2009 set.
The networks were stopped after 99,70, 53, and 42 epochs respectively (unfortunately,without achieving convergence, due to the compe-tition timings).
This resulted in very few trainingsamples compared with the size of the training set:29M in the best case, versus more than 500M ofthe full set.
The training of the NN LMs was ac-complished with the April toolkit (Espan?a-Boqueraet al, 2007; Zamora-Mart?
?nez et al, 2009).
The per-plexity achieved by the 6-gram NN LM in the Span-ish News2009 set was 281, versus 145 obtained withthe standard 6-gram language model with interpola-tion and Kneser-Ney smoothing (Kneser and Ney,1995).The number of sentences in the N -best list wasTable 3: Main results of the experimentationNews2010 News2011System BLEU TER BLEU TERBaseline 29.2 60.0 30.5 58.9Smooth1 29.3 59.9 ?
?Smooth2 29.2 59.9 ?
?Smooth3 29.5 59.6 30.9 58.5+ NN LM 29.9 59.2 31.4 58.0Linear 29.5 59.5 30.9 58.7+ NN LM 30.2 58.8 31.5 57.9set to 2 000 unique output sentences.
Results canbe seen in Table 3.
In order to assess the reliabilityof such results, we computed pairwise improvementintervals as described in (Koehn, 2004), by meansof bootstrapping with 1 000 bootstrap iterations andat a 95% confidence level.
Such confidence test re-ported the improvements to be statistically signifi-cant.
A difference of more than 0.3 points of BLEUis considered significant in the pairwise comparison.The final results leads to 31.5 points of BLEU, posi-tioning this system as second in the final classifica-tion.6 Conclusions and future workThe presented CEU-UPV system, using phrasetranslation models combinations and NN LMs,leads an improvement of 0.4 points of BLEU in thetwo cases: the count smoothing approach (Smooth3system) and the linear interpolation approach (Lin-ear system).
The incorporation of NN LMs inboth systems gets an additional improvement of0.5 BLEU points for the Smooth3 system, and 0.6BLEU points for the Linear system.
The final resultfor the primary system is 31.5 BLEU points.The combination of translation models could beenhanced optimizing the ?t weights over the BLEUscore.
Currently the weights are manually set forthe Smooth[1,2,3] systems, and fixed to the LMweights for the Linear system.
Nevertheless, bothapproaches achieve similar results.
Finally, it is im-portant to emphasize that the use of NN LMs impliesan interesting improvement, though this year?s gainis lower than that obtained by our 2010 system.11Note that the NN LMs didn?t achieve convergence due to494AcknowledgmentsThis work has been partially supported by the Span-ish Goverment under project TIN2010-1958.ReferencesL.
R. Bahl, F. Jelinek, and R. L. Mercer.
1983.
A Max-imum Likelihood Approach to Continuous SpeechRecognition.
IEEE Trans.
on Pat.
Anal.
and Mach.Intel., 5(2):179?190.Y.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003.A Neural Probabilistic Language Model.
Journal ofMachine Learning Research, 3(2):1137?1155.Y.
Bengio.
2008.
Neural net language models.
Scholar-pedia, 3(1):3881.C.
M. Bishop.
1995.
Neural networks for pattern recog-nition.
Oxford University Press.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Comput.
Linguist., 19(2):263?311.M.J.
Castro-Bleda and F. Prat.
2003.
New Directions inConnectionist Language Modeling.
In ComputationalMethods in Neural Modeling, volume 2686 of LNCS,pages 598?605.
Springer-Verlag.S.
Espan?a-Boquera, F.
Zamora-Mart?
?nez, M.J. Castro-Bleda, and J. Gorbe-Moya.
2007.
Efficient BP Al-gorithms for General Feedforward Neural Networks.In Bio-inspired Modeling of Cognitive Tasks, volume4527 of LNCS, pages 327?336.
Springer.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In Proc.
ofEMNLP, EMNLP?10, pages 451?459, Stroudsburg,PA, USA.
Association for Computational Linguistics.F.
Jelinek.
1997.
Statistical Methods for Speech Recog-nition.
Language, Speech, and Communication.
TheMIT Press.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Proc.
of the Interna-tional Conference on Acoustics, Speech, and SignalProcessing (ICASSP?95), volume II, pages 181?184,May.Philipp Koehn and Josh Schroeder.
2007.
Experiments indomain adaptation for statistical machine translation.In Proc.
of WMT?07, pages 224?227.P.
Koehn et al 2007.
Moses: Open Source Toolkitfor Statistical Machine Translation.
In Proc.
of theACL?07 Demo and Poster Sessions, pages 177?180,Prague, Czech Republic.competition timings.P.
Koehn.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Proc.
of EMNLP,EMNLP?04, pages 388?395.
Association for Compu-tational Linguistics.Spyros Matsoukas, Antti-Veikko I. Rosti, and BingZhang.
2009.
Discriminative corpus weight estima-tion for machine translation.
In Proc.
of EMNLP?09,volume 2, pages 708?717, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.F.
Och and H. Ney.
2002.
Discriminative trainingand maximum entropy models for statistical machinetranslation.
In Proc.
of ACL?02, pages 295?302.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.F.J.
Och.
2003.
Minimum Error Rate Training in Statis-tical Machine Translation.
In Proc.
of ACL?03, pages160?167, Sapporo, Japan.K.
Papineni, S. Roukos, and T. Ward.
1998.
Maxi-mum likelihood and discriminative training of directtranslation models.
In Proc.
of the International Con-ference on Acoustics, Speech, and Signal Processing(ICASSP?98), pages 189?192.Philip Resnik and Noah A. Smith.
2003.
The web as aparallel corpus.
Computational Linguistics, 29:349?380.Germa?n Sanchis-Trilles and Francisco Casacuberta.2010.
Bayesian adaptation for statistical machinetranslation.
In Proc.
of SSSPR?10, pages 620?629.H.
Schwenk, D. De?chelotte, and J. L. Gauvain.
2006.Continuous space language models for statistical ma-chine translation.
In Proc.
of the COLING/ACL 2006Main Conference Poster Sessions, pages 723?730.H.
Schwenk.
2007.
Continuous space language models.Computer Speech and Language, 21(3):492?518.H.
Schwenk.
2010.
Continuous space language modelsfor statistical machine translation.
The Prague Bul-letin of Mathematical Linguistics, 93.A.
Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proc.
of the International Con-ference in Spoken Language Processing (ICSLP?02),pages 901?904, September.Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, andEiichiro Sumita.
Method of selecting training data tobuild a compact and efficient translation model.
InProc.
of IJCNLP?10, pages 655?660.Francisco Zamora-Mart?
?nez and Germa?n Sanchis-Trilles.2010.
UCH-UPV English?Spanish System forWMT10.
In Proc.
of WMT?10, pages 207?211, July.F.
Zamora-Mart?
?nez, M.J. Castro-Bleda, and S. Espan?a-Boquera.
2009.
Fast Evaluation of Connectionist Lan-guage Models.
In IWANN, volume 5517 of LNCS,pages 33?40.
Springer.495
