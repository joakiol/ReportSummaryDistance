Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 181?189,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsStreaming First Story Detection with application to TwitterSas?a Petrovic?School of InformaticsUniversity of Edinburghsasa.petrovic@ed.ac.ukMiles OsborneSchool of InformaticsUniversity of Edinburghmiles@inf.ed.ac.ukVictor LavrenkoSchool of InformaticsUniversity of Edinburghvlavrenk@inf.ed.ac.ukAbstractWith the recent rise in popularity and size ofsocial media, there is a growing need for sys-tems that can extract useful information fromthis amount of data.
We address the prob-lem of detecting new events from a stream ofTwitter posts.
To make event detection feasi-ble on web-scale corpora, we present an algo-rithm based on locality-sensitive hashing whichis able overcome the limitations of traditionalapproaches, while maintaining competitive re-sults.
In particular, a comparison with a state-of-the-art system on the first story detectiontask shows that we achieve over an order ofmagnitude speedup in processing time, whileretaining comparable performance.
Event de-tection experiments on a collection of 160 mil-lion Twitter posts show that celebrity deathsare the fastest spreading news on Twitter.1 IntroductionIn the recent years, the microblogging service Twit-ter has become a very popular tool for express-ing opinions, broadcasting news, and simply com-municating with friends.
People often comment onevents in real time, with several hundred micro-blogs(tweets) posted each second for significant events.Twitter is not only interesting because of this real-time response, but also because it is sometimes aheadof newswire.
For example, during the protests fol-lowing Iranian presidential elections in 2009, Iranianpeople first posted news on Twitter, where they werelater picked up by major broadcasting corporations.Another example was the swine flu outbreak whenthe US Centre for disease control (CDC) used Twit-ter to post latest updates on the pandemic.
In ad-dition to this, subjective opinion expressed in postsis also an important feature that sets Twitter apartfrom traditional newswire.New event detection, also known as first story de-tection (FSD)1 is defined within the topic detectionand tracking as one of the subtasks (Allan, 2002).Given a sequence of stories, the goal of FSD is toidentify the first story to discuss a particular event.In this context, an event is taken to be somethingthat happens at some specific time and place, e.g.,an earthquake striking the town of L?Aquila in Italyon April 6th 2009.
Detecting new events from tweetscarries additional problems and benefits comparedto traditional new event detection from newswire.Problems include a much higher volume of data todeal with and also a higher level of noise.
A majorbenefit of doing new event detection from tweets isthe added social component ?
we can understand theimpact an event had and how people reacted to it.The speed and volume at which data is comingfrom Twitter warrants the use of streaming algo-rithms to make first story detection feasible.
Inthe streaming model of computation (Muthukrish-nan, 2005), items (tweets in our case) arrive contin-uously in a chronological order, and we have to pro-cess each new one in bounded space and time.
Recentexamples of problems set in the streaming model in-clude stream-based machine translation (Levenbergand Osborne, 2009), approximating kernel matricesof data streams (Shi et al, 2009), and topic mod-elling on streaming document collections (Yao et al,2009).
The traditional approach to FSD, where eachnew story is compared to all, or a constantly grow-ing subset, of previously seen stories, does not scaleto the Twitter streaming setting.
We present a FSDsystem that works in the streaming model and takesconstant time to process each new document, whilealso using constant space.
Constant processing timeis achieved by employing locality sensitive hashing(LSH) (Indyk and Motwani, 1998), a randomizedtechnique that dramatically reduces the time needed1We will be using the terms first story detection and newevent detection interchangeably.181to find a nearest neighbor in vector space, and thespace saving is achieved by keeping the amount ofstories in memory constant.We find that simply applying pure LSH in a FSDtask yields poor performance and a high variance inresults, and so introduce a modification which vir-tually eliminates variance and significantly improvesperformance.
We show that our FSD system givescomparable results as a state-of-the-art system onthe standard TDT5 dataset, while achieving an orderof magnitude speedup.
Using our system for eventdetection on 160 million Twitter posts shows that i)the number of users that write about an event is moreindicative than the volume of tweets written aboutit, ii) spam tweets can be detected with reasonableprecision, and iii) news about deaths of famous peo-ple spreads the fastest on Twitter.2 First Story Detection2.1 Traditional ApproachThe traditional approach to first story detection is torepresent documents as vectors in term space, wherecoordinates represent the (possibly IDF-weighted)frequency of a particular term in a document.
Eachnew document is then compared to the previous ones,and if its similarity to the closest document (or cen-troid) is below a certain threshold, the new documentis declared to be a first story.
For example, this ap-proach is used in the UMass (Allan et al, 2000) andthe CMU system (Yang et al, 1998).
Algorithm 1shows the exact pseudocode used by the UMass sys-tem.
Note that dismin(d) is the novelty score as-signed to document d. Often, in order to decreasethe running time, documents are represented usingonly n features with the highest weights.Algorithm 1: Traditional FSD system based onnearest-neighbor search.1 foreach document d in corpus do2 foreach term t in d do3 foreach document d?
that contains t do4 update distance(d, d?
)5 end6 end7 dismin(d) = mind?
{distance(d, d?
)}8 add d to inverted index9 end2.2 Locality Sensitive HashingThe problem of finding the nearest neighbor to agiven query has been intensively studied, but as thedimensionality of the data increases none of the cur-rent solutions provide much improvement over a sim-ple linear search (Datar et al, 2004).
More recently,research has focused on solving a relaxed version ofthe nearest neighbor problem, the approximate near-est neighbor, where the goal is to report any pointthat lies within (1 + ?
)r distance of the query point,where r is the distance to the nearest neighbor.
Oneof the first approaches to solving the approximate-NN problem in sublinear time was described in Indykand Motwani (1998), where the authors introduced anew method called locality sensitive hashing (LSH).This method relied on hashing each query point intobuckets in such a way that the probability of collisionwas much higher for points that are near by.
When anew point arrived, it would be hashed into a bucketand the points that were in the same bucket wereinspected and the nearest one returned.Because we are dealing with textual documents,a particularly interesting measure of distance is thecosine between two documents.
Allan et al (2000)report that this distance outperforms the KL diver-gence, weighted sum, and language models as dis-tance functions on the first story detection task.
Thisis why in our work we use the hashing scheme pro-posed by Charikar (2002) in which the probabilityof two points colliding is proportional to the cosineof the angle between them.
This scheme was used,e.g., for creating similarity lists of nouns collectedfrom a web corpus in Ravichandran et al (2005).
Itworks by intersecting the space with random hyper-planes, and the buckets are defined by the subspacesformed this way.
More precisely, the probability oftwo points x and y colliding under such a hashingscheme isPcoll = 1?
?
(x, y)pi , (1)where ?
(x, y) is the angle between x and y.
By us-ing more than one hyperplane, we can decrease theprobability of collision with a non-similar point.
Thenumber of hyperplanes k can be considered as a num-ber of bits per key in this hashing scheme.
In par-ticular, if x ?
ui < 0, i ?
[1 .
.
.
k] for document x andhyperplane vector ui, we set the i-th bit to 0, and1 otherwise.
The higher k is, the fewer collisionswe will have in our buckets but we will spend moretime computing the hash values.2 However, increas-ing k also decreases the probability of collision withthe nearest neighbor, so we need multiple hash ta-bles (each with k independently chosen random hy-perplanes) to increase the chance that the nearestneighbor will collide with our point in at least one of2Probability of collision under k random hyperplanes willbe Pkcoll .182them.
Given the desired number of bits k, and thedesired probability of missing a nearest neighbor ?,one can compute the number of hash tables L asL = log1?Pkcoll ?.
(2)2.3 Variance Reduction StrategyUnfortunately, simply applying LSH for nearestneighbor search in a FSD task yields poor resultswith a lot of variance (the exact numbers are given inSection 6).
This is because LSH only returns the truenear neighbor if it is reasonably close to the querypoint.
If, however, the query point lies far awayfrom all other points (i.e., its nearest neighbor is faraway), LSH fails to find the true near neighbor.
Toovercome this problem, we introduce a strategy bywhich, if the LSH scheme declares a document new(i.e., sufficiently different from all others), we start asearch through the inverted index, but only comparethe query with a fixed number of most recent doc-uments.
We set this number to 2000; preliminaryexperiments showed that values between 1000 and3000 all yield very similar results.
The pseudocodeshown in algorithm 2 summarizes the approach basedon LSH, with the lines 11 and 12 being the variancereduction strategy.Algorithm 2: Our LSH-based approach.input: threshold t1 foreach document d in corpus do2 add d to LSH3 S ?
set of points that collide with d in LSH4 dismin(d) ?
15 foreach document d?
in S do6 c = distance(d, d?
)7 if c < dismin(d) then8 dismin(d) ?
c9 end10 end11 if dismin(d) >= t then12 compare d to a fixed number of mostrecent documents as in Algorithm 1 andupdate dismin if necessary13 end14 assign score dismin(d) to d15 add d to inverted index16 end3 Streaming First Story DetectionAlthough using LSH in the way we just describedgreatly reduces the running time, it is still too expen-sive when we want to deal with text streams.
Textstreams naturally arise on the Web, where millionsof new documents are published each hour.
Socialmedia sites like Facebook, MySpace, Twitter, andvarious blogging sites are a particularly interestingsource of textual data because each new documentis timestamped and usually carries additional meta-data like topic tags or links to author?s friends.
Be-cause this stream of documents is unbounded andcoming down at a very fast rate, there is usually alimit on the amount of space/time we can spend perdocument.
In the context of first story detection,this means we are not allowed to store all of the pre-vious data in main memory nor compare the newdocument to all the documents returned by LSH.Following the previous reasoning, we present thefollowing desiderata for a streaming first story de-tection system: we first assume that each day weare presented with a large volume of documentsin chronological order.
A streaming FSD systemshould, for each document, say whether it discusses apreviously unseen event and give confidence in its de-cision.
The decision should be made in bounded time(preferably constant time per document), and usingbounded space (also constant per document).
Onlyone pass over the data is allowed and the decisionhas to be made immediately after a new documentarrives.
A system that has all of these properties canbe employed for finding first stories in real time froma stream of stories coming down from the Web.3.1 A constant space and time approachIn this section, we describe our streaming FSD sys-tem in more depth.
As was already mentioned inSection 2.2, we use locality sensitive hashing to limitour search to a small number of documents.
How-ever, because there is only a finite number of buck-ets, in a true streaming setting the number of docu-ments in any bucket will grow without a bound.
Thismeans that i) we would use an unbounded amountof space, and ii) the number of comparisons we needto make would also grow without a bound.
To alle-viate the first problem, we limit the number of doc-uments inside a single bucket to a constant.
If thebucket is full, the oldest document in the bucket isremoved.
Note that the document is removed onlyfrom that single bucket in one of the L hash tables?
it may still be present in other hash tables.
Notethat this way of limiting the number of documentskept is in a way topic-specific.
Luo et al (2007) usea global constraint on the documents they keep andshow that around 30 days of data needs to be keptin order to achieve reasonable performance.
Whileusing this approach also ensures that the number ofcomparisons made is constant, this constant can be183rather large.
Theoretically, a new document can col-lide with all of the documents that are left, and thiscan be quite a large number (we have to keep a suffi-cient portion of the data in memory to make sure wehave a representative sample of the stream to com-pare with).
That is why, in addition to limiting thenumber of documents in a bucket, we also limit our-selves to making a constant number of comparisons.We do this by comparing each new document withat most 3L documents it collided with.
Unlike Dataret al (2004), where any 3L documents were used, wecompare to the 3L documents that collide most fre-quently with the new document.
That is, if S is theset of all documents that collided with a new doc-ument in all L hash tables, we order the elementsof S according to the number of hash tables wherethe collision occurred.
We take the top 3L elementsof that ordered set and compare the new documentonly to them.4 Detecting Events in Twitter PostsWhile doing first story detection on a newspaperstream makes sense because all of the incoming doc-uments are actual stories, this is not the case withTwitter posts (tweets).
The majority of tweets arenot real stories, but rather updates on one?s personallife, conversations, or spam.
Thus, simply running afirst story detection system on this data would yieldan incredible amount of new stories each day, mostof which would be of no interest to anyone but a fewpeople.
However, when something significant hap-pens (e.g., a celebrity dies), a lot of users write aboutthis either to share their opinion or just to informothers of the event.
Our goal here is to automati-cally detect these significant events, preferably witha minimal number of non-important events.Threading.
We first run our streaming FSDsystem and assign a novelty score to each tweet.
Inaddition, since the score is based on a cosine dis-tance to the nearest tweet, for each tweet we alsooutput which other tweet it is most similar to.
Thisway, we can analyze threads of tweets, i.e., a subsetof tweets which all discuss the same topic (Nallap-ati et al, 2004).
To explain how we form threadsof tweets, we first introduce the links relation.
Wesay that tweet a links to tweet b if b is the nearestneighbor of a and 1?
cos(a, b) < t, where t is a user-specified threshold.
Then, for each tweet a we eitherassign it to an existing thread if its nearest neighboris within distance t, or say that a is the first tweet ina new thread.
If we assign a to an existing thread,we assign it to the same thread to which its nearestneighbor belongs.
By changing t we can control thegranularity of threads.
If t is set very high, we willhave few very big and broad threads, whereas settingt very low will result in many very specific and verysmall threads.
In our experiments, we set t = 0.5.We experimented with different values of t and foundthat for t ?
[0.5, 0.6] results are very much the same,whereas setting t outside this interval starts to im-pact the results in the way we just explained.Once we have threads of tweets, we are interestedin which threads grow fastest, as this will be an indi-cation that news of a new event is spreading.
There-fore, for each time interval we only output the fastestgrowing threads.
This growth rate also gives us a wayto measure a thread?s impact.5 Related WorkIn the recent years, analysis of social media has at-tracted a lot of attention from the research commu-nity.
However, most of the work that uses socialmedia focuses on blogs (Glance et al, 2004; Bansaland Koudas, 2007; Gruhl et al, 2005).
On the otherhand, research that uses Twitter has so far onlyfocused on describing the properties of Twitter it-self (Java et al, 2007; Krishnamurthy et al, 2008).The problem of online new event detection ina large-scale streaming setting was previously ad-dressed in Luo et al (2007).
Their system used thetraditional approach to FSD and then employed var-ious heuristics to make computation feasible.
Theseincluded keeping only the first stories in memory,limiting the number of terms per document, limitingthe number of total terms kept, and employing par-allel processing.
Our randomized framework gives usa principled way to work out the errors introducedand is more general than the previously mentionedapproach because we could still use all the heuris-tics used by Luo et al (2007) in our system.
Fi-nally, while Luo et al (2007) achieved considerablespeedup over an existing system on a TDT corpus,they never showed the utility of their system on atruly large-scale task.The only work we are aware of that analyzes so-cial media in a streaming setting is Saha and Getoor(2009).
There, the focus was on solving the maxi-mum coverage problem for a stream of blog posts.The maximum coverage problem in their setting,dubbed blog watch, was selecting k blogs that maxi-mize the cover of interests specified by a user.
Thiswork differs from Saha and Getoor (2009) in manyways.
Most notably, we deal with the problem ofdetecting new events, and determining who was thefirst to report them.
Also, there is a difference in thetype and volume of data ?
while Saha and Getoor(2009) use 20 days of blog data totalling two millionposts, we use Twitter data from a timespan of six184months, totalling over 160 million posts.6 Experiments6.1 TDT5 Experimental SetupBaseline.
Before applying our FSD system onTwitter data, we first compared it to a state-of-the-art FSD system on the standard TDT5 dataset.
Thisway, we can test if our system is on par with the bestexisting systems, and also accurately measure thespeedup that we get over a traditional approach.
Inparticular, we compare our system with the UMassFSD system (Allan et al, 2000).
The UMass systemhas participated in the TDT2 and TDT3 competi-tions and is known to perform at least as well as otherexisting systems who also took part in the competi-tion (Fiscus, 2001).
Note that the UMass systemuses an inverted index (as shown in Algorithm 1)which optimizes the system for speed and makes surea minimal number of comparisons is made.
We com-pare the systems on the English part of the TDT5dataset, consisting of 221, 306 documents from a timeperiod spanning April 2003 to September 2003.
Tomake sure that any difference in results is due toapproximations we make, we use the same settingsas the UMass system: 1-NN clustering, cosine as asimilarity measure, and TFIDF weighted documentrepresentation, where the IDF weights are incremen-tally updated.
These particular settings were foundby Allan et al (2000) to perform the best for theFSD task.
We limit both systems to keeping onlytop 300 features in each document.
Using more than300 features barely improves performance while tak-ing significantly more time for the UMass system.3LSH parameters.
In addition, our system hastwo LSH parameters that need to be set.
The num-ber of hyperplanes k gives a tradeoff between timespent computing the hash functions and the timespent computing the distances.
A lower k meansmore documents per bucket and thus more distancecomputations, whereas a higher k means less doc-uments per bucket, but more hash tables and thusmore time spent computing hash functions.
Given k,we can use equation (2) to compute L. In our case,we chose k to be 13, and L such that the probabilityof missing a neighbor within the distance of 0.2 isless than 2.5%.
The distance of 0.2 was chosen as areasonable estimate of the threshold when two docu-ments are very similar.
In general, this distance willdepend on the application, and Datar et al (2004)suggest guessing the value and then doing a binarysearch to set it more accurately.
We set k to 13 be-3In other words, using more features only increases theadvantage of our system over the UMass system.cause it achieved a reasonable balance between timespent computing the distances and the time spentcomputing the hash functions.Evaluation metric.
The official TDT evalua-tion requires each system to assign a confidence scorefor its decision, and this assignment can be madeeither immediately after the story arrives, or aftera fixed number of new stories have been observed.Because we assume that we are working in a truestreaming setting, systems are required to assign aconfidence score as soon as the new story arrives.The actual performance of a system is measuredin terms of detection error tradeoff (DET) curvesand the minimal normalized cost.
Evaluation is car-ried out by first sorting all stories according to theirscores and then performing a threshold sweep.
Foreach value of the threshold, stories with a score abovethe threshold are considered new, and all others areconsidered old.
Therefore, for each threshold value,one can compute the probability of a false alarm, i.e.,probability of declaring a story new when it is actu-ally not, and the miss probability, i.e., probabilityof declaring a new story old (missing a new story).Having computed all the miss and false alarm prob-abilities, we can plot them on a graph showing thetradeoff between these two quantities ?
such graphsare called detection error tradeoff curves.
The nor-malized cost Cdet is computed asCdet = Cmiss ?Pmiss ?Ptarget+CFA?PFA?Pnon?target ,where Cmiss and CFA are costs of miss and falsealarm, Pmiss and PFA are probabilities of a miss andfalse alarm, and Ptarget and Pnon?target are the priortarget and non-target probabilities.
Minimal nor-malized cost Cmin is the minimal value of Cdet overall threshold values (a lower value of Cmin indicatesbetter performance).6.2 TDT5 ResultsAll the results on the TDT5 dataset are shown inTable 1.
In this section, we go into detail in explain-ing them.
As was mentioned in Section 2.2, simplyusing LSH to find a nearest neighbor resulted in poorperformance and a high variance of results.
In par-ticular, the mean normalized cost of ten runs of oursystem without the variance reduction strategy was0.88, with a standard deviation of 0.046.
When us-ing the strategy explained in Section 2.2, the meanresult dropped to 0.70, with a standard deviation of0.004.
Therefore, the results were significantly im-proved, while also reducing standard deviation by anorder of magnitude.
This shows that there is a clearadvantage in using our variance reduction strategy,185Table 1: Summary of TDT5 results.
Numbers next to LSH?ts indicate the maximal number of documents in a bucket,measured in terms of percentage of the expected number of collisions.Baseline Unbounded BoundedPure Variance Red.
Time Space and TimeSystem UMass LSH LSH?
LSH?t LSH?ts 0.5 LSH?ts 0.3 LSH?ts 0.1Cmin 0.69 0.88 0.70 0.71 0.76 0.75 0.73125102040608090.01 .02 .05 .1 .2 .5 1 2 5 10 20 40 60 80 90Missprobability(in %)False Alarms probability (in %)Random PerformanceOur systemUMass systemFigure 1: Comparison of our system with the UMass FSDsystem.and all the following results we report were obtainedfrom a system that makes use of it.Figure 1 shows DET curves for the UMass and forour system.
For this evaluation, our system was notlimited in space, i.e., buckets sizes were unlimited,but the processing time per item was made constant.It is clear that UMass outperforms our system, butthe difference is negligible.
In particular, the min-imal normalized cost Cmin was 0.69 for the UMasssystem, and 0.71 for our system.
On the other hand,the UMass system took 28 hours to complete therun, compared to two hours for our system.
Figure 2shows the time required to process 100 documentsas a function of number of documents seen so far.We can see that our system maintains constant time,whereas the UMass system processing time growswithout a bound (roughly linear with the numberof previously seen documents).The last three columns in Table 1 show the effectthat limiting the bucket size has on performance.Bucket size was limited in terms of the percent ofexpected number of collisions, i.e., a bucket size of0.5 means that the number of documents in a bucketcannot be more than 50% of the expected numberof collisions.
The expected number of collisions can0204060801001200  50000  100000  150000  200000  250000Timeper 100 documents (sec)Number of documents processedUMass systemOur systemFigure 2: Comparison of processing time per 100 docu-ments for our and the UMass system.be computed as n/2k, where n is the total numberof documents, and k is the LSH parameter explainedearlier.
Not surprisingly, limiting the bucket size re-duced performance compared to the space-unlimitedversion, but even when the size is reduced to 10% ofthe expected number of collisions, performance re-mains reasonably close to the UMass system.
Fig-ure 3 shows the memory usage of our system on amonth of Twitter data (more detail about the datacan be found in Section 6.3).
We can see that mostof the memory is allocated right away, after whichthe memory consumption levels out.
If we ran thesystem indefinitely, we would see the memory usagegrow slower and slower until it reached a certain levelat which it would remain constant.6.3 Twitter Experimental SetupCorpus.
We used our streaming FSD system todetect new events from a collection of Twitter datagathered over a period of six months (April 1st 2009to October 14th 2009).
Data was collected throughTwitter?s streaming API.4 Our corpus consists of163.5 million timestamped tweets, totalling over 2billion tokens.
All the tweets in our corpus contain4http://stream.twitter.com/186051015202530354045500  500  1000  1500  2000  2500  3000  3500  4000Percent of memory usedMinutesFigure 3: Memory usage on a month of Twitter data.X-axis shows how long the system has been running for.only ASCII characters and we additionally strippedthe tweets of words beginning with the @ or # sym-bol.
This is because on Twitter words beginning with@ indicate a reply to someone, and words beginningwith # are topic tags.
Although these features wouldprobably be helpful for our task, we decided not touse them as they are specific to Twitter and our ap-proach should be independent of the stream type.Gold standard.
In order to measure how wellour system performs on the Twitter data, we em-ployed two human experts to manually label all thetweets returned by our system as either Event, Neu-tral, or Spam.
Note that each tweet that is returnedby our system is actually the first tweet in a thread,and thus serves as the representative of what thethread is about.
Spam tweets include various ad-vertisements, automatic weather updates, automaticradio station updates, etc.
For a tweet to be la-beled as an event, it had to be clear from the tweetalone what exactly happened without having anyprior knowledge about the event, and the event refer-enced in the tweet had to be sufficiently important.Important events include celebrity deaths, naturaldisasters, major sports, political, entertainment, andbusiness events, shootings, plane crashes and otherdisasters.
Neutral tweets include everything not la-beled as spam or event.
Because the process of man-ual labeling is tedious and time-consuming, we onlylabeled the 1000 fastest growing threads from June2009.
Rate of growth of a thread is measured by thenumber of tweets that belong to that thread in a win-dow of 100,000 tweets, starting from the beginningof the thread.
Agreement between our two annota-tors, measured using Cohen?s kappa coefficient, wassubstantial (kappa = 0.65).
We use 820 tweets onwhich both annotators agreed as the gold standard.Evaluation.
Evaluation is performed by com-puting average precision (AP) on the gold standardsorted according to different criteria, where eventtweets are taken to be relevant, and neutral and spamtweets are treated as non-relevant documents.
Aver-age precision is a common evaluation metric in taskslike ad-hoc retrieval where only the set of returneddocuments and their relevance judgements are avail-able, as is the case here (Croft et al, 2009).
Notethat we are not evaluating our FSD system here.There are two main reasons for this: i) we alreadyhave a very good idea about the first story detectionperformance from the experiments on TDT5 data,and ii) evaluating a FSD system on this scale wouldbe prohibitively expensive as it would involve hu-man experts going through 30 million tweets lookingfor first stories.
Rather, we are evaluating differentmethods of ranking threads which are output from aFSD system for the purpose of detecting importantevents in a very noisy and unstructured stream suchas Twitter.6.4 Twitter ResultsResults for the average precisions are given in Ta-ble 2.
Note that we were not able to compare oursystem with the UMass FSD system on the Twit-ter data, as the UMass system would not finish inany reasonable amount of time.
Different rows ofTable 2 correspond to the following ways of rankingthe threads:?
Baseline ?
random ordering of threads?
Size of thread ?
threads are ranked according tonumber of tweets?
Number of users ?
threads are ranked accordingto number of unique users posting in a thread?
Entropy + users ?
if the entropy of a thread is< 3.5, move to the back of the list, otherwisesort according to number of unique usersResults show that ranking according to size of threadperforms better than the baseline, and ranking ac-cording to the number of users is slightly better.However, a sign test showed that neither of the tworanking strategies is significantly better than thebaseline.
We perform the sign test by splitting thelabeled data into 50 stratified samples and rankingeach sample with different strategies.
We then mea-sure the number of times each strategy performedbetter (in terms of AP) and compute the significancelevels based on these numbers.
Adding the informa-tion about the entropy of the thread showed to be187Table 2: Average precision for Events vs. Rest and forEvents and Neutral vs. Spam.Ranking method events vs. rest spam vs. restBaseline 16.5 84.6Size of thread 24.1 83.5Number of users 24.5 83.9Entropy + users 34.0 96.3Table 3: Average precision as a function of the entropythreshold on the Events vs. Rest task.Entropy 2 2.5 3 3.5 4 4.5AP 24.8 27.6 30.0 34.0 33.2 29.4very beneficial.
Entropy of a thread is computed asHthread = ?
?iniN logniN ,where ni is the number of times word i appears ina thread, and N = ?i ni is the total number ofwords in a thread.
We move the threads with lowentropy (< 3.5) to the back of the list, while we or-der other threads by the number of unique users.A sign test showed this approach to be significantlybetter (p ?
0.01) than all of the previous rankingmethods.
Table 3 shows the effect of varying the en-tropy threshold at which threads are moved to theback of the list.
We can see that adding informa-tion about entropy improves results regardless of thethreshold we choose.
This approach works well be-cause most spam threads have very low entropy, i.e.,contain very little information.We conducted another experiment where eventsand neutral tweets are considered relevant, and spamtweets non-relevant documents.
Results for this ex-periment are given in the third column of Table 2.Results for this experiment are much better, mostlydue to the large proportion of neutral tweets in thedata.
The baseline in this case is very strong andneither sorting according to the size of the threadnor according to the number of users outperformsthe baseline.
However, adding the information aboutentropy significantly (p ?
0.01) improves the perfor-mance over all other ranking methods.Finally, in Table 4 we show the top ten fastestgrowing threads in our data (ranked by the numberof users posting in the thread).
Each thread is repre-sented by the first tweet.
We can see from the tablethat events which spread the fastest on Twitter areTable 4: Top ten fastest growing threads in our data.# users First tweet7814 TMZ reporting michael jackson has had a heartattack.
We r checking it out.
And pulliingvideo to use if confirmed7579 RIP Patrick Swayze...3277 Walter Cronkite is dead.2526 we lost Ted Kennedy :(1879 RT BULLETIN ?
STEVE MCNAIRHAS DIED.1511 David Carradine (Bill in ?Kill Bill?
)found hung in Bangkok hotel.1458 Just heard Sir Bobby Robson has died.
RIP.1426 I just upgraded to 2.0 - The professionalTwitter client.
Please RT!1220 LA Times reporting Manny Ramirez testedpositive for performance enhancing drugs.To be suspended 50 games.1057 A representative says guitar legendLes Paul has died at 94mostly deaths of famous people.
One spam threadthat appears in the list has an entropy of 2.5 anddoesn?t appear in the top ten list when using theentropy + users ranking.7 ConclusionWe presented an approach to first story detection in astreaming setting.
Our approach is based on localitysensitive hashing adapted to the first story detectiontask by introducing a backoff towards exact search.This adaptation greatly improved performance of thesystem and virtually eliminated variance in the re-sults.
We showed that, using our approach, it is pos-sible to achieve constant space and processing timewhile maintaining very good results.
A comparisonwith the UMass FSD system showed that we gainmore than an order of magnitude speedup with only aminor loss in performance.
We used our FSD systemon a truly large-scale task of detecting new eventsfrom over 160 million Twitter posts.
To the best ofour knowledge, this is the first work that does eventdetection on this scale.
We showed that our systemis able to detect major events with reasonable preci-sion, and that the amount of spam in the output canbe reduced by taking entropy into account.AcknowledgmentsThe authors would like to thank Donnla Osborne forher work on annotating tweets.188ReferencesJames Allan, Victor Lavrenko, Daniella Malin, and Rus-sell Swan.
2000.
Detections, bounds, and timelines:Umass and tdt-3.
In Proceedings of Topic Detectionand Tracking Workshop, pages 167?174.James Allan.
2002.
Topic detection and tracking: event-based information organization.
Kluwer AcademicPublishers.Nilesh Bansal and Nick Koudas.
2007.
Blogscope: asystem for online analysis of high volume text streams.In VLDB ?07: Proceedings of the 33rd internationalconference on Very large data bases, pages 1410?1413.VLDB Endowment.Moses S. Charikar.
2002.
Similarity estimation tech-niques from rounding algorithms.
In STOC ?02: Pro-ceedings of the thiry-fourth annual ACM symposium onTheory of computing, pages 380?388, New York, NY,USA.
ACM.W.B.
Croft, D. Metzler, and T. Strohman.
2009.
SearchEngines: Information Retrieval in Practice.
Addison-Wesley Publishing.Mayur Datar, Nicole Immorlica, Piotr Indyk, and Va-hab Mirrokni.
2004.
Locality-sensitive hashing schemebased on p-stable distributions.
In SCG ?04: Proceed-ings of the twentieth annual symposium on Computa-tional geometry, pages 253?262, New York, NY, USA.ACM.J.
Fiscus.
2001.
Overview of results (nist).
In Proceed-ings of the TDT 2001 Workshop.N.
Glance, M. Hurst, and T. Tomokiyo.
2004.
BlogPulse:Automated Trend Discovery for Weblogs.
WWW 2004Workshop on the Weblogging Ecosystem: Aggregation,Analysis and Dynamics, 2004.Daniel Gruhl, R. Guha, Ravi Kumar, Jasmine Novak,and Andrew Tomkins.
2005.
The predictive powerof online chatter.
In KDD ?05: Proceedings of theeleventh ACM SIGKDD international conference onKnowledge discovery in data mining, pages 78?87, NewYork, NY, USA.
ACM.Piotr Indyk and Rajeev Motwani.
1998.
Approximatenearest neighbors: towards removing the curse of di-mensionality.
In STOC ?98: Proceedings of the thirti-eth annual ACM symposium on Theory of computing,pages 604?613, New York, NY, USA.
ACM.Akshay Java, Xiaodan Song, Tim Finin, and Belle Tseng.2007.
Why we twitter: understanding microbloggingusage and communities.
In WebKDD/SNA-KDD ?07:Proceedings of the 9th WebKDD and 1st SNA-KDD2007 workshop on Web mining and social networkanalysis, pages 56?65, New York, NY, USA.
ACM.Balachander Krishnamurthy, Phillipa Gill, and MartinArlitt.
2008.
A few chirps about twitter.
In WOSP?08: Proceedings of the first workshop on Online socialnetworks, pages 19?24, New York, NY, USA.
ACM.Abby Levenberg and Miles Osborne.
2009.
Stream-basedrandomised language models for smt.
In Proceedings ofthe 2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 756?764.Gang Luo, Chunqiang Tang, and Philip S. Yu.
2007.Resource-adaptive real-time new event detection.
InSIGMOD ?07: Proceedings of the 2007 ACM SIG-MOD international conference on Management ofdata, pages 497?508, New York, NY, USA.
ACM.S.
Muthukrishnan.
2005.
Data streams: Algorithms andapplications.
Now Publishers Inc.Ramesh Nallapati, Ao Feng, Fuchun Peng, and JamesAllan.
2004.
Event threading within news topics.
InCIKM ?04: Proceedings of the thirteenth ACM interna-tional conference on Information and knowledge man-agement, pages 446?453, New York, NY, USA.
ACM.Deepak Ravichandran, Patrick Pantel, and Eduard Hovy.2005.
Randomized algorithms and nlp: using localitysensitive hash function for high speed noun clustering.In ACL ?05: Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics, pages622?629, Morristown, NJ, USA.
Association for Com-putational Linguistics.Barna Saha and Lise Getoor.
2009.
On maximum cov-erage in the streaming model & application to multi-topic blog-watch.
In 2009 SIAM International Con-ference on Data Mining (SDM09), April.Qinfeng Shi, James Petterson, Gideon Dror, John Lang-ford, Alex Smola, Alex Strehl, and Vishy Vish-wanathan.
2009.
Hash kernels.
In Proceedings ofthe 12th International Conference on Artificial Intelli-gence and Statistics (AISTATS), pages 496?503.Yiming Yang, Tom Pierce, and Jaime Carbonell.
1998.A study of retrospective and on-line event detection.In SIGIR ?98: Proceedings of the 21st annual inter-national ACM SIGIR conference on Research and de-velopment in information retrieval, pages 28?36, NewYork, NY, USA.
ACM.Limin Yao, David Mimno, and Andrew McCallum.
2009.Efficient methods for topic model inference on stream-ing document collections.
In KDD ?09: Proceedings ofthe 15th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 937?946,New York, NY, USA.
ACM.189
