Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 712?719,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsOrdering Phrases with Function WordsHendra Setiawan and Min-Yen KanSchool of ComputingNational University of SingaporeSingapore 117543{hendrase,kanmy}@comp.nus.edu.sgHaizhou LiInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore 119613hli@i2r.a-star.edu.sgAbstractThis paper presents a Function Word cen-tered, Syntax-based (FWS) solution to ad-dress phrase ordering in the context ofstatistical machine translation (SMT).
Mo-tivated by the observation that functionwords often encode grammatical relation-ship among phrases within a sentence, wepropose a probabilistic synchronous gram-mar to model the ordering of function wordsand their left and right arguments.
We im-prove phrase ordering performance by lexi-calizing the resulting rules in a small numberof cases corresponding to function words.The experiments show that the FWS ap-proach consistently outperforms the base-line system in ordering function words?
ar-guments and improving translation qualityin both perfect and noisy word alignmentscenarios.1 IntroductionThe focus of this paper is on function words, a classof words with little intrinsic meaning but is vital inexpressing grammatical relationships among wordswithin a sentence.
Such encoded grammatical infor-mation, often implicit, makes function words piv-otal in modeling structural divergences, as project-ing them in different languages often result in long-range structural changes to the realized sentences.Just as a foreign language learner often makesmistakes in using function words, we observe thatcurrent machine translation (MT) systems often per-form poorly in ordering function words?
arguments;lexically correct translations often end up reorderedincorrectly.
Thus, we are interested in modelingthe structural divergence encoded by such functionwords.
A key finding of our work is that modelingthe ordering of the dependent arguments of functionwords results in better translation quality.Most current systems use statistical knowledgeobtained from corpora in favor of rich natural lan-guage knowledge.
Instead of using syntactic knowl-edge to determine function words, we approximatethis by equating the most frequent words as func-tion words.
By explicitly modeling phrase orderingaround these frequent words, we aim to capture themost important and prevalent ordering productions.2 Related WorkA good translation should be both faithful with ade-quate lexical choice to the source language and flu-ent in its word ordering to the target language.
Inpursuit of better translation, phrase-based models(Och and Ney, 2004) have significantly improved thequality over classical word-based models (Brown etal., 1993).
These multiword phrasal units contributeto fluency by inherently capturing intra-phrase re-ordering.
However, despite this progress, inter-phrase reordering (especially long distance ones)still poses a great challenge to statistical machinetranslation (SMT).The basic phrase reordering model is a simpleunlexicalized, context-insensitive distortion penaltymodel (Koehn et al, 2003).
This model assumeslittle or no structural divergence between languagepairs, preferring the original, translated order by pe-nalizing reordering.
This simple model works wellwhen properly coupled with a well-trained language712model, but is otherwise impoverished without anylexical evidence to characterize the reordering.To address this, lexicalized context-sensitivemodels incorporate contextual evidence.
The localprediction model (Tillmann and Zhang, 2005) mod-els structural divergence as the relative position be-tween the translation of two neighboring phrases.Other further generalizations of orientation includethe global prediction model (Nagata et al, 2006) anddistortion model (Al-Onaizan and Papineni, 2006).However, these models are often fully lexicalizedand sensitive to individual phrases.
As a result, theyare not robust to unseen phrases.
A careful approx-imation is vital to avoid data sparseness.
Proposalsto alleviate this problem include utilizing bilingualphrase cluster or words at the phrase boundary (Na-gata et al, 2006) as the phrase identity.The benefit of introducing lexical evidence with-out being fully lexicalized has been demonstratedby a recent state-of-the-art formally syntax-basedmodel1, Hiero (Chiang, 2005).
Hiero performsphrase ordering by using linked non-terminal sym-bols in its synchronous CFG production rules cou-pled with lexical evidence.
However, since it is dif-ficult to specify a well-defined rule, Hiero has to relyon weak heuristics (i.e., length-based thresholds) toextract rules.
As a result, Hiero produces grammarsof enormous size.
Watanabe et al (2006) furtherreduces the grammar?s size by enforcing all rules tocomply with Greibach Normal Form.Taking the lexicalization an intuitive a step for-ward, we propose a novel, finer-grained solutionwhich models the content and context informationencoded by function words - approximated by highfrequency words.
Inspired by the success of syntax-based approaches, we propose a synchronous gram-mar that accommodates gapping production rules,while focusing on the statistical modeling in rela-tion to function words.
We refer to our approachas the Function Word-centered Syntax-based ap-proach (FWS).
Our FWS approach is different fromHiero in two key aspects.
First, we use only asmall set of high frequency lexical items to lexi-calize non-terminals in the grammar.
This resultsin a much smaller set of rules compared to Hiero,1Chiang (2005) used the term ?formal?
to indicate the use ofsynchronous grammar but without linguistic commitment,\ 4 ?
{ j?
Q?
?
{ ?\a form is a coll.
of data entry fields on a page((((((((((((((((PPPPPPPPP``````````````Figure 1: A Chinese-English sentence pair.greatly reducing the computational overhead thatarises when moving from phrase-based to syntax-based approach.
Furthermore, by modeling onlyhigh frequency words, we are able to obtain reliablestatistics even in small datasets.
Second, as opposedto Hiero, where phrase ordering is done implicitlyalongside phrase translation and lexical weighting,we directly model the reordering process using ori-entation statistics.The FWS approach is also akin to (Xiong et al,2006) in using a synchronous grammar as a reorder-ing constraint.
Instead of using Inversion Transduc-tion Grammar (ITG) (Wu, 1997) directly, we willdiscuss an ITG extension to accommodate gapping.3 Phrase Ordering around FunctionWordsWe use the following Chinese (c) to English (e)translation in Fig.1 as an illustration to conduct aninquiry to the problem.
Note that the sentence trans-lation requires some translations of English wordsto be ordered far from their original position in Chi-nese.
Recovering the correct English ordering re-quires the inversion of the Chinese postpositionalphrase, followed by the inversion of the first smallernoun phrase, and finally the inversion of the sec-ond larger noun phrase.
Nevertheless, the correctordering can be recovered if the position and the se-mantic roles of the arguments of the boxed functionwords were known.
Such a function word centeredapproach also hinges on knowing the correct phraseboundaries for the function words?
arguments andwhich reorderings are given precedence, in case ofconflicts.We propose modeling these sources of knowl-edge using a statistical formalism.
It includes 1) amodel to capture bilingual orientations of the leftand right arguments of these function words; 2) amodel to approximate correct reordering sequence;and 3) a model for finding constituent boundaries of713the left and right arguments.
Assuming that the mostfrequent words in a language are function words,we can apply orientation statistics associated withthese words to reorder their adjacent left and rightneighbors.
We follow the notation in (Nagata etal., 2006) and define the following bilingual ori-entation values given two neighboring source (Chi-nese) phrases: Monotone-Adjacent (MA); Reverse-Adjacent (RA); Monotone-Gap (MG); and Reverse-Gap (RG).
The first clause (monotone, reverse) in-dicates whether the target language translation orderfollows the source order; the second (adjacent, gap)indicates whether the source phrases are adjacent orseparated by an intervening phrase on the target side.Table 1 shows the orientation statistics for severalfunction words.
Note that we separate the statisticsfor left and right arguments to account for differ-ences in argument structures: some function wordstake a single argument (e.g., prepositions), whileothers take two or more (e.g., copulas).
To han-dle other reordering decisions not explicitly encoded(i.e., lexicalized) in our FWS model, we introduce auniversal token U , to be used as a backoff statisticwhen function words are absent.For example, orientation statistics for 4 (to be)overwhelmingly suggests that the English transla-tion of its surrounding phrases is identical to its Chi-nese ordering.
This reflects the fact that the argu-ments of copulas in both languages are realized inthe same order.
The orientation statistics for post-position ?
(on) suggests inversion which capturesthe divergence between Chinese postposition to theEnglish preposition.
Similarly, the dominant orien-tation for particle { (of) suggests the noun-phraseshift from modified-modifier to modifier-modified,which is common when translating Chinese nounphrases to English.Taking all parts of the model, which we detaillater, together with the knowledge in Table 1, wedemonstrate the steps taken to translate the exam-ple in Fig.
2.
We highlight the function words withboxed characters and encapsulate content words asindexed symbols.
As shown, orientation statisticsfrom function words alone are adequate to recoverthe English ordering - in practice, content words alsoinfluence the reordering through a language model.One can think of the FWS approach as a foreign lan-guage learner with limited knowledge about Chinesegrammar but fairly knowledgable about the role ofChinese function words.,\4 ?
{ j?
Q??
{ ?\X1 4 X2 ?
{ X3 { X4HHj?
X2?9XXXXXzX3 { X5?)XXXXXXXzX4 { X6??
?X1 4 X7X1 4 X4 { X3 { ?
X2,\ 4 ?\ { j?Q??
{ ?
a form is a coll.
of data entry fields on a page#1#2#3 ?
?
?
?
?
?
?
?
?Figure 2: In Step 1, function words (boxed char-acters) and content words (indexed symbols) areidentified.
Step 2 reorders phrases according toknowledge embedded in function words.
A new in-dexed symbol is introduced to indicate previouslyreordered phrases for conciseness.
Step 3 finallymaps Chinese phrases to their English translation.4 The FWS ModelWe first discuss the extension of standard ITG toaccommodate gapping and then detail the statisticalcomponents of the model later.4.1 Single Gap ITG (SG-ITG)The FWS model employs a synchronous grammarto describe the admissible orderings.The utility of ITG as a reordering constraint formost language pairs, is well-known both empirically(Zens and Ney, 2003) and analytically (Wu, 1997),however ITG?s straight (monotone) and inverted (re-verse) rules exhibit strong cohesiveness, which is in-adequate to express orientations that require gaps.We propose SG-ITG that follows Wellington et al(2006)?s suggestion to model at most one gap.We show the rules for SG-ITG below.
Rules 1-3 are identical to those defined in standard ITG, inwhich monotone and reverse orderings are repre-sented by square and angle brackets, respectively.714Rank Word unigram MAL RAL MGL RGL MAR RAR MGR RGR1 { 0.0580 0.45 0.52 0.01 0.02 0.44 0.52 0.01 0.032 ?
0.0507 0.85 0.12 0.02 0.01 0.84 0.12 0.02 0.023  0.0550 0.99 0.01 0.00 0.00 0.92 0.08 0.00 0.004  0.0155 0.87 0.10 0.02 0.00 0.82 0.12 0.05 0.025  0.0153 0.84 0.11 0.01 0.04 0.88 0.11 0.01 0.016 Z 0.0138 0.95 0.02 0.01 0.01 0.97 0.02 0.01 0.007 ?
0.0123 0.73 0.12 0.10 0.04 0.51 0.14 0.14 0.208 ,1 0.0114 0.78 0.12 0.03 0.07 0.86 0.05 0.08 0.019 ?
0.0099 0.95 0.02 0.02 0.01 0.96 0.01 0.02 0.0110 R 0.0091 0.87 0.10 0.01 0.02 0.88 0.10 0.01 0.0021 4 0.0056 0.85 0.11 0.02 0.02 0.85 0.04 0.09 0.0237 ?
0.0035 0.33 0.65 0.02 0.01 0.31 0.63 0.03 0.03- U 0.0002 0.76 0.14 0.06 0.05 0.74 0.13 0.07 0.06Table 1: Orientation statistics and unigram probability of selected frequent Chinese words in the HIT corpus.Subscripts L/R refers to lexical unit?s orientation with respect to its left/right neighbor.
U is the universaltoken used in back-off for N = 128.
Dominant orientations of each word are in bold.
(1) X ?
c/e(2) X ?
[XX] (3) X ?
?XX?
(4) X?
[X X] (5) X?
?X X?
(6) X ?
[X ?X] (7) X ?
?X ?X?SG-ITG introduces two new sets of rules: gap-ping (Rules 4-5) and dovetailing (Rules 6-7) thatdeal specifically with gaps.
On the RHS of the gap-ping rules, a diamond symbol () indicates a gap,while on the LHS, it emits a superscripted symbolX to indicate a gapped phrase (plain Xs withoutsuperscripts are thus contiguous phrases).
Gaps inX are eventually filled by actual phrases via dove-tailing (marked with an ?
on the RHS).Fig.3 illustrates gapping and dovetailing rulesusing an example where two Chinese adjectivalphrases are translated into a single English subordi-nate clause.
SG-ITG can generate the correct order-ing by employing gapping followed by dovetailing,as shown in the following simplified trace:X1 ?
?
1997{??, V.1  1997 ?X2 ?
?
1998{??, V.2  1998 ?X3 ?
[X1 ?X2]?
[ 1997{??
Z 1998{??,V.1  1997 ?
V.2  1998 ]?
1997{??Z1998{??,V.1 and V.2 that were released in 1997 and 1998where X1 and X2 each generate the translation oftheir respective Chinese noun phrase using gappingand X3 generates the English subclause by dovetail-ing the two gapped phrases together.Thus far, the grammar is unlexicalized, and does1997#q{ ??
Z 1998#q{??V.1 and V.2 that were released in 1997 and 1998.!!!!!!
(((((((((((((hhhhhhhhhhhhhPPPPPPPFigure 3: An example of an alignment that can begenerated only by allowing gaps.not incorporate any lexical evidence.
Now we mod-ify the grammar to introduce lexicalized functionwords to SG-ITG.
In practice, we introduce a newset of lexicalized non-terminal symbols Yi, i ?
{1...N}, to represent the topN most-frequent wordsin the vocabulary; the existing unlexicalized X isnow reserved for content words.
This differencedoes not inherently affect the structure of the gram-mar, but rather lexicalizes the statistical model.In this way, although different Yis follow the sameproduction rules, they are associated with differentstatistics.
This is reflected in Rules 8-9.
Rule 8 emitsthe function word; Rule 9 reorders the argumentsaround the function word, resembling our orienta-tion model (see Section 4.2) where a function wordinfluences the orientation of its left and right argu-ments.
For clarity, we omit notation that denoteswhich rules have been applied (monotone, reverse;gapping, dovetailing).
(8) Yi?
c/e (9) X?
XYiXIn practice, we replace Rule 9 with its equivalent2-normal form set of rules (Rules 10-13).
Finally,we introduce rules to handle back-off (Rules 14-16)and upgrade (Rule 17).
These allow SG-ITG to re-715vert function words to normal words and vice versa.
(10) R?
YiX (11) L?
XYi(12) X?
LX (13) X?
XR(14) Yi?
X (15) R?
X(16) L?
X (17) X?
YUBack-off rules are needed when the grammar hasto reorder two adjacent function words, where oneset of orientation statistics must take precedenceover the other.
The example in Fig.1 illustrates sucha case where the orientation of ?
(on) and { (of)compete for influence.
In this case, the grammarchooses to use{ (of) and reverts the function word?
(on) to the unlexicalized form.The upgrade rule is used for cases where there aretwo adjacent phrases, both of which are not functionwords.
Upgrading allows either phrase to act as afunction word, making use of the universal word?sorientation statistics to reorder its neighbor.4.2 Statistical modelWe now formulate the FWS model as a statisticalframework.
We replace the deterministic rules in ourSG-ITG grammar with probabilistic ones, elevatingit to a stochastic grammar.
In particular, we developthe three sub models (see Section 3) which influencethe choice of production rules for ordering decision.These models operate on the 2-norm rules, where theRHS contains one function word and its argument(except in the case of the phrase boundary model).We provide the intuition for these models next, buttheir actual form will be discussed in the next sectionon training.1) Orientation Model ori(o|H,Yi): This modelcaptures the preference of a function word Yi to aparticular orientation o ?
{MA,RA,MG,RG} inreordering its H ?
{left, right} argument X .
Theparameter H determines which set of Yi?s statisticsto use (left or right); the model consults Yi?s left ori-entation statistic for Rules 11 and 13 where X pre-cedes Yi, otherwise Yi?s right orientation statistic isused for Rules 10 and 12.2) Preference Model pref(Yi): This model ar-bitrates reordering in the cases where two functionwords are adjacent and the backoff rules have to de-cide which function word takes precedence, revert-ing the other to the unlexicalized X form.
Thismodel prefers the function word with higher uni-gram probability to take the precedence.3) Phrase BoundaryModel pb(X): This model isa penalty-based model, favoring the resulting align-ment that conforms to the source constituent bound-ary.
It penalizes Rule 1 if the terminal rule Xemits a Chinese phrase that violates the boundary(pb = e?1), otherwise it is inactive (pb = 1).These three sub models act as features alongsideseven other standard SMT features in a log-linearmodel, resulting in the following set of features{f1, .
.
.
, f10}: f1) orientation ori(o|H,Yi); f2)preference pref(Yi); f3) phrase boundary pb(X);f4) language model lm(e); f5 ?
f6) phrase trans-lation score ?
(e|c) and its inverse ?
(c|e); f7 ?
f8)lexical weight lex(e|c) and its inverse lex(c|e); f9)word penalty wp; and f10) phrase penalty pp.The translation is then obtained from the mostprobable derivation of the stochastic SG-ITG.
Theformula for a single derivation is shown in Eq.
(18),where X1, X2, ..., XL is a sequence of rules withw(Xl) being the weight of each particular rule Xl.w(Xl) is estimated through a log-linear model, asin Eq.
(19), with all the abovementioned featureswhere ?j reflects the contribution of each feature fj .P (X1, ..., XL) =?Ll=1w(Xl)(18)w(Xl) =?10j=1fj(Xl)?j(19)5 TrainingWe train the orientation and preference models fromstatistics of a training corpus.
To this end, we firstderive the event counts and then compute the rela-tive frequency of each event.
The remaining phraseboundary model can be modeled by the output of astandard text chunker, as in practice it is simply aconstituent boundary detection mechanism togetherwith a penalty scheme.The events of interest to the orientation model are(Yi, o) tuples, where o ?
{MA,RA,MG,RG} isan orientation value of a particular function wordYi.
Note that these tuples are not directly observablefrom training data.
Hence, we need an algorithm toderive (Yi, o) tuples from a parallel corpus.
Sinceboth left and right statistics share identical trainingsteps, thus we omit references to them.The algorithm to derive (Yi, o) involves severalsteps.
First, we estimate the bi-directional alignment716by running GIZA++ and applying the ?grow-diag-final?
heuristic.
Then, the algorithm enumerates allYi and determines its orientation o with respect toits argument X to derive (Yi, o).
To determine o,the algorithm inspects the monotonicity (monotoneor reverse) and adjacency (adjacent or gap) betweenYi?s and X?s alignments.Monotonicity can be determined by looking at theYi?s alignment with respect to the most fine-grainedlevel of X (i.e., word level alignment).
However,such a heuristic may inaccurately suggest gap ori-entation.
Figure 1 illustrates this problem when de-riving the orientation for the second{ (of).
Look-ing only at the word alignment of its left argument?
(fields) incorrectly suggests a gapped orientation,where the alignment of j?Q?
(data entry) in-tervened.
It is desirable to look at the alignment ofj?Q??
(data entry fields) at the phrase level,which suggests the correct adjacent orientation in-stead.To address this issue, the algorithm uses gap-ping conservatively by utilizing the consistency con-straint (Och and Ney, 2004) to suggest phrase levelalignment of X .
The algorithm exhaustively growsconsistent blocks containing the most fine-grainedlevel of X not including Yi.
Subsequently, it mergeseach hypothetical argument with the Yi?s alignment.The algorithm decides that Yi has a gapped orienta-tion only if all merged blocks violate the consistencyconstraint, concluding an adjacent orientation other-wise.With the event countsC(Yi, o) of tuple (Yi, o), weestimate the orientation model for Yi and U usingEqs.
(20) and (21).
We also estimate the prefer-ence model with word unigram counts C(Yi) usingEqs.
(22) and (23), where V indicates the vocabu-lary size.ori(o|Yi) = C(Yi, o)/C(Yi, ?
), i 6 N(20)ori(o|U) =?i>NC(Yi, o)/?i>NC(Yi, ?
)(21)pref(Yi) = C(Yi)/C(?
), i 6 N(22)pref(U) = 1/(V ?N)?i>NC(Yi)/C(?
)(23)Samples of these statistics are found in Table 1and have been used in the running examples.
Forinstance, the statistic ori(RAL|{) = 0.52, whichis the dominant one, suggests that the grammar in-versely order {(of)?s left argument; while in ourillustration of backoff rules in Fig.1, the grammarchooses{(of) to take precedence since pref({) >pref(?
).6 DecodingWe employ a bottom-up CKY parser with a beamto find the derivation of a Chinese sentence whichmaximizes Eq.
(18).
The English translation is thenobtained by post-processing the best parse.We set the beam size to 30 in our experiment andfurther constrain reordering to occur within a win-dow of 10 words.
Our decoder also prunes entriesthat violate the following constraints: 1) each entrycontains at most one gap; 2) any gapped entries mustbe dovetailed at the next level higher; 3) an entryspanning the whole sentence must not contain gaps.The score of each newly-created entry is derivedfrom the scores of its parts accordingly.
When scor-ing entries, we treat gapped entries as contiguousphrases by ignoring the gap symbol and rely on theorientation model to penalize such entries.
This al-lows a fair score comparison between gapped andcontiguous entries.7 ExperimentsWe would like to study how the FWS model affects1) the ordering of phrases around function words; 2)the overall translation quality.
We achieve this byevaluating the FWS model against a baseline systemusing two metrics, namely, orientation accuracy andBLEU respectively.We define the orientation accuracy of a (function)word as the accuracy of assigning correct orientationvalues to both its left and right arguments.
We reportthe aggregate for the top 1024 most frequent words;these words cover 90% of the test set.We devise a series of experiments and run it in twoscenarios - manual and automatic alignment - to as-sess the effects of using perfect or real-world input.We utilize the HIT bilingual computer manual cor-pus, which has been manually aligned, to performChinese-to-English translation (see Table 2).
Man-ual alignment is essential as we need to measure ori-entation accuracy with respect to a gold standard.717Chinese Englishtrain words 145,731 135,032(7K sentences) vocabulary 5,267 8,064dev words 13,986 14,638(1K sentences) untranslatable 486 (3.47%)test words 27,732 28,490(2K sentences) untranslatable 935 (3.37%)Table 2: Statistics for the HIT corpus.A language model is trained using the SRILM-Toolkit, and a text chunker (Chen et al, 2006) is ap-plied to the Chinese sentences in the test and devsets to extract the constituent boundaries necessaryfor the phrase boundary model.
We run minimum er-ror rate training on dev set using Chiang?s toolkit tofind a set of parameters that optimizes BLEU score.7.1 Perfect Lexical ChoiceHere, the task is simplified to recovering the correctorder of the English sentence from the scrambledChinese order.
We trained the orientation model us-ing manual alignment as input.
The aforementioneddecoder is used with phrase translation, lexical map-ping and penalty features turned off.Table 4 compares orientation accuracy and BLEUbetween our FWS model and the baseline.
Thebaseline (lm+d) employs a language model anddistortion penalty features, emulating the standardPharaoh model.
We study the behavior of theFWS model with different numbers of lexicalizeditems N .
We start with the language model alone(N=0) and incrementally add the orientation (+ori),preference (+ori+pref) and phrase boundary models(+ori+pref+pb).As shown, the language model alone is rela-tively weak, assigning the correct orientation in only62.28% of the cases.
A closer inspection reveals thatthe lm component aggressively promotes reverse re-orderings.
Including a distortion penalty model (thebaseline) improves the accuracy to 72.55%.
Thistrend is also apparent for the BLEU score.When we incorporate the FSW model, includingjust the most frequent word (Y1={), we see im-provement.
This model promotes non-monotone re-ordering conservatively around Y1 (where the dom-inant statistic suggests reverse ordering).
Increasingthe value of N leads to greater improvement.
Themost effective improvement is obtained by increas-pharaoh (dl=5) 22.44 ?
0.94+ori 23.80 ?
0.98+ori+pref 23.85 ?
1.00+ori+pref+pb 23.86 ?
1.08Table 3: BLEU score with the 95% confidence in-tervals based on (Zhang and Vogel, 2004).
All im-provement over the baseline (row 1) are statisticallysignificant under paired bootstrap resampling.ing N to 128.
Additional (marginal) improvementis obtained at the expense of modeling an additional900+ lexical items.
We see these results as validat-ing our claim that modeling the top few most fre-quent words captures most important and prevalentordering productions.Lastly, we study the effect of the pref and pb fea-tures.
The inclusion of both sub models has little af-fect on orientation accuracy, but it improves BLEUconsistently (although not significantly).
This sug-gests that both models correct the mistakes made bythe ori model while preserving the gain.
They arenot as effective as the addition of the basic orienta-tion model as they only play a role when two lexi-calized entries are adjacent.7.2 Full SMT experimentsHere, all knowledge is automatically trained on thetrain set, and as a result, the input word alignmentis noisy.
As a baseline, we use the state-of-the-artphrase-based Pharaoh decoder.
For a fair compari-son, we run minimum error rate training for differentdistortion limits from 0 to 10 and report the best pa-rameter (dl=5) as the baseline.We use the phrase translation table from the base-line and perform an identical set of experiments asthe perfect lexical choice scenario, except that weonly report the result for N=128, due to space con-straint.
Table 3 reports the resulting BLEU scores.As shown, the FWS model improves BLEU scoresignificantly over the baseline.
We observe the sametrend as the one in perfect lexical choice scenariowhere top 128 most frequent words provides the ma-jority of improvement.
However, the pb featuresyields no noticeable improvement unlike in prefectlexical choice scenario; this is similar to the findingsin (Koehn et al, 2003).718N=0 N=1 N=4 N=16 N=64 N=128 N=256 N=1024OrientationAcc.
(%)lm+d 72.55+ori 62.28 76.52 76.58 77.38 77.54 78.17 77.76 78.38+ori+pref 76.66 76.82 77.57 77.74 78.13 77.94 78.54+ori+pref+pb 76.70 76.85 77.58 77.70 78.20 77.94 78.56BLEUlm+d 75.13+ori 66.54 77.54 77.57 78.22 78.48 78.76 78.58 79.20+ori+pref 77.60 77.70 78.29 78.65 78.77 78.70 79.30+ori+pref+pb 77.69 77.80 78.34 78.65 78.93 78.79 79.30Table 4: Results using perfect aligned input.
Here, (lm+d) is the baseline; (+ori), (+ori+pref) and(+ori+pref+pb) are different FWS configurations.
The results of the model (where N is varied) that fea-tures the largest gain are bold, whereas the highest score is italicized.8 ConclusionIn this paper, we present a statistical model to cap-ture the grammatical information encoded in func-tion words.
Formally, we develop the FunctionWordSyntax-based (FWS) model, a probabilistic syn-chronous grammar, to encode the orientation statis-tics of arguments to function words.
Our experimen-tal results shows that the FWS model significantlyimproves the state-of-the-art phrase-based model.We have touched only the surface benefits of mod-eling function words.
In particular, our proposal islimited to modeling function words in the sourcelanguage.
We believe that conditioning on bothsource and target pair would result in more fine-grained, accurate orientation statistics.From our error analysis, we observe that 1) re-ordering may span several levels and the preferencemodel does not handle this phenomena well; 2) cor-rectly reordered phrases with incorrect boundariesseverely affects BLEU score and the phrase bound-ary model is inadequate to correct the boundaries es-pecially for cases of long phrase.
In future, we hopeto address these issues while maintaining the bene-fits offered by modeling function words.ReferencesBenjamin Wellington, Sonjia Waxmonsky, and I. DanMelamed.
2006.
Empirical Lower Bounds onthe Complexity of Translational Equivalence.
InACL/COLING 2006, pp.
977?984.Christoph Tillman and Tong Zhang.
2005.
A LocalizedPrediction Model for Statistical Machine Translation.In ACL 2005, pp.
557?564.David Chiang.
2005.
A Hierarchical Phrase-BasedModel for Statistical Machine Translation.
In ACL2005, pp.
263?270.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3):377?403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase ReorderingModel for Sta-tistical Machine Translation.
In ACL/COLING 2006,pp.
521?528.Franz J. Och and Hermann Ney.
2004.
The AlignmentTemplate Approach to Statistical Machine Translation.Computational Linguistics, 30(4):417?449.Masaaki Nagata, Kuniko Saito, Kazuhide Yamamoto,and Kazuteru Ohashi.
2006.
A Clustered GlobalPhrase Reordering Model for Statistical MachineTranslation.
In ACL/COLING 2006, pp.
713?720.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, Robert L. Mercer 1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19(2):263?311.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In HLT-NAACL2003, pp.
127?133.Richard Zens and Hermann Ney.
2003.
A Compara-tive Study on Reordering Constraints in Statistical Ma-chine Translation.
In ACL 2003.Taro Watanabe, Hajime Tsukada, and Hideki Isozaki.2006.
Left-to-Right Target Generation for Hierarchi-cal Phrase-Based Translation.
In ACL/COLING 2006,pp.
777?784.Wenliang Chen, Yujie Zhang and Hitoshi Isahara 2006.An Empirical Study of Chinese Chunking In ACL2006 Poster Sessions, pp.
97?104.Yaser Al-Onaizan and Kishore Papineni.
2006.
Distor-tion Models for Statistical Machine Translation.
InACL/COLING 2006, pp.
529?536.Ying Zhang and Stephan Vogel.
2004.
Measuring Confi-dence Intervals for theMachine Translation EvaluationMetrics.
In TMI 2004.719
