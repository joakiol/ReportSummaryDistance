Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 1?6,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsClassification of semantic relations by humans and machines ?Erwin Marsi and Emiel KrahmerCommunication and CognitionTilburg University, The Netherlands{e.c.marsi, e.j.krahmer}@uvt.nlAbstractThis paper addresses the classification ofsemantic relations between pairs of sen-tences extracted from a Dutch parallel cor-pus at the word, phrase and sentence level.We first investigate the performance of hu-man annotators on the task of manuallyaligning dependency analyses of the re-spective sentences and of assigning oneof five semantic relations to the alignedphrases (equals, generalizes, specifies, re-states and intersects).
Results indicate thathumans can perform this task well, withan F-score of .98 on alignment and an F-score of .95 on semantic relations (aftercorrection).
We then describe and evalu-ate a combined alignment and classifica-tion algorithm, which achieves an F-scoreon alignment of .85 (using EuroWordNet)and an F-score of .80 on semantic relationclassification.1 IntroductionAn automatic method that can determine how twosentences relate to each other in terms of seman-tic overlap or textual entailment (e.g., (Dagan andGlickman, 2004)) would be a very useful thing tohave for robust natural language applications.
Asummarizer, for instance, could use it to extractthe most informative sentences, while a question-answering system ?
to give a second example ?could use it to select potential answer string (Pun-yakanok et al, 2004), perhaps preferring more spe-cific answers over more general ones.
In general, it?This work was carried out within the IMIX-IMOGEN (In-teractive Multimodal Output Generation) project, sponsored bythe Netherlands Organization of Scientific Research (NWO).is very useful to know whether some sentence S ismore specific (entails) or more general than (is en-tailed by) an alternative sentence S?, or whether thetwo sentences express essentially the same informa-tion albeit in a different way (paraphrasing).Research on automatic methods for recognizingsemantic relations between sentences is still rela-tively new, and many basic issues need to be re-solved.
In this paper we address two such related is-sues: (1) to what extent can human annotators labelsemantic overlap relations between words, phrasesand sentences, and (2) what is the added value oflinguistically informed analyses.It is generally assumed that pure string overlapis not sufficient for recognizing semantic relations;and that using some form of syntactic analysis maybe beneficial (e.g., (Herrera et al, 2005), (Vander-wende et al, 2005)).
Our working hypothesis is thatsemantic overlap at the word and phrase levels mayprovide a good basis for deciding the semantic re-lation between sentences.
Recognising semantic re-lations between sentences then becomes a two-stepprocedure: first, the words and phrases in the re-spective sentences need to be aligned, after whichthe relations between the pairs of aligned words andphrases should be labeled in terms of semantic rela-tions.Various alignment algorithms have been devel-oped for data-driven approaches to machine trans-lation (e.g.
(Och and Ney, 2000)).
Initially workfocused on word-based alignment, but more andmore work is also addressing alignment at the higherlevels (substrings, syntactic phrases or trees), e.g.,(Meyers et al, 1996), (Gildea, 2003).
For our pur-poses, an additional advantage of aligning syntac-tic structures is that it keeps the alignment feasible(as the number of arbitrary substrings that may bealigned grows exponentially to the number of words1in the sentence).
Here, following (Herrera et al,2005) and (Barzilay, 2003), we will align sentencesat the level of dependency structures.
In addition,we will label the alignments in terms of five basicsemantic relations to be defined below.
We will per-form this task both manually and automatically, sothat we can address both of the issues raised above.Section 2 describes a monolingual parallel cor-pus consisting of two Dutch translations, and for-malizes the alignment-classification task to be per-formed.
In section 3 we report the results on align-ment, first describing interannotator agreement onthis task and then the results on automatic alignment.In section 4, then, we address the semantic relationclassification; again, first describing interannotatorresults, followed by results obtained using memory-based machine learning techniques.
We end with ageneral discussion.2 Corpus and Task definition2.1 CorpusWe have developed a parallel monolingual corpusconsisting of two different Dutch translations of theFrench book ?Le petit prince?
(the little prince) byAntoine de Saint-Exupe?ry (published 1943), one byLaetitia de Beaufort-van Hamel (1966) and one byErnst Altena (2000).
For our purposes, this provedto be a good way to quickly find a large enoughset of related sentence pairs, which differ semanti-cally in interesting and subtle ways.
In this work,we used the first five chapters, with 290 sentencesand 3600 words in the first translation, and 277 sen-tences and 3358 words in the second translation.The texts were automatically tokenized and split intosentences, after which errors were manually cor-rected.
Corresponding sentences from both trans-lations were manually aligned; in most cases thiswas a one-to-one mapping, but occasionally a sin-gle sentence in one translation mapped onto two ormore sentences in the other: this occurred 23 timesin all five chapters.
Next, the Alpino parser forDutch (e.g., (Bouma et al, 2001)) was used for part-of-speech tagging and lemmatizing all words, andfor assigning a dependency analysis to all sentences.The POS labels indicate the major word class (e.g.verb, noun, adj, and adv).
The dependency rela-tions hold between tokens and are identical to thoseused in the Spoken Dutch Corpus.
These include de-pendencies such as head/subject, head/modifier andcoordination/conjunction.
If a full parse could notbe obtained, Alpino produced partial analyses col-lected under a single root node.
Errors in lemmati-zation, POS tagging, and syntactic dependency pars-ing were not subject to manual correction.2.2 Task definitionThe task to be performed can be described infor-mally as follows: given two dependency analyses,align those nodes that are semantically related.
Moreprecisely: For each node v in the dependency struc-ture for a sentence S, we define STR(v) as the sub-string of all tokens under v (i.e., the composition ofthe tokens of all nodes reachable from v).
An align-ment between sentences S and S?
pairs nodes fromthe dependency graphs for both sentences.
Aligningnode v from the dependency graph D of sentenceS with node v?
from the graph D?
of S?
indicatesthat there is a semantic relation between STR(v) andSTR(v?
), that is, between the respective substringsassociated with v and v?.
We distinguish five po-tential, mutually exclusive, relations between nodes(with illustrative examples):1. v equals v?
iff STR(v) and STR(v?)
are literallyidentical (abstracting from case).
Example: ?asmall and a large boa-constrictor?
equals ?alarge and a small boa-constrictor?;2.
v restates v?
iff STR(v) is a paraphrase ofSTR(v?)
(same information content but differ-ent wording).
Example: ?a drawing of a boa-constrictor snake?
restates ?a drawing of a boa-constrictor?;3.
v specifies v?
iff STR(v) is more specific thanSTR(v?).
Example: ?the planet B 612?
specifies?the planet?;4.
v generalizes v?
iff STR(v?)
is more specificthan STR(v).
Example: ?the planet?
general-izes ?the planet B 612?;5.
v intersects v?
iff STR(v) and STR(v?)
sharesome informational content, but also each ex-press some piece of information not expressedin the other.
Example: ?Jupiter and Mars?
in-tersects ?Mars and Venus?Figure 1 shows an example alignment with seman-tic relations between the dependency structures of2hebbenkomenhebbenikik op in in aanraking metzo contact met in de loop vanveelheelpersoonserieus veelmassagewichtigheellevenmijnlevenhetmanierdie mensFigure 1: Dependency structures and alignment for the sentences Zo heb ik in de loop van mijn leven heelveel contacten gehad met heel veel serieuze personen.
(lit.
?Thus have I in the course of my life verymany contacts had with very many serious persons?)
and Op die manier kwam ik in het leven met massa?sgewichtige mensen in aanraking.. (lit.
?In that way came I in the life with mass-of weighty/important peoplein touch?).
The alignment relations are equals (dotted gray), restates (solid gray), specifies (dotted black),and intersects (dashed gray).
For the sake of transparency, dependency relations have been omitted.two sentences.
Note that there is an intuitive rela-tion with entailment here: both equals and restatescan be understood as mutual entailment (i.e., if theroot nodes of the analyses corresponding S and S?stand in an equal or restate relation, S entails S?
andS?
entails S), if S specifies S?
then S also entails S?and if S generalizes S?
then S is entailed by S?.In remainder of this paper, we will distinguish twoaspects of this task: alignment is the subtask of pair-ing related nodes ?
or more precise, pairing the to-ken strings corresponding to these nodes; classifica-tion of semantic relations is the subtask of labelingthese alignments in terms of the five types of seman-tic relations.2.3 Annotation procedureFor creating manual alignments, we developed aspecial-purpose annotation tool which shows, sideby side, two sentences, as well as their respectivedependency graphs.
When the user clicks on a nodev in the graph, the corresponding string (STR(v)) isshown at the bottom.
The tool enables the user tomanually construct an alignment graph on the basisof the respective dependency graphs.
This is done byfocusing on a node in the structure for one sentence,and then selecting a corresponding node (if possible)in the other structure, after which the user can selectthe relevant alignment relation.
The tool offers addi-tional support for folding parts of the graphs, high-lighting unaligned nodes and hiding dependency re-lation labels.All text material was aligned by the two authors.They started with annotating the first ten sentencesof chapter one together in order to get a feel forthe task.
They continued with the remaining sen-tences from chapter one individually (35 sentencesand 521 in the first translation, and 35 sentences and481 words in the second translation).
Next, bothannotators discussed annotation differences, whichtriggered some revisions in their respective annota-tion.
They also agreed on a single consensus annota-tion.
Interannotator agreement will be discussed inthe next two sections.
Finally, each author annotatedtwo additional chapters, bringing the total to five.3 Alignment3.1 Interannotator agreementInterannotator agreement was calculated in terms ofprecision, recall and F-score (with ?
= 1) on aligned3(A1, A2) (A1?
, A2?)
(Ac, A1?)
(Ac, A2?
)#real: 322 323 322 322#pred: 312 321 323 321#correct: 293 315 317 318precision: .94 .98 .98 .99recall: .91 .98 .98 .99F-score: .92 .98 .98 .99Table 1: Interannotator agreement with respectto alignment between annotators 1 and 2 before(A1, A2) and after (A1?
, A2?)
revision , and betweenthe consensus and annotator 1 (Ac, A1?)
and annota-tor 2 (Ac, A2?)
respectively.node pairs as follows:precision = | Areal ?
Apred | / | Apred | (1)recall = | Areal ?
Apred | / | Areal | (2)F -score = (2 ?
prec ?
rec) / (prec + rec) (3)where Areal is the set of all real alignments (the ref-erence or golden standard), Apred is the set of allpredicted alignments, and Apred?Areal is the set alcorrectly predicted alignments.
For the purpose ofcalculating interannotator agreement, one of the an-notations (A1) was considered the ?real?
alignment,the other (A2) the ?predicted?.
The results are sum-marized in Table 1 in column (A1, A2).1As explained in section 2.3, both annotators re-vised their initial annotations.
This improved theiragreement, as shown in column (A1?
, A2?).
In ad-dition, they agreed on a single consensus annotation(Ac).
The last two columns of Table 1 show the re-sults of evaluating each of the revised annotationsagainst this consensus annotation.
The F-score of.98 can therefore be regarded as the upper bound onthe alignment task.3.2 Automatic alignmentOur tree alignment algorithm is based on the dy-namic programming algorithm in (Meyers et al,1996), and similar to that used in (Barzilay, 2003).It calculates the match between each node in de-pendency tree D against each node in dependencytree D?.
The score for each pair of nodes only de-pends on the similarity of the words associated withthe nodes and, recursively, on the scores of the best1Note that since there are no classes, we can not calculatechange agreement rethe Kappa statistic.matching pairs of their descendants.
The node simi-larity function relies either on identity of the lemmasor on synonym, hyperonym, and hyponym relationsbetween them, as retrieved from EuroWordNet.Automatic alignment was evaluated with the con-sensus alignment of the first chapter as the goldstandard.
A baseline was constructed by aligningthose nodes which stand in an equals relation to eachother, i.e., a node v in D is aligned to a node v?in D?
iff STR(v) =STR(v?).
This baseline alreadyachieves a relatively high score (an F-score of .56),which may be attributed to the nature of our mate-rial: the translated sentence pairs are relatively closeto each other and may show a sizeable amount of lit-eral string overlap.
In order to test the contributionof synonym and hyperonym information for nodematching, performance is measured with and with-out the use of EuroWordNet.
The results for auto-matic alignment are shown in Table 2.
In compari-son with the baseline, the alignment algorithm with-out use of EuroWordnet loses a few points on preci-sion, but improves a lot on recall (a 200% increase),which in turn leads to a substantial improvement onthe overall F-score.
The use of EurWordNet leads toa small increase (two points) on both precision andrecall, and thus to small increase in F-score.
How-ever, in comparison with the gold standard humanscore for this task (.95), there is clearly room forfurther improvement.4 Classification of semantic relations4.1 Interannotator agreementIn addition to alignment, the annotation procedurefor the first chapter of The little prince by two anno-tators (cf.
section 2.3) also involved labeling of thesemantic relation between aligned nodes.
Interanno-tator agreement on this task is shown Table 3, beforeand after revision.
The measures are weighted preci-sion, recall and F-score.
For instance, the precisionis the weighted sum of the separate precision scoresfor each of the five relations.
The table also showsthe ?-score.
The F-score of .97 can be regarded asthe upper bound on the relation labeling task.
Wethink these numbers indicate that the classificationof semantic relations is a well defined task whichcan be accomplished with a high level of interanno-tator agreement.4Alignment : Prec : Rec : F-score:baseline .87 .41 .56algorithm without wordnet .84 .82 .83algorithm with wordnet .86 .84 .85Table 2: Precision, recall and F-score on automaticalignment(A1, A2) (A1?
, A2?)
(Ac, A1?)
(Ac, A2?
)precision: .86 .96 .98 .97recall: .86 .95 .97 .97F-score: .85 .95 .97 .97?
: .77 .92 .96 .96Table 3: Interannotator agreement with respect to se-mantic relation labeling between annotators 1 and 2before (A1, A2) and after (A1?
, A2?)
revision , andbetween the consensus and annotator 1 (Ac, A1?
)and annotator 2 (Ac, A2?)
respectively.4.2 Automatic classificationFor the purpose of automatic semantic relation la-beling, we approach the task as a classification prob-lem to be solved by machine learning.
Alignmentsbetween node pairs are classified on the basis of thelexical-semantic relation between the nodes, theircorresponding strings, and ?
recursively ?
on previ-ous decisions about the semantic relations of daugh-ter nodes.
The input features used are:?
a boolean feature representing string identitybetween the strings corresponding to the nodes?
a boolean feature for each of the five semanticrelations indicating whether the relation holdsfor at least one of the daughter nodes;?
a boolean feature indicating whether at leastone of the daughter nodes is not aligned;?
a categorical feature representing the lexical se-mantic relation between the nodes (i.e.
thelemmas and their part-of-speech) as found inEuroWordNet, which can be synonym, hyper-onym, or hyponym.2To allow for the use of previous decisions, thenodes of the dependency analyses are traversed ina bottom-up fashion.
Whenever a node is aligned,the classifier assigns a semantic label to the align-ment.
Taking previous decisions into account may2These three form the bulk of all relations in Dutch Eu-roWordnet.
Since no word sense disambiguation was involved,we simply used all word senses.Prec : Rec : F-score:equals .93?
.06 .95?
.04 .94?
.02restates .56?
.08 .78?
.04 .65?
.05specifies n.a.
0 n.a.generalizes .19?
.06 .37?
.09 .24?
.05intersects n.a.
0 n.a.Combined: .62?
.01 .70?
.02 .64?
.02Table 4: Average precision, recall and F-score (andSD) over all 5 folds on automatic classification ofsemantic relationscause a proliferation of errors: wrong classificationof daughter nodes may in turn cause wrong classifi-cation of the mother node.
To investigate this risk,classification experiments were run both with andwithout (i.e.
using the annotation) previous deci-sions.Since our amount of data is limited, we useda memory-based classifier, which ?
in contrast tomost other machine learning algorithms ?
performsno abstraction, allowing it to deal with productivebut low-frequency exceptions typically occurring inNLP tasks(Daelemans et al, 1999).
All memory-based learning was performed with TiMBL, version5.1 (Daelemans et al, 2004), with its default set-tings (overlap distance function, gain-ratio featureweighting, k = 1).The five first chapters of The little prince wereused to run a 5-fold cross-validated classification ex-periment.
The first chapter is the consensus align-ment and relation labeling, while the other four weredone by one out of two annotators.
The alignmentsto be classified are those from to the human align-ment.
The baseline of always guessing equals ?
themajority class ?
gives a precision of 0.26, a recall of0.51, and an F-score of 0.36.
Table 4 presents the re-sults broken down to relation type.
The combined F-score of 0.64 is almost twice the baseline score.
Asexpected, the highest score goes to equals, followedby a reasonable score on restates.
Performance onthe other relation types is rather poor, with even nopredictions of specifies and intersects at all.Faking perfect previous decisions by using theannotation gives a considerable improvement, asshown in Table 5, especially on specifies, general-izes and intersects.
This reveals that the prolifera-tion of classification errors is indeed a problem thatshould be addressed.5Prec : Rec : F-score:equals .99?
.02 .97?
.02 .98?
.01restates .65?
.04 .82?
.04 .73?
.03specifies .60?
.12 .48?
.10 .53?
.09generalizes .50?
.11 .52?
.10 .50?
.09intersects .69?
.27 .35?
.12 .46?
.16Combined: .82?
.02 .81?
.02 .80?
.02Table 5: Average precision, recall and F-score (andSD) over all 5 folds on automatic classification ofsemantic relations without using previous decisions.In sum, these results show that automatic classifi-cation of semantic relations is feasible and promis-ing ?
especially when the proliferation of classifica-tion errors can be prevented ?
but still not nearly asgood as human performance.5 Discussion and Future workThis paper presented an approach to detecting se-mantic relations at the word, phrase and sentencelevel on the basis of dependency analyses.
We inves-tigated the performance of human annotators on thetasks of manually aligning dependency analyses andof labeling the semantic relations between alignednodes.
Results indicate that humans can perform thistask well, with an F-score of .98 on alignment and anF-score of .92 on semantic relations (after revision).We also described and evaluated automatic methodsaddressing these tasks: a dynamic programming treealignment algorithm which achieved an F-score onalignment of .85 (using lexical semantic informationfrom EuroWordNet), and a memory-based seman-tic relation classifier which achieved F-scores of .64and .80 with and without using real previous deci-sions respectively.One of the issues that remains to be addressedin future work is the effect of parsing errors.
Sucherrors were not corrected, but during manual align-ment, we sometimes found that substrings could notbe properly aligned because the parser had failed toidentify them as syntactic constituents.
As far asclassification of semantic relations is concerned, theproliferation of classification errors is an issue thatneeds to be solved.
Classification performance maybe further improved with additional features (e.g.phrase length information), optimization, and moredata.
Also, we have not yet tried to combine au-tomatic alignment and classification.
Yet anotherpoint concerns the type of text material.
The sen-tence pairs from our current corpus are relativelyclose, in the sense that both translations more or lessconvey the same information.
Although this seems agood starting point to study alignment, we intend tocontinue with other types of text material in futurework.
For instance, in extending our work to the ac-tual output of a QA system, we expect to encountersentences with far less overlap.ReferencesR.
Barzilay.
2003.
Information Fusion for Multidocu-ment Summarization.
Ph.D. Thesis, Columbia Univer-sity.G.
Bouma, G. van Noord, and R. Malouf.
2001.
Alpino:Wide-coverage computational analysis of Dutch.
InComputational Linguistics in The Netherlands 2000,pages 45?59.W.
Daelemans, A.
Van den Bosch, and J. Zavrel.
1999.Forgetting exceptions is harmful in language learning.Machine Learning, Special issue on Natural LanguageLearning, 34:11?41.W.
Daelemans, J. Zavrel, K. Van der Sloot, andA.
van den Bosch.
2004.
TiMBL: Tilburg memorybased learner, version 5.1, reference guide.
ILK Tech-nical Report 04-02, Tilburg University.I.
Dagan and O. Glickman.
2004.
Probabilistic textualentailment: Generic applied modelling of languagevariability.
In Learning Methods for Text Understand-ing and Mining, Grenoble.D.
Gildea.
2003.
Loosely tree-based alignment for ma-chine translation.
In Proceedings of the 41st AnnualMeeting of the ACL, Sapporo, Japan.J.
Herrera, A. Pe nas, and F. Verdejo.
2005.
Textualentailment recognition based on dependency analy-sis and wordnet.
In Proceedings of the 1st.
PASCALRecognision Textual Entailment Challenge Workshop.Pattern Analysis, Statistical Modelling and Computa-tional Learning, PASCAL.A.
Meyers, R. Yangarber, and R. Grisham.
1996.
Align-ment of shared forests for bilingual corpora.
In Pro-ceedings of 16th International Conference on Com-putational Linguistics (COLING-96), pages 460?465,Copenhagen, Denmark.F.J.
Och and H. Ney.
2000.
Statistical machine trans-lation.
In EAMT Workshop, pages 39?46, Ljubljana,Slovenia.V.
Punyakanok, D. Roth, and W. Yih.
2004.
Natural lan-guage inference via dependency tree mapping: An ap-plication to question answering.
Computational Lin-guistics, 6(9).L.
Vanderwende, D. Coughlin, and W. Dolan.
2005.What syntax can contribute in entailment task.
In Pro-ceedings of the 1st.
PASCAL Recognision Textual En-tailment Challenge Workshop, Southampton, U.K.6
