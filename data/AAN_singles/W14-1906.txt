Proceedings of the 5th Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 38?45,Baltimore, Maryland USA, August 26 2014. c?2014 Association for Computational LinguisticsPreliminary Test of a Real-Time, Interactive Silent Speech InterfaceBased on Electromagnetic ArticulographJun WangDept.
of BioengineeringCallier Center for Communi-cation DisordersUniversity of Texas at Dallaswangjun@utdallas.eduAshok SamalDept.
of Computer Science &EngineeringUniversity of Nebraska-Lincolnsamal@cse.unl.eduJordan R. GreenDept.
of Communication Sci-ences & DisordersMGH Institute of Health Pro-fessionsjgreen2@mghihp.eduAbstractA silent speech interface (SSI) maps articula-tory movement data to speech output.
Alt-hough still in experimental stages, silentspeech interfaces hold significant potentialfor facilitating oral communication in personsafter laryngectomy or with other severe voiceimpairments.
Despite the recent efforts on si-lent speech recognition algorithm develop-ment using offline data analysis, online testof SSIs have rarely been conducted.
In thispaper, we present a preliminary, online test ofa real-time, interactive SSI based on electro-magnetic motion tracking.
The SSI playedback synthesized speech sounds in responseto the user?s tongue and lip movements.Three English talkers participated in this test,where they mouthed (silently articulated)phrases using the device to complete aphrase-reading task.
Among the three partici-pants, 96.67% to 100% of the mouthedphrases were correctly recognized and corre-sponding synthesized sounds were played af-ter a short delay.
Furthermore, one participantdemonstrated the feasibility of using the SSIfor a short conversation.
The experimental re-sults demonstrated the feasibility and poten-tial of silent speech interfaces based on elec-tromagnetic articulograph for future clinicalapplications.1 IntroductionDaily communication is often a struggle for per-sons who have undergone a laryngectomy, a sur-gical removal of the larynx due to the treatmentof cancer (Bailey et al., 2006).
In 2013, about12,260 new cases of laryngeal cancer were esti-mated in the United States (American CancerSociety, 2013).
Currently, there are only limitedtreatment options for these individuals including(1) esophageal speech, which involves oscillationof the esophagus and is difficult to learn; (2) tra-cheo-esophageal speech, in which a voice pros-thesis is placed in a tracheo-esophageal puncture;and (3) electrolarynx, an external device held onthe neck during articulation, which produces arobotic voice quality (Liu and Ng, 2007).
Per-haps the greatest disadvantage of these ap-proaches is that they produce abnormal soundingspeech with a fundamental frequency that is lowand limited in range.
The abnormal voice qualityoutput severely affects the social life of peopleafter laryngectomy (Liu and Ng, 2007).
In addi-tion, the tracheo-esophageal option requires anadditional surgery, which is not suitable for eve-ry patient (Bailey et al., 2006).
Although re-search is being conducted on improving thevoice quality of esophageal or electrolarynxspeech (Doi et al., 2010; Toda et al., 2012), newassistive technologies based on non-audio infor-mation (e.g., visual or articulatory information)may be a good alternative approach for providingnatural sounding speech output for persons afterlaryngectomy.Visual speech recognition (or automatic lipreading) typically uses an optical camera to ob-tain lip and/or facial features during speech (in-cluding lip contour, color, opening, movement,etc.)
and then classify these features to speechunits (Meier et al., 2000; Oviatt, 2003).
Howev-er, due to the lack of information from tongue,the primary articulator, visual speech recognition(i.e., using visual information only, withouttongue and audio information) may obtain a lowaccuracy (e.g., 30% - 40% for phoneme classifi-cation, Livescu et al., 2007).
Furthermore, Wangand colleagues (2013b) have showed any singletongue sensor (from tongue tip to tongue body38Figure 1.
Design of the real-time silent speech interface.back on the midsagittal line) encodes significant-ly more information in distinguishing phonemesthan do lips.
However, visual speech recognitionis well suited for applications with small-vocabulary (e.g., a lip-reading based command-and-control system for home appliance) or usingvisual information as an additional source foracoustic speech recognition, referred to as audio-visual speech recognition (Potamianos et al.,2003), because such a system based on portablecamera is convenient in practical use.
In contrast,SSIs, with tongue information, have potential toobtain a high level of silent speech recognitionaccuracy (without audio information).
Currently,two major obstacles for SSI development arelack of (a) fast and accurate recognition algo-rithms and (b) portable tongue motion trackingdevices for daily use.SSIs convert articulatory information into textthat drives a text-to-speech synthesizer.
Althoughstill in developmental stages (e.g., speaker-dependent recognition, small-vocabulary), SSIseven have potential to provide speech outputbased on prerecorded samples of the patient?sown voice (Denby et al., 2010; Green et al.,2011; Wang et al., 2009).
Potential articulatorydata acquisition methods for SSIs include ultra-sound (Denby et al., 2011; Hueber et al., 2010),surface electromyography electrodes (Heaton etal., 2011; Jorgensen and Dusan, 2010), and elec-tromagnetic articulograph (EMA) (Fagan et al.,2008; Wang et al., 2009, 2012a).Despite the recent effort on silent speech in-terface research, online test of SSIs has rarelybeen studied.
So far, most of the published workon SSIs has focused on development of silentspeech recognition algorithm through offlineanalysis (i.e., using prerecorded data) (Fagan etal., 2008;  Heaton et al., 2011; Hofe et al., 2013;Hueber et al., 2010; Jorgenson et al., 2010; Wanget al., 2009a, 2012a, 2012b, 2013c).
Ultrasound-based SSIs have been tested online with multiplesubjects and encouraging results were obtainedin a phrase reading task where the subjects wereasked to silently articulate sixty phrases (Denbyet al., 2011).
SSI based on electromagnetic sens-ing has been only tested using offline analysis(using pre-recorded data) collected from singlesubjects (Fagan et al., 2008; Hofe et al., 2013),although some work simulated online testingusing prerecorded data (Wang et al., 2012a,2012b, 2013c).
Online tests of SSIs using elec-tromagnetic articulograph with multiple subjectsare needed to show the feasibility and potentialof the SSIs for future clinical applications.In this paper, we report a preliminary, onlinetest of a newly-developed, real-time, and interac-tive SSI based on a commercial EMA.
EMAtracks articulatory motion by placing small sen-sors on the surface of tongue and other articula-tors (e.g., lips and jaw).
EMA is well suited forthe early state of SSI development because it (1)is non-invasive, (2) has a high spatial resolutionin motion tracking, (3) has a high sampling rate,and (4) is affordable.
In this experiment, partici-pants used the real-time SSI to complete anonline phrase-reading task and one of them had ashort conversation with another person.
The re-sults demonstrated the feasibility and potential ofSSIs based on electromagnetic sensing for futureclinical applications.2 Design2.1 Major designFigure 1 illustrates the three-component designof the SSI: (a) real-time articulatory motiontracking using a commercial EMA, (b) onlinesilent speech recognition (converting articulationinformation to text), and (c) text-to-speech syn-thesis for speech output.The EMA system (Wave Speech Research 39Figure 2.
Demo of a participant using the silent speech interface.
The left picture illustrates thecoordinate system and sensor locations (sensor labels are described in text); in the right picture, aparticipant is using the silent speech interface to finish the online test.system, Northern Digital Inc., Waterloo, Canada)was used to track the tongue and lip movementin real-time.
The sampling rate of the Wave sys-tem was 100 Hz, which is adequate for this ap-plication (Wang et al., 2012a, 2012b, 2013c).The spatial accuracy of motion tracking usingWave is 0.5 mm (Berry, 2011).The online recognition component recognizedfunctional phrases from articulatory movementsin real-time.
The recognition component is mod-ular such that alternative classifiers can easilyreplace and be integrated into the SSI.
In thispreliminary test, recognition was speaker-dependent, where training and testing data werefrom the same speakers.The third component played back either pre-recorded or synthesized sounds using a text-to-speech synthesizer (Huang et al., 1997).2.2 Other designsA graphical user interface (GUI) is integratedinto the silent speech interface for ease of opera-tion.
Using the GUI, users can instantly re-trainthe recognition engine (classifier) when newtraining samples are available.
Users can alsoswitch output voice (e.g., male or female).Data transfer through TCP/IP.
Data transferfrom the Wave system to the recognition unit(software) is accomplished through TCP/IP, thestandard data transfer protocols on Internet.
Be-cause data bandwidth requirement is low (multi-ple sensors, multiple spatial coordinates for eachsensor, at 100 Hz sampling rate), any 3G or fast-er network connection will be sufficient for fu-ture use with wireless data transfer.Extensible (closed) vocabulary.
In the earlystage of this development, closed-vocabularysilent speech recognition was used; however, thevocabulary is extensible.
Users can add newphrases into the system through the GUI.
Addinga new phrase in the vocabulary is done in twosteps.
The user (the patient) first enters thephrase using a keyboard (keyboard input can alsobe done by an assistant or speech pathologist),and then produces a few training samples for thephrase (a training sample is articulatory data la-beled with a phrase).
The system automaticallyre-trains the recognition model integrating thenewly-added training samples.
Users can deleteinvalid training samples using the GUI as well.2.3 Real-time data processingThe tongue and lip movement positional dataobtained from the Wave system were processedin real-time prior to being used for recognition.This included the calculation of head-independent positions of the tongue and lip sen-sors and low pass filtering for removing noise.The movements of the 6 DOF head sensorwere used to calculate the head-independentmovements of other sensors.
The Wave systemrepresents object orientation or rotation (denotedby yaw, pitch, and roll in Euler angles) in qua-ternions, a four-dimensional vector.
Quaternionhas its advantages over Euler angles.
For exam-ple, quaternion avoids the issue of gimbal lock(one degree of freedom may be lost in a series ofrotation), and it is simpler to achieve smooth in-terpolation using quaternion than using Eulerangles (Dam et al., 1998).
Thus, quaternion hasbeen widely used in computer graphics, comput-er vision, robotics, virtual reality, and flight dy-namics (Kuipers, 1999).
Given the unit quaterni-onq = (a, b, c, d)                        (1)where a2 + b2 + c2 + d2 = 1, a 3 ?
3 rotation ma-trix R can be derived using Equation (2): 40??????????+??+???+?++??
?+=222222222222222222222222dcbaabcdacbdabcddcbaadbcacbdadbcdcbaR(2)For details of how the quaternion is used inWave system, please refer to the Wave Real-Time API manual and sample application(Northern Digital Inc., Waterloo, Canada).3 A Preliminary Online Test3.1 Participants & StimuliThree American English talkers participated inthis experiment (two males and one female withaverage age 25 and SD 3.5 years).
No history ofspeech, language, hearing, or any cognitive prob-lems were reported.Sixty phrases that are frequently used in dailylife by healthy people and AAC (augmentativeand alternative communication) users were usedin this experiment.
Those phrases were selectedfrom the lists in Wang et al., 2012a and Beukel-man and Gutmann, 1999.3.2 ProcedureSetupThe Wave system tracks the motion of sensorsattached on the articulators by establishing anelectromagnetic field by a textbook-sized genera-tor.
Participants were seated with their headwithin the calibrated magnetic field (Figure 2,the right picture), facing a computer monitor thatdisplays the GUI of the SSI.
The sensors wereattached to the surface of each articulator usingdental glue (PeriAcryl Oral Tissue Adhesive).Prior to the experiment, each subject participatedin a three-minute training session (on how to usethe SSI), which also helped them adapt to theoral sensors.
Previous studies have shown thosesensors do not significantly affect their speechoutput after a short practice (Katz et al., 2006;Weismer and Bunton, 1999).Figure 2 (left) shows the positions of the fivesensors attached to a participant?s forehead,tongue, and lips (Green et al., 2003; 2013; Wanget al., 2013a).
One 6 DOF (spatial and rotational)head sensor was attached to a nose bridge on apair of glasses (rather than on forehead skin di-rectly), to avoid the skin artifact (Green et al.,2007).
Two 5 DOF sensors - TT (Tongue Tip)and TB (Tongue Body Back) - were attached onthe midsagittal of the tongue.
TT was locatedapproximately 10 mm from the tongue apex(Wang et al., 2011, 2013a).
TB was placed as farback as possible, depending on the participant?stongue length (Wang et al., 2013b).
Lip move-ments were captured by attaching two 5 DOFsensors to the vermilion borders of the upper(UL) and lower (LL) lips at midline.
The foursensors (i.e., TT, TB, UL, and LL) placementswere selected based on literature showing thatthey are able to achieve as high recognition accu-racy as that obtained using more tongue sensorsfor this application (Wang et al., 2013b).As mentioned previously, real-time prepro-cessing of the positional time series was con-ducted, including subtraction of head movementsfrom tongue and lip data and noise reduction us-ing a 20 Hz low pass filter (Green et al., 2003;Wang et al., 2013a).
Although the tongue and lipsensors are 5D, only the 3D spatial data (i.e., x, y,and z) were used in this experiment.TrainingThe training step was conducted to obtain a fewsamples for each phrase.
The participants wereasked to silently articulate all sixty phrases twiceat their comfortable speaking rate, while thetongue and lip motion was recorded.
Thus, eachphrase has at least two samples for training.
Dy-namic Time Warping (DTW) was used as theclassifier in this preliminary test, because of itsrapid execution (Denby et al., 2011), althoughGaussian mixture models may perform well toowhen the number of training samples is small(Broekx et al., 2013).
DTW is typically used tocompare two single-dimensional time-series,Training_AlgorithmLet T1?
Tn  be the sets of training samples for nphrases, whereTi = {Ti,1, ?
Ti,j, ?
Ti,mi} are mi samples forphrase i.1    for i = 1 to n     // n is the number of phrases2 Li = sum(length(Ti,j)) / mi,  j = 1 to mi;3 T = Ti,1;       // first sample of phrase i3 for j = 2 to mi4                (T', T'i,j) = MDTW(T, Ti,j);5         T  =  (T' + T'i,j) / 2;//amplitude mean6         T  =  time_normalize(T, Li);7 end8 Ri = T;   // representative sample for phrase i9     end10   Output(R);Figure 3.
Training algorithm using DTW.
Thefunction call MDTW() returns the averageDTW distances between the correspondingsensors and dimensions of two data samples.
41SubjectAccuracy(%)Latency(s)# of Train-ing SamplesS01 100 3.086 2.0S02 96.67 1.403 2.4S03 96.67 1.524 3.1Table 1.
Phrase classification accuracy andlatency for all three participants.thus we calculated the average DTW distanceacross the corresponding sensors and dimensionsof two data samples.
DTW was adapted as fol-lows for training.The training algorithm generated a repre-sentative sample based on all available trainingsamples for each phrase.
Pseudo-code of thetraining algorithm is provided in Figure 3, for theconvenience of description.
For each phrase i,first, MDTW was applied to the first two trainingsamples, Ti,1 and Ti,2.
Sample T is the amplitude-mean of the warped samples T'i,1 and T'i,2 (time-series) (Line 5).
Next, T was time-normalized(stretched) to the average length of all trainingsamples for this phrase (Li), which was to reducethe effects of duration change caused by DTWwarping (Line 6).
The procedure continued untilthe last training sample Ti, mi (mi is the number oftraining samples for phrase i).
The final T wasthe representative sample for phrase i.The training procedure can be initiated bypressing a button on the GUI anytime during theuse of the SSI.TestingDuring testing, each participant silently articulat-ed the same list of phrases while the SSI recog-nized each phrase and played corresponding syn-thesized sounds.
DTW was used to compare thetest sample with the representative training sam-ple for each phrase (Ri, Figure 3).
The phrase thathad the shortest DTW distance to the test samplewas recognized.
The testing was triggered bypressing a button on the GUI.
If the phrase wasincorrectly predicted, the participant was allowedto add at most two additional training samplesfor that phrase.Figure 2 (right) demonstrates a participant isusing the SSI during the test.
After the partici-pant silently articulated ?Good afternoon?, theSSI displayed the phrase on the screen, andplayed the corresponding synthesized soundsimultaneously.Finally, one participant used the SSI for a bidi-rectional conversation with an investigator.
Sincethis prototype SSI has a closed-vocabularyrecognition component, the participant had tochoose the phrases that have been trained.
Thistask was intended to provide a demo of how theSSI is used for daily communication.
The scriptof the conversation is as below:Investigator: Hi DJ, How are you?Subject: I?m fine.
How are you doing?Investigator: I?m good.
Thanks.Subject: I use a silent speech interface to talk.Investigator: That?s cool.Subject: Do you understand me?Investigator: Oh, yes.Subject: That?s good.4 Results and DiscussionTable 1 lists the performance using the SSI forall three participants in the online, phrase-reading task.
The three subjects obtained aphrase recognition accuracy from 96.67% to100.00%, with a latency from 1.403 second to3.086 seconds, respectively.
The high accuracyand relatively short delay demonstrated the fea-sibility and potential of SSIs based on electro-magnetic articulograph.The order of the participants in the experimentwas S01, S02, and then S03.
After the experi-ment of S01, where all three dimensional data (x,y, and z) were used, we decided to use only y andz for S02 and S03 to reduce the latency.
As listedin Table 1, the latencies of S02 and S03 did sig-nificantly reduce, because less data was used foronline recognition.Surprisingly, phrase recognition without usingx dimension (left-right) data led to a decrease ofaccuracy and more training samples were re-quired; prior research suggests that tonguemovement in this dimension is not significantduring speech in healthy talkers (Westbury,1994).
This observation is supported by partici-pant S01, who had the highest accuracy andneeded fewer training samples for each phrase(column 3 in Table 1).
S02 and S03 used data ofonly y and z dimensions.
Of course, data frommore subjects are needed to confirm the potentialsignificance of the x dimension movement of thetongue to silent speech recognition accuracy.Data transfer between the Wave machine andthe SSI recognition component was done throughTCP/IP protocols and in real-time.
In the future,this design feature will allow the recognitioncomponent to run on a smart phone or any wear-able devices with an Internet connection (Cellu-42lar or Wi-Fi).
In this preliminary test, the indi-vidual delays caused by TCP/IP data transfer,online data preprocessing, and classificationwere not measured and thus unknown.
The delayinformation may be useful for our future devel-opment that the recognition component is de-ployed on a smart-phone.
A further study isneeded to obtain and analyze the delay infor-mation.The bidirectional dialogue by one of the par-ticipants demonstrated how the SSI can be usedin daily conversation.
To our best knowledge,this is the first conversational demo using a SSI.An informal survey to a few colleagues providedpositive feedback.
The conversation was smooth,although it is noticeably slower than a conversa-tion between two healthy talkers.
Importantly,the voice output quality (determined by the text-to-speech synthesizer) was natural, which strong-ly supports the major motivation of SSI research:to produce speech with natural voice quality thatcurrent treatments cannot provide.
A video demois available online (Wang, 2014).The participants in this experiment wereyoung and healthy.
It is, however, unknown ifthe recognition accuracy may decrease or not forusers after laryngectomy, although a single pa-tient study showed the accuracy may decrease15-20% compared to healthy talkers using anultrasound-based SSI (Denby et al., 2011).
Theo-retically, the tongue motion patterns in (silent)speech after the surgery should be no differencewith that of healthy talkers.
In practice, however,some patients after the surgery may be undertreatment for swallowing using radioactive de-vices, which may affect their tongue motion pat-terns in articulation.
Thus, the performance ofSSIs may vary and depend on the condition ofthe patients after laryngectomy.
A test of the SSIusing multiple participants after laryngectomy isneeded to understand the performance of SSIsfor those patients under different conditions.Although a demonstration of daily conversa-tion using the SSI is provided, SSI based on thenon-portable Wave system is currently not readyfor practical use.
Fortunately, more affordableand portable electromagnetic devices are beingdeveloped as are small handheld or wearable de-vices (Fagan et al., 2008).
Researchers are alsotesting the efficacy of permanently implantableand wireless sensors (Chen et al., 2012; Park etal., 2012).
In the future, those more portable, andwireless articulatory motion tracking devices,when they are ready, will be used to develop aportable SSI for practice use.In this experiment, a simple DTW algorithmwas used to compare the training and testingphrases, which is known to be slower than mostmachine learning classifiers.
Thus, in the future,the latency can be significantly reduced by usingfaster classifiers such as support vector machines(Wang et al., 2013c) or hidden Markov models(Heracleous and Hagita, 2011; King et al., 2007;Rudzicz et al., 2012; Uraga and Hain, 2006).Furthermore, in this proof-of-concept design,the vocabulary was limited to a small set ofphrases, because our design required the wholeexperiment (including training and testing) to bedone in about one hour.
Additional work is need-ed to test the feasibility of open-vocabularyrecognition, which will be much more usable forpeople after laryngectomy or with other severevoice impairments.5 Conclusion and Future WorkA preliminary, online test of a SSI based on elec-tromagnetic articulograph was conducted.
Theresults were encouraging revealing high phraserecognition accuracy and short playback laten-cies among three participants in a phrase-readingtask.
In addition, a proof-of-concept demo ofbidirectional conversation using the SSI wasprovided, which shows how the SSI can be usedfor daily communication.Future work includes: (1) testing the SSI withpatients after laryngectomy or with severe voiceimpairment, (2) integrating a phoneme- or word-level recognition (open-vocabulary) using fastermachine learning classifiers (e.g., support vectormachines or hidden Markov models), and (3)exploring speaker-independent silent speechrecognition algorithms by normalizing the articu-latory movement across speakers (e.g., due to theanatomical difference of their tongues).AcknowledgementsThis work was in part supported by the CallierExcellence in Education Fund, University ofTexas at Dallas, and grants awarded by the Na-tional Institutes of Health (R01 DC009890 andR01 DC013547).
We would like to thank Dr.Thomas F. Campbell, Dr. William F. Katz, Dr.Gregory S. Lee, Dr. Jennell C. Vick, LindseyMacy, Marcus Jones, Kristin J. Teplansky, Ve-dad ?Kelly?
Fazel, Loren Montgomery, andKameron Johnson for their support or assistance.We also thank the anonymous reviewers for theircomments and suggestions for improving thequality of this paper.
43ReferencesAmerican Cancer Society.
2013.
Cancer Facts andFigures 2013.
American Cancer Society, Atlanta,GA.
Retrieved on February 18, 2014.Bailey, B. J., Johnson, J. T., and Newlands, S. D.2006.
Head and Neck Surgery ?
Otolaryngology,Lippincot, Williams & Wilkins, Philadelphia, PA,USA, 4th Ed., 1779-1780.Berry, J.
2011.
Accuracy of the NDI wave speechresearch system, Journal of Speech, Language, andHearing Research, 54:1295-1301.Beukelman, D. R., and Gutmann, M. 1999.
GenericMessage List for AAC users with ALS.http://aac.unl.edu/ALS_Message_List1.htmBroekx, L., Dreesen, K., Gemmeke, J. F., and VanHamme, H. 2013.
Comparing and combining clas-sifiers for self-taught vocal interfaces, ACL/ISCAWorkshop on Speech and Language Processing forAssistive Technologies, 21-28, 2013.Chen, W.-H., Loke, W.-F., Thompson, G., and Jung,B.
2012.
A 0.5V, 440uW frequency synthesizer forimplantable medical devices, IEEE Journal of Sol-id-State Circuits, 47:1896-1907.Dam, E. B., Koch, M., and Lillholm, M. 1998.
Qua-ternions, interpolation and animation.
TechnicalReport DIKU-TR-98/5, University of Copenhagen.Denby, B., Cai, J., Roussel, P., Dreyfus, G., Crevier-Buchman, L., Pillot-Loiseau, C., Hueber, and T.,Chollet, G. 2011.
Tests of an interactive, phrase-book-style post-laryngectomy voice-replacementsystem, the 17th International Congress on Pho-netic Sciences, Hong Kong, China, 572-575.Denby, B., Schultz, T., Honda, K., Hueber, T., Gil-bert, J. M., and Brumberg, J. S. 2010.
Silent speechinterface, Speech Communication, 52:270-287.Doi, H., Nakamura, K., Toda, T., Saruwatari, H., Shi-kano, K. 2010.
Esophageal speech enhancementbased on statistical voice conversion with Gaussianmixture models, IEICE Transactions on Infor-mation and Systems, E93-D, 9:2472-2482.Fagan, M. J., Ell, S. R., Gilbert, J. M., Sarrazin, E.,and Chapman, P. M. 2008.
Development of a (si-lent) speech recognition system for patients follow-ing laryngectomy, Medical Engineering & Physics,30(4):419-425.Green, P. D., Khan, Z., Creer, S. M. and Cunningham,S.
P. 2011.
Reconstructing the voice of an individ-ual following Laryngectomy, Augmentative and Al-ternative Communication, 27(1):61-66.Green, J. R., Wang, J., and Wilson, D. L. 2013.SMASH: A tool for articulatory data processingand analysis, Proc.
Interspeech, 1331-35.Green, J. R. and Wang, Y.
2003.
Tongue-surfacemovement patterns during speech and swallowing,Journal of the Acoustical Society of America,113:2820-2833.Hofe, R., Ell, S. R., Fagan, M. J., Gilbert, J. M.,Green, P. D., Moore, R. K., and Rybchenko, S. I.2013.
Small-vocabulary speech recognition using asilent speech interface based on magnetic sensing,Speech Communication, 55(1):22-32.Hofe, R.,  Ell, S. R., Fagan, M. J., Gilbert, J. M.,Green, P. D., Moore, R. K., and Rybchenko, S. I.2011.
Speech Synthesis Parameter Generation forthe Assistive Silent Speech Interface MVOCA,Proc.
Interspeech, 3009-3012.Huang, X. D., Acero, A., Hon, H.-W., Ju, Y.-C., Liu,J., Meredith, S., and Plumpe, M. 1997.
Recent Im-provements on Microsoft?s Trainable Text-to-Speech System: Whistler, Proc.
IEEE Intl.
Conf.on Acoustics, Speech, and Signal Processing, 959-962.Hueber, T., Benaroya, E.-L., Chollet, G., Denby, B.,Dreyfus, G., Stone, M. 2010.
Development of a si-lent speech interface driven by ultrasound and opti-cal images of the tongue and lips, Speech Commu-nication, 52:288?300.Heaton, J. T., Robertson, M., and Griffin, C. 2011.Development of a wireless electromyographicallycontrolled electrolarynx voice prosthesis, Proc.
ofthe 33rd Annual Intl.
Conf.
of the IEEE Engineer-ing in Medicine & Biology Society, Boston, MA,5352-5355.Heracleous, P., and Hagita, N. 2011.
Automaticrecognition of speech without any audio infor-mation, Proc.
IEEE Intl.
Conf.
on Acoustics,Speech, and Signal Processing, 2392-2395.Jorgensen, C. and Dusan, S. 2010.
Speech interfacesbased upon surface electromyography, SpeechCommunication, 52:354?366, 2010.Katz, W., Bharadwaj, S., Rush, M., and Stettler, M.2006.
Influences of EMA receiver coils on speechproduction by normal and aphasic/apraxic talkers,Journal of Speech, Language, and Hearing Re-search, 49:645-659.Kent, R. D., Adams, S. G., and Tuner, G. S. 1996.Models of speech production, in Principles of Ex-perimental Phonetics, Ed., Lass, N. J., Mosby: StLouis, MO.King, S., Frankel, J. Livescu, K., McDermott, E.,Richmond, K., Wester, M. 2007.
Speech produc-tion knowledge in automatic speech recognition,Journal of the Acoustical Society of America,121(2):723-742.Kuipers, J.
B.
1999.
Quaternions and rotation Se-quences: a Primer with Applications to Orbits,Aerospace, and Virtual Reality, Princeton Univer-sity Press, Princeton, NJ.44Liu, H., and Ng, M. L. 2007.
Electrolarynx in voicerehabilitation, Auris Nasus Larynx, 34(3): 327-332.Livescu, K., ?etin, O., Hasegawa-Johnson, Mark,King, S., Bartels, C., Borges, N., Kantor, A., et al.(2007).
Articulatory feature-based methods foracoustic and audio-visual speech recognition:Summary from the 2006 JHU Summer Workshop.Proc.
IEEE Intl.
Conf.
on Acoustics, Speech, andSignal Processing, 621-624.Meier, U., Stiefelhagen, R., Yang, J., and Waibel, A.(2000).
Towards Unrestricted Lip Reading.
Inter-national Journal of Pattern Recognition and Artifi-cial Intelligence, 14(5): 571-585.Oviatt, S. L. 2003.
Multimodal interfaces, in Human?Computer Interaction Handbook: Fundamentals,Evolving Technologies and Emerging Applications,Eds.
Julie A. Jacko and Andrew Sears (Mahwah,NJ:Erlbaum): 286?304.Park, H., Kiani, M., Lee, H. M., Kim, J., Block, J.,Gosselin, B., and Ghovanloo, M. 2012.
A wirelessmagnetoresistive sensing system for an intraoraltongue-computer interface, IEEE Transactions onBiomedical Circuits and Systems, 6(6):571-585.Potamianos, G., Neti, C., Cravier, G., Garg, A. andSenior, A. W. 2003.
Recent advances in the auto-matic recognition of audio-visual speech, Proc.
ofIEEE, 91(9):1306-1326.Rudzicz, F., Hirst, G., Van Lieshout, P. 2012.
Vocaltract representation in the recognition of cerebralpalsied speech, Journal of Speech, Language, andHearing Research, 55(4): 1190-1207.Toda, T., Nakagiri, M., Shikano, K. 2012.
Statisticalvoice conversion techniques for body-conductedunvoiced speech enhancement, IEEE Transactionson Audio, Speech and Language Processing, 20(9):2505-2517.Uraga, E. and Hain, T. 2006.
Automatic speechrecognition experiments with articulatory data,Proc.
Inerspeech, 353-356.Wang, J., Samal, A., Green, J. R., and Carrell, T. D.2009.
Vowel recognition from articulatory positiontime-series data, Proc.
IEEE Intl.
Conf.
on SignalProcessing and Communication Systems, Omaha,NE, 1-6.Wang, J., Green, J. R., Samal, A., and Marx, D. B.2011.
Quantifying articulatory distinctiveness ofvowels, Proc.
Interspeech, Florence, Italy, 277-280.Wang, J., Samal, A., Green, J. R., and Rudzicz, F.2012a.
Sentence recognition from articulatorymovements for silent speech interfaces, Proc.
IEEEIntl.
Conf.
on Acoustics, Speech, and Signal Pro-cessing, 4985-4988.Wang, J., Samal, A., Green, J. R., and Rudzicz, F.2012b.
Whole-word recognition from articulatorymovements for silent speech interfaces, Proc.
In-terspeech, 1327-30.Wang, J., Green, J. R., Samal, A. and Yunusova, Y.2013a.
Articulatory distinctiveness of vowels andconsonants: A data-driven approach, Journal ofSpeech, Language, and Hearing Research, 56,1539-1551.Wang, J., Green, J. R., and Samal, A.
2013b.
Individ-ual articulator's contribution to phoneme produc-tion, Proc.
IEEE Intl.
Conf.
on Acoustics, Speech,and Signal Processing, Vancouver, Canada, 7795-89.Wang, J., Balasubramanian, A., Mojica de La Vega,L.,  Green, J. R., Samal, A., and Prabhakaran, B.2013c.
Word recognition from continuous articula-tory movement time-series data using symbolicrepresentations, ACL/ISCA Workshop on Speechand Language Processing for Assistive Technolo-gies, Grenoble, France, 119-127.Wang J.
2014.
DJ and his friend: A demo of conver-sation using a real-time silent speech interfacebased on electromagnetic articulograph.
[Video].Available: http://www.utdallas.edu/~wangjun/ssi-demo.htmlWeismer, G. and Bunton, K. (1999).
Influences ofpellet markers on speech production behavior:Acoustical and perceptual measures, Journal of theAcoustical Society of America, 105: 2882-2891.Westbury, J.
1994.
X-ray microbeam speech produc-tion database user?s handbook.
University of Wis-consin-Madison, Madison, Wisconsin.45
