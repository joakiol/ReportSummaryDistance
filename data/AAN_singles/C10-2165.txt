Coling 2010: Poster Volume, pages 1444?1452,Beijing, August 2010Machine Transliteration: Leveraging on Third LanguagesMin Zhang          Xiangyu Duan           Vladimir Pervouchine         Haizhou LiInstitute for Infocomm Research, A-STAR{mzhang, xduan, vpervouchine, hli}@i2r.a-star.edu.sgAbstractThis paper presents two pivot strategiesfor statistical machine transliteration,namely system-based pivot strategyand model-based pivot strategy.
Giventwo independent source-pivot and pi-vot-target name pair corpora, the mod-el-based strategy learns a direct source-target transliteration model while thesystem-based strategy learns a source-pivot model and a pivot-target model,respectively.
Experimental results onbenchmark data show that the system-based pivot strategy is effective in re-ducing the high resource requirementof training corpus for low-density lan-guage pairs while the model-based pi-vot strategy performs worse than thesystem-based one.1 IntroductionMany technical terms and proper names, suchas personal, location and organization names,are translated from one language into anotherlanguage with approximate phonetic equiva-lents.
This phonetic translation using computeris referred to as machine transliteration.
Withthe rapid growth of the Internet data and thedramatic changes in the user demographicsespecially among the non-English speakingparts of the world, machine transliteration playa crucial role in  most multilingual NLP, MTand CLIR applications (Hermjakob et al,2008; Mandl and Womser-Hacker, 2004).
Thisis because proper names account for the major-ity of OOV issues and translation lexicons(even derived from large parallel corpora)usually fail to provide good coverage over di-verse, dynamically increasing names acrosslanguages.Much research effort has been done to ad-dress the transliteration issue in the researchcommunity (Knight and Graehl, 1998; Wanand Verspoor, 1998; Kang and Choi, 2000;Meng et al, 2001; Al-Onaizan and Knight,2002; Gao et al, 2004; Klementiev and Roth,2006; Sproat, 2006; Zelenko and Aone, 2006;Li et al, 2004, 2009a, 2009b; Sherif and Kon-drak, 2007; Bertoldi et al, 2008; Goldwasserand Roth, 2008).
These previous work can becategorized into three classes, i.e., grapheme-based, phoneme-based and hybrid methods.Grapheme-based method (Li et al, 2004)treats transliteration as a direct orthographicmapping process and only uses orthography-related features while phoneme-based method(Knight and Graehl, 1998) treats transliterationas a phonetic mapping issue, converting sourcegrapheme to source phoneme followed by amapping from source phoneme to target pho-neme/grapheme.
Hybrid method in machinetransliteration refers to the combination of sev-eral different models or decoders via re-ranking their outputs.
The report of the firstmachine transliteration shared task (Li et al,2009a, 2009b) provides benchmarking data indiverse language pairs and systemically sum-marizes and compares different transliterationmethods and systems using the benchmarkingdata.Although promising results have been re-ported, one of major issues is that the state-of-the-art machine transliteration approaches relyheavily on significant source-target parallelname pair corpus to learn transliteration model.However, such corpora are not always availa-1444ble and the amounts of the current availablecorpora, even for language pairs with Englishinvolved, are far from enough for training, let-ting alone many low-density language pairs.Indeed, transliteration corpora for most lan-guage pairs without English involved are un-available and usually rather expensive to ma-nually construct.
However, to our knowledge,almost no previous work touches this issue.To address the above issue, this paperpresents two pivot language-based translitera-tion strategies for low-density language pairs.The first one is system-based strategy (Khapraet al, 2010), which learns a source-pivot mod-el from source-pivot data and a pivot-targetmodel from pivot-target data, respectively.
Indecoding, it first transliterates a source name toN-best pivot names and then transliterates eachpivot names to target names which are finallyre-ranked using the combined two individualmodel scores.
The second one is model-basedstrategy.
It learns a direct source-target transli-teration model from two independent1 source-pivot and pivot-target name pair corpora, andthen does direct source-target transliteration.We verify the proposed methods using thebenchmarking data released by theNEWS20092 (Li et al, 2009a, 2009b).
Expe-riential results show that without relying onany source-target parallel data the system-based pivot strategy performs quite well whilethe model-based strategy is less effective incapturing the phonetic equivalent information.The remainder of the paper is organized asfollows.
Section 2 introduces the baseline me-thod.
Section 3 discusses the two pivot lan-guage-based transliteration strategies.
Experi-mental results are reported at section 4.
Final-ly, we conclude the paper in section 5.2 The Transliteration ModelOur study is targeted to be language-independent so that it can be applied todifferent language pairs without any adaptationeffort.
To achieve this goal, we use jointsource-channel model (JSCM, also named as1 Here ?independent?
means the source-pivot andpivot-target data are not derived from the sameEnglish name source.2  http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/pages/sharedtask.htmln-gram transliteration model) (Li et la., 2004)under grapheme-based framework as ourtransliteration model due to its state-of-the-artperformance by only using orthographicalinformation (Li et al, 2009a).
In addition,unlike other feature-based methods, such asCRFs (Lafferty et al, 2001), MaxEnt (Bergeret al, 1996) or SVM (Vapnik, 1995), theJSCM model directly computes modelprobabilities using maximum likelihoodestimation (Dempster et al, 1977).
Thisproperty facilitates the implementation of themodel-based strategy.JSCM directly models how both source andtarget names can be generated simultaneously.Given a source name S and a target name T, itestimates the joint probability of S and T asfollows:where    and    is an aligned transliterationunit3 pair, and n is the n-gram order.In implementation, we compare differentunsupervised transliteration alignment me-thods, including Giza++ (Och and Ney, 2003),the JSCM-based EM algorithm (Li et al,2004), the edit distance-based EM algorithm(Pervouchine et al, 2009) and Oh et al?salignment tool (Oh et al, 2009).
Based on thealigned transliteration corpus, we simply learnthe transliteration model using maximum like-lihood estimation (Dempster et al, 1977) anddecode the transliteration resultusing stack decoder(Schwartz and Chow, 1990).3 Transliteration unit is language dependent.
It canbe a Chinese character, a sub-string of Englishwords, a Korean Hangual or a Japanese Kanji orseveral Japanese Katakanas.14453 Pivot Transliteration Strategies3.1 System-based StrategyThe system-based strategy is first proposed byKhapra et al (2010).
They worked on system-based strategy together with CRF and did ex-tensively empirical studies on In-dic/Slavic/Semetic languages and English.Given a source name S, a target name T andlet Z(S, ?)
be the n-best transliterations of S inone or more pivot language ?
4, the system-based transliteration strategy under JSCM canbe formalized as follows:In the above formula, we assume that there isonly one pivot language used in the derivationfrom the first line to the second line.
Under thepivot transliteration framework, we can furthersimplify the above formula by assuming thatis independent of    when given  .
The as-sumption holds because the parallel name cor-pus between S and T is not available under thepivot transliteration framework.
The n-besttransliterations in pivot language are expectedto be able to carry enough information of thesource name S for translating S to target nameT.
Then, we have:Obviously we can train the two JSCMs ofand        using the two parallel cor-pora of        and      , and train the lan-guage model      using the monolingual cor-pus of   .
Following the nature of JSCM, Eq.4 There can be multiple pivot languages used in thetwo strategies.
However, without loss of generality,we only use one pivot language to facilitate ourdiscussion.
It is very easy to extend one pivot lan-guage to multiple ones by considering all the pivottransliterations in all pivot languages.
(1) directly models how the source name S andpivot name   and how the pivot name   andthe target name   are generated simultaneous-ly.
Since   is considered twice in        and, the duplicated impact of   is removedby dividing the model by     .Given the model as described at Eq.
(1), thedecoder can be formulized as:If we consider multiple pivot languages, themodeling and decoding process are:3.2 Model-based StrategyRather than combining the transitive translite-ration results at system level, the model-basedstrategy aims to learn a direct model       bycombining the two individual models ofand       , which are learned fromthe two parallel corpora of       and      ,respectively.
Now let us use bigram as an ex-ample to illustrate how to learn the translitera-tion modelusing the model-based strategy.where,1446The same as the system-based strategy, wecan further simplify the above formula by as-suming that   is independent of    when given.
Indeed,                            cannotbe estimated directly from training corpus.Then we have:where                   ,and            can be directly learned fromtraining corpus.
for Eq (3) canalso be estimatedas follows.In summary, eq.
(1) formulizes the system-based strategy and eq.
(3), (4) and (5) formul-ize the model-based strategy, where we canfind that they share the same nature of generat-ing source, pivot and target names simulta-neously.
The difference is that the model-basedstrategy operates at fine-grained transliterationunit level.3.3 Comparison with Previous WorkAlmost all previous work on machine translite-ration focuses on direct transliteration or trans-literation system combination.
There is onlyone recent work (Khapra et al, 2010) touchingthis issue.
They work on system-based strategytogether with CRF.
Compared with their work,this paper gives more formal definitions andderivations of system-based strategy frommodeling and decoding viewpoints based onthe JSCM model.The pivot-based strategies at both systemand model levels have been explored in ma-chine translation.
Bertoldi et al (2008) studiestwo pivot approaches for phrase-based statis-tical machine translation.
One is at system lev-el and one is to re-construct source-target dataand alignments through pivot data.
Cohn andLapata (2007) explores how to utilize multilin-gual parallel data (rather than pivot data) toimprove translation performance.
Wu andWang (2007, 2009) extensively studies themodel-level pivot approach and also exploreshow to leverage on rule-based translation re-sults in pivot language to improve translationperformance.
Utiyama and Isahara (2007)compares different pivot approaches forphrase-based statistical machine translation.All of the previous work on machine transla-tion works on phrase-based statistical machinetranslation.
Therefore, their translation modelis to calculate phrase-based conditional proba-bilities at unigram level (        ) while ourtransliteration model is to calculate joint trans-literation unit-based conditional probabilitiesat bigram level (                   ).4 Experimental Results4.1 Experimental SettingsWe use the NEWS 2009 benchmark data asour experimental data (Li et al, 2009).
TheNEWS 2009 data includes 8 language pairs,where we select English to Chinese/Japanese/Korean data (E-C/J/K) and based on which wefurther construct Chinese to Japanese/Koreanand Japanese to Korean for our data.Language Pair Training Dev TestEnglish-Chinese 31,961  2896 2896English-Japanese 23,225 1492 1489English-Korean 4,785 987 989Chinese-Japanese 12,417 75 77Chinese-Korean 2,148 32 31Japanese-Korean 6,035 65 69Table 1.
Statistics on the data setTable 1 reports the statistics of all the expe-rimental data.
To have a more accurate evalua-tion, the test sets have been cleaned up to makesure that there is no overlapping between anytest set with any training set.
In addition, thethree E-C/J/K data are generated independentlyso that there is very small percentage of over-1447lapping between them.
This can ensures theevaluation of the pivot study fair and accurate.We compare different alignment algorithmson the DEV set.
Finally we use Pervouchine etal.
(2009)?s alignment algorithm for Chinese-English/Japanese/Korean and Oh et al(2009)?s alignment algorithm for English-Korean and Li et al (2004)?s alignment algo-rithm for English-Japanese and Japanese-Korean.
Given the aligned corpora, we directlylearn each individual JSCM model (i.e., n-gram transliteration model) using SRILM tool-kits (Stolcke, 2002).
We also use SRILM tool-kits to do decoding.
For the system-basedstrategy, we output top-20 pivot transliterationresults.For the evaluation matrix, we mainly usetop-1 accuracy (ACC) (Li et al, 2009a) tomeasure transliteration performance.
For refer-ence purpose, we also report the performanceusing all the other evaluation matrixes used inNEWS 2009 benchmarking (Li et al, 2009a),including F-score, MRR, MAP_ref, MAP_10and MAP_sys.
It is reported that F-score hasless correlation with other matrixes (Li et al,2009a).4.2 Experimental Results4.2.1 Results of Direct TransliterationTable 2 reports the performance of direct trans-literation.
The first three experiments (line 1-3)are part of the NEWS 2009 share tasks and theothers are our additional experiments for ourpivot studies.Comparison of the first three experimentalresults and the results reported at NEWS 2009shows that we achieve comparable perfor-mance with their best-reported systems at thesame conditions of using single system andorthographic features only.
This indicates thatour baseline represents the state-of-the-art per-formance.
In addition, we find that the back-transliteration (line 4-6) consistently performsworse than its corresponding forward-transliteration (line 1-3).
This observation isconsistent with what reported at previous work(Li et al, 2004; Zhang et al, 2004).
The mainreason is because English has much moretransliteration units than foreign C/J/K lan-guages.
This makes the transliteration fromEnglish to C/J/K a many-to-few mapping issueand back-transliteration a few-to-many map-ping issue.
Therefore back-transliteration hasmore ambiguities and thus is more difficult.Overall, the lower six experiments (line 7-12) shows worse performance than the uppersix experiments which has English involved.This is mainly due to the less available trainingdata for the language pairs without Englishinvolved.
This observation motivates our studyusing pivot language for machine translitera-tion.4.2.2 Results of System-based StrategyTable 3 reports three empirical studies of sys-tem-based strategies: Japanese to Chinesethrough English, Chinese to Japanese throughEnglish and Chinese to Korean through Eng-lish.
Considering the fact that those languagepairs with English involved have the mosttraining data, we select English as pivot lan-guage in the system-based study.
Table 3clearly shows that:?
The system-based pivot strategy is veryeffective, achieving significant perfor-mance improvement over the directtransliteration by 0.09, 0.07 and 0.03point of ACC in the three language pairs,respectively;?
Different from other pipeline methodol-ogies, the system-based pivot strategydoes not suffer heavily from the errorpropagation issue.
Its ACC is significant-ly better than the product of the ACCs ofthe two individual systems;?
The combination of pivot system and di-rect system slightly improves overallACC.We then conduct more experiments to figureout the reasons.
Our further statistics and anal-ysis show the following reasons for the aboveobservations:The pivot approach is able to use source-pivot and pivot-target data whose amount ismuch more than that of the available directsource-target data.?
The nature of transliteration is phonetictranslation.
Therefore a little bit variationin orthography may not hurt or even helpto improve transliteration performance insome cases as long as the orthographicalvariations keep the phonetic equivalent1448Language Pairs ACC F-Score MRR MAP_ref MAP_10 MAP_sysEnglish  Chinese 0.678867 0.871497 0.771563 0.678867 0.252382 0.252382English  Japanese 0.482203 0.831983 0.594235 0.471766 0.201510 0.201510English  Korean 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621Chinese  English 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787Japanese  English 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032Korean  English 0.088505 0.494205 0.109249 0.088759 0.034380 0.034380Chinese  Japanese 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948Japanese  Chinese 0.402597 0.714193 0.491595 0.402597 0.165581 0.165581Chinese  Korean 0.290323 0.571587 0.341129 0.290323 0.178652 0.178652Korean  Chinese 0.129032 0.280645 0.156042 0.129032 0.048163 0.048163Japanese  Korean 0.313433 0.678240 0.422862 0.313433 0.208310 0.208310Korean  Japanese 0.089286 0.321617 0.143948 0.091270 0.049992 0.049992Table 2.
Performance of direct transliterationsLanguage Pairs ACC   F-Score MRR MAP_ref MAP_10 MAP_sysJap Eng Chi (Pivot) 0.493506 0.750711 0.617440 0.493506 0.195151 0.195151Jap Eng Chi (Pivot)+ Jap  Chi (Direct)0.506494 0.753958 0.622851 0.506494 0.196017 0.196017Jap  Chi (Direct) 0.402597 0.714193 0.491595 0.402597 0.165581 0.165581Jap  Eng (Direct) 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032Eng  Chi (Direct) 0.678867 0.871497 0.771563 0.678867 0.252382 0.252382Chi Eng Jap (Pivot) 0.456140 0.777494 0.536591 0.414961 0.183222 0.183222Chi Eng Jap (Pivot)+ Chi  Jap (Direct)0.491228 0.801443 0.563297 0.450049 0.191742 0.191742Chi  Jap (Direct) 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948Chi  Eng (Direct) 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787Eng  Jap (Direct) 0.482203 0.831983 0.594235 0.471766 0.201510 0.201510Chi Eng Kor (Pivot) 0.322581 0.628146 0.432642 0.322581 0.175822 0.175822Chi Eng Kor (Pivot)+ Chi  Kor (Direct)0.331631 0.632967 0.439143 0.334222 0.176543 0.176543Chi  Kor (Direct) 0.290323 0.571587 0.341129 0.290323 0.178652 0.178652Chi  Eng (Direct) 0.395250 0.867702 0.518292 0.372403 0.222787 0.222787Eng  Kor (Direct) 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621Table 3.
Performance comparison of system-based strategy on Jap (Japanese) to Chi (Chinese) andChi (Chinese) to Jap (Japanese)/Kor (Korean) through Eng (English) as pivot language,where ??
(Pivot) + ?(Direct)?
means that for the same language pair we merge and re-rank the pivot transliteration and direct  transliteration resultsinformation.
Indeed, given one sourceEnglish names, there are usually morethan one correct transliteration referencesin Japanese/Korean.
This case also hap-pens to English to Chinese although notso heavy as in English to Japa-nese/Korean.1449Language Pairs ACC   F-Score MRR MAP_ref MAP_10 MAP_sysChi Eng Jap(Model-based Pivot: O)0.087719 0.538454 0.117446 0.085770 0.040645 0.040645Chi Eng Jap(Model-based Pivot: R)0.210526 0.746497 0.381210 0.201267 0.156106 0.156106Chi Eng Jap(System-based Pivot)0.456140 0.777494 0.536591 0.414961 0.183222 0.183222Chi  Jap  (Direct) 0.385965 0.769245 0.473851 0.348319 0.159948 0.159948Jap Chi Eng(Model-based Pivot)0.148504 0.724623 0.224253 0.141791 0.088966 0.088966Jap Chi Eng(System-based Pivot)0.201581 0.741627 0.266507 0.191926 0.098024 0.134730Jap  Eng (Direct) 0.334839 0.838212 0.450984 0.319277 0.168032 0.168032Eng Jap Kor(Model-based Pivot)0.206269 0.547732 0.300641 0.206269 0.145882 0.145882Eng Jap Kor(System-based Pivot)0.315470 0.629640 0.404769 0.315723 0.167587 0.225892Eng  Kor (Direct) 0.439838 0.722365 0.543039 0.439585 0.171621 0.171621Table 4.
Performance of Model-based Pivot Transliteration Strategy?
The N-best accuracy of machine transli-teration (of both to and from English) isvery high5.
It means that in most casesthe correct transliteration in pivot lan-guage can be found in the top-20 resultsand the other 19 results hold the similarpronunciations with the correct one,which can serve as alternative ?quasi-correct?
inputs to the second stage trans-literations and thus largely improve theoverall accuracy.The above analysis holds when using Eng-lish as pivot language.
Now let us see the caseof using non-English as pivot language.
Table4 reports two system-based strategies usingChinese and Japanese as pivot languages,5  Both our studies and previous work (Li et al,2004; Zhang et al, 2004) shows that the top-20accuracy from English to J/K is more than 0.85 andmore than 0.95 in English-Chinese case.
The top-20accuracy is a little worse from C/J/K to English, butstill more than 0.7.where we can find that the performance of twosystem-based strategies is worse than that ofthe direct transliterations.
The main reason isbecause that the direct transliteration utilizesmuch more training data than the pivot ap-proach.
However, the good thing is that thesystem-based pivot strategy using non-Englishas pivot language still does not suffer fromerror propagation issue.
Its ACC is significant-ly better than the product of the ACCs of thetwo individual systems.4.2.3 Results of Model-based StrategyTable 4 reports the performance of model-based strategy.
It clearly shows that the model-based strategy is less effective and performsmuch worse than both the system-based strate-gy and direct transliteration.While the model-based strategy works wellat phrase-based statistical machine translation(Wu and Wang, 2007, 2009), it does not workat machine transliteration.
To investigate thereasons, we conduct many additional experi-ments and do statistics on the model and1450aligned training data6.
From this in-depth anal-ysis, we find that main reason is due to the factthat the model-based strategy introduces toomany entries (ambiguities) to the final transli-teration model.
For example, in theJap Chi Eng experiment, the unigram andbigram entries of the transliteration model ob-tained by the model-based strategy are 45 and6.6 times larger than that of the transliterationmodel trained directly from parallel data.
Thisis not surprising.
Given a transliteration unit inpivot language, it can generate     source-to-target transliteration unit mappings (unigramentry of the model), where  is the number ofthe source units that can be mapped to the pi-vot unit and   is the number of the target unitsthat can be mapped from the pivot unit.Besides the ambiguities introduced by thelarge amount of entries in the model, anotherreason that leads to the worse performance ofmodel-based strategy is the size inconsistenceof transliteration unit of pivot language.
Asshown at Table 4, we conduct three experi-ments.
In the first experiment (Chi Eng Jap),we use English as pivot language.
We find thatthe English transliteration unit size inChi Eng model is much larger than that inEng Jap model.
This is because from phoneticviewpoint, in Chi Eng model, the English unitis at syllable level (corresponding one Chinesecharacter) while in Eng Jap model, the Englishunit is at sub-syllable level (consonant or vowelor syllable, corresponding one Japanese Kata-kana).
This is the reason why we conduct twomodel-based experiments for Chi Eng Jap.One is based on the original alignments (Mod-el-based Pivot: O) and one is based on the re-constructed alignments 7  (Model-based Pivot:R).
Experimental results clearly show that thereconstruction improves performance signifi-cantly.
In the second and third experiments(Jap Chi Eng, Eng Jap Kor), we use Chi-nese and Japanese as pivot languages.
Thereforewe do not need to re-construct transliteration6 However, due to space limitation, we are not al-lowed to report the details of those experiments.7Based on the English transliteration units obtainedfrom Chi Eng, we reconstruct the English transli-teration units and alignments in Eng Jap by merg-ing the adjacent units of both English and Japaneseto syllable level.units and alignments.
However, the perfor-mance is still very poor.
This is due to the firstreason of the large amount of ambiguities.The above two reasons (ambiguities andtransliteration unit inconsistence) are mixedtogether, leading to the worse performance ofthe model-based strategy.
We believe that thefundamental reason is because the pivot transli-teration unit is too small to be able to conveyenough phonetic information of source lan-guage to target language and thus generates toomany alignments and ambiguities.5 ConclusionsA big challenge to statistical-based machinetransliteration is the lack of the training data,esp.
to those language pairs without Englishinvolved.
To address this issue, inspired by theresearch in the SMT research community, westudy two pivot transliteration methods.
One isat system level while another one is at modellevel.
We conduct extensive experiments usingNEW 2009 benchmarking data.
Experimentalresults show that system-based method is veryeffective in capturing the phonetic informationof source language.
It not only avoids success-fully the error propagation issue, but also fur-ther boosts the transliteration performance bygenerating more alternative pivot results as theinputs of the second stage.
In contrast, themodel-based method in its current form fails toconvey enough phonetic information fromsource language to target language.For the future work, we plan to study how toimprove the model-based strategy by pruningout the so-called ?bad?
transliteration unitpairs and re-sampling the so-called ?good?
unitpairs for better model parameters.
In addition,we also would like to explore other pivot-based transliteration methods, such as con-structing source-target training data throughpivot languages.ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Trans-lating named entities using monolingual and bi-lingual resources.
ACL-02Adam L. Berger, Stephen A. Della Pietra andVincent J. Della Pietra.
1996.
A Maximum En-tropy Approach to Natural Language Processing.Computational Linguistics.
22(1):39?711451N.
Bertoldi, M. Barbaiant, M. Federico and R. Cat-toni.
2008.
Phrase-based Statistical MachineTranslation with Pivot Languages.
IWSLT-08Trevor Cohn and Mirella Lapata.
2007.
MachineTranslation by Triangulation: Making EffectiveUse of Multi-Parallel Corpora.
ACL-07Andrew Finch and Eiichiro Sumita.
2008.
Phrase-based machine transliteration.
IJCNLP-08Wei Gao, Kam-Fai Wong and Wai Lam.
2004.Phoneme-based Transliteration of ForeignNames for OOV Problems.
IJCLNP-04Dan Goldwasser and Dan Roth.
2008.
Translitera-tion as constrained optimization.
EMNLP-08A.P.
Dempster, N.M. Laird, D.B.Rubin.1977.
Max-imum likelihood from incomplete data via theEM algorithm, J. Roy.
Stat.
Soc., Ser.
B. Vol.
39Ulf Hermjakob, K. Knight and Hal Daum e?.
2008.Name translation in statistical machine transla-tion: Learning when to transliterate.
ACL-08John Lafferty, Fernando Pereira, Andrew McCal-lum.
2001.
Conditional random fields: Probabil-istic models for segmenting and labeling se-quence data.
ICML-01B.J.
Kang and Key-Sun Choi.
2000.
AutomaticTransliteration and Back-transliteration by De-cision Tree Learning.
LREC-00Mitesh Khapra, Kumaran A and Pushpak Bhatta-charyya.
2010.
Everybody loves a rich cousin:An empirical study of transliteration throughbridge languages.
NAACL-HLT-10Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and dis-covery from multilingual comparable corpora.COLING-ACL-06K.
Knight and J. Graehl.
1998.
Machine Translite-ration, Computational Linguistics, Vol 24, No.
4P.
Koehn, F. J. Och and D. Marcu.
2003.
Statisticalphrase-based translation.
HLT-NAACL-03J.
Lafferty, A. McCallum and F. Pereira.
2001.Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.ICML-01Haizhou Li, A Kumaran, Vladimir Pervouchine andMin Zhang.
2009a.
Report of NEWS 2009 Ma-chine Transliteration Shared Task.
IJCNLP-ACL-09 Workshop: NEWS-09Haizhou Li, A Kumaran, Min Zhang and VladimirPervouchine.
2009b.
Whitepaper of NEWS 2009Machine Transliteration Shared Task.
IJCNLP-ACL-09 Workshop: NEWS-09Haizhou Li, Ming Zhang and Jian Su.
2004.
A JointSource-Channel Model for Machine Translitera-tion.
ACL-04Thomas Mandl and Christa Womser-Hacker.
2004.How do Named Entities Contribute to RetrievalEffectiveness?
CLEF-04Helen M. Meng, Wai-Kit Lo, Berlin Chen and Ka-ren Tang.
2001.
Generate Phonetic Cognates toHandle Name Entities in English-Chinese cross-language spoken document retrieval.
ASRU-01Jong-Hoon Oh, Kiyotaka Uchimoto, and k. Torisa-wa.
2009.
Machine Transliteration with Target-Language Grapheme and Phoneme: Multi-Engine Transliteration Approach.
NEWS 2009Franz Josef Och and Hermann Ney.
2003.
A Syste-matic Comparison of Various Statistical Align-ment Models.
Computational Linguistics 29(1)V. Pervouchine, H. Li and B. Lin.
2009.
Translite-ration Alignment.
ACL-IJCNLP-09R.
Schwartz and Y. L. Chow.
1990.
The N-bestalgorithm: An efficient and exact procedure forfinding the N most likely sentence hypothesis,ICASSP-90Tarek Sherif and Grzegorz Kondrak.
2007.
Sub-string-based transliteration.
ACL-07Richard Sproat, Tao Tao and ChengXiang Zhai.2006.
Named entity transliteration with compa-rable corpora.
COLING-ACL-06Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
ICSLP-02Masao Utiyama and Hitoshi Isahara.
2007.
A Com-parison of Pivot Methods for Phrase-based Sta-tistical Machine Translation.
NAACL-HLT-07Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
SpringerStephen Wan and Cornelia Maria Verspoor.
1998.Automatic English-Chinese name transliterationfor development of multilingual resources.
COL-ING-ACL-98Hua Wu and Haifeng Wang.
2007.
Pivot LanguageApproach for Phrase-based Statistical MachineTranslation.
ACL-07Hua Wu and Haifeng Wang.
2009.
Revisiting PivotLanguage Approach for Machine Translation.ACL-09Dmitry Zelenko and Chinatsu Aone.
2006.
Discri-minative methods for transliteration.
EMNLP-06Min Zhang, Haizhou Li and Jian Su.
2004.
DirectOrthographical Mapping for machine translite-ration.
COLING-041452
