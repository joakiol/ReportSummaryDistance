Exploiting Diverse Knowledge Sources via Maximum Entropyin Named Entity RecognitionAndrew Borthwick, John Sterling, Eugene Agichtein and Ralph GrishmanComputer  Science Depar tmentNew York  Un ivers i ty715 Broadway,  7th f loorNew York ,  NY  10003, USA{bor thwic ,s ter l ing ,ag ichtn ,gr i shman } @cs .nyu .eduAbst rac tThis paper describes a novel statistical named-entity (i.e.
"proper name") recognition system builtaround a maximum entity framework.
By work-ing v,ithin the framework of maximum entropy the-ory and utilizing a flexible object-based architecture,the system is able to make use of an extraordinar-ily diverse range of knowledge sources in making itstagging decisions.
These knowledge sources includecapitalization features, lexical features, features in-dicating the current section of text (i.e.
headline ormain body), and dictionaries of single or multi-wordterms.
The purely statistical system contains nohand-generated patterns and achieves a result com-parable with the best statistical systems.
However,when combined with other handcoded systems, thesystem achieves cores that exceed the highest com-parable scores thus-far published.1 INTRODUCTIONNamed entity recognition is one of the simplest ofthe common message understanding tasks.
The ob-jective is to identify and categorize all members ofcertain categories of "proper names" from a givencorpus.
The specific test bed which will be the sub-ject of this paper is that of the Seventh Message Un-derstanding Conference (MUC-7), in which the taskwas to identify "names" falling into one of seven cat-egories: person, organization, location, date, time,percentage, and monetary amount.This paper describes a new system called "Max-imum Entropy Named Entity" or "MENE" (pro-nounced "meanie").
By working within the frame-work of maximum entropy theory and utilizing aflexible object-based architecture, the system is ableto make use of an extraordinarily diverse range ofknowledge sources in making its tagging decision.These knowledge sources include capitalization fea-tures, lexical features, and features indicating thecurrent section of text.
It makes use of a broad arrayof dictionaries of useful single or multi-word termssuch as first names, company names, and corpo-rate suffixes, and automatically handles cases wherewords are in more than one dictionary.
Our dictio-152naries required no manual editing and were eitherdownloaded from the web or were simply "obvious"lists entered by hand.This system, built from off-the-shelf knowledgesources, contained no hand-generated patterns andachieved a result which is comparable with that ofthe best statistical systems.
Further experimentsshowed that when combined with handcoded sys-tems from NYU, the University of Manitoba, andIsoQuest, Inc., MENE was able to generate scoreswhich exceeded the highest scores thus-far eportedby any system on a MUC evaluation.Given appropriate training data, we believe thatthis system is highly portable to other domains andlanguages and have already achieved good resultson upper-case English.
We also feel that there areplenty of avenues to explore in enhancing the sys-tem's performance on English-language newspapertext.2" MAXIMUM ENTROPYGiven a tokenization of a test corpus and a setof n (for MUC-7, n = 7) tags which define thename categories of the task at hand~ the problemof named entity recognition can be reduced to theproblem of assigning one of 4n + l tags to eachtoken.
For any particular tag x from the set ofn tags, we could be in one of 4 states: x_start,x_continue, x_end, and x_unique.
In addition, a to-ken could be tagged as "other" to indicate that it isnot part of a named entity.
For instance, we wouldtag the phrase \[Jerry Lee Lewis flew to Paris\] as \[per-son_start, person_continue, person_end, other, other,location_unique I.
This approach is essentially thesame as (Sekine et al, 1998).The 29 tags of MUC-7 form the space of "fu-tures" for a maximum entropy formulation of ourN.E.
problem.
A maximum entropy solution to this,or any other similar problem allows the computa-tion of p(f\[h) for any f from the space of possiblefutures, F, for every h from the space of possiblehistories, H. A "history" in maximum entropy is allof the conditioning data which enables you to makea decision among the space of futures.
In the namedentity problem, we Could reformulate this in termsof finding the probability of f associated with thetoken at index ~ in the test corpus as:/ " In f ?
rmat i ?n  derivable f romtthe)  p(f\]ht) p \ \ ] l test  corpus relative to tokenThe computation of p(flh) in M.E.
is dependenton a set of "features" which, hopefully, are helpfulin making a prediction about the future.
Like mostcurrent M.E.
modeling efforts in computational lin-guistics we restrict ourselves to features which arebinary functions of the history and future.
For in-stance, one of our features isg(h,f) = 1 : capitalized(h) = trueand f = location_start0 : else(1)Here "current-token-capitalized(h)" is a binary func-tion which returns true if the "current oken" of thehistory h (the token whose tag we are trying to de-termine) has an initial capitalized letter.Given a set of features and some training data,the maximum entropy estimation process producesa model in which every feature gi has associatedwith it a parameter ai.
This allows us to computethe conditional probability as follows (Berger et al,1996):P(flh) = ~i?~ '(h'I) (2)Z~(h)Z~(h) = ~I~I~ '(h'~) (a)ff iThe maximum entropy estimation technique guar-antees that for every feature gi, the expected valueof gi according to the M.E.
model will equal the em-pirical expectation of gi in the training corpus.
Inother words:Z t5(h' f).gi(h, f) = Z 15(h)'Z PME(flh)'gi(h, f)h,f h \[(4)Here P is an empirical probability and PME is theprobability assigned by the M.E.
model.More complete discussions of M.E.
as applied tocomputational linguistics, including a descriptionof the M.E.
estimation procedure can be found in(Berger et al, 1996) and (Della Pietra et al, 1995).The following are some additional references whichare useful as introductions and examples of applica-tions: (Ramaparkhi, 1997b) (Ristad, 1.998) (Jaynes,1996).
As many authors have remarked, though, themost useful thing about maximum entropy modelingis that it allows the modeler to concentrate on find-ing the features that characterize the problem whileletting the M.E.
estimation routine worry about as-signing the relative weights to the features.3 SYSTEM ARCHITECTURE:H is tor ies  and  FuturesMENE consists of a set of C++ and Perl moduleswhich forms a wrapper around a publicly availableM.E.
toolkit (Ristad, 1998) which computes the val-ues of the a parameters of equation 2 from a pairof training files created by MENE.
MENE's flex-ibility is due to its object-based treatment of thethree essential cOmponents of a maximum entropysystem: histories, futures, and features (Borthwicket al, 1997).History objects in MENE act as containers for alist of "history views".
The history view classes eachrepresent a different type of information about thehistory object.
When the features attempt o de-termine whether or not they fire on a given history,they request an appropriate history view object fromthe history object and then query the history viewobject to determine whether their firing conditionsare satisfied.
Note that these history views generallyhold information about a limited window around thecurrent oken.
If the current oken is denoted as w0,then our model only holds information about tokensw-1 .
.
.w l  for all history views except the lexicaiones.
For these views, the window is w--2.., w.z.Future objects, on the other hand, are trivial inthat their only piece of data is an integer indicatingwhich of the 29 members of the future space theyrepresent.4 FEATURESFeatures are implemented as binary valued functionswhich query the history and future objects to deter-mine whether or not they "fire".
In the followingsections, we will look at each of MENE's featureclasses in turn.4.1 B inary  FeaturesWhile all of MENE's features have binary-valuedoutput, the "binary" features are features whose as-sociated history-view can be considered to be eitheron or off for a given token.
Examples are "the tokenbegins with a capitalized letter" or "the token is afour-digit number".
Equation 1 gives an example ofa binary feature.
The 11 binary history-views usedby MENE's binary features are very similar to thoseused in BBN's Nymble/ldentifinder system (Bikel etal., 1997) with two exceptions:?
Nymble used a feature for "significant" (i.e.non-sentence-beginning) capitalization.
Wedidn't include this, believing that MENE couldmake these judgments from the surrounding lex-icai content.?
Nymble's features were non-overlapping.
I.e.the all-cap feature took precedence over theinitial-cap feature.
Given two features, a and153b, when the (history, filture) space on which fea-ture b activates must be a subset of the spacefor feature a, it can be shown that the M.E.model will yield the same results whether a andb are included as features or if (a - b) arid bare features.
Consequently, MENE allows allfeatures to fire in overlapping cases.
For in-stance, in MENE the initial cap features ac-tivate on the histories "Clinton", "IBM", and"ValuJet" while in Nymble the feature wouldonly be active on "Clinton" because the "All-Cap" feature would take precedence on "IBM"and an "Initial-and-internal-cap" feature wouldtake precedence on "ValuJet".4.2 Lexical FeaturesTo create a lexical history view, the tokens atw-2 ... w2 are compared with a vocabulary and theirvocabulary indices are recorded.
For a given train-ing corpus, we define the vocabulary to be all tokenswith a count of three or more.
Words not foundin the vocabulary are assigned a distinguished "Un-known" index.
Lexical feature example:g(h,f) = 1 : View(token_l(h)) = "Mr"and f = person_unique0 : else?
Correctly predicts: Mr ,JonesA more subtle feature picked up by MENE: pre-ceding word is "to" and future is "location_unique".Given the domain of the MUC-7 training data (avi-ation disasters), "to" is a weak indicator, but a realone.
This is an example of a feature which MENEcan make use of but which the constructor of a hand-coded system would probably regard as too risky toincorporate.
This feature, in conjunction with otherweak features, can allow MENE to pick up namesthat other systems might miss.As discussed later, these features are automati-cally acquired and the system can attain a very highlevel of performance using these features alone.
Thisis encouraging since these lexical features are notdependent on any external knowledge source or lin-guistic intuition and thus are completely portable tonew domains.4.3 Sect ion FeaturesThe New York Times articles which constituted theMUC-7 test and training corpora were composed ofsix distinct sections including "Date", "Preamble",and "Text".
Section features activate according towhich of these sections the current token is in.
Ex-ample feature:if Section-View(tokeno(h)) }g(h,f) = 1 : = "Preamble" and f =person_unique0 : elseActivation example: CL INTON WARNS HUS-SE IN  ABOUT IRAQI DEFIANCE.
Note that, as-suming that this headline is in the preamble, theabove feature will fire on all of these words.
Ofcourse, this feature's prediction will only be correcton "CLINTON" and "HUSSEIN".Section features establish the background prob-ability of the occurrence of the different futures.For instance, in NYU's evaluation system, thevalue assigned to the feature which predicts "other"given a current section of "main body of text" is7.9 times stronger than the feature which predicts"person_unique" in the same section.
Thus the sys-tem predicts "other" by default.
On the other hand,in the preamble (which contains headline, author,etc.
information), the feature predicting "other" ismuch weaker in most cases.
It is only about 2.6times as strong as "organization_start" and "organi-zation_end", for instance.4.4 D ic t ionary  FeaturesMulti-word dictionaries are a key element of MENE.Each entry in a MENE dictionary consists of aterm which is one or more tokens long.
Dictionar-ies can be case-sensitive or not on a dictionary-by-dictionary basis.
A pre-processing step summarizesthe information in the dictionary on a token-by-token basis by assigning to every token one of the fol-lowing five tags for each dictionary: start, continue,end, unique, other.
I.e.
if "British Airways" wasin our dictionary, a dictionary feature would see thephrase "on British Airways Flight 962" as "other,start, end, other, other".
Table 1 lists the dictionar-ies.used by MENE in the MUC-7 evaluation.
Belowis an example of a dictionary feature:{ if First-Name-Dictionary- }View( tokeno(h ) ) =g(h,f) = 1 : "unique" and f = per-son_start0 : else?
Example: R ichard  M. Nixon-assuming that"Richard" is in the first name dictionary.Note that, similar to the case of overlapping bi-nary features, we don't have to worry about wordsappearing in the dictionary which are commonlyused in another sense.
I.e.
we can leave dangerous-looking names like "April" in the first-name dictio-nary because whenever the first-name feature fireson "April", the lexical and date-dictionary featuresfor "April" will also fire and, assuming that the useof April as "date" exceeded the use of April as per-son.start or person_unique, we can expect that thelexical feature will have a high enough c~ value tooutweigh the first-name-dictionary feature.
This wasconfirmed in our test runs: no instance of "April"was tagged as a name, including one case, "The154Dictionary:first namescorporate names"'corporate nameswithout suffixescolleges and universitiesNumber Data Sourceof Entries1245 www.babyfikme.com10300 www.marketguide.com103001225"corporate names" processedthrough a Perl scripthttp://www.utexas.edu/world/univ/alpha/Corporate Suffixes 244 Tipster resourceDates and times 51 Hand Entered2-ietter State Abbreviations 50 www.usps.govWorld'Regions 14 www.yahoo.comExamplesJohn, 3ulie, AprilExxon CorporationMotorola, Inc.Exxon; MotorolaNew York University;Oberlin CollegeInc.
; Incorporated; AGWednesday ' April, EST, a.m.NY, CAAfrica, Central America,Caribbean, Pacific RimTable 1: Dictionaries used in MENEdeath of Ron Brown in April in a similar plane crash.
.
. "
which could be thought of as somewhat trickybecause the month was not followed by a specificdate.
Note that the system isn't foolproof: if a "dan-gerous" dictionary word appeared in only one dictio-nary and did not appear often enough in the trainingcorpus to be included in the vocabulary, but did ap-pear in the test corpus, we would probably mistagit.4.5 Externa l  Sys tem FeaturesFor NYU's official entry in the MUC-7 evaluation,MENE took in the output of an enhanced version ofthe more traditional, hand-coded "Proteus" named-entity tagger which we entered in MUC-6(Grishman,1995).
In addition, subsequent to the evaluation, theUniversity of Manitoba (Lin, 1998) and IsoQuest,Inc.
(Krupka and Hausman, 1998) shared with usthe outputs of their systems on our training corporaas well as on various test corpora.
The output sentto us was the standard MUC-7 output, so our col-laborators didn't have to do any special processingfor us.
These systems were incorporated into MENEas simply three more history views by the following2 step process:1.
Each system's output is tokenized by MENE'stokenizer and cross-system tokenization dis-crepancies are resolved.2.
The tag assigned to each token by each sys-tem is noted.
This tag will be one of the 29tags mentioned above (i.e.
person-start, loca-tion.continue, etc.
)The result of all this is that the "futures" pro-duced by the three external systems become three"external system histories" for MENE.
Here is anexample feature:if Proteus-System- /~/iew( tokeno(h ) ) =g(h,f)  = 1 : "person_start' and f= person.start0 : else* Example: R ichard  M. Nixon, in a case whereProteus has correctly tagged "Richard".It is important o note that MENE has featureswhich predict a different future than the future pre-dicted by the external system.
This can be seen asthe process by which MENE learns the errors whichthe external system is likely to make.
An exampleof i:his is that on the evaluation system the featurewhich predicted person_unique given a tag of per-son_unique by Proteus had only a 76% higher weightthan the feature which predicted person-start givenperson_unique.
In other words, Proteus had a ten-dency to chop off multi-word names at the first word.MENE learned this and made it easy to overrideProteus in this way.
In fact, an analysis of the dif-ferences between the Proteus output and the MENE+ Proteus output turned up a significant number ofinstances in which MENE extended or contractedname boundaries in this way.
Given proper trainingdata, MENE can pinpoint and selectively correct heweaknesses of a handcoded system.5 Compound FeaturesMENE currently has no direct ability to learn com-pound features or "patterns"-the "history" side of alexical feature activates based on only a single word,for instance.
A sort of pattern-like ability comesinto the system from multiple features firing at once.I.e.
to predict that "York" in the name "New York"is the end of a location, we will have two featuresfiring: one predicts location_end when token-i is155"new".
The other predicts location_end when tokenois "york".Nevertheless, it is possible that compound fi.,a-tures would behave differently from two simultane-ously firing "atomic" features.
We integrated t:hisinto the model in an ad hoc manner for the exter-nal system features, where we constructed featureswhich essentially query the external system historyand the section history simultaneously to determ!inewhether they fire.
I.e.
a particular feature mightfire if Proteus predicts person_start, he current sec-tion is "main body of text", and the future is "per-son_start".
This allows MENE to assign a lower ato a Proteus prediction in the preamble vs. a predic-tion in the main body of text.
Proteus, like manyhand-coded systems, is more accurate in the mainbody of the text than in headline-type material.
Wefound that this compound feature gave the systemslightly higher performance than we got when wejust used section features and external system fea-tures separately.It seems reasonable that adding an ability to han-dle fully general compound features (i.e.
feature Afires if features B and C both fire) would improvesystem performance based on this limited experi-ment.
In addition to allowing us to predict futuresbased on multi-word patterns, it would also let ususe other promising combinations of features suchas distinguishing between capitalization in a head-line vs. in the main body of the text.
Unfortunately,this experiment will have to wait until we deploy amore sophisticated method of feature selection, asdiscussed in the next section.6 FEATURE SELECT IONFeatures are chosen by a very simple method.
Allpossible features from the classes we want includedin our model are put into a "feature pool".
For in-stance, if we want lexical features in our model whichactivate on a range of token_~.., token.x, our vocab-ulary has a size of V, and we have 29 futures, wewill add (5.
(V + 1).
29) lexical features to the pool.The V + 1 term comes from the fact that we includeall words in the vocabulary plus the unknown word.From this pool, we then select all features which fireat least three times on the training corpus.
Notethat this algorithm is entirely free of human inter-vention.
Once the modeler has selected the classesof features, MENE will both select all the relevantfeatures and train the features to have the properweightings.We deviate from this basic algorithm in threeways:1.
We exclude features which activate on some sortof "default" value of a history view.
Manyhistory views have some sort of default valuewhich they display for the vast majority of to-kens.
For instance, a first-name-dictionary his-tory view would say that the current token isnot a name in over 99% of the cases.
Ratherthan adding features which activate both whenthe token in question is and when it is not afirst name, we only include features which acti-vate when the token is a first name.
A featurewhich activated when a token was not a firstname, while theoretically not harmful, wouldhave practical disadvantages.
First of all, thefeature would probably be redundant, becauseif the frequency of a future given a first-name-dictionary hit is constrained (by equation 4),then the future frequency given a non-hit is alsoimplicitly constrained.
Secondly, since this fea-ture would fire on nearly every token, it wouldslow down run-time performance.
Finally, whilemaximum entropy models are designed to han-dle feature overlap, a very high degree of over-lap requires more iterations of the maximum en-tropy estimation routine and can lead to numer-ical difficulties (Ristad, 1998).2.
Features which predict the future "other" haveto fire six times to be included in the modelrather than three.
Experiments howed thatdoing this had no impact on performance andreduced the size of the model by about 20%.3.
As another way of reducing the model size,lexical features which activate on token_2 andtoken2 are excluded if they predict "other".Like the previous heuristic, this is based on theidea that features predicting named entities aremore useful than features predicting the default.Note that this method of feature selection wouldprobably break down if we tried to incorporate gen-eral compound features into our model as describedin the previous section.
The model currently hasabout 24,000 features when trained on 350 articlesof text.
If we even considered all pairs of features aspotential compound features, the O(n 2) compoundfeatures which we could build from our atomic fea-tures would undoubtedly ield an unacceptable slow-down in the model's performance.
Clearly a moresophisticated feature selection routine such as theones in (Berger et al, 1996), or (Berger and Printz,1998) would be required in this case.7 DECODING and V ITERBISEARCHAfter having trained the features of an M.E.
modeland assigned the proper weight (a values) to eachof the features, decoding (i.e.
"marking up") a newpiece of text is a fairly simple process:1.
Tokenize the text.1562.
Compute each of the history views by lookingup words in the dictionary, checking the outputof the external systems, checking whether wordsare capitalized or not, etc.3.
For each article of the text(a) For each token of the text, check each fea-ture to see whether it fires, and combine thea values of the firing features according toequation 2.
This will give us a conditionalprobability for each of the 29 futures foreach token in the article.
(b) Run a Viterbi search to find the highestprobability legal path through the latticeof conditional probabilities.The Viterbi search is necessary because simplytaking the highest-probability future assigned toeach token would result in incompatible assign-ments.
For instance, an assignment of \[person_start,location_end\] to two consecutive tokens would be in-valid.
The Viterbi search finds the highest probabil-ity path in which there are no two tokens in whichthe second one cannot follow the first, as definedby a table of all such invalid transitions (a similarapproach to (Sekine et al, 1998)).8 RESULTSMENE's maximum entropy training algorithm givesit reasonable performance with moderate-sizedtraining corpora or few information sources, whileallowing it to really shine when more training dataand information sources are added.
Table 2 showsMENE's performance on the MUC-7 "dry run" cor-pus, which consisted of 25 articles mostly on thetopic of aviation disasters.
All systems hown weretrained on 350 articles on the same domain (thistraining corpus consisted of about 270,000 words,which our system turned into 321,000 tokens).Note the smooth progression of the scores asmore data is added to the system.
Also note that,when combined under MENE, the three weakest sys-tems, MENE, Proteus, and Manitoba outperformthe strongest single system, IsoQuest's.
Finally, thetop score of 97.12 from combining all three systemsis a very strong result.
On a different set of data,the MUC-7 formal run data, the accuracy of the twohuman taggers who were preparing the answer keywas tested and it was discovered that one of themhad an F-Measure of 96.95 and the other of 97.60(Marsh and Perzanowski, 1998).
Although we don'thave human performance measures on the dry runtest set, it seems that we have attained a result whichis at least competitive with that of a human.We also did a series of runs to examine how thesystems performed with different amounts of train-ing data.
These experiments are summarized in ta-ble 3.
Note the 97.38 all-systems result which weSystemsMENE (ME)Manitoba (Ma)Proteus (Pr)MENE +lsoQuestMENE +Proteus92.20ME + Pr + IQME + Pr + Ma96.2793.3292.2496.5596 8998 9494 9295 9098 9595.61 97MENE +Manitoba 95.49 97ME + Ma + IQ 96.81 9896.78 9896.48 97ME + Pr + Ma+ IQ 97.12 98949495969596Table 2: Combined systems on unseen data from theMUC-7 dry-run test setachieved by adding 75 articles from the formal-runtest corpus to the basic 350-article training data.
Inaddition to being an outstanding performance fig-ure, this number shows MENE's responsiveness togood training material.
A few other conclusions canbe drawn from this data.
First of all, MENE needsat least 20 articles of tagged training data to get ac-ceptable performance on its own.
Secondly, there isa minimum amount of training data which is neededfor MENE to improve an external system.
For Pro-teus and the Manitoba system, this number seems tobe'about 80 articles, because they show a degrada-tion of performance at40.
Since the IsoQuest systemwas stronger to start with, MENE required 150 arti-cles to show an improvement.
Note the anomaly incomparing the 250 and 350 article columns.
Proteusshows only a very small gain and IsoQuest shows adeterioration.
These last 100 articles added to thesystem were tagged by us at NYU, and we wouldhumbly guess that we tagged them less carefullythan the rest of the data which was tagged by BBNand Science Applications International Corporation(SAIC).MENE has also been run against all-uppercasedata.
On this we achieved an F-measure of 88.19for the MENE-only system and 91.38 for the MENE+ Proteus system.
The latter figure matches thebest currently published result (Bikel et al, 1997)on within-domain all-caps data.
On the other hand,we scored lower on all-caps than BBN's Identifinderin the MUC-7 formal evaluation for reasons whichare probably similar to the ones discussed in section9 in the comparison of our mixed case performances(Miller et al, 1998) (Borthwick et al, 1998).
Wehave put very little effort into optimizing MENE on157Systems 425 , ~ 5 0  250 150 100 80 40 20 10 5MENE ~-~"F '~O'~-9"~, '  90,64 89~17 87,85 84.14 80.97 76.43 63.13MENE + Proteus II 95.73 I 95.61 \[ 95 .56 \ [944 .6 \ [94 .30  \[ 93.44 \[91.69 \[ \[ I IMENE + Manitoba 11 95-6?1 95.49 1 95-2?~ \[ 948- 6 \] 74_-5?
I 94.15 1 93.06 1 I I \]MENE + \]soQuest II 96.73 \[ 96.55 \[ 96.70ME+ Pr + Ma+IQTable 3: Systems' performances with different numbers of articlesthis type of corpus and believe that there is roomfor improvement here.In another experiment, we stripped out all fea-tures other than the lexical features and .,;tillachieved an F-measure of 88.13.
Since these featuresdo not rely on any external knowledge sources andare automatically generated, this result is a strongindicator of MENE's portability.The MUC-7 formal evaluation involved a shift intopic which was not communicated to the partici-pants beforehand-the training data focused on air-line disasters while the test data was on missile androcket launches.
MENE fared much more poorlyon this data than it did on the within-domain dataquoted above, achieving an F-measure of only 88.80for the MENE + Proteus system and 84.22 for theMENE-only system.
While 88.80 was still the fourthhighest score out of the twelve participants in theevaluation, we feel that it is necessary to view thisnumber as a cross-domain portability result ratherthan as an indicator of how the system can do onunseen data within its training domain.
We believethat if the system had been allowed to train on mis-sile/rocket launch articles, its performance on thesearticles would have been much better.
More MENEtest results and discussion of the formal run can befound in (Borthwick et al, 1998).9 RELATED WORKM.E.
has been successfully applied to many othertasks in computational linguistics.
Some recent workfor which there are solid comparable benchmarks ithe work of Adwait Ratnaparkhi at the Universityof Pennsylvania.
He has achieved state-of-the artresults by applying M.E.
to parsing (Ratnaparkhi,1997a), part-of-speech tagging (Ratnaparkhi, 1996),and sentence-boundary detection (Reynar and Rat-naparkhi, 1997).
Other recent work has appliedM.E.
to language modeling (Rosenfeld, 1994), ma-chine translation (Berger et al, 1996), and referenceresolution (Kehler, 1997).
M.E.
was first appliedto named entity recognition at the MUC-7 confer-ence by (Borthwick et al, 1998) and (Mikheev andGrover, 1998).Note that part-of-speech tagging is, in many ways,a very similar task to that of named-entity recogni-tion.
Ratnaparkhi's tagger is similar to MENE, inthat his features look at the surrounding two-wordlexical context, but his system makes less use of dic-tionaries.
On the other hand, his system looks atword suffixes and prefixes in the case of unknownwords, which is something we haven't tried withMENE and looks at its own output by looking atits previous two tags when making its decision.
Wedo this implicitly through our requirement that thefutures we output be consistent, but we found thatan attempt o do this more directly by building aconsistency feature directly into the model had noeffect on our results.At the MUC-7 conference, there were two otherinteresting systems using statistical techniques fromthe Language Technology Group/University of Ed-inborough (Mikheev and Grover, 1998) and BBN(Miller et al, 1998).
Comparisons with the LTGsystem are difficult since it was a hybrid model inwhich the text was passed through a five-stage pro-cess, only three of which involved maximum entropyand over half of the system's recall came from thetwo non-statistical phases.
The LTG system demon-strated superior performance on the formal run rel-ative to the MENE-Proteus hybrid system (93.39vs 88.80), but it isn't clear whether their advan-tage came from superior handcoded rules or supe-rior statistical techniques, because their system isnot as easily broken down into separate componentsas is MENE-Proteus.
It is also possible that tightersystem integration between the statistical and hand-coded components was responsible for some of LTG'srelative advantage, but note that MENE-Proteus ap-pears to have an advantage over LTG in terms ofportability.
We are currently experimenting withporting MENE to Japanese, for instance, and ex-pect that it could be combined with a pre-existingJapanese handcoded system, but it isn't clear thatthis could be done with the LTG system.
Neverthe-less, one of our avenues for future research is to lookat tighter multi-system integration methods whichwon't compromise MENE's essential portability.Table 4 gives a comparison of BBN's HMM-based Identifinder (Miller et al, 1998) and NYU'sMENE and MENE-Proteus systems on differenttraining and test sets.
We are not sure why MENE-Proteus was hurt more badly by the evaluation-time switch from aviation disaster articles to mis-158ConditionsTrained on Official training dataTested on dry run (within domain)Each organization trained on all ofits own data and tested on dry runSame as above, but run againstofficial MUC-7 data\[\] Identifinder MENE MENE + Proteus92.5 89.17 94.3095.1 92.20 95.6190.44 84.22 88.80Table 4: Comparison of BBN and NYU statistical systemssile/rocket launch articles, but suspect hat it mayhave been due to Identifinder's greater quantity andquality of training data.
BBN used 790,000 wordsof training data to our 321,000.
The quality advan-tage may have come from selecting sentences from alarger corpus for their annotators to tag which werechosen so as to increase the variety of training data.When MENE-only and ldentifinder are comparedtraining on the same number of articles and test-ing on within-domain data, Identifinder still has anedge.
We speculate that this is due to the dynamicupdating of Identifinder's vocabulary during decod-ing when person or organization ames are recog-nized, which gives the system a sort of long-distan'cereference resolution which is lacking in MENE.
Inaddition, BBN's HMM-based system implictly pre-dicts named entities based on consecutive pairs ofwords rather than based on single words, as is donein MENE, because ach type of name has its ownbigram language model.
In the decoding process,the Viterbi algorithm chooses the sequence of nameswhich yields the highest joint probability of names,words, and features associated with each word.In comparing the maximum entropy and HMM-based approaches to named entity recognition, weare hopeful that M.E.
will turn out to be the bettermethod in the end.
Ire think it is possible that someof Identifinder's current advantage can be neutral-ized by simply adding the just-mentioned features toMENE.
On the other hand, we have a harder timeseeing how some of MENE's strengths can be inte-grated into an HMM-based system.
It is not clear,for instance, how a wide variety of dictionaries couldbe added to Identifinder or whether the system couldbe combined with a handcoded system as was donewith our system and the one from LTG.10 CONCLUSIONS AND FUTUREWORKMENE is a very new, and, we feel, still immaturesystem.
Work started in October, 1397, and thesystem described above was not in place until mid-February.
1998.
We believe that we can push thescore of the MENE-only system higher by incorpo-rating long-range reference-resolution  MENE'soutput.
We are also missing a large number ofacronyms which could be picked up by dynamicallybuilding them from entities which MENE had taggedelsewhere and then pulling that data in as a newclass of feature.
The other key element missing fromthe current system is a set of general compound fea-tures, which, as discussed above, would require theuse of a more sophisticated feature selection algo-rithm.
All three of these elements are present insystems uch as IsoQuest's (Krupka and Hausman,1998), and their absence from MENE probably ex-plains much of the reason why the MENE-only sys-tem failed to perform at the state-of-the-art.
Weintend to add all of these elements to MENE in thenear future to test this hypothesis.Nevertheless, we believe that we have alreadydemonstrated some very useful results.
MENE ishighly portable, as we have already demonstratedwith our result on upper-case English text and evenin its current state, its results are already compa-rable to that of the only other purely statisticalEnglish NE system which we are aware of (Milleret al, 1998).
As shown with our result on run-ning MENE with only the lexical features that itlearns from the training corpus, porting MENE canbe done with very little effort if appropriate trainingdata is provided-it isn't even necessary to provideit with dictionaries to generate an acceptable r sult.We are working on a port to Japanese NE to furtherdemonstrate MENE's flexibility.However, we believe that the results on combiningMENE with other systems are some of the most in-triguing.
We would hypothesize that, given sufficienttraining data, any handcoded system would benefitfrom having its output passed to MENE as a finalstep.
MENE also opens up new avenues for collabo-ration whereby different organizations could focus ondifferent aspects of the problem of N.E.
recognitionwith the maximum entropy system acting as an ar-bitrator.
MENE also offers the prospect of achievingvery high performance with very little effort.
SinceMENE starts out with a fairly high base score just onits own, we speculate that a MENE user could thenconstruct a hand-coded system which only focusedon MENE's weaknesses, while skipping the areas inwhich MENE is already strong.Finally, one can imagine a user acquiring licenses159to several different N.E.
systems, generating sometraining data, and then combining it all under aMENE-like system.
We have shown that this ap-proach can yield performance which is competitivewith that of a human tagger.11 ACKNOWLEDGMENTSWe would like to thank Troy Straszheim for writingthe Viterbi search routine used in this work.Re ferencesAdam Berger and Harry Printz.
1998.
A compar-ison of criteria for maximum entropy/minimumdivergence feature selection.
In Nancy Ide andAtro Boutilainen, editors, Proceedings ofthe ThirdConference on Empirical Methods in Natural Lan-guage Processing, pages 97-106.
The Associationfor Computation Linguistics, June.Adam L. Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural anguage processing.
Compu-tational Linguistics, 22 (1) :39-71.Daniel M. Bikel, Scott Miller, Richard Schwartz,and Ralph Weischedel.
1997.
Nymble: a high-performance l arning name-finder.
In Fifth Con-ference on Applied Natural Language Processing.Andrew Borthwick, Radu Florian, and Kishore Pa-pineni.
1997.
The system architecture of MENEwas strongly influenced by the architecture of amaximum entropy language model jointly devel-oped by these three authors at IBM Watson Labs,Yorktown Heights, New York, in the summer of1997.Andrew Borthwick, John Sterling, EugeneAgichtein, and Ralph Grishman.
1998.
Nyu:Description of the mene named entity systemas used in muc-7.
In Proceedings of the SeventhMessage Understanding Conference (MUC-7).Stephen Della Pietra, Vincent Della Pietra, andJohn Lafferty.
1995.
Inducing features of ran-dom fields.
Technical Report CMU-CS-95-144,Carnegie Mellon University.Ralph Grishman.
1995.
The nyu system for mue-6 or where's the syntax?
In Proceedings of theSizth Message Understanding Conference.
MorganKaufrnann, November.Edwin T. Jaynes.
1996.
Probability theory: Thelogic of science.
Manuscript for book.
Availableat fftp://bayes.wustl.edu/, May.Andrew Kehler.
1997.
Probabilistic oreference ininformation extraction.
In Empirical Methods inNatural Language Processing, volume 2, August.George R. Krupka and Kevin Hausman.
1998.lsoquest: Description of the netowl(tm) extrac-tor system as used in muc-7.
In Proceedings ofthe Seventh Message Understanding Conference(MUC-7).Dekang Lin.
1998.
Using collocation statistics in in-formation extraction.
In Proceedings of the Sev-enth Message Understanding Conference (MUC-7).Elaine Marsh and Dennis Perzanowski.
1998.
Muc-7 evaluation of i.e.
technology: Overview of re-sults.
In Proceedings ofthe Seventh Message Un-derstanding Conference (MUC-7).Andrei Mikheev and Claire Grover.
1998.
Ltg: De-scription of the ne recognition system used formuc-7.
In Proceedings ofthe Seventh Message Un-derstanding Conference (MUC- 7), April.Scott Miller, Michael Crystal, Heidi Fox, LanceRarnshaw, Richard Schwartz, Rebecca Stone,Ralph Weischedel, and the Annotation Group.1998.
Algorithms that learn to extractinformation-bbn: Description of the sift sys-tem as used for muc-7.
In Proceedings of theSeventh Message Understanding Conference(MUC-7), April.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 133-142.
University of Pennsylva-nia, May.Adwait Ratnaparkhi.
1997a.
A linear observedtime statistical parser based on maximum entropymodels.
In Conference on Empirical Methods inNatural Language Processing.
University of Penn-sylvania, June.Adwait Ratnaparkhi.
1997b.
A simple introductionto maximum entropy models for natural anguageprocessing.
Technical Report 97-08, Institute for"Research in Cognitive Science, University of Penn-sylvania, May.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.A maximum entropy approach to identifying sen-tence structures.
In Fifth Conference on AppliedNatural Language Processing, pages 16-19, April.Eric Sven Ristad.
1998.
Maximum en-tropy modeling toolkit, release 1.6 beta.http://www.mnemonic.com/software/memt,February.
Includes documentation which has anoverview of MaxEnt modeling.Ronald Rosenfeld.
1994.
Adaptive Statistical Lan-guage Modeling: A Maximum Entropy Approach.Ph.D.
thesis, Carnegie Mellon University.
CMUTechnical Report CM U-CS-94-138.Satoshi Sekine, Ralph Grishman, and Hiroyuki Shin-nou.
1998.
A decision tree method for finding andclassifying names in japanese texts.
In Proceedingsof the Sixth Workshop on Very Large Corpora.160
