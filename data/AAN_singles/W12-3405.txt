Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 36?41,Jeju, Republic of Korea, 12 July 2012. c?2012 Association for Computational LinguisticsUsing Nominal Compounds for Word Sense DiscriminationYannick VersleyUniversity of Tu?bingenDepartment of Linguisticsversley@sfs.uni-tuebingen.de Verena HenrichUniversity of Tu?bingenDepartment of Linguisticsverena.henrich@uni-tuebingen.deAbstractIn many morphologically rich languages, con-ceptually independent morphemes are gluedtogether to form a new word (a compound)with a meaning that is often at least in part pre-dictable from the meanings of the contribut-ing morphemes.
Assuming that most com-pounds express a subconcept of exactly onesense of its nominal head, we use compoundsas a higher-quality alternative to simply usinggeneral second-order collocate terms in thetask of word sense discrimination.
We eval-uate our approach using lexical entries fromthe German wordnet GermaNet (Henrich andHinrichs, 2010).1 IntroductionIn several morphologically rich languages such asGerman and Dutch, compounds are usually writtenas one word: In a process where nouns, verbs andother prefixes combine with a head noun (called thesimplex when it occurs on its own), a novel wordcan be formed which is typically interpretable byconsidering its parts and the means of combination.The process of compounding is both highly produc-tive and subject to lexicalization (i.e., the creationof non-transparent compounds that can only be in-terpreted as a whole rather than as a combinationof parts).
The analysis of compounds have beensubject to interest in machine translation as well asin the semantic processing of morphologically richlanguages.
The analysis of compounds is generallychallenging for many reasons.
In particular, com-pounds leave us with the dilemma of either model-ing them as complete units, yielding a more accu-rate picture for lexicalized compounds but creatinga more severe sparse data problem in general, or try-ing to separate out their parts and ending up withproblems of wrongly split lexicalized compounds,or of incurring mis-splits where spurious ambigui-ties occur.The purpose of this paper is to address the ques-tion of whether semantic information of compoundoccurrences can be used to learn more about thesense distribution of the simplex head, with respectto a text collection.
Specifically, this paper focuseson the task of word sense discrimination, where thegoal is to find different senses of a word withoutassuming a hand-crafted lexical resource as train-ing material (in contrast to word sense disambigua-tion, where the exact sense inventory to be taggedis known at training and inference time, and wheremaking effective use of a resource such as WordNet(Miller and Fellbaum, 1991) or GermaNet (Henrichand Hinrichs, 2010) is an important part of the prob-lem to be solved).While the present paper focuses on nominal com-pounds in German, the method as such can also beapplied to other languages where compounds arewritten as one word.2 Related WorkAutomatic word sense discrimination (WSD) is atask that consists of the automatic discovery of asense inventory for a word and of associated exam-ples for each sense.To evaluate systems performing word sense dis-crimination, earlier research such as Schu?tze (1998)36uses either pseudowords ?
two words that have beenartificially conflated to yield an ambiguous conceptsuch as wide range/consulting firm ?
or use (ex-pensive) manually annotated data.
Subsequently,the contexts of these occurrences are clustered intogroups that correspond to training examples for eachpostulated sense.A different approach to the idea of word sensediscrimination can be found in the work of Panteland Lin (2002): they retrieve a set of most-similaritems to the target word, and then cluster these sim-ilar items according to distributional semantic prop-erties.
In Pantel and Lin?s approach, the output ofthe word sense induction algorithm is not a groupof contexts with the target word that will be used torepresent a sense, but instead one or more words thatare (hopefully) related to one particular sense.
Thecontexts in which the related words occur could thenbe used as positive examples for that particular senseof the target word.Pantel and Lin aim at a principled approach tocompare the soft-clustering approaches they pro-pose, in conjunction with a fixed set of relatedwords.
While the main interest of this paper liesin comparing different methods for generating thecandidate set of related words, the exact clusteringmethod is only of marginal interest.
In this paper, asimpler hard clustering method is used and only theassignment for the tight center of a cluster is consid-ered since the non-central items can be different oreven incomparable for the different methods.3 Our ApproachOur approach to word sense discrimination is basedon the idea that different compounds that have thesame simplex word as their head (e.g.
Blu?tenblatt?petal?, and Revolverblatt ?tabloid rag?)
are less am-biguous than the simplex (Blatt ?leaf?, ?newspa-per?)
itself.
This assumption is along the lines withwhat the ?one sense per collocation?
heuristic ofYarowsky (1993) would predict.Yarowsky noted that in a corpus of homo-graphs/homophones/near-homographs, translationdistinctions, and pseudo-words, a single collocation(such as ?foreign?
or ?presidential?)
is often enoughto disambiguate the occurrence of a near-homographsuch as aid/aide.
While Yarowsky claims that mostof the problems of such an approach would be dueto absent or unseen collocates, it is easily imagin-able that collocates such as old or big can occur withmultiple senses of a word.In German, noun compounds usually involve atleast a minimum degree of lexicalization: In En-glish, ?red flag?
and ?red beet?
are lexicalized (i.e.,denote something more specific than the composi-tional interpretation would suggest), but ?red rag?or ?red box?
are purely compositional.
In German,Rotwein ?red wine?
is a compound, but the morecompositional roter Apfel/*Rotapfel ?red apple?
isnot a compound and points to the fact that ?red ap-ple?
only has a compositional interpretation.
Be-cause of this minimal required degree of lexicaliza-tion, we would expect that German nominal com-pounds (as well as any compounds in a language thathas a similar distinction between affixating and non-affixating compounds) are, for the largest portion,compositional enough to be interpretable, but lexi-calized enough that a compound is always specificto only one sense of its head simplex.3.1 Finding CommitteesThe method of finding committees that form senseclusters is illustrated in Figure 1 using the targetword example Blatt.
To generate a candidate listof related terms, our method first retrieves all words(compounds) that have the target word as a suffix(step 3 in Figure 1).
This candidate set is then sortedaccording to distributional similarity with the targetword and cut off after N items (step 2 in Figure 1) toreduce the influence of spurious matches and non-taxonomic compounds and to avoid too much noisein the candidate list.In order to evaluate the method of selecting com-pounds as candidate words, we first cluster the setof candidate words into as many clusters as thereare target word senses represented in the candidatewords (step 3 in Figure 1, again using the distribu-tional similarity vectors of the words described inthe following subsection 3.2).To avoid biasing our method towards any partic-ular method of choosing the candidate words, wesimply assume that it is possible to produce a ?rea-sonable?
number of clusters.
In the next step, themost central items of each cluster (the ?committee?
)are determined, purely by closeness to the cluster?s37Figure 1: Steps in the clustering methodcentroid and disregarding similarity with the targetword.
The committee words are rendered in boldface in the circles in Figure 1.
The quality of theapproach is then evaluated according to whether thecommittees form a suitable representation for the setof senses that the target word possesses.An advantage of only including compounds in thecandidate list of related terms, instead of all words,is that the related words generated by such an ap-proach are conceptually considerably closer to thetarget word than those using all words as candidates:Using all words, the top candidates include the co-ordinate terms Frucht ?fruit?
and Blu?te ?flower?, aswell as more faraway terms such as Tuch ?cloth?
orHaar ?hair?
; using only compounds of the simplex,the candidate list contains mostly hyponyms such asLaubblatt ?leaf?, Titelblatt ?title page?
or Notenblatt?sheet of music?.3.2 Distributional Similarity and ClusteringBoth for the initial selection of candidate words(where the list is cut off after the top-N similarterms) and for the subsequent clustering step, fre-quency profiles from a large corpus are used to cre-ate a semantic vector from the target word and each(potential or actual) candidate word.To construct these frequency profiles, the web-news corpus of Versley and Panchenko (2012) isused, which contains 1.7 billion words of text fromvarious German online newspapers.
The text isparsed using MALTParser (Hall et al, 2006) andthe frequency of collocates with the ATTR (premod-ifying adjective) and OBJA (accusative object) re-lations is recorded.
Vectors are weighted using theconservative pointwise mutual information estimatefrom Pantel and Lin (2002).
For selecting most-similar words in candidate selection, we use a ker-nel based on the Jensen-Shannon Divergence acrossboth grammatical relations, similar to the methodproposed by O?
Se?aghdha and Copestake (2008).The resulting vector representations of words arethen clustered using average-link hierarchical ag-glomerative clustering using the CLUTO toolkit(Zhao and Karypis, 2005), which uses cosine sim-ilarity to assess the similarity of two vectors.
In thestudy of Pantel and Lin (2002), agglomerative clus-tering was among the best-performing off-the-shelfclustering methods.As we initially found that many features that wereused in clustering were less relevant to the differ-ent senses of the head word that were targeted, wealso introduce a method to enforce a focus on tar-get word compatible aspects of those words.
In thebasic approach (raw), the normal vector representa-tion of each word is used.
In the modified approach(intersect), only the features that are relevant for thetarget word are selected, by using for each featurethe minimum value of (i) that feature?s value in thecandidate word?s vector and (ii) that feature?s valuein the target word?s vector.3.3 Competing and Upper BaselinesTo see how well our method performs in relation toother approaches for finding related terms describ-38ing each sense of a synset, two lower baselines andone upper baseline have been implemented.One lower baseline uses general distributionallysimilar items.
This is an intelligent (but realistic)general baseline method ?
close in spirit to Panteland Lin (2002).
It simply consists in retrieving thedistributionally most similar words for the clusteringtask.
Effectively, this resembles our own method,but without the compound filtering step.The second lower baseline assumes that it shouldalways be possible to find one word that is related toone of the senses (yielding poor coverage but trivi-alizing the clustering problem).
This trivial baselineis called one-cluster.The upper baseline (called profile) assumes thatit knows which senses of the word should be mod-eled and that errors can only be introduced by theclustering step not reproducing the original sense.This baseline retrieves the synsets corresponding toeach sense of the word from GermaNet, and, amongthe terms in the neighbouring synsets (synonyms,hypernyms, hyponyms as well as sibling synsets),select those that are both unambiguous (i.e., do nothave other synsets corresponding to that word) andare distributionally most similar to the (ambiguous)target word.4 Evaluation FrameworkOur evaluation framework is based on retrieving aset of words related to the target item (the candidateset), and then using collocate vectors extracted froma corpus to cluster the candidate set into multiplesubsets.Once we have a clustering of the generated terms,we want a quantitative evaluation of the clustering.The underlying idea for this is that we would like tohave, for each sense of the target word, a cluster thathas one or several words describing it.
(We shouldnot assume that it is always possible to find manyrelated words for a particular sense).4.1 Evaluation DataAs target items, we used a list of simplexes thatare most productive in terms of compounding, us-ing a set of gold-standard compound splits that werecreated by Henrich and Hinrichs (2011); candidatewords (both compounds and general neighbours)Figure 2: Evaluation procedure for the committees of re-lated wordswere selected using a frequency list extracted fromthe Tu?PP-D/Z corpus (Mu?ller, 2004).
For the ex-periments themselves, no information about correctsplits of the compounds was assumed and potentialcompounds were simply retrieved as lemma formsthat have the target word as a suffix.The subsets from clustering the candidate setare then evaluated according to whether the most-central related words in that cluster are related to thesame sense of the target word, and how many sensesof the target word are covered by the clusters.4.2 Evaluation MetricGiven the committee lists that are output by the can-didate selection and output, we calculate an evalua-tion score by creating a mapping between senses ofthe target word and the committees that are the out-put of the clustering algorithm, choosing that map-ping according to a quality measure that describeshow well the committee members match that synset(the precision of that possible pairing between acommittee and a sense of the target word), as shownin figure 2.
Each candidate word is assigned a senseof the target word, either because it is a hyponym ofthat sense (for the compound-based method) or be-cause its path distance in GermaNet?s taxonomy isless than four (for the general terms method).
If acandidate word is not near any of the target word?ssense synsets, it is assigned no sense (and always39candidates num vectors score quality coveragecompound 5 intersect 0.399 0.882 0.468compound 30 intersect 0.489 0.721 0.702compound 100 intersect 0.419 0.586 0.769general 5 intersect 0.433 0.882 0.510general 30 intersect 0.528 0.696 0.784general 100 intersect 0.573 0.650 0.896compound 5 raw 0.406 0.898 0.468compound 30 raw 0.479 0.712 0.702compound 100 raw 0.422 0.591 0.769general 5 raw 0.441 0.902 0.510general 30 raw 0.526 0.694 0.784general 100 raw 0.551 0.630 0.896profile 10n intersect 0.737 0.781 0.945profile 10n raw 0.753 0.801 0.946one-cluster 1 ?
0.325 1.000 0.325Table 1: Evaluation scores for the different methods andbaselinescounted wrong).1Given a committee C of these (at most) k most-central candidate words in a cluster, we can calculatea measure P (C, s) = |w2C:sense(w)=s||C| that describeshow well this cluster corresponds to a given sense.
(Ideally, the committee would contain words onlyrelated to one sense).Using the Kuhn-Munkres algorithm (Kuhn,1955), we compute a mapping between each rep-resented synset s and a cluster Cs such thatPs P (Cs, s) is maximized.
The final score for onetarget word is this sum divided by the total numberof synsets for the target word ?
this means that amethod that yields a less representative set of can-didate words will normally not get a better score,unless the clusters are of higher enough quality, thanone that has candidate terms for each cluster.In addition to the score metric, we calculated aquality metric that divides the raw sum by the num-ber of senses covered in the candidate set, and acoverage metric that corresponds to the fraction ofsenses covered by the candidate set in the first place(see Table 1).5 Results and DiscussionTable 1 contains quantitative results for the differ-ent methods and also evaluation statistics for some1If a candidate word is not represented in GermaNet at all,it is discarded before the committee-building step, so that allcommittee words are in fact GermaNet-represented terms.lower and upper baselines: Selecting exactly one re-lated word as a candidate (and putting it in a clus-ter of its own) would yield a quality of 1.0, sincethat cluster is related to exactly one synset, but avery poor coverage of 0.325.
For the profile up-per baseline, which takes related terms from Ger-maNet and uses imperfect information only in clus-tering, we see that our clustering approach is ableto reconstruct committees of sense-identical termsout of the candidate list fairly well: given relatedterms for each sense, distributional similarity yieldsfairly good quality (0.801) and, unsurprisingly, near-perfect coverage for all senses (0.946).For the actual methods using compounds of aword (compound rows in Table 1) or distribution-ally similar words (general rows), we find that thecompound-based candidate selection only reachesvery limited coverage numbers and furthermoregives the best results with a smaller number of can-didate words (30 for compounds versus 100 for gen-eral).
Whether this effect is due to minority sensesbeing less productive in compounding or whethercompounds of the minority senses are not repre-sented in GermaNet is left to be investigated in fu-ture work.6 ConclusionWe used compounds in the selection of candidatewords for representing a target word?s senses in aword sense discrimination approach.
Because com-pounds are less-frequent overall than the similar-frequency coordinate terms that are retrieved in thegeneral baseline approach, the proposed approachdoes less well in covering all senses encoded in thegold standard and gets lower results in our evalua-tion metric.
In contrast to previous work by Panteland Lin, our evaluation approach allows a principledcomparison between approaches to generate candi-date lemmas in such a task and would be applicablealso to other alternative methods to do so.Acknowledgements We would like to thank AnneBrock, as well as several anonymous reviewers, forhelpful comments of an earlier version of this pa-per.
The research in this paper was partially fundedby the Deutsche Forschungsgemeinschaft as part ofCollaborative Research Centre (SFB) 833.40ReferencesAgirre, E., Aldezabal, I., and Pociello, E. (2006).Lexicalization and multiword expression in theBasque WordNet.
In Proceedings of the first In-ternational WordNet Conference.Bentivogli, L. and Pianta, E. (2004).
ExtendingWordNet with syntagmatic information.
In Pro-ceedings of the Second Global WordNet Confer-ence (GWC 2004).Hall, J., Nivre, J., and Nilsson, J.
(2006).
Discrim-inative classifiers for deterministic dependencyparsing.
In Proceedings of the COLING/ACL2006 Main Conference Poster Sessions.Henrich, V. and Hinrichs, E. (2010).
GernEdiT - theGermaNet editing tool.
In LREC 2010.Henrich, V. and Hinrichs, E. (2011).
Determin-ing immediate constituents of compounds in Ger-maNet.
In Proc.
International Conference Re-cent Advances in Language Processing (RANLP2011).Kuhn, H. W. (1955).
The hungarian method for theassignment problem.
Naval Research LogisticsQuarterly, 2:83?97.Miller, G. A. and Fellbaum, C. (1991).
Semanticnetworks of English.
Cognition, 41:197?229.Mu?ller, F. H. (2004).
Stylebook for the Tu?bingenpartially parsed corpus of written German (Tu?PP-D/Z).
Technischer Bericht, Seminar fu?r Sprach-wissenschaft, Universita?t Tu?bingen.O?
Se?aghdha, D. and Copestake, A.
(2008).
Semanticclassification with distributional kernels.
In Pro-ceedings of the 22nd International Conference onComputational Linguistics (COLING 2008).Pantel, P. and Lin, D. (2002).
Discovering wordsenses from text.
In Proceedings of ACM Confer-ence on Knowledge Discovery and Data Mining(KDD-02).Schu?tze, H. (1998).
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Versley, Y. and Panchenko, Y.
(2012).
Not justbigger: Towards better-quality Web corpora.
InProceedings of the 7th Web as Corpus Workshop(WAC-7).Yarowsky, D. (1993).
One sense per collocation.
InHUMAN LANGUAGE TECHNOLOGY: Proceed-ings of a Workshop Held at Plainsboro.Zhao, Y. and Karypis, G. (2005).
Hierarchical clus-tering algorithms for document datasets.
DataMining and Knowledge Discovery, 10:141?168.41
