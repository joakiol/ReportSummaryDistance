Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 190?197,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUnsupervised Model Adaptation using Information-Theoretic CriterionAriya Rastrow1, Frederick Jelinek1, Abhinav Sethy2 and Bhuvana Ramabhadran21Human Language Technology Center of Excellence, andCenter for Language and Speech Processing, Johns Hopkins University{ariya, jelinek}@jhu.edu2IBM T.J. Watson Research Center, Yorktown Heights, NY, USA{asethy, bhuvana}@us.ibm.comAbstractIn this paper we propose a novel generalframework for unsupervised model adapta-tion.
Our method is based on entropy whichhas been used previously as a regularizer insemi-supervised learning.
This technique in-cludes another term which measures the sta-bility of posteriors w.r.t model parameters, inaddition to conditional entropy.
The idea is touse parameters which result in both low con-ditional entropy and also stable decision rules.As an application, we demonstrate how thisframework can be used for adjusting languagemodel interpolation weight for speech recog-nition task to adapt from Broadcast news datato MIT lecture data.
We show how the newtechnique can obtain comparable performanceto completely supervised estimation of inter-polation parameters.1 IntroductionAll statistical and machine learning techniques forclassification, in principle, work under the assump-tion that1.
A reasonable amount of training data is avail-able.2.
Training data and test data are drawn from thesame underlying distribution.In fact, the success of statistical models is cru-cially dependent on training data.
Unfortunately,the latter assumption is not fulfilled in many appli-cations.
Therefore, model adaptation is necessarywhen training data is not matched (not drawn fromsame distribution) with test data.
It is often the casewhere we have plenty of labeled data for one specificdomain/genre (source domain) and little amount oflabeled data (or no labeled data at all) for the de-sired domain/genre (target domain).
Model adapta-tion techniques are commonly used to address thisproblem.
Model adaptation starts with trained mod-els (trained on source domain with rich amount of la-beled data) and then modify them using the availablelabeled data from target domain (or instead unla-beled data).
A survey on different methods of modeladaptation can be found in (Jiang, 2008).Information regularization framework has beenpreviously proposed in literature to control the la-bel conditional probabilities via input distribution(Szummer and Jaakkola, 2003).
The idea is that la-bels should not change too much in dense regionsof the input distribution.
The authors use the mu-tual information between input features and labels asa measure of label complexity.
Another frameworkpreviously suggested is to use label entropy (condi-tional entropy) on unlabeled data as a regularizer toMaximum Likelihood (ML) training on labeled data(Grandvalet and Bengio, 2004).Availability of resources for the target domain cat-egorizes these techniques into either supervised orunsupervised.
In this paper we propose a generalframework for unsupervised adaptation using Shan-non entropy and stability of entropy.
The assump-tion is that in-domain and out-of-domain distribu-tions are not too different such that one can improvethe performance of initial models on in-domain databy little adjustment of initial decision boundaries(learned on out-of-domain data).1902 Conditional Entropy based AdaptationIn this section, conditional entropy and its relationto classifier performance are first described.
Next,we introduce our proposed objective function for do-main adaptation.2.1 Conditional EntropyConsidering the classification problem whereX andY are the input features and the corresponding classlabels respectively, the conditional entropy is a mea-sure of the class overlap and is calculated as followsH(Y|X) = EX[H(Y|X = x)] =?
?p(x)(?yp(y|x) log p(y|x))dx (1)Through Fano?s Inequality theorem, one can seehow conditional entropy is related to classificationperformance.Theorem 1 (Fano?s Inequality) SupposePe = P{Y?
6= Y} where Y?
= g(X) are theassigned labels for the data points, based on theclassification rule.
ThenPe ?H(Y|X)?
1log(|Y| ?
1)where Y is the number of possible classes andH(Y |X) is the conditional entropy with respect totrue distibution.The proof to this theorem can be found in (Cover andThomas, 2006).
This inequality indicates thatY canbe estimated with low probability of error only if theconditional entropy H(Y|X) is small.Although the above theorem is useful in a sensethat it connects the classification problem to Shan-non entropy, the true distributions are almost neverknown to us1.
In most classification methods, a spe-cific model structure for the distributions is assumedand the task is to estimate the model parameterswithin the assumed model space.
Given the model1In fact, Theorem 1 shows how relevant the input featuresare for the classification task by putting a lower bound on thebest possible classifier performance.
As the overlap betweenfeatures from different classes increases, conditional entropy in-creases as well, thus lowering the performance of the best pos-sible classifier.structure and parameters, one can modify Fano?s In-equality as follows,Corollary 1Pe(?)
= P{Y?
6= Y |?}
?
H?(Y|X)?
1log(|Y| ?
1)(2)where Pe(?)
is the classifier probability of errorgiven model parameters, ?
andH?
(Y|X) =??p(x)(?yp?
(y|x) log p?
(y|x))dxHere, H?
(Y|X) is the conditional entropy imposedby model parameters.Eqn.
2 indicates the fact that models with lowconditional entropy are preferable.
However, a lowentropy model does not necessarily have good per-formance (this will be reviewed later on) 22.2 Objective FunctionMinimization of conditional entropy as a frameworkin the classification task is not a new concept andhas been tried by researchers.
In fact, (Grandvaletand Bengio, 2004) use this along with the maxi-mum likelihood criterion in a semi-supervised setup such that parameters with both maximum like-lihood on labeled data and minimum conditional en-tropy on unlabeled data are chosen.
By minimiz-ing the entropy, the method assumes a prior whichprefers minimal class overlap.
Entropy minimiza-tion is used in (Li et al, 2004) as an unsupervisednon-parametric clustering method and is shown toresult in significant improvement over k-mean, hier-archical clustering and etc.These methods are all based on the fact that mod-els with low conditional entropy have their decisionboundaries passing through low-density regions ofthe input distribution, P (X).
This is consistent withthe assumption that classes are well separated so thatone can expect to take advantage of unlabeled exam-ples (Grandvalet and Bengio, 2004).In many cases shifting from one domain to an-other domain, initial trained decision boundaries (on2Imagine a model which classifies any input as class 1.Clearly for this model H?
(Y|X) = 0.191out-of-domain data) result in high conditional en-tropy for the new domain, due to mismatch be-tween distributions.
Therefore, there is a need toadjust model parameters such that decision bound-aries goes through low-density regions of the distri-bution.
This motivates the idea of using minimumconditional entropy criterion for adapting to a newdomain.
At the same time, two domains are oftenclose enough that one would expect that the optimalparameters for the new domain should not deviatetoo much from initial parameters.
In order to formu-late the technique mentioned in the above paragraph,let us define ?init to be the initial model parame-ters estimated on out-of-domain data (using labeleddata).
Assuming the availability of enough amountof unlabeled data for in-domain task, we try to min-imize the following objective function w.r.t the pa-rameters,?new = argmin?H?
(Y|X) + ?
||?
?
?init||p(3)where ||?
?
?init||p is an Lp regularizer and tries toprevent parameters from deviating too much fromtheir initial values3.Once again the idea here is to adjust the param-eters (using unlabeled data) such that low-densityseparation between the classes is achieved.
In thefollowing section we will discuss the drawback ofthis objective function for adaptation in realistic sce-narios.3 Issues with Minimum Entropy CriterionIt is discussed in Section 2.2 that the model param-eters are adapted such that a minimum conditionalentropy is achieved.
It was also discussed how this isrelated to finding decision boundaries through low-density regions of input distribution.
However, theobvious assumption here is that the classes are wellseparated and there in fact exists low-density regionsbetween classes which can be treated as boundaries.Although this is a suitable/ideal assumption for clas-sification, in most practical problems this assump-tion is not satisfied and often classes overlap.
There-fore, we can not expect the conditional entropy to be3The other reason for using a regularizer is to prevent trivialsolutions of minimum entropy criterionconvex in this situation and to achieve minimizationw.r.t parameters (other than the trivial solutions).Let us clarify this through an example.
ConsiderX to be generated by mixture of two 2-D Gaus-sians (each with a particular mean and covariancematrix) where each Gaussian corresponds to a par-ticular class ( binary class situation) .
Also in orderto have linear decision boundaries, let the Gaussianshave same covariance matrix and let the parameterbeing estimated be the prior for class 1, P (Y = 1).Fig.
1 shows two different situations with over-lapping classes and non-overlapping classes.
Theleft panel shows a distribution in which classes arewell separated whereas the right panel correspondsto the situation where there is considerable overlapbetween classes.
Clearly, in the later case there isno low-density region separating the classes.
There-fore, as we change the parameter (here, the prior onthe class Y = 1), there will not be any well definedpoint with minimum entropy.
This can be seen fromFig.
2 where model conditional entropy is plottedvs.
class prior parameter for both cases.
In the caseof no-overlap between classes, entropy is a convexfunction w.r.t the parameter (excluding trivial solu-tions which happens at P (Y = 1) = 0, 1) and isminimum at P (Y = 1) = 0.7 which is the true priorwith which the data was generated.We summarize issues with minimum entropy cri-terion and our proposed solutions as follows:?
Trivial solution: this happens when we put de-cision boundaries such that both classes areconsidered as one class (this can be avoided us-ing the regularizer in Eqn.
3 and the assump-tion that initial models have a reasonable solu-tion, e.g.
close to the optimal solution for newdomain )?
Overlapped Classes: As it was discussed inthis section, if the overlap is considerable thenthe entropy will not be convex w.r.t to modelparameters.
We will address this issue inthe next section by introducing the entropy-stability concept.4 Entropy-StabilityIt was discussed in the previous section that a mini-mum entropy criterion can not be used (by itself) in192?3 ?2 ?1 0 1 2 3 4 5 6 7?4?20246810X1X 2?3 ?2 ?1 0 1 2 3 4 5 6 7?3?2?101234567X1X 2Figure 1: Mixture of two Gaussians and the corresponding Bayes decision boundary: (left) with no class overlap(right) with class overlap0?0.05?0.1?0.15?0.2?0.25?0.3?0?0.005?0.01?0.015?0.02?0.025?0.03?0.035?0?
0.1?
0.2?
0.3?
0.4?
0.5?
0.6?
0.7?
0.8?
0.9?
1?Condi?nal?Entropy?P(Y=1)?without?overlap?with?overlap?Figure 2: Condtional entropy vs. prior parameter, P (Y =1)situations where there is a considerable amount ofoverlap among classes.
Assuming that class bound-aries happen in the regions close to the tail of classdistributions, we introduce the concept of Entropy-Stability and show how it can be used to detectboundary regions.
Define Entropy-Stability to be thereciprocal of the following?????????H?(Y|X)??????????p=?????????????p(x)?
(?y p?
(y|x) log p?(y|x))??
dx???????????
?p(4)Recall: since ?
is a vector of parameters, ?H?(Y|X)?
?will be a vector and by using Lp norm Entropy-stability will be a scalar.The introduced concept basically measures thestability of label entropies w.r.t the model parame-ters.
The idea is that we prefer models which notonly have low-conditional entropy but also have sta-ble decision rules imposed by the model.
Next, weshow through the following theorem how Entropy-Stability measures the stability over posterior prob-abilities (decision rules) of the model.Theorem 2?????????H?(Y|X)??????????p=???????????p(x)(?y?p?(y|x)??
log p?(y|x))dx?????????
?pwhere the term inside the parenthesis is the weightedsum (by log-likelihood) over the gradient of poste-rior probabilities of labels for a given sample xProof The proof is straight forward and uses the factthat?
?p?(y|x)??
= ?(Pp?(y|x))??
= 0 .Using Theorem 2 and Eqn.
4, it should be clearhow Entropy-Stability measures the expected sta-bility over the posterior probabilities of the model.A high value of???????H?(Y|X)???????
?pimplies models withless stable decision rules.
In order to explain howthis is used for detecting boundaries (overlapped193regions) we once again refer back to our mixtureof Gaussians?
example.
As the decision boundarymoves from class specific regions to overlapped re-gions (by changing the parameter which is here classprior probability) we expect the entropy to continu-ously decrease (due to the assumption that the over-laps occur at the tail of class distributions).
How-ever, as we get close to the overlapping regions theadded data points from other class(es) will resistchanges in the entropy.
resulting in stability over theentropy until we enter the regions specific to otherclass(es).In the following subsection we use this idea topropose a new objective function which can be usedas an unsupervised adaptation method even for thecase of input distribution with overlapping classes.4.1 Better Objective FunctionThe idea here is to use the Entropy-Stability con-cept to accept only regions which are close to theoverlapped parts of the distribution (based on ourassumption, these are valid regions for decisionboundaries) and then using the minimum entropycriterion we find optimum solutions for our parame-ters inside these regions.
Therefore, we modify Eqn.3 such that it also includes the Entropy-Stabilityterm?new = argmin?(H?
(Y|X) + ??????????H?(Y|X)?????????
?p?+ ?
||?
?
?init||p)(5)The parameter ?
and ?
can be tuned using smallamount of labeled data (Dev set).5 Speech Recognition TaskIn this section we will discuss how the proposedframework can be used in a speech recognition task.In the speech recognition task, Y is the sequenceof words and X is the input speech signal.
For agiven speech signal, almost every word sequence isa possible output and therefore there is a need fora compact representation of output labels (words).For this, word graphs (Lattices) are generated dur-ing the recognition process.
In fact, each lattice isan acyclic directed graph whose nodes correspondto particular instants of time, and arcs (edges con-necting nodes) represent possible word hypotheses.Associated with each arc is an acoustic likelihoodand language model likelihood scores.
Fig.
3 showsan example of recognition lattice 4 (for the purposeof demonstration likelihood scores are not shown).L.
Mangu et al: Finding Consensus in Speech Recognition 6(a) Input lattice (?SIL?
marks pauses)SILSILSILSILSILSILVEALVERYHAVEMOVEHAVEHAVEITMOVEHAVE ITVERYVERYVERYVERYOFTENOFTENFINEFINEFASTIII(b) Multiple alignment (?-?
marks deletions)- -IMOVEHAVE IT VEALVERYFINEOFTENFASTFigure 1: Sample recognition lattice and corresponding multiple alignment represented asconfusion network.alignment (which gives rise to the standard string edit distance WE (W,R)) witha modified, multiple string alignment.
The new approach incorporates all latticehypotheses7 into a single alignment, and word error between any two hypothesesis then computed according to that one alignment.
The multiple alignment thusdefines a new string edit distance, which we will call MWE (W,R).
While thenew alignment may in some cases overestimate the word error between twohypotheses, as we will show in Section 5 it gives very similar results in practice.The main benefit of the multiple alignment is that it allows us to extractthe hypothesis with the smallest expected (modified) word error very efficiently.To see this, consider an example.
Figure 1 shows a word lattice and the corre-sponding hypothesis alignment.
Each word hypothesis is mapped to a positionin the alignment (with deletions marked by ?-?).
The alignment also supportsthe computation of word posterior probabilities.
The posterior probability of aword hypothesis is the sum of the posterior probabilities of all lattice paths ofwhich the word is a part.
Given an alignment and posterior probabilities, it iseasy to see that the hypothesis with the lowest expected word error is obtainedby picking the word with the highest posterior at each position in the alignment.We call this the consensus hypothesis.7In practice we apply some pruning of the lattice to remove low probability word hypotheses(see Section 3.4).Figure 3: Lattice ExampleSince lattices contain all the likely hypotheses(unlikely hypotheses are pruned during recognitionand will not be included in the lattice), conditionalentropy for any given input speech signal, x, can beapproximated by the conditional entropy of the lat-tice.
That is,H?
(Y|X = xi) = H?
(Y|Li)whereLi is the corresponding decoded lattice (givenspeech recognizer parameters) of utterance xi.For the calculation of entropy we need toknow the distribution of X because H?
(Y|X) =EX [H?
(Y|X = x)] and since this distribution is notknown to us, we use Law of Large Numbers to ap-proximate it by the empirical averageH?
(Y|X) ?
?
1NN?i=1?y?Lip?
(y|Li) log p?
(y|Li) (6)Here N indicates the number of unlabeled utter-ances for which we calculate the empirical value ofconditional entropy.
Similarly, expectation w.r.t in-put distribution in entropy-stability term is also ap-proximated by the empirical average of samples.Since the number of paths (hypotheses) in the lat-tice is very large, it would be computationally infea-sibl to c ute the conditi nal entropy y enumer-ating all possible paths in the lattice and calculating4The figure is adopted from (Mangu et al, 1999)194Element ?p, r?
?p1, r1??
?p2, r2?
?p1p2, p1r2 + p2r1?
?p1, r1??
?p2, r2?
?p1 + p2, r1 + r2?0 ?0, 0?1 ?1, 0?Table 1: First-Order (Expectation) semiring: Definingmultiplication and sum operations for first-order semir-ings.their corresponding posterior probabilities.
Insteadwe use Finite-State Transducers (FST) to representthe hypothesis space (lattice).
To calculate entropyand the gradient of entropy, the weights for the FSTare defined to be First- and Second-Order semirings(Li and Eisner, 2009).
The idea is to use semiringsand their corresponding operations along with theforward-backward algorithm to calculate first- andsecond-order statistics to compute entropy and thegradient of entropy respectively.
Assume we are in-terested in calculating the entropy of the lattice,H(p) = ?
?d?Lip(d)Z log(p(d)Z )= logZ ?
1Z?d?Lip(d) log p(d)= logZ ?
r?Z (7)where Z is the total probability of all the paths inthe lattice (normalization factor).
In order to do so,we need to compute ?Z, r??
on the lattice.
It canbe proved that if we define the first-order semir-ing ?pe, pe log pe?
(pe is the non-normalized score ofeach arc in the lattice) as our FST weights and definesemiring operations as in Table.
1, then applying theforward algorithm will result in the calculation of?Z, r??
as the weight (semiring weight) for the finalnode.The details for using Second-Order semirings forcalculating the gradient of entropy can be foundin (Li and Eisner, 2009).
The same paper de-scribes how to use the forward-backward algorithmto speed-up the this procedure.6 Language Model AdaptationLanguage Model Adaptation is crucial when thetraining data does not match the test data being de-coded.
This is a frequent scenario for all AutomaticSpeech Recognition (ASR) systems.
The applica-tion domain very often contains named entities andN-gram sequences that are unique to the domain ofinterest.
For example, conversational speech hasa very different structure than class-room lectures.Linear Interpolation based methods are most com-monly used to adapt LMs to a new domain.
Asexplained in (Bacchiani et al, 2003), linear inter-polation is a special case of Maximum A Posterior(MAP) estimation, where an N-gram LM is built onthe adaptation data from the new domain and the twoLMs are combined using:p(wi|h) = ?pB(wi|h) + (1?
?
)pA(wi|h)0 ?
?
?
1where pB refers to out-of-domain (background)models and pA is the adaptation (in-domain) mod-els.
Here ?
is the interpolation weight.Conventionally, ?
is calculated by optimizing per-plexity (PPL) or Word Error Rate (WER) on someheld-out data from target domain.
Instead usingour proposed framework, we estimate ?
on enoughamount of unlabeled data from target domain.
Theidea is that resources on the new domain have al-ready been used to build domain specific modelsand it does not make sense to again use in-domainresources for estimating the interpolation weight.Since we are trying to just estimate one parameterand the performance of the interpolated model isbound by in-domain/out-of-domain models, there isno need to include a regularization term in Eqn.
5.Also???????H?(Y|X)???????
?p= |?H?(Y|X)??
| because we onlyhave one parameter.
Therefore, interpolation weightwill be chosen by the following criterion??
= argmin0???1H?
(Y|X) + ?|?H?(Y|X)??
| (8)For the purpose of estimating one parameter ?, weuse ?
= 1 in the above equation7 Experimental SetupThe large vocabulary continuous speech recognition(LVCSR) system used throughout this paper is basedon the 2007 IBM Speech transcription system forGALE Distillation Go/No-go Evaluation (Chen etal., 2006).
The acoustic models used in this system195are state-of-the-art discriminatively trained modelsand are the same ones used for all experiments pre-sented in this paper.For LM adaptation experiments, the out-of-domain LM (pB , Broadcast News LM) trainingtext consists of 335M words from the follow-ing broadcast news (BN) data sources (Chen etal., 2006): 1996 CSR Hub4 Language Modeldata, EARS BN03 closed captions, GALE Phase2 Distillation GNG Evaluation Supplemental Mul-tilingual data, Hub4 acoustic model training tran-scripts, TDT4 closed captions, TDT4 newswire, andGALE Broadcast Conversations and GALE Broad-cast News.
This language model is of order 4-gramwith Kneser-Ney smoothing and contains 4.6M n-grams based on a lexicon size of 84K.The second source of data is the MIT lectures dataset (J.
Glass, T. Hazen, S. Cyphers, I. Malioutov, D.Huynh, and R. Barzilay, 2007) .
This serves as thetarget domain (in-domain) set for language modeladaptation experiments.
This set is split into 8 hoursfor in-domain LM building, another 8 hours servedas unlabeled data for interpolation weight estimationusing criterion in Eqn.
8 (we refer to this as unsuper-vised training data) and finally 2.5 hours Dev set forestimating the interpolation weight w.r.t WER (su-pervised tuning) .
The lattice entropy and gradientof entropy w.r.t ?
are calculated on the unsupervisedtraining data set.
The results are discussed in thenext section.8 ResultsIn order to optimize the interpolation weight ?
basedon criterion in Eqn.
8, we devide [0, 1] to 20 differ-ent points and evaluate the objective function (Eqn.8) on those points.
For this, we need to calculateentropy and gradient of the entropy on the decodedlattices of the ASR system on 8 hours of MIT lectureset which is used as an unlabeled training data.
Fig.4 shows the value of the objective function againstdifferent values of model parameters (interpolationweight ?).
As it can be seen from this figure justconsidering the conditional entropy will result in anon-convex objective function whereas adding theentropy-stability term will make the objective func-tion convex.
For the purpose of the evaluation, weshow the results for estimating ?
directly on the tran-0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Model EntropyModel Entropy+Entropy-StabilityBN-LM MIT-LM?Figure 4: Objective function with and without includingEntropy-Stability term vs. interpolation weight ?
on 8hours MIT lecture unlabeled datascription of the 8 hour MIT lecture data and compareit to estimated value using our framework.
The re-sults are shown in Fig.
5.
Using ?
= 0 and ?
= 1the WERs are 24.7% and 21.1% respectively.
Us-ing the new proposed objective function, the optimal?
is estimated to be 0.6 with WER of 20.1% (Redcircle on the figure).
Estimating ?
w.r.t 8 hour train-ing data transcription (supervised adaptation) willresult in ?
= 0.7 (green circle) andWER of 20.0%.Instead ?
= 0.8 will be chosen by tuning the inter-polation weight on 2.5 hour Dev set with compara-ble WER of 20.1%.
Also it is clear from the figurethat the new objective function can be used to pre-dict the WER trend w.r.t the interpolation weightparameter.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Model Entropy + Entropy StabilityWER24.7%20.0%21.1%supervised tuning?Figure 5: Estimating ?
based on WER vs. theinformation-theoretic criterionTherefore, it can be seen that the new unsuper-196vised method results in the same performance as su-pervised adaptation in speech recognition task.9 Conclusion and Future WorkIn this paper we introduced the notion of entropystability and presented a new criterion for unsu-pervised adaptation which combines conditional en-tropy minimization with entropy stability.
The en-tropy stability criterion helps in selecting parametersettings which correspond to stable decision bound-aries.
Entropy minimization on the other hand tendsto push decision boundaries into sparse regions ofthe input distributions.
We show that combiningthe two criterion helps to improve unsupervised pa-rameter adaptation in real world scenario whereclass conditional distributions show significant over-lap.
Although conditional entropy has been previ-ously proposed as a regularizer, to our knowledge,the gradient of entropy (entropy-stability) has notbeen used previously in the literature.
We presentedexperimental results where the proposed criterionclearly outperforms entropy minimization.
For thespeech recognition task presented in this paper, theproposed unsupervised scheme results in the sameperformance as the supervised technique.As a future work, we plan to use the proposedcriterion for adapting log-linear models used inMachine Translation, Conditional Random Fields(CRF) and other applications.
We also plan to ex-pand linear interpolation Language Model schemeto include history specific (context dependent)weights.AcknowledgmentsThe Authors want to thank Markus Dreyer andZhifei Li for their insightful discussions and sugges-tions.ReferencesM.
Bacchiani, B. Roark, and M. Saraclar.
2003.
Un-supervised language model adaptation.
In Proc.ICASSP, pages 224?227.S.
Chen, B. Kingsbury, L. Mangu, D. Povey, G. Saon,H.
Soltau, and G. Zweig.
2006.
Advances in speechtranscription at IBM under the DARPA EARS pro-gram.
IEEE Transactions on Audio, Speech and Lan-guage Processing, pages 1596?1608.Thomas M. Cover and Joy A. Thomas.
2006.
Elementsof information theory.
Wiley-Interscience, 3rd edition.Yves Grandvalet and Yoshua Bengio.
2004.
Semi-supervised learning by entropy minimization.
InAdvances in neural information processing systems(NIPS), volume 17, pages 529?536.J.
Glass, T. Hazen, S. Cyphers, I. Malioutov, D. Huynh,and R. Barzilay.
2007.
Recent progress in MIT spo-ken lecture processing project.
In Proc.
Interspeech.Jing Jiang.
2008.
A literature survey on domain adapta-tion of statistical classifiers, March.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In EMNLP.Haifeng Li, Keshu Zhang, and Tao Jiang.
2004.
Min-imum entropy clustering and applications to gene ex-pression analysis.
In Proceedings of IEEE Computa-tional Systems Bioinformatics Conference, pages 142?151.Lidia Mangu, Eric Brill, and Andreas Stolcke.
1999.Finding consensus among words: Lattice-based worderror minimization.
In Sixth European Conference onSpeech Communication and Technology.M.
Szummer and T. Jaakkola.
2003.
Information regu-larization with partially labeled data.
In Advances inNeural Information Processing Systems, pages 1049?1056.197
