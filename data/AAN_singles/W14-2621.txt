Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 128?135,Baltimore, Maryland, USA.
June 27, 2014.c?2014 Association for Computational LinguisticsSentiment Classification on Polarity Reviews:An Empirical Study Using Rating-based FeaturesDai Quoc Nguyen?and Dat Quoc Nguyen?and Thanh Vu?andSon Bao Pham?
?Faculty of Information TechnologyUniversity of Engineering and TechnologyVietnam National University, Hanoi{dainq, datnq, sonpb}@vnu.edu.vn?Computing and Communications DepartmentThe Open University, Milton Keynes, UKthanh.vu@open.ac.ukAbstractWe present a new feature type namedrating-based feature and evaluate thecontribution of this feature to the taskof document-level sentiment analy-sis.
We achieve state-of-the-art re-sults on two publicly available stan-dard polarity movie datasets: on thedataset consisting of 2000 reviews pro-duced by Pang and Lee (2004) we ob-tain an accuracy of 91.6% while itis 89.87% evaluated on the dataset of50000 reviews created by Maas et al.(2011).
We also get a performanceat 93.24% on our own dataset consist-ing of 233600 movie reviews, and weaim to share this dataset for further re-search in sentiment polarity analysistask.1 IntroductionThis paper focuses on document-level sen-timent classification on polarity reviews.Specifically, the document-level sentimentanalysis is to identify either a positive ornegative opinion in a given opinionated re-view (Pang and Lee, 2008; Liu, 2010).
Inearly work, Turney (2002) proposed an un-supervised learning algorithm to classify re-views by calculating the mutual informationbetween a given phrase and reference words?excellent?
and ?poor?.
Pang et al.
(2002)applied supervised learners of Naive Bayes,Maximum Entropy, and Support Vector Ma-chine (SVM) to determine sentiment polarityover movie reviews.
Pang and Lee (2004)presented a minimum cut-based approach todetect whether each review?
sentence is morelikely subjective or not.
Then the sentiment ofthe whole document review is determined byemploying a machine learning method on thedocument?s most-subjective sentences.Recently, most sentiment polarity clas-sification systems (Whitelaw et al., 2005;Kennedy and Inkpen, 2006; Martineau andFinin, 2009; Maas et al., 2011; Tu et al., 2012;Wang and Manning, 2012; Nguyen et al.,2013) have obtained state-of-the-art results byemploying machine learning techniques usingcombination of various features such as N-grams, syntactic and semantic representationsas well as exploiting lexicon resources (Wil-son et al., 2005; Ng et al., 2006; Baccianellaet al., 2010; Taboada et al., 2011).In this paper, we firstly introduce a novelrating-based feature for the sentiment polarityclassification task.
Our rating-based featurecan be seen by that the scores ?
which usersemploy to rate entities on review websites ?could bring useful information for improvingthe performance of classifying polarity senti-ment.
For a review with no associated score,we could predict a score for the review in theuse of a regression model learned from an ex-ternal independent dataset of reviews and theiractual corresponding scores.
We refer to the128predicted score as the rating-based feature forlearning sentiment categorization.By combining the rating-based feature withunigrams, bigrams and trigrams, we thenpresent the results from sentiment classifica-tion experiments on the benchmark datasetspublished by Pang and Lee (2004) and Maaset al.
(2011).To sum up, the contributions of our studyare:?
Propose a novel rating-based feature anddescribe regression models learned fromthe external dataset to predict the featurevalue for the reviews in the two experi-mental datasets.?
Achieve state-of-the-art performances inthe use of the rating-based feature for thesentiment polarity classification task onthe two datasets.?
Analyze comprehensively the profi-ciency of the rating-based feature to theaccuracy performance.?
Report additional experimental results onour own dataset containing 233600 re-views.The paper is organized as follows: We pro-vide some related works and describe our ap-proach in section 2 and section 3, respectively.We detail our experiments in section 4.
Fi-nally, section 5 presents concluding remarks.2 Related WorksWhitelaw et al.
(2005) described an approachusing appraisal groups such as ?extremelyboring?, or ?not really very good?
for senti-ment analysis, in which a semi-automaticallyconstructed lexicon is used to return appraisalattribute values for related terms.
Kennedyand Inkpen (2006) analyzed the effect of con-textual valence shifters on sentiment classi-fication of movie reviews.
Martineau andFinin (2009) weighted bag-of-words in em-ploying a delta TF-IDF function for train-ing SVMs to classify the reviews.
Maas etal.
(2011) introduced a model to catch sen-timent information and word meanings.
Tuet al.
(2012) proposed an approach utiliz-ing high-impact parse features for convolutionkernels in document-level sentiment recogni-tion.
Meanwhile, Wang and Manning (2012)obtained a strong and robust performanceby identifying simple NB and SVM vari-ants.
Dahl et al.
(2012) applied the restrictedBoltzmann machine to learn representationscapturing meaningful syntactic and semanticproperties of words.
In addition, Nguyen etal.
(2013) constructed a two-stage sentimentclassifier applying reject option, where docu-ments rejected at the first stage are forwardedto be classified at the second stage.3 Our ApproachWe apply a supervised machine learning ap-proach to handle the task of document-levelsentiment polarity classification.
For machinelearning experiments, besides the N-gram fea-tures, we employ a new rating-based featurefor training models.3.1 Rating-based FeatureOur proposed rating-based feature can be seenby the fact that, on various review websites,users?
reviews of entities such as products,services, events and their properties ordinar-ily associate to scores which the users utilizeto rate the entities: a positive review mostlycorresponds with a high score whereas a neg-ative one strongly correlates to a low score.Therefore, the rated score could bring usefulinformation to enhance the sentiment classifi-cation performance.We consider the rated score associated toeach document review as a feature named RbFfor learning classification model, in whichthe rating-based feature RbF?s value of eachdocument review in training and test setsis estimated based on a regression modellearned from an external independent datasetof reviews along with their actual associatedscores.1293.2 N-gram FeaturesIn most related works, unigrams are consid-ered as the most basic features, in which eachdocument is represented as a collection ofunique unigram words where each word isconsidered as an individual feature.In addition, we take into account bigramsand trigrams since a combination of unigram,bigram and trigram features (N-grams) couldoutperform a baseline performance based onunigram features as pointed out in (Ng et al.,2006; Martineau and Finin, 2009; Wang andManning, 2012).We calculate the value of the N-gram fea-ture ithby using term frequency - inverse doc-ument frequency (tf*idf) weighting scheme forthe document D as follows:NgramiD= log(1 + tfiD) ?
log|{D}|dfiwhere tfiDis the occurrence frequency ofthe feature ithin document D, |{D}| is thenumber of documents in the data corpus {D},and dfiis the number of documents contain-ing the feature ith.
We then normalize N-gramfeature vector of the document D as follows:?????????NgramD=???{D}???????Ngram?
?|{D}| ?
????????NgramD????????
?NgramD4 Experimental Results4.1 Experimental SetupBenchmark datasets.
We conducted exper-imental evaluations on the polarity datasetPL041of 2000 movie reviews constructed byPang and Lee (2004).
The dataset PL04 con-sists of 1000 positive and 1000 negative doc-ument reviews in which each review was splitinto sentences with lowercase normalization.In order to compare with other published re-sults, we evaluate our method according to10-fold cross-validation scheme on the datasetPL04.In addition, we carry out experiments ona large dataset IMDB112of 50000 movie re-views produced by Maas et al.
(2011).
Thelarge dataset IMDB11 contains a training set1http://www.cs.cornell.edu/people/pabo/movie-review-data/2http://ai.stanford.edu/?amaas/data/sentiment/of 25000 labeled reviews and a test set of25000 labeled reviews, where training and testsets have 12500 positive reviews and 12500negative reviews in each.Machine learning algorithm.
We utilizeSVM implementation in LIBSVM3(Changand Lin, 2011) for learning classificationmodels in all our experiments on the twobenchmark datasets.Preprocess.
We did not apply stop-wordremoval, stemming and lemmatization to thedataset in any process in our system, becausesuch stop-words as negation words might in-dicate sentiment orientation, and as pointedout by Leopold and Kindermann (2002) stem-ming and lemmatization processes could bedetrimental to accuracy.In all experiments on PL04, we kept 30000most frequent N-grams in the training set foreach cross-validation run over each polarityclass.
After removing duplication, on an aver-age, there are total 39950 N-gram features in-cluding 10280 unigrams, 20505 bigrams and9165 trigrams.On the dataset IMDB11, it was 40000 mostfrequent N-grams in each polarity class to beselected for creating feature set of 53724 N-grams consisting of 13038 unigrams, 26907bigrams and 13779 trigrams.RbF feature extraction procedure.
Weaim to create an independent dataset for learn-ing a regression model to predict the featureRbF?s value for each document review in ex-perimental datasets.
Since Maas et al.
(2011)also provided 7091 IMDB movie titles4, weused those movie titles to extract all user re-views that their associated scores5are notequal to either 5 or 6 from the IMDB website.3http://www.csie.ntu.edu.tw/?cjlin/libsvm/.
Using linearkernel, default parameter settings.4http://www.imdb.com/.
It is noted that the 7091 movietitles are completely different from those that were used toproduce the datasets PL04 and IMDB11.5The score scale ranges from 1 to 10.
As the reviews cor-responding to rated scores 5 or 6 are likely to be ambiguousfor expressing positive or negative sentiments, we decide toignore those 5-6 score reviews.
We also abandon user reviewshaving no associated rated scores.130Figure 1: The score distribution of SAR14.Consequently, we created an independentscore-associated review dataset (SAR14)6of233600 movie reviews and their accompany-ing actual scores.
The external dataset SAR14consists of 167378 user reviews connected toscores valued from 7 to 10, and 66222 reviewslinked to 1-4 rated ones (as shown in Fig-ure 1).
Using SAR14, we employed SupportVector Regression algorithm implemented inSVMlightpackage7(Joachims, 1999) to learnthe regression model employing unigram fea-tures.
We then applied the learned modelto predict real score values of reviews in thebenchmark datasets, and referred to those val-ues as the values of the feature RbF.Although using N-gram features (consist-ing of unigrams, bigrams and trigrams) maygive better results, we tend to use only uni-grams for learning the regression model be-cause of saving the training time on the largesize of SAR14.
Furthermore, using unigramfeatures is good enough as presented in sec-tion 4.4.
To extract the RbF feature?s valuefor each PL04?s movie review, the regres-sion model was trained with 20000 most fre-6The SAR14 data set is available to download athttps://sites.google.com/site/nquocdai/resources7http://svmlight.joachims.org/.
Using with default param-eter settings.quent unigrams whilst 35000 most frequentunigrams were employed to learn regressionmodel to estimate the RbF feature for each re-view in the dataset IMDB11.4.2 Results on PL04Table 1 shows the accuracy results of ourmethod in comparison with other state-of-the-art SVM-based performances on the datasetPL04.
Our method achieves a baseline accu-racy of 87.6% which is higher than baselinesobtained by all other compared approaches.The accuracy based on only RbF feature is88.2% being higher than those published in(Pang and Lee, 2004; Martineau and Finin,2009; Nguyen et al., 2013).
By exploitinga combination of unigram and RbF features,we gain a result at 89.8% which is compara-ble with the highest performances reached by(Whitelaw et al., 2005; Ng et al., 2006; Wangand Manning, 2012).
It is evident that risingfrom 87.6% to 89.8% proves the effectivenessof using RbF in sentiment polarity classifica-tion.Turning to the use of N-grams, we attainan accuracy of 89.25% which is 1.65% higherthan the baseline result of 87.6%.
This showsthe usefulness of adding bigram and trigram131Features PL04 IMDB11Unigrams (baseline) 87.60 83.69N-grams 89.25 88.67RbF 88.20 89.14Unigrams + RbF 89.80 84.71N-grams + RbF 91.60 89.87Pang and Lee (2004) 87.20 ?
?Whitelaw et al.
(2005) 90.20 ?
?Ng et al.
(2006) 90.50 ?
?Martineau and Finin (2009) 88.10 ?
?Maas et al.
(2011) 88.90 88.89Tu et al.
(2012) 88.50 ?
?Dahl et al.
(2012) ??
89.23Wang and Manning (2012) 89.45 91.22Nguyen et al.
(2013) 87.95 ?
?Table 1: Accuracy results (in %).features to improve the accuracy.
With 91.6%,we reach a new state-of-the-art performanceby combining N-gram and RbF features.
Wealso note that our state-of-the-art accuracy is1.1% impressively higher than the highest ac-curacy published by Ng et al.
(2006).4.3 Results on IMDB11Table 1 also shows the performance resultsof our approach on the dataset IMDB11.
Al-though our method gets a baseline accuracy of83.69% which is lower than other baseline re-sults of 88.23% and 88.29% reported by Maaset al.
(2011) and Wang and Manning (2012)respectively, we achieve a noticeable accuracyof 89.14% based on only RbF feature.Furthermore, starting at the result of88.67% with N-gram features, we obtain asignificant increase to 89.87% by employingN-gram and RbF features.
Particularly, we dobetter than the performance at 89.23% pub-lished by Dahl et al.
(2012) with a 0.64% im-provement in accuracy on 160 test cases.From our experimental results in section4.2 and 4.3, we conclude that there are signif-icant gains in performance results by addingbigrams and trigrams as well as RbF fea-ture for sentiment polarity classification.
Ourmethod combining N-grams and RbF fea-ture outperforms most other published resultson the two benchmark datasets PL04 andIMDB11.4.4 Effects of RbF to AccuracyThis section is to give a detail analysis aboutthe effects of using RbF feature to accuracyresults of our approach (as shown in Figure2) using full combination of N-gram and RbFfeatures in which the RbF feature is predictedby regression models learned on the datasetSAR14 in varying number K of most frequentunigrams from 5000 to 40000.On the dataset PL04, the highest accuracyobtained by using only the RbF feature is88.90% at K?s value of 10000, which it isequal to that published by Maas et al.
(2011).In most cases of using N-gram and RbF fea-tures, we obtain state-of-the-art results whichare higher than 91%.On the IMDB11 dataset, at K?s value of5000, we achieve the lowest accuracy of89.29% by using N-gram and RbF features,which it is slightly higher than the accuracy of89.23% given by Dahl et al.
(2012).
In casesthat K?s value is higher than 10000, accura-cies using only RbF feature are around 89.1%,while using the full combination returns re-sults which are higher than 89.74%.132Figure 2: Effects of rating-based feature to our method?s performance.
The horizontal presentsthe number of unigram features selected for learning regression models.4.5 Results on SAR14As mentioned in section 4.1, our datasetSAR14 contains 233600 movie reviews.
Welabel a review as ?positive?
or ?negative?
ifthe review has a score ?
7 or ?
4 respec-tively.
Therefore, we create a very largedataset of 167378 positive reviews and 66222negative reviews.
Due to the large size of thedataset SAR14 and the training and classifi-cation time, we employed LIBLINEAR8(Fanet al., 2008) for this experiment under 10 foldcross validation scheme.
We kept 50000 N-8Using L2-regularized logistic regression andsetting tolerance of termination criterion to 0.01.http://www.csie.ntu.edu.tw/?cjlin/liblinear/grams over each polarity class in the trainingset for each cross-validation run.
Finally, weobtained an accuracy of 93.24% by using N-gram features.5 ConclusionIn this paper, we conducted an experimen-tal study on sentiment polarity classification.We firstly described our new rating-based fea-ture, in which the rating-based feature is es-timated based on a regression model learnedfrom our external independent dataset SAR14of 233600 movie reviews.
We then exam-ined the contribution of the rating-based fea-ture and N-grams in a machine learning-based133approach on two datasets PL04 and IMDB11.Specifically, we reach state-of-the-art accu-racies at 91.6% and 89.87% on the datasetPL04 and IMDB11 respectively.
Further-more, by analyzing the effects of rating-basedfeature to accuracy performance, we showthat the rating-based feature is very efficient tosentiment classification on polarity reviews.And adding bigram and trigram features alsoenhances accuracy performance.
Further-more, we get an accuracy of 93.24% on thedataset SAR14, and we also share this datasetfor further research in sentiment polarity anal-ysis task.AcknowledgmentThis work is partially supported by the Re-search Grant from Vietnam National Univer-sity, Hanoi No.
QG.14.04.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2010.
Sentiwordnet 3.0: An enhancedlexical resource for sentiment analysis and opinionmining.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), pages 2200?2204.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.George Dahl, Hugo Larochelle, and Ryan P. Adams.2012.
Training restricted boltzmann machines onword observations.
In Proceedings of the 29th Inter-national Conference on Machine Learning (ICML-12), pages 679?686.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A Library for Large Linear Classification.
Journalof Machine Learning Research, 9:1871?1874.Thorsten Joachims.
1999.
Making large-scale sup-port vector machine learning practical.
In BernhardSch?olkopf, Christopher J. C. Burges, and Alexan-der J. Smola, editors, Advances in Kernel Methods:Support Vector Machines, pages 169?184.Alistair Kennedy and Diana Inkpen.
2006.
Senti-ment Classification of Movie Reviews Using Con-textual Valence Shifters.
Computational Intelli-gence, 22(2):110?125.Edda Leopold and J?org Kindermann.
2002.
Text cat-egorization with support vector machines.
how torepresent texts in input space?
Mach.
Learn., 46(1-3):423?444.Bing Liu.
2010.
Sentiment analysis and subjectivity.In Handbook of Natural Language Processing, Sec-ond Edition, pages 1?38.Andrew L. Maas, Raymond E. Daly, Peter T. Pham,Dan Huang, Andrew Y. Ng, and Christopher Potts.2011.
Learning word vectors for sentiment analysis.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, Vol 1, pages 142?150.Justin Martineau and Tim Finin.
2009.
Delta tfidf: animproved feature space for sentiment analysis.
InProceedings of the Third Annual Conference on We-blogs and Social Media, pages 258?261.Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.2006.
Examining the role of linguistic knowledgesources in the automatic identification and classi-fication of reviews.
In Proceedings of the COL-ING/ACL onMain conference poster sessions, pages611?618.Dai Quoc Nguyen, Dat Quoc Nguyen, and Son BaoPham.
2013.
A Two-Stage Classifier for SentimentAnalysis.
In Proceedings of the 6th InternationalJoint Conference on Natural Language Processing,pages 897?901.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summa-rization based on minimum cuts.
In Proceedings ofthe 42nd Meeting of the Association for Computa-tional Linguistics (ACL?04), pages 271?278.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135, January.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theACL-02 conference on Empirical methods in naturallanguage processing - Volume 10, pages 79?86.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Comput.
Lin-guist., 37(2):267?307, June.Zhaopeng Tu, Yifan He, Jennifer Foster, Josef van Gen-abith, Qun Liu, and Shouxun Lin.
2012.
Identify-ing high-impact sub-structures for convolution ker-nels in document-level sentiment classification.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), ACL ?12, pages 338?343.134Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised clas-sification of reviews.
In Proceedings of the 40thAnnual Meeting on Association for ComputationalLinguistics, ACL ?02, pages 417?424.Sida Wang and Christopher D. Manning.
2012.
Base-lines and bigrams: simple, good sentiment and topicclassification.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), ACL ?12, pages90?94.Casey Whitelaw, Navendu Garg, and Shlomo Arga-mon.
2005.
Using appraisal groups for sentimentanalysis.
In Proceedings of the 14th ACM inter-national conference on Information and knowledgemanagement, CIKM ?05, pages 625?631.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT ?05, pages 347?354.135
