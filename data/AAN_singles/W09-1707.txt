Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 54?62,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUsing DEDICOM forCompletely Unsupervised Part-of-Speech TaggingPeter A. Chew, Brett W. BaderSandia National LaboratoriesP.
O.
Box 5800, MS 1012Albuquerque, NM 87185-1012, USA{pchew,bwbader}@sandia.govAlla RozovskayaDepartment of Computer ScienceUniversity of IllinoisUrbana, IL 61801, USArozovska@illinois.eduAbstractA standard and widespread approach topart-of-speech tagging is based on HiddenMarkov Models (HMMs).
An alternativeapproach, pioneered by Sch?tze (1993),induces parts of speech from scratch usingsingular value decomposition (SVD).
Weintroduce DEDICOM as an alternative toSVD for part-of-speech induction.DEDICOM retains the advantages ofSVD in that it is completely unsupervised:no prior knowledge is required to induceeither the tagset or the associations oftypes with tags.
However, unlike SVD, itis also fully compatible with the HMMframework, in that it can be used to esti-mate emission- and transition-probabilitymatrices which can then be used as theinput for an HMM.
We apply theDEDICOM method to the CONLL corpus(CONLL 2000) and compare the output ofDEDICOM to the part-of-speech tagsgiven in the corpus, and find that the cor-relation (almost 0.5) is quite high.
UsingDEDICOM, we also estimate part-of-speech ambiguity for each type, and findthat these estimates correlate highly withpart-of-speech ambiguity as measured inthe original corpus (around 0.88).
Finally,we show how the output of DEDICOMcan be evaluated and compared againstthe more familiar output of supervisedHMM-based tagging.1 IntroductionTraditionally, part-of-speech tagging has been ap-proached either in a rule-based fashion, or stochas-tically.
Harris (1962) was among the first todevelop algorithms of the former type.
The rule-based approach relies on two elements: a dictio-nary to assign possible parts of speech to eachword, and a list of hand-written rules ?
which mustbe painstakingly developed for each new languageor domain ?
to disambiguate tokens in context.Stochastic taggers, on the other hand, avoid theneed for hand-written rules by tabulating probabili-ties of types and part-of-speech tags (which mustbe gathered from a tagged training corpus), andapplying a special case of Bayesian inference(usually, Hidden Markov Models [HMMs]) to dis-ambiguate tokens in context.
The latter approachwas pioneered by Stolz et al (1965) and Bahl andMercer (1976), and became widely known throughthe work of e.g.
Church (1988) and DeRose(1988).A third and more recent approach, known as?distributional tagging?
and exemplified bySch?tze (1993, 1995) and Biemann (2006), aims toeliminate the need for both hand-written rules anda tagged training corpus, since the latter may notbe available for every language or domain.
Distri-butional tagging is fully-unsupervised, unlike thetwo traditional approaches described above.Sch?tze suggests analyzing the distributional pat-terns of words by forming a term adjacency matrix,then subjecting that matrix to Singular Value De-composition (SVD) to reveal latent dimensions.
Heshows that in the reduced-dimensional space im-plied by SVD, tokens do indeed cluster intuitivelyby part-of-speech; and that if context is taken intoaccount, something akin to part-of-speech tagging54can be achieved.
Whereas the performance of sto-chastic taggers is generally sub-optimal when thedomain of the training data differs from that of thetest data, distributional tagging sidesteps this prob-lem, since each corpus can be considered in itsown right.
Sch?tze (1995) notes two general draw-backs of distributional tagging methods: the per-formance is relatively modest compared to that ofsupervised methods; and languages with rich mor-phology may pose a challenge.1In this paper, we present an alternative unsuper-vised approach to distributional tagging.
Instead ofSVD, we use a dimensionality reduction techniqueknown as DEDICOM, which has various advan-tages over the SVD-based approach.
Principalamong these is that, even though no pre-taggedcorpus is required, DEDICOM can easily be usedas input to a HMM-based approach (and the twoshare linear-algebraic similarities, as we will makeclear in section 4).
Although our empirical results,like those of Sch?tze (1995), are perhaps still rela-tively modest, the fact that a clearer connectionexists between DEDICOM and HMMs than be-tween SVD and HMMs gives us good reason tobelieve that with further refinements, DEDICOMmay be able to give us ?the best of both worlds?
inmany respects: the benefits of avoiding the needfor a pre-tagged corpus, with empirical results ap-proaching those of HMM-based tagging.In the following sections, we introduceDEDICOM, describe its applicability to the part-of-speech tagging problem, and outline its connec-tions to the standard HMM-based approach to tag-ging.
We evaluate the use of DEDICOM on theCONLL 2000 shared task data, discuss the resultsand suggest avenues for improvement.2 DEDICOMDEDICOM, which stands for ?DEcomposition intoDIrectional COMponents?, is a linear-algebraicdecomposition method attributable to Harshman(1978) which has been used to analyze matrices of1 We note the latter is also true for languages in which wordorder is relatively free ?
usually the same languages as thosewith rich morphology.
While English word order is signifi-cantly constrained by part-of-speech categorizations, this isnot as true of, say, Russian.
Thus, an adjacency matrix formedfrom a Russian corpus is likely to be less informative aboutpart-of-speech classifications as one formed from an Englishcorpus.
Quite possibly, this is as much of a limitation forDEDICOM as it is for SVD.asymmetrical directional relationships betweenobjects or persons.
Early on, the technique wasapplied by Harshman et al (1982) to the analysisof two types of marketing data: ?free associations??
how often one phrase (describing hair shampoo)evokes another in the minds of survey respondents,and ?car switching data?
?
how often people switchfrom one to another of 16 car types.
Both datasetsare asymmetric and directional: in the first dataset,for example, the phrase ?body?
(referring to sham-poo) evoked the phrase ?fullness?
twice as often inthe minds of respondents as ?fullness?
evoked?body?.
Likewise, the data from Harshman et al(1982) show that in the given period, 3,820 peopleswitched from ?midsize import?
cars to ?midsizedomestic?
cars, but only 2,140 switches were madein the reverse direction.
Another characteristic ofthese ?asymmetric directional?
datasets is that theycan be represented in square matrices.
For exam-ple, the raw car switching data can be representedin a 16 ?
16 matrix, since there are 16 car types.The objective of DEDICOM, which can becompared to that of SVD, is to factorize the rawdata matrices into a lower-dimensional space iden-tifying underlying, idealized directional patterns inthe data.
For example, while there are 16 car typesin the raw car switching data, Harshman showsthat under a 4-dimensional DEDICOM analysis,these can be ?boiled down?
to the basic types ?plainlarge-midsize?, ?specialty?, ?fancy large?, and?small?
?
and that patterns of switching amongthese more basic types can then be identified.If X represents the original n ?
n matrix ofasymmetric relationships, and a general entry xij inX represents the strength of the directed relation-ship of object i to object j, then the single-domainDEDICOM model2 can be written as follows:X = ARAT + E (1)where A denotes an n ?
q matrix of weights of then observed objects in q dimensions (where q < n),and R is a dense q ?
q asymmetric matrix express-ing the directional relationships between the q di-mensions or basic types.
AT is simply the transpose2 There is a dual-domain DEDICOM model, which is alsodescribed in Harshman (1978).
The dual-domain DEDICOMmodel is not relevant to our discussion, and thus it will not bementioned further.
References in this paper to ?DEDICOM?are to be understood as references in shorthand to ?single-domain DEDICOM?.55of A, and E is a matrix of error terms.
Our objec-tive is to minimize E, so we can also write:X  ARAT (2)As noted by Harshman (1978: 209), the fact thatA appears on both the left and right of R meansthat the data is described ?in terms of asymmetricrelations among a single set of things?
?
in otherwords, when objects are on the receiving end of thedirectional relationships, they are still of the sametype as those on the initiating end.One difference between DEDICOM and SVD isthat there is no unique solution: either A or R canbe scaled or rotated without changing the goodnessof fit, so long as the inverse operation is applied tothe other.
For example, if we let ?
= AD, where Dis any diagonal scaling matrix (or, more generally,any nonsingular matrix), then we can writeX  ARAT = ?D-1RD-1?T (3)since ?T = (AD) T = DAT(In our application, we constrain A and R to benonnegative as noted below.
)To our knowledge, there have been no applica-tions of DEDICOM to date in computational lin-guistics.
This is in contrast to SVD, which hasbeen extensively used for text analysis (for appli-cations other than unsupervised part-of-speechtagging, see Baeza-Yates and Ribeiro-Neto 1999).3 Applicability of DEDICOM to part-of-speech taggingSch?tze?s (1993) key insight is that ?
at least inEnglish ?
adjacencies between types are a goodguide to their grammatical functions.
That insightcan be leveraged by applying either SVD orDEDICOM to a type-by-type adjacency matrix.With DEDICOM, however, we add the constraint(already stated) that the types are a ?single set ofthings?
: whether a type ?precedes?
or ?follows?
?i.e., whether it is in a row or a column of the ma-trix ?
does not affect its grammatical function.
Thisconstraint is as it should be, and, to our knowledge,sets DEDICOM apart from all previous unsuper-vised approaches including those of Sch?tze (1993,1995) and Biemann (2006).Given any corpus containing n types and k to-kens, we can let X be an n ?
n token-adjacencymatrix.
Let each entry xij in X denote the numberof times in the corpus that type i immediately pre-cedes type j. X is thus a matrix of bigram frequen-cies.
It follows that the sum of the elements of Xequals k ?
1 (because the first token in the corpusis preceded by nothing, and the last token is fol-lowed by nothing).
Any given row sum of X (thetype frequency corresponding to the particularrow) will equal the corresponding column sum,except if the type happens to occur in the first orlast position in the corpus.
X will be asymmetric,since the frequency of bigram ij is clearly not thesame as that of bigram ji for all i and j.It can be seen, therefore, that our X representsasymmetric directional data, very similar to thedata analyzed in Harshman (1978) and Harshmanet al (1982).
If we fit the DEDICOM model to ourX matrix, then we obtain an A matrix whichrepresents types by latent classes, and an R matrixwhich represents directional relationships betweenlatent classes.
We can think of the latent classes asinduced parts of speech.With SVD, we believe that the orthogonality ofthe reduced-dimensional features militates againstany attempt to correlate these features with parts ofspeech.
From a linguistic point of view, there is noreason to believe that parts of speech are orthogon-al to one another in any sense.
For example, nounsand adjectives (traditionally classified together as?nominals?)
seem to share more in common withone another than nouns and verbs.
WithDEDICOM, this is not an issue, because the col-umns of A are not required to be mutually ortho-gonal to one another, unlike the left and rightsingular vectors from SVD.Thus, the A matrix from DEDICOM shows howstrongly associated each type is with the differentinduced parts of speech; we would expect typeswhich are ambiguous (such as ?claims?, which canbe either a noun or a verb) to have high loadingson more than one column in A.
Again, if theclasses correlate with parts of speech, the R matrixwill show the latent patterns of adjacency betweendifferent parts of speech.4 Connections between DEDICOM andHMM-based taggingFor any HMM, two components are necessary: aset of emission probabilities and a set of transitionprobabilities.
Applying this framework to part-of-56speech tagging, the tags are conceived of as thehidden layer of the HMM and the tokens (each ofwhich is associated with a type) as the visiblelayer.
The emission probabilities are then the prob-abilities of types given the tags, and the transitionprobabilities are the probabilities of the tags giventhe preceding tags.
If these probabilities areknown, then there are algorithms (such as the Vi-terbi algorithm) to determine the most likely se-quence of tags given the visible sequence of types.In the case of supervised learning, we obtain theemission and transition probabilities by observingactual frequencies in a tagged corpus.
Suppose ourcorpus, as previously discussed, consists of n typesand k tokens.
Since we are dealing with supervisedlearning, the number of the tags in the tagset is alsoknown: we denote this number q.
Now, the ob-served frequencies can be represented, respective-ly, as n ?
q and q ?
q matrices: we denote these A*and R*.
Each entry aij in A* denotes the number oftimes type i is associated with tag j, and each entryrij in R* denotes the number of times tag j imme-diately follows tag i.
Moreover, we know someother properties of A* and R*: the respective sums of the elements of A* andR* are equal to k ?
1; each row sum of A* (=qxixa1) corresponds tothe frequency in the corpus of type i; each column sum of A*, as well as the corres-ponding row and column sums of R*, are thefrequencies of the given tags in the corpus (forall j, =====qxjxqxxjqxxj rra111).If A* and R* contain frequencies, however, wemust perform a matrix operation to obtain transi-tion and emission probabilities for use in anHMM-based tagger.
In effect, A* must be madecolumn-stochastic, and R* must be made row-stochastic.
Since the column sums of A* equal therespective row sums of R*, this can be achieved bypost-multiplying both A* and R* by DA, where DAis a diagonal scaling matrix containing the inversesof the column sums of A (or equivalently, the rowsums of R).
Then the matrix of emission probabili-ties is given by A*DA, and the matrix of transitionprobabilities by R*DA.We can now make the connection to DEDICOMexplicit.
Let A = A*DA and R = R*, then we canrewrite (2) as follows:X  ARAT = (A*DA) R* (A*DA)T (4)X  A*DA R*DA A*T (5)In other words, for any corpus we may computea probabilistic representation of the type adjacencymatrix X (which will contain expected frequenciescomparable to the actual frequencies) by multiply-ing the emission probability matrix A*DA, thetransition probability matrix R*DA, and the type-by-tag frequency matrix A*.
(Presumably, thecloser the approximation, the better the tagging inthe training set actually factorizes the true direc-tional relationships.
)Conversely, for fully unsupervised tagging, wecan fit the DEDICOM model to the type adjacencymatrix X.
The resulting A matrix contains esti-mates of what the tags should be (if a tagged train-ing corpus is unavailable), as well as the emissionprobability of each type given each tag, and theresulting R matrix is the corresponding transitionprobability matrix given those tags.
In this case, acolumn-stochastic A can be used directly as theemission probability matrix, and we simply makeR* row-stochastic to obtain the matrix of transitionprobabilities.
The only difference then between theoutput of the fully-unsupervised DEDICOM/HMMtagger and that of a supervised HMM tagger is thatin the first case, the ?tags?
are numeric indicesrepresenting the corresponding column of A, andin the second case, they are the members of thetagset used in the training data.The fact that emission and transition probabili-ties (or at least something very like them) are anatural by-product of DEDICOM sets DEDICOMapart from Sch?tze?s SVD-based approach, and isfor us a significant reason which recommends theuse of DEDICOM.5 EvaluationFor all evaluation described here, we used theCONLL 2000 shared task data (CONLL 2000).This English-language newswire corpus consists of19,440 types and 259,104 tokens (including punc-tuation marks as separate types/tokens).
Each to-ken is associated with a part-of-speech tag and achunk tag, although we did not use the chunk tags57in the work described here.
The tags are from a 44-item tagset.
The CONLL 2000 tags against whichwe measure our own results are in fact assigned bythe Brill tagger (Brill 1992), and while these maynot correlate perfectly with those that would havebeen assigned by a human linguist, we believe thatthe correlation is likely to be good enough to allowfor an informative evaluation of our method.Before discussing the evaluation of unsuper-vised DEDICOM, let us briefly reconsider the si-milarities of DEDICOM to the supervised HMMmodel in the light of actual data in the CONLLcorpus.
We stated in (5) that X  A*DAR*DAA*T.For the CONLL 2000 tagged data, A* is a 19,440?
44 matrix and R* is a 44 ?
44 matrix.
UsingA*DA and R*DA as emission- and transition-probability matrices within a standard HMM(where the entire CONLL 2000 corpus is treated asboth training and test data), we obtained a taggingaccuracy of 95.6%.
By multiplyingA*DAR*DAA*T, we expect to obtain a matrix ap-proximating X, the table of bigram frequencies.This is indeed what we found: it will be apparentfrom Table 1 that the top 10 expected bigram fre-quencies based on this matrix multiplication aregenerally quite close to actual frequencies.
Moreo-ver, the sum of the elements in A*DAR*DAA*T isequal to the sum of the elements in X, and if we letE be the matrix of error terms (X -A*DAR*DAA*T), then we find that ||E|| (the Frobe-nius norm of E) is 38.764% of ||X|| - in otherwords, A*DAR*DAA*T accounts for just over 60%of the data in X.Type 1 Type 2 ActualfrequencyExpectedfrequencyof the 1,421.000 1,202.606in the 1,213.000 875.822for the 553.000 457.067to the 445.000 415.524on the 439.000 271.528the company 383.000 105.794a share 371.000 32.447that the 315.000 258.679and the 302.000 296.737to be 285.000 499.315Table 1.
Actual versus expected frequencies for 10 mostcommon bigrams in CONLL 2000 corpusHaving confirmed that there exists an A(=A*DA) and R (=R*) which both satisfies theDEDICOM model and can be used directly withina HMM-based tagger to achieve satisfactory re-sults, we now consider whether A and R can beestimated if no tagged training set is available.We start, therefore, from X, the square 19,440 ?19,440 (sparse) matrix of raw bigram frequenciesfrom the CONLL 2000 data.
Using Matlab and theTensor Toolbox (Bader and Kolda 2006, 2007), wecomputed the best rank-44 non-negativeDEDICOM3 decomposition of this matrix usingthe 2-way version of the ASALSAN algorithmpresented in Bader et al (2007), which is based oniteratively improving random initial guesses for Aand R. As with SVD, the rank of the decomposi-tion can be selected by the user; we chose 44 sincethat was known to be the number of items in theCONLL 2000 tagset, but a lower number could beselected for a coarser-grained part-of-speech anal-ysis.
Ultimately, perhaps the best way to determinethe optimal rank would be to evaluate differentoptions within a larger end-to-end system, for ex-ample an information retrieval system; this, how-ever, was beyond our scope in this study.As already mentioned, there are indeterminaciesof rotation and scale in DEDICOM.
As Harshmanet al (1982: 211) point out, ?when the columns ofA are standardized?
the R matrix can then be in-terpreted as expressing relationships among thedimensions in the same units as the original data.That is, the R matrix can be interpreted as a ma-trix of the same kind as the original data matrix X,but describing the relations among the latent as-pects of the phrases, rather than the phrases them-selves?.
Thus, if DEDICOM is constrained so thatA is column-stochastic (which is required in anycase of the matrix of emission probabilities), thenthe sum of the elements in R should approximatethe sum of the elements in X. R is therefore com-parable to R* (with some provisos which shall beenumerated below), and to obtain the row-stochastic transition-probability matrix, we simplymultiply R by a diagonal matrix DR whose ele-ments are the inverses of R?s row sums.3 Non-negative DEDICOM imposes the constraint not presentin Harshman (1978, 1982) that all entries in A and R must benon-negative.
This constraint is appropriate in the presentcase, since the entries in A* and R* (and of course the proba-bilities in A*D and R*D) are by definition non-negative.58Table 2.
Partial confusion matrix of gold-standard tags against DEDICOM-induced tags for CONLL 2000 datasetWith A as an emission-probability matrix andRDR as a transition-probability matrix, we nowhave all that is needed for an HMM-based taggerto estimate the most likely sequence of ?tags?
giventhe corpus.
However, since the ?tags?
here are nu-merical indices, as mentioned, to evaluate the out-put we must look at the correlation between these?tags?
and the gold-standard tags given in theCONLL 2000 data.
One way this can be done is bypresenting a 44 ?
44 confusion matrix (of gold-standard tags against induced tags), and then mea-suring the correlation coefficient (Pearson?s R)between that matrix and the ?idealized?
confusionmatrix in which each induced tag corresponds toone and only one ?gold standard?
tag.
Using A andRDR as the input to a HMM-based tagger, wetagged the CONLL 2000 dataset with induced tagsand obtained the confusion matrix shown in Table2 (owing to space constraints, only the first 20 col-umns are shown).
The correlation between thismatrix and the equivalent diagonalized ?ideal?
ma-trix is in fact 0.4942, which is significantly higherthan could have occurred by chance.It should be noted that a lack of correlation be-tween the induced tags and the gold standard tagscan be attributed to at least two independent fac-tors.
The first, of course, is any inability of theDEDICOM model to fit the particular problem anddata.
Clearly, this is undesirable.
The other factorto be borne in mind, which works to DEDICOM?sfavor, is that the DEDICOM model could yield anA and R which factorize the data more optimallythan the A*D and R* implied by the gold-standardtags.
There are three methods we can use to try andtease apart these competing explanations of theresults, two quantitative and the other subjective.Quantitatively, we can compare the respective er-ror matrices E. We have already mentioned that38764.0||X||||ADRDAX|| T*A*A*  (6)Similarly, using the A and R from DEDICOM wecan compute24078.0||X||||ARAX|| T  (7)59The fact that the error is lower in the second caseimplies that DEDICOM allows us to find a part-of-speech ?factorization?
of the data which fits bettereven than the gold standard, although again thereare some caveats to this; we will return to these inthe discussion.Another way to evaluate the output ofDEDICOM is by comparing the number of part-of-speech tags for a type in the gold standard to thenumber of classes in the A matrix with which thetype is strongly associated.
We test this by measur-ing the Pearson correlation between the two va-riables.
First, we compute the average number ofpart-of-speech tags per type using the gold stan-dard.
We refer to this value as ambiguity coeffi-cient; for the CONLL dataset, this is 1.05.
BecauseA is dense, if we count all non-zero columns for atype in the A matrix as possible classes, we obtaina much higher ambiguity coefficient.
We thereforeset a threshold and consider only those columnswhose values exceed a certain threshold.
The thre-shold is selected so that the ambiguity coefficientof the A matrix is the same as that of the gold stan-dard.
For a given type, every column with a valueexceeding the threshold is counted as a possibleclass for that type.
We then compute the Pearsoncorrelation coefficient between the number ofclasses for a type in the A matrix and the numberof part-of speech tags for that type in the CONLLdataset as provided by the Brill tagger.
We ob-tained a correlation coefficient of 0.88, whichshows that there is indeed a high correlation be-tween the induced tags and the gold standard tagsobtained with DEDICOM.Finally, we can evaluate the output subjectivelyby looking at the content of the A matrix.
For each?tag?
(column) in A, the ?types?
(rows) can belisted in decreasing order of their weighting in A.This gives us an idea of which types are most cha-racteristic of which tags, and whether the groupinginto tags makes any intuitive sense.
These results(for selected tags only, owing to limitations ofspace) are given in Table 3.Many groupings in Table 3 do make sense: forexample, the fourth tag is clearly associated withverbs, while the two types with significant weight-ings for tag 2 are both determiners.
By referringback to Table 2, we can see that many tokens in theCONLL 2000 dataset tagged as verbs are indeedtagged by the DEDICOM tagger as ?tag 4?, whilemany determiners are tagged as ?tag 3?.
To under-stand where a lack of correlation may arise, how-ever, it is informative to look at apparentanomalies in the A matrix.
For example, it can beseen from Table 3 that ?new?, an adjective, isgrouped in the third tag with ?a?
and ?the?
(andranking above ?an?).
Although not in agreementwith the CONLL 2000 ?gold standard?
tagging, theidea that determiners are a type of adjective is infact in accordance with traditional English gram-mar.
Here, the grouping of ?new?, ?a?
and ?the?
canbe explained by the distributional similarities (allprecede nouns).
It should also be emphasized thatthe A matrix is essentially a ?soft clustering?
oftypes (meaning that types can belong to more thanone cluster).
Thus, for example, ?u.s.?
(the abbrevi-ation for United States) appears under both tag 2(which appears to have high loadings for nouns)and tag 8 (with high loadings for adjectives).We have alluded above in passing to possiblemethods for improving the results of theDEDICOM analysis.
One would be to pre-processthe data differently.
Here, a variety of options areavailable which maintain a generally unsupervisedapproach (one example is to avoid treating punctu-ation as tokens).
However, variations in pre-processing are beyond the scope of this paper.Tag Top 10 types (by weight) with weightings1 million share said .
year billion inc. corp. years quarter0.0246 0.0146 0.0129 0.0098 0.0088 0.0069 0.0064 0.0061 0.0058 0.00542 company u.s. new first market share year stock .
government0.0264 0.0136 0.0113 0.0095 0.0086 0.0086 0.0079 0.0077 0.0065 0.0063 the a new an other its any addition their 19880.2889 0.1194 0.0121 0.0094 0.0092 0.0085 0.0067 0.0062 0.0062 0.0057?8 the its his about those their all u.s. .
this0.0935 0.0462 0.0208 0.0160 0.0096 0.0095 0.0088 0.0077 0.0074 0.0071?Table 3.
Type weightings in A matrix, by tag60Another method would be to constrainDEDICOM so that the output more closely modelsthe characteristics of A* and R*, the emission- andtransition-probability matrices obtained from atagged training set.
In particular, there is one im-portant constraint on R* which is not replicated inR: the constraint mentioned above that for all j,===qxjxqxxj rr11.
We note that this constraint can besatisfied by Sinkhorn balancing (Sinkhorn 1964)4,although it remains to be seen how the constrainton R can best be incorporated into the DEDICOMarchitecture.
Assuming that A is column-stochastic, another desirable constraint is that therows of A(DR)-1 should sum to the same as therows of X (the respective type frequencies).
Withthe implementation of these (and any other) con-straints, one would expect the fit of DEDICOM tothe data to worsen (cf.
(6) and (7) above), but in-curring this cost could be worthwhile if the payoffwere somehow linguistically interesting (for ex-ample, if it turned out we could achieve a muchhigher correlation to gold-standard tagging).6 ConclusionIn this paper, we have introduced DEDICOM, ananalytical technique which to our knowledge hasnot previously been used in computational linguis-tics, and applied it to the problem of completelyunsupervised part-of-speech tagging.
Theoretical-ly, the model has features which recommend itover other previous approaches to unsupervisedtagging, specifically SVD.
Principal among theadvantages is the compatibility of DEDICOM withthe standard HMM-based approach to part-of-speech tagging, but another significant advantageis the fact that types are treated as ?a single set ofobjects?
regardless of whether they occupy the firstor second position in a bigram.By applying DEDICOM to a tagged dataset, wehave shown that there is a significant correlationbetween the tags induced by unsupervised,DEDICOM-based tagging, and the pre-existinggold-standard tags.
This points both to an inherentvalidity in the gold-standard tags (as a reasonable4 It is also worth noting that Sinkhorn was motivated by thesame problem which concerns us, that of estimating a transi-tion-probability matrix for a Markov model.factorization of the data) and to the fact thatDEDICOM appears promising as a method of in-ducing tags in cases where no gold standard isavailable.We have also shown that the factors ofDEDICOM are interesting in their own right: ourtests show that the A matrix (similar to an emis-sion-probability matrix) models type part-of-speech ambiguity well.
Using insights fromDEDICOM, we have also shown how linear alge-braic techniques may be used to estimate the fit ofa given part-of-speech factorization (whether in-duced or manually created) to a given dataset, bycomparing actual versus expected bigram frequen-cies.In summary, it appears that DEDICOM is apromising way forward for bridging the gap be-tween unsupervised and supervised approaches topart-of-speech tagging, and we are optimistic thatwith further refinements to DEDICOM (such asthe addition of appropriate constraints), more in-sight will be gained on how DEDICOM may mostprofitably be used to improve part-of-speech tag-ging when few pre-existing resources (such astagged corpora) are available.AcknowledgementsWe are grateful to Danny Dunlavy for contributinghis thoughts to this work.Sandia is a multiprogram laboratory operated bySandia Corporation, a Lockheed Martin Company,for the United States Department of Energy?s Na-tional Nuclear Security Administration under con-tract DE-AC04-94AL85000.61ReferencesBrett W. Bader, Richard A. Harshman, and Tamara G.Kolda.
2007.
Temporal analysis of semantic graphsusing ASALSAN.
In Proceedings of the 7th IEEE In-ternational Conference on Data Mining, 33-42.Brett W. Bader and Tamara G. Kolda.
2006.
EfficientMATLAB Computations with Sparse and FactoredTensors.
Technical Report SAND2006-7592, SandiaNational Laboratories, Albuquerque, NM and Liver-more, CA.Brett W. Bader and Tamara G. Kolda.
2007.
TheMATLAB Tensor Toolbox, version 2.2.http://csmr.ca.sandia.gov/~tgkolda/TensorToolbox/.Ricardo Baeza-Yates and Berthier Ribeiro-Neto.
1999.Modern Information Retrieval.
New York: ACMPress.L.
R. Bahl and R. L. Mercer.
1976.
Part of speech as-signment by a statistical decision algorithm.
In Pro-ceedings of the IEEE International Symposium onInformation Theory, 88-89.C.
Biemann.
2006.
Unsupervised part-of-speech taggingemploying efficient graph clustering.
In Proceedingsof the COLING/ACL 2006 Student Research Work-shop, 7-12.E.
Brill.
1992.
A simple rule-based part of speech tag-ger.
In Proceedings of the Third Conference on Ap-plied Natural Language Processing, 152-155.K.
W. Church.
1988.
A stochastic parts program andnoun phrase parser for unrestricted text.
In ANLP1988, 136-143.CONLL 2000.
Shared task data.
Retrieved Dec. 1, 2008from http://www.cnts.ua.ac.be/conll2000/chunking/.S.
J. DeRose.
1988.
Grammatical category disambigua-tion by statistical optimization.
Computational Lin-guistics 14, 31-39.Harris, Z. S. 1962.
String Analysis of Sentence Struc-ture.
Mouton: The Hague.Richard Harshman.
1978.
Models for Analysis ofAsymmetrical Relationships Among N Objects orStimuli.
Paper presented at the First Joint Meeting ofthe Psychometric Society and The Society for Ma-thematical Psychology.
Hamilton, Canada.Richard Harshman, Paul Green, Yoram Wind, and Mar-garet Lundy.
1982.
A Model for the Analysis ofAsymmetric Data in Marketing Research.
MarketingScience 1(2), 205-242.Hinrich Sch?tze.
1993.
Part-of-Speech Induction fromScratch.
In Proceedings of the 31st Annual Meeting ofthe Association for Computational Linguistics, 251-258.Hinrich Sch?tze.
1995.
Distributional Part-of-SpeechTagging.
In Proceedings of the 7th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, 141-148.Richard Sinkhorn.
1964.
A Relationship Between Arbi-trary Positive Matrices and Doubly Stochastic Ma-trices.
The Annals of Mathematical Statistics 35(2),876-879.W.
S. Stolz, P. H. Tannenbaum, and F. V. Carstensen.1965.
A stochastic approach to the grammatical cod-ing of English.
Communications of the ACM 8(6),399-405.62
