Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1117?1126,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUnsupervised Semantic Role Induction via Split-Merge ClusteringJoel Lang and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9AB, UKJ.Lang-3@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we describe an unsupervisedmethod for semantic role induction whichholds promise for relieving the data acqui-sition bottleneck associated with supervisedrole labelers.
We present an algorithm that it-eratively splits and merges clusters represent-ing semantic roles, thereby leading from aninitial clustering to a final clustering of bet-ter quality.
The method is simple, surpris-ingly effective, and allows to integrate lin-guistic knowledge transparently.
By com-bining role induction with a rule-based com-ponent for argument identification we obtainan unsupervised end-to-end semantic role la-beling system.
Evaluation on the CoNLL2008 benchmark dataset demonstrates thatour method outperforms competitive unsuper-vised approaches by a wide margin.1 IntroductionRecent years have seen increased interest in the shal-low semantic analysis of natural language text.
Theterm is most commonly used to describe the au-tomatic identification and labeling of the seman-tic roles conveyed by sentential constituents (Gildeaand Jurafsky, 2002).
Semantic roles describe the re-lations that hold between a predicate and its argu-ments, abstracting over surface syntactic configura-tions.
In the example sentences below.
window oc-cupies different syntactic positions ?
it is the objectof broke in sentences (1a,b), and the subject in (1c)?
while bearing the same semantic role, i.e., thephysical object affected by the breaking event.
Anal-ogously, rock is the instrument of break both whenrealized as a prepositional phrase in (1a) and as asubject in (1b).
(1) a.
[Joe]A0 broke the [window]A1 with a[rock]A2.b.
The [rock]A2 broke the [window]A1.c.
The [window]A1 broke.The semantic roles in the examples are labeledin the style of PropBank (Palmer et al, 2005), abroad-coverage human-annotated corpus of seman-tic roles and their syntactic realizations.
Under thePropBank annotation framework (which we will as-sume throughout this paper) each predicate is as-sociated with a set of core roles (named A0, A1,A2, and so on) whose interpretations are specific tothat predicate1 and a set of adjunct roles (e.g., loca-tion or time) whose interpretation is common acrosspredicates.
This type of semantic analysis is admit-tedly shallow but relatively straightforward to auto-mate and useful for the development of broad cov-erage, domain-independent language understandingsystems.
Indeed, the analysis produced by existingsemantic role labelers has been shown to benefit awide spectrum of applications ranging from infor-mation extraction (Surdeanu et al, 2003) and ques-tion answering (Shen and Lapata, 2007), to machinetranslation (Wu and Fung, 2009) and summarization(Melli et al, 2005).Since both argument identification and labelingcan be readily modeled as classification tasks, moststate-of-the-art systems to date conceptualize se-1More precisely, A0 and A1 have a common interpretationacross predicates as proto-agent and proto-patient in the senseof Dowty (1991).1117mantic role labeling as a supervised learning prob-lem.
Current approaches have high performance ?a system will recall around 81% of the argumentscorrectly and 95% of those will be assigned a cor-rect semantic role (see Ma`rquez et al (2008) fordetails), however only on languages and domainsfor which large amounts of role-annotated trainingdata are available.
For instance, systems trained onPropBank demonstrate a marked decrease in per-formance (approximately by 10%) when tested onout-of-domain data (Pradhan et al, 2008).Unfortunately, the reliance on role-annotated datawhich is expensive and time-consuming to producefor every language and domain, presents a majorbottleneck to the widespread application of semanticrole labeling.
Given the data requirements for super-vised systems and the current paucity of such data,unsupervised methods offer a promising alternative.They require no human effort for training thus lead-ing to significant savings in time and resources re-quired for annotating text.
And their output can beused in different ways, e.g., as a semantic prepro-cessing step for applications that require broad cov-erage understanding or as training material for su-pervised algorithms.In this paper we present a simple approach to un-supervised semantic role labeling.
Following com-mon practice, our system proceeds in two stages.It first identifies the semantic arguments of a pred-icate and then assigns semantic roles to them.
Bothstages operate over syntactically analyzed sentenceswithout access to any data annotated with semanticroles.
Argument identification is carried out througha small set of linguistically-motivated rules, whereasrole induction is treated as a clustering problem.
Inthis setting, the goal is to assign argument instancesto clusters such that each cluster contains argumentscorresponding to a specific semantic role and eachrole corresponds to exactly one cluster.
We formu-late a clustering algorithm that executes a series ofsplit and merge operations in order to transduce aninitial clustering into a final clustering of better qual-ity.
Split operations leverage syntactic cues so as tocreate ?pure?
clusters that contain arguments of thesame role whereas merge operations bring togetherargument instances of a particular role located indifferent clusters.
We test the effectiveness of ourinduction method on the CoNLL 2008 benchmarkdataset and demonstrate improvements over compet-itive unsupervised methods by a wide margin.2 Related WorkAs mentioned earlier, much previous work hasfocused on building supervised SRL systems(Ma`rquez et al, 2008).
A few semi-supervised ap-proaches have been developed within a frameworkknown as annotation projection.
The idea is to com-bine labeled and unlabeled data by projecting an-notations from a labeled source sentence onto anunlabeled target sentence within the same language(Fu?rstenau and Lapata, 2009) or across different lan-guages (Pado?
and Lapata, 2009).
Outwith annota-tion projection, Gordon and Swanson (2007) attemptto increase the coverage of PropBank by leveragingexisting labeled data.
Rather than annotating newsentences that contain previously unseen verbs, theyfind syntactically similar verbs and use their annota-tions as surrogate training data.Swier and Stevenson (2004) induce role labelswith a bootstrapping scheme where the set of la-beled instances is iteratively expanded using a clas-sifier trained on previously labeled instances.
Theirmethod is unsupervised in that it starts with a datasetcontaining no role annotations at all.
However, it re-quires significant human effort as it makes use ofVerbNet (Kipper et al, 2000) in order to identify thearguments of predicates and make initial role assign-ments.
VerbNet is a broad coverage lexicon orga-nized into verb classes each of which is explicitlyassociated with argument realization and semanticrole specifications.Abend et al (2009) propose an algorithm thatidentifies the arguments of predicates by relyingonly on part of speech annotations, without, how-ever, assigning semantic roles.
In contrast, Langand Lapata (2010) focus solely on the role inductionproblem which they formulate as the process of de-tecting alternations and finding a canonical syntacticform for them.
Verbal arguments are then assignedroles, according to their position in this canonicalform, since each position references a specific role.Their model extends the logistic classifier with hid-den variables and is trained in a manner that makesuse of the close relationship between syntactic func-tions and semantic roles.
Grenager and Manning1118(2006) propose a directed graphical model which re-lates a verb, its semantic roles, and their possiblesyntactic realizations.
Latent variables represent thesemantic roles of arguments and role induction cor-responds to inferring the state of these latent vari-ables.Our own work also follows the unsupervisedlearning paradigm.
We formulate the induction ofsemantic roles as a clustering problem and propose asplit-merge algorithm which iteratively manipulatesclusters representing semantic roles.
The motiva-tion behind our approach was to design a concep-tually simple system, that allows for the incorpo-ration of linguistic knowledge in a straightforwardand transparent manner.
For example, argumentsoccurring in similar syntactic positions are likely tobear the same semantic role and should thereforebe grouped together.
Analogously, arguments thatare lexically similar are likely to represent the samesemantic role.
We operationalize these notions us-ing a scoring function that quantifies the compatibil-ity between arbitrary cluster pairs.
Like Lang andLapata (2010) and Grenager and Manning (2006)our method operates over syntactically parsed sen-tences, without, however, making use of any infor-mation pertaining to semantic roles (e.g., in form ofa lexical resource or manually annotated data).
Per-forming role-semantic analysis without a treebank-trained parser is an interesting research direction,however, we leave this to future work.3 Learning SettingWe follow the general architecture of supervised se-mantic role labeling systems.
Given a sentence anda designated verb, the SRL task consists of identify-ing the arguments of the verbal predicate (argumentidentification) and labeling them with semantic roles(role induction).In our case neither argument identification norrole induction relies on role-annotated data or othersemantic resources although we assume that the in-put sentences are syntactically analyzed.
Our ap-proach is not tied to a specific syntactic representa-tion ?
both constituent- and dependency-based rep-resentations could be used.
However, we opted for adependency-based representation, as it simplifies ar-gument identification considerably and is consistentwith the CoNLL 2008 benchmark dataset used forevaluation in our experiments.Given a dependency parse of a sentence, our sys-tem identifies argument instances and assigns themto clusters.
Thereafter, argument instances can belabeled with an identifier corresponding to the clus-ter they have been assigned to, similar to PropBankcore labels (e.g., A0, A1).4 Argument IdentificationIn the supervised setting, a classifier is employedin order to decide for each node in the parse treewhether it represents a semantic argument or not.Nodes classified as arguments are then assigned a se-mantic role.
In the unsupervised setting, we slightlyreformulate argument identification as the task ofdiscarding as many non-semantic arguments as pos-sible.
This means that the argument identificationcomponent does not make a final positive decisionfor any of the argument candidates; instead, this de-cision is deferred to role induction.
The rules givenin Table 1 are used to discard or select argument can-didates.
They primarily take into account the parts ofspeech and the syntactic relations encountered whentraversing the dependency tree from predicate to ar-gument.
For each candidate, the first matching ruleis applied.We will exemplify how the argument identifica-tion component works for the predicate expect in thesentence ?The company said it expects its sales toremain steady?
whose parse tree is shown in Fig-ure 1.
Initially, all words save the predicate itselfare treated as argument candidates.
Then, the rulesfrom Table 1 are applied as follows.
Firstly, wordsthe and to are discarded based on their part of speech(rule (1)); then, remain is discarded because the pathends with the relation IM and said is discarded asthe path ends with an upward-leading OBJ relation(rule (2)).
Rule (3) does not match and is thereforenot applied.
Next, steady is discarded because thereis a downward-leading OPRD relation along the pathand the words company and its are discarded be-cause of the OBJ relations along the path (rule (4)).Rule (5) does not apply but words it and sales arekept as likely arguments (rule (6)).
Finally, rule (7)does not apply, because there are no candidates left.11191.
Discard a candidate if it is a determiner, in-finitival marker, coordinating conjunction, orpunctuation.2.
Discard a candidate if the path of relationsfrom predicate to candidate ends with coordi-nation, subordination, etc.
(see the Appendixfor the full list of relations).3.
Keep a candidate if it is the closest subject(governed by the subject-relation) to the leftof a predicate and the relations from predi-cate p to the governor g of the candidate areall upward-leading (directed as g?
p).4.
Discard a candidate if the path between thepredicate and the candidate, excluding the lastrelation, contains a subject relation, adjectivalmodifier relation, etc.
(see the Appendix forthe full list of relations).5.
Discard a candidate if it is an auxiliary verb.6.
Keep a candidate if the predicate is its parent.7.
Keep a candidate if the path from predicateto candidate leads along several verbal nodes(verb chain) and ends with arbitrary relation.8.
Discard all remaining candidates.Table 1: Argument identification rules.5 Split-Merge Role InductionWe treat role induction as a clustering problem withthe goal of assigning argument instances (i.e., spe-cific arguments occurring in an input sentence) toclusters such that these represent semantic roles.
Inaccordance with PropBank, we induce a separate setof clusters for each verb and each cluster thus repre-sents a verb-specific role.Our algorithm works by iteratively splitting andmerging clusters of argument instances in order toarrive at increasingly accurate representations of se-mantic roles.
Although splits and merges could bearbitrarily interleaved, our algorithm executes a sin-gle split operation (split phase), followed by a se-ries of merges (merge phase).
The split phase par-titions the seed cluster containing all argument in-stances of a particular verb into more fine-grained(sub-)clusters.
This initial split results in a clusteringwith high purity but low collocation, i.e., argumentinstances in each cluster tend to belong to the samerole but argument instances of a particular role areFigure 1: A sample dependency parse with depen-dency labels SBJ (subject), OBJ (object), NMOD(nominal modifier), OPRD (object predicative com-plement), PRD (predicative complement), and IM(infinitive marker).
See Surdeanu et al (2008) formore details on this variant of dependency syntax.located in many clusters.
The degree of dislocationis reduced in the consecutive merge phase, in whichclusters that are likely to represent the same role aremerged.5.1 Split PhaseInitially, all arguments of a particular verb are placedin a single cluster.
The goal then is to partition thiscluster in such a way that the split-off clusters havehigh purity, i.e., contain argument instances of thesame role.
Towards this end, we characterize eachargument instance by a key, formed by concatenat-ing the following syntactic cues:?
verb voice (active/passive);?
argument linear position relative to predicate(left/right);?
syntactic relation of argument to its governor;?
preposition used for argument realization.A cluster is allocated for each key and all argumentinstances with a matching key are assigned to thatcluster.
Since each cluster encodes fine-grained syn-tactic distinctions, we assume that arguments occur-ring in the same position are likely to bear the samesemantic role.
The assumption is largely supportedby our empirical results (see Section 7); the clustersemerging from the initial split phase have a purityof approximately 90%.
While the incorporation ofadditional cues (e.g., indicating the part of speechof the subject or transitivity) would result in evengreater purity, it would also create problematicallysmall clusters, thereby negatively affecting the suc-cessive merge phase.11205.2 Merge PhaseThe split phase creates clusters with high purity,however, argument instances of a particular role areoften scattered amongst many clusters resulting in acluster assignment with low collocation.
The goalof the merge phase is to improve collocation by ex-ecuting a series of merge steps.
At each step, pairsof clusters are considered for merging.
Each pair isscored by a function that reflects how likely the twoclusters are to contain arguments of the same roleand the best scoring pair is chosen for merging.
Inthe following, we will specify which pairs of clus-ters are considered (candidate search), how they arescored, and when the merge phase terminates.5.2.1 Candidate SearchIn principle, we could simply enumerate and scoreall possible cluster pairs at each iteration.
In practicehowever, such a procedure has a number of draw-backs.
Besides being inefficient, it requires a scoringfunction with comparable scores for arbitrary pairsof clusters.
For example, let a, b, c, and d denoteclusters.
Then, score(a,b) and score(c,d) must becomparable.
This is a stronger requirement than de-manding that only scores involving some commoncluster (e.g., score(a,b) and score(a,c)) be com-parable.
Moreover, it would be desirable to ex-clude pairings involving small clusters (i.e., withfew instances) as scores for these tend to be unre-liable.
Rather than considering all cluster pairings,we therefore select a specific cluster at each step andscore merges between this cluster and certain otherclusters.
If a sufficiently good merge is found, it isexecuted, otherwise the clustering does not change.In addition, we prioritize merges between large clus-ters and avoid merges between small clusters.Algorithm 1 implements our merging procedure.Each pass through the inner loop (lines 4?12) selectsa different cluster to consider at that step.
Then,merges between the selected cluster and all largerclusters are considered.
The highest-scoring mergeis executed, unless all merges are ruled out, i.e., havea score below the threshold ?.
After each comple-tion of the inner loop, the thresholds contained inthe scoring function (discussed below) are adjustedand this is repeated until some termination criterionis met (discussed in Section 5.2.3).Algorithm 1: Cluster merging procedure.
Oper-ation merge(Li,L j) merges cluster Li into clusterL j and removes Li from the list L.1 while not done do2 L?
a list of all clusters sorted by numberof instances in descending order3 i?
14 while i < length(L) do5 j?
arg max0?
j?<iscore(Li,L j?
)6 if score(Li,L j)?
?
then7 merge(Li,L j)8 end9 else10 i?
i+111 end12 end13 adjust thresholds14 end5.2.2 Scoring FunctionOur scoring function quantifies whether two clustersare likely to contain arguments of the same role andwas designed to reflect the following criteria:1. whether the arguments found in the two clus-ters are lexically similar;2. whether clause-level constraints are satisfied,specifically the constraint that all argumentsof a particular clause have different semanticroles, i.e., are assigned to different clusters;3. whether the arguments present in the two clus-ters have similar parts of speech.Qualitatively speaking, criteria (2) and (3) providenegative evidence in the sense that they can be usedto rule out incorrect merges but not to identify cor-rect ones.
For example, two clusters with drasticallydifferent parts of speech are unlikely to representthe same role.
However, the converse is not neces-sarily true as part of speech similarity does not im-ply role-semantic similarity.
Analogously, the factthat clause-level constraints are not met provides ev-idence against a merge, but the fact that these aresatisfied is not reliable evidence in favor of a merge.In contrast, lexical similarity implies that the clus-1121ters are likely to represent the same semantic role.It is reasonable to assume that due to selectional re-strictions, verbs will be associated with lexical unitsthat are semantically related and assume similar syn-tactic positions (e.g., eat prefers as an object ediblethings such as apple, biscuit, meat), thus bearing thesame semantic role.
Unavoidably, lexical similaritywill be more reliable for arguments with overt lex-ical content as opposed to pronouns, however thisshould not impact the scoring of sufficiently largeclusters.Each of the criteria mentioned above is quantifiedthrough a separate score and combined into an over-all similarity function, which scores two clusters cand c?
as follows:score(c,c?)
=????
?0 if pos(c,c?)
< ?,0 if cons(c,c?)
< ?,lex(c,c?)
otherwise.
(2)The particular form of this function is motivated bythe distinction between positive and negative evi-dence.
When the part-of-speech similarity (pos) isbelow a certain threshold ?
or when clause-levelconstraints (cons) are satisfied to a lesser extent thanthreshold ?, the score takes value zero and the mergeis ruled out.
If this is not the case, the lexical similar-ity score (lex) determines the magnitude of the over-all score.
In the remainder of this section we willexplain how the individual scores (pos, cons, andlex) are defined and then move on to discuss howthe thresholds ?
and ?
are adjusted.Lexical Similarity We measure lexical similar-ity between two clusters through cosine similarity.Specifically, each cluster is represented as a vec-tor whose components correspond to the occurrencefrequencies of the argument head words in the clus-ter.
The similarity on such vectors x and y is thenquantified as:lex(x,y) = cossim(x,y) =x?y?x??y?
(3)Clause-Level Constraints Arguments occurringin the same clause cannot bear the same role.
There-fore, clusters should not merge if the resulting clus-ter contains (many) arguments of the same clause.For two clusters c and c?
we assess how well theysatisfy this clause-level constraint by computing:cons(c,c?)
= 1?2?
viol(c,c?
)NC +NC?
(4)where viol(c,c?)
refers to the number of pairs of in-stances (d,d?)
?
c?
c?
for which d and d?
occur inthe same clause (each instance can participate in atmost one pair) and NC and NC?
are the number ofinstances in clusters c and c?, respectively.Part-of-speech Similarity Part-of-speech similar-ity is also measured through cosine-similarity (equa-tion (3)).
Clusters are again represented as vectors xand y whose components correspond to argumentpart-of-speech tags and values to their occurrencefrequency.5.2.3 Threshold Adaptation and TerminationAs mentioned earlier the thresholds ?
and ?
whichparametrize the scoring function are adjusted at eachiteration.
The idea is to start with a very restrictivesetting (high values) in which the negative evidencerules out merges more strictly, and then to graduallyrelax the requirement for a merge by lowering thethreshold values.
This procedure prioritizes reliablemerges over less reliable ones.More concretely, our threshold adaptation pro-cedure starts with ?
and ?
both set to value 0.95.Then ?
is lowered by 0.05 at each step, leaving ?unchanged.
When ?
becomes zero, ?
is loweredby 0.05 and ?
is reset to 0.95.
Then ?
is iterativelydecreased again until it becomes zero, after which ?is decreased by another 0.05.
This is repeated until ?becomes zero, at which point the algorithm termi-nates.
Note that the termination criterion is not tiedexplicitly to the number of clusters, which is there-fore determined automatically.6 Experimental SetupIn this section we describe how we assessed the per-formance of our system.
We discuss the dataseton which our experiments were carried out, explainhow our system?s output was evaluated and presentthe methods used for comparison with our approach.Data For evaluation purposes, the system?s out-put was compared against the CoNLL 2008 sharedtask dataset (Surdeanu et al, 2008) which provides1122Syntactic Function Lang and Lapata Split-MergePU CO F1 PU CO F1 PU CO F1auto/auto 72.9 73.9 73.4 73.2 76.0 74.6 81.9 71.2 76.2gold/auto 77.7 80.1 78.9 75.6 79.4 77.4 84.0 74.4 78.9auto/gold 77.0 71.0 73.9 77.9 74.4 76.2 86.5 69.8 77.3gold/gold 81.6 77.5 79.5 79.5 76.5 78.0 88.7 73.0 80.1Table 2: Clustering results with our split-merge algorithm, the unsupervised model proposed in Lang andLapata (2010) and a baseline that assigns arguments to clusters based on their syntactic function.PropBank-style gold standard annotations.
Thedataset was taken from the Wall Street Journal por-tion of the Penn Treebank corpus and converted intoa dependency format (Surdeanu et al, 2008).
Inaddition to gold standard dependency parses, thedataset alo contains automatic parses obtained fromthe MaltParser (Nivre et al, 2007).
Although thedataset provides annotations for verbal and nominalpredicate-argument constructions, we only consid-ered the former, following previous work on seman-tic role labeling (Ma`rquez et al, 2008).Evaluation Metrics For each verb, we determinethe extent to which argument instances in a clustershare the same gold standard role (purity) and theextent to which a particular gold standard role is as-signed to a single cluster (collocation).More formally, for each group of verb-specificclusters we measure the purity of the clusters as thepercentage of instances belonging to the majoritygold class in their respective cluster.
Let N denotethe total number of instances, G j the set of instancesbelonging to the j-th gold class and Ci the set of in-stances belonging to the i-th cluster.
Purity can thenbe written as:PU =1N ?imaxj|G j ?Ci| (5)Collocation is defined as follows.
For each gold role,we determine the cluster with the largest number ofinstances for that role (the role?s primary cluster)and then compute the percentage of instances thatbelong to the primary cluster for each gold role as:CO =1N ?jmaxi|G j ?Ci| (6)The per-verb scores are aggregated into an overallscore by averaging over all verbs.
We use the micro-average obtained by weighting the scores for indi-vidual verbs proportionately to the number of in-stances for that verb.Finally, we use the harmonic mean of purity andcollocation as a single measure of clustering quality:F1 =2?CO?PUCO+PU(7)Comparison Models We compared our split-merge algorithm against two competitive ap-proaches.
The first one assigns argument instancesto clusters according to their syntactic function(e.g., subject, object) as determined by a parser.
Thisbaseline has been previously used as point of com-parison by other unsupervised semantic role label-ing systems (Grenager and Manning, 2006; Langand Lapata, 2010) and shown difficult to outperform.Our implementation allocates up to N = 21 clus-ters2 for each verb, one for each of the 20 most fre-quent functions in the CoNLL dataset and a defaultcluster for all other functions.
The second compar-ison model is the one proposed in Lang and Lapata(2010) (see Section 2).
We used the same model set-tings (with 10 latent variables) and feature set pro-posed in that paper.
Our method?s only parameter isthe threshold ?
which we heuristically set to 0.1.
Onaverage our method induces 10 clusters per verb.7 ResultsOur results are summarized in Table 2.
We re-port cluster purity (PU), collocation (CO) and theirharmonic mean (F1) for the baseline (SyntacticFunction), Lang and Lapata?s (2010) model andour split-merge algorithm (Split-Merge) on four2This is the number of gold standard roles.1123Syntactic Function Split-MergeVerb Freq PU CO F1 PU CO F1say 15238 91.4 91.3 91.4 93.6 81.7 87.2make 4250 68.6 71.9 70.2 73.3 72.9 73.1go 2109 45.1 56.0 49.9 52.7 51.9 52.3increase 1392 59.7 68.4 63.7 68.8 71.4 70.1know 983 62.4 72.7 67.1 63.7 65.9 64.8tell 911 61.9 76.8 68.6 77.5 70.8 74.0consider 753 63.5 65.6 64.5 79.2 61.6 69.3acquire 704 75.9 79.7 77.7 80.1 76.6 78.3meet 574 76.7 76.0 76.3 88.0 69.7 77.8send 506 69.6 63.8 66.6 83.6 65.8 73.6open 482 63.1 73.4 67.9 77.6 62.2 69.1break 246 53.7 58.9 56.2 68.7 53.3 60.0Table 3: Clustering results for individual verbs withour split-merge algorithm and the syntactic functionbaseline.datasets.
These result from the combination of au-tomatic parses with automatically identified argu-ments (auto/auto), gold parses with automatic argu-ments (gold/auto), automatic parses with gold argu-ments (auto/gold) and gold parses with gold argu-ments (gold/gold).
Bold-face is used to highlight thebest performing system under each measure on eachdataset (e.g., auto/auto, gold/auto and so on).On all datasets, our method achieves the highestpurity and outperforms both comparison models bya wide margin which in turn leads to a considerableincrease in F1.
On the auto/auto dataset the split-merge algorithm results in 9% higher purity than thebaseline and increases F1 by 2.8%.
Lang and Lap-ata?s (2010) logistic classifier achieves higher collo-cation but lags behind our method on the other twomeasures.Not unexpectedly, we observe an increase in per-formance for all models when using gold standardparses.
On the gold/auto dataset, F1 increasesby 2.7% for the split-merge algorithm, 2.7% for thelogistic classifier, and 5.5% for the syntactic func-tion baseline.
Split-Merge maintains the highest pu-rity and levels the baseline in terms of F1.
Perfor-mance also increases if gold standard arguments areused instead of automatically identified arguments.Consequently, each model attains its best scores onthe gold/gold dataset.We also assessed the argument identification com-Syntactic Function Split-MergeRole PU CO F1 PU CO F1A0 74.5 87.0 80.3 79.0 88.7 83.6A1 82.3 72.0 76.8 87.1 73.0 79.4A2 65.0 67.3 66.1 82.8 66.2 73.6A3 48.7 76.7 59.6 79.6 76.3 77.9ADV 37.2 77.3 50.2 78.8 37.3 50.6CAU 81.8 74.4 77.9 84.8 67.2 75.0DIR 62.7 67.9 65.2 71.0 50.7 59.1EXT 51.4 87.4 64.7 90.4 87.2 88.8LOC 71.5 74.6 73.0 82.6 56.7 67.3MNR 62.6 58.8 60.6 81.5 44.1 57.2TMP 80.5 74.0 77.1 80.1 38.7 52.2MOD 68.2 44.4 53.8 90.4 89.6 90.0NEG 38.2 98.5 55.0 49.6 98.8 66.1DIS 42.5 87.5 57.2 62.2 75.4 68.2Table 4: Clustering results for individual semanticroles with our split-merge algorithm and the syntac-tic function baseline.ponent on its own (settings auto/auto and gold/auto).It obtained a precision of 88.1% (percentage of se-mantic arguments out of those identified) and recallof 87.9% (percentage of identified arguments out ofall gold arguments).
However, note that these fig-ures are not strictly comparable to those reportedfor supervised systems, due to the fact that our ar-gument identification component only discards non-argument candidates.Tables 3 and 4 shows how performance variesacross verbs and roles, respectively.
We compare thesyntactic function baseline and the split-merge sys-tem on the auto/auto dataset.
Table 3 presents resultsfor 12 verbs which we selected so as to exhibit var-ied occurrence frequencies and alternation patterns.As can be seen, the macroscopic result ?
increasein F1 (shown in bold face) and purity ?
also holdsacross verbs.
Some caution is needed in interpret-ing the results in Table 43 since core roles A0?A3are defined on a per-verb basis and do not necessar-ily have a uniform corpus-wide interpretation.
Thus,conflating scores across verbs is only meaningful tothe extent that these labels actually signify the same3Results are shown for four core roles (A0?A3) and all sub-types of the ArgM role, i.e., adjuncts denoting general purpose(ADV), cause (CAU), direction (DIR), extent (EXT), location(LOC), manner (MNR), and time (TMP), modal verbs (MOD),negative markers (NEG), and discourse connectives (DIS).1124role (which is mostly true for A0 and A1).
Further-more, the purity scores given here represent the av-erage purity of those clusters for which the specifiedrole is the majority role.
We observe that for mostroles shown in Table 4 the split-merge algorithm im-proves upon the baseline with regard to F1, whereasthis is uniformly the case for purity.What are the practical implications of these re-sults, especially when considering the collocation-purity tradeoff?
If we were to annotate the clus-ters induced by our system, low collocation wouldresult in higher annotation effort while low puritywould result in poorer data quality.
Our system im-proves purity substantially over the baselines, with-out affecting collocation in a way that would mas-sively increase the annotation effort.
As an exam-ple, consider how our system could support humansin labeling an unannotated corpus.
(The followingnumbers are derived from the CoNLL dataset4 in theauto/auto setting.)
We might decide to annotate allinduced clusters with more than 10 instances.
Thismeans we would assign labels to 74% of instances inthe dataset (excluding those discarded during argu-ment identification) and attain a role classificationwith 79.4% precision (purity).5 However, insteadof labeling all 165,662 instances contained in theseclusters individually we would only have to assignlabels to 2,869 clusters.
Since annotating a clustertakes roughly the same time as annotating a singleinstance, the annotation effort is reduced by a factorof about 50.8 ConclusionsIn this paper we presented a novel approach to un-supervised role induction which we formulated as aclustering problem.
We proposed a split-merge al-gorithm that iteratively manipulates clusters repre-senting semantic roles whilst trading off cluster pu-rity with collocation.
The split phase creates ?pure?clusters that contain arguments of the same rolewhereas the merge phase attempts to increase col-location by merging clusters which are likely to rep-resent the same role.
The approach is simple, intu-4Of course, it makes no sense to label this dataset as it isalready labeled.5Purity here is slightly lower than the score reported in Ta-ble 2 (auto/auto setting), because it is computed over a differentnumber of clusters (only those with at least 10 instances).itive and requires no manual effort for training.
Cou-pled with a rule-based component for automaticallyidentifying argument candidates our split-merge al-gorithm forms an end-to-end system that is capableof inducing role labels without any supervision.Our approach holds promise for reducing the dataacquisition bottleneck for supervised systems.
Itcould be usefully employed in two ways: (a) to cre-ate preliminary annotations, thus supporting the ?an-notate automatically, correct manually?
methodol-ogy used for example to provide high volume anno-tation in the Penn Treebank project; and (b) in com-bination with supervised methods, e.g., by providinguseful out-of-domain data for training.
An importantdirection for future work lies in investigating howthe approach generalizes across languages as well asreducing our system?s reliance on a treebank-trainedparser.Acknowledgments We are grateful to CharlesSutton for his valuable feedback on this work.
Theauthors acknowledge the support of EPSRC (grantGR/T04540/01).AppendixThe relations in Rule (2) from Table 1 are IM?
?,PRT?, COORD?
?, P?
?, OBJ?, PMOD?, ADV?,SUB?
?, ROOT?, TMP?, SBJ?, OPRD?.
The sym-bols ?
and ?
denote the direction of the dependencyarc (upward and downward, respectively).The relations in Rule (3) are ADV?
?, AMOD??,APPO?
?, BNF?
?-, CONJ?
?, COORD?
?, DIR??,DTV?
?-, EXT?
?, EXTR?
?, HMOD?
?, IOBJ??,LGS?
?, LOC?
?, MNR?
?, NMOD?
?, OBJ??,OPRD?
?, POSTHON?
?, PRD?
?, PRN?
?, PRP??,PRT?
?, PUT?
?, SBJ?
?, SUB?
?, SUFFIX??.
De-pendency labels are abbreviated here.
A detaileddescription is given in Surdeanu et al (2008), intheir Table 4.ReferencesO.
Abend, R. Reichart, and A. Rappoport.
2009.
Un-supervised Argument Identification for Semantic RoleLabeling.
In Proceedings of the 47th Annual Meet-ing of the Association for Computational Linguisticsand the 4th International Joint Conference on NaturalLanguage Processing of the Asian Federation of Natu-ral Language Processing, pages 28?36, Singapore.1125D.
Dowty.
1991.
Thematic Proto Roles and ArgumentSelection.
Language, 67(3):547?619.H.
Fu?rstenau and M. Lapata.
2009.
Graph Aligmentfor Semi-Supervised Semantic Role Labeling.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 11?20, Singa-pore.D.
Gildea and D. Jurafsky.
2002.
Automatic Label-ing of Semantic Roles.
Computational Linguistics,28(3):245?288.A.
Gordon and R. Swanson.
2007.
Generalizing Se-mantic Role Annotations Across Syntactically SimilarVerbs.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics, pages192?199, Prague, Czech Republic.T.
Grenager and C. Manning.
2006.
Unsupervised Dis-covery of a Statistical Verb Lexicon.
In Proceedingsof the Conference on Empirical Methods on NaturalLanguage Processing, pages 1?8, Sydney, Australia.K.
Kipper, H. T. Dang, and M. Palmer.
2000.
Class-Based Construction of a Verb Lexicon.
In Proceedingsof the 17th AAAI Conference on Artificial Intelligence,pages 691?696.
AAAI Press / The MIT Press.J.
Lang and M. Lapata.
2010.
Unsupervised Inductionof Semantic Roles.
In Proceedings of the 11th AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 939?947, Los Angeles, California.L.
Ma`rquez, X. Carreras, K. Litkowski, and S. Stevenson.2008.
Semantic Role Labeling: an Introduction to theSpecial Issue.
Computational Linguistics, 34(2):145?159, June.G.
Melli, Y. Wang, Y. Liu, M. M. Kashani, Z. Shi,B.
Gu, A. Sarkar, and F. Popowich.
2005.
Descriptionof SQUASH, the SFU Question Answering SummaryHandler for the DUC-2005 Summarization Task.
InProceedings of the Human Language Technology Con-ference and the Conference on Empirical Methods inNatural Language Processing Document Understand-ing Workshop, Vancouver, Canada.J.
Nivre, J.
Hall, J. Nilsson, G. Eryigit A. Chanev,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: A Language-independent System for Data-driven Dependency Parsing.
Natural Language Engi-neering, 13(2):95?135.S.
Pado?
and M. Lapata.
2009.
Cross-lingual AnnotationProjection of Semantic Roles.
Journal of Artificial In-telligence Research, 36:307?340.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of SemanticRoles.
Computational Linguistics, 31(1):71?106.S.
Pradhan, W. Ward, and J. Martin.
2008.
Towards Ro-bust Semantic Role Labeling.
Computational Linguis-tics, 34(2):289?310.D.
Shen and M. Lapata.
2007.
Using Semantic Rolesto Improve Question Answering.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing and the Conference on Com-putational Natural Language Learning, pages 12?21,Prague, Czech Republic.M.
Surdeanu, S. Harabagiu, J. Williams, and P. Aarseth.2003.
Using Predicate-Argument Structures for Infor-mation Extraction.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 8?15, Sapporo, Japan.M.
Surdeanu, R. Johansson, A. Meyers, and L. Ma`rquez.2008.
The CoNLL-2008 Shared Task on Joint Parsingof Syntactic and Semantic Dependencies.
In Proceed-ings of the 12th CoNLL, pages 159?177, Manchester,England.R.
Swier and S. Stevenson.
2004.
Unsupervised Seman-tic Role Labelling.
In Proceedings of the Conferenceon Empirical Methods on Natural Language Process-ing, pages 95?102, Barcelona, Spain.D.
Wu and P. Fung.
2009.
Semantic Roles for SMT:A Hybrid Two-Pass Model.
In Proceedings of NorthAmerican Annual Meeting of the Association for Com-putational Linguistics HLT 2009: Short Papers, pages13?16, Boulder, Colorado.1126
