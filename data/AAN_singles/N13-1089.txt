Proceedings of NAACL-HLT 2013, pages 739?745,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsImproving Lexical Semantics for Sentential Semantics: Modeling SelectionalPreference and Similar Words in a Latent Variable ModelWeiwei GuoDepartment of Computer ScienceColumbia Universityweiwei@cs.columbia.eduMona DiabDepartment of Computer ScienceGeorge Washington Universitymtdiab@gwu.eduAbstractSentence Similarity [SS] computes a similar-ity score between two sentences.
The SS taskdiffers from document level semantics tasksin that it features the sparsity of words in adata unit, i.e.
a sentence.
Accordingly it iscrucial to robustly model each word in a sen-tence to capture the complete semantic pictureof the sentence.
In this paper, we hypoth-esize that by better modeling lexical seman-tics we can obtain better sentential semantics.We incorporate both corpus-based (selectionalpreference information) and knowledge-based(similar words extracted in a dictionary) lex-ical semantics into a latent variable model.The experiments show state-of-the-art perfor-mance among unsupervised systems on twoSS datasets.1 IntroductionSentence Similarity [SS] is emerging as a crucialstep in many NLP tasks that focus on sentence levelsemantics such as word sense disambiguation (Guoand Diab, 2010; Guo and Diab, 2012a), summariza-tion (Zhou et al 2006), text coherence (Lapata andBarzilay, 2005), tweet clustering (Sankaranarayananet al 2009; Jin et al 2011), etc.
SS operates in avery small context, on average 11 words per sen-tence in Semeval-2012 dataset (Agirre et al 2012),resulting in inadequate evidence to generalize to ro-bust sentential semantics.Weighted Textual Matrix Factorization [WTMF](Guo and Diab, 2012b) is a latent variable model thatoutperforms Latent Semantic Analysis [LSA] (Deer-wester et al 1990) and Latent Dirichelet Allocation[LDA] (Blei et al 2003) models by a large margin inthe SS task, yielding state-of-the-art performance onthe LI06 (Li et al 2006) SS dataset.
However, all ofthese models make harsh simplifying assumptionson how a token is generated: (1) in LSA/WTMF, atoken is generated by the inner product of the wordlatent vector and the document latent vector; (2) inLDA, all the tokens in a document are sampled fromthe same document level topic distribution.
Underthis framework, they ignore rich linguistic phenom-ena such as inter-word dependency, semantic scopeof words, etc.
This is a result of simply using docu-ment IDs as features to represent a word.Modeling quality lexical semantics in latent vari-able models does not draw enough attention in thecommunity, since people usually apply dimensionreduction techniques for documents, which haveabundant words for extracting the document levelsemantics.
However, in the SS setting, it is crucial tomake good use of each word, given the limited num-ber of words in a sentence.
We believe a reasonableword generation story will avoid introducing noisein sentential semantics, encouraging robust lexicalsemantics which can further boost the sentential se-mantics.
In this paper, we explicitly encode lexicalsemantics, both corpus-based and knowledge-basedinformation, in the WTMF model, by which we areable to achieve even better results in SS task.The additional corpus-based information we ex-ploit is selectional preference semantics (Resnik,1997), a feature already existing in the data yet ig-nored by most latent variable models.
Selectionalpreference focuses on the admissible arguments fora word, thus capturing more nuanced semantics thanthe sentence IDs (when applied to a corpus of sen-tences as opposed to documents).
Consider the fol-lowing example:739Figure 1: matrix factorizationMany analysts say the global Brent crude oil bench-mark price, currently around $111 a barrel ...In WTMF/LSA/LDA, a word will receive semanticsfrom all the other words in a sentence, hence, theword oil, in the above example, will be assigned theincorrect finance topic that reflects the sentence levelsemantics.
Moreover, the problem worsens for ad-jectives, adverbs and verbs, which have a much nar-rower semantic scope than the whole sentence.
Forexample, the verb say should only be associated withanalyst (only receiving semantics from analyst), asit is not related to other words in the sentence.
Incontrast, oil, according to its selectional preference,should be associated with crude indicating the re-source topic.
We believe modeling selectional pref-erence capturing local evidence completes the se-mantic picture for words, hence further renderingbetter sentential semantics.
To our best knowledge,this is the first work to model selectional preferencefor sentence/document semantics.We also integrate knowledge-based semanticsin the WTMF framework.
Knowledge-based se-mantics, a human-annotated clean resource, is animportant complement to corpus-based noisy co-occurrence information.
We extract similar wordpairs from Wordnet (Fellbaum, 1998).
Leveragingthese pairs, an infrequent word such as purchasecan exploit robust latent vectors from its synonymssuch as buy.
Similar words pairs can be seamlesslymodeled in WTMF, since in the matrix factorizationframework a latent vector profile is explicitly createdfor each word, while in LDA all the data structuresare designed for documents/sentences.
We constructa graph to connect words according to the extractedsimilar word pairs, to encourage similar words toshare similar latent vector profiles.
We will refer toour proposed novel model as WTMF+PK.2 Weighted Textual Matrix FactorizationOur previous work (Guo and Diab, 2012b) modelsthe sentences in the weighted matrix factorizationframework (Figure 1).
The corpus is stored in anM ?N matrix X , with each cell containing the TF-IDF values of words.
The rows of X are M distinctwords and columns are N sentences.
As in Figure1, X is approximated by the product of a K ?Mmatrix P and a K?N matrix Q.
Accordingly, eachsentence sj is represented by a K dimensional la-tent vector Q?,j .
Similarly a word wi is generalizedby P?,i.
P and Q is optimized by minimize the ob-jective function:?i?jWij (P?,i ?Q?,j ?Xij)2 + ?||P ||22 + ?||Q||22Wi,j ={1, if Xij 6= 0wm, if Xij = 0(1)where ?
is a regularization term.
Missing tokens aremodeled by assigning a different weightwm for each0 cell in the matrix X .
We can see the inner productof a word vector P?,i and a sentence vector Q?,j isused to approximate the cell Xij .The graphical model of WTMF is illustrated inFigure 2a.
A wi/sj node is a latent vector P?,i/Q?,j ,corresponding to a word/sentence, respectively.
Ashaded node is a non-zero cell in X , representingan observed token in a sentence.
For simplicity, themissing tokens and weights are not shown in thegraph.3 Corpus-based Semantics: SelectionalPreferenceIn this paper, we focus on selectional preference thatreflects the association of two words: if two wordsform a bigram, then the two words should sharesimilar latent dimensions.
In the previous example,crude and oil form a bigram, and they share the re-source topic.
In our framework, this is implementedby adding extra columns in X , so that each addi-tional column corresponds to a bigram, treating eachbigram as a pseudo-sentence for the two words.
Thegraphical model is illustrated in Figure 2b.
There-fore, oil will receive more resource topic from crudethrough the bigram crude oil, instead of only financetopic from the sentence as a whole.Each non-zero cell in the new columns of X , i.e.an observed token in a bigram (pseudo-sentence), isgiven a different weight:Wi,j =??
?1, if Xij 6= 0 and j is a sentence index?
?
freq(j), if Xij 6= 0 and j is a bigram indexwm, if Xij = 0740w1?
w2?
w3?
w4?
w5?s1?
s2?b2?(c)?b1?w1?
w2?
w3?
w4?
w5?s1?
s2?(a)?
(b)?w1?
w2?
w3?
w4?
w5?s1?
s2?b2?b1?Figure 2: WTMF+PK model (WTMF + corpus-based Selectional [P]references semantics + [K]nowledge-basedsemantics): a w/s/b node represents a word/sentence/bigram, respectivelyfreq(j) denotes the frequency of bigram j appear-ing in the corpus, hence the strength of association isdifferentiated such that higher weights are assignedon the more probable bigrams.
The coefficient ?
isthe importance of selectional preference.
A larger?
indicates that we trust the selectional preferenceover the global sentential semantics.4 Knowledge-based Semantics: SimilarWord PairsWe first extract synonym pairs from WordNet, whichare words associated with the same sense, synset.We further expand the set by exploiting the relationsdefined in WordNet.
For the extracted words, weconsider the first sense of each word, and if it is con-nected to other senses by any of the WordNet definedrelations (hypernym, similar words, etc.
), then wetreat the words associated with the other senses assimilar words.
In total, we are able to discover 80Kpairs of similar words for the 46K distinct words inour corpus.Given a pair of similar words wi1/wi2 , we wantthe two corresponding latent vectors P?,i1/P?,i2 to beas close as possible, namely the cosine similarity tobe close to 1.
Accordingly, a term is added in equa-tion 1 for each similar word pair wi1/wi2 :?
?
(P?,i1 ?
P?,i2 ?
|P?,i1 ||P?,i2 |)2 (2)|P?,i| denotes the length of the vector P?,i.
The co-efficient ?, analogous to ?, denotes the importanceof the knowledge-based evidence.
The Figure 2cshows the final WTMF+PK model.5 InferenceIn (Guo and Diab, 2012b) we use Alternating LeastSquare [ALS] for inference, which is to set thederivative of equation 1 for P/Q to 0 and iterativelycompute P/Q by fixing the other matrix (Srebro andJaakkola, 2003).
However, it is no longer applicablewith the new term (equation 2) involving the lengthof word vectors |P?,i|.
Therefore we approximate theobjective function by treating the vector length |P?,i|as fixed values during the ALS iterations:Q?,j =(PW?
(j)P> + ?I)?1PW?
(j)X?,jP?,i =(QW?
(i)Q> + ?I + ?P?,s(i)P>?,s(i))?1(QW?
(i)X>i,?
+ ?LiP?,s(i)Ls(i))(3)where P?,s(i) are the latent vectors of similar wordsof word i; the length of these vectors in the currentiteration are stored in Ls(i) (similarly Li is the cur-rent length of P?,i) (cf.
(Steck, 2010; Guo and Diab,2012b) for optimization details).6 Experimental SettingWe build the model WTMF+PK on the same cor-pora as used in our previous work (Guo and Diab,2012b), comprising the following: Brown corpus(each sentence is treated as a document), sense def-initions from Wiktionary and Wordnet (only defini-tions without target words and usage examples).
Wefollow the preprocessing steps in (Guo and Diab,2012c): tokenization, pos-tagging, lemmatizationand further merge lemmas.
The corpus is used forbuilding matrix X .The evaluation datasets are LI06 dataset andSemeval-2012 STS [STS12] (Agirre et al 2012)dataset.
LI06 consists of 30 sentence pairs (dic-tionary definitions).
For STS12,1 the training data(2000 pairs) are used as the tuning set for setting the1A detailed description of the data sets is provided in (Agirreet al 2012).741parameters of our models.
This data comprises msr-par, msr-vid, smt-eur.
Once the models are tuned,we evaluate them on the STS12 test data that com-prises 3150 sentence pairs from msr-par, msr-vid,smt-eur, smt-news, On-WN.
It is worth noting thatsmt-news and On-WN are not part of the tuning data.We use cosine similarity to measure the similarityscores between two sentences.
Pearson correlationbetween the system?s answer and gold standard sim-ilarity scores is used as the evaluation metric.We include three baselines LSA, LDA andWTMF using the setting described in (Guo andDiab, 2012b).
We run Gibbs Sampling based LDAfor 2000 iterations and average the model over thelast 10 iterations.
For WTMF, we run 20 iterationsand fix the missing words weight at wm = 0.01 witha regularization coefficient set at ?
= 20, which isthe best condition found in (Guo and Diab, 2012b).7 ExperimentsTable 1 summarizes the results at dimension K =100 (the dimension of latent vectors).
To removerandomness, each reported number is the averagedresults of 10 runs.
Based on the STS tuning set,we experiment with different values for the selec-tional preference weight (?
= {0, 1, 2}), and like-wise for the similar word pairs weight varying the ?value as follows ?
= {0, 0.1, 0.3, 0.5, 0.7}.
The per-formance on STS12 tuning and test dataset as wellas on the LI06 dataset are illustrated in Figures 3a,3b and 3d.
The parameters of model 6 in Table 1(?
= 2, ?
= 0.3) are the chosen values based ontuning set performance.7.1 Evaluation on the STS12 datasetsTable 1 shows WTMF is already a very strong base-line: it outperforms LSA and LDA by a large mar-gin.
Same as in (Guo and Diab, 2012b), LSA per-formance degrades dramatically when trained on acorpus of sentence sized documents, yielding resultsworse than the surface words baseline 31% (Agirreet al 2012).
Using corpus-based selectional prefer-ence semantics alone (model 4 WTMF+P in Table1) boosts the performance of WTMF by +1.17% onthe test set, while using knowledge-based semanticsalone (model 5 WTMF+K) improves the over theWTMF results by an absolute +2.31%.
Combiningthem (model 6 WTMF+PK) yields the best results,with an absolute increase of +3.39%, which sug-gests that the two sources of semantic evidence areuseful, but more importantly, they are complemen-tary for each other.Table 1 also presents the performance on each in-dividual dataset.
The gain on each individual sourceis not as much as the overall gain, which suggestspart of the overall gain comes from the correct rank-ing of intra-source pairs.
Note that WTMF+PK im-proves all individual datasets except smt-eur.
Thismay be caused by too many overlapping words inthe sentence pairs in smt-eur, while our approachfocuses on extracting similarity between differentwords.Observing the performance using different valuesof weights in figure 3a and 3b, we can concludethat the selectional preference and similar word pairsyield very promising results.
The trends hold indifferent parameter conditions with a consistent im-provement.
Figure 3c illustrates the impact of di-mension K = {50, 75, 100, 125, 150} on WTMFand WTMF+PK.
Generally a larger K leads to ahigher Pearson correlation, but the improvement istiny when K ?
100 (0.1% increase).Compared to all the unsupervised systems thatparticipated in Semeval STS 2012 task, WTMF+PKyields state-of-the-art performance (70.70%).2 In(Guo and Diab, 2012c) we also apply WTMF (K =100) on STS12, achieving a correlation of 69.5%.However, additional data is incorporated in the train-ing corpora: (1) STS12 tuning set; (2) for WordNetand Wiktionary data, the target words are also in-cluded in the definitions (hence synonym pairs wereused); (3) the usage examples of target words werealso appended to the definitions.3 While trained withthis experimental setting, our model WTMF+PK(?
= 2, ?
= 0.3,K = 100) is able to reach an evenhigher correlation of 72.0%.2WTMF+PK is an unsupervised system, since the gold stan-dard similarly scores are never used in the objective function.Moreover, even without a tuning set, a non-zero value of ?
or ?will always improve the baseline WTMF according to figure 3aand 3b.3We do not adopt this corpora schema, since some defini-tions are test set sentences in On-WN, thereby adding targetwords and usage examples introduces additional informationfor some of the test set sentences742Models Parameters STS12 tune STS12 test msr-par msr-vid On-WN smt-eur smt-news LI061.
LSA - 21.67% 24.41% 27.18% 9.91% 50.93% 27.86% 19.73% 63.77%2.
LDA ?
= 0.05, ?
= 0.05 71.10% 63.18% 29.15% 76.73% 62.81% 47.81% 27.2% 83.71%3.
WTMF - 71.41% 67.31% 44.00% 82.59% 70.78% 50.89% 37.77% 89.81%4.
WTMF+P ?
= 2, ?
= 0 72.94% 68.48% 46.21% 83.29% 70.61% 49.54% 39.50% 90.16%5.
WTMF+K ?
= 0, ?
= 0.3 73.84% 69.64% 45.04% 83.04% 70.40% 49.88% 41.66% 90.11%6.
WTMF+PK ?
= 2, ?
= 0.3 75.29% 70.70% 46.77% 83.90% 71.03% 49.77% 40.48% 90.17%Table 1: Evaluation Results using Pearson Correlation on STS12 and LI060 0.1 0.3 0.5 0.77172737475correlation%?
?=0?=1?=2(a) STS12 tuning set (K = 100)0 0.1 0.3 0.5 0.76768697071?
?=0?=1?=2(b) STS12 test set (K = 100)50 75 100 125 150666768697071KWTMFWTMF+PK, ?=2, ?=0.3(c) STS12 test set0 0.1 0.3 0.5 0.789.59090.591?
?=0?=1?=2(d) LI06 (K = 100)Figure 3: Pearson correlation at different parameter settings7.2 Evaluation on the LI06 datasetFigure 3d presents the results obtained on the LI06data set at different weight values for the corpus-based selectional preference semantics ?
and for theknowledge-based semantics ?.
Our previous exper-iments (Guo and Diab, 2012b) show that WTMFis the state-of-the-art model on LI06.
With lexi-cal semantics explicitly modeled, WTMF+PK yieldsbetter results than WTMF (see Table 1).
It shouldbe noted that LI06 prefers a smaller similar wordpair weight ( a ?
= 0.1 yields the best perfor-mance around of 90.75%), yet in almost all condi-tions WTMF+PK outperforms WTMF as shown inFigure 3d.8 Related WorkSS has progressed immensely in recent years, espe-cially with the establishment of the Semantic Tex-tual Similarity task in SEMEVAL 2012.
Early workin SS focused on word pair similarity in the high di-mensional space (Li et al 2006; Liu et al 2007;Islam and Inkpen, 2008; Tsatsaronis et al 2010; Hoet al 2010), where co-occurrence information wasnot efficiently exploited.
Researchers (O?Shea et al2008) find LSA does not yield good performance.
In(Guo and Diab, 2012b; Guo and Diab, 2012c), weshow the superiority of the latent space approach inWTMF.
In this paper, we improve the WTMF modeland achieve state-of-the-art Pearson correlation ontwo standard SS datasets.There are latent variable models designed for lex-ical semantics, such as word senses (Boyd-Graberet al 2007; Guo and Diab, 2011), function words(Griffiths et al 2005), selectional preference (Ritteret al 2010), synonyms and antonyms (Yih et al2012), etc.
However little improvement is shownon document/sentence level semantics: (Ritter et al2010) and (Yih et al 2012) focus on selectionalpreference and antonym identification, respectively;in (Griffiths et al 2005) the LDA performance de-grades in the text categorization task including themodeling of function words.
Rather, we concentrateon nuanced lexical semantics phenomena that couldbenefit sentential semantics.9 ConclusionWe incorporate corpus-based (selectional prefer-ence) and knowledge-based (similar word pairs) lex-ical semantics into a latent variable model.
Oursystem yields state-of-the-art unsupervised perfor-mance on two most popular and standard SSdatasets.10 AcknowledgmentThis work is supported by the IARPA SCIL pro-gram.743ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In First JointConference on Lexical and Computational Semantics(*SEM).David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
Journal of MachineLearning Research, 3.Jordan Boyd-Graber, David Blei, and Xiaojin Zhu.
2007.A topic model for word sense disambiguation.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning.Scott Deerwester, Susan T Dumais, George W Furnas,Thomas K Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society for Information Science.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Thomas L. Griffiths, Mark Steyvers, David M. Blei, andJoshua B. Tenenbaum.
2005.
Integrating topics andsyntax.
In Advances in Neural Information ProcessingSystems.Weiwei Guo and Mona Diab.
2010.
Combining orthogo-nal monolingual and multilingual sources of evidencefor all words wsd.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics.Weiwei Guo and Mona Diab.
2011.
Semantic topic mod-els: Combining word distributional statistics and dic-tionary definitions.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing.Weiwei Guo and Mona Diab.
2012a.
Learning the latentsemantics of a concept by its definition.
In Proceed-ings of the 50th Annual Meeting of the Association forComputational Linguistics.Weiwei Guo and Mona Diab.
2012b.
Modeling sen-tences in the latent space.
In Proceedings of the 50thAnnual Meeting of the Association for ComputationalLinguistics.Weiwei Guo and Mona Diab.
2012c.
Weiwei: A simpleunsupervised latent semantics based approach for sen-tence similarity.
In First Joint Conference on Lexicaland Computational Semantics (*SEM).Chukfong Ho, Masrah Azrifah Azmi Murad, Rabiah Ab-dul Kadir, and Shyamala C. Doraisamy.
2010.
Wordsense disambiguation-based sentence similarity.
InProceedings of the 23rd International Conference onComputational Linguistics.Aminul Islam and Diana Inkpen.
2008.
Semantictext similarity using corpus-based word similarity andstring similarity.
ACM Transactions on KnowledgeDiscovery from Data, 2.Ou Jin, Nathan N. Liu, Kai Zhao, Yong Yu, and QiangYang.
2011.
Transferring topical knowledge fromauxiliary long texts for short text clustering.
In Pro-ceedings of the 20th ACM international conference onInformation and knowledge management.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and representa-tions.
In Proceedings of the 19th International JointConference on Artificial Intelligence.Yuhua Li, Davi d McLean, Zuhair A. Bandar, James D.O?Shea, and Keeley Crockett.
2006.
Sentence similar-ity based on semantic nets and corpus statistics.
IEEETransaction on Knowledge and Data Engineering, 18.Xiao-Ying Liu, Yi-Ming Zhou, and Ruo-Shi Zheng.2007.
Sentence similarity based on dynamic timewarping.
In The International Conference on Seman-tic Computing.James O?Shea, Zuhair Bandar, Keeley Crockett, andDavid McLean.
2008.
A comparative study of twoshort text semantic similarity measures.
In Proceed-ings of the Agent and Multi-Agent Systems: Technolo-gies and Applications, Second KES International Sym-posium (KES-AMSTA).Philip Resnik.
1997.
Selectional preference and sensedisambiguation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why, What, and How?Alan Ritter, Mausam, and Oren Etzioni.
2010.
A latentdirichlet alcation method for selectional preferences.In Proceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics.Jagan Sankaranarayanan, Hanan Samet, Benjamin E.Teitler, Michael D. Lieberman, and Jon Sperling.2009.
Twitterstand: news in tweets.
In Proceedings ofthe 17th ACM SIGSPATIAL International Conferenceon Advances in Geographic Information Systems.Nathan Srebro and Tommi Jaakkola.
2003.
Weightedlow-rank approximations.
In Proceedings of the Twen-tieth International Conference on Machine Learning.Harald Steck.
2010.
Training and testing of recom-mender systems on data missing not at random.
InProceedings of the 16th ACM SIGKDD InternationalConference on Knowledge Discovery and Data Min-ing.George Tsatsaronis, Iraklis Varlamis, and Michalis Vazir-giannis.
2010.
Text relatedness based on a word the-saurus.
Journal of Articial Intelligence Research, 37.Wentau Yih, Geoffrey Zweig, and John C. Platt.
2012.Polarity inducing latent semantic analysis.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning.744Liang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu,and Eduard Hovy.
2006.
Paraeval: Using paraphrasesto evaluate summaries automatically.
In Proceedingsof Human Language Technology Conference of theNorth American Chapter of the ACL,.745
