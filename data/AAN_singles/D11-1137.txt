Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1479?1488,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsThird-order Variational Reranking on Packed-Shared Dependency ForestsKatsuhiko Hayashi?, Taro Watanabe?, Masayuki Asahara?, Yuji Matsumoto?
?Nara Insutitute of Science and TechnologyIkoma, Nara, 630-0192, Japan?National Institute of Information and Communications TechnologySorakugun, Kyoto, 619-0289, Japan{katsuhiko-h,masayu-a,matsu}@is.naist.jptaro.watanabe@nict.go.jpAbstractWe propose a novel forest reranking algorithmfor discriminative dependency parsing basedon a variant of Eisner?s generative model.
Inour framework, we define two kinds of gener-ative model for reranking.
One is learned fromtraining data offline and the other from a for-est generated by a baseline parser on the fly.The final prediction in the reranking stage isperformed using linear interpolation of thesemodels and discriminative model.
In order toefficiently train the model from and decodeon a hypergraph data structure representing aforest, we apply extended inside/outside andViterbi algorithms.
Experimental results showthat our proposed forest reranking algorithmachieves significant improvement when com-pared with conventional approaches.1 IntroductionRecently, much of research on statistical parsinghas been focused on k-best (or forest) reranking(Collins, 2000; Charniak and Johnson, 2005; Huang,2008).
Typically, reranking methods first generatea list of top-k candidates (or a forest) from a base-line system, then rerank the candidates with arbi-trary features that are intractable within the baselinesystem.
In the reranking framework, the baselinesystem is usually modeled with a generative model,and a discriminative model is used for reranking.Sangati et al (2009) reversed the usual order of thetwo models for dependency parsing by employinga generative model to rescore the k-best candidatesprovided by a discriminative model.
They use a vari-ant of Eisner?s generative model C (Eisner, 1996b;Eisner, 1996a) for reranking and extend it to capturehigher-order information than Eisner?s second-ordergenerative model.
Their reranking model showedlarge improvements in dependency parsing accu-racy.
They reported that the discriminative model isvery effective at filtering out bad candidates, whilethe generative model is able to further refine the se-lection among the few best candidates.In this paper, we propose a forest generativereranking algorithm, opposed to Sangati et al(2009)?s approach which reranks only k-best candi-dates.
Forests usually encode better candidates morecompactly than k-best lists (Huang, 2008).
More-over, our reranking uses not only a generative modelobtained from training data, but also a sentence spe-cific generative model learned from a forest.
In thereranking stage, we use linearly combined modelof these models.
We call this variational rerank-ing model.
The model proposed in this paper isfactored in the third-order structure, therefore, itsnon-locality makes it difficult to perform the rerank-ing with an usual 1-best Viterbi search.
To solvethis problem, we also propose a new search algo-rithm, which is inspired by the third-order dynamicprogramming parsing algorithm (Koo and Collins,2010).
This algorithm enables us an exact 1-bestreranking without any approximation.
We summa-rize our contributions in this paper as follows.?
To extend k-best to forest generative reranking.?
We introduce variational reranking which is acombination approach of generative rerankingand variational decoding (Li et al, 2009).?
To obtain 1-best tree in the reranking stage, we1479propose an exact 1-best search algorithm withthe third-order model.In experiments on English Penn Treebank data,we show that our proposed methods bring signif-icant improvement to dependency parsing.
More-over, our variational reranking framework achievesconsistent improvement, compared to conventionalapproaches, such as simple k-best and forest-basedgenerative reranking algorithms.2 Dependency ParsingGiven an input sentence x ?
X , the task of statis-tical dependency parsing is to predict output depen-dencies y?
for x.
The task is usually modeled within adiscriminative framework, defined by the followingequation:y?
= argmaxy?Ys(x, y)= argmaxy?Y??
?
F(y, x) (1)where Y is the output space, ?
is a parameter vector,and F() is a set of feature functions.We denote a set of candidates as G(x).
By usingG(x), the conditional probability p(y|x) is typicallyderived as follows:p(y|x) = e?
?s(x,y)Z(x) =e?
?s(x,y)?y?G(x) e?
?s(x,y)(2)where s(x, y) is the score function shown in Eq.1and ?
is a scaling factor to adjust the sharpness ofthe distribution and Z(x) is a normarization factor.2.1 Hypergraph RepresentationWe propose to encode many hypotheses in a com-pact representation called dependency forest.
Whilethere may be exponentially many dependency trees,the forest represents them in polynomial space.
Adependency forest (or tree) can be defined as a hy-pergraph data strucureHG (Tu et al, 2010).Figure 1 shows an example of a hypergraph for adependency tree.
A shaded hyperedge e is definedas the following form:e : ?
(I1,2, girl3,5,with5,8), saw1,8?....top0,8.....saw1,8:V...
.I1,2:N .. .
.
.
....girl3,5:N..a3,4:D ... ....with5,8:P.....telescope6,8:N. ..a6,7:D ... .. ..e .
.
.
.
.. .Figure 1: An example of dependency tree for a sentence?I saw a girl with a telescope?.The node saw1,8 is a head node of e. The nodes, I1,2,girl3,5 and with5,8, are tail nodes of e. The hyper-edge e is an incoming edge for saw1,8 and outgoingedge for each of I1,2, girl3,5 and with5,81.More formally, HG(x) of a forest is a pair?V,E?, where V is a set of nodes and E is a setof hyperedges.
Given a length m sentence x =(w1 .
.
.
wm), each node v ?
V is in the form ofwi,j (= (wi .
.
.
wj+1)) which denotes that a wordw dominates the substring from positions i to j. Inour implementation, each word is paired with POS-tag tag(w).
We denote the root node of dependencytree y as top.
Each hyperedge e ?
E is a pair?tails(e), head(e)?, where head(e) ?
V is the headand tails(e) ?
V + are its dependants.
For nota-tional brevity of algorithmic description, we do notdistinguish left and right tails in the definition, but,our implementation implicitly distinguishes left tailstailsL(e) and right tails tailsR(e).
We define the setof incoming edges of a node v as IE(v) and the setof outgoing edges of a node v as OE(v).3 Forest Reranking3.1 Generative Model for RerankingGiven a node v in a dependency tree y, the left andright children are generated as two separate Markovsequences, each conditioned on ancestral and siblinginformation (context).
Like a variation of Eisner?sgenerative model C (Eisner, 1996b; Eisner, 1996a),1In Figure 1, according to custom of dependency treedescription, the direction of hyperedge is written as fromhead to tail nodes.
However, in this paper, ?incoming?
and?outgoing?
have the same meanings as those in (Huang, 2006).1480Table 1: An event list of tri-sibling model whose eventspace is v|h, sib, tsib, dir, extracted from hyperedge e inFigure 1.
EOC is an end symbol of sequence.event spaceI | saw NONE NONE LEOC | saw I NONE Lgirl | saw NONE NONE Rwith | saw girl NONE REOC | saw with girl Rthe probability of our model q is defined as follows:q(v) =|tailsL(e)|?l=1q(vl|C(vl)) ?
q(vl)?|tailsR(e)|?r=1q(vr|C(vr)) ?
q(vr) (3)where |tailsL(e)| and |tailsR(e)| are the number ofleft and right children of v, vl and vr are the left andright child of position l and r in each side.
C(v) isa context event space of v. We explain the contextevent space later in more detail.
The probability ofthe entire dependency tree y is recursively computedby q(y(top)) where y(top) denotes a top node of y.The probability q(v|C(v)) is dependent on a con-text space C(v) for a node v. We define two kinds ofcontext spaces.
First, we define a tri-sibling modelwhose context space consists of the head node, sib-ling node, tri-sibling node and direction of a nodev:q1(v|C(v)) = q1(v|h, sib, tsib, dir) (4)where h, sib and tsib are head, sibling and tri-siblingnode of v, and dir is a direction of v from h. Table1 shows an example of an event list of the tri-siblingmodel, which is extracted from hyperedge e in Fig-ure 1.
EOC indicates the end of the left or right childsequence.
This is factored in a tri-sibling structureshown in the left side of Figure 2.Eq.4 is further decomposed into a product of theform consisting of three terms:q1(v|h, sib, tsib, dir) (5)= q1(dist(v, h), wrd(v), tag(v)|h, sib, tsib, dir)= q1(tag(v)|h, sib, tsib, dir)?q1(wrd(v)|tag(v), h, sib, tsib, dir)?q1(dist(v, h)|wrd(v), tag(v), h, sib, tsib, dir)where tag(v) and wrd(v) are the POS-tag and wordof v and dist(v, h) is the distance between positionsof v and h. The values of dist(v, h) are partitionedinto 4 categories: 1, 2, 3 ?
6, 7 ?
?.Second, following Sangati et al (2009), we definea grandsibling model whose context space consistsof the head node, sibling node, grandparent node anddirection of a node v.q2(v|C(v)) = q2(v|h, sib, g, dir) (6)where g is a grandparent node of v. Analogous toEq.5, Eq.6 is decomposed into three terms:q2(v|h, sib, g, dir) (7)= q2(dist(v, h), wrd(v), tag(v)|h, sib, g, dir)= q2(tag(v)|h, sib, g, dir)?q2(wrd(v)|tag(v), h, sib, g, dir)?q2(dist(v, h)|wrd(v), tag(v), h, sib, g, dir)where notations are the same as those in Eq.5 withthe exception of tri-sibling tsib and grandparent g.This model is factored in a grandsibling structureshown in the right side of Figure 2.The direct estimation of tri-sibling and grandsib-ling models from a corpus suffers from serious datasparseness issues.
To overcome this, Eisner (1996a)proposed a back-off strategy which reduces the con-ditioning of a model.
We show the reductions listfor each term of two models in Table 2.
The usageof reductions list is identical to Eisner (1996a) andreaders may refer to it for further details.The final prediction is performed using a log-linear interpolated model.
It interpolates the base-line discriminative model and two (tri-sibling andgrandsibling) generative models.y?
= argmaxy?G(x)2?n=1log qn(top(y))?n+ log p(y|x)?base (8)where ?
are parameters to adjust the weight of eachterm in prediction.
These parameters are tuned usingMERT algorithm (Och, 2003) on development datausing a criterion of accuracy maximization.
The rea-son why we chose MERT is that it effectively tunesdense parameters with a line search algorithm.1481Table 2: Reduction lists for tri-sibling and grandsibling models: wt(), w() and t() mean word and POS-tag, word,POS-tag for a node.
d indicates the direction.
The first reduction on the list keeps all or most of the original condition;later reductions throw away more and more of this information.tri-sibling grandsibling1-st term 2-nd term 3-rd term 1-st term 2-nd term 3-rd termwt(h),wt(sib),wt(tsib),d wt(h),t(sib),d wt(v),t(h),t(sib),d wt(h),wt(sib),wt(g),d wt(h),t(sib),d wt(v),t(h),t(sib),dwt(h),wt(sib),t(tsib),d t(h),t(sib),d t(v),t(h),t(sib),d wt(h),wt(sib),t(g),d t(h),t(sib),d t(v),t(h),t(sib),dt(h),wt(sib),t(tsib),d ?
?
t(h),wt(sib),t(g),d ?
?wt(h),t(sib),t(tsib),d wt(h),t(sib),t(g),dt(h),t(sib),t(tsib),d ?
?
t(h),t(sib),t(g),d ?
?..h .tsib .sib .v ..g .h .sib .vFigure 2: The left side denotes tri-sibling structure andthe right side denotes grandsibling structure.Table 3: A summarization of the model factorization andorderfirst-order McDonald et al (2005)second-order Eisner (1996a)(sibling) McDonald et al (2005)third-order tri-sibling model(tri-sibling) Model 2 (Koo and Collins, 2010)third-order grandsibling model (Sangati et al, 2009)(grandsibling) Model 1 (Koo and Collins, 2010)3.2 Exact Search AlgorithmOur baseline discriminative model uses first- andsecond-order features provided in (McDonald et al,2005; McDonald and Pereira, 2006).
Therefore,both our tri-sibling model and baseline discrimina-tive model integrate local features that are factoredin one hyperedge.
On the other hand, the grandsib-ling model has non-local features because the grand-parent is not factored in one hyperedge.
We sum-marize the order of each model in Table 3.
Ourreranking models are generative versions of Koo andCollins (2010)?s third-order factorization model.Non-locality of weight function makes it difficultto perform the search of Eq.8 with an usual exactViterbi 1-best algorithm.
One solution to resolvethe intractability is an approximate k-best Viterbisearch.
For a constituent parser, Huang (2008) ap-plied cube pruning techniques to forest rerankingwith non-local features.
Cube pruning is originallyproposed for the decoding of statistical machinetranslation (SMT) with an integrated n-gram lan-guage model (Chiang, 2007).
It is an approximatek-best Viterbi search algorithm using beam searchand lazy computation (Huang and Chiang, 2005).In the case of a dependency parser, Koo andCollins (2010) proposed dynamic-programming-based third-order parsing algorithm, which enumer-ates all grandparents with an additional loop.
Ourhypergraph based search algorithm for Eq.8 sharethe same spirit to their third-order parsing algo-rithm since the grandsibling model is similar to theirmodel 1 in that it is factored in grandsibling struc-ture.
Algorithm 1 shows the search algorithm.
Thisis almost the same bottom-up 1-best Viterbi algo-rithm except an additional loop in line 4.
Line 4 ref-erences outgoing edge e?
of node h from a set of out-going edges OE(h).
tails(e) contains a node v, thesibling node sib and tri-sibling node tsib of v, more-over, the head of e?
(head(e?))
is the grandparent forv and sib.
Thus, in line 5, we can capture tri-siblingand grandsibling information and compute the cur-rent inside estimate of Eq.8.In our actual implementation, each score of com-ponents in Eq.8 is represented as a cost.
This is writ-ten as a shortest path search algorithm with a tropi-cal (real) semiring framework (Mohri, 2002; Huang,2006).
Therefore,?
denotes the min operater and?denotes the + operater.
The function f is defined asfollows:f(d(v1, e), .
.
.
, d(v|e|, e))) =|e|?i=1d(vi, e) (9)where d(vi, e) denotes the current estimate of thebest cost for a pair of node vi and a hyperedge e.?
sums the best cost of a pair of a sub span nodeand hyperedge e. Each ctsib and cgsib in line 5 and7 indicates the cost of tri-sibling and grandsibling1482Algorithm 1 Exact DP-Search Algorithm(HG(x))1: for h ?
V in bottom-up topological order do2: for e ?
IE(h) do3: // tails(e) is {v1, .
.
.
, v|e|}.4: for e?
?
OE(h) do5: d(h, e?)
= ?f(d(v1, e), .
.
.
, d(v|e|, e)) ?
we ?
ctsib(h, tails(e)) ?
cgsib(head(e?
), h, tails(e))6: if h == top then7: d(h) = ?f(d(v1, e), .
.
.
, d(v|e|, e)) ?
we ?
ctsib(h, tails(e))model.
we indicates the cost of hyperedge e com-puted from a baseline discriminative model.
Lines6-7 denote the calculation of the best cost for a topnode.
We do not compute the cost of the grandsib-ling model when h is top node because top node hasno outgoing edges.Our baseline k-best second-order parser is imple-mented using Huang and Chiang (2005)?s algorithm2 whose time complexity is O(m3+mk log k).
Kooand Collins (2010)?s third-order parser has O(m4)time complexity and is theoretically slower than ourbaseline k-best parser for a long sentence.
Oursearch algorithm is based on the third-order parsingalgorithm, but, the search space is previously shrankby a baseline parser?s k-best approximation and aforest pruning algorithm presented in the next sec-tion.
Therefore, the time efficiency of our rerankingis unimpaired.3.3 Forest PruningCharniak and Johnson (2005) and Huang (2008)proposed forest pruning algorithms to reduce thesize of a forest.
Huang (2008)?s pruning algo-rithm uses a 1-best Viterbi inside/outside algorithmto compute an inside probability ?
(v) and an out-side probability ?
(v), while Charniak and Johnson(2005) use the usual inside/outside algorithm.In our experiments, we use Charniak and Johnson(2005)?s forest pruning criterion because the varia-tional model needs traditional inside/outside proba-bilities for its ML estimation.
We prune away allhyperedges that have score < ?
for a threshold ?.score = ??(e)?
(top) .
(10)Following Huang (2008), we also prune away nodeswith all incoming and outgoing hyperedges pruned.4 Variational Reranking ModelIn place of a maximum a posteriori (MAP) decisionbased on Eq.2, the minimum Bayes risk (MBR) deci-sion rule (Titov and Henderson, 2006) is commonlyused and defined as following equation:y?
= argminy?G(x)?y?
?G(x)loss(y, y?
)p(y?|x) (11)where loss(y, y?)
represents a loss function2.
As analternative to the MBR decision rule, Li et al (2009)proposed a variational decision rule that rescorescandidates with an approximate distribution q?
?
Q.y?
= argmaxy?G(x)q?
(y) (12)where q?
minimizes the KL divergence KL(p||q)q?
= argminq?QKL(p||q)= argmaxq?Q?y?G(x)p log q (13)where each p and q represents p(y|x) and q(y).
ForSMT systems, q?
is modeled by n-gram languagemodel over output strings.
While the decoding basedon q?
is an approximation of intractable MAP de-coding3, it works as a rescoring function for candi-dates generated from a baseline model.
Here, wepropose to apply the variational decision rule to de-pendency parsing.
For dependency parsing, we canchoose to model q?
as the tri-sibling and grandsib-ling generative models in section 3.2In case of dependency parsing, Titov and Henderson (2006)proposed that a loss function is simply defined using a depen-dency attachment score.3In SMT, a marginalization of all derivations which yielda paticular translation needs to be carried out for each trans-lation.
This makes the MAP decoding NP-hard in SMT.
Thisvariational approximate framework can be applied to other taskscollapsing spurious ambiguity, such as latent-variable parsing(Matsuzaki et al, 2005).1483Algorithm 2 DP-ML Estimation(HG(x))1: run inside and outside algorithm onHG(x)2: for v ?
V do3: for e ?
IE(v) do4: ctsib = pe ?
?(v)/?
(top)5: for u ?
tails(e) do6: ctsib = ctsib ?
?
(u)7: for e?
?
IE(u) do8: cgsib = pe ?
pe?
?
?(v)/?
(top)9: for u?
?
tails(e) \ u do10: cgsib = cgsib ?
?(u?
)11: for u??
?
tails(e?)
do12: cgsib = cgsib ?
?(u??
)13: for u??
?
tails(e?)
do14: c2(u??|C(u??
))+ = cgsib15: c2(C(u??
))+ = cgsib16: for u ?
tails(e) do17: c1(u|C(u))+ = ctsib18: c1(C(u))+ = ctsib19: MLE estimate q?1 , q?2 using formula Eq.144.1 ML Estimation from a Forestq?
(v|C(v)) is estimated from a forest using a max-imum likelihood estimation (MLE).
The count ofevents is no longer an integer count, but an expectedcount under p, which is formulated as follows:q?
(v|C(v)) = c(v|C(v))c(C(v))=?y p(y|x)cv|C(v)(y)?y p(y|x)cC(v)(y)(14)where ce(y) is the number of event e in y.
The es-timation of Eq.14 can be efficiently performed on ahypergraph data structureHG(x) of a forest.Algorithm 2 shows the estimation algorithm.First, it runs the inside/outside algorithm onHG(x).We denote inside weight for a node v as ?
(v) andoutside weight as ?(v).
For each hyperedge e, wedenote ctsib as the posterior weight for computingexpected count c1 of events in the tri-sibling modelq?1 .
Lines 16-18 compute c1 for all events occuringin a hyperedge e.The expected count c2 needed for the estimationof grandsibling model q?2 is extracted in lines 7-15.c2 for a grandsibling model must be extracted overtwo hyperedges e and e?
because it needs grandpar-ent information.
Lines 8-12 show the algorithm tocompute the posterior weight cgsib of e and e?, which92939495969798991000  200  400  600  800  1000  1200  1400UnlabeledAccuracythe number of hyperedges per sentencep=0.001k=20k=100"kbest""forest"Figure 3: The relationship between tha data size (thenumber of hyperedges) and oracle scores on develop-ment data: Forests encode candidates with high accuracyscores more compactly than k-best lists.is similar to that to compute the posterior weightof rules of tree substitution grammars used in tree-based MT systems (Mi and Huang, 2008).
Lines13-15 compute expected counts c2 of events occur-ing over two hyperedges e and e?.
Finally, line 19estimates q?1 and q?2 using the form in Eq.14.Li et al (2009) assumes n-gram locality of theforest to efficiently train the model, namely, thebaseline n-gram model has larger n than that of vari-ational n-gram model.
In our case, grandsibling lo-cality is not embedded in the forest generated fromthe baseline parser.
Therefore, we need to referenceincoming hyperedges of tail nodes in line 7.y?
of Eq.12 may be locally appropriate but glob-ally inadequate because q?
only approximates p.Therefore, we log-linearly combine q?
with a globalgenerative model estimated from the training dataand the baseline discriminative model.y?
= argmaxy?G(x)2?n=1log qn(top(y))?n+2?n=1log q?n(top(y))?
?n+ log p(y|x)?base (15)Algorithm1 is also applicable to the decoding ofEq.15.
Note that this framework is a combination ofvariational decoding and generative reranking.
Wecall this framework variational reranking.1484Table 4: The statistics of forests and 20-best lists on de-velopment data: this shows the average number of hyper-edges and nodes per sentence and oracle scores.forest 20-bestpruning threshold ?
= 10?3 ?ave.
num of hyperedges 180.67 255.04ave.
num of nodes 135.74 491.42oracle scores 98.76 96.785 ExperimentsExperiments are performed on English Penn Tree-bank data.
We split WSJ part of the Treebank intosections 02-21 for training, sections 22 for develop-ment, sections 23 for testing.
We use Yamada andMatsumoto (2003)?s head rules to convert phrasestructure to dependency structure.
We obtain k-bestlists and forests generated from the baseline discrim-inative model which has the same feature set as pro-vided in (McDonald et al, 2005), using the second-order Eisner algorithms.
We use MIRA for trainingas it is one of the learning algorithms that achievesthe best performance in dependency parsing.
We setthe scaling factor ?
= 1.0.We also train a generative reranking model fromthe training data.
To reduce the data sparsenessproblem, we use the back-off strategy proposed in(Eisner, 1996a).
Parameters ?
are trained usingMERT (Och, 2003) and for each sentence in the de-velopment data, 300-best dependency trees are ex-tracted from its forest.
Our variational rerankingdoes not need much time to train the model be-cause the training is performed over not the train-ing data (39832 sentences) but the development data(1700 sentences)4.
After MERT was performed un-til the convergence, the variational reranking finallyachieved a 94.5 accuracy score on development data.5.1 k-best Lists vs. ForestsFigure 3 shows the relationship between the size ofdata structure (the number of hyperedges) and accu-racy scores on development data.
Obviously, forestscan encode a large number of potential candidatesmore compactly than k-best lists.
This means that4To generate forests, sentences are parsed only once beforethe training.
MERT is performed over the forests.
We can alsoapply a more efficient hypergraph MERT algorithm (Kumar etal., 2009) to the training than a simple MERT algorithm.for reranking, there is more possibility of selectinggood candidates in forests than k-best lists.Table 4 shows the statistics of forests and 20-best lists on development data.
This setting, thresh-old ?
= 10?3 for pruning, is also used for testing.Forests, which have an average of 180.67 hyper-edges per sentence, achieve oracle score of 98.76,which is about 1.0% higher than the 96.78 oraclescore of 20-best lists with 255.04 hyperedges persentence.
Though the size of forests is smaller thanthat of k-best lists, the oracle scores of forests aremuch higher than those of k-best lists.5.2 The Performance of RerankingFirst, we compare the performance of variational de-coding with that of MBR decoding.
The results areshown in Table 5.
Variational decoding outperformsMBR decodings.
However, compared with base-line, the gains of variational and MBR decoding aresmall.
Second, we also compare the performance ofvariational reranking with k-best and forest gener-ative reranking algorithms.
Table 6 shows that ourvariational reranking framework achieves the high-est accuracy scores.Being different from the decoding framework,reranking achieves significant improvements.
Thisresult is intuitively reasonable because the rerank-ing model obtained from training data has the abilityto select a globally consistent candidate, while thevariational approximate model obtained from a for-est only supports selecting a locally consistent can-didate.
On the other hand, the fact that variationalreranking achieves the best results clearly indicatesthat the combination of sentence specific generativemodel and that obtained from training data is suc-cessful in selecting both locally and globally appro-priate candidate from a forest.Table 7 shows the parsing time (on 2.66GHzQuad-Core Xeon) of the baseline k-best, generativereranking and variational reranking parsers (java im-plemented).
The variational reranking parser con-tains the following procedures.1.
k-best forest creation (baseline)2.
Estimation of variational model3.
Forest pruning4.
Search with the third-order modelOur reranking parser incurred little overhead to the1485Table 5: The comparison of the decoding frameworks:MBR decoding seeks a candidate which has the high-est accuracy scores over a forest (Kumar et al, 2009).Variational decoding is performed based on Eq.8.XXXXXXXXXXDecodingEval Unlabeledbaseline 91.9MBR (8-best forest) 91.99Variational (8-best forest) 92.17Table 6: The comparison of the reranking frameworks:Generative means k-best or forest reranking algorithmbased on a generative model estimated from a corpus.Variational reranking is performed based on Eq.15.XXXXXXXXXXRerankingEval UnlabeledGenerative (8-best) 92.66Generative (8-best forest) 92.72Variational (8-best forest) 92.87Table 7: The parsing time (CPU second per sentence) andaccuracy score of the baseline k-best, generative rerank-ing and variational reranking parsersk baseline generative variational2 0.09 (91.9) +0.03 (92.67) +0.05 (92.76)4 0.1 (91.9) +0.05 (92.68) +0.09 (92.81)8 0.13 (91.9) +0.06 (92.72) +0.11 (92.87)16 0.18 (91.9) +0.07 (92.75) +0.12 (92.89)32 0.29 (91.9) +0.07 (92.73) +0.13 (92.89)64 0.54 (91.9) +0.08 (92.72) +0.15 (92.87)Table 8: The comparison of tri-sibling and grandsiblingmodels: the performance of the grandsibling model out-performs that of the tri-sibling model.PPPPPPPModelEval Unlabeledtri-sibling 92.63grandsibling 92.74baseline parser in terms of runtime.
This means thatour reranking parser can parse sentences at reason-able times.5.3 The Effects of Third-order Factors andError AnalysisFrom results in section 5.2, our variational rerank-ing model achieves higher accuracy scores than theothers.
To analyze the factors that improve accu-racy scores, we further investigate whether varia-tional reranking is performed better with the tri-sibling or grandsibling model.
Table 8 indicates thatgrandsibling model achieves a larger gain than thatof tri-sibling model.
Table 9 shows the exampleswhose accuracy scores improved by the grandsib-ling model.
For example, the dependency relation-ship from Verb to Noun phrase was corrected by ourproposed model.On the other hand, many errors remain still inTable 10: Comparison of our best result (using 16-bestforests) with other best-performing Systems on the wholesection 23Parser EnglishMcDonald et al (2005) 90.9McDonald and Pereira (2006) 91.5Koo et al (2008) standard 92.02Huang and Sagae (2010) 92.1Koo and Collins (2010) model1 93.04Koo and Collins (2010) model2 92.93this work 92.89Koo et al (2008) semi-sup 93.16Suzuki et al (2009) 93.79our results.
In our experiments, 48% of sentenceswhich contain errors have Prepositionalword errors.In fact, well-known PP-Attachment is a problem tobe solved for natural language parsers.
Other re-maining errors are caused by symbols such as .,:??
().45% sentences contain such a dependency mistake.Adding features to solve these problems may poten-tially improve our parser more.5.4 Comparison with Other SystemsTable 10 shows the comparison of the performanceof variational reranking (16-best forests) with that ofother systems.
Our method outperforms supervisedparsers with second-order features, and achievescomparable results compared to a parser with third-order features (Koo and Collins, 2010).
We can notdirectly compare our method with semi-supervisedparsers such as Koo et al (2008)?s semi-sup andSuzuki et al (2009), because ours does not use addi-tional unlabeled data for training.
The model trainedfrom unlabeled data can be easily incorporated intoour reranking framework.
We plan to investigatesemi-supervised learning in future work.1486Table 9: Examples of outputs for input sentence No.148 and No.283 in section 23 from baseline and variationalreranking parsers.
The underlined portions show the effect of the grandsibling model.sent (No.148) A quick turnaround is crucial to Quantum because its cash requirements remain heavy .correct 3 3 4 0 4 5 6 4 11 11 12 8 12 4baseline 3 3 4 0 4 5 6 4 11 11 8 8 12 4proposed 3 3 4 0 4 5 6 4 11 11 12 8 12 4sent (No.283) Many called it simply a contrast in styles .correct 2 0 2 6 6 2 6 7 2baseline 2 0 2 2 6 2 6 7 2proposed 2 0 2 6 6 2 6 7 26 Related WorkCollins (2000) and Charniak and Johnson (2005)proposed a reranking algorithm for constituentparsers.
Huang (2008) extended it to a forest rerank-ing algorithm with non-local features.
Our frame-work is for a dependency parser and the decoding inthe reranking stage is done with an exact 1-best dy-namic programming algorithm.
Sangati et al (2009)proposed a k-best generative reranking algorithm fordependency parsing.
In this paper, we use a similargenerative model, but combined with a variationalmodel learned on the fly.
Moreover, our frameworkis applicable to forests, not k-best lists.Koo and Collins (2010) presented third-order de-pendency parsing algorithm.
Their model 1 is de-fined by an enclosing grandsibling for each siblingor grandchild part used in Carreras (2007).
Ourgrandsibling model is similar to the model 1, butours is defined by a generative model.
The decod-ing in the reranking stage is also similar to the pars-ing algorithm of their model 1.
In order to capturegrandsibling factors, our decoding calculates insideprobablities for not the current head node but eachpair of the node and its outgoing edges.Titov and Henderson (2006) reported that theMBR approach could be applied to a projective de-pendency parser.
In the field of SMT, for an approx-imation of MAP decoding, Li et al (2009) proposedvariational decoding and Kumar et al (2009) pre-sented hypergraph MBR decoding.
Our variationalmodel is inspired by the study of Li et al (2009) andwe apply it to a dependency parser in order to selectbetter candidates with third-order information.
Wealso propose an efficient algorithm to estimate thenon-local third-order model structure.7 ConclusionsIn this paper, we propose a novel forest rerankingalgorithm for dependency parsing.
Our rerankingalgorithm is a combination approach of generativereranking and variational decoding.
The search al-gorithm in the reranking stage can be performedusing dynamic programming algorithm.
Our vari-ational reranking is aimed at selecting a candidatefrom a forest, which is correct both in local andglobal.
Our experimental results show more signif-icant improvements than conventional approaches,such as k-best and forest generative reranking.In the future, we plan to investigate more ap-propriate generative models for reranking.
PP-Attachment is one of the most difficult problemsfor a natural language parser.
We plan to exam-ine to model such a complex structure (granduncle)(Goldberg and Elhadad, 2010) or higher-order struc-ture than third-order for reranking which is compu-tationally expensive for a baseline parser.
As wementioned in Section 5.4, we also plan to incorpo-rate semi-supervised learning into our framework,which may potentially improve our reranking per-formance.AcknowledgmentsWewould like to thank GrahamNeubig andMasashiShimbo for their helpful comments and to the anony-mous reviewers for their effort of reviewing our pa-per and giving valuable comments.
This work wassupported in part by Grant-in-Aid for Japan Societyfor the Promotion of Science (JSPS) Research Fel-lowship for Young Scientists.1487ReferencesX.
Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proc.
the CoNLL-EMNLP, pages 957?961.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InProc.
the 43rd ACL, pages 173?180.D.
Chiang.
2007.
Hierarchical phrase-based translation.Computational Linguistics, 33:201?228.M.
Collins.
2000.
Discriminative reranking for naturallanguage parsing.
In Proc.
the ICML.J.
M. Eisner.
1996a.
An empirical comparison of prob-ability models for dependency grammar.
In TechnicalReport, pages 1?18.J.
M. Eisner.
1996b.
Three new probabilistic models fordependency parsing: An exploration.
In Proc.
the 16thCOLING, pages 340?345.Y.
Goldberg and M. Elhadad.
2010.
An efficient algo-rithm for easy-first non-directional dependency pars-ing.
In Proc.
the HLT-NAACL, pages 742?750.L.
Huang and D. Chiang.
2005.
Better k-best parsing.
InProc.
the IWPT, pages 53?64.L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proc.
the ACL,pages 1077?1086.L.
Huang.
2006.
Dynamic programming al-gorithms in semiring and hypergraph frame-works.
Qualification Exam Report, pages 1?19.http://www.cis.upenn.edu/ lhuang3/wpe2/.L.
Huang.
2008.
Forest reranking: Discriminative pars-ing with non-local features.
In Proc.
the 46th ACL,pages 586?594.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proc.
the 48th ACL, pages 1?11.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In Proc.
the ACL,pages 595?603.S.
Kumar, W. Macherey, C. Dyer, and F. Och.
2009.
Effi-cient minimum error rate training and minimum bayes-risk decoding for translation hypergraphs and lattices.In Proc.
the 47th ACL, pages 163?171.Z.
Li, J. Eisner, and S. Khudanpur.
2009.
Variationaldecoding for statistical machine translation.
In Proc.the 47th ACL, pages 593?601.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic cfg with latent annotations.
In Proc.
the ACL, pages75?82.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Proc.EACL, pages 81?88.R.
McDonald, K. Crammer, and F. Pereira.
2005.
Onlinelarge-margin training of dependency parsers.
In Proc.the 43rd ACL, pages 91?98.H.
Mi and L. Huang.
2008.
Forest-based translation ruleextraction.
In Proceedings of EMNLP, pages 206?214.M.
Mohri.
2002.
Semiring framework and algorithmsfor shortest-distance problems.
Automata, Languagesand Combinatorics, 7:321?350.F.
J. Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proc.
the 41st ACL, pages160?167.F.
Sangati, W. Zuidema, and R. Bod.
2009.
A generativere-ranking model for dependency parsing.
In Proc.
the11th IWPT, pages 238?241.J.
Suzuki, H. Isozaki, X. Carreras, and M. Collins.
2009.An empirical study of semi-supervised structured con-ditional models for dependency parsing.
In Proc.
theEMNLP, pages 551?560.I.
Titov and J. Henderson.
2006.
Bayes risk minimiza-tion in natural language parsing.
In Technical Report,pages 1?9.Z.
Tu, Y. Liu, Y. Hwang, Q. Liu, and S. Lin.
2010.
De-pendency forest for statistical machine translation.
InProc.
the 23rd COLING, pages 1092?1100.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Proc.the IWPT, pages 195?206.1488
