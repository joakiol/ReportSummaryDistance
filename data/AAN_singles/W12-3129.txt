Proceedings of the 7th Workshop on Statistical Machine Translation, pages 243?252,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsFully Automatic Semantic MT EvaluationChi-kiu LO, Anand Karthik TUMULURU and Dekai WUHKUSTHuman Language Technology CenterDepartment of Computer Science and EngineeringHong Kong University of Science and Technology{jackielo,aktumuluru,dekai}@cs.ust.hkAbstractWe introduce the first fully automatic, fully seman-tic frame based MT evaluation metric, MEANT,that outperforms all other commonly used auto-matic metrics in correlating with human judgmenton translation adequacy.
Recent work on HMEANT,which is a human metric, indicates that machinetranslation can be better evaluated via semanticframes than other evaluation paradigms, requiringonly minimal effort from monolingual humans to an-notate and align semantic frames in the reference andmachine translations.
We propose a surprisingly ef-fective Occam?s razor automation of HMEANT thatcombines standard shallow semantic parsing witha simple maximum weighted bipartite matching al-gorithm for aligning semantic frames.
The match-ing criterion is based on lexical similarity scoringof the semantic role fillers through a simple con-text vector model which can readily be trained us-ing any publicly available large monolingual cor-pus.
Sentence level correlation analysis, followingstandard NIST MetricsMATR protocol, shows thatthis fully automated version of HMEANT achievessignificantly higher Kendall correlation with hu-man adequacy judgments than BLEU, NIST, ME-TEOR, PER, CDER, WER, or TER.
Furthermore,we demonstrate that performing the semantic framealignment automatically actually tends to be just asgood as performing it manually.
Despite its highperformance, fully automated MEANT is still ableto preserve HMEANT?s virtues of simplicity, repre-sentational transparency, and inexpensiveness.1 IntroductionWe introduce the first fully automatic semantic-frame-based MT evaluation metric capable of outperformingall other commonly used automatic metrics like BLEU,NIST, METEOR, PER, CDER, WER, and TER for eval-uating translation adequacy.
This work, MEANT, can beseen as a fully automated version of HMEANT, which isa human metric, introduced by Lo and Wu (2011b).
De-spite its high performance, MEANT is still able to pre-serve HMEANT?s virtues of Occam?s razor simplicity,representational transparency, and inexpensiveness.For the past decade, MT evaluation has relied heavilyon inexpensive automatic metrics such as BLEU (Pap-ineni et al, 2002), NIST (Doddington, 2002), METEOR(Banerjee and Lavie, 2005), PER (Tillmann et al, 1997),CDER (Leusch et al, 2006), WER (Nie?en et al, 2000),and TER (Snover et al, 2006).
In large part, this is be-cause automatic metrics significantly shorten the evalua-tion cycle by providing a fast, easy and cheap quantita-tive evaluation which can be effectively incorporated intomodern SMT training methods.Despite the fact that HMEANT, a human metric re-cently proposed by Lo and Wu (2011b,c,d), was shownto reflect translation adequacy more accurately than allof these automatic metrics, it is unfortunately infeasibleto incorporate the HMEANT metrics directly into SMTtraining methods, due to the non-automatic processes of(1) semantic parsing and (2) aligning semantic frames.In this paper we introduce an automatic metric in whichboth the semantic parsing and the alignment of semanticframes are fully automated.
Our aim is to show that evenwith full automation, this new metric still outperforms allthe previous automatic metrics mentioned, thus provid-ing a foundation for future incorporation into the trainingof SMT to drive system improvements in providing moreadequate translation output.N-gram oriented automatic MT evaluation metrics likeBLEU perform well at capturing translation fluency, andranking overall systems with respect to each other whentheir scores are averaged over entire documents or cor-pora.
However, they do not fare so well in ranking trans-lations of individual sentences.
As MT systems improve,the n-gram based evaluation metrics have begun to showtheir limits.
State-of-the-art MT systems are often able tooutput translations containing roughly the correct words,while failing to convey important aspects of the meaningof the input sentence.
Cases where BLEU strongly dis-agrees with human judgment of translation quality were243reported in large scale MT evaluation tasks by Callison-Burch et al (2006) and Koehn and Monz (2006).Motivated by the goal of addressing the weaknessesof n-gram oriented automatic MT evaluation metrics atevaluating translation adequacy, the HMEANT metricassesses translation utility by matching the basic eventstructure?
?who did what to whom, when, where andwhy?
(Pradhan et al, 2004)?representing the centralmeaning conveyed by sentences.
As mentioned above,however, HMEANT requires humans to manually anno-tate semantic frames in the reference and machine trans-lations, and then to align the semantic frames?makingit difficult to incorporate HMEANT as an objective func-tion in the MT system training, evaluating, and optimiz-ing cycle.We argue in this paper that both the human seman-tic parsing and the semantic frame alignment tasks per-formed within HMEANT can be successfully automatedto produce a state-of-the-art automatic metric.
Moreover,we show that the spirit of Occam?s razor can be preservedeven for the semantic frame alignment, by demonstratingthe effectiveness of a simple maximum weighted bipar-tite matching algorithm based on the lexical similarity be-tween semantic frames.
In addition, we show empiricallythat performing this semantic frame alignment automati-cally tends to be just as good as performing it manually.Our results indicate that MEANT, the fully automaticversion of HMEANT, achieves levels of correlation withhuman adequacy judgment (in our experiments, approx-imately 0.37) which significantly outperforms the com-monly used automatic metrics BLEU, NIST, METEOR,PER, CDER, WER, and TER (in our experiments, rang-ing between 0.20 and 0.29).2 Related Work2.1 Automatic lexical similarity based metricsBLEU (Papineni et al, 2002) remains the most widelyused MT evaluation metric despite the fact that a num-ber of large scale meta-evaluations (Callison-Burch etal., 2006; Koehn and Monz, 2006) report cases where itstrongly disagrees with human judgments of translationaccuracy.
Other lexical similarity based automatic MTevaluation metrics, like NIST (Doddington, 2002), ME-TEOR (Banerjee and Lavie, 2005), PER (Tillmann et al,1997), CDER (Leusch et al, 2006), WER (Nie?en et al,2000), and TER (Snover et al, 2006), also perform wellin capturing translation fluency, but share the same prob-lem that although evaluation with these metrics can bedone very quickly at low cost, their underlying assump-tionthat a good translation is one that shares thesame lexical choices as the reference translationis notjustified semantically.
Lexical similarity does not ade-quately reflect similarity in meaning.Generating a translation that contains roughly the cor-rect words may be necessary?but is far from sufficient?to preserve the essence of the meaning.
We argue that atranslation metric that reflects meaning similarity needsto be based on similarity of semantic structure, and notmerely flat lexical similarity.2.2 HMEANT (human SRL based metric)As mentioned above, despite the fact that the semi-automatic HMEANT metric recently proposed by Lo andWu (2011b,c,d) shows a higher correlation with humanadequacy judgments than all commonly used automaticMT evaluation metrics, as with other human metrics likeHTER (Snover et al, 2006), it is unfortunately infeasibleto incorporate the HMEANT metrics directly into SMTtraining methods.
HMEANT requires non-automaticmanual steps of (1) semantic parsing and (2) aligningsemantic frames.
Monolingual (or bilingual) annotatorsmust label the semantic roles in both the reference andmachine translations, and then to align the semantic pred-icates and role fillers in the MT output to the referencetranslations.
These annotations allow HMEANT to thenlook at the aligned role fillers, and aggregate the trans-lation accuracy for each role.
In the spirit of Occam?srazor and representational transparency, the HMEANTscore is defined simply in terms of a weighted f-scoreover these aligned predicates and role fillers.
More pre-cisely, HMEANT is defined as follows:1.
Human annotators annotate the shallow semanticstructures of both the references and MT output.2.
Human judges align the semantic frames betweenthe references and MT output by judging the cor-rectness of the predicates.3.
For each pair of aligned semantic frames,(a) Human judges determine the translation cor-rectness of the semantic role fillers.
(b) Human judges align the semantic role fillersbetween the reference and MT output accord-ing to the correctness of the semantic rolefillers.4.
Compute the weighted f-score over the matchingrole labels of these aligned predicates and rolefillers.mi ?#tokens filled in aligned frame i of MTtotal #tokens in MTri ?#tokens filled in aligned frame i of REFtotal #tokens in REFMi,j ?
total # ARG j of aligned frame i in MTRi,j ?
total # ARG j of aligned frame i in REFCi,j ?
# correct ARG j of aligned frame i in MTPi,j ?
# partially correct ARG j of aligned frame i in MT244Figure 1: Examples of human semantic frame annotation.
Semantic parses of the Chinese input and the English reference trans-lation are from the Propbank gold standard.
The MT output is semantically parsed by monolingual lay annotators according to theHMEANT guidelines.
There are no semantic frames for MT3 because there is no predicate.precision =?i miwpred+?j wj(Ci,j+wpartialPi,j)wpred+?j wjMi,j?i mirecall =?i riwpred+?j wj(Ci,j+wpartialPi,j)wpred+?j wjRi,j?i riwhere mi and ri are the weights for frame, i, in theMT/REF respectively.
These weights estimate the degreeof contribution of each frame to the overall meaning ofthe sentence.
Mi,j and Ri,j are the total counts of argu-ment of type j in frame i in the MT and REF respec-tively.
Ci,j and Pi,j are the count of the correctly andpartial correctly translated argument of type j in frame iin the MT output.
Figure 1 shows examples of human se-mantic frame annotation on reference and machine trans-lations as used in HMEANT.
Table 1 shows examples ofhuman judges?
decisions for semantic frame alignmentand translation correctness for each semantic roles, forthe ?MT2?
output from Figure 1.Unlike HMEANT, MEANT is fully automatic; butnevertheless, it adheres to HMEANT?s principles of Oc-cam?s razor simplicity and representational transparency.These properties crucially facilitate error analysis andcredit/blame assignment that are invaluable for MT sys-tem modeling.Furthermore, being fully automatic, MEANT is evenless expensive than HMEANT, which was already shownby Lo and Wu (2011b,c,d) to be significantly less ex-pensive than HTER.
This makes MEANT a much bet-ter candidate than HMEANT for future incorporation intothe automatic training of SMT systems to drive improve-ments in translation adequacy.2.3 Semantic role labels as features in aggregatemetricsGime?nez and Ma`rquez (2007, 2008) introduced ULC, anautomatic MT evaluation metric that aggregates manytypes of features, including several shallow semantic sim-ilarity features.
However, unlike Lo and Wu (2011b),245Table 1: Example of SRL annotation for the MT2 output from figure 1 along with the human judgements of translation correctnessfor each argument.
*Notice that although the decision made by the human judge for ?in mainland China?
in the reference translationand ?the mainland of China?
in MT2 is ?correct?, nevertheless the HMEANT computation will not count this as a match since theirrole labels do not match.REF roles REF MT2 roles MT2 decisionPRED ceased Action stop matchARG0 their sale ?
?
incorrectARGM-LOC in mainland China Agent the mainland of China correct*ARGM-TMP for almost two months Temporal nearly two months correct?
?
Experiencer SK - 2 products incorrectPRED resumed Action resume matchARG0 sales of complete range of SK- II productsExperiencer in the mainland of China tostop selling nearly two monthsof SK - 2 products salesincorrectARGM-TMP Until after , their sales hadceased in mainland China foralmost two monthsTemporal So far partialARGM-TMP now ?
?
incorrectthe ULC representation is based on flat semantic rolelabel features that do not capture the structural rela-tions in semantic frames, i.e., the predicate-argument re-lations.
Also unlike HMEANT, which weights each se-mantic role type according to its empirically determinedrelative importance to the adequate preservation of mean-ing, ULC uses uniform weights.
Although the automaticULC metric shows an improved correlation with humanjudgment of translation quality (Callison-Burch et al,2007; Gime?nez and Ma`rquez, 2007; Callison-Burch etal., 2008; Gime?nez and Ma`rquez, 2008), it is not com-monly used in large-scale MT evaluation campaigns, per-haps due to its high time cost and/or the difficulty of in-terpreting its score because of its highly complex combi-nation of many heterogeneous types of features.Like system combination approaches, ULC is a vastlymore complex aggregate metric compared to widely usedmetrics like BLEU.
We believe it is important for auto-matic semantic MT evaluation metrics to provide rep-resentational transparency via simple, clear, and trans-parent scoring schemes that are (a) easily human read-able to support error analysis, and (b) potentially directlyusable for automatic credit/blame assignment in tuningtree-structured SMT systems.3 MEANT: A fully automatic semanticMT evaluation metricLike HMEANT, our guiding principle is that a goodtranslation is one that is useful, in the sense that hu-man readers may successfully understand at least the ba-sic event structurewho did what to whom, when, whereand why (Pradhan et al, 2004)representing the centralmeaning of the source utterances.
Whereas HMEANTmeasures this using a f-score of correctly translatedsemantic roles in MT output that are annotated andcompared by monolingual human annotators, MEANTautomates HMEANT as follows (the differences fromHMEANT are italicized):1.
Apply an automatic shallow semantic parser on boththe references and MT output.2.
Apply maximum weighted bipartite matching algo-rithm to align the semantic frames between the ref-erences and MT output by the lexical similarity ofthe predicates.3.
For each pair of aligned semantic frames,(a) Lexical similarity scores determine the similar-ity of the semantic role fillers.
(b) Apply maximum weighted bipartite matchingalgorithm to align the semantic role fillers be-tween the reference and MT output accordingto their lexical similarity.4.
Compute the weighted f-score over the matchingrole labels of these aligned predicates and rolefillers.3.1 Automatic semantic parsingTo automate the process of human semantic role label-ing, we apply an automatic shallow semantic parser onboth the reference and MT output that takes the raw trans-lation as input and outputs the corresponding predicate-argument structure.
We choose to semantically parse thetranslation independently, instead of inducing the parses246Figure 2: Examples of automatic shallow semantic parses.
The Chinese input is parsed by a Chinese automatic shallow semanticparser.
The English reference and machine translations are parsed by an English automatic shallow semantic parser.
There are nosemantic frames for mt3 since there is no predicate.from the input, because it captures the raw meaning con-veyed in the translation rather than predicting the mean-ing conveyed in the translation from the input.
Figure 2shows examples of automatic shallow semantic parses onboth reference and machine translations.3.2 Automatic semantic frame alignmentAfter reconstructing the shallow semantic parse, the man-ual semantic frame alignment process is automated byapplying the maximum weighted bipartite matching algo-rithm where the weights of the edges represent the lexicalsimilarity of the predicates.
A wide range of lexical sim-ilarity measures are available to us, including for exam-ple BLEU, METEOR, cosine similarity based on contextvector models (Dagan, 2000), and so forth.
In Section4, we will show the performance of the fully automaticsemantic MT evaluation metric, MEANT ,couple withdifferent lexical similarity metrics and other commonlyused automatic MT evaluation metrics.
In Section 6, wewill discuss aligning the semantic frames according to allsemantic role fillers, instead of the predicates only.Then, for each pair of aligned semantic frames, we es-timate the similarity of the semantic role fillers by sum-ming all the lexical similarity of all the pairwise combi-nation of tokens between the references and MT output.After obtaining the similarity of the semantic role fillers,we again apply the maximum weighted bipartite match-ing algorithm to align the semantic role fillers betweenthe references and MT output.
Table 2 shows examplesof the human judges?
decisions on semantic frame align-ment and translation correctness for each semantic role inthe ?MT2?
output from Figure 2.3.3 Scoring the semantic similarityAfter aligning the semantic frames automatically, thecomputation of the MEANT score is largely the same asstated in Lo and Wu (2011d), except that we now replacethe counts of correctly and partially correctly translatedsemantic role fillers by the similarity scores of the predi-cates and arguments between the references and MT out-put.mi ?#tokens filled in aligned frame i of MTtotal #tokens in MTri ?#tokens filled in aligned frame i of REFtotal #tokens in REFMi,j ?
total # ARG j of aligned frame i in MTRi,j ?
total # ARG j of aligned frame i in REFSi,pred ?
sim.
of pred of REF and MT in aligned frame iSi,j ?
sim.
of ARG j of REF and MT in aligned frame iprecision =?i miwpredSi,pred+?j wjSi,jwpred+?j wjMi,j?i mirecall =?i riwpredSi,pred+?j wjSi,jwpred+?j wjRi,j?i ri247Table 2: Automatic semantic frame alignment of the MT2 output from figure 2, along with the automatic lexical similarity scoringon translation correctness for each argument.REF roles REF MT2 roles MT2 similarityPRED ceased PRED stop 0.0377ARG0 their sales ?
?
?ARGM-LOC in mainland China ?
?
?ARGM-TMP for almost two months ?
?
??
?
PRED selling ??
?
ARG1 nearly two months of SK ?PRED resumed PRED resumed 1.0ARG1 sales of complete range of SK- II productsARG1 2 products sales 0.0836ARGM-TMP now ARGM-TMP So far 0.0459where mi, ri, Mi,j, Ri,j are defined the same as inHMEANT, and Si,pred and Si,j are the lexical similarities(BLEU, METEOR, cosine similarity based on a contextvector model, and so on, as discussed in the followingsection) of the predicates and arguments of type j be-tween the reference translations and the MT output.4 MEANT outperforms all automaticmetricsWe will first show that the fully automatic semantic MTevaluation metric, MEANT, outperforms all the othercommonly used automatic metrics.4.1 Experimental setupFor assessing lexical similarity, a wide range of lexi-cal similarity scoring models are available.
We describea representative subset of a wide range of experimentswe have performed using all the most typical and com-monly used measures.
On one hand, we report experi-ments with integrating two commonly used MT evalua-tion metrics, BLEU and METEOR, as the lexical simi-larity.
On the other hand, we also report experimentson integrating two common similarity measures?cosinesimilarity measure and min/max with mutual information(Dagan, 2000)?that are based on context vector models,and trained from the Gigaword corpus with window sizesof 3 and 5.The cosine similarity between two sequences of wordtokens, ?
?u and ?
?v , is defined as follows:?
?wx = context vector of word token xwxi = attribute i of context vector??
?wxf(x,wxi) =count(x,wxi)count (wxi)cosine(x, y) =?if(x,wxi)?
f(y, wyi)??if(x,wxi)2?
?if(y, wyi)2cosine(?
?u ,?
?v ) = ?i?jcosine(ui, vj)Using the same definition of wxi, the min/max withmutual information similarity between two sequences ofword tokens, ?
?u and ?
?v , is defined as follows:P (wxi ?
x) =count(x,wxi)?i count(x,wxi)P (wxi) =?y count(y, wxi)?y?j count(y, wxj)MI(x,wxi) = log(P (wxi ?
x)P (wxi))MinMax-MI(x, y) =?imin (MI(x,wxi),MI(y, wyi ))?imax (MI(x,wxi),MI(y, wyi ))MinMax-MI(?
?u ,?
?v ) = ?i?jMinMax-MI(ui, vj)For our benchmark comparison, the evaluation datafor our experiments is the same two sets of sentences,GALE-A and GALE-B that were used in Lo and Wu(2011d), where GALE-A is used for estimating theweight parameters of the metric by optimizing the cor-relation with human adequacy judgment, and then thelearned weights are applied to testing on GALE-B.For the automatic semantic role labeling, we used thepublicly available off-the-shelf shallow semantic parser,ASSERT (Pradhan et al, 2004).The correlation with human adequacy judgments onsentence-level system ranking is assessed by the stan-dard NIST MetricsMaTr procedure (Callison-Burch etal., 2010) using Kendall correlation coefficients.248Table 3: Sentence-level correlation with human adequacy judgment on GALE-A (training) and GALE-B (testing) comparing allcommonly used MT evaluation metrics against our proposed new fully automatic semantic frame based MT evaluation metricintegrated with various lexical similarity scores between semantic role fillers: (a) BLEU, (b) METEOR, (c) cosine similarity and(d) MinMax with mutual information.GALE-A (training) GALE-B (testing)Human metricsHMEANT 0.49 0.27HTER 0.43 0.20Automatic metricsMEANT ?
?- with MinMax-MI on context vector model of window size 3 0.37 0.19- with MinMax-MI on context vector model of window size 5 0.37 0.17- with Cosine on context vector model of window size 3 0.32 0.13- with Cosine on context vector model of window size 5 0.30 0.08- with METEOR 0.17 ?- with BLEU 0.00 ?METEOR 0.20 0.21NIST 0.29 0.09TER 0.20 0.10BLEU 0.20 0.12PER 0.20 0.07WER 0.10 0.11CDER 0.12 0.104.2 ResultsTable 3 shows that MEANT significantly outperforms allthe other automatic MT evaluation metrics when inte-grated with a simple similarity measure based on wordcontext vectors trained from a large monolingual corpus.We can also see that using min/max with mutual infor-mation is significantly better than using cosine similarity.Furthermore, context vector models using a window sizeof 3 appear to be as good or better than those using awindow size of 5.Although the human metrics, HMEANT and HTER,obviously remain superior, MEANT performs far betterthan almost all other automatic metrics.
The only excep-tion is the GALE-B dataset, where METEOR performsmarginally better than MEANT and even HTER.
Dataanalysis shows that the marginally higher correlation ofMETEOR on the GALE-B dataset is a statistical outlier;it is quite rare for a lexically based automatic metric tooutperform even the human-driven HTER metric.Interestingly and somewhat surprisingly, using the n-gram based MT evaluation metrics BLEU and METEORas lexical similarity scores does not work well at all forthis purpose, even on the training data (thus obviating theneed to obtain results on the testing data).
Analysis in-dicates that the reason for this is that variation betweenalternative paraphrasing of the role fillers makes the num-ber of matching n-grams quite small, since there are manysynonyms and few exact consecutive n-gram matches.Table 4: Sentence-level correlation with human adequacy judg-ment on GALE-A (training) and GALE-B (testing) for aligningsematnic frame automatically and manually.Semantic frame alignment GALE-A GALE-BAutomatic 0.37 0.19Manual 0.35 0.17In the following sections, we turn to considering sev-eral questions that naturally arise following these strongresults.5 Don?t align semantic frames manuallyOne obvious question is whether the automatic alignmentof semantic frames degrades MEANT?s accuracy, and ifso, the extent to which it hurts.5.1 Experimental setupTo test this question, we compare the best fully automaticresults of the previous section against a semi-automaticvariant of our proposed metric.
In the semi-automaticvariant, the semantic parsing is still performed automati-cally.
However, the semantic frame alignment is insteaddone manually by human annotators.The rest of the experimental setup is the same as thatused in Section 4.2495.2 ResultsTable 4 shows that performing the alignment of semanticframes automatically is as good?or even better than?doing the alignment manually.
We believe the success ofautomatic semantic frame alignment reflects the high de-gree of reliability of our chosen lexical similarity metric,when the candidates for role fillers are restricted to thefairly small set defined by the sentence pairs.6 Look only at predicates when aligningsemantic framesGiven the positive results of the previous sections, it isworth asking a deeper question: would it further improvethe correlation with human adequacy judgment of themetric if the semantic frames were aligned not only bymatching predicates (as HMEANT did), but in additionby trying to also maximize the match of the semantic rolefillers?The reason to revisit this question is that even thoughLo and Wu (2011a) showed that in the case of HMEANTit is effective for human annotators to align semanticframes according to the predicates only, this could eas-ily be due to the mental challenge for lay annotators tocompare and keep in mind all the semantic role fillers atthe same time.
But in the case of a fully automatic metric,on the other hand, it is easy for an algorithm to computethe individual similarities between all the semantic rolefillers and consider the aggregate similarity when opti-mizing the alignment of semantic frames.Surprisingly, however, the results will show that evenin the automated case, this still does not help improve thecorrelation with human adequacy judgments.6.1 Experimental setupTo align semantic frames using all semantic roles, weaggregate the lexical similarity of all the semantic rolefillers into a semantic frame similarity score.
We exper-iment on two variations of the aggregation function (1)simple linear average of the lexical similarity over thenumber of aligned semantic roles in the frames; or (2) theinverse of the sum of the negative log of the role fillerssimilarity.The rest of the experimental setup is the same as thatused in Section 4.6.2 ResultsTable 5 shows that to align semantic frames, using onlythe lexical similarity of the predicates between the framesin the reference translations and the MT output (0.37Kendall in GALE-A and 0.19 Kendall in GALE-B) ismore robust than either of the two natural ways of ag-gregating the lexical similarity of the aligned semanticrole fillers.
Aggregating by linear average yields a lowerTable 5: Sentence-level correlation with human adequacy judg-ments on GALE-A (training set) and GALE-B (testing set) foraligning semantic frames using predicate only vs. using all se-mantic role fillers aggregated by (1) the linear average of thelexical similarity vs. (2) the inverse of the sum of negative logof the lexical similarity.Frame alignment GALE-A GALE-BPredicate only 0.37 0.19Linear average 0.35 0.10Inverse of sum of neg.
log 0.30 0.170.35 Kendall in GALE-A and 0.10 Kendall in GALE-B.Aggregating by the inverse of the sum of negative logsyields a lower 0.30 Kendall in GALE-A and 0.17 Kendallin GALE-B.What might explain this perhaps surprising result?
Ourconjecture is that aggregating the lexical similarities ofthe semantic role fillers fails to help find better seman-tic frame alignments because the lexical similarities areaggregated with uniform weight across different types ofrole fillers.
Therefore, the aggregation ignores the factthat different types of role types contribute to a widelyvarying degree to the meaning of an entire semanticframe?in reality, some role types are much more impor-tant than others.
However, the complexity of the met-ric would be greatly increased if we added weights foreach semantic roles type for semantic frame alignmentprocess, and this would not be likely to be worthwhilegiven that automatic alignment is already performing aswell as human alignment of semantic frames.7 Don?t word align semantic role fillersAnother question that naturally arises from the positiveresults above is: when aligning the semantic frames,would word-aligning the tokens within role fillers help?Specifically, if we had word alignments for every candi-date pair of role filler strings, we could sum the lexicalsimilarities only between the aligned tokens?instead ofwhat we did above, which was to sum the lexical similar-ities of all pairwise combinations of tokens.However, experimental results will show that, surpris-ingly, to judge the similarity of semantic role fillers,summing the lexical similarities over only word-alignedtokens?instead of all pairwise combinations of tokens?does not help to improve the correlation of the semanticMT evaluation with human adequacy judgment.7.1 Experimental setupTo avoid the danger of aligning a token in one segmentto excessive numbers of tokens in the other segment,we adopt a variant of competitive linking by Melamed(1996).
Competitive linking is a greedy best-first wordalignment algorithm.250Table 6: Sentence-level correlation with human adequacy judg-ments on GALE-A (training set) and GALE-B (testing set) forjudging semantic role fillers similarity using pairwise tokens vs.only aligned tokens.Semantic role filler similarity GALE-A GALE-BAll pairwise tokens 0.37 0.19Only aligned tokens 0.36 0.17The rest of the experimental setup is the same as thatused in Section 4.7.2 ResultsTable 6 shows that, surprisingly, judging semantic rolefiller similarity using only the aligned tokens (selectedby competitive linking word alignment algorithm) doesnot help the correlation with human adequacy judgment.This is surprising as, intuitively, using only the alignedtokens should avoid the introduction of noise in judg-ing the similarity between semantic role fillers becauseit avoids adding in similarities for words within semanticrole fillers whose meanings are not close to each other.How might this outcome be explained?
We conjecturethat the word alignments over-constrain the calculationof segment similarities.
The individual lexical similari-ties are already weighted fairly accurately, so the lexicalsimilarities between words that do not correspond do nothurt since they are already close to zero.
On the otherhand, in cases where the word alignment is ambiguous,it is better to aggregate over different possible pairwisealignments?strictly obeying a hard word alignment un-desirably forces dropping of some individual lexical sim-ilarity scores that are actually relevant.8 ConclusionWe have introduced a new fully automatic semantic MTevaluation metric, MEANT, that is fundamentally basedon semantic frames, that is the first such metric to out-perform all other commonly used automatic MT evalu-ation metrics.
Experimental results following the stan-dard NIST MetricsMATR protocol indicate that our pro-posed metric achieves levels of correlation with humanadequacy judgment (in our experiments, approximately0.37) that significantly outperform BLEU, NIST, ME-TEOR, PER, CDER, WER, and TER (in our experiments,ranging between 0.20 and 0.29).We have also shown in this paper that the spirit of Oc-cam?s razor of HMEANT can be preserved even underfull automation by (1) replacing human semantic role an-notation with automatic shallow semantic parsing and (2)replacing human semantic frame alignment with a simplemaximum weighted bipartite matching algorithm basedon the lexical similarity between semantic frames.
Underanalysis, we have further shown empirically that perform-ing this semantic frame alignment automatically tends tobe just as good as performing it manually.
Furthermore,we have shown surprisingly that (1) for aligning seman-tic frames, using only the similarity of predicates is moreaccurate than also taking into account the similarity of se-mantic role fillers, and (2) to judge similarity between se-mantic role fillers, aggregating similarity of all pairwisecombination of word tokens is more accurate than con-sidering only the similarity of the tokens that obey wordalignments.Papineni et al (2002) stated in their conclusion that?We believe that BLEU will accelerate the MT R&D cy-cle by allowing researchers to rapidly home in on effec-tive modeling ideas.?
since fully automatic metrics allowinexpensive training and tuning of SMT systems.
Devel-opments in the past decade have more than borne witnessto this statement.
However, SMT has progressed to thestage where simple metrics like BLEU are no longer ca-pable of driving progress toward preservation of meaningwith respect to proper event structure.
We believe thatMEANT that rapidly and accurately reflects the transla-tion adequacy of MT output by directly assessing who didwhat to whom, when, where and why is needed to bringMT R&D to a new level of improvement in generatingmore meaningful MT output.AcknowledgmentsThis material is based upon work supported in partby the Defense Advanced Research Projects Agency(DARPA) under BOLT contract no.
HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022and HR0011-06-C-0023; by the European Union un-der the FP7 grant agreement no.
287658; and by theHong Kong Research Grants Council (RGC) researchgrants GRF621008, GRF612806, DAG03/04.EG09,RGC6256/00E, and RGC6083/99E.
Any opinions, find-ings and conclusions or recommendations expressed inthis material are those of the authors and do not necessar-ily reflect the views of the RGC, EU, or DARPA.ReferencesSatanjeev Banerjee and Alon Lavie.
METEOR: An AutomaticMetric for MT Evaluation with Improved Correlation withHuman Judgments.
In Proceedings of the 43th Annual Meet-ing of the Association of Computational Linguistics (ACL-05), pages 65?72, 2005.Chris Callison-Burch, Miles Osborne, and Philipp Koehn.
Re-evaluating the role of BLEU in Machine Translation Re-search.
In Proceedings of the 13th Conference of the Eu-ropean Chapter of the Association for Computational Lin-guistics (EACL-06), pages 249?256, 2006.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
(Meta-) evaluation of251Machine Translation.
In Proceedings of the 2nd Workshopon Statistical Machine Translation, pages 136?158, 2007.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
Further Meta-evaluationof Machine Translation.
In Proceedings of the 3rd Workshopon Statistical Machine Translation, pages 70?106, 2008.Chris Callison-Burch, Philipp Koehn, Christof Monz, Kay Pe-terson, Mark Pryzbocki, and Omar Zaidan.
Findings of the2010 Joint Workshop on Statistical Machine Translation andMetrics for Machine Translation.
In Proceedings of the Joint5th Workshop on Statistical Machine Translation and Met-ricsMATR, pages 17?53, Uppsala, Sweden, 15-16 July 2010.Ido Dagan.
Contextual word similarity.
In Robert Dale, Her-man Moisl, and Harold Somers, editors, Handbook of Nat-ural Language Processing, pages 459?476.
Marcel Dekker,New York, 2000.G.
Doddington.
Automatic Evaluation of Machine TranslationQuality using N-gram Co-occurrence Statistics.
In Proceed-ings of the 2nd International Conference on Human Lan-guage Technology Research (HLT-02), pages 138?145, SanFrancisco, CA, USA, 2002.
Morgan Kaufmann PublishersInc.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
Linguistic Features for Au-tomatic Evaluation of Heterogenous MT Systems.
In Pro-ceedings of the 2nd Workshop on Statistical Machine Trans-lation, pages 256?264, Prague, Czech Republic, June 2007.Association for Computational Linguistics.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
A Smorgasbord of Featuresfor Automatic MT Evaluation.
In Proceedings of the 3rdWorkshop on Statistical Machine Translation, pages 195?198, Columbus, OH, June 2008.
Association for Computa-tional Linguistics.Philipp Koehn and Christof Monz.
Manual and AutomaticEvaluation of Machine Translation between European Lan-guages.
In Proceedings of the Workshop on Statistical Ma-chine Translation, pages 102?121, 2006.Gregor Leusch, Nicola Ueffing, and Hermann Ney.
CDer: Effi-cient MT Evaluation Using Block Movements.
In Proceed-ings of the 13th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL-06), 2006.Chi-kiu Lo and Dekai Wu.
A Radically Simple, Effective An-notation and Alignment Methodology for Semantic FrameBased SMT and MT Evaluation.
In Proceedings of Interna-tional Workshop on Using Linguistic Information for HybridMachine Translation (LiHMT 2011), organized by OpenMT-2., 2011.Chi-kiu Lo and Dekai Wu.
MEANT: An Inexpensive, High-Accuracy, Semi-Automatic Metric for Evaluating Transla-tion Utility based on Semantic Roles.
In Proceedings of theJoint conference of the 49th Annual Meeting of the Associa-tion for Computational Linguistics : Human Language Tech-nologies (ACL-HLT-11), 2011.Chi-kiu Lo and Dekai Wu.
SMT vs. AI redux: How seman-tic frames evaluate MT more accurately.
In Proceedings ofthe 22nd International Joint Conference on Artificial Intelli-gence (IJCAI-11), 2011.Chi-kiu Lo and Dekai Wu.
Structured vs. Flat Semantic RoleRepresentations for Machine Translation Evaluation.
In Pro-ceedings of the 5th Workshop on Syntax and Structure in Sta-tistical Translation (SSST-5), 2011.I.
Dan Melamed.
Automatic construction of clean broad-coverage translation lexicons.
In Proceedings of the 2ndConference of the Association for Machine Translation in theAmericas (AMTA-1996), 1996.Sonja Nie?en, Franz Josef Och, Gregor Leusch, and HermannNey.
A Evaluation Tool for Machine Translation: Fast Eval-uation for MT Research.
In Proceedings of the 2nd Inter-national Conference on Language Resources and Evaluation(LREC-2000), 2000.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
BLEU: A Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics (ACL-02), pages 311?318, 2002.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James H. Mar-tin, and Dan Jurafsky.
Shallow Semantic Parsing Using Sup-port Vector Machines.
In Proceedings of the 2004 Confer-ence on Human Language Technology and the North Amer-ican Chapter of the Association for Computational Linguis-tics (HLT-NAACL-04), 2004.Matthew Snover, Bonnie J. Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
A Study of Translation EditRate with Targeted Human Annotation.
In Proceedings of the7th Conference of the Association for Machine Translation inthe Americas (AMTA-06), pages 223?231, 2006.Christoph Tillmann, Stephan Vogel, Hermann Ney, Arkaitz Zu-biaga, and Hassan Sawaf.
Accelerated DP Based SearchFor Statistical Translation.
In Proceedings of the 5th Euro-pean Conference on Speech Communication and Technology(EUROSPEECH-97), 1997.252
