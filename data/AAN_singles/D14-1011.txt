Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 99?109,October 25-29, 2014, Doha, Qatar.
c?2014 Association for Computational LinguisticsAccurate Word Segmentation and POS Tagging for Japanese Microblogs:Corpus Annotation and Joint Modeling with Lexical NormalizationNobuhiro Kaji??
and Masaru Kitsuregawa??
?National Institute of Information and Communications Technology?Institute of Industrial Science, The University of Tokyo?National Institute of Informatics{kaji, kitsure}@tkl.iis.u-tokyo.ac.jpAbstractMicroblogs have recently receivedwidespread interest from NLP re-searchers.
However, current tools forJapanese word segmentation and POStagging still perform poorly on microblogtexts.
We developed an annotated corpusand proposed a joint model for over-coming this situation.
Our annotatedcorpus of microblog texts enables notonly training of accurate statistical modelsbut also quantitative evaluation of theirperformance.
Our joint model with lexicalnormalization handles the orthographicdiversity of microblog texts.
We con-ducted an experiment to demonstratethat the corpus and model substantiallycontribute to boosting accuracy.1 IntroductionMicroblogs, such as Twitter1 and Weibo2, have re-cently become an important target of NLP tech-nology.
Since microblogs offer an instant way ofposting textual messages, they have been givenincreasing attention as valuable sources for suchactions as mining opinions (Jiang et al., 2011)and detecting sudden events such as earthquake(Sakaki et al., 2010).However, many studies have reported that cur-rent NLP tools do not perform well on microblogtexts (Foster et al., 2011; Gimpel et al., 2011).
Inthe case of Japanese text processing, the most se-rious problem is poor accuracy of word segmen-tation and POS tagging.
Since these two tasksare positioned as the fundamental step in the textprocessing pipeline, their accuracy is vital for alldownstream applications.1https://twitter.com2https://www.weibo.com1.1 Development of annotated corpusThe main obstacle that makes word segmentationand POS tagging in the microblog domain chal-lenging is the lack of annotated corpora.
Becausecurrent annotated corpora are from other domains,such as news articles, it is difficult to train modelsthat perform well on microblog texts.
Moreover,system performance cannot be evaluated quantita-tively.We remedied this situation by developing an an-notated corpus of Japanese microblogs.
We col-lected 1831 sentences from Twitter and manuallyannotated these sentences with word boundaries,POS tags, and normalized forms of words (c.f.,Section 1.2).We, for the first time, present a comprehen-sive empirical study of Japanese word segmenta-tion and POS tagging on microblog texts by us-ing this corpus.
Specifically, we investigated howwell current models trained on existing corporaperform in the microblog domain.
We also ex-plored performance gains achieved by using ourcorpus for training, and by jointly performing lex-ical normalization (c.f., Section 1.2).1.2 Joint modeling with lexical normalizationOrthographic diversity in microblog texts causes aproblem when training a statistical model for wordsegmentation and POS tagging.
Microblog textsfrequently contain informal words that are spelledin a non-standard manner, e.g., ?oredi (already)?,?b4 (before)?, and ?talkin (talking)?
(Han andBaldwin, 2011).
Such words, hereafter referredto as ill-spelled words, are so productive that theyconsiderably increase the vocabulary size.
Thismakes training of statistical models difficult.We address this problem by jointly conductinglexical normalization.
Although a wide varietyof ill-spelled words are used in microblog texts,many can be normalized into well-spelled equiva-lents, which conform to standard rules of spelling.99A joint model with lexical normalization is ableto handle orthographic diversity by exploiting in-formation obtainable from the well-spelled equiv-alents.The proposed joint model was empirically eval-uated on the microblog corpus we developed.
Ourexperiment demonstrated that the proposed modelcan perform word segmentation and POS tag-ging substantially better than current state-of-the-art models.1.3 SummaryContributions of this paper are the following:?
We developed a microblog corpus that en-ables not only training of accurate models butalso quantitative evaluation for word segmen-tation and POS tagging in the microblog do-main.3?
We propose a joint model with lexical nor-malization for better handling of ortho-graphic diversity in microblog texts.
In par-ticular, we present a new method of trainingthe joint model using a partially annotatedcorpus (c.f., Section 7.4).?
We, for the first time, present a comprehen-sive empirical study of word segmentationand POS tagging for microblogs.
The experi-mental results demonstrated that both the mi-croblog corpus and joint model greatly con-tributes to training accurate models for wordsegmentation and POS tagging.The remainder of this paper is organized as fol-lows.
Section 2 reviews related work.
Section 3discusses the task of lexical normalization and in-troduces terminology.
Section 4 presents our mi-croblog corpus and results of our corpus analysis.Section 5 presents an overview of our joint modelwith lexical normalization, and Sections 6 and 7provide details of the model.
Section 8 presentsexperimental results and discussions, and Section9 presents concluding remarks.2 Related WorkResearchers have recently developed various mi-croblog corpora annotated with rich linguistic in-formation.
Gimpel et al.
(2011) and Foster etal.
(2011) annotated English microblog posts with3Please contact the first author for this corpus.POS tags.
Han and Baldwin (2011) released a mi-croblog corpus annotated with normalized formsof words.
A Chinese microblog corpus annotatedwith word boundaries was developed for SIGHANbakeoff (Duan et al., 2012).
However, there areno microblog corpora annotated with word bound-aries, POS tags, and normalized sentences.There has been a surge of interest in lexical nor-malization with the advent of microblogs (Han andBaldwin, 2011; Liu et al., 2012; Han et al., 2012;Wang and Ng, 2013; Zhang et al., 2013; Ling etal., 2013; Yang and Eisenstein, 2013; Wang et al.,2013).
However, these studies did not address en-hancing word segmentation.Wang et al.
(2013) proposed a method of jointill-spelled word recognition and word segmenta-tion.
With their method, informal spellings aremerely recognized and not normalized.
Therefore,they did not investigate how to exploit the infor-mation obtainable from well-spelled equivalentsto increase word segmentation accuracy.Some studies also explored integrating the lexi-cal normalization process into word segmentationand POS tagging (Ikeda et al., 2009; Sasano et al.,2013).
A strength of our joint model is that it usesrich character-level and word-level features usedin state-of-the-art models of joint word segmenta-tion and POS tagging (Kudo et al., 2004; Neubiget al., 2011; Kaji and Kitsuregawa, 2013).
Thanksto these features, our model performed much bet-ter than Sasano et al.
?s system, which is the onlypublicly available system that jointly conducts lex-ical normalization, in the experiments (see Section8).
Another advantage is that our model can betrained on a partially annotated corpus.
Further-more, we present a comprehensive evaluation interms of precision and recall on our microblog cor-pus.
Such an evaluation has not been conducted inprevious work due to the lack of annotated cor-pora.43 Lexical Normalization TaskThis section explains the task of lexical normal-ization addressed in this paper.
Since lexical nor-malization is a relatively new research topic, thereare no precise definitions of a lexical normaliza-tion task that are widely accepted by researchers.4Very recently, Saito et al.
(2014) conducted similar em-pirical evaluation on microblog corpus.
However, they usedbiased dataset, in which every sentence includes at least oneill-spelled words.100Table 1: Examples of our target ill-spelled wordsand their well-spelled equivalents.
Phonemes areshown between slashes.
English translations areprovided in parentheses.Ill-spelled word Well-spelled equivalent???
/sugee/ ???
/sugoi/ (great)??
/modoro/ ???
/modorou/ (going to return)??????
/umaiiii/ ???
/umai/ (yummy)Therefore, it is important to clarify our task settingbefore discussing our joint model.3.1 Target ill-spelled wordsMany studies on lexical normalization havepointed out that phonological factors are deeplyinvolved in the process of deriving ill-spelledwords.
Xia et al.
(2006) investigated a Chi-nese chat corpus and reported that 99.2% of theill-spelled words were derived by phonetic map-ping from well-spelled equivalents.
Wang andNg (2013) analyzed 200 Chinese messages fromWeibo and 200 English SMS messages from theNUS SMS corpus (How and Kan, 2005).
Theiranalysis revealed that most ill-spelled words werederived from well-spelled equivalents based onpronunciation similarity.On top of these investigations, we focused onill-spelled words that are derived by phonologi-cal mapping from well-spelled words by assum-ing that such ill-spelled words are dominant inJapanese microblogs as well.
We also assumethat these ill-spelled words can be normalized intowell-spelled equivalents on a word-to-word basis,as assumed in a previous study (Han and Baldwin,2011).
The validity of these two assumptions isempirically assessed in Section 4.Table 1 lists examples of our target ill-spelledwords, their well-spelled equivalents, and theirphonemes.
The ill-spelled word in the first rowis formed by changing the continuous two vowelsfrom /oi/ to /ee/.
This type of change in pronun-ciation is often observed in Japanese spoken lan-guage.
The second row presents contractions.
Thelast vowel character ???
/u/ of the well-spelledword is dropped.
The third row illustrates wordlengthening.
The ill-spelled word is derived by re-peating the vowel character ???
/i/.3.2 TerminologyWe now introduce the terminology that will beused throughout the remainder of this paper.
Theterm word surface form (or surface form for short)is used to refer to the word form observed in anactual text, while word normal form (or normalform) refers to the normalized word form.
Notethat surface forms of well-spelled words are al-ways identical to their normal forms.It is possible that the word surface form and nor-mal form have distinct POS tags, although they areidentical in most cases.
Take the ill-spelled word ????
/modoro/ as an example (the second row ofTable 1).
According to the JUMAN POS tag set,5POS of its surface form is CONTRACTED VERB,while that of its normal form is VERB.6 To handlesuch a case, we strictly distinguish between thesetwo POS tags by referring to them as surface POStags and normal POS tags, respectively.Given these terms, the tasks addressed in thispaper can be stated as follows.
Word segmenta-tion is a task of segmenting a sentence into a se-quence of word surface forms, and POS taggingis a task of providing surface POS tags.
The taskof joint lexical normalization, word segmentation,and POS tagging is to map a sentence into a se-quence of quadruplets: word surface form, surfacePOS tag, normal form, and normal POS tag.4 Microblog CorpusThis section introduces our microblog corpus.
Wefirst explain the process of developing the corpusthen present the results of our agreement study andcorpus analysis.4.1 Data collection and annotationThe corpus was developed by manually annotatingtext messages posted to Twitter.The posts to be annotated were collected as fol-lows.
171,386 Japanese posts were collected usingthe Twitter API7 on December 6, 2013.
Amongthese, 1000 posts were randomly selected thenmanually split into sentences.
As a result, we ob-tained 1831 sentences as a source of the corpus.Two human participants annotated the 1831sentences with surface forms and surface POStags.
Since much effort has already been done toannotate corpora with this information, the anno-tation process here follows the guidelines used to5http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN6In this paper, we use simplified POS tags for explana-tion purposes.
Remind that these tags are different from theoriginal ones defined in JUMAN POS tag set.7https://stream.twitter.com/1.1/statuses/sample.json101develop such corpora in previous studies (Kuro-hashi and Nagao, 1998; Hashimoto et al., 2011).The two participants also annotated ill-spelledwords with their normal forms and normal POStags.
Although this paper targets only infor-mal phonological variations (c.f., Section 3),other types of ill-spelled words were also anno-tated to investigate their frequency distributionin microblog texts.
Specifically, besides infor-mal phonological variations, spelling errors andTwitter-specific abbreviations were annotated.
Asa result, 833 ill-spelled words were identified (Ta-ble 2).
They were all annotated with normal formsand normal POS tags.4.2 Agreement studyWe investigated the inter-annotator agreement tocheck the reliability of the annotation.
During theannotation process, the two participants collabo-ratively annotated around 90% of the sentences(specifically, 1647 sentences) with normal formsand normal POS tags, and elaborated an annota-tion guideline through discussion.
They then inde-pendently annotated the remaining 184 sentences(1431 words), which were used for the agreementstudy.
Our annotation guideline is shown in thesupplementary material.We first explored the extent to which thetwo participants agreed in distinguishing betweenwell-spelled words and ill-spelled words.
For thistask, we observed Cohen?s kappa of 0.96 (almostperfect agreement).
This results show that it iseasy for humans to distinguish between these twotypes of words.Next, we investigated whether the two partici-pants could give ill-spelled words with the samenormal forms and normal POS tags.
For this pur-pose, we regarded the normal forms and normalPOS tags annotated by one participant as goldstan-dards and calculated precision and recall achievedby the other participant.
We observed moder-ate agreement between the two participants: 70%(56/80) precision and 73% (56/76) recall.
Wemanually analyzed the conflicted examples andfound that there were more than one acceptablenormal form in many of these cases.
Therefore,we would like to note that the precision and recallreported above are rather pessimistic estimations.4.3 AnalysisWe conducted corpus analysis to confirm the fea-sibility of our approach.Table 2: Frequency distribution over three types ofill-spelled words in corpus.Type FrequencyInformal phonological variation 804 (92.9%)Spelling error 27 (3.1%)Twitter-specific abbreviation 34 (3.9%)Total 865 (100%)Table 2 illustrates that phonological variationsconstitute a vast majority of ill-spelled words inJapanese microblog texts.
In addition, analysisof the 804 phonological variations showed that793 of them can be normalized into single words.These represent the validity of the two assump-tions we made in Section 3.1.We then investigated whether lexical normaliza-tion can decrease the number of out-of-vocabularywords.
For the 793 ill-spelled words, we countedhow many of their surface forms and normalforms were not registered in the JUMAN dictio-nary.8 The result suggests that 411 (51.8%) and74 (9.3%) are not registered in the dictionary.
Thisindicates the effectiveness of lexical normalizationfor decreasing out-of-vocabulary words.5 Overview of Joint ModelThis section gives an overview of our joint modelwith lexical normalization for accurate word seg-mentation and POS tagging.5.1 Lattice-based approachA lattice-based approach has been commonlyadopted to perform joint word segmentation andPOS tagging (Jiang et al., 2008; Kudo et al., 2004;Kaji and Kitsuregawa, 2013).
In this approach, aninput sentence is transformed into a word latticein which the edges are labeled with surface POStags (Figure 1).
Given such a lattice, word seg-mentation and POS tagging can be performed atthe same time by traversing the lattice.
A discrim-inative model is typically used for the traversal.An advantage of this approach is that, while thelattice can represent an exponentially large num-ber of candidate analyses, it can be quickly tra-versed using dynamic programming (Kudo et al.,2004; Kaji and Kitsuregawa, 2013) or beam search(Jiang et al., 2008).
In addition, a discriminativemodel allows the use of rich word-level featuresto find the correct analysis.8http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN102?
?
?
?
?
?1RXQ9HUE1RXQ1RXQ 1RXQ 1RXQ3DUWLFOH6XIIL[,QSXWVHQWHQFH: ??????
7ROLYHLQ7RN\RPHWURSROLV:RUGODWWLFH:Figure 1: Example lattice (Kudo et al., 2004; Kajiand Kitsuregawa, 2013).
Circle and arrow repre-sent node and edge, respectively.
Bold edges rep-resent correct analysis.,QSXWVHQWHQFH:???????
1RWWRXQGHUVWDQG(QJOLVK:RUGODWWLFH:1RUPDOL]HGVHQWHQFH:??
???
??1RXQ??1RXQ1RXQ??1RXQ6XIIL[??6XIIL[9HUE??
? 9HUE??
??
?
??3DUWLFOH?3DUWLFOH6XIIL[?
?6XIIL[Figure 2: Lattice used to perform joint task.
Nor-mal forms and normal POS tags are shown inparentheses.
As indicated by dotted arrows, nor-malized sentence can be obtained by concatenat-ing normal forms associated with edges in correctanalysis.We propose extending the lattice-based ap-proach to jointly perform lexical normalization,word segmentation, and POS tagging.
We trans-form an input sentence into a word lattice in whichthe edges are labeled with not only surface POStags but normal forms and normal POS tags (Fig-ure 2).
By traversing such a lattice, the threetasks can be performed at the same time.
This ap-proach can not only exploit rich information ob-tainable from word normal forms, but also achieveefficiency similar to the original lattice-based ap-proach.5.2 IssuesIssues on how to develop this lattice-based ap-proach is detailed in Sections 6 and 7.Section 6 describes how to generate a word lat-tice from an input sentence.
This is done us-ing a hybrid approach that combines a statisticalmodel and normalization dictionary.
The normal-ization dictionary is specifically a list of quadru-Table 3: Normalization dictionary.
Columns rep-resent entry ID, surface form, surface POS, normalform, and normal POS, respectively.ID Surf.
Surf.
POS Norm.
Norm.
POSA ???
ADJECTIVE ???
ADJECTIVEB ???
ADJECTIVE ???
ADJECTIVEC ???
VERB ???
VERBD ??
CONTR.
VERB ???
VERBE ???
ADJECTIVE ???
ADJECTIVEF ??????
ADJECTIVE ???
ADJECTIVETable 4: Tag dictionary.ID Surf.
form Surf.
POSa ???
(great) ADJECTIVEb ???
(going to return) VERBc ??
(gonna return) CONTR.
VERBd ???
(yummy) ADJECTIVEplets: word surface form, surface POS tag, normalform, and normal POS tag (Table 3).Section 7 describes a discriminative model forthe lattice traversal.
Our feature design as well astwo training methods are presented.6 Word Lattice GenerationIn this section, we first describe a method of con-structing a normalization dictionary then present amethod of generating a word lattice from an inputsentence.6.1 Construction of normalization dictionaryAlthough large-scale normalization dictionariesare difficult to obtain, tag dictionaries, which listpairs of word surface forms and their surface POStags (Table 4), are widely available in many lan-guages including Japanese.
Therefore, we use anexisting tag dictionary to construct the normaliza-tion dictionary.Due to space limitations, we give only a briefoverview of our construction method, omitting itsdetails.
We note that our method uses hand-craftedrules similar to those used in (Sasano et al., 2013);hence, the proposal of this method is not an im-portant contribution.
To make our experimentalresults reproducible, our normalization dictionary,as well as a tool for constructing it, is released assupplementary material.Our method of constructing the normalizationdictionary takes three steps.
The following ex-plains each step using Tables 3 and 4 as runningexamples.103Step 1 A tag dictionary generally contains asmall number of ill-spelled words, although well-spelled words constitute a vast majority.
We iden-tify such ill-spelled words by using a manually-tailored list of surface POS tags indicative of in-formal spelling (e.g., CONTRACTED VERB).
Forexample, entry (c) in Table 4 is identified as anill-spelled word in this step.Step 2 The tag dictionary is augmented withnormal forms and normal POS tags to constructa small normalization dictionary.
For ill-spelledwords identified in step 1, the normal forms andnormal POS tags are determined by hand-craftedrules.
For example, the normal form is derived byappending the vowel character ???
/u/ to the sur-face form, if the surface POS tag is CONTRACTEDVERB.
This rule derives entry (D) in Table 3 fromentry (c) in Table 4.
For well-spelled words, onthe other hand, the normal forms and normal POStags are simply set the same as the surface formsand surface POS tags.
For example, entries (A),(C), and (E) in Table 3 are generated from entries(a), (b), and (d) in Table 4, respectively.Step 3 Because the normalization dictionaryconstructed in step 2 contains only a few ill-spelled words, it is expanded in this step.
For thispurpose, we use hand-crafted rules to derive ill-spelled words from the entries already registeredin the normalization dictionary.
Some rules aretaken from (Sasano et al., 2013), while the othersare newly tailored.
In Table 3, for example, entry(B) is derived from entry (A) by applying the rulethat substitutes ????
/goi/ with ????
/gee/.A small problem that arises in step 3 is how tohandle lengthened words, such as entry (F) in Ta-ble 3.
While lengthened words can be easily de-rived using simple rules (Brody and Diakopoulos,2011; Sasano et al., 2013), such rules infinitelyincrease the number of entries because an unlim-ited number of lengthened words can be derivedby repeating characters.
To address this problem,no lengthened words are added to the normaliza-tion dictionary in step 3.
We instead use rulesto skip repetitive characters in an input sentencewhen performing dictionary match.6.2 A hybrid approachA word lattice is generated using both a statisti-cal method (Kaji and Kitsuregawa, 2013) and thenormalization dictionary.We begin by generating a word lattice which en-codes only word surface forms and surface POStags (c.f., Figure 1) using the statistical methodproposed by Kaji and Kitsuregawa (2013).
Inter-ested readers may refer to their paper for details.Each edge in the lattice is then labeled with nor-mal forms and normal POS tags.
Note that a sin-gle edge can have more than one candidate normalform and normal POS tag.
In such a case, newedges are accordingly added to the lattice.The edges are labeled with normal forms andnormal POS tags in the following manner.
First,every edge is labeled with a normal form andnormal POS tag that are identical with the sur-face form and surface POS tag.
This is based onour observation that most words are well-spelledones.
The edge is not provided with further nor-mal forms and normal POS tags, if the normaliza-tion dictionary contains a well-spelled word thathas the same surface form as the edge.
Otherwise,we allow the edge to have all pairs of normal formsand normal POS tags that are obtained by using thenormalization dictionary.7 Discriminative Lattice TraversalThis section explains a discriminative model fortraversing the word lattice.
The lattice traversalwith a discriminative model can formally be writ-ten as(w, t,v, s) = argmax(w,t,v,s)?L(x)f(x,w, t,v, s) ?
?.Here, x denotes an input sentence, w, t, v, and sdenote a sequence of word surface forms, surfacePOS tags, normal forms, and normal POS tags, re-spectively, L(x) represents a set of candidate anal-yses represented by the word lattice, and f(?)
and?
are feature and weight vectors.We now describe features, a decoding method,and two training methods.7.1 FeaturesWe use character-level and word-level featuresused for word segmentation and POS tagging in(Kaji and Kitsuregawa, 2013).
To take advan-tage of joint model with lexical normalization, theword-level features are extracted from not onlysurface forms but also normal forms.
See (Kajiand Kitsuregawa, 2013) for the original features.In addition, several new features are introducedin this paper.
We use the quadruplets (wi, ti, vi, si)104and pairs of surface and normal POS tags (ti, si)as binary features to capture probable mappingsbetween ill-spelled words and their well-spelledequivalents.
We use another binary feature indi-cating whether a quadruplet (wi, ti, vi, si) is reg-istered in the normalization dictionary.
Also, weuse a bigram language model feature, which pre-vents sentences from being normalized into un-grammatical and/or incomprehensible ones.
Thelanguage model features are associated with nor-malized bigrams, (vi?1, si?1, vi, si), and take asthe values the logarithmic frequency log10(f +1),where f represents the bigram frequency (Kaji andKitsuregawa, 2011).
Since it is difficult to obtaina precise value of f , it is approximated by the fre-quency of the surface bigram, (wi?1, ti?1, wi, ti),calculated from a large raw corpus automaticallyanalyzed using a system of joint word segmenta-tion and POS tagging.
See Section 8.1 for the rawcorpus and system used in the experiments.7.2 DecodingIt is easy to find the best analysis (w, t,v, s)among the candidates represented by the word lat-tice.
Although we use several new features, wecan still locate the best analysis by using the samedynamic programming algorithm as in previousstudies (Kudo et al., 2004; Kaji and Kitsuregawa,2013).7.3 Training on a fully annotated corpusIt is straightforward to train the joint model pro-vided with a fully annotated corpus, which is la-beled with word surface forms, surface POS tags,normal forms, and normal POS tags.We use structured perceptron (Collins, 2002)for the training (Algorithm 1).
The training be-gins by initializing ?
as a zero vector (line 1).It then reads the annotated corpus C (line 2-9).Given a training example, (x,w, t,v, s) ?
C, thealgorithm locates the best analysis, ( ?w, ?t, ?v, ?s),based on the current weight vector (line 4).
Ifthe best analysis differs from the oracle analy-sis, (w, t,v, s), the weight vector is updated (line5-7).
After going through the annotated corpusm times (m=10 in our experiment), the averagedweight vector is returned (line 10).7.4 Training on a partially annotated corpusAlthough the training with the perceptron algo-rithm requires a fully annotated corpus, it is labor-intensive to fully annotate sentences.
This consid-Algorithm 1 Perceptron training1: ?
?
02: for i = 1 .
.
.
m do3: for (x,w, t,v, s) ?
C do4: ( ?w, ?t, ?v, ?s)?
DECODING(x, ?
)5: if (w, t,v,s) 6= ( ?w, ?t, ?v, ?s) then6: ?
?
?
+ f (x,w, t,v, s)?
f (x, ?w, ?t, ?v, ?s)7: end if8: end for9: end for10: return AVERAGE(?
)Algorithm 2 Latent perceptron training1: ?
?
02: for i = 1 .
.
.
m do3: for (x,w, t) ?
C?
do4: ( ?w, ?t, ?v, ?s)?
DECODING(x, ?
)5: (w, t, ?v, ?s)?
CONSTRAINEDDECODING(x,?
)6: if w 6= ?w or t 6= ?t then7: ?
?
?
+ f (x,w, t, ?v, ?s)?
f (x, ?w, ?t, ?v, ?s)8: end if9: end for10: end for11: return AVERAGE(?
)eration motivates us to explore training our modelwith less supervision.
We specifically explore us-ing a corpus annotated with only word boundariesand POS tags.We use the latent perceptron algorithm (Sun etal., 2013) to train the joint model from such a par-tially annotated corpus (Algorithm 2).
In this sce-nario, a training example is a sentence x pairedwith a sequence of word surface forms w and sur-face POS tags t (c.f., line 3).
Similarly to theperceptron algorithm, we locate the best analy-sis ( ?w, ?t, ?v, ?s) for a given training example, (line4).
We also locate the best analysis, (w, t, ?v, ?s),among those having the same surface forms w andsurface POS tags t as the training example (line5).
If the surface forms and surface POS tags ofthe former analysis differ from the annotations ofthe training example, parameter is updated by re-garding the latter analysis as an oracle (line 6-8).8 ExperimentsWe conducted experiments to investigate how themicroblog corpus and joint model contribute toimproving accuracy of word segmentation andPOS tagging in the microblog domain.8.1 SettingWe constructed the normalization dictionary fromthe JUMAN dictionary 7.0.9 While JUMAN dic-9http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN105tionary contains 750,156 entries, the normaliza-tion dictionary contains 112,458,326 entries.Some features taken from the previous study(Kaji and Kitsuregawa, 2013) are induced using atag dictionary.
For this we used two tag dictionar-ies.
One is JUMAN dictionary 7.0 and the otheris a tag dictionary constructed by listing surfaceforms and surface POS tags in the normalizationdictionary.To compute the language model features, onebillion sentences from Twitter posts were analyzedusing MeCab 0.996.10 We used all bigrams ap-pearing at least 10 times in the auto-analyzed sen-tences.8.2 Results of word segmentation and POStaggingWe first investigated the performance of modelstrained on an existing annotated corpus form newstexts.
For this experiment, our joint model aswell as three state-of-the-art models (Kudo et al.,2004)11(Neubig et al., 2011)12(Kaji and Kitsure-gawa, 2013) were trained on Kyoto UniversityText corpus 4.0 (Kurohashi and Nagao, 1998).Since this training corpus is not annotated withnormal forms and normal POS tags, our modelwas trained using the latent perceptron.
Table5 summarizes the word-level F1-scores (Kudo etal., 2004) on our microblog corpus.
The twocolumns represent the results for word segmenta-tion (Seg) and joint word segmentation and POStagging (Seg+Tag), respectively.We also conducted 5-fold crossvalidation onour microblog corpus to evaluate performance im-provement when these models are trained on mi-croblog texts (Table 6).
In addition to the modelsin Table 5, results of a rule-based system (Sasanoet al., 2013)13 and our joint model trained usingthe perceptron algorithm are also presented.
No-tice that Proposed and Proposed (latent) repre-sent our model trained using perceptron and latentperceptron, respectively.From Tables 5 and 6, as expected, we see thatthe models trained on news texts performed poorlyon microblog texts, while their performance sig-nificantly boosted when trained on the microblogtexts.
This demonstrates the importance of corpusannotation.
An exception was Kudo04.
Its perfor-10https://code.google.com/p/mecab11https://code.google.com/p/mecab12http://www.phontron.com/kytea/13http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMANTable 5: Performance of models trained on thenews articles.Seg Seg+TagKudo04 81.8 71.0Neubig11 80.5 69.1Kaji13 83.2 73.1Proposed (latent) 83.0 73.9mance improved only slightly, even when it wastrained on the microblog texts.
We believe this isbecause their model uses dictionary-based rules toprune candidate analyses; thus, it could not per-form well in the microblog domain, where out-of-vocabulary words are abundant.Table 6 also illustrates that our joint modelsachieved F1-score better than the state-of-the-artmodels trained on the microblog texts.
Thisshows that modeling the derivation process of ill-spelled words makes training easier.
We con-ducted bootstrap resampling (with 1000 samples)to investigate the significance of the improvementsachieved with our joint model.
The results showedthat all improvements over the baselines were sta-tistically significant (p < 0.01).
The differencebetween Proposed and Proposed (latent) werealso statistically significant (p < 0.01).The results of Proposed (latent) are interest-ing.
Table 5 illustrates that our joint model per-forms well even when it is trained on a news cor-pus that rarely contains ill-spelled words and isnot at all annotated with normal forms and nor-mal POS tags.
This indicates the robustness of ourtraining method and the importance of modelingword derivation process in the microblog domain.In Table 6, we observed that Proposed (latent),which uses less supervision, performed better thanProposed.
The reason for this will be examinedlater.In summary, we can conclude that both the mi-croblog corpus and joint model significantly con-tribute to training accurate models for word seg-mentation and POS tagging in the microblog do-main.8.3 Results of lexical normalizationWhile the main goal with this study was to en-hance word segmentation and POS tagging in themicroblog domain, it is interesting to explore howwell our joint model can normalize ill-spelledwords.Table 7 illustrates precision, recall, and F1-score for the lexical normalization task.
To put106Table 6: Results of 5-fold cross-validation on mi-croblog corpus.Seg Seg+TagKudo04 82.7 71.7Neubig11 88.6 75.9Kaji13 90.9 82.1Sasano13 82.7 73.3Proposed 91.3 83.2Proposed (latent) 91.4 83.7Table 7: Results of lexical normalization task interms of precision, recall, and F1-score.Precision Recall F1Neubig11 69.2 35.9 47.3Proposed 77.1 44.6 56.6Proposed (latent) 53.7 24.7 33.9the results into context, we report on the baselineresults of a tagging model proposed by Neubig etal.
(2011).
This baseline conducts lexical normal-ization by regarding it as two independent taggingtasks (i.e., tasks of tagging normal forms and nor-mal POS tags).
The result of the baseline model isalso obtained using 5-fold crossvalidation.Table 7 illustrates that Proposed performed sig-nificantly better than the simple tagging model,Neubig11.
This suggests the effectiveness of ourjoint model.
On the other hand, Proposed (latent)performed poorly in this task.
From this result, wecan argue that Proposed (latent) can achieve su-perior performance in word segmentation and POStagging (Table 6) because it gave up correctly nor-malizing ill-spelled words, focusing on word seg-mentation and POS tagging.The experimental results so far suggest the fol-lowing strategy for training our joint model.
If ac-curacy of word segmentation and POS tagging isthe main concern, we can use the latent percep-tron.
This approach has the advantage of beingable to use a partially annotated corpus.
On theother hand, if performance of lexical normaliza-tion is crucial, we have to use the standard percep-tron algorithm.8.4 Error analysisWe manually analyzed erroneous outputs and ob-served several tendencies.We found that a word lattice sometimes missedthe correct output.
Such an error was, for example,observed in a sentence including many ill-spelledwords, e.g., ??????????????
(benervous about what other people think!
)?, wherethe part ????????
is in ill-spelled words.Improving the lattice generation algorithm is con-sidered necessary to achieve further performancegain.Even if the correct analysis appears in the wordlattice, our model sometimes failed to handleill-spelled words, incorrectly analyzing them asout-of-vocabulary words.
For example, the pro-posed method treated the phrase ????????
(snack time)?
as a single out-of-vocabulary word,even though the correct analysis was found in theword lattice.
More sophisticated features wouldbe required to accurately distinguish between ill-spelled and out-of-vocabulary words.9 Conclusion and Future WorkWe presented our attempts towards developing anaccurate model for word segmentation and POStagging in the microblog domain.
To this end, we,for the first time, developed an annotated corpusof microblogs.
We also proposed a joint modelwith lexical normalization to handle orthographicdiversity in the microblog text.
Intensive exper-iments demonstrated that we could successfullyimprove the performance of word segmentationand POS tagging on microblog texts.
We believethis study will have a large practical impact on avarious research areas that target microblogs.One limitation of our approach is that it cannothandle certain types of ill-spelled words.
For ex-ample, the current model cannot handle the casesin which there are no one-to-one-mappings be-tween well-spelled and ill-spelled words.
Also,our model cannot handle spelling errors, whichare considered relatively frequent in the microblogthan news domains.
The treatment of these prob-lems would require further research.Another future research is to speed-up ourmodel.
Since the joint model with lexical normal-ization significantly increases the search space,it is much slower than the original lattice-basedmodel for word segmentation and POS tagging.AcknowledgmentsThe authors would like to thank Naoki Yoshinagafor his help in developing the microblog corpus aswell as fruitful discussions.107ReferencesSamuel Brody and Nicholas Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
usingword lengthening to detect sentiment in microblogs.In Proceedings of EMNLP, pages 562?570.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP, pages 1?8.Huiming Duan, Zhifang Sui, Ye Tian, and Wenjie Li.2012.
The CIPS-SIGHAN CLP 2012 Chinese wordsegmentation on microblog corpora bakeoff.
In Pro-ceedings of the Second CIPS-SIGHAN Joint Conr-erence on Chinese Language Processing, pages 35?40.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Joseph Le Roux, Stephen Hogan, Joakim Nivre,Deirdre Hogan, and Josef van Genabith.
2011.#hardtoparse: POS tagging and parsing the twit-terverse.
In Proceedings of AAAI Workshop onAnalysing Microtext, pages 20?25.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein,Michael Heilman, Dani Yogatama, Jeffrey Flanigan,and Noah A. Smith.
2011.
Part-of-speech taggingfor twitter: Annotation, features, and experiments.In Proceedings of ACL, pages 42?47.Bo Han and Timothy Baldwin.
2011.
Lexical normal-ization of short text messages: Makin sens a #twitter.In Proceedings of ACL, pages 368?378.Bo Han, Paul Cook, and Timothy Baldwin.
2012.
Au-tomatically constructing a normalisation dictionaryfor microblogs.
In Proceedings of EMNLP-CoNLL,pages 421?432.Chikara Hashimoto, Sadao Kurohashi, Daisuke Kawa-hara, Keiji Shinzato, and Masaaki Nagata.
2011.Construction of a blog corpus with syntac-tic, anaphoric, and semantic annotations (inJapanese).
Journal of Natural Language Process-ing, 18(2):175?201.Yijiu How and Min-Yen Kan. 2005.
Optimizing pre-dictive text entry for short message service on mo-bile phones.
In Proceedings of Human ComputerInterfaces International.Kazushi Ikeda, Tadashi Yanagihara, Kazunori Mat-sumoto, and Yasuhiro Takishima.
2009.
Unsuper-vised text normalization approach for morphologicalanalysis of blog documents.
In Proceedings of Aus-tralasian Joint Conference on Advances in ArtificialIntelligence, pages 401?411.Wenbin Jiang, Haitao Mi, and Qun Liu.
2008.
Wordlattice reranking for Chinese word segmentation andpart-of-speech tagging.
In Proceedings of Coling,pages 385?392.Long Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, andTiejun Zhao.
2011.
Target-dependent Twitter sen-timent classification.
In Proceedings of ACL, pages151?160.Nobuhiro Kaji and Masaru Kitsuregawa.
2011.
Split-ting noun compounds via monolingual and bilingualparaphrasing: A study on Japanese Katakana words.In Proceedings of EMNLP, pages 959?969.Nobuhiro Kaji and Masaru Kitsuregawa.
2013.
Effi-cient word lattice generation for joint word segmen-tation and POS tagging in Japanese.
In Proceedingsof IJCNLP, pages 153?161.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields toJapanese morphological analysis.
In Proceedings ofEMNLP, pages 230?237.Sadao Kurohashi and Makoto Nagao.
1998.
Building aJapanese parsed corpus while improving the parsingsystem.
In Proceedings of LREC, pages 719?724.Wang Ling, Chris Dyer, Alan W Black, and IsabelTrancoso.
2013.
Paraphrasing 4 microblog normal-ization.
In Proceedings of EMNLP, pages 73?84.Xiaohua Liu, Ming Zhou, Xiangyang Zhou,Zhongyang Fu, and Furu Wei.
2012.
Jointinference of named entity recognition and normal-ization for tweets.
In Proceedings of ACL, pages526?535.Graham Neubig, Yousuke Nakata, and Shinsuke Mori.2011.
Pointwise prediction for robust adaptableJapanese morphological analysis.
In Proceedings ofACL, pages 529?533.Itsumi Saito, Kugatsu Sadamitsu, Hisako Asano, andYoshihiro Matsuo.
2014.
Morphological analysisfor Japanese noisy text based on character-level andword-level normalization.
In Proceedings of COL-ING, pages 1773?1782.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquak shakes Twitter users: real-timeevent detection by social sensors.
In Proceedingsof WWW, pages 851?860.Ryohei Sasano, Sadao Kurohashi, and Manabu Oku-mura.
2013.
A simple approach to unknown wordprocessing in Japanese morphological analysis.
InProceedings of IJCNLP, pages 162?170.Xu Sun, Takuya Matsuzaki, and Wenjie Li.
2013.Latent structured perceptrons for large-scale learn-ing with hidden information.
IEEE Transactionson Knowledge and Data Engineering, 25(9):2063?2075.Aobo Wang and Min-Yen Kan. 2013.
Mining informallanguage from Chinese microtext: Joint word recog-nition and segmentation.
In Proceedings of ACL,pages 731?741.108Pidong Wang and Hwee Tou Ng.
2013.
A beam-searchdecoder for normalization of social media text withapplication to machine translation.
In Proceedingsof NAACL, pages 471?481.Aobo Wang, Min-Yen Kan, Daniel Andrade, TakashiOnishi, and Kai Ishikawa.
2013.
Chinese informalword normalization: an experimental study.
In Pro-ceedings of IJCNLP, pages 127?135.Yunqing Xia, Kam-Fai Wong, and Wenjie Li.
2006.
Aphonetic-based approach to Chinese chat text nor-malization.
In Proceedings of ACL, pages 993?1000.Yi Yang and Jacob Eisenstein.
2013.
A log-linearmodel for unsupervised text normalization.
In Pro-ceedings of EMNLP, pages 61?72.Congle Zhang, Tyler Baldwin, Howard Ho, BennyKimelfeld, and Yunyao Li.
2013.
Adaptive parser-centric text normalization.
In Proceedings of ACL,pages 1159?1168.109
