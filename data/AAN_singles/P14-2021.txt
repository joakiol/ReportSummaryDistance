Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 124?129,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsRobust Logistic Regression using Shift ParametersJulie Tibshirani and Christopher D. ManningStanford UniversityStanford, CA 94305, USA{jtibs, manning}@cs.stanford.eduAbstractAnnotation errors can significantly hurtclassifier performance, yet datasets areonly growing noisier with the increaseduse of Amazon Mechanical Turk and tech-niques like distant supervision that auto-matically generate labels.
In this paper,we present a robust extension of logisticregression that incorporates the possibil-ity of mislabelling directly into the objec-tive.
This model can be trained throughnearly the same means as logistic regres-sion, and retains its efficiency on high-dimensional datasets.
We conduct exper-iments on named entity recognition dataand find that our approach can provide asignificant improvement over the standardmodel when annotation errors are present.1 IntroductionAlmost any large dataset has annotation errors,especially those complex, nuanced datasets com-monly used in natural language processing.
Low-quality annotations have become even more com-mon in recent years with the rise of Amazon Me-chanical Turk, as well as methods like distant su-pervision and co-training that involve automati-cally generating training data.Although small amounts of noise may not bedetrimental, in some applications the level canbe high: upon manually inspecting a relation ex-traction corpus commonly used in distant super-vision, Riedel et al (2010) report a 31% falsepositive rate.
In cases like these, annotation er-rors have frequently been observed to hurt perfor-mance.
Dingare et al (2005), for example, con-duct error analysis on a system to extract relationsfrom biomedical text, and observe that over halfof the system?s errors could be attributed to incon-sistencies in how the data was annotated.
Simi-larly, in a case study on co-training for natural lan-guage tasks, Pierce and Cardie (2001) find thatthe degradation in data quality from automatic la-belling prevents these systems from performingcomparably to their fully-supervised counterparts.In this work we argue that incorrect exam-ples should be explicitly modelled during train-ing, and present a simple extension of logistic re-gression that incorporates the possibility of mis-labelling directly into the objective.
Following atechnique from robust statistics, our model intro-duces sparse ?shift parameters?
to allow datapointsto slide along the sigmoid, changing class if ap-propriate.
It has a convex objective, is well-suitedto high-dimensional data, and can be efficientlytrained with minimal changes to the logistic re-gression pipeline.In experiments on a large, noisy NER dataset,we find that this method can provide an improve-ment over standard logistic regression when anno-tation errors are present.
The model also providesa means to identify which examples were misla-belled: through experiments on biological data,we demonstrate how our method can be used toaccurately identify annotation errors.
This robustextension of logistic regression shows particularpromise for NLP applications: it helps accountfor incorrect labels, while remaining efficient onlarge, high-dimensional datasets.2 Related WorkMuch of the previous work on dealing with anno-tation errors centers around filtering the data be-fore training.
Brodley and Friedl (1999) introducewhat is perhaps the simplest form of supervisedfiltering: they train various classifiers, then recordtheir predictions on a different part of the train setand eliminate contentious examples.
Sculley andCormack (2008) apply this approach to spam fil-tering with noisy user feedback.One obvious issue with these methods is that thenoise-detecting classifiers are themselves trained124on noisy labels.
Unsupervised filtering tries toavoid this problem by clustering training instancesbased solely on their features, then using the clus-ters to detect labelling anomalies (Rebbapragadaet al, 2009).
Recently, Intxaurrondo et al (2013)applied this approach to distantly-supervised rela-tion extraction, using heuristics such as the num-ber of mentions per tuple to eliminate suspiciousexamples.Unsupervised filtering, however, relies on theperhaps unwarranted assumption that exampleswith the same label lie close together in featurespace.
Moreover filtering techniques in generalmay not be well-justified: if a training exampledoes not fit closely with the current model, it isnot necessarily mislabelled.
It may represent animportant exception that would improve the over-all fit, or appear unusual simply because we havemade poor modelling assumptions.Perhaps the most promising approaches arethose that directly model annotation errors, han-dling mislabelled examples as they train.
Thisway, there is an active trade-off between fitting themodel and identifying suspected errors.
Bootkra-jang and Kaban (2012) present an extension oflogistic regression that models annotation errorsthrough flipping probabilities.
While intuitive, thisapproach has shortcomings of its own: the objec-tive function is nonconvex and the authors notethat local optima are an issue, and the model canbe difficult to fit when there are many more fea-tures than training examples.There is a growing body of literature on learn-ing from several annotators, each of whom may beinaccurate (Bachrach et al, 2012; Raykar et al,2009).
It is important to note that we are consid-ering a separate, and perhaps more general, prob-lem: we have only one source of noisy labels, andthe errors need not come from the human annota-tors, but could be introduced through contamina-tion or automatic labelling.The field of ?robust statistics?
seeks to developestimators that are not unduly affected by devi-ations from the model assumptions (Huber andRonchetti, 2009).
Since mislabelled points areone type of outlier, this goal is naturally relatedto our interest in dealing with noisy data, and itseems many of the existing techniques would berelevant.
A common strategy is to use a modi-fied loss function that gives less influence to pointsfar from the boundary, and several models along-4 -2 0 2 40.00.20.40.60.81.0original sigmoidstandard LRrobust LRFigure 1: Fit resulting from a standard vs. robustmodel, where data is generated from the dashedsigmoid and negative labels flipped with probabil-ity 0.2.these lines have been proposed (Ding and Vish-wanathan., 2010; Masnadi-Shirazi et al, 2010).Unfortunately these approaches require optimiz-ing nonstandard, often nonconvex objectives, andfail to give insight into which datapoints are mis-labelled.In a recent advance, She and Owen (2011)demonstrate that introducing a regularized ?shiftparameter?
per datapoint can help increase the ro-bustness of linear regression.
Candes et al (2009)propose a similar approach for principal compo-nent analysis, while Wright and Ma (2009) ex-plore its effectiveness in sparse signal recovery.
Inthis work we adapt the technique to logistic re-gression.
To the best of our knowledge, we arethe first to experiment with adding ?shift param-eters?
to logistic regression and demonstrate thatthe model is especially well-suited to the type ofhigh-dimensional, noisy datasets commonly usedin NLP.3 ModelRecall that in binary logistic regression, the prob-ability of an example xibeing positive is modeledasg(?Txi) =11 + e?
?Txi.For simplicity, we assume the intercept term hasbeen folded into the weight vector ?, so ?
?
Rm+1where m is the number of features.Following She and Owen (2011), we proposethe following robust extension: for each datapointi = 1, .
.
.
, n, we introduce a real-valued shift pa-125rameter ?iso that the sigmoid becomesg(?Txi+ ?i) =11 + e??Txi?
?i.Since we believe that most examples are correctlylabelled, we L1-regularize the shift parameters toencourage sparsity.
Letting yi?
{0, 1} be the la-bel for datapoint i and fixing ?
?
0, our objectiveis now given byl(?, ?)
=n?i=1[yilog g(?Txi+ ?i) (1)+ (1?
yi) log(1?
g(?Txi+ ?i))]?
?n?i=1|?i|.These parameters ?ilet certain datapoints shiftalong the sigmoid, perhaps switching from oneclass to the other.
If a datapoint i is correctly an-notated, then we would expect its corresponding?ito be zero.
If it actually belongs to the posi-tive class but is labelled negative, then ?imight bepositive, and analogously for the other direction.One way to interpret the model is that it al-lows the log-odds of select datapoints to beshifted.
Compared to models based on label-flipping, where there is a global set of flippingprobabilities, our method has the advantage of tar-geting each example individually.It is worth noting that there is no difficulty inregularizing the ?
parameters as well.
For exam-ple, if we choose to use an L1penalty then ourobjective becomesl(?, ?)
=n?i=1[yilog g(?Txi+ ?i) (2)+ (1?
yi) log(1?
g(?Txi+ ?i))]?
?m?j=1|?j| ?
?n?i=1|?i|.Finally, it may seem concerning that we haveintroduced a new parameter for each datapoint.But in many applications the number of featuresalready exceeds n, so with proper regularization,this increase is actually quite reasonable.3.1 TrainingNotice that adding these shift parameters is equiv-alent to introducing n features, where the ith newfeature is 1 for datapoint i and 0 otherwise.
Withthis observation, we can simply modify the fea-ture matrix and parameter vector and train the lo-gistic model as usual.
Specifically, we let ?
?=(?0, .
.
.
, ?m, ?1, .
.
.
, ?n) and X?= [X|In] so thatthe objective (1) simplifies tol(??)
=n?i=1[yilog g(?
?Tx?i)+ (1?
yi) log(1?
g(??Tx?i))]?
?m+n?j=m+1|??
(j)|.Upon writing the objective in this way, we imme-diately see that it is convex, just as standard L1-penalized logistic regression is convex.3.2 TestingTo obtain our final logistic model, we keep onlythe ?
parameters.
Predictions are then made asusual:I{g(?
?Tx) > 0.5}.3.3 Selecting Regularization ParametersThe parameter ?
from equation (1) would nor-mally be chosen through cross-validation, but ourset-up is unusual in that the training set may con-tain errors, and even if we have a designated devel-opment set it is unlikely to be error-free.
We foundin simulations that the errors largely do not inter-fere in selecting ?, so in the experiments below wecross-validate as normal.Notice that ?
has a direct effect on the numberof nonzero shifts ?
and hence the suspected num-ber of errors in the training set.
So if we have in-formation about the noise level, we can directlyincorporate it into the selection procedure.
For ex-ample, we may believe the training set has no morethan 15% noise, and so would restrict the choiceof ?
during cross-validation to only those valueswhere 15% or fewer of the estimated shift param-eters are nonzero.We now consider situations in which the ?
pa-rameters are regularized as well.
Assume, for ex-ample, that we use L1-regularization as in equa-tion (2), so that we now need to optimize over both?
and ?.
We perform the following simple proce-dure:1.
Cross-validate using standard logistic regres-sion to select ?.2.
Fix this value for ?, and cross-validate usingthe robust model to find the best choice of ?.126method suspects identified false positivesAlon et al (1999) T2 T30 T33 T36 T37 N8 N12 N34 N36Furey et al (2000) ?
?
?
?
?
?Kadota et al (2003) ?
?
?
?
?
T6, N2Malossini et al (2006) ?
?
?
?
?
?
?
T8, N2, N28, N29Bootkrajang et al (2012) ?
?
?
?
?
?
?Robust LR ?
?
?
?
?
?
?Table 1: Results of various error-identification methods on the colon cancer dataset.
The first row liststhe samples that are biologically confirmed to be suspicious, and each other row gives the output froman automatic detection method.
Bootkrajang et al report confidences, so we threshold at 0.5 to obtainthese results.4 ExperimentsWe conduct two sets of experiments to assess theeffectiveness of the approach, in terms of bothidentifying mislabelled examples and producingaccurate predictions.4.1 Contaminated DataOur first experiment is centered around a biologi-cal dataset with suspected labelling errors.
Calledthe colon cancer dataset, it contains the expres-sion levels of 2000 genes from 40 tumor and 22normal tissues (Alon et al, 1999).
There is evi-dence in the literature that certain tissue samplesmay have been cross-contaminated.
In particular,5 tumor and 4 normal samples should have theirlabels flipped.In this experiment, we examine the model?sability to identify mislabelled training examples.Because there are many more features than data-points and it is likely that not all genes are relevant,we choose to place an L1penalty on ?.Using glmnet, an R package for training reg-ularized models (Friedman et al, 2009), we se-lect ?
and ?
using cross-validation.
Looking atthe resulting values for ?, we find that only 7 ofthe shift parameters are nonzero and that each onecorresponds to a suspicious datapoint.
As furtherconfirmation, the signs of the gammas correctlymatch the direction of the mislabelling.
Comparedto previous attempts to automatically detect errorsin this dataset, our approach identifies at least asmany suspicious examples but with no false posi-tives.
A detailed comparison is given in Table 1.Although Bootkrajang and Kaban (2012) are quiteaccurate, it is worth noting that due to its noncon-vexity, their model needed to be trained 20 timesto achieve these results.4.2 Manually Annotated DataWe now consider the problem of named entityrecognition (NER) to evaluate how our model per-forms in a large-scale prediction task.
In tradi-tional NER, the goal is to determine whether eachword is a person, organization, location, or not anamed entity (?other?).
Since our model is binary,we concentrate on the task of deciding whether aword is a person or not.
(This task does not triv-ially reduce to finding the capitalized words, as themodel must distinguish between people and othernamed entities like organizations).For training, we use a large, noisy NER datasetcollected by Jenny Finkel.
The data was createdby taking various Wikipedia articles and givingthem to five Amazon Mechanical Turkers to anno-tate.
Few to no quality controls were put in place,so that certain annotators produced very noisy la-bels.
To construct the train set we chose a Turkerwho was about average in how much he disagreedwith the majority vote, and used only his annota-tions.
Negative examples are subsampled to bringthe class ratio to a reasonable level, for a total of200,000 negative and 24,002 positive examples.We find that in 0.4% of examples, the majorityagreed they were negative but the chosen annota-tor marked them positive, and 7.5% were labelledpositive by the majority but negative by the an-notator.
Note that we still include examples forwhich there was no majority consensus, so thesenoise estimates are quite conservative.We evaluate on the English development test setfrom the CoNLL shared task (Tjong Kim Sang andMeulder, 2003).
This data consists of news arti-cles from the Reuters corpus, hand-annotated byresearchers at the University of Antwerp.We extract a set of features using Stanford?sNER pipeline (Finkel et al, 2005).
This set was127model precision recall F1standard 76.99 85.87 81.19flipping 76.62 86.28 81.17robust 77.04 90.47 83.22Table 2: Performance of standard vs. robust logis-tic regression in the Wikipedia NER experiment.The flipping model refers to the approach fromBootkrajang and Kaban (2012).chosen for simplicity and is not highly engineered?
it largely consists of lexical features such as thecurrent word, the previous and next words in thesentence, as well as character n-grams and vari-ous word shape features.
With a total of 393,633features in the train set, we choose to use L2-regularization, so that our penalty now becomes12?2m?j=0|?j|2+ ?n?i=1|?i|.This choice is natural as L2is the most commonform of regularization in NLP, and we wish to ver-ify that our approach works for penalties besidesL1.The robust model is fit using Orthant-WiseLimited-Memory Quasi Newton (OWL-QN), atechnique for optimizing an L1-penalized objec-tive (Andrew and Gao, 2007).
We tune bothmodels through 5-fold cross-validation to obtain?2= 1.0 and ?
= 0.1.
Note that from the waywe cross-validate (first tuning ?
using standard lo-gistic regression, fixing this choice, then tuning ?
)our procedure may give an unfair advantage to thebaseline.We also compare against the algorithm pro-posed in Bootkrajang and Kaban (2012), an exten-sion of logistic regression mentioned in the sectionon prior work.
This approach assumes that eachexample?s true label is flipped with a certain prob-ability before being observed, and fits the resultinglatent-variable model using EM.The results of these experiments are shown inTable 2 as well as Figure 2.
Robust logistic re-gression offers a noticeable improvement over thebaseline, and this improvement holds at essentiallyall levels of precision and recall.
Interestingly, be-cause of the large dimension, the flipping modelconsistently learns that no labels have been flippedand thus does not show a substantial differencewith standard logistic regression.0.5 0.6 0.7 0.8 0.9 1.00.20.30.40.50.60.70.80.9RecallPrecisionnormal LRflipping modelrobust LRFigure 2: Precision-recall curve obtained fromtraining on noisy Wikipedia data and testing onCoNLL.
The flipping model refers to the approachfrom Bootkrajang and Kaban (2012).5 Future WorkA natural direction for future work is to extend themodel to a multi-class setting.
One option is tointroduce a ?
for every class except the negativeone, so that there are n(c ?
1) shift parameters inall.
We could then apply a group lasso, with eachgroup consisting of the ?
for a particular datapoint(Meier et al, 2008).
This way all of a datapoint?sshift parameters drop out together, which corre-sponds to the example being correctly labelled.CRFs and other sequence models could alsobenefit from the addition of shift parameters.Since the extra variables can be neatly folded intothe linear term, convexity is preserved and themodel could essentially be trained as usual.AcknowledgmentsStanford University gratefully acknowledges thesupport of the Defense Advanced ResearchProjects Agency (DARPA) Deep Exploration andFiltering of Text (DEFT) Program under AirForce Research Laboratory (AFRL) contract no.FA8750-13-2-0040.
Any opinions, findings, andconclusion or recommendations expressed in thismaterial are those of the authors and do not nec-essarily reflect the view of the DARPA, AFRL, orthe US government.
We are especially grateful toRob Tibshirani and Stefan Wager for their invalu-able advice and encouragement.128ReferencesU.
Alon, N. Barkai, D. A. Notterman, K. Gish,S.
Ybarra, D. Mack, A. J. Levine.
1999.
Broadpatterns of gene expression revealed by clusteringanalysis of tumor and normal colon tissues probedby oligonucleotide arrays.
National Academy of Sci-ences of the USA.Galen Andrew and Jianfeng Gao.
2007.
Scal-able Training of L1-Regularized Log-Linear Mod-els.
ICML.Yoram Bachrach, Thore Graepel, Tom Minka, andJohn Guiver.
2012.
How To Grade a Test With-out Knowing the Answers: A Bayesian GraphicalModel for Adaptive Crowdsourcing and AptitudeTesting.
arXiv preprint arXiv:1206.6386 (2012).Jakramate Bootkrajang and Ata Kaban.
2012.
Label-noise Robust Logistic Regression and Its Applica-tions.
ECML PKDD.Carla E. Brodley and Mark A. Friedl.
1999.
Identify-ing mislabeled Training Data.
JAIR, 11, 131-167.Emmanuel J. Candes, Xiaodong Li, Yi Ma, JohnWright.
2009.
Robust Principal Component Analy-sis?
arXiv preprint arXiv:0912.3599, 2009.Nan Ding and S. V. N. Vishwanathan.
2010. t-Logisticregression.
NIPS.Shipra Dingare, Malvina Nissim, Jenny Finkel,Christopher Manning, and Claire Grover.
2005.
Asystem for identifying named entities in biomedicaltext: How results from two evaluations reflect onboth the system and the evaluations.
Comparativeand Functional Genomics.
6(1?2), 77-85.Jenny Rose Finkel, Trond Grenager, Christopher Man-ning.
2005.
Incorporating Non-local Informationinto Information Extraction Systems by Gibbs Sam-pling.
ACL.Jerome Friedman, Trevor Hastie, Rob Tibshirani 2009.Regularization Paths for Generalized Linear Modelsvia Coordinate Descent.
Journal of statistical soft-ware, 33(1), 1.Terrence S. Furey, Nello Cristianini, Nigel Duffy,David W. Bednarski, Michel Schummer, DavidHaussler.
2000.
Support vector machine classifi-cation and validation of cancer tissue samples usingmicroarray expression data.
Bioinformatics, 16(10),906-914.Peter J. Huber and Elvezio M. Ronchetti.
2000.
RobustStatistics.
John Wiley & Sons, Inc., Hoboken, NJ.Ander Intxaurrondo, Mihai Surdeanu, Oier Lopez deLacalle, and Eneko Agirre.
2013.
RemovingNoisy Mentions for Distant Supervision.
Congresode la Sociedad Espaola para el Procesamiento delLenguaje Natural.Koji Kadota, Daisuke Tominaga, Yutaka Akiyama,Katsutoshi Takahashi.
2003.
Detecting outlyingsamples in microarray data: A critical assessmentof the effect of outliers on sample.
ChemBio Infor-matics Journal, 3(1), 30-45.Andrea Malossini, Enrico Blanzieri, Raymond T. Ng.2006.
Detecting potential labeling errors in microar-rays by data perturbation.
Bioinformatics, 22(17),2114-2121.Hamed Masnadi-Shirazi, Vijay Mahadevan, and NunoVasconcelos.
2010.
On the design of robust classi-fiers for computer vision.
IEEE International Con-ference Computer Vision and Pattern Recognition.Lukas Meier, Sara van de Geer, Peter Buhlmann.
2008.The group lasso for logistic regression.
Journal ofthe Royal Statistical Society, 70(1), 53-71.David Pierce and Claire Cardie.
2001.
Limitations ofco-training for natural language learning from largedatasets.
EMNLP.Vikas Raykar, Shipeng Yu, Linda H. Zhao, Anna Jere-bko, Charles Florin, Gerardo Hermosillo Valadez,Luca Bogoni, and Linda Moy.
2009.
Supervisedlearning from multiple experts: whom to trust wheneveryone lies a bit.
ICML.Umaa Rebbapragada, Lukas Mandrake, Kiri L.Wagstaff, Damhnait Gleeson, Rebecca Castano,Steve Chien, Carla E. Brodley 2009.
Improv-ing Onboard Analysis of Hyperion Images by Fil-tering mislabelled Training Data Examples.
IEEEAerospace Conference.Sebastian Riedel, Limin Yao, Andrew McCallum.2010.
Modeling Relations and Their Mentions with-out Labelled Text.
ECML PKDD.D.
Sculley and Gordon V. Cormack 2008.
FilteringEmail Spam in the Presence of Noisy User Feed-back.
CEAS.Yiyuan She and Art Owen.
2011.
Outlier DetectionUsing Nonconvex Penalized Regression.
Journal ofthe American Statistical Association, 106(494).Erik F. Tjong Kim Sang, Fien De Meulder.
2003.Introduction to the CoNLL-2003 Shared Task:Language-Independent Named Entity Recognition.CoNLL.John Wright and Yi Ma.
2009.
Dense Error Correctionvia l1-Minimization IEEE Transactions on Informa-tion Theory.129
