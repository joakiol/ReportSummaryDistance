Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 72?79, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsUsing Uneven Margins SVM and Perceptron for Information ExtractionYaoyong Li, Kalina Bontcheva and Hamish CunninghamDepartment of Computer Science, The University of Shefeld, Shefeld, S1 4DP, UK{yaoyong,kalina,hamish}@dcs.shef.ac.ukAbstractThe classification problem derived frominformation extraction (IE) has an imbal-anced training set.
This is particularlytrue when learning from smaller datasetswhich often have a few positive trainingexamples and many negative ones.
Thispaper takes two popular IE algorithms ?SVM and Perceptron ?
and demonstrateshow the introduction of an uneven marginsparameter can improve the results on im-balanced training data in IE.
Our experi-ments demonstrate that the uneven marginwas indeed helpful, especially when learn-ing from few examples.
Essentially, thesmaller the training set is, the more bene-ficial the uneven margin can be.
We alsocompare our systems to other state-of-the-art algorithms on several benchmarkingcorpora for IE.1 IntroductionInformation Extraction (IE) is the process of auto-matic extraction of information about pre-specifiedtypes of events, entities or relations from text suchas newswire articles or Web pages.
IE is useful inmany applications, such as information gathering ina variety of domains, automatic annotations of webpages for Semantic Web, and knowledge manage-ment.A wide range of machine learning techniqueshave been used for IE and achieved state-of-the-artresults, comparable to manually engineered IE sys-tems.
A learning algorithm usually learns a modelfrom a set of documents which have been manuallyannotated by the user.
Then the model can be usedto extract information from new documents.
Manualannotation is a time-consuming process.
Hence, inmany cases learning from small data sets is highlydesirable.
Therefore in this paper we also evaluatethe performance of our algorithms on small amountsof training data and show their learning curve.The learning algorithms for IE can be classifiedbroadly into two main categories: rule learning andstatistical learning.
The former induces a set ofrules from training examples.
There are many rulebased learning systems, e.g.
SRV (Freitag, 1998),RAPIER (Califf, 1998), WHISK (Soderland, 1999),BWI (Freitag and Kushmerick, 2000), and (LP )2(Ciravegna, 2001).
Statistical systems learn a statis-tical model or classifiers, such as HMMs (Freigtagand McCallum, 1999), Maximal Entropy (Chieu andNg., 2002), the SVM (Isozaki and Kazawa, 2002;Mayfield et al, 2003), and Perceptron (Carreras etal., 2003).
IE systems also differ from each otherin the NLP features that they use.
These includesimple features such as token form and capitalisa-tion information, linguistic features such as part-of-speech, semantic information from gazetteer lists,and genre-specific information such as documentstructure.
In general, the more features the systemuses, the better performance it can achieve.This paper concentrates on classifier-based learn-ing for IE, which typically converts the recognitionof each information entity into a set of classificationproblems.
In the framework discussed here, two bi-nary classifiers are trained for each type of informa-tion entity.
One classifier is used for recognising theentity?s start token and the other ?
the entity?s endtoken.72The classification problem derived from IE usu-ally has imbalanced training data, in which positivetraining examples are vastly outnumbered by neg-ative ones.
This is particularly true for smaller datasets where often there are hundreds of negative train-ing examples and only few positive ones.
Two ap-proaches have been studied so far to deal with imbal-anced data in IE.
One approach is to under-samplemajority class or over-sample minority class in orderto obtain a relatively balanced training data (Zhangand Mani, 2003).
However, under-sampling canpotentially remove certain important examples, andover-sampling can lead to over-fitting and a largertraining set.
Another approach is to divide the prob-lem into several sub-problems in two layers, each ofwhich has less imbalanced training set than the orig-inal one (Carreras et al, 2003; Sitter and Daelemans,2003).
The output of the classifier in the first layer isused as the input to the classifiers in the second layer.As a result, this approach needs more classifiers thanthe original problem.
Moreover, the classificationerrors in the first layer will affect the performance ofthe second one.In this paper we explore another approach to han-dle the imbalanced data in IE, namely, adaptingthe learning algorithms for balanced classification toimbalanced data.
We particularly study two popularclassification algorithms in IE, Support Vector Ma-chines (SVM) and Perceptron.SVM is a general supervised machine learningalgorithm, that has achieved state of the art per-formance on many classification tasks, includingNE recognition.
Isozaki and Kazawa (2002) com-pared three commonly used methods for named en-tity recognition ?
the SVM with quadratic kernel,maximal entropy method, and a rule based learningsystem, and showed that the SVM-based system per-formed better than the other two.
Mayfield et al(2003) used a lattice-based approach to named en-tity recognition and employed the SVM with cubickernel to compute transition probabilities in a lattice.Their results on CoNLL2003 shared task were com-parable to other systems but were not the best ones.Previous research on using SVMs for IE adoptsthe standard form of the SVM, which treats posi-tive and negative examples equally.
As a result, theydid not consider the difference between the balancedclassification problems, where the SVM performsquite well, and the imbalanced ones.
Li and Shawe-Taylor (2003) proposes an uneven margins versionof the SVM and shows that the SVM with unevenmargins performs significantly better than the stan-dard SVM on document classification problems withimbalanced training data.
Since the classificationproblem for IE is also imbalanced, this paper inves-tigates the SVM with uneven margins for IE tasksand demonstrates empirically that the uneven mar-gins SVM does have better performance than thestandard SVM.Perceptron is a simple, fast and effective learn-ing algorithm, which has successfully been appliedto named entity recognition (Carreras et al, 2003).The system uses a two-layer structure of classifiersto handle the imbalanced data.
The first layer clas-sifies each word as entity or non-entity.
The secondlayer classifies the named entities identified by thefirst layer in the respective entity classes.
Li et al(2002) proposed another variant of Perceptron, thePerceptron algorithm with uneven margins (PAUM),designed especially for imbalanced data.
In this pa-per we explore the application of PAUM to IE.The rest of the paper is structured as follows.
Sec-tion 2 describes the uneven margins SVM and Per-ceptron algorithms.
Sections 3.1 and 3.2 discussthe classifier-based framework for IE and the exper-imental datasets we used, respectively.
We compareour systems to other state-of-the-art systems on threebenchmark datasets in Section 3.3.
Section 3.4 dis-cusses the effects of the uneven margins parameteron the SVM and Perceptron?s performances.
Finally,Section 4 provides some conclusions.2 Uneven Margins SVM and PerceptronLi and Shawe-Taylor (2003) introduced an unevenmargins parameter into the SVM to deal with imbal-anced classification problems.
They showed that theSVM with uneven margins outperformed the stan-dard SVM on document classification problem withimbalanced training data.
Formally, given a trainingset Z = ((x1, y1), .
.
.
, (xm, ym)),where xi is the n-dimensional input vector and yi (= +1 or ?1) itslabel, the SVM with uneven margins is obtained bysolving the quadratic optimisation problem:minw, b, ?
?w,w?
+ Cm?i=1?i73s.t.
?w,xi?
+ ?i + b ?
1 if yi = +1?w,xi?
?
?i + b ?
??
if yi = ?1?i ?
0 for i = 1, ...,mWe can see that the uneven margins parameter?
was added to the constraints of the optimisationproblem.
?
is the ratio of negative margin to thepositive margin of the classifier and is equal to 1 inthe standard SVM.
For an imbalanced dataset witha few positive examples and many negative ones, itwould be beneficial to use larger margin for positiveexamples than for the negative ones.
Li and Shawe-Taylor (2003) also showed that the solution of theabove problem could be obtained by solving a re-lated standard SVM problem by, for example, usinga publicly available SVM package1 .Perceptron is an on-line learning algorithm forlinear classification.
It checks the training exam-ples one by one by predicting their labels.
If theprediction is correct, the example is passed; other-wise, the example is used to correct the model.
Thealgorithm stops when the model classifies all train-ing examples correctly.
The margin Perceptron notonly classifies every training example correctly butalso outputs for every training example a value (be-fore thresholding) larger than a predefined parameter(margin).
The margin Perceptron has better general-isation capability than the standard Perceptron.
Liet al (2002) proposed the Perceptron algorithm withuneven margins (PAUM) by introducing two marginparameters ?+ and ??
into the updating rules for thepositive and negative examples, respectively.
Sim-ilar to the uneven margins parameter in SVM, twomargin parameters allow the PAUM to handle im-balanced datasets better than both the standard Per-ceptron and the margin Perceptron.
Additionally, itis known that the Perceptron learning will stop afterlimited loops only on a linearly separable trainingset.
Hence, a regularisation parameter ?
is used inPAUM to guarantee that the algorithm would stopfor any training dataset after some updates.
PAUMis simple and fast and performed very well on doc-ument classification, in particularly on imbalancedtraining data.1The SVMlight package version 3.5, available fromhttp://svmlight.joachims.org/, was used to learn the SVM clas-sifiers in our experiments.3 Experiments3.1 Classifier-Based Framework for IEIn the experiments we adopted a classifier-basedframework for applying the SVM and PAUM algo-rithms to IE.
The framework consists of three stages:pre-processing of the documents to obtain featurevectors, learning classifiers or applying classifiers totest documents, and finally post-processing the re-sults to tag the documents.The aim of the preprocessing is to form input vec-tors from documents.
Each document is first pro-cessed using the open-source ANNIE system, whichis part of GATE2 (Cunningham et al, 2002).
Thisproduces a number of linguistic (NLP) features, in-cluding token form, capitalisation information, to-ken kind, lemma, part-of-speech (POS) tag, seman-tic classes from gazetteers, and named entity typesaccording to ANNIE?s rule-based recogniser.Based on the linguistic information, an inputvector is constructed for each token, as we iter-ate through the tokens in each document (includ-ing word, number, punctuation and other symbols)to see if the current token belongs to an informationentity or not.
Since in IE the context of the token isusually as important as the token itself, the featuresin the input vector come not only from the currenttoken, but also from preceding and following ones.As the input vector incorporates information fromthe context surrounding the current token, featuresfrom different tokens can be weighted differently,based on their position in the context.
The weight-ing scheme we use is the reciprocal scheme, whichweights the surrounding tokens reciprocally to thedistance to the token in the centre of the contextwindow.
This reflects the intuition that the nearera neighbouring token is, the more important it isfor classifying the given token.
Our experimentsshowed that such a weighting scheme obtained bet-ter results than the commonly used equal weightingof features (Li et al, 2005).The key part of the framework is to convert therecognition of information entities into binary clas-sification tasks ?
one to decide whether a token is thestart of an entity and another one for the end token.After classification, the start and end tags of the2Available from http://www.gate.ac.uk/74entities are obtained and need to be combined intoone entity tag.
Therefore some post-processingis needed to guarantee tag consistency and to tryto improve the results by exploring other informa-tion.
The currently implemented procedure has threestages.
First, in order to guarantee the consistencyof the recognition results, the document is scannedfrom left to right to remove start tags without match-ing end tags and end tags without preceding starttags.
The second stage filters out candidate enti-ties from the output of the first stage, based on theirlength.
Namely, a candidate entity tag is removedif the entity?s length (i.e., the number of tokens) isnot equal to the length of any entity of the same typein the training set.
The third stage puts together allpossible tags for a sequence of tokens and choosesthe best one according to the probability which wascomputed from the output of the classifiers (beforethresholding) via a Sigmoid function.3.2 The Experimental DatasetsThe paper reports evaluation results on three corporacovering different IE tasks ?
named entity recogni-tion (CoNLL-2003) and template filling or scenariotemplates in different domains (Jobs and CFP).
TheCoNLL-20033 provides the most recent evaluationresults of many learning algorithms on named entityrecognition.
The Jobs corpus4 has also been used re-cently by several learning systems.
The CFP corpuswas created as part of the recent Pascal Challengefor evaluation of machine learning methods for IE5.In detail, we used the English part of the CoNLL-2003 shared task dataset, which consists of 946 doc-uments for training, 216 document for development(e.g., tuning the parameters in learning algorithm),and 231 documents for evaluation (i.e., testing), allof which are news articles taken from the ReutersEnglish corpus (RCV1).
The corpus contains fourtypes of named entities ?
person, location, organ-isation and miscellaneous names.
In the other twocorpora domain-specific information was extractedinto a number of slots.
The Job corpus includes 300computer related job advertisements and 17 slots en-coding job details, such as title, salary, recruiter,computer language, application, and platform.
The3See http://cnts.uia.ac.be/conll2003/ner/4See http://www.isi.edu/info-agents/RISE/repository.html.5See http://nlp.shef.ac.uk/pascal/.CFP corpus consists of 1100 conference or work-shop call for papers (CFP), of which 600 were anno-tated.
The corpus includes 11 slots such as work-shop and conference names and acronyms, work-shop date, location and homepage.3.3 Comparison to Other SystemsNamed Entity Recognition The algorithms areevaluated on the CoNLL-2003 dataset.
Since this setcomes with development data for tuning the learningalgorithm, different settings were tried in order toobtain the best performance on the development set.Different SVM kernel types, window sizes (namelythe number of tokens in left or right side of the tokenat the centre of window), and the uneven marginsparameter ?
were tested.
We found that quadratickernel, window size 4 and ?
= 0.5 produced bestresults on the development set.
These settings wereused in all experiments on the CoNLL-2003 datasetin this paper, unless otherwise stated.
The parametersettings for PAUM described in Li et al (2002), e.g.
?+ = 50, ??
= 1, were adopted in all experimentswith PAUM, unless otherwise stated.Table 1 presents the results of our system usingthree learning algorithms, the uneven margins SVM,the standard SVM and the PAUM on the CONLL-2003 test set, together with the results of threeparticipating systems in the CoNLL-2003 sharedtask: the best system (Florian et al, 2003), theSVM-based system (Mayfield et al, 2003) and thePerceptron-based system (Carreras et al, 2003).Firstly, our uneven margins SVM system per-formed significantly better than the other SVM-based system.
As the two systems are different fromeach other in not only the SVM models used butalso other aspects such as the NLP features and theframework, in order to make a fair comparison be-tween the uneven margins SVM and the standardSVM, we also present the results of the two learningalgorithms implemented in our framework.
We cansee from Table 1 that, under the same experimentalsettings, the uneven margins SVM again performedbetter than the standard SVM.Secondly, our PAUM-based system performedslightly better than the system based on voted Per-ceptron, but there is no significant difference be-tween them.
Note that they adopted different mech-anisms to deal with the imbalanced data in IE (refer75Table 1: Comparison to other systems on CoNLL-2003 corpus: F -measure(%) on each entity type and theoverall micro-averaged F-measure.
The 90% confidence intervals for results of other three systems are alsopresented.
The best performance figures for each entity type and overall appear in bold.System LOC MISC ORG PER OverallOur SVM with uneven margins 89.25 77.79 82.29 90.92 86.30Systems Standard SVM 88.86 77.32 80.16 88.93 85.05PAUM 88.18 76.64 78.26 89.73 84.36Participating Best one 91.15 80.44 84.67 93.85 88.76(?0.7)Systems Another SVM 88.77 74.19 79.00 90.67 84.67(?1.0)Voted Perceptron 87.88 77.97 80.09 87.31 84.30(?0.9)to Section 1).
The structure of PAUM system is sim-pler than that of the voted Perceptron system.Finally, the PAUM system performed worse thanthe SVM system.
On the other hand, training timeof PAUM is only 1% of that for the SVM and thePAUM implementation is much simpler than that ofSVM.
Therefore, when simplicity and speed are re-quired, PAUM presents a good alternative.Template Filling On Jobs corpus our systemsare compared to several state-of-the-art learning sys-tems, which include the rule based systems Rapier(Califf, 1998), (LP )2 (Ciravegna, 2001) and BWI(Freitag and Kushmerick, 2000), the statistical sys-tem HMM (Freitag and Kushmerick, 2000), and thedouble classification system (Sitter and Daelemans,2003).
In order to make the comparison as informa-tive as possible, the same settings are adopted in ourexperiments as those used by (LP )2, which previ-ously reported the highest results on this dataset.
Inparticular, the results are obtained by averaging theperformance in ten runs, using a random half of thecorpus for training and the rest for testing.
Only ba-sic NLP features are used: token form, capitalisationinformation, token types, and lemmas.Preliminary experiments established that theSVM with linear kernel obtained better results thanSVM with quadratic kernel on the Jobs corpus (Liet al, 2005).
Hence we used the SVM with linearkernel in the experiments on the Jobs data.
Note thatPAUM always uses linear kernel in our experiments.Table 2 presents the results of our systems as wellas the other six systems which have been evaluatedon the Jobs corpus.
Note that the results for all the17 slots are available for only three systems, Rapier,(LP )2 and double classification, while the resultsfor some slots were available for the other three sys-tems.
We computed the macro-averaged F1 (themean of the F1 of all slots) for our systems as wellas for the three fully evaluated systems in order tomake a comparison of the overall performance.Firstly, the overall performance of our two sys-tems is significantly better than the other three fullyevaluated systems.
The PAUM system achieves thebest performance on 5 out of the 17 slots.
The SVMsystem performs best on the other 3 slots.
Secondly,the double classification system had much worseoverall performance than our systems and other twofully evaluated systems.
HMM was evaluated onlyon two slots.
It achieved best result on one slot butwas much worse on the other slot than our two sys-tems and some of the others.
Finally, somewhat sur-prisingly, our PAUM system achieves better perfor-mance than the SVM system on this dataset.
More-over, the computation time of PAUM is about 1/3 ofthat of the SVM.
Hence, the PAUM system performsquite satisfactory on the Jobs corpus.Our systems were also evaluated by participatingin a Pascal challenge ?
Evaluating Machine Learn-ing for Information Extraction.
The evaluation pro-vided not only the CFP corpus but also the linguisticfeatures for all tokens by pre-processing the docu-ments.
The main purpose of the challenge was toevaluate machine learning algorithms based on thesame linguistic features.
The only compulsory taskis task1, which used 400 annotated documents fortraining and other 200 annotated documents for test-ing.
See Ireson and Ciravegna (2005) for a shortoverview of the challenge.
The learning methods ex-plored by the participating systems included LP 2,HMM, CRF, SVM, and a variety of combinations76Table 2: Comparison to other systems on the jobs corpus: F1 (%) on each entity type and overall perfor-mance as macro-averaged F1.
Standard deviations for the MA F1 of our systems are presented in parenthe-sis.
The highest score on each slot and overall performance appears in bold.Slot SVM PAUM (LP )2 Rapier DCs BWI HMM semi-CRFId 97.7 97.4 100 97.5 97 100 ?
?Title 49.6 53.1 43.9 40.5 35 50.1 57.7 40.2Company 77.2 78.4 71.9 70.0 38 78.2 50.4 60.9Salary 86.5 86.4 62.8 67.4 67 ?
?
?Recruiter 78.4 81.4 80.6 68.4 55 ?
?
?State 92.8 93.6 84.7 90.2 94 ?
?
?City 95.5 95.2 93.0 90.4 91 ?
?
?Country 96.2 96.5 81.0 93.2 92 ?
?
?Language 86.9 87.3 91.0 81.8 33 ?
?
?Platform 80.1 78.4 80.5 72.5 36 ?
?
?Application 70.2 69.7 78.4 69.3 30 ?
?
?Area 46.8 54.0 53.7 42.4 17 ?
?
?Req-years-e 80.8 80.0 68.8 67.2 76 ?
?
?Des-years-e 81.9 85.6 60.4 87.5 47 ?
?
?Req-degree 87.5 87.9 84.7 81.5 45 ?
?
?Des-degree 59.2 62.9 65.1 72.2 33 ?
?
?Post date 99.2 99.4 99.5 99.5 98 ?
?
?MA F1 80.8(?1.0) 81.6(?1.1) 77.2 76.0 57.9 ?
?
?of different learning algorithms.
Firstly, the sys-tem of the challenge organisers, which is based onLP 2 obtained the best result for Task1, followed byone of our participating systems which combined theuneven margins SVM and PAUM (see Ireson andCiravegna (2005)).
Our SVM and PAUM systemson their own were respectively in the fourth and fifthposition among the 20 participating systems.
Sec-ondly, at least six other participating system werealso based on SVM but used different IE frameworkand possibly different SVM models from our SVMsystem.
Our SVM system achieved better resultsthan all those SVM-based systems, showing that theSVM models and the IE framework of our systemwere quite suitable to IE task.
Thirdly, our PAUMbased system was not as good as our SVM systembut was still better than the other SVM based sys-tems.
The computation time of the PAUM systemwas about 1/5 of that of our SVM system.Table 3 presents the per slot results and over-all performance of our SVM and PAUM systemsas well as the system with the best overall result.Compared to the best system, our SVM system per-formed better on two slots and had similar resultson many of other slots.
The best system had ex-tremely good results on the two slots, C-acronymand C-homepage.
Actually, the F1 values of the bestsystem on the two slots were more than double ofthose of every other participating system.3.4 Effects of Uneven Margins ParameterA number of experiments were conducted to inves-tigate the influence of the uneven margins parameteron the SVM and Perceptron?s performances.
Table 4show the results with several different values of un-even margins parameter respectively for the SVMand the Perceptron on two datasets ?
CoNLL-2003and Jobs.
The SVM with uneven margins (?
< 1.0)had better results than the standard SVM (?
= 1).We can also see that the results were similar for the ?between 0.6 and 0.4, showing that the results are notparticularly sensitive to the value of the uneven mar-gins parameter.
The uneven margins parameter hassimilar effect on Perceptron as on the SVM.
Table 4shows that the PAUM had better results than both thestandard Perceptron and the margin Perceptron77Table 3: Results of our SVM and PAUM systemson CFP corpus: F-measures(%) on individual entitytype and the overall figures, together with the systemwith the highest overall score.
The highest score oneach slot appears in bold.SLOT PAUM SVM Best oneW-name 51.9 54.2 35.2W-acronym 50.4 60.0 86.5W-date 67.0 69.0 69.4W-homepage 69.6 70.5 72.1W-location 60.0 66.0 48.8W-submission 70.2 69.6 86.4W-notification 76.1 85.6 88.9W-camera-ready 71.5 74.7 87.0C-name 43.2 47.7 55.1C-acronym 38.8 38.7 90.5C-homepage 7.1 11.6 39.3Micro-average 61.1 64.3 73.4Our conjecture was that the uneven margins pa-rameter was more helpful on small training sets, be-cause the smaller a training set is, the more imbal-anced it could be.
Therefore we carried out exper-iments on a small numbers of training documents.Table 5 shows the results of the SVM and the unevenmargins SVM on different numbers of training doc-uments from CoNLL-2003 and Jobs datasets.
Theperformance of both the standard SVM and the un-even margins SVM improves consistently as moretraining documents are used.
Moreover, comparedto the results one large training sets shown in Table4, the uneven margins SVM obtains more improve-ments on small training sets than the standard SVMmodel.
We can see that the smaller the training setis, the better the results of the uneven margins SVMare in comparison to the standard SVM.4 ConclusionsThis paper studied the uneven margins versions oftwo learning algorithms ?
SVM and Perceptron ?
todeal with the imbalanced training data in IE.
Our ex-periments showed that the uneven margin is helpful,in particular on small training sets.
The smaller thetraining set is, the more beneficial the uneven margincould be.
We also showed that the systems based onthe uneven margins SVM and Perceptron were com-Table 4: The effects of uneven margins parameterof the SVM and Perceptron, respectively: macro av-eraged F1(%) on the two datasets CoNLL-2003 (de-velopment set) and Jobs.
The standard deviations forthe Jobs dataset show the statistical significances ofthe results.
In bold are the best performance figuresfor each dataset and each system.?
1.0 0.8 0.6 0.4 0.2Conll 89.0 89.6 89.7 89.2 85.3Jobs 79.0 79.9 81.0 80.8 79.0?1.4 ?1.2 ?0.9 ?1.0 ?1.3(?+, ??)
(0,0) (1,1) (50,1)Conll 83.5 83.9 84.4Jobs 74.1 78.8 81.6?1.5 ?1.0 ?1.1parable to other state-of-the-art systems.Our SVM system obtained better results thanother SVM-based systems on the CoNLL-2003 cor-pus and CFP corpus respectively, while being sim-pler than most of them.
This demonstrates that ourSVM system is both effective and efficient.We also explored PAUM, a simple and fastlearning algorithm for IE.
The results of PAUMwere somehow worse (about 0.02 overall F-measurelower) than those of the SVM on two out of threedatasets.
On the other hand, PAUM is much fasterto train and easier to implement than SVM.
It is alsoworth noting that PAUM outperformed some otherlearning algorithms.
Therefore, even PAUM on itsown would be a good learning algorithm for IE.Moreover, PAUM could be used in combination withother classifiers or in the more complicated frame-work such as the one in Carreras et al (2003).Since many other tasks in Natural Language Pro-cessing, like IE, often lead to imbalanced classifica-tion problems and the SVM has been used widelyin Natural Language Learning (NLL), we can ex-pect that the uneven margins SVM and PAUM arelikely to obtain good results on other NLL problemsas well.AcknowledgementsThis work is supported by the EU-funded SEKTproject (http://www.sekt-project.org).78Table 5: The performances of the SVM system withsmall training sets: macro-averaged F1(%) on thetwo datasets CoNLL-2003 (development set) andJobs.
The uneven margins SVM (?
= 0.4) is com-pared to the standard SVM model with even margins(?
= 1).
The standard deviations are presented forresults on the Jobs dataset.size 10 20 30 40 50?
= 0.4Conll 60.6 66.4 70.4 72.2 72.8Jobs 51.6 60.9 65.7 68.6 71.1?2.7 ?2.5 ?2.1 ?1.9 ?2.5?
= 1Conll 46.2 58.6 65.2 68.3 68.6Jobs 47.1 56.5 61.4 65.4 68.1?3.4 ?3.1 ?2.7 ?1.9 ?2.1ReferencesM.
E. Califf.
1998.
Relational Learning Techniques forNatural Language Information Extraction.
Ph.D. the-sis, University of Texas at Austin.X.
Carreras, L. Ma`rquez, and L. Padro?.
2003.
Learn-ing a perceptron-based named entity chunker via on-line recognition feedback.
In Proceedings of CoNLL-2003, pages 156?159.
Edmonton, Canada.H.
L. Chieu and H. T. Ng.
2002.
A Maximum En-tropy Approach to Information Extraction from Semi-Structured and Free Text.
In Proceedings of the Eigh-teenth National Conference on Artificial Intelligence,pages 786?791.F.
Ciravegna.
2001.
(LP)2, an Adaptive Algorithm forInformation Extraction from Web-related Texts.
InProceedings of the IJCAI-2001 Workshop on AdaptiveText Extraction and Mining, Seattle.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A Framework and GraphicalDevelopment Environment for Robust NLP Tools andApplications.
In Proceedings of the 40th AnniversaryMeeting of the Association for Computational Linguis-tics (ACL?02).R.
Florian, A. Ittycheriah, H. Jing, and T. Zhang.
2003.Named Entity Recognition through Classifier Combi-nation.
In Proceedings of CoNLL-2003, pages 168?171.
Edmonton, Canada.D.
Freigtag and A. K. McCallum.
1999.
Information Ex-traction with HMMs and Shrinkage.
In Proceesingsof Workshop on Machine Learnig for Information Ex-traction, pages 31?36.D.
Freitag and N. Kushmerick.
2000.
Boosted WrapperInduction.
In Proceedings of AAAI 2000.D.
Freitag.
1998.
Machine Learning for Information Ex-traction in Informal Domains.
Ph.D. thesis, CarnegieMellon University.N.
Ireson and F. Ciravegna.
2005.
Pascal Chal-lenge The Evaluation of Machine Learningfor Information Extraction.
In Proceedings ofDagstuhl Seminar Machine Learning for theSemantic Web (http://www.smi.ucd.ie/Dagstuhl-MLSW/proceedings/).H.
Isozaki and H. Kazawa.
2002.
Efficient SupportVector Classifiers for Named Entity Recognition.
InProceedings of the 19th International Conference onComputational Linguistics (COLING?02), pages 390?396, Taipei, Taiwan.Y.
Li and J. Shawe-Taylor.
2003.
The SVM withUneven Margins and Chinese Document Categoriza-tion.
In Proceedings of The 17th Pacific Asia Con-ference on Language, Information and Computation(PACLIC17), Singapore, Oct.Y.
Li, H. Zaragoza, R. Herbrich, J. Shawe-Taylor, andJ.
Kandola.
2002.
The Perceptron Algorithm with Un-even Margins.
In Proceedings of the 9th InternationalConference on Machine Learning (ICML-2002), pages379?386.Y.
Li, K. Bontcheva, and H. Cunningham.
2005.
SVMBased Learning System For Information Extraction.In Proceedings of Sheffield Machine Learning Work-shop, Lecture Notes in Computer Science.
SpringerVerlag.J.
Mayfield, P. McNamee, and C. Piatko.
2003.
NamedEntity Recognition Using Hundreds of Thousands ofFeatures.
In Proceedings of CoNLL-2003, pages 184?187.
Edmonton, Canada.A.
De Sitter and W. Daelemans.
2003.
Information ex-traction via double classification.
In Proceedings ofECML/PRDD 2003 Workshop on Adaptive Text Ex-traction and Mining (ATEM 2003), Cavtat-Dubrovnik,Croatia.S.
Soderland.
1999.
Learning information extrac-tion rules for semi-structured and free text.
MachineLearning, 34(1):233?272.J.
Zhang and I. Mani.
2003.
KNN Approach to Un-balanced Data Distributions: A Case Study Involv-ing Information Extraction.
In Proceedings of theICML?2003 Workshop on Learning from ImbalancedDatasets.79
