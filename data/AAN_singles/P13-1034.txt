Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 343?351,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsScaling Semi-supervised Naive Bayes with Feature MarginalsMichael R. Lucas and Doug DowneyNorthwestern University2133 Sheridan RoadEvanston, IL 60208mlucas@u.northwestern.eduddowney@eecs.northwestern.eduAbstractSemi-supervised learning (SSL) methodsaugment standard machine learning (ML)techniques to leverage unlabeled data.SSL techniques are often effective in textclassification, where labeled data is scarcebut large unlabeled corpora are readilyavailable.
However, existing SSL tech-niques typically require multiple passesover the entirety of the unlabeled data,meaning the techniques are not applicableto large corpora being produced today.In this paper, we show that improvingmarginal word frequency estimates usingunlabeled data can enable semi-supervisedtext classification that scales to massiveunlabeled data sets.
We present a novellearning algorithm, which optimizes aNaive Bayes model to accord with statis-tics calculated from the unlabeled corpus.In experiments with text topic classifica-tion and sentiment analysis, we show thatour method is both more scalable and moreaccurate than SSL techniques from previ-ous work.1 IntroductionSemi-supervised Learning (SSL) is a MachineLearning (ML) approach that utilizes largeamounts of unlabeled data, combined with asmaller amount of labeled data, to learn a tar-get function (Zhu, 2006; Chapelle et al, 2006).SSL is motivated by a simple reality: the amountof available machine-readable data is exploding,while human capacity for hand-labeling data forany given ML task remains relatively constant.Experiments in text classification and other do-mains have demonstrated that by leveraging un-labeled data, SSL techniques improve machinelearning performance when human input is limited(e.g., (Nigam et al, 2000; Mann and McCallum,2010)).However, current SSL techniques have scal-ability limitations.
Typically, for each targetconcept to be learned, a semi-supervised classi-fier is trained using iterative techniques that exe-cute multiple passes over the unlabeled data (e.g.,Expectation-Maximization (Nigam et al, 2000) orLabel Propagation (Zhu and Ghahramani, 2002)).This is problematic for text classification overlarge unlabeled corpora like the Web: new tar-get concepts (new tasks and new topics of interest)arise frequently, and performing even a single passover a large corpus for each new target concept isintractable.In this paper, we present a new SSL text classi-fication approach that scales to large corpora.
In-stead of utilizing unlabeled examples directly foreach given target concept, our approach is to pre-compute a small set of statistics over the unlabeleddata in advance.
Then, for a given target class andlabeled data set, we utilize the statistics to improvea classifier.Specifically, we introduce a method that ex-tends Multinomial Naive Bayes (MNB) to lever-age marginal probability statistics P (w) of eachword w, computed over the unlabeled data.
Themarginal statistics are used as a constraint to im-prove the class-conditional probability estimatesP (w|+) and P (w|?)
for the positive and negativeclasses, which are often noisy when estimated oversparse labeled data sets.
We refer to the techniqueas MNB with Frequency Marginals (MNB-FM).In experiments with large unlabeled data setsand sparse labeled data, we find that MNB-FM is both faster and more accurate on aver-age than standard SSL methods from previouswork, including Label Propagation, MNB withExpectation-Maximization,, and the recent Semi-supervised Frequency Estimate (SFE) algorithm(Su et al, 2011).
We also analyze how MNB-343FM improves accuracy, and find that surprisinglyMNB-FM is especially useful for improving class-conditional probability estimates for words thatnever occur in the training set.The paper proceeds as follows.
We formally de-fine the task in Section 2.
Our algorithm is definedin Section 3.
We present experimental results inSection 4, and analysis in Section 5.
We discussrelated work in Section 6 and conclude in Section7 with a discussion of future work.2 Problem DefinitionWe consider a semi-supervised classification task,in which the goal is to produce a mappingfrom an instance space X consisting of T -tuplesof non-negative integer-valued features w =(w1, .
.
.
, wT ), to a binary output space Y ={?,+}.
In particular, our experiments will fo-cus on the case in which the wi?s represent wordcounts in a given document, in a corpus of vocab-ulary size T .We assume the following inputs:?
A set of zero or more labeled documentsDL = {(wd, yd)|d = 1, .
.
.
, n}, drawn i.i.d.from a distribution P (w, y) for w ?
X andy ?
Y .?
A large set of unlabeled documents DU ={(wd)|d = n+1, .
.
.
, n+u} drawn from themarginal distribution P (w) =?yP (w, y).The goal of the task is to output a classiferf : X ?
Y that performs well in predicting theclasses of given unlabeled documents.
The met-rics of evaluation we focus on in our experimentsare detailed in Section 4.Our semi-supervised technique utilizes statis-tics computed over the labeled corpus, denoted asfollows.
We use N+w to denote the sum of theoccurrences of word w over all documents in thepositive class in the labeled data DL.
Also, letN+ =?nw?DL N+w be the sum value of all wordcounts in the labeled positive documents.
Thecount of the remaining words in the positive doc-uments is represented as N+?w = N+ ?N+w .
ThequantitiesN?,N?w , andN?
?w are defined similarlyfor the negative class.3 MNB with Feature MarginalsWe now introduce our algorithm, which scalablyutilizes large unlabeled data stores for classifica-tion tasks.
The technique builds upon the multino-mial Naive Bayes model, and is denoted as MNBwith Feature Marginals (MNB-FM).3.1 MNB-FM MethodIn the text classification setting , each feature valuewd represents count of observations of word w indocument d. MNB makes the simplifying assump-tion that word occurrences are conditionally inde-pendent of each other given the class (+ or ?)
ofthe example.
Formally, let the probability P (w|+)of the w in the positive class be denoted as ?+w .
LetP (+) denote the prior probability that a documentis of the positive class, and P (?)
= 1?P (+) theprior for the negative class.
Then MNB representsthe class probability of an example as:P (+|d) =?w?d(?+w )wdP (+)?w?d(?
?w )wdP (?)
+?w?d(?+w )wdP (+)(1)MNB estimates the parameters ?+w from thecorresponding counts in the training set.
Themaximum-likelihood estimate of ?+w is N+w /N+,and to prevent zero-probability estimates we em-ploy ?add-1?
smoothing (typical in MNB) to ob-tain the estimate:?+w =N+w + 1N+ + |T | .After MNB calculates ?+w and ?
?w from the train-ing set for each feature in the feature space, it canthen classify test examples using Equation 1.MNB-FM attempts to improve MNB?s esti-mates of ?+w and ?
?w , using statistics computed overthe unlabeled data.
Formally, MNB-FM leveragesthe equality:P (w) = ?+wPt(+) + ??wPt(?)
(2)The left-hand-side of Equation 2, P (w), repre-sents the probability that a given randomly drawntoken from the unlabeled data happens to be theword w. We write Pt(+) to denote the probabil-ity that a randomly drawn token (i.e.
a word oc-currence) from the corpus comes from the posi-tive class.
Note that Pt(+) can differ from P (+),the prior probability that a document is positive,due to variations in document length.
Pt(?)
is de-fined similarly for the negative class.
MNB-FM ismotivated by the insight that the left-hand-side of344Equation 2 can be estimated in advance, withoutknowledge of the target class, simply by countingthe number of tokens of each word in the unla-beled data.MNB-FM then uses this improved estimate ofP (w) as a constraint to improve the MNB param-eters on the right-hand-side of Equation 2.
Wenote that Pt(+) and Pt(?
), even for a small train-ing set, can typically be estimated reliably?
ev-ery token in the training data serves as an obser-vation of these quantities.
However, for large andsparse feature spaces common in settings like textclassification, many features occur in only a smallfraction of examples?meaning ?+w and ?
?w mustbe estimated from only a handful of observations.MNB-FM attempts to improve the noisy estimates?+w and ?
?w utilizing the robust estimate for P (w)computed over unlabeled data.Specifically, MNB-FM proceeds by assumingthe MLEs for P (w) (computed over unlabeleddata), Pt(+), and Pt(?)
are correct, and re-estimates ?+w and ?
?w under the constraint in Equa-tion 2.First, the maximum likelihood estimates of ?+wand ?
?w given the training data DL are:argmax?+w ,?
?wP (DL|?+w , ?
?w )= argmax?+w ,?
?w?+(N+w )w (1?
?+w )(N+?w)??
(N?w )w (1?
?
?w )(N?
?w)= argmax?+w ,?
?wN+w ln(?+w ) +N+?w ln(1?
?+w )+N?w ln(?
?w ) +N?
?w ln(1?
?
?w )(3)We can rewrite the constraint in Equation 2 as:?
?w = K ?
?+wLwhere for compactness we represent:K = P (w)Pt(?
);L = Pt(+)Pt(?
).Substituting the constraint into Equation 3shows that we wish to choose ?+w as:argmax?+wN+w ln(?+w ) +N+?w ln(1?
?+w )+N?w ln(K ?
L?+w ) +N?
?w ln(1?K + L?+w )The optimal values for ?+w are thus located at thesolutions of:0 = N+w?+w+ N+?w?+w ?
1+ LN?wL?+w ?K+ LN?
?wL?+w ?K + 1Both ?+w and ?
?w are constrained to valid prob-abilities in [0,1] when ?+w ?
[0, KL ].
If N+?w andN?w have non-zero counts, vertical asymptotes ex-ist at 0 and KL and guarantee a solution in thisrange.
Otherwise, a valid solution may not ex-ist.
In that case, we default to the add-1 Smooth-ing estimates used by MNB.
Finally, after optimiz-ing the values ?+w and ?
?w for each word w as de-scribed above, we normalize the estimates to ob-tain valid conditional probability distributions, i.e.with ?w ?+w =?w ?
?w = 13.2 MNB-FM ExampleThe following concrete example illustrates howMNB-FM can improve MNB parameters using thestatistic P (w) computed over unlabeled data.
Theexample comes from the Reuters Aptemod textclassification task addressed in Section 4, usingbag-of-words features for the Earnings class.
Inone experiment with 10 labeled training examples,we observed 5 positive and 5 negative examples,with the word ?resources?
occurring three timesin the set (once in the positive class, twice in thenegative class).MNB uses add-1 smoothing to estimate the con-ditional probability of the word ?resources?
ineach class as ?+w = 1+1216+33504 = 5.93e-5, and?
?w = 2+1547+33504 = 8.81e-5.
Thus, ?+w?
?w= 0.673implying that ?resources?
is a negative indicatorof the Earnings class.
However, this estimate isinaccurate.
In fact, over the full dataset, the pa-rameter values we observe are ?+w = 93168549 =5.70e-4 and ?
?w = 263564717 = 4.65e-4, with a ratioof ?+w?
?w = 1.223.
Thus, in actuality, the word ?re-sources?
is a mild positive indicator of the Earn-ings class.
Yet because MNB estimates its param-eters from only the sparse training data, it can beinaccurate.The optimization in MNB-FM seeks to accordits parameter estimates with the feature frequency,computed from unlabeled data, of P (w) = 4.89e-4.
We see that compared with P (w), the ?+w and?
?w that MNB estimates from the training data areboth too low by almost an order of magnitude.Further, the maximum likelihood estimate for ?
?w(based on an occurrence count of 2 out of 547 ob-servations) is somewhat more reliable than that for?+w (1 of 216 observations).
As a result, ?+w is ad-justed upward relatively more than ?
?w via MNB-FM?s constrained ML estimation.
MNB-FM re-turns ?+w = 6.52e-5 and ?
?w = 6.04e-5.
The ratio345?+w?
?wis 1.079, meaning MNB-FM correctly identi-fies the word ?resources?
as an indicator of thepositive class.The above example illustrates how MNB-FMcan leverage frequency marginal statistics com-puted over unlabeled data to improve MNB?sconditional probability estimates.
We analyzehow frequently MNB-FM succeeds in improvingMNB?s estimates in practice, and the resulting im-pact on classification accuracy, below.4 ExperimentsIn this section, we describe our experiments quan-tifying the accuracy and scalability of our pro-posed technique.
Across multiple domains, wefind that MNB-FM outperforms a variety of ap-proaches from previous work.4.1 Data SetsWe evaluate on two text classification tasks: topicclassification, and sentiment detection.
In topicclassification, the task is to determine whether atest document belongs to a specified topic.
Wetrain a classifier separately (i.e., in a binary clas-sification setting) for each topic and measure clas-sification performance for each class individually.The sentiment detection task is to determinewhether a document is written with a positive ornegative sentiment.
In our case, the goal is to de-termine if the given text belongs to a positive re-view of a product.4.1.1 RCV1The Reuters RCV1 corpus is a standard large cor-pus used for topic classification evaluations (Lewiset al, 2004).
It includes 804,414 documents withseveral nested target classes.
We consider the 5largest base classes after punctuation and stop-words were removed.
The vocabulary consistedof 288,062 unique words, and the total number oftokens in the data set was 99,702,278.
Details ofthe classes can be found in Table 1.4.1.2 Reuters AptemodWhile MNB-FM is designed to improve the scala-bility of SSL to large corpora, some of the com-parison methods from previous work were nottractable on the large topic classification data setRCV1.
To evaluate these methods, we also exper-imented with the Reuters ApteMod dataset (Yangand Liu, 1999), consisting of 10,788 documentsbelonging to 90 classes.
We consider the 10 mostClass # PositiveCCAT 381327 (47.40%)GCAT 239267 (29.74%)MCAT 204820 (25.46%)ECAT 119920 (14.91%)GPOL 56878 (7.07%)Table 1: RCV1 dataset detailsClass # PositiveEarnings 3964 (36.7%)Acquisitions 2369 (22.0%)Foreign 717 (6.6%)Grain 582 (5.4%)Crude 578 (5.4%)Trade 485 (4.5%)Interest 478 (4.4%)Shipping 286 (2.7%)Wheat 283 (2.6%)Corn 237 (2.2%)Table 2: Aptemod dataset detailsfrequent classes, with varying degrees of posi-tive/negative skew.
Punctuation and stopwordswere removed during preprocessing.
The Apte-mod data set contained 33,504 unique words anda total of 733,266 word tokens.
Details of theclasses can be found in Table 2.4.1.3 Sentiment Classification DataIn the domain of Sentiment Classification, wetested on the Amazon dataset from (Blitzer et al,2007).
Stopwords listed in an included file wereignored for our experiments and we only the con-sidered unigram features.
Unlike the two Reutersdata sets, each category had a unique set of doc-uments of varying size.
For our experiments, weonly used the 10 largest categories.
Details of thecategories can be found in Table 3.In the Amazon Sentiment Classification dataset, the task is to determine whether a review ispositive or negative based solely on the reviewer?ssubmitted text.
As such, the positive and negativeClass # Instances # Positive VocabularyMusic 124362 113997 (91.67%) 419936Books 54337 47767 (87.91%) 220275Dvd 46088 39563 (85.84%) 217744Electronics 20393 15918 (78.06%) 65535Kitchen 18466 14595 (79.04%) 47180Video 17389 15017 (86.36%) 106467Toys 12636 10151 (80.33%) 37939Apparel 8940 7642 (85.48%) 22326Health 6507 5124 (78.75%) 24380Sports 5358 4352 (81.22%) 24237Table 3: Amazon dataset details346labels are equally relevant.
For our metrics, wecalculate the scores for both the positive and neg-ative class and report the average of the two (incontrast to the Reuters data sets, in which we onlyreport the scores for the positive class).4.2 Comparison MethodsIn addition to Multinomial Naive Bayes (discussedin Section 3), we evaluate against a variety ofsupervised and semi-supervised techniques fromprevious work, which provide a representation ofthe state of the art.
Below, we detail the compar-ison methods that we re-implemented for our ex-periments.4.2.1 NB + EMWe implemented a semi-supervised version ofNaive Bayes with Expectation Maximization,based on (Nigam et al, 2000).
We found that 15iterations of EM was sufficient to ensure approxi-mate convergence of the parameters.We also experimented with different weightingfactors to assign to the unlabeled data.
While per-forming per-data-split cross-validation was com-putationally prohibitive for NB+EM, we per-formed experiments on one class from each dataset that revealed weighting unlabeled examples at1/5 the weight of a labeled example performedbest.
We found that our re-implementation ofNB+EM slightly outperformed published resultson a separate data set (Mann and McCallum,2010), validating our design choices.4.2.2 Logistic RegressionWe implemented Logistic Regression using L2-Normalization, finding this to outperform L1-Normalized and non-normalized versions.
Thestrength of the normalization was selected for eachtraining data set of each size utilized in our exper-iments.The strength of the normalization in the logis-tic regression required cross-validation, which welimited to 20 values logarithmically spaced be-tween 10?4 and 104.
The optimal value was se-lected based upon the best average F1 score overthe 10 folds.
We selected a normalization param-eter separately for each subset of the training dataduring experimentation.4.2.3 Label PropagationFor our large unlabeled data set sizes, we foundthat a standard Label Propogation (LP) approach,which considers propagating information betweenall pairs of unlabeled examples, was not tractable.We instead implemented a constrained version ofLP for comparison.In our implementation, we limit the number ofedges in the propagation graph.
Each node prop-agates to only to its 10 nearest neighbors, wheredistance is calculated as the cosine distance be-tween the tf-idf representation of two documents.We found the tf-idf weighting to improve perfor-mance over that of simple cosine distance.
Propa-gation was run for 100 iterations or until the en-tropy dropped below a predetermined threshold,whichever occurred first.
Even with these aggres-sive constraints, Label Propagation was intractableto execute on some of the larger data sets, so wedo not report LP results for the RCV1 dataset orfor the 5 largest Amazon categories.4.2.4 SFEWe also re-implemented a version of the recentSemi-supervised Frequency Estimate approach(Su et al, 2011).
SFE was found to outperformMNB and NB+EM in previous work.
Consis-tent with our MNB implementation, we use Add-1 Smoothing in our SFE calculations although itsuse is not specifically mentioned in (Su et al,2011).SFE also augments multinomial Naive Bayeswith the frequency information P (w), although ina manner distinct from MNB-FM.
In particular,SFE uses the equality P (+|w) = P (+, w)/P (w)and estimates the rhs using P (w) computed overall the unlabeled data, rather than using only la-beled data as in standard MNB.
The primary dis-tinction between MNB-FM and SFE is that SFEadjusts sparse estimates P (+, w) in the same wayas non-sparse estimates, whereas MNB-FM is de-signed to adjust sparse estimates more than non-sparse ones.
Further, it can be shown that as P (w)of a word w in the unlabeled data becomes largerthan that in the labeled data, SFE?s estimate of theratio P (w|+)/P (w|?)
approaches one.
Depend-ing on the labeled data, such an estimate can be ar-bitrarily inaccurate.
MNB-FM does not have thislimitation.4.3 ResultsFor each data set, we evaluate on 50 randomlydrawn training splits, each comprised of 1,000 ran-domly selected documents.
Each set included atleast one positive and one negative document.
We347Data Set MNB-FM SFE MNB NBEM LProp Logist.Apte (10) 0.306 0.271 0.336 0.306 0.245 0.208Apte (100) 0.554 0.389 0.222 0.203 0.263 0.330Apte (1k) 0.729 0.614 0.452 0.321 0.267 0.702Amzn (10) 0.542 0.524 0.508 0.475 0.470* 0.499Amzn (100) 0.587 0.559 0.456 0.456 0.498* 0.542Amzn (1k) 0.687 0.611 0.465 0.455 0.539* 0.713RCV1 (10) 0.494 0.477 0.387 0.485 - 0.272RCV1 (100) 0.677 0.613 0.337 0.470 - 0.518RCV1 (1k) 0.772 0.735 0.408 0.491 - 0.774* Limited to 5 of 10 Amazon categoriesTable 4: F1, training size in parenthesesrespected the order of the training splits such thateach sample was a strict subset of any larger train-ing sample of the same split.We evaluate on the standard metric of F1 withrespect to the target class.
For Amazon, in whichboth the ?positive?
and ?negative?
classes are po-tential target classes, we evaluate using macro-averaged scores.The primary results of our experiments areshown in Table 4.
The results show that MNB-FMimproves upon the MNB classifier substantially,and also tends to outperform the other SSL andsupervised learning methods we evaluated.
MNB-FM is the best performing method over all datasets when the labeled data is limited to 10 and 100documents, except for training sets of size 10 inAptemod, where MNB has a slight edge.Tables 5 and 6 present detailed results of theexperiments on the RCV1 data set.
These exper-iments are limited to the 5 largest base classesand show the F1 performance of MNB-FM andthe various comparison methods, excluding LabelPropagation which was intractable on this data set.Class MNB-FM SFE MNB NBEM Logist.CCAT 0.641 0.643 0.580 0.639 0.532GCAT 0.639 0.686 0.531 0.732 0.466MCAT 0.572 0.505 0.393 0.504 0.225ECAT 0.306 0.267 0.198 0.224 0.096GPOL 0.313 0.283 0.233 0.326 0.043Average 0.494 0.477 0.387 0.485 0.272Table 5: RCV1: F1, |DL|= 10Class MNB-FM SFE MNB NBEM Logist.CCAT 0.797 0.793 0.624 0.713 0.754GCAT 0.849 0.848 0.731 0.837 0.831MCAT 0.776 0.737 0.313 0.516 0.689ECAT 0.463 0.317 0.017 0.193 0.203GPOL 0.499 0.370 0.002 0.089 0.114Average 0.677 0.613 0.337 0.470 0.518Table 6: RCV1: F1, |DL|= 100Method 1000 5000 10k 50k 100kMNB-FM 1.44 1.61 1.69 2.47 5.50NB+EM 2.95 3.43 4.93 10.07 16.90MNB 1.15 1.260 1.40 2.20 3.61Labelprop 0.26 4.17 10.62 67.58 -Table 7: Runtimes of SSL methods (sec.
)The runtimes of our methods can be seen in Ta-ble 7.
The results show the runtimes of the SSLmethods discussed in this paper as the size of theunlabeled dataset grows.
As expected, we find thatMNB-FM has runtime similar to MNB, and scalesmuch better than methods that take multiple passesover the unlabeled data.5 AnalysisFrom our experiments, it is clear that the perfor-mance of MNB-FM improves on MNB, and inmany cases outperforms all existing SSL algo-rithms we evaluated.
MNB-FM improves the con-ditional probability estimates in MNB and, sur-prisingly, we found that it can often improve theseestimates for words that do not even occur in thetraining set.Tables 8 and 9 show the details of the improve-ments MNB-FM makes on the feature marginalestimates.
We ran MNB-FM and MNB on theRCV1 class MCAT and stored the computed fea-ture marginals for direct comparison.
For eachword in the vocabulary, we compared each clas-sifier?s conditional probability ratios, i.e.
?+/?
?,to the true value over the entire data set.
We com-puted which classifier was closer to the correct ra-tio for each word.
These results were averagedover 5 iterations.
From the data, we can see thatMNB-FM improves the estimates for many wordsnot seen in the training set as well as the most com-mon words, even with small training sets.5.1 Ranking PerformanceWe also analyzed how well the different meth-ods rank, rather than classify, the test documents.We evaluated ranking using the R-precision met-ric, equal to the precision (i.e.
fraction of positivedocuments classified correctly) of the R highest-ranked test documents, where R is the total num-ber of positive test documents.Logistic Regression performed particularly wellon the R-Precision Metric, as can be seen in Tables10, 11, and 12.
Logistic Regression performedless well in the F1 metric.
We find that NB+EM348Fraction Improved vs MNB Avg Improvement vs MNB Probability MassWord Freq.
Known Half Known Unknown Known Half Known Unknown Known Half Known Unknown0-10?6 - 0.165 0.847 - -0.805 0.349 - 0.02% 7.69%10?6-10?5 0.200 0.303 0.674 0.229 -0.539 0.131 0.00% 0.54% 14.77%10?5-10?4 0.322 0.348 0.592 -0.597 -0.424 0.025 0.74% 10.57% 32.42%10?4-10?3 0.533 0.564 0.433 0.014 0.083 -0.155 7.94% 17.93% 7.39%> 10?3 - - - - - - - - -Table 8: Analysis of Feature Marginal Improvement of MNB-FM over MNB (|DL| = 10).
?Known?indicates words occurring in both positive and negative training examples, ?Half Known?
indicates wordsoccurring in only positive or negative training examples, while ?Unknown?
indicates words that neveroccur in labelled examples.
Data is for the RCV1 MCAT category.
MNB-FM improves estimates by asubstantial amount for unknown words and also the most common known and half-known words.Fraction Improved vs MNB Avg Improvement vs MNB Probability MassWord Freq.
Known Half Known Unknown Known Half Known Unknown Known Half Known Unknown0-10?6 0.567 0.243 0.853 0.085 -0.347 0.143 0.00% 0.22% 7.49%10?6-10?5 0.375 0.310 0.719 -0.213 -0.260 0.087 0.38% 4.43% 10.50%10?5-10?4 0.493 0.426 0.672 -0.071 -0.139 0.067 18.68% 20.37% 4.67%10?4-10?3 0.728 0.669 - 0.233 0.018 - 31.70% 1.56% -> 10?3 - - - - - - - - -Table 9: Analysis of Feature Marginal Improvement of MNB-FM over MNB (|DL| = 100).
Data isfor the RCV1 MCAT category (see Table 8).
MNB-FM improves estimates by a substantial amount forunknown words and also the most common known and half-known words.performs particularly well on the R-precision met-ric on ApteMod, suggesting that its modelling as-sumptions are more accurate for that particulardata set (NB+EM performs significantly worse onthe other data sets, however).
MNB-FM performsessentially equivalently well, on average, to thebest competing method (Logistic Regression) onthe large RCV1 data set.
However, these experi-ments show that MNB-FM offers more advantagesin document classification than in document rank-ing.The ranking results show that LR may be pre-ferred when ranking is important.
However, LRunderperforms in classification tasks (in terms ofF1, Tables 4-6).
The reason for this is that LR?slearned classification threshold becomes less accu-rate when datasets are small and classes are highlyClass MNB-FM SFE MNB NBEM LProp Logist.Apte (10) 0.353 0.304 0.359 0.631 0.490 0.416Apte (100) 0.555 0.421 0.343 0.881 0.630 0.609Apte (1k) 0.723 0.652 0.532 0.829 0.754 0.795Amzn (10) 0.536 0.527 0.516 0.481 0.535* 0.544Amzn (100) 0.614 0.562 0.517 0.480 0.573* 0.639Amzn (1k) 0.717 0.650 0.562 0.483 0.639* 0.757RCV1 (10) 0.505 0.480 0.421 0.450 - 0.512RCV1 (100) 0.683 0.614 0.474 0.422 - 0.689RCV1 (1k) 0.781 0.748 0.535 0.454 - 0.802* Limited to 5 of 10 Amazon categoriesTable 10: R-Precision, training size in parenthesesskewed.
In these cases, LR classifies too fre-quently in favor of the larger class which is detri-mental to its performance.
This effect is visiblein Tables 5 and 6, where LR?s performance sig-nificantly drops for the ECAT and GPOL classes.ECAT and GPOL represent only 14.91% and7.07% of the RCV1 dataset, respectively.6 Related WorkTo our knowledge, MNB-FM is the first approachthat utilizes a small set of statistics computed overData SetMNB-FM SFE MNB NBEM Logist.CCAT 0.637 0.631 0.620 0.498 0.653GCAT 0.663 0.711 0.600 0.792 0.671MCAT 0.580 0.492 0.477 0.510 0.596ECAT 0.291 0.217 0.214 0.111 0.297GPOL 0.354 0.352 0.193 0.341 0.341Average 0.505 0.480 0.421 0.450 0.512Table 11: RCV1: R-Precision, DL= 10Class MNB-FM SFE MNB NBEM Logist.CCAT 0.805 0.797 0.765 0.533 0.809GCAT 0.849 0.858 0.780 0.869 0.843MCAT 0.782 0.753 0.579 0.533 0.774ECAT 0.471 0.293 0.203 0.119 0.498GPOL 0.509 0.370 0.042 0.056 0.520Average 0.683 0.614 0.474 0.422 0.689Table 12: RCV1: R-Precision, DL= 100349a large unlabeled data set as constraints to im-prove a semi-supervised classifier.
Our exper-iments demonstrate that MNB-FM outperformsprevious approaches across multiple text classi-fication techniques including topic classificationand sentiment analysis.
Further, the MNB-FM ap-proach offers scalability advantages over most ex-isting semi-supervised approaches.Current popular Semi-Supervised Learning ap-proaches include using Expectation-Maximizationon probabilistic models (e.g.
(Nigam et al,2000)); Transductive Support Vector Machines(Joachims, 1999); and graph-based methods suchas Label Propagation (LP) (Zhu and Ghahramani,2002) and their more recent, more scalable vari-ants (e.g.
identifying a small number of represen-tative unlabeled examples (Liu et al, 2010)).
Ingeneral, these techniques require passes over theentirety of the unlabeled data for each new learn-ing task, intractable for massive unlabeled datasets.
Naive implementations of LP cannot scaleto large unlabeled data sets, as they have timecomplexity that increases quadratically with thenumber of unlabeled examples.
Recent LP tech-niques have achieved greater scalability throughthe use of parallel processing and heuristics suchas Approximate-Nearest Neighbor (Subramanyaand Bilmes, 2009), or by decomposing the sim-ilarity matrix (Lin and Cohen, 2011).
Our ap-proach, by contrast, is to pre-compute a smallset of marginal statistics over the unlabeled data,which eliminates the need to scan unlabeled datafor each new task.
Instead, the complexity ofMNB-FM is proportional only to the number ofunique words in the labeled data set.In recent work, Su et al propose the Semi-supervised Frequency Estimate (SFE), which likeMNB-FM utilizes the marginal probabilities offeatures computed from unlabeled data to im-prove the Multinomial Naive Bayes (MNB) clas-sifier (Su et al, 2011).
SFE has the same scal-ability advantages as MNB-FM.
However, unlikeour approach, SFE does not compute maximum-likelihood estimates using the marginal statisticsas a constraint.
Our experiments show that MNB-FM substantially outperforms SFE.A distinct method for pre-processing unlabeleddata in order to help scale semi-supervised learn-ing techniques involves dimensionality reductionor manifold learning (Belkin and Niyogi, 2004),and for NLP tasks, identifying word representa-tions from unlabeled data (Turian et al, 2010).
Incontrast to these approaches, MNB-FM preservesthe original feature set and is more scalable (themarginal statistics can be computed in a singlepass over the unlabeled data set).7 ConclusionWe presented a novel algorithm for efficientlyleveraging large unlabeled data sets for semi-supervised learning.
Our MNB-FM technique op-timizes a Multinomial Naive Bayes model to ac-cord with statistics of the unlabeled corpus.
In ex-periments across topic classification and sentimentanalysis, MNB-FM was found to be more accu-rate and more scalable than several supervised andsemi-supervised baselines from previous work.In future work, we plan to explore utilizingricher statistics from the unlabeled data, beyondword marginals.
Further, we plan to experimentwith techniques for unlabeled data sets that alsoinclude continuous-valued features.
Lastly, wealso wish to explore ensemble approaches thatcombine the best supervised classifiers with theimproved class-conditional estimates provided byMNB-FM.8 AcknowledgementsThis work was supported in part by DARPA con-tract D11AP00268.ReferencesMikhail Belkin and Partha Niyogi.
2004.
Semi-supervised learning on riemannian manifolds.
Ma-chine Learning, 56(1):209?239.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In Association for Computational Linguis-tics, Prague, Czech Republic.O.
Chapelle, B. Scho?lkopf, and A. Zien, editors.
2006.Semi-Supervised Learning.
MIT Press, Cambridge,MA.Thorsten Joachims.
1999.
Transductive inference fortext classification using support vector machines.
InProceedings of the Sixteenth International Confer-ence on Machine Learning, ICML ?99, pages 200?209, San Francisco, CA, USA.
Morgan KaufmannPublishers Inc.David D Lewis, Yiming Yang, Tony G Rose, and FanLi.
2004.
Rcv1: A new benchmark collection fortext categorization research.
The Journal of Ma-chine Learning Research, 5:361?397.350Frank Lin and William W Cohen.
2011.
Adaptationof graph-based semi-supervised methods to large-scale text data.
In The 9th Workshop on Mining andLearning with Graphs.Wei Liu, Junfeng He, and Shih-Fu Chang.
2010.
Largegraph construction for scalable semi-supervisedlearning.
In ICML, pages 679?686.Gideon S. Mann and Andrew McCallum.
2010.Generalized expectation criteria for semi-supervisedlearning with weakly labeled data.
J. Mach.
Learn.Res., 11:955?984, March.Kamal Nigam, Andrew Kachites McCallum, SebastianThrun, and Tom Mitchell.
2000.
Text classifica-tion from labeled and unlabeled documents usingem.
Mach.
Learn., 39(2-3):103?134, May.Jiang Su, Jelber Sayyad Shirab, and Stan Matwin.2011.
Large scale text classification using semisu-pervised multinomial naive bayes.
In Lise Getoorand Tobias Scheffer, editors, ICML, pages 97?104.Omnipress.Amar Subramanya and Jeff A. Bilmes.
2009.
En-tropic graph regularization in non-parametric semi-supervised classification.
In Neural InformationProcessing Society (NIPS), Vancouver, Canada, De-cember.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
Urbana, 51:61801.Yiming Yang and Xin Liu.
1999.
A re-examinationof text categorization methods.
In Proceedings ofthe 22nd annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 42?49.
ACM.X.
Zhu and Z. Ghahramani.
2002.
Learning fromlabeled and unlabeled data with label propagation.Technical report, Technical Report CMU-CALD-02-107, Carnegie Mellon University.Xiaojin Zhu.
2006.
Semi-supervised learning litera-ture survey.351
