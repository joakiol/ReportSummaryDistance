Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 17?24,Rochester, New York, April 2007. c?2007 Association for Computational LinguisticsInversion Transduction Grammar for Joint Phrasal Translation ModelingColin CherryDepartment of Computing ScienceUniversity of AlbertaEdmonton, AB, Canada, T6G 2E8colinc@cs.ualberta.caDekang LinGoogle Inc.1600 Amphitheatre ParkwayMountain View, CA, USA, 9403lindek@google.comAbstractWe present a phrasal inversion trans-duction grammar as an alternative tojoint phrasal translation models.
Thissyntactic model is similar to its flat-string phrasal predecessors, but admitspolynomial-time algorithms for Viterbialignment and EM training.
We demon-strate that the consistency constraints thatallow flat phrasal models to scale also helpITG algorithms, producing an 80-timesfaster inside-outside algorithm.
We alsoshow that the phrasal translation tablesproduced by the ITG are superior to thoseof the flat joint phrasal model, producingup to a 2.5 point improvement in BLEUscore.
Finally, we explore, for the firsttime, the utility of a joint phrasal transla-tion model as a word alignment method.1 IntroductionStatistical machine translation benefits greatly fromconsidering more than one word at a time.
Onecan put forward any number of non-compositionaltranslations to support this point, such as the col-loquial Canadian French-English pair, (Wo les mo-teurs, Hold your horses), where no clear word-to-word connection can be drawn.
Nearly all cur-rent decoding methods have shifted to phrasal rep-resentations, gaining the ability to handle non-compositional translations, but also allowing the de-coder to memorize phenomena such as monolingualagreement and short-range movement, taking pres-sure off of language and distortion models.Despite the success of phrasal decoders, knowl-edge acquisition for translation generally beginswith a word-level analysis of the training text, tak-ing the form of a word alignment.
Attempts to applythe same statistical analysis used at the word levelin a phrasal setting have met with limited success,held back by the sheer size of phrasal alignmentspace.
Hybrid methods that combine well-foundedstatistical analysis with high-confidence word-levelalignments have made some headway (Birch et al,2006), but suffer from the daunting task of heuris-tically exploring a still very large alignment space.In the meantime, synchronous parsing methods effi-ciently process the same bitext phrases while build-ing their bilingual constituents, but continue to beemployed primarily for word-to-word analysis (Wu,1997).
In this paper we unify the probability modelsfor phrasal translation with the algorithms for syn-chronous parsing, harnessing the benefits of bothto create a statistically and algorithmically well-founded method for phrasal analysis of bitext.Section 2 begins by outlining the phrase extrac-tion system we intend to replace and the two meth-ods we combine to do so: the joint phrasal transla-tion model (JPTM) and inversion transduction gram-mar (ITG).
Section 3 describes our proposed solu-tion, a phrasal ITG.
Section 4 describes how to ap-ply our phrasal ITG, both as a translation model andas a phrasal word-aligner.
Section 5 tests our systemin both these capacities, while Section 6 concludes.2 Background2.1 Phrase Table ExtractionPhrasal decoders require a phrase table (Koehn etal., 2003), which contains bilingual phrase pairs and17scores indicating their utility.
The surface heuris-tic is the most popular method for phrase-table con-struction.
It extracts all consistent phrase pairs fromword-aligned bitext (Koehn et al, 2003).
The wordalignment provides bilingual links, indicating trans-lation relationships between words.
Consistency isdefined so that alignment links are never broken byphrase boundaries.
For each token w in a consistentphrase pair p?, all tokens linked tow by the alignmentmust also be included in p?.
Each consistent phrasepair is counted as occurring once per sentence pair.The scores for the extracted phrase pairs are pro-vided by normalizing these flat counts according tocommon English or Foreign components, producingthe conditional distributions p(f?
|e?)
and p(e?|f?
).The surface heuristic can define consistency ac-cording to any word alignment; but most often, thealignment is provided by GIZA++ (Och and Ney,2003).
This alignment system is powered by theIBM translation models (Brown et al, 1993), inwhich one sentence generates the other.
These mod-els produce only one-to-many alignments: each gen-erated token can participate in at most one link.Many-to-many alignments can be created by com-bining two GIZA++ alignments, one where Englishgenerates Foreign and another with those roles re-versed (Och and Ney, 2003).
Combination ap-proaches begin with the intersection of the twoalignments, and add links from the union heuris-tically.
The grow-diag-final (GDF) combinationheuristic (Koehn et al, 2003) adds links so that eachnew link connects a previously unlinked token.2.2 Joint phrasal translation modelThe IBM models that power GIZA++ are trainedwith Expectation Maximization (Dempster et al,1977), or EM, on sentence-aligned bitext.
A transla-tion model assigns probabilities to alignments; thesealignment distributions are used to count translationevents, which are then used to estimate new parame-ters for the translation model.
Sampling is employedwhen the alignment distributions cannot be calcu-lated efficiently.
This statistically-motivated processis much more appealing than the flat counting de-scribed in Section 2.1, but it does not directly in-clude phrases.The joint phrasal translation model (Marcu andWong, 2002), or JPTM, applies the same statisticaltechniques from the IBMmodels in a phrasal setting.The JPTM is designed according to a generative pro-cess where both languages are generated simultane-ously.
First, a bag of concepts, or cepts, C is gener-ated.
Each ci ?
C corresponds to a bilingual phrasepair, ci = (e?i, f?i).
These contiguous phrases arepermuted in each language to create two sequencesof phrases.
Initially, Marcu and Wong assume thatthe number of cepts, as well as the phrase orderings,are drawn from uniform distributions.
That leavesa joint translation distribution p(e?i, f?i) to determinewhich phrase pairs are selected.
Given a lexicon ofpossible cepts and a predicate L(E,F,C) that de-termines if a bag of cepts C can be bilingually per-muted to create the sentence pair (E, F ), the proba-bility of a sentence pair is:p(E,F ) ??{C|L(E,F,C)}??
?ci?Cp(e?i, f?i)??
(1)If left unconstrained, (1) will consider every phrasalsegmentation of E and F , and every alignment be-tween those phrases.
Later, a distortion model basedon absolute token positions is added to (1).The JPTM faces several problems when scalingup to large training sets:1.
The alignment space enumerated by the sumin (1) is huge, far larger than the one-to-manyspace explored by GIZA++.2.
The translation distribution p(e?, f?)
will coverall co-occurring phrases observed in the bitext.This is far too large to fit in main memory, andcan be unwieldly for storage on disk.3.
Given a non-uniform p(e?, f?
), there is no effi-cient algorithm to compute the expectation ofphrase pair counts required for EM, or to findthe most likely phrasal alignment.Marcu and Wong (2002) address point 2 with a lexi-con constraint; monolingual phrases that are abovea length threshold or below a frequency thresholdare excluded from the lexicon.
Point 3 is handledby hill-climbing to a likely phrasal alignment andsampling around it.
However, point 1 remains unad-dressed, which prevents the model from scaling tolarge data sets.Birch et al (2006) handle point 1 directly by re-ducing the size of the alignment space.
This is18accomplished by constraining the JPTM to onlyuse phrase pairs that are consistent with a high-confidence word alignment, which is provided byGIZA++ intersection.
We refer to this constrainedJPTM as a C-JPTM.
This strikes an interestingmiddle ground between the surface heuristic de-scribed in Section 2.1 and the JPTM.
Like the sur-face heuristic, a word alignment is used to limit thephrase pairs considered, but the C-JPTM reasonsabout distributions over phrasal alignments, insteadof taking flat counts.
The consistency constraint al-lows them to scale their C-JPTM up to 700,000 sen-tence pairs.
With this constraint in place, the use ofhill-climbing and sampling during EM training be-comes one of the largest remaining weaknesses ofthe C-JPTM.2.3 Inversion Transduction GrammarLike the JPTM, stochastic synchronous grammarsprovide a generative process to produce a sentenceand its translation simultaneously.
Inversion trans-duction grammar (Wu, 1997), or ITG, is a well-studied synchronous grammar formalism.
Terminalproductions of the form A ?
e/f produce a to-ken in each stream, or a token in one stream withthe null symbol ?
in the other.
To allow for move-ment during translation, non-terminal productionscan be either straight or inverted.
Straight produc-tions, with their non-terminals inside square brack-ets [.
.
.
], produce their symbols in the given order inboth streams.
Inverted productions, indicated by an-gled brackets ?.
.
.
?, are output in reverse order in theForeign stream only.The work described here uses the binary bracket-ing ITG, which has a single non-terminal:A ?
[AA] | ?AA?
| e/f (2)This grammar admits an efficient bitext parsing al-gorithm, and holds no language-specific biases.
(2) cannot represent all possible permutations ofconcepts that may occur during translation, becausesome permutations will require discontinuous con-stituents (Melamed, 2003).
This ITG constraint ischaracterized by the two forbidden structures shownin Figure 1 (Wu, 1997).
Empirical studies suggestthat only a small percentage of human translationsviolate these constraints (Cherry and Lin, 2006).e1e2e3e4f1 f2 f3 f4 f1 f2 f3 f4e1e2e3e4Figure 1: The two ITG forbidden structures.calmez vouscalmdowncalmez vouscalmdowncalmez vouscalmdowna) A?
[AA] b) A?<AA>c) A?e/fFigure 2: Three ways in which a phrasal ITG cananalyze a multi-word span or phrase.Stochastic ITGs are parameterized like theirPCFG counterparts (Wu, 1997); productionsA ?
X are assigned probability Pr(X|A).
Theseparameters can be learned from sentence-aligned bi-text using the EM algorithm.
The expectation taskof counting productions weighted by their probabil-ity is handled with dynamic programming, using theinside-outside algorithm extended to bitext (Zhangand Gildea, 2004).3 ITG as a Phrasal Translation ModelThis paper introduces a phrasal ITG; in doing so,we combine ITG with the JPTM.
ITG parsing al-gorithms consider every possible two-dimensionalspan of bitext, each corresponding to a bilingualphrase pair.
Each multi-token span is analyzed interms of how it could be built from smaller spans us-ing a straight or inverted production, as is illustratedin Figures 2 (a) and (b).
To extend ITG to a phrasalsetting, we add a third option for span analysis: thatthe span under consideration might have been drawndirectly from the lexicon.
This option can be addedto our grammar by altering the definition of a termi-nal production to include phrases: A ?
e?/f?
.
Thisthird option is shown in Figure 2 (c).
The modelimplied by this extended grammar is trained usinginside-outside and EM.Our approach differs from previous attempts touse ITGs for phrasal bitext analysis.
Wu (1997)used a binary bracketing ITG to segment a sen-19tence while simultaneously word-aligning it to itstranslation, but the model was trained heuristicallywith a fixed segmentation.
Vilar and Vidal (2005)used ITG-like dynamic programming to drive bothtraining and alignment for their recursive translationmodel, but they employed a conditional model thatdid not maintain a phrasal lexicon.
Instead, theyscored phrase pairs using IBM Model 1.Our phrasal ITG is quite similar to the JPTM.Both models are trained with EM, and both em-ploy generative stories that create a sentence and itstranslation simultaneously.
The similarities becomemore apparent when we consider the canonical-formbinary-bracketing ITG (Wu, 1997) shown here:S ?
A | B | CA ?
[AB] | [BB] | [CB] |[AC] | [BC] | [CC]B ?
?AA?
| ?BA?
| ?CA?
|?AC?
| ?BC?
| ?CC?C ?
e?/f?
(3)(3) is employed in place of (2) to reduce redundantalignments and clean up EM expectations.1 Moreimportantly for our purposes, it introduces a preter-minal C, which generates all phrase pairs or cepts.When (3) is parameterized as a stochastic ITG, theconditional distribution p(e?/f?
|C) is equivalent tothe JPTM?s p(e?, f?
); both are joint distributions overall possible phrase pairs.
The distributions condi-tioned on the remaining three non-terminals assignprobability to concept movement by tracking inver-sions.
Like the JPTM?s distortion model, these pa-rameters grade each movement decision indepen-dently.
With terminal productions producing cepts,and inversions measuring distortion, our phrasal ITGis essentially a variation on the JPTM with an alter-nate distortion model.Our phrasal ITG has two main advantages overthe JPTM.
Most significantly, we gain polynomial-time algorithms for both Viterbi alignment and EMexpectation, through the use of ITG parsing andinside-outside algorithms.
These phrasal ITG algo-rithms are no more expensive asymptotically thantheir word-to-word counterparts, since each poten-tial phrase needs to be analyzed anyway during1If the null symbol ?
is included among the terminals, thenredundant parses will still occur, but far less frequently.constituent construction.
We hypothesize that us-ing these methods in place of heuristic search andsampling will improve the phrasal translation modellearned by EM.
Also, we can easily incorporate linksto ?
by including the symbol among our terminals.To minimize redundancy, we allow only single to-kens, not phrases, to align to ?.
The JPTM does notallow links to ?.The phrasal ITG also introduces two new compli-cations.
ITG Viterbi and inside-outside algorithmshave polynomial complexity, but that polynomial isO(n6), where n is the length of the longer sentencein the pair.
This is too slow to train on large datasets without massive parallelization.
Also, ITG al-gorithms explore their alignment space perfectly, butthat space has been reduced by the ITG constraintdescribed in Section 2.3.
We will address each ofthese issues in the following two subsections.3.1 Pruning SpansFirst, we address the problem of scaling ITG to largedata.
ITG dynamic programming algorithms workby analyzing each bitext span only once, storing itsvalue in a table for future use.
There are O(n4) ofthese spans, and each analysis takes O(n2) time.
Aneffective approach to speeding up ITG algorithmsis to eliminate unlikely spans as a preprocessingstep, assigning them 0 probability and saving thetime spent processing them.
Past approaches havepruned spans using IBM Model 1 probability esti-mates (Zhang and Gildea, 2005) or using agreementwith an existing parse tree (Cherry and Lin, 2006).The former is referred to as tic-tac-toe pruning be-cause it uses both inside and outside estimates.We propose a new ITG pruning method that lever-ages high-confidence links by pruning all spans thatare inconsistent with a provided alignment.
Thisis similar to the constraint used in the C-JPTM,but we do not just eliminate those spans as poten-tial phrase-to-phrase links: we never consider anyITG parse that builds a non-terminal over a prunedspan.2 This fixed-link pruning will speed up bothViterbi alignment and EM training by reducing thenumber of analyzed spans, and so long as we trust2Birch et al (2006) re-introduce inconsistent phrase-pairs incases where the sentence pair could not be aligned otherwise.We allow links to ?
to handle these situations, completely elim-inating the pruned spans from our alignment space.20our high-confidence links, it will do so harmlessly.We demonstrate the effectiveness of this pruningmethod experimentally in Section 5.1.3.2 Handling the ITG ConstraintOur remaining concern is the ITG constraint.
Thereare some alignments that we just cannot build, andsentence pairs requiring those alignments will occur.These could potentially pollute our training data; ifthe system is unable to build the right alignment, thecounts it will collect from that pair must be wrong.Furthermore, if our high-confidence links are notITG-compatible, our fixed-link pruning will preventthe aligner from forming any alignments at all.However, these two potential problems canceleach other out.
Sentence pairs containing non-ITGtranslations will tend to have high-confidence linksthat are also not ITG-compatible.
Our EM learnerwill simply skip these sentence pairs during train-ing, avoiding pollution of our training data.
We canuse a linear-time algorithm (Zhang et al, 2006) todetect non-ITG movement in our high-confidencelinks, and remove the offending sentence pairs fromour training corpus.
This results in only a minor re-duction in training data; in our French-English train-ing set, we lose less than 1%.
In the experiments de-scribed in Section 5, all systems that do not use ITGwill take advantage of the complete training set.4 Applying the modelAny phrasal translation model can be used for twotasks: translation modeling and phrasal word align-ment.
Previous work on JPTM has focused on onlythe first task.
We are interested in phrasal alignmentbecause it may be better suited to heuristic phrase-extraction than word-based models.
This section de-scribes how to use our phrasal ITG first as a transla-tion model, and then as a phrasal aligner.4.1 Translation ModelingWe can test our model?s utility for translation bytransforming its parameters into a phrase table forthe phrasal decoder Pharaoh (Koehn et al, 2003).Any joint model can produce the necessary condi-tional probabilities by conditionalizing the joint ta-ble in both directions.
We use our p(e?/f?
|C) dis-tribution from our stochastic grammar to producep(e?|f?)
and p(f?
|e?)
values for its phrasal lexicon.Pharaoh also includes lexical weighting param-eters that are derived from the alignments used toinduce its phrase pairs (Koehn et al, 2003).
Us-ing the phrasal ITG as a direct translation model,we do not produce alignments for individual sen-tence pairs.
Instead, we provide a lexical preferencewith an IBM Model 1 feature pM1 that penalizes un-matched words (Vogel et al, 2003).
We include bothpM1(e?|f?)
and pM1(f?
|e?
).4.2 Phrasal Word AlignmentWe can produce a translation model using inside-outside, without ever creating a Viterbi parse.
How-ever, we can also examine the maximum likelihoodphrasal alignments predicted by the trained model.Despite its strengths derived from using phrasesthroughout training, the alignments predicted by ourphrasal ITG are usually unsatisfying.
For exam-ple, the fragment pair (order of business, ordre destravaux) is aligned as a phrase pair by our system,linking every English word to every French word.This is frustrating, since there is a clear compo-sitional relationship between the fragment?s com-ponent words.
This happens because the systemseeks only to maximize the likelihood of its train-ing corpus, and phrases are far more efficient thanword-to-word connections.
When aligning text, an-notators are told to resort to many-to-many linksonly when no clear compositional relationship ex-ists (Melamed, 1998).
If we could tell our phrasalaligner the same thing, we could greatly improve theintuitive appeal of our alignments.
Again, we canleverage high-confidence links for help.In the high-confidence alignments provided byGIZA++ intersection, each token participates in atmost one link.
Links only appear when two word-based IBM translation models can agree.
Therefore,they occur at points of high compositionality: thetwo words clearly account for one another.
We adoptan alignment-driven definition of compositional-ity: any phrase pair containing two or more high-confidence links is compositional, and can be sep-arated into at least two non-compositional phrases.By removing any phrase pairs that are compositionalby this definition from our terminal productions,we can ensure that our aligner never creates suchphrases during training or alignment.
Doing so pro-duces far more intuitive alignments.
Aligned with21a model trained using this non-compositional con-straint (NCC), our example now forms three word-to-word connections, rather than a single phrasalone.
The phrases produced with this constraint arevery small, and include only non-compositional con-text.
Therefore, we use the constraint only to trainmodels intended for Viterbi alignment, and not whengenerating phrase tables directly as in Section 4.1.5 Experiments and ResultsIn this section, we first verify the effectiveness offixed-link pruning, and then test our phrasal ITG,both as an aligner and as a translation model.
Wetrain all translation models with a French-EnglishEuroparl corpus obtained by applying a 25 to-ken sentence-length limit to the training set pro-vided for the HLT-NAACL SMT Workshop SharedTask (Koehn and Monz, 2006).
The resulting cor-pus has 393,132 sentence pairs.
3,376 of theseare omitted for ITG methods because their high-confidence alignments have ITG-incompatible con-structions.
Like our predecessors (Marcu and Wong,2002; Birch et al, 2006), we apply a lexicon con-straint: no monolingual phrase can be used by anyphrasal model unless it occurs at least five times.High-confidence alignments are provided by inter-secting GIZA++ alignments trained in each direc-tion with 5 iterations each of Model 1, HMM, andModel 4.
All GIZA++ alignments are trained withno sentence-length limit, using the full 688K corpus.5.1 Pruning Speed ExperimentsTo measure the speed-up provided by fixed-linkpruning, we timed our phrasal inside-outside algo-rithm on the first 100 sentence pairs in our trainingset, with and without pruning.
The results are shownin Table 1.
Tic-tac-toe pruning is included for com-parison.
With fixed-link pruning, on average 95%of the possible spans are pruned, reducing runningtime by two orders of magnitude.
This improvementmakes ITG training feasible, even with large bitexts.5.2 Alignment ExperimentsThe goal of this experiment is to compare the Viterbialignments from the phrasal ITG to gold standardhuman alignments.
We do this to validate our non-compositional constraint and to select good align-ments for use with the surface heuristic.Table 1: Inside-outside run-time comparison.Method Seconds Avg.
Spans PrunedNo Prune 415 -Tic-tac-toe 37 68%Fixed link 5 95%Table 2: Alignment Comparison.Method Prec Rec F-measureGIZA++ Intersect 96.7 53.0 68.5GIZA++ Union 82.5 69.0 75.1GIZA++ GDF 84.0 68.2 75.2Phrasal ITG 50.7 80.3 62.2Phrasal ITG + NCC 75.4 78.0 76.7Following the lead of (Fraser and Marcu, 2006),we hand-aligned the first 100 sentence pairs ofour training set according to the Blinker annota-tion guidelines (Melamed, 1998).
We did not dif-ferentiate between sure and possible links.
We re-port precision, recall and balanced F-measure (Ochand Ney, 2003).
For comparison purposes, we in-clude the results of three types of GIZA++ combina-tion, including the grow-diag-final heuristic (GDF).We tested our phrasal ITG with fixed link prun-ing, and then added the non-compositional con-straint (NCC).
During development we determinedthat performance levels off for both of the ITG mod-els after 3 EM iterations.
The results are shown inTable 2.The first thing to note is that GIZA++ Intersectionis indeed very high precision.
Our confidence in itas a constraint is not misplaced.
We also see thatboth phrasal models have significantly higher recallthan any of the GIZA++ alignments, even higherthan the permissive GIZA++ union.
One factor con-tributing to this is the phrasal model?s use of cepts:it completely interconnects any phrase pair, whileGIZA++ union and GDF may not.
Its global viewof phrases also helps in this regard: evidence for aphrase can be built up over multiple sentences.
Fi-nally, we note that in terms of alignment quality,the non-compositional constraint is an unqualifiedsuccess for the phrasal ITG.
It produces a 25 pointimprovement in precision, at the cost of 2 points22of recall.
This produces the highest balanced F-measure observed on our test set, but the utility ofits alignments will depend largely on one?s desiredprecision-recall trade-off.5.3 Translation ExperimentsIn this section, we compare a number of differentmethods for phrase table generation in a French toEnglish translation task.
We are interested in an-swering three questions:1.
Does the phrasal ITG improve on the C-JPTM?2.
Can phrasal translation models outperform thesurface heuristic?3.
Do Viterbi phrasal alignments provide betterinput for the surface heuristic?With this in mind, we test five phrase tables.
Twoare conditionalized phrasal models, each EM traineduntil performance degrades:?
C-JPTM3 as described in (Birch et al, 2006)?
Phrasal ITG as described in Section 4.1Three provide alignments for the surface heuristic:?
GIZA++ with grow-diag-final (GDF)?
Viterbi Phrasal ITG with and without the non-compositional constraintWe use the Pharaoh decoder (Koehn et al, 2003)with the SMT Shared Task baseline system (Koehnand Monz, 2006).
Weights for the log-linear modelare set using the 500-sentence tuning set providedfor the shared task with minimum error rate train-ing (Och, 2003) as implemented by Venugopaland Vogel (2005).
Results on the provided 2000-sentence development set are reported using theBLEU metric (Papineni et al, 2002).
For all meth-ods, we report performance with and without IBMModel 1 features (M1), along with the size of the re-sulting tables in millions of phrase pairs.
The resultsof all experiments are shown in Table 3.We see that the Phrasal ITG surpasses the C-JPTM by more than 2.5 BLEU points.
A large com-ponent of this improvement is due to the ITG?s useof inside-outside for expectation calculation, though3Supplied by personal communication.
Run with default pa-rameters, but with maximum phrase length increased to 5.Table 3: Translation Comparison.Method BLEU +M1 SizeConditionalized Phrasal ModelC-JPTM 26.27 28.98 1.3MPhrasal ITG 28.85 30.24 2.2MAlignment with Surface HeuristicGIZA++ GDF 30.46 30.61 9.8MPhrasal ITG 30.31 30.39 5.8MPhrasal ITG + NCC 30.66 30.80 9.0Mthere are other differences between the two sys-tems.4 This improvement over search and samplingis demonstrated by the ITG?s larger table size; by ex-ploring more thoroughly, it is extracting more phrasepairs from the same amount of data.
Both systemsimprove drastically with the addition of IBM Model1 features for lexical preference.
These features alsonarrow the gap between the two systems.
To helpcalibrate the contribution of these features, we pa-rameterized the ITG?s phrase table using only Model1 features, which scores 27.17.Although ITG+M1 comes close, neither phrasalmodel matches the performance of the surfaceheuristic.
Whatever the surface heuristic lacks insophistication, it makes up for in sheer coverage,as demonstrated by its huge table sizes.
Even thePhrasal ITG Viterbi alignments, which over-commitwildly and have horrible precision, score slightlyhigher than the best phrasal model.
The surfaceheuristic benefits from capturing as much contextas possible, while still covering smaller translationevents with its flat counts.
It is not held back byany lexicon constraints.
When GIZA++ GDF+M1is forced to conform to a lexicon constraint by drop-ping any phrase with a frequency lower than 5 fromits table, it scores only 29.26, for a reduction of 1.35BLEU points.Phrases extracted from our non-compositionalViterbi alignments receive the highest BLEU score,but they are not significantly better than GIZA++GDF.
The two methods also produce similarly-sizedtables, despite the ITG?s higher recall.4Unlike our system, the Birch implementation does tablesmoothing and internal lexical weighting, both of which shouldhelp improve their results.
The systems also differ in distortionmodeling and ?
handling, as described in Section 3.236 ConclusionWe have presented a phrasal ITG as an alternativeto the joint phrasal translation model.
This syntacticsolution to phrase modeling admits polynomial-timetraining and alignment algorithms.
We demonstratethat the same consistency constraints that allow jointphrasal models to scale also dramatically speed upITGs, producing an 80-times faster inside-outsidealgorithm.
We show that when used to learn phrasetables for the Pharaoh decoder, the phrasal ITG issuperior to the constrained joint phrasal model, pro-ducing tables that result in a 2.5 point improve-ment in BLEU when used alone, and a 1 point im-provement when used with IBM Model 1 features.This suggests that ITG?s perfect expectation count-ing does matter; other phrasal models could benefitfrom either adopting the ITG formalism, or improv-ing their sampling heuristics.We have explored, for the first time, the utility of ajoint phrasal model as a word alignment method.
Wepresent a non-compositional constraint that turns thephrasal ITG into a high-recall phrasal aligner withan F-measure that is comparable to GIZA++.With search and sampling no longer a concern,the remaining weaknesses of the system seem to liewith the model itself.
Phrases are just too efficientprobabilistically: were we to remove all lexicon con-straints, EM would always align entire sentences toentire sentences.
This pressure to always build thelongest phrase possible may be overwhelming oth-erwise strong correlations in our training data.
Apromising next step would be to develop a prior overlexicon size or phrase size, allowing EM to intro-duce large phrases at a penalty, and removing theneed for artificial constraints on the lexicon.Acknowledgments Special thanks to AlexandraBirch for the use of her code, and to our reviewersfor their comments.
The first author is funded byAlberta Ingenuity and iCORE studentships.ReferencesA.
Birch, C. Callison-Burch, M. Osborne, and P. Koehn.
2006.Constraining the phrase-based, joint probability statisticaltranslation model.
In HLT-NAACL Workshop on StatisticalMachine Translation, pages 154?157.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguistics,19(2):263?312.C.
Cherry and D. Lin.
2006.
A comparison of syntacticallymotivated word alignment spaces.
In EACL, pages 145?152.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.
Maxi-mum likelihood from incomplete data via the EM algorithm.Journal of the Royal Statistical Society, 39(1):1?38.A.
Fraser and D. Marcu.
2006.
Semi-supervised training forstatistical word alignment.
In ACL, pages 769?776.P.
Koehn and C. Monz.
2006.
Manual and automatic evalu-ation of machine translation.
In HLT-NACCL Workshop onStatistical Machine Translation, pages 102?121.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statistical phrase-based translation.
In HLT-NAACL, pages 127?133.D.
Marcu and W. Wong.
2002.
A phrase-based, joint probabil-ity model for statistic machine translation.
In EMNLP, pages133?139.I.
D. Melamed.
1998.
Manual annotation of translationalequivalence: The blinker project.
Technical Report 98-07,Institute for Research in Cognitive Science.I.
D. Melamed.
2003.
Multitext grammars and synchronousparsers.
In HLT-NAACL, pages 158?165.F.
J. Och and H. Ney.
2003.
A systematic comparison of vari-ous statistical alignment models.
Computational Linguistics,29(1):19?52.F.
J. Och.
2003.
Minimum error rate training for statisticalmachine translation.
In ACL, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.
BLEU:a method for automatic evaluation of machine translation.
InACL, pages 311?318.A.
Venugopal and S. Vogel.
2005.
Considerations in maximummutual information and minimum classification error train-ing for statistical machine translation.
In EAMT.J.
M. Vilar and E. Vidal.
2005.
A recursive statistical transla-tion model.
In Proceedings of the ACL Workshop on Build-ing and Using Parallel Texts, pages 199?207.S.
Vogel, Y. Zhang, F. Huang, A. Tribble, A. Venugopal,B.
Zhang, and A. Waibel.
2003.
The CMU statistical ma-chine translation system.
In MT Summmit.D.
Wu.
1997.
Stochastic inversion transduction grammars andbilingual parsing of parallel corpora.
Computational Lin-guistics, 23(3):377?403.H.
Zhang and D. Gildea.
2004.
Syntax-based alignment: Su-pervised or unsupervised?
In COLING, pages 418?424.H.
Zhang and D. Gildea.
2005.
Stochastic lexicalized inversiontransduction grammar for alignment.
In ACL, pages 475?482.H.
Zhang, L. Huang, D. Gildea, and K. Knight.
2006.
Syn-chronous binarization for machine translation.
In HLT-NAACL, pages 256?263.24
