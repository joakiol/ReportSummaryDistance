Unsupervised Classification of Sentiment and Objectivityin Chinese TextTarasZagibalov John CarrollUniversity of SussexDepartment of InformaticsBrighton BN1 9QH, UK{T.Zagibalov,J.A.Carroll}@sussex.ac.ukAbstractWe address the problem of sentiment andobjectivity classification of product re-views in Chinese.
Our approach is distinct-ive in that it treats both positive / negativesentiment and subjectivity / objectivity notas distinct classes but rather as a con-tinuum; we argue that this is desirable fromthe perspective of would-be customers whoread the reviews.
We use novel unsuper-vised techniques, including a one-word'seed' vocabulary and iterative retrainingfor sentiment processing, and a criterion of'sentiment density' for determining the ex-tent to which a document is opinionated.The classifier achieves up to 87% F-meas-ure for sentiment polarity detection.1 IntroductionAutomatic classification of sentiment has been afocus of a number of recent research efforts (e.g.
(Turney, 2002; Pang et al, 2002; Dave at al.,2003).
An important potential application of suchwork is in business intelligence: brands and com-pany image are valuable property, so organizationswant to know how they are viewed by the media(what the 'spin' is on news stories, and editorials),business analysts (as expressed in stock market re-ports), customers (for example on product reviewsites) and their own employees.
Another importantapplication is to help people find out others' viewsabout products they have purchased (e.g.
consumerelectronics), services and entertainment (e.g.movies), stocks and shares (from investor bulletinboards), and so on.
In the work reported in this pa-per we focus on product reviews, with the intendedusers of the processing being would-be customers.Our approach is based on the insight that posi-tive and negative sentiments are extreme points ina continuum of sentiment, and that intermediatepoints in this continuum are of potential interest.For instance, in one scenario, someone might wantto get an idea of the types of things people are say-ing about a particular product through reading asample of reviews covering the spectrum fromhighly positive, through balanced, to highly nega-tive.
(We call a review balanced if it is an opinion-ated text with an undecided or weak sentiment di-rection).
In another scenario, a would-be customermight only be interested in reading balanced re-views, since they often present more reasoned ar-guments with fewer unsupported claims.
Such aperson might therefore want to avoid reviews suchas Example (1) ?
written by a Chinese purchaser ofa mobile phone (our English gloss).(1)??????????????????????????????????????????????????????????????????????????????????????????
?The software is bad, some sent SMS are nev-er received by the addressee; compatibilityis also bad, on some mobile phones the re-ceived messages are in a scrambled encod-ing!
And sometimes the phone 'dies'!
Photosare horrible!
It doesn't have a cyclic or pro-304grammable alarm-clock, you have to set itevery time, how cumbersome!
The back cov-er does not fit!
The original software hasmany holes!In a third scenario, someone might decide theywould like only to read opinionated, weakly nega-tive reviews such as Example (2), since these oftencontain good argumentation while still identifyingthe most salient bad aspects of a product.(2)??????????????????30KB??????????MP3??????????????????????????????????????????????????????????????????????
?The response time of this mobile is verylong, MMS should be less than 30kb only tobe downloaded, also it doesn't support MP3ring tones, (while) the built-in tunes are notgood, and from time to time it 'dies', butwhen I was buying it I really liked it: veryoriginal, very nicely matching red and whitecolours, it has its individuality, also it's notexpensive, but when used it always causestrouble, makes one's head acheThe review contains both positive and negativesentiment covering different aspects of the product,and the fact that it contains a balance of viewsmeans that it is likely to be useful for a would-becustomer.
Moving beyond review classification,more advanced tasks such as automatic summa-rization of reviews (e.g.
Feiguina & LaPalme,2007) might also benefit from techniques whichcould distinguish more shades of sentiment thanjust a binary positive / negative distinction.A second dimension, orthogonal to positive /negative, is opinionated / unopinionated (or equiv-alently subjective / objective).
When shopping fora product, one might be interested in the physicalcharacteristics of the product or what features theproduct has, rather than opinions about how wellthese features work or about how well the productas a whole functions.
Thus, if one is looking for areview that contains more factual information thanopinion, one might be interested in reviews likeExample (3).(3)????????????????????????5??800??500???????????????????????WAP????????????
(My) overall feeling about this mobile is notbad, it features: 5 alarm-clocks that switchthe phone on (off), phone book for 800 items(500 people), lunar and solar calendars,fast switching between time and date modes,WAPnetworking, organizer,notebook andso on.This review is mostly neutral (unopinionated), butcontains information that could be useful to awould-be customer which might not be in a prod-uct specification document, e.g.
fast switching be-tween different operating modes.
Similarly, would-be customers might be interested in retrievingcompletely unopinionated documents such as tech-nical descriptions and user manuals.
Again, as withsentiment classification, we argue that opinionatedand unopinionated texts are not easily distinguish-able separate sets, but form a continuum.
In thiscontinuum, intermediate points are of interest aswell as the extremes.A major obstacle for automatic classification ofsentiment and objectivity is lack of training data,which limits the applicability of approaches basedon supervised machine learning.
With the rapidgrowth in textual data and the emergence of newdomains of knowledge it is virtually impossible tomaintain corpora of tagged data that cover all ?
oreven most ?
areas of interest.
The cost of manualtagging also adds to the problem.
Reusing the samecorpus for training classifiers for new domains isalso not effective: several studies report decreasedaccuracy in cross-domain classification (Engstr?m,2004; Aue & Gamon, 2005) a similar problem hasalso been observed in classification of documentscreated over different time periods (Read, 2005).In this paper we describe an unsupervised classi-fication technique which is able to build its ownsentiment vocabulary starting from a very smallseed vocabulary, using iterative retraining to en-large the vocabulary.
In order to avoid problems ofdomain dependence, the vocabulary is built usingtext from the same source as the text which is to beclassified.
In this paper we work with Chinese, butusing a very small seed vocabulary may mean thatthis approach would in principle need very littlelinguistic adjustment to be applied to a different305language.
Written Chinese has some specific fea-tures, one of which is the absence of explicitlymarked word boundaries, which makes word-basedprocessing problematic.
In keeping with our unsu-pervised, knowledge-poor approach, we do not useany preliminary word segmentation tools or higherlevel grammatical analysis.The paper is structured as follows.
Section 2 re-views related work in sentiment classification andmore generally in unsupervised training of classi-fiers.
Section 3 describes our datasets, and Section4 the techniques we use for unsupervised classifi-cation and iterative retraining.
Sections 5 and 6 de-scribe a number of experiments into how well theapproaches work, and Section 7 concludes.2 Related Work2.1 Sentiment ClassificationMost previous work on the problem of categoriz-ing opinionated texts has focused on the binaryclassification of positive and negative sentiment(Turney, 2002; Pang et al, 2002; Dave at al.,2003).
However, Pang & Lee (2005) describe anapproach closer to ours in which they determine anauthor's evaluation with respect to a multi-pointscale, similar to the 'five-star' sentiment scalewidely used on review sites.
However, authors ofreviews are inconsistent in assigning fine-grainedratings and quite often star systems are not consis-tent between critics.
This makes their approachvery author-dependent.
The main differences arethat Pang and Lee use discrete classes (althoughmore than two), not a continuum as in our ap-proach, and use supervised machine learning ratherthan unsupervised techniques.
A similar approachwas adopted by Hagedorn et al (2007), applied tonews stories: they defined five classes encodingsentiment intensity and trained their classifier on amanually tagged training corpus.
They note thatworld knowledge is necessary for accurate classifi-cation in such open-ended domains.There has also been previous work on determin-ing whether a given text is factual or expressesopinion (Yu& Hatzivassiloglu, 2003; Pang & Lee,2004); again this work uses a binary distinction,and supervised rather than unsupervised approach-es.Recent work on classification of terms with re-spect to opinion (Esuli & Sebastiani, 2006) uses athree-category system to characterize the opinion-related properties of word meanings, assigning nu-merical scores to Positive, Negative and Objectivecategories.
The visualization of these scores some-what resembles our graphs in Section 5, althoughwe use two orthogonal scales rather than three cat-egories; we are also concerned with classificationof documents rather than terms.2.2 Unsupervised ClassificationAbney (2002) compares two major kinds of unsu-pervised approach to classification (co-training andthe Yarowsky algorithm).
As we do not use multi-ple classifiers our approach is quite far from co-training.
But it is close to the paradigm describedby Yarowsky (1995) and Turney (2002) as it alsoemploys self-training based on a relatively smallseed data set which is incrementally enlarged withunlabelled samples.
But our approach does not usepoint-wise mutual information.
Instead we use rel-ative frequencies of newly found features in atraining subcorpus produced by the previous itera-tion of the classifier.
We also use the smallest pos-sible seed vocabulary, containing just a singleword; however there are no restrictions regardingthe maximum number of items in the seed vocabu-lary.3 Data3.1 Seed VocabularyOur approach starts out with a seed vocabularyconsisting of a single word, ?
(good).
This wordis tagged as a positive vocabulary item; initiallythere are no negative items.
The choice of wordwas arbitrary, and other words with strongly posi-tive or negative meaning would also be plausibleseeds.
Indeed, ?
might not be the best possibleseed, as it is relatively ambiguous: in some con-texts it means to like or acts as the adverbial very,and is often used as part of other words (althoughusually contributing a positive meaning).
But sinceit is one of the most frequent units in the Chineselanguage, it is likely to occur in a relatively largenumber of reviews, which is important for therapid growth of the vocabulary list.3.2 TestCorpusOur test corpus is derived from product reviewsharvested from the website IT1681.
All the reviewswere tagged by their authors as either positive ornegative overall.
Most reviews consist of two orthree distinct parts: positive opinions, negativeopinions, and comments ('other') ?
although some1http://product.it168.com306reviews have only one part.
We removed duplicatereviews automatically using approximate match-ing, giving a corpus of 29531 reviews of which23122 are positive (78%) and 6409 are negative(22%).
The total number of different products inthe corpus is 10631, the number of product cate-gories is 255, and most of the reviewed productsare either software products or consumer electron-ics.
Unfortunately, it appears that some users mis-used the sentiment tagging facility on the websiteso quite a lot of reviews have incorrect tags.
How-ever, the parts of the reviews are much more reli-ably identified as being positive or negative so weused these as the items of the test corpus.
In the ex-periments described in this paper we used 2317 re-views of mobile phones of which 1158 are nega-tive and 1159 are positive.
Thus random choicewould have approximately 50% accuracy if allitems were tagged either as negative or positive2.4 Method4.1 Sentiment ClassificationAs discussed in Section 1, we do not carry out anyword segmentation or grammatical processing ofinput documents.
We use a very broad notion ofwords (or phrases) in the Chinese language.
Thebasic units of processing are 'lexical items', each ofwhich is a sequence of one or more Chinese char-acters excluding punctuation marks (which mayactually form part of a word, a whole word or a se-quence of words), and `zones', each of which is asequence of characters delimited by punctuationmarks.Each zone is classified as either positive or neg-ative based whether positive or negative vocabu-lary items predominate.
In more detail, a simplemaximum match algorithm is used to find all lexi-cal items (character sequences) in the zone that arein the vocabulary list.
As there are two parts of thevocabulary (positive and negative), we correspond-ingly calculate two scores using Equation (1)3,S i=LdL phraseS d N d (1)where Ld is the length in characters of a matchinglexical item, Lphrase is the length of the current zone2This corpus is publicly available at http://www.informatics.sussex.ac.uk/users/tz21/it168test.zip3In the first iteration, when we have only one item in the vo-cabulary, negative zones are found by means of the negationcheck (so not + good = negative item).in characters, Sd is the current sentiment score ofthe matching lexical item (initially 1.0), and Nd is anegation check coefficient.
The negation check is aregular expression which determines if the lexicalitem is preceded by a negation within its enclosingzone.
If a negation is found then Nd is set to ?1.The check looks for six frequently occurring nega-tions:?
(bu),??
(buhui),??
(meiyou),??(baituo),??
(mianqu), and??
(bimian).The sentiment score of a zone is the sum of sen-timent scores of all the items found in it.
In factthere are two competing sentiment scores for everyzone: one positive (the sum of all scores of itemsfound in the positive part of the vocabulary list)and one negative (the sum of the scores for theitems in the negative part).
The sentiment directionof a zone is determined from the maximum of theabsolute values of the two competing scores for thezone.This procedure is applied to all zones in a docu-ment, classifying each zone as positive, negative,or neither (in cases where there are no positive ornegative vocabulary items in the zone).
To deter-mine the sentiment direction of the whole docu-ment, the classifier computes the difference be-tween the number of positive and negative zones.If the result is greater than zero the document isclassified as positive, and vice versa.
If the result iszero the document is balanced or neutral for senti-ment.4.2 Iterative RetrainingThe task of iterative retraining is to enlarge the ini-tial seed vocabulary (consisting of a single word asdiscussed in Section 3.1) into a comprehensive vo-cabulary list of sentiment-bearing lexical items.
Ineach iteration, the current version of the classifieris run on the product review corpus to classify eachdocument, resulting in a training subcorpus of pos-itive and a negative documents.
The subcorpus isused to adjust the scores of existing positive andnegative vocabulary items and to find new items tobe included in the vocabulary.Each lexical item that occurs at least twice in thecorpus is a candidate for inclusion in the vocabu-lary list.
After candidate items are found, the sys-tem calculates their relative frequencies in both thepositive and negative parts of the current trainingsubcorpus.
The system also checks for negationwhile counting occurrences: if a lexical item is pre-ceded by a negation, its count is reduced by one.This results in negative counts (and thus negativerelative frequencies and scores) for those items that307are usually used with negation; for example, ?????
(the quality is far too bad) is in the positivepart of the vocabulary with a score of ?1.70.
Thismeans that the item was found in reviews classifiedby the system as positive but it was preceded by anegation.
If during classification this item is foundin a document it will reduce the positive score forthat document (as it is in the positive part of thevocabulary), unless the item is preceded by a nega-tion.
In this situation the score will be reversed(multiplied by ?1), and the positive score will beincreased ?
see Equation (1) above.For all candidate items we compare their relativefrequencies in the positive and negative documentsin the subcorpus using Equation (2).difference= ?F p?
F n?
?F p?Fn?/2(2)If difference < 1, then the frequencies are similarand the item does not have enough distinguishingpower, so it is not included in the vocabulary.
Oth-erwise the the sentiment score of the item is (re-)calculated ?
according to Equation (3) for positiveitems, and analogously for negative items.F pF p?F n(3)Finally, the adjusted vocabulary list with the newscores is ready for the next iteration.4.3 Objectivity ClassificationGiven a sentiment classification for each zone in adocument, we compute sentiment density as theproportion of opinionated zones with respect to thetotal number of zones in the document.
Sentimentdensity measures the proportion of opinionated textin a document, and thus the degree to which thedocument as a whole is opinionated.It should be noted that neither sentiment scorenor sentiment density are absolute values, but arerelative and only valid for comparing one docu-ment with other.
Thus, a sentiment density of 0.5does not mean that the review is half opinionated,half not.
It means that the review is less opinionat-ed than a review with density 0.9.5 ExperimentsWe ran the system on the product review corpus(Section 3.2) for 20 iterations.
The results for bina-ry sentiment classification are shown in Table 1.We see increasing F-measure up to iteration 18, af-ter which both precision and recall start to de-screase; we therefore use the version of the classi-fier as it stood after iteration 184.
These figures areonly indicative of the classification accuracy of thesystem.
Accuracy might be lower for unseen text,although since our approach is unsupervised wecould in principle perform further retraining itera-tions on any sample of new text to tune the vocab-ulary list to it.We also computed a (strong) baseline, using asthe vocabulary list the NTU Sentiment Dictionary(Ku et al, 2006)5 which is intended to contain onlysentiment-related words and phrases.
We assignedeach positive and negative vocabulary item a scoreof 1 or ?1 respectively.
This setup achieved 87.77precision and 77.09 recall on the product reviewcorpus.In Section 1 we argued that sentiment and objec-tivity should both be considered as continuums, notTable 1.
Results for binary sentiment classifica-tion during iterative retraining.4The size of the sentiment vocabulary after iteration 18 was22530 (13462 positive and 9068 negative).5Ku et al automatically generated the dictionary by enlargingan initial manually created seed vocabulary by consulting twothesauri, including tong2yi4ci2ci2lin2 and the Academia Sini-ca Bilingual Ontological WordNet 3.Iteration Precision Recall F-measure1 77.62 28.43 41.622 76.15 73.81 74.963 81.15 80.07 80.614 83.54 82.79 83.165 84.66 83.78 84.226 85.51 84.77 85.147 86.59 85.76 86.178 86.78 86.11 86.449 87.15 86.32 86.7410 87.01 86.37 86.6911 86.9 86.15 86.5312 87.05 86.41 86.7313 86.87 86.19 86.5314 87.35 86.67 87.0115 87.13 86.45 86.7916 87.14 86.5 86.8217 86.8 86.24 86.5218 87.57 86.89 87.2219 87.23 86.67 86.9520 87.18 86.54 86.86308binary distinctions.
Section 4.1 describes how ourapproach compares the number of positive andnegative zones for a document and treats the differ-ence as a measure of the 'positivity' or 'negativity'of a review.
The document in Example (2), with 12zones, is assigned a score of ?1 (the least negativescore possible): the review contains some positivesentiment but the overall sentiment direction of thereview is negative.
In contrast, Example (1) isidentified as a highly negative review, as would beexpected, with a score of ?8, from 11 zones.Similarly, with regard to objectivity, the senti-ment density of the text in Example (3) is 0.53,which reflects its more factual character comparedto Example (1), which has a score of 0.91.
We canrepresent sentiment and objectivity on the follow-ing scales:Negative Balanced PositiveUnopinionated Neutral OpinionatedThe scales are orthogonal, so we can combinethem into a single coordinate system:OpinionatedNegative PositiveWe would expect most product reviews to beplaced towards the top of the the coordinate system(i.e.
opinionated), and stretch from left to right.Figure 1 plots the results of sentiment and objec-tivity classification of the test corpus in this two di-mensional coordinate system, where X representssentiment (with scores scaled with respect to thenumber of zones so that ?100 is the most negativepossible and +100 the most positive), and Y repre-sents sentiment density (0 being unopinionated and1 being highly opinionated).Most of the reviews are located in the upper partof the coordinate system, indicating that they havebeen classified as opinionated, with either positiveor negative sentiment direction.
Looking at theoverall shape of the plot, more opinionated docu-ments tend to have more explicit sentiment direc-tion, while less opinionated texts stay closer to thebalanced / neutral region (around X = 0).Figure 1.
Reviews classified according tosentiment (X axis) and degree ofopinionation (Y axis).6 DiscussionAs can be seen in Figure 1, the classifier managedto map the reviews onto the coordinate system.However, there are very few points in the neutralregion, that is, on the same X = 0 line as balancedbut with low sentiment density.
By inspection, weknow that there are neutral reviews in our data set.We therefore conducted a further experiment to in-vestigate what the problem might be.
We tookWikipedia6 articles written in Chinese on mobiletelephony and related issues, as well as several ar-ticles about the technology, the market and the his-tory of mobile telecommunications, and split theminto small parts (about a paragraph long, to maketheir size close to the size of the reviews) resultingin a corpus of 115 documents, which we assume tobe mostly unopinionated.
We processed these doc-uments with the trained classifier and found thatthey were mapped almost exactly where balanceddocuments should be (see Figure 2).Most of these documents have weak sentimentdirection (X = ?5 to +10), but are classified as rel-atively opinionated (Y > 0.5).
The former is to beexpected, whereas the latter is not.
When investi-gating the possible reasons for this behavior we no-ticed that the classifier found not only feature de-scriptions (like ????
nice touch) or expres-sions which describe attitude (??
(one) like(s)),but also product features (for example,??
MMSor ??
TV) to be opinionated.
This is because thepresence of some advanced features such as MMSin mobile phones is often regarded as a positive by6www.wikipedia.org-40 -30 -20 -10 0 10 20 30 40 50 6000.10.20.30.40.50.60.70.80.91309Figure 2.
Classification of a sample of articlesfrom Wikipedia.Figure 3.
Classification of a sample of articlesfrom Wikipedia, using the NTU SentimentDictionary as the vocabulary list.authors of reviews.
In addition, the classifier foundwords that were used in reviews to describe situa-tions connected with a product and its features: forexample,??
(service) was often used in descrip-tions of quite unpleasant situations when a user hadto turn to a manufacturer's post-sales service for re-pair or replacement of a malfunctioning phone, and??
(user) was often used to describe what onecan do with some advanced features.
Thus the clas-sifier was able to capture some product-specific aswell as market-specific sentiment markers, howev-er, it was not able to distinguish the context thesegenerally objective words were used in.
This re-sulted in relatively high sentiment density of neu-tral texts which contained these words but used inother types of context.To verify this hypothesis we applied the sameprocessing to our corpus derived from Wikipediaarticles, but using as the vocabulary list the NTUSentiment Dictionary.
The results (Figure 3) showthat most of the neutral texts are now mapped tothe lower part of the opinionation scale (Y < 0.5),as expected.
Therefore, to successfully distinguishbetween balanced reviews and neutral documents aclassifier should be able to detect when productfeatures are used as sentiment markers and whenthey are not.7 Conclusions and Future WorkWe have described an approach to classification ofdocuments with respect to sentiment polarity andobjectivity, representing both as a continuum, andmapping classified documents onto a coordinatesystem that also represents the difference betweenbalanced and neutral text.
We have presented anovel, unsupervised, iterative retraining procedurefor deriving the classifier, starting from the mostminimal size seed vocabulary, in conjunction witha simple negation check.
We have verified that theapproach produces reasonable results.
The ap-proach is extremely minimal in terms of languageprocessing technology, giving it good possibilitiesfor porting to different genres, domains and lan-guages.We also found that the accuracy of the methoddepends a lot on the seed word chosen.
If the wordhas a relatively low frequency or does not have adefinite sentiment-related meaning, the results maybe very poor.
For example, an antonymous word to?
(good) in Chinese is ?
(bad), but the latter isnot a frequent word: the Chinese prefer to say??
(not good).
When this word was used as the seedword, accuracy was little more than 15%.
Al-though the first iteration produced high precision(82%), the size of the extracted subcorpus wasonly 24 items, resulting in the system being unableto produce a good classifier for the following itera-tions.
Every new iteration produced an even poorerresult as each new extracted corpus was of loweraccuracy.On the other hand, it seems that a seed list con-sisting of several low-frequency one-characterwords can compensate each other and produce bet-ter results by capturing a larger part of the corpus(thus increasing recall).
Nevertheless a single wordmay also produce results even better than those formultiword seed lists.
For example, the two-charac-ter word ??
(comfortable) as seed reached 91%-40 -30 -20 -10 0 10 20 30 40 50 6000.10.20.30.40.50.60.70.80.91-40 -30 -20 -10 0 10 20 30 40 50 6000.10.20.30.40.50.60.70.80.91310accuracy with 90% recall.
We can conclude thatour method relies on the quality of the seed word.We therefore need to investigate ways of choosing'lucky' seeds and avoiding 'unlucky' ones.Future work should also focus on improvingclassification accuracy: adding a little language-specific knowledge to be able to detect some wordboundaries should help; we also plan to experimentwith more sophisticated methods of sentimentscore calculation.
In addition, the notion of 'zone'needs refining and language-specific adjustments(for example, a 'reversed comma' should not beconsidered to be a zone boundary marker, sincethis punctuation mark is generally used for the enu-meration of related objects).More experiments are also necessary to deter-mine how the approach works across domains, andfurther investigation into methods for distinguish-ing between balanced and neutral text.Finally, we need to produce a new corpus thatwould enable us to evaluate the performance of apre-trained version of the classifier that did nothave any prior access to the documents it was clas-sifying: we need the reviews to be tagged not in abinary way as they are now, but in a way that re-flects the two continuums we use (sentiment andobjectivity).AcknowledgementsThe first author is supported by the Ford Founda-tion International Fellowships Program.ReferencesAbney, Steven (2002) Bootstrapping.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, Philadelphia, PA. 360?367.Aue, Anthony & Michael Gamon (2005) Customizingsentiment classifiers to new domains: a case study.
InProceedings of RANLP-2005.Dave, Kushal, Steve Lawrence & David M. Pennock(2003) Mining the peanut gallery: opinion extractionand semantic classification of product reviews.
InProceedings of the Twelfth International World WideWebConference.
519?528.Engstr?m, Charlotte (2004) Topic dependence in senti-ment classification.
Unpublished MPhil dissertation,University of Cambridge.Esuli, Andrea & Fabrizio Sebastiani (2006) SENTI-WORDNET: a publicly available lexical resource foropinion mining.
In Proceedings of LREC-06, the 5thConference on Language Resources and Evaluation,Genoa, Italy.Hagedorn, Bennett, Massimiliano Ciaramita & Jordi At-serias (2007) World knowledge in broad-coverage in-formation filtering.
In Proceedings of the 30th ACMSIGIR Conference on Research and Development inInformation Retrieval.
801?802.Ku, Lun-Wei, Yu-Ting Liang & Hsin-Hsi Chen (2006)Opinion extraction, summarization and tracking innews and blog corpora.
In Proceedings of the AAAI-2006 Spring Symposium on Computational Ap-proaches to Analyzing Weblogs, AAAI Technical Re-port.Feiguina, Olga & Guy Lapalme (2007) Query-basedsummarization of customer reviews.
In Proceedingsof the 20th Canadian Conference on Artificial Intelli-gence, Montreal, Canada.
452?463.Pang, Bo, Lillian Lee & Shivakumar Vaithyanathan(2002) Thumbs up?
Sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, Philadelphia, PA. 79?86.Pang, Bo & Lillian Lee (2004) A sentimental education:sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proceedings of the 42ndAnnual Meeting of the Association for Computation-al Linguistics, Barcelona, Spain.
271?278.Pang, Bo & Lillian Lee (2005) Seeing stars: exploitingclass relationships for sentiment categorization withrespect to rating scales.
In Proceedings of the 43rdAnnual Meeting of the Association for Computation-al Linguistics, Ann Arbor, MI.
115?124.Read, Jonathon (2005) Using emoticons to reduce de-pendency in machine learning techniques for senti-ment classification.
In Proceedings of the StudentResearch Workshop at ACL-05, Ann Arbor, MI.Turney, Peter D. (2002) Thumbs up or thumbs down?Semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Lin-guistics, Philadelphia, PA. 417?424.Yarowsky, David (1995) Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of the 33rd Annual Meeting of the Associa-tion for Computational Linguistics, Cambridge, MA.189?196.Yu, Hong & Vasileios Hatzivassiloglou (2003) Towardsanswering opinion questions: separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of the 2003 Conference onEmpirical Methods in Natural Language Processing,Sapporo, Japan.
129?136.311
