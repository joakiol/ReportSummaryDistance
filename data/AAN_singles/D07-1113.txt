Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
1056?1064, Prague, June 2007. c?2007 Association for Computational LinguisticsCrystal: Analyzing Predictive Opinions on the WebSoo-Min Kim and Eduard HovyUSC Information Sciences Institute4676 Admiralty Way, Marina del Rey, CA 90292{skim,hovy}@ISI.EDUAbstractIn this paper, we present an election predic-tion system (Crystal) based on web users?opinions posted on an election predictionwebsite.
Given a prediction message, Crys-tal first identifies which party the messagepredicts to win and then aggregates predic-tion analysis results of a large amount ofopinions to project the election results.
Wecollect past election prediction messagesfrom the Web and automatically build agold standard.
We focus on capturing lexi-cal patterns that people frequently usewhen they express their predictive opinionsabout a coming election.
To predict elec-tion results, we apply SVM-based super-vised learning.
To improve performance,we propose a novel technique which gener-alizes n-gram feature patterns.
Experimen-tal results show that Crystal significantlyoutperforms several baselines as well as anon-generalized n-gram approach.
Crystalpredicts future elections with 81.68% accu-racy.1 IntroductionAs a growing number of people use the Web as amedium for expressing their opinions, the Web isbecoming a rich source of various opinions in theform of product reviews, travel advice, social issuediscussions, consumer complaints, stock marketpredictions, real estate market predictions, etc.At least two categories of opinions can be iden-tified.
One consists of opinions such as ?Ilike/dislike it?, and the other consists of opinionslike ?It is likely/unlikely to happen.?
We call thefirst category Judgment Opinions and the second(those discussing the future) Predictive Opinions.Judgment opinions express positive or negativesentiment about a topic such as, for example, re-views about cameras, movies, books, or hotels, anddiscussions about topics like abortion and war.
Incontrast, predictive opinions express a person'sopinion about the future of a topic or event such asthe housing market, a popular sports match, andnational election, based on his or her belief andknowledge.Due to the different nature of these two catego-ries of opinion, each has different valences.
Judg-ment opinions have core valences of positive andnegative.
For example, ?liking a product?
and?supporting abortion?
have the valence ?positive?toward each topic (namely ?a product?
and ?abor-tion?).
Predictive opinions have the core valence oflikely or unlikely predicated on the event.
For ex-ample, a sentence ?Housing prices will go downsoon?
carries the valence of ?likely?
for the eventof ?housing prices go down?.The two types of opinions can co-appear.
Thesentence ?I like Democrats but I think they are notlikely to win considering the war issue?
containsboth types of opinion: ?positive?
valence towardsDemocrats and ?unlikely?
valence towards theevent of ?Democrats wins?.
In order to accuratelyidentify and analyze each type of opinion, differentapproaches are desirable.Note that our work is different from predictivedata mining which models a data mining systemusing statistical approaches in order to forecast thefuture or trace a pattern of interest (Rickel and Por-ter, 1997; Rodionov and Martin, 1996).
Exampledomains of predictive data mining include earth-quake prediction, air temperature prediction, for-eign exchange prediction, and energy price predic-1056tion.
However, predictive data mining is only fea-sible when a large amount of structured numericaldata (e.g., in a database) is available.
Unlike thisresearch area which analyzes numeric values, ourstudy mines unstructured text using NLP tech-niques and it can potentially extend the reach ofnumeric techniques.Despite the vast amount of predictive opinionsand their potential applications such as identifica-tion and analysis of people's opinions about thereal estate market or a specific country's economicfuture, studies on predictive opinions have beenneglected in Computational Linguistics, wheremost previous work focuses on judgment opinions(see Section 2).
In this paper, we concentrate onidentifying predictive opinion with its valence.Among many prediction domains on the Web,we focus on election prediction and introduceCrystal, a system to predict election results usingthe public's written viewpoints.
To build our sys-tem, we collect opinions about past electionsposted on an election prediction project websitebefore the election day, and build a corpus1.
Wethen use this corpus to train our system for analyz-ing predictive opinion messages and, using this, topredict the election outcome.
Due to the availabil-ity of actual results of the past elections, we cannot only evaluate how accurately Crystal analyzesprediction messages (by checking agreement withthe gold standard), but also objectively measure theprediction accuracy of our system.The main contributions of this work are as fol-lows:?
an NLP technique for analyzing predictiveopinions in the electoral domain;?
a method of automatically building a corpusof predictive opinions for a supervisedlearning approach; and?
a feature generalization technique that out-performs all the baselines on the task ofidentifying a predicted winning party givena predictive opinion.The rest of this paper is structured as follows.Section 2 surveys previous work.
Section 3 for-mally defines our task and describes our data set.Section 4 describes our system Crystal with pro-posed feature generalization algorithm.
Section 51 The resulting corpus is available athttp://www.isi.edu/ ~skim/Download/Data/predictive.htmreports empirical evidence that Crystal outper-forms several baseline systems.
Finally, Section 6concludes with a description of the impact of thiswork.2 Related WorkThis work is closely related to opinion analysis andtext classification.
Most research on opinion analy-sis in computational linguistics has focused on sen-timent analysis, subjectivity detection, and reviewmining.
Pang et al (2002) and Turney (2002) clas-sified sentiment polarity of reviews at the docu-ment level.
Wiebe et al (1999) classified sentencelevel subjectivity using syntactic classes such asadjectives, pronouns and modal verbs as features.Riloff and Wiebe (2003) extracted subjective ex-pressions from sentences using a bootstrappingpattern learning process.
Wiebe et.
al (2004) andRiloff et.
al (2005) adopted pattern learning withlexical feature generalization for subjective expres-sion detection.
Dave et.
al (2003) and Jindal andLiu (2006) also learned patterns of opinion expres-sion in product reviews.
Yu and Hatzivassiloglou(2003) identified the polarity of opinion sentencesusing semantically oriented words.
These tech-niques were applied and examined in different do-mains, such as customer reviews (Hu and Liu2004; Popescu et al, 2005) and news articles (Kimand Hovy, 2004; Wilson et al, 2005).In text classification, systems typically use bag-of-words models, mostly with supervised learningalgorithms using Naive Bayes or Support VectorMachines (Joachims, 1998) to classify documentsinto several categories such as sports, art, politics,and religion.
Liu et al (2004) and Gliozzo et al(2005) address the difficulty of obtaining trainingcorpora for supervised learning and propose unsu-pervised learning approaches.
Another recent re-lated classification task focuses on academic andcommercial efforts to detect email spam messages.For an SVM-based approach, see (Drucker et al,1999).
In our study, we explore the use of general-ized lexical features for predictive opinion analysisand compare it with the bag-of-words approach.3 Modeling PredictionIn this section, we define the task of analyzing pre-dictive opinions in the electoral domain.10573.1 Task DefinitionWe model predictive opinions in an election asfollows:Valence) (Party,i ni onedicti onOpElecti onPr =where Party is a political party running for an elec-tion (e.g., Democrats and Republicans) and Va-lence is the valence of a predictive opinion whichcan be either ?likely to win?
(WIN) or ?unlikely towin?
(LOSE).
Values for Party vary depending onin which year (e.g., 1996 and 2006) and where anelection takes place (e.g., United States, France, orJapan).
The unit of a predictive opinion is an un-structured textual document such as an article in apersonal blog or a message posted on a news groupdiscussion board about the topic of ?Which partydo you think will win/lose in this election?
?.Figure 1 illustrates an overview of our electionprediction system Crystal in action.
Given eachdocument posted on blogs or message boards (e.g.,www.election prediction.org) as seen in Figure 1.a,a system can determine a Party that the author of adocument thinks to win or lose (Valence), Figure1.b.
For the example document starting with thesentence ?I think this riding will stay NDP as it hasfor the past 11 years.?
in Figure 1.a, our predictiveopinion analysis system aims to recognize NDP asParty and WIN as Valence.
After aggregating thepredictive opinion analysis results of all docu-ments, we project the election results in Figure 1.c.The following section describes how we obtain ourdata set and the subsequent sections describe Crys-tal.3.2 Automatically Labeled DataWe collected messages posted on an election pre-diction project page, www.electionprediction.
org.The website contains various election predictionprojects (e.g., provincial election, federal election,and general election) of different countries (e.g.,Canada and United Kingdom) from 1999 to 2006.For our data set, we downloaded Canadian federalelection prediction data for 2004 and 2006.
TheCanadian federal electoral system is based on 308Figure 1.
Our election prediction system.
Publicopinions are collected from message boards (a)and our system determines for each the electionprediction ?Party?
and ?Valence?
(b).
The outputof the system is a prediction of the election out-come (c).Message text Predicted  winning party Riding Year???
???
???
??
?Message_1457 Party_3 Riding_206 2004Message_1458 Party_2 Riding_206 2004Message_1459 Party_2 Riding_189 2006Message_1460 Party_1 Riding_189 2006Message_1461 Party_2 Riding_189 2006Message_1462 Party_1 Riding_46 2006???
???
???
??
?Table 1.
A snapshot of the processed dataRiding name Party Candidate nameNDP Noreen JohnsBlackstrap Liberal J. Wayne ZimmerPC Lynne YelichTable 2.
An example of our Party-Candidatelisting for a riding (PC: Progressive Conserva-tive)1058ridings (electoral districts).
The website contains308 separate html files of messages correspondingto the 308 ridings for different years.
In total, wecollected 4858 and 4680 messages for the 2004and 2006 federal elections respectively.
On aver-age, a message consists of 98.8 words.To train and evaluate our system, we require agold standard for each message (i.e., which partydoes an author of a message predict to win?).
Oneoption is to hire human annotators to build the goldstandard.
Instead, we used an online party logoimage file that the author of each message alreadylabeled for the message.
Note that authors onlyselect parties they think will win, which means ourgold standard only contains a party with WIN va-lence of each message.
However, we leverage thisinformation to build a system which is able to de-termine a party even with LOSE valence.
We de-scribe this idea in detail in Section 4.Finally, we pre-processed the data by convertingthe downloaded html source files into a structuredformat with the following fields: message, party,riding, and year, where message is a text, party is awinning party predicted in the text, riding is one ofthe 308 ridings, and year is either 2004 or 2006.Table 1 shows a snapshot of the processed data setthat we used for our system training and evalua-tion.
An additional piece of information consistingof a candidate's name for each party for each ridingwas also stored in our data set.
With this informa-tion, the system can infer opinions about a partybased on opinions about candidates who run for theparty.
Table 2 shows an example of a riding.4 Analyzing PredictionsIn this section we describe Crystal.
One simpleapproach could be a system (see NGR system inSection 5) trained by a machine learning techniqueusing n-gram features and classifying a messageinto multiple classes (e.g., NDP, Liberal, or Pro-gressive).
However, we develop a more sophisti-cated algorithm and compare its result with severalbaselines, including the simple n-gram method2.Experimental results in Section 5 show that Crystaloutperforms all the baselines.Our approach consists of three steps: featuregeneralization, classification using SVMs, and2 N-gram approach is often unbeatable (and therefore great) inmany text classification tasks.SVM result integration3.
Crystal generates general-ized sentences in the feature generalization step.Then it classifies each sentence using generalizedlexical features in order to determine Valence ofParty in a sentence.
Finally, it combines results ofsentences to determine Valence and Party of amessage.
Note that the classification using SVM isan intermediate step conducting a binary classifica-tion (i.e., WIN or LOSE) for the final multi-classclassification in result integration.
The followingsections describe each step.4.1 Feature GeneralizationIn the feature generalization step, we generalizepatterns of words used in predictive opinions.
Forexample, instead of using three different trigramslike ?Liberals will win?, ?NDP will win?, and?Conservatives will win?, we generalize these to?PARTY will win?.
The assumption is that thegeneralized patterns can represent better the rela-tionship among Party, Valence, and words sur-rounding Party (e.g., will win) than pure lexicalpatterns.
For this algorithm, we first substitute acandidate's name (both the first name and the lastname) with the political party name that the candi-date belongs to (see Table 2).
We then break eachmessage into sentences4.Table 3 outlines the feature generalization algo-rithm.
Here, our approach is that if a message pre-3 ?feature?
indicates n-grams in our corpus that we use in theSVM classification step.4 The sentence breaker that we used is available athttp://search.cpan.org/ ~shlomoy/Lingua-EN-sentence -0.25/lib/Lingua/EN/Sentence.pm.1 for each message M with a party that M predicts to win, Pw2   for each sentence Si in a message M3      for each party Pj in Si4         valence Vj = +1 if Pj = Pw5         valence Vj = -1  Otherwise6         Generate S'ij by substituting Pj with  PARTY7         and all other parties in Si with OTHER8          Return (Pj, Vj, S'ij)Table 3.
Feature generalization algorithm1059dicts a particular party to win, sentences whichmention that party in the message also imply that itwill win.
Conversely all other parties are assumedto be in sentences that imply they will lose.
Asshown in Section 3.2, a message (M) in our corpushas a label of a party (Pw) that the author of M pre-dicts to win.
After breaking sentences in M, weduplicate a sentence by the number of unique par-ties in the sentence and modify the duplicated sen-tences by substituting the party names withPARTY and OTHER in order to generalize fea-tures.Consider the following sentence:?Dockrill will barely take this riding fromRodger Cuzner?which gets re-written as:?NDP will barely take this riding from Liberal?because Dockrill is an NDP candidate and RodgerCuzner is a Liberal candidate.
Since the sentencecontains two parties (i.e., NDP and Liberal), thealgorithm duplicates the sentence twice, once foreach party (see Lines 4?8 in Table 3)5.
For NDP,the algorithm determines its Valence as -1 becauseNDP is not equal to the predicted winning party(i.e., Liberal) of the message (see Lines 4?5 in Ta-5 In the feature generalization algorithm, we representWIN and LOSE valence as +1 and -1.ble 3).
Then it generates a generalized sentence bysubstituting NDP with PARTY and Liberal withOTHER (Lines 6?7).
It returns (NDP, -1, ?PARTYwill barely take this riding from OTHER?).
ForLiberal, on the other hand, the algorithm deter-mines its Valence as +1 since Liberal is the sameas the predicted winning party of the message.
Af-ter similar generalization, it returns (Liberal, +1,?OTHER will barely take this riding fromPARTY?
).Note that the final result of the feature generali-zation algorithm is a set of triplets: (Party, Va-lence, Generalized Sentence).
Among a triplet, weuse (Valence, Generalized Sentence) to producefeature vectors for a machine learning algorithm(see Section 4.2) and (Party, Valence) to integratesystem results of each sentence for the final deci-sion of Party and Valence of a message (see Sec-tion 4.3).
Figure 2 shows an example of the algo-rithm.4.2 Classification Using SVMsIn this step, we use Support Vector Machines(SVMs) to train our system using the generalizedfeatures described in Section 4.1.
After we ob-tained examples of (Valence, Generalized Sen-tence) in the feature generalization step, we mod-eled a subtask of classifying a Generalized Sen-tence into Valence towards our final goal of deter-mining (Valence, Party) of a message.
This subtaskis a binary classification since Valence has only 2classes: +1 and -16.
Given a generalized sentence?OTHER will barely take this riding fromPARTY?
in Figure 2, for example, the goal of oursystem is to learn WIN valence for PARTY.
Fea-tures for SVMs are extracted from generalized sen-tences.
We implemented our SVM learning modelusing the SVMlight package7.4.3 SVM Result IntegrationIn this step, we combine the valence of each sen-tence predicted by SVMs to determine the finalvalence and predicted party of a message.
For eachparty mentioned in a message, we calculate thesum of the party's valences of each sentence and6 However, the final evaluation of the system and all the base-lines is equally performed on the multi-classification results ofmessages.7 SVMlight is available from http://svmlight.joachims.org/Figure 2.
An example of feature generalizationof a message1060pick a party that has the maximum value.
This in-tegration algorithm can be represented as follows:?=mkkppValence0)(max argwhere p is one of parties mentioned in a message,m is the number of sentences that contains party pin a message, and Valencek(p) is the valence of p inthe kth sentence that contains p. Given the examplein Figure 2, the Liberal party appears twice in sen-tence S0 and S1 and its total valence score is +2,whereas the NDP party appears once in sentenceS1 and its valence sum is -1.
As a result, our algo-rithm picks liberal as the winning party that themessage predicts.5 Experiments and ResultsThis section reports our experimental results show-ing empirical evidence that Crystal outperformsseveral baseline systems.5.1 Experimental SetupOur corpus consists of 4858 and 4680 messagesfrom 2004 and 2006 Canadian federal election pre-diction data respectively described in detail in Sec-tion 3.2.
We split our pre-processed corpus into 10folds for cross-validation.
We implemented thefollowing five systems to compare with Crystal 8.?
NGR: In this algorithm, we train the system us-ing SVM with n-gram features without the gener-alization step described in Section 4.19.
The re-placement of each candidate's first and last nameby his or her party name was still applied.?
FRQ: This system picks the most frequentlymentioned party in a message as the predictedwinning party.
Party name substitution is also ap-plied.
For example, given a message ?This ridingwill go liberal.
Dockrill will barely take this ridingfrom Rodger Cuzner.
?, all candidates' names arereplaced by party names (i.e., ?This riding will goLiberal.
NDP will barely take this riding from Lib-eral.?).
After name replacement, the system picksLiberal as an answer because Liberal appears twicewhereas NDP appears only once.
Note that, unlikeCrystal, this system does not consider the valenceof each party (as done in our sentence duplication8 In our experiments using SVM, we used the linear kernel forall Crystal, NGR, and JDG.9 This system is exactly like Crystal without the feature gener-alization and result integration steps.step of the feature generalization algorithm).
In-stead, it blindly picks the party that appeared mostin a message.?
MJR: This system marks all messages with themost dominant predicted party in the entire dataset.
In our corpus, Conservatives was the majorityparty (3480 messages) followed closely by Liberal(3473 messages).?
INC: This system chooses the incumbent partyas the predicted winning party of a message.
(Thisis a strong baseline since incumbents often win inCanadian politics).
For example, since the incum-bent party of the riding ?Blackstrap?
in 2004 wasConservative, all the messages about Blackstrap in2004 were marked Conservative as their predictedwinning party by this system.?
JDG: This system uses judgment opinion wordsas its features for SVM.
For our list of judgmentopinion words, we use General Inquirer which is apublicly available list of 1635 positive and nega-tive sentiment words (e.g., love, hate, wise, dumb,etc.
)10.5.2 Experimental ResultsWe measure the system performance with its accu-racy in two different ways: accuracy per message(Accmessage) and accuracy per riding (Accriding).
Bothaccuracies are represented as follows:set test ain  messages of # Totallabledcorrectly  system  themessages of #=messageAccset test ain  ridings of # Totalpredictedcorrectly  system  theridings of #=ridingAccWe first report the results with Accmessage inEvaluation1 and then report with Accriding inEvaluation2.Evaluation1: Table 4 shows accuracies of base-lines and Crystal.
We calculated accuracy for eachtest set in 10-fold data sets and averaged it.
Amongthe baselines, MJR performed worst (36.48%).Both FRQ and INC performed around 50%(54.82% and 53.29% respectively).
NGR achievedits best score (62.02%) when using unigram, bi-gram, and trigram features together (uni+bi+tri).We also experimented with other feature combina-tions (see Table 5).
Our system achieved 73.07%which is 11% higher than NGR and around 20%10 Available at http://www.wjh.harvard.edu/~inquirer/homecat.htm1061higher than FRQ and INC.
The best accuracy ofour system was also obtained with the combinationof unigram, bigram, and trigram features.The JDG system, which uses positive and nega-tive sentiment word features, had 66.23% accu-racy.
This is about 7% lower than Crystal.
Sincethe lower performance of JDG might be related tothe number of features it uses, we also experi-mented with the reduced number of features ofCrystal based on the tfidf scores11.
With the samenumber of features (i.e., 1635), Crystal performed70.62% which is 4.4% higher than JDG.
An inter-esting finding was that NGR with 1635 featuresperformed only 54.60% which is significantly11 The total number of all features of Crystal is 689,642.lower than both systems.
This indicates that the1635 pure n-gram features are not as good as thesame number of sentiment words carefully chosenfrom a dictionary but the generalized features ofCrystal represent the predictive opinions betterthan JDG features.Table 5 illustrates the comparison of NGR(without feature generalization) and Crystal (withfeature generalization) in different feature combi-nations.
uni, bi, tri, and four correspond to uni-gram, bigram, trigram, and fourgram.
Our pro-posed technique Crystal performed always betterthan the pure n-gram system (NGR).
Both systemsperformed best (62.02% and 73.07%) with thecombination of unigram, bigram, and trigram(uni+bi+tri).
The second best scores (61.96% and73.01%) are achieved with the combinations of allgrams (uni+bi+tri+four) in both systems.
Usingfourgrams alone performed worst since the systemoverfitted to the training examples.Table 6 presents several examples of frequent n-gram features in both WIN and LOSE classes.
Asshown in Table 6, lexical patterns in the WIN classexpress optimistic sentiments about PARTY (e.g.,PARTY_will_win and go_ PARTY_again)whereas patterns in the LOSE class express pessi-mistic sentiments (e.g., PARTY_don't_have) andoptimistic ones about OTHER (e.g.,want_OTHER).Evaluation2: In this evaluation, we use Accridingcomputed as the number of ridings that a systemcorrectly predicted, divided by the total number ofridings.
For each riding R, systems pick a partythat obtains the majority prediction votes frommessages in R as the winning party of R. For ex-Patterns in WIN class Patterns in LOSE classPARTY_will_win want_OTHERPARTY_hold PARTY_don?t_havePARTY_will_win_this OTHER_andPARTY_win the_PARTYwill_go_PARTY OTHER_will_winPARTY_will_take OTHER_isPARTY_will_take_this to_the_OTHERPARTY_is and_OTHERsafest_PARTY results_OTHERPARTY_has OTHER_hasgo_PARTY_again to_OTHERTable 6.
Examples of frequent features inWIN and LOSE classes.system Accmessage (%) Accriding (%)FRQ 54.82 63.14MJR 36.48 36.63INC 53.29 78.03NGR (uni+bi+tri) 62.02 79.65JDG 66.23 78.68Crystal (uni+bi+tri) 73.07 81.68Table 4.
System performance with accuracyper message (Accmessage ) and accuracy perriding (Accriding): FRQ, MJR, INC, NGR,JDG, and Crystal.Accmessage (%) FeaturesNGR Crystaluni 60.49 72.03bi 58.79 71.81tri 54.04 69.57four 47.25 67.64uni + bi 61.54 72.93uni + tri 61.36 72.20uni + four 60.70 72.84bi + tri 58.68 72.26bi + four 58.54 72.17uni + bi + tri 62.02 73.07uni + bi + four 61.75 72.30uni + tri + four 61.34 72.30bi + tri + four 58.42 72.62uni + bi + tri + four 61.96 73.01Table 5.
System performance with differentfeatures: Pure n-gram (NGR) and General-ized n-gram Crystal.1062ample, if Crystal identified 9 messages predictingfor Conservative Party, 3 messages for NDP, and 1message for Liberal among 13 messages in the rid-ing ?Blackstrap?, the system will predict that theConservative Party would win in ?Blackstrap?.Table 4 shows the system performance with Ac-criding.
Note that people who write messages on aparticular web site are not a random sample forprediction.
So we introduce a measure of confi-dence (ConfidenceScore) of each system and usethe prediction results when the ConfidenceScore ishigher than a threshold.
Otherwise, we use a de-fault party (i.e., the incumbent party) as the win-ning party.
ConfidenceScore of a riding R is calcu-lated as follows:ConfidenceScore =  countmessage(Pfirst) ?
countmes-sage(Psecond)where countmessage(Px) is the number of messagesthat predict a party Px to win, Pfirst is the party thatthe most number of messages predict to win, andPsecond is the party that the second most number ofmessages predict to win.We used 62 ridings to tune the ConfidenceScoreparameter arriving at the value of 4.
As shown inTable 4, the system which just considers the in-cumbent party (INC) performed fairly well(78.03% accuracy) because incumbents are oftenre-elected in Canadian elections.
The upper boundof this prediction task is 88.85% accuracy which isthe prediction result using numerical values of aprediction survey.
FRQ and MJR performed63.14% and 36.63% respectively.
Similarly toEvaluation1, JDG which only uses judgment wordfeatures performed worse than both Crystal andNGR.
Also, Crystal with our feature generalizationalgorithm performed better than NGR with non-generalized n-gram features.
The accuracy of Crys-tal (81.68%) is comparable to the upper bound88.85%.6 DiscussionIn this section, we discuss possible extensions andimprovements of this work.Our experiment focuses on investigating aspectsof predictive opinions by learning lexical patternsand comparing them with judgment opinions.However, this work can be extended to investigat-ing how those two types of opinions are related toeach other and whether lexical features of one(e.g., judgment opinion) can help identify the other(e.g., predictive opinion).
Combining two types ofopinion features and testing on each domain canexamine this issue.In our experiment, we used General Inquirerwords as judgment opinion indicators for JDGbaseline system.
It might be interesting to employdifferent resources for judgment words such as thepolarity lexicon by Wilson et al (2005) and therecently released SentiWordNet12.Our work is an initial step towards analyzing anew type of opinion.
In the future, we plan to in-corporate more features such as priors like incum-bent party in addition to the lexical features to im-prove the system performance.7 ConclusionsIn this paper, we proposed a framework for work-ing with predictive opinion.
Previously, research-ers in opinion analysis mostly focused on judgmentopinions which express positive or negative senti-ment about a topic, as in product reviews and pol-icy discussions.
Unlike judgment opinions, predic-tive opinions express a person's opinion about thefuture of a topic or event such as the housing mar-ket, a popular sports match, and election results,based on his or her belief and knowledge.
Amongthese many kinds of predictive opinions, we fo-cused on election prediction.We collected past election prediction data froman election prediction project site and automati-cally built a gold standard.
Using this data, wemodeled the election prediction task using a super-vised learning approach, SVM.
We proposed anovel technique which generalized n-gram featurepatterns.
Experimental results showed that this ap-proach outperforms several baselines as well as anon-generalized n-gram approach.
This is signifi-cant because an n-gram model without generaliza-tion is often extremely competitive in many textclassification tasks.This work adopts NLP techniques for predictiveopinions and it sets the foundation for exploring awhole new subclass of the opinion analysis prob-lems.
Potential applications of this work are sys-tems that analyze various kinds of election predic-tions by monitoring texts in discussion boards andpersonal blogs.
In the future, we would like to12 http://sentiwordnet.isti.cnr.it/1063model predictive opinions in other domains such asthe real estate market and the stock market whichwould require further exploration of system designand data collection.ReferenceEngelmore, R., and Morgan, A. eds.
1986.
BlackboardSystems.
Reading, Mass.
: Addison-Wesley.Dave, K., Lawrence, S. and Pennock, D. M.  2003.
Min-ing the peanut gallery: Opinion extraction and se-mantic classification of product reviews.
Proc.
ofWorld Wide Web Conference 2003Drucker, H., Wu, D. and Vapnik, V. 1999.
Support vec-tor machines for spam categorization.
IEEE Trans.Neural Netw., 10, pp 1048?1054.Gliozzo, A., Strapparava C. and Dagan, I.
2005.
Investi-gating Unsupervised Learning for Text Categoriza-tion Bootstrapping, Proc.
of EMNLP 2005.
Vancou-ver, B.C., CanadaHu, M. and Liu, B.
2004.
Mining and summarizing cus-tomer reviews.
Proc.
Of KDD-2004, Seattle, Wash-ington, USA.Jindal, N. and Liu, B.
2006.
Mining Comprative Sen-tences and Relations.
Proc.
of 21st National Confer-ence on Artificial Intellgience (AAAI-2006).
2006.Boston, Massachusetts, USAJoachims, T. 1998.
Text categorization with supportvector machines: Learning with many relevant fea-tures, Proc.
of ECML, p. 137?142.Kim, S-M. and Hovy, E. 2004.
Determining the Senti-ment of Opinions.
Proc.
of COLING 2004.Liu, B., Li, X., Lee, W. S. and Yu, P. S. Text Classifica-tion by Labeling Words Proc.
of AAAI-2004, SanJose, USA.Pang, B, Lee, L. and Vaithyanathan, S. 2002.
Thumbsup?
Sentiment Classification using Machine LearningTechniques.
Proc.
of EMNLP 2002.Popescu, A-M. and Etzioni, O.
2005.
Extracting ProductFeatures and Opinions from Reviews, Proc.
of HLT-EMNLP 2005.Rickel, J. and Porter, B.
1997.
Automated Modeling ofComplex Systems to Answer Prediction Questions,Artificial Intelligence Journal, volume 93, numbers1-2, pp.
201?260Riloff, E., Wiebe, J., and Phillips, W. 2005.
ExploitingSubjectivity Classification to Improve InformationExtraction, Proc.
of the 20th National Conference onArtificial Intelligence (AAAI-05) .Riloff, E., Wiebe, J. and Wilson, T. 2003.
Learning Sub-jective Nouns Using Extraction Pattern Bootstrap-ping.
Proc.
of CoNLL 2003. pp 25?32.Rodionov, S. and Martin, J. H. 1996.
A Knowledge-Based System for the Diagnosis and Prediction ofShort-Term Climatic Changes in the North Atlantic,Journal of Climate, 9(8)Turney, P. 2002.
Thumbs Up or Thumbs Down?
Se-mantic Orientation Applied to Unsupervised Classifi-cation of Reviews.
Proc.
of ACL 2002, pp 417?424.Wiebe, J., Bruce, R. and O?Hara, T. 1999.
Developmentand use of a gold standard data set for subjectivityclassifications.
Proc.
of ACL 1999, pp 246?253.Wiebe, J., Wilson, T. , Bruce, R. , Bell , M. and Martin,M.
Learning Subjective Language.
2004.
Computa-tional LinguisticsWilson, T., Wiebe, J. and Hoffmann, P. 2005.
Recog-nizing Contextual Polarity in Phrase-Level SentimentAnalysis.
Proc.
of HLT/EMNLP 2005.Yu, H. and Hatzivassiloglou, V. 2003.
Towards An-swering Opinion Questions: Separating Facts fromOpinions and Identifying the Polarity of OpinionSentences.
Proc.
of EMNLP 2003.1064
