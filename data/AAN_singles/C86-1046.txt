DEPENDENCY UNIFICATION GRAMMARPeter HellwigUniversity of Heidelberg, D-6900 Heidelberg, West GermanyAbstractThis paper describes the analysis component of thelanguage processing system PLAIN from the viewpointof unification grammars.
The principles of DependencyUnification Grammar (DUG) are discussed.
The computerlanguage DRL (Dependency Representation Language) isintroduced J.n which DUGs can be formulated.
A unifi-cation-based parsing procedure is part of the formal-ism.
PLAIN is implemented at the universities of Hei-delberg, Bonn, Flensburg, Kiel, Zurich and CambridgeU.K.1.
IntroductionThe recent development of grammar theory e?hibitsconvergencies among various approaches, such as Gov-ernment-Binding Theory, Generalized Phrase StructureGrammar, Definite Clause Grammar, Lexical FunctionalGrammar, Functional Unification G~ammar, and others.To varying degrees these theories share the followingprinciples:(i) They take into account dependency rela-tions, using notions such as "head" or "governor".Phenomena such as long distance dependencies areviewed as tou?
:hstones for the formalisms.
(ii) They pay attention to functional aspects.The representation of syntactic roles is seen to be atask.
(iii) They agree that syntax must be lexicallyrestricted and thus place a large portion of thegrammatical information in the lexicon.
(iv) They base their algorithms on the princi-ple of unification, i.e.
complex categories arebrought into agreement in the syntactic context.These common features make it possible to com-pare the solutions of the different formalisms aswell as their problems.The main difficulty for the computer applicationof unification grammars lies in their complexity.LFG, for example, bases its syntactic c-structures onthe phrase structure principle, while the functionalf-structures represent dependency relationshipsbetween functors and a~guments.
This causes problemsfor the parser which needs information on bothstructures while it is creating them.
The developmentof GPSG seems to be marked by the effort to introducemore and more types of rules so as to adequatelyconstrain the formalism.
As a result, the control ofanalysis is distributed over many resources and is,therefore, increasingly difficult.
Since a largenumber of constraints are of a lexical nature, thelexicon becomes more and more unwieldy in all of theformalisms.Common advantages and common problems of unifi-cation grammars suggest examining strategies fromstill other frameworks.
This is to be done here withrespect to dependency grammar.
DUG rearranges theavailable means of description.
As a result, the be-nefits of the common principles are fully felt where-as the difficulties mentioned are largely avoided.2.
Dependency Representat ion Language (DRL)Grammar formalisms and computer languages are usuallydeveloped independently.
DRL is both at the sametime.
In the same spirit as PROLOG is tailor-made forthe purposes of logic, DRL has been particularlyadapted to represent linguistic structures.
Whereasthe interpreter for PROLOG includes a theorem prover,the interpreter for DRL is linked with a parser.
(DRLalso serves for the purpose of knowledge representa-tion within the deduction component of PLAIN.
Thisaspect will not be discussed here.
)DRL consists of bracketed expressions which arelists in the sense of list processing.
Conceptually,they represent tree diagrams with nodes and directedarcs.
It is the characteristic feature of DRL thateach node refers to a lexically defined atomic unitof an utterance and that the arcs represent directrelationships between these atomic units.
Accordingto the hierarchical structure of tree diagrams, oneelement in each relationship is dominant, the otherone is dependent.
Dependency grammar assumes thatthis asymmetry reflects the actual situation in natu-ral language.Asymmetries between constituents are commonlyconceded in modern grammar theory.
It seems to becertain that only via the head-complement distinctioncan adequate constraints for the construction of nat-ural language expressions be defined.
Unfortunately,phrase structure, which prevails in most grammar for-malisms, is at odds with the direct asymmetric rela-tions between immediate constituents.
A logical con-sequence would be to chose dependency as the primaryprinciple of representing syntactic structure (seethe arguments ill Hudson 1984).
Nevertheless, thisproposal still encounter:!
{ quite a bit of scepticism.The implementation of an efficient parser (see Hell-wig 1980) has proven the practicability of the depen-dency approach.
However, the formalism for dependencygrammars has had to be substantially augmented.3.
Faetorization of Grammatical InformationWhen designing a computer language that is to serveas a grammatical formalism, it is crucial to providefor a factorization of information that is at thesame time convenient and adequate.
I have stressedthat DRL terms are in a one-to-one relationship withthe basic elements of a natural language.
Since thefeatures of these elements are numerous and varied,every DRL term must be multi-labeled.
As is common inunification grammars, each feature is coded as anattribute-value pair.
The attribute states the fea-ture type, the values represent the concrete fea-tures.
The division into attributes and values allowsfor very general descriptions, since relationshipscan now be formulated on the level of the attributes,no matter which values apply in the individual cases.A complex category consist of any number of attri-butes or" attribute-value assignments.Faced with the unlimited expressiveness of com-plex categories, the key issue now is to carefullyselect and group the attributes in such a way thatthe linguistic phenomena are represented as ade-quately and transparently as possible.
DUG assumesthat a distinction must be made among three dimen-sions in which each element of an utterance partici-pates: lexical meaning, syntagmatic function and out-ward form.
Correspondingly, three types of attributesare grouped together in each DRL-term: a lexeme, asyntagmatic role and a complex morpho-syntactic ca-tegory.
To glve an example:195(i) The cat likes fish.This sentence is represented in DRL as follows, dis-regarding positional attributes for the moment:(2 )  (ILLOCUTION: assertion: clse typ<l>(PREDICATE: like: verb fin<l> hum<l> per<l>(SUBJECT: cat: noun num<l> per<3>(DETERMINER: the: dete))(OBJECT: fish: noun)));We cannot avoid going into a few notational details.Each term, printed on a separate line, corresponds toa word in (1).
The first term is correlated to theperiod, which is also treated as a word.
The paren-theses depict the dependency structure.
The first at-tribute in each term is the role, the second the lex-eme.
Both are identified by position, i.e.
their val-ues are simply written at the first and second po-sition in the term.
Roles and lexemes constitute thesemantic representation.
They are more or less equiv-alent to f-structures in LFG.
The third part of eachterm contains a description of the surface propertiesof the corresponding segments in the utterance.
Itconsists of a main category, generally a word classsuch as verb, noun, determiner, followed by a se-quence of attribute-value subcategories which repre-sent grammatical features such as finiteness, number,person.
The format of subcategories is standardizedin order to facilitate processing.
Attributes aresymbolized by three character-long key words, valuesare coded as numbers in angled brackets.The salient point of this formalism is that thefunctional, the lexematic and the morpho-syntacticproperties coincide in every term, as they do in theelements of natural language.
To put it in the termi-nology of LFG: f-structure and c-structure are to-tally synchronized.
Since this cannot be achieved ina phrase structure representation, it is often as ~sumed that there is a fundamental divergence betweenform and function in natural language.
Admittedly,one prerequisite for a uniform function-form corre-spondence still has to be mentioned.
Since non-termi-nal constituents are not basic, they are usually notrepresented by terms in DRL.
However, there must besomething to denote the suprasegmental meaning that aclause conveys in addition to the semantics of itsconstituents.
As a necessary extension of dependencygrammar, the yield of a clause is - so to speak -lexicalized in DUG and represented by a term thatdominates the corresponding list.
Compare the firstterm in (2).
Punctuation ill written language can beinterpreted as a similar lexicalization of clausalsemantics.4.
Positional FeaturesAn important augmentation of dependency grammar isthe decision to treat positional phenomena in DUG asmorpho-syntactic features and, as a consequence, torepresent them by subcategories in the same way asnumber, person and gender.
The mechanism of unifica-tion can be applied to word order attributes just asadvantageously as to other categories.
The only dif-ference is that the values appertaining to the ele-ments of an utterance are not taken from the lexicon,but are drawn from the situation in the input string.One has to visualize this as follows.Each term in a dependency representation corre-sponds to a segment of the input string.
Each subtreealso corresponds to a segment which is composed ofthe segments corresponding to the terms which formthe tree.
Breaking down a dependency tree into196subtrees thus imposes an implicit constituent struc-ture on the input string.
Incidentally, the constit-uent corresponding to a dependency tree does not needto be continuous.
The positions of the constituentsrelative to each other can be determined and includedas the values of positional attributes in the termsof the dependency trees.
It is stipulated that a po-sitional attribute refers to the implicit constituentcorresponding to the subtree in whose dominating termthe feature is specified.
Tlle attribute expresses asequential relationship between this constituent andthe segment which corresponds to the superordinatedterm.Any sequential order of constituents which canbe defined can be included in the set of attributes.Suppose, for example, that D is a string correspond-ing to a subtree and H is the string that correspondsto the term superordinated to that subtree.
Let usdefine the attribute "sequence" (seq) as having thevalues i: C precedes H, and 2: C follows H. Let usestablish "adjacency" (adj) with the values i: C im-mediately precedes 1{, and 2: C immediately follows H,Finally, let us introduce "delimitation" (lim) withthe values i: C is the leftmost of all of the stringscorresponding to dependents of H, and 2: C is therightmost of of all of the dependents of H. For thesake of comparison, let us consider the following ex-ample which Pereira 1981 uses in order to illustrateExtraposition Grammar:(3) The mouse that the cat that likes fish chasedsqueaks,The following DRL-tree depicts the dependencies andthe word order of this sentence by means of the at-tributes just defined:(4) (ILLOCUTION: assertion: adj<l>(PREDICATE: squeak: adj<l>(SUBJECT: mouse: adj<l>(DETERMINER: the: seq<l>)(ATTRIBUTE: chase: adj<2>(OBJECT: that: lim<l>)(SUBJECT: cat: adj<l>(DETERMINER: the: adj<l>)(ATTRIBUTE: like: adj<2>(SUBJECT: that: lim<l>)(OBJECT: fish: adj<2>)))))));The projection of subtrees and their attributesyields the following constituent analysis of the in-put string:(5) the mouse that the cat that \[ e squeakslikes fish chased \[ adj<l>the \[ , mouse - - ~  \[ that the catI seq<l> adj<2> \] that likes\[ \] fish chasedthat \[ +-- - -  chased\[ lim<l>tile cat that likes fish \[ <I adj<l>the I *- cat - - ~  \[ that likes fishI adj<l> adj<2> Ithat \] ~ - - - l i k e s - - ~  \] fishI lim<l> adj<2> IThere is exactly one sequence of words that is inagreement with all of the attribute-values in thet ree .
It is likely that appropriate attributes canalso be defined for more difficult cases of extrapo-sition.
Since the dislocated elements continue to besubordinated to their heads in their original role,no "gaps", "holes" or 'Ltraces" are part of tile DRL-formalism.
The possibility to do without such enti-ties is attractive.
It arises from the fact that theratio of constituency and dependency is reversed inDUG.
It seems to be easier to augment dependencytrees by c'onstituency information than to process de-pendency features within phrase markers.5.
Morpho-syntact ie Descr ipt ionWithin DRL terms, the following means exist for gen-eralization.
There are variables for roles, lexemesand morpho-syntactic main categories, Subcategoriesallow a disjunction of values as their specification.The ANY-value is assumed whenever a subcategory at-tribute is \]eft out completely.
These means are ap-plied in the so-called base lexicon.
'The base lexiconcreates the \].in\]{ between the segments of ti~e inputlanguage and the terms of DRL.
A few results of thisassignment are to be given just to illustrate theformat :(6) CAT " >CAT S - >LIKE ->LIKES ->LIKE ->FISH -->( * :  ca t :  noun num<\]> per<B>) ;( * :  ca t :  noun hum<2> per<B>) ;( * :  l i ke :  verb  per<\ ]  ,2>) ;(*: l.Jke: verb  num<l> per<3>);( * :  l i ke :  verb  num<2> per<B>) ;( * :  f i sh :  noun per<3>) ;The roles of all lexical items are \].eft open.
Theirvalues are a matter of the syntactic frames Jn whichthe items occurs J.n an utterance.
The same lexeme ap-plies to all inflectional, forms of a word.
The valuesof person and number of CAT and CATS are indicatedbecause they are specific.
FISH, on the other hand,can be both singular and plural.
Hence the number-attribute is omitted altogether.
'?he feattu'es firstand second person of LIKE are combined by dJsjunc..tion.
The choice between both values as well as be-tween the ANY--values of number is left to the con-text.
Ill case of the third person items it cannot beavoided to be more specific.6.
SlotsThe notion of dependency i s  closely related to theidea of intrinsic combination capabilities of tilelexical elements.
This capability is traditionallyreferred to as valency, although this view has oftenbeen restricted to verbs.
DUG generalizes this lex-icalistic approach with respect to all syntagmaticrelationships.
Syntax is completely integrated in thelexicon.
The natural way to state valencies is by as-signing slots to possibly dominating terms.
A slot isa template of the list that would be an appropriatecomplement.
As a rule, only the }lead of this list hasto be described, because head-feature-convention (asknown from GPSG) is a general principle in dependencyrepresentation.
The fo\].lowing is a description of thevalency of LIKES:(7) (*: like: verb fin<l> num<l> per<3>(SUBJECT: .. : noun num<l> per<3> adj<l>)(OBJECT: ._ : noun seg<2>));Slots are the places where roles are introducedinto the formalism.
As a matter of fact:, it is thetask of roles to differentiate conlplements.
The lex-ematic character of the complements is usually unre-stricted and, therefore, represented by a variable.Morpho-syntactics categories express the formal re-quirements, including positional attributes, that thefiller must meet.A direct assignment of slots to a specific \].ex-ical item is good policy only in the case of idiosyn-,cratic complements.
Complements such as subject andobject that are shared by many other verbs should bedescribed in a more general way.
The solution is todraw up completion patterns once and to refer tothose patterns from the various J.ndividual lexemes.
Aseparate pattern should be set up for each syntag-matic relationship.
For example:(8) (*: ~subjeet: verb fin<l>(SUBJECT; : noun num<C> per<C> adj<\].>));(9) (* :  +object( OBJECT : _ : noun seq<2>)) ;The following entries in the valency lexicon illus-trate references to these patterns:(\].0) (: -> (* :  squeak) ( :  +sub ject ) ) ;( l \] .)
(: -> (~: l i ke )  (& ( : - I sub jec t )(: +ob jec t ) ) ) ;In the case of LIKES the effect of (11) is identicalt:o ('~).Certain provisions allow for a maximal general-ity of patterns.
The symbol "C" as subcategory valuein (8) indicates that the respective values of a po-tential filler and the head of the \]Jst must matchwhatever these values may be irl tile concrete case,.
}fence, pattern (8) covers subjects with any numberand person features and, at the same time, controlstheir agreement with the dominating predicate.
Hor-phological features in the head term restrict tile ap-plJcabi\].ity of the pattern.
In the case of (8) thedominating verb must be finite (fin<\]>), because Jtcannot have a subject as complement in the Jnfirli-tJve.
The object pattern, on the contrary, is appli-cable without restrJctJons.An analogy to feature disjunction on the para-digmatic level is slot disjunction on the syntagmaticlevel.
It is the means t:o formalize syntact:ic alter-natives.
The following improved patterns for subjectsand objects include slots for relative pronouns intheir appropriate leftmost position:(12) (*: +subject: verb ~in<l> per<3>(, (SUBJECT: _. : pron rel<l,C> lim<l>)(SUBJECT:_  : noun num<C> per<C> adj<l>)));(13) (~: +object(, (OBJECT: _ : pron rel<l,C> lira<l>)(OB JECT:  : noun seq<2>)));(\]_2) provides for "that \].ikes fish" and (13) for"thatthe (:at chased" in Pereira's example.
The feature"re\].<l>", which is intrinsic to the relative pronoun,is to be passed on to the dominating verb as is Jndi-c, ated by "C".
This is the prerequisite to identifyingthe verb as the \]lead of' a relative clause.
The pat-tern for the relative clause could look like this:(14) (*: +relative clause: noun(ATTRIBUTE.- _ ; verb rel<l> fin<l> adj<2>))The fell.owing patterns and references complete tilesmall grammar that is needed for Pereira's sentence.-197(15 (~: +determiner: noun(DETERMINER: _ : dete seq<l>));(16) (: -> (~: mouse) (& (: +determiner)(: +relative clause)));(17) (: -> (*: cat) (& (: +determiner)(: +relative clause)));(18) (ILLOCUTION: assertion: clse typ<l>(PREDICATE: _ : verb fin<l> adj<l>));Completion patterns capture the same syntacticregularities as rules in other formalisms.
The pecu-liarity of DUG is that it breaks down the complexsyntax of a language into many atomic syntactic rela-tionships.
This has several advantages.
Valency de-scriptions are relatively easy to draw up.
They areto a great extent independent of each other so thatchanges and additions normally have no side effects.Although the grammar is wholly integrated in the lex-icon, the structure of lexical entries is rather sim-ple.
Any new combination of complements which may beencountered is simply a matter of lexical reference,while in rule-based grammars a new rule has to becreated whose application subsequently has to be con-trolled.7.
Parsin~ by UnificationIn  log ic ,  un i f i ca t ion  i s  de f ined  as  a coherent  re -placement of symbols within two formulas so that bothformulas become identical.
The same principle can beapplied advantageously in grammar.
The basis of themechanism is the notion of subsumption.
There are twooccurrences of subsumption in DRL.
Firstly, attributesymbols subsume all of the appertaining values.
Forexample, a role variable covers any role, a morpho-syntactic subcategory covers any element of the de-fined set of features.
Secondly, structure descrip-tions subsume structures.
DRL comprises variableswhich refer to various substructures of trees.
In thepresent context we consider only direct subordinationof slots.It must be the strategy of the grammar writer tokeep any single description as abstract as possibleso that it covers a maximum number of cases.
In thecourse of the analysis, the unification of ex-pressions leads to the replacement of the more gener-al by the more specific.
As opposed to simple patternmatching techniques, replacements of the symbols oftwo expressions occur in both directions.
Continuedunification in the syntagmatic framework leads to anincremental precision of the attributes of all of theconstituents.A prerequisite for a unification-based parser isthe control of the expressions which are to beunified.
The control structure depends on the grammartheory which is at the basis.
The PLAIN parser runsthrough three phases: (i) the consultation of thebase lexicon yielding a lexeme and a morpho-syntacticcharacterization for each basic segment in the ut-terance, (ii) the consultation of the valency lexiconyielding a description of the combination capabili-ties of the basic terms, (iii) a reconstruction ofthe syntactic relationships in the utterance by abottom-up slot-filling mechanism.
Throughout thewhole process previous expressions are unified withsubsequent ones.Let us first consider the lexicon phases.
Theword forms in the utterance are taken as the startingpoints.
According to the base lexicon, they are re-placed by terms which show the identity and the di-vergence of their attributes.
With respect to iden-tity, this is a step similar to unification.
Compare,for example, the terms associated with the word forms198of "to like" in (6), which share the role, the lexemeand the word class properties, with respect to diver-gence, the base lexicon contains just as many fea-tures as can be attributed to a word form out of thesyntactic context.
The valency lexicon, on the otherhand, abstracts just from those features which arenot distinctive for a specific syntactic relation-ship.The parser combines the information from bothlexica by means of unification.
At first, the termsderived from the base lexicon are unified with theleft-hand side of the valency references.
The result-ing specification is transferred to all terms on theright-hand side of the reference.
Each of theseterms, in turn, is unified with the heads of the com ~pletion patterns.
The specifications produced in thecourse of these operations are brought into agreementwith the original terms and, eventually, the appro-priate slots are subordinated to these terms.Once the initial lists are produced comprisingthe combined information from both lexica, the detec-tion of the syntactic structure of the utterance is afairly simple process.
Each of the lists, startingwith the leftmost one, tries to find a slot inanother list.
If a searching list can be unified withthe attributes in a slot, a new list is formed whichcomprises both lists as well as the result of theirmutual specifications.
The new list is stored at theend of the line and, when it is its turn, looks for aslot itself.
This process continues until no morelists are produced and no slots are untried.
Thoselists that comprise exactly one term for each inputsegment are the final parsing results.I would like to stress a few properties thatthis parsing algorithm owes to DUG.
Similar to unifi-cation in theorem proving, the process relies com-pletely on the unification of potential represen-tations of parts of the utterance.
No reference toexternal resources, such as  rules, taint the mecha-nism.
The control is thus extremely data- directed.On the other hand, the unification of DRL lists is aninstrument with an immense combinatorial power.within any term the agreement of function, lexicalselection and morpho-syntactic features is forced.
Inaddition to this horizontal linkage, the attributesof the dominating term as well as the attributes ofthe dependent terms are also subject to unification.The attributes of dependent terms are delineated bythe valency description.
According to congruence con-ditions heads and dependents continue to mutuallyspecify each other.
Feature unification and slot dis-junction also restricts the co-occurrence of depen-dents.
In addition, positional features are contin-uously made to tally with the corresponding sequenceof segments in the input string.
This network of re-lationships prevents the parser from producing inap-propriate lists.
At the same time it results in in-cremental specification, which facilitates the workof the lexicon writer.
What may be theoretically themost interesting is the fact that functional, lexi-cal, morphological and positional features can beprocessed smoothly in parallel.ReferencesHellwig, P.: "PLAIN - A Program System for DependencyAnalysis and for Simulating Natural LanguageInference."
In L. Bolc (ed.
): Representation andProcessing of Natural Language.
London:Macmillan 1980.
271-376.Hudson, R.: Word Grammar.
Oxford: Blackwell 1984.Pereira, F.: "Extraposition Grammars."
AmericanJournal of Computational Linguistics 7 (1981).243-256.
