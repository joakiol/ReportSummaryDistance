Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 127?132,Sydney, July 2006. c?2006 Association for Computational LinguisticsUsing LTAG-Based Features for Semantic Role LabelingYudong Liu and Anoop SarkarComputing Science DepartmentSimon Fraser UniversityBritish Columbia, Canada, V5A 1S6yudongl,anoop@cs.sfu.caAbstractSemantic role labeling (SRL) methodstypically use features from syntactic parsetrees.
We propose a novel method thatuses Lexicalized Tree-Adjoining Gram-mar (LTAG) based features for this task.We convert parse trees into LTAG deriva-tion trees where the semantic roles aretreated as hidden information learned bysupervised learning on annotated data de-rived from PropBank.
We extracted var-ious features from the LTAG derivationtrees and trained a discriminative decisionlist model to predict semantic roles.
Wepresent our results on the full CoNLL 2005SRL task.1 IntroductionSemantic role labeling (SRL) is a natural exten-sion of the syntactic parsing task.
In SRL, par-ticular syntactic constituents in a parse tree for asentence are identified with semantic roles.
Thelabels assigned to various types of arguments andadjuncts differ in different annotation schemes.In this paper, we use the PropBank corpus ofpredicate-argument structures (Palmer, Gildea andKingsbury, 2005).
We assume we are given a syn-tactic parse tree and a particular predicate in thesentence for which we then identify the argumentsand adjuncts and their labels.
In this paper wecompare two models for the identification of se-mantic role labels in a parse tree: A model thatuses a path in the parse tree (or the derived tree inTAG terminology) and various associated featuresrelated to this, and we compare this model with amodel that converts the syntactic parse tree intoa Lexicalized Tree-Adjoining Grammar (LTAG)derivation tree and uses features extracted from theelementary trees and the LTAG derivation tree.In each model the features of that model areused in a discriminative model for semantic rolelabeling.
The model is a simple decision listlearner that uses tree patterns extracted from theLTAG derivation trees in order to classify con-stituents into their semantic roles.
We present re-sults on the full CoNLL 2005 SRL task (Carrerasand Ma`rquez, 2005) a dataset built by combiningthe Treebank and parser data with the PropBankannotations.2 Background about SRLA semantic role is defined to be the relationshipthat a syntactic constituent has with the predicate.For example, the following sentence, taken fromthe PropBank corpus, shows the annotation of se-mantic roles:[A0 Late buying] [V gave] [A2 the ParisBourse] [A1 a parachute] [AM-TMP after itsfree fall early in the day].Here, the arguments for the predicate gave aredefined in the PropBank Frame Scheme (Palmer,Gildea and Kingsbury, 2005) as:V: verb A2: beneficiaryA0: giver AM-TMP: temporalA1: thing givenRecognizing and labeling semantic argu-ments is a key task for answering ?Who?,?When?,?What?, ?Where?, ?Why?, etc.
questionsin Information Extraction, Question Answering,Summarization (Melli et al 2005), and, in general,in all NLP tasks in which some kind of semanticinterpretation is needed.Most previous research treats the semantic rolelabeling task as a classification problem, and di-vides it into two phases: argument identificationand argument classification.
Argument identifi-cation involves classifying each syntactic elementin a sentence into either an argument or a non-argument.
Argument classification involves clas-sifying each argument identified into a specific se-mantic role.
A variety of machine learning meth-ods have been applied to this task.
One of the mostimportant steps in building an accurate classifier isfeature selection.
Different from the widely used127feature functions that are based on the syntacticparse tree (Gildea and Jurafsky, 2002), we explorethe use of LTAG-based features in a simple dis-criminative decision-list learner.3 LTAG Based Feature ExtractionIn this section, we introduce the main componentsof our system.
First, we do a pruning on the givenparse trees with certain constraints.
Then we de-compose the pruned parse trees into a set of LTAGelementary trees.
For each constituent in question,we extract features from its corresponding deriva-tion tree.
We train using these features in a deci-sion list model.3.1 Pruning the Parse TreesGiven a parse tree, the pruning component identi-fies the predicate in the tree and then only admitsthose nodes that are sisters to the path from thepredicate to the root.
It is commonly used in theSRL community (cf.
(Xue and Palmer, 2004)) andour experiments show that 91% of the SRL targetscan be recovered despite this aggressive pruning.There are two advantages to this pruning: the ma-chine learning method used for prediction of SRLsis not overwhelmed with a large number of non-SRL nodes; and the process is far more efficientas 80% of the target nodes in a full parse tree arepruned away in this step.
We make two enhance-ments to the pruned Propbank tree: we enrich thesister nodes with their head information, which isa part-of-speech tag and word pair: ?t, w?
and PPnodes are expanded to include the NP complementof the PP (including the head information).
Notethat the target SRL node is still the PP.
Figure 1shows the pruned parse tree for a sentence fromPropBank section 24.3.2 LTAG-based DecompositionAs next step, we decompose the pruned treearound the predicate using standard head-percolation based heuristic rules1 to convert aTreebank tree into a LTAG derivation tree.
Wedo not use any sophistical adjunct/argument orother extraction heuristics using empty elements(as we don?t have access to them in the CoNLL2005 data).
Also, we do not use any substitutionnodes in our elementary trees: instead we exclu-sively use adjunction or sister adjunction for theattachment of sub-derivations.
As a result the1using http://www.isi.edu/?chiang/software/treep/treep.htmlroot node in an LTAG derivation tree is a spinalelementary tree and the derivation tree providesthe path from the predicate to the constituent inquestion.
Figure 2 shows the resulting elementarytree after decomposition of the pruned tree.
Foreach of the elementary trees we consider theirlabeling in the derivation tree to be their semanticrole labels from the training data.
Figure 3 is thederivation tree for the entire pruned tree.Note that the LTAG-based decomposition of theparse tree allows us to use features that are distinctfrom the usual parse tree path features used forSRL.
For example, the typical parse tree featurefrom Figure 2 used to identify constituent (NP (NNterminal)) as A0 would be the parse tree fragment:NP ?
NP ?
SBAR ?
S ?
V P ?
S ?
V P ?V BG cover (the arrows signify the path throughthe parse tree).
Using the LTAG-based decompo-sition means that our SRL model can use any fea-tures from the derivation tree such as in Figure 2,including the elementary tree shapes.3.3 Decision List Model for SRLBefore we train or test our model, we convertthe training, development and test data into LTAGderivation trees as described in the previous sec-tion.
In our model we make an independence as-sumption that each semantic role is assigned toeach constituent independently, conditional onlyon the path from the predicate elementary treeto the constituent elementary tree in the deriva-tion tree.
Different elementary tree siblings in theLTAG derivation tree do not influence each otherin our current models.
Figure 4 shows the differ-ent derivation trees for the target constituent (NP(NN terminal)): each providing a distinct semanticrole labeling for a particular constituent.
We usea decision list learner for identifying SRLs basedon LTAG-based features.
In this model, LTAG el-ementary trees are combined with some distanceinformation as features to do the semantic role la-beling.
The rationale for using a simple DL learneris given in (Gildea and Jurafsky, 2002) where es-sentially it based on their experience with the set-ting of backoff weights for smoothing, it is statedthat the most specific single feature matching thetraining data is enough to predict the SRL on testdata.
For simplicity, we only consider one inter-mediate elementary tree (if any) at one time in-stead of multiple intermediate trees along the pathfrom the predicate to the argument.128SNPPRP-HHeVP-HVBZ-HbackflipsPPIN-HintoNPNP-HNN-Hterminal,,SBARWHNP-HWDT-HwhichSVP-HVBZ-Hexplodes,,SVP-HVBG-HcoveringNPNN-HfacePPIN-HwithNPNNS-HmicrochipsFigure 1: The pruned tree for the sentence ?He backflips into a desktop computer terminal, which ex-plodes, covering Huntz Hall ?s face with microchips.
?SVP-HVBG-HcoverNPNN-HfacePPIN-HwithNPNNS-HmicrochipsSVP-HVBZ-Hexplodes,,SBARWHNP-HWDT-Hwhichpredicate: A1: A2: NULL: NULL: R-A0:NPNP-HNN-HterminalPPIN-HintoSVP-HVBZ-HbackflipsNPPRP-HHeA0: NULL: NULL: NULL:Figure 2: The resulting elementary trees after decomposition of the pruned tree.129S(backflips)NP(he) PP(into)NP(terminal),(,) SBAR(which)S(explodes),(,) S(cover)NP(face) PP(with)Figure 3: The LTAG derivation tree (with no semantic role labels) corresponding to the pruned tree.A0: NP-NP(NN,terminal)R-A0: SBAR-WHNP(WDT,which)NULL: S-VP(VBZ,explodes)predicate: S-VPH(VBG,cover)A1: NP-NP(NN,terminal)R-A0: SBAR-WHNP(WDT,which)NULL: S-VP(VBZ,explodes)predicate: S-VPH(VBG,cover)A0: NP-NP(NN,terminal)R-A0: SBAR-WHNP(WDT,which)AM-ADV: S-VP(VBZ,explodes)predicate: S-VPH(VBG,cover)Figure 4: Different LTAG derivation trees corresponding to different assignments of semantic roles toconstituents.
The constituent in question is (NP (NN terminal)).NPNPNNterminalSBARSVPSVPVBGcoverVPVBGcoverPPINwithNPNNSmicrochipsSBARWHNPWDTwhichSVPSVPVBGcoverVPVBZexplodesSVPVBGcoverFigure 5: Tree patterns in tree pattern matching130The input to the learning algorithm is labeledexamples of the form (xi, yi).
yi is the label (eitherNULL for no SRL, or the SRL) of the ith example.xi is a feature vector ?P,A,Dist, Position,R-type, ti ?
tI , Distti?, where P is the predicateelementary tree, A is the tree for the constituentbeing labeled with a SRL, tI is a set of interme-diate elementary trees between the predicate treeand the argument tree.
Each P,A, I tree consistsof the elementary tree template plus the tag, wordpair: ?t, w?.All possible combinations of fully-lexicalized/postag/un-lexicalized elementarytrees are used for each example.
Dist and Disttidenote the distance to the predicate from theargument tree and the intermediate elementarytree respectively.
Position is interpreted as theposition that the target is relative to the predicate.R-type denotes the relation type of the predicateand the target constituent.
3 types are defined: ifthe predicate dominates (directly or undirectly)the argument in the derivation tree, we have therelation of type-1; if the other way around, theargument dominates (directly or undirectly) thepredicate then we have the relation of type-2; andfinally type-3 means that neither the predicateor the argument dominate each other in thederivation tree and instead are dominated (again,directly or indirectly) by another elementary tree.The output of the learning algorithm is a func-tion h(x, y) which is an estimate of the conditionalprobability p(y | x) of seeing SRL y given pat-tern x. h is interpreted as a decision list of rulesx ?
y ranked by the score h(x, y).
In testing,we simply pick the first rule that matches the par-ticular test example x.
We trained different mod-els using the same learning algorithm.
In additionto the LTAG-based method, we also implementeda pattern matching based method on the derived(parse) tree using the same model.
In this method,instead of considering each intermediate elemen-tary tree between the predicate and the argument,we extract the whole path from the predicate to theargument.
So the input is more like a tree than adiscrete feature vector.
Figure 5 shows the patternsthat are extracted from the same pruned tree.4 Experiments and ResultsWe use the PropBank corpus of predicate-argument structures (Palmer, Gildea and Kings-bury, 2005) as our source of annotated data for thedev = Sec24 p(%) r(%) f(%)test = Sec23M1: dev 78.42 77.03 77.72M1: test 80.52 79.40 79.96M2: dev 81.11 79.39 80.24M2: test 83.47 81.82 82.64M3: dev 80.98 79.56 80.26M3: test 81.86 83.34 82.60Table 1: Results on the CoNLL 2005 shared taskusing gold standard parse trees.
M1 is the LTAG-based model, M2 is the derived tree pattern match-ing Model, M3 is a hybrid modelSRL task.
However, there are many different waysto evaluate performance on the PropBank, leadingto incomparable results.
To avoid such a situation,in this paper we use the CoNLL 2005 shared SRLtask data (Carreras and Ma`rquez, 2005) whichprovides a standard train/test split, a standardmethod for training and testing on various prob-lematic cases involving coordination.
However, insome cases, the CoNLL 2005 data is not ideal forthe use of LTAG-based features as some ?deep?
in-formation cannot be recovered due to the fact thattrace information and other empty categories likePRO are removed entirely from the training data.As a result some of the features that undo long-distance movement via trace information in theTreeBank as used in (Chen and Rambow, 2003)cannot be exploited in our model.
Our results areshown in Table 1.
Note that we test on the goldstandard parse trees because we want to comparea model using features from the derived parse treesto the model using the LTAG derivation trees.5 Related WorkIn the community of SRL researchers (cf.
(Gildeaand Jurafsky, 2002; Punyakanok, Roth and Yih,2005; Pradhan et al 2005; Toutanova et al,2005)), the focus has been on two different aspectsof the SRL task: (a) finding appropriate features,and (b) resolving the parsing accuracy problem bycombining multiple parsers/predictions.
Systemsthat use parse trees as a source of feature func-tions for their models have typically outperformedshallow parsing models on the SRL task.
Typi-cal features extracted from a parse tree is the pathfrom the predicate to the constituent and variousgeneralizations based on this path (such as phrasetype, position, etc.).
Notably the voice (passive or131active) of the verb is often used and recovered us-ing a heuristic rule.
We also use the passive/activevoice by labeling this information into the parsetree.
However, in contrast with other work, in thispaper we do not focus on the problem of parse ac-curacy: where the parser output may not containthe constituent that is required for recovering allSRLs.There has been some previous work in SRLthat uses LTAG-based decomposition of the parsetree and we compare our work to this moreclosely.
(Chen and Rambow, 2003) discuss amodel for SRL that uses LTAG-based decompo-sition of parse trees (as is typically done for sta-tistical LTAG parsing).
Instead of using the typi-cal parse tree features used in typical SRL models,(Chen and Rambow, 2003) uses the path withinthe elementary tree from the predicate to the con-stituent argument.
They only recover seman-tic roles for those constituents that are localizedwithin a single elementary tree for the predicate,ignoring cases that occur outside the elementarytree.
In contrast, we recover all SRLs regardlessof locality within the elementary tree.
As a result,if we do not compare the machine learning meth-ods involved in the two approaches, but rather thefeatures used in learning, our features are a naturalgeneralization of (Chen and Rambow, 2003).Our approach is also very akin to the approachin (Shen and Joshi, 2005) which uses PropBankinformation to recover an LTAG treebank as if itwere hidden data underlying the Penn Treebank.This is similar to our approach of having severalpossible LTAG derivations representing recoveryof SRLs.
However, (Shen and Joshi, 2005) donot focus on the SRL task, and in both of theseinstances of previous work using LTAG for SRL,we cannot directly compare our performance withtheirs due to differing assumptions about the task.6 Conclusion and Future WorkIn this paper, we proposed a novel model forSRL using features extracted from LTAG deriva-tion trees.
A simple decision list learner is appliedto train on the tree patterns containing new fea-tures.
This simple learning method enables us toquickly explore new features for this task.
How-ever, this work is still preliminary: a lot of addi-tional work is required to be competitive with thestate-of-the-art SRL systems.
In particular, we donot deal with automatically parsed data yet, whichleads to a drop in our performance.
We also do notincorporate various other features commonly usedfor SRL, as our goal in this paper was to make adirect comparison between simple pattern match-ing features on the derived tree and compare themto features from LTAG derivation trees.ReferencesX.
Carreras and L. Ma`rquez 2005.
Introduction tothe CoNLL-2005 Shared Task.
In Proc.
of CoNLL2005.J.
Chen and O. Rambow.
2003.
Use of Deep Linguis-tic Features for the Recognition and Labeling of Se-mantic Arguments.
In Proceedings of the 2003 Con-ference on Empirical Methods in Natural LanguageProcessing, Sapporo, Japan, 2003.D.
Gildea and D. Jurafsky.
2002.
Automatic Label-ing of Semantic Roles.
Computational Linguistics,58(3):245?288M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An Annotated Corpus of Seman-tic Roles.
Computational Linguistics, 31(1).G.
Melli and Y. Wang and Y. Liu and M. Kashani and Z.Shi and B. Gu and A. Sarkar and F. Popowich 2005.Description of SQUASH, the SFU Question An-swering Summary Handler for the DUC-2005 Sum-marization Task.
In Proceeding of Document Un-derstanding Conference (DUC-2005)S. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, andD.
Jurafsky.
2005.
Semantic Role Chunking Com-bining Complementary Syntactic Views, In Pro-ceedings of the 9th Conference on Natural LanguageLearning (CoNLL 2005), Ann Arbor, MI, 2005.V.
Punyakanok, D. Roth, and W Yih.
2005.
Gener-alized Inference with Multiple Semantic Role La-beling Systems (shared task paper).
Proc.
of theAnnual Conference on Computational Natural Lan-guage Learning (CoNLL) pp.
181-184Ruppenhofer, Josef, Collin F. Baker and Charles J. Fill-more.
2002.
The FrameNet Database and Soft-ware Tools.
In Braasch, Anna and Claus Povlsen(eds.
), Proceedings of the Tenth Euralex Interna-tional Congress.
Copenhagen, Denmark.
Vol.
I: 371-375.L.
Shen and A. Joshi.
2005.
Building an LTAG Tree-bank.
Technical Report MS-CIS-05-15, CIS Depart-ment, University of Pennsylvania.K.
Toutanova, A. Haghighi, and C. D. Manning.
2005.Joint learning improves semantic role labeling.
ACL2005N.
Xue and M. Palmer.
2004.
Calibrating Featuresfor Semantic Role Labeling, In Proceedings ofEMNLP-2004.
Barcelona, Spain.132
