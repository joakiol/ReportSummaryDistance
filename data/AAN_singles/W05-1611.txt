Discrete Optimization as an Alternative to Sequential Processing in NLGTomasz Marciniak and Michael StrubeEML Research gGmbHSchloss-Wolfsbrunnenweg 3369118 Heidelberg, Germanyhttp://www.eml-research.de/nlpAbstractWe present an NLG system that uses Integer Lin-ear Programming to integrate different decisionsinvolved in the generation process.
Our approachprovides an alternative to pipeline-based sequentialprocessing which has become prevalent in today?sNLG applications.1 IntroductionFrom an engineering perspective, one of the major consid-erations in building a Natural Language Generation (NLG)system is the choice of the architecture.
Two important issuesthat need to be considered at this stage are firstly, the modu-larization of the linguistic decisions involved in the genera-tion process and secondly, the processing flow (cf.
[De Smedtet al, 1996]).On one side of the spectrum lie integrated systems, withall linguistic decisions being handled within a single process(e.g.
[Appelt, 1985]).
Such architectures are theoretically at-tractive, as they assume a close coordination of different typesof linguistic decisions, which are known to be dependent onone another (cf.
e.g.
[Danlos, 1984]).
A major disadvantageof integrated models is the complexity that they necessarilyinvolve, which results in poor portability and scalability.
Onthe other side of the spectrum there are highly modularizedpipeline architectures.
A prominent example of this secondcase is the consensus pipeline architecture recognized by [Re-iter, 1994] and further elaborated in [Reiter and Dale, 2000].The modularization of Reiter?s model occurs at two levels.First, individual linguistic decisions of the same type (e.g.involving lexical or syntactic choice) are grouped togetherwithin single low level tasks, such as lexicalization, aggre-gation or ordering.
Second, tasks are allocated to three high-level generation stages, i.e.
Document Planning, Microplan-ning and Surface Realization.
The processing flow in thepipeline architecture is sequential, with individual tasks be-ing executed in a predetermined order.A study of applied NLG systems [Cahill and Reape, 1999]reveals, however, that while most applied NLG systems relyon sequential processing, they do not follow the strict modu-larization that the consensus model assumes.
Low-level tasksare spread over various generation stages and may in fact beexecuted more than once at diverse positions in the pipeline.An attempt to account for commonalities that many NLGsystems share, without imposing too many restrictions, as isthe case with Reiter?s ?consensus?
model, is the ReferenceArchitecture for Generation Systems (RAGS) [Mellish et al,2004].
RAGS is an abstract specification of an NLG architec-ture that focuses on two issues: data types that the generationprocess manipulates and a generic model of the interactionsbetween modules, based on a common central server.
Animportant feature of RAGS is that it leaves the question ofprocessing flow to the actual implementation.
Hence it is the-oretically possible to build both fully integrated as well aspipeline-based systems that would observe the RAGS princi-ples.
Two implementations of RAGS presented in [Mellishand Evans, 2004] demonstrate an intermediate way.In this paper we present a novel approach to building anintegrated NLG system, in which the generation process ismodeled as a discrete optimization problem.
It provides anextension to the classification-based generation framework,presented in [Marciniak and Strube, 2004].
We first assumemodularization of the generation process at the lowest possi-ble level: individual tasks correspond to realizations of sin-gle form elements (FEs) that build up a linguistic expression.The decisions that these tasks involve are then representedas classification tasks and integrated via an Integer LinearProgramming (ILP) formulation (see e.g.
[Nemhauser andWolsey, 1999].
This way we avoid the well known orderingproblem that is present in all pipeline-based systems.
Observ-ing, at least partially, the methodological principles of RAGS,we specify the architecture of our system at two independentlevels.
At the abstract level, the low-level generation tasksare defined, all based on the same input/output interface.
Atthe implementation level, the processing flow and integrationmethod are determined.The rest of the paper is organized as follows: in Section 2we present briefly the classification-based generation frame-work and remark on the shortcomings of pipeline-based pro-cessing.
In Section 3 we introduce the ILP formulation of thegeneration task, and in Section 4 we report on the experimentsand evaluation of the system.2 Classification-Based GenerationIn informal terms, classification can be characterized as thetask of assigning a class label to an unknown instance, given aset of its properties and attributes represented as a feature vec-???????
?conn:2adj_dir_dsc:adj_rank_dsc:rightand314prep_lex:phr_type:adj_rank_phr:alongPP11 verb_lex:s_exp: VPverb_form: bare_infcontinuecontinueVVPDcprep_lex:phr_type:adj_rank_phr: 2toPP23conn:adj_rank_dsc: 1nulladj_dir_dsc: leftDc4conn:adj_rank_dsc: ?nulladj_dir_dsc:  ?Dcnull2CONNDcDcDc**DcCONNDcDcDcCONNDcDcDcandcontinue ...andfacing ...nullcontinue ...*CONNDcDcDc*Dcfacing ...nullVPalong the roadto the sports ...**DcVPVPalong the roadVVPcontinueVPVP**PPto the sports ...DcCONN Dcnull turn ...DcCONN Dcturn ...PP PPPPFigure 1: LTAG-based derivation at the clause (left) and discourse levels (right).
Elementary trees are represented as featurevectors.
Adjunction operations are marked with dashed arrows.tor.
In recent years supervised machine learning methods re-lying on pre-classified training data have been applied in var-ious areas of NLP to solve tasks formulated as classificationproblems.
In NLG machine learning methods have been usedto solve single tasks such as content selection and ordering(e.g.
[Duboue, 2004; Dimitromanolaki and Androutsopoulos,2003]), lexicalization (e.g.
[Reiter and Sripada, 2004]) andreferring expressions generation (e.g.
[Cheng et al, 2001]).In these applications classifiers trained on labeled datahave proven more robust and efficient than approaches us-ing explicit expert knowledge.
The difficulty of formaliz-ing the linguistic knowledge involved in the developmentof a knowledge-based system (a.k.a.
knowledge-acquisition-bottleneck) has been replaced with an effort of obtaining theright kind of data, which typically involves annotating man-ually a corpus of relevant texts with the required linguisticinformation (cf.
[Daelemans, 1993]).The classification-based generation framework that we in-troduced in [Marciniak and Strube, 2004] is based on a simpleidea that the linguistic form of an expression can be decom-posed into a set of discrete form elements (FEs) representingboth its syntactic and lexical properties.
The generation pro-cess is then modeled as a series of classification tasks that re-alize individual FEs.
Realization of each FE is then regardedas a single low-level generation task.2.1 Route DirectionsAs the main application for this work we consider the task ofgenerating natural language route directions.
An example ofsuch a text is given below:(a) Facing the Wildcat statue, (b) turn left on thebrick sidewalk (c) and continue along the road tothe Sports Complex.
(d) Make a right onto ConcordRoad, (e) and keep going straight, (f) passing Pres-byterian Church on your left, (g) until you reachCopeland Street.
(h) The library building will bejust around the corner on your right.We analyze the content of instructional texts of this kind interms of temporally related situations, i.e.
actions (b, c, d, e)states (a, h) and events (f, g), denoted by individual discourseunits.
The temporal structure of the texts is then modeled as atree, with nodes representing individual situation descriptionsand edges signaling the relations (see Figure 2).
The seman-tics of each discourse unit is further represented as a featurevector describing the aspectual category and frame structureof the profiled situation.
This tree-based representation of thesemantic content of route directions constitutes the input tothe generation process.
A detailed description of the underly-ing conceptual model and the annotation process is presentedin [Marciniak and Strube, 2005].initial subsequentturn left on the brick sidewalkFacing the Wildcat statue and continue along the road ...subsequentMake a right onto Concord Roadsubsequentand keep going straightongoing ongoingsubsequentuntil you reach Copeland StreetThe library building will be ...passing Presbyterian Church ...Figure 2: Temporal Structure2.2 From LTAG to Form ElementsTo specify an inventory of FEs that would become objects ofthe low-level generation tasks, we first apply the LexicalizedTree Adjoining Grammar (LTAG) formalism (see e.g.
[Joshiand Schabes, 1991]) to model the linguistic form of the texts.In LTAG, the derivation of a linguistic structure starts with aselection of elementary trees, anchored by lexical items, suchas verbs or prepositions at the clause level and discourse con-nectives at the discourse level (cf.
[Webber and Joshi, 1998]).In the next step, elementary trees are put together by meansof adjunction operations that follow the dependency structureprovided by the derivation tree.
We take the temporal struc-ture from Figure 2 to constitute the discourse level derivationtree, with the temporal relationships corresponding to the syn-tactic dependencies.
At the clause level, the derivation tree isisomorphic with the frame-based ontological representationof individual situations (see [Marciniak and Strube, 2005]).The clause- and discourse-level derivation of discourse unit(c) from the above example in the context of (a) and (b) isdepicted in Figure 1.
At the clause level, the set of elemen-tary trees includes one initial tree ?1 anchored by the mainverb, which also specifies the syntactic frame of the clause,and auxiliary trees ?1 and ?2 corresponding to the verb argu-ments.
At the discourse level, the discourse unit which occu-pies the root position in the temporal structure (cf.
Figure 2)Adj.
Rank Adj.
Dir.
Conn. S Exp.
Verb Lex.
Verb Form1 right and VP continue Bare Inf.Phr.
Type1 Prep.
Lex.1 Adj.
Rank1 Phr.
Type2 Prep.
Lex.2 Adj.
Rank2PP along 1 PP to 2Table 1: FEs based form representation of and continue alongthe road to the sport complex.is modeled as the initial tree ?2, and auxiliary trees ?3 and ?4represent the remaining discourse units.To model the whole process in a uniform way we en-code the elementary trees as feature vectors, with individ-ual features conveying syntactic (e.g.
s exp) and lexical (e.g.verb lex) information.
Features adj rank and adj dir denoterespectively the ordering of the adjunction operations and theadjunction direction, which both determine the linear struc-ture of the text.
Hence the form of the whole discourse canbe represented in terms of feature-value pairs used to encodethe initial trees and the derivation process.
On that basis wedefine a set of form elements building up a discourse as di-rectly corresponding to the individual features.
A detaileddescription of the FEs is given below:FE1: Adjunction Rank / Disc.
Level specifies the linearrank of each discourse unit at the local level, i.e.
only clausestemporally related to the same parent clause are considered.FE2: Adjunction Direction is concerned with the positionof the child discourse unit relative to the parent one (e.g.
(a)left of (b), (c) right of (b), etc.
).FE3: Connective determines the lexical form of the dis-course connective (e.g.
null in (a), until in (g)).FE4: S Expansion specifies whether a given discourse unitis realized as a clause with the explicit subject (i.e.
np+vpexpansion of the root S node in a clause) (e.g.
(g, h)) or not(e.g.
(a), (b)).FE5: Verb Form denotes the form of the main verb in aclause (e.g.
gerund in (a), (c), bare infinitive in (b), finitepresent in (g), etc.
).FE6: Verb Lex.
specifies the lexical form of the main verb(e.g.
turn in (b), pass in (f) or reach in (g)).FE7: Phrase Type determines for each argument in a clauseits syntactic realization as a noun phrase (NP) , prepositionalphrase (PP) or a particle (P).FE8: Preposition Lex.
is concerned with the choice of alexical form for prepositions or particles in argument phrases(e.g.
left and on in (b) or along and to in (c)).
If the value ofFE7 is NP, then this FE is set to none.FE9: Adjunction Rank / Phr.
Level specifies the linearrank of each verb argument within a clause.As an example, consider the FEs-based representation ofthe form of clause (c) presented in Table 1.
Realization ofeach FEi is represented as a classification task Ti, with a setof possible class labels corresponding to the different formsthat FEi may take.
Only tasks T1 and T9 associated re-spectively with Adjunction Rank / Disc.
Level and AdjunctionRank / Phr.
Level are split into a series of binary precedenceclassifications that determine the relative position of two dis-course units or phrasal arguments at a time (e.g.
(a) ?
(c), (c)?
(d), and similarly along the road ?
to the sports complexetc.).
These partial results are later combined to determinethe rank of the respective constituents.Arguably, the above FEs and the corresponding tasks areindependent of the underlying grammatical model.
In thiswork we use the abstraction of the grammatical structure pro-vided by LTAG, but the same or a similar set of FEs canbe readily derived from other formalisms (cf.
e.g.
[Meteer,1990]).
The role of the grammatical theory in defining formelements is twofold.
First, it specifies the exact position ofindividual FEs in the grammatical structure, making it clearhow they should be assembled.
Second, it ensures a widecoverage: although the linguistic structures that we considerhere are relatively simple, the use of LTAG as the underlyinggrammatical formalism guarantees that our generation frame-work can be applied to producing much more complex con-structions, both at the clause and discourse levels.
Appar-ently, this would require a richer feature vector representationof the initial trees, and hence a larger number of FEs and thecorresponding generation tasks.
The basic principles of thegeneration process, however, would remain unchanged.Notice also that the tasks considered here can be groupedunder the conventional NLG labels, such as text structuring(i.e.
T1, T2), lexicalization (i.e.
T3, T6, T8) and sentence re-alization (i.e.
T4, T5, T9).
Yet another important NLG task,i.e.
aggregation appears to be handled indirectly by T3 (e.g.Turn left.
Continue along the road.
vs.
Turn left and con-tinue along the road.)
and T5 (e.g.
Keep going straight.
Youwill pass the Presbyterian Church on your right.
vs. Keep go-ing straight, passing the Presbyterian Church on your right.
).We view it as the strength of our approach that regardless oftheir different linguistic character all these tasks are modeledin exactly the same way.2.3 System Architecture and SequentialProcessingAt an abstract level, the architecture of our system consistsof an unordered set of classifiers solving individual genera-tion tasks.
Each classifier is trained on a separate set of dataobtained from the corpus of route directions annotated withboth semantic and grammatical information.In the previous work [Marciniak and Strube, 2004] we fol-lowed the sequential paradigm advocated by [Daelemans andvan den Bosch, 1998] and implemented the system as a cas-cade of classifiers.
In such systems the output representationis built incrementally, with subsequent classifiers having ac-cess to the outputs of previous modules.
An important char-acteristic of this model is its extensibility.
Since classifiersrely on a uniform representation of the input (i.e.
a featurevector) and the output (i.e.
a single feature value), it is easy tochange the ordering or insert new modules at any place in thepipeline.
Both operations only require retraining classifierswith a new selection of the input features.A major problem that we faced was that we found nosatisfactory method to determine the right ordering of in-dividual classifiers that would guarantee optimal realizationof the grammatical form of the generated expression.
Wefound out that no matter what ordering we adopted tasksthat were solved at the begining had a lower accuracy as thenecessary contextual information, i.e.
based on the outcomesfrom other tasks, was missing.
At the same time, subsequentStartl n1 l n2l 22l 21l 11 l 12lnnml 22ml1m1 1TT2Tnc(l    )11c(l)12c(l     )1m1c(l    )222m2c(l     )c(l   )21...........................Figure 3: Sequential processing as a graph.tasks were influenced by the initial decisions, which in somecases led to error propagation.
Apparently, this was due tothe well known fact that elements of the linguistic structureare strongly correlated with one another (see e.g.
[Danlos,1984]).
Hence individual generation decisions should not behandled in isolation and arranging them in a fixed order willalways involve a specific ordering bias.To get a feeling for the limitations that sequential process-ing of generation tasks involves, consider its graphical repre-sentation in Figure 3.
The process corresponds to the best-first traversal of a weighted multi-layered lattice.
Separatelayers T1, ..., Tn correspond to the individual tasks, and thenodes at each layer (li1, ..., limi) represent class labels foreach task1.
In the sequential model only transitions betweennodes belonging to subsequent layers are granted.
Each suchtransition is augmented with a transition cost, which may beaffected by the traversal history but does not consider the fu-ture choices.
Nodes selected in this process represent the out-comes of individual tasks.
As can be seen, the process is lo-cally driven and it does not guarantee an optimal realizationof the tasks.As an example consider three interrelated form elements:Connective, S Exp.
and Verb Form and their different real-izations presented in Table 2.
Apparently each of these FEshas the potential to affect the overall meaning of the discourseunit or its stylistics.
It can also be seen that only certain com-binations of different forms are allowed in the given semanticcontext.
Different realization of any of these FEs would re-quire other elements to be changed accordingly.
To conclude,following Danlos?
observation, we see no a priori reason toimpose any fixed ordering on the respective generation tasks,and the experiments that we describe in Section 4 support thisposition.3 Discrete Optimization ModelAs an alternative to sequential ordering of the generationtasks we consider the metric labeling problem formulated by[Kleinberg and Tardos, 2000], and originally applied in an1Since different generation tasks may have varying numbers oflabels we denote the cardinality of Li, i.e.
the set of possible labelsfor task Ti, as mi.Discourse Unit FE3 FE4 FE5Pass the First Union Bank ... null vp bare inf.It is necessary that you pass ... null np+vp bare inf.Passing the First Union Bank ... null vp gerundAfter passing the First Union Bank ... after vp gerundAfter your passing .
.
.
after np+vp gerundAs you pass the First Union Bank ... as np+vp fin.
pres.Until you pass the First Union Bank ... until np+vp fin.
pres.Until passing .
.
.
until vp gerundTable 2: Different realizations of form elements: Connective,Verb Form and S Expansion.
Rare but correct constructionsare in italics.image restoration application, where classifiers determine the?true?
intensity values of individual pixels.
This task is for-mulated as a labeling function f : P ?
L which maps aset P of n objects onto a set L of m possible labels.
Thegoal is to find an assignment that minimizes the overall costfunction Q(f) which has two components: assignment costs,i.e.
the costs of selecting a particular label for individual ob-jects, and separation costs, i.e.
the costs of selecting a pairof labels for two related objects2.
[Chekuri et al, 2001] pro-posed an integer linear programming (ILP) formulation of themetric labeling problem, with both assignment cost and sep-aration costs being modeled as binary variables of the linearcost function.Recently, [Roth and Yih, 2004] applied an ILP model tothe task of the simultaneous assignment of semantic roles tothe entities mentioned in a sentence and recognition of therelations holding between them.
The assignment costs werecalculated on the basis of predictions of basic classifiers, i.e.trained for both tasks individually with no access to the out-comes of the other task.
The separation costs were formulatedin terms of binary constraints which specified whether a spe-cific semantic role could occur in a given relation, or not.In the remainder of this paper, we present a more generalmodel, which we apply to the generation tasks presented inSection 2.
We put no limits on the number of tasks beingsolved, and express the separation costs as stochastic con-straints, which can be calculated off-line from the availablelinguistic data.3.1 ILP FormulationWe consider a general context in which the generation processcomprises a range of linguistic decisions modeled as a set of nclassification tasks T = {T1, ..., Tn} which potentially formmutually related pairs.Each task Ti consists in assigning a label from Li ={li1, ..., limi} to an instance that represents the particular de-cision.
Assignments are modeled as variables of a linearcost function.
We differentiate between simple variables thatmodel individual assignments of labels and compound vari-ables that represent respective assignments for each pair ofrelated tasks.To represent individual assignments the following proce-dure is applied: for each task Ti, every label from Li is asso-2These costs were calculated as the function of the metric dis-tance between a pair of pixels and the difference in intensity.ciated with a binary variable x(lij).
Each such variable rep-resents a binary choice, i.e.
a respective label lij is selected ifx(lij) = 1 or rejected otherwise.
The coefficient of variablex(lij) which models the assignment cost c(lij) is given by:c(lij) = ?log2(p(lij))where p(lij) is the probability of lij being selected as the out-come of task Ti.
The probability distribution for each taskis provided by the basic classifiers that do not consider theoutcomes of other tasks3.The role of compound variables is to provide pairwise con-straints on the outcomes of individual tasks.
Since we areinterested in constraining only those tasks are that truly de-pendent on one another we first apply the contingency coeffi-cient C to measure the degree of correlation for each pair oftasks4.
In the case of tasks Ti and Tk which are significantlycorrelated, for each pair of labels fromLi?Lk we build a sin-gle variable x(lij , lkp).
Each such variable is associated witha coefficient representing the constraint on the respective pairof labels lij , lkp calculated in the following way:c(lij , lkp) = ?log2(p(lij,lkp))with p(lij,lkp) denoting the prior joint probability of labelslij and lkp in the data, which is independent from the generalclassification context and hence can be calculated off-line5.The ILP model consists of the target function and a set ofconstraints which block illegal assignments (e.g.
only one la-bel of the given task can be selected)6.
In our case the targetfunction is the cost function Q(f), which we want to mini-mize:min Q(f) =?Ti?T?lij?Lic(lij) ?
x(lij)+?Ti,Tk?T,i<k?<lij ,lkp>?Li?Lkc(lij , lkp) ?
x(lij , lkp)Constraints need to be formulated for both the simple andcompound variables.
First we want to ensure that exactly onelabel lij belonging to task Ti is selected, i.e.
only one simplevariable x(lij) representing labels of a given task can be setto 1:?lij?Lix(lij) = 1, ?i ?
{1, ..., n}3In this case the ordering of tasks is not necessary, and the clas-sifiers can run independently from each other.4C is a test for measuring the association of two nominal vari-ables, and hence adequate for the type of tasks that we considerhere.
The coefficient takes values from 0 (no correlation) to 1 (com-plete correlation) and is calculated by the formula: C = (?2/(N +?2))1/2, where ?2 is the chi-squared statistic and N the total num-ber of instances.
The significance of C is then determined from thevalue of ?2 for the given data.
See e.g.
[Goodman and Kruskal,1972].5In Section 4 we discuss an alternative approach which considersthe actual input.6For a detailed overview of linear programming and differenttypes of LP problems see e.g.
[Nemhauser and Wolsey, 1999].l 111TTnl 21T2l n1l 1m1l 2m2l nmn.........c(l    )11c(l    ,l     )11   2m11   21c(l    ,l    )1m21c(l    ,l    )c(l    ,l)21   nmc(l     ,l     )c(l,l)21n1c(l,l)2mn12mnmc(l,l)c(l     )2m21c(l    )c(l    )n1c(l     )nm1mc(l     )1m   2mFigure 4: Graph representation of the ILP model.We also require that if two simple variables x(lij) andx(lkp), modeling respectively labels lij and lkp, are set to1, then the compound variable x(lij , lkp), which models co-occurrence of these labels, is also set to 1.
This is done intwo steps: we first ensure that if x(lij) = 1, then exactly onevariable x(lij , lkp) must also be set to 1:x(lij)?
?lkp?Lkx(lij , lkp) = 0,?i, k ?
{1, ..., n}, i < k ?
j ?
{1, ..., mi}and do the same for variable x(lkp):x(lkp)?
?lij?Lix(lij , lkp) = 0,?i, k ?
{1, ..., n}, i < k ?
p ?
{1, ..., mk}Finally, we constrain the values of both simple and com-pound variables to be binary:x(lij) ?
{0, 1} ?
x(lij , lkp) ?
{0, 1},?i, k ?
{1, ..., n} ?
j ?
{1, ..., mi} ?
p ?
{1, ..., mk}We can represent the decision process that our ILP modelinvolves as a graph, with the nodes corresponding to indi-vidual labels and the edges marking the associations betweenlabels belonging to correlated tasks.
In Figure 4, task T1 iscorrelated with task T2 and task T2 with task Tn.
No corre-lation exists for pair T1, Tn.
Both nodes and edges are aug-mented with costs.
The goal is to select a subset of connectednodes, minimizing the overall cost, given that for each groupof nodes T1, T2, ..., Tn exactly one node must be selected,and the selected nodes, representing correlated tasks, mustbe connected.
We can see that in contrast to the pipeline ap-proach (cf.
Figure 1), no local decisions determine the overallassignment as the global distribution of costs is considered.4 Experiments and ResultsIn order to evaluate our approach we conducted a series ofexperiments with two implementations of the ILP model andtwo different pipelines.
Each system takes as input the tree-based representation of the semantic content of route direc-tions described in Section 2.
The generation process traversesthe temporal tree in a depth-first fashion, and for each node asingle discourse unit is realized.T :  Verb Form5T :  Disc.
Units Rank1T :  Verb Lex6 4T :  S Exp.T :  Connective3T :  Disc.
Units Dir.2T :  Phrase Type7 T :  Prep.
Lex8T :  Phrase Rank9Figure 5: Correlation network for the generation tasks.null and as after until T3 ConnectiveT5 Verb Form0.40 0.18 0 0 0 bare inf0 0 0 0.04 0.01 gerund0.05 0.01 0.06 0.03 0.06 fin pres0.06 0.05 0 0 0 will infTable 3: Joint distribution matrix for selected labels of tasksConnective (horizontal) and Verb Form (vertical), computedfor all discourse units in a corpus.4.1 Correlations Between TasksWe started with running the correlation tests for all pairs oftasks.
The obtained correlation network is presented in Fig-ure 5.
It is interesting to observe that tasks which realize FEsbelonging to the same levels of linguistic organization, andhave traditionally been handled within the same generationstages (i.e.
Text Planning, Microplanning and Realization) areclosely correlated with one another.
This fact supports em-pirically some assumptions behind Reiter?s consensus model.On the other hand, there exist quite a few correlations thatextend over the stage boundaries, and all three lexicalizationtasks i.e.
T3, T6 and T8 are correlated with many tasks of atotally different linguistic character.4.2 ILP SystemsWe used the ILP model described in Section 3 to implementtwo generation systems.
To obtain assignment costs, bothsystems get a probability distribution for each task from ba-sic classifiers trained on the training data.
To calculate theseparation costs, modeling the stochastic constraints on theco-occurrence of labels, we considered correlated tasks only(cf.
Figure 5) and applied two calculation methods, which re-sulted in two different system implementations.In ILP1, for each pair of tasks we computed the joint distri-bution of the respective labels considering all discourse unitsin the training data before the actual input was known.
Suchobtained joint distributions were used for generating all dis-course units from the test data.
An example matrix with jointdistribution for selected labels of tasks Connective and VerbForm is given in Table 3.
An advantage of this approach isthat the computation can be done in an offline mode and hasno impact on the run-time.In ILP2, the joint distribution for a pair of tasks was cal-culated at run-time, i.e.
only after the actual input had beennull and as after until T3 ConnectiveT5 Verb Form0.13 0.02 0 0 0 bare inf0 0 0 0 0 gerund0 0 0.05 0.02 0.27 fin pres0.36 0.13 0 0 0 will infTable 4: Joint distribution matrix for tasks Connective andVerb Form, considering only disc.
units similar to (c): untilyou see the river side in front of you, at Phi-threshold ?
0.8.known.
This time we did not consider all discourse units inthe training data, but only those whose meaning, representedas a feature vector, was similar to the meaning of the inputdiscourse unit.
As a similarity metric we used the Phi co-efficient7, and set the similarity threshold at 0.8.
As can beseen from Table 4, the probability distribution computed inthis way is better suited to the specific semantic context.
Thisis especially important if the available corpus is small and thefrequency of certain pairs of labels might be too low to havea significant impact on the final assignment.4.3 Pipeline SystemsAs a baseline we implemented two pipeline systems.
In thefirst one we used the ordering of tasks that resembles mostclosely the standard NLG pipeline and which we also usedbefore in [Marciniak and Strube, 2004]8.Individual classifiers had access to both the semantic fea-tures, and the features output by the previous modules.
Totrain the classifiers, the correct feature values were extractedfrom the training data and during testing the generated, andhence possibly erroneous, values were taken.In the other pipeline system we wanted to minimize theerror-propagation effect and placed the tasks in the order ofdecreasing accuracy.
To determine the ordering of tasks weapplied the following procedure: the classifier with the high-est baseline accuracy was selected as the first one.
The re-maining classifiers were trained and tested again, but this timethey had access to the additional feature.
Again, the classifierwith the highest accuracy was selected and the procedure wasrepeated until all classifiers were ordered.4.4 EvaluationWe evaluated our system using leave-one-out cross-validation, i.e.
for all texts in the corpus, each text was usedonce for testing, and the remaining texts provided the trainingdata.
To solve individual classification tasks we used the de-cision tree learner C4.5 in the pipeline systems and the NaiveBayes algorithm9 in the ILP systems.
Both learning schemes7Phi is a measure of the extent of correlation between two setsof binary variables, see e.g.
[Edwards, 1976].
To represent multi-class features on a binary scale we applied dummy coding whichtransforms multi class-nominal variables to a set of dummy variableswith binary values.8The ordering of tasks is given in Table 5.9Both implemented in the Weka machine learning software [Wit-ten and Frank, 2000].Pipeline 1 Pipeline 2 ILP 1 ILP 2Tasks Pos.
Accuracy ?
Pos.
Accuracy ?
Accuracy ?
Accuracy ?Dis.Un.
Rank 1 96.81% 90.90% 2 96.81% 90.90% 97.43% 92.66% 97.43% 92.66%Dis.Un.
Pos.
2 98.04% 89.64% 1 98.04% 89.64% 96.10% 77.19% 97.95% 89.05%Connective 3 78.64% 60.33% 8 79.10% 61.14% 79.15% 61.22% 79.36% 61.31%S Exp.
4 95.90% 89.45% 3 96.20% 90.17% 99.48% 98.65% 99.49% 98.65%Verb Form 5 86.76% 77.01% 4 87.83% 78.90% 92.81% 87.60% 93.22% 88.30%Verb Lex.
6 64.58% 60.87% 9 67.40% 64.19% 75.87% 73.69% 76.08% 74.00%Phr.
Type 7 86.93% 75.07% 5 87.08% 75.36% 87.33% 76.75% 88.03% 77.17%Prep.
Lex.
8 86.23% 81.12% 6 86.03% 81.10% 87.28% 82.20% 88.59% 83.24%Phr.
Rank 9 84.73% 75.24% 7 86.95% 78.65% 90.22% 84.02% 91.27% 85.72%Phi 0.85 0.87 0.89 0.90Table 5: Results reached by the implemented ILP systems and two baselines.
For both pipeline systems, Pos.
stands for theposition of the tasks in the pipeline.yielded highest results in the respective configurations10.
Tosolve the ILP models we used lp solve, a highly efficientGNU-licence Mixed Integer Programming (MIP) solver11,that implements the Branch-and-Bound algorithm.
For eachtask we applied a feature selection procedure (cf.
[Kohavi andJohn, 1997]) to determine which semantic features should betaken as the input by the basic classifiers.To evaluate individual tasks we applied two metrics: accu-racy, calculated as the proportion of correct classifications tothe total number of instances, and the ?
statistic, which cor-rects for the proportion of classifications that might occur bychance.
For end-to-end evaluation, we applied the Phi coef-ficient to measure the degree of similarity between the vectorrepresentations of the generated form (i.e.
built from the out-comes of individual tasks) and the reference form obtainedfrom the test data.
The Phi-based similarity metric is simi-lar to ?
as it compensates for the fact that a match betweentwo multi-label features is more difficult to obtain than in thecase of binary features.
This measure tells us how well all thetasks have been solved together, which in our case amountsto generating the whole text.The results presented in Table 5 show that the ILP systemsachieved highest accuracy and ?
for most tasks and reachedthe highest overall Phi score.
Notice that ILP2 improved theaccuracy of both pipeline systems for the three correlatedtasks that we discussed before, i.e.
Connective, S Exp.
andVerb Form.
Another group of correlated tasks for which theresults appear interesting are i.e.
Verb Lex., Phrase Type andPhrase Rank (cf.
Figure 3).
Notice that Verb Lex.
got higherscores in Pipeline2, with outputs from both Phrase Type andPhrase Rank (see the respective pipeline positions), but the re-verse effect did not occur: scores for both phrase tasks werelower in Pipeline1 when they had access to the output fromVerb Lex., contrary to what we might expect.
Apparently, thiswas due to the low accuracy for Verb Lex.
which caused the10We have found that in direct comparison C4.5 performs betterthan Naive Bayes but the probability distribution that it outputs isstrongly biased towards the winning label.
In this case it is practi-cally impossible for the ILP system to change the classifier?s deci-sion, as the costs of other labels get extremely high.
Hence the morebalanced probability distribution given by Naive Bayes can be easiercorrected in the optimization process.11http://www.geocities.com/lpsolve/already mentioned error propagation.
This example showswell the advantage that optimization processing brings: bothILP systems reached much higher scores for all three tasks.Finally, it appears as no coincidence that the three tasks in-volving lexical choice, i.e.
Connective, Verb Lex.
and Prepo-sition Lex.
scored lower than the syntactic tasks in all sys-tems.
This can be attributed partially to the limitations ofretrieval measures which do not allow for the fact, that in agiven semantic content more than one lexical form can be ap-propriate.5 ConclusionsIn this paper we showed that the pipeline architecture in anNLG application can be successfully replaced with an inte-grated ILP-based model which is better suited to handlingcorrelated generation decisions.
To the best of our knowl-edge, linear programming has been used in an NLG relatedwork only by [Althaus et al, 2004] to solve a single task ofdetermining the order of discourse constituents.
In a some-what related context [Dras, 1999] used ILP to optimize thetask of text paraphrasing, given global constraints such as textand sentence length, readibilty, etc.In contrast, in this work we use an ILP model to orga-nize the entire process of generating the surface form froman underlying semantic representation, which involves anintegration of different types of NLG tasks.
Although inour system we use machine learning as the primary deci-sion making mechanism, we believe that the ILP model canalso be used with knowledge-based systems that observe theclassification-oriented formulation of the NLG tasks.Finally, we are convinced that an adequate evaluation of anNLG system must at some stage go beyond the application ofquantitative measures.
Nevertheless, it is reasonable to expectthat the improvement that we reached with the ILP system,especially the increase of the overall Phi score, must correlateto some extent with the quality improvement.
To verify itwe are currently proceeding with qualitative evaluation of theoutput from our system.Acknowledgements: The work presented here has beenfunded by the Klaus Tschira Foundation, Heidelberg, Ger-many.
The first author receives a scholarship from KTF(09.001.2004).References[Althaus et al, 2004] Ernst Althaus, Nikiforos Karamanis, andAlexander Koller.
Computing locally coherent discourses.
InProceedings of the 42nd Annual Meeting of the Association forComputational Linguistics, Barcelona, Spain, July 21-26, 2004,pages 399?406, 2004.
[Appelt, 1985] Douglas Appelt.
Planning English Sentences.
Cam-bridge University Press, Cambridge, UK, 1985.
[Cahill and Reape, 1999] Lynne Cahill and Mike Reape.
Compo-nent tasks in applied NLG systems.
Technical Report ITRI-99-05, ITRI, University of Brighton, March 1999.
[Chekuri et al, 2001] Chandra Chekuri, Sanjeev Khanna, JosephNaor, and Leonid Zosin.
Approximation algorithms for the met-ric labeling problem via a new linear programming formulation.In Proceedings of the 12th Annual ACM SIAM Symposium onDiscrete Algorithms, Washington, DC, pages 109?118, 2001.
[Cheng et al, 2001] Hua Cheng, Massimo Poesio, Renate Hen-schel, and Chris Mellish.
Corpus-based NP modifier generation.In Proceedings of the 2nd Meeting of the North American Chap-ter of the Association for Computational Linguistics, Pittsburgh,PA, 2-7 June, 2001, pages 9?16, 2001.
[Daelemans and van den Bosch, 1998] Walter Daelemans and An-tal van den Bosch.
Rapid development of NLP modules withmemory-based learning.
In Proceedings of ELSNET in Wonder-land.
Utrecht: ELSNET, pages 105?113, 1998.
[Daelemans, 1993] Walter Daelemans.
Memory-based lexical ac-quisition and processing.
In Proceedings of the Third Interna-tional EAMT Workshop on Machine Translation and the Lexicon,Heidelberg, Germany, 26-28 April, 1993, pages 85?98, 1993.
[Danlos, 1984] Laurence Danlos.
Conceptual and linguistic deci-sions in generation.
In Proceedings of the 10th InternationalConference on Computational Linguistics, Stanford, Cal., pages501?504, 1984.
[De Smedt et al, 1996] Koenraad De Smedt, Helmut Horacek, andMichael Zock.
Architectures for natural language generation:Problems and perspectives.
In G. Adorni and M. Zock, editors,Trends in Natural Language Generation: An Artificial Intelli-gence Perspective, pages 17?46.
Springer Verlag, 1996.
[Dimitromanolaki and Androutsopoulos, 2003] Aggeliki Dimitro-manolaki and Ion Androutsopoulos.
Learning to order facts fordiscourse planning in natural language generation.
In Proc.
ofthe 9th European Workshop on Natural Language Generation,Budapest, Hungary, 13 ?
14 April 2003, pages 23?30, 2003.
[Dras, 1999] Mark Dras.
Tree Adjoining Grammar and the Reluc-tant Paraphrasing of Text.
PhD thesis, Macquarie University,Australia, 1999.
[Duboue, 2004] Pablo A. Duboue.
Indirect supervised learningof content selection logic.
In Proceedings of the 3rd Interna-tional Conference on Natural Language Generation, Brocken-hurst, UK, 14-16 July, 2004, pages 41?50, 2004.
[Edwards, 1976] Allen L. Edwards.
An Introduction to Linear Re-gression and Correlation.
W. H. Freema, San Francisco, Cal.,1976.
[Goodman and Kruskal, 1972] Leo A. Goodman and W. H.Kruskal.
Measures of association for cross-classification, iv.Journal of the American Statistical Association, 67:415?421,1972.
[Joshi and Schabes, 1991] Aravind K. Joshi and Yves Schabes.Tree-adjoining grammars and lexicalized grammars.
In MauriceNivat and Andreas Podelski, editors, Definability and Recogniz-ability of Sets of Trees.
Elsevier, 1991.
[Kleinberg and Tardos, 2000] Jon M. Kleinberg and Eva Tardos.Approximation algorithms for classification problems with pair-wise relationships: Metric labeling and Markov random fields.Journal of the ACM, 49(5):616?639, 2000.
[Kohavi and John, 1997] Ron Kohavi and George H. John.
Wrap-pers for feature subset selection.
Artificial Intelligence Journal,97:273?324, 1997.
[Marciniak and Strube, 2004] Tomasz Marciniak and MichaelStrube.
Classification-based generation using TAG.
In Pro-ceedings of the 3rd International Conference on NaturalLanguage Generation, Brockenhurst, UK, 14-16 July, 2004,pages 100?109, 2004.
[Marciniak and Strube, 2005] Tomasz Marciniak and MichaelStrube.
Modeling and annotating the semantics of route di-rections.
In Proceedings of the 6th International Workshop onComputational Semantics, Tilburg, The Netherlands, January12-14, 2005, pages 151?162, 2005.
[Mellish and Evans, 2004] Chris Mellish and Roger Evans.
Imple-mentation architectures for natural language generation.
NaturalLanguage Engineering, 10(3/4):261?282, 2004.
[Mellish et al, 2004] Chris Mellish, Mike Reape, Donia Scott,Lynne Cahill, Roger Evans, and Daniel Paiva.
A reference archi-tecture for generation systems.
Natural Language Engineering,10(3/4):227?260, 2004.
[Meteer, 1990] Marie W. Meteer.
Abstract linguistic resources fortext planning.
In Proceedings of the 5th International Workshopon Natural Language Generation, Pittsburgh, PA, 3-6 June 1990,pages 62?69, 1990.
[Nemhauser and Wolsey, 1999] George L. Nemhauser and Lau-rence A. Wolsey.
Integer and combinatorial optimization.
Wiley,New York, NY, 1999.
[Reiter and Dale, 2000] Ehud Reiter and Robert Dale.
BuildingNatural Language Generation Systems.
Cambridge UniversityPress, Cambridge, UK, 2000.
[Reiter and Sripada, 2004] Ehud Reiter and Somayajulu Sripada.Contextual influences on near-synonym choice.
In Proceedingsof the 3rd International Conference on Natural Language Gen-eration, Brockenhurst, UK, 14-16 July, 2004, pages 161?170,2004.
[Reiter, 1994] Ehud Reiter.
Has a consensus NL generation archi-tecture appeared, and is it psycholinguistically plausible?
InProc.
of the 7th International Workshop on Natural LanguageGeneration, Kennebunkport, MA, 21-24 June 1994, pages 160?173, 1994.
[Roth and Yih, 2004] Dan Roth and Wen-tau Yih.
A linear pro-gramming formulation for global inference in natural languagetasks.
In Proceedings of the 8th Conference on ComputationalNatural Language Learning, Boston, Mass., May 2-7, 2004,pages 1?8, 2004.
[Webber and Joshi, 1998] Bonnie Lynn Webber and Aravind Joshi.Anchoring a lexicalized tree-adjoining grammar for discourse.
InProceedings of the COLING/ACL ?98 Workshop on DiscourseRelations and Discourse Markers, Montre?al, Que?bec, Canada,15 August 1998, pages 86?92, 1998.
[Witten and Frank, 2000] Ian H. Witten and Eibe Frank.
Data Min-ing - Practical Machine Learning Tools and Techniques with JavaImplementations.
Morgan Kaufmann, San Francisco, Cal., 2000.
