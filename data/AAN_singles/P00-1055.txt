Using Confidence Bands for Parallel Texts AlignmentAnt?nio RIBEIRODepartamento de Inform?ticaFaculdade de Ci?ncias e TecnologiaUniversidade Nova de LisboaQuinta da TorreP-2825-114 Monte da CaparicaPortugalambar@di.fct.unl.ptGabriel LOPESDepartamento de Inform?ticaFaculdade de Ci?ncias e TecnologiaUniversidade Nova de LisboaQuinta da TorreP-2825-114 Monte da CaparicaPortugalgpl@di.fct.unl.ptJo?o MEXIADepartamento de Matem?ticaFaculdade de Ci?ncias e TecnologiaUniversidade Nova de LisboaQuinta da TorreP-2825-114 Monte da CaparicaPortugalAbstractThis paper describes a language independentmethod for alignment of parallel texts thatmakes use of homograph tokens for eachpair of languages.
In order to filter outtokens that may cause misalignment, we useconfidence bands of linear regression linesinstead of heuristics which are not theoreti-cally supported.
This method was originallyinspired on work done by Pascale Fung andKathleen McKeown, and Melamed, provid-ing the statistical support those authorscould not claim.IntroductionHuman compiled bilingual dictionaries do notcover every term translation, especially when itcomes to technical domains.
Moreover, we canno longer afford to waste human time and effortbuilding manually these ever changing and in-complete databases or design language specificapplications to solve this problem.
The need foran automatic language independent task forequivalents extraction becomes clear in multi-lingual regions like Hong Kong, Macao,Quebec, the European Union, where texts mustbe translated daily into eleven languages, oreven in the U.S.A. where Spanish and Englishspeaking communities are intermingled.Parallel texts (texts that are mutual transla-tions) are valuable sources of information forbilingual lexicography.
However, they are not ofmuch use unless a computational system mayfind which piece of text in one language corre-sponds to which piece of text in the other lan-guage.
In order to achieve this, they must bealigned first, i.e.
the various pieces of text mustbe put into correspondence.
This makes thetranslations extraction task easier and more reli-able.
Alignment is usually done by findingcorrespondence points ?
sequences of characterswith the same form in both texts (homographs,e.g.
numbers, proper names, punctuation marks),similar forms (cognates, like Region and Regi?oin English and Portuguese, respectively) or evenpreviously known translations.Pascale Fung and Kathleen McKeown (1997)present an alignment algorithm that uses termtranslations as correspondence points betweenEnglish and Chinese.
Melamed (1999) alignstexts using correspondence points taken eitherfrom orthographic cognates (Michel Simard etal., 1992) or from a seed translation lexicon.However, although the heuristics both ap-proaches use to filter noisy points may be intui-tively quite acceptable, they are not theoreticallysupported by Statistics.The former approach considers a candidatecorrespondence point reliable as long as, amongsome other constraints, ?[...]
it is not too faraway from the diagonal [...]?
(Pascale Fung andKathleen McKeown, 1997, p.72) of a rectanglewhose sides sizes are proportional to the lengthsof the texts in each language (henceforth, ?thegolden translation diagonal?).
The latter ap-proach uses other filtering parameters: maxi-mum point ambiguity level, point dispersion andangle deviation (Melamed, 1999, pp.
115?116).Ant?nio Ribeiro et al (2000a) propose amethod to filter candidate correspondence pointsgenerated from homograph words which occuronly once in parallel texts (hapaxes) using linearregressions and statistically supported noisefiltering methodologies.
The method avoidsheuristic filters and they claim high precisionalignments.In this paper, we will extend this work by de-fining a linear regression line with all pointsgenerated from homographs with equal frequen-cies in parallel texts.
We will filter out thosepoints which lie outside statistically definedconfidence bands (Thomas Wonnacott andRonald Wonnacott, 1990).
Our method willrepeatedly use a standard linear regression lineadjustment technique to filter unreliable pointsuntil there is no misalignment.
Points resultingfrom this filtration are chosen as correspondencepoints.The following section will discuss relatedwork.
The method is described in section 2 andwe will evaluate and compare the results in sec-tion 3.
Finally, we present conclusions and fu-ture work.1 BackgroundThere have been two mainstreams for paralleltext alignment.
One assumes that translated textshave proportional sizes; the other tries to uselexical information in parallel texts to generatecandidate correspondence points.
Both use somenotion of correspondence points.Early work by Peter Brown et al (1991) andWilliam Gale and Kenneth Church (1991)aligned sentences which had a proportionalnumber of words and characters, respectively.Pairs of sentence delimiters (full stops) wereused as candidate correspondence points andthey ended up being selected while aligning.However, these algorithms tended to break downwhen sentence boundaries were not clearlymarked.
Full stops do not always mark sentenceboundaries, they may not even exist due to OCRnoise and languages may not share the samepunctuation policies.Using lexical information, Kenneth Church(1993) showed that cheap alignment of textsegments was still possible exploiting ortho-graphic cognates (Michel Simard et al, 1992),instead of sentence delimiters.
They became thenew candidate correspondence points.
Duringthe alignment, some were discarded becausethey lied outside an empirically estimatedbounded search space, required for time andspace reasons.Martin Kay and Martin R?scheisen (1993)also needed clearly delimited sentences.
Wordswith similar distributions became the candidatecorrespondence points.
Two sentences werealigned if the number of correspondence pointsassociating them was greater than an empiricallydefined threshold: ?[...]
more than some mini-mum number of times [...]?
(Martin Kay andMartin R?scheisen, 1993, p.128).
In Ido Daganet al (1993) noisy points were filtered out bydeleting frequent words.Pascale Fung and Kathleen McKeown (1994)dropped the requirement for sentence boundarieson a case-study for English-Chinese.
Instead,they used vectors that stored distances betweenconsecutive occurrences of a word (DK-vec?s).Candidate correspondence points were identifiedfrom words with similar distance vectors andnoisy points were filtered using some heuristics.Later, in Pascale Fung and Kathleen McKeown(1997), the algorithm used extracted terms tocompile a list of reliable pairs of translations.Those pairs whose distribution similarity wasabove a threshold became candidate correspon-dence points (called potential anchor points).These points were further constrained not to be?too far away?
from the ?translation diagonal?.Michel Simard and Pierre Plamondon (1998)aligned sentences using isolated cognates ascandidate correspondence points, i.e.
cognatesthat were not mistaken for others within a textwindow.
Some were filtered out if they eitherlied outside an empirically defined search space,named a corridor, or were ?not in line?
withtheir neighbours.Melamed (1999) also filtered candidate corre-spondence points obtained from orthographiccognates.
A maximum point ambiguity levelfilters points outside a search space, a maximumpoint dispersion filters points too distant from aline formed by candidate correspondence pointsand a maximum angle deviation filters pointsthat tend to slope this line too much.Whether the filtering of candidate correspon-dence points is done prior to alignment or duringit, we all want to find reliable correspondencepoints.
They provide the basic means for ex-tracting reliable information from parallel texts.However, as far as we learned from the abovepapers, current methods have repeatedly usedstatistically unsupported heuristics to filter outnoisy points.
For instance, the ?golden transla-tion diagonal?
is mentioned in all of them butnone attempts filtering noisy points using statis-tically defined confidence bands.2 Correspondence Points Filters2.1 OverviewThe basic insight is that not all candidate corre-spondence points are reliable.
Whatever heuris-tics are taken (similar word distributions, searchcorridors, point dispersion, angle deviation,...),we want to filter the most reliable points.
Weassume that reliable points have similar charac-teristics.
For instance, they tend to gather some-where near the ?golden translation diagonal?.Homographs with equal frequencies may begood alignment points.2.2 Source Parallel TextsWe worked with a mixed parallel corpus con-sisting of texts selected at random from the Offi-cial Journal of the European Communities1(ELRA, 1997) and from The Court of Justice ofthe European Communities2 in eleven lan-guages3.Language Written Questions Debates Judgements Totalda 259k (52k) 2,0M (395k) 16k (3k) 2250kde 234k (47k) 1,8M (368k) 15k (3k) 2088kel 272k (54k) 1,9M (387k) 16k (3k) 2222ken 263k (53k) 2,1M (417k) 16k (3k) 2364kes 292k (58k) 2,2M (439k) 18k (4k) 2507kfi --- --- 13k (3k) 13kfr 310k (62k) 2,2M (447k) 19k (4k) 2564kit 279k (56k) 1,9M (375k) 17k (3k) 2171knl 275k (55k) 2,1M (428k) 16k (3k) 2431kpt 284k (57k) 2,1M (416k) 17k (3k) 2381ksv --- --- 15k (3k) 15kTotal 2468k (55k) 18,4M (408k) 177k (3k) 21005kSub-corpusTable 1: Words per sub-corpus (average per textinside brackets; markups discarded)4.For each language, we included:?
five texts with Written Questions asked bymembers of the European Parliament to theEuropean Commission and their corre-sponding answers (average: about 60k wordsor 100 pages / text);1Danish (da), Dutch (nl), English (en), French (fr),German (de), Greek (el), Italian (it), Portuguese (pt) andSpanish (es).2Webpage address: curia.eu.int3The same languages as those in footnote 1 plusFinnish (fi) and Swedish (sv).4No Written Questions and Debates texts for Finnishand Swedish are available in ELRA (1997) since thetexts provided are from the 1992-4 period and it wasnot until 1995 that the respective countries becamepart of the European Union.?
five texts with records of Debates in theEuropean Parliament (average: about 400kwords or more than 600 pages / text).
Theseare written transcripts of oral discussions;?
five texts with judgements of The Court ofJustice of the European Communities (aver-age: about 3k words or 5 pages / text).In order to reduce the number of possible pairsof parallel texts from 110 sets (11 lan-guages?10) to a more manageable size of 10sets, we decided to take Portuguese as the kernellanguage of all pairs.2.3 Generating Candidate Correspon-dence PointsWe generate candidate correspondence pointsfrom homographs with equal frequencies in twoparallel texts.
Homographs, as a naive and par-ticular form of cognate words, are likely transla-tions (e.g.
Hong Kong in various European lan-guages).
Here is a table with the percentages ofoccurrences of these words in the used texts:Pair Written Questions Debates Judgements Averagept-da 2,8k (4,9%) 2,5k (0,6%) 0,3k (8,1%) 2,5k (1,1%)pt-de 2,7k (5,1%) 4,2k (1,0%) 0,4k (7,9%) 4,0k (1,5%)pt-el 2,3k (4,0%) 1,9k (0,5%) 0,3k (6,9%) 1,9k (0,8%)pt-en 2,7k (4,8%) 2,8k (0,7%) 0,3k (6,2%) 2,7k (1,1%)pt-es 4,1k (7,1%) 7,8k (1,9%) 0,7k (15,2%) 7,4k (2,5%)pt-fi --- --- 0,2k (5,2%) 0,2k (5,2%)pt-fr 2,9k (5,0%) 5,1k (1,2%) 0,4k (9,4%) 4,8k (1,6%)pt-it 3,1k (5,5%) 5,4k (1,3%) 0,4k (9,6%) 5,2k (1,8%)pt-nl 2,6k (4,5%) 4,9k (1,2%) 0,3k (8,3%) 4,7k (1,6%)pt-sv --- --- 0,3k (6,9%) 0,3k (6,9%)Average 2,9k (5,1%) 4,4k (1,1%) 0,4k (8,4%) 4,2k (1,5%)Sub-corpusTable 2: Average number of homographs withequal frequencies per pair of parallel texts (aver-age percentage of homographs inside brackets).For average size texts (e.g.
the Written Ques-tions), these words account for about 5% of thetotal (about 3k words / text).
This number variesaccording to language similarity.
For instance,on average, it is higher for Portuguese?Spanishthan for Portuguese?English.These words end up being mainly numbersand names.
Here are a few examples from aparallel Portuguese?English text: 2002 (num-bers, dates), ASEAN (acronyms), Patten (propernames), China (countries), Manila (cities),apartheid (foreign words), Ltd (abbreviations),habitats (Latin words), ferry (common names),global (common vocabulary).In order to avoid pairing homographs that arenot equivalent (e.g.
?a?, a definite article in Por-tuguese and an indefinite article in English), werestricted ourselves to homographs with thesame frequencies in both parallel texts.
In thisway, we are selecting words with similar distri-butions.
Actually, equal frequency words helpedJean-Fran?ois Champollion to decipher the Ro-setta Stone for there was a name of a King(Ptolemy V) which occurred the same number oftimes in the ?parallel texts?
of the stone.Each pair of texts provides a set of candidatecorrespondence points from which we draw aline based on linear regression.
Points are de-fined using the co-ordinates of the word posi-tions in each parallel text.
For example, if thefirst occurrence of the homograph word Pattenoccurs at word position 125545 in thePortuguese text and at 135787 in the Englishparallel text, then the point co-ordinates are(125545,135787).
The generated points mayadjust themselves well to a linear regression lineor may be dispersed around it.
So, firstly, we usea simple filter based on the histogram of thedistances between the expected and real posi-tions.
After that, we apply a finer-grained filterbased on statistically defined confidence bandsfor linear regression lines.We will now elaborate on these filters.2.4 Eliminating Extreme PointsThe points obtained from the positions of homo-graphs with equal frequencies are still prone tobe noisy.
Here is an example:Noisy Candidate Correspondence Pointsy = 0,9165x + 141,65010000200003000040000500000 10000 20000 30000 40000 50000pt Word PositionsenWordPositionsFigure 1: Noisy versus ?well-behaved?
(?inline?)
candidate correspondence points.
Thelinear regression line equation is shown on thetop right corner.The figure above shows noisy points becausetheir respective homographs appear in positionsquite apart.
We should feel reluctant to acceptdistant pairings and that is what the first filterdoes.
It filters out those points which are clearlytoo far apart from their expected positions to beconsidered as reliable correspondence points.We find expected positions building a linearregression line with all points, and then deter-mining the distances between the real and theexpected word positions:pt en PositionsPosition Word Real Expected Distance3877 I 24998 3695 213039009 etc 22897 8399 1449911791 I 25060 10948 1411215248 As 3398 14117 1071916965 As 3591 15690 1209922819 volume 32337 21056 11281Table 3: A sample of the distances betweenexpected and real positions of noisy points inFigure 1.Expected positions are computed from the lin-ear regression line equation y = ax + b, where ais the line slope and b is the Y-axis intercept (thevalue of y when x is 0), substituting x for thePortuguese word position.
For Table 3, the ex-pected word position for the word I at pt wordposition 3877 is 0.9165 ?
3877 + 141.65 = 3695(see the regression line equation in Figure 1)and, thus, the distance between its expected andreal positions is | 3695 ?
24998 | = 21303.If we draw a histogram ranging from thesmallest to the largest distance, we get:Histogram of Distances0246810027695538830711076138451661419383221522492127690304593322835997Distances between Real and Expected Word PositionsNumber of Pointsfiltered points3297Figure 2: Histogram of the distances betweenexpected and real word positions.In order to build this histogram, we use theSturges rule (see ?Histograms?
in Samuel Kotz etal.
1982).
The number of classes (bars or bins) isgiven by 1 + log2n, where n is the total numberof points.
The size of the classes is given by(maximum distance ?
minimum distance) /number of classes.
For example, for Figure 1, wehave 3338 points and the distances betweenexpected and real positions range from 0 to35997.
Thus, the number of classes is1 + log23338 ?
12.7 ?
13 and the size of theclasses is (35997 ?
0) / 13 ?
2769.
In this way,the first class ranges from 0 to 2769, the secondclass from 2769 to 5538 and so forth.With this histogram, we are able to identifythose words which are too far apart from theirexpected positions.
In Figure 2, the gap in thehistogram makes clear that there is a discontinu-ity in the distances between expected and realpositions.
So, we are confident that all pointsabove 22152 are extreme points.
We filter themout of the candidate correspondence points setand proceed to the next filter.2.5 Confidence Bands of Linear Regres-sion LinesConfidence bands of linear regression lines(Thomas Wonnacott and Ronald Wonnacott,1990, p. 384) help us to identify reliable points,i.e.
points which belong to a regression line witha great confidence level (99.9%).
The band istypically wider in the extremes and narrower inthe middle of the regression line.The figure below shows an example of filter-ing using confidence bands:Linear Regression Line Confidence Bands870088008900900091009400 9450 9500 9550 9600 9650 9700 9750 9800pt Word PositionenWordPositionExpected yReal yConfidence bandFigure 3: Detail of the filter based on confi-dence bands.
Point A lies outside the confidenceband.
It will be filtered out.We start from the regression line defined bythe points filtered with the Histogram technique,described in the previous section, and then wecalculate the confidence band.
Points which lieoutside this band are filtered out since they arecredited as too unreliable for alignment (e.g.Point A in Figure 3).
We repeat this step until nopieces of text belong to different translations, i.e.until there is no misalignment.The confidence band is the error admitted atan x co-ordinate of a linear regression line.
Apoint (x,y) is considered outside a linear regres-sion line with a confidence level of 99.9% if its yco-ordinate does not lie within the confidenceinterval [ ax + b ?
error(x); ax + b + error(x)],where ax + b is the linear regression line equa-tion and error(x) is the error admitted at the xco-ordinate.
The upper and lower limits of theconfidence interval are given by the followingequation (see Thomas Wonnacott & RonaldWonnacott, 1990, p.
385):?=?
?+?+=nii XxXxnstbaxy122005.0)()(1)(where:?
t0.005 is the t-statistics value for a 99.9% con-fidence interval.
We will use the z-statisticsinstead since t0.005 = z0.005 = 3.27 for largesamples of points (above 120);?
n is the number of points;?
s is the standard deviation from the expectedvalue y?
at co-ordinate x (see Thomas Won-nacott & Ronald Wonnacott, 1990, p.
379):baxynyysnii+=??=?=?where,2)?(1?
X is the average value of the various xi:?==niixnX113 EvaluationWe ran our alignment algorithm on the paralleltexts of 10 language pairs as described in section2.2.
The table below summarises the results:Pair Written Questions Debates Judgements Averagept-da 128 (5%) 56 (2%) 114 (35%) 63 (2%)pt-de 124 (5%) 99 (2%) 53 (15%) 102 (3%)pt-el 118 (5%) 115 (6%) 60 (20%) 115 (6%)pt-en 88 (3%) 102 (4%) 50 (19%) 101 (4%)pt-es 59 (1%) 55 (1%) 143 (21%) 56 (1%)pt-fi --- --- 60 (26%) 60 (26%)pt-fr 148 (5%) 113 (2%) 212 (49%) 117 (2%)pt-it 117 (4%) 104 (2%) 25 (6%) 105 (2%)pt-nl 120 (5%) 73 (1%) 53 (15%) 77 (2%)pt-sv --- --- 74 (23%) 74 (23%)Average 113 (4%) 90 (2%) 84 (23%) 92 (2%)Sub-corpusTable 4: Average number of correspondencepoints in the first non-misalignment (averageratio of filtered and initial candidate correspon-dence points inside brackets).On average, we end up with about 2% of theinitial correspondence points which means thatwe are able to break a text in about 90 segments(ranging from 70 words to 12 pages per segmentAfor the Debates).
An average of just three filtra-tions are needed: the Histogram filter plus twofiltrations with the Confidence Bands.The figure below shows an example of a mis-aligning correspondence point.Misalignments(Crossed segments)3004005006007008009001000300 400 500 600 700 800pt Word PositionenWordPositionFigure 4: Bad correspondence points (?
?
mis-aligning points; ?
?
FRUUHVSRQGHQFH SRLQWVHad we restricted ourselves to using homo-graphs which occur only once (hapaxes), wewould get about one third of the final points(Ant?nio Ribeiro et al 2000a).
Hapaxes turn outto be good candidate correspondence pointsbecause they work like cognates that are notmistaken for others within the full text scope(Michel Simard and Pierre Plamondon, 1998).When they are in similar positions, they turn outto be reliable correspondence points.To compare our results, we aligned the BAFCorpus (Michel Simard and Pierre Plamondon,1998) which consists of a collection of paralleltexts (Canadian Parliament Hansards, UnitedNations, literary, etc.
).Filename # Tokens # Segments Chars / Segment # Segments Chars / Segment Ratiociti1.fr 17556 49 1860 742 120 6,6%citi2.fr 33539 48 3360 1393 104 3,4%cour.fr 49616 101 2217 1377 140 7,3%hans.fr 82834 45 8932 3059 117 1,5%ilo.fr 210342 68 15654 7129 137 1,0%onu.fr 74402 27 14101 2559 132 1,1%tao1.fr 10506 52 1019 365 95 14,2%tao2.fr 9825 51 972 305 97 16,7%tao3.fr 4673 44 531 176 62 25,0%verne.fr 79858 29 12736 2521 127 1,2%xerox.fr 66605 114 2917 3454 85 3,3%Average 111883 60 10271 3924 123 1,5%Equal Frequency Homographs BAF AnalysisTable 5: Comparison with the Jacal alignment(Michel Simard and Pierre Plamondon, 1998).The table above shows that, on average, wegot about 1.5% of the total segments, resultingin about 10k characters per segment.
This num-ber ranges from 25% (average: 500 charactersper segment) for a small text (tao3.fr-en) to 1%(average: 15k characters per segment) for a largetext (ilo.fr-en).
Although these are small num-bers, we should notice that, in contrast with Mi-chel Simard and Pierre Plamondon (1998), weare not including:?
words defined as cognate ?if their four firstcharacters are identical?;?
an ?isolation window?
heuristics to reduce thesearch space;?
heuristics to define a search corridor to findcandidate correspondence points;We should stress again that the algorithm re-ported in this paper is purely statistical and re-curs to no heuristics.
Moreover, we did not re-apply the algorithm to each aligned parallelsegment which would result in finding morecorrespondence points and, consequently, fur-ther segmentation of the parallel texts.
Besides,if we use the methodology presented in Joaquimda Silva et al (1999) for extracting relevantstring patterns, we are able to identify more sta-tistically reliable cognates.Ant?nio Ribeiro and Gabriel Lopes (1999) re-port a higher number of segments using clustersof points.
However, the algorithm does not as-sure 100% alignment precision and discardssome good correspondence points which end upin bad clusters.Our main critique to the use of heuristics isthat though they may be intuitively quite accept-able and may significantly improve the results asseen with Jacal alignment for the BAF Corpus,they are just heuristics and cannot be theoreti-cally explained by Statistics.ConclusionsConfidence bands of linear regression lines helpus to identify reliable correspondence pointswithout using empirically found or statisticallyunsupported heuristics.
This paper presents apurely statistical approach to the selection ofcandidate correspondence points for paralleltexts alignment without recurring to heuristics asin previous work.
The alignment is not restrictedto sentence or paragraph level for which clearlydelimited boundaries markers would be needed.It is made at whatever segment size as long asreliable correspondence points are found.
Thismeans that alignment can result at paragraph,sentence, phrase, term or word level.Moreover, the methodology does not dependon the way candidate correspondence points aregenerated, i.e.
although we used homographswith equal frequencies, we could have also boot-strapped the process using cognates (MichelSimard et al 1992) or a small bilingual lexiconto identify equivalents of words or expressions(Dekai Wu 1994; Pascale Fung and KathleenMcKeown 1997; Melamed 1999).
This is a par-ticularly good strategy when it comes to distantlanguages like English and Chinese where thenumber of homographs is reduced.
As Ant?nioRibeiro et al (2000b) showed, these tokens ac-count for about 5% for small texts.
Aligninglanguages with such different alphabets requiresautomatic methods to identify equivalents asPascale Fung and Kathleen McKeown (1997)presented, increasing the number of candidatecorrespondence points at the beginning.Selecting correspondence points improves thequality and reliability of parallel texts alignment.As this alignment algorithm is not restricted toparagraphs or sentences, 100% alignment preci-sion may be degraded by language specific termorder policies in small segments.
On average,three filtrations proved enough to avoid crossedsegments which are a result of misalignments.The method is language and character-set inde-pendent and does not assume any a priori lan-guage knowledge (namely, small bilingual lexi-cons), text tagging, well defined sentence orparagraph boundaries nor one-to-one translationof sentences.Future WorkAt the moment, we are working on alignment ofsub-segments of parallel texts in order to findmore correspondence points within each alignedsegment in a recursive way.
We are also plan-ning to apply the method to large parallel Portu-guese?Chinese texts.
We believe we may sig-nificantly increase the number of segments weget in the end by using a more dynamic ap-proach to the filtering using linear regressionlines, by selecting candidate correspondencepoints at the same time that parallel texts tokensare input.
This approach is similar to Melamed(1999) but, in contrast, it is statistically sup-ported and uses no heuristics.Another area for future experiments will userelevant strings of characters in parallel textsinstead of using just homographs.
For this pur-pose, we will apply a methodology described inJoaquim da Silva et al (1999).
This method wasused to extract string patterns and it will help usto automatically extract ?real?
cognates.AcknowledgementsOur thanks go to the anonymous referees fortheir valuable comments on the paper.
Wewould also like to thank Michel Simard for pro-viding us the aligned BAF Corpus.
This researchwas partially supported by a grant from Funda-?
?o para a Ci?ncia e Tecnologia / Praxis XXI.ReferencesPeter Brown, Jennifer Lai and Robert Mercer (1991)Aligning Sentences in Parallel Corpora.
In ?Pro-ceedings of the 29th Annual Meeting of the Asso-ciation for Computational Linguistics?, Berkeley,California, U.S.A., pp.
169?176.Kenneth Church (1993)  Char_align: A Program forAligning Parallel Texts at the Character Level.
In?Proceedings of the 31st Annual Meeting of theAssociation for Computational Linguistics?,Columbus, Ohio, U.S.A., pp.
1?8.Ido Dagan, Kenneth Church and William Gale (1993)Robust Word Alignment for Machine AidedTranslation.
In ?Proceedings of the Workshop onVery Large Corpora: Academic and IndustrialPerspectives?, Columbus, Ohio, U.S.A., pp.
1?8.ELRA (European Language Resources Association)(1997)  Multilingual Corpora for Co-operation,Disk 2 of 2.
Paris, France.Pascale Fung and Kathleen McKeown (1994)Aligning Noisy Parallel Corpora across LanguageGroups: Word Pair Feature Matching by DynamicTime Warping.
In ?Technology Partnerships forCrossing the Language Barrier: Proceedings of theFirst Conference of the Association for MachineTranslation in the Americas?, Columbia, Maryland,U.S.A., pp.
81?88.Pascale Fung and Kathleen McKeown (1997)  ATechnical Word- and Term-Translation Aid UsingNoisy Parallel Corpora across Language Groups.Machine Translation, 12/1?2 (Special issue),pp.
53?87.William Gale and Kenneth Church (1991)  A Pro-gram for Aligning Sentences in Bilingual Corpora.In ?Proceedings of the 29th Annual Meeting of theAssociation for Computational Linguistics?,Berkeley, California, U.S.A., pp.
177?184 (shortversion).
Also (1993) Computational Linguistics,19/1, pp.
75?102 (long version).Martin Kay and Martin R?scheisen (1993)  Text-Translation Alignment.
Computational Linguistics,19/1, pp.
121?142.Samuel Kotz, Norman Johnson and Campbell Read(1982)  Encyclopaedia of Statistical Sciences.
JohnWiley & Sons, New York Chichester BrisbaneToronto Singapore.I.
Dan Melamed (1999)  Bitext Maps and Alignmentvia Pattern Recognition.
Computational Linguis-tics, 25/1, pp.
107?130.Ant?nio Ribeiro, Gabriel Lopes and Jo?o Mexia(2000a)  Using Confidence Bands for Alignmentwith Hapaxes.
In ?Proceedings of the InternationalConference on Artificial Intelligence (IC?AI2000)?, Computer Science Research, Educationand Applications Press, U.S.A., volume II,pp.
1089?1095.Ant?nio Ribeiro, Gabriel Lopes and Jo?o Mexia(2000b, in press)  Aligning Portuguese and Chi-nese Parallel Texts Using Confidence Bands.
In?Proceedings of the Sixth Pacific Rim InternationalConference on Artificial Intelligence (PRICAI2000) ?
Lecture Notes in Artificial Intelligence?,Springer-Verlag.Joaquim da Silva, Ga?l Dias, Sylvie Guillor?, Jos?Lopes (1999)  Using Localmaxs Algorithms for theExtraction of Contiguous and Non-contiguousMultiword Lexical Units.
In Pedro Barahona andJos?
Alferes, eds., ?Progress in Artificial Intelli-gence ?
Lecture Notes in Artificial Intelligence?,number 1695, Springer-Verlag, Berlin, Germany,pp.
113?132.Michel Simard, George Foster and Pierre Isabelle(1992)  Using Cognates to Align Sentences in Bi-lingual Corpora.
In ?Proceedings of the FourthInternational Conference on Theoretical andMethodological Issues in Machine TranslationTMI-92?, Montreal, Canada, pp.
67?81.Michel Simard and Pierre Plamondon (1998)Bilingual Sentence Alignment: Balancing Robust-ness and Accuracy.
Machine Translation, 13/1,pp.
59?80.Dekai Wu (1994)  Aligning a Parallel English?Chi-nese Corpus Statistically with Lexical Criteria.
In?Proceedings of the 32nd Annual Conference ofthe Association for Computational Linguistics?,Las Cruces, New Mexico, U.S.A., pp.
80?87.Thomas Wonnacott and Ronald Wonnacott (1990)Introductory Statistics.
5th edition, John Wiley &Sons, New York Chichester Brisbane TorontoSingapore, 711 p..
