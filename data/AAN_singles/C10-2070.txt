Coling 2010: Poster Volume, pages 614?622,Beijing, August 2010A Linguistically Grounded Graph Model for Bilingual LexiconExtractionFlorian Laws, Lukas Michelbacher, Beate Dorow, Christian Scheible,Ulrich Heid, Hinrich Schu?tzeInstitute for Natural Language ProcessingUniversita?t Stuttgart{lawsfn,michells,dorowbe}@ims.uni-stuttgart.deAbstractWe present a new method, based ongraph theory, for bilingual lexicon ex-traction without relying on resources withlimited availability like parallel corpora.The graphs we use represent linguis-tic relations between words such as ad-jectival modification.
We experimentwith a number of ways of combiningdifferent linguistic relations and presenta novel method, multi-edge extraction(MEE), that is both modular and scalable.We evaluate MEE on adjectives, verbsand nouns and show that it is superiorto cooccurrence-based extraction (whichdoes not use linguistic analysis).
Finally,we publish a reproducible baseline to es-tablish an evaluation benchmark for bilin-gual lexicon extraction.1 IntroductionMachine-readable translation dictionaries are animportant resource for bilingual tasks like ma-chine translation and cross-language informationretrieval.
A common approach to obtaining bilin-gual translation dictionaries is bilingual lexiconextraction from corpora.
Most work has usedparallel text for this task.
However, parallel cor-pora are only available for few language pairs andfor a small selection of domains (e.g., politics).For other language pairs and domains, monolin-gual comparable corpora and monolingual lan-guage processing tools may be more easily avail-able.
This has prompted researchers to investigatebilingual lexicon extraction based on monolingualcorpora (see Section 2) .In this paper, we present a new graph-theoreticmethod for bilingual lexicon extraction.
Twomonolingual graphs are constructed based on syn-tactic analysis, with words as nodes and relations(such as adjectival modification) as edges.
Eachrelation acts as a similarity source for the nodetypes involved.
All available similarity sourcesinteract to produce one final similarity value foreach pair of nodes.
Using a seed lexicon, nodesfrom the two graphs can be compared to find atranslation.Our main contributions in this paper are: (i) wepresent a new method, based on graph theory,for bilingual lexicon extraction without relyingon resources with limited availability like paral-lel corpora; (ii) we show that with this graph-theoretic framework, information obtained by lin-guistic analysis is superior to cooccurrence dataobtained without linguistic analysis; (iii) we ex-periment with a number of ways of combining dif-ferent linguistic relations in extraction and presenta novel method, multi-edge extraction, which isboth modular and scalable; (iv) progress in bilin-gual lexicon extraction has been hampered by thelack of a common benchmark; we therefore pub-lish a benchmark and the performance of MEE asa baseline for future research.The paper discusses related work in Section 2.We then describe our translation model (Sec-tion 3) and multi-edge extraction (Section 4).
Thebenchmark we publish as part of this paper is de-scribed in Section 5.
Section 6 presents our ex-perimental results and Section 7 analyzes and dis-cusses them.
Section 8 summarizes.2 Related WorkRapp (1999) uses word cooccurrence in a vectorspace model for bilingual lexicon extraction.
De-tails are given in Section 5.Fung and Yee (1998) also use a vector spaceapproach, but use TF/IDF values in the vectorcomponents and experiment with different vec-tor similarity measures for ranking the translationcandidates.
Koehn and Knight (2002) combine614a vector-space approach with other clues such asorthographic similarity and frequency.
They re-port an accuracy of .39 on the 1000 most frequentEnglish-German noun translation pairs.Garera et al (2009) use a vector space modelwith dependency links as dimensions instead ofcooccurring words.
They report outperforminga cooccurrence vector model by 16 percentagepoints accuracy on English-Spanish.Haghighi et al (2008) use a probabilistic modelover word feature vectors containing cooccur-rence and orthographic features.
They then usecanonical correlation analysis to find matchingsbetween words in a common latent space.
Theyevaluate on multiple languages and report highprecision even without a seed lexicon.Most previous work has used vector spaces and(except for Garera et al (2009)) cooccurrencedata.
Our approach uses linguistic relations likesubcategorization, modification and coordinationin a graph-based model.
Further, we evaluate ourapproach on different parts of speech, whereassome previous work only evaluates on nouns.3 Translation ModelOur model has two components: (i) a graph repre-senting words and the relationships between themand (ii) a measure of similarity between wordsbased on these relationships.
Translation is re-garded as cross-lingual word similarity.
We rankwords according to their similarity and choose thetop word as the translation.We employ undirected graphs with typed nodesand edges.
Node types represent parts of speech(POS); edge types represent different kinds of re-lations.
We use a modified version of SimRank(Jeh and Widom, 2002) as a similarity measurefor our experiments (see Section 4 for details).SimRank is based on the idea that two nodesare similar if their neighbors are similar.
We ap-ply this notion of similarity across two graphs.
Wethink of two words as translations if they appearin the same relations with other words that aretranslations of each other.
Figure 1 illustrates thisidea with verbs and nouns in the direct object rela-tion.
Double lines indicate seed translations, i.e.,known translations from a dictionary (see Sec-tion 5).
The nodes buy and kaufen have the samehousemagazinebookthoughtbuyreadHausZeitschriftBuchGedankekaufenlesenFigure 1: Similarity through seed translationsobjects in the two languages; one of these (maga-zine ?
Zeitschrift) is a seed translation.
This re-lationship contributes to the similarity of buy ?kaufen.
Furthermore, book and Buch are similar(because of read ?
lesen) and this similarity willbe added to buy ?
kaufen in a later iteration.
Byrepeatedly applying the algorithm, the initial sim-ilarity introduced by seeds spreads to all nodes.To incorporate more detailed linguistic infor-mation, we introduce typed edges in addition totyped nodes.
Each edge type represents a linguis-tic relation such as verb subcategorization or ad-jectival modification.
By designing a model thatcombines multiple edge types, we can computethe similarity between two words based on mul-tiple sources of similarity.
We superimpose dif-ferent sets of edges on a fixed set of nodes; a nodeis not necessarily part of every relation.The graph model can accommodate any kind ofnodes and relations.
In this paper we use nodesto represent content words (i.e., non-functionwords): adjectives (a), nouns (n) and verbs (v).We extracted three types of syntactic relationsfrom a corpus: see Table 1.Nouns participate in two bipartite relations(amod, dobj) and one unipartite relation (ncrd).This means that the computation of noun similar-ities will benefit from three different sources.Figure 2 depicts a sample graph with all nodeand edge types.
For the sake of simplicity, amonolingual example is shown.
There are fournouns in the sample graph all of which are (i)modified by the adjectives interesting and polit-ical and (ii) direct objects of the verbs like and615relation entities description exampleused in this paperamod a, n adjectival modification a fast cardobj v, n object subcategorization drive a carncrd n, n noun coordination cars and bussesother possible relationsvsub v, n subject subcategorization a man sleepsposs n, n possessive the child?s toyacrd a, a adjective coordination red or blue carTable 1: Relations used in this paper (top) andpossible extensions (bottom).dobjamodncrdverbadjectivenounlike promoteideaarticle bookmagazineinteresting politicalFigure 2: Graph snippet with typed edgespromote.
Based on amod and dobj, the four nounsare equally similar to each other.
However, thegreater similarity of article, book, and magazineto each other can be deduced from the fact thatthese three nouns also occur in the relation ncrd.We exploit this information in the MEE method.Data and Preprocessing.
Our corpus in thispaper is the Wikipedia.
We parse all Germanand English articles with BitPar (Schmid, 2004)to extract verb-argument relations.
We extractadjective-noun modification and noun coordina-tions with part-of-speech patterns based on aversion of the corpus tagged with TreeTagger(Schmid, 1994).
We use lemmas instead of sur-face forms.
Because we perform the SimRankmatrix multiplications in memory, we need to fil-ter out rare words and relations; otherwise, run-ning SimRank to convergence would not be feasi-ble.
For adjective-noun pairs, we apply a filter onpair frequency (?
3).
We process noun pairs byapplying a frequency threshold on words (?
100)and pairs (?
3).
Verb-object pairs (the smallestdata set) were not frequency-filtered.
Based onthe resulting frequency counts, we calculate asso-ciation scores for all relationships using the log-likelihood measure (Dunning, 1993).
For nounpairs, we discard all pairs with an associationscore < 3.84 (significance at ?
= .05).
For allthree relations, we discard pairs whose observedfrequency was smaller than their expected fre-quency (Evert, 2004, p. 76).
As a last step,we further reduce noise by removing nodes of de-gree 1.
Key statistics for the resulting graphs aregiven in Table 2.We have found that accuracy of extraction ispoor if unweighted edges are used.
Using thelog-likelihood score directly as edge weight givestoo much weight to ?semantically weak?
high-frequency words like put and take.
We there-fore use the logarithms of the log-likelihood scoreas edge weights in all SimRank computations re-ported in this paper.nodes n a vde 34,545 10,067 2,828en 22,257 12,878 4,866edges ncrd amod dobjde 65,299 417,151 143,906en 288,889 686,073 510,351Table 2: Node and edge statistics4 SimRankOur work is based on the SimRank graph similar-ity algorithm (Jeh and Widom, 2002).
In (Dorowet al, 2009), we proposed a formulation of Sim-Rank in terms of matrix operations, which can beapplied to (i) weighted graphs and (ii) bilingualproblems.
We now briefly review SimRank andits bilingual extension.
For more details we referto (Dorow et al, 2009).The basic idea of SimRank is to consider twonodes as similar if they have similar neighbor-hoods.
Node similarity scores are recursivelycomputed from the scores of neighboring nodes:the similarity Sij of two nodes i and j is computed616as the normalized sum of the pairwise similaritiesof their neighbors:Sij =c|N(i)| |N(j)|?k?N(i),l?N(j)Skl.where N(i) and N(j) are the sets of i?s and j?sneighbors.
As the basis of the recursion, Sij is setto 1 if i and j are identical (self-similarity).
Theconstant c (0 < c < 1) dampens the contributionof nodes further away.
Following Jeh and Widom(2002), we use c = 0.8.
This calculation is re-peated until, after a few iterations, the similarityvalues converge.For bilingual problems, we adapt SimRank forcomparison of nodes across two graphs A and B.In this case, i is a node in A and j is a node in B,and the recursion basis is changed to S(i, j) = 1 ifi and j are a pair in a predefined set of node-nodeequivalences (seed translation pairs).Sij =c|NA(i)| |NB(j)|?k?NA(i),l?NB(j)Skl.Multi-edge Extraction (MEE) Algorithm Tocombine different information sources, corre-sponding to edges of different types, in one Sim-Rank computation, we use multi-edge extrac-tion (MEE), a variant of SimRank (Dorow et al,2009).
It computes an aggregate similarity matrixafter each iteration by taking the average similar-ity value over all edge types T :Sij =c|T |?t?T1f(|NA,t(i)|)f(|NB,t(j)|)?k?NA,t(i),l?NB,t(j)Skl.f is a normalization function (either f = g,g(n) = n as before or the normalization discussedin the next section).While we have only reviewed the case of un-weighted graphs, the extended SimRank can alsobe applied to weighted graphs.
(See (Dorow etal., 2009) for details.)
In what follows, all graphcomputations are weighted.Square Root Normalization Preliminary ex-periments showed that SimRank gave too muchinfluence to words with few neighbors.
We there-fore modified the normalization function g(n) =n.
To favor words with more neighbors, we wantf to grow sublinearly with the number of neigh-bors.
On the other hand, it is important that,even for nodes with a large number of neigh-bors, the normalization term is not much smallerthan |N(i)|, otherwise the similarity computationdoes not converge.
We use the function h(n) =?n??maxk(|N(k)|).
h grows quickly for smallnode degrees, while returning values close to thelinear term for large node degrees.
This guaran-tees that nodes with small degrees have less influ-ence on final similarity scores.
In all experimentsreported in this paper, the matrices A?, B?
are nor-malized with f = h (rather than using the stan-dard normalization f = g).
In one experiment,accuracy of the top-ranked candidate (acc@1) was.52 for h and .03 for g, demonstrating that thestandard normalization does not work in our ap-plication.Threshold Sieving For larger experiments,there is a limit to scalability, as the similarity ma-trix fills up with many small entries, which take upa large amount of memory.
Since these small en-tries contribute little to the final result, Lizorkin etal.
(2008) proposed threshold sieving: an approxi-mation of SimRank using less space by deletingall similarity values that are below a threshold.The quality of the approximation is set by a pa-rameter ?
that specifies maximum acceptable dif-ference of threshold-sieved similarity and the ex-act solution.
We adapted this to the matrix formu-lation by integrating the thresholding step into astandard sparse matrix multiplication algorithm.We verified that this approximation yields use-ful results by comparing the ranks of exact and ap-proximate solutions.
We found that for the high-ranked words that are of interest in our task, siev-ing with a suitable threshold does not negativelyaffect results.5 Benchmark Data SetRapp?s (1999) original experiment was carried outon newswire corpora and a proprietary Collinsdictionary.
We use the free German (280M to-kens) and English (850M tokens) Wikipedias assource and target corpora.
Reinhard Rapp hasgenerously provided us with his 100 word test set617n a vtraining set .61 .31 .08TS100 .65 .28 .07TS1000 .66 .14 .20Table 3: Percentages of POS in test and training(TS100) and given us permission to redistributeit.
Additionally, we constructed a larger test set(TS1000) consisting of the 1000 most frequentwords from the English Wikipedia.
Unlike thenoun-only test sets used in other studies, (e.g.,Koehn and Knight (2002), Haghighi et al (2008)),TS1000 also contains adjectives and verbs.
Asseed translations, we use a subset of the dict.cconline dictionary.
For the creation of the sub-set we took raw word frequencies from Wikipediaas a basis.
We extracted all verb, noun and ad-jective translation pairs from the original dictio-nary and kept the pairs whose components wereamong the 5,000 most frequent nouns, the 3,500most frequent adjectives and the 500 most fre-quent verbs for each language.
These numbers arebased on percentages of the different node typesin the graphs.
The resulting dictionary contains12,630 pairs: 7,767 noun, 3,913 adjective and 950verb pairs.
Table 3 shows the POS composition ofthe training set and the two test sets.
For experi-ments evaluated on TS100 (resp.
TS1000), the setof 100 (resp.
1000) English words it contains andall their German translations are removed from theseed dictionary.Baseline.
Our baseline is a reimplementationof the vector-space method of Rapp (1999).
Eachword in the source corpus is represented as a wordvector, the dimensions of which are words of seedtranslation pairs.
The same is done for corpuswords in the target language, using the translatedseed words as dimensions.
The value of each di-mension is determined by association statistics ofword cooccurrence.
For a test word, a vector isconstructed in the same way.
The labels on thedimensions are then translated, yielding an inputvector in the target language vector space.
Wethen find the closest corpus word vector in the tar-get language vector space using the city block dis-tance measure.
This word is taken as the transla-tion of the test word.We went to great lengths to implement Rapp?smethod, but omit the details for reasons of space.Using the Wikipedia/dict.cc-based data set, weachieve 50% acc@1 when translating words fromEnglish to German.
While this is somewhat lowerthan the performance reported by Rapp, we be-lieve this is due to Wikipedia being more hetero-geneous and less comparable than news corporafrom identical time periods used by Rapp.Publication.
In conjunction with this paper wepublish the benchmark for bilingual lexicon ex-traction described.
It consists of (i) two Wikipediadumps from October 2008 and the linguistic re-lations extracted from them, (ii) scripts to recre-ate the training and test sets from the dict.ccdata base, (iii) the TS100 and TS1000 test sets,and (iv) performance numbers of Rapp?s systemand MEE.
These can serve as baselines for fu-ture work.
Note that (ii)?
(iv) can be used in-dependently of (i) ?
but in that case the effectof the corpus on performance would not be con-trolled.
The data and scripts are available athttp://ifnlp.org/wiki/extern/WordGraph6 ResultsIn addition to the vector space baseline experi-ment described above, we conducted experimentswith the SimRank model.
Because TS100 onlycontains one translation per word, but words canhave more than one valid translation, we manu-ally extended the test set with other translations,which we verified using dict.cc and leo.org.
Wereport the results separately for the original test set(?strict?)
and the extended test set in Table 4.
Wealso experimented with single-edge models con-sisting of three separate runs on each relation.The accuracy columns report the percentage oftest cases where the correct translation was foundamong the top 1 (acc@1) or top 10 (acc@10)candidate words found by the translation mod-els.
Some test words are not present in the data atall; we count these as 0s when computing acc@1and acc@10.
The acc@10 measure is more use-ful for indicating topical similarity while acc@1measures translation accuracy.MRR is Mean Reciprocal Rank of correct trans-lations: 1n?ni1ranki (Voorhees and Tice, 1999).MRR is a more fine-grained measure than acc@n,618TS100, strict TS100, extended TS1000acc@1 acc@10 MRR acc@1 acc@10 MRR acc@1 acc@10 MRRbaseline .50 .67 .56 .54 .70 .60 .33 .56 .41single .44 .67 .52 .49 .68 .56 .40?
.70?
.50MEE .52 .79?
.62 .58 .82?
.68 .48?
.76?
.58Table 4: Results compared to baseline?e.g., it will distinguish ranks 2 and 10.
All MRRnumbers reported in this paper are consistent withacc@1/acc@10 and support our conclusions.The results for acc@1, the measure that mostdirectly corresponds to utility in lexicon extrac-tion, show that the SimRank-based models out-perform the vector space baseline ?
only slightlyon TS100, but significantly on TS1000.
Using thevarious relations separately (single) already yieldsa significant improvement compared to the base-line.
Using all relations in the integrated MEEmodel further improves accuracy.
With an acc@1score of 0.48, MEE outperforms the baseline by.15 compared to TS1000.
This shows that a com-bination of several sources of information is veryvaluable for finding the correct translation.MEE outperforms the baseline on TS1000 forall parts of speech, but performs especially wellcompared to the baseline for adjectives and verbs(see Table 5).
It has been suggested that vectorspace models perform best for nouns and poorlyfor other parts of speech.
Our experiments seem toconfirm this.
In contrast, MEE exhibits good per-formance for nouns and adjectives and a markedimprovement for verbs.On acc@10, MEE is consistently better than thebaseline, on both TS100 and TS1000.
All threedifferences are statistically significant.6.1 Relation ComparisonTable 5 compares baseline, single-edge and MEEaccuracy for the three parts of speech covered.Each single-edge experiment can compute nounsimilarity; for adjectives and verbs, only amod,dobj and MEE can be used.Performance for nouns varies greatly depend-ing on the relation used in the model.
ncrd per-?We indicate statistical significance at the ?
= 0.05 (?
)and 0.01 level (?)
when compared to the baseline.
We didnot calculate significance for MRR.forms best, while dobj shows the worst perfor-mance.
We hypothesize that dobj performs badlybecause (i) many verbs are semantically non-restrictive with respect to their arguments, (e.g.,use, contain or include) and as a result seman-tically unrelated nouns become similar becausethey share the same verb as a neighbor; (ii) lightverb constructions (e.g., take a walk or give an ac-count) dilute the extracted relations; and (iii) dobjis the only relation we extracted with a syntac-tic parser.
The parser was trained on newswiretext, a genre that is very different from Wikipedia.Hence, parsing is less robust than the relativelystraightforward POS patterns used for the otherrelations.Similarly, many semantically non-restrictiveadjectives such as first and new can modify vir-tually any noun, diluting the quality of the amodsource.
We conjecture that ncrd exhibits the bestperformance because there are fewer semanticallynon-restrictive nouns than non-restrictive adjec-tives and verbs.MEE performance for nouns (.45) is signifi-cantly better than that of the single-edge models.The information about nouns that is contained inthe verb-object and adjective-noun data is inte-grated in the model and helps select better trans-lations.
This, however, is only true for the nounnoun adj verb allTS100 baseline .55 .43 .29 .50amod .15 .71 - .30ncrd .34 - - .22dobj .02 - .43 .04MEE .45 .71 .43 .52TS1000 baseline .42 .26 .18 .33MEE .53 .55 .27 .48Table 5: Relation comparison, acc@1619source acc@1 acc@10dobj .02 .10amod .15 .37amod+dobj .22 .43ncrd+dobj .32 .65ncrd .34 .60ncrd+amod .49 .74MEE .45 .77Table 6: Accuracy of sources for nounsnode type, the ?pivot?
node type that takes part inedges of all three types.
For adjectives and verbs,the performance of MEE is the same as that of thecorresponding single-edge model.We ran three additional experiments each ofwhich combines only two of the three possiblesources for noun similarity, namely ncrd+amod,ncrd+dobj and amod+dobj and performed strictevaluation (see Table 6).
We found that in gen-eral combination increases performance exceptfor ncrd+dobj vs. ncrd.
We attribute this to thelack of robustness of dobj mentioned above.6.2 Comparison MEE vs. All-in-oneAn alternative to MEE is to use untyped edges inone large graph.
In this all-in-one model (AIO),we connect two nodes with an edge if they arelinked by any of the different linguistic relations.While MEE consists of small adjacency matricesfor each type, the two adjacency matrices for AIOare much larger.
This leads to a much denser sim-ilarity matrix taking up considerably more mem-ory.
One reason for this is that AIO contains simi-larity entries between words of different parts ofspeech that are 0 (and require no memory in asparse matrix representation) in MEE.Since AIO requires more memory, we had tofilter the data much more strictly than before to beable to run an experiment.
We applied the follow-ing stricter thresholds on relationships to obtaina small graph: 5 instead of 3 for adjective-nounMEEsmall AIOsmallacc@1 .51 .52acc@10 .72 .75MRR .62 .59Table 7: MEE vs. AIOpairs, and 3 instead of 0 for verb-object pairs,thereby reducing the total number of edges from2.1M to 1.4M.
We also applied threshold sieving(see Section 4) with ?
= 10?10 for AIO.
The re-sults on TS100 (strict evaluation) are reported inTable 7.
For comparison, MEE was also run onthe smaller graph.
Performance of the two modelsis very similar, with AIO being slightly better (notsignificant).
The slight improvement does not jus-tify the increased memory requirements.
MEE isable to scale to more nodes and edge types, whichallows for better coverage and performance.7 Analysis and DiscussionError analysis.
We examined the cases where areference translation was not at the top of the sug-gested list of translation candidates.
There are anumber of elements in the translation process thatcan cause or contribute to this behavior.Our method sometimes picks a cohyponym ofthe correct translation.
In many of these cases, thecorrect translation is in the top 10 (together withother words from the same semantic field).
Forexample, the correct translation of moon, Mond, issecond in a list of words belonging to the semanticfield of celestial phenomena: Komet (comet), Mond(moon), Planet (planet), Asteroid (asteroid), Stern (star),Galaxis (galaxy), Sonne (sun), .
.
.
While this behavioris undesirable for strict lexicon extraction, it canbe exploited for other tasks, e.g.
cross-lingual se-mantic relatedness (Michelbacher et al, 2010).Similarly, the method sometimes puts theantonym of the correct translation in first place.For example, the translation for swift (schnell) isin second place behind langsam (slow).
Basedon the syntactic relations we use, it is difficult todiscriminate between antonyms and semanticallysimilar words if their syntactic distributions aresimilar.Ambiguous source words also pose a problemfor the system.
The correct translation of square(the geometric shape) is Quadrat.
However, 8 outof its top 10 translation candidates are related tothe location sense of square.
The other two are ge-ometric shapes, Quadrat being listed second.
Thisis only a concern for strict evaluation, since cor-rect translations of a different sense were includedin the extended test set.620bed is also ambiguous (piece of furniture vs.river bed).
This introduces translation candidatesfrom the geographical domain.
As an additionalsource of errors, a number of bed?s neighborsfrom the furniture sense have the German transla-tion Bank which is ambiguous between the furni-ture sense and the financial sense.
This ambiguityin the target language German introduces spurioustranslation candidates from the financial domain.Discussion.
The error analysis demonstratesthat most of the erroneous translations are wordsthat are incorrect, but that are related, in some ob-vious way, to the correct translation, e.g.
by co-hyponymy or antonymy.
This suggests anotherapplication for bilingual lexicon extraction.
Oneof the main challenges facing statistical machinetranslation (SMT) today is that it is difficult todistinguish between minor errors (e.g., incorrectword order) and major errors that are completelyimplausible and undermine the users?
confidencein the machine translation system.
For example,at some point Google translated ?sarkozy sarkozysarkozy?
into ?Blair defends Bush?.
Since bilin-gual lexicon extraction, when it makes mistakes,extracts closely related words that a human usercan understand, automatically extracted lexiconscould be used to discriminate smaller errors fromgrave errors in SMT.As we discussed earlier, parallel text is notavailable in sufficient quantity or for all impor-tant genres for many language pairs.
The methodwe have described here can be used in such cases,provided that large monolingual corpora and ba-sic linguistic processing tools (e.g.
POS tagging)are available.
The availability of parsers is a morestringent constraint, but our results suggest thatmore basic NLP methods may be sufficient forbilingual lexicon extraction.In this work, we have used a set of seed trans-lations (unlike e.g., Haghighi et al (2008)).
Webelieve that in most real-world scenarios, whenaccuracy and reliability are important, seed lexicawill be available.
In fact, seed translations can beeasily found for many language pairs on the web.Although a purely unsupervised approach is per-haps more interesting from an algorithmic pointof view, the semisupervised approach taken in thispaper may be more realistic for applications.In this paper, we have attempted to reimplementRapp?s system as a baseline, but have otherwiserefrained from detailed comparison with previouswork as far as the accuracy of results is concerned.The reason is that none of the results published sofar are easily reproducible.
While previous publi-cations have tried to infer from differences in per-formance numbers that one system is better thananother, these comparisons have to be viewed withcaution since neither the corpora nor the gold stan-dard translations are the same.
For example, thepaper by Haghighi et al (2008) (which demon-strates how orthography and contextual informa-tion can be successfully used) reports 61.7% ac-curacy on the 186 most confident predictions ofnouns.
But since the evaluation data sets are notpublicly available it is difficult to compare otherwork (including our own) with this baseline.
Wesimply do not know how methods published so farstack up against each other.For this reason, we believe that a benchmarkis necessary to make progress in the area of bilin-gual lexicon extraction; and that our publication ofsuch a benchmark as part of the research reportedhere is an important contribution, in addition tothe linguistically grounded extraction and the newgraph-theoretical method we present.8 SummaryWe have presented a new method, based on graphtheory, for bilingual lexicon extraction without re-lying on resources with limited availability likeparallel corpora.
We have shown that with thisgraph-theoretic framework, information obtainedby linguistic analysis is superior to cooccurrencedata obtained without linguistic analysis.
We havepresented multi-edge extraction (MEE), a scalablegraph algorithm that combines different linguis-tic relations in a modular way.
Finally, progressin bilingual lexicon extraction has been hamperedby the lack of a common benchmark.
We publishsuch a benchmark with this paper and the perfor-mance of MEE as a baseline for future research.9 AcknowledgementThis research was funded by the German Re-search Foundation (DFG) within the project Agraph-theoretic approach to lexicon acquisition.621ReferencesDorow, Beate, Florian Laws, Lukas Michelbacher,Christian Scheible, and Jason Utt.
2009.
A graph-theoretic algorithm for automatic extension of trans-lation lexicons.
In EACL 2009 Workshop on Geo-metrical Models of Natural Language Semantics.Dunning, Ted.
1993.
Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19(1):61?74.Evert, Stefan.
2004.
The Statistics of Word Cooccur-rences - Word Pairs and Collocations.
Ph.D. thesis,Institut fu?r maschinelle Sprachverarbeitung (IMS),Universita?t Stuttgart.Fung, Pascale and Lo Yuen Yee.
1998.
An IR ap-proach for translating new words from nonparallel,comparable texts.
In COLING-ACL, pages 414?420.Garera, Nikesh, Chris Callison-Burch, and DavidYarowsky.
2009.
Improving translation lexiconinduction from monolingual corpora via depen-dency contexts and part-of-speech equivalences.
InCoNLL ?09: Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learn-ing, pages 129?137, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Haghighi, Aria, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL-08: HLT, pages 771?779, Columbus, Ohio, June.Association for Computational Linguistics.Jeh, Glen and Jennifer Widom.
2002.
Simrank: Ameasure of structural-context similarity.
In KDD?02, pages 538?543.Koehn, Philipp and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InProceedings of the ACL-02 Workshop on Unsuper-vised Lexical Acquisition, pages 9?16.Lizorkin, Dmitry, Pavel Velikhov, Maxim N. Grinev,and Denis Turdakov.
2008.
Accuracy estimate andoptimization techniques for simrank computation.PVLDB, 1(1):422?433.Michelbacher, Lukas, Florian Laws, Beate Dorow, Ul-rich Heid, and Hinrich Schu?tze.
2010.
Buildinga cross-lingual relatedness thesaurus using a graphsimilarity measure.
In Proceedings of the Seventhconference on International Language Resourcesand Evaluation (LREC?10), Valletta, Malta, may.Rapp, Reinhard.
1999.
Automatic identification ofword translations from unrelated English and Ger-man corpora.
In COLING 1999.Schmid, Helmut.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing, pages 44?49.Schmid, Helmut.
2004.
Efficient parsing of highlyambiguous context-free grammars with bit vectors.In COLING ?04, page 162.Voorhees, Ellen M. and Dawn M. Tice.
1999.
TheTREC-8 question answering track evaluation.
InProceedings of the 8th Text Retrieval Conference.622
