Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 795?803,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsConvolution Kernels for Opinion Holder ExtractionMichael Wiegand and Dietrich KlakowSpoken Language SystemsSaarland UniversityD-66123 Saarbru?cken, Germany{Michael.Wiegand|Dietrich.Klakow}@lsv.uni-saarland.deAbstractOpinion holder extraction is one of the impor-tant subtasks in sentiment analysis.
The ef-fective detection of an opinion holder dependson the consideration of various cues on vari-ous levels of representation, though they arehard to formulate explicitly as features.
In thiswork, we propose to use convolution kernelsfor that task which identify meaningful frag-ments of sequences or trees by themselves.We not only investigate how different levelsof information can be effectively combinedin different kernels but also examine how thescope of these kernels should be chosen.
Ingeneral relation extraction, the two candidateentities thought to be involved in a relation arecommonly chosen to be the boundaries of se-quences and trees.
The definition of bound-aries in opinion holder extraction, however, isless straightforward since there might be sev-eral expressions beside the candidate opinionholder to be eligible for being a boundary.1 IntroductionIn recent years, there has been a growing interestin the automatic detection of opinionated contentin natural language text.
One of the more impor-tant tasks in sentiment analysis is the extraction ofopinion holders.
Opinion holder extraction is oneof the critical components of an opinion question-answering system (i.e.
systems which automaticallyanswer opinion questions, such as ?What does [X]like about [Y]??).
Such systems need to be able todistinguish which entities in a candidate answer sen-tence are the sources of opinions (= opinion holder)and which are the targets.On other NLP tasks, in particular, on relation extrac-tion, there has been much work on convolution ker-nels, i.e.
kernel functions exploiting huge amountsof features without an explicit feature representa-tion.
Previous research on that task has shown thatconvolution kernels, such as sequence and tree ker-nels, are quite effective when compared to manualfeature engineering (Moschitti, 2008; Bunescu andMooney, 2005; Nguyen et al, 2009).
In order toeffectively use convolution kernels, it is often nec-essary to choose appropriate substructures of a sen-tence rather than represent the sentence as a wholestructure (Bunescu and Mooney, 2005; Zhang et al,2006; Moschitti, 2008).
As for tree kernels, for ex-ample, one typically chooses the syntactic subtreeimmediately enclosing two entities potentially ex-pressing a specific relation in a given sentence.
Theopinion holder detection task is different from thisscenario.
There can be several cues within a sen-tence to indicate the presence of a genuine opinionholder and these cues need not be member of a par-ticular word group, e.g.
they can be opinion words(see Sentences 1-3), communication words, such asmaintained in Sentence 2, or other lexical cues, suchas according in Sentence 3.1.
The U.S. commanders consideropinion the prisoners to be un-lawful combatantsopinion as opposed to prisoners of war.2.
During the summit, Koizumi maintainedcommunication aclear-cut collaborative stanceopinion towards the U.S. and em-phasized that the President was objectiveopinion and circum-spect.3.
Accordingcue to Fernandez, it was the worst mistakeopinion inthe history of the Argentine economy.795Thus, the definition of boundaries of the structuresfor the convolution kernels is less straightforward inopinion holder extraction.The aim of this paper is to explore in how far convo-lution kernels can be beneficial for effective opinionholder detection.
We are not only interested in howfar different kernel types contribute to this extractiontask but we also contrast the performance of thesekernels with a manually designed feature set usedas a standard vector kernel.
Finally, we also exam-ine the effectiveness of expanding word sequencesor syntactic trees by additional prior knowledge.2 Related WorkChoi et al (2005) examine opinion holder extractionusing CRFs with various manually defined linguis-tic features and patterns automatically learnt by theAutoSlog system (Riloff, 1996).
The linguistic fea-tures focus on named-entity information and syntac-tic relations to opinion words.
In this paper, we usevery similar settings.
The features presented in Kimand Hovy (2005) and Bloom et al (2007) resemblevery much Choi et al (2005).
Bloom et al (2007)also consider communication words to be predictivecues for opinion holders.Kim and Hovy (2006) and Bethard et al (2005) ex-plore the usefulness of semantic roles provided byFrameNet (Fillmore et al, 2003) for both opinionholder and opinion target extraction.
Due to datasparseness, Kim and Hovy (2006) expand FrameNetdata by using an unsupervised clustering algorithm.Choi et al (2006) is an extension of Choi et al(2005) in that opinion holder extraction is learntjointly with opinion detection.
This requires thatopinion expressions and their relations to opinionholders are annotated in the training data.
Seman-tic roles are also taken as a potential source of in-formation.
In our work, we deliberately work withminimal annotation and, thus, do not consider anylabeled opinion expressions and relations to opinionholders in the training data.
We exclusively rely onentities marked as opinion holders.
In many practi-cal situations, the annotation beyond opinion holderlabeling is too expensive.Complex convolution kernels have been success-fully applied to various NLP tasks, such as rela-tion extraction (Bunescu and Mooney, 2005; Zhanget al, 2006; Nguyen et al, 2009), question an-swering (Zhang and Lee, 2003; Moschitti, 2008),and semantic role labeling (Moschitti et al, 2008).In all these tasks, they offer competitive perfor-mance to manually designed feature sets.
Bunescuand Mooney (2005) combine different sequence ker-nels encoding different contexts of candidate en-tities in a sentence.
They argue that several ker-nels encoding different contexts are more effectivethan just using one kernel with one specific context.We build on that idea and compare various scopeseligible for opinion holder extraction.
Moschitti(2008) and Nguyen et al (2009) suggest that differ-ent kinds of information, such as word sequences,part-of-speech tags, syntactic and semantic informa-tion should be contained in separate convolution ker-nels.
We also adhere to this notion.3 DataAs labeled data, we use the sentiment annotation ofthe MPQA 2.0 corpus1.
Opinion holders are not ex-plicitly labeled as such.
However sources of pri-vate states and subjective speech events (Wiebe etal., 2003) are a fairly good approximation of thetask.
Previous work (Choi et al, 2005; Kim andHovy, 2005; Choi et al, 2006) uses similar approxi-mations.4 MethodIn this work, we consider all noun phrases (NPs)as possible candidate opinion holders.
Therefore,the set of all data instances is the set of the NPswithin the MPQA 2.0 corpus.
Each NP is labeledas to whether it is a genuine opinion holder or not.Throughout this section, we will use Sentence 2from Section 1 as an example.4.1 The Different Levels of RepresentationSeveral levels of representation are important foropinion holder extraction.
Table 1 lists all the dif-ferent levels that are used in this work.
Generalizedsequences employ named-entity tags, an OPINIONtag for opinion words and a COMM tag for com-munication words2.
Thus, in a generalized word se-1www.cs.pitt.edu/mpqa/databaserelease2Note that all candidate tokens are reduced to one genericCAND token.
Thus, we hope to account for data sparseness in796quence (WRDGN ) a word is replaced by a general-ized token whereas in a generalized part-of-speechsequence (POSGN ) a part-of-speech tag is replaced.For augmented constituent trees (CONSTAUG), thesame sources of information are used.
The differ-ence to generalizing sequences is that instead of re-placing words by generalized tokens, we add a nodein the syntax tree with a generalized token so that itdominates the pertaining leaf node (see also nodesmarked with AUG in Figure 2).
All sources used forthis type of generalization are known to be predictivefor opinion holder classification (Choi et al, 2005;Kim and Hovy, 2005; Choi et al, 2006; Kim andHovy, 2006; Bloom et al, 2007).Note that the grammatical relation paths, i.e.GRAMWRD and GRAMPOS , can only be appliedin case there is another expression in the focus inaddition to the candidate of the data instance itself,e.g.
the nearest opinion expression to the candidate.Section 4.4 explains in detail how this is done.Predicate-argument structures (PAS) are repre-sented by PropBank trees (Kingsbury and Palmer,2002).4.2 Support Vector Machines and KernelMethodsSupport Vector Machines (SVMs) are one of themost robust supervised machine learning techniquesin which training data instances ~x are separated by ahyperplane H(~x) = ~w ?
~x + b = 0 where w ?
Rnand b ?
R. One advantage of SVMs is that ker-nel methods can be applied which map the data toother feature spaces in which they can be separatedmore easily.
Given a feature function ?
: O ?
R,where O is the set of the objects, the kernel trickallows the decision hyperplane to be rewritten as:H(~x) =(?i=1...lyi?i~xi)?
~x + b =?i=1...lyi?i~xi ?
~x+ b =?i=1...lyi?i?
(oi) ?
?
(o) + bwhere yi is equal to 1 for positive and ?1 fornegative examples, ?i ?
R with ?i ?
0, oi?i ?
{1, .
.
.
, l} are the training instances and the productK(oi, o) = ??
(oi) ?
?(o)?
is the kernel function as-sociated with the mapping ?.case there are several tokens making up the candidate.4.3 Sequence and Tree KernelsA sequence kernel (SK) measures the similarityof two sequences by counting the number of com-mon subsequences.
We use the kernel by Taylorand Christianini (2004) which has the advantage thatit also considers subsequences of the original se-quence with some elements missing.
The extent ofthese gaps in a sequence is suitably reflected by aweighting function incorporated into the kernel.Tree kernels (TKs) represent trees by their sub-structures.
The feature space of these substructures,or fragments, is mapped onto a vector space.
Thekernel function computes the similarity of pairs oftrees by counting the number of common fragments.In this work, we evaluate two tree kernels: SubsetTree Kernel (STK) (Collins and Duffy, 2002) andPartial Tree Kernel (PTKbasic) (Moschitti, 2006).In STK , a tree fragment can be any set of nodesand edges of the original tree provided that everynode has either all or none of its children.
This con-straint makes that kind of kernel well-suited for con-stituency trees which have been generated by con-text free grammars since the constraint correspondsto the restriction that no grammatical rule must bebroken.
For example, STK enforces that a subtree,such as [VP [VBZ, NP]], cannot be matched with[VP [VBZ]] since the latter VP node only possessesone of the children of the former.PTKbasic is more flexible since the constraintof STK on nodes is relaxed.
This makes thistype of tree kernel less suitable for constituencytrees.
We, therefore, apply it only to treesrepresenting predicate-argument structures (PAS)(see Figure 1).
Note that a data instance isrepresented by a set of those structures3 ratherthan a single structure.
Thus, the actual partialtree kernel function we use for this task, PTK ,sums over all possible pairs PASl and PASm oftwo data instances xi and xj: PTK(xi, xj) =?PASl?xi?PASm?xjPTKbasic(PASl, PASm).To summarize, Table 2 lists the different kerneltypes we use coupled with the suitable levels of rep-resentation.
This choice of pairing has already beenmotivated and empirically proven suitable on other3i.e.
all predicate-argument structures of a sentence in whichthe head of the candidate opinion holder occurs797Type Description ExampleWRD sequence of words During the summit , KoizumiCAND maintained a clear-cutcollaborative stance .
.
.WRDGN sequence of generalized words During the summit , CAND COMM OPINION .
.
.POS part-of-speech sequence IN DET NN PUNC CAND VBD DET JJ JJ NN .
.
.POSGN generalized part-of-speech sequence IN DET NN PUNC CAND COMM OPINION .
.
.CONST constituency tree see Figure 2 without nodes marked AUGCONSTAUG augmented constituency tree see Figure 2GRAMWRD grammatical relation path labels with words KoizumiCAND NSUBJ?
maintained DOBJ?
stanceGRAMPOS grammatical relation path labels with part-of-speech tags CAND NSUBJ?
VBD DOBJ?
NNPAS predicate argument structures see Figure 1(a)PASAUG augmented predicate argument structures see Figure 1(b)Table 1: The different levels of representation.
(a) plain(b) augmentedFigure 1: Predicate-argument structures (PAS).tasks (Moschitti, 2008; Nguyen et al, 2009).Type Description Levels of RepresentationSK Sequential Kernel WRD(GN) , POS(GN),GRAMWRD , GRAMPOSSTK Subset Tree Kernel CONST(AUG)PTK Partial Tree Kernel PASV K Vector Kernel not restrictedTable 2: The different types of kernels.4.4 The Different ScopesWe argue that using the entire word sequence or syn-tax tree of the sentence in which a candidate opinionholder is situated to represent a data instance pro-duces too large structures for a convolution kernel.Since a classifier based on convolution kernels hasto derive meaningful features by itself, the largerthese structures are, the more likely noise is includedin the model.
Previous work in relation extractionhas also shown that the usage of more focused sub-structures, e.g.
the smallest subtree containing thetwo candidate entities of a relation, is more effec-tive (Zhang et al, 2006).
Unfortunately, in our taskthere is only one explicit entity we know of for eachdata instance which is the candidate opinion holder.However, there are several indicative cues within thecontext of the candidate which might be consideredimportant.
We identify three different cues being thenearest predicate, i.e.
full verb or nominalization,opinion word and communication word4.
For eachof these expressions, we define a scope where theboundaries are the candidate opinion holder and thepertaining cue.
Given these scopes, we can defineresulting subsequences/subtrees and combine them.We further add two background scopes, one beingthe semantic scope of the candidate opinion holderand the entire sentence.
As semantic scope we con-sider the subclause in which a candidate opinionholder is situated5 .Figure 2 illustrates the different scopes.
Abbre-viations are explained in Table 3.
As already men-tioned in Section 4.1 for grammatical relation paths,a second expression in addition to the candidateopinion holder is required.
These expressions can bederived from the different scopes, i.e.
for PRED it4These three expressions may coincide but do not have to.5Typically, the subtree representing a subclause has the clos-est S node dominating the candidate opinion holder as the rootnode and it contains only those nodes from the original sentenceparse which are also dominated by that S node and whose pathto that node does not contain another S node.798is the nearest predicate to the candidate, for OP it isthe nearest opinion word and for COMM it is thenearest communication word.
For the backgroundscopes SEM and SENT , however, there is no sec-ond expression in focus.
Therefore, grammatical re-lation paths cannot be defined for these scopes.Type DescriptionPRED scope with the boundaries being the candidate opinionholder and the nearest predicateOP scope with the boundaries being the candidate opinionholder and nearest opinion wordCOMM scope with the boundaries being the candidate opinionholder and the nearest communication wordSEM semantic scope of the candidate opinion holder, i.e.subclause containing the candidateSENT entire sentence in which in the opinion holder occursTable 3: The different types of scope.4.5 Manually Designed Feature Set for aStandard Vector KernelIn addition to the different types of convolution ker-nels, we also define an explicit feature set for a vec-tor kernel (V K).
Many of these features mainly de-scribe properties of the relation between the candi-date and the nearest predicate6 since in our initialexperiments the nearest predicate has always beenthe strongest cue.
Adding these types of featuresfor other cues, e.g.
the nearest opinion or commu-nication word, only resulted in a decrease in perfor-mance.
Table 4 lists all the features we use.
Notethat this manual feature set employs all those sourcesof information which are also exploited by the con-volution kernels.
Some of the information containedin the convolution kernels can, however, only be rep-resented in a more simplified fashion when usinga manual feature set.
For example, the first PASin Figure 1(a) is converted to just the pair of pred-icate and argument representing the candidate (i.e.REL:maintain A0:Koizumi).
The entire PAS is notused since it would create too sparse features.
Con-volution kernels can cope with fairly complex struc-tures as input since they internally match substruc-tures.
Manual features are less flexible since they donot account for partial matches.6We select the nearest predicate by using the syntactic parsetree.
Thus, we hope to select the predicate which syntacticallyheadword/governing category of CANDis CAND capitalized/a person?is CAND subj|dobj|iobj|pobj of OPINION/COMM?is CAND preceded by according to?
(Choi et al, 2005)does CAND contain possessive and is followed by OPIN-ION/COMM?
(Choi et al, 2005)is CAND preceded by by which is attached to OPINION/COMM?
(Choi et al, 2005)predicate-argument pairs in which CAND occurslemma/part-of-speech tag/subcategorization frame/voice of nearestpredicateis nearest predicate OPINION/COMM?does CAND precede/follow nearest predicate?words between nearest predicate and CAND (bag of words)part-of-speech sequence between nearest predicate and CANDconstituency path/grammatical relation path from predicate toCANDTable 4: Manually designed feature set.5 ExperimentsWe used 400 documents of the MPQA corpus forfive-fold crossvalidation and 133 documents as a de-velopment set.
We report statistical significance onthe basis of a paired t-test using 0.05 as the signif-icance level.
All experiments were done with theSVM-Light-TK toolkit7.
We evaluated on the basisof exact phrase matching.
We set the trade-off pa-rameter j = 5 for all feature sets.
For the manualfeature set we used a polynomial kernel of third de-gree.
These two critical parameters were tuned onthe development set.
As far as the sequence andtree kernels are concerned, we used the parametersettings from Moschitti (2008), i.e.
?
= 0.4 and?
= 0.4.
Kernels were combined using plain sum-mation.
The documents were parsed using the Stan-ford Parser (Klein and Manning, 2003).
Named-entity information was obtained by the Stanford tag-ger (Finkel et al, 2005).
Semantic roles were ob-tained by using the parser by Zhang et al (2008).Opinion expressions were identified using the Sub-jectivity Lexicon from the MPQA project (Wil-son et al, 2005).
Communication words were ob-tained by using the Appraisal Lexicon (Bloom et al,2007).
Nominalizations were recognized by lookingrelates to the candidate opinion holder.7available at disi.unitn.it/moschitti799Figure 2: Illustration of the different scopes on a CONSTAUG; nodes belonging to the candidate opinion holder aremarked with CAND.up nouns in NOMLEX (Macleod et al, 1998).5.1 NotationEach kernel is represented as a triple?levelOfRepresentation (Table 1), Scope (Table 3), typeOfKernel(Table 2)?, e.g.
?CONST, SENT, STK?
is a SubsetTree Kernel of a constituency parse having thescope of the entire sentence.
Note that not all com-binations of these three parameters are meaningful.In the following, we will just focus on importantand effective combinations.
The kernel composedof manually designed features is denoted by justV K .
The kernel composed of predicate-argumentstructures is denoted by ?PAS, SENT,PTK?.5.2 Vector Kernel (VK)The first line in Table 7 displays the result of thevector kernel using a manually designed feature set.It should be interpreted as a baseline.
Due to thehigh class imbalance we will focus on the compari-son of F(1)-Score throughout this paper rather thanaccuracy which is fairly biased on this data set.
TheF-Score of this classifier is at 56.16%.5.3 Sequence Kernels (SKs)For both sequence and tree kernels we need to findout what the best scope is, whether it is worthwhileto combine different scopes and what different lay-ers of representation can be usefully combined.The upper part of Table 5 lists the results of simpleword kernels using the different scopes.
The perfor-mance of the kernels using individual scopes variesgreatly.
The best scope is PRED (1), the secondbest is SEM (2).
The good performance of PREDdoes not come as a surprise since the sequence is thesmallest among the different scopes, so this scope isleast affected by data sparseness.
Moreover, this re-sult is consistent with our initial experiments on themanual feature set (see Section 4.5).Using different combinations of the word se-quence kernels shows that PRED and SEM (6)are a good combination, whereas OP , COMM ,and SENT (7;8;9) do not positively contribute tothe overall performance which is consistent whichthe individual scope evaluation.
Apparently, thesescopes capture less linguistically relevant structure.The next part of Table 5 shows the contribution ofPOS kernels when added to WRD kernels.
Addingthe corresponding POS kernel to the WRD kernelwith PRED scope (10) results in an improvementby more than 5% in F-Score.
We get another im-provement by approx.
3% when the correspondingSEM kernels (11) are added.
This suggests thatPOS is an effective generalization and that the twoscopes PRED and SEM are complementary.For the GRAMWRD kernel, the PRED scope(12) is again most effective.
We assume that this ker-nel most likely expresses meaningful syntactic rela-tionships for our task.
Adding the GRAMPOS ker-nel (14) gives another boost by almost 4%.Generalized sequence kernels are important.800Adding the corresponding WRDGN kernels to theWRD kernel with PRED and SEM scope resultsin an improvement from 47.77% (1) to 53.00% (15)which is a bit less than the combination of WRDand POS(GN) kernels (16).
However, these types ofkernels seem to be complementary since their com-bination provides an F-Score of 56.06% (17).
Thiskernel combination already performs on a par withthe manually designed vector kernel though less in-formation is taken into consideration.Finally, the best combination of sequence ker-nels (18) comprises WRD, WRDGN , POS, andPOSGN kernels with PRED and SEM scopecombined with a GRAMWRD and a GRAMPOSkernel with PRED scope.
The performance of58.70% significantly outperforms the vector kernel.5.4 Tree Kernels (TKs)Table 6 shows the results of the different tree ker-nels.
The table is divided into two halves.
Theleft half (A) are plain tree kernels, whereas the righthalf (B) are the augmented tree kernels.
As far asCONST kernels are concerned, there is a system-atic improvement by approximately 2% using treeaugmentation.
This proves that further non-syntacticknowledge added to the tree itself results in an im-proved F-Score.
However, tree augmentation doesnot have any impact on the PAS kernels.The overall performance of the tree kernels showsthat they are much more expressive than sequencekernels.
For instance, in order to obtain the sameperformance as of ?CONSTAUG, PRED,STK?
(19B), i.e.
a single kernel with an F-Score 56.52, itrequires several sequence kernels, hence much moreeffort.
The performance of the different CONSTkernels relative to each other resembles the resultsof the WRD kernels.
The best scope is PRED(19).
By far the worst performance is obtained bythe SENT scope (23).
The combination of PREDand SEM scope achieves an F-Score of 59.67%(25B) which is already slightly better than the bestconfiguration of sequence kernels (18).The performance of the PAS kernel (28A) withan F-Score of 53.51% is slightly worse than the bestsingle plain CONST kernel (19A).
The PAS ker-nel and the CONST kernels are complementary,since their best combination (29B) achieves an F-Score of 61.67% which is significantly better thanCombination Acc.
Prec.
Rec.
F1VK 93.63 53.28 59.37 56.16best SKs 94.21 57.64 59.81 58.70best TKs 94.16 56.18 68.36 61.67?VK + best SKs 94.34 58.44 61.27 59.82?VK + best TKs 94.33 57.41 68.03 62.27?best SKs + best TKs 94.49 59.22 63.96 61.49?VK + best SKs + best TKs 94.53 59.10 66.57 62.61?
?Table 7: Results of kernel combinations (?
: significantlybetter than best SKs; ?
: significantly better than best TKs;all convolution kernels are significantly better than VK).the best combination of CONST kernels (25B) orsequence kernels (18).5.5 CombinationsTable 7 lists the results of the different kernel typecombinations.
If VK is added to the best TKs, thebest SKs, or both, a slight increase in F-Score isachieved.
The best performance with an F-Score of62.61% is obtained by combining all kernels.6 ConclusionIn this paper, we compared convolution kernels foropinion holder extraction.
We showed that, in gen-eral, a combination of two scopes, namely the scopeimmediately encompassing the candidate opinionholder and its nearest predicate and the subclausecontaining the candidate opinion holder provide bestperformance.
Tree kernels containing constituencyparse information and semantic roles achieve betterperformance than sequence kernels or vector kernelsusing a manually designed feature set.
Best perfor-mance is achieved if all kernels are combined.AcknowledgementsMichael Wiegand was funded by the German researchcouncil DFG through the International Research TrainingGroup ?IRTG?
between Saarland University and Univer-sity of Edinburgh.The authors would like to thank Yi Zhang for pro-cessing the MPQA corpus with his semantic-role label-ing system, the researchers from the MPQA project forhelping to create an opinion holder corpus, and, in partic-ular, Alessandro Moschitti for insightful comments andsuggestions.801ID Kernel Acc.
Prec.
Rec.
F11 ?WRD, PRED, SK?
93.25 51.08 42.29 46.262 ?WRD, OP, SK?
92.77 46.38 32.52 38.213 ?WRD, COMM, SK?
92.42 43.70 35.99 39.464 ?WRD, SEM,SK?
93.16 50.32 34.65 41.045 ?WRD, SENT, SK?
90.60 29.90 27.29 28.536 ?WRD, PRED, SK?
+ ?WRD, SEM,SK?
93.78 56.55 41.36 47.777Pj?
{PRED,OP,COMM}?WRD, j,SK?
93.55 54.26 39.50 45.718Pj?Scopes\SENT ?WRD, j, SK?
93.82 57.21 40.28 47.269Pj?Scopes?WRD, j, SK?
93.63 55.15 39.52 46.0310 ?WRD, PRED, SK?
+ ?POS, PRED, SK?
93.03 49.39 53.53 51.3711Pi?
{PRED,SEM} (?WRD, i, SK?
+ ?POS, i, SK?)
93.86 55.60 53.22 54.3812Pi?
{PRED,SEM}?WRD, i, SK?
+ ?GRAMWRD , PRED, SK?
94.01 58.19 45.88 51.2913Pi?
{PRED,SEM}?WRD, i, SK?
+Pj?
{PRED,OP,COMM}?GRAMWRD , j, SK?
93.83 56.28 45.64 50.4014Xi?
{PRED,SEM}?WRD, i, SK?+?GRAMWRD, PRED, SK?+?GRAMPOS, PRED, SK?
93.98 56.59 53.92 55.2115Pi?
{PRED,SEM} (?WRD, i, SK?
+ ?WRDGN , i, SK?)
93.97 57.08 49.46 53.0016Pi?
{PRED,SEM} (?WRD, i, SK?
+ ?POSGN , i, SK?)
93.97 56.60 52.42 54.4217Xi?
{PRED,SEM}(?WRD, i, SK?
+ ?WRDGN , i, SK?
+ ?POS, i, SK?
+ ?POSGN , i, SK?)
93.85 55.16 57.00 56.0618Xi?
{PRED,SEM}(?WRD, i, SK?
+ ?WRDGN , i, SK?
+ ?POS, i, SK?
+ ?POSGN , i, SK?)
94.21 57.64 59.81 58.70+?GRAMWRD , PRED, SK?
+ ?GRAMPOS , PRED, SK?Table 5: Results of the different sequence kernels.A Bi = CONST, j = PAS i = CONSTAUG, j = PASAUGID Kernel Acc.
Prec.
Rec.
F1 Acc.
Prec.
Rec.
F119 ?i, PRED, STK?
92.89 48.68 62.34 54.67 93.12 49.99 65.04 56.5220 ?i, OP,STK?
93.04 49.49 54.71 51.96 93.27 50.93 59.06 54.6821 ?i, COMM,STK?
92.76 47.79 55.89 51.50 92.96 49.03 58.85 53.4722 ?i, SEM,STK?
93.70 54.40 52.13 53.23 93.90 55.47 56.59 56.0323 ?i, SENT,STK?
92.42 44.34 39.92 41.99 92.50 45.20 42.40 43.7424Pk?
{PRED,OP,COMM}?i, k, STK?
93.62 53.26 60.05 56.44 93.77 54.06 63.21 58.2625Pk?
{PRED,SEM}?i, k, STK?
93.90 55.26 59.50 57.30 94.13 56.57 63.12 59.6726Pk?Scopes\SENT ?i, k, STK?
94.09 56.65 59.68 58.11 94.21 57.21 62.61 59.8027Pk?Scopes?i, k, STK?
94.14 57.41 57.88 57.63 94.29 58.11 61.10 59.5628 ?j, SENT, PTK?
92.11 45.02 69.96 53.51 91.92 44.27 67.39 53.4329Xk?
{PRED,SEM}?i, k, STK?+?PAS,SENT, PTK?
94.05 55.68 66.01 60.40 94.16 56.18 68.36 61.6730Xk?Scopes\SENT?i, k, STK?
+ ?PAS,SENT, PTK?
94.30 57.95 62.62 60.19 94.36 58.07 64.94 61.31Table 6: Results of the different tree kernels.802ReferencesSteven Bethard, Hong Yu, Ashley Thornton, VasileiosHatzivassiloglou, and Dan Jurafsky.
2005.
ExtractingOpinion Propositions and Opinion Holders using Syn-tactic and Lexical Cues.
In Computing Attitude andAffect in Text: Theory and Applications.
Springer.Kenneth Bloom, Sterling Stein, and Shlomo Argamon.2007.
Appraisal Extraction for News Opinion Analy-sis at NTCIR-6.
In Proceedings of NTCIR-6 WorkshopMeeting, Tokyo, Japan.Razvan C. Bunescu and Raymond J. Mooney.
2005.Subsequence Kernels for Relation Extraction.
In Pro-ceedings of the Conference on Neural InformationProcessing Systems (NIPS), Vancouver, Canada.Yejin Choi, Claire Cardie, Ellen Riloff, and SiddharthPatwardhan.
2005.
Identifying Sources of Opinionswith Conditional Random Fields and Extraction Pat-terns.
In Proceedings of the Conference on HumanLanguage Technology and Empirical Methods in Nat-ural Language Processing (HLT/EMNLP), Vancouver,Canada.Yejin Choi, Eric Breck, and Claire Cardie.
2006.Joint Extraction of Entities and Relations for Opin-ion Recognition.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), Sydney, Australia.Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging.
In Proceedingsof the Annual Meeting of the Association for Compu-tational Linguistics (ACL), Philadelphia, USA.Charles.
J. Fillmore, Christopher R. Johnson, andMiriam R. Petruck.
2003.
Background to FrameNet.International Journal of Lexicography, 16:235 ?
250.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local Informationinto Information Extraction Systems by Gibbs Sam-pling.
In Proceedings of the Annual Meeting of theAssociation for Computational Linguistics (ACL), AnnArbor, USA.Soo-Min Kim and Eduard Hovy.
2005.
IdentifyingOpinion Holders for Question Answering in Opin-ion Texts.
In Proceedings of AAAI-05 Workshopon Question Answering in Restricted Domains, Pitts-burgh, USA.Soo-Min Kim and Eduard Hovy.
2006.
Extracting Opin-ions, Opinion Holders, and Topics Expressed in On-line News Media Text.
In Proceedings of the ACLWorkshop on Sentiment and Subjectivity in Text, Syd-ney, Australia.Paul Kingsbury and Martha Palmer.
2002.
From Tree-Bank to PropBank.
In Proceedings of the 3rd Confer-ence on Language Resources and Evaluation (LREC),Las Palmas, Spain.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics (ACL), Sapporo, Japan.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, and Ruth Reeves.
1998.
NOMLEX:A Lexicon of Nominalizations.
In Proceedings of EU-RALEX, Lie`ge, Belgium.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree Kernels for Semantic Role Label-ing.
Computational Linguistics, 34(2):193 ?
224.Alessandro Moschitti.
2006.
Efficient Convolution Ker-nels for Dependency and Constituent Syntactic Trees.In Proceedings of the 17th European Conference onMachine Learning (ECML), Berlin, Germany.Alessandro Moschitti.
2008.
Kernel Methods, Syn-tax and Semantics for Relational Text Categorization.In Proceedings of the Conference on Information andKnowledge Management (CIKM), Napa Valley, USA.Truc-Vien T. Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Convolution Kernels onConstituent, Dependency and Sequential Structuresfor Relation Extraction.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), Singapore.Ellen Riloff.
1996.
An Empirical Study of AutomatedDictionary Construction for Information Extraction.Artificial Intelligence, 85.John Taylor and Nello Christianini.
2004.
Kernel Meth-ods for Pattern Analysis.
Cambridge University Press.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2003.Annotating Expressions of Opinions and Emotions inLanguage.
Language Resources and Evaluation, 1:2.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing Contextual Polarity in Phrase-level Sentiment Analysis.
In Proceedings of Hu-man Language Technologies Conference/Conferenceon Empirical Methods in Natural Language Process-ing (HLT/EMNLP), Vancouver, Canada.Dell Zhang and Wee Sun Lee.
2003.
Question Classifi-cation using Support Vector Machines.
In Proceedingsof the ACM Special Interest Group on Information Re-trieval (SIGIR), Toronto, Canada.Min Zhang, Jie Zhang, and Jian Su.
2006.
Explor-ing Syntactic Features for Relation Extraction using aConvolution Tree Kernel.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the ACL (HLT/NAACL), NewYork City, USA.Yi Zhang, Rui Wang, and Hans Uszkoreit.
2008.
Hy-brid Learning of Dependency Structures from Het-erogeneous Linguistic Resources.
In Proceedings ofthe Conference on Computational Natural LanguageLearning (CoNLL), Manchester, United Kingdom.803
