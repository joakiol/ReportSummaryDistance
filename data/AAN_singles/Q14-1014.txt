Segmentation for Efficient Supervised Language Annotation with anExplicit Cost-Utility TradeoffMatthias Sperber1, Mirjam Simantzik2, Graham Neubig3, Satoshi Nakamura3, Alex Waibel11Karlsruhe Institute of Technology, Institute for Anthropomatics, Germany2Mobile Technologies GmbH, Germany3Nara Institute of Science and Technology, AHC Laboratory, Japanmatthias.sperber@kit.edu, mirjam.simantzik@jibbigo.com, neubig@is.naist.jps-nakamura@is.naist.jp, waibel@kit.eduAbstractIn this paper, we study the problem of manu-ally correcting automatic annotations of natu-ral language in as efficient a manner as pos-sible.
We introduce a method for automati-cally segmenting a corpus into chunks suchthat many uncertain labels are grouped intothe same chunk, while human supervisioncan be omitted altogether for other segments.A tradeoff must be found for segment sizes.Choosing short segments allows us to reducethe number of highly confident labels that aresupervised by the annotator, which is usefulbecause these labels are often already correctand supervising correct labels is a waste ofeffort.
In contrast, long segments reduce thecognitive effort due to context switches.
Ourmethod helps find the segmentation that opti-mizes supervision efficiency by defining usermodels to predict the cost and utility of su-pervising each segment and solving a con-strained optimization problem balancing thesecontradictory objectives.
A user study demon-strates noticeable gains over pre-segmented,confidence-ordered baselines on two naturallanguage processing tasks: speech transcrip-tion and word segmentation.1 IntroductionMany natural language processing (NLP) tasks re-quire human supervision to be useful in practice,be it to collect suitable training material or to meetsome desired output quality.
Given the high cost ofhuman intervention, how to minimize the supervi-sion effort is an important research problem.
Previ-ous works in areas such as active learning, post edit-(a) It was a bright cold (they) in (apron), and (a) clockswere striking thirteen.
(b) It was a bright cold (they) in (apron), and (a) clockswere striking thirteen.
(c) It was a bright cold (they) in (apron), and (a) clockswere striking thirteen.Figure 1: Three automatic transcripts of the sentence ?Itwas a bright cold day in April, and the clocks were strik-ing thirteen?, with recognition errors in parentheses.
Theunderlined parts are to be corrected by a human for (a)sentences, (b) words, or (c) the proposed segmentation.ing, and interactive pattern recognition have inves-tigated this question with notable success (Settles,2008; Specia, 2011; Gonza?lez-Rubio et al., 2010).The most common framework for efficient anno-tation in the NLP context consists of training an NLPsystem on a small amount of baseline data, and thenrunning the system on unannotated data to estimateconfidence scores of the system?s predictions (Set-tles, 2008).
Sentences with the lowest confidenceare then used as the data to be annotated (Figure 1(a)).
However, it has been noted that when the NLPsystem in question already has relatively high accu-racy, annotating entire sentences can be wasteful, asmost words will already be correct (Tomanek andHahn, 2009; Neubig et al., 2011).
In these cases, itis possible to achieve much higher benefit per anno-tated word by annotating sub-sentential units (Fig-ure 1 (b)).However, as Settles et al.
(2008) point out, sim-ply maximizing the benefit per annotated instanceis not enough, as the real supervision effort varies169Transactions of the Association for Computational Linguistics, 2 (2014) 169?180.
Action Editor: Eric Fosler-Lussier.Submitted 11/2013; Revised 2/2014; Published 4/2014.
c?2014 Association for Computational Linguistics.1 3 5 7 9 11 13 15 17 190246Segment lengthAvg.
time/ instance[sec]Transcription taskWord segmentation taskFigure 2: Average annotation time per instance, plottedover different segment lengths.
For both tasks, the effortclearly increases for short segments.greatly across instances.
This is particularly impor-tant in the context of choosing segments to annotate,as human annotators heavily rely on semantics andcontext information to process language, and intu-itively, a consecutive sequence of words can be su-pervised faster and more accurately than the samenumber of words spread out over several locations ina text.
This intuition can also be seen in our empiri-cal data in Figure 2, which shows that for the speechtranscription and word segmentation tasks describedlater in Section 5, short segments had a longer anno-tation time per word.
Based on this fact, we argueit would be desirable to present the annotator witha segmentation of the data into easily supervisablechunks that are both large enough to reduce the num-ber of context switches, and small enough to preventunnecessary annotation (Figure 1 (c)).In this paper, we introduce a new strategy for nat-ural language supervision tasks that attempts to op-timize supervision efficiency by choosing an appro-priate segmentation.
It relies on a user model that,given a specific segment, predicts the cost and theutility of supervising that segment.
Given this usermodel, the goal is to find a segmentation that mini-mizes the total predicted cost while maximizing theutility.
We balance these two criteria by defining aconstrained optimization problem in which one cri-terion is the optimization objective, while the othercriterion is used as a constraint.
Doing so allowsspecifying practical optimization goals such as ?re-move as many errors as possible given a limited timebudget,?
or ?annotate data to obtain some requiredclassifier accuracy in as little time as possible.
?Solving this optimization task is computationallydifficult, an NP-hard problem.
Nevertheless, wedemonstrate that by making realistic assumptionsabout the segment length, an optimal solution canbe found using an integer linear programming for-mulation for mid-sized corpora, as are common forsupervised annotation tasks.
For larger corpora, weprovide simple heuristics to obtain an approximatesolution in a reasonable amount of time.Experiments over two example scenarios demon-strate the usefulness of our method: Post editingfor speech transcription, and active learning forJapanese word segmentation.
Our model predictsnoticeable efficiency gains, which are confirmed inexperiments with human annotators.2 Problem DefinitionThe goal of our method is to find a segmentationover a corpus of word tokens wN1 that optimizessupervision efficiency according to some predictiveuser model.
The user model is denoted as a set offunctions ul,k(wba) that evaluate any possible sub-sequence wba of tokens in the corpus according tocriteria l2L, and supervision modes k2K.Let us illustrate this with an example.
Sperber etal.
(2013) defined a framework for speech transcrip-tion in which an initial, erroneous transcript is cre-ated using automatic speech recognition (ASR), andan annotator corrects the transcript either by correct-ing the words by keyboard, by respeaking the con-tent, or by leaving the words as is.
In this case,we could define K={TYPE, RESPEAK, SKIP}, eachconstant representing one of these three supervisionmodes.
Our method will automatically determinethe appropriate supervision mode for each segment.The user model in this example might evaluate ev-ery segment according to two criteria L, a cost crite-rion (in terms of supervision time) and a utility cri-terion (in terms of number of removed errors), whenusing each mode.
Intuitively, respeaking should beassigned both lower cost (because speaking is fasterthan typing), but also lower utility than typing on akeyboard (because respeaking recognition errors canoccur).
The SKIP mode denotes the special, unsuper-vised mode that always returns 0 cost and 0 utility.Other possible supervision modes include mul-tiple input modalities (Suhm et al., 2001), severalhuman annotators with different expertise and cost170(Donmez and Carbonell, 2008), and correction vs.translation from scratch in machine translation (Spe-cia, 2011).
Similarly, cost could instead be ex-pressed in monetary terms, or the utility functioncould predict the improvement of a classifier whenthe resulting annotation is not intended for direct hu-man consumption, but as training data for a classifierin an active learning framework.3 Optimization FrameworkGiven this setting, we are interested in simulta-neously finding optimal locations and supervisionmodes for all segments, according to the given cri-teria.
Each resulting segment will be assigned ex-actly one of these supervision modes.
We de-note a segmentation of the N tokens of corpus wN1into M?N segments by specifying segment bound-ary markers sM+11 =(s1=1, s2, .
.
.
, sM+1=N+1).Setting a boundary marker si=a means that weput a segment boundary before the a-th word to-ken (or the end-of-corpus marker for a=N+1).Thus our corpus is segmented into token sequences[(wsj , .
.
.
, wsj+1 1)]Mj=1.
The supervision modesassigned to each segment are denoted by mj .
Wefavor those segmentations that minimize the cumu-lative valuePMj=1[ul,mj (wsj+1sj )] for each criterion l.For any criterion where larger values are intuitivelybetter, we flip the sign before defining ul,mj (wsj+1sj )to maintain consistency (e.g.
negative number of er-rors removed).3.1 Multiple Criteria OptimizationIn the case of a single criterion (|L|=1), we obtaina simple, single-objective unconstrained linear opti-mization problem, efficiently solvable via dynamicprogramming (Terzi and Tsaparas, 2006).
However,in practice one usually encounters several compet-ing criteria, such as cost and utility, and here wewill focus on this more realistic setting.
We balancecompeting criteria by using one as an optimizationobjective, and the others as constraints.1 Let crite-1This approach is known as the bounded objective functionmethod in multi-objective optimization literature (Marler andArora, 2004).
The very popular weighted sum method mergescriteria into a single efficiency measure, but is problematic inour case because the number of supervised tokens is unspec-ified.
Unless the weights are carefully chosen, the algorithmmight find, e.g., the completely unsupervised or completely su-(at)% (what?s)% a% bright% ?%[RESPEAK:1.5/2]/[SKIP:0/0]/1/ cold%2/ 3/ 4/ 5/ 6/[TYPE:2/5]/[TYPE:1/4]/[TYPE:1/4]/[RESPEAK:0/3]/[SKIP:0/0]/Figure 3: Excerpt of a segmentation graph for an ex-ample transcription task similar to Figure 1 (some edgesare omitted for readability).
Edges are labeled with theirmode, predicted number of errors that can be removed,and necessary supervision time.
A segmentation schememight prefer solid edges over dashed ones in this exam-ple.rion l0 be the optimization objective criterion, andlet Cl denote the constraining constants for the cri-teria l 2 L l0 = L \ {l0}.
We state the optimizationproblem:minM ;sM+11 ;mM1MXj=1?ul0,mj wsj+1sj ?s.t.MXj=1?ul,mj wsj+1sj ??
Cl (8l 2 L l0)This constrained optimization problem is difficultto solve.
In fact, the NP-hard multiple-choice knap-sack problem (Pisinger, 1994) corresponds to a spe-cial case of our problem in which the number of seg-ments is equal to the number of tokens, implyingthat our more general problem is NP-hard as well.In order to overcome this problem, we refor-mulate search for the optimal segmentation as aresource-constrained shortest path problem in a di-rected, acyclic multigraph.
While still not efficientlysolvable in theory, this problem is well studied indomains such as vehicle routing and crew schedul-ing (Irnich and Desaulniers, 2005), and it is knownthat in many practical situations the problem canbe solved reasonably efficiently using integer linearprogramming relaxations (Toth and Vigo, 2001).In our formalism, the set of nodes V representsthe spaces between neighboring tokens, at which thealgorithm may insert segment boundaries.
A nodewith index i represents a segment break before thei-th token, and thus the sequence of the indices ina path directly corresponds to sM+11 .
Edges E de-note the grouping of tokens between the respectivepervised segmentation to be most ?efficient.
?171nodes into one segment.
Edges are always directedfrom left to right, and labeled with a supervisionmode.
In addition, each edge between nodes i and jis assigned ul,k(wj 1i ), the corresponding predictedvalue for each criterion l 2 L and supervision modek 2 K, indicating that the supervision mode of thej-th segment in a path directly corresponds to mj .Figure 3 shows an example of what the result-ing graph may look like.
Our original optimizationproblem is now equivalent to finding the shortestpath between the first and last nodes according tocriterion l0, while obeying the given resource con-straints.
According to a widely used formulation forthe resource constrained shortest path problem, wecan defineEij as the set of competing edges betweeni and j, and express this optimization problem withthe following integer linear program (ILP):minxXi,j2VXk2Eijxijkul0,k(sj 1i ) (1)s.t.Xi,j2VXk2Eijxijkul,k(sj 1i ) ?
Cl(8l 2 L l0)(2)Xi2Vk2Eijxijk =Xi2Vk2Eijxjik(8j 2 V \{1, n})(3)Xj2Vk2E1jx1jk = 1 (4)Xi2Vk2Einxink = 1 (5)xijk 2 {0, 1} (8xijk 2 x) (6)The variables x={xijk|i, j 2 V , k 2 Eij} denotethe activation of the k?th edge between nodes i andj.
The shortest path according to the minimizationobjective (1), that still meets the resource constraintsfor the specified criteria (2), is to be computed.
Thedegree constraints (3,4,5) specify that all but the firstand last nodes must have as many incoming as out-going edges, while the first node must have exactlyone outgoing, and the last node exactly one incom-ing edge.
Finally, the integrality condition (6) forcesall edges to be either fully activated or fully deacti-vated.
The outlined problem formulation can solveddirectly by using off-the-shelf ILP solvers, here weemploy GUROBI (Gurobi Optimization, 2012).3.2 Heuristics for ApproximationIn general, edges are inserted for every supervisionmode between every combination of two nodes.
Thesearch space can be constrained by removing someof these edges to increase efficiency.
In this study,we only consider edges spanning at most 20 tokens.For cases in which larger corpora are to be anno-tated, or when the acceptable delay for delivering re-sults is small, a suitable segmentation can be foundapproximately.
The easiest way would be to parti-tion the corpus, e.g.
according to its individual doc-uments, divide the budget constraints evenly acrossall partitions, and then segment each partition inde-pendently.
More sophisticated methods might ap-proximate the Pareto front for each partition, anddistribute the budgets in an intelligent way.4 User ModelingWhile the proposed framework is able to optimizethe segmentation with respect to each criterion, italso rests upon the assumption that we can provideuser models ul,k(wj 1i ) that accurately evaluate ev-ery segment according to the specified criteria andsupervision modes.
In this section, we discuss ourstrategies for estimating three conceivable criteria:annotation cost, correction of errors, and improve-ment of a classifier.4.1 Annotation Cost ModelingModeling cost requires solving a regression prob-lem from features of a candidate segment to annota-tion cost, for example in terms of supervision time.Appropriate input features depend on the task, butshould include notions of complexity (e.g.
a confi-dence measure) and length of the segment, as bothare expected to strongly influence supervision time.We propose using Gaussian process (GP) regres-sion for cost prediction, a start-of-the-art nonpara-metric Bayesian regression technique (Rasmussenand Williams, 2006)2.
As reported on a similartask by Cohn and Specia (2013), and confirmed byour preliminary experiments, GP regression signifi-cantly outperforms popular techniques such as sup-2Code available at http://www.gaussianprocess.org/gpml/172port vector regression and least-squares linear re-gression.
We also follow their settings for GP, em-ploying GP regression with a squared exponentialkernel with automatic relevance determination.
De-pending on the number of users and amount of train-ing data available for each user, models may betrained separately for each user (as we do here), orin a combined fashion via multi-task learning as pro-posed by Cohn and Specia (2013).It is also crucial for the predictions to be reliablethroughout the whole relevant space of segments.If the cost of certain types of segments is system-atically underpredicted, the segmentation algorithmmight be misled to prefer these, possibly a largenumber of times.3 An effective trick to prevent suchunderpredictions is to predict the log time instead ofthe actual time.
In this way, errors in the critical lowend are penalized more strongly, and the time cannever become negative.4.2 Error Correction ModelingAs one utility measure, we can use the number oferrors corrected, a useful measure for post editingtasks over automatically produced annotations.
Inorder to measure how many errors can be removedby supervising a particular segment, we must es-timate both how many errors are in the automaticannotation, and how reliably a human can removethese for a given supervision mode.Most machine learning techniques can estimateconfidence scores in the form of posterior probabil-ities.
To estimate the number of errors, we can sumover one minus the posterior for all tokens, whichestimates the Hamming distance from the referenceannotation.
This measure is appropriate for tasks inwhich the number of tokens is fixed in advance (e.g.a part-of-speech estimation task), and a reasonableapproximation for tasks in which the number of to-kens is not known in advance (e.g.
speech transcrip-tion, cf.
Section 5.1.1).Predicting the particular tokens at which a humanwill make a mistake is known to be a difficult task(Olson and Olson, 1990), but a simplifying constant3For instance, consider a model that predicts well for seg-ments of medium size or longer, but underpredicts the supervi-sion time of single-token segments.
This may lead the segmen-tation algorithm to put every token into its own segment, whichis clearly undesirable.human error rate can still be useful.
For example,in the task from Section 2, we may suspect a certainnumber of errors in a transcript segment, and predict,say, 95% of those errors to be removed via typing,but only 85% via respeaking.4.3 Classifier Improvement ModelingAnother reasonable utility measure is accuracy of aclassifier trained on the data we choose to annotatein an active learning framework.
Confidence scoreshave been found useful for ranking particular tokenswith regards to how much they will improve a clas-sifier (Settles, 2008).
Here, we may similarly scoresegment utility as the sum of its token confidences,although care must be taken to normalize and cali-brate the token confidences to be linearly compara-ble before doing so.
While the resulting utility scorehas no interpretation in absolute terms, it can still beused as an optimization objective (cf.
Section 5.2.1).5 ExperimentsIn this section, we present experimental results ex-amining the effectiveness of the proposed methodover two tasks: speech transcription and Japaneseword segmentation.45.1 Speech Transcription ExperimentsAccurate speech transcripts are a much-demandedNLP product, useful by themselves, as training ma-terial for ASR, or as input for follow-up tasks likespeech translation.
With recognition accuraciesplateauing, manually correcting (post editing) auto-matic speech transcripts has become popular.
Com-mon approaches are to identify words (Sanchez-Cortina et al., 2012) or (sub-)sentences (Sperber etal., 2013) of low confidence, and have a human edi-tor correct these.5.1.1 Experimental SetupWe conduct a user study in which participantspost-edited speech transcripts, given a fixed goalword error rate.
The transcription setup was suchthat the transcriber could see the ASR transcript ofparts before and after the segment that he was edit-ing, providing context if needed.
When imprecisetime alignment resulted in segment breaks that were4Software and experimental data can be downloaded fromhttp://www.msperber.com/research/tacl-segmentation/173slightly ?off,?
as happened occasionally, that contexthelped guess what was said.
The segment itself wastranscribed from scratch, as opposed to editing theASR transcript; besides being arguably more effi-cient when the ASR transcript contains many mis-takes (Nanjo et al., 2006; Akita et al., 2009), prelim-inary experiments also showed that supervision timeis far easier to predict this way.
Figure 4 illustrateswhat the setup looked like.We used a self-developed transcription tool toconduct experiments.
It presents our computed seg-ments one by one, allows convenient input and play-back via keyboard shortcuts, and logs user interac-tions with their time stamps.
A selection of TEDtalks5 (English talks on technology, entertainment,and design) served as experimental data.
Whilesome of these talks contain jargon such as medi-cal terms, they are presented by skilled speakers,making them comparably easy to understand.
Initialtranscripts were created using the Janus recognitiontoolkit (Soltau et al., 2001) with a standard, TED-optimized setup.
We used confusion networks fordecoding and obtaining confidence scores.For reasons of simplicity, and better compara-bility to our baseline, we restricted our experimentto two supervision modes: TYPE and SKIP.
Weconducted experiments with 3 participants, 1 withseveral years of experience in transcription, 2 withnone.
Each participant received an explanation onthe transcription guidelines, and a short hands-ontraining to learn to use our tool.
Next, they tran-scribed a balanced selection of 200 segments ofvarying length and quality in random order.
Thisdata was used to train the user models.Finally, each participant transcribed another 2TED talks, with word error rate (WER) 19.96%(predicted: 22.33%).
We set a target (predicted)WER of 15% as our optimization constraint,6 andminimize the predicted supervision time as our ob-jective function.
Both TED talks were transcribedonce using the baseline strategy, and once using theproposed strategy.
The order of both strategies wasreversed between talks, to minimize learning biasdue to transcribing each talk twice.The baseline strategy was adopted according to5www.ted.com6Depending on the level of accuracy required by our finalapplication, this target may be set lower or higher.Sperber et al.
(2013): We segmented the talk intonatural, subsentential units, using Matusov et al.
(2006)?s segmenter, which we tuned to reproducethe TED subtitle segmentation, producing a meansegment length of 8.6 words.
Segments were addedin order of increasing average word confidence, untilthe user model predicted a WER<15%.
The secondsegmentation strategy was the proposed method,similarly with a resource constraint of WER<15%.Supervision time was predicted via GP regres-sion (cf.
Section 4.1), using segment length, au-dio duration, and mean confidence as input features.The output variable was assumed subject to addi-tive Gaussian noise with zero mean, a variance of5 seconds was chosen empirically to minimize themean squared error.
Utility prediction (cf.
Section4.2) was based on posterior scores obtained fromthe confusion networks.
We found it important tocalibrate them, as the posteriors were overconfidentespecially in the upper range.
To do so, we automat-ically transcribed a development set of TED data,grouped the recognized words into buckets accord-ing to their posteriors, and determined the averagenumber of errors per word in each bucket from analignment with the reference transcript.
The map-ping from average posterior to average number oferrors was estimated via GP regression.
The resultwas summed over all tokens, and multiplied by aconstant human confidence, separately determinedfor each participant.75.1.2 Simulation ResultsTo convey a better understanding of the poten-tial gains afforded by our method, we first present asimulated experiment.
We assume a transcriber whomakes no mistakes, and needs exactly the amount oftime predicted by a user model trained on the data ofa randomly selected participant.
We compare threescenarios: A baseline simulation, in which the base-line segments are transcribed in ascending order ofconfidence; a simulation using the proposed method,in which we change the WER constraint in small in-crements; finally, an oracle simulation, which uses7More elaborate methods for WER estimation exist, such asby Ogawa et al.
(2013), but if our method achieves improve-ments using simple Hamming distance, incorporating more so-phisticated measures will likely achieve similar, or even betteraccuracy.174(3) SKIP: ?nineteen forty six until today you see the green?
(4) TYPE: <annotator types: ?is the traditional?>(5) SKIP: ?Interstate conflict?
(6) TYPE: <annotator types: ?the ones we used to?>(7) SKIP: .
.
.Figure 4: Result of our segmentation method (excerpt).TYPE segments are displayed empty and should be tran-scribed from scratch.
For SKIP segments, the ASR tran-script is displayed to provide context.
When annotating asegment, the corresponding audio is played back.0 10 20 30 40 50 600510152025Post editing time [min]ResultingWER[%]BaselineProposedOracleFigure 5: Simulation of post editing on example TEDtalk.
The proposed method reduces the WER consider-ably faster than the baseline at first, later both converge.The much superior oracle simulation indicates room forfurther improvement.the proposed method, but uses a utility model thatknows the actual number of errors in each segment.For each supervised segment, we simply replace theASR output with the reference, and measure the re-sulting WER.Figure 5 shows the simulation on an exampleTED talk, based on an initial transcript with 21.9%WER.
The proposed method is able to reduce theWER faster than the baseline, up to a certain pointwhere they converge.
The oracle simulation is evenfaster, indicating room for improvement throughbetter confidence scores.5.1.3 User Study ResultsTable 1 shows the results of the user study.
First,we note that the WER estimation by our utilitymodel was off by about 2.5%: While the predictedimprovement in WER was from 22.33% to 15.0%,the actual improvement was from 19.96% to about12.5%.
The actual resulting WER was consistentParticipant Baseline ProposedWER Time WER TimeP1 12.26 44:05 12.18 33:01P2 12.75 36:19 12.77 29:54P3 12.70 52:42 12.50 37:57AVG 12.57 44:22 12.48 33:37Table 1: Transcription task results.
For each user, theresultingWER [%] after supervision is shown, along withthe time [min] they needed.
The unsupervised WER was19.96%.across all users, and we observe strong, consistentreductions in supervision time for all participants.Prediction of the necessary supervision time was ac-curate: Averaged over participants, 45:41 minuteswere predicted for the baseline, 44:22 minutes mea-sured.
For the proposed method, 32:11 minutes werepredicted, 33:37 minutes measured.
On average,participants removed 6.68 errors per minute usingthe baseline, and 8.93 errors per minute using theproposed method, a speed-up of 25.2%.Note that predicted and measured values are notstrictly comparable: In the experiments, to providea fair comparison participants transcribed the sametalks twice (once using baseline, once the proposedmethod, in alternating order), resulting in a notice-able learning effect.
The user model, on the otherhand, is trained to predict the case in which a tran-scriber conducts only one transcription pass.As an interesting finding, without being informedabout the order of baseline and proposed method,participants reported that transcribing according tothe proposed segmentation seemed harder, as theyfound the baseline segmentation more linguisticallyreasonable.
However, this perceived increase in dif-ficulty did not show in efficiency numbers.5.2 Japanese Word Segmentation ExperimentsWord segmentation is the first step in NLP for lan-guages that are commonly written without wordboundaries, such as Japanese and Chinese.
We ap-ply our method to a task in which we domain-adapt aword segmentation classifier via active learning.
Inthis experiment, participants annotated whether ornot a word boundary occurred at certain positions ina Japanese sentence.
The tokens to be grouped intosegments are positions between adjacent characters.1755.2.1 Experimental SetupNeubig et al.
(2011) have proposed a pointwisemethod for Japanese word segmentation that can betrained using partially annotated sentences, whichmakes it attractive in combination with active learn-ing, as well as our segmentation method.
Theauthors released their method as a software pack-age ?KyTea?
that we employed in this user study.We used KyTea?s active learning domain adaptationtoolkit8 as a baseline.For data, we used the Balanced Corpus of Con-temporary Written Japanese (BCCWJ), created byMaekawa (2008), with the internet Q&A subcor-pus as in-domain data, and the whitepaper subcor-pus as background data, a domain adaptation sce-nario.
Sentences were drawn from the in-domaincorpus, and the manually annotated data was thenused to train KyTea, along with the pre-annotatedbackground data.
The goal (objective function) wasto improve KyTea?s classification accuracy on an in-domain test set, given a constrained time budget of30 minutes.
There were again 2 supervision modes:ANNOTATE and SKIP.
Note that this is essentially abatch active learning setup with only one iteration.We conducted experiments with one expert withseveral years of experience with Japanese word seg-mentation annotation, and three non-expert nativespeakers with no prior experience.
Japanese wordsegmentation is not a trivial task, so we providednon-experts with training, including explanation ofthe segmentation standard, a supervised test withimmediate feedback and explanations, and hands-ontraining to get used to the annotation software.Supervision time was predicted via GP regression(cf.
Section 4.1), using the segment length and meanconfidence as input features.
As before, the outputvariable was assumed subject to additive Gaussiannoise with zero mean and 5 seconds variance.
To ob-tain training data for these models, each participantannotated about 500 example instances, drawn fromthe adaptation corpus, grouped into segments andbalanced regarding segment length and difficulty.For utility modeling (cf.
Section 4.3), we first nor-malized KyTea?s confidence scores, which are givenin terms of SVM margin, using a sigmoid function(Platt, 1999).
The normalization parameter was se-8http://www.phontron.com/kytea/active.htmllected so that the mean confidence on a developmentset corresponded to the actual classifier accuracy.We derive our measure of classifier improvement forcorrecting a segment by summing over one minusthe calibrated confidence for each of its tokens.
Toanalyze how well this measure describes the actualtraining utility, we trained KyTea using the back-ground data plus disjoint groups of 100 in-domaininstances with similar probabilities and measuredthe achieved reduction of prediction errors.
The cor-relation between each group?s mean utility and theachieved error reduction was 0.87.
Note that we ig-nore the decaying returns usually observed as moredata is added to the training set.
Also, we did notattempt to model user errors.
Employing a con-stant base error rate, as in the transcription scenario,would change segment utilities only by a constantfactor, without changing the resulting segmentation.After creating the user models, we conducted themain experiment, in which each participant anno-tated data that was selected from a pool of 1000in-domain sentences using two strategies.
The first,baseline strategy was as proposed by Neubig et al.(2011).
Queries are those instances with the low-est confidence scores.
Each query is then extendedto the left and right, until a word boundary is pre-dicted.
This strategy follows similar reasoning aswas the premise to this paper: To decide whether ornot a position in a text corresponds to a word bound-ary, the annotator has to acquire surrounding contextinformation.
This context acquisition is relativelytime consuming, so he might as well label the sur-rounding instances with little additional effort.
Thesecond strategy was our proposed, more principledapproach.
Queries of both methods were shuffledto minimize bias due to learning effects.
Finally, wetrained KyTea using the results of both methods, andcompared the achieved classifier improvement andsupervision times.5.2.2 User Study ResultsTable 2 summarizes the results of our experi-ment.
It shows that the annotations by each partic-ipant resulted in a better classifier for the proposedmethod than the baseline, but also took up consider-ably more time, a less clear improvement than forthe transcription task.
In fact, the total error fortime predictions was as high as 12.5% on average,176Participant Baseline ProposedTime Acc.
Time Acc.Expert 25:50 96.17 32:45 96.55NonExp1 22:05 95.79 26:44 95.98NonExp2 23:37 96.15 31:28 96.21NonExp3 25:23 96.38 33:36 96.45Table 2: Word segmentation task results, for our ex-pert and 3 non-expert participants.
For each participant,the resulting classifier accuracy [%] after supervision isshown, along with the time [min] they needed.
The unsu-pervised accuracy was 95.14%.where the baseline method tended take less time thanpredicted, the proposed method more time.
This isin contrast to a much lower total error (within 1%)when cross-validating our user model training data.This is likely due to the fact that the data for train-ing the user model was selected in a balanced man-ner, as opposed to selecting difficult examples, asour method is prone to do.
Thus, we may expectmuch better predictions when selecting user modeltraining data that is more similar to the test case.Plotting classifier accuracy over annotation timedraws a clearer picture.
Let us first analyze the re-sults for the expert annotator.
Figure 6 (E.1) showsthat the proposed method resulted in consistentlybetter results, indicating that time predictions werestill effective.
Note that this comparison may put theproposed method at a slight disadvantage by com-paring intermediate results despite optimizing glob-ally.For the non-experts, the improvement over thebaseline is less consistent, as can be seen in Fig-ure 6 (N.1) for one representative.
According toour analysis, this can be explained by two factors:(1) The non-experts?
annotation error (6.5% on av-erage) was much higher than the expert?s (2.7%),resulting in a somewhat irregular classifier learn-ing curve.
(2) The variance in annotation timeper segment was consistently higher for the non-experts than the expert, indicated by an averageper-segment prediction error of 71% vs. 58% rela-tive to the mean actual value, respectively.
Infor-mally speaking, non-experts made more mistakes,and were more strongly influenced by the difficultyof a particular segment (which was higher on av-erage with the proposed method, as indicated by a0 10 20 300.9550.9650 10 20 300.9550.9650 10 20 300.9550.9650 10 20 300.9550.9650 10 20 300.9550.9650 10 20 300.9550.9650 10 20 300.9550.9650 10 20 300.9550.965Prop.BaselN.1E.1 N.2E.2 N.3E.3 N.4E.4Annotation time [min.
]Classifier Accuracy.Figure 6: Classifier improvement over time, depicted forthe expert (E) and a non-expert (N).
The graphs shownumbers based on (1) actual annotations and user mod-els as in Sections 4.1 and 4.3, (2) error-free annotations,(3) measured times replaced by predicted times, and (4)both reference annotations and replaced time predictions.lower average confidence).9In Figures 6 (2-4) we present a simulation experi-ment in which we first pretend as if annotators madeno mistakes, then as if they needed exactly as muchtime as predicted for each segment, and then both.This cheating experiment works in favor of the pro-posed method, especially for the non-expert.
Wemay conclude that our segmentation approach is ef-fective for the word segmentation task, but requiresmore accurate time predictions.
Better user modelswill certainly help, although for the presented sce-nario our method may be most useful for an expertannotator.9Note that the non-expert in the figure annotated much fasterthan the expert, which explains the comparable classificationresult despite making more annotation errors.
This is in contrastto the other non-experts, who were slower.1775.3 Computational EfficiencySince our segmentation algorithm does not guar-antee polynomial runtime, computational efficiencywas a concern, but did not turn out problematic.On a consumer laptop, the solver produced seg-mentations within a few seconds for a single docu-ment containing several thousand tokens, and withinhours for corpora consisting of several dozen doc-uments.
Runtime increased roughly quadraticallywith respect to the number of segmented tokens.
Wefeel that this is acceptable, considering that the timeneeded for human supervision will likely dominatethe computation time, and reasonable approxima-tions can be made as noted in Section 3.2.6 Relation to Prior WorkEfficient supervision strategies have been studiedacross a variety of NLP-related research areas, andreceived increasing attention in recent years.
Ex-amples include post editing for speech recogni-tion (Sanchez-Cortina et al., 2012), interactive ma-chine translation (Gonza?lez-Rubio et al., 2010), ac-tive learning for machine translation (Haffari et al.,2009; Gonza?lez-Rubio et al., 2011) and many otherNLP tasks (Olsson, 2009), to name but a few studies.It has also been recognized by the active learn-ing community that correcting the most useful partsfirst is often not optimal in terms of efficiency, sincethese parts tend to be the most difficult to manuallyannotate (Settles et al., 2008).
The authors advocatethe use of a user model to predict the supervision ef-fort, and select the instances with best ?bang-for-the-buck.?
This prediction of supervision effort was suc-cessful, and was further refined in other NLP-relatedstudies (Tomanek et al., 2010; Specia, 2011; Cohnand Specia, 2013).
Our approach to user modelingusing GP regression is inspired by the latter.Most studies on user models consider only super-vision effort, while neglecting the accuracy of hu-man annotations.
The view on humans as a perfectoracle has been criticized (Donmez and Carbonell,2008), since human errors are common and cannegatively affect supervision utility.
Research onhuman-computer-interaction has identified the mod-eling of human errors as very difficult (Olson andOlson, 1990), depending on factors such as user ex-perience, cognitive load, user interface design, andfatigue.
Nevertheless, even the simple error modelused in our post editing task was effective.The active learning community has addressed theproblem of balancing utility and cost in some moredetail.
The previously reported ?bang-for-the-buck?approach is a very simple, greedy approach to com-bine both into one measure.
A more theoreticallyfounded scalar optimization objective is the net ben-efit (utility minus costs) as proposed by Vijaya-narasimhan and Grauman (2009), but unfortunatelyis restricted to applications where both can be ex-pressed in terms of the same monetary unit.
Vijaya-narasimhan et al.
(2010) and Donmez and Carbonell(2008) use a more practical approach that specifies aconstrained optimization problem by allowing onlya limited time budget for supervision.
Our approachis a generalization thereof and allows either specify-ing an upper bound on the predicted cost, or a lowerbound on the predicted utility.The main novelty of our presented approach isthe explicit modeling and selection of segments ofvarious sizes, such that annotation efficiency is opti-mized according to the specified constraints.
Whilesome works (Sassano and Kurohashi, 2010; Neubiget al., 2011) have proposed using subsentential seg-ments, we are not aware of any previous work thatexplicitly optimizes that segmentation.7 ConclusionWe presented a method that can effectively choosea segmentation of a language corpus that optimizessupervision efficiency, considering not only the ac-tual usefulness of each segment, but also the anno-tation cost.
We reported noticeable improvementsover strong baselines in two user studies.
Future userexperiments with more participants would be desir-able to verify our observations, and allow furtheranalysis of different factors such as annotator ex-pertise.
Also, future research may improve the usermodeling, which will be beneficial for our method.AcknowledgmentsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement n 287658 Bridges Across the LanguageDivide (EU-BRIDGE).178ReferencesYuya Akita, Masato Mimura, and Tatsuya Kawahara.2009.
Automatic Transcription System for Meetingsof the Japanese National Congress.
In Interspeech,pages 84?87, Brighton, UK.Trevor Cohn and Lucia Specia.
2013.
Modelling Anno-tator Bias with Multi-task Gaussian Processes: An Ap-plication to Machine Translation Quality Estimation.In Association for Computational Linguistics Confer-ence (ACL), Sofia, Bulgaria.Pinar Donmez and Jaime Carbonell.
2008.
ProactiveLearning : Cost-Sensitive Active Learning with Mul-tiple Imperfect Oracles.
In Conference on Informationand Knowledge Management (CIKM), pages 619?628,Napa Valley, CA, USA.Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart?
?nez, and Fran-cisco Casacuberta.
2010.
Balancing User Effort andTranslation Error in Interactive Machine TranslationVia Confidence Measures.
In Association for Compu-tational Linguistics Conference (ACL), Short PapersTrack, pages 173?177, Uppsala, Sweden.Jesu?s Gonza?lez-Rubio, Daniel Ortiz-Mart?
?nez, and Fran-cisco Casacuberta.
2011.
An active learning scenariofor interactive machine translation.
In InternationalConference on Multimodal Interfaces (ICMI), pages197?200, Alicante, Spain.Gurobi Optimization.
2012.
Gurobi Optimizer Refer-ence Manual.Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.2009.
Active Learning for Statistical Phrase-basedMachine Translation.
In North American Chapterof the Association for Computational Linguistics -Human Language Technologies Conference (NAACL-HLT), pages 415?423, Boulder, CO, USA.Stefan Irnich and Guy Desaulniers.
2005.
Shortest PathProblems with Resource Constraints.
In Column Gen-eration, pages 33?65.
Springer US.Kikuo Maekawa.
2008.
Balanced Corpus of Contem-porary Written Japanese.
In International Joint Con-ference on Natural Language Processing (IJCNLP),pages 101?102, Hyderabad, India.R.
Timothy Marler and Jasbir S. Arora.
2004.
Surveyof multi-objective optimization methods for engineer-ing.
Structural and Multidisciplinary Optimization,26(6):369?395, April.EvgenyMatusov, ArneMauser, and Hermann Ney.
2006.Automatic Sentence Segmentation and PunctuationPrediction for Spoken Language Translation.
In Inter-national Workshop on Spoken Language Translation(IWSLT), pages 158?165, Kyoto, Japan.Hiroaki Nanjo, Yuya Akita, and Tatsuya Kawahara.2006.
Computer Assisted Speech Transcription Sys-tem for Efficient Speech Archive.
In Western PacificAcoustics Conference (WESPAC), Seoul, Korea.Graham Neubig, Yosuke Nakata, and Shinsuke Mori.2011.
Pointwise Prediction for Robust , Adapt-able Japanese Morphological Analysis.
In Associa-tion for Computational Linguistics: Human LanguageTechnologies Conference (ACL-HLT), pages 529?533,Portland, OR, USA.Atsunori Ogawa, Takaaki Hori, and Atsushi Naka-mura.
2013.
Discriminative Recognition Rate Esti-mation For N-Best List and Its Application To N-BestRescoring.
In International Conference on Acoustics,Speech, and Signal Processing (ICASSP), pages 6832?6836, Vancouver, Canada.Judith Reitman Olson and Gary Olson.
1990.
TheGrowth of Cognitive Modeling in Human-ComputerInteraction Since GOMS.
Human-Computer Interac-tion, 5(2):221?265, June.Fredrik Olsson.
2009.
A literature survey of active ma-chine learning in the context of natural language pro-cessing.
Technical report, SICS Sweden.David Pisinger.
1994.
A Minimal Algorithm for theMultiple-Choice Knapsack Problem.
European Jour-nal of Operational Research, 83(2):394?410.John C. Platt.
1999.
Probabilistic Outputs for Sup-port Vector Machines and Comparisons to RegularizedLikelihood Methods.
In Advances in Large MarginClassifiers, pages 61?74.
MIT Press.Carl E. Rasmussen and Christopher K.I.
Williams.
2006.Gaussian Processes for Machine Learning.
MITPress, Cambridge, MA, USA.Isaias Sanchez-Cortina, Nicolas Serrano, Alberto San-chis, and Alfons Juan.
2012.
A prototype for Inter-active Speech Transcription Balancing Error and Su-pervision Effort.
In International Conference on Intel-ligent User Interfaces (IUI), pages 325?326, Lisbon,Portugal.Manabu Sassano and Sadao Kurohashi.
2010.
UsingSmaller Constituents Rather Than Sentences in Ac-tive Learning for Japanese Dependency Parsing.
InAssociation for Computational Linguistics Conference(ACL), pages 356?365, Uppsala, Sweden.Burr Settles, Mark Craven, and Lewis Friedland.
2008.Active Learning with Real Annotation Costs.
InNeural Information Processing Systems Conference(NIPS) - Workshop on Cost-Sensitive Learning, LakeTahoe, NV, United States.Burr Settles.
2008.
An Analysis of Active LearningStrategies for Sequence Labeling Tasks.
In Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 1070?1079, Honolulu, USA.Hagen Soltau, Florian Metze, Christian Fu?gen, and AlexWaibel.
2001.
A One-Pass Decoder Based on Poly-morphic Linguistic Context Assignment.
In Auto-matic Speech Recognition and Understanding Work-179shop (ASRU), pages 214?217, Madonna di Campiglio,Italy.Lucia Specia.
2011.
Exploiting Objective Annota-tions for Measuring Translation Post-editing Effort.
InConference of the European Association for MachineTranslation (EAMT), pages 73?80, Nice, France.Matthias Sperber, Graham Neubig, Christian Fu?gen,Satoshi Nakamura, and Alex Waibel.
2013.
EfficientSpeech Transcription Through Respeaking.
In Inter-speech, pages 1087?1091, Lyon, France.Bernhard Suhm, Brad Myers, and Alex Waibel.
2001.Multimodal error correction for speech user inter-faces.
Transactions on Computer-Human Interaction,8(1):60?98.Evimaria Terzi and Panayiotis Tsaparas.
2006.
Efficientalgorithms for sequence segmentation.
In SIAM Con-ference on Data Mining (SDM), Bethesda, MD, USA.Katrin Tomanek and Udo Hahn.
2009.
Semi-SupervisedActive Learning for Sequence Labeling.
In Interna-tional Joint Conference on Natural Language Process-ing (IJCNLP), pages 1039?1047, Singapore.Katrin Tomanek, Udo Hahn, and Steffen Lohmann.2010.
A Cognitive Cost Model of Annotations Basedon Eye-Tracking Data.
In Association for Compu-tational Linguistics Conference (ACL), pages 1158?1167, Uppsala, Sweden.Paolo Toth and Daniele Vigo.
2001.
The Vehicle RoutingProblem.
Society for Industrial & Applied Mathemat-ics (SIAM), Philadelphia.Sudheendra Vijayanarasimhan and Kristen Grauman.2009.
Whats It Going to Cost You?
: Predicting Ef-fort vs. Informativeness for Multi-Label Image Anno-tations.
In Conference on Computer Vision and Pat-tern Recognition (CVPR), pages 2262?2269, MiamiBeach, FL, USA.Sudheendra Vijayanarasimhan, Prateek Jain, and KristenGrauman.
2010.
Far-sighted active learning on a bud-get for image and video recognition.
In Conferenceon Computer Vision and Pattern Recognition (CVPR),pages 3035?3042, San Francisco, CA, USA, June.180
