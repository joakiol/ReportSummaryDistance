NAACL-HLT 2012 Workshop on Predicting and Improving Text Readability for target reader populations (PITR 2012)., pages 1?7,Montre?al, Canada, June 7, 2012. c?2012 Association for Computational LinguisticsToward Determining the Comprehensibility of Machine TranslationsTucker Maney, Linda Sibert, andDennis PerzanowskiKalyan Gupta and Astrid Schmidt-NielsenNaval Research Laboratory Knexus Research Corporation4555 Overlook Avenue, SW 163 Waterfront Street, Suite 440Washington, DC National Harbor, MD{tucker.maney|linda.sibert|dennis.perzanowski}@nrl.navy.mil{kalyan.gupta.ctr|astrid.schmidtnielsen.ctr}@nrl.navy.milAbstractEconomic globalization and the needs of theintelligence community have brought ma-chine translation into the forefront.
There arenot enough skilled human translators to meetthe growing demand for high quality transla-tions or ?good enough?
translations that suf-fice only to enable understanding.
Muchresearch has been done in creating transla-tion systems to aid human translators and toevaluate the output of these systems.
Metricsfor the latter have primarily focused on im-proving the overall quality of entire test setsbut not on gauging the understanding of in-dividual sentences or paragraphs.
Therefore,we have focused on developing a theory oftranslation effectiveness by isolating a set oftranslation variables and measuring their ef-fects on the comprehension of translations.In the following study, we focus on investi-gating how certain linguistic permutations,omissions, and insertions affect the under-standing of translated texts.1.
IntroductionThere are numerous methods for measuringtranslation quality and ongoing research to im-prove relevant and informative metrics (seehttp://www.itl.nist.gov/iad/mig/tests/metricsmatr)(Przybocki et al, 2008).
Many of these automat-ed metrics, including BLEU and NIST, were cre-ated to be used only for aggregate counts over anentire test-set.
The effectiveness of these methodson translations of short segments remains unclear(Kulesza and Shieber, 2004).
Moreover, most ofthese tools are useful for comparing different sys-tems, but do not attempt to identify the mostdominant cause of errors.
All errors are notequal and as such should be evaluated dependingon their consequences (Schiaffino and Zearo,2005).Recently, researchers have begun looking atthe frequencies of errors in translations of specif-ic language pairs.
Vilar et al (2006) presented atypology for annotating errors and used it to clas-sify errors between Spanish and English andfrom Chinese into English.
Popovic and Ney(2011) used methods for computing Word ErrorRate (WER) and Position-independent word Er-ror Rate (PER) to outline a procedure for auto-matic error analysis and classification.
Theyevaluated their methodology by looking at trans-lations into English from Arabic, Chinese andGerman and two-way English-Spanish data(Popovic and Ney, 2007).
Condon et al (2010)used the US National Institute of Standards andTechnology?s NIST post-editing tool to annotateerrors in English-Arabic translationsThese methods have all focused on findingfrequencies of individual error categories, not ondetermining their effect on comprehension.
Inmachine translation environments where post-editing is used to produce the same linguisticquality as would be achieved by standard humantranslation, such a focus is justified.
A greaterreduction in the time needed to correct a transla-tion would be achieved by eliminating errors thatfrequently occur.However, there are situations in which anytranslation is an acceptable alternative to notranslation, and the direct (not post-edited) con-tent is given to the user.
Friends chatting via in-1stant messaging tools or reading foreign-language e-mail mainly want to understandroughly what is being said.
When a Marine isout patrolling and needs to interact with the localinhabitants to get information, it is ?far better tohave a machine [translation] than to not have an-ything?
(Gallafent, 2011).
For such purposes,automated translation can provide a ?gist?
of themeaning of the original message as long as it iscomprehensible.
In such situations, errors thataffect comprehension trump those that occur fre-quently and should receive a greater focus in ef-forts to improve output quality.Recently, companies have begun customizingtranslation engines for use in specific environ-ments.
IBM and Lionbridge?s GeoFluent(http://en-us.lionbridge.com/GeoFluent/GeoFluent.htm)uses customization to improve translation outputfor online chatting and other situations wherepost-editing is not feasible.
TranSys(http://www.multicorpora.com/en/products/product-options-and-add-ons/multitrans-prism-transys/) from Mutlicorpora and Systran also usescustomization to deliver translations ready forimmediate distribution or for human post-editing.Knowing the major factors for creating under-standable text can play a role in perfecting suchsystems.Research has not settled on a single methodol-ogy for classifying translation errors.
Two of thefive categories proposed by Vilar et al (2006),missing words and word order, are the focus ofthis project.
Missing word errors fall into twocategories, those essential to the meaning of thesentence and those only necessary for grammati-cal correctness.
Only the first of these is ad-dressed here.
Likewise, there is a distinctionbetween word- or phrase-based reordering.
Theresults of the experiment presented in this paperare concerned only with the latter.The present research seeks to determine theimpact of specific error types on comprehension.We contend that research efforts should focus onthose errors resulting in misinterpretation, notjust on those that occur most often.
This projecttherefore focuses on the use of linguistic parame-ters, including omissions and changes in wordorder, to determine the effect on comprehensibil-ity of machine translations at the sentence andparagraph level.2.
MethodologyThe first step in this research was determining thelinguistic parameters to be investigated.
Ninesentence types exhibiting the following charac-teristics were selected:?
Deleted verb?
Deleted adjective?
Deleted noun?
Deleted pronoun?
Modified prepositions in, on, at to an al-ternate one (e.g.
in ?
at)?
Modified word order to SOV  (Subject,Object, Verb)?
Modified word order to VOS?
Modified word order to VSO?
Retained SVO word order (control).The one additional parameter, modifying a prep-osition, was added to the original list because it isa frequent error of translations into English(Takahaski, 1969).The next step was to identify a means to testcomprehension.
Sachs (1967) contends that asentence has been understood if it is representedin one?s memory in a form that preserves itsmeaning, but not necessarily its surface structure.Royer?s (Royer et al, 1987) Sentence Verifica-tion Technique (SVT) is a technique for measur-ing the comprehension of text paragraphs bydetermining if such a representation has beencreated.
It has been used for three decades andbeen shown to be a reliable and valid techniquefor measuring comprehension in a wide varietyof applications (Pichette et al, 2009).In composing SVT tests, several paragraphs,each containing approximately 12 sentences, arechosen.
For each of the sentences appearing inthe original text, four test sentences are created.One is an exact copy of the original sentence andanother, a paraphrase of that sentence.
A ?mean-ing change?
test sentence is one in which a fewwords are changed in order to alter the meaningof the sentence.
The fourth test sentence is a ?dis-tractor?
which is consistent with the text of theoriginal, but is not related in meaning to any sen-tence in the original passage (Royer et al, 1979).We used a similar measure, a variation of theMeaning Identification Technique (MIT)(Marchant et al, 1988), a simpler version of thetest that was developed out of the SVT and cor-2rected for some of its shortfalls.
Here, there areonly two test sentence types presented, either aparaphrase of the original sentence or a ?meaningchange?
sentence.
In the description of the MITtechnique for sentence creation, a paraphrase iscreated for each sentence in the original text andaltering this paraphrase produces the ?meaningchange?
sentence.
In this experiment, the origi-nal sentence, not the paraphrase, was used toproduce a sentence using many of the samewords but with altered meaning.In the test, readers are asked to read a passage,in our case a passage in which the linguistic pa-rameters have been manipulated in a controlledfashion (see Section 3 (2)).
Then with the text nolonger visible, they are presented with a series ofsyntactically correct sentences shown one at atime in random order and asked to label them asbeing ?old?
or ?new?, relative to the passage theyhave just read (see Section 3 (3)).
A sentenceshould be marked ?old?
if it has the same mean-ing as a sentence in the original paragraph and?new?
otherwise.
?New?
sentences contain in-formation that was absent from or contradictoryto that in the original passage.3.
ExperimentThe first requirement of the study was develop-ing paragraphs to be used for the experiment.Eleven passages found on the WEB, many ofwhich were GLOSS(http://gloss.dliflc.edu/search.aspx) online lan-guage lessons, were edited to consist of exactlynine sentences.
These paragraphs, containingwhat will be referred to as the original sentences,served as the basis for building the passages to beread by the participants and for creating the sen-tences to be used in the test.The next step was to apply the linguistic pa-rameters under study to create the paragraphs tobe read initially by the reader.
One of the lin-guistic parameters listed above was randomlychosen and applied to alter a sentence withineach paragraph, so that each paragraph containedexactly one of each of the parameter changes.However, pronouns and prepositions were notpresent in all sentences.
When one of these wasthe parameter to be changed in a given sentencebut was not present, adjustments had to be madein the original pairing of sentences with the otherlinguistic parameters.
The changes were done asrandomly as possible but in such a way that eachparagraph still contained one of each type of pa-rameter modification.In sentences in which the change was anomission, the word to delete was chosen random-ly from all those in the sentence having the samepart of speech (POS).
For sentences in whichthe preposition needed to be modified, the choicewas randomly chosen from the two remainingalternatives as listed above in Section 2.In creating the test sentences, the original sen-tences were again used.
For each sentence withineach paragraph, a committee of four, two ofwhich were linguists, decided upon both a para-phrase and a meaning change sentence.
Then,within each paragraph, the paraphrase of fourrandomly chosen sentences and the meaningchange alternative for four others, also randomlypicked, were selected.
The ninth sentence ran-domly fell in either the paraphrase or meaningchange category.After reading the altered paragraph, the partic-ipant saw four or five sentences that were para-phrases of the original sentences and four or fivesentences that were ?meaning change?
sentences,all in random order.
The following is (1) an ex-ample of part of an original paragraph and (2) thesame section linguistically altered.
In (2), thealterations are specified in brackets after eachsentence.
Participants in the study did not, ofcourse, see these identifiers.
In (3), the samplecomprehension questions posed after individualsread the linguistically altered passages are pre-sented.
In (3), the answers are provided inbrackets after each sentence.
Again, participantsdid not see the latter.
(1) World powers regard space explorations asthe best strategy to enhance their status onthe globe.
Space projects with cutting-edgetechnologies not only serve as the best strate-gy to enhance their status on the globe.
Koreamust have strong policies to catch up withthe space powers.
The nation needs an over-arching organization that manages all itsspace projects, similar to the National Aero-nautics and Space Administration (NASA)and the European Space Agency (ESA).
Inaddition, a national consensus must beformed if a massive budget is to be allocatedwith a long-term vision.
Only under these3circumstances can the nation?s brightestminds unleash their talent in the field.
(2) World powers regard space explorations asthe best strategy to enhance status on theglobe.
[PRO] Space projects with cutting-edge technologies not only as the driver ofgrowth in future industries and technologicaldevelopment, but play a pivotal role in mili-tary strategies.
[VERB]  Korea strong poli-cies space powers the to catch up with havemust.
[SOV] Needs an overarching organiza-tion that manages all its space projects, simi-lar to the National Aeronautics and SpaceAdministration (NASA) and the EuropeanSpace Agency (ESA) the nation.
[VOS] Inaddition, a national consensus must beformed if a massive budget is to be allocatedwith a vision.
[ADJ]  Can unleash, only un-der these circumstances, the nation?s bright-est minds their talent in the field.
[VSO](3) World powers regard space explorations as aviable, but expensive strategy to enhancetheir status among other countries.
[NEW]Though space projects can be important formilitary purposes, the long-term costs canhamper a country?s development in other ar-eas.
[NEW]  To perform on a par with thepredominate players in space exploration,Korea must develop robust policies.
[OLD]Managing all of the nation?s space projectswill require a central organization, similar tothe United States?
National Aeronautics andSpace Administration (NASA).
[OLD]  Se-curing the necessary budget and allocatingthese funds in accordance with a long-termvision will require national consensus.
[OLD] The nation?s brightest minds will beexpected to work in the aerospace field.
[NEW]20 people volunteered as participants, con-sisting of 11 males and 9 females.
All were over25 years of age.
All had at least some college,with 15 of the 20 holding advanced degrees.
On-ly two did not list English as their native lan-guage.
Of these, one originally spoke Polish, theother Farsi/Persian.
Both had learned English bythe age of 15 and considered themselves compe-tent English speakers.Participants were tested individually.
Eachparticipant was seated at a computer workstationequipped with a computer monitor, a keyboardand mouse.
The display consisted of a series ofscreens displaying the passage, followed by thetest sentences and response options.At the start, participants completed two train-ing passages.
The paragraph read in the first hadno linguistic alterations, while the second wasrepresentative of what the participants would seewhen doing the actual experiment.
For both pas-sages, after selecting a response option for a testsentence, the correct answer and reason for it wasshown.
There was an optional third training pas-sage that no one elected to use.During the experiment, participants were askedto read a passage.
After finishing, with the textno longer in view, they were asked to rate a se-ries of sentences as to whether they contained?old?
or ?new?
information, relative to the in-formation presented in the passage.
Every partic-ipant viewed the same passages, but the order inwhich they were shown was randomized.
Like-wise, the sentences to be rated for a given pas-sage were shown in varied order.
Participants?keyboard interactions were time-stamped andtheir choices digitally recorded using softwarespecifically designed for this experiment.After completing the test session, participantswere asked to complete a short online question-naire.
This was used to obtain background in-formation, such as age, educational level, andtheir reactions during the experiment.4.
SoftwareThe interface for the experiment and final ques-tionnaire were developed using QuestSys, a web-based survey system that is part of the customweb application framework, Cobbler, licensed byKnexus Research Corporation.
Cobbler is writ-ten in Python and uses the web frameworkCherryPy and the database engine SQLite, bothfrom the public domain.5.
ResultsDuring the test, participants choose either ?old?or ?new?
after reading each sentence.
The num-ber they correctly identified out of the totalviewed for that condition in all paragraphs wasdetermined.
This score, the proportion correct(pc) for each condition, is as follows:4SVO  0.788 (control)PREP  0.854PRO  0.800SOV  0.790NOUN  0.769VOS  0.769VSO  0.757ADJ  0.689VERB  0.688The average performance for SVT is about 75%correct.
In a valid test, one at the appropriatelevel for the population being tested, overallgroup averages should not fall below 65% orabove 85% (Royer et al, 1987).
The results ofthis experiment were consistent with these expec-tations.Because pc does not take into account a per-son?s bias for answering yes or no, it is consid-ered to be a poor measure of one?s ability torecognize a stimulus.
This is because the re-sponse chosen in a discrimination task is knownto be a product of the evidence for the presenceof the stimulus and the bias of the participant tochoose one response over the other.
Signal De-tection Theory (SDT) is frequently used to factorout bias when evaluating the results of tasks inwhich a person distinguishes between two differ-ent responses to a stimulus (Macmillan andCreelman, 1991).
It has been applied in areassuch as lie detection (truth/lie), inspection (ac-ceptable /unacceptable), information retrieval(relevant /irrelevant) and memory experiments(old/new) (Stanislaw and Todorov, 1999).
In thelatter, participants are shown a list of words andsubsequently asked to indicate whether or notthey remember seeing a particular word.
Thisexperiment was similar:  users were asked, notabout remembering a ?word?, but to determine ifthey had read a sentence having the same mean-ing.The unbiased proportion correct, p(c)max, ametric provided by SDT was used to generateunbiased figures from the biased ones.
For yes-nosituations, such as this experiment,p(c)max = ?
(d'/2), where d?
= z (H) ?
z (F) , Hbeing the hit rate and F, the false alarm rate.Larger d' values indicate that a participant seesa clearer difference between the ?old?
and ?new?data.
The d' values near zero demonstrate chanceperformance.
Perfect performance results in aninfinite d' value.
To avoid getting infinite results,any 0 or 1 values obtained for an individual userwere converted to 1/(2N) and 1-1/(2N) (Macmil-lan and Creelman, 1991).
Negative values,which usually indicate response confusion, wereeliminated.The results of Single Factor Anova of p(c)maxare shown below (Table 1).
Since the F valueexceeds the F-crit, the null hypothesis that alltreatments were essentially equal must be reject-ed at the 0.05 level of significance.Dunnett?s t statistic (Winer et al, 1991) (Table2) was used to determine if there was a signifi-cant difference between any of the eight sentencevariations and the control (SVO).
The results aregiven below.The critical value for a one-tailed 0.05 test: t0.95(9,167) ?
2.40.
The results in Table 2 indicatethat, in this experiment, adjective (ADJ) and verbdeletions (VERB) had a significant effect on theunderstanding of short paragraphs.
Other dele-tions and changes in word order were not shownto significantly alter comprehension.6.
DiscussionThough translation errors vary by language pairand direction, this research focused on two areasthat cause problems in translations into English:word deletion and alterations in word order.
Itlooked at how these errors affect the comprehen-sion of sentences contained in short paragraphs.In the research cited above (Vilar et al (2006),Condon et al (2010), and Popovic and Ney(2007; 2011)), wrong lexical choice caused themost errors, followed by missing words.
For theGALE corpora for Chinese and Arabic transla-tions into English, Popovic and Ney (2011) cate-gorized missing words by POS classes.
The POSthat predominated varied by language but verbswere consistently at the top, adjectives near thebottom.
Our study showed that both significant-ly affect the comprehension of a paragraph.
De-leted nouns, prepositions and pronouns didcontribute to the overall error rate, but noneproved important to the reader in interpreting thetext.
Word order modifications were not a majorcause of errors in the research above, nor did theyappear to cause problems in our experiment.These results lead us to argue that in situationswhere there may be no or limited post-editing,reducing errors in verb translation should be a5SUMMARYGroups Count Sum Average VarianceSVO 19 15.75532 0.829227 0.01104PREP 20 17.12685 0.856343 0.017096PRO 20 16.17873 0.808936 0.013273SOV 20 16.24132 0.812066 0.0135NOUN 20 16.04449 0.802225 0.010088VOS 20 15.9539 0.797695 0.011276VSO 19 15.13767 0.796719 0.020403ADJ 19 13.78976 0.725777 0.010103VERB 19 13.88158 0.730609 0.015428ANOVASource ofVariation SS df MS F P-value F critBetweenGroups 0.27809 8 0.034761 2.563014 0.011608 1.994219813WithinGroups 2.264963 167 0.013563Total 2.543053 175Table 1.
Anova Single Factor of p(c)maxPREP PRO SOV NOUN VOS VSO ADJ VERB0.736215 -0.55093 -0.46596 -0.73316 -0.85615 -0.86029 -2.7377 -2.60981Table 2.
Dunnett?s t statisticmajor focus in machine translation research.Though missing adjectives also significantly af-fected comprehension, a commitment of re-sources to solve an infrequently occurringproblem may be unwarranted.
It must be noted,however, that the data used in reporting error fre-quencies was limited to Chinese and Arabic.
Fur-ther research is still required to determine theapplicability of these findings for translatingfrom other languages into English.7.
ConclusionIn this experiment, the paragraph appears to haveprovided enough context for the reader to correct-ly surmise most missing words and to understandan altered word order.
The deletion of an adjec-tive or verb, however, caused a significant de-cline in comprehensibility.
In research by othersdealing with error frequencies, verbs were fre-quently missing in English translation output,adjectives rarely.This suggests that translation of verbs shouldreceive more attention as research in machinetranslation continues, particularly in systems de-signed to produce ?good enough?
translations.This was a small test and the part of speechchosen for elimination was not necessarily themost salient.
It is unknown if a longer test, in-volving more passages, or passages in which themissing word was always significant, would haveamplified these results.This study used the Sentence VerificationTechnique in a novel way.
Though constructingthe test requires some expertise, it provides a wayto test the comprehensibly of translation outputwithout the use of experienced translators or ref-6erence translations produced by such translators.AcknowledgementsThe authors would like to thank Rachael Rich-ardson for her research contributions.
The USNaval Research Laboratory supported this re-search through funding from the US Office ofNaval Research.ReferencesCondon, Sherri, Dan Parvaz, John Aberdeen, ChristyDoran, Andrew Freeman, and Marwan Awad.
(2010).English and Iraqi Arabic.
In Proceedings of the Sev-enth International Conference on Language Re-sources and Evaluation (LREC'10),Valletta, Malta,May 19-21.Gallafent, Alex.
(2011).
Machine Translation for theMilitary.
In The World, April 26, 2011.Gamon, Michael, Anthony Aue, and Martine Smets.(2005).
Sentence-level-MT evaluation without refer-ence translations: Beyond language modeling.
InEAMT 2005 Conference Proceedings, pp.
103-111,Budapest.Kulesza, Alex and Stuart Shieber.
(2004).
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the 10th International Conferenceon Theoretical and Methodological Issues in MachineTranslation, Baltimore, MD, October 4?6.Lavie, Alon, Kenji Sagae, and ShyamsundarJayaraman.
(2004).
The Significance of Recall in Au-tomatic Metrics for MT Evaluation.
In Proceedings ofthe 6th Conference of the Association for MachineTranslation in the Americas (AMTA-2004), pp.
134?143.
Washington, DC.Macmillan, Neil and C. Douglas Creelman.
(1991).Detection theory: A User?s guide.
Cambridge Univer-sity Press, pp.
10 &125.Marchant, Horace, James Royer and  Barbara Greene.(1988).
Superior reliability and validity for a newform of the Sentence Verification Technique formeasuring comprehension.
In Educational and Psy-chological Measurement, 48, pp.
827-834.Pichette, Fran?ois, Linda De Serres, and Marc Lafon-taine.
(2009).
Measuring L2 reading comprehensionability using SVT tests.
Round Table Panels andPoster Presentation for the Language and ReadingComprehension for Immigrant Children (LARCIC),May, 2009.Popovic, Maja and Hermann Ney.
(2007) Word ErrorRates: Decomposition over POS Classes and Applica-tions for Error Analysis.
In Proceeding of the SecondWorkshop on Statistical Machine Translation, pp.
48-55, Prague.Popovic, Maja and Hermann Ney.
(2011) TowardsAutomatic Error Analysis of Machine TranslationOutput.
Computational Linguistics, 37 (4): 657-688.Przybocki, Mark, Kay Peterson, and S?bastienBronsart.
(2008).
Official results of the NIST 2008"Metrics for MAchine TRanslation" Challenge(MetricsMATR08),http://nist.gov/speech/tests/metricsmatr/2008/results/Royer, James, Barbara Greene, and Gale Sinatra.(1987).
The Sentence Verification Technique: A prac-tical procedure teachers can use to develop their ownreading and listening comprehension tests.
Journal ofReading, 30: 414-423.Royer, James, Nicholas Hastings, and Colin Hook.(1979).
A sentence verification technique for measur-ing reading comprehension.
Journal of Reading Be-havior, 11:355-363.Sachs, Jacqueline.
(1967).
Recognition memory forsyntactic and semantic aspects of connected discourse.Perception & Psychophysics, 1967(2):  437-442.Schiaffino, Riccardo and Franco Zearo.
(2005).
Trans-lation Quality Measurement in Practice.
46th ATAConference, Seattle Technologies.Stanislaw, Harold and Natasha Todorov.
(1999).
Cal-culation of Signal Detection Theory Measures, Behav-ior Research Methods, Instruments, & Computers,31(1): 137-149.Takahaski, George.
(1969).
Perceptions of space andfunction of certain English prepositions.
LanguageLearning, 19: 217-234.Vilar, David, Jia Xu, Luis Fernando D?Haro, andHermann Ney.
(2006).
Error Analysis of StatisticalMachine Translation Output.
In Proceedings of the 5thInternational Conference on Language Resources andEvaluation (LREC?06), pp.
697?702, Genoa, ItalyWiner, B., Donald Brown, and Kenneth Michels.(1991).
Statistical Principles in Experimental Design.3rd Edition.
New York: McGraw?Hill, Inc. pp.
169-171.7
