Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 37?42,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsBuilding a Semantic Lexicon of English Nouns via BootstrappingTing Qian1, Benjamin Van Durme2 and Lenhart Schubert21Department of Brain and Cognitive Sciences2Department of Computer ScienceUniversity of RochesterRochester, NY 14627 USAting.qian@rochester.edu, {vandurme, schubert}@cs.rochester.eduAbstractWe describe the use of a weakly supervisedbootstrapping algorithm in discovering con-trasting semantic categories from a source lex-icon with little training data.
Our method pri-marily exploits the patterns in sentential con-texts where different categories of words mayappear.
Experimental results are presentedshowing that such automatically categorizedterms tend to agree with human judgements.1 IntroductionThere are important semantic distinctions betweendifferent types of English nouns.
For example, somenouns typically refer to a concrete physical object,such as book, tree, etc.
Others are used to representthe process or the result of an event (e.g.
birth, cele-bration).
Such information is useful in disambiguat-ing syntactically similar phrases and sentences, so asto provide more accurate semantic interpretations.For instance, A MAN WITH HOBBIES and A MANWITH APPLES share the same structure, but conveyvery different aspects about the man being referredto (i.e.
activities vs possessions).Compiling such a lexicon by hand, e.g., WordNet(Fellbaum, 1998), requires tremendous time and ex-pertise.
In addition, when new words appear, thesewill have to be analyzed and added manually.
Fur-thermore, a single, global lexicon may contain er-roneous categorizations when used within a specificdomain/genre; we would like a ?flexible?
lexicon,adaptable to a given corpus.
Also, in adapting se-mantic classifications of words to a particular genreor domain, we would like to be able to exploit con-tinuing improvements in methods of extracting se-mantic occurrence patterns from text.We present our initial efforts in discovering se-mantic classes incrementally under a weakly super-vised bootstrapping process.
The approach is ableto selectively learn from its own discoveries, therebyminimizing the effort needed to provide seed exam-ples as well as maintaining a reasonable accuracyrate.
In what follows, we first focus on its appli-cation to an event-noun classification task, and thenuse a physical-object vs non-physical-object experi-ment as a showcase for the algorithm?s generality.2 Bootstrapping AlgorithmThe bootstrapping algorithm discovers words withsemantic properties similar to a small set of labelledseed examples.
These examples can be manually se-lected from an existing lexicon.
By simply changingthe semantic property of the seed set, this algorithmcan be applied to the task of discovering a variety ofsemantic classes.Features Classification is performed using aperceptron-based model (Rosenblatt, 1958) that ex-amines features of each word.
We use two kindsof features in our model: morphological (affix andword length), and contextual.
Suffixes, such as -ion,often reveal the semantic type that a noun belongsto (e.g., destruction, explosion).
Other suffixes like-er typically suggest non-event nouns (e.g.
waiter,hanger).
The set of affixes can be modified to re-flect meaningful distinctions in the task at hand.
Re-garding word length, longer words tend to have more37syllables, and thus are more likely to contain affixes.For example, if a word ends with -ment, its num-ber of letters must be ?
5.
We defined a partitionof words based on word length: shortest (fewer than5 letters), short (5-7), medium (8-12), long (13-19),and longest (> 19).Besides morphological features, we also make useof verbalized propositions resulting from the experi-ments of Van Durme et al (2008) as contextual fea-tures.
These outputs are in the form of world knowl-edge ?factoids?
abstracted from texts, based on log-ical forms from parsed sentences, produced by theKNEXT system (see Schubert (2002) for details).The followings are some sample factoids about theword destruction, extracted from the British Na-tional Corpus.?
A PERSON-OR-ORGANIZATION MAY UNDERGO A DE-STRUCTION?
INDIVIDUAL -S MAY HAVE A DESTRUCTION?
PROPERTY MAY UNDERGO A DESTRUCTIONWe take each verbalization (with the target wordremoved) as a contextual feature, such as PROPERTYMAY UNDERGO A .
Words from the same seman-tic category (e.g., event nouns) should have seman-tic and syntactic similarities on the sentential level.Thus their contextual features, which reflect the useof words both semantically and syntactically, shouldbe similar.
For instance, PROPERTY MAY UNDERGOA PROTECTION is another verbalization producedby KNEXT, suggesting the word protection may be-long to the same category as destruction.A few rough-and-ready heuristics are already em-ployed by KNEXT to do the same task as we wishto automate here.
A built-in classifier judges nomi-nals to be event or non-event ones based on analysisof endings, plus a list of event nouns whose endingsare unrevealing, and a list of non-event nouns whoseendings tend to suggest they are event nouns.
As aresult, the factoids used as contextual features in ourwork already reflect the built-in classifier?s attemptto distinguish event nouns from the rest.
Thus, theuse of these contextual features may bias the algo-rithm to perform seemingly well on event-noun clas-sification.
However, we will show that our algorithmworks for classification of other semantic categories,for which KNEXT does not yet have discriminativeprocedures.Iterative Training We use a bootstrapping pro-cedure to iteratively train a perceptron-based lin-ear classifier.
A perceptron algorithm determineswhether the active features of a test case are similarto those learned from given categories of examples.In an iterative training process, the classifier firstlearns from a small seed set, which contains exam-ples of all categories (in binary classification, bothpositive and negative examples) manually selectedto reflect human knowledge of semantic categories.The classifier then discovers new instances (and cor-responding features) of each category.
Based onactivation values, these newly discovered instancesare selectively admitted into the original training set,which increases the size of training examples for thenext iteration.The iterative training algorithm described aboveis adopted from Klementiev and Roth (2006).
Theadvantage of bootstrapping is the ability to auto-matically learn from new discoveries, which savesboth time and effort required to manually examinea source lexicon.
However, if implemented exactlyas described above, this process has two apparentdisadvantages: New examples may be wrongly clas-sified by the model; and it is difficult to evaluate thediscriminative models produced in successive itera-tions, as there are no standard data against which tojudge them (the new examples are by definition pre-viously unexamined).
We propose two measures toalleviate these problems.
First, we admit into thetraining set only those instances whose activationvalues are higher than the mean activation of theircorresponding categories in an iteration.
This setsa variable threshold that is correlated with the per-formance of the model at each iteration.
Second, weevaluate iterative results post hoc, using a Bootstrap-ping Score.
This measures the efficacy of bootstrap-ping (i.e.
the ratio of correct newly discovered in-stances to training examples) and precision (i.e.
theproportion of correct discoveries among all those re-turned by the algorithm).
We compute this score todecide which iteration has yielded the optimal dis-criminative model.3 Building an Event-noun LexiconWe applied the bootstrapping algorithm to the taskof discovering event nouns from a source lexicon.38Event nouns are words that typically describe theoccurrence, the process, or the result of an event.We first explore the effectiveness of this algorithm,and then describe a method of extracting the optimalmodel.
Top-ranked features in the optimal model areused to find subcategories of event nouns.Experimental Setup The WordNet noun-list ischosen as the source lexicon (Fellbaum, 1998),which consists of 21,512 nouns.
The purpose ofthis task is to explore the separability of event nounsfrom this collection.typical suffixes: appeasement, arrival, renewal,construction, robbery, departure, happeningirregular cases: birth, collapse, crash, death, de-cline, demise, loss, murderTable 1: Examples of event-nouns in initial training set.We manually selected 15 event nouns and 215non-event nouns for the seed set.
Event-noun exam-ples are representative of subcategories within thesemantic class, as well as their commonly seen mor-phological structures (Table 1).
Non-event examplesare primarily exceptions to morphological regulari-ties (to prevent the algorithm from overly relying onaffix features), such as, anything, ambition, diago-nal.
The subset of all contextual and morphologicalfeatures represented by both event and non-event ex-amples are used to bootstrap the training process.Event Noun Discovery Reducing the number ofworking features is often an effective strategy intraining a perceptron.
We experimented with twocut-off thresholds for features: in Trial 1, featuresmust appear at least 10 times (55,354 remaining);in Trial 2, features must appear at least 15 times(35,457 remaining).We set the training process to run for 20 iterationsin both trials.
Classification results of each iterationwere collected.
We expect the algorithm to discoverfew event nouns during early iterations.
But withnew instances found in each subsequent iteration,it ought to utilize newly seen features and discovermore.
Figure 1 confirms our intuition.The number of classified event-noun instances in-creased sharply at the 15th iteration in Trial 1 and the11th iteration in Trial 2, which may suggest overfit-ting of the training examples used in those iterations.If so, this should also correlate with an increase oferror rate in the classification results (error rate de-fined as the percentage of non-event nouns identi-fied as event nouns in all discovered event nouns).We manually marked all misclassified event noun in-stances for the first 10 iterations in both trials.
Theerror rate in Trial 2 is expected to significantly in-crease at the 10th iteration, while Trial 1 should ex-hibit little increase in error rate within this interval.This expectation is confirmed in Figure 2.Extracting the Optimal Model We further pur-sued the task of finding the iteration that has yieldedthe best model.
Optimality is judged from two as-pects: 1) the number of correctly identified eventnouns should be significantly larger than the size ofseed examples; and 2) the accuracy of classificationresults should be relatively high so that it takes lit-tle effort to clean up the result.
Once the optimalmodel is determined, we analyze its most heavilyweighted features and try to derive finer categoriesfrom them.
Furthermore, the optimal model couldbe used to discover new instances from other sourcelexicons in the future.We define a measure called the BootstrappingScore (BS), serving a similar purpose as an F-score.BS is computed as in Formula (1).BS = 2 ?BR ?
PrecisionBR + Precision .
(1)Here the Bootstrapping Rate (BR) is computed as:BR = |NEW ||NEW |+ |SEED| , (2)where |NEW | is the number of correctly identi-fied new instances (seed examples excluded), and|SEED| is the size of seed examples.
The rateof bootstrapping reveals how large the effect of thebootstrapping process is.
Note that BR is differentfrom the classic measure recall, for which the totalnumber of relevent documents (i.e.
true event nounsin English) must be known a priori ?
again, thisknowledge is what we are discovering.
The scoreis a post hoc solution; both BR and precision arecomputed for analysis after the algorithm has fin-ished.
Combining Formulas (1) and (2), a higherBootstrapping Score means better model quality.Bootstrapping scores of models in the first ten it-erations are plotted in Figure 3.
Model quality in395 10 15 200200040006000800010000IterationNumber of Event Nouns DiscoveredTrial 1Trial 2Figure 1: Classification rate2 4 6 8 100.050.100.150.200.250.30IterationError rateTrial 1Trial 2Figure 2: Error rate2 4 6 8 100.820.840.860.880.900.920.94IterationBootstrapping ScoreTrial 1Trial 2Figure 3: Bootstrapping score1 .
.
.
6 .
.
.
10incorrect 5 .
.
.
32 .
.
.
176correct 79 .
.
.
236 .
.
.
497error rate 5.9% .
.
.
11.9% .
.
.
26.2%score 87.0% .
.
.
90.8% .
.
.
83.8%Table 2: From iterations 1 to 10, comparison betweeninstance counts, error rates, and bootstrapping scores asthe measure of model quality.Trial 2 is better than in Trial 1 on average.
In ad-dition, within Trial 2, Iteration 6 yielded the bestdiscriminative model with a bootstrapping score of90.8%.
Compared to instance counts and error ratemeasures as shown in Table 2, this bootstrappingscore provides a balanced measure of model qual-ity.
The model at the 6th iteration (hereafter, Model6) can be considered the optimal model generatedduring the bootstrapping training process.Top-ranked Features in the Optimal Model Inorder to understand why Model 6 is optimal, weextracted its top 15 features that activate the event-noun target in Model 6, as listed in Table 3.
Inter-estingly, top-ranked features are all contextual ones.In fact, in later models where the ranks of mor-phological features are boosted, the algorithm per-formed worse as a result of relying too much onthose context-insensitive features.Collectively, top-ranked features define the con-textual patterns of event nouns.
We are interestedin finding semantic subcategories within the set ofevent nouns (497 nouns, Trial 2) by exploiting thesefeatures individually.
For instance, some events typ-ically happen to people only (e.g.
birth, betrayal),while others usually happen to inanimate objects(e.g.
destruction, removal).
Human actions can alsobe distinguished by the number of participants, suchas group activities (e.g.
election) or individual ac-tivities (e.g.
death).
It is thus worth distinguishingnouns that describe different sorts of events.Manual Classification We extracted the top 100contextual features from Model 6 and groupedthem into feature classes.
A feature class con-sists of contextual features sharing similar mean-ings.
For instance, A COUNTRY MAY UNDERGOand A STATE MAY UNDERGO both belong to theclass social activity.
For each feature class, we enu-merate all words that correspond to its feature in-stances.
Examples are shown in Table 4.Not all events can be unambiguously classifiedinto one of the subcategories.
However, this is alsonot necessary because these categories overlap withone another.
For example, death describes an eventthat tends to occur both individually and briefly.
Inaddition to the six categories listed here, new cate-gories can be added by creating more feature classes.Automatic Clustering Representing each noun asa frequency vector over the top 100 most discrim-inating contextual features, we employed k-meansclustering and compared the results to our manuallycrafted subcategories.Through trial-and-error, we set k to 12, with thesmallest resulting cluster containing 2 nouns (inter-pretation and perception), while the biggest result-ing cluster contained 320 event nouns (that seemedto share no apparent semantic properties).
Otherclusters varied from 5 to 50 words in size, with ex-amples shown in Table 5.The advantage of automatic clustering is that theresults may reflect an English speaker?s impressionof word similarity gained through language use.
Un-40a person-or-organization may undergo a a state may undergo a a can be attempteda country may undergo a a child may have a a can be for a countrya company may undergo a a project may undergo a authority may undergo aan explanation can be for a an empire may undergo a a war may undergo adays may have a a can be abrupt a can be rapidTable 3: Top 15 features that promote activation of the event-noun target, ranked from most weighted to least.human events: adoption, arrival, birth, betrayal,death, development, disappearance, emancipation,funeral .
.
.events of inanimate objects: collapse, construc-tion, definition, destruction, identification, incep-tion, movement, recreation, removal .
.
.individual activities: birth, death, execution, fu-neral, promotion .
.
.social activities: abolition, evolution, federation,fragmentation, invasion .
.
.lasting events: campaign, development, growth,trial .
.
.brief events: awakening, collapse, death, mention,onset, resignation, thunderstorm .
.
.Table 4: Six subcategories of event nouns.fortunately, the discovered clusters do not typicallycome with the same obvious semantic properties aswere defined in manual classification.
In the exam-ple given above, neither of Cluster 1 and Cluster 3seems to have a centralized semantic theme.
ButCluster 2 seems to be mostly about human activities.Comparison with WordNet To compare our re-sults with WordNet resources, we enumerated allchildren of the gloss ?something that happens at agiven place and time?, giving 7655 terms (phrasesexcluded).
This gave a broader range of event nouns,such as proper nouns and procedures (e.g.
9/11, CT,MRI), onomatopoeias (e.g.
mew, moo), and wordswhose event reading is only secondary (e.g.
pic-ture, politics, teamwork).
These types of words tendto have very different contextual features from whatour algorithm had discovered.While our method may be limited by the choice ofseed examples, we were able to discover event nounsnot classified under this set by WordNet, suggest-ing that the discovery mechanism itself is a robustone.
Among them were low-frequency nouns (e.g.crescendo, demise, names of processes (e.g.
absorp-Cluster 1 (17): cancellation, cessation, closure,crackle, crash, demise, disappearance, dismissal, dis-solution, division, introduction, onset, passing, resig-nation, reversal, termination, transformationCluster 2 (32): alienation, backing, betrayal, contem-plation, election, execution, funeral, hallucination,imitation, juxtaposition, killing, mention, moulding,perfection, prosecution, recognition, refusal, removal,resurrection, semblance, inspection, occupation, pro-motion, trial .
.
.Cluster 3 (7): development, achievement, arrival,birth, death, loss, survivalTable 5: Examples resulting from automatic clustering.tion, evolution), and particular cases like thunder-storm.4 Extension to Other Semantic CategoriesTo verify that our bootstrapping algorithm was notsimply relying on KNEXT?s own event classifica-tion heuristics, we set the algorithm to learn thedistinction between physical and non-physical ob-jects/entities.
(Non-)Physical-object Nouns 15 physical-object/entity nouns (e.g.
knife, ring, anthropologist) and34 non-physical ones (e.g.
happiness, knowledge)were given to the model as the initial training set.At the 9th iteration, the number of discovered physi-cal objects (which form the minority group betweenthe two) approaches 2,200 and levels off.
We ran-domly sampled five 20-word groups (a subset ofthese words are listed in Table 6) from this entireset of discovered physical objects, and computed anaverage error rate of 4%.
Prominent features of themodel at the 9th iteration are shown in Table 7.5 Related WorkThe method of using distributional patterns in alarge text corpus to find semantically related En-41heifer, sheriff, collector, hippie, accountant, cape, scab,pebble, box, dick, calculator, sago, brow, ship, ?john,superstar, border, rabbit, poker, garter, grinder, million-aire, ash, herdsman, ?cwm, pug, bra, fulmar, *cam-paign, stallion, deserter, boot, tear, elbow, cavalry,novel, cardigan, nutcase, ?bulge, businessman, cop, fig,musician, spire, butcher, dog, elk, .
.
.Table 6: Physical-object nouns randomly sampled fromresults; words with an asterisk are misclassified, oneswith a question mark are doubtful.a male-individual can be a a can be smalla person can be a a can be largea can be young a can be german-S*morphological feature a can be britisha can be old a can be goodTable 7: Top-10 features that promote activation of thephysical-object target in the model.glish nouns first appeared in Hindle (1990).
Roarkand Charniak (1998) constructed a semantic lexiconusing co-occurrence statistics of nouns within nounphrases.
More recently, Liakata and Pulman (2008)induced a hierarchy over nominals using as featuresknowledge fragments similar to the sort given byKNEXT.
Our work might be viewed as aiming forthe same goal (a lexico-semantic based partition-ing over nominals, tied to corpus-based knowledge),but allowing for an a priori bias regarding preferredstructure.The idea of bootstrapping lexical semantic prop-erties goes back at least to Hearst (1998), where theidea is suggested of using seed examples of a rela-tion to discover lexico-syntactic extraction patternsand then using these to discover further examplesof the desired relation.
The Basilisk system devel-oped by Thelen and Riloff (2002) almost paralleledour effort.
However, negative features ?
featuresthat would prevent a word from being classified intoa semantic category ?
were not considered in theirmodel.
In addition, in scoring candidate words, theiralgorithm only looked at the average relevance ofsyntactic patterns.
Our perceptron-based algorithmexamines the combinatorial effect of those patterns,which has yielded results suggesting improved ac-curacy and bootstrapping efficacy.Similar to our experiments here using k-means,Lin and Pantel (2001) gave a clustering algorithmfor iteratively building semantic classes, using asfeatures argument positions within fragments froma syntactic dependency parser.6 ConclusionWe have presented a bootstrapping approach for cre-ating semantically tagged lexicons.
The method caneffectively classify nouns with contrasting semanticproperties, even when the initial training set is a verysmall.
Further classification is possible with bothmanual and automatic methods by utilizing individ-ual contextual features in the optimal model.AcknowledgmentsThis work was supported by NSF grants IIS-0328849 and IIS-0535105.ReferencesBNC Consortium.
2001.
The British National Corpus,version 2 (BNC World).
Distributed by Oxford Uni-versity Computing Services.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.Marti A. Hearst.
1998.
Automated discovery of Word-Net relations.
In (Fellbaum, 1998), pages 131?153.Donald Hindle.
1990.
Noun classification frompredicate-argument structures.
In ACL.Alexandre Klementiev and Dan Roth.
2006.
Weaklysupervised named entity transliteration and discoveryfrom multilingual comparable corpora.
In ACL.Maria Liakata and Stephen Pulman.
2008.
Auto-matic Fine-Grained Semantic Classification for Do-main Adaption.
In Proceedings of Semantics in TextProcessing (STEP).Dekang Lin and Patrick Pantel.
2001.
Induction of se-mantic classes from natural language text.
In KDD.Brian Roark and Eugene Charniak.
1998.
Noun-phraseco-occurrence statistics for semi-automatic semanticlexicon construction.
In ACL, pages 1110?1116.Frank Rosenblatt.
1958.
The perceptron: A probabilisticmodel for information storage and organization in thebrain.
Psychological Review, 65(6):386?408.Lenhart K. Schubert.
2002.
Can we derive general worldknowledge from text?
In HLT.Michael Thelen and Ellen Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extractionpattern contexts.
In EMNLP.Benjamin Van Durme, Ting Qian, and Lenhart Schubert.2008.
Class-driven Attribute Extraction.
In COLING.42
