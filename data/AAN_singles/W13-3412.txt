Proceedings of the Fourth Workshop on Teaching Natural Language Processing, pages 77?84,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTeaching the Basics of NLP and ML in an Introductory Course toInformation ScienceApoorv AgarwalDepartment of Computer ScienceColumbia UniversityNew York, USAapoorv@cs.columbia.eduAbstractIn this paper we discuss our experience ofteaching basic Natural Language Processing(NLP) and Machine Learning (ML) in an in-troductory course to Information Science.
Wediscuss the challenges we faced while incor-porating NLP and ML to the curriculum fol-lowed by a presentation of how we met thesechallenges.
The overall response (of stu-dents) to the inclusion of this new topic to thecurriculum has been positive.
Students thissemester are pursuing NLP/ML projects, for-mulating their own tasks (some of which arenovel and presented towards the end of the pa-per), collecting and annotating data and build-ing models for their task.1 IntroductionAn introductory course to Information Science hasbeen taught at Columbia University for over adecade.
The main goal of the course is to intro-duce undergraduates at our university to applica-tions of Computer Science.
For most students, thisis their first course in the Computer Science de-partment.
The course has no pre-requisites suchas higher mathematics or programming.
In fact,through a survey we found that about 10% of theclass did not know the meaning of a programminglanguage.Traditionally, the computer science applicationsthat have been taught in this course include HTML(creating a website), Spreadsheets, Database Sys-tems, World Wide Web and the Internet, Algorithmsand programming in Python.
Given the importanceof understanding how humans are building smartmachines and the amount of excitement aroundNatural Language Processing (NLP) and MachineLearning (ML) applications, we decided to include asocial media analysis application ?
sentiment analy-sis of Twitter ?
in the curriculum last year.
The over-all response to this inclusion has been positive.
Oneoutcome of this inclusion is that the students are nowable to build basic models for popular NLP applica-tions such as sentiment analysis of Twitter, spam de-tection of emails, and document classification.
Buta more significant outcome of this inclusion is thatthe students seemed to have gained a general idea ofhow machine learning works, as a result, they findWatson playing Jeopardy!
against the humans, andGoogle?s self-driving car less ?magical?.There were two main challenges in incorporatingan introduction to NLP and ML to the curriculum:(1) we wanted to include this topic without compro-mising the traditionally covered material, which puta constraint on the number of lectures we could usefor introducing NLP and ML and (2) we were re-quired to abstract away from the inherently math-ematical jargon used to explain NLP and ML.
Inthis paper we present the way we met these chal-lenges.
We present our lecture, homework and ex-amination design that enabled us to get some of themost important ideas of NLP and ML across in onelecture.
The students performed exceptionally wellon the NLP/ML section of the examination.
More-over, some students are pursuing projects related tothese topics formulating their own tasks, collectingand annotating data, and building models to answertheir hypotheses.
These are signs that undergrad-uates with a broad spectrum of educational back-grounds and interests are not only capable of tack-ling the basics of NLP and ML, but that they mayeven be doing so with relish.77There has been successful and fruitful effort byresearchers in the NLP community to share theirexperiences and course design through this work-shop in the past (Lee, 2002; Eisner, 2002; Liddyand McCracken, 2005; Freedman, 2005; Zinsmeis-ter, 2008).
Steven Bird (2008) notes that an intro-ductory course needs to serve some common, basicneeds ?
?For some students, it will be the first stepin a pathway leading to specialized courses, grad-uate research, or employment in this field.
For stu-dents who do not continue, the introductory coursewill be their main exposure to the field.
Naturally,this course is also a prime opportunity to promotethe field to newcomers and encourage them to pur-sue advanced studies in this area.?
We share thesame motivation (as (Bird, 2008)) ?
our target audi-ence is in fact ?newbies?
in Computer Science, whomay or may not continue with more advanced topicsin Computer Science, in which case this course willbe their main exposure to the field and thus offers agreat opportunity for us to promote the field.The rest of the paper is structured as follows: Insection 2, we give details of the course and stu-dent demographics.
Section 3 presents the NLP/MLlecture organization and content.
In section 4 wepresent the problems on the mid term examinationand performance of the students on the NLP/MLpart of the exam.
Section 5 describes some of themost interesting student projects that have come outof the course.
We conclude in Section 6.2 Student demographicsStudents enrolling in this introductory course on In-formation Science come from a wide variety of aca-demic backgrounds.
A majority of the class is un-dergraduates who have never taken any course in theComputer Science department before.
The course istaught over a period of 4 months, consisting of 24lectures of 75 minute duration each.Figure 1 and Figure 2 present a distribution of 61students based on their college rank and major (aca-demic background) respectively.Figure 1 shows that a large majority of studentsare freshman and sophomores (50%).
While thesestudents have an idea of what they would like tomajor in, they are not required to finalize their ma-jors until the final semester of their sophomore year.This introductory course is therefore a great oppor-tunity to promote the field by exposing the studentsto some of the most exciting applications of Com-puter Science.
In the first class of the course, weshowed the students a video of Watson playing thepopular gameshow Jeopardy!
against the humans.It was surprising that only a few students knew ofWatson.
But even the ones who knew about it wereexcited and curious to learn how Watson actuallyworks.Figure 1: Student distribution based on College rank.20% Freshman, 30% Sophomore, 16% Junior, 21% Se-nior, 8% Graduate and 5% OtherFigure 2 presents a distribution of students basedon the majors they are pursuing or intend to pursue.For this figure, we grouped the reported majors intothe following broader categories: Math/Engineering(Math, Computer Science, Information Science,Electrical Engineering), Basic sciences (Biology,Zoology, Chemistry, Physics, Neuroscience), Polit-ical Science, Social Science, Language (German,French, Yiddish, English, Linguistics), Arts and Hu-manities (including Literature, Film, Theatre), Re-gional studies (Asian, American, Middle Eastern),and Other (Finance, History, International Relations,Marketing, Philosophy, Psychology).The distribution of majors shows that the studentscome from a wide variety of academic backgrounds.Only about 12% of the students are pursuing or in-tend to pursue a major in Math/Engineering.
Thereis a large majority of students who have only taken78Figure 2: Student distribution based on majors.
16%Economics, 14% Political Science, 14% Basic Sciences,12% Math/Engineering, 11% Arts and Humanities, 11%Other, 9% Social Science, 8% Language, 6% RegionalStudies.SAT level mathematics.
The majority of these stu-dents have never used any programming languagebefore.
Therefore, one of the main challenges ofteaching this course, especially introducing NLP andML, was to abstract away from mathematical jar-gon and convey the fundamentals through the useof analogies and concrete illustrations.3 Lecture organization and contentTo meet the aforementioned challenges, we spentone lecture introducing the class to some basic con-cepts of NLP and ML.
Through homework and ex-amination, we introduced the students to more NLPapplications that also helped them appreciate thestrengths and weaknesses of the simple ML tech-nique we introduced in class.
We geared the Pythonpart of course towards text processing preparing thestudents to implement an end-to-end pipeline of apopular NLP application on another homework.We started the lecture by introducing a concreteand motivating NLP application ?
sentiment analy-sis of Twitter.
In line with Reva Freedman?s (2005)observation, we found that starting with a concreteapplication is important.
We first defined sentimentanalysis as the task of building a machine that is ableclassify the polarity of opinions in text into one oftwo categories: positive and negative.1 We moti-vated this application by briefly discussing some ofits use cases: predicting the outcome of a presiden-tial election, gauging how a company or a productof a company is performing in the market, findingon average how people are feeling based on gender,location, age and weather.2After posing the question ?
how would a machinelearn to predict if a tweet has a positive or a nega-tive sentiment ?
we first drew an analogy of how hu-mans learn new concepts.
Humans learn through ex-amples and counter-examples.
When we see a newobject or learn a new concept, our instinct is to com-pare the new with the familiar.
Our first attempt is tofind similarities and dissimilarities between this newobject with the objects we have already seen.
Simi-larly, to train a machine, we first need to provide itwith some labeled examples.
For the task at hand,examples are tweets and their labels are manuallyannotated sentiment polarity (positive or negative).Using these training examples, the machine learnspatterns of words that signify a particular sentiment.We started with a small list of words, calling them?features?.
The training data and features are pre-sented in Table 1.
We asked the students to fill outeach cell in Table 1 by putting a 1 if a tweet containsa particular word and 0 if it does not contain thatword.
We mentioned that this process is called ?fea-ture extraction?, in which we convert unstructureddata into a structured representation.
This represen-tation is structured because each tweet is representedas an ordered and fixed list of features.We asked the students how they would calculatethe similarity between two tweets.
And we got anobvious answer ?
count the number of words theyhave in common.The next question we asked was ?how might themachine calculate the similarity using the structuredrepresentation??
The answer to this question wasless obvious but once we gave them the formula,1We defined the italicized words and gave examples to helpstudents understand the definitions.
We intentionally kept thedefinition of sentiment analysis simple and restricted to classi-fying polarity of opinions into positive and negative categories.2http://www.wefeelfine.org79Tweet ID Tweet good bad not pretty great LabelT1 It?s a good day :) 1 0 0 0 0 +1T2 The weather is pretty bad 0 1 0 1 0 -1T3 Alice is pretty 0 0 0 1 0 +1T4 Bieber is not all that great 0 0 1 0 1 -1S1 It is a good day for biking 1 0 0 0 0 ?S2 The situation is not pretty 0 0 1 1 0 ?S3 Such a great show :) 0 0 0 0 1 ?Table 1: Training and test data used in class to illustrate how a machine will learn to predict the polarity of tweets.the students were able to grasp it quickly.
We in-troduced the formula as a bit-wise multiplication oflist of features followed by the summation of the re-sulting bits.Sim(T, S) =d?i=1ti ?
siwhere T, S are tweets, d is the number of features inthe list of features, ti, si are the ith bit of tweets Tand S respectively.The next question we asked was given a tweet,whose polarity is unknown (an unseen tweet), howmight they use the training data to predict its po-larity.
This was a harder question, and though wedid not expect an answer, we posed this questionnonetheless to serve as a pause in the lecture andindicate that a key idea was coming.Before revealing the secret sauce, we made theanalogy of how humans would do a similar task.Given two kinds of fish, say sea bass and salmon, theway we would classify a new fish into one of thesetwo categories would be by comparing ?features?
ofthe new fish with the features of sea bass and withthe features of salmon followed by observing if thenew fish is ?closer?
to sea bass or salmon.
Similarly,the machine will compare the list of features of theunseen tweet with the list of features of the positiveand the list of features of the negative tweets andcompute a similarity score that will allow the ma-chine to make a prediction about the polarity of thisunseen tweet.We then introduced the following formula:s =N?i=1Sim(Ti, S)?
Labeliwhere N is the total number of training examples,Ti is the ith training example, S is the test tweet andLabeli is the human annotated polarity of Ti.The machine uses this score to make a final pre-diction.
If the score is less than or equal to 0, the ma-chine predicts the polarity of the tweet as negative.If the score is greater than 0, the machine predictsthe polarity of the tweet as positive.We illustrated this by working out a few examplesof how the machine will go about predicting the po-larity of the following unseen tweets:1.
?It is a good day for biking?2.
?The situation is not pretty?3.
?Such a great show :)?We worked out the first example on the board andasked the students to work out the remaining two ontheir own.
Following is the way in which we workedout the first example on the board.1.
First the machine converts the test tweet S1= ?It is a good day for biking?
into the samestructured representation as that of the trainingtweets.
The list of features for S1 is [1,0,0,0,0](see Table 1).2.
Then the machine compares the list of featuresfor S1 with each of the training tweets as fol-lows:(a) Comparing the list of features for tweetsT1 and S1, the machine finds the bit-wise multiplication of their feature lists[1, 0, 0, 0]?
[1, 0, 0, 0] = [1, 0, 0, 0].
Thenthe machine adds all the bits 1+0+0+0 =1.
We point out there is only one word in80common between the two tweets (namely?good?).
The similarity score between thefirst training example and the test examples1 = 1?
(+1) = 1.
(b) Similarly, comparing the feature lists forT2 and S1, we get a similarity score s2 =([0, 1, 0, 1, 0]?
[1, 0, 0, 0, 0])?
(?1) = 0(c) Comparing the feature lists for T3 andS1, we get a similarity score s3 =([0, 0, 0, 1, 0]?
[1, 0, 0, 0, 0])?
(+1) = 0(d) Finally, comparing the feature lists for T4and S1, we get a similarity score s4 =([0, 0, 1, 0, 0]?
[1, 0, 0, 0, 0])?
(?1) = 03.
Next, the machine adds all the similarity scorestogether to get an aggregated score for the testtweet s = s1 + s2 + s3 + s4 = 1.
Since s > 0,the machine predicts this test tweet T1, ?It is agood day for biking?, has a positive polarity.Having the students work out the other two exam-ples in class on their own and interacting with theirneighbors, they began to see the meaning of patternrecognition.
Bringing their attention to Table 1, wepointed out that the word ?good?
is associated witha positive polarity by virtue of appearing in a posi-tively labeled tweet.
The word ?pretty?
is associatedwith a neutral polarity because it appears both in apositive and in a negative tweet.
This means thatthe machine has learned that it cannot make a pre-diction simply based on the word ?pretty?.
The testtweet ?The situation is not pretty?
makes this pointexplicit.
This tweet is classified correctly as negativebut only because of the presence of the word ?not?,which appears in a negative tweet.In summary, through these worked out examples,we were able to drive home the following points:1.
The machine automatically learns the connota-tion of words by looking at how often certainwords appear in positive and negative tweets.2.
The machine also learns more complex patternsthat have to do with the conjunction and dis-junction of features.3.
The quality and amount of training data is im-portant ?
for if the training data fails to encodea substantial number of patterns important forclassification, the machine is not going to learnwell.Students asked the following questions, whichhelped us build on the aforementioned points.31.
Good and great are synonyms.
Shouldn?t wecount them as one feature?2.
Could we create and use a dictionary that liststhe prior polarity of commonly used words?3.
If the prediction score for the tweet is high,does that mean we the machine is more con-fident about the prediction?4.
In this approach, the sequence of words doesnot matter.
But clearly, if ?not?
does not negatethe words containing opinion, then won?t themachine learn a wrong pattern?5.
If we have too many negative tweets in ourtraining data (as compared to the positivetweets), then would the machine not be pre-disposed to predict the polarity of an unseentweet as negative?Building on these concepts, we had the studentswork through an end-to-end example of classifyingmovie reviews into positive and negative on theirhomework.
What appeared to be a promising ma-chine learning technique in class, seemed to fail forthis task.
They realized that classifying movie re-views is much harder because of the words usedin plot descriptions that mislead the classifier.
Weused examples from the seminal paper by Peter Tur-ney (2002) for this homework problem.4 Problem and performance on the Midterm examinationWe further built on the fundamentals, by askingthe students to classify emails into ?important?
and?unimportant?
by using the same machine learningtechnique (used for sentiment analysis of Twitter)on their mid term examination.
This helped themsee that the ML technique learned in class may beused, in general, for other NLP applications.
AsHeike Zinsmeister (2008) notes, redundancy and it-erative re-introduction could be helpful for students,3Questions are reformulated for succinctness and clarity.81we found that by having the students work out differ-ent NLP applications using the same ML approachhelped them grasp the concepts better and appreci-ate the strengths and weaknesses of this simple MLapproach.Table 2 presents the training data along with thefeatures.
Following are the problems from their mid-term examination.1.
Extract features from the emails in the trainingdata, i.e.
fill Table 2 with ones and zeros.
(5points)2.
What will be the prediction of the machine forthis new incoming email ?It is important thatyou register for this meeting.
?
your phd advi-sor?.
Say if, this is an important email, is theprediction made by your machine correct?
(4 +1 points)3.
What will be the prediction of the machine forthis new incoming email ?Bill, what up??.
Sayif, this is an unimportant email, is the predictionmade by your machine correct?
(4 + 1 points)4.
What is the performance of your current ma-chine learning model on all the test data?
(2points)5.
What other feature(s) will you add to the listof features to improve the performance of yourmachine learning model?
How will this changethe prediction of the two incoming emails?What will be the performance of your newmodel?
(3 + (2 + 2) + 1 points)For problem 5 on the exam, most of the studentscame up with the answer of adding the words ?your?and ?advisor?
to the list of features.
But some stu-dents devised more complex features.
One studentproposed to add the capitalization feature to distin-guish between ?Bill?
and ?bill?.
Another student ex-tended this feature to additionally check if ?Bill?
isa proper noun or not.
The only type of feature weintroduced in class was the binary occurrence andnon-occurrence of words.
It was promising to seethe students expand on the preliminary feature set tocreate novel and more advanced set of features.The duration of the exam was 75 min and it con-sisted of 6 extended problems.
The first two prob-lems were compulsory and the students were askedto do any two out of the remaining four problems(NLP/ML, Logic Gates, Database Design, MachineInstructions).
Each of the remaining four problemswas worth 25 points.
Table 3 shows their perfor-mance on the four problems.
The table shows thatthe students did extremely well on the NLP/MLproblem ?
averaging 20.54 out of 25 with a stan-dard deviation of 4.46.
Note, students unanimouslyattempted the NLP/ML part of the exam ?
only 2 stu-dents scored a zero for this problem as compared to17, 11 and 23 students, who scored a zero on LogicGates, Database Design and Machine Instructionsrespectively.4The performance of students on the mid term ex-amination assured us that they were comfortablewith the terminology and the process of machinelearning.
We decided to build on this foundation byintroducing them to basic text processing, indexing,and stemming in the Python part of the course.
Ontheir Python homework, they implemented a com-plete pipeline, starting from creating a vocabularyfrom the training data, then extracting features, andfinally implementing a simple version of the per-ceptron algorithm to predict sentiment polarity oftweets.
The average on this homework was 87.8 outof 115 with about 60% of the students scoring over100 points.5 Student project descriptionsThe most exciting outcome of including NLP andML to the course has been that some students havesigned up for a course project in their demandingcurriculum.
For the course project, the students wereasked to formulate their own tasks, collect and anno-tate data and build machine learning models for theirtasks.
Following are the two most novel task formu-lations (in students?
own language) followed by a listof other projects.5Detecting liberal or conservative biases (AllenLipson and Tyler H. Dratch): Critics on both sidesof the political spectrum often accuse their adver-saries of employing biased language to promote4The grading was generous and students were given partialcredit for their attempt.
Therefore, we approximate the numberof students who attempted a problem by counting the numberof students who scored a zero on that problem.5The project reports are available atwww.cs.columbia.edu/?apoorv/Teaching/ProjectReports82Email ID Email meeting register unsubscribe bill LabelE1 Meeting at 4, hurry!
?
your advisor.
... ... ... ... +1E2 Free event.
To register click here.
To un-subscribe click here.... ... ... ... -1E3 According to our register, your bill is yetto be paid... ... ... ... +1E4 Register for this useless meeting.
... ... ... ... -1Table 2: Structured representation of the training data or examples from which the machine will learn to differentiatebetween important and unimportant emails.Problem Average Std-dev Median Count (Score < 5) Count (Score == 0)NLP/ML 20.54 4.46 22 2 2Logic Gates 16.94 6.48 20 20 17Database Design 13.63 6.48 14 14 11Machine Instructions 12.8 6.81 14.5 27 23Table 3: Distribution of scores for 53 students on problems on the mid term exam.
Students were required to do anytwo out of these four problems.
Each problem was worth 25 points.
Count (Score < 5) means the number of studentsout of 53 that scored less than 5 points on a problem.
Average, standard deviation and median values exclude studentswho scored a 0.an agenda.
Nowhere in politics is the usage oflanguage more contentious than in the immigrationdebate.
Conservatives lambast ?illegal aliens?
;liberals defend ?undocumented workers.?
Liberalspromote a ?path to citizenship?
; conservatives decry?criminal amnesty?.
But is this bias also presentin major news sources, the supposedly impartialsources of society?s information?
Or are papers likethe New York Times and the Wall Street Journalfirmly on opposite sides of the immigration debate?We want to put this question to the test.
We areconstructing a machine learning algorithm to detectliberal or conservative biases on immigration in theNew York Times and the Wall Street Journal.The Bechdel Test (Michelle Adriana MarguerCheripka and Christopher I.
Young): The BechdelTest is a measure by which it is possible to iden-tify gender bias in fiction.
In order to pass the test,the work of fiction must pass three criteria: theremust be two main female characters, they must havea conversation, and they must be speaking aboutsomething other than a man.
Though primarily usedin film, the Bechdel test can also be applied to lit-erature.
In previous Bechdel experiments, the re-sults indicated traditional, heteronormative pattern.While a text does not necessarily need to be explic-itly feminist in order to pass the test, the test itself isan important gauge for the social roles that societiesuphold and perpetuate.
This particular experimentwas created in order to determine if this trend wasconsistent across mediums.
Considering that chil-dren?s books provide the foundation for a person?sinteraction with literature, the test could identify pat-terns that emerge from an early stages of literatureand address their future impact.Some of the other project proposals are as fol-lows: Gim Hong Lee built a sentiment analysis en-gine to rate a professor based on his/her reviewsavailable on CULPA.info (Columbia UndergroundListing of Professor Ability).
Xueying (Alice) Linbuilt a recommendation system for Yelp.
She ac-quired the data-set from kaggle.com.6 A groupof three students (Roni Saporta, Moti Volpo andMichal Schestowitz) experimented with the breastcancer data-set available at the UCI data-repository.7They used scikit-learn?s8 implementation of the lo-gistic regression algorithm.It is heartening to see that students who had verylimited (or no) idea about how machines learn at thestart of the course are now formulating tasks and at-6http://www.kaggle.com/c/yelp-recruiting7http://archive.ics.uci.edu/ml/8http://scikit-learn.org/stable/83tempting to build their own machine learning mod-els.
What they still do not know, we believe, is thatthey are mapping each document into a finite dimen-sional feature space and calculating dot products be-tween feature vectors to calculate similarity betweendocuments.
While this math vocabulary is probablyrequired to make more progress and dive deeper intoNLP and ML, we believe it is not required to conveythe essence of pattern recognition.6 ConclusionIn this paper, we presented a lecture, homework andexamination design, through which we were able toget some basic ideas of Natural Language Process-ing and Machine Learning across to students whocame from a wide variety of academic backgrounds,majority of whom did not have an advanced mathbackground.
Apart from the challenge of having toabstract away from the inherently mathematical con-cepts, we faced another challenge at the onset of de-signing the lecture ?
we had to deliver the NLP andML material in one or two lectures so that we do notcompromise on the traditionally covered topics.We believe that the lecture, homework and exami-nation design presented in this paper may be used bylecturers teaching introductory course such as oursor by researchers who are interested in presentinga simplified explanation of NLP and ML to generalpopular science audiences.AcknowledgmentsWe would like to thank Kapil Thadani, CaronaeHowell, Kshitij Yadav, Owen Rambow, MeghnaAgarwala, Sara Rosenthal, Daniel Bauer and anony-mous reviewers for useful comments.ReferencesSteven Bird.
2008.
Defining a core body of knowledgefor the introductory computational linguistics curricu-lum.
In Proceedings of the Third Workshop on Issuesin Teaching Computational Linguistics, pages 27?35,Columbus, Ohio, June.
Association for ComputationalLinguistics.Jason Eisner.
2002.
An interactive spreadsheet for teach-ing the forward-backward algorithm.
In Proceed-ings of the ACL-02 Workshop on Effective Tools andMethodologies for Teaching Natural Language Pro-cessing and Computational Linguistics, pages 10?18,Philadelphia, Pennsylvania, USA, July.
Associationfor Computational Linguistics.Reva Freedman.
2005.
Concrete assignments for teach-ing NLP in an M.S.
program.
In Proceedings of theSecond ACL Workshop on Effective Tools and Method-ologies for Teaching NLP and CL, pages 37?42, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Lillian Lee.
2002.
A non-programming introductionto computer science via nlp,ir,and ai.
In Proceed-ings of the ACL-02 Workshop on Effective Tools andMethodologies for Teaching Natural Language Pro-cessing and Computational Linguistics, pages 33?38,Philadelphia, Pennsylvania, USA, July.
Associationfor Computational Linguistics.Elizabeth Liddy and Nancy McCracken.
2005.
Hands-onNLP for an interdisciplinary audience.
In Proceedingsof the Second ACL Workshop on Effective Tools andMethodologies for Teaching NLP and CL, pages 62?68, Ann Arbor, Michigan, June.
Association for Com-putational Linguistics.Peter D. Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifica-tion of reviews.
In the Proceedings of the 40th meet-ing of Association of Computational Linguisitcs (ACL2002).Heike Zinsmeister.
2008.
Freshmen?s CL curriculum:The benefits of redundancy.
In Proceedings of theThird Workshop on Issues in Teaching ComputationalLinguistics, pages 19?26, Columbus, Ohio, June.
As-sociation for Computational Linguistics.84
