RESPONDING TO USER QUERIES IN A COLLABORATIVE ENVIRONMENT*Jennifer ChuDepartment of  Computer  and Information SciencesUniversity of  DelawareNewark, DE 19716, USAInternet: jchu @ cis.udel.eduAbstractWe propose a plan-based approach for respondingto user queries in a collaborative environment.
Weargue that in such an environment, the system shouldnot accept he user's query automatically, but shouldconsider it a proposal open for negotiation.
In this pa-per we concentrate on cases in which the system anduser disagree, and discuss how this disagreement canbe detected, negotiated, and how final modificationsshould be made to the existing plan.1 In t roduct ionIn task-oriented consultation dialogues, the user and ex-pert jointly construct a plan for achieving the user's goal.In such an environment, it is important hat the agentsagree on the domain plan being constructed and on theproblem-solving actions being taken to develop it.
Thissuggests that he participants communicate their disagree-ments when they arise lest the agents work on developingdifferent plans.
We are extending the dialogue under-standing system in \[6\] to include a system that respondsto the user's utterances in a collaborative manner.Each utterance by a participant constitutes a proposalintended to affect he agents' shared plan.
One componentof our architecture, the evaluator, examines the user's pro-posal and decides whether to accept or reject it.
Since theuser has knowledge about his/her particular circumstancesand preferences that influence the domain plan and howit is constructed, the evaluator must be a reactive plannerthat interacts with the user to obtain information usedin building the evaluation meta-plan.
Depending on theevaluation, the system can accept or reject he proposal, orsuggest what it considers to be a better alternative, leadingto an embedded negotiation subdialogue.In addition to the evaluator, our architecture consists ofa goal selector, an intentional planner, and a discourserealizer.
The goal selector, based on the result of theevaluation and the current dialogue model, selects anappropriate intentional goal for the system to pursue.
Theintentional planner builds a plan to achieve the intentionalgoal, and the discourse realizer generates utterances toconvey information based on the intentional plan.This paper describes the evaluator, concentrating oncases in which the system and user disagree.
We show howthe system determines that the user's proposed additionsare erroneous and, instead of directly responding to theuser's utterances, conveys the disagreement.
Thus, ourwork contributes to an overall dialogue system by 1)extending the model in \[6\] to eliminate the assumption thatthe system will automatically answer the user's questionsor follow the user's proposals, and 2) capturing the notion*This material isbased upon work supported by the NationalScience Foundation under Grant No.
IRI-9122026.of cooperative r sponses within an overall collaborativeframework that allows for negotiation.2 The  Tr ipar t i te  Mode lLambert and Carberry proposed a plan-based tripartitemodel of expert/novice onsultation dialogue which in-cludes a domain level, a problem-solving level, and adiscourse level \[6\].
The domain level represents he sys-tem's beliefs about the user's plan for achieving somegoal in the application domain.
The problem-solvinglevel encodes the system's beliefs about how both agentsare going about constructing the domain plan.
The dis-course level represents the system's beliefs about bothagents' communicative actions.
Lambert developed aplan recognition algorithm that uses contextual knowl-edge, world knowledge, linguistic clues, and a libraryof generic recipes for actions to analyze utterances andconstruct a dialogue model\[6\].Lambert's ystem automatically adds to the dialoguemodel all actions inferred from an utterance.
However,we argue that in a collaborative environment, he systemshould only accept he proposed additions if the systembelieves that they are appropriate.
Hence, we separatethe dialogue model into an existing dialogue model and aproposed model, where the former constitutes the sharedplan agreed upon by both agents, and the latter the newlyproposed actions that have not yet been confirmed.Suppose arlier dialogue suggests that the user hasthe goal of getting a Master's degree in CS (Get-Masters(U, CS)).
Figure 1 illustrates the dialogue modelthat would be built after the following utterances by Lam-bert's plan recognition algorithm odified to accommo-date the separation of the existing and proposed ialoguemodels, and augmented with a relaxation algorithm torecognize ill-formed plans\[2\].U: I want o satisfy my seminar course requirement.Who's teaching CS689?3 The  Eva luatorA collaborative system should only incorporate proposedactions into an existing plan if they are considered appro-priate.
This decision is made by the evaluator, which willbe discussed in this section.
This paper only considerscases in which the user's proposal contains an infeasibleaction (one that cannot be performed) or would result inan ill-formed plan (one whose actions do not contributeto one another as intended)\[9\].We argue that the evaluator, in order to check forerroneous plans/goals, only needs to examine actions inthe proposed model, since actions in the existing modelwould have been checked when they were proposed.When a chain of actions is proposed, the evaluator startsexamining from the top-most action so that the mostgeneral action that is inappropriate will be addressed.280Domain Level~~' - \ ] - i  _~o_,o_-_~..~_o~ :m_, .
.
.
.
.
.
......... ~ ....
?--' !
Is~-s,~,,~-co~,~,cs) ~.
.........t ~ , ".P~b, 1 era- So lv-mg_Le v ?1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
"~ , iTal~_Com,~(U,CS689) p...~ .
.
.
.
.
.
.
.
.. .
.
.
.
.
.
.
S--- - -.-.-.~.~- -.~- - --i.~.,~ 9~d-r~c~.s.s~-s*,mo,,~Co,~eJ,cs)~ \[--i": .
- ......... , # :........ i \[ Build -Plma (U,S,TaI~?-Course(U,(~S 689)) I .
.
.
.
.
.
.
::" "~o "on ' lna ~tiat*- Singl e~ V at~l,S,_fae,Tca?
bt s~fae,CS 689)) ', .
: V~po~d~ :~__Ao~ , " .
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
Goa l :  ,: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
-,7:: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i I Obtafin-hffo.Rcf(U,&_f~e,Teach?,(_fae,CS689)) \]\[Ask-Rcf(U,S,_fac,Tcaehes(_fa?,C S 689)) \[i \[ Mal~Q-Accq'tablc ~'s'Teae ~- fae 'c  s689)) I ?\[Givc-B ack~r~u ?
d(U.S,Tcael~-fae,CS689)) \["7\] In fono(U,S,want0J,Satls~-Scminar-Coua~U,CS))) \]IT~CO,S.wa*t(O,S~'Scr~*~C?
?~*?0J.CS))) \[ I ~*f-R*q~a~J,sJ~'TCaev~-f~'cs689)) \[?Suffacc-Say-Prop(U,S,waatfU.
ISatiffy.Seaninar-C~0J,CS))) I Surfae~WH-QfU,S,_fac,Tcachcs(_fae,CS689)) II want to *atirfy my seminar cours~ rttluir~rntnts.
Who's r~aching (:$689?Figure 1: The Structure of the User's UtterancesThe evaluator checks whether the existing and proposedactions together constitute a well-formed plan, one inwhich the children of each action contribute to their parentaction.
Therefore, for each pair of actions, the evaluatorchecks against its recipe library to determine if theirparent-child relationship holds.
The evaluator also checkswhether each additional action is feasible by examiningwhether its applicability conditions are satisfied and itspreconditions ~ can be satisfied.We contend that well-formedness hould be checkedbefore feasibility since the feasibility of an action that doesnot contribute to its parent action is irrelevant.
Similarly,the well-formedness of a plan that attempts to achieve aninfeasible goal is also irrelevant.
Therefore, we argue thatthe processes of checking well-formedness and feasibilityshould be interleaved in order to address the most generalaction that is inappropriate.
We show how this interleavedprocess works by referring back to figure 1.Suppose the system believes that CS689 is not a sem-inar course.
The evaluation process starts from Satisfy-Seminar-Course(U, CS), the top-most action in the pro-posed domain model.
The system's knowledge indi-cates that Satisfy-Seminar-Course(U, CS) contributes toGet-Masters(U, CS).
The system also believes that theapplicability conditions and the preconditions for theSatisfy-Seminar-Course domain plan are satisfied, indi-cating that the action is feasible.
However, the sys-tem's recipe library gives no reason to believe thatTake-Course( U,CS689) contributes to Satisfy-Seminar-Course(U, CS), since CS689 is not a seminar course.
Theevaluator then decides that this pair of proposed actionswould make the domain plan ill-formed.4 When the Proposal is ErroneousThe goal selector's task is to determine, based on thecurrent dialogue model, an intentional goal \[8\] that ismost appropriate for the system to pursue.
An intentionalgoal could be to directly respond to the user's utterance,a Both applicability conditions and preconditions are prereq-uisites fo r  execut ing  a rec ipe .
However ,  it  is unreasonab le  toattempt to satisfy an applicability condition whereas precondi-tions can be planned for.Action: Correct-Inference(..s 1 ,_ 2,_proposed)Recipe-Type: DecompositionAppl Cond: believe(_sl, ~contributes(_actl ,..act2))believe(_s2, contributes(_actl,_act2))Constraints: in-plan(_actl,_proposed) Vin-plan(_act2,_proposed)Body: Modify-Acts(_s 1 ,_s2,_proposed,_actl ,_act2)Insert-Correction(..s I ,_ 2,_proposed)Effects: modified(_proposed)well-formed(_propo sed)Action: Modify-Acts(_sl ,_s2,_proposed,_actl,_act2)Recipe-Type: SpecializationAppl Cond: believe(_s 1, -~contributes(_actl ,_act2))Preconditions: believe(_s2,-,contributes(_actl,_act2))Body: Remove-Act(_sl ,_s2,_proposed,_actl )Alter-Act(_sl,_s2,_proposed,-actl )Effects: modified(_proposed)Goal: modified(_proposed)Figure 2: Two Problem-Solving Recipesto correct a user's misconception, to provide a betteralternative, etc.
In this paper we only discuss the goalselector's task when the user has an erroneous plan/goal.In a collaborative environment, if the system decidesthat the proposed model is infeasible/ill-formed, it shouldrefuse to accept he additions and suggest modificationsto the proposal by entering anegotiation subdialogue.
Forthis purpose, we developed recipes for two problem-solving actions, Correct-Goal and Correct-Inference,each a specialization of a Modify-Proposal action.
Weillustrate the Correct-Inference action in more detail.We show two problem-solving recipes, Correct-Inference and Modify-Acts, in figure 2.
The Correct-Inference recipe is applicable when _s2 believes that_actl contributes to achieving _act2, while _sl believesthat such a relationship does not hold.
The goal isto make the resultant plan a well-formed plan; there-fore, its body consists of an action Modify-Acts thatdeletes the problematic omponents of the plan, andInsert-Correction, that inserts new actions/variables intothe plan.
One precondition in Modify-Acts is be-lieve(_s2, -~contributes(_act l,-act2 ) ) (note that in Correct-Inference, _s2 believes contributes(-actl,-act2)), and thechange in _s2's belief can be accomplished by invokingthe discourse level action Inform so that _sl can conveythe ill-formedness to_s2.
This Inform act may lead to fur-ther negotiation about whether _actl contributes to _act2.Only when _sl receives a positive feedback from _s2,indicating that _s2 accepts _sl's belief, can _sl assumethat the proposed actions can be modified.Earlier discussion shows that the proposed actions infigure 1 would make the domain plan ill-formed.
There-fore, the goal selector posts a goal to modify the proposal,which causes the Correct-Inference r cipe in figure 2 to beselected.
The variables _actl and _act2 are bound to Take-Course( U, CS689 ) and Satisfy-Seminar-Course( U, CS ), re-spectively, since the system believes that the former doesnot contribute to the latter.Figure 3 shows how we envision the planner to expandon the Correct-Inference r cipe, which results in thegeneration of the following two utterances:(1)S" Taking CS689 does not contribute to satisfyingthe seminar course requirement,(2) CS689 is not a seminar course.281Dialogue Model in Figttre 1 ~tProblem-Solving Level J .
.
.
.
.
.
.
.
.
E .
.
.
.
.
.
.
.
.
J ,- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.- .
.
.
.
.
.
.
.
.
.
.,- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
., ~ C-e n?
rate-Respo nse(S, U,Proposed- Model ) J',\[Evaluale-ProposalfS,U,Pro\[n:,sed-Model} I JModif}'-Proposal(S.U,Proposed-Model) It i i', ICorreot'lnfer?ncc(S.U,Pr?P ?sed'Model) I,' Modify -Acts(S,U,Proposcd-Mod?l ,Take-Co ul'se(U. CS6g9 ),f, Safsfv.s~ainar.Course(U.CSl~r~',~ f+~-~ ; q .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
-- :.
:---~- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I n fo rra (S,U,-in fe nm ce(Tak-?-Co urse(n,c$ 689), I_ Saris P / -Scminar -Coorse(U~Te\[~'~S.
U.-in fete n ce(Take ,Co u rse( U,C S689).
A ddress- Belie vabili ty(S, U',-(in fe fence(L__.__ Satis~-Seminar-Co utse(U,CS))) I Ta.k?-Cotn'sed U CS689"~ Satis~-Seminat-Course(U.CS))lVS ur face'Say'Pr ?P(S'U "izffcre nee( I Jlaform(S,U.-isa(CS689,seminar-course)) \] Take-Cours~(U.CS689) .
.
.
.\[Satis fC-Serainar-Course(U,CS111\[T?II(S ,U,-isa(CS689 ,se rain at.-cottrs? ))
Jf .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
j s_~._~_r:~e~_s.y.~=sc_s_+sg___~_.,_~_.~_o~))___Taking CS689 does not contribute to satisfying CS689 is not a seminar ?ours?the seminar course requirementFigure 3: The Dialogue Model for the System's ResponseThe action Inform(_sl,_s2,_prop) has the goal be-lieve(_s2,_prop); therefore, utterance (1) is generated byexecuting the Inform action as an attempt to satisfy thepreconditions for the Modify-Acts recipe.
Utterance (2)results from the Address-Believability action, which is asubaction of Inform, to support he claim in (1).
Theproblem-solving and discourse levels in figure 3 operateon the entire dialogue model shown in figure 1, sincethe evaluation process acts upon this model.
Due to thisnature, the evaluation process can be viewed as a meta-planning process, and when the goal of this process isachieved, the modified dialogue model is returned to.Now consider the case in which the user continues byaccepting utterances (1) and (2), which satisfies the pre-condition of Modify-Acts.
Modify-Acts has two special-izations, Remove-Act, which removes the incorrect action(and all of its children), and Alter-Act, which generalizesthe proposed action so that the plan will be well-formed.Since Take-Course contributes to Satisfy-Seminar-Courseas long as the course is a seminar course, the system gen-eralizes the user's proposed action by replacing CS689with a variable.
This variable may be instantiated by theInsert-Correction subaction of Correct-Inference whenthe dialogue continues.
Note that our model accounts forwhy the user's original question about the instructor ofCS689 is never answered - -a  conflict was detected thatmade the question superfluous.5 Related WorkSeveral researchers have studied collaboration \[1, 3, 10\]and Allen proposed ifferent plan modalities dependingon whether a plan fragment is shared, proposed and ac-knowledged, or merely private \[1\].
However, they haveemphasized iscourse analysis and none has provided aplan-based framework for proposal negotiation, speci fledappropriate system response during collaboration, or ac-counted for why a question might never be answered.Litman and Allen used discourse meta-plans to handlea class of correction subdialogues \[7\].
However, theirCorrect-Plan only addressed cases in which an agent addsa repair step to a pre-existing plan that does not execute asexpected.
Thus their meta-plans do not handle correctionof proposed additions to the dialogue model (since thisgenerally does not involve adding a step to the proposal).Furthermore, they were only concerned with understand-ing utterances, not with generating appropriate r sponses.The work in \[5, 1 I, 9\] addressed generating cooperativeresponses and responding to plan-based misconceptions,but did not capture these within an overall collaborativesystem that must negotiate proposals with the user.
Hee-man \[4\] used meta-plans to account for collaboration onreferring expressions.
We have addressed collaboration iconstructing the user's task-related plan, captured cooper-ative responses and negotiation of how the plan should beconstructed, and provided an accounting for why a user'squestion may never be answered.6 Confusions and Future WorkWe have presented a plan-based framework for generatingresponses in a collaborative environment.
Our frameworkimproves upon previous ones in that, 1) it captures co-operative responses as a part of collaboration, 2) it iscapable of initiating negotiation subdialogues to deter-mine what actions should be added to the shared plan,3) the correction process, instead of merely pointing outproblematic plans/goals to the user, modifies the plan intoits most specific form accepted by both participants, and4) the evaluation/correction process operates at a meta-level which keeps the negotiation subdialogue separatefrom the original dialogue model, while allowing thesame plan-inference mechanism to be used at both levels.We intend to enhance our evaluator so that it alsorecognizes ub-optimal solutions and can suggest bet-ter alternatives.
We will also study the goal selector'stask when the user's plan/goal is well-formed/feasible.This includes identifying a set of intentional goals anda strategy for the goal selector to choose amongst them.Furthermore, we need to develop the intentional plannerwhich constructs a plan to achieve the posted goal, and adiscourse realizer to generate natural anguage text.References\[1\] James Allen.
Discourse structure in the TRAINS project.In Darpa Speech and Natural Language Workshop, 1991.\[2\] Rhonda Eller and Sandra Carberry.
A meta-rule approachto flexible plan recognition in dialogue.
User Modelingand User-Adapted lnteraction, 2:27--53, 1992.\[3\] Barbara Grosz and Candace Sidner.
Plans for discourse.
InCohen et al, editor, Intentions in Communication, pages417--444.
1990.\[4\] Peter Heeman.
A computational model of collaborationon referring expressions.
Master's thesis, University ofToronto, 1991.\[5\] Aravind Joshi, Bonnie Webber, and Ralph Weischedel.Living up to expectations: Computing expert responses.
InProc.
AAAL pages 169--175, 1984.\[6\] Lynn Lambert and Sandra Carberry.
A tripartite plan-basedmodel of dialogue.
In Proc.
ACL, pages 47--54, 1991.\[7\] Diane Litman and James Allen.
A plan recognitionmodel for subdialogues in conversation.
Cognitive Sci-ence, 11:163--200, 1987.\[8\] Johanna Moore and Cecile Paris.
Planning text for advisorydialogues.
In Proc.
ACL, pages 203--211, 1989.\[9\] Mart.ha Pollack.
A model of plan inference that distin-guishes between the beliefs of actors and observers.
InProc.
ACL, pages 207--214, 1986.\[10\] Candace Sidner.
Using discourse to negotiate in collabo-rative activity: An artificial anguage.
In Workshop Notes:AAAI-92 Cooperation Among Heterogeneous IntelligentSystems, pages 121--128, 1992.\[11 \] Peter vanBeek.
A model for generating better explanations.In Proc.
ACL, pages 215--220, 1987.282
