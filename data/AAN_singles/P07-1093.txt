Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 736?743,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Maximum Expected Utility Framework for Binary Sequence LabelingMartin Jansche?jansche@acm.orgAbstractWe consider the problem of predictive infer-ence for probabilistic binary sequence label-ing models under F-score as utility.
For asimple class of models, we show that thenumber of hypotheses whose expected F-score needs to be evaluated is linear in thesequence length and present a framework forefficiently evaluating the expectation of manycommon loss/utility functions, including theF-score.
This framework includes both exactand faster inexact calculation methods.1 Introduction1.1 Motivation and ScopeThe weighted F-score (van Rijsbergen, 1974) playsan important role in the evaluation of binary classi-fiers, as it neatly summarizes a classifier?s ability toidentify the positive class.
A variety of methods ex-ists for training classifiers that optimize the F-score,or some similar trade-off between false positives andfalse negatives, precision and recall, sensitivity andspecificity, type I error and type II error rate, etc.Among the most general methods are those of Mozeret al (2001), whose constrained optimization tech-nique is similar to those in (Gao et al, 2006; Jansche,2005).
More specialized methods also exist, for ex-ample for support vector machines (Musicant et al,2003) and for conditional random fields (Gross et al,2007; Suzuki et al, 2006).All of these methods are about classifier training.In this paper we focus primarily on the related, butorthogonal, issue of predictive inference with a fullytrained probabilistic classifier.
Using the weightedF-score as our utility function, predictive inferenceamounts to choosing an optimal hypothesis whichmaximizes the expected utility.
We refer to this as?Current affiliation: Google Inc. Former affiliation: Centerof Computational Learning Systems, Columbia University.the prediction or decoding task.
In general, decodingcan be a hard computational problem (Casacubertaand de la Higuera, 2000; Knight, 1999).
In this paperwe show that the maximum expected F-score decod-ing problem can be solved in polynomial time undercertain assumptions about the underlying probabil-ity model.
One key ingredient in our solution is avery general framework for evaluating the expectedF-score, and indeed many other utility functions, ofa fixed hypothesis.1 This framework can also be ap-plied to discriminative classifier training.1.2 Background and NotationWe formulate our approach in terms of sequence la-beling, although it has applications beyond that.
Thisis motivated by the fact that our framework for evalu-ating expected utility is indeed applicable to generalsequence labeling tasks, while our decoding methodis more restricted.
Another reason is that the F-scoreis only meaningful for comparing two (multi)sets ortwo binary sequences, but the notation for multisetsis slightly more awkward.All tasks considered here involve strings of binarylabels.
We write the length of a given string y ?
{0,1}n as |y|= n. It is convenient to view such stringsas real vectors ?
whose components happen to be 0or 1 ?
with the dot product defined as usual.
Theny ?
y is the number of ones that occur in the string y.For two strings x,y of the same length |x| = |y| thenumber of ones that occur at corresponding indicesis x ?
y.Given a hypothesis z and a gold standard labelsequence y, we define the following quantities:1.
T = y ?
y, the genuine positives;2.
P = z ?
z, the predicted positives;3.
A = z ?
y, the true positives (predicted positivesthat are genuinely positive);1A proof-of-concept implementation is available at http://purl.org/net/jansche/meu_framework/.7364.
Recl = A/T , recall (a.k.a.
sensitivity or power);5.
Prec = A/P, precision.The ?
-weighted F-score is then defined as theweighted harmonic mean of recall and precision.
Thissimplifies toF?
=(?
+1)AP+?
T (?
> 0) (1)where we assume for convenience that 0/0def= 1 toavoid explicitly dealing with the special case of thedenominator being zero.
We will write the weightedF-score from now on as F(z,y) to emphasize that itis a function of z and y.1.3 Expected F-ScoreIn Section 3 we will develop a method for evaluatingthe expectation of the F-score, which can also beused as a smooth approximation of the raw F-scoreduring classifier training: in that task (which we willnot discuss further in this paper), z are the supervisedlabels, y is the classifier output, and the challenge isthat F(z,y) does not depend smoothly on the param-eters of the classifier.
Gradient-based optimizationtechniques are not applicable unless some of the quan-tities defined above are replaced by approximationsthat depend smoothly on the classifier?s parameters.For example, the constrained optimization methodof (Mozer et al, 2001) relies on approximations ofsensitivity (which they call CA) and specificity2 (theirCR); related techniques (Gao et al, 2006; Jansche,2005) rely on approximations of true positives, falsepositives, and false negatives, and, indirectly, recalland precision.
Unlike these methods we compute theexpected F-score exactly, without relying on ad hocapproximations of the true positives, etc.Being able to efficiently compute the expectedF-score is a prerequisite for maximizing it during de-coding.
More precisely, we compute the expectationof the functiony 7?
F(z,y), (2)which is a unary function obtained by holding thefirst argument of the binary function F fixed.
It willhenceforth be abbreviated as F(z, ?
), and we will de-note its expected value byE [F(z, ?)]
= ?y?
{0,1}|z|F(z,y) Pr(y).
(3)2Defined as [(~1?
z) ?
(~1?
y)]/[(~1?
y) ?
(~1?
y)].This expectation is taken with respect to a probabilitymodel over binary label sequences, written as Pr(y)for simplicity.
This probability model may be condi-tional, that is, in general it will depend on covariatesx and parameters ?
.
We have suppressed both in ournotation, since x is fixed during training and decod-ing, and we assume that the model is fully identifiedduring decoding.
This is for clarity only and does notlimit the class of models, though we will introduceadditional, limiting assumptions shortly.
We are nowready to tackle the inference task formally.2 Maximum Expected F-Score Inference2.1 Problem StatementOptimal predictive inference under F-score utilityrequires us to find an hypothesis z?
of length n whichmaximizes the expected F-score relative to a givenprobabilistic sequence labeling model:z?
= argmaxz?
{0,1}nE [F(z, ?)]
= argmaxz?
{0,1}n?yF(z,y) Pr(y).
(4)We require the probability model to factor into inde-pendent Bernoulli components (Markov order zero):Pr(y = (y1, .
.
.
,yn)) =n?i=1pyii (1?
pi)1?yi .
(5)In practical applications we might choose the overallprobability distribution to be the product of indepen-dent logistic regression models, for example.
Ordi-nary classification arises as a special case when theyi are i.i.d., that is, a single probabilistic classifier isused to find Pr(yi = 1 | xi).
For our present purposesit is sufficient to assume that the inference algorithmtakes as its input the vector (p1, .
.
.
, pn), where pi isthe probability that yi = 1.The discrete maximization problem (4) cannot besolved naively, since the number of hypotheses thatwould need to be evaluated in a brute-force search foran optimal hypothesis z?
is exponential in the sequencelength n. We show below that in fact only a fewhypotheses (n+1 instead of 2n) need to be examinedin order to find an optimal one.The inference algorithm is the intuitive one, analo-gous to the following simple observation: Start withthe hypothesis z = 00 .
.
.0 and evaluate its raw F-score F(z,y) relative to a fixed but unknown binary737string y.
Then z will have perfect precision (no posi-tive labels means no chance to make mistakes), andzero recall (unless y = z).
Switch on any bit of z thatis currently off.
Then precision will decrease or re-main equal, while recall will increase or remain equal.Repeat until z = 11 .
.
.1 is reached, in which case re-call will be perfect and precision at its minimum.
Theinference algorithm for expected F-score follows thesame strategy, and in particular it switches on thebits of z in order of non-increasing probability: startwith 00 .
.
.0, then switch on the bit i1 = argmaxi pi,etc.
until 11 .
.
.1 is reached.
We now show that thisintuitive strategy is indeed admissible.2.2 Outer and Inner MaximizationIn general, maximization can be carried out piece-wise, sinceargmaxx?Xf (x) = argmaxx?
{argmaxy?Y f (y)|Y?pi(X)}f (x),where pi(X) is any family (Y1,Y2, .
.
.)
of nonemptysubsets of X whose union?iYi is equal to X .
(Recur-sive application would lead to a divide-and-conqueralgorithm.)
Duplication of effort is avoided if pi(X)is a partition of X .Here we partition the set {0,1}n into equivalenceclasses based on the number of ones in a string(viewed as a real vector).
Define Sm to be the setSm = {s ?
{0,1}n | s ?
s = m}consisting of all binary strings of fixed length n thatcontain exactly m ones.
Then the maximization prob-lem (4) can be transformed into an inner maximiza-tions?
(m) = argmaxs?SmE [F(s, ?)]
, (6)followed by an outer maximizationz?
= argmaxz?{s?(0),...,s?
(n)}E [F(z, ?)]
.
(7)2.3 Closed-Form Inner MaximizationThe key insight is that the inner maximization prob-lem (6) can be solved analytically.
Given a vectorp = (p1, .
.
.
, pn) of probabilities, define z(m) to be thebinary label sequence with exactly m ones and n?mzeroes where for all indices i,k we have[z(m)i = 1?
z(m)k = 0]?
pi ?
pk.Algorithm 1 Maximizing the Expected F-Score.1: Input: probabilities p = (p1, .
.
.
, pn)2: I?
indices of p sorted by non-increasing probability3: z?
0 .
.
.04: a?
05: v?
expectF(z, p)6: for j?
1 to n do7: i?
I[ j]8: z[i]?
1 // switch on the ith bit9: u?
expectF(z, p)10: if u > v then11: a?
j12: v?
u13: for j?
a+1 to n do14: z[I[ j]]?
015: return (z,v)In other words, the most probable m bits (accordingto p) in z(m) are set and the least probable n?m bitsare off.
We rely on the following result, whose proofis deferred to Appendix A:Theorem 1.
(?s ?
Sm) E [F(z(m), ?)]?
E [F(s, ?
)].Because z(m) is maximal in Sm, we may equatez(m) = argmaxs?Sm E [F(s, ?)]
= s?
(m) (modulo ties,which can always arise with argmax).2.4 Pedestrian Outer MaximizationWith the inner maximization (6) thus solved, the outermaximization (7) can be carried out naively, sinceonly n+ 1 hypotheses need to be evaluated.
Thisis precisely what Algorithm 1 does, which keepstrack of the maximum value in v. On terminationz = argmaxs E [F(s, ?)].
Correctness follows directlyfrom our results in this section.Algorithm 1 runs in time O(n logn+ n f (n)).
Atotal of O(n logn) time is required for accessing thevector p in sorted order (line 2).
This dominates theO(n) time required to explicitly generate the optimalhypothesis (lines 13?14).
The algorithm invokes asubroutine expectF(z, p) a total of n+1 times.
Thissubroutine, which is the topic of the next section,evaluates, in time f (n), the expected F-score (withrespect to p) of a given hypothesis z of length n.3 Computing the Expected F-Score3.1 Problem StatementWe now turn to the problem of computing the ex-pected value (3) of the F-score for a given hypothesisz relative to a fully identified probability model.
Themethod presented here does not strictly require the738zeroth-order Markov assumption (5) instated earlier(a higher-order Markov assumption will suffice), butit shall remain in effect for simplicity.As with the maximization problem (4), the sumin (3) is over exponentially many terms and cannot becomputed naively.
But observe that the F-score (1)is a (rational) function of integer counts which arebounded, so it can take on only a finite, and indeedsmall, number of distinct values.
We shall see shortlythat the function (2) whose expectation we wish tocompute has a domain whose cardinality is exponen-tial in n, but the cardinality of its range is polynomialin n. The latter is sufficient to ensure that its ex-pectation can be computed in polynomial time.
Themethod we are about to develop is in fact very generaland applies to many other loss and utility functionsbesides the F-score.3.2 Expected F-Score as an IntegralA few notions from real analysis are helpful becausethey highlight the importance of thinking about func-tions in terms of their range, level sets, and the equiv-alence classes they induce on their domain (the kernelof the function).
A function g : ??
R is said to besimple if it can be expressed as a linear combinationof indicator functions (characteristic functions):g(x) = ?k?Kak ?Bk(x),where K is a finite index set, ak ?
R, and Bk ?
?.
(?S : S?
{0,1} is the characteristic function of set S.)Let ?
be a countable set and P be a probabilitymeasure on ?.
Then the expectation of g is given bythe Lebesgue integral of g. In the case of a simplefunction g as defined above, the integral, and hencethe expectation, is defined asE [g] =?
?g dP = ?k?KakP(Bk).
(8)This gives us a general recipe for evaluating E[g]when ?
is much larger than the range of g. Instead ofcomputing the sum ?y??
g(y)P({y}) we can com-pute the sum in (8) above.
This directly yields anefficient algorithm whenever K is sufficiently smallandP(Bk) can be evaluated efficiently.The expected F-score is thus the Lebesgue integralof the function (2).
Looking at the definition of the0,0Y:n, n:n 1,1Y:Y0,1n:YY:n, n:n2,2Y:Y1,2n:YY:n, n:n Y:Y0,2n:YY:n, n:n 3,3Y:Y2,3n:YY:n, n:n Y:Y1,3n:YY:n, n:n Y:Y0,3n:YY:n, n:nY:n, n:nY:n, n:nY:n, n:nFigure 1: Finite State Classifier h?.F-score in (1) we see that the only expressions whichdepend on y are A = z ?
y and T = y ?
y (P = z ?
z isfixed because z is).
But 0 ?
z ?
y ?
y ?
y ?
n = |z|.Therefore F(z, ?)
takes on at most (n+1)(n+2)/2,i.e.
quadratically many, distinct values.
It is a simplefunction withK = {(A,T ) ?
N0?N0 | A?
T ?
|z|, A?
z ?
z}a(A,T ) =(?
+1)Az ?
z+?
T where 0/0def= 1B(A,T ) = {y | z ?
y = A, y ?
y = T}.3.3 Computing Membership in BkObserve that the family of sets(B(A,T ))(A,T )?Kis apartition (namely the kernel of F(z, ?))
of the set ?={0,1}n of all label sequences of length n. In turn itgives rise to a function h : ??
K where h(y) = kiff y ?
Bk.
The function h can be computed by adeterministic finite automaton, viewed as a sequenceclassifier: rather than assigning binary accept/rejectlabels, it assigns arbitrary labels from a finite set, inthis case the index set K. For simplicity we showthe initial portion of a slightly more general two-tapeautomaton h?
in Figure 1.
It reads the two sequencesz and y on its two input tapes and counts the numberof matching positive labels (represented as Y) as wellas the number of positive labels on the second tape.Its behavior is therefore h?
(z,y) = (z ?
y, y ?
y).
Thefunction h is obtained as a special case when z (thefirst tape) is fixed.Note that this only applies to the special case when739Algorithm 2 Simple Function Instance for F-Score.def start():return (0,0)def transition(k,z, i,yi):(A,T )?
kif yi = 1 thenT ?
T +1if z[i] = 1 thenA?
A+1return (A,T )def a(k,z):(A,T )?
kF ?(?
+1)Az ?
z+?
T // where 0/0def= 1return FAlgorithm 3 Value of a Simple Function.1: Input: instance g of the simple function interface, strings zand y of length n2: k?
g.start()3: for i?
1 to n do4: k?
g.transition(k,z, i,y[i])5: return g.a(k,z)the family B = (Bk)k?K is a partition of ?.
It is al-ways possible to express any simple function in thisway, but in general there may be an exponential in-crease in the size of K when the family B is requiredto be a partition.
However for the special cases weconsider here this problem does not arise.3.4 The Simple Function TrickIn general, what we will call the simple function trickamounts to representing the simple function g whoseexpectation we want to compute by:1. a finite index set K (perhaps implicit),2. a deterministic finite state classifier h : ??
K,3.
and a vector of coefficients (ak)k?K .In practice, this means instantiating an interface withthree methods: the start and transition function of thetransducer which computes h?
(and from which h canbe derived), and an accessor method for the coeffi-cients a. Algorithm 2 shows the F-score instance.Any simple function g expressed as an instance ofthis interface can then be evaluated very simply asg(x) = ah(x).
This is shown in Algorithm 3.Evaluating E [g] is also straightforward: Composethe DFA h with the probability model p and use an al-gebraic path algorithm to compute the total probabil-ity massP(Bk) for each final state k of the resultingautomaton.
If p factors into independent componentsas required by (5), the composition is greatly sim-Algorithm 4 Expectation of a Simple Function.1: Input: instance g of the simple function interface, string zand probability vector p of length n2: M?Map()3: M[g.start()]?
14: for i?
1 to n do5: N?Map()6: for (k,P) ?M do7: // transition on yi = 08: k0?
g.transition(k,z, i,0)9: if k0 /?
N then10: N[k0]?
011: N[k0]?
N[k0]+P?
(1?
p[i])12: // transition on yi = 113: k1?
g.transition(k,z, i,1)14: if k1 /?
N then15: N[k1]?
016: N[k1]?
N[k1]+P?
p[i]17: M?
N18: E?
019: for (k,P) ?M do20: E?
E +g.a(k,z)?P21: return Eplified.
If p incorporates label history (higher-orderMarkov assumption), nothing changes in principle,though the following algorithm assumes for simplic-ity that the stronger assumption is in effect.Algorithm 4 expands the following composed au-tomaton, represented implicitly: the finite-state trans-ducer h?
specified as part of the simple function objectg is composed on the left with the string z (yieldingh) and on the right with the probability model p. Theouter loop variable i is an index into z and hence astate in the automaton that accepts z; the variablek keeps track of the states of the automaton imple-mented by g; and the probability model has a singlestate by assumption, which does not need to be rep-resented explicitly.
Exploring the states in order ofincreasing i puts them in topological order, whichmeans that the algebraic path problem can be solvedin time linear in the size of the composed automaton.The maps M and N keep track of the algebraic dis-tance from the start state to each intermediate state.On termination of the first outer loop (lines 4?17),the map M contains the final states together withtheir distances.
The algebraic distance of a final statek is now equal to P(Bk), so the expected value Ecan be computed in the second loop (lines 18?20) assuggested by (8).When the utility function interface g is instantiatedas in Algorithm 2 to represent the F-score, the run-time of Algorithm 4 is cubic in n, with very small740constants.3 The first main loop iterates over n. Theinner loop iterates over the states expanded at itera-tion i, of which there are O(i2) many when dealingwith the F-score.
The second main loop iterates overthe final states, whose number is quadratic in n inthis case.
The overall cubic runtime of the first loopdominates the computation.3.5 Other Utility FunctionsWith other functions g the runtime of Algorithm 4will depend on the asymptotic size of the index set K.If there are asymptotically as many intermediatestates at any point as there are final states, then thegeneral asymptotic runtime is O(n |K|).Many loss/utility functions are subsumed by thepresent framework.
Zero?one loss is trivial: the au-tomaton has two states (success, failure); it starts andremains in the success state as long as the symbolsread on both tapes match; on the first mismatch ittransitions to, and remains in, the failure state.Hamming (1950) distance is similar to zero?oneloss, but counts the number of mismatches (boundedby n), whereas zero?one loss only counts up to athreshold of one.A more interesting case is given by the Pk-score(Beeferman et al, 1999) and its generalizations,which moves a sliding window of size k over a pairof label sequences (z,y) and counts the number ofwindows which contain a segment boundary on oneof the sequences but not the other.
To compute itsexpectation in our framework, all we have to do isexpress the sliding window mechanism as an automa-ton, which can be done very naturally (see the proof-of-concept implementation for further details).4 Faster Inexact ComputationsBecause the exact computation of the expected F-score by Algorithm 4 requires cubic time, the overallruntime of Algorithm 1 (the decoder) is quartic.43A tight upper bound on the total number of states of the com-posed automaton in the worst case is?112 n3 + 58 n2 + 1712 n+1?.4It is possible to speed up the decoding algorithm in absoluteterms, though not asymptotically, by exploiting the fact that itexplores very similar hypotheses in sequence.
Algorithm 4 canbe modified to store and return all of its intermediate map data-structures.
This modified algorithm then requires cubic spaceinstead of quadratic space.
This additional storage cost paysoff when the algorithm is called a second time, with its formalparameter z bound to a string that differs from the one of theFaster decoding can be achieved by modifying Al-gorithm 4 to compute an approximation (in fact, alower bound) of the expected F-score.5 This is doneby introducing an additional parameter L which limitsthe number of intermediate states that get expanded.Instead of iterating over all states and their associ-ated probabilities (inner loop starting at line 6), oneiterates over the top L states only.
We require thatL?
1 for this to be meaningful.
Before entering theinner loop the entries of the map M are expandedand, using the linear time selection algorithm, thetop L entries are selected.
Because each state thatgets expanded in the inner loop has out-degree 2, thenew state map N will contain at most 2L states.
Thismeans that we have an additional loop invariant: thesize of M is always less than or equal to 2L.
There-fore the selection algorithm runs in time O(L), andso does the abridged inner loop, as well as the sec-ond outer loop.
The overall runtime of this modifiedalgorithm is therefore O(n L).If L is a constant function, the inexact computationof the expected F-score runs in linear time and theoverall decoding algorithm in quadratic time.
In par-ticular if L = 1 the approximate expected F-score isequal to the F-score of the MAP hypothesis, and themodified inference algorithm reduces to a variant ofViterbi decoding.
If L is a linear function of n, theoverall decoding algorithm runs in cubic time.We experimentally compared the exact quartic-time decoding algorithm with the approximate decod-ing algorithm for L= 2n and for L= 1.
We computedthe absolute difference between the expected F-scoreof the optimal hypothesis (as found by the exact al-gorithm) and the expected F-score of the winninghypothesis found by the approximate decoding algo-rithm.
For different sequence lengths n ?
{1, .
.
.
,50}we performed 10 runs of the different decoding al-gorithms on randomly generated probability vectorsp, where each pi was randomly drawn from a contin-uous uniform distribution on (0,1), or, in a secondexperiment, from a Beta(1/2,1/2) distribution (tosimulate an over-trained classifier).For L = 1 there is a substantial difference of aboutpreceding run in just one position.
This means that the mapdata-structures only need to be recomputed from that positionforward.
However, this does not lead to an asymptotically fasteralgorithm in the worst case.5For error bounds, see the proof-of-concept implementation.7410.6 between the expected F-scores of the winninghypothesis computed by the exact algorithm and bythe approximate algorithm.
Nevertheless the approx-imate decoding algorithm found the optimal hypoth-esis more than 99% of the time.
This is presumablydue to the additional regularization inherent in thediscrete maximization of the decoder proper: eventhough the computed expected F-scores may be farfrom their exact values, this does not necessarily af-fect the behavior of the decoder very much, since itonly needs to find the maximum among a small num-ber of such scores.
The error introduced by the ap-proximation would have to be large enough to disturbthe order of the hypotheses examined by the decoderin such a way that the true maximum is reordered.This generally does not seem to happen.For L = 2n the computed approximate expected F-scores were indistinguishable from their exact values.Consequently the approximate decoder found the truemaximum every time.5 Conclusion and Related WorkWe have presented efficient algorithms for maximumexpected F-score decoding.
Our exact algorithm runsin quartic time, but an approximate cubic-time variantis indistinguishable in practice.
A quadratic-timeapproximation makes very few mistakes and remainspractically useful.We have further described a general frameworkfor computing the expectations of certain loss/utilityfunctions.
Our method relies on the fact that manyfunctions are sparse, in the sense of having a finiterange that is much smaller than their codomain.
Toevaluate their expectations, we can use the simplefunction trick and concentrate on their level sets:it suffices to evaluate the probability of those sets/events.
The fact that the commonly used utility func-tions like the F-score have only polynomially manylevel sets is sufficient (but not necessary) to ensurethat our method is efficient.
Because the coefficientsak can be arbitrary (in fact, they can be generalized tobe elements of a vector space over the reals), we candeal with functions that go beyond simple counts.Like the methods developed by Allauzen et al(2003) and Cortes et al (2003) our technique incor-porates finite automata, but uses a direct threshold-counting technique, rather than a nondeterministiccounting technique which relies on path multiplici-ties.
This makes it easy to formulate the simultaneouscounting of two distinct quantities, such as our A andT , and to reason about the resulting automata.The method described here is similar in spirit tothose of Gao et al (2006) and Jansche (2005), whodiscuss maximum expected F-score training of deci-sion trees and logistic regression models.
However,the present work is considerably more general in twoways: (1) the expected utility computations presentedhere are not tied in any way to particular classifiers,but can be used with large classes of probabilisticmodels; and (2) our framework extends beyond thecomputation of F-scores, which fall out as a specialcase, to other loss and utility functions, including thePk score.
More importantly, expected F-score com-putation as presented here can be exact, if desired,whereas the cited works always use an approximationto the quantities we have called A and T .AcknowledgementsMost of this research was conducted while I was affilated withthe Center for Computational Learning Systems, Columbia Uni-versity.
I would like to thank my colleagues at Google, in partic-ular Ryan McDonald, as well as two anonymous reviewers forvaluable feedback.ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.
2003.
Gen-eralized algorithms for constructing language models.
InProceedings of the 41st Annual Meeting of the Associationfor Computational Linguistics.Doug Beeferman, Adam Berger, and John Lafferty.
1999.
Sta-tistical models for text segmentation.
Machine Learning,34(1?3):177?210.Francisco Casacuberta and Colin de la Higuera.
2000.
Computa-tional complexity of problems on probabilistic grammars andtransducers.
In 5th International Colloquium on GrammaticalInference.Corinna Cortes, Patrick Haffner, and Mehryar Mohri.
2003.
Ra-tional kernels.
In Advances in Neural Information ProcessingSystems, volume 15.Sheng Gao, Wen Wu, Chin-Hui Lee, and Tai-Seng Chua.
2006.A maximal figure-of-merit (MFoM)-learning approach to ro-bust classifier design for text categorization.
ACM Transac-tions on Information Systems, 24(2):190?218.
Also in ICML2004.Samuel S. Gross, Olga Russakovsky, Chuong B.
Do, and Ser-afim Batzoglou.
2007.
Training conditional random fieldsfor maximum labelwise accuracy.
In Advances in NeuralInformation Processing Systems, volume 19.R.
W. Hamming.
1950.
Error detecting and error correctingcodes.
The Bell System Technical Journal, 26(2):147?160.Martin Jansche.
2005.
Maximum expected F-measure trainingof logistic regression models.
In Proceedings of Human Lan-guage Technology Conference and Conference on EmpiricalMethods in Natural Language Processing.742Kevin Knight.
1999.
Decoding complexity in word-replacementtranslation models.
Computational Linguistics, 25(4):607?615.Michael C. Mozer, Robert Dodier, Michael D. Colagrosso,Ce?sar Guerra-Salcedo, and Richard Wolniewicz.
2001.
Prod-ding the ROC curve: Constrained optimization of classifierperformance.
In Advances in Neural Information ProcessingSystems, volume 14.David R. Musicant, Vipin Kumar, and Aysel Ozgur.
2003.Optimizing F-measure with support vector machines.
InProceedings of the Sixteenth International Florida ArtificialIntelligence Research Society Conference.Jun Suzuki, Erik McDermott, and Hideki Isozaki.
2006.
Train-ing conditional random fields with multivariate evaluationmeasures.
In Proceedings of the 21st International Confer-ence on Computational Linguistics and 44th Annual Meetingof the Association for Computational Linguistics.C.
J. van Rijsbergen.
1974.
Foundation of evaluation.
Journalof Documentation, 30(4):365?373.Appendix A Proof of Theorem 1The proof of Theorem 1 employs the following lemma:Theorem 2.
For fixed n and p, let s, t ?
Sm for some m with1 ?
m < n. Further assume that s and t differ only in two bits,i and k, in such a way that si = 1, sk = 0; ti = 0, tk = 1; andpi ?
pk.
Then E [F(s, ?)]?
E [F(t, ?)].Proof.
Express the expected F-score E [F(s, ?)]
as a sum andsplit the summation into two parts:?yF(s,y) Pr(y) = ?yyi=ykF(s,y) Pr(y) +?yyi 6=ykF(s,y) Pr(y).If yi = yk then F(s,y) = F(t,y), for three reasons: the numberof ones in s and t is the same (namely m) by assumption; y isconstant; and the number of true positives is the same, that iss ?
y = t ?
y.
The latter holds because s and y agree everywhereexcept on i and k; if yi = yk = 0, then there are no true positivesat i and k; and if yi = yk = 1 then si is a true positive but sk isnot, and conversely tk is but ti is not.
Therefore?yyi=ykF(s,y) Pr(y) = ?yyi=ykF(t,y) Pr(y).
(9)Focus on those summands where yi 6= yk.
Specifically groupthem into pairs (y,z) where y and z are identical except thatyi = 1 and yk = 0, but zi = 0 and zk = 1.
In other words, the twosummations on the right-hand side of the following equality arecarried out in parallel:?yyi 6=ykF(s,y) Pr(y) = ?yyi=1yk=0F(s,y) Pr(y)+ ?zzi=0zk=1F(s,z) Pr(z).Then, focusing on s first:F(s,y) Pr(y)+F(s,z) Pr(z)=(?
+1)(A+1)m+?T Pr(y)+(?
+1)Am+?T Pr(z)= [(A+1)pi (1?
pk)+A(1?
pi)pk](?
+1)m+?T C= [pi +(pi + pk?2pi pk)A?
pi pk](?
+1)m+?T C= [pi +C0]C1,where A = s ?
z is the number of true positives between s and z(s and y have an additional true positive at i by construction);T = y ?y= z ?z is the number of positive labels in y and z (identicalby assumption); andC =Pr(y)pi (1?
pk)=Pr(z)(1?
pi) pkis the probability of y and z evaluated on all positions except fori and k. This equality holds because of the zeroth-order Markovassumption (5) imposed on Pr(y).
C0 and C1 are constants thatallow us to focus on the essential aspects.The situation for t is similar, except for the true positives:F(t,y) Pr(y)+F(t,z) Pr(z)=(?
+1)Am+?T Pr(y)+(?
+1)(A+1)m+?T Pr(z)= [A pi (1?
pk)+(A+1)(1?
pi)pk](?
+1)m+?T C= [pk +(pi + pk?2pi pk)A?
pi pk](?
+1)m+?T C= [pk +C0]C1where all constants have the same values as above.
But pi ?
pkby assumption, pk +C0 ?
0, and C1 ?
0, so we haveF(s,y) Pr(y)+F(s,z) Pr(z) = [pi +C0]C1?
F(t,y) Pr(y)+F(t,z) Pr(z) = [pk +C0]C1,and therefore?yyi 6=ykF(s,y) Pr(y)?
?yyi 6=ykF(t,y) Pr(y).
(10)The theorem follows from equality (9) and inequality (10).Proof of Theorem 1: (?s ?
Sm) E [F(z(m), ?)]?
E [F(s, ?
)].Observe that z(m) ?
Sm by definition (see Section 2.3).
Form = 0 and m = n the theorem holds trivially because Sm is asingleton set.
In the nontrivial cases, Theorem 2 is appliedrepeatedly.
The string z(m) can be transformed into any otherstring s ?
Sm by repeatedly clearing a more likely set bit andsetting a less likely unset bit.In particular this can be done as follows: First, find the indiceswhere z(m) and s disagree.
By construction there must be an evennumber of such indices; indeed there are equinumerous sets{i??
z(m)i = 1?
si = 0}?{j??
z(m)j = 0?
s j = 1}.This holds because the total number of ones is fixed and identicalin z(m) and s, and so is the total number of zeroes.
Next, sortthose indices by non-increasing probability and represent themas i1, .
.
.
, ik and j1, .
.
.
, jk.
Let s0 = z(m).
Then let s1 be identicalto s0 except that si1 = 0 and s j1 = 1.
Form s2, .
.
.
,sk along thesame lines and observe that sk = s by construction.
By definitionof z(m) it must be the case that pir ?
p jr for all r ?
{1, .
.
.
,k}.Therefore Theorem 2 applies at every step along the way fromz(m) = s0 to sk = s, and so the expected utility is non-increasingalong that path.743
