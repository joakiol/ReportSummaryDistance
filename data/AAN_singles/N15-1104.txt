Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1006?1011,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsNormalized Word Embedding and Orthogonal Transform for BilingualWord TranslationChao XingCSLT, Tsinghua UniversityBeijing Jiaotong UniversityBeijing, P.R.
ChinaChao LiuCSLT, RIIT, Tsinghua UniversityCS Department, Tsinghua UniversityBeijing, P.R.
ChinaDong Wang*CSLT, RIIT, Tsinghua UniversityTNList, ChinaBeijing, P.R.
ChinaYiye LinCSLT, RIIT, Tsinghua UniversityBeijing Institute of TechnologyBeijing, P.R.
ChinaAbstractWord embedding has been found to be high-ly powerful to translate words from one lan-guage to another by a simple linear transfor-m.
However, we found some inconsistenceamong the objective functions of the embed-ding and the transform learning, as well asthe distance measurement.
This paper propos-es a solution which normalizes the word vec-tors on a hypersphere and constrains the lin-ear transform as an orthogonal transform.
Theexperimental results confirmed that the pro-posed solution can offer better performanceon a word similarity task and an English-to-Spanish word translation task.1 IntroductionWord embedding has been extensively studied in re-cent years (Bengio et al, 2003; Turian et al, 2010;Collobert et al, 2011; Huang et al, 2012).
Fol-lowing the idea that the meaning of a word can bedetermined by ?the company it keeps?
(Baroni andZamparelli, 2010), i.e., the words that it co-occurswith, word embedding projects discrete words to alow-dimensional and continuous vector space whereco-occurred words are located close to each other.Compared to conventional discrete representations(e.g., the one-hot encoding), word embedding pro-vides more robust representations for words, partic-ulary for those that infrequently appear in the train-ing data.
More importantly, the embedding encodessyntactic and semantic content implicitly, so that re-lations among words can be simply computed asthe distances among their embeddings, or word vec-tors.
A well-known efficient word embedding ap-proach was recently proposed by (Mikolov et al,2013a), where two log-linear models (CBOW andskip-gram) are proposed to learn the neighboring re-lation of words in context.
A following work pro-posed by the same authors introduces some modifi-cations that largely improve the efficiency of modeltraining (Mikolov et al, 2013c).An interesting property of word vectors learnedby the log-linear model is that the relations amongrelevant words seem linear and can be computed bysimple vector addition and substraction (Mikolov etal., 2013d).
For example, the following relation ap-proximately holds in the word vector space: Paris -France + Rome = Italy.
In (Mikolov et al, 2013b),the linear relation is extended to the bilingual sce-nario, where a linear transform is learned to projectsemantically identical words from one language toanother.
The authors reported a high accuracy on abilingual word translation task.Although promising, we argue that both the wordembedding and the linear transform are ill-posed,due to the inconsistence among the objective func-tion used to learn the word vectors (maximum like-lihood based on inner product), the distance mea-surement for word vectors (cosine distance), and theobjective function used to learn the linear transform(mean square error).
This inconsistence may lead to1006suboptimal estimation for both word vectors and thebilingual transform, as we will see shortly.This paper solves the inconsistence by normaliz-ing the word vectors.
Specifically, we enforce theword vectors to be in a unit length during the learn-ing of the embedding.
By this constraint, all theword vectors are located on a hypersphere and so theinner product falls back to the cosine distance.
Thishence solves the inconsistence between the embed-ding and the distance measurement.
To respect thenormalization constraint on word vectors, the lineartransform in the bilingual projection has to be con-strained as an orthogonal transform.
Finally, the co-sine distance is used when we train the orthogonaltransform, in order to achieve full consistence.2 Related workThis work largely follows the methodology and ex-perimental settings of (Mikolov et al, 2013b), whilewe normalize the embedding and use an orthogonaltransform to conduct bilingual translation.Multilingual learning can be categorized in-to projection-based approaches and regularization-based approaches.
In the projection-based ap-proaches, the embedding is performed for each lan-guage individually with monolingual data, and thenone or several projections are learned using multi-lingual data to represent the relation between lan-guages.
Our method in this paper and the linearprojection method in (Mikolov et al, 2013b) bothbelong to this category.
Another interesting workproposed by (Faruqui and Dyer, 2014) learns lineartransforms that project word vectors of all languagesto a common low-dimensional space, where the cor-relation of the multilingual word pairs is maximizedwith the canonical correlation analysis (CCA).The regularization-based approaches involve themultilingual constraint in the objective function forlearning the embedding.
For example, (Zou et al,2013) adds an extra term that reflects the distancesof some pairs of semantically related words fromdifferent languages into the objective funtion.
Asimilar approach is proposed in (Klementiev et al,2012), which casts multilingual learning as a multi-task learning and encodes the multilingual informa-tion in the interaction matrix.All the above methods rely on a multilingual lex-icon or a word/pharse alignment, usually from amachine translation (MT) system.
(Blunsom et al,2014) proposed a novel approach based on a join-t optimization method for word alignments and theembedding.
A simplified version of this approach isproposed in (Hermann and Blunsom, 2014), wherea sentence is represented by the mean vector of thewords involved.
Multilingual learning is then re-duced to maximizing the overall distance of the par-allel sentences in the training corpus, with the dis-tance computed upon the sentence vectors.3 Normalized word vectorsTaking the skip-gram model, the goal is to predictthe context words with a word in the central position.Mathematically, the training process maximizes thefollowing likelihood function with a word sequencew1, w2...wN:1NN?i=1?
?C?j?C,j 6=0logP (wi+j|wi) (1)where C is the length of the context in concern, andthe prediction probability is given by:P (wi+j|wi) =exp(cTwi+jcwi)?wexp(cTwcwi)(2)where w is any word in the vocabulary, and cwde-notes the vector of word w. Obviously, the wordvectors learned by this way are not constrained anddisperse in the entire M -dimensional space, whereM is the dimension of the word vectors.
An in-consistence with this model is that the distance mea-surement in the training is the inner product cTwcw?,however when word vectors are applied, e.g., to esti-mate word similarities, the metric is often the cosinedistancecTwcw?||cw||||cw?||.
A way to solve this consistenceis to use the inner product in applications, howeverusing the cosine distance is a convention in naturallanguage processing (NLP) and this measure does1007show better performance than the inner product inour experiments.We therefore perform in an opposite way, i.e., en-forcing the word vectors to be unit in length.
The-oretically, this changes the learning of the embed-ding to an optimization problem with a quadraticconstraint.
Solving this problem by Lagrange multi-pliers is possible, but here we simply divide a vectorby its l-2 norm whenever the vector is updated.
Thisdoes not involve much code change and is efficientenough.1The consequence of the normalization is that allthe word vectors are located on a hypersphere, as il-lustrated in Figure 1.
In addition, by the normaliza-tion, the inner product falls back to the cosine dis-tance, hence solving the inconsistence between theembedding learning and the distance measurement.0 20 4060 80 1000204060801000102030405060708090100?1 ?0.50 0.51?1?0.500.51?1?0.8?0.6?0.4?0.200.20.40.60.81Figure 1: The distributions of unnormalized (left)and normalized (right) word vectors.
The red cir-cles/stars/diamonds represent three words that are em-bedded in the two vector spaces respectively.4 Orthogonal transformThe bilingual word translation providedby (Mikolov et al, 2013b) learns a linear transformfrom the source language to the target language bythe linear regression.
The objective function is asfollows:minW?i||Wxi?
zi||2(3)1For efficiency, this normalization can be conducted everyn mini-batches.
The performance is expected to be not muchimpacted, given that n is not too large.where W is the projection matrix to be learned, andxiand ziare word vectors in the source and targetlanguage respectively.
The bilingual pair (xi, zi) in-dicates that xiand ziare identical in semantic mean-ing.
A high accuracy was reported on a word trans-lation task, where a word projected to the vector s-pace of the target language is expected to be as closeas possible to its translation (Mikolov et al, 2013b).However, we note that the ?closeness?
of words inthe projection space is measured by the cosine dis-tance, which is fundamentally different from the Eu-ler distance in the objective function (3) and hencecauses inconsistence.We solve this problem by using the cosine dis-tance in the transform learning, so the optimizationtask can be redefined as follows:maxW?i(Wxi)Tzi.
(4)Note that the word vectors in both the source and tar-get vector spaces are normalized, so the inner prod-uct in (4) is equivalent to the cosine distance.
Aproblem of this change, however, is that the project-ed vector Wxihas to be normalized, which is notguaranteed so far.To solve the problem, we first consider the casewhere the dimensions of the source and target vec-tor spaces are the same.
In this case, the normal-ization constraint on word vectors can be satisfiedby constraining W as an orthogonal matrix, whichturns the unconstrained problem (4) to a constrainedoptimization problem.
A general solver such as SQPcan be used to solve the problem.
However, we seeka simple approximation in this work.
Firstly, solve(4) by gradient descendant without considering anyconstraint.
A simple calculation shows that the gra-dient is as follows:5W=?ixiyTi, (5)and the update rule is simply given by:W = W + ?5W(6)1008where ?
is the learning rate.
After the update, W isorthogonalized by solving the following constrainedquadratic problem:min?W||W ?
?W || s.t.
?WT?W = I.
(7)One can show that this problem can be solved bytaking the singular value decomposition (SVD) ofW and replacing the singular values to ones.For the case where the dimensions of the sourceand target vector spaces are different, the normaliza-tion constraint upon the projected vectors is not easyto satisfy.
We choose a pragmatic solution.
First, weextend the low-dimensional vector space by paddinga small tunable constant at the end of the word vec-tors so that the source and target vector spaces are inthe same dimension.
The vectors are then renormal-ized after the padding to respect the normalizationconstraint.
Once this is done, the same gradient de-scendant and orthognalization approaches are readyto use to learn the orthogonal transform.5 ExperimentWe first present the data profile and configurationsused to learn monolingual word vectors, and thenexamine the learning quality on the word similari-ty task.
Finally, a comparative study is reported onthe bilingual word translation task, with Mikolov?slinear transform and the orthogonal transform pro-posed in this paper.5.1 Monolingual word embeddingThe monolingual word embedding is conductedwith the data published by the EMNLP 2011 SMTworkshop (WMT11)2.
For an easy comparison, welargely follow Mikolov?s settings in (Mikolov et al,2013b) and set English and Spanish as the sourceand target language, respectively.
The data prepa-ration involves the following steps.
Firstly, the textwas tokenized by the standard scripts provided byWMT113, and then duplicated sentences were re-moved.
The numerical expressions were tokenized2http://www.statmt.org/wmt11/3http://www.statmt.orgas ?NUM?, and special characters (such as !
?,:) wereremoved.The word2vector toolkit4was used to train theword embedding model.
We chose the skip-grammodel and the text window was set to 5.
The train-ing resulted in embedding of 169k English tokensand 116k Spanish tokens.5.2 Monolingual word similarityThe first experiment examines the quality of thelearned word vectors in English.
We choose theword similarity task, which tests to what extent theword similarity computed based on word vectors a-grees with human judgement.
The WordSimilarity-353 Test Collection5provided by (Finkelstein et al,2002) is used.
The dataset involves 154 word pairswhose similarities are measured by 13 people andthe mean values are used as the human judgement.In the experiment, the correlation between the co-sine distances computed based on the word vectorsand the humane-judged similarity is used to measurethe quality of the embedding.
The results are shownin Figure 2, where the dimension of the vector s-pace varies from 300 to 1000.
It can be observedthat the normalized word vectors offer a high corre-lation with human judgement than the unnormalizedcounterparts.300 400 500 600 700 800 900 10000.540.550.560.570.580.590.60.610.62DimensionCorrelationUnormalized WVNormalized WVFigure 2: Results on the word similarity task with the nor-malized and unnormalized word vectors.
A higher corre-lation indicates better quality.4https://code.google.com/p/word2vec5http://www.cs.technion.ac.il/ gabr/resources/data/wordsim353/10095.3 Bilingual word translationThe second experiment focuses on bilingual wordtranslation.
We select 6000 frequent words in En-glish and employ the online Google?s translation ser-vice to translate them to Spanish.
The resulting 6000English-Spanish word pairs are used to train and testthe bilingual transform in the way of cross valida-tion.
Specifically, the 6000 pairs are randomly di-vided into 10 subsets, and at each time, 9 subsetsare used for training and the rest 1 subset for testing.The average of the results of the 10 tests is reportedas the final result.
Note that not all the words trans-lated by Google are in the vocabulary of the targetlanguage; the vocabulary coverage is 99.5% in ourtest.5.3.1 Results with linear transformWe first reproduce Mikolov?s work with the lineartransform.
A number of dimension settings are ex-perimented with and the results are reported in Ta-ble 1.
The proportions that the correct translationsare in the top 1 and top 5 candidate list are reportedas P@1 and P@5 respectively.
As can be seen, thebest dimension setting is 800 for English and 200for Spanish, and the corresponding P@1 and P@5are 35.36% and 53.96%, respectively.
These resultsare comparable with the results reported in (Mikolovet al, 2013b).D-EN D-ES P@1 P@5300 300 30.43% 49.43%500 500 25.76% 44.29%700 700 20.69% 39.12%800 200 35.36% 53.96%Table 1: Performance on word translation with unnor-malized embedding and linear transform.
?D-EN?
and?D-ES?
denote the dimensions of the English and Spanishvector spaces, respectively.5.3.2 Results with orthogonal transformThe results with the normalized word vectors andthe orthogonal transform are reported in Table 2.It can be seen that the results with the orthogonaltransform are consistently better than those reportedin Table1 which are based on the linear transform.This confirms our conjecture that bilingual transla-tion can be largely improved by the normalized em-bedding and the accompanied orthogonal transform.D-EN D-ES P@1 P@5300 300 38.99% 59.16%500 500 39.91% 59.82%700 700 41.04% 59.38%800 200 40.06% 60.02%Table 2: Performance on word translation with normal-ized embedding and orthogonal transform.
?D-EN?
and?D-ES?
denote the dimensions of the English and Span-ish vector spaces, respectively.6 ConclusionsWe proposed an orthogonal transform based on nor-malized word vectors for bilingual word translation.This approach solves the inherent inconsistence inthe original approach based on unnormalized wordvectors and a linear transform.
The experimental re-sults on a monolingual word similarity task and anEnglish-to-Spanish word translation task show clearadvantage of the proposal.
This work, however, isstill preliminary.
It is unknown if the normalizedembedding works on other tasks such as relationprediction, although we expect so.
The solution tothe orthogonal transform between vector spaces withmismatched dimensions is rather ad-hoc.
Neverthe-less, locating word vectors on a hypersphere opens adoor to study the properties of the word embeddingin a space that is yet less known to us.AcknowledgementThis work was conducted when CX & YYL werevisiting students in CSLT, Tsinghua University.
Thisresearch was supported by the National ScienceFoundation of China (NSFC) under the projectNo.
61371136, and the MESTDC PhD FoundationProject No.
20130002120011.
It was also supportedby Sinovoice and Huilan Ltd.1010ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Noun-s are vectors, adjectives are matrices: Represent-ing adjective-noun constructions in semantic space.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages1183?1193.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
J. Mach.
Learn.
Res., 3:1137?1155.Phil Blunsom, Karl Moritz Hermann, Tomas Kocisky,et al 2014.
Learning bilingual word representation-s by marginalizing alignments.
In Proceedings of the52nd Annual Meeting of the Association for Computa-tional Linguistics, pages 224?229.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
J.Mach.
Learn.
Res., 12:2493?2537.Manaal Faruqui and Chris Dyer.
2014.
Improving vectorspace word representations using multilingual correla-tion.
Proceeding of EACL.
Association for Computa-tional Linguistics.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, E-hud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: The con-cept revisited.
In Proceedings of The 2002 Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 116?131.Karl Moritz Hermann and Phil Blunsom.
2014.
Multi-lingual distributed representations without word align-ment.
In Proceedings of International Conference onLearning Representations (ICLR).Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y. Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceeding of Annual Meeting of the Associationfor Computational Linguistics (ACL).Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.2012.
Inducing crosslingual distributed representa-tions of words.
In Proceedings of COLING.
Citeseer.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word representa-tions in vector space.
In International Conference onLearning Representations (ICLR).Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013b.Exploiting similarities among languages for machinetranslation.
arXiv preprint arXiv:1309.4168.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corra-do, and Jeff Dean.
2013c.
Distributed representationsof words and phrases and their compositionality.
InAdvances in Neural Information Processing Systems,pages 3111?3119.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013d.
Linguistic regularities in continuous spaceword representations.
In Proceedings of The 2013Conference of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 746?751.
Citeseer.Joseph Turian, Departement d?Informatique Et,Recherche Operationnelle (diro, Universite DeMontreal, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semisupervised learning.
In Proceeding of An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 384?394.Will Y Zou, Richard Socher, Daniel M Cer, and Christo-pher D Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
In Proceedingof Conference on Empirical Methods on Natural Lan-guage Processing (EMNLP), pages 1393?1398.1011
