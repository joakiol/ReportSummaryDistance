Proceedings of the Joint Workshop on Social Dynamics and Personal Attributes in Social Media, pages 17?27,Baltimore, Maryland USA, 27 June 2014.c?2014 Association for Computational LinguisticsThe Enrollment Effect: A Study of Amazon?s Vine ProgramDinesh PuranamSamuel Curtis JohnsonGraduate School of ManagementCornell Universitydp457@cornell.eduClaire CardieDepartment of Computer ScienceDepartment of Information ScienceCornell Universitycardie@cs.cornell.eduAbstractDo rewards from retailers such as freeproducts and recognition in the form ofstatus badges1influence the recipient?s be-havior?
We present a novel applicationof natural language processing to detectdifferences in consumer behavior due tosuch rewards.
Specifically, we investigatethe ?Enrollment?
effect, i.e.
whether re-ceiving products for free affect how con-sumer reviews are written.
Using datafrom Amazon?s Vine program, we con-duct a detailed analysis to detect stylis-tic differences in product reviews writtenby reviewers before and after enrollmentin the Vine program.
Our analysis sug-gests that the ?Enrollment?
effect exists.Further, we are able to characterize theeffect on syntactic and semantic dimen-sions.
This work has implications for re-searchers, firms and consumer advocatesstudying the influence of user-generatedcontent as these changes in style could po-tentially influence consumer decisions.1 IntroductionIn 2007 Amazon introduced its Vine program2.According to Amazon, ?Amazon invites cus-tomers to become Vine Voices based on their re-viewer rank, which is a reflection of the qualityand helpfulness of their reviews as judged by otherAmazon customers.
Amazon provides Vine mem-bers with free products that have been submittedto the program by participating vendors.
Vine re-views are the ?independent opinions of the Vine1A status badge is a special identification usually placednext to a username in online content.2http://blog.librarything.com/main/2007/08/amazon-vine-and-early-reviewers/Voices.
?3There could be potential concerns as towhether this enrollment affects the way reviewsare written, introducing, for example, a positivebias.4In this work, we investigate whether enroll-ment in the Vine program results in changes inthe linguistic style used in reviews.
We investi-gate this by looking at reviews by individuals be-fore and after enrollment in the program.
Follow-ing Feng et al.
(2012) and Bergsma et al.
(2012),we conduct a stylometric analysis using a numberof syntactic and semantic features to detect differ-ences in style.
We believe that detecting changesin consumer behavior due to intervention by a firmis a novel natural language processing task.
Ourapproach offers a framework for analyzing text todetect these changes.
This work is relevant forsocial scientists and consumer advocates as re-search suggests that product reviews are influen-tial (Chevalier and Mayzlin, 2006) and changes instyle could potentially influence consumer deci-sions.2 Related WorkOur work lies at the intersection of research infour broad areas ?
Product Reviews, ProductSampling, Status and Stylometry.Product Reviews Product reviews have re-ceived considerable attention in multiple disci-plines including Marketing, Computer Scienceand Information Science.
Research has addressedquestions such as the influence of product reviewson product sales and on brands (Gopinath et al.
(2014); Chevalier and Mayzlin (2006)), detectionof deceptive reviews (Ott et al., 2011) and senti-ment summarization (Titov and McDonald, 2008).3http://www.amazon.com/gp/vine/help, words italicizedby authors.4http://www.npr.org/blogs/money/2013/10/29/241372607/top-reviewers-on-amazon-get-tons-of-free-stuff.17This list is by no means comprehensive, but it isindicative of the extensive work in this domain.Product Sampling Here, consumers receiveproducts for free ?
as a marketing tactic.
Thisis also a well-studied phenomenon.
Research inthis area has indicated that consumers value freeproducts (Shampanier et al.
(2007); Palmeira andSrivastava (2013)); that product sampling affectsbrand sales (Bawa and Shoemaker, 2004) and thatsampling influences consumer behavior (Wadhwaet al., 2008).Status Research shows that status can influ-ence writing style.
Danescu-Niculescu-Mizil et al.
(2012) study discussions among Wikipedia edi-tors and transcripts of oral arguments before theU.S.
Supreme Court and show how variationsin linguistic style can provide information aboutpower differences within social groups.Stylometry focuses on the recognition of styleelements to identify authors (Rosen-Zvi et al.,2004), detect genders and even determine thevenue where an academic paper was presented(Bergsma et al., 2012).Our work draws from each of these researchareas and in turn hopes to make a contribution toeach in return.
Our primary objective is to es-tablish a framework to detect behavioral changedue to a decision by a firm (in this case enroll-ment to the Vine program characterized by freeproducts and Vine membership status) by analyz-ing product reviews.
Further, we hope to under-stand the dimensions on which this behavior mayhave changed.
Consequently, we pursue a novelstylometric task.
This type of work is especiallyimportant when the traditional numerical measure(rating) suggests there is no difference in the re-view pre and post-enrollment (see Section 4).3 Data & Pre-processing StepsWe gathered all reviews by the top 10,000 review-ers ranked by Amazon as of September, 2012.These rankings are partly driven by helpfulnessand recency of reviews5.
The data collected in-cludes the review text, review title, rating as-signed, date posted, product URL, product price,whether the reviewed product was received forfree via the Vine program (also referred to as5http://www.amazon.com/review/guidelines/top-reviewers.html/?Vine Review?
), ?helpfulness?
votes and badgesreceived by the reviewer .We collected a total of 2,464,141 reviews ofwhich 282,913 reviews were for products receivedfor free via the Vine program.
These reviews cov-ered a total of 9,982 reviewers6of which 3,566were members of the Vine program.
Approxi-mately half the reviews belonged to Vine mem-bers.
We eliminated reviews that did not have arating.
We further excluded reviews where the re-view text was less than 20 words in length.
Wewere left with 1,189,704 reviews by Vine mem-bers.The date of enrollment to the Vine programfor each reviewer is not explicitly available.
Weinfer the date of enrollment in the following man-ner.
We sort in ascending order all the ?VineReviews?
for each reviewer by posted date.
Weassume the earliest posted date for a ?Vine re-view?
is the enrollment date.
This is an importantassumption, as potentially reviewers could havemoved in and out of the program at varying pointsof time.
Reviewers can be moved out of the pro-gram for reasons such as not posting a ?Vine Re-view?
within 30 days of receipt of the product.
Inour data set we found 47,510 ?Vine Reviews?
by163 reviewers who were not actively on the Vineprogram7.
We can view these reviewers as havingbeen dropped from the Vine program.
Given thesmall volume of this type of reviews and review-ers, our assumption on date of enrollment appearsreasonable.MemberTypeFree/PaidEnrollmentTimingReviewCountNon Vine Paid NA 1,169,561Non Vine Free NA 47,510Vine Paid Post 452,729Vine Paid Pre 503,688Vine Free Post 233,287Table 1: Data Summary4 Enrollment EffectThis research seeks to answer the question: doesenrollment in the Vine program change the writ-ing styles of reviewers.
One naive theory is that6During the crawling, ranks changed resulting in fewerthan 10,000 reviewers in our data set.7As these reviewers were not enrolled to Amazon?s VineProgram as of September, 2012, they are excluded from ouranalysis.18perhaps receiving products for free and receivingstatus badges will result in Vine members post-ing more positive reviews.
Interestingly, the av-erage rating for reviews by Vine members postedbefore enrollment is 4.22 and after enrollment is4.21 and this difference is not statistically signif-icant.
In contrast, the length of reviews signifi-cantly increased from 251 words prior to enroll-ment to 306 words post-enrollment.
Natural lan-guage techniques are the only option to furtherinvestigate possible effects of enrollment.
Con-sequently we focus on the review text posted byVine members.4.1 ApproachFollowing Ashok et al.
(2013) and Bergsma et al.
(2012) we construct features that represent writ-ing style from each review (discussed in more de-tail in the next section).
We incorporate these fea-tures in a classification algorithm that attempts toclassify each review as having been written pre orpost-enrollment to the Vine program.
We reportwhether the difference in accuracy for this clas-sifier vs. a majority vote classification is statisti-cally significant or not.
In order to detect differ-ences in style pre and post-enrollment, we need toaddress certain confounding factors ?
ReviewerSpecificity , Product Specificity and Time Speci-ficity.Reviewer Specificity It may be possible thatcertain users post more reviews post-enrollmentthan pre-enrollment.
Consequently the classifiermay simply end up learning the differences instyle between reviewers.
To avoid this, we con-struct a balanced sample where we randomly se-lect 25 reviews for each reviewer prior to and post-enrollment (see Table 2).
This also sets our base-line accuracy at 50%.Product Specificity As the program started in2007, the post-enrollment reviews are likely topredominantly contain products released in after2007.
This might result in the classifier simplylearning the differences between products (say IPhone vs Palm).
Given our focus on style, wedo not use word tokens as such - thus avoidingthe use of product specific features.
However, forsome products, the product specific details mayresult in the use of specific syntactic structures.We assume this is not a significant contributor tothe prediction performance.
A post-hoc analysisof the top features supports this assumption.
Asecond source of change in writing style could bedue to simply whether the product was bought orreceived for free.
We exclude ?Vine Reviews?8toeliminate this confounding factor.Time Specificity A similar concern as ProductSpecificity exists for date references.
By focusingon syntactic and semantic style, we avoid the useof time specific features.Another concern is that perhaps post enroll-ment, reviewers receive writing guidelines fromAmazon.
This does not appear to be the case, asthe writing guidelines9appear to be for all mem-bers.We now turn to the extraction of style fea-tures.Data Type Number ofReviewsNumber ofReviewersTraining 113,250 2,265Test 2,500 50Table 2: Experiment Data4.2 Feature ExtractionWe consider three different features ?
?Bag ofwords/ unigrams?, ?Parse Tree Based Features?and an umbrella category consisting of genre andsemantic features (see Section 4.2.3).4.2.1 Bag of WordsBag of Words/Unigrams (UNIGRAMS) Uni-grams have often been found to be effective pre-dictive features (Joachims, 2001).
In our context,this serves as a competitive baseline for the clas-sification task.4.2.2 Parse Tree Based FeaturesFollowing Feng et al.
(2012) and Ashok et al.
(2013) we use Probabilistic Context Free Gram-mar (PCFG) to construct a parse tree for each sen-tence.
We then generate features from this parsetree and aggregate features to a review level.All Production Rules (?)
This set of featuresinclude all production rule features for each re-view, including the leaves of the parse tree for8Reviews where product was received for free via theVine program.9http://www.amazon.com/gp/community-help/customer-reviews-guidelines19each sentence in the review.
This effectively rep-resents a combination of production rules and un-igrams as features and represents an additionalcompetitive baseline.Non Terminal Production Rules (?N) This ex-cludes the leaves and hence restricts the featureset to non-terminal production rules.
This allowsus to investigate purely syntactic features from thetext.Phrasal/ Clausal Nodes (PHR/CLSL) We alsoinvestigate features that incorporate phrasal orclausal nodes of the parse trees.
Please see Table5 and Table 6 for examples of these features.Parse Tree Measures (PTM) We construct aset of measures for each sentence based on theparse tree.
These measures are maximum heightof parse tree, maximum width of the parse tree andthe number of sentences in each review.Latent Dirichlet Allocation (LDA) We alsoapply Latent Dirichlet Allocation (Blei et al.,2003) to the production rules extracted from theProbabilistic Context Free Grammar.
We use thetopics generated as features in our prediction task.Our objective was to determine whether certainco-occurring production rules offered better clas-sification accuracy.
Our implementation includeshyper-parameter optimization via maximum like-lihood.
The number of topics is selected by maxi-mizing the pairwise cosine distance amongst top-ics.
We used the Stanford Parser (Klein and Man-ning, 2003) to parse each of the reviews and theNatural Language Toolkit (NLTK) (Bird et al.,2009) to post process the results.4.2.3 Genre and Semantic FeaturesStyle Metrics (STYLE ) This includes three dis-tinct types of metrics.
Character Based - Thisincludes counts of uppercased letters, number ofletters, number of spaces and number of vow-els.
Word Based - This includes measures such asnumber of short words (3 characters or less ), longwords (8 characters or less), average word lengthand number of different words.
Syntax Based -This includes measures such as number of peri-ods, commas, common conjunctions, interroga-tives, prepositions, pronouns and verbs.Parts of Speech (POS) features have often beensurprisingly effective in tasks such as predictingdeception (Ott et al., 2011).
Consequently we testthis feature set as well.Domain-independent Dictionary We makeuse of the Linguistic Inquiry and Word Count(LIWC) categorization (Tausczik and Pen-nebaker, 2010).
One key advantage of thiscategorization is that it is domain independentand emphasizes psycho-linguistic cues.
We runtwo variants of this set of features.
The first(LIWC ALL) includes all the categories ?
bothsub-ordinate and super-ordinate categories.
Thesecond (LIWC SUB CATEG.)
only includes thesub-ordinate categories, thus ensuring the featuresare mutually exclusive.Subjectivity Measures (OPINION) We measurenumber of subjective, objective and other (neithersubjective nor objective) sentences in each review.We use the ?OpinionFinder System?
(Wiebe et al.,2005) to classify each sentence with these mea-sures.
We aggregate the count of subjective, ob-jective and other sentences at the review level anduse these aggregates as features.10We also re-port results on experiments where multiple featuretypes are included simultaneously in the model.5 Experimental MethodologyAll experiments use the Fan et al.
(2008) im-plementation of linear Support Vector Machines(Vapnik, 1998).
The linear specification allowsus to infer feature importance.
We learn thepenalty parameter via grid search using 5 foldcross-validation and report performance on a held-out balanced sample of reviews from 50 randomlyselected users (all of whom were excluded fromthe training set) from the group of reviewers withat least 25 reviews in pre and post enrollment peri-ods.
While reporting the results, for some featureswe report the threshold (Thr) value set to excludethe least frequent features.
These thresholds werealso learned via the 5 fold cross validation pro-cess.
Finally, text features can be binarized, meancentered and/or normalized.
Each of these optionswere also selected via 5 fold cross validation.6 Results & AnalysisAll of the feature sets perform statistically better11than a majority vote (50%).Baselines Unsurprisingly, the feature set con-taining all production rules (?)
yields the best ac-10One drawback is that the classifiers are trained on sen-tences from the MPQA corpus.
Domain specificity is likelyto yield poorer classification performance on our data.11as indicated by a paired t-test at p=0.05 on the held outsample20BaselinesStyle Features FeatureCountAccuracyUNIGRAMS 796,826 60.9 %?
(Thr =50) 29,362 62.0 %By Feature TypeStyle Features FeatureCountAccuracy?N(Thr=200) 2,730 59.2 %PHR/CLSL 23 57.4 %PTM 3 55.8 %LDA 200 54.0 %STYLE 26 57.6 %POS 45 57.5 %LIWC ALL 76 59.8 %LIWC SUB CATEG.
67 60.3 %OPINION 3 56.3 %Feature CombinationsStyle Features FeatureCountAccuracy?N(THR=200) + STYLE 2,756 57.9 %?N(THR=200) + OPINION 2,733 56.2 %PHR/CLSL + OPINION 26 58.0 %PHR/CLSL + STYLE 49 57.5 %LIWC + STYLE 93 60.2 %LIWC + PHR/CLSL 90 60.2 %LIWC + ?N(Thr=200) 2,797 59.1 %LIWC + OPINION 70 60.3 %PTM + OPINION 6 57.2 %STYLE + OPINION 29 58.7 %STYLE + PTM 29 57.4 %LIWC +STYLE+PHR/CLSL 116 60.1 %Table 3: Experiment Resultscuracy (62.0 %).
Unfortunately, as expected, thetop features all included terminal production rulesthat signal time or product specificity.
For ex-ample in the pre-enrollment reviews the top 10features for ?
include NNP ?
?Update?, CD ?
?2006?, NNP ?
?XP?
and NNP ?
?Palm?.
Inthe post-enrollment reviews the top 10 features in-clude CD ?
?2012?,CD ?
?2011?, NN ?
?iPad?and NN ?
?iPhone?.
We observe the same issuewith the UNIGRAMS feature set.
This supports ourcontention that the analysis should restrict itselfto style and domain-independent features.
Thebest performing style feature set is LIWC SUBCATEG.
followed by Non Terminal ProductionRules (?N).
OPINION is the most parsimoniousfeature set that performs significantly better thana majority vote.Non Terminal Production Rules (?N) Table 7presents the top Non Terminal Production Rules.We observe the following: First, pre-enrollmentreviews have noun phrases(NP) that contain fewerleaf nodes than in the post-enrollment reviews.This appears to be due to the inclusion of de-terminers (DT), adjectives (JJ), comparative ad-jectives (JJR), personal pronouns (PRP $) orsimply more nouns (NN).
This might indicatethat topics are discussed with more specificsin post-enrollment reviews.
Second, clauses(S)begin with action oriented verb phrases (VP)in the pre-enrollment reviews.
In contrast inthe post-enrollment reviews clauses connect twoclauses using coordinating conjunctions(CC) orprepositions(IN).
One possibility is that review-ers are offering more detail/concepts per sen-tence (where each clause is a detail/concept) inthe post-enrollment reviews.
Finally, we ob-serve that pre-enrollment reviews include adjec-tival phrases (ADJP) connect to superlative ad-verbs (RBS)which convey certainty.
We will re-visit this finding when we review the results fromthe LIWC model below.Phrasal/Clausal (PHR./CLSL.)
Tables 5 and 6suggest that post-enrollment reviews emphasizeinformation using descriptive phrases ?
adjecti-val phrases (ADJP) and adverbial phrases (ADVP)?
and quantifier phrases (QP).
Pre-enrollment re-views appear to have more complex clause struc-tures (SBAR, SINV, SQ, SBARQ - see table 5 fordefinitions).Parse Tree Metrics (PTM) The three featuresused are number of sentences, maximum heightof parse tree and the maximum width of the parsetree, listed here in descending order of importancefor the post-enrollment reviews.
As mentionedearlier in section 4 the average review length ishigher in the post-enrollment reviews so the find-ing that the number of sentences predict post-enrollment reviews is consistent.
Maximum treewidth predicts the pre-enrollment reviews.
Thisflat structure indicates a more complex communi-cation structure.Latent Dirichlet Allocation (LDA) Thismodel did not perform very well, being statis-tically marginally better than majority vote.
Asmentioned before, we selected the number oftopics by maximizing the average cosine distanceamongst topics.
Even with 200 topics, thismeasure was 0.39, suggesting that the topics werethemselves not well separated.
In the limit, eachtopic would be a non-terminal production rule.This is the same as Non Terminal ProductionRules (?N) feature set discussed earlier in thissection.21Predicts PRE Enrollment?number of different words?, ?uppercase?, ?alphas?,?vowels?
, ?short words?, ?words per sentence?, ?to bewords?
, ?punctuation symbols?, ?long words?, ?commonprepositions?Predicts POST Enrollment?average word length?, ?spaces?, ?verbs are?, ?chars persentence?
, ?verbs be?, ?common conjunctions?, ?verbswere?, ?personal pronouns?
, ?verbs was?, ?verbs am?Table 4: Style Metrics: Top FeaturesStyle (STYLE) Table 4 presents the top featuresfor this feature set.
The features suggest thatreviewers used a more varied vocabulary (num-ber of different words), more words per sentence(words per sentence) and more long words (longwords) in pre-enrollment than in post-enrollmentreviews.
This might indicate that sentences inthe pre-enrollment reviews were longer and morecomplex.
Interestingly, the average word lengthdid go up in the post-enrollment reviews as didthe characters per sentence.
In addition, more per-sonal pronouns and conjunctions are used ?
afinding replicated in the model using LIWC fea-tures (see below).Parts of Speech (POS) The top features forpost-enrollment are commas, periods, compara-tive adjectives, verb phrases and coordinating con-junctions.
The top features for pre-enrollment arenouns, noun phrases, determiners , prepositionsand superlative adverbs.
These results are moredifficult to interpret though the use of comparativeadjectives suggest more comparisons between dif-ferent objects in the post enrollment reviews.LIWC SUB CATEG.
The top 10 LIWC fea-tures are shown in Table 8.
LIWC features are cat-egories that are contained in broader categories.For example POSEMO (see Table 8, first feature for?Predicts POST enrollment?)
refers to the class ofpositive emotion words.
POSEMO itself is con-tained in a category called ?Affective Features?which in turn is classified as a Psychological Pro-cess (abbreviated to Pscyh.).
The analysis ofthe categories of features is in itself interesting.Psych./ Cognitive Features occur higher up in fea-tures predictive of pre-enrollment reviews than inthe features predictive of post-enrollment reviews.
?Psych./ Affective Features?
occurs as a top fea-ture for the post-enrollment reviews.
The ac-tual feature from the ?Psych./ Affective Features?category is POSEMO suggesting that the positiveemotion is more strongly conveyed in the post-enrollment reviews than in the pre-enrollment re-views.
Interestingly the corresponding negativefeature NEGEMO is in the top 10 features predict-ing the pre-enrollment reviews.
This is especiallyintriguing since the average rating for reviews inthe pre and post-enrollment reviews is the same(see 4).
We were concerned that possibly our sam-pling had induced a bias in the ratings.
But the av-erage ratings in our sample are 4.18 and 4.19 preand post-enrollment respectively (difference is notstatistically significant).FUNCTION WORDS occur extensively in thepost-enrollment reviews.
We also observe thatinclusive (INCL) and exclusive (EXCL) terms areused more in the post-enrollment reviews.
Its pos-sible that reviewers are seeking to be more bal-anced.
Products are described in personal (I),perceptual (FEEL) and relativistic (SPACE) terms.Pre-enrollment reviews discuss personal concerns(LEISURE, RELIG) , indicate a level of certainty(CERTAIN) and opinions are presented in terms ofthought process (INSIGHT).
Interestingly, the pre-enrollment reviews address the reader (YOU).Opinions (OPINION) Features predicting post-enrollment are number of objective sentences,number of subjective sentences and finally num-ber of other (neither subjective nor objective) sen-tences.
This suggests that reviewers try to writesomewhat more objectively in the post-enrollmentreviews.Feature Combinations With the exception ofthe combinations STYLE + OPINION , PHR/CLSL+OPINION and PTM + OPINION which improveon either feature set used alone, none of theother combinations improved performance overall component feature sets modeled individually.Overall, none of the combinations improved overLIWC SUB CATEG.
Hence we do not delve fur-ther into features from these models.Summary Overall pre-enrollment reviews aremore complex (complex clauses, wide parse trees,varied vocabulary, more words per sentence), havefewer concepts per sentence, contain negativeemotions, addresses the reader directly and aremore certain.
Post-enrollment reviews are longer,more descriptive, contain comparisons, containquantifiers, have more positive emotion and de-scribe the product experience in physical and per-sonal terms.22Predicts PRE Enrollment1 NP (Noun Phrase) 6 LST (List marker.Includes surroundingpunctuation)EXAMPLE EXAMPLENPNNpersonDTanother(3)2 SBAR (Clause introduced bya (possibly empty) subordinat-ing conjunction)7 VP (Verb Phrase)EXAMPLE EXAMPLESBARSVP(...)NP(...)INIfVPNPNNpersonDTanotherVBNloved3 SQ ( Inverted yes/no ques-tion, or main clause of a wh-question, following the wh-phrase in SBARQ)8 PRN (Parenthetical)EXAMPLE EXAMPLESQVPVBmatterNPPRPitVBZdoes(p. 73)4 NAC (Not a Constituent; usedto show the scope of certainprenominal modifiers within anNP)9 SINV ( Inverteddeclarative sentence,i.e.
one in which thesubject follows thetensed verb or modal)EXAMPLE EXAMPLENAC??NNMyJJOhPRP;My??SINV..VP(...
)NPPRPitVBDdidCCNor5 SBARQ (Direct question in-troduced by a wh-word or a wh-phrase)10 NX (Used withincertain complex NPs tomark the head of theNP)EXAMPLE EXAMPLESBARQ.
?SQVPVBGthinkingNPPRPyouVBDwereWHNPWPwhat,,SVPPRTRPonVBComeNXNNPLoveNNPOfNNPNatureTable 5: Phr/Clsl: Top Features PREPredicts POST Enrollment1 S (Simple declarativeclause)6 FRAG (Fragment)EXAMPLE EXAMPLESNPNNSitemsDTthePDTallRBalmostFRAG..SBARSVPVPVPPPNPPPNPNNPWarNNPColdDTtheINofNPNNmidstDTtheINinVBNsoundedVBhaveMDmustNPPRPitWHADVPWRBhowADVPRBespecially2 ADJP ( AdjectivePhrase)7 QP (Quantifier Phrase)EXAMPLE EXAMPLEADJPJJdifferentRBsoRByetQPRBjustINthanJJRmore3 PRT (Particle.
Cat-egory for words thatshould be tagged RP)8 WHNP (Wh-noun Phrase)EXAMPLE EXAMPLEPRTRPupWHNPWDTthat4 ADVP (AdverbPhrase)9 UCP (Unlike CoordinatedPhrase)EXAMPLE EXAMPLEADVPRBRearlierNPNNSyearsCDfourUCPVPVBGgloatingADVPRBjustCCorADJPJJtrue5 X (Unknown, uncer-tain, or unbracketable)10 CONJP (ConjunctionPhrase)EXAMPLE EXAMPLEXInCONJPINthanRBratherTable 6: Phr/Clsl: Top Features POST23Predicts PRE EnrollmentFeature ExamplesROOT?
S (1) And nearly every sin-gle item seemed cute andusable to me.
(2) Lookclosely, (...) overwhelmingpersonal and cultural up-heaval.NP?
NNP NNP (1) Tim Bess (2) JenniferFitchPP?
IN NP (1) for its psychological andemotional richness (2) ofloyaltyNP?
DT NN (1) the price (2) a bookNP?
NNP POS (1) Frost ?s (2) Clough ?sADJP?
RBS JJ (1) most assuredly (2) mostentertainingWHNP?WP (1) who (2) whatNP?
NNP (1) Blessed (2) IndiaPP?
TO NP (1) to the crime (2) to meS?
VP (1) linking Pye to the crimescene (2) Gripping due to(...)Predicts POST EnrollmentFeature ExamplesS?
S , IN S .
(1) It is functionally thesame as Apple?s 10 wattcharger which outputs 2.1A , so it is also suitable forcharging the iPad.
(2) It has3 levels of trays that spreadas you open the box, so youcan easily access contentsin all trays.S?
IN NP VP .
(1) So I don?t think theinvestment in graphics (...)enjoyability in the game.
(2) So we decided to try itagain this year.ROOT?
NP (1) Some kind of (...) disor-der ?
(2) Proper Alignmentand Posture; This segment(...) .S?
S CC S .
(1) Mage and Takumo (...)but lacking in depth.
(2) Thelight feature is great and itpowers off (...).NP?
PRP$ NNP NN (1) your Alpine yodeling(2) my MacBook ProS?
VP .
(1) Enough negativity.
(2)Suffice it to say that (...) .NP?
DT JJR NN (1) a better future (2) aslower flowNP?
DT JJ , JJ NN (1) an immediate , visceralreaction (2)a roots-based,singer-songwriter effortNP ?
DT NNP NNP NNPNNP(1) the Post-Total BodyWeight Training (2) TheGunfighter DVD GregoryPeckWHADVP?WRB RB (1) How far (2) how wellTable 7: ?N: Top Features (PCFG Non Terminal)Predicts PRE EnrollmentFeature Category Examplesleisure Personal Concerns Cook, chat, movieverb Function words Walk, want, seecertain Psych./CognitiveProcessesalways, neverinsight Psych./CognitiveProcessesthink, know, con-sidernegemo Psych./Affective Pro-cessesHurt, ugly, nastyexclam Exclamation !period Period .you Function words 2ndperson , you,yourpreps Function words to, with, aboverelig Personal Concerns 2ndsynagogue, sa-credPredicts POST EnrollmentFeature Category Examplesposemo Psych./Affective Pro-cessesLove, nice, sweetarticle Function words a, an, thei Function words 1stperson singular.space Psych./Relativity Down, in, thiningest Psych./Biological Pro-cessesDish, eat, pizzaipron Function words Impersonal Pro-nouns, it its ,thoseincl Psych./CognitiveProcessesInclusive, and, with, includeconj Function words and, but, whereasexcl Psych./CognitiveProcessesExclusive but,without, excludefeel Psych./Perceptual Pro-cessesfeels , touchTable 8: LIWC Sub Category : Top FeaturesThese reviews are are specific, balanced andcontain more objective sentences as well.Discussion on Readability One possibility isthat the ?Enrollment?
effect leads to reviewerswriting more readable reviews.
To test this hy-pothesis we performed a paired t-test betweenreadability scores for pre and post-enrollment re-views.
Table 9 suggests that indeed this is thecase.
Flesch Reading Ease is the only measurewhere a higher score indicates simpler text.
Forthe rest of the measures a higher score impliesmore complex text.
All of the measures are withinthe average readability range and the magnitudeof the differences are small.
Nevertheless, thesedifferences are statistically significant12with oneexception lending support to the idea that ?Enroll-ment?
effect might lead to reviewers writing morereadable reviews.12The cell size for each class is 57,875, making the modestdifference in magnitude statistically significant.24ReadingMeasure /CitePreMeanPostMeantValueARI /(Senter and Smith,1967)9.16 9.15 (0.45)Coleman Liau /(Colemanand Liau, 1975)8.76 8.68 (6.39)*Flesch Kincaid /(Kincaidet al., 1975)8.75 8.71 (2.19)*Flesch Reading Ease /(Kin-caid et al., 1975)65.63 66.18 6.61*Gunning Fog /(Gunning,1952)11.75 11.70 (2.18)*LIX /(Anderson, 1983) 38.24 38.07 (2.89)*RIX /(Anderson, 1983) 3.74 3.71 (3.05)*SMOG /(McLaughlin,1969)10.59 10.56 (2.56)** Significant at 5% levelTable 9: Readability Measures7 DiscussionSo far we have ignored the possibility that writ-ing styles of reviewers may simply continuouslyevolve with experience and we are simply detect-ing a difference due to this underlying trend.13To address this question we investigated the sub-periods within the the pre and post enrollment pe-riods.We split the post enrollment period (i.e.
fromdate of enrollment to the date the most recent re-view was posted) further into two equal time pe-riods for each reviewer.
As before, we learn aclassifier to discriminate between the sub periods.Interestingly the classifier performed the same aschance at p=0.05 (Test Accuracy= 51.0%).14 15However a similar analysis in the pre-enrollmentperiod results in a test set accuracy of 63.3% (sig-nificant at p=0.05).
So there is a change in writingstyle within the pre-enrollment period, but there isno continued change post-enrollment.
This is notconsistent with the continuous style evolution hy-pothesis.
One account would be that Amazon en-rolls reviewers whose styles have stabilized.
Thisremains a possibility as Amazon actively selectsthe members (and we are not aware of the specificrules used by Amazon).
The trends (see Figure 113Ideally, if a) the enrollment date had been the same forall reviewers and b) the enrollment was random, we wouldhave a clean experimental framework to detect whether asimilar trend exists for non-vine reviewers.
Unfortunately,this is not the case.14We report the results only on POS for conciseness.
Theother feature sets performed similarly.15As before the test sample includes 50 users.
However wesampled only 10 reviews in each sub period.
Correspondingdown sampled performance for Pre vs Post enrollment accu-racy is 57.5% (significant at p=0.05)using POS features.
)suggest that there are changes right upto the en-rollment dateand some levelling out in the post en-rollment period , providing some evidence againstthis hypothesis.Figure 1: Feature TrendsTrainSizeTestSizeAccuracyWithin Pre-Enrollment44,800 1000 63.3%Within Post-Enrollment59,250 1000 51.0%Pre vs Post Enroll.Down Sampled53,840 1000 57.5%Table 10: Sub Period Results8 ConclusionWe view this work as a first step toward inves-tigating this phenomenon further.
In particular,we plan to test the robustness of our results w.r.t.product specificity, to investigate stylistic differ-ences (a) between reviews for purchased productsversus for products received for free amongst Vinemembers and (b) between reviews by Vine review-ers and non-Vine reviewers.
Another line of in-quiry involves decomposing the ?Enrollment?
ef-fect into a reputation/status effect (the influence ofthe status badge - Vine membership) and a productsampling effect (the influence of receiving goodsfor free).
Finally, investigating the temporal dy-namics of style for these reviewers might prove in-teresting as would determining whether these sub-tle differences in style affect the readers and influ-ence purchase decisions.9 AcknowledgementsWe would like to thank our reviewers for insight-ful comments that we sought to address here.
Wewould also like to thank Myle Ott for generouslysharing the data.25ReferencesJonathan Anderson.
Lix and rix: Variations on alittle-known readability index.
Journal of Read-ing, pages 490?496, 1983.Vikas Ganjigunte Ashok, Song Feng, and YejinChoi.
Success with style: Using writing styleto predict the success of novels.
Poetry, 580(9):70, 2013.Kapil Bawa and Robert Shoemaker.
The ef-fects of free sample promotions on incremen-tal brand sales.
Marketing Science, 23(3):345?363, 2004.Shane Bergsma, Matt Post, and David Yarowsky.Stylometric analysis of scientific articles.
InProceedings of the 2012 Conference of theNorth American Chapter of the Association forComputational Linguistics: Human LanguageTechnologies, pages 327?337.
Association forComputational Linguistics, 2012.Steven Bird, Ewan Klein, and Edward Loper.Natural Language Processing with Python.O?Reilly Media, 2009.David M Blei, Andrew Y Ng, and Michael I Jor-dan.
Latent dirichlet allocation.
the Journal ofmachine Learning research, 3:993?1022, 2003.Judith A Chevalier and Dina Mayzlin.
The ef-fect of word of mouth on sales: Online bookreviews.
Journal of marketing research, 43(3):345?354, 2006.Meri Coleman and TL Liau.
A computer readabil-ity formula designed for machine scoring.
Jour-nal of Applied Psychology, 60(2):283, 1975.Cristian Danescu-Niculescu-Mizil, Lillian Lee,Bo Pang, and Jon Kleinberg.
Echoes of power:Language effects and power differences in so-cial interaction.
In Proceedings of the 21stinternational conference on World Wide Web,pages 699?708.
ACM, 2012.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,Xiang-Rui Wang, and Chih-Jen Lin.
Liblin-ear: A library for large linear classification.The Journal of Machine Learning Research, 9:1871?1874, 2008.Song Feng, Ritwik Banerjee, and Yejin Choi.Characterizing stylistic elements in syntacticstructure.
In Proceedings of the 2012 Joint Con-ference on Empirical Methods in Natural Lan-guage Processing and Computational NaturalLanguage Learning, pages 1522?1533.
Associ-ation for Computational Linguistics, 2012.Shyam Gopinath, Jacquelyn S Thomas, and Lak-shman Krishnamurthi.
Investigating the rela-tionship between the content of online wordof mouth, advertising, and brand performance.Marketing Science, 2014.Robert Gunning.
Technique of clear writing.1952.Thorsten Joachims.
A statistical learning learn-ing model of text classification for support vec-tor machines.
In Proceedings of the 24th an-nual international ACM SIGIR conference onResearch and development in information re-trieval, pages 128?136.
ACM, 2001.J Peter Kincaid, Robert P Fishburne Jr, Richard LRogers, and Brad S Chissom.
Derivation of newreadability formulas (automated readability in-dex, fog count and flesch reading ease formula)for navy enlisted personnel.
Technical report,DTIC Document, 1975.Dan Klein and Christopher D Manning.
Accurateunlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Com-putational Linguistics-Volume 1, pages 423?430.
Association for Computational Linguis-tics, 2003.G Harry McLaughlin.
Smog grading: A new read-ability formula.
Journal of reading, 12(8):639?646, 1969.Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey THancock.
Finding deceptive opinion spam byany stretch of the imagination.
In Proceed-ings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pages 309?319.
Association for Computational Linguis-tics, 2011.Mauricio M Palmeira and Joydeep Srivastava.Free offer 6= cheap product: A selective acces-sibility account on the valuation of free offers.Journal of Consumer Research, 40(4):644?656,2013.Michal Rosen-Zvi, Thomas Griffiths, MarkSteyvers, and Padhraic Smyth.
The author-topic model for authors and documents.
In Pro-ceedings of the 20th conference on Uncertaintyin artificial intelligence, pages 487?494.
AUAIPress, 2004.26RJ Senter and EA Smith.
Automated readabil-ity index.
Technical report, DTIC Document,1967.Kristina Shampanier, Nina Mazar, and DanAriely.
Zero as a special price: The true value offree products.
Marketing Science, 26(6):742?757, 2007.Yla R Tausczik and James W Pennebaker.
Thepsychological meaning of words: Liwc andcomputerized text analysis methods.
Journal ofLanguage and Social Psychology, 29(1):24?54,2010.Ivan Titov and Ryan McDonald.
A joint modelof text and aspect ratings for sentiment sum-marization.
In Proceedings of ACL-08: HLT,pages 308?316, Columbus, Ohio, June 2008.Association for Computational Linguistics.Vladimir N. Vapnik.
Statistical Learning Theory.Wiley-Interscience, 1998.Monica Wadhwa, Baba Shiv, and Stephen MNowlis.
A bite to whet the reward appetite:The influence of sampling on reward-seekingbehaviors.
Journal of Marketing Research, 45(4):403?413, 2008.Janyce Wiebe, Theresa Wilson, and Claire Cardie.Annotating expressions of opinions and emo-tions in language.
Language resources andevaluation, 39(2-3):165?210, 2005.27
