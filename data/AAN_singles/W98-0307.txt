Meta-discourse markers and problem-structuring in scientific articlesSimone TeufelCentre for Cognitive ScienceUniversity of EdinburghS.
Teal elQed, ac.
ukAbst ractKnowledge about the argumentative structure of sci-entific articles can, amongst other things, be used toimprove automatic abstracts.
We argue that the argu-mentative structure of scientific discourse can be auto-matically detected because reasordng about problems,research tasks and solutions follows predictable pat-terns.Certain phrases explicitly mark the rhetorical status(communicative function) of sentences with respect othe global argumentative goal.
Examples for suchmeta-diacaurse markers are "in this paper, we havepresented .
.
. "
or "however, their method fails to".We report on work in progress about recognizing suchmeta-comments automatically in research articles fromtwo disciplines: computational linguistics and medicine(cardiology).1 Mot ivat ionWe are interested in a formal description of the docu-ment structure of scientific articles from different dis-ciplines.
Such a description could be of practical usefor many applications in document management; ourspecific motivation for detecting document structure isquality improvement in automatic abstracting.Researchem in the field of automatic abstractinglargely agree that it is currently not technically fea-sible to create automatic abstracts based on full textunderstanding (Sparck Jones 1994).
As a result, manyresearchers have turned to sentence xtraction (Kupiec,Pedersen, & Chen 1995; Brandow, Mitze, & Rau 1995;Hovy & Lin 1997).
Sentence extraction, which doesnot involve any deep analysis, has the huge advantageof being robust with respect o individual writing style,discipline and text type (genre).
Instead of produc-ing abst rac t ,  this results produces only extracts: doc-ument surrogates consisting of a number of sentencesselected verbatim from the original text.We consider a concrete document retrieval (DR) sce-nario in which a researcher wants to select one or morescientific articles from a large scientific database (oreven from the Internet) for further inspection.
Themain task for the searcher is relevance decision for eachpaper: she needs to decide whether or not to spendmore time on a paper (read or skim-read it), dependingon how useful it presumably is to her current informa-tion needs.
Traditional sentence xtracts can be usedas rough-and-ready relevance indicators for this task,but they are not doing a great job at representing thecontents of the original document: searchers often getthe wrong idea about what the text is about.
Muchof this has to do with the fact that extracts are typ-ically incoherent exts, consisting of potentially unre-lated sentences which have been taken out of their con-text.
Crucially, extracts have no handle at revealingthe text's logical and semantic organisation.More sophisticated, user-tailored abstracts could helpthe searcher make a fast, informed relevance decision bytaking factors like the searcher's expertise and currentinformation eed into account.
If the searcher is deal-ing with research she knows well, her information eedsmight be quite concrete: during the process of writingher own paper she might want to find research whichsupports her own claims, find out if there are contra-dictory results to hers in the literature, or compare herresults to those of researchers using a similar methodol-ogy.
A different information eed arises when she wantsto gain an overview of a new research area - as an only"partially informed user" in this field (Kircz 1991) shewill need to find out about specific research goals, thenames of the researchers who have contributed the mainresearch ideas in a given time period, along with infor-mation of methodology and results in this research field.There are new functions these abstracts could fulfil.In order to make an informed relevance decision, thesearcher needs to judge differences and similarities be-tween papers, e.g.
how a given paper relates to similarpapers with respect to research goals or methodology,so that she can place the research described in a givenpaper in the larger picture of the field, a function wecall navigation between research articles.
A similar op-eration is navigation within a paper, which supportssearchers in non-linear eading and allows them to findrelevant information faster, e.g.
numerical results.We believe that a document surrogate that aims atsupporting such functions should characterize researcharticles in terms of the problems, research tasks and43solutions/methodology presented in the specific paper,but it should also represent other researchers" problems,research tasks and solutions mentioned in the paper.Our long-term goal is to automatically reconstruct thisproblem-solution structure from unrestricted text forthe searcher in the form of a problem-structured ab-stract.But how can we find problems, research questions,research tasks and solutions in text without fully un-derstanding the text?
We take sentence xtraction asa starting point due to its inherent robustness.
Usingadditional information about each sentence, viz.
theirrhetorical status with respect to the entire paper, weare in a better position to perform shallow, but guidedinformation extraction, in order to find the informationunits we are interested in.In the next section we introduce the level of docu-ment structure we are talking about, and the kind ofmeta-comments we employ in discovering it.
In therest of the paper, we report on ongoing work on auto-matically filtering meta-comments from annotated andunannotated text.
Finding recta-comments in text isan attractive task because it would allow for the auto-matic adaptation of sytem s using such phrases to newdomains.2 D iscourse  s t ruc ture  and  argumenta -t ion  in sc ient i f i c  a r t i c lesDiscourse linguistic theory suggests that texts serv-ing a common purpose among a community of userseventually take on a predictable structure of presenta-tion (Kintsch & van Dijk 1978) - and scientific articlescertainly serve a well-defined communicative purpose:they "present, retell and refer to the results of spe-cific research" (Salager-Meyer 1992).
Particularly inthe life and experimental sciences, a rigid building planfor research articles has evolved over the years, whererhetorical divisions tend to be very clearly marked insection headers.
Prototypical rhetorical divisions in-clude Introduction, Purpose, Experimental Design, Re-suits, Discussion, Conclusions.
One of the reasons forthis rigidly-defined structure seems to be that the sci-entific community in these fields has more or less agreedon how to do research: methodologies and evaluationmethods are long-lived research entities that do notchange often.One of the corpora we are using is a good exampleof such texts.
It consists of 129 articles in cardiology,taken from the American Heart Journa/, which have afixed structure with respect o rhetorical divisions andsection headers.
The other corpus, in contrast, consist-ing of 123 (mostly conference) articles in computationallinguistics (CL), displays an heterogeneous mixture ofmethodologies and traditions of presentation one wouldexpect in an interdisciplinary field.
Most of the articlescover more than one single discipline, but as a roughestimate one can say that about 45% of the articlesin the collection are predominantly technical in style,describing implementations (i.e.
engineering solutions);about 25% report on research in theoretical linguis-tics, with an argumentative t net; the remaining 30%are empirical (psycholinguistic or psychological experi-ments or corpus studies).
Even though most of the ar-ticles have an introduction and conclusions (sometimesoccurring under headers with different names), and al-most all of them cite previous work, the presentation ofthe problem and the methodology/solution areidiosyn-cratic and depend on individual writing style.
Very fewof the headers in the computational linguistics articlescorrespond to prototypical rhetorical divisions; the restcontain content specific terminology (cf.
Figure 1 whichcompares relative frequencies ofheaders for the two cor-pora).Computat iona lL inguist ics IHeader Freq:'lIntroduction 85% IntroductionConclusion 46% ResultsConclusions 22% DiscussionAcknowledgments 1.7% MethodsDiscussion 12% TablesResults 11% StatisticsExperimental PatientsResults 9% LimitationsRelated Work 77% ConclusionsImplementation 7~ StatisticalEvaluation 7~ AnalysisExample 6?~ .
ConclusionBackground 6% PatientSummary 4cX CharacteristicsCard io logyHeader Freq.too%94%94%92%79%40%29%2s%25%2?.%17%9%Figure 1: Highest-frequency headers (with relative oc-currence frequencies)Because the type of research reported in the compu-tational linguistics corpus differs so much, the descrip-tion of document structure we were looking for had tobe flexible enough to generalize over differences in pre-sentation, yet formal enough for the extraction of theinformation units which are useful for automatic ab-stracts.
We base our model of argumentation in scien-tific articles on Swales' (1990) CAlkS model ("Create aResearch Space").
Swales' claim is that the main com-municative goal of an author of a research article is toconvince readers (potential reviewers) that the researchdescribed in the paper constitutes an actual contribu-tion to science, in order to have the paper reviewedpositively and thus published; this is the case whetheror not the paper tries to give the impression that itreports research in an objective, disinterested way.
Inorder to successfully present heir case, authors arguein a goal-directed and prototypical way about problem-solving activities - -  their own and other researchers'.Swales identified prototypical rhetorical building plansof introduction sections, along with linguistics urfacecues that signal rhetorical moves.
Examples for rhetor-44ical moves include the claim that the paper addresses anew problem or, if it is a well-known problem, then thepresented solution has to be better than that of otherresearchers.A first analysis of the corpora confirmed many ofthe rhetorical building blocks suggested by Swales.
Weadapted Swales' scheme to the one shown in Figure 8(at the end of this paper).
In the medical corpus, al-most all of the moves we found were of type I (Explicitmention); the rigid document structure seems to havereplaced much of the "argument about problem-solvingactivities" (types II-V) for which we found ample evi-dence in the computational linguistics corpus.We are interested in identifying these moves auto-matically and shallowly in text, and we believe thatthis is technically feasible, because the stereotypical,predictable overall structure of the argument can beexploited in doing so.3 Meta -d iscourse  markersIn this paper we focus on the linguistic realisations ofrhetorical moves, i.e.
the surfacy signals of the argumen-tative status of a given sentence.
Consider the strings inboldface on the right hand side of Figure 8.
They coverthe activities of reporting about research (reporting andpresenting verbs), the problem-solving process (prob-lems, solutions, tasks); they also include other semanticlinks like necessity, causality and contrast.
Due to ex-plicit or implicit argumentation, many of these stringsare evaluative ("efficient, elegant, innovative, insight.ftd" vs. "impossible, inadequate, inconclusive, insuf-ficient").
We call them meta-comments because theytalk about information units, as opposed to being sub-ject matter (scientific content).
Such meta-commentsare very frequent in our collection.Our meta-comments are similar Paice's (1981) indi-cator phrases (he was the first to use such phrases forabstracting); they are less similar to cue phrases, thediscourse markers usually studied in discourse analysis,because they are not sentence connectives (with someexceptions), and because they are typically consider-ably longer and far more varied.The fact that the computational linguistics textsstem from an unmoderated medium (i.e.
they are nei-ther chosen for publication or edited by a central au-thority), means that there were no external restrictionson how exactly to say things.
Authors use idiosyncraticstyle, which can vary from formal to informal.
Thereare meta-comments that tend to get used in a fixed, for-mulaic way, but interestingly, we observed a wide rangeof linguistic variability with respect o the realization ofsome of the meta-comments (whereas their semantics iusually perfectly unambiguous).
This effect makes themeta-commen~ in this text type interesting linguisticobjects to study.We observed that there are certain meta-commentswhich are restricted to certain moves, mostly the evalu-ative and contrastive phrases and the phrases occurringin moves of type I (Explicit mention).
Others occur fre-quently across moves, particularly general argumenta-tive phrases and relevance markers uch as "important","in this paper, we".
Argumentative phrases like "we ar.9ue that" appeared with solution and problem-relatedmoves almost as often as with claims and conclusions.These phrases seemed to be the ones that were mostformulaic/fixed across texts.Our goal is to automatically find meta-markers andassociate them with rhetorical moves (where this makessense).
In the next section, we report on a first experi-ment in that direction.4 Our  exper imentIf it is true that most meta-comments are formulaic, re-curring expressions, then frequency information shouldhelp us separate meta-comments from domain-specificparts of the sentence.
Those strings which occur rarelyin the corpus will most likely be domain-specific andwill appear low on frequency listings of strings, whereasmeta-commeats should appear high on the lists.We also used a lexicon of 433 lexical seeds.
Lexi-cal seeds are words which are semantically related tothe activities of reporting, problem-solving, argument-ing or evaluating, or expressions of deixis ( "we... ") orother textual cues (e.g.
literature references in text weremarked up using the symbol \[REF\], which is a signalfor mentions of other researchers' olutions, tasks orproblems).The computational linguistics corpus was drawnfrom the computation and language archive(h~p: / /xxx .
lan l .gov /cmp- lg )  and contains 123articles; the 129 articles of the cardiology corpusappeared in the American Heart Journal.
The medicalcorpus is smaller in overall size (436,909 words vs.654,477; 14,770 sentences vs. 23,072).For the computational linguistics corpus, we addi-tionally had a collection of 948 sentences that had beenidentified as relevant by a human annotator in a priorexperiment (Teufel & Moens 1997).
A human judge an-notated these with respect to the 23 rhetorical movesintroduced in Figure 8.4.1 Fi lter ingFirst, we compiled the two corpora into those bigrams,trigrams, 4-grams, 5-grams and 6-grams which did notcross sentence boundaries.
We worked with a shortstop-list compiled from the corpus (60 highest-frequentwords) from which we had excluded those which we ex-pected to be important in an argumentative domain,e.g.
personal and demonstrative pronouns.
We low-ercased all words and counted punctuation (includingbrackets) as a full word.We then filtered the n-grams through our seed lexi-con, i.e.
we retained those expressions which contain at45least one of the words of the seed-lexicon (or a morpho-logical variant of it).
We also compiled and countedn-grams for the 948 computational  linguistics targetsentences, to see how similar these phrases from the an-notated parts were to the filtered or unfiltered bigramsfrom the entire corpus.Be fore  f i l ter ing354 \[ref\], \[ref\] ,3or \[ref\] ),297 , \[ref\], \[ref\]tTs Ccrefl ) .144 \[ref\], \[ref\] .139 on the other hand134 for example , the118 in this paper,116 the other hand ,111 ?
in ( \[cref\] )110 can be used toAf ter  f i l ter ing118 in this paper,111 in (\[cref\])110 can be used to106 in figure \[cref\] .100 ( \[cref\] ) ,99 on the basis of83 shown in figure \[cref\]83 in section \[cref\] .75 this paper, we75 in section \[cref\] ,71 it is possible toFigure 2: 4-grams in entire CL corpusThe list of 4-grams for the computational linguis-tics corpus shows a typical picture of the outcome ofthis process (Figure 2).
Before filtering, the frequentcorpus n-grams contain general comments and expres-sions like "for example" but content specific expressionsare already filtered out.
(Very rarely, there are somecontent-specific phrases like "natural anguage ~' in thelists - -  this is due to the fact that the corpus, eventhough interdisciplinary in nature, is composed of pa-pers focusing on language.)
After filtering, the meta-comments  on the lists have two properties: (a) theyare frequent (b) they contain lexical items that couldbe related to argumentation about problems, researchtasks, solutions - -  in the computational linguistics cor-pus, these conditions eem to be enough to produce ex-pressions that are good candidates for meta-comments.Unfortunately, condition (a) means that a large numberof recta-comments were lost, because they were of lowfrequency.Target  sentences49 in this paper , \[0 can be used to37 this paper , we 9 in this paper is25 Cref\], \[ref\], 7 \[ref\], \[ref\].22 , Ire.f\], Cref\] 7 described in this paper18 in this paper we 7 this paper ~ iFigure 3: 4-grams in CL target sentencesHow similar are the lists for annotated text and entirecorpus in the computat ional  linguistics domain?
Table3 shows that  they look very similar apart from minordifferences, e.g.
the fact that the list gained from anno-tated data  contains more \[CREF\] items (internal crossreference like ~section \[CREF\]") which tend to appearfrequently in the sentences where authors state the or-ganisation of the paper.
The expressions used in suchorganisation statements are typically formulaic and re-Be fore  f i l ter ing120 p < 0.05 )102 p<0.0t )93 left ventricular ejec-tion fraction86 p < 0.001 )86 p < 0.0001 )85 on the basis of72 at the time of66 of this study was64 in this study ,59 95 % confidence in-terval58 this study was to55 coronary artery dis-ease .49 acute myocardial in-farction .~f thr  f i l te r in~85 on the basis of66 of this study was64 in this study ,58 this study was to57 patients with heartfailure45 the purpose of this44 there were no signifi-cant40 new york heart asso-ciation39 purpose of this study35 there was no signifi-cant32 were no significantdifferences31 p = not significant30 has been shown toFigure 4: 4-grams m medical corpuscurrent, but not many of these sentences were consid-ered relevant when the 948 target sentences were deter-mined.The medical corpus shows significant differences (cf.Figure 4).
Firstly, unfiltered bigrams do not sepa-rate content matter from meta-discourse.
In theselists, there are phrases pertaining to statistical analyses("p < 0.0l") and several domain-specific phrases.
Fil-tering (right hand side of Figure 4) forces the few meta-comments that are being used to the top of  the list; theyare linguistically invariant.
For instance, "studf  seemsthe only acceptable xpression used for the current re-search, whereas the range is much wider in the othercorpus ( "paper, article, study, work, research... ", andall the meta-comment candidates in the top part of thelist belonged to one single rhetorical move, viz.
ExplicitMention of  the Research Task (Ex -T  in Figure 8).A certain amount of noise has been introducedthrough the seed-lexicon because word senses were notdisambiguated: 'failure" was included in the seed lexi-con to indicate mentions of failure of other researchers'solution.
Because this term obviously has the differentmeaning of "heart failure" in the cardiology context, thedesired distinction between subject matter  strings andrecta-comments got lost; similarly "New York" was in-cluded because the word "new" could potential ly pointto novel approaches.
This might mean that  it is neces-sary to use different stop-lists and/or  seed lexicons fordifferent domains.As we have seen before, associating meta-commentswith rhetorical moves is a more difficult task for somemeta-comments than for others.
We tried to anchor theprobable rhetorical move of a phrase in the lexical seedit contains, a simplification we are forced to make due46to the small amount of annotated text we have available(which is reflected in the low numbers).
We are thus inthe process of working on a larger scale annotation.We used the human judgements to count how ofteneach word contained in the target sentences appearswith a certain rhetorical move.
If the difference in fre-quency between the best-scoring moves for that wordwas large enough, we assumed it was a good indicatorfor the highest-scoring move, and we then manually as-sociated the given rhetorical move with the word if itwas contained in the seed lexicon, or to semanticallysimilar seeds.
For example, seeds that are the mostlikely associated with the OWN SOLUTION BETTER (53examples of this move in the target sentences) were"than" (39), "better" (36), "results" (21), "method"(19), "using" (15), "significantly" (14), " outperforms"(12) and "more" (12).
Filtered meta-comments arethen assigned the rhetorical move predicted by the firstseed they contain.
Figure 5 shows the meta-commentsfiltered for the seed "better" from both corpora.
In themedical corpus, there is less argument about method-ology/solutions, and as a result the phrases found areunfortunately not meta-comments but contain medicalterminology.Computat iona lL ingu is t ics64 better than50 a better23 better than the20 much better19 the better19 is better16 significantly better16 be better13 better performance11 are better10 better the9 significantly betterthan6 better suited6 better than that of,=CardiologyI t  a better7 better than6 to better6 significantly better6 better in5 better left5 better left ventricu.lar5 and better4 better preserved4 better in smokers3 to be somewhat bet-ter tolerated3 failure symptoms inspite of better3 better preserved leftventricular systolicfunction3 better in smokersthan in nonsmokersFigure 5: Potential meta-comments with "better" fromboth corporaAlso, we observed that it is not easy to predict he op-t imal length of a certain meta-comment which is indica-tive of a certain rhetorical move.
For moves containingocher problems/solutions~tasks the very short string"\[REF\]" is contained in all successful meta-comments,whereas for explicit mention of research goals, the maxi-mal length 6 of meta-comments which we chose for theseexperiments might even be too short.
As another exam-ple, for the STEP move ("goa/ is achieved by doing so-lution"), the best indicator we found was "in order to".4.2 Eva luat ionWe evaluate the quality of these automatical ly gener-ated meta-comment lists by comparing them to a man-ually created meta-comment list used by a summarisa-tion system, cf.
(Teufel & Moens To Appear).
The per-formance of the system - with the two different meta-comment lists - is measured by precision and recall val-ues of co-selection with the target extracts defined byhuman annotators mentioned earlier.
The summarisa-tion process consists of two consecutive steps, sentenceextraction and rhetorical classification, and uses otherheuristics like location and term frequency.The summarisation system requires a list of meta-comments of arbitrary length, containing a quality scorefor each phrase which estimates how predictive thesephrases are in pointing to extract-worthy sentences, andthe most likely rhetorical label that sentences with thismeta-comment will receive.QualityClass23232Ii322IIIit2iIIIIIIiRhetoricalMoveSTEPEx-TCo-SEx-TEx-TEx-PEx-TEx-TEx-TEx-CEx-TEx-TEx-ENEc-S-TCo-CEx-TMeta -commentpaper ,this paper presents ain order toin this paper , we willin this paper we haveunlike \[ref\]this paper is toin this paper , we describepaper ispaper wethis paper has presented, we propose a methodin p~sage ( \[cref\], we argue thatargue thatmethod forwe show that theshow howproperty and the numberthe advantages ofthe wall street journalthe importance ofhowever , webe used toFigure 6: Extracts from automatic list of meta-commentsWe automatically built the meta-comment list in Fig-ure 6 (containing 318 entries).
We started from alln-grams compiled from the target sentences and tookthe following heuristics into account: Firstly, choosephrases with a high ratio of target frequency to cor-pus frequency, because these are indicative phrases.Set the quality value accordingly.
Secondly, excludephrases with a low overall frequency, or decrease theirquality score, because including/overestimating themmight construct a model that is over-fitted to the data.Thirdly, associate each phrase with its most likely47rhetorical move, by taking the ratio between frequencyin each rhetorical class and the frequency of the rhetor-ical label itself into account.
If below a certain thresh-old, don't associate any move at all (e.g.
"paper ," inFigure 6).The manual meta-comment list, in contrast, wascompiled in an extremely labour intensive manner andrefined over the months.
It consists of 1791 meta-comments (some of which are much longer than themaximum of 6 words that the automatic phrases con-sisted of), along with their most plausible rhetoricalmoves and quality scores.ExtractionClassificationManual Automatic66.4% 52.5%66.3% 54.3%Figure 7: Evaluation results: precmon and recall of co-selectionAs Figure 7 shows, using the automatic meta-comment list instead of the manually created one de-creased the summarizer's performance from 66.4% to52.5% precision and recall for extraction, and from66.3% to 54.3% precision and recall for classification.5 Discussion and further workThe evaluation indicates that the quality of the au-tomatic meta-comment list is not yet high enough toreplace the manual ist in our summarization system.However, a look at the automatic list itself showsthat, even though it is far from perfect, most of thehigh-frequent s rings found are plausible candidates forrecta-comments (or parts of recta-comments).
In mostcases, subject matter can be successfully filtered out.We regard the simple method for automatic meta-comment identification discussed in this paper as abaseline for further work.
We have simplified the prob-lem of finding meta-comments enormously by only con-sidering verbatim substrings.
By doing so, we haveignored discontinuous strings, morphological variationand statistical interaction between the words in thestring.
In addition, the phrases considered so far havebeen short, and we have not collected many of them,because we wanted to rely only on the ones with rea-sonably high frequencies.The biggest problem for now is that highly indica-tive, but infrequent recta-comments cannot be foundwith a simple method like ours.
Therefore, it is essen-tial to perform some generalization over similar phrases.One way would be the automatic lustering of similarconcepts, e.g.
to find out that "argue" and "show" arepresentational verbs with similar semantics.
Anotheridea would be to allow for more flexible patterns con-sisting of short n-grams and other words, in order toskip over intervening words like adjectives and adverbs.This might avoid the data sparseness problems encoun-tered with the longer n-grams.6 SummaryWe have presented some baseline results from our on-going work concerning automatic filtering of meta-comments (indicator phrases) from scientific papers.Meta-comments can vary considerably from one domainto another, as the comparison of the two corpora weconsidered shows.
In the computational linguistics ar-ticles, authors argue explicitly about problems, solu-tions and research tasks, whereas this is less the casein the medical domain, where meta-comments are lessfrequent and more formulaic.We have shown that lists of recta-comments acquiredin a simple automatic process can be used to automati-cally identify a shallow document structure in scientifictext, albeit with a certain quality loss when comparedto manually constructed resources.7 AcknowledgementsData collection of the computational linguistics corpustook place collaboratively with Byron Georgantopolous.We thank Kathy McKeown.
Vasileios Hatzivassiloglouand Olga Merport from Columbia University, NY, forkindly allowing us to use their cardiography corpus.ReferencesBrandow, R.; Mitze, K.; and Rau, L. F. 1995.
Automaticcondensation of electronic publications by sentence selec-tion.
Information Processing and Management 31(5):675-685.Hovy, E. H., and Lin, C. Y.
1997.
Automated text sum-marization in SUMMARIST.
ACL/EACL-97 Workshop'Intelligent Scalable Text Summarization'.Kintsch, W., and van Dijk, T. A.
1978.
Toward a model oftext comprehension a d production.
Psychological Review85{5):363-.-394.Kircz, J. G. 1991.
The rhetorical structure of scientific arti-cles: the case for argumentational analysis in informationretrieval.
Journal of Documentation 47(4):354-372.Kupiec, J.; Pedersen, J. O.; and Chen, F. 1995.
A trainabledocument summarizer.
In Proceedings of the 18th ACM-SIG IR Conference, Association for Computing Machinery,Special Interest Group Information Retrieval, 68-73.Salager-Meyer, F. 1992.
A text-type and move analysisstudy of verb tense and modality distributions in medicalEnglish abstracts.
English for Specific Purposes 11:93-113.Sparck Jones, K. 1994.
Discourse modelling for automaticsummarising.
Technical report TR-290, Computer Labo-ratory, University of Cambridge.Swales, J.
1990.
Genre analysis: English in academic andresearch settings.
Cambridge University Press.Teufel, S., and Moens, M. 1997.
Sentence xtraction as aclassification task.
In ACL/EACL.97 Workshop 'Intelli-gent Scalable Text Summarization', 58---65.Teufel, S., and Moens, M. To Appear.
Argumentative clas-sification of extracted sentences as a first step towards flex-ible abstracting.
In Mani, I., and Maybury, M., eds., Ad-vances in automatic text summarization.
MIT Press.48I.
In t roduct ion  of problems, tasks, so lu t ions  by  exp l i c i t  ment ionEx-T Own research task The  a im of  th is  paper  is to examine the role that training plays in thetagging process.Ex-S Own solution The  basic  idea for the  analys is  can be seen as a logical counterpart at theglue level of the standard type assignment for generalized quant/fiers \[REF\].Ex-O-S Other solution The  t rad i t lona l  approach  has been to plot isoglosses, delineating regionswhere the same word is used for the same concept.Ex-CEx-O-CEx-EEx-ROwn conclusions/ claimsOther conclusions/claimsOwn evaluation method-ologyOwn (numerical) resultsIn our corpus study, we found that  three types of utterances (prompts,repetitions and summaries) were consistently used to signa/control shifts.I t  has often been s ta ted  that  discourse is an inherently collaborative pro-cess.In this section we evaluate the per fo rmance  of  the methodology imple-mented by predicting succeeding words by using preced/ng words.The eva luat ion  of the  accuracy of  the rhetorical structure analysis carriedout previously (\[REF\]) showed 74 %.I I .
Cont ras t ive  in t roduct ion  o f  p rob lems,  tasks ,  so lu t ionsCo-SCo-CContrast between, ownand other solutionsContrast between ownand other tasks/ prob-lemsContrast between ownand other claimsIn this paper, we argue that instead of  applying the arbitration process to thediscourse level, it should  be applied to ~he beliefs proposed by the discourseactions.Unl ike most  research in pragmatics that focuses on certain types of preysuppositions or implicatures, we prov ide  a global framework in which onecan express all these types of pragmatic inferences.Desp i te  the  hypothes is  that  the free word order of German leads to poorperformance of low order HMM taggers when compared with a language likeEngfish, we have shown that  the overall results for German are very muchMong the lines of comparable implementations for English, if not better.I I I .
A t t r ibut ion  o f  p roper t ies  toIMP-THAR.D-TNEw-TGOoD-SBAD-O-SOwn research task is im-portantOwn research task ishardOwn problem/ researchtask is newOwn solution is advanta-geousOther solution is flawedprob lems,  tasks ,  so lu t ionsThe  last decade has seen a growing in teres t  in the application of machinelearning to d/fferent kinds of linguistic domains.One of  the  diff icult  p rob lems in machine translation from Japanese toEnglish or other European languages i  the treatment of articles and numbers.No formal framework has been proposed ,  to our  knowledge,  to regulatethe interaction between regular and exceptional grammatical resources.First, it is in certain respects impler ,  in that it requ i res  no postulation ofotherwise unmotivated ambiguities in the source clause.However, we argue that such formalisms offer l i t t le  he lp  to computationallinguists in practice.IV .
Funct iona l  re la t ions  betweenSOLVES Own solution solves ownAvomsSTEPNEc-S-TNoTSONEw-Presearch taskOwn solutionproblemsavoidsOwn solution is a steptowards research taskOwn solution necessaryto achieve research taskOther solution does notsolve problem/ taskOther solution intro-duces new problemsprob lems,  tasks ,  so lu t ionsThis  account  also exp la ins  s/m/Mr differences m felicity for other coordinat-ing conjunctions as discussed in \[REF\].We have introduced a simple, natural definition of synchronous tree-adjoiningderivation that  avoids the expressivity and implementability prob lems ofthe original rewriting definition.We have thus deve loped an evaluation heuristic that combines everal dif-ferent measures, in o rder  to select the parse that is deemed overall "best".We have argued that obligations play an impor tant  role in accounting forthe interactions in diMog.Dependency grammar uns into substant ia l  d i f f icu l ty  t ry ing  to accountfor the proform one.Specifically, i f a  t reatment  such as \ [REF \ ] ' s  is used to explain the forwardprogression of time in example \[CREF\], then  it must  be exp la ined  whysentence \[CREF\] is as felicitous as sentence \[CREF\].V.
DirectBETTEa-SHARDEa-Tcompar ison of problems, tasks, solutionsOwn solution is better We found that the MDL-based method per forms bet ter  than  the MLE-than other solution based method.Own research task is ... disambiguating word senses to the level of l~ne-grainedness found in Word-harder than other task Net is qu i te  a bit more diff icult than  disambiguation to the level of ho-mographs \[REF\], \[REF\].Figure 8: Rhetor ical  moves in scientific papers; examples from our corpus of computat iona l  l inguistics49
