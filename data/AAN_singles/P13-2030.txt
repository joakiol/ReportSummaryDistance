Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 166?170,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAn Improved MDL-Based Compression Algorithm forUnsupervised Word SegmentationRuey-Cheng ChenNational Taiwan University1 Roosevelt Rd.
Sec.
4Taipei 106, Taiwanrueycheng@turing.csie.ntu.edu.twAbstractWe study the mathematical properties ofa recently proposed MDL-based unsuper-vised word segmentation algorithm, calledregularized compression.
Our analysisshows that its objective function can beefficiently approximated using the nega-tive empirical pointwise mutual informa-tion.
The proposed extension improves thebaseline performance in both efficiencyand accuracy on a standard benchmark.1 IntroductionHierarchical Bayes methods have been main-stream in unsupervised word segmentation sincethe dawn of hierarchical Dirichlet process (Gold-water et al, 2009) and adaptors grammar (Johnsonand Goldwater, 2009).
Despite this wide recog-nition, they are also notoriously computationalprohibitive and have limited adoption on largercorpora.
While much effort has been directedto mitigating this issue within the Bayes frame-work (Borschinger and Johnson, 2011), manyhave found minimum description length (MDL)based methods more promising in addressing thescalability problem.MDL-based methods (Rissanen, 1978) rely onunderlying search algorithms to segment the textin as many possible ways and use descriptionlength to decide which to output.
As differ-ent algorithms explore different trajectories inthe search space, segmentation accuracy dependslargely on the search coverage.
Early work in thisline focused more on existing segmentation algo-rithm, such as branching entropy (Tanaka-Ishii,2005; Zhikov et al, 2010) and bootstrap voting ex-perts (Hewlett and Cohen, 2009; Hewlett and Co-hen, 2011).
A recent study (Chen et al, 2012) ona compression-based algorithm, regularized com-pression, has achieved comparable performanceresult to hierarchical Bayes methods.Along this line, in this paper we present a novelextension to the regularized compressor algorithm.We propose a lower-bound approximate to theoriginal objective and show that, through analy-sis and experimentation, this amendment improvessegmentation performance and runtime efficiency.2 Regularized CompressionThe dynamics behind regularized compression issimilar to digram coding (Witten et al, 1999).
Onefirst breaks the text down to a sequence of char-acters (W0) and then works from that represen-tation up in an agglomerative fashion, iterativelyremoving word boundaries between the two se-lected word types.
Hence, a new sequence Wiis created in the i-th iteration by merging all theoccurrences of some selected bigram (x, y) in theoriginal sequence Wi?1.
Unlike in digram cod-ing, where the most frequent pair of word types isalways selected, in regularized compression a spe-cialized decision criterion is used to balance com-pression rate and vocabulary complexity:min.
?
?f(x, y) + |Wi?1|?H?(Wi?1,Wi)s.t.
either x or y is a characterf(x, y) > nms.Here, the criterion is written slightly differ-ently.
Note that f(x, y) is the bigram fre-quency, |Wi?1| the sequence length of Wi?1, and?H?
(Wi?1,Wi) = H?(Wi)?
H?
(Wi?1) is the dif-ference between the empirical Shannon entropymeasured on Wi and Wi?1, using maximum like-lihood estimates.
Specifically, this empirical esti-mate H?
(W ) for a sequence W corresponds to:log |W | ?
1|W |?x:typesf(x) log f(x).For this equation to work, one needs to estimateother model parameters.
See Chen et al (2012)for a comprehensive treatment.166f(x) f(y) f(z) |W |Wi?1 k l 0 NWi k ?m l ?m m N ?mTable 1: The change between iterations in wordfrequency and sequence length in regularizedcompression.
In the new sequence Wi, each oc-currence of the x-y bigram is replaced with a new(conceptually unseen) word z.
This has an effectof reducing the number of words in the sequence.3 Change in Description LengthThe second term of the aforementioned objectiveis in fact an approximate to the change in descrip-tion length.
This is made obvious by coding up asequence W using the Shannon code, with whichthe description length ofW is equal to |W |H?
(W ).Here, the change in description length between se-quences Wi?1 and Wi is written as:?L = |Wi|H?
(W )?
|Wi?1|H?(Wi?1).
(1)Let us focus on this equation.
Suppose that theoriginal sequence Wi?1 is N -word long, the se-lected word type pair x and y each occurs k and ltimes, respectively, and altogether x-y bigram oc-curs m times in Wi?1.
In the new sequence Wi,each of the m bigrams is replaced with an un-seen word z = xy.
These altogether have reducedthe sequence length by m. The end result is thatcompression moves probability masses from oneplace to the other, causing a change in descrip-tion length.
See Table 1 for a summary to thisexchange.Now, as we expand Equation (1) and reorganizethe remaining, we find that:?L = (N ?m) log(N ?m)?N logN+ k log k ?
(k ?m) log(k ?m)+ l log l ?
(l ?m) log(l ?m)+ 0 log 0?m logm(2)Note that each line in Equation (2) is of the formx1 log x1 ?
x2 log x2 for some x1, x2 ?
0.
Weexploit this pattern and derive a bound for ?Lthrough analysis.
Consider g(x) = x log x.
Sinceg??
(x) > 0 for x ?
0, by the Taylor series we havethe following relations for any x1, x2 ?
0:g(x1)?
g(x2) ?
(x1 ?
x2)g?(x1),g(x1)?
g(x2) ?
(x1 ?
x2)g?
(x2).Plugging these into Equation (2), we have:m log (k ?m)(l ?m)Nm ?
?L ?
?.
(3)The lower bound1 at the left-hand side is a best-case estimate.
As our aim is to minimize ?L, weuse this quantity to serve as an approximate.4 Proposed MethodBased on this finding, we propose the followingtwo variations (see Figure 1) for the regularizedcompression framework:?
G1: Replacing the second term in the origi-nal objective with the lower bound in Equa-tion (3).
The new objective function is writ-ten out as Equation (4).?
G2: Same as G1 except that the lower boundis divided by f(x, y) beforehand.
The nor-malized lower bound approximates the per-word change in description length, as shownin Equation (5).
With this variation, the func-tion remains in a scalarized form as the orig-inal does.We use the following procedure to compute de-scription length.
Given a word sequence W , wewrite out all the induced word types (say, M typesin total) entry by entry as a character sequence, de-noted as C. Then the overall description length is:|W |H?
(W ) + |C|H?
(C) + M ?
12 log |W |.
(6)Three free parameters, ?, ?, and nms remain tobe estimated.
A detailed treatment on parameterestimation is given in the following paragraphs.Trade-off ?
This parameter controls the bal-ance between compression rate and vocabularycomplexity.
Throughout this experiment, we es-timated this parameter using MDL-based gridsearch.
Multiple search runs at different granular-ity levels were employed as necessary.Compression rate ?
This is the minimumthreshold value for compression rate.
The com-pressor algorithm would go on as many iterationas possible until the overall compression rate (i.e.,1Sharp-eyed readers may have noticed the similarity be-tween the lower bound and the negative (empirical) point-wise mutual information.
In fact, when f(z) > 0 in Wi?1, itcan be shown that limm?0 ?L/m converges to the empiricalpointwise mutual information (proof omitted here).167G1 ?
f(x, y)(log (f(x)?
f(x, y))(f(y)?
f(x, y))|Wi?1|f(x, y)?
?
)(4)G2 ?
?
?f(x, y) + log(f(x)?
f(x, y))(f(y)?
f(x, y))|Wi?1|f(x, y)(5)Figure 1: The two newly-proposed objective functions.word/character ratio) is lower than ?.
Setting thisvalue to 0 forces the compressor to go on untilno more can be done.
In this paper, we exper-imented with predetermined ?
values as well asthose learned from MDL-based grid search.Minimum support nms We simply followed thesuggested setting nms = 3 (Chen et al, 2012).5 Evaluation5.1 SetupIn the experiment, we tested our methods onBrent?s derivation of the Bernstein-Ratner cor-pus (Brent and Cartwright, 1996; Bernstein-Ratner, 1987).
This dataset is distributed via theCHILDES project (MacWhinney and Snow, 1990)and has been commonly used as a standard bench-mark for phonetic segmentation.
Our baselinemethod is the original regularized compressor al-gorithm (Chen et al, 2012).
In our experiment, weconsidered the following three search settings forfinding the model parameters:(a) Fix ?
to 0 and vary ?
to find the best value (inthe sense of description length);(b) Fix ?
to the best value found in setting (a)and vary ?
;(c) Set ?
to a heuristic value 0.37 (Chen et al,2012) and vary ?.Settings (a) and (b) can be seen as running astochastic grid searcher one round for each param-eter2.
Note that we tested (c) here only to comparewith the best baseline setting.5.2 ResultTable 2 summarizes the result for each objectiveand each search setting.
The best (?, ?)
pair for2A more formal way to estimate both ?
and ?
is to runa stochastic searcher that varies between settings (a) and (b),fixing the best value found in the previous run.
Here, forsimplicity, we leave this to future work.Run P R FBaseline 76.9 81.6 79.2G1 (a) ?
: 0.030 76.4 79.9 78.1G1 (b) ?
: 0.38 73.4 80.2 76.8G1 (c) ?
: 0.010 75.7 80.4 78.0G2 (a) ?
: 0.002 82.1 80.0 81.0G2 (b) ?
: 0.36 79.1 81.7 80.4G2 (c) ?
: 0.004 79.3 84.2 81.7Table 2: The performance result on the Bernstein-Ratner corpus.
Segmentation performance is mea-sured using word-level precision (P), recall (R),and F-measure (F).G1 is (0.03, 0.38) and the best for G2 is (0.002,0.36).
On one hand, the performance ofG1 is con-sistently inferior to the baseline across all settings.Although approximation error was one possiblecause, we noticed that the compression processwas no longer properly regularized, since f(x, y)and the ?L estimate in the objective are intermin-gled.
In this case, adjusting ?
has little effect inbalancing compression rate and complexity.The second objective G2, on the other hand,did not suffer as much from the aforementionedlack of regularization.
We found that, in all threesettings, G2 outperforms the baseline by 1 to 2percentage points in F-measure.
The best perfor-mance result achieved by G2 in our experiment is81.7 in word-level F-measure, although this wasobtained from search setting (c), using a heuristic?
value 0.37.
It is interesting to note that G1 (b)and G2 (b) also gave very close estimates to thisheuristic value.
Nevertheless, it remains an openissue whether there is a connection between theoptimal ?
value and the true word/token ratio (?0.35 for Bernstein-Ratner corpus).The result has led us to conclude that MDL-based grid search is efficient in optimizing seg-mentation accuracy.
Minimization of descrip-tion length is in general aligned with perfor-mance improvement, although under finer gran-ularity MDL-based search may not be as effec-168Method P R FAdaptors grammar, colloc3-syllable Johnson and Goldwater (2009) 86.1 88.4 87.2Regularized compression + MDL, G2 (b) ?
79.1 81.7 80.4Regularized compression + MDL Chen et al (2012) 76.9 81.6 79.2Adaptors grammar, colloc Johnson and Goldwater (2009) 78.4 75.7 77.1Particle filter, unigram Bo?rschinger and Johnson (2012) ?
?
77.1Regularized compression + MDL, G1 (b) ?
73.4 80.2 76.8Bootstrap voting experts + MDL Hewlett and Cohen (2011) 79.3 73.4 76.2Nested Pitman-Yor process, bigram Mochihashi et al (2009) 74.8 76.7 75.7Branching entropy + MDL Zhikov et al (2010) 76.3 74.5 75.4Particle filter, bigram Bo?rschinger and Johnson (2012) ?
?
74.5Hierarchical Dirichlet process Goldwater et al (2009) 75.2 69.6 72.3Table 3: The performance chart on the Bernstein-Ratner corpus, in descending order of word-level F-measure.
We deliberately reproduced the results for adaptors grammar and regularized compression.
Theother measurements came directly from the literature.tive.
In our experiment, search setting (b) wonout on description length for both objectives, whilethe best performance was in fact achieved by theothers.
It would be interesting to confirm thisby studying the correlation between descriptionlength and word-level F-measure.In Table 3, we summarize many published re-sults for segmentation methods ever tested on theBernstein-Ratner corpus.
Of the proposed meth-ods, we include only setting (b) since it is moregeneral than the others.
From Table 3, we find thatthe performance of G2 (b) is competitive to otherstate-of-the-art hierarchical Bayesian models andMDL methods, though it still lags 7 percentagepoints behind the best result achieved by adap-tors grammar with colloc3-syllable.
We also com-pare adaptors grammar to regularized compressoron average running time, which is shown in Ta-ble 4.
On our test machine, it took roughly 15hours for one instance of adaptors grammar withcolloc3-syllable to run to the finish.
Yet an im-proved regularized compressor could deliver theresult in merely 1.25 second.
In other words, evenin an 100 ?
100 grid search, the regularized com-pressor algorithm can still finish 4 to 5 times ear-lier than one single adaptors grammar instance.6 Concluding RemarksIn this paper, we derive a new lower-bound ap-proximate to the objective function used in theregularized compression algorithm.
As computingthe approximate no longer relies on the change inlexicon entropy, the new compressor algorithm ismade more efficient than the original.
Besides run-Method Time (s)Adaptors grammar, colloc3-syllable 53826Adaptors grammar, colloc 10498Regularized compressor 1.51Regularized compressor, G1 (b) 0.60Regularized compressor, G2 (b) 1.25Table 4: The average running time in seconds onthe Bernstein-Ratner corpus for adaptors grammar(per fold, based on trace output) and regularizedcompressors, tested on an Intel Xeon 2.5GHz 8-core machine with 8GB RAM.time efficiency, our experiment result also showsimproved performance.
Using MDL alone, oneproposed method outperforms the original regu-larized compressor (Chen et al, 2012) in preci-sion by 2 percentage points and in F-measure by 1.Its performance is only second to the state of theart, achieved by adaptors grammar with colloc3-syllable (Johnson and Goldwater, 2009).A natural extension of this work is to repro-duce this result on some other word segmenta-tion benchmarks, specifically those in other Asianlanguages (Emerson, 2005; Zhikov et al, 2010).Furthermore, it would be interesting to investigatestochastic optimization techniques for regularizedcompression that simultaneously fit both ?
and ?.We believe this would be the key to adapt the al-gorithm to larger datasets.AcknowledgmentsWe thank the anonymous reviewers for their valu-able feedback.169ReferencesNan Bernstein-Ratner.
1987.
The phonology of parentchild speech.
Children?s language, 6:159?174.Benjamin Borschinger and Mark Johnson.
2011.
Aparticle filter algorithm for bayesian word segmen-tation.
In Proceedings of the Australasian LanguageTechnology Association Workshop 2011, pages 10?18, Canberra, Australia, December.Benjamin Bo?rschinger and Mark Johnson.
2012.
Us-ing rejuvenation to improve particle filtering forbayesian word segmentation.
In Proceedings of the50th Annual Meeting of the Association for Com-putational Linguistics (Volume 2: Short Papers),pages 85?89, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Michael R. Brent and Timothy A. Cartwright.
1996.Distributional regularity and phonotactic constraintsare useful for segmentation.
In Cognition, pages 93?125.Ruey-Cheng Chen, Chiung-Min Tsai, and Jieh Hsiang.2012.
A regularized compression method to unsu-pervised word segmentation.
In Proceedings of theTwelfth Meeting of the Special Interest Group onComputational Morphology and Phonology, SIG-MORPHON ?12, pages 26?34, Montreal, Canada.Association for Computational Linguistics.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHAN Workshop on Chinese LanguageProcessing, volume 133.
Jeju Island, Korea.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
2009.
A bayesian framework for wordsegmentation: Exploring the effects of context.Cognition, 112(1):21?54, July.Daniel Hewlett and Paul Cohen.
2009.
Bootstrap vot-ing experts.
In Proceedings of the 21st internationaljont conference on Artifical intelligence, IJCAI?09,pages 1071?1076, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.Daniel Hewlett and Paul Cohen.
2011.
Fully unsuper-vised word segmentation with BVE and MDL.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, pages 540?545, Portland, Oregon.
Associationfor Computational Linguistics.Mark Johnson and Sharon Goldwater.
2009.
Im-proving nonparameteric bayesian inference: exper-iments on unsupervised word segmentation withadaptor grammars.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, NAACL ?09, pages317?325, Boulder, Colorado.
Association for Com-putational Linguistics.Brian MacWhinney and Catherine Snow.
1990.
Thechild language data exchange system: an update.Journal of child language, 17(2):457?472, June.Daichi Mochihashi, Takeshi Yamada, and NaonoriUeda.
2009.
Bayesian unsupervised word segmen-tation with nested Pitman-Yor language modeling.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processingof the AFNLP: Volume 1 - Volume 1, ACL ?09, pages100?108, Suntec, Singapore.
Association for Com-putational Linguistics.Jorma Rissanen.
1978.
Modeling by shortest data de-scription.
Automatica, 14(5):465?471, September.Kumiko Tanaka-Ishii.
2005.
Entropy as an indica-tor of context boundaries: An experiment using aweb search engine.
In Robert Dale, Kam-Fai Wong,Jian Su, and Oi Kwong, editors, Natural LanguageProcessing IJCNLP 2005, volume 3651 of LectureNotes in Computer Science, chapter 9, pages 93?105.
Springer Berlin / Heidelberg, Berlin, Heidel-berg.Ian H. Witten, Alistair Moffat, and Timothy C. Bell.1999.
Managing gigabytes (2nd ed.
): compressingand indexing documents and images.
Morgan Kauf-mann Publishers Inc., San Francisco, CA, USA.Valentin Zhikov, Hiroya Takamura, and Manabu Oku-mura.
2010.
An efficient algorithm for unsuper-vised word segmentation with branching entropyand MDL.
In Proceedings of the 2010 Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP ?10, pages 832?842, Cambridge,Massachusetts.
Association for Computational Lin-guistics.170
