Direct Orthographical Mapping for Machine TransliterationZHANG Min          LI Haizhou        SU JianInstitute for Infocomm Research21 Heng Mui Keng Terrace, Singapore 11961{mzhang, hli, sujian}@i2r.a-star.edu.sgAbstractMachine transliteration/back-transliteration playsan important role in many multilingual speech andlanguage applications.
In this paper, a novelframework for machine transliteration/back-transliteration that allows us to carry out directorthographical mapping (DOM) between twodifferent languages is presented.
Under thisframework, a joint source-channel transliterationmodel, also called n-gram transliteration model (n-gram TM), is further proposed to model thetransliteration process.
We evaluate the proposedmethods through several transliteration/back-transliteration experiments for English/Chinese andEnglish/Japanese language pairs.
Our study revealsthat the proposed method not only reduces anextensive system development effort but alsoimproves the transliteration accuracy significantly.1 IntroductionMany technical terms and proper names, such aspersonal, location and organization names, aretranslated from one language into another languagewith approximate phonetic equivalents.
Thephonetic translation from the native language toforeign language is defined as transliteration;conversely, the process of recalling a word innative language from a transliteration is defined asback-transliteration.
For example, English name?Smith?
and ????
(pinyin 1 : Shi-Mi-Si)?
inChinese form a pair of transliteration and back-transliteration.
In many natural languageprocessing tasks, such as multilingual named entityand term processing, machine translation, corpusalignment, cross lingual information retrieval andautomatic bilingual dictionary compilation,automatic name transliteration has become anindispensable component.Recent efforts are reported for several languagepairs, such as English/Chinese (Meng et al, 2001;Virga et al, 2003; Lee et al, 2003; Gao et al,2004; Guo et al, 2004), English/Japanese (Knightet al, 1998; Brill et al, 2001; Bilac et al, 2004),1 Pinyin is the standard Romanization of Chinese.English/Korean (Oh et al, 2002; Sung et al,2000), and English/Arabic (Yaser et al, 2002).Most of the reported works utilize a phonetic clueto resolve the transliteration through a multiplestep phonemic mapping where algorithms, such asdictionary lookup, rule-based and machinelearning-based approaches, have been wellexplored.In this paper, we will discuss the limitation ofthe previous works and present a novel frameworkfor machine transliteration.
The new frameworkcarries out the transliteration by directorthographical mapping (DOM) without anyintermediate phonemic mapping.
Under thisframework, we further propose a joint source-channel transliteration mode (n-gram TM) as analternative machine learning-based approach tomodel the source-target word orthographicassociation.
Without the loss of generality, weevaluate the performance of the proposed methodfor English/Chinese and English/Japanese pairs.An experiment that compares the proposed methodwith several state-of-art approaches is alsopresented.
The results reveal that our methodoutperforms other previous methods significantly.The reminder of the paper is organized asfollows.
Section 2 reviews the previous work.
Insection 3, the DOM framework and n-gram TMmodel are formulated.
Section 4 describes theevaluation results and compares our method withother reported work.
Finally, we conclude thestudy with some discussions.2 Previous WorkThe topic of machine transliteration has beenstudied extensively for several different languagepairs, and many techniques have been proposed.To better understand the nature of the problem, wereview the previous work from two differentviewpoints: the transliteration framework and thetransliteration model.
The transliteration model isbuilt to capture the knowledge of bilingualphonetic association and subsequently is applied tothe transliteration process.2.1 Transliteration FrameworkThe phoneme-based approach has receivedremarkable attention in the previous works (Menget al, 2001; Virga et al, 2003; Knight et al, 1998;Oh et al, 2002; Sung et al, 2000; Yaser et al,2002; Lee et al, 2003).
In general, this approachincludes the following three intermediatephonemic/orthographical mapping steps:1) Conversion of a source language word intoits phonemic representation (grapheme-to-phoneme conversion, or G2P);2) Transformation of the source languagephonemic representation to the targetlanguage phonemic representation;3) Generation of target language orthographyfrom its phonemic representation (phoneme-to-grapheme conversion, or P2G).To achieve phonetic equivalent transliteration,phoneme-based approach has become the mostpopular approach.
However, the success ofphoneme-based approach is limited by thefollowing constraints:1) Grapheme-to-phoneme conversion,originated from text-to-speech (TTS)research, is far from perfect (TheOnomastica Consortium, 1995), especiallyfor the name of different language origins.2) Cross-lingual phonemic mapping presents agreat challenge due to phonemic divergencebetween some language pairs, such asChinese/English, Japanese/English (Wanand Verspoor, 1998; Meng et al, 2001).3) The conversion of phoneme-to-graphemeintroduces yet another level of imprecision,esp.
for the ideographic language, such asChinese.
Virga and Khudanpur (2003)reported 8.3% absolute accuracy drops whenconverting from Pinyin to Chinese character.The three error-prone steps as stated above leadto an inferior overall system performance.
Thecomplication of multiple steps and introduction ofintermediate phonemes also incur high cost insystem development when moving from onelanguage pair to another, because we have to workon language specific ad-hoc phonic rules.2.2 Transliteration ModelTransliteration model is a knowledge base tosupport the execution of transliteration strategy.
Tobuild the knowledge base, machine learning orrule-based algorithms are adopted in phoneme-based approach.
For instance, noisy-channel model(NCM) (Virga et al, 2003; Lee et al, 2003),HMM (Sung et al, 2000), decision tree (Kang etal., 2000), transformation-based learning (Meng etal., 2001), statistical machine transliteration model(Lee et al, 2003), finite state transducers (Knightet al, 1998) and rule-based approach (Wan et al,1998; Oh et al, 2002).
It is observed that thereported transliteration models share a commonstrategy, that is:1) To model the transformation rules;2) To model the target language;3) To model the above both;However, the modeling of different knowledgeis always done independently.
For example, NCMand HMM (Virga et al, 2003; Lee et al, 2003;Sung et al, 2000) model the transformationmapping rules and the target language separately;decision tree (Kang et al, 2000), transformation-based learning (Meng et al, 2001), finite statetransducers (Knight et al, 1998) and statisticalmachine transliteration model (Lee et al, 2003)only model the transformation rules.3 Direct Orthographical MappingTo overcome the limitation of phoneme-basedapproach, we propose a unified framework formachine transliteration, direct orthographicalmapping (DOM).
The DOM framework tries tomodel phonetic equivalent association by fullyexploring the orthographical contextualinformation and the orthographical mapping.Under the DOM framework, we propose a jointsource-channel transliteration model (n-gram TM)to capture the source-target word orthographicalmapping relation and the contextual information.Unlike the noisy-channel model, the joint source-channel model does not try to capture how thesource names can be mapped to the target names,but rather how both source and target names can begenerated simultaneously.The proposed framework is applicable to alllanguage pairs.
For simplicity, in this section, wetake English/Chinese pair as example in theformulation, where E2C refers to English toChinese transliteration and C2E  refers to Chineseto English back-transliteration.3.1 Transliteration Pair and AlignmentSuppose that we have an English name1... ...i mx x x?
=  and a Chinese transliteration1... ...i ny y y?
= where ix are English letters andjy are Chinese characters.
The English name ?and its Chinese Transliteration ?
can besegmented into a series of substrings:1 2... Ke e e?
=  and 1 2... Kc c c?
=  ( min( , )k m n< ).We call the substring as transliteration unit andeach English transliteration unit ie  is aligned witha corresponding Chinese transliteration unit ic  toform a transliteration pair.
An alignment between?
and ?
is defined as ?
with1 1 1, ,e c e c< > =< >2 2 2, ,e c e c< > =< >  ?and , ,K K Ke c e c< > =< > .
A transliteration pairice >< ,  represents a two-way mapping betweenie  and  ic .
A unit could be a Chinese character ora monograph, a digraph or a trigraph and so on forEnglish.
For example, ?
?|a ?|b ?|ru ?|zzo?
isone alignment of Chinese-English word pair  ??????
and ?abruzzo?.3.2 DOM Transliteration FrameworkBy the definition of ?
, ?
and ?
, the E2Ctransliteration can be formulated as),,(maxarg)),,(maxarg(maxarg),,(maxarg),(maxarg,???????????????????PPPP=?==?
(1)Similarly the C2E back-transliteration as,arg max ( , , )P?
??
?
?
??
(2)To reduce the computational complexity, in eqn.
(1), common practice is to replace the summationwith maximization.The eqn.
(1) and (2) formulate the DOMtransliteration framework.
),,( ??
?P is the jointprobability of ?
, ?
and ?
, whose definitiondepends on the transliteration model which will bediscussed in the next two subsections.
Unlike thephoneme-based approach, DOM does not need toexplicitly model any phonetic information of eithersource or target language.
Assuming sufficienttraining corpus, DOM transliteration framework isto capture the phonetic equivalents throughorthographic mapping or transliterationpair ice >< , .
By eliminating the potentialimprecision introduced through a multiple-stepphonetic mapping in the phoneme-based approach,DOM is expected to outperform.
In contrast tophoneme-based approach, DOM is purely data-driven, therefore can be extended across differentlanguage pairs easily.3.3 n-gram TM under DOMGiven ?
and ?
, the joint probability of),,( ??
?P  is the probability of alignment ?
,which can be formulated as follows:?=?><><==Kkkk cecePPPP111 ),|,()(*)|,(),,( ???????
(3)In eqn.
(3), the transliteration pair is used as thetoken to derive n-gram statistics, so we call themodel as n-gram TM transliteration model.Figure 1.
System structure of DOMThe above block diagram illustrates typicalsystem structure of DOM.
The training of n-gramTM model is discussed in section 3.5.
Given alanguage pair, the bidirectional transliterations canbe achieved with the same n-gram TM and usingthe same decoder.3.4 DOM: n-gram TM vs. NCMNoisy-channel model (NCM) has been wellstudied in the phoneme-based approach.
Let?s takeE2C as an example to look into a bigram case tosee what n-gram TM and NCM present to us underDOM.
We have?=?
?=Kkkkkk ccPcePPPP11 )|(*)|()(*)|,(),,( ???????
(4)?=?><><?=Kkkk cecePPPP11 ),|,()(*)|,(),,( ???????
(5)where eqn.
(4) and (5) are the bigram version ofNCM and n-gram TM under DOM, respectively.The formulation of eqn.
(4) could be interpreted asa HMM that has Chinese units as its hidden statesand English transliteration units as the observations(Rabiner, 1989).
Indeed, NCM consists of twomodels; one is the channel model or transliterationmodel, ?=Kkkk ceP1)|( , which tries to estimate themapping probability between the two units;DOM FrameworkName inLanguage ABi-directionalDecoderName inLanguage Bn-gramTManother is the source model or language model,?=?Kkkk ccP11 )|( , which tries to estimate thegenerative probability of the Chinese name, giventhe sequence of Chinese transliteration units.Unlike NCM, n-gram TM model does not try tocapture how source names can be mapped intotarget names, but rather how source and targetnames can be generated simultaneously.We can also study the two models from thecontextual information usage viewpoint.
One findsthat eqn.
(4) can be approximated by eqn.
(5).)|(*)|(),|(*),,|(),|,(1111?????><><=><><kkkkkkkkkkkccPcePcecPcecePceceP(6)Eqn.
(6) shows us that the context information1, ?>< kce and 1?ke  are absent in the channelmodel and source model of NCM, respectively.
Inthis way, one could argue that n-gram TM modelcaptures more context information than traditionalNCM model.
With adequate and sufficient trainingdata, n-gram TM is expected to outperform NCMin the decoding.3.5 Transliteration Alignment TrainingFor the n-gram TM model training, the bilingualname corpus needs to be aligned firstly at thetransliteration unit level.
The maximum likelihoodapproach, through EM algorithm (Dempster et al,1977) is employed to infer such an alignment.The aligning process is different from that oftransliteration given in eqn.
(1) or (2), here wehave a fixed bilingual entries, ?
and ?
.
Thealigning process is just to find the alignmentsegmentation ?
between the two strings thatmaximizes the joint probability:),,(maxarg ????
?P=   (7)Kneser-Ney smoothing algorithm (Chen et al,1998) is applied to smooth the probabilitydistribution.
NCM model training is carried out inthe similar way to n-gram TM.
The differencebetween the two models lies in eqn (4) and (5).3.6 Decoding IssueThe decoder searches for the most probabilisticpath of transliteration pairs, given the word insource language, by resolving differentcombinations of alignments.
Rather than Viterbialgorithm, we use stack decoder (Schwartz et al,1990) to get N-best results for further processing oras output for other applications.4 The Experiments4.1 Testing EnvironmentsWe evaluate our method through severalexperiments for two language pairs:English/Chinese and English/Japanese.For English/Chinese language pair, we use adatabase from the bilingual dictionary ?ChineseTransliteration of Foreign Personal Names?
(Xinhua, 1992).
The database includes a collectionof 37,694 unique English entries and their officialChinese transliteration.
The listing includespersonal names of English, French, and many otherorigins.
The following results for this language pairare estimated by 13-fold cross validation for moreaccurate.
We report two types of error rates: worderror rate and character error rate.
In word errorrate, a word is considered correct only if an exactmatch happens between transliteration and thereference.
The character error rate is the sum ofdeletion, insertion and substitution errors.
Only thetop choice in N-best results is used for charactererror rate reporting.For English/Japanese language pair, we use thesame database as that in the literature (Bilac et al,2004) 2 .
The database includes 7,021 Japanesewords in katakana together with their Englishtranslation extracted from the EDICT dictionary3.714 tokens of these entries are withheld forevaluation.
Only word error rate is reported for thislanguage pair.4.2 ModelingThe alignment is done fully automatically alongwith the n-gram TM training process.# close set bilingual entries (full data)  37,694# unique Chinese transliteration (close) 28,632# training entries for open test 34,777# test entries for open test 2,896# unique transliteration pairs  T 5,640# total transliteration pairs TW  119,364# unique English units E 3,683# unique Chinese units C 374# bigram TM ),|,( 1?><>< kk ceceP  38,655# NCM Chinese bigram )|( 1?kk ccP  12,742Table 1.
Modeling statistics (E-C)Table 1 reports statistics in the model trainingfor English/Chinese pair, and table 2 is forEnglish/Japanese pair.2 We thank Mr. Slaven Bilac for letting us use histesting setup as a reference.3 ftp://ftp.cc.monash.edu.au/pub/nihongo/.# close set bilingual entries (full data)  7,021# training entries for open test 6,307# test entries for open test 714# unique transliteration pairs  T 2,173# total transliteration pairs TW  28,366# unique English units E 1,216# unique Japanese units J 276# bigram TM 1( , | , )k kP e j e j ?< > < >  9,754Table 2.
Modeling statistics (E-J)4.3 E2C TransliterationIn this experiment, we conduct both open andclosed tests for n-gram TM and NCM modelsunder DOM paradigm.
Results are reported inTable 3 and Table 4.open(word)open(char)Closed(word)closed(char)1-gram 45.6% 21.1% 44.8% 20.4%2-gram 31.6% 13.6% 10.8% 4.7%3-gram 29.9% 10.8% 1.6% 0.8%Table 3.
E2C error rates for n-gram TM tests.open(word)open(char)closed(word)closed(char)1-gram 47.3% 23.9% 46.9% 22.1%2-gram 39.6% 20.0% 16.4% 10.9%3-gram 39.0% 18.8% 7.8% 1.9%Table 4.
E2C error rates for NCM testsNot surprisingly, the result shows that n-gramTM, which benefits from the joint source-channelmodel coupling both source and target contextualinformation into the model, is superior to NCM inall the test cases.4.4 C2E Back-TransliterationThe C2E back-transliteration is morechallenging than E2C transliteration.
Experimentresults are reported in Table 5.
As expected, C2Eerror rate is much higher than that of E2C.open(word)Open(letter)closed(word)closed(letter)1 gram 82.3% 28.2% 81% 27.7%2 gram 63.8% 20.1% 40.4% 12.3%3 gram 62.1% 19.6% 14.7% 5.0%Table 5.
C2E error rate for 3-gram TM testsTable 6 reports the N-best word error rates forboth E2C and C2E which implies the potential oferror reduction by using secondary knowledgesource, such as table looking-up.
The N-best errorrates are also reduced greatly at 10-best level.E2CopenE2CclosedC2EopenC2EClosed1-best 29.9% 1.6% 62.1% 14.7%5-best 8.2% 0.94% 43.3% 5.2%10-best 5.4% 0.90% 24.6% 4.8%Table 6.
N-best word error rates for 3-gram TM4.5 Discussions of DOMDue to lack of standard data sets, the DOMframework is unable to make a straightforwardcomparison with other approaches.
Nevertheless,we list some reported studies on other databases ofE2C tasks in Table 7 and those of C2E tasks inTable 8 for reference purpose.
In Table 7, thereference data are extracted from Table 1 and 3 of(Virga et al, 2003), where only character andPinyin error rates are reported.
The first 4 setupsby Virga et al all adopted the phoneme-basedapproach.
In table 8, the reference data areextracted from Table 2 and Figure 4 of (Guo et al,2004), where word error rates are reported.System Training sizeTestsizePinyinerrorsCharerrorsMeng etal.2,233 1,541 52.5% N/ASmall MT 2,233 1,541 50.8% 57.4%Big MT 3,625 250 49.1% 57.4%Huge MT(Big MT)309,019 3,122 42.5% N/A3-gramTM/DOM34,777 2,896 <10.8% 10.8%3-gramNCM/DOM34,777 2,896 <18.8% 18.8%Table 7.
Performance Comparison of E2CSince we have obtained results in characteralready and the character to Pinyin mapping is one-to-one in the 374 legitimate Chinese characters fortransliteration in our implementation, we expectless Pinyin error than character error in Table 7.TrainingsizeTestsize1-best 10-bestGuo et al 424,788 500 >82.0% >50.0%3-gramTM/DOM34,777 2,896 62.1% 24.6%Table 8.
Performance Comparison of C2EFor E2C, Table 7 shows that even with an 8times larger database than ours, Huge MT (BigMT) test case who reports the best performancestill generates 3 times Pinyin error rate than ours.For C2E, Table 8 shows that even with only 9percent training set, our approach can still make 20percent absolute word error rate reduction.
Thus,although the experiment are done in differentenvironments, to some extend, Table 7 and Table 8reveal that the n-gram TM/DOM outperforms othertechniques for the case of English/Chinesetransliteration/back-transliteration significantly.4.6 English/Japanese TransliterationIn this experiment, we conduct both open andclosed tests for n-gram TM on English/Japanesetransliteration and back-transliteration.
We use thesame training and testing setups as those in (Bilacet al, 2004).Table 9 reports the results from three differenttransliteration mechanisms.
Case 1 is the 3-gramTM under DOM; Case 2 is Case 1 integrated witha dictionary lookup validation process duringdecoding; Case 3 is extracted from (Bilac et al,2004).
Similar to English/Chinese transliteration,one can find that J2E back-transliteration is morechallenging than E2J transliteration in both openand closed cases.
It is also found that word errorrates are reduced greatly at 10-best level.
(Bilac et al, 2004) proposed a hybrid-method ofgrapheme-based and phoneme-based for J2E back-transliteration, where the whole EDICT dictionary,including the test set, is used to train a LM.
A LMunit is a word itself.
In this way, the dictionary isused as a lookup table in the decoding process tohelp identify a valid choice among candidates.
Toestablish comparison, we also integrate thedictionary lookup processing with the decoder,which is referred as Case 2 in Table 9.
It is foundthat Case 2 presents a error reduction of43.8%=(14.6-8.2)/14.6% for word over to thosereported in (Bilac et al, 2004).
Furthermore, the n-gram TM/DOM approach is rather straightforwardin implementation where direct orthographicalmapping could potentially handle Japanesetransliteration of names of different languageorigins, while the issues with non-English termsare reported in (Bilac et al, 2004).The DOM framework shows us a greatimprovement in performance with n-gram TMbeing the most successful implementation.Nevertheless, NCM presents another successfulimplementation of DOM framework.
The n-gramTM and NCM under direct orthographic mapping(DOM) paradigm simplify the process and reducethe chances of conversion errors.
The experimentsalso show that even with much less training data,DOM are still much more superior performancethan the state of art solutions.5 ConclusionsIn this paper, we propose a new framework,direct orthographical mapping (DOM) for machinetransliteration and back-transliteration.
Under theDOM framework, we further propose a jointsource-channel transliteration model, also called n-gram TM.
We also implement the NCM modelunder DOM for reference.
We use EM algorithmas an unsupervised training approach to train the n-gram TM and NCM.
The proposed methods aretested on an English-Chinese name corpus andEnglish-Japanese katakana word pair extractedfrom EDICT dictionary.
The data-driven and one-step mapping strategies greatly reduce thedevelopment efforts of machine transliterationsystems and improve accuracy significantly overearlier reported results.
We also find the back-transliteration is more challenging than thetransliteration.The DOM framework demonstrates severalunique edges over phoneme-based approach:English-JapaneseTransliterationJapanese-EnglishBack-transliterationopen test closedtestopen test closedtest1-best 40.5% 13.5% 62.8% 17.9% Case 1: 3-gramTM/DOM 10-best 13.2% 0.8% 17.9% 2.1%1-best 5.4% 0.7% 8.2% 1.2% Case 2: 3-gramTM/DOM withdictionary lookup10-best 0.7% 0% 1.7% 0.3%1-best N/A N/A 14.6% N/A Case 3: Bilac et al,2004 10-best N/A N/A 2.2% N/ATable 9.
Experiment results of English-Japanese Transliteration1) By skipping the intermediate phonemicinterpretation, the transliteration error rate isreduced significantly;2) Transliteration models under DOM are data-driven.
Assuming sufficient training corpus,the modeling approach applies to differentlanguage pairs;3) DOM presents a paradigm shift for machinetransliteration, that provides a platform forimplementation of many other transliterationmodels;The n-gram TM is a successful implementationof DOM framework due to the following aspects:1) N-gram TM captures contextual informationin both source and target languages jointly;unlike the phoneme-based approach, themodeling of transformation rules and targetlanguage is tightly coupled in n-gram TMmodel.2) As n-gram TM uses transliteration pair asmodeling unit, the same model applies to bi-directional transliteration;3) The bilingual aligning process is integratedinto the decoding process in n-gram TM,which allows us to achieve a jointoptimization of alignment and transliterationautomatically.
Hence manual pre-alignmentis unnecessary.Named entities are sometimes translated incombination of transliteration and meanings.
Asthe proposed framework allows directorthographical mapping, we are extending ourapproach to handle such name translation.
We alsoextending our method to handle the disorder andfertility issues in named entity translation.ReferencesChun-Jen Lee and Jason S. Chang,  2003.
Acquisition ofEnglish-Chinese Transliteration Word Pairs fromParallel-Aligned Texts using a Statistical MachineTranslation Model, Proceedings of HLT-NAACLWorkshop: Building and Using parallel Texts DataDriven Machine Translation and Beyond, 2003,Edmonton, pp.
96-103Dempster, A.P., N.M. Laird and D.B.Rubin, 1977.Maximum likelihood from incomplete data via theEM algorithm, J. Roy.
Stat.
Soc., Ser.
B. Vol.
39Eric Brill, Garry Kacmarcik and Chris Brockrtt, 2001.Automatically Harvesting Katakana-English TermPairs from Search Engine Query Logs.
Proceeding ofNLPRS?01Helen M. Meng, Wai-Kit Lo, Berlin Chen and KarenTang.
2001.
Generate Phonetic Cognates to HandleName Entities in English-Chinese cross-languagespoken document retrieval, Proceedings of ASRU2001Jong-Hoon Oh and Key-Sun Choi, 2002.
An English-Korean Transliteration Model Using Pronunciationand Contextual Rules, Proceedings of COLING 2000Kang B.J.
and Key-Sun Choi, 2000.
AutomaticTransliteration and Back-transliteration by DecisionTree Learning, Proceedings of the 2nd InternationalConference on Language Resources and Evaluation,Athens, GreeceK.
Knight and J. Graehl.
1998.
Machine Transliteration,Computational Linguistics, Vol 24, No.
4Paola Virga, Sanjeev Khudanpur, 2003.
Transliterationof Proper Names in Cross-lingual InformationRetrieval.
Proceedings of ACL 2003 workshopMLNER, 2003Rabiner, Lawrence R. 1989, A tutorial on hiddenMarkov models and selected applications in speechrecognition, Proceedings of the IEEE 77(2)Schwartz, R. and Chow Y. L., 1990.
The N-bestalgorithm: An efficient and Exact procedure forfinding the N most likely sentence hypothesis,Proceedings of ICASSP 1990, Albuquerque, pp.
81-84Slaven Bilac and Hozumi Tanaka, 2004.
ImprovingBack-Transliteration by Combining InformationSources.
Proceedings of IJCNLP-04, Haian, pp.
542-547Stephen Wan and Cornelia Maria Verspoor, 1998.Automatic English-Chinese name transliteration fordevelopment of multilingual resources.
Proceedingsof COLING-ACL?98Stanley F. Chen and Joshua Goodman, 1998.
AnEmpirical Study of Smoothing Techniques forLanguage Modeling, TR-10-98, Computer ScienceGroup, Harvard Universituy.
1998Sung Young Jung, Sung Lim Hong and Eunok Paek,2000.
An English to Korean Transliteration Model ofExtended Markov Window, Proceedings of COLING2000The Onomastica Consortium, 1995.
The Onomasticainterlanguage pronunciation lexicon, Proceedings ofEuroSpeech, Madrid, Spain, pp829-832Wei Gao, Kam-Fai Wong and Wai Lam, 2004.Phoneme-based Transliteration of Foreign Namesfor OOV Problems.
Proceedings of IJCLNP-04,Hainan, pp 374-381Xinhua News Agency, 1992.
Chinese transliteration offoreign personal names, The Commercial PressYaser Al-Onaizan and Kevin Knight, 2002.
Translatingnamed entities using monolingual and bilingualresources.
Proceedings of the 40th ACL,Philadelphia, 2002, pp.
400-408Yuqing Guo and  Haifeng Wang, 2004.
Chinese-to-English Backward Machine Transliteration.Companion Volume to the Proceedings of IJCNLP-04, Hainan, pp 17-20
