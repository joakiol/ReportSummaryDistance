Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 338?343,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsIdentifying High-Impact Sub-Structures for Convolution Kernels inDocument-level Sentiment ClassificationZhaopeng Tu?
Yifan He??
Jennifer Foster?
Josef van Genabith?
Qun Liu?
Shouxun Lin?
?Key Lab.
of Intelligent Info.
Processing ?Computer Science Department ?School of ComputingInstitute of Computing Technology, CAS New York University Dublin City University?
{tuzhaopeng,liuqun,sxlin}@ict.ac.cn,?yhe@cs.nyu.edu, ?
{jfoster,josef}@computing.dcu.ieAbstractConvolution kernels support the modeling ofcomplex syntactic information in machine-learning tasks.
However, such models arehighly sensitive to the type and size of syntac-tic structure used.
It is therefore an importan-t challenge to automatically identify high im-pact sub-structures relevant to a given task.
Inthis paper we present a systematic study inves-tigating (combinations of) sequence and con-volution kernels using different types of sub-structures in document-level sentiment classi-fication.
We show that minimal sub-structuresextracted from constituency and dependencytrees guided by a polarity lexicon show 1.45point absolute improvement in accuracy over abag-of-words classifier on a widely used sen-timent corpus.1 IntroductionAn important subtask in sentiment analysis is sen-timent classification.
Sentiment classification in-volves the identification of positive and negativeopinions from a text segment at various levels ofgranularity including document-level, paragraph-level, sentence-level and phrase-level.
This paperfocuses on document-level sentiment classification.There has been a substantial amount of workon document-level sentiment classification.
In ear-ly pioneering work, Pang and Lee (2004) use aflat feature vector (e.g., a bag-of-words) to rep-resent the documents.
A bag-of-words approach,however, cannot capture important information ob-tained from structural linguistic analysis of the doc-uments.
More recently, there have been several ap-proaches which employ features based on deep lin-guistic analysis with encouraging results includingJoshi and Penstein-Rose (2009) and Liu and Senef-f (2009).
However, as they select features manually,these methods would require additional labor whenported to other languages and domains.In this paper, we study and evaluate diverse lin-guistic structures encoded as convolution kernels forthe document-level sentiment classification prob-lem, in order to utilize syntactic structures withoutdefining explicit linguistic rules.
While the applica-tion of kernel methods could seem intuitive for manytasks, it is non-trivial to apply convolution kernelsto document-level sentiment classification: previouswork has already shown that categorically using theentire syntactic structure of a single sentence wouldproduce too many features for a convolution ker-nel (Zhang et al, 2006; Moschitti et al, 2008).
Weexpect the situation to be worse for our task as wework with documents that tend to comprise dozensof sentences.It is therefore necessary to choose appropriatesubstructures of a sentence as opposed to using thewhole structure in order to effectively use convolu-tion kernels in our task.
It has been observed thatnot every part of a document is equally informa-tive for identifying the polarity of the whole doc-ument (Yu and Hatzivassiloglou, 2003; Pang andLee, 2004; Koppel and Schler, 2005; Ferguson etal., 2009): a film review often uses lengthy objectiveparagraphs to simply describe the plot.
Such objec-tive portions do not contain the author?s opinion andare irrelevant with respect to the sentiment classifi-338cation task.
Indeed, separating objective sentencesfrom subjective sentences in a document producesencouraging results (Yu and Hatzivassiloglou, 2003;Pang and Lee, 2004; Koppel and Schler, 2005; Fer-guson et al, 2009).
Our research is inspired by theseobservations.
Unlike in the previous work, however,we focus on syntactic substructures (rather than en-tire paragraphs or sentences) that contain subjectivewords.More specifically, we use the terms in the lexi-con constructed from (Wilson et al, 2005) as theindicators to identify the substructures for the con-volution kernels, and extract different sub-structuresaccording to these indicators for various types ofparse trees (Section 3).
An empirical evaluation ona widely used sentiment corpus shows an improve-ment of 1.45 point in accuracy over the baselineresulting from a combination of bag-of-words andhigh-impact parse features (Section 4).2 Related WorkOur research builds on previous work in the fieldof sentiment classification and convolution kernel-s. For sentiment classification, the design of lexi-cal and syntactic features is an important first step.Several approaches propose feature-based learningalgorithms for this problem.
Pang and Lee (2004)and Dave et al (2003) represent a document as abag-of-words; Matsumoto et al, (2005) extract fre-quently occurring connected subtrees from depen-dency parsing; Joshi and Penstein-Rose (2009) usea transformation of dependency relation triples; Liuand Seneff (2009) extract adverb-adjective-noun re-lations from dependency parser output.Previous research has convincingly demonstrat-ed a kernel?s ability to generate large feature set-s, which is useful to quickly model new and notwell understood linguistic phenomena in machinelearning, and has led to improvements in variousNLP tasks, including relation extraction (Bunescuand Mooney, 2005a; Bunescu and Mooney, 2005b;Zhang et al, 2006; Nguyen et al, 2009), questionanswering (Moschitti and Quarteroni, 2008), seman-tic role labeling (Moschitti et al, 2008).Convolution kernels have been used before in sen-timent analysis: Wiegand and Klakow (2010) useconvolution kernels for opinion holder extraction,Johansson and Moschitti (2010) for opinion expres-sion detection and Agarwal et al (2011) for sen-timent analysis of Twitter data.
Wiegand and K-lakow (2010) use e.g.
noun phrases as possible can-didate opinion holders, in our work we extract anyminimal syntactic context containing a subjectiveword.
Johansson and Moschitti (2010) and Agarwalet al (2011) process sentences and tweets respec-tively.
However, as these are considerably shorterthan documents, their feature space is less complex,and pruning is not as pertinent.3 Kernels for Sentiment Classification3.1 Linguistic RepresentationsWe explore both sequence and convolution kernelsto exploit information on surface and syntactic lev-els.
For sequence kernels, we make use of lexicalwords with some syntactic information in the formof part-of-speech (POS) tags.
More specifically, wedefine three types of sequences:?
SW, a sequence of lexical words, e.g.
: A tragicwaste of talent and incredible visual effects.?
SP, a sequence of POS tags, e.g.
: DT JJ NN INNN CC JJ JJ NNS.?
SWP, a sequence of words and POS tags,e.g.
: A/DT tragic/JJ waste/NN of/IN talent/NNand/CC incredible/JJ visual/JJ effects/NNS.In addition, we experiment with constituency treekernels (CON), and dependency tree kernels (D),which capture hierarchical constituency structureand labeled dependency relations between words,respectively.
For dependency kernels, we test withword (DW), POS (DP), and combined word-and-POS settings (DWP), and similarly for simple se-quence kernels (SW, SP and SWP).
We also use avector kernel (VK) in a bag-of-words baseline.
Fig-ure 1 shows the constituent and dependency struc-ture for the above sentence.3.2 SettingsAs kernel-based algorithms inherently explore thewhole feature space to weight the features, it is im-portant to choose appropriate substructures to re-move unnecessary features as much as possible.339NPPPNPDT JJ NNA tragic wasteNPINofNP NPNNtalentCCandJJ JJ NNSincredible visual effect(a)wastedet amod prep ofA tragic talentconj andeffectsamod amodincredible visual(b)wastedet amod prep ofDT JJ NNconj andNNSamod amodJJ JJ(c)wastedet amod prep ofDTAJJtragicNNtalentconj andNNSeffectsamod amodJJincrediblevisualvisual(d)Figure 1: Illustration of the different tree structures employed for convolution kernels.
(a) Constituent parse tree(CON); (b) Dependency tree-based words integrated with grammatical relations (DW); (c) Dependency tree in (b)with words substituted by POS tags (DP); (d) Dependency tree in (b) with POS tags inserted before words (DWP).NPDT JJ NNA tragic waste(a)wasteamodJJtragic(b)Figure 2: Illustration of the different settings on con-stituency (CON) and dependency (DWP) parse trees withtragic as the indicator word.Unfortunately, in our task there exist several cuesindicating the polarity of the document, which aredistributed in different sentences.
To solve this prob-lem, we define the indicators in this task as subjec-tive words in a polarity lexicon (Wilson et al, 2005).For each polarity indicator, we define the ?scope?
(the minimal syntactic structure containing at leastone subjective word) of each indicator for differentrepresentations as follows:For a constituent tree, a node and its childrencorrespond to a grammatical production.
There-fore, considering the terminal node tragic in the con-stituent structure tree in Figure 1(a), we extract thesubtree rooted at the grandparent of the terminal, seeFigure 2(a).
We also use the corresponding sequenceScopes Trees SizeDocument 32 24Subjective Sentences 22 27Constituent Substructures 30 10Dependency Substructures 40 3Table 1: The detail of the corpus.
Here Trees denotes theaverage number of trees, and Size denotes the averagednumber of words in each tree.of words in the subtree for the sequential kernel.For a dependency tree, we only consider the sub-tree containing the lexical items that are directlyconnected to the subjective word.
For instance, giv-en the node tragic in Figure 1(d), we will extract itsdirect parent waste integrated with dependency rela-tions and (possibly) POS, as in Figure 2(b).We further add two background scopes, one be-ing subjective sentences (the sentences that containsubjective words), and the entire document.4 Experiments4.1 SetupWe carried out experiments on the movie reviewdataset (Pang and Lee, 2004), which consists of3401000 positive reviews and 1000 negative reviews.To obtain constituency trees, we parsed the docu-ment using the Stanford Parser (Klein and Man-ning, 2003).
To obtain dependency trees, we passedthe Stanford constituency trees through the Stanfordconstituency-to-dependency converter (de Marneffeand Manning, 2008).We exploited Subset Tree (SST) (Collins andDuffy, 2001) and Partial Tree (PT) kernels (Mos-chitti, 2006) for constituent and dependency parsetrees1, respectively.
A sequential kernel is appliedfor lexical sequences.
Kernels were combined usingplain (unweighted) summation.
Corpus statistics areprovided in Table 1.We use a manually constructed polarity lexicon(Wilson et al, 2005), in which each entry is annotat-ed with its degree of subjectivity (strong, weak), aswell as its sentiment polarity (positive, negative andneutral).
We only take into account the subjectiveterms with the degree of strong subjectivity.We consider two baselines:?
VK: bag-of-words features using a vector ker-nel (Pang and Lee, 2004; Ng et al, 2006)?
Rand: a number of randomly selected sub-structures similar to the number of extractedsubstructures defined in Section 3.2All experiments were carried out using the SVM-Light-TK toolkit2 with default parameter settings.All results reported are based on 10-fold cross vali-dation.4.2 Results and DiscussionsTable 2 lists the results of the different kernel typecombinations.
The best performance is obtained bycombining VK and DW kernels, gaining a signifi-cant improvement of 1.45 point in accuracy.
As faras PT kernels are concerned, we find dependencytrees with simple words (DW) outperform both de-pendency trees with POS (DP) and those with bothwords and POS (DWP).
We conjecture that in thiscase, as syntactic information is already captured by1A SubSet Tree is a structure that satisfies the constraint thatgrammatical rules cannot be broken, while a Partial Tree is amore general form of substructures obtained by the applicationof partial production rules of the grammar.2available at http://disi.unitn.it/moschitti/Kernels Doc Sent Rand SubVK 87.05VK + SW 87.25 86.95 87.25 87.40VK + SP 87.35 86.95 87.45 87.35VK + SWP 87.30 87.45 87.30 88.15*VK + CON 87.45 87.65 87.45 88.30**VK + DW 87.35 87.50 87.30 88.50**VK + DP 87.75* 87.20 87.35 87.75VK + DWP 87.70* 87.30 87.65 87.80*Table 2: Results of kernels.
Here Doc denotes the wholedocument of the text, Sent denotes the sentences that con-tains subjective terms in the lexicon, Rand denotes ran-domly selected substructures, and Sub denotes the sub-structures defined in Section 3.2.
We use ?*?
and ?**?
todenote a result is better than baseline VK significantly atp < 0.05 and p < 0.01 (sign test), respectively.the dependency representation, POS tags can intro-duce little new information, and will add unneces-sary complexity.
For example, given the substruc-ture (waste (amod (JJ (tragic)))), the PT kernel willuse both (waste (amod (JJ))) and (waste (amod (JJ(tragic)))).
We can see that the former is adding novalue to the model, as the JJ tag could indicate ei-ther positive words (e.g.
good) or negative words(e.g.
tragic).
In contrast, words are good indicatorsfor sentiment polarity.The results in Table 2 confirm two of our hy-potheses.
Firstly, it clearly demonstrates the val-ue of incorporating syntactic information into thedocument-level sentiment classifier, as the tree k-ernels (CON and D*) generally outperforms vectorand sequence kernels (VK and S*).
More impor-tantly, it also shows the necessity of extracting ap-propriate substructures when using convolution ker-nels in our task: when using the dependency kernel(VK+DW), the result on lexicon guided substruc-tures (Sub) outperforms the results on document,sentence, or randomly selected substructures, withstatistical significance (p<0.05).5 Conclusion and Future WorkWe studied the impact of syntactic information ondocument-level sentiment classification using con-volution kernels, and reduced the complexity of thekernels by extracting minimal high-impact substruc-tures, guided by a polarity lexicon.
Experiments341show that our method outperformed a bag-of-wordsbaseline with a statistically significant gain of 1.45absolute point in accuracy.Our research focuses on identifying and usinghigh-impact substructures for convolution kernels indocument-level sentiment classification.
We expectour method to be complementary with sophisticatedmethods used in state-of-the-art sentiment classifica-tion systems, which is to be explored in future work.AcknowledgementThe authors were supported by 863 State KeyProject No.
2006AA010108, the EuroMatrixPlus F-P7 EU project (grant No 231720) and Science Foun-dation Ireland (Grant No.
07/CE/I1142).
Part of theresearch was done while Zhaopeng Tu was visiting,and Yifan He was at the Centre for Next GenerationLocalisation (www.cngl.ie), School of Computing,Dublin City University.
We thank the anonymousreviewers for their insightful comments.
We are al-so grateful to Junhui Li for his helpful feedback.ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Rambow,and Rebecca Passonneau.
2011.
Sentiment analysisof twitter data.
In Proceedings of the Workshop onLanguages in Social Media, pages 30?38.
Associationfor Computational Linguistics.Razvan Bunescu and Raymond Mooney.
2005a.
AShortest Path Dependency Kernel for Relation Extrac-tion.
In Proceedings of Human Language Technolo-gy Conference and Conference on Empirical Methodsin Natural Language Processing, pages 724?731, Van-couver, British Columbia, Canada, oct. Association forComputational Linguistics.Razvan Bunescu and Raymond Mooney.
2005b.
Sub-sequence Kernels for Relation Extraction.
In Y Weis-s, B Sch o lkopf, and J Platt, editors, Proceedings ofthe 19th Conference on Neural Information ProcessingSystems, pages 171?178, Cambridge, MA.
MIT Press.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proceedings of NeuralInformation Processing Systems, pages 625?632.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies repre-sentation.
In Proceedings of the COLING Workshopon Cross-Framework and Cross-Domain Parser Eval-uation, Manchester, August.Paul Ferguson, Neil O?Hare, Michael Davy, AdamBermingham, Paraic Sheridan, Cathal Gurrin, andAlan F. Smeaton.
2009.
Exploring the use ofparagraph-level annotations for sentiment analysis offinancial blogs.
In Proceedings of the Workshop onOpinion Mining and Sentiment Analysis.Richard Johansson and Alessandro Moschitti.
2010.Syntactic and semantic structure for opinion expres-sion detection.
In Proceedings of the Fourteenth Con-ference on Computational Natural Language Learn-ing, pages 67?76, Uppsala, Sweden, July.Mahesh Joshi and Carolyn Penstein-Rose.
2009.
Gen-eralizing Dependency Features for Opinion Mining.In Proceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, pages 313?316, Suntec, Singapore, jul.Suntec, Singapore.Dan Klein and Christopher D Manning.
2003.
Accu-rate Unlexicalized Parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430, Sapporo, Japan, jul.
As-sociation for Computational Linguistics.Moshe Koppel and Jonathan Schler.
2005.
Using neutralexamples for learning polarity.
In Proceedings of In-ternational Joint Conferences on Artificial Intelligence(IJCAI) 2005, pages 1616?1616.Steve Lawrence Kushal Dave and David Pennock.
2003.Mining the peanut gallery: Opinion extraction and se-mantic classification of product reviews.
In Proceed-ings of the 12th International Conference on WorldWide Web, pages 519?528, ACM.
ACM.Jingjing Liu and Stephanie Seneff.
2009. Review Sen-timent Scoring via a Parse-and-Paraphrase Paradigm.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages 161?169, Singapore, aug. Singapore.Shotaro Matsumoto, Hiroya Takamura, and ManabuOkumura.
2005.
Sentiment classification using wordsub-sequences and dependency sub-trees.
Proceed-ings of PAKDD?05, the 9th Pacific-Asia Conference onAdvances in Knowledge Discovery and Data Mining,3518/2005:21?32.Alessandro Moschitti and Silvia Quarteroni.
2008.
K-ernels on Linguistic Structures for Answer Extraction.In Proceedings of ACL-08: HLT, Short Papers, pages113?116, Columbus, Ohio, jun.
Association for Com-putational Linguistics.Alessandro Moschitti, Daniele Pighin, and RobertoBasili.
2008.
Tree kernels for semantic role labeling.Computational Linguistics, 34(2):193?224.Alessandro Moschitti.
2006.
Efficient Convolution Ker-nels for Dependency and Constituent Syntactic Trees.In Proceedings of the 17th European Conference onMachine Learning, pages 318?329, Berlin, Germany,342sep.
Machine Learning: ECML 2006, 17th EuropeanConference on Machine Learning, Proceedings.Vincent Ng, Sajib Dasgupta, and S M Niaz Arifin.
2006.Examining the Role of Linguistic Knowledge Sourcesin the Automatic Identification and Classification ofReviews.
In Proceedings of the COLING/ACL 2006Main Conference Poster Sessions, pages 611?618,Sydney, Australia, jul.
Sydney, Australia.Truc-Vien T Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Convolution kernels onconstituent, dependency and sequential structures forrelation extraction.
Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 1378?1387.Bo Pang and Lillian Lee.
2004.
A Sentimental Educa-tion: Sentiment Analysis Using Subjectivity Summa-rization Based on Minimum Cuts.
In Proceedings ofthe 42nd Annual Meeting of the Association for Com-putational Linguistics, pages 271?278, Barcelona, S-pain, jun.
Barcelona, Spain.Michael Wiegand and Dietrich Klakow.
2010.
Convolu-tion Kernels for Opinion Holder Extraction.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 795?803, Los An-geles, California, jun.
Los Angeles, California.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis.
In Proceedings of HumanLanguage Technology Conference and Conference onEmpirical Methods in Natural Language Processing,pages 347?354, Vancouver, British Columbia, Cana-da, oct. Association for Computational Linguistics.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Toward-s answering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of the 2003 Conference onEmpirical Methods in Natural Language Processing,pages 129?136, Association for Computational Lin-guistics.
Association for Computational Linguistics.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.2006.
A Composite Kernel to Extract Relations be-tween Entities with Both Flat and Structured Features.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 825?832, Sydney, Australia, jul.
Association forComputational Linguistics.343
