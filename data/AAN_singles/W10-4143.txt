Chinese Syntactic Parsing EvaluationQiang ZhouCenter for Speech and Language Tech.Research Institute of Information Tech.Tsinghua Universityzq-lxd@tsinghua.edu.cnJingbo ZhuNatural Language Processing Lab.Northeastern Universityzhujingbo@mail.neu.edu.cnAbstractThe paper introduced the task designingideas, data preparation methods, evalua-tion metrics and results of the secondChinese syntactic parsing evaluation(CIPS-Bakeoff-ParsEval-2010) jointedwith SIGHAN Bakeoff tasks.1 IntroductionSyntactic parsing is an important technique inthe research area of natural language processing.The evaluation-driven methodology is a goodway to spur the its development.
Two main partsof the method are a benchmark database andseveral well-designed evaluation metrics.
Its fea-sibility has been proven in the English language.After the release of the Penn Treebank (PTB)(Marcus et al, 1993) and the PARSEVAL me-trics (Black et al, 1991), some new corpus-based syntactic parsing techniques were ex-plored in the English language.
Based on them,many state-of-art English parser were built, in-cluding the well-known Collins parser (Collins,2003), Charniak parser (Charniak and Johnson,2005) and Berkeley parser (Petrov and Klein,2007).
By automatically transforming the consti-tuent structure trees annotated in PTB to otherlinguistic formalisms, such as dependencygrammar, and combinatory categorical grammar(Hockenmaier and Steedman, 2007), many syn-tactic parser other than the CFG formalism werealso developed.
These include Malt Parser (Ni-vre et al, 2007), MSTParser (McDonald et al,2005), Stanford Parser (Klein and Manning,2003) and C&C Parser (Clark and Curran, 2007).Based on the Penn Chinese Treebank (CTB)(Xue et al, 2002) developed on the similar anno-tation scheme of PTB, these parsing techniqueswere also transferred to the Chinese language.
(Levy and Manning, 2003) explored the feasibil-ity of applying lexicalized PCFG in Chinese.
(Liet al, 2010) proposed a joint syntactic and se-mantic model for parsing Chinese.
But till now,there is not a good Chinese parser whose per-formance can approach the state-of-art Englishparser.
It is still an open challenge for parsingChinese sentences due to some special characte-ristics of the Chinese language.
We need to finda suitable benchmark database and evaluationmetrics for the Chinese language.Last year, we organized the first Chinese syn-tactic parsing evaluation --- CIPS-ParsEval-2009(Zhou and Zhu, 2009).
Five Chinese parsingtasks were designed as follows:z Task 1: Part-of-speech (POS) tagging;z Task 2: Base chunk (BC) parsingz Task 3: Functional chunk (FC) parsingz Task 4: Event description clause (EDC)recognitionz Task 5: Constituent parsing in EDCsThey cover different levels of Chinese syntac-tic parsing, including POS tagging (Task 1),shallow parsing (Task 2 & 3), complex sentencesplitting (Task 4) and constituent tree parsing(Task 5).
The news and academic articles anno-tated in the Tsinghua Chinese Treebank (TCTver1.0) were used to build different gold-standard data for them.
Some detailed informa-tion about CIPS-ParsEval-2009 can be found in(Zhou and Li, 2009).This evaluation found the following difficultpoints for Chinese syntactic parsing.1) There are two difficulties in Chinese POStagging.
One is the nominal verbs.
The POS ac-curacy of them is about 17% lower than theoverall accuracy.
The other is the unknownwords.
The POS accuracy of them is about 40-10% lower than the overall accuracy.2) The chunks with complex internal struc-tures show poor performance in two chunkingtasks.
How to recognize them correctly needsmore lexical semantic knowledge.3) The joint recognition of constituent tag andhead position show poor performance in theconstituent parsing task of EDCs.Therefore, the second Chinese syntactic pars-ing evaluation (CIPS-Bakeoff-ParsEval-2010)jointed with SIGHAN Bakeoff tasks was pro-posed to deal with these problems.
Some newdesigning ideas are as follows:1) We use the segments sentences as the inputof the syntactic parser to test the effects of POStagging for Chinese parsing.2) We design a new metric to evaluate per-formance of event construction recognition in aconstituent parser of EDCs.3) We try to evaluate the performance ofevent relation recognition in Chinese complexsentence.In the following sections, we will introducethe task designing ideas, data preparation me-thods, evaluation metrics and results of the eval-uation.2 Task descriptionFor the syntactic parsing task (Task 2) of theCIPS-Bakeoff-2010, we designed two sub-tasks:Task 2-1: Parsing the syntactic trees in Chi-nese event description clausesTask 2-2: Parsing the syntactic trees in Chi-nese sentences.Each subtask is separated as close and opentrack.
In the close track, only the provided train-ing data can be used to build the parsing model.In the open track, other outside language re-sources can be freely used.We will give two examples to show the de-tailed goals of these two sub-tasks:1) Task 2-1Input:  a Chinese event description clausewith correct word segmentation annotations?
??
???
??
??
?
??
?
??
?
??
?
?
?
?
?
??
?
?
?Ouput: a syntactic parsing tree of the EDCwith appropriate constitutent tag, head positionand POS tag annotations.?
[dj-2 ?
?/s ?/wP  [dj-1 ?
?/rNP  [vp-1 ?
?/d  [vp-0 ?
?/v  [np-0-2 [np-2[vp-1 [pp-1 ?/p  ?
?/v  ] [vp-1 ?/cC?
?/v  ] ] ?/uJDE  ?
?/n  ] ?/wP[np-2 [vp-1 [pp-1 ?/p  [vp-0 ?/v  ?/n  ] ] [vp-1 ?/vM  ?
?/v  ] ] ?/uJDE?
?/n  ] ] ] ] ] ]12) Task 2-2Input:  a Chinese sentence with correct wordsegmentation annotations?
??
?
??
??
??
?
??
?
??
?
??
?
?
?
?
?
??
?
??
?
?
?
??
?
?
?
??
???
??
?
???
?
????
?Output: a syntactic parsing tree of the sen-tence with appropriate constitute tag and POStag annotations.?
[zj [fj [fj [dj?
?/s  ?/wP  [dj ?
?/rNP[vp ?
?/d  [vp ?
?/v  [np [np [vp [pp?/p  ?
?/v  ] [vp ?/cC  ?
?/v  ] ] ?/uJDE  ?
?/n  ] ?/wP  [np [vp [pp ?/p[vp ?/v  ?/n  ] ] [vp ?/vM  ?
?/v  ] ]?/uJDE  ?
?/n  ] ] ] ] ]] ?/wP  [vp ?/d  [vp ?/v  [np ?
?/a  ?/uJDE  ?/n  ] ] ] ]  ?/wP  [dj [np [vp ?
?/v  ?
?/n  ] ?/uJDE  [np ?
?/n  ?/wD  ??
?/n  ] ] ?/wP  ???
?/v  ] ]?/wE  ]We define a Chinese sentence as the Chineseword serials ending with period, question markor exclamation mark in the Chinese text.
Usually,a Chinese sentence can describe a complex sit-uation with several inter-related events.
It con-sists of several clauses separated by commas orsemicolons to describe one or more detailedevent content.
We call these clauses as eventdescription clauses.We use the following example to explain therelationship between a Chinese sentence and1 Each bracketed constituent is annotated with consti-tuent tag and head positions separated by ?-?.Constituent tags used in the sentence are: dj-simplesentence, vp-verb phrase, np-noun phrase, pp-preposition phrase.POS tags used are: s-space noun, wP-comma, rNP-personal pronoun, d-adverb, v-verb, p-preposition,cC-conjunction, uJDE-particle,  n-noun, vM-modality verb;event description clauses.?
[ ????????????????????????????
]?
[ ?????? ]
?
[ ????????????????
]?
(1)?
[ Along the way, we see the trees havebeen cut down for regeneration, and thetrees needed to be cut for building ].
[ Allof them are useful building material ].
[ We also see several freight trucks andtractors for carry away trees going southand north ].The sentence gives us several sequential situa-tions through the vision changing along the au-thor?s journey way: Firstly, we see the trees thathave been cut down.
They are useful buildingmaterial.
Then, we see several trucks and trac-tors to carry away these trees.
They are goingsouth and north busily.
All the above situationsare described through three EDCs annotatedwith bracket pairs in the sentence.Interestingly, in the corresponding Englishtranslation, the same situation is describedthrough three English sentences with completesubject and predicate structures.
They show dif-ference event description characteristics of thesetwo languages.The Chinese author tends to describe a com-plex situation through a sentence.
Many com-plex event relations are implicit in the structuralsequences or semantic connections among theEDCs of the sentence.
So many subjects or ob-jects of an EDC can be easily omitted based onthe adjacent contexts.The English author tends to describe a com-plex situation through several sentences.
Eachsentence can give a complete description of anevent through the subject and predicate structure.The event relations are directly set through theparagraph structures and conjunctions.The distinction between Chinese sentence andEDC can make us focus on different evaluationemphasis in the CIPS-Bakeoff-2010 section.For an EDC, we can focus on the parsing per-formance of event content recognition.
So wedesign a special metric to evaluate the recall ofthe event recognition based on the syntacticparsing results.For a sentence, we can focus on the parsingperformance of event relation recognition.
So weseparate the simple and complex sentence con-stitutes and give different evaluation metrics forthem.Some detailed designations of the evaluationmetrics can be found in section 4.3 Data preparationThe evaluation data were extracted from Tsing-hua Chinese Treebank (TCT) and PKU ChineseTreebank (PKU-CTB).TCT (Zhou, 2004) adopted a new annotationscheme for Chinese Treebank.
Under thisscheme, every Chinese sentence will be anno-tated with a complete parse tree, where eachnon-terminal constituent is assigned with twotags.
One is the syntactic constituent tag, such asnoun phrase(np), verb phrase(vp), simple sen-tence(dj), complex sentence(fj), etc., which de-scribes basic syntactic characteristics of a consti-tuent in the parse tree.
The other is the grammat-ical relation tag, which describes the internalstructural relation of its sub-components, includ-ing the grammatical relations among differentphrases and the event relations among differentclauses.
These two tag sets consist of 16 and 27tags respectively.Now we have two Chinese treebanks anno-tated under above scheme: (1) TCT version 1.0,which is a 1M words Chinese treebank coveringa balanced collection of journalistic, literary,academic, and other documents; (2) TCT-2010,which consists of 100 journalistic annotated ar-ticles.
The following is an annotated sentenceunder TCT scheme:?
[zj-XX [fj-LS [dj-ZW ?
?/rN [vp-PO ?/v[dj-ZW [np-DZ ?/rN ?
?/rN ] [vp-PO ?/v  ?
?/m  ] ] ] ] ?/?
[dj-ZW ?/rN  [vp-LW [vp-PO ?
?/v  [sp-DZ ?
?/n  ?
?/s  ] ] [vp-PO ?
?/v  [np-DZ [mp-DZ ?/m?/qN  ] ?
?/n ] ] ] ] ] ?/? ]
2               (2)PKU-CTB (Zhan et al, 2006) adopted a tradi-tional syntactic annotation scheme.
They anno-tated Chinese sentences with syntactic constitu-2 Some grammatical relation tags used in the sentenceare as follows: LS?complex timing event relation,ZW?subject-predicate relation, DZ?modifier-headrelation,  PO?predicate-object relation.ent and head position tags in a complete parsetree.
The tag set consists of 22 constituent tags.Because every content word is directly annotatedwith suitable constituent tag, there are many un-ary phrases in PKU-CTB annotated sentences.Its current annotation scale is 881,771 Chinesewords, 55264 sentences.
The following is anannotated sentence under PKU-CTB scheme:?
( zj ( !fj ( !fj ( !dj ( np ( vp ( !v ( ?? )
) !np( !n ( ?? )
) ) !vp ( !vp ( !v ( ? )
) np ( !n( ? )
) ) ) wco ( ? )
dj ( np ( ap ( !b ( ?? )
) !np ( !n ( ?? )
) ) !vp ( dp ( !d( ? )
) !vp ( !vp ( !vp ( !v ( ? )
) v ( ? )
)np ( qp ( mp ( !rm ( ? )
) !q ( ? )
) !np ( np( !n ( ??? )
) !np ( !n ( ?? )
) ) ) ) ) ) )wco ( ? )
vp ( c ( ?? )
!vp ( !v ( ? )
np( ap ( !b ( ?? )
) !np ( !n ( ?? )
) ) vp( !vp ( !v ( ?? )
) vp ( !vp ( !v ( ?? )
)vp ( !v ( ?? )
) ) ) ) ) ) wfs ( ? )
) )3      (3)Due to the different annotation schemes andformats used in these two treebanks, we pro-posed the following strategies to build the gold-standard data set for Task 2-1 and Task 2-2:1) Unify POS tag setThe PKU-CTB has 97 POS tags, and TCT has70 POS tags.
After analyzing these POS tags, wefound most of them have same meanings.
So wedesigned a unified POS tag set with 58 inter-sected tags.
All the POS tags used in PKU-CTBand TCT can be automatically mapped to thisunified tag set.2) Transform PKU-CTB annotationsFirstly, we mapped the POS tags into the uni-fied tag set, and transformed the word and POStag format into TCT?s format.
Then, we deletedall unary constituents in PKU-CTB parse treesand transferred the constituent structures andtags into TCT?s constituent tags.
Finally, wemanually proofread the transformed parse treesto modify some constituent structures that areinconsistent with TCT annotation scheme.
About5% constituents are modified.3 The PKU-CTB uses the similar POS and constituenttags with TCT scheme.
The exclamation symbol ?!?
isused to annotate the head of each constituent in theparse tree.3) Extract EDCs and event annotations fromTCTBased on the detailed grammatical relationtags annotated in TCT, we can easily extracteach EDC for a TCT sentence (Zhou and Zhu,2009).
Then, we proposed an algorithm to ex-tract different event constructions in each EDCand build a large scale Chinese event bank.
Itcan be used as a gold-standard data to evaluationthe event recognition performance of an auto-matic syntactic parser in Task 2-1.An event construction is an event chunk serialcontrolled by an event target verb.
It is a basicunit to describe event content.
For example, forthe first EDC extracted from the above sentence(1), we can obtain the follow four event con-structions for the event target verb ???
?, ???
?, ??
?, and ????
.?
[D-sp ?
?/s-@] ?/wP [S-np ?
?/rNP-@  ] [D-dp ?
?/d-@  ] [P-vp-Tgt ?
?/v-@  ] [O-np ?/p  ?
?/v  ?/cC  ?
?/v  ?/uJDE  ?
?/n-@  ?/wP  ?/p  ?/v  ?/n?/vM  ?
?/v  ?/uJDE  ?
?/n-@  ]4?
[D-pp ?/p  ?
?/v-@  ] [P-vp-Tgt ?/vM?
?/v-@ ] ?/uJDE  [H-np ?
?/n-@  ] ??
?
?/p  [P-vp-Tgt ?/v-@  ] [O-np ?/n-@  ] ?/vM  ?
?/v  ?/uJDE  ??/n?
?
[D-pp ?/p ?/v-@ ?/n ] [P-vp-Tgt ?/vM ?
?/v-@ ]?/uJDE  [H-np ?
?/n-@  ]4) Obtain TCT constituent structure treesWe can easily select all syntactic constituenttags annotated in TCT sentences to build thegold-standard parsing trees for Task 2-2.We mainly used the journalistic and academictexts annotated in TCT and PKU-CTB to builddifferent training and test set for task 2-1 and 2-2.Table 1 summarizes current building status ofthese gold-standard data sets.4 Each event chunk is annotated with bracket pairswith functional and constituent tags.
Some functionaltags used in the EDCs are as follows: D?adverbial,S?subject, P?predicate, O?object.
The constituenttags are same with that ones used in above parse tree.The head of each chunk is indicated through ?-@?.DatasetSource Genre Methods2-1,TRTCTver1.0News,AcademyPOS unification,EDC and eventextraction2-1,TSTCT-2010News POS unification,EDC and eventextraction2-2,TRTCTver1.0News,AcademyPOS unification,Parse tree extrac-tion2-2,TSPKU-CTBAcademy POS unification,annotation trans-formationTable 1 Gold-standard data building status(TR=Training data, TS=Test data)We selected all news and academic texts an-notated in TCT ver1.0 to form the training set ofTask 2-1 and 2-2.
1000 EDCs extracted fromTCT-2010 were selected as the test set of Task2-1.
These sentences are extracted from thePeople?s Daily corpus with the same source ofTCT ver1.0.
1000 sentences extracted fromPKU-CTB were selected as the test set of Task2-2.
Most of them are extracted from the tech-nical reports or popular science articles.
Theyhave much more technical terms than the encyc-lopedic articles used in TCT ver1.0.
Table 2shows the basic statistics of all the training andtest sets in Task 2.Data set WordSumSent.SumAverageLength2-1, TR 425619 37219 11.442-1, TS 9182 1000 9.182-2, TR 481061 17529 27.442-2, TS 26381 1000 26.38Table 2 Basic statistics of Task 24 Evaluation metricsFor Task 2-1, we designed three kinds of evalua-tion metrics:1) POS accuracy (POS-A)This metri is used to evaluate the performanceof automatic POS tagging.
Its computation for-mula is as follows:?
POS accuracy = (sum of words with cor-rect POS tags) / (sum of words in gold-standard sentences) * 100%The correctness criteria of POS tagging is asfollows:?
The automatically assigned POS tag issame with the gold-standard one.2) Constituent parsing evaluationWe selected three commonly-used metrics toevaluation the performance of constituent pars-ing: labeled precision, recall, and F1-score.Their computation formulas are as follows:?
Precision = (sum of correctly labeledconstituents ) / (sum of parsed constitu-ents) * 100%?
Recall = (sum of correctly labeled consti-tuents) / (sum of gold-standard constitu-ents) *100%?
F1-score = 2*P*R / (P+R)Two correctness criteria are used for constitu-ent parsing evaluation:?
?B+C?
criteria: the boundaries and syn-tactic tags of the automatically parsedconstituents must be same with the gold-standard ones.?
?B+C+H?
criteria: the boundaries, syntac-tic tags and head positions of the auto-matically parsed constituents must besame with the gold-standard ones.3) Event recognition evaluationWe only considered the recognition recall ofeach event construction annotated in the eventbank, due to the current parsing status of Task 2-1 output.
For each event target verb annotated inthe event bank, we computed their Micro andMacro average recognition recall.
The computa-tion formulas are as follows:?
Micro Recall = (sum of all correctly rec-ognized event constructions) / (sum of allgold standard event constructions) *100%?
Macro Recall = (sum of Micro-R of eachevent target verb ) / (sum of event targetverbs in gold-standard set )The correctness criteria of event recognitionshould consider following two matching condi-tions:Condition 1: Each event chunk in a gold-standard event construction should have a cor-responding constituent in the automatic parsetree.
For the single-word chunk, the automatical-ly assigned POS tag should be same with thegold standard one.
For the multiword chunk, theboundary, syntactic tag and head positions of theautomatically parsed constituent should be samewith the gold-standard ones.
Meanwhile, the cor-responding constituents should have the samelayout sequences with the gold standard eventconstruction.Condition 2: All event-chunk-correspondingconstituents should have a common ancestornode in the parse tree.
One of the left and rightboundaries of the ancestor node should be samewith the left and right boundaries of the corres-ponding event construction.For Task 2-2, we design two kinds of evalua-tion metrics:1) POS accuracy (POS-A)This index is used to evaluate the performanceof automatic POS tagging.
Its formula and cor-rectness criteria are same with the above defini-tions of Task 2-1.2) Constituent parsing evaluationTo evaluate the parsing performance of eventrelation recognition in complex Chinese sen-tences, we firstly divided all parsed constituentsinto following two parts:?
Constituent of complex sentence (C_S),whose tag is ?fj?;?
Constituents in simple sentence (S_S),whose tags are belong to the tag set {dj,vp, ap, np, sp, tp, mp, mbar, dp, pp, bp}.Then we computed the labeled precision, re-call and F1-socre of these two parts and obtainthe arithmetic mean of these two F1-score as thefinal ranking index.
Their computation formulasof each part are as follows:?
Precision = (sum of correctly labeledconstituents in one part) / (sum of parsedconstituents in the part) * 100%?
Recall = (sum of correctly labeled consti-tuents in one part) / (sum of gold-standard constituents in the part) *100%?
F1-score = 2*P*R / (P+R)?
Total F1-Score = (C_S F1 + S_S F1) / 2We use the above ?B+C?
correctness criteriafor constituent evaluation in Task 2-2.ID Participants Task 2-1 Task 2-2TPI Open close TPI open Close01 School of Computer Sci.
and Tech.,Harbin Institute of TechnologyY   Y  102 Knowledge Engineering ResearchCenter, Shenyang Aerospace Univ.Y  3 Y  203 Dalian University of Technology Y  1 Y  104 National Laboratory of Pattern Rec-ognition Institute of Automation,Chinese Academy of ScienceY 2 2 Y 4 205 Beijing University of Posts and Tele-communicationsY  2 Y06 University of Science and Technolo-gy of ChinaY   Y07 Dept.
of Computer Science andTechnology, Shanghai Jiao TongUniversity,Y  3 Y  308 Soochow University Y   Y09 Harbin Institute of Technology Y  1 Y10 German Research Center for Artifi-cial IntelligenceY 1 1 Y 111 China Center for Information Indus-try DevelopmentN   Y 112 City University of Hong Kong Y   Y13 National Central University Y   YTotal  12 3 13 13 6 9Table 3  Result submission data of all participants in Task 2.
(TPI=Take Part In)5 Evaluation resultsThe Task 2 of CIPS-Bakeoff-2010 attracted 13participants.
Almost all of them took part in thetwo subtasks: Task 2-1 and 2-2.
Only one partic-ipant took part in the Task 2-2 subtask alone.Among them, 9 participants submitted parsingresults.
In Task 2-1, we received 16 parsing re-sults, including 13 close track systems and 3open track systems.
In Task 2-2, we received 15parsing results, including 9 close track systemsand 6 open track systems.
Table 3  shows thesubmission information of all participants ofTask 2.5.1 Task 2-1 analysisWe evaluated the parsing performance of EDCon the constituent and event level respectively.The constituent parsing evaluation only consid-ers the parsing performance of one single consti-tuent.
The event recognition evaluation will con-sider the recognition performance of a completeevent construction.
So it can provide more usefulreference information for event extraction appli-cation.Table 5 and Table 6 show the evaluation re-sults of constituent parsing in the close and opentracks respectively.
In the close track, the bestF1-score under ?B+C?
criteria is 85.39%, whilethe best F1 score under ?B+C+H?
criteria is83.66%.
Compared with the evaluation resultsof the task 5 in CIPS-ParEval-2009 under thesimilar training and test conditions (Zhou and Li,2009), the performance of head identification isimproved about 2%.
Table 4 shows the detailedcomparison data.Rank ID ?B+C?
?B+C+H?
POS-A09-1 08 87.22 83.70 Gold09-2 15 86.25 81.75 Gold10-1 02 85.39 83.66 93.9610-2 04 84.36 82.51 91.84Table 4 F1 scores of the Top-2 single-modelclose-track systems in the ParsEval-2009 andParsEval-2010.Table 7 and Table 8 show the evaluation re-sults of event recognition in the close and opentracks respectively.
When we consider the com-plete event constructions contained in a parsetree, the best Macro-Recall is only about 71%.There are still lots of room to improve in the fu-ture.ID Sys-ID Model ?B+C?
?B+C+H?
POS-A RankP R F1 P R F102 SAU01 Single 85.42 85.35 85.39 83.69 83.63 83.66 93.96 102 SAU02 Single 85.02 85.11 85.06 83.21 83.31 83.26 93.96 204 a Single 84.40 84.32 84.36 82.55 82.47 82.51 91.84 304 b Single 83.79 83.74 83.76 81.82 81.78 81.80 91.67 410 DFKI_C Single 82.93 82.85 82.89 80.54 80.46 80.50 81.99 502 SAU03 Single 80.28 79.31 79.79 78.55 77.61 78.08 93.93 607 b Single 78.61 78.76 78.69 76.61 76.75 76.68 92.77 707 c Single 77.78 78.13 77.96 75.78 76.13 75.95 92.77 805 BUPT Single 74.86 76.05 75.45 71.06 72.20 71.63 87.00 905 BUPT Multiple 74.48 75.64 75.05 70.72 71.81 71.26 87.00 1003 DLUT Single 71.42 71.19 71.30 69.22 69.00 69.11 86.69 1109 InsunP Single 70.69 70.48 70.58 67.07 66.87 66.97 77.87 1207 a Single 9.09 12.51 10.53 7.17 9.88 8.31 7.02 13Table 5 Constituent parsing evaluation results of Task 2-1 (Close Track), ranked with ?B+C+H?- F1ID Sys-ID Model ?B+C?
?B+C+H?
POS-A RankP R F1 P R F104 a Single 86.07 86.08 86.08 84.27 84.28 84.27 92.51 104 b Single 83.79 83.74 83.76 81.82 81.78 81.80 91.67 210 DFKI_C Single 82.37 83.05 82.71 79.99 80.65 80.32 81.87 3Table 6 Constituent parsing evaluation results of Task 2-1 (Open Track), ranked with ?B+C+H?- F1ID Sys-ID Model Micro-R Macro-R POS-A Rank02 SAU01 Single 72.47 71.53 93.96 102 SAU02 Single 72.93 70.71 93.96 204 a Single 67.37 65.05 91.84 304 b Single 67.17 64.23 91.67 402 SAU03 Single 63.73 63.54 93.93 507 c Single 63.14 62.48 92.77 607 b Single 62.74 62.47 92.77 710 DFKI_C Single 55.99 53.58 81.99 803 DLUT Single 51.75 53.33 86.69 905 BUPT Single 53.08 48.82 87.00 1005 BUPT Multiple 52.88 48.75 87.00 1109 InsunP Single 43.15 43.14 77.87 1207 a Single 1.13 0.79 7.02 13Table 7  Event recognition evaluation results of Task 2-1 (Close Track), ranked with Macro-RID Sys-ID Model Micro-R Macro-R POS-A Rank04 a Single 70.62 69.33 92.51 104 b Single 67.17 64.23 91.67 210 DFKI_C Single 54.47 52.25 81.87 3Table 8 Event recognition evaluation results of Task 2-1 (Open Track), ranked with Macro-R5.2 Task 2-2 analysisTable 9 and Table 10 show the evaluation resultsof constituent parsing in the close and opentracks of Task 2-2 respectively.
In each track,the F1-score of the complex sentence recogni-tion is about 5-6% lower than that of the consti-tuents in simple sentences.
It indicates the diffi-cultness of event relation recognition in realworld Chinese sentences.
Some new featuresneed to be explored for them.Almost all the parsing performances of thesystems in the open track are better than thatones in the close track.
It indicates some outsidelanguage resources may useful for parsing per-formance improvement.
Compared with thecommonly-used English Treebank PTB withabout 1M words, our current annotated data maybe not enough to train a good Chinese parser.We may need to collect more useful treebankdata in the future evaluation tasks.The F1-scores of constituent parsing in simplesentences of Task 2-2 are still about 5-6% lowerthan that of EDC constituents under ?B+C?
crite-ria in Task 2-1.
It indicates some lower levelerrors may be propagated to up-level constitu-ents during complex sentence parsing.
How torestrict the error propagation chains is an inter-esting issue need to be explored.5.3 POS tagging analysisThe best POS accuracy in Task 2-1 is 93.96%,approaching to the state-of-art performance ofthe Task 1 in CIPS-ParsEval-2009, under similartraining and test conditions.
But the POS accura-cy in Task 2-2 is about 3-4% lower than it.
Apossible reason is that there are lots of unknownwords in the test data of Task 2-2.
Most of themare technical terms outside the training data lex-icon.
How to deal with the unknown words isstill an open challenge for POS tagging.6 ConclusionsThe paper introduced the task designing ideas,data preparation methods, evaluation metrics andresults of the second Chinese syntactic parsingevaluation jointed with SIGHAN Bakeoff tasks.Some new contributions of the evaluation areas follows:1) Set a new metric to evaluate the eventconstruction recognition performance inthe constituent parsing tree;ID Sys-ID Model Constituents in S_S C_S constituent Total POS-A RankP R F1 P R F1 F104 b Single 77.79 77.47 77.63 69.55 76.50 72.86 75.24 88.79 104 a Single 77.91 77.54 77.73 68.47 76.90 72.44 75.08 88.95 2O2 SAU01 Single 78.64 78.73 78.69 70.22 71.62 70.91 74.80 91.05 3O2 SAU02 Single 78.46 78.34 78.40 69.48 72.42 70.92 74.66 91.03 403 DLUT Single 61.67 59.75 60.69 65.27 67.31 66.27 63.48 79.67 501 CHP Single 70.20 69.64 69.92 53.95 59.47 56.58 63.25 89.62 607 b Single 55.33 59.57 57.37 6.25 0.64 1.16 29.26 89.01 707 c Single 52.57 57.69 55.01 7.47 1.68 2.74 28.88 89.01 807 a Single 0.71 1.00 0.83 0.00 0.00 0.00 0.42 1.39 9Table 9  Constituent parsing evaluation results of Task 2-2 (Close Track), ranked with Tot-F1(S_S=simple sentence, C_S=complex sentence)ID Sys-ID Model Constituents in S_S C_S constituent Total POS-A RankP R F1 P R F1 F104 d Single 80.04 79.68 79.86 70.11 76.50 73.17 76.51 89.59 104 a Single 80.27 79.99 80.13 70.36 75.54 72.86 76.50 89.69 204 c Single 80.25 79.95 80.10 70.40 75.30 72.77 76.44 89.78 304 b Single 80.02 79.68 79.85 69.82 75.62 72.60 76.22 89.75 410 DFKI_C Single 79.37 79.27 79.32 71.06 73.22 72.13 75.72 81.23 511* CCID Single / / / / / / / / /Table 10 Constituent parsing evaluation results of Task 2-2 (Open Track), ranked with Tot-F1(S_S=simple sentence, C_S=complex sentence) There are some data format errors in the submittedresults of CCID system (ID=11)2) Set a separated metric to evaluate theevent relation recognition performance incomplex Chinese sentence.Through this evaluation, we found:1) The event construction recognition in aChinese EDC is still a challenge.
Somenew techniques and machine learningmodels need to be explored for this task.2) Compared with about 90% F1-score ofthe state-of-art English parser, the 75%F1-score of current Chinese parser is stillon its primitive stage.
There is a long wayto go in the future.3) The event relation recognition in realworld complex Chinese sentences is a dif-ficult problem.
Some new features andmethods need to be explored for it.They lay good foundations for the new taskdesignation in the future evaluation round.AcknowledgementsThanks Li Yemei for her hard work to organizethe evaluation.
Thanks Li Yanjiao and Li Yumeifor their hard work to prepare the test data forthe evaluation.
Thanks Zhu Muhua for makingthe evaluation tools and processing all the sub-mitted data.
Thanks all participants of the evalu-ation.The work was also supported by the researchprojects of National Science Foundation of Chi-na (Grant No.
60573185, 60873173) and National863 High-Tech research projects (Grant No.2007AA01Z173).ReferencesE.
Black, S. Abney, et al 1991.
A Procedure forQuantitatively Comparing the Syntactic Coverageof English Grammars.
In Speech and natural lan-guage: proceedings of a workshop, held at PacificGrove, California, page 306.E.
Charniak and M. Johnson.
2005.
Coarse-to-finenbest parsing and MaxEnt discriminative rerank-ing.
In Proc.
of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics, page 180.S.
Clark and J.R. Curran.
2007.
Wide-coverage effi-cient statistical parsing with CCG and log-linearmodels.
Computational Linguistics, 33(4):493?552.D.
Klein and C. Manning.
2003.
Accurate Unlexica-lized Parsing.
In Proc.
of ACL-03.M.
Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational linguis-tics, 29(4):589?637.J.
Hockenmaier and M. Steedman.
2007.
CCGbank: acorpus of CCG derivations and dependency struc-tures extracted from the Penn Treebank.
Computa-tional Linguistics, 33(3):355?396.R.
Levy and C. Manning.
(2003).
Is it harder to parseChinese, or the Chinese Treebank?
In Proc.
ofACL-03.J.
Li, G. Zhou, and H.T.
Ng.
2010.
Joint Syntacticand Semantic Parsing of Chinese.
In Proc.
of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1108?1117.R.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Proc.
of HLT/EMNLP,pages 523?530.Mitchell P.Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a Large Anno-tated Corpus of English: The Penn Treebank,Computational Linguistics, 19(2): 313-330J.
Nivre, J.
Hall, J. Nilsson, el.al.
2007.
Malt-Parser:A language-independent system for data drivendependency parsing.
Natural Language Engineer-ing, 13(02):95?135.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In Proc.
of NAACL HLT2007, pages 404?411.N.
Xue, F. Chiou, and M. Palmer.
2002.
Building alarge-scale annotated Chinese corpus.
In Proc.
ofCOLING-2002.Zhan Weidong, Chang Baobao, Dui Huiming, ZhangHuarui.
2006.
Recent Developments in ChineseCorpus Research.
Presented in The 13th NIJL In-ternational Symposium, Language Corpora: TheirCompliation and Application.
Tokyo, Japan.Zhou Qiang, 2004.
Chinese Treebank AnnotationScheme.
Journal of Chinese Information, 18(4):1-8.Zhou Qiang, Li Yuemei.
2009.
Evaluation report ofCIPS-ParsEval-2009.
In Proc.
of First Workshopon Chinese Syntactic Parsing Evaluation, BeijingChina.Zhou Qiang, Zhu Jingbo.
2009.
Evaluation tasks anddata preparation of CIPS-ParsEval-2009,http://www.ncmmsc.org/ CIPS-ParsEval-20
