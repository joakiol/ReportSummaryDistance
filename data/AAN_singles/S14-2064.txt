Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 375?379,Dublin, Ireland, August 23-24, 2014.JU-Evora: A Graph Based Cross-Level Semantic Similarity Analysisusing Discourse InformationSwarnendu GhoshDept.
of ComputerScience and Engi-neeringJadavpur University,Kolkata, Indiaswarbir@gmail.comNibaran DasDept.
of ComputerScience and Engi-neeringJadavpur University,Kolkata, Indiani-baran@ieee.orgTeresa Gon?alvesDept.
of Inform?ticaUniversity of ?vora,?vora, Portugaltcg@evora.ptPaulo QuaresmaDept.
of Inform?ticaUniversity of ?vora,?vora, Portugalpq@evora.ptAbstractText Analytics using semantic information isthe latest trend of research due to its potentialto represent better the texts content comparedwith the bag-of-words approaches.
On thecontrary, representation of semantics throughgraphs has several advantages over the tradi-tional representation of feature vector.
There-fore, error tolerant graph matching techniquescan be used for text comparison.
Neverthe-less, not many methodologies exist in the lit-erature which expresses semantic representa-tions through graphs.
The present system isdesigned to deal with cross level semanticsimilarity analysis as proposed in theSemEval-2014 : Semantic Evaluation, Inter-national Workshop on Semantic Evaluation,Dublin, Ireland.1 IntroductionText Analytics has been the focus of much re-search work in the last years.
State of the art ap-proaches typically represent documents as vec-tors (bag-of-words) and use a machine learningalgorithm, such as k-NN or SVM, to create amodel and to compare and classify new docu-ments.
However, and in spite of being able toobtain good results, these approaches fail to rep-resent the semantic content of the documents,losing much information and limiting the tasksthat can be implemented over the document rep-resentation structures.
To overcome these short-comings some research has been done aiming touse and evaluate more complex knowledge rep-resentation structures.
In this paper, a new ap-proach which integrates a deep linguistic analysisof the documents with graph-based classificationalgorithms and metrics has been proposed.2 Overview of the TaskThis task provides an evaluation for semanticsimilarity across different sizes of text, which werefer to as lexical levels.
Specifically, this taskencompasses four semantic similarity compari-sons:?
paragraph to sentence(P2S),?
sentence to phrase(S2Ph),?
phrase to word(Ph2W), and?
word to sense(W2S).Task participants were provided with pairs ofeach comparison type and asked to rate the pairaccording to the semantic similarity of the small-er item to the larger item.
As an example, given asentence and a paragraph, a system would assesshow similar is the meaning of the sentence to themeaning of the paragraph.
Ideally, a high-similarity sentence would reflect overall meaningof the paragraph.
The participants were expectedto assign a score between [0,4] to each pairs ofsentences, where 0 shows no similarity in con-cept while 4 shows complete similarity in con-cept.3 Theoretical Concepts3.1 Discourse Representation StructuresExtraction and representation of the informationconveyed by texts can be performed throughseveral approaches, starting from statistical anal-ysis to deep linguistic techniques.
In this paperThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers andproceedings footer are added by the organisers.
Licencedetails: http://creativecommons.org/licenses/by/4.0/375we will use a deep linguistic processing se-quence: lexical, syntactic, and semantic analysis.One of the most prominent research work onsemantic analysis is the Discourse Representa-tion Theory (DRT)(Kamp & Reyle, 1993).
InDRT, we aim to associate sentences with expres-sions in a logical language, which indicate theirmeaning.
In DRT, each sentence is viewed as anupdate of an existing context, having as result anew context.DRT provides a very powerful platform forthe representation of semantic structures of doc-uments including complex relations like implica-tions, propositions and negations.
It is also ableto separately analyse almost all kinds of eventsand find out their agent and patient.The main component of DRT is the DiscourseRepresentation Structure (DRS These expres-sions have two main parts: a) a set of referents,which refer to entities present in the context andb) a set of conditions, which are the relations thatexist between the entities.
An example of a DRSrepresentation for the sentence "He throws aball."
is shown below.
[x1, x2, x3:male(x1),ball(x2),throw(x3),event(x3),agent(x3,x1),patient(x3, x2)]3.2 GML StructureGraph Modelling Language (GML)(Himsolt &Passau, 1996) is a simple and efficient way torepresent weighted directed graphs.
A GML fileis basically a 7-bit ASCII file, and, as such, canbe easily read, parsed, and written.
Several opensource applications 1  are available that enableviewing and editing GML files.Graphs are represented by the keys viz.
graph,node and edge.
The basic structure is modelledwith the node's id and the edge's source and tar-get at-tributes.
The id attributes assign numbersto nodes, which are then referenced by sourceand target.
Weights can be represented by thelabel attribute.1http://en.wikipedia.org/wiki/Graph_Modelling_Language3.3 Similarity Metrics for GraphsIt has already been mentioned that the objectiveof the present work is to generate similarityscores among documents of different lexical lev-els using an approach which integrates a deeplinguistic analysis of the documents with graph-based classification algorithms and metrics.Here, five different distance metrics taken from(Bunke, 2010) are utilized for this purpose.
Theyare popularly used in object recognition task, butfor text similarity measure they have not yet beenused.For two graphs    and   , if  (     ) is thedissimilarity/similarity measure, then this meas-ure would be a distance if   has the followingproperties:1.
(     )   , iff2.
(     )   (     )3.
(     )   (     )   (     )The measures used in the present work followthe above rules and the corresponding equationsare(     )|   (     )|(|  | |  |)( )(     )|   (     )||  |  |  |  |   (     )|?
(2)(     )   |  |  |  |    |   (     )|?
(3)(     )  |   (     )|  |   (     )|?
(4)(     )|   (     )||   (     )|( )In the equations    (     )  and(     ) denote maximal common subgraphand minimum common super graphs of twographs    and   .
Theoretically    (     )  isthe largest graph in terms of edges that is iso-morphic to a subgraph of     and   .
The(     ) has been formally defined in a workof Horst Bunke (Bunke, Foggia, Guidobaldi,Sansone, & Vento, 2002).
As stated earlier, it isa NP complete problem and actually, the methodof finding the    ()  is a brute force methodwhich finds all the subgraphs of both the graphsand select the maximum graph which is commonto both.
To make the program computationallyfaster, the program is modified  to an approxi-376mate version of   (     ) on the fact that  thevertices which exhibit greater similarity in theirlocal structures among the two graphs have agreater probability of inclusion in the   ()  Thetwo pass approach used in the present work toform the approximate   (     ) is as follows:?
All the node pairs (one from each graph)are ranked according the number ofmatching self-loops.?
The mcs is built by including each nodepair (starting with the one with the highestnumber of matching self-loops) and con-sidering it as a common node; and then in-clude the rest of the edges (i.e.
non-self-loop edges) which occur in the same fash-ion in both the graphs.In this way it ensures that the approximationversion exhibits most of the properties of a mcs,while keeping the complexity in a polynomialtime.The minimum common supergraph(   )(Angelova & Weikum, 2006) is formedusing the union of two graphs, i.e.
(     )       .The distance metrics of Equations 1-3 wereused directly without any modifications; the onesof Equations 3-4 were divided by (|  |  |  |)and |   (     )     (     )|   respectivelyto make them normalized, keeping the value ofdistance metrics within the range [   ]It is worthy to note that label matching that isperformed during the above mentioned step maynot necessarily be exact matching.
Rather in thiscase we have used the WordNet to find an ap-proximate conceptual similarity between twolabels.
For our experiment we have used the Wuand  Palmer?s conceptual similarity (Wu &Palmer, 1994).
(     ) , where   and    are a pairof concepts corresponding to   two words and(     ) means the lowest super ordinate then,(     )( )(    )     (    )         ( )3.4 Tools UsedIn order to process texts C&C/Boxer (Bos, 2008;Curran, Clark, & Bos, 2007) a well-known opensource  tool available as a plugin to Natural Lan-guage Toolkit (NLTK) is used.
The tool consistsof a combinatory categorical grammar (CCG)(Curran et al., 2007) parser and outputs the se-mantic representations using discourse represen-tation structures (DRS) of Discourse Representa-tion Theory (DRT) (Kamp & Reyle, 1993).4  System DescriptionThe method described in the present work, ismainly divided into three major components.
Thefirst is the creation of the DRS of the semanticinterpretation of the text.
The second is the con-struction of graphs in GML from the obtainedDRS using some predefined rules.
The third oneis the classification phase where the differentgraph distances are assessed using a k-NN classi-fier (Zhang, Li, Sun, & Nadee, 2013).The algorithm semantic evaluation of text con-tent may be described as follows.?
NLTK Module : For each pair of text, toFigure 1: Graphical overview of  mcs and MCS:  (a), (b) graph representation of sentencesmeaning ?Mary drinks water?
and ?David drinks water ?
respectively, (c)  maximum com-mon subgraph,  (d) minimum common supergraph.
377compare their similarity measure we needto find their DRS using the C&C/Boxertoolkit.
The toolkit first uses the C&C Par-ser to find the combinatorial categoricalgrammar(CCG) of the text.
Next the BoxerModule uses the CCG to find the discourserepresentation structures.?
Graph building module : In general Box-er represents a sentence through some dis-course referents and conditions based onthe semantic interpretation of the sentence.In the graph, the referent is represented byvertex after resolving the equity amongdifferent referents of the DRS; and a con-dition is represented by an edge value be-tween two referents.
The condition of asingle referent is represented as a self-loopof the referent (source and destination ref-erents are same).
Special relationshipssuch as proposition, implication etc.
aretreated as edge values between two refer-ents; Agent and patient are also treated asconditions of discourse, hence representedby the edge values of two referents.?
Calculating Similarity Index : It has al-ready been mentioned that the differentdistance metrics (see Equations 1-5) calcu-lated based on the mcs() and MCS().
Thevalues of  mcs() and MCS() are represent-ed by the number of similar edges.
Thus,ten different distances are calculated basedon Equations 1-5.?
Learning : We obtained 5 similarityscores for each pair of texts.
Our task re-quires us to assign a score between 0-4 foreach pair of text.
Hence using the goldstandard a K-NN Classifier have beentrained to find the output score for a testsample.
The value of K has been empiri-cally adjusted using the cross validationtechnique to find the optimal value.Our method works smoothly for the first twolexical levels.
But for the last two levels i.e.phrase to word and word to sense it is not possi-ble to find out DRS for a single word.
Hence wehave used the WordNet(Fellbaum, 1998) to ex-tract the definition of the word in question andcalculate its DRS and proceed with the method.When a word has multiple definitions, all thedefinitions are fused to a single sentence afterconjugating them with the conjunction ?or?.5 Results and DiscussionsThe JU-Evora system performed fairly in theSemEval Competition 2014.
All the correlationscores are not as good as the Baseline(LCS)scores, however it provides a better  Pearson cor-relation score in case of Paragraph to Sentence.The other scores, though not higher, are in thevicinity of the baseline.
All the scores are shownbelow in Table 1.Table 1: Performance of JU-Evora system withrespect to Baseline.6 ConclusionIn this paper a new approach has been proposedto the text comparison task which integrates adeep linguistic analysis of the documents with agraph-based comparison algorithm.
In the lin-guistic analysis, discourse representation struc-tures (DRS) are used to represent text semanticcontent and, afterwards, these structures aretransformed into graphs.
We have evaluated ex-istent graph distance metrics and proposed somemodifications, more adequate to calculate graphdistances between graph-drs structures.
Finally,we integrated graph-drs structures and the pro-posed graph distance metrics into a k-NN classi-fier for calculating the similarity between twodocuments.
Future works in this area would  beconcentrated on the use of external knowledgesources to make the system more robust.ReferencesAngelova, Ralitsa, & Weikum,Gerhard.
(2006).Graph-based Text Classification: Learn from YourNeighbors.
In Proceedings of the 29th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval (pp.
485?492).
New York, NY, USA:ACM.Bos, Johan (2008).
Wide-Coverage SemanticAnalysis with Boxer.
In J. Bos & R. DelmontePEARSON?S CORRELATIONP2S S2Ph Ph2W W2SJU-Evora 0.536 0.442 0.090 0.091Baseline (LCS) 0.527 0.562 0.165 0.109SPEARMAN CORRELATIONJU-Evora 0.533 0.440 0.096 0.075Baseline (LCS) 0.613 0.626 0.162 0.130378(Eds.
), Semantics in Text Processing.
STEP 2008Conference Proceedings (pp.
277?286).
CollegePublications.Bunke, Horst (2010).
Graph Classification andClustering Based on Vector Space Embedding(Vol.
Volume 77, pp.
15?34).
WORLDSCIENTIFIC.doi:doi:10.1142/9789814304726_0002Bunke, Horst, Foggia, Pasquale, Guidobaldi, Corrado,Sansone, Carlo, & Vento, Mario (2002).
AComparison of Algorithms for MaximumCommon Subgraph on Randomly ConnectedGraphs.
In T. Caelli, A. Amin, R. W. Duin, D.Ridder, & M. Kamel (Eds.
), Structural, Syntactic,and Statistical Pattern Recognition SE  - 12 (Vol.2396, pp.
123?132).
Springer Berlin Heidelberg.Curran, James, Clark, Stephen, & Bos, Johan (2007).Linguistically Motivated Large-Scale NLP withC&C and Boxer.
In Proceedings of the 45thAnnual Meeting of the Association forComputational Linguistics Companion VolumeProceedings of the Demo and Poster Sessions (pp.33?36).
Prague, Czech Republic: Association forComputational Linguistics.Fellbaum, Christiane (1998).
WordNet: An ElectronicLexical Database.
British Journal Of HospitalMedicine London England 2005 (Vol.
71, p. 423).Himsolt, Michael, & Passau, Universit?t (1996).GML?
: A portable Graph File Format.
Syntax, 1?11.Kamp, Hans, & Reyle, Uwe (1993).
From discourseto logic: Introduction to model theoretic semanticsof natural language, formal logic and discourserepresentation theory.Wu, Zhibiao, & Palmer, Martha (1994).
Verbsemantics and lexical selection.
In 32nd AnnualMeeting of the Association for ComputationalLinguistics,, 32, 133?138.Zhang, Libiao, Li, Yuefeng, Sun, Chao, & Nadee,Winai (2013).
Rough Set Based Approach to TextClassification.
Web Intelligence (WI) andIntelligent Agent Technologies (IAT), 2013IEEE/WIC/ACM International Joint Conferenceson.379
