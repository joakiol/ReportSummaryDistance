Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2306?2315,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsGraph-based Dependency Parsing with Bidirectional LSTMWenhui Wang Baobao ChangKey Laboratory of Computational Linguistics, Ministry of Education.School of Electronics Engineering and Computer Science, Peking University,No.5 Yiheyuan Road, Haidian District, Beijing, 100871, ChinaCollaborative Innovation Center for Language Ability, Xuzhou, 221009, China.
{wangwenhui,chbb}@pku.edu.cnAbstractIn this paper, we propose a neural networkmodel for graph-based dependency pars-ing which utilizes Bidirectional LSTM(BLSTM) to capture richer contextual in-formation instead of using high-order fac-torization, and enable our model to usemuch fewer features than previous work.In addition, we propose an effective wayto learn sentence segment embedding onsentence-level based on an extra forwardLSTM network.
Although our model usesonly first-order factorization, experimentson English Peen Treebank and ChinesePenn Treebank show that our model couldbe competitive with previous higher-ordergraph-based dependency parsing modelsand state-of-the-art models.1 IntroductionDependency parsing is a fundamental task for lan-guage processing which has been investigated fordecades.
It has been applied in a wide range of ap-plications such as information extraction and ma-chine translation.
Among a variety of dependencyparsing models, graph-based models are attractivefor their ability of scoring the parsing decisionson a whole-tree basis.
Typical graph-based mod-els factor the dependency tree into subgraphs, in-cluding single arcs (McDonald et al, 2005), sib-ling or grandparent arcs (McDonald and Pereira,2006; Carreras, 2007) or higher-order substruc-tures (Koo and Collins, 2010; Ma and Zhao, 2012)and then score the whole tree by summing scoresof the subgraphs.
In these models, subgraphs areusually represented as high-dimensional featurevectors which are then fed into a linear model tolearn the feature weights.However, conventional graph-based modelsheavily rely on feature engineering and their per-formance is restricted by the design of features.In addition, standard decoding algorithm (Eisner,2000) only works for the first-order model whichlimits the scope of feature selection.
To incor-porate high-order features, Eisner algorithm mustbe somehow extended or modified, which is usu-ally done at high cost in terms of efficiency.
Thefourth-order graph-based model (Ma and Zhao,2012), which seems the highest-order model so farto our knowledge, requires O(n5) time and O(n4)space.
Due to the high computational cost, high-order models are normally restricted to produc-ing only unlabeled parses to avoid extra cost in-troduced by inclusion of arc-labels into the parsetrees.To alleviate the burden of feature engineering,Pei et al (2015) presented an effective neural net-work model for graph-based dependency parsing.They only use atomic features such as word uni-grams and POS tag unigrams and leave the modelto automatically learn the feature combinations.However, their model requires many atomic fea-tures and still relies on high-order factorizationstrategy to further improve the accuracy.Different from previous work, we propose anLSTM-based dependency parsing model in thispaper and aim to use LSTM network to capturericher contextual information to support parsingdecisions, instead of adopting a high-order factor-ization.
The main advantages of our model are asfollows:?
By introducing Bidirectional LSTM, ourmodel shows strong ability to capture poten-tial long range contextual information and ex-hibits improved accuracy in recovering longdistance dependencies.
It is different to pre-vious work in which a similar effect is usuallyachieved by high-order factorization.
More-2306over, our model also eliminates the needfor setting feature selection windows and re-duces the number of features to a minimumlevel.?
We propose an LSTM-based sentence seg-ment embedding method named LSTM-Minus, in which distributed representation ofsentence segment is learned by using subtrac-tion between LSTM hidden vectors.
Experi-ment shows this further enhances our model?sability to access to sentence-level informa-tion.?
Last but important, our model is a first-ordermodel using standard Eisner algorithm fordecoding, the computational cost remains atthe lowest level among graph-based models.Our model does not trade-off efficiency foraccuracy.We evaluate our model on the English PennTreebank and Chinese Penn Treebank, experi-ments show that our model achieves competi-tive parsing accuracy compared with conventionalhigh-order models, however, with a much lowercomputational cost.2 Graph-based dependency parsingIn dependency parsing, syntactic relationships arerepresented as directed arcs between head wordsand their modifier words.
Each word in a sen-tence modifies exactly one head, but can have anynumber of modifiers itself.
The whole sentence isrooted at a designated special symbol ROOT, thusthe dependency graph for a sentence is constrainedto be a rooted, directed tree.For a sentence x, graph-based dependency pars-ing model searches for the highest-scoring tree ofx:y?
(x) = argmaxy?
?Y (x)Score(x, y?
; ?)
(1)Here y?
(x) is the tree with the highest score, Y (x)is the set of all valid dependency trees for x andScore(x, y?
; ?)
measures how likely the tree y?
isthe correct analysis of the sentence x, ?
are themodel parameters.
However, the size of Y (x)grows exponentially with respect to the length ofthe sentence, directly solving equation (1) is im-practical.The common strategy adopted in the graph-based model is to factor the dependency tree y?
intoFigure 1: First-order, Second-order and Third-order factorization strategy.
Here h stands for headword, m stands for modifier word, s and t standfor the sibling of m. g stands for the grandparentof m.a set of subgraph c which can be scored in isola-tion, and score the whole tree y?
by summing scoreof each subgraph:Score(x, y?
; ?)
=?c?y?ScoreC(x, c; ?)
(2)Figure 1 shows several factorization strategies.The order of the factorization is defined accord-ing to the number of dependencies that subgraphcontains.
The simplest first-order factorization(McDonald et al, 2005) decomposes a depen-dency tree into single dependency arcs.
Basedon the first-order factorization, second-order fac-torization (McDonald and Pereira, 2006; Carreras,2007) brings sibling and grandparent informationinto their model.
Third-order factorization (Kooand Collins, 2010) further incorporates richer con-textual information by utilizing grand-sibling andtri-sibling parts.Conventional graph-based models (McDonaldet al, 2005; McDonald and Pereira, 2006; Car-reras, 2007; Koo and Collins, 2010; Ma and Zhao,2012) score subgraph by a linear model, whichheavily depends on feature engineering.
The neu-ral network model proposed by Pei et al (2015)alleviates the dependence on feature engineeringto a large extent, but not completely.
We followPei et al (2015) to score dependency arcs usingneural network model.
However, different fromtheir work, we introduce a Bidirectional LSTM tocapture long range contextual information and anextra forward LSTM to better represent segmentsof the sentence separated by the head and modi-fier.
These make our model more accurate in re-covering long-distance dependencies and furtherdecrease the number of atomic features.2307Figure 2: Architecture of the Neural Network.
x1to x5stand for the input token of BidirectionalLSTM.
a1to a5stand for the feature embeddingsused in our model.3 Neural Network ModelIn this section, we describe the architecture of ourneural network model in detail, which is summa-rized in Figure 2.3.1 Input layerIn our neural network model, the words, POStags are mapped into distributed embeddings.
Werepresent each input token xiwhich is the in-put of Bidirectional LSTM by concatenating POStag embedding epi?
Rdeand word embeddingewi?
Rde, deis the the dimensionality of em-bedding, then a linear transformation weis per-formed and passed though an element-wise acti-vation function g:xi= g(we[ewi; epi] + be) (3)where xi?
Rde, we?
Rde?2deis weight matrix,be?
Rdeis bias term.
the dimensionality of inputtoken xiis equal to the dimensionality of word andPOS tag embeddings in our experiment, ReLU isused as our activation function g.3.2 Bidirectional LSTMGiven an input sequence x = (x1, .
.
.
, xn), wheren stands for the number of words in a sentence,a standard LSTM recurrent network computes thehidden vector sequence h = (h1, .
.
.
, hn) in onedirection.Bidirectional LSTM processes the data in bothdirections with two separate hidden layers, whichare then fed to the same output layer.
It com-putes the forward hidden sequence?
?h , the back-ward hidden sequence?
?h and the output sequencev by iterating the forward layer from t = 1 to n,the backward layer from t = n to 1 and then up-dating the output layer:vt=??ht+?
?ht(4)where vt?
Rdlis the output vector of Bidirec-tional LSTM for input xt,??ht?
Rdl,??ht?
Rdl, dlis the dimensionality of LSTM hidden vector.
Wesimply add the forward hidden vector?
?htand thebackward hidden vector?
?httogether, which getssimilar experiment result as concatenating themtogether with a faster speed.The output vectors of Bidirectional LSTM areused as word feature embeddings.
In addition,they are also fed into a forward LSTM networkto learn segment embedding.3.3 Segment EmbeddingContextual information of word pairs1has beenwidely utilized in previous work (McDonald etal., 2005; McDonald and Pereira, 2006; Pei etal., 2015).
For a dependency pair (h,m), previ-ous work divides a sentence into three parts (pre-fix, infix and suffix) by head word h and modifierword m. These parts which we call segments inour work make up the context of the dependencypair (h,m).Due to the problem of data sparseness, conven-tional graph-based models can only capture con-textual information of word pairs by using bigramsor tri-grams features.
Unlike conventional mod-els, Pei et al (2015) use distributed representa-tions obtained by averaging word embeddings insegments to represent contextual information ofthe word pair, which could capture richer syn-tactic and semantic information.
However, theirmethod is restricted to segment-level since theirsegment embedding only consider the word infor-mation within the segment.
Besides, averagingoperation simply treats all the words in segmentequally.
However, some words might carry more1A word pair is limited to the dependency pair (h,m) inour work since we use only first-order factorization.
In previ-ous work, word pair could be any pair with particular relation(e.g., sibling pair (s,m) in Figure 1).2308Figure 3: Illustration for learning segment embed-dings based on an extra forward LSTM network,vh, vmand v1to v7indicate the output vectorsof Bidirectional LSTM for head word h, modifierword m and other words in sentence, hh, hmandh1to h7indicate the hidden vectors of the forwardLSTM corresponding to vh, vmand v1to v7.salient syntactic or semantic information and theyare expected to be given more attention.A useful property of forward LSTM is that itcould keep previous useful information in theirmemory cell by exploiting input, output and for-get gates to decide how to utilize and update thememory of previous information.
Given an in-put sequence v = (v1, .
.
.
, vn), previous work(Sutskever et al, 2014; Vinyals et al, 2014) of-ten uses the last hidden vector hnof the forwardLSTM to represent the whole sequence.
Each hid-den vector ht(1 ?
t ?
n) can capture useful in-formation before and including vt.Inspired by this, we propose a method namedLSTM-Minus to learn segment embedding.
Weutilize subtraction between LSTM hidden vectorsto represent segment?s information.
As illustratedin Figure 3, the segment infix can be described ashm?
h2, hmand h2are hidden vector of the for-ward LSTM network.
The segment embedding ofsuffix can also be obtained by subtraction betweenthe last LSTM hidden vector of the sequence (h7)and the last LSTM hidden vector in infix (hm).
Forprefix, we directly use the last LSTM hidden vec-tor in prefix to represent it, which equals to sub-tract a zero embedding.
When no prefix or suffixexists, the corresponding embedding is set to zero.Specifically, we place an extra forward LSTMlayer on top of the Bidirectional LSTM layer andlearn segment embeddings using LSTM-Minusbased on this forward LSTM.
LSTM-minus en-ables our model to learn segment embeddingsfrom information both outside and inside the seg-ments and thus enhances our model?s ability to ac-cess to sentence-level information.3.4 Hidden layer and output layerAs illustrated in Figure 2, we map all the featureembeddings to a hidden layer.
Following Pei et al(2015), we use direction-specific transformation tomodel edge direction and tanh-cube as our activa-tion function:h = g(?iWdhiai+ bdh)(5)where ai?
Rdaiis the feature embedding, daiindicates the dimensionality of feature embeddingai, Wdhi?
Rdh?daiis weight matrices which cor-responding to ai, dhindicates the dimensionalityof hidden layer vector, bdh?
Rdhis bias term.
Wdhiand bdhare bound with index d ?
{0, 1} which in-dicates the direction between head and modifier.A output layer is finally added on the top of thehidden layer for scoring dependency arcs:ScoreC(x, c) = Wdoh+ bdo(6)Where Wdo?
RL?dhis weight matrices, bdo?
RLis bias term, ScoreC(x, c) ?
RLis the output vec-tor, L is the number of dependency types.
Each di-mension of the output vector is the score for eachkind of dependency type of head-modifier pair.3.5 Features in our modelPrevious neural network models (Pei et al, 2015;Pei et al, 2014; Zheng et al, 2013) normally setcontext window around a word and extract atomicfeatures within the window to represent the con-textual information.
However, context windowlimits their ability in detecting long-distance in-formation.
Simply increasing the context windowsize to get more contextual information puts theirmodel in the risk of overfitting and heavily slowsdown the speed.Unlike previous work, we apply BidirectionalLSTM to capture long range contextual informa-tion and eliminate the need for context windows,avoiding the limit of the window-based featureselection approach.
Compared with Pei et al(2015), the cancellation of the context window al-lows our model to use much fewer features.
More-over, by combining a word?s atomic features (wordform and POS tag) together, our model further de-creases the number of features.2309Pei et al (2015)h?2.w, h?1.w, h.w, h1.w, h2.wh?2.p, h?1.p, h.p, h1.p, h2.pm?2.w, m?1.w, m.w, m1.w, m2.wm?2.p, m?1.p, m.p, m1.p, m2.pdis(h, m)Our basic modelvh, vmdis(h, m)Table 1: Atomic features in our basic model andPei?s 1st-order atomic model.
w is short for wordand p for POS tag.
h indicates head and m indi-cates modifier.
The subscript represents the rela-tive position to the center word.
dis(h,m) is thedistance between head and modifier.
vhand vmin-dicate the outputs of Bidirectional LSTM for headword and modifier word.Table 1 lists the atomic features used in 1st-order atomic model of Pei et al (2015) and atomicfeatures used in our basic model.
Our basic modelonly uses the outputs of Bidirectional LSTM forhead word and modifier word, and the distance be-tween them as features.
Distance features are en-coded as randomly initialized embeddings.
As wecan see, our basic model reduces the number ofatomic features to a minimum level, making ourmodel run with a faster speed.
Based on our ba-sic model, we incorporate additional segment in-formation (prefix, infix and suffix), which furtherimproves the effect of our model.4 Neural TrainingIn this section, we provide details about trainingthe neural network.4.1 Max-Margin TrainingWe use the Max-Margin criterion to train ourmodel.
Given a training instance (x(i), y(i)), weuse Y (x(i)) to denote the set of all possible depen-dency trees and y(i)is the correct dependency treefor sentence x(i).
The goal of Max Margin train-ing is to find parameters ?
such that the differencein score of the correct tree y(i)from an incorrecttree y?
?
Y (x(i)) is at least4(y(i), y?
).Score(x(i),y(i); ?)?Score(x(i),y?
; ?)+4(y(i),y?
)The structured margin loss4(y(i), y?)
is definedas:4(y(i), y?)
=n?j?1{h(y(i), x(i)j) 6= h(y?, x(i)j)}where n is the length of sentence x, h(y(i), x(i)j)is the head (with type) for the j-th word of x(i)intree y(i)and ?
is a discount parameter.
The loss isproportional to the number of word with an incor-rect head and edge type in the proposed tree.Given a training set with size m, The regular-ized objective function is the loss function J(?
)including a l2-norm term:J(?)
=1mm?i=1li(?)
+?2||?||2li(?)
= maxy?
?Y (x(i))(Score(x(i),y?
; ?)+4(y(i),y?
))?Score(x(i),y(i); ?)
(7)By minimizing this objective, the score of thecorrect tree is increased and score of the highestscoring incorrect tree is decreased.4.2 Optimization AlgorithmParameter optimization is performed with the di-agonal variant of AdaGrad (Duchi et al, 2011)with minibatchs (batch size = 20) .
The param-eter update for the i-th parameter ?t,iat time stept is as follows:?t,i= ?t?1,i???
?t?=1g2?,igt,i(8)where ?
is the initial learning rate (?
= 0.2 in ourexperiment) and g??
R|?i|is the subgradient attime step ?
for parameter ?i.To mitigate overfitting, dropout (Hinton et al,2012) is used to regularize our model.
we applydropout on the hidden layer with 0.2 rate.4.3 Model Initialization&HyperparametersThe following hyper-parameters are used in allexperiments: word embedding size = 100, POStag embedding size = 100, hidden layer size =200, LSTM hidden vector size = 100, Bidirec-tional LSTM layers = 2, regularization parameter?
= 10?4.We initialized the parameters using pretrainedword embeddings.
Following Dyer et al (2015),we use a variant of the skip n-gram model in-troduced by Ling et al (2015) on Gigawordcorpus (Graff et al, 2003).
We also exper-imented with randomly initialized embeddings,where embeddings are uniformly sampled fromrange [?0.3, 0.3].
All other parameters are uni-formly sampled from range [?0.05, 0.05].2310Models UAS LAS Speed(sent/s)First-orderMSTParser 91.60 90.39 201st-order atomic (Pei et al, 2015) 92.14 90.92 551st-order phrase (Pei et al, 2015) 92.59 91.37 26Our basic model 93.09 92.03 61Our basic model + segment 93.51 92.45 26Second-orderMSTParser 92.30 91.06 142nd-order phrase (Pei et al, 2015) 93.29 92.13 10Third-order (Koo and Collins, 2010) 93.04 N/A N/AFourth-order (Ma and Zhao, 2012) 93.4 N/A N/AUnlimited-order(Zhang and McDonald, 2012) 93.06 91.86 N/A(Zhang et al, 2013) 93.50 92.41 N/A(Zhang and McDonald, 2014) 93.57 92.48 N/ATable 2: Comparison with previous graph-based models on Penn-YM.5 ExperimentsIn this section, we present our experimental setupand the main result of our work.5.1 Experiments SetupWe conduct our experiments on the English PennTreebank (PTB) and the Chinese Penn Treebank(CTB) datasets.For English, we follow the standard splits ofPTB3.
Using section 2-21 for training, section 22as development set and 23 as test set.
We con-duct experiments on two different constituency-to-dependency-converted Penn Treebank data sets.The first one, Penn-YM, was created by thePenn2Malt tool2based on Yamada and Matsumoto(2003) head rules.
The second one, Penn-SD,use Stanford Basic Dependencies (Marneffe et al,2006) and was converted by version 3.3.03ofthe Stanford parser.
The Stanford POS Tagger(Toutanova et al, 2003) with ten-way jackknifingof the training data is used for assigning POS tags(accuracy ?
97.2%).For Chinese, we adopt the same split of CTB5as described in (Zhang and Clark, 2008).
Follow-ing (Zhang and Clark, 2008; Dyer et al, 2015;Chen and Manning, 2014), we use gold segmen-tation and POS tags for the input.5.2 Experiments ResultsWe first make comparisons with previous graph-based models of different orders as shown in Ta-2http://stp.lingfil.uu.se/nivre/research/Penn2Malt.html3http://nlp.stanford.edu/software/lex-parser.shtmlble 2.
We use MSTParser4for conventional first-order model (McDonald et al, 2005) and second-order model (McDonald and Pereira, 2006).
Wealso include the results of conventional high-ordermodels (Koo and Collins, 2010; Ma and Zhao,2012; Zhang and McDonald, 2012; Zhang et al,2013; Zhang and McDonald, 2014) and the neu-ral network model of Pei et al (2015).
Differentfrom typical high-order models (Koo and Collins,2010; Ma and Zhao, 2012), which need to extendtheir decoding algorithm to score new types ofhigher-order dependencies.
Zhang and McDonald(2012) generalized the Eisner algorithm to handlearbitrary features over higher-order dependenciesand controlled complexity via approximate decod-ing with cube pruning.
They further improve theirwork by using perceptron update strategies for in-exact hypergraph search (Zhang et al, 2013) andforcing inference to maintain both label and struc-tural ambiguity through a secondary beam (Zhangand McDonald, 2014).Following previous work, UAS (unlabeled at-tachment scores) and LAS (labeled attachmentscores) are calculated by excluding punctuation5.The parsing speeds are measured on a workstationwith Intel Xeon 3.4GHz CPU and 32GB RAMwhich is same to Pei et al (2015).
We measurethe parsing speeds of Pei et al (2015) according totheir codes6and parameters.On accuracy, as shown in table 2, our4http://sourceforge.net/projects/mstparser5Following previous work, a token is a punctuation if itsPOS tag is {?
?
: , .
}6https://github.com/Williammed/DeepParser2311MethodPenn-YM Penn-SD CTB5UAS LAS UAS LAS UAS LAS(Zhang and Nivre, 2011) 92.9 91.8 - - 86.0 84.4(Bernd Bohnet, 2012) 93.39 92.38 - - 87.5 85.9(Zhang and McDonald, 2014) 93.57 92.48 93.01 90.64 87.96 86.34(Dyer et al, 2015) - - 93.1 90.9 87.2 85.7(Weiss et al, 2015) - - 93.99 92.05 - -Our basic model + segment 93.51 92.45 94.08 91.82 87.55 86.23Table 3: Comparison with previous state-of-the-art models on Penn-YM, Penn-SD and CTB5.basic model outperforms previous first-ordergraph-based models by a substantial margin,even outperforms Zhang and McDonald (2012)?sunlimited-order model.
Moreover, incorporatingsegment information further improves our model?saccuracy, which shows that segment embeddingsdo capture richer contextual information.
By usingsegment embeddings, our improved model couldbe comparable to high-order graph-based models7.With regard to parsing speed, our model alsoshows advantage of efficiency.
Our model usesonly first-order factorization and requires O(n3)time to decode.
Third-order model requires O(n4)time and fourth-order model requires O(n5) time.By using approximate decoding, the unlimited-order model of Zhang and McDonald (2012) re-quires O(k ?log(k)?n3) time, where k is the beamsize.
The computational cost of our model is thelowest among graph-based models.
Moreover, al-though using LSTM requires much computationalcost.
However, compared with Pei?s 1st-ordermodel, our model decreases the number of atomicfeatures from 21 to 3, this allows our model to re-quire a much smaller matrix computation in thescoring model, which cancels out the extra compu-tation cost introduced by the LSTM computation.Our basic model is the fastest among first-orderand second-order models.
Incorporating segmentinformation slows down the parsing speed while itis still slightly faster than conventional first-ordermodel.
To compare with conventional high-ordermodels on practical parsing speed, we can makean indirect comparison according to Zhang andMcDonald (2012).
Conventional first-order modelis about 10 times faster than Zhang and McDon-7Note that our model can?t be strictly comparable withthird-order model (Koo and Collins, 2010) and fourth-order model (Ma and Zhao, 2012) since they are unlabeledmodel.
However, our model is comparable with all the threeunlimited-order models presented in (Zhang and McDon-ald, 2012), (Zhang et al, 2013) and (Zhang and McDonald,2014), since they all are labeled models as ours.Method Peen-YM Peen-SD CTB5Average 93.23 93.83 87.24LSTM-Minus 93.51 94.08 87.55Table 4: Model performance of different way tolearn segment embeddings.ald (2012)?s unlimited-order model and about 40times faster than conventional third-order model,while our model is faster than conventional first-order model.
Our model should be much fasterthan conventional high-order models.We further compare our model with previousstate-of-the-art systems for English and Chinese.Table 3 lists the performances of our model as wellas previous state-of-the-art systems on on Penn-YM, Penn-SD and CTB5.
We compare to conven-tional state-of-the-art graph-based model (Zhangand McDonald, 2014), conventional state-of-the-art transition-based model using beam search(Zhang and Nivre, 2011), transition-based modelcombining graph-based approach (Bernd Bohnet,2012) , transition-based neural network model us-ing stack LSTM (Dyer et al, 2015) and transition-based neural network model using beam search(Weiss et al, 2015).
Overall, our model achievescompetitive accuracy on all three datasets.
Al-though our model is slightly lower in accuarcythan unlimited-order double beam model (Zhangand McDonald, 2014) on Penn-YM and CTB5,our model outperforms their model on Penn-SD.It seems that our model performs better on datasets with larger label sets, given the number of la-bels used in Penn-SD data set is almost four timesmore than Penn-YM and CTB5 data sets.To show the effectiveness of our segment em-bedding method LSTM-Minus, we compare withaveraging method proposed by Pei et al (2015).We get segment embeddings by averaging the out-put vectors of Bidirectional LSTM in segments.2312Figure 4: Error rates of different distance betweenhead and modifier on Peen-YM.To make comparison as fair as possible, we let twomodels have almost the same number parameters.Table 4 lists the UAS of two methods on test set.As we can see, LSTM-Minus shows better per-formance because our method further incorporatesmore sentence-level information into our model.5.3 Impact of Network StructureIn this part, we investigate the impact of the com-ponents of our approach.LSTM Recurrent NetworkTo evaluate the impact of LSTM, we make er-ror analysis on Penn-YM.
We compare our modelwith Pei et al (2015) on error rates of differentdistance between head and modifier.As we can see, the five models do not showmuch difference for short dependencies whose dis-tance less than three.
For long dependencies, bothour two models show better performance com-pared with the 1st-order model of Pei et al (2015),which proves that LSTM can effectively capturelong-distance dependencies.
Moreover, our mod-els and Pei?s 2nd-order phrase model both im-prove accuracy on long dependencies comparedwith Pei?s 1st-order model, which is in line withour expectations.
Using LSTM shows the sameeffect as high-order factorization strategy.
Com-pared with 2nd-order phrase model of Pei et al(2015), our basic model occasionally performsworse in recovering long distant dependencies.However, this should not be a surprise since higherorder models are also motivated to recover long-distance dependencies.
Nevertheless, with the in-troduction of LSTM-minus segment embeddings,our model consistently outperforms the 2nd-orderphrase model of Pei et al (2015) in accuracies ofall long dependencies.
We carried out significancetest on the difference between our and Pei?s mod-els.
Our basic model performs significantly betterthan all 1st-order models of Pei et al (2015) (t-test with p<0.001) and our basic+segment model(still a 1st-order model) performs significantly bet-ter than their 2nd-order phrase model (t-test withp<0.001) in recovering long-distance dependen-cies.Initialization of pre-trained word embeddingsWe further analyze the influence of using pre-trained word embeddings for initialization.
with-out using pretrained word embeddings, our im-proved model achieves 92.94% UAS / 91.83%LAS on Penn-YM, 93.46% UAS / 91.19% LASon Penn-SD and 86.5% UAS / 85.0% LAS onCTB5.
Using pre-trained word embeddings canobtain around 0.5%?1.0% improvement.6 Related workDependency parsing has gained widespread inter-est in the computational linguistics community.There are a lot of approaches to solve it.
Amongthem, we will mainly focus on graph-based de-pendency parsing model here.
Dependency treefactorization and decoding algorithm are neces-sary for graph-based models.
McDonald et al(2005) proposed the first-order model which de-composes a dependency tree into its individualedges and use a effective dynamic programmingalgorithm (Eisner, 2000) to decode.
Based on first-order model, higher-order models(McDonald andPereira, 2006; Carreras, 2007; Koo and Collins,2010; Ma and Zhao, 2012) factor a dependencytree into a set of high-order dependencies whichbring interactions between head, modifier, siblingsand (or) grandparent into their model.
However,for above models, scoring new types of higher-order dependencies requires extensions of the un-derlying decoding algorithm, which also requireshigher computational cost.
Unlike above models,unlimited-order models (Zhang and McDonald,2012; Zhang et al, 2013; Zhang and McDonald,2014) could handle arbitrary features over higher-order dependencies by generalizing the Eisner al-gorithm.In contrast to conventional methods, neural net-work model shows their ability to reduce the effortin feature engineering.
Pei et al (2015) proposeda model to automatically learn high-order feature2313combinations via a novel activation function, al-lowing their model to use a set of atomic featuresinstead of millions of hand-crafted features.Different from previous work, which is sensi-tive to local state and accesses to larger context byhigher-order factorization.
Our model makes pars-ing decisions on a global perspective with first-order factorization, avoiding the expensive com-putational cost introduced by high-order factoriza-tion.LSTM network is heavily utilized in our model.LSTM network has already been explored intransition-based dependency parsing.
Dyer etal.
(2015) presented stack LSTMs with pushand pop operations and used them to imple-ment a state-of-the-art transition-based depen-dency parser.
Ballesteros et al (2015) replacedlookup-based word representations with character-based representations obtained by BidirectionalLSTM in the continuous-state parser of Dyer etal.
(2015), which was proved experimentally to beuseful for morphologically rich languages.7 ConclusionIn this paper, we propose an LSTM-based neuralnetwork model for graph-based dependency pars-ing.
Utilizing Bidirectional LSTM and segmentembeddings learned by LSTM-Minus allows ourmodel access to sentence-level information, mak-ing our model more accurate in recovering long-distance dependencies with only first-order factor-ization.
Experiments on PTB and CTB show thatour model could be competitive with conventionalhigh-order models with a faster speed.AcknowledgmentsThis work is supported by National Key Ba-sic Research Program of China under GrantNo.2014CB340504 and National Natural ScienceFoundation of China under Grant No.61273318.The Corresponding author of this paper is BaobaoChang.ReferencesMiguel Ballesteros, Chris Dyer, and Noah A. Smith.2015.
Improved transition-based parsing by model-ing characters instead of words with lstms.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP 2015,Lisbon, Portugal, September 17-21, 2015, pages349?359.Jonas Kuhn Bernd Bohnet.
2012.
The best ofboth worlds: a graph-based completion model fortransition-based parsers.
Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics.Xavier Carreras.
2007.
Experiments with a higher-order projective dependency parser.
In EMNLP-CoNLL, pages 957?961.Danqi Chen and Christopher D. Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing, pages 740?750.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, pages 2121?2159.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of the 53rd AnnualMeeting of the Association for Computational Lin-guistics, pages 334?343.Jason Eisner.
2000.
Bilexical Grammars and theirCubic-Time Parsing Algorithms.
Springer Nether-lands.David Graff, Junbo Kong, Ke Chen, and KazuakiMaeda.
2003.
English gigaword.
Linguistic DataConsortium, Philadelphia.Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,Ilya Sutskever, and Ruslan R Salakhutdinov.
2012.Improving neural networks by preventing co-adaptation of feature detectors.
arXiv preprintarXiv:1207.0580.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, pages 1?11.
Association forComputational Linguistics.Wang Ling, Chris Dyer, Alan Black, and IsabelTrancoso.
2015.
Two/too simple adaptations ofword2vec for syntax problems.
In Proceedings ofthe 2015 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies.Xuezhe Ma and Hai Zhao.
2012.
Fourth-order de-pendency parsing.
In COLING 2012, 24th Inter-national Conference on Computational Linguistics,Proceedings of the Conference: Posters, 8-15 De-cember 2012, Mumbai, India, pages 785?796.Marie Catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.Lrec, pages 449?454.2314Ryan T McDonald and Fernando CN Pereira.
2006.Online learning of approximate dependency parsingalgorithms.
In EACL.
Citeseer.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proceedings of the 43rd an-nual meeting on association for computational lin-guistics, pages 91?98.
Association for Computa-tional Linguistics.Wenzhe Pei, Tao Ge, and Baobao Chang.
2014.
Max-margin tensor neural network for chinese word seg-mentation.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(Volume 1: Long Papers), pages 293?303.Wenzhe Pei, Tao Ge, and Baobao Chang.
2015.
Aneffective neural network model for graph-based de-pendency parsing.
In Proceedings of the 53rd An-nual Meeting of the Association for ComputationalLinguistics, pages 313?322.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems 27: Annual Conference on Neural In-formation Processing System, pages 3104?3112.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Oriol Vinyals, Lukasz Kaiser, Terry Koo, SlavPetrov, Ilya Sutskever, and Geoffrey E. Hinton.2014.
Grammar as a foreign language.
CoRR,abs/1412.7449.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural net-work transition-based parsing.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of IWPT, volume 3, pages195?206.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-basedand transition-based dependency parsing.
In 2008Conference on Empirical Methods in Natural Lan-guage Processing, pages 562?571.Hao Zhang and Ryan T. McDonald.
2012.
Generalizedhigher-order dependency parsing with cube prun-ing.
In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 320?331.Hao Zhang and Ryan T. McDonald.
2014.
Enforc-ing structural diversity in cube-pruned dependencyparsing.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguis-tics, pages 656?661.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InThe 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies, pages 188?193.Hao Zhang, Liang Huang Kai Zhao, and Ryan Mcdon-ald.
2013.
Online learning for inexact hypergraphsearch.
Proceedings of Emnlp.Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.2013.
Deep learning for Chinese word segmentationand POS tagging.
In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 647?657, October.2315
