Indexation de textes : l'apprentissage des conceptsC.
Enguehard* - P. Malvache**-  P. Tr igano *?
Universit6 de Technologie de Compi~gneURA CNRS 817 - GIBP 64960206 Compi~gne CEDEX - FRANCEEMail : Tligano@FRUTC51.bitnet?
* Commissariat "~L'Energie AtomiqueCentre d'Etudes de Cadarache13108 Saint-Paul-Lez-Durance - FRANCEABSTRACThi technical fields, mmly documents go unreaddue to a lack of awareness of their existence.
Asystem which indexes texts can find allrelevant texts in response to a query.
Theproblem is to establish the indexation.
Atpresent, adwmced full text systemsautomatically index texts on the completethesaurus with computed weights.
Anotherway of doing this carl be a person choosing theset of relevant concepts.
This second solutionis better but more costly and dependent on theclassification choices made by the operator.To meet these problems, ANA (AuomaticNatural Acquisition) had been developed.
Thissystem automatically extracts relevantconcepts from free texts to produce a semanticnetwork.
It does not rely on grammar orlexicon but, instead, is based on ,an originalstatistical method.This research brings about wo developments :oll one hand the system is also capable ofextracting the simple grammatical structures itencounters, most often in order to improve itsperformance, and on the other hand this willlead to an automatic definition of semanticclasses of concepts, in order to structure thenetwork.ACRES DE COLING-92, NANTES.
23-28 ^Ol~q" 1992 1 I 9 7 PROC.
OF COLING-92.
NANI'ES, AUG. 23-28.
19921 - INTRODUCTION :Le domaine des grandes bases de comlaissances,rassemblant des textes, est apparu vers les anndes 50comme une des applications privilEgiEes de la puissancedes ordinateurs.
Deux besoins cruciaux out Et6identifi~s : l'indexation des textes doit Otre correcte, larecherche dolt /~tre efficace en rdponse ~ une simplequestion.Au cocur de ces probl~mes, e pesent le choix desconcepts et, plus gEnEralement, la definition denouveaux thesaurus.
Sahon avait prEconisd d~s 1966l'automatisation de ces tilches car leur rEalisationmanuelle st coflteuse t non dEtemfiniste \[SALT 66\].Nous prEsentons ici le syst~me ANA(Apprentissage Naturel AutomatisE) qui sElectionne lesconcepts (sur lesquels seront indexes les textes de labase), eL les structures afin de faciliter las interrogationsuttErieures.Nous avons choisi de travailler avec le minimumde connaissances, sails analyseur syntaxique, sansdictiormaire, uniquement par l'observation statistique destextes.
Les concepts ElectionnEs ont alors directementissus de la langue employee.
A cette exigeucc dcsimplicitE, nous avons ajoutd la robustesse.
Le systEmedolt supporter les dysfonctionnements que pourraitcauser une lacune clans ses connaissances.
Enfin, lasimplicitd des ressources utilisEes permet au syst~med'auto-dEcouvrir les connaissances dont il a besoin.l ndexat ion  manue l leLes syst~mes les plus simples et les plus rEpandassont bases sur la selection de mots-clEs clans les textes.Une question utilisant ces mots donne accEs aux textesainsi sdlectionnEs.
Ces syst~mes prEsententl'ineonvEnient d'&re tr~.s rigides : I'ajout d'un nouveaumot-clE oblige ,h reparcourir tousles textes dEjb.
indexespour y rechercher sa presence.
M~me automatisEe, cetteprocedure est trb, s contraignante.
De plus Salton\[SALT 86\] a dEmontr6 les inconvEnients de I'indexationmanuelle.
A titre d'exemple, deux sujets diffErcnts nechoisissent quhh 70% des mots-clEs identiques pourindexer un m~me document ~ I'aide du m~me thesaurus.De plus, des informations, qui, hun moment dnnnd, nesemblent pas pertinentes/~ l'indexeur peuvent jouer unr61e contexte important \[ANDRa\]MEthodes  s ta t i s t iquesLe probi~me du choix des concepts est contournEIorsque l'on utilise le thesaurus en entier.Des crit~:res purement statistiques, e rEfErant ~t la valeurdes termes d'indexation et non/1 leur sens \[DACH\] sontutilisds pour indexer les textes.Trb~s t6t, Stiles a montrd l'intdrt~.t de prendre encompte les occurrences simultandes de termes\[STIL 61\].
Plus rEcemment sont apparus les rEseauxconnexionistes qui permettent de gErer dynamiquementles liens et les coefficients de ponddration affectant lestermes d'indexation du thesaurus \[KIMO 90\].
Dans\[ANDRc\],  on utilise les probabilitEs de Bayesactualis~es en fonction des rEponses et du poidssEmantique des termes dans le thesaurus (ou ledictionnaire).
Cette thEorie oblige /1 distinguerhomographes et synonymes car ceux-ci peuventprovoquer des biais importants.
Turtle tente de simplifierles calculs de probabilitE dont la complexitE grandit defad:on exponentielle avecla taille de la base \[TURT 91\].D'autres mEthodes sont dEveloppEes pourrepresenter le contenu sEmantique de chaque document,en particulier ~ raide de matrices : les lignes 6tant lesdocuments et les colonnes les mots-clEs.
C'est lamdthode de la structuration de la sEmantique latente\[FURN\], \[DEER 88\], \[DEER 90\],Approches mixtesEntre ces deux extremes, l ' intervention del'intelligence humaine darts l'indexation manuelle, et laprise en compte de tout le thesaurus (sanscomprehension), l'Intelligence Artificielle oriente sesrecherches vers I'automatisation du choix des conceptsporteurs de l'indexatiou.
Le problEme est alors de dEfinirles crit~res qui permettront la selection des concepts.Certains syst~mes utilisent des connaissanceslexicales, syntaxiques, parrots sEmantiques (lessynonymes).
S. David pense que l'analyse morpho-syntaxique st une 6tape indispensable : l'utilisation depatrons catEgoriels permet d'isoler les groupes de rootsintEressants \[DAVI\].
Ces approches linguistiques, hpriori les plus appropriEes, sont aussi les plus difficiles himplanter.De nombreux systEmes mixtes font intervenir b. lafois des outils linguistiques et statistiques.
Le systEmeSpirit en est un boll exemple.
Les textes y sont analysesdans le but de repdrer les ElEments articulatoires dulangage qu' util ise l 'analyse l inguistique poursElectionner les concepts jugEs pertinents.
Des filtresstatistiques Evaluent les pondErations \[ANDRa\].2 - PRESENTATIONNous avons choisi d'utiliser l'apprentissage pouracqudrir les concepts correspondants aux textes traitEs,L'apprentissage automatique da langage (russe) par lecomptage d'occurrences a dEj/t 6t~.
EtudiE parAndreewsky \[ANDRb} mats le but Etait alors dedEcouvrir la grammaire de le langue h traversl'agencernent des ddclinaisons.Notre idEe a 6rE de concevoir un syst~:me aussisimple que possible avec le minimum de connaissance,mOme incomplete.Ce syst~me rEpond au problb~me du choix desconcepts en n'utilisant ni l'analyse syntaxique ousEmantique nile dictionnaire.Nous avons essayE d'Evaluer et de rEduire autantque possible les colmaissances, explicites et implicites,fournies au systb, me.
Celui-ci est efficace lorsque lestextes se referent h un domaine technique.
/Is sont alorsgEnEralement Ecrits dans un langage dit "opEratif", unlangage precis comportant peu d'homographes ou desynonymes \[FALZI.La mise en oeuvre d'heuristiques tr~s simplespermet au syst~me d'acquErir une experience des objetsfamiliers du domaine qui apparaissent darts les textesfoumis.
Cette connaissance se rEf~re directement aulangage utilisE dans les textes, m~me si ceux-ci ne sontpas syntaxiqaemant corrects ou si les roots employEs ontun sans different de leur definition.Nous prEsenterons dans un premier temps les processusmis en oeuvre dans le syst~me ANA.
Ensuite, nousACTES DE COLING-92, NAhTE.~, 23-28 hOt~ 1992 1 I 9 8 PgOC.
OF COLING-92.
NANTES, AUG. 23-28.
1992examinerons ses llouvelles fonctioanalitds et losextensions que nous lui avons apportdes.
Enfin, scrontprdsent~,s les rdsultats d'un test sur un corpus de 120 000lOOtS.Notons que nous utilisons uu module qui permet deddfinir, d'instancier et de g6rer des classes d'objets et desliens (Property Driven Model \[BART 79\]).Cette pr6sentation sera inustrde de nonlbreuxexemples pour lesquels nous nollS situerons dans lecadre d'une application domestiquc.3 - LE  SYSTEME ANA \[EN(-IU 911:Le prentier objectif est Ic choix automatiquc deconcepts en vu de I'indexation de textes.
Un concept estla forme canonique corrcspolldant ~ant classe de nlotsou de syntagmes.
"VERRE", par excmplc, klentific lesroots "verre", "verres".?
Les  conna issances  proc6dura lesNous avons utilis6 un ix)stulat se r6fdrant h des aspectsstatistiquas ou surl~.ciques du hmgage :Les 6v~nements frdqnents oot slgnifieatlfs.Ce postulat peut 6tre appliqu6 :- pour rechercher des sdquences de roots r6p6titives,- pour identifier des configurations d6notant desconcepts.Ces configurations privildgidts sont implantdessous forme de deux modules ym6triques que I'on tenterade faire con'espondre avec le texte.Si l'on rencontre l'ane de ces configurations :, ,.
\[ 'mot mot spdcifiant concept'~incomm an scll~nla con.ha JQconc pt u'n sch,ma \[ in~I?tn 0I mot Sl~Scifiant I connuAlors le loot incollou es\[ considdrd col/linesusceptible de dcvcnir un concept.Les roots sp~cifiallt les scht~mas soot acquis parappremissage si le corpus est suffisamment imlmrtant,ou donnds snag lorme d6clarative.-~boots t rap 'Le reseau de concepts est initialise par un6chantillon de concepts que notls appelons 'bootstrap'dans l'esprit de Pitrat \[PITR\].Durant le processus, le syst~me ssaie de confirmercet ensemt)le : tin concept est 'confirmC st, 616 dubootstrap, il est ddcouvert par le syst~me n cours defonctionnement.?
Les  conna issances  d~c lara t ives1 -L l s te  de roots videsLa liste de roots vides rasstmble quelquespr6positions, conjonctions, adverbes .... qui sootconsid6r6s contme non significatifs, ns ne pourront niobtenir le statut de concept b. titre individuel, ni figurerau ddbut ou ~ la fin ffun concept.2 - Liste de roots fortement li(~sCertains roots vnisins penvent st  fondre en anunique mot au mdpris de la nuance exprim6e darts letexte.
Par exentple, darts ces phrases, la variation de sensest trop fine pour que nous onus y attachions.11 a mangd des fraisesnu II a mangd toutes les fraises12J cld de la porteon La cld de cette porteJ'ai signd routes ces lettres urgentesou J'ai signd les lettres urgentes.l.es termes fortement lids, consid6r6s comme ununique ternte, soot g6n6ralement de la forintpr6position-articte cnnmle "de la" ou "d'une".3 - Les sehdmasEidm, les configurations intdressantes signal6es ci-dessus (nous los appelltrons des 'sch6mas') prennent laforme d'une simple lisle de roots comme "de", "de la","de 1", "en", etc.. Mod iSeNous utilisons plusieurs classes d'objcts :- l , '  " "" ' n '  :Ses instmtces sont les concepts connu5 du domaineauxquels viendront s'ajnuter les concepts dgcouverts parIc syst~'.me, lls soot lids par la relation "amont" qui estrole premiere structurati(m du rdseau.Darts les figures Its concepts ont toujours entourdsd'un double cadrc, darts le texte, ils sont 6crits encapitales.- ~ e s  tspressions et des candidats :Ces dcux classes correspondent h deux mdcanismesdc d6couvertc de nouvcal.lx concepts, lls pennettent destocker les occarrellgCS de textes jugdes intdressantes, etde noter la fr6.1tucnce de cos configurations.Fonct ionnement1 ?
Analyse lexicaleL'mmlyse lexicale sc limite ,5 la reconnaissance d sCOllCepts connus.
Toutes les marques de ponctuation sont61imindes.
Cette reconnaissance est tol6rante aux famesd'orthograpbe etaux diffdrentes flexions qui peuvent ~trereucontrdes.Le texte amsi per~u est analys6 en appliquant lepostulat au colltcxte local autour des concepts.2 - Recueil des occurrencesWechniquement, le texte est vu an travers d'unefen&re de quatre roots.
Les mots rides et ceux de moillsde deux lettres ne sont pas pris en compte dans le calculde l'empan de cette fen~tre.La fent~tre st d6placde tout le long du texte, soncontenu ast recueilli suivant trois voles diffiSrentes enflmction de sa nature.AcrEs DE COLlNG-92, Nam'v2s, 23-28 AO~r 1992 1 1 9 9 PIIoc.
OF COL1NG-92, NANTES.
AUG. 23-28, 1992Cas 1Lorsque le systEme voit deux concepts, il noteI'occurrence, c'est ~ dire l'extrait de texte que laisse voirla fen~tre, dmls un objet du type "expression" particulierces deux concepts,ex : Soit le texte : "je voudrais un VERRE d'EAUou de ..."L'occurrence "VERRE d EAU" est dcrite dansl'objet expression eorrespondant.Cas 2S'il ne voit qu'un concept (ici "VERRE"), lecontexte local est analyse pour repErer un schema, etdone un mot potenticllemcnt intEressant ("lair").
Unobjet de type candidat portant son nora recueillcl'occurrence.ex : Soit le texte "j'ai renversd mon VERRE de laildevanl.,.
"L'occurrence "VERRE de lait" est dcrite dartsle candidat "lait"Cas 3Si l'examen du contexte local ne fair apparai'treaucun sch6ma connu, l'occurrence est 6galementconservEe dans un champ spEcifique.
Elie sera traitdediffEremment.ex : Soit le texte "Voici de I'EAU mindrale"L'occurrence "Voici de I'EAU mindrale" est6trite dans le candidat "EAU"3 - Analyse des oceurrenc~Cette phase de lecture est suivie de I'ex,'m~en desinformations recueillies.
Seuls les objets ayant recueilliplus de n occurrences sont examines.Les expressionsSi la m~me configuration, aux variations morpho-syntaxiques prt:s, se prEsente n fois au moins, elledevient un concept sous sa forme la plus frEqucnte,ex : Voici les occnrrcnces de l'expressionrassemblant "VERRE" et "EAU" :"je voudrais un VERRE d'EAU ou de...""Bois un VERRE d'EA U pour faire...""aspirine dans ton VERRE d'EAU..,"L'analyse va qualifier le nouveau concept "VERRED'EAU"Les candidats et les schemasLes candidats dont la frEquence st supErieure auscull m deviennent eux-m~mes des concepts sous laforme morpho-syntaxique la plus fr~luente.ex : Voici les occurrences du candidat "lair" :"j'ai renversd mon POT de lair devant.,.
""distribuer un VERRE de lait ~ chacun.,.
""Boire un VERRE de lait c'est...""Je prdfdre un VERRE de lait t~ature., ""J'ai vidd la BOUTEILLE de lair qui dtait../'L'analyse va qualifier le nouveau concept "LAIT"Les eandidats ans schemaLes concepts existants prEsentant n fois le mOmecontexte local engendrent un nouveau concept intEgrantce contexte.ex : Voici les occurrences sans schema du candidat"VERRE" :"Bois un grand VERRE cela ira mieur,.."'7'ai achetd un VERRE d bidre...""Voici ce grand VERRE dont je t'ai parld..."L'analyse va qualif ier le nouveau concept"GRAND VERRE"Les seuils net  m sum arbitrairement fix6s aux valeurs 3et 5 qui se sore expErimentalement rEvElEes correctespour des corpus de 40 D00 A 200 000 roots.
Cependant ilsemblerait nEcessaire de les rendre adaptatifs quand lecorpus devient rEs grand.Le r~seau obtenuNous reprEsentons les rEsultats obtenus sur lesexemples prEcEdents :f~gure  l4 - Les  r~sultats :Ce systEme rEpond de fac?on trSs satisfaisante t nosattemes.Voici les rEsultats de son fonctionnement sur des textestotalisant environ 120 000 roots et provenantd'interviews relatives au retour d'expErience dudEmarrage du rEacteur ~ neutrons rapides Super-PhEnix.La base initiale comprenait 350 concepts effectivementutilisEs dans les textes analyses.L'analyse a donn6 lieu/t la dEcouverte de 700 nouveauxconcepts dont les deux-t iers ont 6t6 jugSsqualitativcment trEs bons.
D'autre part, 260 des conceptsdu bootstrap om 6t6 confirmEs.D'autres rEsultats sont dEtaillEs darts \[ENGU 91\].4 - LES  EXTENSIONSNous abordons l'apprentissage d s connaissancesutilisEes pour l'apprentissage !
Nous avons vu commentdEcouvrir des concepts.
Le syst~.me va maintenantapprendrc une partie des connaissmlces nEcessaires ~cepremier apprentissage, c'est h dire les connaissancesddclaratives : la liste des roots rides, la liste de rootsfortement lies et les roots spEcifiant les schemas.Les rEsultats de cet apprentissage, l s listes que lesyst~me va 6tablir, ne seront pas exactement identiquesaux listes fixdes h I'avance qui, jusqu'b, present, luidtaient fournies.
Nous nous attentions b. ce que sonfonctionnement en soit amdliord : le processus van6gliger certains schemas, rares darts l'6chantillon, enmettre de nouveaux h jour auxquels noas n'avions paspens6.
Bref, l'addquation ~la langue manipul6e darts lestextcs sera meilleure.Les extensions de l 'apprentissageLe postulat est applique ~ la structure interne desconcepts afin de ddcouvrir la fagon dont ils sont formEs.Ac-r~ DE COLING-92.
NAN'r~s, 23-28 Ao~'r 1992 l 2 0 0 PROC.
OF COLING-92, N.~t~s, AUC.
23-28, 1992Les configurations les plus frdquentes toumir0nt desgdn6ralisations qui serviront ,5 ddgager les schdmas deddcouverte des nouveaux concepts.Examinons l 'apprentissage des conuaissancesddclaratives qui auparavant 6talent fournies au syst~me :la liste des mots rides, la liste de roots fortement lids etles roots spdcifiant les schdnlas.L"~pprentissage d s ennn'dssances d~elarativesAfin de moddliser la structure interne des concepts,nous ddfinissons une nouvene classe d'objets.Ulle nouvellc classe d'objets : les termesLes temles sont les roots composant les concepts.lls SORt lids entre eux par la relation "voisin" quimdmorise la frdquence de chaque association.
De chaqueterrae nous eonnaissons le nonrbre d'occarreuces et lcfait qu'il soit, ou non, concept ~ titre individuel.
Lestermes sont entourds d'un simple cadre dulls lesrepr6sentations graphiques.II WmSE DE dAFfy\[~' .Vo is i j~  ' voi~inBOL DE cA~ If~gure  2Cet exemple montre la ddcomposition des concepts"TASSE DE CAFE" et "BOL DE CAFE" en troistermes chacun.
Remarquons que les termes '"I'ASSE" et"CAFE" sum eux-mt:mes des concepts, alors que "bol"et "de" n'en sont pus.D?~grmination ~le la list~ de roots vi(tc~Pour obtenir une liste de bonne qualit6, il estndcessaire d'utiliser un 6chantillon de textes d'au nloins40 000 mots, soit environ 100 pages (minimum issu del'examen de diffdrents corpus).Le syst~me lit l'dchantillon et compte tousles rootsqu'il rencontre.
Un terme est ici strictement ddfini par saforme, par la chatne ordonnde de caractdres qui lecomposent.
Ainsi, "chaise" et "chaises" sont considdrdscomme deux termes diffdrents.Les diffdrents termes sont ensuite classds enfonction de leurs frdquences ddcroissantes el affectdsd'un numdro correspondant ,5 leur rmlg.La courbe, frdqucnce = f (log (rang)), est seuilldeau rang s tel que raire As (d6finie par la courbe, raxe desabscisses, I 'axe des ordonndes et la droite x = s),approche 95 % de l'aire totale A (ddfinie par la coarbe etl'axe des abscisses).Soient : n, le hombre de temles de l'dchantinorA - f(x).log (x) ,x= lsCherchons  tel queA s < 0,95 A < As+ 1DEs lors, tous les rook'; de rang x <- s sont des inolsrides, lls sont 6crits dmls la liste addquate.k ~ ?
-rots "tbrtement li~.s et des mots tieLes roots caractdrisant les schdmas ont la propridtdde lier des cuncepts.
Nous utilisons cette particularit6pour les isoler.A I'initialisation du syst~me nous disposons del'ensemble des concepts dOlulds dans le tmotstrap.Dans un premier temps, dliminons les conceptscomposds de plusieurs termes, ceux-ci risqucraient debiaiser notre analyse furore, et Iravaillons avec les seulsconcepts implcs.La premiere opdration utilise ces concepts et un6chantillon de textes pour en ddduire des conceptscomposds par la collecte d'occurrences assocides `5 desexpressions.
A ce stade, aucune connaissancen'intervient, nous ne raisons qu'appliquer lepostulat pourregrouper les concepts prdsents afill d'en forlner de pluscomplexes.Aa fur et ,~ mesure de leur crdation, ces conceptssum ddcomposds en termes.
Nous utilisons uneinformation cruciale attachde `5 chaque terme : Est-il unconcept de fa~on individuelle ?Alors, les listes que nous cherchoas peuvent 61re 6tablies- les mots fortement lids :Ce sont Its couples de termes voisins qui ne sont ni runni fautrc des concepts ,5 titre individuel.- les mots de schdma :Pour accdder ,5 ce statut, un lerme dolt vdrifier plusieurscritdres :- 11 n'est pas un concept de fa~on individuel,- Ses frdquences de voisinage droit et gauche sontdu mOme ordre de grmldeur,- 11 lie souvent des ternles qui, eu.x, sont desconcepts.Quelques r~sultatsNous avons appliqud ce nouveau processus ,5 un6chantillon de 60 000 mots.L'analyse statistique 6tablit une liste de 35 mots rides :"a", "'au", "avait ' ,  "c ' ,  "ce ' ,  "cela", "d", "clans","de", "des", "done", "du' ,  "en", "est ' ,  "et", "altair","fait", "il", "je", "1", "la", "le", "its", "n' ,  "he", "on","pus", "ixmr", "qu", "que", "qui", "sur", "un' ,  "une",-y,,.Acl .
'gs DE COLING-92, NANTES, 23-28 AOOr 1992 1 2 0 1 PROC.
OF COLING-92, NANTES, AUG. 23-28, 1992Nous obtenons 110 nouveaux concepts dont voiciquelques exemples :"CAPTEURS DE DEPLACEMENT""CIRCUIT DE VIDANGE DE L'INTERCUVE""CODES DE CALCULS""CONTROLE COMMANDE""CUVE DE RETENTION"L'analyse de ces concepts pernlet :- d'unlfier les termes :"b. la", "de 1", "de la".- de qualifier ies termes earact~ristiqnes de schemas :"de la", "d ' ,  "des", "de", "du".Nous constatons que les tools de sch6rnas retrouv6spar le syst~me sont les plus productifs quant auxnouveaux concepts qu'ils sont susceptibles de d6couvrir.D'autres r6sultats seront expos6s durant laconf6rence.5 - CONCLUSION\[DAVI\]\[DEER 88\]\[DEER 901\ [ENGULe contr0le des connaissances de notre syst~meainsi que leur introduction sous forme d6clarative nousont permis d'exploiter le rdseau de concepts et de termes.Toutefois, il nous reste /t explorer de nouvellesextensions vers une plus grande structuration du r6seau :la d6finition automatique d  classes de mots.
\[FURN\]Le processus d'induction de ces classes sera has6sur rexamen des contextes droits et gauches des termeseomposant les concepts.
L'utilisation des termes dans lelangage reflStant la manipulation des objets dims lemonde physique.
Cet isomorphisme pr6suppos6 desstructures, des termes et des objets, correspond /1 lath~orie psychologique de capture des classes par \[KIMO 90\]prototypage.\[FALZ 89\]6 - B I B L I O G R A P H I E\[ANDRa\] Andreewsky A., Fluhr C., "lndexationautomatique - Construction automatique \[PITR\]des th6saurus c lass i f icat ionautomatique", .Note CEA-N- 1795\[SALT 661\[ANDRb\] Andrcewsky A., Fluhr C., "Le probl~mede l' identification automatique desconcepts ",Note CEA-N- 1816\[SALT 86\]\[ANDRc\] Andreewski A., Debili F., Fluhr C., HlalY., Nicaud L., "R6sum6 des probl~mes deI'indexation automatique t ls qu'ils sontabord6s par le groupe de recherche n \[STIL 61\]liuguistique automatique"\[BART 79\] Barthes J.P., Vayssade M., ZzmmierovskaM., "Property Driven Databases",Tokyo, 1979 \[SPIR\]\[TURT 91\]\[DACH\] Daehelet R., "Etat de l'art de la rechercheen informatique documentaire : larepr6sentation des documents et l'acc~srinfommtion", Ranoort n ?
1201.- 32 pageso Programme 8 - Communication homme-machine, INRIADavid S., Plante P., "De la n6cessit6 d'uneapprocbc morpho-syntaxique en analysede textes"Dcerwester S., Dumais S.T., Funms G.,Landaeur T.K., Harshman R., "Usinglatent semantic analysis to improve accessto textual information", :~A\[\[I~, pp : 281 -286Deerwester S., Dumais S.T., Fumas G.,Landacur T.K., Harshman R., "Indexingby latent semmuic analysis", Journal of themil.~c_~an society for information science.pp: 391 -407, n ?
41, 19909f l l |guehard C., Ma lvache  P.,"Apprentissage Naturel Automatisd",~onventiorl IA 91, pp : 145 - 163, 1991Falzon P., "Ergonomic cognitive dudialogue", Presses Universitaires deGrenoble.
Sciences et Technotooies de la~ ,  chapitre 4, 1989Fumas G.W., "lnfomlation retrieval usinga singular value decomposition model oflatent semantic structure", 2JAB_A.C,MInternational Conference on research anddevelopment in information retrieval, pp :465 - 480Kimoto H., Iwadera T., "Construction of adynamic thesaurus and its use forassociated information retrieval", 13th~ona l  Conference on research anddevelopment in reformation retrieval, pp :227 - 240, 1990Pitrat J., "Textes, ordinateurs etcompr6hension", Eyrolles, 1985Sahon G,  "Information dissemination andautomatic information systems",IEEEE, 54, 12, December, 1966Salton G., "Another look at aotomatic text-retrieval systems", Communications of theA.Cc._.M, 29 (7), pp : 648 - 656, 1986Stiles H.F., "The association factor ininformation retrieval", journal of thevol.
8, pp : 271-279, 1961Spirit, Pr6sentation + Manuel utilisateurTurtle H.R , Croft W.B, "Efficientprobalistic inference for text retrieval",IAg!A.Q\[~, p : 644AClT, s DE COLING-92, N.~/q'rEs, 23-28 AOl\[rr 1992 1 2 0 2 PRoc.
DE COLING-92, NANTI~S, AUC,.
23-28, 1992
