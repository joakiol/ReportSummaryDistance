Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 535?544,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsImproving Alignment of System Combination by UsingMulti-objective OptimizationTian Xia+, Zongcheng Ji?, Shaodan Zhai+, Yidong Chen++, Qun Liu?, Shaojun Wang+++ Xiamen University, Xiamen 361005, P.R.
China+ Wright State University, 3640 Colonel Glenn Hwy, Dayton, OH 45435, USA?
Institute of Computing Technology, Chinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{jizongcheng, liuqun}@ict.ac.cn and ydchen@xmu.edu.cn{xia.7, zhai.6, shaojun.wang}@wright.eduAbstractThis paper proposes a multi-objective opti-mization framework which supports heteroge-neous information sources to improve align-ment in machine translation system combi-nation techniques.
In this area, most oftechniques usually utilize confusion networks(CN) as their central data structure to com-pact an exponential number of an potential hy-potheses, and because better hypothesis align-ment may benefit constructing better qualityconfusion networks, it is natural to add moreuseful information to improve alignment re-sults.
However, these information may be het-erogeneous, so the widely-used Viterbi algo-rithm for searching the best alignment maynot apply here.
In the multi-objective opti-mization framework, each information sourceis viewed as an independent objective, anda new goal of improving all objectives canbe searched by mature algorithms.
The so-lutions from this framework, termed Paretooptimal solutions, are then combined to con-struct confusion networks.
Experiments ontwo Chinese-to-English translation datasetsshow significant improvements, 0.97 and 1.06BLEU points over a strong Indirected HiddenMarkov Model-based (IHMM) system, and4.75 and 3.53 points over the best single ma-chine translation systems.1 IntroductionSystem combination (SC) techniques have thepower of boosting translation quality in BLEU byseveral percent over the best among all input ma-chine translation systems (Bangalore et al 2001;Matusov et al 2006; Sim et al 2007; Rosti et al2007b; Rosti et al 2007a; Huang and Papineni,2007; He et al 2008; Rosti et al 2008; He andToutanova, 2009; Li et al 2009; Feng et al 2009;Pauls et al 2009).
A central data structure in theSC is the confusion network, and its quality greatlyaffects the final performance.
He et al(2008) pro-posed a new hypothesis alignment algorithm forconstructing high-quality confusion networks calledIndirect Hidden Markov Model (IHMM), whichdoes better in synonym matching compared withthe classic translation edit rate (TER) based algo-rithm (Rosti et al 2007b; Rosti et al 2008; Sim etal., 2007).
Now, current state-of-the-art SC systemshave been using IHMM or variants in their align-ment algorithms more or less (Li et al 2009; Fenget al 2009).Our motivation derives from an observation thatin an ideal alignment of a pair of sentences, many-to-many alignments often exist.
For instance, ?be aboutto?
has the same meaning with ?be on the pointof?.
Because Hidden Markov Model based align-ment algorithms, e.g.
IHMM for system combina-tion, HMM in GIZA++ software for statistical ma-chine translation (SMT) (Och and Ney, 2000; Koehnet al 2003), are designed for one-to-many align-ment, and running GIZA++ from two directions togain better performance turns into a standard opera-tion in SMT, therefore we are seeking a way to em-power IHMM by introducing bi-directional informa-tion.However, it appears to be intractable in an IHMMmodel to search the optimal solution by simplydefining a new goal as a product of probabilities535from two directions.
To bypass this problem, Lianget al(2006) adopts a simple and effective variationalinference algorithm.Further, different alignment algorithms capturedifferent information and linguistic phenomena fora pair of sentences, hence more information wouldbe expected to benefit the final alignment.
Liang?smethod may not be suitable for this expected out-come.We propose to adopt multi-objective optimiza-tion framework to support heterogeneous informa-tion sources which may induce difficulties in aconventional search algorithm.
In this framework,there exist a variety of matured multi-objective op-timization algorithms, e.g.
evolutionary algorithm(Deb et al 2000; Deb et al 2002), Tabu search(Hansen, 1997), ants colony (Engelbrecht, 2005),and simulated annealing (Serafini, 1994).
In thiswork, we select the multi-objective evolutionary al-gorithm because of its public open source software(http://www.iitk.ac.in/kangal/codes.shtml).
On theother hand, this framework is also totally unsuper-vised.
It prevents weights of a linearly combinedgoal from training even if all information is homoge-neous and applicable in a Viterbi search (Forney Jr,1973).
This framework views any useful informa-tion benefiting alignment as an independent objec-tive, and researchers just need to write short codesfor objective definitions.
The search algorithm seeksfor potentially better solutions which are no worsethan the current solution set.
The output from multi-objective optimization algorithms includes a set ofsolutions, called Pareto optimal solutions, each onebeing a many-to-many alignment.
We then com-bine and normalize them into a unique one-to-onealignment to perform confusion network construc-tion (Section 3.3).Our work is conducted on the classic pipelinewhich has three modules, pair-wise hypothesisalignment, confusion network construction, andtraining.
Now many work integrates neighboringmodules to avoid propagated errors to gain improvedperformance.
For example, Rosti et al(2008), andLi et al(2009) combine the first and the secondmodule, and He and Toutanova (2009) combine allmodules into one directly.
Nevertheless, the classicstructure also owns its merits.
Because of the in-dependence between modules, a system is relativelysimple to maintain, and improvements on each mod-ule might contribute to final performance additively.Based on our work, lattice-based minimum errorrate training (lattice-MERT) and minimum bayesrisk training techniques (Kumar et al 2009) couldbe adopted on the third module.
And Feng et al(2009) in the second module adopts a different datastructure called lattice which could directly use ourbetter many-to-many alignment for construction.Experiments on the Chinese-to-English task ontwo datasets use four objectives, IHMM probabil-ity (Section 3.2.1), and alignment probability fromGIZA++ (Section 3.2.2) from two directions.
Re-sults show multi-objective optimization frameworkefficiently integrates different information to gainapproximately 1 BLEU point improvement over astrong baseline.2 BackgroundWe briefly give an introduction to confusion net-works, and because the IHMM based alignment isan important objective in our multi-objective frame-work, here we also provide detailed definition of for-mulas for completeness of content.2.1 Confusion NetworkTable 1 shows hypotheses h1 and h2 are aligned toselected backbone h0.
When alignment algorithmobtains good enough results, the expected output?he prefers apples?
is included in its correspondingconfusion network in Figure 1.
This suggests de-veloping better alignment algorithm may help creat-ing high-quality confusion networks.
This also mo-tivates us to use the BLEU of oracle hypotheses toapproximately measure the quality of a set of CNs.We hereafter call it an oracle BLEU of a CN.
Seemore in Section 5.1.h0 :he feels like applesh1 :he prefer ?
applesh2 :him prefers to applesTable 1: A toy example of hypothesis alignment, whereh0 is the backbone hypothesis.
h1and h2 are aligned tothe backbone separately.
The resulting confusion net-work is in Figure 1.A confusion network G = (V,E) is a directedacyclic graph with a unique source and sink vertex,536b b b bhimhepreferspreferfeel?likelikebapplesFigure 1: A classic confusion network, and the bold paththe expected output.formally a weighted finite state automation (FSA),where V is the set of nodes andE is the set of edges.Each edge is restricted to attach to a single word aswell as an associated probability.
A special mark ?is a place-holder denoting no word here.2.2 IHMM-based AlignmentIndirected Hidden Markov Model (IHMM) wasfirstly proposed by He et.
al (2008).
Compared withTER-based alignment performing literal matching,IHMM supports synonym comparison in redefiningemission probabilities in an IHMM model.Let f I = (f1, .
.
.
fI) be a backbone hypothesis,and eJ = (e1, .
.
.
eJ) be a hypothesis aligned to thebackbone, both being English sentences in our ex-periments.
Let aJ = {a1, .
.
.
aj} be an alignment.Suppose the aj th word in f I is aligned to jth wordin eJ , and the conditional probability that the hy-pothesis is generated by the backbone, shown in theupper graph of Figure 3, is given byp(f I , eJ) =?aJJ?j=1{pt(aj |aj?1, I)po(ej |faj )}(1)The distortion probability pt(aj |aj?1, I) from po-sition aj?1 to aj , relies on jumped distance, whichis computed as follows:pt(i?
|i, I) = c(i??
i)?It=1 c(t?
i)(2)The distortion parameters c(d) are grouped into11 buckets, c(?
?4),c(?3),c(?2).
.
.c(5),c(?
6).Because all the hypotheses in system combina-tion are in the same language, the IHMM modelwould support more monotonic alignments, andnon-monotonic alignments will be penalized.c(d) = (1 + |d?
1|)?K , d = ?4 .
.
.
6 (3)where K is tuned on held-out data.Let p0 be the probability of jumping to a nullword state, which is also tuned on held-out data, andthe accurate transition probability becomes:pt(i?
|i, I) ={p0 if i?
= null(1?
p0)pt(i?|i, I) otherwise(4)The output probability po(e|f) from the stateword f to the observation word e, also called trans-lation probability, is a linear interpolation of se-mantic similarity psem(e|f) and surface similaritypsur(e|f), and ?
is the interpolation factor:po(e|f) = ?psem(e|f) + (1?
?
)psur(e|f) (5)When calculating semantic similarity psem(e|f),source sentence src is needed, and a bilingual prob-abilistic dictionary pdic(w1|w2) is necessary.psem(e|f) ?
?c?srcpdic(c|f) ?
pdic(e|c) (6)Note that psem(e|f) has been updated with differ-ent source sentences.The surface similarity psur(e|f) is measured bythe literal matching rate:psur(e, f) = exp{?
[ LMP(f, e)max(|f |, |e|) ?
1]} (7)where LMP(f, e) is the length of the longestmatched prefix, and ?
is a smoothing parameter.3 Multi-objective OptimizationMany decision making problems in the real worldconsider more than one objective.
One natural wayis to scalarize multiple objectives into one by assign-ing it with a weight vector.
This method allows asimple optimization algorithm in many cases, whilein system combination, it would cause problems.In the first module, in order to train suitableweights of objectives, extra labeled data is needed,besides that, the efficient Viterbi algorithm forsearching the optimal alignment would not work for537the alignment objectives in this work.
More, the pa-rameter training in the third module relies on theCNs constructed from the output of the first mod-ule, which increases the instability of the whole sys-tem.
Therefore, an unsupervised multi-objective al-gorithm may be a good choice allowing for morealignment information.There exist other alternative optimization algo-rithms in the multi-objective optimization frame-work, though the evolutionary algorithm is adoptedhere, we only introduce some general concepts.3.1 Pareto Optimal SolutionsA general multi-objective optimization problemconsists of a number of objectives and is associatedwith a number of constraints.
Mathematically, theproblem can be written as follows (Deb, 2001)Maximize fi(x) i = 1 .
.
.Ms.t.
gj(x) ?
0 j = 1 .
.
.
Nhk(x) = 0 k = 1 .
.
.Kwhere x denotes a potential solution, its structure re-lying on different problems, and the number of con-straints M,N,K depend on different problems.
Allthe functions fi, gj , hk map a solution x into a scalar.We will explain them in terms of system combina-tion.In this work, we refer to x = {xi,j |xi,j ?
{0, 1}}as a potential alignment of a pair of hypotheses,where xi,j is a boolean value to denote whether theith word in the first hypothesis is aligned to the jthword in the second hypothesis.
Here the definition ofx seems different from that of a in Formula 1, andthey could convert to each other.
Using a line-basedaccess style, a matrix can be unfolded as a vector.We refer to f as IHMM alignment probability (He etal., 2008) and GIZA++ alignment probability (Chenet al 2009), total four objectives from two direc-tions, and the larger the objectives, the better.
Thegjs and hks serve as the role of checking if x repre-sents a legal alignment.
For instance, the subscriptsof xi,j are not in bounds.Definition 1.
Let x, x?
be two potential align-ments.
If fi(x) ?
fi(x?)
holds for all i, we callthe alignment x dominates the alignment x?.
If there0123450 1 2 3 4 5bp3bp5bp7bp1?p2?
p6?p4X: Reversed IHMM Probability (1e-8)Y:DirectIHMMProbability(1e-8)Figure 2: Sample solutions with only two objectives.Pareto Optimal Solutions p1, p3, p5, p7.
Other pointsp2, p4, p6 are dominated by at least one point in the Paretooptimal solutions.does not exist any alignment x??
to dominate x, wecall the alignment x to be non-dominated.Definition 2.
A alignment x is said to be Paretooptimal if there is no other alignment x?
found todominate x.In Figure 2, p1 dominates p2, and p2 dominatesp4.
To summarize, a point is dominated by the oneson its upper and right side with ties.
In this example,p1, p3, p5, p7 are Pareto optimal.In some cases, Pareto optimal solutions can beused for good candidate solutions.
Consideringthe IHMM model, maximizing Y axis, the top-4best alignments are p1, p2, p3, p4.
But from theview of Pareto optimal, the top-4 alignments wouldbe p1, p3, p5, p7 without order, which considers agreater range than a single optimization model.
Inour method, we just combine these Pareto optimalsolutions equally into a unique alignment (Section3.3).Our adopted multi-objective optimization search-ing algorithm is the non-dominated sorting ge-netic algorithm II (NSGA-II) (Deb et al 2000;Deb et al 2002) with an open source software(http://www.iitk.ac.in/kangal/codes.shtml).
NSGA-II has a complexity of O(mn2), wherem is the num-ber of objectives and n is the population size in anevolutionary algorithm.3.2 Objectives in Evolutionary AlgorithmThe optimization objectives in our experiments canbe categorized as an IHMM alignment probability(He et al 2008) and GIZA++ alignment probability538b b bb b bO:S: f1 f2 f3e1 e2 e3b b bb b bS:O:e1 e2 e3f1 f2 f3BackboneBackboneFigure 3: The same alignment (f1, e1)(f1, e2)(f2, e3) intwo IHMM models.
The upper one is a typical examplein IHMM, and in the bottom one, because any word in theobservation is required not to correspond to two statuses,it has a minor trouble.
S: status sequence, O: observationsequence.
(Chen et al 2009), total four from two directions.3.2.1 IHMM ProbabilityA typical IHMM alignment is demonstratedin the upper graph of Figure 3, where abackbone is acting the role of a status se-quence.
The unnormalized conditional align-ment probability is [pt(1|null)] ?
[pt(1|1)pt(2|1)] ?[po(e1|f1)po(e2|f1)po(e3|f2)].
However, the samealignment (f1, e1)(f1, e2)(f2, e3), if we change thealignment direction, the backbone being observa-tions, would be a bit different.
We offer a minormodification to Formula 1.Look at the bottom graph of Figure 3, the obser-vation f1 has two statuses, e1 and e2 at the sametime, it becomes ambiguous to compute the tran-sitional probability between pt(3|1) and pt(3|2).This is because IHMM algorithm deals with one-to-many alignments, and MOEA permits many-to-many alignments.We hence empirically modify the IHMM modelto support many-to-many alignments.
A new statusis defined, rather than a single position pt(j|i), butas a set of positions pt({j}|{i}).
The positions inone status need not to be adjacent to each other.The redefined transitional probabilitypt({j}|{i}) =1|{j}| ?
|{i}|?i,jpt(j|i)The redefined emission probabilitypo(j|{i}) =?ipo(j|i)We need to note that there is no guarantee onthe closed property of probabilities, though theseapproximations prove to be effective in a practicalsense.
Straightforwardly, when there is only one po-sition in a new status, the expanded IHMM degener-ates to the standard IHMM.Let us return to the second IHMM ex-ample.
The new probability becomes[pt(1|null)pt(2|null)] ?
[12pt(3|1)pt(3|2) ?pt(null|3)] ?
[po(f1|e1)po(f1|e2)po(f2|e3)po(f3|null)].3.2.2 Alignment ProbabilityGIZA++ considers very different and more in-formation in alignment, we attempt to utilize them.All probabilities appearing in below formulas can belooked up in GIZA++.Given a pair of hypotheses f I = (f1, .
.
.
fI),eJ = (e1, .
.
.
eJ), and their alignment a, the align-ment probability could be calculated as followspGiza(eJ |f I ,a) =?eiT (ei|fI ,a)T (ei|fI ,a) ={n(?i|ei)?
(j,i)?a t(ei|fj)a(j|i)/?i if?i 6= 0n(0|ei)t(ei|null)a(0|i) otherwise?i = |{j|(i, j) ?
a}|where ?i is the fertility number, t(e|c) the transla-tion probability for the word pair, z(j|i) alignmentprobability to show how likely a target word at posi-tion i could be translated into a source word at posi-tion j, and n(?|e) is the fertility probability to showhow likely a given target word e is translated into ?source words.In order to increase the coverage of words, we col-lect all the hypothesis pairs in both the tuning setand the test set and feed them into GIZA++.
Thisis an off-line operation, which makes it not suitablefor an online translation system.
In some circum-stances, users submit a pile of documents in the hopeof high-quality translations, thus more useful knowl-edge sources would be helpful.
In our experiments,a pure GIZA++ based system combination does notperform as well as IHMM based, but does benefitthe final translation quality if combined in our multi-objective optimization framework.5393.3 Configuration of Evolutionary Algorithm3.3.1 EncodingGiven a sentence pair <f I , eJ>, we define a two-dimensional matrix x = {zi,j |zij ?
{0, 1}} to en-code a set of possible alignments.
Using a line-basedaccess style, the matrix could be unfolded as a vectorwith |I| ?
|J | bits of length.3.3.2 InitializationBecause in NSGA-II software the initial popu-lation are generated at random.
In order to makeNSGA-II more consistent and flexible, better initialseeds should be fed with, thus we combine an ex-isting word alignment results as input.
Here we usetogether two N-best lists generated from directionalHMM and reversed HMM respectively for initializa-tion.3.3.3 Normalization of Pareto OptimalSolutionsMulti-objective optimization algorithms do notpose weights on objectives, thus they output a setof so-called Pareto optimal solutions, each of whichis a many-to-many alignment.
We can understandthem as an N-best alignment list without explicitpreferences.
We also empirically compare it with theidea that directly cuts an N-best list from the IHMMbased alignment.We describe a two-stage strategy for normaliza-tion.
Firstly, we use a simple and effective votingstrategy to combine a set of many-to-many align-ments into a single many-to-many alignment, andSecondly we normalize it into a one-to-one align-ment for confusion network construction.
In the firststage, we count the number of word-to-word align-ments on each position pair (i, j).
If there is morethan a half number of alignments, then we output 1,otherwise 0.
In the second stage, if any word relatesto more than one word alignment, the one with thehighest posterior probability is selected (He et al2008; Feng et al 2009).
The posterior probabili-ties can be computed in a classic forward-backwardprocedure in IHMM (He et al 2008).4 Training and DecodingOur work does not change the classic pipeline, thusthe model and features are nearly identical to theones in (Rosti et al 2007b; He et al 2008), whichare modeled in a log-linear fashion in Eq.
8.
Trans-lation on a CN is just a concatenation of edges tra-versed, on which 4 categories of features are defined.1.
word posterior probabilities.
In Eq.
8,p(w|sys, span) are word confidence scores.
Ifthe word w comes from the kth hypothesis ofthesys-th system, the raw score should be 1k+1 ,and then it would be normalized by the samesys and span.
The same word coming fromdifferent systems owns a different score, sothere are sys system weights ?sys.2.
logarithm of language model score, L(h).3. number of null edge, Numnull.4.
number of words, Numw.log(h) =?span log(?sys ?sysp(w|sys, span))+ w0L(h) + w1Numnull + w2Numw(8)Decoding a confusion network is straightforward,traversing each node from left to right, and the beamsearch algorithm will retain for each node an N-best list.
The final N-best can be acquired following(Huang and Chiang, 2005).The training process follows minimum error ratetraining (MERT) described in (Och, 2003; Koehn etal., 2003).
In each iteration, the Powell algorithmwould attempt to predict the optimal parameters onthe cumulative N-best list.5 ExperimentsWe evaluate our method in two datasets in theChinese-to-English task.
In the first one, NIST MT2002 and 2005 are used for tuning and testing re-spectively, and in the second, the newswire part ofMT 2006 and 2008 are for tuning and testing.
A 5-gram language model is trained on the Xinhua por-tion of the Gigaword corpus.
We report the case-sensitive NIST-BLEU score.Four single machine translation systems partici-pating in the system combination consist of a BTG-based system using a Max-Entropy based reorderingmodel, a hierarchical phrase-based system, a Mosesdecoder and a syntax-based system.
10-best uniquehypotheses from a single system on the development540SYSTEM MT 2005 MT 2008(news)best single 0.3207 0.3016IHMM* 0.3585(+3.78%) 0.3263(+2.47%)IncIHMM 0.3639(+4.32%) 0.3320(+3.04%)GIZA++ 0.3438(+2.31%) 0.3166(+1.50%)PPBD 0.3619(+4.10%) 0.3306(+2.90%)N-best IHMM 0.3590(+3.83%) 0.3270(+2.54%)dH+rH 0.3604 0.3284dH+dT 0.3610 0.3290dH+rH+dT 0.3609 0.3289dH+rH+rT 0.3630?
(+4.27%) 0.3320?
(+3.04%)dH+rH+dT+rT 0.3682??
(+4.75%) 0.3369??
(+3.53%)Table 2: PPBD is a posterior probabilistic-based decod-ing (section 5.3).
N-best IHMM simulates the Pareto op-timal solutions in our method (section 5.3).
The last fivesystems adopt different objective combinations.
The im-provement percents in parentheses are compared to thebest single.
dH: directed IHMM, rH: reversed IHMM,dT: directed translation probability, rT: reversed transla-tion probability.
??
significance at 0.01 level, and ?
sig-nificance at 0.05 level over the IHMM model.and test sets are collected as the input of the systemcombination.Our baseline systems are described as follows.Two main baseline systems are IHMM based and in-cremental IHMM (Li et al 2009).
The first systemdiffers from our method just in hypothesis alignmentalgorithm, and the second combines the first and sec-ond module of the system combination pipeline.Because our method utilizes bidirectional infor-mation, we also provide another two alternativesystems for comparison, which are GIZA++ basedalignment and the posterior probability based align-ment (Liang et al 2006).
Finally, we also providean N-best alignment IHMM system, which com-bines an N-best alignment list to simulate the Paretooptimal solutions in our method.The method that linearly combines all objectivesis not listed as our baseline like (Duh et al 2012)does, because their algorithm finds the best weightedsolution in a fixed and small solution set, whilein our problem, the solution space is a trellis-stylestructure consisting of an exponential number of so-lutions, and no efficient algorithms apply here.The IHMM based alignment utilizes typical set-tings (He et al 2008; Feng et al 2009).
Thesmoothing factor for the surface similarity model,and ?
= 3 the controlling factor for the distor-tion model, K = 2.
The bilingual probabilisticdictionary is trained in the FBIS corpus which in-cludes about 230k parallel sentence pairs.
GIZA++based system is to run GIZA++ from two directionsto align all the hypotheses, and make the intersec-tion using grow-diag-final heuristics (Koehn et al2003).
The many-to-many alignments are normal-ized with the same method with ours.
Our systememploys NSGA-II software to realize the MOEA al-gorithm.
The main parameters, generation number,cross probability and mutation probability, and pop-ulation size, are empirically set as 100, 0.9, 0.001and 40, and we examine the influence of differencepopulations sizes in the full system combination.5.1 The Quality of Confusion NetworksThis experiment shows the relationship between hy-pothesis alignment and confusion network.
Intu-itively, we expect a better hypothesis alignmentwould reduce the error in constructing confusionnetworks, and then improve the final translationquality.We first use the alignment error rate (AER) (Ochand Ney, 2000), which is widely used to measurethe quality of hypothesis alignment.
The smaller,the better.
For convenience, we only examine exactliteral matching.
IHMM based alignment reachesaround 0.15 in AER, and our method 0.145.As the AER may not vividly reflect the relationsbetween alignment and the final BLEU of systems,and the quality of confusion network is hard to mea-sure directly, we assume that the quality of confu-sion networks could be measured by the oracle hy-potheses that could be generated from them.
We testthe BLEU of the oracle hypotheses.From this angle, we demonstrate several oracleBLEU of CNs generated from some conventionalalignment algorithms.
The results are shown in Ta-ble 3.We find the confusion network from IHMM basedalignment (He et al 2008) is better than that fromTER based alignment (Rosti et al 2007b) by about1 point in both two datasets.
These quantities agreewith the final improvements in the BLEU score in(He et al 2008).
As confusion networks fromMOEA based alignment also show superiority over541alignment MT02 MT05GIZA++ 0.5690 0.5228TER 0.5720 0.5270IHMM 0.5883 0.5382IncIHMM 0.5931 0.5453MOEA 0.6017 0.5526Table 3: Oracle BLEUs of CNs.
GIZA++: invokingGIZA++ software.
TER: minimum translation edit rate.IHMM: indirect hidden markov model.
IncIHMM: in-cremental indirect hidden markov model.
MOEA: multi-objective evolution algorithm.that from IHMM based in the oracle BLEU, we ex-pect our final translation quality would be improved.In Table 3, GIZA++ and TER perform simi-larly, because the former is more capable of tacklingmany-to-many alignments over the latter, while lat-ter based might obtain relatively more precise align-ment information.
Both of the two do not considersynonym matching compared to IHMM.Our method and IncIHMM overpass IHMM onthis metric due to different strategies.
Obtaining bet-ter hypothesis alignment or better construction ofconfusion networks benefit the quality of CNs.5.2 Different Objective CombinationsAs our framework is convenient to support differentalignment information, we test the influence of dif-ferent objective combinations to the final translationquality.
We adopt four objectives to depict the can-didate alignment, directed IHMM probability (dH),reversed IHMM probability (rH), directed alignmentprobability (dT), and reversed alignment probability(rT).
Table 2 demonstrates all the results.We can see that the IHMM based system out-performs the GIZA++ based system by about 1-1.5points in BLEU, which agrees with the difference oforacle BLEU in Table 1.
From (He et al 2008), theIHMM based system outperforms the TER based by1 point, which also agrees with our results in Table1.
Our system, using dH + rH + dT + rT, improvesBLEU score by about 1 points over the IHMM basedsystem.
This comparison verifies our assumption,improving the quality of the confusion network doesimprove system performance.The different feature combinations exhibit inter-esting results.
The system with dH + rH + dT is0.05 point better than the system with dH + rH, andthe system dH + rH + rT is 0.3 point better than sys-tem with dH + rH, so the contributions of featuredT and rT are 0.05 and 0.3 respectively.
While thetwo features are used together in the fourth system,the contribution is about 0.8 point, rather than 0.35.This phenomenon also proves the correlations be-tween different features.Our method explores a way to integrate GIZA++and IHMM, and is supportive of useful features.Compared to the classic and powerful IHMM basedsystem, we obtained an improvement of 0.97 pointson MT 05 and 1.06 points on news of MT 2008,and equivalently over the best single system by 4.75points and 3.53 points respectively.
More, comparedwith the incremental IHMM, our system also showsmoderate improvement, though not much.
We hopethese two ideas could be effectively combined in thefuture work.5.3 Comparison with Other Bi-directionalAlignment MethodsOur method introduces multiple alignment infor-mation into system combination to obtain improve-ments, thus it would be interesting to explore otheralternative methods for utilizing this information.We provide three alternative methods similar to ourmotivations, and they fall into two categories.The first category is from the angle of bi-directional alignment.
We use GiZA++ alignmentand the posterior probability decoding-based align-ment for comparison.
The basic idea for the lat-ter is setting a word-to-word alignment xi,j as 1,if its approximate posterior marginal probabilityq(xi,j , x) = pd(xi,j |x, ?d) ?
pr(xi,j |x, ?r) is greaterthan a threshold ?, where pd and pr are posteriormarginal probabilities from directed and reversedIHMM models, which could be conveniently com-puted with a forward-backward algorithm, and the ?is tuned on a validation-set optimized data.
We justlist some ?
values to examine its best performanceshown in Table 4.The second class is because our method combinesthe Pareto optimal solutions that consist of severalcandidate alignments, thus for fairness we also usea 100-best outputs from the directed IHMM modeland conduct the same normalization technique.The general results are shown in Table 2.
We can542?
MT 2005 MT 2008IHMM 0.3585 0.32630.15 0.3556 0.33910.2 0.3619 0.33060.25 0.3575 0.32780.3 0.3608 0.3259Table 4: Posterior decoding.
When threshold ?
are setto suitable values, simple bi-directional alignment couldoverpass the baseline.see that, GIZA++ leads to the worst performance,which can be explained as GIZA++ does not supportsynonym matching like IHMM.
The N-best IHMMhas a minor improvement over the IHMM method.We found differences in the N-best list are not obvi-ous enough.
In comparison, the posterior decodingmethod brings relatively significant improvementson both datasets.
However, the threshold ?
mustbe selected suitably.
Table 4 lists the ideal results,which will be hampered when tuning on a validationset.All of the three candidate methods can not conve-niently support extra alignment information, and alinear model poses restrictions on features to get anefficient decoding, the multi-objective optimizationmay be a good selection as an inference algorithm inmany circumstances.5.4 Population SizeWe test the influence of final translation quality andtime consumed by different population size.population BLEUsize MT 200520 0.359740 0.368260 0.3655Table 5: Big population size consumes more CPU time.In our experiments, we use a multi-thread technique tospeed up the alignment, and choose 40 as the parameterto leverage the time and BLEU.We expect enlarging the population size wouldimprove the translation quality, but the BLEU inpopulation size set as 60 does not overpass when setas 40.
We conjecture that, in our code, if the N-bestsize from IHMM (we set as 50-best) does not reachthe population size, we would use randomly gener-ated seeds, which may hamper the performance ofMOEA.
We also tried a larger population in MOEA,but did not receive obvious improvement on perfor-mance.We exerted a hard restriction on the genes in evo-lutionary algorithm, that is many-to-many discon-tiguous alignment is forbidden.
This trick speeds uprunning by about 20 times, and does not harm sys-tem performance.
Now our method runs about 0.9seconds to align a pair of hypotheses.
In practice,we utilize multi-thread to speed up.6 ConclusionIn this paper, we explore a multi-objective frame-work to conveniently support more useful alignmentobjectives to improve the hypothesis alignment.
Bya minor modification of the first module in theclassic pipeline, we successfully combine GIZA++and IHMM to obtain significant improvement overa powerful and state-of-the-art IHMM based sys-tem.
In comparison with another genre of improvingsystem combination by combing adjacent modulesof the pipeline, more powerful incremental IHMMhere, our system also show moderate improvement.Though, our best system may not overpass He andToutanova (2009) who combine all the modules intoa unified training procedure, we believe our methodcould boost many work on the higher modules of thepipeline to obtain a further improvement to matchtheir work.7 AcknowledgementThis research is partially supported by Air ForceOffice of Scientific Research under grant FA9550-10-1-0335, the National Science Foundation undergrant IIS RI-small 1218863 and a Google researchaward.
We thank the anonymous reviewers for theirinsightful comments.ReferencesB Bangalore, German Bordel, and Giuseppe Riccardi.2001.
Computing consensus translation from multi-ple machine translation systems.
In Automatic SpeechRecognition and Understanding.Yidong Chen, Xiaodong Shi, Changle Zhou, andQingyang Hong.
2009.
A word alignment543model based on multiobjective evolutionary algo-rithms.
Computers and Mathematics with Applica-tions, 57.Kalyanmoy Deb, Samir Agrawal, Amrit Pratap, andTanaka Meyarivan.
2000.
A fast elitist non-dominatedsorting genetic algorithm for multi-objective optimiza-tion: Nsga-ii.
Lecture notes in computer science,1917:849?858.Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, andTAMT Meyarivan.
2002.
A fast and elitist multiob-jective genetic algorithm: Nsga-ii.
Evolutionary Com-putation, IEEE Transactions on, 6(2):182?197.Kalyanmoy Deb.
2001.
Multi-objective optimization.Multi-objective optimization using evolutionary algo-rithms, pages 13?46.John DeNero, Shankar Kumar, Ciprian Chelba, and FranzOch.
2010.
Model combination for machine transla-tion.
In Proc.
of NAACL, pages 975?983.Kevin Duh, Katsuhito Sudoh, Xianchao Wu, HajimeTsukada, and Masaaki Nagata.
2012.
Learning totranslate with multiple objectives.
In Proc.
of ACL,pages 1?10.Andries P Engelbrecht.
2005.
Fundamentals of compu-tational swarm intelligence, volume 1.
Wiley Chich-ester.Yang Feng, Yang Liu, Haitao Mi, Qun Liu, and Ya-juan L?.
2009.
Lattice-based system combination forstatistical machine translation.
In Proc.
of EMNLP,EMNLP ?09.G David Forney Jr. 1973.
The viterbi algorithm.
Proc.of the IEEE, 61(3):268?278.Michael Pilegaard Hansen.
1997.
Tabu search for mul-tiobjective optimization: Mots.
In Proc.
of MultipleCriteria Decision Making, pages 574?586.Xiaodong He and Kristina Toutanova.
2009.
Joint opti-mization for machine translation system combination.In Proc.
of EMNLP.Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,and Robert Moore.
2008.
Indirect-hmm-based hy-pothesis alignment for combining outputs from ma-chine translation systems.
In Proc.
of EMNLP.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
of IWPT.Fei Huang and Kishore Papineni.
2007.
Hierarchicalsystem combination for machine translation.
In Proc.of EMNLP-CoNLL.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, et al2007.
Moses: Open source toolkit for sta-tistical machine translation.
In Proc.
of ACL: Poster,pages 177?180.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error rate train-ing and minimum bayes-risk decoding for translationhypergraphs and lattices.
In Proc.
of Joint ACL andAFNLP.Zhifei Li and Sanjeev Khudanpur.
2009.
Forest rerank-ing for machine translation with the perceptron algo-rithm.
GALE book chapter on MT From Text.Chi-Ho Li, Xiaodong He, Yupeng Liu, and Ning Xi.2009.
Incremental hmm alignment for mt system com-bination.
In Proc.
of Joint ACL and AFNLP.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proc.
of NAACL.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multiplemachine translation systems using enhanced hypothe-ses alignment.
In Proc.
of EACL.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
Proc.
of ACL-08: HLT, pages 192?199.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
pages 440?447, October.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of ACL, pages160?167.Adam Pauls, John DeNero, and Dan Klein.
2009.
Con-sensus training for consensus decoding in machinetranslation.
In Proc.
of EMNLP.Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang, Spy-ros Matsoukas, Richard Schwartz, and Bonnie Dorr.2007a.
Combining outputs from multiple machinetranslation systems.
In Proc.
of NAACL-HLT.Antti-Veikko I Rosti, Spyros Matsoukas, and RichardSchwartz.
2007b.
Improved word-level system com-bination for machine translation.
In Proc.
of ACL, vol-ume 45.Antti-Veikko I Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2008.
Incremental hypothesisalignment for building confusion networks with appli-cation to machine translation system combination.
InProc.
of WSMT.Paolo Serafini.
1994.
Simulated annealing for multi ob-jective optimization problems.
In Proc.
of MultipleCriteria Decision Making, pages 283?292.
Springer.Khe Chai Sim, William J Byrne, Mark JF Gales, HichemSahbi, and Phil C Woodland.
2007.
Consensus net-work decoding for statistical machine translation sys-tem combination.
In Proc.
of ICASSP, volume 4.Yong Zhao and Xiaodong He.
2009.
Using n-gram basedfeatures for machine translation system combination.In Proc.
of NAACL: Short Papers, pages 205?208.544
