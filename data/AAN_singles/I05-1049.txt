Relative Compositionality of Multi-wordExpressions: A Study of Verb-Noun (V-N)CollocationsSriram Venkatapathy1, and Aravind K. Joshi21 Language Technologies Research Center,International Institute of Information Technology - Hyderabad, Hyderabad, Indiasriram@research.iiit.ac.in2 Department of Computer and Information Scienceand Institute of Research in Cognitive Science,University of Pennsylvania, Philadelphia, PA, USAjoshi@linc.cis.upenn.eduAbstract.
Recognition of Multi-word Expressions (MWEs) and theirrelative compositionality are crucial to Natural Language Processing.Various statistical techniques have been proposed to recognize MWEs.In this paper, we integrate all the existing statistical features and in-vestigate a range of classifiers for their suitability for recognizing thenon-compositional Verb-Noun (V-N) collocations.
In the task of rankingthe V-N collocations based on their relative compositionality, we showthat the correlation between the ranks computed by the classifier and hu-man ranking is significantly better than the correlation between rankingof individual features and human ranking.
We also show that the prop-erties ?Distributed frequency of object?
(as defined in [27]) and ?NearestMutual Information?
(as adapted from [18]) contribute greatly to therecognition of the non-compositional MWEs of the V-N type and to theranking of the V-N collocations based on their relative compositionality.1 IntroductionThe main goals of the work presented in this paper are (1) To investigate a rangeof classifiers for their suitability in recognizing the non-compositional V-N collo-cations, and (2) To examine the relative compositionality of collocations of V-Ntype.
Measuring the relative compositionality of V-N collocations is extremelyhelpful in applications such as machine translation where the collocations thatare highly non-compositional can be handled in a special way.Multi-word expressions (MWEs) are those whose structure and meaning can-not be derived from their component words, as they occur independently.
Ex-amples include conjunctions like ?as well as?
(meaning ?including?
), idioms like Part of the work was done at Institute for Research in Cognitive Science, Universityof Pennsylvania, Philadelphia, PA 19104, USA, when he was visiting IRCS as avisiting Scholar, February to December, 2004.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
553?564, 2005.c?
Springer-Verlag Berlin Heidelberg 2005554 S. Venkatapathy and A.K.
Joshi?kick the bucket?
(meaning ?die?
), phrasal verbs like ?find out?
(meaning ?search?
)and compounds like ?village community?.
A typical natural language system as-sumes each word to be a lexical unit, but this assumption does not hold in caseof MWEs [6] [12].
They have idiosyncratic interpretations which cross wordboundaries and hence are a ?pain in the neck?
[23].
They account for a largeportion of the language used in day-to-day interactions [25] and so, handlingthem becomes an important task.A large number of MWEs have a standard syntactic structure but are non-compositional semantically.
An example of such a subset is the class of non-compositional verb-noun collocations (V-N collocations).
The class of V-N col-locations which are non-compositional is important because they are used veryfrequently.
These include verbal idioms [22], support-verb constructions [1] [2]etc.
The expression ?take place?
is a MWE whereas ?take a gift?
is not a MWE.It is well known that one cannot really make a binary distinction betweencompositional and non-compositional MWEs.
They do not fall cleanly into mu-tually exclusive classes, but populate the continuum between the two extremes[4].
So, we rate the MWEs (V-N collocations in this paper) on a scale from 1to 6 where 6 denotes a completely compositional expression, while 1 denotes acompletely opaque expression.
But, to address the problem of identification, westill need to do an approximate binary distinction.
We call the expressions witha rating of 4 to 6 compositional and the expressions with rating of 1 to 3 asnon-compositional.
(See Section 4 for further details).Various statistical measures have been suggested for identification of MWEsand ranking expressions based on their compositionality.
Some of these are Fre-quency, Mutual Information [9], Log-Likelihood [10] and Pearson?s ?2 [8].Integrating all the statistical measures should provide better evidence for rec-ognizing MWEs and ranking the expressions.
We use various Machine LearningTechniques (classifiers) to integrate these statistical features and classify the V-N collocations as MWEs or Non-MWEs.
We also use a classifier to rank the V-Ncollocations according to their compositionality.
We then compare these rankswith the ranks provided by the human judge.
A similar comparison betweenthe ranks according to Latent-Semantic Analysis (LSA) based features and theranks of human judges has been done by McCarthy, Keller and Caroll [19] forverb-particle constructions.
(See Section 3 for more details).
Some preliminarywork on recognition of V-N collocations was presented in [28].In the task of classification, we show that the technique of weighted featuresin distance-weighted nearest-neighbour algorithm performs slightly better thanother machine learning techniques.
We also find that the ?distributed frequencyof object (as defined by [27])?
and ?nearest mutual information (as adaptedfrom [18])?
are important indicators of the non-compositionality of MWEs.
Inthe task of ranking, we show that the ranks assigned by the classifier correlatedmuch better with the human judgement than the ranks assigned by individualstatistical measures.This paper is organised in the following sections (2) Basic Architecture,(3) Related work, (4) Data used for the experiments, (5) Agreement betweenRelative Compositionality of Multi-word Expressions 555the Judges, (6) Features, (7) Experiments - Classification, (8) Experiments -Ranking and (9) Conclusion.2 Basic ArchitectureRecognition of MWEs can be regarded as a classification task where every V-Ncollocation can be classified either as a MWE or as a Non-MWE.
Every V-Ncollocation is represented as a vector of features which are composed largely ofvarious statistical measures.
The values of these features for the V-N collocationsare extracted from the British National Corpus.
For example, the V-N collocation?raise an eyebrow?
can be represented as[ Frequency = 271, Mutual Information = 8.43, Log-Likelihood = 1456.29, etc.
].Now, to recognise the MWEs, the classifier has to do a binary classificationof this vector.
So, ideally, the classifier should take the above information andclassify ?raise an eyebrow?
as an MWE.
The classifier can also be used to rankthese vectors according to their relative compositionality.3 Related WorkChurch and Hanks (1989) proposed a measure of association called Mutual In-formation [9].
Mutual Information (MI) is the logarithm of the ratio betweenthe probability of the two words occurring together and the product of the prob-ability of each word occurring individually.
The higher the MI, the more likelyare the words to be associated with each other.
The usefulness of the statisticalapproach suggested by Church and Hanks [9] is evaluated for the extractionof V-N collocations from German text Corpora [7].
Several other measures likeLog-Likelihood [10], Pearson?s ?2 [8], Z-Score [8] , Cubic Association Ratio(MI3), Log-Log [17], etc., have been proposed.
These measures try to quan-tify the association of the two words but do not talk about quantifying thenon-compositionality of MWEs.
Dekang Lin proposes a way to automaticallyidentify the non-compositionality of MWEs [18].
He suggests that a possibleway to separate compositional phrases from non-compositional ones is to checkthe existence and mutual-information values of phrases obtained by replacingone of the words with a similar word.
According to Lin, a phrase is proba-bly non-compositional if such substitutions are not found in the collocationsdatabase or their mutual information values are significantly different from thatof the phrase.
Another way of determining the non-compositionality of V-N col-locations is by using ?distributed frequency of object?
(DFO) in V-N collocations[27].
The basic idea in there is that ?if an object appears only with one verb (orfew verbs) in a large corpus we expect that it has an idiomatic nature?
[27].Schone and Jurafsky [24] applied Latent-Semantic Analysis (LSA) to theanalysis of MWEs in the task of MWE discovery, by way of rescoring MWEsextracted from the corpus.
An interesting way of quantifying the relative com-positionality of a MWE is proposed by Baldwin, Bannard, Tanaka and Widdows[3].
They use latent semantic analysis (LSA) to determine the similarity between556 S. Venkatapathy and A.K.
Joshian MWE and its constituent words, and claim that higher similarity indicatesgreat decomposability.
In terms of compositionality, an expression is likely to berelatively more compositional if it is decomposable.
They evaluate their modelon English NN compounds and verb-particles, and showed that the model cor-related moderately well with the Wordnet based decomposibility theory [3].Evert and Krenn [11] compare some of the existing statistical features forthe recognition of MWEs of adjective-noun and preposition-noun-verb types.Galiano, Valdivia, Santiago and Lopez [14] use five statistical measures to clas-sify generic MWEs using the LVQ (Learning Vector Quantization) algorithm.
Incontrast, we do a more detailed and focussed study of V-N collocations and theability of various classifiers in recognizing MWEs.
We also compare the roles ofvarious features in this task.McCarthy, Keller and Caroll [19] judge compositionality according to thedegree of overlap in the set of most similar words to the verb-particle and headverb.
They showed that the correlation between their measures and the humanranking was better than the correlation between the statistical features andthe human ranking.
We have done similar experiments in this paper where wecompare the correlation value of the ranks provided by the classifier with theranks of the individual features for the V-N collocations.
We show that the ranksgiven by the classifier which integrates all the features provides a significantlybetter correlation than the individual features.4 Data Used for the ExperimentsThe data used for the experiments is British National Corpus of 81 million words.The corpus is parsed using Bikel?s parser [5] and the Verb-Object Collocationsare extracted.
There are 4,775,697 V-N of which 1.2 million were unique.
Allthe V-N collocations above the frequency of 100 (n=4405) are taken to conductthe experiments so that the evaluation of the system is feasible.
These 4405V-N collocations were searched in Wordnet, American Heritage Dictionary andSAID dictionary (LDC,2003).
Around 400 were found in at least one of the dic-tionaries.
Another 400 were extracted from the rest so that the evaluation sethas roughly equal number of compositional and non-compositional expressions.These 800 expressions were annotated with a rating from 1 to 6 by using guide-lines independently developed by the authors.
1 denotes the expressions whichare totally non-compositional while 6 denotes the expressions which are totallycompositional.
The brief explanation of the various rating are (1) No word inthe expression has any relation to the actual meaning of the expression.
Exam-ple: ?leave a mark?.
(2) Can be replaced by a single verb.
Example : ?takea look?.
(3) Although meanings of both words are involved, at least one of thewords is not used in the usual sense.
Example : ?break news?.
(4) Relativelymore compositional than (3).
Example : ?prove a point?.
(5) Relatively lesscompositional than (6).
Example : ?feel safe?.
(6) Completely compositional.Example : ?drink coffee?.
For the experiments on classification (Section 7), wecall the expressions with ratings of 4 to 6 as compositional and the expressionsRelative Compositionality of Multi-word Expressions 557with rating of 1 to 3 as non-compositional.
For the experiments on ranking theexpressions based on their relative compositionality, we use all the 6 ratings torepresent the relative compositionality of these expressions.5 Agreement Between the JudgesThe data was annotated by two fluent speakers of English.
For 765 collocationsout of 800, both the annotators gave a rating.
For the rest, atleast one of the an-notators marked the collocations as ?don?t know?.
Table 1 illustrates the detailsof the annotations provided by the two judges.Table 1.
Details of the annotations of the two annotatorsRatings 6 5 4 3 2 1 Compositional Non-Compositional(4 to 6) (1 to 3)Annotator1 141 122 127 119 161 95 390 375Annotator2 303 88 79 101 118 76 470 195From the table we see that annotator1 distributed the rating more uniformlyamong all the collocations while annotator2 observed that a significant propor-tion of the collocations were completely compositional.
To measure the agree-ment between the two annotators, we used the Kendall?s TAU (?).
?
is thecorrelation between the rankings1 of collocations given by the two annotators.W ranges between 0 (little agreement) and 1 (full agreement).
W is calculatedas below,?
=?i<j sgn(xi ?
xj)sgn(yi ?
yj)?
(T0 ?
T1)(T0 ?
T2)where T0 = n(n ?
1)/2, T1 =?ti(ti ?
1)/2, T2 =?ui(ui ?
1)/2 and where,n is the number of collocations, ti is the number of tied x values of ith group oftied x values and ui is the number of tied y values of ith group of tied y values.We obtained a ?
score of 0.61 which is highly significant.
This shows that theannotators were in a good agreement with each other in deciding the rating tobe given to the collocations.
We also compare the ranking of the two annotatorsusing Spearman?s Rank-Correlation coefficient (rs) (more details in section 8).We obtained a rs score of 0.71 indicating a good agreement between the an-notators.
A couple of examples where the annotators differed are (1) ?performa task?
was rated 3 by annotator1 while it was rated 6 by annotator2 and (2)?pay tribute?
was rated 1 by annotator1 while it was rated 4 by annotator2.The 765 samples annotated by both the annotators were then divided into atraining set and a testing set in several possible ways to cross-validate the resultsof classification and ranking.1 Computed from the ratings.558 S. Venkatapathy and A.K.
Joshi6 FeaturesEach collocation is represented by a vector whose dimensions are the statisticalfeatures obtained from the British National Corpus.
This list of features are givenin Table 2.2 While conducting the experiments, all features are scaled from 0 to1 to ensure that all features are represented uniformly.Table 2.
List of features and their top-3 example collocationsFeature Top-3 Feature Top-3take place Mutual Information shrug shoulderFrequency have effect [9] bridge gaphave time plead guiltyCubic Association take place Log-Log shake headMeasure shake head [17] commit suicide(Oakes, 1998) play role fall asleepLog-Likelihood take place Pearson?s ?2 shake head[10] shake head [8] commit suicideplay role fall asleepT-Score take place Z-Score shake head[9] have effect [26] commit suicideshake head fall asleep?-coefficient bridge gap Distributed come trueshrug shoulder freq.
of object become difficultpress button (DFO) make sure[27]Nearest MI Collocations Whether object (Binary feature)(NMI) with no can occur[18] neigh.
MI as a verbWhether object (Binary feature)is a nomin.of some verb7 Experiments - ClassificationThe evaluation data (765 vectors) is divided randomly into training and testingvectors in 10 ways for cross-validation.
The training data consists of 90% of 786vectors and the testing data consists of the remaining.We used various Machine Learning techinques to classify the V-N colloca-tions into MWEs and non-MWEs.
For every classifier, we calculated the averageaccuracy of all the test sets of each of the annotators.
We then compare the aver-age accuracies of all the classifiers.
We found that the classifier that we used, thetechnique of weighted features in distance-weighted nearest-algorithm, performssomewhat better than other machine learning techniques.The following are brief descriptions of the classifiers that we used in thispaper.2 The formulas of features are not given due to lack of space.Relative Compositionality of Multi-word Expressions 5597.1 Nearest-Neighbour AlgorithmThis is an instance-based learning technique where the test vector is classifiedbased on its nearest vectors in the training data.
The simple distance betweentwo vectors xi and xj is defined as d(xi,xj), whered(xi, xj) =???
?n?r=1(ar(xi) ?
ar(xj))2.Here, x is an instance of a vector and ar(x) is the value of the rth feature.One can use K neighbours to judge the class of the test vector.
The testvector is assigned the class of maximum number of neighbours.
This can befurthur modified by calculating the inverse weighted distance between the testvector and the neighbouring training vectors in each of the classes.
The testvector is then assigned the class which has the higher inverse-weighted distance.One can also use all the training vectors and the weighted-distance principle toclassify the test vector.The average classification accuracy of each of the above methods on the testsets of each of the annotators is shown in Table 3.Table 3.
Average accuracies of MWE recognition using simple nearest-neighbouralgorithms and weighted distance nearest neighbour algorithmsSimple K-Nearest neighbour Weighted-distance Nearest neighbourType K=1 K=2 K=3 K=1 K=2 K=3 K=AllAnnot.1 62.35 61.31 62.48 62.35 62.35 62.61 66.66Annot.2 57.64 54.10 60.89 57.64 57.64 60.37 63.527.2 SVM-Based ClassifiersSVMs [15] have been very successful in attaining high accuracy for variousmachine-learning tasks.
Unlike the error-driven algorithms (Perceptron etc.
),SVM searches for the two distinct classes and maximizes the margin betweentwo classes.
Data of higher dimension can also be classified using the appropriateKernel.
We used Linear and Polynomial Kernel (degree=2) to test the evaluationdata.
We also used the radial-basis network in SVMs to compare the resultsbecause of their proximity to the nearest-neigbour algorithms.Table 4.
Average accuracies of MWE recognition using SVMs (Linear, Polynomialand Radial Basis Function Kernel)Linear Ker.
Polynomial Ker.
Radial Basis networksParameters ?
= 0.5 ?
= 1.0 ?
= 1.5 ?
= 2.0Annot.1 65.89 65.75 67.06 66.66 66.93 67.06Annot.2 62.61 65.09 64.17 63.51 62.99 62.99560 S. Venkatapathy and A.K.
JoshiThe average classification accuracy of each of the above methods on the testsets of each of the annotators is shown in Table 4.7.3 Weighted Features in Distance-Weighted Nearest-NeighbourAlgorithmAmong all the features used, only a few might be very relevant to recognizingthe non-compositionality of the MWE.
As a result, the distance metric usedby the nearest-neighbour algorithm which depends on all the features mightbe misleading.
The distance between the neighbour will be dominated by largenumber of irrelevant features.A way of overcoming this problem is to weight each feature differently whencalculating the distance between the two instances.
This also gives us an insightinto which features are mainly responsible for recognizing the non-compositional-ity of MWEs.
The jth feature can be multiplied by the weight zj , where the valuesof z1...zn are chosen to minimize the true classification error of the learningalgorithm [20].
The distance using these weights is represented asd(xi, xj) =???
?n?r=1(zr ?
(ar(xi) ?
ar(xj)))2,where zr is the weight of the rth feature.The values of z1...zn can be determined by cross-validation of the trainingdata.
We use leave-one-out cross-validation [21], in which the set of m trainingvectors are repeatedly divided into a training set of m-1 and a test set of 1,in all possible ways.
So, each vector in the training data is classified using theremaining vectors.
The classification accuracy is defined asClacc = 100 ?
(m?1classify(i)/m)where classify(i)=1, if the ith training example is classified correctly using thedistance-weighted nearest neighbour algorithm, otherwise classify(i)=0.Now, we try to maximize the classification accuracy in the following way,?
In every iteration, vary the weights of the features one by one.?
Choose the feature and its weight which brings the maximum increase in thevalue of Clacc.
One can also choose the feature and its weight such that itbrings the minimum increase in the value of Clacc.?
Update the weight of this particular feature and go for the next iteration.?
If there is no increase in classification accuracy, stop.When the weights are updated such that there is maximum increase in classi-fication accuracy in every step, the average accuracies are 66.92% and 64.30%on the test sets of the two annotators respectively.
But when the weights areupdated such there is a minimum increase in classification accuracy at everyRelative Compositionality of Multi-word Expressions 561Table 5.
The top three features according to the average weight when there is maxi-mum increase in Clacc at every stepAnnotator1 Weight Annotator2 WeightDFO 1.09 MI 1.17T-Score 1.0 T-Score 1.1Z-Score 1.0 ?-coefficient 1.0Table 6.
The top three features according to the average weight calculated when thereis minimum increase in Clacc at every stepAnnot.1 Weight Annot.2 WeightDFO 1.07 MI 2.06NMI 1.02 T-Score 1.0Log-Like.
0.97 ?-coefficient 1.0step, the average accuracies are 66.13% and 64.04% on the test sets of thetwo annotators respectively, which are slightly better than that obtained by theother Machine Learning Techniques.In the above two methods (Updating weights such that there is maximum orminimum increase in classification accuracy), we add the weights of the featuresof each of the evaluation sets.
According to the average weights, the top threefeatures (having high average weight) are shown in Tables 5 and 6.In both the above cases, we find that the properties ?Mutual-Information?and the compositionality oriented feature ?Distributed Frequency of an Object?performed significantly better than the other features.8 Experiments - RankingAll the statistical measures show that the expressions ranked higher accordingto their decreasing values are more likely to be non-compositional.
We comparethese ranks with the average of the ranks given by the annotator (obtained fromhis rating).
To compare, we use Spearman Rank-Order Correlation Coefficient(rs), defined asrs =(Ri ?
R?
)(Si ?
S?)??
(Ri ?
R?)2?
(Si ?
S?
)2where Ri is the rank of ith x value, Si is the rank of ith y value, R?
is the meanof the Ri values and S?
is the mean of Si values.We use an SVM-based ranking system [16] for our training.
Here, we use10% of the 765 vectors for training and the remaining for testing.
The SVM-based ranking system builds a preference matrix of the training vectors to learn.It then ranks the test vectors.
The ranking system takes a lot of time to trainitself, and hence, we decided to use only a small proportion of the evaluation setfor training.562 S. Venkatapathy and A.K.
JoshiTable 7.
The correlation values of the ranking of individual features and the rankingof classifier with the ranking of human judgementsMI -0.125 Z-Score -0.059MI3 0.001 ?-coeff -0.102Log-Log -0.086 DFO -0.113Log-Likelihood 0.005 NMI -0.167?2 -0.056 Class.
0.388T-Score 0.045We also compare our ranks (the average of the ranks suggested by the clas-sifier) with the gold standard using the Spearman Rank-Order Correlation Co-efficient.
The results are shown in Table 7.In Table 7, we observe that the correlation between the ranks computed bythe classifier and human ranking is better than the correlation between rankingof individual statistical features and human ranking.We observe that among all the statistical features the ranks based on theproperties ?Mutual Information?, ?Distributed Frequency of an Object?
[27] and?Nearest mutual information?
[18] correlated better with the ranks providedby the annotator.
This is in accordance with the observation we made whiledescribing the classification experiments, where we observed that the proper-ties ?Distributed Frequency of an Object?
and ?Mutual Information?
contributedmuch to the classification of the expressions.
When we compare the correlationvalues of MI, Log-likelihood and ?2, we see that the Mutual-Information valuescorrelated better.
This result is similar to the observation made by McCarthy,Keller and Caroll [19] for phrasal verbs.9 ConclusionIn this paper, we integrated the statistical features using various classifiers andinvestigated their suitability for recognising non-compositional MWEs of the V-N type.
We also used a classifier to rank the V-N collocations according to theirrelative compositionality.
This type of MWEs constitutes a very large percent-age of all MWEs and are crucial for NLP applications, especially for MachineTranslation.
Our main results are as follows.?
The technique of weighted features in distance-weighted nearest neighbouralgorithm performs better than other Machine Learning Techniques in thetask of recognition of MWEs of V-N type.?
We show that the correlation between the ranks computed by the classi-fier and human ranking is significantly better than the correlation betweenranking of individual features and human ranking.?
The properties ?Distributed frequency of object?
and ?Nearest MI?
contributegreatly to the recognition of the non-compositional MWEs of the V-N typeand to the ranking of the V-N collocations based on their relative composi-tionality.Relative Compositionality of Multi-word Expressions 563Our future work will consist of the following tasks?
Evaluate the effectiveness of the techniques developed in this paper for ap-plications like Machine Translation.?
Improve our annotation guidelines and create more annotated data.?
Extend our approach to other types of MWEs.AcknowledgementsWe want to thank Libin Shen and Nikhil Dinesh for their help in clarifyingvarious aspects of Machine Learning Techniques.
We would like to thank Roder-ick Saxey and Pranesh Bhargava for annotating the data and Mark Mandel forconsiderable editorial help.References1.
Abeille, Anne .
Light verb constuctions and extraction out of NP in a tree adjoininggrammar.
Papers of the 24th Regional Meeting of the Chicago Linguistics Society.(1988)2.
Akimoto, Monoji .
Papers of the 24th Regional Meeting of the Chicago LinguisticsSociety.
Shinozaki Shorin .
(1989)3.
Baldwin, Timothy and Bannard, Colin and Tanaka, Takaaki and Widdows, Do-minic .
An Empirical Model of Multiword Expression .
Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analysis, Acquisition and Treatment.(2003)4.
Bannard, Colin and Baldwin, Timothy and Lascarides, Alex .
A Statistical Ap-proach to the Semantics of Verb-Particles .
Proceedings of the ACL-2003 Workshopon Multiword Expressions: Analysis, Acquisition and Treatment.
(2003)5.
Bikel, Daniel M. .
A Distributional Analysis of a Lexicalized Statistical ParsingModel .
Proceedings of EMNLP .
(2004)6.
Becker, Joseph D. .
The Phrasal Lexicon .
Theoritical Issues of NLP, Workshop inCL, Linguistics, Psychology and AI, Cambridge, MA.
(1975)7.
Breidt, Elisabeth .
Extraction of V-N-Collocations from Text Corpora: A Feasibil-ity Study for German .
CoRR-1996 .
(1995)8.
Church, K. and Gale, W. and Hanks, P. and Hindle, D. .
Parsing, word associationsand typical predicate-argument relations .
Current Issues in Parsing Technology.Kluwer Academic, Dordrecht, Netherlands, 1991 .
(1991)9.
Church, K. and Patrick Hanks .
Word Association Norms, Mutual Information,and Lexicography .
Proceedings of the 27th.
Annual Meeting of the Associationfor Computational Linguistics, 1990 .
(1989)10.
Dunning, Ted .
Accurate Methods for the Statistics of Surprise and Coincidence .Computational Linguistics - 1993 .
(1993)11.
Stefan Evert and Brigitte Krenn .
Methods for the Qualitative Evaluation of LexicalAssociation Measures .
Proceedings of the ACL - 2001 .
(2001)12.
Charles Fillmore .
An extremist approach to multi-word expressions .
A talk givenat IRCS, University of Pennsylvania, 2003.
(2003)13.
Fontenelle and Bruls, Th.
W. and Thomas, L. and Vanallemeersch, T. and Jansen,J.
.
Survey of collocation extraction tools .
Deliverable D-1a, MLAP-Project 93-19DECIDE, University of Liege, Belgium.
(1994)564 S. Venkatapathy and A.K.
Joshi14.
Diaz-Galiano, M.C.
and Martin-Valdivia, M.T.
and Martinez-Santiago, F. andUrena-Lopez, L. A. .
Multi-word Expressions Recognition with the LVQ Algorithm.Proceedings of Methodologies and Evaluation of Multiword Unit in Real-world Ap-plications, LREC, 2004 .
(2004)15.
Joachims, T. .
Making large-Scale SVM Learning Practical .
Advances in KernelMethods - Support Vector Learning .
(1999)16.
Joachims, T. .
Optimizing Search Engines Using Clickthrough Data.
Advancesin Kernel Methods - Support Vector Learning edings of the ACM Conference onKnowledge Discovery and Data Mining (KDD), ACM, 2002.
(2002)17.
Kilgariff, A. and Rosenzweig, J. .
Framework and Results for English Senseval .Computers and the Humanities, 2000 .
(2000)18.
Dekang Lin .
Automatic Identification of non-compositonal phrases.
Proceedingsof ACL- 99, College Park, USA .
(1999)19.
McCarthy, D. and Keller, B. and Carroll, J. .
Detecting a Continuum of Composi-tionality in Phrasal Verbs .
Proceedings of the ACL-2003 Workshop on Multi-wordExpressions: Analysis, Acquisition and Treatment, 2003.
(2003)20.
Mitchell, T. Instance-Based Learning .
Machine Learning, McGraw-Hill Series inComputer Science, 1997 .
(1997)21.
Moore, A. W. and Lee, M.S.
.
Proceedings of the 11 International Conference onMachine Learning, 1994.
(1994)22.
Nunberg, G. and Sag, I.
A. and Wasow, T. .
Idioms .
Language, 1994 .
(1994)23.
Sag, I.
A. and Baldwin, Timothy and Bond, Francis and Copestake, Ann andFlickinger, Dan.
.
Multi-word expressions: a pain in the neck for nlp .
Proceedingsof CICLing , 2002 .
(2002)24.
Schone, Patrick and Jurafsky, Dan.
Is Knowledge-Free Induction of Multiword UnitDictionary Headwords a Solved Problem?
.
Proceedings of EMNLP , 2001 .
(2001)25.
Schuler, William and Joshi, Aravind K. Relevance of tree rewriting systems formulti-word expressions.
To be published.
(2005)26.
Smadja, F. .
Retrieving Collocations from Text : Xtract .
Computational Linguis-tics - 1993 .
(1993)27.
Tapanainen, Pasi and Piitulaine, Jussi and Jarvinen, Timo Idiomatic object usageand support verbs .
36th Annual Meeting of the Association for ComputationalLinguistics .
(1998)28.
Venkatapathy, Sriram and Joshi, Aravind K. Recognition of Multi-word Expres-sions: A Study of Verb-Noun (V-N) Collocations.
Proceedings of the InternationalConference on Natural Language Processing, 2004.
(2004)
