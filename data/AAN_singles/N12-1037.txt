2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 357?361,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsCorrection Detection and Error Type Selection as an ESL Educational AidBen SwansonBrown Universitychonger@cs.brown.eduElif YamangilHarvard Universityelif@eecs.harvard.eduAbstractWe present a classifier that discriminates be-tween types of corrections made by teachersof English in student essays.
We define a setof linguistically motivated feature templatesfor a log-linear classification model, train thisclassifier on sentence pairs extracted fromthe Cambridge Learner Corpus, and achieve89% accuracy improving upon a 33% base-line.
Furthermore, we incorporate our classi-fier into a novel application that takes as inputa set of corrected essays that have been sen-tence aligned with their originals and outputsthe individual corrections classified by errortype.
We report the F-Score of our implemen-tation on this task.1 IntroductionIn a typical foreign language education classroomsetting, teachers are presented with student essaysthat are often fraught with errors.
These errors canbe grammatical, semantic, stylistic, simple spellingerrors, etc.
One task of the teacher is to isolate theseerrors and provide feedback to the student with cor-rections.
In this body of work, we address the pos-sibility of augmenting this process with NLP toolsand techniques, in the spirit of Computer AssistedLanguage Learning (CALL).We propose a step-wise approach in which ateacher first corrects an essay and then a computerprogram aligns their output with the original text andseparates and classifies independent edits.
With theprogram?s analysis the teacher would be providedaccurate information that could be used in effectivelesson planning tailored to the students?
strengthsand weaknesses.This suggests a novel NLP task with two compo-nents: The first isolates individual corrections madeby the teacher, and the second classifies these cor-rections into error types that the teacher would finduseful.
A suitable corpus for developing this pro-gram is the Cambridge Learner Corpus (CLC) (Yan-nakoudakis et al, 2011).
The CLC contains approxi-mately 1200 essays with error corrections annotatedin XML within sentences.
Furthermore, these cor-rections are tagged with linguistically motivated er-ror type codes.To the best of our knowledge our proposed taskis unexplored in previous work.
However, there isa significant amount of related work in automatedgrammatical error correction (Fitzgerald et al, 2009;Gamon, 2011; West et al, 2011).
The HelpingOur Own (HOO) shared task (Dale and Kilgarriff,2010) also explores this issue, with Rozovskaya etal.
(2011) as the best performing system to date.While often addressing the problem of error typeselection directly, previous work has dealt with themore obviously useful task of end to end error detec-tion and correction.
As such, their classification sys-tems are crippled by poor recall of errors as well asthe lack of information from the corrected sentenceand yield very low accuracies for error detection andtype selection, e.g.
Gamon (2011).Our task is fundamentally different as we assumethe presence of both the original and corrected text.While the utility of such a system is not as obvi-ous as full error correction, we note two possibleapplications of our technique.
The first, mentioned357above, is as an analytical tool for language teach-ers.
The second is as a complementary tool for au-tomated error correction systems themselves.
Justas tools such as BLAST (Stymne, 2011) are usefulin the development of machine translation systems,our system can produce accurate summaries of thecorrections made by automated systems even if thesystems themselves do not involve such fine grainederror type analysis.In the following, we describe our experimentalmethodology (Section 2) and then discuss the fea-ture set we employ for classification (Section 3) andits performance.
Next, we outline our application(Section 4), its heuristic correction detection strat-egy and empirical evaluation.
We finish by dis-cussing the implications for real world systems (Sec-tion 5) and avenues for improvement.2 MethodologySentences in the CLC contain one or more error cor-rections, each of which is labeled with one of 75error types (Nicholls, 2003).
Error types includecountability errors, verb tense errors, word order er-rors, etc.
and are often predicated on the part ofspeech involved.
For example, the category AG(agreement) is augmented to form AGN (agreementof a noun) to tag an error such as ?here are someof my opinion?.
For ease of analysis and due tothe high accuracy of state-of-the-art POS tagging,in addition to the full 75 class problem we alsoperform experiments using a compressed set of 15classes.
This compressed set removes the part ofspeech components of the error types as shown inFigure 1.We create a dataset of corrections from the CLCby extracting sentence pairs (x, y) where x is theoriginal (student?s) sentence and y is its correctedform by the teacher.
We create multiple instancesout of sentence pairs that contain multiple correc-tions.
For example, consider the sentence ?With thisletter I would ask you if you wuld change it?.
Thisconsists of two errors: ?ask?
should be replaced with?like to ask?
and ?wuld?
is misspelled.
These aremarked separately in the CLC, and imply the cor-rected sentence ?With this letter I would like to askyou if you would change it?.
Here we extract twoinstances consisting of ?With this letter I would askyou if you would change it?
and ?With this letter Iwould like to ask if you wuld change it?, each pairedwith the fully corrected sentence.
As each correc-tion in the CLC is tagged with an error type t, wethen form a dataset of triples (x, y, t).
This yields45080 such instances.
We use these data in cross-validation experiments with the feature based Max-Ent classifier in the Mallet (McCallum, 2002) soft-ware package.3 Feature SetWe use the minimum unweighted edit distance pathbetween x and y as a source of features.
The edit dis-tance operations that compose the path are Delete,Insert, Substitute, and Equal.
To illustrate, the op-erations we would get from the sentences abovewould be (Insert, ?like?
), (Insert, ?to?
), (Substitute,?wuld?, ?would?
), and (Equal, w, w) for all otherwords w.Our feature set consists of three main categoriesand a global category (See Figure 2).
For each editdistance operation other than Equal we use an indi-cator feature, as well as word+operation indicators,for example ?the word w was inserted?
or ?the wordw1 was substituted with w2?.
The POS Context fea-tures encode the part of speech context of the edit,recording the parts of speech immediately preced-ing and following the edit in the corrected sentence.For all POS based features we use only tags from thecorrected sentence y, as our tags are obtained auto-matically.For a substitution of w2 for w1 we use severaltargeted features.
Many of these are self explana-tory and can be calculated easily without outside li-braries.
The In Dictionary?
feature is indexed bytwo binary values corresponding to the presence ofthe words in the WordNet dictionary.
For the SameStem?
feature we use the stemmer provided in thefreely downloadable JWI (Java Wordnet Interface)library.
If the two words have the same stem thenwe also trigger the Suffixes feature, which is in-dexed by the two suffix strings after the stem hasbeen removed.
For global features, we record thetotal number of non-Equal edits as well as a featurewhich fires if one sentence is a word-reordering ofthe other.358Description (Code) Sample and Correction Total # % AccuracyUnnecessary (U)July is the period of time that suits me best5237 94.0July is the time that suits me bestIncorrect verb tense (TV)She gave me autographs and talk really nicely.2752 85.2She gave me autographs and talked really nicely.Countability error (C)Please help them put away their stuffs.273 65.2Please help them put away their stuff.Incorrect word order (W)I would like to know what kind of clothes should I bring.1410 76.0I would like to know what kind of clothes I should bring.Incorrect negative (X)We recommend you not to go with your friends.124 18.5We recommend you don?t go with your friends.Spelling error (S)Our music lessons are speccial.4429 90.0Our music lessons are special.Wrong form used (F)In spite of think I did well, I had to reapply.2480 82.0In spite of thinking I did well, I had to reapply.Agreement error (AG)I would like to take some picture of beautiful scenery.1743 77.9I would like to take some pictures of beautiful scenery.Replace (R)The idea about going to Maine is common.14290 94.6The idea of going to Maine is common.Missing (M)Sometimes you surprised when you check the balance.9470 97.6Sometimes you are surprised when you check the balance.Incorrect argument structure (AS)How much do I have to bring the money?191 19.4How much money do I have to bring?Wrong Derivation (D)The arrive of every student is a new chance.1643 58.6The arrival of every student is a new chance.Wrong inflection (I)I enjoyded it a lot.590 58.6I enjoyed it a lot.Inappropriate register (L)The girls?d rather play table tennis or badminton.135 23.0The girls would rather play table tennis or badminton.Idiomatic error (ID)The level of life in the USA is similar to the UK.313 15.7The cost of living in the USA is similar to the UK.Figure 1: Error types in the collapsed 15 class set.3.1 EvaluationWe perform five-fold cross-validation and achievea classification accuracy of 88.9% for the 15 classproblem and 83.8% for the full 75 class problem.The accuracies of the most common class base-lines are 33.3% and 7.8% respectively.
The mostcommon confusion in the 15 class case is betweenD (Derivation), R (Replacement) and S (Spelling).These are mainly due to context-sensitive spellingcorrections falling into the Replace category or noisein the mark-up of derivation errors.
For the 75 classcase the most common confusion is between agree-ment of noun (AGN) and form of noun (FN).
This isunsurprising as we do not incorporate long distancefeatures which would encode agreement.To check against over-fitting we performed an ex-periment where we take away the strongly lexical-ized features (such as ?word w is inserted?)
andobserved a reduction from 88.9% to 82.4% for 15class classification accuracy.
The lack of a dramaticreduction demonstrates the generalization power ofour feature templates.4 An Educational ApplicationAs mentioned earlier, we incorporate our classifierin an educational software tool.
The input to thistool is a group of aligned sentence pairs from orig-inal and teacher edited versions of a set of essays.This tool has two components devoted to (1) isola-tion of individual corrections in a sentence pair, and(2) classification of these corrections.
This softwarecould be easily integrated in real world curriculumas it is natural for the teacher to produce correctedversions of student essays without stopping to labeland analyze distribution of correction types.We devise a family of heuristic strategies toseparate independent corrections from one another.Heuristic hi allows at most i consecutive Equal editdistance operations in a single correction.
This im-plies that hn+1 would tend to merge more non-Equal edits than hn.
We experimented with i ?
{0, 1, 2, 3, 4}.
For comparison we also implemented359?
Insert?
Insert?
Insert(w)?
POS Context?
Delete?
Delete?
Delete(w)?
POS Context?
Substitution?
Substitution?
Substitution(w1,w2)?
Character Edit Distance?
Common Prefix Length?
In Dictionary??
Previous Word?
POS of Substitution?
Same Stem??
Suffixes?
Global?
Same Words??
Number Of EditsFigure 2: List of features used in our classifier.a heuristic h?
that treats every non-Equal edit asan individual correction.
This is different than h0,which would merge edits that do not have an in-tervening Equal operation.
F-scores (using 5 foldcross-validation) obtained by different heuristics arereported in Figure 3 for the 15 and 75 class prob-lems.
For these F-scores we attempt to predict boththe boundaries and the labels of the corrections.
Theunlabeled F-score (shown as a line) evaluates theheuristic itself and provides an upper bound for thelabeled F-score of the overall application.
We seethat the best upper bound and F-scores are achievedwith heuristic h0 which merges consecutive non-Equal edits.5 Future WorkThere are several directions in which this work couldbe extended.
The most obvious is to replace thecorrection detection heuristic with a more robust al-gorithm.
Our log-linear classifier is perhaps bettersuited for this task than other discriminative clas-sifiers as it can be extended in a larger frameworkwhich maximizes the joint probability of all correc-tions.
Our work shows that h0 will provide a strongbaseline for such experiments.	   	Figure 3: Application F-score against different correctiondetection strategies.
The left and right bars show the 15and 75 class cases respectively.
The line shows the unla-beled F-score upper bound.While our classification accuracies are quite good,error analysis reveals that we lack the ability tocapture long range lexical dependencies necessaryto recognize many agreement errors.
Incorporatingsuch syntactic information through the use of syn-chronous grammars such as those used by Yamangiland Shieber (2010) would likely lead to improvedperformance.
Furthermore, while in this work wefocus on the ESL motivation, our system could alsobe used to aid development of automated correc-tion systems, as was suggested by BLAST (Stymne,2011) for machine translation.Finally, there would be much to be gained by test-ing our application in real classroom settings.
Ev-ery day, teachers of English correct essays and couldpossibly provide us with feedback.
Our main con-cern from such testing would be the determinationof a label set which is appropriate for the teachers?concerns.
We expect that the 15 class case is toocoarse and the 75 class case too fine grained to pro-vide an effective analysis.ReferencesRobert Dale and Adam Kilgarriff.
2010.
Helping ourown: text massaging for computational linguistics asa new shared task.
In Proceedings of the 6th Inter-national Natural Language Generation Conference,360INLG ?10, pages 263?267, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Erin Fitzgerald, Frederick Jelinek, and Keith Hall.
2009.Integrating sentence- and word-level error identifica-tion for disfluency correction.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing: Volume 2 - Volume 2, EMNLP?09, pages 765?774, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Michael Gamon.
2011.
High-order sequence modelingfor language learner error detection.
In Proceedingsof the 6th Workshop on Innovative Use of NLP forBuilding Educational Applications, IUNLPBEA ?11,pages 180?189, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://www.cs.umass.edu/ mccallum/mallet.D.
Nicholls.
2003.
The cambridge learner corpus: Errorcoding and analysis for lexicography and elt.
In Pro-ceedings of the Corpus Linguistics 2003 conference,pages 572?581.A.
Rozovskaya, M. Sammons, J. Gioja, and D. Roth.2011.
University of illinois system in hoo text cor-rection shared task.Sara Stymne.
2011.
Blast: A tool for error analysis ofmachine translation output.
In ACL (System Demon-strations), pages 56?61.Randy West, Y. Albert Park, and Roger Levy.
2011.Bilingual random walk models for automated gram-mar correction of esl author-produced text.
In Pro-ceedings of the 6th Workshop on Innovative Use ofNLP for Building Educational Applications, IUNLP-BEA ?11, pages 170?179, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Elif Yamangil and Stuart M. Shieber.
2010.
Bayesiansynchronous tree-substitution grammar induction andits application to sentence compression.
In ACL, pages937?947.Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.2011.
A new dataset and method for automaticallygrading esol texts.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1, HLT?11, pages 180?189, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.361
