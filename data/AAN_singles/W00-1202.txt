Sense-Tagging Chinese CorpusHsin-Hsi ChenDepartment ofComputer Science andInformation EngineeringNatioual Taiwan UniversityTaipei, TAIWANhh_chen@csie.ntu .edu.twClii-Ching LinDepartment ofComputer Science andInformation EngineeringNational Taiwan UniversityTaipei, TAIWANcclin@nlg2.csie.ntu.edu.twAbstractContextual information and the mappingfrom WordNet synsets to Cilin sense tagsdeal with word sense disambiguation.
Theaverage performance is 63.36% when smallcategories are used, and 1, 2 and 3candidates are proposed for low, middle andhigh ambiguous words.
The performanceof tagging unknown words is 34.35%, whichis much better than that of baseline mode.The sense tagger achieves the performanceof 76.04%, when unambiguous, ambiguous,and unknown words are tagged.1 IntroductionTagging task, which adds lexical, syntactic orsemantic information to raw text, makesmaterials more valuable.
The researches onpart of speech (POS) tagging have been a longhistory, and achieve very good results.
ManyPOS-tagged corpora are available.
Theaccuracy for POS-tagging is in the range of 95%to 97% 1 .
In contrast, although the researcheson word sense disambiguation (WSD) are alsovery early (Kelly and Stone, 1975), large-scalesense-tagged corpus is relatively few.
InEnglish, only some sense-tagged corpora such asHECTOR (Atkins, 1993), DSO (Ng and Lee,1996), SEMCOR (Fellbaum, 1997), andSENSEVAL (Kilgarriff, 1998) are available.For evaluating word sense disarnbiguationsystems, the first SENSEVAL (Kilgarriff andRosenzweig, 2000) reports that the performancefor a fine-grained word sense disambiguationtask is at around 75 %.1 The pelrforlnancg includes tagging wnzmbiguouswords.
Marslmll (1987) reported that theperformance of CLAWS tagger is 94%.Approximately 65% of words were taggednnambiguously, and the disambigualion programachieved better than 80% success on the ambiguouswords.Tagging accuracy depends on several issues(Manning and Schutze, 1999), e.g., the amountof training data, the granularity of the tagging set,the occurrences of unknown words, and so on.Three approaches have been proposed for WSD,including dictionary/thesaurus-based pproach,supervised learning, and unsupervised learning.The major differences are what kinds ofresources are used, i.e., dictionary versus textcorpus, and sense-tagged corpus versusuntagged eorpns.
A good survey refers to thepaper Ode and Veronis, 1998).
Compared withEnglish, Chinese does not have large-scalesense-tagged corpus.
The widely availablecorpus is Academic Sinica Balanced Corpusabbreviated as ASBC hereafter (I-Iuang andChen, 1995), which is a POS-tagged corpus.Thus, a computer-aided tool to sense-tagChinese corpus is indispensable.This paper presents a sense tagger forMandarin Chinese.
It is organized as follows.Section 2 discusses the degree of polysemy inMandarin Chinese from several viewpoints.Section 3 presents WSD algorithms for taggingambiguous words and unknown words.Section 4 shows our experimental results.Finally, Section 5 concludes the remarks.2 Degree of Polysemy in Mandarin ChineseThe degree of polysemy is defined as theaverage number of senses of words.
We adopttagging set from tong2yi4ei2ci21in2 (~ ~ ~q ~'\]~hk) abbreviated as Cilin (Mei, et al, 1982).
Itis composed of 12 large categories, 94 middlecategories, and 1,428 small categories.Small categories (more fine granularity) areused to compute the distribution of word senses.Besides Cilin, ASBC is employed to countfrequency of a word.
Total 28,321 word typesappear both in Cilin and in ASBC corpus.Here a word type corresponds toa dictionaryTable 1.
The Distribution of Word SensesLow AmbiguityDegree #Word Types2 4261 (71.95%)3 948 (16.01%)4 - 344 (5.81%)Sum 5553 (93.77%) Sum 330 (5.57%)Total Word Types 5922Middle Ambiguity High AmbiguityDegree #Word Types Degree #Word Types Degree #Word Types5 186 (3.14%) 9 14 (0.24%) 146 77 (1.30%) 10 8 (0.14%) 157 42 (0.71%) 11 3 (0.05%) 178 25 (0.42%) 12 4 (0.07%) 1813 5 (0.08%) 20Sum1 (0.02%)1 (0.02%)1 (0.02%)1 (0.02%)1 (0.02%)39 (0.66%)Table 2.
The Distribution of Word Senses with Consideration of POS2Low 3456 Middle789l lHigh 121319Total WordTypesN1441 (81.05%1238 (13.39%55 (3.09% 126 (1.46%1V A1056(71.79%)238 (16.18%)99 (6.73%)580 (79.67%)115 (15.80%)20 (2.75%)41 (2.79%) 9 (1.24%)12 (0.67% 1 13 (0.88%) 2 (0.27%)i3 (0.17%) 13 (0.88%) 2 (0.27%)I2 (0.11%) 6 (0.40%)1 (0.06%)728 17781 (0.07%)1 (0.07%)1 (0.07%)1 (0.07%)1 (0.07%)14711F14 (77.78%)4 (22.22%)18K101 (73.72%)25 (18.25%)7 (5.11%)3 (2.19%)1 (0.73%)i137entry.
Of these, 5,922 words are polysemous,i.e., they have more than one sense.
Table 1lists the statistics.
We divide the ambiguitydegree into three levels according to the numberof senses of a word.
It includes low (2-4),middle (5-8), and high ambiguity (>8).
Thestatistics shows that 93.77% of word typesbelong to the class of low ambiguity.We further consider POS when computingthe distribution of word senses.
Table 2 showsthe statistics.
N, V, A, F, and K denote nouns,verbs, adjectives, numerals, and auxiliaries(adverbs), respectively.
We can find most ofwords belong to the class of low ambiguity nomatter which POSes they are.
Besides, theambiguity is decreased when POS is considered.The number of polysemous words is down to4,132.
For A and K, the number of senses isno more than 7, and the percentages in the classof  low degrees are 98.22% and 97.08%,respectively.
For N and V, there are some highambiguous words.
In particular, the verb (6 ,da3) has 19 senses 2.
The percentages in theclass of low degrees are 97.53% and 94.70%,respectively.Then, the ffi'equency of word types isconsidered.
ASBC corpus is used to computethe occurrences of  word types.
Table 3 fiststhe statistics.
A word token is an occurrence ofa type in the corpus.
On the average, the wordsof low, middle and high ambiguity appear205.96, 1926.65, and 4480.28 times,respectively.
Table 1 shows 93.77% ofpolysemous words belong to the class of lowambiguity, but Table 3 illustrates they only2 The word (~, da3) has 20 senses.
Besides verbusage, it also functions as art auxiliary.Table 3.
The Distribution of Word Senses with Consideration of FrequenciesLow Ambiguity Middle Ambiguity High AmbiguityTypes I Tokens I #Tokens/#Types5553 1143686 205.9693.77% 58.52%Types I Tokens330 6357965.57% 32.53%#Tokens/#Types1926.65Types390.66%Tokens \] #Tokens/#Ty174731 4480.288.94%Table 4.
The Distribution of Word Senses andFrequencies with Consideration of POS,'~uency  Low Middle High Sum Percentage Ambiguity ~Types (C) 3112 !
734?
~ 70131 230955 Low Tokens (A) I 22.54A/C 314.65Types (C) 421 62Middle Tokens (A) i 1905 14667A/C 45.36 236.56Types (C) 0 2High Tokens (A) 0 843A/CTypes (C) 3154Sum Tokens (A) 720360 421.5A/C 22.84Types (C) 76.33%Tokens (A) 5.94% %7981477358195005.57291533075286.4548471211.7539931036905259.681331698791277.295690180948.334132246465 i 893973 i 1212474308.851 4966.52 i19.31%!
4.36% I20.33%i 73.73%i96.64~85.52?A3.22"A14.01%0.15~0.479occupy 58.52% of tokens in ASBC corpus.Table 4 summarizes the distribution ofword senses and frequencies.
Low frequencydenotes the number of occurrences less than 100,middle frequency denotes the number ofoccurrences between 100 and 1000, and highfrequency denotes the number of occurrencesmore than 1000.
Rows C and A in Table 4denote number of word types and word tokens,respectively.
The last column denotespercentage for each ambiguity degree.
Forexample, the percentage of word types with lowambiguity is 96.64% (i.e., 3993/4132).
Thistable shows the following two phenomena:(1) POS information reduces the degree ofambiguities.
Total 8.94% of word tokens arehigh ambiguous in Table 3.
It decreases to0.47% in Table 4.
(2) High ambiguous words tend to be highfrequent.
From the row of low ambiguity,there are 3,112 low-frequent words.
Theyoccur 70,131 times in ASBC corpus.Comparatively, there are only 881 middle- orhigh-frequent words, but they occur 966,774times.
That is, 23.67% of word types aremiddle- or high-frequent words, and theyoccupy 94.06% of word tokens.
From the rowof high ambiguity, there are only a few words,but they occur frequently in the ASBC corpus.It shows that semantic tagging is a ehallengeableproblem in Mandarin Chinese.3 Semantic Tagging3.1 Tagging Unambiguous WordsIn the semantic tagging, the small categories areselected.
We postulate that he sense definitionfor each word in Cilin is complete.
That is, aword that has only one sense in Cilin is called anunambiguous word or a monosemous word.
IfPOS information is also considered, a word maybe unambiguous under a specific POS.Because we do not have a semantically taggedcorpus for training, we try to acquire the contextfor each semantic tag strutting from theunambiguous words.ASBC corpus is the target we study.
Atthe first stage, only those words that areunambiguous in Cilin, and also appear in ASBCcorpus are tagged~ Figure 1 shows this cease.Unambiguous WordsA S B ~ ~ ~Figure 1.
Tagging Unambiguous WordsAn unambiguous word (and hence its sensetag) is characterized bythe words surrounding it.The window size is set to 6, and stop words areremoved.
A list of stop words is trained fromASBC corpus.
The words of POSes Neu (~?~q), DE (~,  .~., ~,~-, ~) ,  SHI (~,.
), FW (J'l '~~) ,  C (i~l~j~q), T (~l~h~q), and I (~*~q)are regarded as stop words.
A sense tag Ctagis in terms of a vector (wl, w2, ..., wn), where nis the vocabulary size and wi is a weight of wordcw.
The weight can be determined by thefollowing two ways.
(1) MI metric (Church, etal., 1989)34l (Ctag ,ew ) =P (Ctag, cw)log 2 P(Ctag )P(cw) =f (Ctag , ew ) ?l?g2 f (Ctag ) f (ew ) x zvwhere P(Ctag) is the probability of Crag,P(cw) is the probability of cw,P(Ctag, cw) is the cooccurrenceprobability of Crag and cw,J(Ctag) is the frequency of Ctag,.
?ew) is the frequency of cw,~Ctag, cw) is the cooccurrencefrequency of Ctag and cw, andN is total number of words in thecorpus.
(2) EM metric (Ballesteros and Croft, 1998)em(Ctag, cw)=( f(Ctag, cw)- En(Ctag, cw) 0 max f(Ctag)+ f(cw) " )FEn (Ctag , cw )= f (Ctag ) f (cw )N3.2 Tagging Ambiguous WordsAt the second stage, we deal with those wordsthat have more than one sense in the Cilin.Figure 2 shows the words we consider.Unambiguous Wordsi  ilinAmbiguous WordsFigure 2.
Tagging Ambiguous WordsThe approach we adopted on semantictagging rests on an underlying assumption: eachsense has a characteristic ontext that isdifferent from the context of all the other senses.In addition, all words expressing the same senseshare the same characteristic context.
We willapply the information trained at the first stage toselecting the best sense tag from the candidatesof each ambiguous word.
Recall that a vectorcorresponds to a sense tag.
We employ thesimilar way specified in Section 3.1 to identifythe context vector of an ambiguous word.
Acosine formula shown as foUows measures thesimilarity between a sense vector and a contextvector, where w and v are a sense vector and acontext vector, respectively.
The sense tag ofthe highest similarity score is chosen.W oV cos (w, v)--IwIIvlWe retrain the sense vector for each sense tagafter the unambiguous words are resolved.3.3 Tagging Unknown WordsThose words that appear in ASBC corpus, butare not gathered in Cilin are called unknownwords.
All the 1,428 sense tags are thepossible candidates.
Intuitively, the algorithmin Section 3.2 can be applied directly to select asense tag from the 1,428 candidates.
However,the candidate set is very large.
Here we adoptoutside evidences from the mapping amongWordNet synsets (Fellbaum, 1998) and Cflin10Cwf synll~w"  | syn12 Mapping Tabler ewl / " \[ among\[ \[ syn2, WordNetr------.t% I" ~ J syn22" '  ~ew2 " \ ]  ~ synsets and~Figure  3.
Flow of Semantic TaggingCandidate List~Ctagl TMCrag2Ctag3IIIsense tags to narrow down the candidate set.Figure 3 summarizes the flow of our algorithm.It is illuslrated as follows.
(1) Find all the English translations of anunknown Chinese word by looking up aChinese-English dictionary.
(2) Find all the symets of the Englishtranslations by looking up WordNet.
We donot resolve translation ambiguity and targetpolysemy at these two steps, thus the retrievedsymets may cover more senses than that of theoriginal Chinese word.
(3) Transform the synsets back to Cilin sensetags by looking up a mapping table.
How themapping table is set up will be discussed inSection 3.3.
I.
(4) Select a sense tag from the candidatesproposed at step (3) by using the WSD inSection 3.2.Figure 4 shows the unknown words we dealwith at this stage.
Those words that are notgathered in our Chineso-English dictionary arenot considered, so that only parts of unknownwords are resolve.
In other words, thoreremain words without sense tags.Unambiguous WordsUnknown "~Words Ambiguous WordsFigure 4.
Tagging Unknown Words3.3.1 Mapping SynSets to Cilin Sense TagsAt first, we put unambiguous words (specifiedin Section 3.1) into WordNet by looking up aChinese-English dictionary.
Although thesewords do not have translation ambiguity, thecorresponding English translation may havetarget polysemy problem.
In other words, theEnglish translation may cover irrelevant sensesbesides the correct one.
The followingalgorithm will find the most similar syuseet withChinese sense tag.
(1) If the English translation corresponds toonly one symet, this symet is the solution.
(2) If the English translation corresponds tomore than one synset, POS is considered:(a) If the Chinese sense tag belongs to one ofcategories A-D in Cilin (i.e., a noun sense),and there is only one noun synset, then thesynset is adopted.
Otherwise, we translatethe context vector of the Chinese sense intoEnglish, compare it with vectors of thesynsets, and select he most similar synset.
(b) If the Chinese sense tag belongs to one ofcategories F-J in Cilin (i.e., a verb sense),we try to find a verb syuset in the similarway as (a).
If it fails, we try noun andadjective synsets instead.
(c) If the Chinese sense tag bdongs to categoryE in Olin (i.e., an adjective sense), we tryadjective, adverb, noun and verb symets insequence.Off) If the Chinese sense tag belongs to categoryK in Cilin (i.e., an adverb sense), onlyadverb syasets are considered.Next, we consider the ambiguous words.Chinese-English dictionary lookup finds all theEnglish translations.
WordNet search coneets11the synset candidates for the translations.Some synsets are selected and regarded as themapping of the Cilin sense tag.
Here theproblems of translation ambiguity and targetpolysemy must be faced.
In other words, notall English translations cover the Cilin sense.Because the goal is to find a mapping tablebetween WordNet synsets and Cflin sense tags,we neglect the problem of translation ambiguityand follow the method in the previous paragraphto choose the most similar synsets.During mapping, English translations of aword may not be found in the Chinese-Englishdictionary, and WordNet may not gather theEnglish translations even dictionary look-up issuccessful.
Thus, only 1,328 of 1,428 Cilintags are mapped to WordNet synsets.
From theother view, there remains some WordNetsynsets that do not correspond toany Cilin sensetags.
Let such a synset be Si.
We follow therelational pointers like hypernym, hyponym,similar, derived, antonym, or participle tocollect the neighboring synsets denoted by Sj.The following method selects suitable Cflintag(s) for Si.
(1) IfSj is the only one syuset that has beenmapped to Cilin tags, we choose a Cilintag and map Si to it.
(2) If there exists more than one Sj (say, Sjl,Sj2, ..., S~) that has been mapped toCilin tags, we choose the Cilin tags thatmore synsets map to.The above method is called a more restrictivescheme.
An alternative method (called lessrestrictive method) is: all the Cilin tags that theneighboring synsets map to are selected.
IfCilin tags cannot be found from neighboringsynsets, we extend the range one more, andrepeat the selection procedure again until all thesyuseets are considered.4 Experiments4.1 Test MaterialsWe sample documents of different categoriesfrom ASBC corpus, including philosophy (10%),science (10%), society (35%), art (5%), life(20%) and literary (20%).
There are 35,921words in the test corpus.
Research associatestag this corpus manually.
At first, they markup the ambiguous words by looking up the Cilindictionary.
Next, they tag the unknown words.A list of candidates i proposed by looking upthe mapping table.
Because the mapping tablemay have errors, the annotators assign a tag"none" when they cannot choose a solution fromthe proposed candidates.
Total 435 of 1,979words are tagged with "none" with the morerestrictive method.
In contrast, only 346 wordsare labeled with "none" with the less restrictivemethod.
The tag mapper achieves 82.52% ofperformance approximately.4.2 Tagging Ambiguous WordsTable 5 shows the performance of taggingambiguous words.
MI defined in Section 3.1 isused.
Total 11,101 words are tagged.
Theperformance of tagging low, middle, and highambiguous words are 62.60%, 31.36%, and27.00%, respectively.
Table 6 shows that theperformance is improved, in particular, theclasses of middle- and high- ambiguity, whenEM (defined in Section 3.1) is used.
Theoverall performance is increased from 49.55%to 52.85%.In the previous experiments, only one sense isreported for each word.
If we report more thanone sense for middle and high ambiguous words,the performance is improved.
Table 7 showsthat the first 2 and 3 candidates are selected.From the diagonal of this table, the performancefor tagging low ambiguity (2-4), middleambiguity (5-8) and high ambiguity (>8) issimilar (i.e., 63.98%, 60.92% and 67.95%) when1 candidate, 2 candidates, and 3 candidates areproposed, respectively.
In this case, 7,034 of11,101 words are tagged correctly.
That is, theperformance is 63.36%.In the next experiment, we adopt middlecategories (i.e., 94 categories) rather than theabove small categories (i.e., 1428 categories).Table 8 shows that the overall performance isimproved by 11.05%.
It also lists the resultswith the combinations of first-n and middlecategories.
Under the middle categories and1-3 proposed candidates, the performance fortagging low, middle and high ambiguous wordsare 71.02%, 73.88%, and 75.94%, respectively.Total 8,033 of 11,101 words are taggedcorrectly.
In other words, the performance is72.36%.12Table 5.
Performance of Tagging Ambi\["''--....AmbiguityWord Tokem~ Low MiddleTotal Tokens 6601 3511Correct Tokens 4132 1101Correct Rate 62.60% 31.36%aous Words using MIHigh ,98926727.00%Summary11101550049.55%TableTotal Tokensaous Words using EMHigh6.
Performance of Tagsing AmbigLow Middle "63.98% 37.99%6601 3511 989Correct Tokens 4223 1334 310Correct Rate 31.34%Summary11101586752.85%-""---~AmbiguityFirst-n1Table 7.
Performance of Tagging usin tLow Middle63.98% 37.99%60.92%71.35%the Firs t-n and E MHigh Middle and High31.34% 36.53%53.99% 55.40%67.95% 70.60%Table 8.
Performance of Ta,F'~st-~cotegoh~s--.-.-......J Low2Small 63.98%;ging using First-n and Middle Cate~Middle High31.34% 37.99%Middle 71.02% 56.19% 43178% 53.47%Small 60.92% 53.99% 59.40%Middle 73.88% 72.09%Small 71.35%79.27% Middle65.72%67.95%75.94%ories , .
,Middle and High36.53%70.60%78.53%4.3 Tagging Unknown WordsThere are 1,979 unknown words in our testcorpus.
Total 1,663 words have been taggedmanually.
In the experiments, we consider theeffects from training corpus and mapping table.Table 9 shows the performance.
M1 and P1employ more restrictive mapping table, whileM2 and P2 adopt less restrictive mapping table.M1 and M2 use the training result in Section 3.1(i.e., unambiguous words), while P1 and P2utilize the training result in Section 3.2 (i.e.,unambiguous and ambiguous words).
In thebaseline model, all 1428 Cilin tags are thecandidates of unknown words.
Theperformance is worse.
On the average, theprecision is 1.22%.
M1 is the best becausemore restrictive mapping table reduces thepossibility of mapping errors.
This table alsolists the perforrnanee of each category.
Itmeets our expectation, i.e., tagging verb isharder than tagging other categories.
Next weuse POS to improve the performance.
POSnarrows down the number of candidates, o thatthe overall performance is enhanced from27.13%% to 34.35%%.In summary, we consider the overallperformance of tagging our sample data.Recall that there are 35,921 words in the testcorpus.
Except the stop words that are nottagged by the sense tagger, there remain 13,586unambiguous words, 11,101 ambiguous words,and 1,633 unknown words for tagging.
FromTables 6 and 9, we know 5,867 unambiguouswords and 561 unknown words are taggedcorrectly.
The sense tagger achieves theperformance of76.04%.5.
ConclusionThis paper analyzes the polysemy degree inMandarin Chinese.
We consider thedistribution of word senses from POS andfrequency.
Under the Cilin small categories,23.67% of word types in ASBC corpus are13CategoriesAll#Tokens1633Table 9.CorrectPerformance of TaB: ~Baseline20M1443ng Unknown WordsM239524.19%P143826.82%P239624.25%MI(POS)56i34.35% Preci~on 1.22% 27.13%Correct 11 255 228 255 231 320N 858Preci~on 1.28% 29.72% 26.57% 29.72% 26.92% 37.30%Correct 5 144 124 137 120 1670.81%25.00%3.19%6195823.26%8.62%25.00%3840.43%VAPrecisionCorrect20.03%8.62%25.00%3739.36PrecisionCorrectPrec~smn22.13?A8.62%25.00%4042.55CorrectPr~ismnF19.39eA8.620A26.98~28K 9448.28~1 425.00% 100.00%39 4241.49 44.68~middle or high frequent words, but they occupy94.06% of word tokens.
We adopt contextualinformation and mapping from WordNet synsetsto Cilin sense tags to deal with thischallengeable problem.
The performances fortagging low, middle and high ambiguous wordsare 63.98%0, 60.92%, and 67.95% when smallproposed.
Comparatively, the performancescategories are used and 1-3 candidates are71.02%, 73.88%, and 75.94% by using middlecategories.
The performance of taggingunknown words is 34.35%.
It is worse thanthat of tagging ambiguous words, but is muchbetter than that of the baseline mode.
Theoverall performance is the sense tagger is76.04%.
Although sense tagging does notachieve the performance of POS tagging, thesense tagger proposed in  this paper is still auseful computer-aided tool to reduce the humancost on tagging a large-scale corpus.ReferencesAtkinn, S. (1993) "Tools for Computer-AidedLexicography: the Hector Project," ActaLinguistica Hungarica, 41, pp.
5-72.Ballesteros, L. and Croft, W.B.
(1998) "ResolvingAmbiguity for Cress-Language InformationRetrieval," Proceedings of the 21st Annuallnternational A CM SIGIR Conference, pp.
64-71.Church, K.W., et al (1989) "Parsing, WordAssociations and Typical Predicate-ArgumentRelations."
Proceedings of InternationalWorkshop on Parsing Technologies, pp.
389-398.Huang, C.R_ and Chen, K.L (1995) "AcademicSinica Balanced Corpus," Technical Report95-02/98-04, Academic Sinica, Taipei, Taiwan.Fellbaum, C. editor (1998) WardNet: An ElectronicLexical Database, MIT Press, Cambridge, Mass.Ide, N. and Veronis, J.
(1998) "Word SenseDisambiguation: The State of Art,"Computational Linguistics, 24( 1 ), pp.
1--40.Kelly, E. and Stone, P. (1975) Computer Recognitionof English Word Senses, North-Holland,Amsterdam.Kilgarriff, A.
(1998) "SENSEVAL: An Exercise inEvalnafiqg Word Sense DisnmbiguationProgram~," Proceedings of First InternationalConference on Language Resources andEvaluation, Granada, pp.
581-588.Kilgarriff, A. and Rosenzweig, J.
(2000) '~,nglishSENSEVAL: Report and Results," Proceedingsof Second International Conference on LanguageResources and Evaluation.Manning, C.D.
and Schutze, I-L (1999) Foundationsof Statistical Natural Language Processing, MITPress, Cambridge, Mass.\]Vlarshall, I.
(1987) "Tag Selection using ProbabilisticMethods," in Roger Garside, Geoffrey Leech andGeotErey Sampson (editors), The ComputationalAnalysis of English, Longman~ pp.
42-56.Mei, J.; et al (1982) tong2yi4ci2ci21in2.
ShanghaiDictionary Press.Ng, I-LT. and Lee, I-LB.
(1996) "Integrating MultipleKnowledge Sources to Disambiguate Word Sense:An Exemplar-Based Approach," Proceedings of34th Annual Meeting of Association forComputational Linguistics, pp.
40--47.14
