PrefaceReferring Expression Generation Challenges 2008 (REG?08) was the secondshared-task evaluation challenge (STEC) in the field of Natural Language Gen-eration (NLG) and took place between September 2007, when REG?08 was firstannounced, and May 2008, when the evaluation of the participating systems wascompleted.REG?08 follows on from the Attribute Selection for Referring Expression Gen-eration Challenge in 2007 (ASGRE?07) which was conceived as a pilot event forshared-task evaluation in NLG.
Shared tasks in NLG are themselves a natural con-tinuation of a growing interest in more comparative forms of evaluation amongNLG researchers, and mirror trends in other HLT fields.Since the foundational work of authors such as Appelt, Kronfeld, Grosz, Joshi,Dale and Reiter, Referring Expression Generation (REG) has been the subject ofintensive research in NLG, and has ?
unusually for NLG ?
led to significantconsensus on the REG problem definition, as well as the nature of the inputs andoutputs of REG algorithms.
This is particularly true of the attribute selection sub-task, perhaps the most widely researched NLG subtask, which was the shared taskin ASGRE?07.
REG?08 included the same task (TUNA-AS), and added two moretasks based on the TUNA corpus: realisation (TUNA-R) and the complete referringexpression generation task (TUNA-REG).
REG?08 also introduced a new data setof short introductory sections from Wikipedia articles on geographic entities andpeople (the GREC corpus) and a new task based on it: generation of referring ex-pressions for named entities in the context of a discourse longer than a sentence.The intended application context for this task is improvement of referential clarityin extractive summaries.In addition to the four shared tasks, REG?08 offered, for each of the twodatasets, (i) an open submission track in which participants could submit any workinvolving the data while opting out of the competetive element, and (ii) an eval-uation track, in which proposals for new evaluation methods for the shared taskcould be submitted.
We believe that these two types of open-access tracks are im-portant because they allow the wider research community to shape the focus andmethodologies of STECs directly.We successfully applied (with the help of support letters from many of lastyear?s participants and other HLT colleagues) for funding from the Engineeringand Physical Sciences Research Council (EPSRC), the main funding body for HLTin the UK.
This support enabled us to double the size of the GREC corpus andto carry out extensive human task performance evaluations, as well as employ adedicated research fellow (Eric Kow) to help with all aspects of REG?08.
It alsoenabled us to enlist the help of Jette Viethen from Macquarie University with theGREC evaluations.REG?08 got underway with a first announcement in September 2007.
We re-leased samples for both datasets and invited preliminary registrations in January2008, and released the full Participants?
Pack including instructions and train-ing/development data on 22nd February to registered participants.
Twelve teamsregistered for one or more of the TUNA tasks, and five for the GREC Task.
Amongthe participants were teams from Australia, Spain, Ireland, UK, USA, Brazil, Bel-gium, Netherlands, India and Germany.181By the deadline of 7th April (which was extended slightly), 8 teams submitted13 systems for TUNA-AS, 4 teams submitted 5 systems for TUNA-R, and 6 teamssubmitted 15 systems for TUNA-REG.
Three teams submitted 6 systems for thebrand new GREC task, to which we added four baseline systems.
We also receivedone submission in the TUNA Open Track which applies Portuguese surface realisa-tion to TUNA attribute sets.The submission process required participants to first submit a report describingtheir approach and reporting results on the development data for the task(s) theyhad participated in.
For this purpose, they were supplied with programs to computethe relevant evaluation metrics.
On submission of the report, they could downloadthe test data, and had 48 hours to submit their outputs on the test set.All submissions are described in the participants?
reports in this volume.
Weare pleased to say that several of the contributions came from students, as well asfrom researchers entirely new to the field of NLG.We had a total of 33 TUNA systems and 10 GREC systems to evaluate.
Weconducted task-performance experiments for all TUNA-REG systems and the 6 sub-mitted GREC systems.
We computed a range of automatic intrinsic measures forall task tracks.
For the GREC task, we also tried out a new evaluation idea: auto-matic extrinsic evaluation using coreference resolution tools.
All these evaluationmethods are described in detail in the two evaluation reports in this volume.Preparations are already underway for a third NLG shared-task evaluation eventin 2009, Generation Challenges 2009, which will include a TUNA-REG progresscheck task, a GREC task and a new task on instruction giving in virtual environ-ments (the GIVE Challenge).
Results will be presented at ENLG?09.Like all STECs, REG?08 would not have been possible without the contributionsof many different people.
We would like to thank the faculty, staff and studentsof Brighton University and the friends who participated in the evaluation exper-iments; the INLG?08 organisers, in particular Mike White; the research supportteam at Brighton University and the EPSRC for help with obtaining funding; JasonBaldridge and Pascal Denis for assistance with the coreference resolvers; and lastbut not least, the REG?08 participants for making the most of the short availabletime to build their systems (and for never once complaining, even when they hadgood reason to).
Special thanks are due to Eric Kow and Jette Viethen who workedextremely hard especially during the four weeks of the evaluation period.Anja Belz and Albert Gatt182
