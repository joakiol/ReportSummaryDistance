Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 13?24,Paris, October 2009. c?2009 Association for Computational LinguisticsWeighted parsing of treesMark-Jan NederhofSchool of Computer Science, University of St AndrewsNorth Haugh, St Andrews, KY16 9SX, ScotlandAbstractWe show how parsing of trees can be for-malized in terms of the intersection of twotree languages.
The focus is on weightedregular tree grammars and weighted treeadjoining grammars.
Potential applica-tions are discussed, such as parameter es-timation across formalisms.1 IntroductionIn parsing theory, strings and trees traditionallyhave had a very different status.
Whereas stringsin general receive the central focus, the trees in-volved in derivations of strings are often seen asauxiliary concepts at best.
Theorems tend to beabout the power of grammatical formalisms toproduce strings (weak generative power) ratherthan trees (strong generative power).This can be explained by looking at typicalapplications of parsing.
In compiler construc-tion for example, one distinguishes between parsetrees and (abstract) syntax trees, the former beingshaped according to a grammar that is massagedto make it satisfy relatively artificial constraints,e.g.
that of LALR(1), which is required by manycompiler generators (Aho et al, 2007).
The formof syntax trees is often chosen to simplify phasesof semantic processing that follow parsing.
Asthe machinery used in such processing is generallypowerful, this offers much flexibility in the choiceof the exact shape and labelling of syntax trees, asintermediate form between parsing and semanticanalysis.In the study of natural languages, parse treeshave played a more important role.
Whereas lin-guistic utterances are directly observable and treesderiving them are not, there are nevertheless tradi-tions within linguistics that would see one struc-tural analysis of a sentence as strongly preferredover another.
Furthermore, within computationallinguistics there are empirical arguments to claimcertain parses are correct and others are incorrect.For example, a question answering systems mayverifiably give the wrong answer if the questionis parsed incorrectly.
See (Jurafsky and Martin,2000) for general discussion on the role of parsingin NLP.Despite the relative importance of strong gen-erative power in computational linguistics, thereis still much freedom in how exactly parse treesare shaped and how vertices are labelled, due tothe power of semantic analysis that typically fol-lows parsing.
This has affected much of the the-oretical investigations into the power of linguisticformalisms, and where strong equivalence is con-sidered at all, it is often ?modulo relabelling?
orallowing minor structural changes.With the advent of syntax-based machine trans-lation, trees have however gained much impor-tance, and are even considered as the main ob-jects of study.
This is because many MT mod-ules have trees both as input and output, whichmeans the computational strength of such mod-ules can be measured only in terms of the tree lan-guages they accept and the transductions betweentree languages that they implement.
See for exam-ple (Knight, 2007).In contrast, trees have always been the centralissue in an important and well-established subfieldof formal language theory that studies tree lan-guages, tree automata and tree transducers (Gc-seg and Steinby, 1997).
The string languages gen-erated by the relevant formalisms in this contextare mostly taken to be of secondary importance, ifthey are considered at all.This paper focuses on tree languages, but in-volves a technique that was devised for string lan-guages, and shows how the technique carries overto tree languages.
The original technique can beseen as the most fundamental idea in the field ofcontext-free parsing, as it captures the essence of13finding hierarchical structure in a linear sequence.The generalization also finds structure in a lin-ear sequence, but now the sequence correspondsto paths in trees each leading down from a vertexto a leaf.
This means that the proposed type ofparsing is orthogonal to the conventional parsingof strings.The insights this offers have the potential to cre-ate new avenues of research into the relation be-tween formalisms that were until now consideredonly in isolation.
We seek credence to this claimby investigating how probability distributions canbe carried over from tree adjoining grammars toregular tree grammars, and vice versa.The implication that the class of tree languagesof tree adjoining grammars (TAGs) is closed underintersection with regular tree languages is not sur-prising, as the linear context-free tree languages(LCFTLs) are closed under intersection with reg-ular tree languages (Kepser and Mo?nnich, 2006).The tree languages of TAGs form a subclass of theLCFTLs, and the main construction in the proofof the closure result for the latter can be suitablyrestricted to the former.The structure of this paper is as follows.
Themain grammatical formalisms considered in thispaper are summarized in Section 2 and Sec-tion 3 discusses a number of analyses of these for-malisms that will be used in later sections.
Sec-tion 4 starts by explaining how parsing of a stringcan be seen as the construction of a grammar thatgenerates the intersection of two languages, andthen moves on to a type of parsing involving in-tersection of tree languages in place of string lan-guages.In order to illustrate the implications of the the-ory, we consider how it can be used to solve a prac-tical problem, in Section 5.
A number of possibleextensions are outlined in Section 6.2 FormalismsIn this section, we recall the formalisms ofweighted regular tree grammars and weighted treeadjoining grammars.
We use similar notation andterminology for both, in order to prepare for Sec-tion 4, where we investigate the combination ofthese formalisms through intersection.
As a conse-quence of the required unified notation, we deviateto some degree from standard definitions, withoutaffecting generative power however.For common definitions of weighted regulartree grammars, the reader is referred to (Graehland Knight, 2004).
Weighted tree adjoining gram-mars are a straightforward generalization of prob-abilistic (or stochastic) tree adjoining grammars,as introduced by (Resnik, 1992) and (Schabes,1992).For both regular tree grammars (RTGs) and treeadjoining grammars (TAGs), we will write a la-beled and ordered tree as A(?).
where A is the la-bel of the root node, and ?
is a sequence of expres-sions of the same form that each represent an im-mediate subtree.
In our presentation, labels do nothave explicit ranks, that is, the number of childrenof a node is not determined by its label.
This al-lows an interesting generalization, to be discussedin Section 6.2.Where we are interested in the string languagegenerated by a tree-generating grammar, we maydistinguish between two kinds of labels, the ter-minal labels, which may occur only at leaves, andthe nonterminal labels, which may occur at anynode.
It is customary to write terminal leaves asa instead of a().
The yield of a tree is the stringof occurrences of terminal labels in it, from left toright.
Note that also nonterminal labels may occurat the leaves, but they will not be included in theyield; cf.
epsilon rules in context-free grammars.2.1 Weighted regular tree grammarsA weighted regular tree grammar (WRTG) is a 4-tuple G = (S,L,R, s`), where S and L are twofinite sets of states and labels, respectively, s` ?
Sis the initial state, and R is a finite set of rules.Each rule has the form:s0 ?
A(s1 ?
?
?
sm) ?w?,where s0, s1, .
.
.
, sm are states (0 ?
m), A is alabel and w is a weight.Rewriting starts with a string containing onlythe initial state s`.
This string is repeatedly rewrit-ten by replacing the left-hand side state of a rule bythe right-hand side of the same rule, until no stateremains.
It may be convenient to assume a canoni-cal order of rewriting, for example in terms of left-most derivations (Hopcroft and Ullman, 1979).Although alternative semirings can be consid-ered, here we always assume that the weightsof rules are non-negative real numbers, and theweight of a derivation of a tree is the product ofthe weights of the rule occurrences.
If several(left-most) derivations result in the same tree, then14the weight of that tree is given by the sum of theweights of those derivations.
Where we are inter-ested in the string language, the weights of treeswith the same yield are added to obtain the weightof that yield.A (weighted) context-free grammar can be seenas a special case of a (weighted) regular tree gram-mar, where the set of states equals the set of labels,and rules have the form:A ?
A(B1 ?
?
?Bm).Also the class of (weighted) tree substitutiongrammars (Sima?an, 1997) can be seen as a spe-cial case of (weighted) regular tree grammars, byletting the set of labels overlap with the set ofstates, and imposing two constraints on the allow-able rules.
The first constraint is that for each la-bel that is also a state, all defining rules are of theform:A ?
A(s1 ?
?
?
sm).The second constraint is that for each state that isnot a label, there is exactly one rule with that statein the left-hand side.
This means that exactly onesubtree (or elementary tree) can be built top-downout of such states, down to a level where we againencounter states that are also labels.
If desired, wecan exclude infinite elementary trees by imposingan additional constraint on allowed sets of rules(no cycles composed of states that are not labels);alternatively, we can demand that the grammardoes not contain any useless rules, which automat-ically excludes such infinite elementary trees.2.2 Weighted linear indexed grammarsAlthough we are mainly interested in the tree lan-guages of tree adjoining grammars, we will usean equivalent representation in terms of linear in-dexed grammars, in order to obtain a uniform no-tation with regard to regular tree grammars.Thus, a weighted linear indexed grammar(WLIG) is a 5-tuple G = (S, I, L,R, s`), whereS, I and L are three finite sets of states, indicesand labels, respectively, s` ?
S is the initial state,and R is a finite set of rules.
Each rule has one ofthe following four forms:1.
s0[??]
?
A( s1[ ] ?
?
?sj?1[ ] sj [??]
sj+1[ ] ?
?
?sm[ ] ) ?w?,where s0, s1, .
.
.
, sm are states (1 ?
j ?
m),A is a label and w is a weight;2. s[ ] ?
A() ?w?;3.
s[??]
?
s?[???]
?w?, where ?
is an index;4.
s[???]
?
s?[??]
?w?.The expression ??
may be thought of as a vari-able denoting a string of indices on a stack, andthis variable is to be consistently substituted inthe left-hand and the right-hand sides of rulesupon application during rewriting.
In other words,stacks are copied from the left-hand side of a ruleto at most one member in the right-hand side,which we will call the head of that rule.
The ex-pression [ ] stands for the empty stack and [???]
de-notes a stack with top element ?.
Thereby, rules ofthe third type implement a stack push and rules ofthe fourth type implement a pop.
Rewriting startsfrom s`[ ].
The four subsets of R containing rulesof the respective four forms above will be referredto as R1, R2, R3 and R4.In terms of tree adjoining grammars, which as-sume a finite number of elementary trees, the in-tuition behind the four types of rules is as fol-lows.
Rules of the first type correspond to con-tinued construction of the same elementary tree.Rules of the third type correspond to the initiationof a newly adjoined auxiliary tree and rules of thefourth type correspond to its completion at a footnode, returning to an embedding elementary treethat is encoded in the index that is popped.
Rulesof the second type correspond to construction ofleaves, as in the case of regular tree grammars.See further (Vijay-Shanker and Weir, 1994) for theequivalence of linear indexed grammars and treeadjoining grammars.Note that regular tree grammars can be seen asspecial cases of linear indexed grammars, by ex-cluding rules of the third and fourth types, whichmeans that stacks of indices always remain empty(Joshi and Schabes, 1997).2.3 Probabilistic grammarsA weighted regular tree grammar, or weighted lin-ear indexed grammar, respectively, is called prob-abilistic if the weights are probabilities, that is,values between 0 and 1.
A probabilistic regulartree grammar (PRTG) is proper if for each states, the probabilities of all rules that have left-handside s sum to one.Properness for a probabilistic linear indexedgrammar (PLIG) is more difficult to define, dueto the possible overlap of applicability between15the four types of rules, listed in the section above.However, if we encode a given TAG as a LIG in areasonable way, then a state s may occur both inleft-hand sides of rules from R1 and in left-handsides of rules from R3, but all other such overlapbetween the four types is precluded.Intuitively, a state may represent an internalnode of an elementary tree, in which case rulesfrom both R1 and R3 may apply, or it may rep-resent a non-foot leaf node, in which case a rulefrom R2 may apply, or it may be a foot node, inwhich case a rule from R4 may apply.With this assumption that the only overlap in ap-plicability is between R1 and R3, properness canbe defined as follows.?
For each state s, either there are no rules inR1 or R3 with s in the left-hand side, or thesum of probabilities of all such rules equalsone.?
For each state s, either there are no rules inR2 with s in the left-hand side, or the sum ofprobabilities of all such rules equals one.?
For each state s and index ?, either thereare no rules in R4 with left-hand side s[???
],or the sum of probabilities of all such rulesequals one.We say a weighted regular tree grammar, orweighted linear indexed grammar, respectively, isconsistent if the sum of weights of all (left-most)derivations is one.
This is equivalent to saying thatthe sum of weights of all trees is one, and to sayingthat the sum of weights of all strings is one.For each consistent WRTG (WLIG, respec-tively), there is an equivalent proper and consistentPRTG (PLIG, respectively).
The proof lies in nor-malization.
For WRTGs this is a trivial extensionof normalization of weighted context-free gram-mars, as described for example by (Nederhof andSatta, 2003).
For WLIGs (and weighted TAGs),the problem of normalization also becomes verysimilar once we consider that the set of derivationtrees of tree adjoining grammars can be describedwith context-free grammars, and that this carriesover to weighted derivation trees.
See also (Sarkar,1998).WLIGs seemingly incur an extra complication,if a state may occur in combination with an indexon top of the associated stack such that no rules areapplicable.
However, for LIGs that encode TAGs,the problem does not arise as, informally, one mayalways resume construction of the embedding el-ementary tree below the foot node of an adjoinedauxiliary tree.We say a LIG is in TAG-normal form if (a) atleast one rule is applicable for each combinationof state s and index ?
such that s[???]
is deriv-able from s`[ ], and (b) the only overlap in ap-plicability of the four types of rules is betweenR1 and R3.
Statements in what follows involv-ing WLIGs (or PLIGs) in TAG-normal form alsohold for weighted (or probabilistic) TAGs.3 Analysis of grammarsWe call a grammar rule useless if it cannot be partof any derivation of a tree (or of a string, in thecase of grammars with an emphasis on string lan-guages).
We say a grammar is reduced if it doesnot contain useless rules.Whereas most grammars written by hand or in-duced by a corpus or treebank are reduced, thereare practical operations that turn reduced gram-mars into grammars with useless rules; we willsee an example in the next section, where gram-mars are constructed that generate the intersectionof two given languages.
In order to determinewhether the intersection is non-empty, it suffices toidentify useless rules in the intersection grammar.If and only if all rules are useless, the generatedlanguage is empty.In the case of context-free grammars (see for ex-ample (Sippu and Soisalon-Soininen, 1988)), theanalysis to identify useless rules can be split intotwo phases:1. a bottom-up phase to identify the grammarsymbols that generate substrings, which mayinclude the start symbol if the generated lan-guage is non-empty; and2.
a top-down phase to identify the grammarsymbols that are reachable from the startsymbol.The intersection of the generating symbols and thereachable symbols gives the set of useful symbols.One can then identify useless rules as those thatcontain one or more symbols that are not useful.The procedure for linear indexed grammars issimilarly split into two phases, of which the firstis given in Figure 1 in the form of a deductionsystem.
The inference rules simultaneously derive16(s, s){s ?
S (a)s{s[ ] ?
A() (b)s1 ?
?
?
sj?1 (sj , s) sj+1 ?
?
?
sm(s0, s){s0[??]
?
A(s1[ ] ?
?
?
sj [??]
?
?
?
sm[ ]) (c)s1 ?
?
?
sms0{s0[??]
?
A(s1[ ] ?
?
?
sj [??]
?
?
?
sm[ ]) (d)(s1, s2)(s3, s4)(s0, s4){s0[??]
?
s1[???]s2[???]
?
s3[??]
(e)(s1, s2)s3s0{s0[??]
?
s1[???]s2[???]
?
s3[??]
(f)Figure 1: Simultaneous analysis of two kinds of subderivations in a LIG.
Items (s, s?)
represent existenceof one or more subderivations s[ ] ??
?(s?
[ ]), where ?
is a tree with a gap in the form of an unresolvedstate s?
associated with an empty stack.
Furthermore, s and s?
are connected through propagation of astack of indices, or in other words, the occurrence of s?
is the head of a rule, of which the left-hand sidestate is the head of another rule, etc., up to s. In the inference rules, items s represent existence of one ormore subderivations s[ ] ??
?, where ?
is a complete tree (without any unresolved states).two types of item.
The generated language is non-empty if the item s` can be derived.We will explain inference rule (f), which is themost involved of the six rules.
The two itemsin the antecedent indicate the existence of deriva-tions s1[ ] ??
?
(s2[ ]) and s3[ ] ??
?.
Notethat s1[ ] ??
?
(s2[ ]) implies s1[?]
??
?(s2[?
]),because an additional element in the bottom ofa stack would not block an existing derivation.Hence s0[ ] ?
s1[?]
??
?(s2[?])
?
?
(s3[ ]) ???(?
), which justifies the item s0 in the consequentof the rule.After determining which items can be derivedthrough the deduction system, it is straightforwardto identify those rules that are useful, by applyingthe inference rules in reverse, from consequent toantecedents, starting with s`.The running time of the analysis is determinedby how often each of the inference rules can beapplied, which is bounded by the number of wayseach can be instantiated with states and rules fromthe grammar.
The six inference rules together giveus O(|S| + |R2| + |R1| ?
|S| + |R1| + |R3| ?
|R4| ?|S| + |R3| ?
|R4|) = O(|S| + |R1| ?
|S| + |R2|+ |R3| ?
|R4| ?
|S|) = |G|3, where we assume areasonable measure for the size |G| of a LIG G, forexample, the total number of occurrences of states,labels and indices in the rules.It is not difficult to see that there is exactly onededuction of s` in the deduction system for eachcomplete derivation in the grammar.
We leave thefull proof to the interested reader, but provide thehint that items (s, s?)
can only play a role in acomplete deduction provided s?
is rewritten by arule that pops an index from the stack.
Becauseof this, derivations in the grammar of the forms[ ] ??
?(s?
[ ]) or of the form s[ ] ??
?
can bedivided in a unique way into subderivations repre-sentable by our items.The above deduction system is conceptuallyvery close to a system of equations that expressesthe sum of weights of all derivations in the gram-mar, or in(s`), in terms of similar values of theform in(s), which is the sum of weights of allsubderivations s[ ] ??
?, and in(s, s?
), which isthe sum of weights of all subderivations s[ ] ???(s?
[ ]).
The equations are given in Figure 2.Although the expressions look unwieldy, they17in(s0) =?s0[ ] ?
A() ?w?w +?s0[??]
?
A(s1[ ] ?
?
?
sj [??]
?
?
?
sm[ ]) ?w?w ?
in(s1) ?
.
.
.
?
in(sm) +?s0[??]
?
s1[???]
?w?s2[???]
?
s3[??]
?v?w ?
v ?
in(s1, s2) ?
in(s3)in(s0, s) = ?
(s0 = s) + ?s0[??]
?
A(s1[ ] ?
?
?
sj [??]
?
?
?
sm[ ]) ?w?w ?
in(s1) ?
.
.
.
?
in(sj , s) ?
.
.
.
?
in(sm) +?s0[??]
?
s1[???]
?w?s2[???]
?
s3[??]
?v?w ?
v ?
in(s1, s2) ?
in(s3, s)Figure 2: The sum of weights of all derivations in a WLIG, or in(s`), is defined by the smallest non-negative solution to a system of equations.
The function ?
with a boolean argument evaluates to 1 if thecondition is true and to 0 otherwise.express exactly the ?inside?
value of the weightedcontext-free grammar that we can extract out ofthe deduction system from Figure 1, by instanti-ating the inference rules in all possible ways, andthen taking the consequent as the left-hand side ofa rule, and the antecedent as the right-hand side.The weight is the product of weights of rules thatappear in the side conditions.
It is possible to ef-fectively solve the system of equations, as shownby (Wojtczak and Etessami, 2007).In the same vein we can compute ?outside?values for weighted linear indexed grammars, asstraightforward analogues of the outside values ofweighted and probabilistic context-free grammars.The outside value is the sum of weights of partialderivations that may lie ?outside?
a subderivations[ ] ??
?
in the case of out(s), or a subderivations[ ] ??
?(s?
[ ]) in the case of out(s, s?).
The equa-tions in Figure 3 again follow trivially from theview of Figure 1 as weighted context-free gram-mar and the usual definition of outside values.The functions in and out are particularly usefulfor PLIGs in TAG-normal form, as they allow theexpected number of occurrences of state s to beexpressed as:E(s) = in(s) ?
out(s)Similarly, the expected number of subderivationsof the form s[ ] ??
?(s?
[ ]) is:E(s, s?)
= in(s, s?)
?
out(s, s?
)We will return to this issue in Section 5.4 Weighted intersectionBefore we discuss intersection on the level oftrees, we first show how a well-established type ofintersection on the level of strings, with weightedcontext-free grammars and weighted finite au-tomata (WFAs), can be trivially extended to re-place CFGs with RTGs or LIGs.
The intersec-tion paradigm is originally due to (Bar-Hillel etal., 1964).
Extension to tree adjoining grammarsand linear indexed grammars was proposed beforeby (Lang, 1994) and (Vijay-Shanker and Weir,1993b).4.1 Intersection of string languagesLet us assume aWLIG G with terminal and nonter-minal labels.
Furthermore, we assume a weightedfinite automaton A, with an input alphabet equalto the set of terminal labels of G. The transitionsof A are of the form:q a7?
q?
?w?,where q and q?
are states, a is a terminal symbol,and w is a weight.
To simplify the presentation,18out(s?)
= ?(s?
= s`) + ?s0[??]
?
A(s1[ ] ?
?
?
sj [??]
?
?
?
sm[ ]) ?w?k ?
{1, .
.
.
, sj?1, sj+1, .
.
.
, sm} s.t.
s?
= skw ?
out(s0, s) ?
in(sj , s) ?
?p /?
{j, k}in(sp) +?s0[??]
?
A(s1[ ] ?
?
?
sj [??]
?
?
?
sm[ ]) ?w?k ?
{1, .
.
.
, sm} s.t.
s?
= skw ?
out(s0) ?
?p 6= kin(sp) +?s0[??]
?
s1[???]
?w?s2[???]
?
s?[??]
?v?w ?
v ?
out(s0) ?
in(s1, s2)out(s?, s) = ?s0[??]
?
A(s1[ ] ?
?
?
sj [??]
?
?
?
sm[ ]) ?w?s?
= sjw ?
out(s0, s) ?
?p 6= jin(sp) +?s0[??]
?
s?[???]
?w?s[???]
?
s3[??]
?v?w ?
v ?
out(s0, s4) ?
in(s3, s4) +?s0[??]
?
s1[???]
?w?s2[???]
?
s?[??]
?v?w ?
v ?
out(s0, s) ?
in(s1, s2) +?s0[??]
?
s?[???]
?w?s[???]
?
s3[??]
?v?w ?
v ?
out(s0) ?
in(s3)Figure 3: The outside values in a WLIG.we ignore epsilon transitions, and assume there isa unique initial state q` and a unique final state qa.We can construct a new WLIG G?
whose gen-erated language is the intersection of the languagegenerated by G and the language accepted by A.The rules of G?
are:1.
(q0, s0, qm)[??]
?A( (q0, s1, q1)[ ] ?
?
?
(qj?2, sj?1, qj?1)[ ](qj?1, sj , qj)[??
](qj , sj+1, qj+1)[ ] ?
?
?
(qm?1, sm, qm)[ ] ) ?w?,for each rule s0[??]
?
A(s1[ ] ?
?
?
sj?1[ ]sj [??]
sj+1[ ] ?
?
?
sm[ ]) ?w?
from G and se-quence q0, .
.
.
, qm of states from A;2.
(q, s, q)[ ] ?
A() ?w?, for each rule s[ ] ?A() ?w?
from G and state q from A;3.
(q, s, q?
)[ ] ?
a ?w ?
v?, for each rule s[ ] ?a ?w?
from G and transition q a7?
q?
?v?
fromA;4.
(q, s, q?)[??]
?
(q, s?, q?)[???]
?w?, for eachrule s[??]
?
s?[???]
?w?
from G and statesq, q?
from A;5.
(q, s, q?)[???]
?
(q, s?, q?)[??]
?w?, for eachrule s[???]
?
s?[??]
?w?
from G and statesq, q?
from A.The new states (q, s, q?)
give (left-most) deriva-tions in G?
that each simultaneously represent one(left-most) derivation in G of a certain substring,starting from state s, and one sequence of transi-tions taking the automaton A from state q to stateq?
while scanning the same substring.
The initialstate of G?
is naturally (q`, s`, qa), which derivesstrings in the intersection of the original two lan-guages.Further note that each derivation in G?
has aweight that is the product of the weight of the cor-19responding derivation in G and the weight of thecorresponding sequence of transitions in A. Thisallows a range of useful applications.
For exam-ple, if A is deterministic (the minimum require-ment is in fact absence of ambiguity) and if it as-signs the weight one to all transitions, then G?
gen-erates a set of trees that is exactly the subset oftrees generated by G whose yields are accepted byA.
Furthermore, the weights of those derivationsare preserved.
If G is a consistent PLIG in TAG-normal form, and if A accepts the language of allstrings containing a fixed substring x, then the sumof probabilities of all derivations in G?
gives thesubstring probability of x.
The effective computa-tion of this probability was addressed in Section 3.An even more restricted, but perhaps more fa-miliar case is if A is a linear structure that acceptsa single input string y of length n. Then G?
gen-erates exactly the set of trees generated by G thathave y as yield.
In other words, the string y isthereby parsed.If G is binary, i.e.
all rules have at most twostates in the right-hand side, then G?
has a sizethat is cubic in n. This may seem surprising, inthe light of the awareness that practical parsing al-gorithms for tree adjoining grammars have a timecomplexity of no less thanO(n6).
However, in or-der to solve the recognition problem, an analysisis needed to determine whether G?
allows at leastone derivation.The analysis from Figure 1 requires O(|S?| +|R?1| ?
|S?| + |R?2| + |R?3| ?
|R?4| ?
|S?|) steps, where|S?| = O(n2) is the number of states of G?, and|R?1| = O(n3), |R?2| = |R?3| = |R?4| = O(n2) arethe numbers of rules of G?, divided into the fourmain types.
This leads to an overall time com-plexity of O(n6), as expected.The observation that recognition can be harderthan parsing was made before by (Lang, 1994).The central new insight this provided was that thenotion of ?parsing?
is ill-defined in the literature.One may choose a form in which to capture allparses of an input allowed by a grammar, but dif-ferent such forms may incur different costs of ex-tracting individual parse trees.In Section 6.2 we will consider the complexityof parsing and recognition if G is not binary.4.2 Intersection of tree languagesWe now shift our attention from strings to trees,and consider the intersection of the tree languagegenerated by a weighted linear indexed grammarG1 and the tree language generated by a weightedregular tree grammar G2.
This intersection is gen-erated by another weighted linear indexed gram-mar G, which has the following rules:1.
(s0, q0)[??]
?
A( (s1, q1)[ ] ?
?
?
(sj?1, qj?1)[ ](sj , qj)[??
](sj+1, qj+1)[ ] ?
?
?
(sm, qm)[ ] ) ?w ?
v?,for each rule s0[??]
?
A(s1[ ] ?
?
?
sj?1[ ]sj [??]
sj+1[ ] ?
?
?
sm[ ]) ?w?
from G1 and eachrule q0 ?
A(q1 ?
?
?
qm) ?v?
from G2;2.
(s, q)[ ] ?
A() ?w ?
v?, for each rule s[ ] ?A() ?w?
from G1 and each rule q ?
A() ?v?from G2;3.
(s, q)[??]
?
(s?, q)[???]
?w?, for each rules[??]
?
s?[???]
?w?
from G1 and state q fromG2;4.
(s, q)[???]
?
(s?, q)[??]
?w?, for each rules[???]
?
s?[??]
?w?
from G1 and state q fromG2.Much as in the previous section, each (left-most) derivation in G corresponds to one (left-most) derivation in G1 and one in G2.
Further-more, these three derivations derive the same la-belled tree, and a derivation in G has a weight thatis the product of the weights of the correspondingderivations in G1 and G2.It can be instructive to look at special cases.Suppose that G2 is an unambiguous regular treegrammar of size O(n) generating a single tree twith n vertices, assigning weight one to all itsrules.
Then the above construction can be seenas parsing of that tree t. The sum of weights ofderivations in G then gives the weight of the treein G1.
See Section 3 once more for a general wayto compute this weight, as the inside value of theinitial state of G, which is naturally (s`, q`).In order to do recognition of t, or in other words,to determine whether G allows at least one deriva-tion, the analysis from Figure 1 can be used, whichhas time complexity O(|S| + |R1| ?
|S| + |R2|+ |R3| ?
|R4| ?
|S|), where |S| = O(n) is thenumber of states of G, and the numbers of rulesare |R1| = O(n), |R2| = |R3| = |R4| = O(n).Note that |R1| = O(n) because we have assumedthat G2 allows only one derivation of one tree t,20hence q0 uniquely determines q1, .
.
.
, qm.
Over-all, we obtain O(n3) steps, which concurs with aknown result about the complexity of TAG parsingof trees, as opposed to strings (Poller and Becker,1998).Another special case is if WLIG G1 simplifiesto a WRTG (i.e.
the stacks of indices remain al-ways empty), which means we compute the inter-section of two weighted regular tree grammars G1and G2.
For recognition, or in other words to de-cide non-emptiness of the intersection, we can stilluse Figure 1, although now only inference rules(b) and (d) are applicable (with a small refinementto the algorithm we can block spurious applicationof (a) where no rules exist that pop indices.)
Thecomplexity is determined by (d), which requiresO(|G1| ?
|G2|) steps.5 Parameter estimationPLIGs allow finer description of probability distri-butions than PRTG, both over string languages andover tree languages.
However, the (string) pars-ing complexity of regular tree grammars is O(n3)and that of LIGs is O(n6).
It may therefore bepreferable for reasons of performance to do pars-ing with a PRTG even when a PTAGs or PLIG isavailable with accurately trained probabilities.
Al-ternatively, one may do both, with a PRTG usedin a first phase to heuristically reduce the searchspace.This section outlines how a suitable PRTG G2can be extracted out of a PLIG G1, assuming theunderlying RTG G?2 without weights is alreadygiven.
The tree language generated by G?2 may bean approximation of that generated by G1.
The ob-jective is to make G2 as close as possible to G1 interms of probability distributions over trees.
Weassume that G?2 is unambiguous, that is, for eachtree it generates, there is at most one derivation.The procedure is a variant of the one describedby (Nederhof, 2005).
The idea is that derivationsin G1 are mapped to those in G?2, via the trees in theintersection of the two tree languages.
The proba-bility distribution of states and rules in G2 is esti-mated based on the expected frequencies of statesand rules from G?2 in the intersection.Concretely, we turn the RTG G?2 into a PRTGG?
?2 that is obtained simply be assigning weightone to all rules.
We then compute the intersec-tion grammar G as in Section 4.2.
Subsequently,the inside and outside values are computed for G,as explained in Section 3.
The expected number ofoccurrences of a rule in G of the form:(s0, q0)[??]
?
A( (s1, q1)[ ] ?
?
?
(sj?1, qj?1)[ ](sj , qj)[??
](sj+1, qj+1)[ ] ?
?
?
(sm, qm)[ ] ) ?w ?
v?,is given by multiplying the outside and insideprobabilities and the rule probability, as usual.We get two terms however that we need to sum.The intuition is that we must count both rule oc-currences used for building initial TAG trees andthose used for building auxiliary TAG trees.
Thisgives:w ?
v ?
out((s0, q0)) ?
?kin((sk, qk)) +w ?
v ?
?s,qout((s0, q0), (s, q)) ?in((sj , qj), (s, q)) ?
?k 6=jin((sk, qk))By summing these expected numbers for differentrules s0[??]
?
A(s1[ ] ?
?
?
sj?1[ ] sj [??]
sj+1[ ]?
?
?
sm[ ]), we obtain the expected number of oc-currences of q0 ?
A(q1 ?
?
?
qm), Let us denotethis sum by E(q0 ?
A(q1 ?
?
?
qm)).
By summingthese for fixed q0, we obtain the expected numberof occurrences of q0, which we denote by E(q0).The probability of q0 ?
A(q1 ?
?
?
qm) in G2 is thenset to be the ratio of E(q0 ?
A(q1 ?
?
?
qm)) andE(q0).By this procedure, the Kullback-Leibler dis-tance between G1 and G2 is minimized.
Althoughthe present paper deals with very different for-malisms, the proof of correctness is identical tothat in (Nederhof, 2005).
The reason is that in bothcases the mathematical analysis must focus on theobjects in the intersection (strings or trees) whichmay correspond to multiple derivations in the orig-inal model (here G1) but to a single derivation inthe unambiguous model to be trained (here G2),and each derivation is composed of rules, whoseprobabilities are to be multiplied.6 Extensions6.1 TransductionFor various formalisms describing (string or tree)languages, there are straightforward generaliza-tions that describe a relation between two or more21languages, which is known as a transduction.
Theidea is that the underlying control mechanism,such as the states in regular tree grammars or lin-ear indexed grammars, is now coupled to two ormore surface forms that are synchronously pro-duced.
For example, a rule in a weighted syn-chronous regular tree grammar (WSRTG) has theform:s0 ?
A(s1 ?
?
?
sm), B(spi(1) ?
?
?
spi(m)) ?w?,where pi is a permutation of 1, .
.
.
,m. We can gen-eralize this to having a third label C and a secondpermutation pi?, in order to describe simultaneousrelations between three tree languages, etc.
In thissection we will restrict ourselves to binary rela-tions however, and call the first surface form theinput and the second surface form the output.
Forsynchronous tree adjoining grammars, see for ex-ample (Shieber, 1994).If we apply intersection on the input or on theoutput of a synchronous grammar formalism, thenthis is best seen as composition.
This is well-known in the case of finite-state transducers andsome forms of context-free transduction (Berstel,1979), and application to a wider range of for-malisms is gaining interest in the area of machinetranslation (Knight, 2007).With the intersection from Section 4.2 triviallyextended to composition, we can now implementcomposition of the form:?1 ?
.
.
.
?
?k,where the different ?j are transducers, of whichk ?
1 are (W)SRTGs and at most one is a(weighted) synchronous LIG ((W)SLIG).
The re-sult of the composition is another (W)SLIG.
Itshould be noted that a (W)RTS (or (W)LIG) canbe seen as a (W)SRTG (or (W)SLIG, respectively)that represents the identity relation on its tree lan-guage.6.2 BinarizationIn the discussion of complexity in Section 4.1, weassumed that rules are binary, that is, that theyhave at most two states in each right-hand side.However, whereas any context-free grammar canbe transformed into a binary form (e.g.
Chomskynormal form), the grammars as we have definedthem cannot be.
We will show that this is to a largeextent a consequence of our definitions, whichwere motivated by presentational ease, rather thanby generality.The main problem is formed by rules of theform s0 ?
A(s1 ?
?
?
sm), where m > 2.
Suchlong rules cannot be broken up into shorter rulesof the same form, as this would require an addi-tional labelled vertex, changing the tree language.An apparent solution lies in allowing branchingrules without any label, for example s1 ?
s2 s3.Regrettably this could create substantial computa-tional problems for intersection of the describedtree languages.
As labels provide the mechanismthrough which to intersect tree languages, rulesof the above form are somewhat similar to unitrules or epsilon rules in context-free grammars, inthat they are not bound to observable elements.Branching rules furthermore have the potentialto generate context-free languages, and thereforethey are more pernicious to intersection, consider-ing that emptiness of intersection of context-freelanguages is undecidable.It therefore seems better to restrict branchingrules s1 ?
s2 s3 to finite-state power, for exam-ple by making these rules exclusively left-linearor right-linear.
A more elegant but equivalent wayof looking at this may be to have rules of the form:s0 ?
A(R),where R is a regular language over states.
In thecase of linear indexed grammars, we would haverules of the form:s[??]
?
A(L s?[??]
R)where L and R are regular languages over expres-sions of the form s[ ].
Appropriate weighted fi-nite automata can be used to assign weights to se-quences of such expressions in L and R. Withthese extended types of rules, our constructionfrom Section 4.2 still works.
The key observationhere is that regular languages are closed under in-tersection.One of the implications of the above extendeddefinitions is that labels appear not only with-out fixed ranks, as we have assumed from thestart in Section 2, but even without a bound onthe rank.
Concretely, a vertex may appear withany number of children in a tree.
Whereas thismay be unconventional in certain areas of formallanguage theory, it is a well-accepted practice inthe parsing of natural language to make the num-ber of constituents of syntactic categories flexi-ble and conceptually unbounded; see for example22(Collins, 1997).
Also the literature on unrankedtree automata is very relevant; see for example(Schwentick, 2007).
Binarization for LIGs wasconsidered before by (Vijay-Shanker and Weir,1993a).6.3 Beyond TAGsIn the light of results by (Kepser and Mo?nnich,2006) it is relatively straightforward to considerlarger classes of linear context-free tree grammarsin place of tree-adjoining grammars, in order togeneralize the construction in Section 4.2.The generalization described in what followsseems less straightforward.
Context-free lan-guages can be characterized in terms of parse treesin which path sets (sets of strings of labels onpaths from the root to a leaf) are regular.
In thecase of tree adjoining languages, the path sets arecontext-free.
There is a hierarchy of classes of lan-guages in which the third step is to consider pathsets that are tree adjoining languages (Weir, 1992).In this paper, we have considered the parsing-as-intersection paradigm for the first two members ofthe hierarchy.
It may be possible that the paradigmis also applicable to the third and following mem-bers.
This avenue is yet to be pursued.7 ConclusionsThis paper has extended the parsing-as-intersection paradigm from string languagesto tree languages.
Probabilities, or weights ingeneral, were incorporated in this framework in anatural way.
We have discussed one particular ap-plication involving a special case of the extendedparadigm.AcknowledgementsHelpful comments by anonymous reviewers aregratefully acknowledged.
The basic result fromSection 4.1 as it pertains to RTGs as subclass ofLIGs was discussed with Heiko Vogler, who pro-posed two alternative proofs.
Sylvain Schmitzpointed out to me the relevance of literature on lin-ear context-free tree languages.ReferencesA.V.
Aho, M.S.
Lam, R. Sethi, and J.D.
Ullman.2007.
Compilers: Principles, Techniques, & Tools.Addison-Wesley.Y.
Bar-Hillel, M. Perles, and E. Shamir.
1964.
Onformal properties of simple phrase structure gram-mars.
In Y. Bar-Hillel, editor, Language and Infor-mation: Selected Essays on their Theory and Appli-cation, chapter 9, pages 116?150.
Addison-Wesley,Reading, Massachusetts.J.
Berstel.
1979.
Transductions and Context-Free Lan-guages.
B.G.
Teubner, Stuttgart.M.
Collins.
1997.
Three generative, lexicalised mod-els for statistical parsing.
In 35th Annual Meeting ofthe Association for Computational Linguistics, Pro-ceedings of the Conference, pages 16?23, Madrid,Spain, July.J.
Graehl and K. Knight.
2004.
Training tree transduc-ers.
In HLT-NAACL 2004, Proceedings of the MainConference, Boston, Massachusetts, USA, May.F.
Gcseg and M. Steinby.
1997.
Tree languages.
InG.
Rozenberg and A. Salomaa, editors, Handbookof Formal Languages, Vol.
3, chapter 1, pages 1?68.Springer, Berlin.J.E.
Hopcroft and J.D.
Ullman.
1979.
Introductionto Automata Theory, Languages, and Computation.Addison-Wesley.A.K.
Joshi and Y. Schabes.
1997.
Tree-adjoininggrammars.
In G. Rozenberg and A. Salomaa, edi-tors, Handbook of Formal Languages.
Vol 3: BeyondWords, chapter 2, pages 69?123.
Springer-Verlag,Berlin/Heidelberg/New York.D.
Jurafsky and J.H.
Martin.
2000.
Speech and Lan-guage Processing.
Prentice-Hall.S.
Kepser and U. Mo?nnich.
2006.
Closure propertiesof linear context-free tree languages with an appli-cation to optimality theory.
Theoretical ComputerScience, 354:82?97.K.
Knight.
2007.
Capturing practical natural languagetransformations.
Machine Translation, 21:121?133.B.
Lang.
1994.
Recognition can be harder than pars-ing.
Computational Intelligence, 10(4):486?494.M.-J.
Nederhof and G. Satta.
2003.
Probabilistic pars-ing as intersection.
In 8th International Workshopon Parsing Technologies, pages 137?148, LORIA,Nancy, France, April.M.-J.
Nederhof.
2005.
A general technique to trainlanguage models on language models.
Computa-tional Linguistics, 31(2):173?185.P.
Poller and T. Becker.
1998.
Two-step TAG pars-ing revisited.
In Fourth International Workshop onTree Adjoining Grammars and Related Frameworks,pages 143?146.
Institute for Research in CognitiveScience, University of Pennsylvania, August.23P.
Resnik.
1992.
Probabilistic tree-adjoining grammaras a framework for statistical natural language pro-cessing.
In Proc.
of the fifteenth International Con-ference on Computational Linguistics, pages 418?424.
Nantes, August.A.
Sarkar.
1998.
Conditions on consistency of prob-abilistic tree adjoining grammars.
In 36th AnnualMeeting of the Association for Computational Lin-guistics and 17th International Conference on Com-putational Linguistics, volume 2, pages 1164?1170,Montreal, Quebec, Canada, August.Y.
Schabes.
1992.
Stochastic lexicalized tree-adjoining grammars.
In Proc.
of the fifteenth Inter-national Conference on Computational Linguistics,pages 426?432.
Nantes, August.Thomas Schwentick.
2007.
Automata for XML?asurvey.
Journal of Computer and System Sciences,73:289?315.S.M.
Shieber.
1994.
Restricting the weak-generativecapacity of synchronous tree-adjoining grammars.Computational Intelligence, 10(4):371?385.K.
Sima?an.
1997.
Efficient disambiguation by meansof stochastic tree substitution grammars.
In D. Jonesand H. Somers, editors, New Methods in LanguageProcessing.
UCL Press, UK.S.
Sippu and E. Soisalon-Soininen.
1988.
Parsing The-ory, Vol.
I: Languages and Parsing, volume 15 ofEATCS Monographs on Theoretical Computer Sci-ence.
Springer-Verlag.K.
Vijay-Shanker and D.J.
Weir.
1993a.
Parsing someconstrained grammar formalisms.
ComputationalLinguistics, 19(4):591?636.K.
Vijay-Shanker and D.J.
Weir.
1993b.
The use ofshared forests in tree adjoining grammar parsing.
InSixth Conference of the European Chapter of the As-sociation for Computational Linguistics, Proceed-ings of the Conference, pages 384?393, Utrecht, TheNetherlands, April.K.
Vijay-Shanker and D.J.
Weir.
1994.
The equiva-lence of four extensions of context-free grammars.Mathematical Systems Theory, 27:511?546.D.J.
Weir.
1992.
A geometric hierarchy beyondcontext-free languages.
Theoretical Computer Sci-ence, 104:235?261.D.
Wojtczak and K. Etessami.
2007.
PReMo: an an-alyzer for Probabilistic Recursive Models.
In Toolsand Algorithms for the Construction and Analysisof Systems, 13th International Conference, volume4424 of Lecture Notes in Computer Science, pages66?71, Braga, Portugal.
Springer-Verlag.24
