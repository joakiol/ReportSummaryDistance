Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 588?597,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUsing Zero-Resource Spoken Term Discovery for Ranked RetrievalJerome WhiteNew York UniversityAbu Dhabi, UAEjerome.white@nyu.eduDouglas W. OardUniversity of MarylandCollege Park, MD USAoard@umd.eduJiaul PaikUniversity of MarylandCollege Park, MD USAjiaul@umd.eduRashmi SankepallyUniversity of MarylandCollege Park, MD USArashmi@umd.eduAren JansenJohn Hopkins HLTCOEBaltimore, MD USAaren@jhu.eduAbstractResearch on ranked retrieval of spoken con-tent has assumed the existence of some auto-mated (word or phonetic) transcription.
Re-cently, however, methods have been demon-strated for matching spoken terms to spokencontent without the need for language-tunedtranscription.
This paper describes the firstapplication of such techniques to ranked re-trieval, evaluated using a newly created testcollection.
Both the queries and the collectionto be searched are based on Gujarati producednaturally by native speakers; relevance assess-ment was performed by other native speak-ers of Gujarati.
Ranked retrieval is based onfast acoustic matching that identifies a deeplynested set of matching speech regions, cou-pled with ways of combining evidence fromthose matching regions.
Results indicate thatthe resulting ranked lists may be useful forsome practical similarity-based ranking tasks.1 IntroductionDespite new methods of interaction, speech contin-ues to be a dominant modality for information ex-change, particularly among the half of the world?salmost five billion mobile phone users who currentlylack text-based Internet access.
Recording speechposes no particular problems, but retrieval of spokencontent using spoken queries is presently availableonly for the approximately two dozen languages inwhich there is an established path to market; En-glish, German, or Chinese, for example.
However,many of the mobile-only users who could benefitmost from such systems speak only one of the sev-eral hundred other languages that each have at leasta million speakers;1Balochi, Mossi or Quechua, forexample.
Addressing this challenge in a scalablemanner requires an integration of speech processingand information retrieval techniques that can be ef-fectively and affordably extended to a large numberof languages.To this end, the experiments in this paper wereconducted in a conventional ranked retrieval frame-work consisting of spoken queries, spoken ?doc-uments?
(responses, hereafter), graded relevancejudgments, and standard evaluation measures.
Aswith other information retrieval tasks, there is an el-ement of uncertainty in our best representations ofwhat was said.
Our focus on speech processing tech-niques that are language-agnostic creates the poten-tial for explosive growth in the uncertainty that oursearch techniques must accommodate.
The designand evaluation of such techniques is therefore thecentral focus of the work explored in this paper.Our results are both heartening and disconcerting.On the positive side, useful responses can often befound.
As one measure of success, we show thata Mean Reciprocal Rank near 0.5 can be achievedwhen more than one relevant response exists; thiscorresponds to a relevant response appearing in thesecond position of a ranked list, on average (by theharmonic mean).
On the negative side, the zero-resource speech processing technique that we relyon to generate indexing terms has quadratic timecomplexity, making even the hundred-hour scale of1There are 393 languages with at least one million speakersaccording to Ethnologue.588the collection on which we have run our experi-ments computationally strenuous.
We believe, how-ever, that by demonstrating the utility of the tech-niques introduced in this paper we can help to moti-vate further work on even more affordable scalablelanguage-agnostic techniques for generating index-able terms from speech.2 Motivation and Related WorkExtending spoken language processing to low-resource languages has been a longstanding goal ofthe Spoken Web Search task of MediaEval.
In thistask, research teams are challenged to identify in-stances of specific spoken terms that are providedas queries in a few hours of speech.
Between 2011and 2013, the task was run three times on a totalof 16 different languages (Rajput and Metze, 2011;Metze et al, 2012; Anguera et al, 2013).2Twobroad classes of techniques over this span provedto be practical: one based on phonetic recognitionfollowed by phonetic matching; the other based ondirect matching of acoustic features.
Of the twoapproaches, phonetic recognition was, at the time,slightly more accurate.
Directly matching acousticfeatures, the focus of this paper, potentially offerseasier extensibility to additional languages.From the perspective of information retrieval, theprincipal limitation of the ?spoken term detection?design of the MediaEval task was the restriction tosingle-term queries.
While single-term queries arecommon in Web search (Spink et al, 2001), thebest reported Actual Term Weighted Value (ATWV)from any MediaEval Spoken Web Search participantwas 0.4846 (Abad and Astudillo, 2012).
This cor-responds to a system that correctly detects 48 percent of all instances of the spoken query terms, whileproducing at most ten false alarms for every misseddetection (Fiscus et al, 2007).
Thus, if users arewilling to tolerate low precision, moderate levelsof recall are possible.
Speech search arguably de-mands higher precision than does Web search, how-ever, since browsing multiple alternatives is eas-ier in text than in speech.
One way of potentiallyimproving retrieval performance is to encourage asearcher to speak at length about what they are look-2For example, Gujarati, isiNdebele, isiXhosa, Sepedi,Setswana, Telugu, Tshivenda, and Xitsonga.ing for (Oard, 2012).
Such an approach, however,introduces the new challenge of properly leveragingthe additional matching potential of verbose multi-term queries (White et al, 2013).To this end, our work builds on two components:a term matching system, and a test collection.
As aterm matching system, we used our zero-knowledgespeech matching system.
In MediaEval 2012, thissystem achieved an ATWV of 0.321 in the Spo-ken Web Search task (Jansen et al, 2012).
A ver-sion of this system has previously been evaluatedin an example-based topic classification task usingEnglish speech, achieving a classification accuracyof 0.8683 (Drezde et al, 2010).
Ranked retrievalusing naturally occurring queries is more challeng-ing, however, both because topics in information re-trieval are often not easily separable, and becausethe form of a query may be unlike the form of theresponses that are sought.
Our goal now, therefore,is to use an information retrieval evaluation frame-work to drive the development of robust techniquesfor accommodating representational uncertainty.Traditional spoken term detection (STD) triesto address uncertainty by learning speech-signalto language-model mappings; using neural net-works (Cui et al, 2013; Gales et al, 2014) orMarkov models (Chan et al, 2013), for example.From a broad perspective, the method utilized in ourwork does not use an acoustic model for its analy-sis.
More fundamentally, however, speech signals inour collection map to dozens of smaller terms thatare not necessarily the same across utterances of thesame word.
Thus, it is more accurate to think of thework herein as matching signal features rather thanlinguistic features.For this reason, widely used techniques such asstemming, spelling correction, and stopword re-moval that rely to some extent on linguistic featuresdo not apply in our setting.
We therefore rely onterm and corpus statistics.
Even here there are limi-tations, since our lexical items are not easily alignedwith those found in other collections.
For this rea-son, we can not leverage external corpus statisticsfrom, for example, Google or Wikipedia (Benderskyet al, 2011; Bendersky et al, 2010; Bendersky andCroft, 2008; Lease, 2009), or phrases from searchlogs (Svore et al, 2010).Evaluation of ranked retrieval for spoken content589x2x1y1y2 r1(a) Term discovery.x1, x2[ ] y1, y2[ ]r1x3, x4[ ] y3, y4[ ]r2?
xm?1, xm[ ] rn ym?1, ym[ ](b) Term extraction.x5, x6[ ] y5, y6[ ]r3x7, x8[ ] y7, y8[ ]r4x1, x2[ ] y1, y2[ ]r1x3, x4[ ] y3, y4[ ]r2e1e2(c) Term overlap.Cluster?2?x5, x6[ ] y5, y6[ ]r3x7, x8[ ] y7, y8[ ]r4e2 Cluster?1?x1, x2[ ] y1, y2[ ]r1x3, x4[ ] y3, y4[ ]r2e1(d) Term clustering.Figure 1: Overview of the pseudo-term creation process.
The term discovery system is run over the audio.A threshold, ?, dictates the acceptable length, r, and thus the number of regions extracted.
Extracted regionsare then made into a graph structure, where vertices are regions of speech, and edges denote a connection be-tween those regions.
A second edge set is added based on region overlap.
Resulting connected componentsare then clustered; these clusters are known as pseudo-terms.in low-resource languages has to date been ham-pered by a lack of suitable test collections.
We havetherefore made our new test collection freely avail-able for research use in recent shared-task informa-tion retrieval evaluations (Oard et al, 2013; Joshiand White, 2014).3 Zero-Resource Term DiscoveryIn traditional speech retrieval applications,document-level features are derived from theoutputs of supervised phonetic or word recognizers.Recent term discovery systems automatically iden-tify repeating words and phrases in large collectionsof audio (Park and Glass, 2008; Jansen et al,2010), providing an alternative means of extractinglexical features for retrieval tasks.
Critically, thisdiscovery is performed without the assistance ofany supervised speech tools by instead resortingto a search for repeated trajectories in a suitableacoustic feature space (for example, Mel FrequencyCepstrum Coefficients (MFCC) and PerceptualLinear Prediction (PLP)) followed by a graphclustering procedure.
We refer to the discoveredunits as pseudo-terms (by analogy to the termsbuilt from character sequences that are commonlyused in text retrieval), and we can represent eachquery and response as a set of pseudo-term offsetsand durations.
We summarize each step in thesubsections below.
Complete specifications can befound in the literature (Drezde et al, 2010; Jansenand Van Durme, 2011).3.1 Repetition and ClusteringOur test collection consists of nearly 100 hoursof speech audio.
Term discovery is inherentlyan O(n2) search problem, and application to a cor-pus of this size is unprecedented in the literature.We applied the scalable system described by Jansenand Van Durme (2011), which employs a pure-to-noisy strategy to achieve a very substantial (orders-of-magnitude) speedup over its predecessor state-of-the-art system (Park and Glass, 2008).
The systemfunctions by constructing a sparse (thresholded) dis-tance matrix across the frames of the entire corpusand then searching for approximately diagonal linestructures in that matrix, as such structures are in-dicative that a word or phrase has been repeated(Figure 1a).To cluster the individual acoustic repetitions intopseudo-term categories we apply a simple graph-based procedure.
First, we construct an unweightedacoustic similarity graph, where each segment ofspeech involved in a discovered repetition becomes avertex, and each match provides an edge (Figure 1b).Since we construct an unweighted graph and employa simple connected-components clustering, it is es-590??????????????????????????????????
???
???
???
???
???
???
???
???
????????????????????????
???????????????
??????
????
?Figure 2: Different pseudo-term nesting structures for various settings of the speech-to-term extractionmodel.
The y-axis represents the number of terms extracted at a given period in time.
This figure representsan approximately twenty second interval of Query 42.sential some DTW distance threshold ?
is appliedbefore a repetition is passed along to the clusteringprocedure.
This produces a graph consisting of a setof disconnected ?dumbbells.
?Finally, the original edge list is augmented with aset of ?overlap?
edges between corresponding nodesin different dumbbells (Figure 1c); these overlapedges indicate that two nodes correspond to essen-tially the same segment of speech.
For two nodes(two segments of speech) to be considered essen-tially the same, we require a minimal fractional over-lap of 0.97, which is set less than unity to allowsome noise in the segment end points.
These over-lap edges act to effectively merge vertexes acrossthe dumbbells, enabling transitive matches betweenacoustic segments that did not match directly.
Thepseudo-terms are defined to be the resulting con-nected components of the graph, each consisting of aset of corresponding acoustic segments that can oc-cur anywhere in the collection (Figure 1d).In the experiments described in this paper, threepseudo-term feature variants arising from three set-tings of the DTW distance threshold are considered.Lower thresholds imply higher fidelity matches thatyield fewer and purer pseudo-term clusters.
Theseare referred to as pure clustering (?
= 0.06, produc-ing 406,366 unique pseudo-terms), medium cluster-ing (?
= 0.07, producing 1,213,223 unique pseudo-terms) and noisy clustering (?
= 0.075, producing1,503,169 unique pseudo-terms).3.2 Nested Pseudo-TermsEach pseudo-term cluster consists of a list of occur-rences.
A term is denoted using start and end off-sets, in units of 10 milliseconds, from the beginningof the file.
It is thus a simple matter of bookkeep-ing to construct a bag-of-pseudo-terms representa-tion for each query and response.
Moreover, becausewe have start and end offsets for each pseudo-term,we can also construct more sophisticated represen-tations that are based on filtering or grouping thepseudo-terms based on the ways in which they over-lap temporally.One interesting effect of pseudo-term creation isthat the pseudo-terms are often ?nested,?
and theyare often nested quite deeply.
This sort of nest-ing has previously been explored for phrase index-ing, where a longer term contains a shorter termthat might also be used independently elsewherein the collection.
As an English text analogy, ifwe index ?White House spokesman?
we might wellalso want to index ?White House?
and ?spokesman?591??????????????????????????????????????????????????????????
??
????
??
???????????????????????????????????????????????
?Figure 3: Example of overlapping pseudo-termswithin Query 42 under medium clustering.
Termsare presented as horizontal bars denoting their startand end time.separately to support partial matching.
Becausepseudo-term detection can find any pair of match-ing regions, we could, continuing the analogy, notonly get pseudo-terms for ?White House Spokes-man?
and ?White House,?
but also for parts of thosewords such as ?Whit?
and ?Whi?.
Indeed, nestingto depth 50 has been observed in practice for noisyclustering, as displayed in Figure 2.
This is a fairlytypical pseudo-term nesting graph, in which noisyclustering yields deeper nesting than medium clus-tering, and much deeper nesting than pure cluster-ing.Figure 3 shows a collection of pseudo-termswithin an overlapping region; in this case a mediumclustering representation of the 1.48 second to3.67 second region of Query 42.3As can be seen,calling this ?nesting?
is somewhat of an oversimpli-fication, the region is actually a set of pseudo-termsthat generally overlap to some degree, although notall pseudo-term pairs in one of these ?nested?
re-gions actually overlap?pseudo-terms P1 and P21,for example.
What gives a nested region its depth3Figure 2 shows the same query between 70 and 90 seconds.is the overlap between pseudo-terms that have adja-cent start times.
Although in this case, as is typi-cal, there is no one dominating pseudo-term for theentire nested region, there are some cases in whichone pseudo-terms is entirely subsumed by another;pseudo-terms P5 and P6, for example.
This trait canbe leveraged during term matching.4 Retrieval ModelsThe development of ranking functions, referred toas ?retrieval models,?
proceeded in three stages.
Toestablish a baseline, we first implemented a stan-dard bag-of-words approach.
We then looked totechniques from Cross-Language Information Re-trieval (CLIR) for inspiration, since CLIR tech-niques must accommodate some degree of transla-tion ambiguity and for which robust techniques havebeen established.
Our zero-resource pseudo-termdiscovery techniques result in representations thatdiffer from the CLIR case in two key ways, however:1) in CLIR the translation relationship is normallyrepresented such that one side (query or document)exhibits no ambiguity, whereas we have ambiguityon both sides; and 2) in CLIR the typical scope of alltranslation alternatives are aligned, whereas we havecomplex nested units that contain terms with differ-ing temporal extents.
We therefore developed a newclass of techniques that leverage the temporal extentof a pseudo-term as a measure of specificity (Fig-ure 2) and the fraction of a nested unit covered bya pseudo-term as a measure of descriptiveness (Fig-ure 3).
This section describes each of these threetypes of retrieval models in turn.Indri (Strohman et al, 2004) indexes were builtusing pseudo-terms from pure, medium or noisyclustering; in each case, stemming and stopword re-moval were disabled.
Indri?s query language pro-vides operators that make it possible to implementall of our retrieval models using query-time process-ing from a single index.4.1 Types of Retrieval ModelsTo explore the balance between specificity and de-scriptiveness, retrieval models were developed thatprimarily differed along three dimensions: struc-tured versus unstructured, selective versus inclusive,and weighted versus unweighted.
Structured mod-592els (S) treat nested pseudo-terms with varying levelsof synonymy.
Unstructured models (U) treat nestedpseudo-terms as independent.
Selective models re-tain only a subset (1 or n) of the pseudo-terms fromeach nested region; inclusive models retain themall (a).
Finally, weighted models (W) include aheuristic adjustment to give some pseudo-terms (inour experiments, longer ones) greater influence; un-weighted models treat each pseudo-term in the samemanner.
Table 1 illustrates the weights given toeach term by each of the retrieval models definedbelow.
Unweighted models implicitly take a binaryapproach to term weighting?with unweighted se-lective models omitting many pseudo-terms?whilestructured and weighted models yield real values be-tween zero and one.
Note that both weighted andunweighted models reward term repetition (term fre-quency) and term specificity (inverse collection fre-quency).4.2 Bag-of-Words Baseline (Ua)Our first set of experiments had three goals: 1) toserve as a dry run for system development, as wehad no prior experience with indexing or ranked re-trieval based on pseudo-terms; 2) to gain experiencewith performing relevance judgments using only theaudio responses; and 3) to understand the feasibilityof speech retrieval based on pseudo-terms.
For theseinitial experiments, each pseudo-term was treated asa ?word?
in a bag-of-words representation (codedUa).
No consideration was given to term length ornesting.
Although this set of runs was largely ex-ploratory, it provided a good baseline for compari-son to other methods considered.4.3 Terms as Synonyms (Sa, U1)Moving beyond the bag of words method of termselection involves various forms of term analysiswithin an overlapping region.
The first family ofmethods treats terms in each overlapping group assynonymous.
Aside from being straightforward,treating terms as unweighted synonyms has beena successful technique in cross-language IR.
Thereare generally two methods that can be used in suchcases.
The first is to treat all overlapping pseudo-terms as synonyms of a single term.
This is accom-plished in Indri by placing each pseudo-term in anoverlapping region within the syn operator.
ThisRetrieval ModelP.
Term Ua Sa U1 Un UaW SaWP21 1.00 0.05 1.00 0.45 0.45P20 1.00 0.05 0.43 0.22P19 1.00 0.05 0.48 0.48P18 1.00 0.05 0.36 0.36P17 1.00 0.05 0.45 0.06P16 1.00 0.05 1.00 0.53 0.53P15 1.00 0.05 0.48 0.11P14 1.00 0.05 0.37 0.12P13 1.00 0.05 0.48 0.22P12 1.00 0.05 0.36 0.02P11 1.00 0.05 0.41 0.22P10 1.00 0.05 0.43 0.24P9 1.00 0.05 1.00 0.54 0.54P8 1.00 0.05 0.45 0.45P7 1.00 0.05 0.39 0.04P6 1.00 0.05 0.37 0.03P5 1.00 0.05 0.40 0.13P4 1.00 0.05 0.41 0.08P3 1.00 0.05 0.47 0.47P2 1.00 0.05 0.40 0.22P1 1.00 0.05 1.00 0.46 0.46Table 1: Weights assigned to pseudo-terms in Fig-ure 3 by each retrieval model (zero values shown asblank).model is coded Sa.One risk with the Sa model is that includingshorter terms may add more noise than signal.
An-other method of dealing with alternatives in thecross-language IR literature is to somehow select asingle term from the set.
For our experiments withthis technique, only the longest pseudo-term froman overlapping set is retained; all other (?nested?
)pseudo-terms are simply deleted from the query.The thinking behind this is that the longest termshould contain the greatest amount of information.This method is coded U1.4.4 Length Measure of Specificity (UaW, SaW)The U1 and Sa models are two extremes on a spec-trum of possibilities; thus, models in which somepseudo-terms receive less weight, rather than beingignored entirely, were also explored.
Care must be593taken, however, to do so in a way that emphasizescoverage rather than nesting depth: more weightshould not be given to some region in a query ora response just because it is deeply nested (indicat-ing extreme uncertainty).
Both the U1 and Sa mod-els do this, but in a rather unnuanced manner.
Fora more nuanced approach, inspiration can be foundin techniques from cross-language IR that give moreweight to some term choices than to others.Our basic approach is to downweight terms thatare dominated temporally by several other terms,where the amount of downweighting is proportionalto the number of terms that cover it.
This is im-plemented by adjusting the contribution of eachpseudo-term based on the extent of its overlap withother pseudo-terms.
This could be done in a way thatwould give the greatest weight to either the shortestor the longest nested pseudo-term.Formally, let T = {t1, t2, .
.
.
, tn} be the nestedterm class, ordered by term length.
Let l(ti) denotethe length of term ti, in seconds.
Further, letw(ti) =??
l(ti)1 + ??
l(ti)be the weight of term ti, where ?
is a free parame-ter.
For our experiments, ?
= 0.5.
The discountedweight isd(ti) =????
?w(ti) i = 1w(ti)?i?1?j=1(1?
w(tj)) otherwise,where tjrefers, implicitly, to other members of T .The factor 1 ?
w(ti) is used to discount theweight of tidue to the contribution made by theprevious term(s).
We assume T to be in de-scending order and define two heuristics: totalweight discounted (UaW) and longest weight dis-counted (SaW).
The former uses Indri?s weightoperator to specify term weights at query time; thelatter uses wsyn.4.5 Coverage Measure of Descriptiveness (Un)Recall Figure 3, a visual display of pseudo-termoverlap within an arbitrary region of speech.
Out-side of the bounds of that figure there is eithersilence?no terms to describe a particular segmentof time?or a region of terms that describe someother utterance within the overall speech.
Of par-ticular note, however, is that within the bounds thereare a potentially large number of terms that can beused to describe a region of speech.
Thus, the largerthe number of terms present, the larger the amount ofredundancy in the segment of speech each term de-scribes.
This observation motivates our final querymethodology: removing redundancy within a regionby extracting a seemingly descriptive subset of termsfrom that region.
Here we begin to move beyond theideas inspired by cross-language IR.Specifically, we posit that an optimal subset con-tains the beginning and ending terms of the region,along with a series of intra-terms that connect thetwo.
It is with this logic that the unweighted shortestpath (coded Un) was conceived.
Un attempts to findthe subset that captures the most information usingthe smallest number of terms.
Formally, consider adirected graph in which the set of vertexes is the setof pseudo-terms within an overlapping region.
Foran arbitrary pair of vertexes, u, v ?
V , there is anoutgoing edge from u to v if y(u) ?
x(v), wherex(?)
and y(?)
denote the start and end time, respec-tively, of a given pseudo-term.
Further, the weight ofsuch an edge is the difference between these times:w(u, v) = y(u)?x(v).
Note that an edge between uand v does not exist if they have the same start time,x(u) = x(v).Let u?
and v?
be the endpoints of the graph; that is,for all u, v ?
P , x(u?)
?
x(u), and y(v?)
?
y(v).Our objective is to find the shortest path from u?
to v?that minimizes the standard deviation of the edgeweights.
Minimizing standard deviation results ina set of terms with more uniform overlaps.5 Building a Test CollectionThe test collection was built using actual spokencontent from the Avaj Otalo (Patel et al, 2010)?speech forum,?
an information service that was reg-ularly used by a select group of farmers in Gujarat.These farmers spoke Gujarati, a language native towestern parts of India and spoken by more than65 million people worldwide.
Most of the farmersknew no other language, and approximately 30 percent were unable to read or write.
The idea wasto provide a resource for the local farming commu-nity to exchange ideas and have their questions an-594swered.
To this end, farmers would call into an Inter-active Voice Response (IVR) system and peruse an-swers to existing questions, or would pose their ownquestions for the community.
Other farmers wouldcall into the system to leave answers to those ques-tions.
On occasion, there were also a small group ofsystem administrators who would periodically callin to leave announcements that they expected wouldbe of interest to the broader farming community.The system was completely automated?no humanintervention or call center was involved.Avaj Otalo?s recorded speech was divided into 50queries and 2,999 responses.
Queries were state-ments on a particular topic, sometimes phrased asa question, sometimes phrased as an announcement.Responses were sometimes answers to questions,sometimes they were related announcements, andsometimes they were questions on a similar topic.This represented approximately two-thirds of the to-tal audio present in the system.
Very short record-ings were omitted, as were those in which littlespeech activity was automatically detected.
The av-erage length of a query is approximately 70 sec-onds (SD = 14.40s), or approximately 61 sec-onds (SD = 15.76s) after automated silence re-moval.
Raw response lengths averaged 110 seconds(SD = 88.80s), and 96.52 seconds (SD = 82.75s)after silence was removed.5.1 Relevance Judgments and EvaluationPools for judgment were formed by combining theresults from every system reported in our results sec-tion below, along with several other systems thatyielded less interesting results that we omit for spacereasons.
Three native speakers of Gujarati per-formed relevance assessment; none of the three hadany role in system development.
Relevance assess-ment was performed by listening to the audio andmaking a graded relevance judgment.
Assessorscould assign one of the following judgments for eachresponse: 1) unable to assess, 2) not relevant, 3) rel-evant, and 4) highly relevant.For evaluation measures that require binary judg-ments, and for computing inter-annotator agree-ment, the relevance judgments were subsequentlybinarized by removing all the unassessable cases.Highly relevant and relevant responses were thencollapsed into a single relevant category.
To com-Retrieval ModelU1 Un Ua UaW Sa SaWMRR 0.447 0.281 0.169 0.204 0.235 0.4320.139 0.071 0.081 0.089 0.242 0.0750.188 0.104 0.109 0.193 0.252 0.105MAP 0.106?0.057 0.047 0.060?0.058 0.1110.023 0.011 0.015 0.018 0.050 0.0100.045 0.013 0.018 0.050 0.058 0.022NDCG 0.237 0.216 0.206 0.219 0.214 0.284?0.122 0.098?0.187 0.195 0.243 0.1940.142 0.089?0.219 0.191 0.285 0.230Table 2: Results for pure (top), medium (middle)and noisy (bottom) clustering for the 10 queries forwhich more than one relevant response is known.Shaded cells are best-performers, per measure;starred values indicate NDCG or MAP is signifi-cantly better or worse than same-row Ua (two-sidedpaired t-test, p < 0.05).pute NDCG, relevant and highly relevant categorieswere assigned the scores 1 and 2, respectively, whilenon-relevant judgments retained a score of 0.
Threerounds of relevance assessments were conducted asquery models were developed and assessor agree-ment was characterized.6 ResultsEach retrieval model was run for each of the threeclustering results.
For each method, there were threemetrics of interest: normalized discounted cumula-tive gain (NDCG), mean reciprocal rank (MRR), andmean average precision (MAP).
Results are outlinedin Table 2.
To limit the effect of quantization noiseon the evaluation measures, results are reported forqueries having three or more relevant documents.There were a total of 10 such queries, having a to-tal of 61 relevant documents and yielding an averageof 6.10 documents per query (SD = 2.13).Low baselines for each evaluation wereestablished?as there were none in prior existence?by randomly sampling 60 documents from the testcollection.
For each of the six randomly selectedtopics, 10 of the 60 randomly selected documentswere add to the judgment pool without replacement.595Relevance judgments were performed in an orderthat obscured, from the assessor, the source of theresponse being judged.
The 10 random selectionswere then evaluated for each of the six topicsas if they had been a system run.
None of the60 randomly selected documents were judged byassessors to be relevant to their respective randomlyselected topic; thus the random baseline for each ofour measures is zero.
Without multiple draws, con-fidence intervals on this value cannot be established.However, we are confident that random baselineseven as high as 0.1 for any of our measures wouldbe surprising.Pure clustering produced the best results with re-spect to other clustering domains.
SaW was, gener-ally, the best performing retrieval model.
AlthoughSaW did not produce the highest pure cluster MRRnumbers, it was within 0.015 of U1, the best per-forming method.
This is notable given that the dif-ference between U1 and the third best method was0.166.
Further, given the highly quantized natureof MRR, a difference of 0.015 says little about anyoverall difference between the rankings.
In the caseof NDCG, SaW was the best performer with pureclustering, significantly better than BoW with pureclustering and second best overall.
Sa with noisyclustering was best numerically with NDCG, but thedifference is minuscule (1/1000th).Under pure clustering, Ua was generally the worstperformer.
Thus, query refinement using the tempo-ral extent of pseudo-terms is a good idea.
Further,the MRR of U1 and SaW both approach one-half.Since MRR is the inverse of the harmonic mean ofthe rank, we can interpret this as meaning that it islikely that a user will get a relevant document some-where in the first three positions of the result set.Such a result is encouraging, as it means that, underthe correct conditions, a retrieval system built usingzero-resource term detection is a potentially usefultool in practice.
We should note, however, that thisresult was obtained for result-rich queries in whichthree or more relevant responses were known to ex-ist; MRR results on needle-in-a-haystack queries forwhich only a single relevance response exists wouldlikely be lower.
As with all search, precision-biasedmeasures benefit from collection richness.7 Conclusions and Future WorkRecent advances in zero-resource term discoveryhave facilitated spoken document retrieval withoutthe need for traditional transcription or ASR.
Thereare still open questions, however, as to best prac-tices around building useful IR systems on top ofthese tools.
This work has been a step in fillingthat void.
The results show that these zero-resourcemethods can be used to find relevant responses, andthat in some cases such relevant responses can alsobe highly ranked.
Retrieval results vary dependingon how much redundancy exists in the transcribeddata, and how that redundancy is handled within thequery.
One common theme, at least for the tech-niques that we have explored, is that pure cluster-ing seems to be the best overall choice when rankedretrieval is the goal.
A promising next step is tolook to techniques from speech retrieval for insightsthat might be applicable to the zero-resource setting.One possibility in this regard is to explore extendingthe zero-resource term matching techniques to gen-erate a lattice representation from which expectedpseudo-term counts could be computed.8 AcknowledgmentsThe authors wish to thank Nitendra Rajput for pro-viding the spoken queries and responses, and forearly discussions about evaluation design; KomalKamdar, Dhwani Patel, and Yash Patel for perform-ing relevance assessments; and Nizar Habash for hisinsightful comments on early drafts.
Thanks is alsoextended to the anonymous reviewers for their com-ments and suggestions.
This work has been sup-ported in part by NSF award 1218159.ReferencesAlberto Abad and Ram?on Fernandez Astudillo.
2012.The L2F spoken web search system.
In MediaEval.Xavier Anguera, Florian Metze, Andi Buzo, Igor Sz?oke,and Luis Javier Rodr??guez-Fuentes.
2013.
The spokenweb search task.
In MediaEval.Michael Bendersky and W. Bruce Croft.
2008.
Discov-ering key concepts in verbose queries.
In Proceed-ings of the 31st Annual International ACM SIGIR Con-ference on Research and Development in RnformationRetrieval, pages 491?498.596Michael Bendersky, Donald Metzler, and W. Bruce Croft.2010.
Learning concept importance using a weighteddependence model.
In Proceedings of the Third ACMInternational Conference on Web Search and DataMining, pages 31?40.Michael Bendersky, Donald Metzler, and W. Bruce Croft.2011.
Parameterized concept weighting in verbosequeries.
In Proceedings of the 34th International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 605?614.Chun-an Chan, Cheng-Tao Chung, Yu-Hsin Kuo, and Linshan Lee.
2013.
Toward unsupervised model-basedspoken term detection with spoken queries without an-notated data.
In International Conference on Acous-tics, Speech and Signal Processing, pages 8550?8554,May.Jia Cui, Xiaodong Cui, B. Ramabhadran, J. Kim,B.
Kingsbury, J. Mamou, L. Mangu, M. Picheny, T.N.Sainath, and A. Sethy.
2013.
Developing speechrecognition systems for corpus indexing under theIARPA Babel program.
In International Conferenceon Acoustics, Speech and Signal Processing, pages6753?6757, May.Mark Drezde, Aren Jansen, Glen Coppersmith, and KenChurch.
2010.
NLP on spoken documents withoutASR.
In Conference on Empirical Methods on NaturalLanguage Processing, pages 460?470.Jonathan Fiscus, Jerome Ajot, John Garofolo, and GeorgeDoddington.
2007.
Results of the 2006 spoken termdetection evaluation.
In SIGIRWorkshop on SearchingSpontaneous Conversational Speech, pages 51?57.Mark Gales, Kate Knill, Anton Ragni, and Shakti Rath.2014.
Speech recognition and keyword spotting forlow resource languages: Babel project research atCUED.
In Spoken Language Technologies for Under-Resourced Languages.Aren Jansen and Benjamin Van Durme.
2011.
Efficientspoken term discovery using randomized algorithms.In Automatic Speech Recognition and Understanding.Aren Jansen, Kenneth Church, and Hynek Hermansky.2010.
Towards spoken term discovery at scale withzero resources.
In Interspeech Conference, pages1676?1679.Aren Jansen, Benjamin Van Durme, and Pascal Clark.2012.
The JHU-HLTCOE spoken web search systemfor MediaEval.
In MediaEval.Hardik Joshi and Jerome White.
2014.
Document sim-ilarity amid automatically detected terms.
Forum forInformation Retrieval Evaluation, December.Matthew Lease.
2009.
An improved Markov randomfield model for supporting verbose queries.
In Pro-ceedings of the 32nd International ACM SIGIR Con-ference on Research and Development in InformationRetrieval, pages 476?483.Florian Metze, Nitendra Rajput, Xavier Anguera, MarelieDavel, Guillaume Gravier, Charl van Heerden, Gau-tam Mantena, Armando Muscariello, Kishore Prahal-lad, Igor Szoke, and Javier Tejedor.
2012.
The spokenweb search task at MediaEval 2011.
In InternationalConference on Acoustics, Speech and Signal Proccess-ing, pages 3487?3491.Douglas Oard, Jerome White, Jiaul Paik, RashmiSankepally, and Aren Jansen.
2013.
The FIRE 2013question answering for the spoken web task.
Forumfor Information Retrieval Evaluation, December.Douglas W. Oard.
2012.
Query by babbling: A researchagenda.
In Workshop on Information and KnowledgeManagement for Developing Regions, pages 17?22.Alex Park and James R. Glass.
2008.
Unsupervisedpattern discovery in speech.
Transactions on Audio,Speech, and Language Processing, 16(1):186?197.Neil Patel, Deepti Chittamuru, Anupam Jain, PareshDave, and Tapan S. Parikh.
2010.
Avaaj Otalo: A fieldstudy of an interactive voice forum for small farmers inrural India.
In Human Factors in Computing Systems,pages 733?742.Nitendra Rajput and Florian Metze.
2011.
Spoken websearch.
In MediaEval.Amanda Spink, Dietman Wolfram, Bernard Jansen, andTefko Saracevic.
2001.
Searching the Web: The pub-lic and their queries.
Journal of the American Societyfor Information Science and Technology, 52(3):226?234.Trevor Strohman, Donald Metzler, Howard Turtle, andW.
Bruce Croft.
2004.
Indri: A language model-basedsearch engine for complex queries.
In InternationalConference on Intelligence Analysis.Krysta Svore, Pallika Kanani, and Nazan Khan.
2010.How good is a span of terms?
exploiting proximityto improve Web retrieval.
In Proceedings of the 33rdInternational ACM SIGIR Conference on Researchand Development in Information Retrieval, pages 154?161.Jerome White, Douglas W. Oard, Nitendra Rajput, andMarion Zalk.
2013.
Simulating early-terminationsearch for verbose spoken queries.
In Empirical Meth-ods on Natural Language Processing, pages 1270?1280.597
