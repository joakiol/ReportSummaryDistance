A Compact  Arch i tec ture  for D ia logue  Management  Based  onScr ip ts  and  Meta -OutputsManny Rayner,  Beth  Ann  Hockey~ Frankie James .
: :Research Institute for Advanced Computer ScienceMail Stop 19-39, NASA Ames Research CenterMoffett Field, CA 94035-1000{manny, bahockey, fjames} @riacs.eduAbstractWe describe an architecture for spoken dialogueinterfaces to semi-autonomous systems that trans-forms speech~signals through successive representa-tions of linguistic, dialogue, and domain knowledge.Each step produces an output, and a meta-outputdescribing the transformation, with an executableprogram in a simple scripting language as the fi-nal result.
The output/meta-output distinction per-mits perspicuous treatment of diverse tasks such asresolving pronouns, correcting user misconceptions,and optimizing scripts.1 Introduction 1The basic task we consider in this paper is that ofusing spoken language to give commands to a semi-autonomous robot or other similar system.
As ev-idence of the importance of this ta~k in the NLPcommunity note that the early, influential systemSHRDLU (Winograd, 1973) was intended to addressjust this type of problem.
More recent work on spo-ken language interfaces to semi-autonomous robotsinclude SRI's Flakey robot (Konolige et al, 1993)and NCARAI's InterBOT project (Perzanowski etal., 1998; Perzanowski et al, 1999).
A number ofother systems have addressed part of the task.
Com-mandTalk (Moore et al, 1997), Circuit Fix-It Shop(Smith, 1997) and Tl:tAINS-96 (Traum and Allen,1994; Traum and Andersen, 1999) are spoken lan-guage systems but they interface to simulation orhelp facilities rather than semi-autonomous agents.Jack's MOOse Lodge (Badler et al, 1999) takes textrather than speech as natural anguage input and theavatars being controlled are not semi-autonomous.Other researchers have considered particular aspectsof the problem such as accounting for various aspectsof actions (Webber, 1995; Pym et al, 1995).
In mostof this and other related work the treatment is somevariant of the following.
If there is a speech inter-face, the input speech signal is converted into text.Text either from the recognizer or directly input byIThis paper also appears in the proceedings of the SixthInternational Conference on Applied Natural Language Pro-cessing~ Seattle, WA, April 2000.the user is then converted into some kind of logi-cal formula, which abstractly represents the user'sintended command; this formula is.then fed into acommand interpreter, which execdtes the command.We do not think the standard treatment outlinedabove is in essence incorrect, but we do believe that,as it stands, it is in-need of some modification.
Thispaper will in particular make three points.
First, wesuggest that the output representation should not beregarded as a logical expression, but: rather as a pro-gram in some kind of scripting language.
Second, weargue that it is not merely the case that the processof converting the input signal to the final represen-tation can sometimes go wrong; rather, this is thenormal course of events, and the inferpretatiofi pro-cess should be organized with that assumption inmind.
Third, we claim, perhaps urprisingly, thatthe first and second points are related.
These claimsare elaborated in Section 2.The remainder of the paper describes an archi-tecture which addresses the issues outlined above,and which has been used to implement a prototypespeech interface to a simulated semi-autonomousrobot intended for deployment on the InternationalSpace Station.
Sections 3 and 4 present an overviewof the implemented interface, focussingon represen-tational issues relevant o dialogue management.
Il-lustrative xamples of interactions with the systemare provided in Section 5.
Section 6 concludes.2 Theoretical Ideas,2.1 Scripts vs Logical FormsLet's first look in a little more detail at the questionof what the output representation f a spoken lan-guage interface to a semi-autonomous robot/agentshould be.
In practice, there seem to be two mainchoices: atheoretical representations, or some kindof logic.Logic is indeed an excellent way to  thinkabout representing static relationships like databasequeries, but it is much less clear that it is a good wayto represent commands.
In real life, when peoplewish to give a command to a computer, they usu-ally do so via its operating system; a complex com-54mand is an expression in a scripting language likeCSHELL, Perl, or VBScript.
These languages arerelated to logical formalisms, but cannot be mappedonto them in a simple way.
Here are some of theobvious differences:?
A scripting language is essentially imperative,rather than relational.?
The notion of temporal sequence is fundamentalto the language.
"Do P and then Q" is not thesame as "Make the goals P and Q true"; it isexplicitly stated that P is to be done first.
Simi~larly, "For each X in the list (A B C), do P (X)"is not the same as "For all X, make P(X) true";once again, the scripting language defines an or-der, but no~ the logical language 2.?
Scripting languages assume that commands donot always succeed.
For example, UNIX-basedscripting languages like CSHELL provide eachscript with the three predefined streams td in ,s tdout  and s tder r .
Input is read from s td inand written to stdout;  error messages, warn-ings and other comments are sent to s tder r .Wedo not think that these properties of scriptinglanguage are accidental.
They have evolved as theresult of strong selectional pressure from real userswith real-world tasks that need to be carried out,and represent a competitive way to meet said users'needs.
We consequently think it is worth taking seri-ously the idea that a target representation producedby a spoken language interface should share many ofthese properties.2.2 Fall ible In terpretat ion :  Outputs  andMeta -outputsWe now move on to the question of modelling the in-terpretation process, that is to say the process thatconverts the input (speech) signal to the output (ex-ecutable) representation.
As already indicated, wethink it is important o realize that interpretationis a process which, like any other process, may suc-ceed more or less well in achieving its intended goals.Users may express themselves unclearly or incom-pletely, or the system may more or less seriouslyfail to understand exactly what they mean.
A goodinterpretation architecture will keep these consider-ations in mind.Taking our lead from the description of scriptinglanguages ketched above, we adapt the notion ofthe "error stream" to the interpretation process.
Inthe course of  interpreting an utterance, the system2In cases like these, the theorem prover or logic program-ming interpreter used to evaluate the logical formula typicallyassigns a conventional order to the conjuncts; note howeverthat this is part of the procedural semantics of the theoremprover/interpreter, and does not follow from the declarativesemantics of the logical formalism.55translates it into successively "deeper" levels of rep-resentation.
Each translation step has not only aninput (the representation consumed) and an output(the representation produced), but also somethingwe will refer to as a "meta-output':  this provides in-formation about how the translation was performed.At a high level of abstraction, our architecture willbe as follows.
Interpretation proceeds as a seriesof non-deterministic ranslation steps, each produc-ing a set of possible outputs and associated meta-outputs.
The final translation step produces an ex-ecutable script.
The interface attempts to simulateexecution of each possible script produced, in or-der to determine what would happen if that scriptwere selected; simulated execution can itself producefurther meta-outputs.
Finally, the system uses themeta-output information to decide what to do withthe various possible interpretations it has produced.Possible actions incl~tde selection and execution ofan output script, paraphrasing meta-output infor-mation back to the user, or some combination of thetwo.In the following section, we present a more de-tailed description showing how the output/meta-output'distinction works in a practical system.3 A Prototype ImplementationThe ideas sketched out above have been realized asa prototype spoken language dialogue interface to asimulated version of the Personal Satellite Assistant(PS i ;  (PSA, 2000)).
This section gives an overviewof the implementation; in the following section, wefocus on the specific aspects of dialogue managementwhich are facilitated by the output/meta-output ar-chitecture.3.1 Levels of  Representat ionThe real PSA is a miniature robot currently beingdeveloped at NASA Ames Research Center, whichis intended for deployment on the Space Shuttleand/or International Space Station.
It will be ca-pable of free navigation in an indoor micro-gravityenvironment, and will provide mobile sensory capac-ity as a backup to a network of fixed sensors.
ThePSA will primarily be controlled by voice commandsthrough a hand-held or head-mounted microphone,with speech and language processing being handledby an offboard processor.
Since the speech process-ing units are not in fact physically connected to thePSA we envisage that they could also be used to con-trol or monitor other environmental functions.
Inparticular, our simulation allows voice access to thecurrent and past values of the fixed sensor eadings.The initial PSA speech interface demo consists ofa simple simulation of the Shuttle.
State parame-ters include the PSA's current position, some envi-ronmental variables uch as local temperature, pres-sure and carbon dioxide levels, and the status of theShuttle's doors (open/closed).
A visual display givesdirect feedback on some of these parameters.The speech and language processing architectureis based on that of the SRI CommandTalk sys-tem (Moore et al, 1997; Stent et al, 1999).
The sys-tem comprises a suite of about 20 agents, connectedtogether using the SRI Open Agent Architecture(OAA; (Martin et al, 1998)).
Speech recognitionis performed using a version of the Nuance recog-nizer (Nuance, 2000).
Initial language processing iscarried out Using the SRI Gemini system (Dowdinget al, 1993), using a domain-independent u ification.grammar and a domain-specific lexicon.
The lan-guage processing rammar is compiled into a recog-nition grammar using the methods of (Moore et al,1997); the n~ resnlt is that only grammatically well-formed utterances Gan be recognized.
Output fromthe initial language-processing step is representedin a version of Quasi Logical Form (van Eijck andMoore, 1992), and passed in that form to the dia-logue manager.
We refer to these as linguistic levelrepresentations.The aspects of the system which are of primary in-terest here concern the dialogue manager (DM) andrelated modules.
Once a linguistic level represen-tation has been produced, the following processingsteps occur:?
The linguistic level representation is convertedinto a discourse level representation.
This pri-marily involves regularizing differences in sur-face form: so, for example, "measure the pres-sure" and "what is the pressure?"
have differ-ent representations at the linguistic level, butthe same representation at the discourse level.?
If necessary, the system attempts to resolve in-stances of ellipsis and anaphoric reference.
Forexample, if the previous command was "mea-sure temperature at flight deck", then the newcommand "lower deck" will be resolved to anexpression meaning "measure temperature atlower deck".
Similarly, if the previous commandwas "move to the crew hatch", then the com-mand "open it" will be resolved to "open thecrew hatch".
We call the output of this step aresolved discourse level representation.?
The resolved discourse level representation isconverted into an executable script in a lan-guage essentially equivalent o a subset ofCSHELL.
This involves two sub-steps.
First,quantified variables are given scope: for exam-ple, "go to the flight deck and lower deck andmeasure pressure" becomes omething approxi-mately equivalent to the scriptforeach x ( f l ight_deck  lower_deck)go_to Sxmeasure  pressureendThe point to note here is that :the foreach hasscope over both the go_to and the measure ac-tions; an alternate (incorrect) sCoping would beforeach x (flight_deck lower_deck)go_to Sxendmeasure  pressureThe second sub-step is to attempt o optimizethe plan.
In the current example, this canbe done by reordering the list ( f l ight_decklowerAeck).
For instance, if the PSA is al-ready at the lower deck, reversing the list willmean that the robot only makes one trip, in-stead of two.
,,The final step in the 'interlJretation process isplan evaluation: the syStem tries to work outwhat will happen if it actually executes theplan.
(The relationship between plan evaluationand plan execution is described in more detailin Section 4.1).
Among other things, this givesthe dialogue manager the possibility of compar-ing different interpretations of the original com-mand, and picking the one which is most effi-cient.3.2 How Meta-outputs  Part ic ipate in theTranslationThe above sketch shows how context-dependentinterpretation is arranged as a series of non-deterministic translation stepS; in each case, we havedescribed the input and the output for the step inquestion.
We now go back to the concerns of Sec-tion 2.
First, note that each translation step is ingeneral fallible.
We give severalexamples:?
One of the most obvious cases arises when theuser simply issues an invalid command, such asrequesting the PSA to open a door D which isalready open.
Here, one of the meta~outputsissued by the plan evaluation step will be thetermpresupposition_failure(already_open(D));the DM can decide to paraphrase this back tothe user as a surface string of the form "D isalready open".
Note that plan evaluation doesnot involve actually executing the final script,which can be important.
For instance, if thecommand is "go to the crew hatch and open it"and the crew hatch is already open, the interfacehas the option of informing the user that thereis a problem without-first carrying out the "goto" action.56?
The resolution step can give rise to similar kindsof meta-output.
For example, a command mayinclude a referring expression that has no deno-tation, or an ambiguous denotation; for exam-ple, the user might say "both decks", presum-ably being unaware that there are in fact threeof them.
This time, the meta-output producedispresupposition_failure (incorrect_size_of_set (2,3))representing the user'sincorrect belief abou\[the number of decks.
The DM then has the pos-sibility of informing the user of this misconcep-tion by realizipg the meta-output term as thesurface stung "in.
fact there are three of them".Ambiguous denotation occurs when a descrip-tion is under-specified.
For instance, the usermight say "the deck" in a situation where thereis no clearly salient deck, either in the discoursesituation :or in the simulated world: here, themeta-output will bepresupposition_failure (under specif ied_def inite (deck))which can be realized as the clarification ques-tion "which deck do you mean?"?
A slightly more complex case involves plancosts.
During plan evaluation, the system simu-lates execution of the output script while keep-ing track of execution cost.
(Currently, the costis just an estimate of the time required to exe-cute the script).
Execution costs are treated asmeta-outputs of the formcost(C)and passed back through the interpreter so thatthe plan optimization step can make use ofthem.Finally,.
we consider what happens when thesystem receives incorrect input from the speechrecognizer.
Although the recognizer's languagemodel is constrained so that it can only pro-duce grammatical utterances, it can still misrec-ognize one grammatical string as another one.Many of these cases fall into one of a smallnumber of syntactic patterns, which function asfairly reliable indicators of bad recognition.
Atypical example is conjunction i volving a pro-noun: if the system hears "it and flight deck",this is most likely a misrecognition f somethinglike "go to flight deck".During the processing phase which translateslinguistic level representations into discourselevel representations, the system attempts tomatch each misrecognition pattern against heinput linguistic form, and if successful producesa meta-output of the formpresupposition_failure (dubious_if (<Type>))These mete-outputs are passed down to theDM, which in the absence of sufficiently com-pelling contrary evidence will normally issue aresponse of the form "I'm sorry, I think I mis-heard you".4 A Compact  Arch i tec ture  forD ia logue  Management  Based  onScr ip ts  and  Meta -OutputsNone of the individual functionalities outlined aboveare particularly novel in themselves.
What we findnew and interesting-is the fact that they can allbe expressed in a uniform way in terms of thescript output/meta-output architecture.
This sec-tion presents three examples illustrating how the ar-chitecture can be used to simplify the overall orga-nization of the system.4.1 Integrat ion of plan evaluation, planexecution and dialogue management .Recall that the DM simulates evaluation of the planbefore running it, in order to obtain relevant meta-information.
At plan execution time, plan actionsresult in changes to the world; at plan evaluationtime, they result in simulated changes to the worldand/or produce meta-outputs.Conceptualizing plans as scripts rather than log-ical formulas permits an elegant reatment of theexecution/evaluation dichotomy.
There is one scriptinterpreter, which functions both as a script exec-utive and a script evaluator, and one set of ruleswhich defines the procedural semantics of script ac-tions.
Rules are parameterized by execution typewhich is either "execute" or "evaluate".
In "evalu-ate" mode, primitive actions modify a state vectorwhich is threaded through the interpreter; in "ex-ecute" mode, they result in commands being sentto (real or simulated) effector agents.
Conversely,"meta-information" actions, such as presuppositionfailures, result in output being sent to the meta-output stream in "evaluate" mode, and in a null ac-tion in "execute" mode.
The upshot is that a simplesemantics can be assigned to rules like the followingone, which defines the action of attempting to opena door which may already be open:procedure (open_door (D) ,if_then_else (status (D, open_closed, open),presupposition_failure (already_open(D)),change_status (D, open_closed, open)))574.2 Using meta-outputs  o choose betweeninterpretat ionsAs described in the preceding section, the resolutionstep is in general non-deterministic and gives rise tometa-outputs which describe the type of resolutioncarried out.
For example, consider a command in-volving a definite description, like "open the door".Depending on the preceding context, resolution willproduce a number of possible interpretations; "thedoor" may be resolved to one or more contextuallyavailable doors, or the expression may be left un-resolved.
In each case, the type of resolution usedappears as a meta-output, and is available to the di-alogue manager when it decides which interpretationis most felicitous.
By default, the DM's strategy is toattempt to~pp ly  antecedents for referring expres-sions, preferring h~ most recently occurring sortallyappropriate candidate.
In some cases, however, it isdesirable to allow the default strategy to be over-ridden: for instance, it may result in a script whichproduces a presupposition failure during plan eval-uation.
Treating resolution choices and plan evalu-ation problems as similar types of objects makes iteasy to implement this kind of idea.4.3 Using meta-outputs  o choose betweendialogue management  movesPerhaps the key advantage ofour architecture is thatcollecting together several types of information as abag of meta-outputs simplifies the top-level struc-ture of the dialogue manager.
In our application,the critical choice of dialogue move comes after thedialogue manager has selected the most plausible in-terpretation.
It now has to make two choices.
First,it must decide whether or not to paraphrase any ofthe meta-outputs back to the user; for example, ifresolution was unable to fill some argument posi-tion or find an antecedent for a pronoun, it may beappropriate to paraphrase the corresponding meta-output as a question, e.g.
"where do you mean?
", or"what do you mean by 'it' ?".
Having all the meta-outputs available together means that the DM isable to plan a coherent response: so if there are sev-eral meta-outputs which could potentially be worthparaphrasing, it typically realizes only the most im-portant one.Second, if interpretation was able to produce awell-formed plan, the DM currently has the threeoptions of executing it, paraphrasing it back to theuser as a confirmation question, or doing nothing.Once again, this decision often requires global infor-mation about what has happened uring the inter-pretation process.
For example, knowing that planevaluation showed that the plan would take signif-icant time to execute makes it more plausible thatthe user would prefer to receive a confirmation.585 ExamplesThis section presents a Commented sample dialoguewith the implemented PSA interface demo chosento illustrate the points discussed inthe previous ec-tions.
We focus in particular On ,the :role that meta-outputs play in interpretation.
Text preceded byUSER represents spoken utterances from the user.Text preceded by PSA represents synthesized spokenresponses from the PSA.
Text inside square bracketsdescribes the actions carried out bythe PSA.1.
The PSA can move to various locations andmeasure nvironmental f ctors.
In the first ex-zhange, reference resolution is used to interpretthe phrase "all three decks" as referring to thelower deck, mid deck and flight deck.
Plan op-timization finds the most efficient way to or-der the locations, an d paraphrases the resultingplan back to the user.USER: Go to all three decks and measure car-bon dioxide.
~ 'PSA: I will move to flight,deck, mid deck andthen lower deck and I will measure carbon' dioxide level, okay?USER: Okay.
\[PSA moves to flight deck\]PSA: The carbon dioxide level at the flightdeck is one percent.
\[PSA: moves to middeck\]PSA.
: The carbon dioxide level at the mid deckis one percent.
\[PSA moves to lower deck\]PSA: The carbon dioxide level at the lowerdeck is one percent.
'2.
The next example involves resolution of ellip-sis.
The relevant context is recovered from thepreceding dialogue.USER: Do the same for the pilot's seat.PSA: I will move to the pilot's seat and I willmeasure carbon dioxide level, okay?USER: Right.
\[PSA moves to pilot's seat\]PSA: The carbon dioxide level at the pilot'sseat is one percent.3.
Resolution of ellipsis can involve filling in un-specified semantic parameters toa syntacticallycomplete command.
It is not meaningful to talkabout a sensor eading without reference to theplace at which that reading was carried out; inthe following example, resolution assumes thatthe location in question is the one most recentlymentioned in the dialogue.Note also that since the PSA does not have tomove in order to carry out the command, and.....hence can execute it immediately, the DM rea-sonably decides to skip the confirmation ques-tion.
This decision is made based on the meta-Output encoding the estimated cost of executingthe :script.'
iUSER: What was the carbon dioxide level atfifteen oh five according to the fixed sen-sors?PSA: According to the fixed sensors, at fifteenoh five the carbon dioxide level at the pi-lot's seat was one percent.
-.The system encounters a user misconception,which results in a "presupposition failure"meta-output being produced during referenceresolutiom It ?onsequently decides to alert theuser to this, rather than attempt o carry outthe command.USER: Close both doors.PSA: There are in fact three of them.The user utterance includes a pronoun, "it".The system resolves "it" to.
"the crew hatch",and issues an appropriate paraphrase in orderto confirm that this is what the user meant.USER: Go to crew hatch and close it.PSA: I will go to crew hatch and I will closecrew hatch, okay?USER: Yeah.
\[PSA moves to crew hatch andcloses it\]The user asks the system to close the door, butit is unclear which door they are referring to.An "ambiguous reference" meta-output is pro-duced by reference resolution, resulting in theDM issuing a clarification question.USER: Close the door.PSA: Which door do you mean?The user response is successfully interpreted byreference resolution as an instance of ellipsis.Trying to close the crew hatch a second timehowever causes a presupposition failure meta-output o be generated during plan evaluation.The system informs the user of this and takesno action.USER: The crew hatch.PSA: The crew hatch is already closed.The user presents three locations in a subopti-mal order.
Plan evaluation finds a more efficientordering.USER: Move to storage lockers, commander'sseat and flight deck and measure tempera-ture.PSA: I will move to flight deck, commander'sseat and then storage lockers and I willmeasure temperature, okay?USER: Sure.\[PSA moves to flight deck, commander'sseat and storage lockers in that order andreports temperature at each location\]9.
The user asks the system to repeat he previouscommand.
Since the PSA is now starting froma different position, its most efficient plan is nolonger the same, and it informs the user of this.USER: Do that again.'
PSA:  I will move to storage lockers, comman-der's seat and then flight deck and I willmeasure temperature, okay?USER: Yes.
\[PSA moves to storage lockers\]PSA: The temperature at the storage lockersis 19.9 degrees Celsius.
\[PSA starts movingto commander's seat\]10.
The user interrupts the PSA in the middle of theplan, and asks it to go back to its last stoppingplace.
Since "stop" may involve a potentiallyhazardous condition, the interface obeys imme-diately, without attempting to confirm first.USER: Stop.
\[PSA stops\]USER: Go back.
\[PSA returns to storage lock-ers\]6 SummaryWe have described a generic architecture for spokenlanguage dialogue interfaces to semi-autonomousagents, based on the standard notion of translatingto successively deeper levels of representation.
Thenovel aspects of our scheme center around two ideas:first, that the final output representations are bestconceptualized not as logical expressions but ratheras programs in a scripting language; second, thatsteps in the translation process hould produce notonly a simple output, but also meta-information de-scribing how the output was produced.
We have pre-sented examples uggesting how several apparentlydiverse types of dialogue behavior can be capturedsimply within our framework, and outlined a proto-type implementation f the scheme.ReferencesN.
Badler, R. Bindiganavale, J. Bourne, J. Allbeck,J.
Shi, and M. Palmer.
1999.
Real time virtualhumans.
In International Conference on DigitalMedia Futures.J.
Dowding, M. Gawron, D. Appelt, L. Cherny,R.
Moore, and D. Moran.
1993.
Gemini: A nat-ural language system for spoken language un-derstanding.
In Proceedings of the Thirty-First59Annual Meeting of the Association for Computa-tional Linguistics.K.
Konolige, K. Myers, E. Ruspini, and A. Saf-fiotti.
1993.
Flakey in action: The 1992 AAAIrobot competition.
Technical Report SRI Techni-cal Note 528, SRI, AI Center, SRI International,333 Ravenswood Ave., Menlo Park, CA 94025.D.
Martin, A. Cheyer, and D. Moran.
1998.
Build-ing distributed software systems with the openagent architecture.
In Proceedings of the ThirdInternational Conference on the Practical Appli-cation of Intelligent Agents and Multi-Agent Tech~nology.R.
Moore, J. Dowding, H. Bratt, J. Gawron,Y.
Gorfu, and A. Cheyer.
1997.
CommandTalk:A spoken:language interface for battlefield simu-lations.
In ProCeedings of the Fifth Conference onApplied NaturaiLanguage Processing, pages 1-7.Nuance, 2000.
Nuance Communications, Inc.http://www.nuance.com.
As of 9 March 2000.D.
Perzanowski, A. Schultz, and W. Adams.
1998.Integrating natural language and gesture in arobotics domain.
In IEEE International Sympo-sium on Intelligent Control: ISIC/CIRA//ISASJoint Conference, pages 247-252, Gaithersburg,MD: National Institute of Standards and Tech-nology.D.
Perzanowski, A. Schultz, W. Adams, andE.
Marsh.
1999.
Goal tracking in a natural an-guage interface: Towards achieving adjustable au-tonomy.
In ISIS/CIRA99 Conference, Monterey,CA.
IEEE.PSA, 2000.
Personal Satellite Assistant (PSA)Project.
http://ic.arc.nasa.gov/ic/psa/.
As of 9March 2000.D.
Pym, L. Pryor, and D. Murphy.
1995.
Actionsas processes: a position on planning.
In WorkingNotes, AAAI Symposium on Extending Theoriesof Action, pages 169-173.R.
W. Smith.
1997.
An evaluation of strategies forselective utterance verification for spoken naturallanguage dialog.
In Proceedings ofthe Fifth Con-ference on Applied Natural Language Processing,pages 41-48.A.
Stent, J. Dowding, J. Gawron, E. Bratt, andR.
Moore.
1999.
The CommandTalk spoken di-alogue system.
In Proceedings of the Thirty-Seventh Annual Meeting of the Association forComputational Linguistics, pages 183-190.D.
R. Traum and J. Allen.
1994.
Discourse obliga-tions in dialogue processing.
In Proceedings oftheThirty-Second Annual Meeting of the Associationfor Computational Linguistics, pages 1-8.D.
R. Traum and C. F. Andersen.
1999.
Represen-tations of dialogue state for domain and task inde-pendent meta-dialogue.
In Proceedings of the I J-CAI'99 Workshop on KnowTedge and Reasoningin Practical Dialogue Systems, pages 113-120.J.
van Eijck and R. Moore.
1992.
Semantic rulesfor English.
In H. Alshawi, editor, The Core Lan-guage Engine.
MIT Press.
~B.
Webber.
1995.
Instructing animated agents:Viewing language in behavioral terms.
In Proceed-ings of the International Conference on Coopera-tive Multi-modal Communication.T.
A. Winograd.
1973.
A procedural model of lan-guage understanding.
In R. C. Shank and K. M.Colby, editors, Computer Models of Thought andLanguage.
Freeman, San Francisco, CA.i60
