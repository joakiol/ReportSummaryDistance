Proceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 107?115,ACL HLT 2011, Portland, Oregon, USA, June 2011. c?2011 Association for Computational LinguisticsUtilizing Target-Side Semantic Role Labels to Assist HierarchicalPhrase-based Machine TranslationQin Gao and Stephan VogelLanguage Technologies Institute, Carnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213{qing, stephan.vogel}@cs.cmu.eduAbstractIn this paper we present a novel approachof utilizing Semantic Role Labeling (SRL)information to improve Hierarchical Phrase-based Machine Translation.
We propose analgorithm to extract SRL-aware SynchronousContext-Free Grammar (SCFG) rules.
Con-ventional Hiero-style SCFG rules will also beextracted in the same framework.
Special con-version rules are applied to ensure that whenSRL-aware SCFG rules are used in deriva-tion, the decoder only generates hypotheseswith complete semantic structures.
We per-form machine translation experiments using 9different Chinese-English test-sets.
Our ap-proach achieved an average BLEU score im-provement of 0.49 as well as 1.21 point reduc-tion in TER.1 IntroductionSyntax-based Machine Translation methods haveachieved comparable performance to Phrase-basedsystems.
Hierarchical Phrase-based Machine Trans-lation, proposed by Chiang (Chiang, 2007), uses ageneral non-terminal label X but does not use lin-guistic information from the source or the target lan-guage.
There have been efforts to include linguis-tic information into machine translation.
Liu et al(2006) experimented with tree-to-string translationmodels that utilize source side parse trees, and laterimproved the method by using the Packed Forestdata structure to reduce the impact of parsing errors(Liu and Huang, 2010).
The string-to-tree (Galleyet al 2006) and tree-to-tree (Chiang, 2010) meth-ods have also been the subject of experimentation, aswell as other formalisms such as Dependency Trees(Shen et al, 2008).One problem that arises by using full syntactic la-bels is that they require an exact match of the con-stituents in extracted phrases, so it faces the riskof losing coverage of the rules.
SAMT (Zollmannand Venugopal, 2006) and Tree Sequence Align-ment (Zhang et al, 2008) are proposed to amend thisproblem by allowing non-constituent phrases to beextracted.
The reported results show that while uti-lizing linguistic information helps, the coverage ismore important (Chiang, 2010).
When dealing withformalisms such as semantic role labeling, the cov-erage problem is also critical.
In this paper we fol-low Chiang?s observation and use SRL labels to aug-ment the extraction of SCFG rules.
I.e., the formal-ism provides additional information and more rulesinstead of restrictions that remove existing rules.This preserves the coverage of rules.Recently there has been increased attention to usesemantic information in machine translation.
Liuand Gildea (2008; 2010) proposed using SemanticRole Labels (SRL) in their tree-to-string machinetranslation system and demonstrated improvementover conventional tree-to-string methods.
Wu andFung (2009) developed a framework to reorder theoutput using information from both the source andthe target SRL labels.
In this paper, we explore anapproach of using the target side SRL informationin addition to a Hierarchical Phrase-based MachineTranslation framework.
The proposed method ex-tracts initial phrases with two different heuristics:The first heuristic is used to extract rules that havea general left-hand-side (LHS) non-terminal tag X ,107Second we must build a flood prevention system , strengthen pre-flood inspections and implement flood prevention measuresarg0 modarg0 modarg0 modpredpredarg1arg1predarg1Figure 1: Example of predicate-argument structure in a sentencei.e., Hiero rules.
The second will extract phrases thatcontain information of SRL structures.
The pred-icate and arguments that the phrase covers will berepresented in the LHS non-terminal tags.
Afterthat, we obtain rules from the initial phrases in thesame way as the Hiero extraction algorithm, whichreplaces nesting phrases with their correspondingnon-terminals.By applying this scheme, we will obtain rules thatcontain SRL information, without sacrificing thecoverage of rules.
In this paper, we call such rulesSRL-aware SCFG rules.
During decoding, both theconventional Hiero-style SCFG rules with generaltag X and SRL-aware SCFG rules are used in a syn-chronous Chart Parsing algorithm.
Special conver-sion rules are introduced to ensure that wheneverSRL-aware SCFG rules are used in the derivation,a complete predicate-argument structure is built.The main contributions are:1. an algorithm to extract SRL-aware SCFG rulesusing target side SRL information.2.
an approach to use Hiero rules side-by-sidewith information-rich SRL-aware SCFG rules,which improves the quality of translation re-sults.In section 2 we briefly review SCFG-based ma-chine translation and SRL.
In section 3, we describethe SRL-aware SCFG rules.
Section 4 providesthe detail of the rule extraction algorithm.
Section5 presents two alternative methods how to utilizethe SRL information.
The experimental results aregiven in Section 6, followed by analysis and conclu-sion in Section 7.2 Background2.1 Hierarchical Phrase-based MachineTranslationProposed by Chiang (2005), the HierarchicalPhrase-based Machine Translation model (com-monly known as the Hiero model) has achieved re-sults comparable, if not superior, to conventionalPhrase-based approaches.
The basic idea is to treatthe translation as a synchronous parsing problem.Using the source side terminals as input, the decodertries to build a parse tree and synchronously generatetarget side terminals.
The rules that generates suchsynchronous parse trees are in the following form:X ?
(f1 X1 f2 X2 f3, e1 X2 e2 X1 e3)where X1 and X2 are non-terminals, and the sub-scripts represents the correspondence between thenon-terminals.
In Chiang?s Hiero model all non-terminals will have the same tag, i.e.
X .
The formal-ism, known as Synchronous Context-Free Grammar(SCFG) does not require the non-terminals to have aunique tag name.
Instead, they may have tags withsyntactic or semantic meanings, such as NP or V P .2.2 Semantic Role Labeling and MachineTranslationThe task of semantic role labeling is to label the se-mantic relationships between predicates and argu-ments.
This relationship can be treated as a depen-dency structure called ?Predicate-Argument Struc-ture?
(PA structure for short).
Figure 1 depicts ex-amples of multiple PA structures in a sentence.
Thelines indicate the span of the predicates and argu-ments of each PA structure, and the tags attached tothese lines show their role labels.Despite the similarity between PA structure anddependency trees, SRL offers a structure that possesbetter granularity.
Instead of trying to analyze alllinks between words in the sentences, PA structureonly deals with the relationships between verbs andconstituents that are arguments of the predicates.This information is useful in preserving the mean-ing of the sentence during the translation process.However, using semantic role representation inmachine translation has its own set of problems.108First, we face the coverage problem.
Some sen-tences might not have semantic structure at all, if,for instance they consist of single noun phrases orcontain only rare predicates that are not covered bythe semantic role labeler.
Moreover, the PA struc-tures are not guaranteed to cover the whole sentence.This is especially true when two or more predicatesare presented in a coordinated structure.
In this case,the arguments of other predicates will not be coveredin the PA structure of the predicate.The second problem is that the SRL labels areonly on the constituents of predicate and arguments.There is no analysis conducted inside the augments.That is different from syntactic parsing or depen-dency parsing, which both provide a complete treefrom the sentence to every individual word.
Aswe can see in Figure 1, words such as ?Second?and ?and?
are not covered.
Inside the NPs suchas ?a flood prevention system?, SRL will not pro-vide more information.
Therefore it is hard to builda self-contained formalization based only on SRLlabels.
Most work on SRL labels is built uponor assisted by other formalisms.
For instance, Liuand Gildea (2010) integrated SRL label into a tree-to-string translation system.
Wu and Fung (2009)used SRL labels for reordering the n-best output ofphrase-based translation systems.
Similarly, in ourwork we also adopt the methodology of using SRLinformation to assist existing formalism.
The dif-ference of our method from Wu and Fung is thatwe embed the SRL information directly into the de-code, instead of doing two-pass decoding.
Also, ourmethod is different from Liu and Gildea (2010) thatwe utilize target side SRL information instead of thesource side.As we will see in section 3, we define a mappingfunction from the SRL structures that a phrase cov-ers to a non-terminal tag before extracting the SCFGrules.
The tags will restrict the derivation of the tar-get side parse tree to accept only SRL structures wehave seen in the training corpus.
The mapping fromSRL structures to non-terminal tags can be definedaccording to the SRL annotation set.In this paper we adopt the PropBank (Palmer etal., 2005) annotation set of semantic labels, becausethe annotation set is relatively simple and easy toparse.
The small set of argument tags also makesthe number of LHS non-terminal tags small, whichalleviates the problem of data scarcity.
However themethodology of this paper is not limited to Prop-Bank tags.
By defining appropriate mapping, it isalso possible to use other annotation sets, such asFrameNet (Baker et al, 2002).3 SRL-aware SCFG RulesThe SRL-aware SCFG rules are SCFG rules.
Theycontain at least one non-terminal label with infor-mation about the PA structure that is covered by thenon-terminal.
The labels are called SRL-aware la-bels, and the non-terminal itself is called SRL-awarenon-terminal.
The non-terminal can be on the lefthand side or right hand side or the rule, and we donot require all the non-terminals in the rules be SRL-aware, thus, the general tag X can also be used.
Inthis paper, we assign SRL-aware labels based on theSRL structure they cover.
The label contains the fol-lowing components:1.
The predicate frame; that is the predicate whosepredicate argument structure belongs to theSRL-aware non-terminal.2.
The set of complete arguments the SRL-awarenon-terminal covers.In practice, the predicates are stemmed.
For ex-ample, if we have a target side phrase: She beatseggs today, where She will be labeled as ARG0 of thepredicate beat, and eggs will be labeled as ARG1, to-day will be labeled as ARG-TMP, respectively.
TheSRL-aware label that covers this phrase is:#beat/0 1 TMPThere are two notes for the definition.
Firstly,the order of arguments is not important in the la-bel.
#beat/0 1 TMP is treated identically to#beat/0 TMP 1.
Secondly, as we always requirethe predicate to be represented, an SRL-aware non-terminal should always cover the predicate.
Thisproperty will be re-emphasized when we discussthe rule extraction algorithm in Section 3.
Figure2 shows some examples of the SRL-aware SCFGrules.When the RHS non-terminal is an SRL-awarenon-terminal, we define the rule as a conversion rule.A conversion rule is only generated when the right109Xinjiang?sYiliholdspropagandadrive????????????????
[#Hold/1][#Hold/0_1][#Hold/0][#Hold][X][X][X][X][X][X]SomeSRL-aware Rules :[#Hold/0_1]?
( [#Hold/0]??????
?,[#Hold/0] propagandadrive)[#Hold/0_1]?
( ???????
[#Hold/1], Xinjiang?sYili[#Hold/1])[#Hold/0_1]?
( ??
[X 1]???
[#Hold/1], Xinjiang?s [X 1] [#Hold/1])[#Hold/0_1]?
( [X 1]hold [X 2], [X 1]hold [X2])[#Hold/1]?([#Hold]??????
?, [#Hold] propaganda drive)[#Hold/0]?(???????
[#Hold], Xinjiang?sYili[#Hold])[#Hold]?(?
?, holds)Special SRL-awareconversionrule:[X] ?
[#Hold/0_1]Figure 2: Example SRL structure with word alignmenthand side is a complete SRL structure.
For exam-ple, #hold/0 is not a complete SRL structure inFigure 2, because it lacks of a required argument,while #hold/0 1 is a complete SRL structure.
Inthis case, the conversion rule X ?
#hold/0 1will be extracted from the example shown in Fig-ure 2, but not the other.
Together with the gluerules that commonly used in Hiero decoder, i.e.S ?
(S X1, S X1) and S ?
(X1, X1), the conver-sion rules ensures that whenever SRL-aware SCFGrules are used in parsing, the output parse tree con-tains only complete SRL structures.
This is becauseonly complete SRL structures that we have observedin the training data can be converted back to the gen-eral tag X .After we have extracted the SRL-aware SCFGrules, derivation can be done on the input of sourcesentence.
For example, the sentence ??
????????????
1 can generate the parse treeand translation in Figure 3a) using the rules shownin Figure 2.
Also, we can see in Figure 3b) that in-complete SRL structures cannot be generated due tothe absence of a proper conversion rule.1The translation is Xinjiang?s Yili holds propaganda driveand the Pinyin transliteration is Xinjiang daguimo kaizhanmianduimian xuanjiang huodong  	      										a) Sample of valid derivationb) Sample of invalid derivationFigure 3: Example of a derivations of sentenceWe can see from the example in Figure 3a), thatthe SRL-aware SCFG rules fit perfectly in the SCFGframework.
Therefore no modification need to bemade on a decoder, such as MosesChart decoder,forinstance (Hoang and Koehn, 2008).
The main prob-lem is how to extract the SRL-aware SCFG rulesfrom the corpus and estimate the feature values sothat it works together with the conventional Hierorules.
In the next two sections we will present therule extraction algorithm and two alternative meth-ods for comparison.4 Rule Extraction AlgorithmThe Hiero rule extraction algorithm uses the follow-ing steps:1.
Extract the initial phrases with the commonlyused alignment template heuristics.
To reducethe number of phrases extracted, an additionalrestriction is applied that the boundary wordsmust be aligned on both sides.
Also, the maxi-mum length of initial phrases is fixed, and usu-ally set to 10.1102.
If an initial phrase pair contains another phrasepair, then we can replace the embedded phrasepairs with non-terminal X .
Restrictions alsoapply in this stage.
Firstly the source sidephrase can only contain two or less non-terminals.
Secondly, two source side non-terminals must not be next to each other.
Andfinally, after the substitution, at least one re-maining terminal in the source side should havealignment links to the target side terminals.It is easy to see this scheme is not able to han-dle the extraction of SRL-aware SCFG rules.
Thelength of initial phrases is limited and it may not beable to cover a complete predicate-argument struc-ture.
In the meantime, the restrictions on unalignedwords on the boundaries will cause a large numberof SRL-aware SCFG rules to be excluded.
There-fore, a modified algorithm is proposed to handle ex-traction of SRL-aware SCFG rules.One sentence may have multiple verbs and, there-fore, multiple PA structures.
Different PA structuresmay be nested within each other.
However we do notwant to complicate the representation by attemptingto build a tree structure from multiple structures.
In-stead, we treat them independently.For each word-aligned sentence pair, if there is noPA structure given, we run the general Hiero extrac-tion algorithm.
Otherwise, for each PA structure, weapply the algorithm for SRL-aware rule extraction,which takes two steps, extracting the initial SRL-aware phrases and extracting the SRL-aware SCFGrules.4.1 Extraction of Initial SRL-aware PhrasesFirst, a different heuristics is used to extract initialSRL-aware phrases.
These phrases have the follow-ing properties:1.
On the target side, the phrase covers at least onecomplete constituent in the PA structure, whichmust include the predicate.
The phrase pair caninclude words that are not part of any argument;however it cannot include partial arguments.
InFigure 4b), the phrase pair is not included in theinitial SRL-aware phrases because it includes aword A from argument ARG2.
However, inFigure 4a), inclusion of the first target word A,which is not part of any argument, is allowed.AAARG0PREDARG1ARG2ARG0PREDARG1ARG2a)Words(A)thatarenotpartofanyb)Words(A)inotherargumentsa)Words (A)that arenot part of anyargument are allowed.b)Words (A)in otherarguments(ARG2) are not allowedBCAARG0PREDARG1ARG2ARG0PREDARG1ARG2)Ulidd(A)thd)Ulidd(BC)thc)Unaligned words (A)on theboundaries of arguments(ARG1)are allowedd)Unaligned words (B,C) on theboundaries of  source sidephrasesare not allowe dFigure 4: Demonstration of restrictions of whether or nota rule is included in initial SRL-aware phrases.
The sub-figures a) and c) show two cases that unaligned wordsor words not in any arguments are allowed in extractedphrases and sub-figures b) and d) show two cases that thephrases are excluded from the phrase table.
The shadedblocks indicate the range of candidate phrases.2.
At least one word pair between the source andthe target side phrase is aligned, and no wordsin the source or the target side phrase alignto words outside the phrase pair.
These arethe standard heuristics used in the hierarchicalphrase extraction algorithm.3.
For the target side, unaligned words on theboundaries are allowed only if the word isfound inside one of the arguments.
On thesource side, however, unaligned words are notallowed on the boundaries.
The idea is demon-strated in Figure 4c) and 4d).
In Figure 4c), theunaligned boundary word A is included in thetarget side phrase because it is part of an argu-ment.
In Figure 4d), unaligned words B and Care not allowed to be included in the proposedphrase.Given a PA structure of the sentence, we appliedfollowing algorithm:1.
Extract all possible target side phrases that con-tain the predicate and any number of argu-ments.2.
For each of the extracted target side phrasesT , find the minimum span of the source sidephrase S that contains all the words aligned to111the target side phrase.
This can be done by sim-ply calculating the minimum and maximum in-dex of the source side words aligned to the tar-get side phrase.3.
Find the minimum span of target side phraseT1 that are aligned to the source side phrase S.If the minimum span is already covered by thetarget side phrase extracted in the previous step,i.e.
T1 = T , we add the phrase pair (S, T ) tothe pool of initial phrases.
If the newly obtainedtarget side phrase is larger than the original one,then we need to decide whether the new spancontains a word in another arguments.
If so,then we do not add the phrase pair, return tostep 2 and continue with the next target sidephrase.
Otherwise, we update T := T1 and goback to step 2.The readers may notice that although in severalsteps we need to determine whether there are linksoutside the phrase pairs, the information is easyto compute.
We only need to keep track of themaximum and minimum indices of words that eachsource and target word aligns to.
With the indicespre-computed, in the worst case scenario we onlyneed to calculate M times for the maximum andminimum indices, where M is the total number ofwords in the source and the target side, before wecan determine the validity of the largest target sideSRL-aware phrase.
The worst case complexity ofthe algorithm is O(N ?M), where N is the num-ber of arguments in the segmentation.
This is onlya rough upper bound for the time complexity; theaverage case will be much better.4.2 Extracting SRL-aware SCFG RulesBefore we generate rules from the extracted initialphrases, we first need to assign non-terminal la-bels to the initial SRL-aware phrases.
We define amap from the SRL structures to non-terminal tagsof SCFG rules.
An SRL-aware non-terminal labelis a combination of the predicate label and the ar-gument labels.
The predicate label is the stemmedpredicate.
We can eliminate the morphology to al-leviate the problem of the data scarcity.
In addition,the argument labels represent all the arguments thatthe current SRL-aware rule covers.
The mapping istrivial given the initial SRL-aware phrase extractionalgorithm, and it can be determined directly in thefirst step.The initial phrases already are SCFG rules.
Toextract rules with non-terminals we will replace thesub-phrases with non-terminals if the sub-phrase isembedded in another phrase pair.
The algorithm issimilar to that described by Chiang (2005).
Howeverwe apply new restrictions because we now have twosets of different initial phrases.
If the outer rule isSRL-aware, we allow both sets of the initial phrasesto be candidates of embedded phrases.
Howeverif the outer rule is X , we do not allow a replace-ment of SRL-aware SCFG rules within it.
There-fore we will have rules where LHS non-terminalsare SRL-aware, and some RHS non-terminals are X ,but not vice versa.
The reason for the restriction isto prevent the conversion of incomplete predicate-argument structures back to X .
As we mentionedbefore, one of the design goals of our algorithm is toensure that once SRL-aware SCFG rules are used inthe derivation, a complete PA structure must be gen-erated before it can be converted back.
The only wayof converting SRL-aware tags back to X is throughspecial conversion rules, whose LHS is the X andthe RHS is a complete SRL-aware tag.
Extractingsuch conversion rules is trivial given the SRL labels.The extracted rules are subject to filtering by thesame restrictions as conventional Hiero rules.
Thefiltering criteria include:1.
Two non-terminals on the source side shouldnot be adjacent.2.
We allow up to two non-terminals on the RHS.3.
The source side rule contains no more than fivetokens including terminals and non-terminals.5 Decoder IntegrationThe extracted SCFG rules, both SRL-aware and X ,will go through the feature estimation process toproduce the rule table.
Integrated with the con-version rules, most chart-based decoders such asMosesChart (Hoang and Koehn, 2008), cdec (Dyeret al 2010) and Joshua (Li et al 2009) can use theserules in decoding.
We applied MosesChart for tun-ing and decoding.While the SRL-aware SCFG rules are used to con-strain the search space and derivation, we do not in-112mt02 mt03 mt04 mt05 mt08 bl-nw bl-wb dv-nw dv-wb avgBLEU 29.56 27.02 30.28 26.80 21.16 21.96 20.10 24.26 20.13 n/aBaseline TER 68.87 70.19 67.18 70.60 69.93 64.44 64.74 63.21 66.61 n/a(T-B)/2 19.66 21.59 18.45 21.90 24.39 21.24 22.32 19.48 23.24 n/aBLEU +0.33 ?0.50 +0.20 +0.47 ?0.16 +1.24 +1.13 +0.39 +1.35 +0.49SRL TER ?1.58 ?1.77 ?1.93 ?1.68 ?0.71 ?0.29 ?0.22 ?1.36 ?1.34 ?1.21(T-B)/2 ?0.95 ?0.63 ?1.07 ?1.08 ?0.28 ?0.76 ?0.68 ?0.88 ?1.35 ?0.85Table 1: Experiment results on Chinese-English translation tasks, bl-nw and bl-wb are newswire and weblog parts forDEV07-blind, dv-nw and dv-wb are newswire and weblog parts for DEV07-dev.
We present the BLEU scores, TERscores and (TER-BLEU)/2.troduce new features into the system.
The featureswe used in the decoder are commonly used, includ-ing source and target rule translation probabilities,the lexical translation probabilities, and the languagemodel probability.
The feature values are calculatedby MLE estimation.Besides the expanded rule table and conversionrules, the decoder does not need to be modified.
Weincorporate MERT to tune the feature weights.
Theminimum modifications for the decoder make theproposed method an easy replacement for Hiero ruleextractors.6 Experiments and discussionWe performed experiments on Chinese to Englishtranslation tasks.
The data set we used in the exper-iments is a subset of the FBIS corpus.
We filter thecorpus with maximum sentence length be 30.
Thecorpus has 2.5 million words in Chinese side and3.1 million on English side.We adopted the ASSERT semantic role labeler(Pradhan et al, 2004) to label the English side sen-tences.
The parallel sentences are aligned usingMGIZA++ (Gao and Vogel, 2008) and then theproposed rule extraction algorithm was used in ex-tracting the SRL-aware SCFG rules.
We used theMosesChart decoder (Hoang and Koehn, 2008) andthe Moses toolkit (Koehn et al 2007) for tuning anddecoding.
The language model is a trigram languagemodel trained on English GIGAWord corpus (V1-V3) using the SRILM toolkit.We used the NIST MT06 test set for tuning, andexperimented with an additional 9 test sets, includ-ing MT02, 03, 04, 05, 08, and GALE test setsDEV07-dev and DEV07-blind.
DEV07-dev andDEV07-blind are further divided into newswire andweblog parts.We experimented with the proposed method andthe alternative methods presented in section 4, andthe results of nine test sets are listed in Table 1.
Aswe can observe from the results, the largest improve-ment we discovered from our proposed method ismore than 1 BLEU point, and a significant drop isonly observed on one test set, MT03, where we lose0.5 BLEU points.
Averaged across all the test sets,the improvement is 0.49 BLEU points on the smalltraining set.
When TER is also taken into account,all of the nine test sets showed consistent improve-ment.
The (TER-BLEU)/2 score, which we usedas the primary evaluation metric, improved by 0.85across nine test sets.As we expected, the coverage of SRL-awareSCFG rules is not as good as the Hiero rules.
Weanalyzed the top-best derivation of the results.
Only1836 out of 7235 sentences in the test sets used SRL-aware SCFG rules.
However, the BLEU scores onthe 1836 sentences improved from 27.98 in the base-line system to 28.80, while the remaining 5399 sen-tences only improved from 30.13 to 30.22.
The ob-servation suggests the potential for further improve-ment if we can increase the coverage by using moredata or by modifying the mapping from tags to thestructures to make rules more general.We display the hypothesis of a sentence in Fig-ure 5 to demonstrate a concrete example of improve-ments obtained by using the method,.
As this figuredemonstrate, the SRL-aware SCFG rules enable thesystem to pick the correct structure and reorderingfor the verbs trigger and enter.Given the results presented in the paper, the ques-tion arises as to whether it is prudent to integratemultiple formalisms or labeling systems, such as113	Ukraine because of the chaostriggered by the presidential election hasentered the third weekUkraine today because of the chaostriggered in the third week of the presidential electionThe chaoscausedby Ukraine's presidential election hasentered its third week.The turmoil in Ukrainetriggered by the presidential electionentered the third weekThe chaossparked off by the presidential election in Ukraine hasentered its third week.Ukraineheads into a third week of turmoilcausedby the presidential electionSRLTag Baseline Source ReferencesFigure 5: An example of improvement caused by better attachment of verbs and its argumentssyntactic parsing or SRL labeling.
Hierarchicalphrase-based machine translation is often criticizedfor not explicitly incorporating linguistic knowl-edge.
On the other hand, fully syntactic-based ma-chine translation suffers from low coverage of rules.The methodology in this paper, in contrast, intro-duces linguistic information to assist a formalismthat does not incorporate linguistic information.
Themerits of doing so are obvious.
While most parts ofthe system are not changed, a portion of the systemis considerably improved.
Also, the system encodesthe information in the non-terminal tags, which iswidely used in other methods such as SAMT.
How-ever, it is not necessary an optimal solution.
Huanget alin a very recent work (Huang et al, 2010) pro-posed using vector space to represent similarity be-tween the syntactic structures.
This is also an inter-esting possible direction to explore in the near fu-ture.7 Conclusion and future workIn this paper we presented a method of utilizing thetarget side predicate-argument structure to assist Hi-erarchical Phrase-based Machine Translation.
Witha hybrid rule extraction algorithm, we can extractSRL-aware SCFG rules together with conventionalHiero rules.
Additional conversion rules ensure thegenerated predicate-argument structures are com-plete when SRL-aware SCFG rules are used in thedecoding procedure.
Experimental results showedimprovement on BLEU and TER metrics with 9test sets, and even larger improvements are observedwhen only considering the sentences in which SRL-aware SCFG rules are used for the top-best deriva-tion.We are currently following three directions forthe future work.
The first focuses on improving thequality of the rules and feature estimation.
We areinvestigating different labeling systems other thanthe relatively simple PropBank labeling system, andplan to experiment with different methods of map-ping structure to the SRL-aware labels.Recent advances in vector space representationson the syntactic structures, which may be able towork with, or replace the SRL-aware non-terminallabels, inspire the second direction.Finally, the third direction is to incorporate sourceside semantic role labeling information into theframework.
Currently our method can only use tar-get side SRL information, but the source side in-formation is also valuable.
Exploring how to buildmodels to represent SRL information from bothsides into one complete framework is a promisingresearch direction to follow.ReferencesCollin F. Baker, Charles J. Fillmore, and Beau Cronin.2002.
The structure of the framenet database.
Inter-national Journal of Lexicography, 16(3):281?296.David Chiang.
2005.
A hierachical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of Association for Computa-tional Linguistics, pages 263?270.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.David Chiang.
2010.
Learning to translate with sourceand target syntax.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 1443?1452.Chris Dyer et al 2010. cdec: A decoder, alignment,and learning framework for finite-state and context-114free translation models.
In Proceedings of the ACL2010 System Demonstrations, ACLDemos ?10, pages7?12.Michel Galley et al 2006.
Scalable inference and train-ing of context-rich syntactic translation models.
InProceedings of the 21st International Conference onComputational Linguistics and the 44th annual meet-ing of the Association for Computational Linguistics,pages 961?968.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, pages 49?57, Columbus, Ohio,June.
Association for Computational Linguistics.Hieu Hoang and Philipp Koehn.
2008.
Design of themoses decoder for statistical machine translation.
InSoftware Engineering, Testing, and Quality Assurancefor Natural Language Processing, SETQA-NLP ?08,pages 58?65, Morristown, NJ, USA.
Association forComputational Linguistics.Zhongqiang Huang, Martin Cmejrek, and Bowen Zhou.2010.
Soft syntactic constraints for hierarchicalphrase-based translation using latent syntactic distri-butions.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 138?147, Cambridge, MA, October.
Associa-tion for Computational Linguistics.Philipp Koehn et al 2007.
Moses: Open source toolkitfor statistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association for Com-putational Linguistics Companion Volume Proceed-ings of the Demo and Poster Sessions, pages 177?180,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Zhifei Li et al 2009.
Joshua: an open source toolkit forparsing-based machine translation.
In Proceedings ofthe Fourth Workshop on Statistical Machine Transla-tion, StatMT ?09, pages 135?139.Ding Liu and Daniel Gildea.
2008.
Improved tree-to-string transducers for machine translation.
In ACLWorkshop on Statistical Machine Translation (ACL08-SMT), pages 62?69.Ding Liu and Daniel Gildea.
2010.
Semantic role fea-tures for machine translation.
In Proceedings of the23rd International Conference on Computational Lin-guistics (COLING-10).Yang Liu and Liang Huang.
2010.
Tree-based and forest-based translation.
In Tutorial Abstracts of ACL 2010,page 2, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for ComputationalLinguistics, pages 609?616.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106, Mar.Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Daniel Jurafsky.
2004.
Shal-low semantic parsing using support vector machines.In Proceedings of the Human Language TechnologyConference/North American chapter of the Associa-tion for Computational Linguistics annual meeting(HLT/NAACL-2004).Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProceedings of ACL-08: HLT, pages 577?585, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Dekai Wu and Pascale Fung.
2009.
Semantic roles forsmt: a hybrid two-pass model.
In Proceedings of The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,Companion Volume: Short Papers, pages 13?16.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tang, and Sheng Li.
2008.
A tree se-quence alignment-based tree-to-tree translation model.In Proceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-08 HLT),pages 559?567.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, pages 138?141.115
