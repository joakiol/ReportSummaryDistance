Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 28?36,Beijing, August 2010Sentence Analysis and Collocation IdentificationEric Wehrli, Violeta Seretan, Luka NerimaLanguage Technology LaboratoryUniversity of Geneva{Eric.Wehrli, Violeta.Seretan, Luka.Nerima}@unige.chAbstractIdentifying collocations in a sentence, inorder to ensure their proper processing insubsequent applications, and performingthe syntactic analysis of the sentence areinterrelated processes.
Syntactic informa-tion is crucial for detecting collocations,and vice versa, collocational informationis useful for parsing.
This article describesan original approach in which collocationsare identified in a sentence as soon as pos-sible during the analysis of that sentence,rather than at the end of the analysis, as inour previous work.
In this way, priority isgiven to parsing alternatives involving col-locations, and collocational informationguide the parser through the maze of alter-natives.
This solution was shown to leadto substantial improvements in the perfor-mance of both tasks (collocation identifi-cation and parsing), and in that of a sub-sequent task (machine translation).1 IntroductionCollocations1 constitute a central language phe-nomenon and an impressive amount of work hasbeen devoted over the past decades to the automa-tic acquisition of collocational resources ?
as at-tested, among others, by initiatives like the MWE2008 shared task aimed at creating a repository ofreference data (Gre?goire et al, 2008).
However,little or no reference exist in the literature about1We adopt the lexicographic understanding for the termcollocation (Benson et al, 1986), as opposed to the Britishcontextualist tradition focused on statistical co-occurrence(Firth, 1957; Sinclair, 1991).the actual use made of these resources in otherNLP applications.In this paper, we consider the particular appli-cation of syntactic parsing.
Just as other types ofmulti-word expressions (henceforth, MWEs), col-locations are problematic for parsing because theyhave to be recognised and treated as a whole, ra-ther than compositionally, i.e., in a word by wordfashion (Sag et al, 2002).
The standard approachin dealing with MWEs in parsing is to applya ?words-with-spaces?
preprocessing step, whichmarks the MWEs in the input sentence as unitswhich will later be integrated as single blocks inthe parse tree built during analysis.We argue that such an approach, albeit suffi-ciently appropriate for some subtypes of MWEs2,is not really adequate for processing colloca-tions.
Unlike other expressions that are fixed orsemi-fixed3, collocations do not allow a ?words-with-spaces?
treatment because they have a highmorpho-syntactic flexibility.There is no systematic restriction, for instance,on the number of forms a lexical item (such as averb) may have in a collocation, on the order ofitems in a collocation, or on the number of wordsthat may intervene between these items.
Collo-cations are situated at the intersection of lexiconand grammar; therefore, they cannot be accountedfor merely by the lexical component of a parsingsystem, but have to be integrated to the grammati-cal component as well, as the parser has to consi-2Sag et al (2002) thoroughly discusses the extend towhich a ?words-with-spaces?
approach is appropriate for dif-ferent kinds of MWEs.3For instance, compound words: by and large, ad hoc;named entities: New York City; and non-decomposableidioms: shoot the breeze.28der all the possible syntactic realisations of collo-cations.Alternatively, a post-processing approach (suchas the one we pursued previously in Wehrli etal.
(2009b)) would identify collocations after thesyntactic analysis has been performed, and out-put a parse tree in which collocational relationsare highlighted between the composing items, inorder to inform the subsequent processing appli-cations (e.g., a machine translation application).Again, this solution is not fully appropriate, andthe reason lies with the important observation thatprior collocational knowledge is highly relevantfor parsing.
Collocational restrictions are, alongwith other types of information like selectionalpreferences and subcategorization frames, a majormeans of structural disambiguation.
Collocationalrelations between the words in a sentence provedvery helpful in selecting the most plausible amongall the possible parse trees for a sentence (Hindleand Rooth, 1993; Alshawi and Carter, 1994; Ber-thouzoz and Merlo, 1997; Wehrli, 2000).
Hence,the question whether collocations should be iden-tified in a sentence before or after parsing is not aneasy one.
The previous literature on parsing andcollocations fails to provide insightful details onhow this circular issue is (or can be) solved.In this paper, we argue that the identification ofcollocations and the construction of a parse treeare interrelated processes, that must be accountedfor simultaneously.
We present a processing mo-del in which collocations, if present in a lexicon,are identified in the input sentence during the ana-lysis of that sentence.
At the same time, they areused to rank competing parsing hypotheses.The paper is organised as follows.
Section 2reviews the previous work on the interrelationbetween parsing and processing of collocations(or, more generally, MWEs).
Section 3 introducesour approach, and section 4 evaluates it by compa-ring it against the standard non-simultaneous ap-proach.
Section 5 provides concluding remarksand presents directions for future work.2 Related WorkExtending the lexical component of a parser withMWEs was proved to contribute to a significantimprovement of the coverage and accuracy of par-sing results.
For instance, Brun (1998) comparedthe coverage of a French parser with and wi-thout terminology recognition in the preproces-sing stage.
She found that the integration of 210nominal terms in the preprocessing components ofthe parser resulted in a significant reduction of thenumber of alternative parses (from an average of4.21 to 2.79).
The eliminated parses were foundto be semantically undesirable.
No valid analy-sis were ruled out.
Similarly, Zhang and Kor-doni (2006) extended a lexicon with 373 additio-nal MWE lexical entries and obtained a significantincrease in the coverage of an English grammar(14.4%, from 4.3% to 18.7%).In the cases mentioned above, a ?words-with-spaces?
approach was used.
In contrast, Ale-gria et al (2004) and Villavicencio et al (2007)adopted a compositional approach to the enco-ding of MWEs, able to capture more morpho-syntactically flexible MWEs.
Alegria et al (2004)showed that by using a MWE processor in the pre-processing stage of their parser (in development)for Basque, a significant improvement in the POS-tagging precision is obtained.
Villavicencio et al(2007) found that the addition of 21 new MWEsto the lexicon led to a significant increase in thegrammar coverage (from 7.1% to 22.7%), withoutaltering the grammar accuracy.An area of intensive research in parsing isconcerned with the use of lexical preferences, co-occurrence frequencies, collocations, and contex-tually similar words for PP attachment disambi-guation.
Thus, an important number of unsupervi-sed (Hindle and Rooth, 1993; Ratnaparkhi, 1998;Pantel and Lin, 2000), supervised (Alshawi andCarter, 1994; Berthouzoz and Merlo, 1997), andcombined (Volk, 2002) methods have been deve-loped to this end.However, as Hindle and Rooth (1993) pointedout, the parsers used by such methods lack pre-cisely the kind of corpus-based information thatis required to resolve ambiguity, because manyof the existing attachments may be missing orwrong.
The current literature provides no indi-cation about the manner in which this circularproblem can be circumvented, and on whetherflexible MWEs should be processed before, du-ring or after the sentence analysis takes place.293 Parsing and CollocationsAs argued by many researchers ?
e.g., Heid (1994)?
collocation identification is best performed onthe basis of parsed material.
This is due to thefact that collocations are co-occurrences of lexi-cal items in a specific syntactic configuration.
Thecollocation break record, for instance, is obtainedonly in the configurations where break is a verbwhose direct object is (semantically) headed bythe lexical item record.
In other words, the collo-cation is not defined in terms of linear proximity,but in terms of a specific grammatical relation.As the examples in this section show, the rela-tive order of the two items is not relevant, nor isthe distance between the two terms, which is unli-mited as long as the grammatical relation holds4.In our system, the grammatical relations are com-puted by a syntactic parser, namely, Fips (Wehrli,2007; Wehrli and Nerima, 2009).
Until now, thecollocation identification process took place at theend of the parse in a so-called ?interpretation?procedure applied to the complete parse trees.
Al-though quite successful, this way of doing pre-sents a major drawback: it happens too late tohelp the parser.
This section discusses this pointand describes the alternative that we are currentlydeveloping, which consists in identifying colloca-tions as soon as possible during the parse.One of the major hurdles for non-deterministicparsers is the huge number of alternatives thatmust be considered.
Given the high fre-quency of lexical ambiguities, the high level ofnon-determinism of natural language grammars,grammar-based parsers are faced with a numberof alternatives which grows exponentially with thelength of the input sentence.
Various methodshave been proposed to reduce that number, andin most cases heuristics are added to the parsingalgorithm to limit the number of alternatives.
Wi-thout such heuristics, the performance of a parsermight not be satisfactory enough for large scaleapplications such as machine translation or othertasks involving large corpora.We would like to argue, along the lines ofprevious work (section 2), that collocations can4Goldman et al (2001) report examples in which the dis-tance between the two terms of a collocation can exceed 30words.contribute to the disambiguation process so cru-cial for parsing.
To put it differently, identifyingcollocations should not be seen as a burden, as anadditional task the parser should perform, but onthe contrary as a process which may help the par-ser through the maze of alternatives.
Collocations,in their vast majority, are made of frequently usedterms, often highly ambiguous (e.g., break record,loose change).
Identifying them and giving themhigh priority over alternatives is an efficient wayto reduce the ambiguity level.
Ambiguity reduc-tion through the identification of collocations isnot limited to lexical ambiguities, but also appliesto attachment ambiguities, and in particular to thewell-known problem of PP attachment.
Considerthe following French examples in which the pre-positions are highlighted:(1)a. ligne de partage des eaux (?watershed?)b.
syste`me de gestion de base de donne?es (?da-tabase management system?)c.
force de maintien de la paix (?peacekeepingforce?)d.
organisation de protection del?environnement (?environmental protectionagency?
)In such cases, the identification of a noun-preposition-noun collocation will prevent or dis-courage any other type of prepositional attach-ment that the parser would otherwise consider.3.1 The MethodTo fulfill the goal of interconnecting the parsingprocedure and the identification of collocations,we have incorporated the collocation identifica-tion mechanism within the constituent attachmentprocedure of our parser Fips (Wehrli, 2007).
Thisparser, like many grammar-based parsers, usesleft attachment and right attachment rules to buildrespectively left subconstituents and right sub-constituents.
Given the fact that Fips?
rules alwaysinvolve exactly two constituents ?
see Wehrli(2007) for details ?
it is easy to add to the attach-ment mechanism the task of collocation identifica-tion.
To take a very simple example, when the ruleattaching a prenominal adjective to a noun applies,the collocation identification procedure is invo-ked.
It first verifies that both terms bear the lexical30feature [+partOfCollocation], which signals that agiven word is associated in our lexical database toone or several collocations, and then searches thecollocation database for an adjective-noun collo-cation with those two terms.
If successful, the cor-responding parse tree will be given a high priority.With examples such as loose change, the iden-tification of the collocation will immediately re-legate any (partial) analysis based on the verbalreading of either terms.To take a somewhat more complex example,consider a verb-object collocation such as breakrecord, as in example (2)5.(2)a.
John broke a record.b.
[TP [DP John ] broke [DP a [NP record ] ] ]Here, it is a right attachment rule which willtrigger the identification procedure.
To be precise,the right attachment rule in this case concerns theattachment of the noun record as complementof the indefinite determiner (head of the DPdirect object of the verb).
The identificationprocedure considers, in turn, all the governingnodes dominating the noun record, halting at thefirst node of category Noun6, Verb or Adjective.In our example, the determiner node and thenthe verb node will be considered.
Notice that theprocedure will, quite correctly, identify a colloca-tion in the French example (3a), but not in (3b),although both structures are identical.
The reasonhas to do with the fact that the noun governingrecord in the first example is a [+number] noun,that is a classifier noun which is transparent forthe identification procedure7.(3)a.
Jean a battu un grand nombre de records.
?Jean broke a large number of records?5We use the following labels in our phrase-structure re-presentations: TP-Tense phrase, for simple sentence (the S ofstandard CFG), CP-Complementizer phrase, for a sentencewith a conjunction or a complementizer, DP-Determinerphrase for standard noun phrases (we assume the DP hy-pothesis, whereby the determiner constitutes the syntac-tic head of a noun phrase), NP-Noun phrase for nominalprojections (nouns with their modifiers/complements), VP-Verb phrase, PP-Prepositional phrase, AP-Adjectival phrase,AdvP-Adverbial phrase, FP-Functional phrase (used for se-condary predicates).6Unless the node it marked [+number], as we will seeshortly.7See Fontenelle (1999) for a detailed account of transpa-rent nouns.b.
Jean a battu le de?tenteur du record.
?Jean has beaten the holder of the record?As in the other examples, an analysis in whicha collocation has been found is given high prio-rity over alternatives.
In the case of (2), this willrelegate potential analyses based on the adjectivalreading of broke or the verbal reading of record.Notice that exactly the same procedure applieswhen the trace of an extraposed element is (right)inserted, as in the examples (4), which illustratethe case of wh-interrogative (a), relative clause(b), tough-movement (c).(4)a.
Which record will Paul try to break ?b.
The record Paul broke was very old.c.
This record is difficult to break.In all such cases, that is, when the right insertedelement is a trace, the identification procedurewill consider its antecedent, or to be more precise,the semantic head of its antecedent.
Finally, thegrammatical processes involved in example (4a,c)can combine as in the more complex example (5),for which we give the slightly simplified structurewith the chain of elements with index i extendingfrom the fronted wh-phrase which record to thedirect object position of the verb break, via thedirect object position of the verb consider and thesubject position of the secondary predicate (FP)headed by the [+tough] adjective difficult.(5)a.
Which record did Paul consider difficult tobreak ?b.
[ CP [ DP which record]i [ TP did [ DP Paul] [ VP consider ][ DP e]i [ FP [ DP e]i [ APdifficult [ TP to [ VP break [ DP e]i ] ] ] ] ] ]3.2 Complex CollocationsAs stated, for instance, by (Heid, 1994), colloca-tions can involve more than two (main) terms andit is possible to adopt a recursive definition of col-locations, i.e., complex collocations can be vie-wed as collocations of collocations.
The colloca-tion identification procedure has been extended tohandle such cases.
Consider examples (6) below.(6)a.
La voiture tombera probablement en panned?essence.
?the car will probably run out of gas?31b.
natural language processingc.
He broke a world record.In the French sentence (6a), panne d?essence(literally, ?breakdown of gas?, ?out of gas?)
isa collocation of type Noun+Prep+Noun, whichcombines with the verb tomber (literally, ?tofall?)
to form a larger collocation of typeVerb+PrepObject tomber en panne d?essence (?torun out of gas?).
Given the strict left to rightprocessing order assumed by the parser, it willfirst identify the collocation tomber en panne (?tobreak down?)
when attaching the word panne.Then, reading the last word, essence (?gas?
), theparser will first identify the collocation panned?essence.
Since that collocation bears the lexi-cal feature [+partOfCollocation], the identifica-tion procedure goes on, through the governorsof that item.
The search succeeds with the verbtomber, and the collocation tomber en panned?essence (?run out of gas?)
is identified.4 Evaluation ExperimentsIn this section, we describe the experiments weperformed in order to evaluate the precision andrecall of the method introduced in section 3, andto compare it against the previous method (fullydescribed in Wehrli et al (2009b)).
We extendthis comparison by performing a task-based eva-luation, which investigates the impact that the newmethod has on the quality of translations produ-ced by a machine translation system relying onour parser (Wehrli et al, 2009a).4.1 Precision EvaluationThe data considered in this experiment consist ofa subpart of a corpus of newspaper articles collec-ted from the on-line version of The Economist8,containing slightly more that 0.5 million words.On these data, we run two versions of our parser:?
V1: a version implementing the previous me-thod of collocation identification,?
V2: a version implementing the new methoddescribed in section 3.8URL:http://www.economist.com/(accessed June, 2010).The lexicon of the parser was kept constant,which is to say that both versions used the samelexicon (which contains slightly more than 7500English collocation entries), only the parsing mo-dule handling collocations was different.
Fromthe output of each parser version, we collectedstatistics on the number of collocations (presentin the lexicon) that were identified in the test cor-pus.
More precisely, we traversed the output treesand counted the items that were marked as col-location heads, each time this was the case (notethat an item may participate in several colloca-tions, not only one).
Table 1 presents the num-ber of collocations identified, both with respect tocollocation instances and collocation types.V1 V2 common V1 only V2 onlyTokens 4716 5412 4347 399 1003Types 1218 1301 1182 143 368Table 1.
Collocation identification results.As the results show, the new method (columnV2) is more efficient in retrieving collocation ins-tances.
It detects 696 more instances, which cor-respond to an increase of 14.8% relative to theprevious method (column V1).
As we lack themeans to compare on a large scale the correspon-ding syntactic trees, we can only speculate that theincrease is mainly due to the fact that more appro-priate analyses are produced by the new method.A large number of instances are found by bothversions of the parser.
The difference betweenthe two methods is more visible for some syn-tactic types than for others.
Table 2 details thenumber of instances of each syntactic type whichare retrieved exclusively by one method or by theother.To measure the precision of the two methods,we randomly selected 20 collocation instancesamong those identified by each version of the par-ser, V1 and V2, and manually checked whetherthese instances are correct.
Correctness meansthat in the given context (i.e., the sentence inwhich they were identified), the word combina-tion marked as instance of a lexicalized colloca-tion is indeed an instance of that collocation.
Acounterexample would be, for instance, to markthe pair decision - make in the sentence in (7) as32Syntactic type V1 V2 Difference V2-V1A-N 72 152 80N-N 63 270 207V-O 22 190 168V-P-N 6 10 4N-P-N 1 62 61V-A 25 166 141P-N 200 142 -58N&N 6 2 -4Adv-Adv 4 9 5Table 2.
Differences between the two methods:number of tokens retrieved exclusively by eachmethod.an instance of the verb-object collocation to makea decision, which is an entry in our lexicon.(7)a.
The decision to make an offer to buy or sellproperty at price is a management decisionthat cannot be delegated to staff.Since judging the correctness of a collocation ins-tance in context is a rather straightforward task,we do not require multiple judges for this evalua-tion.
The precision obtained is 90% for V1, and100% for V2.The small size of test set is motivated by thefact that the precision is expected to be very high,since the presence of both collocation componentsin a sentence in the relevant syntactic relation al-most certainly means that the recognition of thecorresponding collocation is justified.
Exceptionswould correspond to a minority of cases in whichthe parser either wrongly establishes a relationbetween two items which happen to belong to anentry in the lexicon, or the two items are relatedbut the combination corresponds to a literal usage(examples are provided later in this section).The errors of V1 correspond, in fact, to cases inwhich a combination of words used literally waswrongly attributed to a collocation: in example(8a), V1 assigned the words on and business tothe lexical entry on business, and in example (8b),it assigned in and country to the entry in the coun-try9.(8)a.
It is not, by any means, specific to thecountryside, but it falls especially heavily onsmall businesses.9V1 makes the same error on (8a), but does better on (8b).These expressions are frozen and should not be treated asstandard collocations.b.
Industrial labour costs in western Germanyare higher than in any other country.To better pinpoint the difference between V1and V2, we performed a similar evaluation on anadditional set of 20 instances, randomly selectedamong the collocations identified exclusively byeach method.
Thus, the precision of V1, whenmeasured on the tokens in ?V1 only?, was 65%.The precision of V2 on ?V2 only?
was 90%.
The2 errors of V2 concern the pair in country, foundin contexts similar to the one shown in example(8b).
The errors of V1 also concerned the samepair, with one exception ?
the identification of thecollocation world trade from the context the des-truction of the World Trade Centre.
Since WorldTrade Centre is not in the parser lexicon, V1 ana-lysed it and assigned the first two words to the en-try world trade.
World was wrongly attached toTrade, rather than to Centre.When reported on the totality of the instancestested, the precision of V1 is 77.5% and that ofV2 is 95%.
Besides the increase in the precisionof identified collocations, the new method alsocontributes to an increase in the parser coverage10,from 81.7% to 83.3%.
The V1 parser version suc-ceeds in building a complete parse tree for 23187of the total 28375 sentences in the corpus, whileV2 does so for 23629 sentences.4.2 Recall EvaluationTo compare the recall of two methods we perfor-med a similar experiment, in which we run the twoversions of the parser, V1 and V2, on a small col-lection of sentences containing annotated colloca-tion instances.
These sentences were randomlyselected from the Europarl corpus (Koehn, 2005).The collocations they contain are all verb-objectcollocations.
We limit our present investigationto this syntactic type for two reasons: a) anno-tating a corpus with all instances of collocationentries in the lexicon would be a time-consumingtask; and b) verb-object collocations are amongthe most syntactically flexible and therefore diffi-cult to detect in real texts.
Thus, this test set pro-vides realistic information on recall.10Coverage refers more precisely to the ratio of sentencesfor which a complete parse tree could be built.33The test set is divided in two parts: 100 sen-tences are in English, and 100 other in Italian,which allows for a cross-linguistic evaluation ofthe two methods.
Each sentence contains one an-notated collocation instance, and there are 10 ins-tances for a collocation type.
Table 3 lists the col-location types in the test set (the even rows in co-lumn 2 display the glosses for the words in theItalian collocations).English Italianbridge gap assumere atteggiamento?assume?
?attitude?draw distinction attuare politica?carry out?
?policy?foot bill avanzare proposta?advance?
?proposal?give support avviare dialogo?start?
?dialogue?hold presidency compiere sforzo?commit?
?effort?meet condition dare contributo?give?
?contribution?pose threat dedicare attenzione?dedicate?
?attention?reach compromise operare scelta?operate?
?choice?shoulder responsibility porgere benvenuto?give?
?welcome?strike balance raggiungere intesa?reach?
?understanding?Table 3.
Collocation types in the test set.The evaluation results are presented in table 4.V1 achieves 63% recall performance on the En-glish data, and 44% on the Italian data.
V2 showsconsiderably better results: 76% on English and66% on Italian data.
The poorer performanceof both methods on Italian data is explained bythe difference in performance between the Englishand Italian parsers, and more precisely, by the dif-ference in their grammatical coverage.
The En-glish parser succeeds in building a complete parsetree for more than 70% of the sentences in the testset, while the Italian parser only for about 60%.As found in the previous experiment (presen-ted in section 4.1), for both languages consideredin this experiment, the new method of processingcollocations contributes to improving the parsingcoverage.
The coverage of the English parser in-creases from 71% to 76%, and that of the Italianparser from 57% to 61%.V1 V2 Common V1 only V2 onlyEnglish 63 76 61 2 15Italian 44 66 42 2 24Table 4.
Recall evaluation results: number of cor-rect collocation instances identified.4.3 Task-based EvaluationIn addition to reporting the performance results byusing the standard measures of precision and re-call, we performed a task-based performance eva-luation, in which we quantified the impact that thenewly-proposed method has on the quality of theoutput of a machine translation system.
As theexamples in table 3 suggest, a literal translation ofcollocations is rarely the most appropriate.
In fact,as stated by Orliac and Dillinger (2003), know-ledge of collocations is crucial for machine trans-lation systems.
An important purpose in iden-tifying collocations with our parser is to enabletheir proper treatment in our translation system, arule-based system that performs syntactic transferby relying on the structures produced by the par-ser.In this system, the translation of a collocationtakes place as follows.
When the parser identi-fies a collocation in the source sentence, its com-ponent words are marked as collocation mem-bers, in order to prevent their literal translation.When the transfer module processes the collo-cation head, the system checks in the bilinguallexicon whether an entry exists for that colloca-tion.
If not, the literal translation will apply;otherwise, the transfer module projects a target-language structure as specified in the correspon-ding target lexical entry.
More precisely, the trans-fer yields a target language abstract representa-tion, to which grammatical transformations andmorphological generation will apply to create thetarget sentence.
The identification of collocationsin the source text is a necessary, yet not a sufficientcondition for their successful translation.In this experiment, we considered the test setdescribed in section 4.2 and we manually eva-luated the translation obtained for each colloca-tion instance.
Both subsets (100 English sen-tences and 100 Italian sentences) were translatedinto French.
We compared the translations obtai-34Task Measure Test set Language IncreaseCollocation identification precision 40 instances English 17.5%recall 200 instances English, Italian 17.5%100 instances English 13%100 instances Italian 22%Collocation translation precision 200 instances {English, Italian}-French 13%100 instances English-French 10%100 instances Italian-French 16%Parsing coverage 28375 sentences English 1.6%200 sentences English 5%200 sentences Italian 4%Table 5.
Summary of evaluation results.ned by relying on the versions V1 and V2 of ourparser (recall that V2 corresponds to the newly-proposed method and V1 to the previous method).The use of automatic metrics for evaluating thetranslation output was not considered appropriatein this context, since such n-gram based metricsunderestimate the effect that the substitution of asingle word (like in our case, the verb in a verb-object collocation) has on the fluency, adequacy,and even on the interpretability of the output sen-tence.The comparison showed that, for both languagepairs considered (English-French and Italian-French), the version of parser which integrates thenew method is indeed more useful for the ma-chine translation system than the previous version.When V2 was used, 10 more collocation instanceswere correctly translated from English to Frenchthan when using V1.
For the Italian-French pair,V2 helped correctly translating 16 more colloca-tion instances in comparison with V1.
This cor-responds to an increase in precision of 13% on thewhole test set of 200 sentences.
The increase inperformance obtained in all the experiments des-cribed in this section is summarized in table 5.5 ConclusionIn this paper, we addressed the issue of the inter-connection between collocation identification andsyntactic parsing, and we proposed an original so-lution for identifying collocations in a sentence assoon as possible during the analysis (rather than atthe end of the parsing process).
The major advan-tage of this approach is that collocational informa-tion may be used to guide the parser through themaze of alternatives.The experimental results performed showedthat the proposed method, which couples parsingand collocation identification, leads to substan-tial improvements in terms of precision and re-call over the standard identification method, whilecontributing to augment the coverage of the par-ser.
In addition, it was shown that it has a posi-tive impact on the results of a subsequent appli-cation, namely, machine translation.
Future workwill concentrate on improving our method so thatit accounts for all the possible syntactic configu-rations of collocational attachments, and on exten-ding its recall evaluation to other syntactic types.AcknowledgementsThanks to Lorenza Russo and Paola Merlo for athorough reading and comments.
Part of the re-search described in this paper has been supportedby a grant from the Swiss National Science Foun-dation, grant no 100015-117944.ReferencesAlegria, In?aki, Olatz Ansa, Xabier Artola, NereaEzeiza, Koldo Gojenola, and Ruben Urizar.
2004.Representation and treatment of multiword expres-sions in basque.
In Second ACL Workshop on Mul-tiword Expressions: Integrating Processing, pages48?55, Barcelona, Spain.Alshawi, Hiyan and David Carter.
1994.
Trainingand scaling preference functions for disambigua-tion.
Computational Linguistics, 20(4):635?648.Benson, Morton, Evelyn Benson, and Robert Ilson.1986.
The BBI Dictionary of English Word Combi-nations.
John Benjamins, Amsterdam/Philadelphia.Berthouzoz, Cathy and Paola Merlo.
1997.
Statis-tical ambiguity resolution for principle-based par-sing.
In Nicolov, Nicolas and Ruslan Mitkov, edi-35tors, Recent Advances in Natural Language Pro-cessing: Selected Papers from RANLP?97, CurrentIssues in Linguistic Theory, pages 179?186.
JohnBenjamins, Amsterdam/Philadelphia.Brun, Caroline.
1998.
Terminology finite-state pre-processing for computational LFG.
In Proceedingsof the 36th Annual Meeting of the Association forComputational Linguistics and 17th InternationalConference on Computational Linguistics, pages196?200, Morristown, NJ, USA.Firth, John R. 1957.
Papers in Linguistics 1934-1951.Oxford Univ.
Press, Oxford.Fontenelle, Thierry.
1999.
Semantic resources forword sense disambiguation: a sine qua non?
Lin-guistica e Filologia, (9):25?43.
Dipartimento diLinguistica e Letterature Comparate, Universita` de-gli Studi di Bergamo.Goldman, Jean-Philippe, Luka Nerima, and EricWehrli.
2001.
Collocation extraction using a syn-tactic parser.
In Proceedings of the ACL Work-shop on Collocation: Computational Extraction,Analysis and Exploitation, pages 61?66, Toulouse,France.Gre?goire, Nicole, Stefan Evert, and Brigitte Krenn,editors.
2008.
Proceedings of the LREC WorkshopTowards a Shared Task for Multiword Expressions(MWE 2008).
European Language Resources Asso-ciation (ELRA), Marrakech, Morocco.Heid, Ulrich.
1994.
On ways words work together?
research topics in lexical combinatorics.
In Pro-ceedings of the 6th Euralex International Congresson Lexicography (EURALEX ?94), pages 226?257,Amsterdam, The Netherlands.Hindle, Donald and Mats Rooth.
1993.
Structural am-biguity and lexical relations.
Computational Lin-guistics, 19(1):103?120.Koehn, Philipp.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings ofThe Tenth Machine Translation Summit (MT Sum-mit X), pages 79?86, Phuket, Thailand, September.Orliac, Brigitte and Mike Dillinger.
2003.
Collocationextraction for machine translation.
In Proceedingsof Machine Translation Summit IX, pages 292?298,New Orleans, Lousiana, USA.Pantel, Patrick and Dekang Lin.
2000.
An unsuper-vised approach to prepositional phrase attachmentusing contextually similar words.
In Proceedingsof the 38th Annual Meeting of the Association forComputational Linguistics, pages 101?108, HongKong, China.Ratnaparkhi, Adwait.
1998.
Statistical models for un-supervised prepositional phrase attachment.
In Pro-ceedings of the 36th Annual Meeting of the Associa-tion for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics,pages 1079?1085, Montreal, Quebec, Canada.Sag, Ivan A., Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for NLP.
In Pro-ceedings of the Third International Conference onIntelligent Text Processing and Computational Lin-guistics (CICLING 2002), pages 1?15, Mexico City.Sinclair, John.
1991.
Corpus, Concordance, Colloca-tion.
Oxford University Press, Oxford.Villavicencio, Aline, Valia Kordoni, Yi Zhang, MarcoIdiart, and Carlos Ramisch.
2007.
Validation andevaluation of automatically acquired multiword ex-pressions for grammar engineering.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and Computatio-nal Natural Language Learning (EMNLP-CoNLL),pages 1034?1043, Prague, Czech Republic, June.Volk, Martin.
2002.
Combining unsupervised andsupervised methods for PP attachment disambi-guation.
In Proceedings of the 19th Internatio-nal Conference on Computational Linguistics (CO-LING?02), pages 25?32, Taipei, Taiwan.Wehrli, Eric and Luka Nerima.
2009.
L?analyseursyntaxique Fips.
In Proceedings of the IWPT 2009ATALA Workshop: What French parsing systems?,Paris, France.Wehrli, Eric, Luka Nerima, and Yves Scherrer.
2009a.Deep linguistic multilingual translation and bilin-gual dictionaries.
In Proceedings of the FourthWorkshop on Statistical Machine Translation, pages90?94, Athens, Greece.
Association for Computa-tional Linguistics.Wehrli, Eric, Violeta Seretan, Luka Nerima, and Lo-renza Russo.
2009b.
Collocations in a rule-basedMT system: A case study evaluation of their trans-lation adequacy.
In Proceedings of the 13th AnnualMeeting of the European Association for MachineTranslation, pages 128?135, Barcelona, Spain.Wehrli, Eric.
2000.
Parsing and collocations.
InChristodoulakis, D., editor, Natural Language Pro-cessing, pages 272?282.
Springer Verlag.Wehrli, Eric.
2007.
Fips, a ?deep?
linguistic multilin-gual parser.
In ACL 2007 Workshop on Deep Lin-guistic Processing, pages 120?127, Prague, CzechRepublic.Zhang, Yi and Valia Kordoni.
2006.
Automated deeplexical acquisition for robust open texts processing.In Proceedings of LREC-2006, pages 275?280, Ge-noa, Italy.36
