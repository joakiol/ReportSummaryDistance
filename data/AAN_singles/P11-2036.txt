Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 206?211,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsInsertion Operator for Bayesian Tree Substitution GrammarsHiroyuki Shindo, Akinori Fujino, and Masaaki NagataNTT Communication Science Laboratories, NTT Corp.2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan{shindo.hiroyuki,fujino.akinori,nagata.masaaki}@lab.ntt.co.jpAbstractWe propose a model that incorporates an in-sertion operator in Bayesian tree substitutiongrammars (BTSG).
Tree insertion is helpfulfor modeling syntax patterns accurately withfewer grammar rules than BTSG.
The exper-imental parsing results show that our modeloutperforms a standard PCFG and BTSG fora small dataset.
For a large dataset, our modelobtains comparable results to BTSG, makingthe number of grammar rules much smallerthan with BTSG.1 IntroductionTree substitution grammar (TSG) is a promising for-malism for modeling language data.
TSG general-izes context free grammars (CFG) by allowing non-terminal nodes to be replaced with subtrees of arbi-trary size.A natural extension of TSG involves adding aninsertion operator for combining subtrees as intree adjoining grammars (TAG) (Joshi, 1985) ortree insertion grammars (TIG) (Schabes and Wa-ters, 1995).
An insertion operator is helpful for ex-pressing various syntax patterns with fewer gram-mar rules, thus we expect that adding an insertionoperator will improve parsing accuracy and realize acompact grammar size.One of the challenges of adding an insertion op-erator is that the computational cost of grammar in-duction is high since tree insertion significantly in-creases the number of possible subtrees.
Previouswork on TAG and TIG induction (Xia, 1999; Chi-ang, 2003; Chen et al, 2006) has addressed the prob-lem using language-specific heuristics and a maxi-mum likelihood estimator, which leads to overfittingthe training data (Post and Gildea, 2009).Instead, we incorporate an insertion operator in aBayesian TSG (BTSG) model (Cohn et al, 2011)that learns grammar rules automatically withoutheuristics.
Our model uses a restricted variant ofsubtrees for insertion to model the probability dis-tribution simply and train the model efficiently.
Wealso present an inference technique for handling atree insertion that makes use of dynamic program-ming.2 Overview of BTSG ModelWe briefly review the BTSG model described in(Cohn et al, 2011).
TSG uses a substitution operator(shown in Fig.
1a) to combine subtrees.
Subtrees forsubstitution are referred to as initial trees, and leafnonterminals in initial trees are referred to as fron-tier nodes.
Their task is the unsupervised inductionof TSG derivations from parse trees.
A derivationis information about how subtrees are combined toform parse trees.The probability distribution over initial trees is de-fined by using a Pitman-Yor process prior (Pitmanand Yor, 1997), that is,e |X ?
GXGX |dX , ?X ?
PYP (dX , ?X , P0 (?
|X )) ,where X is a nonterminal symbol, e is an initial treerooted with X , and P0 (?
|X ) is a base distributionover the infinite space of initial trees rooted with X .dX and ?X are hyperparameters that are used to con-trol the model?s behavior.
Integrating out all possi-ble values of GX , the resulting distribution is206p (ei |e?i, X, dX , ?X ) = ?ei,X + ?XP0 (ei, |X ) , (1)where ?ei,X =n?iei,X?dX ?tei,X?X+n?i?,Xand ?X =?X+dX ?t?,X?X+n?i?,X.
e?i = e1, .
.
.
, ei?1 are previously gen-erated initial trees, and n?iei,X is the number of timesei has been used in e?i.
tei,X is the number of ta-bles labeled with ei.
n?i?,X =?e n?ie,X and t?,X =?e te,X are the total counts of initial trees and ta-bles, respectively.
The PYP prior produces ?rich getricher?
statistics: a few initial trees are often usedfor derivation while many are rarely used, and this isshown empirically to be well-suited for natural lan-guage (Teh, 2006b; Johnson and Goldwater, 2009).The base probability of an initial tree, P0 (e |X ),is given as follows.P0 (e |X ) =?r?CFG(e)PMLE (r)??A?LEAF(e)sA??B?INTER(e)(1?
sB) , (2)where CFG (e) is a set of decomposed CFG produc-tions of e, PMLE (r) is a maximum likelihood esti-mate (MLE) of r. LEAF (e) and INTER (e) are setsof leaf and internal symbols of e, respectively.
sX isa stopping probability defined for each X .3 Insertion Operator for BTSG3.1 Tree Insertion ModelWe propose a model that incorporates an insertionoperator in BTSG.
Figure 1b shows an example ofan insertion operator.
To distinguish them from ini-tial trees, subtrees for insertion are referred to asauxiliary trees.
An auxiliary tree includes a specialnonterminal leaf node labeled with the same sym-bol as the root node.
This leaf node is referred toas a foot node (marked with the subscript ?*?).
Thedefinitions of substitution and insertion operators areidentical with those of TIG and TAG.Since it is computationally expensive to allow anyauxiliary trees, we tackle the problem by introduc-ing simple auxiliary trees, i.e., auxiliary trees whoseroot node must generate a foot node as an immediatechild.
For example, ?
(N (JJ pretty) N*)?
is a simpleauxiliary tree, but ?
(S (NP ) (VP (V think) S*))?
is(a)(b)Figure 1: Example of (a) substitution and (b) inser-tion (dotted line).not.
Note that we place no restriction on the initialtrees.Our restricted formalism is a strict subset of TIG.We briefly refer to some differences between TAG,TIG and our insertion model.
TAG generates treeadjoining languages, a strict superset of context-free languages, and the computational complexityof parsing is O(n6).
TIG is a similar formalismto TAG, but it does not allow wrapping adjunctionin TAG.
Therefore, TIG generates context-free lan-guages and the parsing complexity is O(n3), whichis a strict subset of TAG.
On the other hand, ourmodel prohibits neither wrapping adjunction in TAGnor simultaneous adjunction in TIG, and allows onlysimple auxiliary trees.
The expressive power andcomputational complexity of our formalism is iden-tical to TIG, however, our model allows us to de-fine the probability distribution over auxiliary treesas having the same form as BTSG model.
This en-sures that we can make use of a dynamic program-ming technique for training our model, which we de-scribe the detail in the next subsection.We define a probability distribution over simpleauxiliary trees as having the same form as eq.
1, thatis,207p (ei |e?i, X, d?X , ?
?X ) = ?
?ei,X + ?
?XP?0 (ei, |X ) , (3)where d?X and ?
?X are hyperparameters of the in-sertion model, and the definition of(?
?ei,X , ?
?X)isthe same as that of (?ei,X , ?X) in eq.
1.However, we need modify the base distributionover simple auxiliary trees, P ?0 (e |X ), as follows,so that all probabilities of the simple auxiliary treessum to one.P ?0 (e |X ) = P?MLE (TOP (e))?
?r?INTER_CFG(e)PMLE (r)?
?A?LEAF(e)sA ??B?INTER(e)(1?
sB) , (4)where TOP (e) is the CFG production thatstarts with the root node of e. For example,TOP (N (JJ pretty) (N*)) returns ?N ?
JJ N*?.INTER_CFG (e) is a set of CFG productions of eexcluding TOP (e).
P ?MLE (r?)
is a modified MLEfor simple auxiliary trees, which is given by{C(r?
)C(X?X?Y )+C(X?Y X?)
if r?includes a foot node0 elsewhere C (r?)
is the frequency of r?
in parse trees.It is ensured that P ?0 (e |X ) generates a foot node asan immediate child.We define the probability distribution over bothinitial trees and simple auxiliary trees with a PYPprior.
The base distribution over initial trees is de-fined as P0 (e |X ), and the base distribution oversimple auxiliary trees is defined as P ?0 (e |X ).
Aninitial tree ei replaces a frontier node with prob-ability p (ei |e?i, X, dX , ?X ).
On the other hand,a simple auxiliary tree e?i inserts an internal nodewith probability aX?p?(e?i??e?
?i, X, d?X , ?
?X), whereaX is an insertion probability defined for each X .The stopping probabilities are common to both ini-tial and auxiliary trees.3.2 Grammar DecompositionWe develop a grammar decomposition technique,which is an extension of work (Cohn and Blunsom,2010) on BTSG model, to deal with an insertionoperator.
The motivation behind grammar decom-position is that it is hard to consider all possibleFigure 2: Derivation of Fig.
1b transformed bygrammar decomposition.CFG rule probabilityNP(NP (DT the) (N girl))?DT(DT the)Nins (N girl) (1?
aDT)?
aNDT(DT the) ?
the 1Nins (N girl) ?Nins (N girl)(N (JJ pretty) N*) ??
(N (JJ pretty) N*),NNins (N girl)(N (JJ pretty) N*) ?
JJ(JJ pretty)N(N girl) (1?
aJJ)?
1JJ(JJ pretty) ?pretty 1N(N girl) ?girl 1Table 1: The rules and probabilities of grammar de-composition for Fig.
2.derivations explicitly since the base distribution as-signs non-zero probability to an infinite number ofinitial and auxiliary trees.
Alternatively, we trans-form a derivation into CFG productions and assignthe probability for each CFG production so that itsassignment is consistent with the probability distri-butions.
We can efficiently calculate an inside prob-ability (described in the next subsection) by employ-ing grammar decomposition.Here we provide an example of the derivationshown in Fig.
1b.
First, we can transform the deriva-tion in Fig.
1b to another form as shown in Fig.
2.In Fig.
2, all the derivation information is embed-ded in each symbol.
That is, NP(NP (DT the) (N girl)) isa root symbol of the initial tree ?
(NP (DT the) (Ngirl))?, which generates two child nodes: DT(DT the)and N(N girl).
DT(DT the) generates the terminal node?the?.
On the other hand, Nins (N girl) denotes thatN(N girl) is inserted by some auxiliary tree, andNins (N girl)(N (JJ pretty) N*) denotes that the inserted simple aux-iliary tree is ?
(N (JJ pretty) (N*))?.
The insertedauxiliary tree, ?
(N (JJ pretty) (N*))?, must generatea foot node: ?
(N girl)?
as an immediate child.208Second, we decompose the transformed tree intoCFG productions and then assign the probability foreach CFG production as shown in Table 1, whereaDT, aN and aJJ are insertion probabilities for non-terminal DT, N and JJ, respectively.
Note that theprobability of a derivation according to Table 1 isthe same as the probability of a derivation obtainedfrom the distribution over the initial and auxiliarytrees (i.e.
eq.
1 and eq.
3).In Table 1, we assume that the auxiliary tree?
(N (JJ pretty) (N*))?
is sampled from the firstterm of eq.
3.
When it is sampled from the sec-ond term, we alternatively assign the probability??
(N (JJ pretty) N*), N.3.3 TrainingWe use a blocked Metropolis-Hastings (MH) algo-rithm (Cohn and Blunsom, 2010) to train our model.The MH algorithm learns BTSG model parametersefficiently, and it can be applied to our insertionmodel.
The MH algorithm consists of the followingthree steps.
For each sentence,1.
Calculate the inside probability (Lari andYoung, 1991) in a bottom-up manner using thegrammar decomposition.2.
Sample a derivation tree in a top-down manner.3.
Accept or reject the derivation sample by usingthe MH test.The MH algorithm is described in detail in (Cohnand Blunsom, 2010).
The hyperparameters of ourmodel are updated with the auxiliary variable tech-nique (Teh, 2006a).4 ExperimentsWe ran experiments on the British National Cor-pus (BNC) Treebank 3 and the WSJ English PennTreebank.
We did not use a development set sinceour model automatically updates the hyperparame-ters for every iteration.
The treebank data was bina-rized using the CENTER-HEAD method (Matsuzakiet al, 2005).
We replaced lexical words with counts?
1 in the training set with one of three unknown1Results from (Cohn and Blunsom, 2010).2Results for length ?
40.3http://nclt.computing.dcu.ie/~jfoster/resources/corpus method F1CFG 54.08BNC BTSG 67.73BTSG + insertion 69.06CFG 64.99BTSG 77.19WSJ BTSG + insertion 78.54(Petrov et al, 2006) 77.931(Cohn and Blunsom, 2010) 78.40Table 2: Small dataset experiments# rules (# aux.
trees) F1CFG 35374 (-) 71.0BTSG 80026 (0) 85.0BTSG + insertion 65099 (25) 85.3(Post and Gildea, 2009) - 82.62(Cohn and Blunsom, 2010) - 85.3Table 3: Full Penn Treebank dataset experimentswords using lexical features.
We trained our modelusing a training set, and then sampled 10k deriva-tions for each sentence in a test set.
Parsing resultswere obtained with the MER algorithm (Cohn et al,2011) using the 10k derivation samples.
We showthe bracketing F1 score of predicted parse trees eval-uated by EVALB4, averaged over three independentruns.In small dataset experiments, we used BNC (1ksentences, 90% for training and 10% for testing) andWSJ (section 2 for training and section 22 for test-ing).
This was a small-scale experiment, but largeenough to be relevant for low-resource languages.We trained the model with an MH sampler for 1kiterations.
Table 2 shows the parsing results forthe test set.
We compared our model with standardPCFG and BTSG models implemented by us.Our insertion model successfully outperformedCFG and BTSG.
This suggests that adding an inser-tion operator is helpful for modeling syntax trees ac-curately.
The BTSG model described in (Cohn andBlunsom, 2010) is similar to ours.
They reportedan F1 score of 78.40 (the score of our BTSG modelwas 77.19).
We speculate that the performance gapis due to data preprocessing such as the treatment ofrare words.4http://nlp.cs.nyu.edu/evalb/209(N?P (N?P ) (: ?
))(N?P (N?P ) (ADVP (RB respectively)))(P?P (P?P ) (, ,))(V?P (V?P ) (RB then))(Q?P (Q?P ) (IN of))( ?SBAR ( ?SBAR ) (RB not))(S?
(S? )
(: ;))Table 4: Examples of lexicalized auxiliary trees ob-tained from our model in the full treebank dataset.Nonterminal symbols created by binarization areshown with an over-bar.We also applied our model to the full WSJ PennTreebank setting (section 2-21 for training and sec-tion 23 for testing).
The parsing results are shown inTable 3.
We trained the model with an MH samplerfor 3.5k iterations.For the full treebank dataset, our model obtainednearly identical results to those obtained with BTSGmodel, making the grammar size approximately19% smaller than that of BTSG.
We can see that onlya small number of auxiliary trees have a great impacton reducing the grammar size.
Surprisingly, thereare many fewer auxiliary trees than initial trees.
Webelieve this to be due to the tree binarization and ourrestricted assumption of simple auxiliary trees.Table 4 shows examples of lexicalized auxiliarytrees obtained with our model for the full treebankdata.
We can see that punctuation (??
?, ?,?, and ?;?
)and adverb (RB) tend to be inserted in other trees.Punctuation and adverb appear in various positionsin English sentences.
Our results suggest that ratherthan treat those words as substitutions, it is more rea-sonable to consider them to be ?insertions?, which isintuitively understandable.5 SummaryWe proposed a model that incorporates an inser-tion operator in BTSG and developed an efficientinference technique.
Since it is computationally ex-pensive to allow any auxiliary trees, we tackled theproblem by introducing a restricted variant of aux-iliary trees.
Our model outperformed the BTSGmodel for a small dataset, and achieved compara-ble parsing results for a large dataset, making thenumber of grammars much smaller than the BTSGmodel.
We will extend our model to original TAGand evaluate its impact on statistical parsing perfor-mance.ReferencesJ.
Chen, S. Bangalore, and K. Vijay-Shanker.
2006.Automated extraction of Tree-Adjoining Grammarsfrom treebanks.
Natural Language Engineering,12(03):251?299.D.
Chiang, 2003.
Statistical Parsing with an Automati-cally Extracted Tree Adjoining Grammar, chapter 16,pages 299?316.
CSLI Publications.T.
Cohn and P. Blunsom.
2010.
Blocked inference inBayesian tree substitution grammars.
In Proceedingsof the ACL 2010 Conference Short Papers, pages 225?230, Uppsala, Sweden, July.
Association for Compu-tational Linguistics.T.
Cohn, P. Blunsom, and S. Goldwater.
2011.
Induc-ing tree-substitution grammars.
Journal of MachineLearning Research.
To Appear.M.
Johnson and S. Goldwater.
2009.
Improving non-parameteric Bayesian inference: experiments on unsu-pervised word segmentation with adaptor grammars.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (HLT-NAACL), pages 317?325, Boulder, Col-orado, June.
Association for Computational Linguis-tics.A.K.
Joshi.
1985.
Tree adjoining grammars: How muchcontext-sensitivity is required to provide reasonablestructural descriptions?
Natural Language Parsing:Psychological, Computational, and Theoretical Per-spectives, pages 206?250.K.
Lari and S.J.
Young.
1991.
Applications of stochasticcontext-free grammars using the inside-outside algo-rithm.
Computer Speech & Language, 5(3):237?257.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with latent annotations.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics (ACL), pages 75?82.
Associationfor Computational Linguistics.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th Annual Meeting of the Association for Computa-tional Linguistics (ICCL-ACL), pages 433?440, Syd-ney, Australia, July.
Association for ComputationalLinguistics.210J.
Pitman and M. Yor.
1997.
The two-parameter Poisson-Dirichlet distribution derived from a stable subordina-tor.
The Annals of Probability, 25(2):855?900.M.
Post and D. Gildea.
2009.
Bayesian learning of atree substitution grammar.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 45?48,Suntec, Singapore, August.
Association for Computa-tional Linguistics.Y.
Schabes and R.C.
Waters.
1995.
Tree insertion gram-mar: a cubic-time, parsable formalism that lexicalizescontext-free grammar without changing the trees pro-duced.
Fuzzy Sets and Systems, 76(3):309?317.Y.
W. Teh.
2006a.
A Bayesian interpretation of interpo-lated Kneser-Ney.
Technical Report TRA2/06, Schoolof Computing, National University of Singapore.Y.
W. Teh.
2006b.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and the 44th Annual Meeting ofthe Association for Computational Linguistics (ICCL-ACL), pages 985?992.F.
Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Proceedings of the 5th Natu-ral Language Processing Pacific Rim Symposium (NL-PRS), pages 398?403.211
