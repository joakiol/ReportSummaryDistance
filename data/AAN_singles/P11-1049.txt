Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 481?490,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsJointly Learning to Extract and CompressTaylor Berg-Kirkpatrick Dan Gillick Dan KleinComputer Science DivisionUniversity of California at Berkeley{tberg, dgillick, klein}@cs.berkeley.eduAbstractWe learn a joint model of sentence extractionand compression for multi-document summa-rization.
Our model scores candidate sum-maries according to a combined linear modelwhose features factor over (1) the n-gramtypes in the summary and (2) the compres-sions used.
We train the model using a margin-based objective whose loss captures end sum-mary quality.
Because of the exponentiallylarge set of candidate summaries, we use acutting-plane algorithm to incrementally de-tect and add active constraints efficiently.
In-ference in our model can be cast as an ILPand thereby solved in reasonable time; we alsopresent a fast approximation scheme whichachieves similar performance.
Our jointlyextracted and compressed summaries outper-form both unlearned baselines and our learnedextraction-only system on both ROUGE andPyramid, without a drop in judged linguis-tic quality.
We achieve the highest publishedROUGE results to date on the TAC 2008 dataset.1 IntroductionApplications of machine learning to automatic sum-marization have met with limited success, and, as aresult, many top-performing systems remain largelyad-hoc.
One reason learning may have provided lim-ited gains is that typical models do not learn to opti-mize end summary quality directly, but rather learnintermediate quantities in isolation.
For example,many models learn to score each input sentence in-dependently (Teufel and Moens, 1997; Shen et al,2007; Schilder and Kondadadi, 2008), and then as-semble extractive summaries from the top-rankedsentences in a way not incorporated into the learn-ing process.
This extraction is often done in thepresence of a heuristic that limits redundancy.
Asanother example, Yih et al (2007) learn predictorsof individual words?
appearance in the references,but in isolation from the sentence selection proce-dure.
Exceptions are Li et al (2009) who take amax-margin approach to learning sentence valuesjointly, but still have ad hoc constraints to handleredundancy.
One main contribution of the currentpaper is the direct optimization of summary qualityin a single model; we find that our learned systemssubstantially outperform unlearned counterparts onboth automatic and manual metrics.While pure extraction is certainly simple and doesguarantee some minimal readability, Lin (2003)showed that sentence compression (Knight andMarcu, 2001; McDonald, 2006; Clarke and Lap-ata, 2008) has the potential to improve the resultingsummaries.
However, attempts to incorporate com-pression into a summarization system have largelyfailed to realize large gains.
For example, Zajic etal (2006) use a pipeline approach, pre-processingto yield additional candidates for extraction by ap-plying heuristic sentence compressions, but theirsystem does not outperform state-of-the-art purelyextractive systems.
Similarly, Gillick and Favre(2009), though not learning weights, do a limitedform of compression jointly with extraction.
Theyreport a marginal increase in the automatic word-overlap metric ROUGE (Lin, 2004), but a decline inmanual Pyramid (Nenkova and Passonneau, 2004).A second contribution of the current work is toshow a system for jointly learning to jointly com-press and extract that exhibits gains in both ROUGEand content metrics over purely extractive systems.Both Martins and Smith (2009) and Woodsend andLapata (2010) build models that jointly extract andcompress, but learn scores for sentences (or phrases)using independent classifiers.
Daume?
III (2006)481learns parameters for compression and extractionjointly using an approximate training procedure, buthis results are not competitive with state-of-the-artextractive systems, and he does not report improve-ments on manual content or quality metrics.In our approach, we define a linear model thatscores candidate summaries according to featuresthat factor over the n-gram types that appear in thesummary and the structural compressions used tocreate the sentences in the summary.
We train theseparameters jointly using a margin-based objectivewhose loss captures end summary quality throughthe ROUGE metric.
Because of the exponentiallylarge set of candidate summaries, we use a cuttingplane algorithm to incrementally detect and add ac-tive constraints efficiently.
To make joint learningpossible we introduce a new, manually-annotateddata set of extracted, compressed sentences.
Infer-ence in our model can be cast as an integer linearprogram (ILP) and solved in reasonable time usinga generic ILP solver; we also introduce a fast ap-proximation scheme which achieves similar perfor-mance.
Our jointly extracted and compressed sum-maries outperform both unlearned baselines and ourlearned extraction-only system on both ROUGE andPyramid, without a drop in judged linguistic quality.We achieve the highest published comparable results(ROUGE) to date on our test set.2 Joint ModelWe focus on the task of multi-document summariza-tion.
The input is a collection of documents, eachconsisting of multiple sentences.
The output is asummary of length no greater than Lmax.
Let x bethe input document set, and let y be a representationof a summary as a vector.
For an extractive sum-mary, y is as a vector of indicators y = (ys : s ?
x),one indicator ys for each sentence s in x.
A sentences is present in the summary if and only if its indica-tor ys = 1 (see Figure 1a).
Let Y (x) be the set ofvalid summaries of document set x with length nogreater than Lmax.While past extractive methods have assignedvalue to individual sentences and then explicitly rep-resented the notion of redundancy (Carbonell andGoldstein, 1998), recent methods show greater suc-cess by using a simpler notion of coverage: bigramsFigure 1: Diagram of (a) extractive and (b) joint extrac-tive and compressive summarization models.
Variablesys indicate the presence of sentences in the summary.Variables yn indicate the presence of parse tree nodes.Note that there is intentionally a bigram missing from (a).contribute content, and redundancy is implicitly en-coded in the fact that redundant sentences coverfewer bigrams (Nenkova and Vanderwende, 2005;Gillick and Favre, 2009).
This later approach is as-sociated with the following objective function:maxy?Y (x)?b?B(y)vb (1)Here, vb is the value of bigram b, andB(y) is the setof bigrams present in the summary encoded by y.Gillick and Favre (2009) produced a state-of-the-artsystem1 by directly optimizing this objective.
Theylet the value vb of each bigram be given by the num-ber of input documents the bigram appears in.
Ourimplementation of their system will serve as a base-line, referred to as EXTRACTIVE BASELINE.We extend objective 1 so that it assigns value notjust to the bigrams that appear in the summary, butalso to the choices made in the creation of the sum-mary.
In our complete model, which jointly extractsand compresses sentences, we choose whether or notto cut individual subtrees in the constituency parses1See Text Analysis Conference results in 2008 and 2009.482of each sentence.
This is in contrast to the extractivecase where choices are made on full sentences.maxy?Y (x)?b?B(y)vb +?c?C(y)vc (2)C(y) is the set of cut choices made in y, and vcassigns value to each.Next, we present details of our representation ofcompressive summaries.
Assume a constituencyparse ts for every sentence s. We represent a com-pressive summary as a vector y = (yn : n ?
ts, s ?x) of indicators, one for each non-terminal node ineach parse tree of the sentences in the document setx.
A word is present in the output summary if andonly if its parent parse tree node n has yn = 1 (seeFigure 1b).
In addition to the length constraint onthe members of Y (x), we require that each noden may have yn = 1 only if its parent pi(n) hasypi(n) = 1.
This ensures that only subtrees maybe deleted.
While we use constituency parses ratherthan dependency parses, this model has similaritieswith the vine-growth model of Daume?
III (2006).For the compressive model we define the set ofcut choices C(y) for a summary y to be the set ofedges in each parse that are broken in order to deletea subtree (see Figure 1b).
We require that each sub-tree has a non-terminal node for a root, and say thatan edge (n, pi(n)) between a node and its parent isbroken if the parent has ypi(n) = 1 but the child hasyn = 0.
Notice that breaking a single edge deletesan entire subtree.2.1 ParameterizationBefore learning weights in Section 3, we parameter-ize objectives 1 and 2 using features.
This entails toparameterizing each bigram score vb and each sub-tree deletion score vc.
For weights w ?
Rd andfeature functions g(b, x) ?
Rd and h(c, x) ?
Rd welet:vb = wTg(b, x)vc = wTh(c, x)For example, g(b, x) might include a feature thecounts the number of documents in x that b appearsin, and h(c, x) might include a feature that indicateswhether the deleted subtree is an SBAR modifyinga noun.This parameterization allows us to cast summa-rization as structured prediction.
We can define afeature function f(y, x) ?
Rd which factors oversummaries y through B(y) and C(y):f(y, x) =?b?B(y)g(b, x) +?c?C(y)h(c, x)Using this characterization of summaries as featurevectors we can define a linear predictor for summa-rization:d(x;w) = argmaxy?Y (x)wTf(y, x) (3)= argmaxy?Y (x)?b?B(y)vb +?c?C(y)vcThe arg max in Equation 3 optimizes Objective 2.Learning weights for Objective 1 where Y (x) isthe set of extractive summaries gives our LEARNEDEXTRACTIVE system.
Learning weights for Objec-tive 2 where Y (x) is the set of compressive sum-maries, and C(y) the set of broken edges that pro-duce subtree deletions, gives our LEARNED COM-PRESSIVE system, which is our joint model of ex-traction and compression.3 Structured LearningDiscriminative training attempts to minimize theloss incurred during prediction by optimizing an ob-jective on the training set.
We will perform discrim-inative training using a loss function that directlymeasures end-to-end summarization quality.In Section 4 we show that finding summaries thatoptimize Objective 2, Viterbi prediction, is efficient.Online learning algorithms like perceptron or themargin-infused relaxed algorithm (MIRA) (Cram-mer and Singer, 2003) are frequently used for struc-tured problems where Viterbi inference is available.However, we find that such methods are unstable onour problem.
We instead turn to an approach thatoptimizes a batch objective which is sensitive to allconstraints on all instances, but is efficient by addingthese constraints incrementally.3.1 Max-margin objectiveFor our problem the data set consists of pairs of doc-ument sets and label summaries, D = {(xi,y?i ) :i ?
1, .
.
.
, N}.
Note that the label summaries483can be expressed as vectors y?
because our trainingsummaries are variously extractive or extractive andcompressive (see Section 5).
We use a soft-marginsupport vector machine (SVM) (Vapnik, 1998) ob-jective over the full structured output space (Taskaret al, 2003; Tsochantaridis et al, 2004) of extractiveand compressive summaries:minw12?w?2 +CNN?i=1?i (4)s.t.
?i,?y ?
Y (xi) (5)wT(f(y?i , xi)?
f(y, xi))?
`(y,y?i )?
?iThe constraints in Equation 5 require that the differ-ence in model score between each possible summaryy and the gold summary y?i be no smaller than theloss `(y,y?i ), padded by a per-instance slack of ?i.We use bigram recall as our loss function (see Sec-tion 3.3).
C is the regularization constant.
When theoutput space Y (xi) is small these constraints can beexplicitly enumerated.
In this case it is standard tosolve the dual, which is a quadratic program.
Un-fortunately, the size of the output space of extractivesummaries is exponential in the number of sentencesin the input document set.3.2 Cutting-plane algorithmThe cutting-plane algorithm deals with the expo-nential number of constraints in Equation 5 by per-forming constraint induction (Tsochantaridis et al,2004).
It alternates between solving Objective 4with a reduced set of currently active constraints,and adding newly active constraints to the set.
Inour application, this approach efficiently solves thestructured SVM training problem up to some speci-fied tolerance .Suppose w?
and ??
optimize Objective 4 under thecurrently active constraints on a given iteration.
No-tice that the y?i satisfyingy?i = argmaxy?Y (xi)[w?Tf(y, xi) + `(y,y?i )](6)corresponds to the constraint in the fully constrainedproblem, for training instance (xi,y?i ), most vio-lated by w?
and ??.
On each round of constraint induc-tion the cutting-plane algorithm computes the argmax in Equation 6 for a training instance, which isreferred to as loss-augmented prediction, and addsthe corresponding constraint to the active set.The constraints from Equation (5) are equivalentto: ?i wTf(y?i , xi) ?
maxy?Y (xi)[wTf(y, xi) +`(y,y?i )]?
?i.
Thus, if loss-augmented predictionturns up no new constraints on a given iteration, thecurrent solution to the reduced problem, w?
and ?
?,is the solution to the full SVM training problem.
Inpractice, constraints are only added if the right handside of Equation (5) exceeds the left hand side by atleast .
Tsochantaridis et al (2004) prove that onlyO(N ) constraints are added before constraint induc-tion finds a C-optimal solution.Loss-augmented prediction is not alwaystractable.
Luckily, our choice of loss function,bigram recall, factors over bigrams.
Thus, we caneasily perform loss-augmented prediction usingthe same procedure we use to perform Viterbiprediction (described in Section 4).
We simplymodify each bigram value vb to include bigramb?s contribution to the total loss.
We solve theintermediate partially-constrained max-marginproblems using the factored sequential minimaloptimization (SMO) algorithm (Platt, 1999; Taskaret al, 2004).
In practice, for  = 10?4, thecutting-plane algorithm converges after only threepasses through the training set when applied to oursummarization task.3.3 Loss functionIn the simplest case, 0-1 loss, the system only re-ceives credit for exactly identifying the label sum-mary.
Since there are many reasonable summarieswe are less interested in exactly matching any spe-cific training instance, and more interested in the de-gree to which a predicted summary deviates from alabel.The standard method for automatically evaluatinga summary against a reference is ROUGE, which wesimplify slightly to bigram recall.
With an extractivereference denoted by y?, our loss function is:`(y,y?)
=|B(y)?B(y?)||B(y?
)|We verified that bigram recall correlates well withROUGE and with manual metrics.4844 Efficient PredictionWe show how to perform prediction with the extrac-tive and compressive models by solving ILPs.
Formany instances, a generic ILP solver can find exactsolutions to the prediction problems in a matter ofseconds.
For difficult instances, we present a fastapproximate algorithm.4.1 ILP for extractionGillick and Favre (2009) express the optimization ofObjective 1 for extractive summarization as an ILP.We begin here with their algorithm.
Let each inputsentence s have length ls.
Let the presence of eachbigram b inB(y) be indicated by the binary variablezb.
LetQsb be an indicator of the presence of bigramb in sentence s. They specify the following ILP overbinary variables y and z:maxy,z?bvbzbs.t.
?slsys ?
Lmax?b?sQsb ?
zb (7)?s, b ysQsb ?
zb (8)Constraints 7 and 8 ensure consistency between sen-tences and bigrams.
Notice that the Constraint 7 re-quires that selecting a sentence entails selecting allits bigrams, and Constraint 8 requires that selectinga bigram entails selecting at least one sentence thatcontains it.
Solving the ILP is fast in practice.
Us-ing the GNU Linear Programming Kit (GLPK) ona 3.2GHz Intel machine, decoding took less than asecond on most instances.4.2 ILP for joint compression and extractionWe can extend the ILP formulation of extractionto solve the compressive problem.
Let ln be thenumber of words node n has as children.
Withthis notation we can write the length restriction as?n lnyn ?
Lmax.
Let the presence of each cut c inC(y) be indicated by the binary variable zc, whichis active if and only if yn = 0 but ypi(n) = 1, wherenode pi(n) is the parent of node n. The constraintson zc are diagrammed in Figure 2.While it is possible to let B(y) contain all bi-grams present in the compressive summary, the re-Figure 2: Diagram of ILP for joint extraction and com-pression.
Variables zb indicate the presence of bigramsin the summary.
Variables zc indicate edges in the parsetree that have been cut in order to remove subtrees.
Thefigure suppresses bigram variables zstopped,in and zfrance,heto reduce clutter.
Note that the edit shown is intentionallybad.
It demonstrates a loss of bigram coverage.duction of B(y) makes the ILP formulation effi-cient.
We omit fromB(y) bigrams that are the resultof deleted intermediate words.
As a result the re-quired number of variables zb is linear in the lengthof a sentence.
The constraints on zb are given inFigure 2.
They can be expressed in terms of the vari-ables yn.By solving the following ILP we can compute thearg max required for prediction in the joint model:maxy,z?bvbzb +?cvczcs.t.
?nlnyn ?
Lmax?n yn ?
ypi(n) (9)?b zb = 1[b ?
B(y)](10)?c zc = 1[c ?
C(y)](11)485Constraint 9 encodes the requirement that only fullsubtrees may be deleted.
For simplicity, we havewritten Constraints 10 and 11 in implicit form.These constraints can be encoded explicitly usingO(N) linear constraints, where N is the numberof words in the document set x.
The reduction ofB(y) to include only bigrams not resulting fromdeleted intermediate words avoids O(N2) requiredconstraints.In practice, solving this ILP for joint extractionand compression is, on average, an order of magni-tude slower than solving the ILP for pure extraction,and for certain instances finding the exact solution isprohibitively slow.4.3 Fast approximate predictionOne common way to quickly approximate an ILPis to solve its LP relaxation (and round the results).We found that, while very fast, the LP relaxation ofthe joint ILP gave poor results, finding unacceptablysuboptimal solutions.
This appears possibly to havebeen problematic for Martins and Smith (2009) aswell.
We developed an alternative fast approximatejoint extractive and compressive solver that givesbetter results in terms of both objective value andbigram recall of resulting solutions.The approximate joint solver first extracts a sub-set of the sentences in the document set that total nomore than M words.
In a second step, we apply theexact joint extractive and compressive summarizer(see Section 4.2) to the resulting extraction.
The ob-jective we maximize in performing the initial extrac-tion is different from the one used in extractive sum-marization.
Specifically, we pick an extraction thatmaximizes?s?y?b?s vb.
This objective rewardsredundant bigrams, and thus is likely to give the jointsolver multiple options for including the same pieceof relevant content.M is a parameter that trades-off between approx-imation quality and problem difficulty.
When Mis the size of the document set x, the approximatesolver solves the exact joint problem.
In Figure 3we plot the trade-off between approximation qualityand computation time, comparing to the exact jointsolver, an exact solver that is limited to extractivesolutions, and the LP relaxation solver.
The resultsshow that the approximate joint solver yields sub-stantial improvements over the LP relaxation, andFigure 3: Plot of objective value, bigram recall, andelapsed time for the approximate joint extractive andcompressive solver against size of intermediate extractionset.
Also shown are values for an LP relaxation approx-imate solver, a solver that is restricted to extractive so-lutions, and finally the exact compressive solver.
Thesesolvers do not use an intermediate extraction.
Results arefor 44 document sets, averaging about 5000 words perdocument set.can achieve results comparable to those produced bythe exact solver with a 5-fold reduction in compu-tation time.
On particularly difficult instances theparameter M can be decreased, ensuring that all in-stances are solved in a reasonable time period.5 DataWe use the data from the Text Analysis Conference(TAC) evaluations from 2008 and 2009, a total of92 multi-document summarization problems.
Eachproblem asks for a 100-word-limited summary of10 related input documents and provides a set offour abstracts written by experts.
These are the non-update portions of the TAC 2008 and 2009 tasks.To train the extractive system described in Sec-tion 2, we use as our labels y?
the extractions withthe largest bigram recall values relative to the setsof references.
While these extractions are inferiorto the abstracts, they are attainable by our model, aquality found to be advantageous in discriminativetraining for machine translation (Liang et al, 2006;486COUNT: 1(docCount(b) = ?)
where docCount(b) is thenumber of documents containing b.STOP: 1(isStop(b1) = ?, isStop(b2) = ?)
whereisStop(w) indicates a stop word.POSITION: 1(docPosition(b) = ?)
where docPosition(b) isthe earliest position in a document of any sen-tence containing b, buckets earliest positions?
4.CONJ: All two- and three-way conjunctions of COUNT,STOP, and POSITION features.BIAS: Bias feature, active on all bigrams.Table 1: Bigram features: component feature functionsin g(b, x) that we use to characterize the bigram b in boththe extractive and compressive models.Chiang et al, 2008).Previous work has referred to the lack of ex-tracted, compressed data sets as an obstacle to jointlearning for summarizaiton (Daume?
III, 2006; Mar-tins and Smith, 2009).
We collected joint data viaa Mechanical Turk task.
To make the joint anno-tation task more feasible, we adopted an approx-imate approach that closely matches our fast ap-proximate prediction procedure.
Annotators wereshown a 150-word maximum bigram recall extrac-tions from the full document set and instructed toform a compressed summary by deleting words un-til 100 or fewer words remained.
Each task was per-formed by two annotators.
We chose the summarywe judged to be of highest quality from each pairto add to our corpus.
This gave one gold compres-sive summary y?
for each of the 44 problems in theTAC 2009 set.
We used these labels to train our jointextractive and compressive system described in Sec-tion 2.
Of the 288 total sentences presented to anno-tators, 38 were unedited, 45 were deleted, and 205were compressed by an average of 7.5 words.6 FeaturesHere we describe the features used to parameterizeour model.
Relative to some NLP tasks, our fea-ture sets are small: roughly two hundred featureson bigrams and thirteen features on subtree dele-tions.
This is because our data set is small; withonly 48 training documents we do not have the sta-tistical support to learn weights for more features.For larger training sets one could imagine lexical-ized versions of the features we describe.COORD: Indicates phrase involved in coordination.
Fourversions of this feature: NP, VP, S, SBAR.S-ADJUNCT: Indicates a child of an S, adjunct to and left ofthe matrix verb.
Four version of this feature:CC, PP, ADVP, SBAR.REL-C: Indicates a relative clause, SBAR modifying anoun.ATTR-C: Indicates a sentence-final attribution clause,e.g.
?the senator announced Friday.
?ATTR-PP: Indicates a PP attribution, e.g.
?according to thesenator.
?TEMP-PP: Indicates a temporal PP, e.g.
?on Friday.
?TEMP-NP: Indicates a temporal NP, e.g.
?Friday.
?BIAS: Bias feature, active on all subtree deletions.Table 2: Subtree deletion features: component featurefunctions in h(c, x) that we use to characterize the sub-tree deleted by cutting edge c = (n, pi(n)) in the jointextractive and compressive model.6.1 Bigram featuresOur bigram features include document counts, theearliest position in a document of a sentence thatcontains the bigram, and membership of each wordin a standard set of stopwords.
We also include allpossible two- and three-way conjuctions of thesefeatures.
Table 1 describes the features in detail.We use stemmed bigrams and prune bigrams thatappear in fewer than three input documents.6.2 Subtree deletion featuresTable 2 gives a description of our subtree tree dele-tion features.
Of course, by training to optimize ametric like ROUGE, the system benefits from re-strictions on the syntactic variety of edits; the learn-ing is therefore more about deciding when an editis worth the coverage trade-offs rather than fine-grained decisions about grammaticality.We constrain the model to only allow subtreedeletions where one of the features in Table 2 (asidefrom BIAS) is active.
The root, and thus the entiresentence, may always be cut.
We choose this par-ticular set of allowed deletions by looking at humanannotated data and taking note of the most commontypes of edits.
Edits which are made rarely by hu-mans should be avoided in most scenarios, and wesimply don?t have enough data to learn when to dothem safely.487System BR R-2 R-SU4 Pyr LQLAST DOCUMENT 4.00 5.85 9.39 23.5 7.2EXT.
BASELINE 6.85 10.05 13.00 35.0 6.2LEARNED EXT.
7.43 11.05 13.86 38.4 6.6LEARNED COMP.
7.75 11.70 14.38 41.3 6.5Table 3: Bigram Recall (BR), ROUGE (R-2 and R-SU4)and Pyramid (Pyr) scores are multiplied by 100; Linguis-tic Quality (LQ) is scored on a 1 (very poor) to 10 (verygood) scale.7 Experiments7.1 Experimental setupWe set aside the TAC 2008 data set (48 problems)for testing and use the TAC 2009 data set (44 prob-lems) for training, with hyper-parameters set to max-imize six-fold cross-validation bigram recall on thetraining set.
We run the factored SMO algorithmuntil convergence, and run the cutting-plane algo-rithm until convergence for  = 10?4.
We usedGLPK to solve all ILPs.
We solved extractive ILPsexactly, and joint extractive and compressive ILPsapproximately using an intermediate extraction sizeof 1000.
Constituency parses were produced usingthe Berkeley parser (Petrov and Klein, 2007).
Weshow results for three systems, EXTRACTIVE BASE-LINE, LEARNED EXTRACTIVE, LEARNED COM-PRESSIVE, and the standard baseline that extractsthe first 100 words in the the most recent document,LAST DOCUMENT.7.2 ResultsOur evaluation results are shown in Table 3.ROUGE-2 (based on bigrams) and ROUGE-SU4(based on both unigrams and skip-bigrams, sepa-rated by up to four words) are given by the offi-cial ROUGE toolkit with the standard options (Lin,2004).Pyramid (Nenkova and Passonneau, 2004) is amanually evaluated measure of recall on facts orSemantic Content Units appearing in the referencesummaries.
It is designed to help annotators dis-tinguish information content from linguistic qual-ity.
Two annotators performed the entire evaluationwithout overlap by splitting the set of problems inhalf.To evaluate linguistic quality, we sent all the sum-maries to Mechanical Turk (with two times redun-System Sents Words/Sent Word TypesLAST DOCUMENT 4.0 25.0 36.5EXT.
BASELINE 5.0 20.8 36.3LEARNED EXT.
4.8 21.8 37.1LEARNED COMP.
4.5 22.9 38.8Table 4: Summary statistics for the summaries gener-ated by each system: Average number of sentences persummary, average number of words per summary sen-tence, and average number of non-stopword word typesper summary.dancy), using the template and instructions designedby Gillick and Liu (2010).
They report that Turk-ers can faithfully reproduce experts?
rankings of av-erage system linguistic quality (though their judge-ments of content are poorer).
The table shows aver-age linguistic quality.All the content-based metrics show substantialimprovement for learned systems over unlearnedones, and we see an extremely large improvementfor the learned joint extractive and compressive sys-tem over the previous state-of-the-art EXTRACTIVEBASELINE.
The ROUGE scores for the learnedjoint system, LEARNED COMPRESSIVE, are, to ourknowledge, the highest reported on this task.
Wecannot compare Pyramid scores to other reportedscores because of annotator difference.
As expected,the LAST DOCUMENT baseline outperforms othersystems in terms of linguistic quality.
But, impor-tantly, the gains achieved by the joint extractive andcompressive system in content-based metrics do notcome at the cost of linguistic quality when comparedto purely extractive systems.Table 4 shows statistics on the outputs of the sys-tems we evaluated.
The joint extractive and com-pressive system fits more word types into a sum-mary than the extractive systems, but also produceslonger sentences on average.
Reading the outputsummaries more carefully suggests that by learningto extract and compress jointly, our joint system hasthe flexibility to use or create reasonable, medium-length sentences, whereas the extractive systems arestuck with a few valuable long sentences, but severalless productive shorter sentences.
Example sum-maries produced by the joint system are given in Fig-ure 4 along with reference summaries produced byhumans.488LEARNED COMPRESSIVE: The country?s work safety authority willrelease the list of the first batch of coal mines to be closed down saidWang Xianzheng, deputy director of the National Bureau of Produc-tion Safety Supervision and Administration.
With its coal miningsafety a hot issue, attracting wide attention from both home and over-seas, China is seeking solutions from the world to improve its coalmining safety system.
Despite government promises to stem the car-nage the death toll in China?s disaster-plagued coal mine industry isrising according to the latest statistics released by the government Fri-day.
Fatal coal mine accidents in China rose 8.5 percent in the firsteight months of this year with thousands dying despite stepped-up ef-forts to make the industry safer state media said Wednesday.REFERENCE: China?s accident-plagued coal mines cause thousandsof deaths and injuries annually.
2004 saw over 6,000 mine deaths.January through August 2005, deaths rose 8.5% over the same periodin 2004.
Most accidents are gas explosions, but fires, floods, and cave-ins also occur.
Ignored safety procedures, outdated equipment, andcorrupted officials exacerbate the problem.
Official responses includeshutting down thousands of ill-managed and illegally-run mines, pun-ishing errant owners, issuing new safety regulations and measures,and outlawing local officials from investing in mines.
China alsosought solutions at the Conference on South African Coal MiningSafety Technology and Equipment held in Beijing.LEARNED COMPRESSIVE: Karl Rove the White House deputy chiefof staff told President George W. Bush and others that he never en-gaged in an effort to disclose a CIA operative?s identity to discredither husband?s criticism of the administration?s Iraq policy accordingto people with knowledge of Rove?s account in the investigation.
In apotentially damaging sign for the Bush administration special counselPatrick Fitzgerald said that although his investigation is nearly com-plete it?s not over.
Lewis Scooter Libby Vice President Dick Cheney?schief of staff and a key architect of the Iraq war was indicted Friday onfelony charges of perjury making false statements to FBI agents andobstruction of justice for impeding the federal grand jury investigatingthe CIA leak case.REFERENCE: Special Prosecutor Patrick Fitzgerald is investigatingwho leaked to the press that Valerie Plame, wife of former Ambas-sador Joseph Wilson, was an undercover CIA agent.
Wilson was acritic of the Bush administration.
Administration staffers Karl Roveand I. Lewis Libby are the focus of the investigation.
NY Times cor-respondent Judith Miller was jailed for 85 days for refusing to testifyabout Libby.
Libby was eventually indicted on five counts: 2 falsestatements, 1 obstruction of justice, 2 perjury.
Libby resigned imme-diately.
He faces 30 years in prison and a fine of $1.25 million ifconvicted.
Libby pleaded not guilty.Figure 4: Example summaries produced by our learnedjoint model of extraction and compression.
These areeach 100-word-limited summaries of a collection of tendocuments from the TAC 2008 data set.
Constituents thathave been removed via subtree deletion are grayed out.References summaries produced by humans are providedfor comparison.8 ConclusionJointly learning to extract and compress within aunified model outperforms learning pure extraction,which in turn outperforms a state-of-the-art extrac-tive baseline.
Our system gives substantial increasesin both automatic and manual content metrics, whilemaintaining high linguistic quality scores.AcknowledgementsWe thank the anonymous reviewers for their com-ments.
This project is supported by DARPA undergrant N10AP20007.ReferencesJ.
Carbonell and J. Goldstein.
1998.
The use of MMR,diversity-based reranking for reordering documentsand producing summaries.
In Proc.
of SIGIR.D.
Chiang, Y. Marton, and P. Resnik.
2008.
Online large-margin training of syntactic and structural translationfeatures.
In Proc.
of EMNLP.J.
Clarke and M. Lapata.
2008.
Global Inference for Sen-tence Compression: An Integer Linear ProgrammingApproach.
Journal of Artificial Intelligence Research,31:399?429.K.
Crammer and Y.
Singer.
2003.
Ultraconservative on-line algorithms for multiclass problems.
Journal ofMachine Learning Research, 3:951?991.H.C.
Daume?
III.
2006.
Practical structured learningtechniques for natural language processing.
Ph.D.thesis, University of Southern California.D.
Gillick and B. Favre.
2009.
A scalable global modelfor summarization.
In Proc.
of ACL Workshop on In-teger Linear Programming for Natural Language Pro-cessing.D.
Gillick and Y. Liu.
2010.
Non-Expert Evaluation ofSummarization Systems is Risky.
In Proc.
of NAACLWorkshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk.K.
Knight and D. Marcu.
2001.
Statistics-basedsummarization-step one: Sentence compression.
InProc.
of AAAI.L.
Li, K. Zhou, G.R.
Xue, H. Zha, and Y. Yu.
2009.Enhancing diversity, coverage and balance for summa-rization through structure learning.
In Proc.
of the 18thInternational Conference on World Wide Web.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In Proc.
of the ACL.489C.Y.
Lin.
2003.
Improving summarization performanceby sentence compression: a pilot study.
In Proc.
ofACL Workshop on Information Retrieval with AsianLanguages.C.Y.
Lin.
2004.
Rouge: A package for automatic evalua-tion of summaries.
In Proc.
of ACL Workshop on TextSummarization Branches Out.A.F.T.
Martins and N.A.
Smith.
2009.
Summarizationwith a joint model for sentence extraction and com-pression.
In Proc.
of NAACL Workshop on Integer Lin-ear Programming for Natural Language Processing.R.
McDonald.
2006.
Discriminative sentence compres-sion with soft syntactic constraints.
In Proc.
of EACL.A.
Nenkova and R. Passonneau.
2004.
Evaluating con-tent selection in summarization: The pyramid method.In Proc.
of NAACL.A.
Nenkova and L. Vanderwende.
2005.
The impact offrequency on summarization.
Technical report, MSR-TR-2005-101.
Redmond, Washington: Microsoft Re-search.S.
Petrov and D. Klein.
2007.
Learning and inference forhierarchically split PCFGs.
In AAAI.J.C.
Platt.
1999.
Fast training of support vector machinesusing sequential minimal optimization.
In Advances inKernel Methods.
MIT press.F.
Schilder and R. Kondadadi.
2008.
Fastsum: Fast andaccurate query-based multi-document summarization.In Proc.
of ACL.D.
Shen, J.T.
Sun, H. Li, Q. Yang, and Z. Chen.
2007.Document summarization using conditional randomfields.
In Proc.
of IJCAI.B.
Taskar, C. Guestrin, and D. Koller.
2003.
Max-marginMarkov networks.
In Proc.
of NIPS.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proc.
of EMNLP.S.
Teufel and M. Moens.
1997.
Sentence extraction asa classification task.
In Proc.
of ACL Workshop onIntelligent and Scalable Text Summarization.I.
Tsochantaridis, T. Hofmann, T. Joachims, and Y. Altun.2004.
Support vector machine learning for interdepen-dent and structured output spaces.
In Proc.
of ICML.V.N.
Vapnik.
1998.
Statistical learning theory.
JohnWiley and Sons, New York.K.
Woodsend and M. Lapata.
2010.
Automatic genera-tion of story highlights.
In Proc.
of ACL.W.
Yih, J. Goodman, L. Vanderwende, and H. Suzuki.2007.
Multi-document summarization by maximizinginformative content-words.
In Proc.
of IJCAI.D.M.
Zajic, B.J.
Dorr, R. Schwartz, and J. Lin.
2006.Sentence compression as a component of a multi-document summarization system.
In Proc.
of the 2006Document Understanding Workshop.490
