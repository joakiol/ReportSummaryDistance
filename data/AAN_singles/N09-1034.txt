Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 299?307,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsUnsupervised Constraint Driven Learning For Transliteration DiscoveryMing-Wei Chang Dan Goldwasser Dan Roth Yuancheng TuUniversity of Illinois at Urbana ChampaignUrbana, IL 61801{mchang21,goldwas1,danr,ytu}@uiuc.eduAbstractThis paper introduces a novel unsupervisedconstraint-driven learning algorithm for iden-tifying named-entity (NE) transliterations inbilingual corpora.
The proposed method doesnot require any annotated data or aligned cor-pora.
Instead, it is bootstrapped using a simpleresource ?
a romanization table.
We show thatthis resource, when used in conjunction withconstraints, can efficiently identify translitera-tion pairs.
We evaluate the proposed methodon transliterating English NEs to three differ-ent languages - Chinese, Russian and Hebrew.Our experiments show that constraint drivenlearning can significantly outperform existingunsupervised models and achieve competitiveresults to existing supervised models.1 IntroductionNamed entity (NE) transliteration is the process oftranscribing a NE from a source language to sometarget language while preserving its pronunciation inthe original language.
Automatic NE transliterationis an important component in many cross-languageapplications, such as Cross-Lingual Information Re-trieval (CLIR) and Machine Translation(MT) (Her-mjakob et al, 2008; Klementiev and Roth, 2006a;Meng et al, 2001; Knight and Graehl, 1998).It might initially seem that transliteration is aneasy task, requiring only finding a phonetic mappingbetween character sets.
However simply matchingevery source language character to its target lan-guage counterpart is not likely to work well as inpractice this mapping depends on the context thecharacters appear in and on transliteration conven-tions which may change across domains.
As a result,current approaches employ machine learning meth-ods which, given enough labeled training data learnhow to determine whether a pair of words consti-tute a transliteration pair.
These methods typicallyrequire training data and language-specific expertisewhich may not exist for many languages.
In this pa-per we try to overcome these difficulties and showthat when the problem is modeled correctly, a sim-ple character level mapping is a sufficient resource.In our experiments, English was used as thesource language, allowing us to use romanization ta-bles, a resource commonly-available for many lan-guages1.
These tables contain an incomplete map-ping between character sets, mapping every charac-ter to its most common counterpart.Our transliteration model takes a discriminativeapproach.
Given a word pair, the model determinesif one word is a transliteration of the other.
Thefeatures used by this model are character n-grammatches across the two strings.
For example, Fig-ure 1 describes the decomposition of a word pair intounigram features as a bipartite graph in which eachedge represents an active feature.We enhance the initial model with constraints, byframing the feature extraction process as a struc-tured prediction problem - given a word pair, the setof possible active features is defined as a set of latentbinary variables.
The contextual dependency be-1The romanization tables available at the Library ofCongress website (http://www.loc.gov/catdir/cpso/roman.html)cover more than 150 languages written in various non-Romanscripts299Figure 1: Top: The space of all possible features that can begenerated given the word pair.
Bottom: A pruned features rep-resentation generated by the inference process.tween features is encoded as a set of constraints overthese variables.
Features are extracted by findingan assignment that maximizes the similarity scorebetween the two strings and conforms to the con-straints.
The model is bootstrapped using a roman-ization table and uses a discriminatively self-trainedclassifier as a way to improve over several trainingiterations.
Furthermore, when specific knowledgeabout the source and target languages exists, it canbe directly injected into the model as constraints.We tested our approach on three very differ-ent languages - Russian, a Slavic language, He-brew a Semitic language, and Chinese, a Sino-Tibetan language.
In all languages, using this sim-ple resource in conjunction with constraints pro-vided us with a robust transliteration system whichsignificantly outperforms existing unsupervised ap-proaches and achieves comparable performance tosupervised methods.The rest of the paper is organized as follows.Sec.
2 briefly examines more related work.
Sec.
3explains our model and Sec.
4 provide a linguisticintuition for it.
Sec.
5 describes our experiments andevaluates our results followed by sec.
6 which con-cludes our paper.2 Related WorksTransliteration methods typically fall into two cate-gories: generative approaches (Li et al, 2004; Junget al, 2000; Knight and Graehl, 1998) that try toproduce the target transliteration given a source lan-guage NE, and discriminative approaches (Gold-wasser and Roth, 2008b; Bergsma and Kondrak,2007; Sproat et al, 2006; Klementiev and Roth,2006a), that try to identify the correct translitera-tion for a word in the source language given severalcandidates in the target language.
Generative meth-ods encounter the Out-Of-Vocabulary (OOV) prob-lem and require substantial amounts of training dataand knowledge of the source and target languages.Discriminative approaches, when used to for dis-covering NE in a bilingual corpora avoid the OOVproblem by choosing the transliteration candidatesfrom the corpora.
These methods typically makevery little assumptions about the source and targetlanguages and require considerably less data to con-verge.
Training the transliteration model is typi-cally done under supervised settings (Bergsma andKondrak, 2007; Goldwasser and Roth, 2008b), orweakly supervised settings with additional tempo-ral information (Sproat et al, 2006; Klementiev andRoth, 2006a).
Our work differs from these worksin that it is completely unsupervised and makes noassumptions about the training data.Incorporating knowledge encoded as constraintsinto learning problems has attracted a lot of atten-tion in the NLP community recently.
This has beenshown both in supervised settings (Roth and Yih,2004; Riedel and Clarke, 2006) and unsupervisedsettings (Haghighi and Klein, 2006; Chang et al,2007) in which constraints are used to bootstrap themodel.
(Chang et al, 2007) describes an unsuper-vised training of a Constrained Conditional Model(CCM), a general framework for combining statisti-cal models with declarative constraints.
We extendthis work to include constraints over possible assign-ments to latent variables which, in turn, define theunderlying representation for the learning problem.In the transliteration community there are sev-eral works (Ristad and Yianilos, 1998; Bergsma andKondrak, 2007; Goldwasser and Roth, 2008b) thatshow how the feature representation of a word paircan be restricted to facilitate learning a string sim-ilarity model.
We follow the approach discussedin (Goldwasser and Roth, 2008b), which considersthe feature representation as a structured predictionproblem and finds the set of optimal assignments (orfeature activations), under a set of legitimacy con-straints.
This approach stresses the importance ofinteraction between learning and inference, as themodel iteratively uses inference to improve the sam-ple representation for the learning problem and usesthe learned model to improve the accuracy of the in-300ference process.
We adapt this approach to unsu-pervised settings, where iterating over the data im-proves the model in both of these dimensions.3 Unsupervised Constraint DrivenLearningIn this section we present our Unsupervised Con-straint Driven Learning (UCDL) model for discov-ering transliteration pairs.
Our task is in essence aranking task.
Given a NE in the source language anda list of candidate transliterations in the target lan-guage, the model is supposed to rank the candidatesand output the one with the highest score.
The modelis bootstrapped using two linguistic resources: a ro-manization table and a set of general and linguisticconstraints.
We use several iterations of self trainingto learn the model.
The details of the procedure areexplained in Algorithm 1.In our model features are character pairs (cs, ct),where cs ?
Cs is a source word character andct ?
Ct is a target word character.
The featurerepresentation of a word pair vs, vt is denoted byF (vs, vt).
Each feature (cs, ct) is assigned a weightW (cs, ct) ?
R. In step 1 of the algorithm we initial-ize the weights vector using the romanization table.Given a pair (vs, vt), a feature extraction processis used to determine the feature based representationof the pair.
Once features are extracted to representa pair, the sum of the weights of the extracted fea-tures is the score assigned to the target translitera-tion candidate.
Unlike traditional feature extractionapproaches, our feature representation function doesnot produce a fixed feature representation.
In step2.1, we formalize the feature extraction process as aconstrained optimization problem that captures theinterdependencies between the features used to rep-resent the sample.
That is, obtaining F (vs, vt) re-quires solving an optimization problem.
The techni-cal details are described in Sec.
3.1.
The constraintswe use are described in Sec.
3.2.In step 2.2 the different candidates for everysource NE are ranked according to the similarityscore associated with their chosen representation.This ranking is used to ?label?
examples for a dis-criminative learning process that learns increasinglybetter weights, and thus improve the representationof the pair: each source NE paired with its topranked transliteration is labeled as a positive exam-ples (step 2.3) and the rest of the samples are consid-ered as negative samples.
In order to focus the learn-ing process, we removed from the training set alnegative examples ruled-out by the constraints (step2.4).
As the learning process progresses, the initialweights are replaced by weights which are discrimi-natively learned (step 2.5).
This process is repeatedseveral times until the model converges, and repeatsthe same ranking over several iterations.Input: Romanization table T : Cs ?
Ct, ConstraintsC, Source NEs: Vs, Target words: Vt1.
Initialize ModelLet W : Cs ?
Ct ?
R be a weight vector.Initialize W using T by the following procedure?
(cs, ct), (cs, ct) ?
T ?
W(cs, ct) = 0,?
(cs, ct),?
((cs, ct) ?
T ) ?W(cs, ct) = ?1,?cs,W(cs, ) = ?1, ?ct,W( , ct) = ?1.2.
Constraints driven unsupervised trainingwhile not converged do1.
?vs ?
Vs, vt ?
Vt, use C and Wto generate a representation F (vs, vt)2.
?vs ?
Vs, find the top ranking transliterationpair (vs, v?t ) by solvingv?t = argmaxvt score(F (vs, vt)).3.
D = {(+, F (vs, v?t )) | ?vs ?
Vs}.4.
?vs ?
Vs, vt ?
Vt, if vt 6= v?t andscore(F (vs, vt)) 6= ?
?, thenD = D ?
{(?, F (vs, vt))}.5.
W ?
train(D)endAlgorithm 1: UCDL Transliteration Framework.In the rest of this section we explain this processin detail.
We define the feature extraction inferenceprocess in Sec.
3.1, the constraints used in Sec.
3.2and the inference algorithm in Sec.
3.3.
The linguis-tic intuition for our model is described in Sec.
4.3.1 Finding Feature Representation asConstrained OptimizationWe use the formulation of Constrainted ConditionalModels (CCMs) (Roth and Yih, 2004; Roth and Yih,2007; Chang et al, 2008).
Previous work on CCMmodels dependencies between different decisions instructured prediction problems.
Transliteration dis-covery is a binary classification problem, however,301the underlying representation of each sample can bemodeled as a CCM, defined as a set of latent vari-ables corresponding to the set of all possible featuresfor a given sample.
The dependencies between thefeatures are captured using constraints.Given a word pair, the set of all possible featuresconsists of all character mappings from the sourceword to the target word.
Since in many cases thesize of the words differ we augment each of thewords with a blank character (denoted as ?
?).
Wemodel character omission by mapping the characterto the blank character.
This process is formally de-fined as an operator mapping a transliteration can-didate pair to a set of binary variables, denoted asAll-Features (AF ).AF = {(cs, ct)|cs ?
vs ?
{ }, ct ?
vt ?
{ }}This representation is depicted at the top of Figure 1.The initial sample representation (AF ) gener-ates features by coupling substrings from the twoterms without considering the dependencies be-tween the possible combinations.
This representa-tion is clearly noisy and in order to improve it weselect a subset F ?
AF of the possible features.The selection process is formulated as a linear op-timization problem over binary variables encodingfeature activations in AF .
Variables assigned 1 areselected to be in F , and those assigned 0 are not.The objective function maximized is a linear func-tion over the variables in AF , each with its weight asa coefficient, as in the left part of Equation 1 below.We seek to maximize this linear sum subject to a setof constraints.
These represent the dependencies be-tween selections and prior knowledge about possiblelegitimate character mappings and correspond to theright side of Equation 1.
In our settings only hardconstraints are used and therefore the penalty (?)
forviolating any of the constraints is set to ?.
The spe-cific constraints used are discussed in Sec.
3.2.
Thescore of the mapping F (vs, vt) can be written as fol-lows:1|vt|(W ?
F (vs, vt)?
?ci?C?ci(F (vs, vt)) (1)We normalize this score by dividing it by the size ofthe target word, since the size of candidates varies,normalization improved the ranking of candidates.The result of the optimization process is a set F ofactive features, defined in Equation 2.
The result ofthis process is described at the bottom of Figure 1.F ?
(vs, vt) = argmaxF?AF (vs,vt)score(F ).
(2)The ranking process done by our model can now benaturally defined - given a source word vs, and aset of candidates target words v0t , .
.
.
, vnt , find thecandidate whose optimal representation maximizesEquation 1.
This process is defined in Equation 3.v?t = argmaxvitscore(F (vs, vit)).
(3)3.2 Incorporating Mapping ConstraintsWe consider two types of constraints: language spe-cific and general constraints that apply to all lan-guages.
Language specific constraints typically im-pose a local restriction such as individually forcingsome of the possible character mapping decisions.The linguistic intuition behind these constraints isdiscussed in Section 4.
General constraints encodeglobal restrictions, capturing the dependencies be-tween different mapping decisions.General constraints: To facilitate readability wedenote the feature mapping the i-th source wordcharacter to the j-th target word character as aBoolean variable aij that is 1 if that feature is activeand 0 otherwise.?
Coverage - Every character must be mappedonly to a single character or to the blank char-acter.
We can formulate this as: ?j aij = 1and ?i aij = 1.?
No Crossing - Every character mapping, exceptmapping to blank character, should preserve theorder of appearance in the source and targetwords, or formally - ?i, j s.t.
aij = 1 ?
?l <i, ?k > j, alk = 0.
Another constraint is ?i, js.t.
aij = 1 ?
?l > i, ?k < j, alk = 0.Language specific constraints?
Restricted Mapping: These constraints restrictthe possible local mappings between sourceand target language characters.
We maintain aset of possible mappings {cs ?
?cs}, where?cs ?
Ct and {ct ?
?ct}, where ?ct ?
Cs.Any feature (cs, ct) such that cs /?
?ct orct /?
?cs is penalized in our model.302?
Length restriction: An additional constraintrestricts the size difference between the twowords.
We formulate this as follows: ?vs ?Vs,?vt ?
Vt, if ?|vt| > |vs| and ?|vs| > |vt|,score(F (vs, vt)) = ??.
Although ?
cantake different values for different languages, wesimply set ?
to 2 in this paper.In addition to biasing the model to choose theright candidate, the constraints also provide a com-putational advantage: a given a word pair is elimi-nated from consideration when the length restrictionis not satisfied or there is no way to satisfy the re-stricted mapping constraints.3.3 InferenceThe optimization problem defined in Equation 2 isan integer linear program (ILP).
However, giventhe structure of the problem it is possible to de-velop an efficient dynamic programming algorithmfor it, based on the algorithm for finding the mini-mal edit distance of two strings.
The complexity offinding the optimal set of features is only quadraticin the size of the input pair, a clear improvementover the ILP exponential time algorithm.
The al-gorithm minimizes the weighted edit distance be-tween the strings, and produces a character align-ment that satisfies the general constraints (Sec.
3.2).Our modifications are only concerned with incorpo-rating the language-specific constraints into the al-gorithm.
This can be done simply by assigning anegative infinity score to any alignment decision notsatisfying these constraints.4 Bootstrapping with LinguisticInformationOur model is bootstrapped using two resources - aromanization table and mapping constraints.
Bothresources capture the same information - charactermapping between languages.
The distinction be-tween the two represents the difference in the con-fidence we have in these resources - the romaniza-tion table is a noisy mapping covering the characterset and is therefore better suited as a feature.
Con-straints, represented by pervasive, correct charactermapping, indicate the sound mapping tendency be-tween source and target languages.
For example,certain n-gram phonemic mappings, such as r ?
lfrom English to Chinese, are language specific andcan be captured by language specific sound changepatterns.Phonemes ConstraintsVowel i ?
y; u ?
w; a ?
aNasal m ?
m; m,n ?
mApproximantr ?
l; l, r ?
ll ?
l; w ?
h,w, fh, o, u, v ?
w; y ?
yFricative v ?
w, b, fs ?
s, x, z; s, c ?
sPlosivep ?
b, p; p ?
pb ?
b; t ?
tt, d ?
d; q ?
kTable 1: All language specific constraints used in our Englishto Chinese transliteration (see Sec.
3.2 for more details).
Con-straints in boldface apply to all positions, the rest apply only tocharacters appearing in initial position.These patterns have been used by other systemsas features or pseudofeatures (Yoon et al, 2007).However, in our system these language specific rule-of-thumbs are systematically used as constraints toexclude impossible alignments and therefore gener-ate better features for learning.
We listed in Table 1all 20 language specific constraints we used for Chi-nese.
There is a total of 24 constraints for Hebrewand 17 for Russian.The constraints in Table 1 indicate a systematicsound mapping between English and Chinese un-igram character mappings.
Arranged by mannersof articulation each row of the table indicates thesound change tendency among vowels, nasals, ap-proximants (retroflex and glides), fricatives and plo-sives.
For example, voiceless plosive sounds such asp, t in English, tend to map to both voiced (such as b,d) and voiceless sounds in Chinese.
However, if thesound is voiceless in Chinese, its backtrack Englishsound must be voiceless.
This voice-voiceless soundchange tendency is captured by our constraints suchas p ?
b, p and p ?
p; t ?
t.5 Experiments and AnalysisIn this section, we demonstrate the effectivenessof constraint driven learning empirically.
We startby describing the datasets and experimental settingsand then proceed to describe the results.
We eval-uated our method on three very different target lan-3030.30.40.50.60.70.80.910  2  4  6  8  10  12  14  16  18  20ACCNumber of Rounds[KR 06] + temporal information[KR 06] All Cons.
+ unsupervsied learningFigure 2: Comparison between our models and weakly su-pervised learning methods (Klementiev and Roth, 2006b).Note that one of the models proposed in (Klementiev and Roth,2006b) takes advantage of the temporal information.
Our bestmodel, the unsupervised learning with all constraints, outper-forms both models in (Klementiev and Roth, 2006b), eventhough we do not use any temporal information.guages: Russian, Chinese, and Hebrew, and com-pared our results to previously published results.5.1 Experimental SettingsIn our experiments the system is evaluated on itsability to correctly identify the gold transliterationfor each source word.
We evaluated the system?sperformance using two measures adopted in manytransliteration works.
The first one is Mean Recip-rocal Rank (MRR), used in (Tao et al, 2006; Sproatet al, 2006), which is the average of the multiplica-tive inverse of the rank of the correct answer.
For-mally, Let n be the number of source NEs.
Let Gol-dRank(i) be the rank the algorithm assigns to thecorrect transliteration.
Then, MRR is defined by:MRR = 1nn?i=11goldRank(i) .Another measure is Accuracy (ACC) used in (Kle-mentiev and Roth, 2006a; Goldwasser and Roth,2008a), which is the percentage of the top rank can-didates being the gold transliteration.
In our im-plementation we used the support vector machine(SVM) learning algorithm with linear kernel as ourunderlying learning algorithm (mentioned in part2.5 of Algorithm 1) .
We used the package LIB-LINEAR (Hsieh et al, 2008) in our experiments.Through all of our experiments, we used the 2-norm0.40.50.60.70.80.911.10  2  4  6  8  10  12  14  16  18  20MRRNumber of Rounds[GR 08] 250 labeled ex.
with cons[GR 08] 250 labeled ex.
w/o consGeneral cons + unsupervised learningAll cons.
+ unsupervised learningFigure 3: Comparison between our works and supervisedmodels in (Goldwasser and Roth, 2008b).
We show the learn-ing curves for Hebrew under two different settings: unsuper-vised learning with general and all constraints.
The results oftwo supervised models (Goldwasser and Roth, 2008b) are alsoincluded here.
Note that our unsupervised model with all con-straints is competitive to the supervised model with 250 labeledexamples.
See the text for more comparisons and details.hinge loss as our loss function and fixed the regular-ization parameter C to be 0.5.5.2 DatasetsWe experimented using three different target lan-guages Russian, Chinese, and Hebrew.
We used En-glish as the source language in all these experiments.The Russian data set2, originally introduced in(Klementiev and Roth, 2006b), is comprised of tem-porally aligned news articles.
The dataset contains727 single word English NEs with a correspond-ing set of 50,648 potential Russian candidate wordswhich include not only name entities, but also otherwords appearing in the news articles.The Chinese dataset is taken directly from anEnglish-Chinese transliteration dictionary, derivedfrom LDC Gigaword corpus3.
The entire dictionaryconsists of 74,396 pairs of English-Chinese NEs,where Chinese NEs are written in Pinyin, a roman-ized spelling system of Chinese.
In (Tao et al, 2006)a dataset which contains about 600 English NEs and700 Chinese candidates is used.
Since the datasetis not publicly available, we created a dataset in asimilar way.
We randomly selected approximately600 NE pairs and then added about 100 candidateswhich do not correspond to any of the English NE2The corpus is available http://L2R.cs.uiuc.edu/?cogcomp.3http://www.ldc.upenn.edu304Language UCDL Prev.
worksRus.
(ACC) 73 63 (41) (KR?06)Heb.
(MRR) 0.899 0.894 (GR?08)Table 2: Comparison to previously published results.
UCDLis our method, KR?06 is described in (Klementiev and Roth,2006b) and GR?08 in (Goldwasser and Roth, 2008b).
Note thatour results for Hebrew are comparable with a supervised sys-tem.previously selected.The Hebrew dataset, originally introduced in(Goldwasser and Roth, 2008a), consists of 300English-Hebrew pairs extracted from Wikipedia.5.3 ResultsWe begin by comparing our model to previouslypublished models tested over the same data, in twodifferent languages, Russian and Hebrew.
For Rus-sian, we compare to the model presented in (Kle-mentiev and Roth, 2006b), a weakly supervised al-gorithm that uses both phonetic information andtemporal information.
The model is bootstrappedusing a set of 20 labeled examples.
In their settingthe candidates are ranked by combining two scores,one obtained using the transliteration model and asecond by comparing the relative occurrence fre-quency of terms over time in both languages.
Dueto computational tractability reasons we slightlychanged Algorithm 1 to use only a small subset ofthe possible negative examples.For Hebrew, we compare to the model presentedin (Goldwasser and Roth, 2008b), a supervisedmodel trained using 250 labeled examples.
Thismodel uses a bigram model to represent the translit-eration samples (i.e., features are generated by pair-ing character unigrams and bigrams).
The modelalso uses constraints to restrict the feature extrac-tion process, which are equivalent to the coverageconstraint we described in Sec.
3.2.The results of these experiments are reported us-ing the evaluation measures used in the original pa-pers and are summarized in Table 2.
The resultsshow a significant improvement over the Russiandata set and comparable performance to the super-vised method used for Hebrew.Figure 2 describes the learning curve of ourmethod over the Russian dataset.
We compared ouralgorithm to two models described in (Klementievand Roth, 2006b) - one uses only phonetic simi-larity and the second also considers temporal co-occurrence similarity when ranking the translitera-tion candidates.
Both models converge after 50 it-erations.
When comparing our model to their mod-els, we found that even though our model ignoresthe temporal information it achieves better resultsand converges after fewer iterations.
Their resultsreport a significant improvement when using tempo-ral information - improving an ACC score of 41%without temporal information to 63% when usingit.
Since the temporal information is orthogonal tothe transliteration model, our model should similarlybenefit from incorporating the temporal information.Figure 3 compares the learning curve of ourmethod to an existing supervised method over theHebrew data and shows we get comparable results.Unfortunately, we could not find a published Chi-nese dataset.
However, our system achieved similarresults to other systems, over a different dataset withsimilar number of training examples.
For example,(Sproat et al, 2006) presents a supervised systemthat achieves a MRR score of 0.89, when evaluatedover a dataset consisting of 400 English NE and 627Chinese words.
Our results for a different dataset ofsimilar size are reported in Table 3.5.4 AnalysisThe resources used in our framework consist of- a romanization table, general and language spe-cific transliteration constraints.
To reveal the impactof each component we experimented with differentcombination of the components, resulting in threedifferent testing configurations.Romanization Table: We initialized the weightvector using a romanization table and did not use anyconstraints.
To generate features we use a modifiedversion of our AF operator (see Sec.
3), which gen-erates features by coupling characters in close posi-tions in the source and target words.
This configura-tion is equivalent to the model used in (Klementievand Roth, 2006b).+General Constraints: This configuration uses theromanization table for initializing the weight vectorand general transliteration constraints (see Sec.
3.2)for feature extraction.+All Constraints: This configuration uses lan-guage specific constraints in addition to the gen-305Settings Chinese Russian HebrewRomanization table 0.019 (0.5) 0.034 (1.0) 0.046 (1.7)Romanization table +learning 0.020 (0.3) 0.048 (1.3) 0.028 (0.7)+Gen Constraints 0.746 (67.1) 0.809 (74.3) 0.533 (45.0)+Gen Constraints +learning 0.867 (82.2) 0.906 (86.7) 0.834 (76.0)+All Constraints 0.801 (73.4) 0.849 (79.3) 0.743 (66.0)+All Constraints +learning 0.889 (84.7) 0.931 (90.0) 0.899 (85.0)Table 3: Results of an ablation study of unsupervised method for three target languages.
Results for ACC are inside parentheses,and for MRR outside.
When the learning algorithm is used, the results after 20 rounds of constraint driven learning are reported.Note that using linguistic constraints has a significant impact in the English-Hebrew experiments.
Our results show that a smallamount of constraints can go a long way, and better constraints lead to better learning performance.eral transliteration constraints to generate the featurerepresentation.
(see Sec.
4).+Learning: Indicates that after initializing theweight vector, we update the weight using Algo-rithm 1.
In all of the experiments, we report theresults after 20 training iterations.The results are summarized in Table 3.
Due to thesize of the Russian dataset, we used a subset consist-ing of 300 English NEs and their matching Russiantransliterations for the analysis presented here.
Af-ter observing the results, we discovered the follow-ing regularities for all three languages.
Using theromanization table directly without constraints re-sults in very poor performance, even after learning.This can be used as an indication of the difficulty ofthe transliteration problem and the difficulties ear-lier works have had when using only romanizationtables, however, when used in conjunction with con-straints results improve dramatically.
For example,in the English-Chinese data set, we improve MRRfrom 0.02 to 0.746 and for the English-Russian dataset we improve 0.03 to 0.8.
Interestingly, the resultsfor the English-Hebrew data set are lower than forother languages - we achieve 0.53 MRR in this set-ting.
We attribute the difference to the quality ofthe mapping in the romanization table for that lan-guage.
Indeed, the weights learned after 20 train-ing iterations improve the results to 0.83.
This im-provement is consistent across all languages, afterlearning we are able to achieve a MRR score of 0.87for the English-Chinese data set and 0.91 for theEnglish-Russian data set.
These results show thatromanization table contains enough information tobootstrap the model when used in conjunction withconstraints.
We are able to achieve results compa-rable to supervised methods that use a similar set ofconstraints and labeled examples.Bootstrapping the weight vector using languagespecific constraints can further improve the results.They provide several advantages: a better startingpoint, an improved learning rate and a better finalmodel.
This is clear in all three languages, for exam-ple results for the Russian and Chinese bootstrappedmodels improve by 5%, and by over 20% for He-brew.
After training the difference is smaller- only3% for the first two and 6% for Hebrew.
Figure 3 de-scribes the learning curve for models with and with-out language specific constraints for the English-Hebrew data set, it can be observed that using theseconstraints the model converges faster and achievesbetter results.6 ConclusionIn this paper we develop a constraints driven ap-proach to named entity transliteration.
In doing itwe show that romanization tables are a very usefulresource for transliteration discovery if proper con-straints are included.
Our framework does not needlabeled data and does not assume that bilingual cor-pus are temporally aligned.
Even without using anylabeled data, our model is competitive to existingsupervised models and outperforms existing weaklysupervised models.7 AcknowledgmentsWe wish to thank the reviewers for their insightfulcomments.
This work is partly supported by NSFgrant SoD-HCER-0613885 and DARPA funding un-der the Bootstrap Learning Program.306ReferencesS.
Bergsma and G. Kondrak.
2007.
Alignment-baseddiscriminative string similarity.
In Proc.
of the AnnualMeeting of the Association of Computational Linguis-tics (ACL), pages 656?663, Prague, Czech Republic,June.
Association for Computational Linguistics.M.
Chang, L. Ratinov, and D. Roth.
2007.
Guiding semi-supervision with constraint-driven learning.
In Proc.of the Annual Meeting of the Association of Compu-tational Linguistics (ACL), pages 280?287, Prague,Czech Republic, Jun.
Association for ComputationalLinguistics.M.
Chang, L. Ratinov, N. Rizzolo, and D. Roth.
2008.Learning and inference with constraints.
In Proc.of the National Conference on Artificial Intelligence(AAAI), July.D.
Goldwasser and D. Roth.
2008a.
Active sample se-lection for named entity transliteration.
In Proc.
of theAnnual Meeting of the Association of ComputationalLinguistics (ACL), June.D.
Goldwasser and D. Roth.
2008b.
Transliteration asconstrained optimization.
In Proc.
of the Conferenceon Empirical Methods for Natural Language Process-ing (EMNLP), pages 353?362, Oct.A.
Haghighi and D. Klein.
2006.
Prototype-driven learn-ing for sequence models.
In Proc.
of the Annual Meet-ing of the North American Association of Computa-tional Linguistics (NAACL).U.
Hermjakob, K. Knight, and H. Daume?
III.
2008.Name translation in statistical machine translation -learning when to transliterate.
In Proc.
of the AnnualMeeting of the Association of Computational Linguis-tics (ACL), pages 389?397, Columbus, Ohio, June.Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, S. SathiyaKeerthi, and S. Sundararajan.
2008.
A dual coordinatedescent method for large-scale linear svm.
In ICML?08: Proceedings of the 25th international conferenceon Machine learning, pages 408?415, New York, NY,USA.
ACM.S.
Jung, S. Hong, and E. Paek.
2000.
An english tokorean transliteration model of extended markov win-dow.
In Proc.
the International Conference on Com-putational Linguistics (COLING), pages 383?389.A.
Klementiev and D. Roth.
2006a.
Named entitytransliteration and discovery from multilingual com-parable corpora.
In Proc.
of the Annual Meeting of theNorth American Association of Computational Lin-guistics (NAACL), pages 82?88, June.A.
Klementiev and D. Roth.
2006b.
Weakly supervisednamed entity transliteration and discovery from mul-tilingual comparable corpora.
In Proc.
of the AnnualMeeting of the Association of Computational Linguis-tics (ACL), pages USS,TL,ADAPT, July.K.
Knight and J. Graehl.
1998.
Machine transliteration.Computational Linguistics, pages 599?612.H.
Li, M. Zhang, and J. Su.
2004.
A joint source-channelmodel for machine transliteration.
In Proc.
of the An-nual Meeting of the Association of Computational Lin-guistics (ACL), pages 159?166, Barcelona, Spain, July.H.
Meng, W. Lo, B. Chen, and K. Tang.
2001.Generating phonetic cognates to handle named en-tities in english-chinese cross-langauge spoken doc-ument retreival.
In Proceedings of the AutomaticSpeech Recognition and Understanding Workshop,pages 389?397.S.
Riedel and J. Clarke.
2006.
Incremental integer linearprogramming for non-projective dependency parsing.In Proc.
of the Conference on Empirical Methods forNatural Language Processing (EMNLP), pages 129?137, Sydney, Australia.E.
S. Ristad and P. N. Yianilos.
1998.
Learning stringedit distance.
IEEE Transactions on Pattern Recogni-tion and Machine Intelligence, 20(5):522?532, May.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.pages 1?8.
Association for Computational Linguistics.D.
Roth and W. Yih.
2007.
Global inference for en-tity and relation identification via a linear program-ming formulation.
In Lise Getoor and Ben Taskar, ed-itors, Introduction to Statistical Relational Learning.MIT Press.R.
Sproat, T. Tao, and C. Zhai.
2006.
Named entitytransliteration with comparable corpora.
In Proc.
ofthe Annual Meeting of the Association of Computa-tional Linguistics (ACL), pages 73?80, Sydney, Aus-tralia, July.T.
Tao, S. Yoon, A. Fister, R. Sproat, and C. Zhai.
2006.Unsupervised named entitly transliteration using tem-poral and phonetic correlation.
In Proc.
of the Con-ference on Empirical Methods for Natural LanguageProcessing (EMNLP), pages 250?257.S.
Yoon, K. Kim, and R. Sproat.
2007.
Multilingualtransliteration using feature based phonetic method.In Proc.
of the Annual Meeting of the Associationof Computational Linguistics (ACL), pages 112?119,Prague, Czech Republic, June.307
