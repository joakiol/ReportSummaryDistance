Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 91?98,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsExtraction of Disease-Treatment Semantic Relations from BiomedicalSentencesOana Frunza and Diana InkpenSchool of Information Technology and EngineeringUniversity of Ottawa Ottawa, ON, Canada, K1N 6N5{ofrunza,diana}@site.uottawa.caAbstractThis paper describes our study on identi-fying semantic relations that exist betweendiseases and treatments in biomedical sen-tences.
We focus on three semantic rela-tions: Cure, Prevent, and Side Effect.
Thecontributions of this paper consists in thefact that better results are obtained com-pared to previous studies and the fact thatour research settings allow the integrationof biomedical and medical knowledge.We obtain 98.55% F-measure for the Curerelation, 100% F-measure for the Preventrelation, and 88.89% F-measure for theSide Effect relation.1 IntroductionResearch in the fields of life-science and bio-medical domain has been the focus of the NaturalLanguage Processing (NLP) and Machine Learn-ing (ML) community for some time now.
Thistrend goes very much inline with the directionthe medical healthcare system is moving to: theelectronic world.
The research focus of scientiststhat work in the filed of computational linguisticsand life science domains also followed the trendsof the medicine that is practiced today, an Evi-dence Based Medicine (EBM).
This new way ofmedical practice is not only based on the experi-ence a healthcare provider acquires as timepasses by, but on the latest discoveries as well.We live in an information explosion era where itis almost impossible to find that piece of relevantinformation that we need.
With easy and cheepaccess to disk-space we sometimes even findchallenging to find our stored local documents.
Itshould come to no surprise that the global trendin domains like biomedicine and not only is torely on technology to identify and upraise infor-mation.
The amount of publications and researchthat is indexed in the life-science domain growsalmost exponentially (Hunter and Cohen (2006)making the task of finding relevant information,a hard and challenging task for NLP research.The search for information in the life-sciencedomain is not only the focus of researchers thatwork in these fields, but the focus of laypeople aswell.
Studies reveal that people are searching theweb for medical-related articles to be better in-formed about their health.
Ginsberg et al (2009)show how a new outbreak of the influenza viruscan be detected from search engine query data.The aim of this paper is to show which NLPand ML techniques are suitable for the task ofidentifying semantic relations between diseasesand treatments in short biomedical texts.
Thevalue of our work stands in the results we obtainand the new feature representation techniques.2 Related WorkThe most relevant work for our study is the workof Rosario and Hearst (2004).
The authors of thispaper are the ones that created and distributed thedata set used in our research.
The data set is an-notated with disease and treatments entities andwith 8 semantic relations between diseases andtreatments.
The main focus of their work is onentity recognition ?
the task of identifying enti-ties, diseases and treatments in biomedical textsentences.
The authors use Hidden MarkovModels and maximum entropy models to per-form both the task of entity recognition and ofrelation discrimination.
Their representationtechniques are based on words in context, part-of-speech information, phrases, and terms fromMeSH1, a medical lexical knowledge-base.
Com-pared to previous work, our research is focused1http://www.nlm.nih.gov/mesh/meshhome.html91on different representation techniques, differentclassification models, and most importantly inobtaining improved results without using the an-notations of the entities (new data will not havethem).
In previous research, the best results wereobtained when the entities involved in the rela-tions were identified and used as features.The biomedical literature contains a wealth ofwork on semantic relation extraction, mostly fo-cused on more biology-specific tasks: subcellu-lar-location (Craven 1999), gene-disorder asso-ciation (Ray and Craven 2001), and diseases anddrugs relations (Srinivasan and Rindflesch 2002,Ahlers et al, 2007).Text classification techniques combined with aNa?ve Bayes classifier and relational learningalgorithms are methods used by Craven (1999).Hidden Markov Models are used in Craven(2001), but similarly to Rosario and Hearst(2004), the research focus was entity recognition.A context based approach using MeSH termco-occurrences are used by Srinivasan and Rind-flesch (2002) for relationship discrimination be-tween diseases and drugs.A lot of work is focused on building rules usedto extract relation.
Feldman et al (2002) use arule-based system to extract relations that arefocused on genes, proteins, drugs, and diseases.Friedman et al (2001) go deeper into building arule-based system by hand-crafting a semanticgrammar and a set of semantic constraints in or-der to recognize a range of biological and mo-lecular relations.3 Task and Data SetsOur task is focused on identifying disease-treatment relations in sentences.
Three relations:Cure, Prevent, and Side Effect, are the main ob-jective of our work.
We are tackling this task byusing techniques based on NLP and supervisedML techniques.
We decided to focus on thesethree relations because these are the ones that arebetter represented in the original data set and inthe end will allow us to draw more reliable con-clusions.
Also, looking at the meaning of all rela-tions in the original data set, the three that wefocus on are the ones that could be useful forwider research goals and are the ones that reallyentail relations between two entities.
In the su-pervised ML settings the amount of training datais a factor that influences the performance; sup-port for this stands not only in the related workperformed on the same data set, but in the re-search literature as well.
The aim of this paper isto focus on few relations of interest and try toidentify what predictive model and what repre-sentation techniques bring the best results ofidentifying semantic relations in short biomedi-cal texts.
We mostly focused on the value thatthe research can bring, rather than on an incre-mental research.As mentioned in the previous section, the dataset that we use to run our experiments is the oneof Rosario and Hearst (2004).
The entire data setis collected from Medline2 2001 abstracts.
Sen-tences from titles and abstracts are annotatedwith entities and with 8 relations, based only onthe information present in a certain sentence.
Thefirst 100 titles and 40 abstracts from each of the59 Medline 2001 files were used for annotation.Table 1, presents the original data set, as pub-lished in previous research.
The numbers in pa-renthesis represent the training and test set sizes.Relationship Definition and ExampleCure810 (648, 162)TREAT cures DISIntravenous immune globulin forrecurrent spontaneous abortionOnly DIS616 (492, 124)TREAT not mentionedSocial ties and susceptibility tothe common coldOnly TREAT166 (132, 34)DIS not mentionedFlucticasome propionate is safe inrecommended dosesPrevent63 (50, 13)TREAT prevents the DISStatins for prevention of strokeVague36 (28, 8)Very unclear relationshipPhenylbutazone and leukemiaSide Effect29 (24, 5)DIS is a result of a TREATMalignant mesodermal mixedtumor of the uterus followingirradiationNO Cure4 (3, 1)TREAT does not cure DISEvidence for double resistance topermethrin and malathion in headliceTotal relevant: 1724 (1377, 347)Irrelevant1771 (1416, 355)Treat and DIS not presentPatients were followed up for 6monthsTotal: 3495 (2793, 702)Table 1.
Original data set.From this original data set, the sentences that areannotated with Cure, Prevent, Side Effect, OnlyDIS, Only TREAT, and Vague are the ones thatused in our current work.
While our main focusis on the Cure, Prevent, and Side Effect, we alsorun experiments for all relations such that a di-rect comparison with the previous work is done.2http://medline.cos.com/92Table 2 describes the data sets that we createdfrom the original data and used in our experi-ments.
For each of the relations of interest wehave 3 labels attached: Positive, Negative, andNeutral.
The Positive label is given to sentencesthat are annotated with the relation in question inthe original data; the Negative label is given tothe sentences labeled with Only DIS and OnlyTREAT classes in the original data; Neutral labelis given to the sentences annotated with Vagueclass in the original data set.Table 2.
Our data sets3.4 MethodologyThe experimental settings that we follow areadapted to the domain of study (we integrate ad-ditional medical knowledge), yielding for themethods to bring improved performance.The challenges that can be encountered whileworking with NLP and ML techniques are: find-ing the suitable model for prediction ?
since theML field offers a suite of predictive models (al-gorithms), the task of finding the suitable onerelies heavily on empirical studies and knowl-edge expertise; and finding the best data repre-sentation ?
identifying the right and sufficientfeatures to represent the data is a crucial aspect.These challenges are addressed by trying variouspredictive algorithms based on different learningtechniques, and by using various textual repre-sentation techniques that we consider suitable.The task of identifying the three semantic rela-tions is addressed in three ways:Setting 1: build three models, each focusedon one relation that can distinguish sentencesthat contain the relation ?
Positive label, fromother sentences that are neutral ?
Neutral label,and from sentences that do not contain relevantinformation ?
Negative label;3The number of sentences available for download isnot the same as the ones from the original data set,published in Rosario and Hearst (?04).Setting 2: build three models, each focused onone relation that can distinguish sentences thatcontain the relation from sentences that do notcontain any relevant information.
This setting issimilar to a two-class classification task in whichinstances are labeled either with the relation inquestion ?
Positive label, or with non-relevantinformation ?
Negative label;Setting 3: build one model that distinguishes thethree relations ?
a three-way classification taskwhere each sentence is labeled with one of thesemantic relations, using the data with all thePositive labels.The first set of experiments is influenced byprevious research done by Koppel and Schler(2005).
The authors claim that for polarity learn-ing ?neutral?
examples help the learning algo-rithms to better identify the two polarities.
Theirresearch was done on a corpus of posts to chatgroups devoted to popular U.S. television andposts to shopping.com?s product evaluation page.As classification algorithms, a set of 6 repre-sentative models: decision-based models (Deci-sion trees ?
J48), probabilistic models (Na?veBayes and complement Na?ve Bayes (CNB),which is adapted for imbalanced class distribu-tion), adaptive learning (AdaBoost), linear classi-fier (support vector machine (SVM) with poly-nomial kernel), and a classifier, ZeroR, that al-ways predicts the majority class in the trainingdata used as a baseline.
All classifiers are part ofa tool called Weka4.As representation technique, we rely on fea-tures such as the words in the context, the nounand verb-phrases, and the detected biomedicaland medical entities.
In the following subsec-tions, we describe all the representation tech-niques that we use.4.1 Bag-of-words representationThe bag-of-words (BOW) representation iscommonly used for text classification tasks.
It isa representation in which the features are chosenamong the words that are present in the trainingdata.
Selection techniques are used in order toidentify the most suitable words as features.
Af-ter the feature space is identified, each trainingand test instance is mapped into this feature rep-resentation by giving values to each feature for acertain instance.
Two feature value representa-tions are the most commonly used for the BOWrepresentation: binary feature values ?
the value4http://www.cs.waikato.ac.nz/ml/weka/TrainRelation Positive Negative NeutralCure 554 531 25Prevent 42 531 25SideEffect 20 531 25TestRelation Positive Negative NeutralCure 276 266 12Prevent 21 266 12SideEffect 10 266 1293of a feature is 1 if the feature is present in theinstance and 0 otherwise, or frequency featurevalues ?
the feature value is the number of timesit appears in an instance, or 0 if it did not appear.Taking into consideration the fact that an in-stance is a sentence, the textual information isrelatively small.
Therefore a frequency valuerepresentation is chosen.
The difference betweena binary value representation and a frequencyvalue representation is not always significant,because sentences tend to be short.
Nonetheless,if a feature appears more than once in a sentence,this means that it is important and the frequencyvalue representation captures this aspect.The selected features are words (not lemma-tized) delimited by spaces and simple punctua-tion marks: space, ( , ) , [ , ] , .
, ' , _ that ap-peared at least three times in the training collec-tion and contain at least an alpha-numeric char-acter, are not part of an English list of stopwords5 and are longer than three characters.
Stopwords are function words that appear in everydocument (e.g., the, it, of, an) and therefore donot help in classification.
The frequency thresh-old of three is commonly used for text collec-tions because it removes non-informative fea-tures and also strings of characters that might bethe result of a wrong tokenization when splittingthe text into words.
Words that have length ofone or two characters are not considered as fea-tures because of two reasons: possible incorrecttokenization and problems with very short acro-nyms in the medical domain that could be highlyambiguous (could be a medical acronym or anabbreviation of a common word).4.2 NLP and biomedical concepts represen-tationThe second type of representation is based onNLP information ?
noun-phrases, verb-phrasesand biomedical concepts (Biomed).
In order toextract this type of information from the data, weused the Genia6 tagger.
The tagger analyzes Eng-lish sentences and outputs the base forms, part-of-speech tags, chunk tags, and named entitytags.
The tagger is specifically tuned for bio-medical text such as Medline abstracts.Figure 1 presents an output example by theGenia tagger for the sentence: ?Inhibition of NF-kappaB activation reversed the anti-apoptoticeffect of isochamaejasmin.?.
The tag O standsfor Outside, B for Beginning, and I for Inside.5http://www.site.uottawa.ca/~diana/csi5180/StopWords6http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/tagger/Figure 1.
Example of Genia tagger outputInhibition     Inhibition  NN  B-NP  Oof       of   IN  B-PP  ONF-kappaB NF-kappaB  NN  B-NP B-proteinactivation    activation   NN  I-NP  Oreversed       reverse  VBD  B-VP  Othe       the   DT  B-NP  Oanti-apoptotic anti-apoptotic JJ  I-NP  Oeffect        effect  NN  I-NP  Oof        of   IN  B-PP  Oisochamaejasmin isochamaejasmin NN B-NP  O.  .
.
O  OThe noun-phrases and verb-phrases identified bythe tagger are considered as features for our sec-ond representation technique.
The following pre-processing steps are applied before defining theset of final features: remove features that containonly punctuation, remove stop-words, and con-sider valid features only the lemma-based formsof the identified noun-phrases, verb-phrases andbiomedical concepts.
The reason to do this isbecause there are a lot of inflected forms (e.g.,plural forms) for the same word and the lemma-tized form (the base form of a word) will give usthe same base form for all the inflected forms.4.3 Medical concepts (UMLS) representa-tionIn order to work with a representation that pro-vides features that are more general than thewords in the abstracts (used in the BOW repre-sentation), we also used the unified medical lan-guage system7 (here on UMLS) concept repre-sentations.
UMLS is a knowledge source devel-oped at the U.S. National Library of Medicine(here on NLM) and it contains a meta-thesaurus,a semantic network, and the specialist lexicon forbiomedical domain.
The meta-thesaurus is organ-ized around concepts and meanings; it links al-ternative names and views of the same conceptand identifies useful relationships between dif-ferent concepts.
UMLS contains over 1 millionmedical concepts, and over 5 million conceptnames which are hierarchical organized.
Eachunique concept that is present in the thesaurushas associated multiple text strings variants(slight morphological variations of the concept).All concepts are assigned at least one semantictype from the semantic network providing a gen-eralization of the existing relations between con-cepts.
There are 135 semantic types in theknowledge base linked through 54 relationships.7 http://www.nlm.nih.gov/pubs/factsheets/umls.html94In addition to the UMLS knowledge base,NLM created a set of tools that allow easier ac-cess to the useful information.
MetaMap8  is atool created by NLM that maps free text to medi-cal concepts in the UMLS, or equivalently, itdiscovers meta-thesaurus concepts in text.
Withthis software, text is processed through a seriesof modules that in the end will give a ranked listof all possible concept candidates for a particularnoun-phrase.
For each of the noun phrases thatthe system finds in the text, variant noun phrasesare generated.
For each of the variant nounphrases, candidate concepts (concepts that con-tain the noun phrase variant) from the UMLSmeta-thesaurus are retrieved and evaluated.
Theretrieved concepts are compared to the actualphrase using a fit function that measures the textoverlap between the actual phrase and the candi-date concept (it returns a numerical value).
Thebest of the candidates are then organized accord-ing to the decreasing value of the fit function.We used the top concept candidate for each iden-tified phrase in an abstract as a feature.
Figure 2presents an example of the output of the Meta-Map system for the phrase ?to an increasedrisk".
The information presented in the brackets,the semantic type, ?Qualitative Concept, Quanti-tative Concept?
for the candidate with the fitfunction value 861 is the feature used for ourUMLS representation.Figure 2.
Example of MetaMap system outputMeta Candidates (6)861 Risk [Qualitative Concept, Quantitative Concept]694 Increased (Increased (qualifier value)) [Func-tional Concept]623 Increase (Increase (qualifier value)) [FunctionalConcept]601 Acquired (Acquired (qualifier value)) [TemporalConcept]601 Obtained (Obtained (attribute)) [Functional Con-cept]588 Increasing (Increasing (qualifier value)) [Func-tional Concept]Another reason to use a UMLS concept represen-tation is the concept drift phenomenon that canappear in a BOW representation.
Especially inthe medical domain texts, this is a frequent prob-lem as stated by Cohen et al (2004).
New arti-cles that publish new research on a certain topicbring with them new terms that might not matchthe ones that were seen in the training process ina certain moment of time.8http://mmtx.nlm.nih.gov/Experiments for the task tackled in our re-search are performed with all the above-mentioned representations, plus combinations ofthem.
We combine the BOW, UMLS and NLPand biomedical concepts by putting all featurestogether to represent an instance.5 ResultsThis section presents the results obtained for thetask of identifying semantic relations with themethods described above.
As evaluation meas-ures we report F-measure and accuracy values.The main evaluation metric that we consider isthe F-measure9, since it is a suitable when thedata set is imbalanced.
We report the accuracymeasure as well, because we want to compareour results with previous work.
Table A1 fromappendix A presents the results that we obtainedwith our methods.
The table contains F-measurescores for all three semantic relations with thethree experimental settings proposed for all com-binations of representation and classification al-gorithms.
In this section, since we cannot reportall the results for all the classification algorithms,we decided to report the classifiers that obtainedthe lower and upper margin of results for everyrepresentation setting.
More detailed descriptionsfor the results are present in appendix A. Weconsider as baseline a classifier that always pre-dicts the majority class.
For the relation Cure theF-measure baseline is 66.51%, for Prevent andSide Effect 0%.The next three figures present the best resultsobtained for the three experimental settings.Figure 3.
Best results for Setting 1.85.14%62.50%34.48%0.00%20.00%40.00%60.00%80.00%100.00%Cure - BOW +NLP + Biomed+UMLS - SMOPrevent -UMLS + NLP +Biomed - SVMSideEffect -BOW- NBResults - Setting1F-measure9F-measure represents the harmonic mean betweenprecision and recall.
Precision represents the percent-age of correctly classified sentences while recallrepresents the percentage of sentences identified asrelevant by the classifier.95Figure 4.
Best results for Setting 2.82.00%84.00%86.00%88.00%90.00%92.00%94.00%96.00%98.00%100.00%Cure -BOW +NLP +Biomed+UMLS - NBPrevent -BOW +NLP +Biomed+UMLS - NBSideEffect- BOW +NLP +Biomed+UMLS -CNB98.55% 100%88.89%Results - Setting 2F-measureFigure 5.
Best results for Setting 3.98.55% 100%88.89%80.00%85.00%90.00%95.00%100.00%Cure -  BOW +NLP +Biomed+UMLS - NBPrevent -BOW + NLP +Biomed+UMLS - NBSideEffect -BOW + NLP +Biomed+UMLS - CNBResults - Setting 3F-measure6 DiscussionOur goal was to obtain high performance resultsfor the three semantic relations.
The first set ofexperiments was influenced by previous work ona different task.
The results obtained show thatthis setting might not be suitable for the medicaldomain, due to one of the following possible ex-planations: the number of examples that are con-sidered as being neutral is not sufficient or notappropriate (the neutral examples are consideredsentences that are annotated with a Vague rela-tion in the original data); or the negative exam-ples are not appropriate (the negative examplesare considered sentences that talk about eithertreatment or about diseases).
The results of theseexperiments are shown in Figure 3.
As futurework, we want to run similar setting experimentswhen considering negative examples sentencesthat are not informative, labeled Irrelevant, fromthe original data set, and the neutral examples theones that are considered negative in this currentexperiments.In Setting 2, the results are better than in theprevious setting, showing that the neutral exam-ples used in the previous experiments confusedthe algorithms and were not appropriate.
Theseresults validate the fact that the previous settingwas not the best one for the task.The best results for the task are obtained withthe third setting, when a model is built andtrained on a data set that contains all sentencesannotated with the three relations.
The represen-tation and the classification algorithms were ableto make the distinction between the relations andobtained the best results for this task.
The resultsare: 98.55% F-measure for the Cure class, 100%F-measure for the Prevent class, and 88.89% forthe Side Effect class.Some important observations can be drawnfrom the obtained results: probabilistic and linearmodels combined with informative feature repre-sentations bring the best results.
They are consis-tent in outperforming the other classifiers in allthe three settings.
AdaBoost classifier was out-performed by other classifiers, which is a littlesurprising, taking into consideration the fact thatthis classifier tends to work better on imbalanceddata.
BOW is a representation technique thateven though it is simplistic, most of the times itis really hard to outperform.
One of the majorcontributions of this work is the fact that the cur-rent experiments show that additional informa-tion used in the representation settings bringsimprovements for the task.
The task itself is aknowledge-charged task and the experimentsshow that classifiers can perform better whenricher information (e.g.
concepts for medicalontologies) is provided.6.1 Comparison to previous workEven though our main focus is on the three rela-tions mentioned earlier, in order to validate ourmethodology, we also performed the 8-classclassification task, similar to the one done byRosario and Hearst (2004).
Figure 3 presents agraphical comparison of the results of our meth-ods to the ones obtained in the previous work.We report accuracy values for these experiments,as it was done in the previous work.In Figure 3, the first set of bar-results repre-sents the best individual results for each relation.The representation technique and classificationmodel that obtains the best results are the onesdescribed on the x-axis.96Figure 3.
Comparison of results.Results for all semantic relations0.00%20.00%40.00%60.00%80.00%100.00%120.00%Cure-BOW+NLP+Biomed+UMLS-CNBNo_CurePrevent-BOW+NLP+Biomed-CNBVague-BOW+NLP+Biomed-NBSideEffect-BOW+NLP+Biomed-NBTrearment_Only-BOW+NLP+Biomed-NBDisease_Only-BOW+NLP+Biomed-J48Irrelevant-BOW+NLP+Biomed+UMLS-AdaBModelsAccuracyBest ModelsBest ModelPrevious WorkThe second series of results represents theoverall best model that is reported for each rela-tion.
The model reported here is a combinationof BOW, verb and noun-phrases, biomedical andUMLS concepts, with a CNB classifier.The third series of results represent the accu-racy results obtained in previous work by Rosa-rio and Hearst (2004).
As we can see from thefigure, the best individual models have a majorimprovement over previous results.
When a sin-gle model is used for all relations, our resultsimprove the previous ones in four relations withthe difference varying from: 3 percentage pointdifference (Cure) to 23 percentage point differ-ence (Prevent).
We obtain the same results fortwo semantic relations, No_Cure and Vague andwe believe that this is the case due to the fact thatthese two classes are significantly under-represented compared to the other ones involvedin the task.
For the Treatment_Only relation ourresults are outperformed with 1.5 percentagepoints and for the Irrelevant relation with 0.1percentage point, only when we use the samemodel for all relations.7 Conclusion and Future WorkWe can conclude that additional knowledge anddeeper analysis of the task and data in questionare required in order to obtain reliable results.Probabilistic models are stable and reliable forthe classification of short texts in the medicaldomain.
The representation techniques highlyinfluence the results, common for the ML com-munity, but more informative representationswhere the ones that consistently obtained the bestresults.As future work, we would like to extend theexperimental methodology when the first settingis applied, and to use additional sources of in-formation as representation techniques.ReferencesAhlers C., Fiszman M., Fushman D., Lang F.-M.,Rindflesch T. 2007.
Extracting semantic predica-tions from Medline citations for pharmacogenom-ics.
Pacific Symposium on Biocomputing, 12:209-220.Craven M. 1999.
Learning to extract relations fromMedline.
AAAI-99 Workshop on Machine Learn-ing for Information Extraction.Feldman R. Regev Y., Finkelstein-Landau M., Hur-vitz E., and Kogan B.
2002.
Mining biomedical lit-erature using information extraction.
Current DrugDiscovery.Friedman C., Kra P., Yu H., Krauthammer M., andRzhetzky A.
2001.
Genies: a natural-languageprocessing system for the extraction of molecularpathways from journal articles.
Bioinformatics,17(1).Ginsberg J., Mohebbi Matthew H., Rajan S. Patel,Lynnette Brammer, Mark S. Smolinski & LarryBrilliant.
2009.
Detecting influenza epidemicsusing search engine query data.
Nature 457,1012-1014.Hunter Lawrence and K. Bretonnel Cohen.
2006.Biomedical Language Processing: What?s BeyondPubMed?
Molecular Cell 21, 589?594.Ray S. and Craven M. 2001.
Representing sentencestructure in Hidden Markov Models for informa-tion extraction.
Proceedings of IJCAI-2001.Rosario B. and Marti A. Hearst.
2004.
Classifyingsemantic relations in bioscience text.
Proceed-ings of the 42nd Annual Meeting on Associationfor Computational Linguistics, 430.Koppel M. and J. Schler.
2005.
Using Neutral Ex-amples for Learning Polarity, Proceedings ofIJCAI, Edinburgh, Scotland.Srinivasan P. and T. Rindflesch 2002.
Exploring textmining from Medline.
Proceedings of the AMIASymposium.97Appendix A.
Detailed Results.Classification Algorithm - F-Measure (%)RelationRepresentationSetting1 Setting2 Setting3Cure NLP+Biomed AdaBZeroR32.2266.51AdaBZeroR35.6967.48CNBSVM87.8894.85BOW AdaBCNB63.6079.22AdaBSVM67.2381.43CNBNB92.5796.80UMLS AdaBNB61.0874.73AdaBNB64.7876.04CNBSVM88.2095.62BOW+UMLS AdaBCNB56.0784.54AdaBNB74.6886.48J48NB96.1397.50NLP+Biomed+UMLSAdaBNB61.0875.18AdaBNB64.7876.70CNBSVM90.8796.58NLP+Biomed+BOWAdaBSVM53.0478.98AdaBCNB77.4681.86J48NB96.1497.86NLP+Biomed+BOW+UMLSAdaBSVM53.0485.14AdaBSVM72.3287.10J48NB96.3298.55Prevent NLP+Biomed AdaBNB017.02AdaB,J48NB022.86Ada,J48CNB055.17BOW CNBNB31.7850J48NB061.9SVMCNB5089.47UMLS AdaBNB028.57J48SVM048.28J48CNB068.75BOW+UMLS J48NB39.0257.14J48NB9.0975.68AdaBCNB6089.47NLP+Biomed+UMLSAdaBSVM062.50J48SVM1657.69J48CNB097.56NLP+Biomed+BOWSVMNB3554.90J48NB066.67AdaBCNB64.5292.31NLP+Biomed+BOW+UMLSJ48NB30.7762.30J48SVM077.78AdaB,J48NB64.52100SideEffectNLP+Biomed AdaBNB,CNB07.69J48,SVMAdaB018.18AdaB,J48CNB033.33BOW AdaBNB034.48AdaB,J48NB050Ada,J48CNB066.67UMLS AdaB,J48,SVM NB022.22J48,SVMNB033.33AdaB,J48NB,CNB046.15BOW+UMLS AdaB,J48NB021.43J48NB047AdaBCNB075NLP+Biomed+UMLSAdaB,J48NB019.35J48NB031.58AdaB.J48NB,CNB046.15NLP+Biomed+BOWAdaB,J48NB033.33J48NB055.56AdaB,J48CNB088.89NLP+Biomed+BOW+UMLSAdaB,J48NB024J48NB046.15AdaBCNB088.89Table A1.
Results obtained with our methods.The Representation column describes all the feature representation techniques that we tried.
The acro-nym NLP stands from verb and noun-phrase features put together and Biomed for bio-medical con-cepts (the ones extracted by Genia tagger).
The first line of results for every representation techniquepresents the classier that obtained the lowest results, while the second line represents the classifierwith the best F-measure score.
In bold we mark the best scores for all semantic relations in each of thethree settings.98
