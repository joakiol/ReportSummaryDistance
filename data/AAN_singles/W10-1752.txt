Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 343?348,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsNormalized Compression Distance Based Measures forMetricsMATR 2010Marcus Dobrinkat and Jaakko Va?yrynen and Tero TapiovaaraAdaptive Informatics Research CentreAalto University School of Science and TechnologyP.O.
Box 15400, FI-00076 Aalto, Finland{marcus.dobrinkat,jaakko.j.vayrynen,tero.tapiovaara}@tkk.fiKimmo KettunenKymenlaakso University of Applied SciencesP.O.
Box 9, FI-48401 Kotka, FinlandKimmo.kettunen@kyamk.fiAbstractWe present the MT-NCD and MT-mNCDmachine translation evaluation metricsas submission to the machine transla-tion evaluation shared task (MetricsMATR2010).
The metrics are based on nor-malized compression distance (NCD), ageneral information theoretic measure ofstring similarity, and evaluated against hu-man judgments from the WMT08 sharedtask.
The experiments show that 1)our metric improves correlation to hu-man judgments by using flexible match-ing, 2) segment replication is effective,and 3) our NCD-inspired method for mul-tiple references indicates improved results.Generally, the proposed MT-NCD andMT-mNCD methods correlate competi-tively with human judgments compared tocommonly used machine translations eval-uation metrics, for instance, BLEU.1 IntroductionThe quality of automatic machine translation(MT) evaluation metrics plays an important rolein the development of MT systems.
Human eval-uation would no longer be necessary if automaticMT metrics correlated perfectly with manual judg-ments.
Besides high correlation with human judg-ments of translation quality, a good metric shouldbe language independent, fast to compute and sen-sitive enough to reliably detect small improve-ments in MT systems.Recently there have been some experimentswith normalized compression distance (NCD) as amethod for automatic evaluation of machine trans-lation.
NCD is a general string similarity measurethat has been useful for clustering in various tasks(Cilibrasi and Vitanyi, 2005).Parker (2008) introduced BADGER, a machinetranslation evaluation metric that uses NCD to-gether with a language independent word normal-ization method.
Kettunen (2009) independentlyapplied NCD to the direct evaluation of transla-tions.
He showed with a small corpus of three lan-guage pairs that the scores of NCD and METEOR(v0.6) from translations of 10?12 MT systemswere highly correlated.Va?yrynen et al (2010) have extended the workby showing that NCD can be used to rank transla-tions of different MT systems so that the rankingorder correlates with human rankings at the samelevel as BLEU (Papineni et al, 2001).
For trans-lations into English, NCD had an overall system-level correlation of 0.66 whereas the best method,ULC had an overall correlation of 0.76, and BLEUhad an overall correlation of 0.65.
NCD presentsa viable alternative to the de facto standard BLEU.Both metrics are language independent, simpleand efficient to compute.
However, NCD is ageneral measure of similarity that has been ap-plied in many domains.
More advanced meth-ods achieve better correlation with human judg-ments, but typically use additional language spe-cific linguistic resources.
Dobrinkat et al (2010)experimented with relaxed word matching, addinglanguage specific resources to NCD.
The metriccalled mNCD, which works similarly to mBLEU(Agarwal and Lavie, 2008), showed improved cor-relation to human judgments in English, the onlylanguage where a METEOR synonym module wasused.The motivation for this challenge submission isto evaluate the MT-NCD and MT-mNCD metricperformance in an open competition with state-of-343the-art MT evaluation metrics.
Our experimentsand submission build on NCD and mNCD.
We ex-pand NCD to handle multiple references and re-port experimental results for replicating segmentsas a preprocessing step that improves the NCD asan MT evaluation metric.2 NCD-based MT evaluation metricsNCD-based MT evaluation metrics build on theidea that a string x is similar to another string y,when both share common substrings.
When de-scribing y, common substrings do not have to berepeated, but can be referenced to x.
This is donewhen compressing the concatenation of x and y,which results in smaller output when more infor-mation of y is already included in x.2.1 Normalized Compression DistanceThe normalized compression distance, as definedby Cilibrasi and Vitanyi (2005) is given in Equa-tion 1, in which C(x) is the length of the compres-sion of x and C(x, y) is the length of the compres-sion of the concatenation of x and y.NCD(x, y) =C(x, y)?min {C(x), C(y)}max {C(x), C(y)}(1)NCD computes the distance as a score closer toone for very different strings and closer to zero formore similar strings.
Most MT evaluation met-rics are defined as similarity measures in contrastto NCD, which is a distance measure.
For eas-ier comparison with other MT evaluation metrics,we define the NCD based MT evaluation similar-ity metric MT-NCD as 1?
NCD.NCD is a practically usable form of the uncom-putable normalized information distance (NID), ageneral metric for the similarity of two objects.NID is based on the notion of Kolmogorov com-plexity K(x), a theoretical measure for the algo-rithmic information content of a string x.
It is de-fined as the shortest universal Turing machine thatprints x and stops (Solomonoff, 1964).
NCD ap-proximates NID by the use of a compressor C(x)that presents a computable approximation of theKolmogorov complexity K(x).2.2 NCD with multiple referencesMost ideas can be described with in differentways, therefore using only one reference transla-tion for the evaluation of a candidate sentence isnot ideal and the exploitation of knowledge in sev-eral different reference translations is helpful forautomatic MT evaluation.One simple way for handling multiple refer-ences is to evaluate against each reference indi-vidually and select the maximum score.
Althoughthis works, it is clearly not optimal.
We developedthe NCDm metric, which is inspired by NCD.
Itconsiders all references simultaneously and thequality of a translation t against multiple refer-ences R = {r1, .
.
.
, rm} is assessed asNCDm(t, R) =max{C(t|R),minr?RC(r|t}max{C(t),minr?RC(r)}(2)where C(x|y) = C(x, y) ?
C(y) approximatesconditional algorithmic information with the com-pressor C. The NCDm similarity metric with asingle reference (m = 1) is equal to NCD in Equa-tion 1.
Again, we define MT-NCDm as 1?NCDm.Figure 1 shows how both, the MT-NCDm andthe BLEU metric change with a different num-ber of references when the translation is variedfrom correct to a random sequence of words.
Thescores are computed with 249 sentences from theLDC2010E28Dev data set using the first referenceas the correct translation.
A higher score with mul-tiple references against the correct translation indi-cates that the measure is able to take into accountinformation from multiple references at the sametime.The words in the candidate translation are re-placed with probability p with a word randomlyselected with uniform probability from a lexiconcreated from all reference translations.
This simu-lates partially correct translations.
The words arechanged in a simple way without deletions, inser-tions or word order permutations.
The MT-NCDmscore increases with more than one referencetranslation and random changes to the sentence re-duce the score roughly proportional to the numberof changed words.
With BLEU, the score is af-fected more by a small number of changes.2.3 mNCDOne enhancement to the basic NCD as auto-matic evaluation metric is mNCD (Dobrinkat etal., 2010), which provides relaxed word matchingbased on the flexible matching modules of ME-TEOR (Agarwal and Lavie, 2008).What mNCD does is that it changes the ref-erence sentence to be more similar to the candi-3440.0 0.2 0.4 0.6 0.8 1.00.00.10.20.30.40.50.6word change probability (p)MTevaluationmetric scoreMT?NCDm with 3 referencesMT?NCDm with 2 referencesMT?NCDm with 1 referenceBLEU with 3 referencesBLEU with 2 referencesBLEU with 1 referenceMT?NCDmBLEUFigure 1: The MT-NCDm and BLEU scores witha different number of multiple references againstcorrect translation with random word changeprobability (p).date, given that some of the words are synonymsor share the same stem.
Subsequent analysis usingany n-gram based automatic analysis should resultin a larger similarity score in the hope that this re-flects more than just the surface similarity betweenthe candidate and the reference.Given suitable Wordnet resources, mNCDshould alleviate the problem of translation vari-ability especially in absence of multiple refer-ence translations.
Our submission uses the de-fault METEOR exact stem synonym mod-ules, which provide synonyms only for English.We base our submission metric on the MT-NCDmetric and therefore define MT-mNCD as 1 ?mNCD.3 MT Evaluation System Description3.1 System ParametersThe system parameters for the submission metricsinclude how candidates and references are prepro-cessed, the choice of compressor for the NCD it-self, as well as the granularity of how large seg-ments are evaluated by NCD and how they arecombined into a final score.Partly due to time constraints we decided not tointroduce language specific parameters, thereforewe chose those parameter values that perform wellin overall and are simple to compute.3.1.1 PreprocessingCharacter casing For MT-NCD, we did ex-periments without preprocessing and with lower-casing candidates and references.
On average overall tasks for language pairs into English, lower-casing consistently decreased the RANK correla-tion scores but increased the CONST correlationscores.
No consistent effect could be found for thelanguage pairs from English.
In our submissionmetrics we use no preprocessing.For MT-mNCD the used METEOR matchingmodule lower-cases the adapted words by default.After adapting a synonym in a reference, we triedto keep the casing as it was in the candidate, whichwe called real-casing.
We use no real-casing forour submitted MT-mNCD metric as this did notimprove results consistently over all task into En-glish.Segment Replication Compression algorithmsmay not work optimally with short strings, whichwould deteriorate the approximation of Kol-mogorov complexity.
Our hypothesis was thata replication of a string (?abc?)
multiple times(3 ?
?abc?
= ?abcabcabc?)
could help the com-pression algorithm to produce a better estimate ofthe algorithmic information.
This was tested inthe MT evaluation framework, and correlation be-tween MT-NCD and human judgments improvedwhen the segments were replicated two times.Further replication did not produce improvements.Results for the MT-NCD metric with replica-tions one, two and three times are shown in Ta-ble 1.
The results are averages over all used lan-guages.
With two compared to one replication, thedetails for each language show that RANK corre-lation is improved for the target languages Englishand French, but degrades for German and Spanish.CONST andYES/NO correlation improve for alllanguages except German.
We did not use repli-cation in our submissions.3.1.2 Block sizeThe block size parameter governs the number ofjoined segments that are compared with NCD as asingle string.
On one extreme, with block size one,345RANKCONSTYES/NOTOTALMT-NCD rep 1 .61 .71 .73 .68MT-NCD rep 2 .62 .73 .75 .70MT-NCD rep 3 .61 .72 .74 .69Table 1: Effect of the replication factor onMT-NCD correlation scores for the bz2 compres-sor with block size one as average over all lan-guages.each segment is evaluated separately and the seg-ment scores are aggregated to a document score.This is similar to how other MT metrics, for ex-ample, BLEU, work.
The other extreme is to joinall segments together, with block size equal to thenumber of segments, and evaluate it as a singlestring, which is similar to document comparison.For block aggregation we experimented with arith-metic and geometric mean and obtained very sim-ilar results.
We selected arithmetic mean for thesubmission metrics.Figure 2 shows the block size effect on the cor-relation between MT-NCD and human judgmentsfor different target languages.
Except for Spanish,our experiments indicate that the block size valuehas little effect.
Therefore, and given how otherevaluation metrics work, we chose a block size ofone for our submission metrics.
We noticed incon-sistencies with Spanish in other settings as welland will investigate these issues further.3.1.3 CompressorThere are several universal compressors that canbe utilized with NCD, for instance, zlib/gzip, bz2and PPMZ, which represent different approachesto compression.
In terms of compression rate,PPMZ is the best of the mentioned methods, butit is considerably slower to compute compared tothe other methods.
In terms of correlation with hu-man judgments, NCD using bz2 performs slightlyworse than using PPMZ.
Given much shorter com-pression times for bz2 with very little correlationperformance degradation, our choice for the sub-mission is the more standard bz2 compressor.3.1.4 Segment InterleavingComputation of NCD between longer texts (e.g.documents) may exceed the internal compressorwindow size that is present in some compression2 5 10 20 50 100 500 2000 50000.00.20.40.60.81.0block size in linessystemlevelcorrelationwith humanjudgementsinto frinto eninto esinto defrenesdeFigure 2: Effect of the block size on the correlationof MT-NCD to human judgments for the systemlevel evaluation.algorithms (Cebrian et al, 2005).
In this case,only a part of the texts to be compared are visibleat any time to the compressor and similarities tothe text outside the window will be missed.
Onesolution for the MT evaluation task is to use uti-lize the known parallel segments of candidate andreference translations.
The two segment lists canbe interleaved so that the corresponding segmentsare always adjacent and the compression windowsize is not exceeded for matching segments.For our submission, we chose a block size ofone, therefore every segment is evaluated individ-ually.
As a result, segment interleaving does nothave any effect.
Segment interleaving is affectivein the block size evaluation and results shown inFigure 2.3.2 Evaluation ExperimentsWe chose parameters and evaluated our metricsusing the WMT08 part of the MetricsMATR 2010development data, which contains human judg-ments of the 2008 ACL Workshop on StatisticalMachine Translation (Callison-Burch et al, 2008)for translations from a total of 30 MT systems be-tween English and five other European languages.There are human evaluations and several auto-matic evaluations for the translations, divided intoseveral tasks defined by the language pair and thedomain of the translated sentences.
For each ofthese tasks, the WMT08 data contains about 2 000346reference sentences (segments) plus their alignedtranslations for 12 to 17 different translation sys-tems, depending on the language pair.The human judgments include three categorieswhich contain evaluations for at most one segmentat a time, not whole documents.
In the RANKcategory, humans had to rank the output of fiveMT systems according to quality.
The CONSTcategory contains rankings for short phrases (con-stituents), and the YES/NO category contains bi-nary answers to judge if a short phrase is an ac-ceptable translation or not.We report RANK, CONST and YES/NO systemlevel correlations to human judgments as results ofour metrics for French, Spanish and German bothfrom and to English.
The English?Spanish newstask was left out as most metrics had negative cor-relation with human judgments.The evaluation methodology used in Callison-Burch et al (2008) allows us to measure how eachMT evaluation metric correlates with human judg-ments on the system level, in which all translationsfrom each MT system are aggregated into a singlescore.
The system rankings based on the scoresare compared to human judgments.Spearman?s rank correlation coefficient ?
wascalculated between each MT metric and humanjudgment category using the simplified equation:?
= 1?6?i din(n2 ?
1)(3)where for each system i, di is the difference be-tween the rank derived from annotators?
input andthe rank obtained from the metric.
From the anno-tators?
input, the nMT systems were ranked basedon the number of times each system?s output wasselected as the best translation divided by the num-ber of times each system was part of a judgment.3.3 ResultsThe results for WMT08 data for our submittedmetrics are shown in Table 2 and are sorted by theRANK category separately for language pairs fromEnglish and into English.For tasks into English, the correlations showthat MT-mNCD improves over the MT-NCD met-ric in all categories.
Also the flexible match-ing seems to work better for NCD-based metricsthan for BLEU, where mBLEU only improvesthe CONST correlation scores.
For tasks fromEnglish, MT-mNCD shows slightly higher cor-relation compared to MT-NCD, except for theYES/NO category.
The standard BLEU correla-tion score is best of the shown evaluation met-rics.
Relaxed matching using mBLEU does notimprove BLEU?s RANK correlation scores hereeither, but CONST and YES/NO correlation per-forms better relative to BLEU than MT-mNCDcompared to MT-NCD.RANKCONSTYES/NOTOTALINTOEN MT-mNCD .61 .74 .75 .70MT-NCD .57 .69 .71 .66mBLEU .50 .76 .70 .65BLEU .50 .72 .74 .65FROMEN BLEU .68 .79 .79 .75MT-mNCD .67 .76 .74 .72MT-NCD .65 .73 .75 .71mBLEU .63 .81 .81 .75Table 2: Average system-level correlations for theWMT08 data sorted by RANK into English andfrom English for our submitted metrics MT-NCDand MT-mNCD and for BLEU and mBLEU4 ConclusionsIn our submissions, we applied MT-NCD andMT-mNCD metrics and extended the NCD MTevaluation metric to handle multiple references.The reported experiment indicate a possible im-provement for the multiple references.We showed that a replication of segments as apreprocessing step improves the correlation to hu-man judgments.
The string replication might alle-viate problems in the compressor for short stringsand thus could provide better estimates of the al-gorithmic information.The results of our experiments show that re-laxed matching in MT-mNCD works well withproper synonym dictionaries, but is less effectivefor tasks from English, which only use stemming.MT-mNCD and MT-NCD are reasonably sim-ple to compute and utilize standard and widelyused resources, such as the bz2 compression al-gorithm and WordNet.
The metrics perform com-parable to the de facto standard BLEU.
Improve-ments with language dependent resources, in par-ticular relaxed matching using synonym dictionar-ies proved to be useful.347ReferencesAbhaya Agarwal and Alon Lavie.
2008.
METEOR,M-BLEU and M-TER: evaluation metrics for high-correlation with human rankings of machine trans-lation output.
In StatMT ?08: Proceedings of theThird Workshop on Statistical Machine Translation,pages 115?118, Morristown, NJ, USA.
Associationfor Computational Linguistics.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InStatMT ?08: Proceedings of the Third Workshopon Statistical Machine Translation, pages 70?106,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Manuel Cebrian, Manuel Alfonseca, and Alfonso Or-tega.
2005.
Common pitfalls using the normalizedcompression distance: What to watch out for in acompressor.
Communications in Information andSystems, 5(4):367?384.Rudi Cilibrasi and Paul Vitanyi.
2005.
Clusteringby compression.
IEEE Transactions on InformationTheory, 51:1523?1545.Marcus Dobrinkat, Tero Tapiovaara, Jaakko J.Va?yrynen, and Kimmo Kettunen.
2010.
Evaluatingmachine translations using mNCD.
In Proceedingsof the ACL-2010 (to appear), Uppsala, Sweden.Kimmo Kettunen.
2009.
Packing it all up in search fora language independent MT quality measure tool.
InProceedings of LTC-09, 4th Language and Technol-ogy Conference, pages 280?284, Poznan.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.2001.
BLEU: a method for automatic evaluationof machine translation.
Technical Report RC22176(W0109-022), IBM Research Division, Thomas J.Watson Research Center.Steven Parker.
2008.
BADGER: A new machine trans-lation metric.
In Metrics for Machine TranslationChallenge 2008, Waikiki, Hawai?i, October.
AMTA.Ray Solomonoff.
1964.
Formal theory of inductiveinference.
Part I.
Information and Control, 7(1):1?22.Jaakko J. Va?yrynen, Tero Tapiovaara, Kimmo Ket-tunen, and Marcus Dobrinkat.
2010.
Normalizedcompression distance as an automatic MT evalua-tion metric.
In Proceedings of MT 25 years on.
Toappear.348
