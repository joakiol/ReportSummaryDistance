Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 566?574,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAn effective Discourse Parser that uses Rich Linguistic InformationRajen Subba ?Display Advertising SciencesYahoo!
LabsSunnyvale, CA, USArajen@yahoo-inc.comBarbara Di EugenioDepartment of Computer ScienceUniversity of IllinoisChicago, IL, USAbdieugen@cs.uic.eduAbstractThis paper presents a first-order logic learn-ing approach to determine rhetorical relationsbetween discourse segments.
Beyond lin-guistic cues and lexical information, our ap-proach exploits compositional semantics andsegment discourse structure data.
We reporta statistically significant improvement in clas-sifying relations over attribute-value learn-ing paradigms such as Decision Trees, RIP-PER and Naive Bayes.
For discourse pars-ing, our modified shift-reduce parsing modelthat uses our relation classifier significantlyoutperforms a right-branching majority-classbaseline.1 IntroductionMany theories postulate a hierarchical structure fordiscourse (Mann and Thompson, 1988; Moser et.al., 1996; Polanyi et.
al., 2004).
Discourse struc-ture is most often based on semantic / pragmatic re-lationships between spans of text and results in a treestructure, as that shown in Figure 1.
Discourseparsing, namely, deriving such tree structures andthe rhetorical relations labeling their inner nodes isstill a challenging and mostly unsolved problem inNLP.
It is linguistically plausible that such structuresare determined at least in part on the basis of themeaning of the related chunks of texts, and of therhetorical intentions of their authors.
However, suchknowledge is extremely difficult to capture.
Hence,previous work on discourse parsing (Wellner et.
al.,2006; Sporleder and Lascarides, 2005; Marcu, 2000;Polanyi et.
al., 2004; Soricut and Marcu, 2003;?This work was done while the author was a student at theUniversity of Illinois at Chicago.Baldridge and Lascarides, 2005) has relied only onsyntactic and lexical information, lexical chains andshallow semantics.We present an innovative discourse parser thatuses compositional semantics (when available) andinformation on the structure of the segment beingbuilt itself.
Our discourse parser, based on a modi-fied shift-reduce algorithm, crucially uses a rhetori-cal relation classifier to determine the site of attach-ment of a new incoming chunk together with the ap-propriate relation label.
Another novel aspect of ourwork is the usage of Inductive Logic Programming(ILP): ILP learns from first-order logic representa-tions (FOL).
The ILP-based relation classifier issignificantly more accurate than relation classifiersthat use competitive propositional ML algorithmssuch as decision trees and Naive Bayes.
In addi-tion, it results in FOL rules that are linguisticallyperspicuous.
Our domain is that of instructionalhow-to-do manuals, and we describe our corpusin Section 2.
In Section 3, we discuss the modifiedshift-reduce parser we developed.
The bulk of thepaper is devoted to the rhetorical relation classifierin Section 4.
Experimental results of both the rela-tion classifier and the discourse parser in its entiretyare discussed in Section 5.
Further details can befound in (Subba, 2008).2 Discourse Annotated InstructionalCorpusExisting corpora annotated with rhetorical relations(Carlson et.
al., 2003; Wolf and Gibson, 2005;Prasad et.
al., 2008) focus primarily on news arti-cles.
However, for us the development of the dis-course parser is parasitic on our ultimate goal: de-veloping resources and algorithms for language in-566s1e1-s5e2general:specificdddddddddddddddddddddddddddddddddddddZZZZZZZZZZZZZZZZZZZs1e1 s2e1-s5e2preparation:actgggggggggggggggggggggggWWWWWWWWWWWWWWWWWWWWWWWAnother way .... sheets.s2e1-s4e1preparation:actRRRRRRRRRRRRRRRRlllllllllllllllls5e1-s5e2act:goalxxxxxFFFFFFFFFFs2e1-s3e2preparation:actRRRRRRRRRRRRRRRRlllllllllllllllls4e1 s5e1 s5e2s2e1-s2e2reason:actFFFFFFFFFFxxxxxs3e1-s3e2disjunctionFFFFFFFFFFxxxxxxxxxxThen laythe sheet.. the panel.Using the .... pattern,mark the panel.s2e1 s2e2 s3e1 s3e2Because thesesheets .... panels,you cantape one .... panel.Mark theopening .... sheet,or cut it.. blade.Figure 1: Discourse Parse Tree of the Text in Example (1)terfaces to instructional applications.
Hence, we areinterested in working with instructional texts.
Weworked with a corpus on home-repair that is about5MB in size and is made up entirely of written En-glish instructions,1 such as that shown in Exam-ple (1).
The text has been manually segmentedinto Elementary Discourse Units (EDUs), the small-est units of discourse.
In total, our corpus contains176 documents with an average of 32.6 EDUs for atotal of 5744 EDUs and 53,250 words.
The structurefor Example (1) is shown in Figure 1.
(1) [Another way to measure and mark panels forcutting is to make a template from the protec-tive sheets.
(s1e1)] [Because these sheets are thesame size as the panels,(s2e1)] [you can tapeone to the wall as though it were a panel.
(s2e2)][Mark the opening on the sheet(s3e1)] [or cutit out with a razor blade.
(s3e2)] [Then lay thesheet on the panel.
(s4e1)] [Using the templateas a pattern,(s5e1)] [mark the panel.
(s5e2)]To explore our hypothesis, that rich linguistic in-formation helps discourse parsing, and that the state1The raw corpus was originally assembled at the Informa-tion Technology Research Institute, University of Brighton.of the art in machine learning supports such anapproach, we needed training data annotated withboth compositional semantics and rhetorical rela-tions.
We performed the first type of annotation al-most completely automatically, and the second man-ually, as we turn now to describing.2.1 Compositional Semantics DerivationThe type of compositional semantics we are inter-ested in is heavily rooted in verb semantics, whichis particularly appropriate for the instructional textwe are working with.
Therefore, we used VerbNet(Kipper et.
al., 2000) as our verb lexicon.
VerbNetgroups together verbs that undergo the same syn-tactic alternations and share similar semantics.
Itaccounts for 4962 distinct verbs classified into 237main classes.
Each verb class is described by the-matic roles, selectional restrictions on the argumentsand frames consisting of a syntactic description andsemantic predicates.
Such semantic classification ofverbs can be helpful in making generalizations, es-pecially when data is not abundant.
Generalizationcan also be achieved by means of the semantic pred-icates.
Although the verb classes of two verb in-stances may differ, semantic predicates are sharedacross verbs.
To compositionally build verb based567semantic representations of our EDUs, we (Subbaet al, 2006) integrated a robust parser, LCFLEX(Rose?, 2000), with a lexicon and ontology basedboth on VerbNet and, for nouns, on CoreLex (Buite-laar, 1998).
The augmented parser was able to de-rive complete semantic representations for 3257 ofthe 5744 EDUs (56.7%).
The only manual step wasto pick the correct parse from a forest of parse trees,since the output of the parser can be ambiguous.2.2 Rhetorical relation annotationThe discourse processing community has not yetreached agreement on an inventory of rhetorical re-lations.
Among the many choices, our codingscheme is a hybrid of (Moser et.
al., 1996) and(Marcu, 1999).
We focused on what we call infor-mational relations, namely, relations in the domain.We used 26 relations, divided into 5 broad classes:12 causal relations (e.g., preparation:act, goal:act,cause:effect, step1:step2); 6 elaboration relations(e.g., general:specific, set:member, object:attribute;3 similarity relations (contrast1:contrast2, com-parison, restatement); 2 temporal relations (co-temp1:co-temp2, before:after); and 4 other rela-tions, including joint and disjunction.The annotation yielded 5172 relations, with rea-sonable intercoder agreement.
On 26% of the data,we obtained ?
= 0.66; ?
rises to 0.78 when the twomost commonly confused relations, preparation:actand step1:step2, are consolidated.
We also anno-tated the relata as nucleus (more important mem-ber) and satellite (contributing member(s)) (Mannand Thompson, 1988), with ?
= 0.67.2 The mostfrequent relation is preparation:act (24.46%), and ingeneral, causal relations are more frequently used inour instructional corpus than in news corpora (Carl-son et.
al., 2003; Wolf and Gibson, 2005).3 Shift-Reduce Discourse ParsingOur discourse parser is a modified version of a shift-reduce parser.
The shift operation places the nextsegment on top of the stack, TOP.
The reduce oper-ation will attach the text segment at TOP to the textsegment at TOP-1.
(Marcu, 2000) also uses a shift-reduce parser, though our parsing algorithm differs2We don?t have space to explain why we annotate for nu-cleus and satellite, even if (Moser et.
al., 1996) argue that thissort of distinction does not apply to informational relations.in two respects: 1) we do not learn shift operationsand 2) in contrast to (Marcu, 2000), the attachmentof an incoming text segment to the emerging treemay occur at any node on the right frontier.
This al-lows for the more sophisticated type of adjunctionoperations required for discourse parsing as mod-eled in D-LTAG (Webber, 2004).
A reduce op-eration is determined by the relation identificationcomponent.
We check if a relation exists betweenthe incoming text segment and the attachment pointson the right frontier.
If more than one attachmentsite exists, then the attachment site for which the rulewith the highest score fired (see below) is chosen forthe reduce operation.
A reduce operation can fur-ther trigger additional reduce operations if there ismore than one tree left in the stack after the first re-duce operation.
When no rules fire, a shift occurs.In the event that all the segments in the input listhave been processed and a full DPT has not beenobtained, then we reduce TOP and TOP-1 using thejoint relation until a single DPT is built.4 Classifying Rhetorical RelationsIdentifying the informational relations between textsegments is central to our approach for building theinformational tree structure of text.
We believe thatthe use of a limited knowledge representation for-malism, essentially propositional logic, is not ad-equate and that a relational model that can handlecompositional semantics is necessary.
We cast theproblem of determining informational relations as aclassification task.
We used the ILP system Alephthat is based on (Muggleton, 1995).
Formulationof any problem within the ILP framework consistsof background knowledge B and the set of exam-ples E (E+?
E?).
In our ILP framework, positiveexamples are ground clauses describing a relationand its relata, e.g.
relation(s5e1,s5e2,act:goal), orrelation(s2e1-s3e2,s4e1,preparation:act) from Fig-ure 1.
If e is a positive example of a relation r, thenit is also a negative example for all the other rela-tions.Background Knowledge (B) can be thought of asfeatures used by ILP to learn rules, as in traditionalattribute-value learning algorithms.
We use the fol-lowing information to learn rules for classifying re-lations.
Figure 2 shows a sample of the background568Verbs + Nouns: verb(?s5e2?,mark).
noun(?s5e2?,panel).Linguistic Cues: firstWordPOS(?s5e2?,?VB?).
lastWordPOS(?s5e2?,?.?
).Similarity: segment sim score(?s5e1?,?s5e2?,0.0).verbclass(?s5e2?,mark,?image impression-25.1?
).agent(?s5e2?,frame(mark),you).Compositional Semantics: destination(?s5e2?,frame(mark),panel).cause(?s5e2?,frame(mark),you,?s5e2-mark-e?).prep(?s5e2?,frame(mark),end(?s5e2-mark-e?
),mark,panel).created image(?s5e2?,frame(mark),result(?s5e2-mark-e?
),mark).Structural Information: same sentence(?s5e1?,?s5e2?
).Figure 2: Example Background Knowledgeknowledge provided for EDU s5e2.Verbs + Nouns: These features were derived bytagging all the sentences in the corpus with a POStagger (Brill, 1995).WordNet: For each noun in our data, we also useinformation on hypernymy and meronymy relationsusing WordNet.
In a sense, this captures the domainrelations between objects in our data.Linguistic Cues: Various cues can facilitate theinference of informational relations, even if it is wellknown that they are based solely on the content ofthe text segments, various cues can facilitate the in-ference of such relations.
At the same time, it iswell known that relations are often non signalled:in our corpus, only 43% of relations are signalled,consistently with figures from the literature (44%in (Williams and Reiter, 2003) and 45% in (Prasadet.
al., 2008)).
Besides lexical cues such as but,and and if, we also include modals, tense, compara-tives and superlatives, and negation.
E.g., wrong-actin relations like prescribe-act:wrong-act is often ex-pressed using a negation.Similarity: For the two segments in question, wecompute the cosine similarity of the segments usingonly nouns and verbs.Compositional semantics: the semantic infor-mation derived by our parser, as described in Sec-tion 2.1.
The semantic representation of segments5e2 from Example (1) is shown in Figure 2.
Eachsemantic predicate is a feature for the classifier.Structural Information: For relations betweentwo EDUs, we use knowledge of whether the twoEDUs are intra-sentential or inter-sentential, sincesome relations, e.g.
criterion:act, are more likely tobe realized intra-sententially than inter-sententially.For larger segments, we also encode the hierarchi-cal representation of text segments that contain morethan one nucleus, the distance between the nucleiof the two segments and any relations that exist be-tween the smaller inner segments.At this point, the attentive reader will be wonder-ing how we encode compositional semantics for re-lations relating text segments larger than one EDU.Clearly we cannot just list the semantics of eachEDU that is dominated by the larger segment.
Wefollow the intuition that nuclei represent the mostimportant portions of segments (Mann and Thomp-son, 1988).
For segments such as s5e1-s5e2 thatcontains a single nucleus, we simply reduce the se-mantic content of the larger segment to that of itsnucleus:s5e1-s5e2verb(?s5e1-s5e2?,mark)....verbclass(?s5e1-s5e2?,..).agent(?s5e1-s5e2?,..
).In this case, the semantics of the complex text seg-ment is represented by the compositional semanticsof the single most important EDU.For segments that contain more than one nu-cleus, such as s3e1-s3e2, the discourse struc-ture information of the segment is represented withthe additional predicates internal relation and par-ent segment.
These predicates can be used recur-sively at every level of the tree to specify the relationbetween the most important segments.
In addition,they also provide a means to represent the compo-sitional semantics of the most important EDUs and569make them available to the relational learning algo-rithm.s3e1-s3e2internal relation(s3e1,s3e2,?disjunction?
).parent segment(s3e1-s3e2,s3e1).parent segment(s3e1-s3e2,s3e2).LLLLLLLLLLLLLLLLLLLLLrrrrrrrrrrrrrrrrrrrrrverb(?s3e1?,mark).noun(?s3e1?,opening)....verbclass(?s3e1?,..).theme(?s3e1?,..
).verb(?s3e2?,cut).noun(?s3e2?,opening)....noun(?s3e1?,blade).4.1 Learning FOL Rules for Discourse ParsingIn Aleph, the hypothesis space is restricted to a set ofrules that conform to a predefined language L. Thisis done with the use of mode declarations which, inother words, introduces a language bias in the learn-ing process.
modeh declarations inform the learningalgorithm about what predicates to use as the headof the rule and modeb specifies what predicates touse in the body of the rule.
Not all the informationin B needs to be included in the body of the rule.This makes sense since we often learn definitions ofconcepts based on more abstract higher level infor-mation that is inferred from some other informationthat is not part of our final definition.
Mode decla-rations are used by Aleph to build the most specificclause (?)
that can be learned for each example.
?constrains the search for suitable hypotheses.
?i isbuilt by taking an example ei ?
E+ and adding lit-erals that are entailed by B and ei.
We then have thefollowing property, whereHi is the hypothesis (rule)we are trying to learn and is a generality operator:  Hi  ?iFinding the most specific clause (?)
provides uswith a partially ordered set of clauses from which tochoose the best hypothesis based on some quantifi-able qualitative criteria.
This sub-lattice is boundedby the most general clause (, the empty clause)from the top and the most specific clause (?)
at thebottom.
We use the heuristic search in Aleph that issimilar to the A*-like search strategy presented by(Muggleton, 1995) to find the best hypothesis (rule).A noise threshold on the number of negative exam-ples that can be covered by a rule can be set.
Welearn a model that learns perfect rules first and thenone that allows for at most 5 negative examples.
Abackoff model that first uses the model trained withnoise = 0 and then noise = 5 if no classificationhas been made is used.
We use the evaluation func-tion in Equation 1 to guide our search through thetree of possible hypotheses.
This evaluation func-tion is also called the compression function since itprefers simpler explanations to more complex ones(Occam?s Razor).
fs is the score for clause cs thatis being evaluated, ps is the number of positive ex-amples, ns is the number of negative examples, ls isthe length of the clause (measured by the number ofclauses).fs = ps ?
(ns + (0.1?
ls)) (1)Classification in most ILP systems, includingAleph, is restricted to binary classification (positivevs.
negative).
In many applications with just twoclasses, this is sufficient.
However, we are facedwith a multi-classification problem.
In order to per-form multi-class classification, we use a decisionlist.
First, we build m binary classifiers for eachrelation r ?
R. Then, we form an ordered list of therules based on the following criterion:1.
Given two rules ri and rj , ri ,is ranked higherthan rj if (pi ?
ni) > (pj ?
nj).2. if (pi?ni) = (pj ?nj), then ri is ranked higherthan rj if ( pipi+ni ) > (pjpj+nj ).3. if (pi ?
ni) = (pj ?
nj) and ( pipi+ni ) = (pjpj+nj )then ri is ranked higher than rj if (li) > (lj).4. default: random orderClassifying an unseen example is done by usingthe first rule in the ordered list that satisfies it.5 Experiments and ResultsWe report our results from experiments on both theclassification task and the discourse parsing task.5.1 Relation Classification ResultsFor the classification task, we conducted exper-iments using the stratified k-fold (k = 5) cross-validation evaluation technique on our data.
Unlike570(Wellner et.
al., 2006; Sporleder and Lascarides,2005), we do not assume that we know the orderof the relation in question.
Instead we treat reversalsof non-commutative relations (e.g.
preparation:actand act:goal) as separate relations as well.
Wecompare our ILP model to RIPPER, Naive Bayesand the Decision Tree algorithm.
We should pointout that since attribute-value learning models can-not handle first-order logic data, they have been pre-sented with features that lose at least some of thisinformation.
While this may then seem to result inan unfair comparison, to the contrary, this is pre-cisely the point: can we do better than very effec-tive attribute-value approaches that however inher-ently cannot take richer information into account?All the statistical significance tests were performedusing the value of F-Score obtained from each of thefolds.
We report performance on two sets of datasince we were not able to obtain compositional se-mantic data for all the EDUs in our corpus:?
Set A: Examples for which semantic data wasavailable for all the nuclei of the segments(1789 total).
This allows us to have a betteridea of how much impact semantic data has onthe performance, if any.?
Set B: All examples regardless of whether ornot semantic data was available for the nucleiof the segments (5475 total).Model Semantics No SemanticsILP 62.78 60.25Decision Tree 56.29 55.45RIPPER 58.02 56.96Naive Bayes 35.83 34.66Majority Class 31.63 31.63Table 1: Classification Performance: Set A (F-Score)Table 1 shows the results on Set A. ILP outper-forms all the other models.
Via ANOVA, we firstconclude that there is a statistically significant differ-ence between the 8 models (p < 2.2e?16).
To thenpinpoint where the difference precisely lies, pair-wise comparisons using Student?s t-test show thatthe difference between ILP (using semantics) and allof the other learning models is statistically signifi-cant at p < 0.05.
Additionally, ILP with semanticsis significantly better than ILP without it (p < 0.05).For Decision Tree, Naive Bayes and RIPPER, theimprovement in using semantics is not statisticallysignificant.Model Semantics No SemanticsILP 59.43 59.22Decision Tree 53.84 53.69RIPPER 51.1 51.36Naive Bayes 49.69 51.62Majority Class 22.01 22.01Table 2: Classification Performance: Set B (F-Score)In Table 2, we list the results on Set B. Onceagain, our ILP model outperforms the other threelearning models.
Naive Bayes is much more com-petitive when using all the examples compared tousing only examples with semantic data.
In the caseof the attribute-value machine learning models, theuse of semantic data seems to marginally hurt theperformance of the classifiers.
However, this is incontrast to the relational ILP model which alwaysperforms better when using semantics.
This resultsuggests that the use of semantic data with loss of in-formation may not be helpful, and in fact, it may ac-tually hurt performance.
Based on ANOVA, the dif-ferences in these 8 models is statistically significantwith p < 6.95e?12.
A pairwise t-test between ILP(using semantics) and each of the other attribute-value learning models shows that our results are sta-tistically significant at p < 0.05.In Table 3, we report the performance of the twoILP models on each relation.3 In general, the modelsperform better on relations that have the most exam-ples.The evaluation of work in discourse parsing ishindered by the lack of a standard corpus or task.Hence, our results cannot be directly comparedto (Marcu, 2000; Sporleder and Lascarides, 2005;Wellner et.
al., 2006), but neither can those worksbe compared among themselves, because of differ-ences in underlying corpora, the type and number ofrelations used, and various assumptions.
However,we can still draw some general comparisons.
OurILP-based models provide as much or significantly3Due to space limitations, only relations with> 10 examplesare shown.571relation Semantics No Semanticspreparation:act 74.86 72.05general:specific 31.74 28.24joint 55.23 52act:goal 86.12 83.85criterion:act 77.37 75.32goal:act 73.43 68.9step1:step2 28.75 35.29co-temp1:co-temp2 48.84 37.84disjunction 83.33 80.81act:criterion 54.29 54.79contrast1:contrast2 22.22 5.0act:preparation 65.31 70.59act:reason 0 10.26cause:effect 19.05 10.53comparison 22.22 10.53Table 3: Classification Performance (F-Score) byRelation: ILP on Set Amore improvement over a majority-class baselinewhen compared to these other works.
This is thecase even though our work is based on less trainingdata, relatively more relations, relations both be-tween just two EDUs and those involving larger textsegments, and we make no assumptions about theorder of the relations.
Our results are comparable to(Marcu, 2000), which reports an accuracy of about61% for his classifier.
His majority class baselineperforms at about 50% accuracy.
(Wellner et.
al.,2006) reports an accuracy of up to 81%, with a ma-jority class baseline performance of 45.7%.
How-ever, our task is more challenging than (Wellner et.al., 2006).
They use only 11 relations compared tothe 26 we use.
They also assume the order of therelation in the examples (i.e.
examples for goal:actwould be treated as examples for act:goal by revers-ing the order of the arguments) whereas we do notmake such assumptions.
In addition, their trainingdata is almost twice as large as ours, based on re-lation instances.
(Sporleder and Lascarides, 2005)also makes the same assumption on the ordering ofthe relations as (Wellner et.
al., 2006).
They re-port an accuracy of 57.75%.
Their work, though,was based on only 5 relations.
Importantly, neither(Wellner et.
al., 2006; Sporleder and Lascarides,2005) model examples with complex text segmentswith more than one EDU.5.2 How interesting are the rules?Given that our ILP models learn first-order logicrules, we can make some qualitative analysis of therules learned, such as those below, learnt by the ILPmodel that uses semantics:(2a) relation(A,B,?act:goal?)
:-firstWordPOS(A,?VBG?),verbclass(A,D,?use-1?),firstWordPOS(B,?VB?).
[pos cover = 23 neg cover = 1](2b) relation(A,B,?preparation:act?)
:-discourse cue(B,front,and),cause(A,frame(C),D,E),theme(B,frame(F),G), theme(A,frame(C),G).
[pos cover = 12 neg cover = 0](2c) relation(A,B,?preparation:act?)
:-discourse cue(B,front,then),parent segment(A,C), parent segment(A,D),internal relation(C,D,?preparation:act?).
[pos cover = 17 neg cover = 0](2a) is learned using examples such asrelation(s5e1,s5e2,?act:goal?)
from Example (1).
(2b) uses relational semantic information.
This rulecan be read as follows:IF segment A contains a cause and atheme, the same object that is the themein A is also the theme in segment B, and Bcontains the discourse cue and at the frontTHEN the relation between A and B ispreparation:act.
(2c) is a rule that makes use of the structural in-formation about complex text segments.
When us-ing Set A, more than about 60% of the rules in-duced include at least one semantic predicate in itsbody.
They occur more frequently in rules for re-lations like preparation:act while less in rules forgeneral:specific and act:goal.5.3 Discourse Parsing ResultsIn order to test our discourse parser, we used 151documents for training and 25 for testing.
We eval-uated the performance of our parser on both thediscourse parse trees it builds at the sentence leveland at the document level.
The test set contained572Sentence Level Document Levelmodel Semantics span nuclearity relation span nuclearity relationSR-ILP yes 92.91 71.83 63.06 70.35 49.47 35.44SR-ILP no 91.98 69.59 58.58 68.95 48.16 33.33Baseline - 93.66 74.44 34.32 70.26 47.98 22.46Table 4: Parsing Performance (F-Score): (Baseline = right-branching majority)341 sentences out of which 180 sentences were seg-mented into more than one EDU.
We ran experi-ments using our two ILP models for the relationidentifier, namely ILP with semantics and withoutsemantics.
Our ILP based discourse parsing modelsare named SR-ILP.
We compare the performance ofour models against a right branching majority classbaseline.
We used the sign-test to determine statis-tical significance of the results.
Using the automaticevaluation methodology in (Marcu, 2000), preci-sion, recall and F-Score measures are computed fordetermining the hierarchical spans, nucleus-satelliteassignments and rhetorical relations.
The perfor-mance on labeling relations is the most importantmeasure since the results on nuclearity and hierar-chical spans are by-products of the decisions madeto attach segments based on relations.On labeling relations, the parser that uses all thefeatures (including compositional semantics) for de-termining relations performs the best with an F-Score of 63.06%.
The difference of about 4.5% (be-tween ILP with semantics and without semantics)in F-Score is statistically significant at p = 0.006.Our best model, SR-ILP (using semantics) beats thebaseline by about 28% in F-Score.
Since the task atthe document level is much more challenging thanbuilding the discourse structure at the sentence level,we were not surprised to see a considerable drop inperformance.
For our best model, the performanceon labeling relations drops to 35.44%.
Clearly, themistakes made when attaching segments at lowerlevels have quite an adverse effect on the overallperformance.
A less greedy approach to parsing dis-course structure is warranted.While we would have hoped for a better perfor-mance than 35.44%, to start with, (Forbes et.
al.,2001), (Polanyi et.
al., 2004), and (Cristea, 2000) donot report the performance of their discourse parsersat all.
(Marcu, 2000) reports precision and recall ofup to 63.2% and 59.8% on labeling relations usingmanually segmented EDUs on three WSJ articles.
(Baldridge and Lascarides, 2005) reports 43.2% F-Score on parsing 10 dialogues using a probabilistichead-driven parsing model.6 ConclusionsIn conclusion, we have presented a relational ap-proach for classifying informational relations and amodified shift-reduce parsing algorithm for buildingdiscourse parse trees based on informational rela-tions.
To our knowledge, this is the first attemptat using a relational learning model for the task ofrelation classification, or even discourse parsing ingeneral.
Our approach is linguistically motivated.Using ILP, we are able to account for rich composi-tional semantic data of the EDUs based on VerbNetas well as the structural relational properties of thetext segments.
This is not possible using attribute-value based models like Decision Trees and RIP-PER and definitely not using probabilistic modelslike Naive Bayes.
Our experiments have shown thatsemantics can be useful in classifying informationalrelations.
For parsing, our modified shift-reduce al-gorithm using the ILP relation classifier outperformsa right-branching baseline model significantly.
Us-ing semantics for parsing also yields a statisticallysignificant improvement.
Our approach is also do-main independent as the underlying model and dataare not domain specific.AcknowledgmentsThis work is supported by the National Science Founda-tion (IIS-0133123 and ALT-0536968) and the Office ofNaval Research (N000140010640).573ReferencesAsher, N., and Lascarides, A.: Logics of Conversation.Cambridge University Press, 2003.Baldridge, J. and Lascarides, A.: Probabilistic Head-Driven Parsing for Discourse Structure In Proceed-ings of the Ninth Conference on Computational Natu-ral Language Learning (CoNNL), Ann Arbor, 2005.Brill, E.: Transformation-based error-driven learningand natural language processing: A case study inpart-of-speech tagging.
Computational Linguistics,21(4):543565, 1995.Buitelaar, P.: CoreLex: Systematic Polysemy and Un-derspecification.
Ph.D. Thesis, Brandies University,1998.Carlson, L. D. M. and Okurowski., M. E.: Building adiscourse-tagged corpus in the framework of rhetoricalstructure theory.
In Current Directions in Discourseand Dialogue pages 85?112, 2003.Cristea, D.: An Incremental Discourse Parser Architec-ture.
In D. Christodoulakis (Ed.)
Proceedings of theSecond International Conference - Natural LanguageProcessing - Patras, Greece, June 2000.Forbes, K., Miltsakaki, E., R. P. A. S. A. J. and Web-ber., B.: D-ltag system - discourse parsing with a lexi-calized tree adjoining grammar.
Information Stucture,Discourse Structure and Discourse Semantics, ESS-LLI 2001.Grosz, B. J. and Sidner, C. L.: Attention, intention andthe structure of discourse.
Computational Linguistics12:175?204, 1988.Hobbs, J. R.: On the coherence and structure of dis-course.
In Polyani, Livia editor, The Structure of Dis-course, 1985.Kipper, K., H. T. D. and Palmer., M.: Class-based con-struction of a verb lexicon.
AAAI-2000, Proceedingsof the Seventeenth National Conference on ArtificialIntelligence, 2000.Mann, W. and Thompson, S.: Rhetorical structure the-ory: Toward a functional theory of text organization.Text, 8(3):243?281, 1988.Marcu, D.: Instructions for Manually Annotating theDiscourse Structures of Texts.
Technical Report, Uni-versity of Southern California, 1999.Marcu, D.: The theory and practice of discourse parsingand summarization.
Cambridge, Massachusetts, Lon-don, England, MIT Press, 2000.Moser, M. G., Moore, J. D., and Glendening, E.: In-structions for Coding Explanations: Identifying Seg-ments, Relations and Minimal Units.
University ofPittsburgh, Department of Computer Science, 1996.Muggleton, S. H.: Inverse entailment and progol.In New Generation Computing Journal 13:245?286,1995.Polanyi, L., Culy, C., van den Berg, M. H. and Thione,G.
L.: A Rule Based Approach to Discourse Pars-ing.
Proceedings of the 5th SIGdial Workshop in Dis-course And Dialogue.
Cambridge, MA USA pp.
108-117., May 1, 2004.Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo,L., Joshi, A., and Webber, B.: The Penn DiscourseTreebank 2.0.
LREC, 2008.Rose?, C. P.: A Syntactic Framework for Semantic In-terpretation, Proceedings of the ESSLLI Workshopon Linguistic Theory and Grammar Implementation,2000.Sporleder, C. and Lascarides., A.: Exploiting linguisticcues to classify rhetorical relations.
Recent Advancesin Natural Language Processing, 2005.Soricut, R. and Marcu., D.: Sentence level discourseparsing using syntactic and lexical information.
Pro-ceedings of the Human Language Technology andNorth American Assiciation for Computational Lin-guistics Conference, 2003.Subba, R., Di Eugenio, B., E. T.: Building lexical re-sources for princpar, a large coverage parser that gen-erates principled semantic representations.
LREC,2006.Subba, R.: Discourse Parsing: A Relational Learn-ing Approach Ph.D. Thesis, University of IllinoisChicago, December 2008.Webber, B.: DLTAG: Extending Lexicalized TAG to Dis-course.
Cognitive Science 28:751-779, 2004.Wellner, B., Pustejovsky, J., C. H. R. S. and Rumshisky.,A.
: Classification of discourse coherence rela-tions: An exploratory study using multiple knowledgesources.
In Proceedings of the 7th SIGDIAL Work-shop on Discourse and Dialogue, 2006.Williams, S. and Reiter, E.: A corpus analysis of dis-course relations for natural language generation.
Pro-ceedings of Corpus Linguistics, pages 899?908, 2003.Wolf, F. and Gibson, E.: Representing discourse coher-ence: A corpus-based analysis.
Computational Lin-guistics 31(2):249?287, 2005.574
