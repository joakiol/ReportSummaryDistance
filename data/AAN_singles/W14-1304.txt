Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 26?34,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsThe (Un)Predictability of Emotional Hashtags in TwitterFlorian Kunneman?, Christine Liebrecht?
?, and Antal van den Bosch?
?Centre for Language Studies?
?Faculty of Social and Behavioral SciencesRadboud University Nijmegen University of Amsterdam{f.kunneman,a.vandenbosch}@let.ru.nlc.c.liebrecht@uva.nlAbstractHashtags in Twitter posts may carry dif-ferent semantic payloads.
Their dual form(word and label) may serve to categorizethe tweet, but may also add content to themessage, or strengthen it.
Some hash-tags are related to emotions.
In a studyon emotional hashtags in Dutch Twitterposts we employ machine learning classi-fiers to test to what extent tweets that arestripped from their hashtag could be re-assigned to this hashtag.
About half of the24 tested hashtags can be predicted withAUC scores of .80 or higher.
However,when we apply the three best-performingclassifiers to unseen tweets that do notcarry the hashtag but might have carriedit according to human annotators, the clas-sifiers manage to attain a precision-at-250of .7 for only two of the hashtags.
We ob-serve that some hashtags are predictablefrom their tweets, and strengthen the emo-tion already expressed in the tweets.
Otherhashtags are added to messages that do notpredict them, presumably to provide emo-tional information that was not yet in thetweet.1 IntroductionSince the launch of Twitter in 2006 the microblog-ging service has proven to be a valuable source ofresearch on the linguistic expression of sentimentand affect.
Sentiments and emotions are impor-tant aspects of status updates and conversations inTwitter messages (Ritter et al., 2010; Dann, 2010).Many Twitter messages (tweets) express an emo-tion of the sender: according to Roberts et al.
(2012), 43 percent of the 7,000 tweets they col-lected are an emotional expression.
Automaticallydetecting the emotion in tweets is key to under-stand the sentiment underlying real world eventsand topics.Potentially, Twitter offers a vast amount of datato exploit for the construction of computationalmodels able to detect certain sentiments or emo-tions in unseen tweets.
Yet, in the typical scenarioof applying supervised machine learning classi-fiers, some annotation effort will be required tolabel sentiments and emotions reliably.
Currentlythere are two main approaches to labeling tweets.The first is the annotation of data by human ex-perts (Alm et al., 2005; Aman and Szpakowicz,2007).
This approach is known to result in high-precision annotated data, but is labor-intensive andtime-consuming.The second approach is to use the annotationsthat Twitter users themselves add to a tweet: hash-tags.
A hashtag (a word prefixed by the typograph-ical hashmark #) is an explicitly marked keywordthat may also serve as a word in the context ofthe other non-tagged words of the post.
The us-age of a hashtag in Twitter serves many purposesbeyond mere categorization, most of which areconversational (Huang et al., 2010).
Hashtags ex-pressing emotions are often used in tweets and aretherefore potentially useful annotations for train-ing data.
Wang et al.
(2012) state that annotatinginterpretative labels by humans other than the au-thor is not as reliable as having the data annotatedby the author himself.
As far as emotions can beself-observed and self-reported, authors arguablyhave the best information about their own emo-tions.
Following Gonz?alez-Ib?a?nez et al.
(2011),Mohammad (2012) presents several experimentsto validate that the emotional labels in tweets areconsistent and match intuitions of trained judges.Therefore, using hashtags as annotated trainingdata may be useful for generating emotion detec-tors.
Yet, not all hashtags are equally suitable forthis task.
Even a high level of consistency and pre-dictability in hashtag usage might not be sufficient.26Mohammad (2012) argues that emotion hashtagsare included in tweets by users in two differentways.
First, the hashtag can strengthen the emo-tion already present in the tweet.
By adding thehashtag in for example ?I hate making homework#fml?
(#fml is an acronym for ?fuck my life?
), thesender reflects on his own negative message andstrengthens it with an abbreviated expletive.Second, the hashtag can add emotion to themessage in order to avoid miscommunication.Lacking the richness of non-verbal cues in face-to-face communication, as well as the space to elab-orate, attenuate, or add nuance, users of Twittermight deploy hashtags to signify the intention oremotion of their message.
In the expression ?Mak-ing homework #fml?
for example, a Twitter useradds sentiment to the message to clarify his nega-tive attitude towards the described activity.
Mo-hammad (2012, p. 248) formulates the secondfunction of a hashtag as follows: ?reading just themessage before the hashtag does not convey theemotions of the tweeter.
Here, the hashtag pro-vides information not present (implicitly or explic-itly) in the rest of the message.
?Arguably, hashtags that are most often used toadd emotion to an otherwise emotionally neutralmessage (the second function) will not provideproper training data for the detection of the emo-tion linked to the hashtag; only examples of thefirst function may serve that purpose.
As this in-formation is not explicit, the suitability of a hash-tag as an emotion label needs to be revealed inanother way.
We propose an automatic methodthat uses machine-learning-based text classifica-tion.
We put this method into practice for anumber of hashtags expressing emotion in Dutchtweets.
The novel contribution of this research liesin the fact that we offer an objective, empiricalhandle of the two usages of emotion hashtags asformulated by Mohammad (2012).
Furthermore,we exemplify a new type of study that tests ourhypothesis in the realistic scenario of testing on afull day of streaming tweets with no filtering.2 Related researchLeveraging uncontrolled labeling to obtain largeamounts of training data is referred to as distantsupervision (Mintz et al., 2009).
With its con-ventions for hashtags as extra-linguistic markers,Twitter is a potentially suitable platform for im-plementing classification based on distant super-vision.
In the field of sentiment analysis, Pakand Paroubek (2010) and Go, Bhayani and Huang(2009) select emoticons representing positive andnegative sentiment to collect tweets with either ofthe polarities.
Several studies focusing on the spe-cific task of emotion detection in Twitter also ap-ply distant supervision.
The studies in which itis applied vary in a number of ways.
First, thetype of markers by which data is collected dif-fers.
Most often only hashtags are used, occa-sionally combined with emoticons.
Davidov, Tsurand Rappoport (2010) use hashtags and emoticonsas distinct prediction labels and find that they areequally useful.
Suttles and Ide (2013) compare theusage of hashtags, emoticons, and emoji1, and findthat emoji form a valuable addition.Second, the selection of emotions and mark-ers differs.
In many of the studies a predefinedset of emotions form the starting point for the se-lection of markers and collection of data.
Emo-tions can be classified according to a set of basicemotions, such as Ekman?s (Ekman, 1971) six ba-sic emotions (happiness, sadness, anger, fear, sur-prise, and disgust), or the bipolar emotions definedby Plutchik?s wheel of emotions (Plutchik, 1980)which are based on the basic emotions anger, fear,sadness, disgust, surprise, anticipation, trust, andjoy.
The majority of the studies rely on such cat-egorizations (Mohammad, 2012; Suttles and Ide,2013; Wang et al., 2012).In spite of the interesting findings in such stud-ies, basic emotions do not tell the whole story;tweets may contain multiple basic emotions com-bining into more complex emotions (Roberts et al.,2012; Kamvar and Harris, 2011).
Furthermore,by selecting a set of hashtags that are assumed tomatch the same emotion, the potential variation inthe usage of specific hashtags by users is ignored.A different approach is to select single hashtagsexpressing emotion as starting points, regardlessof their theoretical status.
Davidov et.al.
(2010)select frequent hashtags from a large twitter cor-pus and let annotators judge the strength of theirsentiment.
The fifty hashtags with the strongestsentiment are used as label.
In our research, wealso single out hashtags, focusing on a set of hash-tags that are linked to emotions, some of which arecomplex.Third, the way in which a classifier is trainedand tested differs.
In some studies multi-class1http://en.wikipedia.org/wiki/Emoji27classification is performed, distinguishing the dif-ferent target emotions and optionally an emotion-ally neutral class (Purver and Battersby, 2012;Wang et al., 2012).
The multitude of classes, classimbalance, and the possibility of single tweetsconveying multiple emotions make this a challeng-ing task.
The alternative is to train a binary clas-sifier for each emotion (Mohammad, 2012; Qadirand Riloff, 2013; Suttles and Ide, 2013), decid-ing for each unseen tweet whether it conveys thetrained emotion.
We apply the latter type of clas-sification.The fourth and final variation is the way inwhich classification is evaluated.
In the discussedpapers, evaluation is either performed in a ten-foldcross-validation setting or by testing the trainedclassifier on a small, manually annotated set oftweets.
We deviate from these approaches by test-ing our classifiers on a large set of uncontrolledtweets gathered in a single day, thereby approx-imating the real world scenario in which emo-tion detection is applied to the stream of incomingtweets.3 ApproachOur approach is to train a machine learning classi-fier on tweets containing an emotion-bearing hash-tag and an equal amount of random tweets ascounter-examples, resulting in a balanced binaryclassifier for the hashtag (which itself is strippedfrom the tweet and purely considered as a label).The classifier is then run on a large sample oftweets, deciding which of the tweets might fit thetarget hashtag.
As some of these test tweets ac-tually contain the hashtag, a first evaluation is toscore the amount of tweets of which the hashtagis correctly predicted by the classifier, when thishashtag is hidden from the classifier.
Second, thetweets not containing the hashtag can be rankedby classifier confidence for the hashtag class, af-ter which the 250 highest ranked tweets are scoredby human annotators, who judge whether thesetweets convey the emotion that is linked to thehashtag.This approach is based on the assumption thata hashtag as a label for emotion detection requirestwo relations between the hashtag and the text withwhich it co-occurs in tweets:1.
The context in which users include the hash-tag is to a certain extent consistent with thehashtag.
In other words, the context (thetweet) would predict the hashtag.
If this isthe case, our classifier should score well onthe retrieval of unseen tweets containing thehashtag (the first evaluation).
Consistencycan arise from many different types of fea-tures, ranging from topical words to emotion-bearing words.2.
The emotion that is denoted by the hashtagshould be reflected in the words surroundingit.
Hashtags that add emotion to an otherwiseneutral message are inappropriate as annota-tion label for emotion detection.
By evalu-ating retrieved tweets that do not contain thehashtag on the conveyed emotion (instead oftheir possible fit with the hashtag) we canscore to what extent the classifier trained amodel of the emotion in tweets successfully.Note that hashtags that add a specific emotion tootherwise unemotional tweets are good indicatorsthemselves for detecting emotion in Twitter.
Ourgoal, however, is to create generalizable modelsof emotion in Twitter that are not restricted to theoccurrence of a hashtag.4 Experimental setup4.1 Data collectionAs a starting point of our experiments we se-lected 24 hashtags used in Dutch tweets.
The se-lection was inspired on a list of the 2,500 mostfrequent hashtags in 2011 and 2012, generatedfrom twiqs.nl, a database of Dutch tweets fromDecember 2010 onwards (Tjong Kim Sang andvan den Bosch, 2013).
Typically, emotion hash-tags are not linked to any specific point in time,and therefore surface in such a list generated froman extended period of tweets.To create the training data, tweets contain-ing any of the hashtags were collected throughtwiqs.nl from the time frame of December2010 up and until January 2013.We queried a large sample of Dutch tweets(3,144,781) posted on February 1st 2013, a smallportion of which was used as negative examplesfor our training data, and the rest was used as testdata.4.2 ClassificationFor each of the hashtags, training data was gener-ated by balancing the amount of collected tweetscontaining the hashtag with an equal amount of28randomly selected tweets (not containing the hash-tag) drawn from the set of tweets collected onFebruary 1st, 2013.
The resulting binary classi-fier was tested on the remainder of tweets in thisset.The tweets were pre-processed by extractingword unigrams, bigrams, and trigrams as features.We maintained capitalization and included punc-tuation and emoticons as tokens in the n-grams, aswe expected such tokens to have predictive powerin the context of emotions.
Both usernames andURLs were normalized to dummy values.
All fea-tures containing a target hashtag were removed.Classification was performed by the BalancedWinnow algorithm (Littlestone, 1988).
This algo-rithm is known to offer state-of-the-art results intext classification, and produces interpretable per-class weights that can be used to, for example, in-spect the highest-ranking features for one class la-bel.
The ?
and ?
parameters were set to 1,05 and0,95 respectively.
The major threshold (?+) andthe minor threshold (??)
were set to 2,5 and 0,5.The number of iterations was bounded to a maxi-mum of three.4.3 EvaluationPerformance was evaluated by classifying all testtweets and counting the number of tweets withthe target hashtag that were positively classifiedas such, deriving a true positive rate (recall), falsepositive rate, and area under the curve (AUC)score (Fawcett, 2004).While this first evaluation gives an indicationof the predictability of any hashtag, the ultimatevalue of a hashtag for emotion detection can bescored by assessing the emotion in positively clas-sified tweets that do not contain the hashtag.
Thisis done by manually annotating the fraction ofthese tweets that are most confidently positivelyranked by the hashtag classifier, as containing theemotion signalled by the hashtag.
Three annota-tors inspected the top-250 of these rankings.5 Results5.1 Hashtag predictabilityThe results of our classifiers labeling a large sam-ple of tweets posted on February 1, 2013 are listedin Table 1.
Each line with a target hashtag repre-sents a separate experiment.
The amount of train-ing tweets ranges from 19 thousand to 677 thou-sand for the target hashtag (balanced by an equalamount of random tweets as negative category).The results are sorted by the AUC score.In this first evaluation our attention focuses onthe tweets that have one of the target hashtags.The hashtags themselves are removed at classifica-tion time, as our goal is to measure how well ourclassifiers are able to detect these ?hidden?
tags.In this particular stream of tweets, only a limitednumber of tweets occur that are labeled with ourhashtags; the most frequent tag #zinin (?lookingforward to it?)
occurs 1,328 times.
Taking #zininas example, the #zinin classifier labels 158,429 ofthe test tweets as likely candidates for the hashtag#zinin.
Although this is a substantial overpredic-tion, partly caused by the 50%-50% ratio betweenpositive and negative cases in the training set, thisstill amounts to a false positive rate of only 6%.More importantly, of the 1,328 cases for which itshould have predicted #zinin, the classifier labels1,186 cases correctly, attaining a true positive rateof 89%.
The area under the curve (AUC) in truepositive rate?false positive rate space is 91%.Inspecting the performance for all 24 hashtagswe observe that about half of the hashtags obtainan AUC of .80 or more.
The influence of theamount of training data on the AUC score seemsperipheral.
Furthermore, there is no clear differ-ence in the predictability of hashtags denoting apositive or negative emotion.
The predominantlynegative hashtags #geenzin, #fml, #balen and #ni-etleuk obtain a high AUC, while the other nega-tive hashtags #grr, #bah and #stom are not as pre-dictable.
There does not seem to be an a prioriproperty that makes a hashtag more or less pre-dictable, indicating the need for experimentationto confirm the usefulness of a hashtag for emotiondetection.Interestingly, some pairs of synonymous hash-tags (#jippie-#joepie, #wauw?#wow, #yes?#yeah,homophonous variants of the same exclama-tion) and antonymous hashtags (#zinin?#geenzin,#fml?#lml) achieve similar AUC scores.
This out-come supports the validity of our approach.
Syn-onymous and antonymous hashtags are employedin similar contexts and should therefore have asimilar predictability.
This is indeed confirmed byour results.
There are counterexamples, however.The pair #yay?#jeej exhibits dissimilar scores.
Inthe case of #leuk there are two antonyms: #ni-etleuk and #stom.
#leuk and #nietleuk have a dis-similar score, while #leuk and #stom are rather29Target hashtag Gloss # Training Target instances Instances Instances TPR FPR AUCtweets on test day classified correct#zinin looking forward to it 677,156 1,328 158,429 1,186 0.89 0.06 0.91#geenzin not looking forward to it 427,602 653 231,463 583 0.89 0.08 0.91#fml fuck my life 139,044 308 126,045 265 0.86 0.05 0.90#lml love my life 41,031 197 343,936 167 0.85 0.11 0.87#balen bummer 219,342 134 271,308 108 0.81 0.09 0.86#jeej yay 107,667 31 353,807 25 0.81 0.12 0.85#nietleuk not nice 85,825 43 359,709 33 0.77 0.12 0.83#yeah yeah 290,288 328 349,598 247 0.75 0.12 0.82#loveit love it 259,935 336 290,822 247 0.74 0.10 0.82#jippie yippie 66,992 27 396,805 21 0.78 0.13 0.82#joepie yippie 53,217 39 422,348 29 0.74 0.14 0.80#yes yes 115,707 151 373,874 104 0.69 0.12 0.78#yay yay 50,737 45 421,660 31 0.69 0.14 0.78#hmm hmm 110,171 95 341,936 63 0.66 0.11 0.78#grr argh 70,659 145 397,201 97 0.67 0.13 0.77#like like 68,499 284 412,714 178 0.63 0.13 0.75#woehoe woohoo 19,236 32 584,552 22 0.69 0.19 0.75#leuk nice 391,626 971 307,277 592 0.61 0.11 0.75#bah grose 298,842 228 273,454 127 0.56 0.10 0.73#stom lame 72,957 99 355,731 57 0.58 0.12 0.73#omg oh my god 590,560 145 394,447 79 0.54 0.13 0.71#wauw wow 146,145 103 467,503 58 0.56 0.15 0.70#wow wow 52,488 50 587,662 29 0.58 0.19 0.70#huh huh 48,456 25 352,396 12 0.48 0.11 0.68Table 1: Results for the prediction of a target hashtag for about 3,1 million Dutch tweets posted onFebruary 1st 2013 (TPR = True Positive Rate, FPR = False Positive Rate, AUC = Area Under the ROCCurvesimilar.5.2 Emotion detectionThe second evaluation is based on the manual an-notation of the 250 tweets most positively rankedby a hashtag classifier, on the emotion linked tothe target hashtag.
Due to the labour-intensive na-ture of this evaluation, it was not possible to ana-lyze all 24 hashtags.
We focused on the output for#zinin, #geenzin, #fml and #omg.
The first threeachieved the highest true positive rates ranging be-tween 86% and 89%, and AUC scores of 90% to91%.
The latter was included as a comparison, ex-pecting a poor emotion detection in view of its badpredictability.For these four hashtags the 250 ?false positives?of which the classifier was most certain were an-notated by the three authors by taking the binarydecision whether a tweet conveyes the emotionpresumed in tweets containing the hashtag.
Theemotions most strongly linked to the four hashtagswere the following:?
#zinin: conveying anticipatory excitement;?
#geenzin: conveying uneagerness?
#fml: conveying self pity?
#omg: conveying an aroused level of indig-nation, fear, or excitementNote that #omg is not linked to a single emo-tion, but rather strengthens several sorts of emo-tions.
This might have been a hampering factorfor its predictability.
In the annotation for #omgwe focused on all three emotions.Table 2 displays the precision scores when tak-ing a simple majority decision over the three anno-tators (67% majority) and when only counting thecases in which all three annotators agreed (100%majority).
The outcomes show reasonably highprecision levels for #zinin (75%) and #fml (69%)along with equally reasonable mutual F-scores be-tween the annotators (67% for #zinin and 81% for#fml), although Cohen?s Kappa is rather low insome cases.
On the other hand, #geenzin lags be-hind with a majority precision of 31%.
Also thetop 250 for #omg does not often display any of thethree most strongly linked emotions.Plotting the annotations of the ranked tweets inprecision-at curves, shown in Figure 1, providesfurther insight into the emotion detection qualityin relation to the confidence ranks.
Precisions athigher rank cutoffs tend to peak early (indicatingthat the first top-ranked tweets fit the hashtag best),and decrease slowly or reach a plateau.30Precision Cohen?s Kappa Mutual F-score(67% majority) (100% majority)#zinin .75 .35 .09 .67#geenzin .31 .21 .60 .73#fml .69 .46 .48 .81#omg .49 .25 .29 .67Table 2: Precision of correct hashtag predictions of the top 250 ?false positives?
based on human annota-tionsThe twofold evaluation that was employed inthis study underlines the difference between hash-tag predictability and emotion detection.
Regard-ing the three best performing hashtags in termsof predictability, only two, #zinin and #fml, pro-vide utilizable data for emotion detection.
Tweetsretrieved based on #geenzin seem to have a lessovert relation to the emotion of uneagerness, al-though other cues (such as topical words indirectlyrelated to the emotion) lead to a fairly correct re-covery of tweets that had the hashtag.
Comparingthe two evaluations for #omg, scoring low on both,we may assume that hashtag predictability is a re-quirement for a proper emotion detection.6 Discussion6.1 Feature categoriesWhile classifier performance gives an indicationof its ability to detect emotional tweets per hash-tag, the strong indicators of those hashtags discov-ered by the classifiers may provide additional in-sight into the usage patterns of emotional hashtagsby Twitter users.
Having scored the emotion de-tection quality of four hashtags, we set out to an-alyze the predictive features of these hashtags.
Tothis end we inspected the feature weights assignedby the Balanced Winnow classifier ranked by thestrength of their connection to the emotion label,taking into account the 150 tokens and n-gramswith the highest positive weight towards the hash-tag.Based on an analysis of the top 150 featuresfor the four hashtags, we distinguished seven cat-egories of features: other emotion-bearing hash-tags, emoticons, exclamations, states of being,time expressions, topic reference, and remainingfeatures.
Example features for each category, aswell as their share in the top 150 features for eachhashtag, are presented in Table 3.
The percentagesgive an impression of the most dominant types offeatures in the prediction of the hashtags.A first observation is that the top features ofthe #geenzin classifier are predominantly topic re-lated; the list hardly contains any feature that bearsemotion.
This is in line with the poor perfor-mance on the emotion detection evaluation, whilethe high AUC score can be explained by a relativeconsistency of the hashtag being used with topi-cal words that have an indirect relation with theemotion, such as homework for school.
The moreaccurate classifier for the opposite of #geenzin,#zinin, uses more temporal references pointing tothe event the person is looking forward to.
Also,Dutch positive adjectives such as ?lekker?
(?nice?
)and ?gezellig?
(multiple translations2), which arestrong predictors for #zinin, add to the accuracy ofthe classifier.
There are no clear counterparts forthe emotion linked to the opposing #geenzin.The percentages for #omg display the largestshares of emotion hashtags, emoticons and ex-clamations, confirming our impression that #omgfunctions as an intensifying marker of differentemotions; this is also reflected in the high percent-age of features in the ?other?
category.The most predictive features for the #fml clas-sifier consist of quite some emoticons, emotionalhashtags and exclamations.
Furthermore, thisclassifier contains most features in the ?state of be-ing?
category, mostly relating to the complex emo-tion of self pity.6.2 Emotional cues in TwitterIn contrast to spoken or face-to-face communica-tion, Twitter does not allow for the use of specialintonation or facial expressions to mark a mes-sage.
However, authors on Twitter have other cuesat their disposal.
Previous studies show, for ex-ample, that they might mark the irony or sarcasmin their message by using linguistic markers such2See http://en.wikipedia.org/wiki/Gezelligheid31Figure 1: Precision at {1 .
.
.
250} on the classes #zinin (top left), #fml (top right), #geenzin (bottom left),and #omg (bottom right).as hyperboles, exclamations and emoticons to helpreaders to correctly interpret the message (Burgerset al., 2012; Liebrecht et al., 2013).
We argue thatthis is also the case for emotional messages.Tweets are written messages with a strongly re-stricted length.
Authors compensate the lack ofnon-verbal cues by adding emotion markers.
Thishypothesis is supported by research in the fieldof Computer-Mediated Communication (CMC),where many studies have been carried out on (thelack of) non-verbal emotional cues in (electronic)messages.
Walther (1992) introduced the SocialInformation Processing Perspective: a theory thatusers can develop relationships via CMC if theyhave sufficient time and message exchanges andif communicative cues, such as non-verbal emo-tional cues, are available.
He argues that hu-mans easily switch between verbal and non-verbalcues.
Based on previous studies, Walther distin-guishes textual cues that express affection: re-lational icons (emoticons, see Asteroff, 1987),electronic paralanguage (such as intentional mis-spelling (veeeery), capitalization (NICE), repeti-tion of exclamation marks (good!!!!!)
and lexicalsurrogates for vocal segregates (hmmm) (Carey,1980).
Later he also recognizes emoticons asnonverbal emotion cues (Walther and D?Addario,2001).
Emoticons can serve many purposes, oneof which is expressing emotions (Agarwal et al.,2011; Davidov et al., 2010).7 ConclusionIn our experiments we showed that machine learn-ing classifiers can be relatively successful bothin predicting the hashtag with tweets which wereindeed tagged with them, and classifying tweetswithout the hashtag as exhibiting the emotion de-noted by the hashtag, for two of the four fully anal-ysed hashtags: #zinin and #fml.
In contrast, theclassifier of the hashtag #geenzin was only ableto re-link tweets that are stripped from the targethashtag with this hashtag, but failed to capture thecomplex emotion behind the hashtag.
The perfor-mance of the #omg classifier lags behind in bothtasks.These findings can be explained by the assump-tion we made that in order to be a proper emotionlabel, the context of the hashtag (the rest of the32Example Percentage in top 150 features#zinin #fml #geenzin #omgemotion hashtag ?#foreveralone?
6.67% 10.00% 2.67% 18.67%emoticon ?:S?
0.00% 4.67% 0.00% 6.67%exclamation ?noooo?
0.00% 2.67% 0.00% 8.67%state of being ?curious?
3.33% 7.33% 3.33% 0.67%temporal reference ?moment?
26.00% 7.33% 10.00% 1.33%topic ?dentist?
52.67% 48.67% 69.33% 25.33%other ?ready to?
11.33% 19.33% 14.67% 38.67%Table 3: Shares (in percentages) of seven categories in the top-150 highest-weighted features for fourhashtags.tweet) would need to convey the same emotionas the hashtag.
This appears to be the case with#zinin and #fml.
We may assume that the messagein tweets with #zinin or #fml carries the emotionitself, which is intensified by the hashtag.
The al-ternative relation between the hashtag and the textis that a hashtag adds emotion to an otherwise neu-tral message: a signalling function.
It seems thatmost of the tweets tagged with #geenzin are ex-amples of this second relation.
The classifier per-formed well at the re-link task, indicating that itwas able to exploit the consistent use of predic-tive words and phrases, but less well as an emotiondetector when we applied the classifier to unseentweets that do not carry the hashtag.
The topicalwords the classifier used as predictive features ap-pear to be used in several other settings in whichno emotion is conveyed, or different emotions thanthe one expressed by #geenzin.
The fourth hash-tag that was fully analysed, #omg, turned out tobe overall difficult for our classifier.
We defined#omg as conveying an aroused level of indigna-tion, fear or excitement.
In comparison to the otherthree hashtags, this definition is less strictly linkedto one emotion (Kim et al., 2012).
Rather, thehashtag is used in the context of three differentemotions and is in itself not an emotion, but anemotion intensifier.
Possibly, as a result thereofthe tweets are more diverse and the hashtag #omgoccurs more frequently with other linguistic ele-ments to express emotion, such as emotional hash-tags, emoticons and exclamations.Although time restrictions prevented us fromperforming a similar analysis of more hashtags,we can conclude that hashtag predictability isfairly high for most of the 24 hashtags in ourset.
Interestingly, a considerate part of the syn-onymous and antonymous hashtags led to similarscores, indicating a relationship between the typeof emotion conveyed by a hashtag and the degreeof consistency by which the hashtag is employedby users.
Whether the degree of consistency, alongwith an intensifying or emotion adding deploy-ment, can be deduced from the inherent propertiesof an emotion hashtag is open for future research.AcknowledgmentsThis research was supported by the Dutch nationalprogram COMMIT as part of the Infiniti project.ReferencesApoorv Agarwal, Boyi Xie, Ilia Vovsha, Owen Ram-bow, and Rebecca Passonneau.
2011.
Sentimentanalysis of twitter data.
In Proceedings of the Work-shop on Languages in Social Media, pages 30?38.Association for Computational Linguistics.Cecilia Ovesdotter Alm, Dan Roth, and RichardSproat.
2005.
Emotions from text: machine learn-ing for text-based emotion prediction.
In Proceed-ings of the Conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 579?586.
Association for Compu-tational Linguistics.Saima Aman and Stan Szpakowicz.
2007.
Identifyingexpressions of emotion in text.
In Text, Speech andDialogue, pages 196?205.
Springer.Christian Burgers, Margot van Mulken, and Peter JanSchellens.
2012.
Verbal irony differences in usageacross written genres.
Journal of Language and So-cial Psychology, 31(3):290?310.John Carey.
1980.
Paralanguage in computer medi-ated communication.
In Proceedings of the 18thannual meeting on Association for ComputationalLinguistics, pages 67?69.
Association for Compu-tational Linguistics.Stephen Dann.
2010.
Twitter content classification.First Monday, 15(12).33Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics:Posters, pages 241?249.
Association for Computa-tional Linguistics.Paul Ekman.
1971.
Universals and cultural differencesin facial expressions of emotion.
In Nebraska sym-posium on motivation.
University of Nebraska Press.T.
Fawcett.
2004.
ROC graphs: Notes and practi-cal considerations for researchers.
Technical ReportHPL-2003-4, Hewlett Packard Labs.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.CS224N Project Report, Stanford, pages 1?12.Roberto Gonz?alez-Ib?a?nez, Smaranda Muresan, andNina Wacholder.
2011.
Identifying sarcasm in twit-ter: A closer look.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages581?586.Jeff Huang, Katherine M Thornton, and Efthimis NEfthimiadis.
2010.
Conversational tagging in twit-ter.
In Proceedings of the 21st ACM conference onHypertext and hypermedia, pages 173?178.
ACM.Sepandar D Kamvar and Jonathan Harris.
2011.
Wefeel fine and searching the emotional web.
In Pro-ceedings of the fourth ACM international conferenceon Web search and data mining, pages 117?126.ACM.Suin Kim, JinYeong Bak, and Alice Haeyun Oh.
2012.Do you feel what i feel?
social aspects of emotionsin twitter conversations.
In ICWSM.Christine Liebrecht, Florian Kunneman, and Antal Vanden Bosch.
2013.
The perfect solution for de-tecting sarcasm in tweets #not.
In Proceedings ofthe 4th Workshop on Computational Approaches toSubjectivity, Sentiment and Social Media Analysis,pages 29?37, Atlanta, Georgia, June.
Associationfor Computational Linguistics.N.
Littlestone.
1988.
Learning quickly when irrelevantattributes abound: A new linear-threshold algorithm.Machine Learning, 2:285?318.Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1003?1011.
Association forComputational Linguistics.Saif M Mohammad.
2012.
#emotional tweets.
In Pro-ceedings of the First Joint Conference on Lexicaland Computational Semantics-Volume 1: Proceed-ings of the main conference and the shared task, andVolume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pages 246?255.Association for Computational Linguistics.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa corpus for sentiment analysis and opinion mining.In LREC.Robert Plutchik.
1980.
Emotion: A psychoevolution-ary synthesis.
Harper & Row New York.Matthew Purver and Stuart Battersby.
2012.
Experi-menting with distant supervision for emotion classi-fication.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 482?491.
Associationfor Computational Linguistics.Ashequl Qadir and Ellen Riloff.
2013.
Bootstrappedlearning of emotion hashtags# hashtags4you.
InWASSA 2013, page 2.Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Un-supervised modeling of twitter conversations.
InHuman Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 172?180, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Kirk Roberts, Michael A Roach, Joseph Johnson, JoshGuthrie, and Sanda M Harabagiu.
2012.
Em-patweet: Annotating and detecting emotions on twit-ter.
In Proceedings of the Eighth InternationalConference on Language Resources and Evaluation,pages 3806?3813.Jared Suttles and Nancy Ide.
2013.
Distant supervi-sion for emotion classification with discrete binaryvalues.
In Computational Linguistics and IntelligentText Processing, pages 121?136.
Springer.Erik Tjong Kim Sang and Antal van den Bosch.
2013.Dealing with big data: The case of twitter.
Com-putational Linguistics in the Netherlands Journal,3:121?134, 12/2013.Joseph B Walther and Kyle P D?Addario.
2001.
Theimpacts of emoticons on message interpretation incomputer-mediated communication.
Social ScienceComputer Review, 19(3):324?347.Joseph B Walther.
1992.
Interpersonal effects incomputer-mediated interaction a relational perspec-tive.
Communication research, 19(1):52?90.Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan,and Amit P Sheth.
2012.
Harnessing twit-ter ?big data?
for automatic emotion identifica-tion.
In Privacy, Security, Risk and Trust (PASSAT),2012 International Conference on and 2012 Inter-national Confernece on Social Computing (Social-Com), pages 587?592.
IEEE.34
