Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 97?105,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsCompetitive Grammar Writing?Jason EisnerDepartment of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218, USAjason@cs.jhu.eduNoah A. SmithLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAnasmith@cs.cmu.eduAbstractJust as programming is the traditional introduction tocomputer science, writing grammars by hand is an ex-cellent introduction to many topics in computational lin-guistics.
We present and justify a well-tested introductoryactivity in which teams of mixed background competeto write probabilistic context-free grammars of English.The exercise brings together symbolic, probabilistic, al-gorithmic, and experimental issues in a way that is acces-sible to novices and enjoyable.1 IntroductionWe describe a hands-on group activity for novicesthat introduces several central topics in computa-tional linguistics (CL).
While the task is intellec-tually challenging, it requires no background otherthan linguistic intuitions, no programming,1 andonly a very basic understanding of probability.The activity is especially appropriate for mixedgroups of linguists, computer scientists, and others,letting them collaborate effectively on small teamsand learn from one another.
A friendly competitionamong the teams makes the activity intense and en-joyable and introduces quantitative evaluation.1.1 Task OverviewEach 3-person team is asked to write a generativecontext-free grammar that generates as much of En-?
This work was supported by NSF award 0121285,?ITR/IM+PE+SY: Summer Workshops on Human LanguageTechnology: Integrating Research and Education,?
and by aFannie and John Hertz Foundation Fellowship to the secondauthor.
We thank David A. Smith and Markus Dreyer for co-leading the lab in 2004?2007 and for implementing various im-provements in 2004?2007 and for providing us with data fromthose years.
The lab has benefited over the years from feedbackfrom the participants, many of whom attended the JHU sum-mer school thanks to the generous support of NAACL.
We alsothank the anonymous reviewers for helpful comments.1In our setup, students do need the ability to invoke scriptsand edit files in a shared directory, e.g., on a Unix system.glish as possible (over a small fixed vocabulary).Obviously, writing a full English grammar wouldtake years even for experienced linguists.
Thus eachteam will only manage to cover a few phenomena,and imperfectly.To encourage precision but also recall and lin-guistic creativity, teams are rewarded for generatingsentences that are (prescriptively) grammatical butare not anticipated by other teams?
grammars.
Thissomewhat resembles scoring in the Boggle wordgame, where players are rewarded for finding validwords in a grid that are not found by other players.A final twist is that the exercise uses probabilisticcontext-free grammars (PCFGs); the actual scoringmethods are based on sampling and cross-entropy.Each team must therefore decide how to allocateprobability mass among sentences.
To avoid assign-ing probability of zero when attempting to parse an-other team?s sentences, a team is allowed to ?backoff?
(when parsing) to a simpler probability model,such as a part-of-speech bigram model, also ex-pressed as a PCFG.1.2 SettingWe have run this activity for six consecutive years,as a laboratory exercise on the very first afternoonof an intensive 2-week summer school on varioustopics in human language technology.2 We allot 3.5hours in this setting, including about 15 minutes forsetup, 30 minutes for instructions, 15 minutes forevaluation, and 30 minutes for final discussion.The remaining 2 hours is barely enough time forteam members to get acquainted, understand the re-quirements, plan a strategy, and make a small dent in2This 2-week course is offered as a prelude to the JohnsHopkins University summer research workshops, sponsored bythe National Science Foundation and the Department of De-fense.
In recent years the course has been co-sponsored by theNorth American ACL.97the problem.
Nonetheless, participants consistentlytell us that the exercise is enjoyable and pedagogi-cally effective, almost always voting to stay an extrahour to make further progress.Our 3-person teams have consisted of approxi-mately one undergraduate, one junior graduate stu-dent, and one more senior graduate student.
If pos-sible, each team should include at least one memberwho has basic familiarity with some syntactic phe-nomena and phrasal categories.
Teams that whollylack this experience have been at a disadvantage inthe time-limited setting.1.3 Resources for InstructorsWe will maintain teaching materials at http://www.clsp.jhu.edu/grammar-writing,for both the laboratory exercise version and forhomework versions: scripts, data, instructions forparticipants, and tips for instructors.
While ourmaterials are designed for participants who arefluent in English, we would gladly host translationsor adaptations into other languages, as well as othervariants and similar assignments.2 Why Grammar Writing?A computer science curriculum traditionally startswith programming, because programming is acces-sible, hands-on, and necessary to motivate or under-stand most other topics in computer science.
We be-lieve that grammar writing should play the same rolein computational linguistics?as it often did beforethe statistical revolution3?and for similar reasons.Grammar writing remains central because manytheoretical and applied CL topics center aroundgrammar formalisms.
Much of the field tries to de-sign expressive formalisms (akin to programminglanguages); solve linguistic problems within them(akin to programming); enrich them with probabil-ities; process them with efficient algorithms; learnthem from data; and connect them to other modulesin the linguistic processing pipeline.3The first author was specifically inspired by his experiencewriting a grammar in Bill Woods?s NLP course at Harvard in1987.
An anonymous reviewer remarks that such assignmentswere common at the time.
Our contributions are to introducestatistical and finite-state elements, to make the exercise into agame, and to provide reusable instructional materials.Of course, there are interesting grammar for-malisms at all levels of language processing.
Onemight ask why syntax is a good level at which to be-gin education in computational linguistics.First, starting with syntax establishes at the startthat there are formal and computational methodsspecific to natural language.
Computational linguis-tics is not merely a set of applied tasks to be solvedwith methods already standardly taught in courseson machine learning, theory of computation,4 orknowledge representation.Second, we have found that syntax captures stu-dents?
interest rapidly.
They quickly appreciate thelinguistic phenomena, see that they are non-trivial,and have little trouble with the CFG formalism.Third, beginning specifically with PCFGs paystechnical dividends in a CL course.
Once one un-derstands PCFG models, it is easy to understand thesimpler finite-state models (including n-gram mod-els, HMMs, etc.)
and their associated algorithms, ei-ther by analogy or by explicit reduction to specialcases of PCFGs.
CFGs are also a good starting pointfor more complex syntactic formalisms (BNF, cate-gorial grammars, TAG, LFG, HPSG, etc.)
and forcompositional semantics.
Indeed, our exercise mo-tivates these more complex formalisms by forcingstudents to work with the more impoverished PCFGformalism and experience its limitations.3 Educational Goals of the ExerciseOur grammar-writing exercise is intended to serveas a touchstone for discussion of many subsequenttopics in NLP and CL (which are italicized below).As an instructor, one can often refer back later to theexercise to remind students of their concrete experi-ence with a given concept.Generative probabilistic models.
The first set ofconcepts concerns language models.
These are eas-iest to understand as processes for generating text.Thus, we give our teams a script for generating ran-dom sentences from their grammar and their backoff4Courses on theory of computation do teach pushdown au-tomata and CFGs, of course, but they rarely touch on parsingor probabilistic grammars, as this exercise does.
Courses oncompilers may cover parsing algorithms, but only for restrictedgrammar families such as unambiguous LR(1) grammars.98model?a helpful way to observe the generative ca-pacity and qualitative behavior of any model.Of course, in practice a generative grammar ismost often run ?backwards?
to parse an observedsentence or score its inside probability, and we alsogive the teams a script to do that.
Most teams do ac-tually run these scripts repeatedly to test their gram-mars, since both scripts will be central in the evalua-tion (where sentences are randomly generated fromone grammar and scored with another grammar).It is common for instructors of NLP to show ex-amples of randomly-generated text from an n-grammodel (e.g., Jurafsky and Martin, 2000, pp.
202?203), yet this amusing demonstration may be misin-terpreted as merely illustrating the inadequacy of n-gram models.
Our use of a hand-crafted PCFG com-bined with a bigram-based (HMM) backoff grammardemonstrates that although the HMM is much worseat generating valid English sentences (precision), itis much better at robustly assigning nonzero proba-bility when analyzing English sentences (recall).Finally, generative models do more than assignprobability.
They often involve linguistically mean-ingful latent variables, which can be recoveredgiven the observed data.
Parsing with an appropri-ate PCFG thus yields a intuitive and useful analy-sis (a syntactic parse tree), although only for thesentences that the PCFG covers.
Even parsing withthe simple backoff grammar that we initially provideyields some coarser analysis, in the form of a part-of-speech tagging, since this backoff grammar is aright-branching PCFG that captures part-of-speechbigrams (for details see ?1.1, ?4.1, and Table 2).
Infact, parsing with the backoff PCFG is isomorphic toViterbi decoding in an HMM part-of-speech tagger,a topic that is commonly covered in NLP courses.Modeling grammaticality.
The next set of con-cepts concerns linguistic grammaticality.
During theevaluation phase of our exercise (see below), stu-dents must make grammaticality judgments on otherteams?
randomly generated sentences?which areusually nonsensical, frequently hard for humans toparse, and sometimes ungrammatical.
This concretetask usually prompts questions from students abouthow grammaticality ought to be defined, both forpurposes of the task and in principle.
It could alsobe used to discuss why some of the sentences areso hard for humans to understand (e.g., garden-pathand frequency effects) and what parsing strategieshumans or machines might use.The exercise of modeling grammaticality withthe CFG formalism, a formalism that appears else-where in the computer science curriculum, high-lights some important differences between naturallanguages and formal languages.
A natural lan-guage?s true grammar is unknown (and may not evenexist: perhaps the CFG formalism is inadequate).Rather, a grammar must be induced or constructedas an approximate model of corpus data and/or cer-tain native-speaker intuitions.
A natural languagealso differs from a programming language in includ-ing ambiguous sentences.
Students observe that theparser uses probabilities to resolve ambiguity.Linguistic analysis.
Grammar writing is an ex-cellent way to get students thinking about linguis-tic phenomena (e.g., adjuncts, embedded sentences,wh-questions, clefts, point absorption of punctuationmarks).
It also forces students to think about appro-priate linguistic formalisms.
Many phenomena aretedious to describe within CFGs (e.g., agreement,movement, subcategorization, selectional restric-tions, morphological inflection, and phonologically-conditioned allomorphy such as a vs. an).
Theycan be treated in CFGs only with a large number ofrepetitive rules.
Students appreciate these problemsby grappling with them, and become very receptiveto designing expressive improvements such as fea-ture structures and slashed categories.Parameter tuning.
Students observe the effectsof changing the rule probabilities by running thescripts.
For example, teams often find themselvesgenerating unreasonably long (or even infinite) sen-tences, and must damp down the probabilities oftheir recursive rules.
Adjusting the rule probabilitiescan also change the score and optimal tree that arereturned by the parser, and can make a big differencein the final evaluation (see ?5).
This appreciation forthe role of numerical parameters helps motivate fu-ture study of machine learning in NLP.Quantitative evaluation.
As an engineering pur-suit, NLP research requires objective evaluationmeasures to know how well systems work.Our first measure is the precision of each team?s99probabilistic grammar: how much of its probabilitymass is devoted to sentences that are truly grammat-ical?
Estimating this requires human grammaticalityjudgments on a random sample C of sentences gen-erated from all teams?
grammars.
These binary judg-ments are provided by the participants themselves,introducing the notion of linguistic annotation (al-beit of a very simple kind).
Details are in ?4.3.3.Our second measure is an upper-bound approx-imation to cross-entropy (or log-perplexity?in ef-fect, the recall of a probability model): how welldoes each team?s probabilistic model (this time in-cluding the backoff model of ?1.1) anticipate unseendata that are truly grammatical?
(Details in ?4.3.3.
)Note that in contrast to parsing competitions, wedo not evaluate the quality of the parse trees (e.g.,PARSEVAL).
Our cross-entropy measure evaluatesonly the grammars?
ability to predict word strings(language modeling).
That is because we impose noannotation standard for parse trees: each team is freeto develop its own theory of syntax.
Furthermore,many sentences will only be parsable by the backoffgrammar (e.g., a bigram model), which is not ex-pected to produce a full syntactic analysis.The lesson about cross-entropy evaluation isslightly distorted by our peculiar choice of test data.In principle, the instructors might prepare a batchof grammatical sentences ahead of time and splitthem into a test set (used to evaluate cross-entropyat the end) and a development set (provided to thestudents at the start, so that they know which gram-matical phenomena are important to handle).
Theactivity could certainly be run in this way to demon-strate proper experimental design for evaluating alanguage model (discussed further in ?5 and ?6).We have opted for the more entertaining ?Boggle-style?
evaluation described in ?1.1, where teams tryto stump one another by generating difficult testdata, using the fixed vocabulary.
Thus, we evaluateeach team?s cross-entropy on all grammatical sen-tences in the collection C, which was generated expost facto from all teams?
grammars.4 Important Details4.1 DataA few elements are provided to participants to getthem started on their grammars.Vocabulary.
The terminal vocabulary ?
consistsof words from early scenes of the film Monty Pythonand the Holy Grail along with some inflected formsand function words, for a total vocabulary of 220words.
For simplicity, only 3rd-person pronouns,nouns, and verbs are included.
All words are case-sensitive for readability (as are the grammar nonter-minals), but we do not require or expect sentence-initial capitalization.All teams are restricted to this vocabulary, so thatthe sentences that they generate will not frustrateother teams?
parsers with out-of-vocabulary words.However, they are free to use words in unexpectedways (e.g., using castle in its verbal sense fromchess, or building up unusual constructions with theavailable function words).Initial lexicon.
The initial lexical rules take theform T ?
w, where w ?
?+ and T ?
T , withT being a set of six coarse part-of-speech tags:Noun: 21 singular nouns starting with consonantsDet: 9 singular determinersPrep: 14 prepositionsProper: 8 singular proper nouns denoting people(including multiwords such as Sir Lancelot)VerbT: 6 3rd-person singular present transitiveverbsMisc: 183 other words, divided into several com-mented sections in the grammar fileStudents are free to change this tagset.
Theyare especially encouraged to refine the Misc tag,which includes 3rd-person plural nouns (includingsome proper nouns), 3rd-person pronouns (nomina-tive, accusative, and genitive), additional 3rd-personverb forms (plural present, past, stem, and partici-ples), verbs that cannot be used transitively, modals,adverbs, numbers, adjectives (including some com-parative and superlative forms), punctuation, coor-dinating and subordinating conjunctions, wh-words,and a few miscellaneous function words (to, not, ?s).The initial lexicon is ambiguous: some words areassociated with more than one tag.
Each rule hasweight 1, meaning that a tag T is equally likely torewrite as any of its allowed nonterminals.Initial grammar.
We provide the ?S1?
rules in Ta-ble 1, so that students can try generating and parsing1001 S1 ?
NP VP .1 VP ?
VerbT NP20 NP ?
Det Nbar1 NP ?
Proper20 Nbar ?
Noun1 Nbar ?
Nbar PP1 PP ?
Prep NPTable 1: The S1 rules: a starting point for building an En-glish grammar.
The start symbol is S1.
The weights inthe first column will be normalized into generative proba-bilities; for example, the probability of expanding a givenNP with NP ?
Det Nbar is actually 20/(20 + 1).1 S2 ?1 S2 ?
Noun1 S2 ?
Misc1 Noun ?
Noun1 Noun ?
Noun Noun1 Noun ?
Noun Misc1 Misc ?
Misc1 Misc ?
Misc Noun1 Misc ?
Misc MiscTable 2: The S2 rules (simplified here where T ={Noun,Misc}): a starting point for a backoff grammar.The start symbol is S2.
The Noun nonterminal gener-ates those phrases that start with Nouns.
Its 3 rules meanthat following a Noun, there is 1/3 probability each ofstopping, continuing with another Noun (via Noun), orcontinuing with a Misc word (via Misc).sentences right away.
The S1 and lexical rules to-gether implement a very small CFG.
Note that noMisc words can yet be generated.
Indeed, this ini-tial grammar will only generate some simple gram-matical SVO sentences in singular present tense, al-though they may be unboundedly long and ambigu-ous because of recursion through Nbar and PP.Initial backoff grammar.
The provided ?S2?grammar is designed to assign positive probabilityto any string in ??
(see ?1.1).
At least initially, thisPCFG generates only right-branching structures.
Itsnonterminals correspond to the states of a weightedfinite-state machine, with start state S2 and one stateper element of T (the coarse parts of speech listedabove).
Table 2 shows a simplified version.From each state, transition into any state exceptthe start state S2 is permitted, and so is stopping.These rules can be seen as specifying the transitionsArthur is the king .Arthur rides the horse near the castle .riding to Camelot is hard .do coconuts speak ?what does Arthur ride ?who does Arthur suggest she carry ?are they suggesting Arthur ride to Camelot ?Guinevere might have known .it is Sir Lancelot who knows Zoot !neither Sir Lancelot nor Guinevere will speak of it .the Holy Grail was covered by a yellow fruit .do not speak !Arthur will have been riding for eight nights .Arthur , sixty inches , is a tiny king .Arthur and Guinevere migrate frequently .he knows what they are covering with that story .the king drank to the castle that was his home .when the king drinks , Patsy drinks .Table 3: Example sentences.
Only the first two can beparsed by the initial S1 and lexical rules.in a bigram hidden Markov model (HMM) on part-of-speech tags, whose emissions are specified by thelexical rules.
Since each rule initially has weight 1,all part-of-speech sequences of a given length areequally likely, but these weights could be changedto arbitrary transition probabilities.Start rules.
The initial grammar S1 and the ini-tial backoff grammar S2 are tied together by a singlesymbol START, which has two production rules:99 START ?
S11 START ?
S2These two rules are obligatory, but their weightsmay be changed.
The resulting model, rooted atSTART, is a mixture of the S1 and S2 grammars,where the weights of these two rules implementthe mixture coefficients.
This is a simple form ofbackoff smoothing by linear interpolation (Jelinekand Mercer, 1980).
The teams are warned to payspecial attention to these rules.
If the weight ofSTART ?
S1 is decreased relative to START ?S2, then the model relies more heavily on the back-off model?perhaps a wise choice for keeping cross-entropy small, if the team has little faith in S1?s abil-ity to parse the forthcoming data.Sample sentences.
A set of 27 example sentencesin ?+ (subset shown in Table 3) is provided for lin-guistic inspiration and as practice data on which to101run the parser.
Since only 2 of these sentences canbe parsed with the initial S1 and lexical rules, thereis plenty of room for improvement.
A further devel-opment set is provided midway through the exercise(?4.3.2).4.2 Computing EnvironmentWe now describe how the above data are made avail-able to students along with some software.4.2.1 ScriptsWe provide scripts that implement two importantcapabilities for PCFG development.
Both scriptsare invoked with a set of grammar files specified onthe command line (typically all of them, ?*.gr?
).A PCFG is obtained by concatenating these filesand stripping their comments, then normalizing theirrule weights into probabilities (see Table 1), andfinally checking that all terminal symbols of thisPCFG are legal words of the vocabulary ?.The random generation script prints a sample ofn sentences from this PCFG.
The generator can op-tionally print trees or flat word sequences.
A startsymbol other than the default S1 may be specified(e.g., NP, S2, START, etc.
), to allow participants totest subgrammars or the backoff grammar.5The parsing script prints the most probable parsetree for each sentence read from a file (or from thestandard input).
A start symbol may again be speci-fied; this time the default is START.
The parser alsoprints each sentence?s probability, total number ofparses, and the fraction of the probability that goesto the best parse.Tree outputs can be pretty-printed for readability.4.2.2 Collaborative SetupTeams of three or four sit at adjacent workstationswith a shared filesystem.
The scripts above are pub-licly installed; a handout gives brief usage instruc-tions.
The instructor and teaching assistant roam theroom and offer assistance as needed.Each team works in its own shared directory.
TheEmacs editor will warn users who are simultane-ously editing the same file.
Individual participantstend to work on different sub-grammar files; all of5For some PCFGs, the stochastic process implemented bythe script has a nonzero probability of failing to terminate.
Thishas not caused problems to date.a team?s files can be concatenated (as *.gr) whenthe scripts are run.
(The directory initially includesseparate files for the S1 rules, S2 rules, and lexi-cal rules.)
To avoid unexpected interactions amongthese grammar fragments, students are advised to di-vide work based on nonterminals; e.g., one memberof a team may claim jurisdiction over all rules of theform VP plural ?
?
?
?.4.3 Activities4.3.1 Introductory LectureOnce students have formed themselves into teamsand managed to log in at adjacent computers, webegin with an 30-minute introductory lecture.
Nobackground is assumed.
We explain PCFGs simplyby showing the S1 grammar and hand-simulating theaction of the random sentence generator.We explain the goal of extending the S1 gram-mar to cover more of English.
We explain how eachteam?s precision will be evaluated by human judg-ments on a sample, but point out that this measuregives no incentive to increase coverage (recall).
Thismotivates the ?Boggle?
aspect of the game, whereteams must also be able to parse one another?s gram-matical sentences, and indeed assign them as higha probability as possible.
We demonstrate how theparser assigns a probability by running it on the sen-tence that we earlier generated by hand.6We describe how the parser?s probabilities areturned into a cross-entropy measure, and discussstrategy.
Finally, we show that parsing a sentencethat is not covered by the S1 grammar will lead toinfinite cross-entropy, and we motivate the S2 back-off grammar as an escape hatch.4.3.2 Midpoint: Development dataOnce or more during the course of the exercise,we take a snapshot of all teams?
S1 grammars andsample 50 sentences from each.
The resulting col-lection of sentences, in random order, is made avail-able to all teams as a kind of development data.While we do not filter for grammaticality as in thefinal evaluation, this gives all participants an ideaof what they will be up against when it comes time6The probability will be tiny, as a product of many rule prob-abilities.
But it may be higher than expected, and students arechallenged to guess why: there are additional parses beyond theone we hand-generated, and the parser sums over all of them.102to parse other teams?
sentences.
Teams are on theirhonor not to disguise the true state of their grammarat the time of the snapshot.4.3.3 Evaluation procedureGrammar development ends at an announceddeadline.
The grammars are now evaluated on thetwo measures discussed in ?3.
The instructors run afew scripts that handle most of this work.First, we generate a collection C by sampling 20sentences from each team?s probabilistic grammar,using S1 as the start symbol.
(Thus, the backoff S2grammar is not used for generation.
)We now determine, for each team, what fractionof its 20-sentence sample was grammatical.
The par-ticipants play the role of grammaticality judges.
Inour randomized double-blind procedure, each indi-vidual judge receives (in his or her team directory)a file of about 20 sentences from C, with instruc-tions to delete the ungrammatical ones and save thefile, implying coarse Boolean grammaticality judg-ments.7 The files are constructed so that each sen-tence in C is judged by 3 different participants; asentence is considered grammatical if ?
2 judgesthinks that it is.We define the test corpus C?
to consist of all sen-tences in C that were judged grammatical.
Eachteam?s full grammar (using START as the start sym-bol to allow backoff) is used to parse C?.
Thisgives us the log2-probability of each sentence in C?
;the cross-entropy score is the sum of these log2-probabilities divided by the length of C?.4.3.4 Group discussionWhile the teaching assistant is running the evalua-tion scripts and compiling the results, the instructorleads a general discussion.
Many topics are possi-ble, according to the interests of the instructor andparticipants.
For example: What linguistic phenom-ena did the teams handle, and how?
Was the CFGformalism adequately expressive?
How well wouldit work for languages other than English?What strategies did the teams adopt, based on theevaluation criteria?
How were the weights chosen?7Judges are on their honor to make fair judgments ratherthan attempt to judge other teams?
sentences ungrammatical.Moreover, such an attempt might be self-defeating, as theymight unknowingly be judging some of their own team?s sen-tences ungrammatical.cross-entropy new rulesteam precision (bits/sent.)
lex.
otherA 0.30 35.57 202 111B 0.00 54.01 304 80C 0.80 38.48 179 48D 0.25 49.37 254 186E 0.55 39.59 198 114F 0.00 39.56 193 37G 0.65 40.97 71 15H 0.30 36.53 176 9I 0.70 36.17 181 54J 0.00 ?
193 29Table 4: Teams?
evaluation scores in one year, and thenumber of new rules (not including weight changes) thatthey wrote.
Only teams A and H modified the relativeweights of the START rules (they used 80/20 and 75/25,respectively), giving them competitive perplexity scores.
(Cross-entropy in this year was approximated by an upperbound that uses only the probability of each sentence?ssingle best parse.
)How would you build a better backoff grammar?8How would you organize a real long-term effortto build a full English grammar?
What would such agrammar be good for?
Would you use any additionaltools, data, or evaluation criteria?5 OutcomesTable 4 shows scores achieved in one year (2002).A valuable lesson for the students was the impor-tance of backoff.
None but the first two of the exam-ple sentences (Table 3) are parseable with the smallS1 grammar.
Thus, the best way to reduce perplexitywas to upweight the S2 grammar and perhaps spenda little time improving its rules or weights.
Teamsthat spent all of their time on the S1 grammar mayhave learned a lot about linguistics, but tended toscore poorly on perplexity.Indeed, the winning team in a later year spentnearly all of their effort on the S2 grammar.
Theyplaced almost all their weight on the S2 grammar,whose rules they edited and whose parameters theyestimated from the example sentences and develop-ment data.
As for their S1 grammar, it generatedonly a small set of grammatical sentences with ob-8E.g., training the model weights, extending it to trigrams,or introducing syntax into the S2 model by allowing it to invokenonterminals of the S1 grammar.103scure constructions that other teams were unlikely tomodel well in their S1 grammars.
This gave them a100% precision score on grammaticality while pre-senting a difficult parsing challenge to other teams.This team gamed our scoring system, exploiting theidiosyncrasy that S2 would be used to parse but notto generate.
(See ?3 for an alternative system.
)We conducted a post hoc qualitative survey of thegrammars from teams in 2002.
Teams were notasked to provide comments, and nonterminal nam-ing conventions often tend to be inscrutable, but theintentions are mostly understandable.
All 10 teamsdeveloped more fine-grained parts of speech, includ-ing coordinating conjunctions, modal verbs, numberwords, adverbs.
9 teams implemented singular andplural features on nouns and/or verbs, and 9 imple-mented the distinction between base, past, present,and gerund forms of verbs (or a subset of those).
7teams brought in other features like comparative andsuperlative adjectives and personal vs. possessivepronouns.
4 teams modeled pronoun case.
TeamC created a ?location?
category.7 teams explicitly tried to model questions, of-ten including rules for do-support; 3 of those teamsalso modeled negation with do-insertion.
2 teamsused gapped categories (team D used them exten-sively), and 7 teams used explicit X?
nonterminals,most commonly within noun phrases (following theinitial grammar).
Three teams used a rudimentarysubcategorization frame model, distinguishing be-tween sentence-taking, transitive, and intransitiveverbs, with an exploded set of production rules asa result.
Team D modeled appositives.The amount of effort teams put into weights var-ied, as well.
Team A used 11 distinct weight valuesfrom 1 to 80, giving 79 rules weights> 1 (next clos-est was team 10, with 7 weight values in [1, 99] andonly 43 up-weighted rules).
Most teams set fewerthan 25 rules?
weights to something other than 1.6 Use as a Homework AssignmentTwo hours is not enough time to complete a goodgrammar.
Our participants are ambitious but nevercome close to finishing what they undertake; Table 4reflects incomplete work.
Nonetheless, we believethat the experience still successfully fulfills many ofthe goals of ?2?3 in a short time, and the participantsenjoy the energy in a roomful of peers racing towarda deadline.
The fact that the task is open-ended andclearly impossible keeps the competition friendly.An alternative would be to allot 2 weeks or moreas a homework assignment, allowing teams to gomore deeply into linguistic issues and/or backoffmodeling techniques.
A team?s grade could belinked to its performance.
In this setting, we recom-mend limiting the team size to 1 or 2 people each,since larger teams may not be able to find time orfacilities to work side-by-side for long.This homework version of our exercise mighthelpfully be split into two assignments:Part 1 (non-competitive, smaller vocabulary).
?Extend the initial S1 grammar to cover a certainsmall set of linguistic phenomena, as illustrated by adevelopment set [e.g., Table 3].
You will be eval-uated on the cross-entropy of your grammar on atest set that closely resembles the development set[see ?3], and perhaps also on the acceptability ofsentences sampled from your grammar (as judgedby you, your classmates, or the instructor).
Youwill also receive qualitative feedback on how cor-rectly and elegantly your grammar solves the lin-guistic problems posed by the development set.
?Part 2 (competitive, full 220-word vocabulary).
?Extend your S1 grammar from Part 1 to generatephenomena that stump other teams, and add an S2grammar to avoid being stumped by them.
You willbe evaluated as follows .
.
.
[see ?4.3.3].
?We have already experimented with simpler non-competitive grammar-writing exercises (similar toPart 1) in our undergraduate NLP courses.
Giventwo weeks, even without teammates, many studentsdo a fine job of covering several non-trivial syntacticphenomena.
These assignments are available for useby others (see ?1.3).
In some versions, students wereasked to write their own random generator, judgetheir own sentences, explain how to evaluate per-plexity, or guess why the S2 grammar was used.7 ConclusionWe hope that other instructors can make use of thesematerials or ideas.
Our competitive PCFG-writinggame touches upon many core CL concepts, is chal-lenging and enjoyable, allows collaboration, and issuitable for cross-disciplinary and intro courses.104ReferencesF.
Jelinek and R. L. Mercer.
1980.
Interpolated estima-tion of Markov source parameters from sparse data.
InProc.
of Workshop on Pattern Recognition in Practice.D.
Jurafsky and J.H.
Martin.
2000.
Speech and Lan-guage Processing.
Prentice Hall.105
