Using Wizard-of-Oz simulations to bootstrap Reinforcement-Learning-based dialog management systemsJason D. Williams Steve YoungDepartment of Engineering, University of Cambridge, Cambridge, CB2 1PZ, United Kingdom{jdw30,sjy}@eng.cam.ac.ukAbstractThis paper describes a method for ?boot-strapping?
a Reinforcement Learning-based dialog manager using a Wizard-of-Oz trial.
The state space and action setare discovered through the annotation,and an initial policy is generated using aSupervised Learning algorithm.
Themethod is tested and shown to create aninitial policy which performs significantlybetter and with less effort than a hand-crafted policy, and can be generated usinga small number of dialogs.1 Introduction and motivationRecent work has successfully applied Rein-forcement Learning (RL) to learning dialog strat-egy from experience, typically formulating theproblem as a Markov Decision Process (MDP).
(Walker et al, 1998; Singh et al, 2002; Levin etal., 2000).
Despite successes, several openquestions remain, especially the issue of how tocreate (or ?bootstrap?)
the initial system prior todata becoming available from on-line operation.This paper proceeds as follows.
Section 2 out-lines the core elements of an MDP and issues re-lated to applying an MDP to dialog management.Sections 3 and 4 detail a method for addressingthese issues, and the procedure used to test themethod, respectively.
Sections 5-7 present the re-sults, a discussion, and conclusions, respectively.2 BackgroundAn MDP is composed of a state space, an actionset, and a policy which maps each state to one ac-tion.
Introducing a reward function allows us tocreate or refine the policy using RL.
(Sutton andBarto, 1998).When the MDP framework is applied to dialogmanagement, the state space is usually constructedfrom vector components including informationstate, dialog history, recognition confidence, data-base status, etc.
In most of the work to date boththe state space and action set are hand selected, inpart to ensure a limited state space, and to ensuretraining can proceed using a tractable number ofdialogs.
However, hand selection becomes im-practical as system size increases, and automaticgeneration/selection of these elements is currentlyan open problem, closely related to the problem ofexponential state space size.3 A method for bootstrapping RL-basedsystemsHere we propose a method for ?bootstrapping?
anMDP-based system; specifically, we address thechoice of the state representation and action set,and the creation of an initial policy.3.1 Step 1: Conduct Wizard-of-Oz dialogsThe method commences with ?talking wizard?interactions in which either the wizard?s voice isdisguised, or a Text-to-speech engine is used.
Wechoose human/wizard rather than human/humandialogs as people behave differently toward (whatthey perceive to be) machines and other people asdiscussed in J?nsson and Dahlbick, 1988 and alsovalidated in Moore and Browning, 1992.
The dia-log, including wizard?s interaction with back-enddata sources is recorded and transcribed.3.2 Step 2: Exclude out-of-domain turnsThe wizard will likely handle a broader set of re-quests than the system will ultimately be able tocover; thus some turns must be excluded.
Step 2begins by formulating a list of tasks which are tobe included in the final system?s repertoire; turnsdealing with tasks outside this repertoire are la-beled out-of-domain (OOD) and excluded.This step takes an approach which is analogousto, but more simplistic than ?Dialogue Distilling?
(Larsson et al, 2000) which changes, adds and re-moves portions of turns or whole turns.
Here rulessimply stipulate whether to keep a whole turn.3.3 Step 3: Enumerate action set and statespaceNext, the in-domain turns are annotated with dia-log acts.
Based on these, an action set is enumer-ated, and a set of state parameters and theirpossible values to form a vector describing thestate space is determined, including:?
Information state (e.g., departure-city, arri-val-city) from the user and database.?
The confidence/confirmation status of in-formation state variables.?
Expressed user goal and/or system goal.?
Low-level turn information (e.g., yes/no re-sponses, backchannel, ?thank you?, etc.).?
Status of database interactions (e.g., when aform can be submitted or has been returned).A variety of dialog-act tagging taxonomies ex-ist in the literature.
Here we avoid a tagging sys-tem that relies on a stack or other recursivestructure (for example, a goal or game stack) as itis not immediately clear how to represent a recur-sive structure in a state space.In practice, many information state componentsare much less important than their correspondingconfirmation status, and can be removed.Even with this reduction, the state space will bemassive ?
probably too large to ever visit all states.We propose using a parameterized value function -- i.e., a value function that shares parametersacross states (including states previously unob-served).
One special case of this is state tying, inwhich a group of states share the same value func-tion; an alternative is to use a Supervised Learningalgorithm to estimate a value function.3.4 Step 4: Form an initial policyFor each turn in the corpus, a vector is created rep-resenting the current dialog state plus the subse-quent wizard action.
Taking the action as the classvariable, Supervised Learning (SL) is used to builda classifier which functions as the initial policy.Depending on the type of SL algorithm used, itmay be possible to produce a prioritized list of ac-tions rather than a single classification; in this case,this list can form an initial list of actions permittedin a given state.As noted by Levin et al (2000), supervisedlearning is not appropriate for optimizing dialogstrategy because of the temporal/environmentalnature of dialog.
Here we do not assert that theSL-learned policy will be optimal ?
simply that itcan be easily created, that it will be significantlybetter than random guessing, and better andcheaper to produce than creating a cursory hand-crafted strategy.3.5 Limitations of the methodThis method has several obvious limitations:?
Because a talking, perfect-hearing wizard isused, no/little account is taken of the recog-nition errors to be expected with automatedspeech recognition (ASR).?
Excluding too much in Step 2 may excludeactions or state components which wouldhave ultimately produced a superior system.4 Experimental designThe proposed approach has been tested using theAutoroute corpus of 166 dialogs, in which a talk-ing wizard answered questions about driving direc-tions in the UK (Moore and Browning, 1992).A small set of in-domain tasks was enumerated(e.g., gathering route details, outputting summaryinformation about a route, disambiguation of placenames, etc.
), and turns which did not deal withthese tasks were labeled OOD and excluded.
Thelatter included gathering the caller?s name and lo-cation (?UserID?
), the most common OOD type.The corpus was annotated using an XMLschema to provide the following:?
15 information components were created(e.g., from, to, time, car-type).?
Each information component was given astatus: C (Confirmed), U (Unconfirmed),and NULL (Not known).?
Up to 5 routes may be under discussion atonce ?
the state tracked the route under dis-cussion (RUD), total number of routes (TR),and all information and status componentsfor each route.?
A component called flow tracked single-turn dialog flow information from the caller(e.g., yes, no, thank-you, silence).?
A component called goal tracked the (mostrecent) goal expressed by the user (e.g.,plan-route, how-far).
Goal is emptyunless explicitly set by the caller, and onlyone goal is tracked at a time.
No attempt ismade to indicate if/when a goal has beensatisfied.33 action types were identified, some of whichtake information slots as parameters (e.g., wh-question, implicit-confirmation) .The corpus gave no indication of database in-teractions other than what can be inferred from thedialog transcripts.
One common wizard actionasked the caller to ?please wait?
when the wizardwas waiting for a database response.
To accountfor this, we provided an additional state componentwhich indicated whether the database was workingcalled db-request, which was set to truewhenever the action taken was please-waitand false otherwise.
Other less common databaseinteractions occurred when town names were am-biguous or not found, and no attempt was made toincorporate this information into the state represen-tation.The state space was constructed using only thestatus of the information slots (not the values); ofthe 15, 4 were occasionally expressed (e.g., day ofthe week) but not used to complete the transactionand were therefore excluded from the state space.Two turns of wizard action history were also in-corporated.
This formulation of the state spaceleads to approximately 1033 distinct states.For evaluation of the method, a hand-craftedpolicy of 30 rules mapping states to actions wascreated by inspecting the dialogs.15 ResultsTable 1 shows in-domain vs. out-of-domain wizardand caller turns.
Figures 1 through 4 show countsof flow values, goal values, action values, and state1 It was not clear in what situations some of the actions shouldbe used, so some (rare) actions were not covered by the rules.components, respectively.
The most common ac-tion type was ?please-wait?
(14.6% of actions).TurntypeTotal IndomainOOD:User IDOOD:OtherWiz-ard3155(100%)2410(76.4%)594(18.8%)151(4.8%)Caller 2466(100%)1713(69.5%)561(22.7%)192(7.8%)Table 1: In-domain and Out-of-domain (OOD) turnsCriteria States VisitsVisited onlyonce1182(85.7%)1182(45.9%)Visited morethan oncewithout a con-flict96(7.0%)353(13.7%)Visited morethan once withconflict101(7.3%)1041(40.3%)TOTAL 1379(100%)2576(100%)Table 2: ?Conflicts?
by state and visitsEstimated action probabilities  Visitsp(action taken | state) > p(anyother action | state)774 (74.3%)p(action taken | state) = p(oneor more other actions | state) >p(all remaining actions | state)119 (11.4%)p(action taken | state) <p(another action | state)148 (14.2%)TOTAL 1041 (100%)Table 3: Estimated probabilities in ?conflict?
statesEngine Class PrecisionAction-type only 72.7% jBNCAction-type & parameters 66.7%Action-type only 79.1% C4.5Action-type & parameters 72.9%Action-type only 58.4% Hand-craft Action-type & parameters 53.9%Table 4: Results from SL training and evaluationIn some cases, the wizard took different actionsin the same state; we labeled this situation a ?con-flict.?
Table 2 shows the number of distinct statesthat were encountered and, for states visited morethan once, whether conflicting actions were se-lected.
Of states with conflicts, Table 3 showsprobabilities estimated from the corpus.The interaction data was then submitted to 2 SLpattern classifiers ?
c4.5 using decision-trees(Quinlan, 1992) and jBNC using Na?ve Bayesians(Sacha, 2003).
Table 4 shows both algorithms?
10-fold cross validation classification error ratesclassifying (1) the action type, and (2) the actiontype with parameters, as well as the results for thehand-crafted policy.Figure 5 show the 10-fold cross validation clas-sification error rates for varying amounts of train-ing data for the two SL algorithms classifyingaction-type and parameters.6 DiscussionThe majority of the data collected was ?usable?
:although 26.7% of turns were excluded, 20.5% ofthese were due to a well-defined task not understudy here (user identification), and only 6.1% felloutside of designated tasks.
That said, it may bedesirable to impose a minimum threshold on howmany times a flow, goal, or action must be ob-served before adding it to the state space or actionset given the ?long tails?
of these elements.0501001502001 2 3 4 5 6 7 8 9 10 11 12Flow component IDDialogscontainingFlowIDFigure 1: Dialogs containing flow components02468101214161 2 3 4 5 6 7 8 9 10 11 12 13Goal component IDDialogscontainingGoalFigure 2: Dialogs containing goal componentsAbout half of the turns took place in stateswhich were visited only once.
This confirms thatmassive amounts of data would be needed to ob-serve all valid dialog states, and suggests dialogsdo not confine themselves to familiar states.Within a given state, the wizard?s behavior isstochastic, occasionally deviating from an other-wise static policy.
Some of this behavior resultsfrom database information not included in the cor-pus and state space; in other cases, the wizard ismaking apparently random choices.0501001502001 5 9 13 17 21 25 29 33Action IDDialogscontainingActionFigure 3: Dialogs containing action types0501001502001 3 5 7 9 11 13Component IDDialogscontainingcomponentFigure 4: Dialogs containing information componentsFigure 5 implies that a relatively small numberof dialogs (several hundred turns, or about 30-40dialogs) contain the vast majority of informationrelevant to SL algorithms ?
less than expected.Correctly predicting the wizard?s action in 72.9%of turns is significantly better than the 58.4% cor-rect prediction rate from the handcrafted policy.When a caller allows the system to retain initia-tive, the policy learned by the c4.5 algorithm han-dled enquiries about single trips perfectly.
Policyerrors start to occur as the user takes more initia-tive, entering less well observed states.Hand examination of a small number of mis-classified actions indicate that about half of theactions were ?reasonable?
?
e.g., including an extraitem in a confirmation.
Hand examination alsoconfirmed that the wizard?s non-deterministic be-havior and lack of database information resulted inmisclassifications.Other sources of mis-classifications derivedprimarily from under-account of the user?s goaland other deficiencies in the expressiveness of thestate space.7 Conclusion & future workThis work has proposed a method for determiningmany of the basic elements of a RL-based spokendialog system with minimal input from dialog de-signers using a ?talking wizard.?
The viability ofthe model has been tested with an existing corpusand shown to perform significantly better than ahand-crafted policy and with less effort to create.Future research will explore refining this ap-proach vis-?-vis user goal, applying this method toactual RL-based systems and finding suitablemethods for parameterized value functionsReferencesA.
J?nsson and N. Dahlbick.
1988.
Talking to A Com-puter is Not Like Talking To Your Best Friend.Proceedings of the Scandinavian Conference onceedings of the Scandinavian Conference onArtificial Intelligence '88, pp.
53-68.Staffan Larsson, Arne J?nsson and Lena Santamarta.2000.
Using the process of distilling dialogues tounderstand dialogue systems.
ICSLP 2000, Beijing.Ester Levin, Roberto Pieraccini and Wieland Eckert.2000.
A Stochastic Model of Human-Machine Inter-action for Learning Dialogue Structures.
IEEETrans on Speech and Audio Processing 8(1):11-23.R.
K. Moore and S. R. Browning.
1992.
Results of anexercise to collect ?genuine?
spoken enquiries usingWizard of Oz techniques.
Proc.
of the Inst.
of Acous-tics.Ross Quinlan.
1992.
C4.5 Release 8.
(Software pack-age).
http://www.cse.unsw.edu.au/~quinlan/Jarek P. Sacha.
2003.  jBNC version 1.0.
(Softwarepackage).
http://sourceforge.net/projects/jbnc/.Satinder Singh, Diane Litman, Michael Kearns, MarilynWalker.
2002.
Optimizing Dialogue Managementwith Reinforcement Learning: Experiments with theNJFun System.
Journal of Artificial Intelligence Re-search, vol 16, 105-133.Richard S. Sutton and Andrew G. Barto.
1998.
Rein-forcement Learning: an Introduction.
The MITPress, Cambridge, Massachusetts, USA.Marilyn A. Walker, Jeanne C. Fromer, Shrikanth Nara-yanan.
1998.
Learning Optimal Dialogue Strate-gies: A Case Study of a Spoken Dialogue Agent forEmail.
Proc.
36th Annual Meeting of the ACM and17th Int?l Conf.
on Comp.
Linguistics, 1345--1352.20.0%30.0%40.0%50.0%60.0%70.0%80.0%0 500 1000 1500 2000 2500Training examples (dialog turns)Classificationerrors(%)c4.5Naive BayesFigure 5: Classification errors vs. training samples for action-type & parameters
