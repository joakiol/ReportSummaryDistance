Proceedings of NAACL HLT 2009: Short Papers, pages 17?20,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsComparison of Extended Lexicon Models in Search and Rescoring for SMTSa?sa Hasan and Hermann NeyHuman Language Technology and Pattern Recognition GroupChair of Computer Science 6, RWTH Aachen University, Germany{hasan,ney}@cs.rwth-aachen.deAbstractWe show how the integration of an extendedlexicon model into the decoder can improvetranslation performance.
The model is basedon lexical triggers that capture long-distancedependencies on the sentence level.
The re-sults are compared to variants of the modelthat are applied in reranking of n-best lists.We present how a combined application ofthese models in search and rescoring givespromising results.
Experiments are reportedon the GALE Chinese-English task with im-provements of up to +0.9% BLEU and -1.5%TER absolute on a competitive baseline.1 IntroductionPhrase-based statistical machine translation has im-proved significantly over the last decade.
The avail-ability of large amounts of parallel data and access toopen-source software allow for easy setup of trans-lation systems with acceptable performance.
Pub-lic evaluations such as the NIST MT Eval or theWMT Shared Task help to measure overall progresswithin the community.
Most of the groups use aphrase-based decoder (e.g.
Pharaoh or the more re-cent Moses) based on a log-linear fusion of modelsthat enable the avid researcher to quickly incorpo-rate additional features and investigate the effect ofadditional knowledge sources to guide the search forbetter translation hypotheses.In this paper, we deal with an extended lexiconmodel and its incorporation into a state-of-the-artdecoder.
We compare the results of the integrationto a similar setup used within a rescoring frame-work and show the benefits of integrating additionalmodels directly into the search process.
As willbe shown, although a rescoring framework is suit-able for obtaining quick trends of incorporating ad-ditional models into a system, an alternative that in-cludes the model in search should be preferred.
Theintegration does not only yield better performance,we will also show the benefit of combining both ap-proaches in order to boost translation quality evenmore.
The extended lexicon model which we applyis motivated by a trigger-based approach (Hasan etal., 2008).
A standard lexicon modeling dependen-cies of target and source words, i.e.
p(e|f), is ex-tended with a second trigger f?on the source side,resulting in p(e|f, f?).
This model allows for a morefine-grained lexical choice of the target word de-pending on the additional source word f?.
Since thesecond trigger can move over the whole sentence,we capture global (sentence-level) context that is notmodeled in local n-grams of the language model orin bilingual phrase pairs that cover only a limitedamount of consecutive words.Related work A similar approach has been triedin the word-sense disambiguation (WSD) domainwhere local but also across-sentence unigram collo-cations of words are used to refine phrase pair selec-tion dynamically by incorporating scores from theWSD classifier (Chan et al, 2007).
A maximum-entropy based approach with different features ofsurrounding words that are locally bound to a con-text of three positions to the left and right is re-ported in (Garc?
?a-Varea et al, 2001).
A logisticregression-based word translation model is investi-gated by Vickrey et al (2005) but has not been eval-uated on a machine translation task.
Another WSDapproach incorporating context-dependent phrasaltranslation lexicons is presented by Carpuat and Wu(2007) and has been evaluated on several translation17tasks.
The triplet lexicon model presented in thiswork can also be interpreted as an extension of thestandard IBM model 1 (Brown et al, 1993) with anadditional trigger.2 SetupThe main focus of this work investigates an extendedlexicon model in search and rescoring.
The modelthat we consider here and its integration in the de-coder and setup for rescoring are presented in thefollowing sections.2.1 Extended lexicon modelThe triplets of the extended lexicon model p(e|f, f?
)are composed of two words in the source languagetriggering one target word.
In order to limit the over-all number of triplets, we apply a training constraintthat reuses the word alignment information obtainedin the GIZA++step.
For source words f , we onlyconsider the ones that are aligned to a target word egiven the GIZA++word alignment.
The second trig-ger f?is allowed to move over the whole source sen-tence, thus capturing long-distance effects that canbe observed in the training data:p(eI1|fJ1, {aij}) =I?i=1p(ei|fJ1, {aij}) =I?i=11Zi?j?
{ai}J?j?=1p(ei|fj, fj?)
(1)where {aij} denotes the alignment matrix of the sen-tence pair fJ1and eI1and the first sum goes over allfjthat are aligned to the current ei(expressed asj ?
{ai}).
The factor Zi= J ?
|{ai}| normalizesthe double summation accordingly.
Eq.
1 is used inthe iterative EM training on all sentence pairs of thetraining data.
Empty words are allowed on the trig-gering part and low probability triplets are trimmed.2.2 DecodingRegarding the search, we can apply this model di-rectly when scoring bilingual phrase pairs.
Given atrained model for p(e|f, f?
), we compute the featurescore htof a phrase pair (e?,?f) asht(e?,?f, {a?ij}, fJ1) = (2)??ilog?j?
{a?i}?j?p(e?i|?fj, fj?)
+?ilogZiwhere i moves over all target words in the phrase e?,the sum over j selects the aligned source words?fjgiven {a?ij}, the alignment matrix within the phrasepair, and j?incorporates the whole source sentencefJ1.
Analogous to Eq.
1, Zi= J ?
|{a?i}| denotesthe number of overall source words times the num-ber of aligned source words to each e?i.
In Eq.
2,we take negative log-probabilities and normalize toobtain the final score (representing costs) for thegiven phrase pair.
Note that in search, we can onlyuse this direction, p(e|f, f?
), since the whole sourcesentence is available for triggering effects whereasnot all target words have been generated so far,as it would be necessary for the reverse direction,p(f |e, e?).
Due to data sparseness, we smooth themodel by using a floor value of 10?7for unseenevents during decoding.
Furthermore, an implicitbackoff to IBM1 exists if the second trigger is theempty word, i.e.
for events of the form p(e|f, ?
).2.3 RescoringIn rescoring, we constrain the scoring of our hy-potheses to a limited set of n-best translations thatare extracted from the word graph, a pruned com-pact representation of the search space.
The advan-tage of n-best list rescoring is the full availability ofboth source text and target translation, thus allow-ing for the application of additional (possibly morecomplex) models that are hard to implement directlyin search, such as e.g.
syntactic models based onparsers or huge LMs that would not fit in memoryduring decoding.
Since we are limiting ourselves toa small extract of translation hypotheses, rescoringmodels cannot outperform the same models if ap-plied directly in search.
One advantage though isthat we can apply the introduced trigger model alsoin the other direction, i.e.
using p(f |e, e?
), where twotarget words trigger one source word.
Generally, thecombination of two directions of a model yields fur-ther improvements, so we investigated how this ad-ditional direction helps in rescoring (cf.
Section 3.1).In our experiments, we use 10 000-best lists ex-tracted from the word graphs.
An initial setting usesthe baseline system, whereas a comparative setup in-corporates the (e|f, f?)
direction of the trigger lexi-con model in search and adds the reversed directionin rescoring.
Additionally, we use n-gram posteri-ors, a sentence length model and two large language18train (ch/en) test08 (NW/WT)Sent.
pairs 9.1M 480 490Run.
words 259M/300M 14.8K 12.3KVocabulary 357K/627K 3.6K 3.2KTable 1: GALE Chinese-English corpus statistics.models, a 5-gram count LM trained on 2.5G runningwords and the Google Web 1T 5-grams.
The featureweights of the log-linear mix are tuned on a separatedevelopment set using the Downhill Simplex algo-rithm.3 ExperimentsThe experiments are carried out with a GALE sys-tem using the official development and test sets ofthe GALE 2008 evaluation.
The corpus statisticsare shown in Table 1.
The triplet lexicon model wastrained on a subset of the overall data.
We used 1.4Msentence pairs with 32.3M running words on the En-glish side.
The vocabulary sizes were 76.5K for thesource and 241.7K for the target language.
The finallexicon contains roughly 62 million triplets.The baseline system incorporates the standardmodel setup used in phrase-based SMT which com-bines phrase translation and word lexicon modelsin both directions, a 5-gram language model, wordand phrase penalties, and two models for reorder-ing (a standard distortion model and a discriminativephrase orientation model).
For a fair comparison, wealso added the related IBM model 1 p(e|f) to thebaseline since it can be computed on the sentence-level for this direction, target given source.
This stepachieves +0.5% BLEU on the development set fornewswire but has no effect on test.
As will be pre-sented in the next section, the extension to anothertrigger results in improvements over this baseline,indicating that the extended triplet model is superiorto the standard IBM model 1.
The feature weightswere optimized on separate development sets forboth newswire and web text.We perform the following pipeline of experi-ments: A first run generates word graphs using thebaseline models.
From this word graph, we ex-tract 10k-best lists and compare the performance toa reranked version including the additional models.In a second step, we add one of the trigger lexi-Chinese-English newswire web textGALE test08 BLEU TER BLEU TERbaseline 32.5 59.4 25.8 64.0rescore, no triplets 32.8 59.0 26.6 63.5resc.
triplets fe+ef 33.2 58.6 27.1 63.0triplets in search ef 33.1 58.8 26.0 63.5rescore, no triplets 33.2 58.6 26.7 63.5rescore, triplets fe 33.7 58.1 27.2 62.0Table 2: Results obtained for the two test sets.
For thetriplet models, ?fe?
means p(f |e, e?)
and ?ef?
denotesp(e|f, f?).
BLEU/TER scores are shown in percent.con models to the search process, regenerate wordgraphs, extract updated n-best lists and add the re-maining models again in a reranking step.3.1 ResultsTable 2 presents results that were obtained on thetest sets.
All results are based on lowercase eval-uations since the system is trained on lowercaseddata in order to keep computational resources fea-sible.
For the newswire setting, the baseline is32.5% BLEU and 59.4% TER.
Rescoring with addi-tional models not including triplets gives only slightimprovements.
By adding the path-aligned tripletmodel in both directions, we observe an improve-ment of +0.7% BLEU and -0.8% TER.
Using thetriplet model in source to target direction (e, f, f?
)during the search process, we arrive at a similarBLEU improvement of +0.6% without any rerank-ing models.
We add the other direction of the triplets(f, e, e?)
(the one that can not be used directly insearch) and obtain 33.7% BLEU on the newswireset.
The overall cumulative improvements of tripletsin search and reranking are +0.9% BLEU and -0.9%TER when compared to the rescored baseline not in-corporating triplet models and +1.2%/-1.3% on thedecoder baseline, respectively.For the web text setting, the baseline is consid-erably lower at 25.8% BLEU and 64.0% TER (cf.right part of Table 2).
We observe an improvementfor the baseline reranking models, a large part ofwhich is due to the Google Web LM.
Adding tripletsto search does not help significantly (+0.2%/-0.5%BLEU/TER).
This might be due to training thetriplet lexicon mainly on newswire data.
Rerank-ing without triplets performs similar to the baseline19experiment.
Mixing in the (f, e, e?)
direction helpsagain: The final score comes out at 27.2% BLEUand 62.0% TER, the latter being significantly betterthan the reranked baseline (-1.5% in TER).3.2 DiscussionThe results indicate that it is worth moving modelsfrom rescoring to the search process.
This is notsurprising (and probably well known in the com-munity).
Interestingly, the triplet model can im-prove translation quality in addition to its relatedIBM model 1 which was already part of the base-line.
It seems that the extension by a second triggerhelps to capture some language specific propertiesfor Chinese-English which go beyond local lexical(word-to-word) dependencies.
In Table 3, we showan example of improved translation quality where atriggering effect can be observed.
Due to the topic ofthe sentence, the phrase local employment was cho-sen over own jobs.
One of the top triplets in this con-text is p(employment | ??
, ??
), where ?
?is ?employment?
due to the path-aligned constraintand ??
means ?talent?.
Note that the distance be-tween these two triggers is five tokens.4 ConclusionWe presented the integration of an extended lexiconmodel into the search process and compared it to avariant which was used in reranking n-best lists.
Inorder to keep the overall number of triplets feasi-ble, and thus memory footprints and training timeslow, we chose a path-constrained triplet model thatrestricts the first source trigger to the aligned targetword, whereas the second trigger can move alongthe whole source sentence.
The motivation was toallow for a more fine-grained lexical choice of tar-get words by looking at sentence-level context.
Theoverall improvements that can be accounted to thetriplets are up to +0.9% BLEU and -1.5% TER.In the future, we plan to investigate more tripletmodel variants and work on additional languagepairs such as French-English or German-English.The reverse direction, p(f |e, e?
), is hard to imple-ment outside of a reranking framework where thefull target hypotheses are already fully generated.
Itmight be worth looking at cross-lingual trigger mod-els such as p(f |e, f?)
or constrained variants likesource ???????????
,??????????????
.baseline germany, in order to protect their ownjobs, the introduction of foreign talent,a relatively high threshold.triplets in order to protect local employment,germany has a relatively high thresholdfor the introduction of foreign talent.reference in order to protect native employment,germany has set a relatively high thresh-old for bringing in foreign talents.Table 3: Translation example on the newswire test set.p(f |e, e?)
with e?< e, i.e.
the second trigger com-ing from the left context within a sentence which hasalready been generated.AcknowledgmentsThis material is partly based upon work supported by theDefense Advanced Research Projects Agency (DARPA)under Contract No.
HR0011-06-C-0023, and was partlyrealized as part of the Quaero Programme, funded byOSEO, French State agency for innovation.The authors would like to thank Juri Ganitkevitch fortraining the triplet model.ReferencesP.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics, 19(2):263?311, June.M.
Carpuat and D. Wu.
2007.
Improving statistical ma-chine translation using word sense disambiguation.
InProc.
EMNLP-CoNLL, Prague, Czech Republic, June.Y.
S. Chan, H. T. Ng, and D. Chiang.
2007.
Word sensedisambiguation improves statistical machine transla-tion.
In Proc.
ACL, pages 33?40, Prague, Czech Re-public, June.I.
Garc?
?a-Varea, F. J. Och, H. Ney, and F. Casacuberta.2001.
Refined lexicon models for statistical machinetranslation using a maximum entropy approach.
InProc.
ACL Data-Driven Machine Translation Work-shop, pages 204?211, Toulouse, France, July.S.
Hasan, J. Ganitkevitch, H. Ney, and J. Andr?es-Ferrer.2008.
Triplet lexicon models for statistical machinetranslation.
In Proc.
EMNLP, pages 372?381, Hon-olulu, Hawaii, October.D.
Vickrey, L. Biewald, M. Teyssier, and D. Koller.
2005.Word-sense disambiguation for machine translation.In Proc.
HLT-EMNLP, pages 771?778, Morristown,NJ, USA.20
