Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 25?32,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Bilingual Word Spectral Clustering for Statistical Machine TranslationBing Zhao?
Eric P. Xing?
?
Alex Waibel?
?Language Technologies Institute?Center for Automated Learning and DiscoveryCarnegie Mellon UniversityPittsburgh, Pennsylvania 15213{bzhao,epxing,ahw}@cs.cmu.eduAbstractIn this paper, a variant of a spectral clus-tering algorithm is proposed for bilingualword clustering.
The proposed algorithmgenerates the two sets of clusters for bothlanguages efficiently with high seman-tic correlation within monolingual clus-ters, and high translation quality acrossthe clusters between two languages.
Eachcluster level translation is considered asa bilingual concept, which generalizeswords in bilingual clusters.
This schemeimproves the robustness for statistical ma-chine translation models.
Two HMM-based translation models are tested to usethese bilingual clusters.
Improved per-plexity, word alignment accuracy, andtranslation quality are observed in our ex-periments.1 IntroductionStatistical natural language processing usually suf-fers from the sparse data problem.
Comparing tothe available monolingual data, we have much lesstraining data especially for statistical machine trans-lation (SMT).
For example, in language modelling,there are more than 1.7 billion words corpora avail-able: English Gigaword by (Graff, 2003).
However,for machine translation tasks, there are typically lessthan 10 million words of training data.Bilingual word clustering is a process of form-ing corresponding word clusters suitable for ma-chine translation.
Previous work from (Wang et al,1996) showed improvements in perplexity-orientedmeasures using mixture-based translation lexicon(Brown et al, 1993).
A later study by (Och,1999) showed improvements on perplexity of bilin-gual corpus, and word translation accuracy using atemplate-based translation model.
Both approachesare optimizing the maximum likelihood of parallelcorpus, in which a data point is a sentence pair: anEnglish sentence and its translation in another lan-guage such as French.
These algorithms are es-sentially the same as monolingual word clusterings(Kneser and Ney, 1993)?an iterative local search.In each iteration, a two-level loop over every possi-ble word-cluster assignment is tested for better like-lihood change.
This kind of approach has two draw-backs: first it is easily to get stuck in local op-tima; second, the clustering of English and the otherlanguage are basically two separated optimizationprocesses, and cluster-level translation is modelledloosely.
These drawbacks make their approachesgenerally not very effective in improving translationmodels.In this paper, we propose a variant of the spec-tral clustering algorithm (Ng et al, 2001) for bilin-gual word clustering.
Given parallel corpus, first, theword?s bilingual context is used directly as features- for instance, each English word is represented byits bilingual word translation candidates.
Second,latent eigenstructure analysis is carried out in thisbilingual feature space, which leads to clusters ofwords with similar translations.
Essentially an affin-ity matrix is computed using these cross-lingual fea-tures.
It is then decomposed into two sub-spaces,which are meaningful for translation tasks: the leftsubspace corresponds to the representation of wordsin English vocabulary, and the right sub-space cor-responds to words in French.
Each eigenvector isconsidered as one bilingual concept, and the bilin-gual clusters are considered to be its realizations intwo languages.
Finally, a general K-means cluster-25ing algorithm is used to find out word clusters in thetwo sub-spaces.The remainder of the paper is structured as fol-lows: in section 2, concepts of translation modelsare introduced together with two extended HMMs;in section 3, our proposed bilingual word cluster-ing algorithm is explained in detail, and the relatedworks are analyzed; in section 4, evaluation metricsare defined and the experimental results are given;in section 5, the discussions and conclusions.2 Statistical Machine TranslationThe task of translation is to translate one sentencein some source language F into a target language E.For example, given a French sentence with J wordsdenoted as fJ1 = f1f2...fJ , an SMT system auto-matically translates it into an English sentence withI words denoted by eI1 = e1e2...eI .
The SMT sys-tem first proposes multiple English hypotheses in itsmodel space.
Among all the hypotheses, the systemselects the one with the highest conditional proba-bility according to Bayes?s decision rule:e?I1 = argmax{eI1}P (eI1|fJ1 ) = argmax{eI1}P (fJ1 |eI1)P (eI1),(1)where P (fJ1 |eI1) is called translation model, andP (eI1) is called language model.
The translationmodel is the key component, which is the focus inthis paper.2.1 HMM-based Translation ModelHMM is one of the effective translation models (Vo-gel et al, 1996), which is easily scalable to verylarge training corpus.To model word-to-word translation, we introducethe mapping j ?
aj , which assigns a French wordfj in position j to a English word ei in positioni = aj denoted as eaj .
Each French word fj isan observation, and it is generated by a HMM statedefined as [eaj , aj], where the alignment aj for po-sition j is considered to have a dependency on theprevious alignment aj?1.
Thus the first-order HMMis defined as follows:P (fJ1 |eI1) =?aJ1J?j=1P (fj |eaj )P (aj |aj?1), (2)where P (aj |aj?1) is the transition probability.
Thismodel captures the assumption that words close inthe source sentence are aligned to words close inthe target sentence.
An additional pseudo word of?NULL?
is used as the beginning of English sen-tence for HMM to start with.
The (Och and Ney,2003) model includes other refinements such as spe-cial treatment of a jump to a Null word, and a uni-form smoothing prior.
The HMM with these refine-ments is used as our baseline.
Motivated by the workin both (Och and Ney, 2000) and (Toutanova et al,2002), we propose the two following simplest ver-sions of extended HMMs to utilize bilingual wordclusters.2.2 Extensions to HMM with word clustersLet F denote the cluster mapping fj ?
F(fj), whichassigns French word fj to its cluster ID Fj = F(fj).Similarly E maps English word ei to its cluster IDof Ei = E(ei).
In this paper, we assume each wordbelongs to one cluster only.With bilingual word clusters, we can extend theHMM model in Eqn.
1 in the following two ways:P (fJ1 |eI1) =?aJ1?Jj=1 P (fj |eaj )?P (aj |aj?1,E(eaj?1),F(fj?1)),(3)where E(eaj?1) and F(fj?1) are non overlappingword clusters (Eaj?1 , Fj?1)for English and Frenchrespectively.Another explicit way of utilizing bilingual wordclusters can be considered as a two-stream HMM asfollows:P (fJ1 , F J1 |eI1, EI1) =?aJ1?Jj=1 P (fj |eaj )P (Fj |Eaj )P (aj |aj?1).
(4)This model introduces the translation of bilingualword clusters directly as an extra factor to Eqn.
2.Intuitively, the role of this factor is to boost the trans-lation probabilities for words sharing the same con-cept.
This is a more expressive model because itmodels both word and the cluster level translationequivalence.
Also, compared with the model in Eqn.3, this model is easier to train, as it uses a two-dimension table instead of a four-dimension table.However, we do not want this P (Fj |Eaj ) to dom-inate the HMM transition structure, and the obser-26vation probability of P (fj |eaj ) during the EM itera-tions.
Thus a uniform prior P (Fj) = 1/|F | is intro-duced as a smoothing factor for P (Fj |Eaj ):P (Fj |Eaj ) = ?P (Fj |Eaj ) + (1?
?
)P (Fj), (5)where |F | is the total number of word clusters inFrench (we use the same number of clusters for bothlanguages).
?
can be chosen to get optimal perfor-mance on a development set.
In our case, we fix it tobe 0.5 in all our experiments.3 Bilingual Word ClusteringIn bilingual word clustering, the task is to build wordclusters F and E to form partitions of the vocabular-ies of the two languages respectively.
The two par-titions for the vocabularies of F and E are aimed tobe suitable for machine translation in the sense thatthe cluster/partition level translation equivalence isreliable and focused to handle data sparseness; thetranslation model using these clusters explains theparallel corpus {(fJ1 , eI1)} better in terms of perplex-ity or joint likelihood.3.1 From Monolingual to BilingualTo infer bilingual word clusters of (F,E), one canoptimize the joint probability of the parallel corpus{(fJ1 , eI1)} using the clusters as follows:(F?, E?)
= argmax(F,E)P (fJ1 , eI1|F,E)= argmax(F,E)P (eI1|E)P (fJ1 |eI1, F, E).(6)Eqn.
6 separates the optimization process into twoparts: the monolingual part for E, and the bilingualpart for F given fixed E. The monolingual part isconsidered as a prior probability:P (eI1|E), and E canbe inferred using corpus bigram statistics in the fol-lowing equation:E?
= argmax{E}P (eI1|E)= argmax{E}I?i=1P (Ei|Ei?1)P (ei|Ei).
(7)We need to fix the number of clusters beforehand,otherwise the optimum is reached when each wordis a class of its own.
There exists efficient leave-one-out style algorithm (Kneser and Ney, 1993), whichcan automatically determine the number of clusters.For the bilingual part P (fJ1 |eI1, F, E), we canslightly modify the same algorithm as in (Kneserand Ney, 1993).
Given the word alignment {aJ1}between fJ1 and eI1 collected from the Viterbi pathin HMM-based translation model, we can infer F?
asfollows:F?
= argmax{F}P (fJ1 |eI1, F,E)= argmax{F}J?j=1P (Fj |Eaj )P (fj |Fj).
(8)Overall, this bilingual word clustering algorithm isessentially a two-step approach.
In the first step, Eis inferred by optimizing the monolingual likelihoodof English data, and secondly F is inferred by op-timizing the bilingual part without changing E. Inthis way, the algorithm is easy to implement withoutmuch change from the monolingual correspondent.This approach was shown to give the best resultsin (Och, 1999).
We use it as our baseline to comparewith.3.2 Bilingual Word Spectral ClusteringInstead of using word alignment to bridge the par-allel sentence pair, and optimize the likelihood intwo separate steps, we develop an alignment-free al-gorithm using a variant of spectral clustering algo-rithm.
The goal is to build high cluster-level trans-lation quality suitable for translation modelling, andat the same time maintain high intra-cluster similar-ity , and low inter-cluster similarity for monolingualclusters.3.2.1 NotationsWe define the vocabulary VF as the French vo-cabulary with a size of |VF |; VE as the English vo-cabulary with size of |VE |.
A co-occurrence matrixC{F,E} is built with |VF | rows and |VE | columns;each element represents the co-occurrence counts ofthe corresponding French word fj and English wordei.
In this way, each French word forms a row vec-tor with a dimension of |VE |, and each dimensional-ity is a co-occurring English word.
The elements inthe vector are the co-occurrence counts.
We can also27view each column as a vector for English word, andwe?ll have similar interpretations as above.3.2.2 AlgorithmWith C{F,E}, we can infer two affinity matrixesas follows:AE = CT{F,E}C{F,E}AF = C{F,E}CT{F,E},where AE is an |VE | ?
|VE | affinity matrix for En-glish words, with rows and columns representingEnglish words and each element the inner productbetween two English words column vectors.
Corre-spondingly, AF is an affinity matrix of size |VF | ?|VF | for French words with similar definitions.
BothAE and AF are symmetric and non-negative.
Nowwe can compute the eigenstructure for both AE andAF .
In fact, the eigen vectors of the two are corre-spondingly the right and left sub-spaces of the orig-inal co-occurrence matrix of C{F,E} respectively.This can be computed using singular value decom-position (SVD): C{F,E} = USV T , AE = V S2V T ,and AF = US2UT , where U is the left sub-space,and V the right sub-space of the co-occurrence ma-trix C{F,E}.
S is a diagonal matrix, with the singularvalues ranked from large to small along the diagonal.Obviously, the left sub-space U is the eigenstructurefor AF ; the right sub-space V is the eigenstructurefor AE .By choosing the top K singular values (the squareroot of the eigen values for both AE and AF ), thesub-spaces will be reduced to: U|VF |?K and V|VE |?Krespectively.
Based on these subspaces, we can carryout K-means or other clustering algorithms to in-fer word clusters for both languages.
Our algorithmgoes as follows:?
Initialize bilingual co-occurrence matrixC{F,E} with rows representing French words,and columns English words.
Cji is the co-occurrence raw counts of French word fj andEnglish word ei;?
Form the affinity matrix AE = CT{F,E}C{F,E}and AF = CT{F,E}C{F,E}.
Kernels can also beapplied here such as AE = exp(C{F,E}CT{F,E}?2 )for English words.
Set AEii = 0 and AF ii = 0,and normalize each row to be unit length;?
Compute the eigen structure of the normalizedmatrix AE , and find the k largest eigen vectors:v1, v2, ..., vk; Similarly, find the k largest eigenvectors of AF : u1, u2, ..., uk;?
Stack the k eigenvectors of v1, v2, ..., vk inthe columns of YE , and stack the eigenvectorsu1, u2, ..., uk in the columns for YF ; Normalizerows of both YE and YF to have unit length.
YEis size of |VE | ?
k and YF is size of |VF | ?
k;?
Treat each row of YE as a point in R|VE |?k, andcluster them into K English word clusters us-ing K-means.
Treat each row of YF as a point inR|VF |?k, and cluster them into K French wordclusters.?
Finally, assign original word ei to cluster Ekif row i of the matrix YE is clustered as Ek;similar assignments are for French words.Here AE and AF are affinity matrixes of pair-wiseinner products between the monolingual words.
Themore similar the two words, the larger the value.In our implementations, we did not apply a kernelfunction like the algorithm in (Ng et al, 2001).
Butthe kernel function such as the exponential func-tion mentioned above can be applied here to controlhow rapidly the similarity falls, using some carefullychosen scaling parameter.3.2.3 Related Clustering AlgorithmsThe above algorithm is very close to the variantsof a big family of the spectral clustering algorithmsintroduced in (Meila and Shi, 2000) and studied in(Ng et al, 2001).
Spectral clustering refers to a classof techniques which rely on the eigenstructure ofa similarity matrix to partition points into disjointclusters with high intra-cluster similarity and lowinter-cluster similarity.
It?s shown to be computingthe k-way normalized cut: K ?
trY TD?
12AD?
12Yfor any matrix Y ?
RM?N .
A is the affinity matrix,and Y in our algorithm corresponds to the subspacesof U and V .Experimentally, it has been observed that usingmore eigenvectors and directly computing a k-waypartitioning usually gives better performance.
In ourimplementations, we used the top 500 eigen vectorsto construct the subspaces of U and V for K-meansclustering.283.2.4 K-meansThe K-means here can be considered as a post-processing step in our proposed bilingual word clus-tering.
For initial centroids, we first compute thecenter of the whole data set.
The farthest centroidfrom the center is then chosen to be the first initialcentroid; and after that, the other K-1 centroids arechosen one by one to well separate all the previouschosen centroids.The stopping criterion is: if the maximal changeof the clusters?
centroids is less than the threshold of1e-3 between two iterations, the clustering algorithmthen stops.4 ExperimentsTo test our algorithm, we applied it to the TIDESChinese-English small data track evaluation test set.After preprocessing, such as English tokenization,Chinese word segmentation, and parallel sentencesplitting, there are in total 4172 parallel sentencepairs for training.
We manually labeled word align-ments for 627 test sentence pairs randomly sampledfrom the dry-run test data in 2001, which has fourhuman translations for each Chinese sentence.
Thepreprocessing for the test data is different from theabove, as it is designed for humans to label wordalignments correctly by removing ambiguities fromtokenization and word segmentation as much as pos-sible.
The data statistics are shown in Table 1.English ChineseTrainSent.
Pairs 4172Words 133598 105331Voc Size 8359 7984TestSent.
Pairs 627Words 25500 19726Voc Size 4084 4827Unseen Voc Size 1278 1888Alignment Links 14769Table 1: Training and Test data statistics4.1 Building Co-occurrence MatrixBilingual word co-occurrence counts are collectedfrom the training data for constructing the matrixof C{F,E}.
Raw counts are collected without wordalignment between the parallel sentences.
Practi-cally, we can use word alignment as used in (Och,1999).
Given an initial word alignment inferred byHMM, the counts are collected from the alignedword pair.
If the counts are L-1 normalized, thenthe co-occurrence matrix is essentially the bilingualword-to-word translation lexicon such as P (fj |eaj ).We can remove very small entries (P (f |e) ?
1e?7),so that the matrix of C{F,E} is more sparse for eigen-structure computation.
The proposed algorithm isthen carried out to generate the bilingual word clus-ters for both English and Chinese.Figure 1 shows the ranked Eigen values for theco-occurrence matrix of C{F,E}.0 100 200 300 400 500 600 700 800 900 10000.511.522.533.5 Eigen values of affinity matricesTop 1000 Eigen ValuesEigen Values(a) co?occur counts from init word alignment(b) raw co?occur counts from dataFigure 1: Top-1000 Eigen Values of Co-occurrenceMatrixIt is clear, that using the initial HMM word align-ment for co-occurrence matrix makes a difference.The top Eigen value using word alignment in plot a.
(the deep blue curve) is 3.1946.
The two plateausindicate how many top K eigen vectors to choose toreduce the feature space.
The first one indicates thatK is in the range of 50 to 120, and the second plateauindicates K is in the range of 500 to 800.
Plot b. isinferred from the raw co-occurrence counts with thetop eigen value of 2.7148.
There is no clear plateau,which indicates that the feature space is less struc-tured than the one built with initial word alignment.We find 500 top eigen vectors are good enoughfor bilingual clustering in terms of efficiency and ef-fectiveness.294.2 Clustering ResultsClusters built via the two described methods arecompared.
The first method bil1 is the two-step op-timization approach: first optimizing the monolin-gual clusters for target language (English), and af-terwards optimizing clusters for the source language(Chinese).
The second method bil2 is our proposedalgorithm to compute the eigenstructure of the co-occurrence matrix, which builds the left and rightsubspaces, and finds clusters in such spaces.
Top500 eigen vectors are used to construct these sub-spaces.
For both methods, 1000 clusters are inferredfor English and Chinese respectively.
The numberof clusters is chosen in a way that the final wordalignment accuracy was optimal.
Table 2 providesthe clustering examples using the two algorithms.settings cluster examplesmono-E1 entirely,mainly,merelymono-E210th,13th,14th,16th,17th,18th,19th20th,21st,23rd,24th,26thmono-E3 drink,anglophobia,carota,giant,gymnasiumbil1-C3 ?,d,?,?,?
?,yQ,ybil2-E1 alcoholic cognac distilled drinkscotch spirits whiskeybil2-C1 ??,?,,??,2,?y,?h,7,??
},6,?,,kbil2-E2 evrec harmony luxury people sedan sedanstour tourism tourist toward travelbil2-C2 ??
,s?,?,?
(,ff?,u?,@q,@?,|,|?,-|Table 2: Bilingual Cluster ExamplesThe monolingual word clusters often containwords with similar syntax functions.
This hap-pens with esp.
frequent words (eg.
mono-E1 andmono-E2).
The algorithm tends to put rare wordssuch as ?carota, anglophobia?
into a very big cluster(eg.
mono-E3).
In addition, the words within thesemonolingual clusters rarely share similar transla-tions such as the typical cluster of ?week, month,year?.
This indicates that the corresponding Chi-nese clusters inferred by optimizing Eqn.
7 are notclose in terms of translational similarity.
Overall, themethod of bil1 does not give us a good translationalcorrespondence between clusters of two languages.The English cluster of mono-E3 and its best alignedcandidate of bil1-C3 are not well correlated either.Our proposed bilingual cluster algorithm bil2generates the clusters with stronger semantic mean-ing within a cluster.
The cluster of bil2-E1 relatesto the concept of ?wine?
in English.
The mono-lingual word clustering tends to scatter those wordsinto several big noisy clusters.
This cluster also has agood translational correspondent in bil2-C1 in Chi-nese.
The clusters of bil2-E2 and bil2-C2 are alsocorrelated very well.
We noticed that the Chineseclusters are slightly more noisy than their Englishcorresponding ones.
This comes from the noise inthe parallel corpus, and sometimes from ambiguitiesof the word segmentation in the preprocessing steps.To measure the quality of the bilingual clusters,we can use the following two kind of metrics:?
Average ?-mirror (Wang et al, 1996): The ?-mirror of a class Ei is the set of clusters inChinese which have a translation probabilitygreater than ?.
In our case, ?
is 0.05, the samevalue used in (Och, 1999).?
Perplexity: The perplexity is defined as pro-portional to the negative log likelihood of theHMM model Viterbi alignment path for eachsentence pair.
We use the bilingual word clus-ters in two extended HMM models, and mea-sure the perplexities of the unseen test data af-ter seven forward-backward training iterations.The two perplexities are defined as PP1 =exp(?
?Jj=1 log(P (fj |eaj )P (aj |aj?1, Eaj?1 ,Fj?1))/J) and PP2 = exp(?J?1?Jj=1 log(P (fj |eaj )P (aj |aj?1)P (Fj?1|Eaj?1))) for thetwo extended HMM models in Eqn 3 and 4.Both metrics measure the extent to which the trans-lation probability is spread out.
The smaller the bet-ter.
The following table summarizes the results on?-mirror and perplexity using different methods onthe unseen test data.algorithms ?-mirror HMM-1 Perp HMM-2 Perpbaseline - 1717.82bil1 3.97 1810.55 352.28bil2 2.54 1610.86 343.64The baseline uses no word clusters.
bil1 and bil2are defined as above.
It is clear that our proposedmethod gives overall lower perplexity: 1611 fromthe baseline of 1717 using the extended HMM-1.If we use HMM-2, the perplexity goes down evenmore using bilingual clusters: 352.28 using bil1, and343.64 using bil2.
As stated, the four-dimensional30table of P (aj |aj?1, E(eaj?1), F (fj?1)) is easilysubject to overfitting, and usually gives worse per-plexities.Average ?-mirror for the two-step bilingual clus-tering algorithm is 3.97, and for spectral cluster-ing algorithm is 2.54.
This means our proposed al-gorithm generates more focused clusters of transla-tional equivalence.
Figure 2 shows the histogram forthe cluster pairs (Fj , Ei), of which the cluster leveltranslation probabilities P (Fj |Ei) ?
[0.05, 1].
Theinterval [0.05, 1] is divided into 10 bins, with first bin[0.05, 0.1], and 9 bins divides[0.1, 1] equally.
Thepercentage for clusters pairs with P (Fj |Ei) fallingin each bin is drawn.Histogram of (F,E) pairs with P(F|E) > 0.0500.10.20.30.40.50.60.05 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9Ten bins for P(F|E) ranging from [0.05, 1.0]spec-bi-clusteringtwo-step-bi-clusteringFigure 2: Histogram of cluster pairs (Fj , Ei)Our algorithm generates much better aligned clus-ter pairs than the two-step optimization algorithm.There are 120 cluster pairs aligned with P (Fj |Ei) ?0.9 using clusters from our algorithm, while thereare only 8 such cluster pairs using the two-step ap-proach.
Figure 3 compares the ?-mirror at differentnumbers of clusters using the two approaches.
Ouralgorithm has a much better ?-mirror than the two-step approach over different number of clusters.Overall, the extended HMM-2 is better thanHMM-1 in terms of perplexity, and is easier to train.4.3 Applications in Word AlignmentWe also applied our bilingual word clustering in aword alignment setting.
The training data is theTIDES small data track.
The word alignments aremanually labeled for 627 sentences sampled fromthe dryrun test data in 2001.
In this manuallyaligned data, we include one-to-one, one-to-many,and many-to-many word alignments.
Figure 4 sum-marizes the word alignment accuracy for differente-mirror over different settings00.511.522.533.544.50 200 400 600 800 1000 1200 1400 1600 1800 2000number of clusterse-mirrorBIL2: Co-occur raw countsBIL2: Co-occur counts from init word-alignBIL1: Two-step optimizationFigure 3: ?-mirror with different settingsmethods.
The baseline is the standard HMM trans-lation model defined in Eqn.
2; the HMM1 is de-fined in Eqn 3, and HMM2 is defined in Eqn 4.
Thealgorithm is applying our proposed bilingual wordclustering algorithm to infer 1000 clusters for bothlanguages.
As expected, Figure 4 shows that usingF-measure of word alignment38.00%39.00%40.00%41.00%42.00%43.00%44.00%45.00%1 2 3 4 5 6 7HMM Viterbi IterationsF-measureBaseline HMMExtended HMM-1Extended HMM-2Figure 4: Word Alignment Over Iterationsword clusters is helpful for word alignment.
HMM2gives the best performance in terms of F-measure ofword alignment.
One quarter of the words in the testvocabulary are unseen as shown in Table 1.
Theseunseen words related alignment links (4778 out of14769) will be left unaligned by translation models.Thus the oracle (best possible) recall we could getis 67.65%.
Our standard t-test showed that signifi-cant interval is 0.82% at the 95% confidence level.The improvement at the last iteration of HMM ismarginally significant.4.4 Applications in Phrase-based TranslationsOur pilot word alignment on unseen data showedimprovements.
However, we find it more effectivein our phrase extraction, in which three key scores31are computed: phrase level fertilities, distortions,and lexicon scores.
These scores are used in a lo-cal greedy search to extract phrase pairs (Zhao andVogel, 2005).
This phrase extraction is more sen-sitive to the differences in P (fj |ei) than the HMMViterbi word aligner.The evaluation conditions are defined in NIST2003 Small track.
Around 247K test set (919 Chi-nese sentences) specific phrase pairs are extractedwith up to 7-gram in source phrase.
A trigramlanguage model is trained using Gigaword XinHuanews part.
With a monotone phrase-based decoder,the translation results are reported in Table 3.
TheEval.
Baseline Bil1 Bil2NIST 6.417 6.507 6.582BLEU 0.1558 0.1575 0.1644Table 3: NIST?03 C-E Small Data Track Evaluationbaseline is using the lexicon P (fj |ei) trained fromstandard HMM in Eqn.
2, which gives a BLEUscore of 0.1558 +/- 0.0113.
Bil1 and Bil2 are usingP (fj |ei) from HMM in Eqn.
4 with 1000 bilingualword clusters inferred from the two-step algorithmand the proposed one respectively.
Using the clus-ters from the two-step algorithm gives a BLEU scoreof 0.1575, which is close to the baseline.
Using clus-ters from our algorithm, we observe more improve-ments with BLEU score of 0.1644 and a NIST scoreof 6.582.5 Discussions and ConclusionsIn this paper, a new approach for bilingual wordclustering using eigenstructure in bilingual featurespace is proposed.
Eigenvectors from this featurespace are considered as bilingual concepts.
Bilin-gual clusters from the subspaces expanded by theseconcepts are inferred with high semantic correla-tions within each cluster, and high translation quali-ties across clusters from the two languages.Our empirical study also showed effectiveness ofusing bilingual word clusters in extended HMMs forstatistical machine translation.
The K-means basedclustering algorithm can be easily extended to do hi-erarchical clustering.
However, extensions of trans-lation models are needed to leverage the hierarchicalclusters appropriately.ReferencesP.F.
Brown, Stephen A. Della Pietra, Vincent.
J.Della Pietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: Parameter es-timation.
In Computational Linguistics, volume 19(2),pages 263?331.David Graff.
2003.
Ldc gigaword corpora: English gi-gaword (ldc catalog no: Ldc2003t05).
In LDC link:http://www.ldc.upenn.edu/Catalog/index.jsp.R.
Kneser and Hermann Ney.
1993.
Improved clus-tering techniques for class-based statistical languagemodelling.
In European Conference on Speech Com-munication and Technology, pages 973?976.Marina Meila and Jianbo Shi.
2000.
Learning segmenta-tion by random walks.
In Advances in Neural Informa-tion Processing Systems.
(NIPS2000), pages 873?879.A.
Ng, M. Jordan, and Y. Weiss.
2001.
On spectralclustering: Analysis and an algorithm.
In Advances inNeural Information Processing Systems 14: Proceed-ings of the 2001.Franz J. Och and Hermann Ney.
2000.
A comparison ofalignment models for statistical machine translation.In COLING?00: The 18th Int.
Conf.
on ComputationalLinguistics, pages 1086?1090, Saarbrucken, Germany,July.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.
InComputational Linguistics, volume 29, pages 19?51.Franz J. Och.
1999.
An efficient method for determin-ing bilingal word classes.
In Ninth Conf.
of the Europ.Chapter of the Association for Computational Linguis-tics (EACL?99), pages 71?76.Kristina Toutanova, H. Tolga Ilhan, and Christopher D.Manning.
2002.
Extensions to hmm-based statisticalword alignment models.
In Proc.
of the Conference onEmpirical Methods in Natural Language Processing.S.
Vogel, Hermann Ney, and C. Tillmann.
1996.
Hmmbased word alignment in statistical machine transla-tion.
In Proc.
The 16th Int.
Conf.
on ComputationalLingustics, (Coling?96), pages 836?841.Yeyi Wang, John Lafferty, and Alex Waibel.
1996.Word clustering with parallel spoken language cor-pora.
In proceedings of the 4th International Con-ference on Spoken Language Processing (ICSLP?96),pages 2364?2367.Bing Zhao and Stephan Vogel.
2005.
A generalizedalignment-free phrase extraction algorithm.
In ACL2005 Workshop: Building and Using Parallel Cor-pora: Data-driven Machine Translation and Beyond,Ann Arbor, Michigan.32
