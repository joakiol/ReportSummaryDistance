Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 399?405,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsHunting for Troll Comments in News Community ForumsTodor MihaylovInstitute for Computational Linguistics?Heidelberg UniversityHeidelberg, Germanymihaylov@cl.uni-heidelberg.dePreslav NakovQatar Computing Research InstituteHamad bin Khalifa UniversityP.O.
box 5825, Doha, Qatarpnakov@qf.org.qaAbstractThere are different definitions of what atroll is.
Certainly, a troll can be somebodywho teases people to make them angry, orsomebody who offends people, or some-body who wants to dominate any singlediscussion, or somebody who tries to ma-nipulate people?s opinion (sometimes formoney), etc.
The last definition is the onethat dominates the public discourse in Bul-garia and Eastern Europe, and this is ourfocus in this paper.In our work, we examine two types ofopinion manipulation trolls: paid trollsthat have been revealed from leaked ?rep-utation management contracts?
and ?men-tioned trolls?
that have been called suchby several different people.
We show thatthese definitions are sensible: we buildtwo classifiers that can distinguish a postby such a paid troll from one by a non-trollwith 81-82% accuracy; the same classi-fier achieves 81-82% accuracy on so calledmentioned troll vs. non-troll posts.1 IntroductionThe practice of using Internet trolls for opinionmanipulation has been reality since the rise of In-ternet and community forums.
It has been shownthat user opinions about products, companies andpolitics can be influenced by opinions posted byother online users in online forums and social net-works (Dellarocas, 2006).
This makes it easy forcompanies and political parties to gain popularityby paying for ?reputation management?
to peo-ple that write in discussion forums and social net-works fake opinions from fake profiles.
?This research started in the Sofia University.Opinion manipulation campaigns are oftenlaunched using ?personal management software?that allows a user to open multiple accounts and toappear like several different people.
Over time,some forum users developed sensitivity abouttrolls, and started publicly exposing them.
Yet, itis hard for forum administrators to block them astrolls try formally not to violate the forum rules.In our work, we examine two types of opinionmanipulation trolls: paid trolls that have been re-vealed from leaked ?reputation management con-tracts?1and ?mentioned trolls?
that have beencalled such by several different people.2 Related WorkTroll detection was addressed using analysis ofthe semantics in posts (Cambria et al, 2010) anddomain-adapting sentiment analysis (Seah et al,2015).
There are also studies on general troll be-havior (Herring et al, 2002; Buckels et al, 2014).Astroturfing and misinformation have been ad-dressed in the context of political elections us-ing mapping and classification of massive streamsof microblogging data (Ratkiewicz et al, 2011).Fake profile detection has been studied in the con-text of cyber-bullying (Gal?an-Garc?
?a et al, 2014).A related research line is on offensive languageuse (Xu and Zhu, 2010).
This is related to cyber-bullying, which has been detected using sentimentanalysis (Xu et al, 2012), graph-based approachesover signed social networks (Ortega et al, 2012;Kumar et al, 2014), and lexico-syntactic featuresabout user?s writing style (Chen et al, 2012).1The independent Bulgarian media Bivol published aleaked contract described the following services in favor ofthe government:?Monthly posting online of 250 commentsby virtual users with varied, typical and evolving profilesfrom different (non-recurring) IP addresses to inform, pro-mote, balance or counteract.
The intensity of the providedonline presence will be adequately distributed and will cor-respond to the political situation in the country.?
See https://bivol.bg/en/category/b-files-en/b-files-trolls-en399Object CountPublications 34,514Comments 1,930,818-of which replies 897,806User profiles 14,598Topics 232Tags 13,575Table 1: Statistics about our dataset.Label CommentsPaid troll comments 650Mentioned troll comments 578Non-troll comments 650+578Table 2: Comments selected for experiments.Trustworthiness of statements on the Web is an-other relevant research direction (Rowe and But-ters, 2009).
Detecting untruthful and deceptive in-formation has been studied using both psychologyand computational linguistics (Ott et al, 2011).A related problem is Web spam detection, whichhas been addressed using spam keyword spotting(Dave et al, 2003), lexical affinity of arbitrarywords to spam content (Hu and Liu, 2004), fre-quency of punctuation and word co-occurrence (Liet al, 2006).
See (Castillo and Davison, 2011) foran overview on adversarial web search.In our previous work, we focused on findingopinion manipulation troll users (Mihaylov et al,2015a) and on modeling the behavior of exposedvs.
paid trolls (Mihaylov et al, 2015b).
Here, wego beyond user profile and we try to detect indi-vidual troll vs. non-troll comments in a news com-munity forum based on both text and metadata.3 DataWe crawled the largest community forum in Bul-garia, that of Dnevnik.bg, a daily newspaper (inBulgarian) that requires users to be signed in orderto read and comment.
The platform allows usersto comment on news, to reply to other users?
com-ments and to vote on them with thumbs up/down.We crawled the Bulgaria, Europe, and World cate-gories for the period 01-Jan-2013 to 01-Apr-2015,together with comments and user profiles: 34,514publications on 232 topics with 13,575 tags and1,930,818 comments (897,806 of them replies) by14,598 users; see Table 1.
We then extracted com-ments by paid trolls vs. mentioned trolls vs. non-trolls; see Table 2.Paid troll comments: We collected them fromthe leaked reputation management documents,which included 10,150 paid troll comments: 2,000in Facebook, and 8,150 in news community fo-rums.
The latter included 650 posted in the forumof Dnevnik.bg, which we used in our experiments.Mentioned troll comments: We further col-lected 1,140 comments that have been replied towith an accusation of being troll comments.
Weconsidered a comment as a potential accusation if(i) it was a reply to a comment, and (ii) it con-tained words such as troll or murzi(lka).2Two an-notators checked these comments and found 578actual accusations.
The inter-annotator agreementwas substantial: Cohen?s Kappa of 0.82.
More-over, a simple bag-of-words classifier could findthese 578 accusations with an F1-score of 0.85.Here are some examples (translated):Accusation: ?To comment from ?Prorok Ilia?
: I can seethat you are a red troll by the words that you are using?Accused troll?s comment: This Boyko3is always in yourmind!
You only think of him.
We like Boko the Potato (thefavorite of the Lamb), the way we like the Karlies.Paid troll?s comment: in the previous protests, the entirecountry participated, but now we only see the paid fans ofGERB.4These are not true protests, but chaotic happenings.Non-troll comments are those posted by usersthat have at least 100 comments in the forum andhave never been accused of being trolls.
We se-lected 650 non-troll comments for the paid trolls,and other 578 for the mentioned trolls as follows:for each paid or mentioned troll comment, we se-lected a non-troll comment at random from thesame thread.
Thus, we have two separate non-trollsets of 650 and of 578 comments.4 FeaturesWe train a classifier to distinguish troll (paid ormentioned) vs. non-troll comments using the fol-lowing features:Bag of words.
We use words and their frequen-cies as features, after stopword filtering.5Bag of stems.
We further experiment with bagof stems, where we stem the words with the Bul-Stem stemmer (Nakov, 2003a; Nakov, 2003b).Word n-grams.
We also experiment with 2-and 3-word n-grams.2Commonly believed in Bulgaria to mean troll in Russian(which it does not).3The Bulgarian Prime Minister Mr. Boyko Borisov.4Boyko Borisov?s party GERB had fallen down due toprotests and here is being accused of organizing protests inturn against the new Socialist government that replaced it.5http://members.unine.ch/jacques.savoy/clef/bulgarianST.txt400Char n-grams.
We further use character n-grams, where for each word token we extract all nconsecutive characters.
We use n-grams of length3 and 4 only as other values did not help.Word prefix.
For each word token, we extractthe first 3 or 4 consecutive characters.Word suffix.
For each word token, we take thelast 3 or 4 consecutive characters.Emoticons.
We extract the standard HTML-based emoticons used in the forum of Dnevnik.bg.Punctuation count.
We count the number ofexclamation marks, dots, and question marks, bothsingle and elongated, the number of words, and thenumber of ALL CAPS words.Metadata.
We use the time of comment posting(worktime: 9:00-19:00h vs. night: 21:00-6:00h),part of the week (workdays: Mon-Fri vs. week-end: Sat-Sun), and the rank of the comment di-vided by the number of comments in the thread.Word2Vec clusters.
We trained word2vecon 80M words from 34,514 publications and1,930,818 comments in our forum, obtaining268,617 word vectors, which we grouped into5,372 clusters using K-Means clustering, and thenwe use these clusters as features.Sentiment.
We use features derived fromMPQA Subjectivity Lexicon (Wilson et al, 2005)and NRC Emotion Lexicon (Mohammad and Tur-ney, 2013) and the lexicon of Hu and Liu (2004).Originally these lexicons were built for English,but we translated them to Bulgarian using GoogleTranslate.
Then, we reused the sentiment analysispipeline from (Velichkov et al, 2014), which weadapted for Bulgarian.Bad words.
We use the number of bad words inthe comment as a feature.
The words come fromthe Bad words list v2.0, which contains 458 badwords collected for a filter of forum or IRC chan-nels in English.6We translated this list to Bul-garian using Google Translate and we removedduplicates to obtain Bad Words Bg 1.
We fur-ther used the above word2vec model to find thethree most similar words for each bad word inBad Words Bg 1, and we constructed another lex-icon: Bad Words Bg 3.7Finally, we generate twofeatures: one for each lexicon.6http://urbanoalvarez.es/blog/2008/04/04/bad-words-list/7https://github.com/tbmihailov/gate-lang-bulgarian-gazetteers/ - GATEresources for Bulgarian, including sentiment lexicons, badwords lexicons, politicians?
names, etc.Mentions.
We noted that trolls use diminutivenames or humiliating nicknames when referringto politicians that they do not like, but use full orfamily names for people that they respect.
Basedon these observations, we constructed several lex-icons with Bulgarian politician names, their varia-tions and nicknames (see footnote 7), and we gen-erated a mention count feature for each lexicon.POS tag distribution.
We also use featuresbased on part of speech (POS).
We tag usingGATE (Cunningham et al, 2011) with a simpli-fied model trained on a transformed version of theBulTreeBank-DP (Simov et al, 2002).
For eachPOS tag type, we take the number of occurrencesin the text divided by the total number of tokens.We use both fine-grained and course-grained POStags, e.g., from the POS tag Npmsi, we generatethree tags: Npmsi, N and Np.Named entities.
We also use the occurrence ofnamed entities as features.
For extracting namedentities such as location, country, person name,date unit, etc., we use the lexicons that comewith Gate?s ANNIE (Cunningham et al, 2002)pipeline, which we translated to Bulgarian.
In fu-ture work, we plan to use a better named entityrecognizer based on CRF (Georgiev et al, 2009).5 Experiments and EvaluationWe train and evaluate an L2-regularized LogisticRegression with LIBLINEAR (Fan et al, 2008) asimplemented in SCIKIT-LEARN (Pedregosa et al,2011), using scaled and normalized features to the[0;1] interval.
As we have perfectly balanced setsof 650 positive and 650 negative examples for paidtroll vs. non-trolls and 578 positive and 578 neg-ative examples for mentioned troll vs. non-trolls,the baseline accuracy is 50%.
Below, we reportF-score and accuracy with cross-validation.Table 3, shows the results for experiments todistinguish comments by mentioned trolls vs. suchby non-trolls, using all features, as well as whenexcluding individual feature groups.
We can seethat excluding character n-grams, word suffixesand word prefixes from the features, as well as ex-cluding bag of words with stems or stop words,yields performance gains; the most sizable gain iswhen excluding char n-grams, which yields onepoint of improvement.
Excluding bad words us-age and emoticons also improves the performancebut insignificantly, which might be because theyare covered by the bag of words features.401Features F AccAll ?
char n-grams 79.24 78.54All ?
word suff 78.58 78.20All ?
word preff 78.51 78.02All ?
bow stems 78.32 77.85All ?
bow with stop 78.25 77.77All ?
bad words 78.10 77.68All ?
emoticons 78.08 77.76All ?
mentions 78.06 77.68All 78.06 77.68All ?
(bow, no stop) 78.04 77.68All ?
NE 77.98 77.59All ?
sentiment 77.95 77.51All ?
POS 77.80 77.33All ?
w2v clusters 77.79 77.25All ?
word 3-grams 77.69 77.33All ?
word 2-grams 77.62 77.25All ?
punct 77.29 76.90All ?
metadata 70.77 70.94Baseline 50.00 50.00Table 3: Mentioned troll vs. non-troll com-ments.
Ablation excluding feature groups.Excluding any of the other features hurts per-formance, the two most important features to keepbeing metadata (as it allows us to see the timeof posting), and bag of words without stopwords(which looks at the vocabulary choice that men-tioned trolls use differently from regular users).Table 4 shows the results for telling apart com-ments by paid trolls vs. such by non-trolls, usingcross-validation and ablation with the same fea-tures as for the mentioned trolls.
There are severalinteresting observations we can make.
First, wecan see that the overall accuracy for finding paidtrolls is slightly higher, namely 81.02, vs. 79.24for mentioned trolls.
The most helpful featureagain is metadata, but this time it is less helpful(excluding it yields a drop of 5 points vs. 8 pointsbefore).
The least helpful feature again are char-acter n-grams.
The remaining features fall in be-tween, and most of them yield better performancewhen excluded, which suggests that there is a lotof redundancy in the features.Next, we look at individual feature groups.
Ta-ble 5 shows the results for comments by men-tioned trolls vs. such by non-trolls.
We can seethat the metadata features are by far the most im-portant: using them alone outperforms the resultswhen using all features by 3.5 points.Features F AccAll ?
char n-grams 81.08 81.77All ?
word suff 81.00 81.77All ?
word preff 80.83 81.62All ?
bow with stop 80.67 81.54All ?
sentiment 80.63 81.46All ?
word 2-grams 80.62 81.46All ?
w2v clusters 80.54 81.38All ?
word 3-grams 80.46 81.38All ?
punct 80.40 81.23All ?
mentions 80.40 81.31All 80.40 81.31All ?
bow stems 80.37 81.31All ?
emoticons 80.33 81.15All ?
bad words 80.09 81.00All ?
NE 80.00 80.92All ?
POS 79.77 80.69All ?
(bow, no stop) 79.46 80.38All ?
metadata 75.37 76.62Baseline 50.00 50.00Table 4: Paid troll vs. non-troll comments.
Ab-lation excluding feature groups.The reason could be that most troll commentsare replies to other comments, while those by non-trolls are mostly not replies.
Adding other fea-tures such as sentiment-based features, bad words,POS, and punctuation hurts the performance sig-nificantly.
Features such as bad words are at thevery bottom: they do not apply to all commentsand thus are of little use alone; similarly for men-tions and sentiment features, which are also quiteweak in isolation.
These results suggest that men-tioned trolls are not that different from non-trollsin terms of language use, but have mainly differentbehavior in terms of replying to other users.Table 6 shows a bit different picture for com-ments by paid trolls vs. such by non-trolls.
Thebiggest difference is that metadata features are notso useful.
Also, the strongest feature set is thecombination of sentiment, bad words distribution,POS, metadata, and punctuation.
This suggeststhat paid trolls are smart to post during time in-tervals and days of the week as non-trolls, butthey use comments with slightly different senti-ment and bad word use than non-trolls.
Fea-tures based on words are also very helpful becausepaid trolls have to defend pre-specified key points,which limits their vocabulary use, while non-trollsare free to express themselves as they wish.402Features F AccAll 78.06 77.68Only metadata 84.14 81.14Sent,bad,pos,NE,meta,punct 77.79 76.73Only bow, no stop 73.41 73.79Only bow with stop 73.41 73.44Only bow stems 72.43 72.49Only word preff 71.11 71.62Only w2v clusters 69.85 70.50Only word suff 69.17 68.95Only word 2-grams 68.96 69.29Only char n-grams 68.44 68.94Only word 3-grams 64.74 67.21Only POS 64.60 65.31Sent,bad,pos,NE 63.68 64.10Only sent,bad 63.66 64.44Only emoticons 63.30 64.96Sent,bad,ment,NE 63.11 64.01Only punct 63.09 64.79Only sentiment 62.50 63.66Only NE 62.45 64.27Only mentions 62.41 64.10Only bad words 62.27 64.01Baseline 50.00 50.00Table 5: Mentioned troll comments vs. non-trollcomments.
Results for individual feature groups.6 DiscussionOverall, we have seen that our classifier for tellingapart comments by mentioned trolls vs. such bynon-trolls performs almost equally well for paidtrolls vs. non-trolls, where the non-troll commentsare sampled from the same threads that the trollcomments come from.
Moreover, the most andthe least important features ablated from all arealso similar.
This suggests that mentioned trollsare very similar to paid trolls (except for their re-ply rate, time and day of posting patterns).However, using just mentions might be a ?witchhunt?
: some users could have been accused of be-ing ?trolls?
unfairly.
One way to test this is to looknot at comments, but at users and to see whichusers were called trolls by several different otherusers.
Table 7 shows the results for distinguishingusers with a given number of alleged troll com-ments from non-troll users; the classification isbased on all comments by the corresponding users.We can see that finding users who have been calledtrolls more often is easier, which suggests theymight be trolls indeed.Features F AccAll 80.40 81.31Sent,bad,pos,NE,meta,punct 78.04 78.15Only bow, no stop 75.95 76.46Only word 2-grams 75.55 74.92Only bow with stop 75.27 75.62Only bow stems 75.25 76.08Only w2v clusters 74.20 74.00Only word preff 74.01 74.77Sent,bad,pos,NE 73.89 73.85Only metadata 73.79 72.54Only char n-grams 73.02 74.23Only POS 72.94 72.69Only word suff 72.03 72.69Only word 3-grams 69.20 68.00Only punct 66.80 65.00Only NE 66.54 64.77Sent,bad,ment,NE 66.04 64.92Only sentiment 64.28 62.62Only mentions 63.28 61.46Only sent,bad 63.14 61.54Only emoticons 62.95 61.00Only bad words 62.22 60.85Baseline 50.00 50.00Table 6: Paid troll vs. non-troll comments.
Re-sults for individual feature groups.5 10 15 20Acc 80.70 81.08 83.41 85.59Diff +8.46 +18.51 +30.81 +32.26Table 7: Mentioned troll vs. non-troll users (notcomments!).
Experiments with different numberof minimum mentions for January, 2015.
?Diff?
isthe difference from the majority class baseline.7 Conclusion and Future WorkWe have presented experiments in predictingwhether a comment is written by a troll or not,where we define troll as somebody who was calledsuch by other people.
We have shown that this is auseful definition and that comments by mentionedtrolls are similar to such by confirmed paid trolls.Acknowledgments.
This research is part ofthe Interactive sYstems for Answer Search (Iyas)project, which is developed by the Arabic Lan-guage Technologies (ALT) group at the QatarComputing Research Institute (QCRI), Hamad binKhalifa University (HBKU), part of Qatar Founda-tion in collaboration with MIT-CSAIL.403ReferencesErin E Buckels, Paul D Trapnell, and Delroy L Paulhus.2014.
Trolls just want to have fun.
Personality andindividual Differences, 67:97?102.Erik Cambria, Praphul Chandra, Avinash Sharma, andAmir Hussain.
2010.
Do not feel the trolls.
In Pro-ceedings of the 3rd International Workshop on So-cial Data on the Web, SDoW ?10, Shanghai, China.Carlos Castillo and Brian D. Davison.
2011.
Adversar-ial web search.
Found.
Trends Inf.
Retr., 4(5):377?486, May.Ying Chen, Yilu Zhou, Sencun Zhu, and Heng Xu.2012.
Detecting offensive language in social me-dia to protect adolescent online safety.
In Proceed-ings of the 2012 International Conference on Pri-vacy, Security, Risk and Trust and of the 2012 In-ternational Conference on Social Computing, PAS-SAT/SocialCom ?12, pages 71?80, Amsterdam,Netherlands.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:an architecture for development of robust HLTapplications.
In Proceedings of 40th AnnualMeeting of the Association for ComputationalLinguistics, ACL ?02, pages 168?175, Philadelphia,Pennsylvania, USA.Hamish Cunningham, Diana Maynard, and KalinaBontcheva.
2011.
Text Processing with GATE.Gateway Press CA.Kushal Dave, Steve Lawrence, and David M Pennock.2003.
Mining the peanut gallery: Opinion extrac-tion and semantic classification of product reviews.In Proceedings of the 12th International World WideWeb conference, WWW ?03, pages 519?528, Bu-dapest, Hungary.Chrysanthos Dellarocas.
2006.
Strategic manip-ulation of internet opinion forums: Implicationsfor consumers and firms.
Management Science,52(10):1577?1593.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: A li-brary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874, June.Patxi Gal?an-Garc?
?a, Jos?e Gaviria de la Puerta, Car-los Laorden G?omez, Igor Santos, and Pablo Garc??aBringas.
2014.
Supervised machine learning forthe detection of troll profiles in Twitter social net-work: Application to a real case of cyberbully-ing.
In Proceedings of the International Joint Con-ference SOCO13-CISIS13-ICEUTE13, Advances inIntelligent Systems and Computing, pages 419?428.Springer International Publishing.Georgi Georgiev, Preslav Nakov, Kuzman Ganchev,Petya Osenova, and Kiril Simov.
2009.
Feature-rich named entity recognition for Bulgarian usingconditional random fields.
In Proceedings of the In-ternational Conference Recent Advances in NaturalLanguage Processing, RANLP ?09, pages 113?117,Borovets, Bulgaria.Susan Herring, Kirk Job-Sluder, Rebecca Scheckler,and Sasha Barab.
2002.
Searching for safety on-line: Managing ?trolling?
in a feminist forum.
TheInformation Society, 18(5):371?384.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, KDD ?04, pages168?177, Seattle, Washington, USA.Srijan Kumar, Francesca Spezzano, and VS Subrah-manian.
2014.
Accurately detecting trolls inslashdot zoo via decluttering.
In Proceedings ofthe 2014 IEEE/ACM International Conference onAdvances in Social Network Analysis and Mining,ASONAM ?14, pages 188?195, Beijing, China.Wenbin Li, Ning Zhong, and Chunnian Liu.
2006.Combining multiple email filters based on multivari-ate statistical analysis.
In Foundations of IntelligentSystems, pages 729?738.
Springer.Todor Mihaylov, Georgi Georgiev, and Preslav Nakov.2015a.
Finding opinion manipulation trolls in newscommunity forums.
In Proceedings of the Nine-teenth Conference on Computational Natural Lan-guage Learning, CoNLL ?15, pages 310?314, Bei-jing, China.Todor Mihaylov, Ivan Koychev, Georgi Georgiev, andPreslav Nakov.
2015b.
Exposing paid opinion ma-nipulation trolls.
In Proceedings of the InternationalConference Recent Advances in Natural LanguageProcessing, RANLP ?15, pages 443?450, Hissar,Bulgaria.Saif M. Mohammad and Peter D. Turney.
2013.Crowdsourcing a word-emotion association lexicon.Computational Intelligence, 29(3):436?465.Preslav Nakov.
2003a.
Building an inflectionalstemmer for Bulgarian.
In Proceedings of the4th International Conference on Computer Systemsand Technologies: E-Learning, CompSysTech ?03,pages 419?424, Rousse, Bulgaria.Preslav Nakov.
2003b.
BulStem: Design and eval-uation of inflectional stemmer for Bulgarian.
InProceedings of Workshop on Balkan Language Re-sources and Tools (1st Balkan Conference in Infor-matics), Thessaloniki, Greece, November, 2003.F.
Javier Ortega, Jos A. Troyano, Fermn L. Cruz, Car-los G. Vallejo, and Fernando Enrquez.
2012.
Prop-agation of trust and distrust for the detection oftrolls in a social network.
Computer Networks,56(12):2884 ?
2895.404Myle Ott, Yejin Choi, Claire Cardie, and Jeffrey T.Hancock.
2011.
Finding deceptive opinion spamby any stretch of the imagination.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies - Volume 1, HLT ?11, pages 309?319, Portland,Oregon.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Jacob Ratkiewicz, Michael Conover, Mark Meiss,Bruno Gonc?alves, Snehal Patil, Alessandro Flam-mini, and Filippo Menczer.
2011.
Truthy: Map-ping the spread of astroturf in microblog streams.In Proceedings of the 20th International ConferenceCompanion on World Wide Web, WWW ?11, pages249?252, Hyderabad, India.Matthew Rowe and Jonathan Butters.
2009.
Assess-ing Trust: Contextual Accountability.
In Proceed-ings of the First Workshop on Trust and Privacy onthe Social and Semantic Web, SPOT ?09, Heraklion,Greece.Chun-Wei Seah, Hai Leong Chieu, Kian Ming AdamChai, Loo-Nin Teow, and Lee Wei Yeong.
2015.Troll detection by domain-adapting sentiment anal-ysis.
In Proceedings of the 18th International Con-ference on Information Fusion, FUSION ?15, pages792?799, Washington, DC, USA.Kiril Simov, Petya Osenova, Milena Slavcheva,Sia Kolkovska, Elisaveta Balabanova, DimitarDoikoff, Krassimira Ivanova, Er Simov, and MilenKouylekov.
2002.
Building a linguistically inter-preted corpus of Bulgarian: the BulTreeBank.
InProceedings of the Third International Conferenceon Language Resources and Evaluation, LREC ?02,Canary Islands, Spain.Boris Velichkov, Borislav Kapukaranov, Ivan Grozev,Jeni Karanesheva, Todor Mihaylov, Yasen Kiprov,Preslav Nakov, Ivan Koychev, and Georgi Georgiev.2014.
SU-FMI: System description for SemEval-2014 task 9 on sentiment analysis in Twitter.
InProceedings of the 8th International Workshop onSemantic Evaluation, SemEval ?14, pages 590?595,Dublin, Ireland.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of theConference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, HLT ?05, pages 347?354, Vancouver, BritishColumbia, Canada.Zhi Xu and Sencun Zhu.
2010.
Filtering offensive lan-guage in online communities using grammatical re-lations.
In Proceedings of the Seventh Annual Col-laboration, Electronic Messaging, Anti-Abuse andSpam Conference, CEAS ?10, Redmond, Washing-ton, USA.Jun-Ming Xu, Xiaojin Zhu, and Amy Bellmore.
2012.Fast learning for sentiment analysis on bullying.
InProceedings of the First International Workshop onIssues of Sentiment Discovery and Opinion Mining,WISDOM ?12, pages 10:1?10:6, Beijing, China.405
