Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 37?45,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAppropriately Handled Prosodic Breaks Help PCFG ParsingZhongqiang Huang1, Mary Harper1,21Laboratory for Computational Linguistics and Information ProcessingInstitute for Advanced Computer StudiesUniversity of Maryland, College Park, MD USA2Human Language Technology Center of ExcellenceJohns Hopkins University, Baltimore, MD USA{zqhuang,mharper}@umiacs.umd.eduAbstractThis paper investigates using prosodic infor-mation in the form of ToBI break indexes forparsing spontaneous speech.
We revisit twopreviously studied approaches, one that hurtparsing performance and one that achievedminor improvements, and propose a newmethod that aims to better integrate prosodicbreaks into parsing.
Although these ap-proaches can improve the performance of ba-sic probabilistic context free grammar (PCFG)parsers, they all fail to produce fine-grainedPCFG models with latent annotations (PCFG-LA) (Matsuzaki et al, 2005; Petrov and Klein,2007) that perform significantly better than thebaseline PCFG-LA model that does not usebreak indexes, partially due to mis-alignmentsbetween automatic prosodic breaks and truephrase boundaries.
We propose two alterna-tive ways to restrict the search space of theprosodically enriched parser models to the n-best parses from the baseline PCFG-LA parserto avoid egregious parses caused by incor-rect breaks.
Our experiments show that allof the prosodically enriched parser models canthen achieve significant improvement over thebaseline PCFG-LA parser.1 IntroductionSpeech conveys more than a sequence of words toa listener.
An important additional type of informa-tion that phoneticians investigate is called prosody,which includes phenomena such as pauses, pitch,energy, duration, grouping, and emphasis.
For areview of the role of prosody in processing spo-ken language, see (Cutler et al, 1997).
Prosodycan help with the disambiguation of lexical meaning(via accents and tones) and sentence type (e.g., yes-no question versus statement), provide discourse-level information like focus, prominence, and dis-course segment, and help a listener to discern aspeaker?s emotion or hesitancy, etc.
Prosody oftendraws a listener?s attention to important informationthrough contrastive pitch or duration patterns associ-ated words or phrases.
In addition, prosodic cues canhelp one to segment speech into chunks that are hy-pothesized to have a hierarchical structure, althoughnot necessarily identical to that of syntax.
This sug-gests that prosodic cues may help in the parsing ofspeech inputs, the topic of this paper.Prosodic information such as pause length, du-ration of words and phones, pitch contours, en-ergy contours, and their normalized values havebeen used in speech processing tasks like sentenceboundary detection (Liu et al, 2005).
In contrast,other researchers use linguistic encoding schemeslike ToBI (Silverman et al, 1992), which encodestones, the degree of juncture between words, andprominence symbolically.
For example, a simplifiedToBI encoding scheme uses the symbol 4 for ma-jor intonational breaks, p for hesitation, and 1 for allother breaks (Dreyer and Shafran, 2007).
In the lit-erature, there have been several attempts to integrateprosodic information to improve parse accuracy ofspeech transcripts.
These studies have used eitherquantized acoustic measurements of prosody or au-tomatically detected break indexes.Gregory et al (2004) attempted to integrate quan-tized prosodic features as additional tokens in thesame manner that punctuation marks are addedinto text.
Although punctuation marks can signif-icantly improve parse accuracy of newswire text,the quantized prosodic tokens were found harm-37ful to parse accuracy when inserted into human-generated speech transcripts of the Switchboard cor-pus.
The authors hypothesized that the insertedpseudo-punctuation break n-gram dependencies inthe parser model, leading to lower accuracies.
How-ever, another possible cause is that the prosody hasnot been effectively utilized due to the fact thatit is overloaded; it not only provides informationabout phrases, but also about the state of the speakerand his/her sentence planning process.
Hence, theprosodic information may at times be more harmfulthan helpful to parsing performance.In a follow-on experiment, Kahn et al (2005), in-stead of using raw quantized prosodic features, usedthree classes of automatically detected ToBI breakindexes (1, 4, or p) and their posteriors.
Rather thandirectly incorporating the breaks into the parse trees,they used the breaks to generate additional featuresfor re-ranking the n-best parse trees from a gener-ative parsing model trained without prosody.
Theywere able to obtain a significant 0.6% improvementon Switchboard over the generative parser, and amore modest 0.1% to 0.2% improvement over thereranking model that also utilizes syntactic features.Dreyer and Shafran (2007) added prosodic breaksinto a generative parsing model with latent vari-ables.
They utilized three classes of ToBI break in-dexes (1, 4, and p), automatically predicted by theapproach described in (Dreyer and Shafran, 2007;Harper et al, 2005).
Breaks were modeled as a se-quence of observations parallel to the sentence andeach break was generated by the preterminal of thepreceding word, assuming that the observation of abreak, b, was conditionally independent of its pre-ceding word, w, given preterminal X:P (w, b|X) = P (w|X)P (b|X) (1)Their approach has advantages over (Gregory et al,2004) in that it does not break n-gram dependenciesin parse modeling.
It also has disadvantages in thatthe breaks are modeled by preterminals rather thanhigher level nonterminals, and thus cannot directlyaffect phrasing in a basic PCFG grammar.
How-ever, they addressed this independence drawback bysplitting each nonterminal into latent tags so that theimpact of prosodic breaks could be percolated intothe phrasing process through the interaction of la-tent tags.
They achieved a minor 0.2% improvementover their baseline model without prosodic cues andalso found that prosodic breaks can be used to buildmore compact grammars.In this paper, we re-investigate the models of(Gregory et al, 2004) and (Dreyer and Shafran,2007), and propose a new way of modeling thatcan potentially address the shortcomings of the twoprevious approaches.
We also attribute part of thefailure or ineffectiveness of the previously investi-gated approaches to errors in the quantized prosodictokens or automatic break indexes, which are pre-dicted based only on acoustic cues and could mis-align with phrase boundaries.
We illustrate thatthese prosodically enriched models are in fact highlyeffective if we systematically eliminate bad phraseand hesitation breaks given their projection onto thereference parse trees.
Inspired by this, we pro-pose two alternative rescoring methods to restrictthe search space of the prosodically enriched parsermodels to the n-best parses from the baseline PCFG-LA parser to avoid egregious parse trees.
The effec-tiveness of our rescoring method suggests that thereranking approach of (Kahn et al, 2005) was suc-cessful not only because of their prosodic feature de-sign, but also because they restrict the search spacefor reranking to n-best lists generated by a syntacticmodel alone.2 Experimental SetupDue to our goal of investigating the effect ofprosodic information on the accuracy of state of theart parsing of conversational speech, we utilize bothPenn Switchboard (Godfrey et al, 1992) and Fishertreebanks (Harper et al, 2005; Bies et al, 2006), forwhich we also had automatically generated break in-dexes from (Dreyer and Shafran, 2007; Harper et al,2005)1.
The Fisher treebank is a higher quality pars-ing resource than Switchboard due to its greater useof audio and refined specifications for sentence seg-mentation and disfluency markups, and so we utilizeits eval set for our parser evaluation; the first 1,020trees (7,184 words) were used for development andthe remaining 3,917 trees (29,173 words) for eval-uation.
We utilized the Fisher dev1 and dev2 setscontaining 16,519 trees (112,717 words) as the maintraining data source and used the Penn Switchboard1A small fraction of words in the Switchboard treebankcould not be aligned with the break indexes that were producedbased on a later refinement of the transcription.
We chose notto alter the Switchboard treebank, so in cases of missing breakvalues, we heuristically added break *1* to words in the middleof a sentence and *4* to words that end a sentence.38treebank containing 110,504 trees (837,863 words)as an additional training source to evaluate the ef-fect of training data size on parsing performance.The treebank trees are normalized by downcasingall terminal strings and deleting punctuation, emptynodes, and nonterminal-yield unary rules that are notrelated to edits.We will compare2 three prosodically enrichedPCFG models described in the next section, with abaseline PCFG parser.
We will also utilize a stateof the art PCFG-LA parser (Petrov and Klein, 2007;Huang and Harper, 2009) to examine the effect ofprosodic enrichment3.
Unlike (Kahn et al, 2005),we do not remove EDITED regions prior to parsingbecause parsing of EDITED regions is likely to ben-efit from prosodic information.
Also, parses from allmodels are compared with the gold standard parsesin the Fisher evaluation set using SParseval bracketscoring (Harper et al, 2005; Roark et al, 2006)without flattening the EDITED constituents.3 Methods of Integrating BreaksRather than using quantized raw acoustic features asin (Gregory et al, 2004), we use automatically gen-erated ToBI break indexes as in (Dreyer and Shafran,2007; Kahn et al, 2005) as the prosodic cues, andinvestigate three alternative methods of modelingprosodic breaks.
Figure 1 shows parse trees for thefour models for processing the spontaneous speechtranscription she?s she would do, where the speakerhesitated after saying she?s and then resumed withanother utterance she would do.
Each word inputinto the parser has an associated break index repre-sented by the symbol 1, 4, or p enclosed in asterisksindicating the break after the word.
The automat-ically detected break *4* after the contraction is astrong indicator of an intonational phrase boundarythat might provide helpful information for parsing ifmodeled appropriately.
Figure 1 (a) shows the ref-erence parse tree (thus the name REGULAR) wherethe break indexes are not utilized.The first method to incorporate break indexes,BRKINSERT, shown in Figure 1 (b), treats the *p*and *4* breaks as tokens, placing them under the2We use Bikel?s randomized parsing evaluation comparatorto determine the significance (p < 0.005) of the difference be-tween two parsers?
outputs.3Due to the randomness of parameter initialization in thelearning of PCFG-LA models with increasing numbers of latenttags, we train each latent variable grammar with 10 differentseeds and report the average F score on the evaluation set.wouldMDdoVBVPVPshePRPNP?sVBZVPshePRPNPSEDITEDS*4* *4**1* *1* *1*MD VBVPVPPRPNPVBZVPPRPNPSEDITEDSBREAK BREAKwould doshe?sshe *4* *4**1* *1* *1*(a) REGULAR (b) BRKINSERTwouldMDdoVBVPVPshePRPNP?sVBZVPshePRPNPSEDITEDS*4* *4**1* *1* *1* wouldMDdoVBVPVPshePRPNP?sVBZVPshePRPNPSEDITEDS*4* *4**1* *1* *1*(c) BRKPOS (d) BRKPHRASEFigure 1: Modeling Methodshighest nonterminal nodes so that the order of wordsand breaks remain unchanged in the terminals.
Thisis similar to (Gregory et al, 2004), except that auto-matically generated ToBI breaks are used rather thanquantized raw prosodic tokens.The second method, BRKPOS, shown in Fig-ure 1 (c), treats breaks as a sequence of observa-tions parallel to the words in the sentence as in(Dreyer and Shafran, 2007).
The dotted edges inFigure 1 (c) represent the relation between pretermi-nals and prosodic breaks, and we call them prosodicrewrites, with analogy to grammar rewrites and lex-ical rewrites.
The generation of words and prosodicbreaks is assumed to be conditionally independentgiven the preterminal, as in Equation 1.The third new method, BRKPHRASE, shown inFigure 1 (d), also treats breaks as a sequence of ob-servations parallel to the sentence; however, ratherthan associating the prosodic breaks with the preter-minals, each is generated by the highest nonterminal(including preterminal) in the parse tree that coversthe preceding word as the right-most terminal.
Theobservation of break, b, is assumed to be condition-ally independent of grammar or lexical rewrite, r,given the nonterminal X:P (r, b|X) = P (r|X)P (b|X) (2)The relation is indicated by the dotted edges in Fig-ure 1 (d), and it is also called a prosodic rewrite.The potential advantage of BRKPHRASE is that itdoes not break or fragment n-gram dependencies ofthe grammar rewrites, as in the BRKINSERT method,and it directly models the dependency betweenbreaks and phrases, which the BRKPOS method ex-plicitly lacks.4 Model TrainingSince automatically generated prosodic breaks areincorporated into the parse trees deterministi-39cally for all of the three enrichment methods(BRKINSERT, BRKPOS, and BRKPHRASE), train-ing a basic PCFG is straightforward; we simply pullthe counts of grammar rules, lexical rewrites, orprosodic rewrites from the treebank and normalizethem to obtain their probabilities.As is well known in the parsing community, thebasic PCFG does not provide state-of-the-art per-formance due to its strong independence assump-tions.
We can relax these assumptions by explicitlyincorporating more information into the conditionalhistory, as in Charniak?s parser (Charniak, 2000);however, this would require sophisticated engineer-ing efforts to decide what to include in the historyand how to smooth probabilities appropriately dueto data sparsity.
In this paper, we utilize PCFG-LAmodels (Matsuzaki et al, 2005; Petrov and Klein,2007) that split each nonterminal into a set of latenttags and learn complex dependencies among the la-tent tags automatically during training.
The result-ing model is still a PCFG, but it is probabilisticallycontext free on the latent tags, and the interactionamong the latent tags is able to implicitly capturehigher order dependencies among the original non-terminals and observations.
We follow the approachin (Huang and Harper, 2009) to train the PCFG-LAmodels.5 ParsingIn a basic PCFG without latent variables, the goalof maximum probability parsing is to find the mostlikely parse tree given a sentence based on the gram-mar.
Suppose our grammar is binarized (so it con-tains only unary and binary grammar rules).
Givenan input sentence wn1 = w1, w2, ?
?
?
, wn, the insideprobability, P (i, j, X), of the most likely sub-treethat is rooted at nonterminal X and generates sub-sequence wji can be computed recursively by:P (i, j, X) = max(maxYP (i, j, Y )P (X ?
Y ),maxi<k<j,Y,ZP (i, k, Y )P (k + 1, j, Z)P (X ?
Y Z)) (3)Backtracing the search process then returns the mostlikely parse tree for the REGULAR grammar.The same parsing algorithm can be directly ap-plied to the BRKINSERT grammar given that thebreak indexes are inserted appropriately into the in-put sentence as additional tokens.
Minor modifica-tion is needed to extend the same parsing algorithmto the BRKPOS grammar.
The only difference is thatthe inside probability of a preterminal is set accord-ing to Equation 1.
The rest of the algorithm proceedsas in Equation 3.However, parsing with the BRKPHRASE grammaris more complicated because whether a nonterminalgenerates a break or not is determined by whetherit is the highest nonterminal that covers the preced-ing word as its right-most terminal.
In this case,the input observation also contains a sequence ofbreak indexes bn1 = b1, b2, ?
?
?
, bn that is parallelto the input sentence wn1 = w1, w2, ?
?
?
, wn.
LetP (i, j, X, 0) be the probability of the most likelysub-tree rooted at nonterminal X over span (i, j)that generates word sequence wji , as well as breakindex sequence bj?1i , excluding bj .
According tothe independence assumption in Equation 2, withthe addition of prosodic edge X ?
bj , the samesub-tree also has the highest probability, denoted byP (i, j, X, 1), of generating word sequence wji to-gether with the break index sequence bji .
Thus wehave:P (i, j, X, 1) = P (i, j, X, 0)P (bj |X) (4)The structural constraint that a break index is onlygenerated by the highest nonterminal that coversthe preceding word as the right-most terminal en-ables a dynamic programming algorithm to computeP (i, j, X, 0) and thus P (i, j, X, 1) efficiently.
If thesub-tree (without the prosodic edge that generatesbj) over span (i, j) is constructed from a unary rulerewrite X ?
Y , then the root nonterminal Y ofsome best sub-tree over the same span (i, j) can notgenerate break bj because it has a higher nontermi-nal X that also covers word wj as its right-most ter-minal.
If the sub-tree is constructed from a binaryrule rewrite X ?
Y Z, then the root nonterminal Yof some best sub-tree over some span (i, k) will gen-erate break bk because Y is the highest nonterminalthat covers word wk as the right-most terminal4.
Incontrast, the root nonterminal Z of some best sub-tree over some span (k+1, j) can not generate breakbj because Z has a higher nonterminal X that alsocovers word wj as its right-most terminal.
Hence,4Use of left-branching is required for the BRKPHRASEmethod to ensure that the prosodic breaks are associated withthe original nonterminals, not intermediate nonterminals in-troduced by binarization.
Binarization is needed for efficientparametrization of PCFG-LA models and left- versus right-branching binarization does not significantly affect model per-formance; hence, we use left-branching for all models.40P (i, j, X, 1) and P (i, j, X, 0) can be computed re-cursively by Equation 4 above and Equation 5 be-low:P (i, j, X, 0) = max(maxYP (i, j, Y, 0)P (X ?
Y ),maxi<k<j,Y,ZP (i, k, Y, 1)P (k + 1, j, Z, 0)P (X ?
Y Z)) (5)Although dynamic programming algorithms existfor maximum probability decoding of basic PCFGswithout latent annotations for all four methods, it isan NP hard problem to find the most likely parse treeusing PCFG-LA models.
Several alternative decod-ing algorithms have been proposed in the literaturefor parsing with latent variable grammars.
We usethe best performing max-rule-product decoding al-gorithm, which searches for the best parse tree thatmaximizes the product of the posterior rule (eithergrammar, lexical, or prosodic) probabilities, as de-scribed in (Petrov and Klein, 2007) for our modelswith latent annotations and extend the dynamic pars-ing algorithm described in Equation 5 for the BRK-PHRASE grammar with latent annotations.6 Results on the Fisher Corpus6.1 Prosodically Enriched ModelsTable 1 reports the parsing accuracy of the four basicPCFGs without latent annotations when trained onthe Fisher training data.
All of the grammars have alow F score of around 65% due to the overly strongand incorrect independence assumptions.
We ob-serve that the BRKPHRASE grammar benefits mostfrom breaks, significantly improving the baselineaccuracy from 64.9% to 67.2%, followed by theBRKINSERT grammar, which at 66.2% achieves asmaller improvement.
The BRKPOS grammar ben-efits the least among the three because breaks areattached to the preterminals and thus have less im-pact on phrasing due to the independence assump-tions in the basic PCFG.
In contrast, both the BRK-PHRASE and BRKINSERT methods directly modelthe relationship between breaks and phrase bound-aries through governing nonterminals; however, theBRKPHRASE method does not directly change anyof the grammar rules in contrast to the BRKINSERTmethod that more or less breaks n-gram dependen-cies and fragments rule probabilities.The bars labeled DIRECT in Figure 2 report theparsing performance of the four PCFG-LA modelstrained on Fisher.
The introduction of latent anno-tations significantly boosts parsing accuracies, pro-viding relative improvements ranging from 16.8%REGULAR BRKINSERT BRKPOS BRKPHRASE64.9 66.2 65.2 67.2Table 1: Fisher evaluation parsing results for the basicPCFGs without latent annotations trained on the Fishertraining set.up to 19.0% when trained on Fisher training datadue to the fact that the PCFG-LA models are ableto automatically learn more complex dependenciesnot captured by basic PCFGs.82.583.584.585.5Regular BrkInsert BrkPos BrkPhrase83.983.284.2 84.284.484.085.084.584.784.085.184.7 84.8DirectOracleOracleRescoreDirectRescoreFigure 2: Parsing results on the Fisher evaluation setof the PCFG-LA models trained on the Fisher trainingdata.
The DIRECT bars represent direct parsing results formodels trained and evaluated on the original data, ORA-CLE bars for models trained and evaluated on the modi-fied oracle data (see Subsection 6.2), and the ORACLE-RESCORE and DIRECTRESCORE bars for results of thetwo rescoring approaches (described in Subsection 6.3)on the original evaluation data.However, the prosodically enriched methods donot significantly improve upon the REGULAR base-line after the introduction of latent annotations.
TheBRKPHRASE method only achieves a minor in-significant 0.1% improvement over the REGULARbaseline; whereas, the BRKINSERT method is a sig-nificant 0.7% worse than the baseline.
Similar re-sults for BRKINSERT were reported in (Gregory etal., 2004), where they attributed the degradation tothe fact that the insertion of the prosodic ?punctua-tion?
breaks the n-gram dependencies.
Another pos-sible cause is that the insertion of ?bad?
breaks thatdo not align with true phrase boundaries hurts per-formance more than the benefits gained from ?good?breaks due to the tightly integrated relationship be-tween phrases and breaks.
For the BRKPOS method,the impact of break indexes is implicitly percolatedto the nonterminals through the interaction amonglatent tags, as discussed in (Dreyer and Shafran,2007), and its performance may thus be less affectedby the ?bad?
breaks.
With latent annotations (in con-trast to the basic PCFG), the model is now signif-icantly better than BRKINSERT and is on par withBRKPHRASE.416.2 Models with Oracle BreaksIn order to determine whether ?bad?
breaks limitthe improvements in parsing performance fromprosodic enrichment, we conducted a simple ora-cle experiment where all *p* and *4* breaks thatdid not align with phrase boundaries in the tree-bank were systematically converted to *1* breaks5.When trained and evaluated on this modified ora-cle data, all three prosodically enriched latent vari-able models improve by about 1% and were thenable to achieve significant improvements over theREGULAR PCFG-LA baseline, as shown by the barslabeled ORACLE in Figure 2.
It should be noted,however, that the BRKINSERT method is much lesseffective than the other two methods in the oracleexperiment, suggesting that broken n-gram depen-dencies affect the model in addition to the erroneousbreaks.6.3 N-Best Re-ScoringAs mentioned previously, prosody does not onlyprovide information about phrases, but also aboutthe state of the speaker and his/her sentence plan-ning process.
Given that our break detector uti-lizes only acoustic knowledge to predict breaks, therecognized *p* and *4* breaks may not correctlyreflect hesitations and phrase boundaries.
Incor-rectly recognized breaks could hurt parsing morethan the benefit brought from the correctly recog-nized breaks, as demonstrated by superior perfor-mance of the prosodically enhanced models in theoracle experiment.
We next describe two alternativemethods to make better use of automatic breaks.In the first approach, which is called ORACLE-RESCORE, we train the prosodically enhancedgrammars on cleaned-up break-annotated trainingdata, where misclassified *p* and *4* breaks areconverted to *1* breaks (as in the oracle experi-ment).
If these grammars were used to directly parsethe test sentences with automatically detected (un-modified) breaks, the results would be quite poordue to mismatch between the training and testingconditions.
However, we can automatically biasagainst potentially misclassified *p* and *4* breaksif we utilize information provided by n-best parsesfrom the baseline REGULAR PCFG-LA grammar.5Other sources of errors include misclassification of *p*breaks as *1* or *4* and misclassification of *4* breaks as *1*or *p*.
Although these errors are not repaired in the oracle ex-periment, fixing them could potentially provide greater gains.For each hypothesized parse tree in the n-best list,the *p* and *4* breaks that do not align with thephrase boundaries of the hypothesized parse tree areconverted to *1* breaks, and then a new score iscomputed using the product of posterior rule proba-bilities6, as in the max-rule-product criterion, for thehypothesized parse tree using the grammars trainedon the cleaned-up training data.
In this approach,we convert the posterior probability, P (T |W, B),of parse tree T given words W and breaks Bto P (B?|W, B)P (T |W, B?
), where B?
is the newbreak sequence constrained by T , and simplify it toP (T |W, B?
), assuming that conversions to a new se-quence of breaks as constrained by a hypothesizedparse tree are equally probable given the original se-quence of breaks.
We consider this to be a reason-able assumption for a small n-best (n = 50) list withreasonably good quality.In the second approach, called DIRECTRESCORE,we train the prosodically enhanced PCFG-LA mod-els using unmodified, automatic breaks, and thenuse them to rescore the n-best lists produced bythe REGULAR PCFG-LA model to avoid the poorerparse trees caused by fully trusting automatic breakindexes.
The size of the n-best list should not be toosmall or too large, or the results would be like di-rectly parsing with REGULAR when n = 1 or withthe prosodically enriched model when n ?
?.The ORACLERESCORE and DIRECTRESCOREbars in Figure 2 report the performance of theprosodically enriched models with the correspond-ing rescoring method.
Both methods use the same50-best lists produced by the baseline REGULARPCFG-LA model using the max-rule-product cri-terion.
Both rescoring methods produce signifi-cant improvements in the performance of all threeprosodically enriched PCFG-LA models.
The pre-viously ineffective (0.7% worse than REGULAR)BRKINSERT PCFG-LA model is now 0.3% and0.5% better than the REGULAR baseline usingthe ORACLERESCORE and DIRECTRESCORE ap-proaches, respectively.
The best performing BRK-POS and BRKPHRASE rescoring models are 0.6-0.9% better than the REGULAR baseline.
It is in-teresting to note that rescoring with models trainedon cleaned up prosodic breaks is somewhat poorer6The product of posterior rule probabilities of a parse treeis more suitable for rescoring than the joint probability of theparse tree and the observables (words and breaks) because thebreaks are possibly different for different trees.42than models trained using all automatic breaks.7 Models with Augmented Training DataFigure 3 reports the evaluation results for mod-els that are trained on the combination of Fisherand Switchboard training data.
With the additionalSwitchboard training data, the nonterminals can besplit into more fine-grained latent tags, enabling thelearning of deeper dependencies without over-fittingthe limited sized Fisher training data.
This improvedall models by at least 2.6% absolute.
Note also thatthe patterns observed for models trained using thelarger training set are quite similar to those from us-ing the smaller training set in Figure 2.
The prosod-ically enriched models all benefit significantly fromthe oracle breaks and from the rescoring methods.The BRKPOS and BRKPHRASE methods, with theadditional training data, also achieve significant im-provements over the REGULAR baseline withoutrescoring.85868788Regular BrkInsert BrkPos BrkPhrase86.586.387.486.887.186.887.787.2 87.386.887.587.2 87.3DirectOracleOracleRescoreDirectRescoreFigure 3: Parsing results on the Fisher evaluation set ofthe PCFG-LA models trained on the Fisher+Switchboardtraining data.8 Error AnalysisIn this section, we compare the errors of theBRKPHRASE PCFG-LA model and the DIRECT-RESCORE approach for that model to each other andto the baseline PCFG-LA model without prosodicbreaks.
All models are trained and tested on Fisheras in Section 6.
The results using other prosodicallyenhanced PCFG-LA models and their rescoring al-ternatives show similar patterns.Figure 4 depicts the difference in F scores be-tween BRKPHRASE and REGULAR and betweenBRKPHRASE+DIRECTRESCORE and REGULAR ona tree-by-tree basis in a 2D plot.
Each quad-rant also contains +/?
signs roughly describing howmuch BRKPHRASE+DIRECTRESCORE is better (+)or worse (?)
than BRKPHRASE and a pair of num-bers (a, b), in which a represents the percentage ofsentences in that quadrant containing *p* or *4*-20-15-10-505101520-20 -15 -10 -5  0  5  10  15  20F(BrkPhrase+DirectRescore)-F(Regular)F(BrkPhrase)-F(Regular)-(47.2%, 25.3%)+++(70.2%, 30.0%)++(48.2%, 27.6%)---(66.7%, 28.1%)Figure 4: 2D plot of the difference in F scores be-tween BRKPHRASE and REGULAR and between BRK-PHRASE+DIRECTRESCORE and REGULAR, on a tree-by-tree basis, where each dot represents a test sentence.Each quadrant also contains +/?
signs roughly describ-ing how much BRKPHRASE+DIRECTRESCORE is better(+) or worse (?)
than BRKPHRASE and a pair of numbers(a, b), in which a represents the percentage of sentencesin that quadrant containing *p* or *4* breaks that do notalign with true phrase boundaries, and b represents thepercentage of such *p* and *4* breaks among the totalnumber of *p* and *4* breaks in that quadrant.breaks that do not align with true phrase bound-aries, and b represents the percentage of such *p*and *4* breaks among the total number of *p* and*4* breaks in that quadrant.Each dot in the top-right quadrant represents atest sentence for which both BRKPHRASE and BRK-PHRASE+DIRECTRESCORE produce better treesthan the baseline REGULAR PCFG-LA model.
TheBRKPHRASE+DIRECTRESCORE approach is on av-erage slightly worse than the BRKPHRASE method(hence the single minus sign), although it also oftenproduces better parses than BRKPHRASE alone.
Incontrast, the BRKPHRASE+DIRECTRESCORE ap-proach on average makes many fewer errors thanBRKPHRASE (hence + +) as can be observed in thebottom-left quadrant, where both approaches pro-duce worse parse trees than the REGULAR base-line.
The most interesting quadrant is on the top-leftwhere the BRKPHRASE approach always producesworse parses than the REGULAR baseline while theBRKPHRASE+DIRECTRESCORE approach is ableto avoid these errors while producing better parsesthan the baseline (hence + + +).
Although the BRK-PHRASE+DIRECTRESCORE approach can also pro-duce worse parses than REGULAR, as in the bottom-right quadrant (hence ?
?
?
), altogether the quad-rants suggest that, by restricting the search space43to the n-best lists produced by the baseline REG-ULAR parser, the BRKPHRASE+DIRECTRESCOREapproach is able to avoid many bad parses treesat the expense of somewhat poorer parses in caseswhen BRKPHRASE is able to benefit from the fullsearch space.The reader should note that the top-left quadrantof Figure 4 has the highest percentage (70.2%) ofsentences with ?bad?
*p* and *4* breaks and thehighest percentage (30.0%) of such ?bad?
breaksamong all breaks.
This evidence supports our argu-ment that ?bad?
breaks are harmful to parsing per-formance and some parse errors caused by mislead-ing breaks can be resolved by limiting the searchspace of the prosodically enriched models to then-best lists produced by the baseline REGULARparser.
However, the significant presence of ?bad?breaks in the top-right quadrant also suggests thatthe prosodically enriched models are able to pro-duce better parses than the baseline despite the pres-ence of ?bad?
breaks, probably because the modelsare trained on the mixture of both ?good?
and ?bad?breaks and are able to somehow learn to use ?good?breaks while avoiding being misled by ?bad?
breaks.BRKPHRASEREGULAR BRKPHRASE +DIRECTRESCORENP 90.4 90.4 90.9VP 84.7 84.7 85.6S 84.4 84.3 85.2INTJ 93.0 93.4 93.4PP 76.5 76.7 77.9EDITED 60.4 62.2 63.3SBAR 67.2 67.0 68.8Table 2: F scores of the seven most frequent non-terminals of the REGULAR, BRKPHRASE, and BRK-PHRASE+DIRECTRESCORE models.Table 2 reports the F scores of the seven most fre-quent phrases for the REGULAR, BRKPHRASE, andBRKPHRASE+DIRECTRESCORE methods trainedon Fisher.
When comparing the BRKPHRASEmethod to REGULAR, the break indexes help to im-prove the score for edits most, followed by inter-jections and prepositional phrases; however, they donot improve the accuracy of any of the other phrases.The BRKPHRASE+DIRECTRESCORE approach ob-tains improvements on all of the major phrases.Figure 5 (a) shows a reference parse tree of atest sentence.
The REGULAR approach correctlyparses the first half (omitted) of the sentence butit fails to correctly interpret the second half (asshown).
The BRKPHRASE approach, in contrast,is misguided by the incorrectly classified inter-ruption point *p* after word ?has?, and so pro-duces an incorrect parse early in the sentence.
TheBRKPHRASE+DIRECTRESCORE approach is ableto provide the correct tree given the n-best list pro-duced by the REGULAR approach, despite the breakindex errors.
(a) Reference, BRKPHRASE+DIRECTRESCORE(b) REGULAR (c) BRKPHRASEFigure 5: Parses for like?1?
has?p?
anything?1?
like?1?affected?1?
you?4?
personally?4?
or?1?
anything?4?9 ConclusionsWe have investigated using prosodic information inthe form of automatically detected ToBI break in-dexes for parsing spontaneous speech by compar-ing three prosodic enrichment methods.
Althoughprosodic enrichment improves the basic PCFGs, thatperformance gain disappears when latent variablesare used, partly due to the impact of misclassified(?bad?)
breaks that are assigned to words that do notoccur at phrase boundaries.
However, we find thatby simply restricting the search space of the threeprosodically enriched latent variable parser modelsto the n-best parses from the baseline PCFG-LAparser, all of them attain significant improvements.Our analysis more fully explains the positive resultsachieved by (Kahn et al, 2005) from reranking withprosodic features and suggests that the hypothesisthat inserted prosodic punctuation breaks n-gram de-pendencies only partially explains the negative re-sults of (Gregory et al, 2004).
Our findings fromthe oracle experiment suggest that integrating ToBIclassification with syntactic parsing should increasethe accuracy of both tasks.AcknowledgmentsWe would like to thank Izhak Shafran for providingbreak indexes for Fisher and Switchboard and for44comments on an earlier draft of this paper.
This re-search was supported in part by NSF IIS-0703859.Opinions, findings, and recommendations expressedin this paper are those of the authors and do not nec-essarily reflect the views of the funding agency orthe institutions where the work was completed.ReferencesAnn Bies, Stephanie Strassel, Haejoong Lee, KazuakiMaeda, Seth Kulick, Yang Liu, Mary Harper, andMatthew Lease.
2006.
Linguistic resources for speechparsing.
In LREC.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In ACL.Anne Cutler, Delphine Dahan, and Wilma v an Donselaar.1997.
Prosody in comprehension of spoken language:A literature review.
Language and Speech.Markus Dreyer and Izhak Shafran.
2007.
Exploitingprosody for PCFGs with latent annotations.
In Inter-speech.John J. Godfrey, Edward C. Holliman, and Jane Mc-Daniel.
1992.
SWITCHBOARD: Telephone speechcorpus for research and development.
In ICASSP.Michelle L. Gregory, Mark Johnson, and Eugene Char-niak.
2004.
Sentence-internal prosody does not helpparsing the way punctuation does.
In NAACL.Mary P. Harper, Bonnie J. Dorr, John Hale, Brian Roark,Izhak Shafran, Matthew Lease, Yang Liu, MatthewSnover, Lisa Yung, Anna Krasnyanskaya, and RobinStewart.
2005.
2005 Johns Hopkins Summer Work-shop Final Report on Parsing and Spoken StructuralEvent Detection.
Technical report, Johns HopkinsUniversity.Zhongqiang Huang and Mary Harper.
2009.
Self-training PCFG grammars with latent annotationsacross languages.
In EMNLP.Jeremy G. Kahn, Matthew Lease, Eugene Charniak,Mark Johnson, and Mari Ostendorf.
2005.
Effectiveuse of prosody in parsing conversational speech.
InEMNLP-HLT.Yang Liu, Andreas Stolcke, Elizabeth Shriberg, and MaryHarper.
2005.
Using conditional random fields forsentence boundary detection in speech.
In ACL.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InACL.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL.Brian Roark, Mary Harper, Yang Liu, Robin Stewart,Matthew Lease, Matthew Snover, Izhak Shafran, Bon-nie J. Dorr, John Hale, Anna Krasnyanskaya, and LisaYung.
2006.
Sparseval: Evaluation metrics for pars-ing speech.
In LREC.Kim Silverman, Mary Beckman, John Pitrelli, Mari Os-tendorf, Colin Wightman, Patti Price, Janet Pierrehum-bert, and Julia Hirshberg.
1992.
ToBI: A standard forlabeling English prosody.
In ICSLP.45
