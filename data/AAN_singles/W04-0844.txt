Structural Semantic Interconnection: a knowledge-based approach to WordSense DisambiguationRoberto NAVIGLIDipartimento di Informatica,Universit?
di Roma ?La Sapienza?Via Salaria, 113 - 00198 Roma, Italynavigli@di.uniroma1.itPaola VELARDIDipartimento di Informatica,Universit?
di Roma ?La Sapienza?Via Salaria, 113 - 00198 Roma, Italyvelardi@di.uniroma1.itAbstractIn this paper we describe the SSI algorithm, astructural pattern matching algorithm forWSD.
The algorithm has been applied to thegloss disambiguation task of Senseval-3.1 IntroductionOur approach to WSD lies in the structuralpattern recognition framework.
Structural orsyntactic pattern recognition (Bunke and Sanfeliu,1990) has proven to be effective when the objectsto be classified contain an inherent, identifiableorganization, such as image data and time-seriesdata.
For these objects, a representation based on a?flat?
vector of features causes a loss ofinformation that negatively impacts onclassification performances.
Word senses clearlyfall under the category of objects that are betterdescribed through a set of structured features.The classification task in a structural patternrecognition system is implemented through theuse of grammars that embody precise criteria todiscriminate among different classes.
Learning astructure for the objects to be classified is often amajor problem in many application areas ofstructural pattern recognition.
In the field ofcomputational linguistics, however, several effortshave been made in the past years to produce largelexical knowledge bases and annotated resources,offering an ideal starting point for constructingstructured representations of word senses.2 Building structural representations ofword sensesWe build a structural representation of wordsenses using a variety of knowledge sources, i.e.WordNet, Domain Labels (Magnini and Cavaglia,2000), annotated corpora like SemCor and LDC-DSO1.
We use this information to automatically1 LDC http://www.ldc.upenn.edu/generate labeled directed graphs (digraphs)representations of word senses.
We call thesesemantic graphs, since they represent alternativeconceptualizations for a lexical item.Figure 1 shows an example of the semanticgraph generated for senses #1 of market, wherenodes represent concepts (WordNet synsets), andedges are semantic relations.
In each graph, weinclude only nodes with a maximum distance of 3from the central node, as suggested by the dashedoval in Figure 1.
This distance has beenexperimentally established.market#1goods#1trading#1glossglossmerchandise#1k ind-ofmonopoly#1kind-ofexport#1has-kindactivity#1has-kindconsumergoods#1grocery#2kind-ofkind-ofload#3kind-ofcommercialenterprise#2has-partcommerce#1 kind -oftransportation#5has-partbusinessactivity#1glossservice#1gloss to picindustry#2kind-ofh as-partglosskind-offood#1clothing#1glossglossenterprise#1kind-ofproduction#1artifact#1k i n d -o fexpress#1kind-ofconsumption#1glossFigure 1.
Graph representations for sense #1 of market.All the used semantic relations are explicitlyencoded in WordNet, except for three relationsnamed topic, gloss and domain, extractedrespectively from annotated corpora, sensedefinitions and domain labels.3 Summary description of the SSI algorithmThe SSI algorithm consists of an initialization stepand an iterative step.In a generic iteration of the algorithm the inputis a list of co-occurring terms T = [ t1, ?, tn ] anda list of associated senses I = ],...,[ 1 ntt SS , i.e.
thesemantic interpretation of T, where itS 2 is eitherthe chosen sense for ti (i.e., the result of a previous2 Note that with itS we refer interchangeably to the semanticgraph associated with a sense or to the sense name.Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of Systemsdisambiguation step) or the empty set (i.e., theterm is not yet disambiguated).A set of pending terms is also maintained, P =}|{ =iti St .
I is named the semantic context of Tand is used, at each step, to disambiguate newterms in P.The algorithm works in an iterative way, so thatat each stage either at least one term is removedfrom P (i.e., at least a pending term isdisambiguated) or the procedure stops because nomore terms can be disambiguated.
The output isthe updated list I of senses associated with theinput terms T.Initially, the list I includes the senses ofmonosemous terms in T. If no monosemous termsare found, the algorithm makes an initial guessbased on the most probable sense of the lessambiguous term.
The initialisation policy isadjusted depending upon the specific WSD taskconsidered.
Section 5 describes the policy adoptedfor the task of gloss disambiguation in WordNet.During a generic iteration, the algorithm selectsthose terms t in P showing an interconnectionbetween at least one sense S of t and one or moresenses in I.
The likelihood for a sense S of beingthe correct interpretation of t, given the semanticcontext I, is estimated by the functionCxTfI : , where C is the set of all theconcepts in the ontology O, defined as follows:=otherwiseSynsetstSensesSifISSStSf I 0)(})'|)',(({),(where Senses(t) is the subset of concepts C in Oassociated with the term t, and})'...|)...(({')',( 1121121 SSSSeeewSS nneneeen =  ,i.e.
a function (?)
of the weights (w) of each pathconnecting S with S?, where S and S?
arerepresented by semantic graphs.
A semantic pathbetween two senses S and S?, '... 11121 SSSS nneneee   ,is represented by a sequence of edge labelsneee...21 .
A proper choice for both  and ?
maybe the sum function (or the average sum function).A context-free grammar G = (E, N, SG, PG)encodes all the meaningful semantic patterns.
Theterminal symbols (E) are edge labels, while thenon-terminal symbols (N) encode (sub)pathsbetween concepts; SG is the start symbol of G andPG the set of its productions.We associate a weight with each productionA in PG, where NAand *)( EN  , i.e. is a sequence of terminal and non-terminalsymbols.
If the sequence of edge labels neee...21belongs to L(G), the language generated by thegrammar, and provided that G is not ambiguous,then )...( 21 neeewis given by the sum of theweights of the productions applied in thederivation nG eeeS+ ...21 .
The grammar G isdescribed in the next section.Finally, the algorithm selects ),(maxarg tSfICSasthe most likely interpretation of t and updates thelist I with the chosen concept.
A threshold can beapplied to ),( tSf to improve the robustness ofsystem?s choices.At the end of a generic iteration, a number ofterms is disambiguated and each of them isremoved from the set of pending terms P. Thealgorithm stops with output I when no sense S canbe found for the remaining terms in P such that0),( >tSfI , that is, P cannot be further reduced.In each iteration, interconnections can only befound between the sense of a pending term t andthe senses disambiguated during the previousiteration.A special case of input for the SSI algorithm isgiven by ]..., ,,[ =I , that is when no initialsemantic context is available (there are nomonosemous words in T).
In this case, aninitialization policy selects a term tT and theexecution is forked into as many processes as thenumber of senses of t.4 The grammarThe grammar G has the purpose of describingmeaningful interconnecting patterns amongsemantic graphs representing conceptualisationsin O.
We define a pattern as a sequence ofconsecutive semantic relations neee...21 whereEei, the set of terminal symbols, i.e.
thevocabulary of conceptual relations in O. Tworelations 1+ii ee are consecutive if the edgeslabelled with ie and 1+ie are incoming and/oroutgoing from the same concept node, that is1)( + ii ee S , 1)( + ii ee S , 1)( + ii ee S , 1)( + ii ee S .
A meaningfulpattern between two senses S and S?
is a sequenceneee...21 that belongs to L(G).In its current version, the grammar G has beendefined manually, inspecting the intersectingpatterns automatically extracted from pairs ofmanually disambiguated word senses co-occurringin different domains.
Some of the rules in G areinspired by previous work on the eXtendedWordNet project described in (Milhalcea andMoldovan, 2001).
The terminal symbols ei are theconceptual relations extracted from WordNet andother on-line lexical-semantic resources, asdescribed in Section 2.G is defined as a quadruple (E, N, SG, PG),where E = { ekind-of, ehas-kind, epart-of, ehas-part, egloss, eis-in-gloss, etopic, ?
}, N = { SG, Ss, Sg, S1, S2, S3, S4, S5,S6, E1, E2, ?
}, and PG includes about 50productions.As stated in previous section, the weight)...( 21 neeewof a semantic path neee...21 is givenby the sum of the weights of the productionsapplied in the derivation nG eeeS+ ...21 .
Theseweights have been learned using a perceptronmodel, trained with standard word sensedisambiguation data, such as the SemCor corpus.Examples of the rules in G are provided in thesubsequent Section 5.5 Application of the SSI algorithm to thedisambiguation of WordNet glossesFor the gloss disambiguation task, the SSIalgorithm is initialized as follows: In step 1, thelist I includes the synset S whose gloss we wish todisambiguate, and the list P includes all the termsin the gloss and in the gloss of the hyperonym ofS.
Words in the hyperonym?s gloss are useful toaugment the context available for disambiguation.In the following, we present a sample execution ofthe SSI algorithm for the gloss disambiguationtask applied to sense #1 of retrospective: ?anexhibition of a representative selection of anartist?s life work?.
For this task the algorithm usesa context enriched with the definition of the synsethyperonym, i.e.
art exhibition#1: ?an exhibition ofart objects (paintings or statues)?.Initially we have:I = { retrospective#1 }3P = { work, object, exhibition, life, statue, artist,selection, representative, painting, art }At first, I is enriched with the senses ofmonosemous words in the definition ofretrospective#1 and its hyperonym:I = { retrospective#1, statue#1, artist#1 }P = { work, object, exhibition, life, selection,representative, painting, art }since statue and artist are monosemous terms inWordNet.
During the first iteration, the algorithmfinds three matching paths4:retrospective#1 2  ofkind exhibition#2, statue#13  ofkind  art#1 and statue#13 For convenience here we denote I as a set ratherthan a list.4 With S R  i S?
we denote a path of i consecutiveedges labeled with the relation R interconnecting Swith S?.6  ofkind object#1This leads to:I = { retrospective#1, statue#1, artist#1,exhibition#2, object#1, art#1 }P = { work, life, selection, representative, painting}During the second iteration, ahyponymy/holonymy path (rule S2) is found:art#1 2  kindhas painting#1 (painting is a kindof art)which leads to:I = { retrospective#1, statue#1, artist#1,exhibition#2, object#1, art#1, painting#1 }P = { work, life, selection, representative }The third iteration finds a co-occurrence (topicrule) path between artist#1 and sense 12 of life(biography, life history):artist#1 topic  life#12then, we get:I = { retrospective#1, statue#1, artist#1,exhibition#2, object#1, art#1, painting#1, life#12}P = { work, selection, representative }The algorithm stops because no additionalmatches are found.
The chosen senses concerningterms contained in the hyperonym?s gloss were ofhelp during disambiguation, but are nowdiscarded.
Thus we have:GlossSynsets(retrospective#1) = { artist#1,exhibition#2, life#12, work#2 }6 EvaluationThe SSI algorithm is currently tailored for noundisambiguation.
Additional semantic knowledgeand ad-hoc rules would be needed to detectsemantic patterns centered on concepts associatedto verbs.
Current research is directed towardsintegrating in semantic graphs information fromFrameNet and VerbNet, but the main problem isharmonizing these knowledge bases withWordNet?s senses and relations inventory.
Asecond problem of SSI, when applied tounrestricted WSD tasks, is that it is designed todisambiguate with high precision, possibly lowrecall.
In many interesting applications of WSD,especially in information retrieval, improveddocument access may be obtained even when onlyfew words in a query are disambiguated, but thedisambiguation precision needs to be well overthe 70% threshold.
Supporting experiments aredescribed in (Navigli and Velardi, 2003).The results obtained by our system in Senseval-3 reflect these limitations (see Figure 2).The main run, named OntoLearn, uses athreshold to select only those senses with a weightover a given threshold.
OntoLearnEx uses a non-greedy version of the SSI algorithm.
Again, athreshold is used to accepts or reject sensechoices.
Finally, OntoLearnB uses the ?firstsense?
heuristics to select a sense, every since asense choice is below the threshold (or no patternsare found for a given word).82.60% 75.30%37.50%68.50%68.40%32.30%39.10%49.70%99.90%0%20%40%60%80%100%OntoLearn OntoLearnB OntoLearnExPrecision Recall AttemptedFigure 2.
Results of three runs submitted to Senseval-3.Table 1 shows the precision and recall ofOntoLearn main run by syntactic category.
Itshows that, as expected, the SSI algorithm iscurrently tuned for noun disambiguation.Nouns Verbs Adj.Precision 86.0% 69.4% 78.6%Recall 44.7% 13.5% 26.2%Attempted 52.0% 19.5% 33.3%Table 1.
Precision and Recall by syntactic category.The official Senseval-3 evaluation has beenperformed against a set of so called ?goldenglosses?
produced by Dan Moldovan and itsgroup5.
This test set however had severalproblems, that we partly detected and submitted tothe organisers.Besides some technical errors in the data set(presence of WordNet 1.7 and 2.0 senses, missingglosses, etc.)
there are sense-tagginginconsistencies that are very evident.For example, one of our highest performingsense tagging rules in SSI is the directhyperonymy path.
This rule reads as follows: ?ifthe word wj appears in the gloss of a synset Si, andif one of the synsets of wj, Sj, is the directhyperonym of Si, then, select Sj as the correctsense for wj?.An example is custom#4 defined as ?habitualpatronage?.
We have that:{custom-n#4} kind _ of  {trade,patronage-n#5}5 http://xwn.hlt.utdallas.edu/wsd.htmltherefore we select sense #5 of patronage, whileMoldovan?s ?golden?
sense is #1.We do not intend to dispute whether the?questionable?
sense assignment is the oneprovided in the golden gloss or rather thehyperonym selected by the WordNetlexicographers.
In any case, the detected patternsshow a clear inconsistency in the data.These patterns (313) have been submitted to theorganisers, who then decided to remove themfrom the data set.7 ConclusionThe interesting feature of the SSI algorithm,unlike many co-occurrence based and statisticalapproaches to WSD, is a justification (i.e.
a set ofsemantic patterns) to support a sense choice.Furthermore, each sense choice has a weightrepresenting the confidence of the system in itsoutput.
Therefore SSI can be tuned for highprecision (possibly low recall), an asset that weconsider more realistic for practical WSDapplications.Currently, the system is tuned for noundisambiguation, since we build structuralrepresentations of word senses using lexicalknowledge bases that are considerably richer fornouns.
Extending semantic graphs associated toverbs and adding appropriate interconnectionrules implies harmonizing WordNet and availablelexical resources for verbs, e.g.
FrameNet andVerbNet.
This extension is in progress.ReferencesH.
Bunke and A. Sanfeliu (editors) (1990)Syntactic and Structural pattern Recognition:Theory and Applications World Scientific, Seriesin Computer Science vol.
7, 1990.A.
Gangemi, R. Navigli and P. Velardi (2003)?The OntoWordNet Project: extension andaxiomatization of conceptual relations inWordNet?, 2nd Int.
Conf.
ODBASE, ed.
SpringerVerlag, 3-7 November 2003, Catania, Italy.B.
Magnini and G. Cavaglia (2000)?Integrating Subject Field Codes into WordNet?,Proceedings of  LREC2000, Atenas 2000.Milhalcea R., Moldovan D. I.
(2001)?eXtended WordNet: progress report?.
NAACL2001 Workshop on WordNet and other lexicalresources, Pittsburg, June 2001.Navigli R. and Velardi P. (2003) ?An Analysisof Ontology-based Query Expansion Strategies?,Workshop on Adaptive Text Extraction andMining September 22nd, 2003 Cavtat-Dubrovnik(Croatia), held in conjunction with ECML 2003.
