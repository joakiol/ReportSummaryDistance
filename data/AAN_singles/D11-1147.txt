Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1589?1599,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsRumor has it: Identifying Misinformation in MicroblogsVahed Qazvinian Emily Rosengren Dragomir R. Radev Qiaozhu MeiUniversity of MichiganAnn Arbor, MI{vahed,emirose,radev,qmei}@umich.eduAbstractA rumor is commonly defined as a state-ment whose true value is unverifiable.
Ru-mors may spread misinformation (false infor-mation) or disinformation (deliberately falseinformation) on a network of people.
Identi-fying rumors is crucial in online social mediawhere large amounts of information are easilyspread across a large network by sources withunverified authority.
In this paper, we addressthe problem of rumor detection in microblogsand explore the effectiveness of 3 categories offeatures: content-based, network-based, andmicroblog-specific memes for correctly iden-tifying rumors.
Moreover, we show how thesefeatures are also effective in identifying disin-formers, users who endorse a rumor and fur-ther help it to spread.
We perform our exper-iments on more than 10,000 manually anno-tated tweets collected from Twitter and showhow our retrieval model achieves more than0.95 in Mean Average Precision (MAP).
Fi-nally, we believe that our dataset is the firstlarge-scale dataset on rumor detection.
It canopen new dimensions in analyzing online mis-information and other aspects of microblogconversations.1 IntroductionA rumor is an unverified and instrumentally relevantstatement of information spread among people (Di-Fonzo and Bordia, 2007).
Social psychologists ar-gue that rumors arise in contexts of ambiguity, whenthe meaning of a situation is not readily apparent,or potential threat, when people feel an acute needfor security.
For instance rumors about ?office ren-ovation in a company?
is an example of an ambigu-ous context, and the rumor that ?underarm deodor-ants cause breast cancer?
is an example of a contextin which one?s well-being is at risk (DiFonzo et al,1994).The rapid growth of online social media has madeit possible for rumors to spread more quickly.
On-line social media enable unreliable sources to spreadlarge amounts of unverified information among peo-ple (Herman and Chomsky, 2002).
Therefore, it iscrucial to design systems that automatically detectmisinformation and disinformation (the former of-ten seen as simply false and the latter as deliberatelyfalse information).Our definition of a rumor is established based onsocial psychology, where a rumor is defined as astatement whose truth-value is unverifiable or delib-erately false.
In-depth rumor analysis such as deter-mining the intent and impact behind the spread ofa rumor is a very challenging task and is not possi-ble without first retrieving the complete set of socialconversations (e.g., tweets) that are actually aboutthe rumor.
In our work, we take this first step toretrieve a complete set of tweets that discuss a spe-cific rumor.
In our approach, we address two basicproblems.
The first problem concerns retrieving on-line microblogs that are rumor-related.
In the secondproblem, we try to identify tweets in which the ru-mor is endorsed (the posters show that they believethe rumor).2 Related WorkWe review related work on 3 main areas: Analyzingrumors, mining microblogs, and sentiment analysisand subjectivity detection.2.1 Rumor Identification and AnalysisThough understanding rumors has been the sub-ject of research in psychology for some time (All-port and Lepkin, 1945), (Allport and Postman,1947), (DiFonzo and Bordia, 2007), research has1589only recently begun to investigate how rumors aremanifested and spread differently online.
Mi-croblogging services, like Twitter, allow smallpieces of information to spread quickly to large au-diences, allowing rumors to be created and spread innew ways (Ratkiewicz et al, 2010).Related research has used different methods tostudy the spread of memes and false informationon the web.
Leskovec et al use the evolutionof quotes reproduced online to identify memes andtrack their spread overtime (Leskovec et al, 2009).Ratkiewicz et al (Ratkiewicz et al, 2010) createdthe ?Truthy?
system, identifying misleading politi-cal memes on Twitter using tweet features, includ-ing hashtags, links, and mentions.
Other projectsfocus on highlighting disputed claims on the Inter-net using pattern matching techniques (Ennals et al,2010).
Though our project builds on previous work,our work differs in its general focus on identifyingrumors from a corpus of relevant phrases and our at-tempts to further discriminate between phrases thatconfirm, refute, question, and simply talk about ru-mors of interest.Mendoza et al explore Twitter data to analyze thebehavior of Twitter users under the emergency situ-ation of 2010 earthquake in Chile (Mendoza et al,).
They analyze the re-tweet network topology andfind that the patterns of propagation in rumors dif-fer from news because rumors tend to be questionedmore than news by the Twitter community.2.2 Sentiment AnalysisThe automated detection of rumors is similar to tra-ditional NLP sentiment analysis tasks.
Previouswork has used machine learning techniques to iden-tify positive and negative movie reviews (Pang etal., 2002).
Hassan et al use a supervised Markovmodel, part of speech, and dependency patterns toidentify attitudinal polarities in threads posted toUsenet discussion posts (Hassan et al, 2010).
Oth-ers have designated sentiment scores for news sto-ries and blog posts based on algorithmically gener-ated lexicons of positive and negative words (God-bole et al, 2007).
Pang and Lee provide a detailedoverview of current techniques and practices in sen-timent analysis and opinion mining (Pang and Lee,2008; Pang and Lee, 2004).Though rumor classification is closely related toopinion mining and sentiment analysis, it presentsa different class of problem because we are con-cerned not just with the opinion of the person post-ing a tweet, but with whether the statements theypost appear controversial.
The automatic identifica-tion of rumors from a corpus is most closely relatedto the identification of memes done in (Leskovec etal., 2009), but presents new challenges since we seekto highlight a certain type of recurring phrases.
Ourwork presents one of the first attempts at automaticrumor analysis.2.3 Mining Twitter DataWith its nearly constant update of new posts andpublic API, Twitter can be a useful source forcollecting data to be used in exploring a num-ber of problems related to natural language pro-cessing and information diffusion (Bifet and Frank,2010).
Pak and Paroubek demonstrated experimen-tally that despite frequent occurrences of irregularspeech patterns in tweets, Twitter can provide a use-ful corpus for sentiment analysis (Pak and Paroubek,2010).
The diversity of Twitter users make thiscorpus especially valuable.
Ratkiewicz et alalsouse Twitter to detect and track misleading politicalmemes (Ratkiewicz et al, 2010).Along with many advantages, using Twitter as acorpus for sentiment analysis does present unusualchallenges.
Because posts are limited to 140 charac-ters, tweets often contain information in an unusu-ally compressed form and, as a result, grammar usedmay be unconventional.
Instances of sarcasm andhumor are also prevalent (Bifet and Frank, 2010).The procedures we used for the collection and anal-ysis of tweets are similar to those described in previ-ous work.
However, our goal of developing compu-tational methods to identify rumors being transmit-ted through tweets differentiates our project.3 Problem DefinitionAssume we have a set of tweets that are about thesame topic that has some controversial aspects.
Ourobjective in this work is two-fold: (1) Extract tweetsthat are about the controversial aspects of the storyand spread misinformation (Rumor retrieval).
(2)Identify users who believe that misinformation ver-sus users who refute or question the rumor (Belief1590Name Rumor Regular Expression Query Status #tweetsobama Is Barack Obama muslim?
Obama & (muslim|islam) false 4975airfrance Air France mid-air crash photos?
(air.france|air france) & (photo|pic|pix) false 505cellphone Cell phone numbers going public?
(cell|cellphone|cell phone) mostly false 215michelle Michelle Obama hired too many staff?
staff & (michelle obama|first lady|1st lady) partly true 299palin Sarah Palin getting divorced?
palin & divorce false 4423Table 1: List of rumor examples and their corresponding queries used to collect data from Twitterclassification).The following two tweets are two instances of thetweets written about president Obama and the Mus-lim world.
The first tweet below is about presidentObama and Muslim world, where the second tweetspread misinformation that president Obama is Mus-lim.
(non-rumor) ?As Obama bows to Muslim leadersAmericans are less safe not only at home but alsooverseas.
Note: The terror alert in Europe...
?
(rumor) ?RT @johnnyA99 Ann Coulter Tells LarryKing Why People Think Obama Is A Muslimhttp://bit.ly/9rs6pa #Hussein via @NewsBusters#tcot ..?The goal of the retrieval task is to discriminatebetween such tweets.
In the second task, we usethe tweets that are flagged as rumorous, and identifyusers that endorse (believe) the rumor versus userswho deny or question it.
The following three tweetsare about the same story.
The first user is a believerand the second and third are not.
(confirm) ?RT @moronwatch: Obama?s a Muslim.
Orif he?s not, he sure looks like one #whyimvotingre-publican.?
(deny) ?Barack Obama is a Christian man who hada Christian wedding with 2 kids baptised in Jesusname.
Tea Party clowns call that muslim #p2 #gop?
(doubtful) ?President Barack Obama?s Religion:Christian, Muslim, or Agnostic?
- The Newsof Today (Google): Share With Friend...http://bit.ly/bk42ZQ?The first task is substantially more challengingthan a standard IR task because of the requirement ofboth high precision (every result should be actuallydiscussing the rumor) and high recall (the set shouldbe complete).
To do this, we submit a handcraftedregexp (extracted from about.com) to Twitter and re-trieve a large primitive set of tweets that is supposedto have a high recall.
This set however, contains a lotof false positives, tweets that match the regexp butare not about the rumor (e.g., ?Obama meets muslimleaders?).
Moreover, a rumor is usually stated usingvarious instances (e.g., ?Barack HUSSEIN Obama?versus ?Obama is muslim?).
Our goal is then to de-sign a learning framework that filters all such falsepositives and retrieves various instances of the samerumorAlthough our second task, belief classification,can be viewed as an opinion mining task, it is sub-stantially different from opinion mining in nature.The difference from a standard opinion mining taskis that here we are looking for attitudes about a sub-tle statement (e.g., ?Palin is getting divorce?)
insteadof the overall sentiment of the text or the opiniontowards an explicit object or person (e.g., ?SarahPalin?
).4 DataAs September 2010, Twitter reports that its userspublish nearly 95 million tweets per day1.
Thismakes Twitter an excellent case to analyze misin-formation in social media.Our goal in this work was to collect and annotatea large dataset that includes all the tweets that arewritten about a rumor in a certain period of time.
Tocollect such a complete and self-contained datasetabout a rumor, we used the Twitter search API, andretrieved all the tweets that matched a given regularexpression.
This API is the only API that returns re-sults from the entire public Twitter stream and nota small randomly selected sample.
To overcome therate limit enforced by Twitter, we collected match-ing tweets once per hour, and remove any duplicates.To use the search API, we carefully designed reg-ular expression queries to be broad enough to match1http://twitter.com/about1591all the tweets that are about a rumor.
Each queryrepresents a popular rumor that is listed as ?false?or only ?partly true?
on About.com?s Urban Leg-ends reference site2 between 2009 and 2010.
Table 1lists the rumor examples that we used to collect ourdataset alng with their corresponding regular ex-pression queries and the number of tweets collected.4.1 AnnotationWe asked two annotators to go over all the tweetsin the dataset and mark each tweet with a ?1?
if itis about any of the rumors from Table 1, and witha ?0?
otherwise.
This annotation scheme will beused in our first task to detect false positives, tweetsthat match the broad regular expressions and are re-trieved, but are not about the rumor.
For instance,both of the following tweets match the regular ex-pression for the palin example, but only the sec-ond one is rumorous.
(0) ?McCain Divorces Palin over her ?untruths and outright lies?
in the book written for her.
McCain?steam says Palin is a petty liar and phony?
(1) ?Sarah and Todd Palin to divorce, according to localAlaska paper.
http://ow.ly/iNxF?We also asked the annotators to mark each pre-viously annotated rumorous tweet with ?11?
if thetweet poster endorses the rumor and with ?12?
if theuser refutes the rumor, questions its credibility, or isneutral.
(12) ?Sarah Palin Divorce Rumor Debunked on Face-book http://ff.im/62Evd?
(11) ?Todd and Sarah Palin to divorcehttp://bit.ly/15StNc?Our annotation of more than 10,400 tweets showsthat %35 of all the instances that matched the regu-lar expressions are false positives, tweets that are notrumor-related but match the initial queries.
More-over, among tweets that are about particular ru-mors, nearly %43 show the poster believe the rumor,demonstrating the importance of identifying misin-formation and those who are misinformed.
Table 2shows the basic statistics extracted from the annota-tions for each story.2http://urbanlegends.about.comRumor non-rumor (0) believe (11) deny/ (12) totaldoubtful/neutralobama 3,036 926 1,013 4975airfrance 306 71 128 505cellphone 132 74 9 215michelle 83 191 25 299palin 86 1,709 2,628 4,423total 3,643 2,971 3,803 10,417Table 2: Number of instances in each class from the an-notated datatask ?rumor retrieval 0.954belief classification 0.853Table 3: Inter-judge agreement in two annotation tasks interms of ?-statistic4.2 Inter-Judge AgreementTo calculate the annotation accuracy, we annotated500 instances twice.
These annotations were com-pared with each other, and the Kappa coefficient (?
)was calculated.
The ?
statistic is formulated as?
= Pr(a)?
Pr(e)1?
Pr(e)where Pr(a) is the relative observed agreementamong raters, and Pr(e) is the probability that anno-tators agree by chance if each annotator is randomlyassigning categories (Krippendorff, 1980; Carletta,1996).
Table 3 shows that annotators can reacha high agreement in both extracting rumors (?
=0.95) and identifying believers (?
= 0.85).5 ApproachIn this section, we describe a general framework,which given a tweet, predicts (1) whether it is arumor-related statement, and if so (2) whether theuser believes the rumor or not.
We describe 3 sets offeatures, and explain why these are intuitive to usefor identification of rumors.We process the tweets as they appear in the usertimeline, and do not perform any pre-processing.Specially, we think that capitalization might be animportant property.
So, we do not lower-case thetweet texts either.Our approach is based on building different Bayesclassifiers as high level features and then learninga linear function of these classifiers for retrieval inthe first task and classification in the second.
Each1592Bayes classifier, which corresponds to a feature fi,calculates the likelihood ratio for a given tweet t, asshown in Equation 1.P (?+i |t)P (?
?i |t)= P (?+i )P (?
?i )P (t|?+i )P (t|?
?i )(1)Here ?+i and ?
?i are two probabilistic models builtbased on feature fi using a set of positive (+) andnegative (?)
training data.
The likelihood ratio ex-presses how many times more likely the tweet t isunder the positive model than the negative modelwith respect to fi.For computational reasons and to avoid dealingwith very small numbers we use the log of the like-lihood ratio to build each classifier.LLi = logP (?+i |t)P (?
?i |t)= log P (?+i )P (?
?i )+ log P (t|?+i )P (t|?
?i )(2)The first term P (?+i )P (?
?i ) can be easily calculated us-ing the maximum likelihood estimates of the prob-abilities (i.e., the estimate of each probability is thecorresponding relative frequency).
The second termis calculated using various features that we explainbelow.5.1 Content-based FeaturesThe first set of features are extracted from the text ofthe tweets.
We propose 4 content based features.
Wefollow (Hassan et al, 2010) and present the tweetwith 2 different patterns:?
Lexical patterns: All the words and segmentsin the tweet are represented as they appear andare tokenized using the space character.?
Part-of-speech patterns: All words are replacedwith their part-of-speech tags.
To find the part-of-speech of a hashtag we treat it as a word(since they could have semantic roles in thesentence), by omitting the tag sign, and thenprecede the tag with the label TAG/.
We alsointroduce a new tag, URL, for URLs that appearin a tweet.From each tweet we extract 4 (2 ?
2) features,corresponding to unigrams and bigrams of each rep-resentation.
Each feature is the log-likelihood ra-tio calculated using Equation 2.
More formally,we represent each tweet t, of length n, lexicallyas (w1w2 ?
?
?wn) and with part-of-speech tags as(p1p2 ?
?
?
pn).
After building the positive and nega-tive models (?+, ??)
for each feature using the train-ing data, we calculate the likelihood ratio as definedin Equation 2 whereP (t|?+)P (t|??)
=n?j=1log P (wj |?+)P (wj |??
)(3)for unigram-lexical features (TXT1) andP (t|?+)P (t|??)
=n?1?j=1log P (wjwj+1|?+)P (wjwj+1|??
)(4)for bigram-based lexical features (TXT2).
Simi-larly, we define the unigram and bigram-based part-of-speech features (POS1 and POS2) as the log-likelihood ratio with respect to the positive and neg-ative part-of-speech models.5.2 Network-based FeaturesThe features that we have proposed so far are allbased on the content of individual tweets.
In thesecond set of features we focus on user behavior onTwitter.
We observe 4 types of network-based prop-erties, and build 2 features that capture them.Twitter enables users to re-tweet messages fromother people.
This interaction is usually easy to de-tect because the re-tweeted messages generally startwith the specific pattern: ?RT @user?.
We use thisproperty to infer about the re-tweeted message.Let?s suppose a user ui re-tweets a message t fromthe user uj (ui: ?RT @uj t?).
Intuitively, t is morelikely to be a rumor if (1) uj has a history of postingor re-tweeting rumors, or (2) ui has posted or re-tweeted rumors in the past.Given a set of training instances, we build a pos-itive (?+) and a negative (??)
user models.
Thefirst model is a probability distribution over all usersthat have posted a positive instance or have been re-tweeted in a positive instance.
Similarly, the sec-ond model is a probability distribution over users1593that have posted (or been re-tweeted in) a negativeinstance.
After building the models, for a giventweet we calculate two log-likelihood ratios as twonetwork-based features.The first feature is the log-likelihood ratio that uiis under a positive user model (USR1) and the sec-ond feature is the log-likelihood ratio that the tweetis re-tweeted from a user (uj) who is under a positiveuser model than a negative user model (USR2).The distinction between the posting user and there-tweeted user is important, since some times theusers modify the re-tweeted message in a way thatchanges its meaning and intent.
In the following ex-ample, the original user is quoting president Obama.The second user is re-tweeting the first user, but hasadded more content to the tweet and made it soundrumorous.original message (non-rumor) ?Obama says he?s do-ing ?Christ?s work?.
?re-tweeted (rumor) ?Obama says he?s doing ?Christ?swork.?
Oh my God, CHRIST IS A MUSLIM.
?5.3 Twitter Specific MemesOur final set of features are extracted from memesthat are specific to Twitter: hashtags and URLs.Previous work has shown the usefulness of thesememes (Ratkiewicz et al, 2010).5.3.1 HashtagsOne emergent phenomenon in the Twitter ecosys-tem is the use of hashtags: words or phrases prefixedwith a hash symbol (#).
These hashtags are createdby users, and are widely used for a few days, thendisappear when the topic is outdated (Huang et al,2010).In our approach, we investigate whether hashtagsused in rumor-related tweets are different from othertweets.
Moreover, we examine whether people whobelieve and spread rumors use hashtags that are dif-ferent from those seen in tweets that deny or ques-tion a rumor.Given a set of training tweets of positive and neg-ative examples, we build two statistical models (?+,??
), each showing the usage probability distributionof various hashtags.
For a given tweet, t, with a setof m hashtags (#h1 ?
?
?#hm), we calculate the log-likelihood ratio using Equation 2 whereFeature LL-ratio modelContentTXT1 content unigram content unigramTXT2 content bigram content unigramPOS1 content pos content pos unigramPOS2 content pos content pos bigramTwitterURL1 content unigram target URL unigramURL2 content bigram target URL bigramTAG hashtag hashtagNetwork USR1 tweeting user all users in the dataUSR2 re-tweeted user all users in the dataTable 4: List of features used in our optimization frame-work.
Each feature is a log-likelihood ratio calculatedagainst a a positive (+) and negative (?)
training models.P (t|?+)P (t|??)
=m?j=1log P (#hj |?+)P (#hj |??
)(5)5.3.2 URLsPrevious work has discussed the role of URLsin information diffusion on Twitter (Honeycutt andHerring, 2009).
Twitter users share URLs in theirtweets to refer to external sources or overcome thelength limit forced by Twitter.
Intuitively, if a tweetis a positive instance, then it is likely to be similar tothe content of URLs shared by other positive tweets.Using the same reasoning, if a tweet is a negativeinstance, then it should be more similar to the webpages shared by other negative instances.Given a set of training tweets, we fetch all theURLs in these tweets and build ?+ and ??
once forunigrams and once for bigrams.
These models aremerely built on the content of the URLs and ignorethe tweet content.
Similar to previous features, wecalculate the log-likelihood ratio of the content ofeach tweet with respect to ?+ and ??
for unigrams(URL1) and bigrams URL2).Table 4 summarizes the set of features used in ourproposed framework, where each feature is a log-likelihood ratio calculated against a positive (+) andnegative (?)
training models.
To build these lan-guage models, we use the CMU Language Modelingtoolkit (Clarkson and Rosenfeld, 1997).5.4 OptimizationWe build an L1-regularized log-linear model (An-drew and Gao, 2007) on various features discussedbefore to predict each tweet.
Suppose, a proceduregenerates a set of candidates for an input x. Also,1594let?s suppose ?
: X ?
Y ?
RD is a function thatmaps each (x, y) to a vector of feature values.
Here,the feature vector is the vector of coefficients corre-sponding to different network, content, and twitter-based properties, and the parameter vector ?
?
RD(D ?
9 in our experiments) assigns a real-valuedweight to each feature.
This estimator chooses ?
tominimize the sum of least squares and a regulariza-tion term R.??
= argmin?{12?i||?
?, xi?
?
yi||22 +R(?)}
(6)where the regularizer term R(?)
is the weighted L1norm of the parameters.R(?)
= ?
?j|?j | (7)Here, ?
is a parameter that controls the amount ofregularization (set to 0.1 in our experiments).Gao et.
al (Gao et al, 2007) argue that op-timizing L1-regularized objective function is chal-lenging since its gradient is discontinuous wheneversome parameters equal zero.
In this work, we usethe orthant-wise limited-memory quasi-Newton al-gorithm (OWL-QN), which is a modification of L-BFGS that allows it to effectively handle the dis-continuity of the gradient (Andrew and Gao, 2007).OWL-QN is based on the fact that when restrictedto a single orthant, the L1 regularizer is differen-tiable, and is in fact a linear function of ?.
Thus,as long as each coordinate of any two consecutivesearch points does not pass through zero R(?)
doesnot contribute at all to the curvature of the functionon the segment joining them.
Therefore, we can useL-BFGS to approximate the Hessian of L(?)
aloneand use it to build an approximation to the full reg-ularized objective that is valid on a given orthant.This algorithm works quite well in practice, and typ-ically reaches convergence in even fewer iterationsthan standard L-BFGS (Gao et al, 2007).6 ExperimentsWe design 2 sets of experiments to evaluate our ap-proach.
In the first experiment we assess the effec-tiveness of the proposed method when employed inan Information Retrieval (IR) framework for rumorretrieval and in the second experiment we employvarious features to detect users?
beliefs in rumors.6.1 Rumor RetrievalIn this experiment, we view different stories asqueries, and build a relevance set for each query.Each relevance set is an annotation of the entire10,417 tweets, where each tweet is marked as rel-evant if it matches the regular expression query andis marked as a rumor-related tweet by the annotators.For instance, according to Table 2 the cellphonedataset has only 83 relevant documents out of theentire 10,417 documents.For each query we use 5-fold cross-validation,and predict the relevance of tweets as a function oftheir features.
We use these predictions and rankall the tweets with respect to the query.
To evalu-ate the performance of our ranking model for a sin-gle query (Q) with the set of relevant documents{d1, ?
?
?
, dm}, we calculate Average Precision asAP (Q) = 1mm?k=1Precision(Rk) (8)where Rk is the set of ranked retrieval results fromthe top result to the kth relevant document, dk (Man-ning et al, 2008).6.1.1 BaselinesWe compare our proposed ranking model with anumber of other retrieval models.
The first two sim-ple baselines that indicate a difficulty lower-boundfor the problem are Random and Uniform meth-ods.
In the Random baseline, documents are rankedbased on a random number assignment to them.
Inthe Uniform model, we use a 5-fold cross validation,and in each fold the label of the test documents is de-termined by the majority vote from the training set.The main baseline that we use in this work, is theregular expression that was submitted to Twitter tocollect data (regexp).
Using the same regular ex-pression to mark the relevance of the documents willcause a recall value of 1.00 (since it will retrieve allthe relevant documents), but will also retrieve falsepositives, tweets that match the regular expressionbut are not rumor-related.
We would like to inves-tigate whether using training data will help us de-crease the rate of false positives in retrieval.Finally, using the Lemur Toolkit software3, weemploy a KL divergence retrieval model with3http://www.lemurproject.org/1595Dirichlet smoothing (KL).
In this model, documentsare ranked according to the negation of the diver-gence of query and document language models.More formally, given the query language model ?Q,and the document language model ?D, the docu-ments are ranked by ?D(?Q||?D), where D is theKL-divergence between the two models.D(?Q||?D) =?wp(w|?Q) logp(w|?Q)p(w|?D)(9)To estimate p(w|?D), we use Bayesian smoothingwith Dirichlet priors (Berger, 1985).ps(w|?D) =C(w,D) + ?.p(w|?S)?+?w C(w,D)(10)where, ?
is a parameter, C is the count function, andthetaS is the collection language model.
Higher val-ues of ?
put more emphasis on the collection model.Here, we try two variants of the model, one usingthe default parameter value in Lemur (?
= 2000),and one in which ?
is tuned based on the the data(?
= 10).
Using the test data to tune the parametervalue, ?, will help us find an upper-bound estimateof the effectiveness of this method.Table 5 shows the Mean Average Precision(MAP) and F?=1 for each method in the rumor re-trieval task.
This table shows that a method thatemploys training data to re-rank documents withrespect to rumors makes significant improvementsover the baselines and outperforms other strong re-trieval systems.6.1.2 Feature AnalysisTo investigate the effectiveness of using indi-vidual features in retrieving rumors, we perform5-fold cross validations for each query, usingdifferent feature sets each time.
Figure 1 showsthe average precision and recall for our pro-posed optimization system when content-based(TXT1+TXT2+POS1+POS2), network-based(USR1+USR2), and twitter specific memes(TAG+URL1+URL2) are employed individually.Figure 1 shows that features that are calculated us-ing the content language models are very effective inachieving high precision and recall.
Twitter specificfeatures, especially hashtags, can result in high pre-cisions but lead to a low recall value because manyFigure 1: Average precision and recall of the proposedmethod employing each set of features: content-based,network-based, and twitter specific.tweets do not share hashtags or are not written basedon the contents of external URLs.Finally, we find that user history can be a goodindicator of rumors.
However, we believe that thisfeature could be more helpful with a complete userset and a more comprehensive history of their activ-ities.6.1.3 Domain Training DataAs our last experiment with rumor retrieval we in-vestigate how much new labeled data from an emer-gent rumor is required to effectively retrieve in-stances of that particular rumor.
This experimenthelps us understand how our proposed frameworkcould be generalized to other stories.To do this experiment, we use the obama story,which is a large dataset with a significant number offalse positive instances.
We extract 400 randomlyselected tweets from this dataset and keep them fortesting.
We also build an initial training dataset ofthe other 4 rumors, and label them as not relevant.We assess the performance of the retrieval model aswe gradually add the rest of the obama tweets.
Fig-ure 2 shows both Average Precision and labeling ac-curacy versus the size of the labeled data used fromthe obama dataset.
This plot shows that both mea-sures exhibit a fast growth and reach 80% when thenumber of labeled data reaches 2000.6.2 Belief ClassificationIn previous experiments we showed that maximiz-ing a linear function of log-likelihood ratios is aneffective method in retrieving rumors.
Here, we in-1596Method MAP 95% C.I.
F?=1 95% C.I.Random 0.129 [-0.065, 0.323] 0.164 [-0.051, 0.379]Uniform 0.129 [-0.066, 0.324] 0.198 [-0.080, 0.476]regexp 0.587 [0.305, 0.869] 0.702 [0.479, 0.925]KL (?
= 2000) 0.678 [0.458, 0.898] 0.538 [0.248, 0.828]KL (?
= 10) 0.803 [0.641, 0.965] 0.681 [0.614, 0.748]LL (all 9 features) 0.965 [0.936, 0.994] 0.897 [0.828, 0.966]Table 5: Mean Average Precision (MAP) and F?=1 of each method in the rumor retrieval task.
(C.I.
: ConfidenceInterval)Method Accuracy Precision Recall F?=1 Win/Loss Ratiorandom 0.501 0.441 0.513 0.474 1.004uniform 0.439 0.439 1.000 0.610 0.781TXT 0.934 0.925 0.924 0.924 14.087POS 0.742 0.706 0.706 0.706 2.873content (TXT+POS) 0.941 0.934 0.930 0.932 15.892network (USR) 0.848 0.873 0.765 0.815 5.583TAG 0.589 0.734 0.099 0.175 1.434URL 0.664 0.630 0.570 0.598 1.978twitter (TAG+URL) 0.683 0.658 0.579 0.616 2.155all 0.935 0.944 0.906 0.925 14.395Table 6: Accuracy, precision, recall, F?=1, and win/loss ratio of belief classification using different features.Figure 2: Average Precision and Accuracy learning curvefor the proposed method employing all 9 features.vestigate whether this method, and in particular, theproposed features are useful in detecting users?
be-liefs in a rumor that they post about.
Unlike re-trieval, detecting whether a user endorses a rumor orrefutes it may be possible using similar methods re-gardless of the rumor.
Intuitively, linguistic featuressuch as negation (e.g., ?obama is not a muslim?
), orcapitalization (e.g., ?barack HUSSEIN obama ...?
),user history (e.g., liberal tweeter vs. conservativetweeter), hashtags (e.g., #tcot vs. #tdot), and URLs(e.g., links to fake airfrance crash photos) shouldhelp to identify endorsements.We perform this experiment by making a poolof all the tweets that are marked as ?rumorous?
inthe annotation task.
Table 2 shows that there are6,774 such tweets, from which 2,971 show beliefand 3,803 tweets show that the user is doubtful, de-nies, or questions it.Using various feature settings, we perform 5-foldcross-validation on these 6,774 rumorous tweets.Table 6 shows the results of this experiment in termsof F-score, classification accuracy, and win/loss ra-tio, the ratio of correct classification to an incorrect1597classification.7 ConclusionIn this paper we tackle the fairly unaddressed prob-lem of identifying misinformation and disinform-ers in Microblogs.
Our contributions in this pa-per are two-fold: (1) We propose a general frame-work that employs statistical models and maximizesa linear function of log-likelihood ratios to retrieverumorous tweets that match a more general query.
(2) We show the effectiveness of the proposed fea-ture in capturing tweets that show user endorsement.This will help us identify disinformers or users thatspread false information in online social media.Our work has resulted in a manually annotateddataset of 10,000 tweets from 5 different controver-sial topics.
To the knowledge of authors this is thefirst large-scale publicly available rumor dataset, andcan open many new dimensions in studying the ef-fects of misinformation or other aspects of informa-tion diffusion in online social media.In this paper we effectively retrieve instances ofrumors that are already identified and evaluated byan external source such as About.com?s Urban Leg-ends reference.
Identifying new emergent rumorsdirectly from the Twitter data is a more challengingtask.
As our future work, we aim to build a sys-tem that employs our findings in this paper and theemergent patterns in the re-tweet network topologyto identify whether a new trending topic is a rumoror not.8 AcknowledgmentsThe authors would like to thank Paul Resnick,Rahul Sami, and Brendan Nyhan for helpful discus-sions.
This work is supported by the National Sci-ence Foundation grant ?SoCS: Assessing Informa-tion Credibility Without Authoritative Sources?
asIIS-0968489.
Any opinions, findings, and conclu-sions or recommendations expressed in this paperare those of the authors and do not necessarily re-flect the views of the supporters.ReferencesFloyd H. Allport and Milton Lepkin.
1945.
Wartime ru-mors of waste and special privilege: why some peoplebelieve them.
Journal of Abnormal and Social Psy-chology, 40(1):3 ?
36.Gordon Allport and Leo Postman.
1947.
The psychologyof rumor.
Holt, Rinehart, and Winston, New York.Galen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of l1-regularized log-linear models.
In ICML ?07,pages 33?40.James Berger.
1985.
Statistical decision theory andBayesian Analysis (2nd ed.).
New York: Springer-Verlag.Albert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in twitter streaming data.
In BernhardPfahringer, Geoff Holmes, and Achim Hoffmann, edi-tors, Discovery Science, volume 6332 of Lecture Notesin Computer Science, pages 1?15.
Springer Berlin /Heidelberg.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: the kappa statistic.
Comput.
Linguist.,22(2):249?254.Philip Clarkson and Roni Rosenfeld.
1997.
Statisticallanguage modeling using the cmu-cambridge toolkit.Proceedings ESCA Eurospeech, 47:45?148.Nicholas DiFonzo and Prashant Bordia.
2007.
Rumor,gossip, and urban legend.
Diogenes, 54:19?35, Febru-ary.Nicholas DiFonzo, P. Prashant Bordia, and Ralph L. Ros-now.
1994.
Reining in rumors.
Organizational Dy-namics, 23(1):47?62.Rob Ennals, Dan Byler, John Mark Agosta, and BarbaraRosario.
2010.
What is disputed on the web?
In Pro-ceedings of the 4th workshop on Information Credibil-ity, WICOW ?10, pages 67?74.Jianfeng Gao, Galen Andrew, Mark Johnson, andKristina Toutanova.
2007.
A comparative study of pa-rameter estimation methods for statistical natural lan-guage processing.
In ACL ?07.Namrata Godbole, Manjunath Srinivasaiah, and StevenSkiena.
2007.
Large-scale sentiment analysis fornews and blogs.
In Proceedings of the InternationalConference on Weblogs and Social Media (ICWSM),Boulder, CO, USA.Ahmed Hassan, Vahed Qazvinian, and Dragomir Radev.2010.
What?s with the attitude?
identifying sentenceswith attitude in online discussions.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1245?1255, Cambridge,MA, October.
Association for Computational Linguis-tics.Edward S Herman and Noam Chomsky.
2002.
Manu-facturing Consent: The Political Economy of the MassMedia.
Pantheon.Courtenay Honeycutt and Susan C. Herring.
2009.
Be-yond microblogging: Conversation and collaboration1598via twitter.
Hawaii International Conference on Sys-tem Sciences, 0:1?10.Jeff Huang, Katherine M. Thornton, and Efthimis N.Efthimiadis.
2010.
Conversational tagging in twitter.In Proceedings of the 21st ACM conference on Hyper-text and hypermedia, HT ?10, pages 173?178.Klaus Krippendorff.
1980.
Content Analysis: An Intro-duction to its Methodology.
Beverly Hills: Sage Pub-lications.Jure Leskovec, Lars Backstrom, and Jon Kleinberg.2009.
Meme-tracking and the dynamics of the newscycle.
In KDD ?09: Proceedings of the 15th ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 497?506.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press.Marcelo Mendoza, Barbara Poblete, and Carlos Castillo.Twitter under crisis: Can we trust what we rt?Alexander Pak and Patrick Paroubek.
2010.
Twit-ter as a corpus for sentiment analysis and opinionmining.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European LanguageResources Association (ELRA).Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: sentiment analysis using subjectivity summariza-tion based on minimum cuts.
In ACL?04, Morristown,NJ, USA.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2:1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification using ma-chine learning techniques.
In Proceedings of confer-ence on Empirical methods in natural language pro-cessing, EMNLP?02, pages 79?86.Jacob Ratkiewicz, Michael Conover, Mark Meiss, BrunoGonc?alves, Snehal Patil, Alessandro Flammini, andFilippo Menczer.
2010.
Detecting and trackingthe spread of astroturf memes in microblog streams.CoRR, abs/1011.3768.1599
