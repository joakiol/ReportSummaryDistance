HMM Revises Low Marginal Probability by CRFfor Chinese Word Segmentation?Degen Huang, Deqin Tong, Yanyan LuoDepartment of Computer Science and EngineeringDalian University of Technologyhuangdg@dlut.edu.cn, {tongdeqin, ziyanluoyu}@gmail.com?The work described in this paper is supported by Microsoft Research Asia Funded Project.AbstractThis paper presents a Chinese wordsegmentation system for CIPS-SIGHAN2010 Chinese language processing task.Firstly, based on Conditional RandomField (CRF) model, with local featuresand global features, the character-basedtagging model is designed.
Secondly,Hidden Markov Models (HMM) is usedto revise the substrings with low marginalprobability by CRF.
Finally, confidencemeasure is used to regenerate the resultand simple rules to deal with the stringswithin letters and numbers.
As is wellknown that character-based approach hasoutstanding capability of discoveringout-of-vocabulary (OOV) word, but ex-ternal information of word lost.
HMMmakes use of word information to in-crease in-vocabulary (IV) recall.
We par-ticipate in the simplified Chinese wordsegmentation both closed and open teston all four corpora, which belong to dif-ferent domains.
Our system achieves bet-ter performance.1 IntroductionChinese Word Segmentation (CWS) has wit-nessed a prominent progress in the first fourSIGHAN Bakeoffs.
Since Xue (2003) usedcharacter-based tagging, this method has at-tracted more and more attention.
Some previouswork (Peng et al, 2004; Tseng et al, 2005; Lowet al, 2005) illustrated the effectiveness of usingcharacters as tagging units, while literatures(Zhang et al, 2006; Zhao and Kit, 2007a; Zhangand Clark, 2007) focus on employing lexicalwords or subwords as tagging units.
Because theword-based models can capture the word-levelcontextual information and IV knowledge.
Be-sides, many strategies are proposed to balancethe IV and OOV performance (Wang et al,2008).CRF has been widely used in sequence label-ing tasks and has a good performance (Laffertyet al, 2001).
Zhao and Kit (2007b; 2008) at-tempt to integrate global information with localinformation to further improve CRF-based tag-ging method of CWS, which provides a solidfoundation for strengthening CRF learning withunsupervised learning outcomes.In order to increase the accuracy of taggingusing CRF, we adopt the strategy, which is: if themarginal probability of characters is lower than athreshold, the modified component based onHMM will be trigged; combining the confidencemeasure the results will be regenerated.2 Our word segmentation systemIn this section, we describe our system in moredetails.
Three modules are included in our sys-tem: a basic character-based CRF tagger, HMMwhich revises the substrings with low marginalprobability and confidence measure which com-bines them to regenerate the result.
In addition,we also use some rules to deal with the stringswithin letters and numbers.2.1 Character-based CRF taggerTag Set A 6-tag set is adopted in our system.
Itincludes six tags: B, B2, B3, M, E and S. Here,Tag B and E stand for the first and the last posi-tion in a multi-character word, respectively.
Sstands for a single-character word.
B2 and B3stand for the second and the third position in amulti-character word.
M stands for the fourth ormore rear position in a multi-character wordwith more than four characters.
The 6-tag set isproved to work more effectively than other tagsets in improving the segmentation performanceof CRFs by Zhao et al (2006).Feature templates In our system, six n-gramtemplates, namely, C-1, C0, C1, C-1C0, C0C1,C-1C1 are selected as features, where C stands fora character and the subscripts -1, 0 and 1 standfor the previous, current and next character, re-spectively.
Furthermore, another one is charactertype feature template T-1T0T1.
We use fourclasses of character sets which are predefined as:class N represents numbers, class L representsnon-Chinese letters, class P represents punctua-tion labels and class C represents Chinese char-acters.Except for the character feature, we also em-ploy global word feature templates.
The basicidea of using global word information for CWSis to inform the supervised learner how likely itis that the subsequence can be a word candidate.The accessor variety (AV) (Feng et al, 2005) isopted as global word feature, which is integratedinto CRF successfully in literatures (Zhao andKit, 2007b; Zhao and Kit, 2008).
The AV valueof a substring s  is defined as:{ }( ) min ( ), ( )av avAV s L s R s=     (1)Where the left and right AV values ( )avL sand ( )avR s  are defined, respectively, as thenumber of its distinct predecessors and thenumber of its distinct successors.Multiple feature templates are used to repre-sent word candidates of various lengths identi-fied by the AV criterion.
Meanwhile, in order toalleviate the sparse data problem, we follow thefeature function definition for a word candidates  with a score ( )AV s  in Zhao and Kit (2008),namely:( )nf s t= , 12 ( ) 2t tAV s +?
<     (2)In order to improve the efficiency, all candi-dates longer than five characters are given up.The AV features of word candidates can?t di-rectly be utilized to direct CRF learning beforebeing transferred to the information of characters.So we only choose the one with the greatest AVscore to activate the above feature function forthat character.In the open test, we only add another featureof ?FRE?, the basic idea of which is if a stringmatches a word in an existing dictionary, it maybe a clue that the string is likely a true word.Then more word boundary information can beobtained, which may be helpful for CRF learn-ing on CWS.
The dictionary we used isdownloaded from the Internet?
and consists of108,750 words with length of one to four char-acters.
We get FRE features similar to the AVfeatures.2.2 HMM revises substrings with low mar-ginal probabilityThe MP (short for marginal probability) of eachcharacter labeled with one of the six tags can begot separately through the basic CRF tagger.
Here,B replaces ?B?
and ?S?
, and I represents othertags (?B2?, ?B3?, ?M?, ?E?).
So each character hascorresponding new MP as defined in formula (3)and (4).
( )S BBtP PPP+= ?
(3)2 3( )B B M EItP P P PPP+ + += ?
(4)Where { }2 3, , , , ,t S B B B M E?
and tP can becalculated by using forward-backward algorithmand more details are in Lafferty et al (2001).A low confident word refers to a word withword boundary ambiguity which can be reflectedby the MP of the first character of a word.
Thatis, it?s a low confident word if the MP of the firstcharacter of the word is lower than a threshold?
(it?s an empirical value and can be obtainedby experiments).
After getting the new MP, allthese low confident candidate words are recom-bined with their direct predecessors until theoccurrence of a word that the MP of its firstcharacter is above the threshold ?
, and then anew substring is generated for post processing.Then, we use class-based HMM to re-segmentthe substrings mentioned above.
Given a word?http://ccl.pku.edu.cn/doubtfire/Course/Chinese%20Information%20Processing/Source_Code/Chapter_8/Lexicon_full.zipwi, a word class ci is the word itself.
Let W  bethe word sequence, let C  be its class sequence,and let#W be the segmentation result with themaximum likelihood.
Then, a class-based HMMmodel (Liu, 2004) can be got.# arg max ( )WW P W== arg max ( | ) ( )WP W C P C=1 21... 1arg max '( | ) ( | )mmi i i iw w w ip w c P c c?=?=1 21... 1arg max ( | )mmi iw w w iP c c?=?
(5)Where 1( | )i iP c c ?
indicates the transitiveprobability from one class to another and it canbe obtained from training corpora.The word boundary of results from HMM isalso represented by tag ?B?
and ?I?
which mean-ing are the same as mentioned in above.2.3 Confidence measure and post process-ing for final resultThere are two segmentation results for substringswith low MP candidates after reprocessing usingHMM.
Analyzing experiments data, we findwrong tags labeled by CRF are mainly: OOVwords in test data, IV words and incorrect wordsrecognized by CRF.
Rectifying the tags withlower MP simply may produce an even worseperformance in some case.
For example, someOOV words are recognized correctly by CRF butwith low MP.
So, we can?t accept the revisedresults completely.
A confidence measure ap-proach is used to resolve this problem.
Its calcu-lation is defined as:(1 )o oC C CP P P?= + ?
(6)oCP is the MP of the character as ?I?, ?
is thepremium coefficient.
Based on the new value, athreshold t  was used, if the value was lowerthan t , the original tag ?I?
will be rejected andchanged into the tag ?B?
which is labeled byHMM.At last, we use a simple rule to post-process theresult directed at the strings that containing letters,numbers and punctuations.
If the punctuation (notall punctuations) is half-width and the string be-fore or after are composed of letters and numbers,combine all into a string as a whole.
For an ex-ample, ?.
?, ?/?, ?
:?, ?%?
and ?\?
are usually recog-nized as split tokens.
So, it needs handling addi-tionally.3 Experiments results and analysisWe evaluate our system on the corpora given byCIPS-SIGHAN 2010.
There are four test corporawhich belong to different domains.
The detailsare showed in table 1.Domain Testing Data OOV rateA 149K 0.069B 165K 0.152C 151K 0.110D 157K 0.087Table 1.
Test corpora detailsA, B, C and D represent literature, computerscience, medical science and finance, respec-tively.3.1 Closed testThe rule for the closed test in Bakeoff is that noadditional information beyond training corpora isallowed.
Following the rule, the closed test isdesigned to compare our system with other CWSsystems.
Five metrics of SIGHAN Bakeoff areused to evaluate the segmentation results: F-score(F), recall (R), precision (P), the recall on IVwords (RIV) and the recall on OOV words (Roov).The closed test results are presented in table 2.Domain R P F Roov R?IV0.932 0.936 0.934 0.662 0.952A0.940 0.942 0.941 0.649 0.9610.950 0.948 0.949 0.831 0.971B0.953 0.950 0.951 0.827 0.9750.934 0.932 0.933 0.751 0.957C0.942 0.936 0.939 0.750 0.9650.955 0.957 0.956 0.837 0.966D0.959 0.960 0.959 0.827 0.972Table 2.
Evaluation closed results on all data sets?In order to analyze our results, we got value of RIV fromthe organizers because it can?t be obtained from the scoringsystem on http://nlp.ict.ac.cn/demo/CIPS-SIGHAN2010/#.In each domain, the first line shows the resultsof our basic CRF segmenter and the second oneshows the final results dealt with HMM throughconfidence measure, which make it clear thatusing the confidence measure can improve theoverall F-score by increasing value of R and P.Domain ID R P F Roov RIV5 0.945 0.946 0.946 0.816 0.954our 0.940 0.942 0.941 0.649 0.961 A12 0.937 0.937 0.937 0.652 0.958our 0.953 0.950 0.951 0.827 0.97511 0.948 0.945 0.947 0.853 0.965 B12 0.941 0.940 0.940 0.757 0.974our 0.942 0.936 0.939 0.750 0.96518 0.937 0.934 0.936 0.761 0.959 C5 0.940 0.928 0.934 0.761 0.962our 0.959 0.960 0.959 0.827 0.97212 0.957 0.956 0.957 0.813 0.971 D9 0.956 0.955 0.956 0.857 0.965Table 3.
Comparison our closed results with the top three in all test setsNext, we compare it with other top three sys-tems.
From the table 3 we can see that our systemachieves better performance on closed test.
Incontrast, the values of RIV of our method are su-perior to others?, which contributes to the modelwe use.
Whether the features of AV for charac-ter-based CRF tagger or HMM revising, they allmake good use of word information of trainingcorpora.3.2 Open testIn the open test, the only additional source weuse is the dictionary mentioned above.
We getone first and two third best.
Our result is showedin table 4.
Compared with closed test, the valueof RIV is increased in all test corpora.
But weonly get the higher value of F in domain of lit-erature.
The reasons will be analyzed as follows:In the open test, the OOV words are split intopieces because our model may be more depend-ent on the dictionary information.
Consequently,we get higher value of R but lower P. The train-ing corpora are the same as closed test, but it isdifferent that FRE features are added.
The addi-tional features enhance the original informationof IV words, so the value of RIV is improved tosome extent.
However, they have side effects forOOV segmentation.
We will continue to solvethis problem in the future work.Domain R P F Roov RIV0.956 0.947 0.952 0.636 0.980A0.958 0.953 0.955 0.655 0.9810.943 0.921 0.932 0.716 0.985B0.948 0.929 0.939 0.735 0.9860.947 0.915 0.931 0.659 0.983C0.951 0.92 0.935 0.67 0.9860.962 0.948 0.955 0.760 0.981D0.964 0.95 0.957 0.763 0.983Table 4.
Evaluation open results on all test sets4 Conclusions and future workIn this paper, a detailed description on a Chinesesegmentation system is presented.
Based onintermediate results from a CRF tagger, whichemploys local features and global features, weuse class-based HMM to revise the substringswith low marginal probabilities.
Then, a confi-dence measure is introduced to combine the tworesults.
Finally, we post process the stringswithin letters, numbers and punctuations usingsimple rules.
The results above show that oursystem achieves the state-of-the-art performance.The MP plays the important role in our methodand HMM revises some errors identified by CRF.Besides, the word features are proved to be in-formative cues in obtaining high quality MP.Therefore, our future work will focus on how tomake CRF generate more reliable MP of char-acters, including exploring other word informa-tion or more unsupervised segmentation infor-mation.ReferencesFeng Haodi, Kang Chen, Chuyu Kit, Xiaotie Deng.2005.
Unsupervised segmentation of Chinese cor-pus using accessor variety, In: Natural LanguageProcessing IJCNLP, pages 694-703, Sanya, China.Lafferty John, Andrew McCallum and FernandoPereira.
2001.
Conditional Random Fields: prob-abilistic models for segmenting and labeling se-quence data, In: Proceedings of ICML-18, pages282-289, Williams College, USA.Liu Qun, Huaping Zhang, Hongkui Yu and XueqiChen.
2004.
Chinese lexical analysis using cas-caded Hidden Markov Model, Journal of computerresearch and development 41(8): 1421-1429.Low Kiat Jin, Hwee Tou Ng and Wenyuan Guo.
2005.A Maximum Entropy Approach to Chinese WordSegmentation.
In: Proceedings of the FourthSIGHAN Workshop on Chinese Language Proc-essing, pages 161-164, Jeju Island, Korea.Peng Fuchun, Fangfang Feng and Andrew McCallum.2004.
Chinese segmentation and new word detec-tion using Conditional Random Fields, In: COL-ING 2004, pages 562-568, Geneva, Switzerland.Tseng Huihsin, Pichuan Chang et al 2005.
A Condi-tional Random Field Word Segmenter for SIGHANBakeoff 2005.
In: Proceedings of the FourthSIGHAN Workshop on Chinese Language Proc-essing, pages 168-171, Jeju Island, Korea.Wang Zhenxing, Changning Huang and Jingbo Zhu.2008.
Which perform better on in-vocabulary wordsegmentation: based on word or character?
In:Processing of the Sixth SIGHAN Workshop onChinese Language Processing, pages 61-68, Hy-derabad, India.Xue Nianwen.
2003.
Chinese word segmentation ascharacter tagging, Computational Linguistics andChinese Language Processing 8(1): 29-48.Zhang Yue and Stephen Clark.
2007.
Chinese Seg-mentation with a Word-Based Perceptron Algo-rithm.
In: Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics,pages 840-847, Prague, Czech Republic.Zhang Ruiqiang, Genichiro Kikui and Eiichiro Sumita.2006.
Subword-based  tagging  by  ConditionalRandom Fields for Chinese word segmentation, In:Proceedings  of  the  Human  LanguageTechnology Conference of the NAACL, pages193-196, New York, USA.Zhao Hai, Changning Huang, Mu Li and Baoliang Lu.2006.
Effective tag set selection in Chinese wordsegmentation via Conditional Random Field mod-eling, In: PACLIC-20, pages 87-94, Wuhan, China.Zhao Hai and Chunyu Kit.
2007a.
Effective subse-quence based tagging for Chinese word segmenta-tion, Journal of Chinese Information Processing21(5): 8-13.Zhao Hai and Chunyu Kit.
2007b.
Incorporatingglobal information into supervised learning forChinese word segmentation, In: PACLING-2007,pages 66-74, Melbourne, Australia.Zhao Hai and Chunyu Kit.
2008.
Unsupervised seg-mentation helps supervised learning of charactertagging for word segmentation and named entityrecognition, In: Proceedings of the Six SIGHANWorkshop on Chinese Language Processing, pages106-111, Hyderabad, India.
