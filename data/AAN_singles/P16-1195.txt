Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2073?2083,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSummarizing Source Code using a Neural Attention ModelSrinivasan Iyer Ioannis Konstas Alvin Cheung Luke ZettlemoyerComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195{sviyer,ikonstas,akcheung,lsz}@cs.washington.eduAbstractHigh quality source code is often pairedwith high level summaries of the compu-tation it performs, for example in codedocumentation or in descriptions postedin online forums.
Such summaries areextremely useful for applications such ascode search but are expensive to manuallyauthor, hence only done for a small frac-tion of all code that is produced.
In thispaper, we present the first completely data-driven approach for generating high levelsummaries of source code.
Our model,CODE-NN , uses Long Short Term Mem-ory (LSTM) networks with attention toproduce sentences that describe C# codesnippets and SQL queries.
CODE-NNis trained on a new corpus that is auto-matically collected from StackOverflow,which we release.
Experiments demon-strate strong performance on two tasks:(1) code summarization, where we estab-lish the first end-to-end learning resultsand outperform strong baselines, and (2)code retrieval, where our learned modelimproves the state of the art on a recentlyintroduced C# benchmark by a large mar-gin.1 IntroductionBillions of lines of source code reside in onlinerepositories (Dyer et al, 2013), and high qualitycode is often coupled with natural language (NL)in the form of instructions, comments, and docu-mentation.
Short summaries of the overall com-putation the code performs provide a particularlyuseful form of documentation for a range of appli-cations, such as code search or tutorials.
However,such summaries are expensive to manually author.1.
Source Code (C#):public int TextWidth(string text) {TextBlock t = new TextBlock ();t.Text = text;return(int)Math.Ceiling(t.ActualWidth );}Descriptions:a.
Get rendered width of string rounded up tothe nearest integerb.
Compute the actual textwidth inside atextblock2.
Source Code (C#):var input = "Hello";var regEx = new Regex("World");return !regEx.IsMatch(input );Descriptions:a.
Return if the input doesn?t contain aparticular word in itb.
Lookup a substring in a string using regex3.
Source Code (SQL):SELECT Max(marks) FROM stud_recordsWHERE marks <(SELECT Max(marks) FROM stud_records );Descriptions:a.
Get the second largest value of a columnb.
Retrieve the next max record in a tableFigure 1: Code snippets in C# and SQL and theirsummaries in NL, from StackOverflow.
Our goalis to automatically generate summaries from codesnippets.As a result, this laborious process is only done fora small fraction of all code that is produced.In this paper, we present the first completelydata-driven approach for generating short high-level summaries of source code snippets in natu-ral language.
We focus on C#, a general-purposeimperative language, and SQL, a declarative lan-guage for querying databases.
Figure 1 shows ex-ample code snippets with descriptions that sum-marize the overall function of the code, with thegoal to generate high level descriptions, such as2073lookup a substring in a string.
Generating sucha summary is often challenging because the textcan include complex, non-local aspects of the code(e.g., consider the phrase ?second largest?
in Ex-ample 3 in Figure 1).
In addition to being di-rectly useful for interpreting uncommented code,high-quality generation models can also be usedfor code retrieval, and in turn, for natural languageprogramming by applying nearest neighbor tech-niques to a large corpus of automatically summa-rized code.Natural language generation has traditionallybeen addressed as a pipeline of modules that de-cide ?what to say?
(content selection) and ?howto say it?
(realization) separately (Reiter and Dale,2000; Wong and Mooney, 2007; Chen et al, 2010;Lu and Ng, 2011).
Such approaches require super-vision at each stage and do not scale well to largedomains.
We instead propose an end-to-end neuralnetwork called CODE-NN that jointly performscontent selection using an attention mechanism,and surface realization using Long Short TermMemory (LSTM) networks.
The system generatesa summary one word at a time, guided by an at-tention mechanism over embeddings of the sourcecode, and by context from previously generatedwords provided by a LSTM network (Hochreiterand Schmidhuber, 1997).
The simplicity of themodel allows it to be learned from the training datawithout the burden of feature engineering (Angeliet al, 2010) or the use of an expensive approx-imate decoding algorithm (Konstas and Lapata,2013).Our model is trained on a new dataset of codesnippets with short descriptions, created usingdata gathered from Stackoverflow,1a popular pro-gramming help website.
Since access is open andunrestricted, the content is inherently noisy (un-grammatical, non-parsable, lacking content), butas we will see, it still provides strong signal forlearning.
To reliably evaluate our model, we alsocollect a clean, human-annotated test set.2We evaluate CODE-NN on two tasks: codesummarization and code retrieval (Section 2).
Forsummarization, we evaluate using automatic met-rics such as METEOR and BLEU-4, together witha human study for naturalness and informative-ness of the output.
The results show that CODE-NN outperforms a number of strong baselines and,1http://stackoverflow.com2Data and code are available at https://github.com/sriniiyer/codenn.to the best of our knowledge, CODE-NN is thefirst approach that learns to generate summaries ofsource code from easily gathered online data.
Wefurther use CODE-NN for code retrieval for pro-gramming related questions on a recent C# bench-mark, and results show that CODE-NN improvesthe state of the art (Allamanis et al (2015b)) formean reciprocal rank (MRR) by a wide margin.2 TasksCODE-NN generates a NL summary of sourcecode snippets (GEN task).
We have also usedCODE-NN on the inverse task to retrieve sourcecode given a question in NL (RET task).Formally, let UCbe the set of all code snippetsand UNbe the set of all summaries in NL.
For atraining corpus with J code snippet and summarypairs (cj, nj), 1 ?
j ?
J, cj?
UC, nj?
UN, wedefine the following two tasks:GEN For a given code snippet c ?
UC, the goalis to produce a NL sentence n??
UNthat max-imizes some scoring function s ?
(UC?
UN?R):n?= argmaxns(c, n) (1)RET We also use the scoring function s to re-trieve the highest scoring code snippet c?jfrom ourtraining corpus, given a NL question n ?
UN:c?j= argmaxcjs(cj, n), 1 ?
j ?
J (2)In this work, s is computed using an LSTM neu-ral attention model, to be described in Section 5.3 Related WorkAlthough we focus on generating high-level sum-maries of source code snippets, there has beenwork on producing code descriptions at other lev-els of abstraction.
Movshovitz-Attias and Co-hen (2013) study the task of predicting class-levelcomments by learning n-gram and topic modelsfrom open source Java projects and testing it us-ing a character-saving metric on existing com-ments.
Allamanis et al (2015a) create modelsfor suggesting method and class names by embed-ding them in a high dimensional continuous space.Sridhara et al (2010) present a pipeline that gener-ates summaries of Java methods by selecting rel-evant content and generating phrases using tem-plates to describe them.
There is also work onimproving program comprehension (Haiduc et al,20742010), identifying cross-cutting source code con-cerns (Rastkar et al, 2011), and summarizing soft-ware bug reports (Rastkar et al, 2010).
To the bestof our knowledge, we are the first to use learningtechniques to construct completely new sentencesfrom arbitrary code snippets.Source code summarization is also related togeneration from formal meaning representations.Wong and Mooney (2007) present a system thatlearns to generate sentences from lambda calculusexpressions by inverting a semantic parser.
Meiet al (2016), Konstas and Lapata (2013), and An-geli et al (2010) create learning algorithms for textgeneration from database records, again assumingdata that pairs sentences with formal meaning rep-resentations.
In contrast, we present algorithmsfor learning from easily gathered web data.In the database community, Simitsis and Ioan-nidis (2009) recognize the need for SQL databasesystems to talk back to users.
Koutrika et al(2010) built an interactive system (LOGOS) thattranslates SQL queries to text using NL templatesand database schemas.
Similarly there has beenwork on translating SPARQL queries to naturallanguage using rules to create dependency treesfor each section of the query, followed by a trans-formation step to make the output more natural(Ngonga Ngomo et al, 2013).
These approachesare not learning based, and require significantmanual template-engineering efforts.We use recurrent neural networks (RNN) basedon LSTMs and neural attention to jointly modelsource code and NL.
Recently, RNN-based ap-proaches have gained popularity for text gener-ation and have been used in machine transla-tion (Sutskever et al, 2011), image and video de-scription (Karpathy and Li, 2015; Venugopalan etal., 2015; Devlin et al, 2015), sentence summa-rization (Rush et al, 2015), and Chinese poetrygeneration (Zhang and Lapata, 2014).
Perhapsmost closely related, Wen et al (2015) generatetext for spoken dialogue systems with a two-stageapproach, comprising an LSTM decoder seman-tically conditioned on the logical representationof speech acts, and a reranker to generate the fi-nal output.
In contrast, we design an end-to-endattention-based model for source code.For code retrieval, Allamanis et al (2015b) pro-posed a system that uses Stackoverflow data andweb search logs to create models for retrievingC# code snippets given NL questions and viceversa.
They construct distributional representa-tions of code structure and language and com-bine them using additive and multiplicative mod-els to score (code, language) pairs, an approachthat could work well for retrieval but cannot beused for generation.
We learn a neural generationmodel without using search logs and show that itcan also be used to score code for retrieval, withmuch higher accuracy.Synthesizing code from language is an alter-native to code retrieval and has been studiedin both the Systems and NLP research com-munities.
Giordani and Moschitti (2012), Liand Jagadish (2014), and Gulwani and Marron(2014) synthesize source code from NL queriesfor database and spreadsheet applications.
Sim-ilarly, Lei et al (2013) interpret NL instruc-tions to machine-executable code, and Kushmanand Barzilay (2013) convert language to regu-lar expressions.
Unlike most synthesis methods,CODE-NN is domain agnostic, as we demonstrateits applications on both C# and SQL.4 DatasetWe collected data from StackOverflow (SO), apopular website for posting programming-relatedquestions.
Anonymized versions of all the postscan be freely downloaded.3Each post can havemultiple tags.
Using the C# tag for C# and the sql,database and oracle tags for SQL, we were ableto collect 934,464 and 977,623 posts respectively.4Each post comprises a short title, a detailed ques-tion, and one or more responses, of which one canbe marked as accepted.
We found that the textin the question and responses is domain-specificand verbose, mixed with details that are irrelevantfor our tasks.
Also, code snippets in responsesthat were not accepted were frequently incorrector tangential to the question asked.
Thus, we ex-tracted only the title from the post and use the codesnippet from those accepted answers that containexactly one code snippet (using <code> tags).
Weadd the resulting (title, query) pairs to our corpus,resulting in a total of 145,841 pairs for C# and41,340 pairs for SQL.Cleaning We train a semi-supervised classifierto filter titles like ?Difficult C# if then logic?
or?How can I make this query easier to write??
thatbear no relation to the corresponding code snippet.3http://archive.org/details/stackexchange4The data was downloaded in Dec 2014.2075To do so, we annotate 100 titles as being clean ornot clean for each language and use them to boot-strap the algorithm.
We then use the remainingtitles in our training set as an unsupervised sig-nal, and obtain a classification accuracy of over73% on a manually labeled test set for both lan-guages.
For the final dataset, we retain 66,015 C#(title, query) pairs and 32,337 SQL pairs that areclassified as clean, and use 80% of these datasetsfor training, 10% for validation and 10% for test-ing.Parsing Given the informal nature of Stack-Overflow, the code snippets are approximate an-swers that are usually incomplete.
For example,we observe that only 12% of the SQL queriesparse without any syntactic errors (using zql5).We therefore aim to perform a best-effort parseof the code snippet, using modified versions ofan ANTLR parser for C# (Parr, 2013) and python-sqlparse (Albrecht, 2015) for SQL.
We strip out allcomments and to avoid being context specific, wereplace literals with tokens denoting their types.In addition, for SQL, we replace table and columnnames with numbered placeholder tokens whilepreserving any dependencies in the query.
Forexample, the SQL query in Figure 1 is repre-sented as SELECT MAX(col0) FROM tab0 WHERE col0 <(SELECT MAX(col0) FROM tab0).Data Statistics The structural complexity andsize of the code snippets in our dataset makes ourtasks challenging.
More than 40% of our C# cor-pus comprises snippets with three or more state-ments and functions, and 20% contains loops andconditionals.
Also, over a third of our SQL queriescontain one or more subqueries and multiple ta-bles, columns and functions (like MIN, MAX, SUM).On average, our C# snippets are 38 tokens longand the queries in our corpus are 46 tokens long,while titles are 9-12 words long.
Table 2 showsthe complete data statistics.Human Annotation For the GEN task, we usen-gram based metrics (see Section 6.1.2) of thesummary generated by our model with respect tothe actual title in our corpus.
Titles can be short,and a given code snippet can be described in manydifferent ways with little overlapping content be-tween them.
For example, the descriptions for thesecond code snippet in Figure 1 share very fewwords with each other.
To address these limita-5http://zql.sourceforge.netC## Statements # Functions?
3 23,611 (44.7%) ?
3 26,541 (51.0%)?
4 17,822 (33.7%) ?
4 20,221 (38.2%)# Loops # Conditionals?
1 10,676 (20.0%) ?
1 11,819 (22.3%)SQL# Subqueries # Tables?
1 11,418 (35%) ?
3 14,695 (44%)?
2 3,625 (11%) ?
4 10,377 (31%)# Columns # Functions?
5 12,366 (37%) ?
3 6,290 (19%)?
6 9,050 (27%) ?
4 3,973 (12%)Table 1: Statistics for code snippets in our dataset.C#Avg.
code length 38 tokens # tokens 91,156Avg.
title length 12 words # words 24,857SQLAvg.
query length 46 tokens # tokens 1,287Avg.
title length 9 words # words 10,086Table 2: Average code and title lengths togetherwith vocabulary sizes for C# and SQL after post-processing.tions, we extend our test set by asking human an-notators to provide two additional titles for 200snippets chosen at random from the test set, mak-ing a total of three reference titles for each codesnippet.
To collect this data, annotators wereshown only the code snippets and were asked towrite a short summary after looking at a few ex-ample summaries.
They were also asked to ?thinkof a question that they could ask on a program-ming help website, to get the code snippet as a re-sponse.?
This encouraged them to briefly describethe key feature that the code is trying to demon-strate.
We use half of this test set for model tuning(DEV, see Section 5) and the rest for evaluation(EVAL).5 The CODE-NN ModelDescription We present an end-to-end genera-tion system that performs content selection andsurface realization jointly.
Our approach uses anattention-based neural network to model the con-ditional distribution of a NL summary n given acode snippet c. Specifically, we use an LSTMmodel that is guided by attention on the sourcecode snippet to generate a summary one word ata time, as shown in Figure 2.6Formally, we represent a NL summary n =n1, .
.
.
, nlas a sequence of 1-hot vectors6We experimented with other sequence (Sutskever et al,2014) and tree based architectures (Tai et al, 2015) as well.None of these models significantly improved performance,however, this is an important area for future work.2076LSTMLSTMLSTM..n1EEn1nl?1???A+A+?A+ENDh1h2hlt1t2tlF??
?hiti?c=c1,c2,...,ckn2Fccch1;m1h2;m2hl?1;ml?1Figure 2: Generation of a title n = n1, .
.
.
, ENDgiven code snippet c1, ..., ck.
The attention cellcomputes a distributional representation tiof thecode snippet based on the current LSTM hiddenstate hi.
A combination of tiand hiis used togenerate the next word, ni, which feeds back intothe next LSTM cell.
This is repeated until a fixednumber of words or END is generated.
?
blocksdenote softmax operations.n1, .
.
.
,nl?
{0, 1}|N |, where N is the vocabu-lary of the summaries.
Our model computes theprobability of n (scoring function s in Eq.
1) as aproduct of the conditional next-word probabilitiess(c, n) =l?i=1p(ni|n1, .
.
.
, ni?1)with,p(ni|n1, .
.
.
, ni?1) ?W tanh(W1hi+W2ti)where, W ?
R|N |?Hand W1,W2?
RH?H, Hbeing the embedding dimensionality of the sum-maries.
tiis the contribution from the attentionmodel on the source code (see below).
hirepre-sents the hidden state of the LSTM cell at the cur-rent time step and is computed based on the pre-viously generated word, the previous LSTM cellstate mi?1and the previous LSTM hidden statehi?1asmi;hi= f(ni?1E,mi?1,hi?1; ?
)where E ?
R|N |?His a word embedding matrixfor the summaries.
We compute f using the LSTMcell architecture used by Zaremba et al (2014).Attention The generation of each word isguided by a global attention model (Luong et al,2015), which computes a weighted sum of the em-beddings of the code snippet tokens based on thecurrent LSTM state (see right part in Figure 2).Formally, we represent c as a set of 1-hot vectorsc1, .
.
.
, ck?
{0, 1}|C|for each source code to-ken; C is the vocabulary of all tokens in our codesnippets.
Our attention model computes,ti=k?j=1?i,j?
cjFwhere F ?
R|C|?His a token embedding matrixand each ?i,jis proportional to the dot product be-tween the current internal LSTM hidden state hiand the corresponding token embedding cj:?i,j=exp(hiTcjF)?kj=1exp(hiTcjF)Training We perform supervised end-to-endtraining using backpropagation (Werbos, 1990) tolearn the parameters of the embedding matrices Fand E, transformation matrices W, W1and W2,and parameters ?
of the LSTM cell that computesf .
We use multiple epochs of minibatch stochas-tic gradient descent and update all parameters tominimize the negative log likelihood (NLL) ofour training set.
To prevent over-fitting we makeuse of dropout layers (Srivastava et al, 2014) atthe summary embeddings and the output softmaxlayer.
Using pre-trained embeddings (Mikolov etal., (2013)) for the summary embedding matrix oradding additional LSTM layers did not improveperformance for the GEN task.
Since the NLLtraining objective does not directly optimize forour evaluation metric (METEOR), we computeMETEOR (see Section 6.1.2) on a small develop-ment set (DEV) after every epoch and save the in-termediate model that gives the maximum score,as the final model.Decoding Given a trained model and an inputcode snippet c, finding the most optimal title en-tails generating the title n?that maximizes s(c, n)(see Eq.
1).
We approximate n?by performingbeam search on the space of all possible sum-maries using the model output.Implementation Details We add specialSTART and END tokens to our training sequencesand replace all tokens and output words occurring2077with a frequency of less than 3 with an UNKtoken, making |C| = 31, 667 and |N | = 7, 470 forC# and |C| = 747 and |N | = 2, 506 for SQL.
Ourhyper-parameters are set based on performanceon the validation set.
We use a minibatch sizeof 100 and set the dimensionality of the LSTMhidden states, token embeddings, and summaryembeddings (H) to 400.
We initialize all modelparameters uniformly between ?0.35 and 0.35.We start with a learning rate of 0.5 and startdecaying it by a factor of 0.8 after 60 epochs ifaccuracy on the validation set goes down, andterminate training when the learning rate goesbelow 0.001.
We cap the parameter gradients to 5and use a dropout rate of 0.5.We use the Torch framework7to train our mod-els on GPUs.
Training runs for about 80 epochsand takes approximately 7 hours.
We computeMETEOR score at every epoch on the develop-ment set (DEV) to choose the best final model,with the best results obtained between 60 and 70epochs.
For decoding, we set the beam size to 10,and the maximum summary length to 20 words.6 Experimental Setup6.1 GEN Task6.1.1 BaselinesFor the GEN task, we compare CODE-NN witha number of competitive systems, none of whichhad been previously applied to generate text fromsource code, and hence we adapt them slightly forthis task, as explained below.IR is an information retrieval baseline that out-puts the title associated with the code cjin thetraining set that is closest to the input code c interms of token Levenshtein distance.
In this case sfrom Eq.1 becomes,s(c, nj) = ?1?
lev(cj, c), 1 ?
j ?
JMOSES (Koehn et al, 2007) is a popularphrase-based machine translation system.
We per-form generation by treating the tokenized codesnippet as the source language, and the title as thetarget.
We train a 3-gram language model usingKenLM (Heafield, 2011) to use with MOSES, andperform MIRA-based tuning (Cherry and Foster,2012) of hyper-parameters using DEV.SUM-NN is the neural attention-based abstrac-tive summarization model of Rush et al (2015).7http://torch.chIt uses an encoder-decoder architecture with an at-tention mechanism based on a fixed context win-dow of previously generated words.
The decoderis a feed-forward neural language model that gen-erates the next word based on previous words ina context window of size k. In contrast, we de-code using an LSTM network that can model longrange dependencies and our attention weights aretied to the LSTM hidden states.
We set the em-bedding and hidden state dimensions and contextwindow size by tuning on our validation set.
Wefound this model to generate overly short titles like?sql server 2008?
when a length restriction was notimposed on the output text.
Therefore, we fix theoutput length to be the average title length in thetraining set while decoding.6.1.2 Evaluation MetricsWe evaluate the GEN task using automatic met-rics, and also perform a human study.Automatic Evaluation We report METEOR(Banerjee and Lavie, 2005) and sentence levelBLEU-4 (Papineni et al, 2002) scores.
ME-TEOR is recall-oriented and measures how wellour model captures content from the references inour output.
BLEU-4 measures the average n-gramprecision on a set of reference sentences, with apenalty for overly short sentences.
Since the gen-erated summaries are short and there are multi-ple alternate summaries for a given code snippet,higher order n-grams may not overlap.
We remedythis problem by using +1 smoothing (Lin and Och,2004).
We compute these metrics on the tuning setDEV and the held-out evaluation set EVAL.Human Evaluation Since automatic metrics donot always agree with the actual quality of the re-sults (Stent et al, 2005), we perform human eval-uation studies to measure the output of our sys-tem and baselines across two modalities, namelynaturalness and informativeness.
For the former,we asked 5 native English speakers to rate each ti-tle against grammaticality and fluency, on a scalebetween 1 and 5.
For informativeness (i.e., theamount of content carried over from the input codeto the NL summary, ignoring fluency of the text),we asked 5 human evaluators familiar with C# andSQL to evaluate the system output by rating thefactual overlap of the summary with the referencetitles, on a scale between 1 and 5.20786.2 RET task6.2.1 Model and BaselinesCODE-NN As described in Section 2, for agiven NL question n in the RET task, we rank allcode snippets cjin our corpus by computing thescoring function s(cj, n), and return the query c?jthat maximizes it (Eq.
2).RET-IR is an information retrieval baseline thatranks the candidate code snippets using cosinesimilarity between the given NL question n andall summaries njin the retrieval set, based on theirvector representations using TF-IDF weights overunigrams.
The scoring function s in Eq.
2 be-comes:s(cj, n) =tf-idf(nj) ?
tf-idf(n)?tf-idf(nj)?
?tf-idf(n)?, 1 ?
j ?
J6.2.2 Evaluation MetricsWe assess ranking quality by computing the MeanReciprocal Rank (MRR) of c?j.
For every snippetcjin EVAL (and DEV), we use two of the threereferences (title and human annotation), namelynj,1, nj,2.
We then build a retrieval set compris-ing (cj, nj,1) together with 49 random distractorpairs (c?, n?
), c?6= cjfrom the test set.
Using nj,2as the natural language question, we rank all 50items in this retrieval set and use the rank of queryc?jto compute MRR.
We average MRR over all re-turned queries c?jin the test set, and repeat this ex-periment for several different random sets of dis-tractors.6.3 Tasks from Allamanis et al (2015b)Allamanis et al (2015b) take a retrieval approachto answer C# related natural language questions(L to C), similar to our RET task.
In addition, theyalso use retrieval to summarize C# source code (Cto L) and evaluate both tasks using the MRR met-ric.
Although they also use data from Stackover-flow, their dataset preparation and cleaning meth-ods differs significantly from ours.
For example,they filter out posts where the question has fewerthan 2 votes, the answer has fewer than 3 votes, orthe post has fewer than 1000 views.
Additionally,they also filter code snippets that cannot be parsedby Roslyn (.NET compiler) or are longer than 300characters.
Thus, to directly compare with theirmodel, we re-train our generation model on theirdataset and use our model score for retrieval ofboth code and summaries.Model METEOR BLEU-4C#IR 7.9 (6.1) 13.7 (12.6)MOSES 9.1 (9.7) 11.6 (11.5)SUM-NN 10.6 (10.3) 19.3 (18.2)CODE-NN 12.3 (13.4) 20.5 (20.4)SQLIR 6.3 (8.0) 13.5 (13.0)MOSES 8.3 (9.7) 15.4 (15.9)SUM-NN 6.4 (8.7) 13.3 (14.2)CODE-NN 10.9 (14.0) 18.4 (17.0)Table 3: Performance on EVAL for the GEN task.Performance on DEV is indicated in parentheses.Model Naturalness InformativenessC#IR 3.42 2.25MOSES 1.41 2.42SUM-NN 4.61* 1.99CODE-NN 4.48 2.83SQLIR 3.21 2.58MOSES 2.80 2.54SUM-NN 4.44 2.75CODE-NN 4.54 3.12Table 4: Naturalness and Informativeness mea-sures of model outputs.
Stat.
sig.
between CODE-NN and others is computed with a 2-tailed Stu-dent?s t-test; p < 0.05 except for *.7 Results7.1 GEN TaskTable 3 shows automatic evaluation metrics forour model and baselines.
CODE-NN outperformsall the other methods in terms of METEOR andBLEU-4 score.
We attribute this to its ability toperform better content selection, focusing on themore salient parts of the code by using its atten-tion mechanism jointly with its LSTM memorycells.
The neural models have better performanceon C# than SQL.
This is in part because, unlikeSQL, C# code contains informative intermediatevariable names that are directly related to the ob-jective of the code.
On the other hand, SQL ismore challenging in that it only has a handful ofkeywords and functions, and summarization mod-els need to rely on other structural aspects of thecode.Informativeness and naturalness scores for eachmodel from our human evaluation study are pre-sented in Table 4.
In general, CODE-NN performswell across both dimensions.
Its superior perfor-mance in terms of informativeness further sup-ports our claim that it manages to select contentmore effectively.
Although SUM-NN performssimilar to CODE-NN on naturalness, its outputlacks content and has very little variation (see Sec-tion 7.4), which also explains its surprisingly low2079Model MRRC#RET-IR 0.42 ?
0.02 (0.44 ?
0.01)CODE-NN 0.58?
0.01 (0.66?
0.02)SQLRET-IR 0.28 ?0.01(0.4?
0.01)CODE-NN 0.44?
0.01 (0.54?
0.02)Table 5: MRR for the RET task.
Dev set results inparentheses.Model MRRL to CAllamanis 0.182 ?0.009CODE-NN 0.590?
0.044C to LAllamanis 0.434 ?0.003CODE-NN 0.461?
0.046Table 6: MRR values for the Language to Code(L to C) and the Code to Language (C to L) tasksusing the C# dataset of Allamanis et al (2015b)score on informativeness.7.2 RET TaskTable 5 shows the MRR on the RET task forCODE-NN and RET-IR, averaged over 20 runs forC# and SQL.
CODE-NN outperforms the baselineby about 16% for C# and SQL.
RET-IR can onlyoutput code snippets that are annotated with NLas potential matches.
On the other hand, CODE-NN can rank even unannotated code snippets andnominate them as potential candidates.
Hence, itcan leverage vast amounts of such code availablein online repositories like Github.
To speed up re-trieval when using CODE-NN , it could be one ofthe later stages in a multi-stage retrieval systemand candidates may also be ranked in parallel.7.3 Comparison with Allamanis et alWe train CODE-NN on their dataset and evaluateusing the same MRR testing framework (see Ta-ble 6).
Our model performs significantly better forthe Language to Code task (L to C) and slightlybetter for the Code to Language task (C to L).
Theattention mechanism together with the LSTM net-work is able to generate better scores for (lan-guage, code) pairs.7.4 Qualitative AnalysisFigure 3 shows the relative magnitudes of the at-tention weights (?i,j) for example C# and SQLcode snippets while generating their correspond-ing summaries.
Darker regions represent strongerweights.
CODE-NN automatically learns to dohow to get selected cell value in datagridview ?MessageBox.Show(dataGridView1.SelectedCells[1].Value.ToString())how to get the difference between two dates in mysql ?selectcol0fromtab0wherecol0<=now()-interval29day;Figure 3: Heatmap of attention weights ?i,jforexample C# (left) and SQL (right) code snippets.The model learns to align key summary words(like cell) with the corresponding tokens in the in-put (SelectedCells).high-quality content selection by aligning keysummary words with informative tokens in thecode snippet.Table 8 shows examples of the output gener-ated by our model and baselines for code snippetsin DEV.
Most of the models produce meaningfuloutput for simple code snippets (first example) butdegrade on longer, compositional inputs.
For ex-ample, the last SQL query listed in Table 8 in-cludes a subquery, where a complete descriptionshould include both summing and concatenation.CODE-NN describes the summation (but not con-catenation), while others return non-relevant de-scriptions.Finally, we performed manual error analysis on50 randomly selected examples from DEV (Ta-ble 7) for each language.
Redundancy is a ma-jor source of error, i.e., generation of extraneouscontent-bearing phrases, along with missing con-tent, e.g., in the last example of Table 8 there is noreference to the concatenation operations presentin the beginning of the query.
Sometimes the out-put from our model can be out of context, in thesense that it does not match the input code.
Thisoften happens for low frequency tokens (7% ofcases), for which CODE-NN realizes them withgeneric phrases.
This also happens when there arevery long range dependencies or compositionalstructures in the input, such as nested queries (13%of the cases).8 ConclusionIn this paper, we presented CODE-NN , an end-to-end neural attention model using LSTMs to2080Error % CasesCorrect 37%Redundancy 17%Missing Info 26%Out of context 20%Table 7: Error analysis on 50 examples in DEVgenerate summaries of C# and SQL code bylearning from noisy online programming websites.Our model outperforms competitive baselines andachieves state of the art performance on automaticmetrics, namely METEOR and BLEU, as wellas on a human evaluation study.
We also usedCODE-NN to answer programming questions byretrieving the most appropriate code snippets froma corpus, and beat previous baselines for this taskin terms of MRR.
We have published our C# andSQL datasets, the accompanying human annotatedtest sets, and our code for the tasks described inthis paper.In future work, we plan to develop better modelsfor capturing the structure of the input, as well asextend the use of our system to other applicationssuch as automatic documentation of source code.AcknowledgementsWe thank Mike Lewis, Chlo?e Kiddon, Kenton Lee,Eunsol Choi and the anonymous reviewers forcomments on an earlier version.
We also thankBill Howe, Dan Halperin and Mark Yatskar forhelpful discussions and Miltiadis Allamanis forproviding the dataset for the comparison study.This research was supported in part by the NSF(IIS-1252835), an Allen Distinguished Investiga-tor Award, and a gift from Amazon.ReferencesAndi Albrecht.
2015. python-sqlparse.Miltiadis Allamanis, Earl T Barr, Christian Bird, andCharles Sutton.
2015a.
Suggesting accurate methodand class names.
In Proceedings of the 2015 10thJoint Meeting on Foundations of Software Engineer-ing, pages 38?49.Miltiadis Allamanis, Daniel Tarlow, Andrew Gordon,and Yi Wei.
2015b.
Bimodal modelling of sourcecode and natural language.
In Proceedings of The32nd International Conference on Machine Learn-ing, pages 2123?2132.Gabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approachMethod OutputC# codevar x = "FundList [10].
Amount";int xIndex = Convert.ToInt32(Regex.Match(x,@"\d+").Value);Gold Identify the number in given stringIR Convert string number to integerMOSES How to xIndex numbers in C#?SUM-NN How can I get the value of a string?CODE-NN How to convert string to int?C# codeforeach (string pTxt in xml.parent) {TreeNode parent = new TreeNode ();foreach (string cTxt in xml.child) {TreeNode child = new TreeNode ();parent.Nodes.Add(child);}}Gold Adding childs to a treenode dynamically inC#IR How to set the name of a tabPage program-maticallyMOSES How can TreeView nodes from XML par-entText string to a treeview nodeSUM-NN How to get data from xml file in C#CODE-NN How to get al child nodes in TreeView?C# codestring url = baseUrl +"/api/Entry/SendEmail?emailId="+ emailId;WebRequest req =WebRequest.Create(url);req.Method = "GET";req.BeginGetResponse(null , null);Gold Execute a get request on a web server andreceive the response asynchronouslyIR How to download a file from another Share-point DomainMOSES How baseUrl emailId C how to a page inBeginGetResponse toSUM-NN How to get data from a file in CCODE-NN How to call a URL from a web api post ?SQL QuerySELECT * FROM tableORDER BY Rand() LIMIT 10Gold Select random rows from mysql tableIR How to select a random record from a mysqldatabase?MOSES How to select all records in mysql ?SUM-NN How can I select random rows from a tableCODE-NN How to get random rows from a mysqldatabase?SQL QuerySELECT Group concat(Concat ws(',', playerid , r1, r2)SEPARATOR ';')FROM (SELECT playerid ,Sum(rank = 1) r1, Sum(rank < 5) r2FROM result GROUP BY playerid) t;Gold Get sum of group values based on conditionand concatenate them into a stringIR Mysql: counting occurences in a table, re-turn as a single rowMOSES Mysql query to get this result of the resultof one column value in mysqlSUM-NN How do i combine these two queries intoone?CODE-NN How to get the sum of a column in a singlequery?Table 8: Examples of outputs generated by eachmodel for code snippets in DEVto generation.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 502?512.2081Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for mt evaluation with improvedcorrelation with human judgments.
In Proceedingsof the acl workshop on intrinsic and extrinsic evalu-ation measures for machine translation and/or sum-marization, volume 29, pages 65?72.David L Chen, Joohyun Kim, and Raymond J Mooney.2010.
Training a multilingual sportscaster: Usingperceptual context to learn language.
Journal of Ar-tificial Intelligence Research, pages 397?435.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages427?436.Jacob Devlin, Hao Cheng, Hao Fang, Saurabh Gupta,Li Deng, Xiaodong He, Geoffrey Zweig, and Mar-garet Mitchell.
2015.
Language models for imagecaptioning: The quirks and what works.
In Proceed-ings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th Interna-tional Joint Conference on Natural Language Pro-cessing (Volume 2: Short Papers), pages 100?105.Robert Dyer, Hoan Anh Nguyen, Hridesh Rajan, andTien N Nguyen.
2013.
Boa: A language and in-frastructure for analyzing ultra-large-scale softwarerepositories.
In Proceedings of the 2013 Interna-tional Conference on Software Engineering, pages422?431.Alessandra Giordani and Alessandro Moschitti.
2012.Translating questions to SQL queries with genera-tive parsers discriminatively reranked.
In Proceed-ings of COLING 2012: Posters, pages 401?410.Sumit Gulwani and Mark Marron.
2014.
Nlyze: Inter-active programming by natural language for spread-sheet data analysis and manipulation.
In Proceed-ings of the 2014 ACM SIGMOD international con-ference on Management of data, pages 803?814.Sonia Haiduc, Jairo Aponte, and Andrian Marcus.2010.
Supporting program comprehension withsource code summarization.
In Proceedings of the32nd ACM/IEEE International Conference on Soft-ware Engineering-Volume 2, pages 223?226.Kenneth Heafield.
2011.
Kenlm: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages187?197.Sepp Hochreiter and J?urgen Schmidhuber.
1997.Long short-term memory.
Neural computation,9(8):1735?1780.Andrej Karpathy and Fei-Fei Li.
2015.
Deep visual-semantic alignments for generating image descrip-tions.
In IEEE Conference on Computer Vision andPattern Recognition, CVPR 2015, pages 3128?3137.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th annual meeting of the ACL oninteractive poster and demonstration sessions, pages177?180.Ioannis Konstas and Mirella Lapata.
2013.
A globalmodel for concept-to-text generation.
Journal of Ar-tificial Intelligence Research, 48(1):305?346.Georgia Koutrika, Alkis Simitsis, and Yannis E Ioanni-dis.
2010.
Explaining structured queries in naturallanguage.
In Data Engineering (ICDE), 2010 IEEE26th International Conference on, pages 333?344.Nate Kushman and Regina Barzilay.
2013.
Using se-mantic unification to generate regular expressionsfrom natural language.
In Proceedings of the 2013Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 826?836.Tao Lei, Fan Long, Regina Barzilay, and Martin Ri-nard.
2013.
From natural language specificationsto program input parsers.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1294?1303.Fei Li and Hosagrahar V Jagadish.
2014.
Nalir: An in-teractive natural language interface for querying re-lational databases.
In Proceedings of the 2014 ACMSIGMOD international conference on Managementof data, pages 709?712.Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluating automatic evaluation metricsfor machine translation.
In Proceedings of the 20thinternational conference on Computational Linguis-tics, page 501.Wei Lu and Hwee Tou Ng.
2011.
A probabilisticforest-to-string model for language generation fromtyped lambda calculus expressions.
In Proceedingsof the 2011 Conference on Empirical Methods inNatural Language Processing, pages 1611?1622.Thang Luong, Hieu Pham, and Christopher D. Man-ning.
2015.
Effective approaches to attention-basedneural machine translation.
In Proceedings of the2015 Conference on Empirical Methods in NaturalLanguage Processing, pages 1412?1421.Hongyuan Mei, Mohit Bansal, and Matthew R. Walter.2016.
What to talk about and how?
selective gener-ation using lstms with coarse-to-fine alignment.
InProceedings of the 2016 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Proceedings of the Inter-national Conference on Learning Representations.2082Dana Movshovitz-Attias and William W. Cohen.
2013.Natural language models for predicting program-ming comments.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics, pages 35?40.Axel-Cyrille Ngonga Ngomo, Lorenz B?uhmann,Christina Unger, Jens Lehmann, and Daniel Gerber.2013.
Sorry, i don?t speak sparql: Translating sparqlqueries into natural language.
In Proceedings of the22Nd International Conference on World Wide Web,pages 977?988.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th annual meeting on association for com-putational linguistics, pages 311?318.Terence Parr.
2013.
The definitive ANTLR 4 reference.Pragmatic Bookshelf.Sarah Rastkar, Gail C Murphy, and Gabriel Mur-ray.
2010.
Summarizing software artifacts: a casestudy of bug reports.
In Proceedings of the 32ndACM/IEEE International Conference on SoftwareEngineering-Volume 1, pages 505?514.Sarah Rastkar, Gail C Murphy, and Alexander WJBradley.
2011.
Generating natural language sum-maries for crosscutting source code concerns.
InSoftware Maintenance (ICSM), 2011 27th IEEE In-ternational Conference on, pages 103?112.Ehud Reiter and Robert Dale.
2000.
Building naturallanguage generation systems.
Cambridge Univer-sity Press, New York, NY.Alexander M. Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 379?389.Alkis Simitsis and Yannis E. Ioannidis.
2009.
Dbmssshould talk back too.
In CIDR 2009, Fourth BiennialConference on Innovative Data Systems Research,Online Proceedings.Giriprasad Sridhara, Emily Hill, Divya Muppaneni,Lori Pollock, and K Vijay-Shanker.
2010.
To-wards automatically generating summary commentsfor java methods.
In Proceedings of the IEEE/ACMinternational conference on Automated software en-gineering, pages 43?52.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Amanda Stent, Matthew Marge, and Mohit Singhai.2005.
Evaluating evaluation methods for generationin the presence of variation.
In Computational Lin-guistics and Intelligent Text Processing, pages 341?351.Ilya Sutskever, James Martens, and Geoffrey E Hin-ton.
2011.
Generating text with recurrent neuralnetworks.
In Proceedings of the 28th InternationalConference on Machine Learning (ICML-11), pages1017?1024.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in neural information process-ing systems, pages 3104?3112.Kai Sheng Tai, Richard Socher, and Christopher D.Manning.
2015.
Improved semantic representa-tions from tree-structured long short-term memorynetworks.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on Nat-ural Language Processing of the Asian Federationof Natural Language Processing, ACL 2015, pages1556?1566.Subhashini Venugopalan, Huijuan Xu, Jeff Donahue,Marcus Rohrbach, Raymond J. Mooney, and KateSaenko.
2015.
Translating videos to natural lan-guage using deep recurrent neural networks.
In InProceedings of the 2015 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 1494?1504.Tsung-Hsien Wen, Milica Gasic, Nikola Mrk?si?c, Pei-Hao Su, David Vandyke, and Steve Young.
2015.Semantically conditioned lstm-based natural lan-guage generation for spoken dialogue systems.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages1711?1721.Paul J Werbos.
1990.
Backpropagation through time:what it does and how to do it.
Proceedings of theIEEE, 78(10):1550?1560.Yuk Wah Wong and Raymond J Mooney.
2007.
Gen-eration by inverting a semantic parser that uses sta-tistical machine translation.
In In Proceedings of the2007 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 172?179.Wojciech Zaremba and Ilya Sutskever.
2014.
Learningto execute.
CoRR, abs/1410.4615.Xingxing Zhang and Mirella Lapata.
2014.
Chi-nese poetry generation with recurrent neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 670?680.2083
