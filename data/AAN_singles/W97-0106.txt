IIIII,IlGrammar  Acquisition Based on Clustering Analysisand Its Application to Statistical ParsingThanaruk  Theersr~unkong Manabu OkumuraJapan  Advanced Inst i tute  of Science and Technology1-1 As~h~dai Tatsunokuch i  Nomi  Ishikawa 923-12 Japan{ping, o~u}?j aist.
a?.
jpAbstractThis paper proposes a new method for learning a context-sensitive conditional probabilitycontext-free grammar from an unlabeled bracketed corpus based on clustering analysis and de-scribes a natural anguage parsing model which uses a probability-based scoring function of thegrammar to rank parses of a sentence.
By grouping brackets in s corpus into a number of sire;farbracket groups based on their local contextual information, the corpus is automatically abeledwith some nonterm~=a\] l bels, and consequently a grammar with conditional probabilities i ac-quired.
The statistical parsing model provides a framework for finding the most likely parse ofa sentence based on these conditional probabilities.
Experiments using Wall Street Journal datashow that our approach achieves a relatively high accuracy: 88 % recaJ1, 72 % precision and 0.7crossing brackets per sentence for sentences shorter than 10 words, and 71 ~ recall, 51 ~0 precisionand 3.4 crossing brackets for sentences between 10-19 words.
This result supports the assump-tion that local contextual statistics obtained from an unlabeled bracketed corpus are effective forlearnln~ a useful grammar and parsing.1 In t roduct ionMost natural language processing systems utilize grammars for parsing sentences in order torecognize their structure and finally to understand their meaning.
Due to the ,l~mculty andcomplexity of constructing a grammar by hand, there were several approaches developed fora, uton~tically training grammars from a large corpus with some probabilistic models.
Thesemethods can be characterized by properties of the corpus they used, such as whether it includesinformation of brackets, lexical \]abels, nontermlnsl labels and so on.Recently several parsed corpora which include full bracketing, tagging and nonterm~l  labelshave been available for researchers to use for constructing a probaMlistic grammar\[Mag91, Bla92,Mag95, Co196\].
Most researches on these grammars calcuLzte statistics of a grammar from a fully-parsed corpus with nonterm;nal lsbeis and apply them to rank the possible parses of a sentence.While these researches report some promising results, due to the cost of corpus construction, it stillseems worth inferring a probabilistic grammar from corpora with less information, such as oneswithout bracketing and/or nonterm~al labels, and use it for parsing.
Unlike the way to annotatebracketings for corpora by hand, the hand-annotation f nonterm~nal l bels need a process that acorpus builder have to determine types of nonterm~nal l bels and their number.
This process is, insome senses, arbitrary and most of such corpora occupy a set of very coarse-grained nonterminsllabels.
Moreover, compared with corpora including nonterm~sl labels, there are more existingcorpora which include bracketings without nonterm~nal labels such as EDR corpus\[EDR94\] andATIS spoken language corpns\[Hem90\].
The well-known standard method to infer a prohabilisticconte.xt-free grammar from a bracketed/unbracketed corpus without nonterminal labels is so-called31Iinside-outside algorithm which w~ originally proposed by Baker\[Bak79\] and was implemented asapplications for speech and language in ~Lar90\], \[Per92\] and \[Sch93\].
Although encouraging resultswere shown in these works, the derived grammars were restricted to Chomsky normal-form CFGsand there were problems of the small size of acceptable trai=~ng corpora and the relatively highcomputation time required for training the grandams.Towards the problems, this paper proposes a new method which can learn a standard CFGwith less computational cost by adopting techniques of clustering analysis to construct a context-sensitive probab'distic grammar from a bracketed corpus where nontermlnal labels are not an-notated.
Another claim of this paper is that statistics from a large bracketed corpus withoutnonterminal labels combined with clustering techniques can help us construct a probabilisticgrammar which produces an accurate natural anguage statistical parser.
In this method, nonter-minal labels for brackets in a bracketed corpus can be automatically assigned by making use oflocal contextual information which is defined as a set of category pairs of left and right words of aconstituent in the phrase structure of a sentence.
In this research, based on the assumption thatnot all contexts are useful in every case, effectiveness of contexts is also investigated.
By usingonly effective contexts, it is possible for us to improve training speed and memory space withouta sacrifice of accuracy.
Finally, a statistical parsing model bawd on the acquired grammar isprovided and the performance is shown through some experiments using the WSJ corpus.IIIIII2 Grammar Acquisition as Clustering Process IIn the past, Theeramunkong\[The96\] proposed a method of grouping brackets in a bracketed corpus(with lexical tags but no nonterminal labels), according to their local contextual information, asa first step towards the automatic acquisition of a context-free grammar.
The basic idea is toapply clustering analysis to find out a number of groups of s;m;\]ar brackets in the corpus and thento ~sign each group with a same nonterminal label.
Clustering analysis is a generic name of avariety of mathematical methods that can be used to find out which objects in a set are s;mi\]sr.Its applications on natural anguage processing are varied such as in areas of word classification,text categorization and so on \[Per93\]\[Iwa95\].
However, there is still few researches which applyclustering analysis for grammar inference and parsing~Vior95\].
This section gives an explanationof grammar acquisition based on clustering analysis.
In the first place, let us consider the followingexample of the parse strnctures of two sentences in the corpus in figure 1.In the parse structures, leaf nodes are given tags while there is no label for intermedLzte nodes.Note that each node corresponds to a bracket in the corpus.
With this corpus, the grammarlearning task corresponds to a process to determ~=e the label for each intermediate node.
In otherwords, this task is concerned with the way to cluster the brackets into some certain groups basedon their similarity and give each group a label.
For instance, in figure 1, it is reasonable to classifythe brackets (c2),(c4) and (c5) into a same group and give them a same label (e.g., NP(nounphrase)).
As the result, we obtain three grammar rules: NP ~ (DT) (NN) ,  NP  ~ (PR .P$) (NN)and NP  ~ (DT)(cl) .
To do this, the grammar acquisition algorithm operates in five steps asfollows.IIIIII1.
Assign a unique label to each node of which lower nodes are assigned labels.
At the initialstep, such node is one whose lower nodes are lexical categories.
For example, in figure 1, thereare three unique labels derived: cl ~ ( J J ) (NN) ,  c2 ~ (DT) (NN)  and ~ ~ (PRP.~)(NN) .This process is performed throughout all parse trees in the corpus.2.
Calculate the similarity of every pair of the derived labels.3.
Merge the most ~m~lar pair to a single new label(i.e., a label group) and recalculate theslmilarity of this new label with other labels.4.
Repeat (3) until a termination condition is detected.
Finally, a certain set of label groups isderived.IIII32 IISentence (1) : A big man slipped on the ice.Parse Tree (1) (((DT,"a")C(J3,"big")(NN,"man')))((VB,"slipped")((IN,"on")I : ((DT,'the') (NN,'ice')))))Sentence (2) : The boy dropped his wallet somewhere.Parse Tzee (2) : (((DT,"the")(NN,"tx~yn))(((VB,'dropp ed~)I ((PI~P$," his") (NN,"wallet"))) (RB,"somewhere ~)))I!I ,: = : ", : : : : ".
:DT JJ NN VB IN DT NN DT NN VB PRP$ NN RIBA big man slipped on the ice The boy dropped his wallet somewhereiIIIIIFigure 1: The graphical representation f the parse structures of a big man slippedon the ice and the boy dropped his wallet somewhere5.
Replace labels in each label group with a new label in the corpus.
For example, i f (DT)(NN)and (PRP$)(NN) are in the same label group, we replace them with a new label (such asNP) in the whole corpus.6.
Repeat (1)-(5) until all nodes in the corpus are assigned labels.To compute the similarity of labels, the concept of local contextual information is applied.In this work, the local contextual information is defined as categories of the words immediatelybefore and after a label.
This information is shown to be powerful for acquiring phrase structuresin a sentence in \[Bri92\].
In our prp|iminary experiments, we also found out that the informationare potential for characterizing constituents in a sentence.IIIII2.1 D is t r ibut iona l  S im; la r i tyWhile there are a number of measures which can be used for representing the sir-ilarity of labels inthe step 2, measures which make use of relative ntropy (Kullback-Leibler distance) are of practicalinterest and scientific.
One of these measures i  divergence which has a symmetrical property.
Itsapplication on natural language processing was firstly proposed by Harris\[Hat51\] and was shownsuccessfully for detecting phrase structures in \[Bri92\]\[Per93\].
Basically, divergence, as well asrelative ntropy, is not exactly s'nnilarity measure instead it indicates distributional dissimilarity.That means the large value it gets, the less similarity it means.
The detail of divergence isiUustrated below.Let P?I and Pc= be two probability distributions of labels cI and ~ over contexts, CT Therelative ntropy between P?~ and P?= is:D(P,,.,IIP,==) = ~ pCel,--.:,.)
?
log pCelc'),~c~ pCelc=)I 33Relative entropy D(Pc~ \[\[Pc2) is a measure of the amount of extra information beyond P?~ neededto describe Pc2- The divergence between Poe and P?2 is defined as D(Pc~ \]lPc~)+D(Pc~\]lPcz), and isa measure of how di~icult it is to distinguish between the two distributions.
The context is definedas a pair of words immediately before and after a label(bracket).
Any two labels are considered tobe identical when they are distributionally siml\]~.r, i.e., the divergence is low.
From the practicalpoint view, this measure addresses a problem of sparseness in limited data.
Particularly, whenp(eJcz) is zero, we cannot calculate the divergence of two probability distributions because thedenomi-ator becomes zero.
To cope with this problem, the original probability can be modifiedby a popular technique into the following formula.
;~(~,e) (I IpCel~) = /v'(~) + - ;910~,1where, N(~) and N(c~, e) are the occurrence frequency of ~ and (~, e), respectively.
IOrl is thenumber of possible contexts and A is an interpolation coefficient.
As defin~-g contexts by the leftand right lexical categories, \[CT\[ is the square of the number of existing lexical categories.
In theformula, the first term means the original estimated probability and the second term expresses auniform distribution, where the probability of all events is estimated to a fixed --~form number.is applied as a balancing weight between the observed istribution and the -=iform distribution.In our experimental results, A is assigned with ~ value of 0.6 which seems to make a good estimate.2 .2  Terminat ion  Cond i t ionDuring iteratively merging the most slm~l~r labels, all labels will finally be gathered to a singlegroup.
Due to this, a criterion is needed for determining whether this merging process hould becontinued or terminated.
In this section, we describe ~ criterion named differential entropy whichis a measure of entropy (perplexity) fluctuation before and after merging a pah- of labels.
Letcl and c2 be the most similar pair of labels.
Also let cs be the result label p(e\[cl), p(e\[c2) andp(e\]c3) are probability distributions over contexts e of cl, c2 and ~,  respectively, p(cl), p(c2) andp(c3) are estimated probabilities of cl, c2 and ca, respectively.
The differential entropy (DE) isdefined as follows.DE" = Consequence E~troFg - Previous E~t~opy= - pCc~) ?
~p(elc~)logp(elc~)?+ pCcl) ?
~pCelcl)logpCelcl) + pCc2) ?
~pCelc~)logpCe\[~)ewhere ~ep(elc/) log P(elc/) is the total entropy over various contexts of label c~.
The larger DEis, the larger the information fluctuation before and after merging becomes.
In general, a smallfluctuation is preferred to s larger one because when DE is large, the current merging processintroduces a large amount of information fluctuation and its reliability becomes low.3 Loca l  Context  E f fec t ivenessAs the s~ml\]~rity of any two labels is estimated based on local contextual information which isdefined by a set of category pairs of left aad right words, there is an interesting question of whichcontexts are useful for calculation of s~ml\]~rity.
In the past, effectiveness of contexts is indicatedin some previous researches \[Bar95\].
One of suitable measures for representing effectiveness of acontext is dispersion of the context on labels.
This measure xpresses that the number of usefulcontexts hould be diverse for different labels.
From this, the effectiveness (E) of a context (c)can be defined using variance as follow:~(c) -- ~ C~(a,c)-~(c)) ~.~A I~1IIIIIIIIIII!IIIIII34 IiIIII=IAIwhere A is a set of all labels and a is one of its individual members.
N(a, c) is the number of timesa label a and a context c are cooccurred.
N(c) is an averaged value of NCa, c) on a label a. Inorder to take large advantage of context in clustering, it is preferable to choose a context c witha high value of E(c) because this context rends to have a high discrlm~nation forcharacterisinglabels.
1~aD~n~ the contexts by the effectiveness value E, some rank higher contexts are selectedfor elustering the labels instead of all contexts.
This enables us to decrease computation timeand space without sacrificing the accuracy of the clustering results and sometimes also helps usto remove some noises due to useless contexts.
Some experiments were done to support hisassumption and their results are shown in the next section.I 4 Statistical Parsing Modeli ,iIThis section describes a statistical parsing model which takes a sentence as input and produce aphrase-structure t e as output: In this problem, there are two components aken into account: astatistical model and parsing process.
The model assigns a probability to every candidate parsetree for a sentence.
Formally, given a sentence S and a tree T, the model estimates the conditionalprobability P(T\[S).
The most likely parse under the model is argma,zrP(T\[S ) and the parsingprocess is a method to find this parse.
~Vhile a model of a simple probabilistic CFG applies theprobability of a parse which is defined as the multiplication of the probability of all applied rules,however, for the purposes of our model where left and right contexts of a constituent are takeninto account, he model estimates P(T\[S) by ass-m~-g that each rule are dependent ot only onthe occurrences of the rule but also on its left and right context as follow.l P(TIS) P(r,,c,)IilIIIwhere r~ is an application rule in the tree and ~ is the left and right contexts at the place the ruleis applied.
SimS|at o most probabilistic models and our clustering process, there is a problemof low-frequency events in this model.
Although some statistical NL applications apply backing-off estimation techniques to handle low-frequency events, our model uses a simple interpolationestimation by adding almlform probability to every event.
Moreover, we make use of the geometricmean of the probability instead of the original probability in order to ~|imlnate he effect of thenumber of rule applications as done in \[Mag91\].
The modified model is:/'(TIS)=C (a*P( r ' , c4 )+Cl -a )*N~N)) r~(~,,cDerHere, a is a balancing weight between the observed distribution and the uniform distribution andit is assigned with 0.95 in our experiments.
The applied parsing algorithm is a simple bottom-upchart parser whose scoring function is based on this model.
The grammar used is the one trainedby the algorithm described in section 2.
A dynamic programming algorithm is used: if there aretwo proposed constituents which span the same set of words and have the same Isbel, then thelower probability constituent can be safely discarded.I 5 Experimental EvaluationTo give some support o our su~ested grammar acquisition metllod and statistical parsing model,three following evaluation experiments are made.
The experiments u e texts from the Wall StreetJournal (WSJ) Corpus and its bracketed version provided by the Penn 'rreebank.
Out of nearly48,000 sentences(i,222,065 words), we extracted 46,000 sentences(I,172,710 words) as possiblematerial source for traiuing a grammar and 2000 sentences(49,355 words) as source for testing.35The first experiment involves an evaluation of performance of our proposed grammar learningmethod shown in the section 2.
In this prp\]imi~ary experiment, only rules which have lexicalcategories as their right hand side are considered and the acquired nontermlnal labels are com-pared with those assigned in the WSJ corpus.
The second experiment stands for investigatingeffectiveness ofcontexts described in section 3.
The purpose is to find out useful contexts and usethem instead of all contexts based on the assumption that not all contexts are useful for clusteringbrackets in grammar acquisition.
Reducing the number of contexts will help us to improve thecomputation time and space.
The last experiment is carried out for evaluating the whole gram-mar which is learned based on local contextual information and indicating the performance ofourstatistical parsing model using the acquired grammar.
The measures used for this evaJuation arebracketing recall, precision and crossing.5.1 Evaluation of Clustering in Grammar AcquisitionThis subsection shows some results of our preliminary experiments to confirm effectiveness ofthe proposed grammar acquisition techniques.
The grammar is learned from the WSJ bracketedcorpus where all nonterm~nals are omitted.
In this experiment, we focus on only the rules withI ~  c~egories as th~ ~ght h~d ~de.
For ~tance ,  ci -~ (~J ) (~N) ,  c2 -~ (DT)(NN) andCs --~ (P.RP$)(N.N') in figure 1.
Due to the reason of computation time and space, we use therule tokens which appear more than 500 times in the corpus.
The number of initial rules is 51.From these rules, the most similar pair is calculated and merged to a new label The mergingprocess is cached out in an iterative way.
In each iterative step of the merging process, differentialentropies are calculated.
During the merging process, there are some sharp pealr~ indicating therapid fluctuation of entropy.
These sharp peaks can be used as a step to terrnln~te the mergingprocess.
In the experhnents, a peak with .DE ~> 0.12 is applied.
As the result, the process ishalted up at the 45th step and 6 groups are obtained.This result is evaluated by comparing the system's result with nontermlnal symbols given inthe WSJ corpus.
The evaluation method utilizes a contingency table model which is introducedin\[Swe69\] and widely used in Information Retrieval and Psychology\[Aga95\]\[lwa95\].
The followingmeasures are considered.?
Posit ive Recal l  (PLY) : ~?
Posit ive Precis ion (PP)  : ~----~?
NeKative Recal l  ( l~t )  :?
Negat ive Precis ion ( I~P)  :* F -measure  (FM) ?
(~2+I)?PP?PR /32 ?PP+ P.Rwhere a is the number of the label pairs which the WS3 corpus assigns in the same group and sodoes the system, 5 is the number of the pairs which the ~WSJ corpus does not assign in the samegroup but the system does, c is the number of the pairs which the WSJ assigned but the systemdoes not, and d is the number of the pairs which both the WSJ and the system does not assignin the same group.
The F-measure is used as a combined measure of recall and precision, where/3 is the weight of recall relative to precision.
Here, we use/5 ---- 1.0, equal weight.The result shows 0.93 ~o PR, 0.93 ~o PP, 0.92 ~0 ~ 0.92 % I~P and 0.93 % FM, whichare all relativeiy good va/ues.
Especially, PP shows that almost all same labels in the WSJ areassigned in same groups.
In order to investigate whether the application of differentia/entropy tocut off the merging process is appropriate, we plot values of these measures at all merging stepsas shown in figure 2.
From the graphs, we found out that the best solution is located at around44th-45th merging steps.
This is consistent with the grouping result of our approach.
Moreover,the precision equals 100 % from 1st-38nd steps, indicating that the merging process is suitable.IIIIIiIIIIIIIIIIII36 IIIi 0.8II o4I .
~ 0.20III I " I I I I\ [ -  -i -.?
-!
..i ......... !
......Reca l l  i i ~ r ' - ' k .
- .I I  : A~ n -_ -a  : : : : : ~ .
.
.
.
.
.
i:! "
.
j i '  I,' i i v - r ' lu?
.~ l~ .
.
.
.
-: !
i i : .
.~ J /  ~ i,~J l IV-Precisioh ..... ~ ~ ~ zi',.~_l, i i :"~ iiI.~ .
.
.
.
.
~ : -nF~s~r~ .
.
.
.
~.
.
.
.
.
.
.
.
.
.~ .
.
.
.
.
.
.
.
.
~ : .
l .
.
.
.
;  .
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
~?.
.
.~.
i~.l i  - : .
.
.
.
"T" - - - "  - :  : : : ?
* .
\[ !
: : : , %:  i .
.
.
.
.
: /  .
.
.
.
= lv: I 1 " / :  : , : ;.
.
.
.
.
?
.
.
.
/~V.
.
.
.h**~ ? '
?
.
.
?
.
o ?~ .
: .i i i i i /  i i : i i| ~ ~ ~ ~ ~"  I~  ~ ~ ~ " ~t .
.
.
.
.
: : : : .
_ .~?
: : : : ; ~: : : .
; , ,  : : : : : ?
~: ;.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
.
.
.
-.
.
.
.
.
.
.
.
F .
- -  .
.
.
.
.
.
.
.
T .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
.
.
'~'*==": : : " : ?
: ~ I  ~l : : ~" : _ _ J  : ~|\[ .
.
.
.
.
.
.
.
, ,0 5 10 15 20 25 30 35 40 45 50Merge StepFigure 2: The traz~sltion of PR, PP, NP~, NP and FM during the merging processi 5.2 Checking Context EffectivenessAs another experiment, we e,y~mine  elfectiveness of contexts in the clustering process in orderto reduce the computation time and space.
Variance is used for expressing effectiveness of acontext.
The assumption is that a context with has the highest variance is the most effective.The experiment is done by selecting the top jV of contexts and use it instead of all contexts inthe clustering process.Besides cases of .N = 10, 50, 200, 400 and ali(2401), a case that 200 contexts are randomlychosen from all contexts, is taken into account in order to e~arnlne the assumption that vaxianceis efficient.
In this case, 3 trials are made and the average value is employed.
Due to the limitof paper space, we show only F-measure in figure 3.
The graphs tell us that the case of top 200seems uperior to the case of 200 random contexts in a11 merging step.
This means that varianceseems to be a good measure for selecting a set of effective contexts in the clustering process.l~rthermore, we can observe that a high accuracy can be achieved even if not a11 contexts aretaXen into account.
From this result, the best F-measures are a11 0.93 and the number of groupsaxe 2, 5, 5 and 6 for each case, i.e., 10, 50, 200 and 400.
Except of the case of 10, a11 cases how agood restdt compared with all contexts (0.93, 6 groups).
This resnlt tells us that it is reasonableto select contexts with large values of ~raxiance to ones with small v'4riance and a relatively \]axgenumber of contexts are enough for the clustering process.
By pr~|im;nary experiments, we foundout that the following criterion is sufficient for determining the number of contexts.
Contexts axeselected in the order of their varLznce and a context wi\]1 be accepted when its variance is morethan 10 % of the average v~iance of the previously selected contexts.5 .3  Per fo rmance  o f  S ta t i s t i ca l  Pars ing  Mode lUtilizing top N contexts, we learn the whole grammar based on the algorithm given in section2.
Brackets(rules) which are occurred more than 40 times in the corpus are considered and thenumber of contexts used is determ;ned by the criterion described in the previous ubsection.
Asthe result of the grammar acquisition process, 1396 rtfles are acquized.
These rules axe attachedwith the conditionalprobability based on contexts (the left and fight categories ofthe rules).
The37?0 =uL0.80 .60 .40 .200i ~ ~( ' J~  i '~ :N = 10 ; .
.
/ ~ : ~ ' .
~  ~!........
!
......... i...... N .
.
'~50"~=~.~;  ........
h"~=~.?:i~'""~~..~.
"~i i N ~ 400 - - - :  i ~_~.
i ;~ .
/ i  \] ; i........ ~ ......... ~ ....... N.  ~ a l  t~ .
.
.
.
~ ........ ~ ~ - - .
.
:.
.
~ .~ ....... " - .~.~ : .- .
i .........
.;? "
" : ~ .
.
I  ?
I ?N = 200 random .
.
.
.
~ i~.
!
; . '
\ [~"  !
i. .
.
.
.
.
.
.
i .
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
:- .
.
.
.
.
.
.
4 - - - - -~- !
- . '
i - : - - - - ,~ .
.
.
.
.
.
.
/ - .
: .
-  .
.
.
.
.
i .
.
.
.
.
.
......... : ........ : ......... ~ ......
~..~.~-.:/...
~.
.......=.....-: ...... !
........5 10  15  20  25  30  35  40  45  50Merge StepFigure 3: FMs when chsnging the number of context~(N)chart parser tries to find the best parse of the sentence.
46,000 sentences are used for traininga grammar and 2000 sentences are for a test set.
To evaluate the performance, the PA.I~.SEVALmeasures as defined in \[Bla91\] are used:Precision =number of correct brackets in proposed parsesnumber of brackets in proposed parsesRecall  =number of correct brackets in proposed parsesnumber of brackets in treebank parsesThe parser generates the most likely parse based on context-seusitive condition probability of thegrammar.
Among 2000 test sentences, only 1874 sentences can be parsed owing to two followingreasons: (1) our algorithm considers rules which occur more than 40 times in the corpus, (2) testsentences have different characteristics from training sentences.
Table 1 displays the detail resultsof our statistical pexser evaluated against he WSJ corpus.93 ~0 of sentences can be parsed with 71 ~ recall, 52 ~0 precision aud 4.5 crossings per sentence.For short sentences (3-9 words), the parser achieves up to 88 % recall and 71% precision withonly 0.71 crossings.
For moderately ong sentences (10-19 and 20-30 words), it works with 60-71% recall and 41-51% precision.
~om this result, the proposed parsing model is shown to succeedwith high bracketing recalls to some degree.
Although our parser cannot achieve good precision,it is not so a serious problem because our parser tries to give more detail bracketing for a sentencethem that given in the WSJ corpus.
In the next section, the comparison with other reseaxches willbe discussed.6 Related Works and DiscussionIn this section, our approach is compared with some previous interesting methods.
These methodscan be classified into non-grammar-based and grammar-based approaches.
For non-grammaz-based approaches, the most successful probabifistic parser named SPATTER is proposed byIIIIiIiIIlIIIIIIII38 ILSent n hComparisonsAvg.
Sent.
Len.TBank ParsesSystem's ParsesCroasings/Sent.Sent.
cross.= 0Sent.
cross.< 1Sent.
cross._< 2 iRecallPrecision3--9 13-15 \[10,19 20-.3013-40393 988 875 484 18627.0 10.3 = 14.0 24.0 16.334.81 6.90 9.37 15.93 10.8510.86 16.58 23.14 40.73 27.180.72 1.89 3.36 7.92 4.5256.7% 33.1% 13.6% 2.5% 19.0%79.4% 50.4% 25.4% 6.0% 30.3%93.4% 67.0% 41.5% 9.5% 41.8%88.2% 79.3% 71.2% 59.7% 70.8%71.9% 60.6% 51.3% 41.2% 52.1%i ,Table 1: Parsing accuracy using the WSJ  corpusMagerman\[Mag95\].
The parser is constructed by using decision-tree l arning techniques andcan succeed up to 86-90 % of bracketing accuracy(both recall and precision) when t r~ ing  withthe WSJ corpus, a fully-parsed corpus with nontermlnvJ labels.
Later Collins\[Col96\] introduceda statistical parser which is based on probabilities of bigzam dependencies between head-wordsin a parse tree.
At least the same accuracy as SPATTER was acquired for this parser.
Thesetwo methods ufflized a corpus which includes both lexical categories and nontermi~al categories.However, it seems a hard task to assign nontermlnsl labels for a corpus and the way to assign anonterminal label to each constituent in the parsed sentence is arduous and arbitrary.
It followsthat it is worth trying to infer a grammar from corpora without nontermlnal labels.One of the most promising results of grammar inference based on grammar-based approaches ithe inside-outside algorithm proposed by Laxi\[Lazg0\] to construct the gr~.mmax from unbracketedcorpus.
This algorithm is an extension of forward-backward algorithm which infers the parametersof a stochastic ontext-free grammar.
In this research the acquired grammar is elr~.luated basedon its entropy or perplexity where the accuracy of parsing is not taken into account.
As anotherresearch, Pereira and Schabes\[Per921\[Sch93 \] proposed a modified method to infer a stochasticgran~ar from a partially parsed corpus and evaluated the results with a bracketed corpus.
Thisapproach gained up to around 90 % bracketing recall for short sentences(0-15 words) but it sut~eredwith a large amount ambiguity for long ones(20-30) where 70 % recall is gained.
The acquiredgr~mrn~T is normally in Chomsky .normal-form which is a special case of gr~mTnar although heclaimed that all of CFGs can be in this form.
This type of the gr=tmrnar makes all output parsesof this method be in the form of binary-branrMng trees and then the bracketing precision cannotbe taken into account because correct parses in the corpus need not be in this form.
On the otherhand, our proposed approach can learn a standard CFG with 88 % recall for short sentences and60 % recall for long ones.
This result shows that our method gets the same level of accuracyas the inside-outside algorithm does.
However, our approach can learn a gr~tmm~.~, which is notrestricted to Chomsky normal-form and performs with leas computational cost compared with theapproaches applying the inside-outside algorithm.7 ConclusionIn this paper, we proposed a method of applying clustering aaalysis to learn a context-sensitiveprobab'flistic grammar from an unlabeled bracketed corpus.
Supported by some experiments,local contextual information which is left and right categories of a constituent was shown tobe useful for acquiring a context-sensitive conditional probability context-free grammar from acorpus.
A probabilistic parsing model using the acquired grammar was described and its potentialwas eT~m{ned.
Through experiments, our parser can achieve high paxsing accuracy to some extentcompared with other previous approaches with less computational cost.
As our further work, there39are still many possibilities for improvement which are encouraging.
For instance, it is possibleto use lexical information and head information in clustering and constructing a probabilisticg~l~Yn Tn ~LY.References\[Aga95\]\[Bak79\]~ar95\]\[Bla91\]\[B1a92\]p3~2\]\[Co196\]\[EDR94\]p~=51\]p~em90\]~w~51~0\ ]~ag911~o~5\]~er92\]\[Per931\[Sch931\[Sw~9\]\[The96\]Agarwal, 1~.
: Evaluation of Semantic Clusters, in Proceeding off $$rd Annual Meeting offthe ACL, pp.
284-286, 1995.Baker, J.: Traina, ble grarom~rs for speech recognition, in Speech Coraranrdcation Papersfor the 97th Meeting of the Acoustical Society of America (D.H. Klatt and J.J.
Wolff,eda.
), pp.
547-550, 1979.Bartell, B., G. Cottreil, and R. Belew: Representing Documents Using an Explicit Modelof Their Sjm;|alJties, 3otlrr~al off the Amebean Society for Infformation Science, Vol.
46,No.
4, pp.
254-271, 1995.Black, E. and et al: A procedure for quantitatively comparing the syntactic overage ofenglish grammars, in Proc.
off the 1991 DARPA Speech and Natural .Language Workshop,pp.
306--311, 1991.Black, E., F. Jelinek, J. La~erty, D. Magerman, R. Mercer, and S. Roukos: TowardsHistory-Based Grammars: Using Richer Models for Proba.bilistic Parsing, in Proc.
offthe 1992 DARPA ,Ypeech and .Natural Language Workshop, pp.
134--189, 1992.Brill, E.: Automatically Acquiring Phrase Structure using Distributional Analysis, inProc.
off Bpeech and Natural Language Workshop, pp.
155--159, 1992.Collins, M. J.: A New Statistical Parser Based on Bigram Lexical Dependencies, inProc.
off the 3~th Annual Meeting off the AUL, pp.
184-191, 1996.EDR: Japan Electronic Dictionary Research Institute: EDR Electric Dictionary User'sMannal (in Japanese), 2.1 edition, 1994.Harris, Z.: Structural LinguL~tics, Chicago: University of Chicago Press, 1951.Hemphill, C., G. J.J., and G. Doddington: The ATIS spoken lauguage systems pilotcorpus, in DARPA Speech and .Natural Language Workshop, 1990.Iwayamv., M. and T. Tokunaga: Hierarchical Bayesian Clustering for Automatic TextClassification, in IJCAI, pp.
1322-1327, 1995.Lari, K. and S. Young: =The Estimation of Stochastic Context-free Grammars Usingthe Inside-Outside Algorithm", Computer speech and languagea, Vol.
4, pp.
35--56, 1990.Magerman, D. M. and M. P. Marcus: Pearl: A Probabilistic Chart Parser, in Proceedingsoff the Ecropean ACL Conference, 1991.Magerman, D. M.: Statistical Decision-Tree Models for Parsing, in Proceeding of $3rdAnnual Meeting of the ACL, pp.
276-283, 1995.Mori, S. and M. Nagso: Parsing without Grammar, in Proc.
off the ~th InternationalWorkshop on Parsing Technologies, pp.
174--185, 1995.Pereira, F. and Y. Schabes: Inside-Outside r estimation from partially bracketed cor-pora, in Proceedings of 30th Annual Meeting of the ACL, pp.
128--135, 1992.Pereira, F., N. Tishby, and L. Lee: Distributional Clustering of English Words, inProceedings of 315~ Annual Meeting off the ACL, pp.
183--190, 1993.Schabes, Y., M. Both, and 1t.
Osborne: Parsing the Wall Street Journal with the InsiderOutside Algorithm, in Proc.
off 6th European Cfiapter off ACL, pp.
341-347, 1993.Swets, J.: Effectiveness of Information Retrieval Methods, American Documentation,Vol.
20, pp.
72--89, 1969.Theeramunkong, T. and M. Okumura~ Towards Automatic Glr~mmar Acquisition froma Bracketed Corpus, in Proc.
of the 4th International Workshop on Very Large Corpora,pp.
168-177, 1996.40IIII!IIIIIIIIIIIIII
