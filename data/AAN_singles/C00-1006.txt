The Effects of Word  Order  and Segmentat ion  on Trans la t ionRetr ieva l  Per fo rmanceT imothy  Ba ldwin  and Hozumi  Tanaka27okyo \]nstil;ul;e ()I "~ I e thno logy2-1.2-1 Ooka,yama, Meguro -ku ,  qlbkyo 1.52-8552 , JAPAN{t im,  tanaka}@cl ,  ca .
t i tech ,  ac .
jpAbst ractThis research looks at tim cIt'ccts of word ordermL(t scgm(mtation on l;ra.nslation retri(~val t)(~rfor-III~\[.11C( ~.
lot" ~.111 eXl)erim(:nta.1 Jal>an(>s(>English (;rm>-lation memory system.
We iml)lem('.nt a num-ber of both bag-of-words and word order-s(msitiv(~s;imilarity metrics, and test each over charact(u-l/ased m~d word-based indexing.
Tim translationr(%rieval )elt'ormmm(~ of ca(:h sysi;em (:ontigurationis (~valuat(~(1 (mq)iri(:ally through tlm n()ti(>n of wordedit distan(:(~ \])(}(;W(}(}IL translation (:ml(li(lal;(~ ()ul;lml;smid tim mo(hd translation.
Ore resull;s in(li('.at(~(;hat(; (:hm'act(!r-l)as(!d indexing is (:(msislxmtly sup(>riot (;() wor(l-bas(:d in(l(:xing, sugg(:sl;ing (;hal; s(:glncn-l;al;ion is ;m mm('.cessary luxury in th(', giv(m domain.\?or(1 ord(:r-s(:nsi(;iv(: al)i)roach('s at(: do.monsl;rat(:dto generally OUtlt(~rform bag-of-words methods, withsom'(:c bmguagc segment-lev(d e it distan(:o, provingth(: most; (:fl'(:(;l;iv(~ similarity m(,,l;ric.1 I n t roduct ionTransla(.ioll m(unorio,q (TM's) m'c a w(~ll-(!slal)lishedI,(:(:\]uloliigy wil,llilL (,h(!
hlunalL and n|a(:hilm ld'an,qla(;ion t'rat('.rnii;i(:s, duo.
to the high (raiLslat;ion lit(!
-(;isioIL (;lmy a flbrd.
Esstml;ially, TM's me a listof t rans la t ion  records (source la.nguage stringspaired with a unique target language translation),which the TM system accesses in suggcsl;ing a listof target languag(', t rans lat ion  cand idates  whichmay l)(,.
hell)tiff to (;h(: translator in translating agiven source language inputJNaturally, TM systems h~w('~ no way of accessingthe (;a.rgcl; la.nguagc quiv;fl(m(; of tit(: soltr(:(: lan-guage input, and hence (;lm list of tautc.l, lanquagctnmslation cmMi(lat(:s is det(:rntined base(l on sourcelanguage similarity between tim (:urr(mt input andtrmlslation examples within the TM, with transla-tion equivalent(s) of maximally similar source lan-guage string(s) given as the translation candidate(s).This is based on the assumption that structural att(tsemantic similarities 1)etwe(m targ(:t language trans-lations will be reflected in the original source lan-guage cquivalenl;s.One reason tbr the popularity of TM's is the lowoperational burden they t)(LS(~ to tim user, in thattranslation pairs are largely acquired automatically1See \])lanas (1998) for a thorough review of commercialTM systems.from observai;ion of l;lm incremental (;rmlsl&Lion pro-(:(:ss, and translation cml(lidates cml \]m l)roduced on(hunand almost insf;ani;ancously.
To support his low()vt}rlma(1, TM systems must allow first access intothe l)Oixmtially la.l'g(,.-s(:ah} TM, lint at the stone timeI)e al)lc to 1)rc(lict .ranslation similarity with high ac-curacy.
Ilere, th(n'(~ is clearly a trade-off between ac-( :ess / re t r i cva l  speed anti predict ive accuracy  of(,he retriewfl m(,.ctmnism.
2haditiomflly, resemch onTM r(~trieval nmthods has focused on Slme(l, with lit-(;1(~ (:ross-(~vahml;ion f (;he accuracy of differ(mr mclh-otis.
\Vc t>r(~t'(u to focus on ac(:tlracy, and t)r(~s(~ll(;(~mlfiLical data (~vid(!ncing tim relative l)r(~di(:l;ivc l>O-((u~iial of difl'<u'(mt similarity metrics over differentl)aram(:t(,.risations.In tiffs l)almr, we focus on comparison of differ(mrretrieval algorithms for non-segmenting la.nguag(~s,1)ascd around a TI~,I sysi;cm from .\]almnese to En-glish.
Non-s(!gm(ml;ing languages are those which (Ionot involve d(:limii;ers (e.g.
spaces) tmtwe(m words,and in(:lude .lapmms(:, (Jhines(: and Thai.
W(: aretmrticularly int(~'r(~st(:(l in the part tim orlhog(mal 1m-rmnet(~rs of s(.,gmentnl;ion and word order play in thest)(!cd/a(:(:uracy trad(!-oti'.
That is, 1)3" doing awaywith segnl(:ntai;ion in relying soMy on ch\[/t'}lc\[(}l-h~v(~l comparis(m (character-1)ased indexing),  dow(: signiti(:mitly degrade match tmrt'ormance, ascom-pared to word-level comparison (word-based in-dexing)?
Similm'ly, by ignoring word order andtreating each sour(:e language string as a "bag ofwords", do \re genuinely lose out over word order-s(msitive apl)roacho.s?
The.
In;fin objective of thisresearch is thus (;o (teJ;ermine whether the COmlmi,a-tioiml overlmad associated with more stringent ap-proaches (i.e.
word-based indexing and word order-sensitive alH)roaches) is commensura.te with the per-formancc gains they ott'er.To l)rccmpt what tollows, the major contrilmtionsof this research are: (a) empirical evaluation of dif-thrcnt comparison methods over actual Japanese-English TM data, focusing on four orthogonal re-triewfl paradigms; (b) the finding that, over tile tar-get; data, character-based indexing is consistentlysuperior to word-based indexing in identii\[ying thetranslation candidate most sinfilar to tile optimaltranslation for a given inlmt; and (c) empirical ver-ification of tim supremacy of word order-sensitiveexhaustiv(: string comparison methods over booleaninal;ch methods.In the %llowing sections we discuss the effects35of segmentation and word order (~ 2) and preseuta number of both bag-el;words and word order-sensitive sinfilarity metrics (?
3), before going on toevaluate the difl'crent lnethods with character-basedand word-based indexing (?
4).
We then concludethe paper in Section 5.2 Segmentation and word orderUsing segmentat ion  to divide strings into compo-nent words or nlori)helnes has tile obvious advml-tage of clustering characters into senlantic units,which in the case of ideogrmn-based languages uchas Japanese (in the fern1 of kanji characters) andChinese, generally disatnbiguates character tnean-ing.
The kanji character ' J  \[', for example, can beused to mean any of "to discern/discriminate", "tospeak/argue" and "a valve", but word context easilyresolves uch mnbiguity, hi this sense, our intuitionis that segmented strings should produce better re-sults than non-segmented strings.Looking to past research on similarity metrics forTM systelns, ahnost all systems involving aal)aneseas the source language rely on segnlentation (e.g.
(Nakanmra, 1989; Sulnita and Tsutsumi, 1991; Ki-talnura and Yamamoto, 1996; Tmtaka, 19971), withSate (1992) and Sate and Kawase (1994) providingrare instances of character-based systelnS.By avoiding tile need to segment text;, we: (a) al-leviate computational overhead; (b) avoid the needto commit ourselves to a particular analysis type inthe case of ambiguity; (c) avoi(1 the issue of' howto deal with unknown words; (d) avoid the needfor stemming/lenlmatisation; a d (e) to a large ex-tent get around problems related to the nornmlisa-tion of lexical alternation (see Baldwin and Tanaka(1999) for a discussion of problems related to lexicalalternation in Jal)anese).
Additionally, we can usethe conmlonly anlbiguous na.ture of individual kanjicharacters to our advantage, in modelling seinan-tic similarity between related words with characteroverlap.
With word-based indexing, this would onlybe possible with tile aid of a thesaurus.Similarly for word order,  we would expect hattranslation records that preserve the word (seg-ment) order observed in the inImt string would pro-vide closer-matching translations than translationrecords containing those stone segnlents in a differ-ent order.
Natur~dly, enforcing preservation of wordorder is going to place a significant burden on thematching mechanism, in that a number of differentsubstring match schenlata re inevitably going tobe produced between rely two strings, each of whichnmst be considered on its own merits.To the authors' knowledge, there is no TM sys-tem operating from Japanese that does not relyon word/segment/character order to some degree.Tanaka (1997) uses pivotal content words identified,by the user to search through the TM and locatetranslation records which contain those same con-tent words in the stone order and preferably the stonesegment distance apart.
Nakamura (1989) similarlygives preference to translation records in which thecontent words contained in the original input occurin the same linear order, although there is tile scopeto back off to translation records which do not I)re-serve the original word order.
Sumita and Tsutsmni(19911 take the opposite tack in iteratively filter-ing out NPs and adverbs to leave only functionalwords and nlatrix-level predicates, and find trmlsla-tion records which contain those same key words inthe same ordering, preferably with the same segmenttypes between them in the same numbers.
Niren-burg et al (1993) propose a word order-sensitivemetric based on "string composition discrepancy",and increlnentally relax the restriction on the qual-ity of match required to inehlde word lenmlata, wordsynonynls and then word hyt)ernylns , increasing thematch penalty as they go.
Sate and Kawase (1994)employ a more local model of character order inmodelling similarity according to N-grams fashionedfrom the original string.The greatest advantage in ignoring word/segnlentorder is computational, in that we significantly re-duce the search space and require only a single over-all comparison per string pair.
Below, we analysewhether this gain in speed outweighs any losses inretrieval perfbrmance.3 S imi la r i ty  metr i csDue to o111" interest in the efli~cts of both word orderand seglnentation, we must have a selection of sim-ilarity lnetrics compatible with the various permu-tations of these two 1)arameter types.
We choose tolook at a nunlber of bag-of-words and word order-sensitive methods which are compatible with bothcharacter-based and word-based indexing, and varythe intmt to model tile etl~ects of the two indexingparadigms.
The particular bag-of-word approactleswe target are tlm vector space model (Manning andSchiitze, 1.999, p300) and "token intersection", asilnple ratio-based similarity nletric.
For word order-sensitive approaches, we test edit distance (Wagnerand Fisher, 1974; Planas and Furuse, 1999), "se-quential correspondence" and "weigllted sequentialcorrespondence".Each of tile similarity metrics eillpirically de-scribes the sintilarity between two inlmt strings tmimid i~., 2 where we define tmi as a source languagestring taken fl'om the TM and i~.
as the input stringwhich we are seeking to 1hatch within the TM.One featnre of all similarity metrics given here isthat they have fine-grained iscriminatory potentialand are able to narrow down the final set of trans-lation candidates to a handfld of, and in nlost casesone, outlmt.
This was a deliberate design decision,and aimed at example-based machine translation ap-plications, where human judgement cannot be reliedupon to single out the most appropriate translationfrom multiple system outputs.
In this, we set our-selves apart from the research of Sunlita and Tsut-sumi (1.991), for example, who judge the system tohave been successful if there are a total of 100 or lessoutputs, aud a useful translation is contained withinthem.
Note that it would be a relatively simple pro-2Note that the ordering here is arbitrary, and that all thesimilarity metrics described herein are commutative for thegiven implementations.36cedure  to fall ()lit the 11111111)e1" of Olltt)lltS to it ill o l l rcase, tly taking tim top n ranking outputs.For all silnitarity metrics, we weight different.\]ai)mmse gment tyl)es according to their exl)ectedimpact on translation, in the form of the sweigh, tf l lnct iol l :Segment ype s,wcightpunctuation 0other segments 1W(' exl)erinlentally trialled intermediate swcight set-tings tbr ditt'erent character tyl)es (in the case ofcharacter-based indexing) or segment yl)eS (in thecase of word-based indexing), none of which wasfomtd to apl)reciat)ly iml)rove performance.
:~a.1 Simi lar i ty metr ics used in this researchVector  space modelWithin our imt)lenmntation of the reactor spaceInodol (VSM), the segment content of each stringis (lescril)('.
(l as a vector, ma(le u l) of 3 s ingle  dimen-sion for each segment tok(,n occurring within tmi orin.
The.
value of each vector eo lnt )onent  is given asthe weighted frequen(-y of that token accor(ling toits sweiqht vahle,  such that any nulnber of 3 giveni)un(:tuation mark will produce a fl'e(luen(:y of 0.
Thestring sinfilarity of t?H, i and in is then detined sis timcosine of the angle l/etween vectors t\[\[~.i and iT\[t, re-Sl)ectivety, calculated as:tT~,i, i~5,cos(t,fi,,,i;4 - It, ll l 0)where  dot  l ) roduct  and vect()r length  (:oin(:i(le wil;hl;he standard detlnitions.The strings tmi of maximal similarity are th()sewhi(:h i ) roduce the  nmxinuun v3hw, for th(!
v(~ctorcosine.Not(; that  VSM c(msi(lers (inly s(' .gment f re(tueneyand is insensitive to word order.Token  intersect ionThe token intersection of tmi 3nd in is defined asthe cumulative intersecting fl'equency of tokens ap-pearing in each of the strings, normalised accordingto the combined segment lengths of tm, i and in.
For-really, this equates to:tint(tm~, in) : e ?
~_~, l ' l i l l  (f,'{?
(htnl (\[),frcqilz(,)) " m~(l,,,~)+>.,,(i,,) (2)where each t is a token (iccurring in e.ither tmi orin, freq,(t) is detined as the swei.qht-l)ased fi'equencyof token t occurring in string s, and Ion(s) is tlmaIf anything, weighting down hi,agana characters, fin" ex-ample, due to their common occurrence as intlectional sufficesor particles (as per Fujii and Croft (1993)) led to a significantdrop in 1)eribrmanee.
Simihwly, weighting down stop word-like flmetional parts-of-sf)eech in ,lat)anese had little eltiect,unlike weighting down stop words in the case of English (seebelow).segment length of string s, that is the swcight-1)asedCOllllt Of segl l lel l ts (:(nltained ill .s'.As tbr VSM, the string(s) tmi most similar t;(i inarc thos(; which general;e the nlaximum value tbrtint(tmi, in).Note that word order does not take any part incalculation.Edit  d istanceThe first of the word order-sensitive methods is editdist3nce (Wagner and Fisher, 1974; l?hmas and Fu-ruse, 1999).
Essentially, the segment-lmsed it dis-tance 1)etwecn strings t'ln, i and in is the minimunlnuml/er of prilnitive edit operations on single seg-ments required to transtbrm tmi into in (and viceversa), 1)ased Ul)On the ol)erations of segment equal-ity (segments tmi,m and in ,  are identical), segmentdeletion (delete segment a fl'OlIl a given 1)osition instring .s') and scgmc'nt insertion (insert segmen~ (tinto a given position in string .s).
The cost asso-ciated with each ol)eration on segment a is defined~/S: 4Operat ion Costsegment equality ()segment deletion swcigh, t(a )s(;gment insertion swcigh, t(a)Unlike other similarity metrics, smaller v31ues in-dicate greater similarity for edit distance, and iden-tical strings have edit distmme 0.The woM order sensitivity of edit distance is per-\]ml)S t)est exeml)litie(l tly way of the following exam-1)le, where segment delimiters are given as :.'.
(1) E - SN-  14-':winter r3in"(2a) 2F- $51.
l+"summer  rain"(21)) 1+" SN- 2F "a rainy summer"Itere, the edit distance from (1) to (2a) is 1 -t- 1 = 2,as one deletion ol/eration is required to remove E\[\]:uyu\] "winter" and one insertion ol)eration requiredto 3dd 2F \[natu\] "summer".
The edit distance from(1) to (21/), on the other hand, is 1 + 1 + 1 + 1 = 4despite (2b) being identical in segment content to(2a).
In terms of edit distance, therefore, (23) isadjudged more similm" to (1) than (21)).Sequent ia l  cor respondenceSequential corresI)ondence is 3 measure of the m3x-innun subsl;ring sinlilarity lmtween tmi and in, nor-malised acc(irding to the comt)ined segment lengthsh'.n(tmi) and len(in).
Essentially, this method re-quires th3t all substring matches ubmatch (tmi, in)between tmi and in be calculated, and the maximumscqcorr ratio returned, where scqcorr is delined as:, .
, 2?max\[su?,mateh(tml,in)\[~ ~ " m~It.,,.)+t~.
(~,) (3)1Note that dm costs  for deletion and insertioil must beequal to maintain commutativity.37IIere, tile cardinality operator applied tosubmatch(tmi,in) returns tile combined seg-ment length of matching substrings, weightedaccording to swcight.
That is:I~,~ .....
t~(~., ,~.~,~)I=~,j  ~ .... igl~t(s,~j,,~) (4)for each segment ssj,t~ of each matching substringssj G submatch(tmi, in).Returning to our exmnple from above, the simi-larity for (1) and (2a) is 2x2 2 whereas that for?
3+3 - -  g(1) and (2b)is ')x~ ,3+3 ~ :~"Weighted sequential correspondenceWeighted sequential correspondence--the lastof theword order-sensitive methods--~is an extension of se-quential correspondence.
It attempts to sut)plementthe deficiency of sequential correspondence that thecontiguity of substring matches is not taken intoconsideration.
Given input string a~ a2a.~a/,, forexample, sequential correspondence would suggestequal similarity (of ~)  with strings a~ ba~ca:~da/,and aj ap.
a3 a 4 cfg, despite the second of these beingmore likely to produce a translation at; least partiallyresembling tlmt of the intmt string.We get around this by associating all incremen-tal weight with each matelfing segment assessingthe contiguity of left-neighl)ouring segments, in themanner (Inscribed by Sato (1992) for chaxactcr-based matclfing.
Namely, the kth segment of amatched substring is given the multiplicative weightrain(k, Max), where Max was set to 4 in evaluationafter Sato.
I submatch,(tmi,iu,)l fi'om equation (3)thus t)ecomes:~ssj ~t,  rain ( k ?
swcight(.ssj,~.
),Ma, z) (5)tbr each sul)string ssj ~ submatch(tmi, i77,).
\?e siln-ilarly modify tile definition of the lea flmction for astring s to:lea(s) =- E jmin  (j x sweight(.,'j),Max ) (6)for each segment .sj of s.3.2 Retrieval speed optirnisationWhile this paper is mainly concerned with accuracy,we take a moment out here to discuss the potentialto accelerate the proposed methods, to get a feel fortheir relative speeds in actual retrieval.One immediate and effective way in which we canlimit the search space for all methods is to use thecurrent op-ranking score in establishing upper andlower t)ounds on the length of strings which havethe potential to better that score.
For token inter-section, for example, fi'om the fixed length lea(in)of input string in and current top score a, we cancalculate the following bounds based on the greatestpossible degree of lnatch between in and tmi:Upper bout, d: le,~(t.~d </(~-~)~n(~'~)J (7) L CZ_ F alen('in) 7 Lower bound: len(tmi) >,  2 - ( , ,  (8)In a similar fashion, we can stipulate a corridor of al-lowable segment lengths for tin i, for sequential corre-spondence and weighted sequential correspondence.For edit distance, we make the observation that tbra current minimum edit distance of a, the followinginequality over Icn(tmi) inust be satisfied for tmi tohave a chance of bettering ct:len(in) - ~ < len(tmi) < len(in) + a (9)We can also limit the numl)er of string compar-isons required to reach the optimal match with in,by indexing each tmi by its component segments andworking through the component segments of in in as-cending order of global fi'equency.
At each iteration,we consider each previously unmatched translationrecord containing the current segment token, adjust-ing the upper and lower bounds as we go, given thattranslation records for a given iteration caiulot hmrecontained segment okens already processed.
Themaxinmm possible segment correspondence b tweenthe strings is therefore decreasing on each iteration.We are also able to completely discomlt strings wit}lno segment component conunon with iTt in this way.Through these two methods, we were able togreatly reduce the number of string comparisons inword-based indexing evaluation for VSM, token in-tersection, sequential correspondence and weightedsequential correspondence methods in particular,and edit distance to a lesser degree.
The degree ofreduction for character-based indexing was not asmarked, due to the massive increase in numbers ofl;ranslation records sharing some character contentwith in.There is also considerable scope to acceleratethe matching mechanisms used by the word order-sensitive approaches.
Currently, all approaches areimplemented in Perl 5, and the word order-sensitiveapproaches use a naive, highly recursive method toexhaustively generate all substring matches and de-ternfine the sinfilarity for each.
One obvious way inwhich we could enhance this implelnentation wouldbe to use an N-gram index as proposed by Nagaoand Mori (1.994).
Dynamic Programming (DP) tech-niques would undoubtedly lead to greater efficiency,as suggested by Crmfias et al (1995, 1997) and alsoPlanas and Furuse (this volume).4 Eva luat ion4.1 Evaluation specificationsEvaluation was partitioned off into character-basedand word-based indexing for the vm'ious similaritymethods.
For word-based indexing, seginentationwas carried out with ChaSen v2.0b (Matsmnoto etal., 1999).
No attempt was made to post-edit hesegmented outtmt, in interests of maintaining con-sistency in the data.
Segmented and non-segmentedstrings were tested using a single program, withsegment length set to a single character for non-segmented strings.As test data, we used 2336 unique translationrecords deriving fi'om technical field reports on con-struction machinery translated from Japanese intoEnglish.
Translation records varied in size from38CIIAI{ACTEI{-BASEl)1NI)EXING\~)~() 1/J )-IL,\SEI)INI)I'iXINGSimilarity metricVector space model (0.5)Token intersection (0.4)Edit distance (/cn(in))-Sequential corr.
(0.4)Weighted seq.
(:orr.
(0.2)Vector sllace model (0.5)Token intersection (0.4)Edit distmme (h,n(in~-Sequential corr, (0.4)Weighted seq.
corr.
(0.2)Accuracy44.044.3Editdiserep.4.863.251.822.922.89Ave,outputs1.04 (0.97)1.01 (0.99)1.39 (0.80)1.02 (0.98)1.04 (0.97)50.246.645.643.7 (-0.8%)43.0 (-2.9%)47.3 (-5.9%)43.1 (-7.4%)40.7 (-10.7%)5.213.122.033.063.301.17 (0.91)1.01 (0.99)1.90 (0.69)1.01 (0.99)1.14 (0.92)Ave.time2.142.244.753.204.100.760.881.001.101.24Table 1: Results for the different similarity metri(:s under character-1)ased and word-based indexingsingle-word technical terms taken f1'Ol12 SI~ technicalglossary, to multiple-sentence strings, at an averagese.glnent length of 13.4 and average character lengthof 26.1.
All .lapane, se strings of length 6 chara(:tersor more (a l;ol;al of 1802 strings) were extracted fl'omthe Ix;st da.ta, leaving a resi(hle gh)ssary of te(:hni(:al1;erltls (533 strings) as we w(nfld not CXl)e('t to finduse, hll nlat(:hes in the TM.
The retrie, val a(:curacy()\,or the 1802 hmger strings was then vcritied t)y \] 0-fokt (:ross wflidation, including the glossary in thetest TM on each iteration.Not(; that the test data was llre-1)artitioned intosingle technical terms, single sentences or sen-tence clusters, each constitut;i21g a single translationrecord.
Partitions were taken as given in evaluation,whereas for reM-worhl TM systems, tim automal;i(mof th is  i)2"()cess (;Oltll)l'ises ;tll il211)ortalll; COlill)()ll(1Iltof the (/verall sysI;(mL 1)re(',eding translation rel,ri(;val.While ackn()wh;(lging the i lnl)ort;an(:(; ()f this step andits int(;ra(:l;ion with r(?ri(;val 1)or\[ormall(:(;, we (:boost,to sideste l) it for the lmri)os(~s of this pal)c.r , andleave it for hltm(; resc.m(:h.In an effort to make evaluation as ol)jeci;ive andempirical as l)ossibh;, apl)r()i)riatencss of transla-tion candidate(s) l rOl)OSed by the different metri(:swas evahmted according to the mil2inlunl edit dis-tahoe between the translation candidate(s) and theunique model translation.
In this, we transferred 1,t2(;edit distance, method described M)ove directly acrossto the ta.rg(% langustge, (English), with segments itswords and the fl)lh)wing s'weight schema:Segment ypetmnctuationstop \VOl'dSother wordsswcight00.21Stol) words are defined as those containcd within theSMART (Salton, 197\].)
stop word l ist) The systemoutput was judged to be correct if it contained atranslation optimally close to the model trmMation;the average ol)timal edit distance h'onl the modeltranslation was 4.73.
'5 \[tp://  fl, p.corne, ll.cs.ed U/l)U b/smar t/english,stopWe set; the additional criterion that the difl'erentmetrics hould be able to determine whether the top-ranking translation (:mMida.te is likeJy to be useflfl tothe translator, and that no outlmt shouhl lm given if'the chlsest nmt('hing translation record was outsidea certain l '~/Ilg( ~.
Of "transla.ti(m uscflflness'.
In p2"ac-tice, this was set to the, edit distance between themodel translation and the empty string (i.e.
the e.dit(:()st; of creating th(; model translation fl'(nn s(:ratch).This cut;off' 1)oint vlts realised for the different sim-ilarity metrics by thrcshohling over the similarit.yscores.
The ditferent hresholds ettled Ull(m experi-mentally for all similarity metrics are given ill t)ra(:k-cts in the second column of Table 1, with the thresh-ohl for (;(lit, distance dynamicMly set t(/the edit dis-lane(; l~etween the input and tim eml)ty string.\Ve set (mrs(;\]ves al)art \]'IX)211 COIlV(;21I;i()IIsll 2'(~S(;D.l'('h()n TM r(;hieval lmrl'o2unan(:(; in a(lol)ting this ()l/-.i(;(:li\'(; mmmrical (~vahmti()n method.
Traditionally,r(:i.ri(~val l)erformalm(~ has 1)(!e,n gauged 1)y tlm sub-j(~(:t;iv(; useflfln(;ss of the closest matching e.lenmnt ofthe syst;(~lll OUtlmt (as judged 1)y a. hunm,d, mid de-scribed by way of a dis(:rete set; of transla.tion (lualit;ydes('ril)tors ((;.g.
(Nakm2mra, 1989; Smnita and Tsut-smni, 1991; Sato, 1992)).
Perhaps the closest evalua-tion a.tte2nt)ts o what we prol)ose are those of' Planasand Nn'use (1.999) in s(!tting a mechanical cutoff for"translation usability" as the al/ility to generate themodel translation from a given translation candidate1)y editing less than half the component words, andNirenburg et al (1993) ill calculating the weightedmmtber of key strokes r(;quirexl to convert he systemoutllut into ;m apl)ropriate translation for the orig-inal inllut.
Tile method of Nirenburg et al (1993)is certainly more indicative of t:rue target languageuseflllness, but is dependent 022 the coml)etence ofthe translator editing the TM system output, andnot automated to the degree our method is.4.2 Resu l tsThe results for the different similarity metrics withcharacter-based and word-based indexing are givenin Tal)le 1, with the two bag-of-words al)t)roachespartitioned off from the three word order-s(msitiveal)I)roaches tor ea(:h indexing paradigm.
"Accuracy"is an indication of the prol)ortion of intmts fbr whi(:h39an optimal translation was produced; character-based indexing accuracies in bold indicate a signifi-cant ~ advantage over the corresponding wprd-basedindexing accuracy, and figures in brackets for word-based indexing indicate the relative pert'ormaimegain over the corresponding character-based index-ing configuration.
"Edit discrep."
refers to the meanminimum edit distance discrepancy between trans-lation candidate(s) and optimal translation(s) in thecase of the translation candidate set containiug uooptimal translations.
"Ave. outputs" describes theaverage number of translation candidates output bythe system, with the figure in brackets being theproportion of int)uts for which a unique translationcandidate was produced.
"Ave. time" describes theaverage time taken to deterlnine the translation era>didate(s) for a single output, relative to the timetaken tbr word-based edit distance retrieval.Perhaps the most striking result is ttmt character-based indexing produces a superior match accuracyto word-based indexing tbr all similarity metrics, at;a significant margin tbr all three word order-basedmethods.
This is the complete opposite of what wehad expected, although it does fit in with the find-ings of Fujii and Croft (1993) that character-basedindexing performs comparably with word-based in-dexing in Japanese information retrieval.Looking to word order, we see that edit distanceoutperforms all other methods for t)oth character-and word-based indexing, peaking at just over 50%for character-based indexing.
Tile relative perfor-mance of the remaining methods is variable, withthe two bag-of-words methods being superior to orroughly equivalent to sequential correspondence andweighted sequential correspondence tbr word-basedindexing, but tile word order-based methods havinga cleat' advantage over the bag-of-words methods forcharacter-based indexing.
It is thus difticult to drawany hard and fast conclusion as to the relative meritsof word order-based versus bag-of words methods,other than to say that edist distance would appearto have a clear advantage over other methods.The figures for edit discrepancy in the case of non-optimal translation candidate(s) are equally inter-esting, and suggest hat on the whole, the variousmethods err more conservatively for character-basedthan word-based indexing.
The most robust methodis (source language) edit distance, at all edit dis-crepancy of 1.82 and 2.O3 for character-based andword-based indexing, respectively.All methods were able to produce just over onetranslation candidate on average, with all other thanedit distance returning a unique translation candi-date over 90% of the time.
The greater number ofouttmts for the edit distance method can certainlybe viewed as one reason for its inflated performance,although the lower level of mnbiguity for character-based indexing but higher accuracy, would tend tosuggest otherwise.Lastly, word-based indexing was found to be fasterthan character-based indexing across the board, forthe simple reason that the immber of character seg-~As determined by the paired t test (p < 0.05).ments is always going to be greater than or equalto the number of word segments.
The average seg-ment lengths quoted above (26.1 characters vs. 13.4words) indicate that we generally have twice as manycharacters as words in a given striug.
Additionally,tile acceleration technique described in ?
3.2 of se-quentially working through the segment componentof the input string in increasing order of global fre-quency, has a greater ett>ct for word-tmsed index-ing than character-based indexing, accentuating anyspeed disparity.4.3 Ref lec t ions  on  the  resul tsAn immediate xlflanation tbr character-based in-dexing's empirical edge over word-based iudexing isthe semantic smoothing effects of individual kanjicharacters, alluded to above (?
2).
To take an exam-ple, the single-segment ouns A': n \[s6sa\] and : ng0\[sadS\] both mean "operation", but would not matchunder word-based indexing.
Character-based index-ing, on the other hand, would recogifise the overlapin character content, and in the process pick up onthe semantic orresi)ondenee b tween the two words.To take tile opposite tack, one reason wily word-based indexing may have been disadvantaged is thewe did not stem or lemmatise words in word-basedindexing.
Having said this, the.
output fl'om ChaSenis such that stems of inflecting words are given asa single segment, with inflectional morphemes eachpresented as sel)arate segments.
In this sense, stem-ruing would only act to delete the inflectional mor-phemes, and not add allything new.Another way in which the outlmt of ChaSencould conceivably have atlbcted retrieval perfor-iilance is that technical terms tended to be over-segmented.
Experilnentally combining recognisedtechnical terms into a single segment (particularlyin the case of contiguous katakana segments in themanner of Nljii and Croft (1993)), however, de-graded rather than lint)roved retrieval performancefor both character-based and word-based indexing.As such, this side-etfect of ChaSen would not appearto have impinged on retriewfl accuracy.One other plausible reason for tile unexpected re-sults is that the test data could have been ill someway inherently better suited to character-based in-dexing than word-based indexing, although the factthat the results were cross-wtlidatcd would tend torule out this possibility.A surprising result was the lacklustre performanceof the weighted sequential correspondence method ascompared to simple sequential correspondence.
Wehave no explanation for the drop in accuracy, otherthan to speculate that either the proposed formu-lation is in some way flawed or contiguity of matchdoes not impinge on translation similarity to the de-gree we had expected.To return to the original question posed above ofretrieval speed vs. accuracy, the word order-sensitiveedit distance approach would seem to hold a gen-uine edge over the other methods, to an order thatwould suggest he extra computational overhead iswarranted, ill both accuracy and translation discrep-ancy.
It must be said that the TM used in evalua-40tion was too small to get a gemfine f(;el for the com-t)ul;ational overhead that would 1)e cxp(,,ri(;ncc, d in~ real-world TM system context of t)ot;entially mil-lions rath(;r than thousands of translation records.A C the saint', (tim(;, however, coding Ul) the c(lit dis-tan(:(; l)roc(',dure in a language fasto, r than Perl usingchara(;l;(?r ~d;h(~,r \[;\]lall SI;t'illg COIlq)arisol~ 1)roc(?
(hlrcsmid ai)l)lying (lynami(" 1)rogl'amming t(whni(lu(,,s orsimilar, may well oIl~set h('.
large \]nero.as(; in numberof comparisons dcmand(',d of the system.5 Concluding remarksThis research is concerned with l;}m r(;lativ(~ iml)orl;ot7 word order and segm(mta.1;ion n translation re-l;rieval i)erformmlc(~ tbr a TM system.
Wc mo(Ml('xlthe elthcts of word order s(msitivity vs. 1)ag-of-wol'dSword order ins(msit;ivity 1)y iml)l(mmnl,ing a total oflive similarity mcla'ics: two bag-of-words al)proach(',s(lhe v(',(:tor spa(:(; model and "tol?
('.n int(us(!
(:tion")and tin'('.
(', w()r(l ord(',r-s(;nsitive al)l)roach(',s ((',(lit; dis-tan(:('., "s(;quential corr(',Sl)ond(',nce" and "wcight(',dsequential corr(~st)ondenc(?').
Ea(:h of th(;s(; nw, tri(',swas then l;(~sl;e(t Hll(ler (:har;~cl;(;r-1)as(~(\[ al~(t word-based in(h'~xing, to deto, rmin(,~ what (;tt'c(:t s(~gm(',nta-l;ion wouhl have, on r('.trieval 1)(~rl'orman(:(h Eml)iri-c~d evaluation })asc, d }l l 'O l l l ld  \[,h( ~, l;alg(!l, languag(', (;(titdistance of t)rot)osed traiMa.tion can(lidal(',s r(~vcaicdthat (:hara(:tcr-1)ascd indexing consist(mtly produ(:edgr('~atcr accuracy than wordq)ased in(lexiltg; and thai;the word or(l(~r-s('atsitivo~ (;(lit distain:(', m(;tri(: clearlyoutl)(',rforme(1 all other methods un(h',r 1)oth in(l(',xingparadigms.The main area in wlfi(',h we, fc!d this r(~s(!ar(:h c(mht1)c, (mhan(:(~d is to validate th(~ findings of this 1)a-per in (~Xlmn(ling evahlati()n 1o olh(w domains midl;esl; Set,q, whi(:h wc h'av(', as ;lll il:(?lll 1'()1 t'ulm(~ re-s(mr(:h. We also skirl;ed m'(mnd lira issu(~ ()f lrmls-lation record partitioning, and wish 11)inv(!stigalehow difl'(;r(mt 1)mtitioning m(~'tho(ls lmrfl)rm againsl;c,;mh other.
One important area in which w(; hop(~to eXl)and our resem'ch is to look at tim etl'(~(:ts ofcharacter type on chm'act(',r-bas(~d in exing, t(anjiwould a,ppear to be helping the case of character-based indexing at t)rc, s(mt, ;rod it woul(\[ 1)e highlyr(;vcaling to look at wh(',th(',r COml)ara,1)l(', ro, sults tot\]losc 1)r(:s(;nt('d h(;r(~ would 1)(', t)ro(ht(:ed \[or fullkaim-basc'd (alphal)c, ti(:) ,lal)an(',sc input, or otlmrall)hal)ct-1)ased n(m-s(~gm(ulting languages such asThai.AcknowledgementsVital input into this research was rcc(~ivcd t?omFrancis Bond (NTT), Emmanu(;1 Planas (NTT), andthree anonylllOUS reviewers.References% Baldwin mul Ill. Tanaka.
1999.
The applications ofunsul)crvised learning to ,I~tl)mmsc gral)\]mmc,-1)honcin(~aligmnent,.
In l'roc, of th.e.
AUL I.Vortc.d~op oa Uu-supervised Learning in Natu'ral Language l~roccs.sin9,pages 9 16.L.
Cranias, H. Ibqmgr.orgiou, and S. Pilmridis.
1995.A Matching Technique in Example-Based MachineTranslation.
cmp-lg/9508005.L.
Cranias, H. Papageorgiou, and S. Piper\]dis.
1997.
E?-amt)h~ retrieval from a trmlslation memory.
NatwralLanguage \]'Jngine.ering, 3(4):255 77.1t.
Fuji\] and W.B.
Croft.
1.993.
A comparison of index-ing tc(:lmiqu(~s fl)r .lal)ancsc t;c.x|; r('.trieval.
In Proc.of 161h International ACM-SIGH~, Cot@fence on Re-search and Dc'vclopmcnt in Information Ib:tricval (SI-GIR'93), pages 237 46.It;.
Kitamura and II.
"~Smmmoto.
1996.
Translationretrieval systo.m using alignment data flom parallc.ltexts, in P~wc.
of the 5&'d Annual Mccting of tit("II'S,I, volmne 2, pages 385 6.
(Ill Ja.t)ancsc ).C.
Manning and II.
S(:hiil;ze.
1999.
Foundations of Sta-tistical Natural La'ngurtgc P~vccssing.
MIT Press.Y.
Matsmnoto, A. I(i/,auchi, T. Yamashita, and Y. IIi-rano.
1999. ,\]apancsc Moudtolo.qical Analysis S?/s-l, cm UttaScn Version 2.0 Manual.
~lt'~chnical l/.eporl;NAISqUIS-Tl199009, NAIST.M.
Nagao and S. Mort.
1994.
A new method of N-grantstatist;its tbr large mmflmr of N and ;mtonmtic ex-\[;ra(;1;ion of words and l)hrases front large text; dataof .lapanese.
In Proc.
of the 15th, lntc~'~u~tioual Con-Jcrcncc on Computational Linguistics (COLING '9/~),pages 611- 5.N.
Nakamma.
1989.
~l?~mslat, ion supl)orf by retrievingbilingual texts.
In l'~wc, of the 38th Annual Mcctin 9of the IPSJ, volume 1, pagt;s 357 8.
(In Jai)ancs(; ).S.
Nirelflmrg, C. l)omashnc.v, and \]).J.
Gramms.
1993.Two apt)roa(:hes to mat;thing in eXaml)h>bas(~d rim-chin(', translation.
In Proc.
of the 5th InternationalCoT@:rc'ncc on 771corctical and Mcthodologic(d lasucsi'tl.
Math, inc. 7!ransl,,tio'a 151'M1-93), pages d7 57.E.
Planas and ().
l:uruse.
1999.
F(wmalizing translationm(m,n'ies.
In l)Twc, o.f Math\]n(: Translation ,%m'mitVII, pages 331 9.1'2.
Planas.
1998.
A Case, Study on Memory Based Ma-chine ~}'anslation 7bols.
Phi) Felkm~ \Vorking 1)al)c.r,Unil;ed Nations University.G.
Salton.
1971.
The SMAR, T It, err\]oval Sy.stevt: E:rpcr-ime.nt.s in Automatic Document Processing.
Prentice-Hall.S.
Sato and 3'.
Kawase.
1994.
A ltigh-Spc.ed B(:st Matchi{e.tricval Method fin" ,\]apancsc ~}:'a;t. Tct:lulical Rctmrt;1S-11R-94-9I, JAIST.S.
Sato.
1992.
CTM: An examlfl('A)ased translation aidsystem.
In l"~vc, of the 141h International Confcrc.nccon Computational Linguistics (COLING '92), pages1259 63.E.
Smnit;}~ mtd Y. Tsutsumi.
1991.
A 1)ract,ical methodof retrieving similar examples 1or trmMation aid.7Yansaction,s of the IEICE, J74-D-II(10):1437 47.
(InJapanese).It.
Tanaka.
1.997.
An efficient way of gauging siinilar-ity lmtwcen hmg .lalmnc, so, expressions.
In Informa-tion l~roccssin9 ,%ciety of Japan SIG Notes, vohun(!,1t7, no.
85, 1)ages 69 74.
(In .l~q)aneso,).A.
Wagner and M. Fisher.
1974.
The' string-to-stringcorrection 1)roblcm.
Journal of the A CM, 21(1):16873.41
