Proceedings of the NAACL HLT 2013 Demonstration Session, pages 24?27,Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational LinguisticsKooSHO: Japanese Text Input Environmentbased on Aerial Hand WritingMasato HagiwaraRakuten Institute of Technology215 Park Avenue South,New York, NY, USA 10003masato.hagiwara@mail.rakuten.comSoh MasukoRakuten Institute of Technology4-13-9 Higashi-shinagawaShinagawa-ku, Tokyo, JAPAN 140-0002so.masuko@mail.rakuten.comAbstractHand gesture-based input systems havebeen in active research, yet most of themfocus only on single character recognition.We propose KooSHO: an environment forJapanese input based on aerial hand ges-tures.
The system provides an integratedexperience of character input, Kana-Kanjiconversion, and search result visualization.To achieve faster input, users only have toinput consonant, which is then converteddirectly to Kanji sequences by direct conso-nant decoding.
The system also shows sug-gestions to complete the user input.
Thecomparison with voice recognition and ascreen keyboard showed that KooSHO canbe a more practical solution compared tothe existing system.1 IntroductionIn mobile computing, intuitive and naturaltext input is crucial for successful user experi-ence, and there have been many methods andsystems proposed as the alternatives to tradi-tional keyboard-and-mouse input devices.
Oneof the most widely used input technologies isvoice recognition such as Apple Inc.?s Siri.
How-ever, it has some drawbacks such as being vul-nerable to ambient noise and privacy issueswhen being overheared.
Virtual keyboards1require extensive practice and could be error-prone compared to physical keyboards.1http://www.youtube.com/watch?v=h9htRy0-sUw(a) (b) (c)Figure 1: Overall text input procedure usingKooSHO ?
(a) Character recognition (b) Kana-Kanji conversion results (c) Search resultsIn order to address these issues, many gesture-based text input interfaces have been proposed,including a magnet-based hand writing device(Han et al 2007).
Because these systems re-quire users to wear or hold special devices, handgesture recognition systems based on video cam-eras are proposed, such as Yoon et al(1999)and Sonoda and Muraoka (2003).
However, alarge portion of the literature only focuses onsingle character input.
One must consider over-all text input experience when users are typingwords and phrases.
This problem is pronouncedfor languages which require explicit conversionfrom Romanized forms to ideographic writingsystems such as Japanese.In this paper, we propose KooSHO: an in-tegrated environment for Japanese text inputbased on aerial hand gestures.
It provides anintegrated experience of character input, Kana-Kanji conversion, i.e., conversion from Roman-ized forms to ideographic (Kanji) ones, andsearch result visualization.
Figure 1 shows theoverall procedure using KooSHO.
First, (a) auser draws alphabetical shapes in the air, whosehand position is captured by Microsoft Kinect.KooSHO then recognizes characters, and after24KooSHO EngineKinectThe InternetScreenFront-EndCharacter Recognition& User InterfaceBack-EndKana-Kanji Conversion& SuggestionFigure 2: Confiugration of the KooSHO systemKana-Kanji conversion, the results are shown ina circle centered at the user?s shoulder (b).
Fi-nally, the user can choose one of the candidatesby ?touching?
it, and (c) the search result usingthe chosen word as the query is shown in circleagain for the user to choose.KooSHO has several novelties to achieveseamless yet robust text input, including:Non-restrictive character forms ?
thesystem does not restrict on the input characterforms, unlike Graffiti 22.Robust continuous recognition and con-version ?
Aerial handwriting poses special dif-ficulties since the system cannot obtain individ-ual strokes.
We solve this problem by employinga discriminative Kana-Kanji conversion modeltrained on the specific domain.Faster input by suggestions and con-sonant input ?
KooSHO shows suggestionsto predict the words the user is about to in-put, while it allows users to type only conso-nants, similar to Tanaka-Ishii et al(2001).We propose direct consonant decoding, whichruns Viterbi search directly on the input con-sonant sequence without converting them backinto Kana candidates.We conducted the evaluations on characterrecognition and Kana-Kanji conversion accu-racy to measure KooSHO?s performance.
Wealso ran an overall user experience test, compar-ing its performance with the voice recognitionsoftware Siri and a screen keyboard.2 Character RecognitionFigure 2 describes the overall configuration.
Auser draws alphabetical shapes in the air, whichis captured by Kinect and sent to KooSHO.
We2http://en.wikipedia.org/wiki/Graffiti_2??
(hokubu)BOS EOSH K B K R??
(kore)?
(huku) ?
(bukuro)?
(huku) ??
(buki) ?
(ro)?
(hu) ????
(kobukuro)Figure 3: Lattice search based on consonantsused the skeleton recognition functionalities in-cluded in Kinect for Windows SDK v1.5.1.
Thesystem consists of the front-end and back-endparts, which are responsible for character recog-nition and user interface, and Kana-Kanji con-version and suggestion, respectively.We continuously match the drawn trajectoryto templates (training examples) using dynamicprogramming.
The trajectory and the templatesare both represented by 8 direction features tofacilitate the match, and the distance is cal-culated based on how apart the directions are.This coding system is robust to scaling of char-acters and a slight variation of writing speed,while not robust to stroke order.
This is re-peated every frame to produce the distance be-tween the trajectory ending at the current frameand each template.
If the distance is below a cer-tain threshold, the character is considered to bethe one the user has just drawn.If more than one characters are detected andtheir periods overlap, they are both sent as al-ternative.
The result is represented as a lattice,with alternation and concatenation.
To each let-ter a confidence score (the inverse of the mini-mum distance from the template) is assigned.3 Kana-Kanji ConversionIn this section, we describe the Kana-Kanjiconversion model we employed to achieve theconsonant-to-Kanji conversion.
As we men-tioned, the input to the back-end part passedfrom the front-end part is a lattice of possi-ble consonant sequences.
We therefore have to?guess?
the possibly omitted vowels somehowand convert the sequences back into intendedKanji sequences.
However, it would be an ex-ponentially large number if we expand the in-put consonant sequence to all possible Kana se-25quences.
Therefore, instead of attempting to re-store all possible Kana sequences, we directly?decode?
the input consonant sequence to ob-tain the Kanji sequence.
We call this processdirect consonant decoding, shown in Figure 3.
Itis basically the same as the vanilla Viterbi searchoften used for Japanese morphological analysis(Kudo et al 2004), except that it runs on a con-sonant sequence.
The key change to this Viterbisearch is to make it possible to look up the dic-tionary directly by consonant substrings.
To dothis, we convert dictionary entries to possibleconsonant sequences referring to Microsoft IMEKana Table3 when the dictionary structure isloaded onto the memory.
For example, for a dic-tionary entry??/?????
hukubukuro, possi-ble consonant sequences such as ?hkbkr,?
?huk-bkr,?
?hkubkr,?
?hukubkr,?
?hkbukr,?...
arestored in the index structure.As for the conversion model, we employed thediscriminative Kana-Kanji conversion model byTokunaga (2011).
The basic algorithm is thesame except that the Viterbi search also runs onconsonant sequences rather than Kana ones.
Weused surface unigram, surface + class (PoS tags)unigram, surface + reading unigram, class bi-gram, surface bigram as features.
The red linesin Figure 3 illustrate the finally chosen path.The suggestion candidates, which is to showcandidates such as hukubukuro (lucky bag) andhontai (body) for an input ?H,?
are chosen from2000 most frequent query fragments issued in2011 at Rakuten Ichiba4.
We annotate eachquery with Kana pronunciation, which is con-verted into possible consonant sequence as inthe previous section.
At run-time, prefix searchis perfomed on this consonant trie to obtain thecandidate list.
The candidate list is sorted bythe frequency, and shown to the user supple-menting the Kana-Kanji conversion results.4 ExperimentsIn this section, we compare KooSHO withSiri and a software keyboard system.
Weused the following three training corpora: 1)3http://support.microsoft.com/kb/883232/ja4http://www.rakuten.co.jp/BCCWJ-CORE (60,374 sentences and 1,286,899tokens)5, 2) EC corpus, consists of 1,230 producttitles and descriptions randomly sampled fromRakuten Ichiba (118,355 tokens).
3) EC querylog (2000 most frequent query fragments issuedin 2011 at Rakuten Ichiba) As the dictionary,we used UniDic6.Character Recognition Firstly, we evaluatethe accuracy of the character recognition model.For each letter from ?A?
to ?Z,?
two subjectsattempted to type the letter for three times, andthe accuracy how many times the character wascorrectly recognized was measured.We observed recognition accuracy varies fromletter to letter.
Letters which have similarforms, such as ?F?
and ?T?
can be easily mis-recognized, leading lower accuracy.
For someof the cases where the letter shape completelycontains a shape of the other, e.g., ?F?
and ?E,?recognition error is inevitable.
The overall char-acter recognition accuracy was 0.76.Kana-Kanji Conversion Secondly, we eval-uate the accuracy of the Kana-Kanji conversionalgorithm.
We used ACC (averaged Top-1 ac-curacy), MFS (mean F score), and MRR (meanreciprocal rank) as evaluation measures (Li etal., 2009).
We used a test set consisting of100 words and phrases which are randomly ex-tracted from Rakuten Ichiba, popular productsand query logs.
The result was ACC = 0.24,MFS = 0.50, and MRR = 0.30, which suggeststhe right choice comes at the top 24 percent ofthe time, about half (50%) the characters of thetop candidate match the answer, and the aver-age position of the answer is 1 / MRR = 3.3.
No-tice that this is a very strict evaluation since itdoes not allow partial input.
For example, evenif ????????????
fittonesushu-zu (fitnessshoes) does not come up at the top, one couldobtain the same result by inputting ????????
(fitness) and ??????
(shoes) separately.Also, we found that some spelling variationssuch as ???
and ???
(both meaning eye-lashes) lower the evaluation result, even though5http://www.ninjal.ac.jp/corpus_center/bccwj/6http://www.tokuteicorpus.jp/dist/26they are not a problem in practice.Overall Evaluation Lastly, we evaluate theoverall input accuracy, speed, and user experi-ence comparing Siri, a screen keyboard (TabletPC Input Panel) controlled by Kinect usingKinEmote7, and KooSHO.First, we measured the recognition accuracyof Siri based on the 100 test queries.
The accu-racy turned out to be 85%, and the queries wererecognized within three to four seconds.
How-ever, about 14% of the queries cannot be recog-nized even after many attempts.
There are es-pecially two types of queries where voice recog-nition performed poorly ?
the first one is rel-atively new, unknown words such as ??????
(ogaland), which obviously depends on therecognition system?s language models and thevocabulary set.
The second the is homonyms,i.e., voice recognition is, in principle, unable todiscern multiple words with the same pronun-ciation, such as ????
(package) and ????
(broadcast) housou, and ???????
(alum) and????
(tomorrow evening) myouban.
This iswhere KooSHO-like visual feedback on the con-version results has a clear advantage.Second, we tried the screen keyboard con-trolled by Kinect.
Using a screen keyboard wasextremely difficult, almost impossible, since itrequires fine movement of hands in order toplace the cursor over the desired keys.
There-fore, only the time required to place the cursoron the desired keys in order was measured.
Thefact that users have to type out all the charactersincluding vowels is making the matter worse.This is also where KooSHO excels.Finally, we measured the time taken forKooSHO to complete each query.
The resultvaried depending on query, but the ones whichcontain characters with low recognition accu-racy such as ?C?
(e.g., ?????
(cheese)) tooklonger.
The average was 35 seconds.Conclusion and Future WorksIn this paper, we proposed a novel environ-ment for Japanese text input based on aerialhand gestures called KooSHO, which provides7http://www.kinemote.net/an integrated experience of character input,Kana-Kanji conversion, and search result vi-sualization.
This is the first to propose aJapanese text input system beyond single char-acters based on hand gestures.
The system hasseveral novelties, including 1) non-restrictivecharacter forms, 2) robust continuous recogni-tion and Kana-Kanji conversion, and 3) fasterinput by suggestions and consonant input.
Thecomparison with voice recognition and a screenkeyboard showed KooSHO can be a more prac-tical solution compared to the screen keyboard.Since KooSHO is an integrated Japanese in-put environment, not just a character recog-nition software, many features implemented inmodern input methods, such as fuzzy match-ing and user personalization, can also be im-plemented.
In particular, how to let the usermodify the mistaken input is a great challenge.ReferencesXinying Han, Hiroaki Seki, Yoshitsugu kamiya, andMasatoshi Hikizu.
2007.
Wearable handwritinginput device using magnetic field.
In Proc.
ofSICE, pages 365?368.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields toJapanese morphological analysis.
In Proc.
ofEMNLP, pages 230?237.Haizhou Li, A Kumaran, Vladimir Pervouchine, andMin Zhang.
2009.
Report of news 2009 machinetransliteration shared task.
In Proc.
of NEWS,pages 1?18.Tomonari Sonoda and Yoishic Muraoka.
2003.
Aletter input system of handwriting gesture (inJapanese).
The Transactions of the Institute ofElectronics, Information and Communication En-gineers D-II, J86-D-II:1015?1025.Kumiko Tanaka-Ishii, Yusuke Inutsuka, and MasatoTakeichi.
2001.
Japanese text input system withdigits.
In Proc.
of HLT, pages 1?8.Hiroyuki Tokunaga, Daisuke Okanohara, and Shin-suke Mori.
2011.
Discriminative method forJapanese kana-kanji input method.
In Proc.
ofWTIM.Ho-Sub Yoon, Jung Soh, Byung-Woo Min, andHyun Seung Yang.
1999.
Recognition of al-phabetical hand gestures using hidden markovmodel.
IEICE Trans.
Fundamentals, E82-A(7):1358?1366.27
