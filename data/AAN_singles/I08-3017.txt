Proceedings of the IJCNLP-08 Workshop on NLP for Less Privileged Languages, pages 105?112,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingAbstractHuman computer interaction through NaturalLanguage Conversational Interfaces plays avery important role in improving the usage ofcomputers for the common man.
It is theneed of time to bring human computerinteraction as close to human-humaninteraction as possible.
There are two mainchallenges that are to be faced inimplementing such an interface that enablesinteraction in a way similar to human-humaninteraction.
These are Speech to Textconversion i.e.
Speech Recognition & TextTo Speech (TTS) conversion.
In this paperthe implementation of one issue SpeechRecognition for Indian Languages ispresented.1    IntroductionIn India if it could be possible to provide humanlike interaction with the machine, the common manwill be able to get the benefits of information andcommunication technologies.
In this scenario theacceptance and usability of the informationtechnology by the masses will be tremendouslyincreased.
Moreover 70% of the Indian populationlives in rural areas so it becomes even moreimportant for them to have speech enabledcomputer application built in their native language.Here we must mention that in the past timedecades, the research has been done on continuous,large vocabulary speech processing systems forEnglish and other European languages; Indianlanguages as Hindi and others were not beingemphasized.
For example  for   English   language,Implementing a Speech Recognition System Interface for IndianLanguagesthe commercial speech recognition systemsavailable in the market are IBM Via Voice andScansoft Dragon system.
These have mature ASRengines with high accuracy.
No such system withthis accuracy is available for Hindi (Kumar et al,2004).
India is passing through the phase ofcomputer revolution.
Therefore it is need of timethat speech processing technology must bedeveloped for Indian languages.Fortunately people have realized the great needof human machine interaction based on Indianlanguages.
Researchers are striving hard currentlyto improve the accuracy of the speech processingtechniques in these languages (Samudravijaya,2000).
Speech technology has made great progresswith the development of better acoustic models,new feature extraction algorithms, better NaturalLanguage Processing (NLP) and Digital SignalProcessing (DSP) tools and significantly betterhardware and software resources.Through this paper, we describe thedevelopment of ASR system for Indian languages.The challenges involved in the development aremet by preparing various algorithms and executingthese algorithms using a high level languageVC++.
Currently a Speech-In, Text-Out interfaceis implemented.The paper is organized as follows: section 2presents the architecture and functioning of ASR.Section 3 describes the modeling and design usedfor speech recognition system.
Section 4 describesthe experiments & results.
Section 5 describes theconclusion and future work.2    Architecture of ASRThe basic structure of a speech recognition systemR.
K. Aggarwal Mayank DaveDeptt.
Of Computer Engg.
Deptt.
Of Computer Engg.N.I.T.
Kurukshetra N.I.T.
Kurukshetrar_k_a@rediffmail.com mdave67@rediffmail.com105is shown in figure 1.2.1   PreprocessingIt covers mainly these tasks- A/D conversion,Background noise filtering, Pre emphasis,Blocking and Windowing.In first task analog electrical signals aredigitized, i.e.
these are converted into a discrete-time, discrete-valued signal.
This process of analogto digital conversion has two steps: sampling andquantization.
A signal is sampled by measuring itsamplitude at a particular time; the sampling rate isthe number of samples taken per second.
Ingeneral, a sampling rate between 8 and 20 kHz isused for speech recognition application (JamesAllen, 2005).
As a point of reference, perceptualstudies generally indicate that frequencies up toabout 10 kHz (10,000 cycles per second) occur inspeech, but speech remains intelligible within aconsiderably narrower range.The second important factor is the quantizationfactor, which determines what scale is used torepresent the signal intensity.
Generally, it appearsthat 11-bit numbers capture sufficient information,although by using a log scale, we can get by with8-bit numbers.
In fact, most current speechrecognition systems end up classifying eachsegment of signal into only one of 256 distinctcategories.AcousticModelsSo a typical representation of a speechsignal is a stream of 8-bit numbers at the rate of10,000 numbers per second clearly a large amountof data.
The challenge for speech recognitionsystem is to reduce this data to some manageablerepresentation.Once signal conversion is complete,background noise is filtered to keep SNR high.While speech capturing, background noise andsilence (absence of speech) will also be quantizedwith speech data.
An important problem in speechprocessing is to detect the presence of speech in abackground of noise and silence from ourrecording speech.
This problem is often referred toas the endpoint detection problem (Rabiner andBishnu, 1976).
By accurately detecting thebeginning and end of an utterance the amount ofprocessing can be kept at minimum.
According toliterature (Reddy, 1976), accurate determination ofend points is not very difficult if the signal-to-noiseratio is high, say greater than 60 dB.The next step is the pre-emphasis.
Themotivation behind it is to emphasize the importantfrequency components in the signal (i.e.
amplifyimportant areas of the spectrum) by spectrallyflatten the signal.
For example hearing is moresensitive in the 1 KHz-5 KHz region of thespectrum.
It amplifies this area of spectrum,assisting the spectral analysis algorithm inmodeling the most perceptually important aspectsof the speech spectrum.Input RecognizedUtterancePreprocessingSXFeatureExtractionPatternClassifier Signalspace PowerSignal Feature SpaceFront ?End-ProcessingLanguageModelsBack-End-ProcessingFigure 1.
Architecture of ASR1062.2    Feature Extraction/Parametric TransformThe goal of feature extraction is to find a set ofproperties of an utterance that have acousticcorrelations in the speech signal, that is parametersthat can some how be computed or estimatedthrough processing of the signal waveform.
Suchparameters are termed as features.
Featureextraction is the parameterization of the speechsignal.
It includes the process of measuring someimportant characteristic of the signal such asenergy or frequency response (i.e.
signalmeasurement), augmenting these measurementswith some perceptually meaningful derivedmeasurements (i.e.
signal parameterization), andstatically conditioning these numbers to formobservation vectors.There are several ways to extract features fromspeech signal as given below:?
Linear Predictive Cepstral Coefficients(LPCC)?
Mel Frequency Cepstral Coefficients(MFCC) (Skowronski et al, 2002)?
Wavelet as feature extractor (Biem, 2001)?
Missing feature approach (Raj et al, 2005)2.3    Acoustic ModelingIn this subsystem, the connection between theacoustic information and phonetics is established.Speech unit is mapped to its acoustic counterpartusing temporal models as speech is a temporalsignal.
There are many models for this purposelike,?
Hidden Markov Model (HMM)?
Artificial Neural Network (Rao etal.,2004)?
Dynamic Bayesian Network (DBN) (Huoet al, 1997)ANN is a general pattern recognition modelwhich found its use in ASR in the early years.Rabiner (1991), first suggested the HMM approachleading to substantial performance improvement.Current major ASR systems use HMM for acousticmodeling.
Since then researchers have tried tooptimize this model for memory and computationrequirements.
In the current state, it seems thatHMM has given the best it could and now we needto find other models to go ahead in this domain.This leads to consideration of other models inwhich Dynamic Bayesian Network seems apromising directionStill our experiments and verification has beendone on a HMM based system.2.4    Language ModelingThe goal of language modeling is to produceaccurate value of probability of a word W, Pr(w).A language model contains the structuralconstraints available in the language to generatethe probabilities.
Intuitively speaking, itdetermines the probability of a word occurringafter a word sequence.
It is easy to see that eachlanguage has its own constraints for validity.
Themethod and complexity of modeling languagewould vary with the speech application.
This leadsto mainly two approaches for language modeling.Generally, small vocabulary constrained tasks likephone dialing can be modeled by grammar basedapproach where as large applications like broadcastnews transcription require stochastic approach.3 Modeling and Design for Implementation3.1   Signal Modeling and Front End DesignSpeech signal is an analog signal at the recordingtime, which varies with time.
To process the signalby digital means, it is necessary to sample thecontinuous-time signal into a discrete-time signal,and then convert the discrete-time continuous-valued signal into a discrete-time, discrete-valued(digital) signal.
The properties of a signal changerelatively slowly with time, so that we can dividethe speech into a sequence of uncorrelatedsegments, or frames, and process the sequence as ifeach frame has fix properties.
Under thisassumption, we can extract the features of eachframe based on the samples inside the frame only.And usually, the feature vector will replace theoriginal signal in the further processing, whichmeans the speech signal is converted from a timevarying analog signal into a sequence of featurevectors.
The process of converting sequences ofspeech samples to feature vectors representing107events in the probability space is called SignalModeling (Picone, 1993; Karanjanadecha andZoharian, 1999).Signal Modeling can be divided into two basicsteps: Preprocessing and Feature Extraction.Preprocessing is to pre-process the digital samplesto make available them for feature extraction andrecognition purpose.
The steps followed duringsignal modeling are as following:?
Background Noise and Silence Removing?
Pre emphasis?
Blocking into Frames?
Windowing?
Autocorrelation Analysis?
LPC Analysis?
LPC Parameter Conversion to CepstralCoefficients3.2    Back End ProcessingThe last section showed how the speech inputcan be passed through signal processingtransformations and turned into a series of vectorsof features, each vector representing one time sliceof the input signal.
How are these feature vectorsturned into probabilities?3.2.1  Computing Acoustic ProbabilitiesThere are two popular versions of continuousapproach.
The more widespread of the two is theuse of Gaussian  pdfs,  in  the  simplest  version  ofWord ModelObservation Sequence       ?
?
(Special feature vectors)which each state has a single Gaussian functionwhich maps the observation vector Ot to aprobability.
An alternative approach is the use ofneural networks or multilayer preceptons whichcan also be trained to assign a probability to a realvalued feature vector.
HMMs with Gaussianobservation-probability-estimators are trained by asimple extension to the forward-backwardalgorithm.
HMMs with neural-net observation-probability-estimators are trained by a completelydifferent algorithm known as error back-propagation.3.2.2 HMM and its Significance to SpeechRecognition:Among the various acoustic models, HMM is sofar the most widely used and most effectiveapproach.
Its popularity is due to an efficientalgorithm for training and recognition and itsperformance superiority over the other modelingstructures.An HMM is a statistical model for an orderedsequence of symbols, acting as a stochastic finitestate machine which is assumed to be build upfrom a finite set of possible states, each of thosestates being associated with a specific probabilitydistribution or probability density function.
Eachstate of machine represents a phonetic event and asymbol is generated each time a transition is madefrom one state to the next.a24 a11 a22????
?O3 O4b1(o2)Start a2 m3 enda33b1(o1)r1b2(o3) ba a12 a23 012(o5) b3(o6)O6 O1 O2 O5Figure 2.
An HMM Pronunciation Network for the Word ?Ram?108There are three basic problems related to HMM:the evolution problem, the decoding problem andthe learning problem.
These problems areaddressed by Forward Algorithm, ViterbiAlgorithm and Baum-Welch Algorithmrespectively.
A detailed discussion of the same isavailable here (Rabiner, 1989).HMM-based recognition algorithms areclassified into two acoustic models, i.e., phoneme-level model and word-level model.
The phoneme-level HMM has been widely used in current speechrecognition systems which permit large sizedvocabularies.
Whereas the word-level HMM hasexcellent performance at isolated word tasks and iscapable of representing speech transitions betweenphonemes.
However its application has remained aresearch-level and been constrained to small sizedvocabularies because of extremely highcomputation cost which is proportional to thenumber of HMM models.
Recognitionperformance in the word HMM is determined bythe number of HMM states and dimensions offeature vectors.
Although high numbers of thestates and the dimensions are effective inimproving recognition accuracy, the computationcost is proportional to these parameters(Yoshizawa et al, 2004).
As conventional methodsin high-speed computation of the outputprobability, Gaussian selection (Knill et al, 1996)and tree structured probability density function(Watanabe et al, 1994) are proposed in thephoneme-level HMM.
These methods use theapproximation to the output probability if exactprobability values are not required.
However in theword HMM, output probability values directlyeffects recognition accuracy and a straight forwardcalculation produces the most successfulrecognition results (Yoshizawa et al, 2004).To see how it can be applied to ASR, we areusing a whole-word isolated word recognizer.
Inour system there is a HMM Mi for each word I thedictionary D. HMM Mi is trained with the speechsamples of word Wi using the Baum-WelchAlgorithm.
This completes the training part of theASR.
At the time of testing the unknownobservation sequence O is scored against each ofthe models using the forward algorithm and theword corresponding to the highest scoring model isgiven as a recognized word.For example  HMM  pronunciation networkfor the word ?Ram?, given in figure 2 shows thetransition probabilities A, a sample observationsequence O and output probabilities B. HMMsused in speech recognition usually use self loopson the states to model variable phone durations;longer phones require more loops through theHMM.3.2.3 Language ModelingIn speech recognition the Language Model is usedfor speech segmentation.
The task of finding wordboundaries is called segmentation (Martin andJurafsky, 2000).
For example decode the followingsentence.
[ ay  hh   er  d  s sh  m  th  ih  ng  ax  b aw  m  whv  ih  ng  r  ih  s  en  l ih ]I heard something about moving recently.
[ aa n iy dh ax]              I need the.4.
Experiment & ResultsThe experiment consist of an evaluation of  thesystem using the room condition, and the standardspeech capturing hardware such as sound blastercard and a headset microphone.
Samplingfrequency of the signal is 16000 Hz with thesample size of 8 bits.
Threshold energy 10.1917 dBis used in the word detection.
In the experimentHidden Markov Model is used for the recognitionof isolated Hindi words.
At the time of speechrecognition, various words are hypothesizedagainst the speech signal.
To compute thelikelihood (probabilistic) of a given word, the wordis broken into its constituent phones, and thelikelihood of the phones is computed from theHMMs.
The combined likelihood of all of thephones represents the likelihood of the word in theacoustic model.
To implement it successfully,transcript preparation and dictionary preparationare the most important steps.
During transcriptpreparation, a text file is prepared in which thecomplete vocabulary of the designed ASR systemis written in Unicode.
The dictionary providespronunciation for the words used in languagemodel.
The pronunciation of a word breaks it into asequence of sub word units that are used in theacoustic model.
The dictionary interface also109supports more than one pronunciation for a singleword.
There are various implementations of adictionary; some load the entire dictionary oninitialization whereas other implementations obtainpronunciation on demand.
Thus dictionary is a filewhich provides a mapping from grapheme tophoneme for a given word.
An example is shownin Table 1.Table 1.
An Example DictionaryFor recording, training and testing purpose wehave designed a common interface as given inFigure 3.SnapshotFigure 3:   Interface for ASR FunctioningFirst we decide the words on which experimentis to be performed and then write these words intothe dictionary.
After completing the transcriptpreparation and dictionary making step we areready for the recording of our transcript.
To recordspeech samples we click the record button of theinterface.
Each word is recorded by specifying adifferent name.
After recording all words the trainbutton is pressed which does the statisticalmodeling.
After this, to recognize the word, wepress the speak button and say the word whicheverwe want to recognize.
Note that the word to berecognized must be available in the dictionary.When a word is recorded it tells us the numberof frames (starting frame index, end frame index,number of frames used) by which word length hasbeen covered.After training, testing is performed.
Duringtesting it will count the right word match anddisplay the accuracy after each word match.We have tested the system for variousparameters and get the following results.4.1 Experiments with Different Number ofTrainingsTwo hundred isolated words of Hindi language arerecorded and trained various numbers of times.Testing of randomly chosen fifty words is madeand the results are as given in figure 4.Figure 4: Accuracy vs. No.
of Training4.2  Experiment with Different VocabularySizesIn this experiment, accuracy of the system wasobserved by varying the size of vocabulary (50words, 70 words, 100 words, 120 words).
Smallerthe size of vocabulary, lesser the chances ofconfusion and hence better should be the accuracy.110This fact is supported by results as shown in thegraph of figure 5.
System is trained five times andtesting of randomly chosen twenty five words ismade.Figure 5.
Accuracy vs. Words4.3 Experiments with Different NoiseEnvironmentsExperiment is performed in various noiseenvironments: in closed room environment wherenoise energy is up to 7dB, in less noisyenvironment where noise energy vary 7dB to12dB, in noisy environment where noise energy isabove 12dB.Figure 6.
Accuracy vs. Environment4.4    Experiments with Different WindowsTo evaluate the effect of different windows such asRectangular window, Hamming window, HanningUsed               AccuracyHamming window                 76 %urplement anSR for Indian language using LPCC for featureas from limiteduous Speechbruary 2001.
Discriminative featurextraction applied to speech recognition In IEEEwindow used in recognition of isolated Hindiwords, a system of hundred isolated word is made.System is trained five times for each window.Twenty five results are made for each windowusing Hidden Markov Model for recognition.Dictionary size is of 100 words.
Results obtainedare given asWindowHanning window                          69 %Rectangular window                 55 %5.
Conclusion and Fut e WorkWe have proposed an approach to imAextraction and HMM to generate speech acousticmodel.
Using our approach we have made aprototype of the recognizer for isolated word,speaker dependent ASR system for Hindi.
Thedesign of our system is such that it can be used togenerate acoustic model for any Indian languageand in a very easy way.
The use of such an ASRsolves the problem of technology acceptance inIndia by bringing human-human interaction closerto human-human interaction.Scope for future work would concentrate onincorporating the featuresvocabulary to large, from isolated word toconnected words or continuous speech, fromspeaker dependent to speaker independent.
Wehave tested our system for Hindi, it can be testedfor other similar Indian languages like Sanskrit,Punjabi etc.
with few modifications.Even in European languages state of the artLVCSR (Large Vocabulary ContinRecognition) systems are very far from the 100%accuracy.
So people are trying new approachesapart from regular LPCC or MFCC at the front endand HMM as acoustic model at the back end.Wavelet and Dynamic Bayesian Network are twopromising approaches that can be used for LVCSRsystems in Indian languages.ReferencesA.E.
Biem.
Fee111.
Missing-Featurepproaches in Speech recognition.
Signaldy.
1976.
Speech Recognition byachine: A Review, IEEE  64(4): 502-531.aturalanguage Understanding, Pearson Education.h &anguage Processing, Pearson Education.odelingechniques in Speech Recognition.
IEEE Proc..ofaussian selection in large vocabulary continuous.
Computer Recognition ofpoken Hindi.
Proceeding of International004.odelling Syllable Duration in Indian Languageson Hiddenarkov Models and Selected Applications inReview of Neuraletworks for Speech Recognition, Massachusettsand J.G.Harris.
2002.reased MFCC Filter Bandwidth For Noise-.
Kumar, A. Verma & N.Rajput.
2004.
A Large.
Karanjanadecha and Stephan A. Zahorian.. Huo, H. Jiang, & C.H.
Lee.
1997.
A Bayesianawrence R. Rabiner  and S. Atal Bishnu.
1976.
Awa, N. Wada, N. Hayasaka, and Y.. Watanabe, K. Shinoda, K. Takagi, and E.Transactions on Acoustics, Speech, and SignalProcessing, vol.
9, pp.
96-108.B.
Raj and Stern.
2005aProcessing magazine, IEEE Volume 22, Issue 5,pp.
101-116.D.
Raj RedMJames Allen.
Third Edition 2005.
NLJ.
H. Martin and Daniel Jurafsky.
2000.
SpeecLJoseph W. Picone.
Sep. 1993.
Signal MTK.M.Knill, M.J.Gales and J.
Young, 1996.
UseGspeech recognition using HMMs, Proc.
IEEEICSLP96, pp 470-473,K.
Samudravijaya.
2000SConference of Speech, Music and Allied SignalProcessing, Triruvananthapuram, pages 8-13.K.S.
Rao, B. Yegnanarayana.
May 2MUsing Neural Networks.
In proceeding of ICASSPMontreal, Qubic, Canada, pp 313-316.Lawrence R. Rabiner.
1989.
A TutorialMSpeech recognition, IEEE.Lippman and P. Richard,NInstitutes of Technology (MIT) LincolnLaboratory, U.S.A.M.
D. SkowronskiIncRobust Phoneme Recognition.
IEEE.Mvocabulary  Continuous  Speech RecognitionSystem   for Hindi.
In IBM Research Lab, vol.48pp.703-  715 .M1999.
Signal Modeling for Isolated WordRecognition, ICAS SP Vol 1, , p 293-296.Qpredictive classification approach to robust speechrecognition, in proc.
IEEE ICASSP, pp.1547-1550.LPattern Recognition Approach to Voice-Unvoiced-Silence Classification with Applications to SpeechRecognition?, IEEE Transaction  ASSP-24(3).S.
YoshizaMiyanaga.
2004.Scalable Architecture for WordHMM-based Speech Recognition, Proc.
IEEEISCAS .TYamada, 1994.
Speech recognition using tree-structured probability density function, Proc.
IEEEICSLP, pp 223-226.112
