Parsing Without (Much) Phrase StructureMichael B. KacDepartment ofLinguisticsUniversity of MinnesotaMinneapolis, MN 55455USAAlexis Manaster-RamerProgram in LinguisticsUniversity of MichiganAnn Arbor, M148109USAApproaches to NL syntax conform in varying degrees to the olderrelational/dependency model, (essentially that assumed intraditional grammar), which treats a sentence as a group of wordsunited by various relations, and the newer constituent model.Some modern approaches have nonetheless involved shifts awayfrom essentially constituent-based models of the sort associatedwith Bloomfield and Chomsky to more relation-based ones (e.g.case grammar, relational grammar, daughter-dependency andword grammar, corepresentational grammar) while some others,notably lexical-functional grammar, have nonetheless continued torely crucially on certain techniques inherited fromconstituency-based grammar, particularly context-free grammar.In computational linguistics there is a strong (if not universal)reliance on phrase structure as the medium via which to representsyntactic structure; call this the CONSENSUS VIEW.
A significantamount of effort has accordingly been invested in techniques bywhich to build such a representation efficiently, which has in turnled to considerable work on the formal and computationalproperties of context-free gramamrs (or natural extensions ofthem) and of the associated languages.
In its strongest form, theconsensus view says that the recovery of a fully specified parsetree is an essential step in computational l nguage processing, andwould, if correct, provide important support for the constituentmodel.
In this paper, we shall critically examine the rationale forthis view, and will sketch (informally) an alternative view whichwe find more defensible.
The actual position we shall take for thisdiscussion, however, is conservative in that we will not argue thatthere is no place whatever for constituent analysis in parsing or insyntactic analysis generally.
What we WILL argue is that phrasestructure is at least partly redundant in that a direct leap to thecomposition of some semantic units is possible from a relativelyunderspecified syntactic representation (as opposed to a completeparse tree).
However, see Rindflesch forthcoming for anapproach to.parsing which entails a much stronger denial of theconsensus view.The rationale for the consensus view consists of four mainpropositions: (i) phrase structure analysis is well entrenched inboth 'pure' and computational linguistics; (ii) phrase structuregrammars are well understood mathematically; (iii) context-freelanguages are provably computationally tractable; (iv) semanticprocessing is either impossible, or at best highly nonoptimal,without a complete parse tree to work with (with the possiblequalification that syntactic and semantic processing might beinterleaved).
We will focus on (ii-iv), (i) amounting to nothingmore than the identification as such of the consensus view.Argument (ii) is compelling if, but only if, one is willing to grantcertain other assumptions.
Since these include the points at issue,namely that phrase structure analysis is in principle adequate to thetask at hand, the argument is circular taken by itself.
With regardto (iii), note that even if NL's (or large tracts of them) arecontext-free, that is SUFFICIENT tO assure that they arecomputationally tractable, but not NECESSARY.
That is, thetractability of a language or sublanguage implies nothing withregard to context-freeness.
1Argument (iv) amounts to saying that the composition of a givensemantic unit can be identified only after the correspondingsyntactic onstituent has been parsed, but this is false.
It is156possible, both in principle and in fact, to recognize at least somesemantic units by operating on an 'impoverished' syntacticrepresentation, i.e.
one which does not yet incorporate anyinformation about he syntactic onstituents corresponding totheunits in question.
The following sentences are offered by way ofillustration: \[1.
John likes Mary 2.
Mary, John likes 3.
I thinkJohn likes Mary 4.
Mary, 1 think John likes \] In these xamples,where all the NP's are single words, it is a trivial matter to assigneach to one of the following schemata: \[1'.
NP 1 P NP 2 2'.NP 1NP 2P  3'.
NP 1P1 NP2P2NP3 4'.
NP 1 NP 2P1NP3P2\] The goal in all four eases is to identify a nonlexical predicateconsisting of likes and Mary and a predication consisting of Johnand the afore-mentioned nonlexical predicate.
In 3-4, thispredication must also be analyzed as a component of a larger one.Under the consensus view, this would require identification ofconstituents of the categories VP or VP/NP prior to recognition ofnonlexical predicates, and the identification of constituents of thecategories S or S/NP prior to the recognition of predications.
Butgiven just the amount of structure in the schemata shown in 1'-4',we can proceed directly to the semantic units as follows.Assuming that processing starts at the left: (a) in a sequence ofthe form NP 1 NP 2 P, leave NP 2 unlabelled; (b) in a sequenceof the form NP P, label the NP as Subject of the P; (c) if no NPappears to the right of a P requiring an NP Object, associate thisfunction with the nearest unlabelled NP to the left.We illustrate with 4.
In either case, at the conclusion of the firstpass, the predication corresponding to the subordinate clause isfully specified and at least the Subject of the predicationcorresponding to the main clause is identified.
On the secondpass, it suffices to search for P's requiring Object complementsand to assign this function to any predication whose own P lies tothe right of such a predicate.
(Discontinuity poses no difficulties,nor is it necessary to make use of auxiliary devices uch as emptycategories to mark the positions of syntactic gaps.)
Further, oncea transitive P and its Object have been identified, these may becomposed into a larger intransitive predicate.A second instructive xample is provided by the problematicalDutch constructions discussed in Bresnan et al 1982.
Theproblem, briefly put, is that there is a class of VP's in Dutchwhich take the form NP n-1 V n but which cannot, apparently, beassigned a center-embedding constituent structure.
Using alexical-functional framework, the authors how that constraints onf-structure can be used as a filter on c-structure which aregenerable by the (context-free) phrase structure component of thegrammar.
If one applies this conception seriously to parsing, thenit follows that what the parser must construct is functionallyannotated parse trees, and yet it is not difficult to see how thefunctional information could be used, much as it was in the earlierexample, to bypass at least some of the steps involved inconslxucting a c-structure.
As an example, consider ... dat JanPier Marie zag helpen zwemmen 'that John saw Piet help Marie toswim'.
One way to look at the problem would be this: imaginethat there is a recursive way of constructing complex verbs out ofsimple verbs such that the complex inherits the arguments of thesimplexes, and that the arguments of the complex must appear in alinear order corresponding to the order of the simplexes withwhich they are associated.
Imagine ful'ther that it is possible tohave rules like \[ 5.
VP -> V" V; 6.
V" -> NP^n V' (UP n OBJDOWN)\] Given a stxing of Object NP's, we would have each ofthem beat" a different relation to the complex verb: the leftmostwould be lOB J, the next leftmost 2OBJ, etc.
There is now nodifficulty coming up with a way to capture the generalization that1OBJ is the OBJ of the first simplex verb, 2OBJ the OBJ oi!
thesecond and so on.
In regard to parsing, we can now see that aslong as there is a way to build up a complex V (we maintain aneutral stance as to how that might be done), then tile compos:,~tionof the semantic unit corresponding to the VP referred to in rule5--and the relations which obtain within it.--can be recoveredwithout actually building the VP constituent of the c-structure.
Aslong as there is a way, somehow, to build up as much structure asis represented in the schema NP NP NP \[V' V V\] V then thefollowing will yield the desired results: (a) leave the initial NPunlabelled on the first pass; (b) for all n _> 2, label the n th NP n- 1OBJ  of V n,In the example under discussion, this will make Piet and Marierespectively IOBJ and 2OBJ of the V' zag..helpen.
The entirepredicate can then be identified by composing the fightmost V withthe expression consisting of the V' and its arguments; by the sametoken, the pairings of arguments of the V' with the appropriatedaughter V's is easily accomplished.
The end result is therecognition of all the f-structures which have to be extracted f?omthe string without prior recognition of either the V" or VPconstituents referred to in the rules (5-6).Our examples are simplified in one respect, namely that theyinvolve no NP's longer than a single word.
It is possible thatsomething mole like phrase structure analysis is required to handlesuch units (as well as the V' referred to in the analysis of theDutch example), though Rindflesch (forthcoming) argues that thisis not the case.
(See also Hudson 1976, 1984.
)Up to this point, we have been concerned with showing that: thecase FOR the consensus view is not especially compelling; wenow proceed to the arguments AGAINST it.
The illustration justgiven actually amounts to an m'gument against since it shows thattile S- or S/NP - mid VP or VP/-NP constituents of a parse trex: aminessential to cue the recognition of predications and nonlexicalpredicates.The arguments up to this point have been concerned with theoutput of a syntactic parser; it needs to be noted as well that thereare some difficulties associated with the idea that a parser operateswith a set of phrase structure rules, or formally similar objects.In Kac et al 1986, it is argued that there are advantages toparsingin a series of graded stages such that at each stage onlay aparticular, restricted type of structural infornlation (e.g.information about relations of subordination among verb!
;) isbeing sought.
A variety of different ypes of information are'compacted' into phrase structure grammars in a way which makes'it difficult to isolate a given type and operate with it independentlyof the others.
While there is nothing in principle to prevent hisinformation from being extracted from a set of PS-rules, theoverhead imposed by the interpretation process makes this anunatn'aetive option.
A preferable stragegy would be to have ahighly structured grammar for the parser to refer to, with ahierarchy of different ypes of information corresponding to thevarious phases via which the entire structural representation isbuilt up.We offer one last example which suggests trongly that phrasestructure analysis is problematical in some cases.
Consider thecoordinate sentence John likes and Bill hates beans.
Oneimmediate observation that we can make is that the sequence Billhates beans would, in isolation, be a sentence, which might inturn lead us to an analysis which, whatever else it might entail,would treat he material to the right of and as an S, coordinated bythe and to some constituent occurring to the left of theconjunction.
An obvious difficulty which stands in the way ofthis conclusion is that there does not appear prima facie to be anyway to treat anything to the left of the and as an S, therebyviolating the widely assumed principle that only like constituentscan be coordinated (the principle of 'categorial harmony').
Fouralternatives thus present themselves: abandon the analysis in favorof one in which the right-conjunct belongs to a category other thanS; abandon the principle of categorial harmony; modify theprinciple of categorial harmony; find some way of analyzing thematerial to tile left of and as an S.The first alternative looks initially most attractive, specially whenseen in the light of the approach to categories originally proposedby Gazdar (1982) and other expositions of GPSG.
We could thusanalyze the example as having the smlcture \[S\[S/NP \[S/NP Johnlikes\] and \[S/NP Bill hates\] bean@ Part of the justification forthis analysis is tile presence of an intonation break directly afterhates that is not present when Bill hates beans is present inisolation.
This move, however, creates two new problems.
Firstof all, it involves a possibly unwarranted intrusion of phonologyinto syntax.
It is one thing to argue that a phrase structure analysiswith purely syntactic motivation serves as an accurate predictor ofwhere intonation breaks will fall, quite another to let the phrasestructure analysis he dictated by phonological considerations (inwhich case the predictions are self-fulfilling).
There is a moreserious difficulty, however, namely that while there is indeed abreak after hates, it is not the major break (which comes directlyafter likes) despite die fact that the analysis places the majorsyntactic boundary at this point.
Full consistency with thephonological facts would require a syntactic analysis like \[S\[S/NPJohn likes\] and \[S \[Bill hates\] beans\]\] We would then run intoproblems with the categories, however, since we would againhave coordination of unlike constituents.
Note, moreover, that itwould not be possible to subsume S and S/NP by an'archicategory' (Sag et al 1985) since the GPSG treatment ofcoordinability depends crucially on the categorical impossibility ofcoordinating X with X/Y (Gazdar 1981).What we have said so far should be enough to make it clear thatfinding a way to analyze an example like the one under discussionin phrase structure terms is not as straightforward a matter as itmight first have appeared to be.
It is conceivable that ways can befound around the difficulties we have mentioned, though onemight reaonably ask whether the effort would be of genuineinterest or whether it would be more in the nature of a holdingaction.
It is, in any case, possible to handle examples like theones under discussion in a straightforward manner withoutattempting a phrase slructure analysis (Kac 1985).Summary:1.
The rationale for phrase structure analysis isuncompelling on both computational and linguistic grounds.2.
A fully specified parse tree is partially redundantinsofar as structural cues for the recovery of semanticinformation ate concerned.3.
Phrase structure rules and allied formalisms do notprovide the optimal way of representing the grammaticalinformation on which a parser depends.4.
Phrase structure analysis is problematical in certaincases.
'\['hese facts imply that alternatives tothe consensus view deserve tobe investigated.157Notei.
There is a deeper difficulty here, namely the presumption thatNL's must be eomputationally tractable.
There is, to ourknowledge, no evidence that this is the case.
While it is undeniablethat humans parse rapidly and effortlessly most of the time, nothingfollows from this fact regarding the computational properties of anyNL taken as a whole.
At most, it shows an understandablepredisposition tocommunicate via easily parsed structures.ReferencesBresnan, J., R.M.
Kaplan, S. Peters, and A. Zaenen.
1982.Cross-serial dependencies in Dutch.
Linguistic Inquiry13.613-635.Gazdar, G. 1981.
Unbounded ependencies and coordinatestructure.
Linguistic Inquiry 12.155-184.--- 1982.
Phrase structure grammar.
In P. Jacobson and G.K.Pullum eds., The Nature of Syntactic Representation.
Dordrecht:Reidel.
131-186.Hudson, R.A. 1976.
Arguments for a NontransformationalGrammar.
Chicago and London: University of Chicago Press.---1984.
Word Grammar.
Oxford: Basil Blaekwell.Kac, M.B.
1985.
Constraints on predicate coordination.Bloomington, IN: Indiana University Linguistics Club.---, T. Rindflesch and K.L.
Ryan.
1986.
Reconnaissance-attackParsing.
This volume.Rindflesch, T. forthcoming.
Doctoral dissertation i preparation,University of Minnesota.Sag, I., G. Gazdar, T. Wasow and S. Weisler.
1985.Coordination and how to distinguish categories.
Natural Languageand Linguistic Theory 3.117-172.158
