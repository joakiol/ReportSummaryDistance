Proceedings of NAACL-HLT 2015, pages 16?20,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsEnhancing Instructor-Student and Student-Student Interactions withMobile Interfaces and SummarizationWencan Luo, Xiangmin Fan, Muhsin Menekse, Jingtao Wang, Diane LitmanUniversity of PittsburghPittsburgh, PA 15260 USA{wel55, xif14, muhsin, jingtaow, dlitman}@pitt.eduAbstractEducational research has demonstrated thatasking students to respond to reflectionprompts can increase interaction between in-structors and students, which in turn can im-prove both teaching and learning especiallyin large classrooms.
However, administer-ing an instructor?s prompts, collecting thestudents?
responses, and summarizing theseresponses for both instructors and studentsis challenging and expensive.
To addressthese challenges, we have developed an ap-plication called CourseMIRROR (Mobile In-situ Reflections and Review with OptimizedRubrics).
CourseMIRROR uses a mobileinterface to administer prompts and collectreflective responses for a set of instructor-assigned course lectures.
After collection,CourseMIRROR automatically summarizesthe reflections with an extractive phrase sum-marization method, using a clustering algo-rithm to rank extracted phrases by student cov-erage.
Finally, CourseMIRROR presents thephrase summary to both instructors and stu-dents to help them understand the difficultiesand misunderstandings encountered.1 IntroductionIn recent years, researchers in education havedemonstrated the effectiveness of using reflectionprompts to improve both instructors?
teaching qual-ity and students?
learning outcomes in domains suchas teacher and science education (Boud et al, 2013;Menekse et al, 2011).
However, administrating aninstructor?s prompts, collecting the students?
reflec-tive responses, and summarizing these responses forinstructors and students is challenging and expen-sive, especially for large (e.g., introductory STEM)and online courses (e.g., MOOCs).
To address thesechallenges, we have developed CourseMIRROR, amobile application1for collecting and sharing learn-ers?
in-situ reflections in large classrooms.
The in-stant on, always connected ability of mobile de-vices makes the administration and collection of re-flections much easier compared to the use of tra-ditional paper-based methods, while the use of anautomatic summarization algorithm provides moretimely feedback compared to the use of manual sum-marization by the course instructor or TA.From a natural language processing (NLP) per-spective, the need in aggregating and displaying re-flections in a mobile application has led us to modifytraditional summarization methods in two primaryways.
First, since the linguistic units of student in-puts range from single words to multiple sentences,our summaries are created from extracted phrasesrather than from sentences.
Phrases are also easyto read and browse, and fit better on small deviceswhen compared to sentences.
Second, based on theassumption that concepts (represented as phrases)mentioned by more students should get more in-structor attention, the phrase summarization algo-rithm estimates the number of students semanticallycovered by each phrase in a summary.
The set ofphrases in a summary and the associated studentcoverage estimates are presented to both the instruc-1CourseMIRROR homepage: http://www.coursemirror.com; free download link in Google PlayStore: https://play.google.com/store/apps/details?id=edu.pitt.cs.mips.coursemirror16tors and the students to help them understand thedifficulties and misunderstandings encountered fromlectures.2 DemonstrationOne key challenge for both instructors and studentsin large classes is how to become aware of the dif-ficulties and misunderstandings that students are en-countering during lectures.
Our demonstration willshow how CourseMIRROR can be used to addressthis problem.
First, instructors use the server side in-terface to configure a set of reflection prompts for anassociated lecture schedule.
Next, students use themobile client to submit reflective responses to eachassigned prompt according to the schedule.
Finally,after each submission deadline, both students and in-structors use CourseMIRROR to review an automat-ically generated summary of the student responsessubmitted for each prompt.
The whole functionalityof CourseMIRROR will be demonstrated using thescenario described below.
In this scenario, Alice isan instructor teaching an introduction to engineeringclass and Bob is one of her students.In order to use CourseMIRROR, Alice first logsin to the server and sets up the lecture schedule anda collection of reflection prompts.Bob can see all the courses he enrolled in af-ter logging into the CourseMIRROR client applica-tion2.
After selecting a course, he can view all thelectures of that course (Fig.
1.a).After each lecture, Bob writes and submits re-flections through the reflection writing interface(Fig.
1.b).
These reflections are transmitted to theserver and stored in the database.
In order to collecttimely and in-situ feedback, CourseMIRROR im-poses submission time windows synchronized withthe lecture schedule (from the beginning of one lec-ture to the beginning of the next lecture, indicatedby an edit icon shown in Fig.
1.a).
In addition, toencourage the students to submit feedback on time,instructors can send reminders via mobile push no-tifications to the students?
devices.After the reflection collection phase for a givenlecture, CourseMIRROR runs a phrase summariza-2Only Android client is provided.
The iOS version is underdevelopment.
Non-Android users now can use an isomorphicweb client, optimized for mobile browsers.tion algorithm on the server side to generate a sum-mary of the reflections for each prompt.
In theCourseMIRROR interface, the reflection promptsare highlighted using a green background, and arefollowed by the set of extracted phrases constitutingthe summary.
The summary algorithm is describedin Section 3; the summary length is controlled bya user-defined parameter and was 4 phrases for theexample in Fig.
1.c.For Bob, reading these summaries (Fig.
1.c) is as-sumed to remind him to recapture the learning con-tent and rethink about it.
It allows him to get anoverview of the peers?
interesting points and confu-sion points for each lecture.
To motivate the studentsto read the summaries, CourseMIRROR highlightsthe phrases (by using light-yellow background) thatwere included or mentioned by the current user.
Thisfunctionality is enabled by the proposed summa-rization technique which tracks the source of eachphrase in the summary (who delivers it).
We hypoth-esize that highlighting the presence of one?s own re-flections in the summaries can trigger the students?curiosity to some extent; thus they would be morelikely to spend some time on reading the summaries.For Alice, seeing both text and student cover-age estimates in the summaries can help her quicklyidentify the type and extent of students?
misunder-standings and tailor future lectures to meet the needsof students.3 Phrase SummarizationWhen designing CourseMIRROR?s summarizationalgorithm, we evaluated different alternatives onan engineering course corpus consisting of hand-written student reflections generated in responseto instructor prompts at the end of each lecture,along with associated summaries manually gener-ated by the course TA (Menekse et al, 2011).
Thephrase summarization method that we incorporatedinto CourseMIRROR achieved significantly betterROUGE scores than baselines including MEAD(Radev et al, 2004), LexRank (Erkan and Radev,2004), and MMR (Carbonell and Goldstein, 1998).The algorithm involves three stages: candidatephrase extraction, phrase clustering, and phraseranking by student coverage (i.e., how many stu-dents are associated with those phrases).17(a) (b) (c)Figure 1: CourseMIRROR main interfaces; a) Lecture list; b) Reflection writing; c) Summary display: the numbersshown in square brackets are the estimated number of students semantically covered by a phrase and a student?s ownphrase is highlighted in yellow.3.1 Candidate Phrase ExtractionTo normalize the student reflections, we use a parserfrom the Senna toolkit (Collobert, 2011) to extractnoun phrases (NPs) as candidate phrases for sum-marization.
Only NP is considered because all re-flection prompts used in our task are asking about?what?, and knowledge concepts are usually repre-sented as NPs.
This could be extended to includeother phrases if future tasks suggested such a need.Malformed phrases are excluded based on Marujoet al (2013) due to the noisy parsers, including sin-gle stop words (e.g.
?it?, ?I?, ?we?, ?there?)
andphrases starting with a punctuation mark (e.g.
?
?t?,?+ indexing?, ???
).3.2 Phrase ClusteringWe use a clustering paradigm to estimate the numberof students who mention a phrase (Fig.
1.c), whichis challenging since different words can be used forthe same meaning (i.e.
synonym, different word or-der).
We use K-Medoids (Kaufman and Rousseeuw,1987) for two reasons.
First, it works with an ar-bitrary distance matrix between datapoints.
Thisgives us a chance to try different distance matrices.Since phrases in student responses are sparse (e.g.,many appear only once), instead of using frequency-based similarity like cosine, we found it more usefulto leverage semantic similarity based on SEMILAR(Rus et al, 2013).
Second, it is robust to noise andoutliers because it minimizes a sum of pairwise dis-similarities instead of squared Euclidean distances.Since K-Medoids picks a random set of seeds to ini-tialize as the cluster centers and we prefer phrases inthe same cluster are similar to each other, the clus-tering algorithm runs 100 times and the result withthe minimal within-cluster sum of the distances isretained.For setting the number of clusters without tun-ing, we adapted the method used in Wan and Yang(2008), by letting K =?V , where K is the numberof clusters and V is the number of candidate phrasesinstead of the number of sentences.3.3 Phrase RankingThe phrase summaries in CourseMIRROR areranked by student coverage, with each phrase itselfassociated with the students who mention it (this en-ables CourseMIRROR to highlight the phrases thatwere mentioned by the current user (Fig.
1.c)).
In or-der to estimate the student coverage number, phrasesare clustered and phrases possessed by the samecluster tend to be similar.
We assume any phrase in acluster can represent it as a whole and therefore thecoverage of a phrase is assumed to be the same asthe coverage of a cluster, which is a union of the stu-dents covered by each phrase in the cluster.
Within acluster, LexRank (Erkan and Radev, 2004) is used toscore the extracted candidate phrases.
Only the topranked phrase in the cluster is added to the output.This process repeats for the next cluster according18to the student coverage until the length limit of thesummary is reached.4 Pilot StudyIn order to investigate the overall usability and effi-cacy of CourseMIRROR, we conducted a semester-long deployment in two graduate-level courses (i.e.,CS2001 and CS2610) during Fall 2014.
These areintroductory courses on research methods in Com-puter Science and on Human Computer Interaction,respectively.
20 participants volunteered for ourstudy; they received $10 for signing up and in-stalling the application and another $20 for complet-ing the study.
Both courses had 21 lectures open forreflections; 344 reflections were collected overall.We used the same reflection prompts as the study byMenekse et al (2011), so as to investigate the impactof mobile devices and NLP on experimental results.Here we only focus on interesting findings from anNLP perspective.
Findings from a human-computerinteraction perspective are reported elsewhere (Fanet al, 2015).Reflection Length.
Students type more wordsthan they write.
The number of words per reflec-tion in both courses using CourseMIRROR is sig-nificantly higher compared to the handwritten re-flections in Menekse?s study (11.6 vs. 9.7, p <0.0001 for one course; 10.9 vs. 9.7, p < 0.0001for the other course) and there is no significant dif-ference between the two CourseMIRROR courses.This result runs counter to our expectation becausetyping is often slow on small screens.
A potentialconfounding factor might be that participants in ourstudy are Computer Science graduate students whileMenekse?s participants are Engineering undergradu-ates at a different university who had to submit thereflection within a few minutes after the lecture.
Weare conducting a larger scale controlled experiment(200+ participants) to further verify this finding.3Questionnaire Ratings.
Students have positiveexperiences with CourseMIRROR.
In the closinglecture of each course, participants were given aLikert-scale questionnaire that included two ques-tions related to summarization (?I often read reflec-3Due to a currently low response rate, we are also deployingCourseMIRROR in another engineering class where about 50out of 68 students regularly submit the reflection feedback.tion summaries?
and ?I benefited from reading thereflection summaries?).
Participants reported posi-tive experiences on both their quantitative and qual-itative responses.
Both questions had modes of 3.7(on a scale of 1-5, ?
= 0.2).
In general, participantsfelt that they benefited from writing reflections andthey enjoyed reading summaries of reflections fromclassmates.
For example, one comment from a freetext answer in the questionnaire is ?It?s interestingto see what other people say and that can teach mesomething that I didn?t pay attention to.?
The partic-ipants also like the idea of highlighting their ownviewpoints in the summaries (Fig.
1.c).
Two ex-ample comments are ?I feel excited when I see mywords appear in the summary.?
and ?Just curiousabout whether my points are accepted or not.
?5 ConclusionOur live demo will introduce CourseMIRROR, amobile application that leverages mobile interfacesand a phrase summarization technique to facili-tate the use of reflection prompts in large class-rooms.
CourseMIRROR automatically producesand presents summaries of student reflections toboth students and instructors, to help them capturethe difficulties and misunderstandings encounteredfrom lectures.
Summaries are produced using acombination of phrase extraction, phrase clusteringand phrase ranking based on student coverage, withthe mobile interface highlighting the students?
ownviewpoint in the summaries and noting the studentcoverage of each extracted phrase.
A pilot deploy-ment yielded positive quantitative as well as quali-tative user feedback across two courses, suggestingthe promise of CourseMIRROR for enhancing theinstructor-student and student-student interactions.In the future, we will examine how the students?
re-sponses (e.g., response rate, length, quality) relate tostudent learning performance.AcknowledgmentsThis research is in-part supported by an RDF fromthe Learning Research and Development Center atthe University of Pittsburgh.
We also thank all theparticipants and anonymous reviewers for insightfulcomments and suggestions.19ReferencesDavid Boud, Rosemary Keogh, David Walker, et al2013.
Reflection: Turning experience into learning.Routledge.Jaime Carbonell and Jade Goldstein.
1998.
The useof mmr, diversity-based reranking for reordering doc-uments and producing summaries.
In Proceedingsof the 21st Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, SIGIR ?98, pages 335?336.Ronan Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In International Conference onArtificial Intelligence and Statistics, number EPFL-CONF-192374.G?unes Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in text sum-marization.
J. Artif.
Int.
Res., 22(1):457?479.Xiangmin Fan, Wencan Luo, Muhsin Menekse, DianeLitman, and Jingtao Wang.
2015.
CourseMIRROR:Enhancing large classroom instructor-student interac-tions via mobile interfaces and natural language pro-cessing.
In Works-In-Progress of ACM Conference onHuman Factors in Computing Systems.
ACM.Leonard Kaufman and Peter Rousseeuw.
1987.
Clus-tering by means of medoids.
Statistical Data Analy-sis Based on the L1-Norm and Related Method, pages405?416.Luis Marujo, M?arcio Viveiros, and Jo?ao Paulo da SilvaNeto.
2013.
Keyphrase cloud generation of broadcastnews.
arXiv preprint arXiv:1306.4606.Muhsin Menekse, Glenda Stump, Stephen J. Krause, andMichelene T.H.
Chi.
2011.
The effectiveness of stu-dents daily reflections on learning in engineering con-text.
In Proceedings of the American Society for Engi-neering Education (ASEE) Annual Conference.Dragomir R. Radev, Hongyan Jing, Ma?gorzata Sty?s,and Daniel Tam.
2004.
Centroid-based summariza-tion of multiple documents.
Inf.
Process.
Manage.,40(6):919?938, November.Vasile Rus, Mihai C Lintean, Rajendra Banjade, Nobal BNiraula, and Dan Stefanescu.
2013.
Semilar: The se-mantic similarity toolkit.
In ACL (Conference SystemDemonstrations), pages 163?168.Xiaojun Wan and Jianwu Yang.
2008.
Multi-documentsummarization using cluster-based link analysis.
InProceedings of the 31st Annual International ACM SI-GIR Conference on Research and Development in In-formation Retrieval, SIGIR ?08.20
