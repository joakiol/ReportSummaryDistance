The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 233?241,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsPrecision Isn?t Everything:A Hybrid Approach to Grammatical Error DetectionMichael Heilman and Aoife Cahill and Joel TetreaultEducational Testing Service660 Rosedale RoadPrinceton, NJ 08541, USA{mheilman,acahill,jtetreault}@ets.orgAbstractSome grammatical error detection methods,including the ones currently used by the Edu-cational Testing Service?s e-rater system (At-tali and Burstein, 2006), are tuned for pre-cision because of the perceived high costof false positives (i.e., marking fluent En-glish as ungrammatical).
Precision, however,is not optimal for all tasks, particularly theHOO 2012 Shared Task on grammatical er-rors, which uses F-score for evaluation.
In thispaper, we extend e-rater?s preposition and de-terminer error detection modules with a large-scale n-gram method (Bergsma et al, 2009)that complements the existing rule-based andclassifier-based methods.
On the HOO 2012Shared Task, the hybrid method performedbetter than its component methods in terms ofF-score, and it was competitive with submis-sions from other HOO 2012 participants.1 IntroductionThe detection of grammatical errors is a challengingproblem that, arguably, requires the use of both lin-guistic knowledge (e.g., in the form of rules or com-plex features) and large corpora for statistical learn-ing.
Additionally, grammatical error detection canbe applied in various scenarios (e.g., automated es-say scoring, writing assistance, language learning),many of which may benefit from task-specific adap-tation or tuning.
For example, one might want totake a different approach when detecting errors forthe purpose of providing feedback than when de-tecting errors to evaluate the quality of writing inan essay.
Thus, it seems desirable to take a flexibleapproach to grammatical error detection that incor-porates multiple, complementary techniques.In this paper, we extend the preposition and de-terminer error detection modules currently used inthe Educational Testing Service?s e-rater automatedessay scoring system (Attali and Burstein, 2006) forthe HOO 2012 Shared Task on grammatical errors(?2).
We refer to this set of modules from e-rater asour ?base system?
(?3).
While the base system usesstatistical methods to learn models of grammaticalEnglish, it also leverages substantial amounts of lin-guistic knowledge in the form of various hand-codedfilters and complex syntactic features.
The base sys-tem is also tuned for high precision at the expenseof recall in order to avoid a high rate of potentiallycostly false positives (i.e., frequent marking of cor-rect English sentences as ungrammatical).We apply the pre-existing base system withoutmodifications but complement it with a large-scalen-gram method (?5) based on work by Bergsma etal.
(2009).
The n-gram method employs very littlelinguistic knowledge and instead relies almost ex-clusively upon corpus statistics.
We also tune theresulting hybrid system with labeled training datain order to maximize the primary evaluation met-ric used in the HOO 2012 Shared Task: balancedF-score, or F1 (?6).
We find that the tuned hybridsystem improves upon the recall and F-score of thebase system.
Also, in the HOO 2012 Shared Task,the hybrid system achieved results that were com-petitive with other submitted grammatical error de-tection systems (?7).2332 Task DefinitionIn this section, we provide a brief overview of theHOO 2012 Shared Task (Dale et al, 2012).
Thetask focuses on prepositions and determiners only,distinguishing the following error types: prepositionselection errors (coded ?RT?
in the data), extraneousprepositions (?UT?
), missing prepositions (?MT?
),determiner selection errors (?RD?
), extraneous de-terminers (?UD?
), and missing determiners (?MD?
).For training and testing data, the shared task usesshort essays from an examination for speakers of En-glish as a foreign language.
The data includes goldstandard human annotations identifying prepositionand determiner errors.
These errors are representedas edits that transform an ungrammatical text intoa grammatical one.
Edits consist of start and endoffsets into the original text and a correction stringthat should replace the original text at the speci-fied offsets.
The offsets differ by error type: wordselection errors include just the word, extraneousword errors include an extra space after the word sothat a blank will result in an appropriate amount ofwhitespace, and missing word errors specify spansof length zero.1There are three subtasks: detection, recognition,and correction.
Each is evaluated according to pre-cision, recall, and F-score according to a set ofgold standard edits produced by human annotation.While the correction subtask requires both correctcharacter offsets and appropriate corrections, the de-tection and recognition subtasks only consider theoffsets.
Detection and recognition are essentially thesame, except that detection allows for loose match-ing of offsets, which permits mismatches betweenthe extraneous use (e.g., UT) and word selection(e.g., RT) error types.
For our submission to theshared task, we chose to tune for the detection sub-task, and we also chose to avoid the correction taskentirely since the interface to the pre-existing basesystem did not give us access to possible corrections.1The offsets for extraneous word errors prior to punctuation,a relatively rare occurrence, include a space before the wordrather than after it.
Our script for converting our system?s outputinto the HOO 2012 format did not account for this, which mayhave decreased recognition performance slightly.3 Base SystemAs our base system, we repurpose a complex sys-tem designed to automatically score student essays(both native and non-native and across a wide rangeof competency levels).
The system is also used togive feedback to essay writers, so precision is fa-vored over recall.
There are three main modules inthe essay-scoring system whose purpose it is to de-tect preposition and determiner errors (as they aredefined in that system).
Many of the details havebeen reported previously (Chodorow and Leacock,2000; Han et al, 2004; Han et al, 2006; Chodorowet al, 2007; Tetreault and Chodorow, 2008), so herewe will only give brief summaries of these modules.It is important to note that this system was runwithout modification.
That is, no training of newmodels or tuning was carried out specifically for theshared task.
In addition, for the two statistical mod-ules, we only had access to the final, boolean deci-sions about whether an error is present or not at aparticular location in text.
That is, we did not haveaccess to confidence scores, and so task-specific tun-ing for F-score was not an option.3.1 Preposition Error DetectionThe base system detects incorrect and extraneousprepositions (Chodorow et al, 2007; Tetreault andChodorow, 2008).
Tetreault and Chodorow (2008)reports approximately 84% precision and 19% re-call on both error types combined when evaluatingthe system on manually annotated non-native text.3.1.1 Incorrect PrepositionsThe module to detect incorrectly used preposi-tions consists of a multiclass logistic regression (i.e.,?Maximum Entropy?)
model of grammatical usage,along with heuristic pre- and post- filters.
The mod-ule works by extracting a set of features from the?context?
around a preposition, generating a distri-bution over possible prepositions using the model ofgrammatical usage, and then flagging an error if thedifference in probability between the text?s originalpreposition and an alternative preposition exceeds acertain threshold.
The probability for any correctionalso needs to exceed another minimum threshold.For this work, we used the pre-existing, manually-set thresholds.234A pre-filter prevents any contexts that containspelling errors from being submitted to the logisticregression model.
The motivation for this is that theNLP components that provide the features for themodel are unreliable on such data, and since the sys-tems favors precision over recall, no attempt is madeto correct prepositions where the system cannot relyon the accuracy of those features.The logistic regression model of correct preposi-tion usage is trained on approximately 82 millionwords from the San Jose Mercury News2 and textsfor 11th to 12th grade reading levels from the Meta-Metrics Lexile corpus, resulting in 7 million prepo-sition contexts.
The model uses 25 types of features:words and part-of-speech tags around the existingpreposition, head verb (or noun) in the precedingVP (or NP), head noun in the following NP, amongothers.
NPs and VPs were detected using chunkingrather than full parsing, as the performance of statis-tical parsers on erroneous text was deemed to be toopoor.A post-filter rules out certain candidates based onthe following heuristics: (1) if the suggested correc-tion is an antonym of the original preposition (e.g.,from vs to), it is discarded; (2) any correction of thebenefactive for is discarded when the head noun ofthe following NP is human (detected as a WordNethyponym of person or group).3.1.2 Extraneous PrepositionsHeuristics are applied to detect common occur-rences of extraneous prepositions in two scenar-ios: (1) accidentally repeated prepositions (e.g., withwith) and (2) insertion of unnecessary prepositionsin plural quantifier constructions (e.g., some of peo-ple).3.2 Determiner Error DetectionThere are two separate components that detect er-rors related to determiners.
The first is a filter-basedmodel that detects determiner errors involving num-ber and person agreement.
The second is a statisticalsystem that supplements the rule-based system anddetects article errors.2The San Jose Mercury News is available from the Linguis-tic Data Consortium (catalog number LDC93T3A).3.2.1 Filter-based systemThe filter-based system combines unsuperviseddetection of a set of possible errors (Chodorow andLeacock, 2000) with hand-crafted filters designedto reduce this set to the largest subset of correctlyflagged errors and the smallest possible numberof false positives.
Chodorow and Leacock (2000)found that low-frequency bigrams (sequences of twolexical categories with a negative log-likelihood) arequite reliable predictors of grammatical errors.
Textis tagged and chunked, and filters that detect likelycases of NP-internal agreement violations are ap-plied.
These filters will mark, for example, a sin-gular determiner followed by a plural noun head andvice versa, or a number disagreement between a nu-meral and the noun it modifies.
This system hasthe ability to take advantage of linguistic knowledge,which contributes to its ability to detect errors withhigh precision.3.2.2 Statistical modelIn addition to the hand-crafted filters describedabove, there is a statistical component that detectsincorrect, missing and extraneous articles (Han etal., 2004; Han et al, 2006).
This component con-sists of a multiclass logistic regression that selectsan appropriate article for every NP from a, an, the,or .
This model is trained on 31.5 million wordsof diverse genres from the MetaMetrics Lexile cor-pus (from 10th to 12th grade reading levels), or 8million NP contexts.
Again, NPs were determinedby chunking.
The model includes various features:words and POS tags around and within the NP, NPhead information including the countability of thehead noun (estimated automatically from large cor-pora), etc.In a cross-validation experiment, the modelachieved approximately 83% accuracy on well-edited text.
In an experiment evaluated on non-native learner text, the model achieved approxi-mately 85% agreement with human annotators.4 Task-Specific Heuristic FilteringThere is not a one-to-one mapping between the def-initions of determiner and preposition errors as usedin the HOO data set and the definitions used in ourbase system.
For example, our base system marks235errors involving every, many and other quantifiers asdeterminer errors, while these are not marked in thecurrent HOO 2012 Shared Task data.To ensure that our system was aligned with theHOO 2012 Shared Task, we automatically extractedlists of the most frequently occurring determinersand prepositions in the HOO training data.
Any RT,UT, RD or UD edit predicted for a word not in thoselists is automatically discarded.
In the training data,this resulted in the removal of 4 of the 463 RT errorsand 98 of the 361 RD errors detected by the basesystem.5 Large-scale n-Gram ModelsIn order to complement the high-precision base sys-tem and increase recall, we incorporate a largescale n-gram model into our full system.
Specifi-cally, we adapt the SUMLM method from Bergsmaet al (2009).
SUMLM creates confusion sets foreach preposition token in an input text and uses theGoogle Web 1T 5-gram Corpus to score each itemin the confusion set.3 We extend SUMLM to sup-port determiners, extraneous use errors, and missingword errors.Consider the case of preposition selection errors.For a preposition token at position i in an input sen-tence w, we compute the following score for eachpossible alternative v, using Eq.
1.4s(w, i, v) =?n=2...5?x?G(w,i,n,v)log(count(x))|G(w, i, n, v)|(1)The function G(w, i, n, v) returns the set of n-grams in w that include the word at position i and3The Google Web 1T 5-gram Corpus is available from theLinguistic Data Consortium (catalog number LDC2006T13).We plan to test other corpora for n-gram counts in future work.4The n-gram approach considers all of the following wordsto be prepositions: to, of, in, for, on, with, at, by, as, from, about,up, over, into, down, between, off, during, under, through,around, among, until, without, along, within, outside, toward,inside, upon, except, onto, towards, besides, beside, and under-neath.
It considers all of the following words to be determiners:a, an, and the.
The sets of possible prepositions and determinersfor the base system are not exactly the same.
Part of speech tagsare not used in the n-gram system except to identify insertionpoints for missing prepositions and determiners.replace that word, wi, with v. For example, if w =Mary and John went at the store to buy milk, n = 4,i = 4, and v = to, then G(w, i, n, v) returns thefollowing 4-grams:?
and John went to?
John went to the?
went to the store?
to the store toThe expression log(count(x)) is the natural loga-rithm of the number of times the n-gram x occurredin the corpus.5 |G(w, i, n, v)| is the number of n-gram count lookups, used to normalize the scores.Note that this normalization factor is not included inthe original SumLM.
When v is an alternative prepo-sition not near the beginning or end of a sentence,|G(w, i, n, v)| = 14 since there are 14 n-gram countlookups in the numerator.
Or, for example, if i = 0,indicating that the preposition occurs at the begin-ning of the sentence, |G(w, i, n, v)| = 4.6Next, we compute the ratio of the score of eachalternative to the score for the original, using Eq.
2.r(w, i, v) =s(w, i, v)s(w, i, wi)(2)We then identify the best scoring alternative, re-quiring that its score be higher than the original (i.e.,r(w, i, v) > 1).
The procedure is the same for deter-miners, except, of course, that the set of alternativesincludes determiners rather than prepositions.To extend the method from Bergsma et al (2009)for extraneous prepositions and determiners, wesimply set v to be a blank and sum over j = 3 .
.
.
5instead.
|G(w, i, n, v)|will then be 12 instead of 14,since bigrams from the original sentence, which be-come unigrams when replacing wi with a blank, areexcluded.To identify positions at which to flag selection orextraneous use errors, we simply scan for words thatmatch an item in our sets of possible prepositionsand determiners.
To extend the method for missing5We use the TrendStream system (Flor, 2012) to retrieve n-gram counts efficiently.6Our n-gram counts do not include start- or end-of-sentencesymbols.
Also, all n-grams are case-normalized with numbersreplaced by a special symbol.236Algorithm 1 tune(W, y, y?
?, ?min):The hill-climbing algorithm for optimizing the n-gram method?s penalty parameters q. W consistsof the training set texts.
y?
is a set of candidate edits.y is a set of gold standard edits.
?
is an initial stepsize, and ?min is a minimum step size.qallbest ?
0scoreallbest ?
eval(qallbest,W,y, y?
)while ?
> ?min doscorebest ?
?
?qbest ?
qallbestfor qtmp ?
perturb(qbest, ?)
doscoretmp ?
eval(qtmp,W,y, y?
)if scoretmp > scorebest thenqbest ?
qtmpscorebest ?
scoretmpend ifend forif scorebest > scoreallbest thenqallbest ?
qbestscoreallbest ?
scorebestelse??
0.5 ?
?end ifend whilereturn qallbestword errors, however, we apply a set of heuristics toidentify potential insertion points.76 TuningThe n-gram approach in ?5 generates a large num-ber of possible edits of different types.
In this sec-tion, we describe how we filter edits using theirscores and how we combine them with edits fromthe base system (?3).As described above, for an alternative v to be con-sidered as a candidate edit, the value of r(w, i, v) inEq.
2 must be greater than a threshold of 1, indicat-ing that the alternative scores higher than the origi-nal word.
However, we observed low precision dur-ing development when including all candidate ed-its and decided to penalize the ratios.
Bergsma etal.
(2009) discuss raising the threshold, which has7The heuristics are based on those used in Gamon (2010)(personal communication).a similar effect.
Preliminary experiments indicatedthat different edits (e.g., extraneous preposition editsand preposition selection edits) should have differ-ent penalties, and we also want to avoid edits withoverlapping spans.
Thus, for each location with oneor more candidate edits, we select the best accordingto Equation 3 and filter out the rest.v?
= argmaxvr(w, i, v)?
penalty(wi, v) (3)penalty(wi, v) is a function that takes the currentword wi and the alternative v and returns one of 6values: qRT for preposition selection, qUT for extra-neous prepositions, qMT for missing prepositions,qRD for determiner selection, qUD for extraneousdeterminers, and qMD for missing determiners.If the value for r(w, i, v?
)?penalty(wi, v?)
doesnot exceed 1, we exclude it from the output.We tune the vector q of all the penalties to op-timize our objective function (F-score, see ?7) onthe training set using the hill-climbing approach de-scribed in Algorithm 1.
The algorithm initializesthe parameter vector to all zeros, and then itera-tively evaluates candidate parameter vectors that re-sult from taking positive and negative steps of size?
in each direction (steps with negative penaltiesare skipped).
The best step is taken if it improvesthe current score, according to the eval function,which returns the training set F-score after filteringbased on the current parameters.8 This process pro-ceeds until there is no improvement.
Then, the stepsize ?
is halved, and the whole process is repeated.The algorithm proceeds as such until the step sizebecomes lower than a specified minimum ?min.When merging edits from the base system and then-gram approach, the hybrid system always prefersedits from the base system if any edit spans overlap,equivalent to including them in Eq.
3 and assigningthem a penalty of ?
?.9 Note that the set of pre-dicted edits y passed as input to the tune algorithm8Our implementation of the tuning algorithm uses the HOO2012 Shared Task?s evalfrag.py module to evaluate the F-score for the error detection subtask.9If the base system produces overlapping edits, we keepthem all.
If there are overlapping edits from the n-gram sys-tem that have the same highest value for the penalized score inEquation 3 and do not overlap with any base system edits, wekeep them all.237textseditseditseditsparametersparametersgold editsbase system -gram systemnfilteringtuningheuristic filteringtraining testingFigure 1: The architecture of the hybrid system.
Differentsteps are discussed in different parts of the paper: ?basesystem?
in ?3, ?n-gram system?
in ?5, ?heuristic filter-ing?
in ?4, and ?tuning?
and ?filtering?
in ?6.includes edits from both the base and n-gram meth-ods.Figure 1 illustrates the processes of training andof producing test output from the hybrid system.7 ResultsTable 1 presents results for the HOO 2012 detec-tion subtask, including errors of all types.
The re-sults here, reproduced from Dale et al (2012), areprior to applying participant-suggested revisions tothe set of gold standard edits.10 We include fourvariations of our approach: the base system (?3, la-beled ?base?
); the n-gram system (?5, labeled ?n-gram?)
by itself, tuned without edits from the basesystem; the hybrid system, tuned with edits from thebase system (?hybrid?
); and a variation of the hy-10After submitting our predictions for the shared task, wenoticed a few minor implementation mistakes in our code re-lated to the conversion of edits from the base system (?3) andthe task-specific heuristic filtering (?4).
We corrected them andretrained our system.
The detection F-scores for the originaland corrected implementations were as follows: 26.45% (orig-inal) versus 26.23% (corrected) for the base system, 30.70%(original) versus 30.45% (corrected) for the n-gram system,35.65% (original) versus 35.24% (corrected) for the hybrid sys-tem, and 31.82% (original) versus 31.45% (corrected) for thehybridindep system.
Except for this footnote, all results in thispaper are for the original system.run P R Fbase 0 52.63 17.66 26.45n-gram ?
25.87 37.75 30.70hybrid 1 33.59 37.97 35.65hybridindep 2 24.88 44.15 31.82UI 8 37.22 43.71 40.20Table 1: Precision, recall, and F-score for the combinedpreposition and determiner error detection subtask forvarious methods, before participant-suggested revisionsto the gold standard were applied.
All values are percent-ages.
Official run numbers are shown in the ?run?
col-umn.
The ?n-gram?
run was not part of our official sub-mission.
For comparison, ?UI?
is the submission, fromanother team, that achieved the highest detection F-scorein the HOO 2012 Shared Task.brid system (?hybridindep?)
with the penalties tunedindependently, rather than jointly, to maximize F-score for detection of each error type.
For compari-son, we also include the best performing run for thedetection subtask in terms of F-score (labeled ?UI?
).We observe that the base and n-gram systems ap-pear to complement each other well for this task: thebase system achieved 26.45% F-score, and the n-gram system achieved 30.70%, while the hybrid sys-tem, with penalties tuned jointly, achieved 35.65%.Table 2 shows further evidence that the two systemshave complementary performance.
We calculate theoverlap between each system?s edits and the goldstandard.
We see that only a small number of editsare predicted by both systems (38 in total, 18 cor-rect and 20 incorrect), and that the base system pre-dicts 62 correct edits that the n-gram method doesnot predict, and similarly the n-gram method pre-dicts 92 correct edits that the base system does notpredict.
The table also verifies that the base systemexhibits high precision (only 68 false positives in to-tal) while the n-gram system is tuned for higher re-call (286 false positives).Not surprisingly, when the n-gram method?spenalties were tuned independently (?hybridindep?
)rather than jointly, the overall score was lower, at31.82% F-score.
However, tuning independentlymight be desirable if one were concerned withperformance on specific error types or if macro-averaged F-score were the objective.The hybrid system performed quite competitively238(1) All models had a UD very strange long shoes made from black skin .
.
.
(2) I think it is a great idea to organise this sort of festival because most of UT people enjoy it.Figure 2: Examples of errors detected by the base system and missed by the n-gram models.
(3) We have to buy for UT some thing.
(4) I am  MD good diffender.Figure 3: Examples of errors detected by the n-gram system and missed by the base model.?
gold /?
gold?
base /?
base ?
base /?
base?
n-gram 18 92 20 266/?
n-gram 62 276 48 ?Table 2: The numbers of edits that overlap in the hybridsystem?s output and the gold standard for the test set.
Thehybrid system?s output is broken down by whether editscame from the base system (?3) or the n-gram method(?5).
The empty cell corresponds to hypothetical editsthat were in neither the gold standard or the system?s out-put (e.g., edits missed by annotators), which we cannotcount.compared to the other HOO 2012 submissions,achieving the 3rd best results out of 14 teams forthe detection and recognition subtasks.
The per-formance of the ?UI?
system was somewhat higher,however, at 40.20% F-score compared to the hybridsystem?s 35.65%.
We speculate that our hybrid sys-tem?s performance could be improved somewhat ifwe also tuned the base system for the task.8 Error AnalysisIt is illustrative to look at some examples of editsthat the base system correctly detects but the n-grammodel does not, and vice versa.
Figure 2 shows ex-amples of errors detected by the base system, butmissed by the n-gram system.
Example (1) illus-trates that the n-gram model has no concept of syn-tactic structure.
The base system, on the other hand,carries out simple processing including POS taggingand chunking, and is therefore aware of at least somelonger-distance dependencies (e.g., a .
.
.
shoes).
Ex-ample (2) shows the effectiveness of the heuris-tics based on quantifier constructions mentioned in?3.1.2.
These heuristics were developed by devel-opers familiar with the kinds of errors that languagelearners frequently make, and are therefore more tar-geted than the general n-gram method.Figure 3 shows examples of errors detected by then-gram system but missed by the base system.
Ex-ample (3) shows an example of where the base sys-tem does not detect the extraneous preposition be-cause it only searches for these in certain quantifierconstructions.
Example (4) contains a spelling error,which confuses the determiner error detection sys-tem.
It has not seen the misspelling often enough tobe able to reliably judge whether it needs an articleor not before it, and so errs on the side of caution.When diffender is correctly spelled as defender, thebase system does detect that there is a missing articlein the sentence.There were a small number of cases where dialectcaused a mismatch between our system?s error pre-dictions and the gold standard.
For example, an ho-tel is not marked as an error in the gold standardsince it is correct in many dialects.
However, it wasalways corrected to a hotel by our system.
Our sys-tem also often corrected determiners before the nouncamp, since in American Standard English it is moreusual to talk about going to Summer Camp ratherthan going to a/the Summer Camp.Although the task was to detect preposition anddeterminer errors in isolation, there was sometimesinterference from other errors in the sentence.
Thisimpacted the task in two ways.
Firstly, in a sentence239with multiple errors, it was sometimes possible tocorrect it in multiple ways, not all of which involvedpreposition or determiner errors.
For example, youcould correct the phrase a women by either chang-ing the a to the, deleting the a entirely or replacingwomen with woman.
The last change would not fallunder the category of determiner error, and so therewas sometimes a mismatch between the correctionspredicted by the system and the gold standard cor-rections.
Secondly, the presence of multiple errorsimpacted the task when a gold standard correctiondepended on another error in the same sentence be-ing corrected in a particular way.
For example, youcould correct I?m really excited to read the book.
asI?m really excited about reading the book., howeverif you add the preposition about without correctingto read this correction results in the sentence becom-ing even more ungrammatical than the original.119 ConclusionIn this paper, we have described a hybrid systemfor grammatical error detection that combines a pre-existing base system, which leverages detailed lin-guistic knowledge and produces high-precision out-put, with a large-scale n-gram approach, which re-lies almost exclusively on simple counting of n-grams in a massive corpus.
Though the base systemwas not tuned at all for the HOO 2012 Shared Task,it performed well in the official evaluation.
The twomethods also complemented each other well: manyof the predictions from one did not appear in the out-put of the other, and the F-score of the hybrid systemwas considerably higher than the scores for the indi-vidual methods.AcknowledgmentsWe thank Martin Chodorow for discussions aboutthe base system, Daniel Blanchard for help with run-ning the base system, Nitin Madnani for discussionsabout the paper and for its title, and Michael Flor forthe TrendStream system.11Many of these cases were addressed in the revised versionof the gold standard data, however we feel that the issue is amore general one and deserves consideration in the design offuture tasks.ReferencesYigal Attali and Jill Burstein.
2006.
Automated Es-say Scoring with e-rater V.2.
Journal of Technol-ogy, Learning, and Assessment, 4(3).
Available fromhttp://www.jtla.org.Shane Bergsma, Dekang Lin, and Randy Goebel.
2009.Web-Scale N-gram Models for Lexical Disambigua-tion.
In Proceedings of the 21st international jointconference on Artifical intelligence, IJCAI?09, pages1507?1512, Pasadena, California.
Morgan KaufmannPublishers Inc.Martin Chodorow and Claudia Leacock.
2000.
An Un-supervised Method for Detecting Grammatical Errors.In Proceedings of the First Meeting of the North Amer-ican Chapter of the Association for ComputationalLinguistics (NAACL), pages 140?147, Seattle, Wash-ington.
Association for Computational Linguistics.Martin Chodorow, Joel Tetreault, and Na-Rae Han.
2007.Detection of Grammatical Errors Involving Preposi-tions.
In Proceedings of the Fourth ACL-SIGSEMWorkshop on Prepositions, pages 25?30, Prague,Czech Republic.
Association for Computational Lin-guistics.Robert Dale, Ilya Anisimoff, and George Narroway.2012.
HOO 2012: A Report on the Preposition andDeterminer Error Correction Shared Task.
In Proceed-ings of the Seventh Workshop on Innovative Use ofNLP for Building Educational Applications, Montreal,Canada.
Association for Computational Linguistics.Michael Flor.
2012.
A fast and flexible archi-tecture for very large word n-gram datasets.Natural Language Engineering, pages 1?33.doi:10.1017/S135132491100034.Michael Gamon.
2010.
Using Mostly Native Data toCorrect Errors in Learners?
Writing.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 163?171, Los An-geles, California.
Association for Computational Lin-guistics.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2004.
Detecting Errors in English Article Usage witha Maximum Entropy Classifier Trained on a Large, Di-verse Corpus.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation(LREC 2004), pages 1625?1628, Lisbon, Portugal.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,12:115?129.
doi:10.1017/S1351324906004190.Joel R. Tetreault and Martin Chodorow.
2008.
TheUps and Downs of Preposition Error Detection in240ESL Writing.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 865?872, Manchester, UK.
Coling2008 Organizing Committee.241
