Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 29?32,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsSyntax-oriented evaluation measures for machine translation outputMaja Popovic?
and Hermann NeyRWTH Aachen UniversityAachen, Germanypopovic,ney@informatik.rwth-aachen.deAbstractWe explored novel automatic evaluationmeasures for machine translation outputoriented to the syntactic structure of thesentence: the BLEU score on the detailedPart-of-Speech (POS) tags as well as theprecision, recall and F-measure obtainedon POS n-grams.
We also introduced F-measure based on both word and POS n-grams.
Correlations between the new met-rics and human judgments were calcu-lated on the data of the first, second andthird shared task of the Statistical MachineTranslation Workshop.
Machine transla-tion outputs in four different Europeanlanguages were taken into account: En-glish, Spanish, French and German.
Theresults show that the new measures cor-relate very well with the human judge-ments and that they are competitive withthe widely used BLEU, METEOR and TERmetrics.1 IntroductionWe proposed several syntax-oriented automaticevaluation measures based on sequences of POStags and investigated how they correlate with hu-man judgments.
The new measures are the POS-BLEU score, i.e.
the BLEU score calculated onPOS tags instead of words, as well as the POSP, thePOSR and the POSF score: precision, recall and F-measure calculated on POS n-grams.
In additionto the metrics based only on POS tags, we investi-gated a WPF score, i.e.
an F-measure which takesinto account both word and POS n-grams.The correlations on the document level werecomputed on the English, French, Spanish andGerman texts generated by various translation sys-tems in the framework of the first (Koehn andMonz, 2006), second (Callison-Burch et al, 2007)and third shared translation task (Callison-Burchet al, 2008).
Preliminary experiments were car-ried out on the data from the first (2006) andthe second task (2007) ?
Spearman?s rank corre-lation coefficients between the adequacy and flu-ency scores and the POSBLEU, POSP, POSR andPOSF scores were calculated.
The POSBLEU andthe POSF score were shown to be the most promis-ing, so that these metrics were submitted to theofficial shared evaluation task 2008.
The resultsof this evaluation showed that these metrics alsocorrelate well on the document level with anotherhuman score, i.e.
the sentence ranking.
However,on the sentence level the results were less promis-ing.
The possible reason for this is the main draw-back of the metrics based on pure POS tags, i.e.neglecting the lexical aspect.
Therefore we alsointroduced a WPF score which takes into accountboth word n-grams and POS n-grams.2 Syntactic-oriented evaluation metricsWe investigated the following metrics oriented onthe syntactic structure of a translation output:?
POSBLEUThe standard BLEU score (Papineni et al,2002) calculated on POS tags instead ofwords;?
POSPPOS n-gram precision: percentage of POS n-grams in the hypothesis which have a coun-terpart in the reference;?
POSRRecall measure based on POS n-grams: per-centage of POS n-grams in the referencewhich are also present in the hypothesis;?
POSFPOS n-gram based F-measure: takes into ac-count all POS n-grams which have a counter-29part, both in the reference and in the hypoth-esis.?
WPFF-measure based both on word and POS n-grams: takes into account all word n-gramsand all POS n-grams which have a counter-part both in the corresponding reference andhypothesis.The prerequisite for all metrics is availability ofan appropriate POS tagger for the target language.It should be noted that the POS tags cannot be onlybasic but must have all details (e.g.
verb tenses,cases, number, gender, etc.
).The n-gram scores as well as the POSBLEUscore are based on fourgrams (i.e.
the value ofmaximal n is 4).
For the n-gram-based measures,two types of n-gram averaging were investigated:geometric mean and aritmetic mean.
Geometricmean is already widely used in the BLEU score, butis also argued not to be optimal because the scorebecomes equal to zero even if only one of the n-gram counts is equal to zero.
However, this prob-lem is probably less critical for POS-based metricsbecause the tag set sizes are much smaller than vo-cabulary sizes.3 Correlations between the new metricsand human judgmentsThe syntax-oriented evaluation metrics were com-pared with human judgments by means of Spear-man correlation coefficients ?.
Spearman?s rankcorrelation coefficient is equivalent to Pearson cor-relation on ranks, and its advantage is that it makesfewer assumptions about the data.
The possiblevalues of ?
range between 1 (if all systems areranked in the same order) and -1 (if all systems areranked in the reverse order).
Thus the higher valueof ?
for an automatic metric, the more similar itis to the human metric.
Correlation coefficientsbetween human scores and three well-known au-tomatic measures BLEU, METEOR and TER werecalculated as well, in order to see how the newmetrics perform in comparison with widely usedmetrics.
The scores were calculated for outputsof translation from Spanish, French and Germaninto English and vice versa.
English and Ger-man POS tags were produced using the TnT tag-ger (Brants, 2000), Spanish texts were annotatedusing the FreeLing analyser (Carreras et al, 2004),and French texts using the TreeTagger1.
In thisway, all references and hypotheses were providedwith detailed POS tags.Experiments on 2006 and 2007 test dataThe preliminary experiments with the new eval-uation metrics were performed on the data fromthe first two shared tasks in order to investigateSpearman correlation coefficients ?
between POS-based evaluation measures and the human scoresadequacy and fluency.
The metrics described inSection 2 (except the WPF score) were calculatedfor all translation outputs.
For each new metric,the ?
coefficient with the adequacy and with thefluency score on the document level were calcu-lated.
Then the results were summarised by aver-aging obtained coefficients over all translation out-puts, and the average correlations are presented inTable 1.2006+2007 adequacy fluencyBLEU 0.590 0.544METEOR 0.598 0.538TER 0.496 0.479POSBLEU 0.642 0.626POSF gm 0.586 0.551am 0.584 0.570POSR gm 0.572 0.576am 0.542 0.544POSP gm 0.551 0.481am 0.531 0.461Table 1: Average system-level correlations be-tween automatic evaluation measures and ade-quacy/fluency scores for 2006 and 2007 test data(gm = geometric mean for n-gram averaging, am= arithmetic mean).Table 1 shows that the new measures havehigh ?
coefficients both with respect to the ade-quacy and to the fluency score.
The POSBLEUscore has the highest correlations, followed by thePOSF score.
Furthermore, the POSBLEU score hashigher correlations than each of the three widelyused metrics, and all the new metrics except thePOSP have higher correlations than the TER.
ThePOSF correlations with the fluency are higher thanthose for the standard metrics, and with the ad-equacy are comparable to those for the METEORand the BLEU score.1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/30Table 2 presents the percentage of the docu-ments for which the particular new metric hashigher correlation than BLEU, METEOR or TER.
Itcan be seen that on the majority of the documentsthe POSBLEU metric outperforms all three stan-dard measures, especially the correlation with thefluency score.
The geometric mean POSF showssimilar behaviour, having higher correlation thanthe standard measures in majority of the cases butslightly less often than the POSBLEU.
The POSRhas higher correlation than the standard measuresin 50-70% of cases, and the POSP score has thelowest percentage, 30-60%.
It can be also seenthat the geometric mean averaging of the n-gramscorrelates better with the human judgments moreoften than the artimetic mean.Experiments on 2008 test dataFor the official shared evaluation task in 2008, thehuman evaluation scores were different ?
the ad-equacy and fluency scores were abandoned beingrather time consuming and often inconsistent, andthe sentence ranking was proposed as one of thehuman evaluation scores: the manual evaluatorswere asked to rank translated sentences relativeto each other.
RWTH participated in this sharedtask with the two most promising metrics accord-ing to the previous experiments, i.e.
POSBLEUand POSF, and the detailed results can be foundin (Callison-Burch et al, 2008).
It was shown thatthese metrics also correlate very well with the sen-tence ranking on the document level.
However,on the sentence level the performance was muchweaker: a percentage of sentence pairs for whichthe human comparison yields the same result asthe comparison using particular automatic metricwas not very high.
We believe that the main rea-son for this is the fact that the metrics based onlyon the POS tags can assign high scores to transla-tions without correct semantic meaning, becausethey are taking into account only syntactic struc-ture without taking into account the actual words.For example, if the reference translation is ?Thissentence is correct?, a translation output ?This treeis high?
would have a POS-based matching scoreof 100%.
Therefore we introduced the WPF score?
an F-measure metrics which counts both match-ing POS n-grams and matching word n-grams.The ?
coefficients for the POSBLEU, POSF andWPF with the sentence ranking averaged over alltranslation outputs are shown in Table 3.
The cor-relations for several known metrics are shown aswell, i.e.
for the BLEU, METEOR and TER alongwith their variants: METEOR-r denotes the vari-ant optimised for ranking, whereas MBLEU andMTER are BLEU and TER computed using theflexible matching as used in METEOR.
It can beseen that the correlation coefficients for all threesyntactic metrics are high.
The POSBLEU scorehas the highest correlation with the sentence rank-ing, followed by POSF and WPF.
All three mea-sures have higher average correlation than MTER,MBLEU and BLEU.
The purely syntactic metricsoutperform also the METEOR scores, whereas theWPF correlations are comparable with those of theMETEOR scores.2008 sentence rankingBLEU 0.526MBLEU 0.504METEOR 0.638METEOR-r 0.603MTER 0.318POSBLEU 0.712POSF gm 0.663am 0.661WPF gm 0.600am 0.628Table 3: Average system-level correlations be-tween automatic evaluation measures and humanranking for 2008 test data.Table 4 presents the percentage of the docu-ments where the particular syntactic metric hashigher correlation with the sentence ranking thanthe particular standard metric.
All syntactic met-rics have higher correlation than the MTER on al-most all documents, and on a large number of doc-uments than the MBLEU score.
The correlationsfor syntactic measures are better than those for theBLEU score for more than 60% of documents.
Asfor the METEOR scores, the syntactic metrics arecomparable (about 50%).4 ConclusionsThe results presented in this article suggest thatthe syntactic information has the potential tostrenghten automatic evaluation metrics, and thereare many possible directions for future work.
Weproposed several syntax-oriented evaluation met-rics based on the detailed POS tags: the POS-BLEU score and POS-n-gram precision, recall and31adequacy fluency2006+2007 BLEU METEOR TER BLEU METEOR TERPOSBLEU 77.3 58.3 75.0 81.8 83.3 83.3POSF gm 72.7 58.3 75.0 63.6 75.0 83.3am 68.2 58.3 75.0 63.6 66.7 68.1POSR gm 63.6 75.0 58.3 68.1 66.7 58.3am 54.5 75.0 58.3 63.6 58.3 50.0POSP gm 63.6 50.0 75.0 45.4 50.0 58.3am 54.5 41.7 66.7 36.4 50.0 58.3Table 2: Percentage of documents from the 2006 and 2007 shared tasks where the particular new metrichas better correlation with adequacy/fluency than the particular standard metric.2008 BLEU MBLEU MTER METEOR METEOR-rPOSBLEU 71.4 85.7 92.8 57.1 64.3POSF am 64.3 78.6 92.8 50.0 50.0gm 64.3 78.6 92.8 57.1 50.0WPF am 57.1 64.3 100 42.8 50.0gm 57.1 64.3 92.8 42.8 50.0Table 4: Percentage of documents from the 2008 shared task where the new metric has better correlationwith the human sentence ranking than the standard metric.F-measure, i.e.
the POSP, POSR, and POSF score.In addition, we introduced a measure which takesinto account both POS tags and words: the WPFscore.
We carried out an extensive analysis ofthe Spearman?s rank correlation coefficients be-tween the syntactic evaluation metrics and the hu-man judgments.
The obtained results showed thatthe new metrics correlate well with human judg-ments, namely the adequacy and fluency scores,as well as the sentence ranking.
The results alsoshowed that the syntax-oriented metrics are com-petitive with the widely used evaluation measuresBLEU, METEOR and TER.
Especially promisingare the POSBLEU and the POSF score.
The cor-relations of the WPF score are slightly lower thanthose of the purely POS based metrics ?
however,this metric has advantage of taking both syntacticand lexical aspect into account.AcknowledgmentsThis work was realised as part of the Quaero Pro-gramme, funded by OSEO, French State agencyfor innovati on.ReferencesThorsten Brants.
2000.
Tnt ?
a statistical part-of-speech tagger.
In Proceedings of the 6th AppliedNatural Language Processing Conference (ANLP),pages 224?231, Seattle, WA.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.
(Meta-)Evaluation of Machine Translation.
In Pro-ceedings of the ACL Workshop on Statistical Ma-chine Translation, pages 136?158, Prague, CzechRepublic, June.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further Meta-Evaluation of Machine Translation.
InProceedings of the Third Workshop on StatisticalMachine Translation, Columbus, Ohio, June.Xavier Carreras, Isaac Chao, Llu?
?s Padro?, and MuntsaPadro?.
2004.
FreeLing: An Open-Source Suite ofLanguage Analyzers.
In Proceedings 4th Interna-tional Conference on Language Resources and Eval-uation (LREC), pages 239?242, Lisbon, Portugal,May.Philipp Koehn and Christof Monz.
2006.
Manual andautomatic evaluation of machine translation betweeneuropean languages.
In Proceedings on the Work-shop on Statistical Machine Translation, New YorkCity, June.Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318,Philadelphia, PA, July.32
