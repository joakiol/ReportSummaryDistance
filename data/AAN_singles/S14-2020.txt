Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 140?144,Dublin, Ireland, August 23-24, 2014.Blinov: Distributed Representations of Words forAspect-Based Sentiment Analysis at SemEval 2014Pavel Blinov, Eugeny KotelnikovVyatka State Humanities University{blinoff.pavel, kotelnikov.ev}@gmail.comAbstractThe article describes our systemsubmitted to the SemEval-2014 taskon Aspect-Based Sentiment Analy-sis.
The methods based on distribut-ed representations of words for theaspect term extraction and aspectterm polarity detection tasks are pre-sented.
The methods for the aspectcategory detection and category po-larity detection tasks are presentedas well.
Well-known skip-grammodel for constructing the distribut-ed representations is briefly de-scribed.
The results of our methodsare shown in comparison with thebaseline and the best result.1 IntroductionThe sentiment analysis became an importantNatural Language Processing (NLP) task in therecent few years.
As many NLP tasks it?s a chal-lenging one.
The sentiment analysis can be veryhelpful for some practical applications.
For ex-ample, it allows to study the users?
opinionsabout a product automatically.Many research has been devoted to the generalsentiment analysis (Pang et al., 2002),(Amine et al., 2013), (Blinov et al., 2013) oranalysis of individual sentences (Yu and Hatzi-vassiloglou, 2003), (Kim and Hovy, 2004),(Wiebe and Riloff, 2005).
Soon it became clearthat the sentiment analysis on the level of awhole text or even sentences is too coarse.
Gen-eral sentiment analysis by its design is not capa-ble to perform the detailed analysis of an ex-pressed opinion.
For example, it cannot correctlydetect the opinion in the sentence ?Great foodbut the service was dreadful!?.
The sentence car-ries opposite opinions on two facets of a restau-rant.
Therefore the more detailed version of thesentiment analysis is needed.
Such a version iscalled the aspect-based sentiment analysis and itworks on the level of the significant aspects ofthe target entity (Liu, 2012).The aspect-based sentiment analysis includestwo main subtasks: the aspect term extractionand its polarity detection (Liu, 2012).
In this arti-cle we describe the methods which address bothsubtasks.
The methods are based on the distribut-ed representations of words.
Such word represen-tations (or word embeddings) are useful in manyNLP task, e.g.
(Turian et al., 2009), (Al-Rfou?
et al., 2013), (Turney, 2013).The remainder of the article is as follows: sec-tion two gives the overview of the data; the thirdsection shortly describes the distributed represen-tations of words.
The methods of the aspect termextraction and polarity detection are presented inthe fourth and the fifth sections respectively.
Theconclusions are given in the sixth section.2 The DataThe organisers provided the train data for restau-rant and laptop domains.
But as it will be clearfurther our methods are heavily dependent onunlabelled text data.
So we additionally collectedthe user reviews about restaurants from tripad-viser.com and about laptops from amazon.com.General statistics of the data are shown in Ta-ble 1.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/140Table 1: The amount of reviews.Domain The amount of reviewsRestaurants 652 055Laptops 109 550For all the data we performed tokenization,stemming and morphological analysis using theFreeLing library (Padr?
and Stanilovsky, 2012).3 Distributed Representations of WordsIn this section we?ll try to give the high levelidea of the distributed representations of words.The more technical details can be found in(Mikolov et al., 2013).It is closely related with a new promising di-rection in machine learning called the deep learn-ing.
The core idea of the unsupervised deeplearning algorithms is to find automatically the?good?
set of features to represent the target ob-ject (text, image, audio signal, etc.).
The objectrepresented by the vector of real numbers iscalled the distributed representation (Ru-melhart et al., 1986).
We used the skip-grammodel (Mikolov et al., 2013) implemented inGensim toolkit (?eh?
?ek and Sojka, 2010).In general the learning procedure is as follows.All the texts of the corpus are stuck together in asingle sequence of sentences.
On the basis of thecorpus the lexicon is constructed.
Next, the di-mensionality of the vectors is chosen (we used300 in our experiments).
The greater number ofdimensions allows to capture more language reg-ularities but leads to more computational com-plexity of the learning.
Each word from the lexi-con is associated with the real numbers vector ofthe selected dimensionality.
Originally all thevectors are randomly initialized.
During thelearning procedure the algorithm ?slides?
withthe fixed size window (it?s algorithm parameterthat was retained by default ?
5 words) along thewords of the sequence and calculates the proba-bility (1) of context words appearance within thewindow based on its central word under review(or more precisely, its vector representation)(Mikolov et al., 2013).?
?
???
Ww wTwwTwIO vvvvwwp IO1 )exp()exp()|(,(1)wherewv  and wv?
are the input and output vectorrepresentations of w;Iw  and Ow  are the currentand predicted words, W  ?
the number of wordsin vocabulary.The ultimate goal of the described process isto get such ?good?
vectors for each word, whichallow to predict its probable context.
All suchvectors together form the vector space wheresemantically similar words are grouped.4 Aspect Term Extraction MethodWe apply the same method for the aspect termextraction task (Pontiki et al., 2014) for both do-mains.
The method consists of two steps: thecandidate selection and the term extraction.4.1 Candidate SelectionFirst of all we collect some statistics about theterms in the train collection.
We analysed twofacets of the aspect terms: the number of wordsand their morphological structure.
The infor-mation about the number of words in a term isshown in Table 2.Table 2: The statistics for the number of wordsin a term.Aspect termDomainRestaurant, % Laptop, %One-word 72.13 55.66Two-word 19.05 32.87Greater 8.82 11.47On the basis of that we?ve decided to processonly single and two-word aspect terms.
From thesingle terms we treat only singular (NN, e.g.staff, rice, texture, processor, ram, insult) andplural nouns (NNS, e.g.
perks, bagels, times,dvds, buttons, pictures) as possible candidates,because they largely predominate among theone-word terms.
All conjunctions of the formNN_NN (e.g.
sea_bass, lotus_leaf, chicken_dish,battery_life, virus_protection, custom-er_disservice) and NN_NNS (e.g.
sushi_places,menu_choices, seafood_lovers, usb_devices, re-covery_discs, software_works) were candidatesfor the two-word terms also because they aremost common in two-word aspect terms.4.2 Term ExtractionThe second step for the aspect term identificationis the term extraction.
As has already been toldthe space (see Section 3) specifies the wordgroups.
Therefore the measure of similarity be-tween the words (vectors) can be defined.
ForNLP tasks it is often the cosine similarity meas-ure.
The similarity between two vectors),...,( 1 naaa ??
and ),...,( 1 nbbb ??
is given by(Manning et al., 2008):141???????
?ni ini ini iibaba12121)cos(?, (2)where ?
?
the angle between the vectors, n ?
thedimensionality of the space.In case of the restaurant domain the categoryand aspect terms are specified.
For each categorythe seed of the aspect terms can be automaticallyselected: if only one category is assigned for atrain sentence then all its terms belong to it.Within each set the average similarity betweenthe terms (the threshold category) can be found.For the new candidate the average similaritieswith the category?s seeds are calculated.
If it isgreater than the threshold of any category thanthe candidate is marked as an aspect term.Also we?ve additionally applied some rules:?
Join consecutive terms in a single term.?
Join neutral adjective ahead the term (seeSection 5.2 for clarification about the neu-tral adjective).?
Join fragments matching the pattern: <anaspect term> of <an aspect term>.In case of the laptop domain there are no spec-ified categories so we treated all terms as theterms belonging to one general category.
And thesame procedure with candidates was performed.4.3 Category DetectionFor the restaurant domain there was also the as-pect category detection task (Ponti-ki et al., 2014).Since each word is represented by a vector,each sentence can be cast to a single point as theaverage of its vectors.
Further average point foreach category can be found by means of the sen-tence points.
Then for an unseen sentence theaverage point of its word vectors is calculated.The category is selected by calculating the dis-tances between all category points and a newpoint and by choosing the minimum distance.4.4 ResultsThe aspect term extraction and the aspect catego-ry detection tasks were evaluated with Precision,Recall and F-measure (Pontiki et al., 2014).
TheF-measure was a primary metric for these tasksso we present only it.The result of our method ranked 19 out of 28submissions (constrained and unconstrained) forthe aspect term extraction task for the laptop do-main and 17 out of 29 for the restaurant domain.For the category detection task (restaurant do-main) the method ranked 9 out of 21.Table 3 shows the results of our method(Bold) for aspect term extraction task in compar-ison with the baseline (Pontiki et al., 2014) andthe best result.
Analogically the results for theaspect category detection task are presented inTable 4.Table 3: Aspect term extraction results(F-measure).Laptop RestaurantBest 0.7455 0.8401Blinov 0.5207 0.7121Baseline 0.3564 0.4715Table 4: Aspect category detection results(F-measure).RestaurantBest 0.8858Blinov 0.7527Baseline 0.63895 Polarity Detection MethodOur polarity detection method also exploits thevector space (from Section 3) because the emo-tional similarity between words can be traced init.
As with the aspect term extraction method wefollow two-stage approach: the candidate selec-tion and the polarity detection.5.1 Candidate SelectionAll adjectives and verbs are considered as thepolarity term candidates.
The amplifiers and thenegations have an important role in the processof result polarity forming.
In our method we tookinto account only negations because it stronglyaffects the word polarity.
We?ve joined into oneunit all text fragments that match the followingpattern: not + <JJ | VB>.5.2 Term Polarity DetectionAt first we manually collected the small etalonsets of positive and negative words for each do-main.
Every set contained 15 words that clearlyidentify the sentiment.
For example, for the posi-tive polarity there were words such as: great,fast, attentive, yummy, etc.
and for the negativepolarity there were words like: terrible, ugly,not_work, offensive, etc.By measuring the average similarity for a can-didate to the positive and the negative seedwords we decided whether it is positive (+1) ornegative (?1).
Also we set up a neutral thresholdand a candidate?s polarity was treated as neutral(0) if it didn?t exceed the threshold.142For each term (within the window of 6 words)we were looking for its closest polarity term can-didate and sum up their polarities.
For the finaldecision about the term?s polarity there weresome conditions:?
If sum > 0 then positive.?
If sum < 0 then negative.?
If sum == 0 and all polarity terms are neu-tral then neutral else conflict.5.3 Category Polarity DetectionBy analogy with the category detection method,using the train collection, we calculate the aver-age polarity points for each category, i.e.
therewere 5?4 such points (5 categories and 4 valuesof polarity).
Then a sentence was cast to a pointas the average of all its word-vectors.
And clos-est polarity points for the specified categoriesdefined the polarity.5.4 ResultsThe results of our method (Bold) for the polaritydetection tasks are around the baseline results forthe Accuracy measure (Tables 5, 6).Table 5: Aspect term polarity detection results(Accuracy).Laptop RestaurantBest 0.7049 0.8095Blinov 0.5229 0.6358Baseline 0.5107 0.6428Table 6: Category polarity detection results(Accuracy).RestaurantBest 0.8293Blinov 0.6566Baseline 0.6566However the test data is skewed to the positiveclass and for that case the Accuracy is a poorindicator.
Because of that we also show macro F-measure results for our and baseline methods(Tables 7, 8).Table 7: Aspect term polarity detection results(F-measure).Laptop RestaurantBlinov 0.3738 0.4334Baseline 0.2567 0.2989Table 8: Category polarity detection results(F-measure).RestaurantBlinov 0.5051Baseline 0.3597From that we can conclude that our method ofthe polarity detection more delicately deals withthe minor represented classes than the baselinemethod.6 ConclusionIn the article we presented the methods for twomain subtasks for aspect-based sentiment analy-sis: the aspect term extraction and the polaritydetection.
The methods are based on the distrib-uted representation of words and the notion ofsimilarities between the words.For the aspect term extraction and categorydetection tasks we get satisfied results which areconsistent with our cross-validation metrics.
Un-fortunately for the polarity detection tasks theresult of our method by official metrics are low.But we showed that the proposed method is notso bad and is capable to deal with the skeweddata better than the baseline method.AcknowledgmentsWe would like to thank the organizers and thereviewers for their efforts.ReferenceAbdelmalek Amine, Reda Mohamed Hamou andMichel Simonet.
2013.
Detecting Opinions inTweets.
International Journal of Data Mining andEmerging Technologies, 3(1):23?32.Christopher Manning, Prabhakar Raghavan and Hin-rich Sch?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, New York,NY, USA.Bo Pang, Lillian Lee and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 79?86.Bing Liu.
2012.
Sentiment Analysis and OpinionMining.
Synthesis Lectures on Human LanguageTechnologies.Pavel Blinov, Maria Klekovkina, Eugeny Kotelnikovand Oleg Pestov.
2013.
Research of lexical ap-proach and machine learning methods for senti-ment analysis.
Computational Linguistics and In-tellectual Technologies, 2(12):48?58.143Janyce Wiebe and Ellen Riloff.
2005.
Creating sub-jective and objective sentence classifiers from un-annotated texts.
In Proceedings of the 6th Interna-tional Conference on Computational Linguisticsand Intelligent Text Processing, pages 486?497.Joseph Turian, Lev Ratinov, Yoshua Bengio and DanRoth.
2009.
A preliminary evaluation of word rep-resentations for named-entity recognition.
In Pro-ceedings of NIPS Workshop on Grammar Induc-tion, Representation of Language and LanguageLearning.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-rado and Jeffrey Dean.
2013.
Distributed Represen-tations of Words and Phrases and their Composi-tionality.
In Proceedings of NIPS, pages 3111?3119.Llu?s Padr?
and Evgeny Stanilovsky.
2012.
FreeLing3.0: Towards Wider Multilinguality.
In Proceed-ings of the Language Resources and EvaluationConference, LREC 2012, pages 2473?2479.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
In Proceedings of the20th International Conference on ComputationalLinguistics, COLING-2004.Maria Pontiki, Dimitrios Galanis, John Pavlopoulos,Harris Papageorgiou, Ion Androutsopoulos andSuresh Manandhar.
2014.
SemEval-2014 Task 4:Aspect Based Sentiment Analysis.
In Proceedingsof the 8th International Workshop on SemanticEvaluation, SemEval 2014, Dublin, Ireland.Peter Turney.
2013.
Distributional semantics beyondwords: Supervised learning of analogy and para-phrase.
Transactions of the Association for Compu-tational Linguistics, 1:353-366.Radim ?eh?
?ek and Petr Sojka.
2010.
SoftwareFramework for Topic Modelling with Large Cor-pora.
In Proceedings of the LREC 2010 Workshopon New Challenges for NLP Frameworks, pages46?50.Rami Al-Rfou?, Bryan Perozzi, Steven Skiena.
2013.Polyglot: Distributed Word Representations forMultilingual NLP.
In Proceedings of Conferenceon Computational Natural Language Learning,CoNLL?2013.David Rumelhart, Geoffrey Hintont, Ronald Wil-liams.
1986.
Learning representations by back-propagating errors.
Nature.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separatingfacts from opinions and identifying the polarity ofopinion sentences.
In Proceedings of the 2003Conference on Empirical Methods in Natural Lan-guage Processing, pages 129?136.144
