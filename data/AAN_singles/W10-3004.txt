Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 26?31,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsA Hedgehop over a Max-Margin Framework Using Hedge CuesMaria GeorgesculISSCO, ETI, University of Geneva40 bd.
du Pont-d'ArveCH-1211 Geneva 4maria.georgescul@unige.chAbstractIn this paper, we describe the experimentalsettings we adopted in the context of the 2010CoNLL shared task for detecting sentencescontaining uncertainty.
The classification resultsreported on are obtained using discriminativelearning with features essentially incorporatinglexical information.
Hyper-parameters are tunedfor each domain: using BioScope training datafor the biomedical domain and Wikipediatraining data for the Wikipedia test set.
Byallowing an efficient handling of combinations oflarge-scale input features, the discriminativeapproach we adopted showed highly competitiveempirical results for hedge detection on theWikipedia dataset: our system is ranked as thefirst with an F-score of 60.17%.1 Introduction and related workOne of the first attempts in exploiting a SupportVector Machine (SVM) classifier to selectspeculative sentences is described in Light et al(2004).
They adopted a bag-of-wordsrepresentation of text sentences occurring inMEDLINE abstracts and reported on preliminaryresults obtained.
As a baseline they used analgorithm based on finding speculative sentencesby simply checking whether any cue (from agiven list of 14 cues) occurs in the sentence to beclassified.Medlock and Briscoe (2007) also used singlewords as input features in order to classifysentences from scientific articles in biomedicaldomain as speculative or non-speculative.
In afirst step they employed a weakly supervisedBayesian learning model in order to derive theprobability of each word to represent a hedgecue.
In the next step, they perform featureselection based on these probabilities.
In the laststep a classifier trained on a given number ofselected features was applied.
Medlock andBriscoe (2007) use a similar baseline as the oneadopted by Light et al (2004), i.e.
a na?vealgorithm based on substring matching, but witha different list of terms to match against.
Theirbaseline has a recall/precision break-even pointof 0.60, while their system improves theaccuracy to a recall/precision break-even point of0.76.
However Medlock and Briscoe (2007) notethat their model is unsuccessful in identifyingassertive statements of knowledge paucity whichare generally marked rather syntactically thanlexically.Kilicoglu and Bergler (2008) proposed a semi-automatic approach incorporating syntactic andsome semantic information in order to enrich orrefine a list of lexical hedging cues that are usedas input features for automatic detection ofuncertain sentences in the biomedical domain.They also used lexical cues and syntacticpatterns that strongly suggest non-speculativecontexts (?unhedges?).
Then they manuallyexpanded and refined the set of lexical hedgingand ?unhedging?
cues using conceptual semanticand lexical relations extracted from WordNet(Fellbaum, 1998) and the UMLS SPECIALISTLexicon (McCray et al 1994).
Kilicoglu andBergler (2008) did experiments on the samedataset as Medlock and Briscoe (2007) and theirexperimental results proved that theclassification accuracy can be improved byapproximately 9% (from an F-score of 76% to anF-score of 85%) if syntactic and semanticinformation are incorporated.The experiments run by Medlock (2008) onthe same dataset as Medlock and Briscoe (2007)show that adding features based on part-of-speech tags to a bag-of-words inputrepresentation can slightly improve the accuracy,but the ?improvements are marginal and notstatistically significant?.
Their experimentalresults also show that stemming can slightly26Dataset#sentences%uncertainsentences#distinctcues#ambiguouscuesPRFWikipedia training 11111 22% 1912 0.32 0.96 0.48Wikipedia test 9634 23% - 188 0.45 0.86 0.59BioScope training  14541 18% 168 0.46 0.99 0.63BioScope test  5003 16% - 96 0.42 0.98 0.59Table 1: The percentage of ?uncertain?
sentences (% uncertain sentences) given the total number ofavailable sentences (#sentences) together with the number of distinct cues in the training corpus andthe performance of the baseline algorithm based on the list of cues extracted from the training corpus.improve the classification accuracy, while usingbigrams brings a statistically significantimprovement over a simple bag-of-wordsrepresentation.
However, Medlock (2008)illustrates that ?whether a particular term acts asa hedge cue is quite often a rather subtle functionof its sense usage, in which case the distinctionsmay well not be captured by part-of-speechtagging?.M?ra et al (2009) also used a machinelearning framework based on lexical inputfeatures and part-of-speech tags.
Other recentwork on hedge detection (Ganter and Strube,2009; Marco and Mercer, 2004; Mercer et al,2004; Morante and Daelemans, 2009a; Szarvas,2008) relied primarily on word frequencies asprimary features including various shallowsyntactic or semantic information.The corpora made available in the CoNLLshared task (Farkas et al, 2010; Vincze et al,2008) contains multi-word expressions that havebeen annotated by linguists as cue words tendingto express hedging.
In this paper, we test whetherit might suffice to rely on this list of cues alonefor automatic hedge detection.
The classificationresults reported on are obtained using supportvector machines trained with features essentiallyincorporating lexical information, i.e.
featuresextracted from the list of hedge cues providedwith the training corpus.In the following, we will first describe somepreliminary considerations regarding the resultsthat can be achieved using a na?ve baselinealgorithm (Section 2).
Section 3 summarizes theexperimental settings and the input featuresadopted, as well as the experimental results weobtained on the CoNLL test data.
We also reporton the intermediate results we obtained whenonly the CoNLL training dataset was available.In Section 4, we conclude with a briefdescription of the theoretical and practicaladvantages of our system.
Future researchdirections are mentioned in Section 5.2 Preliminary Considerations2.1 BenchmarkingAs a baseline for our experiments, we consider anaive algorithm that classifies as ?uncertain?
anysentence that contains a hedge cue, i.e.
any of themulti-word expressions labeled as hedge cues inthe training corpus.Table 1 shows the results obtained when usingthe baseline na?ve algorithm on the CoNLLdatasets provided for training and test purposes1.The performance of the baseline algorithm isdenoted by Precision (P), Recall (R) and F-score(F) measures.
The first three columns of the tableshow the total number of available sentencestogether with the percentage of ?uncertain?sentences occurring in the dataset.
The fourthcolumn of the table shows the total number ofdistinct hedge cues extracted from the trainingcorpus.
Those hedge cues occurring in ?certain?sentences are denoted as ?ambiguous cues?.
Thefifth column of the table shows the number ofdistinct ambiguous cues.As we observe from Table 1, the baselinealgorithm has very high values for the recallscore on the BioScope corpus (both training andtest data).
The small percentage of falsenegatives on the BioScope test data reflects thefact that only a small percentage of ?uncertain?sentences in the reference test dataset do notcontain a hedge cue that occurs in the trainingdataset.The precision of the baseline algorithm hasvalues under 0.5 on all four datasets (i.e.
on bothBioScope and Wikipedia data).
This illustratesthat ambiguous hedge cues are frequently used in?certain?
sentences.
That is, the baselinealgorithm has less true positives than false1In Section 3.2, we provide the performance of the baselinealgorithm obtained when only the CoNLL training datasetwas available.
When we tuned our system, we obviouslyhad available only the results provided in Table 2 (Section3.2).27positives, i.e.
more than 50% of the sentencescontaining a hedge cue are labeled as ?certain?
inthe reference datasets.2.2 Beyond bag-of-wordsIn order to verify whether simply the frequenciesof all words (except stop-words) occurring in asentence might suffice to discriminate between?certain?
and ?uncertain?
sentences, weperformed preliminary experiments with a SVMbag-of-words model.
The accuracy of this systemis lower than the baseline accuracy on bothdatasets (BioScope and Wikipedia).
For instance,the classifier based on a bag-of-wordsrepresentation obtains an F-score ofapproximately 42% on Wikipedia data, while thebaseline has an F-score of 49% on the samedataset.
Another disadvantage of using a bag-of-words input representation is obviously the largedimension of the system?s input matrix.
Forinstance, the input matrix representation of theWikipedia training dataset would haveapproximately 11111 rows and over 150000columns which would require over 6GB of RAMfor a non-sparse matrix representation.3 System Description3.1 Experimental SettingsIn our work for the CoNLL shared task, we usedSupport Vector Machine classification (Fan etal., 2005; Vapnik, 1998) based on the GaussianRadial Basis kernel function (RBF).
We tunedthe width of the RBF kernel (denoted by gamma)and the regularization parameter (denoted by C)via grid search over the following range ofvalues: {2-8, 2-7, 2-6, ?24} for gamma and {1,10..200 step 10, 200..500 step 100} for C.During parameter tuning, we performed 10-foldcross validation for each possible value of theseparameters.
Since the training data areunbalanced (e.g.
18% of the total number ofsentences in the BioScope training data arelabeled as ?uncertain?
), for SVM training weused the following class weights:?
0.1801 for the ?certain?
class and 0.8198for the ?uncertain?
class on the BioScopedataset;?
0.2235 for the ?certain?
class and 0.7764for the ?uncertain?
class on theWikipedia dataset.The system was trained on the training setprovided by the CoNLL shared task organizersand tested on the test set provided.
As inputfeatures in our max-margin framework, wesimply used the frequency of each hedge cueprovided with the training corpus in eachsentence.
We also used as input features duringthe tuning phase of our system 2-grams and 3-grams extracted from the list of hedge cuesprovided with the training corpus.3.2 Classification resultsFigure 1: Contour plot of the classification errorlandscape resulting from a grid search over arange of values of {2-8, 2-7, 2-6, 2-5, 2-4} for thegamma parameter and a range of values of {10,20, ?, 110} for the C parameter on Wikipediadata.Figure 2: Contour plot of the classification errorlandscape resulting from a grid search over arange of values of {2-8, 2-7, 2-6, ?2-2} for thegamma parameter and a range of values of {1,10, 20, 30, ?110} for the C parameter onBioScope data.Figure 1 shows the variability of hedgedetection results on Wikipedia training datawhen changing the RBF-specific kernelparameter and the regularization parameter C.The contour plot shows that there are threeregions (represented in the figure by the darkestlandscape color) for parameter values where thecross validation error is lower than 18.2%.
Oneof these optimal settings for parameter valueswas used for the results submitted to the CoNLLshared task and we obtained an F-score of60.17%.
When the CoNLL test data containing28Table 2: The performance of our system corresponding to the best parameter values.
The performanceis denoted in terms of true positives (TP), false positives (FP), false negatives (FN), precision (P),recall (R) and F-score (F).the reference labels were made available, we alsodid tests with our systemusing the other two optimal settings forparameter values.The optimal classification results on theWikipedia dataset were obtained for a gammavalue equal to 0.0625 and for a C value equal to10, corresponding to a cross validationclassification error of 17.94%.
The modelperformances corresponding to these bestparameter values are provided in Table 2.
The P,R, F-score values provided in Table 2 aredirectly comparable to P, R, F-score values givenin Table 1 since exactly the same datasets wereused during the evaluation.The SVM approach we adopted shows highlycompetitive empirical results for weaseldetection on the Wikipedia test dataset in thesense that our system was ranked as the first inthe CoNLL shared task.
However, the baselinealgorithm described in Section 2 proves to berather difficult to beat given its F-scoreperformance of 59% on the Wikipedia test data.This provides motivation to consider otherrefinements of our system.
In particular, webelieve that it might be possible to improve therecall of our system by enriching the list of inputfeatures using a lexical ontology in order toextract synonyms for verbs, adjectives andadverbs occurring in the current hedge cue list.Figure 2 exemplifies the SVM classificationresults obtained during parameter tuning onBioScope training data.
The optimalclassification results on the BioScope datasetwere obtained for gamma equal to 0.0625and Cequal to 110, corresponding to a cross validationclassification error of 3.73%.
The modelperformance corresponding to the best parametersettings is provided in Table 2.
Our systemobtained an F-score of 0.78 on the BioScope testdataset while the best ranked system in theCoNLL shared task obtained an F-score of 0.86.In order to identify the weaknesses of our systemin this domain, in Subsection 3.2 we will furnishthe intermediate results we obtained on theCoNLL training set.The system is platform independent.
We ranthe experiments under Windows on a Pentium 4,3.2GHz with 3GB RAM.
The run timesnecessary for training/testing on the wholetraining/test dataset are provided in Table 2.Table 3 shows the approximate intervals oftime required for running SVM parameter tuningvia grid search on the entire CoNLL trainingdatasets.Dataset Range of values RuntimeWikipediatraining data{2-8,2-7, ?2-1} forgamma;{10, 20, ?110} forC13 hoursBioScopetraining data{2-8,2-7, ?2-2} forgamma;{10, 20, ?110} forC4 hoursTable 3 : Approximate run times for parametertuning via 10-fold cross validation3.3 Intermediate resultsIn the following we discuss the results obtainedwhen the system was trained on approximately80% of the CoNLL training corpus and theremaining 20% was used for testing.
The 80% ofthe training corpus was also used to extract thelist of hedge cues that were considered as inputfeatures for the SVM machine learning system.The BioScope training corpus provided inCoNLL shared task framework contains 11871sentences from scientific abstracts and 2670sentences from scientific full articles.In a first experiment, we only used sentencesfrom scientific abstracts for training and testing:we randomly selected 9871 sentences for trainingand the remaining 2000 sentences were used fortesting.
The results thus obtained are shown inTable 4 on the second line of the table.Dataset TP FP FN P R F Run TimeWikipedia training 1899 1586 585 0.5449 0.7644 0.6362 49.1 secondsWikipedia test 1213 471 1021 0.7203 0.5429 0.6191 21.5 secondsBioScope training 2508 515 112 0.8296 0.9572 0.8888 19.5 secondsBioScope test 719 322 71 0.6907 0.9101 0.7854 2.6 seconds29Table 4: Performances when considering separately the dataset containing abstracts only and thedataset containing articles from BioScope corpus.
The SVM classifier was trained with gamma = 1and c=10.
Approximately 80% of the CoNLL train corpus was used for training and 20% of the traincorpus was held out for testing.Second, we used only the available 2670sentences from scientific full articles.
Werandomly split this small dataset into a trainingset of 2170 sentences and test set of 500sentences.Third, we used the entire set of 14541sentences (composing scientific abstracts and fullarticles) for training and testing: we randomlyselected 11541 sentences for training and theremaining 3000 sentences were used for testing.The results obtained in this experiment areshown in Table 4 on the fourth line.We observe from Table 3 a difference of 10%between the F-score obtained on the datasetcontaining abstracts and the F-score obtained onthe dataset containing full articles.
Thisdifference in accuracy might simply be due to thefact that the available abstracts training dataset isapproximately 5 times larger than the full articlestraining dataset.
In order to check whether thisdifference in accuracy is only attributable to thesmall size of the full articles dataset, we furtheranalyze the learning curve of SVM on theabstracts dataset.To measure the learning curve, we randomlyselected from the abstracts dataset 2000sentences for testing.
We divided the remainingsentences into 10 parts, we used two parts fortraining, then we increased the size of thetraining dataset by one part incrementally.
Weshow the results obtained in Figure 3.
The x-axisshows the number of sentences used for trainingdivided by 1000.
We observe that the F-score onthe test dataset changed only slightly when morethan 4/10 of the training data (i.e.
more than4800 sentences) were used for training.
We alsoobserve that using 2 folds for training (i.e.approximately 2000 sentences) gives an F-scoreof around 87% on the held-out test data.Therefore, using a similar amount of trainingdata for BioScope abstracts as used for BioScopefull articles, we still have a difference of 8%between the F-score values obtained.
That is, oursystem is more efficient on abstracts than on fullarticles.Figure 3: The performance of our system whenwe used for training various percentages of theBioScope training dataset composed of abstractsonly.4 ConclusionsOur empirical results show that our approachcaptures informative patterns for hedge detectionthrough the intermedium of a simple low-levelfeature set.Our approach has several attractive theoreticaland practical properties.
Given that the systemformulation is based on the max-marginframework underlying SVMs, we can easilyincorporate other kernels that induce a featurespace that might better separate the data.Furthermore, SVM parameter tuning and theprocess of building the feature vector matrix,which are the most time and resource consuming,can be easily integrated in a distributedenvironment considering either cluster-basedcomputing or a GRID technology (Wegener etal., 2007).From a practical point of view, the key aspectsof our proposed system are its simplicity andflexibility.
Additional syntactic and semanticSVM Baseline Dataset content #sentencesused fortraining#sentencesused fortestP R F P R FAbstracts only 9871 2000 0.85 0.94 0.90 0.49 0.97 0.65Full articles only 2170 500 0.72 0.87 0.79 0.46 0.91 0.61Abstracts andfull articles11541 3000 0.81 0.92 0.86 0.47 0.98 0.6430based features can easily be added to the SVMinput.
Also, the simple architecture facilitates thesystem?s integration in an information retrievalsystem.5 Future workThe probabilistic discriminative model we haveexplored appeared to be well suited to tackle theproblem of weasel detection.
This providesmotivation to consider other refinements of oursystem, by incorporating syntactic or semanticinformation.
In particular, we believe that therecall score of our system can be improved byidentifying a list of new potential hedge cuesusing a lexical ontology.ReferencesRong-En Fan, Pai-Hsuen Chen, and Chih-Jen Lin.2005.
Working set selection using the second orderinformation for training SVM.
Journal ofMachine Learning Research, 6: 1889-918.Rich?rd Farkas, Veronika Vincze, Gy?rgy M?ra,J?nos Csirik, and Gy?rgy Szarvas.
2010.
TheCoNLL-2010 Shared Task: Learning to DetectHedges and their Scope in Natural Language Text.In Proceedings of the Fourteenth Conferenceon Computational Natural Language Learning(CoNLL-2010): Shared Task, pages 1?12.Christiane Fellbaum.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press,Cambridge, MA.Viola Ganter, and Michael Strube.
2009.
Findinghedges by chasing weasels: Hedge detection usingWikipedia tags and shallow linguistic features.
InProceedings of joint conference of the 47thAnnual Meeting of the ACL-IJCNLP.Halil Kilicoglu, and Sabine Bergler.
2008.Recognizing Speculative Language in BiomedicalResearch Articles: A Linguistically MotivatedPerspective.
In Proceedings of Current Trendsin Biomedical Natural Language Processing(BioNLP), Columbus, Ohio, USA.Marc Light, Xin Ying Qiu, and Padmini Srinivasan.2004.
The Language of Bioscience: Facts,Speculations, and Statements in between.
InProceedings of the HLT BioLINK.Chrysanne Di Marco, and Robert E. Mercer.
2004.Hedging in Scientific Articles as a Means ofClassifying Citations.
In Proceedings ofWorking Notes of AAAI Spring Symposium onExploring Attitude and Affect in Text:Theories and Applications, Stanford University.Alexa T. McCray, Suresh Srinivasan, and Allen C.Browne.
1994.
Lexical methods for managingvariation in biomedical terminologies.
InProceedings of the 18th Annual Symposium onComputer Applications in Medical Care.Ben Medlock.
2008.
Exploring hedge identification inbiomedical literature.
Journal of BiomedicalInformatics, 41:636-54.Ben Medlock, and Ted Briscoe.
2007.
WeaklySupervised Learning for Hedge Classification inScientific Literature.
In Proceedings of the 45thAnnual Meeting of the Association ofComputational Linguistics.Robert E.Mercer, Chrysanne Di Marco, and FrederickKroon.
2004.
The frequency of hedging cues incitation contexts in scientific writing.
InProceedings of the Canadian Society for theComputational Studies of Intelligence(CSCSI), London, Ontario.Gy?rgy M?ra, Rich?rd Farkas, Gy?rgy Szarvas, andZsolt Moln?r.
2009.
Exploring ways beyond thesimple supervised learning approach for biologicalevent extraction.
In Proceedings of the BioNLP2009 Workshop Companion Volume forShared Task.Roser Morante, and Walter Daelemans.
2009a.Learning the scope of hedge cues in biomedicaltexts.
In Proceedings of the BioNLP 2009Workshop, Boulder, Colorado.
Association forComputational Linguistics.Roser Morante, and Walter Daelemans.
2009b.Learning the scope of hedge cues in biomedicaltexts.
In Proceedings of the Workshop onBioNLP.Gy?rgy Szarvas.
2008.
Hedge classification inbiomedical texts with a weakly supervisedselection of keywords.
In Proceedings of theACL-08: HLT.Vladimir N. Vapnik.
1998.
Statistical learningtheory.
Wiley, New York.Veronika Vincze, Gy?rgy Szarvas, Rich?rd Farkas,Gy?rgy M?ra, and J?nos Csirik.
2008.
TheBioScope corpus: biomedical texts annotated foruncertainty, negation and their scopes.
BMCBioinformatics, 9.Dennis Wegener, Thierry Sengstag, Stelios R.Sfakianakis, and Anthony Assi.
2007.
GridR: AnR-based grid-enabled tool for data analysis inACGT clinico-genomic trials.
In Proceedings ofthe 3rd International Conference on e-Scienceand Grid Computing (eScience 2007).31
