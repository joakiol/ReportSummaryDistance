Coling 2008: Companion volume ?
Posters and Demonstrations, pages 165?168Manchester, August 2008Multilingual Mobile-Phone Translation Services for World TravelersMichael Paul, Hideo Okuma, Hirofumi Yamamoto, Eiichiro Sumita,Shigeki Matsuda, Tohru Shimizu, Satoshi Nakamura?
NICT Spoken Language Communication Group?
ATR Spoken Language Communication Research LabsHikaridai 2-2-2, Keihanna Science City, 619-0288 Kyoto, JapanMichael.Paul@nict.go.jpAbstractThis demonstration introduces two newmultilingual translation services for mo-bile phones.
The first translation serviceprovides state-of-the-art text-to-text trans-lations of Japanese as well as English con-versational spoken language in the traveldomain into 17 languages using statisticalmachine translation technologies trainedautomatically from a large-scale multilin-gual corpus.
The second demonstrationis a speech translation service betweenJapanese and English for real environ-ments.
It is based on distributed speechrecognition with noise suppression.
Flexi-ble interfaces between internal and exter-nal speech translation resources ease theportability of the system to other languagesand enable real-time location-free commu-nication world-wide.1 IntroductionSpoken language translation technologies attemptto bridge the language barriers between peoplewith different native languages who each wantto engage in conversation by using their mother-tongue.
The importance of these technologies isincreasing due to increases in the number of op-portunities for cross-language communication inface-to-face conversation, especially in the domainof tourism.We demonstrate two multilingual translationservices for mobile phones that are built on corpus-based speech recognition and translation technolo-gies.
These services enable smooth and location-free communication in real environments coveringthe major languages of most nations (see Figure 1).c?NICT/ATR, 2008.
Licensed under the CreativeCommons Attribution-Noncommercial-Share Alike 3.0 Un-ported license (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.Figure 1: Global Language CoverageThe first multilingual translation service de-scribed in this paper is a text-to-text translationservice that enables users to translate Japaneseand English conversational spoken language sen-tences in the travel domain into 17 other languages.The system?s core components consist of a mul-tilingual, sentence-aligned spoken language cor-pus covering 18 of the major world languagesand state-of-the-art statistical machine translation(SMT) engines that are trained automatically fromthis corpus covering 306 (=18x17) translation di-rections.
A graphical user-interface (GUI) allows24x7 world-wide access to the translation service(see Section 2).The second multilingual translation service is anextension of the text-based translation service thatadditionally provides speech recognition capabil-ities.
This is the first commercial speech transla-tion service in the world.
The system is based ondistributed speech recognition and operates as fol-lows: (1) front-end processing (noise suppression,feature extraction, and feature parameter compres-sion) is carried out on the mobile phone, (2) back-end processing (recognition, translation) is doneon a server and (3) translation results are sent backand displayed on the mobile phone (see Section 3).2 Multilingual Text Translation Service(MTTS)The multilingual text translation service for mo-bile phones can be accessed via ?http://atr-langue.jp/smlt?
or by using the QR code in Figure 2that also illustrates the graphical user interface of165Figure 2: QR Code and GUI of MTTSthe translation service.
Two different modes aredistinguished: (1) the multilingual mode where theinput is translated into all 17 languages simultane-ously and the translation results are displayed side-by-side and (2) the bilingual mode where a singlelanguage out of 17 languages can be selected as thetarget language of the Japanese or English inputtext translation.
The bilingual mode also featuresback-translation functionality, i.e., a reverse trans-lation of the generated translation output into thesource language, that enables immediate feedbackon the quality of the translation output.
In order tosolve font problems of mobile phones, the trans-lated sentences are rendered on the server side andan image is sent and displayed in the mobile phone.2.1 Multilingual Travel Conversation CorpusThe translation engines used for the translation ser-vice are trained on the Basic Travel ExpressionsCorpus (ATR-BTEC) which is a collection of sen-tences that travel experts consider useful for peopleTable 1: Language CharacteristicsLanguage Order Segments MorphologyArabic (ar) SVO phrase richDanish (da) SVO words mediumGerman (de) SVO words mediumEnglish (en) SVO words poorSpanish (es) SVO words mediumFrench (fr) SVO words mediumIndonesian (id) SVO words richItalian (it) SVO words mediumJapanese (ja) SOV none poorKorean (ko) SOV phrase poorMalay (ms) SVO words richDutch (nl) SVO words mediumPortuguese (pt) SVO words mediumBrazilian (pt-b) SVO words mediumPortugueseRussian (ru) SVO words richThai (th) SVO none noneVietnamese (vi) SVO phrase noneChinese (zh) SVO none nonegoing abroad and cover a large variety of topics intravel situations like shopping or stay (Kikui et al,2006).
The multilingual corpus consists of 160Ksentences for each of the 18 languages, alignedat the sentence-level.
The characteristics of allATR-BTEC corpus languages are summarized inTable 1.
These languages differ largely in word or-der (SVO, SOV), segmentation unit (phrase, word,none), and morphology (poor, medium, rich).
Con-cerning word segmentation, the corpora were pre-processed using simple tokenization tools for allEuropean languages and language-specific word-segmentation tools for languages like Chinese,Japanese, Korean, or Thai that do not use white-space to separate word/phrase tokens.
All data setswere lower-cased and punctuation marks were re-moved.2.2 Statistical Machine Translation EnginesPhrase-based statistical machine translation ap-proaches continue to dominate the field of machinetranslation.
The translation service makes use ofstate-of-the-art phrase-based SMT systems withinthe framework of feature-based exponential mod-els containing the following features:?
Phrase translation probability?
Inverse phrase translation probability?
Lexical weighting probability?
Inverse lexical weighting probability?
Phrase penalty?
Language model probability?
Simple distance-based distortion model?
Word penalty166Table 2: Language Model PerplexityLang Entropy Total Eval Datauage Entropy Words Vocabar 5.73 21,663 3,780 1,067da 5.66 17,411 3,077 884de 5.58 16,698 2,995 910en 4.53 14,370 3,169 807es 5.35 15,622 2,919 943fr 4.77 16,793 3,521 929id 6.09 18,145 2,977 908it 5.52 16,078 2,914 956ja 4.03 15,080 3,745 929ko 4.21 15,011 3,567 943ms 6.43 19,144 2,977 909nl 5.66 17,609 3,110 909pt-b 5.73 16,981 2,962 932pt 5.54 16,064 2,900 946ru 6.20 16,040 2,587 1,143th 5.12 20,230 3,953 738vi 4.84 19,531 4,034 792zh 5.11 14,748 2,887 944The basic framework within which all the MTsystems were constructed is shown in Figure 3.SourceL a n g ua g eI n p utT a rg etL a n g ua g eO ut p utD ecod i n g  A l g ori t h margmax P ( s rc | t rg) *  P ( t rg)T ra n s l a t i onM od el s L a n g ua g eM od el sst a t i st i c a la n a l y si sP a ra l l elT ex t  C orp ora M on ol i n g ua lT ex t  C orp oraFigure 3: SMT FrameworkTranslation examples from the respective bilin-gual text corpus are aligned in order to extractphrasal equivalences and to calculate the bilingualfeature probabilities.
Monolingual features like thelanguage model probability are trained on mono-lingual text corpora of the target language wherebystandard word alignment and language modelingtools were used.
For decoding, the CleopATRadecoder (Finch et al, 2007), a multi-stack phrase-based SMT decoder is used.2.3 EvaluationIn order to get an idea of how difficult the trans-lation tasks are, we trained standard 5gram lan-guage models on 160K sentence pairs and eval-uated the entropy and total entropy, i.e., the en-tropy multiplied by word counts, of each languageon an evaluation data set of 510 sentences each.Table 2 shows that the total entropy of EuropeanTable 3: Automatic Evaluation ResultsBLEU (%) METEOR (%)en-* *-en ja-* *-ja en-* *-en ja-* *-jaar 18.21 51.01 13.03 46.09 40.90 69.01 37.52 58.02da 59.70 70.90 45.94 55.34 75.08 82.56 64.41 65.83de 56.48 69.25 41.99 59.20 74.01 81.48 63.69 69.61en ?
?
61.56 68.53 ?
?
78.19 75.39es 65.22 73.82 51.77 63.24 78.15 85.28 68.30 72.17fr 64.69 71.04 52.36 63.16 79.28 83.05 71.14 72.82id 48.35 59.69 40.59 57.24 66.82 75.83 62.33 69.00it 56.80 70.43 43.45 60.77 72.41 82.96 62.35 70.70ja 68.53 61.56 ?
?
75.39 78.19 ?
?ko 37.00 58.82 69.96 85.10 57.89 75.92 83.25 89.73ms 40.99 57.63 36.13 55.84 61.08 74.75 58.73 67.33nl 57.46 72.85 41.43 59.70 75.88 84.52 63.42 72.19pt-b 59.99 69.41 46.50 58.07 72.77 80.70 64.68 69.14pt 62.81 70.25 48.24 59.20 75.65 83.32 67.38 68.32ru 44.46 61.23 36.08 55.13 66.41 73.75 60.59 64.55th 46.49 51.35 43.75 50.85 62.47 73.12 60.25 62.91vi 55.18 57.42 50.86 55.07 71.04 73.98 68.67 70.81zh 53.08 59.33 51.68 69.43 69.85 74.68 65.88 77.62languages like Danish, German, English, Span-ish, etc.
does not differ much.
Moreover, lan-guages with phrasal segments and/or rich morphol-ogy like Arabic, Malay, Russian or Vietnamesehave a high total entropy and thus can be expectedto be more difficult to translate.
This is confirmedby the translation experiments in which the eval-uation data sets were translated using the serverstranslation engines and the translation quality wasevaluated using the standard automatic evaluationmetrics BLEU (Papineni et al, 2002) and ME-TEOR (Banerjee and Lavie, 2005) where scoresrange between 0 (worst) and 1 (best).
Besides Ko-rean (single references only), all languages wereevaluated using 16 reference translations.
Theevaluation results in Table 3 show that closelyrelated language pairs like Japanese-Korean orPortuguese-Brazilian can be translated very accu-rately, whereas translations into languages withhigh total entropy are of lower quality.3 Multilingual Speech TranslationService (MSTS)The speech translation service1 can be accessed via?http://www.atr-trek.co.jp/contents html?
or usingthe QR code in Figure 4 that also illustrates thegraphical user interface of the translation service.After connecting to the top page, the translationservice is activated by selecting the ?Translation?option.
In order to achieve robust speech recogni-1The speech translation service for Japanese?English onDocomo 905i mobile phones started November 2007.167Speak!Figure 4: QR Code and GUI of MSTStion, the service features a push-to-talk function-ality, i.e., the user (1) presses the key to start theservice (2) speaks freely into the integrated micro-phone of the mobile phone, and (3) presses the keyagain after the speech input is finished.
Fast andaccurate front-end and back-end processing algo-rithms enable high-speed speech translation of theinput.
Both, the speech recognition results as wellas the translation results are sent back to and dis-played on the mobile phone.3.1 Multilingual Speech CorpusSimilar to the statistical machine translation ap-proach introduced in Section 2.2, the speech recog-nition components are based on large-sized mul-tilingual speech corpora.
For Japanese, speechrecordings of 4000 speakers were collected result-ing on a total of 200 hours of speech.
For English,almost the same amount of speech data were col-lected from 500 speakers in North America (300speakers), the UK (100 speakers), and Australia(100 speakers).3.2 Distributed Speech RecognitionThe speech interface is based on distributed speechrecognition (DSR) that is integrated as a client-server architecture compatible with the ETSI ES202 050 standards.
The usage of Speech Trans-lation Markup Language (STML) enables flexibleconnections between internal and external speechtranslation resources like speech recognition andtranslation servers via a network.
Figure 5 illus-trates the architecture of the utilized DSR system.The front-end processing includes noise suppres-sion, feature extraction and feature parameter com-pression and is carried out on the mobile phone.The data stream is then sent via internet to the ap-plication service provider (ASP) for back-end pro-cessing, i.e.
speech recognition and statistical ma-chine translation.
The recognition and translationresults are sent back to the mobile phone for dis-play to the user.Back-e n dN e t w o r kF r o n t -e n dA p p l i cat i o n  S e r v i ce  P r o v i d e r  ( A S P )  A p p l i t i o n  S e r v i e  P r o v i d e r  ( A S P )ETSI ES 202  050c o m p a t i b l e  B i t -s t r e a mD a t a  ( 4 .
8 k b i t s / s )Sp e e c h  R e c o g n i t i o n  a n d  Tr a n s l a t i o n  R e s u l t sSpeechr eco g n i t i o n L a n g u a g et r a n s l a t i o nFigure 5: MSTS Architecture4 ConclusionThis paper introduced the first commercial speechtranslation service in the world.
State-of-the-art spoken language translation technologies (dis-tributed speech recognition with noise suppres-sion, multilingual statistical machine translation)are implemented into a flexible client-server archi-tecture that covers the major languages of mostcountries and enables users to communicate in realenvironments all over the world using their ownmobile phones.5 AcknowledgmentsThis work is partly supported by the Grant-in-Aidfor Scientific Research (C) and the Special Coordi-nation Funds for Promoting Science and Technol-ogy of the Ministry of Education, Culture, Sports,Science and Technology, Japan.ReferencesBanerjee, S. and A. Lavie.
2005.
METEOR: An auto-matic metric for MT evaluation with improved corre-lation with human judgments.
In Proceedings of theACL Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/or Summa-rization, pages 65?72, Ann Arbor, Michigan.Finch, A., E. Denoual, H. Okuma, M. Paul, H. Ya-mamoto, K. Yasuda, R. Zhang, and E. Sumita.2007.
The NICT/ATR Speech Translation Systemfor IWSLT 2007.
In Proc.
of the IWSLT, pages 103?110, Trento, Italy.Kikui, G., S. Yamamoto, T. Takezawa, and E. Sumita.2006.
Comparative study on corpora for speechtranslation.
IEEE Transactions on Audio, Speechand Language Processing, 14(5):1674?1682.Papineni, K., S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proc.
of the 40th ACL, pages311?318, Philadelphia, USA.168
