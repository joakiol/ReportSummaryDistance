Proceedings of the ACL Student Research Workshop, pages 67?73,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTopic Modeling Based Classification of Clinical ReportsEfsun SariogluComputer Science DepartmentThe George Washington UniversityWashington, DC, USAefsun@gwu.eduKabir YadavEmergency Medicine DepartmentThe George Washington UniversityWashington, DC, USAkyadav@gwu.eduHyeong-Ah ChoiComputer Science DepartmentThe George Washington UniversityWashington, DC, USAhchoi@gwu.eduAbstractElectronic health records (EHRs) containimportant clinical information about pa-tients.
Some of these data are in the formof free text and require preprocessing to beable to used in automated systems.
Effi-cient and effective use of this data couldbe vital to the speed and quality of healthcare.
As a case study, we analyzed clas-sification of CT imaging reports into bi-nary categories.
In addition to regulartext classification, we utilized topic mod-eling of the entire dataset in various ways.Topic modeling of the corpora provides in-terpretable themes that exist in these re-ports.
Representing reports according totheir topic distributions is more compactthan bag-of-words representation and canbe processed faster than raw text in sub-sequent automated processes.
A binarytopic model was also built as an unsuper-vised classification approach with the as-sumption that each topic corresponds to aclass.
And, finally an aggregate topic clas-sifier was built where reports are classifiedbased on a single discriminative topic thatis determined from the training dataset.Our proposed topic based classifier systemis shown to be competitive with existingtext classification techniques and providesa more efficient and interpretable repre-sentation.1 IntroductionLarge amounts of medical data are now stored aselectronic health records (EHRs).
Some of thesedata are in the form of free text and they need tobe processed and coded for better utilization in au-tomatic or semi-automatic systems.
One possibleutilization is to support clinical decision-making,such as recommending the need for a certain med-ical test while avoiding intrusive tests or medicalcosts.
This type of automated analysis of patientreports can help medical professionals make clin-ical decisions much faster with more confidenceby providing predicted outcomes.
In this study,we developed several topic modeling based classi-fication systems for clinical reports.Topic modeling is an unsupervised techniquethat can automatically identify themes from agiven set of documents and find topic distribu-tions of each document.
Representing reports ac-cording to their topic distributions is more com-pact and can be processed faster than raw text insubsequent automated processing.
It has previ-ously been shown that the biomedical conceptscan be well represented as noun phrases (Huanget al 2005) and nouns, compared to other partsof speech, tend to specialize into topics (Griffithset al 2004).
Therefore, topic model output of pa-tient reports could contain very useful clinical in-formation.2 BackgroundThis study utilized prospective patient data pre-viously collected for a traumatic orbital fractureproject (Yadav et al 2012).
Staff radiologists dic-tated each CT report and the outcome of acute or-bital fracture was extracted by a trained data ab-stractor.
Among the 3,705 reports, 3,242 had neg-ative outcome while 463 had positive.
A randomsubset of 507 CT reports were double-coded, andinter-rater analysis revealed excellent agreementbetween the data abstractor and study physician,with Cohen?s kappa of 0.97.2.1 Bag-of-Words (BoW) RepresentationText data need to be converted to a suitable formatfor automated processing.
One way of doing thisis bag-of-words (BoW) representation where eachdocument becomes a vector of its words/tokens.67The entries in this matrix could be binary statingthe existence or absence of a word in a documentor it could be weighted such as number of times aword exists in a document.2.2 Topic ModelingTopic modeling is an unsupervised learning al-gorithm that can automatically discover themesof a document collection.
Several techniquescan be used for this purpose such as Latent Se-mantic Analysis (LSA) (Deerwester et al 1990),Probabilistic Latent Semantic Analysis (PLSA)(Hofmann, 1999), and Latent Dirichlet Alloca-tion (LDA) (Blei et al 2003).
LSA is a wayof representing hidden semantic structure of aterm-document matrix where rows are documentsand columns are words/tokens (Deerwester et al1990) based on Singular Value Decomposition(SVD).
One of the problems of LSA is that eachword is treated as having the same meaning due tothe word being represented as a single point; there-fore in this representation, polysemes of wordscannot be differentiated.
Also, the final output ofLSA, which consists of axes in Euclidean space, isnot interpretable or descriptive (Hofmann, 2001).PLSA is considered probabilistic version ofLSA where an unobserved class variable zk ?
{z1, ..., zK} is associated with each occurrence ofa word in a particular document (Hofmann, 1999).These classes/topics are then inferred from the in-put text collection.
PLSA solves the polysemyproblem; however it is not considered a fully gen-erative model of documents and it is known to beoverfitting (Blei et al 2003).
The number of pa-rameters grows linearly with the number of docu-ments.LDA, first defined by (Blei et al 2003), de-fines topic as a distribution over a fixed vocabu-lary, where each document can exhibit them withdifferent proportions.
For each document, LDAgenerates the words in a two-step process:1.
Randomly choose a distribution over topics.2.
For each word in the document:(a) Randomly choose a topic from the dis-tribution over topics.
(b) Randomly choose a word from the cor-responding distribution over the vocab-ulary.The probability of generating the word wj fromdocument di can be calculated as below:P (wj |di; ?, ?)
=K?k=1P (wj |zk;?z)P (zk|di; ?d)where ?
is sampled from a Dirichlet distributionfor each document di and ?
is sampled from aDirichlet distribution for each topic zk.
Eithersampling methods such as Gibbs Sampling (Grif-fiths and Steyvers, 2004) or optimization methodssuch as variational Bayes approximation (Asun-cion et al 2009) can be used to train a topicmodel based on LDA.
LDA performs better thanPLSA for small datasets since it avoids overfittingand it supports polysemy (Blei et al 2003).
It isalso considered a fully generative system for doc-uments in contrast to PLSA.2.3 Text ClassificationText classification is a supervised learning algo-rithm where documents?
categories are learnedfrom pre-labeled set of documents.
Support vec-tor machines (SVM) is a popular classification al-gorithm that attempts to find a decision bound-ary between classes that is the farthest from anypoint in the training dataset.
Given labeled train-ing data (xt, yt), t = 1, ..., N where xt ?
RM andyt ?
{1,?1}, SVM tries to find a separating hy-perplane with the maximum margin (Platt, 1998).2.3.1 EvaluationOnce the classifier is built, its performance is eval-uated on training dataset.
Its effectiveness is thenmeasured in the remaining unseen documents inthe testing set.
To evaluate the classification per-formance, precision, recall, and F-score measuresare typically used (Manning et al 2008).3 Related WorkFor text classification, topic modeling techniqueshave been utilized in various ways.
In (Zhanget al 2008), it is used as a keyword selectionmechanism by selecting the top words from topicsbased on their entropy.
In our study, we removedthe most frequent and infrequent words to have amanageable vocabulary size but we did not utilizetopic model output for this purpose.
(Sarioglu etal., 2012) and (Sriurai, 2011) compare BoW rep-resentation to topic model representation for clas-sification using varying and fixed number of top-ics respectively.
This is similar to our topic vec-68tor classification results with SVM, however (Sri-urai, 2011) uses a fixed number of topics, whereaswe evaluated different number of topics since typ-ically this is not known in advance.
In (Baner-jee, 2008), topics are used as additional features toBoW features for the purpose of classification.
Inour approaches, we used topic vector representa-tion as an alternative to BoW and not additional.This way, we can achieve great dimension reduc-tion.
Finally, (Chen et al 2011) developed a re-sampling approach based on topic modeling whenthe class distributions are not balanced.
In thisstudy, resampling approaches are also utilized tocompare skewed dataset results to datasets withequal class distributions; however, we used ran-domized resampling approaches for this purpose.4 ExperimentsFigure 1 shows the three approaches of using topicmodel of clinical reports to classify them and theyare explained below.4.1 PreprocessingDuring preprocessing, all protected health infor-mation were removed to meet Institutional Re-view Board requirements.
Medical record num-bers from each report were replaced by observa-tion numbers, which are sequence numbers thatare automatically assigned to each report.
Fre-quent words were also removed from the vocabu-lary to prevent it from getting too big.
In addition,these frequent words typically do not add much in-formation; most of them were stop words such asis, am, are, the, of, at, and.4.2 Topic ModelingLDA was chosen to generate the topic models ofclinical reports due to its being a generative prob-abilistic system for documents and its robustnessto overfitting.
Stanford Topic Modeling Toolbox(TMT) 1 was used to conduct the experimentswhich is an open source software that providesways to train and infer topic models for text data.4.3 Topic VectorsTopic modeling of reports produces a topic distri-bution for each report which can be used to repre-sent them as topic vectors.
This is an alternativerepresentation to BoW where terms are replaced1http://nlp.stanford.edu/software/tmt/tmt-0.4/with topics and entries for each report show theprobability of a specific topic for that report.
Thisrepresentation is more compact than BoW as thevocabulary for a text collection usually has thou-sands of entries whereas a topic model is typicallybuilt with a maximum of hundreds of topics.4.4 Supervised ClassificationSVM was chosen as the classification algorithm asit was shown that it performs well in text classifi-cation tasks (Joachims, 1998; Yang and Liu, 1999)and it is robust to overfitting (Sebastiani, 2002).Weka was used to conduct classification which is acollection of machine learning algorithms for datamining tasks written in Java (Hall et al 2009).
Ituses attribute relationship file format (ARFF) tostore data in which each line represents a doc-ument followed by its assigned class.
Accord-ingly, the raw text of the reports and topic vectorsare compiled into individual files with their cor-responding outcomes in ARFF and then classifiedwith SVM.4.5 Aggregate Topic Classifier (ATC)With this approach, a representative topic vectorfor each class was composed by averaging theircorresponding topic distributions in the trainingdataset.
A discriminative topic was then chosen sothat the difference between positive and negativerepresentative vectors is maximum.
The reports inthe test datasets were then classified by analyzingthe values of this topic and a threshold was cho-sen to determine the predicted class.
This thresh-old could be chosen automatically based on classdistributions if the dataset is skewed or cross vali-dation methods can be applied to pick a thresholdthat gives the best classification performance in avalidation dataset.
This approach is called Aggre-gate Topic Classifier (ATC) since training labelswere utilized in an aggregate fashion using an av-erage function and not individually.4.6 Binary Topic Classification (BTC)Topic modeling of the data with two topics wasalso analyzed as an unsupervised classificationtechnique.
In this approach, binary topics wereassumed to correspond to the binary classes.
Aftertopic model was learned, the topic with the higherprobability was assigned as the predicted class foreach document.
If the dataset is skewed, whichtopic corresponds to which class was found out bychecking predicted class proportions.
For datasets69!
"#$%&'($)*+&,'!+$)+*-$,,' ./)$+01,$2'34",,15-"#*%'("6'7$8&'7*)1-'9$-&*+,'7*)1-':*2$41%;'<1%"+='7*)1-'34",,15-"#*%'>;;+$;"&$''7*)1-'34",,15$+'Figure 1: System overviewwith equal class distributions, each of the possi-ble assignments were checked and the one withthe better classification performance was chosen.5 ResultsClassification results using ATC and SVM areshown in Figures 2, 3, and 4 for precision, recall,and f-score respectively.
They are each dividedinto five sections to show the result of using dif-ferent training/testing proportions.
These trainingand test datasets were randomized and stratified tomake sure each subset is a good representation ofthe original dataset.
For ATC, we evaluated differ-ent quantile points: 75, 80, 82, 85, 87 as thresholdand picked the one that gives the best classifica-tion performance.
These were chosen as candi-dates based on the positive class ratio of originaldataset of 12%.
Best classification performancewas achieved with 15 topics for ATC and 100 top-ics for SVM.
For smaller number of topics, ATCperformed better than SVM.
As number of topicsincreased, it got harder to find a very discrimina-tive single topic and therefore ATC?s performancegot worse whereas SVM?s performance got betteras it got more information with more number oftopics.
However, using topic vectors to representreports still provided great dimension reduction asraw text of the reports had 1,296 terms and madethe subsequent classification with SVM faster.
Fi-nally, different training and test set proportions didnot have much effect on both of ATC?s and SVM?sperformance.
This could be considered a goodoutcome as using only 25% of data for trainingwould be sufficient to build an accurate classifier.We analyzed the performance of classificationusing binary topics with three datasets: original,undersampled, and oversampled.
In the under-sampled dataset, excess amount of negative caseswere removed and the resulting dataset consistedof 463 documents for each class.
For oversampleddataset, positive cases were oversampled whilekeeping the total number of documents the same.This approach produced a dataset consisting of1,895 positive and 1,810 negative cases.
Withthe original dataset, we could see the performanceon a highly skewed real dataset and with the re-sampled datasets, we could see the performanceon data with equal class distributions.
Classifica-tion results using this approach are summarizedin Table 2.
As a baseline, a trivial rejector/zerorule classifier was used.
This classifier simplypredicted the majority class.
Balanced datasetsperformed better compared to skewed originaldataset using this approach.
This is also due tothe fact that skewed dataset had a higher baselinecompared to the undersampled and oversampleddatasets.
In Table 3, the best performance of eachFigure 2: PrecisionFigure 3: Recalltechnique for the original dataset is summarized.Although BTC performed better than baseline for70Table 1: Classification performance using ATC and SVMK Dimension Train-Test (%) ATC SVMReduction (%) Precision Recall F-score Precision Recall F-score5 99.6175 - 25 92.15 89.96 90.11 93.19 93.52 93.2866 - 34 92.40 91.26 91.37 92.50 92.85 92.6250 - 50 93.24 92.37 92.44 92.48 92.76 92.5934 - 66 93.50 92.43 92.50 92.80 92.92 92.8625 - 75 93.03 92.84 92.87 92.93 93.06 92.9910 99.2375 - 25 95.65 95.03 95.23 95.01 95.14 95.0566 - 34 95.38 95.23 95.30 94.58 94.76 94.6450 - 50 95.38 95.29 95.33 94.98 95.14 95.0334 - 66 95.61 95.13 95.26 95.11 95.26 95.1625 - 75 95.53 95.07 95.20 94.81 95.00 94.8515 98.8475 - 25 95.61 95.14 95.18 95.48 95.57 95.5166 - 34 95.26 95.23 95.24 95.31 95.39 95.3450 - 50 95.49 95.35 95.41 95.46 95.57 95.4934 - 66 96.07 96.03 96.05 95.58 95.71 95.6125 - 75 95.47 95.43 95.45 95.42 95.57 95.4520 98.4675 - 25 95.45 95.36 95.40 95.62 95.68 95.6566 - 34 90.89 90.62 90.75 95.83 95.87 95.8550 - 50 93.59 93.35 93.40 95.79 95.90 95.8234 - 66 96.07 95.95 95.97 95.77 95.87 95.8025 - 75 95.40 95.28 95.30 96.00 96.11 96.0225 98.0775 - 25 95.85 95.36 95.44 95.89 96.00 95.9266 - 34 93.37 93.16 93.26 95.92 96.03 95.9550 - 50 94.10 94.00 94.05 95.65 95.79 95.6834 - 66 93.38 93.17 93.20 95.52 95.66 95.5525 - 75 94.79 94.56 94.59 95.92 96.04 95.9430 97.6975 - 25 93.12 92.98 93.04 96.23 96.33 96.2666 - 34 94.21 93.64 93.73 95.93 96.03 95.9650 - 50 94.95 94.86 94.90 95.94 96.06 95.9534 - 66 94.05 93.95 94.00 95.85 95.95 95.8825 - 75 94.86 94.71 94.73 95.92 96.04 95.9450 96.1475 - 25 93.75 93.63 93.69 95.53 95.68 95.5466 - 34 92.44 92.21 92.32 95.82 95.95 95.8450 - 50 94.32 94.21 94.26 96.12 96.22 96.1534 - 66 91.78 91.70 91.74 96.02 96.11 96.0425 - 75 93.26 93.20 93.22 96.19 96.29 96.1875 94.2175 - 25 91.21 91.04 91.12 96.35 96.44 96.3066 - 34 91.51 91.26 91.37 96.10 96.19 96.0150 - 50 93.57 93.46 93.51 96.07 96.17 96.0034 - 66 89.43 89.33 89.38 95.91 96.03 95.8925 - 75 91.54 91.47 91.50 95.38 95.54 95.34100 92.2875 - 25 91.63 91.47 91.55 96.59 96.65 96.6166 - 34 91.82 91.57 91.69 96.62 96.66 96.6450 - 50 92.51 92.37 92.44 96.30 96.38 96.3234 - 66 91.21 91.12 91.17 96.16 96.24 96.1925 - 75 91.26 91.18 91.22 96.05 96.15 96.0871Figure 4: F-ScoreTable 2: Binary Topic Classification ResultsDataset Algorithm Precision Recall F-scoreOriginal Baseline 76.6 87.5 81.7BTC 88.6 73.4 77.7Undersampled Baseline 49.6 49.7 47.6BTC 84.4 84.2 84.2Oversampled Baseline 26.2 51.1 34.6BTC 83.4 82.5 82.5datasets with equal class distribution, for the orig-inal skewed dataset, it got worse results than thebaseline.
ATC, on the other hand, got compara-ble results with SVM using both topic vectors andraw text.
In addition, ATC used fewer number oftopics than SVM for its best performance.Table 3: Overall classification performanceAlgorithm Precision Recall F-scoreBaseline 76.6 87.5 81.7BTC 88.6 73.4 77.7ATC 96.1 96.0 96.1Topic vectors 96.6 96.7 96.6Raw Text 96.4 96.3 96.36 ConclusionIn this study, topic modeling of clinical reports areutilized in different ways with the end goal of clas-sification.
Firstly, bag-of-words representation isreplaced with topic vectors which provide gooddimensionality reduction and still get compara-ble classification performance.
In aggregate topicclassifier, representative topic vectors for positiveand negative classes are composed and used asa guide to classify the reports in the test dataset.This approach was competitive with classificationwith SVM using raw text and topic vectors.
Inaddition, it required few topics to get the best per-formance.
And finally, in the unsupervised setting,binary topic models are built for each dataset withthe assumption that each topic corresponds to aclass.
For datasets with equal class distribution,this approach showed improvement over baselineapproaches.ReferencesArthur Asuncion, Max Welling, Padhraic Smyth, andYee-Whye Teh.
2009.
On smoothing and inferencefor topic models.
In UAI.Somnath Banerjee.
2008.
Improving text classificationaccuracy using topic modeling over an additionalcorpus.
In Proceedings of the 31st annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 867?868.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
J. Mach.
Learn.Res., 3:993?1022.Enhong Chen, Yanggang Lin, Hui Xiong, Qiming Luo,and Haiping Ma.
2011.
Exploiting probabilistictopic models to improve text categorization underclass imbalance.
Inf.
Process.
Manage., 47(2):202?214.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
PNAS, 101(suppl.
1):5228?5235.Thomas L. Griffiths, Mark Steyvers, David M. Blei,and Joshua B. Tenenbaum.
2004.
Integrating Topicsand Syntax.
In NIPS, pages 537?544.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In UAI.Thomas Hofmann.
2001.
Unsupervised learning byprobabilistic latent semantic analysis.
Mach.
Learn.,42(1-2):177?196.Yang Huang, Henry J Lowe, Dan Klein, and Russell JCucina.
2005.
Improved identification of nounphrases in clinical radiology reports using a high-performance statistical natural language parser aug-mented with the UMLS specialist lexicon.
J AmMed Inform Assoc, 12(3):275?285.Thorsten Joachims.
1998.
Text Categorization withSupport Vector Machines: Learning with Many Rel-evant Features.
In Proceedings of the 10th EuropeanConference on Machine Learning, pages 137?142.72Christopher D. Manning, Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, New York,NY, USA.John C. Platt.
1998.
Sequential Minimal Optimiza-tion: A Fast Algorithm for Training Support VectorMachines.Efsun Sarioglu, Kabir Yadav, and Hyeong-Ah Choi.2012.
Clinical Report Classification Using Natu-ral Language Processing and Topic Modeling.
11thInternational Conference on Machine Learning andApplications (ICMLA), pages 204?209.Fabrizio Sebastiani.
2002.
Machine learning in au-tomated text categorization.
ACM Comput.
Surv.,34(1):1?47.Wongkot Sriurai.
2011.
Improving Text Categoriza-tion by Using a Topic Model.
Advanced Computing:An International Journal (ACIJ), 2(6).Kabir Yadav, Ethan Cowan, Jason S Haukoos,Zachary Ashwell, Vincent Nguyen, Paul Gennis,and Stephen P Wall.
2012.
Derivation of a clinicalrisk score for traumatic orbital fracture.
J TraumaAcute Care Surg, 73(5):1313?1318.Yiming Yang and Xin Liu.
1999.
A re-examinationof text categorization methods.
In Proceedings ofthe 22nd annual international ACM SIGIR confer-ence on Research and development in informationretrieval, pages 42?49.Zhiwei Zhang, Xuan-Hieu Phan, and SusumuHoriguchi.
2008.
An Efficient Feature SelectionUsing Hidden Topic in Text Categorization.
In Pro-ceedings of the 22nd International Conference onAdvanced Information Networking and Applications- Workshops, AINAW ?08, pages 1223?1228.73
