Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 419?423,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSemantic Roles for String to Tree Machine TranslationMarzieh Bazrafshan and Daniel GildeaDepartment of Computer ScienceUniversity of RochesterRochester, NY 14627AbstractWe experiment with adding semantic roleinformation to a string-to-tree machinetranslation system based on the rule ex-traction procedure of Galley et al (2004).We compare methods based on augment-ing the set of nonterminals by adding se-mantic role labels, and altering the ruleextraction process to produce a separateset of rules for each predicate that encom-pass its entire predicate-argument struc-ture.
Our results demonstrate that the sec-ond approach is effective in increasing thequality of translations.1 IntroductionStatistical machine translation (SMT) has madeconsiderable advances in using syntactic proper-ties of languages in both the training and the de-coding of translation systems.
Over the past fewyears, many researchers have started to realize thatincorporating semantic features of languages canalso be effective in increasing the quality of trans-lations, as they can model relationships that oftenare not derivable from syntactic structures.Wu and Fung (2009) demonstrated the promiseof using features based on semantic predicate-argument structure in machine translation, usingthese feature to re-rank machine translation out-put.
In general, re-ranking approaches are lim-ited by the set of translation hypotheses, leadingto a desire to incorporate semantic features intothe translation model used during MT decoding.Liu and Gildea (2010) introduced two types ofsemantic features for tree-to-string machine trans-lation.
These features model the reorderings anddeletions of the semantic roles in the source sen-tence during decoding.
They showed that additionof these semantic features helps improve the qual-ity of translations.
Since tree-to-string systems aretrained on parse trees, they are constrained by thetree structures and are generally outperformed bystring-to-tree systems.Xiong et al (2012) integrated two discrimi-native feature-based models into a phrase-basedSMT system, which used the semantic predicate-argument structure of the source language.
Theirfirst model defined features based on the context ofa verbal predicate, to predict the target translationfor that verb.
Their second model predicted the re-ordering direction between a predicate and its ar-guments from the source to the target sentence.Wu et al (2010) use a head-driven phrase struc-ture grammar (HPSG) parser to add semantic rep-resentations to their translation rules.In this paper, we use semantic role labels to en-rich a string-to-tree translation system, and showthat this approach can increase the BLEU (Pap-ineni et al, 2002) score of the translations.
Weextract GHKM-style (Galley et al, 2004) transla-tion rules from training data where the target sidehas been parsed and labeled with semantic roles.Our general method of adding information to thesyntactic tree is similar to the ?tree grafting?
ap-proach of Baker et al (2010), although we fo-cus on predicate-argument structure, rather thannamed entity tags and modality.
We modify therule extraction procedure of Galley et al (2004) toproduce rules representing the overall predicate-argument structure of each verb, allowing us tomodel alternations in the mapping from syntax tosemantics of the type described by Levin (1993).2 Semantic Roles for String-to-TreeTranslation2.1 Semantic Role LabelingSemantic Role Labeling (SRL) is the task of iden-tifying the arguments of the predicates in a sen-tence, and classifying them into different argu-ment labels.
Semantic roles can provide a level419of understanding that cannot be derived from syn-tactic analysis of a sentence.
For example, insentences ?Ali opened the door.?
and ?The dooropened?, the word door has two different syntac-tic roles but only one semantic role in the two sen-tences.Semantic arguments can be classified into coreand non-core arguments (Palmer et al, 2010).Core arguments are necessary for understandingthe sentence.
Non-core arguments add more infor-mation about the predicate but are not essential.Automatic semantic role labelers have been de-veloped by training classifiers on hand annotateddata (Gildea and Jurafsky, 2000; Srikumar andRoth, 2011; Toutanova et al, 2005; Fu?rstenau andLapata, 2012).
State-of-the-art semantic role la-belers can predict the labels with accuracies ofaround 90%.2.2 String-to-Tree TranslationWe adopt the GHKM framework of Galley et al(2004) using the parses produced by the split-merge parser of Petrov et al (2006) as the Englishtrees.
As shown by Wang et al (2010), the refinednonterminals produced by the split-merge methodcan aid machine translation.
Furthermore, in allof our experiments, we exclude unary rules duringextraction by ensuring that no rules will have thesame span in the source side (Chung et al, 2011).2.3 Using Semantic Role Labels in SMTTo incorporate semantic information into a string-to-tree SMT system, we tried two approaches:?
Using semantically enriched GHKM rules,and?
Extracting semantic rules separately from theregular GHKM rules, and adding a new fea-ture for distinguishing the semantic rules.The next two sections will explain these twomethods in detail.2.4 Semantically Enriched Rules (Method 1)In this method, we tag the target trees in the train-ing corpus with semantic role labels, and extractthe translation rules from the tagged corpus.
Sincethe SCFG rule extraction methods do not assumeany specific set of non-terminals for the targetparse trees, we can attach the semantic roles ofeach constituent to its label in the tree, and useSNP?ARG0NPBNNeverybodyVPVBG?PREDlendingNP?ARG1NPBDTaNNhandFigure 1: A target tree after inserting semanticroles.
?Lending?
is the predicate, ?everybody?
isargument 0, and ?a hand?
is argument 1 for thepredicate.S-8NP-7-ARG1 1 victimized by NP-7-ARG0 2NP-7-ARG1 1 ?
NP-7-ARG0 2Figure 2: A complete semantic rule.these new labels for rule extraction.
We only la-bel the core arguments of each predicate, to makesure that the rules are not too specific to the train-ing data.
We attach each semantic label to the rootof the subtree that it is labeling.
Figure 1 showsan example target tree after attaching the semanticroles.
We then run a GHKM rule extractor on thelabeled training corpus and use the semanticallyenriched rules with a syntax-based decoder.2.5 Complete Semantic Rules with AddedFeature (Method 2)This approach uses the semantic role labels toextract a set of special translation rules, that onthe target side form the smallest tree fragments inwhich one predicate and all of its core argumentsare present.
These rules model the complete se-mantic structure of each predicate, and are usedby the decoder in addition to the normal GHKMrules, which are extracted separately.Starting by semantic role labeling the targetparse trees, we modify the GHKM component ofthe system to extract a semantic rule for each pred-icate.
We define labels p as the set of semanticrole labels related to predicate p. That includes all420Number of rulesdev testBaseline 1292175 1300589Method 1 1340314 1349070Method 2 1416491 1426159Table 1: The number of the translation rules usedby the three experimented methodsof the labels of the arguments of p, and the labelof p itself.
Then we add the following conditionto the definition of the ?frontier node?
defined inGalley et al (2004):A frontier node must have either all or none ofthe semantic role labels from labels p in its de-scendants in the tree.Adding this new condition, we extract one se-mantic rule for each predicate, and for that rule wediscard the labels related to the other predicates.This semantic rule will then have on its target side,the smallest tree fragment that contains all of thearguments of predicate p and the predicate itself.Figure 2 depicts an example of a complete se-mantic rule.
Numbers following grammatical cat-egories (for example, S-8 at the root) are the re-fined nonterminals produced by the split-mergeparser.
In general, the tree side of the rule mayextend below the nodes with semantic role labelsbecause of the general constraint on frontier nodesthat they must have a continuous span in the source(Chinese) side.
Also, the internal nodes of therules (such as a node with PRED label in Figure2) are removed because they are not used in de-coding.We also extract the regular GHKM rules usingthe original definition of the frontier nodes, andadd the semantic rules to them.
To differentiatethe semantic rules from the non-semantic ones, weadd a new binary feature that is set to 1 for thesemantic rules and to 0 for the rest of the rules.3 ExperimentsSemantic role labeling was done using the Prop-Bank standard (Palmer et al, 2005).
Our labeleruses a maximum entropy classifier and for iden-tification and classification of semantic roles, andhas a percision of 90% and a recall of 88%.
Thefeatures used for training the labeler are a subset ofthe features used by Gildea and Jurafsky (2000),Xue and Palmer (2004), and Pradhan et al (2004).The string-to-tree training data that we used isa Chinese to English parallel corpus that containsmore than 250K sentence pairs, which consist of6.3M English words.
The corpus was drawn fromthe newswire texts available from LDC.1 We useda 392-sentence development set with four refer-ences for parameter tuning, and a 428-sentencetest set with four references for testing.
They aredrawn from the newswire portion of NIST evalua-tion (2004, 2005, 2006).
The development set andthe test set only had sentences with less than 30words for decoding speed.
A set of nine standardfeatures, which include globally normalized countof rules, lexical weighting (Koehn et al, 2003),length penalty, and number of rules used, was usedfor the experiments.
In all of our experiments, weused the split-merge parsing method of Petrov etal.
on the training corpus, and mapped the seman-tic roles from the original trees to the result of thesplit-merge parser.
We used a syntax-based de-coder with Earley parsing and cube pruning (Chi-ang, 2007).
We used the Minimum Error RateTraining (Och, 2003) to tune the decoding param-eters for the development set and tested the bestweights that were found on the test set.We ran three sets of experiments: Baselineexperiments, where we did not do any seman-tic role labeling prior to rule extraction and onlyextracted regular GHKM rules, experiments withour method of Section 2.4 (Method 1), and a setof experiments with our method of Section 2.5(Method 2).Table 1 contains the numbers of the GHKMtranslation rules used by our three method.
Therules were filtered by the development and the testto increase the decoding speed.
The increases inthe number of rules were expected, but they werenot big enough to significantly change the perfor-mance of the decoder.3.1 ResultsFor every set of experiments, we ran MERT on thedevelopment set with 8 different starting weightvectors picked randomly.
For Method 2 we addeda new random weight for the new feature.
We thentested the system on the test set, using for eachexperiment the weight vector from the iteration ofMERT with the maximum BLEU score on the de-velopment set.
Table 3 shows the BLEU scoresthat we found on the test set, and their correspond-ing scores on the development set.1We randomly sampled our data from various differentsources.
The language model is trained on the English sideof entire data (1.65M sentences, which is 39.3M words.
)421Source ??
13?????
,?????
,?????
.Reference to solve the problem of 1.3 billion people , we can only rely on ourselves and nobody else .Baseline cannot rely on others , can only resolve the problem of 13 billion people , on their own .Method 2 to resolve the issue of 1.3 billion people , they can?t rely on others , and it can only rely on themselves .Source ????????
,????????????
.Reference in the new situation of the millennium , the development of asia is facing new opportunities .Baseline facing new opportunities in the new situation in the new century , the development of asia .Method 2 under the new situation in the new century , the development of asia are facing a new opportunity .Source ??
,????????????
???????????
.Reference he said the arab league is the best partner to discuss with the united states about carrying out democratic reforms in the middle east .Baseline arab league is the best with democratic reform in the middle east region in the discussion of the united states , he said .Method 2 arab league is the best partner to discuss the middle east region democratic reform with the united states , he said .Table 2: Comparison of example translations from the baseline method and our Method 2.The best BLEU score on the test set is 25.92,which is from the experiments of Method 2.Method 1 system seems to behave slightly worsethan the baseline and Method 2.
The reason forthis behavior is that the rules that were extractedfrom the semantic role labeled corpus could haveisolated semantic roles in them which would notnecessarily get connected to the right predicateor argument during decoding.
In other words,it is possible for a rule to only contain one orsome of the semantic arguments of a predicate,and not even include the predicate itself, and there-fore there is no guarantee that the predicate will betranslated with the right arguments and in the rightorder.
The difference between the BLEU scoresof the best Method 2 results and the baseline is0.92.
This improvement is statistically significant(p = 0.032) and it shows that incorporating se-mantic roles in machine translation is an effectiveapproach.Table 2 compares some translations from thebaseline decoder and our Method 2.
The first lineof each example is the Chinese source sentence,and the second line is one of the reference trans-lations.
The last two lines compare the baselineand Method 2.
These examples show how ourMethod 2 can outperform the baseline method, bytranslating complete semantic structures, and gen-erating the semantic roles in the correct order inthe target language.
In the first example, the pred-icate rely on for the argument themselves was nottranslated by the baseline decoder, but it was cor-rectly translated by Method 2.
The second ex-ample is a case where the baseline method gener-ated the arguments in the wrong order (in the caseof facing and development), but the translation byMethod 2 has the correct order.
In the last examplewe see that the arguments of the predicate discusshave the wrong order in the baseline translation,BLEU Scoredev testBaseline 26.01 25.00Method 1 26.12 24.84Method 2 26.5 25.92Table 3: BLEU scores on the test and developmentsets, of 8 experiments with random initial featureweights.but Method 2 generated the correct oder.4 ConclusionWe proposed two methods for incorporating se-mantic role labels in a string-to-tree machinetranslation system, by learning translation rulesthat are semantically enriched.
In one approach,the system learned the translation rules by us-ing a semantic role labeled corpus and augment-ing the set of nonterminals used in the rules, andin the second approach, in addition to the regu-lar SCFG rules, the system learned semantic roleswhich contained the complete semantic structureof a predicate, and added a feature to distinguishthose rules.The first approach did not perform any betterthan the baseline, which we explained as being dueto having rules with only partial semantic struc-tures and not having a way to guarantee that thoserules will be used with each other in the right way.The second approach significantly outperformedthe baseline of our experiments, which shows thatcomplete predicate-argument structures can im-prove the quality of machine translation.Acknowledgments Partially funded by NSFgrant IIS-0910611.422ReferencesKathryn Baker, Michael Bloodgood, Chris Callison-Burch, Bonnie J. Dorr, Nathaniel W. Filardo, LoriLevin, Scott Miller, and Christine Piatko.
2010.Semantically-informed machine translation: A tree-grafting approach.
In Proceedings of The Ninth Bi-ennial Conference of the Association for MachineTranslation in the Americas, Denver, Colorado.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Tagyoung Chung, Licheng Fang, and Daniel Gildea.2011.
Issues concerning decoding with synchronouscontext-free grammar.
In Proceedings of the ACL2011 Conference Short Papers, Portland, Oregon.Association for Computational Linguistics.Hagen Fu?rstenau and Mirella Lapata.
2012.
Semi-supervised semantic role labeling via structuralalignment.
Computational Linguistics, 38(1):135?171.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translationrule?
In Proceedings of NAACL-04, pages 273?280,Boston.Daniel Gildea and Daniel Jurafsky.
2000.
Automaticlabeling of semantic roles.
In Proceedings of ACL-00, pages 512?520, Hong Kong, October.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of NAACL-03, pages 48?54, Edmonton,Alberta.Beth Levin.
1993.
English Verb Classes And Alter-nations: A Preliminary Investigation.
University ofChicago Press, Chicago.Ding Liu and Daniel Gildea.
2010.
Semantic role fea-tures for machine translation.
In COLING-10, Bei-jing.Franz Josef Och.
2003.
Minimum error rate trainingfor statistical machine translation.
In Proceedingsof ACL-03, pages 160?167.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Martha Palmer, Daniel Gildea, and Nianwen Xue.2010.
Semantic Role Labeling.
Synthesis Lec-tures on Human Language Technology Series.
Mor-gan and Claypool.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedingsof ACL-02, pages 311?318.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433?440,Sydney, Australia, July.
Association for Computa-tional Linguistics.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2004.
Shallow semanticparsing using support vector machines.
In Proceed-ings of NAACL-04.V.
Srikumar and D. Roth.
2011.
A joint model forextended semantic role labeling.
In EMNLP, Edin-burgh, Scotland.Kristina Toutanova, Aria Haghighi, and ChristopherManning.
2005.
Joint learning improves semanticrole labeling.
In Proceedings of ACL-05, pages 589?596.Wei Wang, Jonathan May, Kevin Knight, and DanielMarcu.
2010.
Re-structuring, re-labeling, andre-aligning for syntax-based machine translation.Computational Linguistics, 36:247?277, June.Dekai Wu and Pascale Fung.
2009.
Semantic roles forsmt: A hybrid two-pass model.
In Proceedings ofthe HLT-NAACL 2009: Short Papers, Boulder, Col-orado.Xianchao Wu, Takuya Matsuzaki, and Jun?ichi Tsujii.2010.
Fine-grained tree-to-string translation rule ex-traction.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, ACL ?10, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Deyi Xiong, Min Zhang, and Haizhou Li.
2012.
Mod-eling the translation of predicate-argument structurefor smt.
In ACL (1), pages 902?911.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedingsof EMNLP.423
