Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1014?1023,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Semantic Representations of Users and Productsfor Document Level Sentiment ClassificationDuyu Tang, Bing Qin?, Ting LiuHarbin Institute of Technology, Harbin, China{dytang, qinb, tliu}@ir.hit.edu.cnAbstractNeural network methods have achievedpromising results for sentiment classifica-tion of text.
However, these models on-ly use semantics of texts, while ignoringusers who express the sentiment and prod-ucts which are evaluated, both of whichhave great influences on interpreting thesentiment of text.
In this paper, we ad-dress this issue by incorporating user- andproduct- level information into a neuralnetwork approach for document level sen-timent classification.
Users and product-s are modeled using vector space mod-els, the representations of which captureimportant global clues such as individu-al preferences of users or overall quali-ties of products.
Such global evidencein turn facilitates embedding learning pro-cedure at document level, yielding bettertext representations.
By combining ev-idence at user-, product- and document-level in a unified neural framework, theproposed model achieves state-of-the-artperformances on IMDB and Yelp dataset-s1.1 IntroductionDocument-level sentiment classification is a fun-damental problem in the field of sentiment analy-sis and opinion mining (Pang and Lee, 2008; Liu,2012).
The task is to infer the sentiment polari-ty or intensity (e.g.
1-5 or 1-10 stars on reviewsites) of a document.
Dominating studies followPang et al (2002; 2005) and regard this problemas a multi-class classification task.
They usually?Corresponding author.1The codes and datasets are available at http://ir.hit.edu.cn/?dytang/use machine learning algorithms, and build sen-timent classifier from documents with accompa-nying sentiment labels.
Since the performanceof a machine learner is heavily dependent on thechoice of data representations (Domingos, 2012),many works focus on designing effective features(Pang et al, 2002; Qu et al, 2010; Kiritchenko etal., 2014) or learning discriminative features fromdata with neural networks (Socher et al, 2013;Kalchbrenner et al, 2014; Le and Mikolov, 2014).Despite the apparent success of neural networkmethods, they typically only use text informationwhile ignoring the important influences of usersand products.
Let us take reviews with respect to1-5 rating scales as an example.
A critical usermight write a review ?it works great?
and mark 4stars, while a lenient user might give 5 stars even ifhe posts an (almost) identical review.
In this case,user preference affects the sentiment rating of a re-view.
Product quality also has an impact on reviewsentiment rating.
Reviews towards high-qualityproducts (e.g.
Macbook) tend to receive higherratings than those towards low-quality products.Therefore, it is feasible to leverage individual pref-erences of users and overall qualities of productsto build a smarter sentiment classifier and achievebetter performance2.In this paper, we propose a new model dubbedUser Product Neural Network (UPNN) to captureuser- and product-level information for sentimentclassification of documents (e.g.
reviews).
UPNNtakes as input a variable-sized document as wellas the user who writes the review and the productwhich is evaluated.
It outputs sentiment polaritylabel of a document.
Users and products are en-coded in continuous vector spaces, the representa-tions of which capture important global clues such2One can manually design a small number of user andproduct features (Gao et al, 2013).
However, we argue thatthey are not effective enough to capture sophisticated seman-tics of users and products.1014as user preferences and product qualities.
Theserepresentations are further integrated with contin-uous text representation in a unified neural frame-work for sentiment classification.We apply UPNN to three datasets derived fromIMDB and Yelp Dataset Challenge.
We compareto several neural network models including recur-sive neural networks (Socher et al, 2013), para-graph vector (Le and Mikolov, 2014), sentiment-specific word embedding (Tang et al, 2014b),and a state-of-the-art recommendation algorithmJMARS (Diao et al, 2014).
Experimental resultsshow that: (1) UPNN outperforms baseline meth-ods for sentiment classification of documents; (2)incorporating representations of users and prod-ucts significantly improves classification accuracy.The main contributions of this work are as follows:?
We present a new neural network method(UPNN) by leveraging users and products fordocument-level sentiment classification.?
We validate the influences of users and prod-ucts in terms of sentiment and text on massiveIMDB and Yelp reviews.?
We report empirical results on three datasets,and show that UPNN outperforms state-of-the-artmethods for sentiment classification.2 Consistency Assumption VerificationWe detail the effects of users and products in termsof sentiment (e.g.
1-5 rating stars) and text, andverify them on review datasets.We argue that the influences of users and prod-ucts include the following four aspects.?
user-sentiment consistency.
A user has spe-cific preference on providing sentiment ratings.Some users favor giving higher ratings like 5 starsand some users tend to give lower ratings.
In oth-er words, sentiment ratings from the same user aremore consistent than those from different users.?
product-sentiment consistency.
Similarwith user-sentiment consistency, a product alsohas its ?preference?
to receive different averageratings on account of its overall quality.
Sentimentratings towards the same product are more consis-tent than those towards different products.?
user-text consistency.
A user likes to use per-sonalized sentiment words when expressing opin-ion polarity or intensity.
For example, a strict usermight use ?good?
to express an excellent attitude,but a lenient user may use ?good?
to evaluate anordinary product.Algorithm 1 Consistency Assumption TestingInput: data X , number of users/products m,number of iterations nOutput: meaSamek, meaDiffk, 1 ?
k ?
nfor k = 1 to n domeaSamek= 0, meaSamek= 0for i = 1 to m doSample xi, x+i, x?ifrom XmeaSamek+= measure(xi, x+i)meaDiffk+= measure(xi, x?i)end formeaSamek/= m, meaDiffk/= mend for?
product-text consistency.
Similar with user-text consistency, a product also has a collection ofproduct-specific words suited to evaluate it.
Forexample, people prefer using ?sleek?
and ?stable?to evaluate a smartphone, while like to use ?wire-less?
and ?mechanical?
to evaluate a keyboard.We test four consistency assumptions men-tioned above with the same testing criterion,which is formalized in Algorithm 1.
For eachconsistency assumption, we test it for n = 50 iter-ations on each of IMDB, Yelp Dataset Challenge2013 and 2014 datasets.
Taking user-sentimentconsistency as an example, in each iteration,we randomly select two reviews xi, x+iwrittenby the same user ui, and a review x?iwrittenby another randomly selected user.
Afterwards,we calculate the measurements of (xi, x+i) and(xi, x?i), and aggregate these statistics for musers.
In user-sentiment assumption test, we useabsolute rating difference ||ratinga?
ratingb||as the measurement between two reviews a andb.
We illustrate the results in Figure 1 (a)3,where 2013same/2014same/amzsame (redplots) means that two reviews are written by asame user, and 2013diff/2014diff/amzdiff(black plots) means that two reviews are writtenby different users.
We can find that: the absoluterating differences between two reviews written bya same user are lower than those written by dif-ferent users (t-test with p-value < 0.01).
In otherwords, sentiment ratings from the same user aremore consistent than those from different users.This validates the user-sentiment consistency.For testing product-sentiment consistency, we3Since the rating scale of IMDB (1-10) is different fromYelp (1-5), we divide the rating difference of IMDB reviewsby two for better visualizing and analyzing the results.10152013Same 2013Diff 2014Same 2014Diff IMDBSame IMDBDiff1.21.251.31.351.41.451.51.551.6(a) user-sentiment consistency2013Same 2013Diff 2014Same 2014Diff IMDBSame IMDBDiff1.151.21.251.31.351.41.451.51.55(b) product-sentiment consistency2013Same 2013Diff 2014Same 2014Diff IMDBSame IMDBDiff0.20.210.220.230.240.250.260.270.28(c) user-text consistency2013Same 2013Diff 2014Same 2014Diff IMDBSame IMDBDiff0.20.210.220.230.240.250.260.27(d) product-text consistencyFigure 1: Assumption testing of user-sentiment, product-sentiment, user-text and product-text consisten-cies.
We test them on the datasets from IMDB and Yelp Dataset Challenge in 2013 and 2014.use absolute rating difference as the measuremen-t.
The reviews xi, x+iare towards a same productpi, and x?iis towards another randomly selectedproduct.
From Figure 1 (b), we can see that sen-timent ratings towards the same product are moreconsistent than those towards different products.In order to verify the assumptions of user-text andproduct-text consistencies, we use cosine similar-ity between bag-of-words of two reviews as themeasurement.
Results are given in Figure 1 (c) and(d).
We can see that the textual similarity betweentwo reviews written by a same user (or towards asame product) are higher than those written by d-ifferent users (or towards different products).3 User Product Neural Network (UPNN)for Sentiment ClassificationWe present the details of User Product Neural Net-work (UPNN) for sentiment classification.
An il-lustration of UPNN is given in Figure 2.
It takesas input a review, the user who posts the review,and the product which is evaluated.
UPNN cap-tures four kinds of consistencies which are veri-fied in Section 2.
It outputs the sentiment category(e.g.
1-5 stars) of a review by considering not onlythe semantics of review text, but also the informa-tion of user and product.
In following subsection-s, we first describe the use of neural network formodeling semantics of variable-sized documents.We then present the methods for incorporating us-er and product information, followed by the useof UPNN in a supervised learning framework forsentiment classification.3.1 Modeling Semantics of DocumentWe model the semantics of documents based onthe principle of compositionality (Frege, 1892),which states that the meaning of a longer expres-sion (e.g.
a sentence or a document) comes fromthe meanings of its words and the rules used tocombine them.
Since a document consists of a listof sentences and each sentence is made up of a listof words, we model the semantic representation ofa document in two stages.
We first produce con-tinuous vector of each sentence from word repre-sentations.
Afterwards, we feed sentence vectorsas inputs to compose document representation.For modeling the semantics of words, we rep-resent each word as a low dimensional, continu-1016Softmax gold rating = 2w1h1Uk Pjh2 hnLookupLinear?
?ConvolutionPooling uk pjvdTanhw1?
?w2Uk Pj w2?
?wnUk Pj wn?
?Figure 2: An illustration of the neural network approach for sentiment classification.
wimeans thei-th word of a review text.
ukand pjare continuous vector representations of user k and product jfor capturing user-sentiment and product-sentiment consistencies.
Ukand Pjare continuous matrixrepresentations of user k and product j for capturing user-text and product-text consistencies.ous and real-valued vector, also known as wordembedding (Bengio et al, 2003).
All the wordvectors are stacked in a word embedding matrixLw?
Rd?|V |, where d is the dimension of wordvector and |V | is the size of word vocabulary.These word vectors can be randomly initializedfrom a uniform distribution, regarded as a param-eter and jointly trained with other parameters ofneural networks.
Alternatively, they can be pre-trained from text corpus with embedding learningalgorithms (Mikolov et al, 2013; Pennington etal., 2014; Tang et al, 2014b), and applied as ini-tial values of word embedding matrix.
We adoptthe latter strategy which better exploits the seman-tic and grammatical associations of words.To model semantic representations of sentences,convolutional neural network (CNN) and recur-sive neural network (Socher et al, 2013) are t-wo state-of-the-art methods.
We use CNN (Kim,2014; Kalchbrenner et al, 2014) in this work asit does not rely on external parse tree.
Specifical-ly, we use multiple convolutional filters with dif-ferent widths to produce sentence representation.The reason is that they are capable of capturing lo-cal semantics of n-grams of various granularities,which are proven powerful for sentiment classifi-cation.
The convolutional filter with a width of 3essentially captures the semantics of trigrams in asentence.
Accordingly, multiple convolutional fil-ters with widths of 1, 2 and 3 encode the semanticsof unigrams, bigrams and trigrams in a sentence.An illustration of CNN with three convolu-tional filters is given in Figure 3.
Let usdenote a sentence consisting of n words as{w1, w2, ...wi, ...wn}.
Each word wiis mapped toits embedding representation ei?
Rd.
A convo-lutional filter is a list of linear layers with sharedparameters.
Let lcfbe the width of a convolution-al filter, and let Wcf, bcfbe the shared parametersof linear layers in the filter.
The input of a linearlayer is the concatenation of word embeddings in afixed-length window size lcf, which is denoted asIcf= [ei; ei+1; ...; ei+lcf?1] ?
Rd?lcf.
The outputof a linear layer is calculated asOcf=Wcf?
Icf+ bcf(1)where Wcf?
Rlen?d?lcf, bcf?
Rlen, len is theoutput length of linear layer.
In order to capturethe global semantics of a sentence, we feed theoutput of a convolutional filter to an average pool-ing layer, resulting in an output vector with fixed-length.
We further add hyperbolic tangent func-tions (tanh) to incorporate element-wise nonlin-earity, and fold (average) their outputs to generatesentence representation.We feed sentence vectors as the input of an av-erage pooling layer to obtain the document rep-resentation.
Alternative document modeling ap-proaches include CNN or recurrent neural net-work.
However, we prefer average pooling for itscomputational efficiency and good performance inour experiment.3.2 Modeling Semantics of Users andProductsWe integrate semantic representations of usersand products in UPNN to capture user-sentiment,1017Foldingw1LookupConvolutionPoolingw2 w3 w4 w?
?1 w?Tanhfilter1 filter2 filter3Figure 3: Convolutional neural network with mul-tiple convolutional filters for sentence modeling.product-sentiment, user-text and product-text con-sistencies.For modeling user-sentiment and product-sentiment consistencies, we embed each user asa continuous vector uk?
Rduand embed eachproduct as a continuous vector pj?
Rdp, where duand dpare dimensions of user vector and productvector, respectively.
The basic idea behind this isto map users with similar rating preferences (e.g.prefer assigning 4 stars) into close vectors in userembedding space.
Similarly, the products whichreceive similar averaged ratings are mapped intoneighboring vectors in product embedding space.In order to model user-text consistency, we rep-resent each user as a continuous matrix Uk?RdU?d, which acts as an operator to modify thesemantic meaning of a word.
This is on the basisof vector based semantic composition (Mitchel-l and Lapata, 2010).
They regard compositionalmodifier as a matrix X1to modify another com-ponent x2, and use matrix-vector multiplicationy = X1?
x2as the composition function.
Multi-plicative semantic composition is suitable for ourneed of user modifying word meaning, and ithas been successfully utilized to model adjective-noun composition (Clark et al, 2008; Baroni andZamparelli, 2010) and adverb-adjective composi-tion (Socher et al, 2012).
Similarly, we modelproduct-text consistency by encoding each prod-uct as a matrix Pj?
RdP?d, where d is the di-mension of word vector, dPis the output lengthof product-word multiplicative composition.
Afterconducting user-word multiplication and product-word multiplication operations, we concatenatetheir outputs and feed them to CNN (detailed inSection 3.1) for producing user and product en-hanced document representation.3.3 Sentiment ClassificationWe apply UPNN to document level sentiment clas-sification under a supervised learning framework(Pang and Lee, 2005).
Instead of using hand-crafted features, we use continuous representationof documents, users and products as discrimina-tive features.
The sentiment classifier is built fromdocuments with gold standard sentiment labels.As is shown in Figure 2, the feature represen-tation for building rating predictor is the concate-nation of three parts: continuous user representa-tion uk, continuous product representation pjandcontinuous document representation vd, where vdencodes user-text consistency, product-text consis-tency and document level semantic composition.We use softmax to build the classifier because itsoutputs can be interpreted as conditional probabil-ities.
Softmax is calculated as given in Equation2, where C is the category number (e.g.
5 or 10).softmaxi=exp(xi)?Ci?=1exp(xi?
)(2)We regard cross-entropy error betweengold sentiment distribution and predicted sen-timent distribution as the loss function ofsoftmax.
We take the derivative of lossfunction through back-propagation with re-spect to the whole set of parameters ?
=[W1,2,3cf; b1,2,3cf;uk; pj;Uk;Pj;Wsoftmax, bsoftmax],and update parameters with stochastic gradientdescent.
We set the widths of three convolutionalfilters as 1, 2 and 3.
We learn 200-dimensionalsentiment-specific word embeddings (Tang etal., 2014b) on each dataset separately, randomlyinitialize other parameters from a uniform distri-bution U(?0.01, 0.01), and set learning rate as0.03.4 ExperimentWe conduct experiments to evaluate UPNN by ap-plying it to sentiment classification of documents.4.1 Experimental SettingExisting benchmark datasets for sentiment clas-sification such as Stanford Sentiment Treebank(Socher et al, 2013) typically only have text infor-mation, but do not contain users who express thesentiment or products which are evaluated.
There-fore, we build the datasets by ourselves.
In orderto obtain large scale corpora without manual anno-tation, we derive three datasets from IMDB (Diao1018Dataset #users #products #reviews #docs/user #docs/product #sents/doc #words/docIMDB 1,310 1,635 84,919 64.82 51.94 16.08 394.6Yelp 2014 4,818 4,194 231,163 47.97 55.11 11.41 196.9Yelp 2013 1,631 1,633 78,966 48.42 48.36 10.89 189.3Table 1: Statistical information of IMDB, Yelp 2014 and Yelp 2013 datasets used for sentiment classifi-cation.
The rating scale of IMDB dataset is 1-10.
The rating scale of Yelp 2014 and Yelp 2013 datasets is1-5.
|V | is the vocabulary size of words in each dataset.
#users is the number of users, #docs/user meansthe average number of documents per user posts in the corpus.et al, 2014) and Yelp Dataset Challenge4in 2013and 2014.
Statistical information of the generateddatasets are given in Table 1.We split each corpus into training, developmentand testing sets with a 80/10/10 split, and conducttokenization and sentence splitting with StanfordCoreNLP (Manning et al, 2014).
We use standardaccuracy (Manning and Sch?utze, 1999; Jurafskyand Martin, 2000) to measure the overall senti-ment classification performance, and use MAEand RMSE to measure the divergences betweenpredicted sentiment ratings (pr) and ground truthratings (gd).MAE =?i|gdi?
pri|N(3)RMSE =??i(gdi?
pri)2N(4)4.2 Baseline MethodsWe compare UPNN with the following baselinemethods for document-level sentiment classifica-tion.
(1) Majority is a heuristic baseline method,which assigns the majority sentiment category intraining set to each review in the test dataset.
(2) In Trigram, we use unigrams, bigrams andtrigrams as features and train classifier with sup-ported vector machine (SVM) (Fan et al, 2008).
(3) In TextFeature, we implement hand-craftedtext features including word/character ngrams,sentiment lexicon features, negation features, etcal.
(Kiritchenko et al, 2014).
(4) We extract user-leniency features (Gao et al,2013) and corresponding product features (denot-ed as UPF) from training data, and concatenatethem with the features in baseline (2) and (3).
(5) We learn word embeddings from trainingand development sets with word2vec (Mikolov etal., 2013), average word embeddings to get docu-ment representation, and train a SVM classifier.4http://www.yelp.com/dataset_challenge(6) We learn sentiment-specific word embed-dings (SSWE) from training and development set-s, and use max/min/average pooling (Tang et al,2014b) to generate document representation.
(7) We represent sentence with RNTN (Socheret al, 2013) and compose document representa-tion with recurrent neural network.
We averagehidden vectors of recurrent neural network as thefeatures for sentiment classification.
(8) We re-implement PVDM in Paragraph Vec-tor (Le and Mikolov, 2014) because its codes arenot officially provided.
The window size is tunedon development set.
(9) We compare with a state-of-the-art recom-mendation algorithm JMARS (Diao et al, 2014),which leverages user and aspects of a review withcollaborative filtering and topic modeling.4.3 Model ComparisonsExperimental results are given in Table 2.
The re-sults are separated into two groups: the methodsabove only use texts of review, and the methodsbelow also use user and product information.From the first group, we can see that majori-ty performs very poor because it does not cap-ture any text or user information.
SVM classi-fiers with trigrams and hand-crafted text featuresare powerful for document level sentiment classi-fication and hard to beat.
We compare the wordembedding learnt from each corpus with off-the-shell general word embeddings5.
Results showthat tailored word embedding from each corpusperforms slightly better than general word embed-dings (about 0.01 improvement in terms of accu-racy).
SSWE performs better than context-basedword embedding by incorporating sentiment in-formation of texts.
Setting a large window size(e.g.
15) is crucial for effectively training SS-WE from documents with accompanying senti-5We compare with Glove embeddings learnt fromWikipedia and Twitter http://nlp.stanford.edu/projects/glove/1019IMDB Yelp 2014 Yelp 2013Acc MAE RMSE Acc MAE RMSE Acc MAE RMSEMajority 0.196 1.838 2.495 0.392 0.779 1.097 0.411 0.744 1.060Trigram 0.399 1.147 1.783 0.577 0.487 0.804 0.569 0.513 0.814TextFeature 0.402 1.134 1.793 0.572 0.490 0.800 0.556 0.520 0.845AvgWordvec + SVM 0.304 1.361 1.985 0.530 0.562 0.893 0.526 0.568 0.898SSWE + SVM 0.312 1.347 1.973 0.557 0.523 0.851 0.549 0.529 0.849Paragraph Vector 0.341 1.211 1.814 0.564 0.496 0.802 0.554 0.515 0.832RNTN + Recurrent 0.400 1.133 1.764 0.582 0.478 0.821 0.574 0.489 0.804UPNN (no UP) 0.405 1.030 1.629 0.585 0.483 0.808 0.577 0.485 0.812Trigram + UPF 0.404 1.132 1.764 0.576 0.471 0.789 0.570 0.491 0.803TextFeature + UPF 0.402 1.129 1.774 0.579 0.476 0.791 0.561 0.509 0.822JMARS N/A 1.285 1.773 N/A 0.710 0.999 N/A 0.699 0.985UPNN (full) 0.435 0.979 1.602 0.608 0.447 0.764 0.596 0.464 0.784Table 2: Sentiment classification on IMDB, Yelp 2014 and Yelp 2013 datasets.
Evaluation metrics areaccuracy (Acc, higher is better), MAE (lower is better) and RMSE (lower is better).
Our full model isUPNN (full).
Our model without using user and product information is abbreviated as UPNN (no UP).The best method in each group is in bold.ment labels.
RNTN+Reccurent is a strong per-former by effectively modeling document repre-sentation with semantic composition.
Our textbased model (UPNN no UP) performs slightly bet-ter than RNTN+Reccurent, trigram and text fea-tures.From the second group, we can see that con-catenating user product feature (UPF) with exist-ing feature sets does not show significant improve-ments.
This is because the dimension of existingfeature sets is typically huge (e.g.
1M trigram fea-tures in Yelp 2014), so that concatenating a smal-l number of UPF features does not have a greatinfluence on the whole model.
We do not evalu-ate JMARS in terms of accuracy because JMARSoutputs real-valued ratings.
Our full model UPNNyields the best performance on all three dataset-s.
Incorporating semantic representations of us-er and product significantly (t-test with p-value <0.01) boosts our text based model (UPNN no UP).This shows the effectiveness of UPNN over stan-dard trigrams and hand-crafted features when in-corporating user and product information.4.4 Model Analysis: Effect of User andProduct RepresentationsWe investigate the effects of vector based user andproduct representations (uk, pj) as well as ma-trix based user and product representations (Uk,Pj) for sentiment classification.
We remove vec-tor based representations (uk, pj) and matrix basedrepresentations (Uk, Pj) from UPNN separately,and conduct experiments on three datasets.
FromTable 3, we can find that vector based representa-tions (uk, pj) are more effective than matrix basedrepresentations (Uk, Pj).
This is because ukandpjencode user-sentiment and product-sentimentconsistencies, which are more directly associat-ed with sentiment labels than user-text (Uk) andproduct-text (Pj) consistencies.
Another reasonmight be that the parameters of vector represen-tations are less than the matrix representations, sothat the vector representations are better estimat-ed.
We also see the contribution from each of userand product by removing (Uk, uk) and (Pj, pj)separately.
Results are given in Table 3.
It is in-teresting to find that user representations are obvi-ously more effective than product representationsfor review rating prediction.4.5 Discussion: Out-Of-Vocabulary Usersand ProductsOut-of-vocabulary (OOV) situation occurs if a us-er or a product in testing/decoding process is n-ever seen in training data.
We give two natu-ral solutions (avg UP and unk UP) to deal withOOV users and products.
One solution (avg UP)is to regard the averaged representations of user-s/products in training data as the representation ofOOV user/product.
Another way (unk UP) is tolearn a shared ?unknown?
user/product represen-tation for low-frequency users in training data, andapply it to OOV user/product.1020IMDB Yelp 2014 Yelp 2013Acc MAE RMSE Acc MAE RMSE Acc MAE RMSEUPNN (full) 0.435 0.979 1.602 0.608 0.447 0.764 0.596 0.464 0.784UPNN ?
uk?
pj0.409 1.021 1.622 0.585 0.483 0.808 0.572 0.491 0.823UPNN ?
Uk?
Pj0.426 0.993 1.607 0.597 0.465 0.789 0.585 0.482 0.802UPNN ?
Uk?
uk0.324 1.209 1.743 0.577 0.475 0.778 0.566 0.505 0.828UPNN ?
Pj?
pj0.397 1.075 1.712 0.595 0.462 0.776 0.590 0.476 0.802Table 3: Influence of user and product representations.
For user k and product j, ukand pjare theircontinuous vector representations, Ukand Pjare their continuous matrix representations (see Figure 2).IMDB Yelp 2014 Yelp 20130.350.40.450.50.550.60.65no UPavg UPunk UPfull UPFigure 4: Accuracy of OOV user and product onOOV test set.In order to evaluate the two strategies for OOVproblem, we randomly select 10 percent users andproducts from each development set, and masktheir user and product information.
We run avgUP, unk UP together with UPNN (no UP) whichonly uses text information, and UPNN (full) whichlearns tailored representation for each user andproduct.
We evaluate classification accuracy onthe extracted OOV test set.
Experimental resultsare given in Figure 5.
We can find that these twostrategies perform slightly better than UPNN (noUP), but still worse than the full model.5 Related Work5.1 Sentiment ClassificationSentiment classification is a fundamental prob-lem in sentiment analysis, which targets at infer-ring the sentiment label of a document.
Pang andLee (2002; 2005) cast this problem a classifica-tion task, and use machine learning method ina supervised learning framework.
Goldberg andZhu (2006) use unlabelled reviews in a graph-based semi-supervised learning method.
Many s-tudies design effective features, such as text top-ic (Ganu et al, 2009), bag-of-opinion (Qu et al,2010) and sentiment lexicon features (Kiritchenkoet al, 2014).
User information is also used forsentiment classification.
Gao et al (2013) de-sign user-specific features to capture user lenien-cy.
Li et al (2014) incorporate textual topic anduser-word factors with supervised topic modeling.Tan et al (2011) and Hu et al (2013) utilize user-text and user-user relations for Twitter sentimen-t analysis.
Unlike most previous studies that usehand-crafted features, we learn discriminative fea-tures from data.
We differ from Li et al (2014) inthat we encode four kinds of consistencies and useneural network approach.
User representation isalso leveraged for recommendation (Weston et al,2013), web search (Song et al, 2014) and socialmedia analytics (Perozzi et al, 2014).5.2 Neural Network for SentimentClassificationNeural networks have achieved promising resultsfor sentiment classification.
Existing neural net-work methods can be divided into two groups:word embedding and semantic composition.
Forlearning word embeddings, (Mikolov et al, 2013;Pennington et al, 2014) use local and global con-texts, (Maas et al, 2011; Labutov and Lipson,2013; Tang et al, 2014b; Tang et al, 2014a;Zhou et al, 2015) further incorporate sentiment oftexts.
For learning semantic composition, Glorotet al (2011) use stacked denoising autoencoder,Socher et al (2013) introduce a family of recursivedeep neural networks (RNN).
RNN is extendedwith adaptive composition functions (Dong et al,2014), global feedbackward (Paulus et al, 2014),feature weight tuning (Li, 2014), and also usedfor opinion relation detection (Xu et al, 2014).Li et al (2015) compare the effectiveness of re-cursive neural network and recurrent neural net-work on five NLP tasks including sentiment clas-sification.
(Kalchbrenner et al, 2014; Kim, 2014;Johnson and Zhang, 2014) use convolutional neu-ral networks.
Le and Mikolov (2014) introduce1021Paragraph Vector.
Unlike existing neural networkapproaches that only use the semantics of texts,we take consideration of user and product rep-resentations and leverage their connections withtext semantics for sentiment classification.
Thiswork is an extension of our previous work (Tang etal., 2015), which only takes consideration of user-word association.6 ConclusionIn this paper, we introduce User Product Neu-ral Network (UPNN) for document level senti-ment classification under a supervised learningframework.
We validate user-sentiment, product-sentiment, user-text and product-text consistencieson massive reviews, and effectively integrate themin UPNN.
We apply the model to three datasetsderived from IMDB and Yelp Dataset Challenge.Empirical results show that: (1) UPNN outper-forms state-of-the-art methods for document levelsentiment classification; (2) incorporating contin-uous user and product representations significantlyboosts sentiment classification accuracy.AcknowledgmentsThe authors give great thanks to Furu Wei, LeiCui, Nan Yang, Jiwei Li, Yaming Sun, Mao Zhengand anonymous reviewers for their valuable com-ments.
We would like to thank Qiming Diao forproviding the IMDB dataset as well as the codes ofJMARS.
This work was supported by the Nation-al High Technology Development 863 Program ofChina (No.
2015AA015407), National NaturalScience Foundation of China (No.
61133012 andNo.
61273321).
Duyu Tang is supported by BaiduFellowship and IBM Ph.D. Fellowship.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Noun-s are vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InEMNLP, pages 1183?1193.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzade-h. 2008.
A compositional distributional modelof meaning.
In Quantum Interaction Symposium,pages 133?140.Qiming Diao, Minghui Qiu, Chao-Yuan Wu, Alexan-der J Smola, Jing Jiang, and Chong Wang.
2014.Jointly modeling aspects, ratings and sentiments formovie recommendation (jmars).
In SIGKDD, pages193?202.
ACM.Pedro Domingos.
2012.
A few useful things to knowabout machine learning.
Communications of theACM, 55(10):78?87.Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2014.Adaptive multi-compositionality for recursive neu-ral models with applications to sentiment analysis.In AAAI, pages 1537?1543.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: Alibrary for large linear classification.
JMLR.Gottlob Frege.
1892.
On sense and reference.
Ludlow(1997), pages 563?584.Gayatree Ganu, Noemie Elhadad, and Am?elie Marian.2009.
Beyond the stars: Improving rating predic-tions using review text content.
In WebDB.Wenliang Gao, Naoki Yoshinaga, Nobuhiro Kaji, andMasaru Kitsuregawa.
2013.
Modeling user leniencyand product popularity for sentiment classification.In ICJNLP, pages 1107?1111.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In ICML,pages 513?520.Andrew B Goldberg and Xiaojin Zhu.
2006.
Seeing s-tars when there aren?t many stars: graph-based semi-supervised learning for sentiment categorization.
InGraphBased Method for NLP, pages 45?52.Xia Hu, Lei Tang, Jiliang Tang, and Huan Liu.
2013.Exploiting social relations for sentiment analysis inmicroblogging.
In WSDM, pages 537?546.Rie Johnson and Tong Zhang.
2014.
Effective use ofword order for text categorization with convolution-al neural networks.
arXiv preprint: 1412.1058.Dan Jurafsky and James H Martin.
2000.
Speech &language processing.
Pearson Education India.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In ACL, pages 655?665.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In EMNLP, pages 1746?1751.Svetlana Kiritchenko, Xiaodan Zhu, and Saif M Mo-hammad.
2014.
Sentiment analysis of short in-formal texts.
Journal of Artificial Intelligence Re-search, pages 723?762.1022Igor Labutov and Hod Lipson.
2013.
Re-embeddingwords.
In Annual Meeting of the Association forComputational Linguistics.Quoc V. Le and Tomas Mikolov.
2014.
Distribut-ed representations of sentences and documents.
InICML, pages 1188?1196.Fangtao Li, Sheng Wang, Shenghua Liu, and MingZhang.
2014.
Suit: A supervised user-item basedtopic model for sentiment analysis.
In AAAI, pages1636?1642.Jiwei Li, Dan Jurafsky, and Eudard Hovy.
2015.
Whenare tree structures necessary for deep learning of rep-resentations?
arXiv preprint:1503.00185.Jiwei Li.
2014.
Feature weight tuning for recursiveneural networks.
Arxiv preprint, 1412.3714.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Tech-nologies, 5(1):1?167.Andrew L Maas, Raymond E Daly, Peter T Pham, DanHuang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
In A-CL, pages 142?150.Christopher D Manning and Hinrich Sch?utze.
1999.Foundations of statistical natural language process-ing.
MIT press.Christopher Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven Bethard, and David McClosky.2014.
The stanford corenlp natural language pro-cessing toolkit.
In ACL, pages 55?60.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In NIPS, pages 3111?3119.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploit-ing class relationships for sentiment categorizationwith respect to rating scales.
In ACL, pages 115?124.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and trends in infor-mation retrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In EMNLP, pages 79?86.Romain Paulus, Richard Socher, and Christopher DManning.
2014.
Global belief recursive neural net-works.
In NIPS, pages 2888?2896.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for wordrepresentation.
In EMNLP, pages 1532?1543.Bryan Perozzi, Rami Al-Rfou, and Steven Skiena.2014.
Deepwalk: Online learning of social repre-sentations.
In SIGKDD, pages 701?710.
ACM.Lizhen Qu, Georgiana Ifrim, and Gerhard Weikum.2010.
The bag-of-opinions method for review ratingprediction from sparse text patterns.
In COLING,pages 913?921.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic Composi-tionality Through Recursive Matrix-Vector Spaces.In EMNLP, pages 1201?1211.Richard Socher, Alex Perelygin, Jean Wu, JasonChuang, Christopher D. Manning, Andrew Ng, andChristopher Potts.
2013.
Recursive deep modelsfor semantic compositionality over a sentiment tree-bank.
In EMNLP, pages 1631?1642.Yang Song, Hongning Wang, and Xiaodong He.
2014.Adapting deep ranknet for personalized search.
InWSDM, pages 83?92.
ACM.Chenhao Tan, Lillian Lee, Jie Tang, Long Jiang, MingZhou, and Ping Li.
2011.
User-level sentimentanalysis incorporating social networks.
In SIGKDD,pages 1397?1405.
ACM.Duyu Tang, Furu Wei, Bing Qin, Ming Zhou, and T-ing Liu.
2014a.
Building large-scale twitter-specificsentiment lexicon: A representation learning ap-proach.
In COLING, pages 172?182.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, TingLiu, and Bing Qin.
2014b.
Learning sentiment-specific word embedding for twitter sentiment clas-sification.
In ACL, pages 1555?1565.Duyu Tang, Bing Qin, Ting Liu, and Yuekui Yang.2015.
User modeling with neural network for re-view rating prediction.
In IJCAI.Jason Weston, Ron J Weiss, and Hector Yee.
2013.Nonlinear latent factorization by embedding multi-ple user interests.
In RecSys, pages 65?68.
ACM.Liheng Xu, Kang Liu, and Jun Zhao.
2014.
Joint opin-ion relation detection using one-class deep neuralnetwork.
In COLING, pages 677?687.Xinjie Zhou, Xiaojun Wan, and Jianguo Xiao.
2015.Representation learning for aspect category detec-tion in online reviews.
In AAAI.1023
