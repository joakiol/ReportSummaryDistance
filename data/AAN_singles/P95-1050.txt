Identi fying Word Translations in Non-Paral lel  TextsReinhard RappISSCO, Universit6 de Gen~ve54 route des AcaciasGen~ve, Switzerlandrapp@divsun.unige.chAbst ractCommon algorithms for sentence andword-alignment allow the automatic iden-tification of word translations from paxalhltexts.
This study suggests that the identi-fication of word translations should also bepossible with non-paxMlel and even unre-lated texts.
The method proposed is basedon the assumption that there is a corre-lation between the patterns of word co-occurrences in texts of different languages.1 In t roduct ionIn a number of recent studies it has been shown thatword translations can be automatically derived fromthe statistical distribution of words in bilingual pax-allel texts (e. g. Catizone, Russell & Warwick, 1989;Brown et al, 1990; Dagan, Church & Gale, 1993;Kay & Rbscheisen, 1993).
Most of the proposedalgorithms first conduct an alignment of sentences,i.
e. those palxs of sentences axe located that aretranslations of each other.
In a second step a wordalignment is performed by analyzing the correspon-dences of words in each pair of sentences.The results achieved with these algorithms havebeen found useful for the compilation of dictionaries,for checking the consistency of terminological usagein translations, and for assisting the terminologicalwork of translators and interpreters.However, despite serious efforts in the compilationof corpora (Church & Mercer, 1993; Armstrong &Thompson, 1995) the availability of a large enoughpaxallel corpus in a specific field and for a given pairof languages will always be the exception, not therule.
Since the acquisition of non-paxallel texts isusually much easier, it would be desirable to havea program that can determine the translations ofwords from comparable or even unrelated texts.2 ApproachIt is assumed that there is a correlation betweenthe co-occurrences of words which are translationsof each other.
If - for example - in a text of onelanguage two words A and B co-occur more oftenthan expected from chance, then in a text of an-other language those words which axe translations ofA and B should also co-occur more frequently thanexpected.
This assumption is reasonable for paralleltexts.
However, in this paper it is further assumedthat the co-occurrence patterns in original texts axenot fundamentally different from those in translatedtexts.Starting from an English vocabulary of six wordsand the corresponding German translations, table laand b show an English and a German co-occurrencemat~x.
In these matrices the entries belonging tothose pairs of words that in texts co-occur more fre-quently than expected have been marked with a dot.In general, word order in the lines and columns of aco-occurrence matrix is independent of each other,but for the purpose of this paper can always be as-sumed to be equal without loss of generality.If now the word order of the English matrix is per-muted until the resulting pattern of dots is most sim-ilar to that of the German matrix (see table lc), thenthis increases the likelihood that the English andGerman words axe in corresponding order.
Word nin the English matrix is then the translation of wordn in the German matrix.3 S imulat ionA simulation experiment was conducted in order tosee whether the above assumptions concerning thesimilarity of co-occurrence patterns actually hold.In this experiment, for an equivalent English andGerman vocabulary two co-occurrence matrices werecomputed and then compared.
As the English vo-cabulary a list of 100 words was used, which h~lbeen suggested by Kent & Rosanoff (1910) for asso-ciation experiments.
The German vocabulary con-sisted of one by one translations of these words aschosen by Russell (1970).The word co-occurrences were computed on thebasis of an English corpus of 33 and a German corpusof 46 million words.
The English corpus consists of320Table 1: When the word orders of the English andthe German matrix correspond, the dot patterns ofthe two matrices are identical.
(a)II1 n213141s161blue 1 ?
?green 2 ?
?plant 3 ?school 4 ?sky 5 ?teacher 6 ?
(b)(c)11112131415181blau 1 ?
?grfin 2 ?
?Himmel 3 ?Lehrer 4 ?Pflanze 5 ?Schule 6 s1 2 5 6 3 4blue 1 * ?green 2 ?
?5 ?6 ?3 ?4 ?skyteacherplantschoolthe Brown Corpus, texts from the Wall Street Your-hal, Grolier's Electronic Encyclopedia nd scientificabstracts from different fields.
The German cor-pus is a compilation of mainly newspaper texts fromFrankfurter Rundschau, Die Zei~ and Mannl~eimerMorgen.
To the knowledge of the author, the Englishand German corpora contain no parallel passages.For each pair of words in the English vocabularyits frequency of common occurrence in the Englishcorpus was counted.
The common occurrence of twowords was defined as both words being separatedby at most 11 other words.
The co-occurrence fre-quencies obtained in this way were used to buildup the English matrix.
Equivalently, the Germanco-occurrence matrix was created by counting theco-occurrences of German word pairs in the Germancorpus.
As a starting point, word order in the twomatrices was chosen such that word n in the Germanmatrix was the translation of word n in the Englishmatrix.Co-occurrence studies like that conducted byWettler & Rapp (1993) have shown that for manypurposes it is desirable to reduce the influence ofword frequency on the co-occurrence counts.
Forthe prediction of word associations they achievedbest results when modifying each entry in the co-occurrence matrix using the following formula:( ' f ( i~J)) '  (1)A, j  -- f ( i ) .
f( j)Hereby f(i&j) is the frequency of common occur-rence of the two words i and j, and f(i) is the corpusfrequency of word i.
However, for comparison, thesimulations described below were also conducted us-ing the original co-occurrence matrices (formula 2)and a measure similar to mutual information (for-mula 3).
1A,,j = f(i&j) (2)f(i&j) (3)ai,i - -  f ( i ) .
f( j)Regardless of the formula applied, the English andthe German matrix where both normalized.
2 Start-ing from the normalized English and German matri-ces, the aim was to determine how far the similarityof the two matrices depends on the correspondenceof word order.
As a measure for matrix similaritythe sum of the absolute differences of the values atcorresponding matrix positions was used.N Ns = ~ ~ \[E, a - G,,jl (4)i=1 ./=1This similarity measure leads to a value of zero foridentical matrices, and to a value of 20 000 in thecase that a non-zero entry in one of the 100 * 100matrices always corresponds to a zero-value in theother.4 ResultsThe simulation was conducted by randomly permut-ing the word order of the German matrix and thencomputing the similarity s to the English matrix.For each permutation it was determined how manywords c had been shifted to positions different fromthose in the original German matrix.
The simulationwas continued until for each value of c a set of 1000similarity values was available.
8 Figure 1 shows forthe three formulas how the average similarity J be-tween the English and the German matrix dependson the number of non-corresponding word positionsc.
Each of the curves increases monotonically, withformula 1 having the steepest, i. e. best discriminat-ing characteristic.
The dotted curves in figure 1 arethe minimum and maximum values in each set of1000 similarity values for formula 1.X The logarithm has been removed from the mutualinformation measure since it is not defined for zero co-occurrences.=Normalization was conducted in such a way that thesuxn of all matrix entries adds up to the number of fieldsin the matrix.Sc ---- 1 is not possible and was not taken into account.321mooo20 ..................................................... -,  O)18 " :"--': <.. .
.16 j ..14 '?
-,~12 E "~/ .....10- I  C '0 10 2"0 3"0 40 5"0 6"0 7"0 8"0 90 100Figure 1: Dependency between the mean similarity iof the English and the German matrix and the num-ber of non-corresponding word positions c for 3 for-mulas.
The dotted lines are the minimum and max-imum values of each sample of 1000 for formula 1.5 Discussion and prospectsIt could be shown that even for unrelated Eng-lish and German texts the patterns of word co-occurrences strongly correlate.
The monotonicallyincreasing chaxacter of the curves in figure 1 indi-cates that in principle it should be possible to findword correspondences in two matrices of ditferentlanguages by randomly permuting one of the ma-trices until the similarity function s reaches a mini-mum and thus indicates maximum similarity.
How-ever, the minimum-curve in figure 1 suggests thatthere are some deep minima of the similarity func-tion even in cases when many word correspondencesaxe incorrect.
An algorithm currently under con-sttuction therefore searches for many local minima,and tries to find out what word correspondences axethe most reliable ones.
In order to limit the seaxchspace, translations that axe known beforehand canbe used as anchor points.Future work will deal with the following as yetunresolved problems:?
Computational limitations require the vocabu-laxies to be limited to subsets of all word typesin large corpora.
With criteria like the corpusfrequency of a word, its specificity for a givendomain, and the salience of its co-occurrencepatterns, it should be possible to make a selec-tion of corresponding vocabularies in the twolanguages.
If morphological tools and disv~m-biguators axe available, preliminaxy lemmatiz~tion of the corpora would be desirable.?
Ambiguities in word translations can be takeninto account by working with continuous prob-abilities to judge whether a word translationis correct instead of making a binary decision.Thereby, different sizes of the two matricescould be allowed for.It can be expected that with such a method the qual-ity of the results depends on the thematic ompara-bility of the corpora, but not on their degree of paz-allelism.
As a further step, even with non parallelcorpora it should be possible to locate comparablepassages of text.AcknowledgementsI thank Susan Armstrong and Manfred Wettler fortheir support of this project.
Thanks also to GrahamRussell and three anonymous reviewers for valuablecomments on the manuscript.ReferencesArmstrong, Susan; Thompson, Henry (1995).
Apresentation of MLCC: Multilingual Corporafor Cooperation.
Linguistic Database Workshop,Groningen.Brown, Peter; Cocke, John; Della Pietra, StephenA.
; Della Pietra, Vincent J.; Jelinek, Fredrick;Lstferty, John D.; Mercer, Robert L.; Rossin, PaulS.
(1990).
A statistical pproach to machine trans-lation.
Computational Linguistics, 16(2), 79-85.Catizone, Roberta; Russell, Graham; Waxwick, Su-san (1989).
Deriving translation data from bilin-gual texts.
In: U. Zernik (ed.
): Proceedings of theFirst International Lezical Acquisition Workshop,Detroit.Church, Kenneth W.; Mercer, Robert L. (1993).Introduction to the special issue on Computa-tional Linguistics using large corpora.
Computa-tional Linguistics, 19(1), 1-24.Dagan, Ido; Church, Kenneth W.; Gale, William A.(1993).
Robust bilingual word alignment for ms-chine aided translation.
Proceedings of the Work-shop on Very Large Corpora: Academic and In-dustrial Perspectives.
Columbus, Ohio, 1-8.Kay, Maxtin; l~Sscheisen, Maxtin (1993).
Text-Translation Alignment.
Computational Linguis-tics, 19(1), 121-142.Kent, G.H.
; R~sanoff, A.J.
(1910).
A study of asso-ciation in insanity.
American Journal of Insanity,67, 37-96, 317-390.Russell, Wallace A.
(1970).
The complete Germanlanguage norms for responses to 100 words fromthe Kent-Rosanoff word association test.
In: L.Postman, G. Keppel (eds.
): Norms of Word As-sociation.
New York: Academic Press, 53-94.Wettler, Manfred; Rapp, Reinhaxd (1993).
Com-putation of word associations based on the co-occurrences of words in large corpora.
In: Pro-ceedings of the Workshop on Very Large Corpora:Academic and Industrial Perspectives, Columbus,Ohio, 84-93.322
