Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 46?54,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing Confusion Networks for Speech SummarizationShasha Xie and Yang LiuDepartment of Computer ScienceThe University of Texas at Dallas{shasha,yangl}@hlt.utdallas.eduAbstractFor extractive meeting summarization, previ-ous studies have shown performance degrada-tion when using speech recognition transcriptsbecause of the relatively high speech recogni-tion errors on meeting recordings.
In this pa-per we investigated using confusion networksto improve the summarization performanceon the ASR condition under an unsupervisedframework by considering more word candi-dates and their confidence scores.
Our ex-perimental results showed improved summa-rization performance using our proposed ap-proach, with more contribution from leverag-ing the confidence scores.
We also observedthat using these rich speech recognition re-sults can extract similar or even better sum-mary segments than using human transcripts.1 IntroductionSpeech summarization has received increasing in-terest recently.
It is a very useful technique thatcan help users to browse a large amount of speechrecordings.
The problem we study in this paper isextractive meeting summarization, which selects themost representative segments from the meeting tran-scripts to form a summary.
Compared to text sum-marization, speech summarization is more challeng-ing because of not only its more spontaneous style,but also word errors in automatic speech recogni-tion (ASR) output.
Intuitively the incorrect wordshave a negative impact on downstream summariza-tion performance.
Previous research has evaluatedsummarization using either the human transcripts orASR output with word errors.
Most of the priorwork showed that performance using ASR output isconsistently lower (to different extent) comparing tothat using human transcripts no matter whether su-pervised or unsupervised approaches were used.To address the problem caused by imperfectrecognition transcripts, in this paper we investigateusing rich speech recognition results for summariza-tion.
N-best hypotheses, word lattices, and confu-sion networks have been widely used as an inter-face between ASR and subsequent spoken languageprocessing tasks, such as machine translation, spo-ken document retrieval (Chelba et al, 2007; Chiaet al, 2008), and shown outperforming using 1-best hypotheses.
However, studies using these richspeech recognition results for speech summariza-tion are very limited.
In this paper, we demonstratethe feasibility of using confusion networks under anunsupervised MMR (maximum marginal relevance)framework to improve summarization performance.Our experimental results show better performanceover using 1-best hypotheses with more improve-ment observed from using confidence measure of thewords.
Moreover, we find that the selected summarysegments are similar to or even better than those gen-erated using human transcripts.2 Related WorkMany techniques have been proposed for the meet-ing summarization task, including both unsuper-vised and supervised approaches.
Since we use un-supervised methods in this study, we will not de-scribe previous work using supervised approachesbecause of the space limit.
Unsupervised meth-46ods are simple and robust to different corpora, anddo not need any human labeled data for training.MMR was introduced in (Carbonell and Goldstein,1998) for text summarization, and was used widelyin meeting summarization (Murray et al, 2005a; Xieand Liu, 2008).
Latent semantic analysis (LSA) ap-proaches have also been used (Murray et al, 2005a),which can better measure document similarity at thesemantic level rather than relying on literal wordmatching.
In (Gillick et al, 2009), the authors intro-duced a concept-based global optimization frame-work using integer linear programming (ILP), whereconcepts were used as the minimum units, and theimportant sentences were extracted to cover as manyconcepts as possible.
They showed better perfor-mance than MMR.
In a follow-up study, (Xie et al,2009) incorporated sentence information in this ILPframework.
Graph-based methods, such as LexRank(Erkan and Radev, 2004), have been originally usedfor extractive text summarization, where the docu-ment is modeled as a graph and sentences as nodes,and sentences are ranked according to its similaritywith other nodes.
(Garg et al, 2009) proposed Clus-terRank, a modified graph-based method in orderto take into account the conversational speech stylein meetings.
Recently (Lin et al, 2009) suggestedto formulate the summarization task as optimizingsubmodular functions defined on the document?s se-mantic graph, and showed better performance com-paring to other graph-based approaches.Rich speech recognition results, such as N-besthypotheses and confusion networks, were first usedin multi-pass ASR systems to improve speech recog-nition performance (Stolcke et al, 1997; Mangu etal., 2000).
They have been widely used in many sub-sequent spoken language processing tasks, such asmachine translation, spoken document understand-ing and retrieval.
Confusion network decoding wasapplied to combine the outputs of multiple machinetranslation systems (Sim et al, 2007; Matusov etal., 2006).
In the task of spoken document retrieval,(Chia et al, 2008) proposed to compute the expectedword counts from document and query lattices, andestimate the statistical models from these counts,and reported better retrieval accuracy than usingonly 1-best transcripts.
(Hakkani-Tur et al, 2006)investigated using confusion networks for name en-tity detection and extraction and user intent classifi-cation.
They also obtained better performance thanusing ASR 1-best output.There is very limited previous work using morethan 1-best ASR output for speech summarization.Several studies used acoustic confidence scores inthe 1-best ASR hypothesis in the summarization sys-tems (Valenza et al, 1999; Zechner and Waibel,2000; Hori and Furui, 2003).
(Liu et al, 2010) eval-uated using n-best hypotheses for meeting summa-rization, and showed improved performance with thegain coming mainly from the first few candidates.
In(Lin and Chen, 2009), confusion networks and po-sition specific posterior lattices were considered ina generative summarization framework for Chinesebroadcast news summarization, and they showedpromising results by using more ASR hypotheses.We investigate using confusion networks for meet-ing summarization in this study.
This work differsfrom (Lin and Chen, 2009) in terms of the languageand genre used in the summarization task, as wellas the summarization approaches.
We also performmore analysis on the impact of confidence scores,different pruning methods, and different ways topresent system summaries.3 Summarization ApproachIn this section, we first describe the baseline sum-marization framework, and then how we apply it toconfusion networks.3.1 Maximum Marginal Relevance (MMR)MMR is a widely used unsupervised approach intext and speech summarization, and has been shownperform well.
We chose this method as the basicframework for summarization because of its sim-plicity and efficiency.
We expect this is a goodstarting point for the study of feasibility of us-ing confusion networks for summarization.
Foreach sentence segment Si in one document D, itsscore (MMR(i)) is calculated using Equation 1according to its similarity to the entire document(Sim1(Si, D)) and the similarity to the already ex-tracted summary (Sim2(Si, Summ)).MMR(i) =??
Sim1(Si, D)?
(1?
?)?
Sim2(Si, Summ)(1)47where parameter ?
is used to balance the two factorsto ensure the selected summary sentences are rel-evant to the entire document (thus important), andcompact enough (by removing redundancy with thecurrently selected summary sentences).
Cosine sim-ilarity can be used to compute the similarity of twotext segments.
If each segment is represented as avector, cosine similarity between two vectors (V1,V2) is measured using the following equation:sim(V1, V2) =?i t1it2i?
?i t21i ??
?i t22i(2)where ti is the term weight for a word wi, for whichwe can use the TFIDF (term frequency, inverse doc-ument frequency) value, as widely used in the fieldof information retrieval.3.2 Using Confusion Networks forSummarizationConfusion networks (CNs) have been used in manynatural language processing tasks.
Figure 1 showsa CN example for a sentence segment.
It is a di-rected word graph from the starting node to the endnode.
Each edge represents a word with its associ-ated posterior probability.
There are several wordcandidates for each position.
?-?
in the CN repre-sents a NULL hypothesis.
Each path in the graph isa sentence hypothesis.
For the example in Figure 1,?I HAVE IT VERY FINE?
is the best hypothesisconsisting of words with the highest probabilities foreach position.
Compared to N-best lists, confusionnetworks are a more compact and powerful repre-sentation for word candidates.
We expect the rich in-formation contained in the confusion networks (i.e.,more word candidates and associated posterior prob-abilities) can help to determine words?
importancefor summarization.Figure 1: An example of confusion networks.The core problems when using confusion net-works under the MMR summarization frameworkare the definitions for Si, D, and Summ, as shownin Equation 1.
The extractive summary unit (foreach Si) we use is the segment provided by the rec-ognizer.
This is often different from syntactic or se-mantic meaningful unit (e.g., a sentence), but is amore realistic setup.
Most of the previous studiesfor speech summarization used human labeled sen-tences as extraction units (for human transcripts, ormap them to ASR output), which is not the real sce-nario when performing speech summarization on theASR condition.
In the future, we will use automaticsentence segmentation results, which we expect arebetter units than pause-based segmentation used inASR.
We still use a vector space model to representeach summarization unit Si.
The entire document(D) and the current selected summary (Summ) areformed by simply concatenating the correspondingsegments Si together.
In the following, we describedifferent ways to represent the segments and how topresent the final summary.A.
Segmentation representationFirst, we construct the vector for each segmentsimply using all the word candidates in the CNs,without considering any confidence measure or pos-terior probability information.
The same TFIDFcomputation is used as before, i.e., counting thenumber of times a word appears (TF) and how manydocuments it appears (used to calculate IDF).Second, we leverage the confidence scores tobuild the vector.
For the term frequency of word wi,we calculate it by summing up its posterior proba-bilities p(wik) at each position k, that is,TF (wi) =?kp(wik) (3)Similarly, the IDF values can also be computed us-ing the confidence scores.
The traditional methodfor calculating a word?s IDF uses the ratio of thetotal number of documents (N ) and the number ofdocuments containing this word.
Using the confi-dence scores, we calculate the IDF values as follows,IDF (wi) = log(N?D (maxk p(wik))) (4)If a word wi appears in the document, we find itsmaximum posterior probability among all the posi-tions it occurs in the CNs, which is used to signalwi?s soft appearance in this document.
We add thesesoft counts for all the documents as the denomina-tor in Equation 4.
Different from the traditional IDF48calculation method, where the number of documentscontaining a word is an integer number, here the de-nominator can be any real number.B.
Confusion network pruningThe above vectors are constructed using the entireconfusion networks.
We may also use the prunedones, in which the words with low posterior prob-abilities are removed beforehand.
This can avoidthe impact of noisy words, and increase the systemspeed as well.
We investigate three different pruningmethods, listed below.?
absolute pruning: In this method, we deletewords if their posterior probabilities are lowerthan a predefined threshold, i.e., p(wi) < ?.?
max diff pruning: First for each position k,we find the maximum probability among allthe word candidates: Pmaxk = maxj p(wjk).Then we remove a word wi in this position ifthe absolute difference of its probability withthe maximum score is larger than a predefinedthreshold, i.e., Pmaxk ?
p(wik) > ?.?
max ratio pruning: This is similar to the aboveone, but instead of absolute difference, we usethe ratio of their probabilities, i.e., p(wik)Pmaxk < ?.Again, for the last two pruning methods, the com-parison is done for each position in the CNs.C.
Summary renderingWith a proper way of representing the text seg-ments, we then extract the summary segments usingthe MMR method described in Section 3.1.
Once thesummary segments are selected using the confusionnetwork input, another problem we need to addressis how to present the final summary.
When usingthe human transcripts or the 1-best ASR hypothesisfor summarization, we can simply concatenate thecorresponding transcripts of the selected sentencesegments as the final summary for the users.
How-ever, when using the confusion networks as the rep-resentation of each sentence segment, we only knowwhich segments are selected by the summarizationsystem.
To provide the final summary to the users,there are two choices.
We can either use the best hy-pothesis from CNs of those selected segments as atext summary; or return the speech segments to theusers to allow them to play it back.
We will evaluateboth methods in this paper.
For the latter, in order touse similar word based performance measures, wewill use the corresponding reference transcripts inorder to focus on evaluation of the correctness of theselected summary segments.4 Experiments4.1 Corpus and Evaluation MeasurementWe use the ICSI meeting corpus, which contains 75recordings from natural meetings (most are researchdiscussions) (Janin et al, 2003).
Each meeting isabout an hour long and has multiple speakers.
Thesemeetings have been transcribed, and annotated withextractive summaries (Murray et al, 2005b).
TheASR output is obtained from a state-of-the-art SRIspeech recognition system, including the confusionnetwork for each sentence segment (Stolcke et al,2006).
The word error rate (WER) is about 38.2%on the entire corpus.The same 6 meetings as in (Murray et al, 2005a;Xie and Liu, 2008; Gillick et al, 2009; Lin et al,2009) are used as the test set in this study.
Fur-thermore, 6 other meetings were randomly selectedfrom the remaining 69 meetings in the corpus toform a development set.
Each meeting in the de-velopment set has only one human-annotated sum-mary; whereas for the test meetings, we use threesummaries from different annotators as referencesfor performance evaluation.
The lengths of the ref-erence summaries are not fixed and vary across an-notators and meetings.
The average word compres-sion ratio for the test set is 14.3%, and the mean de-viation is 2.9%.
We generated summaries with theword compression ratio ranging from 13% to 18%,and only provide the best results in this paper.To evaluate summarization performance, we useROUGE (Lin, 2004), which has been widely usedin previous studies of speech summarization (Zhanget al, 2007; Murray et al, 2005a; Zhu and Penn,2006).
ROUGE compares the system generatedsummary with reference summaries (there can bemore than one reference summary), and measuresdifferent matches, such as N-gram, longest com-mon sequence, and skip bigrams.
In this paper,we present our results using both ROUGE-1 and49ROUGE-2 F-scores.4.2 Characteristics of CNsFirst we perform some analysis of the confusion net-works using the development set data.
We definetwo measurements:?
Word coverage.
This is to verify that CNs con-tain more correct words than the 1-best hy-potheses.
It is defined as the percentage ofthe words in human transcripts (measured us-ing word types) that appear in the CNs.
Weuse word types in this measurement since weare using a vector space model and the multi-ple occurrence of a word only affects its termweights, not the dimension of the vector.
Notethat for this analysis, we do not perform align-ment that is needed in word error rate measure?
we do not care whether a word appears in theexact location; as long as a word appears in thesegment, its effect on the vector space model isthe same (since it is a bag-of-words model).?
Average node density.
This is the average num-ber of candidate words for each position in theconfusion networks.Figure 2 shows the analysis results for these twometrics, which are the average values on the devel-opment set.
In this analysis we used absolute prun-ing method, and the results are presented for dif-ferent pruning thresholds.
For a comparison, wealso include the results using the 1-best hypotheses(shown as the dotted line in the figure), which has anaverage node density of 1, and the word coverage of71.55%.
When the pruning threshold is 0, the resultscorrespond to the original CNs without pruning.We can see that the confusion networks includemuch more correct words than 1-best hypotheses(word coverage is 89.3% vs. 71.55%).
When in-creasing the pruning thresholds, the word coveragedecreases following roughly a linear pattern.
Whenthe pruning threshold is 0.45, the word coverage ofthe pruned CNs is 71.15%, lower than 1-best hy-potheses.
For node density, the non-pruned CNshave an average density of 11.04.
With a very smallpruning threshold of 0.01, the density decreasesrapidly to 2.11.
The density falls less than 2 whenthe threshold is 0.02, which means that for some01234567891011120 0.01 0.02 0.03 0.04 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5Pruning ThresholdNode Density7075808590Word Coverage(%)node density word coverageFigure 2: Average node density and word coverage of theconfusion networks on the development set.nodes there is only one word candidate preservedafter pruning (i.e., only one word has a posteriorprobability higher than 0.02).
When the thresholdincreases to 0.4, the density is less than 1 (0.99),showing that on average there is less than one candi-date left for each position.
This is consistent with theword coverage results ?
when the pruning thresh-old is larger than 0.45, the confusion networks haveless word coverage than 1-best hypotheses becauseeven the top word hypotheses are deleted.
There-fore, for our following experiments we only use thethresholds ?
?
0.45 for absolute pruning.Note that the results in the figure are based onabsolute pruning.
We also performed analysis us-ing the other two pruning methods described in Sec-tion 3.2.
For those methods, because the decisionis made by comparing each word?s posterior proba-bility with the maximum score for that position, wecan guarantee that at least the best word candidate isincluded in the pruned CNs.
We varied the pruningthreshold from 0 to 0.95 for these pruning methods,and observed similar patterns as in absolute prun-ing for the word coverage and node density analysis.As expected, the fewer word candidates are pruned,the better word coverage and higher node density thepruned CNs have.4.3 Summarization Results4.3.1 Results on dev set using 1-best hypothesisand human transcriptsWe generate the baseline summarization resultusing the best hypotheses from the confusion net-50works.
The summary sentences are extracted usingthe MMR method introduced in Section 3.1.
Theterm weighting is the traditional TFIDF value.
TheROUGE-1 and ROUGE-2 scores for the baseline arelisted in Table 1.Because in this paper our task is to evaluate thesummarization performance using ASR output, wegenerate an oracle result, where the summary ex-traction and IDF calculation are based on the humantranscripts for each ASR segment.
These results arealso presented in Table 1.
Comparing the results forthe two testing conditions, ASR output and humantranscripts, we can see the performance degradationdue to recognition errors.
The difference betweenthem seems to be large enough to warrant investiga-tion of using rich ASR output for improved summa-rization performance.ROUGE-1 ROUGE-2Baseline: best hyp 65.60 26.83Human transcript 69.98 33.21Table 1: ROUGE results (%) using 1-best hypotheses andhuman transcripts on the development set.4.3.2 Results on the dev set using CNsA.
Effect of segmentation representationWe evaluate the effect on summarization usingdifferent vector representations based on confusionnetworks.
Table 2 shows the results on the develop-ment set using various input under the MMR frame-work.
We also include the results using 1-best andhuman transcripts in the table as a comparison.
Thethird row in the table uses the 1-best hypothesis, butthe term weight for each word is calculated by con-sidering its posterior probability in the CNs (denotedby ?wp?).
We calculate the TF and IDF values us-ing Equation 3 and 4 introduced in Section 3.2.
Theother representations in the table are for the non-pruned and pruned CNs based on different pruningmethods, and with or without using the posteriors tocalculate term weights.In general, we find that using confusion networksimproves the summarization performance compar-ing with the baseline.
Since CNs contain more can-didate words and posterior probabilities, a naturalsegment representation ROUGE-1 ROUGE-2Best hyp 65.60 26.83Best hyp (wp) 66.83 29.84Non-pruned CNs 66.58 28.22Non-pruned CNs (wp) 66.47 29.27Pruned CNsAbsolute 67.44 29.02Absolute (wp) 66.98 29.99Max diff 67.29 28.97Max diff (wp) 67.10 29.76Max ratio 67.43 28.97Max ratio (wp) 67.06 29.90Human transcript 69.98 33.21Table 2: ROUGE results (%) on the development set us-ing different vector representations based on confusionnetworks: non-pruned and pruned, using posterior prob-abilities (?wp?)
and without using them.question to ask is, which factor contributes more tothe improved performance?
We can compare the re-sults in Table 2 across different conditions that usethe same candidate words, one with standard TFIDF,and the other with posteriors for TFIDF, or that usedifferent candidate words and the same setup forTFIDF calculation.
Our results show that there ismore improvement using our proposed method forTFIDF calculation based on posterior probabilities,especially ROUGE-2 scores.
Even when just us-ing 1-best hypotheses, if we consider posteriors, wecan obtain very competitive results.
There is alsoa difference in the effect of using posterior proba-bilities.
When using the top hypotheses representa-tion, posteriors help both ROUGE-1 and ROUGE-2scores; when using confusion networks, non-prunedor pruned, using posterior probabilities improvesROUGE-2 results, but not ROUGE-1.Our results show that adding more candidates inthe vector representation does not necessarily helpsummarization.
Using the pruned CNs yields bet-ter performance than the non-pruned ones.
There isnot much difference among different pruning meth-ods.
Overall, the best results are achieved by usingpruned CNs: best ROUGE-1 result without usingposterior probabilities, and best ROUGE-2 scoreswhen using posteriors.B.
Presenting summaries using human tran-scripts51segment representation ROUGE-1 ROUGE-2Best hyp 68.26 32.25Best hyp (wp) 69.16 33.99Non-pruned CNs 69.28 33.49Non-pruned CNs (wp) 67.84 32.95Pruned CNsAbsolute 69.66 34.06Absolute (wp) 69.37 34.25Max diff 69.88 34.17Max diff (wp) 69.38 33.94Max ratio 69.76 34.06Max ratio (wp) 69.44 34.39Human transcript 69.98 33.21Table 3: ROUGE results (%) on the development setusing different segment representations, with the sum-maries constructed using the corresponding human tran-scripts for the selected segments.In the above experiments, we construct the finalsummary using the best hypotheses from the con-fusion networks once the summary sentence seg-ments are determined.
Although we notice obviousimprovement comparing with the baseline results,the ROUGE scores are still much lower than usingthe human transcripts.
One reason for this is thespeech recognition errors.
Even if we select the cor-rect utterance segment as in the reference summarysegments, the system performance is still penalizedwhen calculating the ROUGE scores.
In order toavoid the impact of word errors and focus on evalu-ating whether we have selected the correct segments,next we use the corresponding human transcripts ofthe selected segments to obtain performance mea-sures.
The results from this experiment are shown inTable 3 for different segment representations.We can see that the summaries formed using hu-man transcripts are much better comparing with theresults presented in Table 2.
These two setups usethe same utterance segments.
The only differencelies in the construction of the final summary forperformance measurement, using the top hypothe-ses or the corresponding human transcripts for theselected segments.
We also notice that the differ-ence between using 1-best hypothesis and humantranscripts is greatly reduced using this new sum-mary formulation.
This suggests that the incorrectword hypotheses do not have a very negative im-pact in terms of selecting summary segments; how-ever, word errors still account for a significant partof the performance degradation on ASR conditionwhen using word-based metrics for evaluation.
Us-ing the best hypotheses with their posterior proba-bilities we can obtain similar ROUGE-1 score anda little higher ROUGE-2 score comparing to the re-sults using human transcripts.
The performance canbe further improved using the pruned CNs.Note that when using the non-pruned CNs andposterior probabilities for term weighting, theROUGE scores are worse than most of other condi-tions.
We performed some analysis and found thatone reason for this is the selection of some poorsegments.
Most of the word candidates in the non-pruned CNs have very low confidence scores, result-ing in high IDF values using our proposed methods.Since some top hypotheses are NULL words in thepoorly selected summary segments, it did not affectthe results when using the best hypothesis for eval-uation, but when using human transcripts, it leads tolower precision and worse overall F-scores.
This isnot a problem for the pruned CNs since words withlow probabilities have been pruned beforehand, andthus do not impact segment selection.
We will inves-tigate better methods for term weighting to addressthis issue in our future work.These experimental results prove that using theconfusion networks and confidence scores can helpselect the correct sentence segments.
Even thoughthe 1-best WER is quite high, if we can con-sider more word candidates and/or their confidencescores, this will not impact the process of select-ing summary segments.
We can achieve similarperformance as using human transcripts, and some-times even slightly better performance.
This sug-gests using more word candidates and their confi-dence scores results in better term weighting andrepresentation in the vector space model.
Someprevious work showed that using word confidencescores can help minimize the WER of the extractedsummaries, which then lead to better summarizationperformance.
However, we think the main reasonfor the improvement in our study is from selectingbetter utterances, as shown in Table 3.
In our ex-periments, because different setups select differentsegments as the summary, we can not directly com-pare the WER of extracted summaries, and analyzewhether lower WER is also helpful for better sum-52output summarybest hypotheses human transcriptsR-1 R-2 R-1 R-2Best hyp 65.73 26.79 68.60 32.03Best hyp (wp) 65.92 27.27 68.91 32.69Pruned CNs 66.47 27.73 69.53 34.05Human transcript N/A N/A 69.08 33.33Table 4: ROUGE results (%) on the test set.marization performance.
In our future work, we willperform more analysis along this direction.4.3.3 Experimental results on test setThe summarization results on the test set are pre-sented in Table 4.
We show four different evalua-tion conditions: baseline using the top hypotheses,best hypotheses with posterior probabilities, prunedCNs, and using human transcripts.
For each condi-tion, the final summary is evaluated using the besthypotheses or the corresponding human transcriptsof the selected segments.
The summarization systemsetups (the pruning method and threshold, ?
value inMMR function, and word compression ratio) usedfor the test set are decided based on the results onthe development set.For the results on the test set, we observe sim-ilar trends as on the development set.
Using theconfidence scores and confusion networks can im-prove the summarization performance comparingwith the baseline.
The performance improvementsfrom ?Best hyp?
to ?Best hyp (wp)?
and from ?Besthyp (wp)?
to ?Pruned CNs?
using both ROUGE-1and ROUGE-2 measures are statistically significantaccording to the paired t-test (p < 0.05).
When thefinal summary is presented using the human tran-scripts of the selected segments, we observe slightlybetter results using pruned CNs than using humantranscripts as input for summarization, although thedifference is not statistically significant.
This showsthat using confusion networks can compensate forthe impact from recognition errors and still allow usto select correct summary segments.5 Conclusion and Future WorkPrevious research has shown performance degrada-tion when using ASR output for meeting summa-rization because of word errors.
To address thisproblem, in this paper we proposed to use confu-sion networks for speech summarization.
Under theMMR framework, we introduced a vector represen-tation for the segments by using more word can-didates in CNs and their associated posterior prob-abilities.
We evaluated the effectiveness of usingdifferent confusion networks, the non-pruned ones,and the ones pruned using three different methods,i.e., absolute, max diff and max ratio pruning.
Ourexperimental results on the ICSI meeting corpusshowed that even when we only use the top hypothe-ses from the CNs, considering the word posteriorprobabilities can improve the summarization perfor-mance on both ROUGE-1 and ROUGE-2 scores.By using the pruned CNs we can obtain further im-provement.
We found that more gain in ROUGE-2 results was yielded by our proposed soft termweighting method based on posterior probabilities.Our experiments also demonstrated that it is pos-sible to use confusion networks to achieve similaror even better performance than using human tran-scripts if the goal is to select the right segments.
Thisis important since one possible rendering of summa-rization results is to return the audio segments to theusers, which does not suffer from recognition errors.In our experiments, we observed less improve-ment from considering more word candidates thanusing the confidence scores.
One possible reason isthat the confusion networks we used are too confi-dent.
For example, on average 90.45% of the can-didate words have a posterior probability lower than0.01.
Therefore, even though the correct words wereincluded in the confusion networks, their contribu-tion may not be significant enough because of lowterm weights.
In addition, low probabilities alsocause problems to our proposed soft IDF computa-tion.
In our future work, we will investigate prob-ability normalization methods and other techniquesfor term weighting to cope with these problems.6 AcknowledgmentThis research is supported by NSF award IIS-0845484.
Any opinions expressed in this work arethose of the authors and do not necessarily reflectthe views of NSF.
The authors thank Shih-HsiangLin and Fei Liu for useful discussions.53ReferencesJaime Carbonell and Jade Goldstein.
1998.
The use ofMMR, diversity-based reranking for reordering docu-ments and producing summaries.
In Proceedings ofSIGIR.Ciprian Chelba, Jorge Silva, and Alex Acero.
2007.Soft indexing of speech content for search in spokendocuments.
In Computer Speech and Language, vol-ume 21, pages 458?478.Tee Kiah Chia, Khe Chai Sim, Haizhou Li, and Hwee TouNg.
2008.
A lattice-based approach to query-by-example spoken document retrieval.
In Proceedingsof SIGIR.Gunes Erkan and Dragomir R. Radev.
2004.
LexRank:graph-based lexical centrality as salience in text sum-marization.
Artificial Intelligence Research, 22:457?479.Nikhil Garg, Benoit Favre, Korbinian Reidhammer, andDilek Hakkani-Tur.
2009.
ClusterRank: a graph basedmethod for meeting summarization.
In Proceedings ofInterspeech.Dan Gillick, Korbinian Riedhammer, Benoit Favre, andDilek Hakkani-Tur.
2009.
A global optimizationframework for meeting summarization.
In Proceed-ings of ICASSP.Dilek Hakkani-Tur, Frederic Behet, Giuseppe Riccardi,and Gokhan Tur.
2006.
Beyond ASR 1-best: usingword confusion networks in spoken language under-standing.
Computer Speech and Language, 20(4):495?
514.Chiori Hori and Sadaoki Furui.
2003.
A new approach toautomatic speech summarization.
IEEE Transactionson Multimedia, 5(3):368?378.Adam Janin, Don Baron, Jane Edwards, Dan Ellis,David Gelbart, Nelson Morgan, Barbara Peskin, ThiloPfau, Elizabeth Shriberg, Andreas Stolcke, and ChuckWooters.
2003.
The ICSI meeting corpus.
In Pro-ceedings of ICASSP.Shih-Hsiang Lin and Berlin Chen.
2009.
Improvedspeech summarization with multiple-hypothesis repre-sentations and Kullback-Leibler divergence measures.In Proceedings of Interspeech.Hui Lin, Jeff Bilmes, and Shasha Xie.
2009.
Graph-based submodular selection for extractive summariza-tion.
In Proceedings of ASRU.Chin-Yew Lin.
2004.
ROUGE: A package for auto-matic evaluation of summaries.
In the Workshop onText Summarization Branches Out.Yang Liu, Shasha Xie, and Fei Liu.
2010.
Using n-bestrecognition output for extractive summarization andkeyword extraction in meeting speech.
In Proceedingsof ICASSP.Lidia Mangu, Eric Brill, and Andreas Stolcke.
2000.Finding consensus in speech recognition: word errorminimization and other applications of confusion net-works.
Computer Speech and Language, 14:373?400.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multiplemachine translation systems using enhanced hypothe-ses alignment.
In Proceedings of EACL.Gabriel Murray, Steve Renals, and Jean Carletta.
2005a.Extractive summarization of meeting recordings.
InProceedings of Interspeech.Gabriel Murray, Steve Renals, Jean Carletta, and JohannaMoore.
2005b.
Evaluating automatic summaries ofmeeting recordings.
In Proceedings of the ACL Work-shop on Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation.Khe Chai Sim, William Byrne, Mark Gales, HichemSahbi, and Phil Woodland.
2007.
Consensus net-work decoding for statistical machine translation sys-tem combination.
In Proceedings of ICASSP.Andreas Stolcke, Yochai Konig, and Mitchel Weintraub.1997.
Explicit word error minimization in N-best listrescoring.
In Proceedings of Eurospeech.Andreas Stolcke, Barry Chen, Horacio Franco,Venkata Ra mana Rao Gadde, Martin Graciarena,Mei-Yuh Hwang, Katrin Kirchhoff, Arindam Mandal,Nelson Morgan, Xin Lei, Tim Ng, and et al 2006.Recent innovations in speech-to-text transcription atSRI-ICSI-UW.
IEEE Transactions on Audio, Speech,and Language Processing, 14(5):1729?1744.Robin Valenza, Tony Robinson, Marianne Hickey, andRoger Tucker.
1999.
Summarization of spoken audiothrough information extraction.
In Proceedings of theESCA Workshop on Accessing Information in SpokenAudio, pages 111?116.Shasha Xie and Yang Liu.
2008.
Using corpusand knowledge-based similarity measure in maximummarginal relevance for meeting summarization.
InProceedings of ICASSP.Shasha Xie, Benoit Favre, Dilek Hakkani-Tur, and YangLiu.
2009.
Leveraging sentence weights in concept-based optimization framework for extractive meetingsummarization.
In Proceedings of Interspeech.Klaus Zechner and Alex Waibel.
2000.
Minimizing worderror rate in textual summaries of spoken language.
InProceedings of NAACL.Jian Zhang, Ho Yin Chan, Pascale Fung, and Lu Cao.2007.
A comparative study on speech summarizationof broadcast news and lecture speech.
In Proceedingsof Interspeech.Xiaodan Zhu and Gerald Penn.
2006.
Summarization ofspontaneous conversations.
In Proceedings of Inter-speech.54
