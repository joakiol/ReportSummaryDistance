Workshop on Humans and Computer-assisted Translation, pages 38?46,Gothenburg, Sweden, 26 April 2014.c?2014 Association for Computational LinguisticsThe Impact of Machine Translation Quality on Human Post-editingPhilipp Koehn?pkoehn@inf.ed.ac.ukCenter for Speech and Language ProcessingThe Johns Hopkins UniversityUlrich Germann?ugermann@inf.ed.ac.uk?School of InformaticsUniversity of EdinburghAbstractWe investigate the effect of four differentcompetitive machine translation systemson post-editor productivity and behaviour.The study involves four volunteers post-editing automatic translations of news sto-ries from English to German.
We see sig-nificant difference in productivity due tothe systems (about 20%), and even biggervariance between post-editors.1 IntroductionStatistical machine translation (SMT) has madeconsiderable progress over the past two decades.Numerous recent studies have shown productivityincreases with post-editing of MT output over tra-ditional work practices in human translation (e.g.,Guerberof, 2009; Plitt and Masselot, 2010; Garcia,2011; Pouliquen et al., 2011; Skadin?
?s et al., 2011;den Bogaert and Sutter, 2013; Vazquez et al.,2013; Green et al., 2013; L?aubli et al., 2013).The advances in statistical machine translationover the past years have been driven to a large ex-tent by frequent (friendly) competitive MT eval-uation campaigns, such as the shared tasks at theACL WMT workshop series (Bojar et al., 2013)and IWSLT (Cettolo et al., 2013), and the NISTOpen MT Evaluation.1These evaluations usu-ally apply a mix of automatic evaluation metrics,most prominently the BLEU score (Papineni et al.,2001), and more subjective human evaluation cri-teria such as correctness, accuracy, and fluency.How the quality increases measured by auto-matic metrics and subjective evaluation criteria re-late to actual increases in the productivity of post-editors is still an open research question.
It isalso not clear yet if some machine translation ap-proaches ?
say, syntax-based models ?
are bet-ter suited for post-editing than others.
These re-lationships may very well also depend on the lan-1http://www.nist.gov/itl/iad/mig/openmt.cfmguage pair in question and the coarse level of MTquality, from barely good enough for post-editingto almost perfect.The pilot study presented in this paper investi-gates the influence of the underlying SMT systemon post-editing effort and efficiency.
The studyfocuses on translation of general news text fromEnglish into German, with translations created bynon-professional post-editors working on outputfrom four different translation systems.
The datagenerated by this study is available for download.2We find that the better systems lead to a produc-tivity gain of roughly 20% and carry out in-depthanalysis of editing behavior.
A significant find-ing is the high variance in work styles between thedifferent post-editors, compared to the impact ofmachine translation systems.2 Related WorkKoponen (2012) examined the relationship be-tween human assessment of post-editing effortsand objective measures such as post-editing timeand number of edit operations.
She found that seg-ments that require a lot of reordering are perceivedas being more difficult, and that long sentencesare considered harder, even if only few wordschanged.
She also reports larger variance betweentranslators in post-editing time than in post-editingoperations ?
a finding that we confirm here aswell.From a detailed analysis of the types of ed-its performed in sentences with long versus shortpost-edit times, Koponen et al.
(2012) concludethat the observed differences in edit times can beexplained at least in part also by the types of nec-essary edits and the associated cognitive effort.Deleting superfluous function words, for exam-ple, appears to be cognitively simple and takeslittle time, whereas inserting translations for un-translated words requires more cognitive effort2http://www.casmacat.eu/index.php?n=Main.Downloads38Table 1: News stories used in the study (size is given in number of sentences)Source Size TitleBBC 49 Norway?s rakfisk: Is this the world?s smelliest fish?BBC 47 Mexico?s Enrique Pena Nieto faces tough startCNN 45 Bradley Manning didn?t complain about mistreatment, prosecutors contendCNN 63 My Mexican-American identity crisisEconomist 55 Old battles, new Middle EastGuardian 38 Cigarette plain packaging laws come into force in AustraliaNY Times 61 In a Constantly Plugged-In World, It?s Not All Bad to Be BoredNY Times 47 In Colorado, No Playbook for New Marijuana LawTelegraph 95 Petronella Wyatt: I was bullied out of Oxford for being a Toryand takes longer.
They also compare post-editingstyles of different post-editors working on identi-cal post-editing tasks.Another study by Koponen (2013) showed thatinter-translator variance is lower in a controlledlanguage setting when translators are given thechoice of output from three different machinetranslation systems.In the realm of machine translation research,there has been an increasing interest in the useof MT technology by post-editors.
A major pushare the two EU-funded research projects MATE-CAT3and CASMACAT4, which are developing anopen source translation and post-editing work-bench (Federico et al., 2012; Alabau et al., 2013).At this point, we are not aware of any study thatcompares directly the impact of different machinetranslation systems on post-editor productivity andbehaviour.3 Experimental DesignWe thus carried out an experiment on an English?German news translation task, using output fromfour different SMT systems, post-edited by fluentbilingual native speakers of German with no priorexperience in professional translation.3.1 The Translation TaskThe Workshop on Statistical Machine Translation(Bojar et al., 2013) organises an annual evaluationcampaign for machine translation systems.
Thesubject matter is translation of news stories fromsources such as the New York Times or the BBC.We decided to use output from systems submit-ted to this evaluation campaign, not only because3http://www.matecat.com/4http://www.casmacat.eu/their output is freely available,5but also becauseit comes with automatic metric scores and humanjudgements of the translation quality.The translation direction we chose wasEnglish?German, partly due to convenience (theauthors of this study are fluent in both languages),but also because this language pair poses specialchallenges to current machine translation technol-ogy, due to the syntactic divergence of the twolanguages.We selected data from the most recent evalua-tion campaign.
The subset chosen for our post-editing task comprises 9 different news stories,originally written in English, with a total of 500sentences.
Details are shown in Table 1.3.2 Machine Translation SystemsA total of 15 different machine translation systemsparticipated in the evaluation campaign.
We se-lected four different systems that differ in their ar-chitecture and use of training data:?
an anonymized popular online translationsystem built by a large Internet company(ONLINE-B)?
the syntax-based translation system of theUniversity of Edinburgh (UEDIN-SYNTAX;Nadejde et al., 2013)?
the phrase-based translation system of theUniversity of Edinburgh (UEDIN-PHRASE;Durrani et al., 2013)?
the machine translation system of the Univer-sity of Uppsala (UU; Stymne et al., 2013)In the 2013 WMT evaluation campaign, the sys-tems translated a total of 3000 sentences, and their5http://www.statmt.org/wmt13/results.html39Table 2: Machine translation systems used in thestudy, with quality scores in the WMT 2013 eval-uation campaign.System BLEU SUBJECTIVEONLINE-B 20.7 0.637UEDIN-SYNTAX 19.4 0.614UEDIN-PHRASE 20.1 0.571UU 16.1 0.361output was judged with the BLEU score against aprofessional reference translation and by subjec-tive ranking.
The scores obtained for the differentsystems on the full test set are shown in Table 2.The first three systems are fairly close in qual-ity (although the differences in subjective hu-man judgement scores are statistically significant),whereas the fourth system (UU) clearly lags be-hind.
The best system ONLINE-B was ranked firstaccording to human judgement and thus can beconsidered state of the art.From casual observation, the syntax-based sys-tem UEDIN-SYNTAX succeeds more frequentlyin producing grammatically correct translations.The phrase-based system UEDIN-PHRASE, eventhough trained on the same parallel data, hashigher coverage since it does not have the require-ment that translation rules have to match syntac-tic constituents in the target language, which wepresume is the main cause behind the lower BLEUscore.
The two systems use the same languagemodel.System UU is also a phrase based system, with adecoder that is able to consider the document levelcontext.
It was trained on smaller corpora for boththe translation model and the language model.We do not have any insight into the systemONLINE-B, but we conjecture that it is a phrase-based system with syntactic pre-reordering trainedon much larger data sets, but not optimised to-wards the news domain.Notice the inconsistency between BLEU scoreand subjective score for the two systems from theUniversity of Edinburgh.
Results from other eval-uations have also shown (Callison-Burch et al.,2012) that current automatic evaluation metricsdo not as much as human judges appreciate thestrengths of the syntax-based system, which buildssyntactic structures in the target language dur-ing translation.
Hence, we were particularly in-terested how the syntax-based system fares withpost-editors.As mentioned above, the nine documents chosenfor the post-editing task analysed in this paper (cf.Table 1) were part of the WMT 2013 evaluationdata set.
All nine documents had English as theoriginal source language.3.3 Post-EditorsWe recruited four English-German bilingual, na-tive German post-editors.
Three were students,staff, or faculty at the University of Edinburgh;the fourth had been previously employed on a con-tractual basis for linguistic annotation work.6Thepost-editors had no professional experience withtranslation, and differed in language skills.3.4 Assignment of MT OutputThe goal of this study was to investigate how post-editors?
behaviour and productivity are influencedby the quality of the underlying machine transla-tion system.
Ideally, we would want to presentoutput from different systems to the same post-editor and see how their observable behaviourchanges.However, a post-editor who has seen the out-put from one MT system for a sentence will beat an advantage when post-editing the output froma second system, by having already spent signif-icant time understanding the source sentence andconsidering the best translation choices.Hence we used 4 different post-editors, each topost-edit the output in equal amounts from each ofthe 4 machine translation systems under investiga-tion, so that each post-editor worked on each sen-tence once and the entire output from all systemswas post-edited once by one of the 4 post-editors.A concern in this setup is that we never knowif we measure differences in post-editors or differ-ences in machine translations systems when com-paring the behaviour for any given sentence.Therefore, each post-editor was assigned atranslation for each sentence randomly from anyof the machine translation systems.
This randomassignment allows us to marginalise out the depen-dence on the post-editor when assessing statisticsfor the different systems.6The ordering here does not reflect the order of post-editorsin the discussion later in this paper.40Table 3: Post-editing speed by editor and system.Systemseconds / word words / hour1 2 3 4 mean 1 2 3 4 meanONLINE-B 2.95 4.69 9.16 4.98 5.46 1,220 768 393 723 659UEDIN-PHRASE 3.04 5.01 9.22 4.70 5.45 1,184 719 390 766 661UEDIN-SYNTAX 3.03 4.41 9.20 4.97 5.38 1,188 816 391 724 669UU 3.11 5.01 11.59 5.58 6.35 1,158 719 311 645 567mean per editor 3.03 4.78 9.79 5.05 1,188 753 368 7134 ProductivityThe primary argument for post-editing machinetranslation output as opposed to more traditionalapproaches is the potential gain in productivity.
Iftranslation professionals can work faster with ma-chine translation, then this has real economic ben-efits.
There are also other considerations, for ex-ample that post-editing might be done by profes-sionals that are less skilled in the source language(Koehn, 2010).We measure productivity by time spent on eachsentence.
This is not a perfect measure.
Whenworking on a news story, post-editors tend tospeed up when moving down the story since theyhave already solved some reoccurring translationproblems and get more familiar with the context.4.1 Productivity by MT SystemOur main interests is the average translation speed,broken down by machine translation system.
Thecolumns labelled ?mean?
in Table 3 show the re-sults.
While the differences are not big for the topthree systems, the syntax-based system comes outon top.We used bootstrap resampling to test the speeddifferences for statistical significance.
Only sys-tem UU is significantly worse than the others (atp-level < 0.01), with about 20% lower productiv-ity.4.2 Productivity by Post-EditorPost-editing speed is very strongly influenced bythe post-editor?s skill and effort.
Our post-editorswere very diverse, showing large differences intranslation speed.
See the columns labelled 1 to4 in Table 3 for details.In particular, post-editor 3 took more than threetimes as much time as the fastest (PE 1).
Accord-ing to a post-study interview with Post-Editor 3,there were two reasons for this.
First, the post-editor was feeling a bit ?under the weather?
dur-ing the study and found it hard to focus.
Second,(s)he found the texts very difficult to translate andstruggled with idiomatic expressions and culturalreferences that (s)he did not understand immedi-ately.4.3 Productivity by System and Post-EditorWhile the large differences between the post-editors are unfortunate when the goal is consis-tency in results, they provide some data on howpost-editors of different skill levels are influencedby the quality of the machine translation systems.Table 3 breaks down translation speed by ma-chine translation system and post-editor.
Interest-ingly, machine translation quality has hardly anyeffect on the fast Post-Editor 1, and the lowerMT performance of system UU affects only Post-Editors 3 and 4.
Post-Editor 2 is noticeably fasterwith UEDIN-SYNTAX ?
an effect that cannot beobserved for the other post-editors.
The differ-ences between the other systems are not large forany of the post-editors.Statistically significant ?
as determined bybootstrap resampling ?
are only the differencesin post-editing speed for Post-Editor 3 with sys-tem UU versus ONLINE-B and UEDIN-PHRASE atp-level < 0.01, and against UEDIN-SYNTAX at p-level <0.02, and for Post-Editor 4 for UU versusUEDIN-PHRASE at p-level < 0.05.
Note that theabsence of statistical significance in our data hasmuch to do with the small sample size; more ex-tensive experiments may be necessary to ensuremore solid findings.5 Translation Edit RateGiven the inherent difficulties in obtaining tim-ing information, we can also measure the impactof machine translation system quality on post-editing effort in terms of how much the post-editors change the machine translation output, asdone, for example in Cettolo et al.
(2013).41Table 4: Edit rate and types of edits per systemSystem HTER ins del sub shift wide shiftONLINE-B 35.7 4.8 7.4 18.9 4.6 5.8UEDIN-PHRASE 37.9 5.5 7.4 20.0 5.0 6.6UEDIN-SYNTAX 36.7 4.7 7.6 19.8 4.6 5.7UU 43.7 4.6 11.4 21.9 5.8 7.2Table 5: Edit rate and types of edits per post-editorP-E HTER ins del sub shiftwideshift1 35.2 5.4 6.7 18.7 4.4 5.32 43.1 4.1 10.4 23.1 5.4 6.93 37.7 5.9 7.9 18.8 5.0 6.64 37.5 4.3 8.5 19.6 5.1 6.4There are two ways to measure how much themachine translation output was edited by the post-editor.
One way is to compare the final translationwith the original machine translation output.
Thisis what we will do in this section.
In Section 6,we will consider which parts of the final transla-tion were actually changed by the post-editor anddiscuss the difference.5.1 HTER as Quality MeasureThe edit distance between machine translationoutput and human reference translation can bemeasured in the number of insertions, deletions,substitutions and (phrasal) moves.
A metric thatsimply counts the minimal number of such edit op-erations and divides it by the length of the humanreference translation is the translation edit rate,short TER (Snover et al., 2006).If the human reference translation is createdfrom the machine translation output to minimisethe number of edit operations needed for an ac-ceptable translation, this variant is called human-mediated TER, or HTER.
Note that in our experi-ment the post-editors are not strictly trying to min-imise the number of edit operations ?
they maybe inclined to make additional changes due to ar-bitrary considerations of style or perform edits thatare faster rather than minimise the number of oper-ations (e.g., deleting whole passages and rewritingthem).5.2 Edits by MT SystemTable 4 shows the HTER scores ?
keep in mindour desiderata above ?
for the four systems.
Thescores are similar to the productivity number, withthe three leading systems close together and thetrailing system UU well behind.Notably, we draw more statistically significantdistinctions here.
While as above, UU is signif-icantly worse than all other systems (p-level <0.01), we also find that ONLINE-B is better thanUEDIN-PHRASE (p-level < 0.01).Hence, HTER is a more sensitive metric thantranslation speed.
This may be due to the factthat the time measurements are noisier than thecount of edit operations.
But it may also becauseHTER and productivity (i.e., time) do not measurethe exactly the same thing.
For instance, edits thatrequire only a few keystrokes may be cognitivelydemanding (e.g., terminological choices), and thustake more time.We cannot make any strong claim based onour numbers, but it is worth pointing out thatpost-editing UEDIN-SYNTAX was slightly fasterthan ONLINE-B (by 0.08 seconds/word), while theHTER score is lower (by 1 point).
A closer lookat the edit operations reveals that the post-editof UEDIN-SYNTAX output required slightly fewershort and long shifts (movements of phrases), butmore substitutions.
Intuitively, moving a phrasearound is a more time-consuming task than replac-ing a word.
The benefit of a syntax-based sys-tem that aims to produce correct syntactic struc-ture (including word order), may have real benefitsin terms of post-editing time.5.3 Edits by Post-EditorTable 5 displays the edit rate broken down by post-editor.
There is little correlation between edit rateand post-editor speed.
While the fastest Post-Editor 1 produces translations with the smallestedit rate, the difference to two of the others (in-cluded the slowest Post-Editor 3) is not large.
The42+--------+--------------------------------------------------------------------------------------------------------+| sec | current_translation |+--------+--------------------------------------------------------------------------------------------------------+| 0.000 | Norwegen ist es nicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint.
|| 0.508 | Norwegen ist esnicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint.
|| 0.733 | Norwegen ist enicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint.
|| 0.970 | Norwegen ist enicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint.
|| 0.975 | Norwegen ist nicht oft auf die globale Agenda - und die meisten Nachrichten, wie es scheint.
|| 3.317 | Norwegen ist nicht oft auf di globale Agenda - und die meisten Nachrichten, wie es scheint.
|| 3.413 | Norwegen ist nicht oft auf d globale Agenda - und die meisten Nachrichten, wie es scheint.
|| 3.524 | Norwegen ist nicht oft auf de globale Agenda - und die meisten Nachrichten, wie es scheint.
|| 3.588 | Norwegen ist nicht oft auf der globale Agenda - und die meisten Nachrichten, wie es scheint.
|| 5.116 | Norwegen ist nicht oft auf der globalen Agenda - und die meisten Nachrichten, wie es scheint.
|| 17.986 | Norwegen ist nicht oft auf der globalen Agenda - und die meisten , wie es scheint.
|| 19.386 | Norwegen ist nicht oft auf der globalen NachrichtenAgenda - und die meisten , wie es scheint.
|| 20.116 | Norwegen ist nicht oft auf der globalen Nachrichtengenda - und die meisten , wie es scheint.
|| 20.196 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten , wie es scheint.
|| 20.298 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten , wie es scheint.
|| 29.596 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheint.
|| 31.459 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten schein.
|| 31.659 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheine.
|| 31.796 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen.
|| 32.060 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen .
|| 34.283 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen d. || 34.380 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen da.
|| 34.443 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das.
|| 34.636 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das .
|| 35.507 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das s. || 35.637 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so.
|| 35.744 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so .
|| 35.949 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so z.
|| 36.053 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu.
|| 36.166 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu .
|| 36.805 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu w. || 38.011 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu wo.
|| 38.394 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu wol.
|| 38.699 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu woll.
|| 38.795 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu wolle.
|| 38.947 | Norwegen ist nicht oft auf der globalen Nachrichtenagenda - und die meisten scheinen das so zu wollen.
|+--------+--------------------------------------------------------------------------------------------------------+| char | mmmmmmmmmmmmmmmmmmmmmmmmmmmmttmmmmmmmmtmppppppppppptmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmtttttttttttttttttttm || word | m m m m m x x x m m m m x t t t t m |+--------+--------------------------------------------------------------------------------------------------------+Figure 1: Analysis of the post-editing process: Most editing actions that result in changes in the transla-tion are adding or deleting of a character by a keystroke.
However, notice the cut (17.986 sec) and paste(19.386 sec) action.
Each character can be traced either to the original machine translation output (m), atyping action of the post-editor (t), or a pasting action of the post-editor (p).
This then allows tokens inthe output classified as either original MT (m), typed (t), pasted (not in figure) or partially edited (x).outlier here is Post-Editor 2, whose output has amuch larger edit rate.6 Editing ActionsThe HTER is an analysis of the product of post-editing.
The final translation is compared to theoriginal machine translation output.
In this sec-tion, we examine how the process of post-editingis influenced by the machine translation system.Our post-editing workbench provides detailed log-ging of each HCI interaction (key strokes, mouseclicks, etc.).
This allows us to reconstruct how atranslation was generated.
See Figure 1 for an ex-ample how a translated was edited.Table 6: Character provenance by systemSystem MT typed pastedONLINE-B 68.3 28.0 3.3UEDIN-PHRASE 62.9 31.3 5.2UEDIN-SYNTAX 65.9 29.1 4.5UU 56.1 37.9 5.66.1 Character Provenance by SystemIf we follow the editing actions, we can trace theorigin of each letter in the final output: was it partof the original MT output, was it typed in by theuser, or moved in a cut and paste action?
Table 6breaks down the characters in the final translations43Table 7: Token provenance by systemSystem MT typed pasted editedONLINE-B 65.2 21.4 2.3 10.8UEDIN-PHRASE 60.5 24.7 3.9 10.6UEDIN-SYNTAX 62.6 22.4 3.4 11.3UU 53.2 31.0 4.0 11.7by origin for each system.
The numbers corre-spond to the HTER scores, with a remarkable con-sistency ranking for typed and pasted characters.6.2 Token Provenance by SystemWe perform a similar analysis on the word level,introducing a fourth type of provenance: wordswhose characters are of mixed origin, i.e., wordsthat were partially edited.
Table 7 shows the num-bers for each machine translation system.
The sus-picion from the HTER score that the syntax-basedsystem UEDIN-SYNTAX requires less movement isnot confirmed by these numbers.
There are sig-nificantly more words moved by pasting (3.4%)than for ONLINE-B (2.3%).
In general, cutting andpasting is not as common as the HTER score wouldsuggest: the two types of shifts moved 10.3% and10.2% of phrases, respectively.
It seems that mostwords that could be moved are rather deleted andtyped again.6.3 Behaviour By Post-EditorThe post-editors differ significantly in their be-haviour, as the numbers in Table 8 illustrate.
Post-Editor 1, who is the fastest, leaves the most char-acters unchanged (72.9% vs. 57.7?64.4% for theothers).
Remarkably, this did not result in a dra-matically lower HTER score (recall: 35.2 vs. 37.5?43.1 for the others).Post-Editor 3, while taking the longest time,does not change the most number of characters.However, (s)he uses dramatically more cutting andpasting.
Is this activity particularly slow?
Oneway to check is to examine more closely how theTable 8: Character provenance by post-editorPost-Editor MT typed pasted1 72.9 22.9 3.52 57.7 39.4 2.73 58.9 29.5 10.74 64.4 33.5 1.9post-editors spread out their actions over time.7 Editing ActivitiesKoehn (2009) suggests to divide up the time spentby translators and post-editors into intervals of thefollowing types:?
initial pauses: the pause at the beginning ofthe translation, if it exists?
end pause: the pause at the end of the trans-lation, if it exists?
short pause of length 2?6 seconds?
medium pauses of length 6?60 seconds?
big pauses longer than 60 seconds?
various working activities (in our case justtyping and mouse actions)When we break up the time spent on each activ-ity and normalise it by the number of words inthe original machine translation output, we get thenumbers in Table 9, per machine translation sys-tem and post-editor.The worse quality of the UU system causesmainly more work activity, big medium pauses.Each contributes roughly 0.3 seconds per word.The syntax-based system UEDIN-SYNTAX maypose fewer hard translation problems (showing upin initial and big pauses) than the HTER-preferredONLINE-B system, but the effect is not strong.We noted that ONLINE-B has a statistically sig-nificant better HTER score than UEDIN-PHRASE.While this is reflected in the additional workingactivity for the latter (2.41 sec./word vs. 2.26sec./word), time is made up in the pauses.
Our datais not sufficiently conclusive to gain any deeper in-sight here ?
it is certainly a question that we wantto explore in the future.The difference in post-editors mirrors some ofthe earlier findings: The number of characters andwords changed leads to longer working activity,but the slow Post-Editor 3 is mainly slowed downby initial, big and medium pauses, indicating diffi-culties with solving translation problems, and notslow cutting and pasting actions.
The faster Post-Editor 1 rarely pauses long and is quick with typ-ing and mouse movements.8 ConclusionWe compared how four different machine trans-lation systems affect post-editing productivity andbehaviour by analysing final translations and user44Table 9: Time spent on different activities, by machine translation system (top) and post-editor (bottom).System initial pause big pause med.
pause short pause end pause workingONLINE-B 0.37 s/w 0.61 s/w 1.88 s/w 0.30 s/w 0.00 s/w 2.26 s/wUEDIN-PHRASE 0.32 s/w 0.55 s/w 1.74 s/w 0.32 s/w 0.00 s/w 2.41 s/wUEDIN-SYNTAX 0.32 s/w 0.50 s/w 1.90 s/w 0.31 s/w 0.00 s/w 2.30 s/wUU 0.28 s/w 0.74 s/w 2.14 s/w 0.34 s/w 0.00 s/w 2.75 s/wPost-Editor initial pause big pause med.
pause short pause end pause working1 0.35 s/w 0.01 s/w 0.63 s/w 0.27 s/w 0.00 s/w 1.76 s/w2 0.04 s/w 0.19 s/w 1.13 s/w 0.35 s/w 0.00 s/w 3.06 s/w3 0.91 s/w 1.85 s/w 3.99 s/w 0.29 s/w 0.00 s/w 2.53 s/w4 0.02 s/w 0.36 s/w 1.94 s/w 0.35 s/w 0.00 s/w 2.33 s/wactivity data.
The best system under considera-tion yielded abut 20% better productivity than theworst, although the three systems on top are notstatistically significantly different in terms of pro-ductivity.We noted differences in metrics that measureproductivity and edit distance metrics.
The lat-ter allowed us to draw more statistically significantconclusions, but may measure something distinct.Productivity is the main concern of commercialuse of post-editing machine translation, and wefind that better machine translation leads to lesstime spent on editing, but more importantly, lesstime spent of figuring out harder translation prob-lems (indicated by pauses of more than six sec-onds).Finally, an important finding is that the differ-ences between post-editors is much larger than thedifference between machine translation systems.This points towards the importance of skilled post-editors, but this finding should be validated withprofessional post-editors, and not the volunteersused in this study.AcknowledgementsThis work was supported under the CASMACATproject (grant agreement No287576) by theEuropean Union 7thFramework Programme(FP7/2007-2013).ReferencesAlabau, Vicent, Ragnar Bonk, Christian Buck, Michael Carl,Francisco Casacuberta, Mercedes Garc??a-Mart?
?nez, Jes?usGonz?alez, Philipp Koehn, Luis Leiva, Bartolom?e Mesa-Lao, Daniel Ortiz, Herve Saint-Amand, Germ?an San-chis, and Chara Tsoukala.
2013.
?CASMACAT: An opensource workbench for advanced computer aided transla-tion.?
The Prague Bulletin of Mathematical Linguistics,100:101?112.Bojar, Ond?rej, Christian Buck, Chris Callison-Burch, Chris-tian Federmann, Barry Haddow, Philipp Koehn, ChristofMonz, Matt Post, Radu Soricut, and Lucia Specia.
2013.?Findings of the 2013 Workshop on Statistical MachineTranslation.?
Proceedings of the Eighth Workshop on Sta-tistical Machine Translation, 1?44.
Sofia, Bulgaria.Callison-Burch, Chris, Philipp Koehn, Christof Monz, MattPost, Radu Soricut, and Lucia Specia.
2012.
?Findingsof the 2012 workshop on statistical machine translation.
?Proceedings of the Seventh Workshop on Statistical Ma-chine Translation, 10?48.
Montreal, Canada.Cettolo, Mauro, Jan Niehues, Sebastian St?uker, Luisa Ben-tivogli, and Marcello Federico.
2013.
?Report on the10th IWSLT evaluation campaign.?
Proceedings of theInternational Workshop on Spoken Language Translation(IWSLT).den Bogaert, Joachim Van and Nathalie De Sutter.
2013.?Productivity or quality?
Let?s do both.?
Machine Trans-lation Summit XIV, 381?390.Durrani, Nadir, Barry Haddow, Kenneth Heafield, andPhilipp Koehn.
2013.
?Edinburgh?s machine translationsystems for European language pairs.?
Proceedings of theEighth Workshop on Statistical Machine Translation, 114?121.
Sofia, Bulgaria.Federico, Marcello, Alessandro Cattelan, and Marco Trom-betti.
2012.
?Measuring user productivity in machinetranslation enhanced computer assisted translation.?
Pro-ceedings of the Tenth Conference of the Association forMachine Translation in the Americas (AMTA).Garcia, Ignacio.
2011.
?Translating by post-editing: is it theway forward??
Machine Translation, 25(3):217?237.Green, Spence, Jeffrey Heer, and Christopher D. Manning.2013.
?The efficacy of human post-editing for languagetranslation.?
ACM Human Factors in Computing Systems(CHI).Guerberof, Ana.
2009.
?Productivity and quality in mt post-editing.?
MT Summit Workshop on New Tools for Transla-tors.Koehn, Philipp.
2009.
?A process study of computer-aidedtranslation.?
Machine Translation, 23(4):241?263.Koehn, Philipp.
2010.
?Enabling monolingual translators:Post-editing vs.
options.?
Human Language Technolo-gies: The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguistics,537?545.
Los Angeles, California.Koponen, Maarit.
2012.
?Comparing human perceptionsof post-editing effort with post-editing operations.?
Pro-45ceedings of the Seventh Workshop on Statistical MachineTranslation, 227?236.
Montreal, Canada.Koponen, Maarit.
2013.
?This translation is not too bad: ananalysis of post-editor choices in a machine-translationpost-editing task.?
Proceedings of Workshop on Post-editing Technology and Practice, 1?9.Koponen, Maarit, Wilker Aziz, Luciana Ramos, and LuciaSpecia.
2012.
?Post-editing time as a measure of cognitiveeffort .?
AMTA 2012 Workshop on Post-Editing Technol-ogy and Practice (WPTP 2012), 11?20.
San Diego, USA.L?aubli, Samuel, Mark Fishel, Gary Massey, MaureenEhrensberger-Dow, and Martin Volk.
2013.
?Assessingpost-editing efficiency in a realistic translation environ-ment.?
Proceedings of Workshop on Post-editing Technol-ogy and Practice, 83?91.Nadejde, Maria, Philip Williams, and Philipp Koehn.
2013.?Edinburgh?s syntax-based machine translation systems.
?Proceedings of the Eighth Workshop on Statistical Ma-chine Translation, 170?176.
Sofia, Bulgaria.Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-JingZhu.
2001.
BLEU: a Method for Automatic Evaluation ofMachine Translation.
Tech.
Rep. RC22176(W0109-022),IBM Research Report.Plitt, Mirko and Francois Masselot.
2010.
?A productivitytest of statistical machine translation post-editing in a typi-cal localisation context.?
Prague Bulletin of MathematicalLinguistics, 93:7?16.Pouliquen, Bruno, Christophe Mazenc, and Aldo Iorio.
2011.?Tapta: A user-driven translation system for patent docu-ments based on domain-aware statistical machine trans-lation.?
Proceedings of th 15th International Confer-ence of the European Association for Machine Translation(EAMT), 5?12.Skadin?
?s, Raivis, Maris Purin?
?s, Inguna Skadin?a, and AndrejsVasil?jevs.
2011.
?Evaluation of SMT in localization tounder-resourced inflected language.?
Proceedings of the15th International Conference of the European Associa-tion for Machine Translation (EAMT), 35?40.Snover, Matthew, Bonnie J. Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
?A study of transla-tion edit rate with targeted human annotation.?
5th Con-ference of the Association for Machine Translation in theAmericas (AMTA).
Boston, Massachusetts.Stymne, Sara, Christian Hardmeier, J?org Tiedemann, andJoakim Nivre.
2013.
?Tunable distortion limits and cor-pus cleaning for SMT.?
Proceedings of the Eighth Work-shop on Statistical Machine Translation, 225?231.
Sofia,Bulgaria.Vazquez, Lucia Morado, Silvia Rodriguez Vazquez, and Pier-rette Bouillon.
2013.
?Comparing forum data post-editingperformance using translation memory and machine trans-lation output: A pilot study.?
Machine Translation SummitXIV, 249?256.46
