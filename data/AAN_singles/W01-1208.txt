Question Answering Using Encyclopedic KnowledgeGenerated from the WebAtsushi FujiiUniversity of Library andInformation Science1-2 Kasuga, Tsukuba305-8550, JapanCREST, Japan Science andTechnology Corporationfujii@ulis.ac.jpTetsuya IshikawaUniversity of Library andInformation Science1-2 Kasuga, Tsukuba305-8550, Japanishikawa@ulis.ac.jpAbstractWe propose a question answering sys-tem which uses an encyclopedia as aknowledge base.
However, since ex-isting encyclopedias lack technical/newterms, we use an encyclopedia automat-ically generated from the World WideWeb.
For this purpose, we first searchthe Web for pages containing a termin question.
Then linguistic patternsand HTML structures are used to ex-tract text fragments describing the term.Finally, extracted term descriptions areorganized based on word senses anddomains.
We also evaluate our sys-tem by way of experiments, where theJapanese Information-Technology En-gineers Examination is used as a testcollection.1 IntroductionMotivated partially by the TREC-8 QA collec-tion (Voorhees and Tice, 2000), question answer-ing has of late become one of the major topicswithin the natural language processing and infor-mation retrieval communities, and a number ofQA systems targeting the TREC collection havebeen proposed (Harabagiu et al, 2000; Moldovanand Harabagiu, 2000; Prager et al, 2000).Although Harabagiu et al (2000) proposed aknowledge-based QA system, most existing sys-tems rely on conventional IR and shallow NLPmethods.
However, question answering is inher-ently a more complicated procedure that usuallyrequires explicit knowledge bases.In this paper, we propose a question answeringsystem which uses an encyclopedia as a knowl-edge base.
However, since existing (published)encyclopedias usually lack technical/new terms,we generate one based on the World Wide Web,which includes a number of technical and recentinformation.
For this purpose, we use a modifiedversion of our method to extract term descriptionsfrom Web pages (Fujii and Ishikawa, 2000).Intuitively, our system answers interrogativequestions like ?What is X??
in which a QA sys-tem searches an encyclopedia database for one ormore descriptions related to term X.The performance of QA systems can be evalu-ated based on coverage and accuracy.
Coverageis the ratio between the number of questions an-swered (disregarding their correctness) and the to-tal number of questions.
Accuracy is the ratio be-tween the number of correct answers and the totalnumber of answers made by the system.
Whilecoverage can be estimated objectively and sys-tematically, estimating accuracy relies on humansubjects (because it is difficult to define the abso-lute description for term X), and thus is expensive.In view of this problem, we use as a test col-lection Information Technology Engineers Exam-inations1, which are biannual examinations nec-essary for candidates to qualify to be IT engineersin Japan.Among a number of classes, we focus on the?Class II?
examination, which requires funda-1Japan Information-Technology Engineers ExaminationCenter.
http://www.jitec.jipdec.or.jp/mental and general knowledge related to informa-tion technology.
Approximately half of questionsare associated with IT technical terms.
Since pastexaminations and answers are open to the pub-lic, we can objectively evaluate the performanceof our QA system with minimal cost.Our system is not categorized into ?open-domain?
systems, where questions expressed innatural language are not limited to explicit axesincluding who, what, when, where, how and why.However, Moldovan and Harabagiu (2000)found that each of the TREC questions can be re-cast as either a single axis or a combination ofaxes.
They also found that out of the 200 TRECquestions, 64 questions (approximately one third)were associated with the what axis, for whichour encyclopedia-based system is expected to im-prove the quality of answers.Section 2 analyzes the Japanese IT EngineersExamination, and Section 3 explains our questionanswering system.
Then, Sections 4 and 5 elab-orate on our Web-base method for encyclopediageneration.
Finally, Section 6 evaluates our sys-tem by way of experiments.2 IT Engineers ExaminationsThe Class II examination consists of quadruple-choice questions, among which technical termquestions can be subdivided into two types.In the first type of question, examinees choosethe most appropriate description for a given tech-nical term, such as ?memory interleave?
and?router.
?In the second type of question, examineeschoose the most appropriate term for a givenquestion, for which we show examples collectedfrom the examination in the autumn of 1999(translated into English by one of the authors) asfollows:1.
Which data structure is most appropriate forFIFO (First-In First-Out)?a) binary trees, b) queues, c) stacks, d) heaps2.
Choose a LAN access method where mul-tiple terminals transmit data simultaneouslyand thus they potentially collide.a) ATM, b) CSM/CD, c) FDDI, d) token ringIn the autumn of 1999, out of 80 question, thenumber of the first and second types were 22 and18, respectively.3 Overview of our QA systemFor the first type of question (see Section 2),human examinees would search their knowledgebase (i.e., memory) for the description of a giventerm, and compare that description with four can-didates.
Then they would choose the candidatethat is most similar to the description.For the second type of question, human exam-inees would search their knowledge base for thedescription of each of four candidate terms.
Thenthey would choose the candidate term whose de-scription is most similar to the question.The mechanism of our QA system is analogousto the above human methods.
However, our sys-tem uses as a knowledge base an encyclopediagenerated from the Web.To compute the similarity between two de-scriptions, we use techniques developed in IR re-search, in which the similarity between a userquery and each document in a collection is usu-ally quantified based on word frequencies.
In ourcase, a question and four possible answers corre-spond to query and document collection, respec-tively.
We use one of the major probabilistic IRmethod (Robertson and Walker, 1994).To sum up, given a question, its type and fourchoices, our QA system chooses as the answerone of four candidates, in which resolution algo-rithm varies depending on the question type.4 Encyclopedia Generation4.1 OverviewFigure 1 depicts the overall design of our methodto generate an encyclopedia for input terms.
Thisfigure consists of three modules: ?retrieval,?
?ex-traction?
and ?organization,?
among which theorganization module is newly introduced in thispaper.
In principle, the remaining two modules(?retrieval?
and ?extraction?)
are the same as pro-posed by Fujii and Ishikawa (2000).In Figure 1, terms can be submitted either on-line or off-line.
A reasonable method is that whilethe system periodically updates the encyclopediaoff-line, terms unindexed in the encyclopedia aredynamically processed in real-time usage.
In ei-ther case, our system processes input terms oneby one.
We briefly explain each module in thefollowing three sections, respectively.domainmodelWebextractionrulesorganizationencyclopediaretrievalextractionterm(s)descriptionmodelFigure 1: The overview of our Web-based ency-clopedia generation process.4.2 RetrievalThe retrieval module searches the Web for pagescontaining an input term, for which existing Websearch engines can be used, and those with broadcoverage are desirable.However, search engines performing query ex-pansion are not always desirable, because theyusually retrieve a number of pages which do notcontain a query keyword.
Since the extractionmodule (see Section 4.3) analyzes the usage ofthe input term in retrieved pages, pages not con-taining the term are of no use for our purpose.Thus, we use as the retrieval module ?Google,?which is one of the major search engines and doesnot conduct query expansion2.4.3 ExtractionIn the extraction module, given Web pages con-taining an input term, newline codes, redundantwhite spaces and HTML tags that are not used inthe following process are discarded so as to stan-dardize the page format.Second, we (approximately) identify a regiondescribing the term in the page, for which tworules are used.2http://www.google.com/The first rule is based on Japanese linguis-tic patterns typically used for term descrip-tions, such as ?X toha Y dearu (X is Y).
?Following the method proposed by Fujii andIshikawa (2000), we semi-automatically pro-duced 20 patterns based on the Japanese CD-ROM World Encyclopedia (Heibonsha, 1998),which includes approximately 80,000 entries re-lated to various fields.It is expected that a region including the sen-tence that matched with one of those patterns canbe a term description.The second rule is based on HTML layout.
Ina typical case, a term in question is highlightedas a heading with tags such as <DT>, <B> and<Hx> (?x?
denotes a digit), followed by its de-scription.
In some cases, terms are marked withthe anchor <A> tag, providing hyperlinks to pageswhere they are described.Finally, based on the region briefly identifiedby the above method, we extract a page frag-ment as a term description.
Since term descrip-tions usually consist of a logical segment (suchas a paragraph) rather than a single sentence, weextract a fragment that matched with one of thefollowing patterns, which are sorted according topreference in descending order:1. description tagged with <DD> in the casewhere the term is tagged with <DT>3,2.
paragraph tagged with <P>,3.
itemization tagged with <UL>,4.
N sentences, where we empirically setN = 3.4.4 OrganizationFor the purpose of organization, we classify ex-tracted term descriptions based on word sensesand domains.Although a number of methods have been pro-posed to generate word senses (for example, onebased on the vector space model (Schu?tze, 1998)),it is still difficult to accurately identify wordsenses without explicit dictionaries that predefinesense candidates.3<DT> and <DD> are inherently provided to describeterms in HTML.Since word senses are often associated withdomains (Yarowsky, 1995), word senses can beconsequently distinguished by way of determin-ing the domain of each description.
For ex-ample, different senses for ?pipeline (processingmethod/transportation pipe)?
are associated withcomputer and construction domains (fields), re-spectively.To sum up, the organization module classifiesterm descriptions based on domains, for which weuse domain and description models.
In Section 5,we elaborate on the organization model.5 Statistical Organization Model5.1 OverviewGiven one or more (in most cases more than one)descriptions for a single term, the organizationmodule selects appropriate description(s) for eachdomain related to the term.We do not need all the extracted descriptionsas final outputs, because they are usually similarto one another, and thus are redundant.
For themoment, we assume that we know a priori whichdomains are related to the input term.From the viewpoint of probability theory, ourtask here is to select descriptions with greaterprobability for given domains.
The probabilityfor description d given domain c, P (d|c), is com-monly transformed as in Equation (1), throughuse of the Bayesian theorem.P (d|c) = P (c|d) ?
P (d)P (c) (1)In practice, P (c) can be omitted because this fac-tor is a constant, and thus does not affect the rela-tive probability for different descriptions.In Equation (1), P (c|d) models a probabilitythat d corresponds to domain c. P (d) models aprobability that d can be a description for the termin question, disregarding the domain.
We shallcall them domain and description models, respec-tively.To sum up, in principle we select d?s that arestrongly associated with a certain domain, and arelikely to be descriptions themselves.Extracted descriptions are not linguistically un-derstandable in the case where the extraction pro-cess is unsuccessful and retrieved pages inher-ently contain non-linguistic information (such asspecial characters and e-mail addresses).To resolve this problem, we previously useda language model to filter out descriptions withlow perplexity (Fujii and Ishikawa, 2000).
How-ever, in this paper we integrated a descriptionmodel, which is practically the same as a lan-guage model, with an organization model.
Thenew framework is more understandable with re-spect to probability theory.In practice, we first use Equation (1) to com-pute P (d|c) for all the c?s predefined in the do-main model.
Then we discard such c whoseP (d|c) is below a specific threshold.
As a result,for the input term, related domains and descrip-tions are simultaneously selected.
Thus, we donot have to know a priori which domains are re-lated to each term.In the following two sections, we explain meth-ods to realize the domain and description models,respectively.5.2 Domain ModelThe domain model quantifies the extent to whichdescription d is associated with domain c, whichis fundamentally a categorization task.Among a number of existing categorizationmethods, we experimentally used one proposedby Iwayama and Tokunaga (1994), which formu-lates P (c|d) as in Equation (2).P (c|d) = P (c) ?
?tP (t|c) ?
P (t|d)P (t) (2)Here, P (t|d), P (t|c) and P (t) denote probabili-ties that word t appears in d, c and all the domains,respectively.
We regard P (c) as a constant.
WhileP (t|d) is simply a relative frequency of t in d, weneed predefined domains to compute P (t|c) andP (t).
For this purpose, the use of large-scale cor-pora annotated with domains is desirable.However, since those resources are pro-hibitively expensive, we used the ?Nova?
dic-tionary for Japanese/English machine translationsystems4, which includes approximately one mil-lion entries related to 19 technical fields as listedbelow:4Produced by NOVA, Inc.aeronautics, biotechnology, business,chemistry, computers, construction, de-fense, ecology, electricity, energy, fi-nance, law, mathematics, mechan-ics, medicine, metals, oceanography,plants, trade.We extracted words from dictionary entriesto estimate P (t|c) and P (t).
For Japanese en-tries, we used the ChaSen morphological ana-lyzer (Matsumoto et al, 1997) to extract words.We also used English entries because Japanesedescriptions often contain English words.It may be argued that statistics extracted fromdictionaries are unreliable, because word frequen-cies in real word usage are missing.
However,words that are representative for a domain tendto be frequently used in compound word entriesassociated with the domain, and thus our methodis a practical approximation.5.3 Description ModelThe description model quantifies the extent towhich a given page fragment is feasible as a de-scription for the input term.
In principle, we de-compose the description model into language andquality properties, as shown in Equation (3).P (d) = PL(d) ?
PQ(d) (3)Here, PL(d) and PQ(d) denote language andquality models, respectively.It is expected that the quality model discardsincorrect or misleading information contained inWeb pages.
For this purpose, a number of qual-ity rating methods for Web pages (Amento et al,2000; Zhu and Gauch, 2000) can be used.However, since Google (i.e., the search enginewe used in the retrieval module) rates the qualityof pages based on hyperlink information, and se-lectively retrieves those with higher quality (Brinand Page, 1998), we tentatively regarded PQ(d)as a constant.
Thus, in practice the descriptionmodel is approximated solely with the languagemodel as in Equation (4).P (d) ?
PL(d) (4)Statistical approaches to language modelinghave been used in much NLP research, suchas machine translation (Brown et al, 1993) andspeech recognition (Bahl et al, 1983).
Our lan-guage model is almost the same as existing mod-els, but is different in two respects.First, while general language models quantifythe extent to which a given word sequence is lin-guistically acceptable, our model also quantifiesthe extent to which the input is acceptable as aterm description.
Thus, we trained the modelbased on an existing machine readable encyclo-pedia.We used the ChaSen morphological analyzerto segment the Japanese CD-ROM World Ency-clopedia (Heibonsha, 1998) into words (we re-placed headwords with a common symbol), andthen used the CMU-Cambridge toolkit (Clark-son and Rosenfeld, 1997) to model a word-basedtrigram.
Consequently, descriptions in whichword sequences are more similar to those in theWorld Encyclopedia are assigned greater proba-bility scores through our language model.Second, P (d), which is generally a productof probabilities for N -grams in d, is quite sen-sitive to the length of d. In the cases of machinetranslation and speech recognition, this problemis less crucial because multiple candidates com-pared based on the language model are almostequivalent in terms of length.
For example, in thecase of machine translation, candidates are trans-lations for a single input, which are usually com-parable with respect to length.However, since in our case length of descrip-tions are significantly different, shorter descrip-tions are more likely to be selected, regardless ofthe quality.
To avoid this problem, we normalizeP (d) by the number of words contained in d.6 Experimentation6.1 MethodologyWe evaluated the performance of our question an-swering system, for which we used as test in-puts 40 technical term questions collected fromthe Class II examination (the autumn of 1999).First, we generated an encyclopedia including96 terms that are associated with those 40 ques-tions.
For all the 96 test terms, Google retrieved apositive number of pages, and the average num-ber of pages for one term was 196,503.
SinceGoogle practically outputs contents of the top1,000 pages, the remaining pages were not usedin our experiments.For each test term, we computed P (d|c) us-ing Equation (1) and discarded domains whoseP (d|c) was below 0.05.
Then, for each remain-ing domain, the top three descriptions with higherP (d|c) values were selected as the final outputs,because a preliminary experiment showed that acorrect description was generally found in the topthree candidates.In addition, to estimate a baseline perfor-mance, we used the ?Nichigai?
computer dictio-nary (Nichigai Associates, 1996).
This dictio-nary lists approximately 30,000 Japanese techni-cal terms related to the computer field, and con-tains descriptions for 13,588 terms.
In this dictio-nary 42 out of 96 test terms were described.We compared the following three different re-sources as a knowledge base:?
the Nichigai dictionary (?Nichigai?),?
the descriptions generated in the first experi-ment (?Web?),?
combination of both resources (?Nichigai +Web?
).6.2 ResultsTable 1 shows the result of our comparative ex-periment, in which ?C?
and ?A?
denote coverageand accuracy, respectively, for variations of ourQA system.Since all the questions we used are quadruple-choice, in case the system cannot answer thequestion, random choice can be performed to im-prove the coverage to 100%.Thus, for each knowledge resource we com-pared cases without/with random choice, whichare denoted ?w/o Random?
and ?w/ Random?
inTable 1, respectively.Table 1: Coverage and accuracy (%) for differentquestion answering methods.w/o Random w/ RandomResource C A C ANichigai 50.0 65.0 100 45.0Web 92.5 48.6 100 46.9Nichigai + Web 95.0 63.2 100 61.3In the case where random choice was not per-formed, the Web-based encyclopedia noticeablyimproved the coverage for the Nichigai dictio-nary, but decreased the accuracy.
However, bycombining both resources, the accuracy was no-ticeably improved, and the coverage was compa-rable with that for the Nichigai dictionary.On the other hand, in the case where randomchoice was performed, the Nichigai dictionaryand the Web-based encyclopedia were compara-ble in terms of both the coverage and accuracy.Additionally, by combining both resources, theaccuracy was further improved.We also investigated the performance of ourQA system where descriptions related to the com-puter domain are solely used.
For example, thedescription of ?pipeline (transportation pipe)?
isin principle irrelevant or misleading to answerquestions associated with ?pipeline (processingmethod).
?However, coverage/accuracy did not change,because approximately one third of the resultantdescriptions were inherently related to the com-puter domain, and thus those related to minor do-mains did not affect the result.7 ConclusionIn this paper, we proposed a question answeringsystem which uses an encyclopedia as a knowl-edge base.
For this purpose, we reformalizedour Web-based extraction method, and proposeda new statistical organization model to improvethe quality of extracted data.Given a term for which encyclopedic knowl-edge (i.e., descriptions) is to be generated, ourmethod sequentially performs a) retrieval of Webpages containing the term, b) extraction of pagefragments describing the term, and c) organiz-ing extracted descriptions based on domains (andconsequently word senses).For the purpose of evaluation, we used as testquestions the Japanese Information-TechnologyEngineers Examination, and found that our Web-based encyclopedia was comparable with an ex-isting dictionary in terms of the application toquestion answering.
In addition, by using the bothresources the performance of question answeringwas further improved.AcknowledgmentsThe authors would like to thank NOVA, Inc.for their support with the Nova dictionary andKatunobu Itou (The National Institute of Ad-vanced Industrial Science and Technology, Japan)for his insightful comments on this paper.ReferencesBrian Amento, Loren Terveen, and Will Hill.
2000.Does ?authority?
mean quality?
predicting expertquality ratings of Web documents.
In Proceedingsof the 23rd Annual International ACM SIGIR Con-ference on Research and Development in Informa-tion Retrieval, pages 296?303.Lalit.
R. Bahl, Frederick Jelinek, and Robert L. Mer-cer.
1983.
A maximum linklihood approach tocontinuous speech recognition.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,5(2):179?190.Sergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual Web search engine.Computer Networks, 30(1?7):107?117.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Computational Linguistics,19(2):263?311.Philip Clarkson and Ronald Rosenfeld.
1997.
Statisti-cal language modeling using the CMU-Cambridgetoolkit.
In Proceedings of EuroSpeech?97, pages2707?2710.Atsushi Fujii and Tetsuya Ishikawa.
2000.
Utilizingthe World Wide Web as an encyclopedia: Extract-ing term descriptions from semi-structured texts.In Proceedings of the 38th Annual Meeting of theAssociation for Computational Linguistics, pages488?495.Sanda M. Harabagiu, Marius A. Pas?ca, and Steven J.Maiorano.
2000.
Experiments with open-domaintextual question answering.
In Proceedings of the18th International Conference on ComputationalLinguistics, pages 292?298.Hitachi Digital Heibonsha.
1998.
CD-ROM WorldEncyclopedia.
(In Japanese).Makoto Iwayama and Takenobu Tokunaga.
1994.
Aprobabilistic model for text categorization: Basedon a single random variable with multiple values.In Proceedings of the 4th Conference on AppliedNatural Language Processing, pages 162?167.Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,Yoshitaka Hirano, Osamu Imaichi, and TomoakiImamura.
1997.
Japanese morphological analysissystem ChaSen manual.
Technical Report NAIST-IS-TR97007, NAIST.
(In Japanese).Dan Moldovan and Sanda Harabagiu.
2000.
Thestructure and performance of an open-domain ques-tion answering system.
In Proceedings of the 38thAnnual Meeting of the Association for Computa-tional Linguistics, pages 563?570.Nichigai Associates.
1996.
English-Japanese com-puter terminology dictionary.
(In Japanese).John Prager, Eric Brown, and Anni Coden.
2000.Question-answering by predictive annotation.
InProceedings of the 23rd Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 184?191.S.
E. Robertson and S. Walker.
1994.
Some simpleeffective approximations to the 2-poisson model forprobabilistic weighted retrieval.
In Proceedings ofthe 17th Annual International ACM SIGIR Confer-ence on Research and Development in InformationRetrieval, pages 232?241.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Ellen M. Voorhees and Dawn M. Tice.
2000.
Buildinga question answering test collection.
In Proceed-ings of the 23rd Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 200?207.David Yarowsky.
1995.
Unsupervised word sense dis-ambiguation rivaling supervised methods.
In Pro-ceedings of the 33rd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 189?196.Xiaolan Zhu and Susan Gauch.
2000.
Incorporatingquality metrics in centralized/distributed informa-tion retrieval on the World Wide Web.
In Proceed-ings of the 23rd Annual International ACM SIGIRConference on Research and Development in Infor-mation Retrieval, pages 288?295.
