Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 368?376,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsFast Statistical Parsing with Parallel Multiple Context-Free GrammarsKrasimir Angelov and Peter Ljungl?ofUniversity of Gothenburg and Chalmers University of TechnologyG?oteborg, Swedenkrasimir@chalmers.sepeter.ljunglof@cse.gu.seAbstractWe present an algorithm for incrementalstatistical parsing with Parallel MultipleContext-Free Grammars (PMCFG).
Thisis an extension of the algorithm by An-gelov (2009) to which we added statisti-cal ranking.
We show that the new al-gorithm is several times faster than otherstatistical PMCFG parsing algorithms onreal-sized grammars.
At the same time thealgorithm is more general since it supportsnon-binarized and non-linear grammars.We also show that if we make thesearch heuristics non-admissible, the pars-ing speed improves even further, at the riskof returning sub-optimal solutions.1 IntroductionIn this paper we present an algorithm for incre-mental parsing using Parallel Multiple Context-Free Grammars (PMCFG) (Seki et al., 1991).
Thisis a non context-free formalism allowing disconti-nuity and crossing dependencies, while remainingwith polynomial parsing complexity.The algorithm is an extension of the algorithmby Angelov (2009; 2011) which adds statisticalranking.
This is a top-down algorithm, shown byLjungl?of (2012) to be similar to other top-down al-gorithms (Burden and Ljungl?of, 2005; Kanazawa,2008; Kallmeyer and Maier, 2009).
None of theother top-down algorithms are statistical.The only statistical PMCFG parsing algorithms(Kato et al., 2006; Kallmeyer and Maier, 2013;Maier et al., 2012) all use bottom-up parsingstrategies.
Furthermore, they require the gram-mar to be binarized and linear, which means thatthey only support linear context-free rewriting sys-tems (LCFRS).
In contrast, our algorithm natu-rally supports the full power of PMCFG.
By lift-ing these restrictions, we make it possible to ex-periment with novel grammar induction methods(Maier, 2013) and to use statistical disambiguationfor hand-crafted grammars (Angelov, 2011).By extending the algorithm with a statisticalmodel, we allow the parser to explore only partsof the search space, when only the most proba-ble parse tree is needed.
Our cost estimation issimilar to the estimation for the Viterbi probabil-ity as in Stolcke (1995), except that we have totake into account that our grammar is not context-free.
The estimation is both admissible and mono-tonic (Klein and Manning, 2003) which guaran-tees that we always find a tree whose probabilityis the global maximum.We also describe a variant with a non-admissible estimation, which further improves theefficiency of the parser at the risk of returning asuboptimal parse tree.We start with a formal definition of a weightedPMCFG in Section 2, and we continue with a pre-sentation of our algorithm by means of a weighteddeduction system in Section 3.
In Section 4,we prove that our estimations are admissible andmonotonic.
In Section 5 we calculate an esti-mate for the minimal inside probability for everycategory, and in Section 6 we discuss the non-admissible heuristics.
Sections 7 and 8 describethe implementation and our evaluation, and the fi-nal Section 9 concludes the paper.2 PMCFG definitionOur definition of weighted PMCFG (Definition 1)is the same as the one used by Angelov (2009;2011), except that we extend it with weights forthe productions.
This definition is also similar toKato et al (2006), with the small difference that weallow non-linear functions.As an illustration for PMCFG parsing, we usea simple grammar (Figure 1) which can generatephrases like ?both black and white?
and ?eitherred or white?
but rejects the incorrect combina-368Definition 1A parallel multiple context-free grammar is a tupleG = (N,T, F, P, S, d, di, r, a) where:?
N is a finite set of categories and a positive in-teger d(A) called dimension is given for eachA ?
N .?
T is a finite set of terminal symbols which is dis-joint with N .?
F is a finite set of functions where the arity a(f)and the dimensions r(f) and di(f) (1 ?
i ?a(f)) are given for every f ?
F .
For everypositive integer d, (T?
)ddenote the set of all d-tuples of strings over T .
Each function f ?
Fis a total mapping from (T?)d1(f)?
(T?)d2(f)??
?
?
?
(T?
)da(f)(f)to (T?
)r(f), defined as:f := (?1, ?2, .
.
.
, ?r(f))Here ?iis a sequence of terminals and ?k; l?pairs, where 1 ?
k ?
a(f) is called argumentindex and 1 ?
l ?
dk(f) is called constituentindex.?
P is a finite set of productions of the form:Aw??
f [A1, A2, .
.
.
, Aa(f)]where A ?
N is called result category,A1, A2, .
.
.
, Aa(f)?
N are called argumentcategories, f ?
F is the function symbol andw > 0 is a weight.
For the production to bewell formed the conditions di(f) = d(Ai) (1 ?i ?
a(f)) and r(f) = d(A) must hold.?
S is the start category and d(S) = 1.tions both-or and either-and.
We avoid these com-binations by coupling the right pairs of words in asingle function, i.e.
we have the abstract conjunc-tions both and and either or which are linearizedas discontinuous phrases.
The phrase insertion it-self is done in the definition of conjA .
It takes theconjunction as its first argument, and it uses ?1; 1?and ?1; 2?
to insert the first and the second con-stituent of the argument at the right places in thecomplete phrase.A tree of function applications that yelds a com-plete phrase is the parse tree for the phrase.
Forinstance, the phrase ?both red and either black orwhite?
is represented by the tree:(conjA both and red(conjA either or black white))Aw1??
conjA [Conj ,A ,A ]Aw2??
black []Aw3??
white[]Aw4??
red []Conjw5??
both and []Conjw6??
either or[]conjA := (?1; 1?
?2; 1?
?1; 2?
?3; 1?
)black := (?black?
)white := (?white?
)red := (?red?
)both and := (?both?, ?and?
)either or := (?either?, ?or?
)Figure 1: Example GrammarThe weight of a tree is the sum of the weights forall functions that are used in it.
In this case theweight for the example isw1+w5+w4+w1+w6+w2+ w3.
If there are ambiguities in the sentence,the algorithm described in Section 3 always findsa tree which minimizes the weight.Usually the weights for the productions are log-arithmic probabilities, i.e.
the weight of the pro-duction A?
f [~B] is:w = ?
logP (A?
f [~B] | A)where P (A ?
f [~B] | A) is the probability tochoose this production when the result category isfixed.
In this case the probabilities for all produc-tions with the same result category sum to one:?Aw?
?f [~B] ?Pe?w= 1However, the parsing algorithm does not dependon the probabilistic interpretation of the weights,so the same algorithm can be used with any otherkind of weights.3 Deduction SystemWe define the algorithm as weighted deductionsystem (Nederhof, 2003) which generalizes An-gelov?s system.A key feature in his algorithm is that the ex-pressive PMCFG is reduced to a simple context-free grammar which is extended dynamically atparsing time in order to account for context de-pendent features in the original grammar.
This369can be exemplified with the grammar in Fig-ure 1, where there are two productions for cat-egory Conj .
Given the phrase ?both black andwhite?, after accepting the token both, only theproduction Conjw5??
both and [] can be appliedfor parsing the second part of the conjunction.This is achieved by generating a new categoryConj2which has just a single production:Conj2w5??
both and [] (1)The parsing algorithm is basically an extension ofEarley?s (1970) algorithm, except that the parseitems in the chart also keep track of the categoriesfor the arguments.
In the particular case, the cor-responding chart item will be updated to point toConj2instead of Conj .
This guarantees that onlyand will be accepted as a second constituent afterseeing that the first constituent is both.Now since the set of productions is dynamic, theparser must keep three kinds of items in the chart,instead of two as in the Earley algorithm:Productions The parser maintains a dynamic setwith all productions that are derived during theparsing.
The initial state is populated with the pro-ductions from the set P in the grammar.Active Items The active items play the samerole as the active items in the Earley algorithm.They have the form:[kjAw??
f [~B]; l : ?
?
?
;wi;wo]and represent the fact that a constituent l of a cat-egory A has been partially recognized from posi-tion j to k in the sentence.
Here Aw??
f [~B] isthe production and the concatenation ??
is the se-quence of terminals and ?k; r?
pairs which definesthe l-th constituent of function f .
The dot ?
be-tween ?
and ?
separates the part of the constituentthat is already recognized from the part which isstill pending.
Finally wiand woare the inside andoutside weights for the item.Passive Items The passive items are of the form:[kjA; l;?A]and state that a constituent with index l from cate-gory A was recognized from position j to positionk in the sentence.
As a consequence the parser hascreated a new category?A.
The set of productionsderived for?A compactly records all possible waysto parse the j ?
k fragment.3.1 Inside and outside weightsThe inside weight wiand the outside weight wointhe active items deserve more attention since thisis the only difference compared to Angelov (2009;2011).
When the item is complete, it will yield theforest of all trees that derive the sub-string cov-ered by the item.
For example, when the first con-stituent for category Conj is completely parsed,the forest will contain the single production in (1).The inside weight for the active item is the cur-rently best known estimation for the lowest weightof a tree in the forest.
The trees yielded by the itemdo not cover the whole sentence however.
Instead,they will become part of larger trees that cover thewhole sentence.
The outside weight is the esti-mation for the lowest weight for an extension of atree to a full tree.
The sum wi+ woestimates theweight of the full tree.Before turning to the deduction rules we alsoneed a notation for the lowest possible weight fora tree of a given category.
If A ?
N is a categorythenwAwill denote the lowest weight that a tree ofcategoryA can have.
For convenience, we also usew~Bas a notation for the sum?iwBiof the weightof all categories in the vector~B.
If the categoryA is defined in the grammar then we assume thatthe weight is precomputed as described in Section5.
When the parser creates the category, it willcompute the weight dynamically.3.2 Deduction rulesThe deduction rules are shown in Figure 2.
Herethe assumption is that the active items are pro-cessed in the order of increasing wi+ woweight.In the actual implementation we put all activeitems in a priority queue and we always take firstthe item with the lowest weight.
We never throwaway items but the processing of items with veryhigh weight might be delayed indefinitely or theymay never be processed if the best tree is foundbefore that.
Furthermore, we think of the deduc-tion system as a way do derive a set of items, but inour case we ignore the weights when we considerwhether two active items are the same.
In this way,every item is derived only once and the weights forthe active items are computed from the weights ofthe first antecedents that led to its derivation.Finally, we use two more notations in the rules:rhs(g, r) denotes constituent with index r in func-tion g; and ?kdenotes the k-th token in the sen-tence.370INITIAL PREDICTSw??
f [~B][00Sw??
f [~B]; 1 : ?
?
;w + w~B; 0]S = start category, ?
= rhs(f, 1)PREDICTBdw1??
g[~C] [kjAw2??
f [~B]; l : ?
?
?d; r?
?;wi;wo][kkBdw1??
g[~C]; r : ?
?
;w1+ w~C;wi?
wBd+ wo]?
= rhs(g, r)SCAN[kjAw??
f [~B]; l : ?
?
s ?;wi;wo][k+1jAw??
f [~B]; l : ?
s ?
?
;wi;wo]s = ?k+1COMPLETE[kjAw??
f [~B]; l : ?
?
;wi;wo]?Aw??
f [~B] [kjA; l;?A]?A = (A, l, j, k), w?A= wiCOMBINE[ujAw??
f [~B]; l : ?
?
?d; r?
?
;wi;wo] [kuBd; r;?Bd][kjAw??
f [~B{d :=?Bd}]; l : ?
?d; r?
?
?
;wi+ w?Bd?
wBd;wo]Figure 2: Deduction RulesThe first rule on Figure 2 is INITIAL PREDICT andhere we predict the initial active items from theproductions for the start category S. Since thisis the start category, we set the outside weight tozero.
The inside weight is equal to the sum of theweight w for the production and the lowest pos-sible weight w~Bfor the vector of arguments~B.The reason is that despite that we do not know theweight for the final tree yet, it cannot be lower thanw+w~Bsince w~Bis the lowest possible weight forthe arguments of function f .The interaction between inside and outsideweights is more interesting in the PREDICT rule.Here we have an item where the dot is before ?d; r?and from this we must predict one item for eachproduction Bdw1??
g[~C] of category Bd.
The in-side weight for the new item is w1+ w~Cfor thesame reasons as for the INITIAL PREDICT rule.
Theoutside weight however is not zero because thenew item is predicted from another item.
The in-side weight for the active item in the antecedentsis now part of the outside weight of the new item.We just have to subtract wBdfrom wibecause thenew item is going to produce a new tree which willreplace the d-th argument of f .
For this reason theestimation for the outside weight iswi?wBd+wo,where we also added the outside weight for the an-tecedent item.In the SCAN rule, we just move the dot past atoken, if it matches the current token ?k+1.
Boththe inside and the outside weights are passed un-touched from the antecedent to the consequent.In the COMPLETE rule, we have an item where thedot has reached the end of the constituent.
Here wegenerate a new category?A which is unique for thecombination (A, l, j, k), and we derive the produc-tion?Aw??
f [~B] for it.
We set the weight w?Afor?Ato be equal to wiand in Section 4, we will provethat this is indeed the lowest weight for a tree ofcategory?A.In the last rule COMBINE, we combine an activeitem with a passive item.
The outside weight wofor the new active item remains the same.
How-ever, we must update the inside weight since wehave replaced the d-th argument in~B with thenewly generated category?Bd.
The new weight iswi+ w?Bd?
wBd, i.e.
we add the weight for thenew category and we subtract the weight for theprevious category Bd.Now for the correctness of the weights we mustprove that the estimations are both admissible andmonotonic.4 Admissibility and MonotonicityWe will first prove that the weights grow mono-tonically, i.e.
if we derive one active item fromanother then the sum wi+ wofor the new item isalways greater or equal to the sum for the previous371item.
PREDICT and COMBINE are the only two ruleswith an active item both in the antecedents and inthe consequents.Note that in PREDICT we choose one particularproduction for category Bd.
We know that thelowest possible weight of a tree of this categoryis wBd.
If we restrict the set of trees to thosethat not only have the same category Bdbut alsouse the same production Bdw1??
g[~C] on the toplevel, then the best weight for such a tree will bew1+ w~C.
According to the definition of wBd, itmust follow that:w1+ w~C?
wBdFrom this we can trivially derive that:(w1+ w~C) + (wi?
wBd+ wo) ?
wi+ wowhich is the monotonicity condition for rulePREDICT.
Similarly in rule COMBINE, the condition:w?Bd?
wBdmust hold because the forest of trees for?Bdis in-cluded in the forest forBd.
From this we concludethe monotonicity condition:(wi+ w?Bd?
wBd) + wo?
wi+ woThe last two inequalities are valid only if we cancorrectly compute w?Bdfor a dynamically gener-ated category?Bd.
This happens in rule COMPLETE,where we have a complete active item with a cor-rectly computed inside weight wi.
Since we pro-cess the active items in the order of increasingwi+ woweight and since we create?A when wefind the first complete item for category A, it isguaranteed that at this point we have an item withminimal wi+ wovalue.
Furthermore, all itemswith the same result category A and the same startposition j must have the same outside weight.
Itfollows that when we create?A we actually do itfrom an active item with minimal inside weightwi.
This means that it is safe to assign that w?A=wi.It is also easy to see that the estimation is ad-missible.
The only places where we use estima-tions for the unseen parts of the sentence is in therules INITIAL PREDICT and PREDICT where we usethe weights w~Band w~Cwhich may include com-ponents corresponding to function argument thatare not seen yet.
However by definition it is notpossible to build a tree with weight lower than theweight for the category.
This means that the esti-mation is always admissible.5 Initial EstimationThe minimal weight for a dynamically created cat-egory is computed by the parser, but we must ini-tialize the weights for the categories that are de-fined in the grammar.
The easiest way is to justset all weights to zero, and this is safe since theweights for the predefined categories are used onlyas estimations for the yet unseen parts of the sen-tence.
Essentially this gives us a statistical parserwhich performs Dijkstra search in the space of allparse trees.
Any other reasonable weight assign-ment will give us an A?algorithm (Hart et al.,1968).In general it is possible to devise differentheuristics which will give us different improve-ments in the parsing time.
In our current im-plementation of the parser we use a weight as-signment which considers only the already knownprobabilities for the productions in the grammar.The weight for a category A is computed as:wA= minAw?
?f [~B] ?
P(w + w~B)Here the sum w + w~Bis the minimal weight fora tree constructed with the production Aw??
f [~B]at the root.
By taking the minimum over all pro-ductions for A, we get the corresponding weightwA.
This is a recursive equation since its right-hand side contains the valuew~Bwhich depends onthe weights for the categories in~B.
It might hap-pen that there are mutually dependent categorieswhich will lead to a recursion in the equation.The solution is found with iterative assignmentsuntil a fixed point is reached.
In the beginning weassign wA= 0 for all categories.
After that we re-compute the new weights with the equation aboveuntil we reach a fixed point.6 Non-admissible heuristicsThe set of active items is kept in a priority queueand at each step we process the item with the low-est weight.
However, when we experimented withthe algorithm we noticed that most of the time theitem that is selected would eventually contributewith an alternative reading of the sentence but notto the best parse.
What happens is that despite thatthere are already items ending at position k in thesentence, the current best item might have a spani ?
j where j < k. The parser then picks thebest item only to discover later that the item be-came much heavier until it reached the span i?
k.372This suggests that when we compare the weightsof items with different end positions, then we musttake into account the weight that will be accumu-lated by the item that ends earlier until the twoitems align at the same end position.We use the following heuristic to estimate thedifference.
The first time when we extend anitem from position i to position i + 1, we recordthe weight increment w?
(i + 1) for that position.The increment w?is the difference between theweights for the best active item reaching positioni + 1 and the best active item reaching position i.From now on when we compare the weights fortwo items xjand xk, with end positions j and krespectively (j < k), then we always add to thescore wxjof the first item a fraction of the sum ofthe increments for the positions between j and k.In other words, instead of using wxjwhen com-paring with wxk, we usewxj+ h ??j<i?kw?
(i)We call the constant h ?
[0, 1] the ?heuristics fac-tor?.
If h = 0, we obtain the basic algorithm thatwe described earlier which is admissible and al-ways returns the best parse.
However, the evalua-tion in Section 8.3 shows that a significant speed-up can be obtained by using larger values of h.Unfortunately, if h > 0, we loose some accuracyand cannot guarantee that the best parse is alwaysreturned first.Note that the heuristics does not change thecompleteness of the algorithm ?
it will succeedfor all grammatical sentences and fail for all non-grammatical.
But it does not guarantee that thefirst parse tree will be the optimal.7 ImplementationThe parser is implemented in C and is distributedas a part of the runtime system for the open-sourceGrammatical Framework (GF) programming lan-guage (Ranta, 2011).1Although the primary ap-plication of the runtime system is to run GF appli-cations, it is not specific to one formalism, and itcan serve as an execution platform for other frame-works where natural language parsing and gener-ation is needed.The GF system is distributed with a libraryof manually authored resource grammars (Ranta,1http://www.grammaticalframework.org/2009) for over 25 languages, which are used as aresource for deriving domain specific grammars.Adding a big lexicon to the resource grammar re-sults in a highly ambiguous grammar, which cangive rise to millions of trees even for moderatelycomplex sentences.
Previously, the GF system hasnot been able to parse with such ambiguous gram-mars, but with our statistical algorithm it is nowfeasible.8 EvaluationWe did an initial evaluation on the GF English re-source grammar augmented with a large-coveragelexicon of 40 000 lemmas taken from the OxfordAdvanced Learner?s Dictionary (Mitton, 1986).
Intotal the grammar has 44 000 productions.
Therule weights were trained from a version of thePenn Treebank (Marcus et al., 1993) which wasconverted to trees compatible with the grammar.The trained grammar was tested on Penn Tree-bank sentences of length up to 35 tokens, and theparsing times were at most 7 seconds per sentence.This initial test was run on a computer with a 2.4GHz Intel Core i5 processor with 8 GB RAM.
Thisresult was very encouraging, given the complexityof the grammar, so we decided to do a larger testand compare with an existing state-of-the-art sta-tistical PMCFG parser.Rparse (Kallmeyer and Maier, 2013) is a an-other state-of-the-art training and parsing systemfor PMCFG.2It is written in Java and developed atthe Universities of T?ubingen and D?usseldorf, Ger-many.
Rparse can be used for training probabilis-tic PMCFGs from discontinuous treebanks.
It canalso be used for parsing new sentences with thetrained grammars.In our evaluation we used Rparse to extract PM-CFG grammars from the discontinuous GermanTiger Treebank (Brants et al., 2002).
The rea-son for using this treebank is that the extractedgrammars are non-context-free, and our parser isspecifically made for such grammars.8.1 Evaluation dataIn our evaluations we got the same general resultsregardless of the size of the grammar, so we onlyreport the results from one of these runs.In this particular example, we trained the gram-mar on 40 000 sentences from the Tiger Treebankwith lengths up to 160 tokens.
We evaluated on2https://github.com/wmaier/rparse373CountTraining sentences 40 000Test sentences 4 607Non-binarized grammar rules 30 863Binarized grammar rules 26 111Table 1: Training and testing data.4 600 Tiger sentences, with a length of 5?60 to-kens.
The exact numbers are shown in Table 1.All tests were run on a computer with a 2.3 GHzIntel Core i7 processor with 16GB RAM.As a comparison, Maier et al (2012) train onapproximately 15 000 sentences from the NegraTreebank, and only evaluate on sentences of atmost 40 tokens.8.2 Comparison with RparseWe evaluated our parser by comparing it withRparse?s built-in parser.
Note that we are only in-terested in the efficiency of our implementation,not the coverage and accuracy of the trained gram-mar.
In the comparison we used only the ad-missible heuristics, and we did confirm that theparsers produce optimal trees with exactly thesame weight for the same input.Rparse extracts grammars in two steps.
Firstit converts the treebank into a PMCFG, and thenit binarizes that grammar.
The binarization pro-cess uses markovization to improve the precisionand recall of the final grammar (Kallmeyer andMaier, 2013).
We tested both Rparse?s standard(Kallmeyer and Maier, 2013) and its new im-proved parsing alogorithm (Maier et al., 2012).The new algorithm unfortunately works only withLCFRS grammars with a fan-out?
2 (Maier et al.,2012).In this test we used the optimal binarizationmethod described in Kallmeyer (2010, chapter7.2).
This was the only binarization algorithm inRparse that produced a grammar with fan-out?
2.As can be seen in Figure 3, our parser outper-forms Rparse for all sentence lengths.
For sen-tences longer than 15 tokens, the standard Rparseparser needs on average 100 times longer timethan our parser.
This difference increases withsentence length, suggesting that our algorithm hasa better parsing complexity than Rparse.The PGF parser also outperforms the improvedRparse parser, but the relative difference seems tostabilize on a speedup of 10?15 times.0,010,11101005 10 15 20 25 30 35 40Rparse, standardRparse, fanout ?
2PGF, admissibleFigure 3: Parsing time (seconds) compared withRparse.0,010,11101005 10 15 20 25 30 35 40 45 50 55 60PGF, admissiblePGF, h=0.50PGF, h=0.75PGF, h=0.95Figure 4: Parsing time (seconds) with differentheuristics factors.8.3 Comparing different heuristicsIn another test we compared the effect of theheuristic factor h described in Section 6.
We usedthe same training and testing data as before, andwe tried four different heuristic factors: h = 0,0.50, 0.75 and 0.95.
As mentioned in Section 6,a factor of 0 gives an admissible heuristics, whichmeans that the parser is guaranteed to return thetree with the best weight.The parsing times are shown in Figure 4.
Ascan be seen, a higher heuristics factor h gives aconsiderable speed-up.
For 40 token sentences,h = 0.50 gives an average speedup of 5 times,while h = 0.75 is 30 times faster, and h = 0.95 isalmost 500 times faster than using the admissibleheuristics h = 0.
This is more clearly seen in Fig-ure 5, where the parsing times are shown relativeto the admissible heuristics.Note that all charts have a logarithmic y-axis,which means that a straight line is equivalent toexponential growth.
If we examine the graph lines3740,0010,010,115 10 15 20 25 30 35 40PGF, admissiblePGF, h=0.50PGF, h=0.75PGF, h=0.95Figure 5: Relative parsing time for different valuesof h, compared to admissible heuristic.more closely, we can see that they are not straight.The closest curves are in fact polynomial, witha degree of 4?6 depending on the parser and thevalue of h.38.4 Non-admissibility and parsing qualityWhat about the loss of parsing quality when weuse a non-admissible heuristics?
Firstly, as men-tioned in Section 6, the parser still recognizes ex-actly the same language as defined by the gram-mar.
The difference is that it is not guaranteed toreturn the tree with the best weight.In our evaluation we saw that for a factor h =0.50, 80% of the trees are optimal, and only 3%of the trees have a weight more than 5% from theoptimal weight.
The performance gradually getsworse for higher h, and with h = 0.95 almost 10%of the trees have a weight more than 20% from theoptimum.These numbers only show how the parsing qual-ity degrades relative to the grammar.
But sincethe grammar is trained from a treebank it is moreinteresting to evaluate how the parsing quality onthe treebank sentences is affected when we use anon-admissible heuristics.
Table 2 shows how thelabelled precision and recall are changed with dif-ferent values for h. The evaluation was done us-ing the EVALB measure which is implemented inRparse (Maier, 2010).
As can be seen, a factor ofh = 0.50 only results in a f-score loss of 3 points,which is arguably not very much.
On the otherextreme, for h = 0.95 the f-score drops 14 points.3The exception is the standard Rparse parser, which has apolynomial degree of 8.Precision Recall F-scoreadmissible 71.1 67.7 69.3h = 0.50 68.0 64.9 66.4h = 0.75 63.0 60.8 61.9h = 0.95 55.1 55.6 55.3Table 2: Parsing quality for different values of h.9 DiscussionThe presented algorithm is an important general-ization of the classical algorithms of Earley (1970)and Stolcke (1995) for parsing with probabilisticcontext-free grammars to the more general formal-ism of parallel multiple context-free grammars.The algorithm has been implemented as part of theruntime for the Grammatical Framework (Ranta,2011), but it is not limited to GF alone.9.1 PerformanceTo show the universality of the algorithm, we eval-uated it on large LCFRS grammars trained fromthe Tiger Treebank.Our parser is around 10?15 times faster than thelatest, optimized version of the Rparse state-of-the-art parser.
This improvement seems to be con-stant, which means that it can be a consequenceof low-level optimizations.
More important is thatour algorithm does not impose any restrictions atall on the underlying PMCFG grammar.
Rparse onthe other hand requires that the grammar is bothbinarized and has a fan-out of at most 2.By using a non-admissible heuristics, the speedimproves by orders of magnitude, at the expenseof parsing quality.
This makes it possible toparse long sentences (more than 50 tokens) in justaround a second on a standard desktop computer.9.2 Future workWe would like to extend the algorithm to be able touse lexicalized statistical models (Collins, 2003).Furthermore, it would be interesting to developbetter heuristics for A?search, and to investigatehow to incorporate beam search pruning into thealgorithm.375ReferencesKrasimir Angelov.
2009.
Incremental parsing withparallel multiple context-free grammars.
In Pro-ceedings of EACL 2009, the 12th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, Athens, Greece.Krasimir Angelov.
2011.
The Mechanics of the Gram-matical Framework.
Ph.D. thesis, Chalmers Univer-sity of Technology, Gothenburg, Sweden.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Proceedings of TLT 2002, the 1st Work-shop on Treebanks and Linguistic Theories, So-zopol, Bulgaria.H?akan Burden and Peter Ljungl?of.
2005.
Parsing lin-ear context-free rewriting systems.
In Proceedingsof IWPT 2005, the 9th International Workshop onParsing Technologies, Vancouver, Canada.Michael Collins.
2003.
Head-driven statistical modelsfor natural language parsing.
Computational Lin-guistics, 29(4):589?637.Jay Earley.
1970.
An efficient context-free parsing al-gorithm.
Communications of the ACM, 13(2):94?102.Peter Hart, Nils Nilsson, and Bertram Raphael.
1968.A formal basis for the heuristic determination ofminimum cost paths.
IEEE Transactions of SystemsScience and Cybernetics, 4(2):100?107.Laura Kallmeyer and Wolfgang Maier.
2009.
An in-cremental Earley parser for simple range concatena-tion grammar.
In Proceedings of IWPT 2009, the11th International Conference on Parsing Technolo-gies, Paris, France.Laura Kallmeyer and Wolfgang Maier.
2013.
Data-driven parsing using probabilistic linear context-free rewriting systems.
Computational Linguistics,39(1):87?119.Laura Kallmeyer.
2010.
Parsing Beyond Context-FreeGrammars.
Springer.Makoto Kanazawa.
2008.
A prefix-correct Earleyrecognizer for multiple context-free grammars.
InProceedings of TAG+9, the 9th International Work-shop on Tree Adjoining Grammar and Related For-malisms, T?ubingen, Germany.Yuki Kato, Hiroyuki Seki, and Tadao Kasami.
2006.Stochastic multiple context-free grammar for RNApseudoknot modeling.
In Proceedings of TAGRF2006, the 8th International Workshop on Tree Ad-joining Grammar and Related Formalisms, Sydney,Australia.Dan Klein and Christopher D. Manning.
2003.
A?parsing: fast exact Viterbi parse selection.
In Pro-ceedings of HLT-NAACL 2003, the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, Edmonton, Canada.Peter Ljungl?of.
2012.
Practical parsing of parallelmultiple context-free grammars.
In Proceedings ofTAG+11, the 11th International Workshop on TreeAdjoining Grammar and Related Formalisms, Paris,France.Wolfgang Maier, Miriam Kaeshammer, and LauraKallmeyer.
2012.
PLCFRS parsing revisited: Re-stricting the fan-out to two.
In Proceedings ofTAG+11, the 11th International Workshop on TreeAdjoining Grammar and Related Formalisms, Paris,France.Wolfgang Maier.
2010.
Direct parsing of discontin-uous constituents in German.
In Proceedings ofSPRML 2010, the 1st Workshop on Statistical Pars-ing of Morphologically-Rich Languages, Los Ange-les, California.Wolfgang Maier.
2013.
LCFRS binarization and de-binarization for directional parsing.
In Proceedingsof IWPT 2013, the 13th International Conference onParsing Technologies, Nara, Japan.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: the Penn Treebank.
Com-putational Linguistics, 19:313?330.Roger Mitton.
1986.
A partial dictionary of English incomputer-usable form.
Literary & Linguistic Com-puting, 1(4):214?215.Mark-Jan Nederhof.
2003.
Weighted deductive pars-ing and Knuth?s algorithm.
Computational Linguis-tics, 29(1):135?143.Aarne Ranta.
2009.
The GF resource grammar library.Linguistic Issues in Language Technology, 2(2).Aarne Ranta.
2011.
Grammatical Framework: Pro-gramming with Multilingual Grammars.
CSLI Pub-lications, Stanford.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii,and Tadao Kasami.
1991.
On multiple context-free grammars.
Theoretical Computer Science,88(2):191?229.Andreas Stolcke.
1995.
An efficient probabilis-tic context-free parsing algorithm that computesprefix probabilities.
Computational Linguistics,21(2):165?201.376
