Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 977?986,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsDiscourse Complements Lexical Semanticsfor Non-factoid Answer RerankingPeter Jansen and Mihai SurdeanuUniversity of ArizonaTucson, AZ, USA{pajansen,msurdeanu}@email.arizona.eduPeter ClarkAllen Institute for Artificial IntelligenceSeattle, WA, USApeterc@allenai.orgAbstractWe propose a robust answer rerankingmodel for non-factoid questions that inte-grates lexical semantics with discourse in-formation, driven by two representationsof discourse: a shallow representation cen-tered around discourse markers, and adeep one based on Rhetorical StructureTheory.
We evaluate the proposed modelon two corpora from different genres anddomains: one from Yahoo!
Answers andone from the biology domain, and twotypes of non-factoid questions: mannerand reason.
We experimentally demon-strate that the discourse structure of non-factoid answers provides information thatis complementary to lexical semantic sim-ilarity between question and answer, im-proving performance up to 24% (relative)over a state-of-the-art model that exploitslexical semantic similarity alone.
We fur-ther demonstrate excellent domain transferof discourse information, suggesting thesediscourse features have general utility tonon-factoid question answering.1 IntroductionDriven by several international evaluations andworkshops such as the Text REtrieval Conference(TREC)1and the Cross Language Evaluation Fo-rum (CLEF),2the task of question answering (QA)has received considerable attention.
However,most of this effort has focused on factoid questionsrather than more complex non-factoid (NF) ques-tions, such as manner, reason, or causation ques-tions.
Moreover, the vast majority of QA mod-els explore only local linguistic structures, suchas syntactic dependencies or semantic role frames,1http://trec.nist.gov2http://www.clef-initiative.euwhich are generally restricted to individual sen-tences.
This is problematic for NF QA, wherequestions are answered not by atomic facts, butby larger cross-sentence conceptual structures thatconvey the desired answers.
Thus, to answer NFquestions, one needs a model of what these answerstructures look like.Driven by this observation, our main hypothe-sis is that the discourse structure of NF answersprovides complementary information to state-of-the-art QA models that measure the similarity (ei-ther lexical and/or semantic) between question andanswer.
We propose a novel answer reranking(AR) model that combines lexical semantics (LS)with discourse information, driven by two rep-resentations of discourse: a shallow representa-tion centered around discourse markers and sur-face text information, and a deep one based onthe Rhetorical Structure Theory (RST) discourseframework (Mann and Thompson, 1988).
To thebest of our knowledge, this work is the first tosystematically explore within- and cross-sentencestructured discourse features for NF AR.
The con-tributions of this work are:1.
We demonstrate that modeling discourse isgreatly beneficial for NF AR for two typesof NF questions, manner (?how?)
and rea-son (?why?
), across two large datasets fromdifferent genres and domains ?
one from thecommunity question-answering (CQA) siteof Yahoo!
Answers3, and one from a biologytextbook.
Our results show statistically sig-nificant improvements of up to 24% on top ofstate-of-the-art LS models (Yih et al, 2013).2.
We demonstrate that both shallow and deepdiscourse representations are useful, and, ingeneral, their combination performs best.3.
We show that discourse-based QA models us-ing inter-sentence features considerably out-3http://answers.yahoo.com977perform single-sentence models when an-swers span multiple sentences.4.
We demonstrate good domain transfer per-formance between these corpora, suggestingthat answer discourse structures are largelyindependent of domain, and thus broadly ap-plicable to NF QA.2 Related WorkThe body of work on factoid QA is too broad to bediscussed here (see, e.g., the TREC workshops foran overview).
However, in the context of LS, Yihet al (2013) recently addressed the problem of an-swer sentence selection and demonstrated that LSmodels, including recurrent neural network lan-guage models (RNNLM), have a higher contribu-tion to overall performance than exploiting syntac-tic analysis.
We extend this work by showing thatdiscourse models coupled with LS achieve the bestperformance for NF AR.The related work on NF QA is considerablymore scarce, but several trends are clear.
First,most NF QA approaches tend to use multiple sim-ilarity models (information retrieval or alignment)as features in discriminative rerankers (Riezler etal., 2007; Higashinaka and Isozaki, 2008; Ver-berne et al, 2010; Surdeanu et al, 2011).
Sec-ond, and more relevant to this work, all these ap-proaches focus either on bag-of-word representa-tions or linguistic structures that are restricted tosingle sentences (e.g., syntactic dependencies, se-mantic roles, or standalone discourse cue phrases).Answering how questions using a single dis-course marker, by, was previously explored byPrager et al (2000), who searched for by followedby a present participle (e.g.
by *ing) to elevate an-swer candidates in a ranking framework.
Verberneet al (2011) extracted 47 cue phrases such as be-cause from a small collection of web documents,and used the cosine similarity between an answercandidate and a bag of words containing these cuephrases as a single feature in their reranking modelfor non-factoid why QA.
Extending this, Oh etal.
(2013) built a classifier to identify causal re-lations using a small set of cue phrases (e.g., be-cause and is caused by).
This classifier was thenused to extract instances of causal relations in an-swer candidates, which were turned into featuresin a reranking model for Japanense why QA.In terms of discourse parsing, Verberne et al(2007) conducted an initial evaluation of the util-ity of RST structures to why QA by evaluatingFigure 1: Architecture of the reranking framework for QA.performance on a small sample of seven WSJ ar-ticles drawn from the RST Treebank (Carlson etal., 2003).
They later concluded that while dis-course parsing appears to be useful for QA, auto-mated discourse parsing tools are required beforethis approach can be tested at scale (Verberne etal., 2010).
Inspired by this previous work and re-cent work in discourse parsing (Feng and Hirst,2012), our work is the first to systematically ex-plore structured discourse features driven by sev-eral discourse representations, combine discoursewith lexical semantic models, and evaluate theserepresentations on thousands of questions usingboth in-domain and cross-domain experiments.3 ApproachThe proposed answer reranking component is em-bedded in the QA framework illustrated in Figure1.
This framework functions in two distinct sce-narios, which use the same AR model, but differin the way candidate answers are retrieved:CQA: In this scenario, the task is defined asreranking all the user-posted answers for a particu-lar question to boost the community-selected bestanswer to the top position.
This is a commonlyused setup in the CQA community (Wang et al,2009).4Thus, for a given question, all its answersare fetched from the answer collection, and an ini-tial ranking is constructed based on the cosine sim-ilarity between theirs and the question?s lemmavector representations, with lemmas weighted us-ing tf.idf (Ch.
6, (Manning et al, 2008)).4Although most of these works use shallow textual fea-tures and focus mostly on meta data, e.g., number of votesfor a particular answer.
Here we use no meta data and relysolely on linguistic features.978Traditional QA: In this scenario answers aredynamically constructed from larger docu-ments (Pasca, 2001).
We use this setup to answerquestions from a biology textbook, where eachsection is indexed as a standalone document, andeach paragraph in a given document is consideredas a candidate answer.
We implemented the docu-ment indexing and retrieval stage using Lucene5.The candidate answers are scored using a linearinterpolation of two cosine similarity scores:one between the entire parent document andquestion (to model global context), and a secondbetween the answer candidate and question (forlocal context).6Because the number of answercandidates is typically large (e.g., equal to thenumber of paragraphs in the textbook), we returnthe N top candidates with the highest scores.These answer candidates are then passed to theanswer reranking component, the focus of thiswork.
AR analyzes the candidates using moreexpensive techniques to extract discourse and LSfeatures (detailed in ?4), and these features arethen used in concert with a learning framework torerank the candidates and elevate correct answersto higher positions.
For the learning framework,we used SVMrank, a variant of Support VectorMachines for structured output adapted to rank-ing problems.7In addition to these features, eachreranker also includes a single feature containingthe score of each candidate, as computed by theabove candidate retrieval (CR) component.84 Models and FeaturesWe propose two separate discourse representationschemes ?
one shallow, centered around discoursemarkers, and one deep, based on RST.4.1 Discourse Marker ModelThe discourse marker model (DMM) extractscross-sentence discourse structures centeredaround a discourse marker.
This extraction pro-cess is illustrated in the top part of Figure 2.
Thesestructures are represented using three components:(1) A discourse marker from Daniel Marcu?s list5http://lucene.apache.org6We empirically observed that this combination of scoresperforms better than using solely the cosine similarity be-tween the answer and question.7http://www.cs.cornell.edu/people/tj/svm_light/svm_rank.html8Including these scores as features in the reranker modelis a common strategy that ensures that the reranker takes ad-vantage of the analysis already performed by the CR model.
(see Appendix B in Marcu (1997)), that serves asa divisive boundary between sentences.
Examplesof these markers include and, in, that, for, if, as,not, by, and but; (2) two marker arguments, i.e.,text segments before and after the marker, labeledto indicate if they are related to the question text ornot; and (3) a sentence range around the marker,which defines the length of these segments (e.g.,?2 sentences).
For example, a marker featuremay take the form of: QSEG BY OTHER SR2,which means that the the marker by has beendetected in an answer candidate.
Further, the textpreceeding by matches text from the question (andis therefore labeled QSEG), while the text after bydiffers considerably from the question text, andis labeled OTHER.
In this particular example, thescope of this similarity matching occurs over aspan of ?2 sentences around the marker.Note that our marker arguments are akin toEDUs in RST, but, in this shallow representa-tion, they are simply constructed around discoursemarkers and bound by an arbitrary sentence range.Argument Labels: We label marker argumentsbased on their similarity to question content.
Iftext before or after a marker out to a given sen-tence range matches the entire text of the ques-tion (with a cosine similarity score larger than athreshold), that argument takes on the label QSEG,or OTHER otherwise.
In this way the features areonly partially lexicalized with the discourse mark-ers.
Argument labels indicate only if lemmas fromthe question were found in a discourse structurepresent in an answer candidate, and do not speakto the specific lemmas that were found.
We showin ?5 that these lightly lexicalized features performwell in domain and transfer between domains.
Weexplore other argument labeling strategies in ?5.7.Feature Values: Our reranking framework usesreal-valued features.
The values of the discoursefeatures are the mean of the similarity scores (e.g.,cosine similarity using tf.idf weighting) of the twomarker arguments and the corresponding question.For example, the value of the QSEG BY QSEG SR1feature in Figure 2 is the average of the cosine sim-ilarities of the question text with the answer textsbefore/after by out to a distance of one sentencebefore/after the marker.It is important to note that these discoursefeatures are more expressive than features basedon discourse markers alone (Higashinaka andIsozaki, 2008; Verberne et al, 2010).
First,979Figure 2: Top: Example feature generation for the discourse marker model, for one question (Q) and one answer candidate(AC).
Answer candidates are searched for discourse markers (italic) and question word matches (bold), which are used togenerate features both within-sentence (SR0), and ?1 sentence (SR1).
The actual DMM exhaustively generates features for allmarkers and all sentence ranges.
Here we show just a few for brevity.
Bottom: Example feature generation for the discourseparser model using the output of an actual discourse parser.
The DPM creates one feature for each individual discourse relation.the argument sequences used here capture cross-sentence discourse structures.
Second, these fea-tures model the intensity of the match between thetext surrounding the discourse structure and thequestion text using both the assigned argument la-bels and the feature values.4.2 Discourse Parser ModelThe discourse parser model (DPM) is based on theRST discourse framework (Mann and Thompson,1988).
In RST, the text is segmented into a se-quence of non-overlapping fragments called ele-mentary discourse units (EDUs), and binary dis-course relations recursively connect neighboringunits.
Most relations are hypotactic, where oneof the units in the relation (the nucleus) is consid-ered more important than the other (the satellite).A few relations are paratactic, where both partici-pants have equal importance.
In the bottom part ofFigure 2, we show hypotactic relations as directedarrows, from the nucleus to the satellite.
In thiswork, we construct the RST discourse trees usingthe parser of Feng and Hirst (2012).Relying on a proper discourse framework facil-itates the modeling of the numerous implicit re-lations that are not driven by discourse markers(see Ch.
21 in Jurafsky and Martin (2009)).
How-ever, this also introduces noise because discourseanalysis is a complex task and discourse parsersare not perfect.
To mitigate this, we used a sim-ple feature generation strategy, which creates onefeature for each individual discourse relation byconcatenating the relation type with the labels ofthe discourse units participating in it.
To this end,for every relation, we extract the entire text dom-inated by each of its arguments, and we gener-ate labels for the two participants in the relationusing the same strategy as the DMM (based onthe similarity with the question content).
Similarto the DMM, these features take real values ob-tained by averaging the cosine similarity of the ar-guments with the question content.9Fig.
2 showsseveral such features, created around two RSTElaboration relations, indicating that the lattersentences expand on the information at the begin-ning of the answer.
Other common relations in-clude Attribution, Contrast, Background, andEvaluation.4.3 Lexical Semantics ModelInspired by the work of Yih et al (2013), we in-clude lexical semantics in our reranking model.Several of their proposed models rely on propri-etary data; here we focus on LS models that relyon open-source data and frameworks.
In particu-lar, we use the recurrent neural network languagemodel (RNNLM) of Mikolov et al (2013; 2010).Like any language model, a RNNLM estimates theprobability of observing a word given the preced-ing context, but, in this process, it learns wordembeddings into a latent, conceptual space witha fixed number of dimensions.
Consequently, re-lated words tend to have vectors that are close toeach other in this space.We derive two LS measures from these vec-tors, which are then are included as features inthe reranker.
The first is a measure of the over-all LS similarity of the question and answer can-9We investigated more complex features, e.g., by explor-ing depths of two and three in the discourse tree, and alsomodels that relied on tree kernels over these trees, but noneimproved upon this simple representation.
This suggests that,in the domains explored here, there is a degree of noise intro-duced by the discourse parser, and the simple features pro-posed here are the best strategy to avoid overfitting on it.980didate, which is computed as the cosine similaritybetween the two composite vectors of the ques-tion and the answer candidate.
These compositevectors are assembled by summing the vectors forindividual question (or answer candidate) words,and re-normalizing this composite vector to unitlength.
Both this overall similarity score, as wellas the average pairwise cosine similarity betweeneach word in the question and answer candidate,serve as features.5 Experiments5.1 DataTo test the utility of our approach, we experi-mented with the two QA scenarios introduced in?3 using the following two datasets:Yahoo!
Answers Corpus (YA): Yahoo!
An-swers10is an open domain community-generatedQA site, with questions and answers that span for-mal and precise to informal and ambiguous lan-guage.
Due to the speed limitations of the dis-course parser, we randomly drew 10,000 QA pairsfrom the corpus of how questions described bySurdeanu et al (2011) using their filtering crite-ria, with the additional criterion that answers hadto contain at least four community-generated an-swers, one of which was voted as the top answer.The number of answers to each question rangedfrom 4 to over 50, with the average 9.11Biology Textbook Corpus (Bio): This corpus fo-cuses on the domain of cellular biology, and con-sists of 185 how and 193 why questions hand-crafted by a domain expert.
Each question hasone or more gold answers identified in Campbell?sBiology (Reece et al, 2011), a popular under-graduate text.
The entire biology text (at para-graph granularity) serves as the possible set of an-swers.
Note that while our system retrieves an-swers at paragraph granularity, the expert was notconstrained in any way during the annotation pro-cess, so gold answers might be smaller than a para-graph or span multiple paragraphs.
This compli-cates evaluation metrics on this dataset (see ?5.3).10http://answers.yahoo.com11Note that our experimental setup, i.e., reranking all theanswers provided for each question, is different from that ofSurdeanu et al For each question, they retrieved candidateanswers from all answers voted as best for some question inthe collection.
The setup in this paper, commonly used in theCQA community (Wang et al, 2009), is more relevant herebecause it includes both high and low quality answers.For the YA CQA corpora, 50% of QA pairswere used for training, 25% for development, and25% for test.
Because of the small size of theBio corpus, it was evaluated using 5-fold cross-validation, with three folds for training, one fordevelopment, and one for test.The following additional resources were used:DiscourseMarkers: A set of 75 high-frequency12single-word discourse markers were extractedfrom Marcu?s (1997) list of cue phrases, and usedfor feature generation in DMM.
These discoursemarkers are extremely common in the answer cor-pora ?
for example, the YA corpus contains an av-erage of 7 markers per answer.Discourse Trees: We generated all discourse treesusing the parser of Feng and Hirst (2012).
ForYA, we parsed entire answers.
For Bio, we parsedindividual paragraphs.
Note that, because thesedomains are considerably different from the RSTTreebank, the parser fails to produce a tree ona large number of answer candidates: 6.2% forYA, and 41.1% for Bio.
In these situations, weconstructed artificial discourse trees using a right-attachment heuristic and a single relation label X.Lexical Semantics: We trained two differentRNNLMs for this work.
First, for the YA exper-iments we trained an open-domain RNNLM us-ing the entire Gigaword corpus of approximately4G words.13For the Bio experiments, we traineda domain specific RNNLM over a concatenationof the textbook and a subset of Wikipedia spe-cific to biology.
The latter was created by ex-tracting: (a) pages matching a word/phrase in aglossary of biology (derived from the textbook);plus (b) pages hyperlinked from (a) that are alsotagged as being in a small set of (hand-selected)biology-related categories.
The combined datasetcontains 7.7M words.
For all RNNLMs we used200-dimensional vectors.5.2 Hyper Parameter TuningThe following hyper parameters were tuned usinggrid search to maximize P@1 on each develop-ment partition: (a) the segment matching thresh-olds that determine the minimum cosine simi-larity between an answer segment and a ques-tion for the segment to be labeled QSEG; and (b)12We selected all cue phrases with more than 100 occur-rences in the Brown corpus.13LDC catalog number LDC2012T21981P@1 MRR# Model/Features P@1 Impr.
MRR Impr.YA Corpus1 Random Baseline 14.29 26.122 CR Baseline 19.57 43.143 CR + DMM 24.05?+23% 46.40?+8%4 CR + DPM 24.29?+24% 46.81?+9%5 CR + DMM + DPM 24.81?+27% 47.10?+9%6 CR + LS Baseline 26.57 49.317 CR + LS + DMM 29.29?+10% 50.99?+3%8 CR + LS + DPM 28.73?+8% 50.77?+3%9 CR + LS + DMM + DPM 30.49?+15% 51.89?+5%Bio HOW10 CR Baseline 24.12 32.9011 CR + DMM 29.88?+24% 38.88?+18%12 CR + DPM 28.93?+20% 37.75?+15%13 CR + DMM + DPM 30.43?+26% 39.28?+19%14 CR + LS Baseline 25.35 33.7915 CR + LS + DMM 30.09?+19% 39.04?+16%16 CR + LS + DPM 28.50 +12% 37.58?+11%17 CR + LS + DMM + DPM 30.68?+21% 39.44?+17%Bio WHY18 CR Baseline 28.62 38.2519 CR + DMM 38.01?+33% 46.39?+21%20 CR + DPM 38.62?+35% 46.85?+23%21 CR + DMM + DPM 39.36?+38% 47.64?+25%22 CR + LS Baseline 31.73 39.8923 CR + LS + DMM 38.60?+22% 46.41?+16%24 CR + LS + DPM 39.45?+24% 47.38?+19%25 CR + LS + DMM + DPM 39.32?+24% 47.86?+20%Table 1: Overall results across three datasets.
The improve-ments in each section are computed relative to their respectivebaseline (CR or CR + LS).
Bold font indicates the best scorein a given column.
?indicates that a score is significantly bet-ter (p < 0.05) than the score of the corresponding baseline.All significance tests were implemented using one-tailed non-parametric bootstrap resampling using 10,000 iterations.SVMrank?s regularization parameter C. For all ex-periments, the sentence range parameter (SRx) forDMM ranged from 0 (within sentence) to ?3 sen-tences.145.3 Evaluation MetricsFor YA, we used the standard implementations forP@1 and mean reciprocal rank (MRR) (Manninget al, 2008).
In the Bio corpus, because answercandidates are not guaranteed to match gold anno-tations exactly, these metrics do not immediatelyapply.
We adapted them to this dataset by weigh-ing each answer by its overlap with gold answers,where overlap is measured as the highest F1 scorebetween the candidate and a gold answer.
Thus,P@1 reduces to this F1 score for the top answer.For MRR, we used the rank of the candidate withthe highest overlap score, weighed by the inverseof the rank.
For example, if the best answer for aquestion appears at rank 2 with an F1 score of 0.3,the corresponding MRR score is 0.3/2.14This was only limited to reduce the combinatorial expan-sion of feature generation, and in principle could be set muchbroader.5.4 Overall ResultsTable 1 analyzes the performance of the proposedreranking model on the three datasets and againsttwo baselines.
The first baseline sorts the can-didate answers in descending order of the scoresproduced by the candidate retrieval (CR) module.The second baseline (CR + LS) trains a rerank-ing model without discourse, using just the CRand LS features.
For YA, we include an addi-tional baseline that selects an answer randomly.We list multiple versions of the proposed rerank-ing model, broken down by the features used.
ForBio, we retrieved the top 20 answer candidates inCR.
At this setting, the oracle performance (i.e.,the performance with perfect reranking of the 20candidates) was 69.6% P@1 for Bio HOW, and72.3% P@1 for Bio WHY.
These relatively loworacle scores, which serve as a performance ceil-ing for our approach, highlight the difficulty of thetask.
For YA, we used all answers provided foreach given question.
For all experiments we useda linear SVM kernel.15Examining Table 1, several trends are clear.Both discourse models significantly increase bothP@1 and MRR performance over all baselinesbroadly across genre, domain, and question types.More specifically, DMM and DPM show similarperformance benefits when used individually, buttheir combination generally outperforms the indi-vidual models, illustrating the fact that the twomodels capture related but different discourse in-formation.
This is a motivating result for discourseanalysis, especially considering that the discourseparser was trained on a domain different from thecorpora used here.Lexical semantic features increase performancefor all settings, but demonstrate far more utilityto the open-domain YA corpus.
This disparityis likely due to the difficulty in assembling LStraining data at an appropriate level for the bi-ology corpus, contrasted with the relative abun-dance of large scale open-domain lexical seman-tic resources.
For the YA corpus, where lexicalsemantics showed the most benefit, simply adding15The performance of all models can ultimately be in-creased by using more sophisticated learning frameworks,and considering more answer candidates in CR (for Bio).For example, SVMs with polynomial kernels of degree twoshowed approximately half a percent (absolute) performancegain over the linear kernel.
However, this came at the ex-pense of an experiment runtime about an order of magni-tude larger.
Experiments with more answer candidates in Bioshowed similar trends to the results reported.982Q How does myelination affect action potentials?AbaselineThe major selective advantage of myelination is its space ef-ficiency.
A myelinated axon 20 microns in diameter has aconduction speed faster than that of a squid giant axon [.
.
.
].Furthermore, more than 2,000 of those myelinated axons canbe packed into the space occupied by just one giant axon.ArerankA nerve impulse travels [.
.
. ]
to the synaptic terminals bypropagation of a series action potentials along the axon.
Thespeed of conduction increases [.
.
. ]
with myelination.
Actionpotentials in myelinated axons jump between the nodes ofRanvier, a process called saltatory conduction.Table 2: An example question from the Biology corpuswhere the correct answer is elevated to the top position bythe discourse model.
Abaselineis the top answer proposed bythe CR + LS baseline, which is incorrect, whereas Arerankisthe correct answer boosted to the top after reranking.
[.
.
.
]indicates non-essential text that was removed for space.LS features to the CR baseline increases baselineP@1 performance from 19.57 to 26.57, a +36%relative improvement.
Most importantly, compar-ing lines 5 and 9 with their respective baselines(lines 2 and 6, respectively) indicates that LS islargely orthogonal to discourse.
Line 5, the top-performing model with discourse but without LSoutperforms the CR baseline by +5.24 absoluteP@1 improvement.
Similarly, line 9, the top-performing model that combines discourse withLS has a +5.69 absolute P@1 improvement overthe CR + LS baseline.
That this absolute perfor-mance increase is nearly identical indicates thatLS features are complementary to and additivewith the full discourse model.
Indeed, an analy-sis of the questions improved by discourse vs. LS(line 5 vs. 6) showed that the intersection of thetwo sets is low (approximately a third of each set).Finally, while the discourse models performwell for HOW or manner questions, performanceon Bio WHY corpus suggests that reason ques-tions are particularly amenable to discourse anal-ysis.
Relative improvements on WHY questionsreach +38% (without LS) and +24% (with LS),with absolute performance on these non-factoidquestions jumping from 28% to nearly 40% P@1.Table 2 shows one example where discoursehelps boost the correct answer to the top posi-tion.
In this example, the correct answer con-tains multiple Elaboration relations that are bothcross sentence (e.g., between the first two sen-tences) and intra-sentence (e.g., between the firstpart of the second sentence and the phrase ?withmyelination?).
Model features associated withElaboration relations are ranked highly by thelearned model.
In contrast, the answer preferredby the baseline contains mostly Joint relations,Range Bio HOW Bio WHY YACR + LS + DMM + DPMwithin-sentence +0.8% +8.4% +13.1%full model +21.0%?+23.9%?+14.8%Table 3: Relative P@1 performance increase over the CR+ LS baseline for a model containing only intra-sentence fea-tures, compared to the full model.which ?represent the lack of a rhetorical relationbetween the two nuclei?
(Mann and Thompson,1988) and have very small weights in the model.5.5 Intra vs. Inter-sentence FeaturesTo tease apart the relative contribution of dis-course features that occur only within a singlesentence versus features that span multiple sen-tences, we examined the performance of the fullmodel when using only intra-sentence features,i.e., SR0 features for DMM, and features based ondiscourse relations where both EDUs appear in thesame sentence for DPM, versus the full intersen-tence models.
The results are shown in Table 3.For the Bio corpus where answer candidatesconsist of entire paragraphs of a biology text, over-all performance is dominated by inter-sentencediscourse features.
Conversely, for YA, a largeproportion of performance comes from featuresthat span only a single sentence.
This is causedby the fact that YA answers are far shorter andof variable grammatical quality, with 39% of an-swer candidates consisting of only a single sen-tence, and 57% containing two or fewer sentences.All in all, this experiment emphasizes that model-ing both intra- and inter-sentence discourse (whereavailable) is beneficial for non-factoid QA.5.6 Domain TransferBecause these discourse models appear to cap-ture high-level information about answer struc-tures, we hypothesize that the models should makeuse of many of the same discourse features, evenwhen training on data from different domains.
Ta-ble 4 shows that of the highest-weighted SVMfeatures learned when training models for HOWquestions on YA and Bio, many are shared (e.g.,56.5% of the features in the top half of both DPMsare shared), suggesting that a core set of discoursefeatures may be of utility across domains.To test the generality of these features, we per-formed a transfer study where the full model wastrained and tuned on the open-domain YA cor-pus, then evaluated as is on Bio HOW.
This is983Model Top 10% Top 25% Top 50%DMM 20.2% 33.2% 49.4%DPM 22.2% 39.1% 56.5%Table 4: Percentage of top features with the highest SVMweights that are shared between Bio HOW and YA models.a somewhat radical setup, where the target cor-pus has both a different genre (formal text vs.CQA) and different domain (biology vs. open do-main).
These experiments were performed in sev-eral groups: both with and without LS features, aswell as using either a single SVM or an ensem-ble model that linearly interpolates the predictionsof two SVM classifiers (one each for DMM andDPM).16The results are summarized in Table 5.The transferred models always outperform thebaselines, but only the ensemble model?s improve-ment is statistically significant.
This confirms ex-isting evidence that ensemble models perform bet-ter cross-domain because they overfit less (Domin-gos, 2012; Hastie et al, 2009).
The ensemblemodel without LS (third line) has a nearly identi-cal P@1 score as the equivalent in-domain model(line 13 in Table 1), while slightly surpassing in-domain MRR performance.
To the best of ourknowledge, this is one of the most striking demon-strations of domain transfer in answer rankingfor non-factoid QA, and highlights the generalityof these discourse features in identifying answerstructures across domains and genres.The results of the transferred models that in-clude LS features are slightly lower, but still ap-proach statistical significance for P@1 and are sig-nificant for MRR.
We hypothesize that the limitedtransfer observed for models with LS compared totheir counterparts without LS is due to the dispar-ity in the size and utility of the biology LS trainingdata compared to the open-domain LS resources.The open-domain YA model learns to place moreweight on LS features, which are unable to providethe same utility in the biology domain.5.7 Integrating Discourse and LSSo far, we have treated LS and discourse as dis-tinct features in the reranking model, However,given that LS features greatly improve the CRbaseline, we hypothesize that a natural extension16The interpolation parameter was tuned on the YA devel-opment corpus.
The in-domain performance of the ensemblemodel is similar to that of the single classifier in both YA andBio HOW so we omit these results here for simplicity.P@1 MRRModel/Features P@1 Impr.
MRR Impr.Transfer: YA?
Bio HOWCR Baseline 24.12 32.90CR + DMM + DPM 27.13 +13% 36.36?
+11%(CR + DMM) ?
30.10?+25% 39.62?+20%(CR + DPM)CR + LS Baseline 25.35 33.79CR + LS + DMM + DPM 25.79 +2% 35.58 +5%(CR + LS + DMM) ?
29.54?
+17% 38.68?+15%(CR + LS + DPM)Table 5: Transfer performance from YA to Bio HOW forsingle classifiers and ensembles (denoted with a ?).
?
indi-cates approaching statistical significance with p = 0.07 or0.06.to the discourse models would be to make use ofLS similarity (in addition to the traditional infor-mation retrieval similarity) to label discourse seg-ments.
For example, for the question ?How docells replicate?
?, answer discourse segments con-taining LS associates of cell and replicate, e.g., nu-cleus, membrane, genetic, and duplicate, shouldbe considered as related to the question (i.e., belabeled QSEG).
We implemented two such mod-els, denoted DMMLSand DPMLS, by replacingthe component that assigns argument labels withone that relies on LS.
Specifically, as in ?4.3, wecompute the cosine similarity between the com-posite LS vectors of the question text and eachmarker argument (in DMM) or EDU (in DPM),and label the corresponding answer segment QSEGif this score is higher than a threshold, or OTHERotherwise.
This way, the DMM and DPM featuresjointly capture discourse structures and semanticsimilarity between answer segments and question.To test this, we use the YA corpus, which hasthe best-performing LS model.
Because we areadding two new discourse models, we now tunefour segment matching thresholds, one for eachof the DMM, DPM, DMMLS, and DPMLSmod-els.17The results are shown in Table 6.
These re-sults demonstrate that incorporating LS in the dis-course models further increases performance forall configurations, nearly doubling the relative per-formance benefits over models that do not inte-grate LS and discourse (compare with lines 6?9of Table 1).
For example, the last model in thetable, which combines four discourse representa-tions, improves P@1 by 24%, whereas the equiv-alent model without this integration (line 9 in Ta-ble 1) outperforms the baseline by only 15%.17These hyperparameters were tuned on the developmentcorpus, and were found to be stable over broad ranges.984P@1 MRRModel Features P@1 Impr.
MRR Impr.CR + LS Baseline 26.57 49.31CR + LS + DMM + DMMLS32.41?+22% 53.55?+9%CR + LS + DPM + DPMLS31.21?+18% 52.50?+7%CR + LS + DMM + DPM + 32.93?+24% 53.91?+9%DMMLS+ DPMLSTable 6: YA results with integrated discourse and LS.5.8 Error AnalysisWe performed an error analysis of the full QAmodel (CR + LS + DMM + DPM) across the en-tire Bio corpus (lines 17 and 25 from Table 1).
Wechose the Bio setup for this analysis because it ismore complex than the CQA one: here gold an-swers may have a granularity completely differ-ent from what the system choses as best answers(in our particular case, the QA system is currentlylimited to answers consisting of single paragraphs,whereas gold answers may be of any size).Here, 94 of the 378 Bio HOW and WHY ques-tions have improved answer scores, while 36 havereduced performance relative to the CR baseline.Of these 36 questions where answer scores de-creased, nearly two thirds were directly related tothe paragraph granularity of the candidate answerretrieval (see ?5.1):Same Subsection (50%): In these cases, themodel selected an on-topic answer paragraph inthe same subsection of the textbook as a gold an-swer.
Often times this paragraph directly precededor followed the gold answer.Answer Window Size (14%): Here, both the CRand full model chose a paragraph containing a dif-ferent gold answer.
However, as discussed, goldanswers may unevenly straddle paragraph bound-aries, and the paragraph chosen by the model hap-pened to have a somewhat lower overlap with itsgold answer than the one chosen by the baseline.Similar Topic (25%): The model chose a para-graph that had a similar topic to the question, butdoesn?t answer the question.
These are challeng-ing errors, often associated with short questions(e.g.
?How does HIV work??)
that provide fewkeywords.
In these cases, discourse features tendto dominate, and shift the focus towards answersthat have many discourse structures deemed rel-evant.
For example, for the above question, themodel chose a paragraph containing many dis-course structures positively correlated with high-quality answers, but which describes the originsof HIV instead of how the virus enters a cell.Similar Words, Different Topic (8%): Themodel chose a paragraph that had many of thesame words as the question, but is on a differenttopic.
For example, for the question ?How are fos-sil fuels formed, and why do they contain so muchenergy?
?, the model selected an answer that men-tions fossil fuels in a larger discussion of humanecological footprints.
Here, the matching of bothkeywords and discourse structures shifted the an-swer towards a different, incorrect topic.Finally, in one case (3%), the model identifiedan answer paragraph that contained a gold answer,but was missed by the domain expert annotator.In summary, this analysis suggests that, for themajority of errors, the QA system selects an an-swer that is both topical and adjacent to a gold an-swer selected by the domain expert.
This suggeststhat most errors are minor and are driven by cur-rent limitations of our answer boundary selectionmechanism, rather than the inherent limitations ofthe discourse model.6 ConclusionsThis work focuses on two important aspects of an-swer reranking for non-factoid QA: similarity be-tween question and answer content, and answerstructure.
While the former has been addressedwith a variety of lexical-semantic models, the lat-ter has received little attention.
Here we showhow to model answer structures using discourseand how to integrate the two aspects into a holis-tic framework.
Empirically we show that model-ing answer discourse structures is complementaryto modeling lexical semantic similarity and thatthe best performance is obtained when they aretightly integrated.
We evaluate the proposed ap-proach on multiple genres and question types andobtain benefits of up to 24% relative improvementover a strong baseline that combines informationretrieval and lexical semantics.
We further demon-strate that answer discourse structures are largelyindependent of domain and transfer well, even be-tween radically different datasets.This work is open source and available at:http://nlp.sista.arizona.edu/releases/acl2014.AcknowledgementsWe thank the Allen Institute for Artificial Intelli-gence for funding this work.
We would also liketo thank the three anonymous reviewers for theirhelpful comments and suggestions.985ReferencesLynn Carlson, Daniel Marcu, and Mary EllenOkurowski.
2003.
Building a Discourse-TaggedCorpus in the Framework of Rhetorical StructureTheory.
In Jan van Kuppevelt and Ronnie Smith,editors, Current Directions in Discourse and Dia-logue, pages 85?112.
Kluwer Academic Publishers.Pedro Domingos.
2012.
A few useful things to knowabout machine learning.
Communications of theACM, 55(10).Vanessa Wei Feng and Graeme Hirst.
2012.
Text-leveldiscourse parsing with rich linguistic features.
InProceedings of the Association for ComputationalLinguistics.Trevor Hastie, Robert Tibshirani, and Jerome Fried-man.
2009.
The Elements of Statistical Learn-ing: Data Mining, Inference, and Prediction, Sec-ond Edition.
Springer.Ryuichiro Higashinaka and Hideki Isozaki.
2008.Corpus-based question answering for why-questions.
In Proceedings of the Proceedings of theThird International Joint Conference on NaturalLanguage Processing (IJCNLP), pages 418?425,Hyderabad, India.Dan Jurafsky and James H. Martin.
2009.
Speechand Language Processing, Second Edition.
PrenticeHall.William C. Mann and Sandra A. Thompson.
1988.Rhetorical structure theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Daniel Marcu.
1997.
The Rhetorical Parsing, Summa-rization, and Generation of Natural Language Texts.Ph.D.
thesis, University of Toronto.Tomas Mikolov, Martin Karafiat, Lukas Burget, JanCernocky, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In Pro-ceedings of the 11th Annual Conference of the In-ternational Speech Communication Association (IN-TERSPEECH 2010).Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word repre-sentations in vector space.
In Proceedings of theInternational Conference on Learning Representa-tions (ICLR).Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.2013.
Why-question answering using intra- andinter-sentential causal relations.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics (Volume 1: Long Papers),pages 1733?1743, Sofia, Bulgaria, August.
Associa-tion for Computational Linguistics.Marius Pasca.
2001.
High-Performance, Open-Domain Question Answering from Large Text Col-lections.
Ph.D. thesis, Southern Methodist Univer-sity.John Prager, Eric Brown, Anni Coden, and DragomirRadev.
2000.
Question-answering by predictive an-notation.
In Proceedings of the 23rd annual inter-national ACM SIGIR conference on Research anddevelopment in information retrieval, SIGIR ?00,pages 184?191, New York, NY, USA.
ACM.J.B.
Reece, L.A. Urry, M.L.
Cain, S.A. Wasserman,and P.V.
Minorsky.
2011.
Campbell Biology.
Pear-son Benjamin Cummings.Stefan Riezler, Alexander Vasserman, IoannisTsochantaridis, Vibhu Mittal, and Yi Liu.
2007.Statistical machine translation for query expansionin answer retrieval.
In Proceedings of the 45th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 464?471, Prague, CzechRepublic.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answers to non-factoid questions from web collections.
Computa-tional Linguistics, 37(2):351?383.Susan Verberne, Lou Boves, Nelleke Oostdijk, Peter-Arno Coppen, et al 2007.
Discourse-based an-swering of why-questions.
Traitement Automatiquedes Langues, Discours et document: traitements au-tomatiques, 47(2):21?41.Suzan Verberne, Lou Boves, Nelleke Oostdijk, andPeter-Arno Coppen.
2010.
What is not in the bagof words for why-qa?
Computational Linguistics,36(2):229?245.Suzan Verberne, Hans Halteren, Daphne Theijssen,Stephan Raaijmakers, and Lou Boves.
2011.
Learn-ing to rank for why-question answering.
Inf.
Retr.,14(2):107?132, April.Xin-Jing Wang, Xudong Tu, Dan Feng, and Lei Zhang.2009.
Ranking community answers by modelingquestion-answer relationships via analogical reason-ing.
In Proceedings of the Annual ACM SIGIR Con-ference.Wen-tau Yih, Ming-Wei Chang, Christopher Meek, andAndrzej Pastusiak.
2013.
Question answering usingenhanced lexical semantic models.
In Proceedingsof the 51st Annual Meeting of the Association forComputational Linguistics (ACL).986
