Proceedings of NAACL HLT 2007, pages 188?195,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsHybrid Models for Semantic Classification ofChinese Unknown WordsXiaofei LuDepartment of Linguistics and Applied Language StudiesPennsylvania State UniversityUniversity Park, PA 16802, USAxxl13@psu.eduAbstractThis paper addresses the problem of clas-sifying Chinese unknown words intofine-grained semantic categories definedin a Chinese thesaurus.
We describethree novel knowledge-based models thatcapture the relationship between the se-mantic categories of an unknown wordand those of its component characters inthree different ways.
We then combinetwo of the knowledge-based models witha corpus-based model which classifiesunknown words using contextual infor-mation.
Experiments show that theknowledge-based models outperformprevious methods on the same task, butthe use of contextual information doesnot further improve performance.1 IntroductionResearch on semantic annotation has focusedprimarily on word sense disambiguation (WSD),i.e., the task of determining the appropriate sensefor each instance of a polysemous word out of aset of senses defined for the word in some lexi-con.
Much less work has been done on semanticclassification of unknown words, i.e., words thatare not listed in the lexicon.
However, real textstypically contain a large number of unknownwords.
Successful classification of unknownwords is not only useful for lexical acquisition,but also necessary for natural language process-ing (NLP) tasks that require semantic annotation.This paper addresses the problem of classify-ing Chinese unknown words into fine-grainedsemantic categories defined in a Chinese thesau-rus, Cilin (Mei et al, 1984).
This thesaurus clas-sifies over 70,000 words into 12 major catego-ries, including human (A), concrete object (B),time and space (C), abstract object (D), attributes(E), actions (F), mental activities (G), activities(H), physical states (I), relations (J), auxiliaries(K), and honorifics (L).
The 12 major categoriesare further divided into 94 medium categories,which in turn are subdivided into 1428 smallcategories.
Each small category contains syno-nyms that are close in meaning.
For example,under the major category D, the medium cate-gory Dm groups all words that refer to institu-tions, and the small category Dm05 groups allwords that refer to educational institutions, e.g.,??
xu?xi?o ?school?.
Unknown word classifi-cation involves a much larger search space thanWSD.
In classifying words into small categoriesin Cilin, the search space for a polysemousknown word consists of all the categories theword belongs to, but that for an unknown wordconsists of all the 1428 small categories.Research on WSD has concentrated on usingcontextual information, which may be limitedfor infrequent unknown words.
On the otherhand, Chinese characters carry semantic infor-mation that is useful for predicting the semanticproperties of the words containing them.
We pre-sent three novel knowledge-based models thatcapture the relationship between the semanticcategories of an unknown word and those of itscomponent characters in three different ways,and combine two of them with a corpus-basedmodel that uses contextual information to clas-sify unknown words.
Experiments show that thecombined knowledge-based model achieves anaccuracy of 61.6% for classifying unknownwords into small categories in Cilin, but the useof contextual information does not further im-prove performance.The rest of the paper is organized as follows.Section 2 details the three novel knowledge-based models proposed for this task.
Section 3describes a corpus-based model.
Section 4 re-ports the experiment results of the proposed188models.
Section 5 compares these results withprevious results.
Section 6 concludes the paperand points to avenues for further research.2 Knowledge-based ModelsThis section describes three novel knowledge-based models for semantic classification of Chi-nese unknown words, including an overlapping-character model, a character-category associationmodel, and a rule-based model.
These modelsmodel the relationship between the semanticcategory of an unknown word and those of itscomponent characters in three different ways.2.1 The Baseline ModelThe baseline model predicts the category of anunknown word by counting the number of over-lapping characters between the unknown wordand the member words in each category.
Aswords in the same category are similar in mean-ing and the meaning of a Chinese word is gener-ally the composition of the meanings of its char-acters, it is common for words in the same cate-gory to share one or more character.
This modeltests the hypothesis that speakers draw upon therepertoire of characters that relate to a conceptwhen creating new words to realize it.For each semantic category in Cilin, the set ofunique characters in its member words are ex-tracted, and the number of times each characteroccurs in word-initial, word-middle, and word-final positions is recorded.
With this informa-tion, we develop two variants of the baselinemodel, which differ from each other in terms ofwhether it takes into consideration the positionsin which the characters occur in words.In variant 1, the score of a category is the sumof the number of occurrences of each characterof the target word in the category, as in (1),where tj denotes a category, w denotes the targetword, ci denotes the ith character in w, n is thelength of w, and f(ci) is the frequency of ci in tj.
(1) ?== niij cfwtScore1)(),(In variant 2, the score of a category is the sumof the number of occurrences of each characterof the unknown word in the category in its corre-sponding position, as in (2), where pi denotes theposition of ci in w, which could be word-initial,word-middle, or word-final, and f(ci,pi) denotesthe frequency of ci in position pi in tj.
(2) =),( wtScore j ?=niii pcf1),(In each variant, the category with the maxi-mum score for a target word is proposed as thecategory of the word.2.2 Character-Category AssociationsThe relationship between the semantic categoryof an unknown word and those of its componentcharacters can also be captured in a more sophis-ticated way using information-theoretical models.We use two statistical measures, mutual infor-mation and ?2, to compute character-categoryassociations and word-category associations.Chen (2004) used the ?2 measure to computecharacter-character and word-word associations,but not word-category associations.
We useword-category associations to directly predictthe semantic categories of unknown words.The mutual information and ?2 measures arecalculated as in (3) and (4), where Asso(c,tj) de-notes the association between a character c and asemantic category tj, and P(X) and f(X) denotethe probability and frequency of X respectively.
(3))()(),(log),(jjjMI tPcPtcPtcAsso =(4)),(max),(),(2kkjj tctctcAsso ???
=(5))()()],([),(2jjj tfcftcftc +=?Once the character-category associations arecalculated, the association between a word w anda category tj, Asso(w,tj), can be calculated as thesum of the weighted associations between eachof the word?s characters and the category, as in(6), where ci denotes the ith character of w, |w|denotes the length of w, and ?i denotes the weightof Asso(ci,tj).
The ?
?s add up to 1.
The weightsare determined empirically based on the posi-tions of the characters in the word.
(6) =),( jtwAsso ?=||1),(wijii tcAsso?As in variant 2 of the baseline model, thecharacter-category association model can also bemade sensitive to the positions in which thecharacters occur in the words.
To this end, wefirst need to compute the position-sensitive asso-ciations between a category and a character inthe word-initial, word-middle, and word-finalpositions separately.
The position-sensitive asso-ciation between an unknown word and a cate-gory can then be computed as the sum of theweighted position-sensitive associations betweeneach of its characters and the category.189Once the word-category associations are com-puted, we can propose the highest ranked cate-gory or a ranked list of categories for each un-known word.2.3 A Rule-Based ModelThe third knowledge-based model uses linguisticrules to classify unknown words based on thesyntactic and semantic categories of their com-ponent characters.
Rule-based models have notbeen used for this task before.
However, thereare some regularities in the relationship betweenthe semantic categories of unknown words andthose of their component characters that can becaptured in a more direct and effective way bylinguistic rules than by statistical models.A separate set of rules are developed forwords of different lengths.
Rules are initiallydeveloped based on knowledge about Chineseword formation, and are then refined by examin-ing the development data.
In general, the com-plete rule set takes a few hours to develop.The rule in (7) is developed for bisyllabic un-known words.
This rule proposes the commoncategory of a bisyllabic word?s two characters asits category.
It is especially useful for wordswith a parallel structure, i.e., words whose twocharacters have the same meaning and syntacticcategory, e.g., ??
t?nt?
?collapse?, where ?t?n and ?
t?
both mean ?collapse?
and share thecategory Id05.
The thresholds for fA and fB aredetermined empirically and are both set to 1 ifAB is a noun and to 0 and 3 respectively other-wise.
(7) For a bisyllabic word AB, if A and B share a cate-gory c1, let fA and fB denote the number of timesA and B occur in word-initial and word-final po-sitions in c respectively.
If fA and fB both surpassthe predetermined thresholds, propose c for AB.A number of rules are developed for trisyl-labic words.
While most rules in the model aregeneral, the first rule in this set is rather specific,as it handles words with three specific prefixes,?
d?
?big?, ?
xi?o ?little?, and ?
l?o ?old?,which usually do not change the category of theroot word.
The other four rules again utilize thecategories of the unknown word?s componentcharacters.
The rules in (8b) and (8c) are similarto the rule in (7).
The ones in (8d) and (8e)search for neighbor words with a similar struc-ture as the target word.
Eligible neighbors have a1 A and B may each belong to more than one category.common morpheme with the target word in thesame position and a second morpheme thatshares a category with the second morpheme ofthe target word.
For example, an eligibleneighbor for ???
tu?xi?o-sh?ng ?sales-man?is ???
xi?osh?u-sh?ng ?distribut-or?.
Thesetwo words share the morpheme ?
sh?ng ?busi-nessman?
in the word-final position, and themorphemes ??
tu?xi?o ?to market?
and ?
?xi?osh?u ?distribute?
share the category He03.The rule in (8d) therefore applies in this case.
(8) For a trisyllabic word ABC:a.
If A equals ?
d?
?big?, ?
xi?o ?little?, or ?l?o ?old?, propose the category of AB forABC if C is the diminutive suffix ?
er or thecategory of BC for ABC otherwise.b.
If A and BC share a category c, propose c forABC.c.
If AB and C share a category c, propose c forABC.d.
If there is a word XYC such that XY and ABshare a category, propose the category ofXYC for ABC.e.
If there is a word XBC such that X and Ashare a category, propose the category ofXBC for ABC.The rules for four-character words are givenin (9).
Like the rules in (8d) and (8e), these rulesalso search for neighbors of the target word.
(9) For a four-character word ABCD:a.
If there is a word XYZD/YZD such thatXYZ/YZ and ABC share a category, proposethe category of XYZ/YZ for ABCD.b.
If there is a word ABCX such that X and Dshare a category, propose the category ofABCX for ABCD.c.
If there is a word XYCD such that XY and ABshare a category, propose the category ofXYCD for ABCD.d.
If there is a word XBCD/BCD, propose thecategory of XBCD/BCD for ABCD.3 A Corpus-Based ModelThe knowledge-based models described aboveclassify unknown words using information aboutthe syntactic and semantic categories of theircomponent characters.
Another useful source ofinformation is the context in which unknownwords occur.
While contextual information is theprimary source of information used in WSD re-search and has been used for acquiring semanticlexicons and classifying unknown words in otherlanguages (e.g., Roark and Charniak 1998; Ci-190aramita 2003; Curran 2005), it has been used inonly one previous study on semantic classifica-tion of Chinese unknown words (Chen and Lin,2000).
Part of the goal of this study is to investi-gate whether and how these two differentsources of information can be combined to im-prove performance on semantic classification ofChinese unknown words.To this end, we first use the knowledge-basedmodels to propose a list of five candidate catego-ries for the target word, then extract a general-ized context for each category in Cilin from acorpus, and finally compute the similarity be-tween the context of the target word and the gen-eralized context of each of its candidate catego-ries.
Comparing the context of the target wordwith generalized contexts of categories insteadof contexts of individual words alleviates thedata-sparseness problem, as infrequent wordshave limited contextual information.
Limitingthe search space for each target word to the topfive candidate categories reduces the computa-tional cost that comes with the full search space.3.1 Context Extraction and RepresentationA generalized context for each semantic cate-gory is built from the contexts of its memberwords.
This is done based on the assumption thatas the words in the same category have the sameor similar meaning, they tend to occur in similarcontexts.
In terms of context extraction and rep-resentation, we need to consider four factors.Member Words The issue here is whether toinclude the contexts of polysemous memberwords in building the generalized context of acategory.
Including these contexts without dis-crimination introduces noise.
To measure theeffect of such noise, we build two versions ofgeneralized context for each category, one usingcontexts of unambiguous member words only,and the other using contexts of all memberwords.Context Words There are two issues in select-ing words for context representation.
First,words that contribute little information to thediscrimination of meaning of other words, in-cluding conjunctions, numerals, auxiliaries, andnon-Chinese sequences, are excluded.
Second, tomodel the effect of frequency on the contextwords?
contribution to meaning discrimination,we use two sets of context words: one consists ofthe 1000 most frequent words in the corpus; theother consists of all words in the corpus.Window Size For WSD, both topical contextand microcontext have been used (Ide andV?ronis 1998).
Topical context includes substan-tive words that co-occur with the target wordwithin a larger window, whereas microcontextincludes words in a small window around thetarget word.
We experiment with topical contextand microcontext with window sizes of 100 and6 respectively (i.e., 50 and 3 words to the leftand right of the target word respectively).Context Representation We represent the con-text of a category as a vector <w1, w2, ..., wn>,where n is the total number of context words,and wi is the weight of the ith context word.
Toarrive at this representation, we first record thenumber of times each context word occurswithin a specified window of each member wordof a category in the corpus as a vector <f1, f2, ...,fn>, where fi is the number of times the ith con-text word co-occurs with a member word of thecategory.
We then compute the weight of a con-text word w in context c, W(w, c), using mutualinformation and t-test, which were reported byWeeds and Weir (2005) to perform the best on apseudo-disambiguation task.
These weight func-tions are computed as in (10) and (11), where Ndenotes the size of the corpus.
(10))()(),(log),(cPwPcwPcwWPMI =(11)NcwPcPwPcwPcwWt),()()(),(),(?=3.2 Contextual Similarity MeasurementWe compute the similarity between the contextvectors of the unknown word and its candidatecategories using cosine.
The cosine of two n-dimensional vectors xrand yr, cos( xr, yr), is com-puted as in (12), where xi and yi denote theweight of the ith context word in xrand yr.(12)??
?====n1i2n1i2n1i)y,xcos(iiiiyxyxrr4 Results4.1 Experiment SetupThe models are developed and tested using theContemporary Chinese Corpus from PekingUniversity (Yu et al 2002) and the extendedCilin released by the Information Retrieval Labat Harbin Institute of Technology.
The corpus191contains all the articles published in January,1999 in People?s Daily, a major newspaper inChina.
It contains over 1.12 million tokens and isword-segmented and POS-tagged.
Table 1 sum-marizes the distribution of words in Cilin.
Of the76,029 words in Cilin, 35,151 are found in theContemporary Chinese Corpus.Length Unambiguous Polysemous Total1 2,674 2,068 4,7422 39,057 5,403 44,4603 15,112 752 15,8644 9,397 942 10,338?5 590 34 624Total 66,830 9,199 76,029Table 1: Word distribution in the extended CilinWe classify words into the third-level catego-ries in the extended Cilin, which are equivalentto the small categories in the original Cilin.
Thedevelopment and test sets consist of 3,000 wordseach, which are randomly selected from the sub-set of words in Cilin that are two to four charac-ters long, that have occurred in the Contempo-rary Chinese Corpus, and that are tagged asnouns, verbs, or adjectives in the corpus.
Thewords in the development and test sets are alsocontrolled for frequency, with 1/3 of them occur-ring 1-3 times, 3-6 times, and 7 or more times inthe corpus respectively.As Chen (2004) noted, excluding all thewords in the development and test data in thetesting stage worsens the data-sparseness prob-lem for knowledge-based models, as some cate-gories have few member words, and some char-acters appear in few words in some categories.To alleviate this problem, the remove-onemethod is used for testing the knowledge-basedmodels.
In other words, the models are re-trainedfor each test word using information about allthe words in Cilin except the test word.
The cor-pus-based model is trained once using the train-ing data only, as the data-sparseness problem isalleviated by using generalized contexts of cate-gories.
Finally, if a word is polysemous, it isconsidered to have been correctly classified ifthe proposed category is one of its categories.4.2 Results of the Baseline ModelTables 2 and 3 summarize the results of thebaseline model in terms of the accuracy of itsbest guess and best five guesses respectively.The columns labeled ?Non-filtered?
report re-sults where all categories are considered for eachunknown word, and the ones labeled ?POS-filtered?
report results where only the categoriesthat agree with the POS category of the unknownword are considered.
In the latter case, if the tar-get word is a noun, only the small categories un-der major categories A-D are considered; other-wise, only those under major categories E-L areconsidered.
The results show that using POS in-formation about the unknown word to filter cate-gories improves performance.
Variant 2 per-forms better when only the best guess is consid-ered, indicating that it is useful to model the ef-fect of position on a character?s contribution toword meaning in this case.
However, it is nothelpful to be sensitive to character position whenthe best five guesses are considered.Non-filtered POS-filtered Modelvariant Dev Test Dev Test1 0.391 0.398 0.450 0.4642 0.471 0.469 0.514 0.517Table 2: Results of the baseline model: best guessNon-filtered POS-filtered Modelvariant Dev Test Dev Test1 0.757 0.760 0.813 0.8172 0.764 0.762 0.809 0.805Table 3: Results of the baseline model: best 5 guesses4.3 Results of the Character-Category As-sociation ModelIn this model, only categories that agree with thePOS category of the unknown word and thatshare at least one character with the unknownword are considered.
These filtering steps sig-nificantly reduce the search space for this model.We discussed three parameters of the model inSection 2.2, including the statistical measure, thesensitivity to character position in computingcharacter-category associations, and the weightsof the associations between categories and char-acters in different positions.
In addition, thecomputation of the character-category associa-tions can be sensitive or insensitive to the POScategories of the words containing the characters.In the POS-sensitive way, associations are com-puted among nouns (words in categories A-D)and non-nouns (words in categories E-L) sepa-rately, whereas in the POS-insensitive way, theyare computed using all the words.Tables 4 and 5 summarize the results of thecharacter-category association model in terms ofthe accuracy of its best guess and best fiveguesses respectively.
In all cases, the weightsassigned to word-initial, word-middle, and word-final characters are 0.49, 0, and 0.51 respectively.192In terms of the best guess, the model achievesa best accuracy of 58.2%, a 6.5% improvementover the baseline result.
The results show that ?2consistently performs better than mutual infor-mation, and computing position-sensitive char-acter-category associations consistently im-proves performance.
However, computing POS-sensitive associations gives mixed results.In terms of the best five guesses, the modelachieves a best accuracy of 83.8% on the testdata, a 2.1% improvement over the best baselineresult.
Using ?2 again achieves better results.However, in this case, the best results areachieved when the character-category associa-tions are insensitive to both character positionand the POS categories of words.Sensitivity Development TestPOS Position MI ?2 MI ?2Yes Yes 0.482 0.586 0.507 0.582Yes No 0.440 0.578 0.458 0.573No Yes 0.487 0.565 0.511 0.567No No 0.457 0.555 0.459 0.559Table 4: Results of the character-category associationmodel: best guessSensitivity Development TestPOS Position MI ?2 MI ?2Yes Yes 0.735 0.805 0.720 0.810Yes No 0.743 0.828 0.754 0.821No Yes 0.702 0.813 0.718 0.812No No 0.735 0.830 0.746 0.838Table 5: Results of the character-category associationmodel: best 5 guessesDevelopment Test WordLen R P F R P F2 0.159 0.796 0.265 0.158 0.772 0.2623 0.368 0.838 0.511 0.351 0.830 0.4934 0.582 0.852 0.692 0.540 0.900 0.675All 0.218 0.816 0.344 0.216 0.803 0.340Table 6: Results of the rule-based model: best guess4.4 Results of the Rule-Based ModelTable 6 summarizes the results of the rule-based model in terms of recall, precision and F-score.
The model returns multiple categories forsome words, and it is considered to have cor-rectly classified a word only when it returns asingle, correct category for the word.
Precisionof the model is computed over all the caseswhere the model returns a single guess, and re-call is computed over all cases.
The modelachieves an overall precision of 80.3% on thetest data, much higher than the accuracy of theother two knowledge-based models.
However,recall of the model is only 21.6%.
The compara-ble results on the development and test sets indi-cate that the encoded rules are general.
Themodel generally performs better on longer wordsthan on shorter words.4.5 Combining the Character-CategoryAssociation and Rule-Based ModelsGiven that the rule-based model achieves ahigher precision but a lower recall than the char-acter-category association model, the two mod-els can be combined to improve the overall per-formance.
In general, if the rule-based modelreturns one or more categories, these categoriesare first ranked among themselves by their asso-ciations with the unknown word.
They are thenfollowed by the other categories returned by thecharacter-category association model.
Tables 7and 8 summarize the results of combining thetwo models.Sensitivity Development TestPOS Position MI ?2 MI ?2Yes Yes 0.561 0.623 0.572 0.616Yes No 0.536 0.622 0.542 0.615No Yes 0.562 0.610 0.575 0.608No No 0.530 0.601 0.532 0.606Table 7: Results of combining the character-categoryassociation and rule-based models: best guessSensitivity Development TestPOS Position MI ?2 MI ?2Yes Yes 0.834 0.846 0.845 0.843Yes No 0.791 0.860 0.801 0.851No Yes 0.760 0.848 0.742 0.845No No 0.773 0.859 0.782 0.856Table 8: Results of combining the character-categoryassociation and rule-based models: best 5 guessesIn terms of the best guess, the combinedmodel achieves an accuracy of 61.6%, a 3.4%improvement over the best result of the charac-ter-category association model alone.
This isachieved using ?2 with POS-sensitive and posi-tion-sensitive computation of character-categoryassociations.
In terms of the best five guesses,the model achieves an accuracy of 85.6%, a1.8% improvement over the best result of thecharacter-category association model alone.To facilitate comparison with previous studies,the results of the combined model in terms of itsbest guess in classifying unknown words intomajor and medium categories are summarized inTable 9.
As ?2 consistently outperforms mutualinformation, results are reported for ?2 only.With POS-sensitive and position-sensitive com-193putation of character-category associations, thecombined model achieves an accuracy of 83.0%and 69.9% for classifying unknown words intomajor and medium categories respectively.Sensitivity Development TestPOS Position Major Med Major MedYes Yes 0.840 0.705 0.830 0.699Yes No 0.831 0.698 0.828 0.698No Yes 0.832 0.692 0.825 0.692No No 0.821 0.687 0.821 0.689Table 9: Results of the combined model for classify-ing unknown words into major and medium catego-ries: best guess4.6 Results of the Corpus-Based ModelThe corpus-based model re-ranks the five high-est ranked categories proposed by the combinedknowledge-based model.
Table 10 enumeratesthe parameters of the model and lists the labelsused to denote the various settings in Table 11.Parameter Label Setting LabelMemberwordsMW All members wordsUnambiguous membersallunContextwordsCW All words1000 most frequentall1000WindowsizeWS 10061006WeightfunctionWF Mutual informationt-testmitTable 10: Parameter settings of the corpus-basedmodelTable 11 summarizes the results of 16 runs ofthe model with different parameter settings.
Thebest accuracy on the test data is 37.1%, achievedin run 5 with the following parameter settings:using unambiguous member words for buildingcontexts of categories, using all words in thecorpus for context representation, using a win-dow size of 100, and using mutual informationas the weight function.
As the combined knowl-edge-based model gives an accuracy of 85.6%for its best five guesses, the expected accuracyof a naive model that randomly picks a candidatefor each word as its best guess is 17.1%.
Com-pared with this baseline, the corpus-based modelachieves a 13.0% improvement, but it performsmuch worse than the knowledge-based models.Table 12 summarizes the accuracy of the topthree runs of the model on words with differentfrequency in the corpus.
Each of the three groupsconsists of 1,000 words that have occurred 1-2,3-6, and 7 or more times in the corpus respec-tively.
The model consistently performs betteron words with higher frequency, suggesting thatit may benefit from a larger corpus.Parameter Setting Accuracy RunID MW CW WS WF Dev Test1 un 1000 100 mi 0.326 0.3032 un 1000 100 t 0.317 0.2883 un 1000 6 mi 0.304 0.3014 un 1000 6 t 0.299 0.3015 un all 100 mi 0.359 0.3716 un all 100 t 0.292 0.2967 un all 6 mi 0.370 0.3658 un all 6 t 0.322 0.2979 all 1000 100 mi 0.302 0.29410 all 1000 100 t 0.314 0.30411 all 1000 6 mi 0.313 0.31412 all 1000 6 t 0.308 0.30813 all all 100 mi 0.336 0.33314 all all 100 t 0.287 0.30015 all all 6 mi 0.356 0.35616 all all 6 t 0.308 0.308Table 11: Results of the corpus-based modelDevelopment Test RunID 1-2 3-6 ?7 1-2 3-6 ?75 0.331 0.360 0.385 0.323 0.389 0.4027 0.323 0.363 0.423 0.335 0.357 0.40215 0.328 0.346 0.395 0.334 0.355 0.379Table 12: Results of the corpus-based model onwords with different frequency5 Related WorkThe few previous studies on semantic classifica-tion of Chinese unknown word have primarilyadopted knowledge-based models.
Chen (2004)proposed a model that retrieves the word withthe greatest association with the target word.This model is computationally more expensivethan our character-category association model,as it entails computing associations betweenevery character-category, category-character,character-character, and word-word pair.
He re-ported an accuracy of 61.6% on bisyllabic V-Vcompounds.
However, he included all the testwords in training the model.
If we also includethe test words in computing character-categoryassociations, the computationally cheaper modelachieves an overall accuracy of 75.6%, with anaccuracy of 75.1% on verbs.Chen and Chen (2000) adopted similar exem-plar-based models.
Chen and Chen used a mor-phological analyzer to identify the head of thetarget word and the semantic categories of itsmodifier.
They then retrieved examples with thesame head as the target word.
Finally, they com-puted the similarity between two words as the194similarity between their modifiers, using theconcept of information load (IC) of the leastcommon ancestor (LCA) of the modifiers?
se-mantic categories.
They reported an accuracy of81% for classifying 200 unknown nouns.
Giventhe small test set of their study, it is hard to di-rectly compare their results with ours.Tseng used a morphological analyzer in thesame way, but she also derived the morpho-syntactic relationship between the morphemes.She retrieved examples that share a morphemewith the target word in the same position andfiltered those with a different morpho-syntacticrelationship.
Finally, she computed the similaritybetween two words as the similarity betweentheir non-shared morphemes, using a similarconcept of IC of the LCA of two categories.
Sheclassified unknown words into the 12 majorcategories only, and reported accuracies 65.8%on adjectives, 71.4% on nouns, and 52.8% onverbs.
These results are not as good as the 83.0%overall accuracy our combined knowledge-basedmodel achieved for classifying unknown wordsinto major categories.Chen and Lin (2000) is the only study thatused contextual information for the same task.To generate candidate categories for a word,they looked up its translations in a Chinese-English dictionary and the synsets of the transla-tions in WordNet, and mapped the synsets to thecategories in Cilin.
They used a corpus-basedmodel similar to ours to rank the candidates.They reported an accuracy of 34.4%, which isclose to the 37.1% accuracy of our corpus-basedmodel, but lower than the 61.6% accuracy of ourcombined knowledge-based model.
In addition,they could only classify the unknown wordslisted in the Chinese-English dictionary.6 ConclusionsWe presented three knowledge-based modelsand a corpus-based model for classifying Chi-nese unknown words into fine-grained categoriesin the Chinese thesaurus Cilin, a task importantfor lexical acquisition and NLP applications thatrequire semantic annotation.
The knowledge-based models use information about the catego-ries of the unknown words?
component charac-ters, while the corpus-based model uses contex-tual information.
By combining the character-category association and rule-based models, weachieved an accuracy of 61.6%.
The corpus-based model did not improve performance.Several avenues can be taken for further re-search.
First, additional resources, such as bilin-gual dictionaries, morphological analyzers, par-allel corpora, and larger corpora with richer lin-guistic annotation may prove useful for improv-ing both the knowledge-based and corpus-basedmodels.
Second, we only explored one way tocombine the knowledge-based and corpus-basedmodels.
Future work may explore alternativeways to combine these models to make betteruse of contextual information.ReferencesC.-J.
Chen.
2004.
Character-sense association andcompounding template similarity: Automatic se-mantic classification of Chinese compounds.
InProceedings of the 3rd SIGHAN Workshop on Chi-nese Language Processing, pages 33?40.M.
Ciaramita and M. Johnson.
2003.
Supersense tag-ging of unknown nouns in WordNet.
In Proceed-ings of EMNLP-2003, pages 594-602.K.-J.
Chen and C.-J.
Chen.
2000.
Automatic semanticclassification for Chinese unknown compoundnouns.
In Proceedings of COLING-2000, pages173-179.H.-H. Chen and C.-C. Lin.
2000.
Sense-tagging Chi-nese corpus.
In Proceedings of the 2nd ChineseLanguage Processing Workshop, pages 7-14.J.
Curran.
2005.
Supersense tagging of unknownnouns using semantic similarity.
In Proceedings ofACL-2006, pages 26-33.N.
Ide and J. V?ronis.
1998.
Introduction on the spe-cial issue on word sense disambiguation: The stateof the art.
Computational Linguistics 24(1):2?40.J.
Mei, Y. Zhu, Y. Gao, and H. Yin.
(eds.)
1984.Tongyici Cilin [A Thesaurus of Chinese Words].Commercial Press, Hong Kong.B.
Roark and E. Charniak.
1998.
Noun-phrase co-occurrence statistics for semi-automatic semanticlexicon construction.
In Proceedings of COL-ING/ACL-1998, pages 1110-1116.H.
Tseng.
2003.
Semantic classification of Chineseunknown words.
In Proceedings of ACL-2003 Stu-dent Research Workshop, pages 72-79.J.
Weeds and D. Weir.
2005.
Co-occurrence retrieval:A flexible framework for lexical distributionalsimilarity.
Computational Linguistics 31(4):439?475.S.
Yu, H. Duan, X. Zhu, and B.
Sun.
2002.
The basicprocessing of Contemporary Chinese Corpus atPeking University.
Journal of Chinese InformationProcessing 16(5):49?64.195
