Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 2107?2116, Dublin, Ireland, August 23-29 2014.Exploring Fine-grained Entity Type Constraints for Distantly SupervisedRelation ExtractionYang Liu Kang Liu Liheng Xu Jun ZhaoNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of SciencesZhongguancun East Road #95, Beijing 100190, China{yang.liu, kliu, lhxu, jzhao}@nlpr.ia.ac.cnAbstractDistantly supervised relation extraction, which can automatically generate training data by align-ing facts in the existing knowledge bases to text, has gained much attention.
Previous work usedconjunction features with coarse entity types consisting of only four types to train their model-s.
Entity types are important indicators for a specific relation, for example, if the types of twoentities are ?PERSON?
and ?FILM?
respectively, then there is more likely a ?DirectorOf?
rela-tion between the two entities.
However, the coarse entity types are not sufficient to capture theconstraints of a relation between entities.
In this paper, we propose a novel method to explorefine-grained entity type constraints, and we study a series of methods to integrate the constraintswith the relation extracting model.
Experimental results show that our methods achieve bet-ter precision/recall curves in sentential extraction with smoother curves in aggregated extractionwhich mean more stable models.1 IntroductionRelation Extraction is the task of extracting semantic relations between a pair of entities from sentencescontaining them.
It can potentially benefit many applications, such as knowledge base construction,question answering (Ravichandran and Hovy, 2002), textual entailment (Szpektor et al., 2005), etc.
Tra-ditional supervised approaches for relation extraction (Zhou et al., 2005)(Zhou et al., 2007) need tomanually label training data, which is expensive and limits the ability to scale up.
Due to the shortcom-ing of supervised approaches mentioned above, recently, a more promising approach named distantlysupervised relation extraction (or distant supervision for relation extraction) (Mintz et al., 2009) has be-come popular.
Instead of manual labeling, it automatically generates training data by aligning facts inexisting knowledge bases to text.However, the paradigm of distant supervision also causes new problems of noisy training data both inpositive training instances and negative training instances.
To overcome the false positive problem causedby the distant supervision assumption, researches in (Riedel et al., 2010)(Hoffmann et al., 2011)(Sur-deanu et al., 2012) proposed multi-instance models to model noisy positive training data, where theyassumed that at least one sentence in those containing an entity pair is truly positive.
Takamatsu et al.
(Takamatsu et al., 2012) claimed that the at-least-one assumption in multi-instance models would failwhen there was only one sentence containing both entities.
They proposed a method to learn and filternoisy pattern features from training instances to overcome the false positive problem.
Researchers (Xuet al., 2013)(Zhang et al., 2013)(Ritter and Etzioni, 2013) tried to address the problem of false negativetraining data caused by the incomplete knowledge base.
Xu el al.
(Xu et al., 2013) used the pseudo-relevance feedback method trying to find out the false negative instances and add them into positivetraining instances.
Zhang et al.
(Zhang et al., 2013) employed some rules to select negative training in-stances carefully, hoping not to include the false negative instances.
And Ritter et al.
(Ritter and Etzioni,2013) used hidden variables to model the missing data in databases based on a graphical model.
Thetraining data generation process for all the above work is under the framework of (Mintz et al., 2009),This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/2107one important step of which is to recognize entity mentions from text and assign them entity types whichare used to compose features for training the model.
The entity types they used are very coarse only con-sisting of four categories (PERSON, ORGANIZATION, LOCATION, NONE).
We argue that the coarseentity types are not sufficient to indicate relations.A specific relation constrains the entity types of its two entities.
For instance, the SingerOf relationlimits the entity type of its first entity as PERSON or more fine-grained ARTIST, and the entity type ofits second entity as ART or more fine-grained MUSIC.
Therefore, when extracting a relation instance,the entity types of its two entities are important indicators for a specific relation.
Previous work usedconjunction features (Details in Section 3.3) by combining the coarse entity types of entity mentionswith its contextual lexical and syntactic features.
However, the conjunction features may fail to dis-tinguish the relations.
For example, the following two sentences contain two relation instances, one isDirectorOf(Ang Lee, Life of Pi), and the other is AuthorOf(George R.R.
Martin, A Song of Ice and Fire).1.
Ang Lee?s Life of Pi surprised many by scoring a leading four Oscars on Sunday night...2.
Westeros is the premiere fansite for George R.R.
Martin?s A Song of Ice and Fire.Only using the above conjunction features, we cannot tell the difference between the two entity pairs,and are probable to incorrectly classify them as the same relation.
By contrast, if we can assign eachentity with fine-grained entity types, for example, Ang Lee as the entity type ARTIST and George R.R.Martin as AUTHOR, we may succeed in classifying the two entity pairs correctly.To achieve the goal mentioned above, there are mainly three challenges: (1) how to define the fine-grained type set; (2) how to assign the types to entity mentions; (3) how to integrate the fine-grainedentity type constraints with the relation extracting model.
To address these challenges, in this paper,we propose a novel approach to explore the fine-grained entity type constraints for distantly supervisedrelation extraction.
First, we use the types defined in (Ling and Weld, 2012) stemmed from Freebase1as the fine-grained entity type set (introduced in Section 3.1).
Second, we leverage Web knowledgeto train a fine-grained entity type classifier and predict entity types for each entity mention.
Third, westudy several methods to integrate the type constraints with an existing system MULTIR, a multi-instancemulti-label model in (Hoffmann et al., 2011), to train the extractor.In summary, the contribution of this paper can be concluded as follows.
(a) We explore the effect of fine-grained entity type constraints on distantly supervised relation extrac-tion.
A novel method is proposed to leverage Web knowledge to automatically train a fine-grainedentity type classifier, which is used to predict the fine-grained types of each entity mention.
(b) We study a series of methods for integrating the fine-grained entity type constraints with the extract-ing model and compare their performance with different parameter settings.
(c) We conduct experiments to demonstrate the effects of the newly exploited fine-grained entity typeconstraints.
It shows that our method achieves a much better precision/recall curves over the base-line system in sentential extraction, and improves the performance with a smoother precision/recallcurve in aggregated extraction, which means a more stable model.2 Distant Supervision for Relation ExtractionWe define a relation instance (or a fact), which means a binary relation, as r(e1, e2).
r is the relation, ande1and e2mean the two entities in the relation instance, for example, BornIn(Y ao Ming, Shanghai).Distant supervision supplies a method to automatically generate training data.
In this part, we willintroduce the general steps in distant supervision for relation extraction.
First, we define the notationswe use.
?
denotes sentences comprising the corpus, E denotes entity mentions in the corpus which areconsecutive words with the same named entity tags assigned by an NER system, ?
denotes the facts (orrelation instances) in the existing knowledge base.
R denotes the relations in ?.1http://www.freebase.com/2108Figure 1: Fine-grained entity type set.Figure 2: Framework of fine-grained entity type classifier.To generate training data, we align pairs of entity mentions in the same sentence with ?.
The alignedentity mentions Etrainand their sentences ?trainalong with Rtrainare used as training data.
Featuresare extracted from them to train the relation extracting model.To predict the unknown data for extracting new relation instances, we input pairs of entity mentionsEpredictand the sentences containing them ?preidctinto the trained extracting model for extracting newrelation instances.3 Fine-grained Entity Type ConstraintsEntity mentions in sentences are considered consecutive words with the same entity types (Section 2).The entity types are part of the lexical and syntactic features(Mintz et al., 2009), and the feature settingis followed by other related work.
Their entity types are assigned by an NER system and consist offour categories (PERSON, ORGANIZATION, LOCATION, NONE).
The types of entity mentions ina relation are important indicators for the very type of relation.
However, the coarse (only four types)entity types may not capture sufficient constraints to distinguish a relation.
In this section, we explorefine-grained entity type constraints and study different methods to integrate them with the extractingmodel.This section first introduces the fine-grained entity type set(Section 3.1), and then describes our methodwhich leverages Web knowledge to train the fine grained entity type classifier and assign entity mentionswith the fine-grained entity types (Section 3.2).
At last, we illustrate methods to integrate fine-grainedentity type constraints with the relation extracting model.2109Entity pair [Hank Ratner], [Cablevision]SentenceCablevision?s $600 million offer came in the form of a letter to Peter S.Kalikow,chairman of the M.T.A., from the Garden?s vice chairman, Hank Ratner.Conjunction Reverse Left NE1 Middle NE2 RightFeature examplesFalse PER ORGFalse Hank[NMOD] PER [NMOD]chairman ... offer[SBJ] ORGTrue B -1 ORG POS $ ... NN NN, PER .B 1Table 1: Examples of conjunction features.3.1 Fine-grained Entity TypesFigure 1 is the type set we use.
It was introduced in (Ling and Weld, 2012) and was derived fromFreebase types.
The bold types in each small box of Figure 1 are upper-class types for others in thatsmall box.
For example, /actor is a lower-class type of /person which is denoted as /person/actor.And /person and /person/actor coexist in the type set.3.2 Fine-grained Entity Type ClassifierIn this section, we describe our method that leverages Web knowledge to train a fine-grained entity typeclassifier and predict entity types of each entity mention.
Its architecture is shown in Figure 2.3.2.1 TrainingThe training data are obtained from Wikipedia.
Because the defined fine-grained types are tailored basedon Freebase types, we can find the mappings between the two type sets, for example, /person/doctormaps to two Freebase types /medicine/physician and /medicine/surgeon.
And Freebase WEX2supplies a mapping between Freebase types to Wikipedia articles.
As a result, we can map Wikipediaarticles to defined fine-grained types.Based on the mappings, we obtain Wikipedia articles for each type as training data and negativetraining examples are sampled from articles not contained in the mappings.
We preprocess the articlesby: stop words filtering, stemming, and term frequency filtering and use a maxent model to train theclassifier.3.2.2 PredictingTo predict types of each entity mention, we first use search engines to expand entity mentions.
Specif-ically, each entity mention is used as a query sent to the search engine3.
Titles and descriptions of topk returned snippets are selected (We keep the top 20 in the experiments).
The obtained text are pre-processed with the same method as training examples.
Then we use the trained fine-grained entity typeclassifier to predict the types of each entity mention.After predicting, we obtain a ranked list of types for each entity mention, which are ranked by thepredicting scores.3.3 Integrating Fine-grained Entity Type Constraints into the Extracting ModelThis section introduces our methods to integrate the fine-grained entity type constraints with the ex-tracting model.
First of all, we briefly review the features used in previous models which derived from(Mintz et al., 2009) and (Riedel et al., 2010).
Their features mainly comprise two types: lexical features(POS tags, words and entity types) and syntactic features (dependency parsing tags, words and entitytypes).
Each feature is a conjunction with several parts: entity types of two entity mentions, the leftcontext window of the first entity mention, the right context window of the second entity mention andthe part between them (the window contains none or one or two words ).
Table 1 shows an example ofthe conjunction features.2http://wiki.freebase.com/wiki/WEX3We use Bing search API.
http://datamarket.azure.com/dataset/bing/search2110To integrate the exploited fine-grained entity type constraints with the extracting model, we proposedthree methods (substitution, augment and selection) to make the type constraints take effects.3.3.1 Substitution MethodIn this method, we substitute coarse entity types of the features with the entity mentions?
fine-grainedtypes, and use the new features to train the model.
Instead of substituting directly, an entity mentionis first represented by its fine-grained types and the upper-class of the fine-grained type, for example,/person/politician derives two types /person and /person/politician itself.
The reason is that theextracting model can benefit from the related types like the upper-class types.
And then we use theobtained entity types to substitute the old coarse entity types as new features greedily, which mean-s that all the possible combinations of types between the entity pair are considered.
For example,?Barack Obama?
has the fine-grained type /person/politician and his birth place ?Hawaii?
hasthe type /location/island, then there are 4 combinations between the two entities, they are (/person,/location), (/person, /location/island), (/person/politician, /location) and (/person/politician,/location).3.3.2 Augment MethodIn this method, we generate new features by substituting the coarse entity types with predicted fine-grained types, and expand the old features with new features.
Different from the substitution method, wedo not add the upper-class types, for that we think the coarse types in old features have the same effect.In this method, we use the fine-grained constraints as a complementary.3.3.3 Selection MethodThe selection method is similar to the augment method.
The difference is that we do not expand allold features with new features.
We select some of them to expand.
The reason is that some of theconjunction features are of high-precision themselves, it can clearly indicate the relations with its left,middle and right parts, even without the entity types (informative ones).
If we expand these features,it may cause more noisy features.
So we expect to only expand the ones that lack of the indicatingabilities (non-informative ones).
In this paper, we employ a simple method to distinguish between theinformative ones and non-informative ones by the length of the features, which means that the longer ismore informative than the shorter.
In our experiments, the length threshold is set as 20.In the predicting phase (Section 3.2), we obtain a ranked type list for each entity mention.
The top listtypes are considered in our methods.
Experiments in Section 4.3 are conducted on top k {k ?
1, 2, 3}type/types in the obtained ranked list.
And they are combined with a greedy method similar to that in thesubstitution method explained above.4 Experiments4.1 SettingsWe use the same data sets as (Riedel et al., 2010) and (Hoffmann et al., 2011), where NYTimes sentencesin the years 2005-2006 are used as training corpus ?trainfor distant supervision and sentences in 2007are used as testing corpus ?predict.
The data was first tagged with an NER system (Finkel et al., 2005)and consecutive words with the same tag are extracted as entity mentions.
And then, entity mentionsEtrainin training corpus are aligned to facts ?
in Freebase as training examples to train the models.We integrate our fine-grained entity type constraint with MULTIR, an existing multi-instance multi-label extracting model in (Hoffmann et al., 2011).
Following their setttings, we conduct experiments onaggregated extraction and sentential extraction to show the effect of fine-grained entity type constraints.?
Aggregated extraction: Aggregated extraction is corpus-level extraction.
When given an entitypair, it predicts its relation types based on the whole corpus.
After extraction, the precision andrecall are computed by comparing the results with facts in Freebase.
The evaluation underestimatesthe accuracy because there may be correct facts in the extracted results but not existing in Freebase,these facts are labeled as incorrect by mistake here.
Because aggregated extraction is an automaticevaluation, it is used to tune parameters like held-out evaluation in (Mintz et al., 2009).2111(a) PR curves of the substitution method (b) PR curves of the augment method(c) PR curves of the selection method (d) Comparison with other methodsFigure 3: Precision-recall (PR) curves of the aggregated extraction.?
Sentential extraction: Sentential extraction predicts an entity pair only based on a specified sen-tence containing the pair of entities.
We use manually labeled data in (Hoffmann et al., 2011) asbenchmark.
The data consist of 1,000 sentences and are sampled from the results their system out-puts and sentences aligned with facts in Freebase.
As they stated in their paper, these results providea good approximation to the true precision but can overestimate the actual recall.4.2 Experimental ResultsIn aggregated extraction, we first evaluate the three type-constraint integration methods (substitution,augment and selection) with the top k {k ?
1, 2, 3} type/types (Section 3.3).
And then, we compare thebest parameter setting methods with previous work.
In sentential extraction, we compare methods tunedin aggregated extraction with MULTIR.4.2.1 Aggregated ExtractionFigure 3 shows the precision-recall (PR) curves of the aggregated extraction.
In it, Sub topk {k ?1, 2, 3} means using the substitution method (Section 3.3) with top k fine-grained entities types re-turned by the type classifier in Section 3.2.
Correspondingly, Aug topk is for the augment methodand Select topk is for the selection method.Figure 3(a) shows that Sub top3 outperforms the other two settings of k in the substitution method,it seems that more fine-grained types produce better curves.
In Figure 3(b), Aug top1 and Aug top2achieve similar performances.
However, when adding one more type with k = 3, we obtain a lowercurve, which contradicts the trend showed in the curves of the substitution method (Figure 3(a)).
Fig-ure 3(c) shows the PR curves of three selection methods, Select top1 has a better performance at thebeginning.
Then Select top2 exceeds it a bit consistently.In Figure 3(d), we demonstrate the comparison of best tuned methods above with previous work.They are Sub top3, Aug top1 and Select top2.
From Figure 3(d), it shows that, among the three ofour methods, Aug top1 achieves better precisions along the PR curves, and Select top2 reaches the best2112Figure 4: Comparison with MULTIRrecall at the highest recall point.
Comparing to other methods, the PR curve of Aug top1 reaches a higherrecall with 29.3% at the highest recall point than MULTIR (24.5%).
Select top2 achieves 29.3% at thehighest recall point, best among all methods.
And by integrating the fine-grained entity type constraints,they improve the PR curve of MULTIR with a more smoother curve without most of the depressions seenin MULTIR.
As stated in (Hoffmann et al., 2011), the smoother curve indicated a more stable model.4.2.2 Sentential ExtractionFigure 4 shows the precision-recall (PR) curves of the sentential extraction.
In the evaluation, we com-pare the three best integration methods tuned in aggregated extraction with original MULTIR.
Among ourthree method, Aug top1 outperforms in precision and achieves a better curve in general among the threemethods, however, Select top2 gains a better recall at the end.
Sub top3 has the worst recall.
In gen-eral, our methods have much better precisions than MULTIR.
Aug top1 and Select top2 achieve bettercurves than MULTIR.
Since the evaluation of sentential extraction is a good approximation of precision,it implies that the proposed methods are effective.4.2.3 AnalysisOn one hand, among the three proposed integration methods, generally, the augment method and selec-tion method get better performance.
The reason is that substitution method uses predicted fine-grainedentity types to replace the old coarse features in the conjunction features completely, and the conjunctionfeatures are sensitive to entity types for different entity types indicate different conjunction features, asa result, if we can not promise a good accuracy in the type classification which is hard to achieve inclassifying hundreds of fine-grained types, the performance will be badly influenced.
Different from thesubstitution method, augment method and selection method keep the old features with coarse features,they use the features with fine-grained entity type constraints as extra information to help the extractionand achieve better results.On the other hand, comparing to other methods, by integration the exploited fine-grained entity typeconstraints, our methods achieve improvements in both aggregated and sentential extraction.
It provesthat the fine-grained entity type constraints we exploit are effective, and our proposed integration meth-ods succeed in integrating the constraints into the extracting model.
Our augment method outperformsMULTIR in precision along the PR curves in sentential extraction and improve it performance with amore smoother PR curve in aggregated extraction, which indicates a more stable model.
Moreover, themethod gets a better recall.
And our selection method consistently outperforms MULTIR in sentential2113k=1 k=2 k=3Recall@k 0.596 0.740 0.806Table 2: Evaluation of the fine-grained type classifier.extraction.
In aggregated extraction, it also achieves a smoother curve and an impressive promotion atthe highest recall point.
Since the evaluation of aggregated extraction only considers the facts existingin Freebase which may incorrectly label the right extracting results and underestimate the true precision,and based on its better performance of precision in sentential extraction, we consider it is a more promis-ing method.
This paper only employs very naive method to select the non-informative features by itslength (Section 3.3.3), a more effective selecting method may lead further improvements.4.3 Performance of Entity Type ClassifierWe evaluate the performance of the fine-grained entity type classifier (Section 3.2).
In section 3.2, wesample the training examples from a collection of Wikipeida articles mapped with the fine-grained types.To generate test entity mentions, we first remove the sampled training articles from the collection, andthen sample the articles from it, where the titles of sampled articles are used as the test entity mentions(we sample 12,000 test entity mentions) and their mapped fine-grained types are used as benchmark.After that, the predicting method in Section 3.2.2 is used to expand mentions and predict the types ofeach test entity mention.
After predicting, we obtain a ranked list of types for each test entity mention.To evaluate, we define a notation of Hit@k, which equals 1 if the true type of an entity mention ishit in the top k predicted types, otherwise equals 0.
And then we evaluate it by the Recall@k definedbellow.Recall@k =?12000i=1Hit@ki12000(1)In equation (1), i means the ith test entity mention.
Table 2 shows the results for the top 3 predictedtypes.5 Related WorkDistant supervision (also known as weak supervision or self supervision) is used to a broad class of meth-ods in information extraction which aims to automatically generate labeled data by aligning with datain knowledge bases.
It is introduced by Craven and Kumlien (Craven et al., 1999) who used the YeastProtein Database to generate labeled data and trained a naive-Bayes extractor.
Bellare and McCallum(Bellare and McCallum, 2007) used BibTex records as the source of distant supervision.
The KYLINsystem in (Wu and Weld, 2007) used article titles and infoboxes of Wikipedia to label sentences andtrained a CRF extractor aiming to generate infoboxes automatically.
The Open IE systems TEXTRUN-NER (Yates et al., 2007) and WOE (Wu and Weld, 2010) trained their extractors with the automaticlabeled data from Penn Treebank and Wikipedia infoboxes respectively.Mintz (Mintz et al., 2009) first introduced their work that performed distant supervision for relationextraction.
It used Freebase as the knowledge base to align sentences in Wikipedia as training data andtrained a logistic regression classifier to extract relations between entities.Distant supervision supplied amethod to generate training data automatically, however it also bring the problem of noisy labeling.
Aftertheir work, a variety of methods focused to solve this problem.
Riedel (Riedel et al., 2010) proposed amulti-instance model to model the false positive noise in training data with the assumption that at leastone of the labeled sentences truly expressed their relation.
After their work, Hoffmann (Hoffmann etal., 2011) and Surdeanu (Surdeanu et al., 2012) tried to not only model the noisy training data, but alsoovercame the problem of multi-label where two entities may exist more than one relation, they proposedgraphic models as kinds of multi-instance multi-label learning methods and made improvements overprevious work.
The at-least-one assumption would fail when encountering entity pairs with only onealigned sentence.
Takamatsu (Takamatsu et al., 2012) employed an alternative approach without thementioned assumptions.
Their work predicted negative patterns using a generative model and removelabeled data containing negative patterns to reducing noise in labeled data.2114Besides the problem of false positive training examples caused by distant supervision.
There were abunch of researches trying to solve the problem of false negative training examples caused by incompleteknowledge bases.
Zhang (Zhang et al., 2013) made heuristic rules to filter the false negative trainingexamples.
And Xu (Xu et al., 2013) tried to overcom this problem by pseudo-relevance feedback.
Min(Min et al., 2013) improved MIML in (Surdeanu et al., 2012) by adding a new layer in their 3-layergraphic model to model the incomplete knowledge base.
Ritter (Ritter and Etzioni, 2013) employedsimilar intuition with (Xu et al., 2013) that they thought rear entities missing in the database wouldbe often mentioned in the text.
They proposed a latent-variable approach to model it and showed itsimprovement over aggregate and sentential extraction.6 ConclusionIn this paper, we propose a novel approach to explore the fine-grained entity type constraints for distantlysupervised relation extraction.
We leverage Web knowledge to automatically train a fine-grained entitytype classifier and predict entity types of each entity mention.
And we study a series of methods to inte-grate the type constraints with a relation extraction model.
At last, thorough experiments are conducted.The experimental results imply our methods are effective with better precision/recall curves in senten-tial extraction and smoother precision/recall curves in aggregated extraction, which indicate more stablemodels.In the future we hope to explore more details of integration methods that integrates fine-grained entitytype constraints with relation extraction models, especially the selection integration method.
We considerthat a more effective method to distinguish between the informative and non-informative features willlead more improvements.AcknowledgementsThis work was sponsored by the National Basic Research Program of China (No.
2014CB340503) andthe National Natural Science Foundation of China (No.
61202329).
This work was supported in part byNoahs Ark Lab of Huawei Tech.
Co. Ltd.ReferencesKedar Bellare and Andrew McCallum.
2007.
Learning extractors from unlabeled text using relevant databases.
InSixth International Workshop on Information Integration on the Web.Mark Craven, Johan Kumlien, et al.
1999.
Constructing biological knowledge bases by extracting informationfrom text sources.
In Proceedings of the Seventh International Conference on Intelligent Systems for MolecularBiology, pages 77?86.
Heidelberg, Germany.Jenny Rose Finkel, Trond Grenager, and Christopher Manning.
2005.
Incorporating non-local information intoinformation extraction systems by gibbs sampling.
In Proceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 363?370.
Association for Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld.
2011.
Knowledge-basedweak supervision for information extraction of overlapping relations.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics: Human Language Technologies, volume 1, pages 541?550.Xiao Ling and DS Weld.
2012.
Fine-Grained Entity Recognition.
In AAAI.Bonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek.
2013.
Distant supervision for relationextraction with an incomplete knowledge base.
In Proceedings of NAACL-HLT, pages 777?782.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009.
Distant supervision for relation extraction withoutlabeled data.
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2, pages1003?1011.
Association for Computational Linguistics.2115Deepak Ravichandran and Eduard Hovy.
2002.
Learning surface text patterns for a question answering system.
InProceedings of the 40th Annual Meeting on Association for Computational Linguistics, pages 41?47.
Associa-tion for Computational Linguistics.Sebastian Riedel, Limin Yao, and Andrew McCallum.
2010.
Modeling relations and their mentions withoutlabeled text.
In Machine Learning and Knowledge Discovery in Databases, pages 148?163.
Springer.Alan Ritter and Oren Etzioni.
2013.
Modeling Missing Data in Distant Supervision for Information Extraction.Transactions of the Association for Computational Linguistics, 1:367?378.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati, and Christopher D Manning.
2012.
Multi-instance multi-label learning for relation extraction.
In Proceedings of the 2012 Joint Conference on Empirical Methods inNatural Language Processing and Computational Natural Language Learning, pages 455?465.
Association forComputational Linguistics.Idan Szpektor, Hristo Tanev, Ido Dagan, Bonaventura Coppola, et al.
2005.
Scaling Web-based aquisition ofentailment relations.
Ph.D. thesis, Tel Aviv University.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.
2012.
Reducing wrong labels in distant supervision forrelation extraction.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:Long Papers-Volume 1, pages 721?729.
Association for Computational Linguistics.Fei Wu and Daniel S Weld.
2007.
Autonomously semantifying wikipedia.
In Proceedings of the sixteenth ACMconference on Conference on information and knowledge management, pages 41?50.
ACM.Fei Wu and Daniel S Weld.
2010.
Open information extraction using wikipedia.
In Proceedings of the 48th An-nual Meeting of the Association for Computational Linguistics, pages 118?127.
Association for ComputationalLinguistics.W Xu, RH Le Zhao, and R Grishman.
2013.
Filling Knowledge Base Gaps for Distant Supervision of RelationExtraction.
Proceedings of Association for Computational Linguistics.Alexander Yates, Michael Cafarella, Michele Banko, Oren Etzioni, Matthew Broadhead, and Stephen Soderland.2007.
Textrunner: open information extraction on the web.
In Proceedings of Human Language Technolo-gies: The Annual Conference of the North American Chapter of the Association for Computational Linguistics:Demonstrations, pages 25?26.
Association for Computational Linguistics.Xingxing Zhang, jianwen Zhang, Junyu Zeng, Jun Yan, Zheng Chen, and Zuifang Sui.
2013.
Towards AccurateDistant Supervision for Relational Facts Extraction.
In Proceedings of Association for Computational Linguis-tics.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005.
Exploring various knowledge in relation extraction.In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pages 427?434.Association for Computational Linguistics.GuoDong Zhou, Min Zhang, Dong Hong Ji, and Qiaoming Zhu.
2007.
Tree kernel-based relation extraction withcontext-sensitive structured parse tree information.
In Proceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Computational Natural Language Learning.2116
