TOWARDS USING PROSODY IN SPEECH RECOGNITION/UNDERSTANDINGSYSTEMS: DIFFERENCES BETWEEN READ AND SPONTANEOUS SPEECHKim E.A.
Silverman, Eleonora Blaauw l, Judith Spitz, John F PitrelliSpeech Technology GroupArtificial Intelligence LaboratoryNYNEX Science and TechnologyWhite Plains, NY, 10604, U.S.A.1.
Ph.D. student in Institute for Language and Speech, University of Utrecht1.
ABSTRACTA persistent problem for keyword-driven speech recognition sys-tems is that users often embed the to-be-recognized words orphrases in longer utterances.
The recognizer needs to locate therelevant sections of the speech signal and ignore extraneouswords.
Prosody might provide an extra source of information tohelp locate target words embedded inother speech.
In this paperwe examine some prosodic haracteristics of 160 such utterancesand compare matched read and spontaneous versions.
Half of theutterances are from a corpus of spontaneous answers to requestsfor the name of a city, recorded from calls to Directory AssistanceOperators.
The other half are the same word strings read by volun-teers attempting tomodel the real dialogue.
Results how a consis-tent pattern across both sets of data: embedded city names almostalways bear nuclear pitch accents and are in their own intonationalphrases.
However the distributions of tonal make-up of these pro-sodic features differ markedly in read versus pontaneous speech,implying that if algorithms that exploit hese prosodic regularitiesare trained on read speech, then the probabilities are likely to beincorrect models of real user speech.2.
INTRODUCTIONThis work addresses two related questions.
One is whetherspontaneous goal-directed utterances collected from realusers in a particular application domain exhibit reliable pro-sodic patterns that could be exploited by recognition algo-rithms.
We focus on to-be-recognized words that arespoken within longer utterances, in order to investigatewhether these embedded words have particular prosodiccharacteristics that could help a recognizer to locate them.One of the original motivations for this study was ourobservation from informal istening to our corpus that suchembedded words bear nuclear pitch accents.
If this is a con-sistent pattern, it would mean that they are (1) louder,longer and more clearly articulated than they would bewithout nuclear accents, and (2) they would bear character-istic fundamental frequency movements.Corpora of spontaneous goal-directed speech from realusers are not readily obtainable, and so it is common prac-tice to record speech read out by volunteers in order todevelop, train and test recognition algorithms.
To the extentthat the prosody of read speech differs from that of sponta-neous goal-directed speech, such "laboratory" corpora mayobscure or misrepresent any reliable prosodic propertiesfound in spontaneous "real user" speech.
Consequently thesecond question investigated in this work is whether suchpatterns can also be found in recordings of speech reead outby volunteers.
We are interested in prosodic differencesbetween read and spontaneous speech because of their rele-vance to speech recognition, and for increasing the natural-ness of synthetic speech.It is worth pointing out a methodological issue at this stage:the prosody used when people are reading can of coursediffer dramatically from that used in spontaneous commu-niccation.
Speech databases that we know of vary widely inhow much effort was taken to ensure that the prosody ofthe speech realistically reflects the speech that recognizershave to deal with in real-world applications.
In this experi-ment we chose to do everything we could to encourage ourvolunteers to use realistic spontaneous-sounding prosody.Most existing speech corpora used in the recognition fieldhave been collected with less emphasis on realistic pros-ody.
We therefore believe that the read speech in this exper-iment is as similar as possible to spontaneously producedutterances.
The degree of prosodic similarity that we reportbetween read and spontaneous speech represent a "bestcase".3.
DISCOURSE DOMAINOur particular application is automatic recognition of thename of a city in telephone calls to a Directory AssistanceOperator.
A corpus of 26,946 recordings of real users hasbeen collected and was reported on in detail in a previousDARPA meeting\[l\].
In each case a caller was played anautomated request for the name of a city.
In 37% of thoseutterances that do contain a city name, that city name isembedded in a longer string of connected speech.
In somecases there is relatively little extraneous material ("In Bos-ton, please"), but often (55% in this corpus) there is con-siderably more ("Yes Ma' m Central Auto Service in435Stoughton,  p lease."
) .
We refer to these as "complex embed-ded" utterances.
In all of these mbedded utterances speak-ers have considerably more options concerning how theysay the speech than when they say only an isolated cityname, and so we would expect prosody to contribute signif-icantly to the variability in the signal.
The current studydeals with the complex embedded utterances, because (i)these represent the more serous challenge for speech rec-ognition, and (ii) these contain richer prosodic variationthat is more representative of spontaneous speech in otherdiscourse domains.Although the current study focuses on a telephone networkapplication, we believe that he results have general appli-cability to the behavior of users of spoken language systems(SLS).
Often users will answer arequest for a single item ofinformation with a sentence containing not only therequested item, but also (1) extraneous material, and (2)answers to anticipated subsequent requests in the discourse.For example, in an ATIS-like domain, auser may answer arequest for a destination with "I'd like to arrive in Bostonon Tuesday morning before 9:30 am", bundling the arrivalday and arrival time into the same utterance.
Prosody canmark all of the discourse-relevant information-bearingwords in such an answer, and so could help a SLS to avoidreprompting for material that has already been said.For this investigation, we selected 80 of the spontaneouscomplex embedded utterances that reflected the variation inlength and structure of the larger set.
Each utterance wasspoken by a different speaker, half of the speakers weremale and half were female.
The shortest utterance was twowords ("Arlington, McCar thy" )  and the longest was twentywords ( "Have  you  a l is t ing in Jamaica  P la in  fo r  Rober tSche inkopf  - S - C - H - E - I - N - K - 0 - P - F" ) .
Half ofthe 80 utterances were in "telegram" style - bearing few orno function words ( "Boston  Woo lwor th ' s  on Wash ingtonSt reet" ) ,  and the set was chosen to reflect variation inwhether the target city name was a first, medial, or last con-tent word in the utterance.We then collected amatched corpus in which volunteerscalled an automated recording facility in our laboratory andread out orthographic transcriptions of the same utterances.Participants knew that hese texts were originally spoken bypeople calling Directory Assistance asking for informationabout a telephone number, and were encouraged torehearsethe items several times before calling with this in mind.Participants confirmed uring subsequent debriefing thatthey had tried to make their utterances sound realisticallynatural, acting out the situation of making a telephone callto get information.
Each participant read a list of 25 sen-tences: the first and last two were fillers, and one of themiddle sentences was the utterance r levant to the currentstudy.
This "read speech" corpus consists of 2000 utter-ances altogether, collected from 80 volunteers, of whichone utterance per reader is used in this investigation.
Thereader of each utterance was of the same sex as the speakerof the spontaneous version.4.
PROSODIC  ANALYSIS  METHODSWe use "prosody" to refer to the acoustic/phonetic bracket-ing structure, locations of boundaries, and the choice anddistribution of tonal features.
This suprasegmental organi-zation has been shown to affect not only duration and fun-damental frequency but also such phenomena as co-articulation, devoicing, laryngealization, and allophonicvariation.
Thus it is a potential information source for fac-toring out variability in acoustic-phonetic models, locatingword boundaries, disambiguating alternative parses or inter-pretations, and locating embedded keywords.
In this studywe focus primarily on the last of these.The prosody in the read and spontaneous versions of theutterances was manually transcribed by two people viainteractive listening and graphical displays of the speechwaveform, total signal energy, and extracted fundamentalfrequency contour.
This signal-processing and display wasperformed with version 2.0 of the WAVES+ software pack-age\[4\].
Each transcriber labelled an overlapping subset ofthe utterances, enabling us to compare their transcriptionsfor almost half of the corpus.
In addition, a number of directmeasurements were taken from the acoustic signals.4.1.
Prosodic Transcription SchemeThe utterances were transcribed using the draft prosodictranscription conventions developed at the First ProsodicTranscription Workshop hosted by the Spoken LanguageSystems Group in MIT in August 1991.
Briefly, this is a setof labels for the tonal (pitch) structure and boundary struc-ture of spoken American English.
The tonal labels are asubset of Pierrehumbert's model\[2\]: this approach viewspitch contours as composed of a sparse linear sequence ofaccents and tones elected from a relatively small inventory.In the draft scheme used below, the inventory of pitchaccents is reduced to H*, L*, L+H*, L*+H, and the down-step feature is marked explicitly (e.g.
H* versus :H*).
Lackof confidence is marked by affixing a "?"
after the symbol.Boundaries are a subset of the break indices used by Price etal \[3\].
The labelling process consisted of locating and iden-tifying the pitch accents, phrase accents, and boundarytones, and assigning astrength to each inter-word boundary(from 0 => cliticized word; to 4 => full intonational phraseboundary).
A transcriber can affix a + or - to indicate uncer-tainty about he strength of a boundary.One of the transcribers eceived one day's training before-hand, and supplemented that by reading portions of Pierre-humbert \[2\] and Silverman \[5\].
The other transcriberreceived about a half day's training.
Both transcriberswould occasionally consult with the first author concerningparticularly unclear phenomena.4365.
RESULTS5.1.
Reliability Across TranscribersOne of the stated aims of the First Prosodic TranscriptionWorkshop was that the transcription conventions should beeasily taught, and that different transcribers should agree atleast 80% of the time.We could test this in this experiment, because 36 of thespontaneous and read versions (i.e.
72 utterances in all)were labelled by both transcribers.
Because transcriptionsare linear strings of symbols, one way to calculate agree-ment between 2 transcribers is:Matches Agreement = I00( )Matches + Insertions + Substitutionswhere:Matches = number of symbols in the string where the tran-scribers agree concerning location and the symbol itself 1,Insertions = number of symbols marked by one transcriberonly (an omission by either transcriber is equivalent to aninsertion by the other), and Substitutions = number of loca-tions where each transcriber used a different symbol.Table 1 shows the agreement separately calculated for thetonal and boundary transcriptions under two criteria.
Over-all the agreement is quite satisfactory.
Exact match meansboth transcribers had to use exactly the same symbols in thesame locations to score a match.
Near match slightlyrelaxes the criteria for matching in the following ways:Near tonal match: (1) phrase-initial H* matches phrase-initial L+H*; (2) a H* or L+H* match the correspondingdownstepped variants of themselves (!H* and !L+H*,respectively), (3) an accent matches its uncertain variant(e.g.
H* matches H*?
)Near boundary match: (1) a 0 (= clificized word) matchesa 1 (= normal phrase-medial interword boundary), (2) a 1matches a 2 (= separation between words, but with no tonalcorrelates of a boundary)If agreement includes near-matches, then we have clearlymet the reliability criteria.
If not, then we still have met it inthe tonal transcriptions, but not in the boundary transcrip-tions.
Most of the disagreements concerned whether someword boundaries were cliticized (e.g.
between the first andsecond words in "Could I have the..." versus "C' d I havethe...").
In the subsequent preliminary analyses of the tonal1.
Final boundaries at the fight-hand edge of utterancesare excluded from this analysis because they would arti-ficially inflate the agreement scores: both transcribersagreed 100% that all utterances nded with a 4 bound-ary.transcriptions, we used the more experienced transcriber'sdecisions in cases where there is disagreement.Exact Match Near MatchTonal 81% 92%StructureBoundary 68% 94%StructureTable 1: Percent agreement between Iranscfibers ontonal and boundary structure.5.2.
Comparison of Read and SpontaneousVersions: IntonationOur initial informal impression which motivated this studywas borne out by the transcriptions: in both corpora theembedded city names usually bear a nuclear accent (94% ofspontaneous, 97% of read utterances, no significant differ-ences), and are set off in an intonational phrase of their own.Moreover the tonal combinations carried by these citynames represent only a relatively small subset of the possi-ble combinations that can occur in spoken English.
Withinthe transcription framework used in this study, there are 16different possible combinations of pitch accent, phraseaccent and boundary tone (the three tonal elements of a cityname in most of our corpus).
However, the only five thatactually occurred on the city names were:Pitch accent Phrase accentFinalboundarytone aH* L L\]L+H* L L\]L* H H\]H* H H\]L+H* H H\]a.
To avoid confusion with (i) the results expressed below aspercentages, and (ii) initial boundary tones in Pierrehumbert\[2\], we follow the convention i  Silverman \[5\] of using "\]"instead of "%" for final boundary tones.The first two of these tunes are falls, the last three are typesof rises.
These same five tunes occurred in both the read andthe spontaneous corpora.
We interpret this as another simi-larity between the read and spontaneous corpora: the read-ers not only succeeded in putting nuclear accents on the citynames, but also chose from the same inventory that is usedin spontaneous interactions in this domain.
However437although the embedded city names were almost all nuclearin the read and spontaneous tterances, the distributions ofthe five tunes across the corpora were not at all the same.
Itis commonplace in the literature to categorize nuclear pitchvery grossly movements into rising (or high level) versusfalling.
This corresponds in our case largely to the phraseaccent being H or L (this would not be the case if there hadbeen any L phrase accents followed by HI boundary tones).In Table 2 we compare the read and spontaneous versionsof 791 of the pairs of city names according to this grossdivision.
For the few city names that bore prenuclear H*accents, we categorized them as falling if the next accentwas either a L* or downstepped, else as rising.SpontaneousversionRisingFallingRead versionRising Falling27% 47%8% 19%74%27%Table 2: Agreement between spontaneous and readversions of each city name.
All percentages are out of the79 pairs included in this analysisOne common view of prosody is that it is determined bysyntax, that there is a default prosody for any given sen-tence which is derivable from the word string itself.
If thisis true, or if the way city names are read out resembles howthey are spoken spontaneously, then the data should be con-centrated in the upper left and lower right cells of Table 2.In fact, less than half of the data (46%) lies in these twocells.
The main reason is that 47% of the city names werespoken with a rising intonation i  spontaneous versions, butwith falling intonation i  the read versions.This shift from rising to falling intonation is also reflectedin the marginal totals: 73% of the spontaneous city nameshad rises, but only 34% of their read counterparts did.
Thedata argue that prosody is not directly derivable from theword string itself.
Two possible reasons for this differenceare:?
A rising intonation isa marker of politeness in thisparticular dialogue context.
When volunteers partici-pate in a recording session, even when they attemptto act out the real dialogue, they do not feel com-pelled to be polite to the recording equipment.?
In the real interaction with a telephone operator thespeaker uses rising intonation to seek confirmationthat the operator has indeed understood the city1.
One of the read versions was truncated insuch a waythat it had to be left out of this analysisname.
Speakers may not be consciously aware thatthey do this, and so fail to replicate it when attempt-ing to emulate the interactionThe preponderance of falls in read speech, when comparedto the preponderance of rises in spontaneous speech, has anumber of implications for speech technology.
One of theseconcerns the acoustic models in a recognizer: low finalboundary tones (which are located at the right hand edge ofmost of the falling nuclear accents, and therefore are com-mon in read speech) tend to be associated with laryngealiza-tion and devoicing of the segmental material.
Consequentlythese spectral effects will be built into acoustic models thatare trained on read speech, but will be the exception ratherthan the rule in the spontaneous speech that a recognizerwill ultimately have to process.These rising-versus-falling differences also bear on a poten-tial use of prosody for speech understanding systems: in asystem where the user can both ask questions and alsodeliver answers to questions asked by the system itself, thenatural language processing part of the discourse managercould be helped if it could use prosody to distinguishbetween these two different speech acts.
Suggestions havebeen made that questions usually have high or rising intona-tion, whereas information-delivering statements usuallyhave falling intonation.
The current results indicate that atleast in the application domain used in this experiment, thisdistinction is more complicated.
Users are delivenng infor-mation in response to a question from the system, ratherthan asking the system adirect question themselves, and yetthey use rising tunes more often than falling.The tonal differences between the corpora were notrestricted to the phrase accents alone.
In read speech, 81%of the city names carried a H*, whereas in spontaneousspeech this was only 52%, with the remaining cases bearingeither a L?H* (35%) or a L* (8%).
The majority of the citynames were final in their intonational phrase, and thereforecontained an additional boundary tone on the right.
In thespontaneous corpus, 76% of these were H\] and 12% wereL\].
Once again, this order was reversed in the read corpus(28% were HI and 72% L\]).5.3.
Comparison of Read and SpontaneousVersions: PausesThe characteristics and distribution of pauses in these utter-ances also showed reliable patterns and important differ-ences between read and spontaneous speech.
The followingsummary is based on all pauses in the utterances, not justthose around city names.
Some pauses occurred at "gram-matical" positions, as in:"In Boston <...> may I have the number of...""...the number John Smith <...> in Boston",others at "ungrammatical" positions:438"yes the number of <...> John Smith in <...> Boston".This classification of pause types is common in the litera-ture.
While it seems to have intuitive appeal, we believethat it may be more of a continuum than a clear categorydistinction.
Ungrammatical pauses may be reinterpreted asmerely being located at more embedded levels of bracket-ing in a syntactic structure than grammatical pauses.
Atleast in some eases the labelling of a pause as grammaticalor ungrammatical may be a consequence of the researcher'spreferred syntactic theory.
In the current study, 91% of theungrammatical pauses were located after the prepositionwithin a prepositional phrase.Like O'Shaughnessy \[6\], we found that while some pausesare located at grammatical boundaries, others are not.
Butthe ratio distinguished between the two speech modes: 45%of all pauses were "ungrammatical" in the spontaneousspeech, but only 11% in the read speech.
Unlike O'Shaugh-nessy, we found that in both corpora ungrammatical pauseswere longer than grammatical ones.
Silent pauses in gram-matical locations were twice as long in spontaneous speech(mean 0.45 seconds, standard eviation 0.29) as in readspeech (mean 0.21 seconds, standard eviation 0.15).
In theread corpus there was less variability in pause duration(mean 0.23 seconds, standard eviation 0.17) than in thespontaneous speech (mean 0.45 seconds, tandard eviation0.29).
85% of the filled pauses were located at ungrammati-cal positions One striking difference between the corporawas that in the read versions there were no filled pauses atall.
Moreover, in only 18% of the read utterances did thereaders place pauses in the same places as they occurred inthe spontaneous versions.
All of these were grammaticalboundaries ("Cambridge <silence> I'm looking for PizzaRing") which also carried full intonational phrase bound-aries.
All other differences consisted of either omittingungrammatical pauses or inserting rammatical ones wherethe original speakers did not.We believe that in the spontaneous speech the ungrammati-cal pauses, and perhaps also some of the grammatical ones,reflect he speakers' lexical access delay and mark for thelisteners that the post-pausal words are not easily predict-able (i.e.
information-rich) and therefore "worth waitingfor".
In read speech there is no comparable xical accessbecause all the words are already laid out on the printedorthography, and consequently this component of the infor-mation structure is not marked in the readers' utterances.5.4.
P rosod ic  Character i s t i cs  o f  Other  WordsAlthough we do not yet have quantitative analysis pecificto non-target speech, we do notice two consistent prosodicpatterns in the remaining parts of the utterances outside ofthe city names.The first pattern is that content words that are not directlyconveying discourse-relevant i formation either bear noaccent at all, or at their most salient bear only pre-nuclearaccents and are not set off in phrases by themselves.
Exam-pies include the parenthesized words in:"Could I (please) have the (number)for Watertown Police""Cambridge I'm (looking)for Pizza Ring""(I'm) (trying) to (find) the (exchange)for Cape Cod".The second pattern returns us to the issue raised earlier inthis paper that users often anticipate what questions will beasked subsequently in the dialogue.
In the Directory Assis-tance domain subsequent questions will be for the name,and if that is likely to be ambiguous then there will be arequest for further disambiguating information.
The consis-tent behavior of users in our corpus is to mark this informa-tion in a similar way to how they mark the city name.Examples include:"Quincy the Imperial Gardens on Sea Street""Yes I'd like the number of the Langley Deli in Newtonplease.
""Uh this is Quincy I'd like the number of the Quincy Police,not the emergency number of course.
"One similarity is that these items tend to bear nuclearaccents.
But they differ in that these accents are oftennuclear in an intermediate phrase, rather than a full intona-tional phrase.
Thus they do not have the extra boundarytone, they exhibit less phrase-final lengthening, and are lesslikely to be followed by pauses.Another common prosodic pattern arises when these exlxacompound nouns consist of more than one word, as illus-trated in the above examples.
Typically each word will beara pitch accent, but they will not all be of the same type.
Thefirst accent is usually a L+H*, whereas ubsequent ones aresimple H*.
That causes fundamental frequency to start lowand rise to the first noun, and then stay high until the lastone.
Thereafter it moves into the phrase accent and isaccompanied by lengthening of the material.
In those caseswhere that phrase accent is L, then this contour appears tobe an instance from American English of the "hat pattern"that has been described for British English and for Dutch\[7\].
Often in our spoken corpus the phrase accent is H. Butin both cases, these patterns combine to somewhat set offthe whole compound as a separate unit, in a way that couldbe exploited by a recognizer.6.
CONCLUSIONRead speech differs from spontaneous speech in someimportant ways: (i) although the tunes on focussed wordsare selected from the same inventory in both read and spon-taneeous speech, the prior probabilities of the tunes differgreatly -- spontaneous speech predominantly contains rises,read speech predominantly contains falls, (ii) pauses in readspeech are shorter than in spontaneous speech, and they pre-439dominantly are located at structurally predictable positions(grammatical boundaries), whereas in spontaneous speechthis generalization hardly holds true at all, (iii) read speechtends to not contain filled pauses.
These differences arguethat algorithms which are developed to exploit his informa-tion will need to be developed and trained on the basis ofspontaneous speech from real users, rather than just fromread speech.These results are encouraging for locating embedded tar-gets in speech recognition tasks: they show that when usersrespond to a query from an automated system, they markthe embedded information-bearing words with an acousti-cally-salient uclear pitch accent and often precede and/orfollow them by a pause.For speech synthesis n the context of spoken language sys-tems, these results uggest that listeners will better be ableto understand and interpret synthesized utterances if thefocussed information that they contain is (i) bears a nucleartune, and (ii) is preceded by some lengthening of the imme-diately-preceding material and perhaps even the insertion ofa short pause.
Further investigations will address predictionof the tonal makeup of these patterns.AcknowledgmentsSheri Walzman learned prosodic transcription and laboredlong doing careful abelling.
Lisa Russell developed theautomated recording facility, helped find suitable volun-teers, and imposed organization and order on the data col-lection effort.
Without he help of these two people thiswork would never have seen the light of day.
Any abuses oftheir work nevertheless remain our own responsibility.REFERENCESSpitz, J. and the Artificial Intelligence Speech TechnologyGroup.
Collection and Analysis of Data from Real Users:Implications for Speech Recognition/Understanding Sys-tems.
Proceedings of the Fourth DARPA Speech and Natu-ral Language Workshop, 1991.2.
Pierrehumbert, J.
B., The Phonology and Phonetics ofEnglish Intonation.
Ph.D. Dissertation, M1T 1980 (Distrib-uted by Indiana University Linguistics Club, 1987).3.
Price, P., Ostendorf, M., Shattuck-Hufnagel, S., and Fong,C.
The Use of Prosody in Syntactic Disambiguation.
Jour-nal of the Acoustical Society of America, 90, 6, pp 2956-2970, 1991.4.
Talkin.
D. Looking at Speech.
Speech Technology, 4 4,1989.5.
Silverrnan, K. E. A., The Structure and Processing of Fun-damental Frequency Contours.
Ph.D. Dissertation, Cam-bridge University, 19876.
O'Shaughnessy, D. Labelling Hesitation Phenomena inSpontaneous Speech.
Proceedings of the 1991 IEEE Work-7.shop on Automatic Speech Recognition, Arden House,1991.de Pijper, J. R., Modelling British Intonation.
Foris: Dor-drecht, 1983.440
