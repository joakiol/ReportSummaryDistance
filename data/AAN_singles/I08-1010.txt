UCSG: A Wide Coverage Shallow Parsing SystemG.
Bharadwaja Kumar and Kavi Narayana MurthyDepartment of Computer and Information SciencesUniversity of Hyderabadg vijayabharadwaj@yahoo.com, knmuh@yahoo.comAbstractIn this paper, we propose an architecture,called UCSG Shallow Parsing Architecture,for building wide coverage shallow parsers byusing a judicious combination of linguisticand statistical techniques without need forlarge amount of parsed training corpus tostart with.
We only need a large POS taggedcorpus.
A parsed corpus can be developedusing the architecture with minimal manualeffort, and such a corpus can be used forevaluation as also for performance improve-ment.
The UCSG architecture is designed tobe extended into a full parsing system butthe current work is limited to chunking andobtaining appropriate chunk sequences for agiven sentence.
In the UCSG architecture, aFinite State Grammar is designed to acceptall possible chunks, referred to as wordgroups here.
A separate statistical compo-nent, encoded in HMMs (Hidden MarkovModel), has been used to rate and rank theword groups so produced.
Note that we arenot pruning, we are only rating and rankingthe word groups already obtained.
Then weuse a Best First Search strategy to produceparse outputs in best first order, withoutcompromising on the ability to produce allpossible parses in principle.
We propose abootstrapping strategy for improving HMMparameters and hence the performance ofthe parser as a whole.A wide coverage shallow parser has beenimplemented for English starting from theBritish National Corpus, a nearly 100 Mil-lion word POS tagged corpus.
Note that thecorpus is not a parsed corpus.
Also, thereare tagging errors, multiple tags assigned inmany cases, and some words have not beentagged.
A dictionary of 138,000 words withfrequency counts for each word in each taghas been built.
Extensive experiments havebeen carried out to evaluate the performanceof the various modules.
We work with largedata sets and performance obtained isencouraging.
A manually checked parsedcorpus of 4000 sentences has also beendeveloped and used to improve the parsingperformance further.
The entire system hasbeen implemented in Perl under Linux.Key Words:- Chunking, Shallow Parsing,Finite State Grammar, HMM, Best FirstSearch1 IntroductionIn recent times, there has been an increasing interestin wide coverage and robust but shallow parsingsystems.
Shallow parsing is the task of recoveringonly a limited amount of syntactic information fromnatural language sentences.
Often shallow parsing isrestricted to finding phrases in sentences, in whichcase it is also called chunking.
Steve Abney (Abney,1991) has described chunking as finding syntacticallyrelated non-overlapping groups of words.
In CoNLLchunking task (Tjong Kim Sang and Buchholz,2000) chunking was defined as the task of divid-ing a text into syntactically non-overlapping phrases.Most of the shallow parsers and chunkers de-scribed in literature (Tjong Kim Sang and Buchholz,2000; Carreras and Marquez, 2003; Dejean, 2002;Molina and Pla, 2002; Osborne, 2002; Sang, 2002;Abney, 1996; Grefenstette, 1996; Roche, 1997)have used either only rule based techniques or onlymachine learning techniques.
Hand-crafting rules inthe linguistic approach can be very laborious andtime consuming.
Parsers tend to produce a largenumber of possible parse outputs and in the absence72of suitable rating and ranking mechanisms, selectingthe right parse can be very difficult.
Statisticallearning systems, on the other hand, require largeand representative parsed corpora for training, andsuch training corpora are not always available.Perhaps only a good combination of linguistic andstatistical approaches can give us the best resultswith minimal effort.Other important observations from literature thatmotivated the present work are: 1) Most chunkingsystems have so far been tested only on small scaledata 2) Good performance has been obtained onlyunder restricted conditions 3) Performance is oftenevaluated in terms of individual chunks rather thancomplete chunk sequences for a whole sentence, and4) Many chunkers produce only one output, not allpossible outputs in some ranked order.2 UCSG Shallow ParsingArchitectureUCSG shallow parsing architecture is set withinthe UCSG full parsing framework for parsing nat-ural language sentences which was initiated in theearly 1990?s at University of Hyderabad by KaviNarayana Murthy (Murthy, 1995).
In this paper,the focus is only on chunking - identifying chunks orword groups, handling ambiguities, and producingparses (chunk sequences) for given sentences.
Thiscan be extended to include thematic role assignmentand clause structure analysis leading towards a fullparser.
Figure 1 shows the basic UCSG ShallowParsing Architecture (Kumar and Murthy, 2006).Figure 1: UCSG Shallow Parsing ArchitectureThe input to the parsing system is one sentence,either plain or POS tagged.
Output is an orderedset of parses.
Here by parse we mean a sequenceof chunks that covers the given sentence with nooverlaps or gaps.
The aim is to produce all possibleparses in ranked order hoping to get the best parseto the top.A chunk or a ?word group?
as we prefer to call itin UCSG, is ?a structural unit, a non-overlappingand non-recursive sequence of words, that can asa whole, play a role in some predication?
(Murthy,1995).
Note that word groups do not include clauses(relative clauses, for example) or whole sentences.Every word group has a head which defines thetype of the group.
These word groups thus seemto be similar to chunks as generally understood(Molina and Pla, 2002; Sang and Buchholz, 2000;Megyesi, 2002).
However, chunks in UCSG arerequired to correspond to thematic roles, whichmeans for example, that prepositional phrases arehandled properly.
Many chunkers do not even buildprepositional phrases - prepositions are treated asindividual chunks in their own right.
Thematic rolescan be viewed from question-answering perspective.For example, in the sentence ?I teach at Universityof Hyderabad?, ?at University of Hyderabad?
answersthe ?where?
question and should therefore be treatedas a single chunk.
It is well known that prepositionalphrase attachment is a hard problem and the taskwe have set for ourselves here is thus significantlymore challenging.
The parse outputs in UCSGwould be more semantic and hence should be bettersuited for many NLP applications.In UCSG, a Finite State Grammar-Parser systemgenerates all possible chunks in linear time.
Chunklevel HMMs are then used to rate and rank thechunks so produced.
Finally, a kind of best firstsearch strategy is applied to obtain chunk sequenceshopefully in best first order.
The aim is to developwide coverage, robust parsing systems withoutneed for a large scale parsed corpus to start with.Only a large POS tagged corpus is needed and aparsed corpus can be generated from within thearchitecture with minimal manual effort.
Such aparsed corpus can be used for evaluation as also forfurther performance improvements.We will need a dictionary which includes the fre-quency of occurrence of each word in each possibletag.
Such a dictionary can be developed using a largePOS tagged corpus.732.1 Finite State Grammar-ParserHere the task is only to recognize chunks andnot produce a detailed description of the internalstructure of chunks.
Also, chunks by definition arenon-recursive in nature, only linear order, repetitionand optional items need to be considered.
Finitestate grammars efficiently capture linear precedence,repetition and optional occurrence of words inword groups.
Finite state machines are thus bothnecessary and sufficient for recognizing word groups(Murthy, 1995).
It is also well known that FiniteState Machines are computationally efficient - lineartime algorithms exist for recognizing word groups.All possible word groups can be obtained in a singleleft-to-right scan of the given sentence in linear time(Murthy, 1995).
Finite state grammars are alsoconceptually simple and easy to develop and test.The Finite State module accepts a sentence (ei-ther already POS tagged or tagged with all possiblecategories using the dictionary) and produces an un-ordered set of possible chunks taking into account alllexical ambiguities.2.2 HMMs for Rating and Ranking ChunksThe second module is a set of Hidden MarkovModels (HMMs) used for rating and ranking theword groups already produced by the Finite StateGrammar-Parser.
The hope is to get the bestchunks near the top.
This way, we are not pruningand yet we can hope to get the right chunks nearthe top and push down the others.Words are observation symbols and POS tagsare states in our HMMs.
Formally, a HMM model?
= (pi,A,B) for a given chunk type can be de-scribed as follows:Number of States (N) = number of relevant POSCategoriesNumber of Observation Symbols (M) = number ofWords of relevant categories in the languageThe initial state probabilitypii = P{q1 = i} (1)where 1 ?
i ?
N , q1 is a category (state) starting aparticular word group type.State transition probabilityaij = P{qt+1 = j|qt = i} (2)where 1 ?
i, j ?
N and qt denotes the category attime t and qt+1 denotes the category at time t+1.Observation or emission probabilitybj(k) = P{ot = vk|qt = j} (3)where 1 ?
j ?
N , 1 ?
k ?
M and vk denotes thekth word, and qt the current state.We first pass a large POS tagged corpus throughthe Finite State module and obtain all possiblechunks.
Taking these chunks to be equi-probable,we estimate the HMM parameters by taking theratios of frequency counts.
One HMM is developedfor each major category of chunks, say, one fornoun-groups, one for verb-groups, and so on.
The Bmatrix values are estimated from a dictionary thatincludes frequency counts for each word in everypossible category.
These initial models of HMMsare later refined using a bootstrapping technique asdescribed later.We simply estimate the probability of each chunkusing the following equation :P (O,Q|?)
= piq1bq1(o1)aq1,q2bq2(o2)aq2,q3 ?
?
?aqt?1,qtbqt(ot)where q1 ,q2, ?
?
?, qt is a state sequence, o1 , o2,?
?
?,ot is an observation sequence.
Note that no Viterbisearch involved here and the state sequence is alsoknown.
Thus even Forward/Backward algorithmis not required and rating the chunks is thereforecomputationally efficient.The aim here is to assign the highest rank for thecorrect chunk and to push down other chunks.
Sincea final parse is a sequence of chunks that covers thegiven sentence with no overlaps or gaps, we evaluatethe alternatives at each position in the sentence in aleft-to-right manner.Here, we use Mean Rank Score to evaluate the per-formance of the HMMs.
Mean Rank Score is themean of the distribution of ranks of correct chunksproduced for a given training corpus.
Ideally, all cor-rect chunks would be at the top and hence the scorewould be 1.
The aim is to get a Mean Rank Score asclose to 1 as possible.2.3 Parse Generation and RankingParsing is a computationally complex task andgenerating all possible parses may be practicallydifficult.
That is why, a generate-and-test approach74where we first generate all possible parses andthen look for the correct parse among the parsesproduced is impracticable.
Simply producing allor some parses in some random or arbitrary orderis also not of much practical use.
Many chunkersproduce a single output which may or may notbe correct.
Here we instead propose a best firststrategy wherein the very production of possibleparses is in best first order and so, hopefully, wewill get the correct parse within the top few and inpractice we need not actually generate all possibleparses at all.
This way, we overcome the problemsof computational complexity and at the same timeavoid the risk of missing the correct parse if pruningis resorted to.
Performance can be measured notonly in terms of percentage of input sentences forwhich a fully correct parse is produced but also interms of the rank of the correct parse in the top kparses produced, for any chosen value of k.It may be noted that although we have alreadyrated and ranked the chunks, simply choosing thelocally best chunks at each position in a givensentence does not necessarily give us the best parse(chunk sequence) in all cases.
Hence, we havemapped our parse selection problem into a graphsearch problem and used best first search algorithmto get the best parse for a given sentence.Words and chunks in a sentence are referred to interms of the positions they occupy in the sentence.Positions are marked between words, starting fromzero to the left of the first word.
The positions inthe sentence are treated as nodes of the resultinggraph.
If a sentence contains N words then thegraph contains N + 1 nodes corresponding to theN + 1 positions in the sentence.
Word group Wi,j isrepresented as an edge form node i to node j. Wethus have a lattice structure.
The cost of a givenedge is estimated from the probabilities given bythe HMMs.
If and where a parsed training corpus isavailable, we can also use the transition probabilityfrom previous word group type to current wordgroup type.
It is possible to use the system itself toparse sentences and from that produce a manuallychecked parsed corpus with minimal human effort.We always start from the initial node 0.
N is thegoal node.
Now our parse selection problem for asentence containing N words becomes the task offinding an optimal (lowest cost) path from node 0to node N .We use the standard best first search algorithm.In best first search, we can inspect all the currently-available nodes, rank them on the basis of our par-tial knowledge and select the most promising of thenodes.
We then expand the chosen node to gener-ate it successors.
The worst case complexity of bestfirst search algorithm is exponential: O(bm), whereb is the branching factor (i.e., the average number ofnodes added to the open list at each level), and m isthe maximum length of any path in the search space.As an example, a 40 word sentence has been shownto produce more than 1015 different parses (Kumar,2007).
In practice, however, we are usually interestedin only the top k parses for some k and exhaustivesearch is not called for.2.4 BootstrappingThe HMM parameters can be refined through boot-strapping.
We work with large data sets runninginto many hundreds of thousands of sentences andBaum-Welch parameter re-estimation would not bevery practical.
Instead, we use parsed outputs to re-build HMMs.
By parsing a given sentence using thesystem and taking the top few parses only as train-ing data, we can re-build HMMs that will hopefullybe better.
We can also simply use the top-rankedchunks for re-building the HMMs.
This would re-duce the proportion of invalid chunks in the trainingdata and hence hopefully result in better HMM pa-rameters.
As can be seen from the results in the nextsection, this idea actually works and we can signif-icantly improve the HMM parameters and improveparser performance as well.3 Experiments and ResultsThe entire parsing system has been implemented inPerl under Linux.
Extensive experimentation hasbeen carried out to evaluate the performance of thesystem.
However, direct comparisons with otherchunkers and parsers are not feasible as the architec-tures are quite different.
All the experiments havebeen carried out on a system with Pentium Core 2DUO 1.86 GHz Processor and 1 GB RAM.
Tran-scripts from the implemented system have been in-cluded in the next section.3.1 DictionaryWe have developed a dictionary of 138,000 words in-cluding frequency of occurrence for each tag for eachword.
The dictionary includes derived words but notinflected forms.
The dictionary has been built fromthe British National Corpus(BNC) (Burnard, 2000),an English text corpus of about 100 Million words.Closed class words have been manually checked.
Thedictionary has a coverage of 98% on the BNC corpusitself, 86% on the Reuters News Corpus (Rose et75al., 2002) (about 180 Million words in size), 96.36%on the Susanne parsed corpus (Sampson, 1995) and95.27% on the Link parser dictionary.3.2 Sentence Boundary DetectionWe have developed a sentence segmentation moduleusing the BNC corpus as training data.
We haveused delimiter, prefix, suffix and after-word as fea-tures and extracted patterns from the BNC corpus.Decision Tree algorithms have been used and an av-erage F-Measure of 98.70% has been obtained, com-parable to other published results.
See (Htay et al,2006) for more details.3.3 Tag SetWe have studied various tag sets including BNC C5,BNC C7, Susanne and Penn Tree Bank tag sets.Since our work is based on BNC 1996 edition withC5 tag set, we have used C5 tag set and made someextensions as required.
We now have a total of 71tags in our extended tag set (Kumar, 2007).3.4 Manually Parsed CorpusWe have developed a manually checked parsedcorpus of 4000 sentences, covering a wide variety ofsentence structures.
Of these, 1000 sentences havebeen randomly selected from the BNC corpus, 1065sentences from ?Guide to Patterns and Usage inEnglish?
(Hornby, 1975) and 1935 sentences fromthe CoNLL-2000 test data.
This corpus is thus veryuseful for evaluating the various modules of theparsing architecture and also for bootstrapping.This corpus was developed by parsing the sen-tences using this UCSG shallow parser itself and thenmanually checking the top parse and making correc-tions where required.
Our experience shows that thisway we can build manually checked parsed corporawith minimal human effort.3.5 TaggingIf a POS tagger is available, we can POS tag theinput sentences before sending them to the parser.Otherwise, all possible tags from the dictionary maybe considered.
In our work here, we have not usedany POS tagger.
All possible tags are assigned fromour dictionary and a few major rules of inflectionalmorphology of English, including plurals for nouns,past tense, gerundial and participial forms of verbsand degrees of comparison for adjectives are handled.Unresolved words are assigned NP0 (Proper Name)tag.3.6 Finite State GrammarWe have developed a Finite State Grammar foridentifying English word groups.
The Finite StateMachine has a total of 50 states of which 24 are finalstates.
See (Kumar, 2007) for further details.The UCSG Finite State Grammar recognizesverb-groups, noun-groups, adverbial-groups,adjective-groups, to-infinitives, coordinate andsubordinate conjunctions.
There are no separateprepositional phrases - prepositions are treated assurface case markers in UCSG - their primary roleis to indicate the relationships between chunks andthe thematic roles taken up by various noun groups.Prepositional groups are therefore treated on parwith noun groups.We have evaluated the performance of the FSMmodule on various corpora - Susanne Parsed Corpus,CoNLL 2000 test data set and on our manuallyparsed corpus of 4000 sentences.
The evaluationcriteria is Recall (the percentage of correct chunksrecognized) alone since the aim here is only toinclude the correct chunks.
We have achieved a highrecall of 99.5% on manually parsed corpus, 95.06%on CoNLL test data and 88.02% on Susanne corpus.The reason for the relatively low Recall on the Su-sanne corpus is because of the variations in the def-inition of phrases in Susanne corpus.
For example,Susanne corpus includes relative clauses into noungroups.
The reasons for failures on CoNLL test datahave been traced mainly to missing dictionary en-tries and inability of the current system to handlemulti-token adverbs.3.7 Building and Refining HMMsHMMs were initially developed from 3.7 MillionPOS-tagged sentences taken from the BNC corpus.Sentences with more than 40 words were excluded.Since we use an extended C5 tag set, POS tags hadto be mapped to the extended set where necessary.HMM parameters were estimated from the chunksproduced by the Finite State grammar, taking allchunks to be equi-probable.
Separate HMMs werebuilt for noun groups, verb groups, adjective groups,adverb groups, infinitive groups and one HMM forall other chunk types.The chunks produced by the FSM are ranked usingthese HMMs.
It is interesting to observe the Recalland Mean Rank Score within the top k ranks, wherek is a given cutoff rank.
Table 1 shows that there isa clear tendency for the correct chunks to bubble up76close to the top.
For example, more than 95% of thecorrect chunks were found within the top 5 ranks.Table 1: Performance of the HMM Module on theManually Parsed Corpus of 4000 sentencesPlain POS TaggedCut- Mean Cumulative Mean Cumulative-off Rank Recall (%) Rank Recall (%)1 1 43.06 1 62.742 1.38 69.50 1.28 86.973 1.67 84.72 1.43 95.644 1.85 91.69 1.50 98.315 1.96 95.13 1.54 99.25We have also carried out some experiments to seethe effect of the size of training data used to buildHMMs.
We have found that as we use more andmore training data, the HMM performance improvessignificantly, clearly showing the need for workingwith very large data sets.
See (Kumar, 2007) formore details.3.7.1 BootstrappingTo prove the bootstrapping hypothesis, we havecarried out several experiments.
Plain text sentencesfrom BNC corpus, 5 to 20 words in length, have beenused.
All possible chunks are obtained using the Fi-nite State Grammar-Parser and HMMs built fromthese chunks.
In one experiment, only the chunksrated highest by these very HMMs are taken as train-ing data for bootstrapping.
In a second experiment,best first search is also carried out and chunks fromthe top ranked parse alone are taken for bootstrap-ping.
In a third experiment, data from these twosources have been combined.
Best results were ob-tained when the chunks from the top parse alonewere used for bootstrapping.
Table 2 shows the ef-fect of bootstrapping on the HMM module for plainsentences.Table 2: Effect of Bootstrapping: on 4000 sentencesfrom Manually Parsed Corpus containing a total of27703 chunksCutoff Iteration-1 Iteration-2Recall Mean Recall MeanRank Rank1 45.52 1.0 47.25 1.02 71.43 1.36 72.81 1.353 85.22 1.63 85.95 1.604 91.75 1.80 92.20 1.775 94.94 1.90 95.30 1.87It may be observed that both the Recall and MeanRank Scores have improved.
Our experiments showthat there is also some improvement in the final parsewhen the HMMs obtained through bootstrapping areused.
These observations, seen consistently for bothplain and POS tagged sentences, show the effective-ness of the overall idea.3.8 Parse Generation and RankingIt may be noted that in principle the performanceof the parser in terms of its ability to produce thecorrect parse is limited only by the Finite StateGrammar and the dictionary, since the other mod-ules in the UCSG architecture do not resort to anypruning.
However, in practical usage we generallyimpose a time limit or a cutoff and attempt toproduce only the top k parses.
In this latter case,the percentage of cases where the fully correctparse is included would be a relevant performanceindicator.
Percentage of correct chunks in the topparse is another useful indicator.When tested on untagged sentences, on the 1065linguistically rich sentence corpus forming part ofthe manually checked parsed corpus developed byus, the parser could generate fully correct parsewithin the top 5 parses in 930 cases, that is, 87.32%of the cases.
In 683 cases the correct parse wasthe top parse, 146 correct parses were found inposition 2, 56 in position 3, 29 in position 4 and16 in position 5.
Thus the mean rank of thecorrect parses is 1.44.
There is a clear tendencyfor the correct parses to appear close to the top,thereby verifying the best first strategy.
If top 10parses are generated, correct parse is obtained in52 more cases and the Mean Rank Score goes to 1.75.We give below the performance on the whole ofour 4000 strong manually checked corpus.
Plain sen-tences and POS tagged sentences have been testedseparately.
The results are summarized in table 3.Here, we have restricted the parsing time taken bythe best first search algorithm to 3 epoch seconds foreach sentence.77Table 3: Performance of the Best First Search Mod-ule - Test Data of 4000 SentencesRank No.
of Correct Parses(Plain) (POS tagged)1 1210 21932 352 4953 157 1644 83 1295 68 91% of Correct 46.75 76.80Parses in Top 5% of Correct 83.92 88.26Chunks inBest ParseIn about 77% of the cases, the fully correct parseis found within the top 5 parses when the inputsentences are POS tagged.
Given the nature ofchunks produced in UCSG, this is quite encouraging.In fact the top parse is nearly correct in many cases.Further experiments and manual evaluations areplanned.We have also observed that 96.01% of the wordsare assigned correct POS tags in the top parse.
Weobserve that most of the times the top parse givenby the parse generation module is almost correct.Chunkers are usually evaluated just for the per-centage of correct chunks they produce.
We haveplaced greater demands on ourselves and we expectour parser to produce optimal chunk sequence forthe whole sentence.
Further, we produce all (or topfew) combinations and that too in hopefully a bestfirst order.
Also, the very nature of chunks in UCSGmakes the task more challenging.
More over, we haveused a fairly fine grained tag set with more than 70tags.
The data we have started with, namely theBNC POS tagged corpus, is far from perfect.
Giventhese factors, the performance we are able to achieveboth in terms of percentage of correct chunks in thetop parse and rank of the fully correct parse is veryencouraging.4 Transcripts:Here we give the actual transcripts from the system.For want of space, only a very simple example hasbeen included.
Stars have been added in the begin-ning of lines containing correct alternatives.Input: I am studying at University of Hyderabad.Tags from the Dictionary: <PNN_CRD><i>##<VBB><am>##<VVG><studying>##<PRN_PRP_AVP><at>##<NN1><university>##<PRN_PRF_AVP><of>##<NP0><Hyderabad>##Chunks Recognized by the FSM:<ng><0-1><CRD><i><ajg><0-1><CRD><i>*<ng><0-1><PNN><i><vg><1-2><VBB><am>*<vg><1-3><VBB><am>##<VVG><studying><vg><1-4><VBB><am>##<VVG><studying>##<AVP><at><vgs><2-3><VVG><studying><ng><2-3><VVG><studying><ajg><2-3><VVG><studying><vgs><2-4><VVG><studying>##<AVP><at><ng><2-5><VVG><studying>##<PRP><at>##<NN1><university><ng><2-7><VVG><studying>##<PRP><at>##<NN1><university>##<PRF><of>##<NP0><hyderabad><part><3-4><AVP><at><ng><3-5><PRP><at>##<NN1><university>*<ng><3-7><PRP><at>##<NN1><university>##<PRF><of>##<NP0><hyderabad><ng><4-5><NN1><university><ng><4-7><NN1><university>##<PRF><of>##<NP0><hyderabad><part><5-6><AVP><of><ng><5-7><PRF><of>##<NP0><hyderabad><ng><6-7><NP0><hyderabad>Ranking by HMMs:*<ng><0-1><PNN><i><-3.2491231040407><1><3><1><ng><0-1><CRD><i><-9.56376400947296><2><3><1><ajg><0-1><CRD><i><-36.8109739544272><3><3><1><vg><1-2><VBB><am><-7.27367328109116><1><3><2>*<vg><1-3><VBB><am>##<VVG><studying><-15.945895214915><2><3><2><vg><1-4><VBB><am>##<VVG><studying>##<AVP><at><-25.5608664628101><3><3><2><vgs><2-3><VVG><studying><-10.5328994260119><1><6><3><ng><2-3><VVG><studying><-12.7929752284183><2><6><3><vgs><2-4><VVG><studying>##<AVP><at><-20.147870673907><3><6><3><ng><2-5><VVG><studying>##<PRP><at>##<NN1><university><-30.3473074722636><4><6><3><ajg><2-3><VVG><studying><-32.767076078699><5><6><3><ng><2-7><VVG><studying>##<PRP><at>##<NN1><university>##<PRF><of>##<NP0><hyderabad><-35.1643970692879><6><6><3><part><3-4><AVP><at><-7.99897865005313><1><3><4><ng><3-5><PRP><at>##<NN1><university><-15.7772256956695><2><3><4>*<ng><3-7><PRP><at>##<NN1><university>##<PRF><of>##<NP0><hyderabad><-20.5943152926938><3><3><4><ng><4-5><NN1><university><-13.2259579687766><1><2><5><ng><4-7><NN1><university>##<PRF><of>##<NP0><hyderabad><-18.0430475658009><2><2><5><part><5-6><AVP><of><-3.87313237166961><1><2><6><ng><5-7><PRF><of>##<NP0><hyderabad><-19.0843146188301><2><2><6><ng><6-7><NP0><hyderabad><-3.43828759462479><1><1><7>Final Parse:*<ng>[<PNN><i>]</ng> <vg>[<VBB><am>##<VVG><studying>]</vg><ng>[<PRP><at>##<NN1><university>##<PRF><of>##<NP0><hyderabad>]</ng> -- -41.2629507152745<ng>[<PNN><i>]</ng> <vg>[<VBB><am>]</vg> <ng>[<VVG><studying>]</ng><ng>[<PRP><at>##<NN1><university>##<PRF><of>##<NP0><hyderabad>]</ng> -- -46.7375549370651<ng>[<PNN><i>]</ng> <vg>[<VBB><am>]</vg> <ng>[<VVG><studying>##<PRP><at>##<NN1><university>##<PRF><of>##<NP0><hyderabad>]</ng> -- -47.1608105580448<ng>[<CRD><i>]</ng> <vg>[<VBB><am>##<VVG><studying>]</vg><ng>[<PRP><at>##<NN1><university>##<PRF><of>##<NP0><hyderabad>]</ng> -- -47.5775916207068<ng>[<PNN><i>]</ng> <vg>[<VBB><am>##<VVG><studying>##<AVP><at>]</vg><ng>[<NN1><university>##<PRF><of>##<NP0><hyderabad>]</ng> -- -48.3266542362767785 Conclusions:A hybrid architecture for developing wide coverageshallow parsing systems, without need for a largescale parsed corpus to start with, has been proposedand its effectiveness demonstrated by developing awide coverage shallow parser for English.
The sys-tem has been built and tested on very large data sets,covering a wide variety of texts, giving us confidencethat the system will perform well on new, unseentexts.
The system is general and not domain spe-cific, but we can adapt and fine tune for any specificdomain to achieve better performance.
We are con-fident that wide coverage and robust shallow parsingsystems can be developed using the UCSG architec-ture for other languages of the world as well.
Weplan to continue our work on English parsing whilewe also start our work on Telugu.ReferencesSteven P. Abney.
1991.
Parsing by Chunks.
Kluwer,Principle-Based Parsing: Computation and Psy-cholinguistics edition.Steven P. Abney.
1996.
Partial Parsing via Finite-State Cascades.
In Workshop on Robust Parsing,8th European Summer School in Logic, Languageand Information, pages 8?15, Prag.L.
Burnard.
2000.
The Users?
Reference Guide forthe British National Corpus.
Oxford UniversityComputing Services, Oxford.Xavier Carreras and Lluys Marquez.
2003.
PhraseRecognition by Filtering and Ranking with Per-ceptrons.
In Proceedings of the InternationalConference on Recent Advances in Natural Lan-guage Processing, RANLP-2003, pages 127?132,Borovets, Bulgaria.Herve Dejean.
2002.
Learning Rules and their Ex-ceptions.
In Journal of Machine Learning Re-search, Volume 2, pages 669?693.G.
Grefenstette.
1996.
Light Parsing as Finite StateFiltering.
In Workshop on Extended Finite StateModels of Language, Budapest, Hungary.A.
S. Hornby.
1975.
Guide to Patterns and Usage inEnglish.
Oxford University Press.Hla Hla Htay, G. Bharadwaja Kumar, andKavi Narayana Murthy.
2006.
ConstructingEnglish-Myanmar Parallel Corpora.
In Proceed-ings of Fourth International Conference on Com-puter Applications, pages 231?238, Yangon, Myan-mar.G Bharadwaja Kumar and Kavi Narayana Murthy.2006.
UCSG Shallow Parser.
Proceedings of CI-CLING 2006, LNCS, 3878:156?167.G.
Bharadwaja Kumar.
2007.
UCSG ShallowParser: A Hybrid Architecture for a Wide Cover-age Natural Language Parsing System.
Phd thesis,University of Hyderabad.B Megyesi.
2002.
Shallow Parsing with PoS Taggersand Linguistic Features.
In Journal of MachineLearning Research, Volume 2, pages 639?668.Antonio Molina and Ferran Pla.
2002.
Shallow Pars-ing using Specialized HMMs.
In Journal of Ma-chine Learning Research, Volume 2, pages 595?613.Kavi Narayana Murthy.
1995.
Universal ClauseStructure Grammar.
Phd thesis, University of Hy-derabad.Miles Osborne.
2002.
Shallow Parsing using Noisyand Non-Stationary Training Material.
In Journalof Machine Learning Research, Volume 2, pages695?719.E.
Roche.
1997.
Parsing with Finite State Transduc-ers.
MIT Press, finite-State Language Processingedition.T.G.
Rose, M. Stevenson, and M. Whitehead.
2002.The Reuters Corpus Volume 1 - from Yesterday?sNews to Tomorrow?s Language Resources.
In Pro-ceedings of the Third International Conference onLanguage Resources and Evaluation, Las Palmasde Gran Canaria.Geoffrey Sampson.
1995.
English for the Computer.Clarendon Press (The Scholarly Imprint of OxfordUniversity Press).E.
F. Tjong Kim Sang and S. Buchholz.
2000.
Intro-duction to the CoNLL-2000 Shared Task: Chunk-ing.
In Proceedings of CoNLL-2000 and LLL-2000,pages 127?132, Lisbon, Portugal.Erik F. Tjong Kim Sang.
2002.
Memory-Based Shal-low Parsing.
In Journal of Machine Learning Re-search, Volume 2, pages 559?594.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.Introduction to the CoNLL-2000 Shared Task:Chunking.
In Claire Cardie, Walter Daelemans,Claire Nedellec, and Erik Tjong Kim Sang, edi-tors, Proceedings of CoNLL-2000 and LLL-2000,pages 127?132.
Lisbon, Portugal.79
