USING SEMANTICS TO CORRECTPARSER OUTPUT FOR ATIS UTTERANCESSheryl YoungSchool of Computer ScienceCamegie Mellon UniversityPittsburgh, PA 15213ABSTRACTThis paper describes the structure and operation of SouL, or Semantically-Oriented Understanding of Language.
SouL is a knowledge intensivereasoning system which is opportunistically used to provide a morethorough, fme grained analysis of an input utterance following itsprocessing by a case-frame speech parser.
The SOUL postprocessor reliesupon extensive semantic and pragmatic knowledge to correct, rejectand/or clarify the outputs of the CMU PHOl/NlX case-frame parser forspeech and speech transcripts.
Specifically, we describe briefly bothsome of the linguistic phenomena which SouL addresses and how SOULworks to correct inaccurate interpretatiens produced by the PHOENIXparser.
Finally, we present the results on four separate, nen-ovedappingtest sets.
Our "pilot" test sets include the June 1990 DARPA ATIS0 testset and two test sets composed of unseen ATIS0 data distributed in June1990 that, unlike the DARPA test sets, contain tmrestrieted utterances.Our forth test set is the official DARPA February 1991 ATIS1 test set.These evaluations illustrate the decrease in error ram that results fromSOUL's semantic and pragmatic postprocessing are most pronounced inunrestricted, as opposed to carefully constrained test sets.
Specifically, aperformance comparison between unrestricted mad restricted test sets inpilot experiments show that error rates are reduced by 84% as opposed to54% when no utterances are pruned from the speaker t anscripts.OVERVIEWThe DARPA speech and natural language community hasrecently adopted the domain of air travel information for itsspoken language systems.
The domain has been named ATIS,for air travel information system.
Training and test data aregathered for this common task by employing speakers to verballyinteract with a database in order to solve one or more randomlyassigned, predefined problems with predefined goals andprefere~es.
To perform their task, speakers must verbally queryan air travel database.
Speakers are not required to use complete,well formed or syntactically correct utterances.
They can ask forany information, regardless of whether the request is reasonable,or whether the information contained in the database.
Hence,true spontaneous speech is generated.
Data is collected byrecording beth subject utterances and database responses asspeakers perform these verbal tasks.
The recorded ata is thendivided into training and test sets of use by the DARPA com-munity.Thus far, the utterances selected for evaluation test sets arehighly constrained, being restricted to those utterances which canbe answered using the information i  the database and can eitherbe interpreted and answered in isolation, or by using only thecontext of a preceeding utterance.
All utterances that are am-biguous, refer to objects and actions not included in the database,request information that is not available, or contain spontaneousspeech phenomena such as mid-utterance oral edits and correc-tions are removed from the official test sets.For these evaluations, we designed the SOUL system to en-hance the performance of the CMU ATIS system when operatingin isolated or limited context modes.
The system operates in anopportunistic manner.
It is only called upon to perform postprocessing when there is reasonable uncertainty in the case-frameoutput.
This uncertainty can result from large regions of un-accounted-for speech, multiple, competing interpretations andseemingly incomplete or un-meaningful interpretations.
Input toSOUL is the utterance, all the words and phrases matched by thecase-frame parser, PHOENIX, mad a set of hypothesized inter-pretation frames (or instantiated case-frame).
SOUL outputseither an error message (e.g.
No information in database onBOULDER) or a single interpretation composed of corrections,deletions, and all forms of modifications to the instantiated case-frame It does this by using a large semantic and pragmaticknowledge base in conjunction with abducfive reasoning andconstraint satisfaction techniques.
A by-product of the SOULdesign is thatit also provides much of the semantic and pragmaticknowledge required for our complete dialog and predictionfacilities, previously called MINDS system (1988, 1989a'b, 1990)\[1, 2, 3, 4\].As SOUL was designed to deal with all spontaneouslygenerated utterances, we expect there will be some advantagesfor using the system while processing the required, highlyrestricted ata, but that the system will be far more valuable whenprocessing unrestricted input.
To evaluate the effectiveness andrelative payoff of SOUL, we investigated the following twoissues.First, we wanted to see how much, ff any impact use of a largesemantic and pragmatic knowledge base would have in reducingerror relative to a semantically based (by definition) case-frameparser \[5, 6, 7\] when processing only isolated utterances or ut-terances that can be interpreted using only very limited context.Caseframe parsers employ both semantic and syntacticknowledge.
They do not use an extensive knowledge base orinferencing procedures.
However, they have proven to be veryrobust and effective in producing interpretations of both wellformed and ill-formed user input.Secondly, we wanted to determine if the use of a knowledgebase alone would allow us to process all types of utterances,including those which are un-answerable, r quest information notin the database and outside the definition of system capabilities,106are ambiguous, as well as to be able to detect hose utteranceswhich can ordy be answered by using unavailable contextualinformation.To evaluate these questions, we assessed performance of ourcase-frame speech parser, PHOENIX, both with and withoutSOUL on four independent test sets.
Two of the test sets con-tained unrestricted ata -- every utterance generated wasprocessed.
The other two test sets (DARPA ATIS0 and ATIS1)contained restricted utterances, as described above.The remaineder of this paper describes how SOUL uses seman-tic and pragmatic knowledge to correct, reject and/or clarify theoutputs of the POENIX case-frame parser in the ATIS domain.The next section summarizes some of the linguistic phenomenawhich Soul  addresses.
The following section briefly summarizeshow it works to correct inaccurate parses.
The last sectionpresents the results of four performance evaluations which con-mist the performance of the PHOENIX case-frame parser withand without he SOUL postprocessor.LINGUISTIC PHENOMENA EXAMINEDSOUL was developed to cope with errors produced by the CMUPHOENIX speech and transcript parsing software in the ATISdomain.
Specifically, SOUL augments the basic, rapid patternmatching and speech recognition functions of the PHOENIXsystem with knowledge intensive reasoning techniques for morefree grained analysis of the preliminary alternative interpreta-tions.
Initially we analyzed the performance of the PHOENIXsystem on a set of training data.
The data consisted of 472utterances comprising dialogs b0 through bn of the ATIS0 train-ing set.
An evaluation of the performance of the originalPHOENIX system on this data revealed that PHOENIX ex-perienced ifficulties with the following problernatie linguisticphenomena, which composed a total of 44.3 percent of the ut-terances: (Note: underlined information ot in database)Unanswerable queries, no Information in database, or Illegalaction requested) (Found in 19.3% of sen-tences In the trahalng corpus)What ground transportation is available fromthe airport in Denver to Boulder at three pmon the twenty second?
How do I makereservations?
Show all the flights from Dal-la.._~ to Fort Worth Interpreting these ut-terances requires knowledge on the limita-tions of the database, detection of user mis-conceptions and constraint violations as wellas the ability to recognize and understand in-formation ot contained in the database.Context dependent utterances (9.2%)Show me all returning flights To process iso-lated utterances or utterances that are onlyallowed to be interpreted using limited con-textual information, it is helpful to be able torecognize those utterances where critical in-formation cannot be reasonably inferred.Ungrammatical nd ill-formed utterances (3.0%)What date does flight eight seventy sevenfrom San Francisco to Dallas leave from?Ungrammaticalitty is a part of spontaneousspeech.
However, one can also obtain ill-formed or ungrammatical input from mis-recognition of an input string.
Thesephenomena preclude using a strict syntacticconstraints and clues such as definite refer-ence or any type of case marker such as thosetypically used in textual case-frame parsers.Ambiguous queries (6A%)What?s the distance from San Francisco toOakland?
The example query can be inter-preted as meaning the city San Francisco rSan Francisco International irport.
In thecase of the former, no information is con-tained in the database.
In the absence ofdisambiguating context, it is important to beable to recognize all interpretations.Yes/No and Quantified Yes/No's (3.2%)Do all of the flights from Pittsburgh to Bos-ton serve meals?
These, as well as the nextcategory of utterances require that the criticalinformation be detected from the input.However, they are not problematic when ac-curately recognized from the speech input.Superlatives and Comparatives (3.2%)What's the cheapest flight from Atlanta toDFW?SOUL was designed to provide a free grained analysis of inputin an opportunistic manner by relying upon a large knowledgebase.
It was also tuned to "pay attention" to the above listedlinguistic phenomena that posed problems for the original versionof the PHOENIX speech processing ease-frame parser.
Further-more, it was also designed to address ome of the problemsinherent in spontaneously uttered input.THE CMU SYSTEM: OVERVIEWThe CMU ATIS System is composed of three interactingmodules; a speech recognizer (SPHINX), a robust case-frameparser adapted for spontaneous peeeh (PHOENIX), and asemantic and pragmatic processor (SOUL) which can work eitheron isolated utterances orcan incorporate he dialog and predictionfunctionality of the MINDS system.
For results reported in thispaper, SPHINX produces a single output string which is thenprocessed by the speech case-frame parser, PHOENIX (Ward,1990).
The PHOENIX case-frame parser builds plausible parsesof the input string by using robust slot filling heuristics.
Allpossible case-frame slots associated with any portion of an inpututterance with a reasonable recognition probability are filled.Then candidate case-frames are built, bottom up, which try toaccount for as much of the matched input as possible.
Oncecandidate interpretations are generated, PHOENIX either sendsall interpretations to SOUL or else, when operating in the absenceof SOUL (or when an unambiguous interpretation exists), selectsthe interpretation which accounts for the most input.
In eithercase, one interpretation is selected.
This interpretation is then put107into a cannonical form and mapped into an SQL TM databasequery.
This query is then passed to the database and output ispresented on a computer screen to the user.THE SOUL SYSTEMSoUL relies on a semantic and pragmatic knowledge base tocheck for consistency in the output interpretations produced bythe parser.
There are three special features abeut this frame-based system.
First, SoUL not only defines legal values, at-tributes, and concepts within the ATIS domain, but it also ac-counts for much extra-domain i formation as well.
Second, ituses inheritance and reasoning to determine contextual con-straints, so consistency and constraints can be maintained forcombinations of information never before seen.
Third, the sys-tem uses a single reference data structure for determining illegalinput (for which action is prohibited) and unanswerable input (forwhich no information is in the database).REASONINGThe mechanisms underlying SoUL's abilities are the use ofconstraint satisfaction techniques in conjunction with what hasbeen called abductive reasoning \[8, 9\] or concretion \[10\].
Theseare general, domain independent techniques which rely upon adomain specific knowledge base.
The abductive reasoning com-ponent is used to evaluate alternative orcandidate phrase matchesand to decide which phrases modify one another and to determinewhich can be put together to form one or more meaningfulut terances .To illustrate, consider the following utterance: "Does B standfor business class".
The case-frame parser instanriates the fol-lowing sequence of concepts or cases: B = abbreviation B =code B = letter Does = list Does = Explain business class =class-of-service class = class-of-service stand-for = mean.The knowledge base is faced with three basic concepts: B,which is an instance of some abbreviation.
Specifically, B is anabbreviation for Breakfast and for Business-Class.
B can also bea letter, which can be part of a flight number identifying theairline carrier, or part of the call letters of an aircraft, or one ofthe letters composing the name of a person reserving a ticket.Stand-for indicates equivalence, a specific predicate that is eithertrue or false.
Business-class is an interpretation preferable toclass alone, as it is more specific.
Given an equivalence predicateand a specific concept business-class, the only allowable inter-pretation of "B" is that its an abbreviation.
Even in the absenceof an equivalence predicate, there is no additional informationwhich would support he interpretation f "B" as being part of aflight number, carrier identification, aircraft call number or aperson's name.
Now, given "Business-class", aninstance of afare class, an equivalence r lationship and a choice betweenalternative abbreviation expansions for B, the only expansionwhich would make the predicate true is the instance of B ab-breviating "Business-class".CONSTRAINT  REPRESENTATION AND USEThe abductive component ot only determines what possiblephrases compose ameaningful request or statement, i  also spotscombinations which violate domain constraints.
Examples of thetypes of constraint violations which are recognized include viola-tions on beth type constraints of objects and attributes as well asn-tuple constraint violations.To illustrate, consider the following rule for long range trans-portarion taken from the current knowledge base: Objects: long-range vehicle, origin-location, destination-locationinanimate~animate-objects to-be-transported.
Here we have con-straints not only on the type of objects that may fill these roles(and of course information abeut these objects is contained inother portions of the knowledge base) but we have relationalconstraints as well.
The following are the single constraints onthe objects involved in long-range transportation.
Vehicles areconstrained to be included in the instances of a long rangevehicle.
These include airplanes, trains and cars that are not taxior limosines.
The origin and desrinarion are constrained to beeither airports (or their abbreviations ) or locations that mustinclude a city (and may include additional information such asstate and/or location within or relative to the city.
In this ex-ample, there is a single relational, or tuple constraint.
It posesres~crions on the relationship between the origin and destinationslot fdlers.
These include: If two dries are involved, they cannotbe the same, and there must be a set difference between thelistings of the airports that service the two dries.
If two airportsare involved, they must not be the same airport.
If a city and anairport are involved, the city must be served solely by the airportlisted.
Under these rules, you cannot fly from Dallas to FortWorth in this database.
However, you can fly from San Fran-cisco to San Jose.
Similar rules for short-range transportationwould rule out taking a taxi from Pittsburgh to Boston.These types of definitions for events, actions, objects, etc.
andthe constraints placed upon them also allow one to determinewhether or not there is sufficient information upon which to takean action.
Hence, if a flight is not clearly delineated givenwhatever context is allowable under the test set rules, these rulescan determine whether a query is context dependent or insuf-ficiently specified.EXAMPLESThe following examples from the ATIS corpus further il-lustrate how the reasoning component operates.What is the shortest flight from Dallas to Fort Worth?PHOENIX would look for flights from Dallas to Fort Worth, as-suming a correct interpretation.
However, SOUL knows that Dal-las and Fort Worth are beth served only by DFW airport.
Sinceyou cannot akeoff and land in the same place, this is an illegaland unanswerable request.How much would it cost to take a taxi from Pittsburgh toBoston?
PHOENIX recognizes "How much would it cost fromPittsburgh to Boston", and would output he corresponding list offlights and fares.
SOUL recognizes that "to take a taxi" is impor-tant information that has not been included in the interpretation.108It also knows that axis are short-range transportation vehicles.
Ifthe request were legal, SOUL would tell PHOEENqX tO add taxi asmethod of transportation a d delete airplanes as the transpor-tation vehicle.
However, this request violates the constraint onwhat constitutes a short-range trip, so SoUL outputs the violationtype to the speaker (or generates an error message as the CAS).Are there any Concord flights from Dallas to Boston?
HerePHOENIX find a request for flights between Dallas and Boston.SOUL tells the parser to "add Aircraft-Class aircraft_code SSC".Show all the flights to San Francisco on August 18.
HereSoUL recognizes that a departure location has been omitted andcannot be found in any unaccounted-for input.
Hence, this is acontext-dependent s tence.SOUL OUTPUTSoUL takes as input the candidate parses as well as the inputstring.
The output is a list of instructions and codes which aresent back to PHOENIX so they can be incorporated into thedatabase query.
Specifically, SoUL outputs an existing inter-pretation augmented bythe following information:?
When there is missing, critical information, databasetables and bound variables are output.?
When there is a reasonable s lection of informationinterpreted under an incorrect top level frame (e.g.air travel vs ground travel) it provides a correct oplevel interpretation r frame.?
When a specific word string is mis-interpreted, itprovides corrections in the form of additional vari-ables and their bindings to add as well as variablesand bindings to delete.?
When a query involves information not included inthe current, restricted atabase, when the query re-quires information that is out of the chosen domain,and when the user is asking the system to perform afunction it is not designed to do, it outputs pecificerror codes to PHOENIX indicating what theproblem is and outputs specific corrective infer-marion to the user screen.
This is designed to correctuser mis-conceptions.
(e.g.
Show all flights fromDallas to Fort Worth).?
Finally, when two un-related queries are merged intoa single utterance, the system outputs a "break point"so PHOENIX can re-parse the input into twoseparate requests.
(For example the utterance Showall flights from PhUladelohia to Boston and how farBoston is from Cape Cod would be divided up whereas Show all flights from Philladelphia to Boston aswell as their minimum fares, would not.To summarize, SoUL looks for and corrects the following typesof problems: (1) Information that is missing from the output ofPHOENIX, which is added by SoUL.
When too much informationis missing, the system produces the "do-not-understand"response.
(2) Constraint violations.
The speaker is informedwhat is unanswerable, or the parser is given instructions on howto reinterpret the input.
(3) Inaccurate parses, where SOUL tellsthe parser any combination f a basic interpretation frame, infer-marion to add, information to delete, or regions to reparse for aspecific meaning or variable.
(4) Unanswerable queries andcommands, which produce a message to the speaker describingwhat carmot be done.EXPERIMENTAL  RESULTSP ILOT STUDIESThe current implementation f SOUL was trained on the firsttwo-thirds of the ATIS0 training data available in June 1990,consisting of Dialogs B0 through B9 and BA through BN.
Thetraining set contained 472 utterances.
SOUL was evaluated on thefollowing three independent, non-overlapping test sets.
Set 1contained the 94 Class A and context-removable utterances fromthe official June 90 ATIS0 test set.
Sets 2 and 3 both usedsentences from Dialogs BO through BZ from the June 1990 data.These were set aside for use as an independent test set.
What isimportant about his data, is that unlike Set 1, and the February1991 official DARPA test set (Set 4, described later), the data arenot restricted in any manner.
All utterances produced by thespeakers are included in the test set, regardless of whether theyare well formed, within the bounds of the domain, ambiguous,context dependent, etc.
Set 2 included all 232 utterances thatwere not context dependent, and therefore contained un-answerable, ambiguous, ill-formed and ungrammatical ut-terances, as well as Class A and context-removable qu ries.
Set 3consisted of the remaining 29 context-dependent uuerances con-tained in the transcripts from Dialogs BO through BZ.Results of the three evaluations, comparing the performance ofPHOENIX alone and PHOENIX plus SOUL are given in the tablebelow.
These results were obtained using the standardDARPA/NIST scoring software.
However, we allowed for avariety of additional error messages which were more specificthan the generic NIST errors.
Results using Test Set 1, indicateSOUL's ability to detect and correct inaccurate and incompleteoutput from the PHOENIX parser, since these sentences consistonly of answerable, legal and non-ambiguous tterances.
Asthese utterances are constrained, it is expected that only minorirnprovments will result for the addition of SOUL.
In contrast,Test Set 2 contains unrestricted input, namely all utterancesgenerated which are interpretable without context.
Results usingTest Set 2 indicate SoUL's ability to recognize unanswerable,derive multiple interpretations for ambiguous input, to interpretill-formed and un-grarnmatical input and to correct inaccurateoutput from the PHOENIX parser.
Finally, results from Test Set 3indicate SoUL's proficiency in detecting context dependent u -terances.
However, it should be noted that the Test Set 3 resultsare not representative of POENIX performance.
PHOENIX isdesigned to process context dependent utterances only whenusing context.109Results from the Three Test SetsTest Set System Correct IncorrectTest PHOENIX 75 19Set 1 PHOENIX+ SOUL 85 9Test PHOE~X 154 74Set 2 PHOEN/X + SOUL 215 13Test PrIO~NIX 0 29Set 3 PHOENix + SOUL 20 9Error Rate Improvement20.20% N/A9.28% 54.1%32.46% N/A5.70% 83.98%100.00% N/A30.00% 70.0%Official February 1991 Results:Inclusive of Interface BugsTest Set System % Correct % Incorrect % No Answer Total ErrorTYPE A PHOENIX 80.7 16.6 2.8 19.3PHOENrX + SOUL 80.7 11.7 7.6 19.3D1 Po~rIX 13 3 60.5 %PaOE~aX + SoUL 114 8 7 70.9 %Query TypeType AType D1!
Error TypeReal (Semantics / Syntax)!
Interface to PhoenixBackend (CAS Field + Dumps)I Real (Semantics / Syntax)Interface to PhoenixI Backend (CAS Field + Dumps)Number % of Total12 8.283 2.0713 8.972 15.794 5.265 13.16RESULTS: FEBRUARY 1991For the February 1991 official evaluation, we modified thePHOENIX and SOUL systems omewhat, so that all the routinesfor processing superlatives, comparatives and yes/no utteranceswere moved into the PHOENIX system.
Therefore, the resultspresented in this section differ from the Test Set #1 in the pilotstudy section above in that all increases in accuracy due toproperly interpreting superlatives, comparatives and yes/no ques-tions are no longer included in the SOUL results.The February 1991 test set contained 148 isolated utterancesand 38 utterances which are interpreted using limited context, orcontext provided by the preceeding query and its answer.
These148 individual utterances were constrained to be "Class A", oranswerable and unambiguous.
On the 38 "DI" or limited contextqueries, ff the interpretation r the database r sponse produced inresponse to the first utterance is inaccurate, the tested utterancewill probably not be correct.
The results of the evaluation arepresented below.A number of interfaces were modified after performing thepilot evaluation and just prior to performing the February 1991evaluation presented above.
As a result, we introduced a numberof errors into the system.
The following table breaks down thesources of error and the percentages attributable to each source.As seen in Table BQ3 of the 19.3% errors on Class A queries,9% are due to back-end bugs and an additional 2% are due tobugs in the interface between the PHOENIX and SOUL modules.Hence, 12 out of the 19.3 % errors are due to back-end typeinterface bugs.For Class D1 queries, of the 34.21% error, 18.42 are due tointerface problems, a number reasonably proportional tothe ClassA results.
Here, 5.26% is found in the PHOENIX SOUL inter-face and 13.16 in the database interface.All in all, the real errors made by the system for Class Aqueries result in roughly an 8% error rate, where for Class D1queries, the error rate is roughly 16%, exactly double they ClassA error rate.
This is to be expected, as an error made on thecontext setting portion of a query will necessarily result in anerror on the context dependent portion of a set of queries.An analysis of the official results including interface bugsand/or of the results excluding interfaces bugs indicates thatSOUL is responsible for a sight decrease in overall error rate.The results also indicate that SOUL is reasonably good at detect-ing and flagging inaccurate interpretations, even when it is notable to produce acorrect interpretation.ii0Official February 1991 Results:Interface Bugs IgnoredTest Set System % Correct % Incorrect % No Answer Total ErrorTYPE A PHOENqX 89.65 7.58 2.76 10.35PHOENIX + SOUL 91.72 4.14 4.14 8.28D1 POENaX 78.95 15.79 5.26 21.05PHOENIX + SOUL 84.21 5.26 10.52 15.78The February 1991 official results modified by only the back-end bugs are presented below.
These data are derived from theoriginal and official complete log of the February 5, 1991 datarun .SUMMARYTo summarize, SOUL was designed to deal with ambiguous,unanswerable, illegal and context removable utterances.
Theapproach taken was to create an extensive semantic and prag-matic knowledge base for use in abductive reasoning and con-straint satisfaction.
The resulting system performs fine grainedanalysis of an input utterance when criteria for activating thepostprocessor is met.
It was hypothesized that the SOUL proces-sor would contribute more significantly in difficult processing /interpretation situations, while the case-frame parser, itselfsemantically based, would be sufficient for more restricted testsets.
This is shown by comparing error rates of PHOENIX aloneand PHOENIX coupled with SOUL across the two conditions ofrestricted and unrestricted utterance sets.
Test Sets 1 and 4(DARPA June 1990 and February 1991) are highly constrained,while Test Sets 2 and 3 are completely unconstrained.
Ashypothesized, the SOUL system contributes significantly more toreducing error rates and enhancing accuracy when applied to themore difficult, unrestaScted data.
When processing unrestrictedinput, as required in real world applications, the,addition of asemantic and pragmatic postprocessor for performing finegrained analyses results in significant improvements in accuracy.However, it would be expected that given a complete dialogand all the context knowledge a system can capitalize upon, oreven a greater amount of context, SOUL would perform betterthan it does with limited context D1 or no-context Class A ut-terances even if they were constrained.The second question posed was whether a knowledge basealone would enable detection of contextually dependent ut-terances where the appficable context is unavailable.
Results ofTest Set 3 indicate reasonable d tection abilities (70%).In summary, semantic and pragmatic knowledge can be effee-fively used to enhance a system's accuracy of interpreta~tion rates.This effect holds even in isolated utterance processing tasks,which provide a great deal less data than can be derived from acomplete dialog.
In the absence of dialog, the accuracy improve-ments are more marked in more difficult processing conditionsthan when processing constrained, relatively straight forward ut-terances.REFERENCESI.
Haupunann, A. G., Young, S. R. and Ward, W. H., "Using DialogLevel Knowledge Sources to Improve Speech Recognition",Proceedings of the Seventh National Conference on ArtificialIntelligence, Morgan Kaufmann, 1988.2.
Young, S. R., Hanptrnmm, A. G. and Ward, W. H., "LayeringPredictions: Flexible Use of Dialog Expectation i  Speech Recog-nition", in \[JCAl-89, Morgan Kaufmann, 1989.3.
Young, S. R., Hauptmann, A. G., Ward, W. H., Smith, E. T. andWemer, P., "High Level Knowledge Sources in Usable SpeechRecognition Systems", Communications of the ACM, Vol.
32, No.2, 1989, pp..4.
Young, S. R., "Use of Dialog, Pragmatics and Semantics toEnhance Speech Recognition", Speech Communication, Vol.9, 1990, pp..5.
Carbonell, J. G. and Hayes, P. L, "Dynamic Strategy Selection iFlexible Parsing", ACL81proc, ACL8 I, 1981.6.
Hayes, P. J. and CarboneU, J. G., "A Natural Language Process-ing Tutorial", Tech.
report, Carnegie-Mellon U iversity, Com-puter Science Department, 1983.7.
Carbonell, J. G. and Hayes, P. J., "Coping with Extragrarn-maficality", Proceedings of COLING-84, Stanford, CA., June1984.8.
Hobbs, J. R., Sfickel, M., Appelt, D. and MaRin, P., "Inter-pretation as Abduction", Tech.
report Technical Note 499, SRIInternational, 1990.9.
Chamiak, E., "Motivation Analysis, Abductive Unification, andNoumonotonic Equality", Artificial Intelligence, Vol.34(3), 1988.10.
Wilensky, R., Planning and Understanding, Addison Wesley,Reading, MA, 1983.I i i
