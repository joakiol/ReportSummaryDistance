Coling 2010: Poster Volume, pages 570?578,Beijing, August 2010Using Syntactic and Semantic based Relations for Dialogue ActRecognitionTina Klu?wer, Hans Uszkoreit, Feiyu XuDeutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (DFKI)Projektbu?ro Berlin{tina.kluewer,uszkoreit,feiyu}@dfki.deAbstractThis paper presents a novel approach todialogue act recognition employing multi-level information features.
In addition tofeatures such as context information andwords in the utterances, the recognitiontask utilizes syntactic and semantic rela-tions acquired by information extractionmethods.
These features are utilized bya Bayesian network classifier for our dia-logue act recognition.
The evaluation re-sults show a clear improvement from theaccuracy of the baseline (only with wordfeatures) with 61.9% to an accuracy of67.4% achieved by the extended featureset.1 IntroductionDialogue act recognition is an essential task fordialogue systems.
Automatic dialogue act clas-sification has received much attention in the pastyears either as an independent task or as an em-bedded component in dialogue systems.
Variousmethods have been tested on different corpora us-ing several dialogue act classes and informationcoming from the user input.The work presented in this paper is part of adialogue system called KomParse (Klu?wer et al,2010), which is an application of a NL dialoguesystem combined with various question answeringtechnologies in a three-dimensional virtual worldnamed Twinity, a web-based online product of theBerlin startup company Metaversum1.
The Kom-Parse NPCs provide various services through con-1http://www.metaversum.com/versation with game users such as selling piecesof furniture to users via text based conversation.The main task of the input interpretation com-ponent of the agent is the detection of the dialogueacts contained in the user utterances.
This classi-fication is done via a cue-based method with var-ious features from multi-level knowledge sourcesextracted from the incoming utterance consideringa small context of the previous dialogue.In contrast to existing systems using mainlylexical features, i.e.
words, single markers such aspunctuation (Verbree et al, ) or combinations ofvarious features (Stolcke et al, 2000) for the dia-logue act classification, the results of the interpre-tation component presented in this paper are basedon syntactic and semantic relations.
The systemfirst gathers linguistic information coming fromdifferent levels of deep linguistic processing sim-ilar to (Allen et al, 2007).
The retrieved informa-tion is used as input for an information extractioncomponent that delivers the relations embedded inthe actual utterance (Xu et al, 2007).
These rela-tions combined with additional features (a smalldialogue context and mood of the sentence) arethen utilized as features for the machine-learningbased recognition.The classifier is trained on a corpus originatingfrom a Wizard-of-Oz experiment which was semi-automatically annotated.
It contains automaticallyannotated syntactic relations namely, predicate ar-gument structures, which were checked and cor-rected manually afterwards.
Furthermore these re-lations are enriched by manual annotation with se-mantic frame information from VerbNet to gain anadditional level of semantic richness.
These tworepresentations of relations, the syntax-based re-570lations and the VerbNet semantic relations, wereused in separate training steps to detect how muchthe classifier can benefit from either notations.A systematic analysis of the data has been con-ducted.
It turns out that a comparatively small setof syntactic relations cover most utterances, whichcan moreover be expressed by an even smaller setof semantic relations.
Because of this observationas well as the overall performance of the classifierthe interpretation is extended with an additionalrule based approach to ensure the robustness ofthe system.The paper is organized as follows: Section 2provides an overview about existing dialogue actrecognition systems and the features they use forclassification.Section 3 introduces the original data used as ba-sis for the annotation and the classification task.In Section 4 the annotation that provides the nec-essary information for the dialogue act classifi-cation and involves the relation extraction is de-scribed in detail.
The annotation is split into threemain steps: The annotation of dialogue informa-tion (section 4.1), the integration of syntactic in-formation (section 4.2) and finally the manual an-notation of VerbNet predicate and role informa-tion in section 4.3.Section 5 presents the results of the actual classifi-cation task using different feature sets and in Sec-tion 6 the results and methods are summarized.Finally, Section 7 provides a brief description ofthe rule-based interpretation and presents an out-look on future work.2 Related WorkDialogue Acts (DAs) represent the functionallevel of a speaker?s utterance, such as a greeting,a request or a statement.
Dialogue acts are ver-bal or nonverbal actions that incorporate partic-ipant?s intentions originating from the theory ofSpeech Acts by Searle and Austin (Searle, 1969).They provide an abstraction from the original in-put by detecting the intended action of an utter-ance, which is not necessarily inferable from thesurface input (see the two requests in the follow-ing example).Can you show me a red car please?Please show me a red car!To detect the action included in an utterance,different approaches have been suggested in re-cent years which can be clustered into two mainclasses: The first class uses AI planning methodsto detect the intention of the utterance based onbelief states of the communicating agents and theworld knowledge.
These systems are often part ofan entire dialogue system e.g.
in a conversationalagent which provides the necessary informationabout current beliefs and goals of the conversa-tion participants at runtime.
One example is theTRIPS system (Allen et al, 1996).
Because of thehuge amount of reasoning, systems in this classgenerally gather as much linguistic information aspossible.The second class uses cues derived from theactual utterance to detect the right dialogue act,mostly using machine learning methods.
Thisclass gained much attention due to less computa-tional costs.
The probabilistic classifications arecarried out via training on labeled examples ofdialogue acts described by different feature sets.Frequently used cues for dialogue acts are lexi-cal features such as the words of the utterance orngrams of words for example in (Verbree et al,), (Zimmermann et al, 2005) or (Webb and Liu,2008).
Although the performance of the classi-fication task is difficult to compare, because ofthe variety of different corpora, dialogue act setsand algorithms used, these approaches do pro-vide considerably good results.
For example (Ver-bree et al, ) achieve accuracy values of 89% onthe ICSI Meeting Corpus containing 80.000 ut-terances with a dialogue act set of 5 distinct di-alogue act classes and amongst others the features?ngrams of words?
and ?ngrams of POS informa-tion?.Another group of systems utilizes acoustic fea-tures derived from Automatic Speech Recognitionfor automatic dialogue act tagging (Surendran andLevow, 2006), context features like the precedingdialogue act or ngrams of previous dialogue acts(Keizer and Akker, 2006).However grammatical and semantic informa-tion is not that often incorporated into feature sets,with the exception of single features such as the571Dialogue Act Meaning FrequencyREQUEST The utterance contains a wish or demand 449REQUEST INFO The utterance contains a wish or demand regarding information 154PROPOSE The utterance serves as suggestion or showing of an object 216ACCEPT The utterance contains an affirmation 167REJECT The utterance contains a rejection 88PROVIDE INFO The utterance provides an information 156ACKNOWLEDGE The utterance is a backchannelling 9Table 1: The used Dialogue Act Settype of verbs or arguments or the presence or ab-sence of special operators e.g.
wh-phrases (An-dernach, 1996).
(Keizer et al, 2002) use amongothers linguistic features like sentence type forclassification with Bayesian networks.
Although(Jurafsky et al, 1998) already noticed a strongcorrelation between selected dialogue acts andspecial grammatical structures, approaches usinggrammatical structure were not very succesful.While grammatical and semantic features arenot often incorporated into dialogue act recogni-tion, they are a commonly used in related fieldslike automatic classification of rhetorical rela-tions.
For example (Sporleder and Lascarides,2008) and (Lapata and Lascarides, 2004) extractverbs as well as their temporal features derivedfrom parsing to infer sentence internal temporaland rhetorical relations.
Their best model foranalysing temporal relations between two clausesachieves 70.7% accuracy.
(Subba and Eugenio,2009) also show a significant improvement of adiscourse relation classifier incorporating compo-sitional semantics compared to a model withoutsemantic features.
Their VerbNet based frame se-mantics yield in a better result of 4.5%.3 The DataThe data serving as the basis for the relation iden-tification as well as the training corpus for the di-alogue act classifier is taken from a Wizard-of-Ozexperiment (Bertomeu and Benz, 2009) in which18 users furnish a virtual living room with the helpof a furniture sales agent.
Users buy pieces of fur-niture and room decoration from the agent by de-scribing their demands and preferences in a textchat.
During the dialogue with the agent, the pre-ferred objects are then selected and directly put tothe right location in the apartment.
In the exper-iments, users spent one hour each on furnishingthe living room by talking to a human wizard con-trolling the virtual sales agent.
The final corpusconsists of 18 dialogues containing 3,171 turnswith 4,313 utterances and 23,015 alpha-numericalstrings (words).
The following example shows atypical part of such a conversation:USR.1: And do we have a little side table for the TV?NPC.1: I could offer you another small table or a side-board.USR.2: Then I?ll take a sideboard that is similar to myshelf.NPC.2: Let me check if we have something like that.Table 2: Example Conversation from the Wizard-of-Oz Experiment4 AnnotationThe annotation of the corpus is carried out in sev-eral steps.4.1 Pragmatic AnnotationThe first annotation step consists of annotatingdiscourse and pragmatic information including di-alogue acts, projects according to (Clark, 1996),sentence mood, the topic of the conversation andan automatically retrieved information state forevery turn of the conversations.
From the anno-tated information the following elements were se-lected as features in the final recognition system:?
The dialogue acts which carry the intentionsof the actual utterance as well as the last pre-ceding dialogue act.
The set used for anno-tation is a domain specific set containing thedialogue acts shown in table 1.?
The sentence mood.
Sentence mood wasannotated with one of the following values:declarative, imperative, interrogative.572?
The topic of the utterance.
The topic valueis coreferent with the currently discussed ob-ject.
Topic can consist of an object class(e.g.
sofa) or an special object instance(sofa 1836).
The topic of the directly pre-ceding utterance was chosen as a feature too.4.2 Annotation with Predicate ArgumentStructureThe second annotation step, applied to the ut-terance level of the input, automatically enrichesthe annotation with predicate argument structures.Each utterance is parsed with a predicate argu-ment parser and annotated with syntactic relationsorganized according to PropBank (Palmer et al,2005) containing the following features: Predi-cate, Subject, Objects, Negation, Modifiers, Cop-ula Complements.A single relation mainly consists of a predi-cate and the belonging arguments.
Verb modi-fiers like attached PPs are classified as ?argM?together with negation (?argM neg?)
and modalverbs (?argM modal?).
Arguments are labeledwith numbers according to the found informationfor the actual structure.
PropBank is organized intwo layers, the first one being an underspecifiedrepresentation of a sentence with numbered argu-ments, the second one containing fine-grained in-formation about the semantic frames for the predi-cate comparable to FrameNet (Baker et al, 1998).While the information in the second layer is sta-ble for each verb, the values of the numbered ar-guments can change from verb to verb.
Whilefor one verb the ?arg0?
may refer to the subjectof the verb, another verb may encapsulate a di-rect object behind the same notation ?arg0?.
Thisis very complicated to handle in a computationalsetup, which needs continuous labeling for thesuccessive components.
Therefore the argumentswere in general named as in PropBank but con-sistently numbered by syntactic structure.
Thismeans for example that the subject is always la-beled as ?arg1?.Consider the example ?Can you put posters orpictures on the wall??.
The syntactic relation willyield in the following representation:<predicate: put><ArgM_modal: can><Arg1: you><Arg2: posters or pictures><ArgM: on the wall>Predicate Argument Structure Parser Thesyntactic predicate argument structure that consti-tutes the syntactic relations and serves as basis forthe VerbNet annotation, is automatically retrievedby a rule-based predicate argument parser.
Therules utilized by the parser describe subtrees of de-pendency structures in XML by means of relevantgrammatical functions.
For detecting verbs withtwo arguments in the input, for instance, a rulecan be written describing the dependency struc-ture for a verb with a subject and an object.
Thisrule would then detect every occurrence of thestructure ?Verb-Subj-Obj?
in a dependency tree.This sample rule would express the following con-straints: The matrix unit should be of the part ofspeech ?Verb?
, The structure belonging to thisverb must contain a ?nsubj?
dependency and an?obj?
dependency.The rules deliver raw predicate argument struc-tures, in which the detected arguments and theverb serve as hooks for further information lookupin the input.
If a verb fulfills all requirementsdescribed by the rule, in a second step all modi-ficational arguments existing in the structure arerecursively acquired.
The same is done formodal arguments as well as modifiers of the ar-guments such as determiners, adjectives or em-bedded prepositions.
After the generation of themain predicate argument structure from the gram-matical functions, the last step inserts the contentvalues present in the actual input into the structureto get the syntactic relations for the utterance.Before the input can be parsed with the predi-cate argument parser, some preprocessing steps ofthe corpus are needed.
These include:Input Cleaning The input data coming from theusers contain many errors.
Some stringsubstitutions as well as the external Googlespellchecker were applied to the input beforeany further processing.Segmentation For clausal separation we apply asimple segmentation via heuristics based onpunctuation.POS Tagging Then the input is processed by573the external part-of-speech tagger TreeTag-ger (Schmid, 1994).The embedded dependency parser is the Stan-ford Dependency Parser (de Marneffe and Man-ning, 2008), but other dependency parsers couldbe employed instead.
The predicate argumentparser is an standalone software and can be usedeither as a system component or for batch process-ing of a text corpus.4.3 VerbNet Frame AnnotationThe last step of annotation consists of the man-ual annotation of semantic predicate classes andsemantic roles.
Moreover, the automatically de-termined syntactic relations are checked and cor-rected if possible.
VerbNet (Schuler, 2005) is uti-lized as a source for semantic information.
TheVerbNet role set consists of 21 general roles usedin all VerbNet classes.
Examples of roles inthis general role set are ?agent?, ?patient?
and?theme?.For the manual addition of the semantic frameinformation a web-based annotation tool has beendeveloped.
The annotation tool shows the utter-ance which should be annotated in the context ofthe dialogue including the information from thepreceding annotation steps.
All VerbNet classescontaining the current predicate are listed as pos-sibilities for the predicate classification togetherwith their syntactic frames.
The annotators can se-lect the appropriate predicate class and frame ac-cording to the arguments found in the utterance.If an argument is missing in the input that is re-quired in the selected frame a null argument isadded to the structure.
If the right predicate classis existing, but the predicate is not yet a memberof the class, it is added to the VerbNet files.
Incase the right predicate class is found but the fit-ting frame is missing, the frame is added to theVerbNet files.
Thus during annotation 35 newmembers have been added to the existing VerbNetclasses, 4 Frames and 4 new subclasses.
Via thesemodifications, a version of VerbNet has been de-veloped that can be regarded as a domain-specificVerbNet for the sales domain.During the predicate classification, the annota-tors also assign the appropriate semantic roles tothe arguments belonging to the selected predicate.The semantic roles are taken from the selectedVerbNet frame.From the annotated semantic structure, seman-tic relations are inferred such as the one in the fol-lowing example:<predicate: put-3.1><agent: you><theme: posters or pictures><destination: on the wall>5 Dialogue Act RecognitionTwo datasets are derived from the corpus: Thedataset containing the utterances of the users(CST) and one dataset containing the utterancesof the wizard (NPC), whereas the NPC corpus iscleaned from the ?protocol sentences?.
Protocolsentences are canned sentences the wizard usedin every conversation, for example to initializethe dialogue.
For the experiments, the two sin-gle datasets ?NPC?
and ?CST?
as well as a com-bined dataset called ?ALL?
are used.
Unfortu-nately from the original 4,313 utterances in total,many utterances could not be used for the final ex-periments.
First, fragments are removed and onlythe utterances found by the parser to contain avalid predicate argument structure are used.
Afterprotocol sentences are taken out too, a dataset of1702 valid utterances remains.
Moreover, 292 ut-terances are annotated to contain no valid dialogueact and are therefore not suitable for the recogni-tion task.
Of the remaining utterances, 171 predi-cate argument structures were annotated as wrongbecause of completely ungrammatical input.
Inthis way we arrive at a dataset of 804 instances forthe users and 435 for the wizard, summing up to1239 instances in total.The features used for dialogue act recognitionexploit the information extracted from the differ-ent annotation steps:?
Context features: The last preceding dia-logue act, equality between the last preced-ing topic and the actual topic, sentence mood?
Syntactic relation features: Syntactic predi-cate class, arguments, negation?
VerbNet semantic relation features: VerbNetpredicate class, VerbNet frame arguments,negation574?
Utterance features: The original utteranceswithout any modificationsDifferent sets of features for training and eval-uation are generated from these:DATASET Syn: All utterances of the specifieddataset described via syntactic relation andcontext features.DATASET VNSem: All utterances of the speci-fied dataset described via VerbNet semanticrelations and context features.DATASET Syn Only: All utterances of thespecified dataset only described via thesyntactic relations.DATASET VNSem Only: All utterances of thespecified dataset only described via the Verb-Net semantic relations.DATASET Context Only: All utterances of thespecified dataset described via the contextfeatures and negation without any informa-tion regarding relations.DATASET Utterances Context: The utterancesof the specified dataset as strings combinedwith the whole set of context features withoutfurther relation extraction results.DATASET Utterances: Only the utterances ofthe specified dataset as strings.
This and thelast ?Utterances?-set serve as baselines.Dialogue Act Recognition is carried out viathe Bayesian network classifier AOEDsr from theWEKA toolkit.
AODEsr augments AODE, analgorithm averaging over all of a small spaceof alternative naive-Bayes-like models that haveweaker independence assumptions than naiveBayes, with Subsumption Resolution (Zheng andWebb, 2006).
Evaluation is performed usingcrossfolded evaluation.All results of the experiments are given in termsof accuracy.Results for the dataset ?All?
comparing the syn-tactic relations with VerbNet relations as well asthe pure utterances and context are shown in table4.Dataset AccuracyAll Syn 67.4%All VNSem 66.8%All Utterances Context 61.9%All Utterances 48.1%Table 4: Dialogue Act Classification Results forthe ?ALL?
DatasetsThe best result is achieved with the syntactic in-formation, although the VerbNet information pro-vides an abstraction over the predicate classifica-tion.
Both the set containing the VerbNet relationsas well as the syntactic relations are much betterthan the set containing only the context and theoriginal utterances.
The dataset containing onlythe utterances could not reach 50%.Although the experiments show much better re-sults using the relations instead of the original ut-terance, the overall accuracy is not very satisfying.Several reasons for this phenomenon come intoconsideration.
While it can to a certain extend bethe fault of the classifying algorithm (see table 8for some tests with a ROCCHIO based classifier),the main reason might as well lie in the impre-cise boundaries of the dialogue act classes: Sev-eral categories are hard to distinguish even for ahuman annotator as you can see from the wronglyclassified examples in table 3.
Another possibil-ity can be the comparatively small number of totaltraining instances.For the NPC dataset the results are slightly bet-ter and much better still for the set CST, whichis due to a smaller number (6) of dialogue acts:The dialogue act ?PROPOSE?, which is the actfor showing an object or proposing a possibility,was not used by any user, but only by the wizard.Dataset AccuracyCST Syn 73.1%NPC Syn 68.5%Table 5: Dialogue Act Classification Results forDatasets ?CST?
and ?NPC?To find out if one sort of features is espe-cially important for the classification we reorga-575Utterance Right Classification Classified AsWhat do you think about this one?
request info proposeLet see what you have and where we can put it request info requestTable 3: Wrongly classified instancesnize the training sets to contain only the contextfeatures without the relations (All Context Only)on the one hand and only the relational informa-tion without the context features on the other hand(All Syn Only and All VNSem Only).
Resultsare shown in table 6.Dataset AccuracyAll Context Only 56.6%All VNSem Only 53.5%All Syn Only 50.8%Table 6: Dialogue Act Classification Results forContext and Relation setsTable 6 shows that the results are considerablyworse if only parts of the features are used.
Theset with context feature performs 3,1% better thanthe best set with the relations only.
Furthermorethe VerbNet semantic relation set leads to nearly3% better accuracy, which may mean that the ab-straction of semantic predicates provides a bettermapping to dialogue acts after all if used withoutfurther features which may be ranked more impor-tant by the classifier.Besides the experiments with the Bayesian net-works, additional experiments are performed us-ing a modified ROCCHIO algorithm similar to theone in (Neumann and Schmeier, 2002).
Three dif-ferent datasets were tested (see table 7).Dataset AccuracyAll Utterances 70.1%All Utterances Context 73.2%All Syn 74.4%Table 8: Dialogue Act Classification Results usingthe ROCCHIO AlgorithmTable 8 shows that the baseline dataset contain-ing only the utterances already provides much bet-ter results with the ROCCHIO algorithm, deliv-ering 70.1% which is more than 10% more ac-curacy compared to the 48.1% of the Bayesianclassifier.
If tested together with the context fea-tures the accuracy of the utterance dataset raises to73.2% and, after including the relational informa-tion, even to 74.4%.
Thus, the results of this ROC-CHIO experiment also prove that the employmentof the relation information leads to improved ac-curacy of the classification.6 ConclusionThis paper reports on a novel approach to auto-matic dialogue act recognition using syntactic andsemantic relations as new features instead of thetraditional features such as ngrams of words.Different feature sets are constructed via anautomatic annotation of syntactic predicate argu-ment structures and a manual annotation of Verb-Net frame information.
On the basis of this infor-mation, both the syntactic relations as well as thesemantic VerbNet-based relations included in theutterances can be extracted and added to the fea-ture sets for the recognition task.
Besides the re-lation information the employed features includeinformation from the dialogue context (e.g.
thelast preceding dialogue act) and other features likesentence mood.The feature sets have been evaluated with aBayesian network classifier as well as a ROC-CHIO algorithm.
Both classifiers demonstrate thebenefits gained from the relations by exploitingthe additionally provided information.
While thedifference between the best baseline feature setand the best relation feature set in the Bayesiannetwork classifier yields a 5,5% boost in accuracy(61.9% to 67.4%), the ROCCHIO setup exceedsthe boosted accuracy by another 1,5% , startingfrom a higher baseline of 73.2%.
Based on theobserved complexity of the classification task weexpect that the benefit of the relational informa-576Predicate Instances Examplesee-30.1 59 I would like to see a table in front of the sofaput-9.1 74 Can you put it in the corner?reflexive appearance-48.1.2 80 Show me the red oneown-100 137 Do you have wooden chairs?want-32.1 153 I would like some plants over hereTable 7: The Main Semantic Relations Found in the Data Sorted by Predicatetion may turn out to be even more significant onlarger learning data.7 Future WorkThe results in section 5 show that the pure classifi-cation cannot be used as interpretation componentin isolation, but additional methods have to be in-corporated.
In a preceding analysis of the datait was found that certain predicates are very fre-quently uttered by the users.
In the syntactic pred-icate scenario the total number of different predi-cates is 80, whereas the semantic predicates buildup a total number of 66.
The class containing thepredicates with one to ten occurrences constitutes137 of 1239 instances.
The remaining 1101 in-stances are covered by only 21 different predicateclasses.
These predicates together with their ar-guments constitute a set of common domain re-lations for the sales domain.
The main domainrelations found are shown in table 7.The figures suggest that the interpretation atleast for the domain relations can be established ina robust manner, wherefore the agent?s interpreta-tion component was extended to a hybrid moduleincluding a robust rule based method.
To derivethe necessary rules a rule generator was developedand the rules covering the used feature set (includ-ing the context features, sentence mood and thesyntactic relations) were automatically generatedfrom the given data.Future work will focus on the evaluation ofthese automatically derived rules on a recentlycollected but not yet annotated dataset from a sec-ond Wizard-of-Oz experiment, carried out in thesame furniture sales setting.Additional experiments are planned for evalu-ating the relation-based features in dialogue actrecognition on other corpora tagged with differ-ent dialogue acts in order to test the overall per-formance of our classification approach on moretransparent dialogue act sets.AcknowledgementsThe work described in this paper was partiallysupported through the project ?KomParse?
fundedby the ProFIT program of the Federal State ofBerlin, co-funded by the EFRE program of theEuropean Union.
Additional support came fromthe project TAKE, funded by the German Min-istry for Education and Research (BMBF, FKZ:01IW08003).577ReferencesAllen, James F., Bradford W. Miller, Eric K. Ringger,and Teresa Sikorski.
1996.
A robust system for nat-ural spoken dialogue.
In Proceedings of ACL 1996.Allen, James, Mehdi Manshadi, Myroslava Dzikovska,and Mary Swift.
2007.
Deep linguistic processingfor spoken dialogue systems.
In DeepLP ?07: Pro-ceedings of the Workshop on Deep Linguistic Pro-cessing, Morristown, NJ, USA.Andernach, Toine.
1996.
A machine learning ap-proach to the classification of dialogue utterances.CoRR, cmp-lg/9607022.Baker, Collin F., Charles J. Fillmore, and John B.Lowe.
1998.
The berkeley framenet project.
InProceedings of COLING 1998.Bertomeu, Nuria and Anton Benz.
2009.
Annotationof joint projects and information states in human-npc dialogues.
In Proceedings of CILC-09, Murcia,Spain.Clark, H.H.
1996.
Using Language.
Cambridge Uni-versity Press.de Marneffe, Marie C. and Christopher D. Manning.2008.
The Stanford typed dependencies repre-sentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, Manchester, UK.Jurafsky, Daniel, Elizabeth Shriberg, Barbara Fox, andTraci Curl.
1998.
Lexical, prosodic, and syntacticcues for dialog acts.Keizer, Simon and Rieks op den Akker.
2006.Dialogue act recognition under uncertainty usingbayesian networks.
Nat.
Lang.
Eng., 13(4).Keizer, Simon, Rieks op den Akker, and Anton Nijholt.2002.
Dialogue act recognition with bayesian net-works for dutch dialogues.
In Proceedings of the3rd SIGdial workshop on Discourse and dialogue,Morristown, NJ, USA.Klu?wer, Tina, Peter Adolphs, Feiyu Xu, Hans Uszko-reit, and Xiwen Cheng.
2010.
Talking npcs in avirtual game world.
In Proceedings of the SystemDemonstrations Section at ACL 2010.Lapata, Mirella and Alex Lascarides.
2004.
Inferringsentence-internal temporal relations.
In Proceed-ings of the North American Chapter of the Associa-tion for Computational Linguistics, pages 153?160.Neumann, Gu?nter and Sven Schmeier.
2002.
Shal-low natural language technology and text mining.Ku?nstliche Intelligenz.
The German Artificial Intel-ligence Journal.Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpusof semantic roles.
Comput.
Linguist., 31(1).Schmid, Helmut.
1994.
In Proceedings of the Inter-national Conference on New Methods in LanguageProcessing.Schuler, Karin Kipper.
2005.
Verbnet: a broad-coverage, comprehensive verb lexicon.
Ph.D. the-sis, Philadelphia, PA, USA.Searle, John R. 1969.
Speech acts : an essay inthe philosophy of language / John R. Searle.
Cam-bridge University Press, London.Sporleder, Caroline and Alex Lascarides.
2008.
Usingautomatically labelled examples to classify rhetori-cal relations: A critical assessment.
Natural Lan-guage Engineering, 14(3).Stolcke, Andreas, Klaus Ries, Noah Coccaro, Eliza-beth Shriberg, Rebecca Bates, Daniel Jurafsky, PaulTaylor, Rachel Martin, Carol Van, and Ess dykemaMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26.Subba, Rajen and Barbara Di Eugenio.
2009.
An ef-fective discourse parser that uses rich linguistic in-formation.
In NAACL ?09, Morristown, NJ, USA.Surendran, Dinoj and Gina-Anne Levow.
2006.
Di-alog act tagging with support vector machines andhidden markov models.
In Interspeech.Verbree, A.T., R.J. Rienks, and D.K.J.
Heylen.Dialogue-act tagging using smart feature selection:results on multiple corpora.
In Raorke, B., editor,First International IEEE Workshop on Spoken Lan-guage Technology SLT 2006.Webb, Nick and Ting Liu.
2008.
Investigating theportability of corpus-derived cue phrases for dia-logue act classification.
In Proceedings of COLING2008, Manchester, UK.Xu, Feiyu, Hans Uszkoreit, and Hong Li.
2007.
Aseed-driven bottom-up machine learning frameworkfor extracting relations of various complexity.
InProceedings of ACl (07), Prague, Czech Republic.Zheng, Fei and Geoffrey I. Webb.
2006.
Efficientlazy elimination for averaged one-dependence esti-mators.
In ICML, pages 1113?1120.Zimmermann, Matthias, Yang Liu, Elizabeth Shriberg,and Andreas Stolcke.
2005.
Toward joint seg-mentation and classification of dialog acts in mul-tiparty meetings.
In Proc.
Multimodal Interactionand Related Machine Learning Algorithms Work-shop (MLMI05, page 187.578
