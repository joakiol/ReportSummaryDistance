Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 88?97,Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational LinguisticsTowards an ACL Anthology Corpus with Logical Document StructureAn Overview of the ACL 2012 Contributed TaskUlrich Sch?ferDFKI Language Technology LabCampus D 3 1D-66123 Saarbr?cken, Germanyulrich.schaefer@dfki.deJonathon Read, Stephan OepenDepartment of InformaticsUniversitetet i Oslo0316 Oslo, Norway{jread |oe}@ifi.uio.noAbstractThe ACL 2012 Contributed Task is a com-munity effort aiming to provide the full ACLAnthology as a high-quality corpus with richmarkup, following the TEI P5 guidelines?a new resource dubbed the ACL AnthologyCorpus (AAC).
The goal of the task is three-fold: (a) to provide a shared resource for ex-perimentation on scientific text; (b) to serveas a basis for advanced search over the ACLAnthology, based on textual content and cita-tions; and, by combining the aforementionedgoals, (c) to present a showcase of the benefitsof natural language processing to a broader au-dience.
The Contributed Task extends the cur-rent Anthology Reference Corpus (ARC) bothin size, quality, and by aiming to provide toolsthat allow the corpus to be automatically ex-tended with new content?be they scanned orborn-digital.1 Introduction?MotivationThe collection of the Association for ComputationalLinguistics (ACL) Anthology began in 2002, with3,100 scanned and born-digital1 PDF papers.
Sincethen, the ACL Anthology has become the open ac-cess collection2 of scientific papers in the area ofComputational Linguistics and Language Technol-ogy.
It contains conference and workshop proceed-ings and the journal Computational Linguistics (for-merly the American Journal of Computational Lin-guistics).
As of Spring 2012, the ACL Anthol-1The term born-digital means natively digital, i.e.
preparedelectronically using typesetting systems like LATEX, OpenOffice,and the like?as opposed to digitized (or scanned) documents.2http://aclweb.org/anthologyogy comprises approximately 23,000 papers from 46years.Bird et al (2008) started collecting not only thePDF documents, but also providing the textual con-tent of the Anthology as a corpus, the ACL Anthol-ogy Reference Corpus3 (ACL-ARC).
This text ver-sion was generated fully automatically and in differ-ent formats (see Section 2.2 below), using off-the-shelf tools and yielding somewhat variable quality.The main goal was to provide a reference cor-pus with fixed releases that researchers could useand refer to for comparison.
In addition, the visionwas formulated that manually corrected ground-truth subsets could be compiled.
This is accom-plished so far for citation links from paper to paperinside the Anthology for a controlled subset.
Thefocus thus was laid on bibliographic and bibliomet-ric research and resulted in the ACL Anthology Net-work (Radev et al, 2009) as a public, manually cor-rected citation database.What is currently missing is an easy-to-processXML variant that contains high-quality running textand logical markup from the layout, such as sectionheadings, captions, footnotes, italics etc.
In prin-ciple this could be derived from LATEX source files,but unfortunately, these are not available, and fur-thermore a considerable amount of papers have beentypeset with various other word processing software.Here is where the ACL 2012 Contributed Taskstarts: The idea is to combine OCR and PDFBox-like born-digital text extraction methods and re-assign font and logical structure information as partof a rich XML format.
The method would rely onOCR exclusively only in cases where no born-digital3http://acl-arc.comp.nus.edu.sg88PDFs are available?in case of the ACL Anthologymostly papers published before the year 2000.
Cur-rent results and status updates will always be acces-sible through the following address:http://www.delph-in.net/aac/We note that manually annotating the ACL An-thology is not viable.
In a feasibility study we tooka set of five eight-page papers.
After extractingthe text using PDFBox4 we manually corrected theoutput and annotated it with basic document struc-ture and cross-references; this took 16 person-hours,which would suggest a rough estimate of some 25person-years to manually correct and annotate thecurrent ACL Anthology.
Furthermore, the ACL An-thology grows substantially every year, requiring asustained effort.2 State of Affairs to DateIn the following, we briefly review the current statusof the ACL Anthology and some of its derivatives.2.1 ACL AnthologyPapers in the current Anthology are in PDF format,either as scanned bitmaps or digitally typeset withLATEX or word processing software.
Older scannedpapers were often created using type writers, andsometimes even contained hand-drawn graphics.2.2 Anthology Reference Corpus (ACL-ARC)In addition to the PDF documents, the ACL-ARCalso contains (per page and per paper)?
bitmap files (in the PNG file format)?
plain text in ?normal?
reading order?
formatted text (in two columns for most of thepapers)?
XML raw layout format containing position in-formation for each word, grouped in lines, withfont information, but no running text variant.The latter three have been generated using OCRsoftware (OmniPage) operating on the bitmap files.4http://pdfbox.apache.orgHowever, OCR methods tend to introduce charac-ter and layout recognition errors, from both scannedand born-digital documents.The born-digital subset of the ACL-ARC (mostlypapers that appeared in 2000 or later) also containsPDFBox plain text output.
However, this is notavailable for approximately 4% of the born-digitalPDFs due to unusual font encodings.
Note though,that extracting text from PDFs in normal readingorder is not a trivial task (Berg et al, 2012), andmany errors exist.
Furthermore, the plain text isnot dehyphenated, necessitating a language modelor lexicon-based lookup for post-processing.2.3 ACL Anthology NetworkThe ACL Anthology Network (Radev et al, 2009)is based on the ACL-ARC text outputs.
It addition-ally contains manually-corrected citation graphs, au-thor and affiliation data for most of the Anthology(papers until 2009).2.4 Publications with the ACL Anthology as aCorpusWe did a little survey in the ACL Anthology of pa-pers reporting on having used the ACL Anthology ascorpus/dataset.
The aim here is to get an overviewand distribution of the different NLP research tasksthat have been pursued using the ACL Anthology asdataset.
There are probably other papers outside theAnthology itself, but these have not been looked at.The pioneers working with the Anthology as cor-pus are Ritchie et al (2006a, 2006b).
They did workrelated to citations which also forms the largest topiccluster of papers applying or using Anthology data.Later papers on citation analysis, summarization,classification, etc.
are Qazvinian et al (2010), Abu-Jbara & Radev (2011), Qazvinian & Radev (2010),Qazvinian & Radev (2008), Mohammad et al(2009), Athar (2011), Sch?fer & Kasterka (2010),and Dong & Sch?fer (2011).Text summarization research is performed inQazvinian & Radev (2011) and Agarwal et al(2011a, 2011b).The HOO (?Help our own?)
text correction sharedtask (Dale & Kilgarriff, 2010; Zesch, 2011; Ro-zovskaya et al, 2011; Dahlmeier et al, 2011) aimsat developing automated tools and techniques that89assist authors, e.g.
non-native speakers of English,in writing (better) scientific publications.Classification/Clustering related publications areMuthukrishnan et al (2011) and Mao et al (2010).Keyword extraction and topic models based onAnthology data are addressed in Johri et al (2011),Johri et al (2010), Gupta & Manning (2011), Hallet al (2008), Tu et al (2010) and Daudaravic?ius(2012).
Reiplinger et al (2012) use the ACL An-thology to acquire and refine extraction patterns forthe identification of glossary sentences.In this workshop several authors have used theACL Anthology to analyze the history of compu-tational linguistics.
Radev & Abu-Jbara (2012) ex-amine research trends through the citing sentencesin the ACL Anthology Network.
Anderson et al(2012) use the ACL Anthology to perform a people-centered analysis of the history of computationallinguistics, tracking authors over topical subfields,identifying epochs and analyzing the evolution ofsubfields.
Sim et al (2012) use a citation analysis toidentify the changing factions within the field.
Vo-gel & Jurafsky (2012) use topic models to explorethe research topics of men and women in the ACLAnthology Network.
Gupta & Rosso (2012) lookfor evidence of text reuse in the ACL Anthology.Most of these and related works would benefitfrom section (heading) information, and partly theapproaches already used ad hoc solutions to gatherthis information from the existing plain text ver-sions.
Rich text markup (e.g.
italics, tables) couldalso be used for linguistic, multilingual example ex-traction in the spirit of the ODIN project (Xia &Lewis, 2008; Xia et al, 2009).3 Target Text EncodingTo select encoding elements we adopt the TEI P5Guidelines (TEI Consortium, 2012).
The TEI en-coding scheme was developed with the intention ofbeing applicable to all types of natural language, andfacilitating the exchange of textual data among re-searchers across discipline.
The guidelines are im-plemented in XML; we currently use inline markup,but stand-off annotations have also been applied(Ban?ski & Przepi?rkowski, 2009).We use a subset of the TEI P5 Guidelines asnot all elements were deemed necessary.
This pro-cess was made easier through Roma5, an onlinetool that assists in the development of TEI valida-tors.
We note that, while we initially use a simpli-fied version, the schemas are readily extensible.
Forinstance, Przepi?rkowski (2009) demonstrates howconstituent and dependency information can be en-coded following the guidelines, in a manner whichis similar to other prominent standards.A TEI corpus is typically encoded as a sin-gle XML document, with several text elements,which in turn contain front (for abstracts), bodyand back elements (for acknowledgements and bib-liographies).
Then, sections are encoded using divelements (with xml:ids), which contain a heading(head) and are divided into paragraphs (p).
Weaim for accountability when translating between for-mats; for example, the del element records deletions(such as dehyphenation at line breaks).An example of a TEI version of an ACL Anthol-ogy paper is depicted in Figure 1 on the next page.4 An Overview of the Contributed TaskThe goal of the ACL 2012 Contributed Task is toprovide a high-quality version of the textual contentof the ACL Anthology as a corpus.
Its rich textXML markup will contain information on logicaldocument structure such as section headings, foot-notes, table and figure captions, bibliographic ref-erences, italics/emphasized text portions, non-latinscripts, etc.The initial source are the PDF documents of theAnthology, processed with different text extractionmethods and tools that output XML/HTML.
The in-put to the task itself then consists of two XML for-mats:?
PaperXML from the ACL Anthology Search-bench6 (Sch?fer et al, 2011) providedby DFKI Saarbr?cken, of all approximately22,500 papers currently in the Anthology (ex-cept ROCLING which are mostly in Chi-nese).
These were obtained by running a com-mercial OCR program and applying logicalmarkup postprocessing and conversion to XML(Sch?fer & Weitz, 2012).5http://www.tei-c.org/Roma/6http://aclasb.dfki.de90<?xml version="1.0" encoding="UTF-8"?><TEI xmlns="http://www.tei-c.org/ns/1.0"xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"xsi:schemaLocation="http://www.tei-c.org/ns/1.0 aclarc.tei.xsd" xml:lang="en"><teiHeader><fileDesc><titleStmt><title>Task-oriented Evaluation of Syntactic Parsers and Their Representations</title><author>Yusuke Miyao?
Rune S?tre?
Kenji Sagae?
Takuya Matsuzaki?
Jun?ichi Tsujii??
*?Department of Computer Science, University of Tokyo, Japan?School of Computer Science, University of Manchester, UKNational Center for Text Mining, UK{yusuke,rune.saetre,sagae,matuzaki,tsujii}@is.s.u-tokyo.ac.jp</author></titleStmt><publicationStmt><publisher>Association for Computational Linguistics</publisher><pubPlace> Columbus, Ohio, USA</pubPlace><date>June 2008</date></publicationStmt><sourceDesc> [.
.
. ]
</sourceDesc></fileDesc><encodingDesc> [.
.
. ]
</encodingDesc></teiHeader><text><front><div type="abs"><head>Abstract</head><p> [.
.
. ]
</p></div></front><body><div xml:id="SE1"><head>Introduction</head><p>Parsing technologies have improved considerably inthe past few years, and high-performance syntacticparsers are no longer limited to PCFG-based frame<del type="lb">-</del>works (<ref target="#BI6">Charniak, 2000</ref>;[.
.
.
]</p></div></body><back><div type="ack"><head>Acknowledgements</head><p> [.
.
. ]
</p></div><div type="bib"><head>References</head><listBibl><bibl xml:id="BI1">D.
M. Bikel.
2004.
Intricacies of Collins?
parsing model.<hi rend="italic">Computational Linguistics</hi>, 30(4):479?511.</bibl>[.
.
.
]</listBibl><pb n="54"/></div></back></text></TEI>Figure 1: An example of a TEI-compliant version of an ACL Anthology document P08-1006.
Some elements aretruncated ([.
.
. ])
for brevity.91?
TEI P5 XML generated by PDFExtract.
For pa-pers from after 1999, an additional high-qualityextraction step took place, applying state-ofthe art word boundary and layout recognitionmethods directly to the native, logical PDFstructure (Berg et al, 2012).
As no charac-ter recognition errors occur, this will form themaster format for textual content if available.Because both versions are not perfect, a large, ini-tial part of the Contributed Task requires automat-ically adding missing or correcting markup, usinginformation from OCR where necessary (e.g.
for ta-bles).
Hence, for most papers from after 1999 (cur-rently approx.
70% of the papers), the ContributedTask can make use of both representations simulta-neously.The role of paperXML in the Contributed Task isto serve as fall-back source (1) for older, scannedpapers (mostly published before the year 2000), forwhich born-digital PDF sources are not available,or (2) for born-digital PDF papers on which thePDFExtract method failed, or (3) for document partswhere PDFExtract does not output useful markupsuch as currently for tables, cf.
Section 4.2 below.A big advantage of PDFExtract is its ability to ex-tract the full Unicode character range without char-acter recognition errors, while the OCR-based ex-traction methods in our setup are basically limitedto Latin1 characters to avoid higher recognition er-ror rates.We proposed the following eight areas as possiblesubtasks towards our goal.4.1 Subtask 1: FootnotesThe first task addresses identification of footnotes,assigning footnote numbers and text, and generatingmarkup for them in TEI P5 style.
For example:We first determine lexical heads of nonterminalnodes by using Bikel's implementation ofCollins' head detection algorithm<note place="foot" n="9"><hi rend="monospace">http://www.cis.upenn.edu/~dbikel/software.html</hi></note>(<ref target="#BI1">Bikel, 2004</ref>;<ref target="#BI11">Collins, 1997</ref>).Footnotes are handled to some extent in PDFEx-tract and paperXML, but the results require refine-ment.4.2 Subtask 2: TablesTask 2 identifies figure/table references in runningtext and links them to their captions.
The latterwill also have to be distinguished from running text.Furthermore, tables will have to be identified andtransformed into HTML style table markup.
Thisis currently not generated by PDFExtract, but theOCR tool used for paperXML generation quite re-liably recognizes tables and transforms tables intoHTML.
Thus, a preliminary solution would be to in-sert missing table content in PDFExtract output fromthe OCR results.
In the long run, implementing tablehandling in PDFExtract would be desirable.<ref target="#TA3">Table 3</ref> shows thetime for parsing the entire AImed corpus,...<figure xml:id="TA3"><head>Table 3: Parsing time (sec.
)</head><!-- TEI table content markup here --></figure>4.3 Subtask 3: Bibliographic MarkupThe purpose of this task is to identify citations intext and link them to the bibliographic referenceslisted at the end of each paper.
In TEI markup, bibli-ographies are contained in listBibl elements.
Thecontents of listBibl can range from formatted textto moderately-structured entries (biblStruct) andfully-structured entries (biblFull).
For example:We follow the PPI extraction method of<ref target="#BI39">S?tre et al (2007)</ref>,which is based on SVMs ...<div type="bib"><head>References</head><listBibl><bibl xml:id="BI39">R.
S?tre, K. Sagae, and J. Tsujii.
2007.Syntactic features for protein-proteininteraction extraction.
In<hi rend="italic">LBM 2007 short papers</hi>.</bibl></listBibl></div>A citation extraction and linking tool that isknown to deliver good results on ACL Anthologypapers (and even comes with CRF models trainedon this corpus) is ParsCit (Councill et al, 2008).
Inthis volume, Nhat & Bysani (2012) provide an im-plementation for this task using ParsCit and discusspossible further improvements.924.4 Subtask 4: De-hyphenationBoth paperXML and PDFExtract output contain softhyphenation indicators at places where the originalpaper contained a line break with hyphenation.
InpaperXML, they are represented by the Unicode softhyphen character (in contrast to normal dashes thatalso occur).
PDFExtract marks hyphenation fromthe original text using a special element.
How-ever, both tools make errors: In some cases, the hy-phens are in fact hard hyphens.
The idea of thistask is to combine both sources and possibly ad-ditional information, as in general the OCR pro-gram used for paperXML more aggressively pro-poses de-hyphenation than PDFExtract.
Hyphen-ation in names often persists in paperXML andtherefore remains a problem that will have to be ad-dressed as well.
For example:In this paper, we present a comparativeeval<del type="lb">-</del>uation of syntacticparsers and their outputrepresen<del type="lb">-</del>tations based ondifferent frameworks:4.5 Subtask 5: Remove Garbage such asLeftovers from FiguresIn both paperXML and PDFExtract output, textremains from figures, illustrations and diagrams.This occurs more frequently in paperXML than inPDFExtract output because text in bitmap figuresundergoes OCR as well.
The goal of this subtaskis to recognize and remove such text.Bitmaps in born-digital PDFs are embedded ob-jects for PDFExtract and thus can be detected andencoded within TEI P5 markup and ignored in thetext extraction process:<figure xml:id="FI3"><graphic url="P08-1006/FI3.png" /><head>Figure 3: Predicate argument structure</head></figure>4.6 Subtask 6: Generate TEI P5 Markup forScanned Papers from paperXMLDue to the nature of the extraction process, PDFEx-tract output is not available for older, scanned pa-pers.
These are mostly papers from before 2000, butalso e.g.
EACL 2003 papers.
On the other hand, pa-perXML versions exist for almost all papers of theACL Anthology, generated from OCR output.
Theystill need to be transformed to TEI P5, e.g.
usingXSLT.
The paperXML format and transformation toTEI P5 is discussed in Sch?fer & Weitz (2012) inthis volume.4.7 Subtask 7: Add Sentence Splitting MarkupHaving a standard for sentence splitting with uniquesentence IDs per paper to which everyone can referto later could be important.
The aim of this task is toadd sentence segmentation to the target markup.
Itshould be based on an open source tokenizer such asJTok, a customizable open source tool7 that was alsoused for the ACL Anthology Searchbench semanticindex pre-processing, or the Stanford Tokenizer8.<p><s>PPI extraction is an NLP task to identifyprotein pairs that are mentioned as interactingin biomedical papers.</s> <s>Because the numberof biomedical papers is growing rapidly, it isimpossible for biomedical researchers to readall papers relevant to their research; thus,there is an emerging need for reliable IEtechnologies, such as PPI identification.</s></p>4.8 Subtask 8: Math FormulaeMany papers in the Computational Linguistics area,especially those dealing with statistical natural lan-guage processing, contain mathematical formulae.Neither paperXML nor PDFExtract currently pro-vide a means to deal with these.A math formula recognition is a complex task, in-serting MathML9 formula markup from an externaltool (formula OCR, e.g.
from InftyReader10) couldbe a viable solution.For example, the following could become the tar-get format of MathML embedded in TEI P5, for??
> 0 3 f (x) < 1:<mrow><mo> there exists </mo><mrow><mrow><mi> &#916; <!--GREEK SMALL DELTA--></mi><mo> &gt; </mo><mn> 0 </mn>7http://heartofgold.opendfki.de/repos/trunk/jtok; LPGL license8http://nlp.stanford.edu/software/tokenizer.shtml; GPL V2 license9http://www.w3.org/TR/MathML/10http://sciaccess.net/en/InftyReader/93</mrow><mo> such that </mo><mrow><mrow><mi> f </mi><mo> &#2061; <!--FUNCTION APPL.--></mo><mrow><mo> ( </mo><mi> x </mi><mo> ) </mo></mrow></mrow><mo> &lt; </mo><mn> 1 </mn></mrow></mrow></mrow>An alternative way would be to implement mathformula recognition directly in PDFExtract usingmethods known from math OCR, similar to the pagelayout recognition approach.5 Discussion?OutlookThrough the ACL 2012 Contributed Task, we havetaken a (small, some might say) step further towardsthe goal of a high-quality, rich-text version of theACL Anthology as a corpus?making available boththe original text and logical document structure.Although many of the subtasks sketched abovedid not find volunteers in this round, the ContributedTask, in our view, is an on-going, long-term com-munity endeavor.
Results to date, if nothing else,confirm the general suitability of (a) using TEI P5markup as a shared target representation and (b) ex-ploiting the complementarity of OCR-based tech-niques (Sch?fer & Weitz, 2012), on the one hand,and direct interpretation of born-digital PDF files(Berg et al, 2012), on the other hand.
Combin-ing these approaches has the potential to solve thevenerable challenges that stem from inhomogeneoussources in the ACL Anthology?e.g.
scanned, olderpapers and digital newer papers, generated from abroad variety of typesetting tools.However, as of mid-2012 there still is no ready-to-use, high-quality corpus that could serve as a sharedstarting point for the range of Anthology-based NLPactivities sketched in Section 1 above.
In fact, weremain slightly ambivalent about our recommenda-tions for utilizing the current state of affairs and ex-pected next steps?as we would like to avoid muchwork getting underway with a version of the corpusthat we know is unsatisfactory.
Further, obviously,versioning and well-defined release cycles will be aprerequisite to making the corpus useful for compa-rable research, as discussed by Bird et al (2008).In a nutshell, we see two possible avenues for-ward.
For the ACL 2012 Contributed Task, we col-lected various views on the corpus data (as well assome of the source code used in its production) in aunified SVN repository.
Following the open-source,crowd-sourcing philosophy, one option would be tomake this repository openly available to all inter-ested parties for future development, possibly aug-menting it with support infrastructure like, for ex-ample, a mailing list and shared wiki.At the same time, our experience from the pastmonths suggests that it is hard to reach sufficientmomentum and critical mass to make substantialprogress towards our long-term goals, while con-tributions are limited to loosely organized volun-teer work.
A possibility we believe might overcomethese limitations would be an attempt at formaliz-ing work in this spirit further, for example through afunded project (with endorsement and maybe finan-cial support from organizations like the ACL, ICCL,AFNLP, ELRA, or LDC).A potential, but not seriously contemplated ?busi-ness model?
for the ACL Anthology Corpus could bethat only groups providing also improved versionsof the corpus would get access to it.
This wouldcontradict the community spirit and other demands,viz.
that all code should be made publicly available(as open source) that is used to produce the rich-textXML for new papers added to the Anthology.
To de-cide on the way forward, we will solicit commentsand expressions of interest during ACL 2012, in-cluding of course from the R50 workshop audienceand participants in the Contributed Task.
Currentresults and status updates will always be accessiblethrough the following address:http://www.delph-in.net/aac/The ACL publication process for conferences andworkshops already today supports automated collec-tion of metadata and uniform layout/branding.
Forfuture high-quality collections of papers in the areaof Computational Linguistics, the ACL could think94about providing extended macro packages for con-ferences and journals that generate rich text and doc-ument structure preserving (TEI P5) XML versionsas a side effect, in addition to PDF generation.
Tech-nically, it should be possible in both LATEX and (forsure) in word processors such as OpenOffice or MSWord.
It would help reducing errors induced bythe tedious PDF-to-XML extraction this ContributedTask dealt with.Finally, we do think that it will well be possible toapply the Contributed Task ideas and machinery toscientific publications in other areas, including theenvisaged NLP research and existing NLP applica-tions for search, terminology extraction, summariza-tion, citation analysis, and more.6 AcknowledgmentsThe authors would like to thank the ACL, the work-shop organizer Rafael Banchs, the task contributorsfor their pioneering work, and the NUS group fortheir support.
We are indebted to Rebecca Dridanfor helpful feedback on this work.The work of the first author has been fundedby the German Federal Ministry of Education andResearch, projects TAKE (FKZ 01IW08003) andDeependance (FKZ 01IW11003).
The second andthird authors are supported by the Norwegian Re-search Council through the VerdIKT programme.ReferencesAbu-Jbara, A., & Radev, D. (2011).
Coherentcitation-based summarization of scientific papers.In Proceedings of the 49th annual meeting of theassociation for computational linguistics: Humanlanguage techologies (pp.
500?509).
Portland,OR.Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.(2011a).
Scisumm: A multi-document summa-rization system for scientific articles.
In Proceed-ings of the ACL-HLT 2011 system demonstrations(pp.
115?120).
Portland, OR.Agarwal, N., Reddy, R. S., Gvr, K., & Ros?, C. P.(2011b).
Towards multi-document summarizationof scientific articles: Making interesting compar-isons with SciSumm.
In Proceedings of the work-shop on automatic summarization for differentgenres, media, and languages (pp.
8?15).
Port-land, OR.Anderson, A., McFarland, D., & Jurafsky, D.(2012).
Towards a computational history of theACL:1980?2008.
In Proceedings of the ACL-2012 main conference workshop: Rediscovering50 years of discoveries.
Jeju, Republic of Korea.Athar, A.
(2011).
Sentiment analysis of citations us-ing sentence structure-based features.
In Proceed-ings of the ACL 2011 student session (pp.
81?87).Portland, OR.Ban?ski, P., & Przepi?rkowski, A.
(2009).
Stand-offTEI annotation: the case of the National Corpusof Polish.
In Proceedings of the third linguisticannotation workshop (pp.
64?67).
Suntec, Singa-pore.Berg, ?.
R., Oepen, S., & Read, J.
(2012).
To-wards high-quality text stream extraction fromPDF.
Technical background to the ACL 2012Contributed Task.
In Proceedings of the ACL-2012 main conference workshop on Rediscover-ing 50 Years of Discoveries.
Jeju, Republic ofKorea.Bird, S., Dale, R., Dorr, B., Gibson, B., Joseph, M.,Kan, M.-Y., Lee, D., Powley, B., Radev, D., &Tan, Y. F. (2008).
The ACL Anthology ReferenceCorpus: A reference dataset for bibliographic re-search in computational linguistics.
In Proceed-ings of the sixth international conference on lan-guage resources and evaluation (LREC-08).
Mar-rakech, Morocco.Councill, I. G., Giles, C. L., & Kan, M.-Y.
(2008).ParsCit: An open-source CRF reference stringparsing package.
In Proceedings of LREC-2008(pp.
661?667).
Marrakesh, Morocco.Dahlmeier, D., Ng, H. T., & Tran, T. P. (2011).
NUSat the HOO 2011 pilot shared task.
In Proceedingsof the generation challenges session at the 13theuropean workshop on natural language genera-tion (pp.
257?259).
Nancy, France.Dale, R., & Kilgarriff, A.
(2010).
Helping Our Own:Text massaging for computational linguistics as anew shared task.
In Proceedings of the 6th inter-national natural language generation conference.Trim, Co. Meath, Ireland.95Daudaravic?ius, V. (2012).
Applying collocation seg-mentation to the ACL Anthology Reference Cor-pus.
In Proceedings of the ACL-2012 main con-ference workshop: Rediscovering 50 years of dis-coveries.
Jeju, Republic of Korea.Dong, C., & Sch?fer, U.
(2011).
Ensemble-styleself-training on citation classification.
In Pro-ceedings of 5th international joint conference onnatural language processing (pp.
623?631).
Chi-ang Mai, Thailand.Gupta, P., & Rosso, P. (2012).
Text reuse withACL: (upward) trends.
In Proceedings of theACL-2012 main conference workshop: Rediscov-ering 50 years of discoveries.
Jeju, Republic ofKorea.Gupta, S., & Manning, C. (2011).
Analyzing thedynamics of research by extracting key aspects ofscientific papers.
In Proceedings of 5th interna-tional joint conference on natural language pro-cessing (pp.
1?9).
Chiang Mai, Thailand.Hall, D., Jurafsky, D., & Manning, C. D. (2008).Studying the history of ideas using topic models.In Proceedings of the 2008 conference on empir-ical methods in natural language processing (pp.363?371).
Honolulu, Hawaii.Johri, N., Ramage, D., McFarland, D., & Jurafsky,D.
(2011).
A study of academic collaborationsin computational linguistics using a latent mix-ture of authors model.
In Proceedings of the 5thACL-HLT workshop on language technology forcultural heritage, social sciences, and humanities(pp.
124?132).
Portland, OR.Johri, N., Roth, D., & Tu, Y.
(2010).
Experts?retrieval with multiword-enhanced author topicmodel.
In Proceedings of the NAACL HLT 2010workshop on semantic search (pp.
10?18).
LosAngeles, California.Mao, Y., Balasubramanian, K., & Lebanon, G.(2010).
Dimensionality reduction for text usingdomain knowledge.
In COLING 2010: Posters(pp.
801?809).
Beijing, China.Mohammad, S., Dorr, B., Egan, M., Hassan, A.,Muthukrishan, P., Qazvinian, V., Radev, D., & Za-jic, D. (2009).
Using citations to generate surveysof scientific paradigms.
In Proceedings of humanlanguage technologies: The 2009 annual confer-ence of the north american chapter of the associa-tion for computational linguistics (pp.
584?592).Boulder, Colorado.Muthukrishnan, P., Radev, D., & Mei, Q.
(2011).
Si-multaneous similarity learning and feature-weightlearning for document clustering.
In Proceedingsof textgraphs-6: Graph-based methods for natu-ral language processing (pp.
42?50).
Portland,OR.Nhat, H. D. H., & Bysani, P. (2012).
Linking ci-tations to their bibliographic references.
In Pro-ceedings of the ACL-2012 main conference work-shop: Rediscovering 50 years of discoveries.
Jeju,Republic of Korea.Przepi?rkowski, A.
(2009).
TEI P5 as an XML stan-dard for treebank encoding.
In Proceedings of theeighth international workshop on treebanks andlinguistic theories (pp.
149?160).
Milano, Italy.Qazvinian, V., & Radev, D. R. (2008).
Scientificpaper summarization using citation summary net-works.
In Proceedings of the 22nd internationalconference on computational linguistics (COL-ING 2008) (pp.
689?696).
Manchester, UK.Qazvinian, V., & Radev, D. R. (2010).
Identi-fying non-explicit citing sentences for citation-based summarization.
In Proceedings of the 48thannual meeting of the association for computa-tional linguistics (pp.
555?564).
Uppsala, Swe-den.Qazvinian, V., & Radev, D. R. (2011).
Learningfrom collective human behavior to introduce di-versity in lexical choice.
In Proceedings of the49th annual meeting of the association for com-putational linguistics: Human language techolo-gies (pp.
1098?1108).
Portland, OR.Qazvinian, V., Radev, D. R., & Ozgur, A.
(2010).Citation summarization through keyphrase ex-traction.
In Proceedings of the 23rd internationalconference on computational linguistics (COL-ING 2010) (pp.
895?903).
Beijing, China.Radev, D., & Abu-Jbara, A.
(2012).
RediscoveringACL discoveries through the lens of ACL Anthol-ogy Network citing sentences.
In Proceedings of96the ACL-2012 main conference workshop: Redis-covering 50 years of discoveries.
Jeju, Republicof Korea.Radev, D., Muthukrishnan, P., & Qazvinian, V.(2009).
The ACL Anthology Network corpus.
InProceedings of the 2009 workshop on text andcitation analysis for scholarly digital libraries.Morristown, NJ, USA.Radev, D. R., Muthukrishnan, P., & Qazvinian, V.(2009).
The ACL Anthology Network.
In Pro-ceedings of the 2009 workshop on text and cita-tion analysis for scholarly digital libraries (pp.54?61).
Suntec City, Singapore.Reiplinger, M., Sch?fer, U., & Wolska, M. (2012).Extracting glossary sentences from scholarly ar-ticles: A comparative evaluation of pattern boot-strapping and deep analysis.
In Proceedings of theACL-2012 main conference workshop: Rediscov-ering 50 years of discoveries.
Jeju, Republic ofKorea.Ritchie, A., Teufel, S., & Robertson, S. (2006a).Creating a test collection for citation-based IR ex-periments.
In Proceedings of the human languagetechnology conference of the NAACL, main con-ference (pp.
391?398).
New York City.Ritchie, A., Teufel, S., & Robertson, S. (2006b).How to find better index terms through cita-tions.
In Proceedings of the workshop on how cancomputational linguistics improve information re-trieval?
(pp.
25?32).
Sydney, Australia.Rozovskaya, A., Sammons, M., Gioja, J., & Roth,D.
(2011).
University of illinois system in HOOtext correction shared task.
In Proceedings of thegeneration challenges session at the 13th euro-pean workshop on natural language generation(pp.
263?266).
Nancy, France.Sch?fer, U., & Kasterka, U.
(2010).
Scientific au-thoring support: A tool to navigate in typed cita-tion graphs.
In Proceedings of the NAACL HLT2010 workshop on computational linguistics andwriting: Writing processes and authoring aids(pp.
7?14).
Los Angeles, CA.Sch?fer, U., Kiefer, B., Spurk, C., Steffen, J., &Wang, R. (2011).
The ACL Anthology Search-bench.
In Proceedings of the ACL-HLT 2011 sys-tem demonstrations (pp.
7?13).
Portland, OR.Sch?fer, U., & Weitz, B.
(2012).
Combining OCRoutputs for logical document structure markup.Technical background to the ACL 2012 Con-tributed Task.
In Proceedings of the ACL-2012main conference workshop on Rediscovering 50Years of Discoveries.
Jeju, Republic of Korea.Sim, Y., Smith, N. A., & Smith, D. A.
(2012).Discovering factions in the computational linguis-tics community.
In Proceedings of the ACL-2012 main conference workshop: Rediscovering50 years of discoveries.
Jeju, Republic of Korea.TEI Consortium.
(2012, February).
TEI P5: Guide-lines for electronic text encoding and interchange.
(http://www.tei-c.org/Guidelines/P5)Tu, Y., Johri, N., Roth, D., & Hockenmaier, J.(2010).
Citation author topic model in expertsearch.
In COLING 2010: Posters (pp.
1265?1273).
Beijing, China.Vogel, A., & Jurafsky, D. (2012).
He said, she said:Gender in the ACL anthology.
In Proceedings ofthe ACL-2012 main conference workshop: Redis-covering 50 years of discoveries.
Jeju, Republicof Korea.Xia, F., Lewis, W., & Poon, H. (2009).
LanguageID in the context of harvesting language data offthe web.
In Proceedings of the 12th conferenceof the european chapter of the ACL (EACL 2009)(pp.
870?878).
Athens, Greece.Xia, F., & Lewis, W. D. (2008).
Repurposing the-oretical linguistic data for tool development andsearch.
In Proceedings of the third internationaljoint conference on natural language processing:Volume-i (pp.
529?536).
Hyderabad, India.Zesch, T. (2011).
Helping Our Own 2011: UKPlab system description.
In Proceedings of thegeneration challenges session at the 13th euro-pean workshop on natural language generation(pp.
260?262).
Nancy, France.97
