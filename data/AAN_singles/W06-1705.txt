Annotated web as corpusPaul RaysonComputing Department,Lancaster University, UKp.rayson@lancs.ac.ukJames WalkerdineComputing Department,Lancaster University, UKj.walkerdine@lancs.ac.ukWilliam H. FletcherUnited States NavalAcademy, USAfletcher@usna.eduAdam KilgarriffLexical Computing Ltd., UKadam@lexmasterclass.comAbstractThis paper presents a proposal to facili-tate the use of the annotated web as cor-pus by alleviating the annotation bottle-neck for corpus data drawn from the web.We describe a framework for large-scaledistributed corpus annotation using peer-to-peer (P2P) technology to meet thisneed.
We also propose to annotate a largereference corpus in order to evaluate thisframework.
This will allow us to investi-gate the affordances offered by distrib-uted techniques to ensure replicability oflinguistic research based on web-derivedcorpora.1 IntroductionLinguistic annotation of corpora contributes cru-cially to the study of language at several levels:morphology, syntax, semantics, and discourse.Its significance is reflected both in the growinginterest in annotation software for word sensetagging (Edmonds and Kilgarriff, 2002) and inthe long-standing use of part-of-speech taggers,parsers and morphological analysers for datafrom English and many other languages.Linguists, lexicographers, social scientists andother researchers are using ever larger amountsof corpus data in their studies.
In corpus linguis-tics the progression has been from the 1 million-word Brown and LOB corpora of the 1960s, tothe 100 million-word British National Corpus ofthe 1990s.
In lexicography this progression isparalleled, for example, by Collins Dictionaries?initial 10 million word corpus growing to theircurrent corpus of around 600 million words.
Inaddition, the requirement for mega- and evengiga-corpora1 extends to other applications, suchas lexical frequency studies, neologism research,and statistical natural language processing wheremodels of sparse data are built.
The motivationfor increasingly large data sets remains the same.Due to the Zipfian nature of word frequencies,around half the word types in a corpus occuronly once, so tremendous increases in corpussize are required both to ensure inclusion of es-sential word and phrase types and to increase thechances of multiple occurrences of a given type.In corpus linguistics building such mega-corpora is beyond the scope of individual re-searchers, and they are not easily accessible(Kennedy, 1998: 56) unless the web is used as acorpus (Kilgarriff and Grefenstette, 2003).
In-creasingly, corpus researchers are tapping theWeb to overcome the sparse data problem (Kel-ler et al, 2002).
This topic generated intense in-terest at workshops held at the University of Hei-delberg (October 2004), University of Bologna(January 2005), University of Birmingham (July2005) and now in Trento in April 2006.
In addi-tion, the advantages of using linguistically anno-tated data over raw data are well documented(Mair, 2005; Granger and Rayson, 1998).
As thesize of a corpus increases, a near linear increasein computing power is required to annotate thetext.
Although processing power is steadilygrowing, it has already become impractical for asingle computer to annotate a mega-corpus.Creating a large-scale annotated corpus fromthe web requires a way to overcome the limita-tions on processing power.
We propose distrib-uted techniques to alleviate the limitations on the1 See, for example, those distributed by the LinguisticData Consortium: http://www.ldc.upenn.edu/27volume of data that can be tagged by a singleprocessor.
The task of annotating the data will beshared by computers at collaborating institutionsaround the world, taking advantage of processingpower and bandwidth that would otherwise gounused.
Such large-scale parallel processing re-moves the workload bottleneck imposed by aserver based structure.
This allows for tagging agreater amount of textual data in a given amountof time while permitting other users to use thesystem simultaneously.
Vast amounts of data canbe analysed with distributed techniques.
The fea-sibility of this approach has been demonstratedby the SETI@home project2.The framework we propose can incorporateother annotation or analysis systems, for exam-ple, lemmatisation, frequency profiling, or shal-low parsing.
To realise and evaluate the frame-work, it will be developed for a peer-to-peer(P2P) network and deployed along with an exist-ing lexicographic toolset, the Sketch Engine.
AP2P approach allows for a low cost implementa-tion that draws upon available resources (existinguser PCs).
As a case study for evaluation, weplan to collect a large reference corpus from theweb to be hosted on servers from Lexical Com-puting Ltd. We can evaluate annotation speedgains of our approach comparatively against thesingle server version by utilising processingpower in computer labs at Lancaster Universityand the United States Naval Academy (USNA)and we will call for volunteers from the corpuscommunity to be involved in the evaluation aswell.A key aspect of our case study research will beto investigate extending corpus collection to newdocument types.
Most web-derived corpora haveexploited raw text or HTML pages, so effortshave focussed on boilerplate removal and clean-up of these formats with tools like Hyppia-BTE,Tidy and Parcels 3  (Baroni and Sharoff, 2005).Other document formats such as Adobe PDF andMS-Word have been neglected due to the extraconversion and clean-up problems they entail.By excluding PDF documents, web-derived cor-pora are less representative of certain genressuch as academic writing.2 http://setiathome.ssl.berkeley.edu/3 http://www.smi.ucd.ie/hyppia/,http://parcels.sourceforge.net andhttp://tidy.sourceforge.net.2 Related WorkThe vast majority of previous work on corpusannotation has utilised either manual coding orautomated software tagging systems, or else asemi-automatic combination of the two ap-proaches e.g.
automated tagging followed bymanual correction.
In most cases a stand-alonesystem or client-server approach has been takenby annotation software using batch processingtechniques to tag corpora.
Only a handful ofweb-based or email services (CLAWS4, Amal-gam5, Connexor6) are available, for example, inthe application of part-of-speech tags to corpora.Existing tagging systems are ?small scale?
andtypically impose some limitation to prevent over-load (e.g.
restricted access or document size).Larger systems to support multiple documenttagging processes would require resources thatcannot be realistically provided by existing sin-gle-server systems.
This corpus annotation bot-tleneck becomes even more problematic for vo-luminous data sets drawn from the web.
The useof the web as a corpus for teaching and researchon language has been proposed a number oftimes (Kilgarriff, 2001; Robb, 2003; Rundell,2000; Fletcher, 2001, 2004b) and received a spe-cial issue of the journal Computational Linguis-tics (Kilgarriff and Grefenstette, 2003).
Studieshave used several different methods to mine webdata.
Turney (2001) extracts word co-occurrenceprobabilities from unlabelled text collected froma web crawler.
Baroni and Bernardini (2004)built a corpus by iteratively searching Google fora small set of seed terms.
Prototypes of Internetsearch engines for linguists, corpus linguists andlexicographers have been proposed: WebCorp(Kehoe and Renouf, 2002), KWiCFinder(Fletcher, 2004a) and the Linguist?s Search En-gine (Kilgarriff, 2003; Resnik and Elkiss, 2003).A key concern in corpus linguistics and relateddisciplines is verifiability and replicability of theresults of studies.
Word frequency counts ininternet search engines are inconsistent and unre-liable (Veronis, 2005).
Tools based on static cor-pora do not suffer from this problem, e.g.BNCweb7, developed at the University of Zurich,and View 8  (Variation in English Words andPhrases, developed at Brigham Young University)4 http://www.comp.lancs.ac.uk/ucrel/claws/trial.html5 http://www.comp.leeds.ac.uk/amalgam/amalgam/amalghome.htm6 http://www.connexor.com7 http://homepage.mac.com/bncweb/home.html8 http://view.byu.edu/28are both based on the British National Corpus.Both BNCweb and View enable access to anno-tated corpora and facilitate searching on part-of-speech tags.
In addition, PIE9 (Phrases in Eng-lish), developed at USNA, which performssearches on n-grams (based on words, parts-of-speech and characters), is currently restricted tothe British National Corpus as well, althoughother static corpora are being added to its data-base.
In contrast, little progress has been madetoward annotating sizable sample corpora fromthe web.?Real-time?
linguistic analysis of web data atthe syntactic level has been piloted by the Lin-guist?s Search Engine (LSE).
Using this tool,linguists can either perform syntactic searchesvia parse trees on a pre-analysed web collectionof around three million sentences from the Inter-net Archive (www.archive.org) or build theirown collections from AltaVista search engineresults.
The second method pushes the new col-lection onto a queue for the LSE annotator toanalyse.
A new collection does not becomeavailable for analysis until the LSE completesthe annotation process, which may entail signifi-cant delay with multiple users of the LSE server.The Gsearch system (Corley et al, 2001) alsoselects sentences by syntactic criteria from largeon-line text collections.
Gsearch annotates cor-pora with a fast chart parser to obviate the needfor corpora with pre-existing syntactic mark-up.In contrast, the Sketch Engine system to assistlexicographers to construct dictionary entriesrequires large pre-annotated corpora.
A wordsketch is an automatic one-page corpus-derivedsummary of a word's grammatical and colloca-tional behaviour.
Word Sketches were first usedto prepare the Macmillan English Dictionary forAdvanced Learners (2002, edited by MichaelRundell).
They have also served as the startingpoint for high-accuracy Word Sense Disam-biguation.
More recently, the Sketch Engine wasused to develop the new edition of the OxfordThesaurus of English (2004, edited by MauriceWaite).Parallelising or distributing processing hasbeen suggested before.
Clark and Curran?s (2004)work is in parallelising an implementation oflog-linear parsing on the Wall Street JournalCorpus, whereas we focus on part-of-speech tag-ging of a far larger and more varied web corpus,a technique more widely considered a prerequi-site for corpus linguistics research.
Curran (2003)9 http://pie.usna.edu/suggested distributed processing in terms of webservices but only to ?allow components devel-oped by different researchers in different loca-tions to be composed to build larger systems?and not for parallel processing.
Most signifi-cantly, previous investigations have not exam-ined three essential questions: how to apply dis-tributed techniques to vast quantities of corpusdata derived from the web, how to ensure thatweb-derived corpora are representative, and howto provide verifiability and replicability.
Thesecore foci of our work represent crucial innova-tions lacking in prior research.
In particular, rep-resentativeness and replicability are key researchconcerns to enhance the reliability of web datafor corpora.In the areas of Natural Language Processing(NLP) and computational linguistics, proposalshave been made for using the computational Gridfor data-intensive NLP and text-mining for e-Science (Carroll et al, 2005; Hughes et al 2004).While such an approach promises much in termsof emerging infrastructure, we wish to exploitexisting computing infrastructure that is moreaccessible to linguists via a P2P approach.
Insimple terms, P2P is a technology that takes ad-vantage of the resources and services available atthe edge of the Internet (Shirky, 2001).
Betterknown for file-sharing and Instant Messengerapplications, P2P has increasingly been appliedin distributed computational systems.
Examplesinclude SETI@home (looking for radio evidenceof extraterrestrial life), ClimatePrediction.net(studying climate change), Predictor@home (in-vestigating protein-related diseases) and Ein-stein@home (searching for gravitational signals).A key advantage of P2P systems is that theyare lightweight and geared to personal computingwhere informal groups provide unused process-ing power to solve a common problem.
Typically,P2P systems draw upon the resources that al-ready exist on a network (e.g.
home or workPCs), thus keeping the cost to resource ratio low.For example the fastest supercomputer cost over$110 million to develop and has a peak perform-ance of 12.3 TFLOPS (trillions of floating-pointoperations per second).
In contrast, a typical dayfor the SETI@home project involved a perform-ance of over 20 TFLOPS, yet cost only $700,000to develop; processing power is donated by userPCs.
This high yield for low start-up cost makesit ideal for cheaply developing effective compu-tational systems to realise, deploy and evaluateour framework.
The deployment of computa-tional based P2P systems is supported by archi-29tectures such as BOINC10, which provide a plat-form on which volunteer based distributed com-puting systems can be built.
Lancaster's own P2PApplication Framework (Walkerdine et al, sub-mitted) also supports higher-level P2P applica-tion development and can be adapted to makeuse of the BOINC architecture.3 Research hypothesis and aimsOur research hypothesis is that distributed com-putational techniques can alleviate the annotationbottleneck for processing corpus data from theweb.
This leads us to a number of research ques-tions:?
How can corpus data from the web be di-vided into units for processing via distrib-uted techniques??
Which corpus annotation techniques aresuitable for distributed processing??
Can distributed techniques assist in corpusclean-up and conversion to allow inclu-sion of a wider variety of genres and tosupport more representative corpora?In the early stages of our proposed research,we are focussing on grammatical word-classanalysis (part-of-speech tagging) of web-derivedcorpora of English and aspects of corpus clean-up and conversion.
Clarifying copyright issuesand exploring models for legal dissemination ofcorpora compiled from web data are key objec-tives of this stage of the investigation as well.4 MethodologyThe initial focus of the work will be to developthe framework for distributed corpus annotation.Since existing solutions have been centralised innature, we first must examine the consequencesthat a distributed approach has for corpus annota-tion and identify issues to address.A key concern will be handling web pageswithin the framework, as it is essential to mini-mise the amount of data communicated betweenpeers.
Unlike the other distributed analytical sys-tems mentioned above, the size of text documentand analysis time is largely proportional for cor-pora annotation.
This places limitations on workunit size and distribution strategies.
In particular,three areas will be investigated:?
Mechanisms for crawling/discovery of aweb corpus domain - how to identifypages to include in a web corpus.
Also10 BOINC, Berkeley Open Infrastructure for NetworkComputing.
http://boinc.berkeley.edu.investigate appropriate criteria for han-dling pages which are created or modi-fied dynamically.?
Mechanisms to generate work units fordistributed computation - how to splitthe corpus into work units and reduce thecommunication / computation time ratiothat is crucial for such systems to be ef-fective.?
Mechanisms to support the distributionof work units and collection of results -how to handle load balancing.
What datashould be sent to peers and how is theprocessed information handled and ma-nipulated?
What mechanisms should bein place to ensure correctness of results?How can abuse be prevented and secu-rity concerns of collaborating institutionsbe addressed?
BOINC already providesa good platform for this, and these as-pects will be investigated within the pro-ject.Analysis of existing distributed computationsystems will help to inform the design of theframework and tackle some of these issues.
Fi-nally, the framework will also cater for threecommon strategies for corpus annotation:?
Site based corpus annotation - in whichthe user can specify a web site to anno-tate?
Domain based corpus annotation - inwhich the user specifies a content do-main (with the use of keywords) to  an-notate?
Crawler based corpus annotation - moregeneral web based corpus annotation inwhich crawlers are used to locate webpagesFrom a computational linguistic view, theframework will also need to take into account thegranularity of the unit (for example, POS taggingrequires sentence-units, but anaphoric annotationneeds paragraphs or larger).
Secondly, we needto investigate techniques for identifying identicaldocuments, virtually identical documents andhighly repetitive documents, such as those pio-neered by Fletcher (2004b) and shingling tech-niques described by Chakrabarti (2002).The second stage of our work will involve im-plementing the framework within a P2P envi-ronment.
We have already developed a prototypeof an object-oriented application environment tosupport P2P system development using JXTA(Sun's P2P API).
We have designed this envi-ronment so that specific application functionality30can be captured within plug-ins that can then in-tegrate with the environment and utilise its func-tionality.
This system has been successfullytested with the development of plug-ins support-ing instant messaging, distributed video encoding(Hughes and Walkerdine, 2005), distributed vir-tual worlds (Hughes et al, 2005) and digital li-brary management (Walkerdine and Rayson,2004).
It is our intention to implement our dis-tributed corpus annotation framework as a plug-in.
This will involve implementing new func-tionality and integrating this with our existingannotation tools (such as CLAWS11).
The devel-opment environment is also flexible enough toutilise the BOINC platform, and such supportwill be built into it.Using the P2P Application Framework as abasis for the development secures several advan-tages.
First, it reduces development time by al-lowing the developer to reuse existing function-ality; secondly, it already supports essential as-pects such as system security; and thirdly, it hasalready been used successfully to deploy compa-rable P2P applications.
A lightweight version ofthe application framework will be bundled withthe corpus annotation plug-in, and this will thenbe made publicly available for download inopen-source and executable formats.
We envis-age our end-users will come from a variety ofdisciplines such as language engineering and lin-guistics.
For the less-technical users, the proto-type will be packaged as a screensaver or instantmessaging client to facilitate deployment.5 EvaluationWe will evaluate the framework and prototypedeveloped by applying it as a pre-processor stepfor the Sketch Engine system.
The Sketch Enginerequires a large well-balanced corpus which hasbeen part-of-speech tagged and shallow parsed tofind subjects, objects, heads, and modifiers.
Wewill use the existing non-distributed processingtools on the Sketch Engine as a baseline for acomparative evaluation of the AWAC frame-work instantiation by utilising processing powerand bandwidth in learning labs at Lancaster Uni-versity and USNA during off hours.We will explore techniques to make the result-ing annotated web corpus data available in staticform to enable replication and verification ofcorpus studies based on such data.
The initialsolution will be to store the resulting reference11 http://www.comp.lancs.ac.uk/ucrel/claws/corpus in the Sketch Engine.
We will also inves-tigate whether the distributed environment un-derlying our approach offers a solution to theproblem of reproducibility in web-based corpusstudies based in general.
Current practise else-where includes the distribution of URL lists, butgiven the dynamic nature of the web, this is notsufficiently robust.
Other solutions such as com-plete caching of the corpora are not typicallyadopted due to legal concerns over copyright andredistribution of web data, issues considered atlength by Fletcher (2004a).
Other requirementsfor reference corpora such as retrieval and stor-age of metadata for web pages are beyond thescope of what we propose here.To improve the representative nature of web-derived corpora, we will research techniques toenable the importing of additional documenttypes such as PDF.
We will reuse and extendtechniques implemented in the collection, encod-ing and annotation of the PERC Corpus of Pro-fessional English12.
A majority of this corpus hasbeen collected by conversion of on-line academicjournal articles from PDF to XML with a combi-nation of semi-automatic tools and techniques(including Adobe Acrobat version 6).
Basic is-sues such as character encoding, table/figure ex-traction and maintaining text flow around em-bedded images need to be dealt with before an-notation processing can begin.
We will compara-tively evaluate our techniques against others suchas pdf2txt, and Multivalent PDF ExtractText13.Part of the evaluation will be to collect and anno-tate a sample corpus.
We aim to collect a corpusfrom the web that is comparable to the BNC incontent and annotation.
This corpus will betagged using the P2P framework.
It will form atest-bed for the framework and we will utilise thenon-distributed annotation system on the SketchEngine as a baseline for comparison and evalua-tion.
To evaluate text conversion and clean-uproutines for PDF documents, we will use a 5-million-word gold-standard sub-corpus extracted12 The Corpus of Professional English (CPE) is a ma-jor research project of PERC (the Professional Eng-lish Research Consortium) currently underway that,when finished, will consist of a 100-million-wordcomputerised database of English used by profession-als in science, engineering, technology and otherfields.
Lancaster University and Shogakukan Inc. arePERC Member Institutions.
For more details, seehttp://www.perc21.org/13 http://multivalent.sourceforge.net/31from the PERC Corpus of ProfessionalEnglish14.6 ConclusionFuture work includes an analysis of the balancebetween computational and bandwidth require-ments.
It is essential in distributing the corpusannotation to achieve small amounts of datatransmission in return for large computationalgains for each work-unit.In this paper, we have discussed the require-ment for annotation of web-derived corpus data.Currently, a bottleneck exists in the tagging ofweb-derived corpus data due to the voluminousamount of corpus processing involved.
Our pro-posal is to construct a framework for large-scaledistributed corpus annotation using existing peer-to-peer technology.
We have presented the chal-lenges that lie ahead for such an approach.
Workis now underway to address the clean-up of PDFdata for inclusion into corpora downloaded fromthe web.AcknowledgementsWe wish to thank the anonymous reviewers whocommented our paper.
We are grateful to Shoga-kukan Inc. (Tokyo, Japan) for supporting re-search at Lancaster University into the process ofconversion and clean-up of PDF to text, and tothe Professional English Research Consortiumfor the provision of the gold-standard corpus forour evaluation.ReferencesBaroni, M. and Bernardini, S. (2004).
BootCaT:Bootstrapping Corpora and Terms from the Web.In Proceedings of LREC2004, Lisbon, pp.
1313-1316.Baroni, M. and Sharoff, S. (2005).
Creating special-ized and general corpora using automated searchengine queries.
Web as Corpus Workshop, Bir-mingham University, UK, 14th July 2005.Carroll, J., R. Evans and E. Klein (2005) Supportingtext mining for e-Science: the challenges for Grid-enabled natural language processing.
In Workshopon Text Mining, e-Research And Grid-enabledLanguage Technology at the Fourth UK e-ScienceProgramme All Hands Meeting (AHM2005), Not-tingham, UK.14 This corpus has already been manually re-typed atShogakukan Inc. from PDF originals downloadedfrom the web.Chakrabarti, S. (2002) Mining the Web: DiscoveringKnowledge from Hypertext Data.
Morgan Kauf-mann.Clark, S. and Curran, J. R.. (2004).
Parsing the wsjusing ccg and log-linear models.
In Proceedings ofthe 42nd Annual Meeting of the Association forComputational Linguistics (ACL ?04).Corley, S., Corley, M., Keller, F., Crocker, M., &Trewin, S. (2001).
Finding Syntactic Structure inUnparsed Corpora: The Gsearch Corpus QuerySystem.
Computers and the Humanities, 35, 81-94.Curran, J.R. (2003).
Blueprint for a High PerformanceNLP Infrastructure.
In Proc.
of Workshop on Soft-ware Engineering and Architecture of LanguageTechnology Systems (SEALTS) Edmonton, Canada,2003, pp.
40 ?
45.Edmonds, P and Kilgarriff, A.
(2002).
Introduction tothe special issue on evaluating word sense disam-biguation systems.
Journal of Natural LanguageEngineering, 8 (2), pp.
279-291.Fletcher, W. H. (2001).
Concordancing the Web withKWiCFinder.
Third North American Symposiumon Corpus Linguistics and Language Teaching,Boston, MA, 23-25 March 2001.Fletcher, W. H. (2004a).
Facilitating the compilationand dissemination of ad-hoc Web corpora.
In G.Aston, S. Bernardini and D. Stewart (eds.
), Cor-pora and Language Learners, pp.
271 ?
300, JohnBenjamins, Amsterdam.Fletcher, W. H. (2004b).
Making the Web More Use-ful as a Source for Linguistic Corpora.
In UllaConnor and Thomas A. Upton (eds.)
Applied Cor-pus Linguistics.
A Multidimensional Perspective.Rodopi, Amsterdam, pp.
191 ?
205.Granger, S., and Rayson, P. (1998).
Automatic profil-ing of learner texts.
In S. Granger (ed.)
LearnerEnglish on Computer.
Longman, London and NewYork, pp.
119-131.Hughes, B, Bird, S., Haejoong, K., and Klein, E.(2004).
Experiments with data-intensive NLP on acomputational grid.
Proceedings of the Interna-tional Workshop on Human Language Technology.http://eprints.unimelb.edu.au/archive/00000503/.Hughes, D., Gilleade, K., Walkerdine, J. and Mariani,J., Exploiting P2P in the Creation of Game Worlds.In the proceedings of ACM GDTW 2005, Liver-pool, UK, 8th-9th November, 2005.Hughes, D. and Walkerdine, J.
(2005), DistributedVideo Encoding Over A Peer-to-Peer Network.
Inthe proceedings of PREP 2005, Lancaster, UK,30th March - 1st April, 2005Kehoe, A. and Renouf, A.
(2002) WebCorp: Applyingthe Web to Linguistics and Linguistics to the Web.32World Wide Web 2002 Conference, Honolulu, Ha-waii.Keller, F., Lapata, M. and Ourioupina, O.
(2002).Using the Web to Overcome Data Sparseness.
Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, Philadelphia,July 2002, pp.
230-237.Kennedy, G. (1998).
An introduction to corpus lin-guistics.
Longman, London.Kilgarriff, A.
(2001).
Web as corpus.
In Proceedingsof Corpus Linguistics 2001, Lancaster University,29 March - 2 April 2001, pp.
342 ?
344.Kilgarriff, A.
(2003).
Linguistic Search Engine.
Inproceedings of Workshop on Shallow Processing ofLarge Corpora (SProLaC 2003), Lancaster Uni-versity, 28 - 31 March 2003, pp.
53 ?
58.Kilgarriff, A. and Grefenstette, G (2003).
Introductionto the Special Issue on the Web as Corpus.
Compu-tational Linguistics, 29: 3, pp.
333-347.Mair, C. (2005).
The corpus-based study of languagechange in progress: The extra value of tagged cor-pora.
Presentation at the AAACL/ICAME Confer-ence, Ann Arbor, May 2005.Resnik, P. and Elkiss, A.
(2003) The Linguist's SearchEngine: Getting Started Guide.
Technical Report:LAMP-TR-108/CS-TR-4541/UMIACS-TR-2003-109, University of Maryland, College Park, No-vember 2003.Robb, T. (2003) Google as a Corpus Tool?
In ETJJournal, Volume 4, number 1, Spring 2003.Rundell, M. (2000).
"The biggest corpus of all", Hu-manising Language Teaching.
2:3; May 2000.Shirky, C. (2001) Listening to Napster, in Peer-to-Peer: Harnessing the power of Disruptive Tech-nologies, O'Reilly.Turney, P. (2001).
Word Sense Disambiguation byWeb Mining for Word Co-occurrence Probabili-ties.
In proceedings of SENSEVAL-3, Barcelona,Spain, July 2004 pp.
239-242.Veronis, J.
(2005).
Web: Google's missing pages:mystery solved?http://aixtal.blogspot.com/2005/02/web-googles-missing-pages-mystery.html (accessed April 28,2005).Walkerdine, J., Gilleade, K., Hughes, D., Rayson, P.,Simms, J., Mariani, J., and Sommerville, I. AFramework for P2P Application Development.
Pa-per submitted to Software Practice and Experience.Walkerdine, J. and Rayson, P. (2004) P2P-4-DL:Digital Library over Peer-to-Peer.
In Caronni G.,Weiler N., Shahmehri N.
(eds.)
Proceedings ofFourth IEEE International Conference on Peer-to-Peer Computing (PSP2004) 25-27 August 2004,Zurich, Switzerland.
IEEE Computer SocietyPress, pp.
264-265.3334
