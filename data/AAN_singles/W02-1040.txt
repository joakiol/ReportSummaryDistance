The Influence of Minimum Edit Distance on Reference ResolutionMichael StrubeEuropean Media Laboratory GmbHVilla BoschSchlo?-Wolfsbrunnenweg 3369118 Heidelberg, Germanystrube@eml.villa-bosch.deStefan RappSony International (Europe) GmbHAdvanced Technology Center StuttgartHeinrich-Hertz-Stra?e 170327 Stuttgart, Germanyrapp@sony.deChristoph Mu?llerEuropean Media Laboratory GmbHVilla BoschSchlo?-Wolfsbrunnenweg 3369118 Heidelberg, Germanymueller@eml.villa-bosch.deAbstractWe report on experiments in reference res-olution using a decision tree approach.
Westarted with a standard feature set used inprevious work, which led to moderate re-sults.
A closer examination of the perfor-mance of the features for different formsof anaphoric expressions showed good re-sults for pronouns, moderate results forproper names, and poor results for definitenoun phrases.
We then included a cheap,language and domain independent featurebased on the minimum edit distance be-tween strings.
This feature yielded a sig-nificant improvement for data sets consist-ing of definite noun phrases and propernames, respectively.
When applied tothe whole data set the feature produced asmaller but still significant improvement.1 IntroductionFor the automatic understanding of written or spo-ken natural language it is crucial to be able to iden-tify the entities referred to by referring expressions.The most common and thus most important typesof referring expressions are pronouns and definitenoun phrases (NPs).
Supervised machine learningalgorithms have been used for pronoun resolution(Ge et al, 1998) and for the resolution of definiteNPs (Aone and Bennett, 1995; McCarthy and Lehn-ert, 1995; Soon et al, 2001).
An unsupervised ap-proach to the resolution of definite NPs was appliedby Cardie and Wagstaff (1999).
However, thoughmachine learning algorithms may deduce to makebest use of a given set of features for a given prob-lem, it is a linguistic question and a non-trivial taskto identify a set of features which describe the datasufficiently.We report on experiments in the resolution ofanaphoric expressions in general, including definitenoun phrases, proper names, and personal, posses-sive and demonstrative pronouns.
Based on thework mentioned above we started with a feature setincluding NP-level and coreference-level features.Applied to the whole data set these features ledonly to moderate results.
Since the NP form of theanaphor (i.e., whether the anaphoric expression isrealized as pronoun, definite NP or proper name) ap-peared to be the most important feature, we dividedthe data set into several subsets based on the NPform of the anaphor.
This led to the insight that themoderate performance of our system was caused bythe low performance for definite NPs.
We adopteda new feature based on the minimum edit distance(Wagner and Fischer, 1974) between anaphor andantecedent, which led to a significant improvementon definite NPs and proper names.
When applied tothe whole data set the feature yielded a smaller butstill significant improvement.In this paper, we first discuss features that havebeen found to be relevant for the task of referenceresolution (Section 2).
Then we describe our cor-pus, the corpus annotation, and the way we preparedthe data for use with a binary machine learning clas-sifier (Section 3).
In Section 4 we first describe thefeature set used initially and the results it produced.Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
312-319.Proceedings of the Conference on Empirical Methods in NaturalWe then introduce the minimum edit distance fea-ture and give the results it yielded on different datasets.2 Features for Reference Resolution inPrevious WorkDriven by the necessity to provide robust systemsfor the MUC system evaluations, researchers beganto look for those features which were particular im-portant for the task of reference resolution.
Whilemost features for pronoun resolution have been de-scribed in the literature for decades, researchers onlyrecently began to look for robust and cheap features,i.e., features which perform well over several do-mains and can be annotated (semi-) automatically.In the following, we describe a few earlier contri-butions to reference resolution with respect to thefeatures used.Decision tree algorithms were used for ref-erence resolution by Aone and Bennett (1995,C4.5), McCarthy and Lehnert (1995, C4.5) andSoon et al (2001, C5.0).
This approach requiresthe definition of a set of features describingpairs of anaphors and their antecedents, and col-lecting a training corpus annotated with them.Aone and Bennett (1995), working on referenceresolution in Japanese newspaper articles, use66 features.
They do not mention all of theseexplicitly but emphasize the features POS-tag,grammatical role, semantic class and distance.The set of semantic classes they use appears to berather elaborated and highly domain-dependent.Aone and Bennett (1995) report that their bestclassifier achieved an F-measure of about 77% aftertraining on 250 documents.
They mention thatit was important for the training data to containtransitive positives, i.e., all possible coreferencerelations within an anaphoric chain.McCarthy and Lehnert (1995) describe a refer-ence resolution component which they evaluated onthe MUC-5 English Joint Venture corpus.
They dis-tinguish between features which focus on individ-ual noun phrases (e.g.
Does noun phrase contain aname?)
and features which focus on the anaphoricrelation (e.g.
Do both share a common NP?).
Itwas criticized (Soon et al, 2001) that the featuresused by McCarthy and Lehnert (1995) are highly id-iosyncratic and applicable only to one particular do-main.
McCarthy and Lehnert (1995) achieved re-sults of about 86% F-measure (evaluated accord-ing to Vilain et al (1995)) on the MUC-5 data set.However, only a defined subset of all possible ref-erence resolution cases was considered relevant inthe MUC-5 task description, e.g., only entity refer-ences.
For this case, the domain-dependent featuresmay have been particularly important, making it dif-ficult to compare the results of this approach to oth-ers working on less restricted domains.Soon et al (2001) use twelve features (see Table1).
Soon et al (2001) show a part of their decisiontree in which the weak string identity feature (i.e.identity after determiners have been removed) ap-pears to be the most important one.
They also reporton the relative contribution of the features wherethe three features weak string identity, alias (whichmaps named entities in order to resolve dates, per-son names, acronyms, etc.)
and appositive seem tocover most of the cases (the other nine features con-tribute only 2.3% F-measure for MUC-6 texts and1% F-measure for MUC-7 texts).
Soon et al (2001)include all noun phrases returned by their NP iden-tifier and report an F-measure of 62.6% for MUC-6data and 60.4% for MUC-7 data.
They only usedpairs of anaphors and their closest antecedents aspositive examples in training, but evaluated accord-ing to Vilain et al (1995).Cardie and Wagstaff (1999) describe an unsuper-vised clustering approach to noun phrase corefer-ence resolution in which features are assigned tosingle noun phrases only.
They use the featuresshown in Table 2, all of which are obtained auto-matically without any manual tagging.
The featuresemantic class used by Cardie and Wagstaff (1999)seems to be a domain-dependent one which canonly be used for the MUC domain and similarones.
Cardie and Wagstaff (1999) report a perfor-mance of 53,6% F-measure (evaluated according toVilain et al (1995)).3 Data3.1 Text CorpusOur corpus consists of 242 short German texts (to-tal 36924 tokens) about sights, historic events andpersons in Heidelberg.
The average length is 151 to-?
distance in sentences between anaphor and antecedent?
antecedent is a pronoun??
anaphor is a pronoun??
weak string identity between anaphor and antecedent?
anaphor is a definite noun phrase??
anaphor is a demonstrative pronoun??
number agreement between anaphor and antecedent?
semantic class agreement between anaphor and antecedent?
gender agreement between anaphor and antecedent?
anaphor and antecedent are both proper names??
an alias feature (used for proper names and acronyms)?
an appositive featureTable 1: Features used by Soon et al?
position (NPs are numbered sequentially)?
pronoun type (nom., acc., possessive, ambiguous)?
article (indefinite, definite, none)?
appositive (yes, no)?
number (singular, plural)?
proper name (yes, no)?
semantic class (based on WordNet: time, city, animal,human, object; based on a separate algorithm: num-ber, money, company)?
gender (masculine, feminine, either, neuter)?
animacy (anim, inanim)Table 2: Features used by Cardie and Wagstaffkens.
The texts were POS-tagged using TnT (Brants,2000).
A basic identification of markables (refer-ring expressions, i.e.
NPs) was obtained by using theNP-Chunker Chunkie (Skut and Brants, 1998).
ThePOS-tagger was also used for assigning attributeslike e.g.
the NP form to markables.
The automaticannotation was followed by a manual correction andannotation phase in which the markables were anno-tated with further tags (e.g.
semantic class).
In thisphase manual coreference annotation was performedas well.
In our annotation coreference is representedin terms of a member attribute on markables.
Mark-ables with the same value in this attribute are con-sidered coreferring expressions.
The annotation wasperformed by two students.
The reliability of the an-notations was checked using the kappa statistic (Car-letta, 1996).3.2 Data GenerationThe problem of coreference resolution can easily beformulated as a binary classification: Given a pairof potential anaphor and potential antecedent, clas-sify as positive if the antecedent is in fact the closestantecedent, and as negative otherwise.
In anaphoricchains only the immediately adjacent pairs are clas-sified as positive.
We generated data suitable as in-put to a machine learning algorithm from our corpususing a straightforward algorithm which combinedpotential anaphors and their potential antecedents.We then applied the following filters to the resultingpairs: Discard an antecedent-anaphor pair  if the anaphor is an indefinite NP,  if one entity is embedded into the other, e.g.
ifthe potential anaphor is the head of the poten-tial antecedent NP (or vice versa),  if both entities have different values in their se-mantic class attributes1 ,  if either entity has a value other than 3rd personsingular or plural in its agreement feature,  if both entities have different values in theiragreement features2.For some texts, these heuristics (which were ap-plied to the entire corpus) reduced to up to 50%the potential anaphor-antecedent pairs all of whichwould have been negative cases.
We consider thecases discarded as irrelevant because they do notcontribute any knowledge for the classifier.
After ap-plication of the filters, the remaining candidate pairswere labeled as follows:  Pairs of anaphors and their direct (i.e.
clos-est) antecedents were labeled P. This meansthat each anaphoric expression produced ex-actly one positive instance.  Pairs of anaphors and those non-antecedentswhich occurred closer to the anaphor than thedirect antecedent were labeled N. The numberof negative instances that each expression pro-duced thus depended on the number of non-antecedents occurring between the anaphor andthe direct antecedent (or, the beginning of thetext if there was none).Pairs of anaphors and non-antecedents which oc-cured further away than the direct antecedent as wellas pairs of anaphors and non-direct (transitive) an-tecedents were not considered in the data sets.
Thisproduced 242 data sets with a total of 72093 in-stances of potential antecedent-anaphor pairs.4 Results4.1 Our FeaturesThe features for our study were selected accordingto three criteria:1This filter applies only if none of the expressions is a pro-noun.
Otherwise, filtering on semantic class is not possible be-cause in a real-world setting, information about a pronoun?s se-mantic class obviously is not available prior to its resolution.2This filter applies only if the anaphor is a pronoun.
Thisrestriction of the filter is necessary because German allows forcases where an antecedent is referred back to by a non-pronounanaphor which has a different grammatical gender.  relevance according to previous research,  low annotation cost and/or high reliability ofautomatic tagging,  domain-independence.We distinguish between features assigned to nounphrases and features assigned to the potential coref-erence relation.
All features are listed in Table 3 to-gether with their respective possible values.The grammatical function of referring expres-sions has often been claimed to be an important fac-tor for reference resolution and was therefore in-cluded (features 2 and 6).
The surface realizationof referring expressions seems to have an influenceon coreference relations as well (features 3 and 7).Since we use a German corpus and in this languagethe gender and the semantic class do not necessar-ily coincide (i.e., objects are not necessarily neuteras they are in English) we also provide a semanticclass feature (5 and 9) which captures the differencebetween human, concrete objects, and abstract ob-jects.
This basically corresponds to the gender at-tribute in English, for which we introduced an agree-ment feature (4 and 8).
The feature wdist (10) cap-tures the distance in words between anaphor and an-tecedent, while the feature ddist (11) does the samein terms of sentences and mdist (12) in terms ofmarkables.
The equivalence in grammatical func-tion between anaphor and potential antecedent iscaptured in the feature syn par (13), which is true ifboth anaphor and antecedent are subjects or both areobjects, and false in the other cases.
The string identfeature (14) appears to be of major importance sinceit provides for high precision in reference resolution(it almost never fails) while the substring match fea-ture (15) could potentially provide better recall.4.2 Baseline ResultsUsing the features of Table 3, we trained decisiontree classifiers using C5.0, with standard settings forpre and post pruning.
As several features are dis-crete, we allowed the algorithm to use subsets offeature values in questions such as ?Is ana npform inPPER, PPOS, PDS  ??.
We also let C5.0 constructrules from the decision trees, as we found them togive superior results.
In our experiments, the valueDocument level features1.
doc id document number (1 .
.
.
250)NP-level features2.
ante gram func grammatical function of antecedent (subject, object, other)3. ante npform form of antecedent (definite NP, indefinite NP, personal pronoun,demonstrative pronoun, possessive pronoun, proper name)4. ante agree agreement in person, gender, number5.
ante semanticclass semantic class of antecedent (human, concrete object, abstract object)6. ana gram func grammatical function of anaphor (subject, object, other)7. ana npform form of anaphor (definite NP, indefinite NP, personal pronoun,demonstrative pronoun, possessive pronoun, proper name)8. ana agree agreement in person, gender, number9.
ana semanticclass semantic class of anaphor (human, concrete object, abstract object)Coreference-level features10.
wdist distance between anaphor and antecedent in words (1 .
.
.
n)11. ddist distance between anaphor and antecedent in sentences (0, 1,  1)12. mdist distance between anaphor and antecedent in markables (1 .
.
.
n)13. syn par anaphor and antecedent have the same grammatical function (yes, no)14. string ident anaphor and antecedent consist of identical strings (yes, no)15. substring match one string contains the other (yes, no)Table 3: Our Featuresof the ana semanticclass attribute was reset to miss-ing for pronominal anaphors, because in a realisticsetting the semantic class of a pronoun obviously isnot available prior to its resolution.Using 10-fold cross validation (with about 25documents for each of the 10 bins), we achieved anoverall error rate of 1.74%.
Always guessing the byfar more frequent negative class would give an er-ror rate of 2.88% (70019 out of 72093 cases).
Theprecision for finding positive cases is 88.60%, therecall is 45.32%.
The equally weighted F-measure3is 59.97%.Since we were not satisfied with this result weexamined the performance of the features.
Surpris-ingly, against our linguistic intuition the ana npformfeature appeared to be the most important one.
Thus,we expected considerable differences in the perfor-mance of our classifier with respect to the NP formof the anaphor under consideration.
We split the datainto subsets defined by the NP form of the anaphorand trained the classifier on these data sets.
The re-sults confirmed that the classifier performed poorlyon definite NPs (defNP) and demonstrative pronouns3computed as 	(PDS), moderately on proper names (NE) and quitegood on personal pronouns (PPER) and possessivepronouns (PPOS) (the results are reported in Ta-ble 4).
As definite NPs account for 792 out of2074 (38.19%) of the positive cases (and for 48125(66.75%) of all cases), it is evident that the weakperformance for the resolution of definite NPs, es-pecially the low recall of only 8.71% clearly impairsthe overall results.
Demonstrative pronouns appearonly in 0.87% of the positive cases, so the inferiorperformance is not that important.
Proper names(NE) however are more problematic, as they have tobe considered in 644 or 31.05% of the positive cases(22.96% of all).P R FdefNP 87.34% 8.71% 15.84%NE 90.83% 50.78% 65.14%PDS 25.00% 11.11% 15.38%PPER 88.12% 78.07% 82.79%PPOS 82.69% 87.31% 84.94%all 88.60% 45.32% 59.97%Table 4: Baseline results using features 2?15.Antecedent Anaphor?Philips?
?Kurfu?rst Philip?
?vier Schu?lern?
?die Schu?ler?
?die alte Universita?t?
?der alten Universita?t?
?im Studentenkarzer in der Augustinergasse?
?des Studentenkarzers?
?diese hervorragende Bibliothek?
?dieser Bibliothek?Table 5: Anaphors and their direct antecedentsNew coreference-level features16.
ante med minimum edit distance to anaphorff fiflffi "!#!%$'&)(*,+.-0/1-324&17.
ana med minimum edit distance to antecedent5 fi6fl7ffi "!#!8$'9(*1+:-0/-32;49Table 6: Additional Features ( fi ,  , < , = , fl : see text)4.3 Additional featuresSince definite noun phrases constitute more than athird of the anaphoric expressions in our corpus, weinvestigated why the resolution performed so poorlyfor these cases.
The major reason may be that theresolution algorithm relies on surface features anddoes not have access to world or domain knowl-edge, which we did not want to depend upon sincewe were mainly interested in cheap features.
How-ever, the string ident and substring match featuresdid not perform very well either.
The string identfeature had a very high precision (it almost neverfailed) but a poor recall.
The substring match fea-ture was not too helpful either as it does not triggerin many cases.
So, we investigated ways to raise therecall of the string ident and substring match fea-tures without losing too much precision.A look at some relevant cases (Table 5) sug-gested that a large number of anaphoric defi-nite NPs shared some substring with their an-tecedent, but they were not identical nor com-pletely included.
What is needed is a weakenedform of the string ident and substring match fea-tures.
Soon et al (2001) removed determiners be-fore comparing the strings.
Other researchers likeVieira and Poesio (2000) used information about thesyntactic structure and compared only the syntacticheads of the phrases.
However, the feature used bySoon et al (2001) is neither sufficient nor languagedependent, the one used by Vieira and Poesio (2000)is not cheap since it relies on a syntactic analysis.We were looking for a feature which gave usthe improvements of the features used by other re-searchers without their associated costs.
Hence weconsidered the minimum edit distance (MED) (Wag-ner and Fischer, 1974), which has been used forspelling correction and in speech recognizer evalu-ations (termed ?accuracy?
there) in the past.
TheMED computes the similarity of strings by takinginto account the minimum number of editing oper-ations (substitutions, insertions, deletions) neededto transform one string into the other (see alsoJurafsky and Martin (2000, p.153ff.
and p.271)).We included MED into our feature set by comput-ing one value for each editing direction.
Both val-ues share the number of editing operations but theydiffer when anaphor and antecedent have a differ-ent length.
The features ante med (16) and ana med(17) are computed from the number of substitutions< , insertions = , deletions fl and the length of the po-tential antecedent fi or anaphor  as in Table 6.4.4 Improved ResultsThe inclusion of the MED features 16 and 17 led to asignificant improvement (Table 7).
The F-measure isimproved to 67.98%, an improvement of about 8%.Considering the classifiers trained and tested on thedata partitions according to ana npform, we can seethat the improvements mainly stem from defNP andNE.
With respect to definite NPs we gained about18% F-measure, with respect to proper names about11% F-measure.
For pronouns, the results did notvary much.4.5 MUC-style resultsIt is common practice to evaluate coreference reso-lution systems according to a scheme originally de-veloped for MUC evaluation by (Vilain et al, 1995).In order to be able to apply it to our classifier, wefirst implemented a simple reference resolution al-gorithm.
This algorithm incrementally processes areal text by iterating over all referring expressions.Upon encountering a possibly anaphoric expression,it moves upwards (i.e.
in the direction of the be-ginning of the text) and submits each pair of po-tential anaphor and potential antecedent to a clas-sifier trained on the features described above.
Forthe reasons mentioned in Section 4.2, the value ofthe ana semanticclass attribute is reset to missingif the potential anaphor is a pronominal form.
Thealgorithm then selects the first (if any) pair whichthe classifier labels as coreferential.
Once a texthas been completely processed, the resulting coref-erence classes are evaluated by comparing them tothe original annotation according to the scheme pro-posed by (Vilain et al, 1995).
This scheme takesinto account the particularities of coreference reso-lution by abstracting from the question if individ-ual pairs of anaphors and antecedents are found.Instead, it focusses on whether sets of coreferringexpressions are correctly identified.
In contrast tothe experiments reported in Section 4.2 and 4.4, ouralgorithm did not use a C5.0, but a J484 decisiontree classifier, which is a Java re-implementation of4Part of the Weka machine learning library, cf.http://www.cs.waikato.ac.nz/ml/wekaP R FdefNP 69.26% 22.47% 33.94%NE 90.77% 65.68% 76.22%PDS 25.00% 11.11% 15.38%PPER 85.81% 77.78% 81.60%PPOS 82.11% 87.31% 84.63%all 84.96% 56.65% 67.98%Table 7: Improved results using features 2?17.C4.5.
This was done for technical reasons, J48 beingmore easily integrated into our system.
Accompany-ing experimentation revealed that J48?s performanceis only slightly inferior to that of C5.0 for our data.Again using 10-fold cross validation, we obtainedthe results given in Table 8.5 ConclusionsIn this paper we described the influence of featuresbased on the minimum edit distance (MED) be-tween anaphor and antecedent on reference resolu-tion.
Though previous research used several differ-ent string similarity measures, to our knowledge, theMED feature was not used in previous work on ref-erence resolution.
We showed that the feature led toa significant improvement over the standard set offeatures we started with.
It improved the recall fordefinite NPs and proper names considerably with-out losing too much precision.
Also, it did not haveany negative effect on pronouns.
The MED featureis easy to compute and language and domain inde-pendent.
In contrast, features used in previous workwere either language dependent (e.g.
the weak stringidentity feature as used by Soon et al (2001)), do-main dependent (their alias feature or similar fea-tures used by Cardie and Wagstaff (1999)), or reliedon information on the syntactic structure (Vieira andPoesio, 2000).
We consider the MED feature as ageneralization of these features.
It is more abstractthan the features used by other researchers but deliv-ers similar information.We showed that our approach performs very wellfor personal and possessive pronouns and for propernames.
For definite NPs, although they benefit fromthe MED features as well, there is still much roomfor improvement.
We are curious to investigate fur-ther ?cheap?
features and compare them to whatcould be obtained when taking domain or worldknowledge into account.Features P R F2 ?
15 81.31% 47.44% 59.92%2 ?
17 80.17% 55.14% 65.34%Table 8: MUC-style results with different features.Acknowledgments.
The work presented here hasbeen partially funded by the German Ministry ofResearch and Technology as part of the EMBASSIproject (01 IL 904 D/2, 01 IL 904 S 8), by SonyInternational (Europe) GmbH and by the KlausTschira Foundation.
We would like to thank our an-notators Anna Bjo?rk Nikula?sdo?ttir, Berenike Loosand Lutz Wind.ReferencesChinatsu Aone and Scott W. Bennett.
1995.
Evaluatingautomated and manual acquisition of anaphora reso-lution strategies.
In Proceedings of the 33rd AnnualMeeting of the Association for Computational Linguis-tics, Cambridge, Mass., 26?30 June 1995, pages 122?129.Thorsten Brants.
2000.
TnT ?
A statistical Part-of-Speech tagger.
In Proceedings of the 6th Confer-ence on Applied Natural Language Processing, Seat-tle, Wash., 29 April ?
4 May 2000, pages 224?231.Claire Cardie and Kiri Wagstaff.
1999.
Noun phrasecoreference as clustering.
In Proceedings of the 1999SIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora, Col-lege Park, Md., 21?22 June 1999, pages 82?89.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):249?254.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A sta-tistical approach to anaphora resolution.
In Proceed-ings of the Sixth Workshop on Very Large Corpora,Montre?al, Canada, pages 161?170.Daniel Jurafsky and James H. Martin.
2000.
Speech andLanguage Processing.
Prentice Hall, Upper SaddleRiver, N.J.Joseph F. McCarthy and Wendy G. Lehnert.
1995.
Us-ing decision trees for coreference resolution.
In Pro-ceedings of the 14th International Joint Conference onArtificial Intelligence, Montre?al, Canada, 1995, pages1050?1055.Wojciech Skut and Thorsten Brants.
1998.
A maximum-entropy partial parser for unrestricted text.
In 6thWorkshop on Very Large Corpora, Montreal, Canada,pages 143?151.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Renata Vieira and Massimo Poesio.
2000.
Anempirically-based system for processing definite de-scriptions.
Computational Linguistics, 26(4):539?593.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof the 6th Message Understanding Conference (MUC-6), pages 45?52, San Mateo, Cal.
Morgan Kaufmann.Robert A. Wagner and Michael J. Fischer.
1974.
Thestring-to-string correction problem.
Journal of theACM, 21(1):168?173.
