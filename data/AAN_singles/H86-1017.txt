LIVING UP  TO EXPECTAT IONS:COMPUTING EXPERT RESPONSES 'Aravind Joshi and Bonnie WebberDepartment of Computer and Information ScienceMoore School/D2University of PennsylvaniaPhiladelphia PA 19104Ralph M. Weischedel 2Department of Computer & Information SciencesUniversity of DelawareNewark DE 19716ABSTRACTIn cooperative man-machine interaction, it is necessary but not sufficient for a system to respondtruthfully and informatively to a user's question.
In particular, if the system has reason to believe that itsplanned response might mislead the user, then it must block that conclusion by modifying its response.This paper focusses on identifying and avoiding potentially misleading responses by acknowledging typesof "informing behavior" usually expected of an expert.
We attempt o give a formal account of several-types of assertions that should be included in response to questions concerning the achievement of somegoal (in addition to the simple answer), lest the questioner otherwise be misled.1.
Introduction\]In cooperative man-machine interaction, it is necessary but not sufficient for a system to respondtruthfully and informatively to a user's question.
In particular, if the system has reason to believe that itsplanned response might mislead the user to draw a false conclusion, then it must block that conclusion bymodifying or adding to its response.Such cooperative behavior was investigated in \[5\], in which a modification of Grice% Mazim of Quality- "Be truthful" - is proposed:If you, the speaker, plan to say anything which may imply for the hearer something that youbelieve to be false, then provide further information to block it.This behavior was studied in the context of interpreting certain definite noun phrases.
In this paper, weinvestigate this revised principle as applied to responding to users' plan-related questions.
Our overall aimis to:1. characterize tractable cases in which the system as respondent (R) can anticipate thepossibility of the user/questioner (Q) drawing false conclusions from its response and hencealter it so as to prevent his happening;2. develop a formal method for computing the projected inferences that Q may draw from alThis work is partially supported by NSF Grants MCS 81.-07200, MCS 83-.052"21, Lad \[ST 8~11400.2At present visiting the Depm.rtment of Computer ud  lrdrormation Science, University of Pennsylvania PA 10104.179particular response, identifying those factors whose presence or absence catalyzes theinferences;3. enable the .system to generate modifications of its response that can defuse possible falseinferences and that may provide additional useful information as well.In responding to any question, including those related to plans, a respondent (R) must conform toGrice's first Maxim of Quantitlt as well as the revised Maxim of Quality stated above:Make your contribution as informative as is required (for the current purposes of theexchange).At best, if R's response is not so informative, it may be seen as uncooperative.
At worst, it may end upviolating the revised Maxim of Quality, causing Q to conclude something R either believes to be false ordoes not know to be true: the consequences could be dreadful.
Our task is to characterize more preciselywhat this expected informativeness consists of.
la question answering, there seem to be several quitedifferent types of information, over and beyond the simple answer to a question, that are neverthelessexpected.
For example,1.
When a task-related question is posed to an expert {R), R is expected to provide additionalinformation that he recognizes as necessary to the performance of the task, of which theqt:estioner (Q) may be unaware.
Such response behavior was discussed and implemented byAlien \[1\] in a system to simulate a train information booth attendant responding to requestsfor schedule and track information.
In this case, not providing the expected additionalinformation is simply uncooperative: Q won't conclude the train doesn't depart at any time if?~ fails to volunteer one.
: ~=~ i :~ respect o discussions a ld /or  arguments, a speaker contradicting another is expected tosupp~'~ his contrary contention.
Again, failing to provide support would simply be viewed asu.~c~,o~erative \[2, 3\].. With respect o an expert's responses to questions, if Q expects that R would inform him of P!f P were true, then Q may interpre t R's silence regarding P as implying P is not true.
s Thus ifI:~ k.qows P to be true, his silence may lead to Q's being misled.
This third type of expectedinformativeness i  the basis for the potentially misleading responses that we are trying toavoid a.~d that constitute the subject of this paper.What is of interest o us is characterizing the Ps that Q would expect an expert R to inform him of, ifthey hold.
Notice that these Ps differ from script-based expectations \[8\], which are based on what istaken to be the ordinary course of events in a situation.
In describing such a situation, if the speakerd,,esn't explicitly reference some element P of the script, the listener simply assumes it is true.
On theother hand, the Ps of interest here are based on normal cooperative discourse behavior, as set out inGrice's maxims.
If the speaker doesn't make explicit some information P that the listener believes hewould possess and inform the listener of, the listener assumes it is false.In this paper, we attempt to give a formal account of a subclass of Ps that should be included (inaddition to the simple answer) in response to questions involving Q's achieving some goal 4 - e.g., ?
Can I3This is an interactional version of what Reiter \[IS l has called the "Closed World Assumption" and what McCarthy \[O\] hasdiscussed in the context of "Circumscription'.4A companion paper \[6 !
discusses responses which may mislead Q into assuming some default which R knows not to hold.Related work \[4\] discusses providing indirect or  modified responsel to yes/no questions where a direct response, whiletruthful, might mislead Q.180drop CIS5777", "I want to euroi in CIS5777", 'How do I get to/ViarGh Creek on the Exl~ressway?
', etc.,lest that rzsponse otherwise mislead Q.
In this endeavor, our first step is to specify that knowledge that anexpert R must have in order to identify the Ps that Q would expect o be informed of, in response to hisquestion.
Our second step is to formalize that knowledge and show how the system can use it.
Our thirdstep is to show how the system can modify its planned response so as to convey those Ps.
In this paper,Section 2 addresses the first step of this process and Sections 3 and 4 address the second.
The third stepwe mention here only in passing.2.
Factors in Computing Likely Informing Behavior\]Before discussing the factors involved in computing this desired system behavior, we want to callattention to the distinction we are drawing between actions and events, and between the stated goal of aquestion and its intended goal.
We limit the term action to things that Q has some control over.
Thingsbeyond Q's control we will call events, even if performed by other agents.
While events may be likel3/oreven necessary,, Q and R nevertheless can do nothing more than wait for ~hem to happen.
This distinctionbetween ~ctions and events shows up in R's response behavior: if an action is needed, R can suggest hatQ perform it.
If an event is, R can do no more than inform Q.Our second distinction is between the stated goal or "S-goal = of a request and its intended goal or=I-goal =.
The former is the goal most directly associated with Q's request, beyond that Q know theinformation.
That is, we take the S-goal of a request to be the goal directly achieved by using theinformation.Underlying the stated goal of a request hough may be another goal that the speaker wants to achieve.This intended goal or ?l-go3!"
may be related to the S-goal of the request in any of a number of ways:?
The l-goal may be the same as the S-goal.?
The l-goal may be more abstract than the S-goal, which addresses only part of the I-goal.
(This is the standard goal/sub-goal relation found in hierarchical planning \[14\].)
For example,Q's S-goal may be to delete some files (e.g., *How can I delete all but the last version ofFOO.MSS??
), while his l-goal may be to bring his file usage under quota.
This more abstractgoal may also involve archiving some other files, moving some into another person's directory,etc.
* The S-goai may be an enablinK condition for the I-goal.
For example, Q's S-goal may be to getread/write access to a file, while his I-goal may be to alter it.The l-goal may be more ~eneral than the S-goal.
For example, Q's S-goal may be to know howto repeat a control-N, while his l-goal may be to know how to effect multiple sequentialinstances of a control character.Conversely, the l-goal may be more specific than the S-goal - for example, Q's S-goal may beto know how to send files to someone on another machine, while his I-goal is just to send aparticular file to a local network user, which may allow for a specialized procedure.Inferring the l-goal corresponding to an S-goal is an active area.of research \[1, Carberry83, 10, 11\].
Weassume for the purposes of this paper that R can successfuUy do so.
One problem is that the relationshipthat Q believes to hold between his S-goal and his I-goal may not actually hold: for example, the S-goal181may not fulfill part of the bgoal, or it may not instantiate it, or it may not be a pre-condition for it.
Infact, the S-goal may not even be possible to effect!
This failure, under the rubric "relaxing theappropriate-query assumption' ,  is discussed in more detail in \[10, nl.
It is also reason for augmenting R'sresponse with appropriate Ps, as we note informally in this section and more formally in the next.Having drawn these distinctions, we now claim that in order for the system to compute both a directanswer to Q's request and such Ps as he would expect to be informed of, were they true, the system mustbe able to draw upon knowledge/beliefs about?
the events or actions, if any, that can bring about a goal?
their enabling conditions?
the likelihood of an event occuring or the enabling conditions for an action holding, withrespect o a state?
ways of evaluating methods of achieving goals - for example, with respect o simplicity, otherconsequences ( ide effects), likelihood of success, etc.?
general characteristics of cooperative xpert behaviorThe roles played by these different types of knowledge (as well as specific examples of them) are wellillustrated in the next section.3.
Formalizing Knowledge for Expert ResponseIn this section we give examples of how a formal model of user beliefs about cooperative xpert behaviorcan be used to avoid misleading responses to task-related questions - in particular, what is a veryrepresentative s t of questions, those of the form =How do I do X?
=.
Although we use logic for the modelbecause it is clear and precise, we are not proposing theorem proving as the means of computingcooperative behavior.
In Section 4 we suggest a computational mechanism.
The examples are from adomain of advising students and involve responding to the request ?I want to drop CIS577".
The set ofindividuals includes not onlychange states, we representcorresponding to events orconvenient:qRScRB(P)RBQB(P)admissible(4S))likely(a,S)holds(P,S)want(x,P)students, instructors, courses, etc.
but also states.
Since events and actionshem as (possibly parameterized) functions from states to states.
All termsactions will be underlined.
For these examples, the following notation isthe  userthe expertthe current state of the studentR believes proposition PR believes that Q believes Pevent/action e can apply in state Sa is a likely event/action in state SP, a proposition, is true in Sx wants P to be trueTo encode the preconditions and consequences of performing 2n action, we adopt an axiomatization ofSTRIPS operators due to \[Chester83, 7 15\].
The preconditions on an action being applicable are encodedusing "holds" and "admissible" (essentially defining "admissible').
Namely, if cl ..... ca are precondltionson an action a,182holds(cl,s) &...~ holds(ca,s) =~ admissible(a(s))a's immediate consequences pl ..... pm can be ~tated asadmissible(a(s)) =, holds(pl, a(s)) a ... & holds(pm, a(s))A frame axiom states that only pl ..... pm have changed.-~(p=pl) ~ ... ~ ~(p fpm)  & holds(p,s)3 ,% admissible(a(s)) t hoids,a(s))In particular, we can state the preconditions and consequences of dropping CIS577.
(h acd n arevariables, while C stands for CIS577.
)RB(holds(enrolled(h, C, fall), n) & holds(date(n)<Novl6, n)admissible( drop (h, CX . )
) )RB( admissible( drop(h, CX n ) ) =~ holds(-~enrolled( h,C,fall),drop(h, CX n ) ) )RBl-(p=enrolled(h,C,fall)) admissible(drop(h,C)(n)) holds(p,.
)holds(p,drop(h,C)(n)))Of course, this only partially solves the frame problem, since there will be implications of pl ..... pm ingeneral.
For instance, it is likely that one might have an axiom stating that one receives a grade in acourse only if the individual is enrolled in the course.Q's S-goal in dropping CIS577 is not being in the course.
By a process of reasoning discussed in \[10, 11\],R may conclude that Q's likely intended goal (l-goal) is not failing it.
That is, R may believe:RBQB(holds(-ffaii(Q,C), drop(Q,C~(Sc)))5RB(want(Q,-4aii(Q,C))What we claim is: (1) R must give a truthful response addressing at least Q's S-goal; (2) in addition, Rmay have to provide information in order not to mislead Q; and (3) R may give additional information tobe cooperative in other ways.
In the subsections below, we enumerate the cases that R must check ineffecting (2).
In each case, we give both a formal representation of the additional information to beconveyed and a possible English gloss.
In that gloss, the part addressing Q's S-goal wiil appear in normaltype, while the additional information will be underlined.For each case, we give two formulae: a statement of R's beliefs about the current situation and anaxiom stating R's beliefs about Q's expectations.
Formulae of the first type have the form RB(P).Formulae of the second type relate such beliefs to performing an informing action.
They involve astatement of the form~lP l  =~ likely(i, Se),where i is an informing act.
For example, if R believes there is a better way to achieve Q's goal, R islikely to inform Q of that better way.
Since it is assumed that Q has this belief, we haveQB( RB\[P\] = likely(i, Sc)).Sit will also be the ease that RBQB(admlssible(drop(Q,C~S?)))
if Q's asks "How can !
drop CIS5777", but not if he asks"Can i drop CIS577f'.
in the latter e ra ,  Q must of course believe that it may be admissible, or why ask the question.
!aeither ease, R's subsequent behavior dot~a't seem contingent on hil beliefs ab '~ '~ beliefs about admissibility.183where we can equate ?Q believes i is likely" with "Q expects i."
Since R has no direct access to Q'sbeliefs, this must be embedded in R's model of Q's belief space.
Therefore, the axioms have the form(modulo quantifier placement)RBQB( RB\[P l =, likely{i, So) ).An informing act is meant to serve as a command to a natural language generator which selectsappropriate i xical items, phrasing, etc.
for a natural anguage utterance.
Such an act has the forminform-that(R,Q,P) R informs Q that P istrue.3.1.
Fa i lu re  o f  enabl ing eondli~lonaSuppose that it is past the November 15th deadline or that the official records don't show Q enrolled inCIS577.
Then the enabling conditions for dropping it are not met.
That is, R believes Q's S-goal cannot beachieved from So.\[1\] RB(want(Q,-ffail(Q,C)) & -,admissible(drop(Q,C~Sc)))Thus R initially plans to answer "You can't drop CIS577".
Beyond this, there are two possibilities.3.1.1.
A wayIf R knows another action b that would achieve Q's goals (cf.
formula \[2\]), Q would expect to beinformed about it.
If not so informed, Q may mistakenly conclude that there is no other way.
Formula\[3\] states this belief that R has about Q's expectations.I21 RB((3b)\[admissible(b(Sc)) & holds(-,fail(Q,C), b(Sc))i)\[31 RBQB(RBIwant(Q,-faiI(Q,C)) & -,admissible(drop(Q(C\](Sc)) i &nB\[(3b)\[admissible(b(Se)) ~ holds(--faii(Q,C),6(~c))l\]=* likely(inform-that(R, Q(fib) \[admis~ibh:(b(Sc)) e; hold~Ofail(Q,C),b(Sc))can(Q,b)),Sc)\])R's full response is therefore "You can't drop 577; you can b."
For instance, bcould be changing status toauditor, which may be performed until December I.3.1.2.
No wayIf R doesn't know of any action or event that could achieve Q's goal (cf.
\[4\]), Q would expect to be soinformed.
Formula \[5\] states this belief about Q's expectations.\[4\] RB(-,(3a)Iadmissible(a(Sc)) & holds(-,fail(q,C),a(Sc))l)\[5\] RBqB(RB(want(q,--fail(q,c)) & -,(3a)\[admissibleia(S?))"
~ holdsl--ffail(q,c), a(Sc))\])=~ likely(inform-that(R, Q -(3a )\[admissible(a(Se))8 hol~('.$ait(Q.C),a(Sc)}\]),Se))To say only that Q cannot drop the course does not exhibit expert cooperative behavior, since Q would beuncertain as to whether R had considered other alternatives.
Therefore, R's full response is "You can'tdrop 577; there isn ~ anything you can do to prevent failing.=Notice that R's analysis of the situation may turn up additional information which a cooperative xpert184could provide that does not involve avoiding misleading Q.
For instance, R could indicate enablingconditions that prevent here being a solution: suppose the request o drop the course is made after theNovember 15th deadline.
Then R would believe the following, in addition to \[1\]RB(holds(enrolled(Q,C,fall),Sc)/~ hold6(date(Sc)>Nov15,Sc DMore generally, we need a schema such as the following about Q's beliefs:RBQB(RB\[want(Q,'~fail(Q,C D& (holds(Pl, S) &...& hoids(Pn, S) =~ admissible(a(S)))& (-~tholds(Pi, S), for some Pi above)\]=~ iik ely ( in f orm-t hat (R, Q,-,hol ds(Pi, S )),S ) )In this ease the response should be "'You can't drop 577; Pi isn~ true."
Alternatively, the languagegenerator might paraphrase the whole response as, "if Pi were true, you could drop.
"Of course there are potentially many ways to try to achieve a goal: by a single action, by a singleevent, or by an event and an action ....
In fact, the search for a sequence of events or actions that wouldachieve the goal may consider many alternatives.
If all fail, it is far from obvious which blocked conditionto notify Q of, and knowledge is needed to guide the choice.
Some heuristics for dealing with that problem~ .. given in \[12\].3.2.
An nonproduct ive  actSuppose the proposed action does not achieve Q's l-goal, cL \[6\].
For example, dropping the course maystill mean that failing status would be recorded as a WF (withdrawal while failing).
R may initially plan toanswer "You can drop 577 by ...'.
However, Q would expect o be told that his proposed action does notachieve his l-goal.
Formula \[7\] states R's belief about this expectation.\[6\] RB(-holds(-fail(Q,C), drop(Q,C\](Sc)) & admissible(drop(Q,C}(Sc)) )\[7\] RBQB(RB\[ want(Q,-,fail(Q,c)) & -,holds(-fail(Q,C),drop(Q,C\](Sc))I~ admissible( drop (Q, C\]( Sc ) )\]likely( in f orm-t h at (Fl, Q ,-hold~(-/ail(Q,C),drop(Q,C)(Sc))),Sc))R's full response is, "You can drop 577 by ....
However, you will still fail."
Furthermore, given thereasoning in section 3.1.1 above, R's full response would also inform Q if there is an action b that the usercan take instead.3.3 .
A bet ter  waySuppose R believes that  there is a better way to achieve Q's 1-goal, cf.
\[8\] - for example, taking anincomplete to have additional time to perform the work, and thereby not losing all the effort Q hasalready expended.
Q would expect that R, as a cooperative xpert, would inform him of such a betterway, ef.
\[9 I.
If R doesn't, R risks misleading Q that there isn't one.\[8\] RB((3b)\[holds(-fail(Q,C), b(Sc}) &admissible(b(Sc)) & better(b,drop(Q,C)(Sc\])\])I91 RBQB(RB\[want(Q,-4ail(Q,C))\] ,~RB\[(3b)\[holds(-~fail(Q,C), b(Sc)) & admissible(b(Sc))better(b,drop(Q,C)(Sc))=* like ly ( i n form -t h a t (R, Q ,185(3b )\[holda(-., f ail(Q,C),b(Sc)) ~Y admissible(b(Se))better(b, drop(Q,C)(Sc))l\], Se)\])R's direct response is to indicate how f can be done.
R's full response includes, in addition, "b is a betterway.
~Notice that if R doesn't explicitly tell Q that he is presenting a better way (i.e., he just presents themethod), Q may be misled that the response addresses his S-goal: i.e., he may falsely conclude that he isbeing told how to drop the course.
(The possibility shows up clearer in other examples - e.g., if R omitsthe first sentence of the response belowQ: How do I get to Marsh Creek on the Expressway?R: It's faster and shorter to take Route 30.
Go outLancaster Ave until ....Thus even when adhering to expert response behavior in terms of addressing an I-goal, we must keep thesystem aware of potentially misleading aspects of its modified response as well.Note that R may believe that Q expects to be told the best way.
This would change the second axiom toinclude within the scope of the existential quantifier(Va){-,(a=b) =~ \[holds(-,fail(Q,C), a(Sc)) ,.% admissible(a(Sc)) & better(b,a)\]}3.4.
The  only waySuppose there is nothing inconsistent about what the user has proposed - i.e., all preconditions are metand it will achieve the user's goal.
R's direct response would simply be to tell Q how.
However, if Rnotices that that is the only way to achieve the goal (of.
\[10\]), it could optionally notify Q of that, el.
\[111.\[101 RB((3la)\[holds(-,fail(q,C),a(Sc)) & admissible(a(Sc)) & a--=drop(Q,C)(Sc~)\[1 X l RBQB(RB(want(Q,-fail(Q,C)))& RB((3la)\[holds(-ffail(Q,C), a(S?))
& admissible(a(Sc)) & a=drop(Q,C)(Sc~)=~ likely(inform-that(R, Q(3!a )\[holds(- f ait(Q,C),a(Sc))ff admi,eible(a(Sc)) ~ a=drop(Q,C)(Sc)\]), So))R's full response is "You can drop 577 by .... That is the only way to prevent failing."3.5.
Someth ing  Turn ing  UpSuppose there is no appropriate action that Q can take to achieve his I-goal.
That is,RB( ~(3 a)\[admissible(a(Se)) & holds(g, a.\[Sc))\])There may still be some event e out of Q's control that could bring about the intended goal.
This givesseveral more cases of R's modifying his response.3.5.1.
Unl ikely eventIf e is unlikely to occur (cf.
\[12\]), Q would expect R to inform him of e, while noting its implausibility, cf.\[131\[12\] RB((3e)\[admissible(e(Sc)) & holds(-,fai!
(Q,C), e(Sc)),% -,likely(e, Sc)!
)186\[,3l RBQB(RB(want(Q,-f~(Q,C)) & oRB(-(\]a)\[~missible(a(Sc)) & hold,,(-fail(q,c),a(Se))\](3e)ladmissibl~e(Se)) J~ holds(-fail(Q,C),e(Sc)),~ likely(e,Sc)l)=~ likely(inform-that(R, Q(3 e ffadmissible(e, Sc) ~ holds(- f ail(Q,C), e(Sc))- -  '-,J likelJl(e, Se)\]), So))Thus R's full response is, "You can't drop 577.
I f  e occurs, you will not fail  577, but e is unlikely."3.5.2.
L ikely eventIf the event e is likely (cf.
\[14\]), it does not seem necessary to state it, but it is certainly safe to do so.
Aformula representing this case follows.\[14\] RB((3 e)\[a.df missihle(e(Sc))holds(fail(q,C),4Sc)) & likely(e,Sc)DR's beliefs about Q's expectations are the same as the previous case except that likely(e, Sc) replaces-likely(e, Sc).
Thus R's full response may be "You can't drop 577.
However, e is likely to occur, in whichcase you will not fail  577. s3.5.3.
Event  fol lowed by act ionIf event e brings about a state in which the enabling conditions of an effective action a are true, cf.
\[15\]\[15\] RB((3e)(3a)\[~lmissible(e(Sc)) & admissible(a(~Sc))) &holds(-~rail(q,c), a(e(Sc)))\])!,81 RBqB(12B((3e)(3a)\[want(q,-,fail(q,c)) & admissible(e(Sc))& admi.~ible(a(e(Sc))) & holds(-fail(Q,C),a(e(Sc)))\])=~ likely( in f orm-that(R,Q,Oe)Oa ) l oles(-, f ,,i fQ, C),a(e(Sc))))a dmi$sible(a(?
(Sc))\])),Sc))then the same principles about informing q of the likelihood or unlikelihood of e apply as they did before.In addition, R must inform Q of a, cf.
\[16\].
Thus R's full response would be "You can't drop 577.
I f  ewere to occur, which is (un)likely, you could a and thus not fail  577."4.
ReasoningOur intent in using logic has been to have a precise representation language whose syntax informs R'sreasoning about Q's beliefs.
Having computed a full response that conforms to all these expectations, Rmay go on to 'trim' it according to principles of brevity that we do not discuss here.Our proposal is that the informing behavior is "pre-compiled'.
That is, R does not reason explicitlyabout Q's expectations, but rather has compiled the conditions into a case analysis similar to adiscrimination net.
For instance, we can represent informally several of the cases in section 3.If admizsibl~(drop(~f,C~Se))then if holds(fail(q,C),d,' (Q,C Sc))then  begin nonproductive acti f  (3b)\[admissible(b(Sc)) ~fi'olds(-f~il(Q,C),b(Sc))\]then  a wayelae n...o wayendelse i f  (3b)\[admissible(/~Sc)) &187holds(-~fail(Q,C),b(Sc)) & better(b,f)\]then  a betterelse i f  (3 b)\[ad~s~e(b(Sc}} & holds(-~fail(Q,C), b(Sc))\]then  a wayelse n.._o wayNote that we are assuming that R assumes the most demanding expectations by Q.
Therefore, R canreason solely within its own space without missing things.5.
Conclus ionSince the behavior of expert systems will be interpreted in terms of the behavior users expect ofcooperative human experts, we (as system designers} must understand such behavior patterns so as toimplement them in our systems.
If such systems are to be truly cooperative, it is not sufficient for them tobe simply truthful.
Additionally, they must be able to predict limited classes of false inferences that usersmight draw from dialogue with them and also to respond in a way to prevent hose false inferences.
Thecurrent enterprise is a small but non-trivial step in this direction.
\[n addition to questions about achievinggoals, we are investigating other cases where a cooperative xpert should prevent false inferences byanother agent, including preventing inappropriate default reasoning \[6, J~,VW84nonmon\].Future work should includes identification of additional eases where an expert must prevent false inferences by anotheragent,?
formal statement of a general principle for constaining the search for possible false inferences,ands design of a natural language planning component to carry out the informing acts ~sumed inthis paper.ACKNOWLEDGEMENTSWe would like to thank Martha Pollack, Deborah Dahl, Julia Hirschberg, Kathy McCoy and the AAAIprogram committee reviewers for their comments on this paper.188References1.
Allen, J. Recognizing Intentions from Natural Language Utterances.
In Computational Models ofDiscourse, M. Brady, Ed., MIT Press, Cambridge MA, 1982.2.
Birnbaum, L., Flowers, M. & McQuire, R. Towards an AI Model of Argumentation.
Proceedings of1980 Conference, American Assoc.
for Artificial Intelligence, Stanford CA, August, 1980.3.
Cohen, R. A Theory of Discourse Coherence for Argument Understanding.
Proceedings of the 1984Conference, Canadian Society for Computational Studies of Intelligence, University of Western Ontario,London Ontario, May, 1984, pp.
6-10.4.
Hirschberg, J. Scalar lmpllcature and Indirect Responses in Question-Answering.
Proc.
CSCSI-84,London, Ontario, May, 1984.5.
Joshi, A.K.
Mutual Beliefs in Question Answering Systems.
In Mutual Belief, N. Smith, Ed.,Academic Press, New York, 1982.e.
Joshi, A., Webber, B.
& Weischedel, R. Preventing False Inferences.
Proceedings of COLING-84,Stanford CA, July, 1984.7.
Kowalski, Robert.
Logic for Problem Solving.
North Holland, New York, 1979.8.
Lchnert, W. A Computational Theory of Human Question Answering.
In Elements of Discourse\[r:,~,-~tanding, A. Joshi, B. Webber & 1.
Sag, Ed., Cambridge University Press, 1981.~, ~,JicCarthy, John.
"Circumscription - A Form of Non-Monotonic Reasoning ?.
Artificial Intdlig~ce~ (1980), 27-39.10.
Pollack, Martha E. Goal Inference in Expert Systesm.
MS-CIS-84-07, University of Pennsylvania,1984.
Doctoral dissertaion proposal..I I .
Pollack, M. Good Answers to Bad Questions.
Proc.
Canadian Society for Computational Studies ofIntelligence (CSCSI), Univ.
of Western Ontario, Waterloo, Canada, May, 1984.12.
Ramshaw, Lance and Ralph M. Weischedel.
Problem Localization Strategies for PragmaticsProcessing in Natural Language Front Ends.
Proceedings of COLING-84, July, 1984.13.
Reiter, R. Closed World Databases.
In Logic and Databases, H. Gallaire & J. Minker, Ed., PlenumPress, 1978, pp.
149-177.14.
Sacerdoti, Earl D.. A Structure for Plans and Behavior.
American Elsevier, New York, 1977.15.
Warren, D.H.D.
WARPLAN: A System for Generating Plans.
Proceedings of IJCAI-75, August,1975.189
