Learning Question ClassifiersXin Li Dan RothDepartment of Computer ScienceUniversity of Illinois at Urbana-Champaignfxli1,danrg@uiuc.eduAbstractIn order to respond correctly to a free form factual ques-tion given a large collection of texts, one needs to un-derstand the question to a level that allows determiningsome of the constraints the question imposes on a pos-sible answer.
These constraints may include a semanticclassification of the sought after answer and may evensuggest using different strategies when looking for andverifying a candidate answer.This paper presents a machine learning approach toquestion classification.
We learn a hierarchical classi-fier that is guided by a layered semantic hierarchy of an-swer types, and eventually classifies questions into fine-grained classes.
We show accurate results on a large col-lection of free-form questions used in TREC 10.1 IntroductionOpen-domain question answering (Lehnert, 1986;Harabagiu et al, 2001; Light et al, 2001) and storycomprehension (Hirschman et al, 1999) have be-come important directions in natural language pro-cessing.
Question answering is a retrieval task morechallenging than common search engine tasks be-cause its purpose is to find an accurate and conciseanswer to a question rather than a relevant docu-ment.
The difficulty is more acute in tasks such asstory comprehension in which the target text is lesslikely to overlap with the text in the questions.
Forthis reason, advanced natural language techniquesrather than simple key term extraction are needed.One of the important stages in this process is analyz-ing the question to a degree that allows determiningthe ?type?
of the sought after answer.
In the TRECcompetition (Voorhees, 2000), participants are re-quested to build a system which, given a set of En-glish questions, can automatically extract answers(a short phrase) of no more than 50 bytes from a5-gigabyte document library.
Participants have re- Research supported by NSF grants IIS-9801638 and ITR IIS-0085836 and an ONR MURI Award.alized that locating an answer accurately hinges onfirst filtering out a wide range of candidates (Hovyet al, 2001; Ittycheriah et al, 2001) based on somecategorization of answer types.This work develops a machine learning approachto question classification (QC) (Harabagiu et al,2001; Hermjakob, 2001).
Our goal is to categorizequestions into different semantic classes that imposeconstraints on potential answers, so that they canbe utilized in later stages of the question answeringprocess.
For example, when considering the ques-tion Q: What Canadian city has the largest popula-tion?, the hope is to classify this question as havinganswer type city, implying that only candidate an-swers that are cities need consideration.Based on the SNoW learning architecture, we de-velop a hierarchical classifier that is guided by a lay-ered semantic hierarchy of answer types and is ableto classify questions into fine-grained classes.
Wesuggest that it is useful to consider this classifica-tion task as a multi-label classification and find thatit is possible to achieve good classification results(over 90%) despite the fact that the number of dif-ferent labels used is fairly large, 50.
We observe thatlocal features are not sufficient to support this accu-racy, and that inducing semantic features is crucialfor good performance.The paper is organized as follows: Sec.
2 presentsthe question classification problem; Sec.
3 discussesthe learning issues involved in QC and presents ourlearning approach; Sec.
4 describes our experimen-tal study.2 Question ClassificationWe define Question Classification(QC) here to bethe task that, given a question, maps it to one ofk classes, which provide a semantic constraint onthe sought-after answer1.
The intension is that this1We do not address questions like ?Do you have a light?
?,which calls for an action, but rather only factual Wh-questions.classification, potentially with other constraints onthe answer, will be used by a downstream processwhich selects a correct answer from among severalcandidates.A question classification module in a question an-swering system has two main requirements.
First, itprovides constraints on the answer types that allowfurther processing to precisely locate and verify theanswer.
Second, it provides information that down-stream processes may use in determining answer se-lection strategies that may be answer type specific,rather than uniform.
For example, given the ques-tion ?Who was the first woman killed in the VietnamWar??
we do not want to test every noun phrasein a document to see whether it provides an answer.At the very least, we would like to know that thetarget of this question is a person, thereby reducingthe space of possible answers significantly.
The fol-lowing examples, taken from the TREC 10 questioncollection, exhibit several aspects of this point.Q: What is a prism?
Identifying that the target of thisquestion is a definition, strategies that are specific fordefinitions (e.g., using predefined templates) may be use-ful.
Similarly, in:Q: Why is the sun yellow?
Identifying that this questionasks for a reason, may lead to using a specific strategyfor reasons.The above examples indicate that, given that dif-ferent answer types may be searched using differentstrategies, a good classification module may helpthe question answering task.
Moreover, determin-ing the specific semantic type of the answer couldalso be beneficial in locating the answer and veri-fying it.
For example, in the next two questions,knowing that the targets are a city or country willbe more useful than just knowing that they are loca-tions.Q: What Canadian city has the largest population?Q: Which country gave New York the Statue of Liberty?However, confined by the huge amount of man-ual work needed for constructing a classifier for acomplicated taxonomy of questions, most questionanswering systems can only perform a coarse clas-sification for no more than 20 classes.
As a result,existing approaches, as in (Singhal et al, 2000),have adopted a small set of simple answer entitytypes, which consisted of the classes: Person, Loca-tion, Organization, Date, Quantity, Duration, Lin-ear Measure.
The rules used in the classificationwere of the following forms:?
If a query starts with Who or Whom: type Person.?
If a query starts with Where: type Location.?
If a query contains Which or What, the head nounphrase determines the class, as for What X questions.While the rules used have large coverage and rea-sonable accuracy, they are not sufficient to supportfine-grained classification.
One difficulty in sup-porting fine-grained classification is the need to ex-tract from the questions finer features that requiresyntactic and semantic analysis of questions, andpossibly, many of them.
The approach we adoptedis a multi-level learning approach: some of our fea-tures rely on finer analysis of the questions that areoutcomes of learned classifiers; the QC module thenapplies learning with these as input features.2.1 Classification StandardEarlier works have suggested various standards ofclassifying questions.
Wendy Lehnert?s conceptualtaxonomy (Lehnert, 1986), for example, proposesabout 13 conceptual classes including causal an-tecedent, goal orientation, enablement, causal con-sequent, verification, disjunctive, and so on.
How-ever, in the context of factual questions that areof interest to us here, conceptual categories do notseem to be helpful; instead, our goal is to se-mantically classify questions, as in earlier work onTREC (Singhal et al, 2000; Hovy et al, 2001;Harabagiu et al, 2001; Ittycheriah et al, 2001).The key difference, though, is that we attempt todo that with a significantly finer taxonomy of an-swer types; the hope is that with the semantic an-swer types as input, one can easily locate answercandidates, given a reasonably accurate named en-tity recognizer for documents.2.2 Question HierarchyWe define a two-layered taxonomy, which repre-sents a natural semantic classification for typicalanswers in the TREC task.
The hierarchy con-tains 6 coarse classes (ABBREVIATION, ENTITY,DESCRIPTION, HUMAN, LOCATION and NU-MERIC VALUE) and 50 fine classes, Table 1 showsthe distribution of these classes in the 500 ques-tions of TREC 10.
Each coarse class contains anon-overlapping set of fine classes.
The motiva-tion behind adding a level of coarse classes is that ofcompatibility with previous work?s definitions, andcomprehensibility.
We also hoped that a hierarchi-cal classifier would have a performance advantageover a multi-class classifier; this point, however isnot fully supported by our experiments.Class # Class #ABBREV.
9 description 7abb 1 manner 2exp 8 reason 6ENTITY 94 HUMAN 65animal 16 group 6body 2 individual 55color 10 title 1creative 0 description 3currency 6 LOCATION 81dis.med.
2 city 18event 2 country 3food 4 mountain 3instrument 1 other 50lang 2 state 7letter 0 NUMERIC 113other 12 code 0plant 5 count 9product 4 date 47religion 0 distance 16sport 1 money 3substance 15 order 0symbol 0 other 12technique 1 period 8term 7 percent 3vehicle 4 speed 6word 0 temp 5DESCRIPTION 138 size 0definition 123 weight 4Table 1: The distribution of 500 TREC 10 questionsover the question hierarchy.
Coarse classes (in bold) arefollowed by their fine class refinements.2.3 The Ambiguity ProblemOne difficulty in the question classification task isthat there is no completely clear boundary betweenclasses.
Therefore, the classification of a specificquestion can be quite ambiguous.
Consider1.
What is bipolar disorder?2.
What do bats eat?3.
What is the PH scale?Question 1 could belong to definition or dis-ease medicine; Question 2 could belong to food,plant or animal; And Question 3 could be a nu-meric value or a definition.
It is hard to catego-rize those questions into one single class and it islikely that mistakes will be introduced in the down-stream process if we do so.
To avoid this problem,we allow our classifiers to assign multiple class la-bels for a single question.
This method is better thanonly allowing one label because we can apply all theclasses in the later precessing steps without any loss.3 Learning a Question ClassifierUsing machine learning methods for question clas-sification is advantageous over manual methods forseveral reasons.
The construction of a manual clas-sifier for questions is a tedious task that requiresthe analysis of a large number of questions.
More-over, mapping questions into fine classes requiresthe use of lexical items (specific words) and there-fore an explicit representation of the mapping maybe very large.
On the other hand, in our learningapproach one can define only a small number of?types?
of features, which are then expanded in adata-driven way to a potentially large number of fea-tures (Cumby and Roth, 2000), relying on the abil-ity of the learning process to handle it.
It is hard toimagine writing explicitly a classifier that dependson thousands or more features.
Finally, a learnedclassifier is more flexible to reconstruct than a man-ual one because it can be trained on a new taxonomyin a very short time.One way to exhibit the difficulty in manually con-structing a classifier is to consider reformulations ofa question:What tourist attractions are there in Reims?What are the names of the tourist attractions in Reims?What do most tourists visit in Reims?What attracts tourists to Reims?What is worth seeing in Reims?All these reformulations target the same answertype Location.
However, different words and syn-tactic structures make it difficult for a manual clas-sifier based on a small set of rules to generalize welland map all these to the same answer type.
Goodlearning methods with appropriate features, on theother hand, may not suffer from the fact that thenumber of potential features (derived from wordsand syntactic structures) is so large and would gen-eralize and classify these cases correctly.3.1 A Hierarchical ClassifierQuestion classification is a multi-class classifica-tion.
A question can be mapped to one of 50 pos-sible classes (We call the set of all possible classlabels for a given question a confusion set (Goldingand Roth, 1999)).
Our learned classifier is basedon the SNoW learning architecture (Carlson et al,1999; Roth, 1998)2 where, in order to allow theclassifier to output more than one class label, wemap the classifier?s output activation into a condi-tional probability of the class labels and thresholdit.The question classifier makes use of a sequenceof two simple classifiers (Even-Zohar and Roth,2001), each utilizing the Winnow algorithm withinSNoW.
The first classifies questions into coarseclasses (Coarse Classifier) and the second into fineclasses (Fine Classifier).
A feature extractor auto-matically extracts the same features for each clas-sifier.
The second classifier depends on the first in2Freely available at http://L2R.cs.uiuc.edu/cogcomp/cc-software.htmlABBR, ENTITY,DESC,HUMAN,LOC,NUMABBR,ENTITYENTITY,HUMANENTITY,LOC,NUM DESCCoarse ClassifierFine Classifierabb,exp ind, plant dateabb, animal,food, plant?food,plant,ind,group?food, plant,city, state?definition,reason,?Map coarse classesto fine classesC0C1C2C3 abb,def animal,foodall possible subsetsof C0 wih size <= 5all possible subsetsof C2 with size <=5Figure 1: The hierarchical classifierthat its candidate labels are generated by expandingthe set of retained coarse classes from the first intoa set of fine classes; this set is then treated as theconfusion set for the second classifier.Figure 1 shows the basic structure of the hierar-chical classifier.
During either the training or thetesting stage, a question is processed along one pathtop-down to get classified.The initial confusion set of any question is C0=fc1; c2; : : : ; cng, the set of all the coarse classes.The coarse classifier determines a set of preferredlabels, C1= Coarse Classifier(C0), C1 C0so that jC1j  5.
Then each coarse class labelin C1is expanded to a fixed set of fine classesdetermined by the class hierarchy.
That is, sup-pose the coarse class ciis mapped into the setci= ffi1; fi2; : : : ; fimg of fine classes, then C2=Sci2C1ci.
The fine classifier determines a set ofpreferred labels, C3= Fine Classifier(C2) sothat C3 C2and jC3j  5.
C1and C3are the ul-timate outputs from the whole classifier which areused in our evaluation.3.2 Feature SpaceEach question is analyzed and represented as a listof features to be treated as a training or test exam-ple for learning.
We use several types of featuresand investigate below their contribution to the QCaccuracy.The primitive feature types extracted for eachquestion include words, pos tags, chunks (non-overlapping phrases) (Abney, 1991), named entities,head chunks (e.g., the first noun chunk in a sen-tence) and semantically related words (words thatoften occur with a specific question class).Over these primitive features (which we call?sensors?)
we use a set of operators to composemore complex features, such as conjunctive (n-grams) and relational features, as in (Cumby andRoth, 2000; Roth and Yih, 2001).
A simple scriptthat describes the ?types?
of features used, (e.g.,conjunction of two consecutive words and their postags) is written and the features themselves are ex-tracted in a data driven way.
Only ?active?
featuresare listed in our representation so that despite thelarge number of potential features, the size of eachexample is small.Among the 6 primitive feature types, pos tags,chunks and head chunks are syntactic features whilenamed entities and semantically related words aresemantic features.
Pos tags are extracted usinga SNoW-based pos tagger (Even-Zohar and Roth,2001).
Chunks are extracted using a previouslylearned classifier (Punyakanok and Roth, 2001; Liand Roth, 2001).
The named entity classifier isalso learned and makes use of the same technol-ogy developed for the chunker (Roth et al, 2002).The ?related word?
sensors were constructed semi-automatically.Most question classes have a semantically relatedword list.
Features will be extracted for this class ifa word in a question belongs to the list.
For exam-ple, when ?away?, which belongs to a list of wordssemantically related to the class distance, occurs inthe sentence, the sensor Rel(distance) will be ac-tive.
We note that the features from these sensors aredifferent from those achieved using named entitysince they support more general ?semantic catego-rization?
and include nouns, verbs, adjectives ratherthan just named entities.For the sake of the experimental comparison, wedefine six feature sets, each of which is an incre-mental combination of the primitive feature types.That is, Feature set 1 (denoted by Word) containsword features; Feature set 2 (Pos) contains featurescomposed of words and pos tags and so on; The fi-nal feature set, Feature set 6 (RelWord) contains allthe feature types and is the only one that containsthe related words lists.
The classifiers will be exper-imented with different feature sets to test the influ-ence of different features.
Overall, there are about200; 000 features in the feature space of RelWorddue to the generation of complex features over sim-ple feature types.
For each question, up to a coupleof hundreds of them are active.3.3 Decision ModelFor both the coarse and fine classifiers, the samedecision model is used to choose class labels fora question.
Given a confusion set and a question,SNoW outputs a density over the classes derivedfrom the activation of each class.
After ranking theclasses in the decreasing order of density values, wehave the possible class labels C = fc1; c2; : : : ; cng,with their densities P = fp1; p2; : : : ; png (where,Pn1pi= 1, 0  pi 1, 1  i  n).
As dis-cussed earlier, for each question we output the firstk classes (1  k  5), c1; c2; : : : ckwhere k satis-fies,k = min(argmint(tX1pi T ); 5) (1)T is a threshold value in [0,1].
If we treat piasthe probability that a question belongs to Class i,the decision model yields a reasonable probabilisticinterpretation.
We use T = 0:95 in the experiments.4 Experimental StudyWe designed two experiments to test the accuracy ofour classifier on TREC questions.
The first experi-ment evaluates the contribution of different featuretypes to the quality of the classification.
Our hi-erarchical classifier is trained and tested using oneof the six feature sets defined in Sect.
3.2 (we re-peated the experiments on several different trainingand test sets).
In the second experiment, we evalu-ate the advantage we get from the hierarchical clas-sifier.
We construct a multi-class classifier only forfine classes.
This flat classifier takes all fine classesas its initial confusion set and classifies a questioninto fine classes directly.
Its parameters and deci-sion model are the same as those of the hierarchicalone.
By comparing this flat classifier with our hi-erarchical classifier in classifying fine classes, wehope to know whether the hierarchical classifier hasany advantage in performance, in addition to the ad-vantages it might have in downstream processingand comprehensibility.4.1 DataData are collected from four sources: 4,500 Englishquestions published by USC (Hovy et al, 2001),about 500 manually constructed questions for a fewrare classes, 894 TREC 8 and TREC 9 questions,and also 500 questions from TREC 10 which servesas our test set3.These questions were manually labeled accord-ing to our question hierarchy.
Although we allowmultiple labels for one question in our classifiers,in our labeling, for simplicity, we assigned exactly3The annotated data and experimental results are availablefrom http://L2R.cs.uiuc.edu/cogcomp/one label to each question.
Our annotators were re-quested to choose the most suitable class accord-ing to their own understanding.
This methodologymight cause slight problems in training, when thelabels are ambiguous, since some questions are nottreated as positive examples for possible classes asthey should be.
In training, we divide the 5,500questions from the first three sources randomly into5 training sets of 1,000, 2,000, 3,000, 4,000 and5,500 questions.
All 500 TREC 10 questions areused as the test set.4.2 EvaluationIn this paper, we count the number of correctly clas-sified questions by two different precision standardsP1and P5.
Suppose kilabels are output for the i-th question (ki 5) and are ranked in a decreasingorder according to their density values.
We defineIij= f1; if the correct label of the ithquestion is output in rank j;0; otherwise:(2)Then, P1=Pmi=1Ii1=m and P5=Pmi=1Pkij=1Iij=m where m is the total number oftest examples.
P1corresponds to the usual defini-tion of precision which allows only one label foreach question, while P5allows multiple labels.P5reflects the accuracy of our classifier with re-spect to later stages in a question answering sys-tem.
As the results below show, although questionclasses are still ambiguous, few mistakes are intro-duced by our classifier in this step.4.3 Experimental ResultsPerformance of the hierarchical classifierTable 2 shows the P5precision of the hierarchi-cal classifier when trained on 5,500 examples andtested on the 500 TREC 10 questions.
The re-sults are quite encouraging; question classificationis shown to be solved effectively using machinelearning techniques.
It also shows the contributionof the feature sets we defined.
Overall, we get a98.80% precision for coarse classes with all the fea-tures and 95% for the fine classes.P<=5Word Pos Chunk NE Head RelWordCoarse 92.00 96.60 97.00 97.00 97.80 98.80Fine 86.00 86.60 87.60 88.60 89.40 95.00Table 2: Classification results of the hierarchical clas-sifier on 500 TREC 10 questions.
Training is done on5,500 questions.
Columns show the performance fordifference feature sets and rows show the precision forcoarse and fine classes, resp.
All the results are evalu-ated using P5.Inspecting the data carefully, we can observe thesignificant contribution of the features constructedbased on semantically related words sensors.
It isinteresting to observe that this improvement is evenmore significant for fine classes.No.
Train Test P1P<=51 1000 500 83.80 95.602 2000 500 84.80 96.403 3000 500 91.00 98.004 4000 500 90.80 98.005 5500 500 91.00 98.80Table 3: Classification accuracy for coarse classes ondifferent training sets using the feature set RelWord.
Re-sults are evaluated using P1and P5.No.
Train Test P1P<=51 1000 500 71.00 83.802 2000 500 77.80 88.203 3000 500 79.80 90.604 4000 500 80.00 91.205 5500 500 84.20 95.00Table 4: Classification accuracy for fine classes on dif-ferent training sets using the feature set RelWord.
Re-sults are evaluated using P1and P5.Tables 3 and 4 show the P1and P5accuracyof the hierarchical classifier on training sets of dif-ferent sizes and exhibit the learning curve for thisproblem.We note that the average numbers of labels out-put by the coarse and fine classifiers are 1.54 and2.05 resp., (using the feature set RelWord and 5,500training examples), which shows the decision modelis accurate as well as efficient.Comparison of the hierarchical and the flatclassifierThe flat classifier consists of one classifier which isalmost the same as the fine classifier in the hierar-chical case, except that its initial confusion set isthe whole set of fine classes.
Our original hope wasthat the hierarchical classifier would have a betterperformance, given that its fine classifier only needsto deal with a smaller confusion set.
However, itturns out that there is a tradeoff between this factorand the inaccuracy, albeit small, of the coarse levelprediction.
As the results show, there is no perfor-mance advantage for using a level of coarse classes,and the semantically appealing coarse classes do notcontribute to better performance.Figure 2 give some more intuition on the flat vs.hierarchical issue.
We define the tendency of Classi to be confused with Class j as follows:Dij= Errij 2=(Ni+ Nj); (3)where (when using P1), Errijis the number ofquestions in Class i that are misclassified as belong-P1Word Pos Chunk NE Head RelWordh 77.60 78.20 77.40 78.80 78.80 84.20f 52.40 77.20 77.00 78.40 76.80 84.00P<=5Word Pos Chunk NE Head RelWordh 86.00 86.60 87.60 88.60 89.40 95.00f 83.20 86.80 86.60 88.40 89.80 95.60Table 5: Comparing accuracy of the hierarchical (h) andflat (f) classifiers on 500 TREC 10 question; training isdone on 5,500 questions.
Results are shown for differentfeature sets using P1and P5.Fine Classes 1?50FineClasses1?502 24 28 32 37 5022428323750Figure 2: The gray?scale map of the matrix D[n,n].
Thecolor of the small box in position (i,j) denotes Dij.
Thelarger Dijis, the darker the color is.
The dotted linesseparate the 6 coarse classes.ing to Class j, and Ni; Njare the numbers of ques-tions in Class i and j resp.Figure 2 is a gray-scale map of the matrix D[n,n].D[n,n] is so sparse that most parts of the graph areblank.
We can see that there is no good cluster-ing of fine classes mistakes within a coarse class,which explains intuitively why the hierarchical clas-sifier with an additional level coarse classes does notwork much better.4.4 Discussion and ExamplesWe have shown that the overall accuracy of our clas-sifier is satisfactory.
Indeed, all the reformulationquestions that we exemplified in Sec.
3 have beencorrectly classified.
Nevertheless, it is constructiveto consider some cases in which the classifier fails.Below are some examples misclassified by the hier-archical classifier.What French ruler was defeated at the battle of Water-loo?The correct label is individual, but the classifier,failing to relate the word ?ruler?
to a person, sinceit was not in any semantic list, outputs event.What is the speed hummingbirds fly ?The correct label is speed, but the classifier outputsanimal.
Our feature sensors fail to determine thatthe focus of the question is ?speed?.
This exampleillustrates the necessity of identifying the questionfocus by analyzing syntactic structures.What do you call a professional map drawer ?The classifier returns other entities instead ofequivalent term.
In this case, both classes are ac-ceptable.
The ambiguity causes the classifier not tooutput equivalent term as the first choice.5 ConclusionThis paper presents a machine learning approach toquestion classification.
We developed a hierarchicalclassifier that is guided by a layered semantic hier-archy of answers types, and used it to classify ques-tions into fine-grained classes.
Our experimental re-sults prove that the question classification problemcan be solved quite accurately using a learning ap-proach, and exhibit the benefits of features based onsemantic analysis.In future work we plan to investigate further theapplication of deeper semantic analysis (includingbetter named entity and semantic categorization) tofeature extraction, automate the generation of thesemantic features and develop a better understand-ing to some of the learning issues involved in thedifference between a flat and a hierarchical classi-fier.ReferencesS.
P. Abney.
1991.
Parsing by chunks.
In S. P. AbneyR.
C. Berwick and C. Tenny, editors, Principle-basedparsing: Computation and Psycholinguistics, pages257?278.
Kluwer, Dordrecht.A.
Carlson, C. Cumby, J. Rosen, and D. Roth.
1999.The SNoW learning architecture.
Technical ReportUIUCDCS-R-99-2101, UIUC Computer Science De-partment, May.C.
Cumby and D. Roth.
2000.
Relational representationsthat facilitate learning.
In Proc.
of the InternationalConference on the Principles of Knowledge Represen-tation and Reasoning, pages 425?434.Y.
Even-Zohar and D. Roth.
2001.
A sequential modelfor multi class classification.
In EMNLP-2001, theSIGDAT Conference on Empirical Methods in Natu-ral Language Processing, pages 10?19.A.
R. Golding and D. Roth.
1999.
A Winnow based ap-proach to context-sensitive spelling correction.
Ma-chine Learning, 34(1-3):107?130.S.
Harabagiu, D. Moldovan, M. Pasca, R. Mihalcea,M.
Surdeanu, R. Bunescu, R. Girju, V. Rus, andP.
Morarescu.
2001.
Falcon: Boosting knowledge foranswer engines.
In Proceedings of the 9th Text Re-trieval Conference, NIST.U.
Hermjakob.
2001.
Parsing and question classificationfor question answering.
In ACL-2001 Workshop onOpen-Domain Question Answering.L.
Hirschman, M. Light, E. Breck, and J. Burger.
1999.Deep read: A reading comprehension system.
In Pro-ceedings of the 37th Annual Meeting of the Associa-tion for Computational Linguistics.E.
Hovy, L. Gerber, U. Hermjakob, C. Lin, andD.
Ravichandran.
2001.
Toward semantics-based an-swer pinpointing.
In Proceedings of the DARPA Hu-man Language Technology conference (HLT).
SanDiego, CA.A.
Ittycheriah, M. Franz, W-J Zhu, A. Ratnaparkhi, andR.J.
Mammone.
2001.
IBM?s statistical question an-swering system.
In Proceedings of the 9th Text Re-trieval Conference, NIST.W.
G. Lehnert.
1986.
A conceptual theory of questionanswering.
In B. J. Grosz, K. Sparck Jones, and B. L.Webber, editors, Natural Language Processing, pages651?657.
Kaufmann, Los Altos, CA.X.
Li and D. Roth.
2001.
Exploring evidence for shal-low parsing.
In Proc.
of the Annual Conference onComputational Natural Language Learning.M.
Light, G. Mann, E. Riloff, and E. Breck.
2001.Analyses for Elucidating Current Question AnsweringTechnology.
Journal for Natural Language Engineer-ing.
forthcoming.V.
Punyakanok and D. Roth.
2001.
The use of classi-fiers in sequential inference.
In NIPS-13; The 2000Conference on Advances in Neural Information Pro-cessing Systems, pages 995?1001.
MIT Press.D.
Roth and W. Yih.
2001.
Relational learning viapropositional algorithms: An information extractioncase study.
In Proc.
of the International Joint Confer-ence on Artificial Intelligence, pages 1257?1263.D.
Roth, G. Kao, X. Li, R. Nagarajan, V. Punyakanok,N.
Rizzolo, W. Yih, C. O. Alm, and L. G. Moran.2002.
Learning components for a question answeringsystem.
In TREC-2001.D.
Roth.
1998.
Learning to resolve natural language am-biguities: A unified approach.
In Proc.
of the Ameri-can Association of Artificial Intelligence, pages 806?813.A.
Singhal, S. Abney, M. Bacchiani, M. Collins, D. Hin-dle, and F. Pereira.
2000.
AT&T at TREC-8.
In Pro-ceedings of the 8th Text Retrieval Conference, NIST.E.
Voorhees.
2000.
Overview of the TREC-9 questionanswering track.
In The Ninth Text Retrieval Confer-ence (TREC-9), pages 71?80.
NIST SP 500-249.
