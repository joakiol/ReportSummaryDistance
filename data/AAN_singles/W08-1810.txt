Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 74?81Manchester, UK.
August 2008Indexing on Semantic Roles for Question AnsweringLuiz Augusto PizzatoCentre for Language TechnologyMacquarie UniversitySydney, Australiapizzato@ics.mq.edu.auDiego Molla?Centre for Language TechnologyMacquarie UniversitySydney, Australiadiego@ics.mq.edu.auAbstractSemantic Role Labeling (SRL) has beenused successfully in several stages of auto-mated Question Answering (QA) systemsbut its inherent slow procedures make itdifficult to use at the indexing stage of thedocument retrieval component.
In this pa-per we confirm the intuition that SRL atindexing stage improves the performanceof QA and propose a simplified techniquenamed the Question Prediction LanguageModel (QPLM), which provides similar in-formation with a much lower cost.
Themethods were tested on four different QAsystems and the results suggest that QPLMcan be used as a good compromise be-tween speed and accuracy.1 IntroductionSemantic Role Labeling (SRL) has been imple-mented or suggested as a means to aid several Nat-ural Language Processing (NLP) tasks such as in-formation extraction (Kogan et al, 2005), multi-document summarization (Barzilay et al, 1999)and machine translation (Quantz and Schmitz,1994).
Question Answering (QA) is one task thattakes advantage of SRL, and in fact much of theresearch about the application of SRL to NLP isrelated to QA.
Thus, Narayanan and Harabagiu(2004) apply the argument-predicate relationshipfrom PropBank (Palmer et al, 2005) together withthe semantic frames from FrameNet (Baker et al,1998) to create an inference mechanism to improveQA.
Kaisser and Webber (2007) apply semanticc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.relational information in order to transform ques-tions into information retrieval queries and furtheranalyze the results to find the answers for naturallanguage questions.
Sun et al (2005) use a shal-low semantic parser to create semantic roles in or-der to match questions and answers.
Shen and La-pata (2007) developed an answer extraction mod-ule that incorporates FrameNet style semantic roleinformation.
They deal with the semantic role as-signment as a optimization problem in a bipartitegraph and the answer extraction as a graph match-ing over the semantic relations.Most of the studies that use SRL or similar tech-niques to QA apply semantic relation tools on theinput or output of the Information Retrieval phaseof their system.
Our paper investigates the use ofsemantic information for indexing documents.
Ourhypothesis is that allowing Semantic Role infor-mation at the indexing stage the question analyzerand subsequent stages of the QA system can obtainhigher accuracy by providing an implicit query an-alyzer as well as more precise retrieval.
Theoret-ically, the inclusion of this information at index-ing time can also speed up the overall QA processsince syntactic rephrasing or re-ranking of docu-ments based on semantic roles would not be nec-essary.
However, SRL techniques are still highlycomplex and they demand a computational powerthat is not yet available to most research groupswhen working with large corpora.
In our experi-ence the annotation of a 3GB corpus, such as theAQUAINT (Graff, 2002), using a semantic rolelabeler, for instance SwiRL from Surdeanu andTurmo (2005) can take more than one year usinga standard PC configuration1 .In order to efficiently process a corpus with se-1Intel(R) Pentium(R) 4 HT 2.80GHz with 2.0 GB RAM74mantic relations, we have developed an alterna-tive annotation strategy based on word-to-word re-lations instead of noun phrase-to-predicate rela-tions.
We define semantic triples based on syn-tactic clues; this approach was also studied byLitkowski (1999) but some major differences withour work are that we use automatically learnedrules to generate the semantic relations, and thatwe use different semantic labels than those de-fined by Litkowski, some more specific and somemore general.
Our annotation scheme is named theQuestion Prediction Language Model (QPLM) andrepresents relations between pairs of words usinglabels such as Who and When, according to howone word complements the other.In the following section we provide an overviewof the proposed semantic annotation module.
Thenin Section 3 we detail the information retrievalframework used that allows the indexing and re-trieval of semantic information.
Section 4 de-scribes the experimental setup and presents the re-sults.
Finally, Section 5 presents the concludingremarks and some discussion of further work.2 Question Prediction Language ModelQPLM, as described in Pizzato and Molla?
(2007),represents sentences by specifying the semantic re-lationship among its components using questionwords.
In this way, we focus on dividing the prob-lem of representing a large sentence into smallquestions that could be asked about its compo-nents.
QPLM is expressed by triples ?(?)
?
?where ?
is a question word, ?
is the word that con-cerns the question word ?
and ?
is the word thatanswers the relation ?
about ?.
For instance therelation Who(eat) ?
Jack tells us that the per-son who eats is Jack.
The representation of our se-mantic relations as triples ?(?)
?
?
is importantbecause it allows the representation of sentences asdirected graphs of semantic relations.
This repre-sentation has the capacity of generating questionsabout the sentence being analyzed.
Figure 1 showssuch a representation of the sentence: ?John askedthat a flag be placed in every school?.Having the sentence of Figure 1 and remov-ing a possible answer ?
from any relation triple,it is possible to formulate a complete questionabout this sentence that would require ?
as ananswer.
For instance, we can observe that re-moving the node John we obtain the question?Who asked for a flag to be placed in everyJ o h n a s k e dp l a c e d s c h o o le v e r yf l agw h ow h a tw h a t  w h i c hw h e r eFigure 1: Graph Representationschool??
where Who was extracted from the tripleWho(ask) ?
John.
The same is valid for otherrelations, such as removing the word school to ob-tain the question ?Where did John ask for a flagto be placed??.
The name Question Predictionfor this model is due to its capability of generat-ing questions regarding the sentence that has beenmodeled.We have developed a process to automaticallyannotate QPLM information, the process is rulebased where the rules are automatically learnedfrom a corpus obtained from mapping PropBankinto QPLM instances.
The mapping between se-mantic roles and QPLM is not one-to-one, whichreduces the accuracy of the training corpus.
Asample of 40 randomly selected documents wasmanually evaluated showing that nearly 90% of theQPLM triples obtained were correctly convertedfrom the PropBank mapping.
PropBank does notgive us some relations that we wish to include suchas ownership (Whose(car) ?
Maria) or quan-tity (HowMany(country) ?
twenty)), but itdoes give us the benefits of a large training set cov-ering a variety of different predicates.Our QPLM annotation tool, like most SRLtools, makes use of a syntactic parser and a named-entity (NE) recognizer.
We are currently usingConnexor2 for syntactic parsing and LingPipe3 fornamed-entity recognition.An evaluation of our QPLM annotation hasshown a reasonable precision (50%) with a lowrecall (24%).
Both precision and recall seem tobe connected with the choice of training corpus.The high precision is influenced by the large train-ing set and the different variety of predicates.
Thelow recall is due to the low amount of connectionsthat can be mapped from one sentence in Prop-Bank to QPLM.
As we will present in Section 4,QPLM helps to improve results for QA even when2http://www.connexor.com3http://alias-i.com/lingpipe/75this training corpus is not optimal.
This suggeststhat if a more suitable corpus is used to create theQPLM rules then we can improve the already pos-itive results.
An ideal training corpus would con-tain all QPLM pairs; not only verbs and head ofnoun phrases but also connections among all rele-vant words in a sentence.3 Indexing and Retrieving SemanticInformationA document index that contains information aboutsemantic relations provides a way of finding docu-ments on the basis of meaningful relations amongwords instead of simply their co-occurrence orproximity to each other.
A semantic relation indexallows the retrieval of the same piece of informa-tion when queried using syntactic variations of thesame query such as: ?Bill kicked the ball?
or ?Theball was kicked by Bill?.Several strategies can be used to build the index-ing structure that includes relational information.The task of IR requires fast indexing and retrievalof information regardless of the amount of datastored and how it is going to be retrieved.
Fromour experience, the use of relational databases isacceptable only if the amount of documents andspeed of indexing and retrieval is not a concern.When database systems are used on large IR sys-tems there is always a trade off between the speedof indexing and the speed of retrieval as well speedand storage efficiency.The best approach for IR has always been a cus-tom built inverted file structure.
In the semanticrole/QPLM case it is important to develop an in-dexing structure that can maintain the annotationinformation.
Because it is important to allow dif-ferent types of information to be indexed, we im-plemented a framework for information retrievalthat easily incorporates different linguistic infor-mation.
The framework allows fast indexing andretrieval and the implementation of different rank-ing strategies.With the inclusion of relational information, theframework provides a way to retrieve documentsaccording to a query of semantically connectedwords.
This feature is best used when queries areformed as sentences in natural language.
A sim-plified representation of the framework index isshown in Figure 2 for a QPLM annotated sentence.Figure 2 shows that the relation of words are rep-resented by a common relation identifier and a re-QPLM representation for ?Bill kicked the ball?
:ID Relation11 Who(kick) ?
Bill12 What(kick) ?
ballInverted file representation:Term Document Rel.
ID Rel.
Type RoleBill 1 11 Who Argkick 1 11 Who Pred12 What Predball 1 12 What ArgFigure 2: Simplified representation of the indexingof QPLM relationsQuery Returns documents that?
(kick) ?
?
contain the word kickWho(kick) ?
?
inform that someone kicksWho(?)
?
Bill inform that Bill does an actionWho(kick) ?
Bill inform that Bill kicksFigure 3: QPLM Queries (asterisk symbol is usedto represent a wildcard)lation type.
The roles that each word plays in arelation is also included within the same record.The IF is optimized so that redundant informationis not represented, as illustrated by the record ofthe word kick and the single document number.The framework also provides a way to includewords that have not been explicitly related to otherwords in the text just in the same way as a stan-dard bag-of-words (BoW) approach.
This featureis important even when the text is fully semanti-cally or syntactically parsed.
Many words may notbe associated with the others in a sentence becauseof different reasons such as errors in the parser.Therefore, even if the query presented to the re-trieval component is not a proper natural languagesentence or it fails to be analyzed, the system willperform as a normal BoW system.Once the retrieval query is analyzed, it is pos-sible to perform queries that focus on retrievingall documents where a certain relation occurs aswell as all documents where a certain word playsa specific role.
The example in Figure 3 demon-strates some queries and what documents or sen-tences they return.A document containing the sentence ?Bill kickedthe ball?
would be retrieved for all the queries inFigure 3.
The framework also allows the formula-tion of more complex queries such as:(Who(kick) ?
?)
?
(What(kick) ?
ball)76Each token is indexed by itself (i.e not togetherwith the related words) including the informationfrom the relations it is part of.
This is done with nooverhead or redundant information being stored.This approach makes it possible to keep the stan-dard models for document ranking.
A normalcalculation of Term Frequency (TF) and InvertedDocument Frequency (IDF) is performed whentaking the terms individually or as BoW, whileonly a minimal modification of TF/IDF is requiredwhen a more complex retrieval strategy is needed.The ranking strategy is based on a vector spacemodel.
Documents and queries are representedas three different vectors: bag-of-words (BoW-V),partial relation (PR-V) and full relation (FR-V).The weights of the vector tokens are calculated us-ing the weights of their individual tokens in thecontext of the vector being analyzed.
In BoW-V,weights are calculated based on words; PR-V usesindividual words and their relation types; FR-Vuses the association of a specific word with anotherword.
Figure 4 illustrates the contents of these vec-tors for the sentence ?John loves Mary, but Marylikes Brad?
when used as a query:BoW-V: ?
[John:1], [loves:1], [Mary:2],[likes:1], [Brad:1]?PR-V:?
[John:ARG0:1], [loves:PRED:1],[Mary:ARG1:1], [Mary:ARG0:1],[likes:PRED:1], [Brad:ARG1:1]?FR-V:?
[John:ARG0:loves:1],[Mary:ARG1:loves:1],[Mary:ARG0:likes:1],[Brad:ARG1:likes:1]?Figure 4: Vectors used for document rankingThe tokens of the above example would havedifferent weights if the same sentence appeared ina document with additional sentences.
Because oftheir lower frequency, it is expected that the com-ponents of FR-V and, in a lesser extent, of PR-V tohave a stronger impact on the calculation of simi-larity than the components of BoW-V. With thisapproach, for queries with relations that are notindexed, the method is equivalent to a traditionalBoW approach.4 Experiments and EvaluationWe have performed a series of experiments usingthe techniques described on Section 3 in order toverify the usefulness of QPLM in comparison toSRL based on PropBank.
We compared both se-mantic annotations by using it with IR and underQA evaluation methods.4.1 Configuration of experimentsWe performed experiments using data resourcesfrom the QA track of the TREC conferences(Voorhees and Dang, 2006) and the evaluationscripts available at their TREC website of years2004, 2005 and 2006.
The retrieval experimentswere carried out using only a reduced set of docu-ments from the AQUAINT corpus because the se-mantic role labelers tested were not able to parsethe full set, unlike QPLM which parsed all docu-ments successfully.The SRL tool SwiRL (Surdeanu and Turmo,2005) has a good precision and coverage, howeverit is slow and quite unstable when parsing largeamounts of data.
We have assembled a clusterof computers in order to speed up the corpus an-notation, but even when having around ten ded-icated computers the estimated completion timewas larger than one year.
The lack of semantic an-notators that can quickly evaluate large amount ofdata gave us the stimulus needed to use a simplifiedand quicker technique.
We used the QPLM anno-tation tool which takes less than 3 weeks to fullyannotate the 3GB of data from the AQUAINT cor-pus using a single machine.Since we wanted to determinate how QPLMcompares to SRL, particularly on the basis of itsusage for IR and for QA, we performed sometests using the available amount of data anno-tated with semantic roles, and the same docu-ments with QPLM.
The part of the AQUAINTcorpus annotated includes the first 41,116 docu-ments, in chronological order, from the New YorkTimes (NYT) newspaper.
We used the 1,448 ques-tions from the QA track of 2004, 2005 and 2006from the TREC competition.
Since these questionsare not always self contained and in some cases(OTHER-type questions) not even a proper natu-ral language sentence, we performed some ques-tion modification so that the entire topic text couldbe included.
These modifications include substitu-tion of key pronouns as well as the inclusion of thewhole topic text when shorter representations werefound.
In some extreme cases when no substitutionwas possible and the question did not mention thetopic, we added a phrase containing the topic at thestart of the question.
Some examples are presented77Topic: Gordon GekkoQuestion: What year was the movie released?Modification: Regarding Gordon Gekko, what yearwas the movie released?Question: What was Gekko?s profession?Modification: What was Gordon Gekko?s profession?Question: OtherModification: Tell me more about Gordon Gekko.Figure 5: Modifications applied to TREC ques-tionsin Figure 5.Using these questions as queries for our IRframework, we retrieved a set of 50 documents forevery question.
We analyzed the impact of the se-mantic annotation when used on document indicesby checking the presence of the answer string inthe documents returned.
We also obtained a listof 50 documents using solely the BoW approachin order to compare what is the gain over standardretrieval.4.2 Evaluation of retrieval setsTable 1 presents the results of the retrieval set usingTREC?s QA track from 2004, 2005 and 2006 us-ing the BoW, the SRL and the QPLM approaches.Because we performed the evaluation of these doc-uments automatically, we consider a document rel-evant on the only basis of the presence of therequired answer string.
We adopted the evalua-tion metrics for QA documents sets proposed byRoberts and Gaizauskas (2004).
We used the fol-lowing metrics: p@n as the precision at n docu-ments or percentage of documents containing ananswer when retrieving at most n documents; c@nas the coverage at n documents or percentage ofquestions that can be answered using up to n doc-uments for each question; and r@n as the redun-dancy at n document or the average number of an-swers found in the first n documents per question.As observed in Table 1, the SRL approach givesthe best results for all question sets on all evalu-ation metrics, with the exception of c@50 on the2006 question set.
In most other retrieval setsthe baseline performs worse than both QPLM andSRL, however for 2004 questions it performed bet-ter than QPLM on p@50 and r@50.
It is interestingto observe that the QPLM results for the same yearon c@50 are better than the BoW approach indi-cating that a larger amount of questions can poten-tially be answered by QPLM.2004 p@50 c@50 r@50BoW 5.85% 33.33% 2.92SRL 6.40% 35.33% 3.20QPLM 5.58% 34.47% 2.792005 p@50 c@50 r@50BoW 10.03% 41.13% 5.02SRL 11.00% 43.77% 5.50QPLM 10.58% 42.08% 5.292006 p@50 c@50 r@50BoW 7.30% 34.57% 3.65SRL 8.73% 36.33% 4.37QPLM 8.31% 38.45% 4.16Table 1: Experimental results of index approacheson TREC questions4.3 Experiments on QA systemsTo better understand the relation between the re-trieved document sets and question answering weapplied the retrieval sets to four question answer-ing systems:?
Aranea: Developed by Lin (2007), the Araneasystem utilizes the redundancy from theWorld Wide Web using different Web SearchEngines.
The system relies on the text snip-pets to generate candidate answers.
It appliesfiltering techniques based on intuitive rules,as well as the expected answer classes withnamed-entities recognition defined by regularexpressions and a fixed list for some specialcases.?
OpenEphyra: Developed by Schlaefer et al(2007), the OpenEphyra framework attemptsto be a test bench for question answering tech-niques.
The system approaches QA in a fairlystandard way.
Using a three-stage QA archi-tecture (Question Analysis, Information Re-trieval, Answer Extraction), it performed rea-sonably well at the QA Track at TREC 2007by using Web Search engines on its IR stageand mapping the answers back into the TRECcorpus.?
MetaQA System: Similar to the Aranea QAsystem, MetaQA (Pizzato and Molla, 2005)makes heavy use of redundancy and the in-formation provided by Web Search Engines.However it goes a step further by combiningdifferent classes of Web Search engines (in-cluding Web Question Answering Systems)and assigning different confidence scores toeach of the classes.78?
AnswerFinder: Developed by Molla?
and VanZaanen (2006), the AnswerFinder QA systemunique feature is the use of QA graph ruleslearned automatically from a small trainingcorpus.
These graph rules are based onthe maximum common subgraph between thedeep syntactic representation of a questionand a candidate answer sentence.
The graphswere derived from the output of the Connexordependency-based parser.For most of these systems some modifications ofthe standard system configuration were required.All the systems used, with the exception of An-swerFinder, make heavy use of web search en-gines and the redundancy obtained to find theiranswers.
For our experiments we had to turn theWeb search off, causing a significant drop in per-formance when compared to the reported results inthe literature.
Because AnswerFinder?s IR compo-nent is performed offline, the integration is seam-less and only required providing the system witha list of documents in the same format as TRECdistributes the ranked list of files per topic.
TheOpenEphyra framework is well designed and im-plemented, however the interaction between itscomponents still depended on the overall systemarchitecture, which makes the implementation ofnew modules for the system quite difficult.With the exception of AnswerFinder, all the QAsystems received a retrieval set as a collection ofsnippets.
This was based on the fact that thesesystems are based on Web Retrieval and they ex-pect to receive documents in this format.
We ex-tracted for every document the 255 character win-dow where more question words (non-stopwords)were found.
The implementation of different rank-ing strategies for passage retrieval such as thosedescribed by Tellex et al (2003) could improve theresults for individual QA systems.
However, a pre-liminary evaluation of the passage retrieval haveshown us that the 255 character window with thecurrent snippet construction method was enoughto achieve near optimal performance on the docu-ment set used.The results obtained by the QA systems wereprocessed using the answer regular expressionsdistributed by TREC.
The numbers described inthis study show the factoid score for correct an-swers.
We have not used the exact answer be-cause it required some cleaning of the answer logfiles and some modification of some QA systems.2004 2005 2006BoW 5.00% 2.30% 2.10%SRL 6.10% 3.50% 2.70%QPLM 5.00% 2.50% 3.50%Table 2: Factoid results for C@1 on the Araneasystem2004 2005 2006BoW 2.50% 5.10% 3.00%SRL 3.30% 7.00% 4.40%QPLM 2.80% 6.20% 4.20%Table 3: Factoid results for C@1 on the OpenE-phyra systemTherefore, the results shown on Tables 2, 3 and5 are product of the same retrieval set and resultof the same evaluation procedure.
Results of theMetaQA system at Table 4 are presented as cover-age at answer 10 (C@10) since this system has anon standard approach for QA that is invalidatedby the methodology of this test.
The results in theother tables could be understood as either precisionor coverage at answer 1, we will refer to them asC@1.We observed that the results from the QA sys-tem are consistent with the findings from the re-sults of the retrieval system.
The Aranea QA sys-tem results on Table 2 show an average improve-ment for the SRL approach.
QPLM has similarperformance to BoW for 2004 and 2005 questionsbut outperforms both techniques on 2006 ques-tions.The results shown by OpenEphyra in Table 3also demonstrate that semantic annotation can helpquestion answering when used in the IR stages of aQA system.
The best results were observed whenSRL was applied.
QPLM followed SRL and out-performed BoW on three tests.
It is important topoint out that results for the retrieval set alne inTable 1 showed BoW outperforming QPLM for2004 questions on both redundancy and precisionmetrics.
This might be an indication that OpenE-phyra answer extraction modules are more precisethan the other QA systems and do not heavily relyon redundancy as do the Aranea and the MetaQAsystems.Because of the high dependency on Websources, the MetaQA system performed ratherpoorly.
As explained earlier, the results were mea-sured using C@10 instead of C@1.
The reason forthis is that the MetaQA system is meant to be anaggregator of information sources and its ranking792004 2005 2006BoW 0.87% 3.31% 1.24%SRL 2.61% 3.87% 1.99%QPLM 0.43% 3.31% 1.24%Table 4: Factoid results for C@10 on the MetaQAsystem2004 2005 2006BoW 1.10% 2.50% 1.20%SRL 1.80% 2.60% 2.20%QPLM 1.80% 2.70% 2.00%Table 5: Factoid results for C@1 on the An-swerFinder systemmechanisms only work when sufficient evidence isgiven for certain entities.
Not only was the systemnot designed for the single-source setup, but it wasnot designed to provide a single answer.
Neverthe-less, even with the non-conformity of the system,it appears to support that semantic markup can en-hance the IR results for QA.
Not surprisingly theextra redundancy presented in the 2004 BoW re-trieval contributed to better results in this redun-dancy based QA system.Results in Table 5 show that AnswerFinder cor-rectly answered only a few questions for the givenquestion set.
On the other hand, it provided someconsistent results such that the improvements weredue to additional correct answers and not to alarger but different set of correct answers.
TheAnswerFinder QA system showed a similar perfor-mance for both semantic-based strategies and bothoutperformed the BoW strategy.In this section we have shown an evaluation ofdifferent retrieval sets of documents using four dis-tinct QA systems.
We have observed that semanticstrategies not only assist the retrieval of better doc-uments, but also help in finding answers for ques-tions when used with QA systems.5 Concluding RemarksIn this work we propose the use of semantic re-lation in QA.
We also present QPLM as an alter-native to SRL.
QPLM is a simpler approach to se-mantic annotation based on relations between pairsof words, which gives a large advantage in speedperformance over SRL.
We show some compari-son of retrieval sets using the questions from theQA track of TREC and conclude that SRL andQPLM improve the quality of the retrieval set overa standard BoW approach.
From these results wealso observe that QPLM performance does not fallmuch behind SRL.We performed an evaluation using four QA sys-tems.
These systems are conceptually differentwhich gives a broad perspective of the obtained re-sults.
The results once again show the effective-ness of semantic annotation.
Over QA, SRL hasperformed better than the other techniques, but wasclosely followed by QPLM.
The results obtainedhere suggest that QPLM is a cheaper and effectivemethod of semantic annotation that can help in tun-ing the search component of a QA system to findthe correct answers for a question.The results presented in this work for all QAsystems are much lower than those reported inthe literature.
This is an undesirable but ex-pected problem that occurred not only because ofthe modifications carried on the QA systems butmainly because of the reduced number of docu-ments used for this evaluation.
We are looking intomore efficient alternatives for performing the SRLannotation of the AQUAINT corpus.Only recently we have been able to test Koomenet al (2005) SRL tool.
This SRL tool is the topranking SRL tool at the CoNLL-2005 Shared TaskEvaluation and it seems to be much faster thanSwiRL.
Preliminary tests suggest that it is ableto perform the annotation of AQUAINT in almostone full year using a single computer; however,this tool, like SwiRL, is not very stable, crashingseveral times during the experiments.
As furtherwork, we plan to employ several computers andattempt to parse the whole AQUAINT corpus withthis tool.It is important to point out that although the toolof Koomen et al seems much faster than SwiRL,QPLM still outperforms both of them on speed bylarge.
QPLM represents word relations that arebuilt using rules from syntactic and NE informa-tion.
This simpler representation, combined witha smaller number of supporting NLP tools, allowQPLM to be faster than current SRL tools.
Weplan to carry out further work on the QPLM toolto increase its performance on both speed and ac-curacy.
QPLM?s precision and recall figures aregoing to be improved by using a hand annotatedcorpus.
QPLM?s speed suggest that it can be cur-rently used on IR tools as a pre-processing engine.It is understandable that any delay in the IR phasesis undesirable when dealing with large amount ofdata, therefore optimizing the speed of QPLM isone of our priorities.80AcknowledgementThis work was supported by an iMURS scholar-ship from Macquarie University and the CSIRO.ReferencesBaker, Collin F., Charles J. Fillmore, and John B. Lowe.1998.
The berkeley framenet project.
In Proceed-ings of the 17th international conference on Com-putational linguistics, pages 86?90, Morristown, NJ,USA.
Association for Computational Linguistics.Barzilay, Regina, Kathleen R. McKeown, and MichaelElhadad.
1999.
Information fusion in the context ofmulti-document summarization.
In Proceedings ofthe 37th annual meeting of the Association for Com-putational Linguistics on Computational Linguistics,pages 550?557, Morristown, NJ, USA.
Associationfor Computational Linguistics.Graff, David.
2002.
The AQUAINT corpus of englishnews text.
CDROM.
ISBN: 1-58563-240-6.Kaisser, Michael and Bonnie Webber.
2007.
Questionanswering based on semantic roles.
In Proceedingsof the ACL 2007 Workshop on Deep Linguistic Pro-cessing,, page 4148, Prague, Czech Republic, June.c2007 Association for Computational Linguistics.Kogan, Y., N. Collier, S. Pakhomov, and M. Krautham-mer.
2005.
Towards semantic role labeling & ie inthe medical literature.
In American Medical Infor-matics Association Annual Symposium., Washing-ton, DC.Koomen, P., V. Punyakanok, D. Roth, and W. Yih.2005.
Generalized inference with multiple seman-tic role labeling systems (shared task paper).
InDagan, Ido and Dan Gildea, editors, Proc.
of theAnnual Conference on Computational Natural Lan-guage Learning (CoNLL), pages 181?184.Lin, Jimmy.
2007.
An exploration of the principles un-derlying redundancy-based factoid question answer-ing.
ACM Trans.
Inf.
Syst., 25(2):6.Litkowski, K. 1999.
Question-answering using seman-tic relation triples.
In In Proceedings of the 8th TextRetrieval Conference (TREC-8, pages 349?356.Molla, Diego and Menno van Zaanen.
2006.
An-swerfinder at TREC 2005.
In The Fourteenth TextREtrieval Conference (TREC 2005), Gaithersburg,Maryland.
National Institute of Standards and Tech-nology.Narayanan, Srini and Sanda Harabagiu.
2004.
Ques-tion answering based on semantic structures.
InCOLING ?04: Proceedings of the 20th internationalconference on Computational Linguistics, page 693,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Palmer, Martha, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguist., 31(1):71?106.Pizzato, Luiz Augusto and Diego Molla.
2005.
Ex-tracting exact answers using a meta question answer-ing system.
In Proceedings of the Australasian Lan-guage Technology Workshop 2005 (ALTA-2005).,The University of Sydney, Australia, December.Pizzato, Luiz Augusto and Diego Molla?.
2007.
Ques-tion prediction language model.
In Proceedingsof the Australasian Language Technology Workshop2007, pages 92?99, Melbourne, December.Quantz, Joachim and Birte Schmitz.
1994.Knowledge-based disambiguation for machinetranslation.
Minds and Machines, 4(1):39?57,February.Roberts, Ian and Robert J. Gaizauskas.
2004.
Eval-uating passage retrieval approaches for question an-swering.
In McDonald, Sharon and John Tait, edi-tors, Advances in Information Retrieval, 26th Euro-pean Conference on IR Research, ECIR 2004, Sun-derland, UK, April 5-7, 2004, Proceedings, volume2997 of Lecture Notes in Computer Science, pages72?84.
Springer.Schlaefer, N., P. Gieselmann, and G. Sautter.
2007.The ephyra qa system at trec 2006 the Ephyra QAsystem at TREC 2006.
In The Fifteenth Text RE-trieval Conference (TREC 2006).Shen, Dan and Mirella Lapata.
2007.
Using semanticroles to improve question answering.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 12?21, Prague,June 2007.
Association for Computational Linguis-tics.Sun, R. X., J. J. Jiang, Y. F. Tan, H. Cui, T. S. Chua, andM.
Y. Kan. 2005.
Using syntactic and semantic rela-tion analysis in question answering.
In Proceedingsof the TREC.Surdeanu, Mihai and Jordi Turmo.
2005.
Semanticrole labeling using complete syntactic analysis.
InProceedings of CoNLL 2005 Shared Task, June.Tellex, Stefanie, Boris Katz, Jimmy Lin, Aaron Fer-nandes, and Gregory Marton.
2003.
Quantitativeevaluation of passage retrieval algorithms for ques-tion answering.
In SIGIR ?03: Proceedings of the26th annual international ACM SIGIR conference onResearch and development in informaion retrieval,pages 41?47, New York, NY, USA.
ACM Press.Voorhees, Ellen M. and Hoa Trang Dang.
2006.Overview of the TREC 2005 question answeringtrack.
In Text REtrieval Conference.81
