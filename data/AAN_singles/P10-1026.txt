Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 247?256,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsA Bayesian Method for Robust Estimation of Distributional SimilaritiesJun?ichi Kazama Stijn De Saeger Kow KurodaMasaki Murata?
Kentaro TorisawaLanguage Infrastructure Group, MASTAR ProjectNational Institute of Information and Communications Technology (NICT)3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0289 Japan{kazama, stijn, kuroda, torisawa}@nict.go.jp?Department of Information and Knowledge EngineeringFaculty/Graduate School of Engineering, Tottori University4-101 Koyama-Minami, Tottori, 680-8550 Japan?murata@ike.tottori-u.ac.jpAbstractExisting word similarity measures are notrobust to data sparseness since they relyonly on the point estimation of words?context profiles obtained from a limitedamount of data.
This paper proposes aBayesian method for robust distributionalword similarities.
The method uses a dis-tribution of context profiles obtained byBayesian estimation and takes the expec-tation of a base similarity measure underthat distribution.
When the context pro-files are multinomial distributions, the pri-ors are Dirichlet, and the base measure isthe Bhattacharyya coefficient, we can de-rive an analytical form that allows efficientcalculation.
For the task of word similar-ity estimation using a large amount of Webdata in Japanese, we show that the pro-posed measure gives better accuracies thanother well-known similarity measures.1 IntroductionThe semantic similarity of words is a long-standing topic in computational linguistics be-cause it is theoretically intriguing and has manyapplications in the field.
Many researchers haveconducted studies based on the distributional hy-pothesis (Harris, 1954), which states that wordsthat occur in the same contexts tend to have similarmeanings.
A number of semantic similarity mea-sures have been proposed based on this hypothesis(Hindle, 1990; Grefenstette, 1994; Dagan et al,1994; Dagan et al, 1995; Lin, 1998; Dagan et al,1999).
?The work was done while the author was at NICT.In general, most semantic similarity measureshave the following form:sim(w1, w2) = g(v(w1), v(w2)), (1)where v(wi) is a vector that represents the con-texts in which wi appears, which we call a contextprofile of wi.
The function g is a function on thesecontext profiles that is expected to produce goodsimilarities.
Each dimension of the vector corre-sponds to a context, fk, which is typically a neigh-boring word or a word having dependency rela-tions with wi in a corpus.
Its value, vk(wi), is typ-ically a co-occurrence frequency c(wi, fk), a con-ditional probability p(fk|wi), or point-wise mu-tual information (PMI) between wi and fk, whichare all calculated from a corpus.
For g, variousworks have used the cosine, the Jaccard coeffi-cient, or the Jensen-Shannon divergence is uti-lized, to name only a few measures.Previous studies have focused on how to de-vise good contexts and a good function g for se-mantic similarities.
On the other hand, our ap-proach in this paper is to estimate context profiles(v(wi)) robustly and thus to estimate the similarityrobustly.
The problem here is that v(wi) is com-puted from a corpus of limited size, and thus in-evitably contains uncertainty and sparseness.
Theguiding intuition behind our method is as follows.All other things being equal, the similarity witha more frequent word should be larger, since itwould be more reliable.
For example, if p(fk|w1)and p(fk|w2) for two given words w1 and w2 areequal, but w1 is more frequent, we would expectthat sim(w0, w1) > sim(w0, w2).In the NLP field, data sparseness has been rec-ognized as a serious problem and tackled in thecontext of language modeling and supervised ma-chine learning.
However, to our knowledge, there247has been no study that seriously dealt with datasparseness in the context of semantic similaritycalculation.
The data sparseness problem is usu-ally solved by smoothing, regularization, marginmaximization and so on (Chen and Goodman,1998; Chen and Rosenfeld, 2000; Cortes and Vap-nik, 1995).
Recently, the Bayesian approach hasemerged and achieved promising results with aclearer formulation (Teh, 2006; Mochihashi et al,2009).In this paper, we apply the Bayesian frameworkto the calculation of distributional similarity.
Themethod is straightforward: Instead of using thepoint estimation of v(wi), we first estimate thedistribution of the context profile, p(v(wi)), byBayesian estimation and then take the expectationof the original similarity under this distribution asfollows:simb(w1, w2) (2)= E[sim(w1, w2)]{p(v(w1)),p(v(w2))}= E[g(v(w1), v(w2))]{p(v(w1)),p(v(w2))}.The uncertainty due to data sparseness is repre-sented by p(v(wi)), and taking the expectation en-ables us to take this into account.
The Bayesianestimation usually gives diverging distributions forinfrequent observations and thus decreases the ex-pectation value as expected.The Bayesian estimation and the expectationcalculation in Eq.
2 are generally difficult andusually require computationally expensive proce-dures.
Since our motivation for this research is tocalculate good semantic similarities for a large setof words (e.g., one million nouns) and apply themto a wide range of NLP tasks, such costs must beminimized.Our technical contribution in this paper is toshow that in the case where the context profiles aremultinomial distributions, the priors are Dirich-let, and the base similarity measure is the Bhat-tacharyya coefficient (Bhattacharyya, 1943), wecan derive an analytical form for Eq.
2, that en-ables efficient calculation (with some implemen-tation tricks).In experiments, we estimate semantic similari-ties using a large amount of Web data in Japaneseand show that the proposed measure gives bet-ter word similarities than a non-Bayesian Bhat-tacharyya coefficient or other well-known similar-ity measures such as Jensen-Shannon divergenceand the cosine with PMI weights.The rest of the paper is organized as follows.
InSection 2, we briefly introduce the Bayesian esti-mation and the Bhattacharyya coefficient.
Section3 proposes our new Bayesian Bhattacharyya coef-ficient for robust similarity calculation.
Section 4mentions some implementation issues and the so-lutions.
Then, Section 5 reports the experimentalresults.2 Background2.1 Bayesian estimation with Dirichlet priorAssume that we estimate a probabilistic model forthe observed data D, p(D|?
), which is parame-terized with parameters ?.
In the maximum like-lihood estimation (MLE), we find the point esti-mation ??
= argmax?p(D|?).
For example, weestimate p(fk|wi) as follows with MLE:p(fk|wi) = c(wi, fk)/Xkc(wi, fk).
(3)On the other hand, the objective of the Bayesianestimation is to find the distribution of ?
giventhe observed data D, i.e., p(?|D), and use it inlater processes.
Using Bayes?
rule, this can alsobe viewed as:p(?|D) = p(D|?)pprior(?
)p(D) .
(4)pprior(?)
is a prior distribution that represents theplausibility of each ?
based on the prior knowl-edge.
In this paper, we consider the case where?
is a multinomial distribution, i.e.,?k ?k = 1,that models the process of choosing one out of Kchoices.
Estimating a conditional probability dis-tribution ?k = p(fk|wi) as a context profile foreach wi falls into this case.
In this paper, we alsoassume that the prior is the Dirichlet distribution,Dir(?).
The Dirichlet distribution is defined asfollows.Dir(?|?)
=?
(PKk=1 ?k)QKk=1 ?(?k)KYk=1?
?k?1k .
(5)?(.)
is the Gamma function.
The Dirichlet distri-bution is parametrized by hyperparameters ?k(>0).It is known that p(?|D) is also a Dirichlet dis-tribution for this simplest case, and it can be ana-lytically calculated as follows.p(?|D) = Dir(?|{?k + c(k)}), (6)where c(k) is the frequency of choice k in data D.For example, c(k) = c(wi, fk) in the estimationof p(fk|wi).
This is very simple: we just need toadd the observed counts to the hyperparameters.2482.2 Bhattacharyya coefficientWhen the context profiles are probability distribu-tions, we usually utilize the measures on probabil-ity distributions such as the Jensen-Shannon (JS)divergence to calculate similarities (Dagan et al,1994; Dagan et al, 1997).
The JS divergence isdefined as follows.JS(p1||p2) =12(KL(p1||pavg) + KL(p2||pavg)),where pavg = p1+p22 is a point-wise average of p1and p2 and KL(.)
is the Kullback-Leibler diver-gence.
Although we found that the JS divergenceis a good measure, it is difficult to derive an ef-ficient calculation of Eq.
2, even in the Dirichletprior case.1In this study, we employ the Bhattacharyya co-efficient (Bhattacharyya, 1943) (BC for short),which is defined as follows:BC(p1, p2) =KXk=1?p1k ?
p2k.The BC is also a similarity measure on probabil-ity distributions and is suitable for our purposes aswe describe in the next section.
Although BC hasnot been explored well in the literature on distribu-tional word similarities, it is also a good similaritymeasure as the experiments show.3 MethodIn this section, we show that if our base similaritymeasure is BC and the distributions under whichwe take the expectation are Dirichlet distributions,then Eq.
2 also has an analytical form, allowingefficient calculation.Here, we calculate the following value giventwo Dirichlet distributions:BCb(p1, p2) = E[BC(p1, p2)]{Dir(p1|??
),Dir(p2|??
)}=ZZ???Dir(p1|??)Dir(p2|??
)BC(p1, p2)dp1dp2.After several derivation steps (see Appendix A),we obtain the following analytical solution for theabove:1A naive but general way might be to draw samples ofv(wi) from p(v(wi)) and approximate the expectation usingthese samples.
However, such a method will be slow.= ?(??0)?(??0)?(?
?0 + 12 )?(?
?0 + 12 )KXk=1?(?
?k + 12 )?(?
?k + 12 )?(??k)?(?
?k), (7)where ?
?0 =?k ?
?k and ?
?0 =?k ??k.
Note thatwith the Dirichlet prior, ?
?k = ?k + c(w1, fk) and?
?k = ?k + c(w2, fk), where ?k and ?k are thehyperparameters of the priors of w1 and w2, re-spectively.To put it all together, we can obtain a newBayesian similarity measure on words, which canbe calculated only from the hyperparameters forthe Dirichlet prior, ?
and ?, and the observedcounts c(wi, fk).
It is written as follows.BCb(w1, w2) = (8)?
(?0 + a0)?
(?0 + b0)?
(?0 + a0 + 12 )?
(?0 + b0 +12 )?KXk=1?
(?k + c(w1, fk) + 12 )?
(?k + c(w2, fk) +12 )?
(?k + c(w1, fk))?
(?k + c(w2, fk)),where a0 =?k c(w1, fk) and b0 =?k c(w2, fk).
We call this new measure theBayesian Bhattacharyya coefficient (BCb forshort).
For simplicity, we assume ?k = ?k = ?
inthis paper.We can see that BCb actually encodes our guid-ing intuition.
Consider four words, w0, w1, w2,and w4, for which we have c(w0, f1) = 10,c(w1, f1) = 2, c(w2, f1) = 10, and c(w3, f1) =20.
They have counts only for the first dimen-sion, i.e., they have the same context profile:p(f1|wi) = 1.0, when we employ MLE.
WhenK = 10, 000 and ?k = 1.0, the Bayesian similar-ity between these words is calculated asBCb(w0, w1) = 0.785368BCb(w0, w2) = 0.785421BCb(w0, w3) = 0.785463We can see that similarities are different ac-cording to the number of observations, as ex-pected.
Note that the non-Bayesian BC will re-turn the same value, 1.0, for all cases.
Notealso that BCb(w0, w0) = 0.78542 if we use Eq.8, meaning that the self-similarity might not bethe maximum.
This is conceptually strange, al-though not a serious problem since we hardly usesim(wi, wi) in practice.
If we want to fix this,we can use the special definition: BCb(wi, wi) ?1.
This is equivalent to using simb(wi, wi) =E[sim(wi, wi)]{p(v(wi))} = 1 only for this case.2494 Implementation IssuesAlthough we have derived the analytical form(Eq.
8), there are several problems in implement-ing robust and efficient calculations.First, the Gamma function in Eq.
8 overflowswhen the argument is larger than 170.
In suchcases, a commonly used way is to work in the log-arithmic space.
In this study, we utilize the ?logGamma?
function: ln?
(x), which returns the log-arithm of the Gamma function directly without theoverflow problem.2Second, the calculation of the log Gamma func-tion is heavier than operations such as simple mul-tiplication, which is used in existing measures.In fact, the log Gamma function is implementedusing an iterative algorithm such as the Lanczosmethod.
In addition, according to Eq.
8, it seemsthat we have to sum up the values for all k, be-cause even if c(wi, fk) is zero the value inside thesummation will not be zero.
In the existing mea-sures, it is often the case that we only need to sumup for k where c(wi, fk) > 0.
Because c(wi, fk)is usually sparse, that technique speeds up the cal-culation of the existing measures drastically andmakes it practical.In this study, the above problem is solved bypre-computing the required log Gamma values, as-suming that we calculate similarities for a largeset of words, and pre-computing default values forcases where c(wi, fk) = 0.
The following valuesare pre-computed once at the start-up time.For each word:(A) ln?
(?0 + a0) ?
ln?
(?0 + a0 + 12)(B) ln?
(?k+c(wi, fk))?ln?
(?k+c(wi, fk)+ 12)for all k where c(wi, fk) > 0(C) ?
exp(2(ln?
(?k + 12) ?
ln?
(?k)))) +exp(ln?
(?k + c(wi, fk)) ?
ln?
(?k +c(wi, fk) + 12) + ln?
(?k +12) ?
ln?
(?k))for all k where c(wi, fk) > 0;For each k:(D): exp(2(ln?
(?k + 12)).In the calculation of BCb(w1, w2), we first as-sume that all c(wi, fk) = 0 and set the outputvariable to the default value.
Then, we iterateover the sparse vectors c(w1, fk) and c(w2, fk).
If2We used the GNU Scientific Library (GSL)(www.gnu.org/software/gsl/), which implements thisfunction.c(w1, fk) > 0 and c(w2, fk) = 0 (and vice versa),we update the output variable just by adding (C).If c(w1, fk) > 0 and c(w2, fk) > 0, we updatethe output value using (B), (D) and one additionalexp(.)
operation.
With this implementation, wecan make the computation of BCb practically asfast as using other measures.5 Experiments5.1 Evaluation settingWe evaluated our method in the calculation of sim-ilarities between nouns in Japanese.Because human evaluation of word similari-ties is very difficult and costly, we conducted au-tomatic evaluation in the set expansion setting,following previous studies such as Pantel et al(2009).Given a word set, which is expected to con-tain similar words, we assume that a good simi-larity measure should output, for each word in theset, the other words in the set as similar words.For given word sets, we can construct input-and-answers pairs, where the answers for each wordare the other words in the set the word appears in.We output a ranked list of 500 similar wordsfor each word using a given similarity measureand checked whether they are included in the an-swers.
This setting could be seen as document re-trieval, and we can use an evaluation measure suchas the mean of the precision at top T (MP@T ) orthe mean average precision (MAP).
For each inputword, P@T (precision at top T ) and AP (averageprecision) are defined as follows.P@T = 1TTXi=1?
(wi ?
ans),AP = 1RNXi=1?
(wi ?
ans)P@i.?
(wi ?
ans) returns 1 if the output word wi isin the answers, and 0 otherwise.
N is the numberof outputs and R is the number of the answers.MP@T and MAP are the averages of these valuesover all input words.5.2 Collecting context profilesDependency relations are used as context profilesas in Kazama and Torisawa (2008) and Kazama etal.
(2009).
From a large corpus of Japanese Webdocuments (Shinzato et al, 2008) (100 million250documents), where each sentence has a depen-dency parse, we extracted noun-verb and noun-noun dependencies with relation types and thencalculated their frequencies in the corpus.
If anoun, n, depends on a word, w, with a relation,r, we collect a dependency pair, (n, ?w, r?).
Thatis, a context fk, is ?w, r?
here.For noun-verb dependencies, postpositionsin Japanese represent relation types.
Forexample, we extract a dependency relation(??
?, ?
??,?
?)
from the sentence below,where a postposition ??
(wo)?
is used to markthe verb object.???
(wine)?
(wo)??
(buy) (?
buy a wine)Note that we leave various auxiliary verb suf-fixes, such as ???
(reru),?
which is for passiviza-tion, as a part of w, since these greatly change thetype of n in the dependent position.As for noun-noun dependencies, we consideredexpressions of type ?n1 ?
n2?
(?
?n2 of n1?)
asdependencies (n1, ?n2,?
?
).We extracted about 470 million unique depen-dencies from the corpus, containing 31 millionunique nouns (including compound nouns as de-termined by our filters) and 22 million unique con-texts, fk.
We sorted the nouns according to thenumber of unique co-occurring contexts and thecontexts according to the number of unique co-occurring nouns, and then we selected the top onemillion nouns and 100,000 contexts.
We used only260 million dependency pairs that contained boththe selected nouns and the selected contexts.5.3 Test setsWe prepared three test sets as follows.Set ?A?
and ?B?
: Thesaurus siblings Weconsidered that words having a commonhypernym (i.e., siblings) in a manuallyconstructed thesaurus could constitute asimilar word set.
We extracted such setsfrom a Japanese dictionary, EDR (V3.0)(CRL, 2002), which contains concept hier-archies and the mapping between words andconcepts.
The dictionary contains 304,884nouns.
In all, 6,703 noun sibling sets wereextracted with the average set size of 45.96.We randomly chose 200 sets each for sets?A?
and ?B.?
Set ?A?
is a development set totune the value of the hyperparameters and?B?
is for the validation of the parametertuning.Set ?C?
: Closed sets Murata et al (2004) con-structed a dataset that contains several closedword sets such as the names of countries,rivers, sumo wrestlers, etc.
We used all ofthe 45 sets that are marked as ?complete?
inthe data, containing 12,827 unique words intotal.Note that we do not deal with ambiguities in theconstruction of these sets as well as in the calcu-lation of similarities.
That is, a word can be con-tained in several sets, and the answers for such aword is the union of the words in the sets it belongsto (excluding the word itself).In addition, note that the words in these test setsare different from those of our one-million-wordvocabulary.
We filtered out the words that are notincluded in our vocabulary and removed the setswith size less than 2 after the filtering.Set ?A?
contained 3,740 words that are actuallyevaluated, with about 115 answers on average, and?B?
contained 3,657 words with about 65 answerson average.
Set ?C?
contained 8,853 words withabout 1,700 answers on average.5.4 Compared similarity measuresWe compared our Bayesian Bhattacharyya simi-larity measure, BCb, with the following similaritymeasures.JS Jensen-Shannon divergence between p(fk|w1)and p(fk|w2) (Dagan et al, 1994; Dagan etal., 1999).PMI-cos The cosine of the context profile vec-tors, where the k-th dimension is the point-wise mutual information (PMI) betweenwi and fk defined as: PMI(wi, fk) =log p(wi,fk)p(wi)p(fk) (Pantel and Lin, 2002; Pantelet al, 2009).3Cls-JS Kazama et al (2009) proposed usingthe Jensen-Shannon divergence between hid-den class distributions, p(c|w1) and p(c|w2),which are obtained by using an EM-basedclustering of dependency relations with amodel p(wi, fk) =?c p(wi|c)p(fk|c)p(c)(Kazama and Torisawa, 2008).
In order to3We did not use the discounting of the PMI values de-scribed in Pantel and Lin (2002).251alleviate the effect of local minima of the EMclustering, they proposed averaging the simi-larities by several different clustering results,which can be obtained by using different ini-tial parameters.
In this study, we combinedtwo clustering results (denoted as ?s1+s2?
inthe results), each of which (?s1?
and ?s2?
)has 2,000 hidden classes.4 We included thismethod since clustering can be regarded asanother way of treating data sparseness.BC The Bhattacharyya coefficient (Bhat-tacharyya, 1943) between p(fk|w1) andp(fk|w2).
This is the baseline for BCb.BCa The Bhattacharyya coefficient with absolutediscounting.
In calculating p(fk|wi), we sub-tract the discounting value, ?, from c(wi, fk)and equally distribute the residual probabil-ity mass to the contexts whose frequency iszero.
This is included as an example of naivesmoothing methods.Since it is very costly to calculate the sim-ilarities with all of the other words (one mil-lion in our case), we used the following approx-imation method that exploits the sparseness ofc(wi, fk).
Similar methods were used in Panteland Lin (2002), Kazama et al (2009), and Pan-tel et al (2009) as well.
For a given word, wi,we sort the contexts in descending order accord-ing to c(wi, fk) and retrieve the top-L contexts.5For each selected context, we sort the words in de-scending order according to c(wi, fk) and retrievethe top-M words (L = M = 1600).6 We mergeall of the words above as candidate words and cal-culate the similarity only for the candidate words.Finally, the top 500 similar words are output.Note also that we used modified counts,log(c(wi, fk)) + 1, instead of raw counts,c(wi, fk), with the intention of alleviating the ef-fect of strangely frequent dependencies, which canbe found in the Web data.
In preliminary ex-periments, we observed that this modification im-proves the quality of the top 500 similar words asreported in Terada et al (2004) and Kazama et al(2009).4In the case of EM clustering, the number of unique con-texts, fk, was also set to one million instead of 100,000, fol-lowing Kazama et al (2009).5It is possible that the number of contexts with non-zerocounts is less than L. In that case, all of the contexts withnon-zero counts are used.6Sorting is performed only once in the initialization step.Table 1: Performance on siblings (Set A).Measure MAP MP@1 @5 @10 @20JS 0.0299 0.197 0.122 0.0990 0.0792PMI-cos 0.0332 0.195 0.124 0.0993 0.0798Cls-JS (s1) 0.0319 0.195 0.122 0.0988 0.0796Cls-JS (s2) 0.0295 0.198 0.122 0.0981 0.0786Cls-JS (s1+s2) 0.0333 0.206 0.129 0.103 0.0841BC 0.0334 0.211 0.131 0.106 0.0854BCb (0.0002) 0.0345 0.223 0.138 0.109 0.0873BCb (0.0016) 0.0356 0.242 0.148 0.119 0.0955BCb (0.0032) 0.0325 0.223 0.137 0.111 0.0895BCa (0.0016) 0.0337 0.212 0.133 0.107 0.0863BCa (0.0362) 0.0345 0.221 0.136 0.110 0.0890BCa (0.1) 0.0324 0.214 0.128 0.101 0.0825without log(c(wi, fk)) + 1 modificationJS 0.0294 0.197 0.116 0.0912 0.0712PMI-cos 0.0342 0.197 0.125 0.0987 0.0793BC 0.0296 0.201 0.118 0.0915 0.0721As for BCb, we assumed that all of the hyper-parameters had the same value, i.e., ?k = ?.
Itis apparent that an excessively large ?
is not ap-propriate because it means ignoring observations.Therefore, ?must be tuned.
The discounting valueof BCa is also tuned.5.5 ResultsTable 1 shows the results for Set A.
The MAP andthe MPs at the top 1, 5, 10, and 20 are shown foreach similarity measure.
As for BCb and BCa, theresults for the tuned and several other values for ?are shown.
Figure 1 shows the parameter tuningfor BCb with MAP as the y-axis (results for BCaare shown as well).
Figure 2 shows the same re-sults with MPs as the y-axis.
The MAP and MPsshowed a correlation here.
From these results, wecan see that BCb surely improves upon BC, with6.6% improvement in MAP and 14.7% improve-ment in MP@1 when ?
= 0.0016.
BCb achievedthe best performance among the compared mea-sures with this setting.
The absolute discounting,BCa, improved upon BC as well, but the improve-ment was smaller than with BCb.
Table 1 alsoshows the results for the case where we did notuse the log-modified counts.
We can see that thismodification gives improvements (though slight orunclear for PMI-cos).Because tuning hyperparameters involves thepossibility of overfitting, its robustness should beassessed.
We checked whether the tuned ?
withSet A works well for Set B.
The results are shownin Table 2.
We can see that the best ?
(= 0.0016)found for Set A works well for Set B as well.
Thatis, the tuning of ?
as above is not unrealistic in2520.020.0220.0240.0260.0280.030.0320.0340.0361e-06  1e-05  0.0001  0.001  0.01  0.1  1MAP?
(log-scale)BayesAbsolute DiscountingFigure 1: Tuning of ?
(MAP).
The dashed hori-zontal line indicates the score of BC.0.04 0.060.08 0.10.12 0.140.16 0.180.2 0.220.24 0.261e-06  1e-05  0.0001  0.001  0.01MP?
(log-scale)MP@1MP@5MP@10MP@20MP@30MP@40Figure 2: Tuning of ?
(MP).practice because it seems that we can tune it ro-bustly using a small subset of the vocabulary asshown by this experiment.Next, we evaluated the measures on Set C, i.e.,the closed set data.
The results are shown in Ta-ble 3.
For this set, we observed a tendency thatis different from Sets A and B. Cls-JS showed aparticularly good performance.
BCb surely im-proves upon BC.
For example, the improvementwas 7.5% for MP@1.
However, the improvementin MAP was slight, and MAP did not correlatewell with MPs, unlike in the case of Sets A andB.We thought one possible reason is that the num-ber of outputs, 500, for each word was not largeenough to assess MAP values correctly becausethe average number of answers is 1,700 for thisdataset.
In fact, we could output more than 500words if we ignored the cost of storage.
Therefore,we also included the results for the case whereL = M = 3600 and N = 2, 000.
Even withthis setting, however, MAP did not correlate wellwith MPs.Although Cls-JS showed very good perfor-mance for Set C, note that the EM clusteringis very time-consuming (Kazama and Torisawa,2008), and it took about one week with 24 CPUcores to get one clustering result in our computingenvironment.
On the other hand, the preparationTable 2: Performance on siblings (Set B).Measure MAP MP@1 @5 @10 @20JS 0.0265 0.208 0.116 0.0855 0.0627PMI-cos 0.0283 0.203 0.116 0.0871 0.0660Cls-JS (s1+s2) 0.0274 0.194 0.115 0.0859 0.0643BC 0.0295 0.223 0.124 0.0922 0.0693BCb (0.0002) 0.0301 0.225 0.128 0.0958 0.0718BCb (0.0016) 0.0313 0.246 0.135 0.103 0.0758BCb (0.0032) 0.0279 0.228 0.127 0.0938 0.0698BCa (0.0016) 0.0297 0.223 0.125 0.0934 0.0700BCa (0.0362) 0.0298 0.223 0.125 0.0934 0.0705BCa (0.01) 0.0300 0.224 0.126 0.0949 0.0710Table 3: Performance on closed-sets (Set C).Measure MAP MP@1 @5 @10 @20JS 0.127 0.607 0.582 0.566 0.544PMI-cos 0.124 0.531 0.519 0.508 0.493Cls-JS (s1) 0.125 0.589 0.566 0.548 0.525Cls-JS (s2) 0.137 0.608 0.592 0.576 0.554Cls-JS (s1+s2) 0.152 0.638 0.617 0.603 0.583BC 0.131 0.602 0.579 0.565 0.545BCb (0.0004) 0.133 0.636 0.605 0.587 0.563BCb (0.0008) 0.131 0.647 0.615 0.594 0.568BCb (0.0016) 0.126 0.644 0.615 0.593 0.564BCb (0.0032) 0.107 0.573 0.556 0.529 0.496L = M = 3200 and N = 2000JS 0.165 0.605 0.580 0.564 0.543PMI-cos 0.165 0.530 0.517 0.507 0.492Cls-JS (s1+s2) 0.209 0.639 0.618 0.603 0.584BC 0.168 0.600 0.577 0.562 0.542BCb (0.0004) 0.170 0.635 0.604 0.586 0.562BCb (0.0008) 0.168 0.647 0.615 0.594 0.568BCb (0.0016) 0.161 0.644 0.615 0.593 0.564BCb (0.0032) 0.140 0.573 0.556 0.529 0.496for our method requires just an hour with a singlecore.6 DiscussionWe should note that the improvement by using ourmethod is just ?on average,?
as in many other NLPtasks, and observing clear qualitative change is rel-atively difficult, for example, by just showing ex-amples of similar word lists here.
Comparing theresults of BCb and BC, Table 4 lists the numbersof improved, unchanged, and degraded words interms of MP@20 for each evaluation set.
As canbe seen, there are a number of degraded words, al-though they are fewer than the improved words.Next, Figure 3 shows the averaged differences ofMP@20 in each 40,000 word-ID range.7 We canobserve that the advantage of BCb is lessened es-7Word IDs are assigned in ascending order when we chosethe top one million words as described in Section 5.2, andthey roughly correlate with frequencies.
So, frequent wordstend to have low-IDs.253Table 4: The numbers of improved, unchanged,and degraded words in terms of MP@20 for eachevaluation set.# improved # unchanged # degradedSet A 755 2,585 400Set B 643 2,610 404Set C 3,153 3,962 1,738-0.01 00.01 0.020.03 0.040.05 0.060  500000  1e+06Avg.
Diff.of MP@20ID range-0.01 00.01 0.020.03 0.040.05 0.060  500000  1e+06Avg.
Diff.of MP@20ID range-0.01 0 0.010.02 0.03 0.040.05 0.06 0.070.080  500000  1e+06Avg.
Diff.of MP@20ID rangeFigure 3: Averaged Differences of MP@20 be-tween BCb (0.0016) and BC within each 40,000ID range (Left: Set A.
Right: Set B.
Bottom: SetC).pecially for low-ID words (as expected) with on-average degradation.8 The improvement is ?on av-erage?
in this sense as well.One might suspect that the answer words tendedto be low-ID words, and the proposed method issimply biased towards low-ID words because ofits nature.
Then, the observed improvement is atrivial consequence.
Table 5 lists some interest-ing statistics about the IDs.
We can see that BCbsurely outputs more low-ID words than BC, andBC more than Cls-JS and JS.9 However, the av-erage ID of the outputs of BC is already lowerthan the average ID of the answer words.
There-fore, even if BCb preferred lower-ID words thanBC, it should not have the effect of improvingthe accuracy.
That is, the improvement by BCbis not superficial.
From BC/BCb, we can also seethat the IDs of the correct outputs did not becomesmaller compared to the IDs of the system outputs.Clearly, we need more analysis on what causedthe improvement by the proposed method and howthat affects the efficacy in real applications of sim-ilarity measures.The proposed Bayesian similarity measure out-performed the baseline Bhattacharyya coefficient8This suggests the use of different ?s depending on IDranges (e.g., smaller ?
for low-ID words) in practice.9The outputs of Cls-JS are well-balanced in the ID space.Table 5: Statistics on IDs.
(A): Avg.
ID of an-swers.
(B): Avg.
ID of system outputs.
(C): Avg.ID of correct system outputs.Set A Set C(A) 238,483 255,248(B) (C) (B) (C)Cls-JS (s1+s2) 282,098 176,706 273,768 232,796JS 183,054 11,3442 211,671 201,214BC 162,758 98,433 193,508 189,345BCb(0.0016) 55,915 54,786 90,472 127,877BC/BCb 2.91 1.80 2.14 1.48and other well-known similarity measures.
Asa smoothing method, it also outperformed anaive absolute discounting.
Of course, we can-not say that the proposed method is better thanany other sophisticated smoothing method at thispoint.
However, as noted above, there hasbeen no serious attempt to assess the effect ofsmoothing in the context of word similarity cal-culation.
Recent studies have pointed out thatthe Bayesian framework derives state-of-the-artsmoothing methods such as Kneser-Ney smooth-ing as a special case (Teh, 2006; Mochihashi etal., 2009).
Consequently, it is reasonable to re-sort to the Bayesian framework.
Conceptually,our method is equivalent to modifying p(fk|wi)as p(fk|wi) ={?(?0+a0)?
(?k+c(wi,fk)+ 12 )?
(?0+a0+ 12 )?
(?k+c(wi,fk))}2andtaking the Bhattacharyya coefficient.
However,the implication of this form has not yet been in-vestigated, and so we leave it for future research.Our method is the simplest one as a Bayesianmethod.
We did not employ any numerical opti-mization or sampling iterations, as in a more com-plete use of the Bayesian framework (Teh, 2006;Mochihashi et al, 2009).
Instead, we used the ob-tained analytical form directly with the assump-tion that ?k = ?
and ?
can be tuned directly byusing a simple grid search with a small subset ofthe vocabulary as the development set.
If substan-tial additional costs are allowed, we can fine-tuneeach ?k using more complete Bayesian methods.We also leave this for future research.In terms of calculation procedure, BCb has thesame form as other similarity measures, which isbasically the same as the inner product of sparsevectors.
Thus, it can be as fast as other similar-ity measures with some effort as we described inSection 4 when our aim is to calculate similaritiesbetween words in a fixed large vocabulary.
For ex-ample, BCb took about 100 hours to calculate the254top 500 similar nouns for all of the one millionnouns (using 16 CPU cores), while JS took about57 hours.
We think this is an acceptable additionalcost.The limitation of our method is that it can-not be used efficiently with similarity measuresother than the Bhattacharyya coefficient, althoughthat choice seems good as shown in the experi-ments.
For example, it seems difficult to use theJensen-Shannon divergence as the base similar-ity because the analytical form cannot be derived.One way we are considering to give more flexi-bility to our method is to adjust ?k depending onexternal knowledge such as the importance of acontext (e.g., PMIs).
In another direction, we willbe able to use a ?weighted?
Bhattacharyya coeffi-cient:?k ?
(w1, fk)?
(w2, fk)?p1k ?
p2k, wherethe weights, ?
(wi, fk), do not depend on pik, asthe base similarity measure.
The analytical formfor it will be a weighted version of BCb.BCb can also be generalized to the case wherethe base similarity is BCd(p1, p2) =?Kk=1 pd1k ?pd2k, where d > 0.
The Bayesian analytical formbecomes as follows.BCdb (w1, w2) =?
(?0 + a0)?
(?0 + b0)?
(?0 + a0 + d)?
(?0 + b0 + d)?KXk=1?
(?k + c(w1, fk) + d)?
(?k + c(w2, fk) + d)?
(?k + c(w1, fk))?
(?k + c(w2, fk)).See Appendix A for the derivation.
However, werestricted ourselves to the case of d = 12 in thisstudy.Finally, note that our BCb is different fromthe Bhattacharyya distance measure on Dirichletdistributions of the following form described inRauber et al (2008) in its motivation and analyti-cal form:p?(??0)?(?
?0)qQk ?(?
?k)qQk ?(?
?k)?Qk ?((?
?k + ??k)/2)?
( 12PKk (?
?k + ??k)).
(9)Empirical and theoretical comparisons with thismeasure also form one of the future directions.107 ConclusionWe proposed a Bayesian method for robust distri-butional word similarities.
Our method uses a dis-tribution of context profiles obtained by Bayesian10Our preliminary experiments show that calculating sim-ilarity using Eq.
9 for the Dirichlet distributions obtained byEq.
6 does not produce meaningful similarity (i.e., the accu-racy is very low).estimation and takes the expectation of a base sim-ilarity measure under that distribution.
We showedthat, in the case where the context profiles aremultinomial distributions, the priors are Dirichlet,and the base measure is the Bhattacharyya coeffi-cient, we can derive an analytical form, permittingefficient calculation.
Experimental results showthat the proposed measure gives better word simi-larities than a non-Bayesian Bhattacharyya coeffi-cient, other well-known similarity measures suchas Jensen-Shannon divergence and the cosine withPMI weights, and the Bhattacharyya coefficientwith absolute discounting.Appendix AHere, we give the analytical form for the general-ized case (BCdb ) in Section 6.
Recall the followingrelation, which is used to derive the normalizationfactor of the Dirichlet distribution:Z?Yk??
?k?1k d?
=Qk ?(??k)?(?
?0)= Z(??)?1.
(10)Then, BCdb (w1, w2)=ZZ???Dir(?1|??)Dir(?2|??
)Xk?d1k?d2k d?1 d?2= Z(??)Z(??)?ZZ???Yl???l?11lYm??
?m?12mXk?d1k?d2k d?1 d?2| {z }A.Using Eq.
10, A in the above can be calculated asfollows:=Z?Ym???m?12m24Xk?d2kZ????k+d?11kYl?=k??
?l?11l d?135 d?2=Z?Ym???m?12m"Xk?d2k?(?
?k + d)Ql?=k ?(??l)?(?
?0 + d)#d?2=Xk?(?
?k + d)Ql?=k ?(??l)?(?
?0 + d)Z???
?k+d?12kYm ?=k??
?m?12m d?2=Xk?(?
?k + d)Ql?=k ?(??l)?(?
?0 + d)?(?
?k + d)Qm?=k ?(??m)?(?
?0 + d)=Q?(??l)Q?(??m)?(?
?0 + d)?(?
?0 + d)Xk?(?
?k + d)?(??k)?(?
?k + d)?(?
?k).This will give:BCdb (w1, w2) =?(??0)?(??0)?(?
?0 + d)?(?
?0 + d)KXk=1?(?
?k + d)?(?
?k + d)?(??k)?(??k).255ReferencesA.
Bhattacharyya.
1943.
On a measure of divergencebetween two statistical populations defined by theirprobability distributions.
Bull.
Calcutta Math.
Soc.,49:214?224.Stanley F. Chen and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
TR-10-98, Computer Science Group,Harvard University.Stanley F. Chen and Ronald Rosenfeld.
2000.
Asurvey of smoothing techniques for ME models.IEEE Transactions on Speech and Audio Process-ing, 8(1):37?50.Corinna Cortes and Vladimir Vapnik.
1995.
Supportvector networks.
Machine Learning, 20:273?297.CRL.
2002.
EDR electronic dictionary version 2.0technical guide.
Communications Research Labo-ratory (CRL).Ido Dagan, Fernando Pereira, and Lillian Lee.
1994.Similarity-based estimation of word cooccurrenceprobabilities.
In Proceedings of ACL 94.Ido Dagan, Shaul Marcus, and Shaul Markovitch.1995.
Contextual word similarity and estimationfrom sparse data.
Computer, Speech and Language,9:123?152.Ido Dagan, Lillian Lee, and Fernando Pereira.
1997.Similarity-based methods for word sense disam-biguation.
In Proceedings of ACL 97.Ido Dagan, Lillian Lee, and Fernando Pereira.
1999.Similarity-based models of word cooccurrenceprobabilities.
Machine Learning, 34(1-3):43?69.Gregory Grefenstette.
1994.
Explorations In Auto-matic Thesaurus Discovery.
Kluwer Academic Pub-lishers.Zellig Harris.
1954.
Distributional structure.
Word,pages 146?142.Donald Hindle.
1990.
Noun classification frompredicate-argument structures.
In Proceedings ofACL-90, pages 268?275.Jun?ichi Kazama and Kentaro Torisawa.
2008.
In-ducing gazetteers for named entity recognition bylarge-scale clustering of dependency relations.
InProceedings of ACL-08: HLT.Jun?ichi Kazama, Stijn De Saeger, Kentaro Torisawa,and Masaki Murata.
2009.
Generating a large-scaleanalogy list using a probabilistic clustering based onnoun-verb dependency profiles.
In Proceedings of15th Annual Meeting of The Association for NaturalLanguage Processing (in Japanese).Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING/ACL-98, pages 768?774.Daichi Mochihashi, Takeshi Yamada, and NaonoriUeda.
2009.
Bayesian unsupervised word segmen-tation with nested Pitman-Yor language modeling.In Proceedings of ACL-IJCNLP 2009, pages 100?108.Masaki Murata, Qing Ma, Tamotsu Shirado, and Hi-toshi Isahara.
2004.
Database for evaluating ex-tracted terms and tool for visualizing the terms.
InProceedings of LREC 2004 Workshop: Computa-tional and Computer-Assisted Terminology, pages6?9.Patrick Pantel and Dekang Lin.
2002.
Discoveringword senses from text.
In Proceedings of the eighthACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 613?619.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu, and Vishnu Vyas.
2009.
Web-scaledistributional similarity and entity set expansion.
InProceedings of EMNLP 2009, pages 938?947.T.
W. Rauber, T. Braun, and K. Berns.
2008.
Proba-bilistic distance measures of the Dirichlet and Betadistributions.
Pattern Recognition, 41:637?645.Keiji Shinzato, Tomohide Shibata, Daisuke Kawahara,Chikara Hashimoto, and Sadao Kurohashi.
2008.Tsubaki: An open search engine infrastructure fordeveloping new information access.
In Proceedingsof IJCNLP 2008.Yee Whye Teh.
2006.
A hierarchical Bayesian lan-guage model based on Pitman-Yor processes.
InProceedings of COLING-ACL 2006, pages 985?992.Akira Terada, Minoru Yoshida, and Hiroshi Nakagawa.2004.
A tool for constructing a synonym dictionaryusing context information.
In IPSJ SIG TechnicalReport (in Japanese), pages 87?94.256
