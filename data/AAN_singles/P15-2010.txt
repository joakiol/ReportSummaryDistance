Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57?62,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDomain-Specific Paraphrase ExtractionEllie Pavlick1Juri Ganitkevitch2Tsz Ping Chan3Xuchen Yao4Benjamin Van Durme2,5Chris Callison-Burch11Computer and Information Science Department, University of Pennsylvania2Center for Language and Speech Processing, Johns Hopkins University3Bloomberg L.P., New York, NY4kitt.ai?, Seattle, WA5Human Language Technology Center of Excellence, Johns Hopkins UniversityAbstractThe validity of applying paraphrase rulesdepends on the domain of the text thatthey are being applied to.
We developa novel method for extracting domain-specific paraphrases.
We adapt the bilin-gual pivoting paraphrase method to biasthe training data to be more like our tar-get domain of biology.
Our best modelresults in higher precision while retainingcomplete recall, giving a 10% relative im-provement in AUC.1 IntroductionMany data-driven paraphrase extraction algo-rithms have been developed in recent years(Madnani and Dorr, 2010; Androutsopoulos andMalakasiotis, 2010).
These algorithms attemptto learn paraphrase rules, where one phrase canbe replaced with another phrase which has equiv-alent meaning in at least some context.
Deter-mining whether a paraphrase is appropriate fora specific context is a difficult problem (Bhagatand Hovy, 2013), encompassing issues of syntax(Callison-Burch, 2008), word sense (Apidianaki etal., 2014), and style (Xu et al, 2012; Pavlick andNenkova, 2015).
To date, the question of how do-main effects paraphrase has been left unexplored.Although most paraphrase extraction algo-rithms attempt to estimate a confidence with whicha paraphrase rule might apply, these scores arenot differentiated by domain, and instead corre-spond to the general domain represented by themodel?s training data.
As illustrated by Table 1,paraphrases that are highly probable in the gen-eral domain (e.g.
hot = sexy) can be extremelyimprobable in more specialized domains like biol-ogy.
Dominant word senses change depending on?Incubated by the Allen Institute for Artificial Intelli-gence.General Biologyhot warm, sexy, exciting heated, warm, thermaltreat address, handle, buy cure, fight, killhead leader, boss, mind skull, brain, craniumTable 1: Examples of domain-sensitive paraphrases.
Mostparaphrase extraction techniques learn paraphrases for a mixof senses that work well in general.
But in specific domains,paraphrasing should be sensitive to specialized language use.domain: the verb treat is used in expressions liketreat you to dinner in conversational domains ver-sus treat an infection in biology.
This domain shiftchanges the acceptability of its paraphrases.We address the problem of customizing para-phrase models to specific target domains.
We ex-plore the following ideas:1.
We sort sentences in the training corpusbased on how well they represent the targetdomain, and then extract paraphrases from asubsample of the most domain-like data.2.
We improve our domain-specific paraphrasesby weighting each training example based onits domain score, instead of treating each ex-ample equally.3.
We dramatically improve recall while main-taining precision by combining the subsam-pled in-domain paraphrase scores with thegeneral-domain paraphrase scores.2 BackgroundThe paraphrase extraction algorithm that we cus-tomize is the bilingual pivoting method (Bannardand Callison-Burch, 2005) that was used to createPPDB, the paraphrase database (Ganitkevitch etal., 2013).
To perform the subsampling, we adaptand improve the method that Moore and Lewis(2010) originally developed for domain-specificlanguage models in machine translation.572.1 Paraphrase extractionParaphrases can be extracted via bilingual pivot-ing.
Intuitively, if two English phrases e1and e2translate to the same foreign phrase f , we can as-sume that e1and e2have similar meaning, andthus we can ?pivot?
over f and extract ?e1, e2?
as aparaphrase pair.
Since many possible paraphrasesare extracted in this way, and since they vary inquality (in PPDB, the verb treat has 1,160 poten-tial paraphrases, including address, handle, dealwith, care for, cure him, ?m paying, and ?s on thehouse), it is necessary to assign some measure ofconfidence to each paraphrase rule.
Bannard andCallison-Burch (2005) defined a conditional para-phrase probability p(e2|e1) by marginalizing overall shared foreign-language translations f :p(e2|e1) ?
?fp(e2|f)p(f |e1) (1)where p(e2|f) and p(f |e1) are translation modelprobabilities estimated from the bilingual data.Equation 1 approximates the probability withwhich e1can paraphrase as e2, but its estimate in-evitably reflects the domain and style of the bilin-gual training text.
If e1is a polysemous word,the highest probabilities will be assigned to para-phrases of the most frequently occurring sense ofe1, and lower probabilities to less frequent senses.This results in inaccurate probability estimateswhen moving to a domain with different sense dis-tributions compared to the training corpus.2.2 Sorting by domain specificityThe crux of our method is to train a paraphrasemodel on data from the same domain as the one inwhich the paraphrases will be used.
In practice, itis unrealistic that we will be able to find bilingualparallel corpora precompiled for each domain ofinterest.
We instead subsample from a large bitext,biasing the sample towards the target domain.We adapt and extend a method developed byMoore and Lewis (2010) (henceforth M-L), whichbuilds a domain-specific sub-corpus from a large,general-domain corpus.
The M-L method assignsa score to each sentence in the large corpus basedon two language models, one trained on a sam-ple of target domain text and one trained on thegeneral domain.
We want to identify sentenceswhich are similar to our target domain and dissim-ilar from the general domain.
M-L captures thisnotion using the difference in the cross-entropiesaccording to each language model (LM).
That is,for a sentence si, we compute?i= Htgt(si)?Hgen(si) (2)where Htgtis the cross-entropy under the in-domain language model and Hgenis the cross-entropy under the general domain LM.
Cross-entropy is monotonically equivalent to LM per-plexity, in which lower scores imply a better fit.Lower ?isignifies greater domain-specificity.3 Domain-Specific ParaphrasesTo apply the M-L method to paraphrasing, weneed a sample of in-domain monolingual text.This data is not directly used to extract para-phrases, but instead to train an n-gram LM for thetarget domain.
We compute ?ifor the English sideof every sentence pair in our bilingual data, usingthe target domain LM and the general domain LM.We sort the entire bilingual training corpus so thatthe closer a sentence pair is to the top of the list,the more specific it is to our target domain.We can apply Bannard and Callison-Burch(2005)?s bilingual pivoting paraphrase extractionalgorithm to this sorted bitext in several ways:1.
By choosing a threshold value for ?iand dis-carding all sentence pairs that fall outsideof that threshold, we can extract paraphrasesfrom a subsampled bitext that approximatesthe target domain.2.
Instead of simply extracting from a subsam-pled corpus (where each training example isequally weighted), we can weight each train-ing example proportional to ?iwhen comput-ing the paraphrase scores.3.
We can combine multiple paraphrase scores:one derived from the original corpus and onefrom the subsample.
This has the advantageof producing the full set of paraphrases thatcan be extracted from the entire bitext.4 Experimental ConditionsDomain data We evaluate our domain-specificparaphrasing model in the target domain of biol-ogy.
Our monolingual in-domain data is a com-bination of text from the GENIA database (Kimet al, 2003) and text from an introductory biologytextbook.
Our bilingual general-domain data is the109word parallel corpus (Callison-Burch et al,582009), a collection of French-English parallel datacovering a mix of genres from legal text (Stein-berger et al, 2006) to movie subtitles (Tiedemann,2012).
We use 5-gram language models withKneser-Ney discounting (Heafield et al, 2013).Evaluation We measure the precision and recallof paraphrase pairs produced by each of our mod-els by collecting human judgments of what para-phrases are acceptable in sentences drawn fromthe target domain and in sentences drawn from thegeneral domain.
We sample 15K sentences fromour biology data, and 10K general-domain sen-tences from Wikipedia.
We select a phrase fromeach sentence, and show the list of candidate para-phrases1to 5 human judges.
Judges make a binarydecision about whether each paraphrase is appro-priate given the domain-specific context.
We con-sider a paraphrase rule to be good in the domain ifit is judged to be good in least one context by themajority of judges.
See Supplementary Materialsfor a detailed description of our methodology.Baseline We run normal paraphrase extractionover the entire 109word parallel corpus (whichhas 828M words on the English side) without anyattempt to bias it toward the target domain.
Werefer this system as General.Subsampling After sorting the 109word paral-lel corpus by Equation 2, we chose several thresh-old values for subsampling, keeping only top-ranked ?
words of the bitext.
We train models onfor several values of ?
(1.5M, 7M, 35M, and 166Mwords).
We refer to these model as M-L,T=?
.M-L Change Point We test a model where ?
isset at the point where ?iswitches from negativeto positive.
This includes all sentences which lookmore like the target domain than the general.
Thisthreshold is equivalent to sampling 20M words.Weighted Counts Instead of weighting eachsubsampled sentence equally, we test a novel ex-tension of M-L in which we weight each sentenceproportional to ?iwhen computing p(e2|e1).Combined Models We combine the subsam-pled models with the general model, using binarylogistic regression to combine the p(e2|e1) esti-mate of the general model and that of the domain-specific model.
We use 1,000 labeled pairs from1The candidates paraphrases constitute the full set of para-phrases that can be extracted from our training corpus.Figure 1: Precision-recall curves for paraphrase pairs ex-tracted by models trained on data from each of the describedsubsampling methods.
These curves are generated using the15k manually annotated sentences in the biology domain.the target domain to set the regression weights.This tuning set is disjoint from the test set.5 Experimental ResultsWhat is the effect of subsampling?
Figure 1compares the precision and recall of the differ-ent subsampling methods against the baseline oftraining on everything, when they are evaluatedon manually labeled test paraphrases from the bi-ology domain.
All of subsampled models have ahigher precision than the baseline General model,except for the largest of the subsampled models(which was trained on sentence pairs with 166Mwords - many of which are more like the generaldomain than the biology domain).The subsampled models have reduced recallsince many of the paraphrases that occur in the full109word bilingual training corpus do not occur inthe subsamples.
As we increase ?
we improve re-call at the expense of precision, since we are in-cluding training data that is less and less like ourtarget domain.
The highest precision model basedon the vanilla M-L method is M-L Change Point,which sets the subsample size to include exactlythose sentence pairs that look more like the targetdomain than the general domain.Our novel extension of the M-L model (M-LWeighted) provides further improvements.
Here,we weight each sentence pair in the bilingual train-ing corpus proportional to ?iwhen computingthe paraphrase scores.
Specifically, we weightthe counting during the bilingual pivoting so that59(a) Biology domain(b) General domainFigure 2: Performance of models build by combining small domain-specific models trained on subsampled data with generaldomain models trained on all the data.
Performance in the general domain are shown as a control.rather than each occurrence counting as 1, eachoccurrence counts as the ratio of the sentence?scross-entropies:HgenHtgt.
The top-ranked sentencepairs receive an exaggerated count of 52, whilethe bottom ones receive a tiny factional count of0.0068.
Thus, paraphrases extracted from sen-tence pairs that are unlike the biology domain re-ceive very low scores.
This allows us to achievehigher recall by incorporating more training data,while also improving the precision.What is the benefit of combining models?
Wehave demonstrated that extracting paraphrasesfrom subsampled data results in higher precisiondomain-specific paraphrases.
But these modelsextract only a fraction of the paraphrases that areextracted by a general model trained on the fullbitext, resulting in a lower recall.We dramatically improve the recall of ourdomain-specific models by combining the smallsubsampled models with the large general-domainmodel.
We use binary logistic regression to com-bine the p(e2|e1) estimate of the general modelwith that of each domain-specific model.
Figure2(a) shows that we are able to extend the recallof our domain-specific models to match the recallof the full general-domain model.
The precisionscores remain higher for the domain-specific mod-els.
Our novel M-L Weighted model performs thebest.
Table 3 gives the area under the curve (AUC).The best combination improves AUC by morethan 4 points absolute (>10 points relative) in thebiology domain.
Table 2 provides examples ofparaphrases extracted using our domain-specificgeneral / bio-spec.
general / bio-spec.air aerial / atmosphere fruit result / fruitingbalance pay / equilibrate heated lively / hotbreaks pauses / ruptures motion proposal / movementTable 2: Top paraphrase under the general and the bestdomain-specific model, General+M-L Weighted.AUC ?absolute?relativeGeneral 39.5 ?
?Gen.+M-L,T=1 40.8 +1.3 +3.3Gen.+M-L,T=145 40.8 +1.3 +3.3Gen.+M-L,T=29 41.2 +1.7 +4.3Gen.+M-L CP 41.9 +2.4 +6.1Gen.+M-L,T=6 42.3 +2.8 +7.1Gen.+M-L Weighted 43.7 +4.2 +10.6Table 3: AUC (?
100) for each model in the biology domainfrom Figure 2(a).model for biology versus the baseline model.6 Related WorkDomain-specific paraphrasing has not receivedprevious attention, but there is relevant prior workon domain-specific machine translation (MT).
Webuild on the Moore-Lewis method, which hasbeen used for language models (Moore and Lewis,2010) and translation models (Axelrod et al,2011).
Similar methods use LM perplexity torank sentences (Gao et al, 2002; Yasuda et al,2008), rather than the difference in cross-entropy.Within MT, Foster and Kuhn (2007) used log-linear weightings of translation probabilities tocombine models trained in different domains, aswe do here.
Relevant to our proposed method of60fractional counting, (Madnani et al, 2007) usedintroduced a count-centric approach to paraphraseprobability estimation.
Matsoukas et al (2009)and Foster et al (2010) explored weighted trainingsentences for MT, but set weights discriminativelybased on sentence-level features.7 ConclusionWe have discussed the new problem of extractingdomain-specific paraphrases.
We adapt a methodfrom machine translation to the task of learn-ing domain-biased paraphrases from bilingual cor-pora.
We introduce two novel extensions to thismethod.
Our best domain-specific model dramat-ically improves paraphrase quality for the targetdomain.Acknowledgements This research was sup-ported by the Allen Institute for Artificial Intel-ligence (AI2), the Human Language TechnologyCenter of Excellence (HLTCOE), and by giftsfrom the Alfred P. Sloan Foundation, Google, andFacebook.
This material is based in part on re-search sponsored by the NSF under grant IIS-1249516 and DARPA under agreement numberFA8750-13-2-0017 (the DEFT program).
TheU.S.
Government is authorized to reproduce anddistribute reprints for Governmental purposes.The views and conclusions contained in this pub-lication are those of the authors and should not beinterpreted as representing official policies or en-dorsements of DARPA or the U.S. Government.We would like to thank Luke Orland for his con-tributions to this research, and to thank the anony-mous reviewers for their thoughtful comments.ReferencesIon Androutsopoulos and Prodromos Malakasiotis.2010.
A survey of paraphrasing and textual entail-ment methods.
JAIR, pages 135?187.Marianna Apidianaki, Emilia Verzeni, and Diana Mc-Carthy.
2014.
Semantic clustering of pivot para-phrases.
In LREC.Amittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domain dataselection.
In EMNLP, pages 355?362.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In ACL,pages 597?604.Rahul Bhagat and Eduard Hovy.
2013.
What is a para-phrase?
Computational Linguistics, 39(3):463?472.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation.Chris Callison-Burch.
2008.
Syntactic constraintson paraphrases extracted from parallel corpora.
InEMNLP, pages 196?205.
Association for Computa-tional Linguistics.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for smt.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In EMNLP,pages 451?459.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In NAACL-HLT, pages 758?764, Atlanta,Georgia, June.Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-Fu Lee.
2002.
Toward a unified approach to sta-tistical language modeling for chinese.
ACM Trans-actions on Asian Language Information Processing(TALIP), 1(1):3?33.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan HClark, and Philipp Koehn.
2013.
Scalable modifiedkneser-ney language model estimation.
In ACL.J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsu-jii.
2003.
Genia corpusa semantically annotatedcorpus for bio-textmining.
Bioinformatics, 19(suppl1):i180?i182.Nitin Madnani and Bonnie J. Dorr.
2010.
Generatingphrasal and sentential paraphrases: A survey of data-driven methods.
Computational Linguistics, 36.Nitin Madnani, Necip Fazil Ayan, Philip Resnik, andBonnie Dorr.
2007.
Using paraphrases for parame-ter tuning in statistical machine translation.
In Pro-ceedings of the Workshop on Machine Translation.Spyros Matsoukas, Antti-Veikko I Rosti, and BingZhang.
2009.
Discriminative corpus weight esti-mation for machine translation.
In EMNLP, pages708?717.Robert C Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In ACL,pages 220?224.Ellie Pavlick and Ani Nenkova.
2015.
Inducing lexicalstyle properties for paraphrase and genre differenti-ation.
In NAACL.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz Erjavec, Dan Tufis, andD?aniel Varga.
2006.
The jrc-acquis: A multilingualaligned parallel corpus with 20+ languages.
arXivpreprint.61J?org Tiedemann.
2012.
Parallel data, tools and inter-faces in opus.
In LREC, pages 2214?2218.Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, andColin Cherry.
2012.
Paraphrasing for style.
InCOLING, pages 2899?2914.Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,and Eiichiro Sumita.
2008.
Method of selectingtraining data to build a compact and efficient trans-lation model.
In IJCNLP, pages 655?660.62
