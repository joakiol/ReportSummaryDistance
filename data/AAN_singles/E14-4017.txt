Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 84?89,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsPainless Semi-Supervised Morphological Segmentation using ConditionalRandom FieldsTeemu RuokolainenaOskar KohonenbSami VirpiojabMikko KurimoaaDepartment of Signal Processing and Acoustics, Aalto UniversitybDepartment of Information and Computer Science, Aalto Universityfirstname.lastname@aalto.fiAbstractWe discuss data-driven morphologicalsegmentation, in which word forms aresegmented into morphs, that is the surfaceforms of morphemes.
We extend a re-cent segmentation approach based on con-ditional random fields from purely super-vised to semi-supervised learning by ex-ploiting available unsupervised segmenta-tion techniques.
We integrate the unsu-pervised techniques into the conditionalrandom field model via feature set aug-mentation.
Experiments on three di-verse languages show that this straight-forward semi-supervised extension greatlyimproves the segmentation accuracy of thepurely supervised CRFs in a computation-ally efficient manner.1 IntroductionWe discuss data-driven morphological segmenta-tion, in which word forms are segmented intomorphs, the surface forms of morphemes.
Thistype of morphological analysis can be useful foralleviating language model sparsity inherent tomorphologically rich languages (Hirsim?ki et al.,2006; Creutz et al., 2007; Turunen and Kurimo,2011; Luong et al., 2013).
Particularly, we focuson a low-resource learning setting, in which onlya small amount of annotated word forms are avail-able for model training, while unannotated wordforms are available in abundance.We study morphological segmentation usingconditional random fields (CRFs), a discrimina-tive model for sequential tagging and segmenta-tion (Lafferty et al., 2001).
Recently, Ruoko-lainen et al.
(2013) showed that the CRFs canyield competitive segmentation accuracy com-pared to more complex, previous state-of-the-art techniques.
While CRFs yielded generallythe highest accuracy compared to their referencemethods (Poon et al., 2009; Kohonen et al., 2010),on the smallest considered annotated data sets of100 word forms, they were outperformed by thesemi-supervised Morfessor algorithm (Kohonen etal., 2010).
However, Ruokolainen et al.
(2013)trained the CRFs solely on the annotated data,without any use of the available unannotated data.In this work, we extend the CRF-based ap-proach to leverage unannotated data in a straight-forward and computationally efficient manner viafeature set augmentation, utilizing predictions ofunsupervised segmentation algorithms.
Experi-ments on three diverse languages show that thesemi-supervised extension substantially improvesthe segmentation accuracy of the CRFs.
The ex-tension also provides higher accuracies on all theconsidered data set sizes and languages comparedto the semi-supervised Morfessor (Kohonen et al.,2010).In addition to feature set augmentation, thereexists numerous approaches for semi-supervisedCRF model estimation, exemplified by minimumentropy regularization (Jiao et al., 2006), gen-eralized expectations criteria (Mann and McCal-lum, 2008), and posterior regularization (He et al.,2013).
In this work, we employ the feature-basedapproach due to its simplicity and the availabil-ity of useful unsupervised segmentation methods.Varying feature set augmentation approaches havebeen successfully applied in several related tasks,such as Chinese word segmentation (Wang et al.,2011; Sun and Xu, 2011) and chunking (Turian etal., 2010).The paper is organized as follows.
In Section 2,we describe the CRF-based morphological seg-mentation approach following (Ruokolainen et al.,2013), and then show how to extend this approachto leverage unannotated data in an efficient man-ner.
Our experimental setup and results are dis-cussed in Sections 3 and 4, respectively.
Finally,84we present conclusions on the work in Section 5.2 Methods2.1 Supervised Morphological Segmentationusing CRFsWe present the morphological segmentation taskas a sequential labeling problem by assigning eachcharacter to one of three classes, namely {be-ginning of a multi-character morph (B), middleof a multi-character morph (M), single charactermorph (S)}.
We then perform the sequential label-ing using linear-chain CRFs (Lafferty et al., 2001).Formally, the linear-chain CRF model distribu-tion for label sequence y = (y1, y2, .
.
.
, yT) anda word form x = (x1, x2, .
.
.
, xT) is written as aconditional probabilityp (y |x;w) ?T?t=2exp(w ?
?
(yt?1, yt, x, t)),(1)where t indexes the character positions,w denotesthe model parameter vector, and ?
the vector-valued feature extracting function.
The model pa-rameters w are estimated discrimatively based ona training set of exemplar input-output pairs (x, y)using, for example, the averaged perceptron algo-rithm (Collins, 2002).
Subsequent to estimation,the CRF model segments test word forms usingthe Viterbi algorithm (Lafferty et al., 2001).We next describe the feature set{?i(yt?1, yt, x, t)}|?|i=1by defining emissionand transition features.
Denoting the label set {B,M, S} as Y , the emission feature set is defined as{?m(x, t)1(yt= y?t) |m ?
1..M ,?y?t?
Y} ,(2)where the indicator function 1(yt= y?t) returnsone if and only if yt= y?tand zero otherwise, thatis1(yt= y?t) ={1 if yt= y?t0 otherwise, (3)and {?m(x, t)}Mm=1is the set of functions describ-ing the character position t. Following Ruoko-lainen et al.
(2013), we employ binary functionsthat describe the position t of word x using all leftand right substrings up to a maximum length ?.The maximum substring length ?maxis considereda hyper-parameter to be adjusted using a develop-ment set.
While the emission features associatethe input to labels, the transition feature set{1(yt?1= y?t?1)1(yt= y?t) | y?t, y?t?1?
Y} (4)captures the dependencies between adjacent labelsas irrespective of the input x.2.2 Leveraging Unannotated DataIn order to utilize unannotated data, we explore astraightforward approach based on feature set aug-mentation.
We exploit predictions of unsupervisedsegmentation algorithms by defining variants ofthe features described in Section 2.1.
The idea isto compensate the weaknesses of the CRF modeltrained on the small annotated data set using thestrengths of the unsupervised methods that learnfrom large amounts of unannotated data.For example, consider utilizing predictions ofthe unsupervised Morfessor algorithm (Creutz andLagus, 2007) in the CRF model.
In order to ac-complish this, we first learn the Morfessor modelfrom the unannotated training data, and then ap-ply the learned model on the word forms in theannotated training set.
Assuming the annotatedtraining data includes the English word drivers,the Morfessor algorithm might, for instance, re-turn a (partially correct) segmentation driv + ers.We present this segmentation by defining a func-tion ?
(t), which returns 0 or 1, if the position t isin the middle of a segment or in the beginning of asegment, respectively, as int 1 2 3 4 5 6 7xtd r i v e r s?
(t) 1 0 0 0 1 0 0Now, given a set of U functions {?u(t)}Uu=1, wedefine variants of the emission features in (2) as{?u(x, t)?m(x, t)1(yt= y?t) |?u ?
1..U ,?m ?
1..M ,?y?t?
Y} .
(5)By adding the expanded features of form (5), theCRF model learns to associate the output of theunsupervised algorithms in relation to the sur-rounding substring context.
Similarly, an ex-panded transition feature is written as{?u(x, t)1(yt?1= y?t?1)1(yt= y?t) |?u ?
1..U ,?y?t, y?t?1?
Y} .
(6)After defining the augmented feature set, theCRF model parameters can be estimated in a stan-dard manner on the small, annotated training dataset.
Subsequent to CRF training, the Morfessormodel is applied on the test instances in order toallow the feature set augmentation and standarddecoding with the estimated CRF model.
We ex-pect the Morfessor features to specifically improve85segmentation of compound words (for example,brain+storm), which are modeled with high ac-curacy by the unsupervised Morfessor algorithm(Creutz and Lagus, 2007), but can not be learnedfrom the small number of annotated examplesavailable for the supervised CRF training.As another example of a means to augment thefeature set, we make use of the fact that the outputof the unsupervised algorithms does not have to bebinary (zeros and ones).
To this end, we employthe classic letter successor variety (LSV) scorespresented originally by (Harris, 1955).1The LSVscores utilize the insight that the predictability ofsuccessive letters should be high within morphsegments, and low at the boundaries.
Conse-quently, a high variety of letters following a prefixindicates a high probability of a boundary.
We usea variant of the LSV values presented by ?
?ltekin(2010), in which we first normalize the scores bythe average score at each position t, and subse-qently logarithmize the normalized value.
WhileLSV score tracks predictability given prefixes, thesame idea can be utilized for suffixes, providingthe letter predecessor variety (LPV).
Subsequentto augmenting the feature set using the functionsLSV (t) and LPV (t), the CRF model learns toassociate high successor and predecessor values(low predictability) to high probability of a seg-ment boundary.
Appealingly, the Harris featurescan be obtained in a computationally inexpensivemanner, as they merely require counting statisticsfrom the unannotated data.The feature set augmentation approach de-scribed above is computationally efficient, if thecomputational overhead from the unsupervisedmethods is small.
This is because the CRF param-eter estimation is still based on the small amountof labeled examples as described in Section 2.1,while the number of features incorporated in theCRF model (equal to the number of parameters)grows linearly in the number of exploited unsu-pervised algorithms.3 Experimental Setup3.1 DataWe perform the experiments on the Morpho Chal-lenge 2009/2010 data set (Kurimo et al., 2009; Ku-1We also experimented on modifying the output of theMorfessor algorithm from binary to probabilistic, but thesesoft cues provided no consistent advantage over the standardbinary output.English Finnish TurkishTrain (unann.)
384,903 2,206,719 617,298Train (ann.)
1,000 1,000 1,000Devel.
694 835 763Test 10,000 10,000 10,000Table 1: Number of word types in the MorphoChallenge data set.rimo et al., 2010) consisting of manually preparedmorphological segmentations in English, Finnishand Turkish.
We follow the experiment setup, in-cluding data partitions and evaluation metrics, de-scribed by Ruokolainen et al.
(2013).
Table 1shows the total number of instances available formodel estimation and testing.3.2 CRF Feature Extraction and TrainingThe substring features included in the CRF modelare described in Section 2.1.
We include all sub-strings which occur in the training data.
The Mor-fessor and Harris (successor and predecessor va-riety) features employed by the semi-supervisedextension are described in Section 2.2.
We ex-perimented on two variants of the Morfessor al-gorithm, namely, the Morfessor Baseline (Creutzand Lagus, 2002) and Morfessor Categories-MAP(Creutz and Lagus, 2005), CatMAP for short.
TheBaseline models were trained on word types andthe perplexity thresholds of the CatMAP modelswere set equivalently to the reference runs in Mor-pho Challenge 2010 (English: 450, Finnish: 250,Turkish: 100); otherwise the default parameterswere used.
The Harris features do not require anyhyper-parameters.The CRF model (supervised and semi-supervised) is trained using the averagedperceptron algorithm (Collins, 2002).
The num-ber of passes over the training set made by theperceptron algorithm, and the maximum length ofsubstring features are optimized on the held-outdevelopment sets.The experiments are run on a standard desktopcomputer using a Python-based single-threadedCRF implementation.
For Morfessor Baseline, weuse the recently published implementation by Vir-pioja et al.
(2013).
For Morfessor CatMAP, weused the Perl implementation by Creutz and La-gus (2005).863.3 Reference MethodsWe compare our method?s performance withthe fully supervised CRF model and the semi-supervised Morfessor algorithm (Kohonen et al.,2010).
For semi-supervised Morfessor, we use thePython implementation by Virpioja et al.
(2013).4 ResultsSegmentation accuracies for all languages are pre-sented in Table 2.
The columns titled Train (ann.
)and Train (unann.)
denote the number of anno-tated and unannotated training instances utilizedby the method, respectively.
To summarize, thesemi-supervised CRF extension greatly improvedthe segmentation accuracy of the purely super-vised CRFs, and also provided higher accuraciescompared to the semi-supervised Morfessor algo-rithm2.Appealingly, the semi-supervised CRF exten-sion already provided consistent improvementover the supervised CRFs, when utilizing the com-putationally inexpensive Harris features.
Addi-tional gains were then obtained using the Morfes-sor features.
On all languages, highest accuracieswere obtained using a combination of Harris andCatMAP features.Running the CRF parameter estimation (includ-ing hyper-parameters) consumed typically up to afew minutes.
Computing statistics for the Harrisfeatures also took up roughly a few minutes onall languages.
Learning the unsupervised Mor-fessor algorithm consumed 3, 47, and 20 min-utes for English, Finnish, and Turkish, respec-tively.
Meanwhile, CatMAP model estimationwas considerably slower, consuming roughly 10,50, and 7 hours for English, Finnish and Turkish,respectively.
Training and decoding with semi-supervised Morfessor took 21, 111, and 47 hoursfor English, Finnish and Turkish, respectively.5 ConclusionsWe extended a recent morphological segmenta-tion approach based on CRFs from purely super-vised to semi-supervised learning.
We accom-plished this in an efficient manner using feature setaugmentation and available unsupervised segmen-tation techniques.
Experiments on three diverse2The improvements over the supervised CRFs and semi-supervised Morfessor were statistically significant (confi-dence level 0.95) according to the standard 1-sided Wilcoxonsigned-rank test performed on 10 randomly divided, non-overlapping subsets of the complete test sets.Method Train (ann.)
Train (unann.)
F1EnglishCRF 100 0 78.8S-MORF.
100 384,903 83.7CRF (Harris) 100 384,903 80.9CRF (BL+Harris) 100 384,903 82.6CRF (CM+Harris) 100 384,903 84.4CRF 1,000 0 85.9S-MORF.
1,000 384,903 84.3CRF (Harris) 1,000 384,903 87.6CRF (BL+Harris) 1,000 384,903 87.9CRF (CM+Harris) 1,000 384,903 88.4FinnishCRF 100 0 65.5S-MORF.
100 2,206,719 70.4CRF (Harris) 100 2,206,719 78.9CRF (BL+Harris) 100 2,206,719 79.3CRF (CM+Harris) 100 2,206,719 82.0CRF 1,000 0 83.8S-MORF.
1,000 2,206,719 76.4CRF (Harris) 1,000 2,206,719 88.3CRF (BL+Harris) 1,000 2,206,719 88.9CRF (CM+Harris) 1,000 2,206,719 89.4TurkishCRF 100 0 77.7S-MORF.
100 617,298 78.2CRF (Harris) 100 617,298 82.6CRF (BL+Harris) 100 617,298 84.9CRF (CM+Harris) 100 617,298 85.5CRF 1,000 0 88.6S-MORF.
1,000 617,298 87.0CRF (Harris) 1,000 617,298 90.1CRF (BL+Harris) 1,000 617,298 91.7CRF (CM+Harris) 1,000 617,298 91.8Table 2: Results on test data.
CRF (BL+Harris)denotes semi-supervised CRF extension usingMorfessor Baseline and Harris features, whileCRF (CM+Harris) denotes CRF extension em-ploying Morfessor CatMAP and Harris features.languages showed that this straightforward semi-supervised extension greatly improves the seg-mentation accuracy of the supervised CRFs, whilebeing computationally efficient.
The extensionalso outperformed the semi-supervised Morfessoralgorithm on all data set sizes and languages.AcknowledgementsThis work was financially supported by Langnet(Finnish doctoral programme in language studies)and the Academy of Finland under the FinnishCentre of Excellence Program 2012?2017 (grantno.
251170), project Multimodally grounded lan-guage technology (no.
254104), and LASTU Pro-gramme (nos.
256887 and 259934).87ReferencesMichael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing (EMNLP 2002), vol-ume 10, pages 1?8.
Association for ComputationalLinguistics.?agr?
??ltekin.
2010.
Improving successor varietyfor morphological segmentation.
In Proceedings ofthe 20th Meeting of Computational Linguistics in theNetherlands.Mathias Creutz and Krista Lagus.
2002.
Unsuperviseddiscovery of morphemes.
In Mike Maxwell, editor,Proceedings of the ACL-02 Workshop on Morpho-logical and Phonological Learning, pages 21?30,Philadelphia, PA, USA, July.
Association for Com-putational Linguistics.Mathias Creutz and Krista Lagus.
2005.
Inducing themorphological lexicon of a natural language fromunannotated text.
In Timo Honkela, Ville K?n?nen,Matti P?ll?, and Olli Simula, editors, Proceedings ofAKRR?05, International and Interdisciplinary Con-ference on Adaptive Knowledge Representation andReasoning, pages 106?113, Espoo, Finland, June.Helsinki University of Technology, Laboratory ofComputer and Information Science.Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing, 4(1):3:1?3:34, January.Mathias Creutz, Teemu Hirsim?ki, Mikko Kurimo,Antti Puurula, Janne Pylkk?nen, Vesa Siivola, MattiVarjokallio, Ebru Arisoy, Murat Sara?lar, and An-dreas Stolcke.
2007.
Morph-based speech recog-nition and modeling of out-of-vocabulary wordsacross languages.
ACM Transactions on Speech andLanguage Processing, 5(1):3:1?3:29, December.Zellig Harris.
1955.
From phoneme to morpheme.Language, 31(2):190?222.Luheng He, Jennifer Gillenwater, and Ben Taskar.2013.
Graph-based posterior regularization forsemi-supervised structured prediction.
In Proceed-ings of the Seventeenth Conference on Computa-tional Natural Language Learning, pages 38?46,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Teemu Hirsim?ki, Mathias Creutz, Vesa Siivola, MikkoKurimo, Sami Virpioja, and Janne Pylkk?nen.2006.
Unlimited vocabulary speech recognitionwith morph language models applied to Finnish.Computer Speech and Language, 20(4):515?541,October.Feng Jiao, Shaojun Wang, Chi-Hoon Lee, RussellGreiner, and Dale Schuurmans.
2006.
Semi-supervised conditional random fields for improvedsequence segmentation and labeling.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th annual meetingof the Association for Computational Linguistics,pages 209?216.
Association for Computational Lin-guistics.Oskar Kohonen, Sami Virpioja, and Krista Lagus.2010.
Semi-supervised learning of concatenativemorphology.
In Proceedings of the 11th Meeting ofthe ACL Special Interest Group on ComputationalMorphology and Phonology, pages 78?86, Uppsala,Sweden, July.
Association for Computational Lin-guistics.Mikko Kurimo, Sami Virpioja, Ville Turunen,Graeme W. Blackwood, and William Byrne.
2009.Overview and results of Morpho Challenge 2009.
InWorking Notes for the CLEF 2009 Workshop, Corfu,Greece, September.Mikko Kurimo, Sami Virpioja, and Ville Turunen.2010.
Overview and results of Morpho Chal-lenge 2010.
In Proceedings of the Morpho Chal-lenge 2010 Workshop, pages 7?24, Espoo, Finland,September.
Aalto University School of Science andTechnology, Department of Information and Com-puter Science.
Technical Report TKK-ICS-R37.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Carla E. Brodley and Andrea Po-horeckyj Danyluk, editors, Proceedings of the Eigh-teenth International Conference on Machine Learn-ing, pages 282?289, Williamstown, MA, USA.
Mor-gan Kaufmann.Minh-Thang Luong, Richard Socher, and Christo-pher D Manning.
2013.
Better word representa-tions with recursive neural networks for morphol-ogy.
In Proceedings of the Seventeenth Confer-ence on Computational Natural Language Learning(CoNLL), pages 29?37.
Association for Computa-tional Linguistics, August.Gideon Mann and Andrew McCallum.
2008.
General-ized expectation criteria for semi-supervised learn-ing of conditional random fields.
In Proceedingsof ACL-08: HLT, pages 870?878.
Association forComputational Linguistics.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentationwith log-linear models.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, pages 209?217.Association for Computational Linguistics.Teemu Ruokolainen, Oskar Kohonen, Sami Virpioja,and Mikko Kurimo.
2013.
Supervised morpholog-ical segmentation in a low-resource learning settingusing conditional random fields.
In Proceedings of88the Seventeenth Conference on Computational Nat-ural Language Learning (CoNLL), pages 29?37.
As-sociation for Computational Linguistics, August.Weiwei Sun and Jia Xu.
2011.
Enhancing Chineseword segmentation using unlabeled data.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 970?979.
As-sociation for Computational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 384?394.
Association forComputational Linguistics.Ville Turunen and Mikko Kurimo.
2011.
Speech re-trieval from unsegmented Finnish audio using statis-tical morpheme-like units for segmentation, recog-nition, and retrieval.
ACM Transactions on Speechand Language Processing, 8(1):1:1?1:25, October.Sami Virpioja, Peter Smit, Stig-Arne Gr?nroos, andMikko Kurimo.
2013.
Morfessor 2.0: Python im-plementation and extensions for Morfessor Baseline.Report 25/2013 in Aalto University publication se-ries SCIENCE + TECHNOLOGY, Department ofSignal Processing and Acoustics, Aalto University.Yiou Wang, Yoshimasa Tsuruoka Jun?ichi Kazama,Yoshimasa Tsuruoka, Wenliang Chen, Yujie Zhang,and Kentaro Torisawa.
2011.
Improving Chineseword segmentation and POS tagging with semi-supervised methods using large auto-analyzed data.In IJCNLP, pages 309?317.89
