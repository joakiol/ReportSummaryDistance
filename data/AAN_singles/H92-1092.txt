COMPARISON OF AUDITORY MODELS FORROBUST SPEECH RECOGNITION*Charles R. Jankowski Jr. and Richard P LippmannMIT Lincoln Laboratory244 Wood StreetLexington, MA 02173ABSTRACTTwo auditory front ends which emulate some aspects of the humanauditory system were compared using a high performance isolatedword Hidden Markov Model (HMM) speech recognizer.
In theseinitial studies, auditory models from Seneff \[2\] and Ghitza \[4\]were compared using both clean speech and speech corrupted byspeech-like "babble" noise.
Preliminary results indicate that theauditory models reduce the error ate slightly, especially at inter-mediate and high noise levels.1.
MOTIVATIONThe performance of speech recognizers often degrades dra-matically in noise, with different alking styles, when themicrophone is changed, and as a talker moves relative to amicrophone.
New auditory front ends that mimic someaspects of human auditory-nerve and psychoacousticbehavior have been proposed to reduce these problems.Although past limited experiments suggest that these frontends improve robustness, no thorough comparisons havebeen performed using high-performance Hidden MarkovModel (HMM) recognizers.
In addition, few studies haveevaluated the effect of speech babble noise and frequencyresponse variability on performance orexplored alternativeapproaches to feature reduction.
This is an early progressreport on research in this area.
Further ongoing experimentsare exploring additional front ends and alternative datareduction techniques.2.
AUDITORY MODELSTwo auditory front ends which produce features that corre-spond to phase or synchrony information i  the speech sig-nal were explored.
These auditory front ends werecompared to a more conventional reel-scale cepstral frontend.
*This work was sponsored by the Defense AdvancedResearch th-ojects Agency.
The views expressed arethose of the authors and do not reflect the official policyor position of the U. S. Government.2.1.
Mel-Scale CepstraThe reel-scale cepstral front end described by Davis andMermelstein \[1\] was used as a reference.
This is a commonsignal representation which is currently used in all speechrecognition systems at Lincoln Laboratory.
In this frontend, a 20 ms Hamming window is applied to the speechsignal every 10 ms.
The power spectrum of the windowedwaveform is weighted by a series of filters, linearly spacedfrom 0 to 1000 Hz, and logarithmically spaced above 1000Hz.
Each filter "width" is twice the spacing.
An inversecosine transform converts the logarithm of the resulting fil-ter bank coefficients o the cepstral domain.2.2.
Seneff's Auditory ModelThe first auditory front end evaluated was motivated fromphysiological data and described by Seneff \[2,3\].
This frontend incorporates a first stage of 40 linear filters, followedby a series of nonlinearities modelling the transformationfrom basilar membrane motion to auditory nerve stimula-tion.
Such nonlinearities include soft half-wave rectifica-tion, a model for short-term adaptation, and a rapid AGC.Seneff's front end had two outputs.
"Mean rate" outputs aregenerated by detecting the envelope of the nonlinear stageoutput.
These outputs roughly corresponds tospectral mag-nitude information.
"Synchrony" outputs detect he extentthat the nonlinear stage output for a particular channel hasenergy at the center frequency for that channel.
This emu-lates the extent o which the nerve firings from a particularlocation of the basilar membrane are synchronized to the"characteristic frequency" corresponding tothat location.2.3.
EIHThe second auditory front end evaluated was the EnsembleInterval Histogram (EIH) model developed by Ghitza \[4\].The EIH model has a first stage of linear filters similar toSeneff's, but with a considerably higher number of filters(133 instead of 40).
The second stage takes the output ofeach filter and computes the intervals between the positivecrossings of the filtered waveform at various logarithmi-453cally spaced thresholds.
A histogram of the frequencies cor-responding to these intervals is then created.
The final stagecombines the histograms for each of the channels togetherinto the; final output, the Ensemble Interval Histogram.
Inthis respect, the EIH model performs functions imilar toSeneff's "Synchrony" output; measuring the extent hat theoutput of the linear filter is in synchrony with the center fre-quency of that filter.
The EIH model has been shown usefulin performing isolated-word recognition i  high noise con-ditions.3.
EVALUATION CONDIT IONSInitial evaluation is being performed using the TI-105 wordspeech corpus.
\[5\] This corpus includes peech spoken invarious taking styles, includes 8 speakers, (5 male and 3female) and provides 5 training tokens and 2 testing tokensper condition for each vocabulary item.Noise was added to the clean speech condition to evaluateperformance under noisy channel conditions.
Noise wasspeech babble recorded in a public meeting place withmany background speakers.
For this evaluation we usedGhitza's definition of signal to noise ratio \[4\] as the ratio ofthe energy per speech sample in the clean speech and thenoise averaged over the entire duration of the utterance.The recognition system was a word-based HMM systemwith eight speech states per word model, continuous den-sity observations and a single tied diagonal covariancematrix for every state.
This robust recognizer provides lowerror rates on many isolated-word databases.Both auditory front ends produce high-dimensional featurevector outputs.
For classification, the dimensionality wasreduced using the same inverse cosine transform used forthe reel-scale cepstral front end.
For these purposes, allauditory model outputs were treated as representing spec-tral magnitude.
This was done in lieu of a more advanceddata reduction technique such as principal componentsanalysis or linear discriminant analysis.4.
RESULTSTable 1 shows the results of the preliminary recognitionexperiments.
The signal to noise ratio is indicated in thefirst column, followed by the word accuracy results for thereel-frequency epstra, (MFC) the "mean rate" responseand "synchrony" output (SYN) from Seneff's auditorymodel, and the results using the EIH model.
The binomialstandard eviation of the word accuracy rate assuming theMFC performance l vel is indicated in the final column.These preliminary results are encouraging.
There is no deg-radation in performance at low noise levels, and all front-ends provide reduced error rates at both intermediate andhigh noise levels.MEAN SNR MFC RATEclean .5 .730 .6 .824 1.1 .818 2.7 2.112 8.4 7.66 26.9 22.9SYN.8.8.72.07.421.6EIH.8.7.72.17.422.0t~.2.2.3.4.71.0Table 1: Word Accuracy Rates for Various SignalRepresentations5.
FUTURE WORKThese results are preliminary, and more experiments areplanned to further investigate the effect of these and othersignal representations on the performance of speech recog-nition systems.
These include:1.
Evaluation of additional signal representations2.
Exploration of data reduction techniques such asprinciple components analysis and linear discrimi-nant analysis3.
Exploration of techniques such as the combined useof Seneff's "mean rate" and "synchrony" outputs4.
Evaluation of auditory models on continuous speechusing the DARPA Resource Management (RM) cor-pusREFERENCES\[1\]\[2\]\[31\[4\]\[5\]Davis, S. B. and Mermelstein, P., "Comparison of Paramet-ric Representations forMonosyllabic Word Recognition iContinuously Spoken Sentences," IEEE Transactions onAcoustics, Speech, and Signal Processing, August 1980.Seneff, S., "A Computational Model for the PeripheralAuditory System: Application to Speech RecognitionResearch," Proceedings oflCASSP-86, April 1986.Seneff, S., Pitch and Spectral Analysis of Speech Based onAn Auditory Synchrony Model, PhD Thesis, MassachusettsInstitute of Technology, January 1985.Ghitza, O., "Auditory Nerve Representation as a front endfor Speech Recognition i  a Noisy EnvironmenL" Com-puter Speech and Language, 1986.Rajesekaran, E J., Doddington, G. R., and Picone, J. W.,"Recognition of Speech Under Stress and in Noise," Pro-ceedings of lCASSP-86, April 1986.454
