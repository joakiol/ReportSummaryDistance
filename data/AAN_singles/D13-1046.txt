Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 479?489,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsBuilding Specialized Bilingual Lexicons Using Large-Scale BackgroundKnowledgeDhouha Bouamor1, Adrian Popescu1, Nasredine Semmar1, Pierre Zweigenbaum21 CEA, LIST, Vision and Content Engineering Laboratory, 91191Gif-sur-Yvette CEDEX, France; firstname.lastname@cea.fr2LIMSI-CNRS, F-91403 Orsay CEDEX, France; pz@limsi.frAbstractBilingual lexicons are central components ofmachine translation and cross-lingual infor-mation retrieval systems.
Their manual con-struction requires strong expertise in both lan-guages involved and is a costly process.
Sev-eral automatic methods were proposed as analternative but they often rely on resourcesavailable in a limited number of languagesand their performances are still far behindthe quality of manual translations.
We intro-duce a novel approach to the creation of spe-cific domain bilingual lexicon that relies onWikipedia.
This massively multilingual en-cyclopedia makes it possible to create lexi-cons for a large number of language pairs.Wikipedia is used to extract domains in eachlanguage, to link domains between languagesand to create generic translation dictionaries.The approach is tested on four specialized do-mains and is compared to three state of the artapproaches using two language pairs: French-English and Romanian-English.
The newly in-troduced method compares favorably to exist-ing methods in all configurations tested.1 IntroductionThe plethora of textual information shared on theWeb is strongly multilingual and users?
informationneeds often go well beyond their knowledge of for-eign languages.
In such cases, efficient machinetranslation and cross-lingual information retrievalsystems are needed.
Machine translation already hasa decades long history and an array of commercialsystems were already deployed, including GoogleTranslate 1 and Systran 2.
However, due to the intrin-sic difficulty of the task, a number of related prob-lems remain open, including: the gap between textsemantics and statistically derived translations, thescarcity of resources in a large majority of languagesand the quality of automatically obtained resourcesand translations.
While the first challenge is generaland inherent to any automatic approach, the secondand the third can be at least partially addressed byan appropriate exploitation of multilingual resourcesthat are increasingly available on the Web.In this paper we focus on the automatic creation ofdomain-specific bilingual lexicons.
Such resourcesplay a vital role in Natural Language Processing(NLP) applications that involve different languages.At first, research on lexical extraction has relied onthe use of parallel corpora (Och and Ney, 2003).The scarcity of such corpora, in particular for spe-cialized domains and for language pairs not involv-ing English, pushed researchers to investigate theuse of comparable corpora (Fung, 1998; Chiao andZweigenbaum, 2003).
These corpora include textswhich are not exact translation of each other butshare common features such as domain, genre, sam-pling period, etc.The basic intuition that underlies bilingual lexi-con creation is the distributional hypothesis (Harris,1954) which puts that words with similar meaningsoccur in similar contexts.
In a multilingual formu-lation, this hypothesis states that the translations ofa word are likely to appear in similar lexical envi-ronments across languages (Rapp, 1995).
The stan-dard approach to bilingual lexicon extraction builds1http://translate.google.com/2http://www.systransoft.com/479on the distributional hypothesis and compares con-text vectors for each word of the source and tar-get languages.
In this approach, the comparison ofcontext vectors is conditioned by the existence of aseed bilingual dictionary.
A weakness of the methodis that poor results are obtained for language pairsthat are not closely related (Ismail and Manandhar,2010).
Another important problem occurs wheneverthe size of the seed dictionary is small due to ignor-ing many context words.
Conversely, when dictio-naries are detailed, ambiguity becomes an importantdrawback.We introduce a bilingual lexicon extraction ap-proach that exploits Wikipedia in an innovativemanner in order to tackle some of the problemsmentioned above.
Important advantages of usingWikipedia are:?
The resource is available in hundreds of lan-guages and it is structured as unambiguous con-cepts (i.e.
articles).?
The languages are explicitly linked throughconcept translations proposed by Wikipediacontributors.?
It covers a large number of domains and is thuspotentially useful in order to mine a wide arrayof specialized lexicons.Mirroring the advantages, there are a number ofchallenges associated with the use of Wikipedia:?
The comparability of concept descriptions indifferent languages is highly variable.?
The translation graph is partial since, whenconsidering any language pair, only a part ofthe concepts are available in both languagesand explicitly connected.?
Domains are unequally covered in Wikipedia(Halavais and Lackaff, 2008) and efficient do-main targeting is needed.The approach introduced in this paper aims todraw on Wikipedia?s advantages while appropri-ately addressing associated challenges.
Amongthe techniques devised to mine Wikipedia content,we hypothesize that an adequate adaptation of Ex-plicit Semantic Analysis (ESA) (Gabrilovich andMarkovitch, 2007) is fitted to our application con-text.
ESA was already successfully tested in differ-ent NLP tasks, such as word relatedness estimationor text classification, and we modify it to mine spe-cialized domains, to characterize these domains andto link them across languages.The evaluation of the newly introduced approachis realized on four diversified specialized domains(Breast Cancer, Corporate Finance, Wind Energyand Mobile Technology) and for two pairs of lan-guages: French-English and Romanian-English.This choice allows us to study the behavior of dif-ferent approaches for a pair of languages that arerichly represented and for a pair that includes Roma-nian, a language that has fewer associated resourcesthan French and English.
Experimental results showthat the newly introduced approach outperforms thethree state of the art methods that were implementedfor comparison.2 Related WorkIn this section, we first give a review of the stan-dard approach and then introduce methods that buildupon it.
Finally, we discuss works that rely on Ex-plicit Semantic Analysis to solve other NLP tasks.2.1 Standard Approach (SA)Most previous approaches that address bilingual lex-icon extraction from comparable corpora are basedon the standard approach (Fung, 1998; Chiao andZweigenbaum, 2002; Laroche and Langlais, 2010).This approach is composed of three main steps:1.
Building context vectors: Vectors are firstextracted by identifying the words that ap-pear around the term to be translated Wcandin a window of n words.
Generally, asso-ciation measures such as the mutual infor-mation (Morin and Daille, 2006), the log-likelihood (Morin and Prochasson, 2011) or theDiscounted Odds-Ratio (Laroche and Langlais,2010) are employed to shape the context vec-tors.2.
Translation of context vectors: To enable thecomparison of source and target vectors, sourcevectors are translated intoto the target languageby using a seed bilingual dictionary.
When-ever several translations of a context word exist,480all translation variants are taken into account.Words not included in the seed dictionary aresimply ignored.3.
Comparison of source and target vectors:Given Wcand, its automatically translated con-text vector is compared to the context vectorsof all possible translations from the target lan-guage.
Most often, the cosine similarity isused to rank translation candidates but alterna-tive metrics, including the weighted Jaccard in-dex (Prochasson et al 2009) and the city-blockdistance (Rapp, 1999), were studied.2.2 Improvements of the Standard ApproachMost of the improvements of the standard approachare based on the observation that the more repre-sentative the context vectors of a candidate wordare, the better the bilingual lexicon extraction is.
Atfirst, additional linguistic resources, such as special-ized dictionaries (Chiao and Zweigenbaum, 2002) ortransliterated words (Prochasson et al 2009), werecombined with the seed dictionary to translate con-text vectors.The ambiguities that appear in the seed bilingualdictionary were taken into account more recently.
(Morin and Prochasson, 2011) modify the standardapproach by weighting the different translations ac-cording to their frequency in the target corpus.
In(Bouamor et al 2013), we proposed a method thatadds a word sense disambiguation process relyingon semantic similarity measurement from WordNetto the standard approach.
Given a context vector inthe source language, the most probable translation ofpolysemous words is identified and used for build-ing the corresponding vector in the target language.The most probable translation is identified using themonosemic words that appear in the same lexical en-vironment.On specialized French-English comparable cor-pora, this approach outperforms the one proposed in(Morin and Prochasson, 2011), which is itself bet-ter than the standard approach.
The main weaknessof (Bouamor et al 2013) is that the approach relieson WordNet and its application depends on the ex-istence of this resource in the target language.
Also,the method is highly dependent on the coverage ofthe seed bilingual dictionary.2.3 Explicit Semantic AnalysisExplicit Semantic Analysis (ESA) (Gabrilovich andMarkovitch, 2007) is a method that maps textualdocuments onto a structured semantic space usingclassical text indexing schemes such as TF-IDF.
Ex-amples of semantic spaces used include Wikipediaor the Open Directory Project but, due to superiorperformances, Wikipedia is most frequently used.In the original evaluation, ESA outperformed stateof the art methods in a word relatedness estimationtask.Subsequently, ESA was successfully exploited inother NLP tasks and in information retrieval.
Radin-sky and al.
(2011) added a temporal dimension toword vectors and showed that this addition improvesthe results of word relatedness estimation.
(Hassanand Mihalcea, 2011) introduced Salient SemanticAnalysis (SSA), a development of ESA that relieson the detection of salient concepts prior to map-ping words to concepts.
SSA and the original ESAimplementation were tested on several word related-ness datasets and results were mixed.
Improvementswere obtained for text classification when compar-ing SSA with the authors?
in-house representationof the method.
ESA has weak language depen-dence and was already deployed in multilingual con-texts.
(Sorg and Cimiano, 2012) extended ESA toother languages and showed that it is useful in cross-lingual and multilingual retrieval task.
Their focuswas on creating a language independent conceptualspace in which documents would be mapped andthen retrieved.Some open ESA topics related to bilingual lex-icon creation include: (1) the document represen-tation which is simply done by summing individ-ual contributions of words, (2) the adaptation of themethod to specific domains and (3) the coverage ofthe underlying resource in different language.3 ESA for Bilingual Lexicon ExtractionThe main objective of our approach is to devise lex-icon translation methods that are easily applicableto a large number of language pairs, while preserv-ing the overall quality of results.
A subordinatedobjective is to exploit large scale background mul-tilingual knowledge, such as the encyclopedic con-tent available in Wikipedia.
As we mentioned, ESA481Figure 1: Overview of the Explicit SemanticAnalysis enabled bilingual lexicon extraction.
(Gabrilovich and Markovitch, 2007) was exploitedin a number of NLP tasks but not in bilingual lexi-con extraction.Figure 1 shows the overall architecture of the lex-ical extraction process we propose.
The process iscompleted in the following three steps:1.
Given a word to be translated and its con-text vector in the source language, we derivea ranked list of similar Wikipedia concepts (i.e.articles) using the ESA inverted index.2.
Then, a translation graph is used to retrieve thecorresponding concepts in the target language.3.
Candidate translations are found through a sta-tistical processing of concept descriptions fromthe ESA direct index in the target language.In this section, we first introduce the elements ofthe original formulation of ESA necessary in our ap-proach.
Then, we detail the three steps that com-pose the main bilingual lexicon extraction methodillustrated in Figure 1.
Finally, as a complement tothe main method we introduce a measure for domainword specificity and present a method for extractinggeneric translation lexicons.3.1 ESA Word and Concept RepresentationGiven a semantic space structured using a set of Mconcepts and including a dictionary of N words,a mapping between words and concepts can beexpressed as the following matrix:w(W1, C1) w(W2, C1) ... w(WN , C1)w(W1, C2) w(W2, C2) ... w(WN , C2)... ... ...w(W1, CM ) w(W2, CM ) ... w(WN , CM )When Wikipedia is exploited concepts areequated to Wikipedia articles and the texts of the ar-ticles are processed in order to obtain the weightsthat link words and concepts.
In (Gabrilovich andMarkovitch, 2007), the weights w that link wordsand concepts were obtained through a classical TF-IDF weighting of Wikipedia articles.
A series oftweaks destined to improve the method?s perfor-mance were used and disclosed later3.
For instance,administration articles, lists, articles that are tooshort or have too few links are discarded.
Higherweight is given to words in the article title andmore longer articles are favored over shorter ones.We implemented a part of these tweaks and testedour own version of ESA with the Wikipedia ver-sion used in the original implementation.
The cor-relation with human judgments of word relatednesswas 0.72 against 0.75 reported by (Gabrilovich andMarkovitch, 2007).
The ESA matrix is sparse sincethe N size of the dictionary, is usually in the rangeof hundreds of thousands and each concept is usu-ally described by hundreds of distinct words.
Thedirect ESA index from Figure 1 is obtained by read-ing the matrix horizontally while the inverted ESAindex is obtained by reading the matrix vertically.3https://github.com/faraday/wikiprep-esa/wiki/roadmap482Terme Conceptsaction e?valuation d?action, communisme, actionnaire activiste, socialisme,de?velopement durable .
.
.de?ficit crise de la dette dans la zone euro, dette publique, re`gle d?or budge?taire,de?ficit, trouble du de?ficit de l?attention .
.
.cisaillement taux de cisaillement, zone de cisaillement, cisaillement, contrainte de cisaille-ment, viscoanalyseur .
.
.turbine ffc turbine potsdam, turbine a` gaz, turbine, urbine hydraulique, coge?ne?ration.
.
.cryptage TEMPEST, chiffrement, liaison 16, Windows Vista, transfert de fichiers .
.
.protocole Ad-hoc On-demand Distance Vector, protocole de Kyoto, optimized link staterouting protocol, liaison 16, IPv6 .
.
.biopsie biopsie, maladie de Horton, cancer du sein, cancer du poumon, imagerie parre?sonance magne?tique .
.
.palpation cancer du sein, cellulite, examen clinique, appendicite, te?nosynovite .
.
.Table 1: The five most similar Wikipedia concepts to the French terms action[share], de?ficit[deficit], ci-saillement[shear], turbine[turbine], cryptage[encryption], biopsie[biopsie] and palpation[palpation] andtheir context vectors.3.2 Source Language ProcessingThe objective of the source language processing isto obtain a ranked list of similar Wikipedia conceptsfor each candidate word (Wcand) in a specialized do-main.
To do this, a context vector is first built foreach Wcand from a specialized monolingual corpus.The association measure between Wcand and contextwords is obtained using the Odds-Ratio (defined inequation 5).
Wikipedia concepts in the source lan-guage Cs that are similar to Wcand and to a part of itscontext words are extracted and ranked using equa-tion 1.Rank(Cs) = (10 ?max(OddsWcandWsi)?w(Wcand, Cs)) +n?i=1OddsWcandWsi?w(Wsi , Cs)(1)where max(OddsWcandWsi) is the highest Odds-Ratioassociation between Wcand and any of its contextwords Wsi ; the factor 10 was empirically set togive more importance to Wcand over context words;w(Wcand, Cs) is the weight of the association be-tween Wcand and Cs from the ESA matrix; n is thetotal number of words Wsi in the context vector ofWcand; OddsWcandWsiis the association value betweenWcand and Wsi and w(Wsi , Cs) are the weights ofthe associations between each context word Wsi andCs from the ESA matrix.
The use of contextual in-formation in equation 1 serves to characterize thecandidate word in the target domain.In table 1, we present the five most similarWikipedia concepts to the French terms action,de?ficit, cisaillement, turbine, cryptage, biopsie andpalpation and their context vectors.
These terms arepart of the four specialized domains we are studyinghere.
From observing these examples, we note thatdespite the difference between the specialized do-mains and word ambiguity (words action and proto-cole), our method has the advantage of successfullyrepresenting each word to be translated by relevantconceptual spaces.3.3 Translation Graph ConstructionTo bridge the gap between the source and target lan-guages, a concept translation graph that enables themultilingual extension of ESA is used.
This con-cept translation graph is extracted from the explicittranslation links available in Wikipedia articles andis exploited in order to connect a word?s conceptualspace in the source language with the correspond-ing conceptual space in the target language.
Only apart of the articles have translations and the size of483the conceptual space in the target language is usu-ally smaller than the space in the source language.For instance, the French-English translation graphcontains 940,215 pairs of concepts while the Frenchand English Wikipedias contain approximately 1.4million articles, respectively 4.25 million articles.3.4 Target Language ProcessingThe third step of the approach takes place in the tar-get language.
Using the translation graph, we selectthe 100 most similar concept translations (thresh-old determined empirically after preliminary exper-iments) from the target language and use their di-rect ESA representations in order to retrieve poten-tial translations for the candidate word Wcand fromsource language.
These candidate translations Wtare ranked using equation 2.Rank(Wt) = (n?i=1w(Wt, Cti)avg(Cti))?
log(count(Wt,S)) (2)with w(Wt, Cti) is the weight of the translation can-didate WT for concept Cti from the ESA matrixin the target language; avg(Cti) is the average TF-IDF score of words that appear in Cti ; S is the setof similar concepts Cti in the target language andcount(Wt,S) accounts for the number of differentconcepts from S in which the candidate translationWT appears.The accumulation of weights w(Wt, Cti) fol-lows the way original ESA text representationsare calculated (Gabrilovich and Markovitch, 2007)and avg(Cti) is used in order to correct thebias of the TF-IDF scheme towards short articles.log(count(Wt,S)) is used to favor words that areassociated with a larger number of concepts.
logweighting was chosen after preliminary experimentswith a wide range of functions.3.5 Domain SpecificityIn previous works, ESA was usually exploitedin generic tasks that did not require any domainadaptation.
Here we process information fromspecific domains and we need to measure thespecificity of words in those domains.
The domainextraction is seeded by using Wikipedia concepts(noted Cseed) that best describes the domain inthe target language.
For instance, in English,the Corporate Finance domain is seeded withhttps://en.wikipedia.org/wiki/Corporate finance.We extract a set of 10 words with the highestTF-IDF score from this article (noted SW ) and usethem to retrieve a domain ranking of concepts in thetarget language Rankdom(Ct) with equation 3.Rankdom(Ct) = (n?i=1w(Wti , Ct)?
w(Cseed,Wti)) ?
count(SW,Ct) (3)where n is size of the seed list of words (i.e.
10items), w(Wti , Ct) is the weight of the domainwords in the concept Ct ; w(Cseed,Wti) is theweight of Wti in Cseed, the seed concept of the do-main, and count(SW,Ct) is the number of distinctseed words from SW that appear in Ct.The first part of equation 3 sums up the contribu-tions of different words from SW that appear in Ctwhile the second part is meant to further reinforcearticles that contain a larger number of domain key-words from SW .Domain delimitation is performed by retainingarticles whose Rankdom(Ct) is at least 1% or thescore of the top Rankdom(Ct) score.
This thresholdwas set up during preliminary experiments.
Giventhe delimitation obtained with equation 3, we calcu-late a domain specificity score (specifdom(Wt)) foreach word that occurs in the domain ( equation 4).specifdom(Wt) estimates how much of a word?s usein an underlying corpus is related to a target domain.specifdom(Wt) =DFdom(Wt)DFgen(Wt)(4)where DFdom and DFgen stand for the domain andthe generic document frequency of the word Wt.specifdom(Wt) will be used to favor words withgreater domain specificity over more general oneswhen several translations are available in a seedgeneric translation lexicon.
For instance, the Frenchword action is ambiguous and has English transla-tions such as action, stock, share etc.
In a generalcase, the most frequent translation is action whereasin a corporate finance context, share or stock aremore relevant.
The specificity of the three transla-tions, from highest to lowest, is: share, stock and ac-tion and is used to rank these potential translations.4843.6 Generic DictionariesGeneric translation dictionaries, already used by ex-isting bilingual lexicon extraction approaches, canalso be integrated in the newly proposed approach.The Wikipedia translation graph is transformed intoa translation dictionary by removing the disam-biguation marks from ambiguous concept titles, aswell as lists, categories and other administrationpages.
Moreover, since the approach does not han-dle multiword units, we retain only translation pairsthat are composed of unigrams in both languages.When existing, unigram redirections are also addedin each language.The obtained dictionaries are incomplete since:(1) Wikipedia focuses on concepts that are most of-ten nouns, (2) specialized domain terms often do nothave an associated Wikipedia entry and (3) the trans-lation graph covers only a fraction of the conceptsavailable in a language.
For instance, the result-ing translation dictionaries have 193,543 entries forFrench-English and 136,681 entries for Romanian-English.
They can be used in addition to or insteadof other resources available and are especially usefulwhen there are only few other resources that link thepair of languages processed.4 EvaluationThe performances of our approach are evaluatedagainst the standard approach and its developmentsproposed by (Morin and Prochasson, 2011) and(Bouamor et al 2013).
In this section, we firstdescribe the data and resources we used in our ex-periments.
We then present differents parametersneeded in the implementation of the different meth-ods tested.
Finally, we discuss the obtained results.4.1 Data and ResourcesComparable corporaWe conducted our experiments on four French-English and Romanian-English specialized compa-rable corpora: Corporate Finance, Breast Can-cer, Wind Energy and Mobile Technology.
Forthe Romanian-English language pair, we usedWikipedia to collect comparable corpora for all do-mains since they were not already available.
TheWikipedia corpora are harvested using a category-based selection.
We consider the topic in the sourceDomain FR ENCorporate Finance 402,486 756,840Breast Cancer 396,524 524,805Wind Energy 145,019 345,607Mobile Technology 197,689 144,168Domain RO ENCorporate Finance 206,169 524,805Breast Cancer 22,539 322,507Wind Energy 121,118 298,165Mobile Technology 200,670 124,149Table 2: Number of content words in thecomparable corpora.language (for instance Cancer Mamar [Breast Can-cer]) as a query to Wikipedia and extract all its sub-topics (i.e., sub-categories) to construct a domain-specific category tree.
Then, based on the con-structed tree, we collect all Wikipedia articles be-longing to at least one of these categories and useinter-language links to build the comparable cor-pora.Concerning the French-English pair, we followedthe strategy described above to extract the compa-rable corpora related to the Corporate Finance andBreast Cancer domains since they were otherwiseunavailable.
For the two other domains, we usedthe corpora released in the TTC project4.
All cor-pora were normalized through the following linguis-tic preprocessing steps: tokenization, part-of-speechtagging, lemmatization, and function word removal.The resulting corpora5 sizes are presented in Table2.
The size of the domain corpora vary within andacross languages, with the corporate finance domainbeing the richest in both languages.
In Romanian,Breast Cancer is particularly small, with approxi-mately 22,000 tokens included.
This variability willallow us to test if there is a correlation between cor-pus size and quality of results.Bilingual dictionaryThe seed generic French-English dictionary usedto translate French context vectors consists of anin-house manually built resource which containsapproximately 120,000 entries.
For Romanian-4http://www.ttc-project.eu/index.php/releases-publications5Comparable corpora will be shared publicly485Domain FR-EN RO-ENCorporate Finance 125 69Breast Cancer 96 38Wind Energy 89 38Mobile Technology 142 94Table 3: Sizes of the evaluation lists.English, we used the generic dictionary extractedfollowing the procedure described in Subsection 3.6.Gold standardIn bilingual terminology extraction from compara-ble corpora, a reference list is required to evaluatethe performance of the alignment.
Such lists are usu-ally composed of around 100 single terms (Hazemand Morin, 2012; Chiao and Zweigenbaum, 2002).Reference lists6 were created for the four specializeddomains and the two pairs of languages.
For theFrench-English, reference words from the Corpo-rate Finance domain were extracted from the glos-sary of bilingual micro-finance terms7.
For BreastCancer, the list is derived from the MESH and theUMLS thesauri8.
Concerning Wind Energy and Mo-bile Technology, lists were extracted from special-ized glossaries found on the Web.
The Romanian-English gold standard was manually created by a na-tive speaker starting from the French-English lists.Table 3 displays the sizes of the obtained lists.
Ref-erence terms pairs were retained if each word com-posing them appeared at least five times in the com-parable domain corpora.4.2 Experimental setupAside from those already mentioned, three param-eters need to be set up: (1) the window size thatdefines contexts, (2) the association measure thatmeasures the strength of the association betweenwords and the (3) similarity measure that ranks can-didate translations for state of the art methods.
Con-text vectors are defined using a seven-word windowwhich approximates syntactic dependencies.
Theassociation and the similarity measures (DiscountedLog-Odds ratio (equation 5) and the cosine simi-6Reference lists will be shared publicly7http://www.microfinance.lu/en/8http://www.nlm.nih.gov/larity) were set following Laroche and Langlais(2010), a comprehensive study of the influence ofthese parameters on the bilingual alignment.Odds-Ratiodisc = log(O11 + 12 )(O22 +12 )(O12 + 12 )(O21 +12 )(5)where Oij are the cells of the 2?
2 contingency ma-trix of a token s co-occurring with the term S withina given window size.The F-measure of the Top 20 results (F-Measure@20), which measures the harmonic meanof precision and recall, is used as evaluation metric.Precision is the total number of correct translationsdivided by the number of terms for which the systemreturned at least one answer.
Recall is equal to theratio between the number of correct translation andthe total number of words to translate (Wcand).4.3 Results and discussionIn addition to the basic approach based on ESA(denoted ESA), we evaluate the performances ofa method so-called DicoSpec in which the transla-tions are extracted from a generic dictionary anda method we called ESASpec which combine ESAand DicoSpec.
DICOSpec is based on the genericdictionary we presented in subsection 3.6 and pro-ceeds as follows: we extract a list of translations foreach word to be translated from the generic dictio-nary.
The domain specificity introduced in subsec-tion 3.5 is then used to rank these translations.
Forinstance, the french term port referring in the MobileTechnology domain, to the system that allows com-puters to receive and transmit information is trans-lated into port and seaport.
According to domainspecificity values, the following ranking is obtained:the English term port obtain the highest specificityvalue (0.48).
seaport comes next with a specificityvalue of 0.01.
In ESASpec, the translations set out inthe translations lists proposed by both ESA and thegeneric dictionary are weighted according to theirdomain specificity values.
The main intuition be-hind this method is that by adding the informationabout the domain specificity, we obtain a new rank-ing of the bilingual extraction results.The obtained results are displayed in table 4.
Thecomparison of state of the art method shows thatBA13 performs better than STAPP and MP11 forFrench-English and has comparable performances486a)FR-ENMethodF-Measure@20Breast Cancer Corporate Finance Wind Eenrgy Mobile TechnologySTAPP 0.49 0.17 0.08 0.06MP11 0.55 0.33 0.24 0.05BA13 0.61 0.37 0.30 0.24Dicospec 0.50 0.20 0.36 0.25ESA 0.74 0.50 0.83 0.72ESAspec 0.81 0.56 0.86 0.75b)RO-ENMethodF-Measure@20Breast Cancer Corporate Finance Wind Eenrgy Mobile TechnologySTAPP 0.21 0.13 0.08 0.16MP11 0.21 0.13 0.08 0.16BA13 0.21 0.14 0.08 0.17Dicospec 0.44 0.11 0.21 0.16ESA 0.76 0.17 0.58 0.53ESAspec 0.78 0.24 0.58 0.55Table 4: Results of the specialized dictionary creation on four specific domains, two pairs of languages.Threestate of the art methods were used for comparison: STAPP is the standard approach, MP11 is the improve-ment of the standard approach introduced in (Morin and Prochasson, 2011), BA13 is a recent method thatwe developed (Bouamor et al 2013).
Dicospec exploits a generic dictionary, combined with the use of do-main specificity (see Subsection 3.5).
ESA stands for the ESA based approach introduced in this paper (seeFigure 1).
ESAspec combines the results of Dicospec and ESA.for RO-EN.
Consequently, we will use BA13 as themain baseline for discussing the newly introducedapproach.
The results presented in Table 4 showthat ESAspec clearly outperforms the three base-lines for the four domains and the two pairs of lan-guages tested.
When comparing ESAspec to BA13for French-English, improvements range between0.19 for Corporate Finance and 0.56 for Wind En-ergy.
For RO-EN, the improvements vary from 0.1for Corporate Finance to 0.5 for Wind Energy.
Also,except for the Corporate Finance domain in Roma-nian, the performance variation across domains ismuch smaller for ESAspec than for the three stateof the art methods.
This shows that ESAspec is morerobust to domain change and thus more generic.The results obtained with ESA are signifi-cantly better than those obtained with Dicospec andESAspec, their combination, further improves theresults.
The main contribution to ESAspec perfor-mances comes from ESA, a finding that validatesour assumption that the adequate use of a rich multi-lingual resource such as Wikipedia is appropriate forspecialized lexicon translation.
Dicospec is a sim-ple method that ranks the different meanings of acandidate word available in a generic dictionary butits average performances are comparable to thoseof BA13 for FR-EN and higher for RO-EN.
Thisfinding advocates for the importance of good qual-ity generic dictionaries in specialized lexicon trans-lation approaches.
However, it is clear that suchdictionaries are far from being sufficient in orderto cover all possible domains.
There is no clearcorrelation between domain size and quality of re-sults.
Although richer than the other three domains,Corporate Finance has the lowest associated per-formances.
This finding is probably explained bythe intrinsic difficulty of each domain.
When pass-ing from FR-EN to RO-EN the average performancedrop is more significant for BA13 than for the ESAbased methods.
The result indicates that our ap-proach is more robust to language change.5 ConclusionWe have presented a new approach to the creationof specialized bilingual lexicons, one of the central487building blocks of machine translation systems.
Theproposed approach directly tackles two of the ma-jor challenges identified in the Introduction.
Thescarcity of resources is addressed by an adequateexploitation of Wikipedia, a resource that is avail-able in hundreds of languages.
The quality of auto-matic translations was improved by appropriate do-main delimitation and linking across languages, aswell as by an adequate statistical processing of con-cepts similar to a word in a given context.The main advantages of our approach comparedto state of the art methods come from: the increasednumber of languages that can be processed, fromthe smaller sensitivity to structured resources andthe appropriate domain delimitation.
Experimentalvalidation is obtained through evaluation with fourdifferent domains and two pairs of languages whichshows consistent performance improvement.
ForFrench-English, two languages that have rich asso-ciated Wikipedia representations, performances arevery interesting and are starting to approach those ofmanual translations for three domains out of four (F-Measure@20 around 0.8).
For Romanian-English, apair involving a language with a sparser Wikipediarepresentation, the performances of our method dropcompared to French-English .
However, they do notdecrease to the same extent as those of the best stateof the art method tested.
This finding indicates thatour approach is more general and, given its low lan-guage dependence, it can be easily extended to alarge number of language pairs.The results presented here are very encouragingand we will to pursue work in several directions.First, we will pursue the integration of our method,notably through comparable corpora creation usingthe data driven domain delimitation technique de-scribed in Subsection 3.5.
Equally important, thesize of the domain can be adapted so as to findenough context for all the words in domain referencelists.
Second, given a word in a context, we currentlyexploit all similar concepts from the target language.Given that comparability of article versions in thesource and the target language varies, we will eval-uate algorithms for filtering out concepts from thetarget language that have low alignment with theirsource language versions.
A final line of work isconstituted by the use of distributional properties oftexts in order to automatically rank parts of conceptdescriptions (i.e.
articles) by their relatedness to thecandidate word.
Similar to the second direction, thisprocess involves finding comparable text blocks butrather at a paragraph or sentence level than at thearticle level.ReferencesDhouha Bouamor, Nasredine Semmar, and PierreZweigenbaum.
2013.
Context vector disambiguationfor bilingual lexicon extraction.
In Proceedings of the51st Association for Computational Linguistics (ACL-HLT), Sofia, Bulgaria, August.Yun-Chuang Chiao and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents in spe-cialized, comparable corpora.
In Proceedings of the19th international conference on Computational lin-guistics - Volume 2, COLING ?02, pages 1?5.
Associ-ation for Computational Linguistics.Yun-Chuang Chiao and Pierre Zweigenbaum.
2003.
Theeffect of a general lexicon in corpus-based identifi-cation of french-english medical word translations.In Proceedings Medical Informatics Europe, volume95 of Studies in Health Technology and Informatics,pages 397?402, Amsterdam.Pascale Fung.
1998.
A statistical view on bilingual lexi-con extraction: From parallel corpora to non-parallelcorpora.
In Parallel Text Processing, pages 1?17.Springer.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using wikipedia-basedexplicit semantic analysis.
In Proceedings of the20th international joint conference on Artifical intel-ligence, IJCAI?07, pages 1606?1611, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Alexander Halavais and Derek Lackaff.
2008.
An Anal-ysis of Topical Coverage of Wikipedia.
Journal ofComputer-Mediated Communication, 13(2):429?440.Z.S.
Harris.
1954.
Distributional structure.
Word.Samer Hassan and Rada Mihalcea.
2011.
Semantic re-latedness using salient semantic analysis.
In AAAI.Amir Hazem and Emmanuel Morin.
2012.
Adaptive dic-tionary for bilingual lexicon extraction from compara-ble corpora.
In Proceedings, 8th international confer-ence on Language Resources and Evaluation (LREC),Istanbul, Turkey, May.Azniah Ismail and Suresh Manandhar.
2010.
Bilin-gual lexicon extraction from comparable corpora us-ing in-domain terms.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics:Posters, COLING ?10, pages 481?489.
Association forComputational Linguistics.488Audrey Laroche and Philippe Langlais.
2010.
Revisitingcontext-based projection methods for term-translationspotting in comparable corpora.
In 23rd Interna-tional Conference on Computational Linguistics (Col-ing 2010), pages 617?625, Beijing, China, Aug.Emmanuel Morin and Be?atrice Daille.
2006.
Compara-bilite?
de corpus et fouille terminologique multilingue.In Traitement Automatique des Langues (TAL).Emmanuel Morin and Emmanuel Prochasson.
2011.Bilingual lexicon extraction from comparable corporaenhanced with parallel corpora.
In Proceedings, 4thWorkshop on Building and Using Comparable Cor-pora (BUCC), page 27?34, Portland, Oregon, USA.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Comput.
Linguist., 29(1):19?51, March.Emmanuel Prochasson, Emmanuel Morin, and KyoKageura.
2009.
Anchor points for bilingual lexi-con extraction from small comparable corpora.
InProceedings, 12th Conference on Machine TranslationSummit (MT Summit XII), page 284?291, Ottawa, On-tario, Canada.Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,and Shaul Markovitch.
2011.
A word at a time: com-puting word relatedness using temporal semantic anal-ysis.
In Proceedings of the 20th international confer-ence on World wide web, WWW ?11, pages 337?346,New York, NY, USA.
ACM.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguistics,ACL ?95, pages 320?322.
Association for Computa-tional Linguistics.Reinhard Rapp.
1999.
Automatic identification of wordtranslations from unrelated english and german cor-pora.
In Proceedings of the 37th annual meeting of theAssociation for Computational Linguistics on Compu-tational Linguistics, ACL ?99, pages 519?526.
Asso-ciation for Computational Linguistics.P.
Sorg and P. Cimiano.
2012.
Exploiting wikipedia forcross-lingual and multilingual information retrieval.Data Knowl.
Eng., 74:26?45, April.489
