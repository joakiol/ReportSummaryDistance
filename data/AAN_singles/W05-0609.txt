Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 64?71, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsDiscriminative Training of Clustering Functions:Theory and Experiments with Entity IdentificationXin Li and Dan RothDepartment of Computer ScienceUniversity of Illinois, Urbana, IL 61801(xli1,danr)@cs.uiuc.eduAbstractClustering is an optimization procedure thatpartitions a set of elements to optimize somecriteria, based on a fixed distance metric de-fined between the elements.
Clustering ap-proaches have been widely applied in naturallanguage processing and it has been shown re-peatedly that their success depends on defin-ing a good distance metric, one that is appro-priate for the task and the clustering algorithmused.
This paper develops a framework inwhich clustering is viewed as a learning task,and proposes a way to train a distance metricthat is appropriate for the chosen clustering al-gorithm in the context of the given task.
Ex-periments in the context of the entity identifi-cation problem exhibit significant performanceimprovements over state-of-the-art clusteringapproaches developed for this problem.1 IntroductionClustering approaches have been widely applied to nat-ural language processing (NLP) problems.
Typically,natural language elements (words, phrases, sentences,etc.)
are partitioned into non-overlapping classes, basedon some distance (or similarity) metric defined betweenthem, in order to provide some level of syntactic or se-mantic abstraction.
A key example is that of class-basedlanguage models (Brown et al, 1992; Dagan et al, 1999)where clustering approaches are used in order to parti-tion words, determined to be similar, into sets.
Thisenables estimating more robust statistics since these arecomputed over collections of ?similar?
words.
A largenumber of different metrics and algorithms have been ex-perimented with these problems (Dagan et al, 1999; Lee,1997; Weeds et al, 2004).
Similarity between words wasalso used as a metric in a distributional clustering algo-rithm in (Pantel and Lin, 2002), and it shows that func-tionally similar words can be grouped together and evenseparated to smaller groups based on their senses.
At ahigher level, (Mann and Yarowsky, 2003) disambiguatedpersonal names by clustering people?s home pages usinga TFIDF similarity, and several other researchers have ap-plied clustering at the same level in the context of theentity identification problem (Bilenko et al, 2003; Mc-Callum and Wellner, 2003; Li et al, 2004).
Similarly, ap-proaches to coreference resolution (Cardie and Wagstaff,1999) use clustering to identify groups of references tothe same entity.Clustering is an optimization procedure that takes asinput (1) a collection of domain elements along with (2)a distance metric between them and (3) an algorithm se-lected to partition the data elements, with the goal of op-timizing some form of clustering quality with respect tothe given distance metric.
For example, the K-Meansclustering approach (Hartigan and Wong, 1979) seeks tomaximize a measure of tightness of the resulting clustersbased on the Euclidean distance.
Clustering is typicallycalled an unsupervised method, since data elements areused without labels during the clustering process and la-bels are not used to provide feedback to the optimiza-tion process.
E.g., labels are not taken into account whenmeasuring the quality of the partition.
However, in manycases, supervision is used at the application level whendetermining an appropriate distance metric (e.g., (Lee,1997; Weeds et al, 2004; Bilenko et al, 2003) and more).This scenario, however, has several setbacks.
First, theprocess of clustering, simply a function that partitions aset of elements into different classes, involves no learn-ing and thus lacks flexibility.
Second, clustering quality istypically defined with respect to a fixed distance metric,without utilizing any direct supervision, so the practicalclustering outcome could be disparate from one?s inten-tion.
Third, when clustering with a given algorithm anda fixed metric, one in fact makes some implicit assump-tions on the data and the task (e.g., (Kamvar et al, 2002);more on that below).
For example, the optimal conditionsunder which for K-means works are that the data is gen-erated from a uniform mixture of Gaussian models; thismay not hold in reality.This paper proposes a new clustering framework thataddresses all the problems discussed above.
Specifically,64we define clustering as a learning task: in the trainingstage, a partition function, parameterized by a distancemetric, is trained with respect to a specific clustering al-gorithm, with supervision.
Some of the distinct proper-ties of this framework are that: (1) The training stage isformalized as an optimization problem in which a parti-tion function is learned in a way that minimizes a clus-tering error.
(2) The clustering error is well-defined anddriven by feedback from labeled data.
(3) Training adistance metric with respect to any given clustering al-gorithm seeks to minimize the clustering error on train-ing data that, under standard learning theory assumptions,can be shown to imply small error also in the applicationstage.
(4) We develop a general learning algorithm thatcan be used to learn an expressive distance metric overthe feature space (e.g., it can make use of kernels).While our approach makes explicit use of labeled data,we argue that, in fact, many clustering applications in nat-ural language also exploit this information off-line, whenexploring which metrics are appropriate for the task.
Ourframework makes better use of this resource by incorpo-rating it directly into the metric training process; trainingis driven by true clustering error, computed via the spe-cific algorithm chosen to partition the data.We study this new framework empirically on the en-tity identification problem ?
identifying whether differ-ent mentions of real world entities, such as ?JFK?
and?John Kennedy?, within and across text documents, ac-tually represent the same concept (McCallum and Well-ner, 2003; Li et al, 2004).
Our experimental results ex-hibit a significant performance improvement over exist-ing approaches (20% ?
30% F1 error reduction) on allthree types of entities we study, and indicate its promis-ing prospective in other natural language tasks.The rest of this paper discusses existing clustering ap-proaches (Sec.
2) and then introduces our Supervised Dis-criminative Clustering framework (SDC) (Sec.
3) and ageneral learner for training in it (Sec.
4).
Sec.
5 describesthe entity identification problem and Sec.
6 compares dif-ferent clustering approaches on this task.2 Clustering in Natural Language TasksClustering is the task of partitioning a set of elementsS ?
X into a disjoint decomposition 1 p(S) = {S1, S2,?
?
?
, SK} of S. We associate with it a partition functionp = pS : X ?
C = {1, 2, .
.
.K} that maps each x ?
Sto a class index pS(x) = k iff x ?
Sk.
The subscript Sin pS and pS(x) is omitted when clear from the context.Notice that, unlike a classifier, the image x ?
S under apartition function depends on S.In practice, a clustering algorithm A (e.g.
K-Means),and a distance metric d (e.g., Euclidean distance), are typ-1Overlapping partitions will not be discussed here.ically used to generate a function h to approximate thetrue partition function p. Denote h(S) = Ad(S), the par-tition of S by h. A distance (equivalently, a similarity)function d that measures the proximity between two ele-ments is a pairwise function X ?
X ?
R+, which canbe parameterized to represent a family of functions ?metric properties are not discussed in this paper.
For ex-ample, given any two element x1 =< x(1)1 , ?
?
?
, x(m)1 >and x2 =< x(1)2 , ?
?
?
, x(m)2 > in an m-dimensional space,a linearly weighted Euclidean distance with parameters?
= {wl}m1 is defined as:d?
(x1, x2) ????
?m?l=1wl ?
|x(l)1 ?
x(l)2 |2 (1)When supervision (e.g.
class index of elements) is un-available, the quality of a partition function h operatingon S ?
X , is measured with respect to the distance met-ric defined over X .
Suppose h partitions S into disjointsets h(S) = {S?k}K1 , one quality function used in the K-Means algorithm is defined as:qS(h) ?K?k=1?x?S?kd(x, ?
?k)2, (2)where ?
?k is the mean of elements in set S?k.
However, thismeasure can be computed irrespective of the algorithm.2.1 What is a Good Metric?A good metric is one in which close proximity correlateswell with the likelihood of being in the same class.
Whenapplying clustering to some task, people typically decideon the clustering quality measure qS(h) they want to op-timize, and then chose a specific clustering algorithm Aand a distance metric d to generate a ?good?
partitionfunction h. However, it is clear that without any super-vision, the resulting function is not guaranteed to agreewith the target function p (or one?s original intention).Given this realization, there has been some work onselecting a good distance metric for a family of relatedproblems and on learning a metric for specific tasks.
Forthe former, the focus is on developing and selecting gooddistance (similarity) metrics that reflect well pairwiseproximity between domain elements.
The ?goodness?of a metric is empirically measured when combined withdifferent clustering algorithms on different problems.
Forexample (Lee, 1997; Weeds et al, 2004) compare similar-ity metrics such as the Cosine, Manhattan and Euclideandistances, Kullback-Leibler divergence, Jensen-Shannondivergence, and Jaccard?s Coefficient, that could be ap-plied in general clustering tasks, on the task of measur-ing distributional similarity.
(Cohen et al, 2003) com-pares a number of string and token-based similarity met-rics on the task of matching entity names and found that,65overall, the best-performing method is a hybrid scheme(SoftTFIDF) combining a TFIDF weighting scheme oftokens with the Jaro-Winkler string-distance scheme thatis widely used for record linkage in databases.d(x1,x2) = [(x1(1) -x2(1))2+(x1(2) -x2(2))2]1/2 d(x1,x2) = |(x1(1) +x2(1))-(x1(2) +x2(2))|(a) Single-Linkage with Euclidean (b) K-Means with Euclidean (c) K-Means with a Linear MetricFigure 1: Different combinations of clustering algorithmswith distance metrics.
The 12 points, positioned in a two-dimensional space < X(1), X(2) >, are clustered into twogroups containing solid and hollow points respectively.Moreover, it is not clear whether there exists anyuniversal metric that is good for many different prob-lems (or even different data sets for similar problems)and is appropriate for any clustering algorithm.
For theword-based distributional similarity mentioned above,this point was discussed in (Geffet and Dagan, 2004)when it is shown that proximity metrics that are appro-priate for class-based language models may not be ap-propriate for other tasks.
We illustrate this critical point inFig.
1.
(a) and (b) show that even for the same data collec-tion, different clustering algorithms with the same met-ric could generate different outcomes.
(b) and (c) showthat with the same clustering algorithm, different metricscould also produce different outcomes.
Therefore, a gooddistance metric should be both domain-specific and asso-ciated with a specific clustering algorithm.2.2 Metric Learning via Pairwise ClassificationSeveral works (Cohen et al, 2003; Cohen and Rich-man, 2002; McCallum and Wellner, 2003; Li et al,2004) have tried to remedy the aforementioned problemsby attempting to learn a distance function in a domain-specific way via pairwise classification.
In the trainingstage, given a set of labeled element pairs, a functionf : X ?
X ?
{0, 1} is trained to classify any two el-ements as to whether they belong to the same class (1)or not (0), independently of other elements.
The dis-tance between the two elements is defined by convertingthe prediction confidence of the pairwise classifier, andclustering is then performed based on this distance func-tion.
Particularly, (Li et al, 2004) applied this approachto measuring name similarity in the entity identificationproblem, where a pairwise classifier (LMR) is trained us-ing the SNoW learning architecture (Roth, 1998) basedon variations of Perceptron and Winnow, and using a col-lection of relational features between a pair of names.The distance between two names is defined as a softmaxover the classifier?s output.
As expected, experimentalevidence (Cohen et al, 2003; Cohen and Richman, 2002;Li et al, 2004) shows that domain-specific distance func-tions improve over a fixed metric.
This can be explainedby the flexibility provided by adapting the metric to thedomain as well as the contribution of supervision thatguides the adaptation of the metric.A few works (Xing et al, 2002; Bar-Hillel et al, 2003;Schultz and Joachims, 2004; Mochihashi et al, 2004)outside the NLP domain have also pursued this generaldirection, and some have tried to learn the metric withlimited amount of supervision, no supervision or by in-corporating other information sources such as constraintson the class memberships of the data elements.
In most ofthese cases, the algorithm practically used in clustering,(e.g.
K-Means), is not considered in the learning proce-dure, or only implicitly exploited by optimizing the sameobjective function.
(Bach and Jordan, 2003; Bilenko etal., 2004) indeed suggest to learn a metric directly in aclustering task but the learning procedure is specific forone clustering algorithm.3 Supervised Discriminative ClusteringTo solve the limitations of existing approaches, we de-velop the Supervised Discriminative Clustering Frame-work (SDC), that can train a distance function with re-spect to any chosen clustering algorithm in the context ofa given task, guided by supervision.A labeled data set SA SupervisedLearnerTraining Stage:Goal: h*=argmin errS(h,p)A distancemetric d a clustering algorithm A+A unlabeled data set S?
A partition h(S?
)Application Stage: h(S?
)A partition function h(S) = Ad(S)Figure 2: Supervised Discriminative ClusteringFig.
2 presents this framework, in which a cluster-ing task is explicitly split into training and applicationstages, and the chosen clustering algorithm involves inboth stages.
In the training stage, supervision is directlyintegrated into measuring the clustering error errS(h, p)of a partition function h by exploiting the feedback givenby the true partition p. The goal of training is to find a par-tition function h?
in a hypothesis space H that minimizesthe error.
Consequently, given a new data set S?
in the ap-plication stage, under some standard learning theory as-sumptions, the hope is that the learned partition function66can generalize well and achieve small error as well.3.1 Supervised and Unsupervised TrainingLet p be the target function over X , h be a function in thehypothesis space H , and h(S) = {S?k}K1 .
In principle,given data set S ?
X , if the true partition p(S) = {Sk}K1of S is available, one can measure the deviation of h fromp over S, using an error function errS(h, p) ?
R+.
Wedistinguish an error function from a quality function (asin Equ.
2) as follows: an error function measures the dis-agreement between clustering and the target partition (orone?s intention) when supervision is given, while a qual-ity is defined without any supervision.For clustering, there is generally no direct way to com-pare the true class index p(x) of each element with thatgiven by a hypothesis h(x), so an alternative is to mea-sure the disagreement between p and h over pairs of el-ements.
Given a labeled data set S and p(S), one errorfunction, namely weighted clustering error, is defined asa sum of the pairwise errors over any two elements in S,weighted by the distance between them:errS(h, p) ?
1|S|2?xi,xj?S[d(xi, xj)?Aij+(D?d(xi, xj))?Bij ](3)where D = maxxi,xj?S d(xi, xj) is the maximum dis-tance between any two elements in S and I is an indica-tor function.
Aij ?
I[(p(xi) = p(xj) & h(xi) 6= h(xj)]and Bij ?
I[(p(xi) 6= p(xj) & h(xi) = h(xj)] representtwo types of pairwise errors respectively.Just like the quality defined in Equ.
2, this error is afunction of the metric d. Intuitively, the contribution of apair of elements that should belong to the same class butare split by h, grows with their distance, and vice versa.However, this measure is significantly different from thequality, in that it does not just measure the tightness of thepartition given by h, but rather the difference between thetightness of the partitions given by h and by p.Given a set of observed data, the goal of training is tolearn a good partition function, parameterized by specificclustering algorithms and distance functions.
Dependingon whether training data is labeled or unlabeled, we canfurther define supervised and unsupervised training.Definition 3.1 Supervised Training: Given a labeleddata set S and p(S), a family of partition functions H ,and the error function errS(h, p)(h ?
H), the problemis to find an optimal function h?
s.t.h?
= argminh?H errS(h, p).Definition 3.2 Unsupervised Training: Given an unla-beled data set S (p(S) is unknown), a family of partitionfunctions H , and a quality function qS(h)(h ?
H), theproblem is to find an optimal partition function h?
s.t.h?
= argmaxh?H qS(h).With this formalization, SDC along with supervisedtraining, can be distinguished clearly from (1) unsuper-vised clustering approaches, (2) clustering over pairwiseclassification; and (3) related works that exploit partialsupervision in metric learning as constraints.3.2 Clustering via Metric LearningBy fixing the clustering algorithm in the training stage,we can further define supervised metric learning, a spe-cial case of supervised training.Definition 3.3 Supervised Metric Learning: Given a la-beled data set S and p(S), and a family of partition func-tions H = {h} that are parameterized by a chosen clus-tering algorithm A and a family of distance metrics d?(?
?
?
), the problem is to seek an optimal metric d?
?with respect to A, s.t.
for h(S) = A d?
(S)??
= argmin?
errS(h, p).
(4)Learning the metric parameters ?
requires parameteriz-ing h as a function of ?, when the algorithm A is chosenand fixed in h. In the later experiments of Sec.
5, wetry to learn weighted Manhattan distances for the single-link algorithm and other algorithms, in the task of en-tity identification.
In this case, when pairwise featuresare extracted for any elements x1, x2 ?
X , (x1, x2) =<?1, ?2, ?
?
?
, ?m >, the linearly weighted Manhattan dis-tance, parameterized by (?
= {wl}m1 ) is defined as:d(x1, x2) ?m?l=1wl ?
?l(x1, x2) (5)where wl is the weight over feature ?l(x1, x2).
Sincemeasurement of the error is dependent on the metric,as shown in Equ.
3, one needs to enforce some con-straints on the parameters.
One constraint is?ml=1 |wl| =1, which prevents the error from being scale-dependent(e.g., metrics giving smaller distance are always better).4 A General Learner for SDCIn addition to the theoretical SDC framework, we also de-velop a practical learning algorithm based on gradient de-scent (in Fig.
3), that can train a distance function for anychosen clustering algorithm (such as Single-Linkage andK-Means), as in the setting of supervised metric learning.The training procedure incorporates the clustering algo-rithm (step 2.a) so that the metric is trained with respectto the specific algorithm that will be applied in evalua-tion.
The convergence of this general training proceduredepends on the convexity of the error as a function of ?.For example, since the error function we use is linear in ?,the algorithm is guaranteed to converge to a global mini-mum.
In this case, for rate of convergence, one can appealto general results that typically imply, when there existsa parameter vector with zero error, that convergence rate67depends on the ?separation?
of the training data, whichroughly means the minimal error archived with this pa-rameter vector.
Results such as (Freund and Schapire,1998) can be used to extend the rate of convergence re-sult a bit beyond the separable case, when a small numberof the pairs are not separable.Algorithm: SDC-LearnerInput: S and p(S): the labeled data set.
A: the clusteringalgorithm.
errS(h, p): the clustering error function.
?
> 0: the learning rate.
T (typically T is large) : the number ofiterations allowed.Output: ??
: the parameters in the distance function d.1.
In the initial (I-) step, we randomly choose ?0 for d.After this step we have the initial d0 and h0.2.
Then we iterate over t (t = 1, 2, ?
?
?
),(a) Partition S using ht?1(S) ?
A dt?1(S);(b) Compute errS(ht?1, p) and update ?
using theformula: ?t = ?t?1 ?
?
?
?errS(ht?1,p)?
?t?1 .
(c) Normalization: ?t = 1Z ?
?t, where Z = ||?t||.3.
Stopping Criterion: If t > T , the algorithm exits andoutputs the metric in the iteration with the least error.Figure 3: A general training algorithm for SDCFor the weighted clustering error in Equ.
3, and linearlyweighted Manhattan distances as in Equ.
5, the updaterule in Step 2(b) becomeswtl = wt?1l ?
?
?
[?t?1l (p, S)?
?t?1l (h, S)].
(6)where ?l(p, S) ?
1|S|2?xi,xj?S ?l(xi, xj) ?
I[p(xi) =p(xj)] and ?l(h, S) ?
1|S|2?xi,xj?S ?l(xi, xj) ?I[h(xi) = h(xj)], and ?
> 0 is the learning rate.5 Entity Identification in TextWe conduct experimental study on the task of entity iden-tification in text (Bilenko et al, 2003; McCallum andWellner, 2003; Li et al, 2004).
A given entity ?
rep-resenting a person, a location or an organization ?
maybe mentioned in text in multiple, ambiguous ways.
Con-sider, for example, an open domain question answeringsystem (Voorhees, 2002) that attempts, given a questionlike: ?When was President Kennedy born??
to search alarge collection of articles in order to pinpoint the con-cise answer: ?on May 29, 1917.?
The sentence, and eventhe document that contains the answer, may not containthe name ?President Kennedy?
; it may refer to this en-tity as ?Kennedy?, ?JFK?
or ?John Fitzgerald Kennedy?.Other documents may state that ?John F. Kennedy, Jr.was born on November 25, 1960?, but this fact refers toour target entity?s son.
Other mentions, such as ?SenatorKennedy?
or ?Mrs.
Kennedy?
are even ?closer?
to thewriting of the target entity, but clearly refer to differententities.
Understanding natural language requires identi-fying whether different mentions of a name, within andacross documents, represent the same entity.We study this problem for three entity types ?
People,Location and Organization.
Although deciding the coref-erence of names within the same document might be rela-tively easy, since within a single document identical men-tions typically refer to the same entity, identifying coref-erence across-document is much harder.
With no stan-dard corpora for studying the problem in a general setting?
both within and across documents, we created our owncorpus.
This is done by collecting about 8, 600 namesfrom 300 randomly sampled 1998-2000 New York Timesarticles in the TREC corpus (Voorhees, 2002).
Thesenames are first annotated by a named entity tagger, thenmanually verified and given as input to an entity identi-fier.Since the number of classes (entities) for names is verylarge, standard multi-class classification is not feasible.Instead, we compare SDC with several pairwise classifi-cation and clustering approaches.
Some of them (for ex-ample, those based on SoftTFIDF similarity) do not makeuse of any domain knowledge, while others do exploit su-pervision, such as LMR and SDC.
Other works (Bilenkoet al, 2003) also exploited supervision in this problem bydiscriminative training of a pairwise classifier but wereshown to be inferior.1.
SoftTFIDF Classifier ?
a pairwise classifier decidingwhether any two names refer to the same entity, imple-mented by thresholding a state-of-art SoftTFIDF similar-ity metric for string comparison (Cohen et al, 2003).
Dif-ferent thresholds have been experimented but only the bestresults are reported.2.
LMR Classifier (P|W) ?
a SNoW-based pairwise classi-fier (Li et al, 2004) (described in Sec.
2.2) that learns alinear function for each class over a collection of relationalfeatures between two names: including string and token-level features and structural features (listed in Table 1).For pairwise classifiers like LMR and SoftTFIDF, predic-tion is made over pairs of names so transitivity of predic-tions is not guaranteed as in clustering.3.
Clustering over SoftTFIDF ?
a clustering approach basedon the SoftTFIDF similarity metric.4.
Clustering over LMR (P|W) ?
a clustering approach (Li etal., 2004) by converting the LMR classifier into a similar-ity metric (see Sec.
2.2).5.
SDC ?
our new supervised clustering approach.
The dis-tance metric is represented as a linear function over a setof pairwise features as defined in Equ.
5.The above approaches (2), (4) and (5) learn a classifieror a distance metric using the same feature set as in Ta-ble 1.
Different clustering algorithms 2, such as Single-Linkage, Complete-Linkage, Graph clustering (George,2The clustering package Cluster by Michael Eisen at Stan-ford University is adopted for K-medoids and CLUTO by(George, 2003) is used for other algorithms.
Details of thesealgorithms can be found there.68Honorific Equal active if both tokens are honorifics and identical.Honorific Equivalence active if both tokens are honorifics, not identical, but equivalent.Honorific Mismatch active for different honorifics.Equality active if both tokens are identical.Case-Insensitive Equal active if the tokens are case-insensitive equal.Nickname active if tokens have a ?nickname?
relation.Prefix Equality active if the prefixes of both tokens are equal.Substring active if one of the tokens is a substring of the other.Abbreviation active if one of the tokens is an abbreviation of the other.Prefix Edit Distance active if the prefixes of both tokens have an edit-distance of 1.Edit Distance active if the tokens have an edit-distance of 1.Initial active if one of the tokens is an initial of another.Symbol Map active if one token is a symbolic representative of the other.Structural recording the location of the tokens that generate other features in two names.Table 1: Features employed by LMR and SDC.2003) ?
seeking a minimum cut of a nearest neighborgraph, Repeated Bisections and K-medoids (Chu et al,2001) (a variation of K-means) are experimented in (5).The number of entities in a data set is always given.6 Experimental StudyOur experimental study focuses on (1) evaluating thesupervised discriminative clustering approach on entityidentification; (2) comparing it with existing pairwiseclassification and clustering approaches widely used insimilar tasks; and (3) further analyzing the characteris-tics of this new framework.We use the TREC corpus to evaluate different ap-proaches in identifying three types of entities: People,Locations and Organization.
For each type, we generatethree pairs of training and test sets, each containing about300 names.
We note that the three entity types yield verydifferent data sets, exhibited by some statistical proper-ties3.
Results on each entity type will be averaged overthe three sets and ten runs of two-fold cross-validation foreach of them.
For SDC, given a training set with anno-tated name pairs, a distance function is first trained usingthe algorithm in Fig.
3 (in 20 iterations) with respect toa clustering algorithm and then be used to partition thecorresponding test set with the same algorithm.For a comparative evaluation, the outcomes of each ap-proach on a test set of names are converted to a classifi-cation over all possible pairs of names (including non-matching pairs).
Only examples in the set Mp, thosethat are predicated to belong to the same entity (posi-tive predictions) are used in the evaluation, and are com-pared with the set Ma of examples annotated as positive.The performance of an approach is then evaluated by F1value, defined as: F1 = 2|Mp?Ma||Mp|+|Ma| .3The average SoftTFIDF similarity between names of thesame entity is 0.81, 0.89 and 0.95 for people, locations and or-ganizations respectively.6.1 Comparison of Different ApproachesFig.
4 presents the performance of different approaches(described in Sec.
5) on identifying the three entity types.We experimented with different clustering algorithms butonly the results by Single-Linkage are reported for Clus-ter over LMR (P|W) and SDC, since they are the best.SDC works well for all three entity types in spite oftheir different characteristics.
The best F1 values of SDCare 92.7%, 92.4% and 95.7% for people, locations andorganizations respectively, about 20% ?
30% error re-duction compared with the best performance of the otherapproaches.
This is an indication that this new approachwhich integrates metric learning and supervision in a uni-fied framework, has significant advantages 4.6.2 Further Analysis of SDCIn the next experiments, we will further analyze the char-acteristics of SDC by evaluating it in different settings.Different Training Sizes Fig.
5 reports the relationshipbetween the performance of SDC and different trainingsizes.
The learning curves for other learning-based ap-proaches are also shown.
We find that SDC exhibits goodlearning ability with limited supervision.
When trainingexamples are very limited, for example, only 10% of all300 names, pairwise classifiers based on Perceptron andWinnow exhibit advantages over SDC.
However, whensupervision become reasonable (30%+ examples), SDCstarts to outperform all other approaches.Different Clustering Algorithms Fig.
6 shows theperformance of applying different clustering algorithms(see Sec.
5) in the SDC approach.
Single-Linkage andComplete-Linkage outperform all other algorithms.
Onepossible reason is that this task has a great number of4We note that in this experiment, the relative comparisonbetween the pairwise classifiers and the clustering approachesover them is not consistent for all entity types.
This can bepartially explained by the theoretical analysis in (Li et al, 2004)and the difference between entity types.69808284868890929496(a) PeopleF 1 (%)808284868890929496(b) LocationsF 1 (%)808284868890929496(c) OrganizationsF 1 (%)SoftTFIDFLMR (P)LMR (W)Cluster over SoftTFIDFCluster over LMR (P)Cluster over LMR (W)SDCFigure 4: Performance of different approaches.
The results are reported for SDC with a learning rate ?
= 100.0.The Single-Linkage algorithm is applied whenever clustering is performed.
Results are reported in F1 and averagedover the three data sets for each entity type and 10 runs of two-fold cross-validation.
Each training set typicallycontains 300 annotated names.10 20 30 40 50 60 70 80 90 100707580859095100(a) PeoplF 1 (%)LMR (P)LMR (W)Cluster over LMR (P)Cluster over LMR (W)SDC10 20 30 40 50 60 70 80 90 100707580859095100(b) LocationsF 1 (%)10 20 30 40 50 60 70 80 90 100707580859095100(c) OrganizationsF 1 (%)Figure 5: Performance for different training sizes.
Five learning-based approaches are compared.
Single-Linkage isapplied whenever clustering is performed.
X-axis denotes different percentages of 300 names used in training.
Resultsare reported in F1 and averaged over the three data sets for each entity type.People Locations Organizations405060708090Different Entity TypesF 1 (%)GraphK?MedoidsRBComplete?LinkageSingle?LinkageFigure 6: Different clustering algorithms.
Five cluster-ing algorithms are compared in SDC (?
= 100.0).
Re-sults are averaged over the three data sets for each entitytype and 10 runs of two-fold cross-validations.classes (100 ?
200 entities) for 300 names in each sin-gle data set.
The results indicate that the metric learn-ing process relies on properties of the data set, as well asthe clustering algorithm.
Even if a good distance metriccould be learned in SDC, choosing an appropriate algo-rithm for the specific task is still important.Different Learning Rates We also experimented withdifferent learning rates in the SDC approach as shown inFig.
7.
It seems that SDC is not very sensitive to differentlearning rates as long as it is in a reasonable range.People Locations Organizations868890929496Different Entity TypesF 1 (%)?=1.0?=10.0?=100.0?=1000.0Figure 7: Performance for different learning rates.SDC with different learning rates (?
= 1.0, 10.0, 100.0,1000.0) compared in this setting.
Single-Linkage cluster-ing algorithm is applied.6.3 DiscussionThe reason that SDC can outperform existing clusteringapproaches can be explained by the advantages of SDC ?training the distance function with respect to the chosenclustering algorithm, guided by supervision, but they donot explain why it can also outperform the pairwise clas-sifiers.
One intuitive explanation is that supervision in theentity identification task or similar tasks is typically givenon whether two names correspond to the same entity ?entity-level annotation.
Therefore it does not necessarilymean whether they are similar in appearance.
For exam-70ple, ?Brian?
and ?Wilson?
could both refer to a person?Brian Wilson?
in different contexts, and thus this namepair is a positive example in training a pairwise classi-fier.
However, with features that only capture the appear-ance similarity between names, such apparently differentnames become training noise.
This is what exactly hap-pened when we train the LMR classifier with such namepairs.
SDC, however, can employ this entity-level anno-tation and avoid the problem through transitivity in clus-tering.
In the above example, if there is ?Brian Wilson?in the data set, then ?Brian?
and ?Wilson?
can be bothclustered into the same group with ?Brian Wilson?.
Suchcases do not frequently occur for locations and organiza-tion but still exist .7 ConclusionIn this paper, we explicitly formalize clustering as a learn-ing task, and propose a unified framework for traininga metric for any chosen clustering algorithm, guided bydomain-specific supervision.
Our experiments exhibit theadvantage of this approach over existing approaches onEntity Identification.
Further research in this directionwill focus on (1) applying it to more NLP tasks, e.g.coreference resolution; (2) analyzing the related theoret-ical issues, e.g.
the convergence of the algorithm; and(3) comparing it experimentally with related approaches,such as (Xing et al, 2002) and (McCallum and Wellner,2003).Acknowledgement This research is supported byNSF grants IIS-9801638 and ITR IIS-0085836, an ONRMURI Award and an equipment donation from AMD.ReferencesF.
R. Bach and M. I. Jordan.
2003.
Learning spectral clustering.In NIPS-03.A.
Bar-Hillel, T. Hertz, N. Shental, and D. Weinshall.
2003.Learning distance functions using equivalence relations.
InICML-03, pages 11?18.M.
Bilenko, R. Mooney, W. Cohen, P. Ravikumar, and S. Fien-berg.
2003.
Adaptive name matching in information integra-tion.
IEEE Intelligent Systems, pages 16?23.M Bilenko, S. Basu, and R. J. Mooney.
2004.
Integrating con-straints and metric learning in semi-supervised clustering.
InICML-04, pages 81?88.P.
Brown, P. deSouza R. Mercer, V. Pietra, and J. Lai.
1992.Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479.C.
Cardie and K. Wagstaff.
1999.
Noun phrase coreference asclustering.
In EMNLP-99, pages 82?89.S.
C. Chu, J. F. Roddick, and J. S. Pan.
2001.
A comparativestudy and extensions to k-medoids algorithms.
In ICOTA-01.W.
Cohen and J. Richman.
2002.
Learning to match and clus-ter large high-dimensional data sets for data integration.
InKDD-02, pages 475?480.W.
Cohen, P. Ravikumar, and S. Fienberg.
2003.
A comparisonof string metrics for name-matching tasks.
In IIWeb Work-shop 2003, pages 73?78.I.
Dagan, L. Lee, and F. Pereira.
1999.
Similarity-based mod-els of word cooccurrence probabilities.
Machine Learning,34(1-3):43?69.Y.
Freund and R. Schapire.
1998.
Large margin classificationusing the Perceptron algorithm.
In COLT-98.M.
Geffet and I. Dagan.
2004.
Automatic feature vector qualityand distributional similarity.
In COLING-04.K.
George.
2003.
Cluto: A clustering toolkit.
Technical report,Dept of Computer Science, University of Minnesota.J.
Hartigan and M. Wong.
1979.
A k-means clustering algo-rithm.
Applied Statistics, 28(1):100?108.S.
Kamvar, D. Klein, and C. Manning.
2002.
Interpreting andextending classical agglomerative clustering algorithms us-ing a model-based approach.
In ICML-02, pages 283?290.L.
Lee.
1997.
Similarity-Based Approaches to Natural Lan-guage Processing.
Ph.D. thesis, Harvard University, Cam-bridge, MA.X.
Li, P. Morie, and D. Roth.
2004.
Identification and trac-ing of ambiguous names: Discriminative and generative ap-proaches.
In AAAI-04, pages 419?424.G.
Mann and D. Yarowsky.
2003.
Unsupervised personal namedisambiguation.
In CoNLL-03, pages 33?40.A.
McCallum and B. Wellner.
2003.
Toward conditional mod-els of identity uncertainty with application to proper nouncoreference.
In IJCAI Workshop on Information Integrationon the Web.D.
Mochihashi, G. Kikui, and K. Kita.
2004.
Learning non-structural distance metric by minimum cluster distortions.
InCOLING-04.P.
Pantel and D. Lin.
2002.
Discovering word senses from text.In KDD-02, pages 613?619.D.
Roth.
1998.
Learning to resolve natural language ambigui-ties: A unified approach.
In AAAI-98, pages 806?813.M.
Schultz and T. Joachims.
2004.
Learning a distance metricfrom relative comparisons.
In NIPS-04.E.
Voorhees.
2002.
Overview of the TREC-2002 question an-swering track.
In TREC-02, pages 115?123.J.
Weeds, D. Weir, and D. McCarthy.
2004.
Characterisingmeasures of lexical distributional similarity.
In COLING-04.E.
P. Xing, A. Y. Ng, M. I. Jordan, and S. Russell.
2002.Distance metric learning, with application to clustering withside-information.
In NIPS-02.71
