One Tokenization per SourceJin GUtKent Ridge Digital Labs21 Heng Mui Keng Terrace, Singapore 119613AbstractWe report in this paper the observation of onetokenization per source.
That is, the same criticalfragment in different sentences from the samesource almost always realize one and the same ofits many possible tokenizations.
This observation isdemonstrated very helpful in sentence tokenizationpractice, and is argued to be with far-reachingimplications in natural language processing.1 IntroductionThis paper sets to establish the hypothesis of onetokenization per source.
That is, if an ambiguousfragment appears two or more times in differentsentences from the same source, it is extremelylikely that they will all share the sametokenization.Sentence tokenization is the task of mappingsentences from character strings into streams oftokens.
This is a long-standing problem in ChineseLanguage Processing, since, in Chinese, there isan apparent lack of such explicit word delimitersas white-spaces in English.
And researchers havegradually been turning to model the task as ageneral lexicalization or bracketing problem inComputational Linguistics, with the hope that theresearch might also benefit the study of similarproblems in multiple languages.
For instance, inMachine Translation, it is widely agreed that manymultiple-word expressions, such as idioms,compounds and some collocations, while notexplicitly delimited in sentences, are ideally to betreated as single lexicalized units.The primary obstacle in sentence tokenization is inthe existence of uncertainties both in the notion ofwords/tokens and in the recognition ofwords/tokens in context.
The same fragment-indifferent contexts would have to be tokenizeddifferently.
For instance, the character stringtodayissunday would normally be tokenized as457"'today is sunday" but can also reasonably be"'today is sun day".In terms of possibility, it has been argued that nolexically possible tokenization can not begrammatically and meaningfully realized in atleast some special contexts, as every token can beassigned to bear any meaning without anyorthographic means.
Consequently, themainstream research in the literature has beenfocused on the modeling and utilization of localand sentential contexts, either linguistically in arule-based framework or statistically in asearching and optimization set-up (Gan, Palmerand Lua 1996; Sproat, Shih, Gale and Chang1996; Wu 1997; Gut  1997).Hence, it was really a surprise when we firstobserved the regularity of one tokenization persource.
Nevertheless, the regularity turns out to bevery helpful in sentence tokenization practice, andto be with far-reaching implications in naturallanguage processing.
Retrospectively, we nowunderstand that it is by no means an isolatedspecial phenomenon but another display of thepostulated general law of one realization perexpression.In the rest of the paper, we will first present aconcrete corpus verification (Section 2), clarify itsmeaning and scope (Section 3), display its strikingutility value in tokenization (Section 4), and thendisclose its implication for the notion ofwords/tokens (Section 5), and associate thehypothesis with the general aw of one realizationper expression through examination of relatedworks in the literature (Section 6).2 Corpus InvestigationThis section reports a concrete corpusinvestigation aimed at validating the hypothesis.2.1 DataThe two resources used in this study are theChinese PH corpus (Gut 1993) and the Beihangdictionary (Liu and Liang 1989).
The Chinese PHcorpus is a collection of about 4 millionmorphemes of news articles from the single sourceof China's Xinhua News Agency in 1990 and1991.
The Beihang dictionary is a collection ofabout 50,000 word-like tokens, each of whichoccurs at least 5 times in a balanced collection ofmore than 20 million Chinese characters.What is unique in the PH corpus is that all andonly unambiguous token boundaries with respectto the Beihang dictionary have been marked.
Forinstance, if the English character stringfundsandmoney were in the PH corpus, it wouldbe in the form of fundsand/money, since theposition in between character d and m is anunambiguous token boundary with respect tonormal English dictionary, but fundsand could beeither funds/and or fund/sand.There are two types of fragments in betweenadjacent unambiguous token boundaries: thosewhich are dictionary entries on the whole, andthose which are not.2.2 Dict ionary -Ent ry  F ragmentsWe manually tokenized in context each of thedictionary-entry fragments in the first 6,000 linesof the PH corpus.
There are 6,700 differentfragments which cumulatively occur 46,635 times.Among them, 14 fragments (Table 1, Column 1)realize different tokenizations in their 87occurrences.
16 tokenization errors would beintroduced if taking majority tokenizations only(Table 2).Also listed in Table 1 are the numbers offragments tokenized as single tokens (Column 2)or as a stream of multiple tokens (Column 3).
Forinstance, the first fragment must be tokenized as asingle token for 17 times but only for once as atoken-pair.Table 1: Dictionary-entry f agmentsrealizing different tokenizations in the PH corpus.mJmil_lmlmmn nnumnnn: nnumn/ - nnumn nEnnm lmmmlmmnmmn  munmmmm R nnmTable 2: Statistics for dictionary-entry f agments.
(0) (1) (2) (3)=(2)/(1)Fragment All Multiple PercentageOccurrences 46635 87 0.19Forms 6700 14 0.21Errors 46635 16 0.03In short, 0.19% of all the different dictionary-entryfragments, taking 0.21% of all the occurrences,have realized different okenizations, and 0.03%tokenization errors would be introduced if forcedto take one tokenization per fragment.2.3 Non-Dict ionary-Entry  F ragmentsSimilarly, we identified in the PH corpus allfragments that are not entries in the Beihangdictionary, and manually tokenized each of themin context.
There are 14,984 different fragmentswhich cumulatively occur 49,308 times.
Amongthem, only 35 fragments (Table 3) realize differenttokenizations in their 137 occurrences.
39tokenization errors would be introduced if takingmajority tokenizations only (Table 4).Table 3: Non-dictionary-entry fragmentsrealizingd~renttokeni.I~  { I~~--~ +- -~~ - -~ations in the PH corpus.Ak~ :J:Table 4: Statistics for non-dictionary entry fragments.
(0)FragmentForms(1) (2) (3)=(2)/(1)All Multiple Percenta\[~e14984 35 0.23Occurrences 49308 137Errors 49308 390.280.08In short, 0.23% of all the non-dictionary-entryfragments, taking 0.28% of all occurrences, haverealized different tokenizations, and 0.08%tokenization errors would be introduced if forcedto take one tokenization per fragment.2.4 Tokenization CriteriaSome readers might question the reliability of thepreceding results, because it is well-known in theliterature that both the inter- and intra-judgetokenization consistencies can hardly be betterthan 95% but easily go worse than 70%, if the458tokenization is guided solely by the intuition ofhuman judges.To ensure consistency, the manual tokenizationreported in this paper has been independently donetwice under the following three criteria, applied inthat order:(1) Dictionary Existence: The tokenizationcontains no non-dictionary-entry characterfragment.
(2) Structural Consistency: The tokenization hasno crossing-brackets (Black, Garside andLeech 1993) with at least one correct andcomplete structural analysis of its underlyingsentence.
(3) Maximum Tokenization: The tokenization is acritical tokenization (Guo 1997).The basic idea behind is to regard sentencetokenization as a (shallow) type of (phrase-structure-like) morpho-syntactic parsing which isto assign a tree-like structure to a sentence.
Thetokenization of a sentence is taken to be thesingle-layer bracketing corresponding to thehighest-possible cross-section of the sentence tree,with each bracket a token in dictionary.Among the three criteria, both the criterion ofdictionary existence and that of maximumtokenization are well-defined without anyuncertainty, as long as the tokenization dictionaryis specified.However, the criterion of structural consistency issomewhat under-specified since the samelinguistic expression may have different sentencestructural analyses under different grammaticaltheories and/or formalisms, and it may be readdifferently by different people.Fortunately, our tokenization practice has shownthat this is not a problem when all thecontroversial fragments are carefully identifiedand their tokenizations from different grammarschools are purposely categorized.
Note, theemphasis here is not on producing a unique"correct" tokenization but on managing andminimizing tokenization i consistencyL3 One Tokenization per SourceNoticing that all the fragments studied in thepreceding section are critical fragments (Guo1997) from the same source, it becomesreasonable to accept he following hypothesis.One tokenization per source: For any criticalfragment from a given source, if one of itstokenization is correct in one occurrence, thesame tokenization is also correct in all its otheroccurrences.The linguistic object here is a critical fragment,i.e., the one in between two adjacent critical pointsor unambiguous token boundaries (Guo 1997), butnot an arbitrary sentence segment.
The hypothesissays nothing about the tokenization of a non-critical fragment.
Moreover, the hypothesis doesnot apply even if a fragment is critical in someother sentences from the same source, but notcritical in the sentence in question.The hypothesis does not imply contextindependence in tokenization.
While the correcttokenization correlates decisively with its source,it does not indicate that the correct tokenizationhas no association with its local sentential context.Rather, the tokenization of any fragment has to berealized in local and sentential context.It might be arguable that the PH corpus of 4million morphemes is not big enough to enablemany of the critical fragments to realize theirdifferent readings in diverse sentential contexts.To answer the question, I0 colleagues were askedto tokenize, without seeing the context, the mostfrequent 123 non-dictionary-entry criticalfragments extracted from the PH corpus.
Severalof these fragments 2 have thus been marked"context dependent", since they have "obvious"different readings in different contexts.
Shown inFigure 1 are three examples.219\[c< ~J~ 7\]~ >< 5~; ~7~ >1180\[c< ~ ~ >< ?
~ >\]106\[< A .~ ~ >c< X ~"  >\]Figure 1: Critical fragments with "obvious" multiplereadings.
Preceding numbers are their occurrencecounts in the PH corpus.i For instance, the Chinese fragment dp dx(secondary primary school) is taken as "\[secondary(and) primary\] school" by one school of thought, but"\[secondary (school)\] (and) \[primary school\]" byanother.
But both will never agree that the fragmentmust be analyzed ifferently in different context.2 While all fragments are lexically ambiguous intokenization, many of them have received consistentunique tokenizations, as these fragments are, to thehuman judges, self-sufficient for comfortable ambiguityresolution.459We looked all these questionable fragments up ina larger corpus of about 60 million morphemes ofnews articles collected from the same source asthat of the PH corpus in a longer time span from1989 to 1993.
It turns out that all the fragmentseach always takes one and the same tokenizationwith no exception.While we have not been able to specify the notionof source used in the hypothesis to the sameclarity as that of critical fragment and criticaltokenization in (Guo 1997), the above empiricaltest has made us feel comfortable to believe thatthe scope of the source can be sufficiently large tocover any single domain of practical interest.4 Application in TokenizationThe hypothesis of one tokenization per source canbe applied in many ways in sentence tokenization.For tokenization ambiguity resolution, let usexamine the following strategy:Tokenization by memorization: I f  the correcttokenization of a critical fragment is known in onecontext, remember the tokenization.
If  the samecritical fragment is seen again, retrieve its storedtokenization.
Otherwise, if a critical fragmentencountered has no stored tokenization, randomlyselect one of its critical tokenizations.This is a pure and straightforward implementationof the hypothesis of one tokenization per source,as it does not explore any constraints other thanthe tokenization dictionary.While sounds trivial, this strategy performssurprisingly well.
While the strategy is universallyapplicable to any tokenization ambiguityresolution, here we will only examine itsperformance in the resolution of criticalambiguities (Guo 1997), for ease of directcomparison with works in the literature.As above, we have manually tokenized 3 all non-dictionary-entry critical fragments in the PHcorpus; i.e., we have known the correcttokenizations for all of these fragments.
Therefore,if any of these fragments presents omewhere else,its tokenization can be readily retrieved from whatwe have manually done.
If the hypothesis holdsperfect, we could not make any error.3 This is not a prohibitive job but can be done wellwithin one man-month, if the hypothesis i  adopted.460The only weakness of this strategy is its apparentinadequacy in dealing with the sparse dataproblem.
That is, for unseen critical fragments,only the simplest okenization by random selectionis taken.
Fortunately, we have seen on the PHcorpus that, on average, each non-dictionary-entrycritical fragment has just two (100,398 over49,308 or 2.04 to be exact) critical tokenizations tobe chosen from.
Hence, a tokenization accuracy ofabout 50% can be expected for unknown non-dictionary-entry critical fragments.The question then becomes that: what is thechance of encountering a non-dictionary-entrycritical fragment that has not been seen before inthe PH corpus and thus has no known correcttokenization?
A satisfactory answer to thisquestion can be readily derived from the Good-Turing Theorem 4 (Good 1953; Church and Galewith Kruskal 1991, page 49).Table 5: Occurrence distribution of non-dictionary-entry critical fragments in the PH corpus.r 1 2 3 4 5Nr 9587 2181 939 523 339r 6 7 8 9 _>9Nr 230 188 128 94 775Table 4 and Table 5 show that, among the 14,984different non-dictionary-entry critical fragmentsand their 49,308 occurrences in the PH corpus,9,587 different fragments each occurs exactlyonce.
By the Good-Turing Theorem, the chance ofencountering an arbitrary non-dictionary-entrycritical fragment hat is not in the PH corpus isabout 9,587 over 49,308 or slightly less than 20%.In summary, if applied to non-dictionary-entrycritical fragment okenization, the simple strategyof tokenization by memorization delivers virtually100% tokenization accuracy for slightly over 80%of the fragments, and about 50% accuracy for therest 20% fragments, and hence has an overalltokenization accuracy of better than 90% (= 80% x100% + 20% x 50%).4 The theorem states that, when two independentmarginally binomial samples Be and B 2 are drawn, theexpected frequency r" in the sample B~ of typesoccurring r times in B t is r'=(r+I)E(N,.~)/E(N,),where E(N,) is the expectation of the number of typeswhose frequency ina sample is r.What we are looking for here is the quantity ofr'E(N,) for r=O, or E(N~), which can be closelyapproximated by the number of non-dictionary-entryfragments hat occurred exactly once in the PH corpus.This strategy rivals all proposals with directlycomparable performance reports in the literature,including 5 the representative one by Sun andT'sou (1995), which has the tokenization accuracyof 85.9%.
Notice that what Sun and T'souproposed is not a trivial solution.
They developedan advanced four-step decision procedure thatcombines both mutual information and t-scoreindicators in a sophisticated way for sensibledecision making.Since the memorization strategy complementswith most other existing tokenization strategies,certain types of hybrid solutions are viable.
Forinstance, if the strategy of tokenization bymemorization is applied to known criticalfragments and the Sun and T'sou algorithm isapplied to unknown critical fragments, the overallaccuracy of critical ambiguity resolution can bebetter than 97% (= 80% + 20% x 85.9%).The above analyses, together with some othermore or less comparable results in the literature,are summarized in Table 6 below.
It is interestingto note that, the best accuracy registered inChina's national 863-Project evaluation in 1995was only 78%.
In conclusion, the hypothesis ofone tokenization per source is unquestionablyhelpful in sentence tokenization.Table 6: Tokenization performance comparisons.ApproachMemorizationSun et al (1996)Wong et al (1994)Zheng and Liu (1997)863-Project 1995 Evaluation(Zheng and Liu, 1997)Memorization +Sun et alAccuracy, (%)9085.971.2817897s The task there is the resolution of overlappingambiguities, which, while not exactly the same, iscomparable with the resolution of critical ambiguities.The tokenization dictionary they used has about 50,000entries, comparable to the Beihang dictionary we usedin this study.
The corpus they used has about 20 millionwords, larger than the PH corpus.
More importantly, interms of content, it is believed that both the dictionaryand corpus are comparable to what we used in thisstudy.
Therefore, the two should more or less becomparable.4615 The  Notion of TokensUpon accepting the validness of the hypothesis ofone tokenization per source, and afterexperiencing its striking utility value in sentencetokenization, now it becomes compelling for anew paradigm.
Parallel to what Dalton did forseparating physical mixtures from chemicalcompounds (Kuhn 1970, page 130-135), we arenow suggesting to regard the hypothesis as a law-of-language and to take it as the proposition ofwhat a word/token must be.The Notion of Tokens: A stretch of characters isa legitimate token to be put in tokenizationdictionary if and only if it does not introduce anyviolation to the law of one tokenization per source.Opponents hould reject this notion instantly as itobviously makes the law of one tokenization persource a tautology, which was once one of ourown objections.
We recommend these readers toreexamine some of Kuhn's (1970) arguments.Apparently, the issue at hand is not merely over amatter of definition of words/tokens.
The merit ofthe notion, we believe, lies in its far-reachingimplications in natural language processing ingeneral and in sentence tokenization i particular.For instance, it makes the separation betweenwords and non-words operational in Chinese, yetmaintains the cohesiveness of words/tokens as arelatively independent layer of linguistic entitiesfor rigorous scrutiny.
In contrast, while theparadigm of "mutual affinity" represented bymeasurements such as mutual information and t-score has repetitively exhibited inappropriatenessin the very large number of intermediate cases, theparadigm of "linguistic words" represented byterms like syntactic-words, phonolo~cal-wordsand semantic-words i in essence rejecting thenotion of Chinese words/tokens at all, ascompounding, phrase-forming and even sentenceformation in Chinese are governed by more or lessthe same set of regularities, and as the whole isalways larger than the simple sum of its parts.
Weshall leave further discussions to another place.6 DiscussionLike most discoveries in the literature, when wefirst captured the regularity several years ago, wesimply could not believe it.
Then, after carefulexperimental validation on large representativecorpora, we accepted it but still could not imagineany of its utility value.
Finally, after working outways that unquestionably demonstrated itsusefulness, we realized that, in the literature, somany supportive evidences have already beenpresented.
Further, while never consciously in anexplicit form, the hypothesis has actually alreadybeen widely employed.For example, Zheng and Liu (1997) recentlystudied a newswire corpus of about 1.8 millionChinese characters and reported that, among allthe 4,646 different chain-length-l two-character-overlapping-typd s ambiguous fragments whichcumulatively occur 14,581 times in the corpus,only 8 fragments each has different okenizationsin different context, and there is no such fragmentin all the 3,409 different chain-length-2 two-character-overlapping-type 7 ambiguousfragments.Unfortunately, due to the lack of a properrepresentation framework comparable to thecritical tokenization theory employed here, theirobservation is neither complete nor explanatory.
Itis not complete, since the two ambiguous typesapparently do not cover all possible ambiguities.
Itis not explanatory, since both types of ambiguousfragments are not guaranteed to be criticalfragments, and thus may involve other types ofambiguities.Consequently, Zheng and Liu (1997) themselvesmerely took the apparent regularity as a specialcase, and focused on the development of local-context-oriented disambiguation rules.
Moreover,while they constructed for tokenizationdisambiguation a  annotated "phrase base" of allambiguous fragments in the large corpus, they stillconcluded that good results can not come solelyfrom corpus but have to rely on the utilization ofsyntactic, semantic, pragmatic and otherinformation.The actual implementation f the weighted finite-state transducer by Sproat et al (1996) can betaken as an evidence that the hypothesis of onetokenization per source has already in practicaluse.
While the primary strength of such atransducer is its effectiveness in representing and6 Roughly a three-character fragment abc where a, b, c,ab, and bc are all tokens in the tokenization dictionary.7 Roughly a four-character fragment abcd, where a, b,c, d, ab, bc, and cd are all tokens in the tokenizationdictionary.utilizing local and sentential constraints, whatSproat et al (1996) implemented was simply atoken unigram scoring function.
Under thissetting, no critical fragment can realize differenttokenizations in different local sentential context,since no local constraints other than the identity ofa token together with its associated token scorecan be utilized.
That is, the requirement of onetokenization per source has actually beenimplicitly obeyed.We admit here that, while we have been aware ofthe fact for long time, only after the disseminationof the closely related hypotheses of one sense perdiscourse (Gale, Church and Yarowsky 1992)" andone sense per collocation (Yarowsky 1993), weare able to articulate the hypothesis of onetokenization per source.The point here is that, one tokenization per sourceis unlikely an isolated phenomenon.
Rather, theremust exist a general law that covers all the relatedlinguistic phenomena.
Let us speculate that, for aproper linguistic expression in a proper scope,there always exists the regularity of onerealization per expression.
That is, only one of themultiple values on one aspect of a linguisticexpression can be realized in the specified scope.In this way, one tokenization per source becomesa particular articulation of one realization perexpression.The two essential terms here are the properlinguistic expression and the proper scope of theclaim.
A quick example is helpful here: part-of-speech tagging for the English sentence "Can youcan the can?"
If the linguistic expressions aretaken as ordinary English words, they arenevertheless highly ambiguous, e.g., the Englishword can realizes three different part-of-speechesin the sentence.
However, if "the can", "can the"and the like are taken as the underling linguisticexpressions, they are apparently unambiguous:"the can/NN", "can/VB the" and the rest"can/MD".
This fact can largely be predicted bythe hypothesis of one sense per collocation, andcan partially explain the great success of Brill'stransformation-based part-of-speech tagging (Brill1993).As to the hypothesis of one tokenization persource, it is now clear that, the theory of criticaltokenization has provided the suitable means forcapturing the proper linguistic expression.4627 ConclusionThe hypothesis of one tokenization per  sourceconfirms surprisingly well (99.92% ~ 99.97%)with corpus evidences, and works extremely well(90% - 97%) in critical ambiguity resolution.
It isformulated on the critical tokenization theory andinspired by the parallel hypotheses of one senseper  discourse and one sense per  collocation, as ispostulated as a particular articulation of thegeneral law of one realization per  expression.
Wealso argue for the further generalization ofregarding it as a new paradigm for studying thetwin-issue of token and tokenization.AcknowledgementsPart of this paper, especially the Introduction andDiscussion sections, was once presented at theNovember 1997 session of the monthly Symposium onLinguistics and Language Information Researchorganized by COLIPS (Chinese and OrientalLanguages Information Processing Society) inSingapore.
Fruitful discussions, especially with Xu Jie,Ji Donghong, Su Jian, Ni Yibin, and Lua Kim Teng, aregratefully acknowledged, as are the tokenization effortsby dozen of my colleagues and friends.
However, theopinions expressed reflect solely those of the author.ReferencesBlack, Ezra, Roger Garside, and Geoffery Leech(1993).
Statistically-Driven Computer Grammars ofEnglish: The IBM/Lancaster Approach, Amsterdam:Rodopi Publishers.Brill, Eric (1993).
A Corpus-Based Approach toLanguage Learning, Ph.D Dissertation, Departmentof Computer and Information Science, University ofPennsylvania.Church, Kenneth.
W. and William A. Gale (1991).
AComparison of the Enhanced Good-Turing andDeleted Estimation Methods for EstimatingProbabilities of English Bigrams, Computer Speechand Language, Vol.
5, No.
1, pages 19-54.Gale, William A., Kenneth W. Church and DavidYarowsky (1992b).
One Sense Per Discourse, In:Proceedings ofthe 4 '~ DARPA Workshop on Speechand Natural Language, pages 233-237.Gan, Kok-Wee; Palmer, Martha; and Lua, Kim-Teng(1996).
A Statistically Emergent Approach forLanguage Processing: Application to ModelingContext Effects in Ambiguous Chinese WordBoundary Perception.
Computational LinguisticsVol.
22, No.
4, pages 531-553.Good, I. J.
(1953).
The Population Frequencies ofSpecies and the Estimation of PopulationParameters.
Biometrika, Volume 40, pages 237-264.Guo, Jin (1993).
PH - A Free Chinese Corpus,Communications of COUPS, Vol.
3, No.
1, pages45-48.Guo, Jin (1997).
Critical Tokenization and itsProperties, Computational Linguistics, Vol.
23, No.4, pages 569-596.Kuhn, Thomas (1970).
The Structure of ScientificRevolutions.
Second Edition, Enlarged.
TheUniversity of Chicago Press.
Chicago.Liu, Yuan and Nanyuan Liang (1989).
ContemporaryChinese Common Word Frequency Dictionary(Phonetically Ordered Version).
Yuhang Press,Beijing.Sproat, Richard, Chilin Shih, Villiam Gale, and NancyChang (1996).
A Stochastic Finite-State Word-Segmentation Algorithm for Chinese,Computational Linguistics, Vol.
22, No.
3, pages377--404.Sun, Maosong and Benjemin Tsou (1995).
AmbiguityResolution in Chinese Word Segmentation,Proceedings of the 10th Pacific Asia Conference onLanguage, Information and Computation (PACLIC-95), pages 121-126, Hong Kong.Wong, K-F.; Pan, H-H.; Low, B-T.; Cheng, C-H.;Lum, V. and Lam, S-S. (1995).
A Tool forCompute-Assisted Open Response Analysis,Proceedings ofthe 1995 International Conference onComputer Processing of Oriental Languages, pages191-198, Hawaii.Wu, Dekai (1997).
Stochastic Inversion TransductionGrammars and Bilingual Parsing of ParallelCorpora, Computational Linguistics, Vol.
23, No.
3,pages 377-403.Yarowsky, David (1993).
One Sense Per Collocation,In: Proceedings of ARPA Human LanguageTechnology Workshop, Princeton, pages 266-271.Zheng, Jiaheng and Kaiying Liu (1997).
The Researchof Ambiguity Word-Segmentation Technique for theChinese Text, In Chert, Liwai and Qi Yuan (editors).Language Engineering, Tsinghua University Press.Page 201-206.463
