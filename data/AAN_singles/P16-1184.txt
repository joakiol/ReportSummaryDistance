Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1954?1964,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCross-Lingual Morphological Tagging for Low-Resource LanguagesJan BuysDepartment of Computer ScienceUniversity of Oxfordjan.buys@cs.ox.ac.ukJan A. BothaGoogle Inc.Londonjabot@google.comAbstractMorphologically rich languages often lackthe annotated linguistic resources requiredto develop accurate natural language pro-cessing tools.
We propose models suitablefor training morphological taggers withrich tagsets for low-resource languageswithout using direct supervision.
Ourapproach extends existing approaches ofprojecting part-of-speech tags across lan-guages, using bitext to infer constraints onthe possible tags for a given word type ortoken.
We propose a tagging model us-ing Wsabie, a discriminative embedding-based model with rank-based learning.
Inour evaluation on 11 languages, on av-erage this model performs on par with abaseline weakly-supervised HMM, whilebeing more scalable.
Multilingual experi-ments show that the method performs bestwhen projecting between related languagepairs.
Despite the inherently lossy pro-jection, we show that the morphologicaltags predicted by our models improve thedownstream performance of a parser by+0.6 LAS on average.1 IntroductionMorphologically rich languages pose signifi-cant challenges for Natural Language Processing(NLP) due to data-sparseness caused by large vo-cabularies.
Intermediate processing is often re-quired to address the limitations of only using sur-face forms, especially for small datasets.
Commonmorphological processing tasks include segmenta-tion (Creutz and Lagus, 2007; Snyder and Barzi-lay, 2008), paradigm learning (Durrett and DeN-ero, 2013; Ahlberg et al, 2015) and morphologi-cal tagging (M?uller and Schuetze, 2015).
In thispaper we focus on the latter.Parts-of-speech (POS) tagging is the most com-mon form of syntactic annotation.
However, thegranularity of POS varies across languages andannotation-schemas, and tagsets have often beenextended to include tags for morphologically-marked properties such as number, case or de-gree.
To enable cross-lingual learning, a small setof universal (coarse-grained) POS tags have beenproposed (Petrov et al, 2012).
For morphologicalprocessing this can be complemented with a set ofattribute-feature values that makes the annotationmore fine-grained (Zeman, 2008; Sylak-Glassmanet al, 2015b).Tagging text with morphologically-enriched la-bels has been shown to benefit downstream taskssuch as parsing (Tsarfaty et al, 2010) and seman-tic role labelling (Haji?c et al, 2009).
In genera-tion tasks such as machine translation these tagscan help to generate the right form of a wordand to model agreement (Toutanova et al, 2008).Morphological information can also benefit au-tomatic speech recognition for low-resource lan-guages (Besacier et al, 2014).However, annotating sufficient data to learn ac-curate morphological taggers is expensive and re-lies on linguistic expertise, and is therefore cur-rently only feasible for the world?s most widely-used languages.
In this paper we are interested inlearning morphological taggers without the avail-ability of supervised data.
A successful paradigmfor learning without direct supervision is to makeuse of word-aligned parallel text, with a resource-rich language on one side and a resource-poor lan-guage on the other side (Yarowsky et al, 2001;Fossum and Abney, 2005; Das and Petrov, 2011;T?ackstr?om et al, 2013).In this paper we extend these methods, that havemostly been proposed for universal POS-taggers,to learn weakly-supervised morphological taggers.1954Our approach is based on projecting token andtype constraints across parallel text, learning a tag-ger in a weakly-supervised manner from the pro-jected constraints (T?ackstr?om et al, 2013).
Wepropose an embedding-based model trained withthe Wsabie algorithm (Weston et al, 2011), andcompare this approach against a baseline HMMmodel.We evaluate the projected tags for a set of lan-guages for which morphological tags are availablein the Universal Dependency corpora.
To showthe feasibility of our approach, and to compare theperformance of different models, we use Englishas source language.
Then we perform an evalua-tion on all language pairs in the set of target lan-guages which shows that the best performance isobtained when projecting between genealogicallyrelated languages.As an extrinsic evaluation of our approach, weshow that NLP models can benefit from usingthese induced tags even if they are not as accu-rate as tags produced by supervised models, byevaluating the effect of features obtained from tagspredicted by the induced morphological taggers independency parsing.2 Universal Morphological TagsIn order to do cross-lingual learning we require acommon morphological tagset.
To evaluate thesemodels we require datasets in multiple languageswhich have been annotated with such a consistentschema.
The treebanks annotated in the Univer-sal Dependencies (UD) project (de Marneffe et al,2014) are suitable for this purpose.All the data is annotated with universal POStags, a set of 17 tags1.
We use UD v1.2 (Nivreet al, 2015), which contain 25 languages anno-tated with morphological attributes (called fea-tures).
In addition to POS, there are 17 universalattributes, which each takes one of a set of valueswhen annotated.
The morphological tag of a tokendenotes the union of its morphological attribute-value pairs, including its POS.Although the schema is consistent across lan-guages, there are language-specific phenomenaand considerations that result in some mismatchesfor a given pair of languages.
One source of thisis that the UD treebanks were mostly constructedby fully or semi-automatic conversion of exist-1This extends, but is not fully consistent with, the set of12 tags proposed by Petrov et al (2012).ing treebanks which had used different annota-tion schemes.
Furthermore, not all the attributesand values appear in all languages (e.g.
additionalcases in morphologically-rich languages such asFinnish), and there are still a number of language-specific tags not in the universal schema.
Finally,in some instances properties that are not realisedin the surface word form are absent from the an-notation (e.g.
in English the person and number ofverbs are only annotated for third-person singular,as there are no distinct morphological forms fortheir other values).An example of the morphological annotationemployed is given in Figure 1.
Note that the an-notations for aligned word-pairs are not fully con-sistent.
Some attributes appear only in the Englishtreebank (e.g.
Voice), while others appear only inthe Dutch treebank (e.g.
Aspect, Subcat).3 Tag Projection across BitextOur approach to train morphological taggers isbased on the paradigm of projecting token andtype constraints as proposed by T?ackstr?om et al(2013).
The training data consist of parallel textwith the resource-rich language on the source-sideand the low-resource language on the target side.The source-side text is tagged with a supervisedmorphological tagger.
For every target-side sen-tence, the type and token constraints are used toconstruct a set of permitted tags for each token inthe sentence.
These constraints will then be usedto train morphological taggers.3.1 Type and token constraintsTo extract constraints from the parallel text, wefirst obtain bidirectional word alignments.
Toensure high quality alignments, alignment pairswith a confidence below a fixed threshold ?
areremoved.
The motivation for using only high-confidence alignments is that incorrect alignmentswill hurt the performance of the model, while it iseasier to use more parallel text to obtain a suffi-cient number of alignments for training.The first class of constraints that we extractfrom the parallel text is type constraints.
For eachword type, we construct a distribution over tags forthe word by accumulating counts of the morpho-logical tags of source-side tokens that are alignedto instances of the word type.
The set of tagswith probability above some threshold ?
is takenas the tag dictionary entry for that word type.
To1955POS=PRONNumber=PlurPerson=1Poss=YesPronType=PrsPOS=NOUNNumber=SingPOS=AUXMood=IndNumber=SingPerson=3Tense=PresVerbForm=FinPOS=VERBTense=PastVerbForm=PartVoice=Pass POS=ADPPOS=NOUNNumber=SingPOS=NOUNNumber=Sing POS=PUNCTOur independence is guaranteed by law today .Onze onafhankelijkheid wordt vandaag bij wet gegarandeerd .POS=PRONNumber=PlurPerson=1Poss=YesPronType=PrsPOS=NOUNNumber=SingPOS=AUXAspect=ImpMood=IndNumber=SingPerson=3Tense=PresVerbForm=FinPOS=ADVDegree=PosPOS=ADPAdpType=PrepPOS=NOUNNumber=SingPOS=VERBTense=PastVerbForm=PartSubCat=TranPOS=PUNCTFigure 1: A parallel sentence in English and Dutch annotated with universal morphological tags, showinghigh-confidence automatic word-alignments.
Attribute-value pairs that occur only on one side of analigned pair of tokens are indicated in italics.
The dashed line indicates a low-confidence alignmentpoint, which is ignored in our projection method.construct the training examples, each token whosetype occurs in the tag dictionary is restricted to theset of tags in the dictionary entry.
For tokens forwhich the dictionary entry is empty, all the tagsare included in the set of permitted tags (this hap-pens when the tag distribution is too flat and allthe probabilities are below the threshold).
In prin-ciple, type constraints can also be obtained froman external dictionary, but in this paper we assumewe do not have such a resource.The second class of constraints places restric-tions on word tokens.
Every target token is con-strained to the tag of its aligned source token,while unaligned tokens can take any tag.Token constraints are combined with type con-straints as proposed by T?ackstr?om et al (2013): Ifa token is unaligned, its type constraints are used.If the token is aligned, and there is no dictionaryentry for the token type, the token constraint isused.
If there is a dictionary entry for the tokentype, and the token constraint tag is in the dictio-nary, the token constraint is used.
If the token con-straint tag is not in the dictionary entry, the typeconstraints are used.4 Learning from Projected TagsNext we propose models to learn a morphologicaltagger from cross-lingually projected constraints.4.1 Related workHMMs have previously been used for weakly-supervised learning from token or type con-straints (Das and Petrov, 2011; Li et al, 2012;T?ackstr?om et al, 2013).
HMMs are generativemodels, and in this setting the words in the tar-get sentence form the observed sequence and themorphological tags the hidden sequence.
The pro-jected constraints are used as partially observedtraining data for the hidden sequence.T?ackstr?om et al (2013) proposed a discrimina-tive CRF model that relies on incorporating twosets of constraints, of which one is a subset ofthe other.
Ganchev and Das (2013) used a simi-lar CRF model, but instead of using the projectedtags as hard constraints, they were employed assoft constraints with posterior regularization.The model of Wisniewski et al (2014) makesgreedy predictions with a history-based model,that includes previously predicted tags in the se-quence, during training and testing.
The model istrained with a variant of the perceptron algorithmthat allows a set of positive labels.
When an incor-rect prediction is made during training, the param-eters are updated in the direction of all the positivelabels.4.2 HMM modelAs a baseline model we use an HMM where thetransition and emission distributions are param-eterized by log-linear models (a feature-HMM).Training is performed with L-BFGS rather thanwith the EM algorithm.
This parameterization wasproposed by Berg-Kirkpatrick et al (2010) and ap-plied to cross-lingual POS induction by Das and1956Petrov (2011) and T?ackstr?om et al (2013).Let w be the target sentence and t the sequenceof tags for the sentence.
The marginal probabilityof a sequence during training isp(w1:n) =?t1:n?Tn?i=1p(ti|ti?1)p(wi|ti),where T is the set of tag sequences allowed by thetype and token constraints.
The probability of allother tag sequences are assumed to be 0.The features in our model are similar to thoseused by T?ackstr?om et al (2013), including fea-tures based on word and tag identity, suffixesup to length 3, punctuation and word clusters.Word clusters are obtained by clustering frequentwords into 256 clusters with the Exchange al-gorithm (Uszkoreit and Brants, 2008), using thedata and methodology detailed in T?ackstr?om et al(2012).4.3 Wsabie modelWe propose a discriminative model based on Ws-abie (Weston et al, 2011), a shallow neural net-work that learns to optimize precision at the topof a ranked list of labels.
In our application, thegoal is to learn to rank the set of tags allowed bythe projected constraints in the training data aboveall other tags.
In contrast to the HMM, which per-forms inference over the entire sequence, Wsabiemakes the predictions at each token independently,based on a large context-size.
Therefore, Wsabieinference is linear in the number of tags, while foran HMM it is quadratic, making the Wsabie modelmuch faster during training and decoding.Wsabie maps the input features and output la-bels into a low-dimensional joint space.
The inputvector x for a wordw consists of the concatenationof word embeddings and sparse features extractedfrom w and the surrounding context.
A mapping?I(x) = V xmaps x ?
Rdinto RD, with matrix V ?
RD?dof parameters.
The output tag t is mapped into thesame space by?O(t) = Wt,where W ?
RD?Lis a matrix of output tag em-beddings andWtselects the column embedding oftag t. The model score for tag t given input tokenwith feature vector x is the dot productft(x) = ?O(t)T?I(x),where the tags are ranked by the magnitude offt(x).
The norms of the columns of V and W areconstrained, which acts as a regularizer.The loss function is a margin-based hinge lossbased on the rank of a tag given by ft(x).
Therank is estimated by sampling an incorrect tag uni-formly with replacement until the sampled tag vi-olates the margin with a correct tag.
Training isperformed with stochastic gradient descent by per-forming a gradient step against the violating tag.The word embedding features for the Wsabiemodels consist of 64-dimensional word vectorsof the 5 words on either side of a token and ofthe token itself.
The embeddings are trained withword2vec (Mikolov et al, 2013) on large corporaof newswire text.Sparse features are based on prefixes and suf-fixes up to length 3 as well as word cluster fea-tures for a window size 3 around the token, usingthe clusters described in the previous section.5 ExperimentsWe evaluate our model in two settings.
The firstevaluation measures the accuracy of the cross-lingual taggers on language pairs where annotateddata is available for both languages.
The annotatedtarget language data is used only during evaluationand not for training.
Second, we perform a down-stream evaluation by including the morphologicalattributes predicted by the tagger as features in adependency parser to guage the effectiveness ofour approach in a setting where one does not haveaccess to gold morphological annotations.5.1 Experimental setupAs source of parallel training data we use Eu-roparl2(Koehn, 2005) version 7.
Sentences are to-kenized but not lower-cased, and sentences longerthan 80 words are excluded.
In our experimentswe learn taggers for a set of 11 European lan-guages that have both UD training data with mor-phological features, and parallel data in Europarl:Bulgarian, Czech, Danish, Dutch, Finnish, Ital-ian, Polish, Portuguese, Slovene, Spanish andSwedish.
We train cross-lingual models in two se-tups: The first uses English as source language; inthe second we train models with different sourcelanguages for each target language.Word alignments over the parallel data are ob-tained using FastAlign (Dyer et al, 2013).
High-2http://www.statmt.org/europarl/1957confidence bidirectional word alignments are con-structed by intersecting the alignments in the twodirections and including alignment points only ifthe posterior probabilities in both directions areabove the alignment threshold ?.
For each lan-guage pair all the word-aligned parallel data avail-able (between 10 and 50 million target-side tokensper language) are used to extract the type con-straints, and the models are trained on a subset of2 million target-side tokens (optionally with theirtoken constraints).The number of distinct attribute-value pairs ap-pearing in the tagsets depends on the languagepair and ranges between 35 and 79, with 54 onaverage (including POS tags).
The number ofdistinct composite morphological tags is 423 onaverage, with a much larger range, between 81and 1483.
The English UD data has 116 tagscomposed out of 51 distinct attribute-value pairs.Therefore, we can project a reasonable numberof morpho-syntactic attributes from English, al-though the number of attribute combinations thatoccur in the data is less than for morphologicallyricher languages.The source text is tagged with supervised tag-gers, trained with Wsabie on the UD trainingdata for each of the source languages used.
Foreach language pair, we train a distinct source-sidemodel covering only the attribute types appearingin both languages.
This is meant to obtain a max-imally accurate source-side tagger, while accept-ing that our approach cannot predict target-side at-tributes that are absent from the source language.The average accuracy of the English taggers on theUD test data is 94.96%.
The source-side taggersover all the language pairs we experiment on havean average accuracy of 95.75%, with a minimumof 89.14% and a maximum of 98.59%.5.2 TuningThe hyperparameters of the Wsabie taggers aretuned on the English development set, and thesame parameters are used for the Wsabie target-side models trained on the projected tags.
The op-timal setting is a learning rate of 0.01, embeddingdimension size D = 50, margin 0.1, and 25 train-ing iterations.Hyperparameters for the projection models areset by tuning on the UD dev set accuracy for En-glish to Danish.
English was chosen as it is thelanguage with the most available data and the mostlikely to be used when projecting to other lan-guages; Danish simply because its corpus size istypical of the larger languages in Europarl.
Usinga small grid search, we choose the parameters thatgive the best average accuracy across all four pro-jection model instances we consider.
This allowsusing the same hyperparameters for all these mod-els, an important factor in making them compa-rable in the evaluation, since the hyperparametersdetermine the effective training data.
The parame-ters tuned in this manner are the alignment thresh-old ?, which is set to 0.8, and the type distributionthreshold ?, set to 0.3.5.3 Tagging evaluation setupIn order to evaluate the induced taggers on the an-notated UD data for the target languages, we de-fine two settings that circumvent mismatches be-tween source and target language annotations todifferent degrees.The STANDARD setting involves first makingminor corrections to certain predicted POS valuesto account for inconsistencies in the original anno-tated data.
When predicted by the model, the POStag values absent from the target language trainingcorpus are deterministically mapped to the most-related value present in the target language in thefollowing way: PROPN to NOUN; SYM and INTJto X; SYM and X to PUNCT.
Besides POS, the eval-uation considers only those attribute types that ap-pear in both languages?
training corpora, i.e., theset of attributes for which the model was trained.Note that this leaves cases intact where the modelpredicts certain attribute values that appear only inone of the two languages; it is thus penalised formaking mistakes on values that it cannot learn un-der our projection approach.The second evaluation setting, INTERSECTED,relaxes the latter aspect: it only considersattribute-value pairs appearing in the training cor-pora of both languages.
The motivation for this isto get a better measurement of the accuracy of ourmethod, assuming that the tagsets are consistent.In both settings we report macro-averaged F1scores over all the considered attribute types.
Re-sults for Wsabie are averaged over 3 randomrestarts because it uses stochastic optimizationduring training.5.4 Tagging results projecting from EnglishFollowing previous work on projecting POS tagsand the assumption that it is easier to obtain paral-1958Model STANDARD INTERSECTED POSHMM projected type 53.86 (-) 58.67 (-) 79.45 (-)HMM projected type and token 48.49 (-) 52.40 (-) 73.61 (-)unambiguous type 51.72 (0.33) 56.22 (0.36) 79.58 (0.22)projected type 53.60 (0.16) 58.11 (0.18) 80.09 (0.12)projected type and token 53.36 (0.19) 57.77 (0.21) 79.94 (0.11)supervised 1K 62.44 (1.52) 61.74 (1.55) 72.51 (0.82)supervised type 75.55 (1.88) 74.72 (1.95) 75.91 (1.16)Table 1: Cross-lingual morphological tagging from English: Macro F1 scores averaged across 11 lan-guages.
All the results except for the first two rows are for Wsabie models.
The standard deviation over3 runs is given in brackets.lel data between a low-resource language and En-glish than with another language, we start by train-ing cross-lingual taggers using English as sourcelanguage.The overall tagging results are given in Table 1.In addition to evaluating the morphological tags inthe two settings described above, we also reportaccuracies for POS tags only, projected jointlywith the morphological attributes.We find that for both the HMM and Wsabiemodels the performance with type and token con-straints is worse than when only using type con-straints.
T?ackstr?om et al (2013) similarly foundthat for HMMs for POS projection, models withjoint constraints do not perform better than thoseusing only type constraints.
They postulated thatthis is due to the type dictionaries having thesame biases as token projections, and therefore themodel with joint constraints not being able to filterout systematic errors in the projections.For both sets of constraints the performance ofthe Wsabie model is close to that of the corre-sponding HMM, despite the Wsabie model havinga linear runtime against the quadratic runtime ofthe HMM.As another baseline we train a Wsabie model onunambiguous type constraints, i.e., we only extracttraining examples for words which only have asingle tag in the tag dictionary.
Including ambigu-ous type constraints gives an average improvementof 2.2%.As a target ceiling on performance we train aWsabie model with supervised type constraints.This model uses type constraints based on an or-acle morphological tag dictionary extracted fromthe gold training data of the target language.
Itis trained on the same training data as the pro-jected models (without token constraints).
Themodel scores higher on STANDARD than on IN-TERSECTED, as it has access to annotations for thefull set of tags used in the target language, not justthe restricted set that can be projected.
This oracleperforms on average 17% better than the projectedtype constraints model on INTERSECTED.
There-fore, despite the promising results of our approach,there is still a considerable amount of noise in thetype constraints extracted from the aligned data.We also compare the performance of the modelto that of a supervised model trained on a small an-notated corpus.
Average performance when train-ing on 1000 annotated tokens is only a few pointshigher than that of the best projected model for IN-TERSECTED.
Given that is it expensive to let an-notators learn to annotate a large set of attributes,even for a small corpus, it shows that our modelcan bring considerable benefits in practice to thedevelopment of NLP models for low-resource lan-guages.
It is possible to obtain further improve-ments in performance by learning jointly from asmall annotated dataset and parallel data (Duonget al, 2014), but we leave that for future work.The results when evaluating only the POS tagsfollow the same pattern, except that the overalllevel of accuracy is much higher than when con-sidering all morphological attributes.
For POS,the models with projected constraints actually per-form better than those with supervised type con-straints.
In this case the benefits from learningconstraints from a larger set of word types seemto outweigh the noise in the projections.
The pro-jected models are also more accurate than the su-pervised model trained on 1000 tokens.5.5 Multilingual tagging resultsResults for cross-lingual experiments on all pairsof the target languages under consideration are1959bg cs da es fi it nl pl pt sl sv Avg.en 46.7 49.7 58.0 55.7 54.0 59.6 64.1 45.0 57.8 51.0 47.9 53.6bg - 58.3 59.2 51.2 52.6 43.2 38.7 52.8 41.1 49.2 53.6 50.0cs 55.2 - 54.5 42.3 48.4 51.3 45.0 56.8 33.6 67.5 53.2 50.8da 61.9 61.6 - 41.8 49.1 45.5 49.6 53.7 44.0 49.3 72.1 52.9es 54.3 58.8 41.3 - 53.0 74.4 52.1 52.2 69.2 53.8 46.9 55.6fi 46.6 48.7 45.3 39.5 - 50.9 36.8 37.4 30.1 55.5 57.8 44.9it 43.6 59.4 44.0 74.0 53.3 - 54.3 46.5 69.2 55.9 47.0 54.7nl 44.7 59.5 56.2 54.8 54.0 60.3 - 55.9 58.6 48.6 51.6 54.4pl 52.7 58.6 46.3 37.5 42.1 47.9 42.1 - 40.7 56.0 42.6 46.6pt 45.4 45.0 49.6 66.2 42.6 69.5 50.1 43.5 - 47.8 43.9 50.3sl 46.6 60.7 35.2 40.9 49.2 49.8 36.0 54.1 35.0 - 40.4 44.8sv 50.1 54.6 70.7 47.7 57.2 49.7 46.9 41.6 46.3 43.5 - 50.8Avg 49.8 55.9 50.9 50.1 50.5 54.7 46.9 49.0 47.8 52.6 50.6Table 2: Cross-lingual morphological tagging results (STANDARD F1 scores) per source and target lan-guage, Wsabie projected model with type constraints.
Rows indicate source language and columns targetlanguage.given in Table 2, using the STANDARD evaluationsetup.
We make use of Wsabie for these exper-iments, as it is a more efficient model, which isespecially significant when training models withlarge tagsets.We see that there is large variance in themorphological tagging accuracies across languagepairs.
In most cases the source language for whichwe learn the most accurate model for morpholog-ical tagging on the target language is a relatedlanguage.
The Romance languages we consider(Spanish, Italian and Portuguese) seem to trans-fer particularly well across each other.
Swedishand Danish also transfer well to each other, whileEnglish transfers best to Dutch, which the formeris most closely related to among the languagescompared here.
However, there are also somecases of unrelated source languages performingbest: Using Danish as source language gives thehighest performing models for both Bulgarian andCzech.
When comparing these results, however,one should keep in mind that the attribute typesets used to train taggers from different source lan-guages for the same target language is not alwaysthe same (due to our definition of the STANDARDevaluation), therefore these results should not beinterpreted directly as indicating which source lan-guage gives the best target language performanceon a particular tagset.We compare the results of the STANDARD andINTERSECTED evaluations, both when using En-glish as source language, and when using thesource language which gives the highest accuracyon STANDARD for each target language (Table 3).We see that the gap in performance between thetwo evaluations tends to be larger when project-STANDARD INTERSECTEDen- best- en- best-bg 46.7 61.88 51.6 64.97cs 49.7 61.57 55.7 63.97da 58.0 70.74 65.4 73.14es 55.7 74.01 60.7 74.62fi 54.0 57.23 59.1 59.11it 59.6 74.42 66.1 75.32nl 64.1 64.12 64.7 64.66pl 45.0 56.83 47.3 60.39pt 57.8 69.22 60.2 73.10sl 51.0 67.48 53.4 69.86sv 47.9 72.07 55.1 74.60Table 3: Comparison of the performance of themost accurate cross-lingual taggers for each targetlanguage, compared to having English as sourcelanguage.ing from English than when projecting from thesource language which performs best for each tar-get language.One of the main causes of variation in perfor-mance is annotation differences.
Languages thatare morphologically rich tend to have lower per-formance, but we also see variation between simi-lar languages: There is a 10% performance gap be-tween Danish and Swedish when projecting fromEnglish, even though they are closely related.We also investigate the effect of the choice ofsource language on the accuracy of the projectedPOS tags (Table 4).
Again, we compare the per-formance with English as source (which is stan-dard for previous work on POS projection) to thatof the best source language for each target.
Al-though the gap in performance is smaller than for1960Target en- best-bg 81.84 81.84 (en)cs 80.41 86.29 (sl)da 80.69 84.85 (sv)es 86.02 89.04 (it)fi 77.07 77.48 (cs)it 83.46 86.91 (es)nl 73.05 76.02 (da)pl 79.38 82.66 (cs)pt 84.30 87.98 (es)sl 74.71 83.21 (cs)sv 80.37 86.47 (da)Table 4: Wsabie projected model with type con-straints, POS accuracy with English and the bestlanguage for each target as source.the full evaluation, we see that for most target lan-guages we can still do better by projecting from alanguage other than English.Detailed per attribute results for the STANDARDevaluation are given in Table 5, again comparingthe results of projecting from English to that ofthe most accurate model for each target language.We see that there are large differences in accuracyacross attributes and across languages.
In somecases, the transfer is unsuccessful.
For example,degree accuracy in Italian is 2% F1 when project-ing from English and 14% F1 projecting from Por-tuguese.
Some of the cases can be explained bydifferences in where an attribute is marked: Forexample, for definiteness the performance is 1%from English to Bulgarian, as Bulgarian marksdefiniteness on nouns and adjectives rather than ondeterminers.
Other attributes are very language-dependent.
Gender transfers well between Ro-mance languages, but poorly when transferringfrom English.5.6 Parsing evaluationTo evaluate the effect of our models on a down-stream task, we apply the cross-lingual taggers in-duced using English as source language to depen-dency parsing.
This is applicable to a scenariowhere a language might have a corpus annotatedwith dependency trees and universal POS, but notmorphological attributes.
We want to determinehow much of the performance gain from featuresbased on supervised morphological tags we can re-cover with the tags predicted by our model.As baseline we use a reimplementation ofno morph projected type supervisedbg 79.14 78.99 79.62cs 76.88 77.25 79.03da 69.73 70.04 71.51es 77.66 78.08 78.64fi 61.78 62.68 70.42it 81.51 81.49 82.24nl 64.76 65.80 65.92pl 70.83 71.89 74.03pt 75.92 76.71 77.98sl 77.17 77.46 79.25sv 72.92 74.09 74.58Avg.
73.48 74.04 75.75Table 6: Dependency parsing results (LAS) withno, projected and supervised morphological tags.Zhang and Nivre (2011), an arc-eager transition-based dependency parser with a rich feature-set,with beam-size 8, trained for 10 epochs with astructured perceptron.
We assume that universalPOS tags are available, using a supervised SVMPOS tagger for training and evaluation.To include the morphology, we add featuresbased on the predicted tags of the word on top ofthe stack and the first two words on the buffer.Parsing results are given in Table 6.
We reportlabelled attachment scores (LAS) for the baselinewith no morphological tags, the model with fea-tures predicted by Wsabie with projected type con-straints, and the model with features predicted bythe supervised morphological tagger.We obtain improvements in parsing accuraciesfor all languages except Bulgarian when addingthe induced morphological tags.
Using the pro-jected tags as features recovers 24.67% (0.6 LASabsolute) of the average gain that supervised mor-phology features delivers over the baseline parser.The parser with features from the supervised tag-ger trained on 1000 tokens obtains 73.63 LAS onaverage.
This improvement of +0.15 LAS overthe baseline versus the +0.6 of our method showsthat the tags predicted by our projected models aremore useful as features than those predicted by asmall supervised model.To investigate the effect of source languagechoice for the projected models in this evalua-tion, we trained a model for Swedish using Dan-ish as source language.
The parsing performanceis insignificantly different from using English assource, despite the accuracy of the tags projected1961Target bg cs da es fi it nl pl pt sl svSource en da en it en sv en it en sv en pt en en en nl en it en cs en daCase 40 62 2 - 62 18 4 - 5 26 - - 16 16 4 4 50 - 2 68 10 14Definite 1 68 - - 0 64 97 97 - - 89 91 89 89 - - 93 93 2 - 19 66Degree 67 63 69 2 72 77 5 26 50 47 2 14 56 56 57 47 2 18 63 74 70 81Gender 1 6 2 46 7 78 0 85 - - 2 80 0 0 3 0 1 77 2 61 7 81Mood 61 66 55 83 81 94 72 80 69 79 76 83 69 69 58 63 74 75 68 91 73 94Number 69 71 67 75 60 82 54 92 67 68 57 90 78 78 69 63 62 75 68 91 64 94NumType 64 62 91 86 84 - 82 85 86 - 89 66 78 78 - - 63 65 86 73 - -Person 54 28 63 68 56 - 58 79 51 - 57 80 82 82 55 59 61 74 67 91 - -Poss 76 77 90 84 97 98 94 93 - - 87 88 64 64 - - 96 98 67 62 99 97PronType 72 71 41 38 46 2 82 74 42 0 76 71 81 81 38 50 81 79 43 73 0 3Reflex 0 85 0 0 62 - 0 0 61 - 0 0 60 60 0 97 0 0 - - - -Tense 60 63 68 70 81 85 69 81 67 77 75 75 74 74 66 66 64 72 62 74 65 86VerbForm 43 49 59 78 75 79 79 81 64 65 82 81 78 78 56 66 79 72 59 74 73 86Voice 9 75 9 - 6 89 - - 10 76 - - - - 55 - - - - - 15 90Table 5: Cross-lingual tagging results (F1 scores) per language and per attribute (not showing POS anda small number of attribute types that only appear with 1 or 2 language pairs), for Wsabie projected withtype constraints.
English and best source language.from Danish being higher.Faruqui et al (2016) show that features frominduced morpho-syntactic lexicons can also im-prove dependency parsing accuracy.
However,their method relies on having a seed lexicon of1000 annotated word types, while our methoddoes not require any morphological annotations inthe target language.6 Future WorkA big challenge in cross-lingual morphology isthat of relatedness between source and target lan-guages.
Although we evaluate our models on mul-tiple source-target language pairs, more work is re-quired to investigate strategies for choosing whichsource language to use for a low-resource targetlanguage.
A related direction is to constructingmodels from multiple source languages, as our re-sults show that the overall best-performing sourcelanguage for a given target language may not al-ways have the best performance on all attributes.Another direction is to make use of dictionar-ies such as Wiktionary to obtain type constraints,similar to previous work on weakly-supervisedPOS tagging (Li et al, 2012; T?ackstr?om et al,2013).
Sylak-Glassman et al (2015b) and Sylak-Glassman et al (2015a) proposed a morphologicalschema and method to extract annotations in thatschema from Wiktionary.
Although different fromthe schema used in this paper, their method can beused to extract type dictionaries for morphologicaltags that can be used to complement constraintsextracted from parallel data.Finally, greater use can be made of syntactic in-formation: There is a close relation between thesyntactic structure expressed in dependency parsesand inflections in morphologically rich languages;by including this syntactic structure in our modelswe can induce morphological tags, e.g.
related tocase, that is also expressed in dependency parses.7 ConclusionIn this paper we proposed a method that cansuccessfully induce morphological taggers forresource-scarce languages using tags projectedacross bitext.
It relies on access to a morpho-logical tagger for a source-language and a moder-ate amount of bitext.
The method obtains strongperformance on a range of language pairs.
Weshowed that downstream tasks such as dependencyparsing can be improved by using the predictionsfrom the tagger as features.
Our results pro-vide a strong baseline for future work in weakly-supervised morphological tagging.AcknowledgmentsThis research was primarily performed while thefirst author was an intern at Google Inc. Wethank Oscar T?ackstr?om, Kuzman Ganchev, BerndBohnet and Ryan McDonald for valuable assis-tance and discussions about this work.ReferencesMalin Ahlberg, Markus Forsberg, and Mans Hulden.2015.
Paradigm classification in supervised learningof morphology.
In Proceedings of NAACL, pages1024?1029.1962Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?ot?e,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Proceedings ofNAACL, pages 582?590.Lauent Besacier, Ettiene Barnard, Alexey Karpov, andTanja Schultz.
2014.
Automatic speech recognitionfor under-resourced languages: A survey.
SpeechCommunication, 56:85?100.Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing, 4(1).Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-basedprojections.
In Proceedings of ACL, pages 600?609.Marie-Catherine de Marneffe, Timothy Dozat, Na-talia Silveira, Katri Haverinen, Filip Ginter, JoakimNivre, and Christopher D. Manning.
2014.
Uni-versal dependencies: A cross-linguistic typology.
InProceedings of LREC.Long Duong, Trevor Cohn, Karin Verspoor, StevenBird, and Paul Cook.
2014.
What can we get from1000 tokens?
A case study of multilingual pos tag-ging for resource-poor languages.
In Proceedings ofEMNLP, pages 886?897.Greg Durrett and John DeNero.
2013.
Supervisedlearning of complete morphological paradigms.
InProceedings of NAACL, pages 1185?1195.Chris Dyer, Victor Chahuneau, and Noah A Smith.2013.
A simple, fast, and effective reparameteri-zation of IBM Model 2.
In Proceeding of NAACL,pages 682?686.Manaal Faruqui, Ryan McDonald, and Radu Soricut.2016.
Morpho-syntactic lexicon generation usinggraph-based semi-supervised learning.
Transac-tions of the Association for Computational Linguis-tics, 4:1?16.Victoria Fossum and Steven Abney.
2005.
Automati-cally inducing a part-of-speech tagger by projectingfrom multiple source languages across aligned cor-pora.
In Proceedings of IJCNLP, pages 862?873.Kuzman Ganchev and Dipanjan Das.
2013.
Cross-lingual discriminative learning of sequence modelswith posterior regularization.
In Proceedings ofEMNLP, pages 1996?2006.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, et al 2009.
The CoNLL-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of CoNLL:Shared Task, pages 1?18.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT summit, vol-ume 5, pages 79?86.Shen Li, Jo?ao Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In Proceedingsof EMNLP-CoNLL, pages 1389?1398, July.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in neural information processingsystems, pages 3111?3119.Thomas M?uller and Hinrich Schuetze.
2015.
Robustmorphological tagging with word representations.In Proceedings of NAACL, pages 526?536, Denver,Colorado, May?June.Joakim Nivre,?Zeljko Agi?c, Maria Jesus Aranzabe,Masayuki Asahara, Aitziber Atutxa, Miguel Balles-teros, John Bauer, Kepa Bengoetxea, Riyaz Ah-mad Bhat, Cristina Bosco, Sam Bowman, GiuseppeG.
A. Celano, Miriam Connor, Marie-Catherinede Marneffe, Arantza Diaz de Ilarraza, Kaja Do-brovoljc, Timothy Dozat, Toma?z Erjavec, Rich?ardFarkas, Jennifer Foster, Daniel Galbraith, FilipGinter, Iakes Goenaga, Koldo Gojenola, YoavGoldberg, Berta Gonzales, Bruno Guillaume, JanHaji?c, Dag Haug, Radu Ion, Elena Irimia, An-ders Johannsen, Hiroshi Kanayama, Jenna Kan-erva, Simon Krek, Veronika Laippala, Alessan-dro Lenci, Nikola Ljube?si?c, Teresa Lynn, Christo-pher Manning, C?at?alina M?ar?anduc, David Mare?cek,H?ector Mart?
?nez Alonso, Jan Ma?sek, Yuji Mat-sumoto, Ryan McDonald, Anna Missil?a, VerginicaMititelu, Yusuke Miyao, Simonetta Montemagni,Shunsuke Mori, Hanna Nurmi, Petya Osenova, Lilja?vrelid, Elena Pascual, Marco Passarotti, Cenel-Augusto Perez, Slav Petrov, Jussi Piitulainen, Bar-bara Plank, Martin Popel, Prokopis Prokopidis,Sampo Pyysalo, Loganathan Ramasamy, RudolfRosa, Shadi Saleh, Sebastian Schuster, WolfgangSeeker, Mojgan Seraji, Natalia Silveira, Maria Simi,Radu Simionescu, Katalin Simk?o, Kiril Simov,Aaron Smith, Jan?St?ep?anek, Alane Suhr, ZsoltSz?ant?o, Takaaki Tanaka, Reut Tsarfaty, Sumire Ue-matsu, Larraitz Uria, Viktor Varga, Veronika Vincze,Zden?ek?Zabokrtsk?y, Daniel Zeman, and HanzhiZhu.
2015.
Universal dependencies 1.2.
LIN-DAT/CLARIN digital library at Institute of For-mal and Applied Linguistics, Charles University inPrague.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In Proceedings ofLREC.Benjamin Snyder and Regina Barzilay.
2008.
Un-supervised multilingual learning for morphologicalsegmentation.
In Proceedings of ACL, pages 737?745.John Sylak-Glassman, Christo Kirov, Matt Post, RogerQue, and David Yarowsky.
2015a.
A universal fea-ture schema for rich morphological annotation andfine-grained cross-lingual part-of-speech tagging.
In1963Proceedings of Systems and Frameworks for Com-putational Morphology: Fourth International Work-shop, pages 72?93.
Springer International Publish-ing, Cham.John Sylak-Glassman, Christo Kirov, David Yarowsky,and Roger Que.
2015b.
A language-independentfeature schema for inflectional morphology.
In Pro-ceedings of ACL-IJCNLP (short papers), pages 674?680.Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszko-reit.
2012.
Cross-lingual word clusters for directtransfer of linguistic structure.
In Proceedings ofNAACL, pages 477?487.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
Transactions of the Association for Computa-tional Linguistics, 1:1?12.Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.2008.
Applying morphology generation models tomachine translation.
In Proceedings of ACL-HLT,pages 558?566.Reut Tsarfaty, Djam?e Seddah, Yoav Goldberg, San-dra K?ubler, Marie Candito, Jennifer Foster, Yan-nick Versley, Ines Rehbein, and Lamia Tounsi.2010.
Statistical parsing of morphologically richlanguages (SPMRL): what, how and whither.
InProceedings of the NAACL HLT 2010 First Work-shop on Statistical Parsing of Morphologically-RichLanguages, pages 1?12.Jakob Uszkoreit and Thorsten Brants.
2008.
Dis-tributed word clustering for large scale class-basedlanguage modeling in machine translation.
In Pro-ceedings of ACL-HLT, pages 755?762.Jason Weston, Samy Bengio, and Nicolas Usunier.2011.
Wsabie: Scaling up to large vocabulary im-age annotation.
In Proceedings of the InternationalJoint Conference on Artificial Intelligence (IJCAI).Guillaume Wisniewski, Nicolas Pcheux, SouhirGahbiche-Braham, and Franois Yvon.
2014.
Cross-lingual part-of-speech tagging through ambiguouslearning.
In Proceedings of EMNLP, pages 1779?1785.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Incuding multilingual text analysistools via robust projection across aligned corpora.In Proceedings of HLT.Daniel Zeman.
2008.
Reusable tagset conversion us-ing tagset drivers.
In Proceedings of LREC.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of ACL-HLT, pages 188?193.1964
