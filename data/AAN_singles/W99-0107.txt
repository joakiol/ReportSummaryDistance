Building a Tool for Annotating Reference in DiscourseJonathan DeCristofaroCIS DepartmentUniversity of Delaware103 Smith HallNewark, DE 19716, USAdecris~ogcis, udel.
eduMichael StrubeIRCSUniversity of Pennsylvania3401 Walnut Street, Suite 400APhiladelphia, PA 19104, USAstrubeOl inc. cis.
upenn, eduKathleen F. McCoyCIS Department'University of Delaware103 Smith HallNewark, DE 19716, USAmccoySc is.
udel.
eduAbstractWe discuss the development of a system formarking several types of reference tofacilitatethe analysis of reference in discourse.
The toolis designed to be used in three applicationsigenerating training data for machine learningof co-reference r lations, evaluating iheoriesof referring expression generation and resolu-tion in texts, and developing theories for un-derstanding reference indialogs.
The need tomark any of a broad set of relations which mayspan several levels of discourse structure drivesthe system architecture.
The system has theabilityto collect statistics over encoded rela-tions and meastwe inter-coder reliability, andincludes tools to increase the accuracy of theuser's markings by highlighting the di.u:rep-ancies between two sets of markings.
Usingparsed corpora s the input further reduces thehuman workload and increases reliability.1 Mo~a~nTo examine the phenomenon freference in discourse,and to analyze how discourse structure and reference in-teract, we need atool,which allows several kinds of func-tionality including mark-up, visualization, and evalua-tion.
Before desitming slach atool, we must ~ y  an-alyze the kinds of information each application requires.Three applications have driven the design of the sys-tem.
These are: 1) the creation of training data for auto-matic derivation of reference r solution algorithms (/.e.,machine l arning), 2) the formation ofa testhed for eval-uating proposed reference g neration and anaphera eso-lution theories, and 3) the development oftheories aboutunderstanding reference in dialog.
The influence thatthese three areas have upon the functional requirementsof an annotation system are discussed below.In this paper we fn~t describe the requirements thateach of these three related applications demand froma discourse annotation tool geared to aid in answeringquestions concerning reference.
We next discuss omeof the theoretical implications and decisions concerningthe tool development that have arisen from these require-ments.
Next we describe the tool itself.
F'mally, wediscuss related work, future directions of this work, andsome conclusions.1.1 Machine LearningConsider a learning task in which we will present thelearner with a sequence oftriples of the form (E, F, U),where:?
E is a pair of text expressions EA and EB,, F is vector of features describing the expressions,and?
U is the classification: +ff EA and Es co-refer,.otherwise.
(Two expressions co-refer when ~ey denote the samediscourse ntity (DE).)
A successful learner will outputa model which, when given only (E, F), can predict thevalue of G'.
that is, classify the instance, as positive ornegative.
We intend to use the annotation tool to producea set of such instances and features.The first requirement for a tool which would help usgenerate such a body of data is that it must allow us tomark all the potential referring expressions.
This simplymeans that he us~ will have the ability to delineate anyspan of text which represents a DE, and Ireat~that span asa single entity.
Of course, this is a time-consuming andermr-proue process and thus it is helpful to automate asmuch as possible.
In the training phase, the learner mustbe given all the potential antecedents for an anaphoricreference, so that it will know how to distinguish theproper antecedent from all the other candidates.
For thetesting phase, the correct antecedent's span must he in-cluded as a marked entity in the corptis, or the learner hasno chance of getting that instance of co-reference right.The other crucial function of an annotation tool is tolet the user associate attributes, or f~h~re values, with the5400O00000000000000000O0000,000O000000000O000eoe@e@oe@o@@ooQ@o@e@e@ooo@@@@@@e@eeo@/ )@es @eemarked expression.
During the training phase, a learningalgorithm is trying to find correlations between the fea-tures F and the classification 6'.
Choosing the set offeatures to include in the learning phase is a very diffi-cult task.
The set must be sufficiently rich so as to in-clude all of those features which might affect a refer-ring expression's resolution.
On the other hand, sincethe learner will likely find that only certain features pre-dict co-reference, we do not want to burden the learnerwith many useless features that will bog it down withcomputational complexity.
Also, a less restricted set offeatures permits more oppot:tunity for inconsistency in agiven coder's markings and disagreement among coders(Condon & Cech, 1995).We cannot know (before training) exactly which fea-tures are most predictive of co-reference.
So.
we willtry to mark a set of features which is a superset of thenecessary features.
Drawing on the feature sets used inConnolly et al (1997) and Ge et al (1998), we believethe following factors might indicate co-referenco:?
Syntactic role (e.g.
Subject, Object, PrepositionalObject,...),?
Pronominalization (yea or no),?
Distance between EA and Ee (an integer),?
Definiteness (yes or no),?
Semantic role (e.g.
indicating location, manner,time,...),?
Nesting depth of an N'P (an integer),?
Information status (as defined by Strube (1998)) ofthe DE,?
Gender, Number, Animacy.The tool must allow the coder to assign values for thesefeatures to each marked expression, but should not de-mand that every expression has a value assigned for ev-ery feature.Since we cannot claim that this set Of features i  ex-haustive, the tool must allow further features to be addedby the user.
Since reliability of feature assignment isimportant, he tool should have the abifity to extract asmany features aspossible automatically (for example,from a parsed corpus).
In addition since some featuresmust be hand-marked, the tool must have the ability tocompare feature marking between two coders for thesame teXL1.2 Evaluating Anaphora Generation andResolution MgorithmqOur discourse annotation and visualization tool also ful-fills the role of a testhed in which we can examine the-oriea of generating and resolving anaphoric expressions.From the generation perspective., we look for answers toquestions concerning when it isappropriate to generate apronoun versus ome other anaphoric expression (e.g.,a definite description or name; see McCoy & Strube(199%), McCoy & Strube (i 999b))~Some researchers have looked at the question of whento generate a pronoun (versus ome other description)(e.g., McDonald (1980), McKeown (1983), McKeown(1985), Appelt (1981)).
In this work the decision wasbased on a notion of focus of attention (Sidner, 1979) -if an entity was the focus of the previoussentence andis the focus of the current sentence, then use a pronoun.To evaluate such claims, not only must co-reference r -latious be marked in a text, but information concerningfocusing data structures must be kept.Dale (1992) discussed the generation of pronouns inthe context of work on generating referring expressions(Appelt, 1985; Reiter, 1990).
Dale suggests the princi-plea of efficiency and adequacy which favor generatingthe smallest referring expression that distinguishes theobject in question from all others in the context.
Thisnotion was somewhat ltered in Dale & Reiter (1995) tomore adequately reflect human-generated r ferring ex-?
pressious and to be more computationally tractable.Other esearchers have suggested that a notion of dis- ~-~course structure must be taken into account when gener-ating referring expressions.
In particular, Gresz & Sid-her (1986) and Reichman (1985) both suggest that a fullnoun phrase might be generated at discourse segmentboundaries when a pronoun might have been adequate(in Dale's sense).
Passonnean (1996b)argues for the useof the principles of information adequacy and economy.Her algorithm takes discourse segmentation into accountthrough the use of focus paces which am associated withdiscourse segments.
Passonneau rgues that, a fuller de-scription might be used at a bounda~ because the set ofaccessible objects changes at discourse segment bound-aries.Passonneau's work suggests additional features whichmust be marked in a text o evaluate referring expressiongeneration algorithms.
These include discourse segmentboundaries and sets of "confnsable" DE's contained in?
the focus space, Thus the definition of what constitutes adiscourse segment is another item which is open to re-search; our tool should allow for alternative markingsof discourse segments so that various algorithins can beevaluated.
For example, in our current work we look atchanges in time as segment boundaries.
Other definitions -?
are possible.
So, the tool must be able to keep informa-tion for various alternative algorithms.While it is intuitively appealing that notions of dis-course segmentation affect pronoun generation, theabove work fails to identify how a discourse segmentshould he defined to a generation algorithm - thus it isnot clear how this work can he applied to the generationprocess.Given this previous work, we need a tool that will al-55low us to specify (!)
alternative definitions of discoursesegmentation, and (2) alternative algorithms for pronounversus definite description generation (and anaphora res-olution).
The tool must have the ability to then calculatestatistics o that the alternative definitions and algorithmscan be compared.Thus, this application requires the ability to spec-ify co-reference relations, associate various featureswith referring expressions (both syntactic and discourse-relevant), calculate the results of certain well-specifiedalgorithms on the referring expressions, and tabulate ~eresults of such algorithms.
In addition to this informationon referring expressions themselves, the tool must allowthe marking of arbitrary features over arbitrary piecesof text (e.g., for alternative definitions of discourse seg-ments).
Because this work is exploratory in nature, thetool should allow a researcher toeasily find places wherevarious algorithms fail so that hey can be examined andthe algorithms updated as needed.1.3 Underst~mding Spoken DialogThe evaluation of algorithms for anaphora resolution inspoken dialog requires annotation of discourse structureon several levels.
This is because spoken dialog showsmore complex phenomena than written discourse.
Prob-lematic issues in spoken dialog include?
the determination fthe center of attention i  multi-party discourse;?
utterances with no discourse ntities;?
abandoned or partial utterances, interruptions,speech repa!rs;?
the determination futterance boundaries;?
the high frequency of discourse deictic and vagueanaphora (Eckert & Slmbe, 1999).In order to capture the complexity of anaphora resolu-?
tion in spoken dialog, the annotation requires amultitudeof steps.Dialog Acts.
To determine the domain ofanaphoric an-tecedent~ the dialog must be divided into short piece.We have chosen to use units based on dialog acts for thistask.
Therefore, turns have to be segmcnted into dialog~t  units.
Our study of anaphoric extnssious reveals thatin a dialog between two participants A and B, the DE'sintroduced by A are not added to the shared iscoursememory model until ,As contribution has been acknowl-edged by B.
Thus the segment isimportant for resolutionalgorithms.As in all coding schemes, intercoder reliabifity (here.of the dialog act units)must be questioned.
For the pur-pose.of applying the Kappa (g) stnfi~c the segmenta-tion task must be turned into a classification task.
So,we view boundaries between dialog acts as one class andnon-boundaries a  the other (see Passonneau & Litman(1997) for a similar practice).
The next step is to classifydialog act units as particular dialog acts.
For this task thestatistic is also appropriate.Individual and Abstract Object Anaphot~a.
Sincespoken dialog shows a high number of discourse deicticand vague anaphora, pronouns and demonstratives haveto be classified accusingly.
Thus an additional feature,anaphor type, must be marked in the corpus.Co-Indexatlon of Anaphora and Antecedents.Vague pronouns do not have a particular antecedentin the text.
Hence, they cannot be co-indexed withan antecedent.
The co-indexatiun of individual objectanaphora in spoken dialog does not differ from writtendiscourse.
However, the high number of discoursedeicfic pronouns requires a second set of markablessince discourse deictic pronouns can co-specify wi~propositions, sentences and even diseourse segments.Therefore, the reliability of the annotation depends on(1) the marking of the correct ext span and (2) whetherthe correct antecedent is linked with the pronoun.
Deter-mining the reliability of marking spans of text is difficultwhen any span can be marked, since this means almostany word boundary is a candidate segment boundary.Here, the s: statistic does not seem meaningful becauseof the huge disparity in the number of non-boundariesand boundaries.
This highly skewed istribution seemsto overwhelm s~.Thus we are exploring more appropriate measures Ofintereoder reliability on this task.
At the moment, our ap-proach to this problem is to use =, but restrict he anno-tators, so that they are allowed to mark only certain con-tiguous linguistic objects like verb phrases, entences, ora well defined segment spanning more than one turn.2 Annotat ing a Parsed  CorpusAll of the applications discussed in section I depend onhaving a corpus of reliably marked expressions, features,and relations.
In order to determine that these dimen-sions have been '*reliably marked", we need to measureagreement between two codeas marking the same text.One way to increase the relinb'dity of the coding (re-gardless of the method used to measure reliability) is toautomate part of the coding process.
Our system can ex-tract a number of markings, features and relations fromthe parsed, part-of-speech-tagged corpora of the typefound in in the Penn Treebank 2 (Marcus et al, 1994).Use of the Treebank data means we can find most of themarkables and many of the necessary features before giv-ing the task to a human coder.
We do not try to extractany of the co-reference information from the parsed cor-pora.560600000000e000Q00I0006000000000@0000000Oe000eoe@eO@@eee@e,@e@@@e@0e@@@@@@@eeeeeee@e@o@_e@2.1 Extracting MarkablesIn this context, a markable is a text span representinga discourse ntity which can be anaphoricaily referredto in a text or dialog.
The majority of markables arenoun phrases.
Because the Treebank is a fully-parsedand well-defined representation f the text, it is trivialto determine the boundaries of all of the NP's in thetext.
However, the full set of NP's found by the Tree-bank parse is too inclusive for our purposes (/.e., it is asuperset of the NP markables).
While the Treebank de-lineates all NP's at all levels of embedding, it is not thecase that each such NP contributes a distinct DE.
Con-sider the following example containing three NP's in theparsed Treebank:(I) ~ (NP different parts) 0PPof(NPEumpe)))We want to mark both "different parts of Europe" and"Europe", since they both contribute distinct DE's.
How-ever, notice that "different parts" does not contribute aDE since it is not possible to refer to this subexpressionalone in subsequent discourse.To avoid finding such undesirable NP's, our systemhas a heuristic (HI) which says: Pass overan~NP whichis a leftmost child of a top-level NP.
This heuristic is toodrastic, though, eliminating constructions like (2).
(2) (NP (NP the inner bra/n) and OqP the eym))To avoid losing these examples, we include anotherheursitic (H2) which says: HI does not apply when theNP is a sibling of another NP.
A third heuristic mustbe added to overrule HI in the case of a possessor in apossessive construction, such as:(3) (NP (NP Chicago's) South Side)where we should extract both "Chicago" and "Chicago'sSouth Side".
So, the heuristic H3 is introduced: HI doesnot apply when the NP is a posse.~sive form.Even with heuristics eliminating the NP's which wedo not need to consider, there are some NP's that willbe found by the system which cannot be eliminated an-tomadcally.
Copular consU'uctions such as (4) introduceunnecessary NP's.
(4) John is a docto~"John" and "a doctor" are syntactically NP's, but thesecond oes not contribute a unique DE.Also, idiomatic expressions such as (5) must be elim-inated by hand:(5) Ned kick~ the bucket.'
The syntactic NP"the bucket"refers to no DE and cannotbe the antecedent of any future referring expression, so itshould not be marked.At this time, we do not have a way for the expressionextracting system to detect and avoid these xamples.
Asa result, we must introduce a correction phase in which ahuman corrects the markings, eliminating those that aresuperfluous, and adjusting those that may have been mis-marked.
The goal is to have a set of expressions whichis as close as possible to the set of expressions ecessaryand sufficient for the applications.
For example, if thereare many extraneous expressions in the machine learn-ing task, they will act as distractors - examples whichdecrease the accuracy of the learned model by dilutingthe highly correlative data with noise.2.2 Extracting FeaturesIn addition to extractingmany markables themselves, theparsed corpora contain information from which many ofthe features can be automatically derived.
Some fea-tures' values are marked explicitly in the corpus whileothers can be automatically extract.~ by examining thetree structure.
The simplest source of feature values isthe Treebank "functional tags".
For example, the gram-matical functions (syntactic subject, topicalization, logi-cal subject of passives, etc~) of phrases and the semanticrole (vocative, location, manner, etc.)
are marked in thecorpus.Other features must be found by walking the treestructure provided in the Trcebank.
The form of theNP (whether the NP is realized as a personal pronoun,demonstrative pronoun, or definite description) is a func-tion of the part-of-speech tags assigned to the words inthe NP.
Whether the NP is definite, indefinite, or indeter-minable depends on whether an article begins the NP.
Ifthe article is "a', "an", or "some", we assume the NiP isindefinite.
"The" indicates definiteness; otherwise, weassign a value of"none", which simply indicates thatthere is no simple way of classifying this instance.
Thecase of an NP is usually determined by its position inthe tree.
Any child of a VP is marked as an "object".Children of PP's are marked "l~ep-adjunct" unless thePP was tagged "PP-put "t, which indicates that the PPacts as a complement tothe verb.
In this case we tag theNP as "prep-complement".2.3 Relations between ExpressionsWe allow two classea of relations to hold between mark-able entities: the co-referenca relation and an open classof aser-definable directional relations.
A co-referencerelation holds between A and B when A and B are ex-pressions which both refer to the same discourse ntity.Since co-reference is a symmetric, reflexive, and transi-the relation, it divides the set of markables into equiva-leace classes.
Within a given equivalence lass, all mem-tPP's using "in", "on".
or "around" are sometimes markedPP-put.57: - : .
-bet's refer to the same DE.
Intuitively, our co-referencerelation is a set of undirected links connecting all co-referring expressions.
The symmetric property impliesthat it is not meaningful to store the direction of a rela-tion.
However, we do store each markable's antecedentwhen the user defines aco-reference link, so that we canlater econstruct the co-reference hain if necessary.The other kind of link is directional.
We allow the userto define any number of relation.,/which are not sym-metric, reflexive, or transitive.
The  only restriction onthese relations is that they hold between exactly two en-titles.
Initially, we postulate four such relations which arenecessary to handle indirect co-reference r lations, alsocalled bridging relations (see also Passonneau (1996a)):?
Attribute.of(6) \[The car\]~ won't start because \[the ngine\]i is miss-int.
* Propositional-inference(7) \[The man llas agun.\]# \[Thatlj scares me.?
Contains(8) \[The peaches\]s are in a basket.
Give me \[thebiggestlk, o?
Member.of(9) \[Ja?k\]m algl Jill went up the hill.
\[l'hey\],,, werenever seen again.Clearly, these must be directional (/.~, not symmetric)since, for example, if Member-of(A,B), then we shouldnot assume Member-ef{B,A).
The user is not prevented,however, from defining two such links, one in each direc-tion.
In fact.
Contains and Member.of are logical du-als; that is, Contnim(a,b) ~ Member(boO.
However,we are always interested in the relation of a referring ex-pression to its potential antecedents and so require thatthe referring expression be the first argument and the an-tecedent he second.
In (8), Member-of(tim biggest, hepeaches), but in (9), Contains(They, Jack) and Con-t.h~fl~ey, Jm).
"7.4 Mmsur~A~m.eatAll of the annotation discussed in the above Sections isprone to error when a human is involved.
The best way tocombat these rrors to is have several coders annotate thesame corpus according to a coding manual.
2 A high mea-sure of agreement between these coders gives us moreconfidence in the reliability of the data.
Therefore, we2The intent is that the coders will achieve ahigh degree ofconsistency if the manual isclear, and then if the manual ccu-rately represents the desired coding style, consistency amongcoden implies accuracy of all the coding~must be able to measure agreement between two 3 cod-ings of the same text.The first kind of agreement that we need to measure isagreement of two sets of markables.
Since we expect afew of the markables found by the system to need humanediting, we may not assume that two coders working onthe same text will have the same set of markables afterthe correction phase.
We define agreement of two sets ofmarkables 81 and .5'2 as2,c.. Agreement(Sl, $2) = a + \[~where a = \]Sl\], b : IS2\], and c = the number of ex-pressions marked in .St that were marked with exactlythe same boundaries in Sa.
When agreement of mark-ables is found to be less than 1, the coders are shownthe expressions on which they disagree and can cometo agreement (by referring to the coding manual and re-marking those passages).
We are developing a functionof the tool which will simultaneously display the two ver-sions of the text and highlight he expressions which arenot common to the two codings.
This will make it eas-ier to visualize the differences between the codings andreach perfect agreement ofmarkables.The second kind of agreement measures agreementbetween two coders' co-reference odings.
We requirethat he two coders have the same set of markables beforecomparing their co-reference annotations, so achievingmarkable agreement of I is a prerequisite for this calcu-lation.
As discussed in section 2.3, the co-reference r la-tion divides the set of markables into equivalence lasses.A model-theoretic algorithm proposed by Vilain et al(1995) uses these Co-referenc e lasses to define a preci-sion and recall metric which yields intuitively plausibleresults and is easy to calculate.
The method epends oncounting how many co-refereace links must be added toone coder's equivalence lasses to wansform the set intothat found by the other c~ler.
We adopt his method andenable the tool to perform t l~ computation between anytwo codings which fully agree on the underlying set ofmarkahies.Finally.
we can measure feature-value agreement byviewing the featme assignment task as a kind of  clas-sification task and then computing Kappa (a), whichmeasures how well the coders a .g~l  compared to theirrandom eJcpected agreemeat4(CaHetta, 1996).
We con-form to the method proposed in Poesio & Vieira (1998)for computing actual and expected agreement.
(Againwe assume the coders have already agreed on the set of+ markables.)
Suppose we are considering a given featureSAgreemem among aset of n > 2 coders is usmdly calcu-lated as a function of the .
~  pail'wise agreements, sowewill discuss only the pa/rwise case here, realizing that the fullcomlmmion is straightforward.58@@@O0@@0@@@@@0Q@00@@0@0@00@@0 @0@00 g@0 @@0@0t000e00e000000000e0e0000e'e000eee00e0@eooo0f ,  which was marked by two coders on each of N ex-pressions in a corpus.
Percent agreement is Simply thefraction of expressions out of N for which the two codersassigned the same value to .f.
Expected agreement is notcomputed by assuming that each value is equally likely,though.
We compute the expected agreement based onthe actual distribution of values, as follows.
For twocoders, if f takes on values from V,=~v 2 .
N \]where c~(v, f )  is the number of times coder i assignedvalue v to feature f .
Thus, if the coders have used thevalues in a perfectly even distribution among the IV I val-ues, P(E) -- \[~.
Any distribution which is not perfectlyeven will have an expected agreement higher than this.As with measuring markable agreement, we measurefeature-value agreement toensure that we have reliablefeatures before using the data for one of the applicationsdiscussed insection 1.
Therefore, coders can ask the sys-tem to show the examples for which they disagree on aspecified feature.
Again, the coders have the opportunityto recede those examples to achieve perfect agreementbefore passing the data to the application.3 REFEREE: The Discourse AnnotationToolWe have built a discourse annotation and visualizationtool which is designed according to the issues discussedin section I and which has all the capabilities describedin section 2.
RBFEREE s is a graphical interface tool writ-ten in Tclfrk.
This makes i t highly portable and easilyextensible.3.1 Anm~ttion ModesThe tool has three "modes" - reference mode, segmentmode, and dialog mode.
In reference mode, the usercan mark expressions, associate features with any ex-pression, and assign co-reference (or other kinds of ref-erence) links.
Clicking on an expression with the mousedisplays the features of that expression and highlights allother expressions in the text which co-refer with it.
Atthis point, the user can up,t~3f the oo-reference or featureinformation or type some notes to be stored with the ex-pression.
(These notes are shown with the features when-4.n _ P (E )where P(A) is the proportion ftlmes the annotators agree andP(E) is the proportion oftimes the annotators are expected toagree by chance.5for Refening Exwession Reader/Editorever this expression isclicked on in the future.)
Easy vi-sualization of the co-reference equivalence lasses couldaid the user as he clicks through the text and sees howthe co-reference hains thread through discourse.A byproduct of the built-in flexibility of REFEREE isthe ability to use different feature "masks" in case theuser only wants to consider some subset of the completeset of marked features.
For example, the user can con-figure the tool to display and allow changes tO only thepronominalization feature.
Then, the irrelevant featuresare not displayed and cannot be changed until the tool isreconfigured.
This is also useful for associating differentfeature sets with different kinds of expressions.Segment mode allows the user to break the text intoarbitrarily nesting and overlapping segments.
(These donot have to correspond to any certain definition of dis-course segment or text segment.)
This allows the userthe freedom to choose any degree of constraints upon thestructure.
When the user selects a region and clicks onthe "mark" button, a new segment is created spanningthat region.
Thus, we can build up a list of start and endpoints of segments, and automatically determine whichsegments are contained in or overlap with which othersegments.
A separate window displays graphically thestart and end point of each segment.At first glance, this seems to replicate the functional-ity of the reference mode, since both modes allow un-constrained marking (of contiguous ~xt spans).
The im-portant difference is that in reference mode, the user de-lineates referrable ntities, while in segment mode, theuser is marking spans which represent the structure ofdiscourse.
So, a use r Could have many spans marked assegments which exactly coincide with markables in ref-erence mode; this simply represents he fact that he userbelieves it is possible for the text to refer to the segmentsor the pmpositious they express.
Still, the segment mark-ings are not superfluous.
They impose astructure on topof the reference mode markables, even if some of themcoincide.
(While this could be simulated in the refer-ence mode by adding a binary feature for segmenthood,the visualizationofsegments would be lost, as would thedecoupling of the two kinds of spans we mark.
)The last mode of interaction with RF.~ItEe is dialogmode.
This allows the user to code a dialog by breakingit into turns.
Each dialog participant's tm'ncan be bro-ken into utterances which may be labeled as initiation orresponse units.
(In some cases, there is overlap betweenthese two dialog acts.)
The most important function ofdialog mode, as it relates to understanding reference inspoken language is to allow segmentation f the dialog?
into turns assigned to one speaker or the other.
Recall the?
proper.closure of a turn is crucial for determining whichDE's are in the shared iscourse model.594 Prev ious  Work,~  ~ .,.,, to ~.
,~ .
~, .
:~ ,,,..?~.
~.
Previous systems were designed for  different purposes,- - ,  ,.
~.
,~.
~ e,.
.
.
.
.
.
.
,~  ~, ,  m ,,~ ~ and therefore do not provide all of  the functionality that ~td Iht ~?t  ~ tm dJ~ Cc, ~ t~r htw dt~rmr .
Fall I t*m" t* rc.t ?m~t~mbl* tram .
rm dmmr hmar tkww ute tml,,qt nma.
It" this ,-.,,~*~,~ =,u .
, .
--.2o ,o ~.., h,.
~ , ,~.
~.
~ our applications require.
For example, MITRE's Alem-he~ t~d l i t t le  t im In Mhjch to ~ tho ~ | e i  ~ thst m8 ?
.m-  ~.-  ~ ,~  ~.
a , .
.
.~,.
,~,- .
-  ~d ~.
bic Workbench (Day et ai., 1997) builds up an annotatedt t~ H.:20 .
I~* tt ~|d  h,~e ~ ~r l s t~ ?
8 J~  d l  dide~.~ +~ v - .
,..,I e~ .
, , .
.~ .~ w ~ .
eza ,  corpus  from scratch, under a mixed-initiative paradigm-~ ~., ,  ~ e. .
.
.
.
=~ ~ ~ - ?
m-~.
(in which some markings are given by the user, and some Ut4q~tmee f~t~"  .
\[\[ggl~ ~uttd Uhag tht~t hal l~m ~ ~' ?tvlI~lmeto r td~O m t~4 sine era" ru th  ~/ i .
~m emvkgt4P d~ll N~d i ~,~,.J .=.. ~ =~ ,~s..,=,,.,.,.., - ~  e. are automatically inserted by the computer).
Learning anaim ~l~m =~ tt~.
m ~Iv  f.tt~ltg f~dl IB~ that.
.
.
.
.~ , .
.
t .
, .
.
~ .
, .
,  ,,.,, e,, ~ w. ,  information extraction system was a primary function of~111'8 etatmmt te4.II~ ~'omd ~tMnll ?this system.
While the associated Alembic NLP system~,~,~,.~-,.~ ~, .-- m ,,~ ~ e..~.,,.-, e=a-,~ does incorporate some discourse level information into, , .
~ , .~ .
h. ~,~ m-= ,o e .
,~  ~=.
~ ~.
~ the system, the user may not impose an arbitrarily com-I o t~h l  ta d~w~ kit morn x~ xtat l~ l .
~ t~w* m m~ ~ ... ~ ,.
~.
,~ ~,t..a.
~,.
~, .
.~ .~m, .+ m plex discourse structure whose structure the system can i~mt .~.
In  tMo Ic~l  SIp or* t im VMeh ~mwe tt~ lw 'h j  ~ ~~,.
t ,~  ,,, t,,.e.
- rcprescn(.e~m .~.,,.~ ,.- .~.+ v -- ~:.~ ~,~.,~ ,-~, The Discourse Tagging Tool (Aone& Bennett, 1994)~.
,.
+ em ~ ,.~ " .
~,~.?.
,~m,  was designed for tagging multilingnal corpora, and also uar Ihd~eet ~a J l c t t (~,  S h t u t d l t m ~ d ~ f e a~'e, ,~  .
'~t~ .,.ltm.
~ ,,~ .~,., '*' '  ~'~em ~,e ~.~ e~ s - .
.  "
"  "~+ ~" ~' "+ does not allow complex marking of  discourse structure.Furthermore, the tag sets and relations are fixed and may~m's  tm~tm~ *~ ~t~ tt-~ I~mm role bq~st  ~d"~"~"  ~ " ~ ~ " - "*~,"  '~' ~"  " - -  not be elaborated by the user.
Also.
this work was not h l~Oq~f?
,  gatmut~t~ms?~tr tvwl l  .h t l~d l t~,~.. .
M ,,..,...~.~.
*m, e. ,~ e. .
.
-  ~,.
~ .
.
-  concerned with dialogs.
.
~ d~pJ ge Ig~ml~l~*l 4 g4~ gg ~hl  tl~g th~ 1 4m t lt Ig~ tp, .
,~- , - - ,~ , .
zo - -~- , .~  ~ 5 Future  Work  and  Conc lus ionsa, ,mq.,m: +e, m ~ , We are beginning annotation of  a parsed corpus using\[ fs.r/7 "t~e-emmt ~mmua:  ~.Zm f~;D "tmrm" '\[~--Ul-lm~Figure 1: REFEREE in Reference Mode3.2 Interface and Implementatien NotesEach of the three modes (reference, segment, and dia-log) has one main window in which a page of text is dis-played.
For example, Figwe i shows the main screen forreference mode.
In this figure, dark text represents theNP's that have been marked or extracted from the Tre~bank.
The "current expression" is highlighted and core-fen'ins expressions are underlined.
Though this schemeis perhaps visually unpleasing on paper, note that on thecomputer, the application uses vivid colors and easilydifferentiable typefaces.
Furthermore, elements o f  thecolor scheme are cnstomizable by the user.The tool saves the user's aunotafioes in several datafiles while leaving the original text file unchanged.
Otherannotation programs have embedded the annotations intothe text using a sublanguage ofXML.
Hies generated un-?
der either method are equally capable of  representing thedesired levels of  annotation; we separate the text fromthe annotations \[n order to simplify the parsing of thedata.
In case a REFEREE user should want to port somemarked text to a new annotation system, it is straightfor-ward to automatically generate a text-and-annotation fileto conform to any XML-style definition.Referee.
We have found that it is much easier to code acorpus and get reliable results when the system has al-ready found the majority of  the markables.
We intendto improve the tool by providing more functionality andbetter visualization of  patterns in the data.
We hope toadd more complex feature-extraction rules that scarf, h theparse tree more extensively for syntactic features ~ areevident from the tree suucune; We are also interestedin using a lexicalized knowledge base to find semanticrelationships between the marked expressions.We believe that the requirements o f the intended appli-cations dictate the design of  a novel and unique tool forthe analysis of the relationship between discourse struc-ture and reference.
Referee fills this niche, and greatlyreduces the workload placed on the human users.
Fur-thermore, the open design of  Referee makes it flexible,extenm'ble, and appficable to any number of  other appli-cations.6 AcknowledgmentsThis work has been supported by NSF Graduate Trainee-ship Grant GER-9354869 to the University of  Delaware.and a post-d.octoral fellowship award from the Institutefor Research in Cognitive Science (IRCS) at the Univer-sity of  Pennsylvania (NSF SBR 8920230).
Much of  thiswork was completed while the third author was a visitingscholar at IRCS and was supported by a grant from NSF(NSF SBR 8920230).
We would like to thank MiriamEckert for the many valuable discussions of  this work.60OO@OO@@O@OQ@O000O00000000000000000000000Q0000B0000000t00000000000I0000000000000000ReferencesAone, C. & S. W. Bennett (1994).
Discourse tagging tooland discourse-tagged muitilingual corpora.
In Pro-ceedings of the International Workshop on SharableNatural Language Resources ( SNLR ).Appelt, D.E.
(1981).
Planning Natural.Language Ut-terances to Satisfy Multiple Goals, (Ph.D. thesis).Stanford University.
Also appeared as: SRI Inter-national Technical Note 259, March 1982.Appeit, D. E. (1985).
Planning English referring expres-sions.
Artificial Intelligence, 26(1): 1-33.Carletta, J.
(1996).
Assessing agreement on classificationtasks: The kappa statistic.
Computational Linguis-tics, 22(2):249-254.Condon, S. & C. Cech (1995).
Problems for reliable dis-course coding systems.
In Working Notes for AAAISpring Symposium on Empirical Methods in Dis-course Interpretation and Generation, pp.
27-33.Stanford University.Connolly, D., J. D. Burger & D. S. Day (1997).
A ma-chine learning approach to anaphoric reference.
InD.
Jones & H. Somers (Eds.
), New Methods in Lan-guage Processing, pp.
133-143.
Oxford UniversityPress.Dale, R. (1992).
Generating Referring Expressions:Constructing Descriptions in a Domain of Objectsand Processes.
Cambridge; Mass.
: MIT Press.Dale, R. & E. Reiter (i995).
Computational interpre-tations of the Oriceun maxims in the generation ofreferring expressions.
Cognitive Science, 18:233-263.Day, D. S., J. Aberdeen, L. Hir~hman, R. Kozierok,P.
Robinson & M. Vilain (1997).
Mixed-initiativedevelopment of language preceding systems.
InProceedings o f  the F'g~h Conference on AppliedNatura/Langunge Processing, pp.
348-355, Wash-ington, D.C.Eckert, M. & M. Su~be (1999).
Resolving discoursedeictic anaphora in dialogues.
In Proceedings ofthe 9 it' Conference of the European ChaPter of theAssociation for Computational Linguistics, Bergen,Norway, 8-:12 June 1999.
To appear.Go, N., J. Hale & E. Charniak (1998).
A statistical p-proach to anaphora resolution.
In Proceedings ofthe Sixth Workshop on Very Large Corpora.Grosz, B. J.
& C. L. Sidner (I 986).
Attention, intentions,and the structure of discourse.
Computational Lin-guistics, 12(3): i 75-204.Marcus, M., G. Kim, M. A. Marcinkiewicz, R. MacIn-tyre, A. Bies, M. Ferguson, K. Katz & B. Schas-berger (1994).
The Penn treebank: Annotatingpredicate argument structure.
In Proceedings ofARPA Speech and Nataral Language Workshop.McCoy, K. E & M. Strube (1999a).'
Generatinganaphoric expressions: Pronoun or definite descrip-tion?
In ACL '99 Workshop o.n the Relationship be-tween Discourse~Dialogue Structure and Reference,University of Marfland, Marylan~ 21 June, 1999.This volume.McCoy, K. E & M. Strube (1999b).
Taking time to sl~c-ture discourse: Pronoun generation beyond accessi-bility.
In Proceedings of the 21 "t Annual Confer-ence of the Cognitive Science Society, Vancouver,British Columbia, Canada, 19-21 August 1999.
Toappear.McDonald, D. D. (1980).
Natural Language Productionas a Process of Decision Making Under Constraint,(Ph.D. thesis).
MIT.McKeown, If.
17,.
(1983).
Focus constraints on languagegeneration.
InProceedings of the 8 th InternationalJoint Conference on Artificial Intelligence, Karl-sruhe, Germany, August 1983, pp.
582-587.McKeown, ILR.
(1985).
Text Generation: Using Dis-course Strategies and Focus Constraints to Gener-ate NatumlLanguage T xt.
Cambridge.
U.K.: Cam-bridge University Press.Passonneau, R. (1996a).
Instructions for applying dis-course reference annotation for multiple applica.tions (DRAMA).
Colmnbia University, New York,Dept.
of Compute<.
Science.Passonneau, R. (1996b).
Using centering to relaxGricean constraints on disocurse anaphoric nounophrases.
Language and Speech, 39(2):229--264.Passonneau, R. & D. Lilman (1997)~ Discourse segmen-tation by human and automated means.
Compata-tional Linguistics, 23( I): 103-139.Poesio, M. & R. Vieira (1998).
A corpus-based inves-tigation of definite description use.
ComputationalL/ngu/stics, 24(2): 183.-216.Reichman, R. (1985).
Getting Computers to Talk like Youand Me.
Cambridge, Mass.
: MrrPress.61Reiter, E. (1990).
Generating descriptions that exploit auser's domain knowledge.
InR.
Dale, C. Mellish &M. Zock (Eds.
), Current Research in Natural Lan-guage Generation.
London: Academic Press.Sidner.
C. L. (1979).
Towards a Computational The-ory of Definite Anaphora Comprehension i  En-glish.
Technical Report AI-Memo 537, Cambridge,Mass.
: Massachusetts Institute of Technology, AILab.Strube, M. (1998).
Never look back: An alternative tocentering.
In Proceedings of the i7 th InternationalConference on Computational Linguistics and 36 taAnnual Meeting of the As$ocia "tlon for  Computa-tional Linguistics.
Montreal, Qut~3ec, Canada.
10-14 August 1998, Vol.
2, pp.
1251-1257.Vilain, M., J. Burger, I. Aberdeen, D. Connolly &I.,.
Hirschman (1995).
A model-theoretic corefer-ence scoring scheme.
In Proceedings fo the 6 titMessage Understanding Conference (MUC-6), pp.45-52.
San Mate, o, Cal.
: Morgan Kaufmann.620Q000I00Q00000000000O0000Q0000000O0000000000
