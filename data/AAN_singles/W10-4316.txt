Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 83?86,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsUtilizing Review Summarization in a Spoken Recommendation SystemJingjing Liu, Stephanie Seneff and Victor ZueMIT Computer Science & Artificial Intelligence LaboratoryCambridge, U.S.A.{jingl,seneff,zue}@csail.mit.eduAbstractIn this paper we present a framework for spoken rec-ommendation systems.
To provide reliable recom-mendations to users, we incorporate a review summa-rization technique which extracts informative opinionsummaries from grass-roots users?
reviews.
The dia-logue system then utilizes these review summaries tosupport both quality-based opinion inquiry and fea-ture-specific entity search.
We propose a probabilisticlanguage generation approach to automatically creat-ing recommendations in spoken natural languagefrom the text-based opinion summaries.
A user studyin the restaurant domain shows that the proposed ap-proaches can effectively generate reliable and helpfulrecommendations in human-computer conversations.1 IntroductionSpoken dialogue systems are presently availablefor many purposes, such as flight reservations(Seneff and Polifroni, 2000), telephone callsrouting (Gorin et al, 1997), and subway lookup(Johnston et al, 2002).
Recently, we have beenexploring a next generation of intelligent dia-logue systems, which can behave like a humanagent and provide proactive assistance and selec-tive recommendations (e.g., highly-rated restau-rants or hotels) to users.To enhance dialogue systems with intelligentservices, we have to let the system ?grow?
relia-ble knowledge and intelligence.
Luckily, therehas recently been an explosive growth in theavailability of public review sites (e.g., yelp.com,tripadvisor.com, etc.)
which make a perfect re-source for gathering collective opinions.
In thispaper, we will explore how to utilize summariesof public reviews to automatically generate rec-ommendation-type conversations in spoken dia-logue systems.
An example of a conversationwith our recommendation system is shown inFigure 1.U: Are there any American restaurants in Brighton?S: There are 61 American restaurants in Brighton.
All ofthe rated restaurants have good or excellent ambiance.Many are on Washington Street, Commonwealth Avenueand Market Street.U: Show me the ones that have nice ambiance.S: I found 4 entries.
Devlin's Restaurant and Bar is per-fect for romantic date, Stockyard Restaurant has friendlyatmosphere, Tasca Tapas Restaurant has excellent ambi-ance, and Wings Express has friendly vibes.U: Which one has good martinis?S: Devlin's Restaurant and Bar serves American food.
Ithas wonderful martinis, good wine, great fish.
It is perfectfor romantic date.
Great place.Figure 1.
A real conversation with our recom-mendation dialogue system in the restaurant do-main (?U?
is the user and ?S?
is the system).2 Dialogue ManagementIn our previous work (Liu and Seneff, 2009; Liuet al, 2010) we proposed an approach to extract-ing representative phrases and creating aspectratings from public reviews.
An example of anenhanced database entry in the restaurant domainis shown in Figure 2.
Here, we use these ?sum-mary lists?
(e.g., ?
:food?, ?:atmosphere?)
as wellas aspect ratings (e.g., ?:food_rating?)
to addresstwo types of recommendation inquires: ?feature-specific?
(e.g., asking for a restaurant that servesgood martinis or authentic seafood spaghetti),and ?quality-based?
(e.g., looking for restaurantswith good food quality or nice ambiance).
{q restaurant:name "devlin?s restaurant and bar":atmosphere ("romantic date" "elegant decor"):place ("great place"):food ("wonderful martinis" "good wine" "great fish"):atmosphere_rating "4.2":place_rating "4.2":food_rating "4.3":specialty ("martinis" "wine" "fish")     }Figure 2.
A database entry in our system.832.1 Feature-specific Entity SearchTo allow the system to identify feature-relatedtopics in users?
queries, we modify the context-free grammar in our linguistic parser by includ-ing feature-specific topics (e.g., nouns in thesummary lists) as a word class.
When a feature-specific query utterance is submitted by a user(as exemplified in Figure 3), our linguistic parserwill generate a hierarchical structure for the ut-terance, which encodes the syntactic and seman-tic structure of the utterance and, especially,identifies the feature-related topics.
A feature-specific key-value pair (e.g., ?specialty: marti-nis?)
is then created from the hierarchical parsingstructure, with which the system can filter thedatabase and retrieve the entities that satisfy theconstraints.Utterance?Are there any restaurants in Brighton thathave good martinis?
?Key-valuepairs?topic: restaurant,  city: Brighton,specialty: martinis?Databasefilters:specialty = ?martinis?
:city = ?Brighton?
:entity_type  = ?restaurant?Figure 3.
Procedure of feature-specific search.2.2 Quality-based Entity SearchFor quality-based questions, however, similarkeyword search is problematic, as the quality ofentities has variants of expressions.
The assess-ment of different degrees of sentiment in variousexpressional words is very subjective, whichmakes the quality-based search a hard problem.To identify the strength of sentiment in quali-ty-based queries, a promising solution is to maptextual expressions to scalable numerical scores.In previous work (Liu and Seneff, 2009), weproposed a method for calculating a sentimentscore for each opinion-expressing adjective oradverb (e.g., ?bad?
: 1.5, ?good?
: 3.5, ?great?
: 4.0,on a scale of 1 to 5).
Here, we make use of thesesentiment scores and convert the original key-value pair to numerical values (e.g., ?great food??
?food_rating: 4.0?
as exemplified in Figure4).
In this way, the sentiment expressions can beeasily converted to scalable numerical key-valuepairs, which will be used for filtering the data-base by ?aspect ratings?
of entities.
As exempli-fied in Figure 4, all the entities in the requiredrange of aspect rating (i.e., ?
:food_rating   4.0?
)can be retrieved (e.g., the entity in Figure 2 with?food_rating = 4.3?
).Utterance?Show me some american restaurants withgreat food?Key-valuepairs?topic: restaurant, cuisine: american,property: food, quality: great?Convertedk-v pairs?topic: restaurant, cuisine: american,food_rating: 4.0?Databasefilters:food_rating > ?4.0?
:cuisine = ?american?
:entity_type =  ?restaurant?Figure 4.
Procedure of qualitative entity search.3 Probabilistic Language GenerationAfter corresponding entities are retrieved fromthe database based on the user?s query, the lan-guage generation component will create recom-mendations by expanding the summary lists ofthe retrieved database entries into natural lan-guage utterances.Most spoken dialogue systems use predefinedtemplates to generate responses.
However, man-ually defining templates for each specific linguis-tic pattern is tedious and non-scalable.
For ex-ample, given a restaurant with ?nice jazz music,best breakfast spot, great vibes?, three templateshave to be edited for three different topics (e.g.,?<restaurant> plays <adjective> music?
; ?<res-taurant> is <adjective> breakfast spot?
; ?<restau-rant> has <adjective> vibes?).
To avoid the hu-man effort involved in the task, corpus-basedapproaches (Oh and Rudnicky, 2000; Rambow etal., 2001) have been developed for more efficientlanguage generation.
In this paper, we propose acorpus-based probabilistic approach which canautomatically learn the linguistic patterns (e.g.,predicate-topic relationships) from a corpus andgenerate natural sentences by probabilisticallyselecting the best-matching pattern for each top-ic.The proposed approach consists of three stag-es: 1) plant seed topics in the context-free gram-mar; 2) identify semantic structures associatedwith the seeds; 3) extract association pairs of lin-guistic patterns and the seeds, and calculate theprobability of each association pair.First, we extract all the nouns and nounphrases that occur in the review summaries as theseeds.
As aforementioned, our context-freegrammar can parse each sentence into a hierar-chical structure.
We modify the grammar suchthat, when parsing a sentence which contains oneof these seed topics, the parser can identify theseed as an ?active?
topic (e.g., ?vibes?, ?jazz mu-sic?, and ?breakfast spot?
).84The second stage is to automatically identifyall the linguistic patterns associated with eachseed.
To do so, we use a large corpus as the re-source pool and parse each sentence in the cor-pus for linguistic analysis.
We modify our parsersuch that, in a preprocessing step, the predicateand clause structures that are semantically relatedto the seeds will be assigned with identifiabletags.
For example, if the subject or the comple-ment of the clause (or the object of the predicate)is an ?active?
topic (i.e., a seed), an ?active?
tagwill be automatically assigned to the clause (orthe predicate).
In this way, when examining syn-tactic hierarchy of each sentence in the corpus,the system can encode all the linguistic patternsof clauses or predicate-topic relationships associ-ated with the seeds with ?active?
tags.Based on these tags, association pairs of ?ac-tive?
linguistic patterns and ?active?
topics canbe extracted automatically.
For each seed topic,we calculate the probability of its co-occurrencewith each of its associated patterns by:(        |     )?
(1)where       is a seed topic, and          isevery linguistic pattern associated with      .The probability of          for       is thepercentage of the co-occurrences ofand       among all the occurrences ofin the corpus.
This is similar to a bigram lan-guage model.
A major difference is that the lin-guistic pattern is not necessarily the word adja-cent to the seed.
It can be a long distance fromthe seed with strong semantic dependencies, andit can be a semantic chunk of multiple words.The long distance semantic relationships are cap-tured by our linguistic parser and its hierarchicalencoding structure; thus, it is more reliable thanpure co-occurrence statistics or bigrams.
Figure 5shows some probabilities learned from a reviewcorpus.
For example, ?is?
has the highest proba-bility (0.57) among all the predicates that co-occur with ?breakfast spot?
; while ?have?
is thebest-match for ?jazz music?.Association pair Constituent Prob.?at?
: ?breakfast spot?
PP 0.07?is?
: ?breakfast spot?
Clause 0.57?for?
: ?breakfast spot?
PP 0.14?love?
: ?jazz music?
VP 0.08?have?
: ?jazz music?
VP 0.23?enjoy?
: ?jazz music?
VP 0.08Figure 5.
Partial table of probabilities of associa-tion pairs (VP: verb phrase; PP: prepositionphrase).Given these probabilities, we can define pat-tern selection algorithms (e.g., always select thepattern with the highest probability for each top-ic; or rotates among different patterns from highto low probabilities), and generate response ut-terances based on the selected patterns.
The onlydomain-dependent part of this approach is theselection of the seeds.
The other steps all dependon generic linguistic structures and are domain-independent.
Thus, this probabilistic method canbe easily applied to generic domains for custom-izing language generation.4 ExperimentsA web-based multimodal spoken dialogue sys-tem, CityBrowser (Gruenstein and Seneff, 2007),developed in our group, can provide users withinformation about various landmarks such as theaddress of a museum, or the opening hours of arestaurant.
To evaluate our proposed approaches,we enhanced the system with a review-summarydatabase generated from a review corpus that weharvested from a review publishing web site(www.citysearch.com), which contains 137,569reviews on 24,043 restaurants.We utilize the platform of Amazon Mechani-cal Turk (AMT) to conduct a series of user stud-ies.
To understand what types of queries the sys-tem might potentially be handling, we first con-ducted an AMT task by collecting restaurant in-quiries from general users.
Through this AMTtask, 250 sentences were collected and a set ofgeneric templates encoding the language patternsof these sentences was carefully extracted.
Then10,000 sentences were automatically createdfrom these templates for language model trainingfor the speech recognizer.To evaluate the quality of recommendations,we presented the system to real users via custom-ized AMT API (McGraw et al, 2010) and gaveeach subject a set of assignments to fulfill.
Eachassignment is a scenario of finding a particularrestaurant, as shown in Figure 6.
The user cantalk to the system via a microphone and ask forrestaurant recommendations.We also gave each user a questionnaire for asubjective evaluation and asked them to rate thesystem on different aspects.
Through this AMTtask we collected 58 sessions containing 270 ut-terances (4.6 utterances per session on average)and 34 surveys.
The length of the utterances var-ies significantly, from ?Thank you?
to ?Restau-rants along Brattle Street in Cambridge with nice85cocktails.?
The average number of words perutterance is 5.3.Figure 6.
Interface of our system in an AMT as-signment.Among all the 58 sessions, 51 were success-fully fulfilled, i.e., in 87.9% of the cases the sys-tem provided helpful recommendations upon theuser?s request and the user was satisfied with therecommendations.
Among those seven failedcases, one was due to loud background noise,two were due to users?
operation errors (e.g.,clicking ?DONE?
before finishing the scenario),and four were due to recognition performance.The user ratings in the 34 questionnaires areshown in Figure 7.
On a scale of 0 (the center) to5 (the edge), the average rating is 3.6 on the eas-iness of the system, 4.4 on the helpfulness of therecommendations, and 4.1 on the naturalness ofthe system response.
These numbers indicate thatthe system is very helpful at providing recom-mendation upon users?
inquiries, and the re-sponse from the system is present in a naturalway that people could easily understand.Figure 7.
Users?
ratings from the questionnaires.The lower rating of ease of use is partially dueto recognition errors.
For example, a user askedfor ?pancakes?, and the system recommended?pizza places?
to him.
In some audio clips rec-orded, the background noise is relatively high.This may be due to the fact that some AMTworkers work from home, where it can be noisy.5 ConclusionsIn this paper we present a framework for incor-porating review summarization into spoken rec-ommendation systems.
We proposed a set of en-tity search methods as well as a probabilistic lan-guage generation approach to automatically cre-ate natural recommendations in human-computerconversations from review summaries.
A userstudy in the restaurant domain shows that theproposed approaches can make the dialogue sys-tem provide reliable recommendations and canhelp general users effectively.Future work will focus on: 1) improving thesystem based on users?
feedback; and 2) apply-ing the review-based approaches to dialogue sys-tems in other domains.AcknowledgmentsThis research is supported by Quanta Computers,Inc.
through the T-Party project.ReferencesGorin, A., Riccardi, G., and Wright, J. H. 1997.
HowMay I Help You?
Speech Communications.
Vol.23, pp.
113 ?
127.Gruenstein, A. and Seneff, S. 2007.
Releasing a Mul-timodal Dialogue System into the Wild: User Sup-port Mechanisms.
In Proc.
the 8th SIGdial Work-shop on Discourse and Dialogue, pp.
111?119.Johnston, M., Bangalore, S., Vasireddy, G., Stent, A.,Ehlen, P., Walker, M., Whittaker, S., Maloor, P.2002.
MATCH: An Architecture for MultimodalDialogue Systems.
In Proc.
ACL, pp.
376 ?
383.Liu, J. and Seneff, S. 2009. Review sentiment scoringvia a parse-and-paraphrase paradigm, In Proc.EMNLP, Vol.
1.Liu, J., Seneff, S. and Zue, V. 2010.
Dialogue-Oriented Review Summary Generation for SpokenDialogue Recommendation Systems.
In Proc.NAACL-HLT.McGraw, I., Lee, C., Hetherington, L., Seneff, S.,Glass, J.
2010.
Collecting Voices from the Cloud.In Proc.
LREC.Oh, A.H. and Rudnicky, A.I.
2000.
Stochastic Lan-guage Generation for Spoken Dialogue Systems.
InProc.
of ANLP-NAACL, pp.
27-32.Rambow, O., Bangalore, S., Walker, M. 2001.
Natu-ral Language Generation in Dialog Systems.
InProc.
Human language technology research.Seneff, S. and Polifroni, J.
2000.
Dialogue Manage-ment in the Mercury Flight Reservation System.
InProc.
Dialogue Workshop, ANLP-NAACL.01234512 3456789101112131415161718192021222324252627282930313233 34Ease of useHelpfulnessNaturalness86
