Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 359?362,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsHERMIT: Flexible Clustering for the SemEval-2 WSI TaskDavid JurgensUniversity of California, Los AngelesLos Angeles, California, USAjurgens@cs.ucla.eduKeith StevensUniversity of California, Los AngelesLos Angeles, California, USAkstevens@cs.ucla.eduAbstractA single word may have multiple un-specified meanings in a corpus.
Wordsense induction aims to discover these dif-ferent meanings through word use, andknowledge-lean algorithms attempt thiswithout using external lexical resources.We propose a new method for identify-ing the different senses that uses a flexi-ble clustering strategy to automatically de-termine the number of senses, rather thanpredefining it.
We demonstrate the effec-tiveness using the SemEval-2 WSI task,achieving competitive scores on both theV-Measure and Recall metrics, dependingon the parameter configuration.1 IntroductionThe Word Sense Induction task of SemEval 2010compares several sense induction and discrimina-tion systems that are trained over a common cor-pus.
Systems are provided with an unlabeled train-ing corpus consisting of 879,807 contexts for 100polysemous words, with 50 nouns and 50 verbs.Each context consists of several sentences that usea single sense of a target word, where at least onesentence contains the word.
Systems must use thetraining corpus to induce sense representations forthe many word senses and then use those represen-tations to produce sense labels for the same 100words in unseen contexts from a testing corpus.We perform this task by utilizing a distribu-tional word space formed using dimensionalityreduction and a hybrid clustering method.
Ourmodel is highly scalable; the dimensionality of theword space is reduced immediately through a pro-cess based on random projections.
In addition, anonline part of our clustering algorithm maintainsonly a centroid that describes an induced wordsense, instead of all observed contexts, which letsthe model scale to much larger corpora than thoseused in the SemEval-2 WSI task.2 The Word Sense Induction ModelWe perform word sense induction by modelingindividual contexts in a high dimensional wordspace.
Word senses are induced by finding con-texts which are similar and therefore likely to usethe same sense of the target word.
We use a hybridclustering method to group similar contexts.2.1 Modeling ContextFor a word, each of its contexts are represented bythe words with which it co-occurs.
We approx-imate this high dimensional co-occurrence spacewith the Random Indexing (RI) word space model(Kanerva et al, 2000).
RI represents the occur-rence of a word with an index vector, rather thana set of dimensions.
An index vector is a fixed,sparse vector that is orthogonal to all other words?index vectors with a high probability; the totalnumber of dimensions in the model is fixed at asmall value, e.g.
5,000.
Orthogonality is obtainedby setting a small percentage of the vector?s valuesto ?1 and setting the rest to 0.A context is represented by summing the indexvectors corresponding to the n words occurring tothe left and right of the polysemous word.
Eachoccurrence of the polysemous word in the entirecorpus is treated as a separate context.
Contextsare represented by a compact first-order occur-rence vector; using index vectors to represent theoccurrences avoids the computational overhead ofother dimensional reduction techniques such asthe SVD.2.2 Identifying Related ContextsClustering separates similar context vectors intodissimilar clusters that represent the distinctsenses of a word.
We use an efficient hybrid ofonline K-Means and Hierarchical Agglomerative359Clustering (HAC) with a threshold.
The thresh-old allows for the final number of clusters to bedetermined by data similarity instead of having tospecify the number of clusters.The set of context vectors for a word are clus-tered using K-Means, which assigns a context tothe most similar cluster centroid.
If the near-est centroid has a similarity less than the clusterthreshold and there are not K clusters, the contextforms a new cluster.
We define the similarity be-tween contexts vectors as the cosine similarity.Once the corpus has been processed, clustersare repeatedly merged using HAC with the aver-age link criteria, following (Pedersen and Bruce,1997).
Average link clustering defines cluster sim-ilarity as the mean cosine similarity of the pair-wise similarity of all data points from each clus-ter.
Cluster merging stops when the two most sim-ilar clusters have a similarity less than the clus-ter threshold.
Reaching a similarity lower than thecluster threshold signifies that each cluster repre-sents a distinct word sense.2.3 Applying Sense LabelsBefore training and evaluating our model, alloccurrences of the 100 polysemous words werestemmed in the corpora.
Stemming was requireddue to a polysemous word being used in multiplelexical forms, e.g.
plural, in the corpora.
By stem-ming, we avoid the need to combine contexts foreach of the distinct word forms during clustering.After training our WSI model on the trainingcorpus, we process the test corpus and label thecontext for each polysemous word with an inducedsense.
Each test context is labeled with the nameof the cluster whose centroid has the highest co-sine similarity to the context vector.
We representthe test contexts in the same method used for train-ing; index vectors are re-used from training.3 Evaluation and ResultsThe WSI task evaluated the submitted solutionswith two methods of experimentation: an unsuper-vised method and a supervised method.
The unsu-pervised method is measured according to the V-Measure and the F-Score.
The supervised methodis measured using recall.3.1 ScoringThe first measure used is the V-Measure (Rosen-berg and Hirschberg, 2007), which compares theclusters of target contexts to word classes.
Thismeasure rates the homogeneity and completenessof a clustering solution.
Solutions that have wordclusters formed from one word class are homoge-neous; completeness measures the degree to whicha word class is composed of target contexts allo-cated to a single cluster.The second measure, the F-Score, is an ex-tension from information retrieval and provides acontrasting evaluation metric by using a differentinterpretation of homogeneity and completeness.For the F-Score, the precision and recall of all pos-sible context pairs are measured, where a wordclass has the expected context pairs and a providedsolution contains some word pairs that are correctand others that are unexpected.
The F-Score tendsto discount smaller clusters and clusters that can-not be assigned to a word class (Manandhar et al,2010).3.2 Parameter TuningPrevious WSI evaluations provided a test corpus,a set of golden sense labels, and a scoring mecha-nism, which allowed models to do parameter tun-ing prior to providing a set of sense labels.
TheSemEval 2010 task provided a trial corpus thatcontains contexts for four verbs that are not in theevaluation corpus, which can be used for train-ing and testing.
The trial corpus also came with aset of golden sense assignments.
No golden stan-dard was provided for the training or test corpora,which limited any parameter tuning.HERMIT exposes three parameters: clusterthreshold, the maximum number of clusters andthe window size for a context.
An initial anal-ysis from the trial data showed that the windowsize most affected the scores; small window sizesresulted in higher V-Measure scores, while largerwindow sizes maximized the F-Score.
Becausecontexts are represented using only first-order fea-tures, a smaller window size should have less over-lap, which potentially results in a higher numberof clusters.
We opted to maximize the V-Measurescore by using a window size of ?1.Due to the limited number of training instances,our precursory analysis with the trial data did notshow significant differences for the remaining twoparameters; we arbitrarily selected a clusteringthreshold of .15 and a maximum of 15 clusters perword without any parameter tuning.After the release of the testing key, we per-360formed a post-hoc analysis to evaluate the effectsof parameter tuning on the scores.
We include twoalternative parameter configurations that were op-timized for the F-Score (HERMIT-F) and the su-pervised evaluations (HERMIT-S).
The HERMIT-F variation used a threshold of 0.85 and a win-dow size of ?10 words.
The HERMIT-S variationused a threshold of 0.85 and a window size of ?1words.
We did not vary the maximum number ofclusters, which was set at 15.For each evaluation, we provide the scores ofseven systems: the three HERMIT configurations,the highest and lowest scoring submitted systems,the Most Frequent Sense (MFS) baseline, and aRandom baseline provided by the evaluation team.We provide the scores for each experiment whenevaluating all words, nouns, and verbs.
We alsoinclude the system?s rank relative to all submittedsystems and the average number of senses gen-erated for each system; our alternative HERMITconfigurations are given no rank.3.3 Unsupervised EvaluationSystem All Nouns Verbs Rank SensesHERMIT-S 16.2 16.7 15.3 10.83HERMIT 16.1 16.7 15.6 1 10.78Random 4.4 4.6 4.1 18 4.00HERMIT-F 0.015 0.008 0.025 1.54MFS 0.0 0.0 0.0 27 1.00LOW 0.0 0.0 0.1 28 1.01Table 1: V-Measure for the unsupervised evalua-tionSystem All Nouns Verbs Rank SensesMFS 63.4 57.0 72.7 1 1.00HIGH 63.3 57.0 72.4 2 1.02HERMIT-F 62.1 56.7 69.9 1.54Random 31.9 30.4 34.1 25 4.00HERMIT 26.7 30.1 24.4 27 10.78HERMIT-S 26.5 23.9 30.3 10.83LOW 16.1 15.8 16.4 28 9.71Table 2: F-Scores for the unsupervised evaluationThe unsupervised evaluation considers a goldensense labeling to be word classes and a set of in-duced word senses as clusters of target contexts(Manandhar et al, 2010).
Tables 1 and 2 displaythe results for the unsupervised evaluation whenmeasured according to the V-Measure and the F-Score, respectively.
Our system provides the bestV-Measure of all submitted systems for this eval-uation.
This is in part due to the average numberof senses our system generated (10.78), which fa-vors more homogenous clusters.
Conversely, thisconfiguration does poorly when measured by F-Score, which tends to favor systems that generatefewer senses per word.When configured for the F-Score, HERMIT-F performs well; this configuration would haveranked third for the F-Score if it had been submit-ted.
However, its performance is also due to therelatively few senses per word it generates, 1.54.The inverse performance of both optimized con-figurations is reflective of the contrasting nature ofthe two performance measures.3.4 Supervised EvaluationSystem All Noun Verb RankHIGH 62.44 59.43 66.82 1MFS 58.67 53.22 66.620 15HERMIT-S 58.48 54.18 64.78HERMIT 58.34 53.56 65.30 17Random 57.25 51.45 65.69 19HERMIT-F 56.44 53.00 61.46LOW 18.72 1.55 43.76 28Table 3: Supervised recall for the 80/20 splitSystem All Noun Verb RankHIGH 61.96 58.62 66.82 1MFS 58.25 52.45 67.11 12HERMIT 57.27 52.53 64.16 18HERMIT-S 57.10 52.76 63.46Random 56.52 50.21 65.73 20HERMIT-F 56.18 52.26 61.88LOW 18.91 1.52 44.23 28Table 4: Supervised recall for the 60/40 splitThe supervised evaluation simulates a super-vised Word Sense Disambiguation (WSD) task.The induced sense labels for the test corpus aresplit such that the first set is used for mapping in-duced senses to golden senses and the remainingsense labels are treated as sense labels providedby a WSD system, which allows for evaluation.Five splits are done at random to avoid any biasescreated due to the separation of the mapping cor-pus and the evaluation corpus; the resulting scorefor this task is the average recall over the five di-visions.
Two sets of splits were used for evalua-tion: one with 80% of the senses as the mappingportion and 20% as the evaluation portion and onewith 60% as the mapping portion corpus and 40%for evaluation.The results for the 80/20 split and 60/40 splitare displayed in tables 3 and 4, respectively.
Inboth supervised evaluations, our submitted system36104812Clusters Clusters0.250.300.350.400.450.500.550.602  4  6  8 10 12 140.000.020.040.060.080.100.120.140.160.18F-ScoreV-MeasureWindow SizeF-ScoreV-MeasureFigure 1: A comparison for F-Score and V-Measure for different window sizes.
Scores are anaverage using thresholds of 0.15, 0.55 and 0.75.does moderately well.
In both cases it outperformsthe Random baseline and does almost as well asthe MFS baseline.
The submitted system outper-forms the Random baseline and approaches theMFS baseline for the 80/20 split.
The HERMIT-Sversion, which is optimized for this task, providessimilar results.4 DiscussionThe HERMIT system is easily configured toachieve close to state of the art performance foreither evaluation measure on the unsupervisedbenchmark.
This reconfigurability allows the al-gorithm to be tuned for producing a few coarsesenses of a word, or many finer-grained senses.We further investigated the performance withrespect to the window size parameter on both mea-sures.
Since each score can be effectively opti-mized individually, we considered whether bothscores could be maximized concurrently.
Figure1 presents the impact of the window size on bothmeasures using an average of three threshold pa-rameter configurations.The analysis of both measures indicates thatreasonable performance can be obtained from us-ing a slightly larger context window.
For ex-ample, a window size of 4 has an average F-Score of 52.4 and V-Measure of 7.1.
Althoughthis configuration produces scores lower than theoptimized versions, its performance would haveranked 12th according to V-Measure and 15th forF-Score.
These scores are consistent with the me-dian performance of the submitted systems and of-fer a middle ground should a HERMIT user wanta compromise between many fine-grained wordsenses and a few coarse-grained word senses.5 ConclusionWe have shown that our model is a highly flexi-ble and tunable Word Sense Induction model.
De-pending on the task, it can be optimized to gen-erate a set of word senses that range from be-ing broad and representative to highly refined.Furthermore, we demonstrated a balanced perfor-mance setting for both measures for when param-eter tuning is not possible.
The model we sub-mitted and presented is only one possible config-uration available, and in the future we will be ex-ploring the effect of other context features, suchas syntactic structure in the form of word ordering(Sahlgren et al, 2008) or dependency parse trees,(Pado?
and Lapata, 2007), and other clustering al-gorithms.
Last, this model is provided as part ofthe S-Space Package (Jurgens and Stevens, 2010),an open source toolkit for word space algorithms.ReferencesDavid Jurgens and Keith Stevens.
2010.
The S-SpacePackage: An Open Source Package for Word SpaceModels.
In Proceedings of the ACL 2010 SystemDeonstrations.Pentti Kanerva, Jan Kristoferson, and Anders Holst.2000.
Random indexing of text samples for latentsemantic analysis.
In L. R. Gleitman and A. K. Josh,editors, Proceedings of the 22nd Annual Conferenceof the Cognitive Science Society, page 1036.Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan.
2010.
SemEval-2010Task 14: Word Sense Induction & Disambiguation.In Proceedings of SemEval-2.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-Based Construction of Seman-tic Space Models.
Computational Linguistics,33(2):161?199.Ted Pedersen and Rebecca Bruce.
1997.
Distinguish-ing word senses in untagged text.
In Proceedingsof the Second Conference on Empirical Methods inNatural Language Processing, pages 197?207.Andrew Rosenberg and Julia Hirschberg.
2007.
V-Measure: A Conditional Entropy-Based ExternalCluster Evaluation Measure.
In Proceedings ofthe 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL).
ACL.Magnus Sahlgren, Anders Holst, and Pentti Kanerva.2008.
Permutations as a means to encode or-der in word space.
In Proceedings of the 30thAnnual Meeting of the Cognitive Science Society(CogSci?08).362
