Exploiting Sophisticated Representations for DocumentRetrievalSteven FinchLanguage Technology Group, HCRCUniversity of Ed inburghS.
F inch~ed.
ac .
ukAbst rac tThe use of NLP techniques for docu-ment classification has not produced signif-icant improvements in performance withinthe standard term weighting statistical as-signment paradigm (Fagan 1987; Lewis,1992bc; Buckley, 1993).
This perplexingfact needs both an explanation and a so-lution if the power of recently developedNLP techniques are to be successfully ap-plied in IR.
A novel method for adding lin-guistic annotation to corpora is presentedwhich involves using a statistical POS tag-ger in conjunction with unsupervised struc-ture finding methods to derive notions of"noun group", "verb group", and so onwhich is inherently extensible to more so-phisticated annotation, and does not re-quire a pre-tagged corpus to fit.
One of thedistinguishing features of a more linguisti-cally sophisticated representation f docu-ments over a word set based representationof them is that linguistically sophisticatedunits are more frequently individually goodpredictors of document descriptors (key-words) than single words are.
This leadsus to consider the assignment ofdescriptorsfrom individual phrases rather than fromthe weighted sum of a word set representa-tion.
We investigate how sets of individu-ally high-precision rules can result in a lowprecision when used together, and developsome theory about these probably-correctrules.
We then proceed to repeat resultswhich show that standard statistical mod-els are not particularly suitable for exploit-ing linguistically sophisticated representa-tions, and show that a statistically fittedrule-based model provides ignificantly im-proved performance for sophisticated rep-resentations.
It therefore shows that statis-tical systems can exploit sophisticated rep-resentations ofdocuments, and lends somesuppor t to the use of more linguistically65sophisticated representations fordocumentclassification.
This paper reports on workdone for the LRE project SmTA, which iscreating a PC based tool to be used in thetechnical abstracting industry.1 Mode ls  and Representat ionsFirst, I discuss the general paradigm for documentclassification, along with the conventions for nota-tion used throughout this document.
We have aset of documents {zi}, and set of descriptors, {di}.Each document is represented in one or more waysin some domain, usually as a set.
The elements ofthis set will be called diagnostic units or predicates,{wi} or {?i).
These diagnostic units might be thewords comprising the document, or more linguisti-cally sophisticated annotations of parts of the doc-ument.
They may, in general, be predicates overdocuments.
The representation f the document bydiagnostic units will be called the DU-representationof the document, and for a document z, will be de-noted T~(x).
From the DU representation f the doc-uments, one or more descriptors are assigned to eachof them by some automatic system.
This paradigmof description is applicable to much of the work ontext classification (and other fields in informationretrieval).This paper assesses the utility of using linguisti-cally sophisticated diagnostic units together with aslightly non-standard statistical assignment modelin order to assign descriptors to a document.2 The  CorpusThis paper reports work undertaken for the LREproject SISTA (Semi-automatic Indexing System forTechnical Abstracts).
This section briefly describesone of the corpora used by this project.The RAPRA corpus comprises some 212,000 tech-nical abstracts pertaining to research and commer-cial exploitation i the rubber and plastics industry.To each abstract, an average of 15 descriptors e-lected from a thesaurus of some 10,000 descriptorsis assigned to each article.
The frequency of assign-ment of descriptors varies roughly in the same wayas the frequency of word use varies (the frequenciesof descriptor tokens (very) approximately satisfiesthe Zipf-Mandelbrot law).
Descriptors are assignedby expert indexers from the entire article and expertdomain knowledge, not just from the abstract, so itis unlikely that any automatic system which analy-ses only the abstracts can assign all the descriptorswhich are manually assigned to the abstract.We show a fairly typical example below.
It is clearthat many of these descriptors must have been as-signed from the main text of the article, and notfrom the abstract alone.
Moreover, this is commonpractice in the technical abstract indexing industry,so it seems unlikely that the situation will be betterfor other corpora.
Nevertheless, we can hope to fol-low a strategy of assigning descriptors when there isenough information to do so.Macromolecu lar  Deformat ion  Mode l  to EstimateViscoelastic Flow Effects in Polymer MeltsThe elastic deformation of polymer macromolecules in ashear field is used as the basis for quantitative predic-tions of viscoelastic flow effects in a polymer melt.
Non-Newtonian viscosity, capillary end correction factor, maxi-mum die swell, and die swell profile of a polymer melt arcpredicted by the model.
All these effects can be reducedto generic master curves, which are independent ofpolymertype.
Macromolecular deformation also influences the brit-tle failure strength of a processed polymer glass.
The modelgives simple and accurate stimates of practically importantprocessing effects, and uses fitting parameters with the clearphysical identity of viscoelastic constants, which follow wellestablished trends with respect o changes in polymcr com-position or processing conditions.
12 refs.Original assignment: BRITTLE FAILURE; COMPANY;DATA; DIE SWELL; ELASTIC DEFORMATION; EQUATION;GRAPH; MACROMOLECULE; MELT FLOW; MODEL; NON-NEWTONIAN; PLASTIC; POLYMERIC GLASS; PROCESSING;RHEOLOGICAL PROPERTIES; RHEOLOGY; TECHNICAL; THE-ORY; THERMOPLASTIC; VISCOELASTIC PROPERTIES; VIS-COELASTICITY; VISCOSITY3 Mode lsTwo classes of models for assessing descriptor appro-priateness were used.
One class comprises variantsof Salton's term-weighting models, and one is moreallied to fuzzy or default logic in so much as it assignsdescriptors due to the presence of certain diagnos-tic units.
What is interesting for us is that termweighting models do not seem able to easily exploitthe additional information provided by a more so-phisticated representation of a document, while analternative statistical single term model can.3.1 Term weight ing  mode lsThe standard term weighting model is defined bychosing a set of parameters {c~ij } (one for each word-descriptor pair) and {fli} (one for each desc,'iptor)so that a likelihood or appropriateness function, /2,can be defined byC(alw) = (1)wEWThis has been widely used, and is provably equiv-alent to a large class of probabilistic models (e.g.Van Risjbergen, 1979) which make various assump-tions about the independence between descriptorsand diagnostic units (Fuhr & Buckley, 1993).
Vari-ous strategies for estimating the parameters for thismodel have been proposed (e.g.
Salton & Yang,1973, Buckley 1993, Fuhr & Buekley, 1993).
Someof these concentrate on the need for re-estimatingweights according to relevance feedback information,while some make use of various functions of termfrequency, document frequency, maximum within-document frequency, and various other measure-ments of corpora.
Nevertheless, the problem of esti-mating the huge number of parameters needed forsuch a model is statistically problematic, and asBuckley (1993) points out, the choice of weights hasa large influence on the effectiveness of any modelfor classification or for retrieval.There are so many variations on the theme of termweighting models that it is impossible to try themall in one experiment, so this paper uses a variationof a model used by Lewis (1992e) in which he re-.ports the results of some experiments using phrasesIn a term weighting model (which has a probabilisticinterpretation).
Several term weighting models havebeen tried, but they all evaluate within 5 points ofeach other on both precision and recall (when suit-ably tweaked).The model eventually chosen for the tests reportedhere was a smoothed logistic model which gave thebest results of all the probabilistically inspired termweighting models considered.3.2 S ing le  te rm modelIn contrast o making assumptions of independenceabout the relationship between diagnostic units andwords, the next model utilises only those diagnosticunits which strongly predict descriptors (i.e.
havefrequently been associated with descriptors) with-out making assumptions about the independence ofdiagnostic units given descriptors.We shall investigate this class of models usingprobability theory.
The main problem with usingprobability theory for problems in document classi-fication is that while it might be relatively easy toestimate probabilities such as P(dlw ) for some diag-nostic unit w and some descriptor d, it is not possible66to infer much about P(dIw~), where ?
is some ad-ditional information (e.g.
the other DUs which rep-resent the document), since these probabilities havenot been estimated, and would take a far larger cor-pus to reliably estimate in any case.
The situationgets exponentially worse as the information we haveabout the document increases.
The exception to thisrule is when P(dlw ) is close to 1, in which case it isvery unlikely that additional information changes itsvalue much.
This fact is further investigated now.The strategy explored here is to concentrate onfinding "sure-fire" indicators of descriptors, in asomewhat similar manner to how Carnegie's TCSworks, by exploiting the fact that with a pre-classified training corpus we can identify sure-fireindicators empirically and "trawl" in a large set ofinformative diagnostic units for those which identifydescriptors with high precision.
The basis of themodel is the following:We consider a likelihood function, Z: defined by:Z(dlw) = gd~N~That is, the number of articles in the training cor-pus that d was observed to occur with w divided bythe number of articles in which w occurred in thetraining corpus.
This is an empirical estimate of theconditional probability, P(d\[w).
We shall assume(for simplicity's ake) that we have a large enoughcorpus do reliably estimate these probabilities.The strategy for descriptor assignment we are in-vestigating is to assign a descriptor d if and onlyif one of a set of predicates over representations ofdocuments is true.
We define the rule ?
(x) ~ dto be Probably Correct do degree ?
if and only ifP(dl? )
> 1 -?
.
We wish to keep the precision result-ing from using this strategy high while increasing thenumber of rules to improve recall.
The predicates ?we shall consider for this paper will be very simple(they will typically be true iff w E T~(x) for somediagnostic unit w), but in principle, they could bearbitrarily complex (as they are in Carnegie's TCS).The pr imary question of concern is whether the en-semble of rules {?i --~ d} retains precision or not.Unfortunately, the answer to this question is thatthis is not necessarily the case unless we put someconstraints on the predicates.P ropos i t ion  1 Let ?
be a set of predicates with theproperty that for some fixed descriptor d, ?
E ?
---+P(d\]? )
> 1 - ?.
That is each of the rules ?i --+ d isprobably correct to degree c.The expected precision of the rule (V ?i) --* d is_ ne where n is the cardinality, \](I)\].
at least 1Proof :\[Straight-forward and omitted\]This proposition asserts that one cannot be guar-anteed to be able to keep adding diagnostic units toimprove recall without hurting precision, unless the67quality of those diagnostic units is also improved (i.e.c is decreased in proportion to the number of DUswhich are considered).
This is unfortunate, but nev-ertheless the question of how much adding diagnosticunits to help recall will hurt precision is an entirelyempirical matter dependent on the true nature of P;this proposition is a worst case, and gives us reasonto be careful.
Performance will be expected to bepoorest if there are many rules which correspond tothe same true positives, but different sets of falsepositives.
If the predicates are disjoint, for example,then the precision of a disjunction is at least as greatas the precision of applying any single rule.So if we design our predicates so that they are dis-joint, then we retain precision while increasing recall.In practice, this is infeasible, but it is feasible to lookmore carefully at frequently co-occurring predicates,since these will be most likely to reduce precision.
1The main moral we can draw from the above twopropositions is that we must be careful about thecase where diagnostic units are highly correlated.One situation which is relatively frequent as thesophistication of representation increases is thatsome diagnostic units always co-occur with others.For example, if the document were represented bysequences of words, then the sequence "olefin poly-merisation" always occurs whenever the sequence"high temperature olefin polymerisation" occurs.
Inthis case, it might be thought to pay to look onlyat the most specific diagnostic units since we haveif wl --* w2, then P(Z\ ]wlw2C)  = P (X Iw lC)  forany distribution P whatsoever (here, C representsany other contextual information we have, for exam-ple the other diagnostic units representing the doc-ument).
However, if wl is significantly less frequentthan w2 estimation errors of P(d\[wl) will be largerfor P(dlw2) for any descriptor d, so there may not bea significant advantage.
However, it does give us a1 One classic example is the case of the "New Hamp-shire Yankee Power Plant".
In a collection of New YorkTimes articles tudied by Jacobs & Rau (1990), the word"Yankee" was found to predict NUCLEAR POWER becauseof the frequent occurrence of articles about this plant.However, "Yankee" on its own without the other words inthis phrase is a good predictor of articles about the NewYork Yankees, a baseball team.
If highly mutually infor-mative words "are combined into conjunctive predicates(e.g.
"Yankee" E x & "Plant" E x), and a documentis represented by its most specific predicates only, thenwhen "Yankee" appears alone, it will be a good predic-tor of the descriptor SPORT.
This example can also showthat the bound described above is tight.
Imagine (sus-pending belief) that each of the five words in the phrasehave the same number of occurrences, i, in the documentcollection without NUCLEAR POWER where they never oc-cur together palrwise, and always occur all together inj true positives of the descriptor.
Then the precision ofassigning NUCLEAR POWER if any one of them appearsin a document is j+51-'-2--, and since e in this case is i+--~, thebound follows (for the case n = 5) with a little algebra.theoretical reason to believe that representing a doc-ument by its set of most specific predicates i worthinvestigating, and this shall be investigated below.If one considers a calculus similar to the one de-scribed here, but allows ~ to limit to 0, then aweak default logic ensues which has been studiedby Adams (1975), and further investigated by Pearl(1988).4 Adding linguistic descriptionThe simplest way of representing a document is asa set or multi set of words.
Many people (eg.
Lewis1992bc; Jacobs & Rau 1990) have suggested that amore linguistically sophisticated representation f adocument might be more effective for the purposes ofstatistical keyword assignment.
Unfortunately, at-tempts to do this have not been found to reliablyimprove performance as measured by recall and pre-cision for the task of document classification.
I shallpresent evidence that a more sophisticated repre-sentation makes better predictions from the SingleTerm model defined above than it does from stan-dard term weighting models.4.1 Linguistic descriptionThe simplest form of linguistic description of thecontent of a machine-readable document is in theform of a sequence (or a set) of words.
More so-phisticated linguistic information comes in severalforms, all of which may need to be represented ifperformance in an automatic ategorisation exper-iment is to be improved.
Typical examples of lin-guistically sophisticated annotation include taggingwords with their syntactic category (although thishas not been found to be effective for 1R), lemma ofthe word (e.g.
"corpus" for "corpora"), phrasal in-formation (e.g.
identifying noun groups and phrases(Lewis 1992c, Church 1988)), and subject-predicateidentification (e.g.
Hindle 1990).
For the RAPRAcorpus, we currently identify noun groups and ad-jective groups.This is achieved in a manner similar to Church's(1988) PARTS algorithm used by Lewis (1992bc),in the sense that its main properties are robustnessand corpus sensitivity.
All that is important for thispaper is that the technique identifies various group-ings of words (for example, noun-groups, adjectivegroups, and so on) with a high level of accuracy.Major parts of the technique are described in detailin Finch, 1993.
As an example, this is some of thelinguistic markup which represents the title of thesample document shown earlier.?
macromolecular deformation (NG); macromolecular defor-mation model (NG); deformation (NG); deformation model(NG); model (NG); viscoelastic flow (NG); viscoelastic floweffects (NGS); flow (NG); flow effects (NGS); effects (NGS);polymer (NG); polymer melts (NGS); melts (NGS)It is clear that the markup is far from sophisti-cated, and is very much a small variation on a sim-ple sequence-based representation.
Nevertheless, itis fairly accurate in so much as well over 90% ofwhat are claimed to be noun groups can be inter-preted as such.
One very useful by-product of usinga linguistically based representation is that Il~ canhelp in linguistic tasks such as terminological col-lection.
I shall present some examples of diagnosticunits which are highly associated with descriptorslater.5 Predicting from sophisticatedrepresentationsIn what follows, we shall compare the relative per-formance of a term weighting model with the singleterm model as we vary the sophistication of repre-sentation.Proportional ssignmen~ (Lewis 1992b) is used toassign the descriptors from statistical measurementsof their appropriateness.
This method ensures thatroughly the same number of assignments of particu-lar descriptors are made as are actually made in thetest corpus.
The strategy is simply to assign descrip-tor d to the N documents which score highest forthis descriptor, where N is chosen in proportion tothe occurrence of d in the training corpus.
For termweighting models, the score is simply the combinedweight of the document; for the single term model,the score is sup~eT?
(~ ) P(dlw).
The Rule Based as-signment strategy applies only to the single termmodel and the rule w --~ d is included just in caseP(dlw )> 1-?.Figure 1 shows a few of the rules.
All of theseentries share the property that P(d\]w) > 0.8.
Theywere selected at random from the 85,500 associationswhich were found.5.1 Representations and modelsFive paradigms of representation of documents willbe compared, and two term appropriateness modelswill be compared.
This gives us ten combinations.The first representation paradigm is a baseline one:represent documents as the set of the words con-tained in them.
The second paradigm is to repre-sent documents according to word sequences, andthe third is to apply a noun-group and adjective-group recogniser.
The fourth and fifth representa-tion modes consider epresenting documents by onlytheir most specific diagnostic units.
For example, ifthe sequence "thermoplastic elastomer compounds"68polymer materials Research/NG;EEC legislation/NGS;venture partners/NGS;Bergen op/NPsheet lines/NGSrailroad/NGinjection moulding fa~:ility/NGPHENOLPHTHALEIN/NPunsaturated polyester composites/NGSthermoplastic elastomer compounds/NGSproperties features/NGSfiber Glass/NGcomparative performance/NGautomotive hose/NGSBitruder/NPworldwide tyre/NGVictrex polyethersulphone/NPPS melts/NGSviscoelastic haracteristics/NGSplastics waste/NGlattice relaxation/NGfatigue crack propagation/NGunidirectional composites/NGSFlory Huggins interaction/NGDATA"-* LEGISLATION--* JOINT VENTURE--* PLASTIC---* COMPANY--* COMPANY--* PLASTIC--* DATA--~ THERMOSET---* RUBBER---* PLASTIC--* GLASS FIBRE REINFORCED PLASTIC--* DATA--, RUBBER--, EXTRUDERCOMPANIES-'~ COMPANIES---+ PLASTIC---* VISCOELASTIC PROPERTIESRECYCLINGNUCLEAR MAGNETIC RESONANCE"--+ MECHANICAL PROPERTIESREINFORCED PLASTICTECHNICALFigure 1: This figure shows some probably correct rules for the RAPRA corpus.
In all, there are over 85,000such rules.appeared in the abstract, then ordinarily this wouldinclude the sequence "elastomer compounds", whichwould be included in the representation.
The resultsof section 3.2 might encourage us to believe that rep-resenting a document by only its most specific diag-nostic units will improve performance (or, at least,precision).
Consequently, a sequence of words is de-fined to be most specific if (a) it is a diagnostic unitand (b) it is not properly contained in a token of anyother diagnostic unit present in the document.
2The noun-groups are found by performing a sim-ple parse of the documents as described above, andidentifying likely noun groups of length 3 or less.The contingency table of diagnostic units versesmanually assigned escriptors on a training corpus of200,000 documents was collected, and this was usedas the basis for two term appropriateness models.Probabilities were estimated by adding a constant(usually 0.02 was found fairly optimal) to each cell,and directly estimating from these slightly adjustedcounts.The 50,000 most frequent diagnostic unit typeswere chosen, and terms which appeared in more than10% of documents were discarded.2If "elastomer compounds" appeared separately inthe document from "thermoplastic elastomer com-pounds", then both of these sequences would be rep-resented in the experiments reported here.6 ResultsThe results of the experiments on the RAPRA cor-pus are presented below.
3Despite the peculiarities of the corpus, the mes-sage is clear.
The result that the standard modelfares no better on word sequence sets than on wordsets is repeated, and it is clear that the Single Termmodel fares much better than the Logit model onthis data set.
However, what is most interesting isthat the Single Term models fares significantly bet-ter on the more sophisticated sequence based repre-sentations of the document than on the simpler wordbased representation.
There is, however, no signifi-cant advantage identified by parsing the corpus intonoun-groups over simply considering all word se-quences.
The recall scores for the rule-based taggingstrategy show that the improved performance of thesequence based representations can be explained by3All recall and precision scores are microaveraged(Lewis 1992c); they are the expected probability of as-signing or recalling correctly per tagging decision.
Thetraining set was a set of 200,000 abstracts, and the sep-arate test set had 10,000 abstracts.
The experimentslooked at only the 520 most common descriptors.
Inthe table, TW means that a term-weighting model wasused, while ST means that the single term model wasused.
'Word' means the representation was a wordset,'Seq', the set of all sequences, and 'NG' the set of groupsderived from the grammar.
For the sequence represen-tations, either all the possible sequences or groups wereused (denoted by 'all'), or just the most specific oneswere used (denoted by 'spec').69the presence of many more "good" descriptor indi-cators.AssignmentProp.Prop.Prop.Prop.Prop.Prop.Prop.Prop.Prop.Rule ?
= .2Rule ?
= .2Rule ?
= .2Rule ?
= .2Rule ?
= .2ModelTWTWTWTWTWSTSTSTSTSTSTSTSTSTRepn Prec RecWord 33% 32%Seq all 32% 34%Seq spec 33% 34%NG all 31% 36%NG spec 32% 32%Word 54% 48%Seq all 57% 55%Seq spec 55% 55%NG 56% 60%Word 83% 7%Seq all 77% 42%Seq spec 80% 40%NG all 82% 42%NG spec 84% 37%7 Conc lus ionThe significant heoretical result is that as the so-phistication of the representation of abstracts is in-creased, the performance of the single term modelimproves, while the performance of the term weight-ing models does not improve significantly.
Thishas been a fairly universal experience among re-searchers working within the term weighting clas-sification paradigm.Although there is a very marginally significantimprovement from using linguistically sophisticatedrepresentations over simple sequence representationsif all of the sequences are represented, this largely(though not entirely) disappears when only mostspecific sequences are considered, so it might be aresult of the effects discussed in section 3.2.The rule based assignment strategy exploits theSingle Term model's estimates, and also performsmuch better on word sequence representations thanon word set representations.
This assignment strat-egy is promising because it can exploit more sophis-ticated representations well, has a sound theory be-hind it, and will assign descriptors only where it hasenough information to do so.
Some of the descrip-tors in the RAPRA corpus, for example, are onlyever assigned from the entire article from which theabstract is taken, so no assignment strategy will everdo well on these.
On the other hand this model alsoshows promise that IR techniques might be appliedto help infer linguistic resources uch as term banksfrom large classified corpora.The next stage is to add more sophisticated lin-guistic annotation to corpora, and to trawl for rulesin boolean combinations of descriptors, thus ad-dressing the results of section 3.2.
In this way thiswork can be considered similar in spirit to that un-dertaken by Apte et al(1994), but differs in theforms of representation which are being consideredfor documents.Re ferencesAdams, E. (1975) The Logic of Conditionals: an appli-cation of probability to deductive logic Reidel.Apte, C, F. Demerau & S. Weiss (1994) Towards Lan-guage Independent Automated Learning of Text Cate-gorization Methods.
the proceeding of the SeventeenthACM-SIGIR Conference on Information Retrieval.
23-30, DCU, Dublin.Buckley, C. (1993) The Importance of Proper WeightingMethods.
ARPA Workshop on Human Language Tech-nology.Church, K. (1988) A stochastic parts program and nounphrase parser for unrestricted text.
In Second conferenceon applied NLP, pp 136-43.Church, K., W. Gale, P. Hanks & D. Hindle (1989) Pars-ing, Word Associations and Typical Predicate-ArgumentRelations.
In International Parsing Technologies Work-shop.
CMU, Pittsburgh.Fagan, J.
(1987) Experiments in Automatic Phrase In-dexing for Document Retrieval: Comparison of Syntacticand Non-Syntactic Methods.
PhD Thesis.
Cornell Uni-versity, Dept.
of Computer Science.Finch, S. P. & N. Chater (1991) A Hybrid Approach tothe Automatic Learning of Linguistic Categorie s .
Artifi-cial Intelligence and Simulated Behaviour Quarterly.
7816-24.Finch, S. (1993) Finding Structure in Language.
Ph.D.thesis, Centre for Cognitive Science, University of Edin-burgh, Edinburgh.Fuhr, N. (1989) Models for retrieval with probabilistic in-dexing.
Information processing and management.
25(1):55-72.Fuhr, N. & Buckley, C (1993) Optimizing Document In-dexing and Search Term Weighting Based on Probabilis-tic Models First TREC Conference.Hindle, D. (1990) Noun Classification from Predicate-Argument Structures.
In Proceedings of the 22nd meet-ing of the Association of Computational Linguistics.268-75.Jacobs, P. & Rau, L. (1990) SCISOR: Extracting Infor-mation from On-line News Correspondence of the A CM33 11 88-97Kupiec, J.
(1992) Robust part-of-speech tagging Using ahidden Markov model.
Computer Speech and Language,6:3 225-42.Lewis, D. (1991) Evaluating text categorisation.
InSpeech and natural anguage workshop, pp 136-143.Lewis, D. (1992a) Representation a d learning in infor-mation retrieval.
Ph.D. thesis, Computer Science Dept.,Univ.
Mass., Amherst, Ma.Lewis, D. (1992b) An Evaluation of Phrasal and Clus-tered Representations on a Text categorization problem.Proceedings of SIGIR 92.Lewis, D. (1992c) Feature selection and feature extrac-tion for text categorization.
In Speech and Natural Lan-guage: Proceedings of a Workshop held at Harrimn, NY.pp 212-217.Lewis, D. & K. Sparck-Jones (1993) Natural languageprocessing for information retrieval University of Cam-bridge Technical report 307, Cambridge.70Pearl, J.
(1988) Probabilistic Reasoning in IntelligentSystems: Networks of Plausible Inference Morgan Kauf-mann, San Mateo, Ca.van Rijsbergen, C. J.
(1979) Information retrieval.
But-terworths, London.Sacks-Davis, R. (1990) Using Syntactic Analysis in aDocument Retrieval System that Uses Signature Files.A CM SIGIR- 9O.Salton, G. & McGill, M. J.
(1983) Introduction to mod-ern information retrieval.
McGraw-Hill, NY.Salton, G. & C. Buckley (1988) Term Weighting Ap-proaches in Automatic Text Retrieval Information Pro-cessing and Management 24 5 513-23Zadeh, L. (1965) Fuzzy Sets Information and control, bf8 338-53.71
