Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 517?523,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAutomatic prediction of aspectual class of verbs in contextAnnemarie Friedrich and Alexis PalmerDepartment of Computational LinguisticsSaarland University, Saarbr?ucken, Germany{afried,apalmer}@coli.uni-saarland.deAbstractThis paper describes a new approach topredicting the aspectual class of verbs incontext, i.e., whether a verb is used in astative or dynamic sense.
We identify twochallenging cases of this problem: whenthe verb is unseen in training data, andwhen the verb is ambiguous for aspec-tual class.
A semi-supervised approach us-ing linguistically-motivated features and anovel set of distributional features basedon representative verb types allows us topredict classes accurately, even for unseenverbs.
Many frequent verbs can be eitherstative or dynamic in different contexts,which has not been modeled by previouswork; we use contextual features to re-solve this ambiguity.
In addition, we intro-duce two new datasets of clauses markedfor aspectual class.1 IntroductionIn this work, we focus on the automatic predictionof whether a verb in context is used in a stative orin a dynamic sense, the most fundamental distinc-tion in all taxonomies of aspectual class.
The as-pectual class of a discourse?s finite verbs is an im-portant factor in conveying and interpreting tem-poral structure (Moens and Steedman, 1988; Dorr,1992; Klavans and Chodorow, 1992); others aretense, grammatical aspect, mood and whether theutterance represents an event as completed.
Moreaccurate temporal information processing is ex-pected to be beneficial for a variety of natural lan-guage processing tasks (Costa and Branco, 2012;UzZaman et al, 2013).While most verbs have one predominant inter-pretation, others are more flexible for aspectualclass and can occur as either stative (1) or dynamic(2) depending on the context.
There are also casesthat allow for both readings, such as (3).
(1) The liquid fills the container.
(stative)(2) The pool slowly filled with water.
(dynamic)(3) Your soul was made to be filled with GodHimself.
(both) (Brown corpus, religion)Cases like (3) do not imply that there is a thirdclass, but rather that two interpretations are avail-able for the sentence, of which usually one will bechosen by a reader.Following Siegel and McKeown (2000), we aimto automatically classify clauses for fundamentalaspectual class, a function of the main verb anda select group of complements, which may dif-fer per verb (Siegel and McKeown, 2000; Siegel,1998b).
This corresponds to the aspectual classof the clause?s main verb when ignoring any as-pectual markers or transformations.
For exam-ple, English sentences with perfect tense are usu-ally considered to introduce states to the discourse(Smith, 1991; Katz, 2003), but we are interested inthe aspectual class before this transformation takesplace.
The clause John has kissed Mary introducesa state, but the fundamental aspectual class of the?tenseless?
clause John kiss Mary is dynamic.In contrast to Siegel and McKeown (2000), wedo not conduct the task of predicting aspectualclass solely at the type level, as such an approachignores the minority class of ambiguous verbs.
In-stead we predict the aspectual class of verbs inthe context of their arguments and modifiers.
Weshow that this method works better than using onlytype-based features, especially for verbs with am-biguous aspectual class.
In addition, we showthat type-based features, including novel distribu-tional features based on representative verbs, accu-rately predict predominant aspectual class for un-seen verb types.
Our work differs from prior workin that we treat the problem as a three-way clas-sification task, predicting DYNAMIC, STATIVE orBOTH as the aspectual class of a verb in context.5172 Related workAspectual class is well treated in the linguistic lit-erature (Vendler, 1957; Dowty, 1979; Smith, 1991,for example).
Our notion of the stative/dynamicdistinction corresponds to Bach?s (1986) distinc-tion between states and non-states; to states ver-sus occurrences (events and processes) accordingto Mourelatos (1978); and to Vendler?s (1957) dis-tinction between states and the other three classes(activities, achievements, accomplishments).Early studies on the computational modelingof aspectual class (Nakhimovsky, 1988; Passon-neau, 1988; Brent, 1991; Klavans and Chodorow,1992) laid foundations for a cluster of papers pub-lished over a decade ago (Siegel and McKeown,2000; Siegel, 1998b; Siegel, 1998a).
Since then,it has mostly been treated as a subtask withintemporal reasoning, such as in efforts related toTimeBank (Pustejovsky et al, 2003) and the Tem-pEval challenges (Verhagen et al, 2007; Verha-gen et al, 2010; UzZaman et al, 2013), wheretop-performing systems (Jung and Stent, 2013;Bethard, 2013; Chambers, 2013) use corpus-basedfeatures, WordNet synsets, parse paths and fea-tures from typed dependencies to classify eventsas a joint task with determining the event?s span.Costa and Branco (2012) explore the usefulness ofa wider range of explicitly aspectual features fortemporal relation classification.Siegel and McKeown (2000) present the mostextensive study of predicting aspectual class,which is the main inspiration for this work.
Whileall of their linguistically motivated features (seesection 4.1) are type-based, they train on and eval-uate over labeled verbs in context.
Their dataset taken from medical discharge summaries com-prises 1500 clauses containing main verbs otherthan be and have which are marked for aspectualclass.
Their model fails to outperform a baselineof memorizing the most frequent class of a verbtype, and they present an experiment testing on un-seen verb types only for the related task of classi-fying completedness of events.
We replicate theirmethod using publicly available software, createa similar but larger corpus,1and show that it isindeed possible to predict the aspectual class ofunseen verbs.
Siegel (1998a) investigates a classi-fication method for the verb have in context; in-1Direct comparison on their data is not possible; featurevalues for the verbs studied are available, but full texts andthe English Slot Grammar parser (McCord, 1990) are not.COMPLETE W/O have/be/nonegenre clauses ?
clauses ?jokes 3462 0.85 2660 0.77letters 1848 0.71 1444 0.62news 2565 0.79 2075 0.69all 7875 0.80 6161 0.70Table 1: Asp-MASC: Cohen?s observed un-weighted ?.DYNAMIC STATIVE BOTHDYNAMIC 4464 164 9STATIVE 434 1056 29BOTH 5 0 0Table 2: Asp-MASC: confusion matrix for twoannotators, without have/be/none clauses, ?
is 0.7.spired by this work, our present work goes onestep further and uses a larger set of instance-basedcontextual features to perform experiments on aset of 20 verbs.
To the best of our knowledge, thereis no previous work comprehensively addressingaspectual classification of verbs in context.3 DataVerb type seed sets Using the LCS Database(Dorr, 2001), we identify sets of verb types whosesenses are only stative (188 verbs, e.g.
belong,cost, possess), only dynamic (3760 verbs, e.g.
al-ter, knock, resign), or mixed (215 verbs, e.g.
fill,stand, take), following a procedure described byDorr and Olsen (1997).Asp-MASC The Asp-MASC corpus consists of7875 clauses from the letters, news and jokes sec-tions of MASC (Ide et al, 2010), each labeledby two annotators for the aspectual class of themain verb.2Texts were segmented into clauses us-ing SPADE (Soricut and Marcu, 2003) with someheuristic post-processing.
We parse the corpus us-ing the Stanford dependency parser (De Marneffeet al, 2006) and extract the main verb of each seg-ment.
We use 6161 clauses for the classificationtask, omitting clauses with have or be as the mainverb and those where no main verb could be iden-tified due to parsing errors (none).
Table 1 showsinter-annotator agreement; Table 2 shows the con-fusion matrix for the two annotators.
Our two an-notators exhibit different preferences on the 598cases where they disagree between DYNAMIC andSTATIVE.
Such differences in annotation prefer-2Corpus freely available fromwww.coli.uni-saarland.de/?afried.518DYNAMIC STATIVE BOTHDYNAMIC 1444 201 54STATIVE 168 697 20BOTH 44 31 8Table 3: Asp-Ambig: confusion matrix for twoannotators.
Cohen?s ?
is 0.6.ences are not uncommon (Beigman Klebanov etal., 2008).
We observe higher agreement in thejokes and news subcorpora than for letters; textsin the letters subcorpora are largely argumentativeand thus have a different rhetorical style than themore straightforward narratives and reports foundin jokes.
Overall, we find substantial agreement.The data for our experiments uses the label DY-NAMIC or STATIVE whenever annotators agree,and BOTH whenever they disagree or when at leastone annotator marked the clause as BOTH, assum-ing that both readings are possible in such cases.Because we don?t want to model the authors?
per-sonal view of the theory, we refrain from applyingan adjudication step and model the data as is.Asp-Ambig: (Brown) In order to facilitate afirst study on ambiguous verbs, we select 20 fre-quent verbs from the list of ?mixed?
verbs (seesection 3) and for each mark 138 sentences.
Sen-tences are extracted randomly from the Brown cor-pus, such that the distribution of stative/dynamicusages is expected to be natural.
We presententire sentences to the annotators who mark theaspectual class of the verb in question as high-lighted in the sentence.
The data is processed inthe same way as Asp-MASC, discarding instanceswith parsing problems.
This results in 2667 in-stances.
?
is 0.6, the confusion matrix is shown inTable 3.
Details are listed in Table 10.4 Model and FeaturesFor predicting the aspectual class of verbs in con-text (STATIVE, DYNAMIC, BOTH), we assume asupervised learning setting and explore featuresmined from a large background corpus, distribu-tional features, and instance-based features.
If notindicated otherwise, experiments use a RandomForest classifier (Breiman, 2001) trained with theimplementation and standard parameter settingsfrom Weka (Hall et al, 2009).4.1 Linguistic indicator features (LingInd)This set of corpus-based features is a reimple-mentation of the linguistic indicators of SiegelFEATURE EXAMPLE FEATURE EXAMPLEfrequency - continuous continuallypresent says adverb endlesslypast said evaluation betterfuture will say adverb horriblyperfect had won manner furiouslyprogressive is winning adverb patientlynegated not/never temporal againparticle up/in/... adverb finallyno subject - in-PP in an hourfor-PP for an hourTable 4: LingInd feature set and examples for lex-ical items associated with each indicator.FEATURE VALUESpart-of-speech tag of the verb VB, VBG, VBN, ...tense present, past, futureprogressive true/falseperfect true/falsevoice active/passivegrammatical dependents WordNet lexname/POSTable 5: Instance-based (Inst) featuresand McKeown (2000), who show that (some of)these features correlate with either stative or dy-namic verb types.
We parse the AFE and XIE sec-tions of Gigaword (Graff and Cieri, 2003) withthe Stanford dependency parser.
For each verbtype, we obtain a normalized count showing howoften it occurs with each of the indicators in Ta-ble 4, resulting in one value per feature per verb.For example, for the verb fill, the value of thefeature temporal-adverb is 0.0085, meaningthat 0.85% of the occurrences of fill in the corpusare modified by one of the temporal adverbs on thelist compiled by Siegel (1998b).
Tense, progres-sive, perfect and voice are extracted using a set ofrules following Loaiciga et al (2014).34.2 Distributional Features (Dist)We aim to leverage existing, possibly noisy setsof representative stative, dynamic or mixed verbtypes extracted from LCS (see section 3), mak-ing up for unseen verbs and noise by averagingover distributional similarities.
Using an exist-ing large distributional model (Thater et al, 2011)estimated over the set of Gigaword documentsmarked as stories, for each verb type, we builda syntactically informed vector representing thecontexts in which the verb occurs.
We computethree numeric feature values per verb type, whichcorrespond to the average cosine similarities withthe verb types in each of the three seed sets.3We thank the authors for providing us their code.519FEATURES ACCURACY (%)Baseline (Lemma) 83.6LingInd 83.8Inst 70.8Inst+Lemma 83.7Dist 83.4LingInd+Inst+Dist+Lemma 84.1Table 6: Experiment 1: SEEN verbs, using Asp-MASC.
Baseline memorizes most frequent classper verb type in training folds.4.3 Instance-based features (Inst)Table 5 shows our set of instance-based syntac-tic and semantic features.
In contrast to the abovedescribed type-based features, these features donot rely on a background corpus, but are ex-tracted from the clause being classified.
Tense,progressive, perfect and voice are extracted fromdependency parses as described above.
For fea-tures encoding grammatical dependents, we focuson a subset of grammatical relations.
The fea-ture value is either the WordNet lexical filename(e.g.
noun.person) of the given relation?s argu-ment or its POS tag, if the former is not avail-able.
We simply use the most frequent sense forthe dependent?s lemma.
We also include featuresthat indicate, if there are any, the particle of theverb and its prepositional dependents.
For thesentence A little girl had just finished her firstweek of school, the instance-based feature valueswould include tense:past, subj:noun.person,dobj:noun.time or particle:none.5 ExperimentsThe experiments presented in this section aim toevaluate the effectiveness of the feature sets de-scribed in the previous section, focusing on thechallenging cases of verb types unseen in the train-ing data and highly ambiguous verbs.
The featureLemma indicates that the verb?s lemma is used asan additional feature.Experiment 1: SEEN verbsThe setting of our first experiment follows Siegeland McKeown (2000).
Table 6 reports results for10-fold cross-validation, with occurrences of allverbs distributed evenly over the folds.
No featurecombination significantly4outperforms the base-line of simply memorizing the most frequent class4According to McNemar?s test with Yates?
correction forcontinuity, p < 0.01.FEATURES ACCURACY (%)1 Baseline 72.52 Dist 78.3?3 LingInd 80.4?4 LingInd+Dist 81.9*?Table 7: Experiment 2: UNSEEN verb types, Lo-gistic regression, Asp-MASC.
Baseline labels ev-erything with the most frequent class in the train-ing set (DYNAMIC).
*Significantly4different fromline 1.
?Significantly4different from line 3.DATA FEATURES ACC.
(%)one-label Baseline 92.8verbs LingInd 92.8Dist 92.6(1966 inst.)
Inst+Lemma 91.4?LingInd+Inst+Lemma 92.4multi-label Baseline 78.9verbs LingInd 79.0Dist 79.0(4195 inst.)
Inst 67.4?Inst+Lemma 79.9LingInd+Inst+Lemma 80.9*LingInd+Inst+Lemma+Dist 80.2*Table 8: Experiment 3: ?ONE- VS. MULTI-LABEL?
verbs, Asp-MASC.
Baseline as in Table6.
*Indicates that result is significantly4differentfrom the respective baseline.of a verb type in the respective training folds.Experiment 2: UNSEEN verbsThis experiment shows a successful case of semi-supervised learning: while type-based feature val-ues can be estimated from large corpora in an un-supervised way, some labeled training data is nec-essary to learn their best combination.
This exper-iment specifically examines performance on verbsnot seen in labeled training data.
We use 10-foldcross validation but ensure that all occurrences ofa verb type appear in the same fold: verb typesin each test fold have not been seen in the re-spective training data, ruling out the Lemma fea-ture.
A Logistic regression classifier (Hall et al,2009) works better here (using only numeric fea-tures), and we present results in Table 7.
Both theLingInd and Dist features generalize across verbtypes, and their combination works best.Experiment 3: ONE- vs. MULTI-LABEL verbsFor this experiment, we compute results sepa-rately for one-label verbs (those for which all in-stances in Asp-MASC have the same label) and520SYSTEM CLASS ACC.
P R Fbaseline micro-avg.
78.9 0.75 0.79 0.76LingInd DYNAMIC 0.84 0.95 0.89+Inst STATIVE 0.76 0.69 0.72+Lemma BOTH 0.51 0.24 0.33micro-avg.
80.9* 0.78 0.81 0.79Table 9: Experiment 3: ?MULTI-LABEL?, preci-sion, recall and F-measure, detailed class statisticsfor the best-performing system from Table 8.for multi-label verbs (instances have differing la-bels in Asp-MASC).
We expect one-label verbsto have a strong predominant aspectual class, andmulti-label verbs to be more flexible.
Otherwise,the experimental setup is as in experiment 1.
Re-sults appear in Table 8.
In each case, the linguisticindicator features again perform on par with thebaseline.
For multi-label verbs, the feature combi-nation Lemma+LingInd+Inst leads to significant4improvement of 2% gain in accuracy over thebaseline; Table 9 reports detailed class statisticsand reveals a gain in F-measure of 3 points overthe baseline.
To sum up, Inst features are essentialfor classifying multi-label verbs, and the LingIndfeatures provide some useful prior.
These resultsmotivate the need for an instance-based approach.Experiment 4: INSTANCE-BASED classificationFor verbs with ambiguous aspectual class, type-based classification is not sufficient, as this ap-proach selects a dominant sense for any given verband then always assigns that.
Therefore we pro-pose handling ambiguous verbs separately.
AsAsp-MASC contains only few instances of each ofthe ambiguous verbs, we turn to the Asp-Ambigdataset.
We perform a Leave-One-Out (LOO)cross validation evaluation, with results reportedin Table 10.5Using the Inst features alone (notshown in Table 10) results in a micro-average ac-curacy of only 58.1%: these features are only use-ful when combined with the feature Lemma.
Forclassifying verbs whose most frequent class oc-curs less than 56% of the time, Lemma+Inst fea-tures are essential.
Whether or not performanceis improved by adding LingInd/Dist features, withtheir bias towards one aspectual class, dependson the verb type.
It is an open research questionwhich verb types should be treated in which way.5The third column also shows the outcome of using ei-ther only the Lemma, only LingInd or only Dist in LOO; allhave almost the same outcome as using the majority class,numbers differ only after the decimal point.Inst+LemmaInst+Lemma+LingInd+Dist# OF MAJORITYVERB INST.
CLASS5feel 128 96.1 STAT 93.0 93.8say 138 94.9 DYN 93.5 93.5make 136 91.9 DYN 91.9 91.2come 133 88.0 DYN 87.2 87.2take 137 85.4 DYN 85.4 85.4meet 130 83.9 DYN 86.2 87.7stand 130 80.0 STAT 79.2 83.1find 137 74.5 DYN 69.3 68.8accept 134 70.9 DYN 64.9 65.7hold 134 56.0 BOTH 43.3 49.3carry 136 55.9 DYN 55.9 58.1look 138 55.8 DYN 72.5 74.6show 133 54.9 DYN 69.2 68.4appear 136 52.2 STAT 64.7 61.0follow 122 51.6 BOTH 69.7 65.6consider 138 50.7 DYN 61.6 70.3cover 123 50.4 STAT 46.3 54.5fill 134 47.8 DYN 66.4 62.7bear 135 47.4 DYN 70.4 67.4allow 135 37.8 DYN 48.9 51.9micro-avg.
2667 66.3 71.0* 72.0*Table 10: Experiment 4: INSTANCE-BASED.Accuracy (in %) on Asp-Ambig.
*Differssignificantly4from the majority class baseline.6 Discussion and conclusionsWe have described a new, context-aware approachto automatically predicting aspectual class, includ-ing a new set of distributional features.
We havealso introduced two new data sets of clauses la-beled for aspectual class.
Our experiments showthat in any setting where labeled training datais available, improvement over the most frequentclass baseline can only be reached by integratinginstance-based features, though type-based fea-tures (LingInd, Dist) may provide useful priorsfor some verbs and successfully predict predom-inant aspectual class for unseen verb types.
In or-der to arrive at a globally well-performing system,we envision a multi-stage approach, treating verbsdifferently according to whether training data isavailable and whether or not the verb?s aspectualclass distribution is highly skewed.Acknowledgments We thank the anonymousreviewers, Omri Abend, Mike Lewis, ManfredPinkal, Mark Steedman, Stefan Thater and BonnieWebber for helpful comments, and our annotatorsA.
Kirkland and R. K?uhn.
This research was sup-ported in part by the MMCI Cluster of Excellence,and the first author is supported by an IBM PhDFellowship.521ReferencesEmmon Bach.
1986.
The algebra of events.
Linguis-tics and philosophy, 9(1):5?16.Beata Beigman Klebanov, Eyal Beigman, and DanielDiermeier.
2008.
Analyzing disagreements.
In Pro-ceedings of the Workshop on Human Judgements inComputational Linguistics, pages 2?7.
Associationfor Computational Linguistics.Steven Bethard.
2013.
ClearTK-TimeML: A minimal-ist approach to TempEval 2013.
In Second JointConference on Lexical and Computational Seman-tics (* SEM), volume 2, pages 10?14.Leo Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1):5?32.Michael R. Brent.
1991.
Automatic semantic classifi-cation of verbs from their syntactic contexts: an im-plemented classifier for stativity.
In Proceedings ofthe fifth conference on European chapter of the As-sociation for Computational Linguistics, pages 222?226.
Association for Computational Linguistics.Nathanael Chambers.
2013.
Navytime: Event andtime ordering from raw text.
In Second Joint Con-ference on Lexical and Computational Semantics (*SEM), volume 2, pages 73?77.Francisco Costa and Ant?onio Branco.
2012.
Aspec-tual type and temporal relation classification.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 266?275.
Association for Computa-tional Linguistics.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al 2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proceedings of LREC, volume 6, pages449?454.Bonnie J. Dorr and Mari Broman Olsen.
1997.
De-riving verbal and compositional lexical aspect forNLP applications.
In Proceedings of the eighth con-ference on European chapter of the Association forComputational Linguistics, pages 151?158.
Associ-ation for Computational Linguistics.Bonnie J. Dorr.
1992.
A two-level knowledge repre-sentation for machine translation: lexical semanticsand tense/aspect.
In Lexical Semantics and Knowl-edge Representation, pages 269?287.
Springer.Bonnie J. Dorr.
2001.
LCS verb database.
Onlinesoftware database of Lexical Conceptual Structures,University of Maryland, College Park, MD.David Dowty.
1979.
Word Meaning and MontagueGrammar.
Reidel, Dordrecht.David Graff and Christopher Cieri.
2003.
English gi-gaword.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.ACM SIGKDD explorations newsletter, 11(1):10?18.Nancy Ide, Christiane Fellbaum, Collin Baker, and Re-becca Passonneau.
2010.
The manually annotatedsub-corpus: a community resource for and by thepeople.
In Proceedings of the ACL 2010 ConferenceShort Papers, pages 68?73.
Association for Compu-tational Linguistics.Hyuckchul Jung and Amanda Stent.
2013.
ATT1:Temporal annotation using big windows and richsyntactic and semantic features.
In Second JointConference on Lexical and Computational Seman-tics (* SEM), volume 2, pages 20?24.Graham Katz.
2003.
On the stativity of the englishperfect.
Perfect explorations, pages 205?234.Judith L. Klavans and Martin Chodorow.
1992.
De-grees of stativity: the lexical representation of verbaspect.
In Proceedings of the 14th conference onComputational Linguistics, pages 1126?1131.
Asso-ciation for Computational Linguistics.Sharid Loaiciga, Thomas Meyer, and Andrei Popescu-Belis.
2014.
English-French Verb Phrase Align-ment in Europarl for Tense Translation Modeling.In Language Resources and Evaluation Conference(LREC), Reykjavik, Iceland.Michael C. McCord.
1990.
Slot Grammar.
Springer.Marc Moens and Mark J. Steedman.
1988.
Tempo-ral ontology and temporal reference.
ComputationalLinguistics, 14(2):15?28.Alexander P.D.
Mourelatos.
1978.
Events, processes,and states.
Linguistics and philosophy, 2(3):415?434.Alexander Nakhimovsky.
1988.
Aspect, aspectualclass, and the temporal structure of narrative.
Com-putational Linguistics, 14(2):29?43.Rebecca Passonneau.
1988.
A computational modelof the semantics of tense and aspect.
ComputationalLinguistics, Spring 1988.James Pustejovsky, Patrick Hanks, Roser Sauri, An-drew See, Robert Gaizauskas, Andrea Setzer,Dragomir Radev, Beth Sundheim, David Day, LisaFerro, et al 2003.
The timebank corpus.
In Corpuslinguistics, volume 2003, page 40.Eric V. Siegel and Kathleen R. McKeown.
2000.Learning methods to combine linguistic indica-tors: Improving aspectual classification and reveal-ing linguistic insights.
Computational Linguistics,26(4):595?628.522Eric V. Siegel.
1998a.
Disambiguating verbs with theWordNet category of the direct object.
In Proceed-ings of Workshop on Usage of WordNet in NaturalLanguage Processing Systems, Universite de Mon-treal.Eric V. Siegel.
1998b.
Linguistic Indicators forLanguage Understanding: Using machine learn-ing methods to combine corpus-based indicators foraspectual classification of clauses.
Ph.D. thesis,Columbia University.Carlota S. Smith.
1991.
The Parameter of Aspect.Kluwer, Dordrecht.Radu Soricut and Daniel Marcu.
2003.
Sentence leveldiscourse parsing using syntactic and lexical infor-mation.
In Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology-Volume 1, pages 149?156.
Associationfor Computational Linguistics.Stefan Thater, Hagen F?urstenau, and Manfred Pinkal.2011.
Word meaning in context: A simple and ef-fective vector model.
In IJCNLP, pages 1134?1143.Naushad UzZaman, Hector Llorens, Leon Derczyn-ski, Marc Verhagen, James Allen, and James Puste-jovsky.
2013.
Semeval-2013 task 1: Tempeval-3:Evaluating time expressions, events, and temporalrelations.
In Second joint conference on lexical andcomputational semantics (* SEM), volume 2, pages1?9.Zeno Vendler, 1957.
Linguistics in Philosophy, chapterVerbs and Times, pages 97?121.
Cornell UniversityPress, Ithaca, New York.Marc Verhagen, Robert Gaizauskas, Frank Schilder,Mark Hepple, Graham Katz, and James Pustejovsky.2007.
Semeval-2007 task 15: Tempeval temporalrelation identification.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations,pages 75?80.
Association for Computational Lin-guistics.Marc Verhagen, Roser Sauri, Tommaso Caselli, andJames Pustejovsky.
2010.
SemEval-2010 task 13:TempEval-2.
In Proceedings of the 5th Interna-tional Workshop on Semantic Evaluation, pages 57?62.
Association for Computational Linguistics.523
