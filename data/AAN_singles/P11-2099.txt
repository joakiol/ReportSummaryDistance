Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 564?568,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLiars and Saviorsin a Sentiment Annotated Corpus of Comments to Political debatesPaula Carvalho Lu?s SarmentoUniversity of Lisbon Labs Sapo UP & University of PortoFaculty of Sciences, LASIGE Faculty of Engineering, LIACCLisbon, Portugal Porto, Portugalpcc@di.fc.ul.pt las@co.sapo.ptJorge TeixeiraM?rio J. SilvaLabs Sapo UP & University of Porto University of LisbonFaculty of Engineering, LIACC Faculty of Sciences, LASIGEPorto, Portugal Lisbon, Portugaljft@fe.up.pt mjs@di.fc.ul.ptAbstractWe investigate the expression of opinionsabout human entities in user-generated con-tent (UGC).
A set of 2,800 online newscomments (8,000 sentences) was manuallyannotated, following a rich annotationscheme designed for this purpose.
We con-clude that the challenge in performing opi-nion mining in such type of content iscorrectly identifying the positive opinions,because (i) they are much less frequentthan negative opinions and (ii) they are par-ticularly exposed to verbal irony.
We alsoshow that the recognition of human targetsposes additional challenges on mining opi-nions from UGC, since they are frequentlymentioned by pronouns, definite descrip-tions and nicknames.1 IntroductionMost of the existing approaches to opinion miningpropose algorithms that are independent of the textgenre, the topic and the target involved.
However,practice shows that the opinion mining challengesare substantially different depending on these fac-tors, whose interaction has not been exhaustivelystudied so far.This study focuses on identifying the most rele-vant challenges in mining opinions targeting mediapersonalities, namely politicians, in commentsposted by users to online news articles.
We areinterested in answering open research questionsrelated to the expression of opinions about humanentities in UGC.It has been suggested that the target identifica-tion is probably the easiest step in mining opinionson products using product reviews (Liu, 2010).But, is this also true for human targets namely formedia personalities like politicians?
How are theseentities mentioned in UGC?
What are the mostproductive forms of mention?
Is it a standardname, a nickname, a pronoun, a definite descrip-tion?
Additionally, it was demonstrated that ironymay influence the correct detection of positiveopinions about human entities (Carvalho et al,2009); however, we do not know the prevalence ofthis phenomenon in UGC.
Is it possible to establishany type of correlation between the use of ironyand negative opinions?
Finally, approaches to opi-nion mining have implicitly assumed that the prob-lem at stake is a balanced classification problem,based on the general assumption that positive andnegative opinions are relatively well distributed in564texts.
But, should we expect to find a balancednumber of negative and positive opinions in com-ments targeting human entities, or should we beprepared for dealing with very unbalanced data?To answer these questions, we analyzed a col-lection of comments posted by the readers of anonline newspaper to a series of 10 news articles,each covering a televised face-to-face debate be-tween the Portuguese leaders of five political par-ties.
Having in mind the previously outlinedquestions, we designed an original rich annotationscheme to label opinionated sentences targetinghuman entities in this corpus, named SentiCorpus-PT.
Inspection of the corpus annotations supportsthe annotation scheme proposed and helps to iden-tify directions for future work in this research area.2 Related WorkMPQA is an example of a manually annotatedsentiment corpus (Wiebe et al, 2005; Wilson et al,2005).
It contains about 10,000 sentences collectedfrom world press articles, whose private stateswere manually annotated.
The annotation was per-formed at word and phrase level, and the sentimentexpressions identified in the corpus were asso-ciated to the source of the private-state, the targetinvolved and other sentiment properties, like inten-sity and type of attitude.
MPQA is an importantresource for sentiment analysis in English, but itdoes not reflect the semantics of specific text ge-nres or domains.Pang et al (2002) propose a methodology forautomatically constructing a domain-specific cor-pus, to be used in the automatic classification ofmovie reviews.
The authors selected a collection ofmovie reviews where user ratings were explicitlyexpressed (e.g.
?4 stars?
), and automatically con-verted them into positive, negative or neutral polar-ities.
This approach simplifies the creation of asentiment corpus, but it requires that each opinio-nated text is associated to a numeric rating, whichdoes not exist for most of opinionated texts availa-ble on the web.
In addition, the corpus annotationis performed at document-level, which is inade-quate when dealing with more complex types oftext, such as news and comments to news, where amultiplicity of sentiments for a variety of topicsand corresponding targets are potentially involved(Riloff and Wiebe., 2003; Sarmento et al, 2009).Alternative approaches to automatic and manualconstruction of sentiment corpora have been pro-posed.
For example, Kim and Hovy (2007) col-lected web users?
messages posted on an electionprediction website (www.electionprediction.org) toautomatically build a gold standard corpus.
Theauthors focus on capturing lexical patterns thatusers frequently apply when expressing their pre-dictive opinions about coming elections.
Sarmentoet al (2009) design a set of manually crafted rules,supported by a large sentiment lexicon, to speed upthe compilation and classification of opinionatedsentences about political entities in comments tonews.
This method achieved relatively high preci-sion in collecting negative opinions; however, itwas less successful in collecting positive opinions.3 The CorpusFor creating SentiCorpus-PT we compiled a collec-tion of comments posted by the readers of the Por-tuguese newspaper P?blico to a series of 10 newsarticles covering the TV debates on the 2009 elec-tion of the Portuguese Parliament.
These tookplace between the 2nd and the 12th of September,2009, and involved the candidates from the largestPortuguese parties.
The whole collection is com-posed by 2,795 posts (approx.
8,000 sentences),which are linked to the respective news articles.This collection is interesting for several reasons.The opinion targets are mostly confined to a pre-dictable set of human entities, i.e.
the politicalactors involved in each debate.
Additionally, theformat adopted in the debates indirectly encour-aged users to focus their comments on two specificcandidates at a time, persuading them to confronttheir standings.
This is particularly interesting forstudying both direct and indirect comparisons be-tween two or more competing human targets (Ga-napathibhotla and Liu, 2008).Our annotation scheme stands on the followingassumptions: (i) the sentence is the unit of analysis,whose interpretation may require the analysis ofthe entire comment; (ii) each sentence may conveydifferent opinions; (iii) each opinion may havedifferent targets; (iv) the targets, which can beomitted in text, correspond to human entities; (v)the entity mentions are classifiable into syntactic-semantic categories; (vi) the opinionated sentencesmay be characterized according to their polarity565and intensity; (vii) each opinionated sentence mayhave a literal or ironic interpretation.Opinion Target: An opinionated sentence mayconcern different opinion targets.
Typically, targetscorrespond to the politicians participating in thetelevised debates or, alternatively, to other relevantmedia personalities that should also be identified(e.g.
The Minister of Finance is done!).
There arealso cases wherein the opinion is targeting anothercommentator (e.g.
Mr. Francisco de Amarante, didyou watch the same debate I did?!?!?
), and otherswhere expressed opinions do not identify theirtarget (e.g.
The debate did not interest me at all!
).All such cases are classified accordingly.The annotation also differentiates how humanentities are mentioned.
We consider the followingsyntactic-semantic sub-categories: (i) proper name,including acronyms (e.g.
Jos?
S?crates, MFL),which can be preceded by a title or position name(e.g.
Prime-minister Jos?
S?crates; Eng.
S?crates);(ii) position name (e.g.
social-democratic leader);(iii) organization (e.g.
PS party, government); (iv)nickname (e.g.
Pin?crates); (v) pronoun (e.g.
him);(vi) definite description, i.e.
a noun phrase that canbe interpreted at sentence or comment level, afterco-reference resolution (e.g.
the guys at the Minis-try of Education); (vii) omitted, when the referenceto the entity is omitted in text, a situation that isfrequent in null subject languages, like EuropeanPortuguese (e.g.
[He] massacred...).Opinion Polarity and Intensity: An opinion po-larity value, ranging from ?-2?
(the strongest nega-tive value) to ?2?
(the strongest positive value), isassigned to each of the previously identified tar-gets.
Neutral opinions are classified with ?0?, andthe cases that are ambiguous or difficult to interp-ret are marked with ??
?.Because of its subjectivity, the full range of theintensity scale (?-2?
vs.
?-1?
; ?1?
vs.
?2?)
is re-served for the cases where two or more targets are,directly or indirectly, compared at sentence orcomment levels (e.g.
Both performed badly, butS?crates was clearly worse).
The remaining nega-tive and positive opinions should be classified as ?-1?
and ?1?, respectively.Sentences not clearly conveying sentiment oropinion (usually sentences used for contextualizingor quoting something/someone) are classified as?non-opinionated sentences?.Opinion Literality: Finally, opinions are characte-rized according to their literality.
An opinion canbe considered literal, or ironic whenever it conveysa meaning different from the one that derives fromthe literal interpretation of the text (e.g.
Thisprime-minister is wonderful!
Undoubtedly, all thePortuguese need is more taxes!
).4 Corpus AnalysisThe SentiCorpus-PT was partially annotated by anexpert, following the guidelines previously de-scribed.
Concretely, 3,537 sentences, from 736comments (27% of the collection), were manuallylabeled with sentiment information.
Such com-ments were randomly selected from the entire col-lection, taking into consideration that each debateshould be proportionally represented in the senti-ment annotated corpus.To measure the reliability of the sentiment anno-tations, we conducted an inter-annotator agreementtrial, with two annotators.
This was performedbased on the analysis of 207 sentences, randomlyselected from the collection.
The agreement studywas confined to the target identification, polarityassignment and opinion literality, using Krippen-dorff's Alpha standard metric (Krippendorff,2004).
The highest observed agreement concernsthe target identification (?=0.905), followed by thepolarity assignment (?=0.874), and finally the iro-ny labeling (?=0.844).
According to Krippen-dorff?s interpretation, all these values (> 0.8)confirm the reliability of the annotations.The results presented in the following sectionsare based on statistics taken from the 3,537 anno-tated sentences.4.1 Polarity distributionNegative opinions represent 60% of the analyzedsentences.
In our collection, only 15% of the sen-tences have a positive interpretation, and 13% aneutral interpretation.
The remaining 12% are non-opinionated sentences (10%) and sentences whosepolarity is vague or ambiguous (2%).
If one con-siders only the elementary polar values, it can beobserved that the number of negative sentences isabout three times higher than the number of posi-tive sentences (68% vs. 17%).The graphic in Fig.
1 shows the polarity distri-bution per political debate.
With the exception ofthe debate between Jer?nimo de Sousa (C5) and566Paulo Portas (C3), in which the number of positiveand negative sentences is relatively balanced, allthe remaining debates generated comments withmuch more negative than positive sentences.Fig.
1.
Polarity distribution per political debateWhen focusing on the debate participants, it canbe observed that Jos?
S?crates (C1)censured candidate, and Jer?nimo de Sousa (the least censured one, as shown in Fig.ly, the former was reelected as primethe later achieved the lowest percentage of votes inthe 2009 parliamentary election.Fig.
2.
Polarity distribution per candidateAlso interesting is the information contained inthe distributions of positive opinions.that there is a large correlation (The Pearson corrlation coefficient is r = 0.917) between theof comments and the number of votes of each cadidate (Table 1).is the mostC5)2.
Curious--minister, andWe observee-numbern-Candidate (C) #PosComJos?
S?crates (C1)M. Ferreira Leite (C2)Paulo Portas (C3)Francisco Lou??
(C4)Jer?nimo de Sousa (C5)Table 1.
Number of positive comments and4.2 Entity mentionsAs expected, the most frequent type of mentioncandidates is by name, but it only covers 36% ofthe analyzed cases.
Secondly, a proper or commonnoun denoting an organization is used metonymcally for referring its leaders or membersPronouns and free noun-phrases, which can blexically reduced (or omitted) in text, representtogether 38% of the mentions to candidates.
This isa considerable fraction, which cannot be neglecteddespite being harder to recognizeused in almost 5% of the cases.positions/roles of candidates aremention category used in the corpus4.3 IronyVerbal irony is present in approximately 11% ofthe annotated sentences.
The data shows that ironyand negative polarity are proportionally distributedregarding the targets involved (Table 2an almost perfect correlation between them (0.99).Candidate (C) #NegJos?
S?crates (C1)M. Ferreira Leite (C2)Paulo Portas (C3)Francisco Lou??
(C4)Jer?nimo de Sousa (C5)Table 2.
Number of negative and iro5 Main Findings and Future DirectionsWe showed that in our setting negative opinionstend to greatly outnumber positive opinions, leaing to a very unbalanced opinionratio).
Different reasons may explain suchance.
For example, in UGC, readers tend to bemore reactive in case of disagreementexpress their frustrations more vehemently on ma#Votes169 2,077,238100 1,653,66569 592,77879 557,30658 446,279votestoi-(17%).e,.
Nicknames areSurprisingly, thethe least frequent(4%).).
There isr =Com #IronCom766 90390 57156 25171 26109 14nic commentsd-corpus (80/20imbal-, and tend tot-567ters that strongly affect their lives, like politics.Anonymity might also be a big factor here.From an opinion mining point of view, we canconjecture that the number of positive opinions is abetter predictor of the sentiment about a specifictarget than negative opinions.
We believe that thevalidation of this hypothesis requires a thoroughstudy, based on a larger amount of data spanningmore electoral debates.Based on the data analyzed in this work, we es-timate that 11% of the opinions expressed in com-ments would be incorrectly recognized as positiveopinions if irony was not taken into account.
Ironyseems to affect essentially sentences that wouldotherwise be considered positive.
This reinforcesthe idea that the real challenge in performing opi-nion mining in certain realistic scenarios, such asin user comments, is correctly identifying the leastfrequent, yet more informative, positive opinionsthat may exist.Also, our study provides important clues aboutthe mentioning of human targets in UCG.
Most ofthe work on opinion mining has been focused onidentifying explicit mentions to targets, ignoringthat opinion targets are often expressed by othermeans, including pronouns and definite descrip-tions, metonymic expressions and nicknames.
Thecorrect identification of opinions about humantargets is a challenging task, requiring up-to-dateknowledge of the world and society, robustness to?noise?
introduced by metaphorical mentions, neo-logisms, abbreviations and nicknames, and thecapability of performing co-reference resolution.SentiCorpus-PT will be made available on ourwebsite (http://xldb.fc.ul.pt/), and we believe that itwill be an important resource for the communityinterested in mining opinions targeting politiciansfrom user-generated content, to predict future elec-tion outcomes.
In addition, the information pro-vided in this resource will give new insights to thedevelopment of opinion mining techniques sensi-tive to the specific challenges of mining opinionson human entities in UGC.AcknowledgmentsWe are grateful to Jo?o Ramalho for his assistancein the annotation of SentiCorpus-PT.
This workwas partially supported by FCT (Portuguese re-search funding agency) under grant UTAEst/MAI/0006/2009 (REACTION project), andscholarship SFRH/BPD/45416/2008.
We alsothank FCT for its LASIGE multi-annual support.ReferencesCarvalho, Paula, Lu?s Sarmento, M?rio J. Silva, andEug?nio Oliveira.
2009.
?Clues for Detecting Ironyin User-Generated Contents: Oh...!!
It?s ?so easy?
;-)?.
In Proc.
of the 1st International CIKM Workshopon Topic-Sentiment Analysis for Mass Opinion Mea-surement, Hong Kong.Ganapathibhotla, Murthy, and Bing Liu.
2008.
?MiningOpinions in Comparative Sentences?.
In Proc.
of the22nd International Conference on Computational Lin-guistics, Manchester.Kim Soo-Min, and Eduard Hovy.
2007.
?Crystal: Ana-lyzing predictive opinions on the web?.
In Proc.
ofthe Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, Prague.Krippendorff, Klaus.
2004.
Content Analysis: An Intro-duction to Its Methodology, 2nd Edition.
Sage Publi-cations, Thousand Oaks, California.Liu, Bing.
2010.
?Sentiment Analysis: A MultifacetedProblem?.
Invited contribution to IEEE IntelligentSystems.Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan.2002.
?Thumbs up?
Sentiment classification usingmachine learning techniques?.
In Proc.
of the Confe-rence on Empirical Methods in Natural LanguageProcessing, USA.Riloff, Ellen, and Janice Wiebe.
2003.
?Learning extrac-tion patterns for subjective expressions?.
In Proc.
ofthe Conference on Empirical Methods in NaturalLanguage Processing, Sapporo.Sarmento, Lu?s, Paula Carvalho, M?rio J. Silva, andEug?nio Oliveira.
2009.
?Automatic creation of areference corpus for political opinion mining in user-generated content?.
In Proc.
of the 1st InternationalCIKM Workshop on Topic-Sentiment Analysis forMass Opinion Measurement, Hong Kong.Wiebe, Janice, Theresa Wilson, and Claire Cardie.2005.
?Annotating expressions of opinions and emo-tions in language?.
In Language Resources and Eval-uation, volume 39, 2-3.Wilson, Theresa, Janice Wiebe, and Paul Hoffmann.2005.
?Recognizing Contextual Polarity in Phrase-Level Sentiment Analysis?.
In Proc.
of the Joint Hu-man Language Technology Conference and Empiri-cal Methods in Natural Language Processing,Canada.568
