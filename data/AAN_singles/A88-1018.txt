INTEGRATING TOP-DOWN AND BOTTOM-UP STRATEGIESIN  A TEXT PROCESSING SYSTEMLisa F. Rau and Paul S?
JacobsArtificial Intelligence BranchGE Company, Corporate R&DSchenectady, NY 12301Abst rac tThe SCISOR.
system is a computer program designedto scan naturally occurring texts in constrained o-mains, extract information, and answer questionsabout that information.
The system currently readsnewspapers stories in the domain of corporate merg-ers and acquisitions.
The language analysis trategyused by SCISOR combines full syntactic (bottom-up) parsing and conceptual expectation-driven (top-down) parsing.
Four knowledge sources, includ-ing syntactic and semantic information and domainknowledge, interact in a flexible manner.
This in-tegration produces a more robust semantic analyzerdesigned to deal gracefully with gaps in \]exical andsyntactic knowledge, transports easily to new do-mains, and facilitates the extraction of informationfrom texts.INTRODUCTIONThe System for Conceptual Information Summarization,Organization and Retrieval (SCISOR) is an implementedsystem designed to extract information from naturally oc-curring texts in constrained omains.
The derived infor-mation is stored in a conceptual knowledge base and re-trieved using a natural language analyzer and generator.Conceptual information extracted from texts has anumber of advantages over other information-retrievaltechniques \[Rau, 1987a\], in addition to allowing for theautomatic generation of databases from texts.The integration of top-down, expectation driven pro-cessing, and bottom-up, language-driven parsing is impor-tant for text understanding.
Bottom-up strategies identifysurface linguistic relations in the input and produce con-ceptual structures from these relations.
With the input"ACE made ACME an offer", a good "bottom-up" linguisticanalyzer can identify the subject, verb, direct and indirectobjects.
It also can determine that ACME was the recipi-ent of an offer, rather than being made into an offer, as in"ACE made ACME a subsidiary".Top-down methods use extensive knowledge of thecontext of the input, practical constraints, and conceptualexpectations based on previous events to fit new informa-tion into an existing framework.
A good "top-down" an-alyzer might determine from "ACE made ACME an offer"that ACME is the target of a takover (which is not obviousfrom the language, since the offer could be for somethingthat ACME owns), and relate the offer to other events (pre-vious rumors or competing offers).Bottom-up methods tend to produce more accurateparses and semantic interpretations, account for subtletiesin linguistic expression, and detect inconsistencies and lexi-cal gaps.
Top-down methods are more tolerant of unknownwords or grammatical lapses, but are also more apt to de-rive erroneous interpretations, fail to detect inconsisten-cies between what is said and how it is interpreted, andoften cannot produce any results when the text presentsunusual or unexpected information.
Integration of thesetwo approaches can improve the depth and accuracy ofthe understanding process.SCISOR is unique in its integration of the bottom-upprocessing performed by its analyzer, TRUMP (TRans-portable Understanding Mechanism Package) \[Jacobs,1986\], with other sources of information in the form ofconceptual expectations.In this paper, four information sources are describedthat are used by SCISOR to produce meaning represen-tations from texts.
The actual processing sequence andtiming of the application of these sources are illustrated.THE SC ISOR SYSTEMThe SCISOR system is currently being tested with news-paper stories about corporate takeovers.
The domainprovides interesting subject matter as well as oome richlanguage.
The gradual development of the stories overtime motivates a natural language approach, while the re-stricted nature of the material allows us to encode concep-tual expectations necessary for top-down processing.The following is an example of the operation ofSCISOR on a simple news story:W ACOUISITION UPS BID FOE WARNACOWarnaco received another merger offer, valuedat $36 a share, or $360 million.
The buyoutoffer for ~he apparel maker was made by ~heW Acquisition Corporation of Delaware.User: Who took over Warnaco?System: W Acquisition offered $36 per share for Warnaco.User: What happened to Warnaco last Tuesday?System: Warnaco rose 2 1/2 as a result of rumors.129The system has been demonstrated with a small set ofinput texts, and is being expanded to handle large numbersof newswire stories using a large domain knowledge baseand substantial vocabulary.SOURCES OF INFORMATIONText processing in SCISOR is accomplished through theintegration of sources of knowledge and types of processing.The four sources of knowledge that SCISOR uses to extractmeaning from text are as follows:A. Role-filler Expectat ions:  Constraints on what canfill a conceptual role are the primary source of infor-mation used in top-down processing.B.
Event Expectat ions:  Expectationsabout events that may occur in the future are cre-ated from previous tories, and used to predict valuesin the expected events if they occur.C.
Linguistic: Grammatical, exical and phrasal knowl-edge is used whenever it is available and reliable.
Sub-language (domain-specific) linguistic information mayalso be used if available.D.
Wor ld Knowledge Expectat ions:  Worldknowledge xpectations can disambiguate multiple in- .terpretations through domain-specific heuristics.SCISOR can operate with any combination of these infor-mation sources.
When one or more sources are lacking, theinformation extracted from the texts may be more superfi-cial, or less reliable.
The flexibility in depth of processingprovided by these multiple information sources is an inter-esting feature in its own right, in addition to forming thefoundations for a system to "skim" efficiently when a newtext contains material already processed.As an example of each source of information, considerthe following segment from the text printed previously:Warnaco received another merger o f fe r ,va lued  at  $36 a share ,  or $360 mil l ion.Role-filler expectations allow SCISOR to make reli-able interpretations of the dollar figures in spite of incom-plete lexical knowledge of the syntactic roles they play inthe sentence.
This is accomplished because prices of stockare constrained to be "small" numbers, whereas fillers oftakeover-bid value roles are constrained to be "large" quan-tities.
Event expectations lead to the deeper interpretationthat this offer is an increase over a previous offer becauseone expects ome kind of rebuttal to an offer to occur inthe future.
An increased offer is one such rebuttal.
Worldknowledge might allow the system to predict whether theoffer was an increase or a competing offer, depending onwhat other information was available.A unique feature of SCISOR is that partial inguisticknowledge contributes to all of these interpretations, andto the understanding of "received" in this context.
This isnoteworthy because general knowledge about "receive" inthis case interacts with domain knowledge in understand-ing the role of Warnaco in the offer.A robust parser and semantic interpreter could obtainthese features from the texts without the use of expecta-tions.
This would make top-down processing unnecessary.Robust in-depth analysis of texts, however, is beyond thenear-term capabilities of natural language technology; thusSCISOR is designed with the understanding that there willalways be gaps in the system's knowledge base that mustbe dealt with gracefully.Now the four sources of information used to extractinformation are described in more detail, followed by adiscussion of how they interact in the processing of twosample texts.A .
Ro le - f i l l e r  Expectat ionsThe simplest kind of expectation-driven information thatcan be used is termed "role-filler" expectations.
These ex-pectations take the form of constraints on the filler of aconceptual role.
This is the primary source of processingpower in expectation-driven systems uch as FRUMP \[De-Jong, 1979\].
The following list illustrates ome examples ofconstraints on certain fillers of roles used in the corporatetakeover domain.ROLE FILLER-CONSTRAINT EXAMPLEta rget  company-agent ACEsuil;or company-agent ACHEpr ice -per -share  smal l  number $45to ta l  value large number $46 mil l ionThis information isencoded eclaratively in the knowledgebase of the system.
During the processing of a text, rolesmay be filled with more than one hypothesis; however, assoon as a filler for a role is certain, the process of elimi-nation is used to aid in the resolution of other bindings.Thus, if SCISOR determines that ACE is a takeover target,it will assume by default that ACHE is the suitor if thetwo companies appear in the same story and there is noadditional information to aid in the disambiguation.B .
Event  Expectat ionsExpectations that certain events will occur in the futureare a second source of information available to aid in the in-terpretation of new events.
These expectations arise fromthe events in previous tories.
For example, when the sys-tem reads that rumors have been swirling around ACE as atakeover target, an event expectation is set up that antici-pates an offer for ACE in some future story.
When an offerhas been made, expectations are set up that some kind ofrebuttal will take place.
This rebuttal may be a rejectionor an acceptance of the offer.
The acceptance of the offeroption carries with it the event expectation that the totalvalue of the takeover will be the amount of the offer.Event expectations are implemented as domain-dependent, declarative properties of the events in the do-130main.
They are derived from the script-like \[Schank andAbelson, 1977\] representations of typical event sequences.C .
L ingu is t i c  Ana lys i sThe most important source of information used in textprocessing is a full bottom-up arser.
TRUMP is a flexiblelanguage analyzer consisting of a syntactic processor andsemantic interpreter \[Jacobs, 1986, Jacobs, 1987a\].
Thesystem is designed to fill conceptual roles using linguistic,conceptual, and metaphorical relationships distributed ina knowledge hierarchy.Within SCISOR, TRUMP identifies linguistic rela-tionships in the input, using lexical and syntactic knowl-edge.
Knowledge structures produced by TRUMP arepassed through an interface to the top-down processingcomponents.
Pieces of knowledge structures may then betied together with the expectation-driven processing com-ponents.In the case of a complete parse of an input sentence,the knowledge structures produced by TRUMP containmost of the structure of the final interpretation, althoughexpectations often further efine the analysis.
In the case ofpartial parses, more of the structure is determined by role-filler expectations.
The following are two simple examplesof this division of labor:Input :W Acquis i t ion o f fe red  $36 a share fo r  Warnaco.Part ia l  parser  output :(Basic-sentence(NP (Name-NP(Name (C-Name W_Acquisition})))(VP (Adjunct- VP(VP (Transitive.
VP(Verb-part (Basic-verb-part(V offered)))(NP (Postmodified-NP(NP (S-NP SS6))(MOD (Ratio.modifer(R.wo~d a) (N share)))))))(PP (Basic-PP (P foO (NP (Name-NP(G-Name War~aco))))))))TRUMP interpretat ion:(offer(offerer W-Acq-Co)(offeree Warnaco)(offer (dollars (quantity 36)(denominator share}}))Final interpretat ion:(corp-takeover-offer(suitor W-Acq-Co)(target Warnaco)(dps (quantity 36)))Input :Warnaco rece ived another merger o f fe r ,  va lued at$36 a share.Part ia l  parser  output :(Sub j-verb-relation(Subj (NP (Name-NP (Name(C-Name W.Acquisition))))(verb (v received))))(iv offeO(NP (Postmodified-NP (NP (S-NP SS~))(MOD (Ratio-modifier (R-~oo~d ( et a))(Noun-part (N share))))))TRUMP interpretat ion:(offer)(transfer-event (recipient Warnaco))(dollars (quantity 36) (denominator share))Final interpretat ion:(corp-takeover-offer(target Warnaco)(dps (quantity 36)))In the first example above, TRUMP succeeds in pro-ducing a complete syntactic parse, along with the corre-sponding semantic interpretation.
The domain knowledgehelps only to specify the verb sense of "offer".
In the sec-ond example, however, more of the work is done by thedomain-dependent xpectations.
In this case, the unknownwords prevent TRUMP from completing the parse, so theoutput from the parser is a set of linguistic relations.
Theserelations allow the semantic interpreter to produce somesuperficial conceptual structures, but the final conceptualroles are filled using domain knowledge.The distinction between the general offer and themore specific corp-takeover-offer is essential for under-standing texts of this type.
In general, an offer may bemade for something to someone, but it is only in the cor-porate takeover domain that the target of the takeover (thefor role) is by default the same as the recipient of the of-fer (the to role).
Since TRUMP is a domain-independentanalyzer, it cannot itself fill such roles appropriately.The knowledge sources at work in SCISOR and thetiming of the information exchange in the system are de-scribed in the next section.D .
Wor ld  Knowledge  Expectat ionsIf all the above sources of information are still insufficientto determine or satisfactorily disambiguate potential re-lationships between items in the text, so called "worldknowledge" can be called into play.
This world knowledgetakes the form of domain-dependent generalizations, im-plemented as declarative relationships between concepts.For example, in the corporate takeover domain, a piece ofworld knowledge that can aid in the determination ofwhatcompany is taking over what company is the following:131If it is ambiguous whether:Company A is taking over Company Bor Company B is taking over Company AChoose the larger company "co be the suitorand the smaller company to be the targetThis example uses the knowledge that it is almost alwaysthe case that the suitor is larger than the target company.The utilization of this generalization (that typically largercompanies take over smaller companies) requires knowl-edge of the company sizes, assumed to be present in theknowledge base of the system.
Another example is:If it is ambiguous whether:value A is a previous offer or presentstock price and value B is a new offeror vice versa,Choose the larger offer for the nee offeror present stock price, and the smalleroffer for the previous offerIn this case, a company rarely would decrease their of-fer unless something unexpected happened to the targetcompany before the takeover was completed.
Similarly, anoffer is almost always for more than the current value ofthe stock on the stock market.These.
heuristics incorporate xpectations that arisefrom potentially complex explanations.
For example, thereason why a new offer is higher than an old offer restson a complex understanding of the plan of the suitor toreach their goal of taking over the target company.
Theworld knowledge presented here represents a compilationof this complex reasoning into simple heuristics for textunderstanding, albeit ad hoc.Although this type of information is shown in a rule-like form, it is implemented with special relationship linksthat contain information as to how to compute the truthvalue of the relationship.
When this type of knowledgeis needed to disambiguate an input, the system checks ifany objects have these "world knowledge constraints".
Ifso, they are activated and applied to the situation underconsideration.The intuition underlying the inclusion of heuristics ofthis sort is that there is a great deal of "common sense" in-formation that can increase an understanding mechanism'sability to extract meaning.
This type of information is alast resort for determining conceptual relations when othermore principled sources of information are exhausted.KNOWLEDGE INTEGtLkT IONEach of the four sources of information described above isutilized at different points in the processing of the inputtext, and with different degrees of confidence.
The follow-ing algorithm describes a particular instantiation of thisorder for a hypothetical event sequence involving rumorsabout ACE, followed by an offer by ACME for ACE.In general, event expectations are set up as soon as anevent that has an expectation property is detected.
Thatis, as soon as the system sees a rumor, it sets up an ex-pectation that there will be an offer for the company therumor was about sometime in the future.When that event-expectation is confirmed, those ex-pectations are realized and the information expected isadded to the meaning extracted from the text being pro-cessed.
Note that these realized expectations may later beretracted given additional information.
Role-filler expec-tations then create multiple hypotheses about which itemsmay fill what roles.
These are narrowed own by any con-straints already present by event expectations.Linguistic analysis, when it provides a complete finalmeaning representation for a portion of the text containingfeatures of interest, always supercedes a conflicting r~le-filler expectation.
For example, if a role-filler expectationhypothesized that ACE was the target in a takeover, andthe parser determined that ACME was the object of thetakeover, ACME alone would be included as the target.World knowledge xpectations are invoked only in thecase of conflicting or ambiguous interpretations.
For ex-ample, if after all the processing is finished and the systemdoes not know whether ACE is taking over ACME or viceversa, the expectation that the larger company is typicallythe suitor is invoked and used in the final disambiguation.Below are the sample input texts, followed by the se-quence of steps that are taken by the program.ACE, an apparel maker p l~n ing  a leveragedbuyou~, rose $2 I/2 to $3S 3/8, as a rumorspread that another buyer might appear.
Thecompany said there were no corporate develop-ments to account for the rise, and the rumorcould not be confirmed.later onACE received another merger offer, valued at$36 a share, or $360 million.
The buyoutoffer for the apparel maker was made by theACME Corporation of Delaware.
ACE closedyesterday at $3S 3/8.1.
System reads first story and extracts information thatthere are rumors about ACE and that the stock priceis currently $35 3/8, using role-filler expectations.2.
An event expectation is set up that there will be anoffer-event, with ACE as the target of the takeoveroffer.3.
System begins reading story involving a takeover offerand ACE.4.
Target slot of offer is filled with ACE from the eventexpectation.5.
An event expectation is set up that there will be arebuttal to the offer sometime in the future.6.
System encounters ACME which it knows to be a com-pany.
Suitor slot of offer is thus filled with ACME viaa role-filler expectation.1327.
$36 a share is parsed with the phrasal exicon.8.
$36 a share is added as a candidate for either thestock's current price on the stock market or theamount of the ACME offer, due to role-filler expec-tations.9.
$360 million is parsed with the phrasal exicon.10.
$360 million is added as candidate for the total valueof the offer due to a role-filler expectation that expectstotal values to be large numbers.11.
Syntactic and semantic analysis determine that theofferer is ACME, and the target is ACE.
This reinforcesthe interpretations previously hypothesized.12.
Syntactic and semantic analysis determine the loca-tion of the ACME Corporation to be Delaware.13.
$35 3/8 is encountered, which is taken to be a price-per-share amount, due to a role-filler expectation thatexpects prices per share to be small numbers.14.
$35 3/8 a share is added as a candidate for eitherthe stock's current price on the stock market or theamount of the ACME offer.15.
$35 3/8 is taken to be the stock's current price and$36 is taken to be the amount of the ACME offer, dueto the world knowledge xpectation that expects theoffer to exceed the current rading price.The contribution of the various sources of knowledgesvaries with the amount of knowledge they can be broughtto bear on the language being analyzed.
That is, givenmore syntactic and semantic knowledge, TRUMP couldhave done more work in the analyses of these stories.Given more detailed conceptual expectations, the bottom-"up mechanism also could have extracted more meaning.Together, the two mechanisms should combine to producea deeper and more complete meaning representation thaneither one could alone.IMPLEMENTATIONSCISOR consists of a variety of programs and tools, op-erating in conjunction with a declarative knowledge baseof domain-independent li guistic, grammatical nd worldknowledge and domain-dependent lexicon and domainknowledge.
A brief overview of the system may be foundin \[Rau, 1987c\], and a more complete description in \[Rau,1987b\].
The natural anguage input is processed with theTRUMP parser and semantic interpreter \[Jacobs, 1986\].Linguistic knowledge is represented using the Ace linguis-tic knowledge representation framework \[Jacobs and Ran,1985\].
Answers to user's questions and event expectationsare retrieved using the retrieval mechanism described in\[Rau, 1987b\].
Responses to the user will be generatedwith the KING \[Jacobs, 1987b\] natural language gener-ator when that component is integrated with SCISOR;currently output is "canned".
The events in SCISOR arerepresented using the KODIAK knowledge representationlanguage \[Wilensky, 1986\], augmented with some scriptalknowledge of typical events in the domain.SYSTEM STATUSAll the components of SCISOR described here have beenimplemented, although not all have been connected to-gether.
The system can, as of this writing, process a num-ber of stories in the domain.
The processing entails thecombined expectation-driven and language driven capabil-ities described here.
For questions that the system canunderstand, SCISOR retrieves conceptual answers to in-put questions.
These answers are currently output usingpseudo-natural l nguage, but we are in the process of in-tegrating the KING generator.SCISOR is currently being connected to an automaticsource of on-line information (a newswire) for extensivetesting and experimentation.
The goal of this effort is toprove the utility of the system for processing large bodiesof text in a limited domain.Although there will undoubtedly be many lessonsin extending SCISOR to handle thousands of texts,SCISOI:t's first few stories have already demonstrated someof the advantages of the approach described here:1.
Much of the knowledge used in analyzing these storiesis domain-independent.2.
Where top-down strategies fail, SCISOR can still ex-tract some information from the texts and use thisinformation in answering questions.3.
Unknown words (lexical gaps) and grammatical lapsesare tolerated.These three characteristics simply cannot be achieved with-out combining top-down and bottom-up strategies.The major barrier to the practical success of text pro-cessing systems like SCISOR is the vast amount of knowl-edge required to perform accurate analysis of any bodyof text.
This bottleneck has been partially overcome bythe graceful integration of processing strategies in the sys-tem; the program currently operates using only hundredsof known words.
However, SCISOK is designed to benefitultimately from an extended vocabulary (i. e. thousandsof word roots) and increased omain knowledge.
The vo-cabulary and knowledge base of the system are constantlybeing extended using a combination of manual and auto-mated techniques.EXTENSIB IL ITY  AND PORTABIL ITYOur research has combined some of the advantages oftop-down language processing methods (tolerance of un-known inputs, understanding in context) with the assets ofbottom-up strategies (broader linguistic capabilities, par-tial results in the absence of expectations).
The systemdescribed here competently answers questions about con-strained texts, uses the same language analyzer for textprocessing and question answering, and has been appliedto other domains as well as the corporate takeover sto-ries.
SCISOR is thus a state-of-the-art system, but like133other text processing systems the main chore that remainsis to allow for the practical extraction of information fromthousands of real texts.
The following are the main issuesinvolved in making such a system a reality and how weaddress them:Lexicon design: The size of the text-processing lexiconis important, but sheer vocabulary isnot of much help.What is needed is a lexicon that accounts both for thebasic meanings of common words and the specializeduse of terms in a given context.
We use a hierarchicalphrasal exicon \[Besemer and Jacobs, 1987, Dyer andZernik, 1986\] to allow domain-specific vocabulary totake advantage of existing linguistic knowledge and ul-timately to facilitate automatic language acquisition.Grammar :  A disadvantage of many approaches to textprocessing is that it is counterintuitive to assume thatmost language processing is domain-specific.
Whilespecialized knowledge is essential, a portable gram-mar, like a core lexicon, is indispensable.
Languageis too complex to be reduced to a few domain-specificheuristics.
Because specialized constructs may inheritfrom general grammatical rules, TRUMP allows spe-cialized sublanguage grammar to interact with "core"grammar.
It is still a challenge, however, to dealgracefully with constructs in a sublanguage that wouldordinarily be extragrammatical.Conceptua l  Knowledge:  The KODIAK knowledge rep-resentation, used for conceptual knowledge inSCISOR, allows for multiple inheritance as well asstructured relationships among conceptual roles.
Thisrepresentation is useful for the retrieval of conceptualinformation in the system.
A broader base of "com-mon sense" knowledge in KODIAK will be used toincrease the robustness of SCISOR.Our strategy has been to attack the robustness prob-lem by starting with the underlying knowledge represen-tation issues.
There will be no way to avoid the workinvolved in scaling up a system, but with this strategy wehope that much of this work will be useful for text process-ing in general, as well as for analysis within a specializeddomain.FUTURE DIRECTIONSIn the immediate future, we hope to connect SCISOR to acontinuous ource of on-line information to begin collect-ing large amounts of conceptually analyzed material, andextensively testing the system.We also plan to dramatically increase the size of thelexicon through the addition of an on-line source of dic-tionary and thesaurus information.
The system grammaralso will increase in coverage over time, aswe extend andimprove the capabilities of the bottom-up TRUMP parser.Another interesting extension is the full implementa-tion of a parser skimming mode.
This mode of operation,triggered when the system recognizes input events that areidentical to events it has already read about, will cause theparser to perform very superficial processing of the text.This superficial or skimming processing will continue untilthe parser reaches a point in the text where the story isno longer reporting on events the system has already readabout.RELATED RESEARCHThe bulk of the research on natural language text pro-cessing adheres to one of the two approaches integratedin SCISOR.
The practical issue for text processing sys-tems is that it is still far from feasible to design a programthat processes extended, unconstrained text.
Within the"bottom-up" framework, one of the most successful strate-gies, in light of this issue, is to define a specialized omain"sublangnage" \[Kittredge, 1982\] that allows robust pro-cessing so long as the texts use prescribed vocabulary andlinguistic structure.
The "top-down" approach similarlyrelies heavily on the constraints of the textual domain,but in this approach the understanding process is boundby constraints on the knowledge to be derived rather thanrestrictions on the linguistic structures.The bottom-up, or language-driven strategy, has theadvantage of covering a broad class of linguistic phe-nomena and processing even the more intricate details ofa text.
Many systems \[Grishman and Kittredge, 1986\]have depended on this strategy for processing messagesin constrained omains.
Other language-driven programs\[Hobbs, 1986\] do not explicitly define a sublanguage butrely on a robust syntax and semantics to understand theconstrained texts.
These systems build upon existinggrammars, which may make the semantic interpretationof the texts difficult.The top-down, or expectation-driven, approach, of-fers the benefit of being able to "skim" texts for particu-lar pieces of information, passing gracefully over unknownwords or constructs and ignoring some of the complexitiesof the language.
A typical, although early, effort at skim-ming news stories was implemented in FRUMP \[De:long,1979\], which accurately extracted certain conceptual in-formation from texts in preselected topic areas.
FRUMPproved that the expectation-driven strategy was useful forscanning texts in constrained omains.
This strategy in-cludes the banking telex readers TESS \[Young and Hayes,1985\] and ATRANS \[Lytinen and Gershman, 1986\].
Theseprograms all can be easily "fooled" by unusual texts, andcan obtain only the expected information.The difficulty of building a flexible understanding sys-tem inhibits the integration of the two strategies, althoughsome of those mentioned above have research efforts di-rected at integration.
Dyer's BORIS system \[Dyer, 1983\], aprogram designed for in-depth analysis of narratives ratherthan expository text scanning, integrates multiple knowl-edge sources and, like SCISOR, does some dynamic com-bination of top-down and bottom-up strategies.
The lin-guistic knowledge used by BORIS is quite different fromthat of TRUMP, however.
It lacks explicit syntactic struc-134tures and thus, like the sublanguage approach, relies moreheavily on domain-specific l nguistic knowledge.
Lytinen'sMOPTRANS \[Lytinen, 1986\] integrates syntax and seman-tics in understanding, but the syntactic overage of thesystem is in no way comparable to the bottom-up ro-grams.
SCISOR is, to our knowledge, the first text pro-cessing system to integrate full language-driven processingwith conceptual expectations.CONCLUSIONThe analysis of extended texts presents an extremely dif-ficult problem for artificial intelligence systems.
Bottom-up processing, or linguistic analysis, is necessary to avoidmissing information that may be explicitly, although sub-tly, conveyed by the text.
Top-down, or expectation-drivenprocessing, is essential for the understanding of language incontext.
Most text analysis systems have relied too heavilyon one strategy.SC ISOR represents a unique integration of knowledgesources to achieve robust and reliable extraction of in-formation from naturally occurring texts in constraineddomains.
Its ability to use lexical and syntactic knowl-edge when available separates it from purely expectation-driven semantic analyzers.
At the same time, its lack ofreliance on any single source of information and multiple"fall-back" heuristics give the system the ability to focusattention and processing on those items of particular in-terest to be extracted.REFERENCES\[Besemer and Jacobs, 1987\] David Besemer and Paul S.Jacobs.
FLUSH: a flexible lexicon design.
In Proceed-ings of the 25th Meeting of the Association for Com-putational Linguistics, Palo Alto, California, 1987.\[DeJong, 1979\] Gerald DeJong.
Skimming Stories in RealTime: An Ezperiment in Integrated Understanding.Research Report 158, Department of Computer Sci-ence, Yale University, 1979.\[Dyer, 1983\] Michael G. Dyer.
In-Depth Understanding.MIT Press, Cambridge, MA, 1983.\[Dyer and Zernik, 1986\] Michael G. Dyer and Uri Zernik.Encoding and acquiring meanings for figurativephrases.
In Proceedings of the 2Jth Annual Meeting ofthe Association for Computational Linguistics, NewYork, 1986.\[Grisbman and Kittredge, 1986\] Raplh Grishmanand Richard Kittredge, editors.
Analyzing Languagein Restricted Domains: Sublanguage Description andProcessing.
Lawrence Erlbaum, Hillsdale, NJ, 1986.\[Hobbs, 1986\] Jerry R. Hobbs.
Site report: overview ofthe TACITUS project.
Computational Linguistics,12(3):220-222, 1986.\[Jacobs, 1986\] Paul S. Jacobs.
Language analysis in not-so-limited omains.
In Proceedings of the Fall JointComputer Conference, Dallas, Texas, 1986.\[Jacobs, 1987a\] Paul S. Jacobs.
A knowledge frameworkfor natural anguage analysis.
In Proceedings of theTenth International Joint Conference on Artificial In-telligence, Milan, Italy, 1987.\[Jacobs, 1987b\] Paul S. Jacobs.
Knowledge-intensive nat-ural language generation.
Artificial Intelligence,33(3):325-378, November 1987.\[Jacobs and Ran, 1985\] Paul S. Jacobs and Lisa F. Rau.Ace: associating language with meaning.
In TimO'Shea, editor, Advances in Artificial Intelligence,pages 295-304, North Holland, Amsterdam, 1985.\[Kittredge, 1982\] Richard Kittredge.
Variation and homo-geneity of sublanguages.
In Richard Kittredge andJohn Lehrberger, editors, Sublanguages: Studies ofLanguage in Restricted Domains, pages 107-137, Wal-ter DeGruyter, New York, 1982.\[Lytinen, 1986\] Steven Lytinen.
Dynamically combiningsyntax and semantics in natural language processing.In Proceedings of the Fifth National Conference onArtificial Intelligence, Philadelphia, 1986.\[Lytinen and Gershman, 1986\] Steven Lytinen and Ana-tole Gershman.
ATRANS: automatic processing ofmoney transfer messages.
In Proceedings of theFifth National Conference on Artificial Intelligence,Philadelphia, 1986.\[Ran, 1987a\] Lisa F. Rau.
Information retrieval in never-ending stories.
In Proceedings of the Siz~h NationalConference on Artificial Intelligence, pages 317-321,Los Altos, CA, Morgan Kaufmann Inc., Seattle,Washington, July 1987.\[Rau, 1987b\] Lisa F. Rau.
Knowledge organization andaccess in a conceptual information system.
Infor-mation Processing and Management, Special Issueon Artificial Intelligence for Information Retrieval,23(4):269-283, 1987.\[Rau, 1987c\] Lisa F. Rau.
SCISOR: a system for effec-tive information retrieval from text.
In Poster SessionProceedings of the Third IEEE Conference on Artifi-cial Intelligence Applications, Orlando, FL, 1987.\[Schank and Abelson, 1977\] Roger C. Schank andRobert P. Abelson.
Scripts, Plans, Goals, and Un-derstanding.
Lawrence Erlbaum, Halsted, NJ, 1977.\[Wilensky, 1986\] R. Wilensky.
Knowledge representation- a critique and a proposal.
In Janet Kolodner andChris Riesbeck, editors, Experience, Memory, andReasoning, pages 15-28, Lawrence Erlbaum Asso-ciates, Hillsdale, New Jersey, 1986.\[Young and Hayes, 1985\] S. Young and P. Hayes.
Auto-matic classification and summarization of bankingtelexes.
In The Second Conference on Artificial In-telligence Applications, pages 402-208, IEEE Press,1985.13S
