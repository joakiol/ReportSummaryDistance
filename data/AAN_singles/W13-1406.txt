Proceedings of the Second Workshop on Computational Linguistics for Literature, pages 41?46,Atlanta, Georgia, June 14, 2013. c?2013 Association for Computational LinguisticsClustering voices in The Waste LandJulian BrookeDept of Computer ScienceUniversity of Torontojbrooke@cs.toronto.eduGraeme HirstDept of Computer ScienceUniversity of Torontogh@cs.toronto.eduAdam HammondDept of EnglishUniversity of Torontoadam.hammond@utoronto.caAbstractT.S.
Eliot?s modernist poem The Waste Land isoften interpreted as collection of voices whichappear multiple times throughout the text.
Here,we investigate whether we can automaticallycluster existing segmentations of the text intocoherent, expert-identified characters.
We showthat clustering The Waste Land is a fairly dif-ficult task, though we can do much better thanrandom baselines, particularly if we begin witha good initial segmentation.1 IntroductionAlthough literary texts are typically written by a sin-gle author, the style of a work of literature is not nec-essarily uniform.
When a certain character speaks,for instance, an author may shift styles to give thecharacter a distinct voice.
Typically, voice switchesin literature are explicitly marked, either by the useof quotation marks with or without a said quota-tive, or, in cases of narrator switches, by a majortextual boundary (e.g.
the novel Ulysses by JamesJoyce).
However, implicit marking is the norm insome modernist literature: a well-known example isthe poem The Waste Land by T.S.
Eliot, which isusually analyzed in terms of voices that each appearmultiple times throughout the text.
Our interest isdistinguishing these voices automatically.One of the poem?s most distinctive voices is thatof the woman who speaks at the end of its secondsection:I can?t help it, she said, pulling a long face,It?s them pills I took, to bring it off, she said[158?159]Her chatty tone and colloquial grammar and lexisdistinguish her voice from many others in the poem,such as the formal and traditionally poetic voice of anarrator that recurs many times in the poem:Above the antique mantel was displayedAs though a window gave upon the sylvan sceneThe change of Philomel[97?99]Although the stylistic contrasts between these andother voices are clear to many readers, Eliot doesnot explicitly mark the transitions, nor is it obvi-ous when a voice has reappeared.
Our previouswork focused on only the segmentation part of thevoice identification task (Brooke et al 2012).
Here,we instead assume an initial segmentation and thentry to create clusters corresponding to segments ofthe The Waste Land which are spoken by the samevoice.
Of particular interest is the influence of theinitial segmentation on the success of this down-stream task.2 Related WorkThere is a small body of work applying quantita-tive methods to poetry: Simonton (1990) lookedat lexical and semantic diversity in Shakespeareansonnets and correlated this with aesthetic success,whereas Dugan (1973) developed statistics of for-mulaic style and applied them to the Chanson deRoland to determine whether it represents an oralor written style.
Kao and Jurafsky (2012) quantifyvarious aspects of poety, including style and senti-ment, and use these features to distinguish profes-sional and amateur writers of contemporary poetry.41With respect to novels, the work of McKenna andAntonia (2001) is very relevant; they used principalcomponents analysis of lexical frequency to discrim-inate different voices and narrative styles in sectionsof Ulysses by James Joyce.Clustering techniques have been applied to liter-ature in general; for instance, Luyckx (2006) clus-tered novels according to style, and recent work indistinguishing two authors of sections of the Bible(Koppel et al 2011) relies crucially on an initialclustering which is bootstrapped into a supervisedclassifier which is applied to segments.
Beyond lit-erature, the tasks of stylistic inconsistency detec-tion (Graham et al 2005; Guthrie, 2008) and intrin-sic (unsupervised) plagiarism detection (Stein et al2011) are very closely related to our interests here,though in such tasks usually only two authors areposited; more general kinds of authorship identifi-cation (Stamatatos, 2009) may include many moreauthors, though some form of supervision (i.e.
train-ing data) is usually assumed.Our work here is built on our earlier work (Brookeet al 2012).
Our segmentation model for The WasteLand was based on a stylistic change curve whosevalues are the distance between stylistic feature vec-tors derived from 50 token spans on either side ofeach point (spaces between tokens) in the text; thelocal maxima of this curve represent likely voiceswitches.
Performance on The Waste Land was farfrom perfect, but evaluation using standard text seg-mentation metrics (Pevzner and Hearst, 2002) indi-cated that it was well above various baselines.3 MethodOur approach to voice identification in The WasteLand consists first of identifying the boundaries ofvoice spans (Brooke et al 2012).
Given a segmenta-tion of the text, we consider each span as a data pointin a clustering problem.
The elements of the vectorcorrespond to the best feature set from the segmen-tation task, with the rationale that features whichwere useful for detecting changes in style shouldalso be useful for identifying stylistic similarities.Our features therefore include: a collection of read-ability metrics (including word length), frequencyof punctuation, line breaks, and various parts-of-speech, lexical density, average frequency in a largeexternal corpus (Brants and Franz, 2006), lexicon-based sentiment metrics using SentiWordNet (Bac-cianella et al 2010), formality score (Brooke et al2010), and, perhaps most notably, the centroid of 20-dimensional distributional vectors built using latentsemantic analysis (Landauer and Dumais, 1997), re-flecting the use of words in a large web corpus (Bur-ton et al 2009); in previous work (Brooke et al2010), we established that such vectors contain use-ful stylistic information about the English lexicon(including rare words that appear only occasionallyin such a corpus), and indeed LSA vectors were thesingle most promising feature type for segmentation.For a more detailed discussion of the feature set, seeBrooke et al(2012).
All the features are normalizedto a mean of zero and a standard deviation of 1.For clustering, we use a slightly modified ver-sion of the popular k-means algorithm (MacQueen,1967).
Briefly, k-means assigns points to a clusterbased on their proximity to the k cluster centroids,which are initialized to randomly chosen points fromthe data and then iteratively refined until conver-gence, which in our case was defined as a change ofless than 0.0001 in the position of each centroid dur-ing one iteration.1 Our version of k-means is distinctin two ways: first, it uses a weighted centroid wherethe influence of each point is based on the tokenlength of the underlying span, i.e.
short (unreliable)spans which fall into the range of some centroid willhave less effect on the location of the centroid thanlarger spans.
Second, we use a city-block (L1) dis-tance function rather than standard Euclidean (L2)distance function; in the segmentation task, Brookeet alfound that city-block (L1) distance was pre-ferred, a result which is in line with other workin stylistic inconsistency detection (Guthrie, 2008).Though it would be interesting to see if a good kcould be estimated independently, for our purposeshere we set k to be the known number of speakers inour gold standard.4 EvaluationWe evaluate our clusters by comparing them to agold standard annotation.
There are various met-rics for extrinsic cluster evaluation; Amigo?
et al1Occasionally, there was no convergence, at which point wehalted the process arbitrarily after 100 iterations.42(2009) review various options and select the BCubedprecision and recall metrics (Bagga and Baldwin,1998) as having all of a set of key desirable prop-erties.
BCubed precision is a calculation of the frac-tion of item pairs in the same cluster which are alsoin the same category, whereas BCubed recall is thefraction of item pairs in the same category whichare also in the same cluster.
The harmonic meanof these two metrics is BCubed F-score.
Typically,the ?items?
are exactly what has been clustered, butthis is problematic in our case, because we wish tocompare methods which have different segmenta-tions and thus the vectors that are being clusteredare not directly comparable.
Instead, we calculatethe BCubed measures at the level of the token; thatis, for the purposes of measuring performance weact as if we had clustered each token individually,instead of the spans of tokens actually used.Our first evaluation is against a set of 20artificially-generated ?poems?
which are actuallyrandomly generated combinations of parts of 12 po-ems which were chosen (by an English literature ex-pert, one of the authors) to represent the time periodand influences of The Waste Land.
The longest ofthese poems is 1291 tokens and the shortest is just90 tokens (though 10 of the 12 have at least 300 to-kens); the average length is 501 tokens.
Our methodfor creating these poems is similar to that of Kop-pel et al(2011), though generalized for multipleauthors.
For each of the artificial poems, we ran-domly selected 6 poems from the 12 source poems,and then we concatenated 100-200 tokens (or all theremaining tokens, if less than the number selected)from each of these 6 poems to the new combinedpoem until all the poems were exhausted or belowour minimum span length (20 tokens).
This allowsus to evaluate our method in ideal circumstances, i.e.when there are very distinct voices corresponding todifferent poets, and the voice spans tend to be fairlylong.Our gold standard annotation of The Waste Landspeakers is far more tentative.
It is based on anumber of sources: our own English literature ex-pert, relevant literary analysis (Cooper, 1987), andalso The Waste Land app (Touch Press LLP, 2011),which includes readings of the poem by various ex-perts, including T.S.
Eliot himself.
However, thereis inherently a great deal of subjectivity involved inliterary annotation and, indeed, one of the potentialbenefits of our work is to find independent justifi-cation for a particular voice annotation.
Our goldstandard thus represents just one potential interpre-tation of the poem, rather than a true, unique goldstandard.
The average size of the 69 segments inthe gold standard is 50 tokens; the range, however,is fairly wide: the longest is 373 tokens, while theshortest consists of a single token.
Our annotationhas 13 voices altogether.We consider three segmentations: the segmen-tation of our gold standard (Gold), the segmenta-tion predicted by our segmentation model (Auto-matic), and a segmentation which consists of equal-length spans (Even), with the same number of spansas in the gold standard.
The Even segmentationshould be viewed as the baseline for segmentation,and the Gold segmentation an ?oracle?
represent-ing an upper bound on segmentation performance.For the automatic segmentation model, we use thesettings from Brooke et al(2012).
We also com-pare three possible clusterings for each segmenta-tion: no clustering at all (Initial), that is, we assumethat each segment is a new voice; k-means clustering(k-means), as outlined above; and random clustering(Random), in which we randomly assign each voiceto a cluster.
For these latter two methods, which bothhave a random component, we averaged our metricsover 50 runs.
Random and Initial are here, of course,to provide baselines for judging the effectiveness ofk-means clustering model.
Finally, when using thegold standard segmentation and k-means clustering,we included another oracle option (Seeded): insteadof the standard k-means method of randomly choos-ing them from the available datapoints, each cen-troid is initialized to the longest instance of a dif-ferent voice, essentially seeding each cluster.5 ResultsTable 1 contains the results for our first evaluationof voice clustering, the automatically-generated po-ems.
In all the conditions, using the gold segmen-tation far outstrips the other two options.
The au-tomatic segmentation is consistently better than theevenly-spaced baseline, but the performance is actu-ally worse than expected; the segmentation metricswe used in our earlier work43Table 1: Clustering results for artificial poemsConfiguration BCubed metricsPrec.
Rec.
F-scoreInitial Even 0.703 0.154 0.249Initial Automatic 0.827 0.177 0.286Initial Gold 1.000 0.319 0.465Random Even 0.331 0.293 0.307Random Automatic 0.352 0.311 0.327Random Gold 0.436 0.430 0.436k-means Even 0.462 0.409 0.430k-means Automatic 0.532 0.479 0.499k-means Gold 0.716 0.720 0.710k-means Gold Seeded 0.869 0.848 0.855Table 2: Clustering results for The Waste LandConfiguration BCubed metricsPrec.
Rec.
F-scoreInitial Even 0.792 0.069 0.128Initial Automatic 0.798 0.084 0.152Initial Gold 1.000 0.262 0.415Random Even 0.243 0.146 0.183Random Automatic 0.258 0.160 0.198Random Gold 0.408 0.313 0.352k-means Even 0.288 0.238 0.260k-means Automatic 0.316 0.264 0.296k-means Gold 0.430 0.502 0.461k-means Gold Seeded 0.491 0.624 0.550The results for The Waste Land are in Table 2.Many of the basic patterns are the same, includingthe consistent ranking of the methods; overall, how-ever, the clustering is far less effective.
This is par-ticularly true for the gold-standard condition, whichonly increases modestly between the initial and clus-tered state; the marked increase in recall is balancedby a major loss of precision.
In fact, unlike withthe artificial text, the most promising aspect of theclustering seems to be the fairly sizable boost to thequality of clusters in automatic segmenting perfor-mance.
The effect of seeding is also very consistent,nearly as effective as in the automatic case.We also looked at the results for individual speak-ers in The Waste Land; many of the speakers (someof which appear only in a few lines) are very poorlydistinguished, even with the gold-standard segmen-tation and seeding, but there are a few that clusterquite well; the best two are in fact our examples fromSection 1,2 that is, the narrator (F-score 0.869), andthe chatty woman (F-score 0.605).
The former re-sult is particularly important, from the perspectiveof literary analysis, since there are several passageswhich seem to be the main narrator (and our ex-pert annotated them as such) but which are definitelyopen to interpretation.6 ConclusionLiterature, by its very nature, involves combin-ing existing means of expression in surprising newways, resisting supervised analysis methods that de-pend on assumptions of conformity.
Our unsuper-vised approach to distinguishing voices in poetry of-fers this necessary flexibility, and indeed seems towork reasonably well in cases when the stylistic dif-ferences are clear.
The Waste Land, however, is avery subtle text, and our results suggest that we area long way from something that would be a consid-ered a possible human interpretation.
Nevertheless,applying quantitative methods to these kinds of textscan, for literary scholars, bridge the gab betweenabstract interpretations and the details of form andfunction (McKenna and Antonia, 2001).
In our owncase, this computational work is just one aspect ofa larger project in literary analysis where the ulti-mate goal is not to mimic human behavior per se,but rather to better understand literary phenomenaby annotation and modelling of these phenomena(Hammond, 2013; Hammond et al 2013).With respect to future enhancements, improvingsegmentation is obviously important; the best au-tomated efforts so far provide only a small boostover a baseline approach to segmentation.
However,independently of this, our experiments with gold-standard seeding suggest that refining our approachto clustering, e.g.
a method that identifies good ini-tial points for our centroids, may also pay dividendsin the long run.
A more radical idea for future workwould be to remove the somewhat artificial delim-2These passages are the original examples from our earlierwork (Brooke et al 2012), selected by our expert for their dis-tinctness, so the fact that they turned out to be the most easilyclustered is actually a result of sorts (albeit an anecdotal one),suggesting that our clustering behavior does correspond some-what to a human judgment of distinctness.44itation of the task into segmentation and clusteringphases, building a model which works iterativelyto produce segments that are sensitive to points ofstylistic change but that, at a higher level, also formgood clusters (as measured by intrinsic measures ofcluster quality).AcknowledgementsThis work was financially supported by the Natu-ral Sciences and Engineering Research Council ofCanada.ReferencesEnrique Amigo?, Julio Gonzalo, Javier Artiles, and FelisaVerdejo.
2009.
A comparison of extrinsic clusteringevaluation metrics based on formal constraints.
Infor-mation Retrieval, 12:461?486, August.Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Proceedings of the 7th Conference on InternationalLanguage Resources and Evaluation (LREC?10), Val-letta, Malta, May.Amit Bagga and Breck Baldwin.
1998.
Entity-basedcross-document coreferencing using the vector spacemodel.
In Proceedings of the 36th Annual Meetingof the Association for Computational Linguistics and17th International Conference on Computational Lin-guistics (ACL-COLING ?98), pages 79?85, Montreal,Quebec, Canada.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramCorpus Version 1.1.
Google Inc.Julian Brooke, Tong Wang, and Graeme Hirst.
2010.
Au-tomatic acquisition of lexical formality.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics (COLING ?10), Beijing.Julian Brooke, Adam Hammond, and Graeme Hirst.2012.
Unsupervised stylistic segmentation of poetrywith change curves and extrinsic features.
In Proceed-ings of the 1st Workshop on Computational Literaturefor Literature (CLFL ?12), Montreal.Kevin Burton, Akshay Java, and Ian Soboroff.
2009.
TheICWSM 2009 Spinn3r Dataset.
In Proceedings of theThird Annual Conference on Weblogs and Social Me-dia (ICWSM 2009), San Jose, CA.John Xiros Cooper.
1987.
T.S.
Eliot and the politics ofvoice: The argument of The Waste Land.
UMI Re-search Press, Ann Arbor, Mich.Joseph J. Duggan.
1973.
The Song of Roland: Formulaicstyle and poetic craft.
University of California Press.Neil Graham, Graeme Hirst, and Bhaskara Marthi.
2005.Segmenting documents by stylistic character.
NaturalLanguage Engineering, 11(4):397?415.David Guthrie.
2008.
Unsupervised Detection ofAnomalous Text.
Ph.D. thesis, University of Sheffield.Adam Hammond, Julian Brooke, and Graeme Hirst.2013.
A tale of two cultures: Bringing literary analy-sis and computational linguistics together.
In Proceed-ings of the 2nd Workshop on Computational Literaturefor Literature (CLFL ?13), Atlanta.Adam Hammond.
2013.
He do the police in differentvoices: Looking for voices in The Waste Land.
Sem-inar: ?Mapping the Fictional Voice?
American Com-parative Literature Association (ACLA).Justine Kao and Dan Jurafsky.
2012.
A computationalanalysis of style, sentiment, and imagery in contem-porary poetry.
In Proceedings of the 1st Workshop onComputational Literature for Literature (CLFL ?12),Montreal.Moshe Koppel, Navot Akiva, Idan Dershowitz, andNachum Dershowitz.
2011.
Unsupervised decompo-sition of a document into authorial components.
InProceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics (ACL ?11), Port-land, Oregon.Thomas K. Landauer and Susan Dumais.
1997.
A so-lution to Plato?s problem: The latent semantic analysistheory of the acquisition, induction, and representationof knowledge.
Psychological Review, 104:211?240.Kim Luyckx, Walter Daelemans, and Edward Vanhoutte.2006.
Stylogenetics: Clustering-based stylistic analy-sis of literary corpora.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (LREC ?06), Genoa, Italy.J.
B. MacQueen.
1967.
Some methods for classificationand analysis of multivariate observations.
In Proceed-ings of the Fifth Berkeley Symposium on MathematicalStatistics and Probability, pages 281?297.C.
W. F. McKenna and A. Antonia.
2001.
The statisticalanalysis of style: Reflections on form, meaning, andideology in the ?Nausicaa?
episode of Ulysses.
Liter-ary and Linguistic Computing, 16(4):353?373.Lev Pevzner and Marti A. Hearst.
2002.
A critique andimprovement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28:19?36, March.Dean Keith Simonton.
1990.
Lexical choices and aes-thetic success: A computer content analysis of 154Shakespeare sonnets.
Computers and the Humanities,24(4):251?264.Efstathios Stamatatos.
2009.
A survey of modern au-thorship attribution methods.
Journal of the Ameri-can Society for Information Science and Technology,60(3):538?556.45Benno Stein, Nedim Lipka, and Peter Prettenhofer.
2011.Intrinsic plagiarism analysis.
Language Resourcesand Evaluation, 45(1):63?82.Touch Press LLP.
2011.
The Waste Landapp.
http://itunes.apple.com/ca/app/the-waste-land/id427434046?mt=8 .46
