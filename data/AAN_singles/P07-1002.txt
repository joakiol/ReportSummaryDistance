Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 9?16,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Discriminative Syntactic Word Order Model for Machine TranslationPi-Chuan Chang?Computer Science DepartmentStanford UniversityStanford, CA 94305pichuan@stanford.eduKristina ToutanovaMicrosoft ResearchRedmond, WAkristout@microsoft.comAbstractWe present a global discriminative statisticalword order model for machine translation.Our model combines syntactic movementand surface movement information, and isdiscriminatively trained to choose amongpossible word orders.
We show that com-bining discriminative training with featuresto detect these two different kinds of move-ment phenomena leads to substantial im-provements in word ordering performanceover strong baselines.
Integrating this wordorder model in a baseline MT system resultsin a 2.4 points improvement in BLEU forEnglish to Japanese translation.1 IntroductionThe machine translation task can be viewed as con-sisting of two subtasks: predicting the collection ofwords in a translation, and deciding the order of thepredicted words.
For some language pairs, such asEnglish and Japanese, the ordering problem is es-pecially hard, because the target word order differssignificantly from the source word order.Previous work has shown that it is useful to modeltarget language order in terms of movement of syn-tactic constituents in constituency trees (Yamadaand Knight, 2001; Galley et al, 2006) or depen-dency trees (Quirk et al, 2005), which are obtainedusing a parser trained to determine linguistic con-stituency.
Alternatively, order is modelled in termsof movement of automatically induced hierarchicalstructure of sentences (Chiang, 2005; Wu, 1997).?
This research was conducted during the author?s intern-ship at Microsoft Research.The advantages of modeling how a target lan-guage syntax tree moves with respect to a source lan-guage syntax tree are that (i) we can capture the factthat constituents move as a whole and generally re-spect the phrasal cohesion constraints (Fox, 2002),and (ii) we can model broad syntactic reorderingphenomena, such as subject-verb-object construc-tions translating into subject-object-verb ones, as isgenerally the case for English and Japanese.On the other hand, there is also significant amountof information in the surface strings of the sourceand target and their alignment.
Many state-of-the-artSMT systems do not use trees and base the orderingdecisions on surface phrases (Och and Ney, 2004;Al-Onaizan and Papineni, 2006; Kuhn et al, 2006).In this paper we develop an order model for machinetranslation which makes use of both syntactic andsurface information.The framework for our statistical model is as fol-lows.
We assume the existence of a dependency treefor the source sentence, an unordered dependencytree for the target sentence, and a word alignmentbetween the target and source sentences.
Figure 1(a) shows an example of aligned source and targetdependency trees.
Our task is to order the target de-pendency tree.We train a statistical model to select the best or-der of the unordered target dependency tree.
An im-portant advantage of our model is that it is global,and does not decompose the task of ordering a tar-get sentence into a series of local decisions, as in therecently proposed order models for Machine Transi-tion (Al-Onaizan and Papineni, 2006; Xiong et al,2006; Kuhn et al, 2006).
Thus we are able to definefeatures over complete target sentence orders, andavoid the independence assumptions made by these9all constraints are satisfied[??]
[??]
[?]
[???][???]
[????]?restriction??condition?
TOPIC ?all?
?satisfy?
PASSIVE-PRESc d e f g h(a)fe cd g hfe cd ghfe cdg h(b)Figure 1: (a) A sentence pair with source depen-dency tree, projected target dependency tree, andword alignments.
(b) Example orders violating thetarget tree projectivity constraints.models.
Our model is discriminatively trained to se-lect the best order (according to the BLEU measure)(Papineni et al, 2001) of an unordered target depen-dency tree from the space of possible orders.Since the space of all possible orders of an un-ordered dependency tree is factorially large, we trainour model on N-best lists of possible orders.
TheseN-best lists are generated using approximate searchand simpler models, as in the re-ranking approach of(Collins, 2000).We first evaluate our model on the task of orderingtarget sentences, given correct (reference) unorderedtarget dependency trees.
Our results show that com-bining features derived from the source and tar-get dependency trees, distortion surface order-basedfeatures (like the distortion used in Pharaoh (Koehn,2004)) and language model-like features results in amodel which significantly outperforms models usingonly some of the information sources.We also evaluate the contribution of our modelto the performance of an MT system.
We inte-grate our order model in the MT system, by simplyre-ordering the target translation sentences outputby the system.
The model resulted in an improve-ment from 33.6 to 35.4 BLEU points in English-to-Japanese translation on a computer domain.2 Task SetupThe ordering problem in MT can be formulated asthe task of ordering a target bag of words, given asource sentence and word alignments between tar-get and source words.
In this work we also assumea source dependency tree and an unordered targetdependency tree are given.
Figure 1(a) shows an ex-ample.
We build a model that predicts an order ofthe target dependency tree, which induces an orderon the target sentence words.
The dependency treeconstrains the possible orders of the target sentenceonly to the ones that are projective with respect tothe tree.
An order of the sentence is projective withrespect to the tree if each word and its descendantsform a contiguous subsequence in the ordered sen-tence.
Figure 1(b) shows several orders of the sen-tence which violate this constraint.1Previous studies have shown that if both thesource and target dependency trees represent lin-guistic constituency, the alignment between subtreesin the two languages is very complex (Wellington etal., 2006).
Thus such parallel trees would be difficultfor MT systems to construct in translation.
In thiswork only the source dependency trees are linguisti-cally motivated and constructed by a parser trainedto determine linguistic structure.
The target depen-dency trees are obtained through projection of thesource dependency trees, using the word alignment(we use GIZA++ (Och and Ney, 2004)), ensuringbetter parallelism of the source and target structures.2.1 Obtaining Target Dependency TreesThrough ProjectionOur algorithm for obtaining target dependency treesby projection of the source trees via the word align-ment is the one used in the MT system of (Quirket al, 2005).
We describe the algorithm schemat-ically using the example in Figure 1.
Projectionof the dependency tree through alignments is not atall straightforward.
One of the reasons of difficultyis that the alignment does not represent an isomor-phism between the sentences, i.e.
it is very oftennot a one-to-one and onto mapping.2 If the align-ment were one-to-one we could define the parent ofa word wt in the target to be the target word alignedto the parent of the source word si aligned to wt.
Anadditional difficulty is that such a definition could re-sult in a non-projective target dependency tree.
Theprojection algorithm of (Quirk et al, 2005) definesheuristics for each of these problems.
In case ofone-to-many alignments, for example, the case of?constraints?
aligning to the Japanese words for ?re-striction?
and ?condition?, the algorithm creates a1For example, in the first order shown, the descendants ofword 6 are not contiguous and thus this order violates the con-straint.2In an onto mapping, every word on the target side is asso-ciated with some word on the source side.10subtree in the target rooted at the rightmost of thesewords and attaches the other word(s) to it.
In case ofnon-projectivity, the dependency tree is modified byre-attaching nodes higher up in the tree.
Such a stepis necessary for our example sentence, because thetranslations of the words ?all?
and ?constraints?
arenot contiguous in the target even though they form aconstituent in the source.An important characteristic of the projection algo-rithm is that all of its heuristics use the correct targetword order.3 Thus the target dependency trees en-code more information than is present in the sourcedependency trees and alignment.2.2 Task Setup for Reference Sentences vs MTOutputOur model uses input of the same form whentrained/tested on reference sentences and when usedin machine translation: a source sentence with a de-pendency tree, an unordered target sentence withand unordered target dependency tree, and wordalignments.We train our model on reference sentences.
In thissetting, the given target dependency tree contains thecorrect bag of target words according to a referencetranslation, and is projective with respect to the cor-rect word order of the reference by construction.
Wealso evaluate our model in this setting; such an eval-uation is useful because we can isolate the contribu-tion of an order model, and develop it independentlyof an MT system.When translating new sentences it is not possibleto derive target dependency trees by the projectionalgorithm described above.
In this setting, we usetarget dependency trees constructed by our baselineMT system (described in detail in 6.1).
The systemconstructs dependency trees of the form shown inFigure 1 for each translation hypothesis.
In this casethe target dependency trees very often do not con-tain the correct target words and/or are not projectivewith respect to the best possible order.3For example, checking which word is the rightmost for theheuristic for one-to-many mappings and checking whether theconstructed tree is projective requires knowledge of the correctword order of the target.3 Language Model with SyntacticConstraints: A Pilot StudyIn this section we report the results of a pilot study toevaluate the difficulty of ordering a target sentence ifwe are given a target dependency tree as the one inFigure 1, versus if we are just given an unorderedbag of target language words.The difference between those two settings is thatwhen ordering a target dependency tree, many of theorders of the sentence are not allowed, because theywould be non-projective with respect to the tree.Figure 1 (b) shows some orders which violate theprojectivity constraint.
If the given target depen-dency tree is projective with respect to the correctword order, constraining the possible orders to theones consistent with the tree can only help perfor-mance.
In our experiments on reference sentences,the target dependency trees are projective by con-struction.
If, however, the target dependency treeprovided is not necessarily projective with respectto the best word order, the constraint may or maynot be useful.
This could happen in our experimentson ordering MT output sentences.Thus in this section we aim to evaluate the use-fulness of the constraint in both settings: referencesentences with projective dependency trees, and MToutput sentences with possibly non-projective de-pendency trees.
We also seek to establish a baselinefor our task.
Our methodology is to test a simpleand effective order model, which is used by all stateof the art SMT systems ?
a trigram language model?
in the two settings: ordering an unordered bag ofwords, and ordering a target dependency tree.Our experimental design is as follows.
Given anunordered sentence t and an unordered target de-pendency tree tree(t), we define two spaces of tar-get sentence orders.
These are the unconstrainedspace of all permutations, denoted by Permutations(t)and the space of all orders of t which are projec-tive with respect to the target dependency tree, de-noted by TargetProjective(t,tree(t)).
For both spacesS, we apply a standard trigram target languagemodel to select a most likely order from the space;i.e., we find a target order order?S (t) such that:order?S (t) = argmaxorder(t)?SPrLM (order(t)).The operator which finds order?S (t) is difficult toimplement since the task is NP-hard in both set-11Reference SentencesSpace BLEU Avg.
SizePermutations 58.8 261TargetProjective 83.9 229MT Output SentencesSpace BLEU Avg.
SizePermutations 26.3 256TargetProjective 31.7 225Table 1: Performance of a tri-gram language modelon ordering reference and MT output sentences: un-constrained or subject to target tree projectivity con-straints.tings, even for a bi-gram language model (Eisnerand Tromble, 2006).4 We implemented left-to-rightbeam A* search for the Permutations space, and atree-based bottom up beam A* search for the Tar-getProjective space.
To give an estimate of the searcherror in each case, we computed the number of timesthe correct order had a better language model scorethan the order returned by the search algorithm.5The lower bounds on search error were 4% for Per-mutations and 2% for TargetProjective, computed onreference sentences.We compare the performance in BLEU of ordersselected from both spaces.
We evaluate the perfor-mance on reference sentences and on MT outputsentences.
Table 1 shows the results.
In additionto BLEU scores, the table shows the median numberof possible orders per sentence for the two spaces.The highest achievable BLEU on reference sen-tences is 100, because we are given the correct bagof words.
The highest achievable BLEU on MT out-put sentences is well below 100 (the BLEU score ofthe MT output sentences is 33).
Table 3 describesthe characteristics of the main data-sets used in theexperiments in this paper; the test sets we use in thepresent pilot study are the reference test set (Ref-test) of 1K sentences and the MT test set (MT-test)of 1,000 sentences.The results from our experiment show that the tar-get tree projectivity constraint is extremely powerfulon reference sentences, where the tree given is in-deed projective.
(Recall that in order to obtain thetarget dependency tree in this setting we have usedinformation from the true order, which explains inpart the large performance gain.
)4Even though the dependency tree constrains the space, thenumber of children of a node is not bounded by a constant.5This is an underestimate of search error, because we don?tknow if there was another (non-reference) order which had abetter score, but was not found.The gain in BLEU due to the constraint was notas large on MT output sentences, but was still con-siderable.
The reduction in search space size dueto the constraint is enormous.
There are about 230times fewer orders to consider in the space of tar-get projective orders, compared to the space of allpermutations.
From these experiments we concludethat the constraints imposed by a projective targetdependency tree are extremely informative.
We alsoconclude that the constraints imposed by the targetdependency trees constructed by our baseline MTsystem are very informative as well, even thoughthe trees are not necessarily projective with respectto the best order.
Thus the projectivity constraintwith respect to a reasonably good target dependencytree is useful for addressing the search and modelingproblems for MT ordering.4 A Global Order Model for TargetDependency TreesIn the rest of the paper we present our new word or-der model and evaluate it on reference sentences andin machine translation.
In line with previous workon NLP tasks such as parsing and recent work onmachine translation, we develop a discriminative or-der model.
An advantage of such a model is that wecan easily combine different kinds of features (suchas syntax-based and surface-based), and that we canoptimize the parameters of our model directly for theevaluation measures of interest.Additionally, we develop a globally normalizedmodel, which avoids the independence assumptionsin locally normalized conditional models.6 We traina global log-linear model with a rich set of syntacticand surface features.
Because the space of possibleorders of an unordered dependency tree is factori-ally large, we use simpler models to generate N-bestorders, which we then re-rank with a global model.4.1 Generating N-best OrdersThe simpler models which we use to generate N-bestorders of the unordered target dependency trees arethe standard trigram language model used in Section3, and another statistical model, which we call a Lo-cal Tree Order Model (LTOM).
The LTOM model6Those models often assume that current decisions are inde-pendent of future observations.12[??
]this-1 eliminates the six minute delay+1[?
?
-2] [??
? ]
[6] [?]
[?]
[? ]
[??
-1] [? ]
[?
??
?
]Pron Verb Det Funcw Funcw Noun[kore] [niyori] [roku] [fun] [kan] [no] [okure] [ga] [kaishou] [saremasu]Pron Posp Noun Noun Noun Posp Noun Posp Vn Auxv?this?
?by?
6 ?minute?
?period?
?of?
?delay?
?eliminate?
PASSIVEFigure 2: Dependency parse on the source (English)sentence, alignment and projected tree on the target(Japanese) sentence.
Notice that the projected treeis only partial and is used to show the head-relativemovement.uses syntactic information from the source and tar-get dependency trees, and orders each local tree ofthe target dependency tree independently.
It followsthe order model defined in (Quirk et al, 2005).The model assigns a probability to the positionof each target node (modifier) relative to its par-ent (head), based on information in both the sourceand target trees.
The probability of an order of thecomplete target dependency tree decomposes into aproduct over probabilities of positions for each nodein the tree as follows:P (order(t)|s, t) =?n?tP (pos(n, parent(n))|s, t)Here, position is modelled in terms of closenessto the head in the dependency tree.
The closestpre-modifier of a given head has position ?1; theclosest post-modifier has a position 1.
Figure 2shows an example dependency tree pair annotatedwith head-relative positions.
A small set of featuresis used to reflect local information in the dependencytree to model P (pos(n, parent(n))|s, t): (i) lexicalitems of n and parent(n), (ii) lexical items of thesource nodes aligned to n and parent(n), (iii) part-of-speech of the source nodes aligned to the nodeand its parent, and (iv) head-relative position of thesource node aligned to the target node.We train a log-linear model which uses these fea-tures on a training set of aligned sentences withsource and target dependency trees in the form ofFigure 2.
The model is a local (non-sequence) clas-sifier, because the decision on where to place eachnode does not depend on the placement of any othernodes.Since the local tree order model learns to orderwhole subtrees of the target dependency tree, andsince it uses syntactic information from the source, itprovides an alternative view compared to the trigramlanguage model.
The example in Figure 2 showsthat the head word ?eliminates?
takes a dependent?this?
to the left (position ?1), and on the Japaneseside, the head word ?kaishou?
(corresponding to?eliminates?)
takes a dependent ?kore?
(correspond-ing to ?this?)
to the left (position ?2).
The trigramlanguage model would not capture the position of?kore?
with respect to ?kaishou?, because the wordsare farther than three positions away.We use the language model and the local tree or-der model to create N-best target dependency treeorders.
In particular, we generate the N-best listsfrom a simple log-linear combination of the twomodels:P (o(t)|s, t) ?
PLM (o(t)|t)PLTOM (o(t)|s, t)?where o(t) denotes an order of the target.7 We useda bottom-up beam A* search to generate N-best or-ders.
The performance of each of these two modelsand their combination, together with the 30-best or-acle performance on reference sentences is shown inTable 2.
As we can see, the 30-best oracle perfor-mance of the combined model (98.0) is much higherthan the 1-best performance (92.6) and thus there isa lot of room for improvement.4.2 ModelThe log-linear reranking model is defined as fol-lows.
For each sentence pair spl (l = 1, 2, ..., L) inthe training data, we have N candidate target wordorders ol,1, ol,2, ..., ol,N , which are the orders gener-ated from the simpler models.
Without loss of gen-erality, we define ol,1 to be the order with the highestBLEU score with respect to the correct order.8We define a set of feature functions fm(ol,n, spl)to describe a target word order ol,n of a given sen-tence pair spl.
In the log-linear model, a correspond-ing weights vector ?
is used to define the distributionover all possible candidate orders:p(ol,n|spl, ?)
= e?F (ol,n,spl)?n?
e?F (ol,n?
,spl)7We used the value ?
= .5, which we selected on a devel-opment set to maximize BLEU.8To avoid the problem that all orders could have a BLEUscore of 0 if none of them contains a correct word four-gram,we define sentence-level k-gram BLEU, where k is the highestorder, k ?
4, for which there exists a correct k-gram in at leastone of the N-Best orders.13We train the parameters ?
by minimizing the neg-ative log-likelihood of the training data plus aquadratic regularization term:L(?)
= ?
?l log p(ol,1|spi, ?)
+ 12?2?m ?m2We also explored maximizing expected BLEU asour objective function, but since it is not convex, theperformance was less stable and ultimately slightlyworse, as compared to the log-likelihood objective.4.3 FeaturesWe design features to capture both the head-relativemovement and the surface sequence movement ofwords in a sentence.
We experiment with differentcombinations of features and show their contribu-tion in Table 2 for reference sentences and Table 4in machine translation.
The notations used in the ta-bles are defined as follows:Baseline: LTOM+LM as described in Section 4.1Word Bigram: Word bigrams of the target sen-tence.
Examples from Figure 2: ?kore?+?niyori?,?niyori?+?roku?.DISP: Displacement feature.
For each word posi-tion in the target sentence, we examine the align-ment of the current word and the previous word, andcategorize the possible patterns into 3 kinds: (a) par-allel, (b) crossing, and (c) widening.
Figure 3 showshow these three categories are defined.Pharaoh DISP: Displacement as used in Pharaoh(Koehn, 2004).
For each position in the sentence,the value of the feature is one less than the difference(absolute value) of the positions of the source wordsaligned to the current and the previous target word.POSs and POSt: POS tags on the source and targetsides.
For Japanese, we have a set of 19 POS tags.?+?
means making conjunction of features andprev() means using the information associated withthe word from position ?1.In all explored models, we include the log-probability of an order according to the languagemodel and the log-probability according to the lo-cal tree order model, the two features used by thebaseline model.5 Evaluation on Reference SentencesOur experiments on ordering reference sentencesuse a set of 445K English sentences with their ref-erence Japanese translations.
This is a subset of the(N (N-L -L(a) parallel(N (NQ-L -L(b) crossing(N (NQ-L -L(c) wideningFigure 3: Displacement feature: different alignmentpatterns of two contiguous words in the target sen-tence.set MT-train in Table 3.
The sentences were anno-tated with alignment (using GIZA++ (Och and Ney,2004)) and syntactic dependency structures of thesource and target, obtained as described in Section2.
Japanese POS tags were assigned by an automaticPOS tagger, which is a local classifier not using tagsequence information.We used 400K sentence pairs from the completeset to train the first pass models: the language modelwas trained on 400K sentences, and the local treeorder model was trained on 100K of them.
We gen-erated N-best target tree orders for the rest of thedata (45K sentence pairs), and used it for trainingand evaluating the re-ranking model.
The re-rankingmodel was trained on 44K sentence pairs.
All mod-els were evaluated on the remaining 1,000 sentencepairs set, which is the set Ref-test in Table 3.The top part of Table 2 presents the 1-bestBLEU scores (actual performance) and 30-best or-acle BLEU scores of the first-pass models and theirlog-linear combination, described in Section 4.
Wecan see that the combination of the language modeland the local tree order model outperformed eithermodel by a large margin.
This indicates that combin-ing syntactic (from the LTOM model) and surface-based (from the language model) information is veryeffective even at this stage of selecting N-best ordersfor re-ranking.
According to the 30-best oracle per-formance of the combined model LTOM+LM, 98.0BLEU is the upper bound on performance of our re-ranking approach.The bottom part of the table shows the perfor-mance of the global log-linear model, when featuresin addition to the scores from the two first-pass mod-els are added to the model.
Adding word-bigramfeatures increased performance by about 0.6 BLEUpoints, indicating that training language-model likefeatures discriminatively to optimize ordering per-formance, is indeed worthwhile.
Next we compare14First-pass modelsModel BLEU1 best 30 bestLang Model (Permutations) 58.8 71.2Lang Model (TargetProjective) 83.9 95.0Local Tree Order Model 75.8 87.3Local Tree Order Model + Lang Model 92.6 98.0Re-ranking ModelsFeatures BLEUBaseline 92.60Word Bigram 93.19Pharaoh DISP 92.94DISP 93.57DISP+POSs 94.04DISP+POSs+POSt 94.14DISP+POSs+POSt, prev(DISP)+POSs+POSt 94.34DISP+POSs+POSt, prev(DISP)+POSs+POSt, WB 94.50Table 2: Performance of the first-pass order modelsand 30-best oracle performance, followed by perfor-mance of re-ranking model for different feature sets.Results are on reference sentences.the Pharaoh displacement feature to the displace-ment feature we illustrated in Figure 3.
We cansee that the Pharaoh displacement feature improvesperformance of the baseline by .34 points, whereasour displacement feature improves performance bynearly 1 BLEU point.
Concatenating the DISP fea-ture with the POS tag of the source word aligned tothe current word improved performance slightly.The results show that surface movement features(i.e.
the DISP feature) improve the performanceof a model using syntactic-movement features (i.e.the LTOM model).
Additionally, adding part-of-speech information from both languages in combi-nation with displacement, and using a higher orderon the displacement features was useful.
The per-formance of our best model, which included all in-formation sources, is 94.5 BLEU points, which is a35% improvement over the fist-pass models, relativeto the upper bound.6 Evaluation in Machine TranslationWe apply our model to machine translation by re-ordering the translation produced by a baseline MTsystem.
Our baseline MT system constructs, foreach target translation hypothesis, a target depen-dency tree.
Thus we can apply our model to MToutput in exactly the same way as for reference sen-tences, but using much noisier input: a source sen-tence with a dependency tree, word alignment andan unordered target dependency tree as the exampleshown in Figure 2.
The difference is that the targetdependency tree will likely not contain the correctdata set num sent.
English Japaneseavg.
len vocab avg.
len vocabMT-train 500K 15.8 77K 18.7 79KMT-test 1K 17.5 ?
20.9 ?Ref-test 1K 17.5 ?
21.2 ?Table 3: Main data sets used in experiments.target words and/or will not be projective with re-spect to the best possible order.6.1 Baseline MT SystemOur baseline SMT system is the system of Quirk etal.
(2005).
It translates by first deriving a depen-dency tree for the source sentence and then trans-lating the source dependency tree to a target depen-dency tree, using a set of probabilistic models.
Thetranslation is based on treelet pairs.
A treelet is aconnected subgraph of the source or target depen-dency tree.
A treelet translation pair is a pair ofword-aligned source and target treelets.The baseline SMT model combines this treelettranslation model with other feature functions ?
atarget language model, a tree order model, lexicalweighting features to smooth the translation prob-abilities, word count feature, and treelet-pairs countfeature.
These models are combined as feature func-tions in a (log)linear model for predicting a targetsentence given a source sentence, in the frameworkproposed by (Och and Ney, 2002).
The weightsof this model are trained to maximize BLEU (Ochand Ney, 2004).
The SMT system is trained usingthe same form of data as our order model: parallelsource and target dependency trees as in Figure 2.Of particular interest are the components in thebaseline SMT system contributing most to word or-der decisions.
The SMT system uses the same targetlanguage trigram model and local tree order model,as we are using for generating N-best orders for re-ranking.
Thus the baseline system already uses ourfirst-pass order models and only lacks the additionalinformation provided by our re-ranking order model.6.2 Data and Experimental ResultsThe baseline MT system was trained on the MT-traindataset described in Table 3.
The test set for the MTexperiment is a 1K sentences set from the same do-main (shown as MT-test in the table).
The weightsin the linear model used by the baseline SMT systemwere tuned on a separate development set.Table 4 shows the performance of the first-passmodels in the top part, and the performance of our15First-pass modelsModel BLEU1 best 30 bestBaseline MT System 33.0 ?Lang Model (Permutations) 26.3 28.7Lang Model (TargetCohesive) 31.7 35.0Local Tree Order Model 27.2 31.5Local Tree Order Model + Lang Model 33.6 36.0Re-ranking ModelsFeatures BLEUBaseline 33.56Word Bigram 34.11Pharaoh DISP 34.67DISP 34.90DISP+POSs 35.28DISP+POSs+POSt 35.22DISP+POSs+POSt, prev(DISP)+POSs+POSt 35.33DISP+POSs+POSt, prev(DISP)+POSs+POSt, WB 35.37Table 4: Performance of the first pass order modelsand 30-best oracle performance, followed by perfor-mance of re-ranking model for different feature sets.Results are in MT.re-ranking model in the bottom part.
The first rowof the table shows the performance of the baselineMT system, which is a BLEU score of 33.
Our first-pass and re-ranking models re-order the words ofthis 1-best output from the MT system.
As for ref-erence sentences, the combination of the two first-pass models outperforms the individual models.
The1-best performance of the combination is 33.6 andthe 30-best oracle is 36.0.
Thus the best we coulddo with our re-ranking model in this setting is 36BLEU points.9 Our best re-ranking model achieves2.4 BLEU points improvement over the baseline MTsystem and 1.8 points improvement over the first-pass models, as shown in the table.
The trends hereare similar to the ones observed in our reference ex-periments, with the difference that target POS tagswere less useful (perhaps due to ungrammatical can-didates) and the displacement features were moreuseful.
We can see that our re-ranking model al-most reached the upper bound oracle performance,reducing the gap between the first-pass models per-formance (33.6) and the oracle (36.0) by 75%.7 Conclusions and Future WorkWe have presented a discriminative syntax-based or-der model for machine translation, trained to to se-9Notice that the combination of our two first-pass modelsoutperforms the baseline MT system by half a point (33.6 ver-sus 33.0).
This is perhaps due to the fact that the MT systemsearches through a much larger space (possible word transla-tions in addition to word orders), and thus could have a highersearch error.lect from the space of orders projective with respectto a target dependency tree.
We investigated a com-bination of features modeling surface movement andsyntactic movement phenomena and showed thatthese two information sources are complementaryand their combination is powerful.
Our results on or-dering MT output and reference sentences were veryencouraging.
We obtained substantial improvementby the simple method of post-processing the 1-bestMT output to re-order the proposed translation.
Inthe future, we would like to explore tighter integra-tion of our order model with the SMT system and todevelop more accurate algorithms for constructingprojective target dependency trees in translation.ReferencesY.
Al-Onaizan and K. Papineni.
2006.
Distortion models forstatistical machine translation.
In ACL.D.
Chiang.
2005.
A hierarchical phrase-based model for statis-tical machine translation.
In ACL.M.
Collins.
2000.
Discriminative reranking for natural languageparsing.
In ICML, pages 175?182.J Eisner and R. W. Tromble.
2006.
Local search with verylarge-scale neighborhoods for optimal permutations in ma-chine translation.
In HLT-NAACL Workshop.H.
Fox.
2002.
Phrasal cohesion and statistical machine transla-tion.
In EMNLP.M.
Galley, J. Graehl, K. Knight, D. Marcu, S. DeNeefe,W.
Wang, and I. Thayer.
2006.
Scalable inference and train-ing of context-rich syntactic translation models.
In ACL.P.
Koehn.
2004.
Pharaoh: A beam search decoder for phrase-based statistical machine translation models.
In AMTA.R.
Kuhn, D. Yuen, M. Simard, P. Paul, G. Foster, E. Joanis, andH.
Johnson.
2006.
Segment choice models: Feature-richmodels for global distortion in statistical machine transla-tion.
In HLT-NAACL.F.
J. Och and H. Ney.
2002.
Discriminative training and max-imum entropy models for statistical machine translation.
InACL.F.
J. Och and H. Ney.
2004.
The alignment template approachto statistical machine translation.
Computational Linguistics,30(4).K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2001.
BLEU: amethod for automatic evaluation of machine translation.
InACL.C.
Quirk, A. Menezes, and C. Cherry.
2005.
Dependency treelettranslation: Syntactically informed phrasal SMT.
In ACL.B.
Wellington, S. Waxmonsky, and I. Dan Melamed.
2006.Empirical lower bounds on the complexity of translationalequivalence.
In ACL-COLING.D.
Wu.
1997.
Stochastic inversion transduction grammars andbilingual parsing of parallel corpora.
Computational Lin-guistics, 23(3):377?403.D.
Xiong, Q. Liu, and S. Lin.
2006.
Maximum entropy basedphrase reordering model for statistical machine translation.In ACL.K.
Yamada and Kevin Knight.
2001.
A syntax-based statisticaltranslation model.
In ACL.16
