Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1448?1458,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSemantic Frame Identification with Distributed Word RepresentationsKarl Moritz Hermann?
?Dipanjan Das?Jason Weston?Kuzman Ganchev?
?Department of Computer Science, University of Oxford, Oxford OX1 3QD, United Kingdom?Google Inc., 76 9th Avenue, New York, NY 10011, United Stateskarl.moritz.hermann@cs.ox.ac.uk{dipanjand,kuzman}@google.com jaseweston@gmail.comAbstractWe present a novel technique for semanticframe identification using distributed rep-resentations of predicates and their syntac-tic context; this technique leverages auto-matic syntactic parses and a generic setof word embeddings.
Given labeled dataannotated with frame-semantic parses, welearn a model that projects the set of wordrepresentations for the syntactic contextaround a predicate to a low dimensionalrepresentation.
The latter is used for se-mantic frame identification; with a stan-dard argument identification method in-spired by prior work, we achieve state-of-the-art results on FrameNet-style frame-semantic analysis.
Additionally, we reportstrong results on PropBank-style semanticrole labeling in comparison to prior work.1 IntroductionDistributed representations of words have proveduseful for a number of tasks.
By providing richerrepresentations of meaning than what can be en-compassed in a discrete representation, such ap-proaches have successfully been applied to taskssuch as sentiment analysis (Socher et al, 2011),topic classification (Klementiev et al, 2012) orword-word similarity (Mitchell and Lapata, 2008).We present a new technique for semantic frameidentification that leverages distributed word rep-resentations.
According to the theory of frame se-mantics (Fillmore, 1982), a semantic frame rep-resents an event or scenario, and possesses frameelements (or semantic roles) that participate in the?The majority of this research was carried out during aninternship at Google.event.
Most work on frame-semantic parsing hasusually divided the task into two major subtasks:frame identification, namely the disambiguation ofa given predicate to a frame, and argument iden-tification (or semantic role labeling), the analysisof words and phrases in the sentential context thatsatisfy the frame?s semantic roles (Das et al, 2010;Das et al, 2014).1Here, we focus on the first sub-task of frame identification for given predicates;we use our novel method (?3) in conjunction witha standard argument identification model (?4) toperform full frame-semantic parsing.We present experiments on two tasks.
First, weshow that for frame identification on the FrameNetcorpus (Baker et al, 1998; Fillmore et al, 2003),we outperform the prior state of the art (Das et al,2014).
Moreover, for full frame-semantic parsing,with the presented frame identification techniquefollowed by our argument identification method,we report the best results on this task to date.
Sec-ond, we present results on PropBank-style seman-tic role labeling (Palmer et al, 2005; Meyers et al,2004; M`arquez et al, 2008), that approach strongbaselines, and are on par with prior state of the art(Punyakanok et al, 2008).2 OverviewEarly work in frame-semantic analysis was pio-neered by Gildea and Jurafsky (2002).
Subsequentwork in this area focused on either the FrameNetor PropBank frameworks, and research on the lat-ter has been more popular.
Since the CoNLL2004-2005 shared tasks (Carreras and M`arquez,1There are exceptions, wherein the task has been modeledusing a pipeline of three classifiers that perform frame iden-tification, a binary stage that classifies candidate arguments,and argument identification on the filtered candidates (Bakeret al, 2007; Johansson and Nugues, 2007).1448John     bought    a   car   .COMMERCE_BUYbuy.VBuyer GoodsJohn     bought    a   car   .buy.01buy.VA0 A1Mary      sold        a   car   .COMMERCE_BUYsell.VSeller GoodsMary      sold        a   car   .sell.01sell.VA0 A1(a) (b)Figure 1: Example sentences with frame-semantic analyses.FrameNet annotation conventions are used in (a) while (b)denotes PropBank conventions.2004; Carreras and M`arquez, 2005) on PropBanksemantic role labeling (SRL), it has been treatedas an important NLP problem.
However, researchhas mostly focused on argument analysis, skippingthe frame disambiguation step, and its interactionwith argument identification.2.1 Frame-Semantic ParsingClosely related to SRL, frame-semantic parsingconsists of the resolution of predicate sense intoa frame, and the analysis of the frame?s argu-ments.
Work in this area exclusively uses theFrameNet full text annotations.
Johansson andNugues (2007) presented the best performing sys-tem at SemEval 2007 (Baker et al, 2007), and Daset al (2010) improved performance, and later setthe current state of the art on this task (Das et al,2014).
We briefly discuss FrameNet, and subse-quently PropBank annotation conventions here.FrameNet The FrameNet project (Baker et al,1998) is a lexical database that contains informa-tion about words and phrases (represented as lem-mas conjoined with a coarse part-of-speech tag)termed as lexical units, with a set of semanticframes that they could evoke.
For each frame,there is a list of associated frame elements (orroles, henceforth), that are also distinguished ascore or non-core.2Sentences are annotated us-ing this universal frame inventory.
For exam-ple, consider the pair of sentences in Figure 1(a).COMMERCE BUY is a frame that can be evoked bymorphological variants of the two example lexicalunits buy.V and sell.V.
Buyer, Seller and Goods aresome example roles for this frame.2Additional information such as finer distinction of thecoreness properties of roles, the relationship between frames,and that of roles are also present, but we do not leverage thatinformation in this work.PropBank The PropBank project (Palmer et al,2005) is another popular resource related to se-mantic role labeling.
The PropBank corpus hasverbs annotated with sense frames and their ar-guments.
Like FrameNet, it also has a lexi-cal database that stores type information aboutverbs, in the form of sense frames and the possi-ble semantic roles each frame could take.
Thereare modifier roles that are shared across verbframes, somewhat similar to the non-core rolesin FrameNet.
Figure 1(b) shows annotations fortwo verbs ?bought?
and ?sold?, with their lemmas(akin to the lexical units in FrameNet) and theirverb frames buy.01 and sell.01.
Generic core rolelabels (of which there are seven, namely A0-A5 andAA) for the verb frames are marked in the figure.3A key difference between the two annotation sys-tems is that PropBank uses a local frame inven-tory, where frames are predicate-specific.
More-over, role labels, although few in number, take spe-cific meaning for each verb frame.
Figure 1 high-lights this difference: while both sell.V and buy.Vare members of the same frame in FrameNet, theyevoke different frames in PropBank.
In spite ofthis difference, nearly identical statistical modelscould be employed for both frameworks.Modeling In this paper, we model the frame-semantic parsing problem in two stages: frameidentification and argument identification.
Asmentioned in ?1, these correspond to a frame dis-ambiguation stage,4and a stage that finds the var-ious arguments that fulfill the frame?s semanticroles within the sentence, respectively.
This re-sembles the framework of Das et al (2014), whosolely focus on FrameNet corpora, unlike this pa-per.
The novelty of this paper lies in the frameidentification stage (?3).
Note that this two-stageapproach is unusual for the PropBank corporawhen compared to prior work, where the vast ma-jority of published papers have not focused on theverb frame disambiguation problem at all, only fo-cusing on the role labeling stage (see the overviewpaper of M`arquez et al (2008) for example).3NomBank (Meyers et al, 2004) is a similar resource fornominal predicates, but we do not consider it in our experi-ments.4For example in PropBank, the lexical unit buy.V hasthree verb frames and in sentential context, we want to disam-biguate its frame.
(Although PropBank never formally usesthe term lexical unit, we adopt its usage from the frame se-mantics literature.
)14492.2 Distributed Frame IdentificationWe present a model that takes word embeddingsas input and learns to identify semantic frames.A word embedding is a distributed representa-tion of meaning where each word is representedas a vector in Rn.
Such representations allow amodel to share meaning between similar words,and have been used to capture semantic, syntac-tic and morphological content (Collobert and We-ston, 2008; Turian et al, 2010, inter alia).
We useword embeddings to represent the syntactic con-text of a particular predicate instance as a vector.For example, consider the sentence ?He runs thecompany.?
The predicate runs has two syntac-tic dependents ?
a subject and direct object (butno prepositional phrases or clausal complements).We could represent the syntactic context of runs asa vector with blocks for all the possible dependentswarranted by a syntactic parser; for example, wecould assume that positions 0 .
.
.
n in the vectorcorrespond to the subject dependent, n+1 .
.
.
2ncorrespond to the clausal complement dependent,and so forth.
Thus, the context is a vector in Rnkwith the embedding of He at the subject position,the embedding of company in direct object posi-tion and zeros everywhere else.
Given input vec-tors of this form for our training data, we learn amatrix that maps this high dimensional and sparserepresentation into a lower dimensional space.
Si-multaneously, the model learns an embedding forall the possible labels (i.e.
the frames in a givenlexicon).
At inference time, the predicate-contextis mapped to the low dimensional space, and wechoose the nearest frame label as our classifica-tion.
We next describe this model in detail.3 Frame Identification with EmbeddingsWe continue using the example sentence from?2.2: ?He runs the company.?
where we want todisambiguate the frame of runs in context.
First,we extract the words in the syntactic context ofruns; next, we concatenate their word embeddingsas described in ?2.2 to create an initial vector spacerepresentation.
Subsequently, we learn a map-ping from this initial representation into a low-dimensional space; we also learn an embeddingfor each possible frame label in the same low-dimensional space.
The goal of learning is tomake sure that the correct frame label is as close aspossible to the mapped context, while competingframe labels are farther away.Formally, let x represent the actual sentencewith a marked predicate, along with the associatedsyntactic parse tree; let our initial representationof the predicate context be g(x).
Suppose that theword embeddings we start with are of dimensionn.
Then g is a function from a parsed sentencex to Rnk, where k is the number of possible syn-tactic context types.
For example g selects someimportant positions relative to the predicate, andreserves a block in its output space for the embed-ding of words found at that position.
Suppose gconsiders clausal complements and direct objects.Then g : X ?
R2nand for the example sentenceit has zeros in positions 0 .
.
.
n and the embeddingof the word company in positions n+1 .
.
.
2n.g(x) = [0, .
.
.
, 0, embedding of company].Section 3.1 describes the context positions we usein our experiments.
Let the low dimensional spacewe map to be Rmand the learned mapping be M :Rnk?
Rm.
The mapping M is a linear trans-formation, and we learn it using the WSABIE algo-rithm (Weston et al, 2011).
WSABIE also learns anembedding for each frame label (y, henceforth).In our setting, this means that each frame corre-sponds to a point in Rm.
If we have F possi-ble frames we can store those parameters in anF ?m matrix, one m-dimensional point for eachframe, which we will refer to as the linear map-ping Y .
Let the lexical unit (the lemma conjoinedwith a coarse POS tag) for the marked predicatebe `.
We denote the frames that associate with` in the frame lexicon5and our training corpusas F`.
WSABIE performs gradient-based updateson an objective that tries to minimize the distancebetween M(g(x)) and the embedding of the cor-rect label Y (y), while maintaining a large distancebetween M(g(x)) and the other possible labelsY (y?)
in the confusion set F`.
At disambiguationtime, we use a simple dot product similarity as ourdistance metric, meaning that the model choosesa label by computing the argmaxys(x, y) wheres(x, y) = M(g(x)) ?Y (y), where the argmax iter-ates over the possible frames y ?
F`if ` was seenin the lexicon or the training data, or y ?
F , if itwas unseen.6Model learning is performed usingthe margin ranking loss function as described in5The frame lexicon stores the frames, corresponding se-mantic roles and the lexical units associated with the frame.6This disambiguation scheme is similar to the one adoptedby Das et al (2014), but they use unlemmatized words todefine their confusion set.1450Figure 2: Context representation extraction for theembedding model.
Given a dependency parse (1)the model extracts all words matching a set of pathsfrom the frame evoking predicate and its direct de-pendents (2).
The model computes a composed rep-resentation of the predicate instance by using dis-tributed vector representations for words (3) ?
the(red) vertical embedding vectors for each word areconcatenated into a long vector.
Finally, we learn alinear transformation function parametrized by thecontext blocks (4).Weston et al (2011), and in more detail in section3.2.Since WSABIE learns a single mapping from g(x)to Rm, parameters are shared between differentwords and different frames.
So for example ?Heruns the company?
could help the model disam-biguate ?He owns the company.?
Moreover, sinceg(x) relies on word embeddings rather than wordidentities, information is shared between words.For example ?He runs the company?
could helpus to learn about ?She runs a corporation?.3.1 Context Representation ExtractionIn principle g(x) could be any feature function, butwe performed an initial investigation of two partic-ular variants.
In both variants, our representationis a block vector where each block corresponds toa syntactic position relative to the predicate, andeach block?s values correspond to the embeddingof the word at that position.Direct Dependents The first context function weconsidered corresponds to the examples in ?3.
Toelaborate, the positions of interest are the labels ofthe direct dependents of the predicate, so k is thenumber of labels that the dependency parser canproduce.
For example, if the label on the edge be-tween runs and He is nsubj, we would put the em-bedding of He in the block corresponding to nsubj.If a label occurs multiple times, then the embed-dings of the words below this label are averaged.Unfortunately, using only the direct dependentscan miss a lot of useful information.
For exam-ple, topicalization can place discriminating infor-mation farther from the predicate.
Consider ?Heruns the company.?
vs. ?It was the company thathe runs.?
In the second sentence, the discrim-inating word, company dominates the predicateruns.
Similarly, predicates in embedded clausesmay have a distant agent which cannot be capturedusing direct dependents.
Consider ?The athleteran the marathon.?
vs. ?The athlete prepared him-self for three months to run the marathon.?
In thesecond example, for the predicate run, the agentThe athlete is not a direct dependent, but is con-nected via a longer dependency path.Dependency Paths To capture more relevantcontext, we developed a second context functionas follows.
We scanned the training data for agiven task (either the PropBank or the FrameNetdomains) for the dependency paths that connectedthe gold predicates to the gold semantic argu-ments.
This set of dependency paths were deemedas possible positions in the initial vector space rep-resentation.
In addition, akin to the first contextfunction, we also added all dependency labels tothe context set.
Thus for this context function, theblock cardinality k was the sum of the number ofscanned gold dependency path types and the num-ber of dependency labels.
Given a predicate in itssentential context, we therefore extract only thosecontext words that appear in positions warrantedby the above set.
See Figure 2 for an illustrationof this process.We performed initial experiments using con-text extracted from 1) direct dependents, 2) de-pendency paths, and 3) both.
For all our experi-ments, setting 3) which concatenates the direct de-pendents and dependency path always dominatedthe other two, so we only report results for thissetting.3.2 LearningWe model our objective function following We-ston et al (2011), using a weighted approximate-rank pairwise loss, learned with stochastic gradi-ent descent.
The mapping from g(x) to the lowdimensional space Rmis a linear transformation,so the model parameters to be learnt are the matrixM ?
Rnk?mas well as the embedding of eachpossible frame label, represented as another ma-trix Y ?
RF?mwhere there are F frames in total.The training objective function minimizes:?x?y?L(ranky(x))max(0, ?+s(x, y)?s(x, y?
)).1451where x, y are the training inputs and their cor-responding correct frames, and y?
are negativeframes, ?
is the margin.
Here, ranky(x) is therank of the positive frame y relative to all the neg-ative frames:ranky(x) =?y?I(s(x, y) ?
?
+ s(x, y?
)),and L(?)
converts the rank to a weight.
Choos-ing L(?)
= C?
for any positive constant C opti-mizes the mean rank, whereas a weighting such asL(?)
=?
?i=11/i (adopted here) optimizes thetop of the ranked list, as described in (Usunieret al, 2009).
To train with such an objective,stochastic gradient is employed.
For speed thecomputation of ranky(x) is then replaced with asampled approximation: sample N items y?
untila violation is found, i.e.
max(0, ?
+ s(x, y?)
?s(x, y))) > 0 and then approximate the rank with(F ?
1)/N , see Weston et al (2011) for moredetails on this procedure.
For the choices of thestochastic gradient learning rate, margin (?)
anddimensionality (m), please refer to ?5.4-?5.5.Note that an alternative approach could learnonly the matrixM , and then use a k-nearest neigh-bor classifier in Rm, as in Weinberger and Saul(2009).
The advantage of learning an embeddingfor the frame labels is that at inference time weneed to consider only the set of labels for classi-fication rather than all training examples.
Addi-tionally, since we use a frame lexicon that givesus the possible frames for a given predicate, weusually only consider a handful of candidate la-bels.
If we used all training examples for a givenpredicate for finding a nearest-neighbor match atinference time, we would have to consider manymore candidates, making the process very slow.4 Argument IdentificationHere, we briefly describe the argument identifi-cation model used in our frame-semantic parsingexperiments, post frame identification.
Given x,the sentence with a marked predicate, the argu-ment identification model assumes that the pred-icate frame y has been disambiguated.
From aframe lexicon, we look up the set of semantic rolesRythat associate with y.
This set alo contains thenull role r?.
From x, a rule-based candidate argu-ment extraction algorithm extracts a set of spansA that could potentially serve as the overt7argu-7By overtness, we mean the non-null instantiation of asemantic role in a frame-semantic parse.?
starting word of a ?
POS of the starting word of a?
ending word of a ?
POS of the ending word of a?
head word of a ?
POS of the head word of a?
bag of words in a ?
bag of POS tags in a?
a bias feature ?
voice of the predicate use?
word cluster of a?s head?
word cluster of a?s head conjoined with word clusterof the predicate??
dependency path between a?s head and the predicate?
the set of dependency labels of the predicate?s children?
dependency path conjoined with the POS tag of a?shead?
dependency path conjoined with the word cluster ofa?s head?
position of a with respect to the predicate (before, after,overlap or identical)?
whether the subject of the predicate is missing (miss-ingsubj)?
missingsubj, conjoined with the dependency path?
missingsubj, conjoined with the dependency path fromthe verb dominating the predicate to a?s headTable 1: Argument identification features.
The span in con-sideration is termed a.
Every feature in this list has two ver-sions, one conjoined with the given role r and the other con-joined with both r and the frame y.
The feature with a?su-perscript is only conjoined with the role to reduce its sparsity.mentsAyfor y (see ?5.4-?5.5 for the details of thecandidate argument extraction algorithms).Learning Given training data of the form?
?x(i), y(i),M(i)?
?Ni=1, where,M = {(r, a} : r ?
Ry, a ?
A ?Ay}, (1)a set of tuples that associates each role r in Rywith a span a according to the gold data.
Note thatthis mapping associates spans with the null role r?as well.
We optimize the following log-likelihoodto train our model:max?N?i=1|M(i)|?j=1log p?
((r, a)j|x, y,Ry)?
C??
?22where p?
is a log-linear model normalized over theset Ry, with features described in Table 1.
Weset C = 1.0 and use L-BFGS (Liu and Nocedal,1989) for training.Inference Although our learning mechanismuses a local log-linear model, we perform infer-ence globally on a per-frame basis by applyinghard structural constraints.
Following Das et al(2014) and Punyakanok et al (2008) we use thelog-probability of the local classifiers as a score inan integer linear program (ILP) to assign roles sub-ject to hard constraints described in ?5.4 and ?5.5.We use an off-the-shelf ILP solver for inference.14525 ExperimentsIn this section, we present our experiments andthe results achieved.
We evaluate our novel frameidentification approach in isolation and also con-joined with argument identification resulting infull frame-semantic structures; before presentingour model?s performance we first focus on thedatasets, baselines and the experimental setup.5.1 DataWe evaluate our models on both FrameNet- andPropBank-style structures.
For FrameNet, we usethe full-text annotations in the FrameNet 1.5 re-lease8which was used by Das et al (2014, ?3.2).We used the same test set as Das et al contain-ing 23 documents with 4,458 predicates.
Of theremaining 55 documents, 16 documents were ran-domly chosen for development.9For experiments with PropBank, we used theOntonotes corpus (Hovy et al, 2006), version 4.0,and only made use of the Wall Street Journal doc-uments; we used sections 2-21 for training, sec-tion 24 for development and section 23 for testing.This resembles the setup used by Punyakanok etal.
(2008).
All the verb frame files in Ontonoteswere used for creating our frame lexicon.5.2 Frame Identification BaselinesFor comparison, we implemented a set of baselinemodels, with varying feature configurations.
Thebaselines use a log-linear model that models thefollowing probability at training time:p(y|x, `) =e??f(y,x,`)?y??F`e?
?f(y?,x,`)(2)At test time, this model chooses the best frame asargmaxy?
?
f(y, x, `) where argmax iterates overthe possible frames y ?
F`if ` was seen in thelexicon or the training data, or y ?
F , if it was un-seen, like the disambiguation scheme of ?3.
Wetrain this model by maximizing L2regularizedlog-likelihood, using L-BFGS; the regularizationconstant was set to 0.1 in all experiments.For comparison with our model from ?3, whichwe call WSABIE EMBEDDING, we implemented twobaselines with the log-linear model.
Both thebaselines use features very similar to the input rep-resentations described in ?3.1.
The first one com-putes the direct dependents and dependency paths8See https://framenet.icsi.berkeley.edu.9These documents are listed in appendix A.as described in ?3.1 but conjoins them with theword identity rather than a word embedding.
Ad-ditionally, this model uses the un-conjoined wordsas backoff features.
This would be a standard NLPapproach for the frame identification problem, butis surprisingly competitive with the state of the art.We call this baseline LOG-LINEAR WORDS.
The sec-ond baseline, tries to decouple the WSABIE trainingfrom the embedding input, and trains a log linearmodel using the embeddings.
So the second base-line has the same input representation as WSABIEEMBEDDING but uses a log-linear model instead ofWSABIE.
We call this model LOG-LINEAR EMBED-DING.5.3 Common Experimental SetupWe process our PropBank and FrameNet training,development and test corpora with a shift-reducedependency parser that uses the Stanford conven-tions (de Marneffe and Manning, 2013) and usesan arc-eager transition system with beam size of 8;the parser and its features are described by Zhangand Nivre (2011).
Before parsing the data, it istagged with a POS tagger trained with a condi-tional random field (Lafferty et al, 2001) with thefollowing emission features: word, the word clus-ter, word suffixes of length 1, 2 and 3, capitaliza-tion, whether it has a hyphen, digit and punctua-tion.
Beyond the bias transition feature, we havetwo cluster features for the left and right words inthe transition.
We use Brown clusters learned us-ing the algorithm of Uszkoreit and Brants (2008)on a large English newswire corpus for cluster fea-tures.
We use the same word clusters for the argu-ment identification features in Table 1.We learn the initial embedding representationsfor our frame identification model (?3) using adeep neural language model similar to the one pro-posed by Bengio et al (2003).
We use 3 hiddenlayers each with 1024 neurons and learn a 128-dimensional embedding from a large corpus con-taining over 100 billion tokens.
In order to speedup learning, we use an unnormalized output layerand a hinge-loss objective.
The objective tries toensure that the correct word scores higher than arandom incorrect word, and we train with mini-batch stochastic gradient descent.5.4 Experimental Setup for FrameNetHyperparameters For our frame identificationmodel with embeddings, we search for the WSA-BIE hyperparameters using the development data.1453SEMAFOR LEXICON FULL LEXICONDevelopment DataModel All Ambiguous Rare All Ambiguous RareLOG-LINEAR WORDS 96.21 90.41 95.75 96.37 90.41 96.07LOG-LINEAR EMBEDDING 96.06 90.56 95.38 96.19 90.49 95.70WSABIE EMBEDDING (?3) 96.90 92.73 96.44 96.99 93.12 96.39SEMAFOR LEXICON FULL LEXICONModel All Ambiguous Rare Unseen All Ambiguous RareTest DataDas et al (2014) supervised 82.97 69.27 80.97 23.08Das et al (2014) best 83.60 69.19 82.31 42.67LOG-LINEAR WORDS 84.71 70.97 81.70 27.27 87.44 70.97 87.10LOG-LINEAR EMBEDDING 83.42 68.70 80.95 27.97 86.20 68.70 86.03WSABIE EMBEDDING (?3) 86.58 73.67 85.04 44.76 88.73 73.67 89.38Table 2: Frame identification results for FrameNet.
See ?5.6.SEMAFOR LEXICON FULL LEXICONModel Precision Recall F1Precision Recall F1Development DataLOG-LINEAR WORDS 89.43 75.98 82.16 89.41 76.05 82.19WSABIE EMBEDDING (?3) 89.89 76.40 82.59 89.94 76.27 82.54Test DataDas et al supervised 67.81 60.68 64.05Das et al best 68.33 61.14 64.54LOG-LINEAR WORDS 71.16 63.56 67.15 73.35 65.27 69.08WSABIE EMBEDDING (?3) 72.79 64.95 68.64 74.44 66.17 70.06Table 3: Full structure prediction results for FrameNet; this reports frame and argument identification performance jointly.
Weskip LOG-LINEAR EMBEDDING because it underperforms all other models by a large margin.We search for the stochastic gradient learningrate in {0.0001, 0.001, 0.01}, the margin ?
?
{0.001, 0.01, 0.1, 1} and the dimensionality of thefinal vector space m ?
{256, 512}, to maximizethe frame identification accuracy of ambiguouslexical units; by ambiguous, we imply lexical unitsthat appear in the training data or the lexicon withmore than one semantic frame.
The underlinedvalues are the chosen hyperparameters used to an-alyze the test data.Argument Candidates The candidate argumentextraction method used for the FrameNet data, (asmentioned in ?4) was adapted from the algorithmof Xue and Palmer (2004) applied to dependencytrees.
Since the original algorithm was designedfor verbs, we added a few extra rules to handlenon-verbal predicates: we added 1) the predicateitself as a candidate argument, 2) the span rangingfrom the sentence position to the right of the pred-icate to the rightmost index of the subtree headedby the predicate?s head; this helped capture caseslike ?a few months?
(where few is the predicate andmonths is the argument), and 3) the span rangingfrom the leftmost index of the subtree headed bythe predicate?s head to the position immediatelybefore the predicate, for cases like ?your gift toGoodwill?
(where to is the predicate and your giftis the argument).1010Note that Das et al (2014) describe the state of the artin FrameNet-based analysis, but their argument identifica-tion strategy considered all possible dependency subtrees inFrame Lexicon In our experimental setup, wescanned the XML files in the ?frames?
directoryof the FrameNet 1.5 release, which lists all theframes, the corresponding roles and the associ-ated lexical units, and created a frame lexicon tobe used in our frame and argument identificationmodels.
We noted that this renders every lexicalunit as seen; in other words, at frame disambigua-tion time on our test set, for all instances, we onlyhad to score the frames in F`for a predicate withlexical unit ` (see ?3 and ?5.2).
We call this setupFULL LEXICON.
While comparing with prior stateof the art on the same corpus, we noted that Das etal.
(2014) found several unseen predicates at testtime.11For fair comparison, we took the lexicalunits for the predicates that Das et al consideredas seen, and constructed a lexicon with only those;training instances, if any, for the unseen predicatesunder Das et al?s setup were thrown out as well.We call this setup SEMAFOR LEXICON.12We alsoexperimented on the set of unseen instances usedby Das et alILP constraints For FrameNet, we used threeILP constraints during argument identification(?4).
1) each span could have only one role, 2)each core role could be present only once, and 3)all overt arguments had to be non-overlapping.a parse, resulting in a much larger search space.11Instead of using the frame files, Das et al built a framelexicon from FrameNet?s exemplars and the training corpus.12We got Das et al?s seen predicates from the authors.1454Model All Ambiguous RareLOG-LINEAR WORDS 94.21 90.54 93.33LOG-LINEAR EMBEDDING 93.81 89.86 93.73WSABIE EMBEDDING (?3) 94.79 91.52 92.55Dev data ?
?
Test dataModel All Ambiguous RareLOG-LINEAR WORDS 94.74 92.07 91.32LOG-LINEAR EMBEDDING 94.04 90.95 90.97WSABIE EMBEDDING (?3) 94.56 91.82 90.62Table 4: Frame identification accuracy results for PropBank.The model and the column names have the same semanticsas Table 2.Model P R F1LOG-LINEAR WORDS 80.02 75.58 77.74WSABIE EMBEDDING (?3) 80.06 75.74 77.84Dev data ?
?
Test dataModel P R F1LOG-LINEAR WORDS 81.55 77.83 79.65WSABIE EMBEDDING (?3) 81.32 77.97 79.61Table 5: Full frame-structure prediction results for Propbank.This is a metric that takes into account frames and argumentstogether.
See ?5.7 for more details.5.5 Experimental Setup for PropBankHyperparameters As in ?5.4, we made a hyper-parameter sweep in the same space.
The chosenlearning rate was 0.01, while the other values were?
= 0.01 and m = 512.
Ambiguous lexical unitswere used for this selection process.Argument Candidates For PropBank we usethe algorithm of Xue and Palmer (2004) appliedto dependency trees.Frame Lexicon For the PropBank experimentswe scanned the frame files for propositions inOntonotes 4.0, and stored possible core roles foreach verb frame.
The lexical units were simplythe verb associating with the verb frames.
Therewere no unseen verbs at test time.ILP constraints We used the constraints of Pun-yakanok et al (2008).5.6 FrameNet ResultsTable 2 presents accuracy results on frame iden-tification.13We present results on all predicates,ambiguous predicates seen in the lexicon or thetraining data, and rare ambiguous predicates thatappear ?
11 times in the training data.
The WS-ABIE EMBEDDING model from ?3 performs signif-icantly better than the LOG-LINEAR WORDS base-line, while LOG-LINEAR EMBEDDING underperformsin every metric.
For the SEMAFOR LEXICON setup,we also compare with the state of the art from Das13We do not report partial frame accuracy that has beenreported by prior work.Model P R F1LOG-LINEAR WORDS 77.29 71.50 74.28WSABIE EMBEDDING (?3) 77.13 71.32 74.11Dev data ?
?
Test dataModel P R F1LOG-LINEAR WORDS 79.47 75.11 77.23WSABIE EMBEDDING (?3) 79.36 75.04 77.14Punyakanok et al Collins 75.92 71.45 73.62Punyakanok et al Charniak 77.09 75.51 76.29Punyakanok et al Combined 80.53 76.94 78.69Table 6: Argument only evaluation (semantic role labelingmetrics) using the CoNLL 2005 shared task evaluation script(Carreras and M`arquez, 2005).
Results from Punyakanok etal.
(2008) are taken from Table 11 of that paper.et al (2014), who used a semi-supervised learn-ing method to improve upon a supervised latent-variable log-linear model.
For unseen predicatesfrom the Das et al system, we perform better aswell.
Finally, for the FULL LEXICON setting, the ab-solute accuracy numbers are even better for ourbest model.
Table 3 presents results on the fullframe-semantic parsing task (measured by a reim-plementation of the SemEval 2007 shared taskevaluation script) when our argument identifica-tion model (?4) is used after frame identification.We notice similar trends as in Table 2, and our re-sults outperform the previously published best re-sults, setting a new state of the art.5.7 PropBank ResultsTable 4 shows frame identification results on thePropBank data.
On the development set, our bestmodel performs with the highest accuracy on alland ambiguous predicates, but performs worse onrare ambiguous predicates.
On the test set, theLOG-LINEAR WORDS baseline performs best by avery narrow margin.
See ?6 for a discussion.Table 5 presents results where we measure pre-cision, recall and F1for frames and arguments to-gether; this strict metric penalizes arguments formismatched frames, like in Table 3.
We see thesame trend as in Table 4.
Finally, Table 6 presentsSRL results that measures argument performanceonly, irrespective of the frame; we use the eval-uation script from CoNLL 2005 (Carreras andM`arquez, 2005).
We note that with a better frameidentification model, our performance on SRL im-proves in general.
Here, too, the embedding modelbarely misses the performance of the best baseline,but we are at par and sometimes better than the sin-gle parser setting of a state-of-the-art SRL system(Punyakanok et al, 2008).1414The last row of Table 6 refers to a system which used the14556 DiscussionFor FrameNet, the WSABIE EMBEDDING model wepropose strongly outperforms the baselines on allmetrics, and sets a new state of the art.
We be-lieve that the WSABIE EMBEDDING model performsbetter than the LOG-LINEAR EMBEDDING baseline(that uses the same input representation) becausethe former setting allows examples with differ-ent labels and confusion sets to share informa-tion; this is due to the fact that all labels live inthe same label space, and a single projection ma-trix is shared across the examples to map the inputfeatures to this space.
Consequently, the WSABIEEMBEDDING model can share more information be-tween different examples in the training data thanthe LOG-LINEAR EMBEDDING model.
Since the LOG-LINEAR WORDS model always performs better thanthe LOG-LINEAR EMBEDDING model, we concludethat the primary benefit does not come from theinput embedding representation.15On the PropBank data, we see that the LOG-LINEAR WORDS baseline has roughly the same per-formance as our model on most metrics: slightlybetter on the test data and slightly worse on thedevelopment data.
This can be partially explainedwith the significantly larger training set size forPropBank, making features based on words moreuseful.
Another important distinction betweenPropBank and FrameNet is that the latter sharesframes between multiple lexical units.
The ef-fect of this is clearly observable from the ?Rare?column in Table 4.
WSABIE EMBEDDING performspoorly in this setting while LOG-LINEAR EMBEDDINGperforms well.
Part of the explanation has to dowith the specifics of WSABIE training.
Recall thatthe WSABIE EMBEDDING model needs to estimatethe label location in Rmfor each frame.
In otherwords, it must estimate 512 parameters based onat most 10 training examples.
However, since theinput representation is shared across all frames,every other training example from all the lexicalunits affects the optimal estimate, since they allmodify the joint parameter matrixM .
By contrast,in the log-linear models each label has its ownset of parameters, and they interact only via thenormalization constant.
The LOG-LINEAR WORDSmodel does not have this entanglement, but cannotshare information between words.
For PropBank,combination of two syntactic parsers as input.15One could imagine training a WSABIE model with wordfeatures, but we did not perform this experiment.these drawbacks and benefits balance out and wesee similar performance for LOG-LINEAR WORDSand LOG-LINEAR EMBEDDING.
For FrameNet, esti-mating the label embedding is not as much of aproblem because even if a lexical unit is rare, thepotential frames can be frequent.
For example, wemight have seen the SENDING frame many times,even though telex.V is a rare lexical unit.In comparison to prior work on FrameNet, evenour baseline models outperform the previous stateof the art.
A particularly interesting comparison isbetween our LOG-LINEAR WORDS baseline and thesupervised model of Das et al (2014).
They alsouse a log-linear model, but they incorporate a la-tent variable that uses WordNet (Fellbaum, 1998)to get lexical-semantic relationships and smoothsover frames for ambiguous lexical units.
It ispossible that this reduces the model?s power andcauses it to over-generalize.
Another difference isthat when training the log-linear model, they nor-malize over all frames, while we normalize overthe allowed frames for the current lexical unit.This would tend to encourage their model to ex-pend more of its modeling power to rule out pos-sibilities that will be pruned out at test time.7 ConclusionWe have presented a simple model that outper-forms the prior state of the art on FrameNet-style frame-semantic parsing, and performs at parwith one of the previous-best single-parser sys-tems on PropBank SRL.
Unlike Das et al (2014),our model does not rely on heuristics to con-struct a similarity graph and leverage WordNet;hence, in principle it is generalizable to varyingdomains, and to other languages.
Finally, we pre-sented results on PropBank-style semantic role la-beling with a system that included the task of au-tomatic verb frame identification, in tune with theFrameNet literature; we believe that such a sys-tem produces more interpretable output, both fromthe perspective of human understanding as well asdownstream applications, than pipelines that areoblivious to the verb frame, only focusing on ar-gument analysis.AcknowledgmentsWe thank Emily Pitler for comments on an earlydraft, and the anonymous reviewers for their valu-able feedback.1456ReferencesC.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.The berkeley framenet project.
In Proceedings ofCOLING-ACL.C.
Baker, M. Ellsworth, and K. Erk.
2007.
SemEval-2007 Task 19: Frame semantic structure extraction.In Proceedings of SemEval.Y.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.2003.
A neural probabilistic language model.
Jour-nal of Machine Learning Research, 3:1137?1155.X.
Carreras and L. M`arquez.
2004.
Introduction to theCoNLL-2004 shared task: Semantic role labeling.In Proceedings of CoNLL.X.
Carreras and L. M`arquez.
2005.
Introduction to theCoNLL-2005 shared task: semantic role labeling.
InProceedings of CoNLL.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In Proceedings ofICML.D.
Das, N. Schneider, D. Chen, and N. A. Smith.
2010.Probabilistic frame-semantic parsing.
In Proceed-ings of NAACL-HLT.D.
Das, D. Chen, A. F. T. Martins, N. Schneider, andN.
A. Smith.
2014.
Frame-semantic parsing.
Com-putational Linguistics, 40(1):9?56.M.-C. de Marneffe and C. D. Manning, 2013.
Stanfordtyped dependencies manual.C.
Fellbaum, editor.
1998.
WordNet: an electroniclexical database.C.
J. Fillmore, C. R. Johnson, and M. R. Petruck.
2003.Background to FrameNet.
International Journal ofLexicography, 16(3):235?250.C.
J. Fillmore.
1982.
Frame Semantics.
In Linguis-tics in the Morning Calm, pages 111?137.
HanshinPublishing Co., Seoul, South Korea.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistics,28(3):245?288.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: The 90 In Pro-ceedings of NAACL-HLT.R.
Johansson and P. Nugues.
2007.
LTH: semanticstructure extraction using nonprojective dependencytrees.
In Proceedings of SemEval.A.
Klementiev, I. Titov, and B. Bhattarai.
2012.
In-ducing crosslingual distributed representations ofwords.
In Proceedings of COLING.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML.D.
C. Liu and J. Nocedal.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming, 45(3):503 ?
528.L.
M`arquez, X. Carreras, K. C. Litkowski, andS.
Stevenson.
2008.
Semantic role labeling: an in-troduction to the special issue.
Computational Lin-guistics, 34(2):145?159.A.
Meyers, R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman.
2004.The NomBank project: An interim report.
In Pro-ceedings of NAACL/HLT Workshop on Frontiers inCorpus Annotation.J.
Mitchell and M. Lapata.
2008.
Vector-based modelsof semantic composition.
In Proceedings of ACL-HLT.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
TheProposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.V.
Punyakanok, D. Roth, and W. Yih.
2008.
The im-portance of syntactic parsing and inference in se-mantic role labeling.
Computational Linguistics,34(2):257?287.R.
Socher, J. Pennington, E. H. Huang, A. Y. Ng, andC.
D. Manning.
2011.
Semi-supervised recursiveautoencoders for predicting sentiment distributions.In Proceedings of EMNLP.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Wordrepresentations: A simple and general method forsemi-supervised learning.
In Proceedings of ACL,Stroudsburg, PA, USA.N.
Usunier, D. Buffoni, and P. Gallinari.
2009.
Rank-ing with ordered weighted pairwise classification.
InICML.J.
Uszkoreit and T. Brants.
2008.
Distributed wordclustering for large scale class-based language mod-eling in machine translation.
In Proceedings ofACL-HLT.K.
Q. Weinberger and L. K. Saul.
2009.
Distance met-ric learning for large margin nearest neighbor clas-sification.
Journal of Machine Learning Research,10:207?244.J.
Weston, S. Bengio, and N. Usunier.
2011.
Wsabie:Scaling up to large vocabulary image annotation.
InProceedings of IJCAI.N.
Xue and M. Palmer.
2004.
Calibrating features forsemantic role labeling.
In Proceedings of EMNLP2004.Y.
Zhang and J. Nivre.
2011.
Transition-based depen-dency parsing with rich non-local features.
In Pro-ceedings of ACL-HLT.1457Number Filenamedev-1 LUCorpus-v0.3 20000420 xin eng-NEW.xmldev-2 NTI SouthAfrica Introduction.xmldev-3 LUCorpus-v0.3 CNN AARONBROWN ENG 20051101 215800.partial-NEW.xmldev-4 LUCorpus-v0.3 AFGP-2002-600045-Trans.xmldev-5 PropBank TicketSplitting.xmldev-6 Miscellaneous Hijack.xmldev-7 LUCorpus-v0.3 artb 004 A1 E1 NEW.xmldev-8 NTI WMDNews 042106.xmldev-9 C-4 C-4Text.xmldev-10 ANC EntrepreneurAsMadonna.xmldev-11 NTI LibyaCountry1.xmldev-12 NTI NorthKorea NuclearOverview.xmldev-13 LUCorpus-v0.3 20000424 nyt-NEW.xmldev-14 NTI WMDNews 062606.xmldev-15 ANC 110CYL070.xmldev-16 LUCorpus-v0.3 CNN ENG 20030614 173123.4-NEW-1.xmlTable 7: List of files used as development set for the FrameNet 1.5 corpus.A Development DataTable 7 features a list of the 16 randomly selecteddocuments from the FrameNet 1.5 corpus, whichwe used for development.
The resultant develop-ment set consists of roughly 4,500 predicates.
Weuse the same test set as in Das et al (2014), con-taining 23 documents and 4,458 predicates.1458
