Proceedings of ACL-08: HLT, pages 638?646,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsLearning Effective Multimodal Dialogue Strategies from Wizard-of-Ozdata: Bootstrapping and EvaluationVerena RieserSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, GBvrieser@inf.ed.ac.ukOliver LemonSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, GBolemon@inf.ed.ac.ukAbstractWe address two problems in the field of au-tomatic optimization of dialogue strategies:learning effective dialogue strategies when noinitial data or system exists, and evaluating theresult with real users.
We use ReinforcementLearning (RL) to learn multimodal dialoguestrategies by interaction with a simulated envi-ronment which is ?bootstrapped?
from smallamounts of Wizard-of-Oz (WOZ) data.
Thisuse of WOZ data allows development of op-timal strategies for domains where no work-ing prototype is available.
We compare theRL-based strategy against a supervised strat-egy which mimics the wizards?
policies.
Thiscomparison allows us to measure relative im-provement over the training data.
Our resultsshow that RL significantly outperforms Super-vised Learning when interacting in simulationas well as for interactions with real users.
TheRL-based policy gains on average 50-timesmore reward when tested in simulation, andalmost 18-times more reward when interactingwith real users.
Users also subjectively ratethe RL-based policy on average 10% higher.1 IntroductionDesigning a spoken dialogue system is a time-consuming and challenging task.
A developer mayspend a lot of time and effort anticipating the po-tential needs of a specific application environmentand then deciding on the most appropriate systemaction (e.g.
confirm, present items,.
.
.
).
One of thekey advantages of statistical optimisation methods,such as Reinforcement Learning (RL), for dialoguestrategy design is that the problem can be formu-lated as a principled mathematical model which canbe automatically trained on real data (Lemon andPietquin, 2007; Frampton and Lemon, to appear).
Incases where a system is designed from scratch, how-ever, there is often no suitable in-domain data.
Col-lecting dialogue data without a working prototypeis problematic, leaving the developer with a classicchicken-and-egg problem.We propose to learn dialogue strategies bysimulation-based RL (Sutton and Barto, 1998),where the simulated environment is learned fromsmall amounts of Wizard-of-Oz (WOZ) data.
Us-ing WOZ data rather than data from real Human-Computer Interaction (HCI) allows us to learn op-timal strategies for domains where no working di-alogue system already exists.
To date, automaticstrategy learning has been applied to dialogue sys-tems which have already been deployed using hand-crafted strategies.
In such work, strategy learningwas performed based on already present extensiveonline operation experience, e.g.
(Singh et al, 2002;Henderson et al, 2005).
In contrast to this preced-ing work, our approach enables strategy learning indomains where no prior system is available.
Opti-mised learned strategies are then available from thefirst moment of online-operation, and tedious hand-crafting of dialogue strategies is omitted.
This inde-pendence from large amounts of in-domain dialoguedata allows researchers to apply RL to new appli-cation areas beyond the scope of existing dialoguesystems.
We call this method ?bootstrapping?.In a WOZ experiment, a hidden human operator,the so called ?wizard?, simulates (partly or com-638pletely) the behaviour of the application, while sub-jects are left in the belief that they are interactingwith a real system (Fraser and Gilbert, 1991).
Thatis, WOZ experiments only simulate HCI.
We there-fore need to show that a strategy bootstrapped fromWOZ data indeed transfers to real HCI.
Further-more, we also need to introduce methods to learnuseful user simulations (for training RL) from suchlimited data.The use of WOZ data has earlier been proposedin the context of RL.
(Williams and Young, 2004)utilise WOZ data to discover the state and actionspace for MDP design.
(Prommer et al, 2006)use WOZ data to build a simulated user and noisemodel for simulation-based RL.
While both stud-ies show promising first results, their simulated en-vironment still contains many hand-crafted aspects,which makes it hard to evaluate whether the suc-cess of the learned strategy indeed originates fromthe WOZ data.
(Schatzmann et al, 2007) propose to?bootstrap?
with a simulated user which is entirelyhand-crafted.
In the following we propose an en-tirely data-driven approach, where all componentsof the simulated learning environment are learnedfrom WOZ data.
We also show that the resultingpolicy performs well for real users.2 Wizard-of-Oz data collectionOur domains of interest are information-seeking di-alogues, for example a multimodal in-car interfaceto a large database of music (MP3) files.
The corpuswe use for learning was collected in a multimodalstudy of German task-oriented dialogues for an in-car music player application by (Rieser et al, 2005).This study provides insights into natural methodsof information presentation as performed by humanwizards.
6 people played the role of an intelligentinterface (the ?wizards?).
The wizards were ableto speak freely and display search results on thescreen by clicking on pre-computed templates.
Wiz-ards?
outputs were not restricted, in order to explorethe different ways they intuitively chose to presentsearch results.
Wizard?s utterances were immedi-ately transcribed and played back to the user withText-To-Speech.
21 subjects (11 female, 10 male)were given a set of predefined tasks to perform, aswell as a primary driving task, using a driving simu-lator.
The users were able to speak, as well as makeselections on the screen.
We also introduced artifi-cial noise in the setup, in order to closer resemblethe conditions of real HCI.
Please see (Rieser et al,2005) for further detail.The corpus gathered with this setup comprises 21sessions and over 1600 turns.
Example 1 shows atypical multimodal presentation sub-dialogue fromthe corpus (translated from German).
Note that thewizard displays quite a long list of possible candi-dates on an (average sized) computer screen, whilethe user is driving.
This example illustrates that evenfor humans it is difficult to find an ?optimal?
solu-tion to the problem we are trying to solve.
(1) User: Please search for music by Madonna .Wizard: I found seventeen hundred and elevenitems.
The items are displayed on the screen.
[displays list]User: Please select ?Secret?.For each session information was logged, e.g.
thetranscriptions of the spoken utterances, the wizard?sdatabase query and the number of results, the screenoption chosen by the wizard, and a rich set of con-textual dialogue features was also annotated, see(Rieser et al, 2005).Of the 793 wizard turns 22.3% were annotatedas presentation strategies, resulting in 177 instancesfor learning, where the six wizards contributed aboutequal proportions.Information about user preferences was obtained,using a questionnaire containing similar questions tothe PARADISE study (Walker et al, 2000).
In gen-eral, users report that they get distracted from driv-ing if too much information is presented.
On theother hand, users prefer shorter dialogues (most ofthe user ratings are negatively correlated with dia-logue length).
These results indicate that we needto find a strategy given the competing trade-offs be-tween the number of results (large lists are difficultfor users to process), the length of the dialogue (longdialogues are tiring, but collecting more informationcan result in more precise results), and the noise inthe speech recognition environment (in high noiseconditions accurate information is difficult to ob-tain).
In the following we utilise the ratings from theuser questionnaires to optimise a presentation strat-egy using simulation-based RL.639????????????????
?acquisition action:????askASlotimplConfAskASlotexplConfpresentInfo????state:?????
?filledSlot 1 |2 |3 |4 |:{0,1}confirmedSlot 1 |2 |3 |4 |:{0,1}DB:{1--438}?????
?presentation action:[presentInfoVerbalpresentInfoMM]state:?????
?DB low:{0,1}DB med:{0,1}DB high{0,1}??????????????????????
?Figure 1: State-Action space for hierarchical Reinforcement Learning3 Simulated Learning EnvironmentSimulation-based RL (also know as ?model-free?RL) learns by interaction with a simulated environ-ment.
We obtain the simulated components from theWOZ corpus using data-driven methods.
The em-ployed database contains 438 items and is similar inretrieval ambiguity and structure to the one used inthe WOZ experiment.
The dialogue system used forlearning comprises some obvious constraints reflect-ing the system logic (e.g.
that only filled slots can beconfirmed), implemented as Information State Up-date (ISU) rules.
All other actions are left for opti-misation.3.1 MDP and problem representationThe structure of an information seeking dialoguesystem consists of an information acquisition phase,and an information presentation phase.
For informa-tion acquisition the task of the dialogue manager isto gather ?enough?
search constraints from the user,and then, ?at the right time?, to start the informationpresentation phase, where the presentation task is topresent ?the right amount?
of information in the rightway?
either on the screen or listing the items ver-bally.
What ?the right amount?
actually means de-pends on the application, the dialogue context, andthe preferences of users.
For optimising dialoguestrategies information acquisition and presentationare two closely interrelated problems and need tobe optimised simultaneously: when to present in-formation depends on the available options for howto present them, and vice versa.
We therefore for-mulate the problem as a Markov Decision Process(MDP), relating states to actions in a hierarchicalmanner (see Figure 1): 4 actions are available forthe information acquisition phase; once the actionpresentInfo is chosen, the information presen-tation phase is entered, where 2 different actionsfor output realisation are available.
The state-spacecomprises 8 binary features representing the task fora 4 slot problem: filledSlot indicates whether aslots is filled, confirmedSlot indicates whethera slot is confirmed.
We also add features that hu-man wizards pay attention to, using the feature se-lection techniques of (Rieser and Lemon, 2006b).Our results indicate that wizards only pay attentionto the number of retrieved items (DB).
We there-fore add the feature DB to the state space, whichtakes integer values between 1 and 438, resulting in28 ?
438 = 112, 128 distinct dialogue states.
In to-tal there are 4112,128 theoretically possible policiesfor information acquisition.
1 For the presentationphase the DB feature is discretised, as we will furtherdiscuss in Section 3.6.
For the information presenta-tion phase there are 223= 256 theoretically possiblepolicies.3.2 Supervised BaselineWe create a baseline by applying Supervised Learn-ing (SL).
This baseline mimics the average wizardbehaviour and allows us to measure the relative im-provements over the training data (cf.
(Henderson etal., 2005)).
For these experiments we use the WEKAtoolkit (Witten and Frank, 2005).
We learn with thedecision tree J4.8 classifier, WEKA?s implementationof the C4.5 system (Quinlan, 1993), and rule induc-1In practise, the policy space is smaller, as some of combi-nations are not possible, e.g.
a slot cannot be confirmed beforebeing filled.
Furthermore, some incoherent action choices areexcluded by the basic system logic.640baseline JRip J48timing 52.0(?
2.2) 50.2(?
9.7) 53.5(?11.7)modality 51.0(?
7.0) 93.5(?11.5)* 94.6(?
10.0)*Table 1: Predicted accuracy for presentation timing andmodality (with standard deviation ?
), * denotes statisti-cally significant improvement at p < .05tion JRIP, the WEKA implementation of RIPPER (Co-hen, 1995).
In particular, we learn models whichpredict the following wizard actions:?
Presentation timing: when the ?average?
wizardstarts the presentation phase?
Presentation modality: in which modality thelist is presented.As input features we use annotated dialogue con-text features, see (Rieser and Lemon, 2006b).
Bothmodels are trained using 10-fold cross validation.Table 1 presents the results for comparing the ac-curacy of the learned classifiers against the major-ity baseline.
For presentation timing, none of theclassifiers produces significantly improved results.Hence, we conclude that there is no distinctive pat-tern the wizards follow for when to present informa-tion.
For strategy implementation we therefore use afrequency-based approach following the distributionin the WOZ data: in 0.48 of cases the baseline policydecides to present the retrieved items; for the rest ofthe time the system follows a hand-coded strategy.For learning presentation modality, both classifierssignificantly outperform the baseline.
The learnedmodels can be rewritten as in Algorithm 1.
Note thatthis rather simple algorithm is meant to represent theaverage strategy as present in the initial data (whichthen allows us to measure the relative improvementsof the RL-based strategy).Algorithm 1 SupervisedStrategy1: if DB ?
3 then2: return presentInfoVerbal3: else4: return presentInfoMM5: end if3.3 Noise simulationOne of the fundamental characteristics of HCI is anerror prone communication channel.
Therefore, thesimulation of channel noise is an important aspect ofthe learning environment.
Previous work uses data-intensive simulations of ASR errors, e.g.
(Pietquinand Dutoit, 2006).
We use a simple model simulat-ing the effects of non- and misunderstanding on theinteraction, rather than the noise itself.
This methodis especially suited to learning from small data sets.From our data we estimate a 30% chance of userutterances to be misunderstood, and 4% to be com-plete non-understandings.
We simulate the effectsnoise has on the user behaviour, as well as for thetask accuracy.
For the user side, the noise model de-fines the likelihood of the user accepting or rejectingthe system?s hypothesis (for example when the sys-tem utters a confirmation), i.e.
in 30% of the casesthe user rejects, in 70% the user agrees.
These prob-abilities are combined with the probabilities for useractions from the user simulation, as described in thenext section.
For non-understandings we have theuser simulation generating Out-of-Vocabulary utter-ances with a chance of 4%.
Furthermore, the noisemodel determines the likelihood of task accuracy ascalculated in the reward function for learning.
Afilled slot which is not confirmed by the user has a30% chance of having been mis-recognised.3.4 User simulationA user simulation is a predictive model of real userbehaviour used for automatic dialogue strategy de-velopment and testing.
For our domain, the usercan either add information (add), repeat or para-phrase information which was already provided atan earlier stage (repeat), give a simple yes-no an-swer (y/n), or change to a different topic by pro-viding a different slot value than the one asked for(change).
These actions are annotated manually(?
= .7).
We build two different types of usersimulations, one is used for strategy training, andone for testing.
Both are simple bi-gram modelswhich predict the next user action based on the pre-vious system action (P (auser|asystem)).
We facethe problem of learning such models when train-ing data is sparse.
For training, we therefore usea cluster-based user simulation method, see (Rieser641and Lemon, 2006a).
For testing, we apply smooth-ing to the bi-gram model.
The simulations are evalu-ated using the SUPER metric proposed earlier (Rieserand Lemon, 2006a), which measures variance andconsistency of the simulated behaviour with respectto the observed behaviour in the original data set.This technique is used because for training we needmore variance to facilitate the exploration of largestate-action spaces, whereas for testing we need sim-ulations which are more realistic.
Both user simula-tions significantly outperform random and majorityclass baselines.
See (Rieser, 2008) for further de-tails.3.5 Reward modellingThe reward function defines the goal of the over-all dialogue.
For example, if it is most importantfor the dialogue to be efficient, the reward penalisesdialogue length, while rewarding task success.
Inmost previous work the reward function is manu-ally set, which makes it ?the most hand-crafted as-pect?
of RL (Paek, 2006).
In contrast, we learn thereward model from data, using a modified versionof the PARADISE framework (Walker et al, 2000),following pioneering work by (Walker et al, 1998).In PARADISE multiple linear regression is used tobuild a predictive model of subjective user ratings(from questionnaires) from objective dialogue per-formance measures (such as dialogue length).
Weuse PARADISE to predict Task Ease (a variable ob-tained by taking the average of two questions in thequestionnaire) 2 from various input variables, viastepwise regression.
The chosen model comprisesdialogue length in turns, task completion (as manu-ally annotated in the WOZ data), and the multimodaluser score from the user questionnaire, as shown inEquation 2.TaskEase = ?
20.2 ?
dialogueLength+11.8 ?
taskCompletion+ 8.7 ?multimodalScore; (2)This equation is used to calculate the overall re-ward for the information acquisition phase.
Dur-ing learning, Task Completion is calculated onlineaccording to the noise model, penalising all slotswhich are filled but not confirmed.2?The task was easy to solve.
?, ?I had no problems findingthe information I wanted.
?For the information presentation phase, we com-pute a local reward.
We relate the multimodal score(a variable obtained by taking the average of 4 ques-tions) 3 to the number of items presented (DB) foreach modality, using curve fitting.
In contrast tolinear regression, curve fitting does not assume alinear inductive bias, but it selects the most likelymodel (given the data points) by function interpo-lation.
The resulting models are shown in Figure3.5.
The reward for multimodal presentation is aquadratic function that assigns a maximal score toa strategy displaying 14.8 items (curve inflectionpoint).
The reward for verbal presentation is a linearfunction assigning negative scores to all presenteditems ?
4.
The reward functions for informationpresentation intersect at no.
items=3.
A comprehen-sive evaluation of this reward function can be foundin (Rieser and Lemon, 2008a).-80-70-60-50-40-30-20-100100  10  20  30  40  50  60  70userscoreno.
itemsreward function for information presentationintersection pointturning point:14.8multimodal presentation: MM(x)verbal presentation: Speech(x)Figure 2: Evaluation functions relating number of itemspresented in different modalities to multimodal score3.6 State space discretisationWe use linear function approximation in order tolearn with large state-action spaces.
Linear func-tion approximation learns linear estimates for ex-pected reward values of actions in states representedas feature vectors.
This is inconsistent with the idea3?I liked the combination of information being displayed onthe screen and presented verbally.
?, ?Switching between modesdid not distract me.
?, ?The displayed lists and tables containedon average the right amount of information.
?, ?The informationpresented verbally was easy to remember.
?642of non-linear reward functions (as introduced in theprevious section).
We therefore quantise the statespace for information presentation.
We partitionthe database feature into 3 bins, taking the first in-tersection point between verbal and multimodal re-ward and the turning point of the multimodal func-tion as discretisation boundaries.
Previous workon learning with large databases commonly quan-tises the database feature in order to learn with largestate spaces using manual heuristics, e.g.
(Levin etal., 2000; Heeman, 2007).
Our quantisation tech-nique is more principled as it reflects user prefer-ences for multi-modal output.
Furthermore, in pre-vious work database items were not only quantisedin the state-space, but also in the reward function,resulting in a direct mapping between quantised re-trieved items and discrete reward values, whereasour reward function still operates on the continuousvalues.
In addition, the decision when to present alist (information acquisition phase) is still based oncontinuous DB values.
In future work we plan to en-gineer new state features in order to learn with non-linear rewards while the state space is still continu-ous.
A continuous representation of the state spaceallows learning of more fine-grained local trade-offsbetween the parameters, as demonstrated by (Rieserand Lemon, 2008b).3.7 Testing the Learned Policies in SimulationWe now train and test the multimodal presentationstrategies by interacting with the simulated learn-ing environment.
For the following RL experimentswe used the REALL-DUDE toolkit of (Lemon et al,2006b).
The SHARSHA algorithm is employed fortraining, which adds hierarchical structure to thewell known SARSA algorithm (Shapiro and Langley,2002).
The policy is trained with the cluster-baseduser simulation over 180k system cycles, which re-sults in about 20k simulated dialogues.
In total, thelearned strategy has 371 distinct state-action pairs(see (Rieser, 2008) for details).We test the RL-based and supervised baselinepolicies by running 500 test dialogues with asmoothed user simulation (so that we are not train-ing and testing on the same simulation).
We thencompare quantitative dialogue measures performinga paired t-test.
In particular, we compare mean val-ues of the final rewards, number of filled and con-firmed slots, dialog length, and items presented mul-timodally (MM items) and items presented ver-bally (verbal items).
RL performs signifi-cantly better (p < .001) than the baseline strategy.The only non-significant difference is the numberof items presented verbally, where both RL and SLstrategy settled on a threshold of less than 4 items.The mean performance measures for simulation-based testing are shown in Table 2 and Figure 3.The major strength of the learned policy is thatit learns to keep the dialogues reasonably short (onaverage 5.9 system turns for RL versus 8.4 turnsfor SL) by presenting lists as soon as the numberof retrieved items is within tolerance range for therespective modality (as reflected in the reward func-tion).
The SL strategy in contrast has not learned theright timing nor an upper bound for displaying itemson the screen.
The results show that simulation-based RL with an environment bootstrapped fromWOZ data allows learning of robust strategies whichsignificantly outperform the strategies contained inthe initial data set.One major advantage of RL is that it allows usto provide additional information about user pref-erences in the reward function, whereas SL simplymimics the data.
In addition, RL is based on de-layed rewards, i.e.
the optimisation of a final goal.For dialogue systems we often have measures indi-cating how successful and/or satisfying the overallperformance of a strategy was, but it is hard to tellhow things should have been exactly done in a spe-cific situation.
This is what makes RL specificallyattractive for dialogue strategy learning.
In the nextsection we test the learned strategy with real users.4 User Tests4.1 Experimental designFor the user tests the RL policy is ported to a work-ing ISU-based dialogue system via table look-up,which indicates the action with the highest expectedreward for each state (cf.
(Singh et al, 2002)).
Thesupervised baseline is implemented using standardthreshold-based update rules.
The experimental con-ditions are similar to the WOZ study, i.e.
we ask theusers to solve similar tasks, and use similar ques-tionnaires.
Furthermore, we decided to use typeduser input rather than ASR.
The use of text input643Measure SL baseline RL StrategySIM REAL SIM REALav.
turns 8.42(?3.04) 5.86(?3.2) 5.9(?2.4)*** 5.07(?2.9)***av.
speech items 1.04(?.2) 1.29(?.4) 1.1(?.3) 1.2(?.4)av.
MM items 61.37(?82.5) 52.2(?68.5) 11.2(?2.4)*** 8.73(?4.4)***av.
reward -1741.3(?566.2) -628.2(?178.6) 44.06(?51.5)*** 37.62(?60.7)***Table 2: Comparison of results obtained in simulation (SIM) and with real users (REAL) for SL and RL-based strate-gies; *** denotes significant difference between SL and RL at p < .001Figure 3: Graph comparison of objective measures: SLs= SL policy in simulation; SLr = SL policy with realusers; RLs = RL policy in simulation; RLr = RL policywith real users.allows us to target the experiments to the dialoguemanagement decisions, and block ASR quality frominterfering with the experimental results (Hajdinjakand Mihelic, 2006).
17 subjects (8 female, 9 male)are given a set of 6?2 predefined tasks, which theysolve by interaction with the RL-based and the SL-based system in controlled order.
As a secondarytask users are asked to count certain objects in a driv-ing simulation.
In total, 204 dialogues with 1,115turns are gathered in this setup.4.2 ResultsIn general, the users rate the RL-based significantlyhigher (p < .001) than the SL-based policy.
The re-sults from a paired t-test on the user questionnairedata show significantly improved Task Ease, betterpresentation timing, more agreeable verbal and mul-timodal presentation, and that more users would usethe RL-based system in the future (Future Use).
Allthe observed differences have a medium effects size(r ?
|.3|).We also observe that female participants clearlyfavour the RL-based strategy, whereas the ratings bymale participants are more indifferent.
Similar gen-der effects are also reported by other studies on mul-timodal output presentation, e.g.
(Foster and Ober-lander, 2006).Furthermore, we compare objective dialogue per-formance measures.
The dialogues of the RL strat-egy are significantly shorter (p < .005), while feweritems are displayed (p < .001), and the help func-tion is used significantly less (p < .003).
The meanperformance measures for testing with real users areshown in Table 2 and Figure 3.
However, there isno significant difference for the performance of thesecondary driving task.5 Comparison of ResultsWe finally test whether the results obtained in sim-ulation transfer to tests with real users, following(Lemon et al, 2006a).
We evaluate the quality ofthe simulated learning environment by directly com-paring the dialogue performance measures betweensimulated and real interaction.
This comparison en-ables us to make claims regarding whether a policywhich is ?bootstrapped?
from WOZ data is transfer-able to real HCI.
We first evaluate whether objectivedialogue measures are transferable, using a pairedt-test.
For the RL policy there is no statistical dif-ference in overall performance (reward), dialoguelength (turns), and the number of presented items(verbal and multimodal items) between simulated644Measure WOZ SL RLav.
Task Ease .53?.14 .63?.26 .79?.21***av.
Future Use .56?.16 .55?.21 .67?.20***Table 3: Improved user ratings over the WOZ studywhere *** denotes p < .001and real interaction (see Table 2, Figure 3).
This in-dicates that the learned strategy transfers well to realsettings.
For the SL policy the dialogue length forreal users is significantly shorter than in simulation.From an error analysis we conclude that real usersintelligently adapt to poor policies, e.g.
by changingtopic, whereas the simulated users do not react inthis way.Furthermore, we want to know whether the sub-jective user ratings for the RL strategy improvedover the WOZ study.
We therefore compare the userratings from the WOZ questionnaire to the user rat-ings of the final user tests using a independent t-testand a Wilcoxon Signed Ranks Test.
Users rate theRL-policy on average 10% higher.
We are especiallyinterested in the ratings for Task Ease (as this wasthe ultimate measure optimised with PARADISE) andFuture Use, as we believe this measure to be an im-portant indicator of acceptance of the technology.The results show that only the RL strategy leads tosignificantly improved user ratings (increasing av-erage Task Ease by 49% and Future Use by 19%),whereas the ratings for the SL policy are not signifi-cantly better than those for the WOZ data, see Table3.
4 This indicates that the observed difference is in-deed due to the improved strategy (and not to otherfactors like the different user population or the em-bedded dialogue system).6 ConclusionWe addressed two problems in the field of automaticoptimization of dialogue strategies: learning effec-tive dialogue strategies when no initial data or sys-tem exists, and evaluating the result with real users.We learned optimal strategies by interaction with asimulated environment which is bootstrapped from4The ratings are normalised as some of the questions wereon different scales.a small amount of Wizard-of-Oz data, and we evalu-ated the result with real users.
The use of WOZ dataallows us to develop optimal strategies for domainswhere no working prototype is available.
The de-veloped simulations are entirely data driven and thereward function reflects real user preferences.
Wecompare the Reinforcement Learning-based strategyagainst a supervised strategy which mimics the (hu-man) wizards?
policies from the original data.
Thiscomparison allows us to measure relative improve-ment over the training data.
Our results show thatRL significantly outperforms SL in simulation aswell as in interactions with real users.
The RL-basedpolicy gains on average 50-times more reward whentested in simulation, and almost 18-times more re-ward when interacting with real users.
The humanusers also subjectively rate the RL-based policy onaverage 10% higher, and 49% higher for Task Ease.We also show that results obtained in simulation arecomparable to results for real users.
We concludethat a strategy trained from WOZ data via boot-strapping is transferable to real Human-Computer-Interaction.In future work will apply similar techniques tostatistical planning for Natural Language Generationin spoken dialogue (Lemon, 2008; Janarthanam andLemon, 2008), (see the EC FP7 CLASSiC project:www.classic-project.org).AcknowledgementsThe research leading to these results has re-ceived funding from the European Community?s7th Framework Programme (FP7/2007-2013) un-der grant agreement no.
216594 (CLASSiC projectwww.classic-project.org), the EC FP6project ?TALK: Talk and Look, Tools for Am-bient Linguistic Knowledge (IST 507802, www.talk-project.org), from the EPSRC, projectno.
EP/E019501/1, and from the IRTG SaarlandUniversity.645ReferencesW.
W. Cohen.
1995.
Fast effective rule induction.
InProc.
of the 12th ICML-95.M.
E. Foster and J. Oberlander.
2006.
Data-driven gen-eration of emphatic facial displays.
In Proc.
of EACL.M.
Frampton and O.
Lemon.
(to appear).
Recent re-search advances in Reinforcement Learning in SpokenDialogue Systems.
Knowledge Engineering Review.N.
M. Fraser and G. N. Gilbert.
1991.
Simulating speechsystems.
Computer Speech and Language, 5:81?99.M.
Hajdinjak and F. Mihelic.
2006.
The PARADISEevaluation framework: Issues and findings.
Computa-tional Linguistics, 32(2):263?272.P.
Heeman.
2007.
Combining reinforcement learn-ing with information-state update rules.
In Proc.
ofNAACL.J.
Henderson, O.
Lemon, and K. Georgila.
2005.
Hy-brid Reinforcement/Supervised Learning for DialoguePolicies from COMMUNICATOR data.
In Proc.
of IJ-CAI workshop on Knowledge and Reasoning in Prac-tical Dialogue Systems, pages 68?75.S.
Janarthanam and O.
Lemon.
2008.
User simula-tions for online adaptation and knowledge-alignmentin Troubleshooting dialogue systems.
In Proc.
of the12th SEMDIAL Workshop (LONdial).O.
Lemon and O. Pietquin.
2007.
Machine learning forspoken dialogue systems.
In Proc.
of Interspeech.O.
Lemon, K. Georgila, and J. Henderson.
2006a.Evaluating Effectiveness and Portability of Reinforce-ment Learned Dialogue Strategies with real users: theTALK TownInfo Evaluation.
In Proc.
of IEEE/ACLworkshop on Spoken Language Technology (SLT).O.
Lemon, X. Liu, D. Shapiro, and C. Tollander.
2006b.Hierarchical reinforcement learning of dialogue poli-cies in a development environment for dialogue sys-tems: REALL-DUDE.
In Proc.
of the 10th SEMDIALWorkshop (BRANdial).O.
Lemon.
2008.
Adaptive Natural Language Gener-ation in Dialogue using Reinforcement Learning.
InProc.
of the 12th SEMDIAL Workshop (LONdial).E.
Levin, R. Pieraccini, and W. Eckert.
2000.
A stochas-tic model of human-machine interaction for learningdialog strategies.
IEEE Transactions on Speech andAudio Processing, 8(1).T.
Paek.
2006.
Reinforcement learning for spoken dia-logue systems: Comparing strengths and weaknessesfor practical deployment.
In Proc.
Dialog-on-DialogWorkshop, Interspeech.O.
Pietquin and T. Dutoit.
2006.
A probabilisticframework for dialog simulation and optimal strategylearnin.
IEEE Transactions on Audio, Speech andLanguage Processing, 14(2):589?599.T.
Prommer, H. Holzapfel, and A. Waibel.
2006.
Rapidsimulation-driven reinforcement learning of multi-modal dialog strategies in human-robot interaction.
InProc.
of Interspeech/ICSLP.R.
Quinlan.
1993.
C4.5: Programs for Machine Learn-ing.
Morgan Kaufmann.V.
Rieser and O.
Lemon.
2006a.
Cluster-based user sim-ulations for learning dialogue strategies.
In Proc.
ofInterspeech/ICSLP.V.
Rieser and O.
Lemon.
2006b.
Using machine learningto explore human multimodal clarification strategies.In Proc.
of ACL.V.
Rieser and O.
Lemon.
2008a.
Automatic learningand evaluation of user-centered objective functions fordialogue system optimisation.
In LREC.V.
Rieser and O.
Lemon.
2008b.
Does this list con-tain what you were searching for?
Learning adaptivedialogue strategies for interactive question answering.Journal of Natural Language Engineering (special is-sue on Interactive Question answering, to appear).V.
Rieser, I.
Kruijff-Korbayova?, and O.
Lemon.
2005.
Acorpus collection and annotation framework for learn-ing multimodal clarification strategies.
In Proc.
of the6th SIGdial Workshop.V.
Rieser.
2008.
Bootstrapping Reinforcement Learning-based Dialogue Strategies from Wizard-of-Oz data (toappear).
Ph.D. thesis, Saarland University.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye, andS.
Young.
2007.
Agenda-based user simulation forbootstrapping a POMDP dialogue system.
In Proc.
ofHLT/NAACL.D.
Shapiro and P. Langley.
2002.
Separating skills frompreference: Using learning to program by reward.
InProc.
of the 19th ICML.S.
Singh, D. Litman, M. Kearns, and M. Walker.
2002.Optimizing dialogue management with reinforcementlearning: Experiments with the NJFun system.
JAIR,16.R.
Sutton and A. Barto.
1998.
Reinforcement Learning.MIT Press.M.
Walker, J. Fromer, and S. Narayanan.
1998.
Learn-ing optimal dialogue strategies: A case study of aspoken dialogue agent for email.
In Proceedings ofACL/COLING.M.
Walker, C. Kamm, and D. Litman.
2000.
Towards de-veloping general models of usability with PARADISE.Journal of Natural Language Engineering, 6(3).J.
Williams and S. Young.
2004.
Using Wizard-of-Ozsimulations to bootstrap reinforcement-learning-baseddialog management systems.
In Proc.
of the 4th SIG-dial Workshop.I.
Witten and E. Frank.
2005.
Data Mining: Practi-cal Machine Learning Tools and Techniques (2nd Edi-tion).
Morgan Kaufmann.646
