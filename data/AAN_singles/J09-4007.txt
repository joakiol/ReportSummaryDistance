Kernel Methods for Minimally Supervised WSDClaudio Giuliano?Fondazione Bruno Kessler ?
IRSTAlfio Massimiliano Gliozzo?
?Fondazione Bruno Kessler ?
IRSTCarlo Strapparava?Fondazione Bruno Kessler ?
IRSTWe present a semi-supervised technique for word sense disambiguation that exploits externalknowledge acquired in an unsupervised manner.
In particular, we use a combination of basickernel functions to independently estimate syntagmatic and domain similarity, building a set ofword-expert classifiers that share a common domain model acquired from a large corpus of un-labeled data.
The results show that the proposed approach achieves state-of-the-art performanceon a wide range of lexical sample tasks and on the English all-words task of Senseval-3, althoughit uses a considerably smaller number of training examples than other methods.1.
IntroductionA significant challenge in many natural language processing tasks is to reduce the needfor labeled training data while maintaining an acceptable performance.
This is espe-cially true for word sense disambiguation (WSD) because when moving from the some-what artificial lexical-sample task to the more realistic all-words task it is practicallyimpossible to collect a large number of training examples for each word sense.
Thus,many supervised approaches, explicitly designed for the lexical-sample task, cannot beapplied to the all-words task, even though they exhibit excellent performance.
This hasled to the somewhat paradoxical situation in which completely different methods havebeen developed for the two tasks, although they represent two sides of the same coin.To address this problem, in recent work we presented a semi-supervised approachbased on kernel methods for WSD (Strapparava, Gliozzo, and Giuliano 2004; Gliozzo,Giuliano, and Strapparava 2005; Giuliano, Gliozzo, and Strapparava 2006).
In particular,we explored the following research directions: (1) independently modeling domain andsyntagmatic aspects of sense distinction to improve feature representativeness; and(2) exploiting external knowledge acquired from unlabeled data, with the purpose ofdrastically reducing the amount of labeled training data.
The first direction is based onthe linguistic assumption that syntagmatic and domain (associative) relations are crucialfor representing sense distinctions, but they are originated by different phenomena.Regarding the second direction, one can hope to obtain a more accurate prediction?
FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy.
E-mail: giuliano@fbk.eu.??
FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy.
E-mail: gliozzo@fbk.eu.?
FBK-irst, via Sommarive 18, I-38050 Povo, Trento, Italy.
E-mail: strappa@fbk.eu.Submission received: 23 December 2006; revised submission received: 28 February 2008; accepted forpublication: 17 April 2008.?
2009 Association for Computational LinguisticsComputational Linguistics Volume 35, Number 4by taking into account unlabeled data relevant to the learning problem (Chapelle,Scho?lkopf, and Zien 2006).
As a matter of fact, to test this hypothesis, most of the lexicalsample tasks of Senseval-3 (Mihalcea and Edmonds 2004) were provided with a largeamount of unlabeled training data, as well as the usual labeled training data.
However,at that time, we were the only team to use the unlabeled data (Strapparava, Gliozzo,and Giuliano 2004).In this article, we review our technique that combines domain and syntagmaticinformation in order to define a complete kernel for WSD.
The rest of the article isorganized as follows.
In Section 2, we provide a general introduction to the kernelmethods, in which we give the basis for understanding our approach.
Exploiting kernelmethods, we can define and combine individual kernels representing information fromdifferent sources in a principled way.
After this introductory section, in Section 3 wepresent the kernels that we developed for WSD.
This includes a detailed descriptionof the individual kernels and the way we define the composite ones.
We present ourexperiments in Section 4.
The results obtained on a range of lexical-sample tasks and onthe English all-words task of Senseval-3 (Mihalcea and Edmonds 2004) show that ourapproach achieves state-of-the-art performance.
Finally, in Section 5, we offer conclu-sions and some directions for future research.2.
Kernel MethodsKernel methods are a popular machine learning approach within the natural lan-guage processing community.
They are theoretically well founded in statistical learn-ing theory and have shown good empirical results in many applications (Vapnik 1999;Cristianini and Shawe-Taylor 2000; Scho?lkopf and Smola 2002; Shawe-Taylor andCristianini 2004).The strategy adopted by kernel methods consists of splitting the learning probleminto two parts.
They first embed the input data in a suitable feature space, and thenuse a linear algorithm to discover nonlinear patterns in the input space.
Typically, themapping is performed implicitly by a so-called kernel function.
The kernel functionis a similarity measure between the input data that depends exclusively on the specificdata type and domain.
A typical similarity function is the inner product between featurevectors.
Characterizing the similarity of the inputs plays a crucial role in determiningthe success or failure of the learning algorithm, and it is one of the central questions inthe field of machine learning.Formally, the kernel is a function K : X?
X ?
R that takes as input two data objects(e.g., vectors, texts, or parse trees) and outputs a real number characterizing theirsimilarity, with the property that the function is symmetric and positive semi-definite.That is, for all xi, xj ?
X satisfiesK(xi, xj) = ??(xi),?(xj)?
(1)where ?
is an (implicit) mapping from X to an (inner product) feature space F .Kernels are used inside learning algorithms such as support vector machines (SVM)or kernel perceptrons as the interface between the algorithm and the data.
The kernelfunction is then the only domain specific element of the system, while the learningalgorithm is a general purpose component.The idea behind the SVM (one of the best known kernel-based learning algorithms)is to map the set of training data into a high-dimensional feature space F via a mappingfunction?
: X ?
F , and construct a separating hyperplane with maximummargin (i.e.,514Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSDthe minimum distance between the hyperplane and data points) in that space.
The useof an appropriate non-linear transformation ?
of the input yields a nonlinear decisionboundary in the input space.
Kernel functions make possible the use of feature spaceswith an exponential or even infinite number of dimensions.
Instead of performing theexplicit feature mapping ?, one can use a kernel function, which permits the (efficient)computation of inner products in high-dimensional feature spaces without explicitlycarrying out the mapping ?.
This is called the kernel trick in the machine learningliterature (Boser, Guyon, and Vapnik 1992).Finally, we point out the theoretical tools required to create new kernels, and com-bine individual kernels to form composite ones.
Of course, not every similarity functionis a valid kernel because, by definition, kernels should be equivalent to some innerproduct in a feature space.
The function K : X?
X ?
R is a valid kernel provided thatits kernel matrices1 are positive semi-definite2 for all training sets S = {x1, ..., xl}, theso-called finitely positive semi-definite property.
Note that defining similarity measuresby means of kernels may be more intuitive than performing the explicit mapping in thefeature space.
Furthermore, this formulation does not require the set X to be a vectorspace: for example, we shall define kernels that take strings as input.This result is not only useful because it opens new perspectives to define kernelfunctions that only implicitly correspond to a feature mapping ?.
Another consequenceis that it can be used to prove a set of rules for combining basic kernels to obtain compos-ite ones.
This will allow us to integrate heterogeneous sources of information in a simpleand effective way.We shall use the following properties of kernels to define our compos-ite kernels.
Let k1 and k2 be kernels over X?
X; then the following functions are kernels: k(xi, xj) = k1(xi, xj)+ k2(xi, xj) k(xi, xj) = c ?
k1(xi, xj), c ?
R+ k(xi, xj) =k1(xi,xj )?k1(xi,xi )?k1(xj,xj )(normalization)In summary, we can define a kernel function by following different strategies: (1)providing an explicit feature mapping ?
: X ?
Rn; (2) defining a similarity functionthat is symmetric and positive semi-definite; and (3) composing different valid kernels,using the closure properties of kernels.
This forms the basis for the approach describedin the following section.3.
Kernel Methods for WSDOur approach toWSD consists of representing linguistic phenomena independently andthen defining a combinationmethod to integrate them.
As described in the previous sec-tion, the kernel function is the only task-specific component of the learning algorithm.Thus, to develop a WSD system, we only need to define appropriate kernel functions torepresent the domain and syntagmatic aspects of sense distinction and, second, exploitthe properties of kernel functions to define a composite kernel to combine and extendthe individual kernels.The resulting WSD system consists of two families of kernels: the domain and thesyntagmatic kernels.
The former family, described in Section 3.1, models the domain1 Given a set of vectors S = {x1, ..., xl}, the kernel matrix K is defined as the l?
lmatrix Kwhose entriesare Kij = k(xi, xj ) = ??
(xi ),?
(xj )?, where k is a kernel function that evaluates the inner products in afeature space with feature map ?.2 A symmetric matrix is positive semi-definite if its eigenvalues are all non-negative.
Actually, as we willsee in Section 3.2 using Proposition 1, it is quite easy to verify this property.515Computational Linguistics Volume 35, Number 4Table 1An example of a domain matrix.Medicine Computer ScienceHIV 1 0AIDS 1 0virus 0.5 0.5laptop 0 1aspects of sense distinction; it is composed of the domain kernel (KD) and the bag-of-words kernel (KBoW).
The latter, described in Section 3.2, represents the syntagmaticaspects of sense distinction; it is composed of the collocation kernel (KColl) and the part-of-speech kernel (KPoS).
Finally, Section 3.3 describes the composite kernel for WSD.3.1 Domain KernelsIt has been shown that domain information is fundamental for WSD (Magnini et al2002).
For instance, the (domain) polysemy between the computer science and themedicine senses of the word virus can be solved by considering the domain of thecontext in which it appears.
Gliozzo, Strapparava, and Dagan (2004) proposed a WSDmethod that exploits only domain information.In the context of kernel methods, domain information can be exploited by defininga kernel function that estimates the domain similarity between the contexts of thewords to be disambiguated.
The simplest method to estimate the domain similaritybetween two texts is to compute the cosine similarity of their vector representationsin the vector space model (VSM).
The VSM is a k-dimensional space Rk, in which thetext tj is represented by a vector tj, where the ith component is the term frequency ofthe term wi in tj.
However, such an approach does not deal well with lexical variabilityand ambiguity.
For instance, despite the fact that the sentences He is affected by AIDSand HIV is a virus express closely-related concepts, their similarity is zero in the VSMbecause they have no words in common (they are represented by orthogonal vectors).On the other hand, due to the ambiguity of the word virus, the similarity between thesentences The laptop has been infected by a virus and HIV is a virus is greater than zero,even though they convey very different messages.To overcome this problem, we introduce the domain model (DM) and show how touse it to define a domain VSM in which texts and terms are represented in a uniformway.
A DM is composed of soft clusters of terms.
Each cluster represents a semanticdomain, that is, a set of terms that often co-occur in texts having similar topics.
A DMis represented by a k?
k?
rectangular matrix D, containing the degree of associationamong terms and domains, as illustrated in Table 1.The matrix D is used to define a function D : Rk ?
Rk?, that maps the vector tjrepresented in the standard VSM into the vector t?j in the domain VSM.
D is definedas follows:3D(tj) = tj(IIDFD) = t?j (2)3 In Wong, Ziarko, and Wong (1985), Equation (2) is used to define a generalized vector space model, ofwhich the domain VSM is a particular instance.516Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSDwhere tj is represented as a row vector, IIDF is a k?
k diagonal matrix such that iIDFi,i =IDF(wi), and IDF(wi) is the inverse document frequency of wi.In the domain space, the similarity is estimated by taking into account second orderrelations among terms.
For example, the similarity of the two sentences He is affectedby AIDS and HIV is a virus is very high, because the terms AIDS, HIV, and virus arestrongly associated with the medicine domain.A DM can be estimated frommanually constructed lexical resources, such as Word-Net Domains (Magnini and Cavaglia` 2000), or by performing a term-clustering processon a (large) corpus.
However, the second approach is more attractive because it allowsus to automatically acquire DMs for different languages and domains.In Gliozzo, Giuliano, and Strapparava (2005), we use singular valued decomposi-tion (SVD) to acquire DMs from a corpus represented by its term-by-document matrixT, in a unsupervised way.4 SVD decomposes the term-by-document matrix T intothree matrixes T  V?k?UT, where V and U are orthogonal matrices (i.e., VTV = I andUTU = I) whose columns are the eigenvectors of TTT and TTT, respectively, and ?k?is the diagonal k?
k matrix containing the highest k?
 k eigenvalues of T, and all theremaining elements set to 0.
The parameter k?
is the dimensionality of the domain VSMand can be fixed in advance.
Under this setting, we define the domain matrix D asfollows:D = INV??k?
(3)where IN is a diagonal matrix such that iNi,i =1??
w?i ,w?i ?, w?i is the ith row of the matrixV??k?
.5Note that in this case, with respect to Table 1, the domains are represented by thecolumns of the matrix D and they do not have an explicit name.
By using a smallnumber of domains, we can define a very compact representation of the DM and, con-sequently, reduce the memory requirements while preserving most of the information.There exist very efficient algorithms to perform the SVD process on sparse matrices,allowing us to perform this operation on large corpora in a very limited time and withreduced memory requirements.6Therefore, we can define the domain kernel to estimate the domain similaritybetween the contexts of the words to be disambiguated.
It is a variant of the latentsemantic kernel (Shawe-Taylor and Cristianini 2004), in which a DM is exploited todefine an explicit mapping D : Rk ?
Rk?from the classical VSM into the domain VSM.The domain kernel is explicitly defined as follows:KD(ti, tj) = ?D(ti),D(tj)?
(4)where D is the domain mapping defined in Equation (2).4 The SVD algorithm was first adopted to perform latent semantic analysis of terms and latent semanticindexing of documents in large corpora (Deerwester et al 1990).5 When D is substituted in Equation (2) the domain VSM is equivalent to a latent semantic space(Deerwester et al 1990).
The only difference in our formulation is that the vectors representing the termsin the domain VSM are normalized by the matrix IN, and then rescaled, according to their IDF value, bymatrix IIDF.
Note the analogy with the tf-idf term weighting schema (Salton and McGill 1983), widelyused in information retrieval.6 To perform the SVD, we used LIBSVDC, an optimized package for sparse matrices that allowsus to perform this step in a few minutes even for large corpora.
It can be downloaded fromhttp://tedlab.mit.edu/?dr/SVDLIBC/.517Computational Linguistics Volume 35, Number 4A standard approach for detecting topic (domain) similarity is to extract bag-of-words features from a wide window of text around the words to be disambiguated.Based on this representation, we define a linear kernel called the bag-of-words kernel(KBoW).
KBoW is a particular case of the domain kernel in which D = I in Equation (2),where I is the identity matrix.
The BoW kernel does not require a DM; therefore, itcan be applied to the strictly supervised settings, in which external knowledge is notavailable.To summarize, the domain kernel allows us to plug external knowledge into thesupervised learning process; it will be compared and combined with the standard bag-of-words approach in Section 4.
In the following section, we shall see that domainmodels are also useful for defining soft-matching collocation kernels.3.2 Syntagmatic KernelsCollocations (such as bigrams and trigrams) extracted from the local context of the wordto be disambiguated are typically used to capture syntagmatic relations (Yarowsky1994).
However, traditional approaches to WSD fail to represent non-contiguous orshifted collocations, and fail to consider lexical variability.
For example, suppose wehave to disambiguate the verb to score in the sentence Ronaldo scored the first goal, giventhe labeled example The football player scored two goals in the second half as training.
Atraditional approach has no clues to return the right answer because the two sentenceshave no features in common.The use of kernels on strings allows us to overcome the aforementioned problemsby representing (non-contiguous) collocations and exploiting external lexical knowl-edge sources to define non-zero measures of similarity between words (soft-matchingcriteria).
In this formulation, words taken in their context are compared by kernels thatsum the number of common (non-contiguous) collocations of words, considering lexicalvariability, and part-of-speech tags, avoiding an explicit feature mapping that wouldlead to an exponential number of features.String kernels (or sequence kernels) are a family of kernel functions developedto compute the inner product among images of strings in high-dimensional featurespace using dynamic programming techniques.
The gap-weighted subsequences kernelis one of the most general types of kernel based on sequences.
Roughly speaking,it compares two strings by means of the number of contiguous and non-contiguoussubstrings of a given length they have in common.
Non-contiguous occurrences arepenalized according to the number of gaps they contain.
Formally, let ?
be an al-phabet of |?| symbols, and s = s1s2 .
.
.
s|s| be a finite sequence over ?
(i.e., si ?
?, 1 i  |s|).
Let i = [i1, i2, .
.
.
, in], with 1  i1 < i2 < .
.
.
< in  |s|, be a subset of the indicesin s; we will denote as s[i] ?
?n the subsequence si1si2 .
.
.
sin .
Note that s[i] does notnecessarily form a contiguous subsequence of s; for example, if s is the sequence?Ronaldo scored the first goal?
and i = [2, 5], then s[i] is ?scored goal?.
The lengthspanned by s[i] in s is l(i) = in ?
i1 + 1.
The feature space associated with the gap-weighted subsequences kernel of length n is indexed by I = ?n, with the embeddinggiven by?nu(s) =?i:u=s[i]?l(i),u ?
?n (5)518Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSDwhere 0<? 1 is the decay factor used to penalize non-contiguous subsequences.7 Theassociate kernel is defined asKn(s, t) = ??n(s),?n(t)?
=?u?
?n?nu(s)?nu(t) (6)An explicit computation of Equation (6) is unfeasible even for small values of n.To evaluate Kn more efficiently, we use the recursive formulation based on a dynamicprogramming implementation (Lodhi et al 2002; Saunders, Tschach, and Shawe-Taylor2002; Cancedda et al 2003).
It is defined in the following equations:K?0(s, t) = 1,?s, t (7)K?i (s, t) = 0, if min(|s|, |t|) < i (8)K?
?i (s, t) = 0, if min(|s|, |t|) < i (9)K?
?i (sx, ty) ={?K?
?i (sx, t) if x = y;?K?
?i (sx, t)+ ?2K?i?1(s, t) otherwise.
(10)K?i (sx, t) = ?K?i (s, t)+ K?
?i (sx, t) (11)Kn(s, t) = 0, if min(|s|, |t|) < n (12)Kn(sx, t) = Kn(s, t)+?j:tj=x?2K?n?1(s, t[1 : j?
1]) (13)where K?n and K?
?n are auxiliary functions with a similar definition to Kn used to facilitatethe computation.
Based on these definitions, Kn can be computed in O(n|s||t|).
Usingthis recursive definition, it turns out that computing all kernel values for subsequencesof lengths up to n is not significantly more costly than computing the kernel for n only.The syntagmatic kernel is defined as a sum of gap-weighted subsequences kernelsthat operate at word and part-of-speech tag level.
In particular, following the approachproposed by Cancedda et al (2003), it is possible to adapt sequence kernels to operateat word level by instancing the alphabet ?
with the vocabulary V = {w1,w2, .
.
.
,wk}.Moreover, we restrict the generic definition of the gap-weighted subsequences kernelto recognize collocations in the local context of a specified word.
The resulting kernel,called the n-gram collocation kernel (KnColl), operates on sequences of lemmata arounda specified word l0 (i.e., l?3, l?2, l?1, l0, l+1, l+2, l+3).
This formulation allows us toestimate the number of common (sparse) subsequences of lemmata (i.e., collocations)between two examples, in order to capture syntagmatic similarity.
Analogously, wedefine the part-of-speech kernel (KnPoS) to operate on sequences of part-of-speech tagsp?3, p?2, p?1, p0, p+1, p+2, p+3, where p0 is the part-of-speech tag of l0.The collocation kernel and the part-of-speech kernel are defined by Equations (14)and (15), respectively.KColl(s, t) =n?l=1KlColl(s, t) (14)7 Notice that by choosing ?
= 1, sparse subsequences are not penalized.
On the other hand, the kernel doesnot take into account sparse subsequences with ?
?
0.519Computational Linguistics Volume 35, Number 4KPoS(s, t) =n?l=1KlPoS(s, t) (15)Both kernels depend on the parameter n, the length of the non-contiguous subse-quences, and ?, the decay factor.
For example, K2Coll allows us to represent all (sparse)bigrams in the local context of a word.
Finally, the syntagmatic kernel is defined asKSynt(s, t) = KColl(s, t)+ KPoS(s, t) (16)In the preceding definition, only exact word-matches contribute to the similarity.To solve this problem, external lexical knowledge is fed into the supervised learningprocess, allowing us to define the soft-matching collocation kernel.
In particular, we de-fine two alternative soft-matching criteria by exploiting synonymy relations inWordNetand DMs acquired from corpora.
Both criteria are based on the assumption that everyword in a sentence can be substituted by another preserving the original meaning, ifthese words are paradigmatically related (e.g., synonyms, hyponyms, or domain relatedwords).
For example, if we consider as equivalent the terms Ronaldo and football player,then the sentence The football player scores the first goal is equivalent to Ronaldo scores thefirst goal, providing a strong evidence to disambiguate the verb to score in the secondsentence.Following the approach proposed by Shawe-Taylor and Cristianini (2004), the soft-matching gap-weighted subsequences kernel is now calculated recursively using Equa-tions (7)?
(9), (11), and (12), replacing Equation (10) by the equation:K?
?i (sx, ty) = ?K?
?i (sx, t)+ ?2axyK?i?1(s, t),?x, y (17)and modifying Equation (13) to:Kn(sx, t) = Kn(s, t)+|t|?j?2axtjK?n?1(s, t[1 : j?
1]) (18)where axy are entries in a similarity matrix A between terms.
In order to ensure that theresulting kernel is still valid, Amust be positive semi-definite.In the following sections, we describe the two alternative soft-matching criteriabased on WordNet Synonymy and Domain Proximity, respectively.
To show that thesimilarity matrices are positive semi-definite, we use the following result.Proposition 1A matrix A is positive semi-definite if and only if A = BTB for some real matrix B.The proof is given in Shawe-Taylor and Cristianini (2004).WordNet Synonymy.
The first soft-matching criterion is based on WordNet8 to definea similarity matrix between words.
In particular, we substitute two words if they aresynonyms.
To this end, a word is represented as vector whose dimensions are associated8 We used WordNet 1.7.1 and MultiWordNet for English and Italian experiments, respectively.520Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSDwith the synsets.
Formally, we define the term-by-synset matrix S as the matrix whoserows are indexed by the terms and whose columns are indexed by the synsets.
The(i, j)th entry of S is 1 if the synset sj contains the term wi; 0 otherwise.
The matrixS gives rise to the similarity matrix A = SST between terms.
Because A can be re-written as A = (ST )TST = BTB, it follows directly from Proposition 1 that it is positivesemi-definite.Domain Proximity.
The second soft-matching criterion exploits the domain models intro-duced in Section 3.1 to define a similarity matrix between words.
Once a DM has beendefined by the matrixD, the domain space is a k?
dimensional space, in which both textsand terms are represented by means of domain vectors, that is, vectors representing thedomain relevances among the linguistic object and each domain.
The domain vector w?ifor the term wi ?
V is the ith row of D, where V = {w1,w2, .
.
.
,wk} is the vocabulary ofthe corpus.
The term-by-domain matrix D gives rise to the similarity matrix A = DDTbetween terms.
It follows by Proposition 1 that A is positive semi-definite.We shall show that the syntagmatic kernel is more effective than standard bigramsand trigrams of lemmata and part-of-speech tags typically used as features in WSD.3.3 Composite KernelHaving defined all the individual kernels representing syntagmatic and domain aspectsof sense distinction, we can define the composite kernel to combine and extend theindividual kernels.
The closure properties of the kernel functions allows us to definethe composite kernel asKC(xi, xj) =n?l=1Kl(xi, xj)?Kl(xj, xj)Kl(xi, xi)(19)where Kl is a valid individual kernel.
The individual kernels are normalized?this playsan important role in allowing us to integrate information from heterogeneous featurespaces.Recent work (Moschitti 2004; Gliozzo, Giuliano, and Strapparava 2005; Zhao andGrishman 2005; Giuliano, Lavelli, and Romano 2006) has empirically shown the effec-tiveness of combining kernels in this way: The composite kernel consistently improvesthe performance of the individual ones.
In addition, this formulation allows us toevaluate the individual contribution of each information source.In order to show the effectiveness of the proposed domain model in supervisedlearning, we defined twoWSD kernels, Kwsd and K?wsd.
They are completely specified bythe n individual kernels that compose them in Equation (19).Kwsd is composed by KColl, KPoS, and KBoW ;K?wsd is composed by KColl, KPoS, KBoW , and KD.The only difference between the two is that K?wsd uses the domain kernel KD to exploitexternal knowledge while Kwsd only uses the labeled training data.521Computational Linguistics Volume 35, Number 44.
EvaluationExperiments were carried out on various tasks of Senseval-3 (Mihalcea and Edmonds2004).
First of all, we conducted a preliminary set of experiments on the Catalan,English, Italian, and Spanish lexical-sample tasks; the results are shown in Section 4.1.Second, in order to show the general applicability of the proposed method, we evalu-ated the system on the English all-words task; the results are presented in Section 4.2.All the experiments were performed using the SVM package (Chang and Lin 2001)customized to embed our own kernels.
The parameters were optimized by five-foldcross-validation on the training set.4.1 Lexical-Sample TasksIn this section, we report the evaluation of our method on the Catalan, English, Italian,and Spanish lexical-sample tasks of Senseval-3 (Mihalcea and Edmonds 2004).
Table 2describes the tasks we have considered.
For each task, it summarizes the number ofwords to be disambiguated, the mean polysemy, the size of the labeled training set,the size of the test set, and the size of the unlabeled training set, respectively.
For theCatalan, Italian, and Spanish tasks, we acquired the DMs from the unlabeled corporamade available by the task organizers.
For the English task, we used a DM acquiredfrom the British National Corpus (BNC) as the task organizers have not providedany unlabeled training data.
The objectives of these experiments are to (a) estimatethe impact of different knowledge sources in WSD; (b) study the effectiveness of thekernel combination; (c) understand the benefits of plugging external information in asupervised framework; and (d) verify the portability of our methodology to differentlanguages.4.1.1 Results.
Table 3 reports the results of the individual kernels KBoW , KD, KColl, andKPoS and their combinations Kwsd and K?wsd (the baselines for the tasks are reported inTable 5).
In our experiments, the parameters n and ?
(see Equation (5)) are optimizedby five-fold cross-validation.
For KnColl, we obtained the best results with n = 2 and?
= 0.5.
For KnPoS, n = 3 and ?
?
0.
The domain cardinality k?
was set to 50.
Table 4shows the performance of the syntagmatic kernel in different configurations: hard andsoft matching.
As a baseline, we report the result of a standard approach consisting ofexplicit bigrams and trigrams of words and part-of-speech tags around the words tobe disambiguated (Yarowsky 1994).
We evaluated the impact of the domain kernel onthe overall performance by comparing the learning curves of K?wsd and Kwsd on the fourlexical-sample tasks.
Figure 1 shows the results of our experiments.
The points of thelearning curves are obtained by sampling the same percentage of training examples forTable 2Description of the lexical-sample tasks of Senseval-3.Task #w mean polysemy #train #test #unlabCatalan 27 3.11 4,469 2,253 23,935English 57 6.47 7,860 3,944 -Italian 45 6.30 5,145 2,439 74,788Spanish 46 3.30 8,430 4,195 61,252522Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSDTable 3The performance (F1) of the basic and composite kernels on the Catalan, English, Italian, andSpanish lexical-sample tasks of Semeval-3.Kernel Catalan English Italian SpanishKBoW 81.3 63.7 43.3 78.2KD 85.2 65.5 44.5 84.4KColl 84.2 68.5 54.0 83.6KPoS 79.6 64.0 44.4 79.5Kwsd 85.2 69.7 53.1 84.2K?wsd 89.0 73.3 61.3 88.2Table 4Performance (F1) of the syntagmatic kernel for the Catalan, English, Italian, and Spanishlexical-sample tasks of Semeval-3.Method Catalan English Italian SpanishBigrams and trigrams 82.6 67.3 51.0 81.9Hard matching 83.8 67.7 51.9 82.9Soft matching (WordNet) - 67.3 51.3 -Soft matching (Domain proximity) 84.2 68.5 54.0 83.6each word.
Finally, Table 5 summarizes the results we obtained, providing a comparisonwith the state of the art.4.1.2 Discussion.
Table 3 shows that domain information and syntagmatic informationare crucial for WSD, and their combination significantly outperforms the individualkernels, showing the effectiveness of the kernel combination method.In addition, the domain kernel KD outperforms the bag-of-words kernel KBoW ,and the composite kernel K?wsd that makes use of domain information outperforms theone Kwsd based only on the labeled training data, demonstrating our assumption (seeSection 3).Table 4 shows that the syntagmatic kernel outperforms the baseline (bigrams andtrigrams) in any configuration (hard-/soft-matching).
The soft-matching criteria furtherimprove the classification performance.
It is interesting to note that the domain proxim-ity obtained better results thanWordNet synonymy (note that we do not have a Catalanor a Spanish WordNet).
The different results observed for Italian and English usingthe domain proximity soft-matching criterion are probably due to the small size of theunlabeled English corpus.Figure 1 shows that K?wsd outperforms Kwsd on all lexical sample tasks, even with asmall number of examples.
It is worth noting, as reported in Table 5, that K?wsd achievesthe same performance as Kwsd using about half of the labeled training data.
This resultshows that the proposed semi-supervised learning approach consisting of acquiringdomain models from unlabeled corpora is effective, as it allows us to drastically reducethe amount of labeled training data and provide a viable solution for the knowledgeacquisition bottleneck problem in WSD.To the best of our knowledge, K?wsd turns out to be the best system for all the testedtasks of Senseval-3, further improving the state of the art by 0.4% to 8.2% for Englishand Italian, respectively.
Finally, we have demonstrated the language independency523Computational Linguistics Volume 35, Number 4Figure 1From left to right, top to bottom, learning curves for the Catalan, English, Italian, and Spanishlexical-sample tasks of Semeval-3.of our approach.
The DMs have been acquired for different languages from differentunlabeled corpora by adopting exactly the same methodology, without requiring anyexternal lexical resource or ad hoc rule.4.2 All-Words TaskEncouraged by the excellent results obtained on the lexical-sample tasks, we evaluatedour approach on the all-words task, in which a very small amount of labeled trainingTable 5Comparative evaluation on the lexical sample tasks.Task MF Agreement BEST Kwsd K?wsd DM+ % of trainingCatalan 66.3 93.1 85.2 85.2 89.0 3.8 46English 55.2 67.3 72.9 69.7 73.3 3.6 54Italian 18.0 89.0 53.1 53.1 61.3 8.2 51Spanish 67.7 85.3 84.2 84.2 88.2 4.0 50Columns report: theMost Frequent baseline, the inter-annotator agreement, the F1 of the best systemat Senseval-3, the F1 of Kwsd, the F1 of K?wsd, DM+ (the improvement due to DM, i.e., K?wsd ?
Kwsd),and the percentage of sense-tagged examples required by K?wsd to achieve the same performanceas Kwsd with full training.524Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSDTable 6The performance (F1) of the basic kernels and composite kernels on the English all-words task ofSenseval-3.basic kernels composite kernelsKbncD KsemD KBoW KPoS KColl Kwsd K?bncwsd K?semwsdF1 63.0 63.2 63.2 63.4 64.0 64.4 65.0 65.2data is typically available.
We performed the evaluation on the English all-words taskof Senseval-3 (Snyder and Palmer 2004).
The test set was extracted from twoWall StreetJournal articles and one text from the Brown Corpus.
The test set consists of 945 words(2,041 word occurrences) to be disambiguated with WordNet 1.7.1 senses.
The inter-annotator agreement rate in the preparation of the corpus was approximately 72.5%.Themost frequent (MF) baseline using the firstWordNet sense heuristic obtained 60.9%.We have trained and tested the system exploiting the following resources: (1) Word-Net 1.7.1 as sense repository; (2) SemCor,9 considering only those words appearingin the Senseval-3 all-words data set?we extracted about 61,700 tagged examples thatconstitute the only labeled training set exploited by the system; and (3) the BNC, fromwhich we extracted the unlabeled training data.4.2.1 Results.We trained 734 word-expert classifiers on the SemCor corpus.
The labeledexamples for each classifier range from a minimum of one example to a maximumof 2,275 examples.
We return a random sense for those words that have no trainingexamples in SemCor.10 We have acquired two DMs, one from the BNC (i.e., K?bncD ; thesame we used in the lexical-sample task) and one from SemCor (i.e., K?semD ), obtaining aslightly better performance with the latter.Table 6 shows the performance of the individual kernels KBoW , KD, KColl, and KPoS,and their composite kernels Kwsd, K?bncD and K?semD .Since for 210 words in the test set we have no training examples, to better under-stand the results obtained, we performed an evaluation on the subset of the test set forwhich at least one training example is available in SemCor.
Evaluating only on thesewords the performance increases from 65.2% to 70.0%, and the most frequent baselinebecomes 65.7%.
Tables 7 and 8 present a more detailed analysis that considers resultsgrouped according to the amount of training available and the mean polysemy of thewords in the test set, excluding from the data set the monosemous words.
Table 7 showsthe results (F1) of K?semwsd at different ranges of polysemy.
Table 8 presents the results (F1)of K?semwsd on those words that have a given number of training examples.
This evaluationis limited to the best composite kernel K?semwsd.4.2.2 Discussion.
We compared our approach with the three best systems that par-ticipated in the English all-words task of Senseval-3.
The best system (Decadt et al2004) has comparable performance (65.2) to ours; however, it uses a larger training setcomposed of 563,129 sense-tagged words.
The training corpus was built by merging9 Texts semantically annotated with WordNet 1.6 senses (created at Princeton University), andautomatically mapped to WordNet 1.7, WordNet 1.7.1, and WordNet 2.0.
Downloadable fromhttp://www.cs.unt.edu/?rada/downloads.html.10 Note that for these words the WordNet first sense is not necessarily the most frequent sense.525Computational Linguistics Volume 35, Number 4Table 7The performance (F1) of K?semwsd at different ranges of polysemy.
Most Frequent baseline (MF) isalso reported.Range of polysemy2?5 6?10 11?15 16?20 21?25 26?30 31+K?semwsd 73.2 61.4 59.1 33.8 55.2 50.2 37.3MF 70.0 53.4 56.4 25.7 47.2 39.0 21.7Table 8The performance (F1) of K?semwsd on words with a given number of training examples.
MostFrequent baseline (MF) and mean polysemy for each partition are also reported.Range of training examples1?10 11?50 51?100 101?200 201+K?semwsd 76.1 70.8 54.2 67.4 60.0MF 73.5 66.4 49.4 63.2 53.0Mean polysemy 3 5 7 9 15SemCor, and English lexical-sample and all-words data sets taken from all the previouseditions of Senseval.
The system proposed by Mihalcea and Faruque (2004) scored sec-ond (64.6).
The dimension of their training set is comparable to ours; however, they alsouse additional information drawn from WordNet to derive semantic generalizationsusing syntactic dependencies.
Finally, the third system (Yuret 2004) obtained 64.1 usinga larger training data set (Semcor, DSO corpus of sense-tagged English, OpenMindWord Expert, Senseval-2, and Senseval-3 lexical-sample tasks).The small difference between the two domain models seems to indicate that alimited amount of unlabeled data is sufficient to improve the overall performance,and the use of unlabeled data taken from the training set helps to slightly improvethe overall performance.
However, the domain model can be acquired from a differentcorpus (e.g., the BNC) without significantly affecting the overall performance.Finally, the results reported in Tables 7 and 8 show that our approach is able to dis-ambiguate with good accuracy (F1 = 76%) words with a number of training examplesthat ranges from 1 to 10, outperforming the most frequent baseline by 3%.
This is aninteresting result given the extremely small number of training examples available.
Onthe other hand, the more training is available for a given word, the more polysemousthat word is.
Nevertheless, the algorithm always outperforms the baseline and has amore significant difference for increasing values of the mean polysemy (from 3% to16%).
These results, together with the ones obtained in the lexical sample tasks, showthat the domain kernel is able to boost the overall performance when little training dataare available, as well as with enough training data.
The benefit is evenmore pronouncedfor the latter case, even though the disambiguation task is more complex due to the highpolysemy of highly frequent words.5.
ConclusionsThis article summarizes the results of a word expert semi-supervised algorithm forWSD based on a combination of kernel functions.
First, we evaluated our methodology526Giuliano, Gliozzo, and Strapparava Kernel Methods for Minimally Supervised WSDon four lexical-sample tasks of Senseval-3, significantly improving the state of the artfor all of them.
In particular, we demonstrated that using external knowledge inside asupervised framework is a viable methodology to reduce the amount of training datarequired for learning.
In our approach, the external knowledge is represented by meansof domain models automatically acquired from corpora in a totally unsupervised way.Then, we applied the method so defined to the English all-words task of Senseval-3, achieving state-of-the-art performance while requiring less labeled training datacompared to the other systems we have found in the literature.Some slight improvement may be possible by exploiting syntactic information pro-duced by a parser.
In the framework of kernel methods, this expansion can be done byadding a tree kernel (i.e., a kernel function that evaluates the similarity among parsetrees) to our composite kernel.
However, the performance achieved is close to the upperbound, if we consider the inter-annotator agreement as an indication of the upper-bound performance.Finally, we think that our semi-supervised approach is at the moment an effectivesolution for developing a sense-tagging system.
Indeed, we tested the system on theEnglish lexical-sample task of SemEval 2007, still obtaining state-of-the-art performance(Pradhan et al 2007).
Therefore, we plan to make available an optimized version ofour system, and to exploit it for ontology learning, textual entailment, and informationretrieval.AcknowledgmentsClaudio Giuliano was supported by theX-Media project (www.x-media-project.org),sponsored by the European Commission aspart of the Information Society Technologies(IST) programme under EC grant IST-FP6-026978.
Alfio Massimiliano Gliozzo andCarlo Strapparava were supported by theONTOTEXT project, sponsored by theAutonomous Province of Trento under theFUP-2004 research program.ReferencesBoser, Bernhard, Isabelle Guyon, andVladimir Vapnik.
1992.
A trainingalgorithm for optimal margin classifier.In Proceedings of the 5th Annual ACMWorkshop on Computational LearningTheory, pages 144?152, Pittsburgh, PA.Cancedda, Nicola, Eric Gaussier, CyrilGoutte, and Jean-Michel Renders.
2003.Word-sequence kernels.
Journal of MachineLearning Research, 32(6):1059?1082.Chang, Chih-Chung and Chih-Jen Lin,2001.
LIBSVM: A library for support vectormachines.
Software available at www.csie.ntu.edu.tw/?cjlin/libsvm.Chapelle, Olivier, Bernhard Scho?lkopf, andAlexander Zien.
2006.
Semi-SupervisedLearning.
MIT Press, Cambridge, MA.Cristianini, Nello and John Shawe-Taylor.2000.
An Introduction to Support VectorMachines.
Cambridge University Press.Decadt, Bart, Veronique Hoste, WalterDaelemans, and Antal van den Bosch.2004.
GAMBL, genetic algorithmoptimization of memory-based WSD.
InProceedings of Senseval-3, pages 108?112,Barcelona.Deerwester, Scott, Susan Dumais, GeorgeFurnas, Thomas Landauer, and RichardHarshman.
1990.
Indexing by latentsemantic analysis.
Journal of the AmericanSociety of Information Science, 41:391?407.Giuliano, Claudio, Alfio Gliozzo, and CarloStrapparava.
2006.
Syntagmatic kernels: Aword sense disambiguation case study.
InIn Proceedings of the EACL-06 Workshop onLearning Structured Information in NaturalLanguage Applications, pages 57?63, Trento.Giuliano, Claudio, Alberto Lavelli, andLorenza Romano.
2006.
Exploiting shallowlinguistic information for relationextraction from biomedical literature.
InProceedings of the Eleventh Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL-2006),pages 401?408, Trento.Gliozzo, Alfio, Claudio Giuliano, and CarloStrapparava.
2005.
Domain kernels forword sense disambiguation.
In Proceedingsof the 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL-05),pages 403?410, Ann Arbor, MI.Gliozzo, Alfio, Carlo Strapparava, andIdo Dagan.
2004.
Unsupervised andsupervised exploitation of semantic527Computational Linguistics Volume 35, Number 4domains in lexical disambiguation.Computer Speech and Language,18(3):275?299.Lodhi, Huma, John Shawe-Taylor, NelloCristianini, and Chris Watkins.
2002.
Textclassification using string kernels.
Journalof Machine Learning Research, 2(3):419?444.Magnini, Bernardo and Gabriela Cavaglia`.2000.
Integrating subject field codes intoWordNet.
In Proceedings of LREC-2000,pages 1413?1418, Athens.Magnini, Bernardo, Carlo Strapparava,Giovanni Pezzulo, and Alfio Gliozzo.
2002.The role of domain information in wordsense disambiguation.
Natural LanguageEngineering, 8(4):359?373.Mihalcea, Rada and Phil Edmonds, editors.2004.
Proceedings of Senseval-3: ThirdInternational Workshop on the Evaluation ofSystems for the Semantic Analysis of Text.Barcelona.Mihalcea, Rada and Ehsanul Faruque.
2004.SenseLearner: Minimally supervised WSDfor all words in open text.
In Senseval-3:Third International Workshop on theEvaluation of Systems for the SemanticAnalysis of Text, pages 155?158, Barcelona.Moschitti, Alessandro.
2004.
A study onconvolution kernels for shallow statisticparsing.
In Proceedings of the 42nd AnnualMeeting of the Association for ComputationalLinguistics (ACL 2004), pages 335?342,Barcelona.Pradhan, Sameer, Edward Loper, DmitriyDligach, and Martha Palmer.
2007.Semeval-2007 task-17: English lexicalsample, SRL and all words.
In Proceedingsof the Fourth International Workshop onSemantic Evaluations (SemEval-2007),pages 87?92, Prague.Salton, Gerard and Michael J. McGill.
1983.Introduction to Modern Information Retrieval.McGraw-Hill, New York.Saunders, Craig, Hauke Tschach, and JohnShawe-Taylor.
2002.
Syllables and otherstring kernel extensions.
In Proceedings of19th International Conference on MachineLearning (ICML02), pages 530?537, Sydney.Scho?lkopf, Bernhard and Alexander Smola.2002.
Learning with Kernels.
MIT Press,Cambridge, MA.Shawe-Taylor, John and Nello Cristianini.2004.
Kernel Methods for Pattern Analysis.Cambridge University Press.Snyder, Benjamin and Martha Palmer.
2004.The English all-words task.
In Senseval-3:Third International Workshop on theEvaluation of Systems for the SemanticAnalysis of Text, pages 41?43, Barcelona.Strapparava, Carlo, Alfio Gliozzo, andClaudio Giuliano.
2004.
Pattern abstractionand term similarity for word sensedisambiguation: Irst at senseval-3.
InSenseval-3: Third International Workshop onthe Evaluation of Systems for the SemanticAnalysis of Text, pages 229?234, Barcelona.Vapnik, Vladimir N. 1999.
The Nature ofStatistical Learning Theory (InformationScience and Statistics).
Springer, Berlin.Wong, S. K. M., Wojciech Ziarko, andPatrick C. N. Wong.
1985.
Generalizedvector space model in informationretrieval.
In Proceedings of the 8th ACMSIGIR Conference, pages 18?25, Montreal.Yarowsky, David.
1994.
Decision lists forlexical ambiguity resolution: Applicationto accent restoration in Spanish andFrench.
In Proceedings of the 32nd AnnualMeeting of the Association for ComputationalLinguistics (ACL 1994), pages 88?95,Las Cruces, NM.Yuret, Deniz.
2004.
Some experiments with anaive Bayes WSD system.
In Senseval-3:Third International Workshop on theEvaluation of Systems for the SemanticAnalysis of Text, pages 265?268, Barcelona.Zhao, Shubin and Ralph Grishman.
2005.Extracting relations with integratedinformation using kernel methods.
InProceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics(ACL 2005), pages 419?426, Ann Arbor, MI.528
