Proceedings of the Third Workshop on Statistical Machine Translation, pages 191?194,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsRanking vs. Regression in Machine Translation EvaluationKevin Duh?Dept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195kevinduh@u.washington.eduAbstractAutomatic evaluation of machine translation(MT) systems is an important research topicfor the advancement of MT technology.
Mostautomatic evaluation methods proposed todate are score-based: they compute scores thatrepresent translation quality, and MT systemsare compared on the basis of these scores.We advocate an alternative perspective of au-tomatic MT evaluation based on ranking.
In-stead of producing scores, we directly producea ranking over the set of MT systems to becompared.
This perspective is often simplerwhen the evaluation goal is system compari-son.
We argue that it is easier to elicit humanjudgments of ranking and develop a machinelearning approach to train on rank data.
Wecompare this ranking method to a score-basedregression method on WMT07 data.
Resultsindicate that ranking achieves higher correla-tion to human judgments, especially in caseswhere ranking-specific features are used.1 MotivationAutomatic evaluation of machine translation (MT)systems is an important research topic for the ad-vancement of MT technology, since automatic eval-uation methods can be used to quickly determine the(approximate) quality of MT system outputs.
This isuseful for tuning system parameters and for compar-ing different techniques in cases when human judg-ments for each MT output are expensivie to obtain.Many automatic evaluation methods have beenproposed to date.
Successful methods such as BLEU?Work supported by an NSF Graduate Research Fellowship.
(Papineni et al, 2002) work by comparing MT out-put with one or more human reference translationsand generating a similarity score.
Methods differ bythe definition of similarity.
For instance, BLEU andROUGE (Lin and Och, 2004) are based on n-gramprecisions, METEOR (Banerjee and Lavie, 2005)and STM (Liu and Gildea, 2005) use word-classor structural information, Kauchak (2006) leverageson paraphrases, and TER (Snover et al, 2006) usesedit-distances.
Currently, BLEU is the most popu-lar metric; it has been shown that it correlates wellwith human judgments on the corpus level.
How-ever, finding a metric that correlates well with hu-man judgments on the sentence-level is still an openchallenge (Blatz and others, 2003).Machine learning approaches have been proposedto address the problem of sentence-level evalua-tion.
(Corston-Oliver et al, 2001) and (Kuleszaand Shieber, 2004) train classifiers to discrim-inate between human-like translations and auto-matic translations, using features from the afore-mentioned metrics (e.g.
n-gram precisions).
In con-trast, (Albrecht and Hwa, 2007) argues for a re-gression approach that directly predicts human ad-equecy/fluency scores.All the above methods are score-based in thesense that they generate a score for each MT systemoutput.
When the evaluation goal is to compare mul-tiple MT systems, scores are first generated inde-pendently for each system, then systems are rankedby their respective scores.
We think that this two-step process may be unnecessarily complex.
Whysolve a more difficult problem of predicting the qual-ity of MT system outputs, when the goal is simply191to compare systems?
In this regard, we propose aranking-based approach that directly ranks a set ofMT systems without going through the intermediaryof system-specific scores.
Our approach requires (a)training data in terms of human ranking judgmentsof MT outputs, and (b) a machine learning algorithmfor learning and predicting rankings.1The advantages of a ranking approach are:?
It is often easier for human judges to rank MToutputs by preference than to assign absolutescores (Vilar et al, 2007).
This is because it isdifficult to quantify the quality of a translationaccurately, but relative easy to tell which oneof several translations is better.
Thus human-annotated data based on ranking may be lesscostly to acquire.?
The inter- and intra-annotator agreement forranking is much more reasonable than that ofscoring.
For instance, Callison-Burch (2007)found the inter-annotator agreement (Kappa)for scoring fluency/adequency to be around.22-.25, whereas the Kappa for ranking isaround .37-.56.
Thus human-annotated databased on ranking may be more reliable to use.?
As mentioned earlier, when the final goal ofthe evaluation is comparing systems, rankingmore directly solves the problem.
A scoringapproach essentially addresses a more difficultproblem of estimating MT output quality.Nevertheless, we note that score-based ap-proaches remain important in cases when the ab-solute difference between MT quality is desired.For instance, one might wonder by how much doesthe top-ranked MT system outperform the second-ranked system, in which case a ranking-based ap-proach provide no guidance.In the following, Section 2 formulates thesentence-level MT evaluation problem as a rankingproblem; Section 3 explains a machine learning ap-proach for training and predicting rankings; this isour submission to the WMT2008 Shared Evaluation1Our ranking approach is similar to Ye et.
al.
(2007), whowas the first to advocate MT evaluation as a ranking problem.Here we focus on comparing ranking vs. scoring approaches,which was not done in previous work.task.
Ranking vs. scoring approaches are comparedin Section 4.2 Formulation of the Ranking ProblemWe formulate the sentence-level MT evaluationproblem as follows: Suppose there are T source sen-tences to be translated.
Let rt, t = 1..T be the set ofreferences2 .
Corresponding to each source sentence,there are N MT system outputs o(n)t , n = 1..N andMt (Mt ?
N ) human evaluations.
The evaluationsare represented as Mt-dimensional label vectors yt.In a scoring approach, the elements of yt may cor-respond to, e.g.
a fluency score on a scale of 1 to 5.In a ranking approach, they may correspond to rel-ative scores that are used to represent ordering (e.g.yt = [6; 1; 3] means that there are three outputs, andthe first is ranked best, followed by third, then sec-ond.
)In order to do machine learning, we extract fea-ture vectors x(n)t from each pair of rt and o(n)t .3The set {(x(n)t , yt)}t=1..T forms the training set.In a scoring approach, we train a function f withf(x(n)t ) ?
y(n).
In a ranking approach, we trainf such that higher-ranked outputs have higher func-tion values.
In the example above, we would want:f(x(n=1)t ) > f(x(n=3)t ) > f(x(n=2)t ).
Once f istrained, it can be applied to rank any new data: this isdone by extracting features from references/outputsand sorting by function values.3 Implementation3.1 Sentence-level scoring and rankingWe now describe the particular scoring and rank-ing implementations we examined and submitted tothe WMT2008 Shared Evaluation task.
In the scor-ing approach, f is trained using RegressionSVM(Drucker and others, 1996); in the ranking ap-proach, we examined RankSVM (Joachims, 2002)and RankBoost (Freund et al, 2003).
We used onlylinear kernels for RegressionSVM and RankSVM,while allowed RankBoost to produce non-linear fbased on a feature thresholds.2Here we assume single reference for ease of notation; thiscan be easily extended for multiple reference3Only Mt (not N ) features vectors are extracted in practice.192ID Description1-4 log of ngram precision, n=1..45 ratio of hypothesis and reference length6-9 ngram precision, n=1..410-11 hypothesis and reference length12 BLEU13 Smooth BLEU14-20 Intra-set features for ID 5-9, 12,13Table 1: Feature set: Features 1-5 can be combined (withuniform weights) to form the log(BLEU) score.
Features6-11 are redundant statistics, but scaled differently.
Fea-ture 12 is sentence-level BLEU; Feature 13 is a modifiedversion with add-1 count to each ngram precision (thisavoids prevalent zeros).
Features 14-20 are only availablein the ranking approach; they are derived by comparingdifferent outputs within the same set to be ranked.The complete feature set is shown in Table 1.
Werestricted our feature set to traditional BLEU statis-tics since our experimental goal is to directly com-pare regression, ranking, and BLEU.
Features 14-20 are the only novel features proposed here.
Wewanted to examine features that are enabled by aranking approach, but not possible for a scoringapproach.
We thus introduce ?intra-set features?,which are statistics computed by observing the en-tire set of existing features {x(n)t }n=1..Mt .For instance: We define Feature 14 by looking atthe relative 1-gram precision (Feature 1) in the set ofMt outputs.
Feature 14 is set to value 1 for the out-put which has the best 1-gram precision, and value 0otherwise.
Similarly, Feature 15 is a binary variablethat is 1 for the output with the best 2-gram preci-sion, and 0 for all others.
The advantage of intra-setfeatures is calibration.
e.g.
If the outputs for rt=1all have relatively high BLEU compared to thoseof rt=2, the basic BLEU features will vary widelyacross the two sets, making it more difficult to fit aranking function.
On the other hand, intra-set fea-tures are of the same scale ([0, 1] in this case) acrossthe two sets and therefore induce better margins.While we have only explored one particular in-stantiation of intra-set features, many other defini-tions are imaginable.
Novel intra-set features is apromising research direction; experiments indicatethat they are most important in helping ranking out-perform regression.3.2 Corpus-level rankingSentence-level evaluation generates a ranking foreach source sentence.
How does one producean overall corpus-level ranking based on a set ofsentence-level rankings?
This is known as the?consensus ranking?
or ?rank aggregation?
prob-lem, which can be NP-hard under certain formula-tions (Meila?
et al, 2007).
We use the FV heuristic(Fligner and Verducci, 1988), which estimates theempirical probability Pij that system i ranks abovesystem j from sentence-level rankings (i.e.
Pij =number of sentences where i ranks better than j, di-vided by total number of sentences).
The corpus-level ranking of system i is then defined as?j?
Pij?
.4 ExperimentsFor experiments, we split the provided developmentdata into train, dev, and test sets (see Table 2).
Thedata split is randomized at the level of different eval-uation tracks (e.g.
en-es.test, de-en.test are differ-ent tracks) in order to ensure that dev/test are suffi-ciently novel with respect to the training data.
Thisis important since machine learning approaches havethe risk of overfitting and spreading data from thesame track to both train and test could lead to over-optimistic results.Train Dev Test# tracks 8 3 3# sets 1504 (63%) 514 (21%) 390 (16%)# sent 6528 (58%) 2636 (23%) 2079 (19%)Table 2: Data characteristics: the training data contains8 tracks, which contained 6528 sentence evaluations or1504 sets of human rankings (T = 1504).In the first experiment, we compared RegressionSVM and Rank SVM (both used Features 1-12) bytraining on varying amounts of training data.
Thesentence-level rankings produced by each are com-pared to human judgments using the Spearman rankcorrelation coefficient (see Figure 1).In the second experiment, we compared all rank-ing and scoring methods discussed thus far.
The fulltraining set is used; the dev set is used to tune thecost parameter for the SVMs and number of itera-tions for RankBoost, which is then applied withoutmodification to the test set.
Table 3 shows the aver-1930 10 20 30 40 50 60 70 80 90 1000.220.240.260.280.30.320.340.36Percentage of training dataSpearmancoeffData ablation results on Dev SetRankSVMRegressionSVMFigure 1: Ranking slightly outperforms Regression forvarious amounts of training data.
Regression results ap-pear to be less stable, with a rise/fall in average Spear-man coefficent around 20%, possibly because linear re-gression functions become harder to fit with more data.age Spearman coefficient for different methods anddifferent feature sets.
There are several interestingobservations:1.
BLEU performs poorly, but SmoothedBLEU isalmost as good as the machine learning meth-ods that use same set of basic BLEU features.2.
Rank SVM slightly outperforms RankBoost.3.
Regression SVM and Rank SVM gave simi-lar results under the same feature set.
How-ever, Rank SVM gave significant improve-ments when intra-set features are incorporated.The last observation is particularly important: itshows that the training criteria differences betweenthe ranking and regression is actually not critical.Ranking can outperform regression, but only whenranking-specific features are considered.
Withoutintra-set features, ranking methods may be sufferingthe same calibration problems as regression.ReferencesJ.
Albrecht and R. Hwa.
2007.
A re-examination of ma-chine learning approaches for sentence-level MT eval-uation.
In ACL.S.
Banerjee and A. Lavie.
2005.
Meteor: An auto-matic metric for mt evaluation with improved corre-lation with human judgments.
In ACL 2005 Wksp onIntrinsic/Extrinsic Evaluation for MT/Summarization.J.
Blatz et al 2003.
Confidence estimation for machinetranslation.
Technical report, Johns Hopkins Univer-sity, Natural Language Engineering Workshop.C.
Callison-Burch et al 2007.
(meta-) evaluation of ma-chine translation.
In ACL2007 SMT Workshop.Feature Dev TestBLEU 1-5 .14 .05Smoothed BLEU 1-5 .19 .24Regression SVM 1-12 .33 .24RankSVM 1-12 .34 .25RankBoost 1-12 .29 .22RankSVM 1-20 .52 .42RankBoost 1-20 .51 .38Table 3: Average Spearman coefficients on Dev/Test.
Theintra-set features gave the most significant gains (e.g.
.42on test of RankSVM).
Refer to Table 1 to see what fea-tures are used in each row.
The SVM/RankBoost resultsfor features 1-12 and 1-5 are similar; only those of 1-12are reported.S.
Corston-Oliver, M. Gamon, and C. Brockett.
2001.
Amachine learning approach to the automatic evaluationof machine translation.
In ACL.H.
Drucker et al 1996.
Support vector regression ma-chines.
In NIPS.M.
Fligner and J. Verducci.
1988.
Multistage rankingmodels.
Journal of American Statistical Assoc., 88.Y.
Freund, R. Iyer, R. Schapire, and Y.
Singer.
2003.
Anefficient boosting method for combining preferences.JMLR, 4.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In KDD.D.
Kauchak and R. Barzilay.
2006.
Paraphrasing forautomatic evaluation.
In NAACL-HLT.A.
Kulesza and S. Shieber.
2004.
A learning approach toimproving sentence-level mt evaluation.
In TMI.C.-Y.
Lin and F. Och.
2004.
Automatic evaluation of ma-chine translation quality using longest common subse-quence and skip-bigram statistics.
In ACL.D.
Liu and D. Gildea.
2005.
Syntactic features for eval-uation of machine translation.
In ACL 2005 Wksp onIntrinsic/Extrinsic Evaluation for MT/Summarization.M.
Meila?, K. Phadnis, A. Patterson, and J. Bilmes.
2007.Consensus ranking under the exponential model.
InConf.
on Uncertainty in Artificial Intelligence (UAI).K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In ACL.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Conf.
of Assoc.
forMachine Translation in the Americas (AMTA-2006).D.
Vilar, G. Leusch, H. Ney, and R. Banchs.
2007.
Hu-man evaluation of machine translation through binarysystem comparisons.
In ACL2007 SMT Workshop.Y.
Ye, M. Zhou, and C.-Y.
Lin.
2007.
Sentence levelmachine translation evaluation as a ranking problem.In ACL2007 Wksp on Statistical Machine Translation.194
