Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 10?18,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsRefining the most frequent sense baselineJudita PreissDepartment of LinguisticsThe Ohio State Universityjudita@ling.ohio-state.eduJon DehdariDepartment of LinguisticsThe Ohio State Universityjonsafari@ling.ohio-state.eduJosh KingComputer Science and EngineeringThe Ohio State Universitykingjo@cse.ohio-state.eduDennis MehayDepartment of LinguisticsThe Ohio State Universitymehay@ling.ohio-state.eduAbstractWe refine the most frequent sense baselinefor word sense disambiguation using a num-ber of novel word sense disambiguation tech-niques.
Evaluating on the S??????
?-3 Englishall words task, our combined system focuseson improving every stage of word sense dis-ambiguation: starting with the lemmatizationand part of speech tags used, through the ac-curacy of the most frequent sense baseline, tohighly targeted individual systems.
Our super-vised systems include a ranking algorithm anda Wikipedia similarity measure.1 IntroductionThe difficulty of outperforming the most frequentsense baseline, the assignment of the sense whichappears most often in a given annotated corpus, inword sense disambiguation (WSD) has been broughtto light by the recent S???????
WSD system evalu-ation exercises.
In this work, we present a combi-nation system, which, rather than designing a singleapproach to all words, enriches the most frequentsense baseline when there is high confidence for analternative sense to be chosen.WSD, the task of assigning a sense to a givenword from a sense inventory is clearly necessaryfor other natural language processing tasks.
For ex-ample, when performing machine translation, it isnecessary to distinguish between word senses in theoriginal language if the different senses have differ-ent possible translations in the target language (Yn-gve, 1955).
A number of different approaches toWSD have been explored in recent years, with twodistinct approaches: techniques which require anno-tated training data (supervised techniques) and tech-niques which do not (unsupervised methods).It has long been believed that supervised systems,which can be tuned to a word?s context, greatly out-perform unsupervised systems.
This theory was sup-ported in the S???????
WSD system evaluation exer-cises, where the performance gap between the bestsupervised system and the best unsupervised sys-tem is large.
Unsupervised systems were found tonever outperform the most frequent sense (MFS)baseline (a sense assignment made on the basis ofthe most frequent sense in an annotated corpus),while supervised systems occasionally perform bet-ter than the MFS baseline, though rarely by morethan 5%.
However, recent work by McCarthy et al(2007) shows that acquiring a predominant sensefrom an unannotated corpus can outperform manysupervised systems, and under certain conditionswill also outperform the MFS baseline.Rather than proposing a new algorithm which willtackle all words, we focus on improving upon theMFS baseline system when an alternative systemproposes a high confidence answer.
An MFS refin-ing system can therefore benefit from answers sug-gested by a very low recall (but high precision) WSDsystem.
We propose a number of novel approachesto WSD, but also demonstrate the importance of ahighly accurate lemmatizer and part of speech tag-ger to the English all words task of S??????
?-3.1We present our enriched most frequent sense1Unless specified otherwise, we use WordNet 1.7.1 (Milleret al, 1990) and the associated sense annotated SemCor cor-pus (Miller et al, 1993) (translated to WordNet 1.7.1 by RadaMihalcea).10baseline in Section 2, which motivates the lemma-tizer and part of speech tagger refinements presentedin Section 3.
Our novel high precision WSD al-gorithms include a reranking algorithm (Section 4),and a Wikipedia-based similarity measure (Sec-tion 5).
The individual systems are combined inSection 6, and we close with our conclusions in Sec-tion 7.2 Most frequent sense baselineThe most frequent sense (MFS) baseline assumesa sense annotated corpus from which the frequen-cies of individual senses are learnt.
For each tar-get word, a part of speech tagger is used to deter-mine the word?s part of speech, and the MFS forthat part of speech is selected.
Although this is afairly naive baseline, it has been shown to be diffi-cult to beat, with only 5 systems of the 26 submittedto the S??????
?-3 English all words task outperform-ing the reported 62.5% MFS baseline.
The successof the MFS baseline is mainly due to the frequencydistribution of senses, with the shape of the senserank versus frequency graph being a Zipfian curve(i.e., the top-ranked sense being much more likelythan any other sense).However, two different MFS baseline perfor-mance results are reported in Snyder and Palmer(2004), with further implementations being differ-ent still.
The differences in performance of the MFSbaseline can be attributed to a number of factors:the English all words task is run on natural text andtherefore performance greatly depends on the accu-racy of the lemmatizer and the part of speech tag-ger employed.2 If the lemmatizer incorrectly iden-tifies the stem of the word, the MFS will be lookedup for the wrong word and the resulting sense as-signment will be incorrect.
The performance of theMFS given the correct lemma and part of speechinformation is 66%, while the performance of theMFS with a Port Stemmer without any POS infor-mation is 32%.
With a TreeTagger (Schmidt, 1994),and a sophisticated lemma back-off strategy, the per-formance increases to 56%.
It is this difference in2Other possible factors include: 1) The sense distribution inthe corpus which the MFS baseline is drawn from, 2) If SemCoris used as the underlying sense annotated corpus, the accuracyof the mapping from WordNet 1.6 (with which SemCor wasinitially annotated) to WordNet 1.7.1 could also have an effecton the performance).performance which motivates refining the most fre-quent sense baseline, and our work on improvingthe underlying lemmatizer and part of speech taggerpresented in Section 3.Our initial investigation refines the SemCor basedMFS baseline using the automatic method of de-termining the predominant sense presented in Mc-Carthy et al (2007).1.
For nouns and adjectives which appear in Sem-Cor fewer than 5 times, we employ the auto-matically determined predominant sense.2.
For verbs which appear in SemCor fewer than 5times, we employ subcategorization frame sim-ilarity rather than Lesk similarity to give us averb?s predominant sense.2.1 Predominant senseMcCarthy et al (2007) demonstrate that it is possi-ble to acquire the predominant sense for a word ina corpus without having access to annotated data.They employ an automatically created thesaurus(Lin, 1998), and a sense?word similarity metric toassign to each sense si of a word w a score corre-sponding to?n j?Nwdss(w, n j) ?
sss(si, n j)?s?i?senses(w) sss(s?i , n j)where dss(w, n j) reflects the distributional simi-larity of word w to n j, w?s thesaural neighbour, andsss(si, n j) = maxsx?senses(n j) sss?
(si, sx) is the max-imum similarity3 between w?s sense si and a sensesx of w?s thesaural neighbour n j.
The authors showthat although this method does not always outper-form the MFS baseline based on SemCor, it doesoutperform it when the word?s SemCor frequency isbelow 5.
We therefore switch our MFS baseline tothis value for such words.
This result is representedas ?McCarthy?
in Table 1, which contains the resultsof the techniques presented in this Section evaluatedon the S??????
?-3 English all words task.2.2 Verb predominant senseMcCarthy et al (2007) observe that their predom-inant sense method is not performing as well for3We use the Lesk (overlap) similarity as implemented by theWordNet::similarity package (Pedersen et al, 2004).11System Precision Recall F-measureMFS 58.4% 58.4% 58.4%McCarthy 58.5% 58.5% 58.5%Verbs 58.5% 58.5% 58.5%All 58.6% 58.6% 58.6%Table 1: Refining the MFS baseline with predominantsenseverbs as it does for nouns and adjectives.
We hy-pothesize that this is due to the thesaural neighboursobtained from Lin?s thesaurus, and we group verbsaccording to the subcategorization frame (SCF) dis-tributions they present in the ?????
(Korhonen et al,2006) lexicon.
A word w1 is grouped with word w2if the Bhattacharyya coefficientBC(w1, w2) =?x?X?p(x)q(x)where p(x) and q(x) represent the probability val-ues for subcategorization class x, is above a cer-tain threshold.
The BC coefficient then replaces thedss value in the original formula and the predomi-nant senses are obtained.
Again, this system is onlyused for words with frequency lower than 5 in Sem-Cor.
The great advantage of the Bhattacharyya co-efficient over various entropy based similarity mea-sures which are usually used to compare SCF distri-butions (Korhonen and Krymolowski, 2002), is thatit is guaranteed to lie between 0 and 1, unlike theentropy based measures which are not easily com-parable between different word pairs.
This result isrepresented by ?Verbs?
in Table 1.Table 1 displays the results for the MFS, the MFScombined with the two approaches described above,and the MFS combining MFS with verbs and Mc-Carthy.3 Lemmatization and Part of SpeechTaggingWe made use of several lemmatizers and part-of-speech taggers, in order to give the other WSD com-ponents the best starting point possible.3.1 LemmatizationLemmatization, the process of obtaining the canon-ical form of a word, was the first step for us toultimately identify the correct WordNet sense ofa given word in the English all words task.
Wefound that without any lemmatizing of the test input,the maximum f -score possible was in the mid-50?s.Conversely, we found that a basic most-frequent-sense system that had a perfectly-lemmatized inputachieved an f -score in the mid-60?s.
This large dif-ference in the ceiling of a non-lemmatized systemand the floor of a perfectly-lemmatized system mo-tivated us to focus on this task.We looked at three different lemmatizers: the lem-matizing backend of the XTAG project (XTAG Re-search Group, 2001)4, Celex (Baayen et al, 1995),and the lemmatizing component of an enhancedTBL tagger (Brill, 1992).5 We then employed a vot-ing system on these three components, taking thelemma from the most individual lemmatizers.
If allthree differ, we take the lemma from the most accu-rate individual system, namely the TBL tagger.3.1.1 Lemmatizer EvaluationWe evaluated the lemmatizers against the lem-mas found in the S??????
?-3 gold standard.6 Eventhe lowest performing system improved accuracyby 31.74% over the baseline, which baseline sim-ply equates the given token with the lemma.
Ta-ble 2 shows the results of evaluating the lemmatizersagainst the EAW key.While the simple voting system performed bet-ter than any of the individual lemmatizers, hyphen-ated words proved problematic for all of the sys-tems.
Some hyphenated words in the test set re-mained hyphenated in the gold standard, and someothers were separated.
However, evaluation resultsshow that splitting hyphenated words increases lem-matizing accuracy by 0.9% .3.2 Part of Speech TaggingWe also investigated the contribution of part ofspeech taggers to the task of word sense disam-biguation.
We considered three taggers: the El-worthy bigram tagger (Elworthy, 1994) within theRASP parser (Briscoe et al, 2006), an enhanced4http://www.cis.upenn.edu/?xtag5http://gposttl.sourceforge.net6We removed those lines from both the test input and thegold standard which were marked U (= unknown, 34 lines), andwe removed the 40 lines from the test input that were missingfrom the gold standard.
This gave us 2007 words in both thetest set and the gold standard.12Lemmatizer AccuracyBaseline 57.50%XTAG 89.24%Celex 91.58%TBL 92.38%Voting {XTAG,Celex,TBL} 93.77%Voting, no hyphen {XTAG,Celex,TBL} 94.67%Table 2: Accuracy of several lemmatizers on <head>words of EAW task.TBL tagger (Brill, 1992)7, and a TnT-style trigramtagger (Hala?csy et al, 2007).8 The baseline was aunigram tagger which selects the most frequently-occurring tag of singletons when dealing with un-seen words.All three of the main taggers performed compa-rably, although only the Elworthy tagger providesprobabilities associated with tags, rather than get-ting a single tag as output.
This additional infor-mation can be useful, since we can employ differ-ent strategies for a word with one single tag with aprobability of 1, versus a word with multiple tags,the most probable of which might only have a prob-ability of 0.3 for example.
For comparative pur-poses, we mapped the various instantiations of tagsfor nouns, verbs, adjectives, and adverbs to thesefour basic tags, and evaluated the taggers?
resultsagainst the EAW key.
Table 3 shows the results ofthis evaluation.The performance of these taggers on the EAW<head>-words is lower than results reported onother datasets.
This can explained by the lack offrequently-occurring function words, which are easyto tag and raise overall accuracy.
Also, the wordsin the test set are often highly ambiguous not onlywith respect to their word sense, but also their partof speech.4 Supervised Learning of Sparse CategoryIndices for WSDIn this component of our refinement of the base-line, we train a supervised system that performshigher-precision classification, only returning an an-swer when a predictive feature that strongly pre-dicts a particular sense is observed.
To achieve this,7http://gposttl.sourceforge.net8http://code.google.com/p/hunposPOS Tagger AccuracyBaseline 84.10%TBL 90.48%Elworthy 90.58%TnT 91.13%Voting {TBL,Elw.,TnT} 91.88%Table 3: Accuracy of several POS taggers on <head>words of EAW task.we implemented a ?feature focus?
classifier (sparseweighted index) as described in (Madani and Con-nor, 2008, henceforth, MC08).
MC08?s methodsfor restricting and pruning the number of feature-to-class associations are useful for finding and retain-ing only strong predictive features.
Moreover, thisallowed us to use a rich feature set (more than 1.6million features) without an unwieldy explosion inthe number of parameters, as feature-class associa-tions that are not strong enough are simply dropped.4.1 Sparse Category IndicesMC08 describe a space and time efficient methodfor learning discriminative classifiers that rank largenumbers of output classes using potentially millionsof features for many instances in potentially tera-scale data sets.
The authors describe a method forlearning ?category indices?
?
i.e., weighted bipar-tite graphs G ?
F ?W ?C, where F is the set of fea-tures, C is the set of output classes and all weights(or ?associations?)
w ?
W between features and theoutput classes they predict are real-valued and in[0.0, 1.0].
The space and time efficiency of MC08?sapproach stems chiefly from three (parameterisable)restrictions on category indices and how they are up-dated.
First, at any time in the learning process, onlythose edges ( fi, w j, ck) ?
G whose associations w jare a large enough proportion of the sum of all classassociations for fi are retained: that is, only retainw j s.t.
w j ?
wmin.9 Second, by setting an upperbound dmax on the number of associations that afeature fi is allowed to have, only the largest fea-ture associations are retained.
Setting dmax to a lownumber (?
25) makes each feature a high-precision,low-recall predictor of output classes.
Further, thedmax and wmin restrictions on parameter reten-9Recall that w j ?
W are all between 0.0 and 1.0 and sum to1.0.13tion allow efficient retrieval and update of featureweights, as only a small number of feature weightsneed be consulted for predicting output classes orlearning from prediction mistakes in an online learn-ing setting.10 Finally, in the online learning algo-rithm,11 in addition to the small number of featuresthat need be consulted or updated, an error marginmarg can be set so that parameter update only oc-curs when the score(c)?
score(c?)
?
marg, where cis the correct output class and c?
, c is the most con-fident incorrect prediction of the classifier.
Settingmarg = 0.0 leads to purely error-driven learning,while marg = 1.0 always updates on every learninginstance.
Values of marg ?
(0.0, 1.0) will bias thecategory index learner to update at different levelsof separation of the correct class from the most con-fident incorrect class, ranging from almost alwayserror driven (near 0.0) to almost error-insensitivelearning (near 1.0).4.2 Integration into the WSD TaskUsing both the Semcor-3 and English Lexical Sam-ple training data sets (a total of ?45,000 sentences,each with one or more labeled instances), we traineda sparse category index classifier as in MC08 withthe following features: using words, lemmas andparts of speech (POSs) as tokens, we define fea-tures for (1) preceding and following unigrams andbigrams over tokens, as well as (2) the conjunc-tion of the preceding unigrams (i.e., a 3-word win-dow minus the current token) and (3) the conjunc-tion of the preceding and following bigrams (5-word window minus the current token).
Finallyall surrounding lemmas in the sentence are treatedas left- or right-oriented slot-independent featureswith an exponentially decaying level of activationact(li) = 0.5 ?
exp(0.5 ?
?
dist(li , targ wd))?
where dist(li, targ wd) is simply the word dis-tance from the target word to the contextual lemmali.12 Although WSD is not a many-class, large-10dmax bounds the number of feature-class associations (pa-rameters) must be consulted in prediction and updating, but,because of the wmin restriction, MC08 found that, on aver-age, many fewer feature associations ?
?
16 ?
were evertouched per training or testing instance in their classificationexperiments.
See Madani and Connor (2008) for more details.11Again, see Madani and Connor (2008) for more details.12The value 0.5 is also a parameter that we have fixed, but itcould in principle be tuned to a particular data set.
In the interestof simplicity, we have not done this.scale classification task,13 we nevertheless foundMC08?s pruning mechanisms useful for removingweak feature-word associations.
Due to the ag-gressive pruning of feature-class associations, ourmodel only has ?1.9M parameters out of a potential1, 600, 000 ?
200, 000 = 320 billion (the number offeatures times the number of WordNet 3.0 senses).4.3 Individual System ResultsTo integrate the predictions of the classifier into theEAW task, we looked up all senses for each lemma-POS pairing, backing off to looking up the wordsthemselves by the same POS, and finally resortingto splitting hyphenated words and rejoining multi-word units (as marked up in the EAW test set).
Be-ing high precision, the classifier does not return avalid answer for every lemma, so we report resultswith and without backing off to the most frequentsense baseline to fill in these gaps.Individual system scores are listed in Table 4.
Theclassifier on its own returns very few answers (with acoverage ?
as distinct from recall ?
of only 10.4%of the test set items).
Although the classifier-onlyperformance does not have broad enough coveragefor stand-alone use, its predictions are nonethelessuseful in combination with the baseline.
Further, weexpect coverage to grow when trained over a largercorpus (such as the very large web-extracted corpusof Agirre et al (2004), which this learning methodis well suited for).5 Wikipedia for Word SenseDisambiguationWikipedia, an online, user-created encyclopedia,can be considered a collection of articles which linkto each other.
While much information exists withinthe textual content of Wikipedia that may assist inWSD, the approach presented here instead uses thearticle names and link structure within Wikipedia tofind articles which are most related to a WordNetsense or context.
We use the Green method to find arelatedness metric for articles from Wikipedia14 (Ol-13Large-scale data sets are available, but this does not changethe level of polysemy in WordNet, which is not in the thousandsfor any given lemma.14Computations were performed using a January 3rd 2008download of the English Wikipedia.14Back-off Precision Recall Prec.
(n-best) Rec.
(n-best)Y??
0.592 0.589 0.594 0.589N?
0.622 0.065 0.694 0.070Table 4: Precision and recall of sparse category index classifier ?
both ?soft?
scores of standard Senseval script andscores where any correct answer in list returned by the classifier is counted as a correct answer (?n-best?).
?Back-off?signals whether the system backs off to the most frequent sense baseline.livier and Senellart, 2007) based on each sense orcontext of interest.Advantages of this method over alternative meth-ods that attempt to incorporate Wikipedia into WSDis that our system is unsupervised and that no man-ual mapping needs to take place between WordNetand Wikipedia.
Mihalcea (2007) demonstrates thatmanual mappings can be created for a small num-ber of words with relative ease, but for a very largenumber of words the effort involved in mappingwould approach presented involves no be consider-able.
The approach presented here involves no map-ping between WordNet and Wikipedia but human ef-fort in mapping between WordNet and Wikipedia,but instead initializes the Green method with a vec-tor based only on the article names (as described inSection 5.2).5.1 Green MethodThe Green method (Ollivier and Senellart, 2007) isused to determine the importance of one node in adirected graph with respect to other nodes.15 In thecontext of Wikipedia the method finds the articleswhich are most likely to be frequented if a randomwalk were used to traverse the articles, starting witha specific article and returning to that article if therandom walk either strays too far off topic or to anarticle which is generally popular even without thecontext of the initial article.
One of the features ofthe Green method is that it does not simply repro-duce the global PageRank (Brin and Page, 1998),instead determining the related pages nearby due torelevance to the initial node.The probability that the random walker ofWikipedia will transfer to an article is defined as auniform distribution over the outlinks of the pagewhere the random walker is currently located.
Asan approximation to the method described by Ol-15In subsequent sections we give a high-level description ofusing the Green method with Wikipedia, however see Ollivierand Senellart (2007) for a much more detailed explanation.livier and Senellart (2007), we create a subgraph ofWikipedia for every computation, comprised of thearticles within a distance of 2 outlink traversals fromthe initial articles.
Since Wikipedia is very highlyconnected, this constructed subgraph still containsa large number of articles and performance of theGreen method on this subgraph is similar to that onthe whole connectivity graph.5.2 Green Method for ContextsTo use the Green method to find Wikipedia arti-cles which correspond to a given word to be dis-ambiguated, articles which may discuss that wordand the context surrounding that word are found inWikipedia as an initial set of locations for the ran-dom walker to start.
This is done by looking for theword itself as the name of an article.
If there is notan article whose name corresponds to the word inquestion, then articles with the word as a substringof the article name are found.Since the goal of WSD is to choose the best wordsense within the context of other words, we use agiven word?s context to select a set of Wikipedia ar-ticles which may discuss the content of the word inquestion.
The expectation is that the context wordswill aid in disambiguation and that the context wordswill together be associated with an appropriate senseof the word being disambiguated.
For this methodwe defined a word?s context as the word itself, thecontent words in the sentence the word occurs in,and those occurring in the sentences before and af-ter that sentence.5.3 Green Method for SensesEvery sense of a word to be disambiguated alsoneeds to be represented as corresponding articlesin Wikipedia before using the Green method.
Thewords that we search for in the titles of Wikipediaarticles include the word itself, and, for every sense,the content words of the sense?s WordNet gloss, aswell as the content of the sense?s hypernym gloss15and the synonyms of the hypernym.
Exploring thisparticular aspect of this module ?
which informa-tion about a sense to extract before using the GreenMethod ?
is a point for further exploration.5.4 Interpreting ProjectionsThe Green method as described by Ollivier andSenellart (2007) uses, as the initial set of articles,the vector containing only one article: that articlefor which related articles are being searched.
Weuse as the initial set of articles the collection of ar-ticles in Wikipedia corresponding to either the con-text for the word to be disambiguated or the sense ofa word.
The random walker is modeled as startingin any of the articles in this set with uniform proba-bility.
Within the context of the Green method, thismeans that this initial set of articles corresponds towhat would be linked to from a new Wikipedia arti-cle about the sense or context.
Each of the contentwords in this new article (which is not in Wikipedia)would link to one of the articles in the set found bythe methods described above.
In this way the resultsof the Green method computation can be interpretedas a relatedness metric for the sense or context itselfand the articles which are in Wikipedia.5.5 AnalysisThe process of finding the sense of a word to be dis-ambiguated is as follows: the vector output from theGreen method (a relatedness measure between theinitial seed and each article in Wikipedia) for thecontext of the word is compared against the vectoroutput from using the Green method on each sensethat the word could have.
The comparison is doneusing the cosine of the angle between the two vec-tors.To determine for which instances in S???????
thismethod may perform well, an analysis was per-formed on a small development set (15 sentences)from SemCor.
A simple heuristic was formulated,selecting the sense with the nearest Green methodoutput to the sentence?s Green method output whenthe ratio between the first and second highest rankedsenses?
cosine angle scores was above a threshold.Applying this heuristic to the EAW task yieldedan expectedly low recall of 11% but a precision of81% on all the words that this heuristic could apply,but only a precision of 25% (recall 0.5%) for non-monosemous words (which were the desired targetsMFS Rerank WikiMFS ?
94% 97%Rerank 23% ?
99%Wiki 45% 98% ?Table 5: Complementarity between modulesof the method).
Of 37 instances where this methoddiffers from the MFS baseline in the EAW task, 8 in-stances are correctly disambiguated by this module.6 ResultsAlthough the individual systems have fairly low re-call, we can calculate pairwise complementarity be-tween systems si and s j by evaluating(1 ?
|wrong in si and s j||wrong in si|)The results, presented in Table 5, indicate that thesystems complement each other well, and suggestthat a combination system could have a higher per-formance than the individual systems.We investigate a number of techniques to combinethe results ?
while the integration of the lemma / partof speech refinement is done by all modules as a pre-processing step, the method of combination of theresulting modules is less clear.
As shown in Florianet al (2002), a simple voting mechanism achievescomparable performance to a stacking mechanism.We present our results in Table 6, DT gives the re-sult of a 10-fold cross-validation of WEKA stackeddecision trees and nearest neighbours built from theindividual system results (Witten and Frank, 2000).Very few decisions are changed with the votingmethod of combination, and the overall result doesnot outperform the best MFS baseline (presented inthe table as ?All MFS?).
This combination methodmay be more useful with a greater number of sys-tems being combined ?
our system only combinesthree systems (thus only one non-MFS system has tosuggest the MFS for this to be selected), and backsoff to the MFS sense in case all three disagree.
Thedegree of complementarity between the Wiki systemand the MFS system indicates that these will over-ride the Rerank system in many cases.Better results are seen with the simple stackingresult: in this case, systems are ordered and thus16System Precision Recall F-measureAll MFS 58.6% 58.6% 58.6%Voting 58.6% 58.6% 58.6%Stacking 58.9% 58.9% 58.9%Stacked DT/NN 58.7% 58.7% 58.7%Table 6: Resulting refined system (forced-choice)are not being subjected to overriding by other MFSskewed systems.7 ConclusionWe have presented a refinement of the most fre-quent sense baseline system, which incorporates anumber of novel approaches to word sense disam-biguation methods.
We demonstrate the need foraccurate lemmatization and part of speech tagging,showing that that is probably the area where thebiggest boost in performance can currently be ob-tained.
We would also argue that examining the ab-solute performance in a task where the baseline is soexceedingly variable (ourselves, we have found thebaseline to be as low as 56% with restricted lemmabackoff, 58.4% with a fairly sophisticated lemma /PoS module, against published baselines of 61.5%in McCarthy et al, 62.5% reported in Snyder, or theupper bound baseline of 66% using correct lemmasand parts of speech), the performance difference be-tween the baseline used and the resulting system isinteresting in itself.AcknowledgmentsWe would like to thank DJ Hovermale for his inputthroughout this project.ReferencesAgirre, E., , and de Lacalle Lekuona, O. L. (2004).Publicly Available Topic Signatures for all Word-Net Nominal Senses.
In Proceedings of the 4th In-ternational Conference on Languages Resourcesand Evaluations (LREC), Lisbon, Portugal.Baayen, H., Piepenbrock, R., and Gulikers, L.(1995).
The CELEX lexical database (release 2).CD-ROM.
Centre for Lexical Information, MaxPlanck Institute for Psycholinguistics, Nijmegen;Linguistic Data Consortium, University of Penn-sylvania.Brill, E. (1992).
A simple rule-based part of speechtagger.
In Proceedings of the Third Conferenceon Applied Natural Language Processing, pages152?155, Trento, Italy.Brin, S. and Page, L. (1998).
The anatomy of a large-scale hypertextual web search engine.
In Com-puter Networks and ISDN Systems, pages 107?117.Briscoe, E., Carroll, J., and Watson, R. (2006).
Thesecond release of the RASP system.
In Proceed-ings of the COLING/ACL 2006 Interactive Pre-sentation Sessions, Sydney, Australia.Elworthy, D. (1994).
Does Baum-Welch re-estimation help taggers?
In Proceedings of the4th ACL Conference on Applied NLP, pages 53?58, Stuttgart, Germany.Florian, R., Cucerzan, S., Schafer, C., andYarowsky, D. (2002).
Combining classifiers forword sense disambiguation.
Journal of NaturalLanguage Engineering, 8(4):327?342.Hala?csy, P., Kornai, A., and Oravecz, C. (2007).HunPos ?
an open source trigram tagger.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics CompanionVolume Proceedings of the Demo and Poster Ses-sions, pages 209?212, Prague, Czech Republic.Association for Computational Linguistics.Korhonen, A., Krymolovski, Y., and Briscoe, T.(2006).
A large subcategorization lexicon fornatural language processing applications.
InProceedings of the 5th international conferenceon Language Resources and Evaluation, pages1015?1020.Korhonen, A. and Krymolowski, Y.
(2002).
On therobustness of entropy-based similarity measuresin evaluation of subcategorization acquisition sys-tems.
In Proceedings of the 6th Conference onNatural Language Learning, pages 91?97.Lin, D. (1998).
Automatic retrieval and clusteringof similar words.
In Proceedings of the COLING-ACL?98, pages 768?773.Madani, O. and Connor, M. (2008).
Large-Scale17Many-Class Learning.
In Proceedins of the SIAMConference on Data Mining (SDM-08).McCarthy, D., Koeling, R., Weeds, J., and Carroll,J.
(2007).
Unsupervised acquisition of predom-inant word senses.
Computational Linguistics,33(4):553?590.Mihalcea, R. (2007).
Using Wikipedia for automaticword sense disambiguation.
In Human LanguageTechnologies 2007: The Conferece of the NorthAmeric an Chapter of the Association for Compu-tational Linguistics, Rochester, New York.Miller, G., Beckwith, R., Felbaum, C., Gross, D.,and Miller, K. (1990).
Introduction to WordNet:An on-line lexical database.
Journal of Lexicog-raphy, 3(4):235?244.Miller, G., Leacock, C., Ranee, T., and Bunker, R.(1993).
A semantic concordance.
In Proceedingsof the 3rd DARPA Workshop on Human LanguageTechnology, pages 232?235.Ollivier, Y. and Senellart, P. (2007).
Finding relatedpages using Green measures: An illustration withWikipedia.
In Association for the Advancementof Artificial Intelligence Conference on ArtificialIntelligence (AAAI 2007).Pedersen, T., Patwardhan, S., and Michelizzi, J.(2004).
Wordnet::similarity - measuring the re-latedness of concepts.
In Proceedings of FifthAnnual Meeting of the North American Chapterof the Association for Computational Linguistics,pages 38?41.Schmidt, H. (1994).
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofthe International Conference on New Methods inLanguage Processing, pages 44?49.Snyder, B. and Palmer, M. (2004).
The english all-words task.
In Mihalcea, R. and Chklowski, T.,editors, Proceedings of SENSEVAL-3: Third In-ternational Workshop on Evaluating Word SenseDisambiguating Systems, pages 41?43.Witten, I. H. and Frank, E. (2000).
Data min-ing: Practical Machine Learning Tools and Tech-niques with Java Implementations, chapter 8.Morgan Kaufmann Publishers.XTAG Research Group (2001).
A lexicalized treeadjoining grammar for English.
Technical ReportIRCS-01-03, IRCS, University of Pennsylvania.Yarowsky, D. (1993).
One Sense Per Collocation.
InProceedings of the Human Language TechnologyConference, Princeton, NJ, USA.Yngve, V. H. (1955).
Syntax and the problem ofmultiple meaning.
In Locke, W. N. and Booth,A.
D., editors, Machine translation of languages,pages 208?226.
John Wiley and Sons, New York.18
