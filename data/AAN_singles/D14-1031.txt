Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 244?254,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Cognitive Model of Semantic Network LearningAida Nematzadeh, Afsaneh Fazly, and Suzanne StevensonDepartment of Computer ScienceUniversity of Toronto{aida,afsaneh,suzanne}@cs.toronto.eduAbstractChild semantic development includeslearning the meaning of words as well asthe semantic relations among words.
Apresumed outcome of semantic develop-ment is the formation of a semantic net-work that reflects this knowledge.
Wepresent an algorithm for simultaneouslylearning word meanings and graduallygrowing a semantic network, which ad-heres to the cognitive plausibility require-ments of incrementality and limited com-putations.
We demonstrate that the seman-tic connections among words in additionto their context is necessary in forming asemantic network that resembles an adult?ssemantic knowledge.1 IntroductionChild semantic development includes the acquisi-tion of word-to-concept mappings (part of wordlearning), and the formation of semantic connec-tions among words/concepts.
There is consid-erable evidence that understanding the semanticproperties of words improves child vocabulary ac-quisition.
In particular, children are sensitive tocommonalities of semantic categories, and thisabstract knowledge facilitates subsequent wordlearning (Jones et al., 1991; Colunga and Smith,2005).
Furthermore, representation of semanticknowledge is significant as it impacts how wordmeanings are stored in, searched for, and retrievedfrom memory (Steyvers and Tenenbaum, 2005;Griffiths et al., 2007).Semantic knowledge is often represented as agraph (a semantic network) in which nodes cor-respond to words/concepts1, and edges specify1Here we assume that the nodes of a semantic networkare word forms and its edges are determined by the semanticfeatures of those words.the semantic relations (Collins and Loftus, 1975;Steyvers and Tenenbaum, 2005).
Steyvers andTenenbaum (2005) demonstrated that a seman-tic network that encodes adult-level knowledge ofwords exhibits a small-world and scale-free struc-ture.
That is, it is an overall sparse network withhighly-connected local sub-networks, where thesesub-networks are connected through high-degreehubs (nodes with many neighbours).Much experimental research has investigatedthe underlying mechanisms of vocabulary learn-ing and characteristics of semantic knowledge(Quine, 1960; Bloom, 1973; Carey and Bartlett,1978; Gleitman, 1990; Samuelson and Smith,1999; Jones et al., 1991; Jones and Smith,2005).
However, existing computational modelsfocus on certain aspects of semantic acquisition:Some researchers develop computational modelsof word learning without considering the acqui-sition of semantic connections that hold amongwords, or how this semantic knowledge is struc-tured (Siskind, 1996; Regier, 2005; Yu and Bal-lard, 2007; Frank et al., 2009; Fazly et al., 2010).Another line of work is to model formation ofsemantic categories but this work does not takeinto account how word meanings/concepts are ac-quired (Anderson and Matessa, 1992; Griffiths etal., 2007; Fountain and Lapata, 2011).Our goal in this work is to provide a cognitively-plausible and unified account for both acquiringand representing semantic knowledge.
The re-quirements for cognitive plausibility enforce someconstraints on a model to ensure that it is compa-rable with the cognitive process it is formulating(Poibeau et al., 2013).
As we model semantic ac-quisition, the first requirement is incrementality,which means that the model learns gradually asit processes the input.
Also, there is a limit onthe number of computations the model performsat each step.In this paper, we present an algorithm for si-244multaneously learning word meanings and grow-ing a semantic network, which adheres to the cog-nitive plausibility requirements of incrementalityand limited computations.
We examine networkscreated by our model under various conditions,and explore what is required to obtain a structurethat has appropriate semantic connections and hasa small-world and scale-free structure.2 Related WorkModels of Word Learning.
Given a word learn-ing scenario, there are potentially many possiblemappings between words in a sentence and theirmeanings (real-world referents), from which onlysome mappings are correct (the mapping prob-lem).
One of the most dominant mechanismsproposed for vocabulary acquisition is cross-situational learning: people learn word mean-ings by recognizing and tracking statistical reg-ularities among the contexts of a word?s usageacross various situations, enabling them to nar-row in on the meaning of a word that holds acrossits usages (Siskind, 1996; Yu and Smith, 2007;Smith and Yu, 2008).
A number of computa-tional models attempt to solve the mapping prob-lem by implementing this mechanism, and havesuccessfully replicated different patterns observedin child word learning (Siskind, 1996; Yu and Bal-lard, 2007; Fazly et al., 2010).
These models haveprovided insight about underlying mechanisms ofword learning, but none of them consider the se-mantic relations that hold among words, or howthe semantic knowledge is structured.
Recently,we have investigated properties of the semanticstructure of the resulting (final) acquired knowl-edge of such a learner (Nematzadeh et al., 2014).However, that work did not address how suchstructural knowledge might develop and evolve in-crementally within the learning model.Models of Categorization.
Computational mod-els of categorization focus on the problem of form-ing semantic clusters given a defined set of fea-tures for words (Anderson and Matessa, 1992;Griffiths et al., 2007; Sanborn et al., 2010).
An-derson and Matessa (1992) note that a cognitivelyplausible categorization algorithm needs to be in-cremental and only keep track of one potentialpartitioning; they propose a Bayesian framework(the Rational Model of Categorization or RMC)that specifies the joint distribution on features andcategory labels, and allows an unbounded numberof clusters.
Sanborn et al.
(2010) examine differ-ent categorization models based on RMC.
In par-ticular, they compare the performance of the ap-proximation algorithm of Anderson and Matessa(1992) (local MAP) with two other approximationalgorithms (Gibbs Sampling and Particle Filters)in various human categorization paradigms.
San-born et al.
(2010) find that in most of the simula-tions the local MAP algorithm performs as well asthe two other algorithms in matching human be-havior.The Representation of Semantic Knowledge.There is limited work on computational modelsof semantic acquisition that examine the represen-tation of the semantic knowledge.
Steyvers andTenenbaum (2005) propose an algorithm for build-ing a network with small-world and scale-freestructure.
The algorithm starts with a small com-plete graph, incrementally adds new nodes to thegraph, and for each new node uses a probabilisticmechanism for selecting a subset of current nodesto connect to.
However, their approach does notaddress the problem of learning word meanings orthe semantic connections among them.
Fountainand Lapata (2011) propose an algorithm for learn-ing categories that also creates a semantic networkby comparing all the possible word pairs.
How-ever, they too do not address the word learningproblem, and do not investigate the structure of thelearned semantic network to see whether it has theproperties observed in adult knowledge.3 The Incremental Network ModelWe propose here a model that unifies the incre-mental acquisition of word meanings and forma-tion of a semantic network structure that reflectsthe similarities among those meanings.
We usean existing model to learn the meanings of words(Section 3.1), and use those incrementally devel-oping meanings as the input to the algorithm pro-posed here for gradually growing a semantic net-work (Section 3.2).3.1 The Word LearnerWe use the model of Fazly et al.
(2010); this learn-ing algorithm is incremental and involves limitedcalculations, thus satisfying basic cognitive plausi-bility requirements.
A naturalistic language learn-ing scenario consists of linguistic data in the con-text of non-linguistic data, such as the objects,245Utterance: {let, find, a, picture, to, color }Scene: {LET, PRONOUN, HAS POSSESSION, CAUSE,ARTIFACT, WHOLE, CHANGE, .
.
.
}Table 1: A sample utterance-scene pair.events, and social interactions that a child per-ceives.
This kind of input is modeled here asa pair of an utterance (the words a child hears)and a scene (the semantic features representing themeaning of those words), as shown in Table 1 (anddescribed in more detail in Section 5.1).
The wordlearner is an instance of cross-situational learn-ing applied to a sequence of such input pairs: foreach pair of a word w and a semantic feature f ,the model incrementally learns P (f |w) from co-occurrences of w and f across all the utterance-scene pairs.For each word, the probability distributionover all semantic features, P (.|w), represents theword?s meaning.
The estimation of P (.|w) ismade possible by introducing a set of latent vari-ables, alignments, that correspond to the possiblemappings between words and features in a givenutterance?scene pair.
The learning problem is thento find the mappings that best explain the data,which is solved by using an incremental versionof the expectation?maximization (EM) algorithm(Neal and Hinton, 1998).
We skip the details ofthe derivations and only report the resulting for-mulas.The model processes one utterance-scene pair ata time.
For the input pair processed at time t, firstthe probability of each possible alignment (align-ment probability) is calculated as:2P (aij|u, fi) =Pt?1(fi|wj)?w??uPt?1(fi|w?
)(1)where u is the utterance, and aijis the alignmentvariable specifying the word wjthat is mappedto the feature fi.
Pt?1(fi|wj) is taken from themodel?s current learned meaning of word wj.
Ini-tially, P0(fi|wj) is uniformly distributed.
Aftercalculating the alignment probabilities, the learnedmeanings are updated as:Pt(fi|wj) =?u?UtP (aij|u, fi)?f?
?M?u?UtP (aij|u, f?
)(2)where Utis the set of utterances processed so far,andM is the set of features that the model has ob-served.
Note that for each w?f pair, the value ofthe summations in this formula can be incremen-tally updated after processing any utterance that2This corresponds to the expectation step of EM.containsw; the summation does not have to be cal-culated at every step.3.2 Growing a Semantic NetworkIn our extended model, as we learn words incre-mentally (as above), we also structure those wordsinto a semantic network based on the (partially)learned meanings.
At any given point in time, thenetwork will include as its nodes all the word typesthe word learner has been exposed to.
Weightededges (capturing semantic distance) will connectthose pairs of word types whose learned meaningsat that point are sufficiently semantically similar(according to a threshold).
Since the probabilis-tic meaning of a word is adjusted each time it isobserved, a word may either lose or gain connec-tions in the network after each input is processed.Thus, to incrementally develop the network, ateach time step, our algorithm must both examineexisting connections (to see which edges should beremoved) and consider potential new connections(to see which edges should be added).A simple approach to achieve this is to examinethe current semantic similarity between a word win the input and all the current words in the net-work, and include edges between only those wordpairs that are sufficiently similar.
However, com-paring w to all the words in the network each timeit is observed is computationally intensive (and notcognitively plausible).We present an approach for incrementally grow-ing a semantic network that limits the computa-tions when processing each input word w; see Al-gorithm 1.
After the meaning of w is updated, wefirst check all the words that w is currently (di-rectly) connected to, to see if any of those edgesneed to be removed, or have their weight adjusted.Next, to look for new connections forw, the idea isto select only a small subset of words, S , to whichw will be compared.
The challenge then is to se-lect S in a way that will yield a network whose se-mantic structure reasonably approximates the net-work that would result from full knowledge ofcomparing w to all the words.Previous work has suggested picking ?impor-tant?
words (e.g., high-degree words) indepen-dently of the target word w ?
assuming thesemight be words for which a learner might needto understand their relationship to w in the future(Steyvers and Tenenbaum, 2005).
Our proposalis instead to consider for S those words that are246Algorithm 1 Growing a network after each in-put u.for all w in u doupdate P (.|w) using Eqn.
(2)update current connections of wselect S(w), a subset of words in the networkfor all w?in S(w) doif w and w?are sufficiently similar thenconnect w and w?with an edgeend ifend forend forlikely to be similar to w. That is, since the net-work only needs to connect similar words to w, ifwe can guess what (some of) those words are, thenwe will do best at approximating the situation ofcomparing w to all words.The question now is how to find semanticallysimilar words to w that are not already connectedto w in the network.
To do so, we incrementallytrack semantic similarity among words usages astheir meanings are developing.
Specifically wecluster word tokens (not types) according to theircurrent word meanings.
Since the probabilisticmeanings of words are continually evolving, in-cremental clusters of word tokens can capture de-veloping similarities among the various usages ofa word type, and be a clue to which words (types)w might be similar to.
In the next section, we de-scribe the Bayesian clustering process we use toidentify potentially similar words.3.3 Semantic Clustering of Word TokensWe use the Bayesian framework of Anderson andMatessa (1992) to form semantic clusters.3Recallthat for each word w, the model learns its mean-ings as a probability distribution over all seman-tic features, P (.|w).
We represent this probabilitydistribution as a vector F whose length is the num-ber of possible semantic features.
Each element ofthe vector holds the value P (f |w) (which is con-tinuous).
Given a word w and its vector F , weneed to calculate the probability that w belongs toeach existing cluster, and also allow for the pos-sibility of it forming a new cluster.
Using Bayesrule we have:P (k|F ) =P (k)P (F |k)?k?P (k?
)P (F |k?
)(3)3The distribution specified by this model is equivalent tothat of a Dirichlet Process Mixture Model (Neal, 2000).where k is a given cluster.
We thus need to calcu-late the prior probability, P (k), and the likelihoodof each cluster, P (F |k).Calculation of Prior.
The prior probability thatword n + 1 is assigned to cluster k is calculatedas:P (k) ={nkn+?nk> 0?n+?nk= 0 (new cluster)(4)where nkis the number of words in cluster k, nis the number of words observed so far, and ?
is aparameter that determines how likely the creationof a new cluster is.
The prior favors larger clusters,and also discourages the creation of new clustersin later stages of learning.Calculation of Likelihood.
To calculate the like-lihood P (F |k) in Eqn.
(3), we assume that the fea-tures are independent:P (F |k) =?fi?FP (fi= v|k) (5)where P (fi= v|k) is the probability that the valueof the feature in dimension i is equal to v giventhe cluster k. To derive P (fi|k), following An-derson and Matessa (1992), we assume that eachfeature given a cluster follows a Gaussian distri-bution with an unknown variance ?2and mean ?.
(In the absence of any prior information about avariable, it is often assumed to have a Gaussiandistribution.)
The mean and variance of this dis-tribution are inferred using Bayesian analysis: Weassume the variance has an inverse ?2prior, where?20is the prior variance and a0is the confidence inthe prior variance:?2?
Inv-?2(a0, ?20) (6)The mean given the variance has a Gaussian dis-tribution with ?0as the prior mean and ?0as theconfidence in the prior mean.?|?
?
N(?0,?2?0) (7)Given the above conjugate priors, P (fi|k) canbe calculated analytically and is a Student?s t dis-tribution with the following parameters:P (fi|k) ?
tai(?i, ?2i(1 +1?i)) (8)?i= ?0+ nk(9)ai= a0+ nk(10)247?i=?0?0+ nk?f?0+ nk(11)?2i=a0?20+ (nk?
1)s2+?0nk?0+nk(?0+?f)2a0+ nk(12)where?f and s2are the sample mean and varianceof the values of fiin k.Note that in the above equations, the mean andvariance of the distribution are simply derived bycombining the sample mean and variance withthe prior mean and variance while considering theconfidence in the prior mean (?0) and variance(a0).
This means that the number of computationsto calculate P (F |K) is limited as w is only com-pared to the ?prototype?
of each cluster, which isrepresented by ?iand ?iof different features.Adding a word w to a cluster.
We add w tothe cluster k with highest posterior probability,P (k|F ), as calculated in Eqn.
(3).4The parame-ters of the selected cluster (k, ?i, ?i, ?i, and aiforeach feature fi) are then updated incrementally.Using the Clusters to Select the Words in S(w).We can now form S(w) in Algorithm 1 by select-ing a given number of words nswhose tokens areprobabilistically chosen from the clusters accord-ing to how likely each cluster k is given w: thenumber of word tokens picked from each k is pro-portional to P (k|F ) and is equal to P (k|F )?ns.4 EvaluationWe evaluate a semantic network in two regards:The semantic connectivity of the network ?
towhat extent the semantically-related words areconnected in the network; and the structure of thenetwork ?
whether it exhibits a small-world andscale-free structure or not.4.1 Evaluating Semantic ConnectivityThe distance between the words in the network in-dicates their semantic similarity: the more similara word pair, the smaller their distance.
For wordpairs that are connected via a path in the network,this distance is the weighted shortest path lengthbetween the two words.
If there is no path be-tween a word pair, their distance is considered tobe?
(which is represented with a large number).We refer to this distance as the ?learned?
semanticsimilarity.4This approach is referred to as local MAP (Sanborn et al.,2010); because of the incremental nature of the algorithm, itmaximizes the current posterior distribution as opposed to the?global?
posterior.To evaluate the semantic connectivity of thelearned network, we compare these learned sim-ilarity scores to ?gold-standard?
similarity scoresthat are calculated using the WordNet similaritymeasure of Wu and Palmer (1994) (also known asthe WUP measure).
We choose this measure sinceit captures the same type of similarity as in ourmodel: words are considered similar if they belongto the same semantic category.
Moreover, thismeasure does not incorporate information aboutother types of similarities, for example, words arenot considered similar if they occur in similar con-texts.
Thus, the scores calculated with this mea-sure are comparable with those of our learned net-work.Given the gold-standard similarity scores foreach word pair, we evaluate the semantic con-nectivity of the network based on two perfor-mance measures: coefficient of correlation andthe median rank of the first five gold-standard as-sociates.
Correlation is a standard way to com-pare two lists of similarity scores (Budanitskyand Hirst, 2006).
We create two lists, one con-taining the gold-standard similarity scores for allword pairs, and the other containing their corre-sponding learned similarity scores.
We calculatethe Spearman?s rank correlation coefficient, ?, be-tween these two lists of similarity scores.
Notethat the learned similarity scores reflect the seman-tic distance among words whereas the WordNetscores reflect semantic closeness.
Thus, a nega-tive correlation is best in our evaluation, where thevalue of -1 corresponds to the maximum correla-tion.Following Griffiths et al.
(2007), we also cal-culate the median learned rank of the first fivegold-standard associates for all words: For eachword w, we first create a ?gold-standard?
asso-ciates list: we sort all other words based on theirgold-standard similarity to w, and pick the fivemost similar words (associates) to w. Similarly,we create a ?learned associate list?
for w by sort-ing all words based on their learned semantic simi-larity tow.
For all words, we find the ranks of theirfirst five gold-standard associates in their learnedassociate list.
For each associate, we calculate themedian of these ranks for all words.
We only re-port the results for the first three gold-standard as-sociates since the pattern of results is similar forthe fourth and fifth associates; we refer to the me-dian rank of first three gold-standard associates as2481st, 2nd, and 3rd.4.2 Evaluating the Structure of the NetworkA network exhibits a small-world structure whenit is characterized by short path length betweenmost nodes and highly-connected neighborhoods(Watts and Strogatz, 1998).
We first explain howthese properties are measured for a graph with Nnodes and E edges.
Then we discuss how theseproperties are used in assessing the small-worldstructure of a graph.5.Short path lengths.
Most of the nodes ofa small-world network are reachable from othernodes via relatively short paths.
For a connectednetwork (i.e., all the node pairs are reachable fromeach other), this can be measured as the averagedistance between all node pairs (Watts and Stro-gatz, 1998).
Since our networks are not connected,we instead measure this property using the medianof the distances (dmedian) between all node pairs(Robins et al., 2005), which is well-defined evenwhen some node pairs have a distance of?.Highly-connected neighborhoods.
The neigh-borhood of a node n in a graph consists of n andall of the nodes that are connected to it.
A neigh-borhood is maximally connected if it forms a com-plete graph ?i.e., there is an edge between allnode pairs.
Thus, the maximum number of edgesin the neighborhood of n is kn(kn?
1)/2, whereknis the number of neighbors.
A standard metricfor measuring the connectedness of neighbors ofa node n is called the local clustering coefficient(C) (Watts and Strogatz, 1998), which calculatesthe ratio of edges in the neighborhood of n (En)to the maximum number of edges possible for thatneighborhood:C =Enkn(kn?
1)/2(13)The local clustering coefficient C ranges between0 and 1.
To estimate the connectedness of allneighborhoods in a network, we take the averageof C over all nodes, i.e., Cavg.Small-world structure.
A graph exhibits asmall-world structure if dmedianis relatively smalland Cavgis relatively high.
To assess this fora graph g, these values are typically comparedto those of a random graph with the same num-ber of nodes and edges as g (Watts and Strogatz,5We take the description of these measures from Ne-matzadeh et al.
(2014)1998; Humphries and Gurney, 2008).
The ran-dom graph is generated by randomly rearrangingthe edges of the network under consideration (Er-dos and R?enyi, 1960).
Because any pair of nodesis equally likely to be connected as any other, themedian of distances between nodes is expected tobe low for a random graph.
In a small-world net-work, this value dmedianis expected to be as smallas that of a random graph: even though the randomgraph has edges more uniformly distributed, thesmall-world network has many locally-connectedcomponents which are connected via hubs.
On theother hand, Cavgis expected to be much higherin a small-world network compared to its corre-sponding random graph, because the edges of arandom graph typically do not fall into clustersforming highly connected neighborhoods.Given these two properties, the ?small-worldness?
of a graph g is measured as follows(Humphries and Gurney, 2008):?g=Cavg(g)Cavg(random)dmedian(g)dmedian(random)(14)where random is the random graph correspond-ing to g. In a small-world network, it is ex-pected that Cavg(g)  Cavg(random) anddmedian(g) ?
dmedian(random), and thus ?g>1.Note that Steyvers and Tenenbaum (2005) madethe empirical observation that small-world net-works of semantic knowledge had a single con-nected component that contained the majority ofnodes in the network.
Thus, in addition to ?g,we also measure the relative size of a network?slargest connected component having size Nlcc:sizelcc=NlccN(15)Scale-free structure.
A scale-free network hasa relatively small number of high-degree nodesthat have a large number of connections to othernodes, while most of its nodes have a small de-gree, as they are only connected to a few nodes.Thus, if a network has a scale-free structure, its de-gree distribution (i.e., the probability distributionof degrees over the whole network) will follow apower-law distribution (which is said to be ?scale-free?).
We evaluate this property of a network byplotting its degree distribution in the logarithmicscale, which (if a power-law distribution) shouldappear as a straight line.
None of our networks ex-249hibit a scale-free structure; thus, we do not reportthe results of this evaluation, and leave it to futurework for further investigation.5 Experimental Set-up5.1 Input RepresentationRecall that the input to the model consists of asequence of utterance?scene pairs intended to re-flect the linguistic data a child is exposed to, alongwith the associated meaning a child might grasp.As in much previous work (Yu and Ballard, 2007;Fazly et al., 2010), we take child-directed utter-ances from the CHILDES database (MacWhinney,2000) in order to have naturalistic data.
In partic-ular, we use the Manchester corpus (Theakston etal., 2001), which consists of transcripts of conver-sations with 12 British children between the agesof 1; 8 and 3; 0.
We represent each utterance asa bag of lemmatized words (see Utterance in Ta-ble 1).For the scene representation, we have no largecorpus to draw on that encodes the semantic por-tion of language acquisition data.6We thus auto-matically generate the semantics associated withan utterance, using a scheme first introduced inFazly et al.
(2010).
The idea is to first create aninput generation lexicon that provides a mappingbetween all the words in the input data and theirassociated meanings.
A scene is then representedas a set that contains the meanings of all the wordsin the utterance.
We use the input generation lexi-con of Nematzadeh et al.
(2012) because the wordmeanings reflect information about their semanticcategories, which is crucial to forming the seman-tic clusters as in Section 3.3.In this lexicon, the ?true?
meaning for eachword w is a vector over a set of possible seman-tic features for each part of speech; in the vec-tor, each feature is associated with a score for thatword (see Figure 1).
Depending on the word?s partof speech, the features are extracted from various6Yu and Ballard (2007) created a corpus by hand-codingthe objects and cues that were present in the environment,but that corpus is very small.
Frank et al.
(2013) provide alarger manually annotated corpus (5000 utterances), but it isstill very small for longitudinal simulations of word learn-ing.
(Our corpus contains more than 100,000 utterances.
)Moreover, the corpus of Frank et al.
(2013) is limited be-cause a considerable number of words are not semanticallycoded.
(Only a subset of concrete objects in the environmentare coded.
)apple: { FOOD:1, SOLID:.72, ?
?
?
, PLANT-PART:.22,PHYSICAL-ENTITY:.17, WHOLE:.06, ?
?
?
}Figure 1: Sample true meaning features & their scores forapple from Nematzadeh et al.
(2012).lexical resources such as WordNet7, VerbNet8, andHarm (2002).
The score for each feature is calcu-lated using a measure similar to tf-idf that reflectsthe association of the feature with the word andwith its semantic category: term frequency indi-cates the strength of association of the feature withthe word, and inverse document frequency (wherethe documents are the categories) indicates howinformative a feature is for that category.
The se-mantic categories of nouns (which we focus on inour networks) are given by WordNet lex-names9,a set of 25 general categories of entities.
(We useonly nouns in our semantic networks because thesemantic similarity of words with different partsof speech cannot be compared, since their seman-tic features are drawn from different resources.
)The input generation lexicon is used to generatea scene representation for an utterance as follows:For each word w in the utterance, we probabilisti-cally sample features, in proportion to their score,from the full set of features in its true meaning.The probabilistic sampling allows us to simulatethe noise and uncertainty in the input a child per-ceives by omitting some meaning features fromthe scene.
The scene representation is the unionof all the features sampled for all the words in theutterance (see Scene in Table 1).5.2 MethodsWe experiment with our network-growth methodthat draws on the incremental clustering, and cre-ate ?upper-bound?
and baseline networks for com-parison.
Note that all the networks are createdusing our Algorithm 1 (page 4) to grow networksincrementally, drawing on the learned meanings ofwords and updating their connections on the basisof this evolving knowledge.
The only differencein creating the networks resides in how the com-parison set S(w) is chosen for each target word wthat is being added to the growing network at eachtime step.
We provide more details in the para-graphs below.7http://wordnet.princeton.edu8http://verbs.colorado.edu/?mpalmer/projects/verbnet.html9http://wordnet.princeton.edu/wordnet/man/lexnames.5WN.html250Upper-bound.
Recall that one of our main goalsis to substantially reduce the number of similar-ity comparisons needed to grow a semantic net-work, in contrast to the straightforward method ofcomparing each w to all current words.
At thesame time, we need to understand the impact ofthe increased efficiency on the quality of the re-sulting networks.
We thus need to compare thetarget properties of our networks that are learnedusing a small comparison set S , to those of an?upper-bound?
network that takes into account allthe pair-wise comparisons among words.
We cre-ate this upper-bound network by setting S(w) tocontain all words currently in the network.Baselines.
On the other hand, we need to evalu-ate the (potential) benefit of our cluster-driven se-lection process over a more simplistic approach toselecting S(w).
To do so, we consider three base-lines, each using a different criteria for choosingthe comparison set S(w): The Random baselinechooses the members of this set randomly fromthe set of all observed words.
The Context base-line can be seen as an ?informed?
baseline that at-tempts to incorporate some semantic knowledge:Here, we select words that are in the recent contextprior to w in the input, assuming that such wordsare likely to be semantically related to w. We alsoinclude a third baseline, Random+Context, thatpicks half of the members of S randomly and halfof them from the prior context.Cluster-based Methods.
We report results forthree cluster-based networks that differ in theirchoice of S(w) as follows: The Clusters-only net-work chooses words in S(w) from the set of clus-ters, proportional to the probability of each clus-ter k given word w (as explained in Section 3.3).In order to incorporate different types of semanticinformation in selecting S, we also create a Clus-ters+Context network that picks half of the mem-bers of S from clusters (as above), and half fromthe prior context.
For completeness, we include aClusters+Random network that similarly chooseshalf of words in S from clusters and half randomlyfrom all observed words.We have experimented with several other meth-ods, but they all performed substantially worsethan the baselines, and hence we do not reportthem here.
E.g., we tried picking words in S fromthe best cluster.
We also tried a few methods in-spired by (Steyvers and Tenenbaum, 2005): E.g.,we examined a method where if a member of S(w)was sufficiently similar to w, we added the directneighbors of that word to S. We also tried to grownetworks by choosing the members of S accordingto the degree or frequency of nodes in the network.5.3 Experimental ParametersWe use 20, 000 utterance?scene pairs as our train-ing data.
Recall that we use clustering to helpguide our semantic network growth algorithm.Given the clustering algorithm in Section 3.3, weare interested to find the set of clusters that bestexplain the data.
(Other clustering algorithms canbe used instead of this algorithm.)
We performa search on the parameter space, and select theparameter values that result in the best clustering,based on the number of clusters and their averageF-score.
The value of the clustering parametersare as follows: ?
= 49, ?0= 1.0, a0= 2.0,?0= 0.0, and ?0= 0.05.
Two nouns with fea-ture vectors F1and F2are connected in the net-work if cosine(F1, F2) is greater than or equal to0.6.
(This threshold was selected following em-pirical examination of the similarity values we ob-serve among the ?true?
meaning in our input gen-eration lexicon.)
The weight on the edge that con-nects these nouns specifies their semantic distance,which is calculated as 1?
cosine(F1, F2).Because we aim for a network creation methodthat is cognitively plausible in performing a lim-ited number of word-to-word comparisons, weneed to ensure that all the different methods ofselecting the comparison set S(w) yield roughlysimilar numbers of such comparisons.
Keepingthe size of S constant does not guarantee this,because each method can yield differing num-bers of connections of the target word w to otherwords.
We thus parameterize the size of S foreach method to keep the number of computationssimilar, based on experiments on the developmentdata.
In development work we also found that hav-ing an increasing size of S over time improvedthe results, as more words were compared as theknowledge of learned meanings improved.
Toachieve this, we use a percentage of the wordsin the network as the size of S. In practice, thesetting of this parameter yields a number of com-parisons across all methods that is about 8% ofthe maximum possible word-to-word comparisonsthat would be performed in the naive (computa-tionally intensive) approach.251Note that all the Cluster-based, Random andRandom+Context methods include a random se-lection mechanism; thus, we run each of thesemethods 50 times and report the average ?, me-dian ranks and sizelcc(see Section 4).
For the net-works (out of 50 runs) that exhibit a small-worldstructure (small-worldness greater than one), wereport the average small-worldness.
We also re-port the percentage of runs whose resulting net-work exhibit a small-world structure.6 Experimental Results and DiscussionTable 2 presents our results, including the eval-uation measures explained above, for the Upper-bound, Baseline, and Cluster-based networks cre-ated by the various methods described in Sec-tion 5.2.10Recall that the Upper-bound network is formedfrom examining a word?s similarity to all other(observed) words when it is added to the network.We can see that this network is highly connected(0.85) and has a small-world structure (5.5).
Thereis a statistically significant correlation of the net-work?s similarity measures with the gold standardones (?0.38).
For this Upper-bound structure, themedian ranks of the first three associates are be-tween 31 and 42.
These latter two measures onthe Upper-bound network give an indication of thedifficulty of learning a semantic network whoseknowledge matches gold-standard similarities.Considering the baseline networks, we note thatthe Random network is actually somewhat bet-ter (in connectivity and median ranks) than theContext network that we thought would providea more informed baseline.
Interestingly, the cor-relation value for both networks is no worse thanfor the Upper-bound.
The combination of Ran-dom+Context yields a slightly lower correlation,and no better ranks or connectivity than Random.Note that none of the baseline networks exhibit asmall world structure (?g1 for all three, exceptfor one out of 50 runs for the Random method).Recall that the Random network is not a net-work resulting from randomly connecting wordpairs, but one that incrementally compares eachtarget word with a set of randomly chosen wordswhen considering possible new connections.
Wesuspect that this approach performs reasonablywell because it enables the model to find a broad10All the reported co-efficients of correlation (?)
are statis-tically significant at p < 0.01.range of similar words to the target; this might beeffective especially because the learned meaningsof words are changing over time.Turning to the Cluster-based methods, we seethat indeed some diversity in the comparison setfor a target word might be necessary to goodperformance.
We find that the measures on theClusters-only network are roughly the same as onthe Random one, but when we combine the two inClusters+Random we see an improvement in theranks achieved.
It is possible that the selectionfrom clusters does not have sufficient diversity tofind some of the valid new connections for a word.We note that the best results overall occur withthe Clusters+Context network, which combinestwo approaches to selecting words that have goodpotential to be similar to the target word.
Thecorrelation coefficient for this network is at a re-spectable 0.36, and the median ranks are the sec-ond best of all the network-growth methods.
Im-portantly, this network shows the desired small-world structure in most of the runs (77%), withthe highest connectivity and a small-world mea-sure well over 1.The fact that the Clusters+Context network isbetter overall than the networks of the Clusters-only and Context methods indicates that both clus-ters and context are important in making ?in-formed guesses?
about which words are likelyto be similar to a target word.
Given the smallnumber of similarity comparisons used in our ex-periments (only around 8% of all possible word-to-word comparisons), these observations suggestthat both the linguistic context and the evolvingrelations among word usages (captured by the in-cremental clustering of learned meanings) containinformation crucial to the process of growing a se-mantic network in a cognitively plausible way.7 ConclusionsWe propose a unified model of word learning andsemantic network formation, which creates a net-work of words in which connections reflect struc-tured knowledge of semantic similarity betweenwords.
The model adheres to the cognitive plau-sibility requirements of incrementality and use oflimited computations.
That is, when incremen-tally adding or updating a word?s connections inthe network, the model only looks at a subset ofwords rather than comparing the target word to allthe nodes in the network.
We demonstrate that252Comparing all PairsSemantic Connectivity Small WorldMethod ?
1st2nd3rdsizelcc?g(%)Upper-bound ?0.38 31 41 42 0.85 5.5BaselinesRandom ?0.38 56 76.9 68.9 0.6 5.2 (2)Context ?0.39 97 115 89 0.5 0Random+Context ?0.36 63.3 87.2 79.1 0.6 0 (0)Cluster-based MethodsClusters-only ?0.32 58.6 72.0 71.6 0.7 5.5 (43)Clusters+Context ?0.36 53.9 67.6 64.8 0.7 7.2 (77)Clusters+Random ?0.35 48.1 61.2 58.1 0.7 6.9 (48)Table 2: Connectivity and small-worldness measures for the Upper-bound, Baseline, and Cluster-basednetwork-growth methods; best performances across the Baseline and Cluster-based methods are shownin bold.
?
: co-efficient of correlation between similarities of word pairs in network and in gold-standard;1st, 2nd, 3rd: median ranks of corresponding gold-standard associates given network similarities; sizelcc:proportion of network in the largest connected component; ?g: overall ?small-worldness?, should begreater than 1; %: the percentage of runs whose resulting networks exhibit a small-world structure.
Notethere are 1074 nouns in each network.using the evolving knowledge of semantic con-nections among words as well as their context ofusage enables the model to create a network thatshows the properties of adult semantic knowledge.This suggests that the information in the semanticrelations among words and their context can effi-ciently guide semantic network growth.AcknowledgmentsWe would like to thank Varada Kolhatkar for valu-able discussion and feedback.
We are also gratefulfor the financial support from NSERC of Canada,and University of Toronto.ReferencesJohn R. Anderson and Michael Matessa.
1992.
Ex-plorations of an incremental, bayesian algorithm forcategorization.
Machine Learning, 9(4):275?308.Lois Bloom.
1973.
One word at a time: The use ofsingle word utterances before syntax, volume 154.Mouton The Hague.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating wordnet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.Susan Carey and Elsa Bartlett.
1978.
Acquiring a sin-gle new word.Allan M. Collins and Elizabeth F. Loftus.
1975.
Aspreading-activation theory of semantic processing.Psychological review, 82(6):407.Eliana Colunga and Linda B. Smith.
2005.
From thelexicon to expectations about kinds: A role for asso-ciative learning.
Psychological Review, 112(2):347?382.Paul Erdos and Alfr?ed R?enyi.
1960.
On the evolutionof random graphs.
Publ.
Math.
Inst.
Hungar.
Acad.Sci, 5:17?61.Afsaneh Fazly, Afra Alishahi, and Suzanne Steven-son.
2010.
A probabilistic computational model ofcross-situational word learning.
Cognitive Science,34(6):1017?1063.Trevor Fountain and Mirella Lapata.
2011.
Incremen-tal models of natural language category acquisition.In Proceedings of the 32st Annual Conference of theCognitive Science Society.Michael C. Frank, Noah D. Goodman, and Joshua B.Tenenbaum.
2009.
Using speakers referential inten-tions to model early cross-situational word learning.Psychological Science.Michael C. Frank, Joshua B. Tenenbaum, and AnneFernald.
2013.
Social and discourse contributionsto the determination of reference in cross-situationalword learning.
Language Learning and Develop-ment, 9(1):1?24.Lila Gleitman.
1990.
The structural sources of verbmeanings.
Language Acquisition, 1(1):3?55.Thomas L. Griffiths, Mark Steyvers, and Joshua B.Tenenbaum.
2007.
Topics in semantic representa-tion.
Psychological review, 114(2):211.Michael W. Harm.
2002.
Building large scale dis-tributed semantic feature sets with WordNet.
Tech-nical Report PDP.CNS.02.1, Carnegie Mellon Uni-versity.253Mark D. Humphries and Kevin Gurney.
2008.
Net-work small-world-ness: a quantitative method fordetermining canonical network equivalence.
PLoSOne, 3(4):e0002051.Susan S. Jones and Linda B. Smith.
2005.
Object namelearning and object perception: a deficit in late talk-ers.
J. of Child Language, 32:223?240.Susan S. Jones, Linda B. Smith, and Barbara Landau.1991.
Object properties and knowledge in early lex-ical learning.
Child Development, 62(3):499?516.Brian MacWhinney.
2000.
The CHILDES Project:Tools for Analyzing Talk, volume 2: The Database.Erlbaum, 3rd edition.Radford M. Neal and Geoffrey E. Hinton.
1998.
Aview of the em algorithm that justifies incremental,sparse, and other variants.
In Learning in graphicalmodels, pages 355?368.
Springer.Radford M. Neal.
2000.
Markov chain sampling meth-ods for dirichlet process mixture models.
Journalof computational and graphical statistics, 9(2):249?265.Aida Nematzadeh, Afsaneh Fazly, and SuzanneStevenson.
2012.
Interaction of word learning andsemantic category formation in late talking.
In Proc.of CogSci?12.Aida Nematzadeh, Afsaneh Fazly, and SuzanneStevenson.
2014.
Structural differences in the se-mantic networks of simulated word learners.Thierry Poibeau, Aline Villavicencio, Anna Korhonen,and Afra Alishahi, 2013.
Computational Modelingas a Methodology for Studying Human LanguageLearning.
Springer.Willard Van Orman Quine.
1960.
Word and Object.MIT Press.Terry Regier.
2005.
The emergence of words: Atten-tional learning in form and meaning.
Cognitive Sci-ence, 29:819?865.Garry Robins, Philippa Pattison, and Jodie Woolcock.2005.
Small and other worlds: Global networkstructures from local processes1.
American Journalof Sociology, 110(4):894?936.Larissa K. Samuelson and Linda B. Smith.
1999.
Earlynoun vocabularies: do ontology, category structureand syntax correspond?
Cognition, 73(1):1 ?
33.Adam N. Sanborn, Thomas L. Griffiths, and Daniel J.Navarro.
2010.
Rational approximations to rationalmodels: alternative algorithms for category learning.Jeffery Mark Siskind.
1996.
A computational studyof cross-situational techniques for learning word-to-meaning mappings.
Cognition, 61:39?91.Linda B. Smith and Chen Yu.
2008.
Infants rapidlylearn word-referent mappings via cross-situationalstatistics.
Cognition, 106(3):1558?1568.Mark Steyvers and Joshua B. Tenenbaum.
2005.
Thelarge-scale structure of semantic networks: Statisti-cal analyses and a model of semantic growth.
Cog-nitive science, 29(1):41?78.Anna L. Theakston, Elena V. Lieven, Julian M. Pine,and Caroline F. Rowland.
2001.
The role ofperformance limitations in the acquisition of verb?argument structure: An alternative account.
J. ofChild Language, 28:127?152.Duncan J. Watts and Steven H. Strogatz.
1998.
Col-lective dynamics of small-worldnetworks.
nature,393(6684):440?442.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for ComputationalLinguistics, pages 133?138.
Association for Com-putational Linguistics.Chen Yu and Dana H. Ballard.
2007.
A unifiedmodel of early word learning: Integrating statisticaland social cues.
Neurocomputing, 70(1315):2149?
2165.
Selected papers from the 3rd Interna-tional Conference on Development and Learning(ICDL 2004), Time series prediction competition:the CATS benchmark.Chen Yu and Linda B. Smith.
2007.
Rapid word learn-ing under uncertainty via cross-situational statistics.Psychological Science, 18(5):414?420.254
