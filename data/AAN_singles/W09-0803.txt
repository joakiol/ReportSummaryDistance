Proceedings of the EACL 2009 Workshop on Computational Approaches to Semitic Languages, pages 19?26,Athens, Greece, 31 March, 2009. c?2009 Association for Computational LinguisticsRevisiting multi-tape automata for Semitic morphological analysis andgenerationMans HuldenUniversity of ArizonaDepartment of Linguisticsmhulden@email.arizona.eduAbstractVarious methods have been devised to pro-duce morphological analyzers and gen-erators for Semitic languages, rangingfrom methods based on widely used finite-state technologies to very specific solu-tions designed for a specific languageor problem.
Since the earliest propos-als of how to adopt the elsewhere suc-cessful finite-state methods to root-and-pattern morphologies, the solution of en-coding Semitic grammars using multi-tapeautomata has resurfaced on a regular ba-sis.
Multi-tape automata, however, requirespecific algorithms and reimplementationof finite-state operators across the board,and hence such technology has not beenreadily available to linguists.
This paper,using an actual Arabic grammar as a casestudy, describes an approach to encodingmulti-tape automata on a single tape thatcan be implemented using any standardfinite-automaton toolkit.1 Introduction1.1 Root-and-pattern morphology andfinite-state systemsThe special problems and challenges embodied bySemitic languages have been recognized from theearly days of applying finite-state methods to nat-ural language morphological analysis.
The lan-guage model which finite-state methods have beenmost successful in describing?a model wheremorphemes concatenate in mostly strict linearorder?does not translate congenially to the typeof root-and-pattern morphology found in e.g.
Ara-bic and Hebrew (Kataja and Koskenniemi, 1988;Lavie et al, 1988).In Arabic, as in most Semitic languages, verbshave for a long time been analyzed as consist-ing of three elements: a (most often) triconsonan-tal root, such as ktb (H.H ?
), a vowel patterncontaining grammatical information such as voice(e.g.
the vowel a) and a derivational template,such as CVCVC indicating the class of the verb, allof which are interdigitated to build a stem, suchas katab (I.J?
).1 This stem is in turn subject tomore familiar morphological constructions includ-ing prefixation and suffixation, yielding informa-tion such as number, person, etc, such as kataba( I.J?
), the third person singular masculine perfectform.The difficulty of capturing this interdigitationprocess is not an inherent shortcoming of finite-state automata or transducers per se, but rathera result of the methods that are commonly usedto construct automata.
Regular expressions thatcontain operations such as concatenation, union,intersection, as well as morphotactic descriptionsthrough right-linear grammars offer an unwieldyfunctionality when it comes to interleaving stringswith one another in a regulated way.
But, onecould argue, since large scale morphological ana-lyzers as finite-state automata/transducers have in-deed been built (see e.g.
Beesley (1996, 1998b,a)),the question of how to do it becomes one of con-struction, not feasibility.1.2 Multitape automataOne early approach, suggested by Kay (1987) andlater pursued in different variants by Kiraz (1994,2000) among others, was to, instead of modelingmorphology along the more traditional finite-statetransducer, modeling it with a n-tape automaton,where tapes would carry precisely this interleaving1Following autosegmental analyses, this paper assumesthe model where the vocalization is not merged with the pat-tern, i.e.
we do not list separate patterns for vocalizationssuch as CaCaC as is assumed more traditionally.
Which anal-ysis to choose largely a matter of convenience, and the meth-ods in this paper apply to either one.19that is called for in Semitic interdigitation.
How-ever, large-scale multitape solutions containing themagnitude of information in standard Arabic dic-tionaries such as Wehr (1979) have not been re-ported.To our knowledge, two large-scale morphologi-cal analyzers for Arabic that strive for reasonablecompleteness have been been built: one by Xeroxand one by Tim Buckwalter (Buckwalter, 2004).The Xerox analyzer relies on complex extensionsto the finite-state calculus of one and two-tapeautomata (transducers) as documented in Beesleyand Karttunen (2003), while Buckwalter?s systemis a procedural approach written in Perl which de-composes a word and simultaneously consults lex-ica for constraining the possible decompositions.Also, in a similar vein to Xerox?s Arabic analyzer,Yona and Wintner (2008) report on a large-scalesystem for Hebrew built on transducer technology.Most importantly, none of these very large systemsare built around multi-tape automata even thoughsuch a construction from a linguistic perspectivewould appear to be a fitting choice when dealingwith root-and-pattern morphology.1.3 n-tape space complexityThere is a fundamental space complexity problemwith multi-tape automata, which is that when thenumber of tapes grows, the required joint sym-bol alphabet grows with exponential rapidity un-less special mechanisms are devised to curtail thisgrowth.
This explosion in the number of transi-tions in an n-tape automaton can in many casesbe more severe than the growth in the number ofstates of a complex grammar.To take a simple, though admittedly slightly ar-tificial example: suppose we have a 5-tape au-tomaton, each tape consisting of the same alpha-bet of, say 22 symbols {s1, .
.
.
, s22}.
Now, as-sume we want to restrict the co-occurrence of s1on any combination of tapes, meaning s1 can onlyoccur once on one tape in the same position, i.e.we would be accepting any strings containing asymbol such as s1:s2:s2:s2:s2 or s2:s2:s2:s2:s3but not, s1:s2:s3:s4:s1.
Without further treatmentof the alphabet behavior, this yields a multi-tapeautomaton which has a single state, but 5,056,506transitions?each transition naturally representinga legal combination of symbols on the five tapes.This kind of transition blow-up is not completelyinevitable: of course one can devise many tricksto avoid it, such as adding certain semantics tothe transition notation?in our example by per-haps having a special type of ?failure?
transitionwhich leads to non-acceptance.
For the above ex-ample this would cut down the number of tran-sitions from 5,056,506 to 97,126.
The drawbackwith such methods is that any changes will tendto affect the entire finite-state system one is work-ing with, requiring adaptations in almost every un-derlying algorithm to construct automata.
One isthen unable to leverage the power of existing soft-ware designed for finite-state morphological anal-ysis, but needs to build special-purpose softwarefor whatever multi-tape implementation one has inmind.21.4 The appeal of the multi-tape solutionThe reason multi-tape descriptions of natural lan-guage morphology are appealing lies not onlyin that such solutions seem to be able to han-dle Semitic verbal interdigitation, but also inthat a multi-tape solution allows for a naturalalignment of information regarding segments andtheir grammatical features, something which isoften missing in finite-state-based solutions tomorphological analysis.
In the now-classicalway of constructing morphological analyzers, wehave a transducer that maps a string represent-ing an unanalyzed word form, such as kataba( I.J?)
to a string representing an analyzed one,e.g.
ktb +FormI +Perfect +Act +3P+Masc +Sg.
Such transductions seldom pro-vide grammatical component-wise alignment in-formation telling which parts of the unanalyzedwords contribute to which parts of the grammat-ical information.
Particularly if morphemes signi-fying a grammatical category are discontinuous,this information is difficult to provide naturallyin a finite-automaton based system without manytapes.
A multi-tape solution, on the other hand,2Two anonymous reviewers point out the work by Habashet al (2005) and Habash and Rambow (2006) who report aneffort to analyze Arabic with such a multitape system basedon work by Kiraz (2000, 2001) that relies on custom algo-rithms devised for a multitape alphabet.
Although Habashand Rambow do not discuss the space requirements in theirsystem, it is to be suspected that the number of transitionsgrows quickly using such an method by virtue of the argu-ment given above.
These approaches also use a small numberof tapes (between 3 and 5), and, since the number of tran-sitions can increase exponentially with the number of tapesused, such systems do not on the face of it appear to scalewell to more than a handful of tapes without special precau-tions.20Tinput k a t a b aTroot k t bTform Form ITptrn C V C V CTpaff aTaffp +3P+Masc+SgTvoc a aTvocp +Act.
.
.Table 1: A possible alignment of 8 tapes to captureArabic verbal morphology.can provide this information by virtue of its con-struction.
The above example could in an 8-tapeautomaton encoding be captured as illustrated intable 1, assuming here that Tinput is the input tape,the content of which is provided, and the subse-quent tapes are output tapes where the parse ap-pears.In table 1, we see that the radicals on the roottape are aligned with the input, as is the pattern onthe pattern tape, the suffix -a on the suffix tape,which again is aligned with the parse for the suf-fix on the affix parse tape (affp), and finally thevocalization a is aligned with the input and the pat-tern.
This is very much in tune with both the typeof analyses linguists seem to prefer (McCarthy,1981), and more traditional analyses and lexicog-raphy of root-and-pattern languages such as Ara-bic.In what follows, we will present an alternateencoding for multi-tape automata together withan implementation of an analyzer for Arabic ver-bal morphology.
The encoding simulates a multi-tape automaton using a simple one-tape finite-statemachine and can be implemented using standardtoolkits and algorithms given in the literature.
Theencoding also avoids the abovementioned blow-upproblems related to symbol combinations on mul-tiple tapes.2 NotationWe assume the reader is familiar with the basicnotation regarding finite automata and regular ex-pressions.
We will use the standard operators ofKleene closure (L?
), union (L1 ?
L2), intersec-tion (L1 ?
L2), and assume concatenation when-ever there is no overt operator specified (L1L2).We use the symbol ?
to specify the alphabet, andthe shorthand \a to denote any symbol in the al-phabet except a.
Slight additional notation will beintroduced in the course of elaborating the model.3 EncodingIn our implementation, we have decided to encodethe multi-tape automaton functionality as consist-ing of a single string read by a single-tape automa-ton, where the multiple tapes are all evenly inter-leaved.
The first symbol corresponds to the firstsymbol on tape 1, the second to the first on tape 2,etc.
:T1 ?
?
?.
.
.Tn?1 ?
?
?Tn ?
?
?.
.
.For instance, the two-tape correspondence:T1 aT2 b cwould be encoded as the string ab?c, ?
being a spe-cial symbol used to pad the blanks on a tape tokeep all tapes synchronized.This means that, for example, for an 8-tape rep-resentation, every 8th symbol from the beginningis a symbol representing tape 1.Although this is the final encoding we wish toproduce, we have added one extra temporary fea-ture to facilitate the construction: every symbol onany ?tape?
is always preceded by a symbol indi-cating the tape number drawn from an alphabetT1, .
.
.
, Tn.
These symbols are removed eventu-ally.
That means that during the construction, theabove two-tape example would be represented bythe string T1aT2bT1?T2c.
This simple redundancymechanism will ease the writing of grammars andactually limit the size of intermediate automataduring construction.4 Construction4.1 OverviewWe construct a finite-state n-tape simulation gram-mar in two steps.
Firstly we populate each ?tape?with all grammatically possible strings.
Thatmeans that, for our Arabic example, the root tape21should contain all possible roots we wish to ac-cept, the template tape all the possible templates,etc.
We call this language the Base.
The secondstep is to constrain the co-occurrence of symbolson the individual tapes, i.e.
a consonant on the roottape must be matched by a consonant of the inputtape as well as the symbol C on the pattern tape,etc.
Our grammar then consists of all the permit-ted combinations of tape symbols allowed by a)the Base and b) the Rules.
The resulting lan-guage is simply their intersection, viz.
:Base ?
Rules4.2 Populating the tapesWe have three auxiliary functions, TapeL(X,Y),TapeM(X,Y), and TapeA(X,Y), where the ar-gument X is the tape number, and Y the languagewe with to insert on tape X.3 TapeL(X,Y) cre-ates strings where every symbol from the languageY is preceded by the tape indicator TX and wherethe entire tape is left-aligned, meaning there areno initial blanks on that tape.
TapeM is the samefunction, except words on that tape can be pre-ceded by blanks and succeeded by blanks.
TapeAallows for any alignment of blanks within wordsor to the left or right.
Hence, to illustrate thisbehavior, TapeL(4,C V C V C) will producestrings like:XT4CXT4VXT4CXT4VXT4CYwhere X is any sequence of symbols not contain-ing the symbol T4, and Y any sequence possiblycontaining T4 but where T4 is always followed by?, i.e.
we pad all tapes at the end to allow for syn-chronized strings on other tapes containing morematerial to the right.Now, if, as in our grammar, tape 4 is the tem-plate tape, we would populate that tape by declar-ing the language:TapeM(4,Templates)assuming Templates is the language that ac-cepts all legal template strings, e.g.
CVCVC,CVCCVC, etc.Hence, our complete Base language (continu-ing with the 8-tape example) is:3See the appendix for exact definitions of these functions.TapeL(1,Inputs) ?TapeA(2,Roots) ?TapeL(3,Forms) ?TapeM(4,Templates) ?TapeA(5,Affixes) ?TapeM(6,Parses) ?TapeA(7,Voc) ?TapeL(8,VocParses) ?(T1?T2?T3?T4?T5?T6?T7?T8?
)?This will produce the language where all stringsare multiples of 16 in length.
Every other sym-bol is the TX tape marker symbol and every othersymbol is the actual symbol on that tape (allowingfor the special symbol ?
also to represent blanks ona tape).
Naturally, we will want to define Inputsoccurring on tape 1 as any string containing anycombination of symbols since it represents all pos-sible input words we wish to parse.
Similarly, tape2 will contain all possible roots, etc.
This Baselanguage is subsequently constrained so that sym-bols on different tapes align correctly and are onlyallowed if they represent a legal parse of the wordon the input tape (tape 1).4.3 Constructing the rulesWhen constructing the rules that constrain the co-occurrence of symbols on the various tapes weshall primarily take advantage of the ?
oper-ator first introduced for two-level grammars byKoskenniemi (1983).4 The semantics is as fol-lows.
A statement:X ?
L1 R1, .
.
.
, Ln Rnwhere X and Li, Ri are all regular languagesdefines the regular language where every instanceof a substring drawn from the languageX must besurrounded by some pair Li and Ri to the left andright, respectively.5Indeed, all of our rules will consist exclusivelyof?
statements.To take an example: in order to constrain thetemplate we need two rules that effectively say thatevery C and V symbol occurring in the template4There is a slight, but subtle difference in notation,though: the original two-level?
operator constrained singlesymbols only (such as a:b, which was considered at compile-time a single symbol); here, the argument X refers to anyarbitrary language.5Many finite-state toolkits contain this as a separate op-erator.
See Yli-Jyra?
and Koskenniemi (2004) and Hulden(2008) for how such statements can be converted into regularexpressions and finite automata.22tape must be matched by 1) a consonant on the roottape and 2) a vowel on the input tape.
Because ofour single-tape encoding the first rule translates tothe idea that every T4 C sequence must be directlypreceded by T2 followed by some consonant fol-lowed by T3 and any symbol at all:T4 C ?
T2 Cons T3 ?
(1)and the second one translates to:T4 V ?
T1 Vow T2 ?
T3 ?
(2)assuming that Vow is the language that containsany vowel and Cons the language that containsany consonant.Similarly, we want to constrain the Formsparse tape that contains symbols such as Form I,Form II etc., so that if, for example, Form I oc-curs on that tape, the pattern CVCVC must occur onthe pattern tape.6T3 Form I?
TapeM(4,C V C V C) (3)and likewise for all the other forms.
It should benoted that most constraints are very strictly localto within a few symbols, depending slightly on theordering and function of the tapes.
In (1), for in-stance, which constrains a symbol on tape 4 witha consonant on tape 2, there are only 2 interven-ing symbols, namely that of tape 3.
The orderingof the tapes thus has some bearing on both howsimple the rules are to write, and the size of the re-sulting automaton.
Naturally, tapes that constraineach other are ideally placed in adjacent positionswhenever possible.Of course, some long-distance constraints willbe inevitable.
For example, Form II is generallydescribed as a CVCCVC pattern, where the extraconsonant is a geminate, as in the stem kattab,where the t of the root associates with both C?sin the pattern.
To distinguish this C behaviorfrom that of Form X which is also commonly de-scribed with two adjacent C symbols where, how-ever, there is no such association (as in the stemstaktab) we need to introduce another symbol.6To be more exact, to be able to match and parse bothfully vocalized words such as wadarasat (I ?
PX?
), and un-vocalized ones, such as wdrst ( I?PX?
), we want the patternCVCVC to actually be represented by the regular expressionC (V) C (V) C, i.e.
where the vowels are optional.
Note,however, that the rule that constrains T4 V above only re-quires that the V matches if there indeed is one.
Hence,by declaring vowels in patterns (and vocalizations) to be op-tional, we can always parse any partially, fully, or unvocalizedverb.
Of course, fully unvocalized words will be much moreambiguous and yield more parses.This symbol C2 occurs in Form II, which becomesCVCC2VC.
We then introduce a constraint to theeffect that any C2-symbol must be matched on theinput by a consonant, which is identical to the pre-vious consonant on the input tape.7 These long-distance dependencies can be avoided to some ex-tent by grammar engineering, but so long as theydo not cause a combinatorial explosion in the num-ber of states of the resulting grammar automaton,we have decided to include them for the sake ofclarity.To give an overview of some of the subsequentconstraints that are still necessary, we include herea few descriptions and examples (where the starred(***) tape snippets exemplify illegal configura-tions):?
Every root consonant has a matching conso-nant on the input tapeT1 k a t a b aT2 k t bT1 k a t a b aT2*** d r s?
A vowel in the input which is matched by aV in the pattern, must have a correspondingvocalization vowelT1 k a t a b aT4 C V C V CT7 a aT1 k a t a b aT4 C V C V CT7*** u i?
A position where there is a symbol in the in-put either has a symbol in the pattern tape ora symbol in the affix tape (but not both)T1 k a t a b aT4 C V C V CT5 aT1 k a t a b aT4 C V C V CT5***7The idea to preserve the gemination in the grammar issimilar to the solutions regarding gemination and spreadingof Forms II, V, and IX documented in Beesley (1998b) andHabash and Rambow (2006).234.4 The final automatonAs mentioned above, the symbols {T1, .
.
.
, Tn}are only used during construction of the automa-ton for the convenience of writing the grammar,and shall be removed after intersecting the Baselanguage with the Rules languages.
This is a sim-ple substitution TX ?
, i.e.
the empty string.Hence, the grammar is compiled as:Grammar = h(Base ?
Rules)where h is a homomorphism that replaces TXsymbols with , the empty string.5 Efficiency ConsiderationsBecause the construction method proposed canvery quickly produce automata of considerablesize, there are a few issues to consider when de-signing a grammar this way.
Of primary concernis that since one is constructing deterministic au-tomata, long-distance constraints should be keptto a minimum.
Local constraints, which the ma-jority of grammar rules encode, yield so-called k-testable languages when represented as finite au-tomata, and the state complexity of their inter-section grows additively.
For larger k, however,growth is more rapid which means that, for ex-ample, when one is designing the content of theindividual tapes, care should be taken to ensurethat segments or symbols which are related to eachother preferably align very closely on the tapes.Naturally, this same goal is of linguistic interest aswell and a grammar which does not align gram-matical information with segments in the input islikely not a good grammar.
However, there are acouple of ways in which one can go astray.
Forinstance, in the running example we have pre-sented, one of the parse tapes has included thesymbol +3P +Masc +Sg, aligned with the affixthat represents the grammatical information:.
.
.T5 aT6 +3P+Masc+Sg.
.
.However, if it be the case that what the parsetape reflects is a prefix or a circumfix, as will bethe case with the imperfective, subjunctive andjussive forms, the following alignment would besomewhat inefficient:.
.
.T5 t aT6 +3P+Fem+Sg.
.
.This is because the prefix ta, which appearsearly in the word, is reflected on tape 6 at the endof the word, in effect unnecessarily producing avery long-distance dependency and hence dupli-cates of states in the automaton encoding the in-tervening material.
A more efficient strategy is toplace the parse or annotation tape material as closeas possible to the segments which have a bearingon it, i.e.:.
.
.T5 t aT6 +3P+Fem+Sg.
.
.This alignment can be achieved by a constraintin the grammar to the effect that the first non-blanksymbol on the affix tape is in the same position asthe first non-blank symbol on the affix parse tape.It is also worth noting that our implementationdoes not yet restrict the co-occurrence of roots andforms, i.e.
it will parse any word in any root in thelexicon in any of the forms I-VIII, X.
Adding theserestrictions will presumably produce some growthin the automaton.
However, for the time being wehave also experimented with accepting any trilit-eral root?i.e.
any valid consonantal combination.This has drastically cut the size of the resultingautomaton to only roughly 2,000 states withoutmuch overgeneration in the sense that words willnot incorrectly be matched with the wrong root.The reason for this small footprint when not hav-ing a ?real?
lexicon is fairly obvious?all depen-dencies between the root tape and the pattern tapeand the input tape are instantly resolved in the spanof one ?column?
or 7 symbols.6 Algorithmic additionsNaturally, one can parse words by simply inter-secting TapeL(1, word) ?
Grammar, where24word is the word at hand and printing out all thelegal strings.
Still, this is unwieldy because ofthe intersection operation involved and for fasterlookup speeds one needs to consider an algorith-mic extension that performs this lookup directlyon the Grammar automaton.6.1 Single-tape transductionFor our implementation, we have simply modifiedthe automaton matching algorithm in the toolkitwe have used, foma8 to, instead of matching ev-ery symbol, matching the first symbol as the ?in-put?, then outputting the subsequent n (where nis 7 in our example) legal symbols if the subse-quent input symbols match.
Because the grammaris quite constrained, this produces very little tem-porary ambiguity in the depth-first search traversalof the automaton and transduces an input to theoutput tapes in nearly linear time.7 Future workThe transduction mechanism mentioned aboveworks well and is particularly easy to implementwhen the first ?tape?
is the input tape containingthe word one wants to parse, since one can simplydo a depth-first search until the the next symbolon the input tape (in our running example with 8tapes, that would be 7 symbols forward) and dis-card the paths where the subsequent tape 1 sym-bols do not match, resulting in nearly linear run-ning time.
However, for the generation problem,the solution is less obvious.
If one wanted to sup-ply any of the other tapes with a ready input (suchas form, root, and a combination of grammaticalcategories), and then yield a string on tape 1, theproblem would be more difficult.
Naturally, onecan intersect various TapeX(n, content) languagesagainst the grammar, producing all the possible in-put strings that could have generated such a parse,but this method is rather slow and results only ina few parses per second on our system.
Devis-ing a fast algorithm to achieve this would be desir-able for applications where one wanted to, for in-stance, generate all possible vocalization patternsin a given word, or for IR purposes where onewould automatically apply vocalizations to Arabicwords.8See the appendix.8 ConclusionWe have described a straightforward method bywhich morphological analyzers for languages thatexhibit root-and-pattern morphology can be builtusing standard finite-state methods to simulatemulti-tape automata.
This enables one to takeadvantage of already widely available standardtoolkits designed for construction of single-tapeautomata or finite-state transducers.
The feasibil-ity of the approach has been tested with a limitedimplementation of Arabic verbal morphology thatcontains roughly 2,000 roots, yielding automata ofmanageable size.
With some care in constructionthe method should be readily applicable to largerprojects in Arabic and other languages, in partic-ular to languages that exhibit root-and-pattern ortemplatic morphologies.ReferencesBeesley, K. and Karttunen, L. (2003).
Finite-StateMorphology.
CSLI, Stanford.Beesley, K. R. (1996).
Arabic finite-state analysisand generation.
In COLING ?96.Beesley, K. R. (1998a).
Arabic morphology us-ing only finite-state operations.
In Proceedingsof the Workshop on Computational Approachesto Semitic Languages COLING-ACL, pages 50?57.Beesley, K. R. (1998b).
Consonant spreading inArabic stems.
In ACL, volume 36, pages 117?123.
Association for Computational Linguis-tics.Beeston, A. F. L. (1968).
Written Arabic: An ap-proach to the basic structures.
Cambridge Uni-versity Press, Cambridge.Buckwalter, T. (2004).
Arabic morphological an-alyzer 2.0.
LDC.Habash, N. and Rambow, O.
(2006).
MAGEAD:A morphological analyzer and generator for theArabic dialects.
Proceedings of COLING-ACL2006.Habash, N., Rambow, O., and Kiraz, G. (2005).Morphological analysis and generation for Ara-bic dialects.
Proceedings of the Workshopon Computational Approaches to Semitic Lan-guages (ACL ?05).Hulden, M. (2008).
Regular expressions and pred-icate logic in finite-state language processing.25In Piskorski, J., Watson, B., and Yli-Jyra?, A.,editors, Proceedings of FSMNLP 2008.Kataja, L. and Koskenniemi, K. (1988).
Finite-state description of Semitic morphology: a casestudy of ancient Akkadian.
In COLING ?88,pages 313?315.Kay, M. (1987).
Nonconcatenative finite-statemorphology.
In Proceedings of the third con-ference on European chapter of the Associationfor Computational Linguistics, pages 2?10.
As-sociation for Computational Linguistics.Kiraz, G. A.
(1994).
Multi-tape two-level mor-phology: A case study in Semitic non-linearmorphology.
In COLING ?94, pages 180?186.Kiraz, G. A.
(2000).
Multi-tiered nonlinear mor-phology using multitape finite automata: A casestudy on Syriac and Arabic.
Computational Lin-guistics, 26(1):77?105.Kiraz, G. A.
(2001).
Computational nonlinearmorphology: with emphasis on Semitic lan-guages.
Cambridge University Press, Cam-bridge.Koskenniemi, K. (1983).
Two-level morphology:A general computational model for word-formrecognition and production.
Publication 11,University of Helsinki, Department of GeneralLinguistics, Helsinki.Lavie, A., Itai, A., and Ornan, U.
(1988).
On theapplicability of two level morphology to the in-flection of Hebrew verbs.
In Proceedings ofALLC III, pages 246?260.McCarthy, J. J.
(1981).
A Prosodic Theory of Non-concatenative Morphology.
Linguistic Inquiry,12(3):373?418.van Noord, G. (2000).
FSA 6 Reference Manual.Wehr, H. (1979).
A Dictionary of Modern Writ-ten Arabic.
Spoken Language Services, Inc.,Ithaca, NY.Yli-Jyra?, A. and Koskenniemi, K. (2004).
Compil-ing contextual restrictions on strings into finite-state automata.
The Eindhoven FASTAR DaysProceedings.Yona, S. and Wintner, S. (2008).
A finite-statemorphological grammar of Hebrew.
NaturalLanguage Engineering, 14(2):173?190.9 AppendixThe practical implementation described in the pa-per was done with the freely available (GNU Li-cence) foma finite-state toolkit.9.
However, all ofthe techniques used are available in other toolk-its as well, such as xfst (Beesley and Karttunen,2003), or fsa (van Noord, 2000)), and translationof the notation should be straightforward.The functions for populating the tapes in section4.2, were defined in foma as follows:TapeL(X,Y) =[[Y ?
[[0?\X \X]* [0?X]?
]*]2[X E|\X \X]*]TapeM(X,Y) = [[Y ?
[0?
[\X \X|X E]]*[0?\X \X]* [0?X]?
]*]2 [X E|\X \X]*]TapeA(X,Y) = [[Y ?
[0?\X \X|X E]* 0?X?
]*]2;Here, TapeX is a function of two variables, Xand Y. Transducer composition is denoted by ?,cross-product by ?, the lower projection of a re-lation by L2, and union by |.
Brackets indicategrouping and ?
any symbol.
The notation \X de-notes any single symbol, except X .
The symbol ?here is the special ?blank?
symbol used to pad thetapes and keep them synchronized.9http://foma.sourceforge.net26
