Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 1?10,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsDoes Size Matter?Text and Grammar Revision for Parsing Social Media DataMohammad KhanIndiana UniversityBloomington, IN USAkhanms@indiana.eduMarkus DickinsonIndiana UniversityBloomington, IN USAmd7@indiana.eduSandra Ku?blerIndiana UniversityBloomington, IN USAskuebler@indiana.eduAbstractWe explore improving parsing social mediaand other web data by altering the input data,namely by normalizing web text, and by revis-ing output parses.
We find that text normal-ization improves performance, though spellchecking has more of a mixed impact.
We alsofind that a very simple tree reviser based ongrammar comparisons performs slightly butsignificantly better than the baseline and welloutperforms a machine learning model.
Theresults also demonstrate that, more than thesize of the training data, the goodness of fitof the data has a great impact on the parser.1 Introduction and MotivationParsing data from social media data, as well as otherdata from the web, is notoriously difficult, as parsersare generally trained on news data (Petrov and Mc-Donald, 2012), which is not a good fit for social me-dia data.
The language used in social media does notfollow standard conventions (e.g., containing manysentence fragments), is largely unedited, and tendsto be on different topics than standard NLP technol-ogy is trained for.
At the same time, there is a clearneed to develop even basic NLP technology for avariety of types of social media and contexts (e.g.,Twitter, Facebook, YouTube comments, discussionforums, blogs, etc.).
To perform tasks such as sen-timent analysis (Nakagawa et al 2010) or informa-tion extraction (McClosky et al 2011), it helps toperform tagging and parsing, with an eye towardsproviding a shallow semantic analysis.We advance this line of research by investigatingadapting parsing to social media and other web data.Specifically, we focus on two areas: 1) We comparethe impact of various text normalization techniqueson parsing web data; and 2) we explore parse revi-sion techniques for dependency parsing web data toimprove the fit of the grammar learned by the parser.One of the major problems in processing socialmedia data is the common usage of non-standardterms (e.g., kawaii, a Japanese-borrowed net termfor ?cute?
), ungrammatical and (intentionally) mis-spelled text (e.g., cuttie), emoticons, and short postswith little contextual information, as exemplified in(1).1(1) Awww cuttie little kitten, so Kawaii <3To process such data, with its non-standard words,we first develop techniques for normalizing the text,so as to be able to accommodate the wide range ofrealizations of a given token, e.g., all the differentspellings and intentional misspellings of cute.
Whileprevious research has shown the benefit of text nor-malization (Foster et al 2011; Gadde et al 2011;Foster, 2010), it has not teased apart which partsof the normalization are beneficial under which cir-cumstances.A second problem with parsing social media datais the data situation: parsers can be trained on thestandard training set, the Penn Treebank (Marcuset al 1993), which has a sufficient size for train-ing a statistical parser, but has the distinct down-side of modeling language that is very dissimilar1Taken from: http://www.youtube.com/watch?v=eHSpHCprXLA1from the target.
Or one can train parsers on the En-glish Web Treebank (Bies et al 2012), which cov-ers web language, including social media data, butis rather small.
Our focus on improving parsing forsuch data is on exploring parse revision techniquesfor dependency parsers.
As far as we know, de-spite being efficient and trainable on a small amountof data, parse revision (Henestroza Anguiano andCandito, 2011; Cetinoglu et al 2011; Attardi andDell?Orletta, 2009; Attardi and Ciaramita, 2007)has not been used for web data, or more generallyfor adapting a parser to out-of-domain data; an in-vestigation of its strengths and weaknesses is thusneeded.We describe the data sets used in our experimentsin section 2 and the process of normalization in sec-tion 3 before turning to the main task of parsing insection 4.
Within this section, we discuss our mainparser as well as two different parse revision meth-ods (sections 4.2 and 4.3).
In the evaluation in sec-tion 5, we will find that normalization has a positiveimpact, although spell checking has mixed results,and that a simple tree anomaly detection method(Dickinson and Smith, 2011) outperforms a machinelearning reviser (Attardi and Ciaramita, 2007), espe-cially when integrated with confidence scores fromthe parser itself.
In addition to the machine learnerrequiring a weak baseline parser, some of the maindifferences include the higher recall of the simplemethod at positing revisions and the fact that it de-tects odd structures, which parser confidence canthen sort out as incorrect or not.2 DataFor our experiments, we use two main resources, theWall Street Journal (WSJ) portion of the Penn Tree-bank (PTB) (Marcus et al 1993) and the EnglishWeb Treebank (EWT) (Bies et al 2012).
The twocorpora were converted from PTB constituency treesinto dependency trees using the Stanford depen-dency converter (de Marneffe and Manning, 2008).2The EWT is comprised of approximately 16,000sentences from weblogs, newsgroups, emails, re-views, and question-answers.
Instead of examiningeach group individually, we chose to treat all web2http://nlp.stanford.edu/software/stanford-dependencies.shtml1 <<_ -LRB--LRB-_ 2 punct _ _2 File _ NN NN _ 0 root _ _3 : _ : : _ 2 punct _ _4 220b _ GW GW _ 11 dep _ _5 -_ GW GW _ 11 dep _ _6 dg _ GW GW _ 11 dep _ _7 -_ GW GW _ 11 dep _ _8 Agreement _ GW GW _ 11 dep _ _9 for _ GW GW _ 11 dep _ _10 Recruiting _ GW GW _ 11 dep _ _11 Services.doc _ NN NN _ 2 dep _ _12 >>_ -RRB--RRB-_ 2 punct _ _13 <<_ -LRB--LRB-_ 14 punct _ _14 File _ NN NN _ 2 dep _ _15 : _ : : _ 14 punct _ _16 220a _ GW GW _ 22 dep _ _17 DG _ GW GW _ 22 dep _ _18 -_ GW GW _ 22 dep _ _19 Agreement _ GW GW _ 22 dep _ _20 for _ GW GW _ 22 dep _ _21 Contract _ GW GW _ 22 dep _ _22 Services.DOC _ NN NN _ 14 dep _ _23 >>_ -RRB--RRB-_ 14 punct _ _Figure 1: A sentence with GW POS tags.data equally, pulling from each type of data in thetraining/testing split.Additionally, for our experiments, we deleted the212 sentences from EWT that contain the POS tagsAFX and GW tags.
EWT uses the POS tag AFX forcases where a prefix is written as a separate wordfrom its root, e.g., semi/AFX automatic/JJ.
Suchsegmentation and tagging would interfere with ournormalization process.
The POS tag GW is used forother non-standard words, such as document names.Such ?sentences?
are often difficult to analyze anddo not correspond to phenomena found in the PTB(cf., figure 1).To create training and test sets, we broke the datainto the following sets:?
WSJ training: sections 02-22 (42,009 sen-tences)?
WSJ testing: section 23 (2,416 sentences)?
EWT training: 80% of the data, taking the firstfour out of every five sentences (13,130 sen-tences)?
EWT testing: 20% of the data, taking everyfifth sentence (3,282 sentences)23 Text normalizationPrevious work has shown that accounting for vari-ability in form (e.g., misspellings) on the web, e.g.,by mapping each form to a normalized form (Fos-ter, 2010; Gadde et al 2011) or by delexicaliz-ing the parser to reduce the impact of unknownwords (?vrelid and Skj?rholt, 2012), leads to someparser or tagger improvement.
Foster (2010), forexample, lists adapting the parser?s unknown wordmodel to handle capitalization and misspellings offunction words as a possibility for improvement.Gadde et al(2011) find that a model which positsa corrected sentence and then is POS-tagged?theirtagging after correction (TAC) model?outperformsone which cleans POS tags in a postprocessing step.We follow this line of inquiry by developing textnormalization techniques prior to parsing.3.1 Basic text normalizationMachine learning algorithms and parsers are sensi-tive to the surface form of words, and different formsof a word can mislead the learner/parser.
Our ba-sic text normalization is centered around the ideathat reducing unnecessary variation will lead to im-proved parsing performance.For basic text normalization, we reduce all webURLs to a single token, i.e., each web URL is re-placed with a uniform place-holder in the entireEWT, marking it as a URL.
Similarly, all emoticonsare replaced by a single marker indicating an emoti-con.
Repeated use of punctuation, e.g., !!
!, is re-duced to a single punctuation token.We also have a module to shorten words with con-secutive sequences of the same character: Any char-acter that occurs more than twice in sequence willbe shortened to one character, unless they appear ina dictionary, including the internet and slang dictio-naries discussed below, in which case they map tothe dictionary form.
Thus, the word Awww in ex-ample (1) is shortened to Aw, and cooool maps tothe dictionary form cool.
However, since we usegold POS tags for our experiments, this module isnot used in the experiments reported here.3.2 Spell checkingNext, we run a spell checker to normalize mis-spellings, as online data often contains spellingerrors (e.g.
cuttie in example (1)).
Various sys-tems for parsing web data (e.g., from the SANCLshared task) have thus also explored spelling cor-rection; McClosky et al(2012), for example, used1,057 autocorrect rules, though?since these didnot make many changes?the system was not ex-plored after that.
Spell checking web data, such asYouTube comments or blog data, is a challenge be-cause it contains non-standard orthography, as wellas acronyms and other short-hand forms unknownto a standard spelling dictionary.
Therefore, be-fore mapping to a corrected spelling, it is vital todifferentiate between a misspelled word and a non-standard one.We use Aspell3 as our spell checker to recognizeand correct misspelled words.
If asked to correctnon-standard words, the spell checker would choosethe closest standard English word, inappropriate tothe context.
For example, Aspell suggests Lil forlol.
Thus, before correcting, we first check whethera word is an instance of internet speech, i.e., an ab-breviation or a slang term.We use a list of more than 3,000 acronyms toidentify acronyms and other abbreviations not usedcommonly in formal registers of language.
The listwas obtained from NetLingo, restricted to the en-tries listed as chat acronyms and text message short-hand.4 To identify slang terminology, we use theUrban Dictionary5.
In a last step, we combine bothlists with the list of words extracted from the WSJ.If a word is not found in these lists, Aspell is usedto suggest a correct spelling.
In order to restrict As-pell from suggesting spellings that are too differentfrom the word in question, we use Levenshtein dis-tance (Levenshtein, 1966) to measure the degree ofsimilarity between the original form and the sug-gested spelling; only words with small distancesare accepted as spelling corrections.
Since we havewords of varying length, the Levenshtein distance isnormalized by the length of the suggested spelling(i.e., number of characters).
In non-exhaustive testson a subset of the test set, we found that a normal-ized score of 0.301, i.e., a relatively low score ac-cepting only conservative changes, achieves the bestresults when used as a threshold for accepting a sug-3www.aspell.net4http://www.netlingo.com/acronyms.php5www.urbandictionary.com3gested spelling.
The utilization of the threshold re-stricts Aspell from suggesting wrong spellings fora majority of the cases.
For example, for the wordmujahidin, Aspell suggested Mukden, which has ascore of 1.0 and is thus rejected.
Since we do notconsider context or any other information besidesedit distance, spell checking is not perfect and issubject to making errors, but the number of errorsis considerably smaller than the number of correctrevisions.
For example, lol would be changed intoLil if it were not listed in the extended lexicon.
Ad-ditionally, since the errors are consistent throughoutthe data, they result in normalization even when thespelling is wrong.4 Parser revisionWe use a state of the art dependency parser, MST-Parser (McDonald and Pereira, 2006), as our mainparser; and we use two parse revision methods: amachine learning model and a simple tree anomalymodel.
The goal is to be able to learn where theparser errs and to adjust the parses to be more appro-priate given the target domain of social media texts.4.1 Basic parserMSTParser (McDonald and Pereira, 2006)6 is afreely available parser which reaches state-of-the-artaccuracy in dependency parsing for English.
MST isa graph-based parser which optimizes its parse treeglobally (McDonald et al 2005), using a variety offeature sets, i.e., edge, sibling, context, and non-local features, employing information from wordsand POS tags.
We use its default settings for all ex-periments.We use MST as our base parser, training it in dif-ferent conditions on the WSJ and the EWT.
Also,MST offers the possibility to retrieve confidencescores for each dependency edge: We use the KD-Fix edge confidence scores discussed by Mejer andCrammer (2012) to assist in parse revision.
As de-scribed in section 4.4, the scores are used to limitwhich dependencies are candidates for revision: ifa dependency has a low confidence score, it may berevised, while high confidence dependencies are notconsidered for revision.6http://sourceforge.net/projects/mstparser/4.2 Reviser #1: machine learning modelWe use DeSR (Attardi and Ciaramita, 2007) as a ma-chine learning model of parse revision.
DeSR uses atree revision method based on decomposing revisionactions into basic graph movements and learning se-quences of such movements, referred to as a revisionrule.
For example, the rule -1u indicates that thereviser should change a dependent?s head one wordto the left (-1) and then up one element in the tree(u).
Note that DeSR only changes the heads of de-pendencies, but not their labels.
Such revision rulesare learned for a base parser by comparing the baseparser output and the gold-standard of some unseendata, based on a maximum entropy model.In experiments, DeSR generally only considersthe most frequent rules (e.g., 20), as these covermost of the errors.
For best results, the revisershould: a) be trained on extra data other than thedata the base parser is trained on, and b) begin witha relatively poor base parsing model.
As we will see,using a fairly strong base parser presents difficultiesfor DeSR.4.3 Reviser #2: simple tree anomaly modelAnother method we use for building parse revisionsis based on a method to detect anomalies in parsestructures (APS) using n-gram sequences of depen-dency structures (Dickinson and Smith, 2011; Dick-inson, 2010).
The method checks whether the samehead category (e.g., verb) has a set of dependentssimilar to others of the same category (Dickinson,2010).To see this, consider the partial tree in figure 2,from the dependency-converted EWT.7 This tree isconverted to a rule as in (2), where all dependents ofa head are realized.... DT NN IN ...dobjdet prepFigure 2: A sketch of a basic dependency tree(2) dobj?
det:DT NN prep:IN7DT/det=determiner, NN=noun, IN/prep=preposition,dobj=direct object4This rule is then broken down into its componentn-grams and compared to other rules, using the for-mula for scoring an element (ei) in (3).
N -gramcounts (C(ngrm)) come from a training corpus; aninstantiation for this rule is in (4).
(3) s(ei) =?ngrm:ei?ngrm?n?3C(ngrm)(4) s(prep:IN) = C(det:DT NN prep:IN)+ C(NN prep:IN END)+ C(START det:DT NN prep:IN)+ C(det:DT NN prep:IN END)+ C(START det:DT NN prep:IN END)We modify the scoring slightly, incorporating bi-grams (n ?
2), but weighing them as 0.01 of a count(C(ngrm)); this handles the issue that bigrams arenot very informative, yet having some bigrams isbetter than none (Dickinson and Smith, 2011).The method detects non-standard parses whichmay result from parser error or because the textis unusual in some other way, e.g., ungrammatical(Dickinson, 2011).
The structures deemed atypicaldepend upon the corpus used for obtaining the gram-mar that parser output is compared to.With a method of scoring the quality of individualdependents in a tree, one can compare the score ofa dependent to the score obtaining by hypothesizinga revision.
For error detection, this ameliorates theeffect of odd structures for which no better parse isavailable.
The revision checking algorithm in Dick-inson and Smith (2011) posits new labelings andattachments?maintaining projectivity and acyclic-ity, to consider only reasonable candidates8?andchecks whether any have a higher score.9 If so, thetoken is flagged as having a better revision and ismore likely to be an error.In other words, the method checks revisions forerror detection.
With a simple modification of thecode,10 one can also keep track of the best revision8We remove the cyclicity check, in order to be able to detecterrors where the head and dependent are flipped.9We actually check whether a new score is greater than orequal to twice the original score, to account for meaninglessdifferences for large values, e.g., 1001 vs. 1000.
We do notexpect our minor modifications to have a huge impact, thoughmore robust testing is surely required.10http://cl.indiana.edu/?md7/papers/dickinson-smith11.htmlfor each token and actually change the tree structure.This is precisely what we do.
Because the methodrelies upon very coarse scores, it can suggest toomany revisions; in tandem with parser confidence,though, this can filter the set of revisions to a rea-sonable amount, as discussed next.4.4 Pinpointing erroneous parsesThe parse revision methods rely both on being ableto detect errors and on being able to correct them.We can assist the methods by using MST confidencescores (Mejer and Crammer, 2012) to pinpoint can-didates for revision, and only pass these candidateson to the parse revisers.
For example, since APS(anomaly detection) detects atypical structures (sec-tion 4.3), some of which may not be errors, it willfind many strange parses and revise many positionson its own, though some be questionable revisions.By using a confidence filter, though, we only con-sider ones flagged below a certain MST confidencescore.
We follow Mejer and Crammer (2012) anduse confidence?0.5 as our threshold for identifyingerrors.
Non-exhaustive tests on a subset of the testset show good performance with this threshold.In the experiments reported in section 5, if we usethe revision methods to revise everything, we referto this as the DeSR and the APS models; if we fil-ter out high confidence cases and restrict revisionsto low confidence scoring cases, we refer to this asDeSR restricted and APS restricted.Before using the MST confidence scores as partof the revision process, then, we first report on usingthe scores for error detection at the ?0.5 threshold,as shown in table 1.
As we can see, using confi-dence scores allows us to pinpoint errors with highprecision.
With a recall around 40?50%, we find er-rors with upwards of 90% precision, meaning thatthese cases are in need of revision.
Interestingly, thehighest error detection precision comes with WSJas part of the training data and EWT as the test-ing.
This could be related to the great difference be-tween the WSJ and EWT grammatical models andthe greater number of unknown words in this ex-periment, though more investigation is needed.
Al-though data sets are hard to compare, the precisionseems to outperform that of more generic (i.e., non-parser-specific) error detection methods (Dickinsonand Smith, 2011).5Normalization Attach.
Label.
TotalTrain Test (on test) Tokens Errors Errors Errors Precision RecallWSJ WSJ none 4,621 2,452 1,297 3,749 0.81 0.40WSJ EWT none 5,855 3,621 2,169 5,790 0.99 0.38WSJ EWT full 5,617 3,484 1,959 5,443 0.97 0.37EWT EWT none 7,268 4,083 2,202 6,285 0.86 0.51EWT EWT full 7,131 3,905 2,147 6,052 0.85 0.50WSJ+EWT EWT none 5,622 3,338 1,849 5,187 0.92 0.40WSJ+EWT EWT full 5,640 3,379 1,862 5,241 0.93 0.41Table 1: Error detection results for MST confidence scores (?
0.5) for different conditions and normalization settings.Number of tokens and errors below the threshold are reported.5 ExperimentsWe report three major sets of experiments: the firstset compares the two parse revision strategies; thesecond looks into text normalization strategies; andthe third set investigates whether the size of thetraining set or its similarity to the target domain ismore important.
Since we are interested in parsingin these experiments, we use gold POS tags as in-put for the parser, in order to exclude any unwantedinteraction between POS tagging and parsing.5.1 Parser revisionIn this experiment, we are interested in comparing amachine learning method to a simple n-gram revi-sion model.
For all experiments, we use the originalversion of the EWT data, without any normalization.The results of this set of experiments are shownin table 2.
The first row reports MST?s performanceon the standard WSJ data split, giving an idea of anupper bound for these experiments.
The second partshows MST?s performance on the EWT data, whentrained on WSJ or the combination of the WSJ andEWT training sets.
Note that there is considerabledecrease for both settings in terms of unlabeled ac-curacy (UAS) and labeled accuracy (LAS), of ap-proximately 8% when trained on WSJ and 5.5% onWSJ+EWT.
This drop in score is consistent withprevious work on non-canonical data, e.g., web data(Foster et al 2011) and learner language (Krivanekand Meurers, 2011).
It is difficult to compare theseresults, due to different training and testing condi-tions, but MST (without any modifications) reachesresults that are in the mid-high range of results re-ported by Petrov and McDonald (2012, table 4) intheir overview of the SANCL shared task using theEWT data: 80.10?87.62% UAS; 71.04%?83.46%LAS.Next, we look at the performance of the two re-visers on the same data sets.
Note that since DeSRrequires training data for the revision part that is dif-ferent from the training set of the base parser, weconduct parsing and revision in DeSR with two dif-ferent data sets.
Thus, for the WSJ experiment, wesplit the WSJ training set into two parts, WSJ02-11 and WSJ12-2, instead of training on the wholeWSJ.
For the EWT training set, we split this set intotwo parts and use 25% of it for training the parser(EWTs) and the rest for training the reviser (EWTr).In contrast, APS does not need extra data for train-ing and thus was trained on the same data as thebase parser.
While this means that the base parserfor DeSR has a smaller training set, note that DeSRworks best with a weak base parser (Attardi, p.c.
).The results show that DeSR?s performance is be-low MST?s on the same data.
In other words,adding DeSRs revisions decreases accuracy.
APSalso shows a deterioration in the results, but the dif-ference is much smaller.
Also, training on a combi-nation of WSJ and EWT data increases the perfor-mance of both revisers by 2-3% over training solelyon WSJ.Since these results show that the revisions areharmful, we decided to restrict the revisions furtherby using MST?s KD-Fix edge confidence scores, asdescribed in section 4.4.
We apply the revisions onlyif MST?s confidence in this dependency is low (i.e.,below or equal to 0.5).
The results of this experimentare shown in the last section of table 2.
We can see6Method Parser Train Reviser Train Test UAS LASMST WSJ n/a WSJ 89.94 87.24MST WSJ n/a EWT 81.98 78.65MST WSJ+EWT n/a EWT 84.50 81.61DeSR WSJ02-11 WSJ12-22 EWT 80.63 77.33DeSR WSJ+EWTs EWTr EWT 82.68 79.77APS WSJ WSJ EWT 81.96 78.40APS WSJ+EWT WSJ+EWT EWT 84.45 81.29DeSR restricted WSJ+EWTs EWTr EWT 84.40 81.50APS restricted WSJ+EWT WSJ+EWT EWT 84.53 *81.66Table 2: Results of comparing a machine learning reviser (DeSR) with a tree anomaly model (APS), with base parserMST (* = sig.
at the 0.05 level, as compared to row 2).that both revisers improve over their non-restrictedversions.
However, while DeSR?s results are stillbelow MST?s baseline results, APS shows slight im-provements over the MST baseline, significant in theLAS.
Significance was tested using the CoNLL-Xevaluation script in combination with Dan Bikel?sRandomized Parsing Evaluation Comparator, whichis based on sampling.11For the original experiment, APS changes 1,402labels and 272 attachments of the MST output.
Inthe restricted version, label changes are reduced to610, and attachment to 167.
In contrast, DeSRchanges 1,509 attachments but only 303 in the re-stricted version.
The small numbers, given thatwe have more than 3,000 sentences in the test set,show that finding reliable revisions is a difficult task.Since both revisers are used more or less off theshelf, there is much room to improve.Based on these results and other results based ondifferent settings, which, for DeSR, resulted in lowaccuracy, we decided to concentrate on APS in thefollowing experiments, and more specifically focuson the restricted version of APS to see whether thereare significant improvements under different dataconditions.5.2 Text normalizationIn this set of experiments, we investigate the influ-ence of the text normalization strategies presentedin section 3 on parsing and more specifically on ourparse revision strategy.
Thus, we first apply a par-tial normalization, using only the basic text normal-11http://ilk.uvt.nl/conll/software.htmlization.
For the full normalization, we combine thebasic text normalization with the spell checker.
Forthese experiments, we use the restricted APS reviserand the EWT treebank for training and testing.The results are shown in table 3.
Note that sincewe also normalize the training set, MST will alsoprofit from the normalizations.
For this reason, wepresent MST and APS (restricted) results for eachtype of normalization.
The first part of the tableshows the results for MST and APS without any nor-malization; the numbers here are higher than in ta-ble 2 because we now train only on EWT?an issuewe take up in section 5.3.
The second part shows theresults for partial normalization.
These results showthat both approaches profit from the normalizationto the same degree: both UAS and LAS increase byapproximately 0.25 percent points.
When we look atthe full normalization, including spell checking, wecan see that it does not have a positive effect on MSTbut that APS?s results increase, especially unlabeledaccuracy.
Note that all APS versions significantlyoutperform the MST versions but also that both nor-malized MST versions significantly outperform thenon-normalized MST.5.3 WSJ versus domain dataIn these experiments, we are interested in which typeof training data allows us to reach the highest accu-racy in parsing.
Is it more useful to use a large, out-of-domain training set (WSJ in our case), a small,in-domain training set, or a combination of both?Our assumption was that the largest data set, con-sisting of the WSJ and the EWT training sets, would7Norm.
Method UAS LASTrain:no; Test:no MST 84.87 82.21Train:no; Test:no APS restr.
**84.90 *82.23Train:part; Test:part MST *85.12 *82.45Train:part; Test:part APS restr.
**85.18 *82.50Train:full; Test:full MST **85.20 *82.45Train:full; Test:full APS restr.
**85.24 **82.52Table 3: Results of comparing different types of text normalization, training and testing on EWT sets.
(Significancetested for APS versions as compared to the corresponding MST version and for each MST with the non-normalizedMST: * = sig.
at the 0.05 level, ** = significance at the 0.01 level).give the best results.
For these experiments, we usethe EWT test set and different combinations of textnormalization, and the results are shown in table 4.The first three sections in the table show the re-sults of training on the WSJ and testing on the EWT.The results show that both MST and APS profit fromtext normalization.
Surprisingly, the best results aregained by using the partial normalization; adding thespell checker (for full normalization) is detrimental,because the spell checker introduces additional er-rors that result in extra, non-standard words in EWT.Such additional variation in words is not present inthe original training model of the base parser.For the experiments with the EWT and the com-bined WSJ+EWT training sets, spell checking doeshelp, and we report only the results with full normal-ization since this setting gave us the best results.
Toour surprise, results with only the EWT as trainingset surpass those of using the full WSJ+EWT train-ing sets (a UAS of 85.24% and a LAS of 82.52% forEWT vs. a UAS of 82.34% and a LAS of 79.31%).Note, however, that when we reduce the size of theWSJ data such that it matches the size of the EWTdata, performance increases to the highest results,a UAS of 86.41% and a LAS of 83.67%.
Takentogether, these results seem to indicate that quality(i.e., in-domain data) is more important than mere(out-of-domain) quantity, but also that more out-of-domain data can help if it does not overwhelm thein-domain data.
It is also obvious that MST perse profits the most from normalization, but that theAPS consistently provides small but significant im-provements over the MST baseline.6 Summary and OutlookWe examined ways to improve parsing social me-dia and other web data by altering the input data,namely by normalizing such texts, and by revis-ing output parses.
We found that normalization im-proves performance, though spell checking has moreof a mixed impact.
We also found that a very sim-ple tree reviser based on grammar comparisons per-forms slightly but significantly better than the base-line, across different experimental conditions, andwell outperforms a machine learning model.
The re-sults also demonstrated that, more than the size ofthe training data, the goodness of fit of the data hasa great impact on the parser.
Perhaps surprisingly,adding the entire WSJ training data to web trainingdata leads to a deteriment in performance, whereasbalancing it with web data has the best performance.There are many ways to take this work in thefuture.
The small, significant improvements fromthe APS restricted reviser indicate that there is po-tential for improvement in pursuing such grammar-corrective models for parse revision.
The model weuse relies on a simplistic notion of revisions, nei-ther checking the resulting well-formedness of thetree nor how one correction influences other cor-rections.
One could also, for example, treat gram-mars from different domains in different ways toimprove scoring and revision.
Another possibilitywould be to apply the parse revisions also to the out-of-domain training data, to make it more similar tothe in-domain data.For text normalization, the module could benefitfrom a few different improvements.
For example,non-contracted words such as well to mean we?llrequire a more complicated normalization step, in-8Train Test Normalization Method UAS LASWSJ EWT train:no; test:no MST 81.98 78.65WSJ EWT train:no; test:no APS 81.96 78.40WSJ EWT train:no; test:no APS restr 82.02 **78.71WSJ EWT train:no; test:part MST 82.31 79.27WSJ EWT train:no; test:part APS restr.
*82.36 *79.32WSJ EWT train:no; test:full MST 82.30 79.26WSJ EWT train:no; test:full APS restr.
82.34 *79.31EWT EWT train:full; test:full MST 85.20 82.45EWT EWT train:full; test:full APS restr.
**85.24 **82.52WSJ+EWT EWT train:full; test:full MST 84.59 81.68WSJ+EWT EWT train:full; test:full APS restr.
**84.63 *81.73Balanced WSJ+EWT EWT train:full; test:full MST 86.38 83.62Balanced WSJ+EWT EWT train:full; test:full APS restr.
*86.41 **83.67Table 4: Results of different training data sets and normalization patterns on parsing the EWT test data.
(Significancetested for APS versions as compared to the corresponding MST: * = sig.
at the 0.05 level, ** = sig.
at the 0.01 level)volving machine learning or n-gram language mod-els.
In general, language models could be used formore context-sensitive spelling correction.
Giventhe preponderance of terms on the web, using anamed entity recognizer (e.g., Finkel et al 2005)for preprocessing may also provide benefits.AcknowledgmentsWe would like to thank Giuseppe Attardi for his helpin using DeSR; Can Liu, Shoshana Berleant, and theIU CL discussion group for discussion; and the threeanonymous reviewers for their helpful comments.ReferencesGiuseppe Attardi and Massimiliano Ciaramita.2007.
Tree revision learning for dependency pars-ing.
In Proceedings of HLT-NAACL-07, pages388?395.
Rochester, NY.Giuseppe Attardi and Felice Dell?Orletta.
2009.
Re-verse revision and linear tree combination fordependency parsing.
In Proceedings of HLT-NAACL-09, Short Papers, pages 261?264.
Boul-der, CO.Ann Bies, Justin Mott, Colin Warner, and SethKulick.
2012.
English Web Treebank.
Linguis-tic Data Consortium, Philadelphia, PA.Ozlem Cetinoglu, Anton Bryl, Jennifer Foster, andJosef Van Genabith.
2011.
Improving dependencylabel accuracy using statistical post-editing: Across-framework study.
In Proceedings of the In-ternational Conference on Dependency Linguis-tics, pages 300?309.
Barcelona, Spain.Marie-Catherine de Marneffe and Christopher D.Manning.
2008.
The Stanford typed dependenciesrepresentation.
In COLING 2008 Workshop onCross-framework and Cross-domain Parser Eval-uation.
Manchester, England.Markus Dickinson.
2010.
Detecting errors inautomatically-parsed dependency relations.
InProceedings of ACL-10.
Uppsala, Sweden.Markus Dickinson.
2011.
Detecting ad hoc rules fortreebank development.
Linguistic Issues in Lan-guage Technology, 4(3).Markus Dickinson and Amber Smith.
2011.
De-tecting dependency parse errors with minimal re-sources.
In Proceedings of IWPT-11, pages 241?252.
Dublin, Ireland.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of ACL?05, pages 363?370.
Ann Arbor, MI.Jennifer Foster.
2010.
?cba to check the spelling?
:Investigating parser performance on discussionforum posts.
In Proceedings of NAACL-HLT2010, pages 381?384.
Los Angeles, CA.9Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Joseph Le Roux, Joakim Nivre, Deirdre Hogan,and Josef van Genabith.
2011.
From news to com-ment: Resources and benchmarks for parsing thelanguage of web 2.0.
In Proceedings of IJCNLP-11, pages 893?901.
Chiang Mai, Thailand.Phani Gadde, L. V. Subramaniam, and Tanveer A.Faruquie.
2011.
Adapting a WSJ trained part-of-speech tagger to noisy text: Preliminary results.In Proceedings of Joint Workshop on MultilingualOCR and Analytics for Noisy Unstructured TextData.
Beijing, China.Enrique Henestroza Anguiano and Marie Candito.2011.
Parse correction with specialized modelsfor difficult attachment types.
In Proceedings ofEMNLP-11, pages 1222?1233.
Edinburgh, UK.Julia Krivanek and Detmar Meurers.
2011.
Compar-ing rule-based and data-driven dependency pars-ing of learner language.
In Proceedings of the Int.Conference on Dependency Linguistics (Depling2011), pages 310?317.
Barcelona.Vladimir I. Levenshtein.
1966.
Binary codes capableof correcting deletions, insertions, and reversals.Cybernetics and Control Theory, 10(8):707?710.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Compu-tational Linguistics, 19(2):313?330.David McClosky, Wanxiang Che, Marta Recasens,Mengqiu Wang, Richard Socher, and ChristopherManning.
2012.
Stanford?s system for parsing theEnglish web.
In Workshop on the Syntactic Anal-ysis of Non-Canonical Language (SANCL 2012).Montreal, Canada.David McClosky, Mihai Surdeanu, and ChristopherManning.
2011.
Event extraction as dependencyparsing.
In Proceedings of ACL-HLT-11, pages1626?1635.
Portland, OR.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training ofdependency parsers.
In Proceedings of ACL-05,pages 91?98.
Ann Arbor, MI.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsingalgorithms.
In Proceedings of EACL-06.
Trento,Italy.Avihai Mejer and Koby Crammer.
2012.
Are yousure?
Confidence in prediction of dependencytree edges.
In Proceedings of the NAACL-HTL2012, pages 573?576.
Montre?al, Canada.Tetsuji Nakagawa, Kentaro Inui, and Sadao Kuro-hashi.
2010.
Dependency tree-based sentimentclassification using CRFs with hidden variables.In Proceedings of NAACL-HLT 2010, pages 786?794.
Los Angeles, CA.Lilja ?vrelid and Arne Skj?rholt.
2012.
Lexicalcategories for improved parsing of web data.
InProceedings of the 24th International Conferenceon Computational Linguistics (COLING 2012),pages 903?912.
Mumbai, India.Slav Petrov and Ryan McDonald.
2012.
Overviewof the 2012 shared task on parsing the web.In Workshop on the Syntactic Analysis of Non-Canonical Language (SANCL 2012).
Montreal,Canada.10
