Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 627?632,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsNon-linear Mapping for Improved Identification of 1300+ LanguagesRalf D. BrownCarnegie Mellon University Language Technologies Institute5000 Forbes Avenue, Pittsburgh PA 15213 USAralf@cs.cmu.eduAbstractNon-linear mappings of the formP (ngram)?andlog(1+?P (ngram))log(1+?
)are applied to the n-gram probabilitiesin five trainable open-source languageidentifiers.
The first mapping reducesclassification errors by 4.0% to 83.9%over a test set of more than one million65-character strings in 1366 languages,and by 2.6% to 76.7% over a subset of 781languages.
The second mapping improvesfour of the five identifiers by 10.6% to83.8% on the larger corpus and 14.4% to76.7% on the smaller corpus.
The subsetcorpus and the modified programs aremade freely available for download athttp://www.cs.cmu.edu/?ralf/langid.html.1 IntroductionLanguage identification, particularly of shortstrings, is a task which is becoming quite impor-tant as a preliminary step in much automated pro-cessing of online data streams such as microblogs(e.g.
Twitter).
In addition, an increasing num-ber of languages are represented online, so it isdesireable that performance remain high as morelanguages are added to the identifier.In this paper, we stress-test five open-sourcen-gram-based language identifiers by presentingthem with 65-character strings (about one printedline of text in a book) in up to 1366 languages.
Wethen apply a simple modification to their scoringalgorithms which improves the classification ac-curacy of all five of them, three quite dramatically.2 MethodThe selected modification to the scoring algorithmis to apply a non-linear mapping which spreadsout the lower probability values while compact-ing the higher ones.
This low-end spreading ofvalues is the opposite of what one sees in a Zip-fian distribution (Zipf, 1935), where the proba-bilities of the most common items are the mostspread out while the less frequent items becomeever more crowded as there are increasing num-bers of them in ever-smaller ranges.
The hypoth-esis is that regularizing the spacing between val-ues will improve language-identification accuracyby avoiding over-weighting frequent items (fromhaving higher probabilities in the training data andalso occurring more frequently in the test string).Two functions were selected for experiments:x = P (ngram)gamma:y = x?loglike:y =log(1 + 10?x)log(1 + 10?
)The first simply raises the n-gram probabil-ity to a non-unity power; this exponent is named?gamma?
as in image processing (Poynton, 1998).The second mapping function is a normalized vari-ant of the logarithm function; the normalizationprovides fixed points at 0 and 1, as is the case forgamma.
Each of the functions gamma and loglikehas one tunable parameter, ?
and ?
, respectively.3 Related WorkAlthough n-gram statistics as a basis for languageidentification has been in use for two decades sinceCavnar and Trenkle (1994) and Dunning (1994),little work has been done on trying to optimizethe values used for those n-gram statistics.
Wheresome form of frequency mapping is used, it is of-ten implicit (as in Cavnar and Trenkle?s use ofranks instead of frequencies) and generally goesunremarked as such.Vogel and Tresner-Kirsch (2012) use the log-arithm of the frequency for some experimentalruns, reporting that it improved accuracy in somecases.
Gebre et al (2013) used logarithmic term-frequency scaling of words in an English-language627essay to classify the native language of the writer,reporting an improvement from 82.36% accuracyto 84.55% in conjunction with inverse documentfrequency (IDF) weighting, and from 79.18% ac-curacy to 80.82% without IDF.4 Programs4.1 LangDetectLangDetect, version 2011-09-13 (Shuyo,2014), uses the Naive Bayes approach.
Inputs aresplit into a bag of character n-grams of length 1through 3; each randomly-drawn n-gram?s prob-ability in each of the trained models is multipliedby the current score for that model.
After 1000n-grams, or when periodic renormalization intoa probability distribution detects that one modelhas accumulated an overwhelming probabilitymass, the iteration is terminated.
After averagingseven randomized iterations, each with a randomgaussian offset (mean 5?10?6, standard deviation0.5?
10?6) that is added to each probability priorto multiplication (to avoid multiplication by zero),the highest-scoring model is declared to be thelanguage of the input.The mapping function is applied to the model?sprobability before adding the randomized off-set.
To work around the limitation of one modelper language code, disambiguating digits are ap-pended to the language code during training andremoved from the output prior to scoring.4.2 libtextcatlibtextcat, version 2.2-9 (Hugueney, 2011),is a C reimplementation of the Cavnar and Tren-kle (1994) method.
It compiles ?fingerprints?
con-taining a ranked list of the 400 (by default) mostfrequent 1- through 5-grams in the training data.An unknown text is classified by forming its fin-gerprint and comparing that fingerprint against thetrained fingerprints.
A penalty is assigned basedon the number of positions by which each n-gramdiffers between the input and the trained model;n-grams which appear in only one of the two areassigned the maximum penalty, equal to the sizeof the fingerprints.
The model with the lowestpenalty score is selected as the language of the in-put.For this work, the libtextcat source codewas modified to remove the hard-coded fingerprintsize of 400 n-grams.
While adding the frequencymapping, the code was discovered to also hard-code the maximum distortion penalty at 400; thiswas corrected to set the maximum penalty equal tothe maximum size of any loaded fingerprint.1Score mapping was implemented by dividingeach penalty value by the maximum penalty toproduce a proportion, applying the mapping func-tion, and then multiplying the result by the maxi-mum penalty and rounding to an integer (to avoidother code changes).
Because there are only a lim-ited number of possible penalties, a lookup table ispre-computed, eliminating the impact on speed.4.3 mguessermguesser, version 0.4 (Barkov, 2008), is part ofthe mnoGoSearch search engine.
While its doc-umentation indicates that it implements the Cav-nar and Trenkle approach, its actual similaritycomputation is very different.
Each training andtest text is converted into a 4096-element hash ta-ble by extracting byte n-grams of length 6 (trun-cated at control characters and multiple consecu-tive blanks), hashing each n-gram using CRC-32,and incrementing the count for the correspondinghash entry.
The hash table entries are then nor-malized to a mean of 0.0 and standard deviationof 1.0, and the similarity is computed as the inner(dot) product of the hash tables treated as vectors.The trained model receiving the highest similarityscore against the input is declared the language ofthe input.Nonlinear mapping was added by inserting astep just prior to the normalization of the hash ta-ble.
The counts in the table are converted to proba-bilities by dividing by the sum of counts, the map-ping is applied to that probability, and the result isconverted back into a count by multiplying by theoriginal sum of counts.4.4 whatlangwhatlang, version 1.24 (Brown, 2014a), isthe stand-alone identification program from LA-Strings (Brown, 2013).
It performs identifica-tion by computing the inner product of byte tri-grams through k-grams (k=6 by default and inthis work) between the input and the trained mod-els; for speed, the computation is performed in-crementally, adding the length-weighted probabil-1The behavior observed by (Brown, 2013) of performancerapidly degrading for fingerprints larger than 500 disappearswith this correction.
It was an artifact of an increasing pro-portion of n-grams present in the model receiving penaltiesgreater than n-grams absent from the model.628ity of each n-gram as it is encountered in the in-put.
Models are formed by finding the highest-frequency n-grams of the configured lengths, withsome filtering as described in (Brown, 2012).4.5 YALIYALI (Yet Another Language Identifier) (Majlis,2012) is an identifier written in Perl.
It performsminor text normalization by collapsing multipleblanks into a single blank and removing leadingand trailing blanks from lines.
Thereafter, it usesa sliding window to generate byte n-grams of a(configurable) fixed length, and sums the proba-bilities for each n-gram in each trained model.
Aswith whatlang, this effectively computes the in-ner products between the input and the models.Mapping was added by applying the mappingfunction to the model probabilities as they areread in from disk.
As with LangDetect, disam-biguating digits were used to allow multiple mod-els per language code.5 DataThe data used for the experiments described inthis paper comes predominantly from Bible trans-lations, Wikipedia, and the Europarl corpus of Eu-ropean parliamentary proceedings (Koehn, 2005).The 1459 files of the training corpus generate 1483models in 1368 languages.
A number of train-ing files generate models in both UTF-8 and ISO8859-1, numerous languages have multiple train-ing files in different writing systems, and severalhave multiple files for different regional variants(e.g.
European and Brazilian Portugese).The text for a language is split into training,test, and possibly a disjoint development set.
Theamount of text per language varies, with quartilesof 1.19/1.47/2.22 million bytes.
In general, ev-ery thirtieth line of text is reserved for the test set;some smaller languages reserve a higher propor-tion.
If more than 3.2 million bytes remain af-ter reserving the test set, every thirtieth line ofthe remaining text is reserved as a developmentset.
There are development sets for 220 languages.The unreserved test is used for model training.The test data is word-wrapped to 65 charactersor less, and wrapped lines shorter than 25 bytesare excluded.
Up to the first 1000 lines of wrappedtext are used for testing.
One language with fewerthan 50 test strings is excluded from the test set, asis the constructed language Klingon due to heavypollution with English.
In total, the test files con-tain 1,090,571 lines of text in 1366 languages.Wikipedia text and many of the Bible transla-tions are redistributable under Creative Commonslicenses, and have been packaged into the LTILangID Corpus (Brown, 2014b).
This smallercorpus contains 781 languages, 119 of them withdevelopment sets, and a total of 649,589 lines inthe test files.
The languages are a strict subsetof those in the larger corpus, but numerous lan-guages have had Wikipedia text substituted fornon-redistributable Bible translations.6 ExperimentsUsing the data sets described in the previous sec-tion, we ran a sweep of different gamma and tauvalues for each language identifier to determinetheir optimal values on both development and teststrings.
Step sizes for ?
were generally 0.1, whilethose for ?
were 1.0, with smaller steps near theminima.
Since it does not provide explicit con-trol over model sizes, LangDetect was trainedon a maximum of 1,000,000 bytes per model, asreported optimal in (Brown, 2013).
The other pro-grams were trained on a maximum of 2,500,000bytes per model; libtextcat and whatlangused default model sizes of 400 and 3500, respec-tively, while mguesser was set to the previously-reported 1500 n-grams per model.
After some ex-perimentation, YALI was set to use 5-grams, with3500 n-grams per model to match whatlang.7 ResultsTables 1 and 2 show the absolute performance andrelative percentage change in classification errorsfor the five programs using the two mapping func-tions, as well as the values of ?
and ?
at which thefewest errors were made on the development set.Overall, the smaller corpus performed worse dueto the greater percentage of Wikipedia texts, whichare polluted with words and phrases in other lan-guages.
In the test set, this occasionally causesa correct identification as another language to bescored as an error.Figures 2 and 3 graph the classification errorrates (number of incorrectly-labeled strings di-vided by total number of strings in the test set) inpercent for different values of ?.
A gamma of 1.0is the baseline condition.
The dramatic improve-ments in mguesser, whatlang and YALI arequite evident, while the smaller but non-trivial im-629gamma mapping loglike mappingProgram Error% Error% ?% ?
Error% ?% ?LangDet.
3.233 2.767 -14.4 0.80 2.889 -10.6 1.0libtextcat 6.787 6.514 -4.0 2.20 ?
?
?mguesser 15.704 4.330 -72.4 0.39 4.177 -73.4 3.8whatlang 13.309 2.136 -83.9 0.27 2.146 -83.8 4.5YALI 9.883 2.313 -76.6 0.20 2.313 -76.6 8.0Table 1: Language-identification accuracy on the 1366-language corpus.
?
and ?
were tuned on the220-language development set; only marginally better results can be achieved by tuning on the test set.gamma mapping loglike mappingProgram Error% Error% ?% ?
Error% ?% ?LangDet.
3.603 3.093 -14.2 0.68 3.083 -14.4 2.3libtextcat 6.693 6.521 -2.6 1.70 ?
?
?mguesser 14.200 4.936 -65.2 0.40 4.779 -66.3 3.7whatlang 11.879 2.770 -76.7 0.14 2.772 -76.7 5.6YALI 8.726 2.972 -65.9 0.09 2.989 -65.7 9.0Table 2: Language-identification accuracy on the 781-language corpus.
?
and ?
were tuned on the 119-language development set.
libtextcat did not improve with the loglike mapping (see text).provements in libtextcat are difficult to dis-cern at this scale.
Since libtextcat uses muchsmaller models than the others by default, Figure1 gives a closer look at its performance for largermodel sizes.
As the models grow, the absolutebaseline performance improves, but the changefrom gamma-correction decreases and the optimalvalue of ?
also decreases toward 1.0.
This hintsthat the implicit mapping of ranks either becomescloser to optimal, or that gamma becomes less ef-fective at correcting it.
At a model size of 3000n-grams, the baseline error rate is 2.465% whilethe best performance is 2.457% at ?
= 1.10.That the best ?
for libtextcat is greaterthan 1.0 was not entirely unexpected.
The power-law distribution of n-gram frequencies impliesthat the conversion from frequencies to ranks isessentially logarithmic, and log n eventually be-comes less than ncfor any c > 0.
The implicationof ?
> 1 is simply that the conversion to ranksis too strong a correction, which must be partiallyundone by the gamma mapping.Figures 4 and 5 graph the error rates for differ-ent values of ?
.
On the graph, zero is the baselinecondition without mapping for comparison pur-poses; the mapping function is not the identity for?
= 0.
It can clearly be seen that libtextcat ishurt by the loglike mapping, which never reducesvalues, even with negative ?
.
Using the inverse of234567890.0 0.5 1.0 1.5 2.0Error Rate(%)Gamman=400n=500n=2000n=3000Figure 1: libtextcat performance at differentfingerprint sizes.
?
= 1 is the baseline.the loglike mapping should improve performance,but has not yet been tried.
The other programsshow very similar behavior to their results withgamma.8 Conclusions and Future WorkNon-linear mapping is shown to be effective atimproving the accuracy of five different language6302468101214160.0 0.5 1.0 1.5 2.0Error Rate(%)Gammamguesser (n=1500)YALI (5gr, n=3500)libtextcat (n=400)LangDetectwhatlang (n=3500)Figure 2: Performance of the identifiers on the1366-language corpus using the gamma mapping.2468101214160.0 0.5 1.0 1.5 2.0Error Rate(%)Gammamguesser (n=1500)YALI (5gr, n=3500)libtextcat (n=400)LangDetectwhatlang (n=3500)Figure 3: Performance of the identifiers on the781-language corpus using the gamma mapping.identifier using four highly-divergent algorithmsfor computing model scores from n-gram statis-tics.
Improvements range from small ?
2.6% re-duction in classification errors ?
to dramatic forthe three programs with the worst baselines ?
65.2to 76.7% reduction in errors on the smaller cor-pus and 72.4 to 83.9% on the larger.
While bothmappings have similar performance for four of theprograms, libtextcat only benefits from thegamma mapping, as it can also reduce n-gramscores, unlike the loglike mapping.2468101214160.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0Error Rate(%)Taumguesser (n=1500)YALI (5gr, n=3500)libtextcat (n=400)LangDetectwhatlang (n=3500)Figure 4: Performance of the identifiers on the1366-language corpus using the loglike mapping.2468101214160.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0Error Rate(%)Taumguesser (n=1500)YALI (5gr, n=3500)libtextcat (n=400)LangDetectwhatlang (n=3500)Figure 5: Performance of the identifiers on the781-language corpus using the loglike mapping.Training data, source code, and supple-mentary information may be downloaded fromhttp://www.cs.cmu.edu/?ralf/langid.html.Future work includes modifying additional lan-guage identifiers such as langid.py (Lui andBaldwin, 2012) and VarClass (Zampieri andGebre, 2014), experimenting with other mappingfunctions, and investigating the method?s efficacyon pluricentric languages like those VarClass isdesigned to identify.631ReferencesAlexander Barkov.
2008. mguesser ver-sion 0.4. http://www.mnogosearch.org/guesser/-mguesser-0.4.tar.gz (accessed 2014-08-19).Ralf D. Brown.
2012.
Finding and Identifying Text in900+ Languages.
Digital Investigation, 9:S34?S43.Ralf D. Brown.
2013.
Selecting and Weighting N-Grams to Identify 1100 Languages.
In Proceedingsof Text, Speech, and Discourse 2013, September.Ralf Brown.
2014a.
Language-Aware String Extractor,August.
https://sourceforge.net/projects/la-strings/(accessed 2014-08-19).Ralf D. Brown.
2014b.
LTI LangID Corpus, Release1.
http://www.cs.cmu.edu/?ralf/langid.html.William B. Cavnar and John M. Trenkle.
1994.
N-Gram-Based Text Categorization.
In Proceedingsof SDAIR-94, 3rd Annual Symposium on DocumentAnalysis and Information Retrieval, pages 161?175.UNLV Publications/Reprographics, April.Ted Dunning.
1994.
Statistical Identification of Lan-guage.
Technical Report MCCS 94-273, New Mex-ico State University.Binyam Gebrekidan Gebre, Marcos Zampieri, PeterWittenburg, and Tom Heskes.
2013.
Improving Na-tive Language Identification with TF-IDF Weight-ing.
In Proceedings of the 8th NAACL Workshop onInnovative Use of NLP for Building Educational Ap-plications (BEA8).Bernard Hugueney.
2011. libtextcat 2.2-9: Faster Unicode-focused C++ reimplementationof libtextcat.
https://github.com/scientific-coder/-libtextcat (accessed 2014-08-19).Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofthe Tenth Machine Translation Summit (MT SummixX), pages 79?86.Marco Lui and Timothy Baldwin.
2012. langid.py: AnOff-the-shelf Language Identification Tool.
In Pro-ceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-2012),pages 25?30, July.Martin Majlis.
2012.
Yet Another Language Identi-fier.
In Proceedings of the Student Research Work-shop at the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 46?54, Avignon, France, April.
Associationfor Computational Linguistics.Charles Poynton.
1998.
The Rehabilitation ofGamma.
In Human Vision and Electronic Imag-ing III, Proceedings of SPIE/IS&T Conference3299, January.
http://www.poynton.com/PDFs/-Rehabilitation of gamma.pdf.Nakatani Shuyo.
2014.
Language Detection Li-brary for Java, March.
http://code.google.com/p/-language-detection/ (accessed 2014-08-19).John Vogel and David Tresner-Kirsch.
2012.
RobustLanguage Identification in Short, Noisy Texts: Im-provements to LIGA.
In Proceedings of the ThirdInternational Workshop on Mining Ubiquitous andSocial Environments (MUSE 2012), pages 43?50,September.Marcos Zampieri and Binyam Gebrekidan Gebre.2014.
VarClass: An Open Source Language Iden-tification Tool for Language Varieties.
In Proceed-ings of the Ninth International Language Resourcesand Evaluation Conference (LREC 2014), Reyk-javik, Iceland, May.George Kingsley Zipf.
1935.
The Psycho-biology ofLanguage: An Introduction to Dynamic Philology.Houghton-Mifflin Co., Boston.632
