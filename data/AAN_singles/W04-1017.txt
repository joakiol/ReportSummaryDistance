Event-Based Extractive SummarizationElena FilatovaDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USAfilatova@cs.columbia.eduVasileios HatzivassiloglouCenter for Computational Learning SystemsColumbia UniversityNew York, NY 10027, USAvh@cs.columbia.eduAbstractMost approaches to extractive summarization definea set of features upon which selection of sentencesis based, using algorithms independent of the fea-tures themselves.
We propose a new set of featuresbased on low-level, atomic events that describe rela-tionships between important actors in a document orset of documents.
We investigate the effect this newfeature has on extractive summarization, comparedwith a baseline feature set consisting of the wordsin the input documents, and with state-of-the-artsummarization systems.
Our experimental resultsindicate that not only the event-based features of-fer an improvement in summary quality over wordsas features, but that this effect is more pronouncedfor more sophisticated summarization methods thatavoid redundancy in the output.1 IntroductionThe main goal of extractive summarization can beconcisely formulated as extracting from the inputpieces of text which contain the information aboutthe most important concepts mentioned in the inputtext or texts.
This definition conceals a lot of impor-tant issues that should be taken into considerationin the process of summary construction.
First, it isnecessary to identify the important concepts whichshould be described in the summary.
When thoseimportant concepts are identified then the processof summarization can be presented as:1.
Break the input text into textual units (sen-tences, paragraphs, etc.).2.
See what concepts each textual unit covers.3.
Choose a particular textual unit for the outputaccording to the concepts present in all textualunits.4.
Continue choosing textual units until reachingthe desired length of the summary.Some current summarization systems add a clus-tering step, substituting the analysis of all the textualunits by the analysis of representative units fromeach cluster.
Clustering is helpful for avoiding rep-etitions in the summary.In this paper we propose a new representationfor concepts and correspondingly a new feature onwhich summarization can be based.
We adapt thealgorithm we proposed earlier (Filatova and Hatzi-vassiloglou, 2003) for assigning to each sentencea list of low-level, atomic events.
These eventscapture information about important named entitiesfor the input text or texts, and the relationships be-tween these named entities.
We also discuss a gen-eral model which treats summarization as a three-component problem, involving the identification ofthe textual units into which the input text shouldbe broken and which are later used as the con-stituent parts of the final summary, the textual fea-tures which are associated with the important con-cepts described in the input text, and the appropri-ate algorithm for selecting the textual units to be in-cluded into the summary.We focus on the latter two of those steps and ex-plore interdependencies between the choice of fea-tures (step 2) and selection algorithm (step 3).
Weexperimentally test our hypothesis that event-basedfeatures are helpful for summarization by compar-ing the performance of three sentence selection al-gorithms when we use such features versus the casewhere we use another, widely used set of textualfeatures: the words in the input texts, weighted bytheir tf*idf scores.
The results establish that forthe majority of document sets in our test collection,events outperform tf*idf for all algorithms consid-ered.
Furthermore, we show that this benefit is morepronounced when the selection algorithm includessteps to address potential repetition of informationin the output summary.2 General Summarization ModelMany summarization systems (e.g., (Teufel andMoens, 1997; McKeown et al, 1999; Lin and Hovy,2000)) include two levels of analysis: the sentencelevel, where every textual unit is scored according toc1 c2 c3 c4 c5t1 1 1 0 1 1t2 1 0 0 1 0t3 0 1 0 0 1t4 1 0 1 1 1Table 1: Matrix for Summarization Modelthe concepts or features it covers, and the text level,where, before being added to the final output, tex-tual units are compared to each other on the basis ofthose features.In Section 1 we presented a four-step pipelinefor extractive summarization; existing summariza-tion systems largely follow this pipeline, althoughthey introduce different approaches for every stepin it.
We suggest a model that describes the extrac-tive summarization task in general terms.
Considerthe matrix in Table 1.Rows of this matrix represent all textual units intowhich the input text is divided.
Columns representthe concepts discovered for the input text.
Everyconcept is either absent or present in a given textualunit.
Each concept ci has also an associated weightwi indicating the importance of this concept.
Theseweights can be used for scoring the textual units.Thus, the input text and the important informa-tion in it is mapped onto an m?n matrix.
Using theabove matrix it is possible to formulate the extrac-tive summarization problem as extracting the mini-mal amount of textual units which cover all the con-cepts that are interesting or important.
To accountfor the cost of long summaries, we can constrain thetotal length of the summary, or balance it against thetotal weight of covered concepts.The presented model can be also used for com-paring summaries consisting of different textualunits.
For example, a summary consisting only oftextual unit t1 renders the same information as thesummary consisting of textual units t2 and t3.
Boththese summaries cover the same set of concepts,namely c1, c2 and c3.
We explore properties ofthis model in more detail in (Filatova and Hatzivas-siloglou, 2004).3 Associating Concepts with FeaturesBefore extracting a summary, it is necessary to de-fine what concepts in the input text are importantand should be covered by the output text.
There isno exact definition or even agreement between dif-ferent approaches on what an important concept is.In order to use the model of Section 2 one has toapproximate the notion of ?concept?
with some tex-tual features.Current summarization approaches use text fea-tures which give high scores to the textual units thatcontain important information, and low scores tothose textual units which are not highly likely tocontain information worth to be included in the finaloutput.There exist approaches that deal mainly with lex-ical features, like tf*idf weighing of words in theinput text(s), words used in the titles and sectionheadings (Luhn, 1958; Edmundson, 1968), or thepresence or absence of certain cue phrases like sig-nificant, important, and in conclusion (Kupiec etal., 1995; Teufel and Moens, 1997).
Other sys-tems exploit the co-occurrence of particular con-cepts (Barzilay and Elhadad, 1997; Lin and Hovy,2000) or syntactic constraints between concepts(McKeown et al, 1999).
Concepts do not have to bedirectly observable as text snippets?they can rep-resent abstract properties that particular text unitsmay or may not satisfy, for example, status as a firstsentence in a paragraph or generally position in thesource text (Baxendale, 1958; Lin and Hovy, 1997).Some summarization systems assume that the im-portance of a sentence is derivable from a rhetoricalrepresentation of the source text (Marcu, 1997).The matrix representation of the previous sectionoffers a way to formalize the sharing of informationbetween textual units at the individual feature level.Thus, this representation is most useful for content-related concepts that should not be repeated in thesummary.
The representation can however handleindependent features such as sentence position byencoding them separately for each textual unit.4 Atomic EventsAtomic events link major constituent parts of theactions described in a text or collection of textsthrough the verbs or action nouns labeling the eventitself.
The idea behind this technique is that themajor constituent parts of events (participants, lo-cations, times) are usually realized in text as namedentities.
The more important the constituent part,the more often the corresponding named entity ismentioned.Not all the constituent parts of events need to berepresented by named entities.
For example, in anairline crash it is important to report informationabout the passengers and the crew.
These are notmarked by named entities but are highly likely to beamong the most frequently used nouns.
Thus, weadd the top ten most frequent nouns to the list ofnamed entities.We use the algorithm for atomic event extractionproposed in (Filatova and Hatzivassiloglou, 2003).It involves the following steps:1.
Analyze each input sentence1 one at a time; ig-nore sentences that do not contain at least twonamed entities or frequent nouns.2.
Extract all the possible pairs of named enti-ties/frequent nouns in the sentence, preservingtheir order and all the words in between.
Wecall such pairs of named entities relations, andthe words in-between the named entities in arelation connectors.3.
For each relation, count how many times thisrelation is used in the input text(s).4.
Keep only connectors that are content verbsor action nouns, according to WordNet?s (Fell-baum, 1998) noun hierarchy.
For each connec-tor calculate how many times it is used for theextracted relation.After calculating the scores for all relations andall connectors within each relation, we calculatetheir normalized scores The normalized relationscore is the ratio of the count for the current rela-tion (how many times we see the relation within asentence in the input) over the overall count of allrelations.
The normalized connector score is the ra-tio of the count for the current connector (how manytimes we see this connector for the current relation)over the overall count for all connectors for this re-lation.Thus, out of the above procedural definition, anatomic event is a triplet of two named entities (orfrequent nouns) connected by a verb or an action-denoting noun.
To get a score for the atomic eventwe multiply the normalized score for the relation bythe normalized score for the connector.
The scoreindicates how important the triplet is overall.In the above approach to event detection we donot address co-reference, neither we merge togetherthe triplets which describe the same event usingparaphrases, inflected forms and syntactic variants(e.g., active/passive voice).
Our method uses rel-atively simple extraction techniques and shallowstatistics, but it is fully automatic and can serve as afirst approximation of the events in the input text(s).Our approach to defining events is not the onlyone proposed?this is a subject with substantialwork in linguistics, information retrieval, and infor-mation extraction.
In linguistics, events are oftendefined at a fine-grained level as a matrix verb or asingle action noun like ?war?
(Pustejovsky, 2000).In contrast, recent work in information retrieval1We earlier showed empirically (Filatova and Hatzivas-siloglou, 2003) that a description of a single event is usuallybound within one sentence.within the TDT framework has taken event to meanessentially ?narrowly defined topic for search?
(Al-lan et al, 1998).
Finally, for the information extrac-tion community an event represents a template of re-lationships between participants, times, and places(Marsh and Perzanowski, 1997).
It may be possibleto use these alternative models of events as a sourceof content features.We earlier established empirically (Filatova andHatzivassiloglou, 2003) that this technique foratomic event extraction is useful for delineating themajor participants and their relationships from a setof topically related input texts.
For example, from acollection of documents about an airplane crash thealgorithm assigns the highest score to atomic eventsthat link together the name of the airline, the sourceand destination airports and the day when the crashhappened through the verb crashed or its synonyms.It is thus plausible to explore the usefulness of theseevent triplets as the concepts used in the model ofSection 2.5 Textual Unit SelectionWe have formulated the problem of extractive sum-marization in terms of the matrix model, statingthat mapping concepts present in the input text ontothe textual units out of which the output is con-structed can be accomplished by extracting the min-imal amount of textual units which either covermost of the important concepts.
Every time we adda new textual unit to the output it is possible to judgewhat concepts in it are already covered in the finalsummary.
This observation can be used to avoid re-dundancy: before adding a candidate textual unit tothe output summary, we check whether it containsenough new important concepts.We describe in this section several algorithmsfor selecting appropriate textual units for the outputsummary.
These algorithms differ on whether theytake advantage of the redundancy reduction prop-erty of our model, and on whether they prioritize im-portant concepts individually or collectively.
Theyshare, however, a common property: all of them op-erate independently of the features chosen to repre-sent important concepts, and thus can be used withboth our event-based features and other feature sets.The comparison of the results allows us to empir-ically determine whether event-based features canhelp in summarization.5.1 Static Greedy AlgorithmOur first text unit selection algorithm does not sup-port any mechanism for avoiding redundant infor-mation in the summary.
Instead, it rates each textualunit independently.
Textual units are included in thesummary if and only if they cover lots of concepts.More specifically,1.
For every textual unit, calculate the weight ofthis textual unit as the sum of the weights of allthe concepts covered by this textual unit.2.
Choose the textual unit with the maximumweight and add it to the final output.3.
Continue extracting other textual units in orderof total weight till we get the summary of thedesired length.5.2 Avoiding Redundancy in the SummaryTwo popular techniques for avoiding redundancyin summarization are Maximal Marginal Relevance(MMR) (Goldstein et al, 2000) and clustering(McKeown et al, 1999).
In MMR the determinationof redundancy is based mainly on the textual over-lap between the sentence that is about to be added tothe output and the sentences that are already in theoutput.
Clustering offers an alternative: before start-ing the selection process, the summarization systemclusters the input textual units.
This step allows an-alyzing one representative unit from each cluster in-stead of all textual units.We take advantage of the model matrix of Sec-tion 2 to explore another way to avoid redundancy.Rather than making decisions for each textual unitindependently, as in our Static Greedy Algorithm,we globally select the subset of textual units thatcover the most concepts (i.e., information) presentin the input.
Then our task becomes very similar toa classic theory problem, Maximum Coverage.Given C , a finite set of weighted elements, a col-lection T of subsets of C , and a parameter k, themaximum coverage problem is to find k membersof T such that the total weight of the elements cov-ered (i.e., belonging to the k members of the solu-tion) is maximized.
This problem is NP-hard, as itcan be reduced to the well-known set cover problem(Hochbaum, 1997).
Thus, we know only approxi-mation algorithms solving this problem in polyno-mial time.Hochbaum (1997) reports that a greedy algorithmis the best possible polynomial approximation algo-rithm for this problem.
This algorithm iterativelyadds to the solution S the set ti ?
T that locallymaximizes the increase in the total weight of ele-ments covered by S ?
ti.
The algorithm gives a so-lution with weight at least 1/(1 ?
e) of the optimalsolution?s total weight.5.3 Adaptive Greedy AlgorithmThe greedy algorithm for the maximum coverageproblem is not directly applicable to summariza-tion, because the formulation of maximum cover-age assumes that any combination of k sets ti (i.e.,k sentences) is equally good as long as they coverthe same total weight of concepts.
A more realisticlimitation for the summarization task is to aim for afixed total length of the summary, rather than a fixedtotal number of sentences; this approach has beenadopted in several evaluation efforts, including theDocument Understanding Conferences (DUC).
Weconsequently modify the greedy algorithm for themaximum coverage problem to obtain the followingadaptive greedy algorithm for summarization:1.
For each textual unit calculate its weight as thesum of weights of all concepts it covers.2.
Choose the textual unit with the maximumweight and add it to the output.
Add the con-cepts covered by this textual unit to the list ofconcepts covered in the final output.3.
Recalculate the weights of the textual units:subtract from each unit?s weight the weight ofall concepts in it that are already covered in theoutput.4.
Continue extracting text units in order of theirtotal weight (going back to step 2) until thesummary is of the desired length.5.4 Modified Adaptive Greedy AlgorithmThe adaptive greedy algorithm described above pri-oritizes sentences according to the total weight ofconcepts they cover.
While this is a reasonable ap-proach, an alternative is to give increased priority toconcepts that are individually important, so that sen-tences mentioning them have a chance of being in-cluded in the output even if they don?t contain otherimportant concepts.
We have developed the fol-lowing variation of our adaptive greedy algorithm,termed the modified greedy algorithm:1.
For every textual unit calculate its weight asthe sum of weights of all concepts it covers.2.
Consider only those textual units that containthe concept with the highest weight that has notyet been covered.
Out of these, choose the onewith highest total weight and add it to the finaloutput.
Add the concepts which are covered bythis textual unit to the list of concepts coveredin the final output.3.
Recalculate the weights of the textual units:subtract from each unit?s weight the weight ofall concepts in it that are already covered in theoutput.4.
Continue extracting textual units, going backto step 2 each time, until we get a summary ofthe desired length.The modified greedy algorithm has the samemechanism for avoiding redundancy as the adaptivegreedy one, while according a somewhat differentpriority to individual sentences (weight of most im-portant concepts versus just total weight).6 ExperimentsWe chose as our input data the document setsused in the evaluation of multidocument summa-rization during the first Document UnderstandingConference (DUC), organized by NIST (Harmanand Marcu, 2001).
This collection contains 30 testdocument sets, each with approximately 10 newsstories on different events; document sets vary sig-nificantly in their internal coherence.
For each doc-ument set three human-constructed summaries areprovided for each of the target lengths of 50, 100,200, and 400 words.
We selected DUC 2001 be-cause ideal summaries are available for multiplelengths.Concepts and Textual Units Our textual unitsare sentences, while the features representing con-cepts are either atomic events, as described in Sec-tion 4, or a fairly basic and widely used set oflexical features, namely the list of words presentin each input text.
The algorithm for extractingevent triplets assigns a weight to each such triplet,while for words we used as weights their tf*idf val-ues, taking idf values from http://elib.cs.berkeley.edu/docfreq/.Evaluation Metric Given the difficulties in com-ing up with a universally accepted evaluation mea-sure for summarization, and the fact that obtain-ing judgments by humans is time-consuming andlabor-intensive, we adopted an automated pro-cess for comparing system-produced summaries to?ideal?
summaries written by humans.
The method,ROUGE (Lin and Hovy, 2003), is based on n-gramoverlap between the system-produced and idealsummaries.
As such, it is a recall-based measure,and it requires that the length of the summaries becontrolled to allow meaningful comparisons.ROUGE can be readily applied to compare theperformance of different systems on the same setof documents, assuming that ideal summaries areavailable for those documents.
At the same time,ROUGE evaluation has not yet been tested exten-sively, and ROUGE scores are difficult to interpretas they are not absolute and not comparable acrosssource document sets.50 100 200 400events better 53.3% 63.3% 80.0% 80.0%tf*idf better 23.3% 26.7% 20.0% 20.0%equal 23.3% 10.0% 0.0% 0.0%Table 2: Static greedy algorithm, events versustf*idf00.050.10.150.20.250.30.350.40.451 3 5 7 9 11 13 15 17 19 21 23 25 27 29DUC document setsROUGEscoreseventstf*idfFigure 1: ROUGE scores for 400-word summariesfor static greedy algorithm, events versus tf*idf50 100 200 400events better 53.3% 66.7% 86.7% 80.0%tf*idf better 23.3% 20.0% 13.3% 20.0%equal 23.3% 13.3% 0.0% 0.0%Table 3: Adaptive greedy algorithm, events versustf*idfIn our comparison, we used as reference sum-maries those created by NIST assessors for the DUCtask of generic summarization.
The human annota-tors may not have created the same models if askedfor summaries describing the major events in the in-put texts instead of generic summaries.Summary Length For a given set of features andselection algorithm we get a sorted list of sen-tences extracted according to that particular algo-rithm.
Then, for each DUC document set we createfour summaries of length 50, 100, 200, and 400.
Inall the suggested methods a whole sentence is addedat every step.
We extracted exactly 50, 100, 200,and 400 words out of the top sentences (truncatingthe last sentence if necessary).6.1 Results: Static Greedy AlgorithmIn our first experiment we use the static greedy al-gorithm to create summaries of various lengths.
Ta-ble 2 shows in how many cases out of the 30 docu-ment sets the summary created according to atomicevents receives a higher or lower ROUGE scorethan the summary created according to tf*idf fea-tures (rows ?events better?
and ?tf*idf better?
re-spectively).
Row equal indicates how many of the30 cases both systems produce results with the sameROUGE score.
We chose to report the number of00.050.10.150.20.250.30.350.40.451 3 5 7 9 11 13 15 17 19 21 23 25 27 29DUC document setsROUGEscoreseventstf*idfFigure 2: ROUGE scores for 400-word summariesfor adaptive greedy algorithm, events versus tf*idftimes each system is better rather than the averageROUGE score in each case because ROUGE scoresdepend on each particular document set.It is clear from Table 2 that the summaries cre-ated using atomic events are better in the majorityof cases than the summaries created using tf*idf.Figure 1 shows ROUGE scores for 400-word sum-maries.
Although in most cases the performance ofthe event-based summarizer is higher than the per-formance based on tf*idf scores, for some docu-ment sets tf*idf gives the better scores.
This phe-nomenon can be explained through an additionalanalysis of document sets according to their inter-nal coherence.
Atomic event extraction works bestfor a collection of documents with well-defined con-stituent parts of events and where documents areclustered around one specific major event.
For suchdocument sets atomic events are good features forbasing the summary on.
In contrast, some DUC2001 document sets describe a succession of mul-tiple events linked in time or of different events ofthe same type (e.g., Clarence Thomas?
ascendancyto the Supreme Court, document set 7 in Figure 1,or the history of airplane crashes, document set 30in Figure 1).
In such cases, a lot of different par-ticipants are mentioned with only few common ele-ments (e.g., Clarence Thomas himself).
Thus, mostof the atomic events have similar low weights andit is difficult to identify those atomic events that canpoint out the most important textual units.6.2 Results: Adaptive Greedy AlgorithmFor the second experiment we used the adaptivegreedy algorithm, which accounts for informationoverlap across sentences in the summary.
As inthe case of the simpler static greedy algorithm, weobserve that events lead to a better performance inmost document sets than tf*idf (Table 3).
Table 3is in fact similar to Table 2, with slightly increasednumbers of document sets for which events receivehigher ROUGE scores for the 100 and 200-word50 100 200 400static better 0.0% 3.3% 20.0% 23.3%adaptive better 10.0% 16.7% 26.6% 40.0%equal 90.0% 80.0% 53.3% 36.7%Table 4: Adaptive greedy algorithm versus staticgreedy algorithm, using events as features-0.15-0.1-0.0500.050.10.150.21 3 5 7 9 11 13 15 17 19 21 23 25 27 29DUC document setsROUGEscoregainadaptivestaticFigure 3: Gain in ROUGE scores (400-word sum-maries) when using events instead of tf*idf for thestatic and adaptive greedy algorithms50 100 200 400static better 3.3% 26.7% 43.3% 50.0%adaptive better 3.3% 13.3% 30.0% 50.0%equal 93.3% 60.0% 26.7% 0.0%Table 5: Adaptive greedy algorithm versus staticgreedy algorithm, using tf*idf as featuressummaries.
It is interesting to see that the differ-ence between the ROUGE scores for the summariz-ers based on atomic events and tf*idf features be-comes more distinct when the adaptive greedy al-gorithm is used; Figure 2 shows this for 400-wordsummaries.As Table 4 shows, the usage of the adaptivegreedy algorithm improves the performance of asummarizer based on atomic events in comparisonto the static greedy algorithm.
In contrast, the re-verse is true when tf*idf is used (Table 5).
Figure 3shows the change in ROUGE scores that the intro-duction of the adaptive algorithm offers for 400-word summaries.
This indicates that tf*idf is notcompatible with our information redundancy com-ponent; a likely explanation is that words are corre-lated, and the presence of an important word makesother words in the same sentence also potentiallyimportant, a fact not captured by the tf*idf feature.Events, on the other hand, exhibit less of a depen-dence on each other, since each triplet captures aspecific interaction between two entities.6.3 Results: Modified Greedy AlgorithmIn the case of the modified adaptive greedy algo-rithm we see improvement in performance in com-50 100 200 400static better 43.3% 43.3% 36.7% 43.3%modified better 43.3% 56.7% 63.3% 56.7%equal 13.3% 0.0% 0.0% 0.0%Table 6: Modified adaptive greedy algorithm versusstatic greedy algorithm, using events as features50 100 200 400static better 6.7% 26.7% 36.7% 26.7%modified better 30.0% 40.0% 56.7% 73.3%equal 63.3% 33.3% 6.7% 0.0%Table 7: Modified adaptive greedy algorithm versusstatic greedy algorithm, using tf*idf as features50 100 200 400events better 56.7% 70.0% 80.0% 66.6%tf*idf better 33.3% 30.0% 20.0% 33.3%equal 10.0% 0.0% 0.0% 0.0%Table 8: Modified adaptive greedy algorithm, eventsversus tf*idfparison with the summarizers using the static greedyalgorithm for both events and tf*idf (Tables 6 and7).
In other words, the prioritization of individ-ual important concepts addresses the correlation be-tween words and allows the summarizer to benefitfrom redundancy reduction even when using tf*idfas the features.
The modified adaptive algorithm of-fers a slight improvement in ROUGE scores overthe unmodified adaptive algorithm.
Also, as Table 8makes clear, events remain the better feature choiceover tf*idf.6.4 Results: Comparison with DUC systemsFor our final experiment we used the 30 test doc-ument sets provided for DUC 2003 competition,for which the summaries produced by participat-ing summarization systems were also released.
InDUC 2003 the task was to create summaries only oflength 100.We calculated ROUGE scores for the releasedsummaries created by DUC participants and com-pared them to the scores of our system with atomicevents as features and adaptive greedy algorithm asthe filtering method.
In 14 out of 30 cases our sys-tem outperforms the median of the scores of all the15 participating systems over that specific documentset.
We view this comparison as quite encourag-ing, as our system does not employ any of the ad-ditional features (such as sentence position or timeinformation) used by the best DUC summarizationsystems, nor was it adapted to the DUC domain.Again, the suitability (and relative performance) ofthe event-based summarizer varies according to thetype of documents being summarized, indicatingthat using our approach for a subset of documentsets is more appropriate.
For example, our systemscored below all the other systems for the docu-ment set about a meteor shower, which included alot of background information and no well-definedconstituents of events.
On the contrary, our sys-tem performed better than any DUC system for thedocument set describing an abortion-related murder,where it was clear who was killed and where andwhen it happened.7 ConclusionWe have introduced atomic events as a feature thatcan be automatically extracted from text and usedfor summarization, and described algorithms thatutilize this feature to select sentences for the sum-mary while minimizing the overlap of informationin the output.
Our experimental results indicate thatevents are indeed an effective feature, at least incomparison with words in the input texts that formthe basis of many of current summarizers?
featuresets.
With all three of our summarization algo-rithms, we achieved a gain in performance whenusing events.
This gain was actually more pro-nounced with the more sophisticated sentence se-lection methods, establishing that events also ex-hibit less interdependence than features based di-rectly on words.
The advantage was also larger inlonger summaries.Our approach to defining and extracting eventscan be improved in many ways.
We are currentlylooking at ways of matching connectors that aresimilar in meaning, representing paraphrases of thesame event, and methods for detecting and prioritiz-ing special event components such as time and loca-tion phrases.
We are also considering merging infor-mation across many related atomic events to a morestructured representation for each event, and allow-ing for partial matches between such structures andinput sentences.8 AcknowledgementsWe wish to thank Rocco Servedio and MihalisYannakakis for valuable discussions of theoreti-cal foundations of the set cover problem.
Wealso thank Kathy McKeown and Noemie Elhadadfor comments on an earlier version.
This workwas supported by ARDA under Advanced QuestionAnswering for Intelligence (AQUAINT) projectMDA908-02-C-0008.
Any opinions, findings, orrecommendations are those of the authors.ReferencesJames Allan, Jaime Carbonell, George Dodding-ton, Jonathan Yamron, and Yiming Yang.
1998.Topic detection and tracking plot study: Final re-port.
In Proceedings of the DARPA BroadcastNews Transscription Workshop, April.Regina Barzilay and Michael Elhadad.
1997.
Us-ing lexical chains for text summarization.
In Pro-ceedings of the ACL/EACL 1997 Workshop onIntelligent Scalable Text Summarizaion, Madrid,Spain, July.P.
B. Baxendale.
1958.
Machine-made index fortechnical literature?An experiment.
IBM Jour-nal of Research and Development, 2:354?361.H.
P. Edmundson.
1968.
New methods in automaticextracting.
Journal of the Association for Com-puting Machinary, 23(1):264?285, April.Christiane Fellbaum, editor.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press.Elena Filatova and Vasileios Hatzivassiloglou.2003.
Domain-independent detection, extraction,and labeling of atomic events.
In Proceedingsof RANLP, pages 145?152, Borovetz, Bulgaria,September.Elena Filatova and Vasileios Hatzivassiloglou.2004.
A formal model for information selectionin multi-sentence text extraction.
In Proceedingsof COLING, Geneva, Switzerland, August.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, andJamie Callan.
2000.
Creating and evaluatingmulti-document sentence extract summaries.
InProceedings of the 9th CIKM Conference, pages165?172.Donna Harman and Daniel Marcu, editors.
2001.Proceedings of the Document UnderstandingConference (DUC).
NIST, New Orleans, USA,September.Dorit S. Hochbaum.
1997.
Approximating cov-ering and packing problems: Set cover, vertexcover, independent set, and related problems.
InDorit S. Hochbaum, editor, Approximation Al-gorithms for NP-hard Problems, pages 94?143.PWS Publishing Company, Boston, MA.Julian Kupiec, Jan Pedersen, and Francine Chen.1995.
A trainable document summarizer.
In Pro-ceedings of the 18th ACM SIGIR Conference,pages 68?73, Seattle, Washington, May.Chin-Yew Lin and Eduard Hovy.
1997.
Identify-ing topic by position.
In Proceedings of the 5thANLP Conference, Washington, DC.Chin-Yew Lin and Eduard Hovy.
2000.
The au-tomated acquisition of topic signatures for textsummarization.
In Proceedings of the COLINGConference, Saarbru?cken, Germany, July.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of HLT-NAACL, Edmonton, Canada, May.H.
P. Luhn.
1958.
The automatic creation of lit-erature abstracts.
IBM Journal of Research andDevelopment, 2(2):159?165, April.Daniel Marcu.
1997.
From discourse struc-tures to text summaries.
In Proceedings of theACL/EACL 1997 Workshop on Intelligent Scal-able Text Summarizaion, pages 82?88, Madrid,Spain, July.E.
Marsh and D. Perzanowski.
1997.
MUC-7 eval-uation of IE technology: Overview of results.
InProceedings of MUC-7.Kathleen R. McKeown, Judith L. Klavans, VasileiosHatzivassiloglou, Regina Barzilay, and EleazarEskin.
1999.
Towards multidocument sum-marization by reformulation: Progress andprospects.
In Proceedings of AAAI.James Pustejovsky, 2000.
Events and the Seman-tics of Opposition, pages 445?482.
CSLI Publi-cations.Simone Teufel and Marc Moens.
1997.
Sentenceextraction as a classification task.
In Proceed-ings of the ACL/EACL 1997 Workshop on Intelli-gent Scalable Text Summarization, pages 58?65,Madrid, Spain, July.
