Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 825?834,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsBig Data versus the Crowd:Looking for Relationships in All the Right PlacesCe Zhang Feng Niu Christopher Re?
Jude ShavlikDepartment of Computer SciencesUniversity of Wisconsin-Madison, USA{czhang,leonn,chrisre,shavlik}@cs.wisc.eduAbstractClassically, training relation extractors relieson high-quality, manually annotated trainingdata, which can be expensive to obtain.
Tomitigate this cost, NLU researchers have con-sidered two newly available sources of lessexpensive (but potentially lower quality) la-beled data from distant supervision and crowdsourcing.
There is, however, no study com-paring the relative impact of these two sourceson the precision and recall of post-learning an-swers.
To fill this gap, we empirically studyhow state-of-the-art techniques are affected byscaling these two sources.
We use corpus sizesof up to 100 million documents and tens ofthousands of crowd-source labeled examples.Our experiments show that increasing the cor-pus size for distant supervision has a statis-tically significant, positive impact on quality(F1 score).
In contrast, human feedback has apositive and statistically significant, but lower,impact on precision and recall.1 IntroductionRelation extraction is the problem of populating atarget relation (representing an entity-level relation-ship or attribute) with facts extracted from natural-language text.
Sample relations include people?s ti-tles, birth places, and marriage relationships.Traditional relation-extraction systems rely onmanual annotations or domain-specific rules pro-vided by experts, both of which are scarce re-sources that are not portable across domains.
Toremedy these problems, recent years have seen in-terest in the distant supervision approach for rela-tion extraction (Wu and Weld, 2007; Mintz et al,2009).
The input to distant supervision is a set ofseed facts for the target relation together with an(unlabeled) text corpus, and the output is a set of(noisy) annotations that can be used by any ma-chine learning technique to train a statistical modelfor the target relation.
For example, given the tar-get relation birthPlace(person, place) and a seedfact birthPlace(John, Springfield), the sentence?John and his wife were born in Springfield in 1946?
(S1) would qualify as a positive training example.Distant supervision replaces the expensive pro-cess of manually acquiring annotations that is re-quired by direct supervision with resources that al-ready exist in many scenarios (seed facts and atext corpus).
On the other hand, distantly labeleddata may not be as accurate as manual annotations.For example, ?John left Springfield when he was16?
(S2) would also be considered a positive ex-ample about place of birth by distant supervisionas it contains both John and Springfield.
The hy-pothesis is that the broad coverage and high redun-dancy in a large corpus would compensate for thisnoise.
For example, with a large enough corpus, adistant supervision system may find that patterns inthe sentence S1 strongly correlate with seed facts ofbirthPlace whereas patterns in S2 do not qualifyas a strong indicator.
Thus, intuitively the quality ofdistant supervision should improve as we use largercorpora.
However, there has been no study on theimpact of corpus size on distant supervision for re-lation extraction.
Our goal is to fill this gap.Besides ?big data,?
another resource that maybe valuable to distant supervision is crowdsourc-825ing.
For example, one could employ crowd work-ers to provide feedback on whether distant super-vision examples are correct or not (Gormley et al,2010).
Intuitively the crowd workforce is a perfectfit for such tasks since many erroneous distant la-bels could be easily identified and corrected by hu-mans.
For example, distant supervision may mistak-enly consider ?Obama took a vacation in Hawaii?
apositive example for birthPlace simply becausea database says that Obama was born in Hawaii;a crowd worker would correctly point out that thissentence is not actually indicative of this relation.It is unclear however which strategy one shoulduse: scaling the text corpus or the amount of humanfeedback.
Our primary contribution is to empiricallyassess how scaling these inputs to distant supervi-sion impacts its result quality.
We study this ques-tion with input data sets that are orders of magnitudelarger than those in prior work.
While the largestcorpus (Wikipedia and New York Times) employedby recent work on distant supervision (Mintz et al,2009; Yao et al, 2010; Hoffmann et al, 2011) con-tain about 2M documents, we run experiments ona 100M-document (50X more) corpus drawn fromClueWeb.1 While prior work (Gormley et al, 2010)on crowdsourcing for distant supervision used thou-sands of human feedback units, we acquire tens ofthousands of human-provided labels.
Despite thelarge scale, we follow state-of-the-art distant super-vision approaches and use deep linguistic features,e.g., part-of-speech tags and dependency parsing.2Our experiments shed insight on the followingtwo questions:1.
How does increasing the corpus size impact thequality of distant supervision?2.
For a given corpus size, how does increasingthe amount of human feedback impact the qual-ity of distant supervision?We found that increasing corpus size consistentlyand significantly improves recall and F1, despite re-ducing precision on small corpora; in contrast, hu-man feedback has relatively small impact on preci-sion and recall.
For example, on a TAC corpus with1.8M documents, we found that increasing the cor-pus size ten-fold consistently results in statistically1http://lemurproject.org/clueweb09.php/2We used 100K CPU hours to run such tools on ClueWeb.significant improvement in F1 on two standardizedrelation extraction metrics (t-test with p=0.05).
Onthe other hand, increasing human feedback amountten-fold results in statistically significant improve-ment on F1 only when the corpus contains at least1M documents; and the magnitude of such improve-ment was only one fifth compared to the impact ofcorpus-size increment.We find that the quality of distant supervisiontends to be recall gated, that is, for any given rela-tion, distant supervision fails to find all possible lin-guistic signals that indicate a relation.
By expandingthe corpus one can expand the number of patternsthat occur with a known set of entities.
Thus, as arule of thumb for developing distant supervision sys-tems, one should first attempt to expand the trainingcorpus and then worry about precision of labels onlyafter having obtained a broad-coverage corpus.Throughout this paper, it is important to under-stand the difference between mentions and entities.Entities are conceptual objects that exist in the world(e.g., Barack Obama), whereas authors use a varietyof wordings to refer to (which we call ?mention?
)entities in text (Ji et al, 2010).2 Related WorkThe idea of using entity-level structured data (e.g.,facts in a database) to generate mention-level train-ing data (e.g., in English text) is a classic one: re-searchers have used variants of this idea to extractentities of a certain type from webpages (Hearst,1992; Brin, 1999).
More closely related to relationextraction is the work of Lin and Patel (2001) thatuses dependency paths to find answers that expressthe same relation as in a question.Since Mintz et al (2009) coined the name ?dis-tant supervision,?
there has been growing interest inthis technique.
For example, distant supervision hasbeen used for the TAC-KBP slot-filling tasks (Sur-deanu et al, 2010) and other relation-extractiontasks (Hoffmann et al, 2010; Carlson et al, 2010;Nguyen and Moschitti, 2011a; Nguyen and Mos-chitti, 2011b).
In contrast, we study how increas-ing input size (and incorporating human feedback)improves the result quality of distant supervision.We focus on logistic regression, but it is interest-ing future work to study more sophisticated prob-826Training ?Corpus?Testing ?Corpus?1.
Parsing, Entity Linking?Training?Testing?Raw Text?
Structured Text?w/ Entity Mentions?2.
Distant Supervision?
Statistical ?Models?Refined ?Statistical ?Models?Relation Extractors?3.
Human ?Feedback???????4.
Apply & Evaluate?Knowledge-base ?Entities?Knowledge-base ?Relations?Figure 1: The workflow of our distant supervision system.
Step 1 is preprocessing; step 4 is final evaluation.
The keysteps are distant supervision (step 2), where we train a logistic regression (LR) classifier for each relation using (noisy)examples obtained from sentences that match Freebase facts, and human feedback (step 3) where a crowd workforcerefines the LR classifiers by providing feedback to the training data.abilistic models; such models have recently beenused to relax various assumptions of distant supervi-sion (Riedel et al, 2010; Yao et al, 2010; Hoffmannet al, 2011).
Specifically, they address the noisy as-sumption that, if two entities participate in a rela-tion in a knowledge base, then all co-occurrences ofthese entities express this relation.
In contrast, weexplore the effectiveness of increasing the trainingdata sizes to improve distant-supervision quality.Sheng et al (2008) and Gormley et al (2010)study the quality-control issue for collecting train-ing labels via crowdsourcing.
Their focus is the col-lection process; in contrast, our goal is to quantifythe impact of this additional data source on distant-supervision quality.
Moreover, we experiment withone order of magnitude more human labels.
Hoff-mann et al (2009) study how to acquire end-userfeedback on relation-extraction results posted on anaugmented Wikipedia site; it is interesting futurework to integrate this source in our experiments.One technique for obtaining human input is activelearning.
We tried several active-learning techniquesas described by Settles (2010), but did not observeany notable advantage over uniform sampling-basedexample selection.33 Distant Supervision MethodologyRelation extraction is the task of identifying re-lationships between mentions, in natural-languagetext, of entities.
An example relation is that two per-sons are married, which for mentions of entities xand y is denoted R(x, y).
Given a corpus C con-3More details in our technical report (Zhang et al, 2012).taining mentions of named entities, our goal is tolearn a classifier for R(x, y) using linguistic featuresof x and y, e.g., dependency-path information.
Theproblem is that we lack the large amount of labeledexamples that are typically required to apply super-vised learning techniques.
We describe an overviewof these techniques and the methodological choiceswe made to implement our study.
Figure 1 illus-trates the overall workflow of a distant supervisionsystem.
At each step of the distant supervision pro-cess, we closely follow the recent literature (Mintzet al, 2009; Yao et al, 2010).3.1 Distant SupervisionDistant supervision compensates for a lack of train-ing examples by generating what are known assilver-standard examples (Wu and Weld, 2007).
Theobservation is that we are often able to obtain astructured, but incomplete, database D that instanti-ates relations of interest and a text corpus C that con-tains mentions of the entities in our database.
For-mally, a database is a tuple D = (E, R?)
where E isa set of entities and R?
= (R1 .
.
.
, RN ) is a tuple ofinstantiated predicates.
For example, Ri may con-tain pairs of married people.4 We use the facts in Ricombined with C to generate examples.Following recent work (Mintz et al, 2009; Yao etal., 2010; Hoffmann et al, 2011), we use Freebase5as the knowledge base for seed facts.
We use twotext corpora: (1) the TAC-KBP6 2010 corpus that4We only consider binary predicates in this work.5http://freebase.com6KBP stands for ?Knowledge-Base Population.
?827consists of 1.8M newswire and blog articles7, and(2) the ClueWeb09 corpus that is a 2009 snapshotof 500M webpages.
We use the TAC-KBP slot fill-ing task and select those TAC-KBP relations that arepresent in the Freebase schema as targets (20 rela-tions on people and organization).One problem is that relations in D are defined atthe entity level.
Thus, the pairs in such relations arenot embedded in text, and so these pairs lack thelinguistic context that we need to extract features,i.e., the features used to describe examples.
In turn,this implies that these pairs cannot be used directlyas training examples for our classifier.
To generatetraining examples, we need to map the entities backto mentions in the corpus.
We denote the relationthat describes this mapping as the relation EL(e,m)where e ?
E is an entity in the database D and m isa mention in the corpus C. For each relation Ri, wegenerate a set of (noisy) positive examples denotedR+i defined as R+i ={(m1,m2) | R(e1, e2) ?
EL(e1,m1) ?
EL(e2,m2)}As in previous work, we impose the constraint thatboth mentions (m1,m2) ?
R+i are contained in thesame sentence (Mintz et al, 2009; Yao et al, 2010;Hoffmann et al, 2011).
To generate negative ex-amples for each relation, we follow the assumptionin Mintz et al (2009) that relations are disjoint andsample from other relations, i.e., R?i = ?j 6=iR+j .3.2 Feature ExtractionOnce we have constructed the set of possible men-tion pairs, the state-of-the-art technique to generatefeature vectors uses linguistic tools such as part-of-speech taggers, named-entity recognizers, de-pendency parsers, and string features.
Followingrecent work on distant supervision (Mintz et al,2009; Yao et al, 2010; Hoffmann et al, 2011),we use both lexical and syntactic features.
Afterthis stage, we have a well-defined machine learn-ing problem that is solvable using standard super-vised techniques.
We use sparse logistic regression(`1 regularized) (Tibshirani, 1996), which is used inprevious studies.
Our feature extraction process con-sists of three steps:7http://nlp.cs.qc.cuny.edu/kbp/2010/1.
Run Stanford CoreNLP with POS tagging andnamed entity recognition (Finkel et al, 2005);2.
Run dependency parsing on TAC with the En-semble parser (Surdeanu and Manning, 2010)and on ClueWeb with MaltParser (Nivre et al,2007)8; and3.
Run a simple entity-linking system that utilizesNER results and string matching to identifymentions of Freebase entities (with types).9The output of this processing is a repository of struc-tured objects (with POS tags, dependency parse, andentity types and mentions) for sentences from thetraining corpus.
Specifically, for each pair of entitymentions (m1,m2) in a sentence, we extract the fol-lowing features F (m1,m2): (1) the word sequence(including POS tags) between these mentions afternormalizing entity mentions (e.g., replacing ?JohnNolen?
with a place holder PER); if the sequenceis longer than 6, we take the 3-word prefix and the3-word suffix; (2) the dependency path between themention pair.
To normalize, in both features we uselemmas instead of surface forms.
We discard fea-tures that occur in fewer than three mention pairs.3.3 Crowd-Sourced DataCrowd sourcing provides a cheap source of humanlabeling to improve the quality of our classifier.
Inthis work, we specifically examine feedback on theresult of distant supervision.
Precisely, we constructthe union of R+1 ?
.
.
.
R+N from Section 3.1.
Wethen solicit human labeling from Mechanical Turk(MTurk) while applying state-of-the-art quality con-trol protocols following Gormley et al (2010) andthose in the MTurk manual.10These quality-control protocols are critical to en-sure high quality: spamming is common on MTurkand some turkers may not be as proficient or care-ful as expected.
To combat this, we replicateeach question three times and, following Gormley8We did not run Ensemble on ClueWeb because we had veryfew machines satisfying Ensemble?s memory requirement.
Incontrast, MaltParser requires less memory and we could lever-age Condor (Thain et al, 2005) to parse ClueWeb with Malt-Parser within several days (using about 50K CPU hours).9We experiment with a slightly more sophisticated entity-linking system as well, which resulted in higher overall quality.The results below are from the simple entity-linking system.10http://mturkpublic.s3.amazonaws.com/docs/MTURK_BP.pdf828et al (2010), plant gold-standard questions: eachtask consists of five yes/no questions, one of whichcomes from our gold-standard pool.11 By retainingonly those answers that are consistent with this pro-tocol, we are able to filter responses that were notanswered with care or competency.
We only use an-swers from workers who display overall high consis-tency with the gold standard (i.e., correctly answer-ing at least 80% of the gold-standard questions).3.4 Statistical Modeling IssuesFollowing Mintz et al (2009), we use logistic re-gression classifiers to represent relation extractors.However, while Mintz et al use a single multi-classclassifier for all relations, Hoffman et al (2011) anduse an independent binary classifier for each individ-ual relation; the intuition is that a pair of mentions(or entities) might participate in multiple target rela-tions.
We experimented with both protocols; sincerelation overlapping is rare for TAC-KBP and therewas little difference in result quality, we focus on thebinary-classification approach using training exam-ples constructed as described in Section 3.1.We compensate for the different sizes of distantand human labeled examples by training an objec-tive function that allows to tune the weight of humanversus distant labeling.
We separately tune this pa-rameter for each training set (with cross validation),but found that the result quality was robust with re-spect to a broad range of parameter values.124 ExperimentsWe describe our experiments to test the hypothe-ses that the following two factors improve distant-supervision quality: increasing the(1) corpus size, and(2) the amount of crowd-sourced feedback.We confirm hypothesis (1), but, surprisingly, are un-able to confirm (2).
Specifically, when using logis-tic regression to train relation extractors, increasingcorpus size improves, consistently and significantly,the precision and recall produced by distant supervi-sion, regardless of human feedback levels.
Using the11We obtain the gold standard from a separate MTurk sub-mission by taking examples that at least 10 out of 11 turkersanswered yes, and then negate half of these examples by alter-ing the relation names (e.g., spouse to sibling).12More details in our technical report (Zhang et al, 2012).methodology described in Section 3, human feed-back has limited impact on the precision and recallproduced from distant supervision by itself.4.1 Evaluation MetricsJust as direct training data are scarce, ground truthfor relation extraction is scarce as well.
As a result,prior work mainly considers two types of evaluationmethods: (1) randomly sample a small portion ofpredictions (e.g., top-k) and manually evaluate pre-cision/recall; and (2) use a held-out portion of seedfacts (usually Freebase) as a kind of ?distant?
groundtruth.
We replace manual evaluation with a stan-dardized relation-extraction benchmark: TAC-KBP2010.
TAC-KBP asks for extractions of 46 relationson a given set of 100 entities.
Interestingly, the Free-base held-out metric (Mintz et al, 2009; Yao et al,2010; Hoffmann et al, 2011) turns out to be heavilybiased toward distantly labeled data (e.g., increasinghuman feedback hurts precision; see Section 4.6).4.2 Experimental SetupOur first group of experiments use the 1.8M-docTAC-KBP corpus for training.
We exclude from itthe 33K documents that contain query entities inthe TAC-KBP metrics.
There are two key param-eters: the corpus size (#docs) M and human feed-back budget (#examples) N .
We perform differentlevels of down-sampling on the training corpus.
OnTAC, we use subsets with M = 103, 104, 105, and106 documents respectively.
For each value of M ,we perform 30 independent trials of uniform sam-pling, with each trial resulting in a training corpusDMi , 1 ?
i ?
30.
For each training corpus DMi , weperform distant supervision to train a set of logisticregression classifiers.
From the full corpus, distantsupervision creates around 72K training examples.To evaluate the impact of human feedback, werandomly sample 20K examples from the input cor-pus (we remove any portion of the corpus that isused in an evaluation).
Then, we ask three differ-ent crowd workers to label each example as eitherpositive or negative using the procedure described inSection 3.3.
We retain only credible answers usingthe gold-standard method (see Section 3.3), and usethem as the pool of human feedback that we run ex-periments with.
About 46% of our human labels arenegative.
Denote by N the number of examples that829Figure 2: Impact of input sizes under the TAC-KBP metric, which uses documents mentioning 100 predefined entitiesas testing corpus with entity-level ground truth.
We vary the sizes of the training corpus and human feedback whilemeasuring the scores (F1, recall, and precision) on the TAC-KBP benchmark.we want to incorporate human feedback for; we varyN in the range of 0, 10, 102, 103, 104, and 2 ?
104.For each selected corpus and value of N , we per-form without-replacement sampling from examplesof this corpus to select feedback for up to N exam-ples.
In our experiments, we found that on aver-age an M -doc corpus contains about 0.04M distantlabels, out of which 0.01M have human feedback.After incorporating human feedback, we evaluatethe relation extractors on the TAC-KBP benchmark.We then compute the average F1, recall, and preci-sion scores among all trials for each metric and each(M,N) pair.
Besides the KBP metrics, we also eval-uate each (M,N) pair using Freebase held-out data.Furthermore, we experiment with a much larger cor-pus: ClueWeb09.
On ClueWeb09, we vary M over103, .
.
.
, 108.
Using the same metrics, we show ata larger scale that increasing corpus size can signifi-cantly improve both precision and recall.4.3 Overall Impact of Input SizesWe first present our experiment results on the TACcorpus.
As shown in Figure 2, the F1 graph closelytracks the recall graph, which supports our earlierclaim that quality is recall gated (Section 1).
Whileincreasing the corpus size improves F1 at a roughlylog-linear rate, human feedback has little impact un-til both corpus size and human feedback size ap-proch maximum M,N values.
Table 1 shows thequality comparisons with minimum/maximum val-ues of M and N .13 We observe that increasing thecorpus size significant improves per-relation recall13When the corpus size is small, the total number of exam-ples with feedback can be smaller than the budget size N ?
forexample, when M = 103 there are on average 10 exampleswith feedback even if N = 104.M = 103 M = 1.8?
106N = 0 0.124 0.201N = 2?
104 0.118 0.214Table 1: TAC F1 scores with max/min values of M /N .and F1 on 17 out of TAC-KBP?s 20 relations; in con-trast, human feedback has little impact on recall, andonly significantly improves the precision and F1 of9 relations ?
while hurting F1 of 2 relations (i.e.,MemberOf and LivesInCountry).14(a) Impact of corpus size changes.M\N 0 10 102 103 104 2e4103 ?
104 + + + + + +104 ?
105 + + + + + +105 ?
106 + + + + + +106 ?
1.8e6 0 0 0 + + +(b) Impact of feedback size changes.N\M 103 104 105 106 1.8e60?
10 0 0 0 0 010?
102 0 0 0 + +102 ?
103 0 0 0 + +103 ?
104 0 0 0 0 +104 ?
2e4 0 0 0 0 -0?
2e4 0 0 0 + +Table 2: Two-tail t-test with d.f.=29 and p=0.05 on theimpact of corpus size and feedback size changes respec-tively.
(We also tried p=0.01, which resulted in changeof only a single cell in the two tables.)
In (a), each col-umn corresponds to a fixed human-feedback budget sizeN .
Each row corresponds to a jump from one corpus size(M ) to the immediate larger size.
Each cell value indi-cates whether the TAC F1 metric changed significantly:+ (resp.
-) indicates that the quality increased (resp.
de-creased) significantly; 0 indicates that the quality did notchange significantly.
Table (b) is similar.14We report more details on per-relation quality in our tech-nical report (Zhang et al, 2012).830(a) Impact of corpus size changes.
(b) Impact of human feedback size.Figure 3: Projections of Figure 2 to show the impact of corpus size and human feedback amount on TAC-KBP F1,recall, and precision.4.4 Impact of Corpus SizeIn Figure 3(a) we plot a projection of the graphsin Figure 2 to show the impact of corpus size ondistant-supervision quality.
The two curves corre-spond to when there is no human feedback and whenwe use all applicable human feedback.
The factthat the two curves almost overlap indicates that hu-man feedback had little impact on precision or re-call.
On the other hand, the quality improvementrate is roughly log-linear against the corpus size.Recall that each data point in Figure 2 is the aver-age from 30 trials.
To measure the statistical signif-icance of changes in F1, we calculate t-test resultsto compare adjacent corpus size levels given eachfixed human feedback level.
As shown in Table 2(a),increasing the corpus size by a factor of 10 consis-tently and significantly improves F1.
Although pre-cision decreases as we use larger corpora, the de-creasing trend is sub-log-linear and stops at around100K docs.
On the other hand, recall and F1 keepincreasing at a log-linear rate.4.5 Impact of Human FeedbackFigure 3(b) provides another perspective on the re-sults under the TAC metric: We fix a corpus sizeand plot the F1, recall, and precision as functionsof human-feedback amount.
Confirming the trendin Figure 2, we see that human feedback has littleFigure 4: TAC-KBP quality of relation extractors trainedusing different amounts of human labels.
The horizontallines are comparison points.impact on precision or recall with both corpus sizes.We calculate t-tests to compare adjacent humanfeedback levels given each fixed corpus size level.Table 2(b)?s last row reports the comparison, for var-ious corpus sizes (and, hence, number of distant la-bels), of (i) using no human feedback and (ii) usingall of the human feedback we collected.
When thecorpus size is small (fewer than 105 docs), humanfeedback has no statistically significant impact onF1.
The locations of +?s suggest that the influenceof human feedback becomes notable only when thecorpus is very large (say with 106 docs).
However,comparing the slopes of the curves in Figure 3(b)against Figure 3(a), the impact of human feedbackis substantially smaller.
The precision graph in Fig-ure 3(b) suggests that human feedback does not no-831Figure 5: Impact of input sizes under the Freebase held-out metric.
Note that the human feedback axis is in thereverse order compared to Figure 2.tably improve precision on either the full corpus oron a small 1K-doc corpus.
To assess the quality ofhuman labels, we train extraction models with hu-man labels only (on examples obtained from distantsupervision).
We vary the amount of human labelsand plot the F1 changes in Figure 4.
Although theF1 improves as we use more human labels, the bestmodel has roughly the same performance as thosetrained from distant labels (with or without humanlabels).
This suggests that the accuracy of humanlabels is not substantially better than distant labels.4.6 Freebase Held-out MetricIn addition to the TAC-KBP benchmark, we also fol-low prior work (Mintz et al, 2009; Yao et al, 2010;Hoffmann et al, 2011) and measure the quality us-ing held-out data from Freebase.
We randomly par-tition both Freebase and the corpus into two halves.One database-corpus pair is used for training and theother pair for testing.
We evaluate the precision overthe 103 highest-probability predictions on the testset.
In Figure 5, we vary the size of the corpus in thetrain pair and the number of human labels; the pre-cision reaches a dramatic peak when we the corpussize is above 105 and uses little human feedback.This suggests that this Freebase held-out metric isbiased toward solely relying on distant labels alone.4.7 Web-scale CorporaTo study how a Web corpus impacts distant-supervision quality, we select the first 100M Englishwebpages from the ClueWeb09 dataset and measurehow distant-supervision quality changes as we varythe number of webpages used.
As shown in Fig-ure 6, increasing the corpus size improves F1 up toFigure 6: Impact of corpus size on the TAC-KBP qualitywith the ClueWeb dataset.107 docs (p = 0.05), while at 108 the two-tailedsignificance test reports no significant impact on F1(p = 0.05).
The dip in precision in Figure 6 from106 to either 107 or 108 is significant (p = 0.05),and it is interesting future work to perform a de-tailed error analysis.
Recall from Section 3 that topreprocess ClueWeb we use MaltParser instead ofEnsemble.
Thus, the F1 scores in Figure 6 are notcomparable to those from the TAC training corpus.5 Discussion and ConclusionWe study how the size of two types of cheaply avail-able resources impact the precision and recall of dis-tant supervision: (1) an unlabeled text corpus fromwhich distantly labeled training examples can be ex-tracted, and (2) crowd-sourced labels on trainingexamples.
We found that text corpus size has astronger impact on precision and recall than humanfeedback.
We observed that distant-supervision sys-tems are often recall gated; thus, to improve distant-supervision quality, one should first try to enlargethe input training corpus and then increase precision.It was initially counter-intuitive to us that humanlabels did not have a large impact on precision.
Onereason is that human labels acquired from crowd-sourcing have comparable noise level as distant la-bels ?
as shown by Figure 4.
Thus, techniques thatimprove the accuracy of crowd-sourced answers arean interesting direction for future work.
We used aparticular form of human input (yes/no votes on dis-tant labels) and a particular statistical model to in-corporate this information (logistic regression).
Itis interesting future work to study other types ofhuman input (e.g., new examples or features) andmore sophisticated techniques for incorporating hu-man input, as well as machine learning methods thatexplicitly model feature interactions.832AcknowledgementsWe gratefully acknowledge the support of theDefense Advanced Research Projects Agency(DARPA) Machine Reading Program under AirForce Research Laboratory (AFRL) prime contractno.
FA8750-09-C-0181.
Any opinions, findings,and conclusions or recommendations expressed inthis material are those of the author(s) and do notnecessarily reflect the view of DARPA, AFRL, orthe US government.
We are thankful for the gen-erous support from the Center for High Through-put Computing, the Open Science Grid, and MironLivny?s Condor research group at UW-Madison.
Weare also grateful to Dan Weld for his insightful com-ments on the manuscript.ReferencesS.
Brin.
1999.
Extracting patterns and relations from theworld wide web.
In Proceedings of The World WideWeb and Databases, pages 172?183.A.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, E. Hr-uschka Jr, and T. Mitchell.
2010.
Toward an architec-ture for never-ending language learning.
In Proceed-ings of the Conference on Artificial Intelligence, pages1306?1313.J.
Finkel, T. Grenager, and C. Manning.
2005.
Incorpo-rating non-local information into information extrac-tion systems by Gibbs sampling.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics, pages 363?370.M.
Gormley, A. Gerber, M. Harper, and M. Dredze.2010.
Non-expert correction of automatically gen-erated relation annotations.
In Proceedings of theNAACL HLT Workshop on Creating Speech and Lan-guage Data with Amazon?s Mechanical Turk, pages204?207.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14thConference on Computational Linguistics-Volume 2,pages 539?545.R.
Hoffmann, S. Amershi, K. Patel, F. Wu, J. Fogarty,and D.S.
Weld.
2009.
Amplifying community con-tent creation with mixed initiative information extrac-tion.
In Proceedings of the 27th international confer-ence on Human factors in computing systems, pages1849?1858.
ACM.R.
Hoffmann, C. Zhang, and D. Weld.
2010.
Learn-ing 5000 relational extractors.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics, pages 286?295.R.
Hoffmann, C. Zhang, X. Ling, L. Zettlemoyer, andD.
Weld.
2011.
Knowledge-based weak supervisionfor information extraction of overlapping relations.
InProceedings of the Annual Meeting of the Associationfor Computational Linguistics, pages 541?550.H.
Ji, R. Grishman, H.T.
Dang, K. Griffitt, and J. Ellis.2010.
Overview of the TAC 2010 knowledge basepopulation track.
In Text Analysis Conference.D.
Lin and P. Pantel.
2001.
Discovery of inference rulesfor question-answering.
Natural Language Engineer-ing, 7(4):343?360.M.
Mintz, S. Bills, R. Snow, and D. Jurafsky.
2009.
Dis-tant supervision for relation extraction without labeleddata.
In Proceedings of the Annual Meeting of the As-sociation for Computational Linguistics, pages 1003?1011.T.V.T.
Nguyen and A. Moschitti.
2011a.
End-to-end re-lation extraction using distant supervision from exter-nal semantic repositories.
In Proceeding of the AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 277?282.T.V.T.
Nguyen and A. Moschitti.
2011b.
Joint distant anddirect supervision for relation extraction.
In Proceed-ing of the International Joint Conference on NaturalLanguage Processing, pages 732?740.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-parser: A language-independent system for data-driven dependency parsing.
Natural Language Engi-neering, 13(02):95?135.S.
Riedel, L. Yao, and A. McCallum.
2010.
Modelingrelations and their mentions without labeled text.
InProceedings of the European Conference on MachineLearning and Knowledge Discovery in Databases:Part III, pages 148?163.B.
Settles.
2010.
Active learning literature survey.
Tech-nical report, Computer Sciences Department, Univer-sity of Wisconsin-Madison, USA.V.S.
Sheng, F. Provost, and P.G.
Ipeirotis.
2008.
Getanother label?
Improving data quality and data min-ing using multiple, noisy labelers.
In Proceeding ofthe 14th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 614?622.M.
Surdeanu and C. Manning.
2010.
Ensemble modelsfor dependency parsing: Cheap and good?
In Hu-man Language Technologies: The Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 649?652.M.
Surdeanu, D. McClosky, J. Tibshirani, J. Bauer, A.X.Chang, V.I.
Spitkovsky, and C. Manning.
2010.
Asimple distant supervision approach for the TAC-KBPslot filling task.
In Proceedings of Text Analysis Con-ference 2010 Workshop.833D.
Thain, T. Tannenbaum, and M. Livny.
2005.
Dis-tributed computing in practice: The Condor experi-ence.
Concurrency and Computation: Practice andExperience, 17(2-4):323?356.R.
Tibshirani.
1996.
Regression shrinkage and selectionvia the lasso.
Journal of the Royal Statistical Society.Series B (Methodological), pages 267?288.F.
Wu and D. Weld.
2007.
Autonomously semantifyingwikipedia.
In ACM Conference on Information andKnowledge Management, pages 41?50.L.
Yao, S. Riedel, and A. McCallum.
2010.
Collectivecross-document relation extraction without labelleddata.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1013?1023.C.
Zhang, F. Niu, C.
Re?, and J. Shavlik.
2012.
Bigdata versus the crowd: Looking for relationships inall the right places (extended version).
Technical re-port, Computer Sciences Department, University ofWisconsin-Madison, USA.834
