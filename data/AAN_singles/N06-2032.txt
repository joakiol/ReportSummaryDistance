Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 125?128,New York, June 2006. c?2006 Association for Computational LinguisticsStory Segmentation of Brodcast News in English, Mandarin and ArabicAndrew RosenbergComputer Science DepartmentColumbia UniversityNew York City, N.Y. 10027amaxwell@cs.columbia.eduJulia HirschbergComputer Science DepartmentColumbia UniversityNew York City, N.Y. 10027julia@cs.columbia.eduAbstractIn this paper, we present results from aBroadcast News story segmentation sys-tem developed for the SRI NIGHTIN-GALE system operating on English, Ara-bic and Mandarin news shows to provideinput to subsequent question-answeringprocesses.
Using a rule-induction algo-rithm with automatically extracted acous-tic and lexical features, we report successrates that are competitive with state-of-the-art systems on each input language.We further demonstrate that features use-ful for English and Mandarin are not dis-criminative for Arabic.1 IntroductionBroadcast News (BN) shows typically includemultiple unrelated stories, interspersed with anchorpresentations of headlines and commercials.
Tran-sitions between each story are frequently markedby changes in speaking style, speaker participation,and lexical choice.
Despite receiving a consider-able amount of attention through the Spoken Doc-ument Retrieval (SDR), Topic Detection and Track-ing (TDT), and Text Retrieval Conference: Video(TRECVID) research programs, automatic detec-tion of story boundaries remains an elusive prob-lem.
State-of-the-art story segmentation error rateson English and Mandarin BN remain fairly high andArabic is largely unstudied.
The NIGHTINGALEsystem searches a diverse news corpus to return an-swers to user queries.
For audio sources, the iden-tification of story boundaries is crucial, to segmentmaterial to be searched and to provide interpretableresults to the user.2 Related workPrevious approaches to story segmentation havelargely focused lexical features, such as word sim-ilarily (Kozima, 1993), cue phrases (Passonneauand Litman, 1997), cosine similarity of lexical win-dows (Hearst, 1997; Galley et al, 2003), and adap-tive language modeling (Beeferman et al, 1999).Segmentation of stories in BN have included someacoustic features (Shriberg et al, 2000; Tu?r et al,2001).
Work on non-English BN, generally usethis combination of lexical and acoustic measures,such as (Wayne, 2000; Levow, 2004) on Mandarin.And (Palmer et al, 2004) report results from featureselection experiments that include Arabic sources,though they do not report on accuracy.
TRECVIDhas also identified visual cues to story segmentationof video BN (cf.
(Hsu et al, 2004; Hsieh et al, 2003;Chaisorn et al, 2003; Maybury, 1998)).3 The NIGHTINGALE CorpusThe training data used for NIGHTINGALE in-cludes the TDT-4 and TDT5 corpora (Strassel andGlenn, 2003; Strassel et al, 2004).
TDT-4 in-cludes newswire text and broadcast news audioin English, Arabic and Mandarin; TDT-5 containsonly text data, and is therefore not used by oursystem.
The TDT-4 audio corpus includes 312.5hours of English Broadcast News from 450 shows,88.5 hours of Arabic news from 109 shows, and134 hours of Mandarin broadcasts from 205 shows.This material was drawn from six English newsshows ?
ABC ?World News Tonight?, CNN ?Head-line News?, NBC ?Nightly News?, Public RadioInternational ?The World?, MS-NBC ?News withBrian Williams?, and Voice of America, Englishthree Mandarin newscasts ?
China National Ra-dio, China Television System, and Voice of Amer-ica, Mandarin Chinese ?
and two Arabic newscasts?
Nile TV and Voice of America, Modern StandardArabic.
All of these shows aired between Oct. 1,2000 and Jan. 31, 2001.4 Our Features and ApproachOur story segmentation system procedure is es-sentially one of binary classification, trained on avariety of acoustic and lexical cues to the presenceor absence of story boundaries in BN.
Our classi-fier was trained using the JRip machine learning al-125gorithm, a Java implementation of the RIPPER al-gorithm of (Cohen, 1995).1 All of the cues weuse are automatically extracted.
We use as inputto our classifier three types of automatic annotationproduced by other components of the NIGHTIN-GALE system, speech recognition (ASR) transcrip-tion, speaker diarization, sentence segmentation.Currently, we assume that story boundaries occuronly at these hypothesized sentence boundaries.
Forour English corpus, this assumption is true for only47% of story boundaries; the average reference storyboundary is 9.88 words from an automatically rec-ognized sentence boundary2 .
This errorful input im-mediately limits our overall performance.For each such hypothesized sentence boundary,we extract a set of features based on the previousand following hypothesized sentences.
The classi-fier then outputs a prediction of whether or not thissentence boundary coincides with a story boundary.The features we use for story boundary predictionare divided into three types: lexical, acoustic andspeaker-dependent.The value of even errorful lexical information inidentifying story boundaries has been confirmed formany previous story segmentation systems (Beefer-man et al, 1999; Stokes, 2003)).
We include somepreviously-tested types of lexical features in our ownsystem, as well as identifying our own ?cue-word?features from our training corpus.
Our lexical fea-tures are extracted from ASR transcripts producedby the NIGHTINGALE system.
They include lexi-cal similarity scores calculated from the TextTilingalgorithm.
(Hearst, 1997), which determines the lex-ical similarity of blocks of text by analyzing the co-sine similarity of a sequence of sentences; this al-gorithm tests the likelihood of a topic boundary be-tween blocks, preferring locations between blockswhich have minimal lexical similarity.
For En-glish, we stem the input before calculating these fea-tures, using an implementation of the Porter stem-mer (Porter, 1980); we have not yet attempted toidentify root forms for Mandarin or Arabic.
We alsocalculate scores from (Galley et al, 2003)?s LCseg1JRip is implemented in the Weka (Witten et al, 1999) ma-chine learning environment.2For Mandarin and Arabic respectively, true for 69% and62% with the average distance between sentence and storyboundary of 1.97 and 2.91 words.method, a TextTiling-like approach which weightsthe cosine-similarity of a text window by an addi-tional measure of its component LEXICAL CHAINS,repetitions of stemmed content words.
We also iden-tify ?cue-words?
from our training data that we findto be significantly more likely (determined by ?2) tooccur at story boundaries within a window preceed-ing or following a story boundary.
We include asfeatures the number of such words observed within3, 5, 7 and 10 word windows before and after thecandidate sentence boundary.
For English, we in-clude the number of pronouns contained in the sen-tence, on the assumption that speakers would usemore pronouns at the end of stories than at the be-ginning.
We have not yet obtained reliable part-of-speech tagging for Arabic or Mandarin.
Finally, forall three languages, we include features that repre-sent the sentence length in words, and the relativesentence position in the broadcast.Acoustic/prosodic information has been shown tobe indicative of topic boundaries in both sponta-neous dialogs and more structured speech, such as,broadcast news (cf.
(Hirschberg and Nakatani, 1998;Shriberg et al, 2000; Levow, 2004)).
The acous-tic features we extract include, for the current sen-tence, the minimum, maximum, mean, and standarddeviation of F0 and intensity, and the median andmean absolute slope of F0 calculated over the en-tire sentence.
Additionally, we compute the first-order difference from the previous sentence of eachof these.
As a approximation of each sentence?sspeaking rate, we include the ratio of voiced 10msframes to the total number of frames in the sentence.These acoustic values were extracted from the audioinput using Praat speech analysis software(Boersma,2001).
Also, using the phone alignment informationderived from the ASR process, we calculate speak-ing rate in terms of the number of vowels per secondas an additional feature.
Under the hypothesis thattopic-ending sentences may exhibit some additionalphrase-final lenghthening, we compare the length ofthe sentence-final vowel and of the sentence-finalrhyme to average durations for that vowel and rhymefor the speaker, where speaker identify is availablefrom the NIGHTINGALE diarization component;otherwise we use unnormalized values.We also use speaker identification informationfrom the diarization component to extract some fea-126tures indicative of a speaker?s participation in thebroadcast as a whole.
We hypothesize that partici-pants in a broadcast may have different roles, suchas an anchor providing transitions between storiesand reporters beginning new stories (Barzilay et al,2000) and thus that speaker identity may serve asa story boundary indicator.
To capture such infor-mation, we include binary features answering thequestions: ?Is the speaker preceeding this boundarythe first speaker in the show?
?, ?Is this the first timethe speaker has spoken in this broadcast?
?, ?The lasttime?
?, and ?Does a speaker boundary occur at thissentence boundary??.
Also, we include the percent-age of sentences in the broadcast spoken by the cur-rent speaker.We assumed in the development of this systemthat the source of the broadcast is known, specif-ically the source language and the show identity(e. g. ABC ?World News Tonight?, CNN ?Head-line News?).
Given this information, we constructeddifferent classifiers for each show.
This type ofsource-specific modeling was shown to improve per-formance by Tu?r (2001).5 Results and DiscussionWe report the results of our system on En-glish, Mandarin and Arabic in Table 5.
All resultsuse show-specific modeling, which consistently im-proved our results across all metrics, reducing er-rors by between 10% and 30%.
In these tables, wereport the F-measure of identifying the precise lo-cation of a story boundary as well as three metricsdesigned specifically for this type of segmentationtask: the pk metric (Beeferman et al, 1999), Win-dowDiff (Pevzner and Hearst, 2002) and Cseg (Pseg= 0.3) (Doddington, 1998).
All three are derivedfrom the pk metric (Beeferman et al, 1999), and forall, lower values imply better performance.
For eachof these three metrics we let k = 5, as prescribed in(Beeferman et al, 1999).In every system, the best peforming results areachieved by including all features from the lexical,acoustic and speaker-dependent feature sets.
Acrossall languages, our precision?and false alarm rates?are better than recall?and miss rates.
We believethat inserting erroneous story boundaries will leadto more serious downstream errors in anaphora res-olution and summarization than a boundary omis-sion will.
Therefore, high precision is more impor-tant than high recall for a helpful story segmentationsystem.
In the English and Mandarin systems, thelexical and acoustic feature sets perform similarly,and combine to yield improved results.
However,on the Arabic data, the acoustic feature set performsquite poorly, suggesting that the use of vocal cues totopic transitions may be fundamentally different inArabic.
Moreover, these differences are not simplydifferences of degree or direction.
Rather, the acous-tic indicators of topic shifts in English and Man-darin are, simply, not discriminative when appliedto Arabic.
This difference may be due to the style ofArabic newscasts or to the language itself.
Acrossconfigurations, we find that the inclusion of featuresderived from automatic speaker identification (fea-ture set S), errorful as it is, significantly improvesperformance.
This improvement is particularly pro-nounced on the Mandarin material; in China NewsRadio broadcasts, story boundaries are very stronglycorrelated with speaker transitions.It is difficult to determine how well our systemperforms against state-of-the-art story segmentation.There are no comparable results for the TDT-4 cor-pus.
On the English TDT-2 corpus, (Shriberg et al,2000) report a Cseg score of 0.1438.
While our scoreof .0670 is half that, we hesitate to conclude thatour system is significantly better than this system;since the (Shriberg et al, 2000) results are based on aword-level segmentation, the discrepancy may be in-fluenced by the disparate datasets as well as the per-formance of the two systems.
On CNN and Reutersstories from the TDT-1 corpus, (Stokes, 2003) re-port a Pk score of 0.25 and a WD score of 0.253.Our Pk score is better than this on TDT-4, whileour WD score is worse.
(Chaisorn et al, 2003) re-port an F-measure of 0.532 using only audio-basedfeatures on the TRECVID 2003 corpus , which ishigher than our system, however, this allows for?correct?
boundaries to fall within 5 seconds of ref-erence boundaries.
(Franz et al, 2000) present a sys-tem which achieves Cseg scores of 0.067 and Man-darin BN and 0.081 on English audio in TDT-3.
Thissuggests that their system may be better than ourson Mandarin, and worse on English, although wetrained and tested on different corpora.
Finally, weare unaware of any reported story segmentation re-sults on Arabic BN.127Table 1: TDT-4 segmentation results.
(L=lexical feature set, A=acoustic, S=speaker-dependent)English Mandarin ArabicF1(p,r) Pk WD Cseg F1(p,r) Pk WD Cseg F1(p,r) Pk WD CsegL+A+S .421(.67,.31) .194 .318 .0670 .592(.73,.50) .179 .245 .0679 .300(.65,.19) .264 .353 .0850A+S .346(.65,.24) .220 .349 .0721 .586(.72,.49) .178 .252 .0680 .0487(.81,.03) .333 .426 .0999L+S .342(.66,.23) .231 .362 .074 .575(.72,.48) .200 .278 .0742 .285(.68,.18) .286 .372 .0884L+A .319(.66,.21) .240 .376 .0787 .294(.72,.18) .277 .354 .0886 .284(.64,.18) .257 .344 .0851L .257(.68,.16) .261 .399 .0840 .226(.74,.13) .309 .391 .0979 .286(.68,.18) .283 .349 .0849A .194(.63,.11) .271 .412 .0850 .252(.72,.18) .291 .377 .0904 .0526(.81,.03) .332 .422 .09966 ConclusionIn this paper we have presented results of ourstory boundary detection procedures on English,Mandarin, and Arabic Broadcast News from theTDT-4 corpus.
All features are obtained automati-cally, except for the identity of the news show andthe source language, information which is, however,available from the data itself, and could be automat-ically obtained.
Our performance on TDT-4 BN ap-pears to be better than previous work on earlier cor-pora of BN for English, and slightly worse than pre-vious efforts on Mandarin, again for a different cor-pus.
We believe our Arabic results to be the firstreported evaluation for BN in that language.
Oneimportant observation from our study is that acous-tic/prosodic features that correlate with story bound-aries in English and in Mandarin, do not correlatewith Arabic boundaries.
Our further research willadress the study of vocal cues to segmentation inArabic BN.AcknowledgmentsThis research was partially supported by the De-fese Advanced Research Projects Agency (DARPA)under Contract No.
HR0011-06-C-0023.ReferencesR.
Barzilay, M. Collins, J. Hirschberg, and S. Whittaker.
2000.The rules behind roles: Identifying speaker role in radiobroadcasts.
In AAAI/IAAI, 679?684.D.
Beeferman, A. Berger, and J. Lafferty.
1999.
Statistical mod-els for text segmentation.
Machine Learning, 31:177?210.P.
Boersma.
2001.
Praat, a system for doing phonetics by com-puter.
Glot International, 5(9-10):341?345.L.
Chaisorn, T. Chua, C. Koh, Y. Zhao, H. Xu, H. Feng, andQ.
Tian.
2003.
A two-level multi-modeal approach for storysegmentation of large news video corpus.
In TRECVID.W.
Cohen.
1995.
Fast effective rule induction.
In MachineLearning: Proc.
of the Twelfth International Conference,115?123.G.
Doddington.
1998.
The topic detection and tracking phase2 (tdt2) evaluation plan.
In Proccedings DARPA BroadcastNews Transcription and Understanding Workshop, 223?229.M.
Franz, J. S. McCarley, T. Ward, and W. J. Zhu.
2000.
Seg-mentation and detection at ibm: Hybrid statstical models andtwo-tiered clustering.
In Proc.
of TDT-3 Workshop.M.
Galley, K. McKeown, E. Fosler-Lussier, and H. Jing.
2003.Discourse segmentation of multi-party conversation.
In 41stAnnual Meeting of ACL, 562?569.M.
A. Hearst.
1997.
Texttiling: Segmenting text into multi-paragraph subtopic passages.
Computational Linguistics,23(1):33?64.J.
Hirschberg and C. Nakatani.
1998.
Acoustic indicators oftopic segmentation.
In Proc.
of ICSLP, 1255?1258.J.
H. Hsieh, C. H. Wu, and K. A. Fung.
2003.
Two-stage storysegmentation and detection on broadcast news using geneticalgorithm.
In Proc.
of the 2003 ISCA Workshop on Multilin-gual Spoken Document Retrieval (MSDR2003), 55?60.W.
Hsu, L. Kennedy, C. W. Huang, S. F. Chang, C. Y. Lin, andG.
Iyengar.
2004.
News video story segmentation using fu-sion of multi-level multi-modal features in trecvid 2003.
InICASSP.H.
Kozima.
1993.
Text segmentation based on similarity be-tween words.
In 31st Annual Meeting of the ACL, 286?288.G.
A. Levow.
2004.
Assessing prosodic and text features forsegmentation of mandarin broadcast news.
In HLT-NAACL.M.
T. Maybury.
1998.
Discourse cues for broadcast news seg-mentation.
In COLING-ACL, 819?822.D.
D. Palmer, M. Reichman, and E. Yaich.
2004.
Feature selec-tion for trainable multilingual broadcast news segmentation.In HLT/NAACL.R.
J. Passonneau and D. J. Litman.
1997.
Discourse segmenta-tion by human and automated means.
Computational Liun-guistics, 23(1):103?109.L.
Pevzner and M. Hearst.
2002.
A critique and improvementof an evaluation metric for text segmentation.
ComputationalLinguistics, 28(1):19?36.M.
Porter.
1980.
An algorithm for suffix stripping.
Program,14(3):130?137.E.
Shriberg, A. Stolcke, D. Hakkani-Tu?r, and G. Tu?r.
2000.Prosody based automatic segmentation of speech into sen-tences and topics.
Speech Comm., 32(1-2):127?154.N.
Stokes.
2003.
Spoken and written news story segmentationusing lexical chains.
In Proc.
of the Student Workshop atHLT-NAACL2003, 49?53.S.
Strassel and M. Glenn.
2003.
Creatingthe annotated tdt-4 y2003 evaluation corpus.http://www.nist.gov/speech/tests/tdt/tdt2003/papers/ldc.ppt.S.
Strassel, M. Glenn, and J. Kong.
2004.
Creatingthe tdt5 corpus and 2004 evalutation topics at ldc.http://www.nist.gov/speech/tests/tdt/tdt2004/papers/LDC-TDT5.ppt.G.
Tu?r, D. Hakkani-Tu?r, A. Stolcke, and E. Shriberg.
2001.
In-tegrating prosodic and lexical cues for automatic topic seg-mentation.
Computational Linguistics, 27:31?57.C.
L. Wayne.
2000.
Multilingual topic detection and tracking:Successful research enabled by corpora and evaluation.
InLREC, 1487?1494.I.
Witten, E. Frank, L. Trigg, M. Hall, G. Holmes, andS.
Cunningham.
1999.
Weka: Practical machine learn-ing tools and techniques with java implementation.
InICONIP/ANZIIS/ANNES, 192?196.128
