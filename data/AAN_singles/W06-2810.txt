Finding Similar Sentences across Multiple Languages in WikipediaSisay Fissaha Adafre Maarten de RijkeISLA, University of AmsterdamKruislaan 403, 1098 SJ Amsterdamsfissaha,mdr@science.uva.nlAbstractWe investigate whether the Wikipedia cor-pus is amenable to multilingual analysisthat aims at generating parallel corpora.We present the results of the application oftwo simple heuristics for the identificationof similar text across multiple languagesin Wikipedia.
Despite the simplicity of themethods, evaluation carried out on a sam-ple of Wikipedia pages shows encouragingresults.1 IntroductionParallel corpora form the basis of much multilin-gual research in natural language processing, rang-ing from developing multilingual lexicons to sta-tistical machine translation systems.
As a conse-quence, collecting and aligning text corpora writ-ten in different languages constitutes an importantprerequisite for these research activities.Wikipedia is a multilingual free online encyclo-pedia.
Currently, it has entries for more than 200languages, the English Wikipedia being the largestone with 895,674 articles, and no fewer than eightlanguage versions having upwards of 100,000 ar-ticles as of January 2006.
As can be seen in Fig-ure 1, Wikipedia pages for major European lan-guages have reached a level where they can sup-port multilingual research.
Despite these devel-opments in its content, research on Wikipedia haslargely focused on monolingual aspects so far; seee.g., (Voss, 2005) for an overview.In this paper, we focus on multilingual aspectsof Wikipedia.
Particularly, we investigate to whatextent we can use properties of Wikipedia itselfto generate similar sentences acrose different lan-guages.
As usual, we consider two sentences sim-ilar if they contain (some or a large amount of)overlapping information.
This includes cases inwhich sentences may be exact translations of eachother, one sentence may be contained within an-other, or both share some bits of information.en de fr ja pl it sv nl pt es zh ru no fi da01000002000003000004000005000006000007000008000009000001000000Figure 1: Wikipedia pages for the top 15 lan-guagesThe conceptually simple but fundamental taskof identifying similar sentences across multiplelanguages has a number of motivations.
For astart, and as mentioned earlier, sentence alignedcorpora play an important role in corpus based lan-guage processing methods in general.
Second, inthe context of Wikipedia, being able to align sim-ilar sentences across multiple languages providesinsight into Wikipedia as a knowledge source: towhich extent does a given topic get different kindsof attention in different languages?
And thirdly,the ability to find similar content in other lan-guages while creating a page for a topic in one lan-guage constitutes a useful type of editing support.Furthermore, finding similar content acrose differ-ent languages can form the basis for multilingualsummarization and question answering support for62Wikipedia; at present the latter task is being devel-oped into a pilot for CLEF 2006 (WiQA, 2006).There are different approaches for finding sim-ilar sentences across multiple languages in non-parallel but comparable corpora.
Most methodsfor finding similar sentences assume the availabil-ity of a clean parallel corpus.
In Wikipedia, twoversions of a Wikipedia topic in two different lan-guages are a good starting point for searching sim-ilar sentences.
However, these pages may not al-ways conform to the typical definitions of a bitextwhich current techniques assume.
Bitext gener-ally refers to two versions of a text in two differ-ent languages (Melamed, 1996).
Though it is notknown how information is shared among the dif-ferent languages in Wikipedia, some pages tend tobe translations of each other whereas the majorityof the pages tend to be written independently ofeach other.
Therefore, two versions of the sametopic in two different languages can not simply betaken as parallel corpora.
This in turn limits theapplication of some of the currently available tech-niques.In this paper, we present two approaches forfinding similar sentences across multiple lan-guages in Wikipedia.
The first approach usesfreely available online machine translation re-sources for translating pages and then carries outmonolingual sentence similarity.
The approachneeds a translation system, and these are not avail-able for every pair of languages in Wikipedia.This motivates a second approach to findingsimilar sentences across multiple languages, onewhich uses a bilingual title translation lexicon in-duced automatically using the link structure ofWikipedia.
Briefly, two sentences are similar ifthey link to the same entities (or rather: to pagesabout the same entities), and we use Wikipedia it-self to relate pages about a given entity across mul-tiple languages.
In Wikipedia, pages on the sametopic in different languages are topically closelyrelated.
This means that even if one page is nota translation of another, they tend to share somecommon information.
Our underlying assumptionhere is that there is a general agreement on thekind of information that needs to be included in thepages of different types of topics such as a biogra-phy of a person, and the definition and descriptionof a concept etc., and that this agreement is to aconsderable extent ?materialized?
in the hypertextlinks (and their anchor texts) in Wikipedia.Our main research question in this paper is this:how do the two methods just outlined differ?
Apriori it seems that the translation based approachto finding similar sentences across multiple lan-guages will have a higher recall than the link-based method, while the latter outperforms the for-mer in terms of precision.
Is this correct?The remainder of the paper is organized as fol-lows.
In Section 2, we briefly discuss relatedwork.
Section 3 provides a detailed descriptionof Wikipedia as a corpus.
The two approaches toidentifying similar sentences across multiple lan-guages are presented in Section 4.
An experimen-tal evaluation is presented in Section 5.
We con-clude in Section 6.2 Related WorkThe main focus of this paper lies with multilin-gual text similarity and its application to infor-mation access in the context of Wikipedia.
Cur-rent research work related to Wikipedia mostlydescribes its monolingual properties (Ciffolilli,2003; Vie?gas et al, 2004; Lih, 2004; Miller,2005; Bellomi and Bonato, 2005; Voss, 2005; Fis-saha Adafre and de Rijke, 2005).
This is proba-bly due to the fact that different language versionsof Wikipedia have different growth rates.
Othersdescribe its application in question answering andother types of IR systems (Ahn et al, 2005).
Webelieve that currently, Wikipedia pages for majorEuropean languages have reached a level wherethey can support multilingual research.On the other hand, there is a rich body of knowl-edge relating to multilingual text similarity.
Theseinclude example-based machine translation, cross-lingual information retrieval, statistical machinetranslation, sentence alignment cost functions, andbilingual phrase translation (Kirk Evans, 2005).Each approach uses relatively different features(content and structural features) in identifyingsimilar text from bilingual corpora.
Furthermore,most methods assume that the bilingual corporacan be sentence aligned.
This assumption doesnot hold for our case since our corpus is not par-allel.
In this paper, we use content based fea-tures for identifying similar text across multilin-gual corpora.
Particularly, we compare bilinguallexicon and MT system based methods for identi-fying similar text in Wikipedia.633 Wikipedia as a Multilingual CorpusWikipedia is a free online encyclopedia which isadministered by the non-profit Wikimedia Foun-dation.
The aim of the project is to develop freeencyclopedias for different languages.
It is a col-laborative effort of a community of volunteers, andits content can be edited by anyone.
It is attractingincreasing attention amongst web users and hasjoined the top 50 most popular sites.As of January 1, 2006, there are versions ofWikipedia in more than 200 languages, with sizesranging from 1 to over 800,000 articles.
We usedthe ascii text version of the English and DutchWikipedia, which are available as database dumps.Each entry of the encyclopedia (a page in the on-line version) corresponds to a single line in the textfile.
Each line consists of an ID (usually the nameof the entity) followed by its description.
The de-scription part contains the body of the text that de-scribes the entity.
It contains a mixture of plaintext and text with html tags.
References to otherWikipedia pages in the text are marked using ?[[??]]?
which corresponds to a hyperlink on the on-line version of Wikipedia.
Most of the formattinginformation which is not relevant for the currenttask has been removed.3.1 Links within a single languageWikipedia is a hypertext document with a rich linkstructure.
A description of an entity usually con-tains hypertext links to other pages within or out-side Wikipedia.
The majority of these links cor-respond to entities, which are related to the en-tity being described, and have a separate entryin Wikipedia.
These links are used to guide thereader to a more detailed description of the con-cept denoted by the anchor text.
In other words,the links in Wikipedia typically indicate a topicalassociation between the pages, or rather the enti-ties being described by the pages.
E.g., in describ-ing a particular person, reference will be made tosuch entities as country, organization and other im-portant entities which are related to it and whichthemselves have entries in Wikipedia.
In general,due to the peculiar characteristics of an encyclope-dia corpus, the hyperlinks found in encyclopediatext are used to exemplify those instances of hy-perlinks that exist among topically related entities(Ghani et al, 2001; Rao and Turoff, 1990).Each Wikipedia page is identified with a uniqueID.
These IDs are formed by concatenating thewords of the titles of the Wikipedia pages whichare unique for each page, e.g., the page on Vin-cent van Gogh has ?Vincent van Gogh?
as its ti-tle and ?Vincent van Gogh?
as its ID.
Each pagemay, however, be represented by different anchortexts in a hyperlink.
The anchor texts may be sim-ple morphological variants of the title such as plu-ral form or may represent closely related seman-tic concept.
For example, the anchor text ?Dutch?may point to the page for the Netherlands.
In asense, the IDs function as the canonical form forseveral related concepts.3.2 Links across different languagesDifferent versions of a page in different languagesare also hyperlinked.
For a given page, transla-tions of its title in other languages for which pagesexist are given as hyperlinks.
This property is par-ticularly useful for the current task as it helps us toalign the corpus at the page level.
Furthermore, italso allows us to induce bilingual lexicon consist-ing of the Wikipedia titles.
Conceptual mismatchbetween the pages (e.g.
Roof vs Dakconstructie)is rare, and the lexicon is generally of high qual-ity.
Unlike the general lexicon, this lexicon con-tains a relatively large number of names of indi-viduals and other entities which are highly infor-mative and hence are useful in identifying similartext.
This lexicon will form the backbone of oneof the methods for identifying similar text acrossdifferent languages, as will be shown in Section 4.4 ApproachesWe describe two approaches for identifying simi-lar sentences across different languages.
The firstuses an MT system to obtain a rough translation ofa given page in one language into another and thenuses word overlap between sentences as a similar-ity measure.
One advantage of this method is thatit relies on a large lexical resource which is biggerthan what can be extracted from Wikipedia.
How-ever, the translation can be less accurate especiallyfor the Wikipedia titles which form part of the con-tent of a page and are very informative.The second approach relies on a bilingual lexi-con which is generated from Wikipedia using thelink structure: pages on the same topic in differ-ent languages are hyperlinked; see Figure 2.
Weuse the titles of the pages that are linked in thismanner to create a bilingual lexicon.
Thus, ourbilingual lexicon consists of terms that represent64concepts or entities that have entries in Wikipedia,and we will represent sentences by entries fromthis lexicon: an entry is used to represent the con-tent of a sentence if the sentence contains a hy-pertext link to the Wikipedia page for that entry.Sentence similarity is then captured in terms of theshared lexicon entries they share.
In other words,the similarity measure that we use in this approachis based on ?concept?
or ?page title?
overlap.
In-tuitively, this approach has the advantage of pro-ducing a brief but highly accurate representationof sentences, more accurate, we assume than theMT approach as the titles carry important seman-tic information; it will also be more accurate thanthe MT approach because the translations of thetitles are done manually.Figure 2: Links to pages devoted to the same topicin other languages.Both approaches assume that the Wikipedia cor-pus is aligned at the page level.
This is eas-ily achieved using the link structure since, again,pages on the same topic in different languages arehyperlinked.
This, in turns, narrows down thesearch for similar text to a page level.
Hence, fora given text of a page (sentence or chunk) in onelanguage, we search for its equivalent text (sen-tence or chunk) only in the corresponding page inthe other language, not in the entire corpus.We now describe the two approaches in moredetail.
To remain focused and avoid getting lostin technical details, we consider only two lan-guages in our technical descriptions and evalua-tions below: Dutch and English; it will be clearfrom our presentation, however, that our secondapproach can be used for any pair of languages inWikipedia.4.1 An MT based approachIn this approach, we translate the Dutch Wikipediapage into English using an online MT system.
Werefer to the English page as source and the trans-lated (Dutch page) version as target.
We used theBabelfish MT system of Altavista.
It supports anumber of language pairs among which are Dutch-English pairs.
After both pages have been madeavailable in English, we split the pages into sen-tences or text chucks.
We then link each text chunkor sentence in the source to each chuck or sentencein the target.
Following this we compute a simpleword overlap score for each pair.
We used the Jac-card similarity measure for this purpose.
Contentwords are our main features for the computationof similarity, hence, we remove stopwords.
Gram-matically correct translations may not be neces-sary since we are using simple word overlap as oursimilarity measure.The above procedure will generate a large setof pairs, not all of which will actually be similar.Therefore, we filter the list assuming a one-to-onecorrespondence, where for each source sentencewe identify at most one target sentence.
This isa rather strict criterion (another possibility beingone-to-many), given the fact that the corpus is gen-erally assumed to be not parallel.
But it gives someidea on how much of the text corpus can be alignedat smaller units (i.e., sentence or text chunks).Filtering works as follows.
First we sort thepairs in decreasing order of their similarity scores.This results in a ranked list of text pairs in whichthe most similar pairs are ranked top whereas theleast similar pairs are ranked bottom.
Next we takethe top most ranking pair.
Since we are assuminga one-to-one correspondence, we remove all otherpairs ranked lower in the list containing either ofthe the sentences or text chunks in the top rankingpair.
We then repeat this process taking the secondtop ranking pair.
Each step results in a smaller list.The process continues until there is no more pairto remove.4.2 Using a link-based bilingual lexiconAs mentioned previously, this approach makesuse of a bilingual lexicon that is generated fromWikipedia using the link structure.
A high leveldescription of the algorithm is given in Figure 3.Below, we first describe how the bilingual lexiconis acquired and how it is used for enriching the linkstructure of Wikipedia.
Finally, we detail how the65?
Generating bilingual lexicon?
Given a topic, get the corresponding pagesfrom English and Dutch Wikipedia?
Split pages into sentences and enrich thehyperlinks in the sentence or identifynamed-entities in the pages.?
Represent the sentences in these pages us-ing the bilingual lexicon.?
Compute term overlap between the sen-tences thus represented.Figure 3: The Pseudo-algorithm for identifyingsimilar sentences using a link-based bilingual lex-icon.bilingual lexicon is used for the identification ofsimilar sentences.Generating the bilingual lexiconUnlike the MT based approach, which uses con-tent words from the general vocabulary as fea-tures, in this approach, we use page titles and theirtranslations (as obtained through hyperlinks as ex-plained above) as our primitives for the compu-tation of multilingual similarity.
The first step ofthis approach, then, is acquiring the bilingual lexi-con, but this is relatively straightforward.
For eachWikipedia page in one language, translations ofthe title in other languages, for which there areseparate entries, are given as hyperlinks.
This in-formation is used to generate a bilingual transla-tion lexicon.
Most of these titles are content bear-ing noun phrases and are very useful in multilin-gual similarity computation (Kirk Evans, 2005).Most of these noun phrases are already disam-buiguated, and may consist of either a single wordor multiword units.Wikipedia uses a redirection facility to mapseveral titles into a canonical form.
These titlesare mostly synonymous expressions.
We usedWikipedia?s redirect feature to identify synony-mous expression.Canonical representation of a sentenceOnce we have the bilingual lexicon, the next stepis to represent the sentences in both language pairsusing this lexicon.
Each sentence is represented bythe set of hyperlinks it contains.
We search eachhyperlink in the bilingual lexicon.
If it is found,we replace the hyperlink with the correspondingunique identification of the bilingual lexicon entry.If it is not found, the hyperlink will be included asis as part of the representation.
This is done sinceDutch and English are closely related languagesand may share many cognate pairs.Enriching the Wikipedia link structureAs described in the previous section, the methoduses hyperlinks in a sentence as a highly focusedentity-based representation of the aboutness of thesentence.
In Wikipedia, not all occurrences ofnamed-entities or concepts that have entries inWikipedia are actually used as anchor text of ahypertext link; because of this, a number of sen-tences may needlessly be left out from the simi-larity computation process.
In order to avoid thisproblem, we automatically identify other relevanthyperlinks using the bilingual lexicon generated inthe previous section.Identification of additional hyperlinks inWikipedia sentences works as follows.
Firstwe split the sentences into constituent words.We then generate N gram words keeping therelative order of words in the sentences.
Since theanchor texts of hypertext links may be multiwordexpressions, we start with higher order N gramwords (N=4).
We search these N grams in thebilingual lexicon.
If the N gram is found in thelexicon, it is taken as a new hyperlink and willform part of the representation of a sentence.
Theprocess is repeated for lower order N grams.Identifying similar sentencesOnce we are done representing the sentences asdescribed previously, the final step involves com-putation of the term overlap between the sentencepairs and filtering the resulting list.
The remain-ing steps are similar to those described in the MTbased approach.
For completeness, we briefly re-peat the steps here.
First, all sentences from aDutch Wikipedia page are linked to all sentencesof the corresponding English Wikipedia page.
Wethen compute the similarity between the sentencerepresentations, using the Jaccard similarity coef-ficient.A sentence in Dutch page may be similar toseveral sentences in English page which may re-sult in a large number of spurious pairs.
There-fore, we filter the list using the following recursiveprocedure.
First, the sentence pairs are sorted bytheir similarity scores.
We take the pairs with thehighest similarity scores.
We then eliminate all66other sentence pairs from the list that contain ei-ther of sentences in this pair.
We continue this pro-cess taking the second highest ranking pair.
Notethat this procedure assumes a one-to-one matchingrule; a sentences in Dutch can be linked to at mostone sentence in English.5 Experimental EvaluationNow that we have described the two algorithmsfor identifying similar sentences, we return to ourresearch questions.
In order to answer them werun the experiment described below.5.1 Set-upWe took a random sample of 30 English-DutchWikipedia page pairs.
Each page is split into sen-tences.
We generated candidate Dutch-Englishsentence pairs and passed them on to the twomethods.
Both methods return a ranked list of sen-tence pairs that are similar.
As explained above,we assumed a one-to-one correspondence, i.e., oneEnglish sentence can be linked to at most to oneDutch sentence.The outputs of the systems are manually evalu-ated.
We apply a relatively lenient criteria in as-sessing the results.
If two sentences overlap in-terms of their information content then we con-sider them to be similar.
This includes cases inwhich sentences may be exact translation of eachother, one sentence may be contained within an-other, or both share some bits of information.5.2 ResultsTable 1 shows the results of the two methods de-scribed in Section 4.
In the table, we give twotypes of numbers for each of the two methodsMT and Bilingual lexicon: Total (the total numberof sentence pairs) and Match (the number of cor-rectly identified sentence pairs) generated by thetwo approaches.Overall, the two approaches tend to producesimilar numbers of correctly identified similar sen-tence pairs.
The systems seem to perform wellon pages which tend to be alignable at sentencelevel, i.e., parallel.
This is clearly seen on thefollowing pages: Pierluigi Collina, Marcus Cor-nelius Fronto, George F. Kennan, which show ahigh similarity at sentence level.
Some pages con-tain very small description and hence the figuresfor correct similar sentences are also small.
Othertopics such as Classicism (Dutch: Classicisme),Tennis, and Tank, though they are described in suf-ficient details in both languages, there tends to beless overlap among the text.
The methods tend toretrieve more accurate similar pairs from personpages than other pages especially those pages de-scribing a more abstract concepts.
However, thisneeds to be tested more thoroughly.When we look at the total number of sentencepairs returned, we notice that the bilingual lexi-con based method consistently returns a smalleramount of similar sentence pairs which makesthe method more accurate than the MT based ap-proach.
On average, the MT based approach re-turns 4.5 (26%) correct sentences and the bilinguallexicon based approach returns 2.9 correct sen-tences (45%).
But, on average, the MT approachreturns three times as many sentence pairs as bilin-gual lexicon approach.
This may be due to the factthat the former makes use of restricted set of im-portant terms or concepts whereas the later uses alarge general lexicon.
Though we remove someof the most frequently occuring stopwords in theMT based approach, it still generates a large num-ber of incorrect similar sentence pairs due to somecommon words.In general, the number of correctly identifiedsimilar pages extracted seems small.
However,most of the Dutch pages are relatively small,which sets the upper bound on the number ofcorrectly identified sentence pairs that can be ex-tracted.
On average, each Dutch Wikipedia pagein the sample contains 18 sentences whereas En-glish Wikipedia pages contain 65 sentences.
Ex-cluding the pages for Tennis, Tank (Dutch: vo-ertuig), and Tricolor, which are relatively large,each Dutch page contains on average 8 sentences,which is even smaller.
Given the fact that thepages are in general not parallel, the methods,using simple heuristics, identified high qualitytranslation equivalent sentence pairs from mostWikipedia pages.
Furthermore, a close examina-tion of the output of the two approaches show thatboth tend to identify the same set of similar sen-tence pairs.We ran our bilingual lexicon based approach onthe whole Dutch-English Wikipedia corpus.
Themethod returned about 80M of candidate similarsentences.
Though we do not have the resourcesto evaluate this output, the results we got fromsample data (cf.
Table 1) suggest that it containsa significant amount of correctly identified similar67Title MT Bilingual LexiconEnglish Dutch Total Match Total MatchHersfeld Rotenburg Hersfeld Rotenburg 2 3 2Manganese nodule Mangaanknol 5 2 1 1Kettle Ketel 1 1Treason Landverraad 2 1Pierluigi Collina Pierluigi Collina 14 13 13 11Province of Ferrara Ferrara (provincie) 7 1 1 1Classicism Classicisme 8 1Tennis Tennis 93 4 15 3Hysteria Hysterie 14 6 9 5George F. Kennan George Kennan 27 12 29 11Marcus Cornelius Fronto Marcus Cornelius Fronto 11 9 5 5Delphi Delphi (Griekenland) 34 2 8 1De Beers De Beers 11 5 10 5Pavel Popovich Pavel Popovytsj 7 4 4 4Rice pudding Rijstebrij 11 1 4Manta ray Reuzenmanta 15 3 7 2Michelstadt Michelstadt 1 1 1 1Tank Tank (voertuig) 84 3 27 2Cheyenne(Wyoming) Cheyenne(Wyoming) 5 2 2 2Goa Goa(deelstaat) 13 4 6 1Tricolour Driekleur 57 36 13 12Oral cancer Mondkanker 25 2 7 2Pallium Pallium 12 2 5 4Ajanta Ajanta 3 3 2 2Captain Jack (band) Captain Jack 16 3 2 2Proboscis Monkey Neusaap 15 6 4 1Patti Smith Patti Smith 6 2 4 2Flores Island, Portugal Flores (Azoren) 3 2 1 1Mercury 8 Mercury MA 8 11 3 4 1Mutation Mutatie 16 4 6 3Average 17.6 4.5 6.5 2.9Table 1: Test topics (column 1 and 2).
The total number of sentence pairs (column 3) and the numberof correctly identified similar sentence pairs (column 4) returned by the MT based approach.
The to-tal number of sentence pairs (column 5) and the number of correctly identified similar sentence pairs(column 6) returned by the method using a bilingual lexicon.sentences.6 ConclusionIn this paper we focused on multilingual aspects ofWikipedia.
Particularly, we investigated the poten-tial of Wikipedia for generating parallel corpora byapplying different methods for identifying similartext across multiple languages.
We presented twomethods and carried out an evaluation on a sam-ple of Dutch-English Wikipedia pages.
The resultsshow that both methods, using simple heuristics,were able to identify similar text between the pairof Wikipedia pages though they differ in accuracy.The bilingual lexicon approach returns fewer in-correct pairs than the MT based approach.
Weinterpret this as saying that our bilingual lexiconbased method provides a more accurate represen-tation of the aboutness of sentences in Wikipediathan the MT based approach.
Furthermore, the re-sult we obtained on a sample of Wikipedia pagesand the output of running the bilingual based ap-proach on the whole Dutch-English gives some in-dication of the potential of Wikipedia for generat-ing parallel corpora.68As to future work, the sentence similarity de-tection methods that we considered are not perfect.E.g., the MT based approach relies on rough trans-lations; it is important to investigate the contri-bution of high quality translations.
The bilinguallexicon approach uses only lexical features; otherlanguage specific sentence features might help im-prove results.AcknowledgmentsThis research was supported by the Nether-lands Organization for Scientific Research (NWO)under project numbers 017.001.190, 220-80-001, 264-70-050, 612-13-001, 612.000.106,612.000.207, 612.066.302, 612.069.006, 640.-001.501, and 640.002.501.ReferencesD.
Ahn, V. Jijkoun, G. Mishne, K. Mu?ller, M. de Rijke,and S. Schlobach.
2005.
Using Wikipedia at theTREC QA Track.
In E.M. Voorhees and L.P. Buck-land, editors, The Thirteenth Text Retrieval Confer-ence (TREC 2004).F.
Bellomi and R. Bonato.
2005.
Lex-ical authorities in an encyclopedic cor-pus: a case study with wikipedia.
URL:http://www.fran.it/blog/2005/01/lexical-authorities-in-encyclopedic.htm%l.
Site accessed on June 9, 2005.A.
Ciffolilli.
2003.
Phantom authority, selfselective re-cruitment and retention of members in virtual com-munities: The case of Wikipedia.
First Monday,8(12).S.
Fissaha Adafre and M. de Rijke.
2005.
Discoveringmissing links in Wikipedia.
In Proceedings of theWorkshop on Link Discovery: Issues, Approachesand Applications (LinkKDD-2005).R.
Ghani, S. Slattery, and Y. Yang.
2001.
Hypertextcategorization using hyperlink patterns and metadata.
In Carla Brodley and Andrea Danyluk, ed-itors, Proceedings of ICML-01, 18th InternationalConference on Machine Learning, pages 178?185.D.
Kirk Evans.
2005.
Identifying similarityin text: Multi-lingual analysis for summariza-tion.
URL: http://www1.cs.columbia.edu/nlp/theses/dave_evans.pdf.
Siteaccessed on January 5, 2006.A.
Lih.
2004.
Wikipedia as participatory journalism:Reliable sources?
Metrics for evaluating collabora-tive media as a news resource.
In Proceedings of the5th International Symposium on Online Journalism.D.
Melamed.
1996.
A geometric approach to mappingbitext correspondence.
In Eric Brill and KennethChurch, editors, Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 1?12, Somerset, New Jersey.
Associationfor Computational Linguistics.N.
Miller.
2005.
Wikipedia and the disappearing?Author?.
ETC: A Review of General Semantics,62(1):37?40.U.
Rao and M. Turoff.
1990.
Hypertext functionality:A theoretical framework.
International Journal ofHuman-Computer Interaction.F.
Vie?gas, M. Wattenberg, and D. Kushal.
2004.Studying cooperation and conflict between authorswith history flow visualization.
In Proceedings ofthe 2004 conference on Human factors in comput-ing systems.J.
Voss.
2005.
Measuring Wikipedia.
In Proceedings10th International Conference of the InternationalSociety for Scientometrics and Informetrics.WiQA.
2006.
Question answering using Wikipedia.URL: http://ilps.science.uva.nl/WiQA/.
Site accessed on January 5, 2006.69
