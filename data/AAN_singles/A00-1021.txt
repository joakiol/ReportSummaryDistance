Ranking suspected answers to natural language questions usingpredictive annotationDragomir R. Radev"School of InformationUniversity of MichiganAnn Arbor, MI 48103radev@umich, eduJohn PragerTJ  Watson Research CenterIBM Research DivisionHawthorne, NY 10532jprager@us.
ibm.comValerie Samn*Teachers CollegeColumbia UniversityNew York, NY 10027vs l l5@columbia.eduAbstractIn this paper, we describe a system to rank sus-pected answers to natural language questions.We process both corpus and query using a newtechnique, predictive annotation, which aug-ments phrases in texts with labels anticipatingtheir being targets of certain kinds of questions.Given a natural anguage question, our IR sys-tem returns a set of matching passages, whichwe then rank using a linear function of sevenpredictor variables.
We provide an evaluation ofthe techniques based on results from the TRECQ&A evaluation in which our system partici-pated.1 IntroductionQuestion Answering is a task that calls for acombination of techniques from Information Re-trieval and Natural Language Processing.
Theformer has the advantage of years of develop-ment of efficient techniques for indexing andsearching large collections of data, but lacks ofany meaningful treatment of the semantics ofthe query or the texts indexed.
NLP tacklesthe semantics, but tends to be computationallyexpensive.We have attempted to carve out a middleground, whereby we use a modified IR systemaugmented by shallow NL parsing.
Our ap-proach was motivated by the following problemwith traditional IR systems.
Suppose the userasks "Where did <some event> happen?".
Ifthe system does no pre-processing of the query,then "where" will be included in the bag ofwords submitted to the search engine, but thiswill not be helpful since the target text willbe unlikely to contain the word "where".
Ifthe word is stripped out as a stop-word, then* The work presented in this paper was performed whilethe first and third authors were at 1BM Research.the search engine will have no idea that a lo-cation is sought.
Our approach, called predic-tive annotation, is to augment he query withsemantic ategory markers (which we call QA-Tokens), in this case with the PLACES to-ken, and also to label with QA-Tokens all oc-currences in text that are recognized entities,(for example, places).
Then traditional bag-of-words matching proceeds uccessfully, and willreturn matching passages.
The answer-selectionprocess then looks for and ranks in these pas-sages occurrences ofphrases containing the par-ticular QA-Token(s) from the augmented query.This classification of questions is conceptuallysimilar to the query expansion in (Voorhees,1994) but is expected to achieve much betterperformance since potentially matching phrasesin text are classified in a similar and synergisticway.Our system participated in the official TRECQ&A evaluation.
For 200 questions in the eval-uation set, we were asked to provide a list of50-byte and 250-byte xtracts from a 2-GB cor-pus.
The results are shown in Section 7.Some techniques used by other participants inthe TREC evaluation are paragraph indexing,followed by abductive inference (Harabagiu andMaiorano, 1999) and knowledge-representationcombined with information retrieval (Breck etal., 1999).
Some earlier systems related to ourwork are FaqFinder (Kulyukin et al, 1998),MURAX (Kupiec, 1993), which uses an encyclo-pedia as a knowledge base from which to extractanswers, and PROFILE (Radev and McKeown,1997) which identifies named entities and nounphrases that describe them in text.2 System descriptionOur system (Figure 1) consists of two pieces:an IR component (GuruQA) that which returnsmatching texts, and an answer selection compo-150neat (AnSel/Werlect) that extracts and rankspotential answers from these texts.This paper focuses on the process of rank-ing potential answers selected by the IR engine,which is itself described in (Prager et al, 1999).~ lndexer~ Searc~x'~ GuruQA\\Rankcd ~ AnSel/ ~ Hit ListH tL st \[ \[ Werlect IAnswer selectionFigure 1: System Architecture.2.1 The Information RetrievalcomponentIn the context of fact-seeking questions, wemade the following observations:?
In documents that contain the answers, thequery terms tend to occur in close proxim-ity to each other.?
The answers to fact-seeking questions areusually phrases: "President Clinton", "inthe Rocky Mountains", and "today").?
These phrases can be categorized by a set ofa dozen or so labels (Figure 2) correspond-ing to question types.?
The phrases can be identified in text bypattern matching techniques (without fullNLP).As a result, we defined a set of about 20 cat-egories, each labeled with its own QA-Token,and built an IR system which deviates from thetraditional model in three important aspects.?
We process the query against a set of ap-proximately 200 question templates which,may replace some of the query wordswith a set of QA-Tokens, called a SYN-class.
Thus "Where" gets mappedto "PLACES", but "How long " goesto "@SYN(LENGTH$, DURATIONS)".Some templates do not cause complete re-placement of the matched string.
For ex-ample, the pattern "What is the popula-tion" gets replaced by "NUMBERS popu-lation'.?
Before indexing the text, we process itwith Textract (Byrd and Ravin, 1998;Wacholder et al, 1997), which performslemmatization, and discovers proper namesand technical terms.
We added a newmodule (Resporator) which annotates textsegments with QA-Tokens using patternmatching.
Thus the text "for 5 centuries"matches the DURATIONS pattern "for:CARDINAL _timeperiod", where :CAR-DINAL is the label for cardinal numbers,and _timeperiod marks a time expression.?
GuruQA scores text passages instead ofdocuments.
We use a simple document-and collection-independent weightingscheme: QA-Tokens get a weight of 400,proper nouns get 200 and any other word- 100 (stop words are removed in queryprocessing after the pattern templatematching operation).
The density ofmatching query tokens within a passage iscontributes a score of 1 to 99 (the highestscores occur when all matched terms areconsecutive).Predictive Annotation works best for Where,When, What, Which and How+adjective ques-tions than for How+verb and Why questions,since the latter are typically not answered byphrases.
However, we observed that "by" +the present participle would usually indicatethe description of a procedure, so we instan-tiate a METHODS QA-Token for such occur-rences.
We have no such QA-Token for Whyquestions, but we do replace the word "why"with "@SYN(result, cause, because)", since theoccurrence of any of these words usually beto-kens an explanation.3 Answer  select ionSo far, we have described how we retrieve rel-evant passages that may contain the answer toa query.
The output of GuruQA is a list of10 short passages containing altogether a large151QA-Token Question type ExamplePLACESCOUNTRY$STATESPERSONSROLESNAMESORG$DURATIONSAGESYEARSTIMESDATESVOLUMESAREASLENGTHSWEIGHTSNUMBERSMETHODSRATESMONEYSWhereWhere/What countryWhere/What stateWhoWhoWho/What/WhichWho/WhatHow longHow oldWhen/What yearWhenWhen/What dateHow bigHow bigHow big/long/highHow big/heavyHow manyHowHow muchHow muchIn the Rocky MountainsUnited KingdomMassachusettsAlbert EinsteinDoctorThe Shakespeare F stivalThe US Post OfficeFor 5 centuries30 years old1999In the afternoonJuly 4th, 17763 gallons4 square inches3 miles25 tons1,234.5By rubbing50 per cent4 million dollarsFigure 2: Sample QA-Tokens.number (often more than 30 or 40) of potentialanswers in the form of phrases annotated withQA-Tokens.3.1 Answer  rank ingWe now describe two algorithms, AnSel andWerlect, which rank the spans returned by Gu-ruQA.
AnSel and Werlect 1 use different ap-proaches, which we describe, evaluate and com-pare and contrast.
The output of either systemconsists of five text extracts per question thatcontain the likeliest answers to the questions.3.2 Sample  Input to AnSel /Wer lectThe role of answer selection is to decide whichamong the spans extracted by GuruQA aremost likely to contain the precise answer to thequestions.
Figure 3 contains an example of thedata structure passed from GuruQA to our an-swer selection module.The input consists of four items:?
a query (marked with <QUERY> tokensin the example),?
a list of 10 passages (one of which is shownabove),?
a list of annotated text spans within thepassages, annotated with QA-Tokens, and1 from ANswer SELect and ansWER seLECT, respec-tively?
the SYN-class corresponding to the type ofquestion (e.g., "PERSONS NAMES").The text in Figure 3 contains five spans (po-tential answers), of which three ("Biography ofMargaret Thatcher", "Hugo Young", and "Mar-garet Thatcher") are of types included in theSYN-class for the question (PERSON NAME).The full output of GuruQA for this question in-cludes a total of 14 potential spans (5 PERSONsand 9 NAMEs).3.3 Sample  Output  o f  AnSel /Wer lectThe answer selection module has two outputs:internal (phrase) and external (text passage).Internal output:  The internal output is aranked list of spans as shown in Table 1.
Itrepresents a ranked list of the spans (potentialanswers) sent by GuruQA.Externa l  output :  The external output isa ranked list of 50-byte and 250-byte xtracts.These extracts are selected in a way to coverthe highest-ranked spans in the list of potentialanswers.
Examples are given later in the paper.The external output was required for theTREC evaluation while system's internal out-put can be used in a variety of applications, e.g.,to highlight the actual span that we believe isthe answer to the question within the contextof the passage in which it appears.152<p> <NUMBER> 1 </NUMBER> </p><p><QUERY>Who is the author of the book, "The Iron Lady: A Biography of Margaret Thatcher"?</QUERY></p><p> <PROCESSED_QUERY> @excwin(*dynamic* @weight (200 * Iron_Lady) @weight (200Biography_of_Margaret_Thatcher) @weight(200 Margaret) @weight(100 author)@weight(100 book) @weight(100 iron) @weight(100 lady) @weight(100 :) @weight(100 biography)@weight(100 thatcher) @weight(400 @syn(PERSON$ NAME$)))</PROCESSED_QUERY></p><p> <DOC>LA090290-0118</DOC> </p> <p> <SCORE> 1020.8114</SCORE> d/p><TEXT><p>THE IRON LADY; A <span class="NAME">Biography of Margaret Thatcher </span>by <span class--"PERSON">Hugo Y ung</span> (<span class='ORG">Farrar , Straus& Giroux</span>) The central riddle revealed here is why, as a woman <span class--'PLACEDEF'>in aman</span>'s world, <span class--'PERSON'>Margaret Thatcher</span> evinces uch an exclusionaryattitude toward women.</p></TEXT>Figure 3: Input sent from GuruQA to AnSel/Werlect.Score Span5.06-8.14-13.60-18.00-19.38-26.06-31.75-32.38-36.78-42.68-198.34-217.80-234.55Hugo YoungBiography of Margaret ThatcherDavid WilliamsWilliamsSir Ronald MillarSantiagoOxfordMaggieSeriously RichFTMargaret ThatcherThatcherIron LadyQuestlon/Answer (TR38)Q: Who was Johnny Mathis' high schooltrack coach?A: Lou VasquezQ: What year was the Magna Carta signed?A: 1215Q: What two companies produce bovinesomatotropin?A: Monsanto and Eli LillyFigure 4: Sample questions from TR38.Table 1: Ranked potential answers to Quest.
1.4 Ana lys i s  o f  corpus  and quest ionsetsIn this section we describe the corpora used fortraining and evaluation as well as the questionscontained in the training and evaluation ques-tion sets.4.1 Corpus  analysisFor both training and evaluation, we used theTREC corpus, consisting of approximately 2GB of articles from four news agencies.4.2 Training set TR38To train our system, we used 38 questions (seeFigure 4) for which the answers were providedby NIST.4.3 Test set T200The majority of the 200 questions (see Figure 5)in the evaluation set (T200) were not substan-tially different from these in TR38, although theintroduction of "why" and "how" questions aswell as the wording of questions in the format"Name X" made the task slightly harder.Questlon/Answer (T200)Q: Why did David Koresh ask the FBI for aword processor?A: to record his revelations.Q: How tall is the Matterhorn?A: 14,776 feet 9 inchesQ: How tall is the replica of the Matterhornat Disneyland?A: 147-footFigure 5: Sample questions from T200.Some examples of problematic questions areshown in Figure 6.153Q: Why did David Koresh ask the FBI fora word processor?Q: Name the first private citizen to fly inspace.Q: What is considered the costliest disasterthe insurance industry has ever faced?Q: What did John Hinckley do to impressJodie Foster?Q: How did Socrates die?Figure 6: Sample harder questions from T200.5 AnSe lAnSel uses an optimization algorithm with 7predictive variables to describe how likely agiven span is to be the correct answer to aquestion.
The variables are illustrated with ex-amples related to the sample question number10001 from TR38 "Who was Johnny Mathis'high school track coach?".
The potential an-swers (extracted by GuruQA) are shown in Ta-ble 2.5.1 Feature selectionThe seven span features described below werefound to correlate with the correct answers.Number :  position of the span among M1 spansreturned from the hit-list.Rspanno:  position of the span among all spansreturned within the current passage.Count:  number of spans of any span class re-trieved within the current passage.Notinq: the number of words in the span thatdo not appear in the query.Type: the position of the span type in the listof potential span types.
Example: Type("Lou Vasquez") = 1, because the spantype of "Lou Vasquez", namely "PER-SON" appears first in the SYN-class "PER-SON ORG NAME ROLE".Avgdst:  the average distance in words betweenthe beginning of the span and query wordsthat also appear in the passage.
Example:given the passage "Tim O'Donohue, Wood-bridge High School's varsity baseball coach,resigned Monday and will be replaced byassistant Johnny Ceballos, Athletic Direc-tor Dave Cowen said."
and the span "TimO'Donohue", the value of avgdst is equalto 8.Sscore: passage relevance as computed by Gu-ruQA.Number :  the position of the span among allretrieved spans.5.2 AnSel  algorithmThe TOTAL score for a given potential answeris computed as a linear combination of the fea-tures described in the previous ubsection:TOTAL  = ~ w~ , fiiThe Mgorithm that the training componentof AnSel uses to learn the weights used in theformula is shown in Figure 7.For each <question,span> tuple in trainingset :i. Compute features for each span2.
Compute TOTAL score for each spanusing current set of weightsKepeat3.
Compute performance on trainingset4.
Adjust weights wi throughlogistic regressionUntil performance > thresholdFigure 7: Training algorithm used by AnSel.Training discovered the following weights:Wnurnbe r -~ --0.3; Wrspann o -~ --0.5; Wcount :3 .0 ;  Wnot inq  = 2 .0 ;  Wtypes  = 15 .0 ;  Wavgdst  -----1.0; W~score = 1.5At runtime, the weights are used to rank po-tential answers.
Each span is assigned a TO-TAL score and the top 5 distinct extracts of50 (or 250) bytes centered around the span areoutput.
The 50-byte xtracts for question 10001are shown in Figure 8.
For lack of space, we areomitting the 250-byte xtracts.6 Wer lec tThe Werlect algorithm used many of the samefeatures of phrases used by AnSel, but employeda different ranking scheme.6.1 ApproachUnlike AnSel, Werlect is based on a two-step,rule-based process approximating a functionwith interaction between variables.
In the firststage of this algorithm, we assign a rank to154Spat ,Ollie MatsonLou VasquezTim O'DonohueAthlet ic Director Dave CowenJohnny CeballosCivic Center Director Mart in DurhamJohnny HodgesDerric EvansNEWSWIRE Johnny MajorsWoodbr idge High SchoolEvanGary EdwardsO.
J .
SimpsonSouth Lake TahoeWashington HighMorganTennessee footballEl l ingtonassistantthe VolunteersJ ohnny  MathisMathiscoachType  Nun lberPERSON 3PERSON 1PERSON 17PERSON 23PERSON 22PERSON 13PERSON 25PERSON 33PERSON 3OORG 18PERSON 37PERSON 38NAME 2NAME 7NAME 10NAME 26NAME 31NAME 24ROLE 21ROLE 34PERSON 4NAME 14ROLE 19Rspanno Count  Not lnq  Type  Avgdst  Sscore3 6 2 I 12 0 .025071 6 2 1 16 0 .025071 4 2 I 8 0 .022576 4 4 1 I I  0 .022575 4 I I 9 0 .02257I 2 5 1 16 0 .025052 4 I I 15 0 .022564 4 2 l 14 0 .022561 4 2 1 17 0 .022562 4 1 2 6 0 .022576 4 1 1 14 0 .022567 4 2 1 17 0 .022562 6 2 3 12 0 .025075 6 3 3 14 0 .025076 6 1 3 18 0 .025073 4 1 3 12 0 .022562 4 1 3 15 0 .022561 4 1 3 20 0 .022564 4 1 4 8 0 .022575 4 2 4 14 0 .022564 6 - I00  I I I  0 .025072 2 -100 3 I0  0 .025053 4 -100 4 4 0 .02257Table 2: Feature set and span rankings for training question 10001.Document  ID Score ExtractLA053189-0069 892.5LA053189-0069 890.1LA060889-0181 887.4LA060889-0181 884.1LA060889-0181 880.9of O.J.
Simpson , Ollie Matson and Johnny MathisLou Vasquez , track coach of O.J.
Simpson , OllieTim O'Donohue, Woodbridge High School's varsitynny Ceballos , Athletic Director Dave Cowen said.aced by assistant Johnny Ceballos , Athletic DirecFigure 8: Fifty-byte extracts.TOTAL-7.53-9.93-12.57-15.87-19.07-19.36-25.22-25.37-25.47-28.37-29.57-30,87-37.40-40.06-49.80-52 .52-56.27-59.42-62.77-71.17-211.33-254.16-259.67every relevant phrase within each sentence ac-cording to how likely it is to be the target an-swer.
Next, we generate and rank each N-bytefragment based on the sentence score given byGuruQA, measures of the fragment's relevance,and the ranks of its component phrases.
UnlikeAnSel, Werlect was optimized through manualtrial-and-error using the TR38 questions.6.2 Step  One: Feature  Se lect ionThe features considered in Werlect that werealso used by AnSel, were Type, Avgdst and Ss-core.
Two additional features were also takeninto account:Not inqW:  a modified version of Notinq.
Asin AnSel, spans that are contained in thequery are given a rank of 0.
However, par-tial matches are weighted favorably in somecases.
For example, if the question asks,"Who was Lincoln's Secretary of State?
"a noun phrase that contains "Secretary ofState" is more likely to be the answer thanone that does not.
In this example, thephrase, "Secretary of State William Se-ward" is the most likely candidate.
Thiscriterion also seems to play a role in theevent that Resporator fails to identify rel-evant phrase types.
For example, in thetraining question, "What shape is a por-poise's tooth?"
the phrase "spade-shaped"is correctly selected from among all nounsand adjectives of the sentences returned byGuru-QA.F requency :  how often the span occurs acrossdifferent passages.
For example, the testquestion, "How many lives were lost in thePan Am crash in Lockerbie, Scotland?"
re-sulted in four potential answers in the firsttwo sentences returned by Guru-QA.
Ta-ble 3 shows the frequencies of each term,and their eventual influence on the spanrank.
The repeated occurrence of "270",helps promote it to first place.6.3 Step  two:  rank ing the  sentencespansAfter each relevant span is assigned a rank, werank all possible text segments of 50 (or 250)bytes from the hit list based on the sum of thephrase ranks plus additional points for otherwords in the segment that match the query.The algorithm used by Werlect is shown inFigure 9.155I n i t ia l  Sentence  Rank  Phrase  F requency  Span Rank1 Two 5 21 365 million 1 31 11 1 42 270 7 1 (ranked highest)Table 3: Influence of frequency on span rank.i.
Let cand idate_set  = all potentialanswers, ranked and sorted.2.
For each hit-l ist passage, extractali spans of 50 (or 250) bytes, onword boundaries.3.
Rank and sort all segments basedon phrase ranks, matching terms,and sentence ranks.4.
For each candidate in sortedcandidate_set- Let highest_ranked_span= highest-ranked spancontaining candidate- Let answer_set\[i++\] =h ighest_ rankedspan- Remove every candidate fromcandidate_set that is found inh ighest_ rankedspan- Exit if i > 55.
Output answer_setnoted that on the 14 questions we were unableto classify with a QA-Token, Werlect (runs W50and W250) achieved an MRAR of 3.5 to Ansel's2.0.The cumulative RAR of A50 on T200 (Ta-ble 4) is 63.22 (i.e., we got 49 questions amongthe 198 right from our first try and 39 otherswithin the first five answers).The performance of A250 on T200 is shownin Table 5.
We were able to answer 71 questionswith our first answer and 38 others within ourfirst five answers (cumulative RAR = 85.17).To better characterize the performance of oursystem, we split the 198 questions into 20 groupsof 10 questions.
Our performance on groupsof questions ranged from 0.87 to 5.50 MRARfor A50 and from 1.98 to 7.5 MRAR for A250(Table 6).Figure 9: Algorithm used by Werlect.7 Eva luat ionIn this section, we describe the performance ofour system using results from our four officialruns.7.1 Evaluat ion schemeFor each question, the performance is computedas the reciprocal value of the rank (RAR) ofthe highest-ranked correct answer given by thesystem.
For example, if the system has giventhe correct answer in three positions: second,third, and fifth, RAR for that question is !
2"The Mean Reciprocal Answer Rank (MRAR)is used to compute the overall performance ofsystems participating in the TREC evaluation:RAR - rank i  MRAR = .
rank i  )$7.2 Per fo rmance  on the officialevaluat ion dataOverall, Ansel (runs A50 and A25) performedmarginally better than Werlect.
However, we50 bytes 250 bytesnAvgMinMaxStd Dev203.190.875.501.17204.301.987.501.27Table 6: Performance on groups of ten questionsFinally, Table 7 shows how our official runscompare to the rest of the 25 official submis-sions.
Our performance using AnSel and 50-byte output was 0.430.
The performance ofWerlect was 0.395.
On 250 bytes, AnSel scored0.319 and Werlect - 0.280.8 Conc lus ionWe presented a new technique, predict ive an-notat ion,  for finding answers to natural an-guage questions in text corpora.
We showedthat a system based on predictive annotationcan deliver very good results compared to othercompeting systems.We described a set of features that correlatewith the plausibility of a given text span be-ing a good answer to a question.
We experi-156nb of casesPointsnb of casesPointsFirst I Second Third Fourth Fifth TOTAL49 I 15 ll 9 4 8849.00 7.50 3.67 2.25 0.80 63.22Table 4: Performance of A50 on T200First Second Third Fourth Fifth TOTAL71 16 11 6 5 10971.00 8.00 3.67 1.50 1.00 85.17RunTableMedian AverageW50 0.12A50 0.12W250 0.29A250 0.295: Performance of A250 onOur Average0.2800.3190.3950.430T200Nb Times Nb Times> Median -=- Median56 12672 11260 10666 110Nb Times< Median16143222Table 7: Comparison of our system with the other participantsmented with two algorithms for ranking poten-tial answers based on these features.
We discov-ered that a linear combination of these featuresperforms better overall, while a non-linear algo-rithm performs better on unclassified questions.9 AcknowledgmentsWe would like to thank Eric Brown, Anni Co-den, and Wlodek Zadrozny from IBM Researchfor useful comments and collaboration.
Wewould also like to thank the organizers of theTREC Q~zA evaluation for initiating such awonderful research initiative.ReferencesEric Breck, John Burger, David House, MarcLight, and Inderjeet Mani.
1999.
Ques-tion answering from large document collec-tions.
In Proceedings of AAAI Fall Sympo-sium on Question Answering Systems, NorthFalmouth, Massachusetts.Roy Byrd and Yael Ravin.
1998.
Identifyingand extracting relations in text.
In Proceed-ings of NLDB, Klagenfurt, Austria.Sanda Harabagiu and Steven J. Maiorano.1999.
Finding answers in large collections oftexts : Paragraph indexing + abductive in-ference.
In Proceedings ofAAAI Fall Sympo-sium on Question Answering Systems, NorthFalmouth, Massachusetts.Vladimir Kulyukin, Kristian Hammond, andRobin Burke.
1998.
Answering questionsfor an organization online.
In Proceedings ofAAAI, Madison, Wisconsin.Julian M. Kupiec.
1993.
MURAX: A robustlinguistic approach for question answering us-ing an ondine encyclopedia.
In Proceedings,16th Annual International ACM SIGIR Con-ference on Research and Development in In-formation Retrieval.John Prager, Dragomir R. Radev, Eric Brown,Anni Coden, and Valerie Samn.
1999.
Theuse of predictive annotation for question an-swering in TREC8.
In Proceedings o/TREC-8, Gaithersburg, Maryland.Dragomir R. Radev and Kathleen R. McKe-own.
1997.
Building a generation knowledgesource using internet-accessible newswire.
InProceedings ofthe 5th Conference on AppliedNatural Language Processing, pages 221-228,Washington, DC, April.Ellen Voorhees.
1994.
Query expansion usinglexical-semantic relations.
In Proceedings ofA CM SIGIR, Dublin, Ireland.Nina Wacholder, Yael Ravin, and Misook Choi.1997.
Disambiguation of proper names intext.
In Proceedings ofthe Fifth Applied Nat-ural Language Processing Conference, Wash-ington, D.C. Association for ComputationalLinguistics.157
