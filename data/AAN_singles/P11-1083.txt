Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 825?834,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsHow to train your multi bottom-up tree transducerAndreas MalettiUniversita?t Stuttgart, Institute for Natural Language ProcessingAzenbergstra?e 12, 70174 Stuttgart, Germanyandreas.maletti@ims.uni-stuttgart.deAbstractThe local multi bottom-up tree transducer isintroduced and related to the (non-contiguous)synchronous tree sequence substitution gram-mar.
It is then shown how to obtain a weightedlocal multi bottom-up tree transducer froma bilingual and biparsed corpus.
Finally,the problem of non-preservation of regular-ity is addressed.
Three properties that ensurepreservation are introduced, and it is discussedhow to adjust the rule extraction process suchthat they are automatically fulfilled.1 IntroductionA (formal) translation model is at the core of ev-ery machine translation system.
Predominantly, sta-tistical processes are used to instantiate the for-mal model and derive a specific translation device.Brown et al (1990) discuss automatically trainabletranslation models in their seminal paper.
However,the IBM models of Brown et al (1993) are string-based in the sense that they base the translation de-cision on the words and their surrounding context.Contrary, in the field of syntax-based machine trans-lation, the translation models have full access to thesyntax of the sentences and can base their decisionon it.
A good exposition to both fields is presentedin (Knight, 2007).In this paper, we deal exclusively with syntax-based translation models such as synchronous treesubstitution grammars (STSG), multi bottom-up treetransducers (MBOT), and synchronous tree-sequencesubstitution grammars (STSSG).
Chiang (2006)gives a good introduction to STSG, which originatefrom the syntax-directed translation schemes of Ahoand Ullman (1972).
Roughly speaking, an STSGhas rules in which two linked nonterminals are re-placed (at the same time) by two corresponding treescontaining terminal and nonterminal symbols.
Inaddition, the nonterminals in the two replacementtrees are linked, which creates new linked nontermi-nals to which further rules can be applied.
Hence-forth, we refer to these two trees as input and outputtree.
MBOT have been introduced in (Arnold andDauchet, 1982; Lilin, 1981) and are slightly moreexpressive than STSG.
Roughly speaking, they al-low one replacement input tree and several outputtrees in a single rule.
This change and the pres-ence of states yields many algorithmically advanta-geous properties such as closure under composition,efficient binarization, and efficient input and outputrestriction [see (Maletti, 2010)].
Finally, STSSG,which have been derived from rational tree rela-tions (Raoult, 1997), have been discussed by Zhanget al (2008a), Zhang et al (2008b), and Sun et al(2009).
They are even more expressive than the lo-cal variant of the multi bottom-up tree transducer(LMBOT) that we introduce here and can have sev-eral input and output trees in a single rule.In this contribution, we restrict MBOT to a formthat is particularly relevant in machine translation.We drop the general state behavior of MBOT and re-place it by the common locality tests that are alsopresent in STSG, STSSG, and STAG (Shieber andSchabes, 1990; Shieber, 2007).
The obtained deviceis the local MBOT (LMBOT).Maletti (2010) argued the algorithmical advan-tages of MBOT over STSG and proposed MBOT asan implementation alternative for STSG.
In partic-ular, the training procedure would train STSG; i.e.,it would not utilize the additional expressive power825of MBOT.
However, Zhang et al (2008b) and Sunet al (2009) demonstrate that the additional expres-sivity gained from non-contiguous rules greatly im-proves the translation quality.
In this contributionwe address this separation and investigate a trainingprocedure for LMBOT that allows non-contiguousfragments while preserving the algorithmic advan-tages of MBOT.
To this end, we introduce a rule ex-traction and weight training method for LMBOT thatis based on the corresponding procedures for STSGand STSSG.
However, general LMBOT can be tooexpressive in the sense that they allow translationsthat do not preserve regularity.
Preservation of reg-ularity is an important property for efficient repre-sentations and efficient algorithms [see (May et al,2010)].
Consequently, we present 3 properties thatensure that an LMBOT preserves regularity.
In addi-tion, we shortly discuss how these properties couldbe enforced in the rule extraction procedure.2 NotationThe set of nonnegative integers is N. We write [k]for the set {i | 1 ?
i ?
k}.
We treat functions asspecial relations.
For every relation R ?
A?B andS ?
A, we writeR(S) = {b ?
B | ?a ?
S : (a, b) ?
R}R?1 = {(b, a) | (a, b) ?
R} ,where R?1 is called the inverse of R.Given an alphabet ?, the set of all words (or se-quences) over ?
is ?
?, of which the empty word is ?.The concatenation of two words u and w is simplydenoted by the juxtaposition uw.
The length of aword w = ?1 ?
?
?
?k with ?i ?
?
for all i ?
[k]is |w| = k. Given 1 ?
i ?
j ?
k, the (i, j)-span w[i, j] of w is ?i?i+1 ?
?
?
?j .The set T?
of all ?-trees is the smallest set Tsuch that ?
(t) ?
T for all ?
?
?
and t ?
T ?.We generally use bold-face characters (like t) forsequences, and we refer to their elements using sub-scripts (like ti).
Consequently, a tree t consists ofa labeled root node ?
followed by a sequence t ofits children.
To improve readability we sometimeswrite a sequence t1 ?
?
?
tk as t1, .
.
.
, tk.The positions pos(t) ?
N?
of a tree t = ?
(t) areinductively defined by pos(t) = {?
}?pos(t), wherepos(t) =?1?i?|t|{ip | p ?
pos(ti)} .Note that this yields an undesirable difference be-tween pos(t) and pos(t), but it will always be clearfrom the context whether we refer to a single tree ora sequence.
Note that positions are ordered via the(standard) lexicographic ordering.
Let t ?
T?
andp ?
pos(t).
The label of t at position p is t(p), andthe subtree rooted at position p is t|p.
Formally, theyare defined byt(p) ={?
if p = ?t(p) otherwiset(ip) = ti(p)t|p ={t if p = ?t|p otherwiset|ip = ti|pfor all t = ?
(t) and 1 ?
i ?
|t|.
As demonstrated,these notions are also used for sequences.
A posi-tion p ?
pos(t) is a leaf (in t) if p1 /?
pos(t).
Givena subset NT ?
?, we let?NT(t) = {p ?
pos(t) | t(p) ?
NT, p leaf in t} .Later NT will be the set of nonterminals, so thatthe elements of ?NT(t) will be the leaf nonterminalsof t. We extend the notion to sequences t by?NT(t) =?1?i?|t|{ip | p ?
?NT(ti)} .We also need a substitution that replaces sub-trees.
Let p1, .
.
.
, pn ?
pos(t) be pairwise in-comparable positions and t1, .
.
.
, tn ?
T?.
Thent[pi ?
ti | 1 ?
i ?
n] denotes the tree that is ob-tained from t by replacing (in parallel) the subtreesat pi by ti for every i ?
[k].Finally, let us recall regular tree languages.
A fi-nite tree automaton M is a tuple (Q,?, ?, F ) suchthat Q is a finite set, ?
?
Q?
?
?
?
Q is a fi-nite relation, and F ?
Q.
We extend ?
to a map-ping ?
: T?
?
2Q by?(?
(t)) = {q | (q, ?, q) ?
?, ?i ?
[ |t| ] : qi ?
?
(ti)}for every ?
?
?
and t ?
T ??.
The finite tree automa-ton M recognizes the tree languageL(M) = {t ?
T?
| ?
(t) ?
F 6= ?}
.A tree language L ?
T?
is regular if there exists afinite tree automaton M such that L = L(M).826VPVBDsignedPP ?PVtwlY,NP-OBJNPDET-NNAltwqyEPP1SNP-SBJ VP?VPPV NP-OBJ NP-SBJ121Figure 1: Sample LMBOT rules.3 The modelIn this section, we recall particular multi bottom-up tree transducers, which have been introducedby Arnold and Dauchet (1982) and Lilin (1981).
Adetailed (and English) presentation of the generalmodel can be found in Engelfriet et al (2009) andMaletti (2010).
Using the nomenclature of Engel-friet et al (2009), we recall a variant of linear andnondeleting extended multi bottom-up tree transduc-ers (MBOT) here.
Occasionally, we will refer to gen-eral MBOT, which differ from the local variant dis-cussed here because they have explicit states.Throughout the article, we assume sets ?
and ?of input and output symbols, respectively.
More-over, let NT ?
?
??
be the set of designated non-terminal symbols.
Finally, we avoid weights in theformal development to keep it simple.
It is straight-forward to add weights to our model.Essentially, the model works on pairs ?t,u?consisting of an input tree t ?
T?
and a se-quence u ?
T ??
of output trees.
Each such pair iscalled a pre-translation and the rank rk(?t,u?)
thepre-translation ?t,u?
is |u|.
In other words, the rankof a pre-translation equals the number of output treesstored in it.
Given a pre-translation ?t,u?
?
T?
?T k?and i ?
[k], we call ui the ith translation of t. Analignment for the pre-translation ?t,u?
is an injec-tive mapping ?
: ?NT(u) ?
?NT(t) ?
N such that(p, j) ?
?
(?NT(u)) for every (p, i) ?
?
(?NT(u))and j ?
[i].
In other words, an alignment should re-quest each translation of a particular subtree at mostonce and if it requests the ith translation, then itshould also request all previous translations.Definition 1 A local multi bottom-up tree trans-ducer (LMBOT) is a finite setR of rules such that ev-ery rule, written l ??
r, contains a pre-translation?l, r?
and an alignment ?
for it.The component l is the left-hand side, r isthe right-hand side, and ?
is the alignment of arule l??
r ?
R. The rules of an LMBOT are similarto the rules of an STSG (synchronous tree substitu-tion grammar) of Eisner (2003) and Shieber (2004),but right-hand sides of LMBOT contain a sequenceof trees instead of just a single tree as in an STSG.
Inaddition, the alignments in an STSG rule are bijec-tive between leaf nonterminals, whereas our modelpermits multiple alignments to a single leaf nonter-minal in the left-hand side.
A model that is evenmore powerful than LMBOT is the non-contiguousversion of STSSG (synchronous tree-sequence sub-stitution grammar) of Zhang et al (2008a), Zhanget al (2008b), and Sun et al (2009), which al-lows sequences of trees on both sides of rules [seealso (Raoult, 1997)].
Figure 1 displays sample rulesof an LMBOT using a graphical representation of thetrees and the alignment.Next, we define the semantics of an LMBOT R.To avoid difficulties1, we explicitly exclude ruleslike l ??
r where l ?
NT or r ?
NT?
; i.e.,rules where the left- or right-hand side are onlyleaf nonterminals.
We first define the traditionalbottom-up semantics.
Let ?
= l ??
r ?
R be arule and p ?
?NT(l).
The p-rank rk(?, p) of ?
isrk(?, p) = |{i ?
N | (p, i) ?
?
(?NT(r))}|.Definition 2 The set ?
(R) of pre-translations of anLMBOT R is inductively defined to be the smallestset such that: If ?
= l ??
r ?
R is a rule,?tp,up?
?
?
(R) is a pre-translation of R for everyp ?
?NT(l), and?
rk(?, p) = rk(?tp,up?),?
l(p) = tp(?
), and1Actually, difficulties arise only in the weighted setting.827PPINforNPNNPSerbia?PPPREPEnNPNN-PROPSrbyA?VPVBDsignedPPINforNPNNPSerbia?
PVtwlY,NP-OBJNPDET-NNAltwqyEPPPREPEnNPNN-PROPSrbyA?S.
.
.
VPVBDsignedPPINforNPNNPSerbia?VPPVtwlYNP-OBJNPDET-NNAltwqyEPPPREPEnNPNN-PROPSrbyA.
.
.
?Figure 2: Top left: (a) Initial pre-translation; Top right: (b) Pre-translation obtained from the left rule of Fig.
1 and (a);Bottom: (c) Pre-translation obtained from the right rule of Fig.
1 and (b).?
r(p?)
= up??
(i) with ?(p?)
= (p?
?, i)for every p?
?
?NT(r), then ?t,u?
?
?
(R) where?
t = l[p?
tp | p ?
?NT(l)] and?
u = r[p?
?
(up??
)i | p?
?
??1(p?
?, i)].In plain words, each nonterminal leaf p in theleft-hand side of a rule ?
can be replaced by theinput tree t of a pre-translation ?t,u?
whose rootis labeled by the same nonterminal.
In addition,the rank rk(?, p) of the replaced nonterminal shouldmatch the rank rk(?t,u?)
of the pre-translation andthe nonterminals in the right-hand side that arealigned to p should be replaced by the translationthat the alignment requests, provided that the non-terminal matches with the root symbol of the re-quested translation.
The main benefit of the bottom-up semantics is that it works exclusively on pre-translations.
The process is illustrated in Figure 2.Using the classical bottom-up semantics, we sim-ply obtain the following theorem by Maletti (2010)because the MBOT constructed there is in fact anLMBOT.Theorem 3 For every STSG, an equivalent LMBOTcan be constructed in linear time, which in turnyields a particular MBOT in linear time.Finally, we want to relate LMBOT to the STSSGof Sun et al (2009).
To this end, we also introducethe top-down semantics for LMBOT.
As expected,both semantics coincide.
The top-down semantics isintroduced using rule compositions, which will playan important rule later on.Definition 4 The set Rk of k-fold composed rules isinductively defined as follows:?
R1 = R and?
` ??
s ?
Rk+1 for all ?
= l ??
r ?
R and?p = lp ?
?p rp ?
Rk such that?
rk(?, p) = rk(?lp, rp?),?
l(p) = lp(?
), and?
r(p?)
= rp??
(i) with ?(p?)
= (p?
?, i)for every p ?
?NT(l) and p?
?
?NT(r) where?
` = l[p?
lp | p ?
?NT(l)],?
s = r[p?
?
(rp??
)i | p?
?
??1(p?
?, i)], and?
?
(p?p) = p???p??
(ip) for all positionsp?
?
??1(p?
?, i) and ip ?
?NT(rp??
).The rule closureR??
ofR isR??
=?i?1Ri.
Thetop-down pre-translation of R is?t(R) = {?l, r ?
| l??
r ?
R?
?, ?NT(l) = ?}
.828XX?Xa X,Xa X12XX?Xb X,Xb X12XXX?Xa Xb X,Xa Xb X12Figure 3: Composed rule.The composition of the rules, which is illus-trated in Figure 3, in the second item of Defini-tion 4 could also be represented as ?
(?1, .
.
.
, ?k)where ?1, .
.
.
, ?k is an enumeration of the rules{?p | p ?
?NT(l)} used in the item.
The follow-ing theorem is easy to prove.Theorem 5 The bottom-up and top-down semanticscoincide; i.e., ?
(R) = ?t(R).Chiang (2005) and Graehl et al (2008) argue thatSTSG have sufficient expressive power for syntax-based machine translation, but Zhang et al (2008a)show that the additional expressive power of tree-sequences helps the translation process.
This ismostly due to the fact that smaller (and less specific)rules can be extracted from bi-parsed word-alignedtraining data.
A detailed overview that focusses onSTSG is presented by Knight (2007).Theorem 6 For every LMBOT, an equivalent STSSGcan be constructed in linear time.4 Rule extraction and trainingIn this section, we will show how to automaticallyobtain an LMBOT from a bi-parsed, word-alignedparallel corpus.
Essentially, the process has twosteps: rule extraction and training.
In the rule ex-traction step, an (unweighted) LMBOT is extractedfrom the corpus.
The rule weights are then set in thetraining procedure.The two main inspirations for our rule extractionare the corresponding procedures for STSG (Galleyet al, 2004; Graehl et al, 2008) and for STSSG (Sunet al, 2009).
STSG are always contiguous in boththe left- and right-hand side, which means that they(completely) cover a single span of input or outputwords.
On the contrary, STSSG rules can be non-contiguous on both sides, but the extraction proce-dure of Sun et al (2009) only extracts rules that arecontiguous on the left- or right-hand side.
We canadjust its 1st phase that extracts rules with (poten-tially) non-contiguous right-hand sides.
The adjust-ment is necessary because LMBOT rules cannot have(contiguous) tree sequences in their left-hand sides.Overall, the rule extraction process is sketched inAlgorithm 1.Algorithm 1 Rule extraction for LMBOTRequire: word-aligned tree pair (t, u)Return: LMBOT rules R such that (t, u) ?
?
(R)while there exists a maximal non-leaf nodep ?
pos(t) and minimal p1, .
.
.
, pk ?
pos(u)such that t|p and (u|p1 , .
.
.
, u|pk) have a con-sistent alignment (i.e., no alignments fromwithin t|p to a leaf outside (u|p1 , .
.
.
, u|pk) andvice versa)do2: add rule ?
= t|p ??
(up1 , .
.
.
, upk) to Rwith the nonterminal alignments ?// excise rule ?
from (t, u)4: t?
t[p?
t(p)]u?
u[pi ?
u(pi) | i ?
{1, .
.
.
, k}]6: establish alignments according to positionend whileThe requirement that we can only have one in-put tree in LMBOT rules indeed might cause the ex-traction of bigger and less useful rules (when com-pared to the corresponding STSSG rules) as demon-strated in (Sun et al, 2009).
However, the stricterrule shape preserves the good algorithmic proper-ties of LMBOT.
The more powerful STSSG rules cancause nonclosure under composition (Raoult, 1997;Radmacher, 2008) and parsing to be less efficient.Figure 4 shows an example of biparsed alignedparallel text.
According to the method of Galley etal.
(2004) we can extract the (minimal) STSG ruledisplayed in Figure 5.
Using the more liberal formatof LMBOT rules, we can decompose the STSG rule ofFigure 5 further into the rules displayed in Figure 1.The method of Sun et al (2009) would also extractthe rule displayed in Figure 6.Let us reconsider Figures 1 and 2.
Let ?1 bethe top left rule of Figure 2 and ?2 and ?3 be the829SNP-SBJNMLJJYugoslavNNPPresidentNNPVoislavVPVBDsignedPPINforNPNNPSerbiaVPPVtwlYNP-OBJNPDET-NNAltwqyEPPPREPEnNPNN-PROPSrbyANP-SBJNPDET-NNAlr}ysDET-ADJAlywgwslAfyNPNN-PROPfwyslAfFigure 4: Biparsed aligned parallel text.SNP-SBJ VPVBDsignedPP?VPPVtwlYNP-OBJNPDET-NNAltwqyEPPNP-SBJ11Figure 5: Minimal STSG rule.left and right rule of Figure 1, respectively.
Wecan represent the lower pre-translation of Figure 2by ?3(?
?
?
, ?2(?1)), where ?2(?1) represents the up-per right pre-translation of Figure 2.
If we nameall rules of R, then we can represent each pre-translation of ?
(R) symbolically by a tree contain-ing rule names.
Such trees containing rule namesare often called derivation trees.
Overall, we obtainthe following result, for which details can be foundin (Arnold and Dauchet, 1982).Theorem 7 The setD(R) is a regular tree languagefor every LMBOT R, and the set of derivations is alsoregular for every MBOT.VBDsigned,INfor?PVtwlY,NPDET-NNAltwqyE,PREPEnFigure 6: Sample STSSG rule.Moreover, using the input and output product con-structions of Maletti (2010) we obtain that even theset Dt,u(R) of derivations for a specific input tree tand output tree u is regular.
Since Dt,u(R) is reg-ular, we can compute the inside and outside weightof each (weighted) rule of R following the methodof Graehl et al (2008).
Similarly, we can adjustthe training procedure of Graehl et al (2008), whichyields that we can automatically obtain a weightedLMBOT from a bi-parsed parallel corpus.
Details onthe run-time can be found in (Graehl et al, 2008).5 Preservation of regularityClearly, LMBOT are not symmetric.
Although, thebackwards application of an LMBOT preserves regu-larity, this property does not hold for forward appli-cation.
We will focus on forward application here.Given a set T of pre-translations and a tree language830L ?
T?, we letTc(L) = {ui | (u1, .
.
.
, uk) ?
T (L), i ?
[k]} ,which collects all translations of input trees in L.We say that T preserves regularity if Tc(L) is regu-lar for every regular tree language L ?
T?.
Corre-spondingly, an LMBOT R preserves regularity if itsset ?
(R) of pre-translations preserves regularity.As mentioned, an LMBOT does not necessarilypreserve regularity.
The rules of an LMBOT haveonly alignments between the left-hand side (inputtree) and the right-hand side (output tree), which arealso called inter-tree alignments.
However, severalalignments to a single nonterminal in the left-handside can transitively relate two different nontermi-nals in the output side and thus simulate an intra-tree alignment.
For example, the right rule of Fig-ure 1 relates a ?PV?
and an ?NP-OBJ?
node to a sin-gle ?VP?
node in the left-hand side.
This could leadto an intra-tree alignment (synchronization) betweenthe ?PV?
and ?NP-OBJ?
nodes in the right-hand side.Figure 7 displays the rules R of an LMBOTthat does not preserve regularity.
This can easilybe seen on the leaf (word) languages because theLMBOT can translate the word x to any elementof L = {wcwc | w ?
{a, b}?}.
Clearly, this wordlanguage L is not context-free.
Since the leaf lan-guage of every regular tree language is context-freeand regular tree languages are closed under inter-section (needed to single out the translations thathave the symbol Y at the root), this also proves that?(R)c(T?)
is not regular.
Since T?
is regular, thisproves that the LMBOT does not preserve regularity.Preservation of regularity is an important propertyfor a number of translation model manipulations.For example, the bucket-brigade and the on-the-flymethod for the efficient inference described in (Mayet al, 2010) essentially build on it.
Moreover, a reg-ular tree grammar (i.e., a representation of a regulartree language) is an efficient representation.
Morecomplex representations such as context-free treegrammars [see, e.g., (Fujiyoshi, 2004)] have worsealgorithmic properties (e.g., more complex parsingand problematic intersection).In this section, we investigate three syntactic re-strictions on the set R of rules that guarantees thatthe obtained LMBOT preserves regularity.
Then weshortly discuss how to adjust the rule extraction al-gorithm, so that the extracted rules automaticallyhave these property.
First, we quickly recall the no-tion of composed rules from Definition 4 becauseit will play an essential role in all three properties.Figure 3 shows a composition of two rules from Fig-ure 7.
Mind thatR2 might not contain all rules ofR,but it contains all those without leaf nonterminals.Definition 8 An LMBOT R is finitely collapsing ifthere is n ?
N such that ?
: ?NT(r)?
?NT(l)?
{1}for every rule l??
r ?
Rn.The following statement follows from a more gen-eral result of Raoult (1997), which we will introducewith our second property.Theorem 9 Every finitely collapsing LMBOT pre-serves regularity.Often the simple condition ?finitely collapsing?
isfulfilled after rule extraction.
In addition, it is au-tomatically fulfilled in an LMBOT that was obtainedfrom an STSG using Theorem 3.
It can also be en-sured in the rule extraction process by introducingcollapsing points for output symbols that can appearrecursively in the corpus.
For example, we could en-force that all extracted rules for clause-level outputsymbols (assuming that there is no recursion not in-volving a clause-level output symbols) should haveonly 1 output tree in the right-hand side.However, ?finitely collapsing?
is a rather strictproperty.
Finitely collapsing LMBOT have onlyslightly more expressive power than STSG.
In fact,they could be called STSG with input desynchro-nization.
This is due to the fact that the alignmentin composed rules establishes an injective relationbetween leaf nonterminals (as in an STSG), but itneed not be bijective.
Consequently, there can beleaf nonterminals in the left-hand side that have noaligned leaf nonterminal in the right-hand side.
Inthis sense, those leaf nonterminals are desynchro-nized.
This feature is illustrated in Figure 8 andsuch an LMBOT can compute the transformation{(t, a) | t ?
T?
}, which cannot be computed by anSTSG (assuming that T?
is suitably rich).
Thus STSGwith input desynchronization are more expressivethan STSG, but they still compute a class of trans-formations that is not closed under composition.831Xx?Xc, XcXX?Xa X,Xa X12XX?Xb X,Xb X12YX?YX X12Figure 7: Output subtree synchronization (intra-tree).XX X?
aXa??
?Figure 8: Finitely collapsing LMBOT.Theorem 10 For every STSG, we can construct anequivalent finitely collapsing LMBOT in linear time.Moreover, finitely collapsing LMBOT are strictlymore expressive than STSG.Next, we investigate a weaker property by Raoult(1997) that still ensures preservation of regularity.Definition 11 An LMBOT R has finite synchroniza-tion if there is n ?
N such that for every rulel ??
r ?
Rn and p ?
?NT(l) there exists i ?
Nwith ?
?1({p} ?
N) ?
{iw | w ?
N?
}.In plain terms, multiple alignments to a single leafnonterminal at p in the left-hand side are allowed,but all leaf nonterminals of the right-hand side thatare aligned to p must be in the same tree.
Clearly,an LMBOT with finite synchronization is finitely col-lapsing.
Raoult (1997) investigated this restrictionin the context of rational tree relations, which are ageneralization of our LMBOT.
Raoult (1997) showsthat finite synchronization can be decided.
The nexttheorem follows from the results of Raoult (1997).Theorem 12 Every LMBOT with finite synchroniza-tion preserves regularity.MBOT can compute arbitrary compositions ofSTSG (Maletti, 2010).
However, this no longer re-mains true for MBOT (or LMBOT) with finite syn-chronization.2 In Figure 9 we illustrate a transla-tion that can be computed by a composition of twoSTSG, but that cannot be computed by an MBOT(or LMBOT) with finite synchronization.
Intuitively,when processing the chain of ?X?s of the transforma-tion depicted in Figure 9, the first and second suc-2This assumes a straightforward generalization of the ?finitesynchronization?
property for MBOT.YX...XYt1 t2t3?Zt1 t2 t3Figure 9: Transformation that cannot be computed by anMBOT with finite synchronization.cessor of the ?Z?-node at the root on the output sidemust be aligned to the ?X?-chain.
This is necessarybecause those two mentioned subtrees must repro-duce t1 and t2 from the end of the ?X?-chain.
Weomit the formal proof here, but obtain the followingstatement.Theorem 13 For every STSG, we can construct anequivalent LMBOT with finite synchronization in lin-ear time.
LMBOT and MBOT with finite synchroniza-tion are strictly more expressive than STSG and com-pute classes that are not closed under composition.Again, it is straightforward to adjust the rule ex-traction algorithm by the introduction of synchro-nization points (for example, for clause level outputsymbols).
We can simply require that rules extractedfor those selected output symbols fulfill the condi-tion mentioned in Definition 11.Finally, we introduce an even weaker version.Definition 14 An LMBOT R is copy-free if there isn ?
N such that for every rule l ??
r ?
Rn andp ?
?NT(l) we have (i) ?
?1({p} ?
N) ?
N, or(ii) ?
?1({p} ?
N) ?
{iw | w ?
N?}
for an i ?
N.Intuitively, a copy-free LMBOT has rules whoseright hand sides may use all leaf nonterminals thatare aligned to a given leaf nonterminal in the left-hand side directly at the root (of one of the trees832XX...XX?Xa Xa .
.
.Xa X,Xa Xa .
.
.Xa X12Figure 10: Composed rule that is not copy-free.in the right-hand side forest) or group all those leafnonterminals in a single tree in the forest.
Clearly,the LMBOT of Figure 7 is not copy-free because thesecond rule composes with itself (see Figure 10) toa rule that does not fulfill the copy-free condition.Theorem 15 Every copy-free LMBOT preservesregularity.Proof sketch: Let n be the integer of Defini-tion 14.
We replace the LMBOT with rules R by theequivalent LMBOT M with rules Rn.
Then all ruleshave the form required in Definition 14.
Moreover,let L ?
T?
be a regular tree language.
Then wecan construct the input product of ?
(M) with L. Inthis way, we obtain an MBOT M ?, whose rules stillfulfill the requirements (adapted for MBOT) of Defi-nition 14 because the input product does not changethe structure of the rules (it only modifies the statebehavior).
Consequently, we only need to show thatthe range of the MBOT M ?
is regular.
This can beachieved using a decomposition into a relabeling,which clearly preserves regularity, and a determinis-tic finite-copying top-down tree transducer (Engel-friet et al, 1980; Engelfriet, 1982).
2Figure 11 shows some relevant rules of a copy-free LMBOT that computes the transformation ofFigure 9.
Clearly, copy-free LMBOT are more gen-eral than LMBOT with finite synchronization, so weagain can obtain copy-free LMBOT from STSG.
Inaddition, we can adjust the rule extraction processusing synchronization points as for LMBOT with fi-nite synchronization using the restrictions of Defini-tion 14.Theorem 16 For every STSG, we can constructan equivalent copy-free LMBOT in linear time.YX S?ZS S S12XX?
?S , S?12XYS S?
?S , S?12Figure 11: Copy-free LMBOT for the transformationof Figure 9.Copy-free LMBOT are strictly more expressive thanLMBOT with finite synchronization.6 ConclusionWe have introduced a simple restriction of multibottom-up tree transducers.
It abstracts from thegeneral state behavior of the general model andonly uses the locality tests that are also present inSTSG, STSSG, and STAG.
Next, we introduced arule extraction procedure and a corresponding ruleweight training procedure for our LMBOT.
However,LMBOT allow translations that do not preserve reg-ularity, which is an important property for efficientalgorithms.
We presented 3 properties that ensurethat regularity is preserved.
In addition, we shortlydiscussed how these properties could be enforced inthe presented rule extraction procedure.AcknowledgementsThe author gratefully acknowledges the support byKEVIN KNIGHT, who provided the inspiration andthe data.
JONATHAN MAY helped in many fruitfuldiscussions.The author was financially supported bythe German Research Foundation (DFG) grantMA / 4959 / 1-1.833ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
The Theoryof Parsing, Translation, and Compiling.
Prentice Hall.Andre?
Arnold and Max Dauchet.
1982.
Morphismeset bimorphismes d?arbres.
Theoret.
Comput.
Sci.,20(1):33?93.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D. Laf-ferty, Robert L. Mercer, and Paul S. Roossin.
1990.
Astatistical approach to machine translation.
Computa-tional Linguistics, 16(2):79?85.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
Mathematics ofstatistical machine translation: Parameter estimation.Computational Linguistics, 19(2):263?311.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
ACL, pages263?270.
Association for Computational Linguistics.David Chiang.
2006.
An introduction to synchronousgrammars.
In Proc.
ACL.
Association for Computa-tional Linguistics.
Part of a tutorial given with KevinKnight.Jason Eisner.
2003.
Simpler and more general mini-mization for weighted finite-state automata.
In Proc.NAACL, pages 64?71.
Association for ComputationalLinguistics.Joost Engelfriet, Grzegorz Rozenberg, and Giora Slutzki.1980.
Tree transducers, L systems, and two-way ma-chines.
J. Comput.
System Sci., 20(2):150?202.Joost Engelfriet, Eric Lilin, and Andreas Maletti.
2009.Composition and decomposition of extended multibottom-up tree transducers.
Acta Inform., 46(8):561?590.Joost Engelfriet.
1982.
The copying power of one-statetree transducers.
J. Comput.
System Sci., 25(3):418?435.Akio Fujiyoshi.
2004.
Restrictions on monadic context-free tree grammars.
In Proc.
CoLing, pages 78?84.Association for Computational Linguistics.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Proc.HLT-NAACL, pages 273?280.
Association for Compu-tational Linguistics.Jonathan Graehl, Kevin Knight, and Jonathan May.
2008.Training tree transducers.
Computational Linguistics,34(3):391?427.Kevin Knight.
2007.
Capturing practical natu-ral language transformations.
Machine Translation,21(2):121?133.Eric Lilin.
1981.
Proprie?te?s de clo?ture d?une ex-tension de transducteurs d?arbres de?terministes.
InProc.
CAAP, volume 112 of LNCS, pages 280?289.Springer.Andreas Maletti.
2010.
Why synchronous tree substi-tution grammars?
In Proc.
NAACL, pages 876?884.Association for Computational Linguistics.Jonathan May, Kevin Knight, and Heiko Vogler.
2010.Efficient inference through cascades of weighted treetransducers.
In Proc.
ACL, pages 1058?1066.
Associ-ation for Computational Linguistics.Frank G. Radmacher.
2008.
An automata theoretic ap-proach to rational tree relations.
In Proc.
SOFSEM,volume 4910 of LNCS, pages 424?435.
Springer.Jean-Claude Raoult.
1997.
Rational tree relations.
Bull.Belg.
Math.
Soc.
Simon Stevin, 4(1):149?176.Stuart M. Shieber and Yves Schabes.
1990.
Synchronoustree-adjoining grammars.
In Proc.
CoLing, volume 3,pages 253?258.
Association for Computational Lin-guistics.Stuart M. Shieber.
2004.
Synchronous grammars as treetransducers.
In Proc.
TAG+7, pages 88?95, Vancou-ver, BC, Canada.
Simon Fraser University.Stuart M. Shieber.
2007.
Probabilistic synchronous tree-adjoining grammars for machine translation: The ar-gument from bilingual dictionaries.
In Proc.
SSST,pages 88?95.
Association for Computational Linguis-tics.Jun Sun, Min Zhang, and Chew Lim Tan.
2009.
A non-contiguous tree sequence alignment-based model forstatistical machine translation.
In Proc.
ACL, pages914?922.
Association for Computational Linguistics.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008a.
A tree se-quence alignment-based tree-to-tree translation model.In Proc.
ACL, pages 559?567.
Association for Compu-tational Linguistics.Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, andSheng Li.
2008b.
Grammar comparison study fortranslational equivalence modeling and statistical ma-chine translation.
In Proc.
CoLing, pages 1097?1104.Association for Computational Linguistics.834
