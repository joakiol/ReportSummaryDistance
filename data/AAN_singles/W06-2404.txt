Chunking Japanese Compound Functional Expressionsby Machine LearningMasatoshi Tsuchiya?
and Takao Shime?
and Toshihiro Takagi?Takehito Utsuro??
and Kiyotaka Uchimoto??
and Suguru Matsuyoshi?Satoshi Sato??
and Seiichi Nakagawa??
?Computer Center / ?
?Department of Information and Computer Sciences,Toyohashi University of Technology, Tenpaku-cho, Toyohashi, 441?8580, JAPAN?Graduate School of Informatics, Kyoto University, Sakyo-ku, Kyoto, 606?8501, JAPAN?
?Graduate School of Systems and Information Engineering, University of Tsukuba,1-1-1, Tennodai, Tsukuba, 305-8573, JAPAN?
?National Institute of Information and Communications Technology,3?5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619?0289 JAPAN?
?Graduate School of Engineering, Nagoya University,Furo-cho, Chikusa-ku, Nagoya, 464?8603, JAPANAbstractThe Japanese language has various typesof compound functional expressions,which are very important for recogniz-ing the syntactic structures of Japanesesentences and for understanding theirsemantic contents.
In this paper, weformalize the task of identifying Japanesecompound functional expressions in atext as a chunking problem.
We apply amachine learning technique to this task,where we employ that of Support VectorMachines (SVMs).
We show that the pro-posed method significantly outperformsexisting Japanese text processing tools.1 IntroductionAs in the case of other languages, the Japaneselanguage has various types of functional wordssuch as post-positional particles and auxiliaryverbs.
In addition to those functional words,the Japanese language has much more compoundfunctional expressions which consist of more thanone words including both content words and func-tional words.
Those single functional words aswell as compound functional expressions are veryimportant for recognizing the syntactic structuresof Japanese sentences and for understanding theirsemantic contents.
Recognition and understandingof them are also very important for various kindsof NLP applications such as dialogue systems, ma-chine translation, and question answering.
How-ever, recognition and semantic interpretation ofcompound functional expressions are especiallydifficult because it often happens that one com-pound expression may have both a literal (in otherwords, compositional) content word usage anda non-literal (in other words, non-compositional)functional usage.For example, Table 1 shows two example sen-tences of a compound expression ??
(ni)???
(tsuite)?, which consists of a post-positional par-ticle ??
(ni)?, and a conjugated form ????(tsuite)?
of a verb ???
(tsuku)?.
In the sentence(A), the compound expression functions as a case-marking particle and has a non-compositionalfunctional meaning ?about?.
On the other hand,in the sentence (B), the expression simply corre-sponds to a literal concatenation of the usages ofthe constituents: the post-positional particle ??(ni)?
and the verb ????
(tsuite)?, and has acontent word meaning ?follow?.
Therefore, whenconsidering machine translation of those Japanesesentences into English, it is necessary to preciselyjudge the usage of the compound expression ??(ni)???
(tsuite)?, as shown in the English trans-lation of the two sentences in Table 1.There exist widely-used Japanese text process-ing tools, i.e., pairs of a morphological analysistool and a subsequent parsing tool, such as JU-MAN1+ KNP2 and ChaSen3+ CaboCha4.
How-ever, they process those compound expressionsonly partially, in that their morphological analy-sis dictionaries list only limited number of com-pound expressions.
Furthermore, even if certainexpressions are listed in a morphological analysis1http://www.kc.t.u-tokyo.ac.jp/nl-resource/juman-e.html2http://www.kc.t.u-tokyo.ac.jp/nl-resource/knp-e.html3http://chasen.naist.jp/hiki/ChaSen/4http://chasen.org/?taku/software/cabocha/25Table 1: Translation Selection of a Japanese Compound Expression ??
(ni)???
(tsuite)??
(watashi) ?
(ha) ?
(kare) ?
(ni)???
(tsuite) ???
(hanashita)(A) (I) (TOP) (he) (about) (talked)(I talked about him.)?
(watashi) ?
(ha) ?
(kare) ?
(ni) ???
(tsuite) ???
(hashitta)(B) (I) (TOP) (he) (ACC) (follow) (ran)(I ran following him.
)Table 2: Classification of Functional Expressions based on Grammatical Function# of major # ofGrammatical Function Type expressions variants Examplesubsequent to predicate 36 67 ???
?post-positional / modifying predicate (to-naru-to)particle subsequent to nominal 45 121 ????
?type / modifying predicate (ni-kakete-ha)subsequent to predicate, nominal 2 3 ??
?/ modifying nominal (to-iu)auxiliary verb type 42 146 ???
(te-ii)total 125 337 ?dictionary, those existing tools often fail in resolv-ing the ambiguities of their usages, such as thosein Table 1.
This is mainly because the frame-work of those existing tools is not designed so asto resolve such ambiguities of compound (possi-bly functional) expressions by carefully consider-ing the context of those expressions.Considering such a situation, it is necessaryto develop a tool which properly recognizes andsemantically interprets Japanese compound func-tional expressions.
In this paper, we apply a ma-chine learning technique to the task of identify-ing Japanese compound functional expressions ina text.
We formalize this identification task as achunking problem.
We employ the technique ofSupport Vector Machines (SVMs) (Vapnik, 1998)as the machine learning technique, which has beensuccessfully applied to various natural languageprocessing tasks including chunking tasks suchas phrase chunking (Kudo and Matsumoto, 2001)and named entity chunking (Mayfield et al, 2003).In the preliminary experimental evaluation, we fo-cus on 52 expressions that have balanced distribu-tion of their usages in the newspaper text corpusand are among the most difficult ones in terms oftheir identification in a text.
We show that the pro-posed method significantly outperforms existingJapanese text processing tools as well as anothertool based on hand-crafted rules.
We further showthat, in the proposed SVMs based framework, it issufficient to collect and manually annotate about50 training examples per expression.2 Japanese Compound FunctionalExpressions and their ExampleDatabase2.1 Japanese Compound FunctionalExpressionsThere exist several collections which list Japanesefunctional expressions and examine their usages.For example, (Morita and Matsuki, 1989) examine450 functional expressions and (Group Jamashii,1998) also lists 965 expressions and their exam-ple sentences.
Compared with those two collec-tions, Gendaigo Hukugouji Youreishu (NationalLanguage Research Institute, 2001) (henceforth,denoted as GHY) concentrates on 125 major func-tional expressions which have non-compositionalusages, as well as their variants5 (337 expressionsin total), and collects example sentences of thoseexpressions.
As a first step of developing a tool foridentifying Japanese compound functional expres-sions, we start with those 125 major functional ex-pressions and their variants.
In this paper, we takean approach of regarding each of those variants asa fixed expression, rather than a semi-fixed expres-sion or a syntactically-flexible expression (Sag etal., 2002).
Then, we focus on evaluating the ef-fectiveness of straightforwardly applying a stan-5For each of those 125 major expressions, the differencesbetween it and its variants are summarized as below: i) in-sertion/deletion/alternation of certain particles, ii) alternationof synonymous words, iii) normal/honorific/conversationalforms, iv) base/adnominal/negative forms.26Table 3: Examples of Classifying Functional/Content UsagesExpression Example sentence (English translation) Usage(1) ????
?????????????
?????????
?functional(to-naru-to) (The situation is serious if it is not effec-tive against this disease.)(????
(to-naru-to) = if)(2) ????
???????????????????
???????
?content(to-naru-to) (They think that it will become a require-ment for him to be the president.)(?????
(to-naru-to)= that (something) becomes ?
)(3) ?????
????????
?????
?????????????
?functional(ni-kakete-ha) (He has a great talent for earning money.)
(??????
(ni-kakete-ha)= for ?
)(4) ?????
????
?????
????
content(ni-kakete-ha) (I do not worry about it.
)( (??)??????((?
)-wo-ki-ni-kakete-ha)= worry about ?
)(5) ???
???????
???
???????
?functional(to-iu) (I heard that he is alive.)
(????
(to-iu) = that ?
)(6) ???
?????????????
?????
content(to-iu) (Somebody says ?Please visit us.?.)
(????
(to-iu)= say (that) ?
)(7) ???
????????????
???
?
functional(te-ii) (You may have a break after we finish thisdiscussion.)(????
(te-ii) = may ?
)(8) ???
?????????
???
?
content(te-ii) (This bag is nice because it is big.)
(????
(te-ii)= nice because ?
)dard chunking technique to the task of identifyingJapanese compound functional expressions.As in Table 2, according to their grammat-ical functions, those 337 expressions in totalare roughly classified into post-positional particletype, and auxiliary verb type.
Functional expres-sions of post-positional particle type are furtherclassified into three subtypes: i) those subsequentto a predicate and modifying a predicate, whichmainly function as conjunctive particles and areused for constructing subordinate clauses, ii) thosesubsequent to a nominal, and modifying a predi-cate, which mainly function as case-marking parti-cles, iii) those subsequent to a nominal, and modi-fying a nominal, which mainly function as adnom-inal particles and are used for constructing adnom-inal clauses.
For each of those types, Table 2 alsoshows the number of major expressions as well asthat of their variants listed in GHY, and an exam-ple expression.
Furthermore, Table 3 gives exam-ple sentences of those example expressions as wellas the description of their usages.2.2 Issues on Identifying CompoundFunctional Expressions in a TextThe task of identifying Japanese compound func-tional expressions roughly consists of detectingcandidates of compound functional expressions ina text and of judging the usages of those can-didate expressions.
The class of Japanese com-pound functional expressions can be regarded asclosed and their number is at most a few thousand.27Table 4: Examples of Detecting more than one Candidate ExpressionExpression Example sentence (English translation) Usage(9) ???
?????
???
????????
functional(to-iu) (That?s why a match is not so easy.)
(NP1???
(to-iu)NP2= NP2called as NP1)(10) ??????
???
??????
????????
functional(to-iu-mono-no) (Although he won, the score is bad.)(???????
(to-iu-mono-no)= although ?
)Therefore, it is easy to enumerate all the com-pound functional expressions and their morphemesequences.
Then, in the process of detecting can-didates of compound functional expressions in atext, the text are matched against the morphemesequences of the compound functional expressionsconsidered.Here, most of the 125 major functional expres-sions we consider in this paper are compound ex-pressions which consist of one or more contentwords as well as functional words.
As we intro-duced with the examples of Table 1, it is oftenthe case that they have both a compositional con-tent word usage as well as a non-compositionalfunctional usage.
For example, in Table 3, theexpression ?????
(to-naru-to)?
in the sen-tence (2) has the meaning ?
that (something) be-comes ?
?, which corresponds to a literal concate-nation of the usages of the constituents: the post-positional particle ??
?, the verb ???
?, and thepost-positional particle ??
?, and can be regardedas a content word usage.
On the other hand, inthe case of the sentence (1), the expression ?????
(to-naru-to)?
has a non-compositional func-tional meaning ?if?.
Based on this discussion, weclassify the usages of those expressions into twoclasses: functional and content.
Here, functionalusages include both non-compositional and com-positional functional usages, although most of thefunctional usages of those 125 major expressionscan be regarded as non-compositional.
On theother hand, content usages include compositionalcontent word usages only.More practically, in the process of detectingcandidates of compound functional expressions ina text, it can happen that more than one can-didate expression is detected.
For example, inTable 4, both of the candidate compound func-tional expressions ????
(to-iu)?
and ???????
(to-iu-mono-no)?
are detected in the sen-tence (9).
This is because the sequence of the twomorphemes ??
(to)?
and ???
(iu)?
constitutingthe candidate expression ????
(to-iu)?
is a sub-sequence of the four morphemes constituting thecandidate expression ???????
(to-iu-mono-no)?
as below:Morpheme sequence?
(to) ??
(iu) ??
(mono) ?
(no)Candidate expression???
(to-iu)?
(to) ??
(iu) ??
(mono) ?
(no)Candidate expression??????
(to-iu-mono-no)?
(to) ??
(iu) ??
(mono) ?
(no)This is also the case with the sentence (10).Here, however, as indicated in Table 4, the sen-tence (9) is an example of the functional usage ofthe compound functional expression ????
(to-iu)?, where the sequence of the two morphemes ??
(to)?
and ???
(iu)?
should be identified andchunked into a compound functional expression.On the other hand, the sentence (10) is an ex-ample of the functional usage of the compoundfunctional expression ???????
(to-iu-mono-no)?, where the sequence of the four morphemes ??
(to)?, ???
(iu)?, ???
(mono)?, and ??
(no)?should be identified and chunked into a compoundfunctional expression.
Actually, in the result ofour preliminary corpus study, at least in about 20%of the occurrences of Japanese compound func-tional expressions, more than one candidate ex-pression can be detected.
This result indicates thatit is necessary to consider more than one candidateexpression in the task of identifying a Japanesecompound functional expression, and also in thetask of classifying the functional/content usage ofa candidate expression.
Thus, in this paper, basedon this observation, we formalize the task of iden-tifying Japanese compound functional expressionsas a chunking problem, rather than a classificationproblem.28Table 5: Number of Sentences collected from1995 Mainichi Newspaper Texts (for 337 Expres-sions)# of expressions50 ?
# of sentences 187 (55%)0 < # of sentences < 50 117 (35%)# of sentences = 0 33 (10%)2.3 Developing an Example DatabaseWe developed an example database of Japanesecompound functional expressions, which is usedfor training/testing a chunker of Japanese com-pound functional expressions (Tsuchiya et al,2005).
The corpus from which we collect examplesentences is 1995 Mainichi newspaper text corpus(1,294,794 sentences, 47,355,330 bytes).
For eachof the 337 expressions, 50 sentences are collectedand chunk labels are annotated according to thefollowing procedure.1.
The expression is morphologically analyzedby ChaSen, and its morpheme sequence6 isobtained.2.
The corpus is morphologically analyzed byChaSen, and 50 sentences which include themorpheme sequence of the expression arecollected.3.
For each sentence, every occurrence of the337 expressions is annotated with one of theusages functional/content by an annotator7.Table 5 classifies the 337 expressions accord-ing to the number of sentences collected from the1995 Mainichi newspaper text corpus.
For morethan half of the 337 expressions, more than 50 sen-tences are collected, although about 10% of the377 expressions do not appear in the whole cor-pus.
Out of those 187 expressions with more than50 sentences, 52 are those with balanced distribu-tion of the functional/content usages in the news-paper text corpus.
Those 52 expressions can be re-garded as among the most difficult ones in the taskof identifying and classifying functional/content6For those expressions whose constituent has conjugationand the conjugated form also has the same usage as the ex-pression with the original form, the morpheme sequence isexpanded so that the expanded morpheme sequences includethose with conjugated forms.7For the most frequent 184 expressions, on the average,the agreement rate between two human annotators is 0.93 andthe Kappa value is 0.73, which means allowing tentative con-clusions to be drawn (Carletta, 1996; Ng et al, 1999).
For65% of the 184 expressions, the Kappa value is above 0.8,which means good reliability.usages.
Thus, this paper focuses on those 52 ex-pressions in the training/testing of chunking com-pound functional expressions.
We extract 2,600sentences (= 52 expressions ?
50 sentences) fromthe whole example database and use them fortraining/testing the chunker.
The number of themorphemes for the 2,600 sentences is 92,899.
Weignore the chunk labels for the expressions otherthan the 52 expressions, resulting in 2,482/701chunk labels for the functional/content usages, re-spectively.3 Chunking Japanese CompoundFunctional Expressions with SVMs3.1 Support Vector MachinesThe principle idea of SVMs is to find a separatehyperplane that maximizes the margin betweentwo classes (Vapnik, 1998).
If the classes are notseparated by a hyperplane in the original inputspace, the samples are transformed in a higher di-mensional features space.Giving x is the context (a set of features) ofan input example; xi and yi(i = 1, ..., l, xi ?Rn, yi?
{1,?1}) indicate the context of the train-ing data and its category, respectively; The deci-sion function f in SVM framework is defined as:f(x) = sgn( l?i=1?iyiK(xi,x) + b)(1)where K is a kernel function, b ?
R is a thresh-old, and ?i are weights.
Besides, the weights ?isatisfy the following constraints:0 ?
?i ?
C (i = 1, ..., l) (2)?li=1 ?iyi = 0 (3)where C is a misclassification cost.
The xi withnon-zero ?i are called support vectors.
To trainan SVM is to find the ?i and the b by solving theoptimization problem; maximizing the followingunder the constraints of (2) and (3):L(?)
=l?i=1?i?12l?i,j=1?i?jyiyjK(xi,xj) (4)The kernel function K is used to transform thesamples in a higher dimensional features space.Among many kinds of kernel functions available,we focus on the d-th polynomial kernel:K(x,y) = (x ?
y + 1)d (5)29Through experimental evaluation on chunkingJapanese compound functional expressions, wecompared polynomial kernels with d = 1, 2, and3.
Kernels with d = 2 and 3 perform best, whilethe kernel with d = 3 requires much more compu-tational cost than that with d = 2.
Thus, through-out the paper, we show results with the quadratickernel (d = 2).3.2 Chunking with SVMsThis section describes details of formalizing thechunking task using SVMs.
In this paper, we usean SVMs-based chunking tool YamCha8 (Kudoand Matsumoto, 2001).
In the SVMs-basedchunking framework, SVMs are used as classi-fiers for assigning labels for representing chunksto each token.
In our task of chunking Japanesecompound functional expressions, each sentenceis represented as a sequence of morphemes, wherea morpheme is regarded as a token.3.2.1 Chunk RepresentationFor representing proper chunks, we employIOB2 representation, one of those which havebeen studied well in various chunking tasks of nat-ural language processing (Tjong Kim Sang, 1999;Kudo and Matsumoto, 2001).
This method usesthe following set of three labels for representingproper chunks.I Current token is a middle or the end of achunk consisting of more than one token.O Current token is outside of any chunk.B Current token is the beginning of a chunk.As we described in section 2.2, given a candi-date expression, we classify the usages of the ex-pression into two classes: functional and content.Accordingly, we distinguish the chunks of the twotypes: the functional type chunk and the contenttype chunk.
In total, we have the following five la-bels for representing those chunks: B-functional,I-functional, B-content, I-content, and O. Ta-ble 6 gives examples of those chunk labels rep-resenting chunks.Finally, as for exending SVMs to multi-classclassifiers, we experimentally compare the pair-wise method and the one vs. rest method, wherethe pairwise method slightly outperformed the onevs.
rest method.
Throughout the paper, we showresults with the pairwise method.8http://chasen.org/?taku/software/yamcha/3.2.2 FeaturesFor the feature sets for training/testing ofSVMs, we use the information available in the sur-rounding context, such as the morphemes, theirparts-of-speech tags, as well as the chunk labels.More precisely, suppose that we identify the chunklabel ci for the i-th morpheme:??
Parsing Direction ?
?Morpheme mi?2mi?1mimi+1mi+2Feature set Fi?2Fi?1FiFi+1Fi+2at a positionChunk label ci?2ci?1ciHere, mi is the morpheme appearing at i-th po-sition, Fi is the feature set at i-th position, and ciis the chunk label for i-th morpheme.
Roughlyspeaking, when identifying the chunk label ci forthe i-th morpheme, we use the feature sets Fi?2,Fi?1, Fi, Fi+1, Fi+2 at the positions i ?
2, i ?
1,i, i + 1, i + 2, as well as the preceding two chunklabels ci?2 and ci?1.The detailed definition of the feature set Fi at i-th position is given below.
The feature set Fi is de-fined as a tuple of the morpheme feature MF (mi)of the i-th morpheme mi, the chunk candidate fea-ture CF (i) at i-th position, and the chunk contextfeature OF (i) at i-th position.Fi = ?
MF (mi), CF (i), OF (i) ?The morpheme feature MF (mi) consists of thelexical form, part-of-speech, conjugation type andform, base form, and pronunciation of mi.The chunk candidate feature CF (i) and thechunk context feature OF (i) are defined consid-ering the candidate compound functional expres-sion, which is a sequence of morphemes includ-ing the morpheme mi at the current position i. Aswe described in section 2, the class of Japanesecompound functional expressions can be regardedas closed and their number is at most a few thou-sand.
Therefore, it is easy to enumerate all thecompound functional expressions and their mor-pheme sequences.
Chunk labels other than Oshould be assigned to a morpheme only when itconstitutes at least one of those enumerated com-pound functional expressions.
Suppose that a se-quence of morphemes mj .
.
.
mi .
.
.
mk includingmi at the current position i constitutes a candidatefunctional expression E as below:mj?2mj?1mj.
.
.
mi.
.
.
mkmk+1mk+2candidate E ofa compoundfunctional expressionwhere the morphemes mj?2, mj?1, mk+1, andmk+2 are at immediate left/right contexts of E.Then, the chunk candidate feature CF (i) at i-thposition is defined as a tuple of the number of mor-phemes constituting E and the position of mi inE.
The chunk context feature OF (i) at i-th posi-tion is defined as a tuple of the morpheme features30Table 6: Examples of Chunk Representation and Chunk Candidate/Context Features(a) Sentence (7) of Table 3(English Chunk candidate Chunk contextMorpheme translation) Chunk label feature feature??
(kono) (this) O ?
???
(giron) (discussion) O ?
??
(ga) (NOM) O ?
????
(owatt) (finish) O ?
???
(tara) (after) O ?
???
(kyuukei) (break) O ?
??
(shi) (have) O ?
??
(te) (may) B-functional ?2, 1?
?
MF (??
(kyuukei)), ?, MF (?
(shi)), ?,??
(ii) I-functional ?2, 2?
MF (?
(period)), ?, ?, ?
??
(period) (period) O ?
?
(b) Sentence (8) of Table 3(English Chunk candidate Chunk contextMorpheme translation) Chunk label feature feature??
(kono) (this) O ?
????
(bag) (discussion) O ?
??
(ha) (TOP) O ?
????
(ookiku) (big) O ?
??
(te) (because) B-content ?2, 1?
?
MF (?
(ha)), ?, MF (???
(ookiku)), ?,??
(ii) (nice) I-content ?2, 2?
MF (?
(period)), ?, ?, ?
??
(period) (period) O ?
?as well as the chunk candidate features at immedi-ate left/right contexts of E.CF (i) = ?
length of E, position of miin E ?OF (i) = ?
MF (mj?2), CF (j ?
2),MF (mj?1), CF (j ?
1),MF (mk+1), CF (k + 1),MF (mk+2), CF (k + 2) ?Table 6 gives examples of chunk candidate fea-tures and chunk context featuresIt can happen that the morpheme at the cur-rent position i constitutes more than one candidatecompound functional expression.
For example,in the example below, the morpheme sequencesmi?1mimi+1, mi?1mi, and mimi+1mi+2 consti-tute candidate expressions E1, E2, and E3, respec-tively.Morpheme sequence mi?1mimi+1mi+2Candidate E1mi?1mimi+1Candidate E2mi?1miCandidate E3mimi+1mi+2In such cases, we prefer the one starting with theleftmost morpheme.
If more than one candidateexpression starts with the leftmost morpheme, weprefer the longest one.
In the example above, weprefer the candidate E1and construct the chunkcandidate features and chunk context features con-sidering E1only.4 Experimental EvaluationThe detail of the data set we use in the experimen-tal evaluation was presented in section 2.3.
As weshow in Table 7, performance of our SVMs-basedchunkers as well as several baselines including ex-isting Japanese text processing tools is evaluatedin terms of precision/recall/F?=1 of identifyingfunctional chunks.
Performance is evaluated alsoin terms of accuracy of classifying detected can-didate expressions into functional/content chunks.Among those baselines, ?majority ( = functional)?always assigns functional usage to the detectedcandidate expressions.
?Hand-crafted rules?
aremanually created 145 rules each of which has con-ditions on morphemes constituting a compoundfunctional expression as well as those at immedi-ate left/right contexts.
Performance of our SVMs-based chunkers is measured through 10-fold crossvalidation.As shown in Table 7, our SVMs-based chunkerssignificantly outperform those baselines both inF?=1 and classification accuracy9.
We also evalu-ate the effectiveness of each feature set, i.e., themorpheme feature, the chunk candidate feature,and the chunk context feature.
The results in thetable show that the chunker with the chunk candi-date feature performs almost best even without thechunk context feature10.9Recall of existing Japanese text processing tools is low,because those tools can process only 50?60% of the whole52 compound functional expressions, and for the remaining40?50% expressions, they fail in identifying all of the occur-rences of functional usages.10It is also worthwhile to note that training the SVMs-based chunker with the full set of features requires computa-tional cost three times as much as training without the chunk31Table 7: Evaluation Results (%)Identifying Acc.
of classifyingfunctional chunks functional/contentPrec.
Rec.
F?=1 chunksmajority ( = functional) 78.0 100 87.6 78.0Baselines Juman/KNP 89.2 49.3 63.5 55.8ChaSen/CaboCha 89.0 45.6 60.3 53.2hand-crafted rules 90.7 81.6 85.9 79.1SVM morpheme 88.0 91.0 89.4 86.5(feature morpheme + chunk-candidate 91.0 93.2 92.1 89.0set) morpheme + chunk-candidate/context 91.1 93.6 92.3 89.2Figure 1: Change of F?=1 with Different Numberof Training InstancesFor the SVMs-based chunker with the chunkcandidate feature with/without the chunk contextfeature, Figure 1 plots the change of F?=1 whentraining with different number of labeled chunksas training instances.
With this result, the increasein F?=1 seems to stop with the maximum num-ber of training instances, which supports the claimthat it is sufficient to collect and manually annotateabout 50 training examples per expression.5 Concluding RemarksThe Japanese language has various types of com-pound functional expressions, which are very im-portant for recognizing the syntactic structures ofJapanese sentences and for understanding their se-mantic contents.
In this paper, we formalizedthe task of identifying Japanese compound func-tional expressions in a text as a chunking prob-lem.
We applied a machine learning techniqueto this task, where we employed that of Sup-port Vector Machines (SVMs).
We showed thatthe proposed method significantly outperforms ex-isting Japanese text processing tools.
The pro-context feature.posed framework has advantages over an approachbased on manually created rules such as the one in(Shudo et al, 2004), in that it requires human costto manually create and maintain those rules.
Onthe other hand, in our framework based on the ma-chine learning technique, it is sufficient to collectand manually annotate about 50 training examplesper expression.ReferencesJ.
Carletta.
1996.
Assessing agreement on classificationtasks: the Kappa statistic.
Computational Linguistics,22(2):249?254.Group Jamashii, editor.
1998.
Nihongo Bunkei Jiten.Kuroshio Publisher.
(in Japanese).T.
Kudo and Y. Matsumoto.
2001.
Chunking with supportvector machines.
In Proc.
2nd NAACL, pages 192?199.J.
Mayfield, P. McNamee, and C. Piatko.
2003.
Named entityrecognition using hundreds of thousands of features.
InProc.
7th CoNLL, pages 184?187.Y.
Morita and M. Matsuki.
1989.
Nihongo Hyougen Bunkei,volume 5 of NAFL Sensho.
ALC.
(in Japanese).National Language Research Institute.
2001.
GendaigoHukugouji Youreishu.
(in Japanese).H.
T. Ng, C. Y. Lim, and S. K. Foo.
1999.
A case study oninter-annotator agreement for word sense disambiguation.In Proc.
ACL SIGLEXWorkshop on Standardizing LexicalResources, pages 9?13.I.
Sag, T. Baldwin, F. Bond, A. Copestake, and D. Flickinger.2002.
Multiword expressions: A pain in the neck for NLP.In Proc.
3rd CICLING, pages 1?15.K.
Shudo, T. Tanabe, M. Takahashi, and K. Yoshimura.
2004.MWEs as non-propositional content indicators.
In Proc.2nd ACL Workshop on Multiword Expressions: Integrat-ing Processing, pages 32?39.E.
Tjong Kim Sang.
1999.
Representing text chunks.
InProc.
9th EACL, pages 173?179.M.
Tsuchiya, T. Utsuro, S. Matsuyoshi, S. Sato, and S. Nak-agawa.
2005.
A corpus for classifying usages of Japanesecompound functional expressions.
In Proc.
PACLING,pages 345?350.V.
N. Vapnik.
1998.
Statistical Learning Theory.
Wiley-Interscience.32
