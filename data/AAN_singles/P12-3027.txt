Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 157?162,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsFLOW: A First-Language-Oriented Writing Assistant SystemMei-Hua Chen*, Shih-Ting Huang+, Hung-Ting Hsieh*, Ting-Hui Kao+, Jason S. Chang+* Institute of Information Systems and Applications+ Department of Computer ScienceNational Tsing Hua UniversityHsinChu, Taiwan, R.O.C.
30013{chen.meihua,koromiko1104,vincent732,maxis1718,jason.jschang}@gmail.comAbstractWriting in English might be one of the mostdifficult tasks for EFL (English as a ForeignLanguage) learners.
This paper presentsFLOW, a writing assistance system.
It is builtbased on first-language-oriented input functionand context sensitive approach, aiming atproviding immediate and appropriatesuggestions including translations, paraphrases,and n-grams during composing and revisingprocesses.
FLOW is expected to help EFLwriters achieve their writing flow without beinginterrupted by their insufficient lexicalknowledge.1.
IntroductionWriting in a second language (L2) is a challengingand complex process for foreign language learners.Insufficient lexical knowledge and limitedexposure to English might interrupt their writingflow (Silva, 1993).
Numerous writing instructionshave been proposed (Kroll, 1990) as well aswriting handbooks have been available forlearners.
Studies have revealed that during thewriting process, EFL learners show the inclinationto rely on their native languages (Wolfersberger,2003) to prevent a breakdown in the writingprocess (Arndt, 1987; Cumming, 1989).
However,existing writing courses and instruction materials,almost second-language-oriented, seem unable todirectly assist EFL writers while writing.This paper presents FLOW1 (Figure 1), aninteractive system for assisting EFL writers in1 FLOW: http:// flowacldemo.appspot.comcomposing and revising writing.
Different fromexisting tools, its context-sensitive and first-language-oriented features enable EFL writers toconcentrate on their ideas and thoughts withoutbeing hampered by the limited lexical resources.Based on the studies that first language use canpositively affect second language composing,FLOW attempts to meet such needs.
Given any L1input, FLOW displays appropriate suggestionsincluding translation, paraphrases, and n-gramsduring composing and revising processes.
We usethe following example sentences to illustrate thesetwo functionalities.Consider the sentence ?We propose a methodto?.
During the composing stage, suppose a writeris unsure of the phrase ?solve the problem?, hecould write ?????
?, a corresponding word inhis native language, like ?We propose a method to?????.
The writer?s input in the writing areaof FLOW actively triggers a set of translationsuggestions such as ?solve the problem?
and?tackle the problem?
for him/her to complete thesentence.In the revising stage, the writer intends toimprove or correct the content.
He/She is likely tochange the sentence illustrated above into ?We tryall means to solve the problem.?
He would selectthe phrase ?propose a method?
in the originalsentence and input a L1 phrase ???
?, whichspecifies the meaning he prefers.
The L1 inputtriggers a set of context-aware suggestionscorresponding to the translations such as ?try ourbest?
and ?do our best?
rather than ?try your best?and ?do your best?.
The system is able to do thatmainly by taking a context-sensitive approach.FLOW then inserts the phrase the writer selectsinto the sentence.157Figure 1.
Screenshot of FLOWIn this paper, we propose a context-sensitivedisambiguation model which aims to automaticallychoose the appropriate phrases in different contextswhen performing n-gram prediction, paraphrasesuggestion and translation tasks.
As described in(Carpuat and Wu, 2007), the disambiguation modelplays an important role in the machine translationtask.
Similar to their work, we further integrate themulti-word phrasal lexical disambiguation modelto the n-gram prediction model, paraphrase modeland translation model of our system.
With thephrasal disambiguation model, the output of thesystem is sensitive to the context the writer isworking on.
The context-sensitive feature helpswriters find the appropriate phrase whilecomposing and revising.This paper is organized as follows.
We reviewthe related work in the next section.
In Section 3,we brief our system and method.
Section 4 reportsthe evaluation results.
We conclude this paper andpoint out future directions to research in Section 5.2.
Related Work2.1 Sub-sentential paraphrasesA variety of data-driven paraphrase extractiontechniques have been proposed in the literature.One of the most popular methods leveragingbilingual parallel corpora is proposed by Bannardand Callison-Burch (2005).
They identifyparaphrases using a phrase in another language as apivot.
Using bilingual parallel corpora forparaphrasing demonstrates the strength of semanticequivalence.
Another line of research furtherconsiders context information to improve theperformance.
Instead of addressing the issue oflocal paraphrase acquisition, Max (2009) utilizesthe source and target contexts to extract sub-sentential paraphrases by using pivot SMTsystems.2.2 N-gram suggestionsAfter a survey of several existing writing tools, wefocus on reviewing two systems closely related toour study.PENS (Liu et al 2000), a machine-aided Englishwriting system, provides translations of thecorresponding English words or phrases forwriters?
reference.
Different from PENS, FLOWfurther suggests paraphrases to help writers revisetheir writing tasks.
While revising, writers wouldalter the use of language to express their thoughts.The suggestions of paraphrases could meet theirneed, and they can reproduce their thoughts morefluently.Another tool, TransType (Foster, 2002), a texteditor, provides translators with appropriatetranslation suggestions utilizing trigram languagemodel.
The differences between our system andTransType lie in the purpose and the input.
FLOWaims to assist EFL writers whereas TransType is atool for skilled translators.
On the other hand, inTransType, the human translator types translationof a given source text, whereas in FLOW the input,158either a word or a phrase, could be source or targetlanguages.2.3 Multi-word phrasal lexical disambiguationIn the study more closely related to our work,Carpuat and Wu (2007) propose a novel method totrain a phrasal lexical disambiguation model tobenefit translation candidates selection in machinetranslation.
They find a way to integrate the state-of-the-art Word Sense Disambiguation (WSD)model into phrase-based statistical machinetranslation.
Instead of using predefined sensesdrawn from manually constructed senseinventories, their model directly disambiguatesbetween all phrasal translation candidates seenduring SMT training.
In this paper, we also use thephrasal lexical disambiguation model; however,apart from using disambiguation model to helpmachine translation, we extend the disambiguationmodel.
With the help of the phrasal lexicaldisambiguation model, we build three models: acontext-sensitive n-gram prediction model, aparaphrase suggestion model, and a translationmodel which are introduced in the followingsections.3.
Overview of FLOWThe FLOW system helps language learners in twoways: predicting n-grams in the composing stageand suggesting paraphrases in the revising stage(Figure 2).3.1  System architectureComposing StageDuring the composing process, a user inputs S.FLOW first determines if the last few words of S isa L1 input.
If not, FLOW takes the last k words topredict the best matching following n-grams.Otherwise, the system uses the last k words as thequery to predict the corresponding n-gramtranslation.
With a set of prediction (eithertranslations or n-grams), the user could choose anappropriate suggestion to complete the sentence inthe writing area.NOWriting processInput KK consists of  first languageFirst-Language-Oriented N-gramPredictionUser interfaceContext-Sensitive N-gram PredictionYESRevising processGet word sequence L and Rsurrounding user selected text KForeign Language Fis inputOntext-Sensitive ParaphraseSuggestionFirst-Language-Oriented ParaphraseSuggestionUser interfaceInput SNO  YES159Figure 2.
Overall Architecture of FLOW in writing andrevising processesRevising StageIn the revising stage, given an input I and the userselected words K, FLOW obtains the wordsequences L and R surrounding K as reference forprediction.
Next, the system suggests sub-sentential paraphrases for K based on theinformation of L and R. The system then searchesand ranks the translations.3.2  N-gram predictionIn the n-gram prediction task, our model takes thelast k words with m 2 English words and n foreignlanguage words, {e1, e2, ?em, f1, f2 ?fn}, of thesource sentences S as the input.
The output wouldbe a set of n-gram predictions.
These n-grams canbe concatenated to the end of the user-composedsentence fluently.Context-Sensitive N-gram Prediction (CS-NP)The CS-NP model is triggered to predict afollowing n-gram when a user composes sentencesconsisted of only English words with no foreignlanguage words, namely, n is equal to 0.
The goalof the CS-NP model is to find the English phrase ethat maximizes the language model probability ofthe word sequence, {e1, e2, ?em, e}:?
?
argmax?,???????|?
?, ?
?, ?
??????|?
?, ?
?, ?
???
?
???
?, ?
?, ?
?
?, ?????
?, ?
?, ?
??
?Translation-based N-gram Prediction (TB-NP)When a user types a set of L1 expression f = { f1, f2?fn }, following the English sentences S, theFLOW system will predict the possible translationsof f. A simple way to predict the translations is tofind the bilingual phrase alignments T(f) using themethod proposed by (Och and Ney, 2003).However, the T(f) is ambiguous in differentcontexts.
Thus, we use the context {e1, e2, ?em}proceeding f to fix the prediction of the translation.Predicting the translation e can be treated as a sub-sentential translation task:2 In this paper, m = 5.?
?
argmax??????
???|??
?, ?
?, ?
??
?,where we use the user-composed context {e1, e2,?em} to disambiguate the translation of f.Although there exist more sophisticated modelswhich could make a better prediction, a simplena?ve-Bayes model is shown to be accurate andefficient in the lexical disambiguation taskaccording to (Yarowsky and Florian, 2002).Therefore, in this paper, a na?ve-Bayes model isused to disambiguate the translation of f. Inaddition to the context-word feature, we also usethe context-syntax feature, namely surroundingPOS tag Pos, to constrain the syntactic structure ofthe prediction.
The TB-NP model could berepresented in the following equation:??
?
argmax?????
?1, ?2,?
?
?, 1?, ???,2??,????
?
??
?, ?
?, ?
??
?According to the Bayes theorem,????
?1, ?2, ??
?, 1?, ???,2????????|??
?
????????????????
?The probabilities can be estimated using a parallelcorpus, which is also used to obtain bilingualphrase alignment.3.3  Paraphrase SuggestionUnlike the N-gram prediction, in the paraphrasesuggestion task, the user selects k words, {e1, e2,?ek}, which he/she wants to paraphrase.
Themodel takes the m words {r1, r2, ?rm} and n words{l1, l2, ?ln} in the right and left side of the user-selected k words respectively.
The system alsoaccepts an additional foreign language input, {f1,f2,?fl}, which helps limit the meaning of suggestedparaphrases to what the user really wants.
Theoutput would be a set of paraphrase suggestionsthat the user-selected phrases can be replaced bythose paraphrases precisely.Context-Sensitive Paraphrase Suggestion (CS-PS)The CS-PS model first finds a set of localparaphrases P of the input phrase K using the160pivot-based method proposed by Bannard andCallison-Burch (2005).
Although the pivot-basedmethod has been proved efficient and effective infinding local paraphrases, the local paraphrasesuggestions may not fit different contexts.
Similarto the previous n-gram prediction task, we use thena?ve-Bayes approach to disambiguate these localparaphrases.
The task is to find the best e such thate with the highest probability for the given contextR and L. We further require paraphrases to havesimilar syntactic structures to the user-selectedphrase in terms of POS tags, Pos.??
?
argmax?????
?|?1, ?2,?
?
?, 1?, 2?,?
???,??
?Translation-based Paraphrase Suggestion (TB-PS)After the user selects a phrase for paraphrasing,with a L1 phrase F as an additional input, thesuggestion problem will be:??
?
argmax???????????????????|?
?, ?
?, ?
?
?, ?
?, ?
?, ?
?
?, ???
?The TB-PS model disambiguates paraphrases fromthe translations of F instead of paraphrases P.4.
Experimental ResultsIn this section, we describe the experimentalsetting and the preliminary results.
Instead oftraining a whole machine translation using toolkitssuch as Moses (Koehn et.
al, 2007), we used onlybilingual phrase alignment as translations toprevent from the noise produced by the machinetranslation decoder.
Word alignments wereproduced using Giza++ toolkit (Och and Ney,2003), over a set of 2,220,570 Chinese-Englishsentence pairs in Hong Kong Parallel Text(LDC2004T08) with sentences segmented usingthe CKIP Chinese word segmentation system (Maand Chen, 2003).
In training the phrasal lexicaldisambiguation model, we used the English part ofHong Kong Parallel Text as our training data.To assess the effectiveness of FLOW, we selected10 Chinese sentences and asked two students totranslate the Chinese sentences to Englishsentences using FLOW.
We kept track of thesentences the two students entered.
Table 1 showsthe selected results.Model ResultsTB-PS ???
?, the price of rice...in shortall in allin a nutshellin a wordto sum upCS-PS She looks forward to cominglook forward tolooked forward tois looking forward toforward toexpectCS-PS there is no doubt that ?there is no questionit is beyond doubtI have no doubtbeyond doubtit is trueCS-NP We put forward ?the proposaladditionalour opinionthe motionthe billTB-NP ...on ways to identify tackle ?
?money launderingmoneyhisforum entitledmoney laundryTable 1.
The preliminary results of FLOWBoth of the paraphrase models CS-PS and TB-PSperform quite well in assisting the user in thewriting task.
However, there are still someproblems such as the redundancy suggestions, e.g.,?look forward to?
and ?looked forward to?.Besides, although we used the POS tags asfeatures, the syntactic structures of the suggestionsare still not consistent to an input or selectedphrases.
The CS-NP and the TB-NP model alsoperform a good task.
However, the suggestedphrases are usually too short to be a semantic unit.The disambiguation model tends to produce shorterphrases because they have more common contextfeatures.1615.
Conclusion and Future WorkIn this paper, we presented FLOW, an interactivewriting assistance system, aimed at helping EFLwriters compose and revise without interruptingtheir writing flow.
First-language-oriented andcontext-sensitive features are two maincontributions in this work.
Based on the studies onsecond language writing that EFL writers tend touse their native language to produce texts and thentranslate into English, the first-language-orientedfunction provides writers with appropriatetranslation suggestions.
On the other hand, due tothe fact that selection of words or phrases issensitive to syntax and context, our systemprovides suggestions depending on the contexts.Both functions are expected to improve EFLwriters?
writing performance.In future work, we will conduct experiments togain a deeper understanding of EFL writers?writing improvement with the help of FLOW, suchas integrating FLOW into the writing courses toobserve the quality and quantity of students?writing performance.
Many other avenues exist forfuture research and improvement of our system.For example, we are interested in integrating theerror detection and correction functions intoFLOW to actively help EFL writers achieve betterwriting success and further motivate EFL writersto write with confidence.ReferencesValerie Arndt.
1987.
Six writers in search of texts: Aprotocol based study of L1 and L2 writing.
ELTJournal, 41, 257-267.Colin Bannard and Chris Callison-Burch.
2005.Paraphrasing with bilingual parallel corpora.
InProceedings of ACL, pp.
597-604.Marine Carpuat and Dekai Wu.
2007.
ImprovingStatistical Machine Translation using Word SenseDisambiguation.
In Proceedings of EMNLP-CoNLL,pp 61?72.Alister Cumming.
1989.
Writing expertise and secondlanguage proficiency.
Language Learning, 39, 81-141.George Foster, Philippe Langlais, and Guy Lapalme.2002.
Transtype: Text prediction for translators.
InProceedings of ACL Demonstrations, pp.
93-94.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan,Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstrantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of ACL Demonstration Session, pp.177?180.Barbara Kroll.
1990.
Second Language Writing:Research Insights for the Classroom.
CambridgeUniversity Press, Cambridge.Aur?elien Max.
2009.
Sub-sentential Paraphrasing byContextual Pivot Translation.
In Proceedings of the2009 Workshop on Applied Textual Inference, ACL-IJCNLP, pp 18-26.Tony Silva.
1993.
Toward an Understanding of theDistinct Nature of L2 Writing: The ESL Researchand Its Implications.
TESOL Quarterly 27(4): 657?77.Liu, Ting, Mingh Zhou, JianfengGao, Endong Xun, andChangning Huan.
2000.
PENS: A Machine-AidedEnglish Writing System for Chinese Users.
InProceedings of ACL, pp 529-536.Mark Wolfersberger.
2003.
L1 to L2 writing processand strategy transfer: a look at lower proficiencywriters.
TESL-EJ: Teaching English as a Second orForeign Language, 7(2), A6 1-15.162
