Ontology-based Contextual Coherence ScoringRobert Porzel Iryna Gurevych Christof E. Mu?llerEuropean Media LaboratorySchloss-Wolfsbrunnenweg 31cD-69118 Heidelberg{porzel,gurevych,mueller2@eml.org}AbstractIn this paper we present a contextual ex-tension to ONTOSCORE, a system forscoring sets of concepts on the basis ofan ontology.
We apply the contextuallyenhanced system to the task of scoringalternative speech recognition hypothe-ses (SRH) in terms of their semantic co-herence.
We conducted several annota-tion experiments and showed that humanannotators can reliably differentiate be-tween semantically coherent and incoher-ent speech recognition hypotheses (bothwith and without discourse context).
Wealso showed, that annotators can reliablyidentify the overall best hypothesis froma given n-best list.
While the originalONTOSCORE system correctly assigns thehighest score to 84.06% of the corpus,the inclusion of the conceptual context in-creases the number of correct classifica-tions to yield 86.76%, given a baseline of63.91% in both cases.1 IntroductionFollowing Allen et al (2001), we can distinguishbetween controlled and conversational dialogue sys-tems.
Since controlled and restricted interactionsbetween the user and the system increase recogni-tion and understanding accuracy, such systems arereliable enough to be deployed in various real worldapplications, e.g.
public transportation or cinema in-formation systems.
The more conversational a dia-logue system becomes, the less predictable are theusers?
utterances.
Recognition and processing be-come increasingly difficult and unreliable.Today?s dialogue systems employ domain- anddiscourse-specific knowledge bases, so-called on-tologies, to represent the individual discourse enti-ties as concepts as well as their relations to eachother.
In this paper we employ an algorithm for mea-suring the semantic coherence of sets of concepts us-ing such an ontology and show how its performancecan be improved by means of an inclusion of theconceptual context.
Thereby creating a method forscoring the contextual coherence of individual setsof concepts.In the following, we will show how the contex-tual coherence measurement can be applied to esti-mate how well a given speech recognition hypoth-esis (SRH) fits with respect to the existing knowl-edge representation and the given conceptual con-text, thereby providing a mechanism that increasesthe robustness and reliability of dialogue systems.We can, therefore, show how the algorithm can besuccessfully employed by a spoken dialogue systemto enhance the interface between automatic speechrecognition (ASR) and natural language understand-ing (NLU).In Section 2 we discuss the problem of scoringand classifying SRHs in terms of their semantic co-herence followed by a description of our annotationexperiments and the corresponding results in Sec-tion 3.
Section 4 contains a description of the kindof knowledge representations and the algorithm em-ployed by ONTOSCORE.
In Section 5 we present thecontextually enhanced system.
Evaluations of thecorresponding system for scoring SRHs are given inSection 6.
A conclusion and additional applicationsare given in Section 7.2 Semantic Coherence and SpeechRecognition HypothesesWhile a simple one-best hypothesis interface be-tween ASR and NLU suffices for restricted dialoguesystems, more complex systems either operate on n-best lists as ASR output or convert ASR word graphs(Oerder and Ney, 1993) into n-best lists, given thedistribution of acoustic and language model scores(Schwartz and Chow, 1990; Tran et al, 1996).
Forexample, in our data a user expressed the wish to getfrom Cologne to Heidelberg and then to continue hisvisit in Heidelberg, as:1(1) ichImo?chtewantaufondemtheschnellstenfastestWegwayvonfromKo?lnColognenachtoHeidelberg.Heidelberg.
(2) wiehowkommecanichIininHeidelbergHeidelbergweiter.continue.Looking at the SRHs from the ensuing n-best list ofExample (1) we found that Example (1a) constitutedthe best representation of the utterance, whereasall others constituted less adequate representationsthereof.
(1a) ichImo?chtewantaufonschnellstenfastestWegwayvonfromKo?lnColognenachtoHeidelberg.Heidelberg.
(1b) ichImo?chtewantaufonschnellstenfastestWegwayKo?lnColognenachtoHeidelberg.Heidelberg.
(1c) ichImo?chtewantFolkfolkWegwayvonfromKo?lnColognenachtoHeidelberg.Heidelberg.
(1d) ichImo?chtewantaufonschnellstenfastestWegwayvorbeforeKo?lnColognenachtoHeidelberg.Heidelberg.1All examples are displayed with the German original on topand a glossed translation below.
(1e) ichImo?chtewantvorbeforeschnellstenfastestWegwayvonfromKo?lnColognenachtoHeidelberg.Heidelberg.Facing multiple representations of a single utter-ance consequently poses the question, which of thedifferent hypotheses corresponds most likely to theuser?s utterance.
Several ways of solving this prob-lem have been proposed and implemented in var-ious systems.
Frequently the scores provided bythe ASR system itself are used, e.g.
acoustic andlanguage model probabilities.
More recently alsoscores provided by the NLU system have been em-ployed, e.g.
parsing scores (Engel, 2002) or dis-course model scores (Pfleger et al, 2002).
However,these methods often assign very high scores to SRHswhich are semantically incoherent and low scores tosemantically coherent ones.In the case of Example (1) all scores, i.e.
theacoustic, language model, parsing and the ON-TOSCORE scores assign the highest score to Exam-ple (1a) (see Table 2 for the actual numbers).
SRH1a can consequently be chosen as the best SRH.As we will show in Section 6, the scoring of theSRHs from Example (2) differs substantially, andonly the contextual coherence score manages to pickan adequate SRH.
The fact that neither of the otherscoring approaches systematically employs the sys-tem?s knowledge of the domains at hand, can re-sult in passing suboptimal SRHs through the system.This means that, while there was a better represen-tation of the actual utterance in the n-best list, theNLU system is processing an inferior one, therebycausing overall dialogue metrics, in the sense ofWalker et al (2000), to decrease.
We, therefore,propose an alternative way to rank SRHs on the ba-sis of their contextual coherence, i.e.
with respectto a given ontology representing the domains of thesystem and the given conceptual context.3 Annotation ExperimentsThe experiments reported here are based on thedata collected in hidden-operator tests where sub-jects were prompted to say certain inputs.
We ob-tained 232 dialogues, which were divided into 1479audio files with single user utterances.
Each ut-terance corresponded to a single intention, e.g.
aroute- or a sight information request.
Firstly, all ut-terances were also transcribed.
Then the audio fileswere sent to the speech recognizer.
We logged thespeech recognition output, i.e.
n-best lists of SRHsfor all utterances.
A subset of the corpus was used tolog also the scores of the recognizer, parser and thatof OntoScore - including context-independent andcontext-dependent semantic coherence scores.
Thistrial resulted in a sub-corpus of 552 utterances cor-responding to 1,375 SRHs along with the respectiveconfidence scores.We, then, conducted several annotation experi-ments with a two-fold motivation.
In the first place,it was necessary to produce a hand-annotated corpusto be used as a gold standard for the evaluation ofthe contextual coherence scores.
Furthermore, wewanted to test whether human subjects were ableto annotate the data reliably according to our anno-tation schemata.
We had two annotators speciallytrained for each of these particular annotation tasks.In an earlier annotation experiment reported inGurevych et al (2002), the task of annotators wasto classify a subset of the corpus of SRHs as eithercoherent or incoherent.
Here we randomly mixedSRHs in order to avoid contextual priming.2 In thefirst new experiment, a sub-corpus of 552 utteranceswas annotated within the discourse context, i.e.
theSRHs were presented in their original dialogue or-der.
For each SRH, a decision again had to be madewhether it is semantically coherent or incoherentwith respect to the best SRH representing the previ-ous user utterance.
Given a total of 1,375 markables,the annotators reached an agreement of 79.71%, i.e.1,096 markables.In the second new annotation experiment, the an-notators saw the SRHs together with the transcribeduser utterances.
The task of annotators was to deter-mine the best SRH from the n-best list of SRHs cor-responding to a single user utterance.
The decisionhad to be made on the basis of several criteria.
Themost important criteria was how well the SRH cap-tures the intentional content of the user?s utterance.2As reported elsewhere the resulting Kappa statistics (Car-letta, 1996) over the annotated data yields ?
= 0.7, which in-dicates that human annotators can reliably distinguish betweencoherent samples and incoherent ones.If none of the SRHs captured the user?s intention ad-equately, the decision had to be made by looking atthe actual word error rate.
In this experiment theinter-annotator agreement was 90.69%, i.e.
1,247markables out of 1,375.3 Each corpus was then tran-formed into an evaluation gold standard by means ofthe annotators agreeing on a single solution for thecases of disagreement.The aim of the work presented here, then, was toprovide a knowledge-based score, that can be em-ployed by any NLU system to select the best hypoth-esis from a given n-best list.
The corresponding ON-TOSCORE system will be described below, followedby its evaluation against the human gold standards.4 The Knowledge Base and OntoScoreIn this section, we provide a description of theunderlying algorithm and knowledge sources em-ployed by the original ONTOSCORE system (inpress).
It is important to note that the ontologyemployed in this and the previous evaluations ex-isted already and was crafted as a general knowl-edge representation for various processing moduleswithin the system.4 Ontologies have traditionallybeen used to represent general and domain specificknowledge and are employed for various natural lan-guage understanding tasks, e.g.
semantic interpreta-tion (Allen, 1987) and in spoken dialogue systems,e.g.
for discourse modeling, modality fusion anddialogue management, see also Porzel et al (2003)for an overview.
ONTOSCORE offers an additionalway of employing ontologies, i.e.
to use the knowl-edge modeled therein as the basis for evaluatingthe semantic coherence of sets of concepts.
It canbe employed independently of the specific ontologylanguage used, as the underlying algorithm oper-ates only on the nodes and named edges of the di-rected graph represented by the ontology.
The spe-cific knowledge base, e.g.
written in DAML+OIL3A Kappa-statistic suitable for measuring the reliability ofannotations is not possible in this case.
The Kappa-statistic isclass-based and cannot, therefore, be applied to the best SRHlabeling, due to the different number of SRHs in the n-best lists.Therefore, we calculated the percentage of utterances, wherethe annotators agreed on the best SRH.4Alternative knowledge representations, such as WORD-NET, could have been employed in theory as well, howevermost of the modern domains of the system, e.g.
electronic me-dia or program guides, are not covered by WORDNET.or OWL,5 is converted into a graph, consisting ofthe class hierarchy, with each class corresponding toa concept representing either an entity or a processand their slots, i.e.
the named edges of the graph cor-responding to the class properties, constraints andrestrictions.The ontology employed for the evaluation hasabout 730 concepts and 200 relations.
It includesa generic top-level ontology whose purpose is toprovide a basic structure of the world, i.e.
abstractclasses to divide the universe in distinct parts as re-sulting from the ontological analysis.6 The model-ing of Processes and Physical Objects as a kind ofevent that is continuous and homogeneous in nature,follows the frame semantic analysis used for gener-ating the FRAMENET data (Baker et al, 1998).
Thehierarchy of Processes is connected to the hierarchyof Physical Objects via slot-constraint definitions.See also (Gurevych et al, 2003b) for a further de-scription of the ontology.ONTOSCORE performs a number of processingsteps.
A first preprocessing step is to convert eachSRH into a concept representation (CR).
For thatpurpose we augmented the system?s lexicon withspecific concept mappings.
That is, for each entry inthe lexicon either zero, one or many correspondingconcepts where added.
A simple vector of concepts -corresponding to the words in the SRH for which en-tries in the lexicon exist - constitutes each resultingCR.
All other words with empty concept mappings,e.g.
articles and aspectual markers, are ignored inthe conversion.
Due to lexical ambiguity, i.e.
theone to many word - concept mappings, this process-ing step yields a set I = {CR1, CR2, .
.
.
, CRn} ofpossible interpretations for each SRH.ONTOSCORE converts the domain model, i.e.
anontology, into a directed graph with concepts asnodes and relations as edges.
In order to find theshortest path between two concepts, ONTOSCOREemploys the single source shortest path algorithmof Dijkstra (Cormen et al, 1990).
Thus, the minimalpaths connecting a given concept ci with every other5DAML+OIL and OWL are frequently used knowl-edge modeling languages originating in W3C and Seman-tic Web projects.
For more details, see www.w3c.org andwww.daml.org.6The top-level was developed following the procedure out-lined in Russell and Norvig (1995).concept in CR (excluding ci itself) are selected, re-sulting in an n?
n matrix of the respective paths.To score the minimal paths connectingall concepts with each other in a givenCR, we adopted a method proposed byDemetriou and Atwell (1994) to score the se-mantic coherence of alternative sentence inter-pretations against graphs based on the LongmanDictionary of Contemporary English (LDOCE).As defined by Demetriou and Atwell (1994),R = {r1, r2, .
.
.
, rn} is the set of direct relations(both isa and semantic relations) that can connecttwo nodes (concepts); and W = {w1, w2, .
.
.
, wn}is the set of corresponding weights, where theweight of each isa relation is set to 0 and that ofeach other relation to 1.The algorithm selects from the set of all pathsbetween two concepts the one with the smallestweight, i.e.
the cheapest.
The distances between allconcept pairs in CR are summed up to a total score.The set of concepts with the lowest aggregate scorerepresents the combination with the highest seman-tic relatedness.
The ensuing distance between twoconcepts, e.g.
D(ci, cj) is, then, defined as the min-imum score derived between ci and cj .Demetriou and Atwell (1994) do not provide con-crete evaluation results for the method.
Also, theiralgorithm only allows for a relative judgment stat-ing which of a set of interpretations given a singlesentence is more semantically related.Since our objective is to compute coherencescores of arbitrary CRs on an absolute scale, certainextensions were necessary.
In this application theCRs to be scored can differ in terms of their content,the number of concepts contained therein and theirmappings to the original SRH.
Moreover, in order toachieve absolute values, the final score should be re-lated to the number of concepts in an individual setand the number of words in the original SRH.
There-fore, the results must be normalized in order to allowfor evaluation, comparability and clearer interpreta-tion of the semantic coherence scores.We modified the algorithm described above tomake it applicable and evaluatable with respect tothe task at hand as well as other possible tasks.
Thebasic idea is to calculate a score based on the pathdistances in CR.
Since short distances indicate co-herence and many concept pairs in a given CR mayhave no connecting path, we define the distance be-tween two concepts ci and cj that are not connectedin the knowledge base as Dmax.
This maximumvalue can also serve as a maximum for long dis-tances and can thus help to prune the search tree forlong paths.
This constant has to be set according tothe structure of the knowledge base.
For example,employing the ontology described above, the max-imum distance between two concepts does not ex-ceed ten and we chose in that case Dmax = 10.We can now define the semantic coherence scorefor CR as the average path length between all con-cept pairs in CR:S(CR) =?ci,cj?CR,ci 6=cj D(ci, cj)|CR|2 ?
|CR|Since the ontology is a directed graph, we have|CR|2 ?
|CR| pairs of concepts with possible di-rected connections, i.e., a path from concept ci toconcept cj may be completely different to that fromcj to ci or even be missing.
As a symmetric alter-native, we may want to consider a path from ci to cjand a path from cj to ci to be semantically equivalentand thus model every relation in a bidirectional way.We can then compute a symmetric score S?
(CR) asS?
(CR) = 2?ci,cj?CR,i<j min(D(ci, cj)D(cj , ci))|CR|2 ?
|CR|ONTOSCORE implements both options.
As theontology currently employed features mostly unidi-rectional relations we chose the S?
(CR) function forthe evaluation, i.e.
only the best path D(ci, cj) be-tween a given pair of concepts, regardless of the di-rection, is taken into account.
A detailed descriptionof the original system can be found in (Gurevych etal., 2003a).5 Contextual Coherence ScoringThe contextually enhanced ONTOSCORE systemperforms a number of additional processing steps,each of them will be described below.5.1 Scoring Conceptual ContextRepresentationsA necessary preprocessing step for the conceptualcontext scoring of SRHs is to build a conceptual con-text representation CR?
(SRHn+1) resulting from apair of concept representations:- a concept representation of the SRH to bescored, i.e.
CR(SRHn+1),- and a concept representation of the precedingutterance?s SRH, i.e.
CR(SRHn).For that purpose, the ONTOSCORE stores the bestconcept representation from each dialogue turn asCRbest(SRH).
By the best CR we mean the in-terpretation which received the highest score fromthe ONTOSCORE system, from the list of alter-native interpretations of the utterance.
For ex-ample CRbest for the utterance shown in Exam-ple (1) is the CR of the SRH given in (1e), i.e.
{EmotionExperiencerSubjectProcess, Person, Two-PointRelation, Route, Town, Town}.To produce a conceptual context representationfor SRHn+1, we build a union of each of its possibleinterpretations I = {CR1, CR2, .
.
.
, CRn} withthe stored CRbest(SRHn) from the previous utter-ance.
This results in a contextually augmented newset I ?
= {CR?1, CR?2, .
.
.
, CR?n} representing pos-sible conceptual context interpretations of SRHn+1as shown in Table 1.I(SRHn+1) I ?
(SRHn+1)CR1 ?
CRbest(SRHn) = CR?1CR2 ?
CRbest(SRHn) = CR?2... ... ...CRn ?
CRbest(SRHn) = CR?nTable 1: Creating conceptual context representationsIf, however, the calculated score of CRbest(SRHn)is below a certain threshold, meaning that even thebest prior hypothesis is most likely not semanti-cally coherent, then CRbest(SRHn) = {?}.
SeeSection 6.2 for the corresponding numbers with re-spect to the coherent versus incoherent classifica-tion.
Thusly, only if CRbest(SRHn) is empty thensolely the concept representations of SRHn+1 aretaken into account.
This is, of course, also the caseat the first dialogue turn.In order to score the alternative conceptual con-text representations defined by I ?
(SRHn+1), theformula for S?
(CR) is employed.
This means thatwe calculate a conceptual context coherence scoreS?
for each conceptual context representation CR?.We also perform an inverse linear transformation ofthe scores resulting in numbers from 0 to 1, so thathigher scores indicate better contextual coherence.5.2 ONTOSCORE at WorkLooking at an example of ONTOSCORE at work, wewill examine the following discourse fragment con-sisting of the two sequential utterances given in Ex-ample (1) and (2).
As shown in Table 2, in the caseof Example (1) all scores indicate the SRH given inExample (1a) to be the best one.SRH recognizer parser OntoScore1a 1 1 .61b .74 .94 .61c .63 .94 .541d .78 .89 .541e .74 .88 .54Table 2: The scores for the SRHs of Example (1).Example (2) yields the following SRHs with the cor-responding context-independent CRs and context-dependent CR?s:2a RennenRaceLiedsongComedycomedyShowshowHeidelbergHeidelbergweiter.continue.CR{MusicPiece, Genre, Genre, Town}CR?
{MusicPiece, Genre, Genre, Town,EmotionExperiencerSubjectProcess,Person,TwoPointRelation, Route }2b dennthenwiehowComedycomedyHeidelbergHeidelbergweiter.continue.CR{Genre, Town}CR?
{Genre, Town,EmotionExperiencerSubjectProcess,Person,TwoPointRelation, Route }2c dennthenwiehowComedycomedyShowshowweiter.continue.CR{Genre, Genre}CR?
{Genre, Genre,EmotionExperiencerSubjectProcess, Person,TwoPointRelation, Route }2d dennthenwiehowComedycomedyweiter.continue.CR{Genre}CR?
{Genre,EmotionExperiencerSubjectProcess, Person,TwoPointRelation, Route }2e dennthenwiehowkommecanichIininHeidelbergHeidelbergweiter.continue.CR{MotionDirectedTransliterated, Person,Town}CR?
{MotionDirectedTransliterated, Person,Town, EmotionExperiencerSubjectProcess,TwoPointRelation, Route }Adding the conceptual context we get the resultsshown in Table 3 for Example (2):SRH recognizer parser OntoScore2a 1 .25 .322b .52 .2 .482c .34 .2 .392d .35 .12 02e .52 .08 .71Table 3: The scores for the SRHs of Example 2.As evident from Table 3, CR?best corresponds to Ex-ample 2e.
This means that 2e constitutes a morecontextually coherent concept structure than the al-ternative SRHs.
This SRH was also labeled both asthe best and as a coherent SRH by the annotators.6 EvaluationThe ONTOSCORE software runs as a module in theSMARTKOM multi-modal and multi-domain spokendialogue system (Wahlster et al, 2001).
The sys-tem features the combination of speech and gestureas its input and output modalities.
The domains ofthe system include cinema and TV program infor-mation, home electronic device control as well asmobile services for tourists, e.g.
tour planning andsights information.ONTOSCORE operates on n-best lists of SRHsproduced by the language interpretation module outof the ASR word graphs.
It computes a numericalranking of alternative SRHs and thus provides animportant aid to the spoken language understand-ing component.
More precisely, the task of ON-TOSCORE in the system is to identify the best SRHsuitable for further processing and evaluate it interms of its contextual coherence against the domainand discourse knowledge.The ONTOSCORE module currently employs twoknowledge sources, an ontology (about 730 con-cepts and 200 relations) and a lexicon (ca.
3.600words) with word to concept mappings, covering therespective domains of the system.
The evaluationof ONTOSCORE was carried out on a set of 95 di-alogues.
The resulting dataset contained 552 utter-ances resulting in 1,375 SRHs, corresponding to anaverage of 2.49 SRHs per user utterance.
The corpushad been annotated by humans subjects according totwo separate annotation schemata.
The results of an-notation experiments are reported in Section 3.6.1 Identifying the Best SRHThe task of ONTOSCORE in our multimodal dia-logue system is to determine the best SRH fromthe n-best list of SRHs corresponding to a givenuser utterance.
The baseline for this evaluationwas computed by adding the individual ratios of ut-terance/SRHs - corresponding to the likelihood ofguessing the best one in each individual case - anddividing it by the number of utterances - yielding theoverall likelihood of guessing the best one 63.91%.The accuracy of ONTOSCORE on this taskamounts to 86.76%.
This means that in 86.76%of all cases the best SRH defined by the humangold standard is among the best scored by the ON-TOSCORE module.
The ONTOSCORE module with-out the conceptual context feature yields the accu-racy of only 84.06% on the same task.
This suggeststhat the overall results in identifying the best SRHin the speech recognizer output can by improved bytaking the knowledge of conceptual context into ac-count.6.2 Classifying the SRHs as SemanticallyCoherent versus IncoherentFor this evaluation we used the same corpus, whereeach SRH was labeled as being either semanticallycoherent versus incoherent with respect to the previ-ous discourse context.
We defined a baseline basedon the majority class, i.e.
coherent, in the corpus,63.05%.
In order to obtain a binary classificationinto semantically coherent and incoherent SRHs, acutoff threshold must be set.Employing a cutoff threshold of 0.44, we find thatthe contextually enhanced ONTOSCORE system cor-rectly classifies 70.98% of SRHs in the corpus.
Thisindicates the improvement of 7.93% over the base-line.
We also conducted the same classification ex-periment with ONTOSCORE without using the con-ceptual context feature.
In this case we obtained69.96% accuracy.From these results we can conclude that the taskof an absolute classification of coherent versus inco-herent is substantially more difficult than that of de-termining the best SRH, both for human annotators(see Section 3) and for ONTOSCORE.
Both humanand the system?s reliability is lower in the coherentversus incoherent classification task, which allowsto classify zero, one or multiple SRHs from one ut-terance as coherent or incoherent.
In both tasks,however, ONTOSCORE?s performance mirrors andapproaches human performance.7 Concluding RemarksThe contextually enhanced ONTOSCORE systemdescribed herein automatically performs ontology-based scoring of sets of concepts which constitutean adequate and suitable representation of a speechrecognition hypothesis and the prior conceptual con-text.
This conceptual context is an analogous con-ceptual representation of the previous user utterance.To date, the algorithm has been implemented in asoftware which is employed by a multi-domain spo-ken dialogue system and applied to the task of scor-ing n-best lists of SRH, thus producing a score ex-pressing how well a given SRH fits within the do-main model and the given discourse.
In the evalu-ation of our system we employed an ontology thatwas not designed for this task, but already existed asthe system?s internal knowledge representation.
Asshown above, the inclusion of the conceptual dis-course context yields an improvement of almost 3%as compared to the context-independent system.As future work we will examine how the com-putation of a contextual coherence score, i.e.
howwell a given SRH fits within the domain modelwith respect to the previous discourse, can be em-ployed to detect domain changes in complex multi-modal and multi-domain spoken dialogue systems.As one would expect, a contextual coherence scoreas described above actually decreases when the userchanged from one domain to another, which mostlikely also accounts for a set of the actual misclassi-fications.
As a future enhancement we will integrateand evaluate an automatic domain change detectionfunction, which, if activated, will cause the systemto employ the context-independent scoring function.Currently, we are also investigating whether the pro-posed method can be applied to scoring sets of po-tential candidates for resolving the semantic inter-pretation of ambiguous, polysemous and metonymiclanguage use (Porzel and Gurevych, 2003).
Addi-tionally, As ontology building is constly, we exam-ine the feasibility to employ alternative knowledgesources, that are generated automatically from cor-pora, e.g.
via self organizing maps.AcknowledgmentsThere work described herein was conducted withinthe SmartKom project partly funded by the Germanministry of Research and Technology under grant01IL95I7 and by the Klaus Tschira Foundation.ReferencesJames F. Allen, Georga Ferguson, and Amanda Stent.2001.
An architecture for more realistic conversa-tional system.
In Proceedings of Intelligent User In-terfaces, pages 1?8, Santa Fe, NM.James F. Allen.
1987.
Natural Language Understanding.Menlo Park, Cal.
: Benjamin Cummings.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet Project.
In Proceed-ings of COLING-ACL, Montreal, Canada.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: The kappa statistic.
Computational Lin-guistics, 22(2):249?254.Thomas H. Cormen, Charles E. Leiserson, and Ronald R.Rivest.
1990.
Introduction to Algorithms.
MIT press,Cambridge, MA.George Demetriou and Eric Atwell.
1994.
A seman-tic network for large vocabulary speech recognition.In Lindsay Evett and Tony Rose, editors, Proceedingsof AISB workshop on Computational Linguistics forSpeech and Handwriting Recognition, University ofLeeds.Ralf Engel.
2002.
SPIN: Language understanding forspoken dialogue systems using a production systemapproach.
In Proceedings of ICSLP 2002.Iryna Gurevych, Robert Porzel, and Michael Strube.2002.
Annotating the semantic consistency of speechrecognition hypotheses.
In Proceedings of the ThirdSIGdial Workshop on Discourse and Dialogue, pages46?49, Philadelphia, USA, July.Iryna Gurevych, Rainer Malaka, Robert Porzel, andHans-Peter Zorn.
2003a.
Semantic coherence scoringusing an ontology.
In Proceedings of the HLT-NAACLConference.
to appear.Iryna Gurevych, Robert Porzel, Elena Slinko, Nor-bert Pfleger, Jan Alexandersson, and Stefan Merten.2003b.
Less is more: Using a single knowledge rep-resentation in dialogue systems.
In Proceedings of theHLT-NAACL?03 Workshop on Text Meaning, Edmon-ton, Canada.Martin Oerder and Hermann Ney.
1993.
Wordgraphs: An efficient interface between continuous-speech recognition and language understanding.
InICASSP Volume 2, pages 119?122.Norbert Pfleger, Jan Alexandersson, and Tilman Becker.2002.
Scoring functions for overlay and their ap-plication in discourse processing.
In KONVENS-02,Saarbru?cken, September ?
October.Robert Porzel and Iryna Gurevych.
2003.
Contextualcoherence in natural language processing.
Modelingand Using Context, Springer, LNCS:to appear.Robert Porzel, Norbert Pfleger, Stefan Merten, MarkusLo?ckelt, Iryna Gurevych, Ralf Engel, and Jan Alexan-dersson.
2003.
More on less: Further applications ofontologies in multi-modal dialogue systems.
In Pro-ceedings of the IJCAI?03 Workshop on Knowledge andReasoning in Practical Dialogue Systems, page to ap-pear.Stuart J. Russell and Peter Norvig.
1995.
Artificial In-telligence.
A Modern Approach.
Prentice Hall, Engle-wood Cliffs, N.J.Richard Schwartz and Ye-Lo Chow.
1990.
The n-bestalgorithm: an efficient and exact procedure for findingthe n most likely sentence hypotheses.
In Proceedingsof ICASSP?90, Albuquerque, USA.Bach-Hiep Tran, Frank Seide, Volker Steinbiss, RichardSchwartz, and Ye-Lo Chow.
1996.
A word graphbased n-best search in continuous speech recognition.In Proceedings of ICSLP?96.Wolfgang Wahlster, Norbert Reithinger, and AnselmBlocher.
2001.
SmartKom: Multimodal communi-cation with a life-like character.
In Proceedings of the7th European Conference on Speech Communicationand Technology., pages 1547?1550.Marilyn A. Walker, Candace A. Kamm, and Diane J. Lit-man.
2000.
Towards developing general model ofusability with PARADISE.
Natural Language Enge-neering, 6.
