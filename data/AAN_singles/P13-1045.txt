Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 455?465,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsParsing with Compositional Vector GrammarsRichard Socher John Bauer Christopher D. Manning Andrew Y. NgComputer Science Department, Stanford University, Stanford, CA 94305, USArichard@socher.org, horatio@gmail.com, manning@stanford.edu, ang@cs.stanford.eduAbstractNatural language parsing has typicallybeen done with small sets of discrete cat-egories such as NP and VP, but this rep-resentation does not capture the full syn-tactic nor semantic richness of linguisticphrases, and attempts to improve on thisby lexicalizing phrases or splitting cate-gories only partly address the problem atthe cost of huge feature spaces and sparse-ness.
Instead, we introduce a Compo-sitional Vector Grammar (CVG), whichcombines PCFGs with a syntactically un-tied recursive neural network that learnssyntactico-semantic, compositional vectorrepresentations.
The CVG improves thePCFG of the Stanford Parser by 3.8% toobtain an F1 score of 90.4%.
It is fastto train and implemented approximately asan efficient reranker it is about 20% fasterthan the current Stanford factored parser.The CVG learns a soft notion of headwords and improves performance on thetypes of ambiguities that require semanticinformation such as PP attachments.1 IntroductionSyntactic parsing is a central task in natural lan-guage processing because of its importance in me-diating between linguistic expression and mean-ing.
For example, much work has shown the use-fulness of syntactic representations for subsequenttasks such as relation extraction, semantic role la-beling (Gildea and Palmer, 2002) and paraphrasedetection (Callison-Burch, 2008).Syntactic descriptions standardly use coarsediscrete categories such as NP for noun phrasesor PP for prepositional phrases.
However, recentwork has shown that parsing results can be greatlyimproved by defining more fine-grained syntactic(riding,V,       )    (a,Det,       )        (bike,NN,       )(a bike,NP,       )(riding a bike,VP,       )Discrete Syntactic ?
Continuous SemanticRepresentations in the Compositional Vector GrammarFigure 1: Example of a CVG tree with (cate-gory,vector) representations at each node.
Thevectors for nonterminals are computed via a newtype of recursive neural network which is condi-tioned on syntactic categories from a PCFG.categories, which better capture phrases with simi-lar behavior, whether through manual feature engi-neering (Klein and Manning, 2003a) or automaticlearning (Petrov et al, 2006).
However, subdi-viding a category like NP into 30 or 60 subcate-gories can only provide a very limited represen-tation of phrase meaning and semantic similarity.Two strands of work therefore attempt to go fur-ther.
First, recent work in discriminative parsinghas shown gains from careful engineering of fea-tures (Taskar et al, 2004; Finkel et al, 2008).
Fea-tures in such parsers can be seen as defining effec-tive dimensions of similarity between categories.Second, lexicalized parsers (Collins, 2003; Char-niak, 2000) associate each category with a lexicalitem.
This gives a fine-grained notion of semanticsimilarity, which is useful for tackling problemslike ambiguous attachment decisions.
However,this approach necessitates complex shrinkage esti-mation schemes to deal with the sparsity of obser-vations of the lexicalized categories.In many natural language systems, single wordsand n-grams are usefully described by their distri-butional similarities (Brown et al, 1992), amongmany others.
But, even with large corpora, many455n-grams will never be seen during training, espe-cially when n is large.
In these cases, one cannotsimply use distributional similarities to representunseen phrases.
In this work, we present a new so-lution to learn features and phrase representationseven for very long, unseen n-grams.We introduce a Compositional Vector GrammarParser (CVG) for structure prediction.
Like theabove work on parsing, the model addresses theproblem of representing phrases and categories.Unlike them, it jointly learns how to parse and howto represent phrases as both discrete categories andcontinuous vectors as illustrated in Fig.
1.
CVGscombine the advantages of standard probabilisticcontext free grammars (PCFG) with those of re-cursive neural networks (RNNs).
The former cancapture the discrete categorization of phrases intoNP or PP while the latter can capture fine-grainedsyntactic and compositional-semantic informationon phrases and words.
This information can helpin cases where syntactic ambiguity can only be re-solved with semantic information, such as in thePP attachment of the two sentences: They ate udonwith forks.
vs.
They ate udon with chicken.Previous RNN-based parsers used the same(tied) weights at all nodes to compute the vectorrepresenting a constituent (Socher et al, 2011b).This requires the composition function to be ex-tremely powerful, since it has to combine phraseswith different syntactic head words, and it is hardto optimize since the parameters form a very deepneural network.
We generalize the fully tied RNNto one with syntactically untied weights.
Theweights at each node are conditionally dependenton the categories of the child constituents.
Thisallows different composition functions when com-bining different types of phrases and is shown toresult in a large improvement in parsing accuracy.Our compositional distributed representation al-lows a CVG parser to make accurate parsing de-cisions and capture similarities between phrasesand sentences.
Any PCFG-based parser can be im-proved with an RNN.
We use a simplified versionof the Stanford Parser (Klein and Manning, 2003a)as the base PCFG and improve its accuracy from86.56 to 90.44% labeled F1 on all sentences of theWSJ section 23.
The code of our parser is avail-able at nlp.stanford.edu.2 Related WorkThe CVG is inspired by two lines of research:Enriching PCFG parsers through more diversesets of discrete states and recursive deep learningmodels that jointly learn classifiers and continuousfeature representations for variable-sized inputs.Improving Discrete Syntactic RepresentationsAs mentioned in the introduction, there are severalapproaches to improving discrete representationsfor parsing.
Klein and Manning (2003a) usemanual feature engineering, while Petrov etal.
(2006) use a learning algorithm that splitsand merges the syntactic categories in orderto maximize likelihood on the treebank.
Theirapproach splits categories into several dozensubcategories.
Another approach is lexicalizedparsers (Collins, 2003; Charniak, 2000) thatdescribe each category with a lexical item, usuallythe head word.
More recently, Hall and Klein(2012) combine several such annotation schemesin a factored parser.
We extend the above ideasfrom discrete representations to richer continuousones.
The CVG can be seen as factoring discreteand continuous parsing in one model.
Anotherdifferent approach to the above generative modelsis to learn discriminative parsers using many welldesigned features (Taskar et al, 2004; Finkel etal., 2008).
We also borrow ideas from this line ofresearch in that our parser combines the generativePCFG model with discriminatively learned RNNs.Deep Learning and Recursive Deep LearningEarly attempts at using neural networks to de-scribe phrases include Elman (1991), who used re-current neural networks to create representationsof sentences from a simple toy grammar and toanalyze the linguistic expressiveness of the re-sulting representations.
Words were representedas one-on vectors, which was feasible since thegrammar only included a handful of words.
Col-lobert and Weston (2008) showed that neural net-works can perform well on sequence labeling lan-guage processing tasks while also learning appro-priate features.
However, their model is lackingin that it cannot represent the recursive structureinherent in natural language.
They partially cir-cumvent this problem by using either independentwindow-based classifiers or a convolutional layer.RNN-specific training was introduced by Gollerand Ku?chler (1996) to learn distributed represen-tations of given, structured objects such as logi-cal terms.
In contrast, our model both predicts thestructure and its representation.456Henderson (2003) was the first to show that neu-ral networks can be successfully used for largescale parsing.
He introduced a left-corner parser toestimate the probabilities of parsing decisions con-ditioned on the parsing history.
The input to Hen-derson?s model consists of pairs of frequent wordsand their part-of-speech (POS) tags.
Both the orig-inal parsing system and its probabilistic interpre-tation (Titov and Henderson, 2007) learn featuresthat represent the parsing history and do not pro-vide a principled linguistic representation like ourphrase representations.
Other related work in-cludes (Henderson, 2004), who discriminativelytrains a parser based on synchrony networks and(Titov and Henderson, 2006), who use an SVM toadapt a generative parser to different domains.Costa et al (2003) apply recursive neural net-works to re-rank possible phrase attachments inan incremental parser.
Their work is the first toshow that RNNs can capture enough informationto make correct parsing decisions, but they onlytest on a subset of 2000 sentences.
Menchetti etal.
(2005) use RNNs to re-rank different parses.For their results on full sentence parsing, they re-rank candidate trees created by the Collins parser(Collins, 2003).
Similar to their work, we use theidea of letting discrete categories reduce the searchspace during inference.
We compare to fully tiedRNNs in which the same weights are used at everynode.
Our syntactically untied RNNs outperformthem by a significant margin.
The idea of untyinghas also been successfully used in deep learningapplied to vision (Le et al, 2010).This paper uses several ideas of (Socher et al,2011b).
The main differences are (i) the dualrepresentation of nodes as discrete categories andvectors, (ii) the combination with a PCFG, and(iii) the syntactic untying of weights based onchild categories.
We directly compare models withfully tied and untied weights.
Another work thatrepresents phrases with a dual discrete-continuousrepresentation is (Kartsaklis et al, 2012).3 Compositional Vector GrammarsThis section introduces Compositional VectorGrammars (CVGs), a model to jointly find syntac-tic structure and capture compositional semanticinformation.CVGs build on two observations.
Firstly, that alot of the structure and regularity in languages canbe captured by well-designed syntactic patterns.Hence, the CVG builds on top of a standard PCFGparser.
However, many parsing decisions showfine-grained semantic factors at work.
Thereforewe combine syntactic and semantic informationby giving the parser access to rich syntactico-semantic information in the form of distributionalword vectors and compute compositional semanticvector representations for longer phrases (Costaet al, 2003; Menchetti et al, 2005; Socher etal., 2011b).
The CVG model merges ideas fromboth generative models that assume discrete syn-tactic categories and discriminative models thatare trained using continuous vectors.We will first briefly introduce single word vec-tor representations and then describe the CVG ob-jective function, tree scoring and inference.3.1 Word Vector RepresentationsIn most systems that use a vector representa-tion for words, such vectors are based on co-occurrence statistics of each word and its context(Turney and Pantel, 2010).
Another line of re-search to learn distributional word vectors is basedon neural language models (Bengio et al, 2003)which jointly learn an embedding of words into ann-dimensional feature space and use these embed-dings to predict how suitable a word is in its con-text.
These vector representations capture inter-esting linear relationships (up to some accuracy),such as king?man+woman ?
queen (Mikolovet al, 2013).Collobert and Weston (2008) introduced a newmodel to compute such an embedding.
The ideais to construct a neural network that outputs highscores for windows that occur in a large unla-beled corpus and low scores for windows whereone word is replaced by a random word.
Whensuch a network is optimized via gradient ascent thederivatives backpropagate into the word embed-ding matrix X .
In order to predict correct scoresthe vectors in the matrix capture co-occurrencestatistics.For further details and evaluations of these em-beddings, see (Turian et al, 2010; Huang et al,2012).
The resulting X matrix is used as follows.Assume we are given a sentence as an ordered listof m words.
Each word w has an index [w] = iinto the columns of the embedding matrix.
Thisindex is used to retrieve the word?s vector repre-sentation aw using a simple multiplication with abinary vector e, which is zero everywhere, except457at the ith index.
So aw = Lei ?
Rn.
Henceforth,after mapping each word to its vector, we representa sentence S as an ordered list of (word,vector)pairs: x = ((w1, aw1), .
.
.
, (wm, awm)).Now that we have discrete and continuous rep-resentations for all words, we can continue withthe approach for computing tree structures andvectors for nonterminal nodes.3.2 Max-Margin Training Objective forCVGsThe goal of supervised parsing is to learn a func-tion g : X ?
Y , where X is the set of sentencesand Y is the set of all possible labeled binary parsetrees.
The set of all possible trees for a given sen-tence xi is defined as Y (xi) and the correct treefor a sentence is yi.We first define a structured margin loss ?
(yi, y?
)for predicting a tree y?
for a given correct tree.The loss increases the more incorrect the proposedparse tree is (Goodman, 1998).
The discrepancybetween trees is measured by counting the numberof nodes N(y) with an incorrect span (or label) inthe proposed tree:?
(yi, y?)
=?d?N(y?
)?1{d /?
N(yi)}.
(1)We set ?
= 0.1 in all experiments.
For a givenset of training instances (xi, yi), we search for thefunction g?, parameterized by ?, with the smallestexpected loss on a new sentence.
It has the follow-ing form:g?
(x) = arg maxy?
?Y (x)s(CVG(?, x, y?
)), (2)where the tree is found by the Compositional Vec-tor Grammar (CVG) introduced below and thenscored via the function s. The higher the score ofa tree the more confident the algorithm is that itsstructure is correct.
This max-margin, structure-prediction objective (Taskar et al, 2004; Ratliffet al, 2007; Socher et al, 2011b) trains the CVGso that the highest scoring tree will be the correcttree: g?
(xi) = yi and its score will be larger up toa margin to other possible trees y?
?
Y(xi):s(CVG(?, xi, yi)) ?
s(CVG(?, xi, y?))
+ ?
(yi, y?
).This leads to the regularized risk function for mtraining examples:J(?)
= 1mm?i=1ri(?)
+?2 ??
?22, whereri(?)
= maxy?
?Y (xi)(s(CVG(xi, y?))
+ ?
(yi, y?))?
s(CVG(xi, yi)) (3)Intuitively, to minimize this objective, the score ofthe correct tree yi is increased and the score of thehighest scoring incorrect tree y?
is decreased.3.3 Scoring Trees with CVGsFor ease of exposition, we first describe how toscore an existing fully labeled tree with a standardRNN and then with a CVG.
The subsequent sec-tion will then describe a bottom-up beam searchand its approximation for finding the optimal tree.Assume, for now, we are given a labeledparse tree as shown in Fig.
2.
We definethe word representations as (vector, POS) pairs:((a,A), (b, B), (c, C)), where the vectors are de-fined as in Sec.
3.1 and the POS tags come froma PCFG.
The standard RNN essentially ignores allPOS tags and syntactic categories and each non-terminal node is associated with the same neuralnetwork (i.e., the weights across nodes are fullytied).
We can represent the binary tree in Fig.
2in the form of branching triplets (p ?
c1c2).Each such triplet denotes that a parent node p hastwo children and each ck can be either a wordvector or a non-terminal node in the tree.
Forthe example in Fig.
2, we would get the triples((p1 ?
bc), (p2 ?
ap1)).
Note that in orderto replicate the neural network and compute noderepresentations in a bottom up fashion, the parentmust have the same dimensionality as the children:p ?
Rn.Given this tree structure, we can now computeactivations for each node from the bottom up.
Webegin by computing the activation for p1 usingthe children?s word vectors.
We first concatenatethe children?s representations b, c ?
Rn?1 into avector[bc]?
R2n?1.
Then the compositionfunction multiplies this vector by the parameterweights of the RNN W ?
Rn?2n and applies anelement-wise nonlinearity function f = tanh tothe output vector.
The resulting output p(1) is thengiven as input to compute p(2).p(1) = f(W[bc]), p(2) = f(W[ap1])458(A , a=       )        ( B , b=       )       ( C, c=       )P(1 ), p(1 )=P(2 ), p(2 )=Standard Recursive Neural Network= f   W bc= f   W ap(1 )Figure 2: An example tree with a simple RecursiveNeural Network: The same weight matrix is repli-cated and used to compute all non-terminal noderepresentations.
Leaf nodes are n-dimensionalvector representations of words.In order to compute a score of how plausible ofa syntactic constituent a parent is the RNN uses asingle-unit linear layer for all i:s(p(i)) = vT p(i),where v ?
Rn is a vector of parameters that needto be trained.
This score will be used to find thehighest scoring tree.
For more details on how stan-dard RNNs can be used for parsing, see Socher etal.
(2011b).The standard RNN requires a single composi-tion function to capture all types of compositions:adjectives and nouns, verbs and nouns, adverbsand adjectives, etc.
Even though this function isa powerful one, we find a single neural networkweight matrix cannot fully capture the richness ofcompositionality.
Several extensions are possible:A two-layered RNN would provide more expres-sive power, however, it is much harder to train be-cause the resulting neural network becomes verydeep and suffers from vanishing gradient prob-lems.
Socher et al (2012) proposed to give ev-ery single word a matrix and a vector.
The ma-trix is then applied to the sibling node?s vectorduring the composition.
While this results in apowerful composition function that essentially de-pends on the words being combined, the numberof model parameters explodes and the composi-tion functions do not capture the syntactic com-monalities between similar POS tags or syntacticcategories.Based on the above considerations, we proposethe Compositional Vector Grammar (CVG) thatconditions the composition function at each nodeon discrete syntactic categories extracted from a(A , a=       )        ( B , b=       )       ( C, c=       )P(1 ), p(1 )=P(2 ), p(2 )=Syntactically Untied Recursive Neural Network= f   W (B ,C) bc= f   W (A ,P  ) ap(1 )(1 )Figure 3: Example of a syntactically untied RNNin which the function to compute a parent vectordepends on the syntactic categories of its childrenwhich we assume are given for now.PCFG.
Hence, CVGs combine discrete, syntacticrule probabilities and continuous vector composi-tions.
The idea is that the syntactic categories ofthe children determine what composition functionto use for computing the vector of their parents.While not perfect, a dedicated composition func-tion for each rule RHS can well capture commoncomposition processes such as adjective or adverbmodification versus noun or clausal complementa-tion.
For instance, it could learn that an NP shouldbe similar to its head noun and little influenced bya determiner, whereas in an adjective modificationboth words considerably determine the meaning ofa phrase.
The original RNN is parameterized by asingle weight matrixW .
In contrast, the CVG usesa syntactically untied RNN (SU-RNN) which hasa set of such weights.
The size of this set dependson the number of sibling category combinations inthe PCFG.Fig.
3 shows an example SU-RNN that com-putes parent vectors with syntactically untiedweights.
The CVG computes the first parent vec-tor via the SU-RNN:p(1) = f(W (B,C)[bc]),where W (B,C) ?
Rn?2n is now a matrix that de-pends on the categories of the two children.
Inthis bottom up procedure, the score for each nodeconsists of summing two elements: First, a singlelinear unit that scores the parent vector and sec-ond, the log probability of the PCFG for the rulethat combines these two children:s(p(1))=(v(B,C))T p(1) + logP (P1 ?
B C),(4)459where P (P1 ?
B C) comes from the PCFG.This can be interpreted as the log probability of adiscrete-continuous rule application with the fol-lowing factorization:P ((P1, p1)?
(B, b)(C, c)) (5)= P (p1 ?
b c|P1 ?
B C)P (P1 ?
B C),Note, however, that due to the continuous natureof the word vectors, the probability of such a CVGrule application is not comparable to probabilitiesprovided by a PCFG since the latter sum to 1 forall children.Assuming that node p1 has syntactic categoryP1, we compute the second parent vector via:p(2) = f(W (A,P1)[ap(1)]).The score of the last parent in this trigram is com-puted via:s(p(2))=(v(A,P1))T p(2) + logP (P2 ?
A P1).3.4 Parsing with CVGsThe above scores (Eq.
4) are used in the search forthe correct tree for a sentence.
The goodness of atree is measured in terms of its score and the CVGscore of a complete tree is the sum of the scores ateach node:s(CVG(?, x, y?))
=?d?N(y?)s(pd).
(6)The main objective function in Eq.
3 includes amaximization over all possible trees maxy?
?Y (x).Finding the global maximum, however, cannot bedone efficiently for longer sentences nor can weuse dynamic programming.
This is due to the factthat the vectors break the independence assump-tions of the base PCFG.
A (category, vector) noderepresentation is dependent on all the words in itsspan and hence to find the true global optimum,we would have to compute the scores for all bi-nary trees.
For a sentence of length n, there areCatalan(n) many possible binary trees which isvery large even for moderately long sentences.One could use a bottom-up beam search, keep-ing a k-best list at every cell of the chart, possiblyfor each syntactic category.
This beam search in-ference procedure is still considerably slower thanusing only the simplified base PCFG, especiallysince it has a small state space (see next section fordetails).
Since each probability look-up is cheapbut computing SU-RNN scores requires a matrixproduct, we would like to reduce the number ofSU-RNN score computations to only those treesthat require semantic information.
We note thatlabeled F1 of the Stanford PCFG parser on the testset is 86.17%.
However, if one used an oracle toselect the best tree from the top 200 trees that itproduces, one could get an F1 of 95.46%.We use this knowledge to speed up inference viatwo bottom-up passes through the parsing chart.During the first one, we use only the base PCFG torun CKY dynamic programming through the tree.The k = 200-best parses at the top cell of thechart are calculated using the efficient algorithmof (Huang and Chiang, 2005).
Then, the secondpass is a beam search with the full CVG model (in-cluding the more expensive matrix multiplicationsof the SU-RNN).
This beam search only consid-ers phrases that appear in the top 200 parses.
Thisis similar to a re-ranking setup but with one maindifference: the SU-RNN rule score computation ateach node still only has access to its child vectors,not the whole tree or other global features.
Thisallows the second pass to be very fast.
We use thissetup in our experiments below.3.5 Training SU-RNNsThe full CVG model is trained in two stages.
Firstthe base PCFG is trained and its top trees arecached and then used for training the SU-RNNconditioned on the PCFG.
The SU-RNN is trainedusing the objective in Eq.
3 and the scores as ex-emplified by Eq.
6.
For each sentence, we use themethod described above to efficiently find an ap-proximation for the optimal tree.To minimize the objective we want to increasethe scores of the correct tree?s constituents anddecrease the score of those in the highest scor-ing incorrect tree.
Derivatives are computed viabackpropagation through structure (BTS) (Gollerand Ku?chler, 1996).
The derivative of tree i hasto be taken with respect to all parameter matricesW (AB) that appear in it.
The main difference be-tween backpropagation in standard RNNs and SU-RNNs is that the derivatives at each node only addto the overall derivative of the specific matrix atthat node.
For more details on backpropagationthrough RNNs, see Socher et al (2010)4603.6 Subgradient Methods and AdaGradThe objective function is not differentiable due tothe hinge loss.
Therefore, we generalize gradientascent via the subgradient method (Ratliff et al,2007) which computes a gradient-like direction.Let ?
= (X,W (??
), v(??))
?
RM be a vector of allM model parameters, where we denote W (??)
asthe set of matrices that appear in the training set.The subgradient of Eq.
3 becomes:?J??
=?i?s(xi, y?max)??
?
?s(xi, yi)??
+ ?,where y?max is the tree with the highest score.
Tominimize the objective, we use the diagonal vari-ant of AdaGrad (Duchi et al, 2011) with mini-batches.
For our parameter updates, we first de-fine g?
?
RM?1 to be the subgradient at time step?
and Gt = ?t?=1 g?gT?
.
The parameter update attime step t then becomes:?t = ?t?1 ?
?
(diag(Gt))?1/2 gt, (7)where ?
is the learning rate.
Since we use the di-agonal of Gt, we only have to store M values andthe update becomes fast to compute: At time stept, the update for the i?th parameter ?t,i is:?t,i = ?t?1,i ???
?t?=1 g2?,igt,i.
(8)Hence, the learning rate is adapting differ-ently for each parameter and rare parameters getlarger updates than frequently occurring parame-ters.
This is helpful in our setting since some Wmatrices appear in only a few training trees.
Thisprocedure found much better optima (by ?3% la-beled F1 on the dev set), and converged morequickly than L-BFGS which we used previouslyin RNN training (Socher et al, 2011a).
Trainingtime is roughly 4 hours on a single machine.3.7 Initialization of Weight MatricesIn the absence of any knowledge on how to com-bine two categories, our prior for combining twovectors is to average them instead of performing acompletely random projection.
Hence, we initial-ize the binary W matrices with:W (??)
= 0.5[In?nIn?n0n?1] + ,where we include the bias in the last column andthe random variable is uniformly distributed:  ?U [?0.001, 0.001].
The first block is multiplied bythe left child and the second by the right child:W (AB)??ab1??
=[W (A)W (B)bias]??ab1?
?= W (A)a+W (B)b+ bias.4 ExperimentsWe evaluate the CVG in two ways: First, by a stan-dard parsing evaluation on Penn Treebank WSJand then by analyzing the model errors in detail.4.1 Cross-validating HyperparametersWe used the first 20 files of WSJ section 22to cross-validate several model and optimizationchoices.
The base PCFG uses simplified cate-gories of the Stanford PCFG Parser (Klein andManning, 2003a).
We decreased the state split-ting of the PCFG grammar (which helps both bymaking it less sparse and by reducing the num-ber of parameters in the SU-RNN) by addingthe following options to training: ?-noRightRec -dominatesV 0 -baseNP 0?.
This reduces the num-ber of states from 15,276 to 12,061 states and 602POS tags.
These include split categories, such asparent annotation categories like VP?S.
Further-more, we ignore all category splits for the SU-RNN weights, resulting in 66 unary and 882 bi-nary child pairs.
Hence, the SU-RNN has 66+882transformation matrices and scoring vectors.
Notethat any PCFG, including latent annotation PCFGs(Matsuzaki et al, 2005) could be used.
However,since the vectors will capture lexical and semanticinformation, even simple base PCFGs can be sub-stantially improved.
Since the computational com-plexity of PCFGs depends on the number of states,a base PCFG with fewer states is much faster.Testing on the full WSJ section 22 dev set (1700sentences) takes roughly 470 seconds with thesimple base PCFG, 1320 seconds with our newCVG and 1600 seconds with the currently pub-lished Stanford factored parser.
Hence, increasedperformance comes also with a speed improve-ment of approximately 20%.We fix the same regularization of ?
= 10?4for all parameters.
The minibatch size was set to20.
We also cross-validated on AdaGrad?s learn-ing rate which was eventually set to ?
= 0.1 andword vector size.
The 25-dimensional vectors pro-vided by Turian et al (2010) provided the best461Parser dev (all) test?
40 test (all)Stanford PCFG 85.8 86.2 85.5Stanford Factored 87.4 87.2 86.6Factored PCFGs 89.7 90.1 89.4Collins 87.7SSN (Henderson) 89.4Berkeley Parser 90.1CVG (RNN) 85.7 85.1 85.0CVG (SU-RNN) 91.2 91.1 90.4Charniak-SelfTrain 91.0Charniak-RS 92.1Table 1: Comparison of parsers with richer staterepresentations on the WSJ.
The last line is theself-trained re-ranked Charniak parser.performance and were faster than 50-,100- or 200-dimensional ones.
We hypothesize that the largerword vector sizes, while capturing more seman-tic knowledge, result in too many SU-RNN matrixparameters to train and hence perform worse.4.2 Results on WSJThe dev set accuracy of the best model is 90.93%labeled F1 on all sentences.
This model re-sulted in 90.44% on the final test set (WSJ sec-tion 23).
Table 1 compares our results to thetwo Stanford parser variants (the unlexicalizedPCFG (Klein and Manning, 2003a) and the fac-tored parser (Klein and Manning, 2003b)) andother parsers that use richer state representations:the Berkeley parser (Petrov and Klein, 2007),Collins parser (Collins, 1997), SSN: a statisticalneural network parser (Henderson, 2004), Fac-tored PCFGs (Hall and Klein, 2012), Charniak-SelfTrain: the self-training approach of McCloskyet al (2006), which bootstraps and parses addi-tional large corpora multiple times, Charniak-RS:the state of the art self-trained and discrimina-tively re-ranked Charniak-Johnson parser combin-ing (Charniak, 2000; McClosky et al, 2006; Char-niak and Johnson, 2005).
See Kummerfeld et al(2012) for more comparisons.
We compare alsoto a standard RNN ?CVG (RNN)?
and to the pro-posed CVG with SU-RNNs.4.3 Model AnalysisAnalysis of Error Types.
Table 2 shows a de-tailed comparison of different errors.
We usethe code provided by Kummerfeld et al (2012)and compare to the previous version of the Stan-ford factored parser as well as to the Berkeleyand Charniak-reranked-self-trained parsers (de-fined above).
See Kummerfeld et al (2012) fordetails and comparisons to other parsers.
One ofError Type Stanford CVG Berkeley Char-RSPP Attach 1.02 0.79 0.82 0.60Clause Attach 0.64 0.43 0.50 0.38Diff Label 0.40 0.29 0.29 0.31Mod Attach 0.37 0.27 0.27 0.25NP Attach 0.44 0.31 0.27 0.25Co-ord 0.39 0.32 0.38 0.231-Word Span 0.48 0.31 0.28 0.20Unary 0.35 0.22 0.24 0.14NP Int 0.28 0.19 0.18 0.14Other 0.62 0.41 0.41 0.50Table 2: Detailed comparison of different parsers.the largest sources of improved performance overthe original Stanford factored parser is in the cor-rect placement of PP phrases.
When measuringonly the F1 of parse nodes that include at least onePP child, the CVG improves the Stanford parserby 6.2% to an F1 of 77.54%.
This is a 0.23 re-duction in the average number of bracket errorsper sentence.
The ?Other?
category includes VP,PRN and other attachments, appositives and inter-nal structures of modifiers and QPs.Analysis of Composition Matrices.
An analy-sis of the norms of the binary matrices revealsthat the model learns a soft vectorized notion ofhead words: Head words are given larger weightsand importance when computing the parent vec-tor: For the matrices combining siblings with cat-egories VP:PP, VP:NP and VP:PRT, the weights inthe part of the matrix which is multiplied with theVP child vector dominates.
Similarly NPs dom-inate DTs.
Fig.
5 shows example matrices.
Thetwo strong diagonals are due to the initializationdescribed in Sec.
3.7.Semantic Transfer for PP Attachments.
In thissmall model analysis, we use two pairs of sen-tences that the original Stanford parser and theCVG did not parse correctly after training onthe WSJ.
We then continue to train both parserson two similar sentences and then analyze if theparsers correctly transferred the knowledge.
Thetraining sentences are He eats spaghetti with afork.
and She eats spaghetti with pork.
The verysimilar test sentences are He eats spaghetti with aspoon.
and He eats spaghetti with meat.
Initially,both parsers incorrectly attach the PP to the verbin both test sentences.
After training, the CVGparses both correctly, while the factored Stanfordparser incorrectly attaches both PPs to spaghetti.The CVG?s ability to transfer the correct PP at-tachments is due to the semantic word vector sim-ilarity between the words in the sentences.
Fig.
4shows the outputs of the two parsers.462(a) Stanford factored parserSNPPRPHeVPVBZeatsNPNPNNSspaghettiPPINwithNPDTaNNspoonSNPPRPHeVPVBZeatsNPNPNNSspaghettiPPINwithNPPRPmeat(b) Compositional Vector GrammarSNPPRPHeVPVBZeatsNPNNSspaghettiPPINwithNPDTaNNspoonSNPPRPHeVPVBZeatsNPNPNNSspaghettiPPINwithNPNNmeatFigure 4: Test sentences of semantic transfer for PP attachments.
The CVG was able to transfer se-mantic word knowledge from two related training sentences.
In contrast, the Stanford parser could notdistinguish the PP attachments based on the word semantics.10 20 30 40 50510152025 ?0.200.20.40.60.8DT-NP10 20 30 40 50510152025?0.4?0.200.20.40.6VP-NP10 20 30 40 50510152025 ?0.200.20.40.6ADJP-NPFigure 5: Three binary composition matricesshowing that head words dominate the composi-tion.
The model learns to not give determinersmuch importance.
The two diagonals show clearlythe two blocks that are multiplied with the left andright children, respectively.5 ConclusionWe introduced Compositional Vector Grammars(CVGs), a parsing model that combines the speedof small-state PCFGs with the semantic richnessof neural word representations and compositionalphrase vectors.
The compositional vectors arelearned with a new syntactically untied recursiveneural network.
This model is linguistically moreplausible since it chooses different compositionfunctions for a parent node based on the syntac-tic categories of its children.
The CVG obtains90.44% labeled F1 on the full WSJ test set and is20% faster than the previous Stanford parser.AcknowledgmentsWe thank Percy Liang for chats about the paper.Richard is supported by a Microsoft Research PhDfellowship.
The authors gratefully acknowledgethe support of the Defense Advanced ResearchProjects Agency (DARPA) Deep Exploration andFiltering of Text (DEFT) Program under Air ForceResearch Laboratory (AFRL) prime contract no.FA8750-13-2-0040, and the DARPA Deep Learn-ing program under contract number FA8650-10-C-7020.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe view of DARPA, AFRL, or the US govern-ment.463ReferencesY.
Bengio, R. Ducharme, P. Vincent, and C. Janvin.2003.
A neural probabilistic language model.
Jour-nal of Machine Learning Research, 3:1137?1155.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. DellaPietra, and J. C. Lai.
1992.
Class-based n-grammodels of natural language.
Computational Lin-guistics, 18.C.
Callison-Burch.
2008.
Syntactic constraints onparaphrases extracted from parallel corpora.
In Pro-ceedings of EMNLP, pages 196?205.E.
Charniak and M. Johnson.
2005.
Coarse-to-finen-best parsing and maxent discriminative reranking.In ACL.E.
Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of ACL, pages 132?139.M.
Collins.
1997.
Three generative, lexicalised modelsfor statistical parsing.
In ACL.M.
Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguis-tics, 29(4):589?637.R.
Collobert and J. Weston.
2008.
A unified archi-tecture for natural language processing: deep neuralnetworks with multitask learning.
In Proceedings ofICML, pages 160?167.F.
Costa, P. Frasconi, V. Lombardo, and G. Soda.
2003.Towards incremental parsing of natural language us-ing recursive neural networks.
Applied Intelligence.J.
Duchi, E. Hazan, and Y.
Singer.
2011.
Adaptive sub-gradient methods for online learning and stochasticoptimization.
JMLR, 12, July.J.
L. Elman.
1991.
Distributed representations, sim-ple recurrent networks, and grammatical structure.Machine Learning, 7(2-3):195?225.J.
R. Finkel, A. Kleeman, and C. D. Manning.
2008.Efficient, feature-based, conditional random fieldparsing.
In Proceedings of ACL, pages 959?967.D.
Gildea and M. Palmer.
2002.
The necessity of pars-ing for predicate argument recognition.
In Proceed-ings of ACL, pages 239?246.C.
Goller and A. Ku?chler.
1996.
Learning task-dependent distributed representations by backprop-agation through structure.
In Proceedings of the In-ternational Conference on Neural Networks.J.
Goodman.
1998.
Parsing Inside-Out.
Ph.D. thesis,MIT.D.
Hall and D. Klein.
2012.
Training factored pcfgswith expectation propagation.
In EMNLP.J.
Henderson.
2003.
Neural network probability esti-mation for broad coverage parsing.
In Proceedingsof EACL.J.
Henderson.
2004.
Discriminative training of a neu-ral network statistical parser.
In ACL.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the 9th InternationalWorkshop on Parsing Technologies (IWPT 2005).E.
H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.2012.
Improving Word Representations via GlobalContext and Multiple Word Prototypes.
In ACL.D.
Kartsaklis, M. Sadrzadeh, and S. Pulman.
2012.
Aunified sentence space for categorical distributional-compositional semantics: Theory and experiments.Proceedings of 24th International Conference onComputational Linguistics (COLING): Posters.D.
Klein and C. D. Manning.
2003a.
Accurate un-lexicalized parsing.
In Proceedings of ACL, pages423?430.D.
Klein and C.D.
Manning.
2003b.
Fast exact in-ference with a factored model for natural languageparsing.
In NIPS.J.
K. Kummerfeld, D. Hall, J. R. Curran, and D. Klein.2012.
Parser showdown at the wall street corral: Anempirical investigation of error types in parser out-put.
In EMNLP.Q.
V. Le, J. Ngiam, Z. Chen, D. Chia, P. W. Koh, andA.
Y. Ng.
2010.
Tiled convolutional neural net-works.
In NIPS.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Proba-bilistic cfg with latent annotations.
In ACL.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Ef-fective self-training for parsing.
In NAACL.S.
Menchetti, F. Costa, P. Frasconi, and M. Pon-til.
2005.
Wide coverage natural language pro-cessing using kernel methods and neural networksfor structured data.
Pattern Recognition Letters,26(12):1896?1906.T.
Mikolov, W. Yih, and G. Zweig.
2013.
Linguis-tic regularities in continuous spaceword representa-tions.
In HLT-NAACL.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In NAACL.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable treeannotation.
In Proceedings of ACL, pages 433?440.N.
Ratliff, J.
A. Bagnell, and M. Zinkevich.
2007.
(On-line) subgradient methods for structured prediction.In Eleventh International Conference on ArtificialIntelligence and Statistics (AIStats).R.
Socher, C. D. Manning, and A. Y. Ng.
2010.
Learn-ing continuous phrase representations and syntacticparsing with recursive neural networks.
In Proceed-ings of the NIPS-2010 Deep Learning and Unsuper-vised Feature Learning Workshop.464R.
Socher, E. H. Huang, J. Pennington, A. Y. Ng, andC.
D. Manning.
2011a.
Dynamic Pooling and Un-folding Recursive Autoencoders for Paraphrase De-tection.
In NIPS.
MIT Press.R.
Socher, C. Lin, A. Y. Ng, and C.D.
Manning.
2011b.Parsing Natural Scenes and Natural Language withRecursive Neural Networks.
In ICML.R.
Socher, B. Huval, C. D. Manning, and A. Y. Ng.2012.
Semantic Compositionality Through Recur-sive Matrix-Vector Spaces.
In EMNLP.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proceedings ofEMNLP, pages 1?8.I.
Titov and J. Henderson.
2006.
Porting statisticalparsers with data-defined kernels.
In CoNLL-X.I.
Titov and J. Henderson.
2007.
Constituent parsingwith incremental sigmoid belief networks.
In ACL.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In Proceedings of ACL, pages384?394.P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Jour-nal of Artificial Intelligence Research, 37:141?188.465
