Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 648?657,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsBuilding an Evaluation Scale using Item Response TheoryJohn P. Lalor1, Hao Wu2, Hong Yu1,31 University of Massachusetts, MA, USA2 Boston College, MA, USA3 Bedford VAMC and CHOIR, MA, USAlalor@cs.umass.edu, hao.wu.5@bc.edu, hong.yu@umassmed.eduAbstractEvaluation of NLP methods requires test-ing against a previously vetted gold-standardtest set and reporting standard metrics (ac-curacy/precision/recall/F1).
The current as-sumption is that all items in a given test set areequal with regards to difficulty and discrim-inating power.
We propose Item ResponseTheory (IRT) from psychometrics as an alter-native means for gold-standard test-set gener-ation and NLP system evaluation.
IRT is ableto describe characteristics of individual items -their difficulty and discriminating power - andcan account for these characteristics in its es-timation of human intelligence or ability foran NLP task.
In this paper, we demonstrateIRT by generating a gold-standard test set forRecognizing Textual Entailment.
By collect-ing a large number of human responses andfitting our IRT model, we show that our IRTmodel compares NLP systems with the per-formance in a human population and is able toprovide more insight into system performancethan standard evaluation metrics.
We showthat a high accuracy score does not always im-ply a high IRT score, which depends on theitem characteristics and the response pattern.11 IntroductionAdvances in artificial intelligence have made it pos-sible to compare computer performance directlywith human intelligence (Campbell et al, 2002; Fer-rucci et al, 2010; Silver et al, 2016).
In most cases,a common approach to evaluating the performance1Data and code will be made available for download athttps://people.cs.umass.edu/lalor/irt.htmlof a new system is to compare it against an unseengold-standard test dataset (GS items).
Accuracy, re-call, precision and F1 scores are commonly used toevaluate NLP applications.
These metrics assumethat GS items have equal weight for evaluating per-formance.
However, individual items are different:some may be so hard that most/all NLP systems an-swer incorrectly; others may be so easy that everyNLP system answers correctly.
Neither item typeprovides meaningful information about the perfor-mance of an NLP system.
Items that are answeredincorrectly by some systems and correctly by oth-ers are useful for differentiating systems accordingto their individual characteristics.In this paper we introduce Item Response The-ory (IRT) from psychometrics and demonstrate itsapplication to evaluating NLP systems.
IRT is a the-ory of evaluation for characterizing test items andestimating human ability from their performance onsuch tests.
IRT assumes that individual test ques-tions (referred to as ?items?
in IRT) have uniquecharacteristics such as difficulty and discriminatingpower.
These characteristics can be identified by fit-ting a joint model of human ability and item charac-teristics to human response patterns to the test items.Items that do not fit the model are removed and theremaining items can be considered a scale to eval-uate performance.
IRT assumes that the probabil-ity of a correct answer is associated with both itemcharacteristics and individual ability, and thereforea collection of items of varying characteristics candetermine an individual?s overall ability.Our aim is to build an intelligent evaluation metricto measure performance for NLP tasks.
With IRT we648can identify an appropriate set of items to measureability in relation to the overall human population asscored by an IRT model.
This process serves twopurposes: (i) to identify individual items appropri-ate for a test set that measures ability on a particulartask, and (ii) to use the resulting set of items as anevaluation set in its own right, to measure the abilityof future subjects (or NLP models) for the same task.These evaluation sets can measure the ability of anNLP system with a small number of items, leaving alarger percentage of a dataset for training.Our contributions are as follows: First, we in-troduce IRT and describe its benefits and method-ology.
Second, we apply IRT to Recognizing Tex-tual Entailment (RTE) and show that evaluationsets consisting of a small number of sampled itemscan provide meaningful information about the RTEtask.
Our IRT analyses show that different items ex-hibit varying degrees of difficulty and discrimina-tion power and that high accuracy does not alwaystranslate to high scores in relation to human perfor-mance.
By incorporating IRT, we can learn moreabout dataset items and move past treating each testcase as equal.
Using IRT as an evaluation metricallows us to compare NLP systems directly to theperformance of humans.2 Background and Related Work2.1 Item Response TheoryIRT is one of the most widely used methodologiesin psychometrics for scale construction and eval-uation.
It is typically used to analyze human re-sponses (graded as right or wrong) to a set of ques-tions (called ?items?).
With IRT individual abilityand item characteristics are jointly modeled to pre-dict performance (Baker and Kim, 2004).
This sta-tistical model makes the following assumptions: (a)Individuals differ from each other on an unobservedlatent trait dimension (called ?ability?
or ?factor?
);(b) The probability of correctly answering an itemis a function of the person?s ability.
This functionis called the item characteristic curve (ICC) and in-volves item characteristics as parameters; (c) Re-sponses to different items are independent of eachother for a given ability level of the person (?lo-cal independence assumption?
); (d) Responses fromdifferent individuals are independent of each other.Figure 1: Example ICC for a 3PL model with the followingparameters: a = 1.0, b = 0.0, c = 0.25.More formally, if we let j be an individual, i be anitem, and ?j be the latent ability trait of individual j,then the probability that individual j answers item icorrectly can be modeled as:pij(?j) = ci +1?
ci1 + e?ai(?j?bi)(1)where ai, bi, and ci are item parameters: ai (theslope or discrimination parameter) is related to thesteepness of the curve, bi (the difficulty parameter)is the level of ability that produces a chance of cor-rect response equal to the average of the upper andlower asymptotes, and ci (the guessing parameter)is the lower asymptote of the ICC and the proba-bility of guessing correctly.
Equation 1 is referredto as the three-parameter logistic (3PL) IRT model.A two-parameter logistic (2PL) IRT model assumesthat the guessing parameter ci is 0.Figure 1 shows an ICC of a 3PL model.
TheICC for a good item will look like a sigmoid plot,and should exhibit a relatively steep increasing ICCbetween ability levels ?3 and 3, where most peo-ple are located, in order to have appropriate powerto differentiate different levels of ability.
We havedescribed a one factor IRT model where ability isuni-dimensional.
Multi-factor IRT models would in-volve two or more latent trait dimensions and willnot be elaborated here.To identify the number of factors in an IRT model,the polychoric correlation matrix of the items is cal-culated and its ordered eigenvalues are plotted.
The649number of factors is suggested by the number oflarge eigenvalues.
It can be further established byfitting (see below) and comparing IRT models withdifferent numbers of factors.
Such comparison mayuse model selection indices such as Akaike Infor-mation Criterion (AIC) and Conditional BayesianInformation Criterion (CBIC) and should also takeinto account the interpretablility of the loading pat-tern that links items to factors.An IRT model can be fit to data with the marginalmaximum likelihood method through an EM algo-rithm (Bock and Aitkin, 1981).
The marginal likeli-hood function is the probability to observe the cur-rent response patterns as a function of the item pa-rameters with the persons?
ability parameters inte-grated out as random effects.
This function is max-imized to produce estimates of the item parameters.For IRT models with more than one factor, the slopeparameters (i.e.
loadings) that relate items and fac-tors must be properly rotated (Browne, 2001) be-fore they can be interpreted.
Given the estimateditem parameters, Bayesian estimates of the individ-ual person?s ability parameters are obtained with thestandard normal prior distribution.After determining the number of factors and fit-ting the model, the local independence assumptioncan be checked using the residuals of marginal re-sponses of item pairs (Chen and Thissen, 1997) andthe fit of the ICC for each item can be checked withitem fit statistics (Orlando and Thissen, 2000) to de-termine whether an item should be retained or re-moved.
If both tests are passed and all items haveproper discrimination power, then the set of items isconsidered a calibrated measurement scale and theestimated item parameters can be further used to es-timate an individual person?s ability level.IRT accounts for differences among items whenestimating a person?s ability.
In addition, ability es-timates from IRT are on the ability scale of the pop-ulation used to estimate item parameters.
For exam-ple, an estimated ability of 1.2 can be interpreted as1.2 standard deviations above the average ability inthis population.
The traditional total number of cor-rect responses generally does not have such quanti-tative meaning.IRT has been widely used in educational test-ing.
For example, it plays an instrumental role inthe construction, evaluation, or scoring of standard-ized tests such as the Test of English as a ForeignLanguage (TOEFL), Graduate Record Examinations(GRE) and the SAT college admissions standardizedtest.2.1.1 IRT TerminologyHere we outline common IRT terminology interms of RTE.
An item refers to a pair of sentences towhich humans or NLP systems assign a label (entail-ment, contradiction, or neutral).
A set of responsesto all items (each graded as correct or incorrect) is aresponse pattern.
An evaluation scale is a test set ofitems to be administered to an NLP system and as-signs an ability score (or theta score) to the systemas its performance.2.2 Recognizing Textual EntailmentRTE was introduced to standardize the challengeof accounting for semantic variation when buildingmodels for a number of NLP applications (Daganet al, 2006).
RTE defines a directional relationshipbetween a pair of sentences, the text (T) and the hy-pothesis (H).
T entails H if a human that has readT would infer that H is true.
If a human would in-fer that H is false, then H contradicts T. If the twosentences are unrelated, then the pair are said to beneutral.
Table 1 shows examples of T-H pairs andtheir respective classifications.
Recent state-of-the-art systems for RTE require a large amount of fea-ture engineering and specialization to achieve highperformance (Beltagy et al, 2015; Lai and Hocken-maier, 2014; Jimenez et al, 2014).A number of gold-standard datasets are availablefor RTE (Marelli et al, 2014; Young et al, 2014;Levy et al, 2014).
We consider the Stanford Natu-ral Language Inference (SNLI) dataset (Bowman etal., 2015).
SNLI examples were obtained using onlyhuman-generated sentences with Amazon Mechan-ical Turk (AMT) to mitigate the problem of poordata that was being used to build models for RTE.In addition, SNLI included a quality control assess-ment of a sampled portion of the dataset (about 10%,56,951 sentence pairs).
This data was provided to 4additional AMT users to provide labels (entailment,contradiction, neutral) for the sentence pairs.
If atleast 3 of the 5 annotators (the original annotatorand 4 additional annotators) agreed on a label theitem was retained.
Most of the items (98%) received650Text Hypothesis LabelRetained - 4GS1.
A toddler playing with a toy car next to a dog A toddler plays with toy carswhile his dog sleepsNeutral2.
People were watching the tournament in the stadium The people are sitting outside onthe grassContradictionRetained - 5GS3.
A person is shoveling snow It rained today Contradiction4 Two girls on a bridge dancing with the city skyline in thebackgroundThe girls are sisters.
Neutral5.
A woman is kneeling on the ground taking a photograph A picture is being snapped EntailmentRemoved - 4GS6.
Two men and one woman are dressed in costume hats The people are swingers Neutral7.
Man sweeping trash outside a large statue A man is on vacation Contradiction8.
A couple is back to back in formal attire Two people are facing awayfrom each otherEntailment9.
A man on stilts in a purple, yellow and white costume A man is performing on stilts EntailmentRemoved - 5GS10.
A group of soccer players are grabbing onto each otheras they go for the ballA group of football players areplaying a gameContradiction11.
Football players stand at the line of scrimmage The players are in uniform Neutral12.
Man in uniform waiting on a wall Near a wall, a man in uniform iswaitingEntailmentTable 1: Examples of retained & removed sentence pairs.
The selection is not based on right/wrong labels but based on IRT modelfitting and item elimination process.
Note that no 4GS entailment items were retained (Section 4.2)a gold-standard label.
Specifics of SNLI generationare at Bowman et al (2015).2.3 Related WorkTo identify low-quality annotators (spammers),Hovy et al (2013) modeled annotator responses, ei-ther answering correctly or guessing, as a randomvariable with a guessing parameter varying onlyacross annotators.
Passonneau and Carpenter (2014)used the model of Dawid and Skene (1979) in whichan annotator?s response depends on both the true la-bel and the annotator.
In both models an annotator?sresponse depends on an item only through its correctlabel.
In contrast, IRT assumes a more sophisticatedresponse mechanism involving both annotator qual-ities and item characteristics.
To our knowledge weare the first to introduce IRT to NLP and to create agold standard with the intention of comparing NLPapplications to human intelligence.Bruce and Wiebe (1999) analyze patterns ofagreement between annotators in a case-study sen-tence categorization task, and use a latent-traitmodel to identify true labels.
That work uses 4 an-notators at varying levels of expertise and does notconsider the discriminating power of dataset items.Current gold-standard dataset generation methodsinclude web crawling (Guo et al, 2013), automaticand semi-automatic generation (An et al, 2003), andexpert (Roller and Stevenson, 2015) and non-experthuman annotation (Bowman et al, 2015; Wiebe etal., 1999).
In each case validation is required toensure that the data collected is appropriate and us-able for the required task.
Automatically generateddata can be refined with visual inspection or post-collection processing.
Human annotated data usu-ally involves more than one annotator, so that com-parison metrics such as Cohen?s or Fleiss?
?
can beused to determine how much they agree.
Disagree-ments between annotators are resolved by researcherintervention or by majority vote.3 MethodsWe collected and evaluated a random selection fromthe SNLI RTE dataset (GSRTE) to build our IRTmodels.
We first randomly selected a subset ofGSRTE , and then used the sample in an AMT Hu-man Intelligence Task (HIT) to collect more labels651for each text-hypothesis pair.
We then applied IRTto evaluate the quality of the examples and used thefinal IRT models to create evaluation sets (GSIRT )to measure ability for RTE.3.1 Item SelectionFor our evaluation we looked at two sets of data:sentence-pairs selected from SNLI where 4 out of5 annotators agreed on the gold-standard label (re-ferred to as 4GS), and sentence-pairs where 5 out of5 annotators agreed on the gold-standard label (re-ferred to as 5GS).
We make the assumption for ouranalysis that the 4GS items are harder than the 5GSitems due to the fact that there was not a unanimousdecision regarding the gold-standard label.We selected the subset of GSRTE to use as an ex-amination set in 4GS and 5GS according to the fol-lowing steps: (1) Identify all ?quality-control?
itemsfrom GSRTE (i.e.
items where 5 annotators pro-vided labels, see ?2.2), (2) Identify items in this sec-tion of the data where 4 of the 5 annotators agreed onthe eventual gold label (to be selected from for 4GS)and 5 of the 5 annotators agreed on the gold standardlabel (to be selected from for 5GS), (3) Randomlyselect 30 entailment sentence pairs, 30 neutral pairs,and 30 contradiction pairs from those items where 4of 5 annotators agreed on the gold label (4GS) andthose items where 5 of 5 annotators agreed on thegold label (5GS) to obtain two sets of 90 sentencepairs.90 sentence pairs for 4GS and 5GS were sam-pled so that the annotation task (supplying 90 labels)could be completed in a reasonably short amount oftime during which users remained engaged.
We se-lected items from 4GS and 5GS because both groupsare considered high quality for RTE.
We evaluatedthe selected 180 sentence pairs using the modelprovided with the original dataset (Bowman et al,2015) and found that accuracy scores were similarcompared to performance on the SNLI test set.3.2 AMT AnnotationFor consistency we designed our AMT HIT to matchthe process used to validate the SNLI quality con-trol items (Bowman et al, 2015) and to generate la-bels for the SICK RTE dataset (Marelli et al, 2014).Each AMT user was shown 90 premise-hypothesispairs (either the full 5GS or 4GS set) one pair at atime, and was asked to choose the appropriate labelfor each.
Each user was presented with the full set,as opposed to one-label subsets (e.g.
just the entail-ment pairs) in order to avoid a user simply answeringwith the same label for each item.For each 90 sentence-pair set (5GS and 4GS), wecollected annotations from 1000 AMT users, result-ing in 1000 label annotations for each of the 180 sen-tence pairs.
While there is no set standard for sam-ple sizes in IRT models, this sample size satisfiesthe standards based on the non-central ?2 distribu-tion (MacCallum et al, 1996) used when comparingtwo multidimensional IRT models.
This sample sizeis also appropriate for tests of item fit and local de-pendence that are based on small contingency tables.Only AMT users with approval ratings above 97%were used to ensure that users were of a high qual-ity.
The task was only available to users located inthe United States, as a proxy for identifying Englishspeakers.
Attention check questions were includedin the HIT, to ensure that users were paying attentionand answering to the best of their ability.
Responseswhere the attention-check questions were answeredincorrectly were removed.
After removing individ-uals that failed the attention-check, we retained 976labels for each example in the 4GS set and 983 labelsfor each example in the 5GS set.
Average time spentfor each task was roughly 30 minutes, a reasonableamount for AMT users.3.3 Statistical AnalysisData collected for 4GS and 5GS were analyzed sep-arately in order to evaluate the differences between?easier?
items (5GS) and ?harder?
items (4GS), andto demonstrate the ability to show that theta score isconsistent even if dataset difficulty varies.
For bothsets of items, the number of factors was identifiedby a plot of eigenvalues of the 90 x 90 tetrachoriccorrelation matrix and by a further comparison be-tween IRT models with different number of factors.A target rotation (Browne, 2001) was used to iden-tify a meaningful loading pattern that associates fac-tors and items.
Each factor could then be interpretedas the ability of a user to recognize the correct rela-tionship between the sentence pairs associated withthat factor (e.g.
contradiction).Once the different factors were associated withdifferent sets of items, we built a unidimensional6524GS 5GS OverallPairs with majorityagreement95.6% 96.7% 96.1%Pairs with superma-jority agreement61.1% 82.2% 71.7%Table 2: Summary statistics from the AMT HITs.IRT model for each set of items associated with asingle factor.
We fit and compared one- and two-factor 3PL models to confirm our assumption andthe unidimensional structure underlying these items,assuming the possible presence of guessing in peo-ple?s responses.
We further tested the guessing pa-rameter of each item in the one factor 3PL model.
Ifthe guessing parameter was not significantly differ-ent from 0, a 2PL ICC was used for that particularitem.Once an appropriate model structure was deter-mined, individual items were evaluated for goodnessof fit within the model (?2.1).
If an item was deemedto fit the ICC poorly or to give rise to local depen-dence, it was removed for violating model assump-tions.
Furthermore, if the ICC of an item was tooflat, it was removed for low discriminating powerbetween ability levels.
The model was then refit withthe remaining items.
This iterative process contin-ued until no item could be removed (2 to 6 iterationsdepending on how many items were removed fromeach set).The remaining items make up our final test set(GSIRT ), which is a calibrated scale of ability tocorrectly identify the relationship between the twosentence pairs.
Parameters of these items were esti-mated as part of the IRT model and the set of itemscan be used as an evaluation scale to estimate abilityof test-takers or RTE systems.
We used the mirt Rpackage (Chalmers et al, 2015) for our analyses.4 Results4.1 Response StatisticsTable 2 lists key statistics from the AMT HITs.
Mostof the sampled sentence pairs resulted in a gold stan-dard label being identified via a majority vote.
Dueto the large number of individuals providing labelsduring the HIT, we also wanted to see if a gold stan-dard label could be determined via a two-thirds su-permajority vote.
We found that 28.3% of the sen-Fleiss?
?
4GS 5GS Bowman et al 2015Contradiction 0.37 0.59 0.77Entailment 0.48 0.63 0.72Neutral 0.41 0.54 0.6Overall 0.43 0.6 0.7Table 3: Comparison of Fleiss?
?
scores with scores from SNLIquality control sentence pairs.tence pairs did not have a supermajority gold label.This highlights the ambiguity associated with iden-tifying entailment.We believe that the items selected for analysisare appropriate for our task in that we chose high-quality items, where at least 4 annotators selectedthe same label, indicating a strong level of agree-ment (Section 3.1).
We argue that our sample is ahigh-quality portion of the dataset, and further anal-ysis of items where the gold-standard label was onlyselected by 3 annotators originally would result inlower levels of agreement.Table 3 shows that the level of agreement as mea-sured by the Fleiss?
?
score is much lower when thenumber of annotators is increased, particularly forthe 4GS set of sentence pairs, as compared to scoresnoted in Bowman et al (2015).
The decrease inagreement is particularly large with regard to con-tradiction.
This could occur for a number of rea-sons.
Recognizing entailment is an inherently dif-ficult task, and classifying a correct label, particu-larly for contradiction and neutral, can be difficultdue to an individual?s interpretation of the sentencesand assumptions that an individual makes about thekey facts of each sentence (e.g.
coreference).
It mayalso be the case that the individuals tasked with cre-ating the sentence pairs on AMT created sentencesthat appeared to contradict a premise text, but can beinterpreted differently given a different context.Before fitting the IRT models we performed a vi-sual inspection of the 180 sentence pairs and re-moved items clearly not suitable for an evaluationscale due to syntactic or semantic discrepancies.
Forexample item 10 in Table 1 was removed from the5GS contradiction set for semantic reasons.
Whilemany people would agree that the statement is a con-tradiction due to the difference between football andsoccer, individuals from outside the U.S. would pos-sibly consider the two to be synonyms and classifythis as entailment.
Six such pairs were identified653and removed from the set of 180 items, leaving 174items for IRT model-fitting.4.2 IRT Evaluation4.2.1 IRT ModelsWe used the methods described in Section 3.3 tobuild IRT models to scale performance according tothe RTE task.
For both 4GS and 5GS items threefactors were identified, each related to items for thethree GSRTE labels (entailment, contradiction, neu-tral).
This suggests that items with the sameGSRTElabel within each set defines a separate ability.
In thesubsequent steps, items with different labels wereanalyzed separately.
After analysis, we were leftwith a subset of the 180 originally selected items.Refer to Table 1 for examples of the retained andremoved items based on the IRT analysis.
We re-tained 124 of the 180 items (68.9%).
We were ableto retain more items from the 5GS datasets (76 outof 90 - 84%) than from the 4GS datasets (48 outof 90 - 53.5%).
Items that measure contradictionwere retained at the lowest rate for both 4GS and5GS datasets (66% in both cases).
For the 4GS en-tailment items, our analysis found that a one-factormodel did not fit the data, and a two-factor modelfailed to yield an interpretable loading pattern afterrotation.
We were unable to build an IRT model thataccurately modeled ability to recognize entailmentwith the obtained response patterns.
As a result, noitems from the 4GS entailment set were retained.Figure 2 plots the empirical spline-smoothed ICCof one item (Table 1, item 9) with its estimated re-sponse curve.
The ICC is not continuously increas-ing, and thus a logistic function is not appropriate.This item was spotted for poor item fit and removed.Figure 3 shows a comparison between the ICC plotof a retained item (Table 1, item 4) and the ICC ofa removed item (Table 1, item 8).
Note that the re-moved item has an ICC that is very flat between -3and 3.
This item cannot discriminate individuals atany common level of ability and thus is not useful.The items retained for each factor can be consid-ered as an evaluation scale that measures a singleability of an individual test-taker.
As each factor isassociated with a separate gold-standard label, eachfactor (?)
is a person?s ability to correctly classifythe relationship between the text and hypothesis forFigure 2: Estimated (solid) and actual (dotted) response curvesfor a removed item.Figure 3: ICCs for retained (solid) and removed (dotted) items.one such label (e.g.
entailment).4.2.2 Item Parameter EstimationParameter estimates of retained items for each la-bel are summarized in Table 4, and show that allparameters fall within reasonable ranges.
All re-tained items have 2PL ICCs, suggesting no signif-icant guessing.
Difficulty parameters of most itemsare negative, suggesting that an average AMT userhas at least 50% chance to answer these items cor-rectly.
Although some minimum difficulties arequite low for standard ranges for a human popula-tion, the low range of item difficulty is appropriatefor the evaluation of NLP systems.
Items in eachscale have a wide range of difficulty and discrimina-tion power.With IRT we can use the heterogeneity of items toproperly account for such differences in the estima-tion of a test-taker?s ability.
Figure 4 plots the esti-mated ability of each AMT user from IRT againsttheir total number of correct responses to the re-tained items in the 4GS contradiction item set.
Thetwo estimates of ability differ in many aspects.
First,test-takers with the same total score may differ intheir IRT score because they have different response654Item Set Min.Diffi-cultyMax.Diffi-cultyMin.SlopeMax.Slope5GSContradiction -2.765 0.704 0.846 2.731Entailment -3.253 -1.898 0.78 2.61Neutral -2.082 -0.555 1.271 3.5984GSContradiction -1.829 1.283 0.888 2.753Neutral -2.148 0.386 1.133 3.313Table 4: Parameter estimates of the retained itemsFigure 4: Plot of total correct answers vs. IRT scores.patterns (i.e.
they made mistakes on different items),showing that IRT is able to account for differencesamong items.
Second, despite a rough monotonictrend between the two scores, people with a highernumber of correct responses may have a lower abil-ity estimate from IRT.We can extend this analysis to the case of RTEsystems, and use the newly constructed scales toevaluate RTE systems.
A system could be trained onan existing dataset and then evaluated using the re-tained items from the IRT models to estimate a newability score.
This score would be a measurement ofhow well the system performed with respect to thehuman population used to fit the model.
With thisapproach, larger sections of datasets can be devotedto training, with a small portion held out to build anIRT model that can be used for evaluation.4.2.3 Application to an RTE SystemAs a demonstration, we evaluate the LSTM modelpresented in Bowman et al (2015) with the items inour IRT evaluation scales.
In addition to the thetascores, we calculate accuracy for the binary classi-fication task of identifying the correct label for allItem Set Theta Score Percentile TestAcc.5GSEntailment -0.133 44.83% 96.5%Contradiction 1.539 93.82% 87.9%Neutral 0.423 66.28% 88%4GSContradiction 1.777 96.25% 78.9%Neutral 0.441 67% 83%Table 5: Theta scores and area under curve percentiles forLSTM trained on SNLI and tested on GSIRT .
We also reportthe accuracy for the same LSTM tested on all SNLI quality con-trol items (see Section 3.1).
All performance is based on binaryclassification for each label.items eligible for each subset in Table 5 (e.g.
all testitems where 5 of 5 annotators labeled the item as en-tailment for 5GS).
Note that these accuracy metricsare for subsets of the SNLI test set used for binaryclassifications and therefore do not compare with thestandard SNLI test set accuracy measures.The theta scores from IRT in Table 5 show that,compared to AMT users, the system performed wellabove average for contradiction items compared tohuman performance, and performed around the av-erage for entailment and neutral items.
For both theneutral and contradiction items, the theta scores aresimilar across the 4GS and 5GS sets, whereas theaccuracy of the more difficult 4GS items is consis-tently lower.
This shows the advantage of IRT to ac-count for item characteristics in its ability estimates.A similar theta score across sets indicates that wecan measure the ?ability level?
regardless of whetherthe test set is easy or hard.
Theta score is a con-sistent measurement, compared to accuracy whichvaries with the difficulty of the dataset.The theta score and accuracy for 5GS entailmentshow that high accuracy does not necessarily meanthat performance is above average when comparedto human performance.
However, theta score is notmeant to contradict accuracy score, but to provide abetter idea of system performance compared againsta human population.
The theta scores are a result ofthe IRT model fit using human annotator responsesand provide more context about the system perfor-mance than an accuracy score can alone.
If accuracyis high and theta is close to 0 (as is the case with 5GSentailment), we know that the performance of RTE655is close to the average level of the AMT user pop-ulation and that 5GS entailment test set was ?easy?to both.
Theta score and percentile are intrinsicallyin reference to human performance and independentof item difficulty, while accuracy is intrinsically inreference to a specific set of items.5 Discussion and Future WorkAs NLP systems have become more sophisticated,sophisticated methodologies are required to com-pare their performance.
One approach to create anintelligent gold standard is to use IRT to build mod-els to scale performance on a small section of itemswith respect to the tested population.
IRT modelscan identify dataset items with different difficultylevels and discrimination powers based on humanresponses, and identify items that are not appropriateas scale items for evaluation.
The resulting small setof items can be used as a scale to score an individ-ual or NLP system.
This leaves a higher percentageof a dataset to be used in the training of the system,while still having a valuable metric for testing.IRT is not without its challenges.
A large popu-lation is required to provide the initial responses inorder to have enough data to fit the models; however,crowdsourcing allows for the inexpensive collectionof large amounts of data.
An alternative methodol-ogy is Classical Test Theory, which has its own limi-tations, in particular that it is test-centric, and cannotprovide information for individual items.We have introduced Item Response Theory frompsychometrics as an alternative method for generat-ing gold-standard evaluation datasets.
Fitting IRTmodels allows us to identify a set of items that whentaken together as a test set, can provide a meaningfulevaluation of NLP systems with the different diffi-culty and discriminating characteristics of the itemstaken into account.
We demonstrate the usefulnessof the IRT-generated test set by showing that highaccuracy does not necessarily indicate high perfor-mance when compared to a population of humans.Future work can adapt this analysis to create eval-uation mechanisms for other NLP tasks.
The ex-pectation is that systems that perform well using astandard accuracy measure can be stratified basedon which types of items they perform well on.
Highquailty systems should also perform well when themodels are used together as an overall test of abil-ity.
This new evaluation for NLP systems can leadto new and innovative methods that can be testedagainst a novel benchmark for performance, insteadof gradually incrementing on a classification accu-racy metric.AcknowledgmentsWe thank the AMT Turkers who completed our an-notation task.
We would like to also thank theanonymous reviewers for their insightful comments.This work was supported in part by the HSR&Daward IIR 1I01HX001457 from the United StatesDepartment of Veterans Affairs (VA).
We also ac-knowledge the support of HL125089 from the Na-tional Institutes of Health.
This work was also sup-ported in part by the Center for Intelligent Informa-tion Retrieval.
The contents of this paper do not rep-resent the views of CIIR, NIH, VA, or the UnitedStates GovernmentReferencesJoohui An, Seungwoo Lee, and Gary Geunbae Lee.2003.
Automatic Acquisition of Named Entity TaggedCorpus from World Wide Web.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics - Volume 2, ACL ?03, pages 165?168, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Frank B. Baker and Seock-Ho Kim.
2004.
Item Re-sponse Theory: Parameter Estimation Techniques,Second Edition.
CRC Press, July.Islam Beltagy, Stephen Roller, Pengxiang Cheng, Ka-trin Erk, and Raymond J. Mooney.
2015.
Represent-ing Meaning with a Combination of Logical Form andVectors.
arXiv:1505.06816 [cs].
arXiv: 1505.06816.R Darrell Bock and Murray Aitkin.
1981.
Marginal max-imum likelihood estimation of item parameters: Appli-cation of an em algorithm.
Psychometrika, 46(4):443?459.Samuel R. Bowman, Gabor Angeli, Christopher Potts,and Christopher D. Manning.
2015.
A large anno-tated corpus for learning natural language inference.In Proceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing (EMNLP).Association for Computational Linguistics.Michael W Browne.
2001.
An overview of analytic ro-tation in exploratory factor analysis.
Multivariate Be-havioral Research, 36(1):111?150.656Rebecca F Bruce and Janyce M Wiebe.
1999.
Recog-nizing subjectivity: a case study in manual tagging.Natural Language Engineering, 5(02):187?205.Murray Campbell, A Joseph Hoane, and Feng-hsiungHsu.
2002.
Deep blue.
Artificial intelligence,134(1):57?83.Phil Chalmers, Joshua Pritikin, Alexander Robitzsch, andMateusz Zoltak.
2015. mirt: Multidimensional ItemResponse Theory, November.Wen-Hung Chen and David Thissen.
1997.
Local De-pendence Indexes for Item Pairs Using Item ResponseTheory.
Journal of Educational and Behavioral Statis-tics, 22(3):265?289, September.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL Recognising Textual EntailmentChallenge.
In Machine Learning Challenges.
Evalu-ating Predictive Uncertainty, Visual Object Classifica-tion, and Recognising Tectual Entailment, pages 177?190.
Springer.
DOI: 10.1007/11736790 9.Alexander Philip Dawid and Allan M Skene.
1979.
Max-imum likelihood estimation of observer error-rates us-ing the em algorithm.
Applied statistics, pages 20?28.David Ferrucci, Eric Brown, Jennifer Chu-Carroll, JamesFan, David Gondek, Aditya A Kalyanpur, Adam Lally,J William Murdock, Eric Nyberg, John Prager, et al2010.
Building watson: An overview of the deepqaproject.
AI magazine, 31(3):59?79.Weiwei Guo, Hao Li, Heng Ji, and Mona T. Diab.
2013.Linking Tweets to News: A Framework to EnrichShort Text Data in Social Media.
In ACL (1), pages239?249.
Citeseer.Dirk Hovy, Taylor Berg-Kirkpatrick, Ashish Vaswani,and Eduard H Hovy.
2013.
Learning whom to trustwith mace.
In HLT-NAACL, pages 1120?1130.Sergio Jimenez, George Duenas, Julia Baquero, Alexan-der Gelbukh, Av Juan Dios Btiz, and Av Mendizbal.2014.
UNAL-NLP: Combining soft cardinality fea-tures for semantic textual similarity, relatedness andentailment.
SemEval 2014, page 732.Alice Lai and Julia Hockenmaier.
2014.
Illinois-LH: ADenotational and Distributional Approach to Seman-tics.
SemEval 2014, page 329.Omar Levy, Ido Dagan, and Jacob Goldberger.
2014.Focused entailment graphs for open IE propositions.Proc.
CoNLL.Robert C MacCallum, Michael W Browne, and Hazuki MSugawara.
1996.
Power analysis and determination ofsample size for covariance structure modeling.
Psy-chological methods, 1(2):130.Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaella Bernardi, Roberto Zamparelli,and Fondazione Bruno Kessler.
2014.
A SICK curefor the evaluation of compositional distributional se-mantic models.Maria Orlando and David Thissen.
2000.
Likelihood-Based Item-Fit Indices for Dichotomous Item Re-sponse Theory Models.
Applied Psychological Mea-surement, 24(1):50?64, March.Rebecca J Passonneau and Bob Carpenter.
2014.
Thebenefits of a model of annotation.
Transactions ofthe Association for Computational Linguistics, 2:311?326.Roland Roller and Mark Stevenson.
2015.
Held-out versus Gold Standard: Comparison of Evalua-tion Strategies for Distantly Supervised Relation Ex-traction from Medline abstracts.
In Sixth Interna-tional Workshop on Health Text Mining and Informa-tion Analysis (LOUHI), page 97.David Silver, Aja Huang, Chris J Maddison, ArthurGuez, Laurent Sifre, George van den Driessche, JulianSchrittwieser, Ioannis Antonoglou, Veda Panneershel-vam, Marc Lanctot, et al 2016.
Mastering the gameof go with deep neural networks and tree search.
Na-ture, 529(7587):484?489.Janyce M. Wiebe, Rebecca F. Bruce, and Thomas P.O?Hara.
1999.
Development and use of a gold-standard data set for subjectivity classifications.
InProceedings of the 37th annual meeting of the Associ-ation for Computational Linguistics on ComputationalLinguistics, pages 246?253.
Association for Computa-tional Linguistics.Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-enmaier.
2014.
From image descriptions to visual de-notations: New similarity metrics for semantic infer-ence over event descriptions.
Transactions of the As-sociation for Computational Linguistics, 2(0):67?78,February.657
