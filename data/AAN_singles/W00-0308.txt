Epiphenomenal Grammar Acquisition with GG SGMarsal GavaldhInteractive Systems, Inc.1900 Murray Ave. Suite 203Pittsburgh, PA 15217, U.S.A.marsal?interactivesys, cornAbstractAs a step toward conversational systems that al-low for a more natural human-computer interac-tion, we rep6r~ on GSG, a system that, while pro-viding a natural-l~nguage interface to a variety ofapplications, engages in clarification dialogues withthe end user through which new semantic mappingsare dynamically acquired.
GsG exploits task- andlanguage-dependent information but is fully task-and language-independent in its architecture andstrategies.1 IntroductionAs conversational systems move from the realm ofscience fiction and research labs into people's every-day life, and as they evolve from the plain, system-directed interactions ~ la press or say one of so-called interactive voice response systems based onisolated-word recognizers and fixed-menu naviga-tion, to the more open, mixed-initiative dialoguescarried out in spoken dialogue systems based onlarge-vocabulary continuous peech recognizers andflexible dialogue managers (see, e.g., (Allen et al,1996; Denecke, 1997; Walker et al, 1998; Rudnickyet al, 1999; Zue et al, 2000)), the overall experien-tial quality of the human-computer interaction be-comes increasingly important.
That is, beyond theobvious factors of speech recognition accuracy andspeech synthesis naturalness, the most critical chal-lenge is that of providing conversational interactionsthat feel natural to human users (cf.
(Glass, 1999)).This, we believe, mainly translates into building sys-tems that possess ome degree of linguistic, reason-ing, and learning abilities.In this paper we report on GSG, a conversationalsystem that partially addresses these issues by beingable to dynamically extend its linguistic knowledgethrough simple, natural-language only interactionswith non-expert users: On a purely on-need basis,i.e., when the system does not understand what theuser means, GSG makes educated guesses, poses con-firmation and clarification questions, and learns newsemantic mappings from the answers given by theusers, as well as from other linguistic informationthat they may volunteer.
GSG provides, therefore,an extremely robust interface, and, at the same time,significantly reduces grammar development time be-cause the original grammar, while complete withrespect to the semantic representation of the do-main at hand, need only cover a small portion ofthe surface variability, since it will be automaticallyextended as an epip~enomenon f engaging in clari-fication dialogues with end users.2 Brief System DescriptionAs sketched in Figure 1, GSG is a conversational 1system built around the Soup parser (Gavald~,2000).GSG's principal (and possibly sole) knowledgesource is a task-dependent, semantic context-freegrammar (the Kernel Grammar).
At run-time, theGrammar is initialized as the union of the KernelGrammar and, possibly, the User Grammar A (user-dependent rules learned in previous essions).
TheGrammar gives rise to the 0nto\]ogy and to a parse-bank (collection of parse trees), which, togetherwith a possible Kernel Parsebank, becomes the Parse-bank, from which the statistical Prediction Modelsare trained.
The Ontology is a directed acyelicgraph automatically derived from the Grammar inwhich the nodes correspond to grammar nontermi-nals (NTs) and the arcs record immediate domi-nance relation, i.e., the presence of, say, NTi in aright-hand side (RHS) alternative of NTj will re-sult in an arc from NTi to NTj.
Nodes are an-notated as being "Principal" vs. "Auxiliary" (vianaming convention), "Top-level" vs. "Non-top level"(i.e., whether they are starting symbols of the gram-mar), and with having "Only NT daughters" vs."Only T daughters" vs. "Mixed"; arcs are anno-tated as being "Is-a" (estimated from being theonly non-optional NT in a RHS alternative) vs."Expresses" links, "Always-required" vs. "Always-optional" vs. "Mixed," and "Never-repeatable" vs.ZIn the work reported here, GsG's interactions are text-based (keyboard as input, text window as output), but GsGis being integrated with both a speech recognizer and a speechsynthesizer.36I Back-end Application Manager I)Figure 1: GsG's system diagram.
Ovals enclose knowledge sources, rectangles modules, and arrows indicateinformation flow.
Dashed components are optional.StrategyAll-top ParsingAnchor Mother PredictionsRequired/Is-a/... Daughters SearchVerbal Head SearchParser PredictionsKnowledge SourceGrammarPrediction ModelsOntologyPOS Tagger, OntologyGrammarTable 1: List of Gsc's main prediction and learning strategies.
"Always-repeatable" vs. "Mixed".
Also, a topo-logical sort 2 on the nodes is computed to derive ageneral-to-specific partial order of the NTs.A full system description is beyond the scope ofthis paper, but, very briefly, the User Interface me-diates all interactions with the end-user, the stack-based Dialogue Manager keeps track of current andpast utterances and ensuing clarification dialogues,and, together with the History Interaction, ensuresthat no answered question is asked again.
The GSGEngine manages the core of the systems' "intelli-gence," namely hypothesizing interpretations (to-gether with the Parse Tree Builder) and on-line learn-ing of semantic mappings.3 Example  D ia logueTo illustrate the workings of GSG, let's analyze anexample interaction i  an e-mail client task.
Figure 2shows the example dialogue, Figure 3 presents a rel-evant fragment of the semantic ontext-free gram-mar 3 used to analyze the input, and Table 1 above2Requires that the grammar be acyclic.3Containing, approximately, 300 NTs, 500 Ts, and 800RHS alternatives, out of which about 55% is dedicated tolists the main prediction and learning strategies em-ployed.The user inputs (U1) and, as it turns out, nei-ther retrieve, nor bob, nor christmas are in the gram-mar and no full parse of (U1) can be found.
Ratherthan responding with a I don't understand - -  pleaserephrase, GSG sets its predictive machinery in mo-tion with the goal of constructing a full interpreta-tion for (U1).
4The first step is to reparse (U1) in a mode inwhich all NTs are considered top-level, i.e., able toslmnd at the root Of a parse tree.
This producesa set of still unparsed words, such as retrieve, andparse subtrees, such as from bob being parsed underSENDER (via grammar ules (R6) to (R10)) and, am-biguously, under \[sourceFolder\] as well (via gram-mar rules (Rl l )  and (R12)).
All of it (i.e., the fullcontent of the chart) is taken as evidence by  thePrediction Models to postulate the overall meaningof the original utterance.
In this case (see (U2)cover temporal and numeric expressions, and digits and let-ters.4The hypothesized interpretation assumes that a singleparse tree will cover the entire problematic sentence.37(U9)(UIO)(Ull)(U12)(U1) U: retrieve all messages from bob that were sent after christmas(U2) S: I don't understand right away what you mean but let me guess..."retrieve all messages from bob that were sent after christmas" is a way to express:1. send mail2.
reply mail3.
read mall0.
None of the above(U3) U: none(U4) S: "retrieve all messages from bob that were sent after christmas" is a way to express:1. move mail2.
list mail0.
None of the above(U5) U: list(U6) S: By "from bob" do you mean:1. source folder2.
sender(UT) U: sender(US) S " 2christmas" is a way to express:-1. yesterday (date relative yesterday)2. tomorrow (date relative tomorrow)3. today (date relative today)0.
None of the aboveU: by christmas i'm referring to december twenty fifthS: Thanks for teaching me the meaning of "christmas"!Thanks for teaching me the meaning of "retrieve all messages from bob that were sent after christmas"!I understand "retrieve all messages from bob that were sent after december twenty fifth"U: retrieve last email to mary before christmasS: I understand "retrieve last email to mary before december twenty fifth"Figure 2: Example dialogue between a user (U)to (U5) 5) the suggestions of the Prediction Modelsare not particularly accurate (the correct choice ispresented only in fifth place), but, considering thatthe head verb (retrieve) is not even in the grammar,such a response to (U1) is definitely better than giv-ing up.
The effect of (U5) is to select \ [ l i s tMa i l \ ]as (U1)'s "anchor mother" (logical root of the over-all interpretation).
But to complete the parse treea few details still need to be filled in.
To that ef-fect (U6) is generated to disambiguate ~rorn bob and(US) to find the right mapping for christmas.
Thereasoning behind the rather puzzling choices offeredby (US) comes from applying the Parser Predictionsstrategy: given the context in which an unparsed se-quence (in this case, single word) christmas appears,i.e., the subtree DATE..AFTER..PRE covering after (via(Pal4)), the grammar is traversed to find likely con-tinuations of the context (left context only in thiscase).
Since DATE.AFTER_PRE can be immediatelyfollowed by \ [datehf ter \ ]  (see (R13)) that makes\ [datehf ter \ ]  a candidate to cover the unparsed se-5The options presented in (U2) and (U4) are generatedat the same time, the only reason why they are split is toprevent overwhelming the end user, who may be hearing thechoices poken over the telephone.
Also, note that in (U3)the user could have also said zero, or none of the above andachieve the same result - -  or, alternatively, they could havevolunteered information as in (ug).and the system (S) on an e-mall client task.quence christmas.
However, since, according to theOntology, \ [dateAf ter \ ]  does not allow terminals asimmediate daughters, a search is performed to findNTs under \ [dateAf ter \ ]  that permit it.
In this case(via (R15) to (R19)) it suggests yesterday, tomorrow,etc.
6 The user, though, realizing that the systemdoes not directly understand christmas, volunteers(ug) 7, from which the mapping (M2) in Figure 4 islearned.At this point one may wonder about the fate of theunparsed word retrieve, since no question was askedabout it.
The answer is that GsG need not ask aboutevery single prediction, if the confidence value is highenough.
In this case, as soon as \ [ l i s tga i l \ ]  wasestablished (in (U5)) as the anchor mother, a VerbalHead Search strategy was launched to see whether,among the unparsed words, a verb was found thatcould be placed in a mostly-verb NT s directly under~In fact it suggests \[DATE_RELATIVE:yesterday\],\[DATE_RELATIVE:tomorrow\], etc, but it presents an ex-ample automatically generated from such NTs.7Obviously "the meaning of Christmas" (cf.
cheerful(U10)) may be much more profound than a shorthand forDecember 25 - -  but, alas, conveying that is well beyond thesimple grammar presented here.SA "verbness" ratio is automatically computed for eachcandidate NT.
by running the POS tagger on automaticallygenerated sentences from the NTs in question.
(%Ve used a38(1~I) \[listMail\] +---(R2) \[moveMail\](R3) LIST ,--(R4) MOVE(R5) MAIL_ARGUMENTS(I~6) SENDER(R.7) SENDER_PRE(I:~8) \[sender\] +----(R9) \[name: STRING\](RI0) PERSON_0R_INSTITUTION_NAME +---(RiI) \[sourceFolder\] +---(R12) \[folderName : STRING\] +.---(R13) \[dateRange\] +----(R14) DATE_AFTER_PRE(RlS) \[dateAfte=\] +--(1~16) \[datePoint :DATE\] +--(RIT) DATE_POINT.ARGUMENT(RI8) \[datcP_oint : DATE_RELATIVE\] +---(R19) \[DATE~ELATIVE: yesterday\]*VERB_DESIKE LIST *T0.FOR.ME +MAIL_ARGUMENTS*VERB_DESIRE MOVE *+MAIL_ARGUMENTS *\[sourceFolder\] \[destinationFolder\]list I getmoveSENDER I ~CIP IENT I S~JECT I DnE  I MESSAGE_IDX I ._*SENDER_PRE \[sender\]from I by\[name:STRING\] I \[emailhddress:STRING\]PERSON_0R_INSTITUTION_NAME I MAILING_LIST_NAME+WILDCARDfrom \[folderName:STRING\] *FOLDERWILDCARD(DATE_AFTER_PKE \[dateAfter\]) I (DATE_BEFORE_PKE \[dateBefore\]) ...after I f  tom I since\[datePoint : DATE\]+DATE_POINT.ARGUMENT\[datePoiRt:DATE_RELATIVE\] I \[daZePoint:DATE_FIXED\] I ...\[DATE_RELATIVE:yesterday\] \[ \[DATE_RELATIVE:tomorrow\] I ...yesterdayFigure 3: Grammar fragment for an e-mail client task.
'*' indicates optiSnality of adjacent oken, '+'repeatability, and '1' separates RHS alternatives.
Terminals are italicized.
NILDCARD is a special NT thatmatches any out-of-vocabulary word or any in-vocabulary word present in a list for that purpose.\ [ l i s tMa i l \ ] .
The result was highly positive and ledto the acquisition of the RHS alternative (M1).It is worth mentioning here that there are twokinds of mappings that GSG learns: RHS alterna-tives and subtree mappings.
Learning new RHS al-ternatives is the preferred way because the knowl-edge can be incorporated into the Parsebank (and, inturn, into the Prediction Models).
That is the effectof adding (M1) to the Grammar: Since the Parsebankand the Prediction Models are updated on-line, thepresence of the word retrieve in subsequent utter-ances becomes a strong indicator of LIST and, asso-ciatively, of \ [ l i s tMa?l \ ] .
However, when the sourceexpression can not be mapped into the desired targetstructure via grammar rules, as in (M2), the only so-lution is to remember the equivalence.
This kind oflearning, although definitely useful since the mean-ing of the source expression will be henceforth re-membered, cannot be incorporated into the Predic-tion Models.Right after (U9), (U1) is considered fully un-derstood and the interpretation is automaticallymapped into the feature structure (FS1) 9 in Fig-ure 5, which is then shipped to the Back-end Ap-plication Manager.Finally, when (Ul l )  comes in, a correct analysis isproduced thanks to the mappings just learned from(U1), 1?
and (FS2) in is generated.modified version of Brill's tagger (Brill 1994).
)9The mapping is simply a removal of auxiliary NTs fromthe parse tree, plus value extraction of dates, numbers andstrings from certain subtrees, e.g., subtree in (M2) becomesthe substructure tinder da~ePoin~ in (FSI).1?Note that rule \[listMail\] ~ LIST +MAIL_ARGUMENTS4 D iscuss ionThe example above illustrates the philosophy ofGSG, n namely, to exploit task and linguistic knowl-edge to pose clarification questions in the face ofincomplete analyses, 12 build correct interpretations,and acquire new semantic mappings.
Thus, a contri-bution of Gso, is the demonstration that from a sim-ple context-free grammar, with a very lightweightformalism, one can extract enough information (On-tology, Parsebank, Parser Predictions strategy) toconduct meaningful clarification dialogues.
Note,moreover, that such dialogues occur entirely withinGSG, with the Back-end Application Manager receiv-ing only finalized feature structures.
13Another advantage is the ease with which natural-language interfaces can be constructed for new do-mains: Since all the task and linguistic knowledgeis extracted from the grammar, 14 one need only de-velop a Kernel Grammar that models the domain at(extracted from the final interpretation of (U1)) would havebeen learned too, but its subsumption by existing rule (R1)was automatically detected.nBased on the pioneering work of (Lehman, 1989).12Detected by a lack of interpretation, excessively frag-mented interpretation, orby being told by the end user thatthe automatically generated paraphrase oftheir input is notwhat they meant.lsOf course, prediction accuracy can improve if the Back-end Application Manager can be incorporated as a knowledgesource to, for example, contribute in the ranking of hypothe-ses, but the point is that it is not necessary and that, as longas the capabilities of the back-end application are adequatelymodeled by the Grammar, the construction of the correct in-terpretation can be performed within Gsc alone.t4Except for the POS Tagger and the Syntactic Grammar.39(M1)(M2)DATE_POINT_ARGUMENTMONTHMONTH_VAL\[month:f2\]decemberDAY_OF_MONTH\[dayOfMonth:INTEGER\]ORDINAL-NUMBER-O-99CARDINAL-NUMBER-TENS\[INTEGER-CARDINAL:20\]twentyORDINAL-NUMBER-UNITS\[INTEGER-ORDINAL:5\]99hLIST ~ retrievechristmaslistMailmessageIdx: ullsendername: bobdateRangedateAfterdatePointmonth: I~dayOfMonth: ~5Figure 4: Mappings learned from the dialogue in Figure 2.listMailmessageIdx wlastrecipientname: marydateRangedateBeforedatePointmonth: 12dayOfMonth: 25(FS1) (FS2)Figure 5: Feature structures ent to the Back-end Application Manager after (U10) and (U12) in Figure 2.hand via its NTs 15 but need not provide a high cov-erage of the utterances possible in the domain (datawhich may not be available anyway).
Also, reuse ofexisting grammar modules for, e.g., dates and num-bers, is straightforward.However, a fear of letting the end user (indirectly)modify a grammar is that the grammar may growuntamed and become filled with new rules that dis-rupt the Kernel Grammar.
To prevent hat, besidesthe careful construction of interpretations via thestrategies described above, GSG employs two safetymechanisms: before a rule is added to the gram-mar, it is checked whether it introduces ambiguityto the grammar, 16 and whether it disrupts existing15Knowledge of, e.g., how the Ontology is computed helps,but it coincides with the most natural way of writing well-structured, context-free semantic grammars.lSAccomplished by using the SouP parser in yet anothermode: parsing of RHSs (expanded to RHS paths) instead ofterminals.
In this case, existence of a parse tree covering anentire RHS path indicates ambiguity.
Note that if all RHSpaths of the new rule can be parsed under the current RHS ofthe new rule's left-hand side, then the new rule is subsumedby the existing RHS and can therefore be discarded (cf.
noteIo).
(correct) interpretations.
17 In this way, some of thenew rules may have to be discarded, but at least thehealth of the grammar is preserved, isAnother concern may be that the new mappingsend up generating feature structures that are not un-derstood by the Back-end Application Manager.
Toavoid that, GSG only allows a principal NT to bedominated by another principal NT if such domi-nance relation is licensed by the Kernel Grammar.This guarantees that all resulting feature structuresbe structurally correct (although they may containunexpected atomic values).A current limitation of GsG lies in the difficulty ofsegmenting long sequences of unparsed words: GSGuses POS tagging followed by noun-phrase bracket-ing (via parsing with a shallow Syntactic Grammar),which represents an improvement over the SingleSegment Assumption (cf.
(Lehman, 1989)), but isstill far from perfect and can disrupt the ensuingclarification dialogue.
Also, the number of questionsthat the system can pose as it builds an interpre-17Achieved by reparsing (a subset of) the Parsebank.
Notethat SOUP can typically parse in the order of 100 utterancesper second (cf.
(Gavaldb.
2000)).ISAssuming minimally co6perative and consistent users.40tation, may, in occasion, exceed the patience of theend user (but the command cancel is always under-stood).The hardest problem we have encountered so far istypical of natural-language interfaces but is exacer-bated in GSG (as it treats every unparsable sentenceas an opportunity to learn), and that is the difficultyof identifiying in-domain end-user sentences that gobeyond the capabilities of the end application, or, inother words, are not expressible in the grammar.Finally, as Gsc becomes fully integrated with aspeech recognizer, it remains to be seen how an op-timal point in the tradeoff between the wide cover-age but relatively low word recognition accuracy ob-tained with a loose dictation grammar, and the nar-row coverage but high word accuracy achieved witha tight, task-dependent grammar, can be found, andhow the degradations of the input is going to affectGSG'S behavior.Overall, however, we believe that Gsc, by virtueof its built-in robustness, minimal initial knowledgerequirements, and learning abilities, begins to em-body the kind of qualities that are necessary for con-versational systems, if they are to provide, withoutexorbitant development effort, an interaction thayfeels truly natural to humans.Re ferencesAllen, James, et al (1996).
Robust Understandingin a Dialogue System.
In Proceedings o\] A CL-1996.Brill, Eric.
(1994).
Some Advances in Part of SpeechTagging.
In Proceedings o\] AAAI-1994.Denecke, Matthias.
(1997).
An Information-based Approach for Guiding Multi-modal Human-Computer Interaction.
In .Proceedings of IJCAI-199ZGavald~, Marsal.
(2000).
SouP: A Parser for Real-world Spontaneous Speech.
In Proceedings o\] theSixth International Workshop on Parsing Tech-nologies (IWPT-2000).Glass, James.
(1999).
Challenges for Spoken Dia-logue Systems.
In Proceedings of the 1999 IEEEASRU Workshop.Lehman, Jill.
(1989).
Adaptive Parsing: Sell-extending Natural Language Interfaces.
Ph.D. dis-sertation, School of Computer Science, CarnegieMellon University.Rudnicky, Alex, et al (1999).
Creating NaturalDialogs in the Carnegie Mellon COMMUNICATORSystem.
In Proceedings o\] Eurospeech-1999.Walker, Marilyn, et al (1998).
Learning OptimalDialogue Strategies: A Case Study of a Spo-ken Dialogue Agent for Email.
In Proceedings ofCOLING/A CL-i998.Zue, Victor, et al (2000).
JUPITER: A Telephone-Based Conversational Interface for Weather Infor-mation.
In IEEE Transactions on Speech and Au-dio Processing, Vol.
8 , No.
1.41
