IIiHiHImimimillIHiIHIHHIIIIIiILinguistic Theory in Statistical Language LearningChrister SamuelssonBell Laboratories, Lucent Technologies600 Mountain Ave, Room 2D-339,Murray Hill, NJ 07974, USAchr i s te r@research ,  be l l - labs ,  tomAbstractThis article attempts to determine what ele-ments of linguistic theory are used in statisti-cal language learning, and why the extractedlanguage models look like they do.
The studyindicates that some linguistic elements, uch asthe notion of a word, are simply too useful tobe ignored.
The second most important factorseems to be features inherited from the origi-nal task for which the technique was used, forexample using hidden Markov models for part-of-speech tagging, rather than speech recogni-tion.
The two remaining important factors areproperties of the runtime processing schemeemploying the extracted language model, andthe properties of the available corpus resourcesto which the statistical learning techniques areapplied.
Deliberate attempts to include lin-guistic theory seem to end up in a fifth place.1 IntroductionWhat role does linguistics play in statistical an-guage learning?
"None at all!"
might be the answer,if we ask hard-core speech-recognition professionals.But even the most nonlinguistic language model, forexample a statistic word bigram model, actually re-lies on key concepts integral to virtually all linguistictheories.
Words, for example, and the notion thatsequences of words form utterances.Statistical language learning is applied to some setof data to extract a language model of some kind.This language model can serve a purely decorativepurpose, but is more often than not used to pro-cess data in some way, for example to aid speechrecognition.
Anyone working under the pressure ofproducing better esults, and who employs languagemodels to this purpose, such a researchers in thefield of speech recognition, will have a high incen-tive of incorporating useful aspects of language intohis or her language models.
Now, the most useful,and thus least controversial ways of describing lan-guage will, due to their usefulness, find their wayinto most linguistic theories and, for the very samereason, be used in models that strive to model lan-guage successfully.So what do the linguistic theories underlying vari-ous statistical language models look like?
And why?It may be useful to distinguish between those aspectsof linguistic theory that are incidentally in the lan-guage model, and those that are there intentionally.We will start our tour of statistical language learn-ing by inspecting language models with "very little"linguistic content, and then proceed to analyse in-creasingly more linguistic models, until we end withmodels that are entirely linguistic, in the sense thatthey are pure grammars, associated with no statis-tical parameters.2 Word N-gr_~m ModelsLet us return to the simple bigram word model,where the probability of each next word is deter-mined from the current one.
We already noted thatthis model relies on the notion of a word, the notionof an utterance, and the notion that an utterance isa sequence of words.The way this model is best visualized, and as ithappens, best implemented, is as a finite-state au-tomaton (FSA), with arcs and states both labelledwith words, and transition probabilities associatedwith each arc.
For example, there will be one statelabelled The with one arc to each other state, forexample to the state Cat, and this arc will be la-belled cat.
The reason for labelling both arcs andstates with words is that the states constitute theonly memory device available to an FSA.
To re-member that the most recent word was "cat", allarcs labelled cat must fall into the same state Cat.The transition probability from the state The alongthe unique arc labelled cat to the state Cat will bethe probability of the word "cat" following the word"the", P(cat l the).More generally, we enumerate the wordsSamuelsson 83 Linguistic TheoryChrister Samuelsson, Bell Laboratories (1998) Linguistic Theory in Statistical Language Learning.
In D.M.W.
Powers (ed.
)NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 83-89.
{Wa,...,wlv} and associate a state Si witheach word wl.
Now the automaton has the states{$1,.
.
.
,  SN} and from each state Si there is an arclabelled wj to state Sj with transition probabilityP(wj I wi), the word bigram probability.
Toestablish the probabilities of each word starting orfinishing off the utterance, we introduce the specialstate So and special word w0 that marks the endof the utterance, and associate the arc from So toSi with the probability of wi starting an utterance,and the arc from Si to So with the probability ofan utterance nding with word wl.If we want to calculate the probability of a wordsequence wil ... wi,,, we simply multiply the bigramprobabilities:P(wi~ .
..wi,~) == P(wi, I WO)" P(wi2 I w i , ) .
.
.
.
.
P(wo I Wi.
)We now recall something from formal languagetheory about the equivalence between finite-stateautomata nd regular languages.
What does theequivalent regular language look like?
Let's just firstrename So S and, by stretching it just a little, let theend-of-utterance marker wo be ~, the empty string.S ~ wiSi P(wi \[ e)S~ ~ wjSj P(wj \[wi)Si -* e P(~ \[ wi)Does this give us any new insight?
Yes, it does!
Let'sdefine a string rewrite in the usual way: cA7 =~ a~7if the rule A -+ fl is in the grammar.
We can thenderive the string Wil ... wl, from the top symbol Sin n+l  steps:S ::~ WilSil ~ WilWi2Si2 :=~ .. .Wi 1 ?
.
.
Wi nNow comes the clever bit: if we define the deriva-tion probability as the product of the rewrite proba-bilities, and identify the rewrite and the rule proba-bilities, we realize that the string probability is sim-ply the derivation probability.
This illustrates oneof the most central aspects of probabilistic parsing:String probabilities are defined in terms o\]derivation probabilities.So the simple word bigram model not only em-ploys highly useful notions from linguistic theory,it implicitly employs the machinery of rewrite rulesand derivations from formal anguage theory, and italso assigns string probabilities in terms of deriva-tion probabilities, just like most probabilistic pars-ing schemes around.
However, the heritage fromfinite-state automata results in simplistic models ofinterword ependencies.General word N-gram models, of which word bi-gram models are a special case with "N" equal totwo, can be accommodated in very much the sameway by introducing states that remember not onlythe previous word, but the N-1 previous words.
Thisgeneralization is purely technical and adds little orno linguistic fuel to the model from a theoreticalpoint of view.
From a practical point of view, thegain in predictive power using more conditioning inthe probability distributions is very quickly over-come by the difficulty in estimating these probabil-ity distributions accurately from available trainingdata; the perennial sparse-data problem.So why does this model look like it does?
Weconjecture the following explanations: Firstly, it isdirectly applicable to the representation used by anacoustic speech recognizer, and this can be done ef-ficiently as it essentially involves intersecting twofinite-state automata.
Secondly, the model parame-ters - -  the word bigram probabilities - -  Can be es-timated directly from electronically readable texts,and there is a lot of that a~ilable.3 Tag N-gram ModelsLet us now move on to a somewhat more linguisti-cally sophisticated language model, the tag N-grammodel.
Here, the interaction between words is me-diated by part-of-speech (PoS) tags, which consti-tute linguistically motivated labels that we assign toeach word in an utterance.
For example, we mightlook at the basic word classes adjectives, adverbs,articles, conjunctions, nouns, numbers, prepositions,pronouns and verbs, essentially introduced alreadyby the ancient Greek Dionysius Thrax.
We imme-diately realise that this gives us the opportunity toinclude a vast amount of linguistic knowledge intoour model by selecting the set of PoS tags appro-priately; consequently, this is a much debated andcontroversial issue.Such a representation can be used for disambigua-tion, as in the case of the well-known, highly ambigu-ous example sentence "Time flies like an arrow".
Wecan for example prescribe that "Time" is a noun,"flies" is a verb, "like" is a preposition (or adverb,according to your taste), "an" is an article, and that"arrow" is a noun.
In effect, a label, i.e., a part-of-speech tag, has been assigned to each word.
Werealise that words may be assigned ifferent labelsin different context, or in different readings; for ex-ample, if we instead prescribe that '2\]ies" is a nounand "like" is a verb, we get another eading of thesentence.Samuelsson 84 Linguistic TheoryIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII II IIIIIIII IIIIIIIIII IIIIII IIII II II II ImWhat does this language model ook like in moredetail?
We can actually recast it in virtually thesame terms as the word bigram model, the only dif-ference being that we interpret each state Si as aPoS tag (in the bigram case, and as a tag sequencein the general N-gram case):S -+ wkSi P(Si ~ wk \[ S)S~ -+ ~kSj P(Sj ~ wk I S~)S~ -+ e P(e I &)Note that we have now separated the words Wk fromthe states Si and that thus in principle, any statecan generate any word.
This is actually a slightlymore powerful formalism than the standard hidden-markov model (HMM) used for N-gram PoS tagging(5).
We recast it as follows:S ~ TiS~ P(S~ I s)s~ -+ TjSj P(s# I&)Si -+ e P(e I S i)Ti -~ w~ P(wk \[ Ti)Here we have the rules of the form Si ~ TjSj,  withthe corresponding probabilities P(Sj  \[ Si), encodingthe tag N-gram statistics.
This is the probabilitythat the tag Tj will follow the tag Ti, (in the bigramcase, or the sequence ncoded by Si in the generalN-gram case).
The rules Ti -+ wk with probabilitiesP(Wk \[ Ti) are the lexical probabilities, describingthe probability of tag Ti being realised as word wk.The latter probabilities seem a bit backward, as wewould rather think in terms of the converse probabil-ity P(Ti \[ Wk) of a particular word wk being assignedsome PoS tag Ti, but one is easily recoverable formthe other using Bayesian inversion:P(Ti I wk) " e(wk)P(wk IT i) = P(T~)We now connect he second formulation with thefirst one by unfolding each rule Tj ---> wk into eachrule Si -+ TjSj.
This lays bare the independenceassumptionP(s j  & ~k I s~) = P(% I si) .
P(Wk I Tj)As should be clear from this correspondence, theHMM-based PoS-tagging model can be formulatedas a (deterministic) FSA, thus allowing very fast pro-cessing, linear in string length.The word string wkl .
.
.
wk, can be derived fromthe top symbol S in 2n+l steps:S ~ Ti, Si a =:~ wkxSi, =~ WklTi2Si2 ::~Wkx Wk2 Si2 =~ ?
?
?
=~ Wkx ?
?
?
Wk,Samuelsson 85The interpretation of this is that we start off in theinitial state S, select a PoS tag Tia at random, ac-cording to the probability distribution in state S,then generate the word wkl at random accordingto the lexical distribution associated with tag Til,then draw a next PoS tag Ti2 at random accordingto the transition probabilities associated with stateSi~, hop to the corresponding state Si2, generate theword wk2 at random according to the lexical distri-bution associated with tag Ti2, etcetera.Another general lesson can be learned from this: Ifwe wish to calculate the probability of a word string,rather than of a word string with a particular tagassbciated with each word, as the model does as itstands, it would be natural to sum over the set ofpossible ways of assigning PoS tags to the words ofthe string.
This means that:The probability of a word string is the sumof its derivation probabilities.The model parameters P(Sj \[ Si) and P(wk I Tj)can be estimated essentially in two different ways.The first employs manually annotated training dataand the other uses unannotated data and some rees-timation technique such as Baum-Welch reestima-tion (1).
In both cases, an optimal set of parame-ters is sought, which will maximize the probabilityof the training data, supplemented with a portionof the black art of smoothing.
In the former case,we are faced with two major problems: a shortageof training data, and a relatively high noise level inexisting data, in terms of annotation i consistencies.In the latter case, the problems are the instability ofthe resulting parameters as a function of the initiallexieal bias required, and the fact that the chancesof finding a global optimum using any computation-ally feasible technique rapidly approach zero as thesize of the model (in terms of the number of tags,and N) increases.
Experience as shown that, despitethe noise level, annotated training data yields bettermodels.Let us take a step back and see what we have got:We have notion of a word, the notion of an utterance,the notion that an utterance is a sequence of words,the machinery of rewrite rules and derivations, andstring probabilities are defined as the sum of the ofderivation probabilities.
In addition to this, we havethe possibility to include a lot of linguistic knowl-edge into the model by selecting an appropriate setof PoS tags.
We also need to somehow specify themodel parameters P(Sj  \[ Si) and P(wk I Tj).
Oncethis is done, the model is completely determined.In particular, the only way that syntactic relationsare modelled are by the probability of one PoS tagLinguistic Theorygiven the previous tag (or in the general N-gramcase, given the previous N-1 tags).
And just as inthe case of word N-grams, the sparse data problemsets severe bounds on'N, effectively limiting it toabout three.We conjecture that the explanation to why thismodel looks like it does is that it was importedwholesale from the field of speech recognition, andproved to allow fast, robust processing at accuracylevel that until recently were superior to, or on parwith, those of hand-crafted rule-based approaches.4 Stochastic Grammar ModelsTo gain more control over the syntactic relation-ships between the words, we turn to stochasticcontext-free grammars (SCFGs), originally proposedby Booth and Thompson (4).
This is the frameworkin which we have already discussed the N-gram mod-els, and it has been the starting point for many ex-cursions into probabilistic-parsing land.
A stochas-tic context-free grammar is really just a context-free grammar where each grammar ule has beenassigned a probability.
If we keep the left-hand-side(LHS) symbol of the rule fix, and sum these proba-bilities over the different RHSs, we get one, since theprobabilities are conditioned on the LHS symbol.The probability of a particular parse tree is theprobability of its derivation, which in turn is theproduct of the probability of each derivation step.The probability of a derivation step is the proba-bility of rewriting a given symbol using some gram-mar rule, and equals the rule probability.
Thus, theparse-tree probability is the product of the rule prob-abilities.
Since the same parse tree can be derivedin different ways by first rewriting some symbol andthen another, or vice versa, we need to specify the or-der in which the nonterminal symbols of a sententialform are rewritten.
We require that in each deriva-tion step, the leftmost nonterminal is always rewrit-ten, which yields us the leftmost derivation.
This es-tablishes a one-to-one correspondence between parsetrees and derivations.We now have plenty of opportunity to include lin-guistic theory into our model by the choice of syn-tactic categories, and by the selection of grammarrules.
The probabilistic limitations of the model mir-ror the expressive power of context-free grammars,as the independence assumptions exactly match thecompositionality assumptions.
For this reason, thereis an efficient algorithm for finding the most proba-ble pares tree, or calculating the string probabilityunder an SCFG.
The algorithm is a variant of theCocke-Kasami-Younger (CKY) algorithm (17), butcan also be seen as an incarnation of a more gen-eral dynamic-programming scheme, and it is cubicin string length and grammar size.
We conjecturethat exactly the properties of SCFGs discussed inthis paragraph explain why the model looks like itdoes.We again have the choice between training themodel parameters, the rule probabilities, on anno-tated data, or use unannotated data and some rees-timation method like the inside-outside algorithm,which is the natural generalization of the Baum-Welch method of the previous section.
If the chancesof finding a global optimum were slim using theBaurn-Welch algorithm, they're virtually zero us-ing the inside-outside algorithm.
There is also verymuch instability in terms of what set of rule prob-abilities one arrives at as a function of the initialassignment of rule probabilities in the reestimationprocess.
The other option, training on annotateddata, is also problematic, as there is precious littleof it available, and what exist is quite noisy.
A cor-pus of CFG-analysed sentences is known as a treebank, and tree banks will be the topic of the nextsection.As we have been stressing, the key idea is to assignprobabilities to derivation steps.
If we instead lookat the rightmost derivation in reverse, as constructedby an LR  parser, we can take as the derivation prob-ability the probability of the action sequence, i.e.,the product of the probabilities of each shift and re-duce action in it.
This isn't exactly the same thinkas an SCFG, since the probabilities are typically notconditioned on the LHS symbol of some grammarrule, but on the current internal state and the cur-rent lookahead symbol.
As observed by FernandoPereira (12), this gives us the possibility to throw ina few psycho-linguistic features such as right associ-ation and minimal attachment by preferring shift ac-tions to reductions, and longer reductions to shorterones, respectively.
So if these features are present inlanguage, they should show up in our training data,and thus in our language model.
Whether these fea-tures are introduced or incidental is debatable.We can take the idea of derivational stochasticgrammars one step further and claim that a parsetree constructed by any sequence of derivation ac-tions, regardless of what the derivation actions are,should be assigned the product of the probabilitiesof each derivation step, appropriately conditioned.This idea will be crucial for the various extensionsto SCFGs  discussed in the next section.5 Models Using Tree BanksAs previously mentioned, a tree bank is.a corpus ofCFG-annotated sentences, i.e., a collection of parseSamuelsson 86 Linguistic Theory!1!iIiIIII!1!1IIIIIIIiIiIIIIIIII!1iillillHIiHImmI IHIIglHIHiEtrees.
The mere existence of a tree bank actu-ally inspired a statistic language model, namely thedata-oriented parsing (DOP) model (3) advocatedby Remko Scha and Rens Bod.
This model parsesnot only with the entire tree bank as its grammar,but with a grammar consisting of each subtree ofeach tree in the tree bank.
One interesting conse-quence of this is that there will in general be manydifferent leftmost derivations of any given parse tree.This can most easily be seen by noting that there isone leftmost derivation for each way of cutting up aparse tree into subtrees.
Therefore, the parse prob-ability is defined as the sum of the derivation prob-abilities, which is the source to the NP-hardness offinding the most probable parse tree for a given in-put sentence under this model, as demonstrated byKhalil Sima'an (15).There aren't really that many tree banks around,and the by far most popular one for experiment-ing with probabilistic parsing is the Penn Treebank(11).
This leads usto the final source of influence onthe linguistic theory employed in statistical languagelearning: the available training and testing data.The annotators of the Penn Treebank may haveoverrated the minimal-attachment principle, result-ing in very fiat rules with a minimum of recursion,and thus in very many rules.
In fact, the Wall-Street-Journal portion of it consists of about a mil-lion words analysed using literally tens of thousandsof distinct grammar rules.
For example, there is onerule of the formNP ~ Det Noun (, Noun )n Conj Nounfor each value of n seen in the corpus.
There isnot even close to enough data to accurately estimatethe probabilities of most rules seen in the trainingdata, let alne to achieve any type of robustness forunseen rules.
This inspired David Magerman andsubsequently Michael Collins to instead generate theRHS dynamically during parsing.Magerman (10) grounded this in the idea that aparse tree is constructed by a sequence of gener-alized derivation actions and the derivation prob-ability is the parse probability, a framework that issometimes referred to as history-based parsing (2),at least when decision trees are employed to deter-mine the probability of each derivation action taken.More specifically, to allow us to assemble the RI-ISsas we go along, any previously constructed syntac-tic constituent is assigned the role of the leftmost,rightmost, middle or single daughter of some otherconstituent with some probability.
It may or maynot also be the syntactic head of the other con-stituent, and here we have another piece of highlySamuelsson 87useful linguistic theory incorporated into a statis-tical language model: the grammatical notion of asyntactic head.
The idea here is to propagate up thelexical head to use (amongst other things) lexicalcollocation statistics on the dependency level to de-termine the constituent boundaries and attachmentpreferences.Collins (6; 7) followed up on these ideas and addedfurther elegance to the scheme by instead generat-ing the head daughter first, and then the rest of thedaughters as two zero-order Markov processes, onegoing left and one going right from it.
He also man-aged to adapt essentially the standard SCFG pars-ing scheme to his model, thus allowing polynomialprocessing time.
It is interesting to note that al-though the conditioning of the probabilities are top-down, parsing is performed bottom-up, just as isthe case with SCPGs.
This allows him to condi-tion his probabilities on the word string dominatedby the constituent, which he does in terms of a dis-tance between the head constituent and the currentone being generated.
This in turn makes it possibleto let phrase-boundary indicators uch as punctua-tion marks influence the probabilities, and gives themodel the chance to infer preferences for, e.g., rightassociation.In addition to this, Collins incorporated the no-tion of lexical complements and wh-movement ~ laGeneralized Phrase-Structure Grammar (GPSG) (8)into his probabilistic language model.
The former isdone by knocking off complements from a hypoth-esised complement list as the Markov chain of thesiblings of the head constituent are generated.
Thelatter is achieved by adding hypothesised NP gapsto these lists, requiring that they be either matchedagainst an NP on the complement list, or passed onto one of the sibling constituents or the head con-stituent itself, thus mimicking the behavior of the"slash feature" used in GPSG.
The model earns theprobabilities for these rather sophisticated deriva-tion actions under various conditionings.
Not badfor something that started out as a simple SCFG!6 A Non-Derivational ModelThe Constraint Grammar framework (9) introducedby Fred Karlsson and championed by Atro Vouti-lainen is a grammar formalism without derivations.It's not even constructive, but actually rather de-structive.
In fact, most of it is concerned with de-stroying hypotheses.
Of course, you first have tohave some hypotheses if you are going to destroythem, so there are a few components whose task itis to generate hypotheses.
The first one is a lexicon,which assigns a set of possible morphological read-Linguistic TheoryillaaIaaIelHImmmmmiiHimiiiTreebank".
Computational Linguistics 19(2), pp.313-330.
ACL.\[12\] Fernando Pereira.
1985.
"A New Character-ization of Attachment Preferences".
In NaturalLanguage Parsing, pp.
307-319.
Cambridge Uni-versity Press.\[13\] Christer Samuelsson and Atro Voutilainen.1997.
"Comparing a Linguistic and a Stochas-tic Tagger".
In Procs.
Joint 85th Annual Meet-ing of the Association for Computational Linguis-tics and 8th Conference of the European Chapterof the Association for Computational Linguistics,pp.
246-253.
ACL.\[14\] Christer Samuelsson, Pasi Tapanainen andAtro Voutilainen.
1996.
"Inducing ConstraintGrammars".
In Grammatical Inference: Learn-ing Syntaz from Sentences, pp.
146-155, SpringerVerlag.\[15\] Khalil Sima'an.
1996.
"Computational Corn-plexity of Probabilistic Disambiguations bymeansof Tree-Grammars".
In Procs.
16th InternationalConference on Computational Linguistics, at thevery end.
ICCL.\[16\] Pasi Tapanainen.
1996.
The Constraint Gram-mar Parser CG-~.
Publ.
27, Dept.
General Lin-guistics, University of Helsinki.\[17\] David H. Younger 1967.
"Recognition andParsing of Context-Free Languages in Time n 3".In Information and Control 10(2), pp.
189-208.Samuebson 89 Linguistic TheorymmII////EI i/Ui
