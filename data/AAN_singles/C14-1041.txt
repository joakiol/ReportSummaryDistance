Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 421?432, Dublin, Ireland, August 23-29 2014.Investigating the Usefulness of Generalized Word Representations in SMTNadir DurraniUniversity of Edinburghdnadir@inf.ed.ac.ukHelmut Schmid Alexander FraserLudwig Maximilian University Munichfraser,schmid@cis.uni-muenchen.dePhilipp KoehnUniversity of Edinburghpkoehn@inf.ed.ac.ukAbstractWe investigate the use of generalized representations (POS, morphological analysis and wordclusters) in phrase-based models and the N-gram-based Operation Sequence Model (OSM).
Ourintegration enables these models to learn richer lexical and reordering patterns, consider widercontextual information and generalize better in sparse data conditions.
When interpolating gen-eralized OSM models on the standard IWSLT and WMT tasks we observed improvements of upto +1.35 on the English-to-German task and +0.63 for the German-to-English task.
Using auto-matically generated word classes in standard phrase-based models and the OSM models yieldsan average improvement of +0.80 across 8 language pairs on the IWSLT shared task.1 IntroductionThe increasing availability of digital text has galvanized the use of empirical methods in many fieldsincluding Machine Translation.
Given bilingual text, it is now possible to automatically learn translationrules that required years of effort previously.
Bilingual data, however, is abundantly available for only ahandful of language pairs.
The problem of reliably estimating statistical models for translation becomesmore of a challenge under sparse data conditions especially when translating into morphologically richor syntactically divergent languages.
The former becomes challenging due to lexical sparsity and thelatter suffers from sparsity in learning underlying reordering patterns.
The last decade of research inStatistical Machine Translation has witnessed many attempts to integrate linguistic analysis into SMTmodels, to address the challenges of (i) translating into morphologically rich language languages, (ii)modeling syntactic divergence across languages for better generalization in sparse data conditions.The integration of the Operation Sequence Model into phrase-based paradigm (Durrani et al., 2013a;Durrani et al., 2013b) improved the reordering capability and addressed the problem of the phrasal inde-pendence assumption in the phrase-based models.
The OSM model integrates translation and reorderinginto a single generative story.
By jointly considering translation and reordering context across phrasalboundaries, the OSM model considers much richer conditioning than phrasal translation and lexicalizedreordering models.
However, due to data sparsity the model often falls back to very small context sizes.We address this problem by learning operation sequences over generalized representations such as POSand Morph tags.
This enables us to learn richer translation and reordering patterns that can general-ize better in sparse data conditions.
The model benefits from wider contextual information as we showempirically in our results.We investigate two methods to combine generalized OSM models with the lexically driven OSMmodel and experimented on German-English translation tasks.
Our best system that uses a linear combi-nation of different OSM models gives significant improvements over a competitive baseline system.
Animprovement of up to +1.35 was observed on the English-to-German and up to +0.63 BLEU points onthe German-to-English task over a factored augmented baseline system (Koehn and Hoang, 2007).POS taggers and morphological analyzers, however, are not available for many resource poor lan-guages.
In the second half of the paper we investigate whether annotating the data with automatic wordThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/421clusters helps improve the performance.
Word clustering is similar to POS-tagging/Morphological anno-tation except that it also captures interesting syntactic and lexical semantics, for example countries andlanguages are grouped in separate clusters, animate objects are differentiated from inanimate objects,colors are grouped in a separate cluster etc.
Word clusters, however, deterministically map each wordtype to a unique1cluster, unlike POS/Morph tagging, and therefore might be less useful for disambigua-tion.
We use the mkcls utility in GIZA (Och and Ney, 2003) to cluster source and target vocabulariesinto classes and will therefore refer to automatic classes as Och clusters/classes in this paper.We first use Och classes as an additional factor in phrase-based translation model, along with a targetLM model over cluster-ids to improve the baseline system.
We then additionally use the OSM modelover cluster-ids.
Our experiments include translation from English to Dutch, French, Italian, Polish,Portuguese, Russian, Spanish, Slovenian and Turkish on IWSLT shared task data.
Our results show anaverage improvement of +0.80, ranging from +0.41 to +2.02.
Compared to the improved baseline systemobtained by using Och classes as a factor in phrase-based translation models, adding an OSM model overcluster-ids improved performance in four (French, Spanish, Dutch and Slovenian) out of eight cases.
Inother cases performance stayed constant or dropped slightly.
We also used POS annotations for threetasks, namely translating from English into French, Spanish and Dutch to compare the performance ofthe two different kinds of generalizations.
Surprisingly, using Och classes always performed better thanusing POS annotations.
The rest of the paper is organized as follows.
Section 2 gives an account onrelated work.
Section 3 discusses the factor-based OSM model.
Section 4 presents the experimentalsetup and the results.
Section 5 concludes the paper.2 Related WorkPrevious work on integrating linguistic knowledge into SMT models can be broken into two groups.
Thefirst group focuses on using linguistic knowledge to improve reordering between syntactically differentlanguages.
A second group focuses on translating into morphologically rich languages.Initial efforts to use linguistic annotation focused on rearranging source sentences to be in the targetorder.
Xia and McCord (2004) proposed a method to automatically learn rewrite rules to preorder sourcesentences.
Collins et al.
(2005) and Popovi?c and Ney (2006) proposed methods for reordering the sourceusing a small set of handcrafted rules.
Crego and Mari?no (2007) use syntactic trees to derive rewriterules.
Hoang and Koehn (2009) used POS tags to create templates for surface word translation to createlonger phrase translation.
A whole new paradigm of using syntactic annotation to address long rangereorderings has emerged following Galley et al.
(2006), Zollmann and Venugopal (2006), Chiang (2007)etc.
Crego and Yvon (2010) and Niehues et al.
(2011) used a Tuple Sequence Model (TSM) over POStags in an N-gram-based search to improve mid-range reorderings.
Our work is similar to them exceptthat OSM model is substantially different from the TSM model as it integrates both the translation andreordering mechanisms into a combined model.
Therefore both translation and reordering decisions canbenefit from richer generalized representations.A second group of work addresses the problem of translating into morphologically richer languages.The idea of translating to stems and then inflecting the stems in a separate step has been studied byToutanova et al.
(2008), de Gispert and Mari?no (2008), Fraser et al.
(2012), Chahuneau et al.
(2013) andothers.
Koehn and Hoang (2007) proposed to integrate different levels of linguistic information as factorsinto the phrase-based translation model.
Yeniterzi and Oflazer (2010) used source syntactic structures asadditional complex tag factors for English-to-Turkish phrase-based machine translation.
Green and DeN-ero (2012) proposed a target-side, class-based agreement model to handle morpho-syntactic agreementerrors when translating from English-to-Arabic.
El Kholy and Habash (2012) tested three models to findout which features are best handled by modeling them as a part of translation, and which ones are betterpredicted through generation, also in the English-to-Arabic task.
Several researchers attempted to useword lattices to handle generalized representation (Dyer et al., 2008; Hardmeier et al., 2010; Wuebkerand Ney, 2012).
Automatically clustering the training data into word classes in order to obtain smoother1We are referring to hard clustering here.
Soft clustering is intractable as it requires a marginalization over all possibleclasses when calculating the n-gram probabilities.422Figure 1: Operation Sequence Model ?
Training Sentence with Generation and Test Sentencesdistributions and better generalizations has been a widely known and applied technique in natural lan-guage processing.
Training based on word classes has been previously explored by various researchers.Cherry (2013) addressed data sparsity in lexicalized reordering models by using sparse features basedon word classes.
Other parallel attempts on using word-class models include Wuebker et al.
(2013),Chahuneau et al.
(2013) and Bisazza and Monz (2014).More recent research has started to set apart from the conventional maximum likelihood estimatestoward neural network-based models that use continuous space representation (Schwenk, 2012; Le et al.,2012; Hu et al., 2014; Gao et al., 2014).
Although these methods have achieved impressive improve-ments, traditional models continue to dominate the field due to their simplicity and low computationalcomplexity.
How much of the improvement will be retained when scaling these models to all availabledata instead of a limited amount will be interesting.3 Operation Sequence ModelThe Operation Sequence Model (Durrani et al., 2011) is an instance of the N-gram based SMT framework(Casacuberta and Vidal, 2004; Mari?no et al., 2006).
It represents the translation process through asequence of operations.
An operation can be to simultaneously generate source or target words or toperform reordering.
Reordering is carried out through jump and gap operations.
The model is differentfrom its ancestors in that it strongly integrates translation and reordering into a single generative story inwhich translation decisions can influence and get impacted by the reordering decisions and vice versa.Given a bilingual sentence pair < F,E > and its alignment A, a sequence of operations o1, o2.
.
.
, oJis generated deterministically through a conversion algorithm.
The model is learned by learning Markovchains over these sequences and is formally defined as:posm(F,E,A) =J?j=1p(oj|oj?n+1, ..., oj?1)Figure 1 shows an example of an aligned bilingual sentence pair and the corresponding operation se-quence used to generate it.
There is a 1-1 correspondence between a sentence pair and its operationsequence.
We thus get a unique sequence for every bilingual sentence pair given the alignment.3.1 MotivationDue to data sparsity it is impossible to observe all possible reordering patterns with all possible lexicalchoices in translation operations.
The lexically driven OSM model therefore often backs off to verysmall context sizes.
Coming back to the training example in Figure 1.
The useful reordering pattern423learned through this example is:Ich kann umstellen?
I can rearrangewhich is memorized through the operation sequence:Generate(Ich, I) ?
Generate(kann, can) ?
Insert Gap ?
Generate(umstellen, rearrange)It can generalize to the test sentence shown in Figure 1(a).
However, it fails to generalize to the sentencesin Figure 1(b) and (c) although the underlying reordering pattern is the same.
The second part of theGerman verb complex usually appears at the end of a clause or a sentence and needs to be moved in orderto produce the correct English word order.
However, due to data sparsity such a combination of lexicaldecisions and reordering decisions may not be observed during training.
The model would therefore failto generalize in such circumstances.
This problem can be addressed by learning a generalized form ofthe same reordering rule.
By annotating the corpus with word classes such as POS tags, we obtain thereordering pattern:PPER VMFIN VVINF?
PP MD VBmemorized through the operation sequence:Generate (PPER,PP) ?
Generate (VMFIN,MD) ?
Insert Gap ?
Generate (VVINF,VB)This rule generalizes to all the test sentences in Figure 1.
Since the OSM model strongly couplestranslation and reordering, the probability of each translation or reordering operation depends on then previous translation/reordering decisions.
The generalization of the model by replacing words withPOS tags allows the model to consider a wider syntactic context, thus improving lexical decisions andthe reordering capability of the model.
Using different kinds of word classes, we can also control thetype of abstraction.
Using lemmas for example, we can map different forms of the verb ?k?onnen ?
can?
(kann, kannst, konnte) to a single class.
Och clusters can provide different levels of granularity.3.2 ModelsGiven that we can learn OSM models over different word representations, the question then is howto combine the lexically driven OSM model with an OSM model based on a generalized word repre-sentation.
The simplest approach is to treat each OSM model as a separate feature in the log-linearframework, thus summing up the weighted log probabilities.
The effect of this is similar to an Andoperation.
A translation is considered good if both, the word-based OSM and the POS-based OSMmodels indicate that it is a good translation.
However, an Or operation might be more desirable insome scenarios.
The operation Generate (trotz, in spite of) should be ranked high although the POS-based operation Generate(APPR, IN IN IN) is improbable.
Similarly, the generalized operation sequence:Insert Gap ?
Generate (ADJ, JJ) ?
Jump Back ?
Generate (NOM, NN)that captures the swapping of noun and adjective in French-English, should be ranked highereven though noir (black) never appeared after cheval (horse) during training and the sequence:Insert Gap ?
Generate (noir, black) ?
Jump Back ?
Generate (cheval , horse)is never observed.
Instead of using both the models, a single model that could switch betweendifferent generalized OSMs during translation and choose the one which gives the best predictionin each situation, can be used.
In order to achieve this effect, we formulated a second model thatinterpolates the lexically driven OSM model with its generalized variants.
However, we can only424interpolate two models that predict the same representation.
The lexically driven OSM predicts thesurface forms whereas the POS-based OSM predicts POS translations.
To make the two comparable,we multiply the POS-based OSM probability with the probability of the lexical operation given the POSoperation.
More specifically the probability of the generalized model gm can be defined as:pgm(oj|oj?1j?n+1) = posmpos(o?j|o?j?1j?n+1) p(oj|o?j) (1)where posmposis the operation sequence model learned over POS tags and p(oj|o?j) is the probability ofthe lexical operation given the POS-based operation.
It is 1 for all reordering operations.
We assume herethat for each lexical operation oja corresponding POS-based operation o?jis uniquely determined.
Withposmsur= posmsur(oj|oj?1j?n+1) (lexically driven OSM model) and pgm= pgm(oj|oj?1j?n+1) (generalizedOSM model as described above), the overall probability of the new model posmis defined as:posm= ?posmsur+ (1?
?
)pgm(2)Such an interpolation is expensive in the discriminative training.
It would require a sub-tuning routineinside of tuning, a main loop to train all the features including the OSM model and an inner loop todistribute the weight assigned to OSM model among lexically driven and POS-based OSM models.
Wetherefore just take the larger one of the two model values and add a POS-based translation penalty ?.
Thevalue of this penalty is the number of times that the POS-based operation was chosen when translatinga sentence.
This penalty acts similarly as the prior ?
above.
Using this formulation, the model couldtherefore be redefined as:posm={posmsurif posmsur?
e?pgme?pgmotherwise(3)where ?
is the weight for the POS driven translation penalty ?.
This allows the optimizer to controlwhether it prefers the lexically driven or the POS-driven OSM model.
By setting a very low weight ?the optimizer can force the translator to always choose lexically driven OSM.
This formulation can beextended to multiple generalized OSM models based on e.g.
POS tags, morphological tags, or wordclusters.
Equation 2 can be rewritten as follows:posm= ?1posmsur+n?i=2?ipgmi(4)with?ni=1?i= 1 and pgmidefined analogous to Equation 1.Setting pgm1= posmsurand ?1= 0, we can again simplify Equation 4 by taking the maximum to:posm=nmaxi=1e?ipgmi(5)We use a translation penalty ?ifor each generalized model and tune its weight ?ialong with the weightsof other features.
We will refer to this model as Modelorin this paper and the commonly used log-linear interpolation of the features as Modeland.
The intuition behind Modeloris that we back-offto generalized representations only when the lexically driven model doesn?t provide enough contextualevidence.
The downside of this approach, however, is that unlike Modeland, it cannot distribute weightsover multiple features and solely relies on a single model.4 EvaluationData: We ran experiments with data made available for the translation task of the IWSLT-13 (Cettolo etal., 2013): International Workshop on Spoken Language Translation2and WMT-13 (Bojar et al., 2013):Eighth Workshop on Statistical Machine Translation.3The sizes of bitext used for the estimation oftranslation and monolingual language models are reported in Table 1.We used LoPar (Schmid, 2000) to obtain morphological analysis and POS annotation of German andMXPOST (Ratnaparkhi, 1998), a maximum entropy model for English POS tags.
For other languagepairs we used TreeTagger (Schmid, 1994).2http://www.iwslt2013.org/3http://www.statmt.org/wmt13/425Pair Parallel Monolingual Pair Parallel Monolingual Pair Parallel Monolingualde?en ?4.6 M ?287.3 M en?de ?4.6 M ?59.5 M en?fr ?5.5 M ?69 Men?es ?4.1 M ?59.6 M en?nl ?2.1 M ?21.7 M en?ru ?1.15 M ?21 Men?pt ?1.0 M ?2.3 M en?pl ?0.77 M ?0.8 M en?sl ?0.63 M ?0.65 Men?tr ?0.13 M ?0.14 MTable 1: Number of Sentences (in Millions) used for TrainingModel iwslt10wmt13iwslt10wmt13English-to-German German-to-EnglishBaseline 23.56 20.38 31.46 27.27Mand(pos,pos)23.93?+0.37 20.61 ?+0.23 31.91?+0.45 27.55 ?+0.28Mand(pos,morph)24.62?+1.06 20.88?+0.50 32.09?+0.63 27.62?+0.35Mand(all)24.91?+1.35 20.93?+0.55 32.00?+0.54 27.71?+0.44Mor(pos,pos)23.61 ?+0.05 20.24 ?-0.14 31.55 ?+0.09 27.32 ?+0.05Mor(pos,morph)23.83 ?+0.27 20.44 ?+0.08 31.58 ?+0.12 27.20 ?-0.07Mor(all)23.88 ?+0.32 20.55 ?+0.17 31.40 ?-0.06 27.15 ?-0.12Table 2: Evaluating Generalized OSM Models for German-English pairs ?
Bold: Statistically Significant(Koehn, 2004) w.r.t BaselineBaseline System: We trained a Moses system (Koehn et al., 2007), replicating the settings describedin (Birch et al., 2013) developed for the 2013 Workshop on Spoken Language Translation.
The featuresincluded: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ align-ments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011)used at runtime, a lexically-driven 5-gram operation sequence model (Durrani et al., 2013b) with 4additional supportive features: 2 gap-based penalties, 1 distance-based feature and 1 deletion penalty,lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6,100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), Cube Prun-ing (Huang and Chiang, 2007), with a stack-size of 1000 during tuning and 5000 during test and theno-reordering-over-punctuation heuristic.
We used the compact phrase table representation by Junczys-Dowmunt (2012).
For our German-to-English experiments, we used compound splitting (Koehn andKnight, 2003).
German-to-English and English-to-German baseline systems also used POS and mor-phological target sequence models built on the in-domain subset of the parallel corpus using Kneser-Neysmoothed 7-gram models and as additional factors in phrase translation models (Koehn and Hoang,2007).
We used an unsupervised transliteration model (Durrani et al., 2014) to transliterate OOV wordswhen translating into Russian.Tuning and Test: The systems were tuned on the dev2010 dataset and evaluated on the test2010-2013datasets made available for the IWSLT-13 workshop.
We performed a secondary set of experimentsfor German-English pairs using tuning and test sets made available for the WMT-13 workshop.
Weconcatenated the news-test sets 2008 and 2009 to obtain a large dev-set of 4576 sentences.
Evaluationwas performed on the news-test set 2013 which contains 3000 sentences.
Tuning was performed usingthe k-best batch MIRA algorithm (Cherry and Foster, 2012) with at most 25 iterations.
We use BLEU(Papineni et al., 2002) as a metric to evaluate our results.Results I ?
Using Linguistic Annotation: We trained 5-gram OSM models over different representa-tions and added these to the baseline system.
First we evaluated Modeland(Mand) which uses a MIRAtuned linear combination of different OSM models versus Modelor(Mor) which computes only oneOSM model but allows the generator to switch between different OSM models built on various gener-alized forms.
Table 2 shows results from running experiments on German-English pairs.
We found thatthe simpler model Modelandoutperforms Modelorin all the experiments.
Modelordoes not givesignificant improvements over the baseline system and shows an occasional drop.
This result is contraryto the expectation formulated in Section 3.2.
We speculate that the optimizer faces problems to train thiskind of model, because it cannot take into account that the selected OSM model can change when theweight parameter is modified.
It assumes that the feature stays constant.
In our formulation the same426derivation can occur with different feature scores in different decoding runs and the optimizer is unableto handle this.
Our speculation is based on the observation of ?
?, the weight of feature ?
which allowsthe translator to switch between different OSM models.
The value of ?
?was not stable across differentiterations and different experiments.Modelandconsistently improves the baseline.
Adding an OSM model over [pos, morph] (source:pos,target:morph) combination gave the best results, giving a statistically significant gain of +1.06 on theiwslt10test-set and +0.50 on the wmt13test-set.
Using an OSM model over a [pos,pos] combinationalso showed improvements, however, not as much as using morphological tags.
Morphological tags pro-vide richer information for disambiguation when translating into German.
Note that the baseline systemalso used a target sequence model over morphological tags.
Nevertheless using an OSM [pos,morph]model still gives significant improvements which shows that learning a joint model over source and tar-get units is more fruitful than only considering target-side information.
Using both the models togethergave best results for English-to-German giving a further improvement of +0.29 on the iwslt10task butno real gain on the wmt13task.
Using morphological tags also produced the best results for the German-to-English pair, giving a statistically significant gain of +0.63 on iwslt10and +0.35 on wmt13.
Usingboth the models together did not give any further significant improvements.
The results changed by+0.10 and -0.09 on the wmt13and iwslt10test-sets respectively.Results-II ?
Using Och Classes: In our secondary experiments we tested the effect of using Ochclusters.
The overall goal was to study whether using unsupervised word classes can serve the samepurpose as POS tags and to compare the two methods of annotating the data.
We obtained Och clustersusing the mkcls utility (Och, 1999) in GIZA++ (Och and Ney, 2003).
This is generally run duringthe alignment process where data is divided into 50 classes to estimate IBM Model-4.
Chahuneau etal.
(2013) found mapping data to 600 Och clusters useful, so we used this as well.
We additionallyexperimented with using 200 and 1000 classes.
We integrated Och clusters as additional factors4whentraining the phrase-translation models and used a monolingual n-gram model over cluster-ids built on thetarget-side of the in-domain corpus.
Then we added a 5-gram OSM model over cluster-ids.
We replacesurface forms with their cluster-ids in source and target corpus and convert it to operation sequences,that jointly generate source and target cluster-ids.
We only used Modelandfor these experiments whenadding an OSM model over cluster-ids.B050 200 600 1000 POS 50 200 600 1000 POSTarget Sequence Model over Word Clusters Operation Sequence Model over Word Clustersen?
fr 33.17 33.30 33.40 33.05 33.05 33.14 33.76 33.74 33.58 33.75 33.03en?
es 34.14 34.33 34.58 34.46 33.96 33.91 34.73 34.62 34.60 34.55 34.35en?
nl 26.51 26.67 26.15 26.31 26.47 26.55 26.91 26.52 26.61 26.49 26.62en?
ru 13.12 13.34 13.51 13.53 13.97 ?
13.61 13.66 13.80 13.63 ?en?
sl 17.98 18.67 18.55 17.67 17.97 ?
18.64 18.91 18.17 17.98 ?en?
pt 30.80 31.62 32.21 32.40 32.44 ?
31.77 32.44 32.34 31.90 ?en?
pl 9.74 9.90 10.11 10.05 10.43 ?
10.06 10.19 10.24 10.14 ?en?
tr 7.18 7.43 7.45 7.50 7.50 ?
7.26 7.28 7.51 7.54 ?Table 3: Evaluating Phrase-based and N-gram-based Translation Models over Och ClustersTable 3 shows results from using models based on cluster-ids.
The left side of the table evaluate theuse of adding a target sequence model over cluster-ids using a factored-based translation model.
Resultsimproved consistently in all resource poor languages (pt, pl, tr) giving significant improvements in mostof the cases.
Mixed results were obtained for the pairs with a reasonable amount of parallel data (fr,es, nl), showing an occasional drop in performance.
However, improvements can be found for all thelanguage pairs.4Note that adding cluster-ids in factored models alone has no impact in this scenario, as we are using hard clustering (eachword deterministically maps onto a unique cluster-id).
In a joint source-target factored model which is what we are using, itwill result in an identical distribution as the baseline system.427In the right half of the table we tested whether additionally using an OSM model built over cluster-ids,on top of a phrase-based system that uses cluster-ids as factor and target language model, improves theperformance any further.
Consistent improvements were seen in Spanish and French.
Better systemswere produced in the case of French, Spanish, Dutch and Slovenian.
No improvements were observedfor Turkish and Portuguese whereas the performance got worse in Polish and Russian.Using 50 classes consistently improved the baseline.
Different numbers of clusters provide differentlevels of abstraction and granularity.
We also tried using OSM models over different numbers of clus-ters simultaneously for English-to-Spanish, English-to-French and English-to-Dutch pairs in an effort toexplore whether using different numbers of clusters to classify data provides different information.
Aslight gain was observed for EN-ES as the best system improved from 34.73 to 34.95.
No further gainswere observed for the other two pairs.We also used POS annotation as a factor instead of Och clusters in French, Spanish and Dutch.
Seethe POS columns of Table 3.
Using POS as an additional factor, did not improve over the baselineperformance.
A significant drop was seen in the case of English-to-Spanish.
Using a POS-based OSMon top of the POS-based phrase-model did not help either except for Spanish where results got improvedby +0.44 over its phrase-based variant that used a POS factor.
However, using Och clusters producedbetter results in all three cases.
We speculate that the reason for this result is that Och clusters aremore evenly distributed as compared to POS tags where the distribution is biased toward noun classand secondly Och clusters are optimized for language modeling.
Also each word is deterministicallymapped to a single class but can have multiple POS tags.
The latter thus causes a sparser translationmodel.
Finally Table 4 shows the comparison of results on iwslt11?13by running baseline B0and bestsystems Bxin Tables 3.iwslt11iwslt12iwslt13AvgB0BxB0BxB0BxB0Bx?en?
fr 39.84 40.63 40.50 41.24 ?
?
40.24 40.94 +0.70en?
es 32.89 33.24 26.45 26.81 34.01 34.73 31.12 31.60 +0.48en?
nl 30.01 30.31 26.40 26.72 24.96 25.57 27.12 27.53 +0.41en?
ru 14.93 15.91 13.01 13.53 15.65 16.4 14.53 15.28 +0.75en?
sl ?
?
11.34 12.40 12.85 13.73 12.09 13.10 +1.01en?
pt 31.61 33.62 33.24 34.91 30.83 33.24 31.89 33.92 +2.02en?
pl 12.73 13.13 9.52 10.50 11.30 11.54 11.18 11.72 +0.53en?
tr 7.01 7.42 6.99 7.43 6.21 6.84 6.74 7.23 +0.49Avg 24.15 24.89 20.93 21.69 19.40 20.29 21.49 22.29 +0.80Table 4: Evaluating on Test Sets iwslt11?13?
B0= Baseline System, Bx= Best Systems in Tables 2Analysis: In a post-evaluation analysis we confirmed whether using generalized OSM models actuallyconsider a wider contextual window than its lexically driven variant.
The graph shown in Figure 2 showsaverage context size considered (on top of each set of bars) and percentages of 1-5 gram matches bydifferent OSM models.
The results show that the probability of an operation is conditioned on less than atrigram in the OSM model over surface forms.
In comparison OSM models over POS, morph or cluster-ids consider a window of roughly 4 previous operations thus considering more contextual information.The percentage of 5-gram matches increases from 15.5% to 59.2% using POS-based OSM model andup to 45.6% in morph-based OSM model, the number of unigram matches are decreased from 8.30% toless than 1% in both the models.
Similar observation is made for the OSM models over clusters where5-gram matches improve from 12% to 30% on average, showing the ability of the generalized models touse richer conditioning thus improving the translation quality.We also analyzed what kind of words are clustered together using Och classes and found that clusterscapture both syntax and lexical semantics.
Figure 2 (b) shows several useful clusters to exhibit this.
Wealso saw negative examples where words from different classes are clustered together.
?Boy?, ?Girl?
and?Man?
for example were clustered into a single class but ?Woman?
in another.
Similarly ?Grey?
and?Orange?
were grouped together with animated objects.428Figure 2: (a) Average Size of N-grams Used in Different OSM Models and Percentages of 1-5 GramMatches in Three Language Pairs (b) Different Word Clusters using 50 Classes5 ConclusionIn this paper we investigated the usefulness of integrating word classes in phrase-based models andOperation Sequence N-gram models.
We explored two models of interpolating generalized OSM modelsand tested variations on the standard IWSLT and WMT tasks.
Our results showed that the simpler morecommonly used method of integrating the models in the log-linear framework worked best.
We showedthat by learning OSM models over generalized POS and morphological representations, we were ableto build richer models that outperformed state-of-the-art baseline systems.
Statistically significant gainsof up to +1.35 and +0.63 were observed in English-to-German and German-to-English tasks.
We alsomade use of Och classes as additional factors in phrase translation and language models.
These weretested translating from English to 8 different languages which includes a mixture of morphologicallyrich (French, Spanish and Russian, Dutch, and Turkish) and sparse data (Portuguese, Polish, Slovenianand Turkish) languages.
Our results show that using clusters was helpful in all of the cases.
Usingthe OSM model over word-clusters additionally improved the performance further.
Our results show anaverage improvement of +0.80, ranging from +0.41 to +2.02.
Our EN-FR systems were ranked third (ontst2013) and second (on tst2011-tst2012) in IWSLT-13 translation task following EU-Bridge (Freitag etal., 2013) which used our output for system combination.
The code to train class-based models has beenmade available to the research community via the Moses toolkit.
See Advanced Features5in the MosesDecoder for details.AcknowledgementsWe would like to thank the anonymous reviewers for their helpful feedback and suggestions.
The re-search leading to these results has received funding from the European Union Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreements n?287658 (EU-Bridge) and n?287688 (MateCat).Alexander Fraser was funded by Deutsche Forschungsgemeinschaft grant Models of Morphosyntax forStatistical Machine Translation.
Helmut Schmid was supported by Deutsche Forschungsgemeinschaftgrant SFB 732.
This publication only reflects the authors?
views.ReferencesAlexandra Birch, Nadir Durrani, and Philipp Koehn.
2013.
Edinburgh SLT and MT System Description for theIWSLT 2013 Evaluation.
In Proceedings of the 10th International Workshop on Spoken Language Translation,5http://www.statmt.org/moses/?n=Moses.AdvancedFeatures429pages 40?48, Heidelberg, Germany, December.Arianna Bisazza and Christof Monz.
2014.
Class-Based Language Modeling for Translating into MorphologicallyRich Languages.
In Proceedings of the 25th Annual Conference on Computational Linguistics (COLING),Dublin, Ireland, August.Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, ChristofMonz, Matt Post, Radu Soricut, and Lucia Specia.
2013.
Findings of the 2013 Workshop on Statistical MachineTranslation.
In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.Francisco Casacuberta and Enrique Vidal.
2004.
Machine Translation with Inferred Stochastic Finite-State Trans-ducers.
Computational Linguistics, 30:205?225.Mauro Cettolo, Jan Niehues, Sebastian St?uker, Luisa Bentivogli, and Marcello Federico.
2013.
Report on the 10thIWSLT Evaluation Campaign.
Proceedings of the International Workshop on Spoken Language Translation,Heidelberg, Germany.Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer.
2013.
Translating into Morphologically RichLanguages with Synthetic Phrases.
In Proceedings of the 2013 Conference on Empirical Methods in NaturalLanguage Processing.Colin Cherry and George Foster.
2012.
Batch Tuning Strategies for Statistical Machine Translation.
In Proceed-ings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics:Human Language Technologies, pages 427?436, Montr?eal, Canada, June.
Association for Computational Lin-guistics.Colin Cherry.
2013.
Improved Reordering for Phrase-Based Translation using Sparse Features.
In Proceedings ofthe 2013 Conference of the North American Chapter of the Association for Computational Linguistics: HumanLanguage Technologies, pages 22?31, Atlanta, Georgia, June.
Association for Computational Linguistics.David Chiang.
2007.
Hierarchical Phrase-Based Translation.
Computational Linguistics, 33(2):201?228.Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005.
Clause Restructuring for Statistical Machine Trans-lation.
In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL?05),pages 531?540, Ann Arbor, MI.Josep M. Crego and Jos?e B. Mari?no.
2007.
Syntax-Enhanced N-gram-Based SMT.
In Proceedings of the 11thMachine Translation Summit, MT Summit XI, pages 111?118.Josep M. Crego and Franc?ois Yvon.
2010.
Improving Reordering with Linguistically Informed Bilingual N-Grams.
In Coling 2010: Posters, pages 197?205, Beijing, China, August.
Coling 2010 Organizing Committee.Adri`a de Gispert and Jos?e B. Mari?no.
2008.
On the Impact of Morphology in English to Spanish statistical MT.Speech Communication, 50(11-12):1034?1046.Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011.
A Joint Sequence Translation Model with IntegratedReordering.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies, pages 1045?1054, Portland, Oregon, USA, June.Nadir Durrani, Alexander Fraser, and Helmut Schmid.
2013a.
Model With Minimal Translation Units, But DecodeWith Phrases.
In The 2013 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Atlanta, Georgia, USA, June.
Association for Computational Lin-guistics.Nadir Durrani, Alexander Fraser, Helmut Schmid, Hieu Hoang, and Philipp Koehn.
2013b.
Can Markov ModelsOver Minimal Translation Units Help Phrase-Based SMT?
In Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics, Sofia, Bulgaria, August.
Association for Computational Linguistics.Nadir Durrani, Hassan Sajjad, Hieu Hoang, and Philipp Koehn.
2014.
Integrating an Unsupervised TransliterationModel into Statistical Machine Translation.
In Proceedings of the 15th Conference of the European Chapter ofthe ACL (EACL 2014), Gothenburg, Sweden, April.
Association for Computational Linguistics.Christopher Dyer, Smaranda Muresan, and Philip Resnik.
2008.
Generalizing Word Lattice Translation.
InProceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1012?1020, Columbus, OH, USA.
The Association for Computer Linguistics.Ahmed El Kholy and Nizar Habash.
2012.
Translate, Predict or Generate: Modeling Rich Morphology in Statis-tical Machine Translation.
volume 12.430Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap.
2012.
Modeling Inflection and Word-Formation in SMT.
In Proceedings of the 13th Conference of the European Chapter of the Association forComputational Linguistics, pages 664?674, Avignon, France, April.
Association for Computational Linguistics.Markus Freitag, Stephan Peitz, Joern Wuebker, Hermann Ney, Nadir Durrani, Matthias Huck, Philipp Koehn,Thanh-Le Ha, Jan Niehues, Mohammed Mediani, Teresa Herrmann, Alex Waibel, Nicola Bertoldi, Mauro Cet-tolo, and Marcello Federico.
2013.
EU-BRIDGE MT: Text Translation of Talks in the EU-BRIDGE Project.
InInternational Workshop on Spoken Language Translation, Heidelberg, Germany, December.Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer.2006.
Scalable Inference and Training of Context-Rich Syntactic Translation Models.
In Proceedings ofCOLING-ACL, pages 961?968, Sydney, Australia.
Association for Computational Linguistics.Jianfeng Gao, Xiaodong He, Scott Wen-tau Yih, and Li Deng.
2014.
Learning Continuous Phrase Representationsfor Translation Modeling.
In Proceedings of the Association for Computational Linguistics, Baltimore, MD,USA, June.Spence Green and John DeNero.
2012.
A Class-Based Agreement Model for Generating Accurately InflectedTranslations.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics(Volume 1: Long Papers), pages 146?155, Jeju Island, Korea, July.
Association for Computational Linguistics.Christian Hardmeier, Arianna Bisazza, and Marcello Federico.
2010.
FBK at WMT 2010: Word Lattices for Mor-phological Reduction and Chunk-Based Reordering.
In Proceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 88?92, Uppsala, Sweden, July.
Association for ComputationalLinguistics.Eva Hasler, Barry Haddow, and Philipp Koehn.
2012.
Sparse Lexicalised features and Topic Adaptation forSMT.
In Proceedings of the seventh International Workshop on Spoken Language Translation (IWSLT), pages268?275.Kenneth Heafield.
2011.
KenLM: Faster and Smaller Language Model Queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages 187?197, Edinburgh, Scotland, United Kingdom, July.Hieu Hoang and Philipp Koehn.
2009.
Improving Mid-Range Re-Ordering Using Templates of Factors.
InProceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 372?379, Athens,Greece, March.
Association for Computational Linguistics.Yuening Hu, Michael Auli, Qin Gao, and Jianfeng Gao.
2014.
Minimum Translation Modeling with RecurrentNeural Networks.
In Proceedings of the 14th Conference of the European Chapter of the Association for Com-putational Linguistics, pages 20?29, Gothenburg, Sweden, April.
Association for Computational Linguistics.Liang Huang and David Chiang.
2007.
Forest Rescoring: Faster Decoding with Integrated Language Models.In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 144?151,Prague, Czech Republic, June.
Association for Computational Linguistics.Marcin Junczys-Dowmunt.
2012.
Phrasal Rank-Encoding: Exploiting Phrase Redundancy and TranslationalRelations for Phrase Table Compression.
The Prague Bulletin of Mathematical Linguistics, 98:63?74.Philipp Koehn and Hieu Hoang.
2007.
Factored Translation Models.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Processing and Computational Natural Language Learning(EMNLP-CoNLL), pages 868?876, Prague, Czech Republic, June.
Association for Computational Linguistics.Philipp Koehn and Kevin Knight.
2003.
Empirical methods for compound splitting.
In Proceedings of the 10thConference of the European Chapter of the Association for Computational Linguistics (EACL), pages 187?193,Morristown, NJ.Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: Open source toolkit for statistical machine translation.
In ACL 2007 Demonstrations,Prague, Czech Republic.Philipp Koehn.
2004.
Statistical Significance Tests for Machine Translation Evaluation.
In Dekang Lin and DekaiWu, editors, Proceedings of EMNLP 2004, pages 388?395, Barcelona, Spain, July.Shankar Kumar and William J. Byrne.
2004.
Minimum Bayes-Risk Decoding for Statistical Machine Translation.In HLT-NAACL, pages 169?176.431Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012.
Continuous Space Translation Models with NeuralNetworks.
In Proceedings of the 2012 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies, pages 39?48, Montr?eal, Canada, June.
Association forComputational Linguistics.Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Jos?e A. R. Fonollosa, andMarta R. Costa-juss`a.
2006.
N-gram-Based Machine Translation.
Computational Linguistics, 32(4):527?549.Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex Waibel.
2011.
Wider Context by Using Bilingual Lan-guage Models in Machine Translation.
In Proceedings of the Sixth Workshop on Statistical Machine Translation,pages 198?206, Edinburgh, Scotland, July.
Association for Computational Linguistics.Franz J. Och and Hermann Ney.
2003.
A Systematic Comparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Franz J. Och.
1999.
An Efficient Method for Determining Bilingual Word Classes.
In Processings of EACL, pages71?76, Bergen, Norway.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for Automatic Eval-uation of Machine Translation.
In Proceedings of the 40th Annual Meeting on Association for ComputationalLinguistics, ACL ?02, pages 311?318, Morristown, NJ, USA.Maja Popovi?c and Hermann Ney.
2006.
POS-based Word Reorderings for Statistical Machine Translation.
InInternational Conference on Language Resources and Evaluation, pages 1278?1283, Genoa, Italy.Adwait Ratnaparkhi.
1998.
Maximum Entropy Models for Natural Language Ambiguity Resolution.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA.Helmut Schmid.
1994.
Probabilistic Part-of-Speech Tagging Using Decision Trees.
In International Conferenceon New Methods in Language Processing, pages 44?49, Manchester, UK.Helmut Schmid.
2000.
Lopar: Design and implementation.
Bericht des sonderforschungsbereiches ?sprachtheo-retische grundlagen fr die computerlinguistik?, Institute for Computational Linguistics, University of Stuttgart.Holger Schwenk.
2012.
Continuous Space Translation Models for Phrase-Based Statistical Machine Translation.In Proceedings of COLING 2012: Posters, pages 1071?1080, Mumbai, India, December.
The COLING 2012Organizing Committee.Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008.
Applying Morphology Generation Models toMachine Translation.
In Proceedings of ACL-08: HLT, pages 514?522, Columbus, Ohio, June.
Association forComputational Linguistics.Joern Wuebker and Hermann Ney.
2012.
Phrase Model Training for Statistical Machine Translation with WordLattices of Preprocessing Alternatives.
In NAACL 2012 Seventh Workshop on Statistical Machine Translation,pages 450?459, Montreal, Canada, June.
Association for Computational Linguistics.Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney.
2013.
Improving Statistical Machine Translationwith Word Class Models.
In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing, pages 1377?1381, Seattle, Washington, USA, October.
Association for Computational Linguistics.Fei Xia and Michael McCord.
2004.
Improving a Statistical MT System with Automatically Learned RewritePatterns.
In Proceedings of Coling 2004, pages 508?514, Geneva, Switzerland, Aug 23?Aug 27.
COLING.Reyyan Yeniterzi and Kemal Oflazer.
2010.
Syntax-to-Morphology Mapping in Factored Phrase-Based StatisticalMachine Translation from English to Turkish.
In Proceedings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 454?464, Uppsala, Sweden, July.
Association for Computational Linguistics.Andreas Zollmann and Ashish Venugopal.
2006.
Syntax Augmented Machine Translation via Chart Parsing.In Proceedings on the Workshop on Statistical Machine Translation, pages 138?141, New York City, June.Association for Computational Linguistics.432
