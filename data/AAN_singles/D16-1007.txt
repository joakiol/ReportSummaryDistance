Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 65?74,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Position Encoding Convolutional Neural Network Based on DependencyTree for Relation ClassificationYunlun Yang Yunhai Tong?
Shulei Ma Zhi-Hong Deng?
{incomparable-lun, yhtong, mashulei, zhdeng}@pku.edu.cnKey Laboratory of Machine Perception (Ministry of Education),School of Electronics Engineering and Computer Science, Peking University,Beijing 100871, ChinaAbstractWith the renaissance of neural network in re-cent years, relation classification has againbecome a research hotspot in natural lan-guage processing, and leveraging parse treesis a common and effective method of tack-ling this problem.
In this work, we offer anew perspective on utilizing syntactic infor-mation of dependency parse tree and presenta position encoding convolutional neural net-work (PECNN) based on dependency parsetree for relation classification.
First, tree-based position features are proposed to en-code the relative positions of words in depen-dency trees and help enhance the word repre-sentations.
Then, based on a redefinition of?context?, we design two kinds of tree-basedconvolution kernels for capturing the semanticand structural information provided by depen-dency trees.
Finally, the features extracted byconvolution module are fed to a classifier forlabelling the semantic relations.
Experimentson the benchmark dataset show that PECNNoutperforms state-of-the-art approaches.
Wealso compare the effect of different positionfeatures and visualize the influence of tree-based position feature by tracing back the con-volution process.1 IntroductionRelation classification focuses on classifying the se-mantic relations between pairs of marked entities ingiven sentences (Hendrickx et al, 2010).
It is a fun-damental task which can serve as a pre-existing sys-tem and provide prior knowledge for information ex-?Corresponding authorstraction, natural language understanding, informa-tion retrieval, etc.
However, automatic recognitionof semantic relation is challenging.
Traditional fea-ture based approaches rely heavily on the quantityand quality of hand-crafted features and lexical re-sources, and it is time-consuming to select an op-timal subset of relevant features in order to maxi-mize performance.
Though kernel based methodsget rid of the feature selection process, they needelaborately designed kernels and are also computa-tionally expensive.Recently, with the renaissance of neural network,deep learning techniques have been adopted to pro-vide end-to-end solutions for many classic NLPtasks, such as sentence modeling (Socher, 2014;Kim, 2014) and machine translation (Cho et al,2014).
Recursive Neural Network (RNN) (Socheret al, 2012) and Convolutional Neural Network(CNN) (Zeng et al, 2014) have proven powerfulin relation classification.
In contrast to traditionalapproaches, neural network based methods own theability of automatic feature learning and alleviate theproblem of severe dependence on human-designedfeatures and kernels.However, previous researches (Socher et al,2012) imply that some features exploited by tradi-tional methods are still informative and can help en-hance the performance of neural network in relationclassification.
One simple but effective approach isto concatenate lexical level features to features ex-tracted by neural network and directly pass the com-bined vector to classifier.
In this way, Socher et al(2012), Liu et al (2015) achieve better performanceswhen considering some external features produced65by existing NLP tools.
Another more sophisticatedmethod adjusts the structure of neural network ac-cording to the parse trees of input sentences.
Theresults of (Li et al, 2015) empirically suggest syn-tactic structures from recursive models might offeruseful power in relation classification.
Besides rela-tion classification, parse tree also gives neural net-work a big boost in other NLP tasks (Mou et al,2015; Tai et al, 2015).caused[Convulsions] are byoccur [fever]that after aDTaP[Convulsions] that occur after DTaP are caused by a [fever].Figure 1: A dependency tree example.
Words in square brack-ets are marked entities.
The red dashed-line arrows indicate thepath between two entities.Dependency parse tree is valuable in relation clas-sification task.
According to our observation, depen-dency tree usually shortens the distances betweenpairs of marked entities and helps trim off redundantwords, when comparing with plain text.
For exam-ple, in the sentence shown in Figure 1, two markedentities span the whole sentence, which brings muchnoise to the recognition of their relation.
By con-trast, in the dependency tree corresponding to thesentence, the path between two marked entities com-prises only four words and extracts a key phrase?caused by?
that clearly implies the relation of enti-ties.
This property of dependency tree is ubiquitousand consistent with the Shortest Path Hypothesiswhich is accepted by previous studies (Bunescu andMooney, 2005; Xu et al, 2015a; Xu et al, 2015b).To better utilize the powerful neural network andmake the best of the abundant linguistic knowledgeprovided by parse tree, we propose a position encod-ing convolutional neural network (PECNN) basedon dependency parse tree for relation classification.In our model, to sufficiently benefit from the impor-tant property of dependency tree, we introduce theposition feature and modify it in the context of parsetree.
Tree-based position features encode the rela-tive positions between each word and marked en-tities in a dependency tree, and help the networkpay more attention to the key phrases in sentences.Moreover, with a redefinition of ?context?, we de-sign two kinds of tree-based convolution kernels forcapturing the structural information and salient fea-tures of sentences.To sum up, our contributions are:1) We propose a novel convolutional neural networkwith tree-based convolution kernels for relationclassification.2) We confirm the feasibility of transferring the po-sition feature from plain text to dependency tree,and compare the performances of different posi-tion features by experiments.3) Experimental results on the benchmark datasetshow that our proposed method outperforms thestate-of-the-art approaches.
To make the mech-anism of our model clear, we also visualize theinfluence of tree-based position feature on rela-tion classification task.2 Related WorkRecent studies usually present the task of relationclassification in a supervised perspective, and tra-ditional supervised approaches can be divided intofeature based methods and kernel methods.Feature based methods focus on extracting andselecting relevant feature for relation classifica-tion.
Kambhatla (2004) leverages lexical, syntacticand semantic features, and feeds them to a maxi-mum entropy model.
Hendrickx et al (2010) showthat the winner of SemEval-2010 Task 8 used themost types of features and resources, among all par-ticipants.
Nevertheless, it is difficult to find an opti-mal feature set, since traversing all combinations offeatures is time-consuming for feature based meth-ods.To remedy the problem of feature selection men-tioned above, kernel methods represent the inputdata by computing the structural commonness be-tween sentences, based on carefully designed ker-nels.
Mooney and Bunescu (2005) split sentencesinto subsequences and compute the similarities us-ing the proposed subsequence kernel.
Bunescu and66caused[Convulsions] are by[fever]softmaxword representation: word embedding +tree-based position featureconvolution withtree-based kernels max-poolingfully connected layer +softmax classifierFigure 2: The framework of PECNN.
The red and blue circles represent the word embeddings and tree-based position features ofwords.
The yellow and green circles stand for the feature maps extracted by two kinds of convolution kernels respectively.Mooney (2005) propose a dependency tree kerneland extract information from the Shortest Depen-dency Path (SDP) between marked entities.
Sincekernel methods require similarity computation be-tween input samples, they are relatively computa-tionally expensive when facing large-scale datasets.Nowadays, deep neural network based ap-proaches have become the main solutions to relationclassification.
Among them, some handle this taskby modifying sentence modeling methods.
Socher etal.
(2012) build RNN on constituency trees of sen-tences, and apply the model to relation recognitiontask.
Zeng et al (2014) propose the use of positionfeature for improving the performance of CNN inrelation classification.
dos Santos et al (2015) di-minish the impact of noisy class by using a pairwiseranking loss function based CNN.
Meanwhile, in-spired by the ideas of traditional methods, some re-cent researches concentrate on mining informationfrom the SDP.
Xu et al (2015b) use a multichan-nel LSTM network to model the SDP in given sen-tences.
Liu et al (2015) reserve the subtrees attachedto the SDP and propose an augmented SDP basedCNN.
Neural network based methods offer the ad-vantage of automatic feature learning and also scalewell with large amounts of data.3 Proposed ModelGiven a sentence s with two marked entities e1 ande2, we aim to identify the semantic relation betweene1 and e2 in relation classification.
As the set oftarget relations is predefined, this task can be formu-lated as a multi-class classification problem.
In thissection, we detailedly describe our proposed modeldesigned for this problem.3.1 FrameworkThe schematic illustration of the framework isshown in Figure 2.First, the dependency tree of a sentence is gen-erated by the Stanford Parser (Klein and Manning,2003).
For each word in the tree, its word embed-ding and tree-based position features are concate-nated as its representation.
The position feature of aword is determined by the relative position betweenthe word and marked entities in the dependency tree.Next, with tree-based kernels, convolution opera-tions are conducted on each node of the dependencytree.
Compared with plain text, dependency treecould provide a word with more meaningful con-text, thus making tree-based kernel more effective.After convolution, we apply max-pooling over theextracted feature maps to capture the most importantfeatures.At last, the output of max-pooling layer, i.e.
thefeature vector of input sentence, is fed to a softmaxclassifier for labelling the semantic relation of enti-ties in each sentence.3.2 Word RepresetationThe representation of a word is composed of twoparts: word embedding and tree-based position fea-ture.673.2.1 Word EmbeddingDistributed representation of words in a vectorspace help learning algorithms to achieve better per-formance in NLP tasks (Mikolov et al, 2013).
Suchrepresentation is usually called word embedding inrecent works.
High-quality word embedding is ableto capture precise syntactic and semantic informa-tion by training unsupervisedly on large-scale cor-pora.In our model, we initialize the word embeddingsby pretraining them on a large corpus and furtherfine-tune them in training phase.3.2.2 Tree-based Position FeaturePosition Feature (PF) is first proposed by (Col-lobert et al, 2011) for semantic role labeling.
(Zenget al, 2014) exploit position feature as a substitutefor traditional structure features in relation classifi-cation.
The main idea of position feature is to mapeach discrete distance to a real-valued vector.
It issimilar to word embedding, except that words arereplaced by discrete distances.
For instance, let usexamine again the sentence shown in Figure 1,[Convulsions]e1 that occur after DTaP are caused bya [fever]e2.the relative distances of caused to Convulsions andfever are respectively 6 and ?3.
Each relative dis-tance is further mapped to a dpf (a hyperparameter)dimensional vector, which is randomly initialized.Supposing pf6 and pf?3 are the corresponding vec-tors of distance 6 and ?3, the position feature ofcaused is given by concatenating these two vectors[pf6,pf?3].Position feature on plain text proves to be infor-mative (dos Santos et al, 2015), while it may suf-fer from several problems.
According to our casestudy, adverbs or unrelated entities that appear be-tween two entities in a sentence could significantlyaffect the performance of position feature, as thesewords only change the relative distance to entitieswithout providing any more useful information forrelation classification.
Similarly, position feature of-ten fails to handle sentences in which marked enti-ties are too far from each other.On the other hand, dependency tree focuses on theaction and agents in a sentence (Socher et al, 2014),which is valuable for relation classification.
As wehave mentioned above, dependency tree is able toshorten the distances between pairs of marked enti-ties and help trim off redundant words.
Therefore,it is straightforward and reasonable to transfer theposition feature from plain text to dependency tree.We propose two kinds of Tree-based Position Fea-ture which we refer as TPF1 and TPF2.TPF1 encodes the relative distances of currentword to marked entities in dependency trees.
The?relative distance?
here refers to the length of theshortest dependency path between current word andtarget entity.
The sign of the distance is used to dis-tinguish whether current word is a descendant of tar-get entity.
After calculating the relative distances ofwords in the tree, we can get their TPF1 by mappingrelative distances to corresponding vectors, which isthe same as the PF in plain text.To more precisely describe the position of a word,TPF2 incorporates more information given by de-pendency tree.
TPF2 represents the relative posi-tions between current word and marked entities byencoding their shortest paths.
For a word and an en-tity, the shortest path between them can be separatedby their lowest common ancestor, and the lengthsof the two sub-paths are sufficient for encoding theshortest path and the relative position between theword and the entity.
As a result, we formally rep-resent the relative position using a 2-tuple, in whichtwo elements are the lengths of the two separatedsub-paths respectively.
Thereafter, each unique rel-ative position is mapped to a real-valued vector.caused[Convulsions] are byoccur [fever]that after aDTaP0-1-2 -2-312234(0,2) (0,2)(0,3)(0,1)(0,0) (1,1)(1,0)(1,1)(1,2)(1,3)Figure 3: Example of Tree-based Position Features.
The rednumbers are relative distances in TPF1.
The blue 2-tuples arerelative positions in TPF2.For example, in Figure 3, the path between Con-vulsions and by is Convulsions?
caused?by.
In68TPF1, the relative distance of by to Convulsions is 2,the length of this path.
In TPF2, the lowest commonancestor caused splits the path into two subpaths oflength 1, so the relative position between Convul-sions and by is (1, 1) (encoded in 2-tuple).
More ex-amples of the tree-based position features are shownin Figure 3.TPF1 and TPF2 both offer good strategies for en-coding word position in dependency tree.
TPF2 ismore fine-grained than TPF1 and TPF1 is a simpli-fied version of TPF2.In our model, for each word in dependency trees,its word embedding and tree-based position featureare concatenated to form its representation, which issubsequently fed to the convolutional layer.3.3 Convolution MethodsIn the classic CNN architecture of (Collobert et al,2011) and its variants (Kim, 2014), a convolutionwindow covers a word and its context, i.e.
its neigh-boring words.
Thus convolution only captures localfeatures around each word.
Words that are not in asame window will not interact, even if they are syn-tactically related.Compared with plain text, dependency tree couldprovide a word with more meaningful context.
Ina dependency tree, words are connected if they arein some dependency relationship.
To capitalize onthese syntactic information, we regard the parent andchildren of a word (i.e.
nodes neighboring this word)as its new context.
Changing the definition of ?con-text?
leads to modification of convolution kernel.
Toimplement this idea, we design two kinds of tree-based kernels (Kernel-1 and Kernel-2), and applythem to sentences in dependency tree form.Formally, for a word x in the dependency tree,let p be its parent and c1, ?
?
?
, cn be its n children.Their vector representation are respectively denotedby x, p, c1, ?
?
?
, cn ?
Rd.
The convolution processof Kernel-1 is formulated asz1xi =g(W 1x ?
x+W 1p ?
p+W 1c ?
ci)for i = 1, ?
?
?
, n(1)where z1xi ?
Rn1 and n1 is the number of Kernel-1,and W 1x ,W 1p ,W 1c ?
Rn1?d are weight parameterscorresponding to the word, its parent and childrenrespectively.
g is the non-linear activation function.For leaf nodes which have no child, i.e.
n = 0, weassign each of them a child of which the vector rep-resentation is 0.
For the root node, p is set to be0.Similarly, the output of Kernel-2 is given byz2xi =g(W 2x ?
x+W 2lc ?
ci +W 2rc ?
ci+1)for i = 1, ?
?
?
, n?
1 (2)where z2xi ?
Rn2 and n2 is the number of Kernel-2, and W 2x ,W 2lc,W 2rc ?
Rn2?d are weight parame-ters associated with the word and its two neighbor-ing children.
If n ?
1, we simply add one or two 0children, just like the zero padding strategy.Kernel-1 aims at extracting features from wordsof multiple levels in dependency tree, while Kernel-2 focuses on mining the semantic information be-tween words which share the same parent.
Kernel-1 and Kernel-2 both consider 3 words at a timebecause the experimental results of previous re-searches (Zeng et al, 2014; dos Santos et al, 2015)suggest that trigram features are relatively more use-ful in relation classification.
And it is also straight-forward to extend these kernels to a larger size andapply them to other tasks.After convolution with tree-based kernels, we ap-ply a global max-pooling operation over extractedfeatures by taking the maximum value in each di-mension, which is formulated ash1 = elemaxx,iz1xi (3)h2 = elemaxx,iz2xi (4)where h1 ?
Rn1 , h2 ?
Rn2 , and elemax is the op-eration which gives the element-wise maximum ofall input vectors.
As a consequence, the output ofconvolution process is [h1,h2], the combination offeatures extracted by two kinds of kernels.3.4 Output and Training ObjectiveAfter convolution, the extracted feature is furtherpassed to a fully connected softmax layer whose out-put is the probability distribution over all types ofrelations.69Since we treat the relation classification task as amulti-class classification problem, the training ob-jective is the cross-entropy error.
For regularization,we apply dropout (Srivastava et al, 2014) to the fea-ture vector extracted by convolution and penalize thefully connected layer with l2 regularizer as well.Some other dependency tree based methods like(Liu et al, 2015), (Xu et al, 2015a) and (Xu et al,2015b), all focus on using different kinds of neu-ral networks to model the shortest dependency path(SDP) between entities.
By contrast, PECNN ex-tracts features from the whole dependency tree, sothat the information out of SDP will be taken intoconsideration as well.
The empirical results of (dosSantos et al, 2015) suggest that when position fea-tures exist, modeling the full sentence yields a bet-ter performance than only using the subsentence be-tween entities.
With the help of tree-based positionfeature, our model is capable of evaluating the im-portance of different parts of dependency trees andtends to pay relatively more attention to SDP.Some methods enhancing their performances byproposing dataset-specific strategies.
dos Santos etal.
(2015) treat the class Other as a special class andomit its embedding.
Xu et al (2015a) take the re-lation dimensionality into account and introduce anegative sampling strategy to double the number oftraining samples, which can be regarded as data aug-mentation.
These strategies do not conflict with ourmodel, but we decide not to integrate them into ourmethods as we aim to offer a general and effectivefeature extraction model for relation classification.4 Experiments4.1 Dataset and Evaluation MetricTo evaluate our method, we conduct experiments onthe SemEval2010 Task 8 dataset which is a widelyused benchmark for relation classification.
Thedataset contains 8, 000 training sentences and 2, 717test sentences.
In each sentence, two entities aremarked as target entities.The predefined target relations include 9 directedrelations and an undirected Other class.
The 9directed relations are Cause-Effect, Component-Whole, Content-Container, Entity- Destination,Entity-Origin, Instrument-Agency, Member-Collection, Message-Topic and Product-Producer.?Directed?
here means, for example, Cause-Effect(e1, e2) and Cause-Effect(e2, e1) are twodifferent relations.
In another word, the direction-ality of relation also matters.
And sentences thatdo not belong to any directed relation are labelledas Other.
Therefore, relation classification on thisdataset is a 19-class classification problem.Following previous studies, we use the officialevaluation metric, macro-averaged F1-score with di-rectionality taken into account and the Other classignored.4.2 Training DetailsSince there is no official validation set, 10% of thetraining sentences are taken out for hyperparametertuning and early stopping.When converting sentences to dependency trees,we note that some prepositions such as ?by?, ?in?and ?of?, might be important clues to relation clas-sification.
To reserve these valuable information, weuse the Stanford Parser without the collapsed op-tion.In the dataset, there are some entities consisting ofmultiple words, which make the calculation of rela-tive position ambiguous.
To solve this problem, wetake the last word as the representation of an entity,as the last word is usually the predominant word.For word embeddings, we initialize them usingthe 300-dimensional word2vec vectors1.
The vec-tors are trained on 100 billion words from GoogleNews.
Words not present in the word2vec vectorsare initialized by sampling each dimension from auniform distribution (Kim, 2014).
Tree-based posi-tion features are 50-dimensional and initialized ran-domly.
Therefore the representation of each wordhas dimensionality of 400.We use ReLU as the activation function.
Thenumber of convolution kernels is 500 for each kind,1, 000 in total.
The dropout rate is 0.5, and the co-efficient of l2 penalty of fully connected layer is setto 10?6.
These parameters are selected through gridsearch on validation set.
The network is trained withthe Adadelta update rule (Zeiler, 2012).
The net-work is implemented with Theano (Theano Devel-opment Team, 2016).1https://code.google.com/p/word2vec/70Classifier Features F1Without External Lexical FeaturesMVRNN word embedding, constituency tree 79.1CNN word embedding, position feature 78.9CR-CNN word embedding 82.8?word embedding, position feature 84.1?depLCNN word embedding, dependency tree 81.9word embedding, dependency tree 84.0?SDP-LSTM word embedding, dependency tree 83.0PECNN word embedding, dependency tree, tree-based position feature 84.0With External Lexical FeaturesSVMPOS, prefixes, morphological, WordNet, dependency parse82.2Levin classes, PropBankFrameNet, NomLex-Plus, Google n-gramparaphrases, TextRunnerMVRNN word embedding, constituency tree, POS, NER, WordNet 82.4CNN word embedding, position feature, WordNet 82.7DepNN word embedding, dependency tree, WordNet 83.0word embedding, dependency tree, NER 83.6depLCNN word embedding, dependency tree, WordNet 83.7word embedding, dependency tree, WordNet 85.6?SDP-LSTM word embedding, dependency tree, POS embedding 83.7WordNet embedding, grammar relation embeddingPECNN word embedding, dependency tree, tree-based position feature, POS 84.6NER, WordNetTable 1: Comparison of different relation classification models.
The symbol ?
indicates the results with special treatment of theclass Other.
The symbol ?
indicates the results with data augmentation strategy.4.3 ResultsThe performances of our proposed model and otherstate-of-the-art methods are shown in Table 1.First, we compare PECNN with the followingbaselines when no external lexical feature is used.Socher et al (2012) assign a vector and a matrixto each word for the purpose of semantic composi-tion, and build recursive neural network along con-stituency tree (MVRNN).
It is noteworthy that thiswork is the first one who confirms the feasibility ofapplying neural network to relation classification.Following the ideas of (Collobert et al, 2011),Zeng et al (2014) first solve relation classifica-tion using convolutional neural network (CNN).
Theposition feature introduced by them proves effec-tive.
dos Santos et al (2015) build a similar CNNcalled CR-CNN but replace the objective functionwith a pairwise ranking loss.
By treating the noisyclass Other as a special class, this method achievesan F1 of 84.1.
The F1 score is 82.7 if no specialtreatment is applied.The rest two baselines focus on modeling theShortest Dependency Paths (SDP) between markedentities.
Xu et al (2015a)) (depLCNN) integrate therelation directionality into CNN and achieve an F1of 84.0 with a data augmentation strategy callednegative sampling.
Without such data augmenta-tion, their F1 score is 81.9.
Xu et al (2015b) (SDP-LSTM) represent heterogeneous features as embed-dings and propose a multichannel LSTM based re-current neural network for picking up informationalong the SDP.
Their F1 score is 83.0 when onlyword embedding is used as the word representation.Without considering any external lexical featureand dataset-specific strategy, our model achieve anF1 of 84.0, suggesting that tree-based position fea-tures and kernels are effective.
Comparing with theCNN based on plain text, our model benefits fromdependency tree based network and obtain a notable71[Convulsions] that occur after DTaP are caused by a [fever]Word0.000.050.100.150.200.25ProportionNo Postion FeatureTree-based Position FeatureFigure 4: Visualization of the effect of tree-based position feature.
The proportions of words change with the use of tree-basedposition feature.improvement.When external lexical features are available, wetake two more baselines into account.
The first one(SVM) is a typical example of traditional feature-based methods which rely largely on hand-craftedfeatures.
Benefitting from various features and re-sources, this method won the SemEval 2010 Task 8by a large margin (Hendrickx et al, 2010).
Liu et al(2015) (DepNN) reserve the subtrees attached to theSDP and propose an augmented SDP based CNN.Most of these baselines concatenate external lex-ical features to features extracted by neural networkand directly pass the combined vector to classifier.SDP-LSTM represents lexical features as embed-dings and enhances its word representation.
For faircomparison, we add three features (POS tags, NERtags and WordNet hypernyms of marked entities) tothe vector extracted by our model and retrain the net-work.
Thus, our model achieves an F1 of 84.6 andoutperforms all existing baselines in a fair conditionwhere no data augmentation strategy is adopted.
Theenhancement we gain from external features is less,comparing with other baselines.
This implies thatour model is able to mine useful features from lim-ited resources, even no extra information is avail-able.4.4 Effect of Different Position FeaturesPosition Feature F1plain text PF 83.21TPF1 83.99TPF2 83.90Table 2: Comparison of different position features.Table 2 summarizes the performances of proposedmodel when different position features are exploited.To concentrate on studying the effect of position fea-tures, we do not involve lexical features in this sec-tion.
As the table shows, the position feature onplain text is still effective in our model and we ac-credit its satisfactory result to the dependency in-formation and tree-based kernels.
The F1 scores oftree-based position features are higher since they are?specially designed?
for our model.Contrary to our expectation, the more fine-grainedTPF2 does not yield a better performance thanTPF1, and two kinds of TPF give fairly close results.One possible reason is that the influence of a moreelaborated definition of relative position is minimal.As most sentences in this dataset are of short lengthand their dependency trees are not so complicated,replacing TPF1 with TPF2 usually brings little newstructural information and thus results in a similarF1 score.However, though the performances of differentposition features are close, tree-based position fea-ture is an essential part of our model.
The F1 scoreis severely reduced to 75.22 when we remove thetree-based position feature in PECNN.4.5 Effect of Tree-based Position FeatureFor shallow CNN in NLP, visualization offers clearand convincing explanations for the mechanism ofneural networks (dos Santos and Gatti, 2014; Mouet al, 2015).
Moreover, it is easy to implement.Note that in the max-pooling step, for each ker-nel, we select the feature which has the largest value.This feature corresponds to 3 words in the convolu-72tion step, and we regard them as the most relevantwords extracted by this kernel, with respect to thesentence .
Since there are 1, 000 kernels in total, wecount 3, 000 words (0 will be ignored) and calculatethe proportion of each different word.
Intuitively,the more important a word is in this task, the largerits proportion will be.In Figure 4, we compare the proportions of wordsin the example sentence when tree-based positionfeature (TPF) is used and not.
As we can see, theproportions of two entities, Convulsions and fever,and the phrase caused by all increase visibly withthe presence of TPF, suggesting that TPF is effec-tive in helping the neural network pay more atten-tion to the crucial words and phrases in a sentence.The word occur is also picked up by our model sinceit is an important candidate clue to relation classifi-cation.
Meanwhile, the influence of irrelevant entityDTaP is remarkably diminished as expected.5 ConclusionThis work presents a dependency parse tree basedconvolutional neural network for relation classifica-tion.
We propose tree-based position features to en-code the relative positions of words in a dependencytree.
Meanwhile, tree-based convolution kernels aredesigned to gather semantic and syntactic informa-tion in dependency trees.
Experimental results provethe effectiveness of our model.
Comparing withplain text based CNN, our proposed kernels and po-sition features boost the performance of network byutilizing dependency trees in a new perspective.6 AcknowledgementsThis work is partially supported by the NationalHigh Technology Research and Development Pro-gram of China (Grant No.
2015AA015403) andthe National Natural Science Foundation of China(Grant No.
61170091).
We would also like tothank the anonymous reviewers for their helpfulcomments.ReferencesRazvan C Bunescu and Raymond J Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In Proceedings of the conference on Human Lan-guage Technology and Empirical Methods in NaturalLanguage Processing, pages 724?731.
Association forComputational Linguistics.Kyunghyun Cho, Bart van Merrienboer, C?aglar Gu?lc?ehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phrase represen-tations using RNN encoder-decoder for statistical ma-chine translation.
In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP 2014, October 25-29, 2014, Doha,Qatar, A meeting of SIGDAT, a Special Interest Groupof the ACL, pages 1724?1734.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.The Journal of Machine Learning Research, 12:2493?2537.C?cero Nogueira dos Santos and Ma?ra Gatti.
2014.
Deepconvolutional neural networks for sentiment analysisof short texts.
In Proceedings of the 25th InternationalConference on Computational Linguistics (COLING),Dublin, Ireland.C?
?cero Nogueira dos Santos, Bing Xiang, and BowenZhou.
2015.
Classifying relations by ranking withconvolutional neural networks.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7th International JointConference on Natural Language Processing of theAsian Federation of Natural Language Processing,ACL 2015, July 26-31, 2015, Beijing, China, Volume1: Long Papers, pages 626?634.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, PreslavNakov, Diarmuid O?
Se?aghdha, Sebastian Pado?, MarcoPennacchiotti, Lorenza Romano, and Stan Szpakow-icz.
2010.
Semeval-2010 task 8: Multi-way classi-fication of semantic relations between pairs of nom-inals.
In Proceedings of the 5th International Work-shop on Semantic Evaluation, pages 33?38.
Associa-tion for Computational Linguistics.Nanda Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy mod-els for extracting relations.
In Proceedings of theACL 2004 on Interactive poster and demonstrationsessions, page 22.
Association for Computational Lin-guistics.Yoon Kim.
2014.
Convolutional neural networksfor sentence classification.
In Proceedings of the2014 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2014, October 25-29,2014, Doha, Qatar, A meeting of SIGDAT, a SpecialInterest Group of the ACL, pages 1746?1751.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-73tional Linguistics-Volume 1, pages 423?430.
Associ-ation for Computational Linguistics.Jiwei Li, Thang Luong, Dan Jurafsky, and Eduard H.Hovy.
2015.
When are tree structures necessary fordeep learning of representations?
In Proceedings ofthe 2015 Conference on Empirical Methods in NaturalLanguage Processing, EMNLP 2015, Lisbon, Portu-gal, September 17-21, 2015, pages 2304?2314.Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,and Houfeng Wang.
2015.
A dependency-based neu-ral network for relation classification.
In Proceed-ings of the 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing ofthe Asian Federation of Natural Language Processing,ACL 2015, July 26-31, 2015, Beijing, China, Volume2: Short Papers, pages 285?290.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InC.J.C.
Burges, L. Bottou, M. Welling, Z. Ghahramani,and K.Q.
Weinberger, editors, Advances in Neural In-formation Processing Systems 26, pages 3111?3119.Curran Associates, Inc.Raymond J Mooney and Razvan C Bunescu.
2005.
Sub-sequence kernels for relation extraction.
In Advancesin neural information processing systems, pages 171?178.Lili Mou, Hao Peng, Ge Li, Yan Xu, Lu Zhang, andZhi Jin.
2015.
Discriminative neural sentence mod-eling by tree-based convolution.
In Proceedings ofthe 2015 Conference on Empirical Methods in Natu-ral Language Processing, EMNLP 2015, Lisbon, Por-tugal, September 17-21, 2015, pages 2315?2325.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 1201?1211.
Association for Computational Linguistics.Richard Socher, Andrej Karpathy, Quoc V Le, Christo-pher D Manning, and Andrew Y Ng.
2014.
Groundedcompositional semantics for finding and describingimages with sentences.
Transactions of the Associa-tion for Computational Linguistics, 2:207?218.Richard Socher.
2014.
Recursive Deep Learning forNatural Language Processing and Computer Vision.Ph.D.
thesis, Stanford University.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
The Journal of Machine LearningResearch, 15(1):1929?1958.Kai Sheng Tai, Richard Socher, and Christopher D. Man-ning.
2015.
Improved semantic representations fromtree-structured long short-term memory networks.
InProceedings of the 53rd Annual Meeting of the As-sociation for Computational Linguistics and the 7thInternational Joint Conference on Natural LanguageProcessing of the Asian Federation of Natural Lan-guage Processing, ACL 2015, July 26-31, 2015, Bei-jing, China, Volume 1: Long Papers, pages 1556?1566.Theano Development Team.
2016.
Theano: A Pythonframework for fast computation of mathematical ex-pressions.
arXiv e-prints, abs/1605.02688, May.Kun Xu, Yansong Feng, Songfang Huang, and DongyanZhao.
2015a.
Semantic relation classification viaconvolutional neural networks with simple negativesampling.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 536?540.Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,and Zhi Jin.
2015b.
Classifying relations via longshort term memory networks along shortest depen-dency paths.
In Proceedings of the 2015 Conferenceon Empirical Methods in Natural Language Process-ing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, pages 1785?1794.Matthew D. Zeiler.
2012.
ADADELTA: an adaptivelearning rate method.
CoRR, abs/1212.5701.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, andJun Zhao.
2014.
Relation classification via convolu-tional deep neural network.
In Proceedings of COL-ING, pages 2335?2344.74
