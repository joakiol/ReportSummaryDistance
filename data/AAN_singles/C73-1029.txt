MurrAY T. WILTONBILINGUAL LEXICOGRAPHY: COMPUTER-AIDED EDITINGBilingual dictionaries present special difficulties for the lexicogra-pher who is determined to employ the computer to facilitate his ed-itorial work.
In a sense, these dictionaries include evdrything containedin a normal monolingual edition and a good deal more.
The single-language definition dictionary is consulted as the authority on orthog-raphy, pronunciation and stress-pattern, grammar, level of formality,field of application, definitions, examples, usage and etymology.
Abilingual dictionary which purports to be more than a pocket editionwill treat all of these with the exception of etymology, which is notnormally in the domain of the translator.
In addition, it will devoteitself to providing accurate translations, which necessarily presupposean intimate acquaintance with the correct definitions in both languages.Such a dictionary?
is a far cry from its mediaeval ancestor, the two-lan-guage glossary, which was usually a one-way device furnishing equi-valent forms for simple words and expressions in the opposite language.The modern bilingual dictionary is usually two-way, each section consti-tuting a complete dictionary in its own right and contrived to cater fora variety of translation requirements.
Yet the two sections are inextric-?
ably linked by an intricate network of translations and cross-referenceswhich guide the consulter and ensure that he does not falter when se-mantic equivalence fails to overlap smoothly.
Since semantic equivalenceis the important basic feature of bilingual dictionaries, deviations fromthe normal pattern will require special treatment.
In closely relatedlanguages, like French and English, numerous pairs of words of com-mon origin are only slightly, if at all, altered in their modern form(e.g.
Eng.
versatile/ Fr.
versatile).
But the disparate development oftwo modes of expression in different cultural and historical environ-ments has left a residue of such word pairs whose only similarity is infact the visual image of the sign.
Their definitions are often very re-mote from each other.
It is yet another task of bilingual exicographyto distinguish clearly between the meanings of these deceptive cognatesor "faux amis "22338 MURRAY T. WILTONThese, then, in very brief outline, are some of the features commonto all good bilingual dictionaries.
The Canadian Dictionary (/Diction-naire canadien) i is no exception to these general remarks.
First publishedten years ago under the editorship of Professor Jean-Paul Vinay atthe University of Montreal, it is now undergoing a major revision andupdating at the University of Victoria, still under Vinay's supervision.The new editions hould see the corpus of the original version increasedfrom 40,000 to about 100,000 entry words.
The first edition was spe-cifically tailored for the unique linguistic situation in Canada and takesinto account he two main dialects of each of the official languages itrepresents, namely, European and Canadian French, and British andCanadian English.
This, however, is a gross simplification of a compli-cated dialect situation fraught with all the problems associated withsocial and official acceptability.
But it is sufficient for the purposes ofthis discussion to mention that a good deal of importance is attached toCanadian content in both languages, thereby adding a further unitof complexity to the material to be presented.
Accordingly, in additionto the data common to all bilingual dictionaries, The Canadian Diction-ary furnishes information on the dialect status of most words andexpressions.In the textual published form of the dictionary a variety of symbolsand abbreviations are employed to designate special features, uch asdialect ype, semantic level, range of formality, "faux amis " and soon.
The basic element of the manual editing process is the fde cardupon which all the data for a given entry word is recorded in textualformat.
This file card, which is designed to conform to the exigenciesof the final typographical l yout, is called the "physical fiche " to distin-guish it from the fde card to be used in_computer-aided editing, knownas the "software fiche ".
It includes, in addition to lexicographicalmaterial, details of the source of the words and expressions and oftheir translations as well as the signature of the autor of the fiche.
Dur-ing manual editing these details may be required for verification pur-poses.The first stage in converting the complex textual data to machine-readable form is the recording of the material on the " software fiche ",which has been designed in such a way that it is readily interpreted bykeypunch operators and yet remains a comprehensible manual file atthe same time, in case manual editing should be necessary.
The structure1 j_p.
Vm'AY, et al (1962).BILINGUAL LEXICOGRAPHY" COMPUTER-AIDED EDITING 339of this fiche reflects the conceptual format of the informatic data file,which must be so organized that useful manipulations can be effectedrapidly and accurately within the file and across files in either or bothlanguages.
The unit of manipulation i each data file is the 'entry word,the address for the whole file which enables the file itself to be acces-sed or sorted according to editorial requirements.
Interior addressesenable the file to be structured so that data can be "exploded" inorder to reveal certain elements which that file contains in commonwith other files and which the editor wishes to cluster for variousreasons.
For instance, it will be possible, under this system, to obtaina printout of all words concerned with, say, information theory, so thatthe total list can be submitted for verification to a translator with exper-tise in this field.
This kind of procedure is precluded in manual editingunless the corpus is built up and maintained in subject-fields; even then,it is impossible to ensure that all the words in this area have been suc-cessfully gathered, since many will belong to other fields as well; andat some stage of development the fields must be disintegrated in orderto position the words in alphabetical order within the corpus.Interior addresses are to be reserved for key items which the ed-itors want to be accessible for various reasons: grammatical category,pronunciation, subject field, dialect, level of formality, usage, translations,cross-references and usage notes being those envisaged-at the moment.Three phases have been anticipated for this project.
The first phase,which is presently being operated, consists in the production of simple"skeleton" word lists containing a few essential features uch as gram-mar, subject field and dialect, but without translations or any othertextual material.
The purpose of this exercise is to be able to producelists for verification by spelling and pronunciation consultants and tohave check lists of words by subject matter without the necessity forbreaking alphabetical dictionary order.
The second phase will elaborateon phase one by adding textual material and transforming the entirecorpus to computational form.
The third phase is a feasibility study ofthe pros and cons of using automatic typesetting procedures in thefinal published version.One of the basic problems tudied in the first phase was the neces-sity of maintaining the entry word as the unique address for its filewithout requiring special programmes to be written to account foraccented words in sorting procedures.
The question of homographsof the type fine (noun), fine (adjective), fine (verb), fine (adverb) andFrench fine (feminine noun), fine (feminine adjective) is resolved byf340 MURRAY T. WILTONthe grammatical function attached to the word which readily distin-guishes one file from the other.
Where the main difficulty arises isin the orthography of accented French words and capitalized wordsboth languages.
If printouts of the French wordlists were to be cor-rectly spelt with accents included, using a French print train, somemeans had to be devised for representing the spelling internally withoutinterfering with the address.
Any intrusion of special symbols to des-signate accented letters would mean that existing programmes for sor-ting.
would have to be abandoned and special programmes written.Two solutions presented themselves to avoid doing this.
LEXAOTOM,an automatic dictionary research project at the University of Victo-riaJ represented the entry word twice in two separate fields of eachfile.
The first field contained the entry word without special indicationsfor accents :(e.g.
dtd is represented internally in the entry field as ETE).This procedure nables the entry word to be sorted and accessed withexisting programmes.
The second field, which is located at the endof the fde, is known as the "graphics " field and duplicates the entryword but with the addition of special characters for internal represen-tation of the thirteen accented letters of French.
Unaccented wordswould have this field of their file left blank, although the fixed formatused requires that the appropriate space be reserved anyway.
The systemwas devised for a limited corpus of 100 words for which it was adequate.However, in a corpus of 100,000 words it would be somewhat cum-bersome; it involves duplication of information on the fiche, is wastefulof space and makes sight checking of input cards rather difficult.
Asecond solution is to have a "code " field immediately following the" entry " field in which special characters indicate that a given letterof the "entry"  field is to be treated in a certain way.
For instance,to ensure that p~cher (verb), p~cher (verb) and p~cher (noun) are distin-guished from each other and placed in their correct alphabetical order,or to show that the noun Italien is to be capitalized to distinguish theperson it denotes from the noun italien, which indicates the language,and the adjective italien.
The code system renders duplication unneces-sary and is much less wasteful of storage space since accented wordsusually contain only one, or at most two or three accents and capital-ization involves only the first letter of the word.
In sorting proceduresthe ".
entry" field provides the cue for positioning while the "code"See J-P. VINAY, B. KALLIO, Bilingual Lexicography and the Computer, Universityof Victoria (mimeographed), 1971.BILINGUAL LEXICOGRAPHY: COMPUTER-AIDED EDITING 341field ensures distinction between homographs and correct spelling forprintouts.In the first phase, because of the limited size of fdes, it is possiblefor a fixed-length format to be employed without undue storage waste.A typical file will contain the entry word, its grammatical function,subject field or fields and a dialect indication where necessary.
Anyof these features, located at different but fixed levels in the file, willbe accessible.
Other non-accessible elements which may be useful inprintout lists are cross-references, usage notes, and an indication as towhether inversion of the fiche has been carried out.The second phase of the project involves the conversion of the entirecorpus to machine-readable form.
If programme systems for the "skel-eton" word lists have been successful, this phase should be an extensionof the first, avoiding the necessity to duplicate keypunching of all thematerial.
The adoption of on-line procedures would enable all editor-ial tasks in the final stages to be accomplished rapidly and automati-cally, perhaps from remote terminals with video screens.
The datafile in this phase is clearly much more complex and gives rise to theproblem of variability in fde size; in textual form, entry word "files "range from one or two lines to several columns or even pages of contin-uous typographical text.
In order to maintain the capability of accessto certain elements of the file, these elements must be located at fixedlevels.
But a fixed format file system is tremendously wasteful of stor-age space since, in theory at least, every file must have at least he capacityof the largest of them, whereas most files are, in fact, relatively small.Thus the only solution, other than employing an individual programmefor each file, is to design a variable length format.The question of variability is currently being studied by program-mers at the Computer Centre of the University of Victoria and I.~.M.
(Canada) Ltd.
The most likely outcome of this study is a theoreticalconception of the data file based on the "catalogue" system devised bythe Rand Corporation for the production of the Random House Dic-tionary (0aonolingual) but suitably adapted in certain respects for thebilingual situation.
The Rand system was designed with a structurewhich could accommodate n w elements of variable size and yet remainsubject to a single management programme applicable to every file.The individuality of each file is preserved by means of a key, a repro-duction in miniature of the contents of the file.
This key, in coded form,describes the data dasses included in the file and the byte space occu-pied by them.
The key is always interpreted first, prior to the reading342 MURRAY T. WILTONof the file itself, and therefore has the total effect of an individual prog-ramme for that file.
For The Canadian Dictionary the catalogue wouldcontain a common structure of ten or eleven levels corresponding tothe elements for which an accessible address is required.
These discretelevels are fixed, so that the code for their access remains constant; it isonly the coding of the variable contents of the file which is flexible.If it becomes possible to employ a system of this type, given theexpense and difficulties of keypunching a vast quantity of complextextual data, it will permit the editors to manipulate their material invarious ways without requiring a large research team and many man-hours of work.
One particularly time-consuming and laborious taskcarried out in manual bilingual lexicography is the inversion of the con-tents of a fiche.
This procedure is to ensure that translations of entrywords or expressions in the source language are converted to entrywords or derivative xpressions in the target language.
In fact, the cir-cularity of dictionaries i such that enormous quantities of a bilingualwork could theoretically be built up by a continuos "back-and-forth "movement of translation equivalents, each translated word giving rise,in turn, to a whole series of different words and expressions.
In practice,however, inversion is carried out only to establish the existence of im-mediate translations in the opposite-language section of the dictionary.Computational procedures would accomplish this far more rapidlyand fully than the present manual methods and should ensure that, asfar as possible, semantic losure is achieved.
Similarly, many othereditorial tasks, from simple checking of spelling to complete updatingof files, will become relatively elementary operations consuming farless time and effort.
But computer-aided bilingual exicography looksfurther ahead than the mere acceleration of the editorial process.
Theautomated ictionary will overcome the problem shared by all neweditions-the stigma of instant obsolescence at the very moment ofbirth.
For a new edition can now be produced virtually at a moment'snotice with the rapid addition of new material and weeding out of out-moded forms.
The time required to produce such a revised editioncan be reduced from decades to months, or even weeks, and 'in generalthere will be a proliferation of useful consultative material which isalways up to date.\
