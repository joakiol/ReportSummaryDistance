Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 1?9,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsFast Unsupervised Dependency Parsing with Arc-Standard TransitionsMohammad Sadegh RasooliDepartment of Computer EngineeringIran University of Science and TechnologyNarmak, Tehran, Iranrasooli@comp.iust.ac.irrasooli.ms@gmail.comHeshaam FailiSchool of Electricaland Computer EngineeringUniversity of TehranAmir-Abaad, Tehran, Iranhfaili@ut.ac.irAbstractUnsupervised dependency parsing is one ofthe most challenging tasks in natural lan-guages processing.
The task involves find-ing the best possible dependency trees fromraw sentences without getting any aid fromannotated data.
In this paper, we illus-trate that by applying a supervised incre-mental parsing model to unsupervised pars-ing; parsing with a linear time complex-ity will be faster than the other methods.With only 15 training iterations with lineartime complexity, we gain results compara-ble to those of other state of the art methods.By employing two simple universal linguis-tic rules inspired from the classical depen-dency grammar, we improve the results insome languages and get the state of the artresults.
We also test our model on a part ofthe ongoing Persian dependency treebank.This work is the first work done on the Per-sian language.1 IntroductionUnsupervised learning of grammars has achievedconsiderable focus in recent years.
The lackof sufficient manually tagged linguistic data andthe considerable successes of unsupervised ap-proaches on some languages have motivated re-searchers to test different models of unsupervisedlearning on different linguistic representations.Since the introduction of the dependency modelwith valence (DMV) proposed by Klein and Man-ning (2004), dependency grammar induction hasreceived great attention by researchers.
DMVwas the first model to outperform the right attach-ment accuracy in English.
Since this achievement,the model has been used by many researchers(e.g.
(Cohen and Smith, 2010); (Gillenwater et al,2011); (Headden III et al, 2009); and (Spitkovskyet al, 2011b)).The main task of unsupervised dependencyparsing is to obtain the most likely dependencytree of a sentence without using any annotatedtraining data.
In dependency trees, each word hasonly one head and the head of the sentence is a de-pendent of an artificial root word.
Problems suchas data sparsity and a large search space that in-creases the ambiguity have made the task difficult.Even deciding the direction of the link betweentwo words in a dependency relation has made thetask more difficult than finding phrase structuresthemselves (Klein and Manning, 2004).In this paper, we propose a model based onArc-Standard Transition System of Nivre (2004),which is known as an incremental greedy projec-tive parsing model that parses sentences in lin-ear time.
To the best of our knowledge, the onlyincremental unsupervised dependency parsing isthe model of Daume?
III (2009) with Shift-Reduceparsing model (Nivre, 2003).1Our model is not lexicalized, has a simple fea-ture space and converges in 15 iterations witha linear (O(n)) parsing and training time, whileother methods based on DMV in the best casework inO(n3) time complexity withO(n3) mem-ory use for sentences with of length n. We be-lieve that the output of this model can also im-prove DMV.2 In addition, we use punctuationclues (Spitkovsky et al, 2011c), tying feature sim-ilarity in the transition system configuration, and1The other study is in Seginer (2007) that is for con-stituency parsing (phrase structure extraction).2For the effect of model initialization in unsupervised de-pendency parsing, see Gimpel and Smith (2011).1?baby steps?
notion (Spitkovsky et al, 2009) toimprove the model accuracy.We test our model on 9 CoNLL 2006 and 2007shared task data sets (Buchholz and Marsi, 2006;Nivre et al, 2007) and WSJ part of Penn treebankand show that in some languages our model is bet-ter than the recent models.
We also test our modelon a part of an ongoing first Persian dependencycorpus (Rasooli et al, 2011).
Our study may bethe first work to test dependency parsing on thePersian language.The remainder of this paper is organized as fol-lows.
In Section 2, related work on unsuperviseddependency parsing is reviewed.
In Section 3, wedescribe our dependency parsing model.
In Sec-tion 4 and Section 5, after the reporting experi-mental results on several languages, the conclu-sion is made.2 Related WorkThe first considerable work on unsupervised de-pendency parsing which outperforms the base-line (right attachment) accuracy in English wasproposed by Klein and Manning (2004).
Themodel is called dependency model with valence(DMV).
In the DMV, each word can be thehead of the sentence with the probability ofP (root|X).
Each word X , decides to get achild Y from a direction (right or left), withthe probability PCHOOSE(X|Y, dir, adj), whereadj is a Boolean value indicating whether theword has gotten a child in the direction dir ornot.
The other probability used in the DMVis PSTOP (X|Y, dir, adj) that means whether tostop getting dependents from the direction withadjacency value or not.
All the probabilities in themodel are assumed to be independent and the de-pendency tree likelihood is a product of all prob-abilities.
Only part of speech (POS) tags are usedas features and the probabilities are multinomial.The model uses the inside-outside algorithm tofind all possible subtrees efficiently in Expecta-tion Maximization (EM) algorithm.Several researchers have tried to improve andmodify the DMV.
In Headden III et al (2009), byusing the lexical values with the frequency morethan 100 and defining tied probabilistic contextfree grammar (PCFG) and Dirichlet priors, the ac-curacy is improved.
In Smith and Eisner (2005),by producing artificial neighbors of the featurespace via actions such as deletion of one word,substitution of adjacent words and adding a word,the likelihood of the true feature space in allneighbors is calculated.
That method is knownas contrastive estimation (CE).In Spitkovsky et al (2009), the idea of learningthe initial parameters of the model from shortersentences leads to a method named ?baby steps?.In ?baby steps?, the model prior of each trainingset with the sentence length less than or equal toN , is achieved by training DMV on the trainingset with the sentence length less than or equal toN ?
1.
The other method used in the mentionedwork, is ?less is more?
which hypothesize thattraining on a subset of all data (with the lengthof less than or equal to 15) in batch mode is moreuseful than training on all data.
In Spitkovsky etal.
(2010a), a combination of ?baby steps?
and?less is more?, named ?leapfrog?
is applied tothe DMV.
In Spitkovsky et al (2011b), a mixtureof EMs is used to improve the DMV by tryingto escape from local maxima; i.e., changing theEM policy in some iterations in order to escapefrom local maxima.
The model is termed ?lateen?EM.
In Spitkovsky et al (2010b), HTML hyper-text tags are used as indicators of phrases in or-der to localize the search space of the dependencymodel.
In Spitkovsky et al (2011c), punctuationmarks are used as indicators of local dependenciesof the words in the sentence.In Cohen and Smith (2010), shared logistic nor-mal distribution is used to tie grammatical rolesthat are not assumed to be independent from eachother.
In the study, the bilingual similarity of eachPOS tag probability in the dependency model isapplied to the probability model.
In Blunsom andCohn (2010), Pitman-Yor priors (PYP) are ap-plied to the DMV.
Furthermore, tree substitutiongrammar (TSG) is used as an intermediate repre-sentation of the tree.
In Gillenwater et al (2011),a mathematical model is employed to overcomethe posterior sparsity in the DMV, by definingconstraints on the probability model.There are also some models different fromDMV.
In Daume?
III (2009), based on a stochas-tic search method, Shift-Reduce transition pars-ing model of Nivre (2003) is applied.
The modelis greedy and selects an action stochastically ac-cording to each action probability at the time.
Theadvantage of the model lies on its parsing andtraining speed.
In Naseem and Barzilay (2011),sparse semantic annotations in texts are used as2Initialization ?nil,W,?
?Termination ?S, nil, A?Left-Reduce ?wiwj |S, I, A?
?
?wj |S, I, A ?
?wj , wi?
?Right-Reduce ?wiwj |S, I, A?
?
?wi|S, I, A ?
?wi, wj?
?Shift ?S,wi|I, A?
?
?wi|S, I, A?Figure 1: Actions in Arc-Standard Transition System (Nivre, 2004)clues to unsupervised parsing.
In Marec?ek andZ?abokrtsky?
(2011), by applying Gibbs samplingmethod to count the data occurrences, a simpleprobability model (the fraction of each depen-dency relation divided by the number of head POStags) is used.
In that model, non-projective depen-dency trees are allowed and all noun-root depen-dency probabilities are multiplied by a small num-ber, to decrease the chance of choosing a noun-root dependency.
There are also some studies inwhich labeled data in one language is employedto guide unsupervised parsing in the others (Co-hen et al, 2011).3 Fast Unsupervised ParsingIn this section, after a brief description of the Arc-Standard parsing model, our probability model,and the unsupervised search-based structure pre-diction (Daume?
III, 2009) are reviewed.
Afterthese descriptions, we go through ?baby steps,?the use of curricula in unsupervised learning (Tuand Honavar, 2011), and the use of punctuationin unsupervised parsing.
Finally, we describe ourtied feature model that tries to overcome the datasparsity.
In this paper, a mixture of ?baby steps?and punctuation clues along with search-basedstructure prediction is applied to the Arc-Standardmodel.3.1 Arc-Standard Transition ModelThe parser in this model has a configuration repre-sented by ?S, I, A?, where S is a stack of words,I is a buffer of input words which are not pro-cessed yet andA is the list of all arcs that are madeuntil now.
The parser initializes with ?nil,W, ?
?in which W is a string of all words in the sen-tence, nil shows a stack with a root word and ?shows an empty set.
The termination configura-tion is shown as ?S, nil, A?, where S shows anempty stack with only root word, nil shows anempty buffer and A is the full arc set.
An arc inwhich wj is the head of wi is shown by wj ?
wior (wj , wi).As shown in Figure 1, there are three actionsin this model.
In the shift action, the top-mostinput word goes to the top of the stack.
In the left-reduce action, the top-most stack word becomesthe head of the second item in the stack and thesecond item is removed from the stack.
On theother hand, in the right-reduce action, the secondword in the stack becomes the head of the top itemin the stack and the top item is removed from thestack.3.2 Feature Space and Probability ModelThe feature space that we use in this model isa tuple of three POS tags; i.e., the first item inthe buffer, the top-most and the second item inthe stack.
The probability of each action is in-spired from Chelba and Jelinek (2000) as in equa-tion (1).
In each step in the configuration, theparser chooses an action based on the probabilityin equation (1), where feat is the feature valueand act is an action.P (act, feat) = P (act) ?
P (feat|act) (1)The action selection in the training phase isdone stochastically.
In other words, in every stepthere is a maximum of 3 actions and a minimumof one action.3 After calculating all probabili-ties, a roulette wheel is made to do multinomialsampling.
The sampling is done with stochas-tic EM (Celeux and Diebolt, 1985) in a roulettewheel selection model.The probabilities are initialized equally (exceptthat P (shift) = 0.5 and P (right ?
reduce) =P (left?
reduce) = 0.25).
After sampling fromthe data, we update the model as in equations 2?4, where ?
is a smoothing variable and Nf is thenumber of all possible unique features in the dataset.
C(?)
is a function that counts the data from3For example, in the first state only shift is possible andin the last state only right-reduce is possible.3samples.
In equations 3 and 4, sh, r ?
r andl?r are shift, right-arc and left-arc actions respec-tively.
C(Action, Feature) is obtained from thesamples drawn in the training phase.
For exam-ple, if the right-reduce action is selected, we addits probability to C(right?
reduce, feature).P (feat|act) =C(act, feat) + ?C(act) +Nf?
(2)P (sh) = 0.5 (3)P (act) =C(act) + ?C(r ?
r) + C(l ?
r) + 2?
;act 6= Shift(4)3.3 Unsupervised Search-Based StructurePredictionSince there are 32n+1 possible actions for a sen-tence with the length of n it seems impracticalto track the search space for even middle-lengthsentences.
Accordingly, in Daume?
III (2009) astochastic search model is designed to improvethe model accuracy based on random actions.
Inthat work, with each configuration step, the trainerselects one of the actions according to the proba-bility of each action stochastically.
By choosingactions stochastically, a set of samples is drawnfrom the data and the model parameters are up-dated based on the pseudo-code in Figure 2.
InFigure 2, pi is known as the policy of the prob-ability model and ?
is a constant number whichchanges in each iteration based on the iterationnumber (?
= 1iteration#3 ).
We employ this modelin our work to learn probability values in equa-tion 1.
The learning from samples is done viaequations (2?4).3.4 ?Baby Steps?
Incremental ParsingIn Spitkovsky et al(2009), the idea that shortersentences are less ambiguous, hence more in-formative, is applied to the task.
Spitkovsky etal.
(2009) emphasize that starting from sentenceswith a length of 1 and iterating on sentences withthe length ?
N from the probabilities gainedfrom the sentences with the length ?
N ?
1,leads to better results.Initialize pi = pi?while not convergeTake samples stochasticallyh?
learn from samplespi = ?pi + (1?
?
)hend whilereturn piFigure 2: Pseudo-code of the search-based structureprediction model in Daume?
III (2009)We also use ?baby steps?
on our incrementalmodel.
For the sentences having length 1 through5, we only iterate once in each sentence length.At those sentence lengths, the full search space isexplored (all trees are made by doing all possi-ble actions in each state of all possible configura-tions), while for sentence length 6 towards 15, weiterate at each step 3 times, only choosing one ac-tion stochastically at each state.
The procedure isdone similarly for all languages with the same pa-rameters.
In fact, the greedy nature of the modelencourages us to bail out of each sentence lengthquickly.
In other words, we want to jump out ofearly local maxima, as in early-terminating lateenEM (Spitkovsky et al, 2011b).In curricula (Tu and Honavar, 2011), smooth-ing variable for shorter sentences is larger thansmoothing variable for longer sentences.
With re-gards to the idea, we start with smoothing variableequal to 1 and multiply it on each sentence lengthby a constant value equal to e?1.3.5 Punctuation CluesSpitkovsky et al (2011c) show that about 74.0%of words in English texts occurring between twopunctuation marks have only one word linkingwith other words of the sentence.
This character-istic is known as ?loose?.
We apply this restrictionon our model to improve the parsing accuracy anddecrease the total search space.
We show that thisclue not only does not improve the dependencyparsing accuracy, but also decreases it in some oc-casions.3.6 Tying Probabilities with FeatureSimilarity MeasureWe assume that the most important features inthe feature set for right-reduce and left-reduce ac-tions are the two top words in the stack.
On theother hand, for the shift action, the most impor-4tant words are first buffer and top stack words.
Inorder to solve the sparsity problem, we modifythe probability of each action based on equation(5).
In this equation, neigh(act, feat) is gainedvia searching over all features with the same topand second stack item for left-reduce and right-reduce, and all features with the same top stackand first buffer item for the shift action.P ?
(feat|act) =P (feat|act) +?f ?
?neigh(act,feat) P (f?|act)C(neigh(act,feat))2(5)3.7 Universal Linguistic Heuristics toImprove Parsing AccuracyBased on the nature of dependency grammar, weapply two heuristics.
In the first heuristic, we mul-tiply the probability of the last verb reduction by10?10 in order to keep verbocentricity of the de-pendency grammar.
The last verb reduction oc-curs when there is neither a verb in the buffernor in the stack except the one that is going tobe reduced by one of the right-arc or left-arc ac-tions.
In other words, the last verb remainingon the stack should be less likely to be removedthan the other actions in the current configura-tion.4 In the second heuristic, in addition to thefirst heuristic, we multiply each noun ?
verb,adjective ?
verb, and adjective ?
noun by0.1 in order to keep the nature of dependencygrammar in which nouns and adjective in mostcases are not able to be the head of a verb and anadjective is not able to be the head of a noun.5 Weshow in the experiments that, in most languages,considering this nature will help improve the pars-ing accuracy.We have tested our model on 9 CoNLL datasets (Buchholz and Marsi, 2006; Nivre et al,2007).
The data sets include Arabic, Czech, Bul-garian, Danish, Dutch, Portuguese, Slovenian,Spanish, and Swedish.
We have also tested ourmodel on a part of the ongoing project of Persiandependency treebank.
The data set includes 2,1134It is important to note that the only reason that wechoose a very small number is to decrease the chance ofverb-reduction among three possible actions.
Using othervalues?
0.01 does not change results significantly.5The are some exceptions too.
For example, in the sen-tence: ?I am certain your work is good.?
Because of that, wedo not choose a very small number.train and 235 test sentences.6As shown in Figure 3, we use the same proce-dure as in Daume?
III (2009), except that we re-strict ?
to not be less than 0.005 in order to in-crease the chance of finding new search spacesstochastically.
As in previous works, e.g., Smithand Eisner (2005), punctuation is removed forevaluation.iteration# = 0for i=1 to 15 doTrain-set=all sentences-length?imax-iter=3if(i?
5)max-iter=1end-iffor j=1 to max-iter do?
= max( 1iteration#3 , 0.005)iteration#?
iteration# + 1if(i ?
5)samples?
find all subtreesend-ifelsesamples?
sample instances stochasticallyend-elseh?
learn from samplespi = ?pi + (1?
?)hend-for?
= ?
?
e?1end-forFigure 3: Pseudo-code of the unsupervised Arc-Standard training model4 Evaluation ResultsAlthough training is done on sentences of lengthless than 16, the test was done on all sentences inthe test data without dropping any sentences formthe test data.
Results are shown in Table 1 on 9languages.
In Table 1, ?h1?
and ?h2?
refer to thetwo linguistic heuristics that are used in this pa-per.
We also compare our work with Spitkovskyet al (2011b) and Marec?ek and Z?abokrtsky?
(2011)6This dataset is obtained via contacting with the projectteam at http://www.dadegan.ir/en/.
Recently an officialpre-version of the dataset is released, consisting morethan 12,000 annotated sentences (Dadegan Research Group,2012).
We wish to report results on the dataset in our futurepublications.5Baselines Using Heuristic 1 and 2 Using Heuristic 1 Without any heuristicLanguage Rand LA RA fs+punc fs punc fs+punc punc punc+fs simp.Arabic?07 3.90 59.00 06.00 52.05 52.05 52.05 54.55 54.55 55.64 55.64Bulgarian 8.00 38.80 17.90 52.48 53.86 46.36 42.75 37.35 35.99 35.99Czech?07 7.40 29.60 24.20 42.37 42.40 39.31 30.21 27.94 25.17 25.17Danish 6.70 47.80 13.10 52.14 53.11 52.14 51.10 51.70 46.01 46.01Dutch 7.50 24.50 28.00 48.14 48.80 48.20 28.30 28.36 23.47 23.45Persian 9.50 03.90 29.16 51.65 51.37 50.99 49.78 50.99 26.87 26.87Portuguese 5.80 31.20 25.80 54.86 55.84 46.82 33.84 33.62 28.83 28.83Slovenian 7.90 26.60 24.30 22.44 22.44 22.43 21.31 21.30 19.47 19.45Spanish 4.30 29.80 24.70 30.88 31.16 30.88 32.33 32.33 29.63 29.69Swedish 7.80 27.80 25.90 32.74 34.33 33.52 28.48 28.48 25.74 25.74Turkish 6.40 01.50 65.40 33.83 27.39 38.13 61.27 47.92 30.56 34.52Average 6.84 29.14 25.86 43.05 42.98 41.89 39.45 37.69 31.58 31.94Table 1: Results tested on CoNLL data sets and the Persian data set.
?Rand?, ?LA?
and ?RA?
stand for random,left-attach and right-attach, respectively; ?punc?
refers to punctuation clues and fs refers to feature similarity cue;?all?
refers to using both heuristics h1 and h2; and ?simp.?
refers to the simple model.in Table 2.
As shown in Table 2, our model out-performs the accuracy in 7 out of 9 languages.The Effect of Feature SimilarityAs shown in Table 1, feature similarity cannothave any effect on the simple model.
When weadd linguistic information to the model, this fea-ture similarity measure keeps the trainer fromdiverging.
In other words, the greedy natureof the model becomes endangered when incom-plete knowledge (as in our linguistic heuristics)is used.
Incomplete knowledge may cause earlydivergence.
In other words, the greedy algo-rithm tracks the knowledge which it has and doesnot consider other probable search areas.
Thisphenomenon may cause early divergence in themodel.
By using feature similarity, we try to es-cape from this event.The Effect of Punctuation CluesAs shown in Table 1, in most languages punc-tuation clues do not improve the accuracy.
Thismaybe arises out of the fact that ?loose?
is not agood clue for incremental parsing.
The other clueis ?sprawl?
in which the external link restrictionis lifted.
This restriction is in 92.9% of fragmentsin English texts (Spitkovsky et al, 2011c), but itis not implemented and tested in this paper.4.1 Evaluation on EnglishWe also test our data on Penn Treebank but we donot gain better results than state of the art meth-ods.
We use the same train and test set as inModel WSJ?10 WSJ ?
?h1+fs 45.16 31.97h1+fs+punc 44.17 30.17Stoch.
EM(1-5) 40.86 33.65Stoch.
EM(1-5)+h1 52.70 42.85Stoch.
EM(1-5)+h1+h2 50.30 41.37A1+fs+h1 49.9 43.3Klein and Manning (2004) 43.2 -Daume?
III (2009) 45.4 -Blunsom and Cohn (2010) 67.7 55.7Spitkovsky et al (2011a) - 59.1Table 3: Results of our model on WSJ, comparedto its counterpart Daume?
III (2009) and other DMV-based models.
Since in Blunsom and Cohn (2010) andSpitkovsky et al (2011b), other results are reported,we only limit our report to some of the results on WSJ.In the Table, ?h1?
shows heuristic 1 and ?fs?
showsthe use of feature similarity.
Stochastic EM(1-5) isone test that have done only by applying baby stepson sentences with the length 1 to 5 without using un-supervised search-based model.
A1 refers to a changein the model in which smoothing variable in steps 1 to5 is multiplied by 10.Spitkovsky et al (2009).
We convert the Penntreebank data via automatic ?head-percolation?rules (Collins, 1999).
We have also tested ourmodel via simple stochastic EM (without usingunsupervised structure prediction) and show thatthe main problem with this method in English isits fast divergence when jumping from sentencelength 5 to 6.
In the model settings tested forEnglish, the model with heuristic 1 with the fea-ture similarity is the best setting that we find.
Bytesting with a smoothing variable ten times bigger6``````````````LanguageMethod NameMZ-NR MZ Spi5 Spi6 Our BestArabic?07 24.8 25.0 22.0 49.5 55.64Bulgarian 51.4 25.4 44.3 43.9 53.86Czech?07 33.3 24.3 31.4 28.4 42.40Danish 38.6 30.2 44.0 38.3 53.11Dutch 43.4 32.2 32.5 27.8 48.80Persian - - - - 51.65Portuguese 41.8 43.2 34.4 36.7 55.84Slovenian 34.6 25.4 33.6 32.2 22.44Spanish 54.6 53.0 33.3 50.6 32.33Swedish 26.9 23.3 42.5 50.0 34.33Turkish 32.1 32.2 33.4 35.9 61.27Average (except Persian) 38.1 31.4 35.1 39.3 46.00Table 2: Comparison of our work to those of Table 5 (?Spi5?)
and Table 6 (?Spi6?)
in Spitkovsky et al (2011b)and Marec?ek and Z?abokrtsky?
(2011) with Noun-Root constraint (?MZ-NR?)
and no constraint (?MZ?).
Thecomparison results are from Table 4 in Marec?ek and Z?abokrtsky?
(2011).
?Our Best?
refers to the bold scores inTable 1.in the first 5 steps, we have seen that the resultschange significantly.
The results are shown in Ta-ble 3.One main problem of the converted depen-dencies in English is their conversion errors likemulti-root trees.7 There are many trees in thecorpus that have wrong multi-root dependencies.Such problems lead us to believe that we shouldnot rely too much on the results on WSJ part ofthe Penn treebank.5 Analysis and ConclusionOne main aspect of incremental methods is theirsimilarity to the way that humans read and learnsentences in language.
The interesting character-istic of incremental parsing lies on its speed andlow memory use.
In this paper, we use one newincremental parsing model on unsupervised pars-ing for the first time.
The simple mathematicalmodel and its linear training order has made themodel flexible to be used for bigger data sets.In addition to testing recently used heuristics inunsupervised parsing, by inspiring basic depen-dency theory from linguistics, parsing accuracyhas been increased in some languages.
We seethat this model is capable of detecting many truedependencies in many languages.Some observations show that choosing inap-7We assume to find only trees that are projective andsingle-rooted.propriate parameters for the model may lead tounwanted divergence in the model.
The diver-gence is mostly seen in English, where we seea significant accuracy decrease at the last step incomparison to step 5 instead of seeing an increasein the accuracy.
With one setting in English, wereach the accuracy equal to 43% (13% more thanthe accuracy of the model reported in this paper).In some languages like Slovenian, we see thateven with a good undirected accuracy, the modeldoes not succeed in finding the dependency di-rection with heuristics.
While in Czech, Dutch,and Bulgarian the second heuristic works well,it does not change accuracy a lot in other lan-guages (in languages like Turkish and English thisheuristic decreases accuracy).
We believe thatchoosing better linguistic knowledge like the onesin Naseem et al (2010), tying grammatical rulesfrom other languages similar to the work in Cohenand Smith (2010), and choosing better probabilitymodels that can be enriched with lexical featuresand broad context consideration (like the works insupervised incremental dependency parsing) willhelp the model perform better on different lan-guages.Despite the fact that our study is the first workdone on Persian, we believe that the results thatwe achieve for Persian is very considerable, re-garding the free-word order nature of the Persianlanguage.7AcknowledgmentsThe paper is funded by Computer Research Cen-ter of Islamic Sciences (CRCIS).
We would liketo appreciate Valentin Spitkovsky and Shay Co-hen for their technical comments on the re-search and paper draft.
We would also thankMaryam Faal-Hamedanchi, Manouchehr Kouhes-tani, Amirsaeid Moloodi, Hamid Reza Ghader,Maryam Aminian and anonymous reviewers fortheir comments on the draft version.
We wouldalso like to thank Jason Eisner, Mark Johnson,Noah Smith and Joakim Nivre for their help inanswering our questions and Saleh Ziaeinejad andYounes Sangsefidi for their help on the project.ReferencesPhil Blunsom and Trevor Cohn.
2010.
Unsupervisedinduction of tree substitution grammars for depen-dency parsing.
In 2010 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2010), pages 1204?1213.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceeding of the Tenth Conforence on Computa-tional Natural Language Learning (CoNLL).Gilles Celeux and Jean Diebolt.
1985.
The SEM al-gorithm: A probabilistic teacher algorithm derivedfrom the em algorithm for the mixture problem.Computational Statistics Quarterly, 2:73?82.Ciprian Chelba and Frederick Jelinek.
2000.
Struc-tured language modeling.
Computer Speech & Lan-guage, 14(4):283?332.Shay B. Cohen and Noah A. Smith.
2010.
Co-variance in unsupervised learning of probabilisticgrammars.
Journal of Machine Learning Research(JMLR), 11:3117?3151.Shay B. Cohen, Dipanjan Das, and Noah A. Smith.2011.
Unsupervised structure prediction with Non-Parallel multilingual guidance.
In Conference onEmpirical Methods in Natural Language Process-ing (EMNLP 2011).Michael Collins.
1999.
Head-driven statistical mod-els for natural language parsing.
Ph.D. thesis, Uni-versity of Pennsylvania.Dadegan Research Group.
2012.
Persian DependencyTreebank Version 0.1, Annotation Manual and UserGuide.
http://dadegan.ir/en/.Hal Daume?
III, John Langford, and Daniel Marcu.2009.
Search-based structured prediction.
MachineLearning, 75(3):297?325.Hal Daume?
III.
2009.
Unsupervised search-basedstructured prediction.
In 26th International Confer-ence on Machine Learning (ICML), pages 209?216.ACM.Jennifer Gillenwater, Kuzman Ganchev, Joa?o Grac?a,Fernando Pereira, and Ben Taskar.
2011.
Poste-rior sparsity in unsupervised dependency parsing.Journal of Machine Learning Research (JMLR),12:455?490.Kevin Gimpel and Noah A. Smith.
2011.
Concav-ity and initialization for unsupervised dependencygrammar induction.
Technical report.William P. Headden III, Mark Johnson, and DavidMcClosky.
2009.
Improving unsupervised depen-dency parsing with richer contexts and smoothing.In Human Language Technologies: The 2009 An-nual Conference of the North American Chapter ofthe ACL, pages 101?109.Dan Klein and Christopher D. Manning.
2004.Corpus-based induction of syntactic structure:Models of dependency and constituency.
In Asso-ciation for Computational Linguistics (ACL).David Marec?ek and Zdene?k Z?abokrtsky?.
2011.
Gibbssampling with treeness constraint in unsuperviseddependency parsing.
In RANLP Workshop on Ro-bust Unsupervised and Semisupervised Methods inNatural Language Processing.Tahira Naseem and Regina Barzilay.
2011.
Using se-mantic cues to learn syntax.
In 25th Conference onArtificial Intelligence (AAAI-11).Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using universal linguistic knowl-edge to guide grammar induction.
In 2010 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2010).Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceeding of CoNLL 2007.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In International Work-shop on Parsing Technologies, pages 149?160.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In Workshop on IncrementalParsing: Bringing Engineering and Cognition To-gether, pages 50?57.Mohammad Sadegh Rasooli, Amirsaeid Moloodi,Manouchehr Kouhestani, and Behrouz Minaei-Bidgoli.
2011.
A syntactic valency lexicon forPersian verbs: The first steps towards Persian de-pendency treebank.
In 5th Language & TechnologyConference (LTC): Human Language Technologiesas a Challenge for Computer Science and Linguis-tics, pages 227?231.Yoav Seginer.
2007.
Fast unsupervised incremen-tal parsing.
In 45th Annual Meeting of the Asso-ciation of Computational Linguistics (ACL), pages384?391.Noah A. Smith and Jason Eisner.
2005.
Guiding un-supervised grammar induction using contrastive es-timation.
In IJCAI Workshop on Grammatical In-ference Applications, pages 73?82.8Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2009.
Baby steps: How ?Less is more?
inunsupervised dependency parsing.
In NIPS 2009Workshop on Grammar Induction, Representationof Language and Language Learning.Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2010a.
From baby steps to leapfrog: How?Less is more?
in unsupervised dependency pars-ing.
In Human Language Technologies: The 11thAnnual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL HLT 2010).Valentin I. Spitkovsky, Daniel Jurafsky, and Hiyan Al-shawi.
2010b.
Profiting from Mark-Up: Hyper-Text annotations for guided parsing.
In 48th AnnualMeeting of the Association for Computational Lin-guistics (ACL 2010).Valentin I. Spitkovsky, Hiyan Alshawi, Angel XChang, and Daniel Jurafsky.
2011a.
Unsuperviseddependency parsing without gold Part-of-Speechtags.
In 2011 Conference on Empirical Methods inNatural Language Processing (EMNLP 2011).Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2011b.
Lateen EM: unsupervised train-ing with multiple objectives, applied to dependencygrammar induction.
In 2011 Conference on Em-pirical Methods in Natural Language Processing(EMNLP 2011).Valentin I. Spitkovsky, Hiyan Alshawi, and DanielJurafsky.
2011c.
Punctuation: Making a pointin unsupervised dependency parsing.
In FifteenthConference on Computational Natural LanguageLearning (CoNLL-2011).Kewei Tu and Vasant Honavar.
2011.
On the utility ofcurricula in unsupervised learning of probabilisticgrammars.
In 22nd International Joint Conferenceon Artificial Intelligence (IJCAI 2011).9
