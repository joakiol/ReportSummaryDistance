Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 400?409,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPNatural Language Generation with Tree Conditional Random FieldsWei Lu1, Hwee Tou Ng1,2, Wee Sun Lee1,21Singapore-MIT Alliance2Department of Computer ScienceNational University of Singaporeluwei@nus.edu.sg{nght,leews}@comp.nus.edu.sgAbstractThis paper presents an effective methodfor generating natural language sentencesfrom their underlying meaning represen-tations.
The method is built on top ofa hybrid tree representation that jointlyencodes both the meaning representationas well as the natural language in a treestructure.
By using a tree conditionalrandom field on top of the hybrid treerepresentation, we are able to explicitlymodel phrase-level dependencies amongstneighboring natural language phrases andmeaning representation components in asimple and natural way.
We show thatthe additional dependencies captured bythe tree conditional random field allows itto perform better than directly inverting apreviously developed hybrid tree semanticparser.
Furthermore, we demonstrate thatthe model performs better than a previ-ous state-of-the-art natural language gen-eration model.
Experiments are performedon two benchmark corpora with standardautomatic evaluation metrics.1 IntroductionOne of the ultimate goals in the field of natural lan-guage processing (NLP) is to enable computers toconverse with humans through human languages.To achieve this goal, two important issues needto be studied.
First, it is important for comput-ers to capture the meaning of a natural languagesentence in a meaning representation.
Second,computers should be able to produce a human-understandable natural language sentence from itsmeaning representation.
These two tasks are re-ferred to as semantic parsing and natural languagegeneration (NLG), respectively.In this paper, we use corpus-based statisticalmethods for constructing a natural language gener-ation system.
Given a set of pairs, where each pairconsists of a natural language (NL) sentence andits formal meaning representation (MR), a learn-ing method induces an algorithm that can be usedfor performing language generation from otherpreviously unseen meaning representations.A crucial question in any natural language pro-cessing system is the representation used.
Mean-ing representations can be in the form of a treestructure.
In Lu et al (2008), we introduced ahybrid tree framework together with a probabilis-tic generative model to tackle semantic parsing,where tree structured meaning representations areused.
The hybrid tree gives a natural joint tree rep-resentation of a natural language sentence and itsmeaning representation.A joint generative model for natural languageand its meaning representation, such as that usedin Lu et al (2008) has several advantages over var-ious previous approaches designed for semanticparsing.
First, unlike most previous approaches,the generative approach models a simultaneousgeneration process for both NL and MR. One el-egant property of such a joint generative modelis that it allows the modeling of both semanticparsing and natural language generation within thesame process.
Second, the generative process pro-ceeds as a recursive top-down Markov process ina way that takes advantage of the tree structureof the MR.
The hybrid tree generative model pro-posed in Lu et al (2008) was shown to give state-of-the-art accuracy in semantic parsing on bench-mark corpora.While semantic parsing with hybrid trees hasbeen studied in Lu et al (2008), its inverse task?
NLG with hybrid trees ?
has not yet been ex-plored.
We believe that the properties that makethe hybrid trees effective for semantic parsing alsomake them effective for NLG.
In this paper, we de-velop systems for the generation task by building400on top of the generative model introduced in Lu etal.
(2008) (referred to as the LNLZ08 system).We first present a baseline model by directly?inverting?
the LNLZ08 system, where an NL sen-tence is generated word by word.
We call thismodel the direct inversion model.
This model isunable to model some long range global depen-dencies over the entire NL sentence to be gener-ated.
To tackle several weaknesses exhibited bythe baseline model, we next introduce an alterna-tive, novel model that performs generation at thephrase level.
Motivated by conditional randomfields (CRF) (Lafferty et al, 2001), a different pa-rameterization of the conditional probability of thehybrid tree that enables the model to encode somelonger range dependencies amongst phrases andMRs is used.
This novel model is referred to asthe tree CRF-based model.Evaluation results for both models are pre-sented, through which we demonstrate that the treeCRF-based model performs better than the directinversion model.
We also compare the tree CRF-based model against the previous state-of-the-artmodel of Wong and Mooney (2007).
Further-more, we evaluate our model on a dataset anno-tated with several natural languages other than En-glish (Japanese, Spanish, and Turkish).
Evalua-tion results show that our proposed tree CRF-basedmodel outperforms the previous model.2 Related WorkThere have been substantial earlier research ef-forts on investigating methods for transformingMR to their corresponding NL sentences.
Mostof the recent systems tackled the problem throughthe architecture of chart generation introduced byKay (1996).
Examples of such systems includethe chart generator for Head-Driven Phrase Struc-ture Grammar (HPSG) (Carroll et al, 1999; Car-roll and Oepen, 2005; Nakanishi et al, 2005), andmore recently for Combinatory Categorial Gram-mar (CCG) (White and Baldridge, 2003; White,2004).
However, most of these systems only fo-cused on surface realization (inflection and order-ing of NL words) and ignored lexical selection(learning the mappings from MR domain conceptsto NL words).The recent work by Wong and Mooney (2007)explored methods for generation by inverting asystem originally designed for semantic pars-ing.
They introduced a system named WASP?1that employed techniques from statistical ma-chine translation using Synchronous Context-FreeGrammar (SCFG) (Aho and Ullman, 1972).
Thesystem took in a linearized MR tree as input, andtranslated it into a natural language sentence asoutput.
Unlike most previous systems, their sys-tem integrated both lexical selection and surfacerealization in a single framework.
The perfor-mance of the system was enhanced by incorpo-rating models borrowed from PHARAOH (Koehn,2004).
Experiments show that this new hybridsystem named WASP?1++ gives state-of-the-artaccuracies and outperforms the direct translationmodel obtained from PHARAOH, when evaluatedon two corpora.
We will compare our system?sperformance against that of WASP?1++ in Sec-tion 5.3 The Hybrid Tree Framework and theLNLZ08 SystemQUERY : answer(RIVER)RIVER : longest(RIVER)RIVER : exclude(RIVER1RIVER2)RIVER : river(all) RIVER : traverse(STATE)STATE : stateid(STATENAME)STATENAME : texaswhat is the longest river thatdoes not run through texasFigure 1: An example MR paired with its NL sen-tence.Following most previous works in thisarea (Kate et al, 2005; Ge and Mooney, 2005;Kate and Mooney, 2006; Wong and Mooney,2006; Lu et al, 2008), we consider MRs in theform of tree structures.
An example MR andits corresponding natural language sentence areshown in Figure 1.
The MR is a tree consistingof nodes called MR productions.
For example,the node ?QUERY : answer(RIVER)?
is one MRproduction.
Each MR production consists of asemantic category (?QUERY?
), a function symbol(?answer?)
which can be optionally omitted, aswell as an argument list which possibly contains401QUERY : answer(RIVER)RIVER : longest(RIVER)RIVER : exclude(RIVER1RIVER2)RIVER : traverse(STATE)STATE : stateid(STATENAME)STATENAME : texastexasrun throughthat does notRIVER : river(all)riverthe longestwhat isFigure 2: One possible hybrid tree T1child semantic categories (?RIVER?
).Now we give a brief overview of the hybrid treeframework and the LNLZ08 system that was pre-sented in Lu et al (2008).
The training corpus re-quired by the LNLZ08 system contains examplepairs d(i)= (?m(i),?w(i)) for i = 1 .
.
.
N , whereeach?m(i)is an MR, and each?w(i)is an NL sen-tence.
The system makes the assumption that theentire training corpus is generated from an under-lying generative model, which is specified by theparameter set ?.The parameter set ?
includes the following: theMR model parameter ?
(mj|mi, argk) which mod-els the generation of an MR production mjfromits parent MR production mias its k-th child, theemission parameter ?(t|mi,?)
that is responsiblefor generation of an NL word or a semantic cate-gory t from the MR production mi(the parent oft) under the context ?
(such as the token to the leftof the current token), and the pattern parameter?
(r|mi), which models the selection of a hybridpattern r that defines globally how the NL wordsand semantic categories are interleaved given aparent MR production mi.
All these parametersare estimated from the corpus during the trainingphase.
The list of possible hybrid patterns is givenin Table 1 (at most two child semantic categoriesare allowed ?
MR productions with more child se-mantic categories are transformed into those withtwo).In the table, m refers to the MR production, thesymbol w denotes an NL word sequence and isoptional if it appears inside [].
The symbol Y andZ refer to the first and second semantic categoryunder the MR production m respectively.# RHS Hybrid Pattern # Patterns0 m ?
w 11 m ?
[w]Y[w] 42m ?
[w]Y[w]Z[w] 8m ?
[w]Z[w]Y[w] 8Table 1: The list of possible hybrid patterns, [] de-notes optionalThe generative process recursively creates MRproductions as well as NL words at each gen-eration step in a top-down manner.
This pro-cess results in a hybrid tree for each MR-NLpair.
The list of children under each MR pro-duction in the hybrid tree forms a hybrid se-quence.
One example hybrid tree for the MR-NL pair given in Figure 1 is shown in Figure 2.In this hybrid tree T1, the list of children underthe production RIVER : longest(RIVER) formsthe hybrid sequence ?the longest RIVER :exclude(RIVER1RIVER2)?.
The yield of the hy-brid tree is exactly the NL sentence.
The MR canalso be recovered from the hybrid tree by record-ing all the internal nodes of the tree, subject to thereordering operation required by the hybrid pat-tern.To illustrate, consider the generation of the hy-brid tree T1shown in Figure 2.
The model firstgenerates an MR production from its parent MRproduction (empty as the MR production is theroot in the MR).
Next, it selects a hybrid patternm ?
wY from the predefined list of hybrid pat-terns, which puts a constraint on the set of all al-lowable hybrid sequences that can be generated:the hybrid sequence must be an NL word sequence402followed by a semantic category.
Finally, actualNL words and semantic categories are generatedfrom the parent MR production.
Now the genera-tion for one level is complete, and the above pro-cess repeats at the newly generated MR produc-tions, until the complete NL sentence and MR areboth generated.Mathematically, the above generative processyields the following formula that models the jointprobability for the MR-NL pair, assuming the con-text ?
for the emission parameter is the precedingword or semantic category (i.e., the bigram modelis assumed, as discussed in Lu et al (2008)):p(T1(?w,?m))= ?
(QUERY : answer(RIVER)|?, arg1)??
(m ?
wY|QUERY : answer(RIVER))??
(what|QUERY : answer(RIVER),BEGIN)??
(is|QUERY : answer(RIVER),what)??
(RIVER|QUERY : answer(RIVER),is)??
(END|QUERY : answer(RIVER),RIVER)??
(RIVER : longest(RIVER)|QUERY : answer(RIVER), arg1)?
.
.
.
(1)where T1(?w,?m) denotes the hybrid tree T1whichcontains the NL sentence?w and MR?m.For each MR-NL pair in the training set, therecan be potentially many possible hybrid trees asso-ciated with the pair.
However, the correct hybridtree is completely unknown during training.
Thecorrect hybrid tree is therefore treated as a hiddenvariable.
An efficient inside-outside style algo-rithm (Baker, 1979) coupled with further dynamicprogramming techniques is used for efficient pa-rameter estimation.During the testing phase, the system makes useof the learned model parameters to determine themost probable hybrid tree given a new natural lan-guage sentence.
The MR contained in that hybridtree is the output of the system.
Dynamic pro-gramming techniques similar to those of trainingare also employed for efficient decoding.The generative model used in the LNLZ08 sys-tem has a natural symmetry, allowing for easytransformation from NL to MR, as well as fromMR to NL.
This provides the starting point for ourwork in ?inverting?
the LNLZ08 system to gener-ate natural language sentences from the underly-ing meaning representations.4 Generation with Hybrid TreesThe task of generating NL sentences from MRscan be defined as follows.
Given a training cor-pus consisting of MRs paired with their NL sen-tences, one needs to develop algorithms that learnhow to effectively ?paraphrase?
MRs with natu-ral language sentences.
During testing, the sys-tem should be able to output the most probable NL?paraphrase?
for a given new MR.The LNLZ08 system models p(T (?w,?m)), thejoint generative process for the hybrid tree con-taining both NL and MR.
This term can be rewrit-ten in the following way:p(T (?w,?m)) = p(?m)?
p (T (?w,?m)|?m) (2)In other words, we reach an alternative view ofthe joint generative process as follows.
We chooseto generate the complete MR?m first.
Given?m, wegenerate hybrid sequences below each of its MRproduction, which gives us a complete hybrid treeT (?w,?m).
The NL sentence?w can be constructedfrom this hybrid tree exactly.We define an operation yield(T ) which returnsthe NL sentence as the yield of the hybrid tree T .Given an MR?m, we find the most probable NLsentence?w?as follows:?w?= yield(argmaxTp(T |?m))(3)In other words, we first find the most probablehybrid tree T that contains the provided MR?m.Next we return the yield of T as the most probableNL sentence.Different assumptions can be made in the pro-cess of finding the most probable hybrid tree.
Wefirst describe a simple model which is a direct in-version of the LNLZ08 system.
This model, as abaseline model, generates a complete NL sentenceword by word.
Next, a more sophisticated modelthat exploits NL phrase-level dependencies is builtthat tackles some weaknesses of the simple base-line model.4.1 Direct Inversion ModelAssume that a pre-order traversal of theMR?m gives us the list of MR productionsm1,m2, .
.
.
,mS, where S is the number of MRproductions in?m.
Based on the independenceassumption made by the LNLZ08 system, eachMR production independently generates a hybrid403sequence.
Denote the hybrid sequence gener-ated under the MR production msas hs, fors = 1, .
.
.
, S. We call the list of hybrid sequencesh = ?h1, h2, .
.
.
, hS?
a hybrid sequence listassociated with this particular MR.
Thus, our goalis to find the optimal hybrid sequence list h?forthe given MR?m, which is formulated as follows:h?= ?h?1, .
.
.
, h?S?
= argmaxh1,...,hSS?s=1p(hs|ms) (4)The optimal hybrid sequence list defines the op-timal hybrid tree whose yield gives the optimal NLsentence.Due to the strong independence assumption in-troduced by the LNLZ08 system, the hybrid treegeneration process is in fact highly decompos-able.
Optimization of the hybrid sequence list?h1, .
.
.
, hS?
can be performed individually sincethey are independent of one another.
Thus, math-ematically, for s = 1, .
.
.
, S, we have:h?s= argmaxhsp(hs|ms) (5)The LNLZ08 system presented three models forthe task of transforming NL to MR.
In this in-verse task, for generation of a hybrid sequence,we choose to use the bigram model (model II).
Wechoose this model mainly due to its stronger abil-ity in modeling dependencies between adjacentNL words, which we believe to be quite importantin this NL generation task.
With the bigram modelassumption, the optimal hybrid sequence that canbe generated from each MR production is definedas follows:h?s= argmaxhsp(hs|ms)= argmaxhs{?(r|ms)?|hs|+1?j=1?
(tj|ms, tj?1)}(6)where tiis either an NL word or a semantic cat-egory with t0?
BEGIN and t|hs|+1?
END, andr is the hybrid pattern that matches the hybrid se-quence hs, which is equivalent to t1, .
.
.
, t|hs|.Equivalently, we can view the problem in thelog-space:h?s= argminhs{?
log ?(r|ms)+|hs|+1?j=1?
log ?
(tj|ms, tj?1)}(7)Note the term ?
log ?
(r|ms) is a constant fora particular MR production msand a particu-lar hybrid pattern r. This search problem canbe equivalently cast as the shortest path problemwhich can be solved efficiently with Dijkstra?s al-gorithm (Cormen et al, 2001).
We define a setof states.
Each state represents a single NL wordor a semantic category, including the special sym-bols BEGIN and END.
A directed path betweentwo different states tuand tvis associated witha distance measure ?
log ?
(tv|ms, tu), which isnon-negative.
The task now is to find the short-est path between BEGIN and END1.
The sequenceof words appearing in this path is simply the mostprobable hybrid sequence under this MR produc-tion ms. We build this model by directly invertingthe LNLZ08 system, and this model is thereforereferred to as the direct inversion model.A major weakness of this baseline model is thatit encodes strong independence assumptions dur-ing the hybrid tree generation process.
Thoughshown to be effective in the task of transform-ing NL to MR, such independence assumptionsmay introduce difficulties in this NLG task.
Forexample, consider the MR shown in Figure 1.The generation steps of the hybrid sequencesfrom the two adjacent MR productions QUERY :answer(RIVER) and RIVER : longest(RIVER)are completely independent of each other.
Thismay harm the fluency of the generated NL sen-tence, especially when a transition from one hy-brid sequence to another is required.
In fact, dueto such an independence assumption, the modelalways generates the same hybrid sequence fromthe same MR production, regardless of its contextsuch as parent or child MR productions.
Such alimitation points to the importance of better uti-lizing the tree structure of the MR for this NLGtask.
Furthermore, due to the bigram assumption,the model is unable to capture longer range depen-dencies amongst the words or semantic categoriesin each hybrid sequence.To tackle the above issues, we explore ways ofrelaxing various assumptions, which leads to an1In addition, we should make sure that the generated hy-brid sequence t0.
.
.
t|hs|+1is a valid hybrid sequence thatcomply with the hybrid pattern r. For example, the MRproduction STATE : loc 1(RIVER) can generate the follow-ing hybrid sequence ?BEGIN have RIVER END?
but notthis hybrid sequence ?BEGIN have END?.
This can beachieved by finding the shortest path from BEGIN to RIVER,which then gets concatenated to the shortest path from RIVERto END.404QUERY : answer(RIVER)RIVER : longest(RIVER)RIVER : exclude(RIVER1RIVER2)RIVER : river(all) RIVER : traverse(STATE)STATE : stateid(STATENAME)STATENAME : texaswhat is RIVER1the longest RIVER1RIVER1that does not RIVER2river run through STATE1STATENAME1texasFigure 3: An MR (left) and its associated hybrid sequences (right)alternative model as discussed next.4.2 Tree CRF-Based ModelBased on the belief that using known phrases usu-ally leads to better fluency in the NLG task (Wongand Mooney, 2007), we explore methods for gen-erating an NL sentence at phrase level rather thanat word level.
This is done by generating hybridsequences as complete objects, rather than sequen-tially one word or semantic category at a time,from MR productions.We assume that each MR production can gen-erate a complete hybrid sequence below it from afinite set of possible hybrid sequences.
Each suchhybrid sequence is called a candidate hybrid se-quence associated with that particular MR produc-tion.
Given a set of candidate hybrid sequences as-sociated with each MR production, the generationtask is to find the optimal hybrid sequence list h?for a given MR?m:h?= argmaxhp(h|?m) (8)Figure 3 shows a complete MR, as well as apossible tree that contains hybrid sequences as-sociated with the MR productions.
For exam-ple, in the figure the MR production RIVER :traverse(STATE) is associated with the hybrid se-quence run through STATE1.
Each MR pro-duction can be associated with potentially manydifferent hybrid sequences.
The task is to deter-mine the most probable list of hybrid sequences asthe ones appearing on the right of Figure 3, one foreach MR production.To make better use of the tree structure of MR,we take the approach of modeling the conditionaldistribution using a log-linear model.
Followingthe conditional random fields (CRF) framework(Lafferty et al, 2001), we can define the probabil-ity of the hybrid sequence list given the completeMR?m, as follows:p(h|?m) =1Z(?m)exp(?i?V?k?kgk(hi,?m, i)+?
(i,j)?E?k?kfk(hi, hj,?m, i, j))(9)where V is the set of all the vertices in the tree, andE is the set of the edges in the tree, consisting ofparent-child pairs.
The function Z(?m) is the nor-malization function.
Note that the dependenciesamong the features here form a tree, unlike the se-quence models used in Lafferty et al (2001).
Thefunction fk(hi, hj,?m, i, j) is a feature function ofthe entire MR tree?m and the hybrid sequences atvertex i and j.
These features are usually referredto as the edge features in the CRF framework.
Thefunction gk(hi,?m, i) is a feature function of thehybrid sequence at vertex i and the entire MR tree.These features are usually referred to as the vertexfeatures.
The parameters ?kand ?kare learnedfrom the training data.In this task, we are given only MR-NL pairsand do not have the hybrid tree corresponding toeach MR as training data.
Now we describe howthe set of candidate hybrid sequences for each MRproduction is obtained as well as how the train-ing data for this model is constructed.
After thejoint generative model is learned as done in Lu etal.
(2008), we first use a Viterbi algorithm to findthe optimal hybrid tree for each MR-NL pair inthe training set.
From each optimal hybrid tree,we extract the hybrid sequence hibelow each MRproduction mi.
Using this process on the train-ing MR-NL pairs, we can obtain a set of candidate405hybrid sequences that can be associated with eachMR production.
The optimal hybrid tree generatedby the Viterbi algorithm in this way is consideredthe ?correct?
hybrid tree for theMR-NL pair and isused as training data.
While this does not providehand-labeled training data, we believe the hybridtrees generated this way form a high quality train-ing set as both the MR and NL are available whenViterbi decoding is performed, guaranteeing thatthe generated hybrid tree has the correct yield.There exist several advantages of such a modelover the simple generative model.
First, this modelallows features that specifically model the depen-dencies between neighboring hybrid sequences inthe tree to be used.
In addition, the model can effi-ciently capture long range dependencies betweenMR productions and hybrid sequences since eachhybrid sequence is allowed to depend on the entireMR tree.For features, we employ four types of simplefeatures, as presented below.
Note that the firstthree types of features are vertex features, and thelast are edge features.
Examples are given basedon Figure 3.
All the features are indicator func-tions, i.e., a feature takes value 1 if a certain com-bination is present, and 0 otherwise.
The last threefeatures explicitly encode information from thetree structure of MR.Hybrid sequence features : one hybrid sequencetogether with the associated MR production.For example:g1: ?run through STATE1,RIVER : traverse(STATE)?
;Two-level hybrid sequence features : one hy-brid sequence, its associated MR production,and the parent MR production.
For example:g2: ?run through STATE1,RIVER : traverse(STATE),RIVER : exclude(RIVER1,RIVER2)?
;Three-level hybrid sequence features : one hy-brid sequence, its associated MR production,the parent MR production, and the grandpar-ent MR production.
For example:g3: ?run through STATE1,RIVER : traverse(STATE),RIVER : exclude(RIVER1,RIVER2),RIVER : longest(RIVER)?
;Adjacent hybrid sequence features : two adja-cent hybrid sequences, together with their as-sociated MR productions.
For example:f1: ?run through STATE1,RIVER1that does not RIVER2,RIVER : traverse(STATE),RIVER : exclude(RIVER1,RIVER2)?
.For training, we use the feature forest model(Miyao and Tsujii, 2008), which was originallydesigned as an efficient algorithm for solving max-imum entropy models for data with complex struc-tures.
The model enables efficient training overpacked trees that potentially represent exponen-tial number of trees.
The tree conditional randomfields model can be effectively represented usingthe feature forest model.
The model has also beensuccessfully applied to the HPSG parsing task.To train the model, we run the Viterbi algorithmon the trained LNLZ08 model and perform convexoptimization using the feature forest model.
TheLNLZ08 model is trained using an EM algorithmwith time complexity O(MN3D) per EM itera-tion, where M and N are respectively the maxi-mum number of MR productions and NL wordsfor each MR-NL pair, and D is the number oftraining instances.
The time complexity of theViterbi algorithm is also O(MN3D).
For trainingthe feature forest, we use the Amis toolkit (Miyaoand Tsujii, 2002) which utilizes the GIS algorithm.The time complexity for each iteration of the GISalgorithm is O(MK2D), where K is the maxi-mum number of candidate hybrid sequences asso-ciated with each MR production.
Finally, the timecomplexity for generating a natural language sen-tence from a particular MR is O(MK2).5 ExperimentsIn this section, we present the results of our sys-tems when evaluated on two standard benchmarkcorpora.
The first corpus is GEOQUERY, whichcontains Prolog-based MRs that can be used toquery a US geographic database (Kate et al,2005).
Our task for this domain is to generateNL sentences from the formal queries.
The secondcorpus is ROBOCUP.
This domain contains MRswhich are instructions written in a formal languagecalled CLANG.
Our task for this domain is to gen-erate NL sentences from the coaching advice writ-ten in CLANG.406GEOQUERY (880) ROBOCUP (300)BLEU NIST BLEU NISTDirect inversion model 0.3973 5.5466 0.5468 6.6738Tree CRF-based model 0.5733 6.7459 0.6220 6.9845Table 2: Results of automatic evaluation of both models (bold type indicates the best performing system).GEOQUERY (880) ROBOCUP (300)BLEU NIST BLEU NISTWASP?1++ 0.5370 6.4808 0.6022 6.8976Tree CRF-based model 0.5733 6.7459 0.6220 6.9845Table 3: Results of automatic evaluation of our tree CRF-based model and WASP?1++.English Japanese Spanish TurkishBLEU NIST BLEU NIST BLEU NIST BLEU NISTWASP?1++ 0.6035 5.7133 0.6585 4.6648 0.6175 5.7293 0.4824 4.3283Tree CRF-based model 0.6265 5.8907 0.6788 4.8486 0.6382 5.8488 0.5096 4.5033Table 4: Results on the GEOQUERY-250 corpus with 4 natural languages.The GEOQUERY domain contains 880 in-stances, while the ROBOCUP domain contains 300instances.
The average NL sentence length for thetwo corpora are 7.57 and 22.52 respectively.
Fol-lowing the evaluation methodology of Wong andMooney (2007), we performed 4 runs of the stan-dard 10-fold cross validation and report the aver-aged performance in this section using the stan-dard automatic evaluation metric BLEU (Papineniet al, 2002) and NIST (Doddington, 2002)2.
TheBLEU and NIST scores of the WASP?1++ sys-tem reported in this section are obtained fromthe published paper of Wong and Mooney (2007).Note that to make our experimental results directlycomparable to Wong and Mooney (2007), we usedthe identical training and test data splits for the 4runs of 10-fold cross validation used by Wong andMooney (2007) on both corpora.Our system has the advantage of always pro-ducing an NL sentence given any input MR, evenif there exist unseen MR productions in the inputMR.
We can achieve this by simply skipping thoseunseen MR productions during the generation pro-cess.
However, in order to make a fair comparisonagainst WASP?1++, which can only generate NLsentences for 97% of the input MRs, we also donot generate any NL sentence in the case of ob-serving an unseen MR production.
All the evalu-ations discussed in this section follow this evalu-2We used the official evaluation script (version 11b) pro-vided by http://www.nist.gov/.ation methodology, but we notice that empiricallyour system is able to achieve higher BLEU/NISTscores if we allow generation for those MRs thatinclude unseen MR productions.5.1 Comparison between the two modelsWe compare the performance of our two modelsin Table 2.
From the table, we observe that thetree CRF-based model outperforms the direct in-version model on both domains.
This validatesour earlier belief that some long range dependen-cies are important for the generation task.
In ad-dition, while the direct inversion model performsreasonably well on the ROBOCUP domain, it per-forms substantially worse on the GEOQUERY do-main where the sentence length is shorter.
We notethat the evaluation metrics are strongly correlatedwith the cumulative matching n-grams betweenthe output and the reference sentence (n rangesfrom 1 to 4 for BLEU, and 1 to 5 for NIST).
Thedirect inversion model fails to capture the transi-tional behavior from one phrase to another, whichmakes it more vulnerable to n-gram mismatch, es-pecially when evaluated on the GEOQUERY cor-pus where phrase-to-phrase transitions are morefrequent.
On the other hand, the tree CRF-basedmodel does not suffer from this problem, mainlydue to its ability to model such dependencies be-tween neighboring phrases.
Sample outputs fromthe two models are shown in Figure 4.407Reference: what is the largest state bordering texasDirect inversion model: what the largest states border texasTree CRF-based model: what is the largest state that borders texasReference: if DR2C7 is true then players 2 , 3 , 7 and 8should pass to player 4Direct inversion model: if DR2C7 , then players 2 , 3 7 and 8 shouldball to player 4Tree CRF-based model: if the condition DR2C7 is true then players 2 ,3 , 7 and 8 should pass to player 4Figure 4: Sample outputs from the two models, for GEOQUERY domain (top) and ROBOCUP domain(bottom) respectively.5.2 Comparison with previous modelWe also compare the performance of our tree CRF-based model against the previous state-of-the-artsystem WASP?1++ in Table 3.
Our tree CRF-basedmodel achieves better performance on both cor-pora.
We are unable to carry out statistical sig-nificance tests since the detailed BLEU and NISTscores of the cross validation runs of WASP?1++as reported in the published paper of Wong andMooney (2007) are not available.The results confirm our earlier discussions: thedependencies between the generated NL wordsare important and need to be properly modeled.The WASP?1++ system uses a log-linear modelwhich incorporates two major techniques to at-tempt to model such dependencies.
First, a back-off language model is used to capture dependen-cies at adjacent word level.
Second, a techniquethat merges smaller translation rules into a singlerigid rule is used to capture dependencies at phraselevel (Wong, 2007).
In contrast, the proposed treeCRF-based model is able to explicitly and flexiblyexploit phrase-level features that model dependen-cies between adjacent phrases.
In fact, with thehybrid tree framework, the better treatment of thetree structure of MR enables us to model some cru-cial dependencies between the complete MR treeand generated NL phrases.
We believe that thisproperty plays an important role in improving thequality of the generated sentences in terms of flu-ency, which is assessed by the evaluation metrics.Furthermore, WASP?1++ employs minimumerror rate training (Och, 2003) to directly optimizethe evaluation metrics.
We have not done so butstill obtain better performance.
In future, we planto explore ways to directly optimize the evaluationmetrics in our system.5.3 Experiments on different languagesFollowing the work of Wong and Mooney (2007),we also evaluated our system?s performance ona subset of the GEOQUERY corpus with 250 in-stances, where sentences of 4 natural languages(English, Japanese, Spanish, and Turkish) areavailable.
The evaluation results are shown in Ta-ble 4.
Our tree CRF-based model achieves betterperformance on this task compared to WASP?1++.We are again unable to conduct statistical signifi-cance tests for the same reason reported earlier.6 ConclusionsIn this paper, we presented two novel models forthe task of generating natural language sentencesfrom given meaning representations, under a hy-brid tree framework.
We first built a simple di-rect inversion model as a baseline.
Next, to ad-dress the limitations associated with the direct in-version model, a tree CRF-based model was in-troduced.
We evaluated both models on standardbenchmark corpora.
Evaluation results show thatthe tree CRF-based model performs better than thedirect inversion model, and that the tree CRF-basedmodel also outperforms WASP?1++, which was aprevious state-of-the-art system reported in the lit-erature.AcknowledgmentsThe authors would like to thank Seung-Hoon Nafor his suggestions on the presentation of this pa-per, Yuk Wah Wong for answering various ques-tions related to the WASP?1++ system, and theanonymous reviewers for their thoughtful com-ments on this work.408ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1972.
TheTheory of Parsing, Translation and Compiling.Prentice-Hall, Englewood Clis, NJ.James K. Baker.
1979.
Trainable grammars for speechrecognition.
In Proceedings of the Spring Confer-ence of the Acoustical Society of America, pages547?550, Boston, MA, June.John Carroll and Stephan Oepen.
2005.
High ef-ficiency realization for a wide-coverage unificationgrammar.
In Proceedings of the 2nd InternationalJoint Conference on Natural Language Processing(IJCNLP 2005), pages 165?176.John Carroll, Ann Copestake, Dan Flickinger, and Vic-tor Poznanski.
1999.
An efficient chart generatorfor (semi-) lexicalist grammars.
In Proceedings ofthe 7th European Workshop on Natural LanguageGeneration (EWNLG 1999), pages 86?95.Thomas H. Cormen, Charles E. Leiserson, Ronald L.Rivest, and Clifford Stein.
2001.
Introduction toAlgorithms (Second Edition).
MIT Press.George Doddington.
2002.
Automatic evaluationof machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the 2nd In-ternational Conference on Human Language Tech-nology Research (HLT 2002), pages 138?145.Ruifang Ge and Raymond J. Mooney.
2005.
A statis-tical semantic parser that integrates syntax and se-mantics.
In Proceedings of the 9th Conference onComputational Natural Language Learning (CoNLL2005), pages 9?16.Rohit J. Kate and Raymond J. Mooney.
2006.
Us-ing string-kernels for learning semantic parsers.
InProceedings of the 21st International Conferenceon Computational Linguistics and the 44th AnnualMeeting of the Association for Computational Lin-guistics (COLING/ACL 2006), pages 913?920.Rohit J. Kate, Yuk Wah Wong, and Raymond J.Mooney.
2005.
Learning to transform natural to for-mal languages.
In Proceedings of the 20th NationalConference on Artificial Intelligence (AAAI 2005),pages 1062?1068.Martin Kay.
1996.
Chart generation.
In Proceedingsof the 34th Annual Meeting of the Association forComputational Linguistics (ACL 1996), pages 200?204.Philipp Koehn.
2004.
Pharaoh: a beam search de-coder for phrase-based statistical machine transla-tion models.
In Proceedings of the 6th Conferenceof the Association for Machine Translation in theAmericas (AMTA 2004), pages 115?124.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the 18th Inter-national Conference on Machine Learning (ICML2001), pages 282?289.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S.Zettlemoyer.
2008.
A generative model for pars-ing natural language to meaning representations.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2008), pages 783?792.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximumentropy estimation for feature forests.
In Proceed-ings of the 2nd International Conference on HumanLanguage Technology Research (HLT 2002), pages292?297.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Compu-tational Linguistics, 34(1):35?80.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic models for disambiguation of anHPSG-based chart generator.
In Proceedings of the9th International Workshop on Parsing Technologies(IWPT 2005), volume 5, pages 93?102.Franz J. Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics (ACL 2003), pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL 2002), pages 311?318.Michael White and Jason Baldridge.
2003.
Adaptingchart realization to CCG.
In Proceedings of the 9thEuropean Workshop on Natural Language Genera-tion (EWNLG 2003), pages 119?126.Michael White.
2004.
Reining in CCG chart realiza-tion.
In Proceeding of the 3rd International Confer-ence on Natural Language Generation (INLG 2004),pages 182?191.Yuk Wah Wong and Raymond J. Mooney.
2006.Learning for semantic parsing with statistical ma-chine translation.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics (HLT/NAACL 2006), pages 439?446.Yuk Wah Wong and Raymond J. Mooney.
2007.Generation by inverting a semantic parser that usesstatistical machine translation.
In Proceedings ofthe Human Language Technology Conference ofthe North American Chapter of the Associationfor Computational Linguistics (NAACL/HLT 2007),pages 172?179.Yuk Wah Wong.
2007.
Learning for Semantic Parsingand Natural Language Generation Using StatisticalMachine Translation Techniques.
Ph.D. thesis, TheUniversity of Texas at Austin.409
