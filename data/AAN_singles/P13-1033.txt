Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 333?342,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Markov Model of Machine Translation usingNon-parametric Bayesian InferenceYang Feng and Trevor CohnDepartment of Computer ScienceThe University of SheffieldSheffield, United Kingdomyangfeng145@gmail.com and t.cohn@sheffield.ac.ukAbstractMost modern machine translation systemsuse phrase pairs as translation units, al-lowing for accurate modelling of phrase-internal translation and reordering.
How-ever phrase-based approaches are muchless able to model sentence level effectsbetween different phrase-pairs.
We pro-pose a new model to address this im-balance, based on a word-based Markovmodel of translation which generates tar-get translations left-to-right.
Our modelencodes word and phrase level phenom-ena by conditioning translation decisionson previous decisions and uses a hierar-chical Pitman-Yor Process prior to pro-vide dynamic adaptive smoothing.
Thismechanism implicitly supports not onlytraditional phrase pairs, but also gappingphrases which are non-consecutive in thesource.
Our experiments on Chinese toEnglish and Arabic to English translationshow consistent improvements over com-petitive baselines, of up to +3.4 BLEU.1 IntroductionRecent years have witnessed burgeoning develop-ment of statistical machine translation research,notably phrase-based (Koehn et al, 2003) andsyntax-based approaches (Chiang, 2005; Galleyet al, 2006; Liu et al, 2006).
These approachesmodel sentence translation as a sequence of sim-ple translation decisions, such as the applicationof a phrase translation in phrase-based methodsor a grammar rule in syntax-based approaches.In order to simplify modelling, most MT mod-els make an independence assumption, stating thatthe translation decisions in a derivation are in-dependent of one another.
This conflicts withthe intuition behind phrase-based MT, namely thattranslation decisions should be dependent on con-text.
On one hand, the use of phrases can mem-orize local context and hence helps to generatebetter translation compared to word-based models(Brown et al, 1993; Och and Ney, 2003).
On theother hand, this mechanism requires each phraseto be matched strictly and to be used as a whole,which precludes the use of discontinuous phrasesand leads to poor generalisation to unseen data(where large phrases tend not to match).In this paper we propose a new model to dropthe independence assumption, by instead mod-elling correlations between translation decisions,which we use to induce translation derivationsfrom aligned sentences (akin to word alignment).We develop a Markov model over translation de-cisions, in which each decision is conditioned onprevious n most recent decisions.
Our approachemploys a sophisticated Bayesian non-parametricprior, namely the hierarchical Pitman-Yor Process(Teh, 2006; Teh et al, 2006) to represent back-off from larger to smaller contexts.
As a result,we need only use very simple translation units?
primarily single words, but can still describecomplex multi-word units through correlations be-tween their component translation decisions.
Wefurther decompose the process of generating eachtarget word into component factors: finishing thetranslating, jumping elsewhere in the source, emit-ting a target word and deciding the fertility of thesource words.Overall our model has the following features:1. enabling model parameters to be shared be-tween similar translation decisions, therebyobtaining more reliable statistics and gener-alizing better from small training sets.2.
learning a much richer set of transla-tion fragments, such as gapping phrases,e.g., the translation for the German werde.
.
.
ankommen in English is will arrive .
.
.
.3. providing a unifying framework spanningword-based and phrase-based model of trans-lation, while incorporating explicit transla-333tion, insertion, deletion and reordering com-ponents.We demonstrate our model on Chinese-Englishand Arabic-English translation datasets.
Themodel produces uniformly better translations thanthose of a competitive phrase-based baseline,amounting to an improvement of up to 3.4 BLEUpoints absolute.2 Related WorkWord based models have a long history in machinetranslation, starting with the venerable IBM trans-lation models (Brown et al, 1993) and the hid-den Markov model (Vogel et al, 1996).
Thesemodels are still in wide-spread use today, albeitonly as a preprocessing step for inferring wordlevel alignments from sentence-aligned parallelcorpora.
They combine a number of factors, in-cluding distortion and fertility, which have beenshown to improve word-alignment and translationperformance over simpler models.
Our approachis similar to these works, as we also develop aword-based model, and explicitly consider simi-lar translation decisions, alignment jumps and fer-tility.
We extend these works in two importantrespects: 1) while they assume a simple parame-terisation by making iid assumptions about eachtranslation factor, we instead allow for rich cor-relations by modelling sequences of translationdecisions; and 2) we develop our model in theBayesian framework, using a hierarchical Pitman-Yor Process prior with rich backoff semantics be-tween high and lower order sequences of transla-tion decisions.
Together this results in a modelwith rich expressiveness but can still generalizewell to unseen data.More recently, a number of authors have pro-posed Markov models for machine translation.Vaswani et al (2011) propose a rule Markovmodel for a tree-to-string model which modelscorrelations between pairs of mininal rules, anduse Kneser-Ney smoothing to alleviate the prob-lems of data sparsity.
Similarly, Crego et al(2011) develop a bilingual language model whichincorporates words in the source and target lan-guages to predict the next unit, which they use asa feature in a translation system.
This line of workwas extended by Le et al (2012) who develop anovel estimation algorithm based around discrimi-native projection into continuous spaces.
Also rel-evant is Durrani et al (2011), who present a se-quence model of translation including reordering.Our work also uses bilingual information, usingthe source words as part of the conditioning con-text.
In contrast to these approaches which pri-marily address the decoding problem, we focus onthe learning problem of inferring alignments fromparallel sentences.
Additionally, we develop a fullgenerative model using a Bayesian prior, and in-corporate additional factors besides lexical items,namely jumps in the source and word fertility.Another aspect of this paper is the implicit sup-port for phrase-pairs that are discontinous in thesource language.
This idea has been developedexplicitly in a number of previous approaches, ingrammar based (Chiang, 2005) and phrase-basedsystems (Galley and Manning, 2010).
The latter ismost similar to this paper, and shows that discon-tinuous phrases compliment standard contiguousphrases, improving expressiveness and translationperformance.
Unlike their work, here we developa complimentary approach by constructing a gen-erative model which can induce these rich rulesdirectly from sentence-aligned corpora.3 ModelGiven a source sentence, our model infers a la-tent derivation which produces a target translationand meanwhile gives a word alignment betweenthe source and the target.
We consider a pro-cess in which the target string is generated usinga left-to-right order, similar to the decoding strat-egy used by phrase-based machine translation sys-tems (Koehn et al, 2003).
During this process wemaintain a position in the source sentence, whichcan jump around to allow for different sentenceordering in the target vs. source languages.
Incontrast to phrase-based models, we use words asour basic translation unit, rather than multi-wordphrases.
Furthermore, we decompose the deci-sions involved in generating each target word toa number of separate factors, where each factor ismodelled separately and conditioned on a rich his-tory of recent translation decisions.3.1 Markov TranslationOur model generates target translation left-to-right word by word.
The generative processemploys the following recursive procedure toconstruct the target sentence conditioned on thesource:i?
1while Not finished doDecide whether to finish the translation, ?i334Step Source sentence Translation finish jump emission0 Je le prends1 Je le prends I no monotone Je?
I2 Je le prends I ?ll no insert null?
?ll3 Je le prends I ?ll take no forward prends?
take4 Je le prends I ?ll take that no backward le?
that5 Je le prends I ?ll take that one no stay le?
one6 Je le prends I ?ll take that one yesFigure 1: Translation agenda of Je le prends?
I ?ll take that one.if ?i = false thenSelect a source word to jump toEmit a target word for the source wordend ifi?
i+ 1end whileIn the generation of each target word, our modelincludes three separate factors: the binary finishdecision, a jump decision to move to a differentsource word, and emission which translates or oth-erwise inserts a word in the target string.
This gen-erative process resembles the sequence of transla-tion decisions considered by a standard MT de-coder (Koehn et al, 2003), but note that our ap-proach differs in that there is no constraint that allwords are translated exactly once.
Instead sourcewords can be skipped or repeatedly translated.This makes the approach more suitable for learn-ing alignments, e.g., to account for word fertilities(see ?3.3), while also permitting inference usingGibbs sampling (?4).More formally, we can express our probabilisticmodel aspbs(eI1, aI1|fJ1 ) =I+1?i=1p(?i|f i?1ai?n, ei?1i?n)?I?i=1p(?i|f i?1ai?n, ei?1i?n)?I?i=1p(ei|?i, f iai?n, ei?1i?n) (1)where ?i is the finish decision for target posi-tion i, ?i is the jump decision to source word faiand f iai?n is the source words for target positionsi ?
n, i ?
n + 1, ..., i.
Each of the three distribu-tions (finish, jump and emission) is drawn respec-tive from hierarchical Pitman-Yor Process priors,as described in Section 3.2.The jump decision ?i in Equation 1 demandsfurther explanation.
Instead of modelling jumpdistances explicitly, which poses problems forgeneralizing between different lengths of sen-tences and general parameter explosion, we con-sider a small handful of types of jump based onthe distance between the current source word aiand the previous source word ai?1, i.e., di =ai ?
ai?1.1 We bin jumps into five types:a) insert;b) backward, if di < 0;c) stay, if di = 0;d) monotone, if di = 1;e) forward, if di > 1.The special jump type insert handles null align-ments, denoted ai = 0 which licence spurious in-sertions in the target string.To illustrate this translation process, Figure 1shows the example translation <Je le prends, I ?lltake that one>.
Initially we set the source positionbefore the first source word Je.
Then in step 1,we decide not to finish (finish=no), jump to sourceword Je and translate it as I.
Next, we again de-cide not to finish, jump to the null source wordand insert ?ll.
The process continues until in step6 we elect to finish (finish=yes), at which point thetranslation is complete, with target string I ?ll takethat one.3.2 Hierarchical Pitman-Yor ProcessThe Markov assumption limits the context of eachdistribution to the n most recent translation deci-sions, which limits the number of model param-eters.
However for any non-trivial value n >0, overfitting is a serious concern.
We counterthe problem of a large parameter space using aBayesian non-parametric prior, namely the hier-archical Pitman-Yor Process (PYP).
The PYP de-scribes distributions over possibly infinite eventspaces that follow a power law, with few eventstaking the majority of the probability mass and along tail of less frequent events.
We consider a hi-erarchical PYP, where a sequence of chained PYP1For a target position aligned to null, we denote its sourceword as null and set its aligned source position as that of theprevious target word that is aligned to non-null.335priors allow backoff from larger to smaller con-texts such that our model can learn rich contextualmodels for known (large) contexts while also stillbeing able to generalize well to unseen contexts(using smaller histories).3.2.1 Pitman-Yor ProcessA PYP (Pitman and Yor, 1997) is defined by itsdiscount parameter 0 ?
a < 1, strength parameterb > ?a and base distribution G0.
For a distri-bution drawn from a PYP, G ?
PYP(a, b,G0),marginalising out G leads to a simple distributionwhich can be described using a variant of the Chi-nese Restaurant Process (CRP).
In this analogy weimagine a restaurant has an infinite number of ta-bles and each table can accommodate an infinitenumber of customers.
Each customer (a samplefrom G) walks in one at a time and seats them-selves at a table.
Finally each table is served acommunal dish (a draw from G0), which is servedto each customer seated at the table.
The assign-ment of customers to tables is such that populartables are more likely to be chosen, and this rich-get-richer dynamic produces power-law distribu-tions with few events (the dishes at popular tables)dominating the distribution.More formally, at time n a customer enters andselects a table k which is either a table having beenseated (1 ?
k ?
K?)
or an empty table (k =K?
+ 1) byp(tn = k|t?n) ={c?tk?an?1+b 1 ?
k ?
K?aK?+bn?1+b k = K?
+ 1where tn is the table selected by the customer n,t?n is the seating arrangement of previous n ?
1customers, c?tk is the number of customers seatedat table k in t?n andK?
= K(t?n) is the numberof tables in t?n.If the customer sits at an empty table, a dish his served to his table by the probability of G0(h),otherwise, he can only share with others the dishhaving been served to his table.2 Overall, the prob-ability of the customer being served a dish h isp(on = h|t?n,o?n) =c?oh ?
aK?hn?
1 + b+ (aK?
+ b)n?
1 + b G0(h)where on is the dish served to the customer n, o?nis the dish accommodation of previous n?
1 cus-tomers, c?oh is the number of customers who are2We also say the customer is served with this dish.served with the dish h in o?n and K?h is the num-ber of tables served with the dish h in t?n.The hierarchical PYP (hPYP; Teh (2006)) is anextension of the PYP in which the base distribu-tion G0 is itself a PYP distribution.
This parent(base) distribution can itself have a PYP as a basedistribution, giving rise to hierarchies of arbitrarydepth.
Like the PYP, inference under the hPYPcan be also described in terms of CRP wherebyeach table in one restaurant corresponds to a dishin the next deeper level, and is said to share thesame dish.
Whenever an empty table is seated inone level, a customer must enter the restaurant inthe next deeper level and find a table to sit.
Thisprocess continues until the customer is assigned ashared table or the deepest level of the hierarchyis reached.
A similar process occurs when a cus-tomer leaves, where newly emptied tables must bepropagated up the hierarchy in the form of depart-ing customers.
There is not space for a completetreatment of the hPYP and the particulars of infer-ence; we refer the interested reader to Teh (2006).3.2.2 A Hierarchical PYP Translation ModelWe draw the distributions for the various transla-tion factors from respective hierarchical PYP pri-ors, as shown in Figure 2 for the finish, jump andemission factors.
For the emission factor (Fig-ure 2c), we draw the target word ei from a distribu-tion conditioned on the last two source and targetwords, as well as the current source word, fai andthe current jump type ?i.
Here the draw of a tar-get word corresponds to a customer entering andwhich target word to emit corresponds to whichdish to be served to the customer in the CRP.
Thehierarchical prior encodes a backoff path in whichthe jump type is dropped first, followed by pairs ofsource and target words from least recent to mostrecent.
The final backoff stages drop the currentsource word, terminating with the uniform basedistribution over the target vocabulary V .The distributions over the other two factors inFigure 2 follow a similar pattern.
Note howeverthat these distributions don?t condition on the cur-rent source word, and consequently have fewerlevels of backoff.
The terminating base distribu-tion for the finish factor is a uniform distributionwith equal probability for finishing versus contin-uing.
The jump factor has an additional condition-ing variable t which encodes whether the previousalignment is near the start or end of the source sen-tence.
This information affects which of the jumpvalues are legal from the current position, such336?i|f i?1ai?2, ei?1i?2 ?
G?f i?1ai?2,ei?1i?2G?f i?1ai?2,ei?1i?2 ?
PYP(a?3, b?3, G?fai?1,ei?1)G?fai?1,ei?1 ?
PYP(a?2, b?2, G?)G?
?
PYP(a?1, b?1, G?0)G?0 ?
U(12)(a) Finish factor?i|f i?1ai?2, ei?1i?2, t ?
G?f i?1ai?2,ei?1i?2,tG?f i?1ai?2,ei?1i?2,t ?
PYP(a?3 , b?3 , G?fai?1,ei?1,t)G?fai?1,ei?1,t ?
PYP(a?2 , b?2 , G?t )G?t ?
PYP(a?1 , b?1 , G?0,t)G?0,t ?
U(b) Jump factorei|?i, f iai?2, ei?1i?2 ?Ge?i,f iai?2,ei?1i?2Ge?i,f iai?2,ei?1i?2 ?
PYP(ae5, be5, Gef iai?2,ei?1i?2)Gef iai?2,ei?1i?2 ?
PYP(ae4, be4, Gef iai?1,ei?1)Gef iai?1,ei?1 ?
PYP(ae3, be3, Gefai)Gefai ?
PYP(ae2, be2, Ge)Ge ?
PYP(ae1, be1, Ge0)Ge0 ?
U( 1|V |)(c) Emission factorFigure 2: Distributions over the translation factors and their hierarchical priors.that a jump could not go outside the bounds of thesource sentence.
Accordingly we maintain sepa-rate distributions for each setting, and each has adifferent uniform base distribution parameterizedaccording to the number of possible jump types.3.3 FertilityFor each target position, our Markov model mayselect a source word which has been covered,which means a source word may be linked to sev-eral target positions.
Therefore, we introduce fer-tility to denote the number of target positions asource word is linked to in a sentence pair.
Brownet al (1993) have demonstrated the usefulness offertility in probability estimation: IBM models 3?5 exhibit large improvements over models 1?2.
Onthese grounds, we include fertility to produce ouradvanced model,pad(eI1, aI1|fJ1 )=pbs(eI1, aI1|fJ1 )J?j=1p(?j |f jj?n) (2)where ?j is the fertility of source word fj in thesentence pair < fJ1 , eI1 > and pbs is the basicmodel defined in Eq.
1.
In order to avoid prob-lems of data sparsity, we bin fertility into threetypes, a) zero, if ?
= 0; b) single, if ?
= 1;and c) multiple, if ?
> 1.We draw the fertility variables from a hierarchi-cal PYP distribution, using three levels of backoff,?j |f jj?1 ?
G?fjj?1G?fjj?1?
PYP(a?3 , b?3 , G?fj )G?fj ?
PYP(a?2 , b?2 , G?)G?
?
PYP(a?1 , b?1 , G?0 )G?0 ?
U(13)where we condition the fertility of each word to-ken on the token to its left, which we drop duringthe first stage of backoff to simple word-based fer-tility.
The last level of backoff further generalisesto a shared fertility across all words.
In this waywe gain the benefits of local context on fertility,while including more general levels to allow widerapplicability.4 Gibbs SamplingTo train the model, we use Gibbs sampling, aMarkov Chain Monte Carlo (MCMC) techniquefor posterior inference.
Specifically we seek toinfer the latent sequence of translation decisionsgiven a corpus of sentence pairs.
Given the struc-ture of our model, a word alignment uniquelyspecifies the translation decisions and the se-quence follows the order of the target sentence leftto right.
Our Gibbs sampler operates by samplingan update to the alignment of each target wordin the corpus.
It visits each sentence pair in thecorpus in a random order and resamples the align-ments for each target position as follows.
First wediscard the alignment to the current target wordand decrement the counts of all factors affectedby this alignment in their top level distributions(which will percolate down to the lower restau-rants).
Next we calculate posterior probabilitiesfor all possible alignment to this target word basedon the table occupancies in the hPYP.
Finally wedraw an alignment and increment the table countsfor the translation decisions affected by the newalignment.More specifically, we consider sampling fromEquation 2 with n = 2.
When changing the align-ment to a target word ei from j?
to j, the fin-ish, jump and emission for three target positionsi, i+ 1, i+ 2 and fertility for two source positionsj, j?
may be affected.
This leads to the following337decrement increment?
(no | null, ?ll, Je, I) ?
(no | null, ?ll, Je, I)?
(no | p..s, take, null, ?ll) ?
(no | Je, take, null, ?ll)?
(no | le, that, p..s, take) ?
(no | le, that, Je, take)?
(f | null, ?ll, Je, I) ?
(s| null, ?ll, Je, I)?
(b | p..s, take, null, ?ll) ?
(m| Je, take, null, ?ll)?
(s | le, that, p..s, take) ?
(s| le, that, Je, take)e(take |f , p..s, null, ?ll, Je, I) e(take |s, Je, null, ?ll, Je, I)e(that |b, le, p..s, take, null, ?ll) e(that |m, le, Je, take, null, ?ll)e(one |s, le, le, that, p..s, take) e(one |s, le, le, that, Je, take)?
(single | p..s, le) ?
(multiple | Je, <s>)Table 1: The count update when changing thealigned source word of take from prends to Je inFigure 1.
Key: f?forward s?stay b?backward m?monotone p..s?prends.posterior probabilityp(ai = j|t?i,o?i) ?i+2?l=ip(?l)p(?l)p(el)?
p(?j + 1)p(?j?
?
1)p(?j)p(?j?
)(3)where ?j , ?j?
are the fertilities before changing thelink and for brevity we omit the conditioning con-texts.
For example, in Figure 1, we sample fortarget word take and change the aligned sourceword from prends to Je, then the items for whichwe need to decrement and increment the counts byone are shown in Table 1 and the posterior prob-ability corresponding to the new alignment is theproduct of the hierarchical PYP probabilities of allincrement items divided by the probability of thefertility of prends being single.Maintaining the current state of the hPYP asevents are incremented and decremented is non-trivial and the naive approach requires significantbook-keeping and has poor runtime behaviour.
Forthis we adopt the approach of Blunsom et al(2009b), who present a method for maintainingtable counts without needing to record the tableassignments for each translation decision.
Briefly,this algorithm samples the table assignment duringthe increment and decrement operations, which isthen used to maintain aggregate table statistics.This can be done efficiently and without the needfor explicit table assignment tracking.4.1 Hyperparameter InferenceIn our model, we treat all hyper-parameters{(ax, bx), x ?
(?, ?, e, ?)}
as latent random vari-ables rather than fixed parameters.
This means ourmodel is parameter free, and requires no user inter-vention when adapting to different data sets.
Forthe discount parameter, we employ a uniform Betadistribution ax ?
Beta(1, 1) while for the strengthparameter, we employ a vague Gamma distribu-tion bx ?
Gamma(10, 0.1).
All restaurants inthe same level share the same hyper-prior and thehyper-parameters for all levels are resampled us-ing slice sampling (Johnson and Goldwater, 2009)every 10 iterations.4.2 Parallel ImplementationAs mentioned above, the hierarchical PYP takesinto consideration a rich history to evaluate theprobabilities of translation decisions.
But thisleads to difficulties when applying the model tolarge data sets, particularly in terms of trackingthe table and customer counts.
We apply the tech-nique from Blunsom et al (2009a) of using multi-ple processors to perform approximate Gibbs sam-pling which they showed achieved equivalent per-formance to the exact Gibbs sampler.
Each pro-cess performs sampling on a subset of the corpususing local counts, and communicates changes tothese counts after each full iteration.
All the countdeltas are then aggregated by each process to re-fresh the counts at the end of each iteration.
Inthis way each process uses slightly ?out-of-date?counts, but can process the data independently ofthe other processes.
We found that this approxi-mation improved the runtime significantly with nonoticeable effect on accuracy.5 ExperimentsIn principle our model could be directly used as aMT decoder or as a feature in a decoder.
Howeverin this paper we limit our focus to inducing wordalignments, i.e., by using the model to infer align-ments which are then used in a standard phrase-based translation pipeline.
We leave full decod-ing for later work, which we anticipate would fur-ther improve performance by exploiting gappingphrases and other phenomena that implicitly formpart of our model but are not represented in thephrase-based decoder.
Decoding under our modelwould be straight-forward in principle, as the gen-erative process was designed to closely parallel thesearch procedure in the phrase-based model.3Three data sets were used in the experi-ments: two Chinese to English data sets on small(IWSLT) and larger corpora (FBIS), and Arabic3However the reverse translation probability would be in-tractable, as this does not decompose following a left-to-rightgeneration order in the target language.338to English translation.
Our experiments seek totest how the model compares to a GIZA++ base-line, quantifies the effect of each factor in theprobabilistic model (i.e., jump, fertility), and theeffect of different initialisations of the sampler.We present results on translation quality and wordalignment.5.1 Data SetupThe Markov order of our model in all experimentswas set to n = 2, as shown in Equation 2.
For eachdata set, Gibbs sampling was performed on thetraining set in each direction (source-to-target andtarget-to-source), initialized using GIZA++.4 Weused the grow heuristic to combine the GIZA++alignments in both directions (Koehn et al, 2003),which we then intersect with the predictions ofGIZA++ in the relevant translation direction.
Thisinitialisation setup gave the best results (we com-pare other initialisations in ?5.2).
The two Gibbssamplers were ?burned in?
for the first 1000 it-erations, after which we ran a further 500 itera-tions selecting every 50th sample.
A phrase ta-ble was constructed using these 10 sets of multi-ple alignments after combining each pair of direc-tional alignments using the grow-diag-final heuris-tic.
Using multiple samples in this way constitutesMonte Carlo averaging, which provides a betterestimate of uncertainty cf.
using a single sample.5The alignment used for the baseline results wasproduced by combining bidirectional GIZA++alignments using the grow-diag-final heuristic.We used the Moses machine translation decoder(Koehn et al, 2007), using the default featuresand decoding settings.
We compared the perfor-mance of Moses using the alignment produced byour model and the baseline alignment, evaluatingtranslation quality using BLEU (Papineni et al,2002) with case-insensitive n-gram matching withn = 4.
We used minimum error rate training (Och,2003) to tune the feature weights to maximise theBLEU score on the development set.5.2 IWSLT CorpusThe first experiments are on the IWSLT data setfor Chinese-English translation.
The training dataconsists of 44k sentences from the tourism andtravel domain.
For the development set we useboth ASR devset 1 and 2 from IWSLT 2005, and4All GIZA++ alignments used in our experiments wereproduced by IBM model4.5The effect on translation scores is modest, roughlyamounting to +0.2 BLEU versus using a single sample.System Dev IWSLT05baseline 45.78 49.98Markov+fs+e 49.13 51.54Markov+fs+e+j 49.68 52.55Markov+fs+e+j+ft 51.32 53.41Table 2: Impact of adding factors to our Markovmodel, showing BLEU scores on IWSLT.
Key: fs?finish e?emission j?jump ft?fertility.for the test set we use the IWSLT 2005 test set.The language model is a 3-gram language modeltrained using the SRILM toolkit (Stolcke, 2002)on the English side of the training data.
Becausethe data set is small, we performed Gibbs samplingon a single processor.First we check the effect of the model factorsjump and fertility.
Both emission and finish fac-tors are indispensable to the generative translationprocess, and consequently these two factors are in-cluded in all runs.
Table 2 shows translation resultfor various models, including a baseline and ourMarkov model with different combinations of fac-tors.
Note that even the simplest Markov model faroutperforms the GIZA++ baseline (+1.5 BLEU)despite the baseline (IBM model 4) including anumber of advanced features (e.g., jump, fertility)that are not present in the basic Markov model.This improvement is a result of the Markov modelmaking use of rich bilingual contextual informa-tion coupled with sophisticated backoff, as op-posed to GIZA++ which considers much more lo-cal events, with nothing larger than word-class bi-grams.
Our model shows large improvements asthe extra factors are included.
Jump yields an im-provement of +1 BLEU by capturing consistent re-ordering patterns.
Adding fertility results in a fur-ther +1 BLEU point improvement.
Like the IBMmodels, our approach allows each source word toproduce any number of target words.
This capac-ity allows for many non-sensical alignments suchas dropping many source words, or aligning sin-gle source words to several target words.
Explic-itly modelling fertility allows for more consistentalignments, especially for special words such aspunctuation which usually have a fertility of one.Next we check the stability of our model withdifferent initialisations.
We compare differentcombination techniques for merging the GIZA++alignments: grow-diag-final (denoted as gdf ), in-tersection and grow.
Table 3 shows that the dif-ferent initialisations have only a small effect on339system gdf intersection growbaseline 49.98 48.44 50.11our model 52.96 52.79 53.41Table 3: Machine translation performance inBLEU % on the IWSLT 2005 Chinese-English testset.
The Gibbs samplers were initialized with threedifferent alignments, shown as columns.the results of our model.
While the baseline re-sults vary by up to 1.7 BLEU points for the differ-ent alignments, our Markov model provided morestable results with the biggest difference of 0.6.Among the three initialisations, we get the bestresult with the initialisation of grow.
Gdf of-ten introduces alignment links involving functionwords which should instead be aligned to null.
In-tersection includes many fewer alignments, typi-cally only between content words, and the sparsitymeans that words can only have a fertility of ei-ther 0 or 1.
This leads to the initialisation being astrong mode which is difficult to escape from dur-ing sampling.
Despite this problem, it has onlya mild negative effect on the performance of ourmodel, which is probably due to improvementsin the alignments for words that truly should bedropped or aligned only to one word.
Grow pro-vides a good compromise between gdf and inter-section, and we use this initialisation in all oursubsequent experiments.Figure 3 shows an example comparing align-ments produced by our model and the GIZA++baseline, in both cases after combining the two di-rectional models.
Note that GIZA++ has linkedmany function words which should be left un-aligned, by using rare English terms as garbagecollectors.
Consequently this only allows for theextraction of few large phrase-pairs (e.g.
<?
?, ?m looking for>) and prevents the extractionof some good phrases (e.g.
<??
??
?,grill-type>, for ???
and ??
??
are wronglyaligned to ?grill-type?).
In contrast, our modelbetter aligns the function words, such that manymore useful phrase pairs can be extracted, i.e.,<?, ?m>,<?, looking for>,<???
?, grill-type> and their combinations with neighbouringphrase pairs.5.3 FBIS CorpusTheoretically, Bayesian models should out-perform maximum likelihood approaches on smalldata sets, due to their improved modelling of un-(a) GIZA++ baseline?
?
?
?
?
?
?
?
, ???
?????
???i'mlookingforanice,quietgrill-typerestaurant.
(b) our modelFigure 3: Comparison of an alignment inferred bythe baseline vs. our approach.certainty.
For larger datasets, however, the dif-ference between the two techniques should nar-row.
Hence one might expect that upon movingto larger translation datasets our gains might evap-orate.
This chain of reasoning ignores the fact thatour model is considerably richer than the baselineIBM models, in that we model rich contextual cor-relations between translation decisions, and con-sequently our approach has a lower inductive bias.For this reason our model should continue to im-prove with more data, by inferring better estimatesof translation decision n-grams.
A caveat thoughis that inference by sampling becomes less effi-cient on larger data sets due to stronger modes,requiring more iterations for convergence.To test whether our improvements carry over tolarger datasets, we assess the performance of ourmodel on the FBIS Chinese-English data set.
Herethe training data consists of the non-UN portionsand non-HK Hansards portions of the NIST train-ing corpora distributed by the LDC, totalling 303ksentence pairs with 8m and 9.4m words of Chi-nese and English, respectively.
For the develop-ment set we use the NIST 2002 test set, and eval-uate performance on the test sets from NIST 2003340NIST02 NIST03 NIST05baseline 33.31 30.09 29.01our model 33.83 31.02 30.23Table 4: Translation performance on Chinese toEnglish translation, showing BLEU% for modelstrained on the FBIS data set.and 2005.
The language model is a 3-gram LMtrained on Xinhua portion of the Gigaword corpususing the SRILM toolkit with modified Kneser-Ney smoothing.
As the FBIS data set is large, weemployed 3-processor MPI for each Gibbs sam-pler, which ran in half the time compared to usinga single processor.Table 4 shows the results on the FBIS data set.Our model outperforms the baseline on both testsets by about 1 BLEU.
This provides evidence thatour model performs well in the large data setting,with our rich modelling of context still provinguseful.
The non-parametric nature of the model al-lows for rich dynamic backoff behaviour such thatit can learn accurate models in both high and lowdata scenarios.5.4 Arabic English translationTranslation between Chinese and English is verydifficult, particularly due to word order differenceswhich are not handled well by phrase-based ap-proaches.
In contrast Arabic to English translationneeds less reordering, and phrase-based modelsproduce better translations.
This translation taskis a good test for the generality of our approach.Our Ar-En training data comprises several LDCcorpora,6 using the same experimental setup as inBlunsom et al (2009a).
Overall there are 276ksentence pairs and 8.21m and 8.97m words in Ara-bic and English, respectively.
We evaluate on theNIST test sets from 2003 and 2005, and the 2002test set was used for MERT training.Table 5 shows the results.
On all test sets ourapproach outperforms the baseline, and for theNIST03 test set the improvement is substantial,with a +0.74 BLEU improvement.
In generalthe improvements are more modest than for theChinese-English results above.
We suggest thatthis is due to the structure of Arabic-English trans-lation better suiting the modelling assumptions be-hind IBM model 4, particularly its bias towardsmonotone translations.
Consequently the addi-6LDC2004E72, LDC2004T17, LDC2004T18,LDC2006T02F1% NIST02 NIST03 NIST05baseline 64.9 57.00 48.75 48.93our model 65.7 57.14 49.49 48.96Table 5: Translation performance on Arabic toEnglish translation, showing BLEU%.
Also shownis word-alignment alignment accuracy.tional context provided by our model is less im-portant.
Table 5 also reports alignment results onmanually aligned Ar-En sentence pairs,7 measur-ing the F1 score for the GIZA++ baseline align-ments and the alignment from the final samplewith our model.8 Our model outperforms the base-line, although the improvement is modest.6 Conclusions and Future WorkThis paper proposes a word-based Markov modelof translation which correlates translation deci-sions by conditioning on recent decisions, andincorporates a hierarchical Pitman-Yor processprior permitting elaborate backoff behaviour.
Themodel can learn sequences of translation deci-sions, akin to phrases in standard phrase-basedmodels, while simultaneously learning word levelphenomena.
This mechanism generalises theconcept of phrases in phrase-based MT, whilealso capturing richer phenomena such as gappingphrases in the source.
Experiments show that ourmodel performs well both on the small and largedatasets for two different translation tasks, con-sistently outperforming a competitive baseline.
Inthis paper the model was only used to infer wordalignments; in future work we intend to developa decoding algorithm for directly translating withthe model.AcknowledgementsThis work was supported by the EPSRC (grantEP/I034750/1).ReferencesPhil Blunsom, Trevor Cohn, Chris Dyer, and MilesOsborne.
2009a.
A Gibbs sampler for phrasalsynchronous grammar induction.
In Proc.
of ACL-IJCNLP, pages 782?790.7LDC2012T168Directional alignments are intersected using the grow-diag-final heuristic.341Phil Blunsom, Trevor Cohn, Sharon Goldwater, andMark Johnson.
2009b.
A note on the implemen-tation of hierarchical dirichlet processes.
In Proc.
ofACL-IJCNLP, pages 337?340.Peter E. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19:263?331.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Proc.of ACL, pages 263?270.Josep Maria Crego, Franc?ois Yvon, and Jose?
B.Marin?o.
2011.
Ncode: an open source bilingual n-gram SMT toolkit.
Prague Bull.
Math.
Linguistics,96:49?58.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with in-tegrated reordering.
In Proc.
of ACL:HLT, pages1045?1054.Michel Galley and Christopher D. Manning.
2010.Accurate non-hierarchical phrase-based translation.In Proc.
of NAACL, pages 966?974.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of ACL, pages 961?968.Mark Johnson and Sharon Goldwater.
2009.
Improv-ing nonparameteric bayesian inference: experimentson unsupervised word segmentation with adaptorgrammars.
In Proc.
of HLT-NAACL, pages 317?325.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
of HLT-NAACL, pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proc.
of ACL.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012.
Continuous space translation models withneural networks.
In Proc.
of NAACL, pages 39?48.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of COLING-ACL, pages 609?616, July.Frans J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29:19?51.Frans J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
of ACL, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proc.
of ACL,pages 311?318.Jim Pitman and Marc Yor.
1997.
The two-parameterpoisson-dirichlet distribution derived from a stablesubordinator.
The Annals of Probability, 25(2):855?900.Andreas Stolcke.
2002.
SRILM: An extensible lan-guage modeling toolkit.
In Proc.
of ICSLP.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M.Blei.
2006.
Hierarchical Dirichlet processes.Journal of the American Statistical Association,101(476):1566?1581.Yee Whye Teh.
2006.
A hierarchical Bayesian lan-guage model based on Pitman-Yor processes.
InProc.
of ACL, pages 985?992.Ashish Vaswani, Haitao Mi, Liang Huang, and DavidChiang.
2011.
Rule markov models for fast tree-to-string translation.
In Proc.
of ACL, pages 856?864.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statisticaltranslation.
In Proc.
of COLING, pages 836?841.342
