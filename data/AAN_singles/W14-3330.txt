Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246?253,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsLIMSI @ WMT?14 Medical Translation TaskNicolas P?echeux1,2, Li Gong1,2, Quoc Khanh Do1,2, Benjamin Marie2,3,Yulia Ivanishcheva2,4, Alexandre Allauzen1,2, Thomas Lavergne1,2,Jan Niehues2, Aur?elien Max1,2, Franc?ois Yvon2Univ.
Paris-Sud1, LIMSI-CNRS2B.P.
133, 91403 Orsay, FranceLingua et Machina3, Centre Cochrane franc?ais4{firstname.lastname}@limsi.frAbstractThis paper describes LIMSI?s submissionto the first medical translation task atWMT?14.
We report results for English-French on the subtask of sentence trans-lation from summaries of medical ar-ticles.
Our main submission uses acombination of NCODE (n-gram-based)and MOSES (phrase-based) output andcontinuous-space language models used ina post-processing step for each system.Other characteristics of our submission in-clude: the use of sampling for buildingMOSES?
phrase table; the implementationof the vector space model proposed byChen et al.
(2013); adaptation of the POS-tagger used by NCODE to the medical do-main; and a report of error analysis basedon the typology of Vilar et al.
(2006).1 IntroductionThis paper describes LIMSI?s submission to thefirst medical translation task at WMT?14.
Thistask is characterized by high-quality input textand the availability of large amounts of trainingdata from the same domain, yielding unusuallyhigh translation performance.
This prompted usto experiment with two systems exploring differ-ent translation spaces, the n-gram-based NCODE(?2.1) and an on-the-fly variant of the phrase-based MOSES (?2.2), and to later combine theiroutput.
Further attempts at improving translationquality were made by resorting to continuous lan-guage model rescoring (?2.4), vector space sub-corpus adaptation (?2.3), and POS-tagging adap-tation to the medical domain (?3.3).
We also per-formed a small-scale error analysis of the outputsof some of our systems (?5).2 System Overview2.1 NCODENCODE implements the bilingual n-gram ap-proach to SMT (Casacuberta and Vidal, 2004;Mari?no et al., 2006; Crego and Mari?no, 2006) thatis closely related to the standard phrase-based ap-proach (Zens et al., 2002).
In this framework, thetranslation is divided into two steps.
To translatea source sentence f into a target sentence e, thesource sentence is first reordered according to aset of rewriting rules so as to reproduce the tar-get word order.
This generates a word lattice con-taining the most promising source permutations,which is then translated.
Since the translation stepis monotonic, the peculiarity of this approach is torely on the n-gram assumption to decompose thejoint probability of a sentence pair in a sequenceof bilingual units called tuples.The best translation is selected by maximizinga linear combination of feature functions using thefollowing inference rule:e?= argmaxe,aK?k=1?kfk(f , e,a) (1)where K feature functions (fk) are weighted bya set of coefficients (?k) and a denotes the set ofhidden variables corresponding to the reorderingand segmentation of the source sentence.
Alongwith the n-gram translation models and target n-gram language models, 13 conventional featuresare combined: 4 lexicon models similar to the onesused in standard phrase-based systems; 6 lexical-ized reordering models (Tillmann, 2004; Crego etal., 2011) aimed at predicting the orientation ofthe next translation unit; a ?weak?
distance-baseddistortion model; and finally a word-bonus modeland a tuple-bonus model which compensate for thesystem preference for short translations.
Featuresare estimated during the training phase.
Trainingsource sentences are first reordered so as to match246the target word order by unfolding the word align-ments (Crego and Mari?no, 2006).
Tuples are thenextracted in such a way that a unique segmenta-tion of the bilingual corpus is achieved (Mari?no etal., 2006) and n-gram translation models are thenestimated over the training corpus composed of tu-ple sequences made of surface forms or POS tags.Reordering rules are automatically learned duringthe unfolding procedure and are built using part-of-speech (POS), rather than surface word forms,to increase their generalization power (Crego andMari?no, 2006).2.2 On-the-fly System (OTF)We develop an alternative approach implement-ing an on-the-fly estimation of the parameter ofa standard phrase-based model as in (Le et al.,2012b), also adding an inverse translation model.Given an input source file, it is possible to computeonly those statistics which are required to trans-late the phrases it contains.
As in previous workson on-the-fly model estimation for SMT (Callison-Burch et al., 2005; Lopez, 2008), we first builda suffix array for the source corpus.
Only a lim-ited number of translation examples, selected bydeterministic random sampling, are then used bytraversing the suffix array appropriately.
A coher-ent translation probability (Lopez, 2008) (whichalso takes into account examples where translationextraction failed) is then estimated.
As we cannotcompute exactly an inverse translation probability(because sampling is performed independently foreach source phrase), we resort to the following ap-proximation:p(?f |e?)
= min(1.0,p(e?|?f)?
freq(?f)freq(e?
))(2)where the freq(?)
is the number of occurrences ofthe given phrase in the whole corpus, and the nu-merator p(e?|?f)?freq(?f) represents the predictedjoint count of?f and e?.
The other models in thissystem are the same as in the default configurationof MOSES.2.3 Vector Space Model (VSM)We used the vector space model (VSM) of Chenet al.
(2013) to perform domain adaptation.
Inthis approach, each phrase pair (?f, e?)
present inthe phrase table is represented by a C-dimensionalvector of TF-IDF scores, one for each sub-corpus,where C represents the number of sub-corpora(see Table 1).
Each component wc(?f, e?)
is a stan-dard TF-IDF weight of each phrase pair for thecthsub-corpus.
TF(?f, e?)
is the raw joint count of(?f, e?)
in the sub-corpus; the IDF(?f, e?)
is the in-verse document frequency across all sub-corpora.A similar C-dimensional representation of thedevelopment set is computed as follows: we firstperform word alignment and phrase pairs extrac-tion.
For each extracted phrase pair, we computeits TF-IDF vector and finally combine all vectorsto obtain the vector for the develompent set:wdevc=J?j=0K?k=0countdev(?fj, e?k)wc(?fj, e?k) (3)where J and K are the total numbers of sourceand target phrases extracted from the developmentdata, respectively, and countdev(?fj, e?k) is the jointcount of phrase pairs (?fj, e?k) found in the devel-opment set.
The similarity score between eachphrase pair?s vector and the development set vec-tor is added into the phrase table as a VSM fea-ture.
We also replace the joint count with themarginal count of the source/target phrase to com-pute an alternative average representation for thedevelopment set, thus adding two VSM additionalfeatures.2.4 SOULNeural networks, working on top of conventionaln-gram back-off language models, have been in-troduced in (Bengio et al., 2003; Schwenk et al.,2006) as a potential means to improve discretelanguage models.
As for our submitted transla-tion systems to WMT?12 and WMT?13 (Le et al.,2012b; Allauzen et al., 2013), we take advantageof the recent proposal of (Le et al., 2011).
Usinga specific neural network architecture, the Struc-tured OUtput Layer (SOUL), it becomes possibleto estimate n-gram models that use large vocab-ulary, thereby making the training of large neuralnetwork language models feasible both for targetlanguage models and translation models (Le et al.,2012a).
Moreover, the peculiar parameterizationof continuous models allows us to consider longerdependencies than the one used by conventionaln-gram models (e.g.
n = 10 instead of n = 4).Additionally, continuous models can also beeasily and efficiently adapted as in (Lavergne etal., 2011).
Starting from a previously trainedSOUL model, only a few more training epochs are247Corpus Sentences Tokens (en-fr) Description wrd-lm pos-lmin-domainCOPPA 454 246 10-12M -3 -15EMEA 324 189 6-7M 26 -1PATTR-ABSTRACTS 634 616 20-24M 22 21PATTR-CLAIMS 888 725 32-36M 6 2PATTR-TITLES 385 829 3-4M 4 -17UMLS 2 166 612 8-8M term dictionary -7 -22WIKIPEDIA 8 421 17-18k short titles -5 -13out-of-domainNEWSCOMMENTARY 171 277 4-5M 6 16EUROPARL 1 982 937 54-60M -7 -33GIGA 9 625 480 260-319M 27 52all parallel all 17M 397-475M concatenation 33 69target-lmmedical-data -146M 69 -wmt13-data -2 536M 49 -devel/testDEVEL 500 10-12k khresmoi-summaryLMTEST 3 000 61-69k see Section 3.4NEWSTEST12 3 003 73-82k from WMT?12TEST 1 000 21-26k khresmoi-summaryTable 1: Parallel corpora used in this work, along with the number of sentences and the number of Englishand French tokens, respectively.
Weights (?k) from our best NCODE configuration are indicated for eachsub-corpora?s bilingual word language model (wrd-lm) and POS factor language model (pos-lm).needed on a new corpus in order to adapt the pa-rameters to the new domain.3 Data and Systems Preparation3.1 CorporaWe use all the available (constrained) medical dataextracted using the scripts provided by the orga-nizers.
This resulted in 7 sub-corpora from themedical domain with distinctive features.
As out-of-domain data, we reuse the data processed forWMT?13 (Allauzen et al., 2013).For pre-processing of medical data, we closelyfollowed (Allauzen et al., 2013) so as to be able todirectly integrate existing translation and languagemodels, using in-house text processing tools fortokenization and detokenization steps (D?echelotteet al., 2008).
All systems are built using a?true case?
scheme, but sentences fully capital-ized (plentiful especially in PATTR-TITLES) arepreviously lowercased.
Duplicate sentence pairsare removed, yielding a sentence reduction up to70% for EMEA.
Table 1 summarizes the data usedalong with some statistics after the cleaning andpre-processing steps.3.2 Language ModelsA medical-domain 4-gram language model is builtby concatenating the target side of the paral-lel data and all the available monolingual data1,with modified Kneser-Ney smoothing (Kneser andNey, 1995; Chen and Goodman, 1996), using theSRILM (Stolcke, 2002) and KENLM (Heafield,2011) toolkits.
Although more similar to term-to-term dictionaries, UMLS and WIKIPEDIA provedbetter to be included in the language model.The large out-of-domain language model used forWMT?13 (Allauzen et al., 2013) is additionalyused (see Table 1).3.3 Part-of-Speech TaggingMedical data exhibit many peculiarities, includ-ing different syntactic constructions and a specificvocabulary.
As standard POS-taggers are knownnot to perform very well for this type of texts, weuse a specific model trained on the Penn Treebankand on medical data from the MedPost project(Smith et al., 2004).
We use Wapiti (Lavergneet al., 2010), a state-of-the-art CRF implementa-tion, with a standard feature set.
Adaptation is per-formed as in (Chelba and Acero, 2004) using theout-of-domain model as a prior when training thein-domain model on medical data.
On a medicaltest set, this adaptation leads to a 8 point reduc-tion of the error rate.
A standard model is used forWMT?13 data.
For the French side, due to the lackof annotaded data for the medical domain, corporaare tagged using the TreeTagger (Schmid, 1994).1Attempting include one language model per sub-corporayielded a significant drop in performance.2483.4 Proxy Test SetFor this first edition of a Medical Translation Task,only a very small development set was made avail-able (DEVEL in Table 1).
This made both systemdesign and tuning challenging.
In fact, with such asmall development set, conventional tuning meth-ods are known to be very unstable and prone tooverfitting, and it would be suboptimal to selecta configuration based on results on the develop-ment set only.2To circumvent this, we artificiallycreated our own internal test set by randomly se-lecting 3 000 sentences out from the 30 000 sen-tences from PATTR-ABSTRACTS having the low-est perplexity according to 3-gram language mod-els trained on both sides of the DEVEL set.
Thistest set, denoted by LMTEST, is however highlybiaised, especially because of the high redundancyin PATTR-ABSTRACTS, and should be used withgreat care when tuning or comparing systems.3.5 SystemsNCODE We use NCODE with default settings, 3-gram bilingual translation models on words and 4-gram bilingual translation factor models on POS,for each included corpora (see Table 1) and for theconcatenation of them all.OTF When using our OTF system, all in-domain and out-of-domain data are concatenated,respectively.
For both corpora, we use a maxi-mum random sampling size of 1 000 examples anda maximum phrase length of 15.
However, allsub-corpora but GIGA3are used to compute thevectors for VSM features.
Decoding is done withMOSES4(Koehn et al., 2007).SOUL Given the computational cost of com-puting n-gram probabilities with neural networkmodels, we resort to a reranking approach.
Inthe following experiments, we use 10-gram SOULmodels to rescore 1 000-best lists.
SOUL modelsprovide five new features: a target language modelscore and four translation scores (Le et al., 2012a).We reused the SOUL models trained for our par-ticipation to WMT?12 (Le et al., 2012b).
More-over, target language models are adapted by run-ning 6 more epochs on the new medical data.2This issue is traditionally solved in Machine Learning byfolded cross-validation, an approach that would be too pro-hibitive to use here.3The GIGA corpus is actually very varied in content.4http://www.statmt.org/moses/System Combination As NCODE and OTF dif-fer in many aspects and make different errors, weuse system combination techniques to take advan-tage of their complementarity.
This is done byreranking the concatenation of the 1 000-best listsof both systems.
For each hypothesis within thislist, we use two global features, correspondingeither to the score computed by the correspond-ing system or 0 otherwise.
We then learn rerank-ing weights using Minimum Error Rate Training(MERT) (Och, 2003) on the development set forthis combined list, using only these two features(SysComb-2).
In an alternative configuration, weuse the two systems without the SOUL rescoring,and add instead the five SOUL scores as features inthe system combination reranking (SysComb-7).Evaluation Metrics All BLEU scores (Pap-ineni et al., 2002) are computed using casedmulti-bleu with our internal tokenization.
Re-ported results correspond to the average and stan-dard deviation across 3 optimization runs to bet-ter account for the optimizer variance (Clark et al.,2011).4 Experiments4.1 Tuning Optimization MethodMERT is usually used to optimize Equation 1.However, with up to 42 features when usingSOUL, this method is known to become very sen-sitive to local minima.
Table 2 compares MERT,a batch variant of the Margin Infused RelaxationAlgorithm (MIRA) (Cherry and Foster, 2012) andPRO (Hopkins and May, 2011) when tuning anNCODE system.
MIRA slightly outperforms PROon DEVEL, but seems prone to overfitting.
How-ever this was not possible to detect before the re-lease of the test set (TEST), and so we use MIRAin all our experiments.DEVEL TESTMERT 47.0?
0.4 44.1?
0.8MIRA 47.9?
0.0 44.8?
0.1PRO 47.1?
0.1 45.1?
0.1Table 2: Impact of the optimization method duringthe tuning process on BLEU score, for a baselineNCODE system.2494.2 Importance of the Data SourcesTable 3 shows that using the out-of-domain datafrom WMT?13 yields better scores than only usingthe provided medical data only.
Moreover, com-bining both data sources drastically boosts perfor-mance.
Table 1 displays the weights (?k) given byNCODE to the different sub-corpora bilingual lan-guage models.
Three corpora seems particularyuseful: EMEA, PATTR-ABSTRACTS and GIGA.Note that several models are given a negativeweight, but removing them from the model sur-prisingly results in a drop of performance.DEVEL TESTmedical 42.2?
0.1 39.6?
0.1WMT?13 43.0?
0.1 41.0?
0.0both 48.3?
0.1 45.4?
0.0Table 3: BLEU scores obtained by NCODE trainedon medical data only, WMT?13 data only, or both.4.3 Part-of-Speech TaggingUsing the specialized POS-tagging models formedical data described in Section 3.3 instead of astandart POS-tagger, a 0.5 BLEU points increaseis observed.
Table 4 suggests that a better POStagging quality is mainly beneficial to the reorder-ing mechanism in NCODE, in contrast with thePOS-POS factor models included as features.Reordering Factor model DEVEL TESTstd std 47.9?
0.0 44.8?
0.1std spec 47.9?
0.1 45.0?
0.1spec std 48.4?
0.1 45.3?
0.1spec spec 48.3?
0.1 45.4?
0.0Table 4: BLEU results when using a standard POStagging (std) or our medical adapted specializedmethod (spec), either for the reordering rule mech-anism (Reordering) or for the POS-POS bilinguallanguage models features (Factor model).4.4 Development and Proxy Test SetsIn Table 5, we assess the importance of domainadaptation via tuning on the development set usedand investigate the benefits of our internal test set.Best scores are obtained when using the pro-vided development set in the tuning process.
Us-DEVEL LMTEST NEWSTEST12 TEST48.3?
0.1 46.8?
0.1 26.2?
0.1 45.4?
0.041.8?
0.2 48.9?
0.1 18.5?
0.1 40.1?
0.139.8?
0.1 37.4?
0.2 29.0?
0.1 39.0?
0.3Table 5: Influence of the choice of the develop-ment set when using our baseline NCODE system.Each row corresponds to the choice of a develop-ment set used in the tuning process, indicated by asurrounded BLEU score.Table 6: Contrast of our two main systems andtheir combination, when adding SOUL language(LM) and translation (TM) models.
Stars indicatean adapted LM.
BLEU results for the best run onthe development set are reported.DEVEL TESTNCODE 48.5 45.2+ SOUL LM 49.4 45.7+ SOUL LM?49.8 45.9+ SOUL LM + TM 50.1 47.0+ SOUL LM?+ TM 50.1 47.0OTF 46.6 42.5+ VSM 46.9 42.8+ SOUL LM 48.6 44.0+ SOUL LM?48.4 44.2+ SOUL LM + TM 49.6 44.8+ SOUL LM?+ TM 49.7 44.9SysComb-2 50.5 46.6SysComb-7 50.7 46.5ing NEWSTEST12 as development set unsurpris-ingly leads to poor results, as no domain adapta-tion is carried out.
However, using LMTEST doesnot result in much better TEST score.
We also notea positive correlation between DEVEL and TEST.From the first three columns, we decided to use theDEVEL data set as development set for our sub-mission, which is a posteriori the right choice.4.5 NCODE vs. OTFTable 6 contrasts our different approaches.
Prelim-inary experiments suggest that OTF is a compara-ble but cheaper alternative to a full MOSES sys-tem.5We find a large difference in performance,5A control experiment for a full MOSES system (using asingle phrase table) yielded a BLEU score of 45.9 on DEVELand 43.2 on TEST, and took 3 more days to complete.250extra missing incorrect unknownword content filler disamb.
form style term order word term allsyscomb 4 13 20 47 62 8 18 21 1 11 205OTF+VSM+SOUL 4 4 31 44 82 6 20 42 3 12 248Table 7: Results for manual error analysis following (Vilar et al., 2006) for the first 100 test sentences.NCODE outperforming OTF by 2.8 BLEU pointson the TEST set.
VSM does not yield any signifi-cant improvement, contrarily to the work of Chenet al.
(2013); it may be the case all individual sub-corpus are equally good (or bad) at approximatingthe stylistic preferences of the TEST set.4.6 Integrating SOULTable 6 shows the substantial impact of addingSOUL models for both baseline systems.
Withonly the SOUL LM, improvements on the test setrange from 0.5 BLEU points for NCODE systemto 1.2 points for the OTF system.
The adaptationof SOUL LM with the medical data brings an ad-ditional improvement of about 0.2 BLEU points.Adding all SOUL translation models yield animprovement of 1.8 BLEU points for NCODE andof 2.4 BLEU points with the OTF system usingVSM models.
However, the SOUL adaptation stephas then only a modest impact.
In future work, weplan to also adapt the translation models in orderto increase the benefit of using in-domain data.4.7 System CombinationTable 6 shows that performing the system combi-nation allows a gain up to 0.6 BLEU points on theDEVEL set.
However this gain does not transfer tothe TEST set, where instead a drop of 0.5 BLEUis observed.
The system combination using SOULscores showed the best result over all of our othersystems on the DEVEL set, so we chose this (aposteriori sub-obtimal) configuration as our mainsystem submission.Our system combination strategy chose for DE-VEL about 50% hypotheses among those producedby NCODE and 25% hypotheses from OTF, theremainder been common to both systems.
As ex-pected, the system combination prefers hypothe-ses coming from the best system.
We can observenearly the same distribution for TEST.5 Error AnalysisThe high level of scores for automatic metricsencouraged us to perform a detailed, small-scaleanalysis of our system output, using the error typesproposed by Vilar et al.
(2006).
A single annota-tor analyzed the output of our main submission, aswell as our OTF variant.
Results are in Table 7.Looking at the most important types of errors,assuming the translation hypotheses were to beused for rapid assimilation of the text content, wefind a moderate number of unknown terms and in-correctly translated terms.
The most frequent er-ror types include missing fillers, incorrect disam-biguation, form and order, which all have somesignificant impact on automatic metrics.
Compar-ing more specifically the two systems used in thissmall-scale study, we find that our combination(which reused more than 70% of hypotheses fromNCODE) mostly improves over the OTF variant onthe choice of correct word form and word order.We may attribute this in part to a more efficientreordering strategy that better exploits POS tags.6 ConclusionIn this paper, we have demonstrated a successfulapproach that makes use of two flexible transla-tion systems, an n-gram system and an on-the-flyphrase-based model, in a new medical translationtask, through various approaches to perform do-main adaptation.
When combined with continu-ous language models, which yield additional gainsof up to 2 BLEU points, moderate to high-qualitytranslations are obtained, as confirmed by a fine-grained error analysis.
The most challenging partof the task was undoubtedly the lack on an internaltest to guide system development.
Another inter-esting negative result lies in the absence of successfor our configuration of the vector space modelof Chen et al.
(2013) for adaptation.
Lastly, a morecareful integration of medical terminology, as pro-vided by the UMLS, proved necessary.7 AcknowledgementsWe would like to thank Guillaume Wisniewski andthe anonymous reviewers for their helpful com-ments and suggestions.251ReferencesAlexandre Allauzen, Nicolas P?echeux, Quoc KhanhDo, Marco Dinarelli, Thomas Lavergne, Aur?elienMax, Hai-son Le, and Franc?ois Yvon.
2013.
LIMSI@ WMT13.
In Proceedings of the Workshkop onStatistical Machine Translation, pages 62?69, Sofia,Bulgaria.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3(6):1137?1155.Chris Callison-Burch, Colin Bannard, and JoshSchroeder.
2005.
Scaling phrase-based statisti-cal machine translation to larger corpora and longerphrases.
In Proceedings of ACL, Ann Arbor, USA.Francesco Casacuberta and Enrique Vidal.
2004.
Ma-chine translation with inferred stochastic finite-statetransducers.
Computational Linguistics, 30(3):205?225.Ciprian Chelba and Alex Acero.
2004.
Adaptationof maximum entropy classifier: Little data can helpa lot.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), Barcelona, Spain.Stanley F. Chen and Joshua T. Goodman.
1996.
Anempirical study of smoothing techniques for lan-guage modeling.
In Proceedings of the 34th AnnualMeeting of the Association for Computational Lin-guistics (ACL), pages 310?318, Santa Cruz, NM.Boxing Chen, Roland Kuhn, and George Foster.
2013.Vector space model for adaptation in statistical ma-chine translation.
In Proceedings of ACL, Sofia,Bulgaria.Colin Cherry and George Foster.
2012.
Batch tun-ing strategies for statistical machine translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 427?436.
Association for Computational Lin-guistics.Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah ASmith.
2011.
Better Hypothesis Testing for Statisti-cal Machine Translation : Controlling for OptimizerInstability.
In Better Hypothesis Testing for Statisti-cal Machine Translation : Controlling for OptimizerInstability, pages 176?181, Portland, Oregon.Josep M. Crego and Jos?e B. Mari?no.
2006.
Improvingstatistical MT by coupling reordering and decoding.Machine Translation, 20(3):199?215.Josep M. Crego, Franc?ois Yvon, and Jos?e B. Mari?no.2011.
N-code: an open-source bilingual N-gramSMT toolkit.
Prague Bulletin of Mathematical Lin-guistics, 96:49?58.Daniel D?echelotte, Gilles Adda, Alexandre Allauzen,Olivier Galibert, Jean-Luc Gauvain, H?el`ene May-nard, and Franc?ois Yvon.
2008.
LIMSI?s statisti-cal translation systems for WMT?08.
In Proc.
of theNAACL-HTL Statistical Machine Translation Work-shop, Columbus, Ohio.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 187?197, Edinburgh, Scotland, July.
Associa-tion for Computational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,EMNLP ?11, pages 1352?1362, Stroudsburg, PA,USA.
Association for Computational Linguistics.Reinhard Kneser and Herman Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processing, ICASSP?95,pages 181?184, Detroit, MI.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings the 48th Annual Meeting of the Association forComputational Linguistics (ACL), pages 504?513.Association for Computational Linguistics, July.Thomas Lavergne, Hai-Son Le, Alexandre Allauzen,and Franc?ois Yvon.
2011.
LIMSI?s experimentsin domain adaptation for IWSLT11.
In Mei-YuhHwang and Sebastian St?uker, editors, Proceedingsof the heigth International Workshop on SpokenLanguage Translation (IWSLT), San Francisco, CA.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011.
Structuredoutput layer neural network language model.
In Pro-ceedings of ICASSP, pages 5524?5527.Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.2012a.
Continuous space translation models withneural networks.
In Proceedings of the 2012 confer-ence of the north american chapter of the associa-tion for computational linguistics: Human languagetechnologies, pages 39?48, Montr?eal, Canada, June.Association for Computational Linguistics.Hai-Son Le, Thomas Lavergne, Alexandre Al-lauzen, Marianna Apidianaki, Li Gong, Aur?elien252Max, Artem Sokolov, Guillaume Wisniewski, andFranc?ois Yvon.
2012b.
LIMSI @ WMT12.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 330?337, Montr?eal,Canada.Adam Lopez.
2008.
Tera-Scale Translation Modelsvia Pattern Matching.
In Proceedings of COLING,Manchester, UK.Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,Adri`a de Gispert, Patrick Lambert, Jos?e A.R.
Fonol-losa, and Marta R. Costa-Juss`a.
2006.
N-gram-based machine translation.
Computational Linguis-tics, 32(4):527?549.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics, pages 160?167, Stroudsburg, PA,USA.
Association for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,USA, July.
Association for Computational Linguis-tics.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofInternational Conference on New Methods in Lan-guage Processing, September.Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-vain.
2006.
Continuous space language models forstatistical machine translation.
In Proceedings of theCOLING/ACL on Main conference poster sessions,pages 723?730, Morristown, NJ, USA.
Associationfor Computational Linguistics.L.
Smith, T. Rindflesch, and W. J. Wilbur.
2004.
Med-post: a part of speech tagger for biomedical text.Bioinformatics, 20(14):2320?2321.A.
Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of the In-ternational Conference on Spoken Language Pro-cessing (ICSLP), pages 901?904, Denver, Colorado,September.Christoph Tillmann.
2004.
A unigram orientationmodel for statistical machine translation.
In Pro-ceedings of HLT-NAACL, pages 101?104.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error Analysis of Statistical Ma-chine Translation Output.
In LREC, Genoa, Italy.Richard Zens, Franz Joseph Och, and Herman Ney.2002.
Phrase-based statistical machine translation.In M. Jarke, J. Koehler, and G. Lakemeyer, editors,KI-2002: Advances in artificial intelligence, volume2479 of LNAI, pages 18?32.
Springer Verlag.253
