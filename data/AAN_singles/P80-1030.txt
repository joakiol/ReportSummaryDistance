PHRAN - A Knowledge-Based Nature\] Language UnderstenderRobert Wilensky and Yigal ArenaUniversity of California at BerkeleyAbstractWe have developed an approach to natural languageprocessing in which the natural language processor isviewed as a knowledge-based system whose knowledge isabout the meanings of the utterances of its language.The approach is orzented around the phrase rather thanthe word as the basic unit.
We believe that thisparad i~ for language processing not only extends thecapabilities of other natural language systems, buthandles those tasks that previous systems could performin e more systematic and extensible manner.We have construqted a natural language analysis programcalled PHRAN (PHRasal ANalyzer) based in this approach.This model has a number of advantages over existingsystems, including the ability to understand a widervariety of language utterances, increased processlngspeed in some cases, a clear separation of controlstructure from data structure, a knowledge base thatcould be shared by a language productxon mechanism,greater ease of extensibility, and the ability to storesome useful forms of knowledge that cannot readily beadded to other systems.1.0 INTRODUCTIONThe problem of constructing a natural language ~rocessingsystem may be viewed as a problem oz constructing aknowledge-based system.
From this orientation, thequestions to ask are the following: What sort ofknowledge does a system need about a language in order tounderstand the meaning of an utterance or to produce anutterance in that language?
How can this knowledge aboutone's language best be represented, organized andutilized?
Can these tasks be achieved so that theresulting system is easy to add to and modify?
Moreover,can the system be made to emulate a human language user?Existing natural language processing systems varyconsiderably in the kinds of knowledge about languagethey possess, as well as in how thxs knowledge isrepresented, organized and utilized.
However, most ofthese systems are based on ideas about language that donot come to grips with the fact that a natural, languageprocessor neeos a great deal of knowledge aoout themeaning of its language's utterances.Part of the problem is that most current natural languagesystems assume that the meaning of a natural languageutterance can be computed as a function of theconstituents of the utterance.
The basic constituents ofutterances are assumed to be words, and all the knowledgethe system has about ~he semantics of its language zsstored at the word level (~i~nbaum eta l ,  1979) (Riesbecket al 1975) (Wilks, 197~) (Woods, 1970).
However, manynatural language utterances have interpretations thatcannot be found by examining their components.
Idioms,canned phrases, lexical collocations, and structuralformulas are instances of large classes of languageutterances whose interpretation require knowledge aboutShe entire phrase independent of its individual words(Becker, 19q5) (Mitchell, 19~71).We propose as an alternative a model of language use thatcomes from viewing language processing systems asknowledge-based systems tha?require the representationand organization of large amounts of knowledge about whatthe utterances of a language mean.
This model has thefollowing properties:I.
It has knowledge about the meaning of the wordsof the language, but in addition, much of thesystem's knowledge is about the meaning oflarger forms of u~terancas.2.
This knowledge is stored in the form ofpattern-concept pairs.
A pattern is a phrasalcons~ruc~ oI varyxng degrees of specificity.
Aconcept is a notation that represents themeaning of the phrase.
Together, this pairassociates different forms of utterances withtheir meanings.3.
The knowledge about language contained in thesystem is kept separate from the processingstrategies that apply this knowledge to theunderstanding and production tasks.4.
The understanding component matches incomingutterances against known patterns, and then usesthe concepts associated with the matchedpatterns to represent the utterance's meaning.5.
The production component expresses itself b \ [lookxng for concepts in the caza oase ~net matchthe concept it wishes to express.
The phrasalpatterns associated with these concepts are usedto generate the natural language utterance.6.
The data-base of pattern-concept pairs is sharedby both the unaerstanding mechanism and themechanism of language production.7.
Other associations besides meanings may be keptalong with a phrase.
For example, a descriptionof the contexts in which the phrase is anappropriate way to express its meaning may bestored.
A erson or situation stronglyassociated wi~h the phrase may also be tied toit.PHRAN CPHRasal ANalyzer) is a natural languageunderstanding system based on this view of language use.PNNAN reads English text and produces structures thatrepresent its meaning.
As it reads an utterance, PHRANsearches its knowledge base of pattern-conceptpairs forpatterns that best interpret the text.
The conceptportion of these pairs is then used to produce themeaning representation for the utterance.PHRAN has a number of advantages over previous systems:I.
The system is able to handle phrasal languageunits that are awkwardly handled by previoussystems but which are found with great frequencyin ordinary speech and common natural languagetexts.2.
It is simpler to add new information to thesystem because control and representation arekept separate.
To extend the system, newpattern-concept pairs are simply added to thedata-base.3.
The knowledge base used by PHRAN is declarative,and is in principle sharable by a system forlanguage productioD (Such a mechanism is n~wunder construction).
Thus adding xnxorma~lon ~othe base should extend the capabz\]ities of bothmechanisms.4.
Because associations other than meanings can bestored along with phrasal unzts, theidentification of a phrase can providecontextual clues not otherwise available tosubsequent processing mechanisms.5.
The model seems to more adequately reflect thepsychological reality of human language use.2.0 pHRASAL LANGUAGE CONSTRUCTSBy the term "phrasal language constructs" we refer tothose language units of which the language user hass~ecific knowledge.
We cannot present our entireclassification oF these constructs here.
However, ourphrasal constructs range greatly in flexibility.
Forexample, fixed expressions like "by and large , the BigApple (meaning N.Y.C.
), and lexical collocations such as"eye dro~per" and "weak safety" allow little or nomodificatxonA idioms like "kick the bucket" and "burythe hatchet alow the verb in them to s~pear in variousforms- discontinuous dependencies like look ... up"permi~ varying positional relationships of theirconstituents.
All these constructs are phrasal in thatthe language user must know the meaning of the constructas a whole In order to use it correctly.In the most general case, a phrase may express the usageof a word sense.
For example, to express one usage ofthe verb kick, the phrase "<person> <kick-form> <object>"is used.
This denotes a person followed by some verbform inyolving kick (e.g., kick, kicked, would ~avekicked") fo l lowe~"~ some utterance ueno~ing an oojec~.Our notion of a phrasal language construct is similar toa structural formula (Fillmore, 1979)- However, ourcriterion for dlr~trl'F/~ing whether a set of forms should117be accomodated by the same phrasal pattern is essential lya conceptual one.
Since each phrasal pattern in PHRAN isassociated with a concept, if the msenlngs of phrases aredifferent, they should be matched by different patterns.If the surface structure of the phrases is similar andthey seem to mean the same thing, %hen they should beaccomodated by one pattern.3.0 PHRANP~AN (PHRasal ANalyzer) is an English languageunderstanding system which integrates both generative andnon-productive language abilities to provide a relativelyflexzble and extenstble natural language understandingfacility.
While PHRAN does have knowledge aboutindividual words, it is not limited to such knowledge,nor ms its processing capabil ity constrained by aword-based bias.Here are some examples of sentences PHRAN can understand:e 0i%men are encouraged by the amount of oil discoveredin the Baltimore Canyon, an undersea trough 100 m$1esoff the shore of New Jersey.
(Newsweek, Feb 1980)* The young man was told to drive quickly over to~erkeley.
* If John gives Bill the big apple then Bill won't behungry.
* Wills will drive Bill to The Big Apple if she isgiven twenty five dollars.
* If Mary brings John we'll go to a Chinese restaurant.
* Wills gives me a headache.
(The previous sentences are analyzed by an uncompiledversion of PHRAN on the DEC-20/4Q system at UC Eerkeleyin from 2 to 9 seconds of CPU time).At the center of PHRAN is a knowledge base of phrasalpatterns.
These include literal strings such as "so'syour old man"; patterns such as "<nationality>restaurant", and very ~eneral phrases such as "<person><give> <person> <object> .Associated with each phrasal pattern is a conceptualtemplate.
A conceptual template is a piece of meanln~representation with possible references to pieces of theassociated phrasal pattern.
For example, associated withthe phrasal pattern "<nationality> restaurant" is theconceptual template denoting a restaurant that serves<nationality> type food; associated with the phrasalpattern "<person~> <give> <personJ> <object>" is theconceptual template that denotes a transfer of possessionby <person1> of <object> to <personJ> from <person1>.~.O HOW PH~AN WORKS~.1 Overall AlgorithmFHRAN is made up of three parts - a database ofpattern-concept pairs, a set of comprehension routines,and a routine which suggests appropriate pattern, conceptpairs.
PHRAN takes as input an English sentence, and asxt reads it from left to right, PHRAN comnares thesentence against patterns from the database.
Whenever amatching pattern is found, PHRAN interprets that part ofthe sentence that matched the pattern as describing theconcept associated with the pattern in thepattern-concept pair.4.1.1 Overview ?
Of Processing -When PHRAN analyzes a sentence, it reads the words one ata time, from left to right.
It does just enoughmorphological analysis to recognize contractions and"'s s. The pattern suggesting routine determines if anynew patterns should be tried, and PHRAN checks all thenew patterns to see if they agree with that part of thesentence already analyzed, discarding those that don't.A word's meaning is determined simply by its matching apattern consisting of that literal word.
Then a term isformed with the properties specified in the conceptassociated with the word, and th:s term is added to alist PHRAN maintains.
PHRAN checks if the term it justadded ~ the list completes or extends patterns that hadalread3 been partially matched by the previous terms.
Ifa pattern is completely matched, the terms matching the*pattern are removed and a new term, specified by th,concept part of the nattern-conceDt pair, is formed andreplaces the terms the pattern matched.When PHRAN finishes processing one word it reads thenext, iterating thls procedure until it reaches the end118of e sentence.
At this point, it should end up with asingle term on its list.
This term con%sins theconceptualization representing the meaning of the wholesentence.4.1.2 Overview Of PHRAN Patterns -A pattern-concept pair consists of a specification of thephrasal unit, an associated concept, and some additionalinformation about how the two are related.
When PHRANinstantiates a concept, it creates an item called a termthat includes the concept as well as some additionalinformation.A pattern is a sequence of conditions that must hold truefor a sequence of terms.
A pattern may specify optionalterms toq, the place where these may appear, ana whateffect (if any) their appearance will have on theproperties of the term formea if the pattern is matched.For example, consider the following informal descriptionof one of the patterns suggested by the mention of theverb 'to eat' in certain contexts.
{ pa;tern to recognize -|<first term: represents a person>?
<second term: is an actlve form of EAT><OPTIONAL third term: represents food>\]term to form -( INGEST(ACTOR <first term>)(OBJECT <third term, if present,else FOOD>)) }Notice that the third term is marked as optional.
If itis not present in the text, PHRAN will fi l l 'the OBJECTslot with a default representing generic food.4.1 .
.
~ Simple Example -The following is a highly simplified example of how PHRANprocesses the sentence "John dropped out of school":First the word "John" is read.
"John" matches thepatter~ consisting of the literal "John", and the conceptassociated with this pattern causes a term to be formedthat represents a noun phrase and a particular maleerson named John.
No other patterns were suggested.~his term is added on to *CONCEPTS, the list of termsPHRAN keeps and which will eventually contain the meaningof the sentence.
Thus *CONCEPT* looks like< \[JORNI - person, NP\] >"Dropped" is read next.
It matches the literal"dropped", and an appropriate term is formed.
Thepattern suggesting routine instructs PHRAN to consider%he 'basic pattern associated with the verb 'to drop',which is:I \[<person> <DROP> <object>\] \[ ...
I 1Its initial condition is found to be satisfied by thefirst term in *CONCE ~PT e -- this fact is stored under thatterm so that succeeding ones will be checked to see ifthis partial match continues.
The term that was formedafter reading "dropped" is now added to the list.
*CONCEPT* is now< \[JOMNI - person, NP\] , \[DROP - verb\] >PHRAN now checks to see if the pattern stored under thefirst term matches the term just added to CONCEPT too,and it does.
This new fact is now stored under the lastterm.Next the word "out" is read.
The pattern suggestionmechanism is alerted by the occurence of the verb 'drop'followed by the word 'out', and at this point Itinstructs PHRAN to consi ;r the patternI \[<person> <DROP> "out" "of" <school> I \[ ... \] !The list in *CONCEPT* is checked against this pattern tosee if it matches its first two terms, end since that isthe case, this fact is stored under the secord term.
Aterm associated with 'out' is now added to *CONCEPT*:< \[JOHNI - person, NP\] , \[DROP - verb\] , lOUT \] >The two patterns that have matched up to DROP are checkedto see if the new term extends them.
This is true onlyfor the second pattern, a~d this fact is stored unde~ thenext term.
The pattern l<person> <DROP> <object>) isdiscarded.Now the word "of" is read.
A term is formed and added to*CONCEPT*.
The pattern that matched to OUT isextended by OF so %he pattern is moved to ~e next term.The word "high" is read and a term is formed and added to*CONCEPt .
Now the pattern under OF is compared againstHIGH.
It doesn't satisfy the next condition.
PHRANreads "school", and the pattern suggestion routinepresents PHRAN with two patterns:I. I \[ "high .... school" \] \[ represention denoting aschool $o~ IOth through 12thgraders~ |2.
I \[<adjective> ~noun>\] \[ representation denotingnoun modified by adjectiveJ 1Both patterns are satisfied by the previous term and thisfact is stored under it.
The new term is added to*CONCEPT*, now:< ~JOHNI - person ~V2 \] ,~\[DROP - verb\] , \[OUT\]\[0FT , \[HIGH - sdjl , \[SCHOOL - sch6ol, noun\]'>The two patterns are compared against the last term, andboth are matched.
The last two terms a~'e removed from*CONCEPT*, and the patterns under 0F are checked todetermine which of the two possible meanings we haveshould be chosen.
Patterns are suggested such that themore specific ones appear first, so that the morespecific interpretation will be chosen if all patternsmatch equally well.. 0nly if the second meanin~ (i.e.
aschool that is high) were explicitly specifled by aprevious pattern, would it have been chosen.A term is formed and added to *CONCEPT*, which nowcontains< \[JOHNI - person, NP~ .
\[DROP - verb\] \[OUT\] ,\[0FI , \[HIGH-SCHOOLI - school, NPJ >The pattern under OF is checked against the last term in*CONCEPT ~.
PHRAN finds a complete match, so all thematched terms are removed and replaced by the conceptassociated with this pattern.
*CONCEPT* now contains this concept as the final result:< \[ ($SCHOOLING (STUDENT JOHNI) .
(SCHOOL HIGH-SCHOOLI)(TERMINATION PREMATURE)) \] >4.2 Pattern-Concept Pairs In More Detaild.2.1 The Pattern -The pattern portion of a pattern-concept pair consists ofa sequence of predicates.
These may take one of severalforms:1.
A word; which will match only a termrepresenting this exact word.2.
A class name (in parentheses); will match  anyterm ~epresenting a member @f this class (e.g.
"(FOOD)" or "(PHYSICAL-OBJECT)").~.
A pair, the first element of which is a propertyname end the second is a value; will match any~e rm hav%ng the required valge of the property e.g.
"(Part-0f-Speech VERB)").In addition, we may negate a condition or specify that aconjunction or disjunction of several must hold.The following is one of the patterns which may besuggested by the occurrence of the verb 'give' in anutterance:\[(PERSON) (BOOT GIVE) (PERSON) (PNYSOB)I4.2 .1 .1  Optional Parts -To indicate the presence of optional terms, a list ofpattern concept-pairs is inserted into the pattern at theappropriate place.
These pairs have as their firstelement a sub-pattern that will match the optional terms.The second part describes how the new term to be formedif the maxo pattern is found should be modified toreflect the existence of the optional sub-pattern.The concept corresponding to the optional part of apattern zs treated in a form slightly different from theway we treat regular concept parts of pattern-conceptpairs.
As usual, it consists of pairs of expressions.The first of each pair will be places as is at ~he end ofthe properties o~ the term to be formed, end the secondwill be evaluated first and then placed on that list.For example, another pattern suggested when 'give' isseen is the following:\[(PERSON) (ROOT ~VE).~PHYSOB)(~\[T0 (PERSON))(TO (OPT-VAL 2 CD-FORM))\])\]The terms of this pattern describe a person, the verbgive, and then some pnysical object.
The last termdescribes the optional terms, consisting of the word tofollowed by a person description.
Associated with th~pattern is a concept part that specifies what to do withthe optional part if it is there.
Here it specifies thatthe second term in the optional pattern should fill inthe TO slot in the conceptualization associated with thewhole pattern.This particular pattern need not be a separate pattern inPHRAN from the one that looks for the verb followed bythe recipient followed by the object transferred.
Weoften show patterns without all the alternatives that arepossible for expositional purposes.
Sometimes it issimpler to write the actual patterns separately, althoughwe attach no theoretical significance to thxsdisposition.4.2.2 The Concept -When a pattern is matched.
PHRAN removes the terms thatmatch zt from *CONCEPT* and replaces them with a newterm, as defined by the second part of thepattern-concept pair.
For example, here is apattern-concept pazr that may be suggested when the verb"eat' is encountered:(\[(PERSON) (BOOT EAT) (\[((FOOD))(FOOD (OPT-VAL I CD-FOBM))\])\]\[P-O-S 'SENTENCECD-FORM '(INGEST (ACTO~ ?ACTOR) (OBJECT ?FOOD))ACTOR (VAL~ I CD-FORM)FOOD 'FOOD\])The concept portion of this pair describes a termcovering an entire sentence, and whose ~eaning is theaction of INGESTing some food (Schank, 1975).
The nexttwo descriptors specify how $o fill in vaTiable parts ofthis action.
The expression (VALUE n prop) specifies the'prop' property of the n'th term in the matched sequenceof the pattern (not including optional terms).
OFT-VALdoes the same thing with regards to a matched optionalsub-pattern.
Thus the concept description abovespecifies that the actor of the action is to be the termmatching the first condition.
The object eaten will beeither the default concept food, or, if the optionalsub-pattern was found, the term corresponding to thissuo-pattern.Sometimes a slot in the conceptualization can be filledby a term in a higher level pattern of which this one isan element.
For example, when analyzing "John wanted toeat a cupcake" a slight modification of the previouspattern is used to find the meaning of "to eat acupcake".
Since no subject appears In this form, thehigher level pattern specifies where it may find it.That is, a pattern associated with "want" looks like thefollowing:{ ~<person> <WANT> <in$initive>\],infinitive  DFOHMThis specifies that the subject of the clause followingwant is the same as the subject of went.4.5 Pattern Manipulation In More Detail4.~.I Reading A Word -When s word is read PHRAN compares the ~atterns offeredby the pattern suggestin?
routine with the list *CONCEPT*in ~ne manner aescrioea in ~ne example in section 4.1.3.It discards patterns that confllct with *CONCEPT* andretains the rest.
Then FH~AN tries to determine whichmeaning ?f the word to choose, using the "active"patterns (those that have matched up to the point wherePHRAN has read).
It checks if there is a particularmeaning that will match the next slot in some pattern orif no such definition exists if there is a meanin?
thatmight be the beginning of a' sequence of terms -whosemeaning, as determined via a pa~tern-concept pair, willsatisfy the next slot in one of the active patterns.
Ifthis is the case, that meanin~ of the word is chosen.Otherwise PHRAR defaults to the fzrst of the meanings ofthe word.A new term is formed and if it satisfies the nextcondition in one of these patterns, the appropriate~atzsrn Is moved to the pattern-list of the new term.
Ifzhe next condition in the pattern indicates that the termspeczfled is optional, %hen PHRAN checks for theseOptlonal terms, and if it is convinced that they are notpresent, it checks to see if the new term satisfies thecondition following the optional ones in the pattern.119a.3.2 A Pattern Is Matched -When a pattern has been matched completely, PHRANcontinues checking all the other patterns on thepattern-list.
When it has finished, PHRAN will take thelongest pattern that was matched and will consider theconcept of its pattern-concept pair to be the meaning ofthe sequence.
If there are several patterns of the samelength :hat we re matched PHRAN will group all theirmeanings together.New patterns are suggested end a disembiguation processfollows, exactly as in the case of a new word being read.For example, the words "the big apple", when recognized,will have two possible meanings: one being a largefruit, the other being New York Clty, PHRAN will checkthe patterns active at that time %0 determine if one ofthese two meanings satisfies the next condition in one ofthe patterns.
If so, then that meaning will be chosen,Otherwise 'a large fruit' will be the default, as it isthe first in the list of possible meanings.4.~ Adverbs And Adverbial PhrasesIn certain cases there is need for slightly modifiednotions of pattern and concept, the most prominentexamples being adverbs and adverbial phrases.
Suchphrases are also recognized through the use of patterns.However, upon recognizing an adverb, PHRAN searcheswithin the active patterns for an action that it canmodify.
When such an action is found the concept part ofthe pair associated with the adverb is used to modify theconcept of the original action.Adverbs such as "quickly" and "slowly" are currentlydefined and can be used to modify conceptualizationscontaining various actions.
Thus PHRAN can handleconstructs like:John ate slowly.Ouickly, John left the house.John left the house quickly.John slowly ate the apple.John wanted slowly to eat the apple.Some special cases of negation are handled by specificpatterns.
For example, the negation of the verb wantusually is interpreted ss meaning "want not" - " ~didn't want to go ~o school" means the same thing as"Mary wanted not to go:to school".
Thus PHRAN conzainsthe specifi~ pattern \[<person> (do> "not" <want><inf-phrase>!
which Is associated with thisinterpretation.~-5 Indexing And Pattern SuggestionRetrieving the phrasal pattern matching a particularutterance from PHRAN's knowledge base is sn importantproblem that we have not yet solved to our completesatisfaction.
We find some consolation in the fact thatthe problem of indexing a large data base is a neccesaryand familiar problem for all Enowledge based systems.We have tried two pattern suggestion mechanisms withPHRAN:I. Keying oatterns off individual words orpreviously matched patterns.2.
Indexing patterns under ordered seouences ofcues go%ten from the sentence a~d phras~Tpaz~erns recognized in it.The first indexing mechanism works but it requires thatany pattern used to recognize a phrasal expressions besuggested by some word in it.
This is unacceptablebecause it will cause the pattern to be suggestedwhenever the word it is triggered by is mentioned.
Thedifficulties inherent in such an indexing scheme can beappreciated by considering which word in the phrase "byana large" should be used to trigger it.
Any choice wemake will cause the pattern ~o be suggested very often incontexts when it is not appropriate.
~nth is  form,FHRAN's ~rocessing roughly resembles ELI's (Riesbeck etel, 19V59.We therefore developed the second mechanism.
The~ atterns-concapt pairs of the database are indexed in s ree.
As words are read, the pattern suggestingmechanism travels down this tree, choosing branchesaccording to the meanings of the words.
It suggests toPHRAN the patterns found at the nodes it has arrived at.The list of nodes is remembered, and when the next wordis read the routine continues to branch from them, inaddition to starting from the root.
In practice, thenumber of nodes in the list is rather smsll.For example, whenever a noun-phrase is followed by anactive form of some verb, the suggesting routineinstructs PHRAN to consider the simple declarative formsof the verb.
When a noun-phrase is followed by the vero'to be' followed by the perfective form of some verb, theroutine instructs PHRAN to consider the passive uses ofthe last verb.
The phrasal pattern that will recognizethe expression "by and large" is found st the nodereaches only after seeing those three worasconsecutively.
In this manner this pattern will besuggested only when neccessary.The main problem with this scheme is that it does notlend itself well to allowing contextual cues to influencethe choice of patterns PHRAN should t ry .
This is onearea where future research will be concentrates.5.O COMPARISON TO OTHER SYSTEMSThere are a number of other natural lenguage processingsystems that either use some notion of patterns orproduce meaning structures as output.
We contrast PHRANw~th some of these.An example of a natural language understanding systemthat produces declarative meaning representations SsRiesbeck's "conceptual analyzer" (Riesbeck, 1974).Riesbeck's system (and the various systems that havedescended from it) works by attaching routines toind~vidusl words.
These routines are generallyresponsible for building pieces of s meaningreprDsentstion.
When a word is reed by the system, theroutines associated with that word are used to build up ameaning structure that eventually denotes the messing ofthe entire utterance.While our sims are much in the spirit of Riesbeck'sanalyzer, we believe there ere both practical andtheoreticsl d~fficulties inherent in his approach.
Forexample, in R~esbeck's conceptual analyzer, specificunderstanding routines are needed for each word known tothe system.
Thus extending the system's vocabularyrequires the creation and ?debugging of new code.
Inaddition, these routines function only in theunderstanding process.
The knowledge they embody isinaccessible to other mechanisms, in particular, toproduction procedures.Moreover, because Riesbeck's approach is word-oriented,it is difficult to incorporate phrssel structures intohis model.
Some word of the phrase must have a routineassociated w~tb it that checks for that phrase.
At best,this implementation is awkward.One of the earliest language understanding systems toincorporate phrasal patterns is Colby's PARRY.
PARRY is8 s~mulation of a paranoid me~tal patient that contains anatural language front and (Psrklnson st al, 19~).
Itreceives a sentence as input and ,na\]yzes it in severalseparate "stages".
In effect, PARRY replaces the inputwi~h sentences of successively simpler form.
In %hesimplified sentence PARRY searches for patterns, of whichthere ere two bssic types: patterns used to interpretthe whole ~entence, snd those used on~y to interpretparts of ~t {relative clauses, for example).For PARRY, the purpose of the natural language analyzeris only to translate the input into a simplified formthat a model of  a paranoid person may use to determine anappropriate response.
No attempt Js made to model theanalyzer itself after a human language user, as we aredoing, nor are claims made to this effect.
A systemattempting to model human language analysis could notpermit several unre\]e+ed passes, the use of s transitionnetwork grsmmsr to interpret only certain sub-strings inthe input, or a rule permitting it to simply ignore partsof the input.This theoretical shortcoming of PARRY - hsving separategrammar rules for the complete sentence ~nd for sub-partso" it - is shsred by Henarix's LYFER (Hendrix.
IO77).LIFER is designed to enable a database to be queriedusJn~ 8 subset of the English language.
As is t~_  casefor PARRY, the natural language ansAysis done by ~Ar~R isnot meant to model humans.
Rather, its function is totranslate the input into instructions and produce s replyas efficiently es possible, and nothing resembling srepresentation of tne  meaning of the input is everl ormea, u: course the purpose of LIFE~ is not to be th ~front end of a system that understands coherent textsand which must therefore perform subsequent inferenceprocesses.
Wh~le LIFER provides s workable solution tothe natural language problem in a limited context I msnygeneral problems of language analysis are not adoresseoin that context.SOPHYE (Burton, 1976) was designed to assist students inlearning about simple electronic circuits.
It canconduct a dialogue with the user in a restricted subsetof the English language, and it uses knowledge aboutpatterns of speech to interpret the input.
SOPHIEaccepts only certain questions and instructionsconcerning a few tasks.
As is the case with LI-FER.
thelangusge utterances acceptable to the system are120restricted to such an extent that many natural languageprocessing problems need not be deelt with and otherproblems have solutions appropriate only to this context.In addition, SOPHIE does not produce any representationof the meanin~ of the input, and it makes more than onepass on the Input i~morlng unknown words, practices thatnave already been crlticized.The augmented finite state transition network (ATN) hasbeen used by a number of researchers to aid in theanalysis of natural language sentences (for example, seeWoods 1970).
However, most systems that use ATN'sincorporate one feature which we find objectioneble onboth theoretical and practical grounds.
This is theseparation of analysis into syntactic and semanticphases.
The efficacy and psychological validity of theseparation of syntactic and sementicprocessing has beenargued at lengthelsewhere (see Schar~ 1975 for example).In addition, most ATN based systems (for .xample Woods'LUNAR program) do not produce represents%ions, butrather, run queries of a data base.In contrast to the systems just described, Wilks'English-French machine ~ranslstor do~s not share severalof their shortcomings (Wilks, 197~).
It produces arepresentation of the meaning of an utterance, and itattempts to deal with unrestricted natural language.
Themaxn difference between Wilk's system and system wedescribe is that Wilks' patterns are matched againstconcepts mentioned in a sentence.
To recognize theseconcepts he attaches representations to words in edictionary.The problem is that this presupposes that there is asimple correspondence between %he form of a concept andthe form of a language utterance.
However, it is thefact that this correspondence is not simple that leads tothe difficulties we are addressing in our work.
In fact,since the correspondence of words to meanings is complex,it would appear ~hat a program like Wilks' translatorwill even~ually need %he kind of knowledge embodied inPHRAN to complete its analysis.One recent attempt at natural language analysis thatradically departs f~om pattern-based approaches is Rieger' and Small's system (Smell, 1978).
This system uses wordexperts rather than patterns as its basic mechsnxsm.~nelr system acknowledges the enormity of the knowledgebase required for language understanding, and proposes sway of addressing the relevant issues.
However, the ideaof puttin~ as much information as possible underindividual words is about as far from our -conception oflanguage analysis as one can get, and we would argue,would exemplify all the problems we have described inword-based systems.ReferencesBecket, Joseph D. (1975).
The phrssel lexicon.
InTheoretical Issues in Natural Language Processing.
R.Scnenk ano B.L.
~a~T~-We~oer ~eds.~.
Camorluge, Mass.Birnbaum, L. and Selfridge, M. (1979).
Problems inconceptual analysis of natural lenguage.
Yale Un versityDepartment of Computer Science Research Report I~8.Burton., Richard R. (1976).
Semantic Grammar: AnEngineering Technique for Constructing Natural LanguageUnderstanding Systems.
BaN Report No.
3a53, Dec 1976.Fillmore, C.J.
(1979).
Innocence: A SecondIdealization for Linguistics.
In Proceedings of theFifth Berkeley Language Symposium, Ber~eiey, c~/l-iTE~nia.Hendrix, Gary G. (197").
~"%e Lifer Menus\]: A Guide toBuilding Practical Netursl Language Interfaces.
SRY!nterns~ionel: AI Center Tachnicel Note 138, Feb 1977.Mitchell, T. F. (1971).
Linguistic "Goings On";Collocations and Other Matters Arising on th~ SyntacticRecord.
Arch~vum Linguisticum 2 (new series 3~-69.IQ7 P~rkinson, R.C., Colby, K.M., and Faught, W.S.
( .
~ ?Conversational Language Comprehension Using IntegratedPattern-Matching and Parsing.
Artificial Inte\]ll~ence 9,111-134.Riesbeck, C. K. (1975).
Conceptual anelysis.
In R. C.Sohenk Conceptual Informetion Processing.
AmericanElsevier ~uoAlsoing uompany, ~nc,, Sew York.R~esbeck C. K. and Schank, R. C. (1975).Comprehension by computer: expectation-based analysis ofsentences in context.
Yale University Resesrch Report78.Schank.
R. C. (1975).
Conceptual InformationProcessing.
American Elsevler ~uollsnlng 5ompeny, Inc.,Row lOr~.Small, S. (1978).
Concegtuel language analysis forstory comprehension.
Technical Repor~ No.
565, Dept.of Computer Science, University of Maryland, CollegePark ,  Maryland.Wilks, Yorick (1973).
An AI Approach to MachineTranslation.
In Computer Models of Thought and Language,R.C.
Schsnk and K.~.
uoioy L eds.-'T, w.H.
:foeman andCo., San Francisco, 1973.Woods, W. A.
(1970).
Transition Network Grommets forNatural Language Anelysis.
CACM 13, 591-606.121
