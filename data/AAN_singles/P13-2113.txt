Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 640?644,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsPart-of-speech tagging with antagonistic adversariesAnders S?gaardCenter for Language TechnologyUniversity of CopenhagenDK-2300 Copenhagen Ssoegaard@hum.ku.dkAbstractSupervised NLP tools and on-line servicesare often used on data that is very dif-ferent from the manually annotated dataused during development.
The perfor-mance loss observed in such cross-domainapplications is often attributed to covari-ate shifts, with out-of-vocabulary effectsas an important subclass.
Many discrim-inative learning algorithms are sensitive tosuch shifts because highly indicative fea-tures may swamp other indicative features.Regularized and adversarial learning algo-rithms have been proposed to be more ro-bust against covariate shifts.
We presenta new perceptron learning algorithm us-ing antagonistic adversaries and compareit to previous proposals on 12 multilin-gual cross-domain part-of-speech taggingdatasets.
While previous approaches donot improve on our supervised baseline,our approach is better across the boardwith an average 4% error reduction.1 IntroductionMost learning algorithms assume that training andtest data are governed by identical distributions;and more specifically, in the case of part-of-speech(POS) tagging, that training and test sentenceswere sampled at random and that they are identi-cally and independently distributed.
Significanceis usually tested across data points in standardNLP test sets.
Such datasets typically contain run-ning text rather than independently sampled sen-tences, thereby violating the assumption that datapoints are independently distributed and sampledat random.
More importantly, significance acrossdata points only says something about the likely-hood of observing the same effect on more datasampled the same way, but says nothing aboutlikely performance on sentences sampled fromdifferent sources or different domains.This paper considers the POS tagging problem,i.e.
where we have training and test data consist-ing of sentences in which all words are assigneda label y chosen from a finite set of class labels{NOUN, VERB, DET,.
.
.
}.
We assume that weare interested in performance across data sets ordomains rather than just performance across datapoints, but that we do not know the target domainin advance.
This is often the case when we developNLP tools and on-line services.
We will do cross-domain experiments using several target domainsin order to compute significance across domains,enabling us to say something about likely perfor-mance on new domains.Several authors have noted how POS taggingperformance is sensitive to cross-domain shifts(Blitzer et al, 2006; Daume III, 2007; Jiang andZhai, 2007), and while most authors have as-sumed known target distributions and pool unla-beled target data in order to automatically correctcross-domain bias (Jiang and Zhai, 2007; Fos-ter et al, 2010), methods such as feature bag-ging (Sutton et al, 2006), learning with randomadversaries (Globerson and Roweis, 2006) andL?-regularization (Dekel and Shamir, 2008) havebeen proposed to improve performance on un-known target distributions.
These methods ex-plicitly or implicitly try to minimize average orworst-case expected error across a set of possi-ble test distributions in various ways.
These al-gorithms are related because of the intimate rela-tionship between adversarial corruption and reg-ularization (Ghaoui and Lebret, 1997; Xu et al,6402009; Hinton et al, 2012).
This paper presents anew method based on learning with antagonisticadversaries.Outline.
Section 2 introduces previous work onrobust perceptron learning, as well as the meth-ods dicussed in the paper.
Section 3 motivatesand introduces learning with antagonistic adver-saries.
Section 4 presents experiments on POS tag-ging and discusses how to evaluate cross-domainperformance.
Learning with antagonistic adver-saries is superior to the other approaches across10/12 datasets with an average error reduction of4% over a supervised baseline.Motivating example.
The problem without-of-vocabulary effects can be illus-trated using a small labeled data set:{x1 = ?1, ?0, 1, 0?
?,x2 = ?1, ?0, 1, 1?
?,x3 =?0, ?0, 0, 0?
?,x4 = ?1, ?0, 0, 1??}.
Say we trainour model on x1?3 and evaluate it on the fourthdata point.
Most discriminate learning algorithmsonly update parameters when training examplesare misclassified.
In this example, a modelinitialized by zero weights would misclassify x1,update the parameter associated with feature x2at a fixed rate ?, and the returned model wouldthen classify all data points correctly.
Hencethe parameter associated with feature x3 wouldnever be updated, although this feature is alsocorrelated with class.
If x2 is missing in our testdata (out-of-vocabulary), we end up classifyingall data points as negative.
In this case, we wouldwrongly predict that x4 is negative.2 Robust perceptron learningOur framework will be averaged perceptron learn-ing (Freund and Schapire, 1999; Collins, 2002).We use an additive update algorithm and aver-age parameters to prevent over-fitting.
In adver-sarial learning, adversaries corrupt the data pointby applying transformations to data points.
An-tagonistic adversaries choose transformations in-formed by the current model parameters w, butrandom adversaries randomly select transforma-tions from a predefined set of possible transforma-tions, e.g.
deletions of at most k features (Glober-son and Roweis, 2006).Feature bagging.
In feature bagging (Sutton et al,2006), the data is represented by different bags offeatures or different views, and the models learnedusing different feature bags are combined by aver-aging.
We can reformulate feature bagging as anadversarial learning problem.
For each pass, theadversary chooses a deleting transformation cor-responding to one of the feature bags.
In Sut-ton et al (2006), the feature bags simply dividethe features into two or more representations.
Inan online setting feature bagging can be modelledas a game between a learner and an adversary, inwhich (a) the adversary can only choose betweendeleting transformations, (b) the adversary cannotsee model parameters when choosing a transfor-mation, and in which (c) the adversary only movesin between passes over the data.1Learning with random adversaries(LRA).
Globerson and Roweis (2006) let anadversary corrupt labeled data during trainingto learn better models of test data with missingfeatures.
They assume that missing featuresare randomly distributed and show that theoptimization problem is a second-order coneprogram.
LRA is an adversarial game in whichthe two players are unaware of the other player?scurrent move, and in particular, where the ad-versary does not see model parameters and onlyrandomly corrupts the data points.
Globersonand Roweis (2006) formulate LRA as a batchlearning problem of minimizing worst case lossunder deleting transformations deleting at mostk features.
This is related to regularization in thefollowing way: If model parameters are chosento minimize expected error in the absence of anyk features, we explicitly prevent under-weightingmore than n ?
k features, i.e.
the model must beable to classify data well in the absence of any kfeatures.
The sparsest possible model would thusassign weights to k + 1 parameters.L?-regularization hedges its bets even more thanadversarial learning by minimizing expected er-ror with max ||w|| < C. In the online setting,this corresponds to playing against an adversarythat clips any weight above a certain threshold C,whether positive or negative (Dekel and Shamir,2008).
In geometric terms the weights are pro-jected back onto the hyper-cube C. A relatedapproach, which is not explored in the experi-ments below, is to regularize linear models towardweights with low variance (Bergsma et al, 2010).1Note that the batch version of feature bagging is an in-stance of group L1 regularization (Jacob et al, 2009; Schmidtand Murphy, 2010; Martins et al, 2011).
Often group regu-larization is about finding sparser models rather than robustmodels.
Sparse models can be obtained by grouping corre-lated features; non-sparse models can be obtained by usingindependent, exhaustive views.6411: X = {?yi,xi?
}Ni=1, ?
deletion rate2: w0 = 0,v = 0, i = 03: for k ?
K do4: for n ?
N do5: ?1 ?
random.sample(P (1) = 1?
?
)6: ?2 ?
||w|| < ?||w|| + ?||w||7: ?
?
(?1 + ?2)(0,1)8: if sign(w ?
xn ?
?)
6= yn then9: wi+1 ?
update(wi)10: i?
i+ 111: end if12: v?
v + wi13: end for14: end for15: return w = v/(N ?K)Figure 1: Learning with antagonistic adversaries3 Learning with antagonistic adversariesThe intuition behind learning with antagonistic ad-versaries is that the adversary should focus on themost predictive features.
In the prediction game,this would allow the adversary to inflict more dam-age, corrupting data points by removing good fea-tures (rather than random ones).
If the adversaryfocuses on the most predictive features, she is im-plicitly regularizing the model to obtain a moreequal distribution of weights.We draw random binary vectors with P (1) =1 ?
?
as in adversarial learning, but deletions areonly effective if ?j = 0 and the weight wj is morethan a standard deviation (?||w||) from the meanof the current absolute weight distribution (?||w||).In other words, we only delete the predictive fea-tures, with predictivity being relative to the currentmean weight.The algorithm is presented in Figure 1.
For eachdata point, we draw a random binary vector ?1with ?
chance of zeros.
?2 is a vector with theith scalar zero if and only if the absolute value ofthe weight wi in w is more than a standard devia-tion higher than the current mean.
The ith scalarin ?
is only zero if the ith scalars in both ?1 and ?2are zero.
The corresponding features are a randomsubset of the predictive features.22The approach taken is similar in spirit to confidence-weighted learning (Dredze et al, 2008).
The intuition behindconfidence-weighted learning is to more agressively updaterare features or features that we are less confident about.
Inlearning with antagonistic adversaries the adversaries deletepredictive features; that is, features that we are confidentabout.
When these features are deleted, we do not updatethe corresponding weights.
In relative terms, we thereforeupdate rare features more aggressively than common ones.Note also that by doing so we regularize toward weights withlow variance (Bergsma et al, 2010).4 ExperimentsWe consider part-of-speech (POS) tagging, i.e.
theproblem of assigning syntactic categories to wordtokens in running text.
POS tagging accuracy isknown to be very sensitive to domain shifts.
Fos-ter et al (2011) report a POS tagging accuracy onsocial media data of 84% using a tagger that ac-chieves an accuracy of about 97% on newspaperdata.
In the case of social media data, many errorsoccur due to different spelling and capitalizationconventions.
The main source of error, though, isthe increased out-of-vocabulary rate, i.e.
the manyunknown words.
While POS taggers can often re-cover the part of speech of a previously unseenword from the context it occurs in, this is harderthan for previously seen words.We use the LXMLS toolkit3 as our baselinewith the default feature model, but use the PTBtagset rather than the Google tagset (Petrov etal., 2011) used by default in the LXMLS toolkit.We use four groups of datasets.
The first groupcomes from the English Web Treebank (EWT),4also used in the Parsing the Web shared task(Petrov and McDonald, 2012).
We train our tag-ger on Sections 2?21 of the WSJ data in the Penn-III Treebank (PTB), Ontonotes 4.0 release.
TheEWT contains development and test data for fivedomains: answers (from Yahoo!
), emails (fromthe Enron corpus), BBC newsgroups, Amazon re-views, and weblogs.
We use the emails develop-ment section for development and test on the re-maining four test sets.
We also do experimentswith additional data from PTB.
For these experi-ments we use the 0th even split of the biomedicalsection (PTB-biomedical) as development data,the 9th split and the chemistry section (PTB-chemistry) as test data, and the remaining biomed-ical data (splits 1?8) as training data.
This datawas also used for developing and testing in theCoNLL 2007 Shared Task (Nivre et al, 2007).Our third group of datasets also comes fromOntonotes 4.0.5 We use the Chinese Ontonotes(CHO) data, covering five different domains.
Weuse newswire for training data and randomly sam-pled broadcasted news for development.
Finallywe do experiments with the Danish section of theCopenhagen Dependency Treebank (CDT).
ForCDT we rely on the treebank meta-data and sin-3https://github.com/gracaninja/lxmls-toolkit4LDC Catalog No.
: LDC2012T13.5LDC Catalog No.
: LDC2011T03.642SP Our L?
LRAEWT-answers 86.04 86.06 85.90 86.06EWT-newsgroups 87.70 87.92 87.78 87.66EWT-reviews 85.96 86.10 85.80 86.00EWT-weblogs 87.59 87.89 87.60 87.54PTB-biomedical 95.05 95.26 95.46 94.43PTB-chemistry 90.32 90.60 90.56 90.58CHO-broadcast 78.38 78.42 78.27 78.28CHO-magazines 78.50 78.57 76.80 78.29CHO-weblogs 79.64 79.76 79.24 79.37CDT-law 93.96 95.64 93.91 94.25CDT-literature 93.93 94.19 94.15 94.15CDT-magazines 94.95 95.06 94.71 95.04Wilcoxon p <0.01macro-av.
err.red 4.0 -1.2 -0.2Table 1: Results (in %).gle out the newspaper section as training data anduse held-out newspaper data for development.We observe two characteristics about ourdatasets: (a) The class distributions are relativelystable across domains.
For CDT, for example,we see almost identical distributions of parts ofspeech, except literature has more prepositions.
(b) The OOV rate is significantly higher across do-mains than within domains.
This holds even forthe PTB datasets, where the OOV rate is 14.6% onthe biomedical test data, but 43.3% on the chem-istry test data.
These two observations confirmthat cross-domain data is primarily biased by co-variate shifts.All learning algorithms do the same number ofpasses over each training data set.
The numberof iterations was set optimizing baseline systemperformance on development data.
For EWT andCHO, we do 10 passes over the data.
For PTB,we do 15 passes over the data, and for CDT, wedo 25 passes over the data.
The deletion rate inadversarial learning was fixed to 0.1% (optimizedon the EWT emails data; not optimized on PTB,CHO or CDT).
In L?-regularization, the parame-ter C was optimized the same way and set to 20.Results are averages over five runs.4.1 ResultsThe results are presented in Table 1.
Learn-ing with antagonistic adversaries performs signifi-cantly better than structured perceptron (SP) learn-ing, L?-regularization and LRA across the board.We follow Demsar (2006) in computing signif-icance across datasets using a Wilcoxon signedrank test.
This is a strong result given that our al-gorithm is as computationally efficient as SP anddoes not pool unlabeled data to adapt to a spe-cific target distribution.
What we see is that let-ting an antagonistic adversary corrupt our labeleddata - somewhat surprisingly, maybe - leads to bet-ter cross-domain performance.
L?-regularizationleads to worse performance, and LRA performsvery similar to SP on average.
Improvementsto LRA have also been explored in Trafalis andGilbert (2007) and Dekel and Shamir (2008).We note that on the in-domain dataset (PTB-biomedical), L?-regularization performs best, butour approach also performs better than the struc-tured perceptron baseline on this dataset.4.2 AnalysisThe number of zero weights or very small weightsis significantly lower for learning with antagonis-tic adversaries than for the baseline structured per-ceptron.
So our models become less sparse.
Onthe other hand, we have more parameters with av-erage weights in our models.
Weights are in otherwords better distributed.
We also observe that pa-rameters are updated slightly more with antago-nistic adversaries.
In our PTB experiments, forexample, the mean weight is 14.2 in structuredperceptron learning, but 14.5 with antagonistic ad-versaries.
On the other hand, weight variance isslightly lower; recall the connection to varianceregularization (Bergsma et al, 2010).
Note thatL?-regularization with C = 20 corresponds toclipping all weights above 20, i.e.
roughly a thirdof the weights in this case.
To validate our intu-itions about what is going on, we also tried to in-crease the deletion rate.
If ?
is increased to 1%,the mean weight goes up to 19.2.
The adversarialmodel is less sparse than the baseline model.A last observation is that the structured percep-tron baseline model expectedly fits the trainingdata better than the robust models.
On CDT, thestructured perceptron has an accuracy of 98.26%on held-out training data, whereas our model hasan accuracy of only 97.85%.
The L?-regularizedhas an accuracy of 97.82%, whereas LRA has anaccuracy of 98.18%.5 ConclusionWe presented a discriminative learning algorithmsfor cross-domain structured prediction that seemsmore robust to covariate shifts than previous ap-proaches.
Our approach was superior to previousapproaches across 12 multilingual cross-domainPOS tagging datasets, with an average error reduc-tion of 4% over a structured perceptron baseline.643ReferencesShane Bergsma, Dekang Lin, and Dale Schuurmans.2010.
Improved natural language learning viavariance-regularization support vector machines.
InCoNLL.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.Michael Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models.
In EMNLP.Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In ACL.Ofer Dekel and Ohad Shamir.
2008.
Learning to clas-sify with missing and corrupted features.
In ICML.Janez Demsar.
2006.
Statistical comparisons of clas-sifiers over multiple data sets.
Journal of MachineLearning Research, 7:1?30.Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.
InICML.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In EMNLP.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Josef Le Roux, Joakim Nivre, Deirde Hogan, andJosef van Genabith.
2011.
From news to comments:Resources and benchmarks for parsing the languageof Web 2.0.
In IJCNLP.Yoav Freund and Robert Schapire.
1999.
Large marginclassification using the perceptron algorithm.
Ma-chine Learning, 37:277?296.Laurent El Ghaoui and Herve Lebret.
1997.
Robustsolutions to least-squares problems with uncertaindata.
In SIAM Journal of Matrix Analysis and Ap-plications.Amir Globerson and Sam Roweis.
2006.
Nightmareat test time: robust learning by feature deletion.
InICML.Geoffrey Hinton, N. Srivastava, A. Krizhevsky,I.
Sutskever, and R. Salakhutdinov.
2012.
Improv-ing neural networks by preventing co-adaptation offeature detectors.
http://arxiv.org/abs/1207.0580.Laurent Jacob, Guillaume Obozinski, and Jean-Philippe Vert.
2009.
Group lasso with overlap andgraph lasso.
In ICML.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in NLP.
In ACL.Andre Martins, Noah Smith, Pedro Aguiar, and MarioFigueiredo.
2011.
Structured sparsity in structuredprediction.
In EMNLP.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-Donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 Shared Task on De-pendency Parsing.
In EMNLP-CoNLL.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 Shared Task on Parsing the Web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Slav Petrov, Dipanjan Das, and Ryan McDonald.2011.
A universal part-of-speech tagset.
CoRRabs/1104.2086.Mark Schmidt and Kevin Murphy.
2010.
Convexstructure learning in log-linear models: beyond pair-wise potentials.
In AISTATS.Charles Sutton, Michael Sindelar, and Andrew McCal-lum.
2006.
Reducing weight undertraining in struc-tured discriminative learning.
In NAACL.T Trafalis and R Gilbert.
2007.
Robust support vec-tor machines for classification and computational is-sues.
Optimization Methods and Software, 22:187?198.Huan Xu, Constantine Caramanis, and Shie Mannor.2009.
Robustness and regularization of support vec-tor machines.
In JMLR.644
