Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1023?1032,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSubtree Extractive Summarization via Submodular MaximizationHajime MoritaTokyo Institute of Technology, Japanmorita@lr.pi.titech.ac.jpHiroya TakamuraTokyo Institute of Technology, Japantakamura@pi.titech.ac.jpRyohei SasanoTokyo Institute of Technology, Japansasano@pi.titech.ac.jpManabu OkumuraTokyo Institute of Technology, Japanoku@pi.titech.ac.jpAbstractThis study proposes a text summarizationmodel that simultaneously performs sen-tence extraction and compression.
Wetranslate the text summarization task intoa problem of extracting a set of depen-dency subtrees in the document cluster.We also encode obligatory case constraintsas must-link dependency constraints in or-der to guarantee the readability of the gen-erated summary.
In order to handle thesubtree extraction problem, we investigatea new class of submodular maximizationproblem, and a new algorithm that hasthe approximation ratio 12(1 ?
e?1).
Ourexperiments with the NTCIR ACLIA testcollections show that our approach outper-forms a state-of-the-art algorithm.1 IntroductionText summarization is often addressed as a taskof simultaneously performing sentence extractionand sentence compression (Berg-Kirkpatrick etal., 2011; Martins and Smith, 2009).
Joint mod-els of sentence extraction and compression havea great benefit in that they have a large degree offreedom as far as controlling redundancy goes.
Incontrast, conventional two-stage approaches (Za-jic et al, 2006), which first generate candidatecompressed sentences and then use them to gen-erate a summary, have less computational com-plexity than joint models.
However, two-stage ap-proaches are suboptimal for text summarization.For example, when we compress sentences first,the compressed sentences may fail to contain im-portant pieces of information due to the lengthlimit imposed on each sentence.
On the otherhand, when we extract sentences first, an impor-tant sentence may fail to be selected, simply be-cause it is long.
Enumerating a huge numberof compressed sentences is also infeasible.
Jointmodels can prune unimportant or redundant de-scriptions without resorting to enumeration.Meanwhile, submodular maximization has re-cently been applied to the text summarization task,and the methods thereof have performed very well(Lin and Bilmes, 2010; Lin and Bilmes, 2011;Morita et al, 2011).
Formalizing summarizationas a submodular maximization problem has an im-portant benefit inthat the problem can be solved byusing a greedy algorithm with a performance guar-antee.We therefore decided to formalize the task of si-multaneously performing sentence extraction andcompression as a submodular maximization prob-lem.
That is, we extract subsentences for mak-ing the summary directly from all available sub-sentences in the documents and not in a stepwisefashion.
However, there is a difficulty with sucha formalization.
In the past, the resulting maxi-mization problem has been often accompanied bythousands of linear constraints representing logi-cal relations between words.
The existing greedyalgorithm for solving submodular maximizationproblems cannot work in the presence of such nu-merous constraints although monotone and non-monotone submodular maximization with con-straints other than budget constraints have beenstudied (Lee et al, 2009; Kulik et al, 2009; Guptaet al, 2010).
In this study, we avoid this difficultyby reducing the task to one of extracting depen-dency subtrees from sentences in the source doc-uments.
The reduction replaces the difficulty ofnumerous linear constraints with another difficultywherein two subtrees can share the same word to-1023ken when they are selected from the same sen-tence, and as a result, the cost of the union of thetwo subtrees is not always the mere sum of theircosts.
We can overcome this difficulty by tacklinga new class of submodular maximization prob-lem: a budgeted monotone nondecreasing sub-modular function maximization with a cost func-tion, where the cost of an extraction unit variesdepending on what other extraction units are se-lected.
By formalizing the subtree extraction prob-lem as this new maximization problem, we cantreat the constraints regarding the grammaticalityof the compressed sentences in a straightforwardway and use an arbitrary monotone submodularword score function for words including our wordscore function (shown later).
We also propose anew greedy algorithm that solves this new class ofmaximization problem with a performance guar-antee 12(1?
e?1).We evaluated our method on by using it to per-form query-oriented summarization (Tang et al,2009).
Experimental results show that it is supe-rior to state-of-the-art methods.2 Related WorkSubmodularity is formally defined as a property ofa set function for a finite universe V .
The functionf : 2V ?
R maps a subset S ?
V to a real value.If for any S, T ?
V , f(S ?
T ) + f(S ?
T ) ?f(S)+f(T ), f is called submodular.
This defini-tion is equivalent to that of diminishing returns,which is well known in the field of economics:f(S ?{u})?
f(S) ?
f(T ?{u})?
f(T ), whereT ?
S ?
V and u is an element of V .
Di-minishing returns means that the value of an el-ement u remains the same or decreases as S be-comes larger.
This property is suitable for sum-marization purposes, because the gain of adding anew sentence to a summary that already containssufficient information should be small.
Therefore,many studies have formalized text summarizationas a submodular maximization problem (Lin andBilmes, 2010; Lin and Bilmes, 2011; Morita etal., 2011).
Their approaches, however, have beenbased on sentence extraction.
To our knowledge,there is no study that addresses the joint task ofsimultaneously performing compression and ex-traction through an approximate submodular max-imization with a performance guarantee.In the field of constrained maximization prob-lems, Kulik et al (2009) proposed an algorithmthat solves the submodular maximization problemunder multiple linear constraints with a perfor-mance guarantee 1?
e?1 in polynomial time.
Al-though their approach can represent more flexibleconstraints, we cannot use their algorithm to solveour problem, because their algorithm needs to enu-merate many combinations of elements.
Integerlinear programming (ILP) formulations can repre-sent such flexible constraints, and they are com-monly used to model text summarization (McDon-ald, 2007).
Berg-Kirkpatrick et al (2011) formu-lated a unified task of sentence extraction and sen-tence compression as an ILP.
However, it is hard tosolve large-scale ILP problems exactly in a practi-cal amount of time.3 Budgeted Submodular Maximizationwith Cost Function3.1 Problem DefinitionLet V be the finite set of all valid subtrees inthe source documents, where valid subtrees aredefined to be the ones that can be regarded asgrammatical sentences.
In this paper, we regardsubtrees containing the root node of the sentenceas valid.
Accordingly, V denotes a set of allrooted subtrees in all sentences.
A subtree con-tains a set of elements that are units in a de-pendency structure (e.g., morphemes, words orclauses).
Let us consider the following problemof budgeted monotone nondecreasing submodu-lar function maximization with a cost function:maxS?V {f(S) : c (S) ?
L} , where S is a sum-mary represented as a set of subtrees, c(?)
is thecost function for the set of subtrees, L is our bud-get, and the submodular function f(?)
scores thesummary quality.
The cost function is not alwaysthe sum of the costs of the covered subtrees, butdepends on the set of the covered elements by thesubtrees.
Here, we will assume that the generatedsummary has to be as long as or shorter than thegiven summary length limit, as measured by thenumber of characters.
This means the cost of asubtree is the integer number of characters it con-tains.V is partitioned into exclusive subsetsB of validsubtrees, and each subset corresponds to the orig-inal sentence from which the valid subtrees de-rived.
However, the cost of a union of subtreesfrom different sentences is simply the sum of thecosts of subtrees, while the cost of a union of sub-trees from the same sentence is smaller than thesum of the costs.
Therefore, the problem can berepresented as follows:1024maxS?V{f(S) :?B?Bc (B ?
S) ?
L}.
(1)For example, if we add a subtree t containingwords {wa,wb,wc} to a summary that alreadycovers words {wa, wb, wd} from the same sen-tence, the additional cost of t is only c({wc}) be-cause wa and wb are already covered1.The problem has two requirements.
The firstrequirement is that the union of valid subtrees isalso a valid subtree.
The second requirement isthat the union of subtrees and a single valid sub-tree have the same score and the same cost if theycover the same elements.
We will refer to the sin-gle valid subtree as the equivalent subtree of theunion of subtrees.
These requirements enable usto represent sentence compression as the extrac-tion of subtrees from a sentence.
This is becausethe requirements guarantee that the extracted sub-trees represent a sentence.3.2 Greedy AlgorithmWe propose Algorithm 1 that solves the maximiza-tion problem (Eq.1).
The algorithm is based onones proposed by Khuller et al (1999) and Krauseet al (2005).
Instead of enumerating all candidatesubtrees, we use a local search to extract the ele-ment that has the highest gain per cost.
In the al-gorithm, Gi indicates a summary set obtained byadding element si to Gi?1.
U means the set ofsubtrees that are not extracted.
The algorithm it-eratively adds to the current summary the elementsi that has the largest ratio of the objective func-tion gain to the additional cost, unless adding itviolates the budget constraint.
We set a parame-ter r that is the scaling factor proposed by Lin andBilmes (2010).
After the loop, the algorithm com-pares Gi with the {s?}
that has the largest value ofthe objective function among all subtrees that areunder the budget, and it outputs the summary can-didate with the largest value.Let us analyze the performance guarantee of Al-gorithm 12.1Each subset B corresponds to a kind of greedoid con-straint.
V implicitly constrains the model such that it canonly select valid subtrees from a set of nodes and edges.2Our performance guarantee is lower than that reportedby Lin and Bilmes (2010).
However, their proof is er-roneous.
In their proof of Lemma 2, they derive ?u ?S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)Crvi, for any i(1 ?
i ?
|G|),from line 4 of their Algorithm 1, which selects the densestelement out of all available elements.
However, the inequal-ity does not hold for i, for which element u selected on line4 is discarded on line 5 of their algorithm.
The performanceguarantee of their algorithm is actually the same as ours, sinceAlgorithm 1 Modified greedy algorithm for budgetedsubmodular function maximization with a cost function .1: G0 ?
?2: U ?
V3: i?
14: while U 6= ?
do5: si ?
argmaxs?U f(Gi?1?{s})?f(Gi?1)(c(Gi?1?
{s})?c(Gi?1))r6: if c({si} ?Gi?1) ?
L then7: Gi ?
Gi?1 ?
{si}8: i?
i + 19: end if10: U ?
U\{si}11: end while12: s??
argmaxs?V,c(s)?L f({s})13: return Gf = argmaxS?{{s?
},Gi} f(S)Theorem 1 For a normalized monotone submod-ular function f(?
), Algorithm 1 has a constantapproximation factor when r = 1 as follows:f(Gf ) ?(12(1?
e?1))f(S?
), (2)where S?
is the optimal solution and, Gf is thesolution obtained by Greedy Algorithm 1.Proof.
See appendix.3.3 Relation with Discrete OptimizationWe argue that our optimization problem can beregarded as an extraction of subtrees rooted at agiven node from a directed graph, instead of froma tree.
Let D be the set of edges of the directedgraph, F be a subset of D that is a subtree.
In thefield of combinatorial optimization, a pair (D, F)is a kind of greedoid: directed branching greedoid(Schmidt, 1991).
A greedoid is a generalization ofthe matroid concept.
However, while matroids areoften used to represent constraints on submodularmaximization problems (Conforti and Cornue?jols,1984; Calinescu et al, 2011), greedoids have notbeen used for that purpose, in spite of their highrepresentation ability.
To our knowledge, this isthe first study that gives a constant performanceguarantee for the submodular maximization undergreedoid (non-matroid) constraints.the guarantee 12 (1?
e?1) was already proved by Krause andGuestrin (2005).
We show a counterexample.
Suppose thatV is { e1(density 4:cost 6), e2(density 2:cost 4), e3(density3:cost 1), e4(density 1:cost 1) }, and cost limit K is 10.
Theoptimal solution is S?
= {e1, e2}.
Their algorithm selectse1, e3, e4 in this order.
However the algorithm selects e2 online 4 after selecting e3, and it drops e2 on line 5.
As a result,e4 selected by the algorithm does not satisfy the inequality?u ?
S?\Gi?1, ?u(Gi?1)Cru ?
?vi (Gi?1)Crvi.10254 Joint Model of Extraction andCompressionWe will formalize the unified task of sentencecompression and extraction as a budgeted mono-tone nondecreasing submodular function maxi-mization with a cost function.
In this formaliza-tion, a valid subtree of a sentence represents acandidate of a compressed sentence.
We will re-fer to all valid subtrees of a given sentence as avalid set.
A valid set corresponds to all candi-dates of the compression of a sentence.
Note thatalthough we use the valid set in the formaliza-tion, we do not have to enumerate all the candi-dates for each sentence.
Since, from the require-ments, the union of valid subtrees is also a validsubtree in the valid set, the model can extract oneor more subtrees from one sentence, and generatea compressed sentence by merging those subtreesto generate an equivalent subtree.
Therefore, thejoint model can extract an arbitrarily compressedsentence as a subtree without enumerating all can-didates.
The joint model can remove the redundantpart as well as the irrelevant part of a sentence, be-cause the model simultaneously extracts and com-presses sentences.
We can approximately solve thesubtree extraction problem by using Algorithm 1.On line 5 of the algorithm, the subtree extractionis performed as a local search that finds maximaldensity subtrees from the whole documents.
Themaximal density subtree is a subtree that has thehighest score per cost of subtree.
We use a costfunction to represent the cost, which indicates thelength of word tokens in the subtree.In this paper, we address the task of summariza-tion of Japanese text by means of sentence com-pression and extraction.
In Japanese, syntacticsubtrees that contain the root of the dependencytree of the original sentence often make gram-matical sentences.
This means that the require-ments mentioned in Section 3.1 that a union ofvalid subtrees is a valid and equivalent tree is of-ten true for Japanese.
The root indicates the pred-icate of a sentence, and it is syntactically modi-fied by other prior words.
Some modifying wordscan be pruned.
Therefore, sentence compressioncan be represented as edge pruning.
The linguis-tic units we extract are bunsetsu phrases, whichare syntactic chunks often containing a functionalword after one or more content words.
We will re-fer to bunsetsu phrases as phrases for simplicity.Since Japanese syntactic dependency is generallydefined between two phrases, we use the phrasesas the nodes of subtrees.In this joint model, we generate a compressedsentence by extracting an arbitrary subtree from adependency tree of a sentence.
However, not allsubtrees are always valid.
The sentence generatedby a subtree can be unnatural even though the sub-tree contains the root node of the sentence.
Toavoid generating such ungrammatical sentences,we need to detect and retain the obligatory de-pendency relations in the dependency tree.
Weaddress this problem by imposing must-link con-straints if a phrase corresponds to an obligatorycase of the main predicate.
We merge obligatoryphrases with the predicate beforehand so that themerged nodes make a single large node.Although we focus on Japanese in this pa-per, our approach can be applied to English andother languages if certain conditions are satisfied.First, we need a dependency parser of the lan-guage in order to represent sentence compressionas dependency tree pruning.
Moreover, although,in Japanese, obligatory cases distinguish whichedges of the dependency tree can be pruned or not,we need another technique to distinguish them inother languages.
For example we can distinguishobligatory phrases from optional ones by using se-mantic role labeling to detect arguments of predi-cates.
The adaptation to other languages is left forfuture work.4.1 Objective FunctionWe extract subtrees from sentences in order tosolve the query-oriented summarization problemas a unified one consisting of sentence compres-sion and extraction.
We thus need to allocate aquery relevance score to each node.
Off-the-shelfsimilarity measures such as the cosine similarity ofbag-of-words vectors with query terms would al-locate scores to the terms that appear in the query,but would give no scores to terms that do not ap-pear in it.
With such a similarity, sentence com-pression extracts nearly only the query terms andfails to contain important information.
Instead,we used Query SnowBall (QSB) (Morita et al,2011) to calculate the query relevance score ofeach phrase.
QSB is a method for query-orientedsummarization, which calculates the similarity be-tween query terms and each word by using co-occurrences within the source documents.
Al-though the authors of QSB also provided scoresof word pairs to avoid putting excessive penalties1026on word overlaps, we do not score word pairs.
Thescore function is supermodular as a score functionof subtree extraction3, because the union of twosubtrees can have extra word pairs that are not in-cluded in either subtree.
If the extra pair has a pos-itive score, the score of the union is greater thanthe sum of the score of the subtrees.
This violatesthe definition of submodularity, and invalidates theperformance guarantee of our algorithms.We designed our objective function by combin-ing this relevance score with a penalty for redun-dancy and too-compressed sentences.
Importantwords that describe the main topic should occurmultiple times in a good summary.
However, ex-cessive overlap undermines the quality of a sum-mary, as do irrelevant words.
Therefore, the scoresof overlapping words should be lower than thoseofnew words.
The behavior can be represented by asubmodular objective function that reduces wordscores depending on those already included in thesummary.
Furthermore, a summary consisting ofmany too-compressed sentences would lack read-ability.
We thus gives a positive reward to longsentences.
The positive reward leads to a natu-ral summary being generated with fewer sentencesand indirectly penalizes too short sentences.
Ourpositive reward for long sentences is representedasreward(S) = c(S)?
|S|, (3)where c(S) is the cost of summary S, and |S| is thenumber of sentences in S. Since a sentence mustcontain more than one character, the reward con-sistently gives a positive score, and gives a higherscore to a summary that consists of fewer sen-tences.Let d be the damping rate, countS(w) be thenumber of sentences containing word w in sum-mary S, words(S) be the set of words included insummary S, qsb(w) be the query relevance scoreof word w, and ?
be a parameter that adjusts therate of sentence compression.
Our score functionfor a summary S is as follows:f(S) =?w?words(S)???countS(w)?1?i=0qsb(w)di??
?+ ?
reward(S).
(4)An optimization problem with this objectivefunction cannot be regarded as an ILP problem be-cause it contains non-linear terms.
It is also ad-3The score is still submodular for the purpose of sentenceextraction.vantageous that the submodular maximization candeal with such objective functions.
Note that theobjective function is such that it can be calculatedaccording to the type of word.
Due to the na-ture of the objective function, we can use dynamicprogramming to effectively search for the subtreewith the maximal density.4.2 Local Search for Maximal DensitySubtreeLet us now discuss the local search used on line5 of Algorithm 1.
We will use a fast algorithm tofind the maximal density subtree (MDS) of a givensentence for each cost in Algorithm 1.Consider the objective function Eq.
4, We canignore the second term of the reward functionwhile looking for the MDS in a sentence becausethe number of sentences is the same for everyMDS in a sentence.
That is, the gain function ofadding a subtree to a summary can be representedas the sum of gains for words:g(t) =?w?t{gainS(w) + freqt(w)c(w)?
},gainS(w) = qsb(w)dcountS(w),where freqt(w) is the number of ws in subtreet, and gainS(w) is the gain of adding the wordw to the summary S. Our algorithm is based ondynamic programming, and it selects a subtree thatmaximizes the gain function per cost.When the word gain is a constant, the algorithmproposed by Hsieh et al (2010) can be used tofind the MDS.
We extended this algorithm to workfor submodular word gain functions that are notconstant.
Note that the gain of a word that oc-curs only once in the sentence, can be treated asa constant.
In what follows, we will describe anextended algorithm to find the MDS even if thereis word overlap.For example, let us describe how to obtain theMDS in the case of a binary tree.
First let us tacklethe case in which the gain is always constant.
Letn be a node in the tree, a and b be child nodes of n,c(n) be the cost of n, mdsca be the MDS rooted ata and have cost c. mdsn = {mdsc(n)n , .
.
.
,mdsLn}denotes the set of MDSs for each cost and its rootnode n. The valid subtrees rooted at n can be ob-tained by taking unions of n with one or both oft1 ?
mdsa and t2 ?
mdsb.
mdscn is the union thathas the largest gain over the union with the cost ofc (by enumerating all the unions).
The MDS for1027the sentence root can be found by calculating eachmdscn from the bottom of the tree to the top.Next, let us consider the objective function thatreturns the sum of values of submodular word gainfunctions.
When there is no word overlap withinthe union, we can obtain mdscn in the same man-ner as for the constant gain.
In contrast, if theunion includes word overlap, the gain is less thanthe sum of gains: g(mdscn) ?
g(n) + g(mdska) +g(mdsc?k?c(n)b ), where k and c are variables.
Thescore reduction can change the order of the gainsof the union.
That is, it is possible that anotherunion without word overlaps will have a largergain.
Therefore, the algorithm needs to knowwhether each t ?
mdsn has the potential to haveword overlaps with other MDSs.
Let O be the setof words that occur twice or more in the sentenceon which the local seach focuses.
The algorithmstores MDS for each o ?
O, as well as each cost.By storing MDS for each o and cost as shownin Fig.
1, the algorithm can find MDS with thelargest gain over the combinations of subtrees.Algorithm 2 shows the procedure.
In it, t andmdenote subtrees, words(t) returns a set of wordsin the subtree, g(t) returns the gain of t, tree(n)means a tree consisting of node n, and t ?m de-notes the union of subtrees: t and m. subt in-dicates a set of current maximal density subtreesamong the combinations calculated before.
newtindicates a set of temporary maximal density sub-trees for the combinations calculated from line 4to 8. subt[cost,ws] indicates a element of subt thathas a cost cost and contains a set of words ws.newt[cost,ws] is defined similarly.
Line 1 sets subtto a set consisting of a subtree that indicates noden itself.
The algorithm calculates maximal den-sity subtrees within combinations of the root noden and MDSs rooted at child nodes of n. Line 3iteratively adds MDSs rooted at a next child nodeto the combinations; the algorithm then calculatesMDSs newt between subt and the MDSs of thechild node.
The procedure from line 6 to 8 selectsa subtree that has a larger gain from the tempo-rary maximal subtree and the union of t and m.The computational complexity of this algorithm isO(NC2) when there is no word overlap within thesentence, where C denotes the cost of the wholesentence, and N denotes the number of nodes inthe sentence.
The complexity order is the sameas that of the algorithm of Hsieh et al (2010).When we treat word overlaps, we need to countAlgorithm 2 Algorithm for finding maximal densitysubtree for each cost: MDSs.Function: MDSsRequire: root node n1: subt[c(n),words(n)?O] = tree(n)2: newt = ?3: for i ?
child node of n do4: for t ?MDSs(i) do5: for m ?
subt do6: index = [c(t ?m), words(t ?m) ?
O]7: newtindex = argmaxj?
{newtindex,t?m} g(j)8: end for9: end for10: subt = newt11: end for12: return subtFigure 1: Maximal density subtree extraction.
Theright table enumerates the subtrees rooted at w2 inthe left tree for all indices.
The number in eachtree node is the score of the word.all unions of combinations of the stored MDSs.There are at most (C2|O|) MDSs that the algo-rithm needs to store at each node.
Therefore thetotal computational complexity is O(NC222|O|).Since it is unlikely that a sentence contains manyword tokens of one type, the computational costmay not be so large in practical situations.5 Experimental SettingsWe evaluate our method on Japanese QA testcollections from NTCIR-7 ACLIA1 and NTCIR-8 ACLIA2 (Mitamura et al, 2008; Mitamura etal., 2010).
The collections contain questions andweighted answer nuggets.
Our experimental set-tings followed the settings of (Morita et al, 2011),except for the maximum summary length.
Wegenerated summaries consisting of 140 Japanesecharacters or less, with the question as the queryterms.
We did this because our aim is to use ourmethod in mobile situations.
We used ?ACLIA1test data?
to tune the parameters, and evaluated ourmethod on ?ACLIA2 test?
data.We used JUMAN (Kurohashi and Kawahara,2009a) for word segmentation and part-of-speechtagging, and we calculated idf over Mainichinewspaper articles from 1991 to 2005.
For the de-1028POURPRE Precision Recall F1 F3Lin and Bilmes (2011) 0.215 0.126 0.201 0.135 0.174Subtree extraction (SbE) 0.268 0.238 0.213 0.159 0.190Sentence extraction (NC) 0.278 0.206 0.215 0.139 0.183Table 1: Results on ACLIA2 test data.pendency parsing, we used KNP (Kurohashi andKawahara, 2009b).
Since KNP internally has aflag that indicates either an ?obligatory case?
or an?adjacent case?, we regarded dependency relationsflagged by KNP as obligatory in the sentence com-pression.
KNP utilizes Kyoto University?s caseframes (Kawahara and Kurohashi, 2006) as the re-source for detecting obligatory or adjacent cases.To evaluate the summaries, we followed thepractices of the TAC summarization tasks (Dang,2008) and NTCIR ACLIA tasks, and computedpyramid-based precision with the allowance pa-rameter, recall, and F?
(where ?
is 1 or 3)scores.
The allowance parameter was determinedfrom the average nugget length for each questiontype of the ACLIA2 collection (Mitamura et al,2010).
Precision and recall are computed from thenuggets that the summary covered along with theirweights.
One of the authors of this paper man-ually evaluated whether each nugget matched thesummary.
We also used the automatic evaluationmeasure, POURPRE (Lin and Demner-Fushman,2006).
POURPRE is based on word matchingof reference nuggets and system outputs.
We re-garded as stopwords the most frequent 100 wordsin Mainichi articles from 1991 to 2005 (the doc-ument frequency was used to measure the fre-quency).
We also set the threshold of nuggetmatching as 0.5 and binarized the nugget match-ing, following the previous study (Mitamura et al,2010).
We tuned the parameters by using POUR-PRE on the development dataset.Lin and Bilmes (2011) designed a monotonesubmodular function for query-oriented summa-rization.
Their succinct method performed wellin DUC from 2004 to 2007.
They proposed apositive diversity reward function in order to de-fine a monotone submodular objective function forgenerating a non-redundant summary.
The diver-sity reward gives a smaller gain for a biased sum-mary, because it consists of gains based on threeclusters and calculates a square root score withrespect to each sentence.
The reward also con-tains a score for the similarity of a sentence tothe query, for purposes of query-oriented summa-Recall Length # of nuggetsSubtree extraction 0.213 11,143 100Reconstructed (RC) 0.228 13,797 108Table 2: Effect of sentence compression.rization.
Their objective function also includes acoverage function based on the similarity wi,j be-tween sentences.
In the coverage function minfunction limits the maximum gain ?
?i?V wi,j ,which is a small fraction ?
of the similarity be-tween a sentence j and the all source documents.The objective function is the sum of the positivereward R and the coverage function L over thesource documents V , as follows:F(S) = L(S) +3?k=1?kRQ,k(S),L(S) = ?i?Vmin???
?j?Swi,j , ??k?Vwi,k???
,RQ,k =?c?Ck????
?j?S?c( ?N?i?Vwi,j + (1?
?
)rj,Q),where ?, ?
and ?k are parameters, and rj,Q repre-sents the similarity between sentence j and queryQ.
We tuned the parameters on the developmentdataset.
Lin and Bilmes (2011) used three clustersCk with different granularities, which were calcu-lated in advance.
We set the granularity to (0.2N ,0.15N , 0.05N ) according to the settings of them,where N is the number of sentences in a docu-ment.We also regarded as stopwords ????
(tell),????
(know),?
??
(what)?
and their conjugatedforms, which are excessively common in ques-tions.
For the query expansion in the baseline, weused Japanese WordNet to obtain synonyms andhypernyms of query terms.6 ResultsTable 1 summarizes our results.
?Subtree ex-traction (SbE)?
is our method, and ?Sentence ex-traction (NC)?
is a version of our method with-out compression.
The NC has the same objec-tive function but only extracts sentences.
The F1-measure and F3-measure of our method are 0.159and 0.190 respectively, while those of the state-of-1029the-art baseline are 0.135 and 0.174 respectively.Unfortunately, since the document set is small, thedifference is not statistically significant.
Compar-ing our method with the one without compression,we can see that there are improvements in the F1and F3 scores of the human evaluation, whereasthe POURPRE score of the version of our methodwithout compression is higher than that of ourmethod with compression.
The compression im-proved the precision of our method, but slightlydecreased the recall.For the error analyses, we reconstructed theoriginal sentences from which our method ex-tracted the subtrees.
Table 2 shows the statisticsof the summaries of SbE and reconstructed sum-maries (RC).
The original sentences covered 108answer nuggets in total, and 8 of these answernuggets were dropped by the sentence compres-sion.
Comparing the results of SbE and RC, wecan see that the sentence compression caused therecall of SbE to be 7% lower than that of RC.However, the drop is relatively small in light ofthe fact that the sentence compression can discard19% of the original character length with SbE.This suggests that the compression can efficientlyprune words while avoiding pruning informativecontent.Since the summary length is short, we can selectonly two or three sentences for a summary.
AsMorita et al (2011) mentioned, answer nuggetsoverlap each other.
The baseline objective func-tion R tends to extract sentences from variousclusters.
If the answer nuggets are present in thesame cluster, the objective function does not fit thesituation.
However, our methods (SbE and NC)have a parameter d that can directly adjust overlappenalty with respect to word importance as wellas query relevance.
This may help our methods tocover similar answer nuggets.
In fact, the develop-ment data resulted in a relatively high parameter d(0.8) for NC compared with 0.2 for SbE.7 Conclusions and Future WorkWe formalized a query-oriented summarization,which is a task in which one simultaneously per-forms sentence compression and extraction, as anew optimization problem: budgeted monotonenondecreasing submodular function maximizationwith a cost function.
We devised an approximatealgorithm to solve the problem in a reasonablecomputational time and proved that its approxima-tion rate is 12(1 ?
e?1).
Our approach achievedan F3-measure of 0.19 on the ACLIA2 Japanesetest collection, which is 9.2 % improvement overa state-of-the-art method using a submodular ob-jective function.Since our algorithm requires that the objectivefunction is the sum of word score functions, ourproposed method has a restriction that we cannotuse an arbitrary monotone submodular function asthe objective function for the summary.
Our fu-ture work will improve the local search algorithmto remove this restriction.
As mentioned before,we also plan to adapt of our system to other lan-guages.AppendixHere, we analyze the performance guarantee ofAlgorithm 1.
We use the following notation.
S?
isthe optimal solution, cu(S) is the residual cost ofsubtree u when S is already covered, and i?
is thelast step before the algorithm discards a subtrees ?
S?
or a part of the subtree s. This is becausethe subtree does not belong to either the approxi-mate solution or the optimal solution.
We can re-move the subtree s?
from V without changing theapproximate rate.
si is the i-th subtree obtained byline 5 of Algorithm 1.
Gi is the set obtained afteradding subtree si to Gi?1 from the valid set Bi.Gf is the final solution obtained by Algorithm 1.f(?)
: 2V ?
R is a monotone submodular func-tion.We assume that there is an equivalent sub-tree with any union of subtrees in a valid set B:?t1, t2,?te, te ?
{t1, t2}.
Note that for any or-der of the set, the cost or profit of the set is fixed:?ui?S={u1,...,u|S|} cui(Si?1) = c(S).Lemma 1 ?X,Y ?
V, f(X) ?
f(Y ) +?u?X\Y ?u(Y ), where ?u(S) = f(S ?
{u}) ?f(S).The inequality can be derived from the definitionof submodularity.
2Lemma 2 For i = 1, .
.
.
, i?+1, when 0 ?
r ?
1,f(S?
)?f(Gi?1)?Lr |S?|1?rcsi (Gi?1)(f(Gi?1?
{si})?f(Gi?1)),where cu(S)=c(S?{u})?c(S).Proof.
From line 5 of Algorithm 1, we have?u ?
S?\Gi?1,?u(Gi?1)cu(Gi?1)r?
?si(Gi?1)csi(Gi?1)r.Let B be a valid set, and union be a func-tion that returns the union of subtrees.
We have1030?T ?
B, ?b ?
B, b = union(T ), because wehave an equivalent tree b ?
B for each unionof trees T in a valid set B.
That is, for anyset of subtrees, we have an equivalent set of sub-trees, where bi ?
Bi.
Without loss of generality,we can replace the difference set S?\Gi?1 witha set T ?i?1 = {b0, .
.
.
, b|T ?i?1|} that does not con-tain any two elements extracted from the samevalid set.
Thus when 0 ?
r ?
1 and 0 ?i ?
i?
+ 1, ?s?\Gi?1 (Gi?1)cS?\Gi?1 (Gi?1)r =?T ?i?1 (Gi?1)cT ?i?1 (Gi?1)r , and?bj ?
T ?i?1,?bj (Gi?1)cbj (Gi?1)r?
?si (Gi?1)csi (Gi?1)r .
Thus,?T ?i?1 (Gi?1) =?u?T ?i?1?u(Gi?1)?
?si (Gi?1)csi (Gi?1)r?u?T ?i?1cu(Gi?1)r?
?si (Gi?1)csi (Gi?1)r |T?i?1|(?u?T ?i?1cu(Gi?1)|T ?i?1|)r?
?si (Gi?1)csi (Gi?1)r |T?i?1|1?r(?u?T ?i?1cu(?))r?
?si (Gi?1)csi (Gi?1)r |S?|1?rLr,where the second inequality is from Ho?lder?s in-equality.
The third inequality uses the submodu-larity of the cost function,cu(Gi?1) = c({u} ?Gi?1)?
c(Gi?1) ?
cu(?
)and the fact that |S?| ?
|S?\Gi?1| ?
|T ?i?1|, and?u?T ?i?1 cu(?)
= c(T?i?1) ?
L .As a result, we have?s?\Gi?1(Gi?1) = ?T ?i?1(Gi?1)?
?si(Gi?1)csi(Gi?1)r|S?|1?rLr.Let X = S?
and Y = Gi?1.
Applying Lemma1 yieldsf(S?)
?
f(Gi?1) + ?u?S?\Gi?1(Gi?1).?
f(Gi?1) +?si(Gi?1)csi(Gi?1)|S?|1?rLr.The lemma follows as a result.Lemma 3 For a normalized monotone submodu-lar f(?
), for i = 1, .
.
.
, i?
+ 1 and 0 ?
r ?
1 andletting si be the i-th unit added into G and Gi bethe set after adding si, we havef(Gi) ?(1?i?k=1(1?
csk(Gk?1)rLr|S?|1?r))f(S?).Proof.
This is proved similarly to Lemma 3 of(Krause and Guestrin, 2005) using Lemma 2.Proof of Theorem 1.
This is proved similarly toTheorem 1 of (Krause and Guestrin, 2005) usingLemma 3.ReferencesTaylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies - Volume 1, HLT ?11, pages481?490, Stroudsburg, PA, USA.
Association forComputational Linguistics.Calinescu Calinescu, Chandra Chekuri, Martin Pa?l,and Jan Vondra?k.
2011.
Maximizing a monotonesubmodular function subject to a matroid constraint.SIAM Journal on Computing, 40(6):1740?1766.Michele Conforti and Ge?rard Cornue?jols.
1984.
Sub-modular set functions, matroids and the greedy al-gorithm: Tight worst-case bounds and some gener-alizations of the rado-edmonds theorem.
DiscreteApplied Mathematics, 7(3):251 ?
274.Hoa Trang Dang.
2008.
Overview of the tac2008 opinion question answering and summariza-tion tasks.
In Proceedings of Text Analysis Confer-ence.Anupam Gupta, Aaron Roth, Grant Schoenebeck, andKunal Talwar.
2010.
Constrained non-monotonesubmodular maximization: offline and secretaryalgorithms.
In Proceedings of the 6th interna-tional conference on Internet and network eco-nomics, WINE?10, pages 246?257, Berlin, Heidel-berg.
Springer-Verlag.Sun-Yuan Hsieh and Ting-Yu Chou.
2010.
Theweight-constrained maximum-density subtree prob-lem and related problems in trees.
The Journal ofSupercomputing, 54(3):366?380, December.Daisuke Kawahara and Sadao Kurohashi.
2006.
Afully-lexicalized probabilistic model for japanesesyntactic and case structure analysis.
In Proceedingsof the main conference on Human Language Tech-nology Conference of the North American Chap-ter of the Association of Computational Linguistics,HLT-NAACL ?06, pages 176?183, Stroudsburg, PA,USA.
Association for Computational Linguistics.Samir Khuller, Anna Moss, and Joseph S. Naor.
1999.The budgeted maximum coverage problem.
Infor-mation Processing Letters, 70(1):39?45.Andreas Krause and Carlos Guestrin.
2005.
Anote on the budgeted maximization on submodularfunctions.
Technical Report CMU-CALD-05-103,Carnegie Mellon University.1031Ariel Kulik, Hadas Shachnai, and Tami Tamir.
2009.Maximizing submodular set functions subject tomultiple linear constraints.
In Proceedings ofthe twentieth Annual ACM-SIAM Symposium onDiscrete Algorithms, SODA ?09, pages 545?554,Philadelphia, PA, USA.
Society for Industrial andApplied Mathematics.Sadao Kurohashi and Daisuke Kawahara, 2009a.Japanese Morphological Analysis System JUMAN6.0 Users Manual.
http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?JUMAN.Sadao Kurohashi and Daisuke Kawahara, 2009b.
KNparser (Kurohashi-Nagao parser) 3.0 Users Man-ual.
http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?KNP.Jon Lee, Vahab S. Mirrokni, Viswanath Nagarajan, andMaxim Sviridenko.
2009.
Non-monotone submod-ular maximization under matroid and knapsack con-straints.
In Proceedings of the 41st annual ACMsymposium on Theory of computing, STOC ?09,pages 323?332, New York, NY, USA.
ACM.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submod-ular functions.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 912?920, Stroudsburg, PA,USA.
Association for Computational Linguistics.Hui Lin and Jeff Bilmes.
2011.
A class of submodularfunctions for document summarization.
In Proceed-ings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 510?520,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Jimmy Lin and Dina Demner-Fushman.
2006.
Meth-ods for automatically evaluating answers to com-plex questions.
Information Retrieval, 9(5):565?587, November.Andre?
F. T. Martins and Noah A. Smith.
2009.
Sum-marization with a joint model for sentence extractionand compression.
In Proceedings of the Workshopon Integer Linear Programming for Natural Lan-gauge Processing, ILP ?09, pages 1?9, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Ryan McDonald.
2007.
A study of global inferencealgorithms in multi-document summarization.
InProceedings of the 29th European conference on IRresearch, ECIR?07, pages 557?564, Berlin, Heidel-berg.
Springer-Verlag.Teruko Mitamura, Eric Nyberg, Hideki Shima,Tsuneaki Kato, Tatsunori Mori, Chin-Yew Lin, Rui-hua Song, Chuan-Jie Lin, Tetsuya Sakai, DonghongJi, and Noriko Kando.
2008.
Overview of theNTCIR-7 ACLIA Tasks: Advanced Cross-LingualInformation Access.
In Proceedings of the 7th NT-CIR Workshop.Teruko Mitamura, Hideki Shima, Tetsuya Sakai,Noriko Kando, Tatsunori Mori, Koichi Takeda,Chin-Yew Lin, Ruihua Song, Chuan-Jie Lin, andCheng-Wei Lee.
2010.
Overview of the ntcir-8 acliatasks: Advanced cross-lingual information access.In Proceedings of the 8th NTCIR Workshop.Hajime Morita, Tetsuya Sakai, and Manabu Okumura.2011.
Query snowball: a co-occurrence-based ap-proach to multi-document summarization for ques-tion answering.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies: short pa-pers - Volume 2, HLT ?11, pages 223?229, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Wolfgang Schmidt.
1991.
Greedoids and searches indirected graphs.
Discrete Mathmatics, 93(1):75?88,November.Jie Tang, Limin Yao, and Dewei Chen.
2009.
Multi-topic based query-oriented summarization.
In Pro-ceedings of 2009 SIAM International ConferenceData Mining (SDM?2009), pages 1147?1158.David M. Zajic, Bonnie J. Dorr, Jimmy Lin, andRichard Schwartz.
2006.
Sentence compressionas a component of a multi-document summariza-tion system.
In Proceedings of the 2006 Doc-ument Understanding Conference (DUC 2006) atNLT/NAACL 2006.1032
