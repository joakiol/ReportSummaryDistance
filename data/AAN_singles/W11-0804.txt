Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 14?19,Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational LinguisticsDecreasing lexical data sparsity in statistical syntactic parsing - experimentswith named entitiesDeirdre Hogan, Jennifer Foster and Josef van GenabithNational Centre for Language TechnologySchool of ComputingDublin City UniversityDublin 9, Irelanddhogan,jfoster,josef@computing.dcu.ieAbstractIn this paper we present preliminary exper-iments that aim to reduce lexical data spar-sity in statistical parsing by exploiting infor-mation about named entities.
Words in theWSJ corpus are mapped to named entity clus-ters and a latent variable constituency parseris trained and tested on the transformed cor-pus.
We explore two different methods formapping words to entities, and look at the ef-fect of mapping various subsets of named en-tity types.
Thus far, results show no improve-ment in parsing accuracy over the best base-line score; we identify possible problems andoutline suggestions for future directions.1 IntroductionTechniques for handling lexical data sparsity inparsers have been important ever since the lexical-isation of parsers led to significant improvementsin parser performance (Collins, 1999; Charniak,2000).
The original treebank set of non-terminal la-bels is too general to give good parsing results.
Toovercome this problem, in lexicalised constituencyparsers, non-terminals are enriched with lexical in-formation.
Lexicalisation of the grammar vastlyincreases the number of parameters in the model,spreading the data over more specific events.
Statis-tics based on low frequency events are not as reliableas statistics on phenomena which occur regularly inthe data; frequency counts involving words are typi-cally sparse.Word statistics are also important in more re-cent unlexicalised approaches to constituency pars-ing such as latent variable parsing (Matsuzaki et al,2005; Petrov et al, 2006).
The basic idea of latentvariable parsing is that rather than enrich the non-terminal labels by augmenting them with words, aset of enriched labels which can encapsulate the syn-tactic behaviour of words is automatically learnedvia an EM training mechanism.Parsers need to be able to handle both low fre-quency words and words occurring in the test setwhich were unseen in the training set (unknownwords).
The problem of rare and unknown words isparticularly significant for languages where the sizeof the treebank is small.
Lexical sparseness is alsocritical when running a parser on data that is in a dif-ferent domain to the domain upon which the parserwas trained.
As interest in parsing real world dataincreases, a parsers ability to adequately handle out-of-domain data is critical.In this paper we examine whether clusteringwords based on their named entity category can beuseful for reducing lexical sparsity in parsing.
In-tuitively word tokens in the corpus such as, say,?Dublin?
and ?New York?
should play similar syn-tactic roles in sentences.
Likewise, it is difficult tosee how different people names could have differ-ent discriminatory influences on the syntax of sen-tences.
This paper describes experiments at replac-ing word tokens with special named entity tokens(person names are mapped to PERSON tokens andso on).
Words in the original WSJ treebank aremapped to entity types extracted from the BBN cor-pus (Weischedel and Brunstein, 2005) and a latentvariable parser is trained and tested on the mappedcorpus.
Ultimately, the motivation behind groupingwords together in this fashion is to make it easier for14the parser to recognise regularities in the data.1The structure of paper is as follows: A brief sum-mary of related work is given in Section 2.
Thisincludes an outline of a common treatment of lowfrequency and rare words in constituency parsing,involving a mapping process that is similar to thenamed entity mappings.
Section 3 presents the ex-periments carried out, starting with a short introduc-tion of the named entity resource used in our exper-iments and a description of the types of basic entitymappings we examine.
In ?3.1 and ?3.2 we describethe two different types of mapping technique.
Re-sults are presented in Section 4, followed by a briefdiscussion in Section 5 indicating possible problemsand avenues worth pursuing.
Finally, we conclude.2 Related WorkMuch previous work on parsing and multiword units(MWUs) adopts the words-with-spaces approachwhich treats MWUs as one token (by concatenat-ing the words together) (Nivre and Nilsson, 2004;Cafferkey et al, 2007; Korkontzelos and Manand-har, 2010).
Alternative approaches are that of Finkeland Manning (2009) on joint parsing and named en-tity recognition and the work of (Wehrli et al, 2010)which uses collocation information to rank compet-ing hypotheses in a symbolic parser.
Also relatedis work on MWUs and grammar engineering, suchas (Zhang et al, 2006; Villavicencio et al, 2007)where automatically detected MWUs are added tothe lexicon of a HPSG grammar to improve cover-age.Our work is most similar to the words-with-spaces approach.
Our many-to-one experiments(see ?3.1) in particular are similar to previouswork on parsing words-with-spaces, except that wemap words to entity types rather than concatenatedwords.
Results are difficult to compare however, dueto different parsing methodologies, different typesof MWUs, as well as different evaluation methods.Other relevant work is the integration of named1It is true that latent variable parsers automatically inducecategories for similar words, and thus might be expected toinduce a category for say names of people if examples ofsuch words occurred in similar syntactic patterns in the data.Nonetheless, the problem of data sparsity remains - it is diffi-cult even for latent variable parsers to learn accurate patternsbased on words which only occur say once in the training set.entity types in a surface realisation task by Rajku-mar et al (2009) and the French parsing experimentsof (Candito and Crabbe?, 2009; Candito and Sed-dah, 2010) which involve mapping words to clustersbased on morphology as well as clusters automati-cally induced via unsupervised learning on a largecorpus.2.1 Parsing unknown wordsMost state-of-the-art constituency parsers (e.g.
(Petrov et al, 2006; Klein and Manning, 2003))take a similar approach to rare and unknown words.At the beginning of the training process very lowfrequency words in the training set are mapped tospecial UNKNOWN tokens.
In this way, someprobability mass is reserved for occurrences of UN-KNOWN tokens and the lexicon contains produc-tions for such tokens (X ?
UNKNOWN), with as-sociated probabilities.
When faced with a word inthe test set that the parser has not seen in its train-ing set - the unknown word is mapped to the specialUNKNOWN token.In syntactic parsing, rather than map all low fre-quency words to one generic UNKNOWN type, itis useful to have several different clusters of un-known words, grouped according to morphologi-cal and other ?surfacey?
clues in the original word.For example, certain suffixes in English are strongpredictors for the part-of-speech tag of the word(e.g.
?ly?)
and so all low frequency words end-ing in ?ly?
are mapped to ?UNKNOWN-ly?.
Aswell as suffix information, UNKNOWN words arecommonly grouped based on information on capi-talisation and hyphenation.
Similar techniques forhandling unknown words have been used for POStagging (e.g.
(Weischedel et al, 1993; Tseng etal., 2005)) and are used in the Charniak (Char-niak, 2000), Berkeley (Petrov et al, 2006) and Stan-ford (Klein and Manning, 2003) parsers, as well asin the parser used for the experiments in this paper,an in-house implementation of the Berkeley parser.3 ExperimentsThe BBN Entity Type Corpus (Weischedel andBrunstein, 2005) consists of sentences from thePenn WSJ corpus, manually annotated with namedentities.
The Entity Type corpus includes annota-15type count examplesPERSON 11254 Kim CattrallPER DESC 21451 president,chief executive officer,FAC 383 office, Rockefeller CenterFAC DESC 2193 chateau ,stadiums, golf courseORGANIZATION 24239 Securities and Exchange CommissionORG DESC 15765 auto maker, collegeGPE 10323 Los Angeles,South AfricaGPE DESC 1479 center, nation, countryLOCATION 907 North America,Europe, Hudson RiverNORP 3269 Far EasternPRODUCT 667 Maxima, 300ZXPRODUCT DESC 1156 carsEVENT 296 Vietnam war,HUGO ,World War IIWORK OF ART 561 Revitalized Classics Take..LAW 300 Catastrophic Care Act,Bill of RightsLANGUAGE 62 LatinCONTACT INFO 30 555 W. 57th St.PLANT 172 crops, treeANIMAL 355 hawksSUBSTANCE 2205 gold,drugs, oilDISEASE 254 schizophrenia,alcoholismGAME 74 football senior tennis and golf toursTable 1: Name expression entity types (sections 02-21)tion for three classes of named entity: name expres-sions, time expressions and numeric expressions (inthis paper we focus on name expressions).
Theseare further broken down into types.
Table 1 displaysname expression entity types, their frequency in thetraining set (sections 02-21), as well as some illus-trative examples from the training set data.We carried out experiments with different subsetsof entity types.
In one set of experiments, all nameexpression entities were mapped, with no restrictionon the types (ALL NAMED).
We also carriedout experiments on a reduced set of named entities- where only entities marked as PERSON, ORGA-NIZATION, or GPE and LOCATION were mapped(REDUCED).
Finally, we ran experiments whereonly one type of named entity was mapped at a time.In all cases the words in the named entities were re-placed by their entity type.3.1 Many-to-one MappingIn the many-to-one mapping all words in a namedentity were replaced with one named entity typetoken.
This approach is distinct from the words-with-spaces approach previously pursued in parsingwhere, for example, ?New York?
would be replacedwith ?New York?.
Instead, in our experiments ?NewYork?
is replaced with ?GPE?
(geo-political entity).In both approaches, the parser is forced to respectunk map NE map #unks f-score POSgenericnone (baseline 1) 2966 (4.08%) 88.69 95.57ALL NAMED 1908 (2.73%) 89.21 95.49REDUCED 2122 (3.02%) 89.43 96.08Person 2671 (3.68%) 88.98 95.55Organisation 2521 (3.55%) 89.38 95.92Location 2945 (4.05%) 89.00 95.62sigsnone (baseline 2) 2966 (4.08%) 89.72 96.51ALL NAMED 1908 (2.73%) 89.67 95.99REDUCED 2122 (3.02%) 89.53 96.65Person 2671 (3.68%) 89.32 96.47Organisation 2521 (3.55%) 89.53 96.64Location 2945 (4.05%) 89.20 96.52Table 2: Many-to-One Parsing Results.the multiword unit boundary (and analyses whichcontain constituents that cross the MWU boundarywill not be considered by the parser).
Intuitively,this should help parser accuracy and speed.
The ad-vantage of mapping the word tokens to their entitytype rather than to a words-with-spaces token is thatin addition we will be reducing data sparsity.One issue with the many-to-one mapping is thatin evaluation exact comparison with a baseline re-sult is difficult because the tokenisation of test andgold sets is different.
When named entities spanmore than one word, we are reducing the numberof words in the sentences.
As parsers tend to do bet-ter on short sentences than on long sentences, thiscould make parsing somewhat easier.
However, wefound that the average number of words in a sen-tence before and after this mapping does not changeby much.
The average number of words in the devel-opment set is 23.9.
When we map words to namedentity tokens (ALL NAMED), the average dropsby just one word to 22.9.23.2 One-to-one MappingIn the one-to-one experiments we replaced eachword in named entity with a named entity type to-ken (e.g.
Ada Lovelace ?
pperson pperson).3 Themotivation was to measure the effect of reducingword sparsity using named entities without alteringthe original tokenisation of the data.42A related issue is that the resulting parse tree will lack ananalysis for the named entity.3The entity type was given an extra letter where needed (e.g.?pperson?)
to avoid the conflation of a mapped entity token withan original word (e.g.
?person?)
in the corpus.4Note, where there is punctuation as part of a named entitywe do not map the punctuation.16unk map NE map #unks f-score POSgenericnone (baseline 1) 2966 (4.08%) 88.69 95.57ALL NAMED 1923 (2.64%) 89.28 94.99REDUCED 2122 (2.90%) 88.76 95.76Person 2654(3.65%) 88.95 95.57Organisation 2521 (3.45%) 88.80 95.59Location 2945 (4.04%) 88.88 95.66sigsnone (baseline 2) 2966 (4.08%) 89.72 96.51ALL NAMED 1923 (2.64%) 89.36 95.64REDUCED 2122 (2.90%) 89.01 96.32Person 2654(3.65%) 89.30 96.52Organisation 2521 (3.45%) 89.29 96.30Location 2945 (4.04%) 89.55 96.54Table 3: One-to-One Parsing ResultsIn an initial experiment, where the mapping wassimply the word to the named entity type, many sen-tences received no parse.
This happened often whena named entity consisted of three or more words andresulted in a sentence such as ?But while the Oor-ganization Oorganization Oorganization Oorganiza-tion did n?t fall apart Friday?.
We found that refiningthe named entity by adding the number of the wordin the entity to the mapping resolved the coverageproblem.
The example sentence is now: ?But whilethe Oorganization1 Oorganization2 Oorganization3Oorganization4 did n?t fall apart Friday?.
See ?5 fora possible explanation for the parser?s difficulty withone-to-one mappings to coarse grained entity types.4 ResultsTable 2 and Table 3 give the results for the many-to-one and one-to-one experiments respectively.
Re-sults are given against a baseline where unknownsare given a ?generic?
treatment (baseline 1) - i.e.they are not clustered according to morphologicaland surface information - and for the second baseline(baseline 2), where morphological or surface featuremarkers (sigs) are affixed to the unknowns.5The results indicate that though lexical spar-sity is decreasing, insofar as the number of un-known words (#unks column) in the developmentset decreases with all named entity mappings, thenamed entity clusters are not informative enoughand parser accuracy falls short of the previous bestresult.
For all experiments, a pattern that emerges5For all experiments, a split-merge cycle of 5 was used.
Fol-lowing convention, sections 02-21 were used for training.
Sec-tions 22 and 24 (sentences less than or equal to 100 words) wereused for the development set.
As experiments are ongoing wedo not report results on a test set.is that mapping words to named entities improvesresults when low frequency words are mapped toa generic UNKNOWN token.
However, when lowfrequency words are mapped to more fine-grainedUNKNOWN tokens, mapping words to named enti-ties decreases accuracy marginally.If a particular named entity occurs often in the textthen data sparsity is possibly not a problem for thisword.
Rather than map all occurrences of a namedentity to its entity type, we experimented with map-ping only low frequency entities.
These named en-tity mapping experiments now mirror more closelythe unknown words mappings - low frequency en-tities are mapped to special entity types, then theparser maps all remaining low frequency words toUNKNOWN types.
Table 4 shows the effect of map-ping only entities that occur less than 10 times in thetraining set, to the person type and the reduced setof entity types.
Results somewhat improve for allbut one of the one-to-one experiments, but nonethe-less remain below the best baseline result.
There isstill no advantage in mapping low frequency personname words to, say, the person cluster, rather than toan UNKNOWN-plus-signature cluster.5 DiscussionOur results thus far suggest that clusters based onmorphology or surface clues are more informativethan the named entity clusters.For the one-to-one mappings one obvious prob-lem that emerged is that all words in entities (in-cluding function words for example) get mapped toa generic named entity token.
A multi-word namedentity has its own internal syntactic structure, re-flected for example in its sequence of part-of-speechtags.
By replacing each word in the entity withthe generic entity token we end up loosing informa-tion about words, conflating words that take differ-ent part-of-speech categories, and in fact make pars-ing more difficult.
The named entity clusters in thiscase are too coarse-grained and words with differentsyntactic properties are merged into the one cluster,something we would like to avoid.In future work, as well as avoiding mapping morecomplex named entities, we will refine the namedentity clusters by attaching to the entity type signa-tures similar to those attached to the UNKNOWN17unk map NE map one2one f-score many2one f-scoregenericPerson 88.95 88.98Person < 10 88.97 89.05Reduced 88.76 89.43Reduced < 10 89.51 88.85sigsPerson 89.30 89.32Person < 10 89.49 89.33Reduced 89.01 89.53Reduced < 10 89.42 89.15Table 4: Measuring the effect of mapping only low fre-quency named entities.types.
It would also be interesting to examine the ef-fect of mapping other types of named entities, suchas dates and numeric expressions.
Finally, we intendtrying similar experiments on out-of-domain data,such as social media text where unknown words aremore problematic.6 ConclusionWe have presented preliminary experiments whichtest the novel technique of mapping word tokens tonamed entity clusters, with the aim of improvingparser accuracy by reducing data sparsity.
While ourresults so far are disappointing, we have identifiedpossible problems and outlined future experiments,including suggestions for refining the named entityclusters so that they become more syntactically ho-mogenous.ReferencesConor Cafferkey, Deirdre Hogan, and Josef van Gen-abith.
2007.
Multi-word units in treebank-based prob-abilistic parsing and generation.
In Proceedings of the10th International Conference on Recent Advances inNatural Language Processing (RANLP-07), Borovets,Bulgaria.Marie Candito and Benoit Crabbe?.
2009.
Improving gen-erative statistical parsing with semi-supervised wordclustering.
In Proceedings of the International Work-shop on Parsing Technologies (IWPT-09).Marie Candito and Djame?
Seddah.
2010.
Lemmatizationand statistical lexicalized parsing of morphologically-rich languages.
In Proceedings of the NAACL/HLTWorkshop on Statistical Parsing of MorphologicallyRich Languages (SPMRL).Eugene Charniak.
2000.
A maximum entropy-inspiredparser.
In Proceedings of the 1st North AmericanChapter of the Association for Computational Linguis-tics (NAACL).Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Jenny Rose Finkel and Christopher D. Manning.
2009.Joint parsing and named entity recognition.
In Pro-ceedings of the North American Chapter of the Asso-ciation for Computational Linguistics (NAACL-2009).Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association of ComputationalLinguistics (ACL).Ioannis Korkontzelos and Suresh Manandhar.
2010.
Canrecognising multiword expressions improve shallowparsing?
In Proceedings of the Conference of theNorth American Chapter of the ACL (NAACL-10), LosAngeles, California.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic cfg with latent annotations.
InProceedings of the 43rd Annual Meeting of the ACL,pages 75?82, Ann Arbor, June.Joakim Nivre and Jens Nilsson.
2004.
Multiword unitsin syntactic parsing.
In Workshop on Methodologiesand Evaluation of Multiword Units in Real-World Ap-plications.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact and inter-pretable tree annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th Annual Meeting of the ACL, Sydney,Australia, July.Rajakrishnan Rajkumar, Michael White, and DominicEspinosa.
2009.
Exploiting named entity classes inccg surface realisation.
In Proceedings of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL-09).Huihsin Tseng, Daniel Jurafsky, and Christopher Man-ning.
2005.
Morpholgical features help pos taggingof unknown words across language varieties.
In Pro-ceedings of the Fourth SIGHAN Workshop on ChineseLanguage Processing.Aline Villavicencio, Valia Kordoni, Yi Zhang, MarcoIdiart, and Carlos Ramisch.
2007.
Validation andevaluation of automatically acquired multiword ex-pressions for grammar engineering.
In Proceedings ofthe Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL).Eric Wehrli, Violeta Seretan, and Luke Nerima.
2010.Sentence analysis and collocation identification.
InProceedings of the Workshop on Multiword Expres-sion: From Theory to Applications (MWE).Ralph Weischedel and Ada Brunstein.
2005.
BBN pro-noun coreference and entity type corpus.
In TehcnicalReport.18Ralph Weischedel, Richard Schwartz, Jeff Palmucci,Marie Meteer, and Lance Ramshaw.
1993.
Copingwith ambiguity and unknown words through proba-bilistic models.
Computational Linguistics, 19(2).Yi Zhang, Valia Kordoni, Aline Villavicencio, and MarcoIdiart.
2006.
Automated multiword expression pre-diction for grammar engineering.
In Proceedings ofthe Workshop on Multiword Expressions: Identifyingand Exploiting Underlying Properties.19
