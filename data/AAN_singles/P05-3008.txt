Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 29?32, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Voice Enabled Procedure Browserfor the International Space StationManny Rayner, Beth Ann Hockey, Nikos Chatzichrisafis, Kim FarrellICSI/UCSC/RIACS/NASA Ames Research CenterMoffett Field, CA 94035?1000mrayner@riacs.edu, bahockey@email.arc.nasa.govNikos.Chatzichrisafis@web.de, kfarrell@email.arc.nasa.govJean-Michel RendersXerox Research Center Europe6 chemin de Maupertuis, Meylan, 38240, FranceJean-Michel.Renders@xrce.xerox.comAbstractClarissa, an experimental voice enabledprocedure browser that has recently beendeployed on the International Space Sta-tion (ISS), is to the best of our knowl-edge the first spoken dialog system inspace.
This paper gives backgroundon the system and the ISS procedures,then discusses the research developed toaddress three key problems: grammar-based speech recognition using the Regu-lus toolkit; SVM based methods for openmicrophone speech recognition; and ro-bust side-effect free dialogue managementfor handling undos, corrections and con-firmations.1 OverviewAstronauts on the International Space Station (ISS)spend a great deal of their time performing com-plex procedures.
Crew members usually have todivide their attention between the task and a pa-per or PDF display of the procedure.
In addition,since objects float away in microgravity if not fas-tened down, it would be an advantage to be ableto keep both eyes and hands on the task.
Clarissa,an experimental speech enabled procedure navigator(Clarissa, 2005), is designed to address these prob-lems.
The system was deployed on the ISS on Jan-uary 14, 2005 and is scheduled for testing later thisyear; the initial version is equipped with five XML-encoded procedures, three for testing water qualityand two for space suit maintenance.
To the best ofour knowledge, Clarissa is the first spoken dialogueapplication in space.The system includes commands for navigation:forward, back, and to arbitrary steps.
Other com-mands include setting alarms and timers, record-ing, playing and deleting voice notes, opening andclosing procedures, querying system status, and in-putting numerical values.
There is an optional modethat aggressively requests confirmation on comple-tion of each step.
Open microphone speech recog-nition is crucial for providing hands free use.
Tosupport this, the system has to discriminate betweenspeech that is directed to it and speech that is not.Since speech recognition is not perfect, and addi-tional potential for error is added by the open micro-phone task, it is also important to support commandsfor undoing or correcting bad system responses.The main components of the Clarissa system area speech recognition module, a classifier for exe-cuting the open microphone accept/reject decision,a semantic analyser, and a dialogue manager.
Therest of this paper will briefly give background on thestructure of the procedures and the XML representa-tion, then describe the main research content of thesystem.2 Voice-navigable proceduresISS procedures are formal documents that typicallyrepresent many hundreds of person hours of prepa-ration, and undergo a strict approval process.
Onerequirement in the Clarissa project was that the pro-cedures should be displayed visually exactly as they29Figure 1: Adding voice annotations to a group ofstepsappear in the original PDF form.
However, readingthese procedures verbatim would not be very useful.The challenge is thus to let the spoken version di-verge significantly from the written one, yet still besimilar enough in meaning that the people who con-trol the procedures can be convinced that the twoversions are in practice equivalent.Figure 1 illustrates several types of divergencesbetween the written and spoken versions, with?speech bubbles?
showing how procedure text is ac-tually read out.
In this procedure for space suit main-tenance, one to three suits can be processed.
Thegroup of steps shown cover filling of a ?dry LCVG?.The system first inserts a question to ask which suitsrequire this operation, and then reads the passageonce for each suit, specifying each time which suit isbeing referred to; if no suits need to be processed, itjumps directly to the next section.
Step 51 points theuser to a subprocedure.
The spoken version asks ifthe user wants to execute the steps of the subproce-dure; if so, it opens the LCVG Water Fill procedureand goes directly to step 6.
If the user subsequentlygoes past step 17 of the subprocedure, the systemwarns that the user has gone past the required steps,and suggests that they close the procedure.Other important types of divergences concern en-try of data in tables, where the system reads out anappropriate question for each table cell, confirms thevalue supplied by the user, and if necessary warnsabout out-of-range values.Rec Patterns ErrorsReject Bad TotalText LF 3.1% 0.5% 3.6%Text Surface 2.2% 0.8% 3.0%Text Surface+LF 0.8% 0.8% 1.6%SLM Surface 2.8% 7.4% 10.2%GLM LF 1.4% 4.9% 6.3%GLM Surface 2.9% 4.8% 7.7%GLM Surface+LF 1.0% 5.0% 6.0%Table 1: Speech understanding performance on sixdifferent configurations of the system.3 Grammar-based speech understandingClarissa uses a grammar-based recognition architec-ture.
At the start of the project, we had two main rea-sons for choosing this approach over the more popu-lar statistical one.
First, we had no available trainingdata.
Second, the system was to be designed for ex-perts who would have time to learn its coverage, andwho moreover, as former military pilots, were com-fortable with the idea of using controlled language.Although there is not much to be found in the litera-ture, an earlier study in which we had been involved(Knight et al, 2001) suggested that grammar-basedsystems outperformed statistical ones for this kindof user.
Given that neither of the above arguments isvery strong, we wanted to implement a frameworkwhich would allow us to compare grammar-basedmethods with statistical ones, and retain the optionof switching from a grammar-based framework to astatistical one if that later appeared justified.
TheRegulus and Alterf platforms, which we have devel-oped under Clarissa and other earlier projects, aredesigned to meet these requirements.The basic idea behind Regulus (Regulus, 2005;Rayner et al, 2003) is to extract grammar-based lan-guage models from a single large unification gram-mar, using example-based methods driven by smallcorpora.
Since grammar construction is now acorpus-driven process, the same corpora can be usedto build statistical language models, facilitating a di-rect comparison between the two methodologies.On its own, however, Regulus only permits com-parison at the level of recognition strings.
Alterf(Rayner and Hockey, 2003) extends the paradigm to30ID Rec Features Classifier Error ratesClassification TaskIn domain Out AvGood Bad1 SLM Confidence Threshold 5.5% 59.1% 16.5% 11.8% 10.1%2 GLM Confidence Threshold 7.1% 48.7% 8.9% 9.4% 7.0%3 SLM Confidence + Lexical Linear SVM 2.8% 37.1% 9.0% 6.6% 7.4%4 GLM Confidence + Lexical Linear SVM 2.8% 48.5% 8.7% 6.3% 6.2%5 SLM Confidence + Lexical Quadratic SVM 2.6% 23.6% 8.5% 5.5% 6.9%6 GLM Confidence + Lexical Quadratic SVM 4.3% 28.1% 4.7% 5.5% 5.4%Table 2: Performance on accept/reject classification and the top-level task, on six different configurations.the semantic level, by providing a trainable seman-tic interpretation framework.
Interpretation uses aset of user-specified patterns, which can match ei-ther the surface strings produced by both the statisti-cal and grammar-based architectures, or the logicalforms produced by the grammar-based architecture.Table 1 presents the result of an evaluation, car-ried out on a set of 8158 recorded speech utterances,where we compared the performance of a statisti-cal/robust architecture (SLM) and a grammar-basedarchitecture (GLM).
Both versions were trained offthe same corpus of 3297 utterances.
We also showresults for text input simulating perfect recognition.For the SLM version, semantic representations areconstructed using only surface Alterf patterns; forthe GLM and text versions, we can use either sur-face patterns, logical form (LF) patterns, or both.The ?Error?
columns show the proportion of ut-terances which produce no semantic interpretation(?Reject?
), the proportion with an incorrect seman-tic interpretation (?Bad?
), and the total.Although the WER for the GLM recogniser isonly slightly better than that for the SLM recogniser(6.27% versus 7.42%, 15% relative), the differenceat the level of semantic interpretation is considerable(6.3% versus 10.2%, 39% relative).
This is mostlikely accounted for by the fact that the GLM ver-sion is able to use logical-form based patterns, whichare not accessible to the SLM version.
Logical-formbased patterns do not appear to be intrinsically moreaccurate than surface (contrast the first two ?Text?rows), but the fact that they allow tighter integrationbetween semantic understanding and language mod-elling is intuitively advantageous.4 Open microphone speech processingThe previous section described speech understand-ing performance in terms of correct semantic inter-pretation of in-domain input.
However, open micro-phone speech processing implies that some of the in-put will not be in-domain.
The intended behaviourfor the system is to reject this input.
We wouldalso like it, when possible, to reject in-domain inputwhich has not been correctly recognised.Surface output from the Nuance speech recog-niser is a list of words, each tagged with a confidencescore; the usual way to make the accept/reject deci-sion is by using a simple threshold on the averageconfidence score.
Intuitively, however, we should beable to improve the decision quality by also takingaccount of the information in the recognised words.By thinking of the confidence scores as weights,we can model the problem as one of classifying doc-uments using a weighted bag of words model.
Itis well known (Joachims, 1998) that Support Vec-tor Machine methods are very suitable for this task.We have implemented a version of the method de-scribed by Joachims, which significantly improveson the naive confidence score threshold method.Performance on the accept/reject task can be eval-uated directly in terms of the classification error.
Wecan also define a metric for the overall speech under-standing task which includes the accept/reject deci-sion, as a weighted loss function over the differenttypes of error.
We assign weights of 1 to a false re-ject of a correct interpretation, 2 to a false accept ofan incorrectly interpreted in-domain utterance, and 3to a false accept of an out-of-domain utterance.
This31captures the intuition that correcting false accepts isconsiderably harder than correcting false rejects, andthat false accepts of utterances not directed at thesystem are worse than false accepts of incorrectlyinterpreted utterances.Table 2 summarises the results of experimentscomparing performance of different recognisers andaccept/reject classifiers on a set of 10409 recordedutterances.
?GLM?
and ?SLM?
refer respectively tothe best GLM and SLM recogniser configurationsfrom Table 1.
?Av?
refers to the average classi-fier error, and ?Task?
to a normalised version of theweighted task metric.
The best SVM-based method(line 6) outperforms the best naive threshold method(line 2) by 5.4% to 7.0% on the task metric, a relativeimprovement of 23%.
The best GLM-based method(line 6) and the best SLM-based method (line 5) areequally good in terms of accept/reject classificationaccuracy, but the GLM?s better speech understand-ing performance means that it scores 22% better onthe task metric.
The best quadratic kernel (line 6)outscores the best linear kernel (line 4) by 13%.
Allthese differences are significant at the 5% level ac-cording to the Wilcoxon matched-pairs test.5 Side-effect free dialogue managementIn an open microphone spoken dialogue applicationlike Clarissa, it is particularly important to be ableto undo or correct a bad system response.
Thissuggests the idea of representing discourse statesas objects: if the complete dialogue state is an ob-ject, a move can be undone straightforwardly byrestoring the old object.
We have realised this ideawithin a version of the standard ?update seman-tics?
approach to dialogue management (Larssonand Traum, 2000); the whole dialogue managementfunctionality is represented as a declarative ?updatefunction?
relating the old dialogue state, the inputdialogue move, the new dialogue state and the out-put dialogue actions.In contrast to earlier work, however, we includetask information as well as discourse information inthe dialogue state.
Each state also contains a back-pointer to the previous state.
As explained in detailin (Rayner and Hockey, 2004), our approach per-mits a very clean and robust treatment of undos, cor-rections and confirmations, and also makes it muchsimpler to carry out systematic regression testing ofthe dialogue manager component.AcknowledgementsWork at ICSI, UCSC and RIACS was supportedby NASA Ames Research Center internal fund-ing.
Work at XRCE was partly supported by theIST Programme of the European Community, un-der the PASCAL Network of Excellence, IST-2002-506778.
Several people not credited here as co-authors also contributed to the implementation ofthe Clarissa system: among these, we would par-ticularly like to mention John Dowding, SusanaEarly, Claire Castillo, Amy Fischer and VladimirTkachenko.
This publication only reflects the au-thors?
views.ReferencesClarissa, 2005. http://www.ic.arc.nasa.gov/projects/clarissa/.As of 26 April 2005.T.
Joachims.
1998.
Text categorization with support vec-tor machines: Learning with many relevant features.In Proceedings of the 10th European Conference onMachine Learning, Chemnitz, Germany.S.
Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, and I. Lewin.
2001.
Comparing grammar-basedand robust approaches to speech understanding: a casestudy.
In Proceedings of Eurospeech 2001, pages1779?1782, Aalborg, Denmark.S.
Larsson and D. Traum.
2000.
Information state anddialogue management in the TRINDI dialogue moveengine toolkit.
Natural Language Engineering, Spe-cial Issue on Best Practice in Spoken Language Dia-logue Systems Engineering, pages 323?340.M.
Rayner and B.A.
Hockey.
2003.
Transparent com-bination of rule-based and data-driven approaches in aspeech understanding architecture.
In Proceedings ofthe 10th EACL (demo track), Budapest, Hungary.M.
Rayner and B.A.
Hockey.
2004.
Side effect freedialogue management in a voice enabled procedurebrowser.
In Proceedings of INTERSPEECH 2004, JejuIsland, Korea.M.
Rayner, B.A.
Hockey, and J. Dowding.
2003.
Anopen source environment for compiling typed unifica-tion grammars into speech recognisers.
In Proceed-ings of the 10th EACL, Budapest, Hungary.Regulus, 2005. http://sourceforge.net/projects/regulus/.As of 26 April 2005.32
