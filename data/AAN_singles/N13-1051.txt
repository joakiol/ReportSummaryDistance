Proceedings of NAACL-HLT 2013, pages 482?486,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsParameter Estimation for LDA-FramesJir???
MaternaCentre for Natural Language ProcessingFaculty of Informatics, Masaryk UniversityBotanicka?
68a, 602 00, Brno, Czech Republicxmaterna@fi.muni.czAbstractLDA-frames is an unsupervised approach foridentifying semantic frames from semanti-cally unlabeled text corpora, and seems tobe a useful competitor for manually createddatabases of selectional preferences.
The mostlimiting property of the algorithm is such thatthe number of frames and roles must be pre-defined.
In this paper we present a modifi-cation of the LDA-frames algorithm allowingthe number of frames and roles to be deter-mined automatically, based on the characterand size of training data.1 IntroductionSemantic frames and valency lexicons are usefullexical sources capturing semantic roles valid fora set of lexical units.
The structures of linked seman-tic roles are called semantic frames.
Linguists areusing them for their ability to describe an interfacebetween syntax and semantics.
In practical naturallanguage processing applications, they can be used,for instance, for the word sense disambiguation taskor in order to resolve ambiguities in syntactic analy-sis of natural languages.The lexicons of semantic frames or verb valenciesare mainly created manually or semi-automaticallyby highly trained linguists.
Manually created lex-icons involve, for instance, a well-known lexi-con of semantic frames FrameNet (Ruppenhoferet al 2006) or a lexicon of verb valencies VerbNet(Schuler, 2006).
These and other similar lexical re-sources have many promising applications, but suf-fer from several disadvantages:?
Creation of them requires manual work oftrained linguists which is very time-consumingand expensive.?
Coverage of the resources is usually small orlimited to some specific domain.?
Most of the resources do not provide anyinformation about relative frequency of us-age in corpora.
For instance, both patterns[Person] acquire [Physical object]and [Person] acquire [Disease] reflectcorrect usage of verb acquire, but the formeris much more frequent in English.?
Notion of semantic classes and frames is sub-jectively biased when the frames are createdmanually without corpus evidence.In order to avoid those problems we proposeda method for creating probabilistic semantic framescalled LDA-frames (Materna, 2012).
The main ideaof LDA-frames is to generate the set of semanticframes and roles automatically by maximizing pos-terior probability of a probabilistic model on a syn-tactically annotated training corpus.
A semantic roleis represented as probability distribution over all itsrealizations in the corpus, a semantic frame as a tu-ple of semantic roles, each of them connected withsome grammatical relation.
For every lexical unit(a verb in case of computing verb valencies), a prob-ability distribution over all semantic frames is gen-erated, where the probability of a frame correspondsto the relative frequency of usage in the corpus fora given lexical unit.
An example of LDA-frames482computed on the British National Corpus is avail-able at the LDA-frames website1.The original LDA-frames algorithm has two pa-rameters that must be predefined ?
number of framesand number of roles ?
which is the most limitingproperty of the algorithm.
A simple cross-validationapproach can be used in case of very small data.However, real data is much bigger and it is not re-commended to use such techniques.
For example,the inference on the British National Corpus using asingle core 2.4 GHz CPU takes several days to com-pute one reasonable combination of parameters.In this paper we present a non-parametric modifi-cation of the LDA-frames algorithm allowing to de-termine the parameters automatically, based on thecharacter and size of training data.2 LDA-FramesLDA-frames (Materna, 2012) is an unsupervised ap-proach for identifying semantic frames from seman-tically unlabeled text corpora.
In the LDA-frames,a frame is represented as a tuple of semantic roles,each of them connected with a grammatical rela-tion i.e.
subject, object, modifier, etc.
These framesare related to a lexical unit via probability distribu-tion.
Every semantic role is represented as probabil-ity distribution over its realizations.The method of automatic identification of se-mantic frames is based on probabilistic generativeprocess.
Training data for the algorithm consistsof tuples of grammatical relation realizations ac-quired using a dependency parser from the train-ing corpus for every lexical unit.
For example, sup-pose that the goal is to generate semantic frames ofverbs from a corpus for grammatical relations sub-ject and object.
The training data for lexical uniteat may look like {(peter, cake), (man,breakfast), (dog, meat), ...}, wherethe first component of the tuples corresponds to sub-ject and the second to object.In the generative process, each grammatical rela-tion realization is treated as being generated froma given semantic frame according to the realiza-tion distribution of the corresponding semantic role.Supposing the number of frames is given by param-eter F , the number of semantic roles by R, the num-1http://nlp.fi.muni.cz/projekty/lda-frames/ber of slots (grammatical relations) by S and the sizeof vocabulary is V .
The realizations are generated asfollows.For each lexical unit u ?
{1, 2, .
.
.
, U}:1.
Choose a frame distribution ?u from Dir(?).2.
For each lexical unit realizationt ?
{1, 2, .
.
.
, Tu} choose a frame fu,t fromMultinomial(?u), where fu,t ?
{1, 2, .
.
.
, F}.3.
For each slot s ?
{1, 2, .
.
.
, S} of framefu,t, generate a grammatical relation realiza-tion wu,t,s from Multinomial(?rfu,t,s), whererf,s is a projection (f, s) 7?
r, which assignsa semantic role for each slot s in frame f .
Themultinomial distribution of realizations, sym-bolized by ?r, for semantic role r is generatedfrom Dir(?
).The graphical model for LDA-Frames is shownin figure 1.
It is parametrized by hyperparameters ofprior distributions ?
and ?, usually set by hand toa value between 0.01 ?
0.1.??
frwU TS?
?F Su u,tf,su,t,sr RFigure 1: Graphical model for LDA-frames.The inference is performed using the CollapsedGibbs sampling (Neal, 2000), where the ?
and ?
dis-tributions are marginalized out of the equations.
Ineach iteration, latent variables fu,t and rf,s are sam-pled as followsP (fu,t|f?
(u,t), r,w, ?, ?)
?(fc?
(u,t)fu,t,u + ?)S?s=1wc?
(u,t,s)wu,t,s,rfu,t,s + ?wc?
(u,t,s)?,rfu,t,s + V ?
(1)483P (rf,s|f , r?
(f,s),w, ?, ?)
?V?v=1(wc?
(f,s)v,rf,s + ?wc?
(f,s)?,rf,s + V ?
)wcf,s,v,(2)where fc?
(u,t)f,u is the number of times frame f isassigned to lexical unit u excluding (u, t), wc?
(u,t,s)v,ris the number of times word v is assigned to roler excluding (u, t, s), and wcf,s,v is the number oftimes word v is assigned to slot s in frame f .
Theasterisk sign * stands for any value in its position.After having all latent variables f and r inferred,one can proceed to compute the lexical unit?framedistribution and the semantic role?word distributionusing the following formulas:?u =fcf,u + ?
?f fcf,u + F?
(3)?r =wcv,r + ?
?v wcv,r + V ?.
(4)3 Parameter EstimationAs one can see from the LDA-frames model, therequirement is to define the number of frames androles in advance.
It is not clear, however, how to se-lect the best values that depend on several factors.First of all, the number of frames and roles usuallyincrease with the growing size of training corpus.
Ifthe training data is small and covers just a small pro-portion of lexical unit usage patterns, the number ofsemantic frames should be small as well.
The pa-rameters are also affected by the granularity of rolesand frames.
One way to estimate the parameters au-tomatically is to select those that maximize posteriorprobability of the model given training data.LDA-frames algorithm generates frames from theDirichlet distribution (DD) which requires a fixednumber of components.
Similarly, the latent vari-ables rf,s are chosen from a fixed set of semanticroles.
In order to be able to update the number offrames and roles during the inference process, wepropose to add the Chinese restaurant process (CRP)(Aldous, 1985) prior for the rf,s variables, and to re-place the Dirichlet distribution the semantic framesare generated from with the Dirichlet process (Fer-guson, 1973).3.1 Number of Semantic RolesIn the original version of the LDA-frames model,the latent variables rf,s, representing semantic roleassignment for slot s in frame f , are chosen froma fixed set of semantic roles without any prior distri-bution.
We propose to generate rf,s from the CRP,which is a single parameter distribution over parti-tions of integers.
The generative process can be de-scribed by using an analogy with a Chinese restau-rant.
Consider a restaurant with an infinite numberof tables, each of them associated with some dish,and N customers choosing a table.
The first cus-tomer sits at the first table.
The nth customer sits attable t drawn from the following distributionP (t = occupied table i) =ni?
+ n?
1P (t = next unoccupied table) =??
+ n?
1,(5)where ni is the number of customers sitting at thetable i and ?
> 0 is a concentration parameter whichcontrols how often a customer chooses a new table.The seating plan makes a partition of the customers(Aldous, 1985).In the proposed modification of the LDA-framesmodel, the dishes are replaced with the semantic rolenumbers and customers with slots of frames.
In themodel we use prior distribution ?
corresponding tothe CRP with concentration parameter ?.
The latentvariables rf,s are then sampled as followsP (rf,s|f , r?
(f,s),w, ?, ?, ?)
?(rc?
(f,s)rf,s + ?)V?v=1(wc?
(f,s)v,rf,s + ?wc?
(f,s)?,rf,s + V ?
)wcf,s,v,(6)where rc?
(f,s)r is the number of times role r is usedin any frame and slot excluding slot s in frame f .Notice that the sampling space hasR+1 dimensionswith the probability of the last unseen componentproportional to?V?v=11V wcf,s,v.
(7)3.2 Number of Semantic FramesEstimating the number of frames is a little bit morecomplicated than the case of semantic roles.
Theidea is to replace DD ?u with the Dirichlet process.484The Dirichlet process DP (?0, G0) is a stochasticprocess that generates discrete probability distribu-tions.
It has two parameters, a base distribution G0and a concentration parameter ?0 > 0.
A samplefrom the Dirichlet process (DP) is thenG =??k=1?k?
?k , (8)where ?k are independent random variables dis-tributed according to G0, ?
?k is an atom at ?k, andweights ?k are also random and dependent on theparameter ?0 (Teh et al 2006).
Simply, DP is a dis-tribution over some infinite and discrete distribu-tions.
It is the reason why DP is often used instead ofDD in order to avoid using a fixed number of com-ponents.The question, however, is how to make the sam-pled frames shared between different lexical units.We propose to generate base distributions of theDPs from GEM distribution (Pitman, 2002) ?
withconcentration parameter ?.
The idea is inspired bythe Hierarchical Dirichlet Process (Teh et al 2006)used for topic modeling.
The graphical model of thenon-parametric LDA-frames is shown in figure 2.??
frwU T S?
?Su u,tf,su,t,sr??
???
?Figure 2: Graphical model for non-parametric LDA-frames.Since it is hard to integrate out the DP with basedistribution generated from GEM in this model, weproceeded to sample ?
separately (Porteous, 2010).The base distribution proportions can be sampledby simulating how new components are created forfcf,u draws from DP with the concentration param-eter ?
?f , which is a sequence of Bernoulli trials foreach u and f (Heinrich, 2011):P (uf,u,r = 1) =??f?
?f + r ?
1?r ?
[1, fcf,u]?
?
Dir({uf}f , ?)
with uf =?u?ruf,u,r.
(9)Finally, the latent variables fu,t are sampled as fol-lowsP (fu,t|f?
(u,t), r,w, ?, ?, ?)
?(fc?
(u,t)fu,t,u + ?
?f )S?s=1wc?
(u,t,s)wu,t,s,rfu,t,s + ?wc?
(u,t,s)?,rfu,t,s + V ?.
(10)4 EvaluationThe non-parametric algorithm was evaluated byan experiment on a synthetic data set consistingof 155 subject-object tuples.
The training datawas generated randomly from a predefined set of 7frames and 4 roles for 16 verbs using the followingalgorithm.
For every lexical unit u:1.
Choose a number of corpus realizations Nu ?
{5, .
.
.
, 15} from the uniform distribution.2.
For each realization nu ?
{1, .
.
.
, Nu}, amongall permitted frames for lexical unit u, choosea semantic frame fnu from the uniform distri-bution.3.
For each frame fnu , generate a realization of allits roles from the uniform distribution.Each semantic role had 6 possible realizations onaverage, some of them assigned to more than one se-mantic role to reflect the character of real languages.Since the data was generated artificially, we knewthe number of frames and roles, how the frames weredefined, and which frame and which role was re-sponsible for generating each realization in the data.We ran the non-parametric algorithm with hyper-parameters ?
= 5, ?
= ?
= 0.1, ?
= 1.5.
It hasbeen shown that the selection of hyperparametershas little impact on the resulting frames when theyare in some reasonable range, thus, the hyperparam-eters were chosen empirically by hand.
The experi-ment led to correct assignments of fu,t and rf,s after56 iterations on average (based on 10 independentruns of the algorithm).485In order to compare the non-parametric algorithmwith the original, we ran the original algorithm withthe same data that had the number of frames androles set to R ?
{1 .
.
.
10}, F ?
{1 .
.
.
20}, andmeasured the perplexity of the data given to themodel after convergence.
The perplexities for allsettings are shown in figure 3.
The lowest perplexitywas reached with F = 7, R = 4 and had the samevalue as the case of the non-parametric algorithm.The fu,t and rf,s assignments were correct as well.Figure 3: Perplexities for different values of F and R.We also ran the non-parametric algorithm with thesame hyperparameters on real data (1.4 millions ofsubject-object tuples) acquired from the British Na-tional Corpus2 using the Stanford Parser (de Marn-effe et al 2006).
The algorithm reached the opti-mal perplexity with 427 frames and 144 roles.
Thisexperiment has been performed only for illustratingthe algorithm on real data.
Because of long runningtime of the algorithm on such huge data set, we didnot perform the same experiments as with the caseof the small synthetic data.5 ConclusionIn this paper we presented a method for estimat-ing the number of frames and roles for the LDA-frames model.
The idea is based on using the Chi-nese Restaurant Process and the Dirichlet Processinstead of the Dirichlet Distributions and selectingsuch parameters that maximize the posterior proba-bility of the model for given training data.
An ex-periment showed that the non-parametric algorithm2http://www.natcorp.ox.ac.ukinfers correct values of both the number of framesand roles on a synthetic data set.AcknowledgmentsThis work has been partly supported by the Min-istry of Education of the Czech Republic under theproject LINDAT-Clarin LM2010013.ReferencesAldous, D. J.
(1985).
Exchangeability and Related Top-ics.
E?cole d?E?te?
de Probabilite?s de Saint-Flour XIII ?1983, 1117:1 ?
198.de Marneffe, M.-C., MacCartney, B., and Manning, C. D.(2006).
Generating Typed Dependency Parses fromPhrase Structure Parses.
In The International Confer-ence on Language Resources and Evaluation (LREC)2006.Ferguson, T. S. (1973).
A Bayesian Analysis of SomeNonparametric Problems.
The Annals of Statistics,1:209 ?
230.Heinrich, G. (2011).
?Infinite LDA?
?
Implementing theHDP with Minimum Code complexity.
Technical re-port.Materna, J.
(2012).
LDA-Frames: An Unsupervised Ap-proach to Generating Semantic Frames.
In Gelbukh,A., editor, Proceedings of the 13th International Con-ference CICLing 2012, Part I, pages 376?387, NewDelhi, India.
Springer Berlin / Heidelberg.Neal, R. M. (2000).
Markov Chain Sampling Methodsfor Dirichlet Process Mixture Models.
Journal of com-putational and graphical statistics, 9(2):249?265.Pitman, J.
(2002).
Combinatorial Stochastic Processes.Lecture Notes for St. Flour Summer School.Porteous, I.
(2010).
Networks of mixture blocks for nonparametric bayesian models with applications.
PhDthesis, University of California.Ruppenhofer, J., Ellsworth, M., Petruck, M. R. L.,Johnson, C. R., and Scheffczyk, J.
(2006).FrameNet II: Extended Theory and Practice.http://www.icsi.berkeley.edu/framenet.Schuler, K. K. (2006).
VerbNet: A Broad-Coverage,Comprehensive Verb Lexicon.
PhD thesis, Universityof Pennsylvania.Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M.(2006).
Hierarchical Dirichlet processes .
Journalof the American Statistical Association, 101:1566 ?1581.486
