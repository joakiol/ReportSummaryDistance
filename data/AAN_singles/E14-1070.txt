Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 664?672,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsEncoding Semantic Resources in Syntactic Structuresfor Passage RerankingKateryna TymoshenkoTrento RISE38123 Povo (TN), Italyk.tymoshenko@trentorise.euAlessandro MoschittiQatar Computing Research Instit.5825 Doha, Qataramoschitti@qf.org.qaAliaksei SeverynUniversity of Trento38123 Povo (TN), Italyseveryn@disi.unitn.itAbstractIn this paper, we propose to use seman-tic knowledge from Wikipedia and large-scale structured knowledge datasets avail-able as Linked Open Data (LOD) forthe answer passage reranking task.
Werepresent question and candidate answerpassages with pairs of shallow syntac-tic/semantic trees, whose constituents areconnected using LOD.
The trees are pro-cessed by SVMs and tree kernels, whichcan automatically exploit tree fragments.The experiments with our SVM rank algo-rithm on the TREC Question Answering(QA) corpus show that the added relationalinformation highly improves over the stateof the art, e.g., about 15.4% of relative im-provement in P@1.1 IntroductionPast work in TREC QA, e.g.
(Voorhees, 2001),and more recent work (Ferrucci et al., 2010) inQA has shown that, to achieve human perfor-mance, semantic resources, e.g., Wikipedia1,must be utilized by QA systems.
This requiresthe design of rules or machine learning featuresthat exploit such knowledge by also satisfyingsyntactic constraints, e.g., the semantic type ofthe answer must match the question focus words.The engineering of such rules for open domainQA is typically very costly.
For instance, forautomatically deriving the correctness of theanswer passage in the following question/answerpassage (Q/AP) pair (from the TREC corpus2):Q: What company owns the soft drink brand ?Gatorade?
?A: Stokely-Van Camp bought the formula and startedmarketing the drink as Gatorade in 1967.
Quaker Oats Co.took over Stokely-Van Camp in 1983.1http://www.wikipedia.org2It will be our a running example for the rest of the paper.we would need to write the following complexrules:is(Quaker Oats Co.,company),own(Stokely-Van Camp,Gatorade),took over(Quaker Oats Co.,Stokely-Van Camp),took over(Y, Z)?own(Z,Y),and carry out logic unification and resolution.Therefore, approaches that can automaticallygenerate patterns (i.e., features) from syntacticand semantic representations of the Q/AP areneeded.
In this respect, our previous work, e.g.,(Moschitti et al., 2007; Moschitti and Quarteroni,2008; Moschitti, 2009), has shown that treekernels for NLP, e.g., (Moschitti, 2006), canexploit syntactic patterns for answer passagereranking significantly improving search enginebaselines.
Our more recent work, (Severyn andMoschitti, 2012; Severyn et al., 2013b; Severynet al., 2013a), has shown that using automaticallyproduced semantic labels in shallow syntactictrees, such as question category and questionfocus, can further improve passage reranking andanswer extraction (Severyn and Moschitti, 2013).However, such methods cannot solve the classof examples above as they do not use backgroundknowledge, which is essential to answer com-plex questions.
On the other hand, Kalyanpuret al.
(2011) and Murdock et al.
(2012) showedthat semantic match features extracted from large-scale background knowledge sources, includingthe LOD ones, are beneficial for answer rerank-ing.In this paper, we tackle the candidate answerpassage reranking task.
We define kernel func-tions that can automatically learn structural pat-terns enriched by semantic knowledge, e.g., fromLOD.
For this purpose, we carry out the follow-ing steps: first, we design a representation for theQ/AP pair by engineering a pair of shallow syn-tactic trees connected with relational nodes (i.e.,664NLPAnnotatorsFocus and Question classifiersNLPAnnotatorsFocus and Question classifierssyntactic/semantic graphsyntactic/semantic graph train/testdataKernel-based rerankerKernel-based rerankerRerankedAPEvaluationEvaluationCandidateAPQuestionUIMA pipelineSearch engineSearch engineq/a similarity featuresq/a similarity featuresWikipedia link annotatorWikipedia link annotatorWikipediaWikipediaLOD type annotatorLOD type annotatorLOD datasetsLOD datasetsFigure 1: Kernel-based Answer Passage Reranking Systemthose matching the same words in the question andin the answer passages).Secondly, we use YAGO (Suchanek et al.,2007), DBpedia (Bizer et al., 2009) and Word-Net (Fellbaum, 1998) to match constituents fromQ/AP pairs and use their generalizations in oursyntactic/semantic structures.
We employ wordsense disambiguation to match the right entities inYAGO and DBpedia, and consider all senses of anambiguous word from WordNet.Finally, we experiment with TREC QA and sev-eral models combining traditional feature vectorswith automatic semantic labels derived by statis-tical classifiers and relational structures enrichedwith LOD relations.
The results show that ourmethods greatly improve over strong IR baseline,e.g., BM25, by 96%, and on our previous state-of-the-art reranking models, up to 15.4% (relativeimprovement) in P@1.2 Reranking with Tree KernelsIn contrast to ad-hoc document retrieval, struc-tured representation of sentences and paragraphshelps to improve question answering (Bilotti et al.,2010).
Typically, rules considering syntactic andsemantic properties of the question and its candi-date answer are handcrafted.
Their modeling is ingeneral time-consuming and costly.
In contrast,we rely on machine learning and automatic fea-ture engineering with tree kernels.
We used ourstate-of-the-art reranking models, i.e., (Severyn etal., 2013b; Severyn et al., 2013a) as a baseline.Our major difference with such approach is thatwe encode knowledge and semantics in differentways, using knowledge from LOD.
The next sec-tions outline our new kernel-based framework, al-though the detailed descriptions of the most inno-vative aspects such as new LOD-based representa-tions are reported in Section 3.2.1 Framework OverviewOur QA system is based on a rather simple rerank-ing framework as displayed in Figure 1: given aquestion Q, a search engine retrieves a list of can-didate APs ranked by their relevancy.
Next, thequestion together with its APs are processed bya rich NLP pipeline, which performs basic tok-enization, sentence splitting, lemmatization, stop-word removal.
Various NLP components, em-bedded in the pipeline as UIMA3annotators, per-form more involved linguistic analysis, e.g., POS-tagging, chunking, NE recognition, constituencyand dependency parsing, etc.Each Q/AP pair is processed by a Wikipedialink annotator.
It automatically recognizes n-grams in plain text, which may be linked toWikipedia and disambiguates them to WikipediaURLs.
Given that question passages are typicallyshort, we concatenate them with the candidate an-swers to provide a larger disambiguation contextto the annotator.These annotations are then used to producecomputational structures (see Sec.
2.2) input to thereranker.
The semantics of such relational struc-tures can be further enriched by adding links be-tween Q/AP constituents.
Such relational linkscan be also generated by: (i) matching lemmasas in (Severyn and Moschitti, 2012); (ii) match-ing the question focus type derived by the ques-tion classifiers with the type of the target NE asin (Severyn et al., 2013a); or (iii) by matching theconstituent types based on LOD (proposed in thispaper).
The resulting pairs of trees connected bysemantic links are then used to train a kernel-basedreranker, which is used to re-order the retrievedanswer passages.2.2 Relational Q/AP structuresWe use the shallow tree representation that weproposed in (Severyn and Moschitti, 2012) as abaseline structural model.
More in detail, each Qand its candidate AP are encoded into two trees,where lemmas constitute the leaf level, the part-of-speech (POS) tags are at the pre-terminal leveland the sequences of POS tags are organized intothe third level of chunk nodes.
We encoded struc-tural relations using the REL tag, which links therelated structures in Q/AP, when there is a match3http://uima.apache.org/665Figure 2: Basic structural representations using a shallow chunk tree structure for the Q/AP in the running example.
Curvedline indicates the tree fragments in the question and its answer passage linked by the relational REL tag.between the lemmas in Q and AP.
We marked theparent (POS tags) and grand parent (chunk) nodesof such lemmas by prepending a REL tag.However, more general semantic relations, e.g.,derived from the question focus and category, canbe encoded using the REL-FOCUS-<QC> tag,where <QC> stands for the question class.
In(Severyn et al., 2013b; Severyn et al., 2013a), weused statistical classifiers to derive question focusand categories of the question and of the namedentities in the AP.
We again mark (i) the focuschunk in the question and (ii) the AP chunks con-taining named entities of type compatible with thequestion class, by prepending the above tags totheir labels.
The compatibility between the cat-egories of named entities and questions is evalu-ated with a lookup to a manually predefined map-ping (see Table 1 in (Severyn et al., 2013b)).
Wealso prune the trees by removing the nodes beyonda certain distance (in terms of chunk nodes) fromthe REL and REL-FOCUS nodes.
This removesirrelevant information and speeds up learning andclassification.
We showed that such model outper-forms bag-of-words and POS-tag sequence mod-els (Severyn et al., 2013a).An example of a Q/AP pair encoded using shal-low chunk trees is given in Figure 2.
Here, for ex-ample, the lemma ?drink?
occurs in both Q and AP(we highlighted it with a solid line box in the fig-ure).
?Company?
was correctly recognized as a fo-cus4, however it was misclassified as ?HUMAN?(?HUM?).
As no entities of the matching type?PERSON?
were found in the answer by a NERsystem, no chunks were marked as REL-FOCUSon the answer passage side.We slightly modify the REL-FOCUS encod-ing into the tree.
Instead of prepending REL-FOCUS-<QC>, we only prepend REL-FOCUSto the target chunk node, and add a new nodeQC as the rightmost child of the chunk node, e.g,in Figure 2, the focus node would be marked asREL-FOCUS and the sequence of its childrenwould be [WP NN HUM].
This modification in-4We used the same approach to focus detection and ques-tion classification used in (Severyn et al., 2013b)tends to reduce the feature sparsity.3 LOD for Semantic StructuresWe aim at exploiting semantic resources for build-ing more powerful rerankers.
More specifically,we use structured knowledge about properties ofthe objects referred to in a Q/AP pair.
A largeamount of knowledge has been made available asLOD datasets, which can be used for finding addi-tional semantic links between Q/AP passages.In the next sections, we (i) formally define novelsemantic links between Q/AP structures that weintroduce in this paper; (ii) provide basic notionsof Linked Open Data along with three of its mostwidely used datasets, YAGO, DBpedia and Word-Net; and, finally, (iii) describe our algorithm togenerate linked Q/AP structures.3.1 Matching Q/AP Structures: Type MatchWe look for token sequences (e.g., complex nomi-nal groups) in Q/AP pairs that refer to entities andentity classes related by isa (Eq.
1) and isSubclas-sOf (Eq.
2) relations and then link them in thestructural Q/AP representations.isa : entity ?
class?
{true, false} (1)isSubclassOf : class?
class?
{true, false} (2)Here, entities are all the objects in the worldboth real or abstract, while classes are sets of en-tities that share some common features.
Informa-tion about entities, classes and their relations canbe obtained from the external knowledge sourcessuch as the LOD resources.
isa returns true if anentity is an element of a class (false otherwise),while isSubclassOf(class1,class2) returns true ifall elements of class1 belong also to class2.We refer to the token sequences introducedabove as to anchors and the entities/classes theyrefer to as references.
We define anchors to be ina Type Match (TM) relation if the entities/classesthey refer to are in isa or isSubclassOf relation.More formally, given two anchors a1and a2be-longing to two text passages, p1and p2, respec-tively, and given an R(a, p) function, which re-turns a reference of an anchor a in passage p, wedefine TM (r1, r2) as666{isa (r1, r2) : if isEntity (r1) ?
isClass (r2)subClassOf (r1, r2) : if isClass (r1) ?
isClass (r2)(3)where r1= R(a1, p1), r2= R(a2, p2) and isEn-tity(r) and isClass(r) return true if r is an entity ora class, respectively, and false otherwise.
It shouldbe noted that, due to the ambiguity of natural lan-guage, the same anchor may have different refer-ences depending on the context.3.2 LOD for linking Q/A structuresLOD consists of datasets published online accord-ing to the Linked Data (LD) principles5and avail-able in open access.
LOD knowledge is repre-sented following the Resource Description Frame-work (RDF)6specification as a set of statements.A statement is a subject-predicate-object triple,where predicate denotes the directed relation, e.g.,hasSurname or owns, between subject and object.Each object described by RDF, e.g., a class oran entity, is called a resource and is assigned aUnique Resource Identifier (URI).LOD includes a number of common schemas,i.e., sets of classes and predicates to be reusedwhen describing knowledge.
For example, oneof them is RDF Schema (RDFS)7, which containspredicates rdf:type and rdfs:SubClassOfsimilar to the isa and subClassOf functions above.LOD contains a number of large-scale cross-domain datasets, e.g., YAGO (Suchanek et al.,2007) and DBpedia (Bizer et al., 2009).
Datasetscreated before the emergence of LD, e.g., Word-Net, are brought into correspondence with the LDprinciples and added to the LOD as well.3.2.1 Algorithm for detecting TMAlgorithm 1 detects n-grams in the Q/AP struc-tures that are in TM relation and encodes TMknowledge in the shallow chunk tree representa-tions of Q/AP pairs.
It takes two text passages, P1and P2, and a LOD knowledge source, LODKS,as input.
We run the algorithm twice, first withAP as P1and Q as P2and then vice versa.
Forexample, P1and P2in the first run could be, ac-cording to our running example, Q and AP candi-date, respectively, and LODKScould be YAGO,DBpedia or WordNet.Detecting anchors.
getAnchors(P2,LODKS)in line 1 of Algorithm 1 returns all anchors in the5http://www.w3.org/DesignIssues/LinkedData.html6http://www.w3.org/TR/rdf-concepts/7http://www.w3.org/TR/rdf-schema/Algorithm 1 Type Match algorithmInput: P1, P2- text passages; LODKS- LOD knowledgesource.1: for all anchor ?
getAnchors(P2,LODKS) do2: for all uri ?
getURIs(anchor,P2,LODKS) do3: for all type ?
getTypes(uri,LODKS) do4: for all ch ?
getChunks(P1) do5: matchedTokens ?
checkMatch(ch,type.labels)6: if matchedTokens 6= ?
then7: markAsTM(anchor,P2.parseTree)8: markAsTM(matchedTokens,P1.parseTree)given text passage, P2.
Depending on LODKSone may have various implementations of this pro-cedure.
For example, when LODKSis Word-Net, getAnchor returns token subsequences of thechunks in P2of lengths n-k, where n is the numberof tokens in the chunk and k = [1, .., n?
1).In case when LODKSis YAGO or DBpedia,we benefit from the fact that both YAGO and DB-pedia are aligned with Wikipedia on entity level byconstruction and we can use the so-called wikifica-tion tools, e.g., (Milne and Witten, 2009), to detectthe anchors.
The wikification tools recognize n-grams that may denote Wikipedia pages in plaintext and disambiguate them to obtain a uniqueWikipedia page.
Such tools determine whethera certain n-gram may denote a Wikipedia page(s)by looking it up in a precomputed vocabulary cre-ated using Wikipedia page titles and internal linknetwork (Csomai and Mihalcea, 2008; Milne andWitten, 2009).Obtaining references.
In line 2 of Algorithm 1for each anchor, we determine the URIs of enti-ties/classes it refers to in LODKS.
Here again,we have different strategies for different LODKS.In case of WordNet, we use the all-senses strat-egy, i.e., getURI procedure returns a set of URIsof synsets that contain the anchor lemma.In case when LODKSis YAGO or DBpedia,we use wikification tools to correctly disambiguatean anchor to a Wikipedia page.
Then, Wikipediapage URLs may be converted to DBpedia URIs bysubstituting the en.wikipedia.org/wiki/prefix to the dbpedia.org/resource/; andYAGO URIs by querying it for subjects of theRDF triples with yago:hasWikipediaUrl8as a predicate and Wikipedia URL as an object.For instance, one of the anchors detected inthe running example AP would be ?Quaker oats?,8yago: is a shorthand for the http prefix http://yago-knowledge.org/resource/667a wikification tool would map it to wiki:Quaker_Oats_Company9, and the respectiveYAGO URI would be yago:Quaker_Oats_Company.Obtaining type information.
Given a uri, if itis an entity, we look for all the classes it belongsto, or if it is a class, we look for all classes forwhich it is a subclass.
This process is incorpo-rated in the getTypes procedure in line 3 of Algo-rithm 1.
We call such classes types.
If LODKSis WordNet, then our types are simply the URIs ofthe hypernyms of uri.
If LODKSis DBpedia orYAGO, we query these datasets for the values ofthe rdf:type and rdfs:subClassOf prop-erties of the uri (i.e., objects of the triples with urias subject and type/subClassOf as predicates) andadd their values (which are also URIs) to the typesset.
Then, we recursively repeat the same queriesfor each retrieved type URI and add their results tothe types.
Finally, the getTypes procedure returnsthe resulting types set.The extracted URIs returned by getTypes areHTTP ids, however, frequently they have human-readable names, or labels, specified by the rdfs:label property.
If no label information for aURI is available, we can create the label by re-moving the technical information from the typeURI, e.g., http prefix and underscores.
type.labelsdenotes a set of type human-readable labels fora specific type.
For example, one of the typesextracted for yago:Quaker_Oats_Companywould have label ?company?.Checking for TM.
Further, the checkMatchprocedure checks whether any of the labels in thetype.labels matches any of the chunks in P1re-turned by getChunks, fully or partially (line 5 ofAlgorithm 1).
Here, getChunks procedure returnsa list of chunks recognized in P1by an externalchunker.More specifically, given a chunk, ch, and a typelabel, type.label, checkMatch checks whether thech string matches10type.label or its last word(s).If no match is observed, we remove the first to-ken from ch and repeat the procedure.
We stopwhen the match is observed or when no tokensin ch are left.
If the match is observed, check-Match returns all the tokens remaining in ch asmatchedTokens.
Otherwise, it returns an emptyset.
For example, the question of the running ex-9wiki: is a shorthand for the http prefix http://en.wikipedia.org/wiki/10case-insensitive exact string matchample contains the chunk ?what company?, whichpartially matches the human readable ?company?label of one of the types retrieved for the ?Quakeroats?
anchor from the answer.
Our implemen-tation of the checkMatch procedure would re-turn ?company?
from the question as one of thematchedTokens.If the matchedTokens set is not empty,this means that TM(R(anchor, P2), R(matchedTokens, P1))in Eq.
3 returns true.Indeed, a1is an anchor and a2is the matched-Tokens sequence (see Eq.
3), and their respectivereferences, i.e., URI assigned to the anchor andURI of one of its types, are either in subClassOfor in isa relation by construction.
Naturally, thisis only one of the possible ways to evaluate theTM function, and it may be noise-prone.Marking TM in tree structures.
Finally,if the TM match is observed, i.e., matchedTo-kens is not an empty set, we mark tree substruc-tures corresponding to the anchor in the struc-tural representation of P2(P2.parseTree) andthose corresponding to matchedTokens in that ofP1(P1.parseTree) as being in a TM relation.
Inour running example, we would mark the substruc-tures corresponding to ?Quaker oats?
anchor in theanswer (our P2) and the ?company?
matchedTo-ken in the question (our P1) shallow syntactic treerepresentations.
We can encode TM match infor-mation into a tree in a variety of ways, which wedescribe below.3.2.2 Encoding TM knowledge in the treesa1and a2from Eq.
3 are n-grams, therefore theycorrespond to the leaf nodes in the shallow syn-tactic trees of p1and p2.
We denote the set oftheir preterminal parents as NTM.
We consid-ered the following strategies of encoding TM re-lation in the trees: (i) TM node (TMN).
Add leafsibling tagged with TM to all the nodes in NTM.
(ii) Directed TM node (TMND).
Add leaf sib-ling tagged with TM-CHILD to all the nodes inNTMcorresponding to the anchor, and leaf sib-lings tagged with TM-PARENT to the nodes cor-responding to matchedTokens.
(iii) Focus TM(TMNF).
Add leaf siblings to all the nodes inNTM.
If matchedTokens is a part of a questionfocus label them as TM-FOCUS.
Otherwise, la-bel them as TM.
(iv) Combo TMNDF.
Encodeusing the TMNDstrategy.
If matchedTokens is apart of a question focus label then also add a childlabeled FOCUS to each of the TM labels.
Intu-668Figure 3: Fragments of a shallow chunk parse tree anno-tated in TMNDmode.itively, TMND, TMNF, TMNDFare likely to re-sult in more expressive patterns.
Fig.
3 shows anexample of the TMNDannotation.3.3 Wikipedia-based matchingLemma matching for detecting REL may result inlow coverage, e.g., it is not able to match differ-ent variants for the same name.
We remedy thisby using Wikipedia link annotation.
We considertwo word sequences (in Q and AP, respectively)that are annotated with the same Wikipedia linkto be in a matching relation.
Thus, we add newREL tags to Q/AP structural representations as de-scribed in Sec.
2.2.4 ExperimentsWe evaluated our different rerankers encoding sev-eral semantic structures on passage retrieval task,using a factoid open-domain TREC QA corpus.4.1 Experimental SetupTREC QA 2002/2003.
In our experiments, weopted for questions from years 2002 and 2003,which totals to 824 factoid questions.
TheAQUAINT corpus11is used for searching the sup-porting passages.Pruning.
Following (Severyn and Moschitti,2012) we prune the shallow trees by removing thenodes beyond distance of 2 from the REL, REL-FOCUS or TM nodes.LOD datasets.
We used the core RDF distribu-tion of YAGO212, WordNet 3.0 in RDF13, and thedatasets from the 3.9 DBpedia distribution14.Feature Vectors.
We used a subset of the sim-ilarity functions between Q and AP described in(Severyn et al., 2013b).
These are used alongwith the structural models.
More explicitly: Term-overlap features: i.e., a cosine similarity overquestion/answer, simCOS(Q,AP ), where the in-put vectors are composed of lemma or POS-tag11http://catalog.ldc.upenn.edu/LDC2002T3112http://www.mpi-inf.mpg.de/yago-naga/yago1_yago2/download/yago2/yago2core_20120109.rdfs.7z13http://semanticweb.cs.vu.nl/lod/wn30/14http://dbpedia.org/Downloads39n-grams with n = 1, .., 4.
PTK score: i.e., out-put of the Partial Tree Kernel (PTK), defined in(Moschitti, 2006), when applied to the structuralrepresentations of Q and AP, simPTK(Q,AP ) =PTK(Q,AP ) (note that, this is computed withina pair).
PTK defines similarity in terms of thenumber of substructures shared by two trees.Search engine ranking score: the ranking score ofour search engine assigned to AP divided by a nor-malizing factor.SVM re-ranker.
To train our models, we useSVM-light-TK15, which enables the use of struc-tural kernels (Moschitti, 2006) in SVM-light(Joachims, 2002).
We use default parameters andthe preference reranking model described in (Sev-eryn and Moschitti, 2012; Severyn et al., 2013b).We used PTK and the polynomial kernel of degree3 on standard features.Pipeline.
We built the entire processing pipelineon top of the UIMA framework.We included manyoff-the-shelf NLP tools wrapping them as UIMAannotators to perform sentence detection, tok-enization, NE Recognition, parsing, chunking andlemmatization.
Moreover, we used annotatorsfor building new sentence representations startingfrom tools?
annotations and classifiers for questionfocus and question class.Search engines.
We adopted Terrier16using theaccurate BM25 scoring model with default param-eters.
We trained it on the TREC corpus (3Gb),containing about 1 million documents.
We per-formed indexing at the paragraph level by splittingeach document into a set of paragraphs, which arethen added to the search index.
We retrieve a list of50 candidate answer passages for each question.Wikipedia link annotators.
We use theWikipedia Miner (WM) (Milne and Witten,2009)17tool and the Machine Linking (ML)18web-service to annotate Q/AP pairs with links toWikipedia.
Both tools output annotation confi-dence.
We use all WM and ML annotations withconfidence exceeding 0.2 and 0.05, respectively.We obtained these figures heuristically, they arelow because we aimed to maximize the Recall ofthe Wikipedia link annotators in order to maxi-15http://disi.unitn.it/moschitti/Tree-Kernel.htm16http://terrier.org17http://sourceforge.net/projects/wikipedia-miner/files/wikipedia-miner/wikipedia-miner_1.1, we use only topic detectormodule which detects and disambiguates anchors18http://www.machinelinking.com/wp669System MRR MAP P@1BM25 28.02?2.94 0.22?0.02 18.17?3.79CH+V (CoNLL, 2013) 37.45 0.3 27.91CH+V+QC+TFC(CoNLL, 2013)39.49 0.32 30CH + V 36.82?2.68 0.30?0.02 26.34?2.17CH + V+ QC+TFC 40.20?1.84 0.33?0.01 30.85?2.35CH+V+QC+TFC* 40.50?2.32 0.33?0.02 31.46?2.42Table 1: Baseline systemsmize the number of TMs.
In all the experiments,we used a union of the sets of the annotations pro-vided by WM and ML.Metrics.
We used common QA metrics: Precisionat rank 1 (P@1), i.e., the percentage of questionswith a correct answer ranked at the first position,and Mean Reciprocal Rank (MRR).
We also reportthe Mean Average Precision (MAP).
We perform5-fold cross-validation and report the metrics aver-aged across all the folds together with the std.dev.4.2 Baseline Structural RerankingIn these experiments, we evaluated the accuracyof the following baseline models: BM25 is theBM25 scoring model, which also provides the ini-tial ranking; CH+V is a combination of tree struc-tures encoding Q/AP pairs using relational linkswith the feature vector; and CH+V+QC+TFC isCH+V extended with the semantic categorial linksintroduced in (Severyn et al., 2013b).Table 1 reports the performance of our base-line systems.
The lines marked with (CoNLL,2013) contain the results we reported in (Sev-eryn et al., 2013b).
Lines four and five reportthe performance of the same systems, i.e., CH+Vand CH+V+QC+TFC, after small improvementand changes.
Note that in our last version, wehave a different set of V features than in (CoNLL,2013).
Finally, CH+V+QC+TFC* refers to theperformance of CH+V+QC+TFC with questiontype information of semantic REL-FOCUS linksrepresented as a distinct node (see Section 2.2).The results show that this modification yields aslight improvement over the baseline, thus, inthe next experiments, we add LOD knowledge toCH+V+QC+TFC*.4.3 Impact of LOD in Semantic StructuresThese experiments evaluated the accuracy of thefollowing models (described in the previous sec-tions): (i) a system using Wikipedia to establishthe REL links; and (ii) systems which use LODknowledge to find type matches (TM).The first header line of the Table 2 shows whichbaseline system was enriched with the TM knowl-edge.
Type column reports the TM encoding strat-egy employed (see Section 3.2.2).
Dataset columnreports which knowledge source was employed tofind TM relations.
Here, yago is YAGO2, db isDBpedia, and wn is WordNet 3.0.
The first re-sult line in Table 2 reports the performance ofthe strong CH+V and CH+V+QC+TFC* base-line systems.
Line with the ?wiki?
dataset re-ports on CH+V and CH+V+QC+TFC* usingboth Wikipedia link annotations provided by MLand MW and hard lemma matching to find the re-lated structures to be marked by REL (see Sec-tion 3.3 for details of the Wikipedia-based RELmatching).
The remainder of the systems is builton top of the baselines using both hard lemma andWikipedia-based matching.
We used bold font tomark the top scores for each encoding strategy.The tables show that all the systems ex-ploiting LOD knowledge, excluding those us-ing DBpedia only, outperform the strong CH+Vand CH+V+QC+TFC* baselines.
Note thatCH+V enriched with TM tags performs com-parably to, and in some cases even outper-forms, CH+V+QC+TFC*.
Compare, for exam-ple, the outputs of CH+V+TMNDFusing YAGO,WordNet and DBpedia knowledge and those ofCH+V+QC+TFC* with no LOD knowledge.Adding TM tags to the top-performing base-line system, CH+V+QC+TFC*, typically re-sults in further increase in performance.
Thebest-performing system in terms of MRR andP@1 is CH+V+QC+TFC*+TMNFsystem us-ing the combination of WordNet and YAGO2 assource of TM knowledge and Wikipedia for REL-matching.
It outperforms the CH+V+QC+TFC*baseline by 3.82% and 4.15% in terms of MRRand P@1, respectively.
Regarding MAP, a num-ber of systems employing YAGO2 in combina-tion with WordNet and Wikipedia-based REL-matching obtain 0.37 MAP score thus outperform-ing the CH+V+QC+TFC* baseline by 4%.We used paired two-tailed t-test for evaluatingthe statistical significance of the results reported inTable 2. ?
and ?
correspond to the significance lev-els of 0.05 and 0.1, respectively.
We compared (i)the results in the wiki line to those in the none line;and (ii) the results for the TM systems to those inthe wiki line.The table shows that we typically obtain bet-ter results when using YAGO2 and/or WordNet.In our intuition this is due to the fact that theseresources are large-scale, have fine-grained class670Type Dataset CH + V CH + V + QC + TFC*MRR MAP P@1 MRR MAP P@1- none 36.82?2.68 0.30?0.02 26.34?2.17 40.50?2.32 0.33?0.02 31.46?2.42- wiki 39.17?1.29?
0.31?0.01?
28.66?1.43?
41.33?1.17 0.34?0.01 31.46?1.40TMNdb 40.60?1.88 0.33?0.01?
31.10?2.99?
40.80?1.01 0.34?0.01 30.37?1.90TMNwn 41.39?1.96?
0.33?0.01?
31.34?2.94 42.43?0.56 0.35?0.01 32.80?0.67TMNwn+db 40.85?1.52?
0.33?0.01?
30.37?2.34 42.37?1.12 0.35?0.01 32.44?2.64TMNyago 40.71?2.07 0.33?0.03?
30.24?2.09?
43.28?1.91?
0.36?0.01?
33.90?2.75TMNyago+db 41.25?1.57?
0.34?0.02?
31.10?1.88?
42.39?1.83 0.35?0.01 32.93?3.14TMNyago+wn 42.01?2.26?
0.34?0.02?
32.07?3.04?
43.98?1.08?
0.36?0.01?
35.24?1.46?TMNyago+wn+db 41.52?1.85?
0.34?0.02?
30.98?2.71?
43.13?1.38 0.36?0.01 33.66?2.77TMNFdb 40.67?1.94?
0.33?0.01?
30.85?2.22?
41.43?0.70 0.35?0.01 31.22?1.09TMNFwn 40.95?2.27?
0.33?0.01?
30.98?3.74 42.37?0.98 0.35?0.01 32.56?1.76TMNFwn+db 40.84?2.18?
0.34?0.01?
30.73?3.04 43.08?0.83?
0.36?0.01?
33.54?1.29?TMNFyago 42.01?2.44?
0.34?0.02?
32.07?3.01?
43.82?2.36?
0.36?0.02?
34.88?3.35TMNFyago+db 41.32?1.70?
0.34?0.02?
31.10?2.48?
43.19?1.17?
0.36?0.01?
33.90?1.86TMNFyago+wn 41.69?1.66?
0.34?0.02?
31.10?2.44?
44.32?0.70?
0.36?0.01?
35.61?1.11?TMNFyago+wn+db 41.56?1.41?
0.34?0.02?
30.85?2.22?
43.79?0.73?
0.37?0.01?
34.88?1.69?TMNDdb 40.37?1.87 0.33?0.01?
30.37?2.17 41.58?1.02 0.35?0.01?
31.46?1.59TMNDwn 41.13?2.14?
0.33?0.01?
30.73?2.75 42.19?1.39 0.35?0.01 32.32?1.36TMNDwn+db 41.28?1.03?
0.34?0.01?
30.73?0.82?
42.37?1.16 0.36?0.01 32.44?2.71TMNDyago 42.11?3.24?
0.34?0.02?
32.07?4.06?
44.04?2.05?
0.36?0.01?
34.63?2.17?TMNDyago+db 42.28?2.01?
0.35?0.01?
32.44?1.99?
43.77?2.02?
0.37?0.01?
34.27?2.42TMNDyago+wn 42.96?1.45?
0.35?0.01?
33.05?2.04?
44.25?1.32?
0.37?0.00?
34.76?1.61?TMNDyago+wn+db 42.56?1.25?
0.35?0.01?
32.56?1.91?
43.91?1.01?
0.37?0.01?
34.63?1.32?TMNDFdb 40.40?1.93?
0.33?0.01?
30.49?1.78?
41.85?1.05 0.35?0.01?
31.83?0.80TMNDFwn 40.84?1.69?
0.33?0.01?
30.49?2.24 41.89?0.99 0.35?0.01 31.71?0.86TMNDFwn+db 41.14?1.29?
0.34?0.01?
30.73?1.40?
42.31?0.92 0.36?0.01 32.32?2.36TMNDFyago 42.31?2.57?
0.35?0.02?
32.68?3.01?
44.22?2.38?
0.37?0.02?
35.00?2.88?TMNDFyago+db 41.96?1.82?
0.35?0.01?
32.32?2.24?
43.82?1.95?
0.37?0.01?
34.51?2.39?TMNDFyago+wn 42.80?1.19?
0.35?0.01?
33.17?1.86?
43.91?0.98?
0.37?0.01?
34.63?0.90?TMNDFyago+wn+db 43.15?0.93?
0.35?0.01?
33.78?1.59?
43.96?0.94?
0.37?0.01?
34.88?1.69?Table 2: Results in 5-fold cross-validation on TREC QA corpustaxonomy and contain many synonymous labelsper class/entity thus allowing us to have a goodcoverage with TM-links.
DBpedia ontology thatwe employed in the db experiments is more shal-low and contains fewer labels for classes, there-fore the amount of discovered TM matches isnot always sufficient for increasing performance.YAGO2 provides better coverage for TM relationsbetween entities and their classes, while Word-Net contains more relations between classes19.Note that in (Severyn and Moschitti, 2012), wealso used supersenses of WordNet (unsuccess-fully) whereas here we use hypernymy relationsand a different technique to incorporate semanticmatch into the tree structures.Different TM-knowledge encoding strategies,TMN, TMND, TMNF, TMNDFproduce smallchanges in accuracy.
We believe, that the differ-ence between them would become more signifi-cant when experimenting with larger corpora.5 ConclusionsThis paper proposes syntactic structures whosenodes are enriched with semantic informationfrom statistical classifiers and knowledge fromLOD.
In particular, YAGO, DBpedia and Word-Net are used to match and generalize constituentsfrom QA pairs: such matches are then used in19We consider the WordNet synsets to be classes in thescope of our experimentssyntactic/semantic structures.
The experimentswith TREC QA and the above representationsalso combined with traditional features greatly im-prove over a strong IR baseline, e.g., 96% onBM25, and on previous state-of-the-art rerank-ing models, up to 15.4% (relative improvement)in P@1.
In particular, differently from previouswork, our models can effectively use semanticknowledge in statistical learning to rank methods.These promising results open interesting futuredirections in designing novel semantic structuresand using innovative semantic representations inlearning algorithms.AcknowledgmentsThis research is partially supported by the EU?s 7thFramework Program (FP7/2007-2013) (#288024LIMOSINE project) and by a Shared UniversityResearch award from the IBM Watson ResearchCenter - Yorktown Heights, USA and the IBMCenter for Advanced Studies of Trento, Italy.
Thethird author is supported by the Google EuropeFellowship 2013 award in Machine Learning.ReferencesMatthew W. Bilotti, Jonathan L. Elsas, Jaime Car-bonell, and Eric Nyberg.
2010.
Rank learningfor factoid question answering with linguistic andsemantic constraints.
In Proceedings of the 19th671ACM international Conference on Information andKnowledge Management (CIKM), pages 459?468.Christian Bizer, Jens Lehmann, Georgi Kobilarov,S?oren Auer, Christian Becker, Richard Cyganiak,and Sebastian Hellmann.
2009.
Dbpedia - a crys-tallization point for the web of data.
Web Seman-tics: Science, Services and Agents on the World WideWeb, 7(3):154?165, September.Andras Csomai and Rada Mihalcea.
2008.
Linkingdocuments to encyclopedic knowledge.
IEEE Intel-ligent Systems, 23(5):34?41.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press.David Ferrucci, Eric Brown, Jennifer Chu-Carroll,James Fan, David Gondek, Aditya Kalyanpur, AdamLally, J. William Murdock, Eric Nyberg, JohnPrager, Nico Schlaefer, and Chris Welty.
2010.Building watson: An overview of the deepqaproject.
AI Magazine, 31(3).Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In ACM SIGKDD Con-ference on Knowledge Discovery and Data Mining(KDD), pages 133?142.
ACM.Aditya Kalyanpur, J William Murdock, James Fan, andChristopher Welty.
2011.
Leveraging community-built knowledge for type coercion in question an-swering.
In The Semantic Web?ISWC 2011, pages144?156.
Springer.David Milne and Ian H Witten.
2009.
An open-sourcetoolkit for mining wikipedia.
In New Zealand Com-puter Science Research Student Conference (NZC-SRSC).Alessandro Moschitti and Silvia Quarteroni.
2008.Kernels on linguistic structures for answer extrac-tion.
In Proceedings of the 46th Annual Meet-ing of the Association for Computational Linguis-tics on Human Language Technologies: Short Pa-pers (ACL), pages 113?116.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploitingsyntactic and shallow semantic kernels for ques-tion/answer classification.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics (ACL), pages 776?783.Alessandro Moschitti.
2006.
Efficient convolutionkernels for dependency and constituent syntactictrees.
In Proceedings of the 17th European Confer-ence on Machine Learning (ECML), pages 318?329.Springer.Alessandro Moschitti.
2009.
Syntactic and seman-tic kernels for short text pair categorization.
InProceedings of the 12th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL), pages 576?584.
Association forComputational Linguistics.J William Murdock, Aditya Kalyanpur, Chris Welty,James Fan, David A Ferrucci, DC Gondek, LeiZhang, and Hiroshi Kanayama.
2012.
Typing can-didate answers using type coercion.
IBM Journal ofResearch and Development, 56(3.4):7?1.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning ofanswer re-ranking.
In Proceedings of the 35th in-ternational ACM SIGIR conference on Research anddevelopment in information retrieval (SIGIR), pages741?750.
ACM.Aliaksei Severyn and Alessandro Moschitti.
2013.
Au-tomatic feature engineering for answer selection andextraction.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP), pages 458?467.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013a.
Building structures from clas-sifiers for passage reranking.
In Proceedings of the22nd ACM international conference on Conferenceon information & knowledge management (CIKM),pages 969?978.
ACM.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013b.
Learning adaptable patterns forpassage reranking.
In Proceedings of the Seven-teenth Conference on Computational Natural Lan-guage Learning (CoNLL).Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In Proceedings of the 16th international con-ference on World Wide Web (WWW), pages 697?706.ACM Press.Ellen M Voorhees.
2001.
Overview of the TREC2001 Question Answering Track.
In Proceedings ofTREC, pages 42?51.672
