Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 934?944,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA chance-corrected measure of inter-annotator agreement for syntaxArne Skj?rholtLanguage technology group, dept.
of informaticsUniversity of Osloarnskj@ifi.uio.noAbstractFollowing the works of Carletta (1996)and Artstein and Poesio (2008), there is anincreasing consensus within the field thatin order to properly gauge the reliabilityof an annotation effort, chance-correctedmeasures of inter-annotator agreementshould be used.
With this in mind, itis striking that virtually all evaluationsof syntactic annotation efforts use uncor-rected parser evaluation metrics such asbracket F1(for phrase structure) and ac-curacy scores (for dependencies).In this work we present a chance-correctedmetric based on Krippendorff?s ?, adaptedto the structure of syntactic annotationsand applicable both to phrase structureand dependency annotation without anymodifications.
To evaluate our metric wefirst present a number of synthetic experi-ments to better control the sources of noiseand gauge the metric?s responses, beforefinally contrasting the behaviour of ourchance-corrected metric with that of un-corrected parser evaluation metrics on realcorpora.11 IntroductionIt is a truth universally acknowledged that an an-notation task in good standing be in possessionof a measure of inter-annotator agreement (IAA).However, no such measure is in widespread usefor the task of syntactic annotation.
This is due toa mismatch between the formulation of the agree-ment measures, which assumes that the annota-tions have no or relatively little internal structure,1The code used to produce the data in this paper,and some of the datasets used, are available to download athttps://github.com/arnsholt/syn-agreement/and syntactic annotation where structure is the en-tire point of the annotation.
For this reason effortsto gauge the quality of syntactic annotation arehampered by the need to fall back to simple ac-curacy measures.
As shown in Artstein and Poesio(2008), such measures are biased in favour of an-notation schemes with fewer categories and do notaccount for skewed distributions between classes,which can give high observed agreement, even ifthe annotations are inconsistent.In this article we propose a family of chance-corrected measures of agreement, applicable toboth dependency- and constituency-based syntac-tic annotation, based on Krippendorff?s ?
and treeedit distance.
First we give an overview of tradi-tional agreement measures and why they are insuf-ficient for syntax, before presenting our proposedmetrics.
Next, we present a number of syntheticexperiments performed in order to find the bestdistance function for this kind of annotation; fi-nally we contrast our new metric and simple accu-racy scores as applied to real-world corpora beforeconcluding and presenting some potential avenuesfor future work.1.1 Previous workThe definitive reference for agreement measuresin computational linguistics is Artstein and Poe-sio (2008), who argue forcefully in favour of theuse of chance-corrected measures of agreementover simple accuracy measures.
However, mostevaluations of syntactic treebanks use simple ac-curacy measures such as bracket F1scores forconstituent trees (NEGRA, Brants, 2000; TIGER,Brants and Hansen, 2002; Cat3LB, Civit et al,2003; The Arabic Treebank, Maamouri et al,2008) or labelled or unlabelled attachment scoresfor dependency syntax (PDT, Haji?c, 2004; PCEDTMikulov?a and?St?ep?anek, 2010; Norwegian De-pendency Treebank, Skj?rholt, 2013).
The onlywork we know of using chance-corrected metrics934is Ragheb and Dickinson (2013), who use MASI(Passonneau, 2006) to measure agreement on de-pendency relations and head selection in multi-headed dependency syntax, and Bhat and Sharma(2012), who compute Cohen?s ?
(Cohen, 1960)on dependency relations in single-headed depen-dency syntax.
A limitation of the first approach isthat token ID becomes the relevant category forthe purposes of agreement, while the second ap-proach only computes agreements on relations, noton structure.In grammar-driven treebanking (or parsebank-ing), the problems encountered are slightly differ-ent.
In HPSG and LFG treebanking annotators donot annotate structure directly.
Instead, the gram-mar parses the input sentences, and the annotatorselects the correct parse (or rejects all the candi-dates) based on discriminants2of the parse forest.In this context, de Castro (2011) developed a vari-ant of ?
that measures agreement over discrimi-nant selection.
This is different from our approachin that agreement is computed on annotator deci-sions rather than on the treebanked analyses, andis only applicable to grammar-based approachessuch as HPSG and LFG treebanking.The idea of using edit distance as the basisfor an inter-annotator agreement metric has previ-ously been explored by Fournier (2013).
Howeverthat work used a boundary edit distance as the ba-sis of a metric for the task of text segmentation.1.2 NotationIn this paper, we mostly follow the notation andterminology of Artstein and Poesio (2008), withsome additions.
The key components in an agree-ment study are the items annotated, the coders whomake judgements on individual items, and the an-notations created for the items.
We denote these asfollows:?
The set of items I = {i1, i2, .
.
.
}?
The set of coders C = {c1, c2, .
.
.
}?
The set of annotations X is a set of sets X ={Xi|i ?
I} where each set Xi= {xic|c ?C} contains the annotations for each item.
Ifnot all coders annotate all items, the differentXiwill be of different sizes.2A discriminant is an attribute of the analyses producedby the grammar where some of the analyses differ, e.g.
is theword jump a noun or a verb, or does a PP attach to a VP orthe VP?s object NP.In the case of nominal categorisation we will alsouse the set K of possible categories.2 The metricThe most common metrics used in computationallinguistics are the metrics ?
(Cohen, 1960, in-troduced to computational linguistics by Carletta,1996) and pi (Scott, 1955).
These metrics expressagreement on a nominal coding task as the ra-tio ?, pi =Ao?Ae/1?Aewhere Aois the observedagreement andAethe expected agreement accord-ing to some model of ?random?
annotation.
Bothmetrics have essentially the same model of ex-pected agreement:Ae=?k?KP (k|c1)P (k|c2) (1)differing only in how they estimate the probabil-ities: ?
assigns separate probability distributionsto each coder based on their observed behaviour,while pi uses the same distribution for both codersbased on their aggregate behaviour.Now, if we want to perform this same kind ofevaluation on syntactic annotation it is not possibleto use ?
or pi directly.
In the case of dependency-based syntax we could conceivably use a variantof these metrics by considering the ID of a to-ken?s head as a categorical variable (the approachtaken in Ragheb and Dickinson, 2013), but we ar-gue that this is not satisfactory.
This use of themetrics would consider agreement on categoriessuch as ?tokens whose head is token number 24?,which is obviously not a linguistically informativecategory.
Thus we have to reject this way of as-sessing the reliability of dependency syntax anno-tation.
Also, this approach is not directly general-isable to constituency-based syntax.For dependency syntax we could generalisethese metrics similarly to how ?
is generalised to?wto handle partial credit for overlapping annota-tions.
Let the function LAS(t1, t2) be the numberof tokens with the same head and label in the twotrees t1and t2, T (i) the set of trees possible foran item i ?
I , and tokens the number of tokensin the corpus.
Then we can compute an expectedagreement as follows:Ae=1tokens?i?I?t1,t2?T (i)2LASe(t1, t2) (2)LASe(t1, t2) = P (t1|c1)P (t2|c2)LAS(t1, t2)935ROOTsawmantheDETISUBJOBJPRED(a) The original depen-dency treePREDOBJDETSUBJ(b) The tree used in com-parisonsFigure 1: Transformation of dependency trees be-fore comparisonWe see three problems with this approach.
Firstof all the number of possible trees for a sentencegrows exponentially with sentence length, whichmeans that explicitly iterating over all possiblesuch pairs is computationally intractable, nor havewe been able to easily derive an algorithm for thisparticular problem from standard algorithms.Second, the question of which model to use forP (t|c) is not straightforward.
It is possible to usegenerative parsing models such as PCFGs or thegenerative dependency models of Eisner (1996),but agreement metrics require a model of randomannotation, and as such using models designed forparsing runs the risk of over-estimating Ae, result-ing in artificially low agreement scores.Finally, it may be hard to establish a consensusin the field of which particular metric to use.
Asshown by the existence of three different metrics(?, pi and S (Bennett et al, 1954)) for the rela-tively simple task of nominal coding, the choiceof model for P (t|c) will not be obvious, and thusdiffering choices of generative model as well asdifferent choices for parameters such as smooth-ing will result in subtly different agreement met-rics.
The results of these different metrics will notbe directly comparable, which will make the re-sults of groups using different metrics unnecessar-ily hard to compare.Instead, we propose to use an agreement mea-sure based on Krippendorff?s ?
(Krippendorff,1970; Krippendorff, 2004) and tree edit distance.In this approach we compare tree structures di-rectly, which is extremely parsimonious in termsof assumptions, and furthermore sidesteps theproblem of probabilistically modelling annotators?behaviour entirely.
Krippendorff?s?
is not as com-monly used as ?
and pi, but it has the advantage ofbeing expressed in terms of an arbitrary distancefunction ?.A full derivation of ?
is beyond the scope ofthis article, and we will simply state the formulaused to compute the agreement.
Krippendorff?s?
is normally expressed in terms of the ratio ofobserved and expected disagreements: ?
= 1 ?Do/De, where Dois the mean squared distance be-tween annotations of the same item and Dethemean squared distance between all pairs of anno-tations:Do=?i?I1|Xi| ?
1?c?C?c??C?
(xic, xic?
)2De=1?i?I|Xi| ?
1?i?I?c?C?i??I?c??C?
(xic, xi?c?
)2Note that in the expression for De, we are com-puting the difference between annotations for dif-ferent items; thus, our distance function for syn-tactic trees needs to be able to compute the differ-ence between arbitrary trees for completely unre-lated sentences.
The function ?
can be any func-tion as long as it is a metric; that is, it must be(1) non-negative, (2) symmetric, (3) zero only foridentical inputs, and (4) it must obey the triangleinequality:1.
?x, y : ?
(x, y) ?
02.
?x, y : ?
(x, y) = ?
(x, y)3.
?x, y : ?
(x, y) = 0?
x = y4.
?x, y, z : ?
(x, y) + ?
(y, z) ?
?
(x, z)This immediately excludes metrics like Pars-Eval (Black et al, 1991) and Leaf-Ancestor(Sampson and Babarczy, 2003), since they assumethat the trees being compared are parses of thesame sentence.
Instead, we base our work on treeedit distance.
The tree edit distance (TED) prob-lem is defined analogously to the more familiarproblem of string edit distance: what is the min-imum number of edit operations required to trans-form one tree into the other?
See Bille (2005)for a thorough introduction to the tree edit dis-tance problem and other related problems.
Forthis work, we used the algorithm of Zhang andShasha (1989).
Tree edit distance has previouslybeen used in the TEDEVAL software (Tsarfaty etal., 2011; Tsarfaty et al, 2012) for parser evalua-tion agnostic to both annotation scheme and the-oretical framework, but this by itself is still an936PREDOBJDETSUBJPREDOBJATRPREDOBJDETDETSUBJPREDSUBJFigure 2: Three trees with distance zero using?diffuncorrected accuracy measure and thus unsuitablefor our purposes.3When comparing syntactic trees, we only wantto compare dependency relations or non-terminalcategories.
Therefore we remove the leaf nodes inthe case of phrase structure trees, and in the case ofdependency trees we compare trees whose edgesare unlabelled and nodes are labelled with the de-pendency relation between that word and its head;the root node receives the label .
An example ofthis latter transformation is shown in Figure 1.We propose three different distance functionsfor the agreement computation: the unmodifiedtree edit distance function, denoted ?plain, a sec-ond function ?diff(x, y) = TED(x, y)?abs(|x|?|y|), the edit distance minus the difference inlength between the two sentences, and finally?norm(x, y) =TED(x,y)/|x|+|y|, the edit distancenormalised to the range [0, 1].4The plain TED is the simplest in terms of parsi-mony assumptions, however it may overestimatethe difference between sentences, we intuitivelyfind to be syntactically similar.
For example theonly difference between the two leftmost trees inFigure 2 is a modifier, but ?plaingives them dis-tance 4 and ?diff0.
On the other hand, ?diffmightunderestimate some distances as well; for exam-3While it is quite different from other parser evaluationschemes, TEDEVAL does not correct for chance agreementand is thus an uncorrected metric.
It could of course formthe basis for a corrected metric, given a suitable measure ofexpected agreement.4We can easily show that |x| + |y| is an upper bound onthe TED, corresponding to deleting all nodes in the sourcetree and inserting all the nodes in the target.ple the leftmost and rightmost trees also have dis-tance zero using ?diff, despite our syntactic intu-ition that the difference between a transitive andan intransitive should be taken account of.The third distance function, ?norm, takes intoaccount a slightly different concern; namely thatwhen comparing a long sentence and a short sen-tence, the distance has to be quite large simply toaccount for the difference in number of nodes, un-like comparing two short or two long sentences.Normalising to the range [0, 1] puts all pairs on anequal footing.However, we cannot a priori say which of thethree functions is the optimal choice of distancefunctions.
The different functions have differentproperties, and different advantages and draw-backs, and the nature of their strengths and weak-nesses differ.
We will therefore perform a numberof synthetic experiments to investigate their prop-erties in a controlled environment, before applyingthem to real-world data.3 Synthetic experimentsIn the previous section, we proposed threedifferent agreement metrics ?plain, ?diffand?norm, each involving different trade-offs.
Decid-ing which of these metrics is the best one for ourpurposes of judging the consistency of syntacticannotation poses a bit of a conundrum.
We couldat this point apply our metrics to various real cor-pora and compare the results, but since the consis-tency of the corpora is unknown, it?s impossible tosay whether the best metric is the one resulting inthe highest scores, the lowest scores or somewherein the middle.
To properly settle this question, wefirst performed a number of synthetic experimentsto gauge how the different metrics respond to dis-agreement.The general approach we take is based on thatused by Mathet et al (2012), adapted to depen-dency trees.
An already annotated corpus, in ourcase 100 randomly selected sentences from theNorwegian Dependency Treebank (Solberg et al,2014), are taken as correct and then permuted toproduce ?annotations?
of different quality.
For de-pendency trees, the input corpus is permuted asfollows:1.
Each token has a probability prelabelof beingassigned a different label uniformly at ran-dom from the set of labels used in the corpus.937-0.200.20.40.60.810 0.2 0.4 0.6 0.8 1Agreementprelabel= preattach?plain?diff?normLASFigure 3: Mean agreement over ten runs2.
Each token has a probability preattachof be-ing assigned a new head uniformly at randomfrom the set of tokens not dominated by thetoken.The second permutation process is dependent onthe order the tokens are processed, and we con-sider the tokens in the post-order5as dictated bythe original tree.
This way tokens close to the roothave a fair chance of having candidate heads ifthey are selected.
A pre-order traversal would re-sult in tokens close to the root having few options,and in particular if the root has a single child, thatnode has no possible new heads unless one of itschildren has been assigned the root as its new headfirst.
For example in the trees in figure 2, assign-ing any other head than the root to the PRED nodesdirectly dominated by the root will result in in-valid (cyclic and unconnected) dependency trees.Traversing the tokens in the linear order dictatedby the sentence has similar issues for tokens closeto the root and close to the start of the sentence.For our first set of experiments, we setprelabel= preattachand evaluated the differentagreement metrics for 10 evenly spaced p-valuesbetween 0.1 and 1.0.
Initial exploration of thedata showed that the mean follows the medianvery closely regardless of metric and perturbationlevel, and therefore we only report the mean scoresacross runs in this paper.
The results of these ex-periments are shown in Figure 3, with the labelledattachment score6(LAS) for comparison.5That is, the child nodes of a node are all processed beforethe node itself.
Nodes on the same level are traversed fromleft to right.6The de facto standard parser evaluation metric in depen--0.200.20.40.60.810 0.2 0.4 0.6 0.8 1Agreementprelabel?plain?diff?normLASFigure 4: Mean agreement over ten runs,preattach= 0The ?diffmetric is clearly extremely sensitiveto noise, with p = 0.1 yielding mean ?diff=15.8%, while ?normis more lenient than bothLAS and ?plain, with mean ?norm= 14.5% atp = 1, quite high compared to LAS = 0.9%,?plain= ?6.8% and ?diff= ?246%.
To fur-ther study the sensitivity of the metrics to the twokinds of noise, we performed an additional set ofexperiments, setting one p = 0 while varying theother over the same range as in the previous exper-iment, the results of which are shown in Figures 4and 5.The LAS curves are mostly unremarkable, withone exception: Mean LAS at preattach= 1 of Fig-ure 5 is 23.9%, clearly much higher than we wouldexpect if the trees were completely random.
Incomparison, mean LAS when only labels are per-turbed is 4.1%, and since the sample space of treesof size n is clearly much larger than that of rela-bellings, a uniform random selection of tree wouldyield a LAS much closer to 0.
This shows that ourtree shuffling algorithm has a non-uniform distri-bution over the sample space.While the behaviour of our alphas and LAS arerelatively similar in Figure 3, Figures 4 and 5 showthat they do in fact have important differences.Whereas LAS responds linearly to perturbation ofboth labels and structure, with its parabolic be-haviour in Figure 3 being simply the product ofthese two linear responses, the ?
metrics responddifferently to structural noise and label noise, withlabel disagreements being penalised less harshlydency parsing: the percentage of tokens that receive the cor-rect head and dependency relation.938-0.200.20.40.60.810 0.2 0.4 0.6 0.8 1Agreementpreattach?plain?diff?normLASFigure 5: Mean agreement over ten runs,prelabel= 0than structural disagreements.The reason for the strictness of the ?diffmet-ric and the laxity of ?normis the effects the mod-ified distance functions have on the distributionof distances.
The ?difffunction causes an ex-treme shift of the distances towards 0; more than30% of the sentence pairs have distance 0, 1, or2, which causes Ddiffeto be extremely low andthus gives disproportionally large weight to non-zero distances in Ddiffo.
On the other hand ?normcauses a rightward shift of the distances, which re-sults in a highDnormeand thus individual disagree-ments having less weight.4 Real-world corporaSynthetic experiments do not always fully re-flect real-world behaviour, however.
Therefore wewill also evaluate our metrics on real-world inter-annotator agreement data sets.
In our evaluation,we will contrast labelled accuracy, the standardparser evaluation metric, and our three ?
metrics.In particular, we are interested in the correlation(or lack thereof) between LAS and the alphas,and whether the results of our synthetic experi-ments correspond well with the results on real-world IAA sets.
Finally, we also evaluate the met-ric on both dependency and phrase structure data.4.1 The corporaWe obtained7data from four different corpora.Three of the data sets are dependency treebanks7We contacted a number of treebank projects, amongthem the Penn Treebank and the Prague Dependency Tree-bank, but not all of them had data available.Corpus Sentences TokensNDT 1a130 1674NDT 2a110 1594NDT 3a150 1997CDT (da)a162 2394CDT (en)a264 5528CDT (es)b55 924CDT (it)c136 3057PCEDTd3531 61737SSDe96 1581a2 annotatorsb4 annotators, avg.
2.8 annotators/text (min.
2, max.
4)c3 annotators, avg.
2.7 annotators/textd11 annotators, avg.
2.5 annotators/text (min.
2, max.
6)e3 annotators, avg.
2.9 annotators/sent.Table 1: Sizes of the different IAA corpora(NDT, CDT, PCEDT) and one phrase structuretreebank (SSD), and of the dependency tree-banks the PCEDT contains semantic dependen-cies, while the other two have traditional syntac-tic dependencies.
The number of annotators andsizes of the different data sets are summarised inTable 1.NDT The Norwegian Dependency Treebank(Solberg et al, 2014) is a dependency treebankconstructed at the National Library of Norway.The data studied in this work has previously beenused by Skj?rholt (2013) to study agreement,but using simple accuracy measures (UAS, LAS)rather than chance-corrected measures.
The IAAdata set is divided into three parts, correspondingto different parsers used to preprocess the data be-fore annotation; what we term NDT 1 through 3correspond to what Skj?rholt (2013) labels Dan-ish, Swedish and Norwegian, respectively.CDT The Copenhagen Dependency Treebanks(Buch-Kromann et al, 2009; Buch-Kromann andKorzen, 2010) is a collection of parallel depen-dency treebanks, containing data from the DanishPAROLE corpus (Keson, 1998b; Keson, 1998a)in the original Danish and translated into English,Italian and Spanish.PCEDT The Prague Czech-English Depen-dency Treebank 2.0 Haji?c et al (2012) is a par-allel corpus of English and Czech, consisting ofEnglish data from the Wall Street Journal Sectionof the Penn Treebank (Marcus et al, 1993) and939Czech translations of the English data.
The syn-tactic annotations are layered and consist of ananalytical layer similar to the annotations in mostother dependency treebanks, and a more semantictectogrammatical layer.Our data set consists of a common set of analyt-ical annotations shared by all the annotators, andthe tectogrammatical analyses built on top of thiscommon foundation.
A distinguishing feature ofthe tectogrammatical analyses, vis a vis the othertreebanks we are using, is that semantically emptywords only take part in the analytical annotationlayer and nodes are inserted at the tectogrammat-ical layer to represent covert elements of the sen-tence not present in the surface syntax of the ana-lytical layer.
Thus, inserting and deleting nodes isa central part of the task of tectogrammatical an-notation, unlike the more surface-oriented annota-tion of our other treebanks, where the tokenisationis fixed before the text is annotated.SSD The Star-Sem Data is a portion of thedataset released for the *SEM 2012 sharedtask (Morante and Blanco, 2012), parsed usingthe LinGO English Resource Grammar (ERG,Flickinger, 2000) and the resulting parse forestdisambiguated based on discriminants.
The ERGis an HPSG-based grammar, and as such its analy-ses are attribute-value matrices (AVMs); an AVMis not a tree but a directed acyclic graph however,and for this reason we compute agreement not onthe AVM but the so-called derivation tree.
Thistree describes the types of the lexical items in thesentence and the bottom-up ordering of rule ap-plications used to produce the final analysis andcan be handled by our procedure like any phrase-structure tree.4.2 Agreement resultsTo evaluate our corpora, we compute the three ?variants described in the previous two sections,and compare these with labelled accuracy scores.When there are more than two annotators, wegeneralise the metric to be the average pairwiseLAS for each sentence, weighted by the length ofthe sentence.
Let LAS(t1, t2) be the fraction of to-kens with identical head and label in the trees t1and t2; the pairwise labelled accuracy LASp(X)of a set of annotations X as described in sectionCorpus ?plain?diff?normLASNDT 1 98.4 93.0 98.8 94.0NDT 2 98.9 95.0 99.1 94.4NDT 3 97.9 91.2 98.7 95.3CDT (da) 95.7 84.7 96.2 90.4CDT (en) 92.4 70.7 95.0 88.4CDT (es) 86.6 48.8 85.8 78.9aCDT (it) 84.5 55.7 89.2 81.3bPCEDT 95.9 89.9 96.5 68.0cSSD 99.1 98.6 99.3 87.9da2 sentences ignoredb15 sentences ignoredc1178 sentences ignoreddMean pairwise Jaccard similarityTable 2: Agreement scores on real-world corpora1.2 is:LASp(X) =1?i|xi1|?|xi1|?(Xi)|Xi|(|Xi|?1)/2(3)?
(Xi) =|C|?c=1|C|?c?=c+1LAS(xic, xic?
)This is equivalent to the traditional metric in thecase where there are only two annotators.As our uncorrected metric for comparing twophrase structure trees we do not use the traditionalbracket F1as it does not generalise well to morethan two annotators, but rather Jaccard similarity.The Jaccard similarity of two sets A and B is theratio of the size of their intersection to the sizeof their union: J(A,B) =|A?B|/|A?B|, and weuse the Jaccard similarity of the sets of labelledbracketings of two trees as our uncorrected mea-sure.
To compute the similarity for a complete setof annotations we use the mean pairwise Jaccardsimilarity weighted by sentence length; that is, thesame procedure as in 3, but using Jaccard similar-ity rather than LAS.Since LAS assumes that both of the sentencescompared have identical sets of tokens, we hadto exclude a number of sentences from the LAScomputation in the cases of the English and Ital-ian CDT corpora, and especially the PCEDT.
Thelarge number of sentences excluded in the PCEDTis due to the fact that in the tectogrammatical anal-ysis of the PCEDT, inserting and deleting nodes isan important part of the annotation task.Looking at the results in Table 2, we observe94040506070809010070 75 80 85 90 95 100?LAS?plain?diff?normFigure 6: Correlation of LAS with ?two things.
Most obvious, is the extremely largegap between the LAS and ?
metrics for thePCEDT data.
However, there is a more subtlepoint; the orderings of the corpora by the differ-ent metrics are not the same.
LAS order the cor-pora NDT 3, 2, 1, CDT da, en, it, es, PCEDT,whereas ?diffand ?normgives the order NDT 2,1, 3, PCEDT, CDT da, en, it, es, and ?plaingivesthe same order as the other alphas but with CDT esand it changing places.
Furthermore, as the scatter-plot in Figure 6 shows, there is a clear correlationbetween the ?
metrics and LAS, if we disregardthe PCEDT results.The reason the PCEDT gets such low LAS isessentially the same as the reason many sentenceshad to be excluded from the computation in thefirst place; since inserting and deleting nodes isan integral part of the tectogrammatical annotationtask, the assumption implicit in the LAS computa-tion that sentences with the same number of nodeshave the same nodes in the same order is obviouslyfalse, resulting in a very low LAS.The corpus that scores the highest for all threemetrics is the SSD corpus; the reason for this isuncertain, as our corpora differ along many dimen-sions, but the fact that the annotation was done byprofessional linguists who are very familiar withthe grammar used to parse the data is likely acontributing factor.
The difference between the ?metrics and the Jaccard similarity is larger thanthe difference between ?
and LAS for our depen-dency corpora, however the two similarity metricsare not comparable, and it is well known that forphrase structures single disagreements such as aPP-attachment disagreement can result in multipledisagreeing bracketings.5 ConclusionThe most important conclusion we draw from thiswork is the most appropriate agreement metric forsyntactic annotation.
First of all, we disqualify theLAS metric, primarily due to the methodologi-cal inadequacies of using an uncorrected measure.While our experiments did not reveal any seri-ous shortcomings (unlike those of Mathet et al,2012 who in the case of categorisation showedthat for large p the uncorrected measure can beincreasing), the methodological problems of un-corrected metrics makes us wary of LAS as anagreement metric.
Next, of the three ?
metrics,?plainis clearly the best; ?diffis extremely sen-sitive to even moderate amounts of disagreement,while ?normis overly lenient.Looking solely at Figure 3, one might be led tobelieve that LAS and ?plainare interchangeable,but this is not the case.
As shown by Figures 4and 5, the paraboloid shape of the LAS curve inFigure 3 is simply the combination of the met-ric?s linear responses to both label and structuralperturbations.
The behaviour of ?
on the otherhand is more complex, with structural noise be-ing penalised harder than perturbations of the la-bels.
Thus, the similarity of LAS and ?plainis notat all assured when the amounts of structural andlabelling disagreements differ.
Additionally, weconsider this imbalanced weighting of structuraland labelling disagreements a benefit, as structureis the larger part of syntactic annotation comparedto the labelling of the dependencies/bracketings.Finally our experiments show that ?
is a singlemetric that is applicable to both dependencies andphrase structure trees.Furthermore, ?
metrics are far more flexiblethan simple accuracy metrics.
The use of a dis-tance function to define the metric means thatmore fine-grained distinctions can be made; forexample, if the set of labels on the structures ishighly structured, partial credit can be given fordiffering annotations that overlap.
For example, ifdifferent types of adverbials (temporal, negation,etc.)
receive different relations, as is the case inthe Swedish Talbanken05 (Nivre et al, 2006) cor-pus, confusion of different adverbial types can begiven less weight than confusion between subjectand object.
The ?-based metrics are also far easierto apply to a more complex annotation task such941as the tectogrammatical annotation of the PCEDT.In this task inserting and deleting nodes is an in-tegral part of the annotation, and if two annotatorsinsert or delete different nodes the all-or-nothingrequirement of identical yield of the LAS metricmakes it impossible as an evaluation metric in thissetting.5.1 Future workIn future work, we would like to investigate the useof other distance functions, in particular the useof approximate tree edit distance functions suchas the pq-gram algorithm (Augsten et al, 2005).For large data sets such as the PCEDT set used inthis work, computing ?
with tree edit distance asthe distance measure can take a very long time.8This is due to the fact that ?
requires O(n2) com-parisons to be made, each of which is O(n2) us-ing our current approach.
The problem of directedgraph edit distance is NP-hard, which means thatto apply our method to HPSG analyses directly ap-proximate algorithms are a requirement.Another avenue for future work is improvedsynthetic experiments.
As we saw, our implemen-tation of tree perturbations was biased towardstrees similar in shape to the source tree, and an im-proved permutation algorithm may reveal interest-ing edge-case behaviour in the metrics.
A methodfor perturbing phrase structure trees would alsobe interesting, as this would allow us to repeatthe synthetic experiments performed here usingphrase structure corpora to compare the behaviourof the metrics on the two types of corpus.Finally, annotator modelling techniques likethat presented in Passonneau and Carpenter (2013)has obvious advantages over agreement coeffi-cients such as ?.
These techniques are interpretedmore easily than agreement coefficients, and theyallow us to assess the quality of individual annota-tors, a crucial property in crowd-sourcing settingsand something that?s impossible using agreementcoefficients.AcknowledgementsI would like to thank Jan?St?ep?anek at Charles Uni-versity for data from the PCEDT and help withthe conversion process, the CDT project for pub-lishing their agreement data, Per Erik Solberg at8The Python implementation used in this work, usingNumPy and the PyPy compiler, took seven and a half hourscompute a single ?
for the PCEDT data set on an Intel Corei7 2.9 GHz computer.
The program is single-threaded.the Norwegian National Library for data fromthe NDT, and Emily Bender at the University ofWashington for the SSD data.ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Compu-tational Linguistics, 34(4):555?596.Nikolaus Augsten, B?ohlen Michael, and Johann Gam-per.
2005.
Approximate Matching of HierarchicalData Using pq-Grams.
In Proceedings of the 31stinternational conference on Very large data bases,pages 301?312, Trondheim.
VLDB Endowment.E.
M. Bennett, R. Alpert, and A. C. Goldstein.
1954.Communications Through Limited-Response Ques-tioning.
Public Opinion Quarterly, 18(3):303?308.Riyaz Ahmad Bhat and Dipti Misri Sharma.
2012.A Dependency Treebank of Urdu and its Evalua-tion.
In Proceedings of the Sixth Linguistic Anno-tation Workshop, pages 157?165, Jeju.
Associationfor Computational Linguistics.Philip Bille.
2005.
A survey on tree edit distance andrelated problems.
Theoretical Computer Science,337(1-3):217?239, June.Ezra Black, Steven Abney, Dan Flickinger, ClaudiaGdaniec, Ralph Grishman, Philip Harrison, Don-ald Hindle, Robert Ingria, Frederick Jelinek, Ju-dith Klavans, Mark Liberman, Mitchell P. Mar-cus, Salim Roukos, Beatrice Santorini, and TomekStrzalkowski.
1991.
A Procedure for Quantita-tively Comparing the Syntactic Coverage of EnglishGrammars.
In Proceedings of the workshop onSpeech and Natural Language, pages 306?311, Pa-cific Grove, USA.Sabine Brants and Silvia Hansen.
2002.
Developmentsin the TIGER Annotation Scheme and their Realiza-tion in the Corpus.
In Proceedings of the Third In-ternational Conference on Language Resources andEvaluation, pages 1643?1649.Thorsten Brants.
2000.
Inter-Annotator Agreement fora German Newspaper Corpus.
In Proceedings of theSecond International Conference on Language Re-sources and Evaluation.Matthias Buch-Kromann and I?rn Korzen.
2010.
Theunified annotation of syntax and discourse in theCopenhagen Dependency Treebanks.
In Proceed-ings of the Fourth Linguistic Annotation Workshop,pages 127?131, Uppsala.
Association for Computa-tional Linguistics.Matthias Buch-Kromann, I?rn Korzen, and Hen-rik H?eg M?uller.
2009.
Uncovering the ?lost?
struc-ture of translations with parallel treebanks.
In FabioAlves, Susanne G?opferich, and Inger Mees, editors,942Methodology, Technology and Innovation in Trans-lation Process Research, pages 199?224.
Samfund-slitteratur, Frederiksberg.Jean Carletta.
1996.
Assessing Agreement on Classi-fication Tasks: The Kappa Statistic.
ComputationalLinguistics, 22(2):249?254.Montserrat Civit, Alicia Ageno, Borja Navarro, N?uriaBuf?
?, and M. Ant`onia Mart??.
2003.
Qualitative andQuantitative Analysis of Annotators?
Agreement inthe Development of.
In Joakim Nivre and ErhardHinrichs, editors, Proceedings of the Second Work-shop on Treebanks and Linguistic Theories, pages21?32, V?axj?o.
V?axj?o University Press.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and PsychologicalMeasurement, 20(1):37?46, April.S?ergio Ricardo de Castro.
2011.
Developing relia-bility metrics and validation tools for datasets withdeep linguistic information.
Master?s thesis, Univer-sidade de Lisboa.Jason M. Eisner.
1996.
Three New ProbabilisticModels for Dependency Parsing: An Exploration.In Proceedings of the 16th International Confer-ence on Computational Linguistics, pages 340?345,Stroudsburg.
Association for Computational Lin-guistics.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6(1):15?28, March.Chris Fournier.
2013.
Evaluating Text Segmentationusing Boundary Edit Distance.
In Proceedings ofthe 51st Annual Meeting of the Association for Com-putational Linguistics, pages 1702?1712, Sofia.
As-sociation for Computational Linguistics.Jan Haji?c, Eva Haji?cov?a, Jarmila Panevov?a, Petr Sgall,Silvie Cinkov?a, Eva Fu?c?
?kov?a, Marie Mikulov?a, PetrPajas, Jan Popelka, Ji?r??
Semeck?y, Jana?Sindlerov?a,Jan?St?ep?anek, Josef Toman, Zde?nka Ure?sov?a, andZd?en?ek?Zabokrtsk?y.
2012.
Prague Czech-EnglishDependency Treebank 2.0.Jan Haji?c.
2004.
Complex Corpus Annotation: ThePrague Dependency Treebank.
Jazykovedn?y ?ustav?L.
?St?ura, SAV.Britt Keson.
1998a.
The Danish MorphosyntacticallyTagged PAROLE Corpus.
Technical report, DanishSociety for Literature and Language, Copenhagen.Britt Keson.
1998b.
Vejledning til det danske morfo-syntaktisk taggede PAROLE-korpus.
Technical re-port, Danish Society for Literature and Language,Copenhagen.Klaus Krippendorff.
1970.
Estimating the Reliabil-ity, Systematic Error and Random Error of IntervalData.
Educational and Psychological Measurement,30(1):61?70, April.Klaus Krippendorff.
2004.
Content Analysis: An in-troduction to its methodology.
Sage Publications,Thousand Oaks, 2nd edition.Mohamed Maamouri, Ann Bies, and Seth Kulick.2008.
Enhancing the Arabic Treebank : A Collab-orative Effort toward New Annotation Guidelines.In Proceedings of the Sixth International Confer-ence on Language Resources and Evaluation, pages3192?3196.
European Language Resources Associ-ation.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Yann Mathet, Antoine Widl?ocher, Kar?en Fort, ClaireFranc?ois, Olivier Galibert, Cyril Grouin, JulietteKahn, Sophie Rosset, and Pierre Zweigenbaum.2012.
Manual Corpus Annotation : Giving Meaningto the Evaluation Metrics.
In Proceedings of COL-ING 2012, pages 809?818, Mumbai.Marie Mikulov?a and Jan?St?ep?anek.
2010.
Ways ofEvaluation of the Annotators in Building the PragueCzech-English Dependency Treebank.
In Proceed-ings of the Seventh International Conference onLanguage Resources and Evaluation, pages 1836?1839, Valletta.
European Language Resources As-sociation.Roser Morante and Eduardo Blanco.
2012.
*SEM2012 Shared Task : Resolving the Scope and Fo-cus of Negation.
In The First Joint Conference onLexical and Computational Semantics, pages 265?274, Montreal.
Association for Computational Lin-guistics.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.
Tal-banken05 : A Swedish Treebank with Phrase Struc-ture and Dependency Annotation.
In Proceedingsof the Fifth International Conference on LanguageResources and Evaluation.Rebecca J. Passonneau and Bob Carpenter.
2013.
TheBenefits of a Model of Annotation.
In Proceedingsof the 7th Linguistic Annotation Workshop and In-teroperability with Discourse, pages 187?195, Sofia.Association for Computational Linguistics.Rebecca J. Passonneau.
2006.
Measuring Agreementon Set-valued Items (MASI) for Semantic and Prag-matic Annotation.
In Proceedings of the Fifth In-ternational Conference on Language Resources andEvaluation, pages 831?836.Marwa Ragheb and Markus Dickinson.
2013.
Inter-annotator Agreement for Dependency Annotation ofLearner Language.
In Proceedings of the EighthWorkshop on Innovative Use of NLP for BuildingEducational Applications, pages 169?179, Atlanta.Association for Computational Linguistics.943Geoffrey Sampson and Anna Babarczy.
2003.
A test ofthe leaf-ancestor metric for parse accuracy.
NaturalLanguage Engineering, 9(4):365?380, December.William A. Scott.
1955.
Reliability of Content Anal-ysis: The Case of Nominal Scale Coding.
PublicOpinion Quarterly, 19(3):321?325, January.Arne Skj?rholt.
2013.
Influence of preprocessingon dependency syntax annotation: speed and agree-ment.
In Proceedings of the 7th Linguistic Annota-tion Workshop and Interoperability with Discourse,pages 28?32, Sofia.
Association for ComputationalLinguistics.Per Erik Solberg, Arne Skj?rholt, Lilja ?vrelid, KristinHagen, and Janne Bondi Johannesen.
2014.
TheNorwegian Dependency Treebank.
In Proceed-ings of the Ninth International Conference on Lan-guage Resources and Evaluation, Reykjavik.
Euro-pean Language Resources Association.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2011.
Evaluating Dependency Parsing: Robust andHeuristics-Free Cross-Annotation Evaluation.
InProceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages385?396, Edinburgh.
Association for ComputationalLinguistics.Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.2012.
Cross-Framework Evaluation for StatisticalParsing.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Com-putational Linguistics, pages 44?54, Avignon.
As-sociation for Computational Linguistics.Kaizhong Zhang and Dennis Shasha.
1989.
SimpleFast Algorithms for the Editing Distance betweenTrees and Related Problems.
SIAM Journal on Com-puting, 18(6):1245?1262, December.944
