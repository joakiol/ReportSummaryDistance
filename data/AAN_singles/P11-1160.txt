Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1597?1606,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsPartial Parsing from Bitext ProjectionsPrashanth Mannem and Aswarth DaraLanguage Technologies Research CenterInternational Institute of Information TechnologyHyderabad, AP, India - 500032{prashanth,abhilash.d}@research.iiit.ac.inAbstractRecent work has shown how a parallelcorpus can be leveraged to build syntac-tic parser for a target language by project-ing automatic source parse onto the targetsentence using word alignments.
The pro-jected target dependency parses are not al-ways fully connected to be useful for train-ing traditional dependency parsers.
In thispaper, we present a greedy non-directionalparsing algorithm which doesn?t need afully connected parse and can learn frompartial parses by utilizing available struc-tural and syntactic information in them.Our parser achieved statistically signifi-cant improvements over a baseline systemthat trains on only fully connected parsesfor Bulgarian, Spanish and Hindi.
It alsogave a significant improvement over pre-viously reported results for Bulgarian andset a benchmark for Hindi.1 IntroductionParallel corpora have been used to transfer in-formation from source to target languages forPart-Of-Speech (POS) tagging, word sense disam-biguation (Yarowsky et al, 2001), syntactic pars-ing (Hwa et al, 2005; Ganchev et al, 2009; Jiangand Liu, 2010) and machine translation (Koehn,2005; Tiedemann, 2002).
Analysis on the sourcesentences was induced onto the target sentence viaprojections across word aligned parallel corpora.Equipped with a source language parser and aword alignment tool, parallel data can be used tobuild an automatic treebank for a target language.The parse trees given by the parser on the sourcesentences in the parallel data are projected onto thetarget sentence using the word alignments fromthe alignment tool.
Due to the usage of automaticsource parses, automatic word alignments and dif-ferences in the annotation schemes of source andtarget languages, the projected parses are not al-ways fully connected and can have edges missing(Hwa et al, 2005; Ganchev et al, 2009).
Non-literal translations and divergences in the syntaxof the two languages also lead to incomplete pro-jected parse trees.Figure 1 shows an English-Hindi parallel sen-tence with correct source parse, alignments andtarget dependency parse.
For the same sentence,Figure 2 is a sample partial dependency parse pro-jected using an automatic source parser on alignedtext.
This parse is not fully connected with thewords banaa, kottaige and dikhataa left withoutany parents.para bahuta haiThe cottage built on the hill looks very beautifulpahaada banaa huaa kottaige sundara dikhataaFigure 1: Word alignment with dependencyparses for an English-Hindi parallel sentenceTo train the traditional dependency parsers (Ya-mada and Matsumoto, 2003; Eisner, 1996; Nivre,2003), the dependency parse has to satisfy fourconstraints: connectedness, single-headedness,acyclicity and projectivity (Kuhlmann and Nivre,2006).
Projectivity can be relaxed in some parsers(McDonald et al, 2005; Nivre, 2009).
But theseparsers can not directly be used to learn from par-tially connected parses (Hwa et al, 2005; Ganchevet al, 2009).In the projected Hindi treebank (section 4) thatwas extracted from English-Hindi parallel text,only 5.9% of the sentences had full trees.
In1597Spanish and Bulgarian projected data extracted byGanchev et al (2009), the figures are 3.2% and12.9% respectively.
Learning from data with suchhigh proportions of partially connected depen-dency parses requires special parsing algorithmswhich are not bound by connectedness.
Its onlyduring learning that the constraint doesn?t satisfy.For a new sentence (i.e.
during inference), theparser should output fully connected dependencytree.para bahuta haipahaada banaa huaa kottaige sundara dikhataaon cottage very beautifulbuild lookhill PastPart.
Be.Pres.Figure 2: A sample dependency parse with partialparsesIn this paper, we present a dependency pars-ing algorithm which can train on partial projectedparses and can take rich syntactic information asfeatures for learning.
The parsing algorithm con-structs the partial parses in a bottom-up manner byperforming a greedy search over all possible rela-tions and choosing the best one at each step with-out following either left-to-right or right-to-lefttraversal.
The algorithm is inspired by earlier non-directional parsing works of Shen and Joshi (2008)and Goldberg and Elhadad (2010).
We also pro-pose an extended partial parsing algorithm that canlearn from partial parses whose yields are partiallycontiguous.Apart from bitext projections, this work can beextended to other cases where learning from par-tial structures is required.
For example, whilebootstrapping parsers high confidence parses areextracted and trained upon (Steedman et al, 2003;Reichart and Rappoport, 2007).
In cases wherethese parses are few, learning from partial parsesmight be beneficial.We train our parser on projected Hindi, Bulgar-ian and Spanish treebanks and show statisticallysignificant improvements in accuracies betweentraining on fully connected trees and learning frompartial parses.2 Related WorkLearning from partial parses has been dealt in dif-ferent ways in the literature.
Hwa et al (2005)used post-projection completion/transformationrules to get full parse trees from the projectionsand train Collin?s parser (Collins, 1999) on them.Ganchev et al (2009) handle partial projectedparses by avoiding committing to entire projectedtree during training.
The posterior regularizationbased framework constrains the projected syntac-tic relations to hold approximately and only in ex-pectation.
Jiang and Liu (2010) refer to align-ment matrix and a dynamic programming searchalgorithm to obtain better projected dependencytrees.
They deal with partial projections by break-ing down the projected parse into a set of edgesand training on the set of projected relations ratherthan on trees.While Hwa et al (2005) requires full projectedparses to train their parser, Ganchev et al (2009)and Jiang and Liu (2010) can learn from partiallyprojected trees.
However, the discriminative train-ing in (Ganchev et al, 2009) doesn?t allow forricher syntactic context and it doesn?t learn fromall the relations in the partial dependency parse.By treating each relation in the projected depen-dency data independently as a classification in-stance for parsing, Jiang and Liu (2010) sacrificethe context of the relations such as global struc-tural context, neighboring relations that are crucialfor dependency analysis.
Due to this, they reportthat the parser suffers from local optimization dur-ing training.The parser proposed in this work (section 3)learns from partial trees by using the availablestructural information in it and also in neighbor-ing partial parses.
We evaluated our system (sec-tion 5) on Bulgarian and Spanish projected depen-dency data used in (Ganchev et al, 2009) for com-parison.
The same could not be carried out forChinese (which was the language (Jiang and Liu,2010) worked on) due to the unavailability of pro-jected data used in their work.
Comparison withthe traditional dependency parsers (McDonald etal., 2005; Yamada and Matsumoto, 2003; Nivre,2003; Goldberg and Elhadad, 2010) which train oncomplete dependency parsers is out of the scope ofthis work.3 Partial ParsingA standard dependency graph satisfies four graphconstraints: connectedness, single-headedness,acyclicity and projectivity (Kuhlmann and Nivre,2006).
In our work, we assume the dependencygraph for a sentence only satisfies the single-1598a)parapahaada banaa huaa kottaige bahuta sundara dikhataa haihill on build PastPart.
cottage very beautiful look Be.Pres.b)para bahuta haipahaada banaa huaa kottaige sundara dikhataac)para haibanaa huaa kottaige sundara dikhataapahaadabahutad)haibanaa huaa kottaige sundara dikhataapahaadabahutaparae)haibanaa kottaige sundara dikhataapahaadabahutapara huaaf)banaa kottaige sundara dikhataapahaadabahutapara huaa haig)sundarabahuta haipahaadaparabanaa kottaige dikhataahuaah)haipahaadaparasundarabahutabanaa kottaige dikhataahuaaFigure 3: Steps taken by GNPPA.
The dashed arcs indicate the unconnected words in unConn.
Thedotted arcs indicate the candidate arcs in candidateArcs and the solid arcs are the high scoring arcs thatare stored in builtPPsheadedness, acyclicity and projectivity constraintswhile not necessarily being connected i.e.
all thewords need not have parents.Given a sentence W=w0 ?
?
?
wn with a set ofdirected arcs A on the words in W , wi ?
wj de-notes a dependency arc from wi to wj , (wi,wj) A.
wi is the parent in the arc and wj is the child inthe arc.
???
denotes the reflexive and transitive clo-sure of the arc.
wi???
wj says that wi dominateswj , i.e.
there is (possibly empty) path from wi towj .A node wi is unconnected if it does not havean incoming arc.
R is the set of all such uncon-nected nodes in the dependency graph.
For theexample in Figure 2, R={banaa, kottaige,dikhataa}.
A partial parse rooted at node widenoted by ?
(wi) is the set of arcs that can be tra-versed from node wi.
The yield of a partial parse?
(wi) is the set of nodes dominated by it.
Weuse pi(wi) to refer to the yield of ?
(wi) arrangedin the linear order of their occurrence in the sen-tence.
The span of the partial tree is the first andlast words in its yield.The dependency graph D can now be rep-resented in terms of partial parses by D =(W,R, %(R)) where W={w0 ?
?
?
wn} is the sen-tence, R={r1 ?
?
?
rm} is the set of unconnectednodes and %(R)= {?
(r1) ?
?
?
?
(rm)} is the set ofpartial parses rooted at these unconnected nodes.w0 is a dummy word added at the beginning ofW to behave as a root of a fully connected parse.A fully connected dependency graph would haveonly one element w0 in R and the dependencygraph rooted at w0 as the only (fully connected)parse in %(R).We assume the combined yield of %(R) spansthe entire sentence and each of the partial parses in%(R) to be contiguous and non-overlapping withone another.
A partial parse is contiguous if itsyield is contiguous i.e.
if a node wj  pi(wi), thenall the words between wi and wj also belong topi(wi).
A partial parse ?
(wi) is non-overlapping ifthe intersection of its yield pi(wi) with yields of allother partial parses is empty.3.1 Greedy Non-directional Partial ParsingAlgorithm (GNPPA)Given the sentence W and the set of unconnectednodes R, the parser follows a non-directionalgreedy approach to establish relations in a bottomup manner.
The parser does a greedy search overall the possible relations and picks the one with1599the highest score at each stage.
This process is re-peated until parents for all the nodes that do notbelong to R are chosen.Algorithm 1 lists the outline of the greedy non-directional partial parsing algorithm (GNPPA).builtPPs maintains a list of all the partialparses that have been built.
It is initializedin line 1 by considering each word as a sep-arate partial parse with just one node.
can-didateArcs stores all the arcs that are possi-ble at each stage of the parsing process in abottom up strategy.
It is initialized in line 2using the method initCandidateArcs(w0 ?
?
?
wn).initCandidateArcs(w0 ?
?
?
wn) adds two candidatearcs for each pair of consecutive words with eachother as parent (see Figure 3b).
If an arc has oneof the nodes in R as the child, it isn?t included incandidateArcs.Algorithm 1 Partial Parsing AlgorithmInput: sentence w0 ?
?
?
wn and set of partial tree roots un-Conn={r1 ?
?
?
rm}Output: set of partial parses whose roots are in unConn(builtPPs = {?
(r1) ?
?
?
?
(rm)})1: builtPPs = {?
(r1) ?
?
?
?
(rn)} ?
{w0 ?
?
?
wn}2: candidateArcs = initCandidateArcs(w0 ?
?
?
wn)3: while candidateArcs.isNotEmpty() do4: bestArc = argmaxci  candidateArcsscore(ci,?
?w )5: builtPPs.remove(bestArc.child)6: builtPPs.remove(bestArc.parent)7: builtPPs.add(bestArc)8: updateCandidateArcs(bestArc,candidateArcs, builtPPs, unConn)9: end while10: return builtPPsOnce initialized, the candidate arc with thehighest score (line 4) is chosen and acceptedinto builtPPs.
This involves replacing the bestarc?s child partial parse ?
(arc.child) and parentpartial parse ?
(arc.parent) over which the archas been formed with the arc ?
(arc.parent) ??
(arc.child) itself in builtPPs (lines 5-7).
In Figure3f, to accept the best candidate arc ?
(banaa) ??
(pahaada), the parser would remove the nodes?
(banaa) and ?
(pahaada) in builtPPs and add?
(banaa) ?
?
(pahaada) to builtPPs (see Fig-ure 3g).After the best arc is accepted, the candidateArcshas to be updated (line 8) to remove the arcs thatare no longer valid and add new arcs in the con-text of the updated builtPPs.
Algorithm 2 showsthe update procedure.
First, all the arcs that endon the child are removed (lines 3-7) along withthe arc from child to parent.
Then, the immedi-ately previous and next partial parses of the bestarc in builtPPs are retrieved (lines 8-9) to add pos-sible candidate arcs between them and the partialparse representing the best arc (lines 10-23).
Inthe example, between Figures 3b and 3c, the arcs?
(kottaige) ?
?
(bahuta) and ?(bahuta)?
?
(sundara) are first removed and the arc?
(kottaige) ?
?
(sundara) is added to can-didateArcs.
Care is taken to avoid adding arcs thatend on unconnected nodes listed in R.The entire GNPPA parsing process for the ex-ample sentence in Figure 2 is shown in Figure 3.Algorithm 2 updateCandidateArcs(bestArc, can-didateArcs, builtPPs, unConn)1: baChild = bestArc.child2: baParent = bestArc.parent3: for all arc  candidateArcs do4: if arc.child = baChild or(arc.parent = baChild andarc.child = baParent) then5: remove arc6: end if7: end for8: prevPP = builtPPs.previousPP(bestArc)9: nextPP = builtPPs.nextPP(bestArc)10: if bestArc.direction == LEFT then11: newArc1 = new Arc(prevPP,baParent)12: newArc2 = new Arc(baParent,prevPP)13: end if14: if bestArc.direction == RIGHT then15: newArc1 = new Arc(nextPP,baParent)16: newArc2 = new Arc(baParent,nextPP)17: end if18: if newArc1.parent /?
unConn then19: candidateArcs.add(newArc1)20: end if21: if newArc2.parent /?
unConn then22: candidateArcs.add(newArc2)23: end if24: return candidateArcs3.2 LearningThe algorithm described in the previous sectionuses a weight vector ?
?w to compute the best arcfrom the list of candidate arcs.
This weight vec-tor is learned using a simple Perceptron like algo-rithm similar to the one used in (Shen and Joshi,2008).
Algorithm 3 lists the learning frameworkfor GNPPA.For a training sample with sentence w0 ?
?
?
wn,projected partial parses projectedPPs={?
(ri) ?
?
??
(rm)}, unconnected words unConn and weightvector ?
?w , the builtPPs and candidateArcs are ini-tiated as in algorithm 1.
Then the arc with thehighest score is selected.
If this arc belongs tothe parses in projectedPPs, builtPPs and candi-dateArcs are updated similar to the operations in1600a)para haipahaada banaa huaa kottaige bahuta sundara dikhataahill on build PastPart.
cottage very beautiful look Be.Pres.b)para haipahaada banaa huaa kottaige bahuta sundara dikhataac)haibahutapahaada para banaa huaa kottaige sundara dikhataad)haipara bahutapahaada banaa huaa kottaige sundara dikhataaFigure 4: First four steps taken by E-GNPPA.
The blue colored dotted arcs are the additional candidatearcs that are added to candidateArcsalgorithm 1.
If it doesn?t, it is treated as a neg-ative sample and a corresponding positive candi-date arc which is present both projectedPPs andcandidateArcs is selected (lines 11-12).The weights of the positive candidate arc are in-creased while that of the negative sample (best arc)are decreased.
To reduce over fitting, we use aver-aged weights (Collins, 2002) in algorithm 1.Algorithm 3 Learning for Non-directional GreedyPartial Parsing AlgorithmInput: sentence w0 ?
?
?
wn, projected partial parses project-edPPs, unconnected words unConn, current ?
?wOutput: updated ?
?w1: builtPPs = {?
(r1) ?
?
?
?
(rn)} ?
{w0 ?
?
?
wn}2: candidateArcs = initCandidateArcs(w0 ?
?
?
wn)3: while candidateArcs.isNotEmpty() do4: bestArc = argmaxci  candidateArcsscore(ci,?
?w )5: if bestArc ?
projectedPPs then6: builtPPs.remove(bestArc.child)7: builtPPs.remove(bestArc.parent)8: builtPPs.add(bestArc)9: updateCandidateArcs(bestArc,candidateArcs, builtPPs, unConn)10: else11: allowedArcs = {ci | ci  candidateArcs && ci projectedArcs}12: compatArc = argmaxci  allowedArcsscore(ci,?
?w )13: promote(compatArc,?
?w )14: demote(bestArc,?
?w )15: end if16: end while17: return builtPPs3.3 Extended GNPPA (E-GNPPA)The GNPPA described in section 3.1 assumes thatthe partial parses are contiguous.
The exam-ple in Figure 5 has a partial tree ?
(dikhataa)which isn?t contiguous.
Its yield doesn?t con-tain bahuta and sundara.
We call such non-contiguous partial parses whose yields encompassthe yield of an other partial parse as partially con-tiguous.
Partially contiguous parses are commonin the projected data and would not be parsable bythe algorithm 1 (?(dikhataa)?
?
(kottaige)would not be identified).para bahuta haipahaada banaa huaa kottaige sundara dikhataahill on build cottage very beautiful lookPastPart.
Be.Pres.Figure 5: Dependency parse with a partially con-tiguous partial parseIn order to identify and learn from relationswhich are part of partially contiguous partialparses, we propose an extension to GNPPA.
Theextended GNPAA (E-GNPPA) broadens its scopewhile searching for possible candidate arcs givenR and builtPPs.
If the immediate previous orthe next partial parses over which arcs are tobe formed are designated unconnected nodes, theparser looks further for a partial parse over whichit can form arcs.
For example, in Figure 4b, thearc ?
(para) ?
?
(banaa) can not be added tothe candidateArcs since banaa is a designatedunconnected node in unConn.
The E-GNPPAlooks over the unconnected node and adds the arc?
(para) ?
?
(huaa) to the candidate arcs listcandidateArcs.E-GNPPA differs from algorithm 1 in lines 2and 8.
The E-GNPPA uses an extended initializa-tion method initCandidateArcsExtended(w0) for1601Parent and Child par.pos, chd.pos, par.lex, chd.lexSentence Contextpar-1.pos, par-2.pos, par+1.pos, par+2.pos, par-1.lex, par+1.lexchd-1.pos, chd-2.pos, chd+1.pos, chd+2.pos, chd-1.lex, chd+1.lexStructural InfoleftMostChild(par).pos, rightMostChild(par).pos, leftSibling(chd).pos,rightSibling(chd).posPartial Parse Context previousPP().pos, previousPP().lex, nextPP().pos, nextPP().lexTable 1: Information on which features are defined.
par denotes the parent in the relation and chd thechild.
.pos and .lex is the POS and word-form of the corresponding node.
+/-i is the previous/nextith word in the sentence.
leftMostChild() and rightMostChild() denote the left most and right mostchildren of a node.
leftSibling() and rightSibling() get the immediate left and right siblings of a node.previousPP() and nextPP() return the immediate previous and next partial parses of the arc in builtPPs atthe state.candidateArcs in line 2 and an extended proce-dure updateCandidateArcsExtended to update thecandidateArcs after each step in line 8.
Algorithm4 shows the changes w.r.t algorithm 2.
Figure 4presents the steps taken by the E-GNPPA parserfor the example parse in Figure 5.Algorithm 4 updateCandidateArcsExtended( bestArc, candidateArcs, builtPPs,unConn )?
?
?
lines 1 to 7 of Algorithm 2 ?
?
?prevPP = builtPPs.previousPP(bestArc)while prevPP ?
unConn doprevPP = builtPPs.previousPP(prevPP)end whilenextPP = builtPPs.nextPP(bestArc)while nextPP ?
unConn donextPP = builtPPs.nextPP(nextPP)end while?
?
?
lines 10 to 24 of Algorithm 2 ?
?
?3.4 FeaturesFeatures for a relation (candidate arc) are definedon the POS tags and lexical items of the nodes inthe relation and those in its context.
Two kindsof context are used a) context from the input sen-tence (sentence context) b) context in builtPPs i.e.nearby partial parses (partial parse context).
In-formation from the partial parses (structural info)such as left and right most children of the par-ent node in the relation, left and right siblings ofthe child node in the relation are also used.
Ta-ble 1 lists the information on which features aredefined in the various configurations of the threelanguage parsers.
The actual features are combi-nations of the information present in the table.
Theset varies depending on the language and whetherits GNPPA or E-GNPPA approach.While training, no features are defined onwhether a node is unconnected (present in un-Conn) or not as this information isn?t availableduring testing.4 Hindi Projected Dependency TreebankWe conducted experiments on English-Hindi par-allel data by transferring syntactic informationfrom English to Hindi to build a projected depen-dency treebank for Hindi.The TIDES English-Hindi parallel data con-taining 45,000 sentences was used for this pur-pose 1 (Venkatapathy, 2008).
Word alignmentsfor these sentences were obtained using the widelyused GIZA++ toolkit in grow-diag-final-and mode(Och and Ney, 2003).
Since Hindi is a morpho-logically rich language, root words were used in-stead of the word forms.
A bidirectional EnglishPOS tagger (Shen et al, 2007) was used to POStag the source sentences and the parses were ob-tained using the first order MST parser (McDon-ald et al, 2005) trained on dependencies extractedfrom Penn treebank using the head rules of Ya-mada and Matsumoto (2003).
A CRF based HindiPOS tagger (PVS.
and Gali, 2007) was used toPOS tag the target sentences.English and Hindi being morphologically andsyntactically divergent makes the word alignmentand dependency projection a challenging task.The source dependencies are projected using anapproach similar to (Hwa et al, 2005).
Whilethey use post-projection transformations on theprojected parse to account for annotation differ-ences, we use pre-projection transformations onthe source parse.
The projection algorithm pro-1The original data had 50,000 parallel sentences.
It waslater refined by IIIT-Hyderabad to remove repetitions andother trivial errors.
The corpus is still noisy with typographi-cal errors, mismatched sentences and unfaithful translations.1602duces acyclic parses which could be unconnectedand non-projective.4.1 Annotation Differences in Hindi andEnglishBefore projecting the source parses onto the tar-get sentence, the parses are transformed to reflectthe annotation scheme differences in English andHindi.
While English dependency parses reflectthe PTB annotation style (Marcus et al, 1994),we project them to Hindi to reflect the annotationscheme described in (Begum et al, 2008).
Thedifferences in the annotation schemes are with re-spect to three phenomena: a) head of a verb groupcontaining auxiliary and main verbs, b) preposi-tions in a prepositional phrase (PP) and c) coordi-nation structures.In the English parses, the auxiliary verb is thehead of the main verb while in Hindi, the mainverb is the head of the auxiliary in the verb group.For example, in the Hindi parse in Figure 1,dikhataa is the head of the auxiliary verb hai.The prepositions in English are realized as post-positions in Hindi.
While prepositions are theheads in a preposition phrase, post-positions arethe modifiers of the preceding nouns in Hindi.
Inpahaada para (on the hill), hill is the headof para.
In coordination structures, while En-glish differentiates between how NP coordinationand VP coordination structures behave, Hindi an-notation scheme is consistent in its handling.
Left-most verb is the head of a VP coordination struc-ture in English whereas the rightmost noun is thehead in case of NP coordination.
In Hindi, the con-junct is the head of the two verbs/nouns in the co-ordination structure.These three cases are identified in the sourcetree and appropriate transformations are made tothe source parse itself before projecting the rela-tions using word alignments.5 ExperimentsWe carried out all our experiments on paral-lel corpora belonging to English-Hindi, English-Bulgarian and English-Spanish language pairs.While the Hindi projected treebank was obtainedusing the method described in section 4, Bulgar-ian and Spanish projected datasets were obtainedusing the approach in (Ganchev et al, 2009).
Thedatasets of Bulgarian and Spanish that contributedto the best accuracies for Ganchev et al (2009)Statistic Hindi Bulgarian SpanishN(Words) 226852 71986 133124N(Parent==-1) 44607 30268 54815P(Parent==-1) 19.7 42.0 41.1N(Full trees) 593 1299 327N(GNPPA) 30063 10850 19622P(GNPPA) 16.4 26.0 25.0N(E-GNPPA) 35389 12281 24577P(E-GNPPA) 19.3 29.4 30.0Table 2: Statistics of the Hindi, Bulgarian and Spanishprojected treebanks used for experiments.
Each of them has10,000 randomly picked parses.
N(X) denotes number of Xand P(X) denotes percentage of X. N(Words) is the numberof words.
N(Parents==-1) is the number of words without aparent.
N(Full trees) is the number of parses which are fullyconnected.
N(GNPPA) is the number of relations learnt byGNPPA parser and N(E-GNPPA) is the number of relationslearnt by E-GNPPA parser.
Note that P(GNPPA) is calculatedas N(GNPPA)/(N(Words) - N(Parents==-1)).were used in our work (7 rules dataset for Bulgar-ian and 3 rules dataset for Spanish).
The Hindi,Bulgarian and Spanish projected dependency tree-banks have 44760, 39516 and 76958 sentences re-spectively.
Since we don?t have confidence scoresfor the projections on the sentences, we picked10,000 sentences randomly in each of the threedatasets for training the parsers2.
Other methodsof choosing the 10K sentences such as those withthe max.
no.
of relations, those with least no.
ofunconnected words, those with max.
no.
of con-tiguous partial trees that can be learned by GNPPAparser etc.
were tried out.
Among all these, ran-dom selection was consistent and yielded the bestresults.
The errors introduced in the projectedparses by errors in word alignment, source parserand projection are not consistent enough to be ex-ploited to select the better parses from the entireprojected data.Table 2 gives an account of the randomly cho-sen 10k sentences in terms of the number of words,words without parents etc.
Around 40% of thewords spread over 88% of sentences in Bulgarianand 97% of sentences in Spanish have no parents.Traditional dependency parsers which only trainfrom fully connected trees would not be able tolearn from these sentences.
P(GNPPA) is the per-centage of relations in the data that are learned bythe GNPPA parser satisfying the contiguous par-tial tree constraint and P(E-GNPPA) is the per-2Exactly 10K sentences were selected in order to compareour results with those of (Ganchev et al, 2009).1603ParserHindi Bulgarian SpanishPunct NoPunct Punct NoPunct Punct NoPunctBaseline 78.70 77.39 51.85 55.15 41.60 45.61GNPPA 80.03* 78.81* 77.03* 79.06* 65.49* 68.70*E-GNPPA 81.10*?
79.94*?
78.93*?
80.11*?
67.69*?
70.90*?Table 3: UAS for Hindi, Bulgarian and Spanish with the baseline, GNPPA and E-GNPPA parsers trainedon 10k parses selected randomly.
Punct indicates evaluation with punctuation whereas NoPunct indicateswithout punctuation.
* next to an accuracy denotes statistically significant (McNemar?s and p < 0.05)improvement over the baseline.
?
denotes significance over GNPPAcentage that satisfies the partially contiguous con-straint.
E-GNPPA parser learns around 2-5% moreno.
of relations than GNPPA due to the relaxationin the constraints.The Hindi test data that was released as part ofthe ICON-2010 Shared Task (Husain et al, 2010)was used for evaluation.
For Bulgarian and Span-ish, we used the same test data that was used inthe work of Ganchev et al (2009).
These testdatasets had sentences from the training section ofthe CoNLL Shared Task (Nivre et al, 2007) thathad lengths less than or equal to 10.
All the testdatasets have gold POS tags.A baseline parser was built to compare learningfrom partial parses with learning from fully con-nected parses.
Full parses are constructed frompartial parses in the projected data by randomlyassigning parents to unconnected parents, similarto the work in (Hwa et al, 2005).
The uncon-nected words in the parse are selected randomlyone by one and are assigned parents randomly tocomplete the parse.
This process is repeated for allthe sentences in the three language datasets.
Theparser is then trained with the GNPPA algorithmon these fully connected parses to be used as thebaseline.Table 3 lists the accuracies of the baseline,GNPPA and E-GNPPA parsers.
The accuraciesare unlabeled attachment scores (UAS): the per-centage of words with the correct head.
Table4 compares our accuracies with those reported in(Ganchev et al, 2009) for Bulgarian and Spanish.5.1 DiscussionThe baseline reported in (Ganchev et al, 2009)significantly outperforms our baseline (see Table4) due to the different baselines used in both theworks.
In our work, while creating the data forthe baseline by assigning random parents to un-connected words, acyclicity and projectivity con-Parser Bulgarian SpanishGanchev-Baseline 72.6 69.0Baseline 55.15 45.61Ganchev-Discriminative 78.3 72.3GNPPA 79.06 68.70E-GNPPA 80.11 70.90Table 4: Comparison of baseline, GNPPA and E-GNPPA with baseline and discriminative modelfrom (Ganchev et al, 2009) for Bulgarian andSpanish.
Evaluation didn?t include punctuation.straints are not enforced.
Ganchev et al (2009)?sbaseline is similar to the first iteration of their dis-criminative model and hence performs better thanours.
Our Bulgarian E-GNPPA parser achieved a1.8% gain over theirs while the Spanish results arelower.
Though their training data size is also 10K,the training data is different in both our works dueto the difference in the method of choosing 10Ksentences from the large projected treebanks.The GNPPA accuracies (see table 3) for all thethree languages are significant improvements overthe baseline accuracies.
This shows that learningfrom partial parses is effective when compared toimposing the connected constraint on the partiallyprojected dependency parse.
Even while project-ing source dependencies during data creation, itis better to project high confidence relations thanlook to project more relations and thereby intro-duce noise.The E-GNPPA which also learns from partiallycontiguous partial parses achieved statistically sig-nificant gains for all the three languages.
Thegains across languages is due to the fact that inthe 10K data that was used for training, E-GNPPAparser could learn 2 ?
5% more relations overGNPPA (see Table 2).Figure 6 shows the accuracies of baseline and E-1604304050607080  012345678910Unlabeled AccuracyThousands ofsentencesBulgarian Hindi Spanishhn-baselinebg-baselinees-baselineFigure 6: Accuracies (without punctuation) w.r.tvarying training data sizes for baseline and E-GNPPA parsers.GNPPA parser for the three languages when train-ing data size is varied.
The parsers peak early withless than 1000 sentences and make small gainswith the addition of more data.6 ConclusionWe presented a non-directional parsing algorithmthat can learn from partial parses using syntac-tic and contextual information as features.
AHindi projected dependency treebank was devel-oped from English-Hindi bilingual data and ex-periments were conducted for three languagesHindi, Bulgarian and Spanish.
Statistically sig-nificant improvements were achieved by our par-tial parsers over the baseline system.
The partialparsing algorithms presented in this paper are notspecific to bitext projections and can be used forlearning from partial parses in any setting.ReferencesR.
Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai,and R. Sangal.
2008.
Dependency annotationscheme for indian languages.
In In Proceedings ofThe Third International Joint Conference on NaturalLanguage Processing (IJCNLP), Hyderabad, India.Michael John Collins.
1999.
Head-driven statisticalmodels for natural language parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA, USA.AAI9926110.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: theory and experi-ments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing - Volume 10, EMNLP?02, pages 1?8, Morristown, NJ, USA.
Associationfor Computational Linguistics.Jason M. Eisner.
1996.
Three new probabilistic mod-els for dependency parsing: an exploration.
In Pro-ceedings of the 16th conference on Computationallinguistics - Volume 1, pages 340?345, Morristown,NJ, USA.
Association for Computational Linguis-tics.Kuzman Ganchev, Jennifer Gillenwater, and BenTaskar.
2009.
Dependency grammar induction viabitext projection constraints.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 1 - Volume 1, ACL-IJCNLP ?09, pages 369?377, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 742?750, Morristown, NJ,USA.
Association for Computational Linguistics.Samar Husain, Prashanth Mannem, Bharath Ambati,and Phani Gadde.
2010.
Icon 2010 tools contest onindian language dependency parsing.
In Proceed-ings of ICON 2010 NLP Tools Contest.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrappingparsers via syntactic projection across parallel texts.Nat.
Lang.
Eng., 11:311?325, September.Wenbin Jiang and Qun Liu.
2010.
Dependency parsingand projection based on word-pair classification.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, ACL ?10,pages 12?20, Morristown, NJ, USA.
Association forComputational Linguistics.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT summit, volume 5.Citeseer.Marco Kuhlmann and Joakim Nivre.
2006.
Mildlynon-projective dependency structures.
In Proceed-ings of the COLING/ACL on Main conference postersessions, pages 507?514, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Mitchell P. Marcus, Beatrice Santorini, and Mary A.Marcinkiewicz.
1994.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19(2):313?330.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of the Annual Meeting of the Associa-tion for Computational Linguistics (ACL).1605Jens Nilsson and Joakim Nivre.
2008.
Malteval:an evaluation and visualization tool for dependencyparsing.
In Proceedings of the Sixth InternationalLanguage Resources and Evaluation (LREC?08),Marrakech, Morocco, may.
European LanguageResources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proceedings of the CoNLLShared Task Session of EMNLP-CoNLL 2007, pages915?932, Prague, Czech Republic.
Association forComputational Linguistics.Joakim Nivre.
2003.
An Efficient Algorithm for Pro-jective Dependency Parsing.
In Eighth InternationalWorkshop on Parsing Technologies, Nancy, France.Joakim Nivre.
2009.
Non-projective dependency pars-ing in expected linear time.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages351?359, Suntec, Singapore, August.
Associationfor Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Avinesh PVS.
and Karthik Gali.
2007.
Part-Of-SpeechTagging and Chunking using Conditional RandomFields and Transformation-Based Learning.
In Pro-ceedings of the IJCAI and the Workshop On ShallowParsing for South Asian Languages (SPSAL), pages21?24.Roi Reichart and Ari Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisti-cal parsers trained on small datasets.
In Proceed-ings of the 45th Annual Meeting of the Associa-tion of Computational Linguistics, pages 616?623,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Libin Shen and Aravind Joshi.
2008.
LTAG depen-dency parsing with bidirectional incremental con-struction.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 495?504, Honolulu, Hawaii, October.
As-sociation for Computational Linguistics.L.
Shen, G. Satta, and A. Joshi.
2007.
Guided learn-ing for bidirectional sequence classification.
In Pro-ceedings of the 45th Annual Meeting of the Associa-tion for Computational Linguistics (ACL).Mark Steedman, Miles Osborne, Anoop Sarkar,Stephen Clark, Rebecca Hwa, Julia Hockenmaier,Paul Ruhlen, Steven Baker, and Jeremiah Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In Proceedings of the tenth conference onEuropean chapter of the Association for Computa-tional Linguistics - Volume 1, EACL ?03, pages 331?338, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Jrg Tiedemann.
2002.
MatsLex - a multilingual lex-ical database for machine translation.
In Proceed-ings of the 3rd International Conference on Lan-guage Resources and Evaluation (LREC?2002), vol-ume VI, pages 1909?1912, Las Palmas de Gran Ca-naria, Spain, 29-31 May.Sriram Venkatapathy.
2008.
Nlp tools contest - 2008:Summary.
In Proceedings of ICON 2008 NLP ToolsContest.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical Dependency Analysis with Support Vector Ma-chines.
In In Proceedings of IWPT, pages 195?206.David Yarowsky, Grace Ngai, and Richard Wicen-towski.
2001.
Inducing multilingual text analysistools via robust projection across aligned corpora.In Proceedings of the first international conferenceon Human language technology research, HLT ?01,pages 1?8, Morristown, NJ, USA.
Association forComputational Linguistics.1606
