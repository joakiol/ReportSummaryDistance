Proceedings of the 10th International Workshop on Finite State Methods and Natural Language Processing, pages 45?49,Donostia?San Sebastia?n, July 23?25, 2012. c?2012 Association for Computational LinguisticsWFST-based Grapheme-to-Phoneme Conversion: Open Source Tools forAlignment, Model-Building and DecodingJosef R. Novak, Nobuaki Minematsu, Keikichi HiroseGraduate School of Information Science and TechnologyThe University of Tokyo, Japan{novakj,mine,hirose}@gavo.t.u-tokyo.ac.jpAbstractThis paper introduces a new open source,WFST-based toolkit for Grapheme-to-Phoneme conversion.
The toolkit is efficient,accurate and currently supports a range offeatures including EM sequence alignmentand several decoding techniques novel inthe context of G2P.
Experimental resultsshow that a combination RNNLM systemoutperforms all previous reported results onseveral standard G2P test sets.
Preliminaryexperiments applying Lattice MinimumBayes-Risk decoding to G2P conversion arealso provided.
The toolkit is implementedusing OpenFst.1 IntroductionGrapheme-to-Phoneme (G2P) conversion is an im-portant problem related to Natural Language Pro-cessing, Speech Recognition and Spoken DialogSystems development.
The primary goal of G2Pconversion is to accurately predict the pronunciationof a novel input word given only the spelling.
Forexample, we would like to be able to predict,PHOENIX ?
/f i n I k s/given only the input spelling and a G2P model or setof rules.
This problem is straightforward for somelanguages like Spanish or Italian, where pronuncia-tion rules are consistent.
For languages like Englishand French however, inconsistent conventions makethe problem much more challenging.In this paper we present a fully data-driven,state-of-the-art, open-source toolkit for G2P conver-sion, Phonetisaurus [1].
It includes a novel mod-ified Expectation-Maximization (EM)-driven G2Psequence alignment algorithm, support for joint-sequence language models, and several decoding so-lutions.
The paper also provides preliminary in-vestigations of the applicability of Lattice Mini-mum Bayes-Risk (LMBR) decoding [2; 3] and N-best rescoring with a Recurrent Neural NetworkLanguage Model (RNNLM) [4; 5] to G2P con-version.
The Weighted Finite-State Transducer(WFST) framework is used throughout, and the opensource implementation relies on OpenFst [6].
Ex-perimental results are provided illustrating the speedand accuracy of the proposed system.The remainder of the paper is structured as fol-lows.
Section 2 provides background, Section 3 out-lines the alignment approach, Section 4 describesthe joint-sequence LM.
Section 5 describes decod-ing approaches.
Section 6 discusses preliminary ex-periments, Section 7 provides simple usage com-mands and Section 8 concludes the paper.2 G2P problem outlineGrapheme-to-Phoneme conversion has been a pop-ular research topic for many years.
Many differ-ent approaches have been proposed, but perhaps themost popular is the joint-sequence model [6].
Mostjoint-sequence modeling techniques focus on pro-ducing an initial alignment between correspondinggrapheme and phoneme sequences, and then mod-eling the aligned dictionary as a series of joint to-kens.
The gold standard in this area is the EM-driven joint-sequence modeling approach describedin [6] that simultaneously infers both alignments andsubsequence chunks.
Due to space constraints thereader is referred to [6] for a detailed background ofprevious research.The G2P conversion problem is typically bro-ken down into several sub-problems: (1) Sequencealignment, (2) Model training and, (3) Decoding.The goal of (1) is to align the grapheme andphoneme sequence pairs in a training dictionary.The goal of (2) is to produce a model able to gen-erate new pronunciations for novel words, and the45goal of (3) is to find the most likely pronunciationgiven the model.3 AlignmentThe proposed toolkit implements a modified WFST-based version of the EM-driven multiple-to-multiplealignment algorithm proposed in [7] and elaboratedin [8].
This algorithm is capable of learning naturalG-P relationships like igh?/AY/ which were notpossible with previous 1-to-1 algorithms like [9].The proposed alignment algorithm includes threemodifications to [7]: (1) A constraint is imposedsuch that only m-to-one and one-to-m arcs areconsidered during training.
(2) During initializationa joint alignment lattice is constructed for each in-put entry, and any unconnected arcs are deleted.
(3)All arcs, including deletions and insertions are ini-tialized to and constrained to maintain a non-zeroweight.These minor modifications appear to result in asmall but consistent improvement in terms of WordAccuracy (WA) on G2P tasks.
The Expectation andMaximization steps for the EM training procedureare outlined in Algorithms 2, 3.
The EM algorithmAlgorithm 1: EM-driven M2One/One2MInput: xT , yV , mX , mY , dX , dYOutput: ?, AlignedLattices1 foreach sequence pair (xT , yV ) do2 InitFSA(xT , yV , mX , mY , dX , dY )3 foreach sequence pair (xT , yV ) do4 Expectation(xT , yV , mX , mY , ?
)5 Maximization(?
)is initialized by generating an alignment FSA foreach dictionary entry, which encodes all valid G-Palignments, given max subsequence parameters sup-plied by the user.
Any unconnected arcs are deletedand all remaining arcs are initialized with a non-zeroweight.
In Algorithm 2 lines 2-3 compute the for-ward and backward probabilities.
Lines 4-8 com-pute the arc posteriors and update the current model.In Algorithm 3 lines 1-2 normalize the probabilitydistribution.
Lines 3-6 update the alignment latticearc weights with the new model.Algorithm 2: Expectation stepInput: AlignedLatticesOutput: ?, total1 foreach FSA alignment lattice F do2 ??
ShortestDistance(F )3 ?
?
ShortestDistance(FR)4 foreach state q ?
Q[F ] do5 foreach arc e ?
E[q] do6 v ?
((?[q]?w[e])??
[n[e]])?
[0];7 ?[i[e]]?
?[i[e]]?
v;8 total?
total ?
v;Algorithm 3: Maximization stepInput: ?, totalOutput: AlignedLattices1 foreach arc e in E[?]
do2 ?new[i[e]]?
w[e]/total; ?[i[e]]?
0;3 foreach FSA alignment lattice F do4 foreach state q ?
Q[F ] do5 foreach arc e ?
E[q] do6 w[e]?
?new[i[e]];4 Joint Sequence N-gram modelThe pronunciation model implemented by thetoolkit is a straightforward joint N-gram model.
Thetraining corpus is constructed by extracting the bestalignment for each entry, e.g.
:a}x b}b a}@ c|k}ka}x b}b a}@ f}f t}tThe training procedure is then, (1) Convert alignedsequence pairs to sequences of aligned joint labelpairs, (g1:p1, g2:p2, ..., gn:pn); (2) Train an N-grammodel from (1); (3) Convert the N-gram model toa WFST.
Step (3) may be performed with any lan-guage modeling toolkit.
In this paper mitlm [11] isutilized.5 DecodingThe proposed toolkit provides varying support forthree different decoding schemes.
The default de-coder provided by the distribution simply extractsthe shortest path through the phoneme lattice createdvia composition with the input word,Hbest = ShortestPath(Projecto(w ?M)) (1)46whereHbest refers to the lowest cost path, Projectorefers to projecting the output labels, w refers to theinput word, M refers to the G2P model, and ?
indi-cates composition.5.1 RNNLM N-best rescoringRecurrent Neural Network Language Models haverecently enjoyed a resurgence in popularity in thecontext of ASR applications [4].
In another re-cent publication we investigated the applicabilityof this approach to G2P conversion with joint se-quence models by providing support for the rnnlmtoolkit [5].
The training corpus for the G2P LMis a corpus of joint sequences, thus it can be usedwithout modification to train a parallel RNNLM.
N-best reranking is then accomplished with the pro-posed toolkit by causing the decoder to output theN-best joint G-P sequences, and employing rnnlmto rerank the the N-best joint sequences,HNbest =NShortestPaths(w ?M)Hbest =Projecto(Rescorernn(HNbest)).
(2)In practice the rnnlm models require considerabletuning, and somewhat more time to train, but pro-vide a consistent WA boost.
For further details onalgorithm as well as tuning for G2P see [4; 10].5.2 Lattice Minimum Bayes-Risk decoding forG2PIn [2] the authors note that the aim of MBR decod-ing is to find the hypothesis that has the ?least ex-pected loss under the model?.
MBR decoding wassuccessfully applied to Statistical Machine Trans-lation (SMT) lattices in [2], and significantly im-proved in [3].
Noting the similarities between G2Pconversion and SMT, we have begun work imple-menting an integrated LMBR decoder for the pro-posed toolkit.Our approach closely follows that describedin [3], and the algorithm implementation is sum-marized in Algorithm 4.
The inputs are the fullphoneme lattice that results from composing the in-put word with the G2P model and projecting outputlabels, an exponential scale factor ?, and N-gramprecision factors ?0?N .
The ?n are computed us-ing a linear corpus BLEU [2] N-gram precision p,and a match ratio r using the following equations,?0 = ?1/T ; ?n = 1/(NTprn?1).
T is a constantAlgorithm 4: G2P Lattice MBR-DecodeInput: E ?
Projecto(w ?M), ?, ?0?n1 E ?ScaleLattice(??
E)2 NN ?ExtractN-grams(E)3 for n?
1 to N do4 ?n ?MakeMapper(Nn)5 ?Rn ?MakePathCounter(Nn)6 Un ?
Opt((E ?
?n) ?
?Rn )7 ?n = ?n8 for state q ?
Q[?n] do9 for arc e ?
E[q] do10 w[e]?
?n ?
U(o[e])11 P ?
Projectinput(E?0 ?
?1)12 for n?
2 to N do13 P ?
Projectinput(P ?
?n)14 Hbest = ShortestPath(P)which does not affect the MBR decision [2].
Line1 applies ?
to the raw lattice.
In effect this controlshow much we trust the raw lattice weights.
Afterapplying ?, E is normalized by pushing weights tothe final state and removing any final weights.
Inline 2 all unique N-grams up to order N are ex-tracted from the lattice.
Lines 4-10 create, for eachorder, a context-dependency FST (?n) and a spe-cial path-posterior counting WFST (?Rn ), which arethen used to compute N-gram posteriors (Un), andfinally to create a decoder WFST (?n).
The fullMBR decoder is then computed by first making anunweighted copy of E , applying ?0 uniformly to allarcs, and iteratively composing and input-projectingwith each ?n.
The MBR hypothesis is then the bestpath through the result P .
See [2; 3] for furtherdetails.6 Experimental resultsExperimental evaluations were conducted utilizingthree standard G2P test sets.
These included repli-cations of the NetTalk, CMUdict, and OALD En-glish language dictionary evaluations described indetail in [6].
Results comparing various configu-ration of the proposed toolkit to the joint sequencemodel Sequitur [6] and an alternative discriminativetraining toolkit direcTL+ [8] are described in Ta-ble 1.
Here m2m-P indicates the proposed toolkitusing the alignment algorithm from [7], m2m-fst-P47System NT15k CMUdict OALDSequitur [6] 66.20 75.47 82.51direcTL+ [8] ?
75.52 83.32m2m-P 66.39 75.08 81.20m2m-fst-P 66.41 75.25 81.86rnnlm-P 67.77 75.56 83.52Table 1: Comparison of G2P WA(%) for previous sys-tems and variations of the proposed toolkit.indicates the alternative FST-based alignment algo-rithm, and rnnlm-P indicates the use of RNNLM N-best reranking.The results show that the improved alignment al-gorithm contributes a small but consistent improve-ment to WA, while RNNLM reranking contributes afurther small but significant boost to WA which pro-duces state-of-the-art results on all three test sets.The WA gains are interesting, however a majorplus point for the toolkit is speed.
Table 2 comparestraining times for the proposed toolkit with previ-ously reported results.
The m2m-fst-P for system forSystem NETtalk-15k CMUdictSequitur [6] Hours DaysdirecTL+ [8] Hours Daysm2m-P 2m56s 21m58sm2m-fst-P 1m43s 13m06srnnlm-P 20m 2hTable 2: Training times for the smallest (15k entries) andlargest (112k entries) training sets.CMUdict performs %0.27 worse than the state-of-the-art, but requires just a tiny fraction of the train-ing time.
This turn-around time may be very impor-tant for rapid system development.
Finally, Figure.
1plots WA versus decoding time for m2m-fst-P on thelargest test set, further illustrating the speed of thedecoder, and the impact of using larger models.Preliminary experiments with the LMBR decoderwere also carried out using the smaller NT15kdataset.
The ?n values were computed using p, r,and T from [2] while ?
was tuned to 0.6.
Re-sults are described in Table 3.
The system matchedthe basic WA for N=6, and achieved a small im-provement in PA over m2m-fst-P (%91.80 versus%91.82).
Tuning the loss function for the G2P taskshould improve performance.!
"#!$#$"#$$#%"#%$#&"#&$#'"#()# (!# (%# ('# )"# ))# )!# )%# )'#!"#$%&''(#)'*%+,-%./'"$012%34/%+5/'-%678$0'9%:;<=::;<%%!
"#$%&''(#)'*%>5?%@04/%A"#%BC;D:;%*+#,-#./01#Figure 1: Decoding speed vs. WA plot for various N-gram orders for the CMUdict 12k/112k test/train set.Times averaged over 5 run using ctime.NT15k N=1 N=2 N=3 N=4 N=5 N=6WA 28.88 65.48 66.03 66.41 66.37 66.50PA 83.17 91.74 91.79 91.87 91.82 91.82Table 3: LMBR decoding Word Accuracy (WA) andPhoneme Accuracy (PA) for order N=1-6.7 Toolkit distribution and usageThe preceding sections introduced various theoreti-cal aspects of the toolkit as well as preliminary ex-perimental results.
The current section provides sev-eral introductory usage commands.The toolkit is open source and released underthe liberal BSD license.
It is available for down-load from [1], which also includes detailed com-pilation instructions, tutorial information and addi-tional examples.
The examples that follow utilizethe NETTalk dictionary.Align a dictionary:$ phonetisaurus-align --input=test.dic \--ofile=test.corpusTrain a 7-gram model with mitlm:$ estimate-ngram -o 7 -t test.corpus \-wl test.arpaConvert the model to a WFSA$ phonetisaurus-arpa2fst --input=test.arpa \--prefix=testApply the default decoder$ phonetisaurus-g2p --model=test.fst \--input=abbreviate --nbest=3 --wordsabbreviate 25.66 @ b r i v i e t48abbreviate 28.20 @ b i v i e tabbreviate 29.03 x b b r i v i e tApply the LMBR decoder$ phonetisaurus-g2p --model=test.fst \--input=abbreviate --nbest=3 --words \--mbr --order=7abbreviate 1.50 @ b r i v i e tabbreviate 2.62 x b r i v i e tabbreviate 2.81 a b r i v i e t8 Conclusion and Future workThis work introduced a new Open Source WFST-driven G2P conversion toolkit which is both highlyaccurate as well as efficient to train and test.
It incor-porates a novel modified alignment algorithm.
Toour knowledge the RNNLM N-best reranking andLMBR decoding are also novel applications in thecontext of G2P.Both the RNNLM N-best reranking and LMBRdecoding are promising but further work is requiredto improve usability and performance.
In particularRNNLM training requires considerable tuning, andwe would like to automate this process.
The pro-visional LMBR decoder achieved a small improve-ment but further work will be needed to tune theloss function.
Several known optimizations are alsoplanned to speed up the LMBR decoder.Nevertheless the current release of the toolkit pro-vides several novel G2P solutions, achieves state-of-the-art WA on several test sets and is efficient forboth training and decoding.ReferencesJ.
Novak, et al [1].http://code.google.com/p/phonetisaurusR.
Tromble and S. Kumar and F. Och and W.
Macherey.[2].
Lattice Minimum Bayes-Risk Decoding for Sta-tistical Machine Translation, Proc.
EMNLP 2007, pp.620-629.G.
Blackwood and A. Gispert and W. Byrne.
[3].
Effi-cient path counting transducers for minimum bayes-risk decoding of statistical machine translation lat-tices, Proc.
ACL 2010, pp.
27-32.T.
Mikolov and M. Karafiat and L. Burget and J.
C?er-nock?
and S. Khundanpur.
[4].
Recurrent Neural Net-work based Language Model, Proc.
InterSpeech, 2010.T.
Mikolov and S. Kombrink and D. Anoop and L. Burgetand J.
C?ernock?.
[5].
RNNLM - Recurrent Neural Net-work Language Modeling Toolkit, ASRU 2011, demosession.C.
Allauzen and M. Riley and J. Schalkwyk and W. Skutand M. Mohri.
[6].
OpenFST: A General and Effi-cient Weighted Finite-State Transducer Library, Proc.CIAA 2007, pp.
11-23.M.
Bisani and H. Ney.
[6].
Joint-sequence models forgrapheme-to-phoneme conversion, Speech Communi-cation 50, 2008, pp.
434-451.S.
Jiampojamarn and G. Kondrak and T. Sherif.
[7].
Ap-plying Many-to-Many Alignments and Hidden MarkovModels to Letter-to-Phoneme Conversion, NAACLHLT 2007, pp.
372-379.S.
Jiampojamarn and G. Kondrak.
[8].
Letter-to-Phoneme Alignment: an Exploration, Proc.
ACL2010, pp.
780-788.E.
Ristad and P. Yianilos.
[9].
Learning String Edit Dis-tance, IEEE Trans.
PRMI 1998, pp.
522-532.J.
Novak and P. Dixon and N. Minematsu and K. Hiroseand C. Hori and H. Kashioka.
[10].
Improving WFST-based G2P Conversion with Alignment Constraintsand RNNLM N-best Rescoring, Interspeech 2012 (Ac-cepted).B.
Hsu and J.
Glass.
[11].
Iterative Language Model Es-timation: Efficient Data Structure & Algorithms, Proc.Interspeech 2008.49
