Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1526?1534,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsDoes String-Based Neural MT Learn Source Syntax?Xing Shi, Inkit Padhi, and Kevin KnightInformation Sciences Institute & Computer Science DepartmentUniversity of Southern Californiaxingshi@isi.edu, ipadhi@usc.edu, knight@isi.eduAbstractWe investigate whether a neural, encoder-decoder translation system learns syntactic in-formation on the source side as a by-productof training.
We propose two methods to de-tect whether the encoder has learned local andglobal source syntax.
A fine-grained analy-sis of the syntactic structure learned by theencoder reveals which kinds of syntax arelearned and which are missing.1 IntroductionThe sequence to sequence model (seq2seq) has beensuccessfully applied to neural machine translation(NMT) (Sutskever et al, 2014; Cho et al, 2014)and can match or surpass MT state-of-art.
Non-neural machine translation systems consist chieflyof phrase-based systems (Koehn et al, 2003) andsyntax-based systems (Galley et al, 2004; Galleyet al, 2006; DeNeefe et al, 2007; Liu et al, 2011;Cowan et al, 2006), the latter of which adds syntac-tic information to source side (tree-to-string), targetside (string-to-tree) or both sides (tree-to-tree).
Asthe seq2seq model first encodes the source sentenceinto a high-dimensional vector, then decodes into atarget sentence, it is hard to understand and interpretwhat is going on inside such a procedure.
Consider-ing the evolution of non-neural translation systems,it is natural to ask:1.
Does the encoder learn syntactic informationabout the source sentence?2.
What kind of syntactic information is learned,and how much?3.
Is it useful to augment the encoder with addi-tional syntactic information?In this work, we focus on the first two questionsand propose two methods:?
We create various syntactic labels of the sourcesentence and try to predict these syntactic la-bels with logistic regression, using the learnedsentence encoding vectors (for sentence-levellabels) or learned word-by-word hidden vectors(for word-level label).
We find that the encodercaptures both global and local syntactic infor-mation of the source sentence, and different in-formation tends to be stored at different layers.?
We extract the whole constituency tree ofsource sentence from the NMT encoding vec-tors using a retrained linearized-tree decoder.
Adeep analysis on these parse trees indicates thatmuch syntactic information is learned, whilevarious types of syntactic information are stillmissing.2 ExampleAs a simple example, we train an English-FrenchNMT system on 110M tokens of bilingual data (En-glish side).
We then take 10K separate English sen-tences and label their voice as active or passive.
Weuse the learned NMT encoder to convert these sen-tences into 10k corresponding 1000-dimension en-coding vectors.
We use 9000 sentences to train alogistic regression model to predict voice using theencoding cell states, and test on the other 1000 sen-tences.
We achieve 92.8% accuracy (Table 2), farabove the majority class baseline (82.8%).
Thismeans that in reducing the source sentence to a1526Model AccuracyMajority Class 82.8English to French (E2F) 92.8English to English (E2E) 82.7Table 1: Voice (active/passive) prediction accuracy using theencoding vector of an NMT system.
The majority class baselinealways chooses active.fixed-length vector, the NMT system has decided tostore the voice of English sentences in an easily ac-cessible way.When we carry out the same experiment on anEnglish-English (auto-encoder) system, we find thatEnglish voice information is no longer easily ac-cessed from the encoding vector.
We can only pre-dict it with 82.7% accuracy, no better than chance.Thus, in learning to reproduce input English sen-tences, the seq2seq model decides to use the fixed-length encoding vector for other purposes.3 Related workInterpreting Recurrent Neural Networks.
Themost popular method to visualize high-dimensionalvectors, such as word embeddings, is to project theminto two-dimensional space using t-SNE (van derMaaten and Hinton, 2008).
Very few works try tointerpret recurrent neural networks in NLP.
Karpa-thy et al (2016) use a character-level LSTM lan-guage model as a test-bed and find several activationcells that track long-distance dependencies, such asline lengths and quotes.
They also conduct an er-ror analysis of the predictions.
Li et al (2016) ex-plore the syntactic behavior of an RNN-based sen-timent analyzer, including the compositionality ofnegation, intensification, and concessive clauses, byplotting a 60-dimensional heat map of hidden unitvalues.
They also introduce a first-order derivativebased method to measure each unit?s contribution tothe final decision.Verifying syntactic/semantic properties.
Severalworks try to build a good distributional representa-tion of sentences or paragraph (Socher et al, 2013;Kalchbrenner et al, 2014; Kim, 2014; Zhao etal., 2015; Le and Mikolov, 2014; Kiros et al,2015).
They implicitly verify the claimed syntac-tic/semantic properties of learned representations byapplying them to downstream classification taskssuch as sentiment analysis, sentence classification,semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification, etc.Novel contributions of our work include:?
We locate a subset of activation cells that areresponsible for certain syntactic labels.
We ex-plore the concentration and layer distribution ofdifferent syntactic labels.?
We extract whole parse trees from NMT encod-ing vectors in order to analyze syntactic prop-erties directly and thoroughly.?
Our methods are suitable for large scale mod-els.
The models in this work are 2-layer 1000-dimensional LSTM seq2seq models.4 Datasets and modelsWe train two NMT models, English-French (E2F)and English-German (E2G).
To answer whetherthese translation models?
encoders to learn storesyntactic information, and how much, we employtwo benchmark models:?
An upper-bound model, in which the encoderlearns quite a lot of syntactic information.
Forthe upper bound, we train a neural parser thatlearns to ?translate?
an English sentence to itslinearized constitutional tree (E2P), followingVinyals et al (2015).?
An lower-bound model, in which the encoderlearns much less syntactic information.
Forthe lower bound, we train two sentence auto-encoders: one translates an English sentence toitself (E2E), while the other translates a per-muted English sentence to itself (PE2PE).
Wealready had an indication above (Section 2) thata copying model does not necessarily need toremember a sentence?s syntactic structure.Figure 1 shows sample inputs and outputs of theE2E, PE2PE, E2F, E2G, and E2P models.We use English-French and English-German datafrom WMT2014 (Bojar et al, 2014).
We take 4MEnglish sentences from the English-German data totrain E2E and PE2PE.
For the neural parser (E2P),we construct the training corpus following the recipeof Vinyals et al (2015).
We collect 162K trainingsentences from publicly available treebanks, includ-ing Sections 0-22 of the Wall Street Journal PennTreebank (Marcus et al, 1993), Ontonotes version 51527Model Target LanguageInputvocabularysizeOutputvocabularysizeTrain/Dev/TestCorpora Sizes(sentence pairs)BLEUE2E English 200K 40K 4M/3000/2737 89.11PE2PE Permuted English 200K 40K 4M/3000/2737 88.84E2F French 200K 40K 4M/6003/3003 24.59E2G German 200K 40K 4M/3000/2737 12.60E2P Linearizedconstituency tree 200K 121 8162K/1700/2416 n/aTable 2: Model settings and test-set BLEU-n4r1 scores (Papineni et al, 2002).Figure 1: Sample inputs and outputs of the E2E, PE2PE, E2F,E2G, and E2P models.
(Pradhan and Xue, 2009) and the English Web Tree-bank (Petrov and McDonald, 2012).
In addition tothese gold treebanks, we take 4M English sentencesfrom English-German data and 4M English sen-tences from English-French data, and we parse these8M sentences with the Charniak-Johnson parser1(Charniak and Johnson, 2005).
We call these 8,162Kpairs the CJ corpus.
We use WSJ Section 22 as ourdevelopment set and section 23 as the test set, wherewe obtain an F1-score of 89.6, competitive with thepreviously-published 90.5 (Table 4).Model Architecture.
For all experiments2,we use a two-layer encoder-decoder with longshort-term memory (LSTM) units (Hochreiter andSchmidhuber, 1997).
We use a minibatch of 128, ahidden state size of 1000, and a dropout rate of 0.2.1The CJ parser is here https://github.com/BLLIP/bllip-parser and we used the pretrained model ?WSJ+Gigaword-v2?.2We use the toolkit: https://github.com/isi-nlp/Zoph RNNParser WSJ 23F1-score# valid trees(out of 2416)CJ Parser 92.1 2416E2P 89.6 2362(Vinyals et al, 2015) 90.5 unkTable 3: Labeled F1-scores of different parsers on WSJ Section23.
The F1-score is calculated on valid trees only.For auto-encoders and translation models, we train8 epochs.
The learning rate is initially set as 0.35and starts to halve after 6 epochs.
For E2P model,we train 15 epochs.
The learning rate is initializedas 0.35 and starts to decay by 0.7 once the perplexityon a development set starts to increase.
All parame-ters are re-scaled when the global norm is larger than5.
All models are non-attentional, because we wantthe encoding vector to summarize the whole sourcesentence.
Table 4 shows the settings of each modeland reports the BLEU scores.5 Syntactic Label Prediction5.1 Experimental SetupIn this section, we test whether different seq2seqsystems learn to encode syntactic information aboutthe source (English) sentence.With 1000 hidden states, it is impractical to in-vestigate each unit one by one or draw a heat map ofthe whole vector.
Instead, we use the hidden statesto predict syntactic labels of source sentences via lo-gistic regression.
For multi-class prediction, we usea one-vs-rest mechanism.
Furthermore, to identifya subset of units responsible for certain syntactic la-bels, we use the recursive feature elimination (RFE)strategy: the logistic regression is first trained using1528Label Train TestNumberofclassesMostfrequentlabelVoice 9000 1000 2 ActiveTense 9000 1000 2 Non-pastTSS 9000 1000 20 NP-VPPOS 87366 9317 45 NNSPC 81292 8706 24 NPTable 4: Corpus statistics for five syntactic labels.Figure 2: The five syntactic labels for sentence ?This time , thefirms were ready?.all 1000 hidden states, after which we recursivelyprune those units whose weights?
absolute values aresmallest.We extract three sentence-level syntactic labels:1.
Voice: active or passive.2.
Tense: past or non-past.3.
TSS: Top level syntactic sequence of the con-stituent tree.
We use the most frequent 19 se-quences (?NP-VP?, ?PP-NP-VP?, etc.)
and la-bel the remainder as ?Other?.and two word-level syntactic labels:1.
POS: Part-of-speech tags for each word.2.
SPC: The smallest phrase constituent thatabove each word.Both voice and tense labels are generated usingrule-based systems based on the constituent tree ofthe sentence.Figure 2 provides examples of our five syntacticlabels.
When predicting these syntactic labels usingcorresponding cell states, we split the dataset intotraining and test sets.
Table 4 shows statistics of eachlabels.For a source sentence s,s = [w1, ..., wi, ..., wn]the two-layer encoder will generate an array of cellvectors c during encoding,c = [(c1,0, c1,1), ..., (ci,0, ci,1), ..., (cn,0, cn,1)]We extract a sentence-level syntactic label Ls, andpredict it using the encoding cell states that will befed into the decoder:Ls = g(cn,0) or Ls = g(cn,1)where g(?)
is the logistic regression.Similarly, for extracting word-level syntactic la-bels:Lw = [Lw1, ..., Lwi, ..., Lwn]we predict each label Lwi using the cell states im-mediately after encoding the word wi:Lwi = g(ci,0) or LWi = g(ci,1)5.2 Result AnalysisTest-set prediction accuracy is shown in Figure 3.For voice and tense, the prediction accuracy of twoauto-encoders is almost same as the accuracy of ma-jority class, indicating that their encoders do notlearn to record this information.
By contrast, boththe neural parser and the NMT systems achieve ap-proximately 95% accuracy.
When predicting thetop-level syntactic sequence (TSS) of the whole sen-tence, the Part-of-Speech tags (POS), and small-est phrase constituent (SPC) for each word, all fivemodels achieve an accuracy higher than that of ma-jority class, but there is still a large gap between theaccuracy of NMT systems and auto-encoders.
Theseobservations indicate that the NMT encoder learnssignificant sentence-level syntactic information?itcan distinguish voice and tense of the source sen-tence, and it knows the sentence?s structure to someextent.
At the word level, the NMT?s encoder alsotends to cluster together the words that have similarPOS and SPC labels.Different syntactic information tends to be storedat different layers in the NMT models.
For word-level syntactic labels, POS and SPC, the accuracyof the lower layer?s cell states (C0) is higher thanthat of the upper level (C1).
For the sentence-level1529E2P E2F E2G E2E PE2PE0.750.800.850.900.951.00AccuracyVoiceE2P E2F E2G E2E PE2PE0.700.750.800.850.900.951.00AccuracyTenseE2P E2F E2G E2E PE2PE0.50.60.70.80.91.0AccuracyTSSE2P E2F E2G E2E PE2PE0.20.40.60.81.0AccuracyPOSE2P E2F E2G E2E PE2PE0.50.60.70.80.91.0AccuracySPCMajority ClassC0 AllC0 Top10C1 AllC1 Top10Figure 3: Prediction accuracy of five syntactic labels on test.
Each syntactic label is predicted using both the lower-layer cell states(C0) and higher-layer cell states (C1).
For each cell state, we predict each syntactic label using all 1000 units (All), as well as thetop 10 units (Top10) selected by recursive feature elimination.
The horizontal blue line is the majority class accuracy.labels, especially tense, the accuracy of C1 is largerthan C0.
This suggests that the local features aresomehow preserved in the lower layer whereas moreglobal, abstract information tends to be stored in theupper layer.For two-classes labels, such as voice and tense,the accuracy gap between all units and top-10 unitsis small.
For other labels, where we use a one-versus-rest strategy, the gap between all units andtop-10 units is large.
However, when predictingPOS, the gap of neural parser (E2P) on the lowerlayer (C0) is much smaller.
This comparison in-dicates that a small subset of units explicitly takescharge of POS tags in the neural parser, whereas forNMT, the POS info is more distributed and implicit.There are no large differences between encodersof E2F and E2G regarding syntactic information.Figure 4: E2F and E2F2P share the same English encoder.When training E2F2P, we only update the parameters of lin-earized tree decoder, keeping the English encoder?s parametersfixed.15306 Extract Syntactic Trees from Encoder6.1 Experimental SetupWe now turn to whether NMT systems capturedeeper syntactic structure as a by-product of learn-ing to translate from English to another language.We do this by predicting full parse trees from the in-formation stored in encoding vectors.
Since this isa structured prediction problem, we can no longeruse logistic regression.
Instead, we extract a con-stituency parse tree from the encoding vector of amodel E2X by using a new neural parser E2X2Pwith the following steps:1.
Take the E2X encoder as the encoder of the newmodel E2X2P.2.
Initialize the E2X2P decoder parameters with auniform distribution.3.
Fine-tune the E2X2P decoder (while keepingits encoder parameters fixed), using the CJ cor-pus, the same corpus used to train E2P .Figure 4 shows how we construct model E2F2Pfrom model E2F.
For fine-tuning, we use the samedropout rate and learning rate updating configura-tion for E2P as described in Section 4.6.2 EvaluationWe train four new neural parsers using the encodersof the two auto-encoders and the two NMT modelsrespectively.
We use three tools to evaluate and ana-lyze:1.
The EVALB tool3 to calculate the labeledbracketing F1-score.2.
The zxx package4 to calculate Tree edit dis-tance (TED) (Zhang and Shasha, 1989).3.
The Berkeley Parser Analyser5 (Kummerfeld etal., 2012) to analyze parsing error types.The linearized parse trees generated by these neu-ral parsers are not always well-formed.
They can besplit into the following categories:?
Malformed trees: The linearized sequence cannot be converted back into a tree, due to miss-ing or mismatched brackets.?
Well-formed trees: The sequence can be con-verted back into a tree.
Tree edit distance canbe calculated on this category.3http://nlp.cs.nyu.edu/evalb/4https://github.com/timtadh/zhang-shasha5https://github.com/jkkummerfeld/berkeley-parser-analyser?
Wrong length trees: The number of treeleaves does not match the number ofsource-sentence tokens.?
Correct length trees: The number of treeleaves does match the number of source-sentence tokens.Before we move to results, we emphasize the fol-lowing points:First, compared to the linear classifier used in Sec-tion 5, the retrained decoder for predicting a lin-earized parse tree is a highly non-linear method.The syntactic prediction/parsing performance willincrease due to such non-linearity.
Thus, we donot make conclusions based only on absolute per-formance values, but also on a comparison againstthe designed baseline models.
An improvement overthe lower bound models indicates that the encoderlearns syntactic information, whereas a decline fromthe upper bound model shows that the encoder losescertain syntactic information.Second, the NMT?s encoder maps a plain Englishsentence into a high-dimensional vector, and ourgoal is to test whether the projected vectors forma more syntactically-related manifold in the high-dimensional space.
In practice, one could also pre-dict parse structure for the E2E in two steps: (1) useE2E?s decoder to recover the original English sen-tence, and (2) parse that sentence with the CJ parser.But in this way, the manifold structure in the high-dimensional space is destroyed during the mapping.6.2.1 Result AnalysisTable 5 reports perplexity on training and devel-opment sets, the labeled F1-score on WSJ Section23, and the Tree Edit Distance (TED) of various sys-tems.Tree Edit Distance (TED) calculates theminimum-cost sequence of node edit opera-tions (delete, insert, rename) between a gold treeand a test tree.
When decoding with beam size10, the four new neural parsers can generate well-formed trees for almost all the 2416 sentences in theWSJ section 23.
This makes TED a robust metricto evaluate the overall performance of each parser.Table 5 reports the average TED per sentence.
Treesextracted from E2E and PE2PE encoding vectors(via models E2E2P and PE2PE2P, respectively)get TED above 30, whereas the NMT systems get1531Model Perplexityon TrainPerplexityon WSJ 22Labeled F1on WSJ23# EVALB-trees(out of 2416)Average TEDper sentence# Well-formedtrees(out of 2416)PE2PE2P 1.83 1.92 46.64 818 34.43 2416E2E2P 1.69 1.77 59.35 796 31.25 2416E2G2P 1.39 1.41 80.34 974 17.11 2340E2F2P 1.36 1.38 79.27 1093 17.77 2415E2P 1.11 1.18 89.61 2362 11.50 2415Table 5: Perplexity, labeled F1-score, and Tree Edit Distance (TED) of various systems.
Labeled F1-scores are calculated onEVALB-trees only.
Tree edit distances are calculated on the well-formed trees only.
EVALB-trees are those whose number ofleaves match the number of words in the source sentence, and are otherwise accepted by standard Treebank evaluation software.approximately 17 TED.Among the well-formed trees, around half havea mismatch between number of leaves and numberof tokens in the source sentence.
The labeled F1-score is reported over the rest of the sentences only.Though biased, this still reflects the overall perfor-mance: we achieve around 80 F1 with NMT en-coding vectors, much higher than with the E2E andPE2PE encoding vectors (below 60).6.2.2 Fine-grained AnalysisBesides answering whether the NMT encoderslearn syntactic information, it is interesting to knowwhat kind of syntactic information is extracted andwhat is not.As Table 5 shows, different parsers generate dif-ferent numbers of trees that are acceptable to Tree-bank evaluation software (?EVALB-trees?
), havingthe correct number of leaves and so forth.
We se-lect the intersection set of different models?
EVALB-trees.
We get a total of 569 shared EVALB-trees.The average length of the corresponding sentence is12.54 and the longest sentence has 40 tokens.
Theaverage length of all 2416 sentences in WSJ section23 is 23.46, and the longest is 67.
As we do not ap-ply an attention model for these neural parsers, it isdifficult to handle longer sentences.
While the in-tersection set may be biased, it allows us to explorehow different encoders decide to capture syntax onshort sentences.Table 6 shows the labeled F1-scores and Part-of-Speech tagging accuracy on the intersection set.
TheNMT encoder extraction achieves around 86 per-cent tagging accuracy, far beyond that of the auto-encoder based parser.Model Labeled F1 POSTagging AccuracyPE2PE2P 58.67 54.32E2E2P 70.91 68.03E2G2P 85.36 85.30E2F2P 86.62 87.09E2P 93.76 96.00Table 6: Labeled F1-scores and POS tagging accuracy on theintersection set of EVALB-trees of different parsers.
There are569 trees in the intersection, and the average length of corre-sponding English sentence is 12.54.Besides the tagging accuracy, we also utilize theBerkeley Parser Analyzer (Kummerfeld et al, 2012)to gain a more linguistic understanding of predictedparses.
Like TED, the Berkeley Parser Analyzer isbased on tree transformation.
It repairs the parse treevia a sequence of sub-tree movements, node inser-tions and deletions.
During this process, multiplebracket errors are fixed, and it associates this groupof node errors with a linguistically meaningful errortype.The first column of Figure 5 shows the averagenumber of bracket errors per sentence for model E2Pon the intersection set.
For other models, we reportthe ratio of each model to model E2P.
Kummerfeldet al (2013) and Kummerfeld et al (2012) give de-scriptions of different error types.
The NMT-basedpredicted parses introduce around twice the brack-eting errors for the first 10 error types, whereas for?Sense Confusion?, they bring more than 16 timesbracket errors.
?Sense confusion?
is the case wherethe head word of a phrase receives the wrong POS,1532Sense ConfusionSingle Word PhraseDifferent labelNoun boundary errorNP InternalUnaryModifier AttachVerb taking wrong argumentsPP AttachVP AttachCo-ordination0.0570.1500.1370.0220.0530.1230.2050.0350.2420.0240.081E2P(Ave.
Bracket Err)16.582.742.522.202.171.981.461.441.441.361.14E2F2P(Ratio)17.773.312.423.201.832.252.032.751.271.271.05E2G2P(Ratio)23.775.015.003.103.173.211.693.501.824.551.78E2E2P(Ratio)32.195.125.265.103.583.711.822.442.275.640.22PE2PE2P(Ratio)Figure 5: For model E2P (the red bar), we show the average number of bracket errors per sentence due to the top 11 error types.For other models, we show the ratio of each model?s average number of bracket errors to that of model E2P .
Errors analyzed onthe intersection set.
The table is sorted based on the ratios of the E2F2P model.resulting in an attachment error.
Figure 6 shows anexample.Even though we can predict 86 percent of parts-of-speech correctly from NMT encoding vectors, theother 14 percent introduce quite a few attachmenterrors.
NMT sentence vectors encode a lot of syntax,but they still cannot grasp these subtle details.7 ConclusionWe investigate whether NMT systems learn source-language syntax as a by-product of training on stringpairs.
We find that both local and global syntactic in-formation about source sentences is captured by theencoder.
Different types of syntax is stored in dif-ferent layers, with different concentration degrees.We also carry out a fine-grained analysis of the con-stituency trees extracted from the encoder, highlight-ing what syntactic information is still missing.AcknowledgmentsThis work was supported by ARL/ARO (W911NF-10-1-0533) and DARPA (HR0011-15-C-0115).ReferencesOndrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Matous Machacek,Christof Monz, Pavel Pecina, Matt Post, Herv Saint-Amand, Radu Soricut, and Lucia Specia, editors.2014.
Proc.
Ninth Workshop on Statistical MachineTranslation.Figure 6: Example of Sense Confusion.
The POS tag for word?beyond?
is predicted as ?RB?
instead of ?IN?, resulting in amissing prepositional phrase.1533Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative rerank-ing.
In Proc.
ACL.Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phrase represen-tations using RNN encoder-decoder for statistical ma-chine translation.
In Proc.
EMNLP.Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
In Proc.
EMNLP.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learn fromphrase-based MT?
In Proc.
EMNLP-CoNLL.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What ?
s in a translation rule ?
Infor-mation Sciences, 2004.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.ACL.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural Computation, 9(8).Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network for mod-elling sentences.
In Proc.
ACL.Andrej Karpathy, Justin Johnson, and Li Fei-Fei.
2016.Visualizing and understanding recurrent networks.
InProc.
ICLR.Yoon Kim.
2014.
Convolutional neural networks for sen-tence classification.
In Proc.
EMNLP.Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,Richard Zemel, Raquel Urtasun, Antonio Torralba,and Sanja Fidler.
2015.
Skip-thought vectors.
InProc.
NIPS.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.NAACL.Jonathan K. Kummerfeld, David Hall, James R. Curran,and Dan Klein.
2012.
Parser showdown at the WallStreet Corral: An empirical investigation of error typesin parser output.
In Proc.
EMNLP-CoNLL.Jonathan K. Kummerfeld, Daniel Tse, James R Curran,and Dan Klein.
2013.
An empirical examination ofchallenges in Chinese parsing.
In Proc.
ACL.Qv Le and Tomas Mikolov.
2014.
Distributed represen-tations of sentences and documents.
In Proc.
ICML.Jiwei Li, Xinlei Chen, Eduard Hovy, and Dan Jurafsky.2016.
Visualizing and understanding neural models innlp.
In Proc.
NAACL.Yang Liu, Qun Liu, and Yajuan Lu?.
2011.
Adjoiningtree-to-string translation.
In Proc.
ACL.Mitchell P Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
ACL.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on Parsing the Web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Sameer S Pradhan and Nianwen Xue.
2009.
Ontonotes:the 90% solution.
In Proc.
NAACL.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng, andChristopher Potts.
2013.
Recursive deep models forsemantic compositionality over a sentiment treebank.In Proc.
EMNLP.Ilya Sutskever, Oriol Vinyals, and Quoc V Le.
2014.
Se-quence to sequence learning with neural networks.
InProc.
NIPS.Laurens van der Maaten and Geoffrey Hinton.
2008.Visualizing data using t-SNE.
Journal of MachineLearning Research, 9.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Grammaras a foreign language.
In Proc.
NIPS.Kaizhong Zhang and Dennis Shasha.
1989.
Simple fastalgorithms for the editing distance between trees andrelated problems.
SIAM Journal on Computing, 18(6).Han Zhao, Zhengdong Lu, and Pascal Poupart.
2015.Self-adaptive hierarchical sentence model.
In Proc.
IJ-CAI.1534
