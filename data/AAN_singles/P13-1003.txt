Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 22?31,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTraining Nondeficient Variants of IBM-3 and IBM-4 for Word AlignmentThomas SchoenemannHeinrich-Heine-Universita?t Du?sseldorf, GermanyUniversita?tsstr.
140225 Du?sseldorf, GermanyAbstractWe derive variants of the fertility basedmodels IBM-3 and IBM-4 that, whilemaintaining their zero and first order pa-rameters, are nondeficient.
Subsequently,we proceed to derive a method to com-pute a likely alignment and its neighborsas well as give a solution of EM training.The arising M-step energies are non-trivialand handled via projected gradient ascent.Our evaluation on gold alignments showssubstantial improvements (in weighted F-measure) for the IBM-3.
For the IBM-4 there are no consistent improvements.Training the nondeficient IBM-5 in theregular way gives surprisingly good re-sults.Using the resulting alignments for phrase-based translation systems offers no clearinsights w.r.t.
BLEU scores.1 IntroductionWhile most people think of the translation andword alignment models IBM-3 and IBM-4 as in-herently deficient models (i.e.
models that assignnon-zero probability mass to impossible events),in this paper we derive nondeficient variants main-taining their zero order (IBM-3) and first order(IBM-4) parameters.
This is possible as IBM-3and IBM-4 are very special cases of general log-linear models: they are properly derived by thechain rule of probabilities.
Deficiency is only in-troduced by ignoring a part of the history to beconditioned on in the individual factors of thechain rule factorization.
While at first glance thisseems necessary to obtain zero and first order de-Figure 1: Plot of the negative log.
likelihoods(the quantity to be minimized) arising in trainingdeficient and nondeficient models (for EuroparlGerman | English, training scheme 15H53545).1/3/4=IBM-1/3/4, H=HMM, T=Transfer iteration.The curves are identical up to iteration 11.Iteration 11 shows that merely 5.14% of the(HMM) probability mass are covered by theViterbi alignment and its neighbors.
With deficientmodels (and deficient empty words) the final neg-ative log likelihood is higher than the initial HMMone, with nondeficient models it is lower than forthe HMM, as it should be for a better model.pendencies, we show that with proper renormal-ization all factors can be made nondeficient.Having introduced the model variants, we pro-ceed to derive a hillclimbing method to computea likely alignment (ideally the Viterbi alignment)and its neighbors.
As for the deficient models, thisplays an important role in the E-step of the sub-sequently derived expectation maximization (EM)training scheme.
As usual, expectations in EM areapproximated, but we now also get non-trivial M-step energies.
We deal with these via projectedgradient ascent.22The downside of our method is its resource con-sumption, but still we present results on corporawith 100.000 sentence pairs.
The source code ofthis project is available in our word alignment soft-ware RegAligner1, version 1.2 and later.Figure 1 gives a first demonstration of howmuch the proposed variants differ from the stan-dard models by visualizing the resulting negativelog likelihoods2, the quantity to be minimized inEM-training.
The nondeficient IBM-4 derives alower negative log likelihood than the HMM, theregular deficient variant only a lower one thanthe IBM-1.
As an aside, the transfer iterationfrom HMM to IBM3 (iteration 11) reveals thatonly 5.14% of the probability mass3 are preservedwhen using the Viterbi alignment and its neighborsinstead of all alignments.Indeed, it is widely recognized that ?
withproper initialization ?
fertility based models out-perform sequence based ones.
In particular, se-quence based models can simply ignore a part ofthe sentence to be conditioned on, while fertilitybased models explicitly factor in a probability ofwords in this sentence to have no aligned words(or any other number of aligned words, called thefertility).
Hence, it is encouraging to see that thenondeficient IBM-4 indeed derives a higher likeli-hood than the sequence based HMM.Related Work Today?s most widely used mod-els for word alignment are still the models IBM1-5 of Brown et al (1993) and the HMM of Vo-gel et al (1996), thoroughly evaluated in (Ochand Ney, 2003).
While it is known that fertility-based models outperform sequence-based ones,the large bulk of word alignment literature follow-ing these publications has mostly ignored fertility-based models.
This is different in the present paperwhich deals exclusively with such models.One reason for the lack of interest is surely thatcomputing expectations and Viterbi alignments forthese models is a hard problem (Udupa and Maji,2006).
Nevertheless, computing Viterbi align-1https://github.com/Thomas1205/RegAligner,for the reported results we used a slightly earlier version.2Note that the figure slightly favors IBM-1 and HMM asfor them the length J of the foreign sequence is assumed tobe known whereas IBM-3 and IBM-4 explicitly predict it.3This number regards the corpus probability as in (9) tothe power of 1/S, i.e.
the objective function in maximumlikelihood training.
The number is not entirely fair as align-ments where more than half the words align to the emptyword are assigned a probability of 0.
Still, this is an issueonly for short sentences.ments for the IBM-3 has been shown to oftenbe practicable (Ravi and Knight, 2010; Schoen-emann, 2010).Much work has been spent on HMM-basedformulations, focusing on the computationallytractable side (Toutanova et al, 2002; Sumita etal., 2004; Deng and Byrne, 2005).
In addition,some rather complex models have been proposedthat usually aim to replace the fertility based mod-els (Wang and Waibel, 1998; Fraser and Marcu,2007a).Another line of models (Melamed, 2000; Marcuand Wong, 2002; Cromie`res and Kurohashi, 2009)focuses on joint probabilities to get around thegarbage collection effect (i.e.
that for conditionalmodels, rare words in the given language align totoo many words in the predicted language).
Thedownside is that these models are computationallyharder to handle.A more recent line of work introduces variousforms of regularity terms, often in the form ofsymmetrization (Liang et al, 2006; Grac?a et al,2010; Bansal et al, 2011) and recently by usingL0 norms (Vaswani et al, 2012).2 The models IBM-3, IBM-4 and IBM-5We begin with a short review of fertility-basedmodels in general and IBM-3, IBM-4 and IBM-5 specifically.
All are due to (Brown et al, 1993)who proposed to use the deficient models IBM-3and IBM-4 to initialize the nondeficient IBM-5.For a foreign sentence f = fJ1 = (f1, .
.
.
, fJ)with J words and an English one e = eI1 =(e1, .
.
.
, eI) with I words, the (conditional) proba-bility p(fJ1 |eI1) of getting the foreign sentence as atranslation of the English one is modeled by intro-ducing the word alignment a as a hidden variable:p(fJ1 |eI1) =?ap(fJ1 ,a|eI1)All IBM models restrict the space of alignmentsto those where a foreign word can align to at mostone target word.
The resulting alignment is thenwritten as a vector aJ1 , where each aj takes integralvalues between 0 and I , with 0 indicating that fjhas no English correspondence.The fertility-based models IBM-3, IBM-4and IBM-5 factor the (conditional) probabilityp(fJ1 , aJ1 |eI1) of obtaining an alignment and atranslation given an English sentence according tothe following generative story:231.
For i = 1, 2, .
.
.
, I , decide on the number ?iof foreign words aligned to ei.
This numberis called the fertility of ei.
Choose with prob-ability p(?i|eI1,?i?11 ) = p(?i|ei).2.
Choose the number ?0 of unaligned wordsin the (still unknown) foreign sequence.Choose with probability p(?0|eI1,?I1) =p(?0|?Ii=1 ?i).
Since each foreign word be-longs to exactly one English position (includ-ing 0), the foreign sequence is now known tobe of length J = ?Ii=0 ?i.3.
For each i = 1, 2, .
.
.
, I , and k = 1, .
.
.
,?idecide on(a) the identity fi,k of the next foreignword aligned to ei.
Choose with probabilityp(fi,k|eI1,?I0,di?11 , di,1, .
.
.
, di,k?1, fi,k) =p(fi,k|ei), where di comprises all di,k forword i (see point b) below) and fi,k com-prises all foreign words known at that point.
(b) the position di,k of the just gener-ated foreign word fi,k, with probabilityp(di,k|eI1,?I0,di?11 , di,1, .
.
.
, di,k?1, fi,k, fi,k)= p(di,k|ei,di?11 , di,1, .
.
.
, di,k?1, fi,k, J).4.
The remaining ?0 open positions in the for-eign sequence align to position 0.
Decideon the corresponding foreign words withp(fd0,k |e0), where e0 is an artificial ?emptyword?.To model the probability for the number of un-aligned words in step 2, each of the?Ii=1 ?i prop-erly aligned foreign words generates an unalignedforeign word with probability p0, resulting inp(?0???I?i=1?i)=???I?i=1?i?0??
?p?i0 (1?p0)(?i ?i)?
?0 ,with a base probability p0 and the combinato-rial coefficients( nk)= n!k!(n?k)!
, where n!
=?nk=1 k denotes the factorial of n. The main dif-ference between IBM-3, IBM-4 and IBM-5 is thechoice of probability model in step 3 b), called adistortion model.
The choices are now detailed.2.1 IBM-3The IBM-3 implements a zero order distortionmodel, resulting inp(di,k|i, J) .Since most of the context to be conditioned on isignored, this allows invalid configurations to occurwith non-zero probability: some foreign positionscan be chosen several times, while others remainempty.
One says that the model is deficient.
Onthe other hand, the model for p(?0|?Ii=1 ?i) isnondeficient, and in training this often results invery high probabilities p0.
To prevent this it iscommon to make this model deficient as well (Ochand Ney, 2003), which improves performance im-mensely and gives much better results than simplyfixing p0 in the original model.As for each i the di,k can appear in any order(i.e.
need not be in ascending order), there are?Ii=1 ?i!
ways to generate the same alignment aJ1(where the ?i are the fertilities induced by aJ1 ).In total, the IBM-3 has the following probabilitymodel:p(fJ1 , aJ1 |eI1) =J?j=1[p(fj |eaj ) ?
p(j|aj , J)](1)?
p(?0|I?i=1?i)?I?i=1?i!
p(?i|ei) .Reducing the Number of Parameters Whileusing non-parametric models p(j|i, J) is conve-nient for closed-form M-steps in EM training,these parameters are not very intuitive.
Instead,in this paper we use the parametric modelp(j|i, J) = p(j|i)?Jj=1 p(j|i)(2)with the more intuitive parameters p(j|i).
Thearising M-step energy is addressed by projectedgradient ascent (see below).These parameters are also used for the nondefi-cient variants.
Using the original non-parametricones can be handled in a very similar manner tothe methods set forth below.2.2 IBM-4The distortion model of the IBM-4 is a first orderone that generates the di,k of each English positioni in ascending order (i.e.
for 1 < k ?
?i we havedi,k > di,k?1).
There is then a one-to-one cor-respondence between alignments aJ1 and (valid)distortion parameters (di,k)i=1,...,I, k=1,...,?i andtherefore no longer a factor of?Ii=1 ?i!
.The IBM-4 has two sub-distortion models, onefor the first aligned word (k = 1) of an English po-sition and one for all following words (k > 1, only24if ?i > 1).
For position i, let [i]=arg max{i?|1?i?
< i,?i?
> 0} denote4 the closest preceding En-glish word that has aligned foreign words.
Thealigned foreign positions of [i] are combined intoa center position [i], the rounded average of thepositions.
Now, the distortion probability for thefirst word (k = 1) isp=1(di,1|[i],A(fi,1),B(e[i]), J) ,where A gives the word class of a foreign wordand B the word class of an English word (there aretypically 50 classes per language, derived by ma-chine learning techniques).
The probability is fur-ther reduced to a dependency on the difference ofthe positions, i.e.
p=1(di,1?
[i] | A(fi,1),B(e[i])).For k > 1 the model isp>1(di,k|di,k?1,A(fi,k), J) ,which is likewise reduced to p>1(di,k ?di,k?1 | A(fi,k)).
Note that in both difference-based formulations the dependence on J has tobe dropped to get closed-form solutions of theM-step in EM training, and Brown et al notethemselves that the IBM-4 can place words beforethe start and after the end of the sentence.Reducing Deficiency In this paper, we also in-vestigate the effect of reducing the amount ofwasted probability mass by enforcing the depen-dence on J by proper renormalization, i.e.
usingp=1(j|j?,A(fi,1),B(e[i]), J) = (3)p=1(j ?
j?|A(fi,1),B(e[i]))?Jj?
?=1 p=1(j??
?
j?|A(fi,1),B(e[i])),for the first aligned word andp>1(j|j?,A(fi,k), J) = (4)p>1(j ?
j?
| A(fi,k))?Jj?
?=1 p>1(j??
?
j?
| A(fi,k))for all following words, again handling the M-stepin EM training via projected gradient ascent.
Withthis strategy words can no longer be placed out-side the sentence, but a lot of probability mass isstill wasted on configurations where at least oneforeign (or predicted) position j aligns to two ormore positions i, i?
in the English (or given) lan-guage (and consequently there are more unaligned4If the set is empty, instead a sentence start probabilityis used.
Note that we differ slightly in notation compared to(Brown et al, 1993).source words than the generated ?0).
Therefore,here, too, the probability for ?0 has to be madedeficient to get good performance.In summary, the base model for the IBM-4 is:p(fJ1 , aJ1 |eI1) = p(?0|I?i=1?i)(5)?J?j=1p(fj |eaj ) ?I?i=1p(?i|ei)?
?i:?i>0[p=1(di,1 ?
[i]|A(fi,1),B(e[i]))?
?i?k=2p>1(di,k ?
di,k?1|A(fi,k))],where empty products are understood to be 1.2.3 IBM-5We note in passing that the distortion model of theIBM-5 is nondeficient and has parameters for fill-ing the nth open gap in the foreign sequence giventhat there are N positions to choose from ?
seethe next section for exactly what positions one canchoose from.
There is also a dependence on wordclasses for the foreign language.This is neither a zero order nor a first order de-pendence, and in (Och and Ney, 2003) the first or-der model of the IBM-4, though deficient, outper-formed the IBM-5.
The IBM-5 is therefore rarelyused in practice.
This motivated us to instead re-formulate IBM-3 and IBM-4 as nondeficient mod-els.
In our results, however, the IBM-5 gave sur-prisingly good results and was often superior to allvariants of the IBM-4.3 Nondeficient Variants of IBM-3 andIBM-4From now on we always enforce that for each po-sition i the indices di,k are generated in ascendingorder (di,k > di,k?1 for k > 1).
A central con-cept for the generation of di,k in step 3(b) is theset of positions in the foreign sequence that arestill without alignment.
We denote the set of thesepositions byJi,k,J = {1, .
.
.
, J} ?
{di,k?
| 1 ?
k?
< k}?{di?,k?
| 1 ?
i?
< i, 1 ?
k?
?
?i?
}where the dependence on the various di?,k?
is notmade explicit in the following.It is tempting to think that in a nondeficientmodel all members of Ji,k,J can be chosen for25di,k, but this holds only ?i = 1.
Otherwise, therequirement of generating the di,k in ascending or-der prevents us from choosing the (?i?k) largestentries inJi,k,J .
For k > 1 we also have to removeall positions smaller than di,k?1.Let J ?ii,k,J denote the set where these positionshave been removed.
With that, we can state thenondeficient variants of IBM-3 and IBM-4.3.1 Nondeficient IBM-3For the IBM-3, we define the auxiliary quantityq(di,k = j | i,J ?ii,k,J) ={p(j|i) if j ?
J ?ii,k,J0 else ,where we use the zero order parameters p(j|i) wealso use for the standard (deficient) IBM-3, com-pare (2).
To get a nondeficient variant, it remainsto renormalize, resulting inp(di,k = j|i,J ?ii,k,J) =q(j|i,J ?ii,k,J)?Jj=1 q(j|i,J ?ii,k,J).
(6)Further, note that the factors ?i!
now have tobe removed from (1) as the di,k are generated inascending order.
Lastly, here we use the originalnondeficient empty word model p(?0|?Ii=1 ?i),resulting in a totally nondeficient model.3.2 Nondeficient IBM-4With the notation set up, it is rather straightfor-ward to derive a nondeficient variant of the IBM-4.
Here, there are the two cases k = 1 and k > 1.We begin with the case k = 1.
Abbreviating?
= A(fi,1) and ?
= B(e[i]), we define the auxil-iary quantityq=1(di,1 = j|[i], ?, ?,J ?ii,k,J) = (7){p=1(j ?
[i]|?, ?)
if j ?
J ?ii,k,J0 else ,again using the - now first order - parametersof the base model.
The nondeficient distributionp=1(di,1 = j|[i], ?, ?,J ?ii,k,J) is again obtainedby renormalization.For the case k > 1, we abbreviate ?
= A(fi,k)and introduce the auxiliary quantityq>1(di,k = j|di,k?1, ?,J ?ii,k,J) = (8){p>1(j ?
di,k?1|?)
if j ?
J ?ii,k,J0 else ,from which the nondeficient distributionp>1(di,k=j|di,k?1, ?,J ?ii,k,J) is again obtained byrenormalization.4 Training the New VariantsFor the task of word alignment, we infer the pa-rameters of the models using the maximum likeli-hood criterionmax?S?s=1p?
(fs|es) (9)on a set of training data (i.e.
sentence pairs s =1, .
.
.
, S).
Here, ?
comprises all base parametersof the respective model (e.g.
for the IBM-3 allp(f |e), all p(?, e) and all p(j|i) ) and p?
signifiesthe dependence of the model on the parameters.Note that (9) is truly a constrained optimizationproblem as the parameters ?
have to satisfy a num-ber of probability normalization constraints.When p?(?)
denotes a fertility based model theresulting problem is a non-concave maximizationproblem with many local minima and no (known)closed-form solutions.
Hence, it is handled bycomputational methods, which typically apply thelogarithm to the above function.Our method of choice to attack the maximumlikelihood problem is expectation maximization(EM), the standard in the field, which we explainbelow.
Due to non-concaveness the starting pointfor EM is of extreme importance.
As is common,we first train an IBM-1 and then an HMM beforeproceeding to the IBM-3 and finally the IBM-4.As in the training of the deficient IBM-3 andIBM-4 models, we approximate the expectationsin the E-step by a set of likely alignments, ideallycentered around the Viterbi alignment, but alreadyfor the regular deficient variants computing it isNP-hard (Udupa and Maji, 2006).
A first task istherefore to compute such a set.
This task is alsoneeded for the actual task of word alignment (an-notating a given sentence pair with an alignment).4.1 Alignment ComputationFor computing alignments, we use the commonprocedure of hillclimbing where we start with analignment, then iteratively compute the probabili-ties of all alignments differing by a move or a swap(Brown et al, 1993) and move to the best of theseif it beats the current alignment.Since we cannot ignore parts of the history andstill get a nondeficient model, computing the prob-abilities of the neighbors cannot be handled in-crementally (or rather only partially, for the dic-tionary and fertility models).
While this does in-crease running times, in practice the M-steps takelonger than the E-steps.26For self-containment, we recall here that for analignment aJ1 applying the move aJ1 [j?
i] resultsin the alignment a?J1 defined by a?j = i and a?j?=aj?for j?
6= j.
Applying the swap aJ1 [j1 ?
j2] resultsin the alignment a?J1 defined by a?j1 =aj2 , a?j2 =aj1and a?j?
= aj?
elsewhere.
If aJ1 is the alignmentproduced by hillclimbing, the move matrix m ?IRJ?I+1 is defined bymj,i being the probability ofaJ1 [j ?
i] as long as aj 6= i, otherwise 0.
Likewisethe swap matrix s ?
IRJ?J is defined as sj1,j2being the probability of aJ1 [j1 ?
j2] for aj1 6=aj2 ,0 otherwise.
The move and swap matrices are usedto approximate expectations in EM training (seebelow).4.2 Parameter UpdateNaive Scheme It is tempting to account for thechanges in the model in hillclimbing, but to oth-erwise use the regular M-step procedures (closedform solution when not conditioning on J for theIBM-4 and for the non-parametric IBM-3, other-wise projected gradient ascent) for the deficientmodels.
However, we verified that this is not agood idea: not only can the likelihood go downin the process (even if we could compute expecta-tions exactly), but these schemes also heavily in-crease p0 in each iteration, i.e.
the same problemOch and Ney (2003) found for the deficient mod-els.
There is therefore the need to execute the M-step properly, and when done the problem is in-deed resolved.Proper EM The expectation maximization(EM) framework (Dempster et al, 1977; Neal andHinton, 1998) is a class of template procedures(rather than a proper algorithm) that iterativelyrequires solving the taskmax?kS?s=1?asp?k?1(as|fs, es) log(p?k(fs,as|es))(10)by appropriate means.
Here, ?k?1 are the parame-ters from the previous iteration, while ?k are thosederived in the current iteration.
Of course, hereand in the following the normalization constraintson ?
apply, as already in (9).
On explicit requestof a reviewer we give a detailed account for oursetting here.
Readers not interested in the detailscan safely move on to the next section.Details on EM For the corpora occurring inpractice, the function (10) has many more termsthan there are atoms in the universe.
The trick isthat p?k(fs,as|es) is a product of factors, whereeach factor depends on very few components of?k only.
Taking the logarithm gives a sum oflogarithms, and in the end we are left with theproblem of computing the weights of each factor,which turn out to be expectations.
To apply thisto the (deficient) IBM-3 model with parametricdistortion we simplify p?k?1(as|fs, es) = p(as)and define the counts nf,e(as) = ?Jsj=1 ?
(fsj , f) ??
(esasj , e), n?,e(as) =?Isi=1 ?
(esi , e) ??(?i(as),?
)and nj,i(as) = ?
(asj , i).
We also use short handnotations for sets, e.g.
{p(f |e)} is meant as theset of all translation probabilities induced by thegiven corpus.
With this notation, after reorderingthe terms problem (10) can be written asmax{p(f |e)},{p(?|e)},{p(j|i)}(11)?e,f[ S?s=1?asp(as)nf,e(as)]log(p(f |e))+?e,?
[ S?s=1?asp(as)n?,e(as)]log(p(?, e))+?i,j[ S?s=1?asp(as)nj,i(as)]log(p(j|i, J)).Indeed, the weights in each line turn out to benothing else than expectations of the respectivefactor under the distribution p?k?1(as|fs, es) andwill henceforth be written as wf,e, w?,e and wj,i,J .Therefore, executing an iteration of EM requiresfirst calculating all expectations (E-step) and thensolving the maximization problems (M-step).
Formodels such as IBM-1 and HMM the expectationscan be calculated efficiently, so the enormous sumof terms in (10) is equivalently written as a man-ageable one.
In this case it can be shown5 thatthe new ?k must have a higher likelihood (9) than?k?1 (unless a stationary point is reached).
In fact,any ?
that has a higher value in the auxiliary func-tion (11) than ?k?1 must also have a higher like-lihood.
This is an important background for para-metric models such as (2) where the M-step cannotbe solved exactly.For IBM-3/4/5 computing exact expectations isintractable (Udupa and Maji, 2006) and approx-imations have to be used (in fact, even comput-ing the likelihood for a given ?
is intractable).
We5See e.g.
the author?s course notes (in German), currentlyhttp://user.phil-fak.uni-duesseldorf.de/?tosch/downloads/statmt/wordalign.pdf.27use the common procedure based on hillclimbingand the move/swap matrices.
The likelihood is notguaranteed to increase but it (or rather its approx-imation) always did in each of the five run itera-tions.
Nevertheless, the main advantage of EM ispreserved: problem (11) decomposes into severalsmaller problems, one for each probability distri-bution since the parameters are tied by the nor-malization constraints.
The result is one problemfor each e involving all p(f |e), one for each e in-volving all p(?|e) and one for each i involving allp(j|i).The problems for the translation probabilitiesand the fertility probabilities yield the known stan-dard update rules.
The most interesting case is theproblem for the (parametric) distortion models.
Inthe deficient setting, the problem for each i ismax{p(j|i)}?Jwi,j,J log(p(j|i)?Jj?=1 p(j?|i))In the nondeficient setting, we now drop the sub-scripts i, k, J and the superscript ?
from the setsdefined in the previous sections, i.e.
we write Jinstead of J ?i,k,J .
The M-step problem is thenmax{p(j|i)}Ei =?j?J :j?Jwj,i,J log(p(j|i,J )),where wj,i,J (with j ?
J ) is the expectation foraligning j to iwhen one can choose among the po-sitions inJ , and with p(j|i,J ) as in (6).
In princi-ple there is an exponential number of expectationswj,i,J .
However, since we approximate expecta-tions from the move and swap matrices, and henceby O((I + J) ?
J) alignments per sentence pair,in the end we get a polynomial number of terms.Currently we only consider alignments with (ap-proximated) p?k?1(as|fs, es) > 10?6.Importantly, the fact that we get separate M-stepproblems for different i allows us to reduce mem-ory consumption by using refined data structureswhen storing the expectations.For both the deficient and the nondeficient vari-ants, the M-step problems for the distortion pa-rameters p(j|i) are non-trivial, non-concave andhave no (known) closed form solutions.
We ap-proach them via the method of projected gradientascent (PGA), where the gradient for the nondefi-cient problem is?Ei?p(j|i) =?J :j?J[wj,Jp(j|i) ??j?
?J wj?,J?j?
?J p(j?|i)].When running PGA we guarantee that the result-ing {p(j|i)} has a higher function value Ei thanthe input ones (unless a stationary point is input).We stop when a cutoff criterion indicates a localmaximum or 250 iterations are used up.Projected Gradient Ascent This method isused in a couple of recent papers, notably (Schoen-emann, 2011; Vaswani et al, 2012) and is brieflysketched here for self-containment (see those pa-pers for more details).
To solve a maximizationproblemmaxp(j|i)?0,?j p(j|i)=1Ei({p(j|i)})for some (differentiable) function Ei(?
), one iter-atively starts at the current point {pk(j|i)}, com-putes the gradient ?Ei({pk(j|i)}) and goes to thepointq(j|i) = pk(j|i) + ?
?Ei(pk(j|i)) , j = 1, .
.
.
, Jfor some step-length ?.
This point is generallynot a probability distribution, so one computes thenearest probability distributionminq?
(j|i)?0,?j q?(j|i)=1J?j=1(q?(j|i)?
q(j|i))2 ,a step known as projection which we solve withthe method of (Michelot, 1986).
The new dis-tribution {q?
(j|i)} is not guaranteed to have ahigher Ei(?
), but (since the constraint set is a con-vex one) a suitable interpolation of {pk(j|i)} and{q?
(j|i)} is guaranteed to have a higher value (un-less {pk(j|i)} is a local maximum or minimumof Ei(?)).
Such a point is computed by back-tracking line search and defines the next iterate{pk+1(j|i)}.IBM-4 When moving from the IBM-3 to theIBM-4, only the last line in (11) changes.
Inthe end one gets two new kinds of problems, forp=1(?)
and p>1(?).
For p=1(?)
we have one prob-lem for each foreign class ?
and each English class?, of the formmax{p=1(j|j?,?,?)}?j,j?,Jwj,j?,J,?,?
log(p=1(j|j?, ?, ?, J))for reduced deficiency (with p=1(j|j?, ?, ?, J) asin (3) ) and of the formmax{p=1(j|j?,?,?
)}?j,j?,Jwj,j?,J ,?,?
log(p=1(j|j?, ?, ?,J ))28Model Degree of Deficiency De|En En|De Es|En En|EsHMM nondeficient (our) 73.8 77.6 77.4 76.1IBM-3 full (GIZA++) 74.2 76.5 74.3 74.5IBM-3 full (our) 75.6 79.2 75.2 73.7IBM-3 nondeficient (our) 76.1 79.8 76.8 75.5IBM-4, 1 x 1 word class full (GIZA++) 77.9 79.4 78.6 78.4IBM-4, 1 x 1 word class full (our) 76.1 81.5 77.8 78.0IBM-4, 1 x 1 word class reduced (our) 77.2 80.6 77.9 78.3IBM-4, 1 x 1 word class nondeficient (our) 77.6 81.5 80.0 78.4IBM-4, 50 x 50 word classes full (GIZA++) 78.6 80.4 79.3 79.3IBM-4, 50 x 50 word classes full (our) 78.0 82.4 79.2 79.4IBM-4, 50 x 50 word classes reduced (our) 78.5 82.1 79.2 79.0IBM-4, 50 x 50 word classes nondeficient (our) 77.9 82.5 79.7 78.2IBM-5, 50 word classes nondeficient (GIZA++) 79.4 81.1 80.0 79.5IBM-5, 50 word classes nondeficient (our) 79.2 82.7 79.7 79.5Table 1: Alignment accuracy (weighted F-measure times 100, ?
= 0.1) on Europarl with 100.000sentence pairs.
Reduced deficiency means renormalization as in (3) and (4), so that words cannot beplaced before or after the sentence.
For the IBM-3, the nondeficient variant is clearly best.
For theIBM-4 it is better in roughly half the cases, both with and without word classes.for the nondeficient variant, withp=1(j|j?, ?, ?,J ) based on (7).For p>1(?)
we have one problem per foreignclass ?, of the formmax{p>1(j|j?,?)}?j,j?,Jwj,j?,J,?
log(p>1(j|j?, ?, J))for reduced deficiency, with p>1(j|j?, ?, J) basedon (4), and for the nondeficient variant it has theformmax{p>1(j|j?,?
)}?j,j?,Jwj,j?,J ,?
log(p>1(j|j?, ?,J )),with p>1(j|j?, ?,J ) based on (8).
Calculating thegradients is analogous to the IBM-3.5 ExperimentsWe test the proposed methods on subsets of theEuroparl corpus for German and English as wellas Spanish and English, using lower-cased cor-pora.
We evaluate alignment accuracies on goldalignments6 in the form of weighted F-measureswith ?=0.1, which performed well in (Fraser andMarcu, 2007b).
In addition we evaluate the effecton phrase-based translation on one of the tasks.We implement the proposed methods in ourown framework RegAligner rather than GIZA++,6from (Lambert et al, 2005) and fromhttp://user.phil-fak.uni-duesseldorf.de/?tosch/downloads.html.which is only rudimentally maintained.
Therefore,we compare to the deficient models in our ownsoftware as well as to those in GIZA++.We run 5 iterations of IBM-1, followed by 5iterations of HMM, 5 of IBM-3 and finally 5 ofIBM-4.
The first iteration of the IBM-3 collectscounts from the HMM, and likewise the first iter-ation of the IBM-4 collects counts from the IBM-3 (in both cases the move and swap matrices arefilled with probabilities of the former model, thentheses matrices are used as in a regular model iter-ation).
A nondeficient IBM-4 is always initializedby a nondeficient IBM-3.
We did not set a fertilitylimit (except for GIZA++).Experiments were run on a Core i5 with 2.5GHz and 8 GB of memory.
The latter was themain reason why we did not use still larger cor-pora7.
The running times for the entire trainingwere half a day without word classes and a daywith word classes.
With 50 instead of 250 PGA it-erations in all M-steps we get only half these run-ning times, but the resulting F-measures deterio-rate, especially for the IBM-4 with classes.The running times of our implementation of theIBM-5 are much more favorable: the entire train-ing then runs in little more than an hour.7The main memory bottleneck is the IBM-4 (6 GB with-out classes, 8 GB with).
Using refined data structures shouldreduce this bottleneck.295.1 Alignment AccuracyThe alignment accuracies ?
weighted F-measureswith ?
= 0.1 ?
for the tested corpora and modelvariants are given in Table 1.
Clearly, nondefi-ciency greatly improves the accuracy of the IBM-3, both compared to our deficient implementationand that of GIZA++.For the IBM-4 we get improvements for thenondeficient variant in roughly half the cases, bothwith and without word classes.
We think this isan issue of local minima, inexactly solved M-stepsand sensitiveness to initialization.Interestingly, also the reduced deficient IBM-4is not always better than the fully deficient variant.Again, we think this is due to problems with thenon-concave nature of the models.There is also quite some surprise regarding theIBM-5: contrary to the findings of (Och and Ney,2003) the IBM-5 in GIZA++ performs best inthree out of four cases - when competing with bothdeficient and nondeficient variants of IBM-3 andIBM-4.
Our own implementation gives slightlydifferent results (as we do not use smoothing), butit, too, performs very well.5.2 Effect on Translation PerformanceWe also check the effect of the various align-ments (all produced by RegAligner) on trans-lation performance for phrase-based translation,randomly choosing translation from German toEnglish.
We use MOSES with a 5-gram lan-guage model (trained on 500.000 sentence pairs)and the standard setup in the MOSES Experi-ment Management System: training is run in bothdirections, the alignments are combined usingdiag-grow-final-and (Och and Ney, 2003)and the parameters of MOSES are optimized on750 development sentences.The resulting BLEU-scores are shown in Table2.
However, the table shows no clear trends andeven the IBM-3 is not clearly inferior to the IBM-4.
We think that one would need to handle largercorpora (or run multiple instances of Minimum Er-ror Rate Training with different random seeds) toget more meaningful insights.
Hence, at presentour paper is primarily of theoretical value.6 ConclusionWe have shown that the word alignment modelsIBM-3 and IBM-4 can be turned into nondeficientModel #Classes Deficiency BLEUHMM - nondeficient 29.72IBM-3 - deficient 29.63IBM-3 - nondeficient 29.73IBM-4 1 x 1 fully deficient 29.91IBM-4 1 x 1 reduced deficient 29.88IBM-4 1 x 1 nondeficient 30.18IBM-4 50 x 50 fully deficient 29.86IBM-4 50 x 50 reduced deficient 30.14IBM-4 50 x 50 nondeficient 29.90IBM-5 50 nondeficient 29.84Table 2: Evaluation of phrase-based translationfrom German to English with the obtained align-ments (for 100.000 sentence pairs).
Training is runin both directions and the resulting alignments arecombined via diag-grow-final-and.
Thetable shows no clear superiority of any method.In fact, the IBM-4 is not superior to the IBM-3and the HMM is about equal to the IBM-3.
Wethink that one needs to handle larger corpora toget clearer insights.variants, an important aim of probabilistic model-ing for word alignment.Here we have exploited that the models areproper applications of the chain rule of probabili-ties, where deficiency is only introduced by ignor-ing parts of the history for the distortion factors inthe factorization.
By proper renormalization thedesired nondeficient variants are obtained.The arising models are trained via expectationmaximization.
In the E-step we use hillclimb-ing to get a likely alignment (ideally the Viterbialignment).
While this cannot be handled fullyincrementally, it is still fast enough in practice.The M-step energies are non-concave and have no(known) closed-form solutions.
They are handledvia projected gradient ascent.For the IBM-3 nondeficiency clearly improvesalignment accuracy.
For the IBM-4 we get im-proved accuracies in roughly half the cases, bothwith and without word classes.
The IBM-5 per-forms surprisingly well, it is often best and hencemuch better than its reputation.
An evaluation ofphrase based translation showed no clear insights.Nevertheless, we think that nondeficiency infertility based models is an important issue, andthat at the very least our paper is of theoreticalvalue.
The implementations are publicly availablein RegAligner 1.2.30ReferencesM.
Bansal, C. Quirk, and R. Moore.
2011.
Gappyphrasal alignment by agreement.
In Annual Meet-ing of the Association for Computational Linguistics(ACL), Portland, Oregon, June.P.F.
Brown, S.A. Della Pietra, V.J.
Della Pietra, andR.L.
Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?311, June.F.
Cromie`res and S. Kurohashi.
2009.
An alignmentalgorithm using Belief Propagation and a structure-based distortion model.
In Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL), Athens, Greece, April.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical So-ciety, Series B, 39(1):1?38.Y.
Deng and W. Byrne.
2005.
HMM word and phrasealignment for statistical machine translation.
InHLT-EMNLP, Vancouver, Canada, October.A.
Fraser and D. Marcu.
2007a.
Getting the structureright for word alignment: LEAF.
In Conference onEmpirical Methods in Natural Language Processing(EMNLP), Prague, Czech Republic, June.A.
Fraser and D. Marcu.
2007b.
Measuring wordalignment quality for statistical machine translation.Computational Linguistics, 33(3):293?303, Septem-ber.J.
Grac?a, K. Ganchev, and B. Taskar.
2010.
Learningtractable word alignment models with complex con-straints.
Computational Linguistics, 36, September.P.
Lambert, A.D. Gispert, R. Banchs, and J.B. Marino.2005.
Guidelines for word alignment evaluation andmanual alignment.
Language Resources and Evalu-ation, 39(4):267?285.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In Human Language Technology Con-ference of the North American Chapter of the As-sociation of Computational Linguistics, New York,New York, June.D.
Marcu and W. Wong.
2002.
A phrase-based,joint probability model for statistical machine trans-lation.
In Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), Philadelphia,Pennsylvania, July.D.
Melamed.
2000.
Models of translational equiv-alence among words.
Computational Linguistics,26(2):221?249.C.
Michelot.
1986.
A finite algorithm for finding theprojection of a point onto the canonical simplex ofIRn.
Journal on Optimization Theory and Applica-tions, 50(1), July.R.M.
Neal and G.E.
Hinton.
1998.
A view of theEM algorithm that justifies incremental, sparse, andother variants.
In M.I.
Jordan, editor, Learning inGraphical Models.
MIT press.F.J.
Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.S.
Ravi and K. Knight.
2010.
Does GIZA++ makesearch errors?
Computational Linguistics, 36(3).T.
Schoenemann.
2010.
Computing optimal align-ments for the IBM-3 translation model.
In Confer-ence on Computational Natural Language Learning(CoNLL), Uppsala, Sweden, July.T.
Schoenemann.
2011.
Regularizing mono- and bi-word models for word alignment.
In InternationalJoint Conference on Natural Language Processing(IJCNLP), Chiang Mai, Thailand, November.E.
Sumita, Y. Akiba, T. Doi, A. Finch, K. Imamura,H.
Okuma, M. Paul, M. Shimohata, and T. Watan-abe.
2004.
EBMT, SMT, Hybrid and more: ATRspoken language translation system.
In Interna-tional Workshop on Spoken Language Translation(IWSLT), Kyoto, Japan, September.K.
Toutanova, H.T.
Ilhan, and C.D.
Manning.
2002.Extensions to HMM-based statistical word align-ment models.
In Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),Philadelphia, Pennsylvania, July.R.
Udupa and H.K.
Maji.
2006.
Computational com-plexity of statistical machine translation.
In Con-ference of the European Chapter of the Associationfor Computational Linguistics (EACL), Trento, Italy,April.A.
Vaswani, L. Huang, and D. Chiang.
2012.
Smalleralignment models for better translations: Unsuper-vised word alignment with the l0-norm.
In AnnualMeeting of the Association for Computational Lin-guistics (ACL), Jeju, Korea, July.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Inter-national Conference on Computational Linguistics(COLING), pages 836?841, Copenhagen, Denmark,August.Y.-Y.
Wang and A. Waibel.
1998.
Modeling withstructures in statistical machine translation.
In In-ternational Conference on Computational Linguis-tics (COLING), Montreal, Canada, August.31
