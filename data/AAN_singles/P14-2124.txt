Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 765?771,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsXMEANT: Better semantic MT evaluation without reference translationsLo, Chi-kiu Beloucif, Meriem Saers, Markus Wu, DekaiHKUSTHuman Language Technology CenterDepartment of Computer Science and EngineeringHong Kong University of Science and Technology{jackielo|mbeloucif|masaers|dekai}@cs.ust.hkAbstractWe introduce XMEANT?a new cross-lingualversion of the semantic frame based MTevaluation metric MEANT?which can cor-relate even more closely with human ade-quacy judgments than monolingual MEANTand eliminates the need for expensive hu-man references.
Previous work establishedthat MEANT reflects translation adequacywith state-of-the-art accuracy, and optimiz-ing MT systems against MEANT robustly im-proves translation quality.
However, to gobeyond tuning weights in the loglinear SMTmodel, a cross-lingual objective function thatcan deeply integrate semantic frame crite-ria into the MT training pipeline is needed.We show that cross-lingual XMEANT out-performs monolingual MEANT by (1) replac-ing the monolingual context vector model inMEANT with simple translation probabilities,and (2) incorporating bracketing ITG con-straints.1 IntroductionWe show that XMEANT, a new cross-lingual ver-sion of MEANT (Lo et al, 2012), correlates withhuman judgment even more closely than MEANTfor evaluating MT adequacy via semantic frames,despite discarding the need for expensive humanreference translations.
XMEANT is obtained by(1) using simple lexical translation probabilities,instead of the monolingual context vector modelused in MEANT for computing the semantic rolefillers similarities, and (2) incorporating bracket-ing ITG constrains for word alignment within thesemantic role fillers.
We conjecture that the rea-son that XMEANT correlates more closely withhuman adequacy judgement than MEANT is thaton the one hand, the semantic structure of theMT output is closer to that of the input sentencethan that of the reference translation, and on theother hand, the BITG constraints the word align-ment more accurately than the heuristic bag-of-word aggregation used in MEANT.
Our resultssuggest that MT translation adequacy is more ac-curately evaluated via the cross-lingual semanticframe similarities of the input and the MT outputwhich may obviate the need for expensive humanreference translations.The MEANT family of metrics (Lo and Wu,2011a, 2012; Lo et al, 2012) adopt the princi-ple that a good translation is one where a humancan successfully understand the central meaningof the foreign sentence as captured by the basicevent structure: ?who did what to whom, when,where and why?
(Pradhan et al, 2004).
MEANTmeasures similarity between the MT output andthe reference translations by comparing the simi-larities between the semantic frame structures ofoutput and reference translations.
It is well estab-lished that the MEANT family of metrics corre-lates better with human adequacy judgments thancommonly used MT evaluation metrics (Lo andWu, 2011a, 2012; Lo et al, 2012; Lo and Wu,2013b; Mach?a?cek and Bojar, 2013).
In addition,the translation adequacy across different genres(ranging from formal news to informal web fo-rum and public speech) and different languages(English and Chinese) is improved by replacingBLEU or TER with MEANT during parametertuning (Lo et al, 2013a; Lo and Wu, 2013a; Loet al, 2013b).In order to continue driving MT towards bettertranslation adequacy by deeply integrating seman-tic frame criteria into theMT training pipeline, it isnecessary to have a cross-lingual semantic objec-tive function that assesses the semantic frame sim-ilarities of input and output sentences.
We there-fore propose XMEANT, a cross-lingual MT evalu-ation metric, that modifies MEANT using (1) sim-ple translation probabilities (in our experiments,765from quick IBM-1 training), to replace the mono-lingual context vector model in MEANT, and (2)constraints from BITGs (bracketing ITGs).
Weshow that XMEANT assesses MT adequacy moreaccurately than MEANT (as measured by correla-tion with human adequacy judgement) without theneed for expensive human reference translations inthe output language.2 Related Work2.1 MT evaluation metricsSurface-form oriented metrics such as BLEU (Pa-pineni et al, 2002), NIST (Doddington, 2002),METEOR (Banerjee and Lavie, 2005), CDER(Leusch et al, 2006), WER (Nie?en et al, 2000),and TER (Snover et al, 2006) do not correctly re-flect the meaning similarities of the input sentence.In fact, a number of large scale meta-evaluations(Callison-Burch et al, 2006; Koehn and Monz,2006) report cases where BLEU strongly dis-agrees with human judgments of translation ade-quacy.This has caused a recent surge of work to de-velop better ways to automatically measure MTadequacy.
Owczarzak et al (2007a,b) improvedcorrelation with human fluency judgments by us-ing LFG to extend the approach of evaluating syn-tactic dependency structure similarity proposed byLiu and Gildea (2005), but did not achieve highercorrelation with human adequacy judgments thanmetrics like METEOR.
TINE (Rios et al, 2011) isa recall-oriented metric which aims to preserve thebasic event structure but it performs comparablyto BLEU and worse than METEOR on correlationwith human adequacy judgments.
ULC (Gim?enezand M`arquez, 2007, 2008) incorporates severalsemantic features and shows improved correla-tion with human judgement on translation quality(Callison-Burch et al, 2007, 2008) but no workhas been done towards tuning an SMT system us-ing a pure form of ULC perhaps due to its expen-sive run time.
Similarly, SPEDE (Wang and Man-ning, 2012) predicts the edit sequence for match-ing the MT output to the reference via an inte-grated probabilistic FSM and PDA model.
Sagan(Castillo and Estrella, 2012) is a semantic textualsimilarity metric based on a complex textual en-tailment pipeline.
These aggregated metrics re-quire sophisticated feature extraction steps, con-tain several dozens of parameters to tune, and em-ploy expensive linguistic resources like WordNetFigure 1: Monolingual MEANT algorithm.or paraphrase tables; the expensive training, tun-ing, and/or running time makes them hard to in-corporate into the MT development cycle.2.2 The MEANT family of metricsMEANT (Lo et al, 2012), which is the weighted f-score over the matched semantic role labels of theautomatically aligned semantic frames and rolefillers, that outperforms BLEU, NIST, METEOR,WER, CDER and TER in correlation with humanadequacy judgments.
MEANT is easily portableto other languages, requiring only an automatic se-mantic parser and a large monolingual corpus inthe output language for identifying the semanticstructures and the lexical similarity between thesemantic role fillers of the reference and transla-tion.Figure 1 shows the algorithm and equations forcomputing MEANT.
q0i,jand q1i,jare the argumentof type j in frame i in MT and REF respectively.w0iand w1iare the weights for frame i in MT/REFrespectively.
These weights estimate the degree ofcontribution of each frame to the overall meaningof the sentence.
wpredand wjare the weights ofthe lexical similarities of the predicates and rolefillers of the arguments of type j of all frame be-tween the reference translations and the MT out-766Figure 2: Examples of automatic shallow semantic parses.
The input is parsed by a Chinese automaticshallow semantic parser.
The reference and MT output are parsed by an English automatic shallowsemantic parser.
There are no semantic frames for MT3 since the system decided to drop the predicate.put.
There is a total of 12 weights for the setof semantic role labels in MEANT as defined inLo and Wu (2011b).
For MEANT, they are de-termined using supervised estimation via a sim-ple grid search to optimize the correlation withhuman adequacy judgments (Lo and Wu, 2011a).For UMEANT (Lo and Wu, 2012), they are es-timated in an unsupervised manner using relativefrequency of each semantic role label in the refer-ences and thus UMEANT is useful when humanjudgments on adequacy of the development set areunavailable.si,predand si,jare the lexical similarities basedon a context vector model of the predicates androle fillers of the arguments of type j between thereference translations and the MT output.
Lo et al(2012) and Tumuluru et al (2012) described howthe lexical and phrasal similarities of the semanticrole fillers are computed.
A subsequent variant ofthe aggregation function inspired by Mihalcea etal.
(2006) that normalizes phrasal similarities ac-cording to the phrase length more accurately wasused in more recent work (Lo et al, 2013a; Loand Wu, 2013a; Lo et al, 2013b).
In this paper,we employ a newer version of MEANT that usesf-score to aggregate individual token similaritiesinto the composite phrasal similarities of seman-tic role fillers, as our experiments indicate this ismore accurate than the previously used aggrega-tion functions.Recent studies (Lo et al, 2013a; Lo and Wu,2013a; Lo et al, 2013b) show that tuning MT sys-tems against MEANT produces more robustly ad-equate translations than the common practice oftuning against BLEU or TER across different datagenres, such as formal newswire text, informalweb forum text and informal public speech.2.3 MT quality estimationEvaluating cross-lingual MT quality is similar tothe work of MT quality estimation (QE).
Broadlyspeaking, there are two different approaches toQE: surface-based and feature-based.Token-based QE models, such as those in Gan-drabur et al (2006) and Ueffing and Ney (2005)fail to assess the overall MT quality because trans-lation goodness is not a compositional property.
Incontrast, Blatz et al (2004) introduced a sentence-level QE system where an arbitrary threshold isused to classify the MT output as good or bad.The fundamental problem of this approach is thatit defines QE as a binary classification task ratherthan attempting to measure the degree of goodnessof the MT output.
To address this problem, Quirk(2004) related the sentence-level correctness of theQE model to human judgment and achieved a highcorrelation with human judgement for a small an-notated corpus; however, the proposed model doesnot scale well to larger data sets.Feature-based QE models (Xiong et al, 2010;He et al, 2011; Ma et al, 2011; Specia, 2011;Avramidis, 2012; Mehdad et al, 2012; Almaghoutand Specia, 2013; Avramidis and Popovi?c, 2013;Shah et al, 2013) throw a wide range of linguis-tic and non-linguistic features into machine learn-767Figure 3: Cross-lingual XMEANT algorithm.ing algorithms for predicting MT quality.
Al-though the feature-based QE system of Avramidisand Popovi?c (2013) slightly outperformed ME-TEOR on correlation with human adequacy judg-ment, these ?black box?
approaches typically lackrepresentational transparency, require expensiverunning time, and/or must be discriminatively re-trained for each language and text type.3 XMEANT: a cross-lingual MEANTLike MEANT, XMEANT aims to evaluate howwell MT preserves the core semantics, whilemaintaining full representational transparency.But whereas MEANT measures lexical similar-ity using a monolingual context vector model,XMEANT instead substitutes simple cross-linguallexical translation probabilities.XMEANT differs only minimally fromMEANT, as underlined in figure 3.
The sameweights obtained by optimizing MEANT againsthuman adequacy judgement were used forXMEANT.
The weights can also be estimated inunsupervised fashion using the relative frequencyof each semantic role label in the foreign input, asin UMEANT.To aggregate individual lexical translation prob-abilities into phrasal similarities between cross-lingual semantic role fillers, we compared two nat-ural approaches to generalizing MEANT?s methodof comparing semantic parses, as described below.3.1 Applying MEANT?s f-score withinsemantic role fillersThe first natural approach is to extend MEANT?sf-score based method of aggregating semanticparse accuracy, so as to also apply to aggregat-ing lexical translation probabilities within seman-tic role filler phrases.
However, since we are miss-ing structure information within the flat role fillerphrases, we can no longer assume an injectivemapping for aligning the tokens of the role fillersbetween the foreign input and the MT output.
Wetherefore relax the assumption and thus for cross-lingual phrasal precision/recall, we align each to-ken of the role fillers in the output/input stringto the token of the role fillers in the input/outputstring that has the maximum lexical translationprobability.
The precise definition of the cross-lingual phrasal similarities is as follows:ei,pred?
the output side of the pred of aligned frame ifi,pred?
the input side of the pred of aligned frame iei,j?
the output side of the ARG j of aligned frame ifi,j?
the input side of the ARG j of aligned frame ip(e, f) =?t (e|f) t (f |e)prece,f=?e?emaxf?fp(e, f)|e|rece,f=?f?fmaxe?ep(e, f)|f|si,pred=2 ?
precei,pred,fi,pred?
recei,pred,fi,predprecei,pred,fi,pred+ recei,pred,fi,predsi,j=2 ?
precei,j,fi,j?
recei,j,fi,jprecei,j,fi,j+ recei,j,fi,jwhere the joint probability p is defined as the har-monized the two directions of the translation tablet trained using IBM model 1 (Brown et al, 1993).prece,fis the precision and rece,fis the recall ofthe phrasal similarities of the role fillers.
si,predand si,jare the f-scores of the phrasal similaritiesof the predicates and role fillers of the argumentsof type j between the input and the MT output.3.2 Applying MEANT?s ITG bias withinsemantic role fillersThe second natural approach is to extendMEANT?s ITG bias on compositional reorder-ing, so as to also apply to aggregating lexicaltranslation probabilities within semantic role fillerphrases.
Addanki et al (2012) showed empiri-cally that cross-lingual semantic role reordering ofthe kind that MEANT is based upon is fully cov-ered within ITG constraints.
In Wu et al (2014),we extend ITG constraints into aligning the tokenswithin the semantic role fillers within monolingualMEANT, thus replacing its previous monolingualphrasal aggregation heuristic.
Here we borrow the768idea for the cross-lingual case, using the length-normalized inside probability at the root of a BITGbiparse (Wu, 1997; Zens and Ney, 2003; Saers andWu, 2009) as follows:G ?
?
{A} ,W0,W1,R,A?R ?
{A ?
[AA] ,A ?
?AA?,A ?
e/f}p ([AA] |A) = p (?AA?|A) = 0.25p (e/f |A) =12?t (e|f) t (f |e)si,pred=11?ln(P(A??ei,pred/fi,pred|G))max(|ei,pred|,|fi,pred|)si,j=11?ln(P(A?
?ei,j/fi,j|G))max(|ei,j|,|fi,j|)where G is a bracketing ITG, whose only nonter-minal is A, and where R is a set of transductionrules where e ?
W0?
{?}
is an output token(or the null token), and f ?
W1?
{?}
is an in-put token (or the null token).
The rule probabil-ity function p is defined using fixed probabilitiesfor the structural rules, and a translation table ttrained using IBM model 1 in both directions.
Tocalculate the inside probability of a pair of seg-ments, P(A??
e/f|G), we use the algorithm de-scribed in Saers et al (2009).
si,predand si,jarethe length normalized BITG parsing probabilitiesof the predicates and role fillers of the argumentsof type j between the input and the MT output.4 ResultsTable 1 shows that for human adequacy judgmentsat the sentence level, the f-score based XMEANT(1) correlates significantly more closely than othercommonly used monolingual automatic MT eval-uation metrics, and (2) even correlates nearly aswell as monolingual MEANT.
This suggests thatthe semantic structure of the MT output is indeedcloser to that of the input sentence than that of thereference translation.Furthermore, the ITG-based XMEANT (1) sig-nificantly outperforms MEANT, and (2) is an au-tomatic metric that is nearly as accurate as theHMEANT human subjective version.
This indi-cates that BITG constraints indeed provide a morerobust token alignment compared to the heuris-tics previously employed in MEANT.
It is alsoconsistent with results observed while estimatingword alignment probabilities, where BITG con-straints outperformed alignments from GIZA++(Saers and Wu, 2009).Table 1: Sentence-level correlation with HAJ(GALE phase 2.5 evaluation data)Metric KendallHMEANT 0.53XMEANT (BITG) 0.51MEANT (f-score) 0.48XMEANT (f-score) 0.46MEANT (2013) 0.46NIST 0.29BLEU/METEOR/TER/PER 0.20CDER 0.12WER 0.105 ConclusionWe have presented XMEANT, a new cross-lingualvariant of MEANT, that correlates even moreclosely with human translation adequacy judg-ments thanMEANT, without the expensive humanreferences.
This is (1) accomplished by replacingmonolingual MEANT?s context vector model withsimple translation probabilities when computingsimilarities of semantic role fillers, and (2) fur-ther improved by incorporating BITG constraintsfor aligning the tokens in semantic role fillers.While monolingual MEANT alone accurately re-flects adequacy via semantic frames and optimiz-ing SMT against MEANT improves translation,the new cross-lingual XMEANT semantic objec-tive function moves closer toward deep integrationof semantics into the MT training pipeline.The phrasal similarity scoring has only beenminimally adapted to cross-lingual semantic rolefillers in this first study of XMEANT.
We expectfurther improvements to XMEANT, but these firstresults already demonstrate XMEANT?s potentialto drive research progress toward semantic SMT.6 AcknowledgmentsThis material is based upon work supportedin part by the Defense Advanced ResearchProjects Agency (DARPA) under BOLT con-tract nos.
HR0011-12-C-0014 and HR0011-12-C-0016, and GALE contract nos.
HR0011-06-C-0022 and HR0011-06-C-0023; by the EuropeanUnion under the FP7 grant agreement no.
287658;and by the Hong Kong Research Grants Council(RGC) research grants GRF620811, GRF621008,and GRF612806.
Any opinions, findings andconclusions or recommendations expressed in thismaterial are those of the authors and do not nec-essarily reflect the views of DARPA, the EU, orRGC.769ReferencesKarteek Addanki, Chi-Kiu Lo, Markus Saers, andDekai Wu.
LTG vs. ITG coverage of cross-lingualverb frame alternations.
In 16th Annual Conferenceof the European Association for Machine Transla-tion (EAMT-2012), Trento, Italy, May 2012.Hala Almaghout and Lucia Specia.
A CCG-based qual-ity estimation metric for statistical machine transla-tion.
In Machine Translation Summit XIV (MT Sum-mit 2013), Nice, France, 2013.Eleftherios Avramidis and Maja Popovi?c.
Machinelearning methods for comparative and time-orientedquality estimation of machine translation output.
In8th Workshop on Statistical Machine Translation(WMT 2013), 2013.Eleftherios Avramidis.
Quality estimation for machinetranslation output using linguistic analysis and de-coding features.
In 7th Workshop on Statistical Ma-chine Translation (WMT 2012), 2012.Satanjeev Banerjee and Alon Lavie.
METEOR: Anautomatic metric for MT evaluation with improvedcorrelation with human judgments.
In Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, Ann Ar-bor, Michigan, June 2005.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
Confidence estimationfor machine translation.
In 20th international con-ference on Computational Linguistics, 2004.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
The mathemat-ics of machine translation: Parameter estimation.Computational Linguistics, 19(2):263?311, 1993.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
Re-evaluating the role of BLEU in machinetranslation research.
In 11th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics (EACL-2006), 2006.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
(meta-)evaluation of machine translation.
In Second Work-shop on Statistical Machine Translation (WMT-07),2007.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
Furthermeta-evaluation of machine translation.
In ThirdWorkshop on Statistical Machine Translation (WMT-08), 2008.Julio Castillo and Paula Estrella.
Semantic textual sim-ilarity for MT evaluation.
In 7th Workshop on Sta-tistical Machine Translation (WMT 2012), 2012.George Doddington.
Automatic evaluation of machinetranslation quality using n-gram co-occurrencestatistics.
In The second international conference onHuman Language Technology Research (HLT ?02),San Diego, California, 2002.Simona Gandrabur, George Foster, and Guy Lapalme.Confidence estimation for nlp applications.
ACMTransactions on Speech and Language Processing,2006.Jes?us Gim?enez and Llu?
?s M`arquez.
Linguistic featuresfor automatic evaluation of heterogenous MT sys-tems.
In Second Workshop on Statistical MachineTranslation (WMT-07), pages 256?264, Prague,Czech Republic, June 2007.Jes?us Gim?enez and Llu?
?s M`arquez.
A smorgasbordof features for automatic MT evaluation.
In ThirdWorkshop on Statistical Machine Translation (WMT-08), Columbus, Ohio, June 2008.Yifan He, Yanjun Ma, Andy Way, and Josef vanGenabith.
Rich linguistic features for translationmemory-inspired consistent translation.
In 13th Ma-chine Translation Summit (MT Summit XIII), 2011.Philipp Koehn and Christof Monz.
Manual and auto-matic evaluation of machine translation between eu-ropean languages.
In Workshop on Statistical Ma-chine Translation (WMT-06), 2006.Gregor Leusch, Nicola Ueffing, and Hermann Ney.CDer: Efficient MT evaluation using block move-ments.
In 11th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL-2006), 2006.Ding Liu and Daniel Gildea.
Syntactic features forevaluation of machine translation.
In Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, Ann Ar-bor, Michigan, June 2005.Chi-kiu Lo and Dekai Wu.
MEANT: An inexpensive,high-accuracy, semi-automatic metric for evaluatingtranslation utility based on semantic roles.
In 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies(ACL HLT 2011), 2011.Chi-kiu Lo and Dekai Wu.
SMT vs. AI redux: Howsemantic frames evaluate MT more accurately.
InTwenty-second International Joint Conference onArtificial Intelligence (IJCAI-11), 2011.Chi-kiu Lo and Dekai Wu.
Unsupervised vs. super-vised weight estimation for semantic MT evalua-tion metrics.
In Sixth Workshop on Syntax, Seman-tics and Structure in Statistical Translation (SSST-6), 2012.Chi-kiu Lo and Dekai Wu.
Can informal genres bebetter translated by tuning on automatic semanticmetrics?
In 14th Machine Translation Summit (MTSummit XIV), 2013.Chi-kiu Lo and Dekai Wu.
MEANT at WMT 2013:A tunable, accurate yet inexpensive semantic framebased mt evaluation metric.
In 8th Workshop on Sta-tistical Machine Translation (WMT 2013), 2013.Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu.Fully automatic semantic MT evaluation.
In 7thWorkshop on Statistical Machine Translation (WMT2012), 2012.Chi-kiu Lo, Karteek Addanki, Markus Saers, andDekai Wu.
Improving machine translation by train-ing against an automatic semantic frame based eval-770uation metric.
In 51st Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2013),2013.Chi-kiu Lo, Meriem Beloucif, and Dekai Wu.
Im-proving machine translation into Chinese by tun-ing against Chinese MEANT.
In InternationalWorkshop on Spoken Language Translation (IWSLT2013), 2013.Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-abith.
Consistent translation using discriminativelearning: a translation memory-inspired approach.In 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies (ACL HLT 2011).
Association for Computa-tional Linguistics, 2011.Matou?s Mach?a?cek and Ond?rej Bojar.
Results of theWMT13 metrics shared task.
In Eighth Workshop onStatistical Machine Translation (WMT 2013), Sofia,Bulgaria, August 2013.Yashar Mehdad, Matteo Negri, and Marcello Federico.Match without a referee: evaluating mt adequacywithout reference translations.
In 7th Workshop onStatistical Machine Translation (WMT 2012), 2012.Rada Mihalcea, Courtney Corley, and Carlo Strappar-ava.
Corpus-based and knowledge-based measuresof text semantic similarity.
In The Twenty-first Na-tional Conference on Artificial Intelligence (AAAI-06), volume 21.
Menlo Park, CA; Cambridge, MA;London; AAAI Press; MIT Press; 1999, 2006.Sonja Nie?en, Franz Josef Och, Gregor Leusch, andHermann Ney.
A evaluation tool for machine trans-lation: Fast evaluation for MT research.
In TheSecond International Conference on Language Re-sources and Evaluation (LREC 2000), 2000.Karolina Owczarzak, Josef van Genabith, and AndyWay.
Dependency-based automatic evaluation formachine translation.
In Syntax and Structure in Sta-tistical Translation (SSST), 2007.Karolina Owczarzak, Josef van Genabith, and AndyWay.
Evaluating machine translation with LFG de-pendencies.
Machine Translation, 21:95?119, 2007.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
BLEU: a method for automatic evalua-tion of machine translation.
In 40th Annual Meet-ing of the Association for Computational Linguistics(ACL-02), pages 311?318, Philadelphia, Pennsylva-nia, July 2002.Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Dan Jurafsky.
Shallow seman-tic parsing using support vector machines.
In Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL 2004), 2004.Christopher Quirk.
Training a sentence-level machinetranslation confidence measure.
In Fourth Interna-tional Conference on Language Resources and Eval-uation (LREC 2004), Lisbon, Portugal, May 2004.Miguel Rios, Wilker Aziz, and Lucia Specia.
Tine: Ametric to assess MT adequacy.
In Sixth Workshop onStatistical Machine Translation (WMT 2011), 2011.Markus Saers and Dekai Wu.
Improving phrase-basedtranslation via word alignments from stochastic in-version transduction grammars.
In Third Workshopon Syntax and Structure in Statistical Translation(SSST-3), Boulder, Colorado, June 2009.Markus Saers, Joakim Nivre, and Dekai Wu.
Learningstochastic bracketing inversion transduction gram-mars with a cubic time biparsing algorithm.
In 11thInternational Conference on Parsing Technologies(IWPT?09), Paris, France, October 2009.Kashif Shah, Trevor Cohn, and Lucia Specia.
An in-vestigation on the effectiveness of features for trans-lation quality estimation.
In Machine TranslationSummit XIV (MT Summit 2013), Nice, France, 2013.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
A study of trans-lation edit rate with targeted human annotation.
In7th Biennial Conference Association for MachineTranslation in the Americas (AMTA 2006), pages223?231, Cambridge, Massachusetts, August 2006.Lucia Specia.
Exploiting objective annotations formeasuring translation post-editing effort.
In 15thConference of the European Association for Ma-chine Translation, pages 73?80, 2011.Anand Karthik Tumuluru, Chi-kiu Lo, and Dekai Wu.Accuracy and robustness in measuring the lexicalsimilarity of semantic role fillers for automatic se-mantic MT evaluation.
In 26th Pacific Asia Confer-ence on Language, Information, and Computation(PACLIC 26), 2012.Nicola Ueffing and Hermann Ney.
Word-level con-fidence estimation for machine translation usingphrase-based translation models.
In Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Process-ing, pages 763?770, 2005.Mengqiu Wang and Christopher D. Manning.
SPEDE:Probabilistic edit distance metrics for MT evalua-tion.
In 7th Workshop on Statistical Machine Trans-lation (WMT 2012), 2012.Dekai Wu, Chi-kiu Lo, Meriem Beloucif, and MarkusSaers.
IMEANT: Improving semantic frame basedMT evaluation via inversion transduction grammars.Forthcoming, 2014.Dekai Wu.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
Computa-tional Linguistics, 23(3):377?403, 1997.Deyi Xiong, Min Zhang, and Haizhou Li.
Error detec-tion for statistical machine translation using linguis-tic features.
In 48th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL 2010),2010.Richard Zens and Hermann Ney.
A comparativestudy on reordering constraints in statistical machinetranslation.
In 41st Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-2003),pages 144?151, Stroudsburg, Pennsylvania, 2003.771
