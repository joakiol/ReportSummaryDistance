A Dynamic  Language Mode l  for Speech  Recogn i t ionF.
Jelinek, B. Merialdo, S. Roukos, and M. St rauss  IIBM Research Division, Thomas J. Watson Research Center,Yorktown Heights, NY 10598ABSTRACTIn the case of a trlgr~m language model, the proba-bility of the next word conditioned on the previous twowords is estimated from a large corpus of text.
The re-sulting static trigram language model (STLM) has fixedprobabilities that are independent of the document beingdictated.
To improve the language mode\] (LM), one canadapt the probabilities of the trigram language model tomatch the current document more closely.
The partiallydictated document provides significant clues about whatwords ~re more likely to be used next.
Of many meth-ods that can be used to adapt the LM, we describe in thispaper a simple model based on the trigram frequencies es-timated from the partially dictated ocument.
We call thismodel ~ cache trigram language model (CTLM) since weare c~chlng the recent history of words.
We have foundthat the CTLM red,aces the perplexity of a dictated doc-ument by 23%.
The error rate of a 20,000-word isolatedword recognizer decreases by about 5% at the beginning ofa document and by about 24% after a few hundred words.INTRODUCTIONA language model is used in speech recognitionsystems and automatic translation systems to improvethe performance of such systems.
A trigram languagemodel \[1\], whose parameters are estimated from alarge corpus of text (greater than a few million words),has been used successfully in both applications.
Thetrigram language model has a probability distribu-tion for the next word conditioned on the previoustwo words.
This static distribution is obtained a.s anaverage over many documents.
Yet we know that sev-I Now at Rutgers University, N J, work performed while vis-iting IBMeral words are bursty by nature, i.e., one expects theword "language" to occur in this paper at a signif-icantly higher rate than the average frequency esti-mated from a large collection of text.
To capture the"dynamic" nature of the trigram probabilities in aparticular document, we present a "cache;' trigramlanguage model (CTLM) that uses a window of the nmost recent words to determine the probability dis-tribution of the next word.The idea of using a window of the recent history toadjust the LM probabilities was proposed in \[2, 3\].
In\[2\] tile dynamic component adjusted the conditionalprobability, p , (w,+l  \] g,+l),  of the next word, wn+l,given a predicted part-oPspeech (POS), g,~+l, in a tri-part-of-speech language model.
Each POS had a sep-arate cache where the frequencies of all the words thatoccurred with a POS determine the dynamic compo-nent of the language model.
As a word is observed itis tagged and the appropriate POS cache is updated.At least 5 occurrences per cache are required beforeactivating it.
P~eliminary results for a couple POScaches indicate that with appropriate smoothing theperplexity, for example, on NN-words is decreased bya factor of 2.5 with a cache-based conditional wordprobability given the POS category instead of a staticprobability model.In \[3\], the dynamic model uses two bigram lan-guage models, p,(w,.+l \] w,,  D), where D=I  for wordsw,+l that have occurred in the cache window andD=0 for words that have occurred in the cache win-dow.
For cache sizes from 128 to 4096, the reportedres,lts indicate an improvement in the average rankof tile correct word predicted by the model by 7% to293TestSetABCstatic dynalnicPerplexity Perplexity9i 7 553 49262 202~c0.070.120.07Table 1: Perplexity of static and dynamic language mod-els.Perplexity 262 217Table 2: Perplexity ~s ~ function of cache size on test setC.Static Unigram TrigramCache Cache262 230 202Table 3: Perplexity of unigram and trigram caches.17% over the static model assuming one knows if thenext word is in the cache or not.In this paper, we will present a new cache lan-guage model and compare its performance to a tri-gram language model.
In Section 2, we present ourproposed ynamic omponent and some results com-paring static and dynamic trigram language modelsusing perplexity.
In Section 3, we present our methodfor incorporating the dynamic language model in anisolated 20,000 word speech recognizer and its effecton recognition performance.CACHE LANGUAGE MODELUsing a window of the n most recent words, we canestimate a unigram frequency distribution f , (w,+l) ,a bigram frequency distribution, fn(w,?l I w,), and atrigram frequency distribution, f , (w,+l  } w,,w,-1) .The resulting 3dynamic estimators are linearly smoothedtogether to obtain a dynamic trigram model denotedby pc,(w,+l I w,., w,-a).
The dynamic trigram modelassigns a non-zero probability for the words that haveoccurred in the window of the previous n words.
Sincethe next word may not be in the cache and since thecache contains very few trigrams, we interpolate lin-early the dynamic model with the the static trigramlanguage model:I = (1)\[ w,,, w._ l )  +(1  - I w,,.,w,,.-1)where p,(..
.)
is the usual static trigram language model.We use the forward-backward algorithm to estimatethe interpolation parameter ),c \[1\].
This parametervaries between 0.07 and 0.28 depending on the par-ticular static trigram language model (we used tri-gram language models estimated from different sizecorpora) and the cache size (varying from 200 to 1000words.
)We have evaluated this cache language model bycomputing the perplexity on three test sets:?
Test sets A and B are each about 100k wordsof text that were excised from a corpus of docu-ments from an insurance company that was usedfor building the static trigram language modelfor a 20,000-word vocabulary.?
Test set C which consists of 7 documents (about4000 words) that were dictated in a field trialin the same insurance company on TANGORA(the 20,000-word isolated word recognizer devel-oped at IBM.
)Table \] shows the perplexity of the static and dy-namic language models for the three test sets.
Thecache size was 1000 words and was updated wordsynchronously.
The static language model was esti-mated from about 1.2 million words of insurance doc-uments.
The dynamic language model yields from 8%to 23% reduction in perplexity, with the larger reduc-tion occurring with the test sets with larger perplex-ity.
The interpolation weight Ac was estimated usingset B when testing on sets A and C and set.
A whentesting on set B.
Table 2 shows the effect of cache sizeon perplexity where it appears that a larger cache ismore useful.
These results were on test set C. Ontest set C, the rate that the next word is in the cacheranges from 75% for a cache window of 200 words to83% for a window of 1000.
Table 3 compares a cachewith unigrams only with a full trigram cache (for thetrigram cache, the weights for the unigram, bigram,and trigram frequencies were 0.25, 0.25, 0.5 respec-tively and were selected by hand.)
A second set ofweights (0.25,0.5,0.25) produced a perplexity of 190for the trigram cache.
In all the above experiments,the cache was not flushed between documents.
In thenext section, we compare the different models in anisolated speech recognition experiment.We have tried using a fancier interpolation schemewhere the reliance on the cache depends on the cur-294Text 0-100Length% Reductionin Error 6.1%Rate100-200 200-300 300-400 400-500 500-8005.3% 4.7% 10.5% 16.3% 23.8%Table 4: Percentage r duction in error rate with trigr~m cache.rent word wn with the expectation that some wordswill tend to be followed by bursty words whereas otherwords will tend to be followed by non-bursty words.We typically used about 50 buckets (or weighting pa-rameters).
However, we have found that the perplex-ity on independent data to be no better than the singleparameter interpolation.ISOLATED SPEECH RECOGNIT IONWe incorporated the cache language model intothe TANGORA isolated speech recognition system.We evaluated two cache update strategies.
In the firstone, the cache is updated at the end of every utter-ance, i.e., when the speaker turns off the microphone.An utterance may be a partial sentence or a completesentence or several sentences depending on how thespeaker dictated the document.
In the second strat-egy, the cache is updated as soon as the recognizermakes a decision about what was spoken.
This typ-ically corresponds to a delay of about 3 words.
Thecache is updated with the correct ext which requiresthat the speaker correct any errors that may occur.This may be unduly difficult with the second updatestrategy.
But in the context of our experiments, wehave found that using the simpler (and more realistic)update strategy, i.e., after an utterance is completed,to be as effective as the more elaborate update strat-egy.The TANGORA system uses a 20,000-word officecorrespondence vocabulary with a trigram languagemodel estimated from a few hundred million wordsfrom several sources.
The cache language model wastested on a set of 14 documents dictated by 5 speak-ers with an internal telephone system (private branchexchange.)
The speakers were form the speech grouptypically dictating electronic mail messages or inter-nal memoranda.
The size of a document ranged fromabout 120 words to 800 words.
The total test cor-pus was about.
5000 words.
The maximum cache size(4000 words) was \]a.rger than any of the documents.In these tests, the cache is flushed at the beginning ofeach document.In these experiments, the weights for interpolatingthe dynamic unigram, bigram, and trigram hequen-cies were 0.4, 0.5, and 0.1, respectively.
The weight ofthe cache probability, At, relative to the static trigramprobability was 0.2.
Small changes in this weight doesnot seem to affect recognition performance.
The po-tential benefit of a cache depends on the amount oftext that has been observed.
Table 4 shows the per-centage reduction in error rate as a function of thelength of the observed text.
We divided the docu-ments into 100-word bins and computed the error ratein each bin.
For the static language model, the errorrate should be constant except for statistical fluctua-tions, whereas one expects that the error rate of thecache to decrease with longer documents.
As can beseen from Table 4, the cache reduces the error rate byabout 5% for shorter documents and up to 24% forlonger documents.
The trigram cache results in anaverage reduction in error rate of 10% for these doc-uments whose average size is about 360 words.
Thetrigram cache is very slightly better than a unigramcache eventhough the earlier results using perplexityas a measure of performance indicated a bigger differ-ence between the two caches.REFERENCES\[l\] Bahl, L., Jelinek, F., and Mercer, R.,A Statisti-cal Approach to Continuous Speech Recognition,IEEE Trans.
on PAMI, 1983.\[2\] Kuhn, R., Speech Recognition and the Frequencyo\] Recently Used Words: a Modified MarkovModel for Natural Language, Proceedings ofCOLING B,dapest, Vol.
1, pp.
348-350, 1988.Vol.
I July 1988\[3\] Kupiec, J. Probabilistie Models of Short and LongDistance Word Dependencies in Running Text,Proceedings of Speech and Natural LanguageDARPA Workshop, pp.
290-295, Feb. 1989.295
