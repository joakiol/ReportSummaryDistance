Two Ways to Use a Noisy Parallel News corpus for improving StatisticalMachine TranslationSouhir Gahbiche-Braham He?le`ne Bonneau-MaynardUniversite?
Paris-Sud 11LIMSI-CNRS91403 Orsay, France{souhir,hbm,yvon}@limsi.frFranc?ois YvonAbstractIn this paper, we present two methods to usea noisy parallel news corpus to improve sta-tistical machine translation (SMT) systems.Taking full advantage of the characteristics ofour corpus and of existing resources, we usea bootstrapping strategy, whereby an existingSMT engine is used both to detect parallel sen-tences in comparable data and to provide anadaptation corpus for translation models.
MTexperiments demonstrate the benefits of vari-ous combinations of these strategies.1 IntroductionIn Statistical Machine Translation (SMT), systemsare created from parallel corpora consisting of a setof source language texts aligned with its translationin the target language.
Such corpora however onlyexist (at least are publicly documented and avail-able) for a limited number of domains, genres, reg-isters, and language pairs.
In fact, there are a fewlanguage pairs for which parallel corpora can be ac-cessed, except for very narrow domains such as po-litical debates or international regulatory texts.
An-other very valuable resource for SMT studies, espe-cially for under-resource languages, are comparablecorpora, made of pairs of monolingual corpora thatcontain texts of similar genres, from similar periods,and/or about similar topics.The potential of comparable corpora has longbeen established as a useful source from which toextract bilingual word dictionaries (see eg.
(Rapp,1995; Fung and Yee, 1998)) or to learn multilingualterms (see e.g.
(Lange?, 1995; Smadja et al, 1996)).More recently, the relative corpus has caused theusefulness of comparable corpora be reevaluated asa potential source of parallel fragments, be theyparagraphs, sentences, phrases, terms, chunks, orisolated words.
This tendency is illustrated by thework of e.g.
(Resnik and Smith, 2003; Munteanuand Marcu, 2005), which combines Information Re-trieval techniques (to identify parallel documents)and sentence similarity detection to detect parallelsentences.There are many other ways to improve SMT mod-els with comparable or monolingual data.
For in-stance, the work reported in (Schwenk, 2008) drawsinspiration from recent advances in unsupervisedtraining of acoustic models for speech recognitionand proposes to use self-training on in-domain datato adapt and improve a baseline system trainedmostly with out-of-domain data.As discussed e.g.
in (Fung and Cheung, 2004),comparable corpora are of various nature: there ex-ists a continuum between truly parallel and com-pletely unrelated texts.
Algorithms for exploitingcomparable corpora should thus be tailored to thepeculiarities of the data on which they are applied.In this paper, we report on experiments aimed atusing a noisy parallel corpus made out of news sto-ries in French and Arabic in two different ways: first,to extract new, in-domain, parallel sentences; sec-ond, to adapt our translation and language models.This approach is made possible due to the specifici-ties of our corpus.
In fact, our work is part of aproject aiming at developing a platform for process-ing multimedia news documents (texts, interviews,images and videos) in Arabic, so as to streamline the44Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 44?51,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguisticswork of a major international news agency.
As partas the standard daily work flow, a significant por-tion of the French news are translated (or adapted)in Arabic by journalists.
Having access to one fullyear of the French and Arabic corpus (consisting, todate, of approximately one million stories (150 mil-lion words)), we have in our hands an ideal compa-rable resource to perform large scale experiments.These experiments aim at comparing variousways to build an accurate machine translation sys-tem for the news domain using (i) a baseline systemtrained mostly with out-of-domain data (ii) the com-parable dataset.
As will be discussed, given the verylarge number of parallel news in the data, our bestoption seems to reconstruct an in-domain trainingcorpus of automatically detected parallel sentences.The rest of this paper is organized as follows.In Section 2, we relate our work to some exist-ing approaches for using comparable corpora.
Sec-tion 3 presents our methodology for extracting par-allel sentences, while our phrase-table adaptationstrategies are described in Section 4.
In Section 5,we describe our experiments and contrast the resultsobtained with several adaptation strategies.
Finally,Section 6 concludes the paper.2 Related workFrom a bird?s eye view, attempts to use comparablecorpora in SMT fall into two main categories: first,approaches aimed at extracting parallel fragments;second, approaches aimed at adapting existing re-sources to a new domain.2.1 Extracting parallel fragmentsMost attempts at automatically extracting parallelfragments use a two step process (see (Tillmann andXu, 2009) for a counter-example): a set of candidateparallel texts is first identified; within this short listof possibly paired texts, parallel sentences are thenidentified based on some similarity score.The work reported in (Zhao and Vogel, 2002) con-centrates on finding parallel sentences in a set ofcomparable stories pairs in Chinese/English.
Sen-tence similarity derives from a probabilistic align-ment model for documents, which enables to recog-nize parallel sentences based on their length ratio,as well as on the IBM 1 model score of their word-to-word alignment.
To account for various levels ofparallelism, the model allows some sentences in thesource or target language to remain unaligned.The work of (Resnik and Smith, 2003) considersmining a much larger ?corpora?
consisting of docu-ments collected on the Internet.
Matched documentsand sentences are primarily detected based on sur-face and/or formal similarity of the web addressesor of the page internal structure.This line of work is developed notably in(Munteanu and Marcu, 2005): candidate paralleltexts are found using Cross-Lingual Information Re-trieval (CLIR) techniques; sentence similarity is in-directly computed using a logistic regression modelaimed at detecting parallel sentences.
This formal-ism allows to enrich baseline features such as thelength ratio, the word-to-word (IBM 1) alignmentscores with supplementary scores aimed at reward-ing sentences containing identical words, etc.
Morerecently, (Smith et al, 2010) reported significant im-provements mining parallel Wikipedia articles us-ing more sophisticated indicators of sentence par-allelism, incorporating a richer set of features andcross-sentence dependencies within a ConditionalRandom Fields (CRFs) model.
For lack of findenough parallel sentences, (Munteanu and Marcu,2006; Kumano and Tokunaga, 2007) consider themore difficult issue of mining parallel phrases.In (Abdul-Rauf and Schwenk, 2009), the authors,rather than computing a similarity score between asource and a target sentence, propose to use an ex-isting translation engine to process the source sideof the corpus, thus enabling sentence comparison tobe performed in the target language, using the editdistance or variants thereof (WER or TER).
Thisapproach is generalized to much larger collectionsin (Uszkoreit et al, 2010), which draw advantageof working in one language to adopt efficient paral-lelism detection techniques (Broder, 2000).2.2 Comparable corpora for adaptationAnother very productive use of comparable corporais to adapt or specialize existing resources (dictio-naries, translation models, language models) to spe-cific domains and/or genres.
We will only focus hereon adapting the translation model; a review of theliterature on language model adaptation is in (Bella-garda, 2001) and the references cited therein.45Figure 1: Extraction of parallel corporaThe work in (Snover et al, 2008) is a first steptowards augmenting the translation model with newtranslation rules: these rules associate, with a tinyprobability, every phrase in a source document withthe most frequent target phrases found in a compa-rable corpus specifically built for this document.The study in (Schwenk, 2008) considers self-training, which allows to adapt an existing systemto new domains using monolingual (source) data.The idea is to automatically translate the source sideof an in-domain corpus using a reference translationsystem.
Then, according to some confidence score,the best translations are selected to form an adap-tation corpus, which can serve to retrain the trans-lation model.
The authors of (Cettolo et al, 2010)follow similar goals with different means: here, thebaseline translation model is used to obtain a phrasealignment between source and target sentences ina comparable corpus.
These phrase alignments arefurther refined, before new phrases not in the origi-nal phrase-table, can be collected.The approaches developed below borrow fromboth traditions: given (i) the supposed high degreeof parallelism in our data and (ii) the size of theavailable comparable data, we are in a position toapply any of the above described technique.
Thisis all the easier to do as all stories are timestamped,which enables to easily spot candidate parallel texts.In both cases, we will apply a bootstrapping strat-egy using as baseline a system trained with out-of-domain data.3 Extracting Parallel CorporaThis section presents our approach for extracting aparallel corpus from a comparable in-domain cor-pora so as to adapt a SMT system to a specific do-main.
Our methodology assumes that both a base-line out-of-domain translation system and a compa-rable in-domain corpus are available, two require-ments that are often met in practice.As shown in Figure 1, our approach for extractingan in-domain parallel corpus from the in-domaincomparable corpus consists in 3 steps and closelyfollows (Abdul-Rauf and Schwenk, 2009):translation: translating the source side of thecomparable corpora;document pairs selection : selecting, in the com-parable corpus, documents that are similar to thetranslated output;sentence pairs selection : selecting parallel sen-tences among the selected documents.The main intuition is that computing documentsimilarities in one language enables to use simpleand effective comparison procedures, instead of hav-ing to define ad hoc similarities measures based oncomplex underlying alignment models.The translation step consists here in translatingthe source (Arabic) side of the comparable corpususing a baseline out-of-domain system, which hasbeen trained on parallel out-of-domain data.The document selection step consists in tryingto match the automatic translations (source:target)with the original documents in the target language.For each (source:target) document, a similarity scorewith all the target documents is computed.
We con-tend here with a simple association score, namelythe Dice coefficient, computed as the number ofwords in common in both documents, normalized bythe length of the (source:target) document.A priori knowledge, such as the publication dates46of the documents, are used to limit the number ofdocument pairs to be compared.
For each sourcedocument, the target document that has the bestscore is then selected as a potential parallel docu-ment.
The resulting pairs of documents are then fil-tered depending on a threshold Td, so as to avoidfalse matches (in the experiments described below,the threshold has been set so as to favor precisionover recall).At the end of this step, a set of similar sourceand target document pairs has been selected.
Thesepairs may consist in documents that are exact trans-lations of each other.
In most cases, the documentsare noisy translation and only a subset of their sen-tences are mutual translation.The sentence selection step then consists in per-forming a sentence level alignment of each pair ofdocuments to select a set of parallel sentences.
Sen-tence alignment is then performed with the hunalignsentence alignment tool (Varga et al, 2005), whichalso provides alignment confidence measures.
Asfor the document selection step, only sentence pairsthat obtain an alignment score greater than a prede-fined threshold Ts are selected, where Ts is againchosen to favor prevision of alignments of recall.From these, 1 : 1 alignments are retained, yieldinga small, adapted, parallel corpus.
This method isquite different from (Munteanu and Marcu, 2005)?swork where the sentence selection step is done by aMaximum Entropy classifier.4 Domain AdaptationIn the course of mining our comparable corpus, wehave produced a translation into French for all thesource language news stories.
This means that wehave three parallel corpora at our disposal:?
The baseline training corpus, which is large(a hundred million words), delivering a reason-able translation performance quality of transla-tion, but out-of-domain;?
The extracted in-domain corpus, which ismuch smaller, and potentially noisy;?
The translated in-domain corpus, which is ofmedium-size, and much worse in quality thanthe others.Considering these three corpora, different adapta-tion methods of the translation models are explored.The first approach is to concatenate the baseline andin-domain training data (either extracted or trans-lated) to train a new translation model.
Given thedifference in size between the two corpus, this ap-proach may introduce a bias in the translation modelin favor of out-of-domain.The second approach is to train separate transla-tion models with baseline on the one hand, and within-domain on the other data and to weight their com-bination with MERT (Och, 2003).
This alleviatesthe former problem but increases the number of fea-tures that need to be trained, running the risk to makeMERT less stable.A last approach is also considered, which consistsin using only the in-domain data to train the trans-lation model.
In that case, the question is the smallsize of the in-domain data.The comparative experiments on the three ap-proaches, using the three corpora are described innext section.5 Experiments and results5.1 Context and dataThe experiments have been carried out in the con-text of the Cap Digital SAMAR1 project which aimsat developping a platform for processing multimedianews in Arabic.
Every day, about 250 news in Ara-bic, 800 in French and in English2 are produced andaccumulated on our disks.
News collected from De-cember 2009 to December 2010 constitute the com-parable corpora, containing a set of 75,975 news forthe Arabic part and 288,934 news for the French part(about 1M sentences for Arabic and 5M sentencesfor French).The specificity of this comparable corpus is thatmany Arabic stories are known to be translation ofnews that were first written in French.
The transla-tions may not be entirely faithful: when translatinga story, the journalist is in fact free to rearrange thestructure, and to some extend, the content of a doc-ument (see example Figure 2).In our experiments, the in-domain comparablecorpus then consists in a set of Arabic and French1http://www.samar.fr2The English news have not been used in this study.47Arabic:?AJJ?
@ 	??
AJKY?
?KA?
B ?A?g ??
?m' 	?A?
@?I?DK @ ???
@???J?
@ 	???????
@ ??k?Q??AJ.
??
@Q?
HA??A???
@.
?
?AJK AJK A??
??K?
@ ?
?Ag ???@?
A?D?
@And he added, we in Hamas don?t have a problem toresume indirect negotiations about the deal from thepoint at which it ended and at which Netanyahu triedto fail.French: Le porte-parole a re?affirme?
que le Hamas e?taitpre?t a` reprendre les tractations au point ou` elles s?e?taientarre?te?es.The spokesman reaffirmed that Hamas was ready toresume negotiations at the point where they stopped.Figure 2: An example of incorrect/inexact transla-tion in a pair of similar documents.documents which are parallel, partly parallel, or notparallel at all, with no explicit link between Arabicand French parts.5.2 Baseline translation systemThe baseline out-of-domain translation system wastrained on a corpus of 7.6 million of parallel sen-tences (see Table 1), that was harvested from pub-licly available sources on the web: the United Na-tions (UN) document database, the website of theWorld Health Organization (WHO) and the ProjectSyndicate Web site.
The ?UN?
data constitutes byfar the largest portion of this corpus, from whichonly the Project Syndicate documents can be con-sidered as appropriate for the task at hand.A 4-gram backoff French language model wasbuilt on 2.4 billion words of running texts, takenfrom the parallel data, as well as notably the Giga-word French corpus.ar frCorpus #tokens voc #tokens vocbaseline 162M 369K 186M 307Kextracted 3.6M 72K 4.0M 74Ktranslated 20.8M 217 K 22.1M 181KTable 1: Corpus statistics: total number of tokensin the French and Arabic sides, Arabic and Frenchvocabulary size.
Numbers are given on the prepro-cessed data.Arabic is a rich and morphologically complex lan-guage, and therefore data preprocessing is necessaryto deal with data scarcity.
All Arabic data were pre-processed by first transliterating the Arabic text withthe BAMA (Buckwalter, 2002) transliteration tool.Then, the Arabic data are segmented into sentences.A CRF-based sentence segmenter for Arabic wasbuilt with the Wapiti3 (Lavergne et al, 2010) pack-age.
A morphological analysis of the Arabic text isthen done using the Arabic morphological analyzerand disambiguation tool MADA (Nizar Habash andRoth, 2009), with the MADA-D2 since it seems tobe the most efficient scheme for large data (Habashand Sadat, 2006).The preprocessed Arabic and French data werealigned using MGiza++4 (Gao and Vogel, 2008).The Moses toolkit (Koehn et al, 2007) is then usedto make the alignments symmetric using the grow-diag-final-and heuristic and to extract phrases withmaximum length of 7 words.
A distortion modellexically conditioned on both the Arabic phrases andFrench phrases is then trained.
Feature weights wereset by running MERT (Och, 2003) on the develop-ment set.5.3 Extraction of the in-domain parallel corpusWe follow the method described in Section 3: Ara-bic documents are first translated into French usingthe baseline SMT system.
For the document selec-tion step each translated (ar:fr) document is com-pared only to the French documents of the same day.The thresholds for document selection and sentenceselection were respectively set to 0.5 and 0.7.
For apair of similar documents, the average percentage ofselected sentences is about 43%.The document selection step allows to select doc-uments containing around 35% of the total numberof sentences from the initial Arabic part of the com-parable corpus, a percentage that goes down to 15%after the sentence alignment step.
The resulting in-domain parallel corpus thus consists in a set of 156Kpairs of parallel sentences.
Data collected during thelast month of the period was isolated from the re-sulting corpus, and was used to randomly extract adevelopment and a test set of approximately 1,0003http://wapiti.limsi.fr4http://geek.kyloo.net/software/doku.php/mgiza:overview48Reference: Le ministre russe des Affaires e?trange`res, Sergue??
Lavrov a pre?venu mercredi [...]Baseline: Pronostiquait Ministre des affaires e?trange`res russe, Sergei Lavrov mercredi [...]Extracted: Le ministre russe des Affaires e?trange`res, Sergue??
Lavrov a averti mercredi [...]Reference: Le porte-parole de Mme Clinton, Philip Crowley, a toutefois reconnu [...]Baseline: Pour ukun FILIP Cruau porte-parole de Clinton a reconnu ...Extracted: Mais Philip Crowley, le porte-parole de Mme Clinton a reconnu [...]Figure 3: Comparative translations using the baseline translation and the extracted translation systems oftwo sentences: ?Russian Minister of Foreign Affairs, Sergue??
Lavrov, informed Wednesday [...]?
and ?Thespokesman for Mrs. Clinton, Philip Crowley, however, acknowledged [...]?.lines each.
These 2,160 sentences were manuallychecked to evaluate the precision of the approach,and we found that 97.2% of the sentences were cor-rectly paired.
Table 1 compares the main character-istics of the three corpora used for training.5.4 Translation ResultsTranslation results obtained on the test set are re-ported in terms of BLEU scores in Table 2, alongwith the corresponding phrase table sizes.
The dif-ferent adaptation approaches described in Section 4were experimented with both extracted and trans-lated corpora as adaptation corpus (see Section 3).As expected, adapting the translation model to theSMT System #Phrase pairs BLEUbaseline 312.4M 24.0extracted 10.9M 29.2baseline+extracted(1 table)321.6M 29.0baseline+extracted(2 tables)312.4M + 9.9M 30.1translated 39M 26.7extracted+translated(2 tables)9.9M + 39M 28.2Table 2: Arabic to French translation BLEU scoreson a test set of 1000 sentencesnews domain is very effective.
Compared to thebaseline system, all adapted systems obtain muchbetter results (from 2 to 6 BLEU points).
The ex-tracted system outperforms the baseline system by5 BLEU points, even though the training set is muchsmaller (3.6M compared to 162M tokens).
This re-sult indirectly validates the precision of our method-ology.Concatenating the baseline and extracted data totrain a single translation model does not improvethe smaller extracted system, thus maybe reflect-ing the fact that the large out-of-domain corpusoverwhelms the contribution of the in-domain data.However, a log-linear combination of the corre-sponding phrase tables brings a small improvement(0.8 BLEU point).Another interesting result comes from the perfor-mance of the system trained only on the translatedcorpus.
Without using any filtering of the automatictranslations, this artificial dataset enables to buildanother system which outperforms the baseline sys-tem by 2.5 BLEU points.
This is another illustrationof the greater importance of having matched domaindata, even of a poorer quality, than good parallel out-of-domain sentences (Cettolo et al, 2010).In the last experiment, all the available in-domaindata (extracted and translated) are used in conjunc-tion, with a separate phrase-table trained on eachcorpus.
However, this did not enable to match theresults of the extracted system, a paradoxical resultthat remains to be analyzed more carefully.
Filteringautomatic translations may be an issue.A rapid observation of the translations providedby both the baseline system and the extracted sys-tem shows that the produced output are quite dif-ferent.
Figure 3 displays two typical examples: thefirst one illustrates the different styles in Arabic(?News?
style often put subject ?Le ministre russedes affaires e?trange`res?
before verb ?
a pre?venu?or ?a averti?
?
which are semantically equiva-lent ?
whereas ?UN?
style is more classical, withthe verb ?Pronostiquait?
followed by the subject?ministre russe des Affaires e?trange`res?).
The sec-ond one shows how adaptation fixes the transla-49tion of words (here ?Philip Crowley?)
that were not(correctly) translated by the baseline system (?ukunFILIP Cruau?
).6 ConclusionWe have presented an empirical study of variousmethodologies for (i) extracting a parallel corpusfrom a comparable corpus (the so-called ?NoisyCorpus?)
and (ii) using in-domain data to adapt abaseline SMT system.
Experimental results, ob-tained using a large 150 million word Arabic/Frenchcomparable corpus, allow to jointly validate the ex-traction of the in-domain parallel corpus and the pro-posed adaptation methods.
The best adapted sys-tem, trained on a combination of the baseline andthe extracted data, improves the baseline by 6 BLEUpoints.
Preliminary experiments with self-trainingalso demonstrate the potential of this technique.As a follow-up, we intend to investigate the evo-lution of the translation results as a function of theprecision/recall quality of the extracted corpus, andof the quality of the automatically translated data.We have also only focused here on the adaptationof the translation model.
We expect to achieve fur-ther gains when combining these techniques withLM adaptation techniques.This work was partly supported by theFUI/SAMAR project funded by the Cap Digi-tal competitivity cluster.ReferencesSadaf Abdul-Rauf and Holger Schwenk.
2009.
Onthe use of comparable corpora to improve smt per-formance.
In Proceedings of the 12th Conference ofthe European Chapter of the Association for Compu-tational Linguistics, EACL ?09, pages 16?23, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Je?ro?me R. Bellagarda.
2001.
An overview of statis-tical language model adaptation.
In Proceedings ofthe ISCA Tutorial and Research Workshop (ITRW) onAdaptation Methods for Speech Recognition, pages165?174, Sophia Antipolis, France.Andrei Z. Broder.
2000.
Identifying and filtering near-duplicate documents.
In Proceedings of the 11th An-nual Symposium on Combinatorial Pattern Matching,COM ?00, pages 1?10, London, UK.
Springer-Verlag.Tim Buckwalter.
2002.
Buckwalter Arabic Mor-phological Analyzer.
Linguistic Data Consortium.
(LDC2002L49).Mauro Cettolo, Marcello Federico, and Nicola Bertoldi.2010.
Mining Parallel Fragments from ComparableTexts.
In Marcello Federico, Ian Lane, Michael Paul,and Franc?ois Yvon, editors, Proceedings of the seventhInternational Workshop on Spoken Language Transla-tion (IWSLT), pages 227?234.Pascale Fung and Percy Cheung.
2004.
Multilevel boot-strapping for extracting parallel sentences from a quasiparallel corpus.
In Proceedings of the conference onEmpirical Methods in Natural Language Processing(EMNLP 04), pages 1051?1057.Pascale Fung and Lo Yuen Yee.
1998.
An IR approachfor translating new words from nonparallel, compara-ble texts.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguisticsand 17th International Conference on ComputationalLinguistics-Volume 1, pages 414?420.
Association forComputational Linguistics.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engineer-ing, Testing, and Quality Assurance for Natural Lan-guage Processing, SETQA-NLP ?08, pages 49?57.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.
InProceedings of the Human Language Technology Con-ference of the NAACL, Companion Volume: Short Pa-pers, NAACL-Short ?06, pages 49?52, Morristown,NJ, USA.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, ACL ?07,pages 177?180, Morristown, NJ, USA.
Association forComputational Linguistics.Tadashi Kumano and Hideki Tanaka Takenobu Tokunaga.2007.
Extracting phrasal alignments from compara-ble corpora by using joint probability smt model.
InAndy Way and Barbara Gawronska, editors, Proceed-ings of the 11th International Conference on Theoreti-cal and Methodological Issues in Machine Translation(TMI?07), Sko?vde, Sweden.Jean-Marc Lange?.
1995.
Mode`les statistiques pourl?extraction de lexiques bilingues.
Traitement Automa-tique des Langues, 36(1-2):133?155.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale crfs.
In Proceed-ings of the 48th Annual Meeting of the Association for50Computational Linguistics, pages 504?513, Uppsala,Sweden.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31(4):477?504.Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting parallel sub-sentential fragments from non-parallel corpora.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th annual meeting of the Association for Compu-tational Linguistics, ACL-44, pages 81?88, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Owen Rambow Nizar Habash and Ryan Roth.
2009.Mada+tokan: A toolkit for arabic tokenization, di-acritization, morphological disambiguation, pos tag-ging, stemming and lemmatization.
In Khalid Choukriand Bente Maegaard, editors, Proceedings of the Sec-ond International Conference on Arabic Language Re-sources and Tools, Cairo, Egypt, April.
The MEDARConsortium.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 160?167, Sapporo, Japan.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguistics,ACL ?95, pages 320?322, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Philip Resnik and Noah A. Smith.
2003.
The Web asa parallel corpus.
Computational Linguistics, 29:349?380.Holger Schwenk.
2008.
Investigations on large-scale lightly-supervised training for statistical machinetranslation.
In Proceedings of the International Work-shop on Spoken Language Translation, pages 182?189, Hawaii, USA.Frank Smadja, Kathleen R. McKeown, and VasileiosHatzivassiloglou.
1996.
Translating collocations forbilingual lexicons: a statistical approach.
Computa-tional Linguistics, 22:1?38, March.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from comparablecorpora using document level alignment.
In HumanLanguage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, HLT ?10, pages 403?411,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and translation model adaptation us-ing comparable corpora.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP ?08, pages 857?866, Stroudsburg,PA, USA.
Association for Computational Linguistics.Christoph Tillmann and Jian-ming Xu.
2009.
A sim-ple sentence-level extraction algorithm for comparabledata.
In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics, Companion Volume: Short Papers, NAACL-Short ?09, pages 93?96, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Jakob Uszkoreit, Jay M. Ponte, Ashok C. Popat, andMoshe Dubiner.
2010.
Large scale parallel documentmining for machine translation.
In Proceedings ofthe 23rd International Conference on ComputationalLinguistics, COLING ?10, pages 1101?1109, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Da?niel Varga, La?szlo?
Ne?meth, Pe?ter Hala?csy, Andra?s Ko-rnai, Viktor Tro?n, and Viktor Nagy.
2005.
Parallelcorpora for medium density languages.
In Proceed-ings of the RANLP, pages 590?596.Bing Zhao and Stephan Vogel.
2002.
Adaptive paral-lel sentence mining from Web bilingual news collec-tion.
In Proceedings of the International Conferenceon Data Mining, pages 745?748.
IEEE Computer So-ciety.51
