Unsupervised Lexical Learning with Categorial GrammarsStephen Watk inson  and  Suresh  Manandhar ,Department  of Computer  Science,University of York,York YO10 5DD,UK.Abst rac tIn this paper we report on an unsupervised ap-proach to learning Categorial Grammar (CG)lexicons.
The learner is provided with a setof possible lexical CG categories, the forwardand backward application rules of CG and un-marked positive only corpora.
Using the cate-gories and rules, the sentences from the corpusare probabilistically parsed.
The parses and thehistory of previously parsed sentences are usedto build a lexicon and annotate the corpus.
Wereport the results from experiments on a num-ber of small generated corpora, that contain ex-amples from subsets of the English language.These show that the system is able to gener-ate reasonable lexicons and provide accuratelyparsed corpora in the process.
We also discussways in which the approach can be scaled up todeal with larger and more diverse corpora.1 In t roduct ionIn this paper we discuss a potential solution totwo problems in Natural Language Processing(NLP), using a combination of statistical andsymbolic machine learning techniques.
The firstproblem is learning the syntactic roles, or cat-egories, of words of  a language i.e.
learning alexicon.
Secondly, we discuss a method of an-notating a corpus with parses.The aim is to learn Categorial Grammar(CG) lexicons, starting from a set of lexical cat-egories, the functional application rules of CGand an unannotated corpus of positive exam-ples.
The CG formalism (discussed in Section2) is chosen because it assigns distinct categoriesto words of different ypes, and the categoriesdescribe the exact syntactic role each word canplay in a sentence.This problem is similar to the unsupervisedpart of speech tagging work of, for example,Brill (Brill, 1997) and Kupiec (Kupiec, 1992).In Brill's work a lexicon containing the parts ofspeech available to each word is provided anda simple tagger attaches a complex tag to eachword in the corpus, which represents all the pos-sible tags that word can have.
Transformationrules are then learned which use the context ofa word to determine which simple tag it shouldbe assigned.
The results are good, generallyachieving around 95% accuracy on large corporasuch as the Penn Treebank.Kupiec (Kupiec, 1992) uses an unsupervisedversion of the Baum-Welch algorithm, which isa way of using examples to iteratively estimatethe probabilities of a Hidden Markov Model forpart of speech tagging.
Instead of supplyinga lexicon, he places the words in equivalenceclasses.
Words in the same equivalence classmust take one of a specific set of parts of speech.This improves the accuracy of this algorithm toabout the same level as Brill's approach.In both cases, the learner is provided with alarge amount of background knowledge - eithera complete lexicon or set of equivalence classes.In the approach presented here, the most thatis provided is a small partial lexicon.
In factthe system learns the lexicon.The second problem - annotating the corpus- is solved because of the approach we use tolearn the lexicon.
The system uses parsing todetermine which are the correct lexical entriesfor a word, thus annotating the corpus with theparse derivations (also providing less probableparses if desired).
An example of another ap-proach to doing this is the Fidditch parser ofHindle (Hindle, 1983) (based on the determin-istic parser of Marcus (Marcus, 1980)), whichwas used to annotate the Penn Treebank (Mar-cus et al, 1993).
However, instead of learningthe lexicon, a complete grammar and lexicon59must be supplied to the Fidditch parser.Our work also relates to CG induction, whichhas been attempted by a number of people.
Os-borne (Osborne, 1997) has an algorithm that.learns a grammar for sequences ofpart-of-speechtags from a tagged corpora, using the MinimumDescription Length (MDL) principle - a well-defined form of compression.
While this is asupervised setting of the problem, the use ofthe more formal approach to compression is ofinterest for future work.
Also, results of 97%coverage are impressive, even though the prob-lem is rather simpler.
Kanazawa (Kanazawa,1994) and Buszkowski (Buszkowski, 1987) use aunification based approach with a corpus anno-tated with semantic structure, which in CG is astrong indicator of the syntactic structure.
Un-fortunately, they do not present results of exper-iments on natural language corpora and againthe approach is essentially supervised.Two unsupervised approaches to learningCGs are presented by Adriaans (Adriaans,1992) and Solomon (Solomon, 1991).
Adriaans,describes a purely symbolic method that usesthe context of words to define their category.An oracle is required for the learner to test itshypotheses, thus providing negative evidence.This would seem to be awkward from a engi-neering view point i.e.
how one could providean oracle to achieve this, and implausible froma psychological point of view, as humans do notseem to receive such evidence (Pinker, 1990).Unfortunately, again no results on natural an-guage corpora seem to be available.Solomon's approach (Solomon, 1991) usesunannotated corpora, to build lexicons for sim-ple CG.
He uses a simple corpora of sentencesfrom children's books, with a slightly ad hoc andnon-incremental, heuristic approach to develop-ing categories for words.
The results show thata wide range of categories can be learned, butthe current algorithm, as the author admits, isprobably too naive to scale up to working onfull corpora.
No results on the coverage of theCGs learned are provided.In Section 3 we discuss our learner.
In Sec-tion 4 we describe xperiments on three corporacontaining examples of a subset of English andSection 5 contains the results, which are encour-aging with respect o both problems.
Finally,in Section 6, we compare the results with thesystems mentioned above and discuss ways thesystem can be expanded and larger scale exper-iments may be carried out.
Next, however, wedescribe Categorial Grammar.2 Categor ia l  GrammarCategorial Grammar (CG) (Wood, 1993; Steed-man, 1993) provides a functional approach tolexicalised grammar, and so, can be thought ofas defining a syntactic calculus.
Below we de-scribe the basic (AB) CG, although in futureit will be necessary to pursue a more flexibleversion of the formalism.There is a set of atomic categories in CG,which are usually nouns (n), noun phrases (np)and sentences i ).
It is then possible to build upcomplex categories using the two slash operators"/" and "\".
I fA and B are categories then A/Bis a category and A\B is a category.
With basicCG there are just two rules for combining cat-egories: the forward (FA) and backward (BA).functional application rules.
Following Steed-man's notation (Steedman, 1993) these are:X/Y  Y ~ X (FA)Y X \Y  ~ X (BA)Therefore, for an intransitive verb like "run" thecomplex category is s\np and for a transitiveverb like "take" it is (s\np)/np.
In Figure 1the parse derivation for "John ate the apple" isJohn ate the applenp (s~np)/np np/n nFAnpFApresented.s~npBAsFigure 1: A Example Parse in Pure CGThe CG described above has been shownto be weakly equivalent to context-free phrasestructure grammars (Bar-Hillel et al, 1964).While such expressive power covers a largeamount of natural language structure, it hasbeen suggested that a more flexible and expres-sive formalism may capture natural languagemore accurately (Wood, 1993; Steedman, 1993).60This has led to some distinct branches of re-search into usefully extending CG, which willbe investigated in the future.CG has at least the following advantages forour task.?
Learning the lexicon and the grammar isone task.?
The syntax directly corresponds to the se-mantics.The first of these is vital for the work pre-sented here.
Because the syntactic structure isdefined by the complex categories assigned tothe words, it is not necessary to have separatelearning procedures for the lexicon and for thegrammar rules.
Instead, it is just one procedurefor learning the lexical assignments o words.Secondly, the syntactic structure in CG par-allels the semantic structure, which allows anelegant interaction between the two.
Whilethis feature of CG is not used in the cur-rent system, it could be used in the future toadd semantic background knowledge to aid thelearner (e.g.
Buszkowski's discovery procedures(Buszkowski, 1987)).3 The  LearnerThe system we have developed for learning lex-icons and assigning parses to unannotated sen-tences is shown diagrammatically in Figure 2.In the following sections we explain the learningsetting and the learning procedure respectively.3.1 The  Learn ing  Set t ingThe input to the learning setting has five parts:the corpus, the lexicon, the CG rules, the setof legal categories and a probabilistic parser,which are discussed below.The  Corpus  The corpus is a set of unanno-tated positive examples represented in Prologas facts containing a list of words e.g.ex ( \[mary, loved,  a, computer\] ) .The  Lexicon The lexicon is a set of Prologfacts of the form:lex(Word, Category, Frequency).Where Word is a word, Category is a Prologrepresentation of the CG category assigned tothat word and Frequency is the number of timesthis category has been assigned to this word upto the current point in the learning process.The Ru les  The CG functional applicationrules (see Section 2) are supplied to the learner.Extra rules may be added in future for fullergrammatical coverage.The  Categor ies  The learner has a completeset of the categories that can be assigned to aword in the lexicon.
The complete set is shownin Table 1.The  Parser  The system employs a proba-bilistic chart parser, which calculates the Nmost probable parses, where N is the beam setby the user.
The probability of a word beingassigned a category is based on the relative fre-quency, which is calculated from the current lex-icon.
This probability is smoothed (for wordsthat have not been given fixed categories priorto execution) to allow the possibility that theword may appear as other categories.
For allcategories for which the word has not appeared,it' is given a frequency of one.
This is partic-ularly useful for new words, as it ensures thecategory of a word is determined by its context.Each non-lexical edge in the chart has a prob-ability calculated by multiplying the probabili-ties of the two edges that are combined to formit.
Edges between two vertices are not added ifthere axe N edges labelled with the same cate-gory and a higher probability, between the sametwo vertices (if one has a lower probability itis replaced).
Also, for efficiency, edges are notadded between vertices if there is an edge al-ready in place with a much higher probability.The chart in Figure 3 shows examples of edgesthat would not be added.
The top half of thechart shows one parse and the bottom half an-other.
If N was set to 1 then the dashed edgespanning all the vertices would not be added,as it has a lower probability than the other sedge covering the same vertices.
Similarly, thedashed edge between the first and third verticeswould not be added, as the probability of the nis so much lower than the probability of the np.It is important that the parser is efficient, asit is used on every example and each word in anexample may be assigned any category.
As willbe seen it is also used extensively in selectingthe best parses.
In future we hope to inves-tigate the possibility of using more restrictedparsing techniques, e.g.
deterministic parsingtechnology such as that described by Marcus(Marcus, 1980), to increase fficiency and allow61Syntactic Role CG Category ExampleSentenceNounNoun PhraseIntransitive VerbTransitive VerbDitransitive VerbSentential Complement VerbDeterminerAdjectiveAuxiliary VerbThat complementizerPrepositionsnnps\np(s\np)/np((s\np)/np)/np(s\np)/snp/nn/n(s\.p)/(s\.p)np/s(n\n)/np((s\np)\(s\np))/npthe dog randogthe dogrankickedgavebelievethehungrydoesthattoTable 1: The categories available to the learner(/~Co~us~ Example .~\[ Fh'obabilisfic I-- ~te~ories~~ exicon~ odifier\[Figure 2: A Diagram of the Structure of the Learnerlarger scale experiments.3.2 The  Learn ing  ProcedureHaving described the various components withwhich the learner is provided, we now describehow they are used in the learning procedure.Pars ing  the  Examples  Examples are takenfrom the corpus one at a time and parsed.
Eachexample is stored with the group of parses gen-erated for it, so they can be efficiently accessedin future.
The parse that is selected (see below)as the current correct parse is maintained at thehead of this group.
The head parse contributesinformation to the lexicon and annotates thecorpus.
The parses are also used extensivelyfor the efficiency of the parse selection module,as will be described below.
When the parserfails to find an analysis of an example, eitherbecause it is ungrammatical, or because of theincompleteness of the coverage of the grammar,the system skips to the next example.The Parse  Selector  Once an example hasbeen parsed, the N most probable parses areconsidered in turn to determine which can beused to make the most compressive l xicon (bya given measure), following the compression aslearning approach of, for example, Wolff (Wolff,1987).
The current size measure for the lexicon62s - 0 .512D~\ / \ \  -- / / \  .
.
.
.
/ / r,:.
,'" " ~.
n - 0 .0008 sup  - 0 .009  ~ s s s - 0 .0009Figure 3: Example chart showing edge pruningis the sum of the sizes of the categories for eachlexical entry.
The size of a category is the num-ber of atomic categories within it.
However, itis not enough to look at what a parse wouldadd to the lexicon.
The effect of changing thelexicon on the parses of previous examples mustbe considered.
Changes in the frequency of as-signments can cause the probabilities of previ-ous parses to change and thus correct mistakesmade earlier when the evidence from the lex-icon was too weak to assign the correct parse.This correction is affected by reparsing previousexamples that may be affected by the additionof the new parse to the lexicon.
Not reparsingthose examples that will not be affected, savesa great deal of time.
In this way a new lexiconis built from the reparsed examples for each hy-pothesised parse of the current example.
Theparse leading to the most compressive of theseis chosen.
The amount of reparsing is also re-duced by using stored parse information.This may appear an expensive way of deter-mining which parse to select, but it enables thesystem to calculate the most compressive l xi-con and keep an up-to-date annotation for thecorpus.
Also, the chart parser works in poly-nomial time and it is possible to do significantpruning, as outlined, so few sentences need tobe reparsed each time.
However, in the futurewe will look at ways of determining which parseto select hat do not require complete reparsing.Lexicon Modif icat ion The final stage takesthe current lexicon and replaces it with the lex-icon built with the selected parse.
The wholeprocess is repeated until all the examples havebeen parsed.
The final lexicon is left after thefinal modification.
The most probable annota-tion of the corpus is the set of top-most parsesafter the final parse selection.4 Exper imentsExperiments were performed on three differentcorpora all containing only positive examples.Experiments were performed with and withouta partial lexicon of closed-class words (wordsof categories with a finite number of members)with fixed categories and probabilities, e.g.
de-terminers and prepositions.
All experimentswere carried out on a SGI Origin 2000.Exper iments  on Corpus 1 The first corpuswas built from a context-free grammar (CFG),using a simple random generation algorithm.The CFG (shown in Figure 4) covers a range ofsimple declarative sentences with intransitive,transitive and ditransitive verbs and with ad-jectives.
The lexicon of the CFG contained 39words with an example of noun-verb ambiguity.The corpus consisted of 500 such sentences (Fig-ure 5 shows examples).
As the size of the lexiconwas small and there was only a small amountof ambiguity, it was unnecessary to supply thepartial exicon, but the experiment was carriedout for comparison.
We also performed an ex-periment on 100 unseen examples to see howaccurately they were parsed with the learnedlexicon.
The results were manually verified todetermine how many sentences were parsed cor-rectly.S ~ NP VP VP --4 VbarVbar ~ IV Vbar ~ TV NPVbar ~ DV NP NP NP ~ PNNP ~ Nbar Nbar --4 Det NN~Adj  NPN ~ john Det --+ theN ~ boy Adj --~ smallIV ~ ran TV ~ timedDV ~ gaveFigure 4: The CFG used to generate Corpus 1with example lexical entriesExper iments  on Corpus 2 The second cor-pus was generated in the same way, but us-ing extra rules (see Figure 6) to include prepo-sitions, thus making the fragment of English63ex ( \[mary, ran\]  ) .ex ( \ [ john ,  gave,  john,  a,  boy \ ] ) .ex ( \ [a ,  dog, ca l led ,  the ,  f i sh ,  a,  smal l ,ug ly ,  desk \ ] ) .Figure 5: Examples from Corpus 1more complicated.
The lexicon used for gener-ating the corpus was larger - 44 words in total.Again 500 examples were generated (see Figure7 for examples) and experiments were carriedout both with and without the partial lexicon.Again we performed an experiment on 100 un-seen examples to see how accurately they areparsed.NP --+ Nbar PP VP -4 Vbar PPPP --4 P NPP~onFigure 6: The extra rules required for generat-ing Corpus 2 with example lexical entriesex(\[the, fish, with, a, elephant, gave,banks, a, dog, with, a, bigger, statue\]).ex(\[a, elephant, with, jim,walked, on, a, desk\]).ex(\[the, girl, kissed, the, computer,on, a, fish\]).Figure 7: Examples from Corpus 2Exper iments  on Corpus 3 (The LLL Cor-pus) Finally, we performed experiments u ingthe LLL corpus (Kazakov et al, 1998).
Thisis a corpus of generated sentences for a sub-stantial fragment of English.
It is annotatedwith a certain amount of semantic information,which was ignored.
The corpus contains 554sentences, however, because of the restricted setof categories and CG rules, we limited the ex-periments to the 157 declarative sentences (895words, with 152 unique words) in the corpus.Examples are shown in Figure 8.
While our CGrules can handle a reasonable variety of declar-ative sentences it is by no means complete, notallowing any movement (e.g.
topicalised sen-tences) or even any adverbs yet.
This was, un-surprisingly, something of a limitation.
Also,this corpus is very small and sparse, makinglearning difficult.
It was determined to experi-ment to see how well the system performed un-der these conditions.
Again we performed ex-periments with and without fixed closed-classwords.
Due to the lack of examples it was notpossible to perform a test on unseen examples,which need to be pursued in the future.ex(\[no, manager, in, sandy,reads, every, machine\]).ex(\[the, manual, isnt, continuing\]).ex(\[no, telephone, sees, the, things\]) .Figure 8: Examples from Corpus 3All experiments were performed with theminimum number of categories needed to coverthe corpus, so for example, in the experimentson Corpus 1 the categories for prepositions werenot available to the parser.
This will obviouslyaffect the speed with which the learner per-forms.
Also, the parser was restricted to twopossible parses in each case.5 Resu l tsIn Table 2 we report the results of these ex-periments.
The CCW Preset column indicateswhether the closed-class words were providedor not.
The lexicon accuracy column is a mea-sure, calculated by manual analysis, of the per-centage of lexical entries i.e.
entries that haveword-category pairs that can plausibly be ac-cepted as existing in English.
This should betaken together with the parse accuracy, which isthe percentage ofcorrectly parsed examples i.e.a linguistically correct syntactic analysis.
TheCorpus CCW LexiccPreset Acc.
(~?
100~/ 100~/ 100x 14.7x/ 77.7on Parse Exec.%) Acc.
(%) Time (s)1001001000.658.9529762510524164151361Table 2: Accuracies and timings for the differentlearning experimentsresults for the first two corpora are extremelyencouraging with 100% accuracy in both mea-sures.
While these experiments are only on rel-atively simple corpora, these results stronglysuggest hat the approach can be effective.
It64should be noted that any experiment on cor-pus 2 without the closed-class words being setdid not terminate, as the sentences in that cor-pus are significantly longer and each word maybe a large number of categories.
It is thereforeclear, that setting the closed-class words greatlyincreases peed and that we need to considermethods of relieving the strain on the parser ifthe approach is to be useful on more complexcorpora.The results with the LLL corpus are also en-couraging in part.
A lexical accuracy of 77.7%and a parse accuracy of nearly 60% (note thismeasure of accuracy is strict) on such a smallsparse corpus is a good result and analysis ug-gests most errors were made due to the smallcoverage of the grammar - especially not al-lowing any movement.
Errors also suggest hatadding some further linguistic onstraints - forexample not allowing words to be assigned thebasic category s - and strengthening the com-pression heuristic may provide improvements.It was these problems, along with the sparse-ness of the corpus, that led to the poor resultswith the LLL corpus without preset words.Table 3 shows predictably good results forparsing the test sets with the learned lexicons.Corpus Closed-Class Parse Accuracy (%)1 x 1001 , /  1oo2 x 1002 x/ 100Table 3: Unseen example parsing accuracy6 Conc lus ionsWe have presented an unsupervised learner thatis able to both learn CG lexicons and annotatenatural anguage corpora, with less backgroundknowledge than other systems in the literature.Results from preliminary experiments are en-couraging with respect o both problems, par-ticularly as the system appears to be reasonablyeffective on small, sparse corpora.
It is encour-aging that where errors arose this was often dueonly to incomplete background knowledge.The results presented are encouraging withrespect o the work that has already been men-tioned - 100~ can clearly not be improved uponand compares very favourable with the systemsmentioned in Section 1.
However, it is also clearthat this was achieved on unrealistically sim-ple corpora and when the system was used onthe more diverse LLL corpus it did not fair aswell.
However, given the fact that the problemsetting discussed here is somewhat harder thanthat attempted by other systems and the lackof linguistic background knowledge supplied, itis hoped that it will be possible to use the ap-proach on wider coverage corpora more effec-tively in the future.The use of CGs to solve the problem providesan elegant way of using syntactic informationto constrain the learning problem and providesthe opportunity for expansion to a full gram-mar learning system in the future by the devel-opment of a category hypothesizer.
It is hopedthat this will be part of future work..We also hope to carry out experiments onlarger and more diverse corpora, as the corporaused thus far are too small to be a an exactingtest for the approach.
We need to expand thegrammar to cover more linguistic phenomena toachieve this, as well as considering other mea-sures for compressing the lexicon (e.g.
usingan MDL-based approach).
Larger experimentswill lead to a need for increased efficiency in theparsing and reparsing processes.
This could bedone by considering deterministic parsing ap-proaches (Marcus, 1980), or perhaps hallowersyntactic analysis.While many extensions may be considered forthis work, the evidence thus far suggests thatthe approach outlined in this paper is effectiveand efficient for these natural anguage learningtasks.ReferencesPieter Willem Adriaans.
1992.
LanguageLearning .from a Categorial Perspective.Ph.D.
thesis, Universiteit van Amsterdam.Y.
Bar-Hillel, C. Gaifman, and E. Shamir.
1964.On categorial and phrase structure gram-mars.
In Language and Information (Bar-Hillel, 1964), pages 99 - 115.
First appearedin The Bulletin of the Research Council ofIsrael, vol.
9F, pp.
1-16, 1960.Y.
Bar-Hillel.
1964.
Language and Information.Addison-Wesley.65Eric Brill.
1997.
Unsupervised learning of dis-ambiguation rules for part of speech tagging.In Natural Language Processing Using VeryLarge Corpora.
Kluwer Academic Press.Wojciech Buszkowski.
1987.
Discovery proce-dures for categorial grammars.
In Ewan Kleinand Johan van Benthem, editors, Categories,Polymorphism and Unification, pages 35 - 64.Centre for Cognitive Science, University ofEdinburgh and Institue for Language, Logicand Information, University of Amsterdam.Donald Hindle.
1983.
Deterministic parsing ofsyntactic non-fluencies.
In Mitch Marcus, ed-itor, Proceedings of the 21st Annual Meet-ing of the Association for Computational Lin-guistics, pages 123 - 128.
Association forComputational Linguistics.Makoto Kanazawa.
1994.
Learnable Classes ofCategorial Grammars.
Ph.D. thesis, Institutefor Logic, Language and Computation, Uni-versity of Amsterdam.Dimitar Kazakov, Stephen Pulman, andStephen Muggleton.
1998.
The FraCasdataset and the LLL challenge.
Technical re-port, SRI International.Julian Kupiec.
1992.
Robust part-of-speechtagging using a hidden markov model.
Com-puter Speech and Language, 6:225-242.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of english: The penntreebank.
Technical Report IRCS-93-47, In-stitution for Research in Cognitive Science.Mitchell P. Marcus.
1980.
A Theory of Syn-tactic Recognition.
The MIT Press Series inArtificial Intelligence.
The MIT Press.Miles Osborne.
1997.
Minimisation, indiffer-ence and statistical anguage learning.
InWorkshop on Empirical Learning of NaturalLanguage Processing Tasks, ECML'97, pages113 - 124.Steven Pinker.
1990.
Language acquisition.
InDaniel N. Oshershon and Howard Lasnik, edi-tors, An Invitation to Cognitive Science: Lan-guage, volume 1, pages 199-241.
The MITPress.W.
Daniel Solomon.
1991.
Learning a gram-mar.
Technical Report UMCS-AI-91-2-1, De-partment of Computer Science, Artificial In-telligence Group, University of Manchester.Mark Steedman.
1993.
Categorial grammar.Lingua, 90:221 - 258.J.G.
Wolff.
1987.
Cognitive development as op-timisation.
In Leonard Bolc, editor, Com-putational Models of Learning, Symboliccomputation-artificial intelligence.
SpringerVerlag.Mary McGee Wood.
1993.
Categorial Gram-mars.
Linguistic Theory Guides.
Routledge.General Editor Richard Hudson.66
