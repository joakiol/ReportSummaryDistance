Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 568?575,Sydney, July 2006. c?2006 Association for Computational LinguisticsUnsupervised Relation Disambiguation with Order IdentificationCapabilitiesJinxiu Chen1 Donghong Ji1 Chew Lim Tan2 Zhengyu Niu11Institute for Infocomm Research 2Department of Computer Science21 Heng Mui Keng Terrace National University of Singapore119613 Singapore 117543 Singapore{jinxiu,dhji,zniu}@i2r.a-star.edu.sg tancl@comp.nus.edu.sgAbstractWe present an unsupervised learning ap-proach to disambiguate various relationsbetween name entities by use of variouslexical and syntactic features from thecontexts.
It works by calculating eigen-vectors of an adjacency graph?s Lapla-cian to recover a submanifold of datafrom a high dimensionality space andthen performing cluster number estima-tion on the eigenvectors.
This methodcan address two difficulties encouteredin Hasegawa et al (2004)?s hierarchicalclustering: no consideration of manifoldstructure in data, and requirement to pro-vide cluster number by users.
Experimentresults on ACE corpora show that thisspectral clustering based approach outper-forms Hasegawa et al (2004)?s hierarchi-cal clustering method and a plain k-meansclustering method.1 IntroductionThe task of relation extraction is to identify vari-ous semantic relations between name entities fromtext.
Prior work on automatic relation extractioncome in three kinds: supervised learning algorithms(Miller et al, 2000; Zelenko et al, 2002; Culottaand Soresen, 2004; Kambhatla, 2004; Zhou et al,2005), semi-supervised learning algorithms (Brin,1998; Agichtein and Gravano, 2000; Zhang, 2004),and unsupervised learning algorithm (Hasegawa etal., 2004).Among these methods, supervised learning is usu-ally more preferred when a large amount of la-beled training data is available.
However, it istime-consuming and labor-intensive to manually taga large amount of training data.
Semi-supervisedlearning methods have been put forward to mini-mize the corpus annotation requirement.
Most ofsemi-supervised methods employ the bootstrappingframework, which only need to pre-define some ini-tial seeds for any particular relation, and then boot-strap from the seeds to acquire the relation.
How-ever, it is often quite difficult to enumerate all classlabels in the initial seeds and decide an ?optimal?number of them.Compared with supervised and semi-supervisedmethods, Hasegawa et al (2004)?s unsupervised ap-proach for relation extraction can overcome the dif-ficulties on requirement of a large amount of labeleddata and enumeration of all class labels.
Hasegawaet al (2004)?s method is to use a hierarchical cluster-ing method to cluster pairs of named entities accord-ing to the similarity of context words intervening be-tween the named entities.
However, the drawback ofhierarchical clustering is that it required providingcluster number by users.
Furthermore, clustering isperformed in original high dimensional space, whichmay induce non-convex clusters hard to identified.This paper presents a novel application of spec-tral clustering technique to unsupervised relation ex-traction problem.
It works by calculating eigenvec-tors of an adjacency graph?s Laplacian to recover asubmanifold of data from a high dimensional space,and then performing cluster number estimation ona transformed space defined by the first few eigen-vectors.
This method may help us find non-convexclusters.
It also does not need to pre-define the num-ber of the context clusters or pre-specify the simi-larity threshold for the clusters as Hasegawa et al568(2004)?s method.The rest of this paper is organized as follows.
Sec-tion 2 formulates unsupervised relation extractionand presents how to apply the spectral clusteringtechnique to resolve the task.
Then section 3 reportsexperiments and results.
Finally we will give a con-clusion about our work in section 4.2 Unsupervised Relation ExtractionProblemAssume that two occurrences of entity pairs withsimilar contexts, are tend to hold the same relationtype.
Thus unsupervised relation extraction prob-lem can be formulated as partitioning collections ofentity pairs into clusters according to the similarityof contexts, with each cluster containing only entitypairs labeled by the same relation type.
And then, ineach cluster, the most representative words are iden-tified from the contexts of entity pairs to induce thelabel of relation type.
Here, we only focus on theclustering subtask and do not address the relationtype labeling subtask.In the next subsections we will describe our pro-posed method for unsupervised relation extraction,which includes: 1) Collect the context vectors inwhich the entity mention pairs co-occur; 2) Clusterthese Context vectors.2.1 Context Vector and Feature DesignLet X = {xi}ni=1 be the set of context vectors of oc-currences of all entity mention pairs, where xi repre-sents the context vector of the i-th occurrence, and nis the total number of occurrences of all entity pairs.Each occurrence of entity mention pairs can bedenoted as follows:R ?
(Cpre, e1, Cmid, e2, Cpost) (1)where e1 and e2 represents the entity mentions, andCpre,Cmid,and Cpost are the contexts before, be-tween and after the entity pairs respectively.We extracted features from e1, e2, Cpre, Cmid,Cpost to construct context vectors, which are com-puted from the parse trees derived from CharniakParser (Charniak, 1999) and the Chunklink script 1written by Sabine Buchholz from Tilburg University.1 Software available at http://ilk.uvt.nl/ sabine/chunklink/Words: Words in the two entities and three contextwindows.Entity Type: the entity type of both entity men-tions, which can be PERSON, ORGANIZA-TION, FACILITY, LOCATION and GPE.POS features: Part-Of-Speech tags correspondingto all words in the two entities and three con-text windows.Chunking features: This category of features areextracted from the chunklink representation,which includes:?
Chunk tag information of the two entities andthree context windows.
The ?0?
tag means thatthe word is outside of any chunk.
The ?I-XP?
tagmeans that this word is inside an XP chunk.
The?B-XP?
by default means that the word is at the be-ginning of an XP chunk.?
Grammatical function of the two entities andthree context windows.
The last word in each chunkis its head, and the function of the head is the func-tion of the whole chunk.
?NP-SBJ?
means a NPchunk as the subject of the sentence.
The otherwords in a chunk that are not the head have ?NO-FUNC?
as their function.?
IOB-chains of the heads of the two entities.
So-called IOB-chain, noting the syntactic categories ofall the constituents on the path from the root nodeto this leaf node of tree.We combine the above lexical and syntactic fea-tures with their position information in the contextto form the context vector.
Before that, we filter outlow frequency features which appeared only once inthe entire set.2.2 Context ClusteringOnce the context vectors of entity pairs are prepared,we come to the second stage of our method: clusterthese context vectors automatically.In recent years, spectral clustering technique hasreceived more and more attention as a powerful ap-proach to a range of clustering problems.
Amongthe efforts on spectral clustering techniques (Weiss,1999; Kannan et al, 2000; Shi et al, 2000; Ng et al,2001; Zha et al, 2001), we adopt a modified version(Sanguinetti et al, 2005) of the algorithm by Ng etal.
(2001) because it can provide us model order se-lection capability.Since we do not know how many relation typesin advance and do not have any labeled relation569Table 1: Context Clustering with Spectral-based Clusteringtechnique.Input: A set of context vectors X = {x1, x2, ..., xn},X ?
<n?d;Output: Clustered data and number of clusters;1.
Construct an affinity matrix by Aij = exp(?
s2ij?2 ) if i 6=j, 0 if i = j.
Here, sij is the similarity between xi andxj calculated by Cosine similarity measure.
and the freedistance parameter ?2 is used to scale the weights;2.
Normalize the affinity matrix A to create the matrix L =D?1/2AD?1/2, where D is a diagonal matrix whose (i,i)element is the sum of A?s ith row;3.
Set q = 2;4.
Compute q eigenvectors of L with greatest eigenvalues.Arrange them in a matrix Y .5.
Perform elongated K-means with q + 1 centers on Y ,initializing the (q + 1)-th mean in the origin;6.
If the q+1-th cluster contains any data points, then theremust be at least an extra cluster; set q = q + 1 and goback to step 4.
Otherwise, algorithm stops and outputsclustered data and number of clusters.training examples at hand, the problem of modelorder selection arises, i.e.
estimating the ?opti-mal?
number of clusters.
Formally, let k be themodel order, we need to find k in Equation: k =argmaxk{criterion(k)}.
Here, the criterion is de-fined on the result of spectral clustering.Table 1 shows the details of the whole algorithmfor context clustering, which contains two mainstages: 1) Transformation of Clustering Space (Step1-4); 2) Clustering in the transformed space usingElongated K-means algorithm (Step 5-6).2.3 Transformation of Clustering SpaceWe represent each context vector of entity pair as anode in an undirected graph.
Each edge (i,j) in thegraph is assigned a weight that reflects the similaritybetween two context vectors i and j.
Hence, the re-lation extraction task for entity pairs can be definedas a partition of the graph so that entity pairs thatare more similar to each other, e.g.
labeled by thesame relation type, belong to the same cluster.
As arelaxation of such NP-hard discrete graph partition-ing problem, spectral clustering technique computeseigenvalues and eigenvectors of a Laplacian matrixrelated to the given graph, and construct data clus-ters based on such spectral information.Thus the starting point of context clustering is toconstruct an affinity matrix A from the data, whichis an n ?
n matrix encoding the distances betweenthe various points.
The affinity matrix is then nor-malized to form a matrix L by conjugating with thethe diagonal matrix D?1/2 which has as entries thesquare roots of the sum of the rows of A.
This is totake into account the different spread of the variousclusters (points belonging to more rarified clusterswill have lower sums of the corresponding row ofA).
It is straightforward to prove that L is positivedefinite and has eigenvalues smaller or equal to 1,with equality holding in at least one case.Let K be the true number of clusters present inthe dataset.
If K is known beforehand, the first Keigenvectors of L will be computed and arranged ascolumns in a matrix Y .
Each row of Y correspondsto a context vector of entity pair, and the above pro-cess can be considered as transforming the originalcontext vectors in a d-dimensional space to new con-text vectors in the K-dimensional space.
Therefore,the rows of Y will cluster upon mutually orthogonalpoints on the K dimensional sphere,rather than onthe coordinate axes.2.4 The Elongated K-means algorithmAs the step 5 of Table 1 shows, the result of elon-gated K-means algorithm is used to detect whetherthe number of clusters selected q is less than the truenumber K, and allows one to iteratively obtain thenumber of clusters.Consider the case when the number of clusters qis less than the true cluster number K present in thedataset.
In such situation, taking the first q < Keigenvectors, we will be selecting a q-dimensionalsubspace in the clustering space.
As the rows of theK eigenvectors clustered along mutually orthogo-nal vectors, their projections in a lower dimensionalspace will cluster along radial directions.
Therefore,the general picture will be of q clusters elongated inthe radial direction, with possibly some clusters verynear the origin (when the subspace is orthogonal tosome of the discarded eigenvectors).Hence, the K-means algorithm is modified asthe elongated K-means algorithm to downweightdistances along radial directions and penalize dis-tances along transversal directions.
The elongatedK-means algorithm computes the distance of pointx from the center ci as follows:?
If the center is not very near the origin, cTi ci > ?
(?
is aparameter to be fixed by the user), the distances are cal-570-4 -3 -2 -1 0 1 2 3 4-4-3-2-101234(a)-4 -3 -2 -1 0 1 2 3 4-4-3-2-101234(b)0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08-0.08-0.06-0.04-0.0200.020.040.060.080.1(c)-4 -3 -2 -1 0 1 2 3 4-4-3-2-101234(d)Figure 1: An Example:(a) The Three Circle Dataset.
(b) The clustering result using K-means; (c) Threeelongated clusters in the 2D clustering space usingSpectral clustering: two dominant eigenvectors; (d)The clustering result using Spectral-based clustering(?2=0.05).
(4,?
and + denote examples in differentclusters)culated as: edist(x, ci) = (x ?
ci)TM(x ?
ci), whereM = 1?
(Iq ?cicTicTi ci) + ?
cicTicTi ci, ?
is the sharpness param-eter that controls the elongation (the smaller, the moreelongated the clusters) 2.?
If the center is very near the origin,cTi ci < ?, the dis-tances are measured using the Euclidean distance.In each iteration of procedure in Table 1, elon-gated K-means is initialized with q centers corre-sponding to data points in different clusters and onecenter in the origin.
The algorithm then will drag thecenter in the origin towards one of the clusters notaccounted for.
Compute another eigenvector (thusincreasing the dimension of the clustering space toq + 1) and repeat the procedure.
Eventually, whenone reach as many eigenvectors as the number ofclusters present in the data, no points will be as-signed to the center at the origin, leaving the clusterempty.
This is the signal to terminate the algorithm.2.5 An exampleFigure 1 visualized the clustering result of three cir-cle dataset using K-means and Spectral-based clus-tering.
From Figure 1(b), we can see that K-meanscan not separate the non-convex clusters in three cir-cle dataset successfully since it is prone to local min-imal.
For spectral-based clustering, as the algorithmdescribed, initially, we took the two eigenvectors ofL with largest eigenvalues, which gave us a two-dimensional clustering space.
Then to ensure thatthe two centers are initialized in different clusters,one center is set as the point that is the farthest fromthe origin, while the other is set as the point thatsimultaneously farthest the first center and the ori-gin.
Figure 1(c) shows the three elongated clusters inthe 2D clustering space and the corresponding clus-tering result of dataset is visualized in Figure 1(d),which exploits manifold structure (cluster structure)in data.3 Experiments and Results3.1 Data SettingOur proposed unsupervised relation extraction isevaluated on ACE corpus, which contains 519 filesfrom sources including broadcast, newswire, andnewspaper.
We only deal with intra-sentence ex-plicit relations and assumed that all entities have2 In this paper, the sharpness parameter ?
is set to 0.2571Table 2: Frequency of Major Relation SubTypes in the ACEtraining and devtest corpus.Type SubType Training DevtestROLE General-Staff 550 149Management 677 122Citizen-Of 127 24Founder 11 5Owner 146 15Affiliate-Partner 111 15Member 460 145Client 67 13Other 15 7PART Part-Of 490 103Subsidiary 85 19Other 2 1AT Located 975 192Based-In 187 64Residence 154 54SOC Other-Professional 195 25Other-Personal 60 10Parent 68 24Spouse 21 4Associate 49 7Other-Relative 23 10Sibling 7 4GrandParent 6 1NEAR Relative-Location 88 32been detected beforehand in the EDT sub-task ofACE.
To verify our proposed method, we only col-lect those pairs of entity mentions which have beentagged relation types in the given corpus.
Then therelation type tags were removed to test the unsuper-vised relation disambiguation.
During the evalua-tion procedure, the relation type tags were used asground truth classes.
A break-down of the data by24 relation subtypes is given in Table 2.3.2 Evaluation method for clustering resultWhen assessing the agreement between clusteringresult and manually annotated relation types (groundtruth classes), we would encounter the problem thatthere was no relation type tags for each cluster in ourclustering results.To resolve the problem, we construct a contin-gency table T , where each entry ti,j gives the num-ber of the instances that belong to both the i-th es-timated cluster and j-th ground truth class.
More-over, to ensure that any two clusters do not sharethe same labels of relation types, we adopt a per-mutation procedure to find an one-to-one mappingfunction ?
from the ground truth classes (relationtypes) TC to the estimated clustering result EC.There are at most |TC| clusters which are assignedrelation type tags.
And if the number of the esti-mated clusters is less than the number of the groundtruth clusters, empty clusters should be added so that|EC| = |TC| and the one-to-one mapping can beperformed, which can be formulated as the function:??
= argmax?
?|TC|j=1 t?
(j),j , where ?
(j) is the in-dex of the estimated cluster associated with the j-thclass.Given the result of one-to-one mapping, we adoptPrecision, Recall and F-measure to evaluate theclustering result.3.3 Experimental DesignWe perform our unsupervised relation extraction onthe devtest set of ACE corpus and evaluate the al-gorithm on relation subtype level.
Firstly, we ob-serve the influence of various variables, includingDistance Parameter ?2, Different Features, ContextWindow Size.
Secondly, to verify the effectivenessof our method, we further compare it with super-vised method based on SVM and other two unsuper-vised methods.3.3.1 Choice of Distance Parameter ?2We simply search over ?2 and pick the valuethat finds the best aligned set of clusters on thetransformed space.
Here, the scattering criteriontrace(P?1W PB) is used to compare the cluster qual-ity for different value of ?2 3, which measures the ra-tio of between-cluster to within-cluster scatter.
Thehigher the trace(P?1W PB), the higher the clusterquality.In Table 3 and Table 4, with different settings offeature set and context window size, we find out thecorresponding value of ?2 and cluster number whichmaximize the trace value in searching for a range ofvalue ?2.3.3.2 Contribution of Different FeaturesAs the previous section presented, we incorporatevarious lexical and syntactic features to extract rela-3 trace(P?1W PB) is trace of a matrix which is the sum ofits diagonal elements.
PW is the within-cluster scatter matrixas: PW =?cj=1?Xi?
?j (Xi ?
mj)(Xi ?
mj)t and PBis the between-cluster scatter matrix as: PB =?cj=1(mj ?m)(mj ?
m)t, where m is the total mean vector and mj isthe mean vector for jth cluster and (Xj ?
mj)t is the matrixtranspose of the column vector (Xj ?mj).572Table 3: Contribution of Different FeaturesFeatures ?2 cluster number trace value Precison Recall F-measureWords 0.021 15 2.369 41.6% 30.2% 34.9%+Entity Type 0.016 18 3.198 40.3% 42.5% 41.5%+POS 0.017 18 3.206 37.8% 46.9% 41.8%+Chunking Infomation 0.015 19 3.900 43.5% 49.4% 46.3%Table 4: Different Context Window Size SettingContext Window Size ?2 cluster number trace value Precision Recall F-measure0 0.016 18 3.576 37.6% 48.1% 42.2%2 0.015 19 3.900 43.5% 49.4% 46.3%5 0.020 21 2.225 29.3% 34.7% 31.7%tion.
To measure the contribution of different fea-tures, we report the performance by gradually in-creasing the feature set, as Table 3 shows.Table 3 shows that all of the four categories of fea-tures contribute to the improvement of performancemore or less.
Firstly,the addition of entity type fea-ture is very useful, which improves F-measure by6.6%.
Secondly, adding POS features can increaseF-measure score but do not improve very much.Thirdly, chunking features also show their great use-fulness with increasing Precision/Recall/F-measureby 5.7%/2.5%/4.5%.We combine all these features to do all other eval-uations in our experiments.3.3.3 Setting of Context Window SizeWe have mentioned in Section 2 that the contextvectors of entity pairs are derived from the contextsbefore, between and after the entity mention pairs.Hence, we have to specify the three context windowsize first.
In this paper, we set the mid-context win-dow as everything between the two entity mentions.For the pre- and post- context windows, we couldhave different choices.
For example, if we specifythe outer context window size as 2, then it means thatthe pre-context (post-context)) includes two wordsbefore (after) the first (second) entity.For comparison of the effect of the outer contextof entity mention pairs, we conducted three differentsettings of context window size (0, 2, 5) as Table 4shows.
From this table we can find that with the con-text window size setting, 2, the algorithm achievesthe best performance of 43.5%/49.4%/46.3% inPrecision/Recall/F-measure.
With the context win-dow size setting, 5, the performance becomes worseTable 5: Performance of our proposed method (Spectral-based clustering) compared with supervised method (SVM) andunsupervised methods((Hasegawa et al, 2004))?s method andK-means clustering.Precision Recall F-measureSVM 61.2% 49.6% 54.8%Hasegawa?s Method1 38.7% 29.8% 33.7%Hasegawa?s Method2 37.9% 36.0% 36.9%Kmeans 34.3% 40.2% 36.8%Our Proposed Method 43.5% 49.4% 46.3%because extending the context too much may includemore features, but at the same time, the noise alsoincreases.3.3.4 Comparison with Supervised methodsand other Unsupervised methodsTo explore the effectiveness of our unsupervisedmethod compared to supervised method, we performSVM technique with the same feature set defined inour proposed method.
The LIBSVM tool is used inthis test 4.
The kernel function we used is linearand SVM models are trained using the training setof ACE corpus.In (Hasegawa et al, 2004), they preformed un-supervised relation extraction based on hierarchicalclustering and they only used word features betweenentity mention pairs to construct context vectors.
Wereported the clustering results using the same clus-tering strategy as Hasegawa et al (2004) proposed.In Table 5, Hasegawa?s Method1 means the test usedthe word feature as Hasegawa et al (2004) whileHasegawa?s Method2 means the test used the samefeature set as our method.
In both tests, we specified4 LIBSVM : a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/ cjlin/libsvm.
Itsupports multi-class classification.573Table 6: Comparison of the existing efforts on ACE RDC task.Relation Dectection Relation Classificationon Types on SubtypesMethod P R F P R F P R FCulotta and Soresen (2004) Tree kernel based 81.2 51.8 63.2 67.1 35.0 45.8 - - -Kambhatla (2004) Feature based, Maxi-mum Entropy- - - - - - 63.5 45.2 52.8Zhou et al (2005) Feature based,SVM 84.8 66.7 74.7 77.2 60.7 68.0 63.1 49.5 55.5the cluster number as the number of ground truthclasses.We also approached the relation extraction prob-lem using the standard clustering technique, K-means, where we adopted the same feature set de-fined in our proposed method to cluster the con-text vectors of entity mention pairs and pre-specifiedthe cluster number as the number of ground truthclasses.Table 5 reports the performance of our pro-posed method comparing with SVM-based super-vised method and the other two unsupervised meth-ods.
As the result shows, SVM-based method by us-ing the same feature set in our proposed method canachieve 61.2%/49.6%/54.8% in Precision/Recall/F-measure.
Table 5 also shows our proposed spec-tral based method clearly outperforms the othertwo unsupervised methods by 12.5% and 9.5% inF-measure respectively.
Moreover, the incorpora-tion of various lexical and syntactic features intoHasegawa et al (2004)?s method2 makes it outper-form Hasegawa et al (2004)?s method1 which onlyuses word feature.3.4 DiscussionIn this paper, we have shown that the modified spec-tral clustering technique, with various lexical andsyntactic features derived from the context of en-tity pairs, performed well on the unsupervised re-lation disambiguation problem.
Our experimentsshow that by the choice of the distance parameter?2, we can estimate the cluster number which pro-vides the tightest clusters.
We notice that the es-timated cluster number is less than the number ofground truth classes in most cases.
The reason forthis phenomenon may be that some relation typescan not be easily distinguished using the context in-formation only.
For example, the relation subtypes?Located?, ?Based-In?
and ?Residence?
are difficultto disambiguate even for human experts to differen-tiate.The results also show that various lexical andsyntactic features contain useful information for thetask.
Especially, although we did not concern thedependency tree and full parse tree information asother supervised methods (Miller et al, 2000; Cu-lotta and Soresen, 2004; Kambhatla, 2004; Zhou etal., 2005), the incorporation of simple features, suchas words and chunking information, still can providecomplement information for capturing the charac-teristics of entity pairs.
Another observation fromthe result is that extending the outer context windowof entity mention pairs too much may not improvethe performance since the process may incorporatemore noise information and affect the clustering re-sult.As regards the clustering technique, the spectral-based clustering performs better than direct cluster-ing, K-means.
Since the spectral-based algorithmworks in a transformed space of low dimension-ality, data can be easily clustered so that the al-gorithm can be implemented with better efficiencyand speed.
And the performance using spectral-based clustering can be improved due to the reasonthat spectral-based clustering overcomes the draw-back of K-means (prone to local minima) and mayfind non-convex clusters consistent with human in-tuition.Currently most of works on the RDC task of ACEfocused on supervised learning methods.
Table 6lists a comparison of these methods on relation de-tection and relation classification.
Zhou et al (2005)reported the best result as 63.1%/49.5%/55.5% inPrecision/Recall/F-measure on the extraction ofACE relation subtypes using feature based method,which outperforms tree kernel based method byCulotta and Soresen (2004).
Although our unsu-pervised method still can not outperform these su-574pervised methods, from the point of view of un-supervised resolution for relation extraction, ourapproach already achieves best performance of43.5%/49.4%/46.3% in Precision/Recall/F-measurecompared with other clustering methods.4 Conclusion and Future workIn this paper, we approach unsupervised relation dis-ambiguation problem by using spectral-based clus-tering technique with diverse lexical and syntacticfeatures derived from context.
The advantage of ourmethod is that it doesn?t need any manually labeledrelation instances, and pre-definition the number ofthe context clusters.
Experiment results on the ACEcorpus show that our method achieves better perfor-mance than other unsupervised methods.Currently we combine various lexical and syn-tactic features to construct context vectors for clus-tering.
In the future we will further explore othersemantic information to assist the relation extrac-tion problem.
Moreover, instead of cosine similar-ity measure to calculate the distance between con-text vectors, we will try other distributional similar-ity measures to see whether the performance of re-lation extraction can be improved.
In addition, if wecan find an effective unsupervised way to filter outunrelated entity pairs in advance, it would make ourproposed method more practical.ReferencesAgichtein E. and Gravano L.. 2000.
Snowball: Ex-tracting Relations from large Plain-Text Collections,In Proc.
of the 5th ACM International Conference onDigital Libraries (ACMDL?00).Brin Sergey.
1998.
Extracting patterns and relationsfrom world wide web.
In Proc.
of WebDB Workshop at6th International Conference on Extending DatabaseTechnology (WebDB?98).
pages 172-183.Charniak E.. 1999.
A Maximum-entropy-inspired parser.Technical Report CS-99-12.. Computer Science De-partment, Brown University.Culotta A. and Soresen J.
2004.
Dependency tree kernelsfor relation extraction, In proceedings of 42th AnnualMeeting of the Association for Computational Linguis-tics.
21-26 July 2004.
Barcelona, Spain.Defense Advanced Research Projects Agency.
1995.Proceedings of the Sixth Message Understanding Con-ference (MUC-6) Morgan Kaufmann Publishers, Inc.Hasegawa Takaaki, Sekine Satoshi and Grishman Ralph.2004.
Discovering Relations among Named Enti-ties from Large Corpora, Proceeding of ConferenceACL2004.
Barcelona, Spain.Kambhatla N. 2004.
Combining lexical, syntactic andsemantic features with Maximum Entropy Models forextracting relations, In proceedings of 42th AnnualMeeting of the Association for Computational Linguis-tics.
21-26 July 2004.
Barcelona, Spain.Kannan R., Vempala S., and Vetta A.. 2000.
On cluster-ing: Good,bad and spectral.
In Proceedings of the 41stFoundations of Computer Science.
pages 367-380.Miller S.,Fox H.,Ramshaw L. and Weischedel R. 2000.A novel use of statistical parsing to extract informationfrom text.
In proceedings of 6th Applied Natural Lan-guage Processing Conference.
29 April-4 may 2000,Seattle USA.Ng Andrew.Y, Jordan M., and Weiss Y.. 2001.
On spec-tral clustering: Analysis and an algorithm.
In Pro-ceedings of Advances in Neural Information Process-ing Systems.
pages 849-856.Sanguinetti G., Laidler J. and Lawrence N.. 2005.
Au-tomatic determination of the number of clusters us-ing spectral algorithms.In: IEEE Machine Learningfor Signal Processing.
28-30 Sept 2005, Mystic, Con-necticut, USA.Shi J. and Malik.J.
2000.
Normalized cuts and imagesegmentation.
IEEE Transactions on Pattern Analysisand Machine Intelligence.
22(8):888-905.Weiss Yair.
1999.
Segmentation using eigenvectors: Aunifying view.
ICCV(2).
pp.975-982.Zelenko D., Aone C. and Richardella A.. 2002.
Ker-nel Methods for Relation Extraction, Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).
Philadelphia.Zha H.,Ding C.,Gu.M,He X.,and Simon H.. 2001.
Spec-tral Relaxation for k-means clustering.
In Neural In-formation Processing Systems (NIPS2001).
pages1057-1064, 2001.Zhang Zhu.
2004.
Weakly-supervised relation classifi-cation for Information Extraction, In proceedings ofACM 13th conference on Information and KnowledgeManagement (CIKM?2004).
8-13 Nov 2004.
Wash-ington D.C.,USA.Zhou GuoDong, Su Jian, Zhang Jie and Zhang min.2005.
Exploring Various Knowledge in Relation Ex-traction, In proceedings of 43th Annual Meeting of theAssociation for Computational Linguistics.
USA.575
