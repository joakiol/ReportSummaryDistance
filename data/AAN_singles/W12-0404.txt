Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 23?30,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsIn Search of a Gold Standard in Studies of DeceptionStephanie Gokhman1, Jeff Hancock1,3, Poornima Prabhu2, Myle Ott2, Claire Cardie2,3Departments of Communication1, Computer Science2, and Information Science3Cornell University, Ithaca, NY 14853{sbg94,jth34,pmp67,mao37,ctc9}@cornell.eduAbstractIn this study, we explore several populartechniques for obtaining corpora for decep-tion research.
Through a survey of tra-ditional as well as non-gold standard cre-ation approaches, we identify advantagesand limitations of these techniques for web-based deception detection and offer crowd-sourcing as a novel avenue toward achiev-ing a gold standard corpus.
Through an in-depth case study of online hotel reviews,we demonstrate the implementation of thiscrowdsourcing technique and illustrate itsapplicability to a broad array of online re-views.1 IntroductionLeading deception researchers have recently ar-gued that verbal cues are the most promising indi-cators for detecting deception (Vrij, 2008) whilelamenting the fact that the majority of previousresearch has focused on nonverbal cues.
At thesame time, increasing amounts of language arebeing digitized and stored on computers and theInternet ?
from email, Twitter and online datingprofiles to legal testimony and corporate commu-nication.
With the recent advances in natural lan-guage processing that have enhanced our abilityto analyze language, researchers now have an op-portunity to similarly advance our understandingof deception.One of the crucial components of this enter-prise, as recognized by the call for papers for thepresent workshop, is the need to develop corporafor developing and testing models of deception.To date there has not been any systematic ap-proach for corpus creation within the deceptionfield.
In the present study, we first provide anoverview of traditional approaches for this task(Section 2) and discuss recent deception detec-tion methods that rely on non-gold standard cor-pora (Section 3).
Section 4 introduces novel ap-proaches for corpus creation that employ crowd-sourcing and argues that these have several ad-vantages over traditional and non-gold standardapproaches.
Finally, we describe an in-depthcase study of how these techniques can be im-plemented to study deceptive online hotel reviews(Section 5).2 Traditional ApproachesThe deception literature involves a number ofwidely used traditional methods for gatheringdeceptive and truthful statements.
We classifythese according to whether they are sanctioned,in which the experimenter supplies instructions toindividuals to lie or not lie, or unsanctioned ap-proaches, in which the participant lies of his orher own accord.2.1 Sanctioned DeceptionThe vast majority of studies examining deceptionemploy some form of the sanctioned lie method.A common example is recruiting participants for astudy on deception and randomly assigning themto a lie or truth condition.
A classic example ofthis kind of procedure is the original study by Ek-man and Friesen (1969), in which nurses wererequired to watch pleasant or highly disturbingmovie clips.
The nurses were instructed to indi-cate that they were watching a pleasing movie,which required the nurses watching the disturbingclips to lie about their current emotional state.In another example, Newman et.
al.
(2003) ask23participants about their beliefs concerning a giventopic, such as abortion, and then instruct partici-pants to convince a partner that they hold the op-posite belief.Another form of sanctioned deception is to in-struct participants to engage in some form ofmock crime and then ask them to lie about it.
Forexample, in one study (Porter and Yuille, 1996),participants were asked to take an item, such asa wallet, from a room and then lie about it after-wards.
The mock crime approach improves theecological validity of the deception, and makes itthe case that the person actually did in fact act acertain way that they then must deny.2.1.1 Advantages and LimitationsThe advantages are obvious for these sanc-tioned lie approaches.
The researcher has largedegrees of experimental control over what the par-ticipant lies about and when, which allows forcareful comparison across the deceptive and non-deceptive accounts.
Another advantage is the rel-ative ease of instructing participants to lie vs. try-ing to identify actual (but unknown) lies in a dia-logue.The limitations for this approach, however, arealso obvious.
In asking participants to lie, theresearcher is essentially giving permission to theperson to lie.
This should affect the partici-pant?s behavior as the lie is being conducted atthe behest of a power figure, essentially actingout their deception.
Indeed, a number of schol-ars have pointed out this problem (Frank and Ek-man, 1997), and have suggested that unless highstakes are employed the paradigm produces datathat does not replicate any typical lying situation.High stakes refers to the potential for punishmentif the lie is detected or reward if the lie goes unde-tected.
Perhaps because of the difficulty in creat-ing high-stakes deception scenarios, to date thereare few corpora involving high-stakes lies.2.2 Unsanctioned DeceptionUnsanctioned lies are those that are told withoutany explicit instruction or permission from the re-searcher.
These kinds of lies have been collectedin a number of ways.2.2.1 Diary studies and surveysTwo related methods for collecting informationabout unsanctioned lies are diary studies and sur-vey studies.
In diary studies participants are askedon an ongoing basis (e.g., every night) to recalllies that they told over a given period (e.g., a day,a week) (DePaulo et al, 1996; Hancock et al,2004).
Similarly, recent studies have asked par-ticipants in national surveys how often they havelied in the last 24 hours (Serota et al, 2010).One important feature of these approaches isthat the lies have already taken place, and thusthey do not share the same limitations as sanc-tioned lies.
There are several drawbacks, how-ever, especially given the current goal to collectdeception corpora.
First, both diary studies andsurvey approaches require self-reported recall ofdeception.
Several biases are likely to affect theresults, including under-reporting of deception inorder to reduce embarrassment and difficult-to-remember deceptions that have occurred over thetime period.
More importantly, this kind of ap-proach does not lend itself to collecting the actuallanguage of the lie, for incorporation into a cor-pus: people have a poor memory for conversationrecall (Stafford and Sharkey, 1987).2.2.2 Retrospective IdentificationOne method for getting around the memorylimitations for natural discourse is to record thediscourse and ask participants to later identify anydeceptions in their discourse.
For instance, onestudy (Feldman and Happ, 2002) asked partici-pants to meet another individual and talk for tenminutes.
After the discussion, participants wereasked to examine the videotape of the discussionand indicated any times in which they were de-ceptive.
More recently, others have used the ret-rospective identification technique on mediatedcommunication, such as SMS, which producesan automatic record of the conversation that canbe reviewed for deception (Hancock, 2009).
Be-cause this approach preserves a record that theparticipant can use to identify the deception, thistechnique can generate data for linguistic analy-sis.
However, an important limitation, as with thediary and survey data, is that the researcher mustassume that the participant is being truthful abouttheir deception reporting.2.2.3 Cheating ProceduresThe last form of unsanctioned lying involvesincentivizing participants to first cheat on a taskand to then lie when asked about the cheating be-havior.
Levine et al (2010) have recently used24this approach, which involved students perform-ing a trivia quiz.
During the quiz, an opportunityto cheat arises where some of the students willtake the opportunity.
At this point, they have notyet lied, but, after the quiz is over, all studentsare asked whether they cheated by an interviewerwho does not know if they cheated or not.
Whilemost of the cheaters admit to cheating, a smallfraction of the cheaters deny cheating.
This sub-set of cheating denials represents real deception.The advantages to this approach are three-fold: (1) the deception is unsanctioned, (2) itdoes not involve self-report, and (3) the decep-tions have objective ground-truth.
Unfortunately,these kinds of experiments are extremely effort-intensive given the number of deceptions pro-duced.
Only a tiny fraction of the participantstypically end up cheating and subsequently lyingabout the cheating.2.2.4 LimitationsWhile these techniques have been useful inmany psychology experiments, in which assess-ing deception detection has been the priorityrather than corpus creation, they are not veryfeasible when considering obtaining corpora forlarge-scale settings, e.g., the web.
Furthermore,the techniques are limited in the kinds of con-texts that can be created.
For instance, in manycases, e.g., deliberate posting of fake online re-views, subjects can be both highly incentivizedto lie and highly concerned with getting caught.One could imagine surveying hotel owners as towhether they have ever posted a fake review?butit would seem unlikely that any owner would everadmit to having done so.3 Non-gold Standard ApproachesRecently, alternative approaches have emerged tostudy deception in the absence of gold standarddeceptive data.
These approaches can typicallybe broken up into three distinct types.
In Sec-tion 3.1, we discuss approaches to deception cor-pus creation that rely on the manual annotation ofdeceptive instances in the data.
In Section 3.2, wediscuss approaches that rely on heuristic methodsfor deriving approximate, but non-gold standarddeception labels.
In Section 3.3, we discuss a re-cent approach that uses assumptions about the ef-fects of deception to identify examples of decep-tion in the data.
We will refer to the latter as theunlabeled approach to deception corpus creation.3.1 Manual Annotations of DeceptionIn Section 2.2, we discussed diary and self-reportmethods of obtaining gold standard labels of de-ception.
Recently, work studying deceptive (fake)online reviews has suggested using manual anno-tations of deception, given by third-party humanjudges.Lim et al (2010) study deceptive product re-views found on Amazon.com.
They develop asophisticated software interface for manually la-beling reviews as deceptive or truthful.
The inter-face allows annotators to view all of each user?sreviews, ranked according to dimensions poten-tially of importance to identifying deception, e.g.,whether the review is duplicated, whether the re-viewer has authored many reviews in a single daywith identical high or low ratings, etc.Wu et al (2010a) also study deceptive onlinereviews of TripAdvisor hotels, manually labelinga set of reviews according to ?suspiciousness.
?This manually labeled dataset is then used to val-idate eight proposed characteristics of deceptivehotels.
The proposed characteristics include fea-tures based on the number of reviews written, e.g.,by first-time reviewers, as well as the review rat-ings, especially as they compare to other ratingsof the same hotel.Li et al (2011) study deceptive product reviewsfound on Epinions.com.
Based on user-providedhelpfulness ratings, they first draw a subsample ofreviews such that the majority are considered tobe unhelpful.
They then manually label this sub-sample according to whether or not each reviewseems to be fake.3.1.1 LimitationsManual annotation of deception is problematicfor a number of reasons.
First, many of the samechallenges that face manual annotation efforts inother domains also applies to annotations of de-ception.
For example, manual annotations can beexpensive to obtain, especially in large-scale set-tings, e.g., the web.Most seriously however, is that human abil-ity to detect deception is notoriously poor (Bondand DePaulo, 2006).
Indeed, recent studies haveconfirmed that human agreement and deceptiondetection performance is often no better thanchance (Ott et al, 2011); this is especially the25case when considering the overtrusting nature ofmost human judges, a phenomenon referred to inthe psychological deception literature as a truthbias (Vrij, 2008).3.2 Heuristically LabeledWork by Jindal and Liu (2008) studying the char-acteristics of untruthful (deceptive) Amazon.comreviews, has instead developed an approach forheuristically assigning approximate labels of de-ceptiveness, based on a set of assumptions spe-cific to their domain.
In particular, after re-moving certain types of irrelevant ?reviews,?
e.g.,questions, advertisements, etc., they determinewhether each review has been duplicated, i.e.,whether the review?s text heavily overlaps withthe text of other reviews in the same corpus.
Then,they simply label all discovered duplicate reviewsas untruthful.Heuristic labeling approaches do not produce atrue gold-standard corpus, but for some domainsmay offer an acceptable approximation.
How-ever, as with other non-gold standard approaches,certain behaviors might have other causes, e.g.,duplication could be accidental, and just becausesomething is duplicated does not make the origi-nal (first) post deceptive.
Indeed, in cases wherethe original review is truthful, its duplication isnot a good example of deceptive reviews writtenfrom scratch.3.3 UnlabeledRather than develop heuristic labeling ap-proaches, Wu et al (2010b) propose a novel strat-egy for evaluating hypotheses about deceptive ho-tel reviews found on TripAdvisor.com, based ondistortions of popularity rankings.
Specifically,they test the Proportion of Positive Singletons andConcentration of Positive Singletons hypothesesof Wu et al (2010a) (Section 3.1), but instead ofusing manually-derived labels they evaluate theirhypotheses by the corresponding (distortion) ef-fect they have on the hotel rankings.Unlabeled approaches rely on assumptionsabout the effects of the deception.
For example,the approach utilized by Wu et al (2010b) observ-ing distortion effects on hotel rankings, relies onthe assumption that the goal of deceivers in theonline hotel review setting is to increase a hotel?sranking.
And while this may be true for positivehotel reviews, it is likely to be very untrue for fakenegative reviews intended to defame a competitor.Indeed, great care must be taken in making suchassumptions in unlabeled approaches to studies ofdeception.4 Crowdsourcing ApproachesAs with traditional sanctioned deception ap-proaches (see Section 2.1), one way of obtain-ing gold standard labels is to simply create goldstandard deceptive content.
Crowdsourcing plat-forms are a particularly compelling space to pro-duce such deceptive content: they connect peoplewho request the completion of small tasks withworkers who will carry out the tasks.
Crowd-sourcing platforms that solicit small copywritingtasks include Clickworker, Amazon?s MechanicalTurk, Fiverr, and Worth1000.
Craigslist, while nota crowdsourcing platform, also promotes similarsolicitations for writing.
In the case of fake onlinereviews (see Section 5), and by leveraging plat-forms such as Mechanical Turk, we can often gen-erate gold standard deceptive content in contextsvery similar to those observed in practice.Mihalcea and Strapparava (2009) were amongthe first to use Mechanical Turk to collect decep-tive and truthful opinions ?
personal stances onissues such as abortion and the death penalty.
Inparticular, for a given topic, they solicited onetruthful and one deceptive stance from each Me-chanical Turk participant.Ott et al (2011) have also used MechanicalTurk to produce gold standard deceptive content.In particular, they use Mechanical Turk to gener-ate a dataset of 400 positive (5-star), gold stan-dard deceptive hotel reviews.
These were com-bined with 400 (positive) truthful reviews cov-ering the same set of hotels and used to train alearning-based classifier that could distinguish de-ceptive vs. truthful positive reviews at 90% accu-racy levels.
The truthful reviews were mined di-rectly from a well-known hotel review site.
TheOtt et al (2011) approach for collecting the goldstandard deceptive reviews is the subject of thecase study below.5 Case Study: Crowdsourcing DeceptiveReviewsTo illustrate in more detail how crowdsourcingtechniques can be implemented to create goldstandard data sets for the study of deception, we26draw from the Ott et al (2011) approach thatcrowdsources the collection of deceptive positivehotel reviews using Mechanical Turk.
The keyassumptions of the approach are as follows:?
We desire a balanced data set, i.e., equalnumbers of truthful and deceptive reviews.This is so that statistical analyses of the dataset won?t be biased towards either type of re-view.?
The truthful and deceptive reviews shouldcover the same set of entities.
If the twosets of reviews cover different entities (e.g.,different hotels), then the language that dis-tinguishes truthful from deceptive reviewsmight be attributed to the differing entitiesunder discussion rather than to the legiti-macy of the review.?
The resulting data set should be of a rea-sonable size.
Ott et al (2011) found thata dataset of 800 total reviews (400 truthful,400 deceptive) was adequate for their goalof training a learning-based classifier.?
The truthful and deceptive reviews shouldexhibit the same valence, i.e., sentiment.If the truthful reviews gathered from the on-line site are positive reviews, the deceptivereviews should be positive as well.?
More generally, the deceptive reviewsshould be generated under the same ba-sic guidelines as governs the generationof truthful reviews.
E.g., they should havethe same length constraints, the same qualityconstraints, etc.Step 1: Identify the set of entities to be cov-ered in the truthful reviews.
In order to de-fine a set of desirable reviews, a master database,provided by the review site itself, is mined toidentify the most commented (most popular) en-tities.
These are a good source of truthful re-views.
In particular, previous work has hypoth-esized that popular offerings are less likely tobe targeted by spam (Jindal and Liu, 2008), andtherefore reviews for those entities are less likelyto be deceptive?enabling those reviews to latercomprise the truthful review corpus.
The reviewsite database typically divides the entity set intosubcategories that differ across contexts: in thecase of hotel reviews the subcategories might re-fer to cities, or in the case of doctor reviewssubcategories might refer to specialties.
To en-sure that enough reviews of the entity can be col-lected, it may be important to select subcategoriesthat themselves are popular.
The study of Ott etal.
(2011), for example, focused on reviews of ho-tels in Chicago, IL, gathering positive (i.e., 5-star)reviews for the 20 most popular hotels.Step 2: Develop the crowdsourcing prompt.Once a set of entities has been identified for thedeceptive reviews (Step 1), the prompt for Me-chanical Turk is developed.
This begins with asurvey of other solicitations for reviews within thesame subcategory through searching MechanicalTurk, Craigslist, and other online resources.
Us-ing those solicitations as reference, a scenario canthen be developed that will be used in the promptto achieve the appropriate (in our case, positive)valence.
The result is a prompt that mimics thevocabulary and tone that ?Turkers?
(i.e., the work-ers on Mechanical Turk) may find familiar and de-sirable.For example, the prompt of Ott et al (2011)read: Imagine you work for the marketing depart-ment of a hotel.
Your boss asks you to write a fakereview for the hotel (as if you were a customer) tobe posted on a travel review website.
The reviewneeds to sound realistic and portray the hotel ina positive light.
Look at their website if you arenot familiar with the hotel.
(A link to the websitewas provided.
)Step 3: Attach appropriate warnings to thecrowdsource solicitation.
It is important thatwarnings are attached to the solicitation to avoidgathering (and paying for) reviews that wouldinvalidate the review set for the research.
Forexample, because each review should be writtenby a different person, the warning might disallowcoders from performing multiple reviews; forbidany form of plagiarism; require that reviews be?on topic,?
coherent, etc.
Finally, the promptmay inform the Turker that this exercise is foracademic purposes only and will not be postedonline, however, if such a notice is presentedbefore the review is written and submitted, theresulting lie may be overly sanctioned.27Step 4: Incorporate into the solicitation ameans for gathering additional data.
Appendto the end of the solicitation some mechanism(e.g., Mechanical Turk allows for a series of ra-dio buttons) to input basic information about age,gender, or education of the coder.
This allows forpost-hoc understanding of the demographic of theparticipating Turkers.
Ott et al (2011) also sup-ply a space for comments by the workers, with anadded incentive of a potential bonus for particu-larly helpful comments.
Ott et al (2011) foundthis last step critical to the iterative process forproviding insights from coders on inconsistencies,technical difficulties, and other unforeseen prob-lems that arise in the piloting phase.Step 5: Gather the deceptive reviews inbatches.
The solicitation is then published in asmall pilot test batch.
In Ott et al (2011), each pi-lot requested ten (10) reviews from unique work-ers.
Once the pilot run is complete, the resultsare evaluated, with particular attention to the com-ments, and is then iterated upon in small batchesof 10 until there are no technical complaints andthe results are of desired experiment quality.Once this quality is achieved, the solicitation isthen published as a full run, generating 400 re-views by unique workers.
The results are man-ually evaluated and cleaned to ensure all reviewsare valid, then filtered for plagiarism.
The result-ing set of gold standard online deceptive spam isthen used to train the algorithm for deceptive pos-itive reviews.5.1 Handling PlagiarismOne of the main challenges facing crowdsourceddeceptive content is identifying plagiarism.
Forexample, when a worker on Mechanical Turkis asked to write a deceptive hotel review, thatworker may copy an available review from var-ious sources on the Internet (e.g., TripAdvisor).These plagiarized reviews lead to flaws in ourgold standard.
Hence there arises a need to detectsuch reviews and separate them from the entirereview set.One way to address this challenge is to doa manual check of the reviews, one-by-one, us-ing online plagiarism detection web services, e.g.,plagiarisma.net or searchenginereports.net.
Themanual process is taxing, especially when thereare reviews in large numbers (as large as 400) tobe processed.
This illustrates a need to have atool which automates the detection of plagiarizedcontent in Turker submissions.
There are severalplagiarism detection softwares which are widelyavailable in the market.
Most of them maintaina database of content against which to check forplagiarism.
The input content is checked againstthese databases and the content is stored in thesame database at the end of the process.
Suchtools are an appropriate fit for detecting plagia-rized content in term papers, course assignments,journals etc.
However, online reviews define aseparate need which checks for plagiarism againstthe content available on the web.
Hence the avail-able software offerings are not adequate.We implemented a command line tool using theYahoo!
BOSS API, which is used to query sen-tences on the web.
Each of the review files isparsed to read as individual sentences.
Each sen-tence is passed as a query input to the API.
Weintroduce the parameters, n and m, defined as:1.
Any sentence which is greater than n wordsis considered to be a ?long sentence?
in theapplication usage.
If the sentence is a ?longsentence?
and the Yahoo!
BOSS API returnsno result, we query again using the first nwords of the sentence.
Here n is a config-urable parameter, and in our experiments weconfigured n = 10.2.
A sentence that is commonly used on theweb can return many matches, even if it wasnot plagiarized.
Thus, we introduce anotherparameter, m, such that if the number ofsearch results returned by the Yahoo!
BOSSAPI is greater than m, then the sentence isconsidered common and is ignored.
Our ob-servations indicate that such frequently usedsentences are likely to be short.
For exam-ple: ?We are tired,?
?No room,?
etc.
For ourusage we configured m = 30.We consider a sentence to be plagiarized if thetotal number of results returned by the Yahoo!BOSS API is less than m. Hence each sentenceis assigned a score as follows:?
If the total number of results is greater thanm: assign a score of 0?
If the total number of results is less than orequal to m: assign a score of 128We then divide the sum of the sentence scores in areview by the total number of sentences to obtainthe ratio of the number of matches to total num-ber of sentences.
We use this ratio to determinewhether or not a review was plagiarized.6 Discussion and ConclusionWe have discussed several techniques for creatingand labeling deceptive content, including tradi-tional, non-gold standard, and crowdsourced ap-proaches.
We have also given an illustrative in-depth look at how one might use crowdsourcingservices such as Mechanical Turk to solicit decep-tive hotel reviews.While we argue that the crowdsourcing ap-proach to creating deceptive statements hastremendous potential, there remain a number ofimportant limitations, some shared by the pre-vious traditional methods laid out above.
First,workers are given ?permission?
to lie, so theselies are sanctioned and have the same concernsas the traditional sanctioned methods, includingthe concern that the workers are just play-actingrather than lying.
Other unique limitations in-clude the current state of knowledge about work-ers.
In a laboratory setting we can fairly tightlymeasure and control for gender, race, and evensocioeconomic status, but this is not the case forthe Amazon Turkers, who potentially make up amuch more diverse population.Despite these issues we believe that the ap-proach has much to offer.
First, and perhaps mostimportantly, the deceptions are being solicited inexactly the manner real-world deceptions are ini-tiated.
This is important in that the deception task,though sanctioned, is precisely the same task thata real-world deceiver might use, e.g., to collectfake hotel reviews for themselves.
Second, thisapproach is extremely cost effective in terms ofthe time and finances required to create customdeception settings that fit a specific context.
Herewe looked at creating fake hotel reviews, but wecan easily apply this approach to other types ofreviews, including reviews of medical profession-als, restaurants, and products.AcknowledgmentsThis work was supported in part by National Sci-ence Foundation Grant NSCC-0904913, and theJack Kent Cooke Foundation.
We also thank theEACL reviewers for their insightful comments,suggestions and advice on various aspects of thiswork.ReferencesC.F.
Bond and B.M.
DePaulo.
2006.
Accuracy of de-ception judgments.
Personality and Social Psychol-ogy Review, 10(3):214.B.M.
DePaulo, D.A.
Kashy, S.E.
Kirkendol, M.M.Wyer, and J.A.
Epstein.
1996.
Lying in everydaylife.
Journal of personality and social psychology,70(5):979.P.
Ekman and W. V. Friesen.
1969.
Nonverbal Leak-age And Clues To Deception, volume 32.Forrest J.
A. Feldman, R. S. and B. R. Happ.
2002.Self-presentation and verbal deception: Do self-presenters lie more?
Basic and Applied Social Psy-chology, 24:163?170.M.G.
Frank and P. Ekman.
1997.
The Ability To De-tect Deceit Generalizes Across Different Types ofHigh-Stake Lies.
Journal of Personality and SocialPsychology, 72:1429?1439.J.T.
Hancock, J. Thom-Santelli, and T. Ritchie.
2004.Deception and design: The impact of communi-cation technology on lying behavior.
In Proceed-ings of the SIGCHI conference on Human factors incomputing systems, pages 129?134.
ACM.J.T.
Hancock.
2009.
Digital Deception: The Practiceof Lying in the Digital Age.
Deception: Methods,Contexts and Consequences, pages 109?120.N.
Jindal and B. Liu.
2008.
Opinion spam and analy-sis.
In Proceedings of the international conferenceon Web search and web data mining, pages 219?230.
ACM.Kim R. K. Levine, T. R. and J. P. Blair.
2010.
(In)accuracy at detecting true and false confessionsand denials: An initial test of a projected motivemodel of veracity judgments.
Human Communica-tion Research, 36:81?101.F.
Li, M. Huang, Y. Yang, and X. Zhu.
2011.
Learningto identify review spam.
In Twenty-Second Interna-tional Joint Conference on Artificial Intelligence.E.P.
Lim, V.A.
Nguyen, N. Jindal, B. Liu, and H.W.Lauw.
2010.
Detecting product review spammersusing rating behaviors.
In Proceedings of the 19thACM international conference on Information andknowledge management, pages 939?948.
ACM.R.
Mihalcea and C. Strapparava.
2009.
The lie de-tector: Explorations in the automatic recognition ofdeceptive language.
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 309?312.
Association for Computational Linguistics.M.L.
Newman, J.W.
Pennebaker, D.S.
Berry, and J.M.Richards.
2003.
Lying words: Predicting decep-tion from linguistic styles.
Personality and SocialPsychology Bulletin, 29(5):665.29M.
Ott, Y. Choi, C. Cardie, and J.T.
Hancock.
2011.Finding deceptive opinion spam by any stretch ofthe imagination.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies-Volume 1,pages 309?319.
Association for Computational Lin-guistics.S.
Porter and J.C. Yuille.
1996.
The language of de-ceit: An investigation of the verbal clues to decep-tion in the interrogation context.
Law and HumanBehavior, 20:443?458.K.B.
Serota, T.R.
Levine, and F.J. Boster.
2010.The prevalence of lying in america: Three studiesof self-reported lies.
Human Communication Re-search, 36(1):2?25.Burggraf C. S. Stafford, L. and W.F.
Sharkey.
1987.Conversational Memory The Effects of Time, Re-call, Mode, and Memory Expectancies on Remem-brances of Natural Conversations.
Human Commu-nication Research, 14:203?229.A.
Vrij.
2008.
Detecting lies and deceit: Pitfalls andopportunities.
Wiley-Interscience.G.
Wu, D. Greene, B. Smyth, and P. Cunningham.2010a.
Distortion as a validation criterion in theidentification of suspicious reviews.
In Proceedingsof the First Workshop on Social Media Analytics,pages 10?13.
ACM.G.
Wu, D. Greene, B. Smyth, and P. Cunningham.2010b.
Distortion as a validation criterion inthe identification of suspicious reviews.
Techni-cal report, UCD-CSI-2010-04, University CollegeDublin.30
