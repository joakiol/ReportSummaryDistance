Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 474?482,Beijing, August 2010An Empirical Study on Web Mining of Parallel DataGumwon Hong1, Chi-Ho Li2, Ming Zhou2 and Hae-Chang Rim11Department of Computer Science & En-gineering, Korea University{gwhong,rim}@nlp.korea.ac.kr2Natural Language Computing Group,Microsoft Research Asia{chl,mingzhou}@microsoft.comAbstractThis paper 1  presents an empirical ap-proach to mining parallel corpora.
Con-ventional approaches use a readilyavailable collection of comparable, non-parallel corpora to extract parallel sen-tences.
This paper attempts the muchmore challenging task of directly search-ing for high-quality sentence pairs fromthe Web.
We tackle the problem byformulating good search query using?Learning to Rank?
and by filteringnoisy document pairs using IBM Model1 alignment.
End-to-end evaluationshows that the proposed approach sig-nificantly improves the performance ofstatistical machine translation.1 IntroductionBilingual corpora are very valuable resources inNLP.
They can be used in statistical machinetranslation (SMT), cross language informationretrieval, and paraphrasing.
Thus the acquisitionof bilingual corpora has received much attention.Hansards, or parliamentary proceedings inmore than one language, are obvious source ofbilingual corpora, yet they are about a particulardomain and therefore of limited use.
Many re-searchers then explore the Web.
Some approachattempts to locate bilingual text within a webpage (Jiang et al, 2009); some others attempt tocollect web pages in different languages anddecide the parallel relationship between the webpages by means of structural cues, like exist-ence of a common ancestor web page, similaritybetween URLs, and similarity between theHTML structures (Chen and Nie, 2000; Resnik1 This work has been done while the first author was visit-ing Microsoft Research Asia.and Smith, 2003; Yang and Li, 2003; Shi et al,2006).
The corpora thus obtained are generallyof high quality and wide variety in domain, butthe amount is still limited, as web pages thatexhibit those structural cues are not abundant.Some other effort is to mine bilingual corporaby textual means only.
That is, two pieces oftext are decided to be parallel merely from thelinguistic perspective, without considering anyhint from HTML markup or website structure.These approaches (Zhao and Vogel, 2002;Utiyama and Isahara 2003; Fung and Cheung,2004; Munteanu and Marcu, 2005; Abdul-Raufand Schwenk, 2009) share roughly the sameframework:Phase 1: Document Pair Retrieval1) documents in some target language (TL) arestored in some database;2) each document in some source language (SL)is represented by some TL keywords;3) the TL keywords in (2) are used to assignsome TL documents to a particular SL doc-ument, using some information retrieval (IR)technique.
For example, Munteanu and Mar-cu (2005) apply the Lemur IR toolkit,Utiyama and Isahara (2003) use the BM25similarity measure, and Fung and Cheung(2004) use cosine similarity.
Each TL docu-ment pairs up with the SL document to forma candidate parallel document pair.Phase 2: Sentence Pair Extraction1) sentence pairs can be obtained by runningsentence alignment over all candidate docu-ment pairs (or a selection of them) (Zhao andVogel, 2002; Utiyama and Isahara, 2003);2) sentence pairs can also be selected, by someclassifier or reliability measure, from thecandidate sentence pairs enumerated fromthe candidate document pairs (Munteanu andMarcu, 2005).Note that the primary interest of these ap-proaches is sentence pairs rather than document474pairs, partially because document pair retrievalis not accurate, and partially because the ulti-mate purpose of these corpora is SMT training,which is based on sentence pairs.
It is found thatmost of the sentence pairs thus obtained are nottruly parallel; rather they are loose translationsof each other or they carry partially similar mes-sages.
Such bilingual corpora are thus known ascomparable corpora, while genuinely mutualtranslations constitute parallel corpora.Note also that all these comparable corpusmining approaches are tested on closed docu-ment collections only.
For example, Zhao andVogel (2002), Utiyama and Isahara (2003), andMunteanu and Marcu (2005) all acquire theircomparable corpora from a collection of newsarticles which are either downloaded from theWeb or archived by LDC.
The search of candi-date document pairs in such a closed collectionis easy in three ways:1) all the TL documents come from the samenews agency and they are not mixed up withsimilar documents from other news agencies;2) all the TL documents are news text and theyare not mixed up with text of other domains;3) in fact, the search in these approaches ismade easier by applying tricks like date win-dow.There is no evidence that these methods applyto corpus mining from an open document col-lection (e.g.
the entire Web) without search con-straint.
The possibility of open-ended text min-ing is a crucial problem.This paper focuses on bilingual corpus min-ing using only textual means.
It attempts to an-swer two questions:1) Can comparable corpus mining be applied toan open document collection, i.e., the Web?2) Can comparable corpus mining be adapted toparallel corpus mining?We give affirmation to both questions.
For thefirst problem, we modify document pairretrieval so that there is no longer a closed set ofTL documents.
Instead we search for candidateTL documents for a particular SL documentfrom the Web by means of some Web searchengine.
For the second problem, in Phase 2 wereplace the sentence pair classifier by adocument pair filter and a sentence alignmentmodule.
Based on end-to-end SMT experiments,we will show that 1) high quality bilingualcorpora can be mined from the Web; 2) the veryfirst key to Web-mining of bilingual corpus isthe formulation of good TL keywords torepresent a SL document; 3) a simple documentpair filter using IBM Model 1 probabilities isable to identify parallel corpus out of noisycomparable text; and 4) Web-mined parallelcorpus, despite its smaller size, improves SMTmuch more than Web-mined comparable corpus.2 Problem SettingOur ultimate goal is to mine from the Webtraining data for translation from Chinese (SL)to English (TL).
As the first step, about 11,000Chinese web pages of news articles are crawledfrom some Chinese News sites.
Then the task isto search for the English sentences correspond-ing to those in the selected SL articles.
Theseselected SL news articles all contain cue phraseslike ????????
(according to foreign me-dia), as these cue phrases suggest that the Chi-nese articles are likely to have English counter-parts.
Moreover, each selected SL article has atleast 500 words (empirically determined) sincewe assume that it is much easier to formulatereliable keywords from a long document than ashort one.3 Document Pair RetrievalConventional approaches to comparable corpusmining usually start with document pair retriev-al, which assigns to each SL document a set ofcandidate TL documents.
This step is essentiallya preliminary search for candidate sentencepairs for further scrutiny in Phase 2.
The targetis to find document pairs which may containmany good sentence pairs, rather than to discarddocument pairs which may not contain goodsentence pairs.
Therefore, recall is much moreemphasized than precision.Document pair retrieval in conventional ap-proaches presumes a closed set of TL docu-ments which some IR system can handle easily.In this paper we override this presumption andattempt a much more challenging retrieval task,viz.
to search for TL documents among the Web,using the search engines of Google and Yahoo.Therefore we are subject to a much noisier datadomain.
The correct TL documents may not beindexed by the search engines at all, and evenwhen the target documents are indexed, it re-475quires a more sophisticated formulation of que-ries to retrieve them.In response to these challenges, we proposevarious kinds of queries (elaborated in the fol-lowing subsections).
Moreover, we merge theTL documents found by each query into a bigcollection, so as to boost up the recall.
In case aquery fails to retrieve any document, we itera-tively drop a keyword in the query until somedocuments are found.
On the other hand, alt-hough the document pairs in question are ofnews domain, we use the general Google/Yahooweb search engines instead of the specific newssearch engines, because 1) the news search en-gines keep only a few web pages for all pagesabout the same news event, and 2) we leaveopen possibility for correct TL documents to befound in non-news web pages.3.1 Simple QueriesThere are three baseline formulations of queries:1) Query of translations of SL TF-IDF-rankedkeywords (QSL-TFIDF).
This is the methodproposed by Munteanu and Marcu (2005).All the words in a SL document are rankedby TF-IDF and the top-N words are selected.Each keyword is then translated into a fewTL words by a statistically learned diction-ary.
In our experiments the dictionary islearned from NIST SMT training data.2) Query of TF-IDF-ranked machine translatedkeywords (QTL-TFIDF).
It is assumed that amachine translation (MT) system is better athandling lexical ambiguity than simple dic-tionary translation.
Thus we propose to firsttranslate the SL document into TL and ex-tract the top-N TF-IDF-ranked words asquery.
In our experiments the MT systemused is hierarchical phrase-based system(Chiang, 2007).23) Query of named entities (QNE).
Anotherway to tackle the drawback of QSL-TFIDF is tofocus on named entities (NEs) only, sinceNEs often provide strong clue for identify-ing correspondence between two languages.All NEs in a SL document are ranked byTF-IDF, and the top-N NEs are then trans-lated (word by word) by dictionary.
In ourexperiments we identify SL (Chinese) NEs2 We also try online Google translation service, and theperformance was roughly the same.implicitly found by the word segmentationalgorithm stated in Gao et al (2003), andthe dictionaries for translating NEs includethe same one used for QSL-TFIDF, and theLDC  Chinese/English NE dictionary.
Forthe NEs not covered by our dictionary, weuse Google translation service as a back-up.A small-scale experiment is run to evaluatethe merits of these queries.
300 Chinese newsweb pages in three different periods (each 100)are collected.
For each Chinese text, each query(containing 10 keywords) is constructed andsubmitted to both Google and Yahoo Search,and top-40 returned English web pages for eachsearch are kept.
Note that the Chinese news ar-ticles are not part of 11,000 pages in section 2.In fact, they do not only satisfy the requirementof length and cue phrases (described in section2), but they also have another property that theyare translated from some English news articles(henceforth target pages) on the Web.
Thus theyare ideal data for studying the performance ofdocument pair retrieval.To test the influence of translation quality indocument pair retrieval, we also try ?oracle que-ries?, i.e.
queries formulated directly from thetarget pages:1) OQTFIDF.
This is the query of the top-N TF-IDF-ranked words from the target page.2) OQNE.
This is the query of the top-N TF-IDF-ranked NEs from the target web page.We define recall as the proportion of SL docu-ments whose true target pages are found.
Thecomparison between a retrieved page and thetarget page is done by Longest Common Subse-quence (LCS) ratio, defined as the length of thelongest common word sequence of two docu-ments divided by the length of the longer of twodocuments.
The threshold 0.7 is adopted as it isstrict enough to distinguish parallel documentpairs from non-parallel ones.Table 1 shows the recalls for various queries.It can be seen from Tests 6 and 7 that the largestrecall, 85% (within top 40 search results), isachieved when the word distributions in the tar-get web pages are known.
In the real scenariowhere the true English word distribution is notknown, the recalls achieved by the simple que-ries are very unsatisfactory, as shown by Tests 1to 3.
This clearly shows how challenging Web-based mining of bilingual corpora is.
Anotherchallenge can be observed in comparing across476columns, viz.
it is much more difficult to re-trieve outdated news document pairs.
This im-plies that bilingual news mining must be incre-mentally carried out.Comparing Test 1 to Tests 2 and 3, it is obvi-ous that QSL-TFIDF is not very useful in documentpair retrieval.
This confirms our hypothesis thatsuitable TL keywords are not likely to be ob-tained by simple dictionary lookup.
While therecalls by QTL-TFIDF are similar to those by QNE,the two queries contribute in different ways.Test 4 simply merges the Web search results inTests 2 and 3.
The significantly higher recalls inTest 4 imply that each of the two queries findssubstantially different targets than each other.The comparison of Test 5 to Test 4 further con-firms the weakness of QSL-TFIDF.The huge gap between the three simple que-ries and the oracle queries shows that the qualityof translation of keywords from SL to TL is amajor obstacle.
There are two problems in trans-lation quality: 1) the MT system or dictionarycannot produce any translation for a SL word(let us refer to such TL keywords as ?Utopiantranslations?
); 2) the MT system or dictionaryproduces an incorrect translation for a SL word.We can do very little for the Utopian transla-tions, as the only solution is simply to use a bet-ter MT system or a larger dictionary.
On thecontrary, it seems that the second problem cansomewhat be alleviated, if we have a way todistinguish those terms that are likely to be cor-rect translations from those terms that are not.In other words, it may be worthwhile to reordercandidate TL keywords by our confidence in itstranslation quality.Tests 8 and 9 in Table 1 show that this hy-pothesis is promising.
In both tests the TF-IDF-based (Test 8) or the NE-based (Test 9) key-words are selected from only those TL wordsthat appear both in the target page and the ma-chine translated text of the source page.
In otherwords, we ensure that the keywords in the querymust be correct translations.
The recalls (espe-cially the recalls by NE-based query in Test 9)are very close to the recalls by oracle queries.The conclusion is, even though we cannot pro-duce the Utopian translations, document pairretrieval can be improved to a large extent byremoving incorrect translations.
Even an imper-fect MT system or NE dictionary can help usachieve as good document pair retrieval recallas oracle queries.In the next subsection we will take this in-sight into our bilingual data mining system, byselecting keywords which are likely to be cor-rect translation.3.2 Re-ranked QueriesMachine learning is applied to re-rank key-words for a particular document.
The re-rankingof keywords is based on two principles.
Thefirst one is, of course, the confidence on thetranslation quality.
The more likely a keywordis a correct translation, the higher this keywordshould be ranked.
The second principle is therepresentativeness of document.
The more rep-resentative of the topic of the document where akeyword comes from, the higher this keywordshould be ranked.
The design of features shouldincorporate both principles.The representativeness of document is mani-fested in the following features for each key-word per each document:?
TF: the term frequency.?
IDF: the inverted document frequency.?
TF-IDF: the product of TF and IDF.?
Title word: it indicates whether a key-word appears in the title of the document.?
Bracketed word: it indicates whether aword is enclosed in a bracket in thesource document.?
Position of first appearance: the positionwhere a keyword first appears in a doc-ument, normalized by number of wordsin the document.ID Query Remote Near Recent1 QSL-TFIDF 7 6 82 QTL-TFIDF 16 19 323 QNE 16 21 384 union(2,3) 27 31 485 union(1,2,3) 28 31 486 OQTFIDF 56 66 827 OQNE 62 68 858 OverlapTFIDF 52 51 749 OverlapNE 55 62 83Table 1: Recall (%age) of simple queries.
?Remote?refers to news documents more than a year ago;?Near?
refers to documents about 3 months ago; ?Re-cent?
refers to documents in the last two weeks.477?
NE types: it indicates whether a keywordis a person, organization, location, nu-merical expression, or non NE.The confidence on translation quality is man-ifested in the following features:?
Translation source: it indicates whetherthe keyword (in TL) is produced by MTsystem, dictionary, or by both.?
Original word: it indicates whether thekeyword is originally written in Englishin the source document.
Note that thisfeature also manifests the representative-ness of a document.?
Dictionary rank: if the keyword is a NEproduced by dictionary, this feature indi-cates the rank of the NE keyword amongall translation options registered in thedictionary.It is difficult to definitely classify a TL key-word into good or bad translation in absolutesense, and therefore we take the alternative ofranking TL keywords with respect to the twoprinciples.
The learning algorithm used is Rank-ing SVM (Herbrich et al, 2000; Joachims,2006), which is a state-of-the-art method of the?Learning to rank?
framework.The training dataset of the keyword re-rankercomprises 1,900 Chinese/English news docu-ment pairs crawled from the Web3.
This set isnot part of 11,000 pages in section 2.
Thesedocument pairs share the same properties asthose 300 pairs used in Section 3.1.
For eachEnglish/target document, we build a set TALL,which contains all words in the English docu-ment, and also a set TNE, which is a subset ofTALL such that all words in TNE are NEs in TALL.The words in both sets are ranked by TFIDF.On the other hand, for each Chinese/sourcedocument, we machine-translate it and thenstore the translated words into a set S, and wealso add the dictionary translations of the sourceNEs into S. Note that S is composed of bothgood translations (appearing in the target docu-ment) and bad translations (not appearing in thetarget document).Then there are two ways to assign labels tothe words in S. In the first way of labeling(LALL), the label 3 is assigned to those words inS which are ranked among top 5 in TALL, label 23 We also attempt to add more training data for re-rankingbut the performance remain the same.to those ranked among top 10 but not top 5 inTALL, 1 to those beyond top 10 but still in TALL,and 0 to those words which do not appear inTALL at all.
The second way of labeling, LNE, isdone in similar way with respect to TNE.
Col-lecting all training samples over all documentpairs, we can train a model, MALL, based on la-beling LALL, and another model MNE, based onlabeling LNE.The trained models can then be applied to re-rank the keywords of simple queries.
In thiscase, a set STEST is constructed from the 300Chinese documents in similar way of construct-ing S. We repeat the experiment in Section 3.1with two new queries:1) QRANK-TFIDF: the top N keywords from re-ranking STEST by MALL;2) QRANK-NE: the top N keywords from rerank-ing STEST by MNE.Again N is chosen as 10.The results shown in Table 2 indicate that,while the re-ranked queries still perform muchpoorer than oracle queries (Tests 6 and 7 in Ta-ble 1), they show great improvement over thesimple queries (Tests 1 to 5 in Table 1).
Theresults also show that re-ranked queries basedon NEs are more reliable than those based oncommon words.4 Sentence pair ExtractionThe document pairs obtained by the variousqueries described in Section 3 are used to pro-duce sentence pairs as SMT training data.
Thereare two different methods of extraction for cor-pora of different nature.4.1 For Comparable CorporaSentence pair extraction for comparable corpusis the same as that elaborated in Munteanu andMarcu (2005).
All possible sentence pairs areenumerated from all candidate document pairsproduced in Phase 1.
These huge number ofcandidate sentence pairs are first passed to acoarse sentence pair filter, which discards veryunlikely candidates by heuristics like sentenceID Query Remote Near Recent10 QRANK-TFIDF 18 20 2911 QRANK-NE 35 43 5412 union(10,11) 39 49 63Table 2: Recall (%age) of re-ranked queries.478length ratio and percentage of word pairs regis-tered in some dictionary.The remaining candidates are then given to aMaximum Entropy based classifier (Zhang,2004), which uses features based on alignmentpatterns produced by some word alignmentmodel.
In our experiment we use the HMMalignment model with the NIST SMT trainingdataset.
The sentence pairs which are assignedas positive by the classifier are collected as themined comparable corpus.4.2 For Parallel CorporaThe sentence pairs obtained in Section 4.1 arefound to be mostly not genuine mutual transla-tions.
Often one of the sentences contains someextra phrase or clause, or even conveys differentmeaning than the other.
It is doubtful if the doc-ument pairs from Phase 1 are too noisy to beprocessed by the sentence pair classifier.
Analternative way for sentence pair extraction is tofurther filter the document pairs and discard anypairs that do not look like parallel.It is hypothesized that the parallel relation-ship between two documents can be assimilatedby the word alignment between them.
The doc-ument pair filter produces the Viterbi alignment,with the associated probability, of each docu-ment pair based on IBM Model 1 (Brown et al,1993).
The word alignment model (i.e.
the sta-tistical dictionary used by IBM Model 1) istrained on the NIST SMT training dataset.
Theprobability of the Viterbi alignment of a docu-ment pair is the sole basis on which we decidewhether the pair is genuinely parallel.
That is,an empirically determined threshold is used todistinguish parallel pairs from non-parallel ones.In our experiment, a very strict threshold is se-lected so as to boost up the precision at the ex-pense of recall.There are a few important details that enablethe document pair filter succeed in identifyingparallel text:1) Function words and other common wordsoccur frequently and so any pair of commonword occupies certain probability mass inan alignment model.
These common wordsenable even non-parallel documents achievehigh alignment probability.
In fact, it is wellknown that the correct alignment of com-mon words must take into account position-al and/or structural factors, and it is benefi-cial to a simple alignment model like IBMModel 1 to work on data without commonwords.
Therefore, all words on a compre-hensive stopword list must be removedfrom a document pair before word align-ment.2) The alignment probability must be normal-ized with respect to sentence length, so thatthe threshold applies to all documents re-gardless of document length.Subjective evaluation on selected samplesshows that most of the document pairs kept bythe filter are genuinely parallel.
Thus the docu-ment pairs can be broken down into sentencepairs simply by a sentence alignment method.For the sentence alignment, our experiments usethe algorithm in Moore (2002).5 ExperimentsIt is a difficult task to evaluate the quality ofautomatically acquired bilingual corpora.
As ourultimate purpose of mining bilingual corpora isto provide more and better training data forSMT, we evaluate the parallel and comparablecorpora with respect to improvement in Bleuscore (Papineni et al, 2002).5.1 Experiment SetupOur experiment starts with the 11,000 Chinesedocuments as described in Section 2.
We usevarious combinations of queries in documentpair retrieval (Section 3).
Based on the candi-date document pairs, we produce both compara-ble corpora and parallel corpora using sentencepair extraction (Section 4).
The corpora are thengiven to our SMT systems as training data.The SMT systems are our implementations ofphrase-based SMT (Koehn et al, 2003) and hi-erarchical phrase-based SMT (Chiang, 2007).The two systems employ a 5-gram languagemodel trained from the Xinhua section of theGigaword corpus.
There are many variations ofthe bilingual training dataset.
The B1 section ofthe NIST SMT training set is selected as thebaseline bilingual dataset; its size is of the sameorder of magnitude as most of the mined corpo-ra so that the comparison is fair.
Each of themined bilingual corpora is compared to thatbaseline dataset, and we also evaluate the per-formance of the combination of each mined bi-lingual corpus with the baseline set.479The SMT systems learn translation knowledge(phrase table and rule table) in standard way.The parameters in the underlying log-linearmodel are trained by Minimum Error RateTraining (Och, 2003) on the development set ofNIST 2003 test set.
The quality of translationoutput is evaluated by case-insensitive BLEU4on NIST 2005 and NIST 2008 test sets4.5.2 Experimental resultTable 3 lists the size of various mined paralleland comparable corpora against the baseline B1bilingual dataset.
It is obvious that for a specifictype of query in document pair retrieval, theparallel corpus is significantly smaller than thecorresponding comparable corpus.The apparent explanation is that a lot of doc-ument pairs are discarded due to the document4 It is checked that there is no sentence in the test setsoverlapping with any sentences in the mined corpus.pair filter.
Note that the big difference in size ofthe two comparable corpora by single queries,i.e., QRANK-NE and M&M, verifies again that re-ranked queries based on NEs are more reliablein sentence pair extraction.Table 4 lists the Bleu scores obtained byaugmenting the baseline bilingual training setwith the mined corpora.
The most importantobservation is that, despite their smaller size,parallel corpora lead to no less, and often better,improvement in translation quality than compa-rable corpora.
That is especially true for thecase where document pair retrieval is based onall five types of query5.
The superiority of paral-lel corpora confirms that, in Phase 2 (sentencepair extraction), quality is more important thanquantity and thus the filtering of documentpair/sentence pair must not be generous.On the other hand, sentence pair extractionfor parallel corpora generally achieves the bestresult when all queries are applied in documentpair retrieval.
It is not sufficient to use the moresophisticated re-ranked queries.
That means inPhase 1 quantity is more important and we mustseek more ways to retrieve as many documentpairs as possible.
That also confirms the empha-sis on recall in document pair retrieval.Looking into the performance of comparablecorpora, it is observed that the M&M querydoes not effectively apply to Web mining ofcomparable corpora but the proposed queries do.Any of the proposed query leads to better resultthan the conventional method, i.e.
M&M.Moreover, it can be seen that all four combina-tions of proposed queries achieve similar per-5 QSL-TFIDF, QTL-TFIDF, QNE, QRANK-TFIDF, and QRANK-NEQueries SPextraction#SP #SLwords#TLwordsBaseline: B1 in NIST 68K 1.7M 1.9MM&M comparable 43K 1.1M 1.2MQRANK-NE comparable 98K 2.7M 2.8Mall simple comparable 98K 2.6M 2.9Mall ranked comparable 115K 3.1M 3.3Mall query comparable 135K 3.6M 4.0MQRANK-NEall simpleparallelparallel66K52K1.9M1.5M1.8M1.4Mall ranked parallel 73K 2.1M 2.0Mall query parallel 90K 2.5M 2.4MTable 3: Statistics on corpus size.
SP means sentencepair.
?all simple?, ?all ranked?, and ?all query?
refer tothe merge of the retrieval results of all simple queries,all re-ranked queries, and all simple and re-ranked que-ries, respectively; M&M (after Munteanu and Marcu(2005)) refers to QSL-TFIDF.Bilingual Training CorpusPhrase-based SMT (PSMT) Hierarchical PSMTNIST 2005 NIST 2008 NIST 2005 NIST 2008B1 (baseline) 33.08 21.66 32.85 21.18B1+comparable(M&M) 33.51(+0.43) 22.71(+1.05) 32.99(+0.14) 22.11(+0.93)B1+comparable(QRANK-NE) 34.81(+1.73) 23.30(+1.64) 34.43(+1.58) 22.85(+1.67)B1+comparable(all simple) 34.74(+1.66) 23.48(+1.82) 34.28(+1.43) 23.18(+2.00)B1+comparable(all ranked) 34.79(+1.71) 23.48(+1.82) 34.37(+1.52) 23.06(+1.88)B1+comparable(all query) 34.74(+1.66) 23.19(+1.53) 34.46(+1.61) 23.12(+1.94)B1+parallel(QRANK-NE) 34.75(+1.67) 23.37(+1.71) 34.24(+1.39) 23.45(+2.27)B1+parallel(all simple) 34.99(+1.91) 23.96(+2.30) 34.94(+2.09) 23.35(+2.17)B1+parallel(all ranked) 34.76(+1.68) 23.41(+1.75) 34.54(+1.69) 23.59(+2.41)B1+parallel(all query) 35.40(+2.32) 23.47(+1.81) 35.27(+2.42) 23.61(+2.43)Table 4: Evaluation of translation quality improvement by mined corpora.
The figures inside brackets referto the improvement over baseline.
The bold figures indicate the highest Bleu score in each column forcomparable corpora and parallel corpora, respectively.480formance.
This illustrates a particular advantageof using a single re-ranked query, viz.
QRANK-NE,because it significantly reduces the retrievaltime and downloading space required for docu-ment pair retrieval as it is the main bottleneck ofwhole process.Table 5 lists the Bleu scores obtained by re-placing the baseline bilingual training set withthe mined corpora.
It is easy to note that transla-tion quality drops radically by using mined bi-lingual corpus alone.
That is a natural conse-quence of the noisy nature of Web mined data.We should not be too pessimistic about Webmined data, however.
Comparing the Bleuscores for NIST 2005 test set to those for NIST2008 test set, it can be seen that the reduction oftranslation quality for the NIST 2008 set ismuch smaller than that for the NIST 2005 set.
Itis not difficult to explain the difference.
Boththe baseline B1 training set and the NIST 2005comprise news wire (in-domain) text only.
Alt-hough the acquisition of bilingual data also tar-gets news text, the noisy mined corpus can nev-er compete with the well prepared B1 dataset.On the contrary, the NIST 2008 test set containsa large portion of out-of-domain text, and so theB1 set does not gain any advantage over Webmined corpora.
It might be that better and/orlarger Web mined corpus achieves the sameperformance as manually prepared corpus.Note also that the reduction in Bleu score byeach mined corpus is roughly the same as thatby each other, while in general parallel corporaare slightly better than comparable corpora.6 Conclusion and Future WorkIn this paper, we tackle the problem of miningparallel sentences directly from the Web astraining data for SMT.
The proposed methodessentially follows the corpus mining frame-work by pioneer work like Munteanu and Mar-cu (2005).
However, unlike those conventionalapproaches, which work on closed documentcollection only, we propose different ways offormulating queries for discovering paralleldocuments over Web search engines.
Usinglearning to rank algorithm, we re-rank keywordsbased on representativeness and translationquality.
This new type of query significantlyoutperforms existing query formulation in re-trieving document pairs.
We also devise a doc-ument pair filter based on IBM model 1 forhandling the noisy result from document pairretrieval.
Experimental results show that theproposed approach achieves substantial im-provement in SMT performance.For mining news text, in future we plan toapply the proposed approach to other languagepairs.
Also, we will attempt to use meta-information implied in SL document, such as?publishing date?
or ?news agency name?, asfurther clue to the document pair retrieval.
Suchmeta-information may likely to increase theprecision of retrieval, which is important to theefficiency of the retrieval process.An important contribution of this work is toshow the possibility of mining text other thannews domain from the Web, which is anotherpiece of future work.
The difficulty of this taskshould not be undermined, however.
Our suc-cess in mining news text from the Web dependson the cue phrases available in news articles.These cue phrases more or less indicate the ex-istence of corresponding articles in another lan-guage.
Therefore, to mine non-news corpus, weshould carefully identify and select cue phrases.Bilingual Training CorpusPhrase-based SMT Hierarchical PSMTNIST 2005 NIST 2008 NIST 2005 NIST 2008B1 (baseline) 33.08 21.66 32.85 21.18comparable(M&M) 20.84(-12.24) 14.33(-7.33) 20.65(-12.20) 13.73(-7.45)comparable(QRANK-NE) 26.78(-6.30) 18.54(-3.12) 27.10(-5.75) 18.02(-3.16)comparable(all simple) 26.39(-6.69) 18.52(-3.14) 26.40(-6.45) 18.22(-2.96)comparable(all ranked) 27.36(-5.72) 18.89(-2.77) 27.40(-5.45) 18.72(-2.46)comparable(all query) 27.96(-5.12) 19.27(-2.39) 27.83(-5.02) 19.46(-1.72)parallel(QRANK-NE) 26.37(-6.71) 18.70(-2.96) 26.47(-6.38) 18.51(-2.67)parallel(all simple) 25.65(-7.43) 18.69(-2.97) 25.28(-7.57) 18.55(-2.63)parallel(all ranked) 26.86(-6.22) 18.94(-2.72) 27.10(-5.75) 18.78(-2.40)parallel(all query) 27.58(-5.50) 19.73(-1.93) 28.10(-4.75) 19.52(-1.66)Table 5: Evaluation of translation quality by mined corpora.481ReferencesAbdul-Rauf, Sadaf and Holger Schwenk.
2009.
Ex-ploiting Comparable Corpora with TER andTERp.
In Proceedings of ACL-IJCNLP 2009workshop on Building and Using ComparableCorpora, pages 46?54.Brown, Peter F., Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2): 263-311.Chen, Jiang and Jian-Yun Nie.
2000.
AutomaticConstruction of Parallel Chinese-English Corpusfor Cross-Language Information Retrieval.
InProceedings of NAACL-ANLP, pages 21-28.Chiang, David.
2007.
Hierarchical Phrase-basedTranslation.
Computational Linguistics, 33(2):202-228.Fung, Pascale, and Percy Cheung.
2004.
Mining verynon-parallel corpora: Parallel sentence and lexi-con extraction via bootstrapping and EM.
In Pro-ceedings of 2004 Conference on Empirical Meth-ods in Natural Language Processing, pages 57-63.Gao, Jianfeng, Mu Li, and Changning Huang.
2003.Improved Source-Channel Models for ChineseWord Segmentation.
In Proceedings of the 41stAnnual Meeting of the Association for Computa-tional Linguistics, pages 272-279.Herbrich, Ralf, Thore Graepel, and Klaus Obermayer.2000.
Large margin rank boundaries for ordinalregression.
In Advances in Large Margin Classifi-ers, pages 115?132.
MIT Press, Cambridge, MA.Jiang, Long, Shiquan Yang, Ming Zhou, XiaohuaLiu, and Qingsheng Zhu.
2009.
Mining BilingualData from the Web with Adaptively Learnt Pat-terns.
In Proceedings of the 47th Annual Meetingof the Association for Computational Linguisticsand 4th International Joint Conference on NaturalLanguage Processing, pages 870-878.Joachims, Thorsten.
2006.
Training Linear SVMs inLinear Time.
In Proceedings of the 12th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 217-226.Koehn, Philipp, Franz Och, and Daniel Marcu.
2003.Statistical Phrase-based Translation.
In Proceed-ings of conference combining Human LanguageTechnology conference series and the NorthAmerican Chapter of the Association for Compu-tational Linguistics conference series, pages 48-54.Moore, Robert.
2002.
Fast and Accurate SentenceAlignment of Bilingual Corpora.
In Proceedingsof the 5th conference of the Association for Ma-chine Translation in the Americas, pages 135?144.Munteanu, Dragos, and Daniel Marcu.
2005.
Im-proving Machine Translation Performance by Ex-ploiting Non-Parallel Corpora.
ComputationalLinguistics, 31(4): 477-504.Och, Franz J.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofthe 41st Annual Meeting of the Association forComputational Linguistics, pages 160-167.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a Method for Auto-matic Evaluation of Machine Translation.
In Pro-ceedings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 311-318.Resnik, Philip, and Noah Smith.
2003.
The Web as aParallel Corpus.
Computational Linguistics, 29(3):349-380.Shi, Lei, Cheng Niu, Ming Zhou, and Jianfeng Gao.2006.
A DOM Tree Alignment Model for MiningParallel Data from the Web.
In Proceedings of the21st International Conference on ComputationalLinguistics and the 44th Annual Meeting of the As-sociation for Computational Linguistics, pages489-496.Utiyama, Masao, and Hitoshi Isahara.
2003.
ReliableMeasures for Aligning Japanese-English NewsArticles and Sentences.
In Proceedings of the 41stAnnual Meeting of the Association for Computa-tional Linguistics, pages 72-79.Vogel, Stephan.
2003.
Using noisy bilingual data forstatistical machine translation.
In Proceedings ofthe 10th Conference of the European Chapter ofthe Association for Computational Linguistics,pages 175-178.Yang, Christopher C., and Kar Wing Li.
2003.
Au-tomatic construction of English/Chinese parallelcorpora.
Journal of the American Society for In-formation Science and Technology, 54(8):730?742.Zhang, Le.
2004.
Maximum Entropy ModelingToolkit for Python and C++.http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htmlZhao, Bing, and Stephan Vogel.
2002.
Adaptive Par-allel Sentences Mining from Web Bilingual NewsCollection.
In Proceedings of IEEE internationalconference on data mining, pages 745-750.482
