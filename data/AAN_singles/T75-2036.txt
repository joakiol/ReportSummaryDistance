THE COMMONSENSE ALGORITHMAS A BASIS FOR COMFUTER MODELSOF HUMAN MEMORY, INFERENCE, BELIEFAND CONTEXTUAL LANGUAGE COMPREHENSIONChuck RiegerDepartment of Computer ScienceUnivers i ty of MarylandABSTRACTThe notion of a commonsensea lgor i thm is presented as a basicdata structure for model ing humancognit ion.
This data structureunif ies many current ideas abouthuman memory and informationprocess ing .
The structure isdefined by specifying a set ofproposed cognit ive primitive linkswhich, when used to build up largestructures of actions, states,statechanges and tendencies,provide an adequate formal ism forexpressing human plans andactivit ies, as well as generalmechanisms and computer algorithms.The commonsense algor i thm is a typeof framework (as Minsky has definedthe term) for represent ingalgor i thmic processes, hopeful lythe way humans do.I.
INTRODUCTION AND MOTIVATIONIt is becoming increasingly evident tohuman intel l igence model bui lders andtheorists that, in order to character izehuman knowledge and bel ief as computer datastructures and processes, it is necessary todeal with very large, expl ic i t ly  unif iedstructures rather than smaller, ununif iedfragments.
The reason for this seems to bethat the or iginal  experiences which causedthe structures and processes to exist in thefirst place come in chunks themselves;knowledge is never gained outside of somecontext, and in gaining some piece ofknowledge X in context C, X and C becomeinseparable.
This suggests that it ismeaningless to model "a piece of knowledge"without regard for the larger structures ofwhich it is a part.
If our goal is to builda robot which behaves and perceives inmanners s imi lar to a human, this means thatthe process by which the robot selects apiece of knowledge as being appl icable tothe planning, executory, inferential  orinterpret ive process at hand at the momentis a function not only of the specif icproblem, but also of the larger context inwhich that instance of planning, execution,inference or interpretat ion occurs.
If, forexample, our robot sees his friend with awretched facial expression, the inference hemakes about the reasons for his fr iend'smisery wil l  reflect the larger picture ofwhich he is aware at the time: his friendhas just returned from a trip to purchaseopera t ickets vs. his friend has Just eatenthe cache of mushrooms col lected yesterdayvs .
.
.
.
.
The same pervasiveness of contextexists in the realm of the robot'sinterpretat ions of visual perceptions: thevery same object (visible at eye level) wi l l18obe perceived out of the corner of his eye inone situation as the cyl indr ical  top of hiselectr ic coffee grinder (he is at home inhis kitchen), but as the f lasher of a policecar (he is speeding on the freeway) inanother.
This suggests that at everymoment, some fairly large swatch of hisknowledge about the world somehow has foundits way to the foreground to exert itsinfluence; as our robot moves about,swatches must fade in and out, sometimescoalescing, so that at any moment, just theright one is standing by to help guide actsof planning, infePence and perception.Marvin Minsky has captured this wholeidea very neatly in his wide ly-c i rcu lated"Frames" paper \[MI\].
While this paperdescribes an overal l  approach to model inghuman memory, inference and beliefs, westi l l  lack any specif ic formulat ion of theingredients which make up the large,expl ic i t ly -uni f ied structures which seem tounderl ie many higher- level  human cognit ivefunctions.
It is the purpose of this paperto define the notion "commonsense algor ithm"(CSA) and to propose the CSA as the basicCognit ive structure which underl ies thehuman processes of planning, inference andcontextual  interpretat ion of meaning.I do not have a complete theory yet:the intent of this paper is to record amemory dump of ideas accumulated over thepast few months and to show how they canunify my past ideas on inference and memory,as well as the ideas of others.II.
THE SCOPE OF THE CSA'S APPLICABIL ITYMost of human knowledge can bec lassi f ied as either static or dvnamic.
Forexample, a person's static knowledge of anautomobi le tells him its general physicalshape, size, posit ion of steering wheel,wheels, engine, seats, etc.
; these are theabstract aspects of a car which, a l thoughmany differ in detail  from car to car, areinherent ly  unchanging.
They are in essencethe physical  def init ion of a car.
On theother hand, a person s dynamic knowledge ofa car tel ls him the functions of the variouscomponents and how and why to coordinatethem when the car is appl ied to some goal.The static knowledge tells the person whereto expect the steering wheel to be when hegets in; the dynamic knowledge tells him howto get in in the first place, and what to dowith the wheel (and why) once he is in.
Fora robot immersed in a highly k inematicworld -- physical ly, psychologica l ly  andsocial ly  -- a very large part of his bel iefsand knowledge must relate to dynamics: howhe can effect changes in himself  and hisworld, and how he perceives other robotsef fect ing changes.
It is the purpose of theCSA to capture the dynamics of the world inbel ief structures which are amenable tocomputer manipulat ion of plans, inferenceand contextual  interpretat ion.It should be stressed that the phrase"dynamics of the world" is intended in itsbroadest possible sense.
As wil l  bee laborated upon in a later section, theIIIIIIiIiIIIphrase is intended to encompass suchseemingly diverse robot/human act ivit ies as:I. communicat ing with anotherrobot/human (e.g., how to transferinformation, insti l l  wants,convince, etc.)2.
getting about in the world3.
building things (both physical andmental) and understanding theoperation of things already builtby others4.
conceiving, designing .andimplementing computer programs andother commonsense algorithms (aspecial form of building)5. interpret ing sequences ofperceptions (e.g., languageutterances) in context6.
making contextual ly meaningfulinferences from perceptionsI am convinced that all such dynamicsof the world can and should be expressed ina uniform CSA formalism built around arelat ively small number of cognit ivelyprimitive ingredients.III.
EVOLUTION OF THE CSA IDEAThe next section will define a CSA as anetwork- l ike structure consist ing of eventstied together by primitive links.
Taken asa whole, the CSA specif ies a process: how toget something done, how something works,etc.
A computer scientist 's first reactionto this type of structure is "Oh yes, that'san AND/OR problem-reduct ion graph" (seeNilsson \[NI\] for example).
Figure I showsan AND/OR graph for how to achieve the goalstate "a McDonald's hamburger is in P'sstomach."
Edges with an arc through themspecify AND successors of a node (subgoals,all of which achieved imply the parent nodehas been achieved); edges with no arcthrough them specify OR successors(subgoals, any one of which being suff ic ientto achieve the parent goal).AND/OR graphs have been demonstratedadequate in practice for guiding variousaspects of problem-solving behavior inexist ing robots (see \[$I\] for example) .However, they are intuit ively nottheoret ical ly  adequate structures forrepresenting general knowledge of worlddynamics: their principal def ic iency is thatthey are ad-hoc construct ions which expressneither the implicit conceptualrelat ionships among their components, northe inherent types of their components.Because of this, there is no constraint ontheir organization, and this means that twoAND/OR graphs which accomplish or model thesame thing might bear very l ittleresemblance to one-another when in fact theyare conceptual ly very similar.
This may belittle more than a nuisance in practice, butit is undesirable in principle because itmakes learning, reasoning by analogy,sharing of subgoals, etc.
ted ious  if notimpossible in a general ized problem solver.A refinement of the notion of an AND/ORgraph introduces the concepts of causal ityand enablement, and actions and states(statechanges); edges in the graph aredist inguished as either causal or enabling,the nodes are dist inguished as eitheractions or states, and the graph obeys thesyntactic contraints:(a) actions cause states(b) states enable actionsBob Abelson \[AI\] was among the first toemploy these histor ical ly very old conceptsin the framework of a computer model ofhuman belief, and since then, numerouscomputer-or iented systems of knowledgereDresentat ion (e.g., Schank's conceptualdeDendency\[S2\] ,  Schmidt's models of personalcausation \[$4\]), as well as systems ofinference (Rieger \[RI\], Charniak \[CI\]) havefound these four concepts to be vital tomeaning representat ion and inference.
Insome sense, enablement, causality, statesand actions seem to be cognitive primitives.Figure 2 is a ref inement of Figure I whichmakes explicit the nature of each node andeach connect ing arc, and hence theunderlying gross conceptual structure of thealgorithm.While the inclusion of these fourconcepts (and their result ing syntacticconstraints) in the basic paradigm makes fora theoret ical ly more coherentrepresentation, the scheme is still toocoarse to capture the kinds of detai ledknowledge of algor ithms people possess.
Thefol lowing section proposes an extendedframework of event types and eventconnectors based on these four notions andsome others.
These event types andconnectors will be regarded asmodel -pr imit ives which hopeful ly are incorrespondence with "psychologicalpr imit ives" in humans.IV.
DEFINITION OF THE COMMONSENSE ALGORITHMIn the new formalism, a CSA consists ofnodes of five types:I. WANTS2.
ACTIONS3.
STATES4.
STATECHANGES5.
TENDENCIESThe first four types are not new (see \[$3\]for example), and wil l  not be covered herebeyond the fol lowing brief mention.
A WANTis some goal state which is desired by apotential  actor.
An action is something an(animate) actor does or can do: it isenabled by certain states (certaincondit ions which must be true in order forthe action to begin and/or proceed), and inturn causes other states (discrete) orstatechanges (continuous) to occur.
Actionsare character ized by an actor, amodel-pr imit ive action, a time aspect, alocation aspect, and a conceptual caseframework which is specif ic to eachmodel -pr imit ive action.
States arecharacter ized by an object, an attribute, a181value and a time aspect; statechanges arecharacter ized by an object, a continuousstate scale (temperature, degree of anger,distance, etc.
), a time aspect and beginningand end points on the scale.It is the notion of a tendency which isnew and which serves to unify a class ofproblems which have been cont inual lyexper ienced in represent ing processes.Basical ly, a tendency is an actor lessaction.
Tendencies are character ized byspeci fy ing a set of enabling condit ions ,anda set of result states and/or statechanges.Whenever the enabling condit ions aresatisf ied, the tendency, by some unspeci f iedmeans, causes the states and statechangesspeci f ied as the tendency's results.
Hence,a tendency may be regarded as a special  typeof non-purposive action which must occurwhenever all its enabling condit ions aresatisf ied.
Contrast ing the notion of atendency with the notion of an action yieldsa rather compact def init ion of what makes a"vol i t ional" act ion vol it ional:  a vol i t ionalact ion is an action which need not occureven though all its physical  enabl ingcondit ions are met.
The reason it may notoccur is, of course, that the actor does notdesire it to occur; tendencies have no suchdesires.The abstract notion of a tendency ismeant to be general-purpose, to character izea wide variety of phenomena which are notactions, but action-l ike.
Examples oftendencies are:I.
GRAVITY, PRESSURE, MAGNETISM,ATOMIC-FISSION, HEAT-FLOW, and the hostof other physical principles.Commonsense GRAVITY might be capturedas fol lows:**((TYPE .
TENDENCY)(REFERENCE-NAME .
GRAVITY)(ENABLEMENTS .
(UNSUPPORTED OBJ)(LESSP (DISTANCE OBJ EARTH)(ORDERMILES) )(RESULTS .
(STATECHANGE OBJ VELOCITY XX+d (LOC OBJ)(LOC EARTH)))2. human biological  functions: a tendencyto GROW-HUNGRY, GROW-SLEEPY, GROW-OLDER(sole enabl ing condit ion is the passageof time!
), GROW-LARGER, etc.
Forexample:((TYPE .
TENDENCY)(REFERENCE-NAME GROW-HUNGRY)(ENABLEMENTS .
(iNOT (LOC NUTRIENTSSTOMACH))(DURATION * ORDERHOURS)))(RESULTS .
(WANT P (INGEST P NUTRIENTSMOUTH STOMACH))))3. human psychological  functions: thetendency to GROW-LONELY, the tendencyto FORGET, etc.
For example:**The LISP notat ion ref lects someconcurrent thinking on how acommonsense algor i thm system mightactual ly be engineered.
A forthcomingreport wil l  describe progress towardimplementing the ideas in this paper.
182((TYPE ?
TENDENCY)(REFERENCE-NAME .
GROW-LONELY)(ENABLEMENTS ((ALONE P)iDURATION * ORDERDAYS))(RESULTS .
(WANT P (COMMUNICATE PX))))((TYPE .
TENDENCY)(REFERENCE-NAME .
FORGET)(ENABLEMENTS .
(INHEAD ITEM P)((UNREFERENCED ITEM P)(DURATION * ORDER??
)))(RESULTS (STATECHANGE ITEMREFERENCE-DIFFICULTY X X+d))Tendencies, thus character ized, willplay an important role in model ingalgor i thmic processes via CSA's.
In fact,adopt ing the notion of a tendency as a modelpr imit ive points out a rather ubiquitousprinciple: humans spend a large amount oftime in planning either how to overcometendencies which stand in the way of theirgoals, or how to harness them at the propertimes in place of an action (e.g., droppingthe large rock on the coconut).
Although atendency 's  pr imary use is at the edge of theworld model, where things happen simplybecause "that's the way things are", it willprobably be desirable to have the abi l i ty toregard as tendencies things which in factcan be explained.
Character iz ing somethingas a tendency even though it may bereduceable to further algor i thms is probablyone tactic a human employs when confrontedwith the analysis of very complex, olnypart ia l ly understood processes.
Even thoughsomething ~ be further explained, thesystem of representat ion should al low thatsomething to be treated as though it were atendency.Tendencies have numerous aspects whichwil l  require expl ic it  character izat ion in acomputer model.
Two such aspects relate to(I) the inherent rapidity with which atendency exerts itself  and (2) thetendency's  periodicity,  if any.
That is,how quickly does a person become hungry(slope of curve), how long does it take toforget something, how rapidly does an objectaccelerate, how fast does the water f lowthrough the nozzle, etc.?
If the tendency isperiodic, what are the parameters descr ib ingits per iodic i ty?
The primit ive CSA linksdescr ibed in the next section wil l  serve inpart to capture such aspects, but they arenot yet adequate.The CS~ nrimit ive L in~ Using thesefive event-types as bui lding blocks (WANTS,ACTIONS, STATES, STATECHANGES, TENDENCIES),the goal is to be able to express thedynamics of just about anything, be it aphysical  device, a psychological  tacticemployed by one person on another, how aperson purchases a McDonald's  hamburger, orhow a computer program functions or wasconstructed.
There are 25 pr imit ive l inksin the current formulation.
They will onlybe defined here, leaving Just i f icaion anddetails of their use for the examples whichwil l  follow, and for subsequent papers onthe subject.
In the fo l lowing definit ions,W, A, S, SC and T wil l  stand for WANT,ACTION, STATE, STATECHANGE and TENDENCY,respectively.II!lIIiIiIiIIIIiIIITYPE I: ONE-SHOT CAUSALITY fA -r~l Action A or tendency T causes state S. 7 The action or tendency  need occur only once; thereafter S will persist until altered by another j action or tendency.
For any given S, there willordinarily be numerous alternative A's or T's in thealgorithmic base which would provide the one-shotcausality.l TYPE 2: CONTINUOUS CAUSALITYI Action or tendency A,T's continuing existencecontinually causes state or statechange S,SC.Whether one-shot or continuous causality isrequired to maintain S or SC is both a function of S or?
SC and its particular environment in a particularalgorithm (i.e., what other tendencies and actions arei influencing it).
Again, there will ordinarily benumerous actions or tendencies in the algorithmic basewhich could provide continuous causality for any givenstate or statechange.I TYPES 3,4: GATED ONE-SHOT AND CONTINUOUS CAUSALITYA,T causes S,SC either one-shot or continuously, \[i providing that all states in \[S\] are satisfied.
!
~4- - '~The flow of causality cannot occur unless statesspecified by \[S\] exist.
That is, even though A,T isoccuring and there is a potential causal relationship ~rbetween A,T and S,SC, the relationship will not bei realized until the gating states become true.TYPE 5: ONE-SHOT ENABLEMENTl State S's one-time existence allows action A ortendency T to proceed.Thereafter, A,T's continuation is no longer afunction of S. A,T will ordinarily have numerousone-shot enablements, in which case, all must besatisfied in order for A or T to proceed.
'State S's continued presence is requisite toaction A's or tendency T's continuance.i S's removal causes A or T to halt.
Any given A orT will ordinarily have numerous continuous enablements,in which case all must reamin true in order for A or Tto proceed.TYPE 7: CAUSAL STATE COUPLINGStates $I, S2 or statechangges SCI, SC2 arecausally coupled; because of this coupling, changes in$I or SCI are synonomous with changes in $2 or SC2.This link provides a way of capturing the relatednessof various aspects of the same situation.TYPE 8: GATED CAUSAL STATE COUPLINGState $2 or statechange(causally coupled to) $Istates in \[S\] are true.SC2 is synonymous withor SCI, provided that allThis link is similar to ungated state coupling, exceptfor the existence of factors which could disrupt thecoupling.
To illustrate, the flow of a fluid into acontainer (a statechange in location of the water) issynonymous with an increase in the amount of water in183the container (another statechange), but only providingthat there is no souL~ce of exit from the container'sbottom.TYPE 9,10,11,12:GATED/NON-GATED)BYPRODUCT (0NE-SHOT/CONTINUOUS,State S or statechange SC is a causal byproduct ofaction A, relative to goal state Sg or SCg.That is, the actor of A, wishing to achieve stateSg or statechange SCg also produces state S orstatechange SC.
The byproduct link 'is truly a causallink; what is and is not a by product must obviouslyrelate to the motive of the actor in performing theaction.
Where gated, all states in \[S\] must besatisf ied in order for the byproduct to occur.TYPE 13: ORIGINAL INTENTWant W is the original  desire (goal state) of anactor.
W is external to the CSA in that its origin isnot expl icable within the CSA itself; it is the outsidedirective which motivated the invocation of some acton.Within an algor i thm for achieving some goal,mot ivat ions are explicable: every subaction is, by itsnature, designed to produce subgoal states which, takentogether, meet the original intent.TYPE 14: ACTION CONCURRENCYActions AI , .
.
.
,An must be concurrent ly executed.This link will arise in the dynamics of an actualplan, rather than be stored or ig inal ly  in thealgor ithmic base explicit ly.
As plans evolve and theactor learns concurrency by rote, the link will beginto appear in the algor ithmic base as well.
Actionconcurrency is nearly always caused by mult ipleenabl ing states for some other action, all of whichmust be cont inual ly  present, or one-t ime synchronizedas a col lect ion of one-shot enablements.TYPE 15: DYNAMIC ANTAGONISMState $I or statechange SCI is antagonist ic  tostate $2 or statechange SC2 along some dimension.This link relates two states Or statechanges whichare opposites in some sense; typical ly  the antagonismlink wil l  make explicit the final link in some sort offeedback cycle in an algorithm.
The link is hard todescribe outside the context of an example; exampleswill appear in the next section.TYPE 16: MOTIVATING DYNAMIC ~NTAGONISMAs with ordinary dynamic antagonism, $I, $2 areantagonist ic  states.
Typically, $2 is required as anenabl ing state (continuous) for some action, but thataction, or some other action, produces $I as abyproduct; this gives rise to the need for anothercorrect ive action A which can suppress the byproduct,therby preserving the original  required enablement.This link is intended to capture the execution dynamicsof a s ituation in which antagonist ic  states areexpected to arise.
That is, it wil l  provide arepresentat ion wherein antagonisms can be ant ic ipatedin advance of the SCA's actual execution.
An exampleof mot ivat ing dynamic antagonism is included in thenext section.184IIiIIiI!IiIIIiII1IIITYPE 17: GOAL-REALIZATION COUPL INGState S is an alternat ive way of expressingoriginal goal W or subgoal Sg .This link suppl ies a way of speci fy ing terminat ioncriteria for CSA's involving repretit ion.
Its use isi l lustrated in one of the examples~TYPE 18: COMPOUND GOAL STATE DEFINITIONState S is a shorthand for expresing the set ofgoal states SI,.
.
.
,Sn.This link allows a "situation" to be character izedas a col lect ion of goal states.
When all goal statesare satisf ied, the situation is satisf ied.
An exampleof a compound goal state would be: "get the kids readyfor the car trip", where this means a set of thingsrather than one thing.TYPES 19,20,21,22: DISENABLEMENTGATED/NON-GATED)(ONE-SHOT/CONTINUOUS,Action A or tendency T one-shot /cont inual ly  causesstate S or statechange SC not to exist.These four forms are shorthands for causal i ty inconjunct ion with antagonism.
They will be pr incipal lyuseful for represent ing acts of d isenabl ing unwantedtendencies.TYPE 23: REPETITION UNTIL THRESHOLDAction A or tendency T occurs repeatedly untilstate S becomes true.This link provides for the repeated appl icat ion ofan action or tendency.
Normally, the action ortendency will, d irect ly or indirectly, causal ly producea statechange along some scale; this statechange willeventual ly threshold at state S.TYPE 24: INDUCEMENTState S's or statechange SC's existence induceswant W in a potential  actor.Origins of wants can be expl ic i t ly  represented viathis link.
Typically, W wil l  be a stabe which isantagonist ic  to S or SC.
For example ,  if thetemperature is too high in the room, the want is thatthe temperature become lower; if the tendency,PRESSURE, has been enabled, al lowing blood to flow outof P's body, the induced want is that this tendency bedisenabled, and hence that the antagonism of one ofPRESSURE's enabl ing states start to exist.TYPE 25: OPTIMIZATION MARKERState S is an enabling condit ion for action A, andthis re lat ionship makes possible an opt imizat ion duringthe execution of A in a part icular environment.When several actions arise in a plan, they may shareenabling states.
This means that when the plans areconsidered together, some of the states needed for oneaction may coincide with those needed for another.
Theopt imizat ion marker allows this phenomenon to berecorded.
Its interpretat ion is: when state S becomestrue, consider performing acton A~ because action Aalso has S as an enabling state.
~ denotes a savings.185DThese are the commonsense algorithmprimitive links.
It is felt that they areconceptually independent enough ofone-another so that unique algorithms willbe forced into unique, or at least similar,representations under this formalism.Although it is the eventual intent of thetheory to be able to capture all the nuancesof intentional human problem-solvingbehavior, there is no real feeling yet forthe completeness of this set of links inthis regard; all that can be said now isthat they do seem to suggest a reasonableapproach to representing large classes ofpurposive human behavior.
The adequacy ofthese primitives for representing devicesand mechanisms, on the other hand, is easierto see, at least intuitively; the links seemto be adequate for some fairly complex"purposive" mechanisms.
Accordingly, thefirst example of their use will be tocharacterize a mechanism very dear to mostof us.V.
EXAMPLES OF COMMONSENSE ALGORITHMSEXAMPLE I.
Operation of g reverse-traptoilet \[Figure 3\]As a first test of the theory, thereverse-trap toilet is a relativelydemanding mechanism.
It is a complexfeedback mechanism which is the product ofsome rather sophisticated humanproblem-solving.
It is thereforeinteresting both in its own right and as atangible manifestation of human-concoctedcausality and enablement.
By one simpleaction, a complex sequence of tendencies isunleasehed; the sequence not only stopsitself, but restores the system to itsinitial state, and does something useful inthe process.The English description of theschematic of Figure 4 is as follows: Thetrip handle is pushed down, one-shot causingthe flush-ball to be raised; this one-shotenables the tendency to float, in turncontinually causing the float ball to remainraised.
The float ball's being raised issynonomous with the flush valve being open,and this openness continuously enab les  thetendency of gravity to move water from thetank to the bowl beneath (as long as waterremains in the tank, of course.)
Thismovement of water is synonomous with twoother state changes: a decrease of waterheight in the tank, and an increase of waterheight in the bowl.
The increase of bowlwater height thresholds when the waterreaches waste channel lip level, at whichtime it begins providing continuousenablement for gravity to move the waterinto the waste channel; this movementthresholds when the channel fills, providingthe beginning of continuous enablement ofthe tendency capillary action.
Thistendency, in turn, sustains the flow ofwater from the bowl to the waste channel,continually moving waste water into thesewer.
This action ceases when the bowlbecomes empty.
Meanwhile, the tendencygravity is continually moving water from thetank to the bowl.
This is synonomous with a186decrease in tank water height, and thisdecrease thresholds at point X, synonomouswith the fresh water supply valve opening.This opening enables the tendency pressureto move water from the fresh water line intothe tank; this is synonomous with anincrease in tank water height, but onlyproviding that the flush valve is closed(this will have to wait for the movement ofwaste from tank to bowl to cease).
When thetank water height finally begins itsincrease, this increase will threshold atpoint X again, this time being synchronouswith the ball cock supply valve's beingclosed, stopping the fresh water and hencethe tank water height increase.
At thispoint, the system has become quiescentagain.
(Note: in the actual simulationwhich will be performed, flow rates, or moregenerally, rates of statechanges, will beincorporated.
)EXAMPLE 2.
Sawing a hoard in half todecrease its length (Figure 5)Figure 5 is a bare-bones representationOf a purposive human process: sawing a boardin two using a handsaw.
This CSAil lustrates the concepts of motivatingdynamic antagonsim, original intent andbyproduct with respect to a goal.
Theschematic of Figure 5 is only a fragment ofthe larger algorithm; many enabling statesand byproducts, as well as theircompensatory actions have been omitted.
Inthis CSA, the act of sawing for the purposeof decreasing the board's length produces,among others, the byproduct of the board'smoving.
Since a stationary board is a gatecondition on the flow of causality from thesawing action to the statechange in cutdepth, the two states joined by themotivating dynamic antagonsim link form anantagonistic pair, indicating in advance ofactual execution that it will be necessaryto perform a compensatory action: hold thewood down.
If we were to illustrate more ofthis algorithm, it might be found thatholding the wood down would require morehands than were available.
This wouldprovide another dynamic antagonsim whichwould motivate the engagement of anothercompensatory action, such as "call forhelp," "go to a vise," etc.It should again be pointed out thatpoints of antagonism could alternatively bedetected at the execution time of the CSAand compensatory solutions dynamicallyfabricated.
This would likely occur viasome sort of interrupt mechanism.
But theantagonsim link allows for planning ahead(e.g.
when two arbitrary algorithms areselected to accomplish a task, theircoexistence will probably not always bewithout antagonism -- this allows theplanning mechanism to anticipate and solvesuch antagonisms before execution).
Also,after a successful plan involvingantagonisms has actually been executed, thislink provides a means of recording once andfor all the compensating actions which wereperformed.IIIIIIIEXAMPLE 3.
Vicious cycles <Figure 6)Consider tendencies such as fire andforgetfulness.
Both roughly follow theparadigm: a tendency has state S as acontinuous enablement, and produces the samestate as continuous causality.
Oncestarted, such a system is self-sustaining.In the case of fire, a one-shot causingaction causes a statechange in temperaturewhich thresholds at the point of thematerial 's  combustion temperature; thisenables the tendency to burn, which in turnproduces as a continual byproduct heat,causing a vicious cycle.
In forgetting, thetendency to forget X is enabled by notreferencing X for periods of time; but as Xgrows more forgotten, it becomes lessreferenceable.
Here, dynamic antagonismlies at the root of the vicious cycle.EXAMPLE 4.
Descr ipt ion (synthesis) ofcomputer algor i thm (Figure 7)Suppose the goal is to compute theaverage of a table of numbers,TABLE(1), .
.
.
,TABLE(n).
Figure 7 shows bothhow to conceive of the algor i thm and how thealgor i thm wil l  actual ly run.
As a computeralgorithm, this is not as fully explicit asmight be desired: it lacks expliciti teration and explicit termination cr i ter iontesting.
These will have to be worked outbefore the theory adequately handlesrepetit ion.iCausal gating seems to play a centralrole in this sort of computer algorithm.Intuitively, this is the case because,though a computer instruct ion typical ly hasno physical enabling condit ions (it could beissued at any time), desired effects can beachieved only by tying the syntax ofinstruct ion causal ity to the semantics oflogical causality.
For example, the flow ofcausal ity from the action "fetch locationSUM to ACI" to the logical semantic state"partial sum in ACI" can take place only iflocation SUM logical ly contains the actualpartial sum at that point!
Otherwise,garbage is fetched.The relat ionships of certain types ofcausal gating and state coupling (e.g.
thevalve closing because the float has risen inthe toilet tank) are not completely apparentyet.
Perhaps state coupling is a shorthandfor an implicit sequence of gatedcausal i t ies between two statechanges.
Onthe other hand, state coupling between twostates, as opposed to statechanges, seems tobe a concept which is independent of gatedcausality.
To i l lustrate; "a nail throughtwo pieces of wood" (state I) has to beregarded as state-coupled to "the pieces ofwood are joined" (state 2, a descr ipt ion ofthe same situation, but at a differentlevel):187~WOODI ,  WOOD2~- _L_Tooo ,JOINED ~/In this type of situation, the statecoupling concept is required at this levelto stop the representat ion of some sort ofinexpl icable "micro-causal i ty" when ittranscends the model's knowledge of theworld.VI.
ALTERNATIVE ACTION SELECTIONIn looking at devices and simpleprocesses such as sawing a board in half,there have been few choices; the causal ityand enablement are in a sense already builtin or strongly prescribed.
In a realplanning environment on the other hand,there will ordinar i ly be numerousalternat ive actions which could causal lyproduce some desired goal state, providingall gating condit ions could be met.
Forexample, if the goal of a planner is toproduce a statechange in his location tosome specif ied point, the various subplansof walking, driving a car, hitching a ride,bicycling, taking a plane, etc.
all suggestthemselves as potential ly relevant, somemore than others.
The one the planneractual ly selects will be a function of morethan Just the relative costs of eachalternative; the select ion will also relateto the inherent appl icabi l i ty,  orreasonableness of the plan, based on thesDecif%cs of where his dest ination isrelative to his current location, weatherconditions, etc.
Of course, all therelevant factors could eventual ly bediscovered by s imulat ing each alternat iveplan before choosing, watching out forundesirable or suboptimal events.
Forexample, in s imulat ing the walking,h i tchhik ing or bicycl ing plans, the plannerfinds himself  outside for potent ia l ly  longdurations.
Hence, if it is raining, thecost is judged high.
If the distance isless than a mile, or is indoors, s imulat ionof the airplane plan leads to some absurdlyhigh costs and perhaps some unsolvableantagonisms.
Certainly, a degree of suchforward simulat ion must occur in planning;however, it seems that the process ofselect ing among alternative actions is,intuit ively, more unif ied than just acol lect ion of forward simulations.For this reason, the model of CSA'sincorporates the notion of a selector,denoted by the construction:SEL is a place where heuristics, as well asforward simulat ion control can reside, Theheurist ics test relevant dimensions(e.g.
distance, weather conditions, etc.)
ofthe context in which the state orstatechange is being sought (either forexecution of some larger plan, or forinterpret ing what another might do in somecontext).
Based on the outcomes of suchtests, the SELector chooses one alternat iveaction as most reasonable.
Currently, theselector function is imagined to exist"outside" the CAS formalism as 'anunrestr icted program which runs and decides.Eventually, since it is one goal of the CSAformalism to be able to represent arbitrarydecision processes (these are, after all,just other algorithms), the SELectorfunction should simply reference other CSA'swhich carry out the heurist ic testing.
Inother words, defer the " intel l igence" inselect ing an alternat ive at this level tounintel l igent CSA's at the next level, andSO on .VII.
LEVELS OF RESOLUTIONS IN CSA'SThe algor ithmic content of a CSA can bedescribed at many different levels ofresolution.
For example, the "action" "takea plane to San Francisco" is quite a bithigher in level and more abstract than theaction "grasp a saw".
In the former, theact of taking a plane somewhere is notreally an action at all, but rather adescr ipt ion of an entire set of actions,themselves related in a CSA; "take a planeto San Francisco" is a high level surrogatefor a low level col lect ion of true actionsin the sense of actual ly performing physicalmovements, etc.
in the real world (thingslike grasping a saw, reaching into pantspocket for some money, and so on).Another example of resolut ion leveldif ferences relates to how enabling statesfor actions are characterized.
For example,in (A2) Abelson employs the primit ive (OKFORobject appl ication),  as in (OKFOR AUTOTRAVEL).
The quest ion here is, what is therelat ionship between this high leveldescr ipt ion of OKness and the specif ics ofwhat OKFOR means for any given object?
Thatis, for a car, OKFOR means "gas in tank","tires inflated", "battery charged",.
.
.
,whereas (OKFOR TOILET FLUSHING) means quitea different set of things.
The basic issueis: should the memory plan and interpret inthe abstract realm of OKFORedness, theninstant iate with details later, or must thedetails serve as the primary planning basis,with the abstract ideas being reserved forother higher level processes such asreasoning by analogy, general izat ion and soforth?
There is probably no cut-and-dr iedanswer; however, the tendency in a CSAsystem would be to favor the details overthe abstract.
But the CSA representat ion isintended to be f lexible enough to accomodateboth the abstract and the concrete.
Theidea of state coupling is an i l lustrat ion ofthis.188VIII.
THE THEORY HAS ONLY JUST BEGUNA later version of this paper willcontain more examples of the CSA, includingits use in language context problems.
Thetheory is by no means complete; toi l lustrate:(I) Is there such a thing as gatedenablement?
The answer seems to be"yes", since it seems reasonable toregard enablement as a flow whichcan be cut off in much the same wayas causality.
Perhaps an exampleof gated enablement i s  when thehorses begin their race at theracetrack: the start gate's beingopen is a one-shot enablement forthe horse to run, but only if thehorse is in the box to start with!If he's not in the box, the gate'sposit ion isn't relevant as anenablement to run; its flow issevered.
(2) What kinds of time and sequencinginformation need to be incorporatedin the formal ism?
For example,causal i ty can be either abrupt orgradual: taking medicine for anulcer p rov ides  a conceptual lygradual statechange in thestomach's condition, whereassurgery provides a conceptual lyabrupt cure!
This suggests the needfor c lass i fy ing statechanges onsome discrete conceptual  scale.Another inadequacy of the presentmodel is its inabi l i ty to specifytime sequencing; adoption of sometradit ional  f lowchart concepts willprobably prove adequate for this.
(3) There is no convenient way to modeldecis ion-making processes on thepart of the planner of a CSA.
Thiswil l  have to be developed.IX.
APPLICATIONS OF THE CSAOn the brighter side, the CSA providesa unif ied basis for problem-so lv ing-re latedcognit ive models.
Specif ical ly,  I believeit shores up, under one basic datastructure, the ideas presented in my ownpast research in conceptual  memory andinference (RI,R2) and in conceptual  overlays(R3) which suggests a meaning contextmechanism for language comprehension basedaround CSA's.
I want to conclude by l ist ingant ic ipated appl icaions of CSA's.
Theappl icat ions have been divided into twocategories: general (those which are centralto some major theoret ical  issues in languageunderstanding and problem-solving),  andspecif ic (those which provide some localinsights into memory organization).General ApPl icat ionsI.
As the basis for active Rroblem-solv~ngThe CSA suppl ies an algor ithmic formatwherein plans can be conceived, synthesizedand executed.
One immediate goal ofIIIIIII!IiIiIresearch should be to construct acommonsense algorithm1 interpreter whichcould "execute" the contents of portions ofits own CSA memory in order to effectactions of moving about, communicating, andso forth.2.
As the basis for conceptual inferenceIn (RI), which describes a theory ofconceptual memory and inference, sixteenclasses of conceptual inference wereidenti f ied as the logical foundation of'alanguage-based meaning comprehension system.Interest ingly enough (but not surprising),nine of those inference classes corresponddirectly to traversals of CAS primitivelinks.
In the theory  of (RI), everylanguage stimulus, represented in conceptualform via Schank's conceptual dependencynotation (S2), was subjected to aspontaneous expansion in "inference space"along the sixteen dimensions correspondingto the sixteen inference classes.
Making aninference in that model corresponds toidenti fying each perception as a step in oneor more CSA's, then expanding outward fromthose points along the CSA linksbreadth-f irst.
Although there is certainlya class of more goal-directed conceptualinference, this kind of spontaneousexpansion seems necessary to generalcomprehension, and the CSA is a naturalformalism to use.
The nine classes ofinference which relate directly to CSA linksare:I. causative2.
resultat ive3.
motivat ional4.
enablement5.
function6.
enablement-predict ion7.
missing enablement8.
intervention9.
act ion-predict ion3.
As the basis for the conceptualrepresentat ion o_~f language.A very large percentage of what peoplecommunicate deals with algorithms, the howand why of their act ivit ies in the world.Schank's conceptual dependency frameworkdoes a good job at representing rathercomplex utterances which referenceunderlying actions, states and statechanges.This theory of CSA's extends this frameworkto accomodate larger chunks of experienceand language to begin dealing withparagraphs and stories instead of isolatedsentences.4.
As the basis for model in~ mechanismsEvery man-made mechanism, as well asevery natural ly-occurr ing biological system,is rich in algorithmic content.
Asi l lustrated in a previous example, CSA's cando a respectable job at character iz ingcomplex servo- and feedback mechanisms.
Itis not hard to envision the CSA as a basisfor physiological  models in such anappl icat ion as medical diagnosis.
Since allbiological  systems are purposivelyconstructed mechanisms in the evolut ionary189sense, representing such mechanisms in termsof causality, enablement, byproducts,thresholds, etc.
is quite meaningful.5.
As a basis for model ing dynamicmeaning context i__nn languagecomprehension and general perception(R3) describes an expectancy-basedsystem cal led "conceptual overlays" whichcan impose high-level, contextualinterpretat ions on sentences by consult ingits algor ithmic base.
In that paradigm,some stimuli (i.e.
meaning graphs result ingfrom a conceptual parser " which receiveslanguage utterances as input) act ivateaction overlays, while other stimuli fitinto previously act ivated action overlays.Since an overlay is a col lection of pointersto CSA's in the algor ithmic base which havebeen predicted as l ikely to occur next, to"fit into" is to identify subsequent inputas steps in the various algor ithms actorshave been predicted to engage.
For example,knowing what the sentence "John asked Maryfor the keys" means contextual ly is quite abit more simply understanding the "picture"this utterance el icits (its conceptualdependency representation).
If we know thatJohn was hungry:John hadn't eaten in days.John asked Mary for the car keys.we activate an overlay which expects thatJohn will engage CSA's which will a l leviatehis inferred hunger; needing car keys fitsnicely as a continuous enablement in severalof these algorithms.
Of course, the v i r tueof such a system is that it allows thehigh- level  interpretat ion of a sentence tochange as a function of its contextualenvironment:John had some hamburger stuckin his teeth.John asked Mary for the car keys.Change the expectancies, and theinterpretat ion changes!6.
As ~he basis forcomputer a l~or l thm synthesis~ndSince a computer algor ithm is arelat ively direct ref lect ion of aprogrammer's internal model of analgor i thmic process, it seems reasonablethat both the processes of synthesis andfinal implementat ion be represented in thesame terms as his internal model.
Thepresent theory only suggests an approach; itis not yet adequate for general computeralgorithms.
But it seems that the idea of aCSA might be very relevant to recentresearch in the area of automaticprogramming, at least as a basis ofrepresentation.7.
As ~ basis o__ff g self-modelIf a CSA interpreter  can indeed bedefined, and if indeed the CSA caneventual ly capture any computer algorithm,then creating a self -model amounts tospecifying the CSA interpreter in terms ofCSA's.
For example, an act of communicat ionamounts to the communicat ion of enoughreferential  information (features ofobjects, times, etc.)
to enable thecomprehender to identify, in his own model,the concepts being communicated.
Thehow-to-communicate algor i thm which the CSAinterpreter employs could itself be a CSA.8.
As a basis for invest igat ionof a lgor i thm learningIf we posit the existence of a smallset of primit ive CSA links and make theassumption that these are either part of thebrain's hardware, or learned impl ic i t ly  assoon as the intel lect begins perceiving, wehave a basis from which to study how a childlearns world dynamics.
For example, how,and at what point, does the toddler knowthat he must grasp the cup in an act ofcontinuous enablement before he can lift itto his mouth, and how does he know it mustbe at his mouth before he can successful lydrink?
Perhaps algorithmic knowledgedevelops from random exper imentat ion withinthe syntactic constraints imposed by the setof CSA primitive links.Specif ic CSA Appl icat ionsI.
For represent ing the functions of objectsAs with mechanisms, any man-made objectis made for a purpose.
Translated to CSA's,this means that part of everypurpos ive ly-constructed object's def init ionis a set of pointers into the algor i thm baseto CSA's in which the object occurs.
Thisis true for all objects from pencils, tofurnaces, to window shades, to a baublewhich provided its constructor amusement, tonewspapers.
An object in memory can becompletely character ized (in the abstract)by a set of intr insic features (shape, size,color, etc) and this set of pointers toCSA's.2.
For represent ing people's professionsTo say (ISA JOHNI PLUMBER) skirts whatit means to be a plumber.
Rather, to be aplumber means to engage plumbing algor i thmsas a principal source of income.
Thus, aprofession can be defined by a set ofpointers to the CSA's which arecharacter ist ic  of that profession.
Thismakes it possible to observe someone at workand identi fy his profession, to compareprofessions, etc.
; these would not bepossible if CSA's were not the basis ofrepresentat ion.3.
For detect ing and explaininganomalous situations and potential lyantagonist ic  statesA person notices a l icense plate yearlyst icker on upside down; a person notices twofire engines approaching an intersection,rushing to a fire; at the intersection, oneturns left, the other turns right; a personnotices that the rain that morning will190interfere with the picnic plans thatafternoon.
How do such situations getjudged "anomalous", and how does theperceiver try to explain or cope with them?The answer undoubtedly relates toexpectancies and a knowledge of a lgor i thmsfor putt ing things on one-another, gett ingsomewhere in a hurry and antagonist ic stateswhen eating outdoors.
By playing experienceagainst CSA's we discover things which wouldnot otherwise be discovered.4.
For f i l l ing in missing informationIf a person is perceiving in a noisy orincomplete environment, having CSA'savai lable to guide his interpretat ions ofperceptions provides enough momentum to fillin miss ing details, scarcely notic ing theirabsence.
If John is hammering a nail intothe wall with his hand on the backswing, butthe object in his hand is occluded, itrequires very l ittle effort to surmise thatit is a hammer.
If we believe that Mary isgoing to McDonald's  to buy a hamburger, butshe comes back into the house saying "Itwon't start", we have a pretty good idea"it" refers to the car.
This appl icat ion ofCSA's corresponds to the notion of aspeci f icat ion inference in (RI).X.
CONCLUSIONSInstead of a conclusion, I wil l  s implystate the order in which research along CSAlines should, and hopeful ly w i l l  at theUniversity of Maryland, progress:I. Reimplementat ion of the conceptualoverlays prototype system described in (R3)to ref lect the new CSA ideas and replace thead-hoc AND/OR graph approach descr ibed inthat report.2.
Implementat ion of a mechanism simulatorwhich could accept, in CSA terms, thedef init ion of a complex mechanism(electronic circuit or toilet), s imulate it,respond to art i f ic ia l ly - inducedmalfunct ions,  and answer questions about themechanism's cause and effect structure.3.
Engineer ing of a new total conceptualmemory, along the lines of the original  oneof (RI), but incorporatng CSA's and the newidea of a tendency.
This would involvere implement ing the inference mechanism andvarious searchers.4.
Development of a CSA interpreter whichcould not only use CSA's as data structuresin the various cognit ive processes, but alsocould execute them to drive itself.5.
Applying CSA's to medical  diagnosis andautomatic programming.6.
Invest igat ing the problem of storycomprehension via conceptual overlays andCSA's.
Perhaps also invest igat inggenerat ion of stories (e.g.
the story ofthe trip to McDonald's) or the generat ion ofa descr ipt ion of a complex electroniccircuit, encoded as a CSA, in layman'sterms.XI.
ACKNOWLEDGEMENTSMy thanks to the members of theCommonsense Algorithm Study Group at theUniversity of Maryland: Bob Eberlein, MiltGrinberg, Bob Kirby, Phil London and TomSkillman.
They have provided considerableintellectual stimulation.
We hope tocontinue as a group and eventually issue aworking paper and computer system.REFERENCES(AI) Abelson, R., "The Structure of BeliefSystems," in Schank and Colby (eds.
),Computer Models of Thought andLanguage, W.H.
Freeman, 1973(A2) Abelson, R., "Frames for UnderstandingSocial Actions," Paper for CarbonellConference, Pajarro Dunes CA, May 1974.
(CI) Charniak, E., "Toward a Model ofChildren's Story Comprehension,"Doctoral dissertation, M.I.T., AITR-266, 1972.
(MI) Minsky, M., "A Framework forRepresenting Knowledge," M.I.T.
AITR-306, 1974.
(NI) Nilsson, N., Probelm Solving Methods i__nnArtificial Intelligence, McGraw Hill,1971.
(RI) Rieger, C., "Conceptual Memory: ATheory and Computer Program forProcessing the Meaning Content ofNatural Language Utterances," Doctoraldissertation, Stanford Univ.
AI Memo233, 1974.
(R2) Rieger, C., "Understanding byConceptual Inference," American Journalof Computational Linguistics (inpress), 1975.
(Also available as Univ.of Maryland Technical Report #353)(R3) Rieger, C., "Conceptual Overlays: AMechanism for the Interpretation ofSentence Meaning in Context," to appearin Proceedings 4IJCAI 1975.
(Alsoavailable as Univ.
of MarylandTechnical Report #354)($I) Sacerdoti, E., "Planning in a Hierarchyof Abstraction Spaces," in Proceedings3IJCAI 1973.
($2) Schank, R., "Identifications ofConceptualizations Underlying NaturalLanguage", in Schank and Colby,Computer Models of Thought andLanguage, W.H.
Freeman, 1973.
($3) Schank, R., Goldman, N., Rieger, C.,and Riesbeck, C., "Primitive ConceptsUnderlying Verbs of Thought," StanfordUniv.
AI Memo 162, 1972.
($4) Schmidt, C., and D'Addamio, J., "AModel of the Common-Sense Theory ofIntention and Personal Causation,"Proceedings 3IJCAI, 1973.191Figure !Unrestricted AND/OR graph for getting aMcDonald's hamburger into stomach.e.,~\e.Figure 2Hamburger algorithm, with actions, states,causality and enablement explicit.6~6~r o~ ?uo#T~L~t.
~A~If, Q.: IvPPI.YLtN~- "r~ TA~V~tank from ~ / /trip _# float~j~ / ~ J  ~anolellf~ ~ armsupply II1 dis- _ ~ ~ _ ~ ,  channel valve Ill harge I I I~  pesupply |~ flushstewOer plpe~_ ValeV ~i t r ip~ f'~,refi I l- %\ ]   t~i/icTAN KFIGURE 3A reverse-trap toilet., i m,  ,ll!ftI "~ Over.fl ow' ~ ' - ' )  p, pe?
~flush ballK~Bk~6 c~c~ sutp6y Fuu~ ~F t~bV~#~.v~Lv~Bo~L.
1"o w~rs~n~G,'Z~W, T7~.
w ~,~J~wWr~ F~15owu 6J~TF.~ H?16.~o~rr~ L~P ~F wksl~FIGURE 4O~eration of the reverse-trap toilet.192IIIIII _ _IIIIIIIIOF I,~o0I (oe,,.,,~.
(Co~t~UouSJ r~  ,WooO'Figure 5_ _ .
+Sawing a board in half to decrease its length.193I T~,~,~'~.~ ~ I T;~- ~, .  '
k /~ ?
o - I - - - - - "  I , T~ ~ .
|IIICOMBUSTION, II|~~'"  ~!
!IFORGETFULNESS !FIGURE 6Vicious Cycles.
!I194 !ts ~j  /tc2,?
I~~F-.ONf.x.T P~t~E~T TA~LF_DJVII>( ,4c~.
J ~c Z.ll l l lFIGURE 7Computer algorithm to compute theaverage of TABLE(1),...,TABLE(N)expressed as a commonsense algorithm.
(NOTE: Initialization has not been shown.
The assumptions arethat AC3 begins with zero, that ACI begins with zero,and that N and TABLE(1),...,TABLE(N) exist in core.)
