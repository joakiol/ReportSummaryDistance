Exponential Priors for Maximum Entropy ModelsJoshua GoodmanOne Microsoft WayRedmond, WA 98052joshuago@microsoft.comAbstractMaximum entropy models are a common mod-eling technique, but prone to overfitting.
Weshow that using an exponential distribution asa prior leads to bounded absolute discountingby a constant.
We show that this prior is bettermotivated by the data than previous techniquessuch as a Gaussian prior, and often produceslower error rates.
Exponential priors also leadto a simpler learning algorithm and to easier tounderstand behavior.
Furthermore, exponentialpriors help explain the success of some previ-ous smoothing techniques, and suggest simplevariations that work better.1 IntroductionConditional Maximum Entropy (maxent) models havebeen widely used for a variety of tasks, including lan-guage modeling (Rosenfeld, 1994), part-of-speech tag-ging, prepositional phrase attachment, and parsing (Rat-naparkhi, 1998), word selection for machine translation(Berger et al, 1996), and finding sentence boundaries(Reynar and Ratnaparkhi, 1997).
They are also some-times called logistic regression models, maximum likeli-hood exponential models, log-linear models, and are evenequivalent to a form of perceptrons/single layer neuralnetworks.
In particular, perceptrons that use the standardsigmoid function, and optimize for log-loss are equivalentto maxent.
Multi-layer neural networks that optimize log-loss are closely related, and much of the discussion willapply to them implicitly.Conditional maxent models have traditionally eitherbeen unregularized or regularized by using a Gaussianprior on the parameters.
We will show that by using anexponential distribution as the prior, several advantagescan be gained.
We will show that in at least one case, anexponential prior experimentally better matches the ac-tual distribution of the parameters.
We will also show thatit can lead to improved accuracy, and a simpler learningalgorithm.
In addition, the exponential prior inspires animproved version of Good Turing discounting with lowerperplexity.Conditional maxent models are of the formP?
(y|x) =exp?Fi=1 ?ifi(x, y)?y?
exp?i ?ifi(x, y?
)where x is an input vector, y is an output, the f i are the so-called indicator functions or feature values that are trueif a particular property of x, y is true, F is the numberof such features, ?
represents the parameter set ?1...?n,and ?i is a weight for the indicator fi.
For instance,if trying to do word sense disambiguation for the word?bank?, x would be the context around an occurrence ofthe word; y would be a particular sense, e.g.
financial orriver; fi(x, y) could be 1 if the context includes the word?money?
and y is the financial sense; and ?i would be alarge positive number.Maxent models have several valuable properties(Della Pietra et al (1997) give a good overview.)
Themost important is constraint satisfaction.
For a given f i,we can count how many times fi was observed in thetraining data with value y, observed[i] =?j fi(xj , yj).For a model P?
with parameters ?, we can see how manytimes the model predicts that fi would be expected tooccur: expected[i] =?j,y P?
(y|xj)fi(xj , y).
Maxentmodels have the property that expected[i] = observed[i]for all i and y.
These equalities are called constraints.The next important property is that the likelihood of thetraining data is maximized (thus, the name maximumlikelihood exponential model.)
Third, the model is assimilar as possible to the uniform distribution (minimizesthe Kullback-Leibler divergence), given the constraints,which is why these models are called maximum entropymodels.This last property ?
similarity to the uniform distribu-tion ?
is a form of regularization.
However, it turns out tobe an extremely weak one ?
it is not uncommon for mod-els, especially those that use all or most possible features,to assign near-zero probabilities (or, if ?s may be infi-nite, even actual zero probabilities), and to exhibit othersymptoms of severe overfitting.
There have been a num-ber of approaches to this problem, which we will discussin more detail in Section 3.
The most relevant approach,however, is the work of Chen and Rosenfeld (2000), whoimplemented a Gaussian prior for maxent models.
Theycompared this technique to most of the previously im-plemented techniques, on a language modeling task, andconcluded that it was consistently the best.
We thus use itas a baseline for our comparisons, and similar considera-tions motivate our own technique, an exponential prior.Chen et al place a Gaussian prior with 0 mean and ?
2ivariance on the model parameters (the ?
is), and then finda model that maximizes the posterior probability of thedata and the model.
In particular, maxent models withoutpriors use the parameters ?
that maximizeargmax?n?j=1P?
(yj |xj)where xj , yj are training data instances.
With a Gaussianprior we findarg max?n?j=1P?
(yj|xj) ?F?i=11?2??2iexp(?
?2i2?2i)In this case, a trained model does not satisfy the con-straints expected[i] = observed[i], but, as Chen andRosenfeld show, instead satisfies constraintsexpected[i] = observed[i]?
?i?2i(1)That is, instead of a model that matches the observedcount, we get a model matching the observed count minus?i?2i: in language modeling terms, this is ?discounting.
?We do not believe that all models are generated bythe same process, and therefore we do not believe thata single prior will work best for all problem types.In particular, as we will describe in our experimen-tal results section, when looking at one particular setof parameters, we noticed that it was not at all Gaus-sian, but much more similar to a 0 mean Laplacian,f(?i) =12?iexp(?|?i|?i), or to an exponential distri-bution f(?i) = ?i exp (?
?i?i), which is non-zero onlyfor non-negative ?i.
In some cases, learned parameterdistributions will not match the prior distribution, but insome cases they will, so it seemed worth exploring oneof these alternate forms.
Later, when we try to optimizeour models, the parameter seach will turn out to be muchsimpler with an exponential prior, so we focus on thatdistribution.With an exponential prior, we maximizearg max??0n?j=1P?
(yj |xj) ?F?i=1?i exp (?
?i?i) (2)As we will describe, it is significantly simpler to performthis maximization than the Gaussian maximization.
Fur-thermore, as we will describe, models satisfying Equa-tion 2 will have the property that, for each ?
i, eithera) ?i = 0 and expected[i] ?
observed[i] ?
?i or b)expected[i] = observed[i]??i.
In other words, we essen-tially just discount the observed counts by the constant?i (which is the reciprocal of the standard deviation),subject to the constraint that ?i is non-negative.
This ismuch simpler and more intuitive than the constraints withthe Gaussian prior (Equation 1), since those constraintschange as the values of ?i change.
Furthermore, as wewill describe in Section 3, discounting by a constant is acommon technique for language model smoothing (Neyet al, 1994; Chen and Goodman, 1999), but one that hasnot previously been well justified; the exponential priorgives some Bayesian justification.In Section 5 we will show that on two very differenttasks ?
grammar checking and a collaborative filteringtask ?
the exponential prior yields lower error rates thanthe Gaussian.2 Learning algorithms and discountingIn this section we derive a learning algorithm for expo-nential priors, with provable convergence properties, andshow that it leads to a simple discounting formula.
Notethat the simple learning algorithm is an important con-tribution: the algorithm for a Gaussian prior is quite abit more complicated, and previous related work with theLaplacian prior (two-sided exponential) has had a diffi-cult time finding learning algorithms; because the Lapla-cian does not have a continuous first derivative, and be-cause the exponential prior is bounded at 0, standard gra-dient descent type algorithms may exhibit poor behav-ior.
Williams (1995) devotes a full ten pages to describ-ing a somewhat heuristic approach for solving this prob-lem, and even this discussion concludes ?In summary itis left to the reader to supply the algorithms for deter-mining successive search directions and the initially pre-ferred value of?
the step size (page 130).1 We show thata very simple variation on a standard algorithm, General-ized Iterative Scaling (GIS) (Darroch and Ratcliff, 1972),solves this problem.
In particular, as we will show, whileGIS uses an update rule of the form?i := ?i +1f#logobserved[i]expected[i]1Williams?
algorithm is for the much more complex case ofa multi-layer network, while ours is for the one layer case, butthere are no obvious simplifications to his approach for the onelayer case.our modified algorithm uses a rule of the form?i := max(0,(?i +1f#logobserved[i]?
?iexpected[i]))(3)Note that there are two different styles of model thatone can use, especially in the common case that there aretwo outputs (values for y.)
Consider a word sense disam-biguation problem, trying to determine whether a wordlike ?bank?
means the river or financial sense, with ques-tions like whether or not the word ?water?
occurs nearby.One could have a single indicator function f1(x, y) =1 if water occurs nearby and values in the range ??
<?1 < ?.
We call this style ?double sided?
indicators.Alternatively, one could have two indicator functions,f1(x, y) = 1 if water occurs nearby and y=river andf2(x, y) = 1 if water occurs nearby and y=financial.
Inthis case, one could allow either ??
< ?1, ?2 < ?
or0 ?
?1, ?2 < ?
.
We call the style with two indicators?single sided.?
With a Gaussian prior, it does not mat-ter which style one uses ?
one can show that by chang-ing ?2, exactly the same results will be achieved.
With aLaplacian (double sided exponential), one could also useeither style.
With an exponential prior, only positive val-ues are allowed, so one must use the double sided style,so that one can learn that some indicators push towardsone sense, and some push towards the other ?
that is,rather than having one weight which is positive or neg-ative, we have two weights which are positive or zero,one of which pushes towards one answer, and the otherpushing towards the other.We derive our constraints and learning algorithm byvery closely following the derivation of the algorithm byChen and Rosenfeld.
It will be convenient to maximizethe log of the expression in 2 rather than the expressionitself.
This leads to an objective function:L(?
)=?jlog P?
(yj |xj) ?F?i=1?i?i + log ?i=?jF?i=1?ifi(xj , yj) (4)?
?jlog?yexp(F?i=1?ifi(xj , y))?F?i=1?i?i+log?iNote that this objective function is convex (since it is thesum of two convex functions.)
Thus, there is a globalmaximum value for this objective function.Now, we wish to find the maximum.
Normally, wewould do this by setting the derivative to 0, but the boundof ?k ?
0 changes things a bit.
The maximum can thenoccur at the discontinuity in the derivative (?k = 0) orwhen ?k > 0.
We can explicitly check the value of theobjective function at the point ?k = 0.
When there is amaximum with ?k > 0 we know that the partial deriva-tive with respect to ?k will be 0.??
?k?j?Fi=1 ?ifi(xj , yj)?
?j log?y exp(?Fi=1 ?ifi(xj , y))?
?Fi=1 ?i?i + const(?
)=?j fk(xj , yj)?
?j?yfk(xj,y) exp(?Fi=1?ifi(xj,y))?yexp(?Fi=1?ifi(xj,y))?
?k=?j fk(xj , yj)?
?j?y fk(xj , y)P?(y|xj)?
?kThis implies that at the optimum, when ?k > 0,?jfk(xj , yj) ?
?j?yfk(xj , y)P?(y|xj)?
?k = 0observed[k]?
expected[k] ?
?k = 0observed[k] ?
?k = expected[k] (5)In other words, we discount the observed count by ?
k ?the absolute discounting equation.
Sometimes it is betterfor ?k to be set to 0 ?
another possible optimal point iswhen ?k = 0 and observed[k] ?
?k < expected[k].
Oneof these two cases must hold at the optimum.Notice an important property of exponential priors(analogous to a similiar property for Laplace priors(Williams, 1995; Tibshirani, 1994)): they often favor pa-rameters that are exactly 0.
This leads to a kind of natu-ral pruning for exponential priors, not found in Gaussianpriors, which are only very rarely 0.
(Note, however, thatone should not increase the ?
?s in the exponential priorto control pruning, as this may lead to oversmoothing.
Ifadditional pruning is needed for speed or memory sav-ings, feature selection techniques should be used, suchas pruning small or infrequent parameters, instead of astrengthened prior.
)Now, we can derive the update equations.
The deriva-tion is exactly the same as Chen and Rosenfeld?s (2000)with the minor change of an exponential prior instead ofa Gaussian prior (we include it in the appendix.)
Letf#(x, y) =?i fi(x, y).
Then, in the end, we get anupdate equation of the form?k := max(0, ?k +1f#logobserved[k]?
?kexpected[k])Compare this equation to the corresponding equationwith a Gaussian prior (Chen and Rosenfeld., 2000).
Witha Gaussian prior, one can derive an equation of the formobserved[k]?
?k?2k= expected[k] exp(f#?k)and then solve for ?k.
There is no closed form solution:it must be solved using numerical methods, such as New-ton?s method, making this update much more complexand time consuming than the exponential prior.One can also derive variations on this update.
For in-stance, in the Appendix, we derive an update for Im-proved Iterative Scaling (Della Pietra et al, 1997) withan exponential prior.
Perhaps more importantly, one canalso derive updates for Sequential Conditional General-ized Iterative Scaling (SCGIS) (Goodman, 2002), whichis several times faster than GIS.
The SCGIS update forbinary features with an exponential prior is simply?k := max(0, ?k + logobserved[k]?
?kexpected[k])One might wonder why we simply don?t use conju-gate gradient (CG) methods, which have been shown toconverge quickly for maxent.
There has not been a for-mal comparison of SCGIS to conjugate gradient methods.In our own pilot studies, SCGIS is sometimes faster andsometimes slower.
Also, some versions of CG use heuris-tics for the step size, and lose convergence guarantees.Finally, SCGIS is simpler to implement than conjugategradient, and even for those with a conjugate gradient li-brary, because of the parameter constraints (for an expo-nential prior) or discontinuous derivatives (for a Lapla-cian) standard conjugate gradient techniques need to beat least somewhat modified.Good-Turing discounting has been used or suggestedfor language modeling several times (Rosenfeld, 1994,page 38), (Lau, 1994).
In particular, it has been suggestedto use an update of the form?k := ?k +1f#logobserved[k]?expected[k]where observed[k]?
is the Good-Turing discounted valueof observed[k].
This update has a problem, as noted by itsproponents: the constraints are probably now inconsistent?
there is no model that can simultaneously satisfy them.We note that a simple variation on this update, inspiredby the exponential prior, does not have these problems:?k := max 0,(?k +1f#logobserved[k]?expected[k])In particular, this can be thought of as picking an?observed[k] for each observed[k].
This does not con-stitute a Bayesian prior, since the value is picked after thecounts are observed, but it does lead to a convex objectivefunction with a global maximum, and the update functionwill converge towards this maximum.
Variations on theconstraints of Equation 5 will apply for this modified ob-jective function.
Furthermore, in the experimental resultssection, we will see that on a language modeling task,this modified update function outperforms the traditionalupdate.
By using a well motivated approach inspired byexponential priors, we can find a simple variation that hasbetter performance both theoretically and empirically.3 Previous WorkThere has been a fair amount of previous work on regu-larization of maxent models.
Early approaches focusedon feature selection (Della Pietra et al, 1997) or, simi-larly, count cutoffs (Rosenfeld, 1994).
By not using allfeatures, there is typically extra probability left-over forunobserved events, which is distributed in a maximumentropy fashion.
The problem with this approach is thatit ignores useful information ?
although low count or lowdiscrimination features may cause overfitting, they docontain valuable information.
Because of this, more re-cent approaches (Rosenfeld, 1994, page 38), (Lau, 1994)have tried techniques such as Good-Turing discounts(Good, 1953).There are a number of other approaches (Khudanpur,1995; Newman, 1997) based on the fuzzy maxent frame-work (Della Pietra and Della Pietra, 1993).
Chen andRosenfeld (Chen and Rosenfeld., 2000) give a more com-plete discussion of those approaches.Chen and Rosenfeld (2000), following a suggestion ofLafferty, implemented a Gaussian prior for maxent mod-els.
They compared this technique to most of the previ-ously discussed techniques, on a language modeling task,and concluded that it was consistently the best technique.Tibshirani (1994) introduced Laplacian priors for lin-ear models (linear regressions) and showed that an ob-jective function that minimizes the absolute values of theparameters corresponds to a Laplacian prior.
He calledthis the Least Absolute Shrinkage and Selection Operator(LASSO) and showed that it leads to a type of feature se-lection.
Exponential priors are sometimes called single-sided Laplacians, so obviously, the two techniques arevery closely related, so closely that we would not want toclaim that either was better than the other.Williams (1995) introduced a Laplacian prior for neu-ral networks.
Single layer neural networks with cer-tain loss functions are equivalent to logistic regres-sion/maximum entropy models.
Williams?
algorithm wasfor a more general case, and much more complex than theone we describe.More recently, Figueiredo et al (2003) in unpublishedindependent work also examined Laplacian priors for lo-gistic regression, deriving a somewhat more complex al-gorithm than ours, but one that they extended to partiallysupervised learning.
He has shown that the results arecomparable to transductive SVMs.Perkins and Theiler (2003) used logisitic regressionwith a Laplacian prior as well.
Their learning algorithmwas conjugate gradient descent.
Normally, conjugate gra-dient methods are only used on data that has a continuousfirst derivative, so the code was modified to prune weightsthat go exactly to zero.Our main contribution then is not the use of Laplacianpriors with logistic regression, nor even the first goodlearning algorithm for the model type.
Our contributionsare performing an explicit comparison to a Gaussian priorand showing improved performance on real data; noticingthat the fixed point of the models leads to absolute dis-counting; showing that iterative-scaling style algorithmsincluding GIS, IIS, and SCGIS can be trivially modifiedto use this prior; and explicitly showing that in at least onecase, parameters are actually consistent with this prior.4 Kneser-Ney smoothingIn this section, we help explain the excellent performanceof Kneser-Ney smoothing, the best performing languagemodel smoothing technique.
This justification not onlyanswers an important question ?
why do discountingmethods work well ?
but also gives guidance for extend-ing Kneser-Ney smoothing to problems with fractionalcounts, where solutions were not previously known.Chen and Goodman (1999) performed an extensivecomparison of different smoothing (regularization) tech-niques for language modeling.
They found that a ver-sion of Kneser-Ney smoothing (Kneser and Ney, 1995)consistently was the best performing technique.
Unfortu-nately, while there are partial theoretical justifications forKneser-Ney smoothing, in terms of preserving marginals,one important part has previously had no justification:Kneser-Ney smoothing discounts counts, while most con-ventional regularization techniques, justified by Dirichletpriors, add to counts.
Given that discounting was pre-viously unjustified, it is exciting that we have found away to explain it.
In fact, we can show that the Back-off version of Kneser-Ney smoothing is an excellent ap-proximation to a maximum entropy model with an ex-ponential prior.
In particular, Kneser-Ney smoothing isderived by assuming absolute discounting, and then find-ing the distribution that preserves marginals, i.e.
mak-ing expected = observed - discount.
The differences be-tween Backoff Kneser-Ney smoothing and maxent mod-els with exponential priors are twofold.
First, the backoffversion does not exactly preserve marginals ?
an approx-imation is made.
Second, Backoff Kneser-Ney alwaysperforms discounting, even when this effectively resultsin lowering the probability, e.g.
the equivalent of a neg-ative value for ?.
There is also an interpolated versionof Kneser-Ney smoothing.
The interpolated version ofKneser Ney smoothing works even better.
It exactly pre-serves marginals (except for discounting.)
Also, the sec-ondary distribution is combined with the primary distri-bution; this has several effects, including that it is un-likely to get the equivalent of a large negative ?
value.5 Experimental ResultsIn this section, we detail our experimental results, show-ing that exponential priors outperform Gaussian priors ontwo different data sets, and inspire improvements for athird.
For all experiments, except the language model ex-periments, we used a single variance for both the Gaus-sian and the exponential prior, rather than one per param-eter, with the variance optimized on held out data.
For thelanguage modeling experiments, we used three variances,one each for the unigram, bigram, and trigram models,again optimized on held out data.We were inspired to use an exponential prior by anactual examination of a data set.
In particular, we usedthe grammar-checking data of Banko and Brill (2001).We chose this set because there are commonly used ver-sions both with small amounts of data (which is whenwe expect the prior to matter) and with large amounts ofdata (which is required to easily see what the distribu-tion over ?correct?
parameter values is.)
For one exper-iment, we trained a model using a Gaussian prior, usinga large amount of data.
We then found those parameters(?
?s) that had at least 35 training instances ?
enough totypically overcome the prior and train the parameter re-liably.
We then graphed the distribution of these param-eters.
While it is common to look at the distribution ofdata, the NLP and machine learning communities rarelyexamine distributions of model parameters, and yet thisseems like a good way to get inspiration for priors to try,using those parameters with enough data to help guessthe priors for those with less, or at least to determine thecorrect form for the prior, if not the exact values.
2 Theresults are shown in Figure 1, which is a histogram of ?
?swith a given value.
If the distribution were Gaussian, wewould expect this to look like an upside-down parabola.If the distribution were Laplacian, we would expect it toappear as a triangle (the bottom formed from the X-axis.
)Indeed, it does appear to be roughly triangular, and to theextent that it diverges from this shape, it is convex, whilea Gaussian would be concave.
We don?t believe that theexponential prior is right for every problem ?
our argu-ment here is that based on both better accuracy (our nextexperiment) and a better fit to at least some of the param-eters, that the exponential prior is better for some models.We then tried actually using exponential priors withthis application, and were able to demonstrate improve-2Of course, those parameters with lots of data might be gen-erated from a different prior than those with little data.
Thistechnique is meant as a form of inspiration and evidence, butnot of proof.
Similarly, all parameters may be generated bysome other process, e.g.
a mixture of Gaussians.
Finally, a priormight be accurate but still behave poorly, because it might in-teract poorly with other approximations.
For instance, it mightinteract poorly with the fact that we use argmax rather than thetrue Bayesian posterior over models.110100100010000-3 -2 -1  0  1  2  3Number of ParametersinBucketValue of LambdaFigure 1: Histogram of ?
valuesments in error rate.
We used a small data set, 100,000sentences of training data and ten different confusableword pairs.
(Most training sentences did not contain ex-amples of the confusable word pairs of interest, so theactual number of training examples for each word-pairwas less than 100,000).
We tried different priors forthe Gaussian and exponential prior, and found the bestsingle prior on average across all ten pairs.
With thisbest setting, we achieved a 14.51% geometric averageerror rate with the exponential prior, and 15.45% withthe Gaussian.
To avoid any form of cheating, we thentried 10 different word pairs (the same as those used byBanko and Brill (2001)) with this best parameter setting.The results were 18.07% and 19.47% for the exponen-tial and Gaussian priors respectively.
(The overall higherrate is due to the test set words being slightly more dif-ficult.)
We also tried experiments with 1 million and 10million words, but there were not consistent differences;improved smoothing mostly matters with small amountsof training data.We also tried experiments with a collaborative-filteringstyle task, television show recommendation, based onNielsen data.
The dataset used, and the definition of a col-laborative filtering (CF) score is the same as was used byKadie et al (2002), although our random train/test splitis not the same, so the results are not strictly comparable.We first ran experiments with different priors on a held-out section of the training data, and then using the singlebest value for the prior (the same one across all features),we ran on the test data.
With a Gaussian prior, the CFscore was 42.11, while with an exponential prior, it was45.86, a large improvement.Finally, we ran experiments with language modeling,with mixed success.
We used 1,000,000 words of train-ing data (a small model, but one where smoothing mat-ters) from the WSJ corpus and a trigram model with acluster-based speedup (Goodman, 2001).
We evaluatedon test data using the standard language modeling mea-sure, perplexity, where lower scores are better.
We triedfive experiments: using Katz smoothing (a widely usedversion of Good-Turing smoothing) (perplexity 238.0);using Good Turing discounting to smooth maxent (per-plexity 224.8); using our variation on Good-Turing, in-spired by exponential priors, where ?
?s are bounded at 0(perplexity 204.5); using an exponential prior (perplex-ity 190.8); using a Gaussian prior (perplexity 183.7); andusing interpolated modified Kneser-Ney smoothing (per-plexity 180.2).
On the one hand, an exponential prior isworse than a Gaussian prior in this case, and modified in-terpolated Kneser-Ney smoothing is still the best knownsmoothing technique (Chen and Goodman, 1999), withinnoise of a Gaussian prior.
On the other hand, searchingfor parameters is extremely time consuming, and Good-Turing is one of the few parameter-free smoothing meth-ods.
Of the three Good-Turing smoothing methods, theone inspired by exponentials priors was the best.Note that perplexity is 2entropy and in general, wehave found that exponential priors work slightly worseon entropy measures than the Gaussian prior, even whenthey are better on accuracy.
This may be due to the factthat an exponential prior ?throws away?
some informa-tion, whenever the ?
would be negative.
(In a pilot exper-iment with a variation that does not throw away informa-tion, the entropies are closer to the Gaussian.
)6 ConclusionWe have shown that an exponential prior for maxent mod-els leads to a simple update formula that is easy to im-plement, and to models that are easy to understand: ob-servations are discounted, subject to the constraint that?
?
0.
We have also shown that in at least one case,this prior better matches the underlying model, and thatfor two applications, it leads to improved accuracy.
Theprior also inspired an improved version of Good-Turingsmoothing with lower perplexity.
Finally, an exponentialprior explains why models that discount by a constantcan be Bayesian, giving an alternative to Dirichlet pri-ors which add a constant.
This helps justify Kneser-Neysmoothing, the best performing smoothing technique inlanguage modeling.
In the future, we would like to useour technique of examining the distribution of model pa-rameters to see if other problems exhibit other priors be-sides Gaussian and Laplacian/exponential, and if perfor-mance on those problems can be improved through thisobservation.AcknowledgmentsThanks to John Platt, who suggested looking at Lapla-cian priors, and to Chris Meek for helpful discussions,and Jeff Bilmes for reading an earlier version of this pa-per.
Finally, thanks to Stan Chen and Roni Rosenfeld:our derivation for exponential priors closely follows thetext of their derivation for Gaussian priors.ReferencesM.
Banko and E. Brill.
2001.
Mitigating the paucity ofdata problem.
In HLT.Adam L. Berger, Stephen A. Della Pietra, and Vincent J.Della Pietra.
1996.
A maximum entropy approach tonatural language processing.
Computational Linguis-tics, 22(1):39?71.Stanley F. Chen and Joshua Goodman.
1999.
An empir-ical study of smoothing techniques for language mod-eling.
Computer Speech and Language, 13:359?394,October.Stanley Chen and Ronald Rosenfeld.
2000.
A survey ofsmoothing techniques for ME models.
IEEE Trans.
onSpeech and Audio Processing, 8(2):37?50, January.J.N.
Darroch and D. Ratcliff.
1972.
Generalized iterativescaling for log-linear models.
The Annals of Mathe-matical Statistics, 43:1470?1480.Stephen Della Pietra and Vincent Della Pietra.
1993.
Sta-tistical modeling by maximum entropy.
UnpublishedManuscript.Stephen Della Pietra, Vincent Della Pietra, and John Laf-ferty.
1997.
Inducing features of random fields.
IEEETransactions on Pattern Analysis and Machine Intelli-gence, 19(4):380?393, April.Mario A. T. Figueiredo, Balaji Krishnapuram, LawrenceCarin, and Alexander J. Hartemink.
2003.
Supervisedand semi-supervised sparse Bayesian classification.I.J.
Good.
1953.
The population frequencies ofspecies and the estimation of population parameters.Biometrika, 40(3 and 4):237?264.Joshua Goodman.
2001.
Classes for fast maximum en-tropy training.
In ICASSP 2001.Joshua Goodman.
2002.
Sequential conditional general-ized iterative scaling.
In ACL ?02.Carl M. Kadie, Christopher Meek, and David Hecker-man.
2002.
CFW: A collaborative filtering systemusing posteriors over weights of evidence.
In Proceed-ings of UAI, pages 242?250.S.
Khudanpur.
1995.
A method of maximum entropy es-timation with relaxed constraints.
In 1995 Johns Hop-kins University Language Modeling Workshop.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language modeling.
InICASSP, volume 1, pages 181?184.Raymond Lau.
1994.
Adaptive statistical language mod-elling.
Master?s thesis, MIT.W.
Newman.
1997.
Extension to the maximum en-tropy method.
IEEE Trans.
on Information Theory,IT-23(1):89?93, January.Hermann Ney, Ute Essen, and Reinhard Kneser.
1994.On structuring probabilistic dependences in stochasticlanguage modeling.
Computer, Speech, and Language,8:1?38.Simon Perkins and James Theiler.
2003.
Online featureselection using grafting.
August.Adwait Ratnaparkhi.
1998.
Maximum Entropy Modelsfor Natural Language Ambiguity Resolution.
Ph.D.thesis, University of Pennsylvania.J.
Reynar and A. Ratnaparkhi.
1997.
A maximum en-tropy approach to identifying sentence boundaries.
InANLP.Ronald Rosenfeld.
1994.
Adaptive Statistical LanguageModeling: A Maximum Entropy Approach.
Ph.D. the-sis, Carnegie Mellon University, April.Robert Tibshirani.
1994.
Regression shrinkage and se-lection via the lasso.
Technical report.Peter M. Williams.
1995.
Bayesian regularization andpruning using a Laplace prior.
Neural Computation,7:117?143.A Derivation of Update EquationIn each iteration, we try to find ?
= {?i} that maximizesthe increase in the objective function (subject to the con-straint that ?i + ?i ?
0).
From Equation 4,L(?
+ ?)
?
L(?)
=?j?i?ifi(xj , yj)??jlog?yP?
(y|xj) exp(?i?ifi(xj , y))?
?i?i?iAs with the Gaussian prior, it is not clear how to maxi-mize this function directly, so instead we use an auxiliaryfunction, B(?
), with three important properties: first, wecan maximize it; second, it bounds this function from be-low; third, it is larger than zero whenever ?
is not at a lo-cal optimum, i.e.
does not satisfy the constraints in Equa-tion 5.
Using the well-known inequality log x ?
x ?
1,which implies ?logx ?
1 ?
x, we getLX(?
+ ?)
?
LX(?)
?
?j?i?ifi(xj , yj)+?j1??yP?
(y|xj) exp(?i?ifi(xj , y))?
?i?i?i (6)Let f#(x, y) =?i fi(x, y).
Modify Equation 6 to:LX(?
+ ?)
?
LX(?)
?
?j?i?ifi(xj , yj)+?j1 ??yP?
(y|xj) exp(f#(xj , y)?i?ifi(xj , y)f#(xj , y))?
?i?i?i (7)Now, recall Jensen?s inequality, which states that for aconvex function g,?yp(x)g(x) ?
g(?xp(x)x)Notice that fi(x,y)f#(x,y)is a probability distribution.
Thus, wegetLX(?
+ ?)
?
LX(?)
?
?j?i?ifi(xj , yj)+?j1??yP?
(y|xj)?ifi(xj , y)f#(xj , y)exp(f#(xj , y)?i)?
?i?i?i (8)Now, we would like to find ?
that maximizes Equation8.
Thus, we take partial derivatives and set them to zero,remembering to also check whether a maximum occurswhen ?k = 0.???k??
?j?i?ifi(xj , yj) +?j1??yP?
(y|xj)?ifi(xj , y)f#(xj , y)exp(f#(xj , y)?i)?
?i?i?i)=?jfk(xj , yj)+?j??yP?
(y|xj)fk(xj , y)f#(xj , y)??
?kexp(f#(xj , y)?k)?
?k=?jfk(xj , yj)??j?yP?
(y|xj)fk(xj , y) exp(f#(xj , y)?k)?
?k= 0This gives us a version of Improved Iterative Scaling withan exponential Prior.
In general, however, we prefer vari-ations of Generalized Iterative Scaling, which may notconverge as quickly, but lead to simpler algorithms.
Inparticular, we set f# = maxx,y f#(x, y).
Then, insteadof Equation 7, we getLX(?
+ ?)?
LX(?)?
?j?i?ifi(xj , yj) +?j1 ??yP?
(y|xj) exp(f#?i?ifi(xj , y)f#)?
?i?i?i (9)We can follow essentially the same derivation from there.
(Technically, we need to add in a slack parameter; theslack parameter can then be given a near-zero varianceprior so that its value stays at 0, and thus in practice it canbe ignored.)
We thus get:???k??
?j?i?ifi(xj , yj)+?j1??yP?
(y|xj)?ifi(xj , y)f#exp(f#?i ??i?i?i)?
?=?jfk(xj , yj)??j?yP?
(y|xj)fk(xj , y) exp(f#?k)?
?k= observed[k]?
expected[k] exp(f#?k)?
?k= 0 (10)From Equation 10 we get?k =1f#logobserved[k]?
?kexpected[k]Now, ?k + ?k may be less than 0; in this case, an illegalnew value for ?k would result.
We know, however, fromthe monotonicity of all the equations with respect to ?kthat the lowest legal value of ?k will be the best, and wethus arrive at?k = max(?
?k,1f#logobserved[k] ?
?kexpected[k])or equivalently?k := max(0, ?k +1f#logobserved[k]?
?kexpected[k])
