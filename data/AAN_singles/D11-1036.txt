Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 385?396,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsEvaluating Dependency Parsing:Robust and Heuristics-Free Cross-Annotation EvaluationReut TsarfatyUppsala UniversitySwedenJoakim NivreUppsala UniversitySwedenEvelina AnderssonUppsala UniversitySwedenAbstractMethods for evaluating dependency parsingusing attachment scores are highly sensitiveto representational variation between depen-dency treebanks, making cross-experimentalevaluation opaque.
This paper develops a ro-bust procedure for cross-experimental eval-uation, based on deterministic unification-based operations for harmonizing differentrepresentations and a refined notion of treeedit distance for evaluating parse hypothe-ses relative to multiple gold standards.
Wedemonstrate that, for different conversions ofthe Penn Treebank into dependencies, perfor-mance trends that are observed for parsingresults in isolation change or dissolve com-pletely when parse hypotheses are normalizedand brought into the same common ground.1 IntroductionData-driven dependency parsing has seen a consid-erable surge of interest in recent years.
Dependencyparsers have been tested on parsing sentences in En-glish (Yamada and Matsumoto, 2003; Nivre andScholz, 2004; McDonald et al, 2005) as well asmany other languages (Nivre et al, 2007a).
Theevaluation metric traditionally associated with de-pendency parsing is based on scoring labeled orunlabeled attachment decisions, whereby each cor-rectly identified pair of head-dependent words iscounted towards the success of the parser (Buchholzand Marsi, 2006).
As it turns out, however, suchevaluation procedures are sensitive to the annotationchoices in the data on which the parser was trained.Different annotation schemes often make differ-ent assumptions with respect to how linguistic con-tent is represented in a treebank (Rambow, 2010).The consequence of such annotation discrepancies isthat when we compare parsing results across differ-ent experiments, even ones that use the same parserand the same set of sentences, the gap between re-sults in different experiments may not reflect a truegap in performance, but rather a difference in the an-notation decisions made in the respective treebanks.Different methods have been proposed for makingdependency parsing results comparable across ex-periments.
These methods include picking a singlegold standard for all experiments to which the parseroutput should be converted (Carroll et al, 1998; Ceret al, 2010), evaluating parsers by comparing theirperformance in an embedding task (Miyao et al,2008; Buyko and Hahn, 2010), or neutralizing thearc direction in the native representation of depen-dency trees (Schwartz et al, 2011).Each of these methods has its own drawbacks.Picking a single gold standard skews the results infavor of parsers which were trained on it.
Trans-forming dependency trees to a set of pre-defined la-beled dependencies, or into task-based features, re-quires the use of heuristic rules that run the risk ofdistorting correct information and introducing noiseof their own.
Neutralizing the direction of arcs islimited to unlabeled evaluation and local context,and thus may not cover all possible discrepancies.This paper proposes a new three-step protocol forcross-experiment parser evaluation, and in particu-lar for comparing parsing results across data setsthat adhere to different annotation schemes.
In the385first step all structures are brought into a single for-mal space of events that neutralizes representationpeculiarities (for instance, arc directionality).
Thesecond step formally computes, for each sentencein the data, the common denominator of the differ-ent gold standards, containing all and only linguisticcontent that is shared between the different schemes.The last step computes the normalized distance fromthis common denominator to parse hypotheses, mi-nus the cost of distances that reflect mere annotationidiosyncrasies.
The procedure that implements thisprotocol is fully deterministic and heuristics-free.We use the proposed procedure to compare de-pendency parsing results trained on Penn Treebanktrees converted into dependency trees according tofive different sets of linguistic assumptions.
Weshow that when starting off with the same set ofsentences and the same parser, training on differ-ent conversion schemes yields apparently significantperformance gaps.
When results across schemes arenormalized and compared against the shared linguis-tic content, these performance gaps decrease or dis-solve completely.
This effect is robust across parsingalgorithms.
We conclude that it is imperative thatcross-experiment parse evaluation be a well thought-through endeavor, and suggest ways to extend theprotocol to additional evaluation scenarios.2 The Challenge: Treebank TheoriesDependency treebanks contain information aboutthe grammatically meaningful elements in the utter-ance and the grammatical relations between them.Even if the formal representation in a dependencytreebank is well-defined according to current stan-dards (Ku?bler et al, 2009), there are different waysin which the trees can be used to express syntacticcontent (Rambow, 2010).
Consider, for instance, al-gorithms for converting the phrase-structure trees inthe Penn Treebank (Marcus et al, 1993) into depen-dency structures.
Different conversion algorithmsimplicitly make different assumptions about how torepresent linguistic content in the data.
When mul-tiple conversion algorithms are applied to the samedata, we end up with different dependency trees forthe same sentences (Johansson and Nugues, 2007;Choi and Palmer, 2010; de Marneffe et al, 2006).Some common cases of discrepancies are as follows.Lexical vs. Functional Head Choice.
In linguis-tics, there is a distinction between lexical heads andfunctional heads.
A lexical head carries the seman-tic gist of a phrase while a functional one marks itsrelation to other parts of the sentence.
The two kindsof heads may or may not coincide in a single wordform (Zwicky, 1993).
Common examples refer toprepositional phrases, such as the phrase ?on Sun-day?.
This phrase has two possible analyses, one se-lects a lexical head (1a) and the other selects a func-tional one (1b), as depicted below.
(1a) Sundayon(1b) onSundaySimilar choices are found in phrases which containfunctional elements such as determiners, coordina-tion markers, subordinating elements, and so on.Multi-Headed Constructions.
Some phrases areconsidered to have multiple lexical heads, for in-stance, coordinated structures.
Since dependency-based formalisms require us to represent all con-tent as binary relations, there are different ways wecould represent such constructions.
Let us considerthe coordination of nominals below.
We can choosebetween a functional head (1a) and a lexical head(2b, 2c).
We can further choose between a flat rep-resentation in which the first conjunct is a singlehead (2b), or a nested structure where each con-junct/marker is the head of the following element(2c).
All three alternatives empirically exist.
Exam-ple (2a) reflects the structures in the CoNLL 2007shared task data (Nivre et al, 2007a).
Johanssonand Nugues (2007) use structures like (2b).
Exam-ple (2c) reflects the analysis of Mel?c?uk (1988).
(2a) andearth wind fire(2b) earthwind and fire(2c) earthwindandfirePeriphrastic Marking.
When a phrase includesperiphrastic marking ?
such as the tense and modalmarking in the phrase ?would have worked?
below?
there are different ways to consider its divisioninto phrases.
One way to analyze this phrase wouldbe to choose auxiliaries as heads, as in (3a).
Anotheralternative would be to choose the final verb as theprep pobjconj conjconj cccoordconjcoordconjconj386Experiment Gold Parse#1 arriveonSundayarriveonSunday#2 arriveSundayonarriveSundayonGold: #1 # 2Parse#1 1.0 0.0#2 0.0 1.0Figure 1: Calculating cross-experiment LAS resultsmain head, and let the auxiliaries create a verb chainwith different levels of projection.
Each annotationdecision dictates a different direction of the arcs andimposes its own internal division into phrases.
(3a) wouldhaveworked(3b) workedhavewouldIn standard settings, an experiment that usesa data set which adheres to a certain annotationscheme reports results that are compared against theannotation standard that the parser was trained on.But if parsers were trained on different annotationstandards, the empirical results are not comparableacross experiments.
Consider, for instance, the ex-ample in Figure 1.
If parse1 and parse2 are com-pared against gold2 using labeled attachment scores(LAS), then parse1 results are lower than the resultsof parse2, even though both parsers produced lin-guistically correct and perfectly useful output.Existing methods for making parsing results com-parable across experiments include heuristics forconverting outputs into dependency trees of a prede-fined standard (Briscoe et al, 2002; Cer et al, 2010)or evaluating the performance of a parser within anembedding task (Miyao et al, 2008; Buyko andHahn, 2010).
However, heuristic rules for cross-annotation conversion are typically hand written anderror prone, and may not cover all possible discrep-ancies.
Task-based evaluation may be sensitive tothe particular implementation of the embedding taskand the procedures that extract specific task-relatedfeatures from the different parses.
Beyond that,conversion heuristics and task-based procedures arecurrently developed almost exclusively for English.Other languages typically lack such resources.A recent study by Schwartz et al (2011) takesa different approach towards cross-annotation eval-uation.
They consider different directions ofhead-dependent relations (such as on?Sundayand Sunday?on) and different parent-child andgrandparent-child relations in a chain (such asarrive?on and arrive?sunday in ?arrive on sun-day?)
as equivalent.
They then score arcs that fallwithin corresponding equivalence sets.
Using thesenew scores Schwartz et al (2011) neutralize certainannotation discrepancies that distort parse compar-ison.
However, their treatment is limited to localcontext and does not treat structures larger than twosequential arcs.
Additionally, since arcs in differ-ent directions are typically labeled differently, thismethod only applies for unlabeled dependencies.What we need is a fully deterministic and for-mally precise procedure for comparing any set of la-beled or unlabeled dependency trees, by consolidat-ing the shared linguistic content of the complete de-pendency trees in different annotation schemes, andcomparing parse hypotheses through sound metricsthat can take into account multiple gold standards.3 The Proposal: Cross-AnnotationEvaluation in Three Simple StepsWe propose a new protocol for cross-experimentparse evaluation, consisting of three fundamentalcomponents: (i) abstracting away from annotationpeculiarities, (ii) generalizing theory-specific struc-tures into a single linguistically coherent gold stan-dard that contains all and only consistent informa-tion from all sources, and (iii) defining a sound met-ric that takes into account the different gold stan-dards that are being considered in the experiments.In this section we first define functional trees asthe common space of formal objects and define a de-terministic conversion procedure from dependencytrees to functional trees.
Next we define a set of for-mal operations on functional trees that compute, forevery pair of corresponding trees of the same yield, asingle gold tree that resolves inconsistencies amonggold standard alternatives and combines the infor-mation that they share.
Finally, we define scoresbased on tree edit distance, refined to consider thedistance from parses to the overall gold tree as wellas the different annotation alternatives.vg vgvgvgtmodpobj pobjpreppreptmodtmodtmod387Preliminaries.
Let T be a finite set of terminalsymbols and let L be a set of grammatical relationlabels.
A dependency graph d is a directed graphwhich consists of nodes Vd and arcs Ad ?
Vd ?
Vd.We assume that all nodes in Vd are labeled by ter-minal symbols via a function labelV : Vd ?
T .
Awell-formed dependency graph d = (Vd, Ad) for asentence S = t1, t2, ..., tn is any dependency graphthat is a directed tree originating out of a node v0labeled t0 = ROOT , and spans all terminals inthe sentence, that is, for every ti ?
S there existsvj ?
Vd labeled labelV (vj) = ti.
For simplicity weassume that every node vj is indexed according tothe position of the terminal label, i.e., that for eachti labeling vj , i always equals j.
In a labeled de-pendency tree, arcs in Ad are labeled by elementsof L via a function labelA : Ad ?
L that encodesthe grammatical relation between the terminals la-beling the connected nodes.
We define two auxiliaryfunctions on nodes in dependency trees.
The func-tion subtree : Vd ?
P(Vd) assigns to every nodev ?
Vd the set of nodes accessible by it throughthe reflexive transitive closure of the arc relation Ad.The function span : Vd ?
P(T ) assigns to everynode v ?
Vd a set of terminals such that span(v) ={t ?
T |t = labelV (u) and u ?
subtree(v)}.1Step 1: Functional Representation Our first goalis to define a representation format that keeps allfunctional relationships that are represented in thedependency trees intact, but remains neutral withrespect to the directionality of the head-dependentrelations.
To do so we define functional trees?
linearly-ordered labeled trees which, instead ofhead-to-head binary relations, represent the com-plete functional structure of a sentence.
Assumingthe same sets of terminal symbols T and grammat-ical relation labels L, and assuming extended setsof nodes V and arcs A ?
V ?
V , a functional treepi = (V,A) is a directed tree originating from a sin-gle root v0 ?
V where all non-terminal nodes inpi are labeled with grammatical relation labels thatsignify the grammatical function of the chunk theydominate inside the tree via labelNT : V ?
L. All1If a dependency tree d is projective, than for all v ?
Vd theterminals in span(v) form a contiguous segment of S. The cur-rent discussion assumes that all trees are projective.
We com-ment on non-projective dependencies in Section 4.terminal nodes in pi are labeled with terminal sym-bols via a labelT : V ?
T function.
The functionspan : V ?
P(V ) now picks out the set of ter-minal labels of the terminal nodes accessible by anode v ?
V via A.
We obtain functional trees fromdependency trees using the following procedure:?
Initialize the set of nodes and arcs in the tree.V := VdA := Ad?
Label each node v ?
V with the label of itsincoming arc.labelNT (v) = labelA(u, v)?
In case |span(v)| > 1 add a new node u as adaughter designating the lexical head, labeledwith the wildcard symbol *:V := V ?
{u}A := A ?
{(v, u)}labelNT (u) = ??
For each node v such that |span(v)| = 1, add anew node u as a daughter, labeled with its ownterminal:V := V ?
{u}A := A ?
{(v, u)}if (labelNT (v) = ?
)labelT (u) := labelV (v)elselabelT (u) := labelV (parent(v))That is to say, we label all nodes with spansgreater than 1 with the grammatical function of theirhead, and for each node we add a new daughter udesignating the head word, labeled with its gram-matical function.
Wildcard labels are compatiblewith any, more specific, grammatical function of theword inside the phrase.
This gives us a constituency-like representation of dependency trees labeled withfunctional information, which retains the linguis-tic assumptions reflected in the dependency trees.When applying this procedure, examples (1)?
(3) gettransformed into (4)?
(6) respectively.
(4a) ...prepon*Sunday(4b) ...*onpobjSunday388(5a) ...conjearthconjwind*andconjfire(5b) ...*earthconjwindccandconjfire(5c) ...*earthcoord*windcoord*andconjfire(6a) ...*wouldvg*havevgworked(6b) ...vgvgwould*have*workedConsidering the functional trees resulting fromour procedure, it is easy to see that for tree pairs(4a)?
(4b) and (5a)?
(5b) the respective functionaltrees are identical modulo wildcards, while tree pairs(5b)?
(5c) and (6a)?
(6b) end up with different treestructures that realize different assumptions con-cerning the internal structure of the tree.
In orderto compare, combine or detect inconsistencies in theinformation inherent in different functional trees, wedefine a set of formal operations that are inspired byfamiliar notions from unification-based formalisms(Shieber (1986) and references therein).Step 2: Formal Operations on Trees The intu-ition behind the formal operations we define is sim-ple.
A completely flat tree over a span is the mostgeneral structural description that can be given to it.The more nodes dominate a span, the more linguis-tic assumptions are made with respect to its struc-ture.
If an arc structure in one tree merely elaboratesan existing flat span in another tree, the theories un-derlying the schemes are compatible, and their in-formation can be combined.
Otherwise, there existsa conflict in the linguistic assumptions, and we needto relax some of the assumptions, i.e., remove func-tional nodes, in order to obtain a coherent structurethat contains the information on which they agree.Let pi1, pi2 be functional trees over the same yieldt1, .., tn.
Let the function span(v) pick out the ter-minals labeling terminal nodes that are accessiblevia a node v ?
V in the functional tree through therelation A.
We define first the tree subsumption re-lation for comparing the amount of information in-herent in the arc-structure of two trees.2T-Subsumption, denoted t, is a relation be-tween trees which indicates that a tree pi1 isconsistent with and more general than treepi2.
Formally: pi1 t pi2 iff for every noden ?
pi1 there exists a node m ?
pi2 suchthat span(n) = span(m) and label(n) =label(m).Looking at the functional trees of (4a)?
(4b) wesee that their unlabeled skeletons mutually subsumeeach other.
In their labeled versions, however, eachtree contains labeling information that is lacking inthe other.
In the functional trees (5b)?
(5c) a flatstructure over a span in (5b) is more elaborated in(5c).
In order to combine information in trees withcompatible arc structures, we define tree unification.T-Unification, denoted unionsqt, is the operation thatreturns the most general tree structure pi3 thatis subsumed by both pi1, pi2 if such exists, andfails otherwise.
Formally: pi1 unionsqt pi2 = pi3 iffpi1 t pi3 and pi2 t pi3, and for all pi4 such thatpi1 t pi4 and pi2 t pi4 it holds that pi3 t pi4.Tree unification collects the information from twotrees into a single result if they are consistent, anddetects an inconsistency otherwise.
In case of aninconsistency, as is the case in the functional trees(6a) and (6b), we cannot unify the structures dueto a conflict concerning the internal division of anexpression into phrases.
However, we still want togeneralize these two trees into one tree that containsall and only the information that they share.
For thatwe define the tree generalization operation.T-Generalization, denoted t, is the operationthat returns the most specific tree that is moregeneral than both trees.
Formally, pi1 t pi2 =pi3 iff pi3 t pi1 and pi3 t pi2, and for every pi4such that pi4 t pi1 and pi4 t pi2 it holds thatpi4 t pi3.2Note that the wildcard symbol * is equal to any other sym-bol.
In case the node labels consist of complex feature structuresmade of attribute-value lists, we replace label(n) = label(m)in the subsumption definition with label(n)  label(m) in thesense of (Shieber, 1986).389Unlike unification, generalization can never fail.For every pair of trees there exists a tree that is moregeneral than both: in the extreme case, pick the com-pletely flat structure over the yield, which is moregeneral than any other structure.
For (6a)?
(6b), forinstance, we get that (6a)t(6b) is a flat tree overpre-terminals where ?would?
and ?have?
are labeledwith ?vg?
and ?worked?
is the head, labeled with ?
*?.The generalization of two functional trees pro-vides us with one structure that reflects the commonand consistent content of the two trees.
These struc-tures thus provide us with a formally well-definedgold standard for cross-treebank evaluation.Step 3: Measuring Distances.
Our functionaltrees superficially look like constituency-basedtrees, so a simple proposal would be to use Parse-val measures (Black et al, 1991) for comparing theparsed trees against the new generalized gold trees.Parseval scores, however, have two significant draw-backs.
First, they are known to be too restrictivewith respect to some errors and too permissive withrespect to others (Carroll et al, 1998; Ku?bler andTelljohann, 2002; Roark, 2002; Rehbein and vanGenabith, 2007).
Secondly, F1 scores would stillpenalize structures that are correct with respect tothe original gold, but are not there in the generalizedstructure.
Here we propose to adopt measures thatare based on tree edit distance (TED) instead.
TED-based measures are, in fact, an extension of attach-ment scores for dependency trees.
Consider, for in-stance, the following operations on dependency arcs.reattach-arc remove arc (u, v) ?
Ad and addan arc Ad ?
{(w, v)}.relabel-arc relabel arc l1(u, v) as l2(u, v)Assuming that each operation is assigned a cost,the attachment score of comparing two dependencytrees is simply the cost of all edit operations that arerequired to turn a parse tree into its gold standard,normalized with respect to the overall size of the de-pendency tree and subtracted from a unity.3 Herewe apply the idea of defining scores by TED costsnormalized relative to the size of the tree and sub-stracted from a unity, and extend it from fixed-sizedependency trees to ordered trees of arbitrary size.3The size of a dependency tree, either parse or gold, is al-ways fixed by the number of terminals.Our formalization follows closely the formulationof the T-Dice measure of Emms (2008), building onhis thorough investigation of the formal and empir-ical differences between TED-based measures andParseval.
We first define for any ordered and labeledtree pi the following operations.relabel-node change the label of node v in pidelete-node delete a non-root node v in pi withparent u, making the children of v the childrenof u, inserted in the place of v as a subsequencein the left-to-right order of the children of u.insert-node insert a node v as a child of u inpi making it the parent of a consecutive subse-quence of the children of u.An edit script ES(pi1, pi2) = {e0, e1....ek} betweenpi1 and pi2 is a set of edit operations required for turn-ing pi1 into pi2.
Now, assume that we are given a costfunction defined for each edit operation.
The cost ofES(pi1, pi2) is the sum of the costs of the operationsin the script.
An optimal edit script is an edit scriptbetween pi1 and pi2 of minimum cost.ES?
(pi1, pi2) = argminES(pi1,pi2)?e?ES(pi1,pi2)cost(e)The tree edit distance problem is defined to be theproblem of finding the optimal edit script and com-puting the corresponding distance (Bille, 2005).A simple way to calculate the error ?
of a parsewould be to define it as the edit distance betweenthe parse hypothesis pi1 and the gold standard pi2.?
(pi1, pi2) = cost(ES?
(pi1, pi2))However, in such cases the parser may still get pe-nalized for recovering nodes that are lacking in thegeneralization.
To solve this, we refine the distancebetween a parse tree and the generalized gold treeto discard edit operations on nodes that are there inthe native gold tree but are eliminated through gen-eralization.
We compute the intersection of the editscript turning the parse tree into the generalize goldwith the edit script turning the native gold tree intothe generalized gold, and discard its cost.
That is, ifparse1 and parse2 are compared against gold1 andgold2 respectively, and if we set gold3 to be the re-sult of gold1tgold2, then ?new is defined as:390Figure 2: The evaluation pipeline.
Different versions of the treebank go into different experiments, resulting indifferent parse and gold files.
All trees are transformed into functional trees.
All gold files enter generalization toyield a new gold.
The different ?
arcs represent the different tree distances used for calculating the TED-based scores.
?new(parse1, gold1,gold3) =?(parse1,gold3)?cost(ES?(parse1,gold3)?ES?
(gold1,gold3))Now, if gold1 and gold3 are identi-cal, then ES?(gold1,gold3)=?
and we fallback on the simple tree edit distance score?new(parse1,gold1,gold3)=?
(parse1, gold3).When parse1 and gold1 are identical,i.e., the parser produced perfect out-put with respect to its own scheme, then?new(parse1,gold1,gold3)=?new(gold1,gold1,gold3)=?(gold1,gold3)?
cost(ES?
(gold1,gold3))=0, andthe parser does not get penalized for recovering acorrect structure in gold1 that is lacking in gold3.In order to turn distances into accuracy measureswe have to normalize distances relative to the maxi-mal number of operations that is conceivable.
In theworst case, we would have to remove all the internalnodes in the parse tree and add all the internal nodesof the generalized gold, so our normalization factor?
is defined as follows, where |pi| is the size4 of pi.?
(parse1,gold3) = |parse1| + |gold3|We now define the score of parse1 as follows:51?
?new(parse1,gold1,gold3)?
(parse1,gold3)Figure 2 summarizes the steps in the evalu-ation procedure we defined so far.
We startoff with two versions of the treebank, TB1 andTB2, which are parsed separately and provide theirown gold standards and parse hypotheses in a la-beled dependencies format.
All dependency trees4Following common practice, we equate size |pi| with thenumber of nodes in pi, discarding the terminals and root node.5If the trees have only root and leaves, ?
= 0, score := 1.are then converted into functional trees, and wecompute the generalization of each pair of goldtrees for each sentence in the data.
This pro-vides the generalized gold standard for all exper-iments, here marked as gold3.6 We finally com-pute the distances ?new(parse1,gold1,gold3) and?new(parse2,gold2,gold3) using the different treeedit distances that are now available, and we repeatthe procedure for each sentence in the test set.To normalize the scores for an entire test set ofsize n we can take the arithmetic mean of the scores.
?|test-set|i=1 score(parse1i,gold1i,gold3i)|test-set|Alternatively we can globally average of all edit dis-tance costs, normalized by the maximally possibleedits on parse trees turned into generalized trees.1?
?|test-set|i=1 ?new(parse1i,gold1i,gold3i)?|test-set|i=1 ?
(parse1i,gold3i)The latter score, global averaging over the entire testset, is the metric we use in our evaluation procedure.4 ExperimentsWe demonstrate the application of our procedure tocomparing dependency parsing results on differentversions of the Penn Treebank (Marcus et al, 1993).The Data We use data from the PTB, convertedinto dependency structures using the LTH soft-ware, a general purpose tool for constituency-to-dependency conversion (Johansson and Nugues,2007).
We use LTH to implement the five differentannotation standards detailed in Table 3.6Generalization is an associative and commutative opera-tion, so it can be extended for n experiments in any order.TB1 parse1.depgold1.depparse2.depgold2.depparse1gold3gold1parse2gold2TB2parseparse?
(parse1,gold3)?
(gold1,gold3)?
(parse2,gold3)?
(gold2,gold3)parse transform generalizeparse391Train Default Old LTH CoNLL07GoldDefault UAS 0.9142 0.6077 0.7772LAS 0.8820 0.4801 0.6454U-TED 0.9488 0.8926 0.9237L-TED 0.9241 0.7811 0.8441Old LTH UAS 0.6053 0.8955 0.6508LAS 0.4816 0.8644 0.5771U-TED 0.8931 0.9564 0.9092L-TED 0.7811 0.9317 0.8197CoNLL07 UAS 0.7734 0.6474 0.8917LAS 0.6479 0.5722 0.8736U-TED 0.9260 0.9097 0.9474L-TED 0.8480 0.8204 0.9233Default-OldLTH U-TED 0.9500 0.9543L-TED 0.9278 0.9324Default-CoNLL07 U-TED 0.9444?
0.9453?L-TED 0.9266?
0.9260?oldLTH-CoNLL07 U-TED 0.9519 0.9490L-TED 0.9323 0.9283default-oldLTH-CoNLL U-TED 0.9464?
0.9515 0.9471?L-TED 0.9281?
0.9336 0.9280?Train CoNLL07 Functional LexicalGoldCoNLL07 UAS 0.8917 0.8054 0.6986LAS 0.8736 0.7895 0.6831U-TED 0.9474 0.9357 0.9237L-TED 0.9233 0.8960 0.8606Functional UAS 0.8040 0.8970 0.6110LAS 0.7873 0.8793 0.5977U-TED 0.9347 0.9466 0.9107L-TED 0.8948 0.9239 0.8316Lexical UAS 0.7013 0.6138 0.8823LAS 0.6875 0.6022 0.8635U-TED 0.9252 0.9132 0.9500L-TED 0.8623 0.8345 0.9266CoNLL07-Functional U-TED 0.9473?
0.9473?L-TED 0.9233 0.9247CoNLL07-Lexical U-TED 0.9490?
0.9500?L-TED 0.9253?
0.9266?Functional-Lexical U-TED 0.9489?
0.9501?L-TED 0.9266?
0.9267?CoNLL07-Functional-Lexical U-TED 0.9489?
0.9489?
0.9501?L-TED 0.9254?
0.9266?
0.9267?Table 1: Cross-experiment dependency parsing evaluation for MaltParser trained on multiple schemes.
We report stan-dard LAS scores and TEDEVAL global average metrics.
Boldface results outperform the rest of the results reportedin the same row.
The ?
sign marks pairwise results where the difference is not statistically significant.Train Default Old LTH CoNLL07GoldDefault UAS 0.9173 0.6085 0.7709LAS 0.8833 0.4780 0.6414U-TED 0.9513 0.8903 0.9236L-TED 0.9249 0.7727 0.8424Old LTH UAS 0.6078 0.8952 0.6415LAS 0.4809 0.8471 0.5669U-TED 0.8960 0.9550 0.9096L-TED 0.7823 0.9224 0.8170CoNLL07 UAS 0.7767 0.6517 0.8991LAS 0.6504 0.5725 0.8709U-TED 0.9289 0.9087 0.9479L-TED 0.8502 0.8159 0.9208Default-oldLTH U-TED 0.9533 0.9515L-TED 0.9289 0.9224Default-CoNLL U-TED 0.9474?
0.9460?L-TED 0.9281 0.9238OldLTH-CoNLL U-TED 0.9479 0.9493L-TED 0.9234 0.9258Default-OldLTH-CoNLL U-TED 0.9492?
0.9461 0.9480?L-TED 0.9298 0.9241?
0.9258?Train CoNLL07 Functional LexicalGoldCoNLL07 UAS 0.8991 0.8077 0.7018LAS 0.8709 0.7902 0.6804U-TED 0.9479 0.9373 0.9221L-TED 0.9208 0.8955 0.8505Functional UAS 0.8083 0.8978 0.6150LAS 0.7895 0.8782 0.5975U-TED 0.9356 0.9476 0.9092L-TED 0.8929 0.9226 0.8218Lexical UAS 0.6997 0.6161 0.8826LAS 0.6835 0.6034 0.8491U-TED 0.9259 0.9152 0.9483L-TED 0.8593 0.8340 0.9160CoNLL-Functional U-TED 0.9479?
0.9487?L-TED 0.9209 0.9237CoNLL-Lexical U-TED 0.9497 0.9483L-TED 0.9228 0.9161Functional-Lexical U-TED 0.9504 0.9483L-TED 0.9258 0.9161CoNLL-Functional-Lexical U-TED 0.9498 0.9504?
0.9483?L-TED 0.9229 0.9258 0.9161Table 2: Cross-experiment dependency parsing evaluation for the MST parser trained on multiple schemes.
Wereport standard LAS scores and TEDEVAL global average metrics.
Boldface results outperform the rest of the resultsreported in the same row.
The ?
sign marks pairwise results where the difference is not statistically significant.ID DescriptionDefault The LTH conversion default settingsOldLTH The conversion used in Johansson and Nugues (2007)CoNLL07 The conversion used in the CoNLL shared task (Nivre et al, 2007a)Lexical Same as CoNLL, but selecting only lexical heads when a choice existsFunctional Same as CoNLL, but selecting only functional heads when a choice existsTable 3: LTH conversion schemes used in the experiments.
The LTH conversion settings in terms of the completefeature-value pairs associated with the LTH parameters in different schemes are detailed in the supplementary material.392The Default, OldLTH and CoNLL schemesmainly differ in their coordination structure, and theFunctional and Lexical schemes differ in their selec-tion of a functional and a lexical head, respectively.All schemes use the same inventory of labels.7 TheLTH parameter settings for the different schemes areelaborated in the supplementary material.The Setup We use two different parsers: (i) Malt-Parser (Nivre et al, 2007b) with the arc eager algo-rithm as optimized for English in (Nivre et al, 2010)and (ii) MSTParser with the second-order projec-tive model of McDonald and Pereira (2006).
Bothparsers were trained on the different instances ofsections 2-21 of the PTB obeying the different an-notation schemes in Table 3.
Each trained modelwas used to parse section 23.
All non-projective de-pendencies in the training and gold sets were projec-tivized prior to training and parsing using the algo-rithm of Nivre and Nilsson (2005).
A more princi-pled treatment of non-projective dependency trees isan important topic for future research.
We evaluatedthe parses using labeled and unlabeled attachmentscores, and using our TEDEVAL software package.Evaluation Our TEDEVAL software package im-plements the pipeline described in Section 3.
Weconvert all parse and gold trees into functionaltrees using the algorithm defined in Section 3, andfor each pair of parsing experiments we calculatea shared gold standard using generalization deter-mined through a chart-based greedy algorithm.8 Ourscoring procedure uses the TED algorithm definedby Zhang and Shasha (1989).9 The unlabeled scoreis obtained by assigning cost(e) = 0 for every e re-labeling operation.
To calculate pairwise statisticalsignificance we use a shuffling test with 10,000 it-erations (Cohen, 1995).
A sample of all files in theevaluation pipeline for a subset of 10 PTB sentencesis available in the supplementary materials.107In case the labels are not taken from the same inventory,e.g., subjects in one scheme are marked as SUB and in the othermarked as SBJ, it is possible define a a set of zero-cost operationtypes ?
in such case, to the operation relabel(SUB,SBJ) ?
inorder not to penalize string label discrepancies.8Our algorithm has space and runtime complexity ofO(n2).9Available via http://web.science.mq.edu.au/?swan/howtos/treedistance/10The TEDEVAL software package is available via http://stp.lingfil.uu.se/?tsarfaty/uniparResults Table 1 reports the results for the inter-and cross-experiment evaluation of parses producedby MaltParser.
The left hand side of the tablepresents the parsing results for a set of experimentsin which we compare parsing results trained on theDefault, OldLTH and CoNLL07 schemes.
In a sec-ond set of experiments we compare the CoNLL07,Lexical and Functional schemes.
Table 2 reports theevaluation of the parses produced by MSTParser forthe same experimental setup.
Our goal here is not tocompare the parsers, but to verify that the effects ofswitching from LAS to TEDEVAL are robust acrossparsing algorithms.In each of the tables, the top three groups of fourrows compare results of parsed dependency treestrained on a particular scheme against gold trees ofthe same and the other schemes.
The next threegroups of two rows report the results for compar-ing pairwise sets of experiments against a general-ized gold using our proposed procedure.
In the lastgroup of two rows we compare all parsing resultsagainst a single gold obtained through a three-waygeneralization.As expected, every parser appears to perform atits best when evaluated against the scheme it wastrained on.
This is the case for both LAS and TEDE-VAL measures and the performance gaps are statis-tically significant.
When moving to pairwise evalu-ation against a single generalized gold, for instance,when comparing CoNLL07 to the Default settings,there is still a gap in performance, e.g., betweenOldLTH and CoNLL07, and between OldLTH andDefault.
This gap is however a lot smaller and is notalways statistically significant.
In fact, when evalu-ating the effect of linguistically disparate annotationvariations such as Lexical and Functional on the per-formance of MaltParser, Table 1 shows that whenusing TEDEVAL and a generalized gold the perfor-mance gaps are small and statistically insignificant.Moreover, observed performance trends whenevaluating individual experiments on their originaltraining scheme may change when compared againsta generalized gold.
The Default scheme, for Malt-Parser, appears better than OldLTH when both areevaluated against their training schemes.
But look-ing at the pairwise-evaluated experiments, it is theother way round (the difference is smaller, but statis-tically significant).
In evaluating against a three-way393generalization, all the results obtained for differenttraining schemes are on a par with one another, withminor gaps in performance, rarely statistically sig-nificant.
This suggests that apparent performancetrends between experiments when evaluating withrespect to the training schemes may be misleading.These observations are robust across parsing algo-rithms.
In each of the tables, results obtained againstthe training schemes show significant differenceswhereas applying our cross-experimental procedureshows small to no gaps in performance across dif-ferent schemes.
Annotation variants which seem tohave crucial effects have a relatively small influencewhen parsed structures are brought into the sameformal and theoretical common ground for compar-ison.
Of course, it may be the case that one parser isbetter trained on one scheme while the other utilizesbetter another scheme, but objective performancegaps can only be observed when they are comparedagainst shared linguistic content.5 Discussion and ExtensionsThis paper addresses the problem of cross-experiment evaluation.
As it turns out, this prob-lem arises in NLP in different shapes and forms;when evaluating a parser against different annota-tion schemes, when evaluating parsing performanceacross parsers and different formalisms, and whencomparing parser performance across languages.We consider our contribution successful if afterreading it the reader develops a healthy suspicion toblunt comparison of numbers across experiments, orbetter yet, across different papers.
Cross-experimentcomparison should be a careful and well thought-through endeavor, in which we retain as much infor-mation as we can from the parsed structures, avoidlossy conversions, and focus on an object of evalua-tion which is agreed upon by all variants.Our proposal introduces one way of doing so ina streamlined, efficient and formally worked outway.
While individual components may be furtherrefined or improved, the proposed setup and imple-mentation can be straightforwardly applied to cross-parser and cross-framework evaluation.
In the fu-ture we plan to use this procedure for comparingconstituency and dependency parsers.
A conversionfrom constituency-based trees into functional treesis straightforward to define: simply replace the nodelabels with the grammatical function of their domi-nating arc ?
and the rest of the pipeline follows.A pre-condition for cross-framework evaluationis that all representations encode the same set ofgrammatical relations by, e.g., annotating arcs in de-pendency trees or decorating nodes in constituencytrees.
For some treebanks this is already the case(Nivre and Megyesi, 2007; Skut et al, 1997; Hin-richs et al, 2004) while for others this is still lack-ing.
Recent studies (Briscoe et al, 2002; de Marn-effe et al, 2006) suggest that evaluation through asingle set of grammatical relations as the commondenominator is a linguistically sound and practicallyuseful way to go.
To guarantee extensions for cross-framework evaluation it would be fruitful to makesure that resources use the same set of grammaticalrelation labels across different formal representationtypes.
Moreover, we further aim to inquire whetherwe can find a single set of grammatical relation la-bels that can be used across treebanks for multiplelanguages.
This would then pave the way for the de-velopment of cross-language evaluation procedures.6 ConclusionWe propose an end-to-end procedure for compar-ing dependency parsing results across experimentsbased on three steps: (i) converting dependency treesto functional trees, (ii) generalizing functional treesto harmonize information from different sources,and (iii) using distance-based metrics that take thedifferent sources into account.
When applied toparsing results of different dependency schemes,dramatic gaps observed when comparing parsing re-sults obtained in isolation decrease or dissolve com-pletely when using our proposed pipeline.Acknowledgments We thank the developers ofthe LTH and TED software who made their codeavailable for our use.
We thank Richard Johanssonfor providing us with the LTH parameter settings ofexisting dependency schemes.
We thank Ari Rap-poport, Omri Abend, Roy Schwartz and members ofthe NLP lab at the Hebrew University of Jerusalemfor stimulating discussion.
We finally thank threeanonymous reviewers for useful comments on anearlier draft.
The research reported in the paper waspartially funded by the Swedish Research Council.394ReferencesPhilip Bille.
2005.
A survey on tree edit distanceand related.
problems.
Theoretical Computer Science,337:217?239.Ezra Black, Steven P. Abney, D. Flickenger, ClaudiaGdaniec, Ralph Grishman, P. Harrison, Donald Hin-dle, Robert Ingria, Frederick Jelinek, Judith L. Kla-vans, Mark Liberman, Mitchell P. Marcus, SalimRoukos, Beatrice Santorini, and Tomek Strzalkowski.1991.
Procedure for quantitatively comparing the syn-tactic coverage of English grammars.
In E. Black, ed-itor, Proceedings of the workshop on Speech and Nat-ural Language, HLT, pages 306?311.
Association forComputational Linguistics.Ted Briscoe, John Carroll, Jonathan Graham, and AnnCopestake.
2002.
Relational evaluation schemes.In Proceedings of LREC Workshop?Beyond Parseval?
Towards improved evaluation measures for parsingsystems?.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProceedings of CoNLL-X, pages 149?164.Ekaterina Buyko and Udo Hahn.
2010.
Evaluatingthe impact of alternative dependency graph encodingson solving event extraction tasks.
In Proceedings ofEMNLP, pages 982?992.John Carroll, Ted Briscoe, and Antonio Sanfilippo.
1998.Parser evaluation: a survey and a new proposal.
InProceedings of LREC, pages 447?454.Daniel Cer, Marie-Catherine de Marneffe, Daniel Juraf-sky, and Christopher D. Manning.
2010.
Parsing tostanford dependencies: Trade-offs between speed andaccuracy.
In Proceedings of LREC.Jinho D. Choi and Martha Palmer.
2010.
Robustconstituent-to-dependency conversion for English.
InProceedings of TLT.Paul Cohen.
1995.
Empirical Methods for Artificial In-telligence.
The MIT Press.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC, pages 449?454.Martin Emms.
2008.
Tree-distance and some other vari-ants of evalb.
In Proceedings of LREC.Erhard Hinrichs, Sandra Ku?bler, Karin Naumann, HeikeTelljohan, and Julia Trushkina.
2004.
Recent develop-ment in linguistic annotations of the Tu?Ba-D/Z Tree-bank.
In Proceedings of TLT.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProceedings of NODALIDA.Sandra Ku?bler and Heike Telljohann.
2002.
Towardsa dependency-oriented evaluation for partial parsing.In Proceedings of LREC Workshop?Beyond Parseval?
Towards improved evaluation measures for parsingsystems?.Sandra Ku?bler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Number 2 in SynthesisLectures on Human Language Technologies.
Morgan& Claypool Publishers.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19:313?330.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Proceedings of EACL, pages 81?88.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In Proceedings of ACL, pages 91?98.Igor Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-oriented eval-uation of syntactic parsers and their representations.
InProceedings of ACL, pages 46?54.Joakim Nivre and Beata Megyesi.
2007.
Bootstrappinga Swedish Treebank using cross-corpus harmonizationand annotation projection.
In Proceedings of TLT.Joakim Nivre and Jens Nilsson.
2005.
Pseudo projectivedependency parsing.
In Proceeding of ACL, pages 99?106.Joakim Nivre and Mario Scholz.
2004.
Deterministicdependency parsing of English text.
In Proceedings ofCOLING, pages 64?70.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007a.
The CoNLL 2007 shared task on dependencyparsing.
In Proceedings of the CoNLL Shared TaskSession of EMNLP-CoNLL 2007, pages 915?932.Joakim Nivre, Jens Nilsson, Johan Hall, Atanas Chanev,Gu?lsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,and Erwin Marsi.
2007b.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(1):1?41.JoakimNivre, Laura Rimell, RyanMcDonald, and CarlosGo?mez-Rodr??guez.
2010.
Evaluation of dependencyparsers on unbounded dependencies.
pages 813?821.Owen Rambow.
2010.
The Simple Truth about Depen-dency and Phrase Structure Representations: An Opin-ion Piece.
In Proceedings of HLT-ACL, pages 337?340.Ines Rehbein and Josef van Genabith.
2007.
Why is it sodifficult to compare treebanks?
Tiger and Tu?Ba-D/Zrevisited.
In Proceedings of TLT, pages 115?126.395Brian Roark.
2002.
Evaluating parser accuracy us-ing edit distance.
In Proceedings of LREC Work-shop?Beyond Parseval ?
Towards improved evaluationmeasures for parsing systems?.Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-poport.
2011.
Neutralizing linguistically problematicannotations in unsupervised dependency parsing eval-uation.
In Proceedings of ACL, pages 663?672.Stuart M. Shieber.
1986.
An Introduction to Unification-Based Grammars.
Center for the Study of Languageand Information.Wojciech Skut, Brigitte Krenn, Thorsten Brants, andHans Uszkoreit.
1997.
An annotation scheme for freeword-order languages.
In Proceedings of the fifth con-ference on Applied natural language processing, pages88?95.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProceeding of IWPT, pages 195?206.Kaizhong Zhang and Dennis Shasha.
1989.
Simple fastalgorithms for the editing distance between trees andrelated problems.
In SIAM Journal of Computing, vol-ume 18, pages 1245?1262.Arnold M. Zwicky.
1993.
Heads, bases, and functors.In G.G.
Corbett, N. Fraser, and S. McGlashan, editors,Heads in Grammatical Theory, pages 292?315.
Cam-bridge University Press.396
