Proceedings of the ACL Interactive Poster and Demonstration Sessions,pages 77?80, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Practical Solution to the Problem ofAutomatic Part-of-Speech Induction from TextReinhard RappUniversity of Mainz, FASKD-76711 Germersheim, Germanyrapp@mail.fask.uni-mainz.deAbstractThe problem of part-of-speech inductionfrom text involves two aspects: Firstly, aset of word classes is to be derived auto-matically.
Secondly, each word of a vo-cabulary is to be assigned to one or sev-eral of these word classes.
In this paperwe present a method that solves bothproblems with good accuracy.
Our ap-proach adopts a mixture of statistical me-thods that have been successfully appliedin word sense induction.
Its main advan-tage over previous attempts is that it re-duces the syntactic space to only the mostimportant dimensions, thereby almost eli-minating the otherwise omnipresent prob-lem of data sparseness.1 IntroductionWhereas most previous statistical work concerningparts of speech has been on tagging, this paperdeals with part-of-speech induction.
In part-of-speech induction two phases can be distinguished:In the first phase a set of word classes is to be de-rived automatically on the basis of the distributionof the words in a text corpus.
These classes shouldbe in accordance with human intuitions, i.e.
com-mon distinctions such as nouns, verbs and adjec-tives are desirable.
In the second phase, based onits observed usage each word is assigned to one orseveral of the previously defined classes.The main reason why part-of-speech inductionhas received far less attention than part-of-speechtagging is probably that there seemed no urgentneed for it as linguists have always consideredclassifying words as one of their core tasks, and asa consequence accurate lexicons providing suchinformation are readily available for many lan-guages.
Nevertheless, deriving word classes auto-matically is an interesting intellectual challengewith relevance to cognitive science.
Also, advan-tages of the automatic systems are that they shouldbe more objective and can provide precise infor-mation on the likelihood distribution for each of aword?s parts of speech, an aspect that is useful forstatistical machine translation.The pioneering work on class based n-grammodels by Brown et al (1992) was motivated bysuch considerations.
In contrast, Sch?tze (1993) byapplying a neural network approach put the em-phasis on the cognitive side.
More recent work in-cludes Clark (2003) who combines distributionaland morphological information, and Freitag (2004)who uses a hidden Marcov model in combinationwith co-clustering.Most studies use abstract statistical measuressuch as perplexity or the F-measure for evaluation.This is good for quantitative comparisons, butmakes it difficult to check if the results agree withhuman intuitions.
In this paper we use a straight-forward approach for evaluation.
It involves check-ing if the automatically generated word classesagree with the word classes known from grammarbooks, and whether the class assignments for eachword are correct.2 ApproachIn principle, word classification can be based on anumber of different linguistic principles, e.g.
onphonology, morphology, syntax or semantics.However, in this paper we are only interested insyntactically motivated word classes.
With syntac-tic classes the aim is that words belonging to thesame class can substitute for one another in a sen-tence without affecting its grammaticality.As a consequence of the substitutability, whenlooking at a corpus words of the same class typi-cally have a high agreement concerning their leftand right neighbors.
For example, nouns are fre-quently preceded by words like a, the, or this, andsucceeded by words like is, has or in.
In statistical77terms, words of the same class have a similar fre-quency distribution concerning their left and rightneighbors.
To some extend this can also be ob-served with indirect neighbors, but with them theeffect is less salient and therefore we do not con-sider them here.The co-occurrence information concerning thewords in a vocabulary and their neighbors can bestored in a matrix as shown in table 1.
If we nowwant to discover word classes, we simply computethe similarities between all pairs of rows using avector similarity measure such as the cosine coef-ficient and then cluster the words according tothese similarities.
The expectation is that unambi-guous nouns like breath and meal form one cluster,and that unambiguous verbs like discuss and pro-tect form another cluster.Ambiguous words like link or suit should notform a tight cluster but are placed somewhere inbetween the noun and the verb clusters, with theexact position depending on the ratios of the occur-rence frequencies of their readings as either a nounor a verb.
As this ratio can be arbitrary, accordingto our experience ambiguous words do not se-verely affect the clustering but only form someuniform background noise which more or less can-cels out in a large vocabulary.1 Note that the cor-rect assignment of the ambiguous words to clustersis not required at this stage, as this is taken care ofin the next step.This step involves computing the differentialvector of each word from the centroid of its closestcluster, and to assign the differential vector to themost appropriate other cluster.
This process can berepeated until the length of the differential vectorfalls below a threshold or, alternatively, the agree-ment with any of the centroids becomes too low.This way an ambiguous word is assigned to severalparts of speech, starting from the most commonand proceeding to the least common.
Figure 1 il-lustrates this process.1An alternative to relying on this fortunate but somewhat un-satisfactory effect would be not to use global co-occurrencevectors but local ones, as successfully proposed in word senseinduction (Rapp, 2004).
This means that every occurrence of aword obtains a separate row vector in table 1.
The problemwith the resulting extremely sparse matrix is that most vectorsare either orthogonal to each other or duplicates of some othervector, with the consequence that the dimensionality reductionthat is indispensable for such matrices does not lead to sensi-ble results.
This problem is not as severe in word sense induc-tion where larger context windows are considered.The procedure that we described so far works intheory but not well in practice.
The problem with itis that the matrix is so sparse that sampling errorshave a strong negative effect on the results of thevector comparisons.
Fortunately, the problem ofdata sparseness can be minimized by reducing thedimensionality of the matrix.
An appropriate alge-braic method that has the capability to reduce thedimensionality of a rectangular matrix is SingularValue Decomposition (SVD).
It has the propertythat when reducing the number of columns thesimilarities between the rows are preserved in thebest possible way.
Whereas in other studies thereduction has typically been from several ten thou-sand to a few hundred, our reduction is from sev-eral ten thousand to only three.
This leads to a verystrong generalization effect that proves useful forour particular task.left neighbors right neighborsa we the you a can is wellbreath  11 0 18 0 0 14 19 0discuss 0 17 0 10 9 0 0 8link  14 6 11 7 10 9 14 3meal 15 0 17 0 0 9 12 0protect  0 15 1 12 14 0 0 4suit 5 0 8 3 0 8 16 2Table 1.
Co-occurrence matrix of adjacent words.Figure 1.
Constructing the parts of speech for can.3 ProcedureOur computations are based on the unmodified textof the 100 million word British National Corpus(BNC), i.e.
including all function words and with-out lemmatization.
By counting the occurrencefrequencies for pairs of adjacent words we com-piled a matrix as exemplified in table 1.
As thismatrix is too large to be processed with our algo-rithms (SVD and clustering), we decided to restrictthe number of rows to a vocabulary appropriate forevaluation purposes.
Since we are not aware of anystandard vocabulary previously used in relatedwork, we manually selected an ad hoc list of 5078words with BNC frequencies between 5000 and6000 as shown in table 2.
The choice of 50 wasmotivated by the intention to give complete clus-tering results in graphical form.
As we did notwant to deal with morphology, we used base formsonly.
Also, in order to be able to subjectively judgethe results, we only selected words where we feltreasonably confident about their possible parts ofspeech.
Note that the list of words was compiledbefore the start of our experiments and remainedunchanged thereafter.The co-occurrence matrix based on the restrictedvocabulary and all neighbors occurring in the BNChas a size of 50 rows times 28,443 columns.
As ourtransformation function we simply use the loga-rithm after adding one to each value in the matrix.2As usual, the one is added for smoothing purposesand to avoid problems with zero values.
We de-cided not to use a sophisticated association meas-ure such as the log-likelihood ratio because it hasan inappropriate value characteristic that preventsthe SVD, which is conducted in the next step, fromfinding optimal dimensions.3The purpose of the SVD is to reduce the numberof columns in our matrix to the main dimensions.However, it is not clear how many dimensionsshould be computed.
Since our aim of identifyingbasic word classes such as nouns or verbs requiresstrong generalizations instead of subtle distinc-tions, we decided to take only the three main di-mensions into account, i.e.
the resulting matrix hasa size of 50 rows times 3 columns.4 The last step inour procedure involves applying a clustering algo-rithm to the 50 words corresponding to the rows inthe matrix.
We used hierarchical clustering withaverage linkage, a linkage type that provides con-siderable tolerance concerning outliers.4 Results and EvaluationOur results are presented as dendrograms which incontrast to 2-dimensional dot-plots have the advan-tage of being able to correctly show the true dis-tances between clusters.
The two dendrograms infigure 2 where both computed by applying the pro-cedure as described in the previous section, with2For arbitrary vocabularies the row vectors should be dividedby the corpus frequency of the corresponding word.3We are currently investigating if replacing the log-likelihoodvalues by their ranks can solve this problem.4Note that larger matrices can require a few more dimensions.the only difference that in generating the upperdendrogram the SVD-step has been omitted,whereas in generating the lower dendrogram it hasbeen conducted.
Without SVD the expected clus-ters of verbs, nouns and adjectives are not clearlyseparated, and the adjectives widely and rural areplaced outside the adjective cluster.
With SVD, all50 words are in their appropriate clusters and thethree discovered clusters are much more salient.Also, widely and rural are well within the adjectivecluster.
The comparison of the two dendrogramsindicates that the SVD was capable of making ap-propriate generalizations.
Also, when we look in-side each cluster we can see that ambiguous wordslike suit, drop or brief are somewhat closer to theirsecondary class than unambiguous words.Having obtained the three expected clusters, thenext investigation concerns the assignment of theambiguous words to additional clusters.
As de-scribed previously, this is done by computing dif-ferential vectors, and by assigning these to themost similar other cluster.
Hereby for the cosinesimilarity we set a threshold of 0.8.
That is, only ifthe similarity between the differential vector andits closest centroid was higher than 0.8 we as-signed the word to this cluster and continued tocompute differential vectors.
Otherwise we as-sumed that the differential vector was caused bysampling errors and aborted the process of search-ing for additional class assignments.The results from this procedure are shown in ta-ble 2 where for each of the 50 words all computedclasses are given in the order as they were obtainedby the algorithm, i.e.
the dominant assignments arelisted first.
Although our algorithm does not namethe classes, for simplicity we interpret them in theobvious way, i.e.
as nouns, verbs and adjectives.
Acomparison with WordNet 2.0 choices is given inbrackets.
For example, +N means that WordNetlists the additional assignment noun, and -A indi-cates that the assignment adjective found by thealgorithm is not listed in WordNet.According to this comparison, for all 50 wordsthe first reading is correct.
For 16 words an addi-tional second reading was computed which is cor-rect in 11 cases.
16 of the WordNet assignmentsare missing, among them the verb readings for re-form, suit, and rain and the noun reading for serve.However, as many of the WordNet assignmentsseem rare, it is not clear in how far the omissionscan be attributed to shortcomings of the algorithm.79accident N expensive A  reform N (+V)belief N familiar A (+N) rural Abirth N (+V) finance N V  screen N (+V)breath N grow V N (-N) seek V (+N)brief A N imagine V  serve V (+N)broad A (+N) introduction N  slow A Vbusy A V link N V  spring N A V (-A)catch V N lovely A (+N) strike N Vcritical A lunch N (+V) suit N (+V)cup N (+V) maintain V  surprise N Vdangerous A occur V N (-N) tape N Vdiscuss V option N  thank V A (-A)drop V N pleasure N  thin A (+V)drug N (+V) protect V  tiny Aempty A V (+N) prove V  widely A N (-N)encourage V quick A (+N) wild A (+N)establish V  rain N (+V)Table 2.
Computed parts of speech for each word.5 Summary and ConclusionsThis work was inspired by previous work on wordsense induction.
The results indicate that part ofspeech induction is possible with good successbased on the analysis of distributional patterns intext.
The study also gives some insight how SVDis capable of significantly improving the results.Whereas in a previous paper (Rapp, 2004) wefound that for word sense induction the local clus-tering of local vectors is more appropriate than theglobal clustering of global vectors, for part-of-speech induction our conclusion is that the situa-tion is exactly the other way round, i.e.
the globalclustering of global vectors is more adequate (seefootnote 1).
This finding is of interest when tryingto understand the nature of syntax versus semanticsif expressed in statistical terms.AcknowledgementsI would like to thank Manfred Wettler and Chris-tian Biemann for comments, Hinrich Sch?tze forthe SVD-software, and the DFG (German Re-search Society) for financial support.ReferencesBrown, Peter F.; Della Pietra, Vincent J.; deSouza, PeterV.
; Lai, Jennifer C.; Mercer, Robert L. (1992).
Class-based n-gram models of natural language.
Computa-tional Linguistics 18(4), 467-479.Clark, Alexander (2003).
Combining distributional andmorphological information for part of speech induc-tion.
Proceedings of 10th EACL, Budapest, 59-66.Freitag, Dayne (2004).
Toward unsupervised whole-corpus tagging.
Proceedings of COLING, Geneva,357-363.Rapp, Reinhard (2004).
A practical solution to the prob-lem of automatic word sense induction.
Proceedingsof ACL (Companion Volume), Barcelona, 195-198.Sch?tze, Hinrich (1993).
Part-of-speech induction fromscratch.
Proceedings of ACL, Columbus, 251-258.0.80.40.01.00.50.0Figure 2.
Syntactic similarities with (lower dendrogram) and without SVD (upper dendrogram).80
