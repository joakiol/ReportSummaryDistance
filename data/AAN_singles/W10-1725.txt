Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 172?176,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsUPV-PRHLT English?Spanish system for WMT10Germa?n Sanchis-Trilles and Jesu?s Andre?s-Ferrer and Guillem Gasco?Jesu?s Gonza?lez-Rubio and Pascual Mart?
?nez-Go?mez and Martha-Alicia RochaJoan-Andreu Sa?nchez and Francisco CasacubertaInstituto Tecnolo?gico de Informa?ticaDepartamento de Sistemas Informa?ticos y Computacio?nUniversidad Polite?cnica de Valencia{gsanchis|jandres|fcn}@dsic.upv.es{ggasco|jegonzalez|pmartinez}@dsic.upv.es{mrocha|jandreu}@dsic.upv.esAbstractIn this paper, the system submitted bythe PRHLT group for the Fifth Work-shop on Statistical Machine Translation ofACL2010 is presented.
On this evalua-tion campaign, we have worked on theEnglish?Spanish language pair, puttingspecial emphasis on two problems derivedfrom the large amount of data available.The first one, how to optimize the use ofthe monolingual data within the languagemodel, and the second one, how to makegood use of all the bilingual data providedwithout making use of unnecessary com-putational resources.1 IntroductionFor this year?s translation shared task, the Pat-tern Recognition and Human Language Technolo-gies (PRHLT) research group of the UniversidadPolite?cnica de Valencia submitted runs for theEnglish?Spanish translation task.
In this paper, wereport the configuration of such a system, togetherwith preliminary experiments performed to estab-lish the final setup.As in 2009, the central focus of the Shared Taskis on Domain Adaptation, where a system typi-cally trained using out-of-domain data is adjustedto translate news commentaries.For the preliminary experiments, we used only asmall amount of the largest available bilingual cor-pus, i.e.
the United Nations corpus, by includinginto our system only those sentences which wereconsidered similar.Language model interpolation using a develop-ment set was explored in this work, together witha technique to cope with the problem of ?out ofvocabulary words?.Finally, a reordering constraint using walls andzones was used in order to improve the perfor-mance of the submitted system.In the final evaluation, our system was rankedfifth, considering only primary runs.2 Language Model interpolationNowadays, it is quite common to have very largeamounts of monolingual data available from sev-eral different domains.
Despite of this fact, inmost of the cases we are only interested in trans-lating from one specific domain, as is the case inthis year?s shared task, where the provided mono-lingual training data belonged to European parlia-mentary proceedings, news related domains, andthe United Nations corpus, which consists of datacrawled from the web.Although the most obvious thing to do is to con-catenate all the data available and train a singlelanguage model on the whole data, we also inves-tigated a ?smarter?
use of such data, by trainingone language model for each of the available cor-pora.3 Similar sentences selectionCurrently, it is common to of huge bilingual cor-pora for SMT.
For some common language pairs,corpora of millions of parallel sentences are avail-able.
In some of the cases big corpora are usedas out-of-domain corpora.
For example, in thecase of the shared task, we try to translate a newstext using a small in-domain bilingual news corpus(News Commentary) and two big out-of-domaincorpora: Europarl and United Nations.Europarl is a medium size corpus and can becompletely incorporated to the training set.
How-ever, the use of the UN corpus requires a big com-putational effort.
In order to alleviate this prob-lem, we have chosen only those bilingual sen-tences from the United Nations that are similar tothe in-domain corpus sentences.
As a similaritymeasure, we have chosen the alignment score.Alignment scores have already been used as a172filter for noisy corpora (Khadivi and Ney, 2005).We trained an IBM model 4 using GIZA++ (Ochand Ney, 2003) with the in-domain corpus andcomputed the alignment scores over the UnitedNations sentences.
We assume that the alignmentscore is a good measure of similarity.An important factor in the alignment score isthe length of the sentences, so we clustered thebilingual sentences in groups with the same sum ofsource and target language sentence sizes.
In eachof the groups, the higher the alignment score is,the more similar the sentence is to the in-domaincorpus sentences.
Hence, we computed the aver-age alignment score for each one of the clustersobtained for the corpus considered in-domain (i.e.the News-Commentary corpus).
This being done,we assessed the similarity of a given sentence bycomputing the probability of such sentence withrespect to the alignment model of the in-domaincorpus, and established the following similaritylevels:?
Level 1: Sentences with an alignment scoreequal or higher than the in-domain average.?
Level 2: Sentences with an alignment scoreequal or higher than the in-domain average,minus one standard deviation.?
Level 3: Sentences with an alignment scoreequal or higher than the in-domain average,minus two standard deviations.Naturally, such similarity levels establish parti-tions of the out-of-domain corpus.
Then, such par-titions were included into the training set used forbuilding the SMT system, and re-built the com-plete system from scratch.4 Out of Vocabulary RecoveryAs stated in the previous section, in order to avoida big computational effort, we do not use thewhole United Nations corpus to train the trans-lation system.
Out of vocabulary words are acommon problem for machine translation systems.When translating the test set, there are test wordsthat are not in the reduced training set (out of vo-cabulary words).
Some of those out of vocabularywords are present in the sentences discarded fromthe United Nations Corpus.
Thus, recovering thediscarded sentences with out of vocabulary wordsis needed.The out of vocabulary words recovery methodis simple: the out of vocabulary words from thetest, when taking into account the reduced trainingset, are obtained and then discarded sentences thatcontain at least one of them are retrieved.
Then,those sentences are added to the reduced trainingset.Finally, alignments with the resulting trainingset were computed and the usual training proce-dure for phrase-based systems was performed.5 Walls and zonesIn translation, as in other linguistics areas, punc-tuation marks are essential as they help to un-derstand the intention of a message and organisethe ideas to avoid ambiguity.
They also indicatepauses, hierarchies and emphasis.In our system, punctuation marks have beentaken into account during decoding.
Traditionally,in SMT punctuation marks are treated as wordsand this has undesirable effects (Koehn and Had-dow, 2009).
For example, commas have a highprobability of occurrence and many possible trans-lations are generated.
Most of them are not consis-tent across languages.
This introduces too muchnoise to the phrase tables.
(Koehn and Haddow, 2009) established aframework to specify reordering constraints withwalls and zones, where commas and endof sentence are not mixed with various clauses.Gains between 0.1 and 0.2 of BLEU are reported.Specifying zones and walls with XML tagsin input sentences allows us to identify structuredfragments that the Moses decoder uses with thefollowing restrictions:1.
If a <zone> tag is detected, then a blockis identified and must be translated until a</zone> tag is found.
The text between tags<zone> and </zone> is identified and trans-lated as a block.2.
If the decoder detects a <wall/> tag, the textis divided into a prefix and suffix and Mosesmust translate all the words of the prefix be-fore the suffix.3.
If both zones and walls are specified,then local walls are considered wherethe constraint 2 applies only to the area es-tablished by zones.173corpus Language |S| |W | |V |Europarl v5Spanish1272K28M 154KEnglish 27M 106KNCSpanish81K1.8M 54KEnglish 1.6M 39KTable 1: Main figures of the Europarl v5 andNews-Commentary (NC) corpora.
K/M standsfor thousands/millions.
|S| is the number of sen-tences, |W | the number of running words, and |V |the vocabulary size.
Statistics are reported on thetokenised and lowercased corpora.We used quotation marks, parentheses, bracketsand dashes as zone delimiters.
Quotation marks(when appearing once in the sentence), com-mas, colons, semicolons, exclamation and ques-tion marks and periods are used as wall delimiters.The use of zone delimiters do not alter the per-formance.
When using walls, a gain of 0.1BLEU is obtained in our best model.6 Experiments6.1 Experimental setupFor building our SMT systems, the open-sourceSMT toolkit Moses (Koehn et al, 2007) was usedin its standard setup.
The decoder includes a log-linear model comprising a phrase-based transla-tion model, a language model, a lexicalised dis-tortion model and word and phrase penalties.
Theweights of the log-linear interpolation were opti-mised by means of MERT (Och, 2003).
In addi-tion, a 5-gram LM with Kneser-Ney (Kneser andNey, 1995) smoothing and interpolation was builtby means of the SRILM (Stolcke, 2002) toolkit.For building our baseline system, the News-Commentary and Europarl v5 (Koehn, 2005) datawere employed, with maximum sentence lengthset to 40 in the case of the data used to build thetranslation models, and without restriction in thecase of the LM.
Statistics of the bilingual data canbe seen in Table 1.In all the experiments reported, MERT was runon the 2008 test set, whereas the test set 2009 wasconsidered as test set as such.
In addition, all theexperiments described below were performed inlowercase and tokenised conditions.
For the fi-nal run, the detokenisation and recasing was per-formed according to the technique described in theWorkshop baseline description.corpus |S| |W | |V |Europarl 1822K 51M 172KNC 108K 3M 68KUN 6.2M 214M 411KNews 3.9M 107M 512KTable 2: Main figures of the Spanish resourcesprovided: Europarl v5, News-Commentary (NC),United Nations (UN) and News-shuffled (News).6.2 Language Model interpolationThe final system submitted to the shared taskincluded a linear interpolation of four languagemodels, one for each of the monolingual resourcesavailable for Spanish (see Table 2).
The resultscan be seen in Table 3.
As a first experiment, onlythe in-domain corpus, i.e.
the News-Commentarydata (NC data) was used for building the LM.Then, all the available monolingual Spanish datawas included into a single LM, by concatenat-ing all the data together (pooled).
Next, ininterpolated, one LM for each one of theprovided monolingual resources was trained, andthen they were linearly interpolated so as to min-imise the perplexity of the 2008 test set, and fedsuch interpolation to the SMT system.
We foundout that weights were distributed quite unevenly,since the News-shuffled LM received a weight of0.67, whereas the other three corpora received aweight of 0.11 each.
It must be noted that eventhe in-domain LM received a weight of 0.11 (lessthan the News-shuffled LM).
The reason for thismight be that, although the in-domain LM shouldbe more appropriate and should receive a higherweight, the News-shuffled corpus is also news re-lated (hence not really out-of-domain), but muchlarger.
For this reason, the result of using onlysuch LM (News) was also analysed.
As expected,the translation quality dropped slightly.
Never-theless, since the differences are not statisticallysignificant, we used the News-shuffled LM for in-ternal development purposes, and the interpolatedLM only whenever an improvement prooved to beuseful.6.3 Including UN dataWe analysed the impact of the selection techniquedetailed in Section 3.
In this case, the LM usedwas the interpolated LM described in the previoussection.
The result can be seen in Table 4.
Asit can be seen, translation quality as measured by174Table 3: Effect of considering different LMsLM used BLEUNC data 21.86pooled 23.53interpolated 24.97news 24.79BLEU improves constantly as the number of sen-tences selected increases.
However, further sen-tences were not included for computational rea-sons.In the same table, we also report the effect ofadding the UN sentences selected by our out-of-vocabulary technique described in Section 4.
Inthis context, it should be noted that MERT wasnot rerun once such sentences had been selected,since such sentences are related with the test set,and not with the development set on which MERTis run.Table 4: Effect of including selected sentencessystem BLEUbaseline 24.97+ oovs 25.08+ Level 1 24.98+ Level 2 25.07+ Level 3 25.136.4 Final systemSince the News-shuffled, UN and Europarl cor-pora are large corpora, a new LM interpolationwas estimated by using a 6-gram LM on each oneof these corpora, obtaining a gain of 0.17 BLEUpoints by doing so.
Further increments in the n-gram order did not show further improvements.In addition, preliminary experimentation re-vealed that the use of walls, as described inSection 5, also provided slight improvements, al-though using zones or combining both did notprove to improve further.
Hence, only wallswere included into the final system.Lastly, the final system submitted to the Work-shop was the result of combining all the techniquesdescribed above.
Such combination yielded a fi-nal BLEU score of 25.31 on the 2009 test set, and28.76 BLEU score on the 2010 test set, both intokenised and lowercased conditions.7 Conclusions and future workIn this paper, the SMT system presented by theUPV-PRHLT team for WMT 2010 has been de-scribed.
Specifically, preliminary results abouthow to make use of larger data collections fortranslating more focused test sets have been pre-sented.In this context, there are still some things whichneed a deeper investigation, since the results pre-sented here give only a small insight about the po-tential of the similar sentence selection techniquedescribed.However, a deeper analysis is needed in orderto assess the potential of such technique and otherstrategies should be implemented to explore newkids of reordering constraints.AcknowledgmentsThis paper is based upon work supported bythe EC (FEDER/FSE) and the Spanish MICINNunder the MIPRCV ?Consolider Ingenio 2010?program (CSD2007-00018),iTrans2 (TIN2009-14511) project, and the FPU scholarship AP2006-00691.
This work was also supported by the Span-ish MITyC under the erudito.com (TSI-020110-2009-439) project and by the Generalitat Valen-ciana under grant Prometeo/2009/014 and schol-arships BFPI/2007/117 and ACIF/2010/226 andby the Mexican government under the PROMEP-DGEST program.ReferencesShahram Khadivi and Hermann Ney.
2005.
Automaticfiltering of bilingual corpora for statistical machinetranslation.
In Natural Language Processing and In-formation Systems, 10th Int.
Conf.
on Applicationsof Natural Language to Information Systems, vol-ume 3513 of Lecture Notes in Computer Science,pages 263?274, Alicante, Spain, June.
Springer.R.
Kneser and H. Ney.
1995.
Improved backing-offfor m-gram language modeling.
IEEE InternationalConference on Acoustics, Speech and Signal Pro-cessing, II:181?184, May.Philipp Koehn and Barry Haddow.
2009.
Edinburgh?ssubmission to all tracks of the WMT2009 sharedtask with reordering and speed improvements toMoses.
In The 4th EACL Workshop on StatisticalMachine Translation, ACL, pages 160?164, Athens,Greece, March.
Springer.P.
Koehn et al 2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
In Proceedings of175the ACL Demo and Poster Sessions, pages 177?180,Prague, Czech Republic.P.
Koehn.
2005.
Europarl: A parallel corpus for statis-tical machine translation.
In MT Summit.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.F.J.
Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofACL, pages 160?167, Sapporo, Japan.A.
Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proc.
of ICSLP?02, pages 901?904, September.176
