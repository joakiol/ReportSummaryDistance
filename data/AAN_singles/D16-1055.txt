Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 573?584,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsLearning to Translate for Multilingual Question AnsweringFerhan TureComcast Labs?1110 Vermont Ave NW Ste 600Washington, DC, 20005 USAferhan ture@cable.comcast.comElizabeth BoscheeRaytheon BBN Technologies10 Moulton StCambridge, MA, 02138 USAeboschee@bbn.comAbstractIn multilingual question answering, either thequestion needs to be translated into the docu-ment language, or vice versa.
In addition todirection, there are multiple methods to per-form the translation, four of which we explorein this paper: word-based, 10-best, context-based, and grammar-based.
We build a fea-ture for each combination of translation direc-tion and method, and train a model that learnsoptimal feature weights.
On a large forumdataset consisting of posts in English, Arabic,and Chinese, our novel learn-to-translate ap-proach was more effective than a strong base-line (p < 0.05): translating all text into En-glish, then training a classifier based only onEnglish (original or translated) text.1 IntroductionQuestion answering (QA) is a specific form of theinformation retrieval (IR) task, where the goal isto find relevant well-formed answers to a posedquestion.
Most QA pipelines consist of three mainstages: (a) preprocessing the question and collec-tion, (b) retrieval of candidate answers in the col-lection, and (c) ranking answers with respect to theirrelevance to the question and return the top N an-swers.
The types of questions can range from fac-toid (e.g., ?What is the capital of France??)
to causal(e.g., ?Why are trees green??
), and opinion ques-tions (e.g., ?Should USA lower the drinking age??
).The most common approach to multilingual QA(MLQA) has been to translate all content into its?This work was completed while author was an employeeof Raytheon BBN Technologies.most probable English translation via machine trans-lation (MT) systems.
This strong baseline, which werefer to as one-best MT (1MT), has been successfulin prior work (Hartrumpf et al, 2009; Lin and Kuo,2010; Shima and Mitamura, 2010).
However, re-cent advances in cross-lingual IR (CLIR) show thatone can do better by representing the translationspace as a probability distribution (Ture and Lin,2014).
In addition, MT systems perform substan-tially worse with user-generated text, such as webforums (Van der Wees et al, 2015), which provideextra motivation to consider alternative translationapproaches for higher recall.
To our knowledge, ithas yet to be shown whether these recent advance-ments in CLIR transfer to MLQA.We introduce a novel answer ranking approach forMLQA (i.e., Learning to Translate or L2T), a modelthat learns the optimal translation of question and/orcandidate answer, based on how well it discrimi-nates between good and bad answers.
We achievethis by introducing a set of features that encapsulatelexical and semantic similarities between a questionand a candidate answer through various translationstrategies (Section 3.1).
The model then learns fea-ture weights for each combination of translation di-rection and method, through a discriminative train-ing process (Section 3.2).
Once a model is trained,it can be used for MLQA, by sorting each candidateanswer in the collection by model score.
Instead oflearning a single model to score candidate answersin any language, it might be meaningful to train aseparate model that can learn to discriminate be-tween good and bad answers in each language.
Thiscan let each model learn feature weights custom to573the language, therefore allowing a more fine-grainedranking (Section 3.4).
We call this alternative ap-proach Learning to Custom Translate (L2CT).Experiments on the DARPA Broad OperationalLanguage Technologies (BOLT) IR task1 confirmthat L2T yields statistically significant improve-ments over a strong baseline (p < 0.05), in three outof four experiments.
L2CT outperformed the base-line as well, but was not more effective than L2T.2 Related WorkFor the last decade or so, research in QA has mostlybeen driven by annual evaluation campaigns likeTREC,2 CLEF,3 and NTCIR.4 Most earlier work re-lied on either rule-based approaches where a set ofrules were manually crafted for each type of ques-tion, or IR-like approaches where each pair of ques-tion and candidate answer was scored using retrievalfunctions (e.g., BM25 (Robertson et al, 2004)).
Onthe other hand, training a classifier for ranking can-didate answers allows the exploitation of variousfeatures extracted from the question, candidate an-swer, and surrounding context (Madnani et al, 2007;Zhang et al, 2007).
In fact, an explicit comparisonat 2007 TREC confirmed the superiority of machinelearning-based (ML-based) approaches (F-measure35.9% vs 38.7%) (Zhang et al, 2007).
Learning-to-rank approaches have also been applied to QA suc-cessfully (Agarwal et al, 2012).Previous ML-based approaches have introduceduseful features from many aspects of natural lan-guage, including lexical (Brill et al, 2001; At-tardi et al, 2001), syntactic (Alfonseca et al, 2001;Katz et al, 2005), semantic (Cui et al, 2005;Katz et al, 2005; Alfonseca et al, 2001; Hovy etal., 2001), and discourse features, such as coref-erence resolution (Morton, 1999), or identifyingtemporal/spatial references (Saquete et al, 2005;Harabagiu and Bejan, 2005), which are especiallyuseful for ?why?
and ?how?
questions (Kolomiyetsand Moens, 2011).
Additionally, semantic role la-beling and dependency trees are other forms ofsemantic analysis used widely in NLP applica-tions (Shen and Lapata, 2007; Cui et al, 2005).1http://www.darpa.mil/Our_Work/I2O/Programs2http://trec.nist.gov3http://www.clef-initiative.eu4http://research.nii.ac.jp/ntcir/index.htmlWhen dealing with multilingual collections, mostprior approaches translate all text into English be-forehand, then treat the task as monolingual retrieval(previously referred to as 1MT).
At recent evalua-tion campaigns like CLEF and NTCIR,5 almost allteams simply obtained the one-best question trans-lation, treating some online MT system as a blackbox (Adafre and van Genabith, 2009; Hartrumpf etal., 2009; Martinez-Gonzalez et al, 2009; Lin andKuo, 2010; Shima and Mitamura, 2010), with fewnotable exceptions that took term importance (Renet al, 2010), or semantics (Munoz-Terol et al, 2009)into account.
Even for non-factoid MLQA, mostprior work does not focus on the translation com-ponent (Luo et al, 2013; Chaturvedi et al, 2014).Contributions.
Ture and Lin described three meth-ods for translating queries into the collection lan-guage in a probabilistic manner, improving docu-ment retrieval effectiveness over a one-best transla-tion approach (2014).
Extending this idea to MLQAappears as a logical next step, yet most prior workrelies solely on the one-best translation of questionsor answers (Ko et al, 2010b; Garc?
?a-Cumbreras etal., 2012; Chaturvedi et al, 2014), or selects thebest translation out of few options (Sacaleanu et al,2008; Mitamura et al, 2006).
Mehdad et al re-ported improvements by including the top ten trans-lations (instead of the single best) and computinga distance-based entailment score with each (2010).While Espla-Gomis et al argue that using MT as ablack box is more convenient (and modular) (2012),there are potential benefits from a closer integra-tion between statistical MT and multilingual re-trieval (Nie, 2010).
To the best of our knowledge,there is no prior work in the literature, where theoptimal query and/or answer translation is learnedvia machine learning.
This is our main contribu-tion, with which we outperform the state of the art.In addition to learning the optimal translation, welearn the optimal subset of the training data for agiven task, where the criteria of whether we includea certain data instance is based on either the sourcelanguage of the sentence, or the language in whichthe sentence was annotated.
Training data selectionstrategies have not been studied extensively in the5Most recent MLQA tracks were in 2008 (CLEF) and 2010(NTCIR).574QA literature, therefore the effectiveness of our sim-ple language-related criteria can provide useful in-sights to the community.When there are multiple independent approachesfor ranking question-answer pairs, it is requiredto perform a post-retrieval merge: each approachgenerates a ranked list of answers, which are thenmerged into a final ranked list.
This type of sys-tem combination approach has been applied to var-ious settings in QA research.
Merging at thedocument-level is common in IR systems (e.g., (Tsaiet al, 2008)), and has shown to improve multilin-gual QA performance as well (Garc?
?a-Cumbreraset al, 2012).
Many QA systems combine an-swers obtained by different variants of the under-lying model (e.g., (Brill et al, 2001) for monolin-gual, (Ko et al, 2010a; Ko et al, 2010b) for multi-lingual QA).
We are not aware, however, of any priorwork that has explored the merging of answers thatare generated by language-specific ranking mod-els.
Although this does not show increased effec-tiveness in our experiments, we believe that it bringsa new perspective to the problem.3 ApproachOur work is focused on a specific stage of theQA pipeline, namely answer ranking: Given anatural-language question q and k candidate answerss1, .
.
.
, sk, we score each answer in terms of its rel-evance to q.
In our case, candidate answers are sen-tences extracted from all documents retrieved in theprevious stage of the pipeline (using Indri (Metzlerand Croft, 2005)).
Hereafter, sentence and answermight be used interchangeably.While our approach is not language-specific, weassume (for simplicity) that questions are in English,whereas sentences are in either English, Arabic, orChinese.
Non-English answers are translated backinto English before returning to user.Our approach is not limited to any question type,factoid or non-factoid.
Our main motivation is toprovide good QA quality on any multilingual Webcollection.
This entails finding answers to questionswhere there is no single answer, and for which hu-man agreement is low.
We aim to build a systemthat can successfully retrieve relevant informationfrom open-domain and informal-language content.In this scenario, two assumptions made by many ofthe prior approaches fail:1) We can accurately classify questions via tem-plate patterns (Chaturvedi et al argue that this doesnot hold for non-factoid questions (2014))2) We can accurately determine the relevance ofan answer, based on its automatic translation intoEnglish (Wees et al show how recall decreaseswhen translating user-generated text (2015))To avoid these assumptions, we opted for a moreadaptable approach, in which question-answer rele-vance is modeled as a function of features, intendedto capture the relationship between the question andsentence text.
Also, instead of relying solely on asingle potentially incorrect English translation, weincrease our chances of a hit by translating both thequestion and the candidate answer, using four dif-ferent translation methods.
Our main features, de-scribed throughout this section, are based on lexicalsimilarity computed using these translations.
Theclassifier is trained on a large number of question-answer pairs, each labeled by a human annotatorwith a binary relevance label.63.1 RepresentationIn MLQA, since questions and answers are in differ-ent languages, most approaches translate both intoan intermediary language (usually English).
Due tothe error-prone nature of MT, valuable informationoften gets ?lost in translation?.
These errors are es-pecially noticeable when translating informal text orless-studied languages (Van der Wees et al, 2015).Translation Direction.
We perform a two-waytranslation to better retain the original meaning:in addition to translating each non-English sen-tence into English, we also translate the Englishquestions into Arabic and Chinese (using multipletranslation methods, described below).
For eachquestion-answer pair, we have two ?views?
: com-paring translated question to the original sentence(i.e., collection-language (CL) view); and compar-ing original question to the translated sentence (i.e.,question-language (QL) view).Translation Method.
When translating text for re-trieval tasks like QA, including a variety of alterna-6Annotators score each answer from 1 to 5.
We label anyscore of 3 or higher as relevant.575tive translations is as important as finding the mostaccurate translation, especially for non-factoid ques-tions, where capturing (potentially multiple) under-lying topics is essential.
Recent work in cross-language IR (CLIR) has shown that incorporatingprobabilities from the internal representations of anMT system to ?translate?
the question can accom-plish this, outperforming standard one-best transla-tion (Ture and Lin, 2014).
We hypothesize that theseimprovements transfer to multilingual QA as well.In addition to translation directions, we exploredfour translation methods for converting the Englishquestion into a probabilistic representation (in Ara-bic and Chinese).
Each method builds a probabilitydistribution for every question word, expressing thetranslation space in the collection language.
Moredetails of first three methods can be found in (Tureand Lin, 2014), while fourth method is a novel querytranslation method adapted from the neural networktranslation model described in (Devlin et al, 2014).Word: In MT, a word alignment is a many-to-many mapping between source- and target-languagewords, learned without supervision, at the beginningof the training pipeline (Och, 2003).
These align-ments can be converted into word translation proba-bilities for CLIR (Darwish and Oard, 2003).For example, in an English-Arabic parallel corpus,if an English word appears m times in total and isaligned to a certain Arabic word k times, we assigna probability of km for this translation.
This simpleidea has performed greatly in IR for generating aprobability distribution for query word translations.Grammar: Probabilities are derived from a syn-chronous context-free grammar, which is a typicaltranslation model found in MT systems (Ture andLin, 2014).
The grammar contains rules r thatfollow the form ?|?|A|`(r), stating that source-language word ?
can be translated into target-language word ?
with an associated likelihood value`(r) (A represents word alignments).
For each rule rthat applies to the question, we identify each sourceword sj .
From the word alignment information in-cluded in the rule, we can find all target words thatsj is aligned to.
By processing all the rules to ac-cumulate likelihood values, we construct translationprobabilities for each word in the question.10-best: Statistical MT systems retrieve a ranked listof translations, not a single best.
Ture and Lin ex-ploited this to obtain word translation probabilitiesfrom the top 10 translations of the question (2014).For each question word w, we can extract whichgrammar rules were used to produce the translation?
once we have the rules, word alignments allow usto find all target-language words that w translatesinto.
By doing this for each question translation, weconstruct a probability distribution that defines thetranslation space of each question word.Context: Neural network-based MT models learncontext-dependent word translation probabilities ?the probability of a target word is dependent on thesource word it aligns to, as well as a 5-word windowof context (Devlin et al, 2014).
For example, if theSpanish word ?placer?
is aligned to the English word?pleasure?, the model will not only learn from thisword-to-word alignment but also consider the sourcesentence context (e.g., ?Fue un placer conocerte ytenerte unos meses.?).
However, since short ques-tions might lack full sentence context, our modelshould have the flexibility to translate under par-tial or no context.
Instead of training the NN-basetranslation model on full, well-formed sentences, wecustom-fit it for question translation: words in thecontext window are randomly masked by replacingit with a special filler token <F>.
This teaches themodel how to accurately translate with full, partialcontext, or no context.
For the above example, wegenerate partial contexts such as ?fue un placer <F>y?
or ?<F> <F> placer conocerte y?.
Since there aremany possibilities, if the context window is large,we randomly sample a few of the possibilities (e.g.,4 out of 9) per training word.In Figure 1, we display the probabilistic structureproduced the grammar-based translation method,when implemented as described above.
Each En-glish word in the question is translated into a prob-abilistic structure, consisting of Chinese words andcorresponding probabilities that represent how muchweight the method decides to put on that specificword.
Similar structures are learned with the otherthree translation methods.We are not aware of any other MLQA approachthat represents the question-answer pair based ontheir probabilistic translation space.576child:'['0.32'0.25'0.21'0.15'...']'labor:'['0.36'0.26'0.17'0.13'...']'Africa:'['0.89'	0.02'0.02'0.01'...']'non#$ development$of$child$labor$ child$$$$$$$$$$$$$$$$$$$$$$$$children$$$$$$$$$$$$$$$$$$$$child$labor$$$$$$$$$$$$$$$$$$$$$$$$$labor$$$$$$$$$$$$$$$$$$$$$$$$labor$force$Africa$ South$Africa$Figure 1: Probabilistic grammar-based translation of examplequestion.
The example question ?Tell me about child labor inAfrica?
is simplified by our preprocessing engine to ?child laborafrica?.3.2 FeaturesGiven two different translation directions (CL andQL), and four different translation methods (Word,Grammar, 10-best, Context), our strategy is to lever-age a machine learning process to determine howhelpful each signal is with respect to the end task.For this, we introduced separate question-answersimilarity features based on each combination oftranslation direction and method.Collection-language Features.
In order to computea single real-valued vector to represent the questionin the collection language (LexCL), we start withthe probabilistic structure representing the questiontranslation (e.g., Figure 1 is one such structure whenthe translation method is grammar-based).
Foreach word in the collection-language vocabulary, wecompute a weight by averaging its probability acrossthe terms in the probabilistic structure.vqgrammar(w) = avgiPr(w|qi) (1)where w is a non-Engish word and Pr(w|qi) is theprobability of w in the probability distribution cor-responding to the ith query term.Figure 2 shows the real-valued vector computedbased on the probabilistic question translation inFigure 1.
The Chinese word translated as ?child la-bor?
has a weight of 0.32, 0.36, and 0 in the proba-bility distributions of the three query terms, respec-tively.
Averaging these three values results in the fi-nal weight of 0.23 in vqgrammar in Figure 2.
Noticethat these weights are normalized by construction.Similarly, a candidate answer s in Chinese is rep-resented by normalized word frequencies:vs(w) =freq(w|s)?w?
freq(w?|s)(2)Given the two vectors, we compute the co-sine similarity.
Same process is repeated for theFeature Question Sentence Featurecategory repr.
(vq?)
repr.
(vs?)
ValueLexCLvqword vsvq10best vscosine(vq?
, vs?
)vqcontext vsvqgrammar vsLexQL vq vs1bestTable 1: List of features used in L2T, and how the values arecomputed from vector representations.other three translation methods.
The four lexicalcollection-language similarity features are collec-tively called LexCL.vqgrammar:([(0.30(0.23(0.08(0.09(?
(](s:(	vs:([(2.0((1.0(1.0((1.0(?
(](Figure 2: Vector representation of grammar-translated question(qgrammar) and sentence (s).Question-language Features.
As mentioned be-fore, we also obtain a similarity value by translatingthe sentence (s1best) and computing the cosine sim-ilarity with the original question (q).
vq and vs1bestare computed using Equation 2.
Although it is pos-sible to translate the sentence into English using thesame four methods, we only used the one-best trans-lation due to the computational cost.
Hence, we haveonly one lexical similarity feature in the QL view(call LexQL).The computation process for the five lexical sim-ilarity features is summarized in Table 1.
Af-ter computation, feature weights are learned via amaximum-entropy model.7 Although not includedin the figure or table, we also include the same setof features from the sentence preceding the answer(within the corresponding forum post), in order torepresent the larger discourse.3.3 Data SelectionIn order to train a machine learning model with ournovel features, we need positive and negative exam-ples of question-answer pairs (i.e., (q, s)).
For this,for each training question, our approach is to hire7Support vector machines yielded worse results.577human annotators to label sentences retrieved fromthe non-English collections used in our evaluation.It is possible to label the sentences in the source lan-guage (i.e., Arabic or Chinese) or in the questionlanguage (i.e., translated into English).
In this sec-tion, we explore the question of whether it is usefulto distinguish between these two independently cre-ated labels, and whether this redundancy can be usedto improve the machine learning process.We hypothesize two reasons why selecting train-ing data based on language might benefit MLQA:i) The translation of non-English candidate an-swers might lack in quality, so annotators are likelyto judge some relevant answers as non-relevant.Hence, training a classifier on this data might leadto a tendency to favor English answers.ii) For the question-answer pairs that were annotatedin both languages, we can remove noisy (i.e., la-beled inconsistently by annotators) instances fromthe training set.The question of annotation is an unavoidable partof evaluation of MLQA systems, so finding the opti-mal subset for training is a relevant problem.
In or-der to explore further, we generated six subsets withrespect to (a) the original language of the answer, or(b) the language of annotation (i.e., based on origi-nal text or its English translation):en: Sentences from the English corpus.ar/ch: Sentences from the Arabic / Chinese corpus(regardless of how it was judged).consist: All sentences except those that were judgedinconsistently.src+: Sentences judged only in original text, orjudged in both consistently.en+: Sentences that are either judged only in En-glish, or judged in both original and English transla-tion consistently.all: All sentences.These subsets were determined based on linguis-tically motivated heuristics, but choosing the mostsuitable one (for a given task) is done via machinelearning (see Section 4).3.4 Language-specific RankingScoring Arabic sentences with respect to a questionis inherently different than scoring English (or Chi-nese) sentences.
The quality of resources, grammar,etc., as well as other internal dynamics might differgreatly across languages.
We hypothesize that thereis no one-size-fits-all model, so the parameters thatwork best for English retrieval might not be as usefulwhen scoring sentences in Arabic, and/or Chinese.Our proposed solution is to apply a separate clas-sifier, custom-tuned to each collection, and retrievethree single-language ranked lists (i.e., in English,Arabic, and Chinese).
In addition to comparing eachcustom-tuned, language-specific classifier to a sin-gle, language-independent one, we also use this ideato propose an approach for MLQA:L2CT(n) Retrieve answers from each language us-ing separate classifiers (call these lists English-only,Arabic-only, and Chinese-only), take the best an-swers from each language, then merge them into amixed-language set of n answers.We compare this to the standard approach:L2T(n) Retrieve up to n mixed-language answersusing a single classifier.Four heuristics were explored for merging lists inthe L2CT approach.8 Two common approaches areuniform and alternate merging (Savoy, 2004):Uniform: A straightforward merge can be achievedby using the classifier scores (i.e., probability of an-swer relevance, given question) to sort all answers,across all languages, and include the top n in the fi-nal list of answers.
Classifier scores are normalizedinto the [0,1] range for comparability.Alternate: We alternate between the lists, pickingone answer at a time from each, stopping when thelimit n has been reached.Since answers are expected in English, there is anatural preference for answers that were originallywritten English, avoiding noisy text due to transla-tion errors.
However, it is also important not to re-strict answers entirely to English sources, since thatwould defeat the purpose of searching in a multi-lingual collection.
We implemented the followingmethods to account for language preferences:English first: We keep all sufficiently-confident (i.e.,normalized score above a fixed threshold) answersfrom the English-only list first, and start includinganswers from Arabic- and Chinese-only lists only ifthe limit of n answers has not been reached.8In addition to these heuristics, the optimal merge could belearned from training data, as a ?learning to rank?
problem.
Thisis out of the scope of this paper, but we plan to explore the ideain the future.578Weighted: Similar to Uniform, but we weight thenormalized scores before sorting.
The optimalweights can be learned by using a grid-search pro-cedure and a cross-validation split.4 EvaluationIn order to perform controlled experiments and gainmore insights, we split our evaluation into fourseparate tasks: three tasks focus on retrieving an-swers from posts written in a specified language(English-only, Arabic-only, or Chinese-only) 9, andthe last task is not restricted to any language (Mixed-language).
All experiments were conducted on theDARPA BOLT-IR task.
The collection consists of12.6M Arabic, 7.5M Chinese, and 9.6M EnglishWeb forum posts.
All runs use a set of 45 non-factoid (mostly opinion and causal) English ques-tions, from a range of topics.
All questions and fo-rum posts were processed with an information ex-traction (IE) toolkit (Boschee et al, 2005), whichperforms sentence-splitting, named entity recogni-tion, coreference resolution, parsing, and part-of-speech tagging.All non-English posts were translated into En-glish (one-best only), and all questions were trans-lated into Arabic and Chinese (probabilistic transla-tion methods from Section 3.1).
For all experiments,we used the same state-of-the-art English?Arabic(En-Ar) and English?Chinese (En-Ch) MT sys-tems (Devlin et al, 2014).
Models were trainedon parallel corpora from NIST OpenMT 2012, inaddition to parallel forum data collected as part ofthe BOLT program (10M En-Ar words; 30M En-Ch words).
Word alignments were learned withGIZA++ (Och and Ney, 2003) (five iterations ofIBM Models 1?4 and HMM).After all preprocessing, features were computedusing the original post and question text, and theirtranslations.
Training data were created by havingannotators label all sentences of the top 200 docu-ments retrieved by Indri from each collection (foreach question).
Due to the nature of retrieval tasks,training data usually contains an unbalanced portionof negative examples.
Hence, we split the data intobalanced subsets (each sharing the same set of pos-itively labeled data) and train multiple classifiers,9Shortened as Eng, Arz, and Cmn, respectively.then take a majority vote when predicting.For testing, we froze the set of candidate answersand applied the trained classifier to each question-answer pair, generating a ranked list of answers foreach question.
This ranked list was evaluated by av-erage precision (AP).10 Due to the size and redun-dancy of the collections, we sometimes end up withover 1000 known relevant answers for a question.So it is neither reasonable nor meaningful to com-pute AP until we reach 100% recall (e.g., 11-pointAP) for these cases.
Instead, we computed AP-k, byaccumulating precision values at every relevant an-swer until we get k relevant answers.11 In order toprovide a single metric for the test set, it is commonto report the mean average precision (MAP), whichin this case is the average of the AP-k values acrossall questions.Baseline.
As described earlier, the baseline systemcomputes similarity between question text and theone-best translation of the candidate answer (we runthe sentence through our state-of-the-art MT sys-tem).
After translation, we compute similarity viascoring the match between the parse of the ques-tion text and the parse of the candidate answer, us-ing our finely-tuned IE toolkit [reference removedfor anonymization].
This results in three differentsimilarity features: matching the tree node similar-ity, edge similarity, and full tree similarity.
Fea-ture weights are then learned by training this clas-sifier discriminatively on the training data describedabove.
This already performs competitively, outper-forming the simpler baseline where we compute asingle similarity score between question and trans-lated text, and matching the performance of thesystem by Chaturvedi et al on the BOLT evalua-tion (2014).
Baseline MAP values are reported onthe leftmost column of Table 2.Data effect.
In the baseline approach, we do notperform any data selection, and use all availabledata for training the classifier.
In order to test ourhypothesis that selecting a linguistically-motivatedsubset of the training data might help, we used 10-fold cross-validation to choose the optimal data set10Many other metrics (e.g., NDCG, R-precision) were ex-plored during BOLT, and results were very similar.11k was fixed to 20 in our evaluation, although we verifiedthat conclusions do not change with varying k.579(among seven options described in Section 3.3).
Re-sults indicate that including English or Arabic sen-tences when training a classifier for Chinese-onlyQA is a bad idea, since effectiveness increases whenrestricted to Chinese sentences (lang=ch).
On theother hand, for the remaining three tasks, the mosteffective training data set is annot=en+consist.These selections are consistent across all ten folds,and the difference is statistically significant for allbut Arabic-only.
The second column in Table 2 dis-plays the MAP achieved when data selection is ap-plied before training the baseline model.Feature effect.
To measure the impact of our novelfeatures, we trained classifiers using either LexCL,LexQL, or both feature sets (Section 3.2).
In theseexperiments, the data is fixed to the optimal subsetfound earlier.
Results are summarized on right sideof Table 2.
Statistically significants improvementsover Baseline/Baseline+Data selection are indicatedwith single/double underlining.For Arabic-only QA, adding LexQL featuresyields greatest improvements over the baseline,while the same statement holds for LexCL featuresfor the Chinese-only task.
For the English-onlyand mixed-language tasks, the most significant in-crease in MAP is observed with all of our proba-bilistic bilingual features.
For all but Arabic-onlyQA, the MAP is statistically significantly better (p <0.05) than the baseline; for Chinese-only and mixed-language tasks, it also outperforms baseline plusdata selection (p < 0.05).12 All of this indicatesthe effectiveness of our probabilistic question trans-lation, as well as our data selection strategy.Task Base +Data +FeatsCmn 0.416 0.425 (ch) 0.451 (LexCL)Arz 0.421 0.423 (en+) 0.425 (LexQL)Eng 0.637 0.657 (en+) 0.660 (all)Mixed 0.665 0.675 (en+) 0.681 (all)Table 2: L2T evaluated using MAP with 10-fold cross-validation for each task.
A statistically significant increaseover Baseline/Base+Data is shown by single/double underlin-ing (p < 0.05).Understanding the contribution of each of the four12Note that bilingual features are not expected to help on theEnglish-only task, and the improvements come solely from dataselection.LexCL features is also important.
To gain insight,we trained a classifier using all LexCL features (us-ing the optimal data subset learned earlier for eachtask), and then incrementally removed one of thefeatures, and tested on the same task.
This con-trolled experiment revealed that the word translationfeature is most useful for Chinese-only QA (i.e., re-moving it produces largest drop in MAP, 0.6 points),whereas context translation appears to be most use-ful (by a slighter margin) in Arabic-only QA.
In theformer case, the diversity provided by word transla-tion might be useful at increasing recall in retriev-ing Chinese answers.
In retrieving Arabic answers,using context to disambiguate the translation mightbe useful at increasing precision.
This result furtheremphasizes the importance of a customized transla-tion approach for MLQA.Furthermore, to test the effectiveness of the prob-abilistic translation approach (Section 3.1), we re-placed all LexCL features with a single lexical sim-ilarity feature computed from the one-best ques-tion translation.
This resulted in lower MAP: 0.427to 0.423 for Arabic-only, and 0.451 to 0.425 forChinese-only task (p < 0.01), supporting the hy-pothesis that probabilistic translation is more effec-tive than the widely-used one-best translation.
Infact, almost all gains in Chinese-only QA seems tobe coming from the probabilistic translation.For a robustness test, we let cross-validation se-lect the best combination of (data, feature), mimick-ing a less controlled, real-world setting.
In this case,the best MAP for the Arabic-, Chinese-, English-only, and Mixed-language tasks are 0.403, 0.448,0.657, and 0.679, respectively.
In all but Arabic-only, these are statistically significantly better (p <0.05) than not tuning the feature set or training data(i.e., Baseline).
This result suggests that our ap-proach can be used for any MLQA task out of thebox, and provide improvements.Learning to Custom Translate (L2CT).
We tookthe ranked list of answers output by each language-specific model, and merged all of them into a rankedlist of mixed-language answers.
For the weightedheuristic, we tried three values for the weight.
InTable 3, we see that training separate classifiersfor each subtask does not bring overall improve-ments to the end task.
Amongst merging strategies,580the most effective were weighted (weights for eachquery learned by performing a grid-search on otherqueries) and English first ?
however, both are sta-tistically indistinguishable from the single classifierbaseline.
In the latter case, the percentage of Englishanswers is highest (88%), which might not be desir-able.
Depending on the application, the ratio of lan-guages can be adjusted with an appropriate mergingmethod.
For instance, alternate and norm heuristicstend to represent languages almost equally.Approach (En-Ch-Ar) % MAPL2T 64-19-16 0.681L2CTUniform 24-35-41 0.548Alt.
32-34-34 0.574Eng.
First 88-6-6 0.668Weight2 37-30-34 0.5995 51-24-25 0.65410 61-20-19 0.669Table 3: L2T vs. L2CT for multilingual QA.Even though we get lower MAP in the overalltask, Table 2 suggests that it is worthwhile customiz-ing classifiers for each subtask (e.g., the Chinese re-sponses in the ranked list of L2CT are more relevantthan Single.).
The question of how to effectivelycombine the results into a mixed-language list, how-ever, remains an open question.5 ConclusionsWe introduced L2T, a novel approach for MLQA,inspired from recent success in CLIR research.
Toour knowledge, this is the first use of probabilistictranslation methods for this task, and the first at-tempt at using machine learning to learn the optimalquestion translation.We also proposed L2CT, which uses language-specific classifiers to treat the ranking of English,Arabic, and Chinese answers as three separate sub-tasks, by applying a separate classifier for eachlanguage.
While post-retrieval merging has beenstudied in the past, we have not come across anywork that applies this idea specifically to create alanguage-aware ranking for MLQA.Our experimental analysis shows the importanceof data selection when dealing with annotations onsource and translated text, and the effect of com-bining translation methods.
L2T improved answerranking effectiveness significantly for Chinese-only,English-only, and mixed-language QA.Although results did not support the hypothesisthat learning a custom classifier for the retrieval ofeach language would outperform the single classi-fier baseline, we think that more research is neededto fully understand how language-specific modelingcan benefit MLQA.
More sophisticated merging ofmultiple ranked lists of answers need to be explored.Learning to rank between answers from differentlanguages might be more effective than heuristics.This would allow us to predict the final language ra-tio, based on many features (e.g., general collectionstatistics, quality of candidate answers, question cat-egory and complexity, MT system confidence levels)to merge question-answer pairs.An even more comprehensive use of machinelearning would be to learn word-level translationscores, instead of relying on translation probabili-ties from the bilingual dictionary, resulting in a fullycustomized translation.
Similar approaches have ap-peared in learning-to-rank literature for monolingualIR (Bendersky et al, 2010), but not for multilingualretrieval.
Another extension of this work would beto apply the same translation for translating answersinto the question language (in addition to questiontranslation).
By doing this, we would be able to cap-ture the semantics of each answer much better, sincewe have discussed that one-best translation discardsa lot of potentially useful information.Finally, since one of the take-away messages ofour work is that a deeper understanding of linguisticcontext can improve QA effectiveness via more so-phisticated question translation, we are hoping to seeeven more improvements by creating features basedon word embeddings.
One potential next step is tolearn bilingual embeddings directly for the task ofQA, for which we have started adapting some re-lated work (Bai et al, 2010).AcknowledgementsJacob Devlin has provided great help in the designand implementation of the context-based questiontranslation approach.
We would also like to thankthe anonymous reviewers for their helpful feedback.581ReferencesSisayFissaha Adafre and Josef van Genabith.
2009.Dublin city university at qaclef 2008.
In Carol Pe-ters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo,GarethJ.F.
Jones, Mikko Kurimo, Thomas Mandl,Anselmo Pen?as, and Vivien Petras, editors, EvaluatingSystems for Multilingual and Multimodal InformationAccess, volume 5706 of Lecture Notes in ComputerScience, pages 353?360.
Springer Berlin Heidelberg.Arvind Agarwal, Hema Raghavan, Karthik Subbian,Prem Melville, Richard D Lawrence, David CGondek, and James Fan.
2012.
Learning to Rankfor Robust Question Answering.
In Proceedings ofthe 21st ACM International Conference on Informa-tion and Knowledge Management, CIKM ?12, pages833?842, New York, NY, USA.
ACM.Enrique Alfonseca, Marco De Boni, Jose?-Luis Jara-Valencia, and Suresh Manandhar.
2001.
A prototypequestion answering system using syntactic and seman-tic information for answer retrieval.
In TREC.Giuseppe Attardi, Antonio Cisternino, FrancescoFormica, Maria Simi, and Alessandro Tommasi.2001.
Piqasso: Pisa question answering system.
InTREC.Bing Bai, Jason Weston, David Grangier, RonanCollobert, Kunihiko Sadamasa, Yanjun Qi, OlivierChapelle, and Kilian Q. Weinberger.
2010.
Learn-ing to rank with (a lot of) word features.
Inf.
Retr.,13(3):291?314.Michael Bendersky, Donald Metzler, and W. Bruce Croft.2010.
Learning concept importance using a weighteddependence model.
In Proceedings of the Third ACMInternational Conference on Web Search and DataMining, WSDM ?10, pages 31?40, New York, NY,USA.
ACM.Elizabeth Boschee, Ralph Weischedel, and Alex Zama-nian.
2005.
Automatic information extraction.
InProceedings of the International Conference on Intel-ligence Analysis, volume 71.Eric Brill, Jimmy Lin, Michele Banko, Susan Dumais,and Andrew Ng.
2001.
Data-intensive question an-swering.
In In Proceedings of the Tenth Text REtrievalConference (TREC, pages 393?400.Snigdha Chaturvedi, Vittorio Castelli, Radu Florian,Ramesh M Nallapati, and Hema Raghavan.
2014.Joint Question Clustering and Relevance Predictionfor Open Domain Non-factoid Question Answering.In Proceedings of the 23rd International Conferenceon World Wide Web, WWW ?14, pages 503?514, NewYork, NY, USA.
ACM.Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan, and Tat-Seng Chua.
2005.
Question answering passage re-trieval using dependency relations.
In Proceedingsof the 28th Annual International ACM SIGIR Confer-ence on Research and Development in Information Re-trieval, SIGIR ?05, pages 400?407, New York, NY,USA.
ACM.Kareem Darwish and Douglas W. Oard.
2003.
Proba-bilistic structured query methods.
In SIGIR.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard M. Schwartz, and John Makhoul.2014.
Fast and robust neural network joint modelsfor statistical machine translation.
In Proceedings ofthe 52nd Annual Meeting of the Association for Com-putational Linguistics, ACL 2014, June 22-27, 2014,Baltimore, MD, USA, pages 1370?1380.Miquel Espla`-Gomis, Felipe Sa?nchez-Mart?
?nez, andMikel L Forcada.
2012.
UAlacant: Using OnlineMachine Translation for Cross-lingual Textual Entail-ment.
In Proceedings of the First Joint Conferenceon Lexical and Computational Semantics - Volume 1:Proceedings of the Main Conference and the SharedTask, and Volume 2: Proceedings of the Sixth Inter-national Workshop on Semantic Evaluation, SemEval?12, pages 472?476, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.M A?
Garc?
?a-Cumbreras, F Mart?
?nez-Santiago, and L AUren?a Lo?pez.
2012.
Architecture and Evaluation ofBRUJA, a Multilingual Question Answering System.Inf.
Retr., 15(5):413?432, October.Sanda Harabagiu and Cosmin Adrian Bejan.
2005.Question answering based on temporal inference.
InProceedings of the AAAI-2005 workshop on inferencefor textual question answering, pages 27?34.Sven Hartrumpf, Ingo GlA?ckner, and Johannes Level-ing.
2009.
Efficient question answering with ques-tion decomposition and multiple answer streams.
InCarol Peters, Thomas Deselaers, Nicola Ferro, JulioGonzalo, GarethJ.F.
Jones, Mikko Kurimo, ThomasMandl, Anselmo PeA?
?as, and Vivien Petras, editors,Evaluating Systems for Multilingual and MultimodalInformation Access, volume 5706 of Lecture Notesin Computer Science, pages 421?428.
Springer BerlinHeidelberg.Eduard Hovy, Laurie Gerber, Ulf Hermjakob, Chin-Yew Lin, and Deepak Ravichandran.
2001.
Towardsemantics-based answer pinpointing.
In Proceedingsof the first international conference on Human lan-guage technology research, pages 1?7.
Association forComputational Linguistics.Boris Katz, Gary Borchardt, and Sue Felshin.
2005.
Syn-tactic and semantic decomposition strategies for ques-tion answering from multiple resources.
In Proceed-ings of the AAAI 2005 workshop on inference for tex-tual question answering, pages 35?41.Jeongwoo Ko, Luo Si, and Eric Nyberg.
2010a.
Com-bining Evidence with a Probabilistic Framework for582Answer Ranking and Answer Merging in QuestionAnswering.
Inf.
Process.
Manage., 46(5):541?554,September.Jeongwoo Ko, Luo Si, Eric Nyberg, and Teruko Mi-tamura.
2010b.
Probabilistic Models for Answer-ranking in Multilingual Question-answering.
ACMTrans.
Inf.
Syst., 28(3):16:1?-16:37, July.Oleksandr Kolomiyets and Marie-Francine Moens.
2011.A survey on question answering technology from aninformation retrieval perspective.
Information Sci-ences, 181(24):5412?5434.Chuan-Jie Lin and Yu-Min Kuo.
2010.
Descriptionof the ntou complex qa system.
In Proceedings ofNTCIR-8 Workshop.Xiaoqiang Luo, Hema Raghavan, Vittorio Castelli,Sameer Maskey, and Radu Florian.
2013.
Findingwhat matters in questions.
In Proceedings of NAACL-HLT?13.N Madnani, Jimmy Lin, and Bonnie J Dorr.
2007.
TREC2007 ciQA Task: University of Maryland.
Proceed-ings of TREC.A?ngel Martinez-Gonzalez, Cesar de Pablo-Sanchez,Concepcion Polo-Bayo, MarA?aTeresa Vicente-Diez,Paloma Martinez-Fernandez, and Jose Luis Martinez-Fernandez.
2009.
The miracle team at the clef 2008multilingual question answering track.
In Carol Pe-ters, Thomas Deselaers, Nicola Ferro, Julio Gonzalo,GarethJ.F.
Jones, Mikko Kurimo, Thomas Mandl,Anselmo PeA?
?as, and Vivien Petras, editors, Evalu-ating Systems for Multilingual and Multimodal Infor-mation Access, volume 5706 of Lecture Notes in Com-puter Science, pages 409?420.
Springer Berlin Heidel-berg.Yashar Mehdad, Matteo Negri, and Marcello Federico.2010.
Towards Cross-lingual Textual Entailment.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, HLT ?10,pages 321?324, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Donald Metzler and W Bruce Croft.
2005.
A Markovrandom field model for term dependencies.
In Pro-ceedings of the 28th annual international ACM SIGIRconference on Research and development in informa-tion retrieval, SIGIR ?05, pages 472?479, New York,NY, USA.
ACM.Teruko Mitamura, Mengqiu Wang, Hideki Shima, andFrank Lin.
2006.
Keyword translation accuracyand cross-lingual question answering in chinese andjapanese.
In Proceedings of the Workshop on Multilin-gual Question Answering, MLQA ?06, pages 31?38,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Thomas S Morton.
1999.
Using Coreference for Ques-tion Answering.
In Proceedings of the Workshopon Coreference and Its Applications, CorefApp ?99,pages 85?89, Stroudsburg, PA, USA.
Association forComputational Linguistics.Rafael Munoz-Terol, Marcel Puchol-Blasco, Maria Par-dino, Jose Manuel Gomez, Sandra Roger, Katia Vila,Antonio Ferrandez, Jesus Peral, and Patricio Martinez-Barco.
2009.
Integrating logic forms and anaphoraresolution in the aliqan system.
In Evaluating Systemsfor Multilingual and Multimodal Information Access,LNCS, pages 438?441.
Springer Berlin Heidelberg.Jian-Yun Nie.
2010.
Cross-language information re-trieval.
Synthesis Lectures on Human Language Tech-nologies, 3(1):1?125.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics - Volume 1, ACL ?03, pages 160?167, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Han Ren, Donghong Ji, and Jing Wan.
2010.
Whu ques-tion answering system at ntcir-8 aclia task.
In Pro-ceedings of NTCIR-8 Workshop.Stephen Robertson, Hugo Zaragoza, and Michael Taylor.2004.
Simple {BM25} extension to multiple weightedfields.
In Proc.
CIKM, pages 42?49.Bogdan Sacaleanu, Gu?nter Neumann, and ChristianSpurk.
2008.
Dfki-lt at qaclef 2008.
In Carol Petersand et al, editors, CLEF 2008 Working Notes, Work-ing Notes.
Springer Verlag.E Saquete, J L Vicedo, P Mart?
?nez-Barco, R Mun?oz,and F Llopis.
2005.
Evaluation of Complex Tem-poral Questions in CLEF-QA.
In Proceedings of the5th Conference on Cross-Language Evaluation Fo-rum: Multilingual Information Access for Text, Speechand Images, CLEF?04, pages 591?596, Berlin, Heidel-berg.
Springer-Verlag.Jacques Savoy.
2004.
Combining multiple strategiesfor effective monolingual and cross-language retrieval.Information Retrieval, 7(1-2):121?148.Dan Shen and Mirella Lapata.
2007.
Using seman-tic roles to improve question answering.
In EMNLP-CoNLL, pages 12?21.
Citeseer.Hideki Shima and Teruko Mitamura.
2010.
Bootstrappattern learning for open-domain clqa.
In Proceedingsof NTCIR-8 Workshop.Ming-Feng Tsai, Yu-Ting Wang, and Hsin-Hsi Chen.2008.
A study of learning a merge model for multilin-gual information retrieval.
In Proceedings of the 31st583Annual International ACM SIGIR Conference on Re-search and Development in Information Retrieval, SI-GIR ?08, pages 195?202, New York, NY, USA.
ACM.Ferhan Ture and Jimmy Lin.
2014.
Exploiting represen-tations from statistical machine translation for cross-language information retrieval.
ACM Trans.
Inf.
Syst.,32(4):19:1?19:32, October.Marlies Van der Wees, Arianna Bisazza, WouterWeerkamp, and Christof Monz.
2015.
What?s in a do-main?
analyzing genre and topic differences in statis-tical machine translation.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing (Volume 2: Short Pa-pers), pages 560?566, Beijing, China, July.
Associa-tion for Computational Linguistics.Chen Zhang, Matthew Gerber, Tyler Baldwin, StevenEmelander, Joyce Chai, and Rong Jin.
2007.
Michi-gan State University at the 2007 TREC ciQA Task.In Proceedings of the Sixteenth Text Retrieval Confer-ence, Gaithersburg, Maryland, November.584
