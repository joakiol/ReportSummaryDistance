Two-Level, Many-Paths GenerationKev in  Kn ightUSC/ In fo rmat ion  Sciences Inst i tute4676 Admira l ty  WayMar ina del Rey, CA 90292knight@isi .eduVas i le ios  Hatz ivass i log louDepar tment  of Computer  ScienceCo lumbia  Univers i tyNew York, NY 10027vh@cs.columbia.eduAbstractLarge-scale natural anguage generation re-quires the integration of vast mounts ofknowledge: lexical, grammatical, and concep-tual.
A robust generator must be able tooperate well even when pieces of knowledgeaxe missing.
It must also be robust againstincomplete or inaccurate inputs.
To attackthese problems, we have built a hybrid gen-erator, in which gaps in symbolic knowledgeare filled by statistical methods.
We describealgorithms and show experimental results.
Wealso discuss how the hybrid generation modelcan be used to simplify current generators andenhance their portability, even when perfectknowledge is in principle obtainable.1 In t roduct ionA large-scale natural language generation (NLG)system for unrestricted text should be able to op-erate in an environment of 50,000 conceptual termsand 100,000 words or phrases.
Turning conceptualexpressions into English requires the integration oflarge knowledge bases (KBs), including grammar,ontology, lexicon, collocations, and mappings be-tween them.
The quality of an NLG system dependson the quality of its inputs and knowledge bases.Given that perfect KBs do not yet exist, an impor-tant question arises: can we build high-quality NLGsystems that are robust against incomplete KBs andinputs?
Although robustness has been heavily stud-ied in natural anguage understanding (Weischedeland Black, 1980; Hayes, 1981; Lavie, 1994), it hasreceived much less attention in NLG (Robin, 1995).We describe a hybrid model for natural languagegeneration which offers improved performance in thepresence of knowledge gaps in the generator (thegrammar and the lexicon), and of errors in the se-mantic input.
The model comes out of our practi-cal experience in building a large Japanese-Englishnewspaper machine translation system, JAPAN-GLOSS (Knight et al, 1994; Knight et al, 1995).This system translates Japanese into representationswhose terms are drawn from the SENSUS ontol-ogy (Knight and Luk, 1994), a 70,000-node knowl-edge base skeleton derived from resources like Word-Net (Miller, 1990), Longman's Dictionary (Procter,1978), and the PENMAN Upper Model (Bateman,1990).
These representations are turned into En-glish during generation.
Because we are processingunrestricted newspaper text, all modules in JAPAN-GLOSS must be robust.In addition, we show how the model is useful insimplifying the design of a generator and its knowl-edge bases even when perfect knowledge is available.This is accomplished by relegating some aspects oflexical choice (such as preposition selection and non-compositional interlexical constraints) to a statisti-cal component.
The generator can then use simplerrules and combine them more freely; the price of thissimplicity is that some of the output may be invalid.At this point, the statistical component intervenesand filters from the output all but the fluent expres-sions.
The advantage of this two-level approach isthat the knowledge bases in the generator becomesimpler, easier to develop, more portable across do-mains, and more accurate and robust in the presenceof knowledge gaps.2 Knowledge  GapsIn our machine translation experiences, we tracedgeneration disfluencies to two sources: 1 (1) incom-plete or inaccurate conceptual (interlingua) struc-tures, caused by knowledge gaps in the source lan-guage analyzer, and (2) knowledge gaps in the gen-erator itself.
These two categories of gaps include:?
Interlingual analysis often does not include ac-curate representations of number, definiteness,or time.
(These are often unmarked in Japaneseand require exceedingly difficult inferences torecover).?
The generation lexicon does not mark rarewords and generally does not distinguish be-tween near synonyms (e.g., finger vs. ?digil).1See also (Kukich, 1988) for a discussion of fluencyproblems in NLG systems.252?
The generation lexicon does not contain muchcollocational knowledge (e.g., on the field vs.*on the end zone).?
Lexico-syntactic constraints (e.g., tell her hi vs.*say her hi), syntax-semantics mappings (e.g.,the vase broke vs. *the food ate), and selectionalrestrictions are not always available or accurate.The generation system we use, PENMAN (Pen-man, 1989), is robust because it supplies appropriatedefaults when knowledge is missing.
But the defaultchoices frequently are not the optimM ones; the hy-brid model we describe provides more satisfactorysolutions.3 I ssues  in  Lex ica l  Cho iceThe process of selecting words that will lexicalizeeach semantic concept is intrinsically linked withsyntactic, semantic, and discourse structure issues.
2Multiple constraints apply to each lexical decision,often in a highly interdependent manner.
However,while some lexical decisions can affect future (orpast) lexical decisions, others are purely local, inthe sense that they do not affect the lexicMizationof other semantic roles.
Consider the case of timeadjuncts that express a single point in time, and as-sume that the generator has already decided to usea prepositional phrase for one of them.
There areseveral forms of such adjuncts, e.g.,at five.She left on Monday.in February.In terms of their interactions with the rest ofthe sentence, these manifestations of the adjunctare identical.
The use of different prepositions isan interlexical constraint between the semantic andsyntactic heads of the PP that does not propagateoutside the PP.
Consequently, the selection of thepreposition can be postponed until the very end.Existing generation models however select thepreposition according to defaults or randomly.among possible alternatives or by explicitly encod-ing the lexical constraints.
The PENMAN gener-ation system (Penman, 1989) defaults the preposi-tion choice for point-time adjuncts to at, the mostcommonly used preposition in such cases.
TheFUF/SURGE (Elhadad, 1993) generation system isan example where prepositional lexical restrictionsin time adjuncts are encoded by hand, producingfluent expressions but at the cost of a larger gram-mar.Collocational restrictions are another example oflexical constraints.
Phrases such as three straight~We consider lexical choice as a general problem forboth open and closed class words, not limiting it tothe former only as is sometimes done in the generationliterature.victories, which are frequently used in sports reportsto express historical information, can be decomposedsemantically into the head noun plus its modifiers.However, when ellipsis of the head noun is consid-ered, a detailed corpus analysis of actual basketballgame reports (Robin, 1995) shows that the formswonflost hree straight X, won~lost ~hree consecutiveX, and won~lost hree straight are regularly used,but the form *won~lost three consecutive is not.
Toachieve fluent output within the knowledge-basedgeneration paradigm, lexical constraints of this typemust be explicitly identified and represented.Both the above examples indicate the presence of(perhaps domain-dependent) lexical constraints thatare not explainable on semantic grounds.
In the caseof prepositions in time adjuncts, the constraints areinstitutionalized in the language, but still nothingabout the concept MONTH relates to the use of thepreposition in with month names instead of, say, on(Herskovits, 1986).
Furthermore, lexical constraintsare not limited to the syntagmatic, interlexical con-straints discussed above.
For a generator to be ableto produce sufficiently varied text, multiple rendi-tions of the same concept must be accessible.
Then,the generator is faced with paradigmatic hoicesamong alternatives that without sufficient informa-tion may look equivalent.
These choices includechoices among synonyms (and near-synonyms), andchoices among alternative syntactic realizations of asemantic role.
However, it is possible that not all thealternatives actually share the same level of fluencyor currency in the domain, even if they are roughparaphrases.In short, knowledge-based generators are facedwith multiple, complex, and interacting lexical con-straints, 3 and the integration of these constraints ia difficult problem, to the extent that the need fora different specialized architecture for lexical choicein each domain has been suggested (Danlos, 1986).However, compositional pproaches to lexical choicehave been successful whenever detailed representa-tions of lexical constraints can be collected and en-tered into the lexicon (e.g., (Elhadad, 1993; Ku-kich et al, 1994)).
Unfortunately, most of theseconstraints must be identified manually, and evenwhen automatic methods for the acquisition of sometypes of this lexical knowledge xist (Smadja andMcKeown, 1991), the extracted constraints muststill be transformed to the generator's representa-tion language by hand.
This narrows the scope ofthe lexicon to a specific domain; the approach failsto scale up to unrestricted language.
When the goalis domain-independent generation, we need to inves-tigate methods for producing reasonable output inthe absence of a large part of the information tradi-3Including constraints not discussed above, originat-ing for example from discourse structure, the user modelsfor the speaker and hearer, and pragmatic needs.253tionally available to the lexical chooser.4 Cur rent  So lu t ionsTwo strategies have been used in lexical choice whenknowledge gaps exist: selection of a default, 4 andrandom choice among alternatives.
Default choiceshave the advantage that they can be carefully chosento mask knowledge gaps to some extent.
For exam-ple, PENMAN defaults article selection to the andtense to present, so it will produce The dog chasesthe cat in the absence of definiteness information.Choosing the is a good tactic, because the workswith mass, count, singular, plural, and occasionallyeven proper nouns, while a does not.
On the downside, the's only outnumber a's and an's by abouttwo-to-one (Knight and Chander, 1994), so guess-ing the will frequently be wrong.
Another ploy is togive preference to nominalizations over clauses.
Thisgenerates entences like They plan the statement ofthe filing for bankruptcy, avoiding disasters like Theyplan that it is said to file for bankruptcy.
Of course,we also miss out on sparkling renditions like Theyplan to say that they will file for bankruptcy.
Thealternative of randomized ecisions offers increasedparaphrasing power but also the risk of producingsome non-fluent expressions; we could generate sen-tences like The dog chased a cat and A dog will chasethe cat, but also An earth circles a sun.To sum up, defaults can help against knowledgegaps, but they take time to construct, limit para-phrasing power, and only return a mediocre level ofquality.
We seek methods that can do better.5 S ta t i s t i ca l  MethodsAnother approach to the problem of incompleteknowledge is the following.
Suppose that accordingto our knowledge bases, input I may be rendered assentence A or sentence B.
If we had a device thatcould invoke new, easily obtainable knowledge toscore the input/output pair (I, A) against (I, B), wecould then choose A over B, or vice-versa.
An alter-native to this is to forget I and simply score A and Bon the basis of fluency.
This essentially assumes thatour generator produces valid mappings from I, butmay be unsure as to which is the correct rendition.At this point, we can make another approximation--modeling fluency as likelihood.
In other words, howoften have we seen A and B in the past?
If A has oc-curred fifty times and B none at all, then we chooseA.
But i fA and B are long sentences, then probablywe have seen neither.
In that case, further approxi-mations are required.
For example, does A containfrequent hree-word sequences?
Does B?Following this reasoning, we are led into statisti-cal language modeling.
We built a language model4See also (Harbusch et al, 1994) for a thorough dis-cussion of defaulting in NLG systems.for the English language by estimating bigram andtrigram probabilities from a large collection of 46million words of Wall Street Journal material.
5 Wesmoothed these estimates according to class mem-bership for proper names and numbers, and accord-ing to an extended version of the enhanced Good-Turing method (Church and Gale, 1991) for the re-maining words.
The latter smoothing operation otonly optimally regresses the probabilities of seen n-grams but also assigns a non-zero probability to allunseen n-grams which depends on how likely theircomponent m-grams (m < n, i.e., words and bi-grams) are.
The resulting conditional probabilitiesare converted to log-likelihoods for reasons of nu-merical accuracy and used to estimate the overallprobability P(S) of any English sentence S accord-ing to a Markov assumption, i.e.,log P(S) = Z log P(w, \[Wi_l) for bigramsilog P(S) = Z log P(wilwi_z, w i -2 )  for trigramsiBecause both equations would assign lower andlower probabilities to longer sentences and we needto compare sentences of different lengths, a heuristicstrictly increasing function of sentence length, f( l) =0.5l, is added to the log-likelihood estimates.6 F i r s t  Exper imentOur first goal was to integrate the symbolic knowl-edge in the PENMAN system with the statisticalknowledge in our language model.
We took a se-mantic representation generated automatically froma short Japanese sentence.
We then used PEN-MAN to generate 3,456 English sentences corre-sponding to the 3,456 (= 2 ' .
33) possible com-binations of the values of seven binary and threeternary features that were unspecified in the se-mantic input.
These features were relevant o thesemantic representation but their values were notextractable from the Japanese sentence, and thuseach of their combinations corresponded to a par-ticular interpretation among the many possible inthe presence of incompleteness in the semantic in-put.
Specifying a feature forced PENMAN to makea particular linguistic decision.
For example, adding( : ident i f iab i l i ty -q  t )  forces the choice of de-terminer, while the : lex  feature offers explicit con-trol over the selection of open-class words.
A literaltranslation of the input sentence was something likeAs for new company, there is plan to establish inFebruary.
Here are three randomly selected transla-tions; note that the object of the "establishing" ac-tion is unspecified in the Japanese input, but PEN-MAN supplies a placeholder it when necessary, toensure grammaticality:SAvailable from the ACL Data Collection Initiative,as CD ROM 1.254A new company will have in mind that itis establishing it on February.The new company plans the launchingon February.New companies will have as a goalthe launching at February.We then ranked the 3,456 sentences using thebigram version of our statistical anguage model,with the hope that good renditions would come outon top.
Here is an abridged list of outputs, log-likelihood scores heuristically corrected for length,and rankings:1 The new company plans tolaunch i t  in February.
\[ -13.568260 \]2 The new company plans thefoundation in February.
\[ -13.755152 \]3 The new company plans theestablishment in February.
\[ -13.821412 \]4 The new company plans toestabl ish i t  in February.
\[ -14.
121367 \].
.
.
.
.
, , .
.
.
.
.
, .
, .
.
.
.
.
, .
, ,  .
.
.
.
.
* o  .
.
.
.
* o60 The new compan ies  plan theestablishment on February.
\[ -16.350112 \]61 The new compan ies  plan thelaunching in February.
\[ -16.530286 \].
.
.
,  .
.
.
.
.
* *  .
.
.
.
.
.
, .
.
.
.
.
.
.
?
,  .
.
.
.
.
.
, .
.
.400 The new compan ies  have  as a goal thefoundation at February.
\[ -23.836556 \]401 The new companies wil l  have in mind toestabl ish i t  at February.
\[ -23.842337 \], , , .
.
.
.
.
.
?
,  .
.
.
.
.
, , .
,  .
.
.
.
.
, .
.
.
.
.
.
.
.
.
.
.While this experiment shows that statistical mod-els can help make choices in generation, it fails as acomputational strategy.
Running PENMAN 3,456times is expensive, but nothing compared to thecost of exhaustively exploring all combinations inlarger input representations corresponding to sen-tences typically found in newspaper text.
Twenty orthirty choice points typically multiply into millionsor billions of potential sentences, and it is infeasibleto generate them all independently.
This leads us toconsider other algorithms.7 Many-Paths  Generat ionInstead of explicitly constructing all possible rendi-tions of a semantic input and running PENMANon them, we use a more efficient data structureand control algorithm to express possible ambigui-ties.
The data structure is a word laltice--an acyclicstate transition etwork with one start state, one fi-nal state, and transitions labeled by words.
Wordlattices are commonly used to model uncertainty inspeech recognition (Waibel and Lee, 1990) and arewell adapted for use with n-gram models.As we discussed in Section 3, a number of gen-eration difficulties can be traced to the existence ofconstraints between words and phrases.
Our genera-tor operates on lexical islands, which do not interactwith other words or concepts.
6 How to identify suchislands is an important problem in NLG: grammat-ical rules (e.g., agreement) may help group wordstogether, and collocational knowledge can also markthe boundaries of some lexical islands (e.g., nomi-nal compounds).
When no explicit information ispresent, we can resort to treating single words as lex-ical islands, essentially adopting a view of maximumcompositionality.
Then, we rely on the statisticalmodel to correct this approximation, by identifyingany violations of the compositionality principle onthe fly during actual text generation.The type of the lexical islands and the mannerby which they have been identified do not affect theway our generator processes them.
Each island cor-responds to an independent component of the finalsentence.
Each individual word in an island specifiesa choice point in the search and causes the creationof a state in the lattice; all continuations of alterna-tive lexicalizations for this island become paths thatleave this state.
Choices between alternative lexi-cal islands for the same concept also become statesin the lattice, with arcs leading to the sub-latticescorresponding to each island.Once the semantic input to the generator hasbeen transformed to a word lattice, a search com-ponent identifies the N highest scoring paths fromthe start to the final state, according to our statisti-cal language model.
We use a version of the N-bestalgorithm (Chow and Schwartz, 1989), a Viterbi-style beam search algorithm that allows extractionof more than just the best scoring path.
(Hatzivas-siloglou and Knight, 1995) has more details on oursearch algorithm and the method we applied to es-timate the parameters of the statistical model.Our approach differs from traditional top-downgeneration in the same way that top-down andbottom-up parsing differ.
In top-down parsing,backtracking is employed to exhaustively examinethe space of possible alternatives.
Similarly, tra-ditional control mechanisms in generation operatetop-down, either deterministically (Meteer et al,1987; Tomita and Nyberg, 1988; Penman, 1989) orby backtracking to previous choice points (Elhadad,1993).
This mode of operation can unnecessarily du-plicate a lot of work at run time, unless sophisticatedcontrol directives are included in the search engine(Elhadad and Robin, 1992).
In contrast, in bottom-up parsing and in our generation model, a specialdata structure (a chart or a lattice respectively) isused to efficiently encode multiple analyses, and toallow structure sharing between many alternatives,eliminating repeated search.What should the word lattices produced by a gen-erator look like?
If the generator has complete6At least as far as the generator knows.255knowledge, the word lattice will degenerate to astring, e.g.
:the _ ~ large _/"% Federal L/"~ deficit ~,q.~ fellSuppose we are uncertain about definiteness andnumber.
We can generate a lattice with eight pathsinstead of one:the deficit(* stands for the empty string.)
But we run the riskthat the n-gram model will pick a non-grammaticalpath like a large Federal deficits fell.
So we can pro-duce the following lattice instead:large -~J-J~ Federal ~)  deficits a(~In this case, we use knowledge about agreement toconstrain the choices offered to the statistical model,from eight paths down to six.
Notice that the six-path lattice has more states and is more complexthan the eight-path one.
Also, the n-gram length iscritical.
When long-distance f atures control gram-maticality, we cannot rely on the statistical model.Fortunately, long-distance features like agreementare among the first that go into any symbolic gen-erator.
This is our first example of how symbolicand statistical knowledge sources contain comple-mentary information, which is why there is a sig-nificant advantage to combining them.Now we need an algorithm for converting ener-ator inputs into word lattices.
Our approach is toassign word lattices to each fragment of the input,in a bottom-up compositional fashion.
For example,consider the following semantic input, which is writ-ten in the PENMAN-style Sentence Plan Language(SPL) (Penman, 1989), with concepts drawn fromthe SENSUS ontology (Knight and Luk, 1994), andmay be rendered in English as It is easy for  Amer i -cans to obtain guns:(A / Ihave the qual i ty  of beingl:DOMAIN (P / Iprocurel:AGENT (A2 /\]American\]):PATIENT (G / \[gun, arm\[)):RANGE (E / Jeasy, e f fo r t less J ) )We process emantic subexpressions in a bottom-up order, e.g., A2, G, P, ~., and finally A.
The grammarassigns what we call an e-structure to each subex-pression.
An e-structure consists of a list of dis-tinct syntactic ategories, paired with English wordlattices: (<syn, la t>,  <syn, la t>,  .
.
. )
.
As weclimb up the input expression, the grammar gluestogether various word lattices.
The grammar isorganized around semantic feature patterns ratherthan English syntax--rather than having one S ->NP-VP rule with many semantic triggers, we have oneAGENT-PATIENT rule with many English renderings.Here is a sample rule:((xl :agent) (x2 :patient) (x3 :rest)->(s (seq (xl rip) (x3 v-tensed) (x2 np)))(s (seq (xl np) (x3 v-tensed) (wrd "that")(x2 s)))(s (seq (xl np) (x3 v-tensed)(x2 (*OR* in, inf -raise))))(s (seq (x2 np) (x3 v-passive) (vrd "by")(xl rip)))( in, (seq (wrd "for") (xl np) (wrd "to")(x3 v) (x2 np)))( inf-raise (seq (xl np)(or (seq (wrd "of") (x3 np)(x2 np))(seq (wrd "to 't ) (x3 v)(x2 rip)))))(rip (seq (x3 rip) (vrd "of f' ) (x2 np)(wrd "by") (xl np))))Given an input semantic pattern, we locate thefirst grammar ule that matches it, i.e., a rule whoseleft-hand-side f atures except :res t  are contained inthe input pattern.
The feature : res t  is our mech-anism for allowing partial matchings between rulesand semantic inputs.
Any input features that arenot matched by the selected rule are collected in: res t ,  and recursively matched against other gram-mar rules.For the remaining features, we compute new e-structures using the rule's right-hand side.
In thisexample, the rule gives four ways to make a syntacticS, two ways to make an infinitive, and one way tomake an NP.
Corresponding word lattices are builtout of elements that include:?
(seq x y .
.
.  )
- -create a lattice by sequentiallygluing together the lattices x, y, and .
.
.?
(or  x y .
.
.  )
- -create a lattice by branching onx, y, and .
.
.?
(wrd w)--create the smallest lattice: a singlearc labeled with the word w.?
(xn <syn>) - - i f  the e-structure for the se-mantic material under the xn feature contains<syn,  la t>,  return the word lattice la t ;  oth-erwise fail.Any failure inside an alternative right-hand side ofa rule causes that alternative to fail and be ignored.When all alternatives have been processed, resultsare collected into a new e-structure.
If two or moreword lattices can be created from one rule, they aremerged with a final or.256Because our grammar is organized around seman-tic patterns, it nicely concentrates all of the mate-rial required to build word lattices.
Unfortunately,it forces us to restate the same syntactic constraintin many places.
A second problem is that sequentialcomposition does not allow us to insert new wordsinside old lattices, as needed to generate sentenceslike John looked it up.
We have extended our no-tation to allow such constructions, but the full so-lution is to move to a unification-based framework,in which e-structures are replaced by arbitrary fea-ture structures with syn, sere, and la t  fields.
Ofcourse, this requires extremely efficient handling ofthe disjunctions inherent in large word lattices.8 Resu l tsWe implemented a medium-sized grammar of En-glish based on the ideas of the previous section, foruse in experiments and in the JAPANGLOSS ma-chine translation system.
The system converts a se-mantic input into a word lattice, sending the resultto one of three sentence xtraction programs:?
RANDOM--follows a random path through thelattice.?
DEFAULT--follows the topmost path in the lat-tice.
All alternatives are ordered by the gram-mar writer, so that the topmost lattice path cor-responds to various defaults.
In our grammar,defaults include singular noun phrases, the def-inite article, nominal direct objects, in versuson, active voice, that versus who, the alphabet-ically first synonym for open-class words, etc.?
STATISTICAL--a sentence extractor based onword bigram probabilities, as described in Sec-tions 5 and 7.For evaluation, we compare English outputs fromthese three sources.
We also look at lattice prop-erties and execution speed.
Space limitations pre-vent us from tracing the generation of many longsentences--we show instead a few short ones.
Notethat the sample sentences shown for the RANDOM ex-traction model are not of the quality that would nor-mally be expected from a knowledge-based genera-tor, because of the high degree of ambiguity (un-specified features) in our semantic input.
This in-completeness can be in turn attributed in part tothe lack of such information in Japanese source textand in part to our own desire to find out how muchof the ambiguity can be automatically resolved withour statistical model.INPUT(A / \[accusel:AGENT SHE:PATIENT (T / \[thieve\[:AGENT HE:PATIENT (M / Imotorcar\])))LATTICE CREATED44 nodes, 217 arcs, 381,440 paths;59 distinct unigrams, 430 distinct bigrams.RANDOM EXTRACTIONHer incriminates for him to thieve anautomobiles.She am accusing for him to steal autos.She impeach that him thieve that therewas the auto.DEFAULT EXTRACTIONShe accuses that he steals the auto.STATISTICAL BIGRAM EXTRACTION1 She charged that he stole the car.2 She charged that he stole the cars.3 She charged that he stole cars.4 She charged that he stole car.5 She charges that he stole the car.TOTAL EXECUTION TIME: 22.77 CPU seconds.INPUT(A / Ihave the qua l i ty  of beingl:DOMAIN (P / \[procurel:AGENT (A2 / \[American\[):PATIENT (G / \[gun, arml)):RANGE (E / \[easy, effortless\[))LATTICE CREATED64 nodes, 229 arcs ,  1,345,536 paths;47 d i s t inc t  unigrams, 336 d is t inc t  bigrams.RANDOM EXTRACTIONProcurals of guns by Americans were easiness.A procurements of guns by a Americans willbe an effortlessness.It is easy that Americans procure thatthere is gun.DEFAULT EXTRACTIONThe procural of the gun by the American iseasy.STATISTICAL BIGRAM EXTRACTION1 It is easy for Americans to obtain a gun.2 It will be easy for Americans to obtain a257gun.3 It is easy for Americans to obtain gun.4 It is easy for American to obtain a gun.5 It was easy for Americans to obtain a gun.TOTAL EXECUTION TIME: 23.30 CPU seconds.INPUT(H / \[have the quality of being\[:DOMAIN (H2 / \[have the quality of being\]:DOMAIN (E / lear ,  take inl:AGENT YOU:PATIENT (P / IpouletJ)):RANGE (0 / Jobligatory\[)):RANGE (P2 / \[possible, potential\[))LATTICE CREATED260 nodes, 703 arcs, 10,734,304 paths;48 distinct unigrams, 345 distinct bigrams.RANDOM EXTRACTIONYou may be obliged to eat that there wasthe poulet.An consumptions of poulet by you may bethe requirements.It might be the requirement that the chickenare eaten by you.DEFAULT EXTRACTIONThat the consumption of the chicken by youis obligatory is possible.STATISTICAL BIGRAM EXTRACTION1 You may have to eat chicken.2 You might have to eat chicken.3 You may be requ i red  to eat chicken.4 You might be requ i red  to eat chicken.5 You may be obliged to eat chicken.TOTAL EXECUTION TIME: 58.78 CPU seconds.A final (abbreviated) example comes from interlin-gua expressions produced by the semantic analyzerof JAPANGLOSS, involving long sentences charac-teristic of newspaper text.
Note that although thelattice is not much larger than in the previous ex-amples, it now encodes many more paths.LATTICE CREATED267 nodes, 726 arcs ,4,831,867,621,815,091,200 paths;67 d i s t inc t  unigrams, 332 d is t inc t  bigraras.RANDOM EXTRACTIONSubsidiary on an Japan's of Perkin ElmerCo.
's hold a stocks's majority, and as fora beginnings, productia of an stepper andan dry etching devices which were appliedfor an constructia of microcircuitmicrochip was planed.STATISTICAL BIGRAM EXTRACTIONPerkin Elmer Co. ' s  Japanese subs id ia ryholds major i ty  of s tocks ,  and as for  thebeginn ing,  product ion of s teppers  and dryetch ing devices that  w i l l  be used toconst ruct  mic roc i rcu i t  chips are planned.TOTAL EXECUTION TIME: 106.28 CPU seconds.9 St rengths  and  WeaknessesMany-paths generation leads to a new style of incre-mental grammar building.
When dealing with somenew construction, we first rather mindlessly overgen-erate, providing the grammar with many ways to ex-press the same thing.
Then we watch the statisticalcomponent make its selections.
If the selections arecorrect, there is no need to refine the grammar.For example, in our first grammar, we did notmake any lexical or grammatical case distinctions.So our lattices included paths like Him saw I aswell as He saw me.
But the statistical model stu-diously avoided the bad paths, and in fact, we haveyet to see an incorrect case usage from our genera-tor.
Likewise, our grammar proposes both his boxand the box of he/him, but the former is statisticallymuch more likely.
Finally, we have no special ruleto prohibit articles and possessives from appearingin the same noun phrase, but the bigram the his isso awful that the null article is always selected inthe presence of a possessive pronoun.
So we can getaway with treat ing possessive pronouns like regularadjectives, greatly simpli fying our grammar.We have also been able to simplify the genera-t ion of morphological variants.
While true irregularforms (e.g., child~children) must be kept in a smallexception table, the problem of "multiple regular"patterns usually increases the size of this table dra-matically.
For example, there are two ways to plu-ralize a noun ending in -o, but often only one is cor-rect for a given noun (potatoes, but photos).
Thereare many such inflectional and derivational patterns.Our approach is to apply all patterns and insert allresults into the word lattice.
Fortunately, the statis-tical model steers clear of sentences containing non-words like potatos and photoes.
We thus get by witha very small exception table, and furthermore, ourspelling habits automatically adapt to the trainingcorpus.258Most importantly, the two-level generation modelallows us to indirectly apply lexical constraints forthe selection of open-class words, even though theseconstraints are not explicitly represented in the gen-erator's lexicon.
For example, the selection of aword from a pair of frequently co-occurring adja-cent words will automatically create a strong biasfor the selection of the other member of the pair, ifthe latter is compatible with the semantic onceptbeing lexicalized.
This type of collocational knowl-edge, along with additional collocational informationbased on long- and variable-distance d pendencies,has been successfully used in the past to increasethe fluency of generated text (Smadja and McKe-own, 1991).
But, although such collocational infor-mation can be extracted automatically, it has to bemanually reformulated into the generator's represen-tational framework before it can be used as an addi-tional constraint during pure knowledge-based gen-eration.
In contrast, the two-level model providesfor the automatic ollection and implicit represen-tation of collocational constraints between adjacentwords.In addition, in the absence of external lexical con-straints the language model prefers words more typ-ical of and common in the domain, rather thangeneric or overly specialized or formal alternatives.The result is text that is more fluent and closely sim-ulates the style of the training corpus in this respect.Note for example the choice of obtain in the secondexample of the previous ection in favor of the moreformal procure.Many times, however, the statistical model doesnot finish the job.
A bigram model will happily se-lect a sentence like I only hires men who is goodpilots.
If we see plenty of output like this, thengrammatical work on agreement is needed.
Or con-sider They planned increase in production, wherethe model drops an article because planned in-crease is such a frequent bigram.
This is a subtleinteraction--is planned a main verb or an adjective?Also, the model prefers short sentences to long oneswith the same semantic ontent, which favors con-ciseness, but sometimes selects bad n-grams to avoida longer (but clearer) rendition.
This an interestingproblem not encountered in otherwise similar speechrecognition models.
We are currently investigatingsolutions to all of these problems in a highly exper-imental setting.10 Conc lus ionsStatistical methods give us a way to address a widevariety of knowledge gaps in generation.
They evenmake it possible to load non-traditional duties ontoa generator, such as word sense disambiguation formachine translation.
For example, bei in Japanesemay mean either American or rice, and sha maymean shrine or company.
If for some reason, theanalysis of beisha fails to resolve these ambiguities,the generator can pass them along in the lattice itbuilds, e.g.
:the American shrineIn this case, the statistical model has a strongpreference for the American company, which isnearly always the correct ranslation.
7Furthermore, our two-level generation model canimplicitly handle both paradigmatic and syntag-matic lexical constraints, leading to the simplifica-tion of the generator's grammar and lexicon, andenhancing portability.
By retraining the statisti-cal component on a different domain, we can au-tomatically pick up the peculiarities of the sublan-guage such as preferences for particular words andcollocations.
At the same time, we take advantageof the strength of the knowledge-based approachwhich guarantees grammatical inputs to the statisti-cal component, and reduces the amount of languagestructure that is to be retrieved from statistics.
Thisapproach addresses the problematic aspects of bothpure knowledge-based generation (where incompleteknowledge is inevitable) and pure statistical bag gen-eration (Brown et al, 1993) (where the statisticalsystem has no linguistic guidance).Of course, the results are not perfect.
We can im-prove on them by enhancing the statistical model,or by incorporating more knowledge and constraintsin the lattices, possibly using automatic knowledgeacquisition methods.
One direction we intend topursue is the rescoring of the top N generated sen-tences by more expensive (and extensive) methods,incorporating for example stylistic features or ex-plicit knowledge of flexible collocations.AcknowledgmentsWe would like to thank Yolanda Gil, Eduard Hovy,Kathleen McKeown, Jacques Robin, Bill Swartout,and the ACL reviewers for helpful comments on ear-lier versions of this paper.
This work was supportedin part by the Advanced Research Projects Agency(Order 8073, Contract MDA904-91-C-5224) and bythe Department of Defense.Re ferencesJohn Bateman.
1990.
Upper modeling: A levelof semantics for natural language processing.
In7See also (Dagan and Itai, 1994) for a study of theuse of lexical co-occurrences to choose among open-classword translations.259Proc.
Fifth International Workshop on NaturalLanguage Generation, pages 54-61, Dawson, PA.Peter F. Brown, S. A. Della Pietra, V. J. DellaPietra, and R. L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2), June.Yen-Lu Chow and Richard Schwartz.
1989.
TheN-Best algorithm: An efficient search procedurefor finding top N sentence hypotheses.
In Proc.DARPA Speech and Natural Language Workshop,pages 199-202.Kenneth W. Church and William A. Gale.
1991.A comparison of the enhanced Good-Turing anddeleted estimation methods for estimating proba-bilities of English bigrams.
Computer Speech andLanguage, 5:19-54.Ido Dagan and Alon Itai.
1994.
Word sense dis-ambiguation using a second language monolingualcorpus.
Computational Linguistics, 20(4):563-596.Laurence Danlos.
1986.
The Linguistic Basis ofText Generation.
Studies in Natural LanguageProcessing.
Cambridge University Press.Michael Elhadad and Jacques Robin.
1992.
Con-trolling content realization with functional unifi-cation grammars.
In Robert Dale, Eduard Hovy,Dietmar RSsner, and Oliviero Stock, editors, As-pects of Automated Natural Language Generation,pages 89-104.
Springier Verlag.Michael Elhadad.
1993.
Using Argumentation toControl Lexical Choice: A Unification-Based Im-plementation.
Ph.D. thesis, Computer ScienceDepartment, Columbia University, New York.Karin Harbusch, Gen-ichiro Kikui, and Anne Kil-ger.
1994.
Default handling in incremental gener-ation.
In Proc.
COLING-9~, pages 356-362, Ky-oto, Japan.Vasileios Hatzivassiloglou and Kevin Knight.
1995.Unification-based glossing.
In Proc.
IJCALPhilip J. Hayes.
1981.
A construction-specific ap-proach to focused interaction in flexible parsing.In Proc.
ACL, pages 149-152.Annette Herskovits.
1986.
Language and spatialcognition: an interdisciplinary study of the prepo-sitions of English.
Studies in Natural LanguageProcessing.
Cambridge University Press.Kevin Knight and Ishwar Chander.
1994.
Auto-mated postediting of documents.
In Proc.
AAALKevin Knight and Steve K. Luk.
1994.
Buildinga large-scale knowledge base for machine transla-tion.
In Proc.
AAALKevin Knight, Ishwar Chander, Matthew Haines,Vasileios Hatzivassiloglou, Eduard Hovy, MasayoIida, Steve K. Luk, Akitoshi Okumura, RichardWhitney, and Kenji Yamada.
1994.
Integratingknowledge bases and statistics in MT.
In Proc.Conference of the Association for Machine Trans-lation in the Americas (AMTA).Kevin Knight, Ishwar Chander, Matthew Haines,Vasileios Hatzivassiloglou, Eduard Hovy, MasayoIida, Steve K. Luk, Richard Whitney, and KenjiYamada.
1995.
Filling knowledge gaps in a broad-coverage MT system.
In Proc.
IJCAI.Karen Kukich, K. McKeown, J. Shaw, J. Robin,N.
Morgan, and J. Phillips.
1994.
User-needsanalysis and design methodology for an auto-mated document generator.
In A. Zampolli,N.
Calzolari, and M. Palmer, editors, Current Is-sues in Computational Linguistics: In Honour ofDon Walker.
Kluwer Academic Press, Boston.Karen Kukich.
1988.
Fluency in natural languagereports.
In David D. McDonald and LeonardBolc, editors, Natural Language Generation Sys-tems.
Springer-Verlag, Berlin.Alon Lavie.
1994.
An integrated heuristic schemefor partial parse evaluation.
In Proc.
A CL (stu-dent session).Marie W. Meteer, D. D. McDonald, S. D. Anderson,D.
Forster, L. S. Gay, A. K. Huettner, and P. St-bun.
1987.
Mumble-86: Design and implementa-tion.
Technical Report COINS 87-87, Universityof Massachussets at Amherst, Ahmerst, MA.George A. Miller.
1990.
Wordnet: An on-line lexicaldatabase.
International Journal of Lexicography,3(4).
(Special Issue).Penman.
1989.
The Penman documentation.
Tech-nical report, USC/Information Sciences Institute.Paul Procter, editor.
1978.
Longman Dictionary ofContemporary English.
Longman, Essex, UK.Jacques :Robin.
1995.
Revision-Based Generation ofNatural Language Summaries Providing HistoricalBackground: Corpus-based Analysis, Design, Im-plementation, and Evaluation.
Ph.D. thesis, Com-puter Science Department, Columbia University,New York, NY.
Also, Technical Report CU-CS-034-94.Frank Smadja and Kathleen R. McKeown.
1991.Using collocations for language generation.
Com-putational Intelligence, 7(4):229-239, December.M.
Tomita and E. Nyberg.
1988.
The GenKit andTransformation Kit User's Guide.
Technical Re-port CMU-CMT-88-MEMO, Center for MachineTranslation, Carnegie Mellon University.A.
Waibel and K. F. Lee, editors.
1990.
Readingsin Speech Recognition.
Morgan Kaufmann, SanMateo, CA.R.
Weischedel and J.
Black.
1980.
Responding topotentially unparseable sentences.
Am.
J. Com-putational Linguistics, 6.260
