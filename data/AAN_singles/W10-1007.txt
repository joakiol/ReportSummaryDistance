Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 49?56,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsPredicting Cloze Task Quality for Vocabulary TrainingAdam Skory Maxine EskenaziLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh PA 15213, USA{askory,max}@cs.cmu.eduAbstractComputer  generation  of  cloze  tasks  still  fallsshort of full automation; most current systemsare  used  by  teachers  as  authoring  aids.Improved methods to estimate cloze quality areneeded  for  full  automation.
We  investigatedlexical reading difficulty as a novel automaticestimator  of  cloze  quality,  to  which  co-occurrence  frequency of  words was comparedas an alternate estimator.
Rather than relying onexpert evaluation of cloze quality, we submittedopen  cloze  tasks  to  workers  on  AmazonMechanical  Turk (AMT) and discuss ways  tomeasure  of  the  results  of  these  tasks.
Resultsshow  one  statistically  significant  correlationbetween  the  above  measures  and  estimators,which  was  lexical  co-occurrence  and  ClozeEasiness.
Reading difficulty was not found tocorrelate  significantly.
We  gave  subsets  ofcloze sentences to an English teacher as a goldstandard.
Sentences  selected  by co-occurrenceand Cloze Easiness  were  ranked most  highly,corroborating the evidence from AMT.1 Cloze TasksCloze  tasks,  described  in  Taylor  (1953),  areactivities  in  which  one  or  several  words  areremoved from a sentence and a student is asked tofill  in the missing content.
That  sentence can bereferred  to  as  the  'stem',  and  the  removed  termitself as the 'key'.
(Higgins, 2006)  The portion ofthe sentence from which the key has been removedis the 'blank'.
'Open cloze' tasks are those in whichthe student can propose any answer.
'Closed cloze'describes multiple choice tasks in which the key ispresented along with a set of several 'distractors'.1.1 Cloze Tasks in AssessmentAssessment is the best known application of clozetasks.
As described in (Alderson, 1979), the ?clozeprocedure?
is  that  in  which  multiple  words  areremoved at  intervals  from a text.
This  is  mostlyused  in  first  language  (L1)  education.
Aldersondescribes  three  deletion  strategies:  randomdeletion, deletion of every nth word, and targeteddeletion,  in  which  certain  words  are  manuallychosen and deleted by an instructor.
Theories  oflexical  quality  (Perfetti  & Hart,  2001)  and wordknowledge levels (Dale, 1965) illustrate why clozetasks can effectively assess multiple dimensions ofvocabulary knowledge.Perfetti & Hart explain that lexical knowledgecan  be  decomposed  into  orthographic,  phonetic,syntactic,  and  semantic  constituents.
The  lexicalquality of a given word can then be defined as ameasure based on both the depth of knowledge ofeach  constituent  and  the  degree  to  which  thoseconstituents are bonded together.
Cloze tasks allowa test author to select for specific combinations ofconstituents to assess (Bachman, 1982).1.2 Instructional Cloze TasksCloze tasks can be employed for instruction as wellas  assessment.
Jongsma  (1980)  showed  thattargeted deletion is an effective use of instructionalpassage-based  cloze  tasks.
Repeated  exposure  tofrequent words leads first to familiarity with thosewords, and increasingly to suppositions about theirsemantic  and  syntactic  constituents.
Producingcloze tasks through targeted deletion takes implicit,receptive word knowledge, and forces the student49to consider explicitly how to match features of thestem with  what  is  known about  features  of  anykeys she may consider.2 Automatic Generation of Cloze TasksMost  cloze  task  ?generation?
systems  are  reallycloze task  identification systems.
That is, given aset  of  requirements,  such  as  a  specific  key  andsyntactic structure (Higgins 2006) for the stem, asystem looks into a database of pre-processed textand attempts to identify sentences matching thosecriteria.
Thus,  the content  generated for a closedcloze is the stem (by deletion of the key), and a setof  distractors.
In  the  case  of  some  systems,  ahuman  content  author  may  manually  tailor  theresulting stems to meet further needs.Identifying  suitable  sentences  from  naturallanguage  corpora  is  desirable  because  thesentences  that  are  found  will  be  authentic.Depending  on  the  choice  of  corpora,  sentencesshould also be well-formed and suitable in terms ofreading level and content.
Newspaper text is onepopular source (Hoshino & Nakagawa, 2005; Liuet  al.,  2005;  Lee  &  Seneff,  2007).
Pino  et  al.
(2008)  use  documents  from  a  corpus  of  textsretrieved  from  the  internet  and  subsequentlyfiltered  according  to  readability  level,  category,and  appropriateness  of  content.
Using  a  broadercorpus  increases  the  number  and  variability  ofpotential  matching sentences, but also lowers theconfidence that sentences will be well-formed andcontain appropriate language (Brown & Eskenazi,2004).2.1 Tag-based Sentence SearchSeveral cloze item authoring tools (Liu et al 2005;Higgins,  2006)  implement  specialized  tag-basedsentence  search.
This  goes  back  to  the  originaldistribution  of  the  Penn  Treebank  and  thecorresponding  tgrep program.
Developed by Pitoin  1992  (Pito,  1994)  this  program  allowsresearchers to search for corpus text according tosequences  of part  of  speech (POS) tags  and treestructure.The linguists' Search Engine (Resnik & Elkiss,2005)  takes  the  capabilities  of  tgrep yet  further,providing  a  simplified  interface  for  linguists  tosearch within tagged corpora along both syntacticand lexical features.Both  tgrep  and  the  Linguists'  Search  Enginewere not designed as cloze sentence search tools,but they paved the way for similar tools specializedfor this task.
For example, Higgins' (2006) systemuses  a  regular  expression  engine  that  can  workeither on the tag level, the text level or both.
Thisallows  test  content  creators  to  quickly  findsentences  within  very  narrow  criteria.
They  canthen alter these sentences as necessary.Liu et al (2005) use sentences from a corpus ofnewspaper  text  tagged  for  POS  and  lemma.Candidate sentences are found by searching on thekey and its POS as well as the POS sequence ofsurrounding  terms.
In  their  system  results  arefiltered for proper word sense by comparing otherwords  in  the  stem with data  from WordNet  andHowNet,  databases  of  inter-word  semanticrelations.2.2 Statistical Sentence SearchPino et al(2009) use co-occurrence frequencies toidentify  candidate  sentences.
They  used  theStanford Parser (Klein & Manning, 2003) to detectsentences within a desired range of complexity andlikely well-formedness.
Co-occurrence frequenciesof words in the corpus were calculated and keyswere  compared  to  other  words  in  the  stem  todetermine  cloze quality,  producing suitable clozequestions  66.53%  of  the  time.
This  methodoperates  on  the  theory  that  the  quality  of  thecontext  of  a  stem is  based on  the  co-occurrencescores of other words in the sentence.
Along withthis  result,  Pino  et  al.
incorporated  syntacticcomplexity in terms of the number of parses found.Hoshino  &  Nakagawa  (2005)  use  machinelearning  techniques  to  train  a  cloze  task  searchsystem.
Their system, rather than finding sentencessuitable  for  cloze  tasks,  attempts  to  automatedeletion for passage-based cloze.
The features usedinclude  sentence  length  and  POS  of  keys  andsurrounding words.
Both a Na?ve Bayes and a K-Nearest Neighbor classifier were trained to find themost likely words for deletion within news articles.To train the system they labeled cloze sentencesfrom a TOEIC training test as true, then shifted theposition  of  the  blanks  from those  sentences  and50labeled  the  resulting  sentences  as  false.
Manualevaluation  of  the  results  showed  that,  for  bothclassifiers, experts saw over 90% of the deletionsas either easy to solve or merely possible to solve.3 Reading Level and Information TheoryAn  information-theoretical  basis  for  an  entirelynovel approach to automated cloze sentence searchis  found  in  Finn  (1978).
Finn  defines  ClozeEasiness as ?the percent of subjects filling in thecorrect word in a cloze task.?
Another metric of thequality of  a  cloze task is  context  restriction;  thenumber of solutions perceived as acceptable keysfor a given stem.
Finn's theory of lexical  featuretransfer  provides  one  mechanism  to  explaincontext  restriction.
The  theory  involves  theinformation content of a blank.According to Shannon's  (1948) seminal  workon information theory,  the  information containedin a given term is inverse to its predictability.
Inother words, if a term appears despite following ahistory after which is it considered very unlikely tooccur, that word has high information content.
Forexample, consider the partial sentence ?She drivesa  nice...?.
A  reader  forms  hypotheses  about  thenext  word  before  seeing  it,  and  thus  expects  anoverall  meaning  of  the  sentence.
A  word  thatconforms to this hypothesis, such as the word 'car',does little to change a reader's knowledge and thushas little  information.
If instead the next word is'taxi', 'tank', or 'ambulance', unforeseen knowledgeis gained and relative information is higher.According to Finn (1978) the applicability ofthis  theory  to  Cloze  Easiness  can  be  explainedthough lexical transfer features.
These features canbe both syntactic and semantic, and they serve tointerrelate  words  within  a  sentence.
If  a  largenumber  of  lexical  transfer  features  are  within  agiven proximity of a blank, then the set of wordsmatching those features will  be highly restricted.Given that each choice of answer will  be from asmaller  pool  of  options,  the  probability  of  thatanswer  will  be  much  higher.
Thus,  a  highlyprobable key has correspondingly low informationcontent.Predicting  context  restriction  is  of  benefit  toautomatic  generation  of  cloze  tasks.
ClozeEasiness  improves  if  a  student  chooses  from  asmaller set of possibilities.
The instructional valueof  a  highly  context-restricted  cloze  task  is  alsohigher by providing a richer set of lexical transferfeatures with which to associate vocabulary.Finn's  application  of  information  theory  toCloze Easiness and context restriction provides onepossible  new  avenue  to  improve  the  quality  ofgenerated cloze tasks.
We hypothesize that wordsof higher reading levels contain higher numbers oftransfer  features  and  thus  their  presence  in  asentence  can  be  correlated  with  its  degree  ofcontext  restriction.
To  the  authors'  knowledgereading  level  has  not  been previously applied  tothis problem.We can use a unigram reading level model toinvestigate  this  hypothesis.
Returning  to  theexample words for the partial sentence ?She drivesa  nice...?,  we  can  see  that  our  current  modelclassifies the highly expected word, 'car', at readinglevel  1,  while 'taxi','tank',  and 'ambulance',  are atreading levels 5, 6, and 11 respectively.3.1 Reading Level EstimatorsThe estimation of reading level is a complex topicunto  itself.
Early  work  used  heuristics  based  onaverage  sentence  length  and  the  percentage  ofwords deemed unknown to a baseline reader.
(Dale& Chall, 1948; Dale, 1965) Another early measure,the Flesch-Kincaid measure, (Kincaid et al, 1975)uses a function of the syllable length of words in adocument and the average sentence length.More recent work on the topic also focuses onreadability  classification  at  the  document  level.Collins-Thompson  & Callan  (2005)  use  unigramlanguage  models  without  syntactic  features.Heilman et al (2008) use a probabilistic parser andunigram language models to combine grammaticaland lexical features.
(Petersen & Ostendorf, 2006)add higher-order n-gram features to the above totrain  support  vector  machine  classifiers  for  eachgrade level.These  recent  methods  perform  well  tocharacterize the level  of  an entire  document,  butthey are untested for single sentences.
We wish toinvestigate if  a  robust  unigram model  of readinglevel can be employed to improve the estimation ofcloze quality at the sentence level.
By extension ofFinn's  (1978)  hypothesis,  it  is  in  fact  not  the51overall  level  of  the sentence that has a predictedeffect  on cloze context  restriction,  but  rather  thereading  level  of  the  words  in  proximity  to  theblank.
Thus we propose that it should be possibleto find a correlation between cloze quality and thereading levels of words in near context to the blankof a cloze task.4 The ApproachWe investigate a multi-staged filtering approach tocloze sentence generation.
Several variations of thefinal filtering step of this approach were employedand correlations sought between the resulting setsof  each  filter  variation.
The  subset  predicted  tocontain the best sentences by each filter was finallysubmitted to expert review as a gold standard testof cloze quality.This study compares two features of sentences,finding  the  levels  of  context  restrictionexperimentally.
The first feature in question is themaximum reading level  found in near-context  tothe  blank.
The  second  feature  is  the  mean  skipbigram co-occurrence score  of words within thatcontext.Amazon Mechanical Turk (AMT) is used as anovel  cloze  quality  evaluation  method.
Thismethod  is  validated  by  both  positive  correlationwith   the  known-valid  (Pino  et  al.,  2008)  co-occurrence  score  predictor,  and  an  expert  goldstandard.
Experimental results from AMT are thenused to evaluate the hypothesis that reading levelcan be used as a new, alternative predictor of clozequality.4.1 Cloze Sentence FilteringThe first step in preparing material for this studywas to obtain a set of keys.
We expect that in mostapplications of sentence-based cloze tasks the setof  keys  is  pre-determined  by instructional  goals.Due  to  this  constraint,  we  choose  a  set  of  keysdistributed across several reading levels and hold itas fixed.
Four words were picked from the set ofwords common in texts labeled as grades four, six,eight, ten, and twelve respectively.201,025  sentences  containing  these  keys  wereautomatically  extracted  from  a  corpus  of  webdocuments  as  the  initial  filtering  step.
Thiscollection  of  sentences  was  then  limited  tosentences of length 25 words or less.
Filtering bysentence  length  reduced  the  set  to  136,837sentences.A probabilistic  parser  was  used  to  score  eachsentence.
This parser gives log-probability valuescorresponding to confidence of the best parse.
Athreshold  for  this  confidence  score  was  chosenmanually  and  sentences  with  scores  below  thethreshold were removed,  reducing the number  ofsentences to 29,439.4.2 Grade LevelGrade  level  in  this  study  is  determined  by  asmoothed  unigram  model  based  on  normalizedconcentrations  within  labeled  documents.
Asentence is assigned the grade level of the highestlevel word in context of the key.4.3 Co-occurrence ScoresSkip bigram co-occurrence counts were calculatedfrom  the  Brown  (Francis  &  Kucera,  1979)  andOANC (OANC, 2009) corpora.
A given sentence'sscore is calculated as the mean of the probabilitiesof finding that sentence's context for the key.These  probabilities  are  defined  on  the  triplet(key, word, window size), in which key is the targetword to be removed, word any term in the corpus,and  window size is a positive integer less than orequal to the length of the sentence.This probability is estimated as the number  oftimes  word is found within the same sentence askey and  within  an  absolute  window  size  of 2positions from key, divided by the total number oftimes  all  terms  are  found in that  window.
Thesescores are thus maximum likelihood estimators ofthe probability of word given key and window size:4th: 'little', 'thought', 'voice',  'animals'6th: ?president', 'sportsmanship', 'national',  experience'8th: 'college', 'wildlife', 'beautiful', 'competition'10th: 'medical', 'elevations','qualities', 'independent'12th: 'scientists',  'citizens', 'discovered', 'university'Figure 1: common words per grade level.52(1) For some key k , word w, and window-size m :Cj(w, k) := count of times w found j words from theposition of k, within the same sentence.
(2) For a vocabulary V and for some positive integerwindow-size m, let n = (m-1) / 2, then:i.e.
if our corpus consisted of the single sentence?This is a good example sentence.?
:C?1 (w = good, k = example) = 1C1 (w = sentence, k = example) = 1P (w = good | k = example, m = 3) =  1 / (1+1)= .5Finally, the overall score of the sentence is takento be the mean of the skip bigram probabilities ofall words in context of the key.4.4 Variable Filtering by Grade and ScoreSkip bigram scores were calculated for all wordsco-occurrent  in  a  sentence  with  each  of  our  20keys.
To maximize the observable effect of the twodimensions of grade level and co-occurrence score,the  goal  was  to  find  sentences  representingcombinations  of  ranges  within those  dimensions.To  achieve  this  it  was  necessary  to  pick  thewindow size that best  balances variance of thesedimensions  with a  reasonably flat  distribution ofsentences.In terms  of  grade level,  smaller  window sizesresulted  in  very few sentences  with  at  least  onehigh-level  word,  while  larger  window  sizesresulted in few sentences with no high-level words.Variance  in  co-occurrence  score,  on  the  otherhand, was maximal at a window size of 3 words,and  dropped  off  until  nearly  flattening  out  at  awindow size  of  20 words.
A window size  of  15words was found to offer a reasonable distributionof grade level while preserving sufficient varianceof co-occurrence score.Using the above window-size, we created filtersaccording to maximum grade level:  one each forthe grade ranges 5-6, 7-8, 9-10,  and 11-12.
Fourmore  filters  were  created  according  to  co-occurrence score: one selecting the highest-scoringquartile  of  sentences,  one  the  second  highest-scoring quartile, and so on.
Each grade level filterwas combined with each co-occurrence score filtercreating 4x4=16 composite filters.
By combiningthese filters we can create a final set of sentencesfor  analysis  with  high  confidence  of  having  asignificant  number  of  sentences  representing  allpossible values  of grade level  and co-occurrencescore.
At most two sentences were chosen for eachof the 20 keys using these composite filters.
Thefinal number of sentences was 540.4.5 Experimental Cloze QualityPrevious  evaluation  of  automatically  generatedcloze tasks has relied on expert judgments.
(Pino etal., 2008; Liu et al, 2005) We present the use ofcrowdsourcing techniques as a new approach forthis  evaluation.
We believe  the approach can bevalidated  by  statistically  significant  correlationswith predicted cloze quality and comparison withexpert judgments.The  set  of  540  sentences  were  presented  toworkers  from Amazon  Mechanical  Turk  (AMT),an  online  marketplace  for  ?human  intelligencetasks.?
Each worker was shown up to twenty of thestems of these sentences as open cloze tasks.
Noworker was allowed to see more than one stem forthe  same  key.
Workers  were  instructed  to  enteronly those words that  ?absolutely make  sense inthis context?, but were not encouraged to submitany particular  number  of answers.
Workers werepaid US$.04 per sentence, and the task was limitedto workers with approval ratings on past tasks at orabove 90%.For  each  sentence  under  review  each  workercontributes one subset of answers.
Cloze Easiness,as  defined  by  Finn  (1978)  is  calculated  as  thepercentage of these subsets containing the originalkey.
We  define  context  restriction on  n as  thepercentage of answer subsets containing n or fewerwords.Using the example sentence: ?Take this clozesentence, for    (example)  .?
We can find the set ofanswer subsets A:A  =  {  A1={example, free, fun, me}A2={example,instance}A3={instance}     }Then, Cloze Easiness is |{A1,A2}| / |A| ?
.67 andContext restriction (on one or two words) is |{A2,A3}| / |A| ?
.67535 ResultsEach sentence in the final set was seen, on average,by  27  Mechanical  Turk  workers.
We  wish  tocorrelate measures of Cloze Easiness and contextrestriction  with  cloze  quality  predictors  ofmaximum  grade  level  and  score.
We  use  thePearson correlation  coefficient  (PCC)  to  test  thelinear relationship between each measure of clozequality and each predictor.Table (1)  shows these PCC values.
All of  thevalues are positive, meaning there is a correlationshowing that one value will tend to increase as theother increases.
The strongest correlation is that ofco-occurrence and Cloze Easiness.
This is also theonly statistically significant correlation.
The valueof  P(H0)  represents  the  likelihood  of  the  nullhypothesis:  that  two  random  distributionsgenerated  the  same  correlation.
Values  of  P(H0)under  0.05  can  be  considered  statisticallysignificant.Figure  (3)  shows  scatter  plots  of  these  fourcorrelations  in  which  each  dot  represents  onesentence.The top-leftmost  plot  shows the correlation ofco-occurrence  score  (on  the  x-axis),  and  ClozeEasiness (on the y-axis).
Co-occurrence scores areshown on a log-scale.
The line through these pointsrepresents a linear regression, which is in this casestatistically significant.The  bottom-left  plot  shows  correlation  of  co-occurrence score (x-axis) with context restriction.In this case context  restriction was calculated onn=2,  i.e.
the  percent  of  answers  containing  onlyCloze Easiness PCC = 0.2043P(H0)=1.6965e-06PCC = 0.0671P(H0)=0.1193ContextRestriction (2)PCC = 0.0649P(H0)=0.1317PCC = 0.07P(H0)=0.1038Co-occurrence Maximum GradeTable (1): Pearson Correlation Coefficient andprobability of null hypothesis for estimators andmeasures of cloze quality.Figure (3): Scatter plots of all sentences with cloze quality measure as y-axis, and cloze quality estimator as x-axis.The linear regression of each distribution is shown.54one  or  two  words.
The  linear  regression  showsthere  is  a  small  (statistically  insignificant)correlation.The  top-right  plot  shows  Cloze  Easiness  (y-axis)  per  grade  level  (x-axis).
The  bottom  leftshows context restriction (y-axis) as a function ofgrade level.
In both cases linear regressions herealso show small, statistically insignificant positivecorrelations.The lack of significant correlations for three outof four combinations of measures and estimators isnot grounds to dismiss these measures.
Across allsentences,  the  measure  of  context  restriction  ishighly variant, at 47.9%.
This is possibly the resultof the methodology; in an attempt to avoid biasingthe AMT workers, we did not specify the desirablenumber  of  answers.
This  led  to  many  workersinterpreting the task differently.In terms of maximum grade level, the lack of asignificant correlation with context restriction doesnot  absolutely  refute  Finn  (1978)'s  hypothesis.Finn  specifies  that  semantic  transfer  featuresshould be in  ?lexical  scope?
of a  blank.
A cleardefinition of ?lexical scope?
was not presented.
Wegeneralized scope to mean proximity within a fixedcontextual window size.
It is possible that a moreprecise definition of ?lexical scope?
will provide astronger  correlation  of  reading  level  and  contextrestriction.5.1 Expert ValidationFinally,  while  we  have  shown  a  statisticallysignificant  positive  correlation  between  co-occurrence  scores  and  Cloze  Easiness,  we  stillneed to demonstrate that Cloze Easiness is a validmeasure of cloze quality.
To do so, we selected theset  of  20  sentences  that  ranked  highest  by  co-occurrence score and by Cloze Easiness to submitto expert evaluation.
Due to overlap between thesetwo  sets,  choosing  distinct  sentences  for  bothwould  require  choosing  some  sentences  rankedbelow the top 20 for each category.
Accordingly,we  chose  to  submit  just  one  set  based  on  bothcriteria in combination.Along with these 20 sentences, as controls, wealso  selected  two  more  distinct  sets  of  20sentences:  one  set  of  sentences  measuring  mosthighly  in  context  restriction,  and  one  set  mosthighly estimated by maximum grade level.We asked a former English teacher to read eachopen cloze,  without  the  key,  and rate,  on  a  fivepoint  Likert  scale,  her  agreement  with  thestatement  ?This  is  a  very  good  fill-in-the-blanksentence.?
where 1 means strong agreement, and 5means strong disagreement.Expert evaluation on5-point ScaleMean StandardDeviation20bestsentencesasdeterminedby:Cloze Easiness and co-occurrence score 2.25 1.37Context restriction 3.05 1.36Maximum grade level 3.15 1.2Table (2): Mean ratings for each sentence category.The results in Table (2) show that, on average,the correlated results of selecting sentences basedon Cloze Easiness and co-occurrence score are infact rated more highly by our expert as comparedto sentences selected based on context restriction,which is, in turn, rated more highly than sentencesselected  by maximum grade  level.
Using  a  one-sample t-test and a population mean of 2.5, we finda p-value of .0815 for our expert's ratings.6 ConclusionWe  present  a  multi-step  filter-based  paradigmunder which diverse estimators of cloze quality canbe applied towards the goal of full automation ofcloze  task  generation.
In  our  implementation  ofthis  approach sentences  were  found  for  a  set  ofkeys,  and  then  filtered  by  maximum  length  andlikelihood  of  well-formedness.
We  then  testedcombinations  of  two  estimators  and  twoexperimental measures of cloze quality for the nextfiltering step.We presented an information-theoretical  basisfor the use of reading level as a novel estimator forcloze quality.
The hypothesis that maximum gradelevel should be correlated with context restrictionwas  not,  however,  shown  with  statisticalsignificance.
A  stronger  correlation  might  beshown with a different experimental methodologyand a more refined definition of lexical scope.55As an alternative to expert evaluation of clozequality,  we  investigated  the  use  of  non-expertworkers  on  AMT.
A  statistically  significantcorrelation was found between the co-occurrencescore of a sentence and its experimental measure ofCloze  Easiness.
This  is  evidence  thatcrowdsourcing  techniques  agree  with  expertevaluation of co-occurrence scores in past studies.To gain further evidence of the validity of theseexperimental  results,  sentences  selected  by  acomposite filter of co-occurrence score and ClozeEasiness were compared to sentences selected bycontext  restriction  and  reading  level.
An  expertevaluation  showed  a  preference  for  sentencesselected by the composite filter.We  believe  that  this  method  of  cloze  taskselection is promising.
It will now be tested in areal  learning  situation.
This  work  contributesinsight  into  methods  for  improving  technologiessuch as intelligent tutoring systems and languagegames.ReferencesAlderson,  J.  C.  (1979).
The  Cloze  Procedure  andProficiency  in  English  as  a  Foreign  Language.
TESOLQuarterly, 13(2), 219-227. doi: 10.2307/3586211.Bachman, L. F. (1982).
The Trait Structure of Cloze TestScores.
TESOL Quarterly, 16(1), 61.Brown, J., & Eskenazi, M. (2004).
Retrieval of AuthenticDocuments  for  Reader-Specific  Lexical  Practice.
InInSTIL/ICALL Symposium (Vol.
2).
Venice, Italy.Collins-Thompson,  K.,  &  Callan,  J.
(2005).
Predictingreading difficulty with statistical language models.
Journalof  the  American  Society  for  Information  Science  andTechnology, 56(13), 1448-1462.Dale, E. (1965).
Vocabulary measurement: Techniques andmajor findings.
Elementary English, 42, 395-401.Dale, E., & Chall, J. S. (1948).
A Formula for PredictingReadability:  Instructions.
Educational  Research  Bulletin,Vol.
27(2), 37-54.Finn,  P.  J.
(1978).
Word  frequency,  information  theory,and  cloze  performance:  A  transfer  feature  theory  ofprocessing in reading.
Reading Research Quarterly, 13(4),508-537.Francis,  W.  N.  &  Kucera,  H.  (1979).
Brown   CorpusManual,  Brown  University  Department  of  Linguistics.Providence, RIHeilman,  M.,  Collins-Thompson,  K.,  &  Eskenazi,  M.(2008).
An Analysis of Statistical Models and Features forReading  Difficulty  Prediction.
3rd  Workshop  onInnovative  Use  of  NLP  for  Building  EducationalApplications.
Assoc.
for Computational Linguistics.Higgins,  D.  (2006).
Item  Distiller:  Text  retrieval  forcomputer-assisted test item creation.
ETS, Princeton, NJ.Hoshino,  A.,  &  Nakagawa,  H.  (2005).
A  real-timemultiple-choice question generation for language testing ?a  preliminary  study?.
In  2nd  Workshop  on  BuildingEducational  Applications  Using  NLP (pp.
17-20).
AnnArbor, MI: Association for Computational Linguistics.Jongsma, E. (1980).
Cloze instructional research: A secondlook.
Newark,  DE:  International  Reading  Association.Urbana, IL.Kincaid,  J.,  Fishburne,  R.,  Rodgers,  R.,  &  Chissom,  B.(1975).
Derivation  of  new readability  formulas  for  navyenlisted  personnel.
Research  Branch  Report.
Millington,TN.Klein, D. & Manning, C. (2003).
Accurate UnlexicalizedParsing.
(pp.
423-430) In Proceedings of the 41st Meetingof the Assoc.
for Computational Linguistics.Lee,  J.,  &  Seneff,  S.  (2007).
Automatic  Generation  ofCloze Items for Prepositions.
Proceedings of In.Liu,  C.,  Wang,  C.,  Gao,  Z.,  &  Huang,  S.  (2005).Applications  of  Lexical  Information  for  AlgorithmicallyComposing Multiple-Choice Cloze Items.
In  Proceedingsof the 2nd Workshop on Building Educational ApplicationsUsing  NLP (p.  1?8).
Ann  Arbor,  MI:  Association  forComputational Linguistics.Open  American  National  Corpus  (2009)americannationalcorpus.org/OANC/Perfetti,  C.,  &  Hart,  L.  (2001).
Lexical  bases  ofcomprehension skill.
(D. Gorfein) (pp.
67-86).
WashingtonD.C.
: American Psychological Association.Petersen,  S.  E.,  &  Ostendorf,  M.  (2006).
Assessing  thereading level of web pages.
In ICSLP (Vol.
pages, pp.
833-836).Pino, J., Heilman, M., & Eskenazi, M. (2008).
A SelectionStrategy  to  Improve  Cloze  Question  Quality.
InProceedings  of  the  Workshop  on  Intelligent  TutoringSystems for Ill-Defined Domains.Pito, R. (1994).
tgrep READMEwww.ldc.upenn.edu/ldc/online/treebank/README.longResnik,  P.,  &  Elkiss,  A.
(2005).
The  Linguist?s  SearchEngine:  An  Overview.
Association  for  ComputationalLinguistics, (June), 33-36.Shannon,  C.  (1948).
A  Mathematical  Theory  ofCommunication.
Bell System Technical Journal,  27, 379?423, 623?656.Taylor,  W.  L.  (1953).
Cloze  procedure:  a  new  tool  formeasuring readability.
Journalism Quarterly, 30, 415-453.56
