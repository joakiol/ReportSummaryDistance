ACQUIRING CORE MEANINGS OF WORDS, REPRESENTED ASJACKENDOFF-STYLE CONCEPTUAL STRUCTURES, FROMCORRELATED STREAMS OF LINGUISTIC AND NON-LINGUISTICINPUTJeffrey Mark Siskind*M. I. T. Artificial Intelligence Laboratory545 Technology Square, Room NE43-800bCambridge MA 02139617/253-5659internet: Qobi~AI.MIT.EDUAbstractThis paper describes an operational system whichcan acquire the core meanings of words without anyprior knowledge of either the category or meaningof any words it encounters.
The system is givenas input, a description of sequences of scenes alongwith sentences which describe the \ [EVENTS\]  takingplace as those scenes unfold, and produces as out-put, a lexicon consisting of the category and mean-ing of each word in the input, that allows the sen-tences to describe the \[EVENTS\].
It is argued, thateach of the three main components of the system, theparser, the linker and the inference component, makeonly linguistically and cognitively plausible assump-tions about the innate knowledge needed to supporttractable learning.
The paper discusses the theoryunderlying the system, the representations and al-gorithms used in the implementation, the semanticconstraints which support the heuristics necessaryto achieve tractable learning, the limitations of thecurrent theory and the implications of this work forlanguage acquisition research.1 IntroductionSeveral natural language systems have been reportedwhich learn the meanings of new words\[5, 7, 1, 16,17, 13, 14\].
Many of these systems (in particular\[5, 7, 1\]) learn the new meanings based upon expec-tations arising from the morphological, syntactic, se-*Supported by an AT&T Bell Laboratories Ph.D. scholar-ship.
Part of this research was performed while the author wasvisiting Xerox PARC as a research intern and as a consultant.mantic and pragmatic ontext of the unknown wordin the text being processed.
For example, if such asystem encounters the sentence "I woke up yesterday,turned off my alarm clock, took a shower, and cookedmyself two grimps for breakfast\[5\]" it might concludethat grimps is a noun which represents a type offood.
Such systems ucceed in learning new wordsonly when the context offers sufficient constraint tonarrow down the possible meanings to make the ac-quisition unambiguous.
Accordingly, such a theoryaccounts only for the type of learning which ariseswhen an adult encounters an unknown word whilereading a text comprised mostly of known words.
Itcan not explain the kind of learning which a youngchild performs during the early stages of languageacquisition when it starts out knowing the meaningsof few if any words.In this paper, I present a new theory which canaccount for the language learning which a child ex-hibits.
In this theory, the learner is presented witha training session consisting of a sequence of sce-narios.
Each scenario contains both linguistic andnon-linguistic (i.e.
visual) information.
The non-linguistic information for each scenario consists ofa time-ordered sequence of scenes, each depicted viaa conjunction of true and negated atomic formulasdescribing that scene.
Likewise, the linguistic infor-mation for each scenario consists of a time-orderedsequence of sentences.
Initially, the learner knowsnothing about the words comprising the sentences inthe training session, neither their lexical category northeir meaning.
From the two correlated sources of in-put, the linguistic and the non-linguistic, the learnercan infer the set of possible lexicons (i.e.
the possible143categories and meanings of the words in the linguisticinput) which allow the linguistic input to describe oraccount for the non-linguistic input.
This inferenceis accomplished by applying a compositional seman-tics linking rule in reverse and then performing someconstraint satisfaction.This theory has been implemented in a workingcomputer program.
The program succeeds and istractable because of a small number of judicious se-mantic constraints and a small number of heuristicswhich order and eliminate much of the search.
Thispaper explains the general theory as well as the im-plementation details which make it work.
In ad-dition, it discusses some limitations in the currenttheory, among which is one which prevents it fromconverging on a single definition of some words.2 BackgroundIn \[15\], Rayner et.
al.
describe a system whichcan determine the lexical category of each wordin a corpus of sentences.
They observe thatwhile in the original formulation, a definite clausegrammar\[12\] normally defines a two-argument pred-icate parser(Sentence,Tree)  with the lexicon rep-resented irectly in the clauses of the grammar, analternative formulation would allow the lexicon to berepresented explicitly as an additional argument tothe parser elation, yielding a three argument predi-cate paxser(Sentence,Tree,Lexicon).
This threeargument relation can be used to learn lexical cate-gory information by a technique summarized in Fig-ure I.
Here, a query is formed containing a conjunc-tion of calls to the parser, one for each sentence inthe corpus.
All of the calls share a common Lexicon,while in each call, the Tree is left unbound.
TheLexicon is initialized with an entry for each wordappearing in the corpus where the lexical categoryof each such initial entry is left unbound.
The pur-pose of this initial lexicon is to enforce the constraintthat each word in the corpus be assigned a uniquelexical category.
This restriction, the monosemy con-straint, will play an important role in the work wedescribe later.
The result of issuing the query in theabove example is a lexicon, with instantiated lexicalcategories for each lexical entry, such that with thatlexicon, all of the words in the corpus can be parsed.Note that there could be several such lexicons, eachproduced by backtracking.In this paper we extend the results of Rayner et.al.
to the learning of representations of word mean-ings in addition to lexical category information.
Ourtheory is implemented in an operational computerprogram called MAIMRA.
1 Unlike Rayner et.
al.
'ssystem, which is given only a corpus of sentences asinput, MAIMRA is given two correlated streams ofinput, one linguistic and one non-linguistic, the latermodeling the visual context in which the former wereuttered.
This is intended to more closely model thekind of learning exhibited by a child with no priorlexical knowledge.
The task faced by MAIMRA is il-lustrated in Figure 2.MAIMRA does not attempt to solve the perceptionproblem; both the linguistic and non-linguistic nputare presented in symbolic form to MAIMRA.
Thus,the session given in Figure 2 would be presented toMAIMRA as the following two input pairs:(BE(cup, AT(John))A }-~BE(cup, AT(Mary)));(BE(cup, AT(Mary))A-~BE(cup, AT(John)))The cup slid from John fo Mary.
(BE(cup, AT(Mary))A }-~BE(cup, AT(Bill)));(BE(cup, AT(Bill))^-~BE(cup, AT(Mary) ) )The cup slid from Mary ~o Bill.MAIMRA attempts to infer both category and mean-ing information from input such as this.3 ArchitectureMAIMRA operates as a collection of modules whichmutually constrain various mental representations:The organization of these modules is illustrated inFigure 3.
Conceptually, each of the modules is non-directional; each module simply constrains the val-ues which may appear concurrently on each of itsinputs.
Thus the parser enforces a relation betweena time-ordered sequence of sentences and a corre-sponding time-ordered sequence of syntactic struc-tures or parse trees which are licensed by the lexi-cal category information from a lexicon.
The linkerimposes compositional semantics on the parse treesproduced by the parser, relating the meanings of in-dividual words found in the lexicon, to the meaningsof entire utterances, through the mediation of thesyntactic structures consistent with the parser.
Fi-nally, the inference component relates a time-orderedsequence of observations from the non-linguistic in-put, to a time-ordered sequence of semantic struc-tures which in some sense explain the non-linguisticinput.
The non-directional collection of modules can1MAIMRA, or t~lr~FJ, is the Aramaic word for word.144?- Lexicon - \ [entry( the,_ ) ,entry(cup,_) ,ent ry (s l id , _ ) ,entry(from,_) ,entry(john,_),entry(to,_) ,entry(mary,_),ent ry (b i l l , _ ) \ ] ,parser(\[the,cup,sl id,from,john,to,mary\] ,_,Lexicon),parser( \ [ the,cup,s l id, f rom,mary,to,bi l l \ ] ,_ ,Lexicon),parser( \ [ the,cup,s l id , f rom,bi l l , to , john\] ,_ ,Lexicon) .Lexicon = \ [entry(the,det) ,entry(cup,n),entry(s l id ,v) ,entry(from,p),entry(john,n),entry(to,p),entry(mary,n),entry(bill,n)\].Figure h The technique used by Rayner et.
al.
in \[15\] to acquire lexical category information from a corpusof sentences.Input:rlCeP~flOr m  ?BE(cup,A'r(John))A~B~cap J%T(Mary ))rllCUtO ?B~cup~%T(M~y)~The cup slid from John to Maryrso~mioB~cup ,AT(Mary))A-,BE(cup,AT{roll ))rm=elt$~'y am BNcu p,AT{,Bill )g"-BNcup &~Mary))The cup slid from Mary to Bill I!JOutput:The : DETcup : N \[Thing cup\]slia: v \[ v,nt GO(x,\[Path z\])\]from: P \[Path FROM(\[elace AT(x)\])\]lo: P \[Path TO(\[Place AT(x)\])\]John : N \[Thing John\]Mary : N \[Thing Mary\]Bill : N \[Thing Bill\]Figure 2: A sample learning session with MAIMRA.
MAIMRA is given the two scenarios as input.
Each sce-nario comprises linguistic information, in the form of a sequence of sentences, and non-linguistic information.The non-linguistic information is a sequence of conceptual structure \[STATE\] descriptions which describe asequence of visual scenes.
MAIMRA produces as output, a lexicon which allows the linguistic input to explainthe non-linguistic input.145lexiconFigure 3: The cognitive architecture used byMAIMRA.be used in three ways.
Given a lexicon and a se-quence of sentences as input, the architecture couldproduce as output, a sequence of observations whichare predicted by the sentences.
This corresponds tolanguage understanding.
Likewise, given a lexiconand a sequence of observations as input, the archi-tecture could produce as output, a sequence of sen-tences which explain the observations.
This corre-sponds to language generation.
Finally, given a se-quence of observations and a sequence of sentencesas input, the architecture could produce as output,a lexicon which allows the sentences to explain theobservations.
This last alternative, corresponding tolanguage acquisition, is what interests us here.Of the five mental representations used byMAIMRA, only three are externally visible, namelythe linguistic input, the non-linguistic nput and thelexicon.
Syntactic and semantic structures exist onlyinternal to MAIMRA and are not externally visible.When using the cognitive architecture from Figure 3for learning, the values of two of the mental rep-resentations, namely the sentences and the observa-tions, are deterministic, since they are fixed as input.The remaining three representations may be nonde-terministic; there may be multiple lexicons, syntac-tic structure sequences and semantic structure se-quences which are consistent with the fixed input.In general, each of the three modules alone providesonly limited constraint on the possible values for eachof the mental representations.
Thus taken alone, sig-nificant nondeterminism is introduced by each mod-ule in isolation.
Taken together however, the mod-ules offer much greater constraint on the mutuallyconsistent values for the mental representations, thusreducing the amount of nondeterminism.
Much ofthe success of MAIMRA hinges on efficient ways ofrepresenting this nondeterminism.Conceptually, MAIMRA could have been imple-mented using techniques similar to Rayner et.
al.'ssystem.
Such a naive implementation would directlyreflect the architecture given in Figure 3 and is il-lustrated in Figure 4.
The predicate aaimra wouldrepresent the conjunction of constraints introducedby the parser,  l inker  and in:ference modules, ul-timately constraining the mutually consistent val-ues for sentence and observation sequences and thelexicon.
Learning a lexicon would be accomplishedby forming a conjunction of queries to maimra,one for each scenario, where a single Lexicon isshared among the conjoined queries.
This lexi-con is a list of lexical entries, each of the formentry(Word,Category,Meaning).
The monosemyconstraint is enforced by initializing the Lexicon tocontain a single entry for each word, each entry hav-ing unbound Category and Heaning slots.
The re-sult of processing such a query would be bindings forthose Category and Heaning slots which allow theSentences to explain the Observations.The naive implementation is too inefficient to bepractical.
This inefficiency results from two sources:inefficient representation f nondeterministic valuesand non-directional computation.
Nondeterministicmental representations are expressed in the naive im-plementation via backtracking.
Expressing nonde-terminism this way requires that substructure sharedacross different alternatives for a mental representa-tion be multiplied out.
For example, if MAIMRA isgiven as input, a sequence of two sentences $1; S~,where the first sentence has n parses and the sec-ond m parses, then there would be m x n distinctvalues for the parse tree sequence produced by theparser for this sentence sequence.
Each such parsetree sequence would be represented as a distinctbacktrack possibility by the naive implementation.The actual implementation i stead represents thisnondeterminism explicitly as AND/OR trees and ad-ditionally factors out much of the shared commonsubstructure to reduce the size of the mental rep-resentations and the time needed to process them.As noted previously, the individual modules them-selves offer little constraint on the mental represen-tations.
A given sentence sequence corresponds tomany parse tree sequences which in turn correspondsto an even greater number of semantic structure se-quences.
Most of these are filtered out, only at theend by the inference component, because they donot correspond to the non-linguistic input.
Ratherthen have these modules operate as non-directed setsof constraints, direction-specific algorithms are usedwhich are tailored to producing the factored mentalrepresentations i  an efficient order.
First, the in-ference component is called to produce all semanticstructure sequences which correspond to the observa-tion sequence.
Then, the parser is called to produce146maiDra (Sentences, Lexicon, Observations ) : -parser (Sentences, Synt act icStructures, Lexicon),l inker  (Trees, ConceptualStructures, Lexicon),inference (ConceptualStructures, Observat ions).7-  Lexicon - \[entry(the,_,_),entry(cup .
.
.
.
),entry (sl id .
.
.
.
),entry(from .
.
.
.
),entry (john .
.
.
.
),entry (to .
.
.
.  )
,entry (mary .
.
.
.
),entry(bi l l  .
.
.
.
)\],mainLra( \[\[the, cup, sl id, from, john, to ,mary\] \ ,Lexicon,be (cup, at ( j ohn) ) R'be ( cup (at (mary)) ) :be (cup, at (mary) ) R'be (cup (at (john) ) ) ),maimra ( \[ \[the, cup, sl id, from,mary, to,bi l l \]  \],Lexicon,be ( cup, at (mary)) R-be (cup (at (b i l l ) )  ) ;be (cup, at (b i l l ) )  R-be (cup (at (mary) ) ) ) .=~Lexicon - \[entry (the, det, noSemant ics),entry (cup, n, cup),entry(slid,v,go(x, \[from(y) ,to(z)\]),entry (from, p, at (x)),entry(john,n, j ohn),entry (to ,p, at (x)),entry (mary,n, mary),entry(b i l l ,n ,b i l l ) \ ] .Figure 4: A naive implementation f the cognitive architecture from Figure 3 using techniques similar tothose used by Rayner et.
al.
in \[15\].all syntactic structure sequences which correspondto the sentence sequence.
Finally, the linking com-ponent is run in reverse to produce meanings of lex-ical items by correlating the syntactic and semanticstructure sequences previously produced.
The de-tails of the factored representation, and the algo-rithms used to create it, will be discussed in Sec-tion 5.Several of the mental representations u ed byMAIMRA require a method for representing semanticinformation.
We have chosen Jackendoff's theory ofconceptual structure, presented in \[6\], as our modelfor semantic representation.
It should be stressedthat although we represent conceptual structure viaa decomposition i to primitives much in the sameway as does Schank\[18\], unlike both Schank andJackendoff, we do not claim that any particular suchdecompositional theory is adequate as a basis for ex-pressing the entire range of human thought and themeanings of even most words in the lexicon.
Clearly,much of human experience is well beyond formaliza-tion within the current state of the art in knowledgerepresentation.
We are only concerned with repre-senting and learning the meanings of words describ-ing simple spatial movements of objects within thevisual field of the learner.
For this limited task, aprimitive decompositional theory such as Jackend-off's seems adequate.Conceptual structures appear within three of themental representations used by MAIMrtA.
First, thesemantic structures produced by the linker, as mean-ings of entire utterances, are represented as eitherconceptual structure \[STATE\] or \[EVENT\] descrip-tions.
Second, the observation sequence comprisingthe non-linguistic nput is represented asa conjunc-tion of true and negated \[STATE\] descriptions.
Only\[STATE\] descriptions appear in the observation se-quence.
It is the function of the inference componentto infer the possible \[EVENT\] descriptions whichaccount for the observed \[STATE\] sequences.
Fi-nally, meaning components of lexical entries are rep-resented as fragments of conceptual structure whichcontain variables.
The conceptual structure frag-ments are combined by the linker, filling in the vari-ables with other fragments, to produce the variablefree conceptual structures representing the meaningsof whole utterances from the meanings of their con-stituent words.4 Learning ConstraintsEach of the three modules implements some linguis-tic or cognitive theory, and accordingly, makes omeassumptions about what knowledge is innate andwhat can be learned.
Additionally, each module cur-rently implements only a simple theory and thus haslimitations on the linguistic and cognitive phenom-ena that it can account for.
This section discussesthe innateness assumptions and limitations of each147S --~g --.NP --,VPpp -.-,AUX{COMP} \[~\]{DEW} ~ {S\[NP\[VP\[PP}"{AUX} ~ {glNPIVPIPP }"\[~\] {g\[NPIVP\[PP}"{DOIBEI{MODALITOI{{MODALITO}} HAVE} {BE}}Figure 5: The context free grammar used byMAIMRA.
This grammar is motivated by X-theory.The head of each rule is enclosed in a box.
This headinformation is used by the linker.module in greater detail.4.1 The  ParserWhile MAIMRA can learn lexical category informa-tion required by the parser, the parser is given a fixedcontext-free grammar which is assumed to be innate.This fixed grammar used by MAIMRA is shown inFigure 5.
At first glance it might seem unreasonableto assume that the grammar given in Figure 5 isinnate.
A closer look however, reveals that the par-ticular context-free grammar we use is not entirelyarbitrary; it is motivated by X-theory\[2, 3\] whichmany linguists take to be innate.
Our grammar canbe derived from X-theory as follows.
We start with aversion of X-theory which allows non-binary branch-ing nodes and where maximal projections carry bar-level one (i.e.
XP is X--).
First, fix the parametersHEAD-first and SPEC-first to yield the prototyperule:XP ---* {XsPEc} X complement*.Second, instantiate this rule for each of the lexi-cal categories N, V and P viewing NSPEC as DET,VSPEC as AUX and making PSpEC degenerate.Third, add the rules for S and S stipulating thatis a maximal projection.
2 Fourth, declare all max-imal projections to be valid complements.
Finally,add in the derivation for the English auxiliary sys-tem.
Thus, our particular context-free grammar islittle more than instantiating X-theory with the En-glish lexical categories N, V and P, the English pa-rameters HEAD-first and SPEC-first and the Englishauxiliary system.2A more principled way of deriving the rides for S andfrom T-theory is given in \[4\]We make no claim that the syntactic theory im-plemented by MAIMRA is complete.
Many linguisticphenomena remain unaccounted for in our grammar,among them agreement, tense, aspect, adjectives, ad-verbs, negation, coordination, quantifiers, wh-words,pronouns, reference and demonstratives.
While thegrammar is motivated by GB theory, the only com-ponents of GB theory which have been implementedare T-theory and 0-theory.
(0-theory is enforced viathe linking rule discussed in the next subsection.
)Although future work may increase the scope andaccuracy of the syntactic theory incorporated intoMAIMRA, even the current limited grammar offersa sufficiently rich framework for investigating lan-guage acquisition.
It's most severe limitation is alack of subcategorization; the grammar allows nouns,verbs and prepositions to take any number of com-plements of any kind.
This causes the grammar toseverely overgenerate and results in a high degree ofnon-determinism in the representation f syntacticstructure.
It is interesting that despite the use of ahighly ambiguous grammar, the combination of theparser with the linker and inference component, o-gether with the non-linguistic context, provide suffi-cient constraint for the system to learn words quicklywith few training scenarios.
This gives evidence thatmany of the constraints normally assumed to be im-posed by syntax, actually result from the interplayof multiple modules in a broad cognitive system.4.2 The  L inkerThe linking component of MAIMRA implements asingle linking rule which is assumed to be innate.This rule is best illustrated by way of the exam-ple given in Figure 6.
Linking proceeds in a bottomup fashion from the leaves of the parse tree towardsits root.
Each node in the parse tree is annotatedwith a fragment of conceptual structure.
The anno-tation of leaf nodes comes from the meaning entry forthat word in the lexicon.
Every non-leaf node has adistinguished daughter called the head.
Knowledgeof which daughter node is the head for any givenphrasal category is assumed to be innate.
For thegrammar used by MAIMRA, this information is indi-cated in Figure 5 by the categories enclosed in boxes.The annotation of a non-leaf node is formed by copy-ing the annotation of its head daughter node, whichmay contain variables, and filling some of its variableslots with the annotation of the remaining non-headdaughters.
Note that this is a nondeterministic pro-cess; there is no stipulation of which variables getlinked to which complements.
Because of this non-determinism, there can be many linkings associated148with any given lexicon and parse tree.
In additionto this linking ambiguity, existence of multiple lexi-cal entries with different meanings for the same wordcan cause meaning ambiguity.A given variable may appear multiple times in afragment of conceptual structure.
The linking rulestipulates that when a variable is linked to an argu-ment, all instances of the same variable get linked tothat argument as well.
Additionally, the linking rulemaintains the constraint that the annotation of theroot node, as well as any node which is a sister to ahead, must be variable free.
Linkings which violatethis constraint are discarded.
There must be at leastas many distinct variables in the conceptual struc-ture annotating the head as there are sisters of thehead.
Again, if there are insufficient variables in thehead the partial linking is discarded.
There may bemore, however, which means that the annotation ofthe parent will contain variables.
This is acceptableif the parent is not itself a sister to a head.MAIMRA imposes two additional constraints onthe linking process.
First, meanings of lexical itemsmust have some semantic ontent; they can not besimply a variable.
Second, the functor of a con-ceptual structure fragment can not be a variable.In other words, it is not possible to have a frag-ment FROM(z( John) )  which would link with ATto produce FROM(AT( John)) .
These constraintshelp reduce the space of possible lexicons and sup-port search pruning heuristics which make learningfaster.In summary, the linking component makes use ofsix pieces of knowledge which are assumed to be in-nate.1.
The linking rule.2.
The head category associated with each phrasalcategory.3.
The requirement that the root semantic struc-ture be variable free.4.
The requirement that conceptual structure frag-ments associated with sisters of heads be vari-able free.5.
The requirement hat no lexical item haveempty semantics.6.
The requirement that no conceptual structurefragment contain variable functors.There are at least two limitations in the theory oflinking discussed above.
First, there is no attempt ogive an adequate semantics for the categories DET,AUX and COMP.
Currently, the linker assumes thatnodes labeled with these categories have no concep-tual structure annotation.
Furthermore, DET, AUXand COMP nodes which are sisters to a head are notlinked to any variable in the conceptual structure an-notating the head.
Second, while the above linkingrule can account for predication, it cannot accountfor the semantics of adjuncts.
This shortcoming re-sults not just from limitations in the linking rule butalso from the fact that Jackendoff's conceptual struc-ture is unable to represent adjunct information.4 .3  The  In ference  ComponentThe inference component imposes the constraint thatthe linguistic input must "explain" the non-linguisticinput.
This notion of explanation is assumed to beinnate and comprises four principles.
First, eachsentence must describe some subsequence of scenes.Everything the teacher says must be true in thecurrent non-linguistic ontext of the learner.
Theteacher cannot say something which is either falseor unrelated to the visual field of the learner.
Sec-ond, while the teacher is constrained to makingonly true statements about the visual field of thelearner, the teacher is not required to state every-thing which is true; some non-linguistic data may goundescribed.
Third, the order of the linguistic de-scription must match the order of occurrence of thenon-linguistic \[EVENTS\].
This is necessary becausethe language fragment handled by MAIMRA does notsupport tense and aspect.
It also adds substantialconstraint to the learning process.
Finally, sentencesmust describe non-overlapping scene sequences.
Ofthese principles, the first two seem very reasonable.The third is in accordance with the evidence thatchildren acquire tense and aspect later in the lan-guage learning process.
Only the fourth principle isquestionable.
The motivation for the fourth principleis that it enables the use of the inference algorithmdiscussed in Section 5.
More recent work, beyond thescope of this paper, suggests using a different infer-ence algorithm which does not require this principle.The above four learning principles make use ofthe notion of a sentence "describing" a sequence ofscenes.
The notion of description isexpressed via theset of inference rules given in Figure 7.
Each ruleenables the inference of the \[EVENT\] or \[STATE\]description on its right hand side from a sequenceof \[STATE\] descriptions which match the pattern onits left hand side.
For example, Rule 1 states thatif there is a sequence of scenes which can be dividedinto two concatenated subsequences of scenes, suchthat each subsequence contains at least one scene,and in every scene in that first subsequence, x is at149NPcupDET N cupIThe cupSGO(cup, \[FROM(AT(John)), TO(AT(Mary))\])VPGO(z, \[FROM(AT(John)), TO(AT(Mary))I)V PP PPGO(x, \[y, z\]) FROM(AT(John)) TO(AT(Mary))P NP P NPslid FROM(AT(x)) John TO(AT(x)) MaryI I I IN Nfrom John to Mary?
I IJohn MaryFigure 6: An example of the linking rule used by MAIMRA showing the derivation of conceptual structurefor the sentence The cup slid from John to Mary from the conceptual structure meanings of the individualwords, along with a syntactic structure for the sentence.y and not at z, while in every scene in the secondsubsequence, x is at z but not at y, then we can de-scribe that entire sequence of scenes by saying that xwent on a path from y to z.
This rule does not stip-ulate that other things can't be true in those scenesembodying an \[EVENT\] of type GO, just that ata minimum, the conditions on the right hand sidemust hold over that scene sequence.
In general, anygiven observation may entail multiple descriptions,each describing some subsequence of scenes whichmay overlap with other descriptions.MAIMRA currently assumes that these inferencerules are innate.
This seems tenable as these rules arevery low level and are probably implemented by thevision system.
Nonetheless, current work is focus-ing on removing the innateness requirement of theserules from the inference component.One severe limitation of the current set of inferencerules is the lack of rules for describing the causalityincorporated in the CAUSE and LET primitive con-ceptual functions.
One method we have consideredis to use rules like:CAUSE(w, GO(x, \[FROM(y), TO(z)\]))(BE(w, y) A BE(x, y) A -,BE(x, z))+;(BE(x, z) A -~BE(x, y))+.This states that w caused z to move from y to z ifw was at the same location y, as x was, at the startof the motion.
This is clearly unsatisfactory.
Onewould like to incorporate a more accurate notion ofcausality such as that discussed in \[9\].
Unfortunately,it seems that Jackendoff's conceptual structures arenot expressive nough to support the more complexnotions of causality.
This is another area for futurework.5 ImplementationAs mentioned previously, MAIMRA uses directed al-gorithms, rather than non-directed constraint pro-cessing, to produce a lexicon.
When processing ascenario, MAIMRA first applies the inference compo-nent to the non-linguistic input to produce semanticstructures.
Then, it applies the parser to the linguis-tic input to produce syntactic structures.
Finally,it applies the linking component in reverse, to boththe syntactic structures and semantic structures, toproduce a lexicon as output.
This process is bestillustrated by way of an example.150GO(z, \[FROM(y), TO(z)\])GO(z, FROM(y))GO(x, TO(z))GO(z, \[ 1)STAY(z, y)STAY(z, \[ \])GOExt (z, \[FROM(y), TO(z)\])GOExt (z, FROM(y))GOExt(z, TO(z))BE(z,y)ORIENT(z, \[FROM(y), TO(z)\])ORIENT(z, FROM(y))ORIENT(z, TO(y))(BE(z, y) ^  -"BE(z, z))+; (BE(z, z) ^  --BE(z, y))+ (1)?
-- (BE(z, y) A --BE(z, z))+; (BE(z, z) A --BE(z, y))+ (2)(BE(z, y) ^  -~BE(z, z))+; (BE(z, z) ^  --BE(z, y))+ (3)~- (BE(z, y) ^ -.BE(z, z))+; (BE(z, z) ^ -.BE(x, y))+ (4)~- BE(z,y);(BE(z, y))+ (5)~- BE(z,y); (BE(z,y))+ (6)?
-- (BE(z, y) ^  BE(z, z) ^  y # z) + (7)?
-- (BE(z,y) ^ BE(z, z) A y # z) + (8).-- (BE(z, y) ^ BE(z, z) ^ y # z) + (9)BE(z, y)+ (10)~-- ORIENT(z,\[FROM(y),TO(z)\]) + (11)?
-- (ORIENT(z, \[FROM(y), TO(z)\]) V ORIENT(x, FROM(y))) + (12)(ORIENT(z, \[FROM(y), TO(z)\]) v ORIENT(z, TO(y))) + (13)Figure 7: The inference rules used by the inference component of MAIMRA to infer \[EVENTS\] from \[STATES\].Consider the following input scenario.
(BE(cup, AT(John)));(BE(cup, AT(Mary))A--BE(cup, AT(John)));(BE(cup, AT(Mary)));(BE(cup, AT(Bill))A-,BE(cup, AT(Mary)));The cup slid from John to Mary.
;The cup slid from Mary to Bill.This scenario contains four scenes and two sentences.First, frame axioms are applied to the scene se-quence, yielding a sequence of scene descriptions con-taining all of the true \[STATE\] descriptions pertain-ing to those scenes, and only those true \[STATE\]descriptions.BE(cup, AT(John));BE(cup, AT(Mary));BE(cup, AT(Mary));BE(cup, AT(Bill))Given a scenario with n sentences and m scenes,find all possible ways of partitioning the m scenesinto sequences of n partitions, where the partitionseach contain a contiguous subsequence of scenes, butwhere the partitions themselves do not overlap andneed not be contiguous.
If we abbreviate the abovesequence of four scenes as a; b; e; d, then partitioningfor a scenario containing two sentences produces thefollowing disjunction:{\[a\]; (\[b\] V \[c\] V \[d\] V \[b;c\] v \[c;d\] v \[b; c;d\])}v{(\[b\] V \[a; b\]); (\[c\] V \[d\] V \[c; d\])}V{(\[c\] V \[b;c\] V \[a; b; c\]); \[d\]}.Next, apply the inference rules from Figure 7 to eachpartition in the resulting disjunctive formula, replac-ing each partition with a disjunction of all \[EVENTS\]and \[STATES\] which can describe that partition.
Forour example, this results in the replacements givenin Figure 8.The disjunction that remains after these replace-ments describes all possible sequences comprised oftwo \[EVENTS\] or \[STATES\] that can explain theinput scene sequence.
Notice how non-determinismis managed with a factored representation produceddirectly by the algorithm.After the inference component produces the se-mantic structure sequences corresponding to thenon-linguistic input, the parser produces the syntac-tic structure sequences corresponding to the linguis-tic input.
A variant of the CKY algorithm\[8, 19\] isused to produce factored parse trees.
Finally, thelinker is applied in reverse to each correspondingparse-tree/semantic-structure pai .This inverse linking process is termed fracturing.Fracturing is a recursive process applied to a parsetree fragment and a conceptual structure fragment.At each step, the conceptual structure fragment is as-signed to the root node of the parse tree fragment.
Ifthe root node of the parse tree has n non-head augh-ters, then compute all possible ways of extractingn variable-free subexpressions from the conceptualstructure fragment and assigning them to the non-head daughters, leaving distinct variables behind asplace holders.
The residue after subexpression ex-traction is assigned to the head daughter.
Fractur-ing is applied recursively to the conceptual structures151\[a\] =~ BE(cup, AT(John))\[b\],\[c\] =~ BE(cup, AT(Mary))\[d\] =~ BE(cup, AT(Bill))\[a;b\], [a;b;c\] ::~ (GO(cup,\[FROM(AT(John)),TO(AT(Mary))\]) vGO(cup, FROM(AT(John))) vGO(cup, TO(AT(Mary))) vGO(cup, \[ \]))\[b; c\] ::~ (BE(cup, AT(Mary)) VSTAY(cup, AT(Mary)))\[c; d\], \[b; c; d\] ::~ (GO(cup, \[FROM(AT(Mary)),TO(AT(Bill))\]) VGO(cup, FROM(AT(Mary))) VGO(cup, TO(AT(Bill))) vGO(cup, \[\])).Figure 8: The replacements resulting from the application of the inference rules from Figure 7 to the examplegiven in the text.assigned to daughters of the root node of the parsetree fragment, along with their annotations.
Theresults of these reeursive calls are then conjoined to-gether.
Finally, a disjunction is formed over eachpossible way of performing the subexpression extrac-tion.
This process is illustrated by the following ex-ample.
Consider fracturing the conceptual structurefragmentGO(z, \[FROM(AT(John)), TO(AT(Mary))\])along with a VP node with a head daughter labeledV and two sister daughters labeled PP.
This producesthe set of possible xtractions shown in Figure 9.
Thefracturing recursion terminates when a lexical itemis fractured.
This returns a lexical entry triple com-prising the word, its category and a representationof its meaning.
The end result of the fracturing pro-cess is a monotonic Boolean formula over definitiontriples which concisely represents he set of all pos-sible lexicons which allow the linguistic input from ascenario to explain the non-linguistic nput.
Such afactored lexicon (arising when processing a scenariosimilar to the second scenario f the training sessiongiven in Figure 2) is illustrated in Figure 10.The disjunctive lexicon produced by the fractur-ing process may contain lexicons which assign morethan one meaning to a given word.
We incorporate amonosemy constraint to rule out such lexicons.
Con-ceptually, this is done by converting the factored is-junctive lexicon to disjunctive normal form and re-moving lexicons which contain more than one lex-ical entry for the same word.
Computationally, amore efficient way of accomplishing the same task isto view the factored isjunctive lexicon as a mono-tonic Boolean formula (I) whose propositions are lex-ical entries.
We conjoin ?
with all conjunctions ofthe form ~ where the ai and ~j are both dis-tinct lexieal entries for the same word that appearin ~.
The resulting formula is no longer monotonic.Satisfying assignments for this formula correspondto conjunctive lexicons which meet the monosemyconstraint.
The satisfying assignments can be foundusing well known constraint satisfaction techniquessuch as truth maintenance systems\[10, 11\].
Whilethe problem of finding satisfying assignments for aBoolean formula (i.e.
SAT) is NP-complete, our ex-perience is that in practice, the SAT problems gen-erated by MAIMRA are easy to solve and that thefracturing process of generating the SAT problemstakes far more time than actually solving them.The monosemy constraint may seem a bit restric-tive.
It can be relaxed somewhat by allowing upto n alternate meanings for a word by conjoining inconjunctions of the formn+lA~i j  j=lwhere each of the aij are distinct lexical entries forthe same word that appear in ~, instead of the pair-wise conjunctions used previously.152GO(z, \[y, z\])GO(z, \[y, 4)GO(z, \[FROM(y), z\])GO(z, \[FROM(y), z\])GO(z, \[FROM(AT(y)), z\])GO(z, \[FROM(AT(y)), z\])GO(z, \[y, TO(z)\])GO(x, \[y, TO(z)\])GO(z, \[FROM(y), TO(z)\])GO(z, \[FROM(y), TO(z)\])GO(z, \[FROM(AT(y)), TO(z)\])GO(z, \[FROM(AT(y)), TO(z)\])GO(z, \[y, TO(AT(z))\])GO(z, \[y, TO(AT(z))\])GO(z, \[FROM(y), TO(AT(z))\])GO(z, \[FROM(y), TO(AT(z))\])GO(z, \[FROM(AT(y)), TO(AT(z))\])GO(z, \[FROM(AT(y)), TO(AT(z))\])FROM(AT(John)) TO(AT(Mary))TO(AT(Mary)) FROM(AT(John))AT(John) TO(AT(Mary))TO(AT(Mary)) AT(John)John TO(AT(Mary))TO(AT(Mary)) JohnFROM(AT(John)) AT(Mary)AT(Mary) FROM(AT(John))AT(John) AT(Mary)AT(Mary) AT(John)John AT(Mary)AT(Mary) JohnFROM(AT(John)) MaryMary FROM(AT(John))AT(John) MaryMary AT(John)John MaryMary Johni ?conju.ctiondisjunction.Figure 9: A recursive step of the fracturing process illustrating all possible subexpression extractions fromthe conceptual structure fragment given in the text, and their assignments o non-head aughters.
Thecenter column contains fragments annotating the first PP while the rightmost column contains fragmentsannotating the second PP.
The leftmost column shows the residue which annotates the head.
Each row isone distinct possible extraction.
(AND (DEFINITION CUP N CuP)(OR (AND (OR (A~D (DEFINITIONIt~RY N (IT It~RY))(DEFINITIONTO P (TO 70)))(AND (DEFINITION MARY N MARY)(DEFINITION TO P (TO (AT ?0)))))(OR (AND (OR (AND (DEFINITION JOHN N (AT JOHN))(DEFINITION FROM P (FROM 70)))(AND (DEFINITION JOHN N JOHN)(DEFINITION FROM P (FROM (AT 70)))))(DEFINITION SLID V (GO 70 (PATH 71 72))))(AND (DEFINITION JOHN N JOHN)(DEFINITION FROM P (AT 70))(DEFINITION SLID V (GO ?0 (PATH 71 (FROM ?2)))))))(AND (DEFINITION MARY N MARY)(DEFINITION TO P (AT 70))(OR (AND (OR (AND (DEFINITION JOHN N (AT JOHN))(DEFINITION FROM P (FROM ?0)))(AND (DEFINITION JOHN N JOHN)(DEFINITION FROM P (FROM (AT 70) ) ) ) )(DEFINITION SLID V (GO 70 (PATH 71 (TO 72) ) ) ) )(AND (DEFINITION JOHN N JOHN)(DEFINITION FROM P (AT 70))(DEFINITION SLID V (GO ?0 (PATH (FROM ?I) (TO ?2)))))))))Figure 10: A portion of the disjunctive l xicon which results from processing a scenario similar to the secondscenario f the training session given in Figure 2.1536 D iscuss ionWhen presented with a training session 3 much likethat given in Figure 2, MAIMRA converges to aunique lexicon within six scenarios and several min-utes of CPU time.
It is not however, able to convergeto a unique meaning for the word enter when givenscenarios of the form:(BE(John, AT(outside))A }-,BE(John, IN(room)));(BE(John, IN(room))A .--BE(John, AT(outside)))John entered the room.It turns out that there is no way to force MAIMRAto realize that the sentence describes the entire sce-nario and not just the first or last scene alone.
ThusMAIMRA does not rule out the possibility that en-ter might mean "to be somewhere."
The reasonMAIMRA is successful with the session from Figure 2is that the empty semantics constraint rules out asso-ciating the sentences with just the first or last scenebecause the semantic structures representing thosescene subsequences have too little semantic materialto distribute among the words of the sentence.
Oneway around this problem would be for MAIMRA toattempt o choose the lexicon which maximizes theamount of non-linguistic data which is accounted for.Future work will investigate this issue further.We make three claims as a result of this work.First, this work demonstrates that the combina-tion of syntactic, semantic and pragmatic modules,each incorporating coguitively plausible innatenessassumptions, offers sufficient constraint for learningword meanings with no prior lexical knowledge inthe context of non-linguistic input.
This offers ageneral framework for explaining meaning acquisi-tion.
Second, appropriate choices of representationand algorithms allow efficient implementation withinthe general framework.
While no claim is being madethat children employ the mechanisms described here,they nonetheless can be used to construct useful en-gineered systems which learn language.
The third3Although not strictly required by either the theory orthe implementation, we currently incorporate into the train-ing session given to MAIMRA, all initial lexicon telling it that' John,' 'Mary' and 'Bill' are nouns, 'from' and 'to' are preposi-tions and 'the' is a determiner.
This is to reduce the combina~torics of generating ambiguous parses.
Category informationis not given for any other words, nor is meaning informationgiven for any words occurring in the training session.
In the-ory it would be possible to efficiently bootstrap the categoriesfor these words as well, via a longer training session containinga few shorter sentences to constrain the possible categories forthese words.
We have not done so yet, however.claim is more bold.
Most language acquisition re-search operates under a tacit assumption that chil-dren acquire individual pieces of knowledge aboutlanguage by experiencing single short stimuli n iso-lation.
This is often extended to an assumption thatknowledge of language isacquired by discovering dis-tinct cues in the input, each cue elucidating one pa-rameter setting in a parameterized linguistic theory.We will call this assumption the local learning hy-pothesis.
This is in contrast o our approach whereknowledge of language is acquired by finding dataconsistent across longer correlated sessions.
Our ap-proach requires the learner to do some puzzle solvingor constraint satisfaction.
4 It is normally believedthat the latter approach is not cognitively plausi-ble.
The evidence for this is that children seem tohave short "input buffers."
The limited size of theinput buffers is taken to imply that only short iso-lated stimuli can take part in inferring each new lan-guage fact.
MAIMRA demonstrates that despite ashort input buffer with the ability of retaining onlyone scenario at a time, it is nonetheless possible toproduce a disjunctive representation which supportsconstraint solving across multiple scenarios.
We be-lieve that without cross scenario constraint solving, itis impossible to account for meaning acquisition andthus the local learning hypothesis wrong.
Our ap-proach offers a viable alternative to the local learninghypothesis consistent with the observed short inputbuffer effect.7 Re la ted  WorkWhile most prior computational work on meaning ac-quisition focuses on contextual learning by scanningtexts, some notable work has pursued a path simi-lax to that described here attempting to learn fromcorrelated linguistic and non-linguistic input.
In\[16, 17\], Salveter describes a system called MORAN.The non-linguistic component of each scenario pre-sented to MORAN consists of a sequence of exactlytwo scenes, where each scene is described by a con-junction of atomic formula.
The linguistic compo-nent of each scenario is a preparsed case frame anal-ysis of a single sentence describing the state changeoccurring between those two scenes.
From each sce-nario in isolation, MORAN infers what Salveter callsa Conceptual Meaning Structure (CMS) which at-tempts to capture the essence of the meaning of theverb in the sentence.
This CMS is a subset of the4We are not claiming that such puzzle solving is conscious.It is likely that constraint satisfaction, if done by children oradults, is a very low level subconscious cognitive function notsubject o introspective observation.154two scenes identifying the portion of the scenes re-ferred to by the sentence, with the arguments of theatomic formula linked to noun phrases replaced byvariables labeled with the syntactic positions thosenoun phrases fill in the sentence.
The process ofinferring CMSs  involves two processes reminiscentof tasks performed by MAIMRA, namely the fig-ure/ground distinction whereby the inference com-ponent suggests possible subsets of the non-linguisticinput as being referred to by the linguistic input (asdistinct from the part which is not referred to) andthe fracturing process whereby verb meanings areconstructed by extracting out arguments from wholesentence meanings.
MORAN's variants of these tasksare much simpler than the analogous tasks performedby MAIMRA.
First, the figure/ground distinction iseasier since each scenario presented to MORAN con-tains but a single sentence and a pair of scenes.MORAN need not figure out which subsequence ofscenes corresponds to each sentence.
Second, thelinguistic input comes to MORAN preparsed whichrelies on preexisting knowledge of the lexical cate-gories of the words in the sentence.
MORAN does notacquire category information, and furthermore doesnot deal with any ambiguity that might arise fromthe parsing process or the figure/ground distinction.Finally, the training session presented to MORAN re-lies on a subtle implicit link between the objects inthe world and linguistic tokens used to refer to them.Part of the difficulty faced by MAIMRA is discerningthat the linguistic token John refers to the concep-tual structure fragment John.
MORAN is given thatinformation a pr/or/by lacking a formal distinctionbetween the notion of a linguistic token and concep-tual structure.
Given this information, the fractur-ing process becomes trivial.
MORAN therefore, doesnot exhibit the cross-scenario correlational behaviorattributed to MAIMRA and in fact learns every verbmeaning with just a single training scenario.
Thisseems very implausible as a model of child languageacquisition.
In contrast to MAIMRA, MORAN is ableto learn polysemous senses for verbs; one for each sce-nario provided for a given verb.
MORAN focuses onextracting out the common substructure for polyse-mous meanings attempting to maximize commonal-ity between different word senses and build a catalogof higher level conceptual building blocks, a task notattempted by MAIMRA.In \[13, 14\], Pustejovsky describes a system calledTULLY, which also operates in a fashion similar toMAIMRA arid MORAN,  learning word meanings frompairs of linguistic and non-linguistic input.
LikeMORAN, the linguistic input given to TULLY foreach scenario is a single parsed sentence.
The non-linguistic input given along with that parsed sentenceis a predicate calculus description of three parts ofa single event, its beginning, middle and end.
Fromthis input, TULLY derives a Thematic Mapping In-dex, a data structure representing the 8-roles borneby each of the arguments to the main predicate.
LikeMORAN,  the task faced by TULLY is much simplerthan that faced by MAIMRA, since TULLY is pre-sented with unambiguous parsed input, is given thecorrespondence between nouns and their referentsand is given the correspondence between a single sen-tence and the semantic representation of the eventdescribed by that sentence.
TULLY does not learnlexical categories, does not have to determine fig-ure/ground partitioning of non-linguistic input andimplausibly learns verb meanings from single scenar-ios without any cross-scenario correlation.
Multiplescenarios for the same verb cause TULLY to gener-alize to the least common generalization of the in-dividual instances.
TULLY  however, goes beyondMAIMRA in trying to account for the acquisition ofa variety of markedness features for 0-roles includ-ing \[+motion\], \[+abstract\], \[?direct\], \[?partitive\] and\[?animate\]8 ConclusionThe MAIMRA system successfully earns word mean-ings with no prior lexical knowledge of any words.It works by applying syntactic, semantic and prag-matic constraints to correlated linguistic and non-linguistic input.
In doing so, it more accurately re-flects the type of learning performed by children,in contrast to previous lexical acquisition systemswhich focus on learning unknown words encounteredwhile reading texts.
Although, each module imple-ments a weak theory, and in isolation offers only lim-ited constraint on possible mental representations,the collective constraint provided by the combina-tion of modules is sufficient o reduce the nondeter-minism to a manageable vel.
It demonstrates thatwith a reasonable set of assumptions about innateknowledge, combined with appropriate representa-tions and algorithms, tractable learning is possiblewith short training sessions and limited processing.Though there may be disagreement as to the lin-guistic and cognitive plausibility of some of the in-nateness assumptions, and while the particular syn-tactic, semantic and pragmatic theories currently in-corporated into MAIMRA may be only approxima-tions to reality, nonetheless, the general frameworkshows promise of explaining how children acquireword meanings.
In particular, it offers a viable al-155ternative to the local learning hypothesis which canexplain how children acquire meanings that requirecorrelation of experience across many input scenar-ios, with only limited size input buffers.
Future workwill attempt o address these potential shortcomingsand will focus on supporting more robust acquisitionof a broader class of word meanings.ACKNOWLEDGMENTSI would like to thank Peter Szolovitz, Patrick Win-ston and Victor Zue for giving me the freedom toembark on this project and encouraging me to elab-orate on it; AT&T Bell Laboratories for supportingthis work through a Ph.D. scholarship; Johan deK-leer, Kris Halvorsen and everybody at Xerox PARCfor listening to half-baked versions of this work priorto completion; Bob Berwick, Barbara Grosz, DavidMcAllester and George Lakoff for many interestingdiscussions; and Ron Rivest for pushing me to com-plete this paper.References\[1\] Robert C. Berwick.
Learning word meaningsfrom examples.
In Proceedings of the Eighth In-ternational Joint Conference on Artificial Intel-ligence, pages 459-461, 1983.\[2\] Noam Chomsky.
Lectures on Government andBinding, volume 9 of Studies in GenerativeGrammar.
Forts Publications, 1981.\[3\] Noam Chornsky.
Some Concepts and Conse-quences of the Theory of Government and Bind-ing, volume 6 of Linguistic lnquiry Monographs.The M. I. T. Press, Cambridge, Massachusettsand London, England, 1982.\[4\] Noam Chomsky.
Barriers, volume 13 of Lin-guistic Inquiry Monographs.
The M. I. T. Press,Cambridge, Massachusetts and London, Eng-land, 1986.\[5\] Richard H. Granger, Jr. FOUL-UP a programthat figures out meanings of words from context.In Proceedings of the Fifth International JointConference on Artificial Intelligence, pages 172-178, 1977.\[6\] Ray Jackendoff.
Semantics and Cognition.
TheM.
I. T. Press, Cambridge, Massachusetts andLondon, England, 1983.156\[7\] Paul Jacobs and Uri Zernik.
Acquiring lexicalknowledge from text: A case study.
In Proceed-ings of the Seventh National Conference on Ar-tifical Intelligence, pages 739-744, August 1988.\[8\] T. Kasami.
An efficient recognition and syn-tax algorithm for context-free languages.
Sci-entific Report AFCRL-65-758, Air Force Cam-bridge Research Laboratory, Bedford MA, 1965.\[9\] George Lakoff and Mark Johnson.
MetaphorsWe Live By.
The University of Chicago Press,1980.\[10\] David Allen McAllester.
Solving SAT problemsvia dependency directed backtracking.
Unpub-lished manuscript received irectly from author.\[11\] David Allen McAllester.
An outlook on truthmaintenance.
A I.
Memo 551, M. I. T. ArtificialIntelligence Laboratory, August 1980.\[12\] Fernando C. N. Pereira and David It.
D. War-ren.
Definite clause grammars for languageanalysis--a survey of the formalism and a com-parison with augmented transition networks.Artificial Intelligence, 13(3):231-278, 1980.\[13\] James Pustejovsky.
On the acquisition of lexi-cal entries: The perceptual origin of thematicrelations.
In Proceedings of the 25 th AnnualMeeting of the Association for ComputationalLinguistics, pages 172-178, July 1987.\[14\] James Pustejovsky.
Constraints on the acquisi-tion of semantic knowledge.
International Jour-nal of Intelligent Systems, 3(3):247-268, 1988.\[15\] Manny Rayner, /~sa Hugosson, and GSranHagert.
Using a logic grammar to learn a lex-icon.
Technical Report 1%88001, Swedish Insti-tute of Computer Science, 1988.\[16\] Sharon C. Salveter.
Inferring conceptual graphs.Cognitive Science, 3(2):141-166, 1979.\[17\] Sharon C. Salveter.
Inferring building blocks forknowledge representation.
In Wendy G. Lehn-eft and Martin H. Ringle, editors, Strategies forNatural Language Processing, chapter 12, pages327-344.
Lawrence Erlbaum Associates, 1982.\[18\] Roger C. Schank.
The fourteen primitive actionsand their inferences.
Memo AIM-183, StanfordArtificial Intelligence Laboratory, March 1973.\[19\] D. H. Younger.
Recognition and parsing ofcontext-free languages in time O(n3).
Informa-tion and Control, 10(2):189-208, 1967.
