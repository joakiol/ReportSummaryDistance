Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1898?1907,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsA multi-Teraflop Constituency Parser using GPUsJohn Canny David Hall Dan KleinUC BerkeleyBerkeley, CA, 94720canny@berkeley.edu, dlwh,klein@cs.berkeley.eduAbstractConstituency parsing with rich grammars re-mains a computational challenge.
Graph-ics Processing Units (GPUs) have previouslybeen used to accelerate CKY chart evalua-tion, but gains over CPU parsers were mod-est.
In this paper, we describe a collection ofnew techniques that enable chart evaluation atclose to the GPU?s practical maximum speed(a Teraflop), or around a half-trillion rule eval-uations per second.
Net parser performanceon a 4-GPU system is over 1 thousand length-30 sentences/second (1 trillion rules/sec), and400 general sentences/second for the BerkeleyParser Grammar.
The techniques we introduceinclude grammar compilation, recursive sym-bol blocking, and cache-sharing.1 IntroductionConstituency parsing with high accuracy (e.g.
latentvariable) grammars remains a computational chal-lenge.
The O(Gs3) complexity of full CKY pars-ing for a grammar with G rules and sentence lengths, is daunting.
Even with a host of pruning heuris-tics, the high cost of constituency parsing limits itsuses.
The most recent Berkeley latent variable gram-mar for instance, has 1.7 million rules and requiresabout a billion rule evaluations for inside scoring ofa single length-30 sentence.
GPUs have previouslybeen used to accelerate CKY evaluation, but gainsover CPU parsers were modest.
e.g.
in Yi et al(2011) a GPU parser is described for the BerkeleyParser grammar which achieves 5 sentences per sec-ond on the first 1000 sentences of Penn Treebanksection 22 Marcus et al(1993), which is compa-rable with the best CPU parsers Petrov and Klein(2007).
Our parser achieves 120 sentences/secondper GPU for this sentence set, and over 250 sen-tences/sec on length ?
30 sentences.
These resultsuse a Berkeley Grammar approximately twice as bigas Yi et al(2011), an apparent 50x improvement.On a 4-GPU system, we achieve 1000 sentences/secfor length ?
30 sentences.
This is 2 orders of mag-nitude faster than CPU implementations that relyheavily on pruning, and 5 orders of magnitude fasterthan full CKY evaluation on a CPU.Key to these results is a collection of new tech-niques that enable GPU parsing at close to theGPU?s practical maximum speed (a Teraflop for re-cent GPUs), or around a half-trillion rule evaluationsper second.
The techniques are:1.
Grammar compilation, which allows register-to-register code for application of grammarrules.
This gives an order of magnitude (10x)speedup over alternative approaches that useshared memory.2.
Symbol/rule blocking of the grammar to re-spect register, constant and instruction cachelimits.
This is precondition for 1 above, andthe details of the partitioning have a big (> 4x)effect on performance.3.
Sub-block partitioning to distribute rules acrossthe stream processors of the GPU and allow L2cache acceleration.
A factor of 2 improvement.The code generated by our parser comes close to thetheoretical limits of the GPU.
80% of grammar rules1898are evaluated using a single-cycle register-to-registerinstruction.2 GPU Design PrinciplesIn this paper, we focus on the architecture of recentNVIDIA R?
GPUs, though many of the principles wedescribe here can be applied to other GPUs (e.g.those made by AMD R?.)
The current NVIDIA R?KeplerTM series GPU contains between 2 and 16?stream processors?
or SMX?s which share an L2cache interfacing to the GPUs main memory Anony-mous (2013).
The SMXs in turn comprise 192cores which share a memory which is partitionedinto ?shared memory?
and L1 cache.
Shared mem-ory supports relatively fast communication betweenthreads in an SMX.
Communication between SMXshas to pass through slower main memory.The execution of instructions within SMXs is vir-tualized and pipelined - i.e.
it is not a simple task tocount processors, although there are nominally 192in the KeplerTM series.
Register storage is not at-tached to cores, instead registers are associated inblocks of 63 or 255 (depending on KeplerTM sub-architecture) with running threads.
Because of this,it is usually easier for the programmer to think ofthe SMXes as 1024 thread processors.
These 1024threads are grouped into 32 groups of 32 threadscalled warps.
Each warp of threads shares a programcounter and executes code in lock-step.
However,execution is not SIMD - all threads do not execute allinstructions.
When the warp encounters a branchinginstruction, all branches that are satisfied by somethread will be executed in sequence.
Each threadonly executes the instructions for its own branch,and idles for the others.
NVIDIA R?
calls this modelSIMT (Single Instruction, Multiple Threads).
Ex-ecution of diverging branches by a warp is calledwarp divergence.
While it simplifies programming,warp divergence understandably hurts performanceand our first goal is to avoid it.GPUs are generally optimized for single-precision floating point arithmetic in support ofrendering and simulation.
Table 1 shows instructionthroughput (number of instructions that are executedper cycle on each SMX).
The KeplerTM series hastwo architectural sub-generations (3.0 and 3.5) withsignificant differences in double-precision support.Data from Anonymous (2012) and NVIDIA (2012).Instruction typeArchitecture3.0 3.5Shared memory word access 32 32FP arithmetic +,-,*,FMA 192 192DP arithmetic +,-,*,FMA 8 64Integer +,- 160 160Integer *,FMA 32 32float sin, exp, log,... 32 32Table 1: Instructions per cycle per SMX in generation 3.0and 3.5 KeplerTM devicesIn the table, FP is floating point, DP is doubleprecision, and FMA is a single-cycle floating-piontfused multiply-add used in most matrix and vectoroperations (A?
A+B ?C).
Note next that floatingpoint (single precision) operations are extremely fastand there is an FPU for each of the 192 processors.Double precision floating point is 3x slower on high-end 3.5 GPUS, and much slower (24x) on the com-modity 3.0 machines.
While integer addition is fast,integer multiply is much slower.
Perhaps most sur-prising is the speed of single-precision transcenden-tal function evaluation, log, exp, sin, cos, tan, etc.,which are as fast as shared memory accesses or in-teger multiplication, and which amount to a quarter-trillion transcendental evaluations per second on aGTX-680/K10.PCFG grammar evaluation nominally requirestwo multiplications and an addition per rule (section4) which can be written:Sij,m =?k=1...j; n,p?QSik,nS(k+1)j,pcmnp (1)i.e.
the CKY node scores are sums of products ofpairs of scores and a weight.
This suggests that atleast in principle, it?s possible to achieve a trillionrule evaluations per second on a 680 or K10 device,using a * and an FMA operation for each rule.
Thatassumes we are doing register-to-register operationshowever.
If we worked through shared memory (firstline of the table), we would be limited to about 80billion evaluations/sec, 20 times slower.
The anal-ysis underscores that high performance for parsingon a GPU is really a challenge of data movement.We next review the different storage types and their1899bandwidths, since prudent use of, and movement be-tween storage types is the key to performance.2.1 Memory Types and SpeedsThere are six types of storage on the GPU whichmatter for us.
For each type, we give the capac-ity and aggregate bandwidth on a typical device (aGTX-680 or K10 running at 1GHz).Register files These are virtualized and associatedwith threads rather than processors.
256kB perSMX.
Each thread in architecture 3.0 devices canaccess 63 32-bit registers, or 255 registers for 3.5devices.
Aggregate bandwidth 40 TB/s.Shared memory/L1 cache is shared by all threadsin an SMX.
64kB per SMX partitioned into sharedmemory and cache functions.
Aggregate bandwidthabout 1 TB/s.Constant memory Each SMX has a 48kB read/onlycache separate from the L1 cache.
It can store gram-mar constants and has much higher bandwidth thanshared memory.
Broadast bandwidth 13 TB/s.Instruction cache is 8 KB per SMX.
Aggregatebandwidth 13 TB/s.L2 cache is 0.5-1.5 MB, shared between all SMXs.Aggregate bandwidth 500 GB/s.Global memory is 2-6GB typically, and is sharedby all SMXs.
GPUs use a particularly fast form ofSDRAM (compared to CPUs) but it is still muchslower than the other memory types above.
Ag-gregate bandwidth about 160 GB/s.There is one more very important principle: Coa-lesced main memory access.
From the above it canbe seen that main memory access is much slowerthan other memories and can easily bottleneck thecalculations.
The figure above (160 GB/s) for mainmemory access assumes such access is coalesced.Each thread in the GPU has a thread and a blocknumber which determines where it runs on the hard-ware.
Consecutively-numbered threads should ac-cess consecutive main memory locations for fastmemory access.These parameters suggest a set of design princi-ples for peak performance:1.
Maximize use of registers for symbol scores,and minimize use of shared memory (in fact wewill not use it at all).2.
Maximize use of constant memory for ruleweights, and minimize use of shared memory.3.
Partition the rule set into blocks that respect thelimits on number of registers, constant memory(needed for grammar rules probabilities) andinstruction cache limits.4.
Minimize main memory access and use L2cache to speed it up.Lets look in more detail at how to achieve this.3 Anatomy of an Efficient GPU ParserHigh performance on the GPU requires us to mini-mize code divergence.
This suggests that we do notuse a lexicalized grammar or a grammar that is sen-sitive to the position of a span within the sentence.These kinds of grammars?while highly accurate?have irregular memory access patterns that conflictwith SIMD execution.
Instead, an unlexicalized ap-proach like that of Johnson (2011) or Klein andManning (2003), or a latent variable approach likethat of Matsuzaki et al(2005) or Petrov et al(2006)are more appropriate.
We opt for the latter kind: la-tent variable grammars are fairly small, and their ac-curacies rival lexicalized approaches.Our GPU-ized inside algorithm maintains twodata structures: parse charts that store scores foreach labeled span, as usual, and a ?workspace?
thatis used to actually perform the updates of the in-side algorithm.
Schematically, this memory lay-out is represented in Figure 1.
A queue is main-tained CPU-side that enqueues work items of theform (s, p, l, r), where s is a sentence, and p, l, andr specify the index in the parse chart for parent, leftchild, and right child, respectively.
The outer loopproceeds in increasing span length (or height of par-ent node scores to be computed).
Next the algorithmiterates over the available sentences.
Then it iteratesover the parent nodes at the current length in thatsentences, and finally over all split points for the cur-rent parent node.
In each case, work items are sent tothe queue with that span for all possible split points.When the queue is full?or when there are nomore work items of that length?the queue is flushed1900Figure 1: The architecture of the system.
Parse charts arestored in triangular arrays laid out consecutively in mem-ory.
Scores for left and right children are transposed andcopied into the ?workspace?
array, and the inside updatesare calculated for the parent.
Scores are then pushed backto the appropriate cell in the parse charts, maxing themwith scores that are already there.
Transposition ensuresthat reads and writes are coalesced.to the GPU, which executes three steps.
First, thescores for each left and right child are copied intothe corresponding column in the workspace.
Theninside updates are applied in parallel for all cells toget parent scores.
Then parents are entered back totheir appropriate cells in the parse charts.
This istypically a many-to one atomic reduction (either asum for probability scores, or a max for max-sumlog probability scores).
This process repeats untilall span lengths have been processed.3.1 The Inside UpdatesThe high-level goal of our parser is to use SIMDparallelism to evaluate the same rule across manyspans (1024 threads are currently used to process8192 spans in each kernel).
This approaches allowsus to satisfy the GPU performance desiderata fromthe previous section.
As discussed in section 5 eachGPU kernel actually processes a small subset of thesymbols and rules for the grammar, and kernels areexecuted in sequence until the entire grammar hasbeen processed.
Each thread iterates over the rulesin the same order, reading in symbols from the leftchild and right child arrays in main memory as nec-essary.The two-dimensional work arrays must be storedin ?symbol-major?
order for this to work.
That is,the parent VP for one work item is stored next to theparent VP for the next work item, while the VP sym-bol for the first work item is stored on the next ?row?of the work array.
The reason the workspace cellsare stored in ?symbol-major?
order is to maximizecoalesced access: each thread in the SMX accessesthe same symbol for a different work item in paral-lel, and those work items are in consecutive memorylocations.3.2 The Copy-transpose OperationsUnlike the workspace arrays, the arrays for the parsecharts are stored in ?span-major?
order, transposedfrom how they are stored in the workspace arrays.That is, for a given span, the NP symbol is nextto the same span?s VP symbol (for example).
Thisorder accelerates both symbol loading and Viterbisearch later on.
It requires a transpose-copy in-stead of ?non-transposed?
copy to move from chartto workspace arrays and back again, but note that anon-transposed copy (or direct access to the chartby the GPU compute kernel) would probably beslower.
The reason is that any linear ordering ofcells in the triangle table will produce short seg-ments (less than 32 words and often less than 16)of consecutive memory locations.
This will lead tomany non-coalesced memory accesses.
By contrastthe span-major representation always uses vectorswhose lengths equals the number of symbols (500-1000), and these can be accessed almost entirelywith coalesced operations.
The copy-transpose op-erations are quite efficient (the transpose itself ismuch faster than the I/O), and come close to the 160GB/s GPU main memory limit.The reverse copy-transpose (from parentworkspace cells to chart) is typically many-to-one,since parent scores derive from multiple splits.
Theyare implemented using atomic reduce operations(either atomic sum or atomic max) to ensure dataconsistency.At the heart of our approach is the use of grammarcompilation and symbol/rule blocking, describednext.19014 Grammar CompilationEach rule in a probabilistic context-free grammarcan be evaluated with an update of the form:Sij,m =?k=1...j; n,p?QSik,nS(k+1)j,pcmnp (2)where Sij,m is the score for symbol m as a generatorof the span of words from position i to j in the in-put sentence, cmnp is the probability that symbol mgenerates the binary symbol pair n, p, and Q is theset of symbols.
The scores will be stored in a CKYchart indexed by the span ij and the symbol m.To evaluate (2) as fast as possible, we want touse register variables which are limited in number.The location indices i, j, k can be moved outside theGPU kernel to reduce the variable count.
We usesymbols P , L and R for respectively the score of theparent, left child and right child in the CKY chart.Then the core relation in (2) can be written as:Pm =?n,p?QLnRpcmnp (3)In the KeplerTM architecture, register arguments arenon-indexed, i.e.
one cannot access register 3 as anarray variable R[i] with i=31.
So in order to useregister storage for maximum speed, we must open-code the grammar.
Symbols like L3, R17 are en-coded as variables L003 and R017, and each rulemust appear as a line of C code:P043 += L003*R017*0.023123f;P019 += L012*R123*6.21354e-7f;: : : :Open-coding the grammar likely has a host of per-formance advantages.
It allows both compiler andhardware to ?see?
what arguments are coming andschedule the operations earlier than a ?grammaras data?
approach.
Note that we show here thesum-product code for computing inner/outer symbolprobabilities.
For Viterbi parse extraction we replace+,* with max,+ and work on log scores.L and R variables must be loaded from mainmemory, while P-values are initialized to zero andthen atomically combined (sum or max) with P-values in memory.
Loads are performed as late as1Even if indexing were possible, it is extremely unlikely thatsuch accesses could complete in a single cyclepossible, that is, a load instruction will immediatelyprecede the first use of a symbol:float R031 = right[tid+65*stride];P001 += L001*R031*1.338202e-001f;where tid is the thread ID plus an offset, and strideis the row dimension of the workspace (typically8192), and right is the main memory array of rightsymbol scores.
Similarly, atomic updates to P-values occur as early as possible, right after the lastupdate to a value:G020 += L041*R008*6.202160e-001f;atomicAdd(&par[tid+6*stride],G020);These load/store strategies minimize the active lifeof each variable and allow reuse of register variablesfor symbols whose lifetimes do not overlap.
Thiswill be critical to successful blocking, described inthe next section.4.1 Common subexpressionsOne interesting discovery made by the compiler wasthat the same L,R pair is repeated in several rules.
Inhindsight, this is obvious because the symbols in thisgrammar are splits of base symbols, and so splits ofthe parent symbol will be involved in rules with eachpair of L,R splits.
The compiler recognized this byturning the L,R pair into a common subexpressionin a register.
i.e.
the compiler convertsP008 += L041*R008*6.200769e-001f;P009 += L041*R008*6.201930e-001f;P010 += L041*R008*6.202160e-001f;intofloat LRtmp = L041*R008;P008 += LRtmp*6.200769e-001f;P009 += LRtmp*6.201930e-001f;P010 += LRtmp*6.202160e-001f;and inspection of the resulting assembly code showsthat each rule is compiled into a single fusedmultiply-add of LRtmp and a value from con-stant memory into the P symbol register.
This al-lows grammar evaluation to approach the theoreticalGflop limit of the GPU.
For this to occur, the rulesneed to be sorted with matching L,R pairs consecu-tive.
The compiler does not discover this constraintotherwise or reorder instructions to make it possible.19024.2 Exploiting L2 cacheFinally, we have to generate code to evaluate distinctminor cube rulesets on each of the 8 SMXes con-currently in order to benefit from the L2 cache, asdescribed in the next section.
CUDATM (NVIDIA?sGPU programming Platform) does not allow directcontrol of SMX target, but we can achieve this byrunning the kernel as 8 thread blocks and then test-ing the block ID within the kernel and dispatching toone of 8 blocks of rules.
The CUDATM schedulerwill execute each thread block on a different SMXwhich gives the desired distribution of code.5 Symbol and Rule BlockingThe grammar formula (3) is very sparse.
i.e.
mostproductions are impossible and most cmnp are zero.For the Berkeley grammar used here, only 0.2% ofpotential rules occur.
Normally this would be badnews for performance because it suggests low vari-able re-use.
However, the update relation is a tensorrather than a matrix product.
The re-use rate is deter-mined by the number of rules in which a particularsymbol occurs, which is actually very high (morethan 1000 on average).The number of symbols is about 1100 in thisgrammar, and only a fraction can be stored in athread?s register set at one time (which is either 63 or255 registers).
To compute all productions we willneed to break the calculation into smaller groups ofvariables that can fit in the available register space.We can visualize this geometrically in figure 2.The vectors of symbols P , L and R form the lead-ing edges of this cube.
The cube will be partitionedinto smaller subcubes indexed by subsets of thosesymbols, and containing all the rules that apply be-tween those symbols.
The partitioning is chosen sothat the symbols in that subset can fit into availableregister storage.
In addition, the partitioning is cho-sen to induce the same number of rules in each cube- otherwise different code paths in the kernel will runlonger than others, and reduce overall performance.This figure is a simplification - in order to balancethe number of rules in each subcube, the partition-ing is not uniform in number of symbols as the figuresuggests.As can be seen in figure 2, cube partitioning hastwo levels.
The original P-L-R cube is first par-P RLFigure 2: Partition of the cube of symbol combinationsinto major subcubes (left) and minor subcubes (right).titioned into ?major?
cubes, which are then parti-tioned into ?minor?
cubes (2x2x2 in the figure).
Amajor cube holds the symbols and rules that are ex-ecuted in a single GPU kernel.
The minor cubes in amajor cube hold the symbols and rules that are exe-cuted in a particular SMX.
For the GTX-680 or K10with 8 SMXs, this allows different SMXs to concur-rently work on different 2x2x2 subcubes in a majorcube.
This arrangement substantially reduces mainmemory bandwidth through the L2 cache (which isshared between SMXes).
Each symbol in a majorcube will be loaded just once from main memory,but loaded into (up to) 4 different SMXes throughthe L2 cache.
Subcube division for caching in ourexperiments roughly doubled the kernel?s speed.However, simple partitioning will not work.
e.g.if we blocked into groups of 20 P, L, R symbols(in order to fit into 60 registers), we would need1100/20 = 55 blocks along each edge, and a total of553 ?
160, 000 cells.
Each symbol would need tobe loaded 552 = 3025 times, there would be almostno symbol re-use.
Throughput would be limited bymain memory speed to about 100 Gflops, an orderof magnitude slower than our target.
Instead, weuse a rule partitioning scheme that creates as small asymbol footprint as possible in each cube.
We use aspectral method to do this.Before describing the spectral method we men-tion an optimization that drops the symbol count by2.
Symbols are either terminal or non-terminal, andin the Berkeley latent variable grammar there areroughly equal numbers of them (503 non-terminalsand 631 terminals).
All binary rules involve a non-terminal parent.
L and R symbols may be either ter-minal or non-terminal, so there are 4 distinct types1903of rules depending on the L, R, types.
We handleeach of these cases with a different kernel, whichrougly halves the number of rules along each edge(it is either 503 or 631 along each edge).
Further-more, these kernels are called in different contexts,and a different number of times.
e.g.
XX (L, R, bothnon-terminal) kernels are called O(s3) times for sen-tences of length s because both L, R children can oc-cur at every position in the chart.
XT and TX kernels(with one terminal and one non-terminal symbol) arecalled only O(s2) times since one of L or R must beat the base of the chart.
Finally TT kernels (both Land R are terminals) will be called O(s) times.
Per-formance is therefore dominated by the XX kernel.5.1 Spectral PartitioningWe explored a number of partitioning schemes forboth symbol and rule partitioning.
In the end wesettled on a spectral symbol partitioning scheme.Each symbol is a node in the graph to be parti-tioned.
Each node is assigned a feature vector de-signed to match it to other nodes with similar sym-bols occuring in many rules.
There was considerableevolution of this feature set to improve partitioning.In the end the vector for a particular P symbol isa = (a1, 0.1 ?
a2, 0.1 ?
a3) where a1 is a vectorwhose elements are indexed by L, R pairs and whosevalues represent the number of rules involving boththose symbols (and the parent symbol P), a2 encodesL symbols and counts the number of rules contain-ing that L symbol and P, and a3 encodes the R sym-bols and counts rules containing that R symbol andP.
This feature vector produces a high similarity be-tween P symbols that exactly share many L,R pairsand lower similarity for shared L and R.A spectral clustering/partitioning algorithm ap-proximately minimizes the total edge weight ofgraph cuts.
In our case, the total weight of a cut isto first order the product of the number of L,R pairsthat occur on each side of the cut, and to second or-der the count of individual L and R pairs that spanthe cut.
Let S and T be the counts for a particularLR pair or feature, then we are trying to minimizethe product S*T while keeping the sum S+T, whichis the total occurences of the feature on both sides ofthe partition, constant.
Such a product is minimizedwhen one of S or T is zero.
Since many symbolsare involved, this typically does not happen to an in-dividual symbol, but this heuristic is successful atmaking the individual symbol or LR pair distribu-tions across the cuts as unbalanced as possible.
i.e.one side of the cut has very few instances of a givensymbol.
The number of instances of a symbol is anupper bound on the number of subcells in which thansymbol occurs, and therefore on the number of timesit needs to be loaded from memory.
Repeating thisoperation recursively to produce a 3d cell decom-position also concentrates each symbol in relativelyfew cells, and so tends to reduce the total registercount per cell.In a bit more detail, from the vectors a above weconstruct a matrix A whose columns are the fea-ture vectors for each P symbol.
Next we constructthe symmetric normalized Laplacian L for the adja-cency matrix ATA.
We then compute the eigende-composition of L, and extract the eigenvector cor-responding to the second-smallest eigenvalue.
Eachnode in the graph is assign a real weight from thecorresponding element of this eigenvector.
We sortby these weights, and partition the symbols usingthis sort order.
We tried both recursive binary par-titioning, and partitioning into k intervals using theoriginal sort order, and obtained better results withthe latter.Partitioning is applied in order P, L, R to gener-ate the major cubes of the rule/symbol partition, andthen again to generate minor cubes.
This partition-ing is far more efficient than a naive partitioning.The XX ruleset for our Berkeley grammar has about343,000 rules over a 5033 cube of non-terminal sym-bols.
The optimal PxLxR cube decomposition (op-timal in net kernel throughput) for this ruleset was6x2x2 for major cubes, and then 2x2x2 for minorcubes.
This requires 6x2x2=24 GPU kernels, eachof which encodes 2x2x2=8 code blocks (recall thateach of the 8 SMXs executes a different code blockfrom the same kernel)2.
Most importantly the reloadrate (the mean number of major cells containing agiven symbol, or the mean number of times a sym-bol needs to be reloaded from main memory) dropsto about 6 (vs. 3000 for naive partitioning).
Thisis very significant.
Each symbol is used on average343, 000/501 ?
6000 times overall by the XX ker-2This cube decomposition also respects the constant cacheand instruction cache limits1904nel.
Dropping the reload factor to 6 means that forevery 1 main memory load of a symbol, there areapproximately 1000 register or L2 cache reuses.
Alittle further calculation shows that L2 cache itemsare used a little more than twice, so the register reuserate within kernel code blocks is close to 500 on av-erage.
This is what allows teraflop range speeds.Note that while the maximum number of registersper thread in the GTX-680 or K10 is 63, the aver-age number of variables per minor cube is over 80for our best-performing kernel, showing a numberof variables have non-overlapping lifetimes.
Sortingrules lexicographically by (L,R,P) does a good jobof minimizing variable lifetime overlap.
Howeverthe CUDATM compiler reorders variables anywaywith slightly worse performance on average (thereseems to be no way around this, other than generat-ing assembly code directly).6 GPU Viterbi Parse ExtractionIn sequential programs for chart generation, it is pos-sible to compute and save a pointer to the best splitpoint and score at each node in the chart.
However,here the scores at each node are computed with fine-grained parallelism.
The best split point and scorecannot be computed until all scores are available.Thus there is a separate Viterbi step after chart scor-ing.The gap between GPU and CPU performanceis large enough that CPU Viterbi search was abottleneck, even though it requires asymptoticallyless work (O(Gs2) worst case, O(Gs) typical) vsO(Gs3) to compute the CKY scores.
Thereforewe wrote a non-recusive GPU-based Viterbi search.Current GPUs support ?high-level?
recursion, butthere is no stack in the SMX.
A recursive pro-gram must create software stack space in eithershared memory or main memory which serious per-formance impact on small function calls.
Instead,we use an iterative version of Viterbi parse extrac-tion which uses pre-allocated array storage to storeits output, and such that the partially-complete out-put array encodes all the information the algorithmneeds to proceed - i.e.
the output array is also thealgorithm?s work queue.Ignoring unaries for the moment, a binary parsetree for a sentence of length n has 2n ?
1 nodes,including preterminals, internal nodes, and the root.We can uniquely represent a tree as an array with2n ?
1 elements.
In this representation, each indexcorresponds to a node in prefix (depth-first) order.For example, the root is always at position 0, and thesecond node will correspond to the root?s left child.If this second node has a left child, it will be the thirdnode, otherwise the third node will be the second?sright sibling.We can uniquely identify the topology of the treeby storing the ?width?
of each node in this array,where the width is the number of words governedby that constituent.
For a node at position p, its leftchild will always be at p + 1, and its right child willalways be at p+ 2 ?w`, where w` is the width of theleft child.
The symbol for each node can obviouslybe stored with the height.
For unaries, we requireexactly one unary rule per node, with the possibil-ity that it is the identity rule, and so we store twonodes: one for the ?pre-unary?
symbol, and one forthe ?post-unary.?
(Identity unary transitions are re-moved in post-processing.
)Algorithm 1 Non-recursive Viterbi implementation.The algorithm proceeds left-to-right in depth-firstorder along the array representing the tree.Input: Sentence length n, parse chart V[i,j]Output: Array tree of size 2?n?2tree[0].preunary?
ROOTtree[0].width?
ni?
0 .
Current leftmost position for spanfor p?
0 to 2?n?2 doj?
i + tree[p].width .
Rightmost positionpostu?
BestUnary(V, tree[p].preunary, i, j)tree[p].postunary?
parentif tree[p].width = 1 theni?
i + 1elselc, rc, k?
BestBinary(V, parent, i, j)tree[p + 1].preunary?
lctree[p + 1].width?
k ?
itree[p + 2?(k?i)].width?
j - ktree[p + 2?(k?i)].preunary?
rcend ifend forArmed with this representation, we are ready todescribe algorithm 1.
The algorithm proceeds in1905left-to-right order along the array.
First, the sym-bol of the root is recorded.
Then, for each node inthe tree, we search for the best unary rule continuingit.
If the node is a terminal, then no more nodes cancontain the current word, and so we advance the po-sition of the left most child.
Otherwise, if the nodeis a non-terminal, we then find its left and right chil-dren, entering their respective symbols and widthsinto the array representing the tree.The GPU implementation follows the algorithmoutline above although is somewhat technical.
Eachparse tree is handled by a separate thread block(thread blocks are groups of threads that can com-municate through shared memory, and run on a sin-gle SMX).
Each thread block includes a number ofthreads which are used to rapidly (in partly parallelfashion) iterate through rulesets and symbol vectorsfor the BestBinary and BestUnary operations usingcoalesced memory accesses.
Each thread block firstloads the complete set of L and R scores for thecurrent split being explored.
Recall that these arein consecutive memory locations using the ?span-major?
ordering, so these loads are coalesced.
Thenthe thread block parallel-iterates through the rulesfor the current parent symbol, which will be in a con-tiguous block of memory since the rules are sortedby parent symbol, and again are coalesced.
Thethread block therefore needs storage for all the L, Rsymbol scores and in addition working storage pro-portional to the number of threads (to hold the bestchild symbol and its score from each thread).
Thenumber of threads is chosen to maximize speed: toofew will cause each thread to do more work and torun more slowly.
Too many will limit the number ofthread blocks (since the total threads concurrentlyrunning on an SMX is 1024) that can run concur-rently.
We found 128 to be optimum.With these techniques, Viterbi search consumesapproximately 1% of the parser?s running time.
Itsthroughput is around 10 Gflops, and it is 50-100xfaster than a CPU reference implementation.7 ExperimentsThe parser was tested in an desktop computer withone Intel E5-2650 processor, 64 GB ram, and2 GTX-690 dual GPUs (effectively 4 GTX-680GPUs).
The high-level parser code is written in amatrix library in the Scala language, which accessGPU code through JNI and using the JCUDA wrap-per library for CUDATM.XX-kernel throughput was 900 Gflops per GPUfor sum-product calculation (which uses a singleFMA for most rules) and 700 Gflops per GPU formax-sum calculations (which requires two instruc-tions for most rules).
Net parser throughput in-cluding max-sum CKY evaluation, Viterbi scoringtraspose-copy etc was between 500 and 600 gi-gaflops per GPU, or about 2 teraflops total.
Parsingmax-length-30 sentences from the Penn Treebanktest set ran at 250 sentences/sec per GPU, or 1000sentences/sec total.
General sentences were parsedat about half this rate, 120 sentences/sec per GPU,or 480 sentences/sec for the system.8 Conclusions and Future WorkWe described a new approach to GPU constituencyparsing with surprisingly fast performance, closeto the theoretical limits of the GPU and similar todense matrix multiplication which achieves the de-vices highest practical throughput.
The resultingparser parses 1000 length-30 sentences per secondin a 4-GPU computer.
The parser has immediate ap-plication to parsing and eventually to parser training.The two highest-priority extensions are:Addition of pruning: coarse-to-fine score pruningshould be applicable to our GPU design as it is toCPU parsers.
GPU pruning will not be as granu-lar as CPU pruning and is unlikely to yield as largespeedups (4-5 orders of magnitude are common forCPU parser pruning).
But on the other hand, wehardly need speedups that large, and 1-2 orders ofmagnitude would be very useful.Direct generation of assembly code.
Currently ourcode generator produces (> 1.7 million lines, aboutsame as the number of rules) C source code whichmust be compiled into GPU binary code.
While ittakes only 8 seconds to generate the source code, ittakes more than an hour to compile it.
The com-piler evidently applies a number of optimizationsthat we cannot disable, and this takes time.
Thisis an obstacle to e.g.
using this framework to traina parser where there would be frequent updates tothe grammar.
However, since symbol variables cor-respond almost one-to-one with registers (modulo1906lifetime overlap and reuse, which our code gener-ator is slightly better at than the compiler), there isno reason for our code generator not to generate as-sembly code directly.
Presumably assembly code ismuch faster to translate into kernel modules than Csource, and hopefully this will lead to much fasterkernel generation.8.1 Code ReleaseThe code will be released under a BSD-style opensource license once its dependencies are fully in-tegrated.
Pre- and Final releases will be herehttps://github.com/jcanny/BIDParseReferencesAnonymous.
2012.
CUDA C PROGRAMMINGGUIDE.
Technical Report PG-02829-001-v5.0.
In-cluded with CUDATM Toolkit.Anonymous.
2013.
NVIDIA?s next generation CUDAcompute architecture: KeplerTM GK110.
Technicalreport.
Included with CUDATM Tootkit.Mark Johnson.
2011.
Parsing in parallel on mul-tiple cores and gpus.
In Proceedings of the Aus-tralasian Language Technology Association Workshop2011, pages 29?37, Canberra, Australia, December.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stAnnual Meeting of the Association for ComputationalLinguistics, pages 423?430, Sapporo, Japan, July.
As-sociation for Computational Linguistics.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
COMPUTA-TIONAL LINGUISTICS, 19(2):313?330.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProceedings of the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL?05), pages75?82, Ann Arbor, Michigan, June.
Association forComputational Linguistics.NVIDIA.
2012. private communication.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Human Language Tech-nologies 2007: The Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics; Proceedings of the Main Conference, pages404?411, Rochester, New York, April.
Association forComputational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 433?440, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Youngmin Yi, Chao-Yue Lai, Slav Petrov, and KurtKeutzer.
2011.
Efficient parallel cky parsing on gpus.In Proceedings of the 2011 Conference on ParsingTechnologies, Dublin, Ireland, October.1907
