In: Proceedings of CoNLL-2000 and LLL-2000, pages 142-144, Lisbon, Portugal, 2000.Use of ',Support Vector Learningfor Chunk IdentificationTaku  Kudoh and Yu j i  MatsumotoGraduate School of Information Science, Nara Inst i tute of Science and Technology{taku-ku, matsu}@is, aist-nara, ac.
jp1 In t roduct ionIn this paper, we explore the use of Support Vec-tor Machines (SVMs) for CoNLL-2000 sharedtask, chunk identification.
SVMs are so-calledlarge margin classifiers and are well-known astheir good generalization performance.
We in-vestigate how SVMs with a very large numberof features perform with the classification taskof chunk labelling.2 Suppor t  Vector  Mach inesSupport Vector Machines (SVMs), first intro-duced by Vapnik (Cortes and Vapnik, 1995;Vapnik, 1995), are relatively new learning ap-proaches for solving two-class pattern recog-nition problems.
SVMs are well-known fortheir good generalization performance, and havebeen applied to many pattern recognition prob-lems.
In the field of natural anguage process-ing, SVMs are applied to text categorization,and are reported to have achieved high accu-racy without falling into over-fitting even witha large number of words taken as the features(Joachims, 1998; Taira and Haruno, 1999)First of all, let us define the training datawhich belongs to either positive or negative classas follows:(Xl,YX),..., (Xl,Yl) Xi 6 R n, Yi 6 {+1,-1}xi is a feature vector of the i-th sample repre-sented by an n dimensional vector, yi is theclass (positive(+l) or negative(-1) class) labelof the i-th data.
In basic SVMs framework, wetry to separate the positive and negative xam-ples by hyperplane written as:(w-x )+b=0 w 6Rn,bE  R.SVMs find the "optimal" hyperplane (optimalparameter w, b) which separates the training00', ~Oo' ,  O ?o <~I",.
?
oSmall Margin0 , ,0 0"0~',, ?Large MarginFigure 1: Two possible separating hyperplanesdata into two classes precisely.
What "opti-mal" means?
In order to define it, we needto consider the marg in  between two classes.Figures 1 illustrates this idea.
The solid linesshow two possible hyperplanes, each of whichcorrectly separates the training data into twoclasses.
The two dashed lines parallel to theseparating hyperplane show the boundaries inwhich one can move the separating hyperplanewithout misclassification.
We call the distancebetween each parallel dashed lines as margin .SVMs take a simple strategy that finds the sep-arating hyperplane which maximizes its margin.Precisely, two dashed lines and margin (d) canbe written as:(w. x) + b = :kl, d = 2111wll.SVMs can be regarded as an optimization prob-lem; finding w and b which minimize \[\[w\[\[ underthe constraints: yi\[(w ?
xi) + b\] > 1.Furthermore, SVMs have potential to copewith the linearly unseparable training data.
Weleave the details to (Vapnik, 1995), the opti-mization problems can be rewritten into a dualform, where all feature vectors appear in theirdot product.
By simply substituting every dotproduct of xi and xj in dual form with any Ker-nel function K(xl, xj), SVMs can handle non-linear hypotheses.
Among the many kinds ofKernel functions available, we will focus on the142d-th polynomial kernel:K(xi,xj) = (x i .x j  + 1) dUse of d-th polynomial kernel function allowsus to build an optimal separating hyperplanewhich takes into account all combination of fea-tures up to d.We believe SVMs have advantage over con-ventional statistical learning algorithms, uch asDecision Tree, and Maximum Entropy Models,from the following two aspects:?
SVMs have high generalization perfor-mance independent of dimension of fea-ture vectors.
Conventional algorithms re-quire careful feature selection, which is usu-ally optimized heuristically, to avoid over-fitting.?
SVMs can carry out their learning withall combinations of given features with-out increasing computational complexityby introducing the Kernel function.
Con-ventional algorithms cannot handle thesecombinations efficiently, thus, we usuallyselect "important" combinations heuristi-cally with taking the trade-off between ac-curacy and computational complexity intoaccount.3 Approach  for Chunk  Ident i f i ca t ionThe chunks in the CoNLL-2000 shared task arerepresented with IOB based model, in which ev-ery word is to be tagged with a chunk label ex-tended with I (inside a chunk), O (outside achunk) and B (inside a chunk, but the preced-ing word is in another chunk).
Each chunk typebelongs to I or B tags.
For example, NP couldbe considered as two types of chunk, I-NP orB-NP.
In training data of CoNLL-2000 sharedtask, we could find 22 types of chunk 1 consid-ering all combinations of IOB-tags and chunktypes.
We simply formulate the chunking taskas a classification problem of these 22 types ofchunk.Basically, SVMs are binary classifiers, thus wemust extend SVMs to multi-class classifiers inorder to classify these 22 types of chunks.
It is1Precisely, the number  of combinat ion becomes 23.However, we do not consider I -LST tag since it dose notappear in t ra in ing data.known that there are mainly two approaches toextend from a binary classification task to thosewith K classes.
First approach is often usedand typical one "one class vs. all others".
Theidea is to build K classifiers that separate oneclass among from all others.
Second approachis pairwise classification.
The idea is to buildK ?
(K - 1)/2 classifiers considering all pairs ofclasses, and final class decision is given by theirmajority voting.
We decided to construct pair-wise classifiers for all the pairs of chunk labels,so that the total number of classifiers becomes22x21 231.
The reasons that we use pairwise 2 - -classifiers are as follows:?
Some experiments report hat combinationof pairwise classifier perform better than Kclassifier (Kret~el, 1999).?
The amount of training data for a pair isless than the amount of training data forseparating one class with all others.For the features, we decided to use all the in-formation available in the surrounding contexts,such as the words, their POS tags as well as thechunk labels.
More precisely, we give the fol-lowing for the features to identify chunk labelci at i-th word:w j, tjcj(j = i -2 ,  i -1 ,  i, i+1, i+ 2)(j = i -2 ,  i -1 )where wi is the word appearing at i-th word, tiis the POS tag of wi, and c/ is the (extended)chunk label at i-th word.
Since the chunk labelsare not given in the test data, they are decideddynamically during the tagging of chunk labels.This technique can be regarded as a sort of Dy-namic Programming (DP) matching, in whichthe best answer is searched by maximizing thetotal certainty score for the combination oftags.In using DP matching, we decided to keep notall ambiguities but a limited number of them.This means that a beam search is employed,and only the top N candidates are kept for thesearch for the best chunk tags.
The algorithmscans the test data from left to right and callsthe SVM classifiers for all pairs of chunk tagsfor obtaining the certainty score.
We definedthe certainty score as the number of votes forthe class (tag) obtained through the pairwisevoting.143Since SVMs are vector based classifier, theyaccept only numerical values for their features.To cope with this constraints, we simply expandall features as a binary-value taking either 0 or1.
By taking all words and POS tags appearingin the training data as features, the total dimen-sion of feature vector becomes as large as 92837.Generally, we need vast computational complex-ity and memories to handle such a huge dimen-sion of vectors.
In fact, we can reduce thesecomplexity considerably by holding only indicesand values of non-zero elements, since the fea-ture vectors are usually sparse, and SVMs onlyrequire the evaluation of dot products of eachfeature vectors for their training.In addition, although we could apply somecut-off threshold for the number of occurrencein the training set, we decided to use everything,not only POS tags but also words  themselves.The  reasons are that we  s imply do not wantto employ  a kind of "heuristics", and  SVMsare known to have a good  generalization per-fo rmance even with very large featm:es.4 Resu l tsWe have applied our proposed method to thetest data of CoNLL-2000 shared task, whiletraining with the complete training data.
Forthe kernel function, we use the 2-nd polynomialfunction.
We set the beam width N to 5 ten-tatively.
SVMs training is carried out with theSVM light package, which is designed and opti-mized to handle large sparse feature vector andlarge numbers of training examples(Joachims,2000; Joachims, 1999a).
It took about 1 dayto train 231 classifiers with PC-Linux (Celeron500Mhz, 512MB).Figure 1 shows the results of our experiments.The all the values of the chunking F-measure arealmost 93.5.
Especially, our method performswell for the chunk types of high frequency, suchas NP, VP and PP.5 D iscuss ionIn this paper, we propose Chunk identificationanalysis based on Support Vector Machines.Although we select features for learning invery straight way - -  using all available featuressuch as the words their POS tags without anycut-off threshold for the number of occurrence,we archive high performance for test data.test dataADJPADVPCONJPINT JLSTNPPPPRTSBARVPprecision79.22%80.86%62.50%100.00%0.00%93.72%96.60%80.58%89.29%93.76%all 93.45% 93.51%recall FZ=i69.63% 74.1280.48% 80.6755.56% 58.8250.00% 66.670.00% 0.0094.02% 93.8797.94% 97.2678.30% 79.4384.11% 86.6293.84% 93.8093.48Table 1: The results per chunk type with ourproposed SVMs based methodWhen we use other learning methods uch asDecision Tree, we have to select feature set man-ually to avoid over-fitting.
Usually, these fea-ture selection depends on heuristics, so that itis difficult to apply them to other classificationproblems in other domains.Memory based learning method can also hamdle all available features.
However, the functionto compute the distance between the test pat-tern and the nearest cases in memory is usuallyoptimized in an ad-hoc wayThrough our experiments, we have shown thehigh generalization performance and high lea-ture selection abilities of SVMs.ReferencesC.
Cortes and Vladimir N. Vapnik.
1995.
SupportVector Networks.
Machine Learning, 20:273-297.Thorsten Joachims.
1998.
Text Categorization withSupport Vector Machines: Learning with ManyRelevant Features.
In European Conference onMachine Learning (ECML).Thorsten Joachims.
1999a.
Making Large-ScaleSupport Vector Machine Learning Practical.
InAdvances in Kernel Methods.
MIT Press.Thorsten Joachims.
2000.
SVM tight version3.02.
http://www-ai.cs.uni-dortmund.de/SOFT-WARE/ S VM_LI G H T / svm_light.eng.html.Ulrich H.-G Krefiel.
1999.
Pairwise Classificationand Support Vector Machines.
In Advances inKernel Methods.
MIT Press.Hirotoshi Taira and Masahiko Haruno.
1999.
Fea-ture Selection in SVM Text Categorization.
InAAAI-99.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer.144
