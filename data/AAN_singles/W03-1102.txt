A Practical Text Summarizer by Paragraph Extraction for ThaiChuleerat Jaruskulchai and Canasai KruengkraiIntelligent Information Retrieval and Database LaboratoryDepartment of Computer Science, Faculty of ScienceKasetsart University, Bangkok, Thailandfscichj,g4364115@ku.ac.thAbstractIn this paper, we propose a practical ap-proach for extracting the most relevantparagraphs from the original documentto form a summary for Thai text.
Theidea of our approach is to exploit boththe local and global properties of para-graphs.
The local property can be consid-ered as clusters of significant words withineach paragraph, while the global propertycan be though of as relations of all para-graphs in a document.
These two proper-ties are combined for ranking and extract-ing summaries.
Experimental results onreal-world data sets are encouraging.1 IntroductionThe growth of electronic texts is becoming increas-ingly common.
Newspapers or magazines tend tobe available on the World-Wide Web.
Summarizingthese texts can help users access to the informationcontent more quickly.
However, doing this task byhumans is costly and time-consuming.
Automatictext summarization is a solution for dealing with thisproblem.Automatic text summarization can be broadlyclassified into two approaches: abstraction and ex-traction.
In contrast to abstraction that requires usingheavy machinery from natural language processing(NLP), including grammars and lexicons for pars-ing and generation (Hahn and Mani, 2000), extrac-tion can be easily viewed as the process of selectingrelevant excerpts (sentences, paragraphs, etc.)
fromthe original document and concatenating them into ashorter form.
Thus, most of recent works in this re-search area are based on extraction (Goldstein et al,1999).
Although one may argue that extraction ap-proach makes the text hard to read due to the lack ofcoherence, it also depends on the objective of sum-marization.
If we need to generate summaries thatcan be used to indicative what topics are addressedin the original document, and thus can be used toalert the uses as the source content, i.e., the indica-tive function (Mani et al, 1999), extraction approachis capable of handling this kind of tasks.There have been many researches on text sum-marization problem.
However, in Thai, we are inthe initial stage of developing mechanisms for au-tomatically summarizing documents.
It is a chal-lenge to summarize these documents, since they areextremely different from documents written in En-glish.
Similar to Chinese or Japanese, for the Thaiwriting system, there are no boundaries between ad-joining words, and also there are no explicit sen-tences boundaries within the document.
Fortunately,there is the use of the paragraph structure in theThai writing system, which is indicated by inden-tations and blank lines.
Therefore, extracting textspans from Thai documents at the paragraph level isa more practical way.In this paper, we propose a practical approach toThai text summarization by extracting the most rel-evant paragraphs from the original document.
Ourapproach considers both the local and global prop-erties of these paragraphs, which their meaning willbecome clear later.
We also present an efficient ap-proach for solving Thai word segmentation problem,which can enhance a basic word segmentation algo-rithm yielding more useful output.
We provide ex-perimental evidence that our approach achieves ac-ceptable performance.
Furthermore, our approachdoes not require the external knowledge other thanthe document itself, and be able to summarize gen-eral text documents.The remainder of this paper is organized as fol-lows.
In Section 2, we review some related workand contrast it with our work.
Section 3 describesthe preprocessing for Thai text, particularly on wordsegmentation.
In Section 4, we present our approachfor extracting relevant paragraphs in detail, includ-ing how to find clusters of significant words, how todiscover relations of paragraphs, and an algorithmfor combining these two approaches.
Section 5 de-scribes our experiments.
Finally, we conclude inSection 6 with some directions of future work.2 Related WorkA comprehensive survey of text summarization ap-proaches can be found in (Mani, 1999).
Webriefly review here based on extraction approach.Luhn (1959) proposed a simple but effective ap-proach by using term frequencies and their relatedpositions to weight sentences that are extracted toform a summary.
Subsequent works have demon-strated the success of Luhn?s approach (Buyukkok-ten et al, 2001; Lam-Adesina and Jones, 2001;Jaruskulchai et al, 2003).
Edmunson (1969) pro-posed the use of other features such as title words,sentence locations, and bonus words to improve sen-tence extraction.
Goldstein et al (1999) presentedan extraction technique that assigns weighted scoresfor both statistical and linguistic features in the sen-tence.
Recently, Salton et al (1999) have developeda model for representing a document by using undi-rected graphs.
The basic idea is to consider verticesas paragraphs and edges as the similarity betweentwo paragraphs.
They suggested that the most im-portant paragraphs should be linked to many otherparagraphs, which are likely to discuss topic coveredin those paragraphs.Statistical learning approaches have also beenstudied in text summarization problem.
The firstknown supervised learning algorithm was proposedby Kupiec et al (1995).
Their approach estimatesthe probability that a sentence should be includedin a summary given its feature values based on theindependent assumption of Bayes?
Rule.
Other su-pervised learning algorithms have already been in-vestigated.
Chuang and Yang (2000) studied severalalgorithms for extracting sentence segments, such asdecision tree, naive Bayes classifier, and neural net-work.
They also used rhetorical relations for rep-resenting features.
One drawback of the supervisedlearning algorithms is that they require an annotatedcorpus to learn accurately.
However, they may per-form well for summarizing documents in a specificdomain.This paper presents an approach for extracting themost relevant paragraphs from the original docu-ment to form a summary.
The idea of our approachis to exploit both the local and global properties ofparagraphs.
The local property can be considered asclusters of significant words within each paragraph,while the global property can be though of as re-lations of all paragraphs in the document.
Thesetwo properties can be combined and tuned to pro-duce a single measure reflecting the informativenessof each paragraph.
Finally, we can apply this combi-nation measure for ranking and extracting the mostrelevant paragraphs.3 Preprocessing for Thai TextThe first step for working with Thai text is to tok-enize a given text into meaningful words, since theThai writing system has no delimiters to indicateword boundaries.
Thai words are not delimited byspaces.
The spaces are only used to break the ideaor draw readers?
attention.
In order to determineword boundaries, we employed the longest matchingalgorithm (Sornlertlamvanich, 1993).
The longestmatching algorithm starts with a text span that couldbe a phrase or a sentence.
The algorithm tries toalign word boundaries according to the longest pos-sible matching character compounds in a lexicon.
Ifno match is found in the lexicon, it drops the right-most character in that text according to the morpho-logical rules and begins the same search.
If a word isfound, it marks a boundary at the end of the longestword, and then begins the same search starting at theremainder following the match.In our work, the lexicon contained 32675 words.However, the limitation of this algorithm is that ifthe target words are compound words or unknownwords, it tends to produce incorrect results.
For ex-ample, a compound word is segmented as the fol-lowing:??????????????????
(Human Rights Organization)??????_?????_???_??_?
?Since this compound word does not appear in thelexicon, it becomes small useless words after theword segmentation process.
We further describe anefficient approach to alleviate this problem by usingan idea of phrase construction (Ohsawa et al, 1998).Let wi be a word that is firstly tokenized by us-ing the longest matching algorithm.
We refer tow1w2 .
.
.
wn as a phrase candidate, if n > 1, andno punctuation and stopwords occur between w1and wn.
It is well accepted in information retrievalcommunity that words can be broadly classified intocontent-bearing words and stopwords.
In Thai, wefound that words that perform as function words canbe used in place of stopwords similar to English.We collected 253 most frequently occurred wordsfor making a list of Thai stopwords.Given a phrase candidate consisting of n words,we can generate a set of phrases in the followingform:W =??????
?w1w2 w1w2w3 .
.
.
w1w2w3 .
.
.
wn?1wnw2w3 .
.
.
w2w3 .
.
.
wn?1wn...wn?1wn???????
(1)For example, if a phrase candidate consistsof four words, w1w2w3w4, we then obtain W ={w1w2, w1w2w3, w1w2w3w4, w2w3, w2w3w4, w3w4}.Let l be the number of set elements that can becomputed from l = (n ?
(n?
1))/2 = (4 ?
3)/2 = 6.Since we use both stopwords and punctuationfor bounding the phrase candidate, this approachproduces a moderate number of set elements.Let V be a temporary lexicon.
After buildingall the phrase candidates in the document and gen-erating their sets of phrases, we can construct Vby adding phrases that the number of occurrencesexceeds some threshold.
This idea is to exploitredundancy of phrases occurring in the document.If a generated phrase frequently occurs, this indi-cates that it may be a meaningful phrase, and shouldbe included in the temporary lexicon using for re-segmenting words.We denote U to be a main lexicon.
After obtain-ing the temporary lexicon V , we then re-segmentwords in the document by using U ?
V .
With us-ing the combination of these two lexicons, we canrecover some words from the first segmentation.
Al-though we have to do the word segmentation pro-cess twice, the computation time is not prohibitive.Furthermore, we obtain more meaningful words thatcan be extracted to form keywords of the document.4 Generating Summaries by Extraction4.1 Finding Clusters of Significant WordsIn this section, we first describe an approach forfinding clusters of significant words in each para-graph to calculate the local clustering score.
Ourapproach is reminiscent of Luhn?s approach (1959)but uses the other term weighting technique insteadof the term frequency.
Luhn suggested that the fre-quency of a word occurrence in a document, as wellas its relative position determines its significance inthat document.
More recent works have also em-ployed Luhn?s approach as a basis component forextracting relevant sentences (Buyukkokten et al,2001; Lam-Adesina and Jones, 2001).
This ap-proach performs well despite of its simplicity.
In ourprevious work (Jaruskulchai et al, 2003), we alsoapplied this approach for summarizing and brows-ing Thai documents through PDAs.Let ?
be a subset of a continuous sequence ofwords in a paragraph, {wu .
.
.
wv}.
The subset ?is called a cluster of significant words if it has thesecharacteristics:?
The first word wu and the last word wv in thesequence are significant words.?
Significant words are separated by not morethan a predefined number of insignificantwords.For example, we can partition a continuous se-quence of words in a paragraph into clusters asshown in Figure 1.
The paragraph consists of twelvewords.
We use the boldface to indicate positionsof significant words.
Each cluster is enclosed withbrackets.
In this example, we define that a clusteris created whereby significant words are separatedby not more than three insignificant words.
Notethat many clusters of significant words can be foundin the paragraph.
The highest score of the clustersfound in the paragraph is selected to be the para-graph score.
Therefore, the local clustering scorefor paragraph si can be calculated as follows:Lsi = argmax?ns(?, si)2n(?, si), (2)where ns(?, si) is the number of bracketed signif-icant words, and n(?, si) is the total number ofbracketed words.We can see that the first important step in this pro-cess is to mark positions of significant words foridentifying the clusters.
Our goal is to find topicalwords, which are indicative of the topics underly-ing the document.
According to Luhn?s approach,the term frequencies is used to weight all the words.The other term weighting scheme frequently usedis TFIDF (Term Frequency Inverse Document Fre-quency) (Salton and Buckley, 1988).
However, thistechnique needs a corpus for computing IDF score,causing the genre-dependent problem for generictext summarization task.In our work, we decide to use TLTF (Term LengthTerm Frequency) term weighting technique (Bankoet al, 1999) for scoring words in the document in-stead of TFIDF.
TLTF multiplies a monotonic func-tion of the term length by a monotonic function ofthe term frequency.
The basic idea of TLTF is basedon the assumption that words that are used morefrequently tend to be shorter.
Such words are notstrongly indicative of the topics underlying in thedocument, such as stopwords.
In contrast, wordsthat are used less frequently tend to be longer.
Onesignificant benefit of using TLTF term weightingtechnique for our task is that it does not requireany external resources, only using the informationwithin the document.w1[w2w3w4] w5w6w7w8[w9w10w11w12]Figure 1: Clusters of significant words.4.2 Discovering Relations of ParagraphsWe now move on to describe an approach for dis-covering relations of paragraphs.
Given a docu-ment D, we can represent it by an undirected graphG = (V,E), where V = {s1, .
.
.
, sm} is the set ofparagraphs in that document.
An edge (si, sj) isin E, if the cosine similarity between paragraphssi and sj is above a certain threshold, denoted ?.A paragraph si is considered to be a set of words{wsi,1 , wsi,2 , .
.
.
, wsi,t}.
The cosine similarity be-tween two paragraphs can be calculated by the fol-lowing formula:sim(si, sj) =?tk=1 wsi,kwsj,k?
?tk=1 w2si,k?tk=1 w2sj,k.
(3)The graph G is called the text relationship map ofD (Salton et al, 1999).
Let dsi be the degree of nodesi.
We then refer to dsi as the global connectivityscore.
Generating a summary for a given documentcan be processed by sorting all the nodes with dsi indecreasing order, and then extracting n top-rankednodes, where n is the target number of paragraphsin the summary.This idea is based on Salton et al?s approach thatalso performs extraction at the paragraph level.
Theysuggested that since a highly bushy node is linkedto a number of other nodes, it has an overlappingvocabulary with several paragraphs, and is likely todiscuss topics covered in many other paragraphs.Consequently, such nodes are good candidates forextraction.
They then used a global bushy path thatis constructed out of n most bushy nodes to form thesummary.
Their experimental results on encyclope-dia articles demonstrates reasonable results.However, when we directly applied this approachfor extracting paragraphs from moderately-sizeddocuments, we found that using only the global con-nectivity score is inadequate to measure the infor-mativeness of paragraphs in some case.
In orderto describe this situation, we consider an exampleof a text relationship map in Figure 2.
The map isP1P2P3P4P5P6P7P8 P9P10Figure 2: Text relationship map of an online news-paper article using ?
= 0.10.P1P2P3P4P5P6P7P8 P9P10Figure 3: Text relationship map of the same article,but using ?
= 0.20.constructed from an online newspaper article.1 Thesimilarity threshold ?
is 0.1.
As a result, edges withsimilarities less than 0.1 do not appear on the map.Node P4 obtains the maximum global connectivityscore at 9.
However, the global connectivity scoreof nodes P3, P5, and P6 is 7, and nodes P7 and P8 is6, which are slightly different.
When we increase thethreshold ?
= 0.2, we obtain a text relationship mapas shown in Figure 3.
Nodes P4 and P7 now achievethe same maximum global connectivity score at 5.Nodes P3, P5, and P6 get the same score at 4.From above example, it is hard to determine that1The article is available at: http://mickey.sci.ku.ac.th/?TextSumm/sample/t1.htmlnode P4 is more relevant than nodes such as P3 orP5, since their scores are only different at 1 point.Our preliminary experiments with many other docu-ments lead to the suggestion that the global connec-tivity score of nodes in the text relation map tendsto be slightly different on some document lengths.Given a compression rate (ratio of the summarylength to the source length), if we immediately ex-tract these nodes of paragraphs, many paragraphswith the same score are also included in the sum-mary.4.3 Combining Local and Global PropertiesIn this section, we present an algorithm that takesadvantage of both the local and global propertiesof paragraphs for generating extractive summaries.From previous sections, we describe two differ-ent approaches that can be used to extract relevantparagraphs.
However, these extraction schemes arebased on different views and concepts.
The localclustering score only captures the content of infor-mation within paragraphs, while the global connec-tivity score mainly considers the structural aspectof the document to evaluate the informativeness ofparagraphs.
This leads to our motivation for uni-fying good aspects of these two properties.
Wecan consider the local clustering score as the localproperty of paragraphs, and the global connectivityscore as the global property.
Here we propose analgorithm that combines the local clustering scorewith the global connectivity score to get a singlemeasure reflecting the informativeness of each para-graph, which can be tuned according to the relativeimportance of properties.Our algorithm proceeds as follows.
Given a doc-ument, we start by eliminating stopwords and ex-tracting all unique words in the document.
Theseunique words are used to be the document vocabu-lary.
Therefore, we can represent a paragraph si as avector.
We then compute similarities between all theparagraph vectors using equation (3), and eliminateedges with similarities less than a threshold in orderto build the text relationship map.
This process auto-matically yields the global connectivity scores of theparagraphs.
Next, we weight each word in the doc-ument vocabulary using TLTF term weighting tech-nique.
All the words are sorted by their TLTF scores,and top r words are selected to be significant words.We mark positions of significant words in each para-graph to calculate the local clustering score.
Afterobtaining both scores, for each paragraph si, we cancompute the combination score by using the follow-ing ranking function:F (si) = ?G?
+ (1 ?
?)L?
, (4)where G?
is the normalized global connectivityscore, and L?
is the normalized local clusteringscore.
The normalized global connectivity score G?can be calculated as follows:G?
=dsidmax, (5)where dmax is the degree of the node that has themaximum edges using for normalization, resultingthe score in the range of [0, 1].
Using equation (2),L?
is given by:L?
=LsiLmax, (6)where Lmax is the maximum local clustering scoreusing for normalization.
Similarly, it results thisscore in the range of [0, 1].
The parameter ?
is var-ied depending on the relative importance of the com-ponents G?
and L?.
Therefore, we can rank all theparagraphs according to their combination scores indecreasing order.
We finally extract n top-rankedparagraphs corresponding to the compression rate,and rearrange them in chronological order to formthe output summary.5 Experiments5.1 Data SetsThe typical approach for testing a summarizationsystem is to create an ?ideal?
summary, eitherby professional abstractors or merging summariesprovided by multiple human subjects using meth-ods such as majority opinion, union, or intersec-tion (Jing et al, 1998).
This approach is knownas intrinsic method.
Unlike in English, standarddata sets in Thai are not yet available for evaluat-ing text summarization system.
However, in orderto observe characteristics of our algorithm, we col-lected Thai documents, including agricultural news(D1.AN), general news (D2.GN), and columnist?sarticles (D3.CA) to make data sets.
Each data setconsists of 10 documents, and document sizes rangefrom 1 to 4 pages.
We asked a student in the Depart-ment of Thais, Faculty of Liberal Arts, for manualsummarization by selecting the most relevant para-graphs that can indicate the main points of the docu-ment.
These paragraphs are called extracts, and thenare used for evaluating our algorithm.5.2 Performance EvaluationsWe evaluate results of summarization by using thestandard precision, recall, and F1.
Let J be the num-ber of extracts in the summary, K be the number ofselected paragraphs in the summary, and M be thenumber of extracts in the test document.
We thenrefer to precision of the algorithm as the fraction be-tween the number of extracts in the summary and thenumber of selected paragraphs in the summary:Precision =JK, (7)recall as the fraction between the number of extractsin the summary and the number of extracts in the testdocument:Recall =JM.
(8)Finally, F1, a combination of precision and recall,can be calculated as follows:F1 =2 ?
Precision ?RecallPrecision + Recall.
(9)5.3 Experimental ResultsIn this section, we provide experimental evidencethat our algorithm gives acceptable performance.The compression rate of paragraph extraction toform a summary is 20% and 30%.
These rates yieldthe number of extracts in the summary comparableto the number of actual extracts in a given test doc-ument.
The threshold ?
of the cosine similarity is0.2.
The parameter ?
for combining the local andglobal properties is 0.5.
For the distance betweensignificant words in a cluster, we set that significantwords are separated by not more than three insignif-icant words.Table 1 and 2 show a summary of precision, re-call, and F1 for each compression rate, respectively.We can see that average precision values of our al-gorithm slightly decrease, but average recall val-ues increase when we increase the compression rate.?????????
(Keywords):???????????
?, ?????????????????
?, ??????????
?, ?????
?, ??????????????
?, ????????
?, ?????????,??????????
?, ????????????
?, ???????????
?, ??????????????????????????????
20% (Summarization result at 20%):?????????????????????
???????????
???
??????????????
????
????????????????????????????????????????????????
???????????????
?????????????????????????????????????????????????????????????????????????????????????????????????
?????????????????????????????????????????????
?????????????????????????????????????????????????????????????????????????????
????????????????????????????????????????
???????????
?????????????????????????????????????????????????????????????????????????????????????????
1-4 ??????????????????????
??????????????????????????????????????
??????????
??????????
(Centrino)????????????????????????????
????????????????????????????
?????????????????????????????????????
(WiFi) ??????????
?Figure 4: An example of keywords and extracted summaries in Thai.Data set Precision Recall F1D1.AN 0.600 0.448 0.509D2.GN 0.518 0.385 0.431D3.CA 0.530 0.330 0.404Table 1: Evaluation results obtained by using com-pression rate 20%.Since using higher compression rate tends to selectmore paragraphs from the document, it increases thechance that the selected paragraphs will be matchedwith the target extracts.
On the other hand, it alsoselects irrelevant paragraphs to be included in thesummary, so precision can decrease.
Further experi-ments on larger text corpora are needed to determinethe performance of our summarizer.
However, thesepreliminary results are very encouraging.
Figure4 illustrates an example of keywords and extractedsummaries for a Thai document using compressionrate 20% .
The implementation of our algorithm isnow available for user testing at http://mickey.sci.ku.ac.th/?TextSumm/index.html.
The com-putation time to summarize moderately-sized docu-ments, such as newspaper articles, is less one sec-ond.6 Conclusions and Future WorkIn this paper, we have presented a practical ap-proach to Thai text summarization by extracting theData set Precision Recall F1D1.AN 0.550 0.577 0.555D2.GN 0.464 0.467 0.453D3.CA 0.523 0.462 0.488Table 2: Evaluation results obtained by using com-pression rate 30%.most relevant paragraphs from the original docu-ment.
Our approach takes advantage of both thelocal and global properties of paragraphs.
The algo-rithm that combines these two properties for rankingand extracting paragraphs is given.
Furthermore, thealgorithm does not require the external knowledgeother than the document itself, and be able to sum-marize general text documents.In future work, we intend to conduct experimentswith different document genres.
We continue to fur-ther develop standard data sets for evaluating Thaitext summarization system.
Many research ques-tions remain.
Since extraction performs at the para-graph level, the paragraph lengths may affect thesummarization results.
The recent approach forediting extracted text spans (Jing and McKeown,2000) may also produce improvement for our algo-rithm.
We believe that our algorithm is language-independent, which can summarize documents writ-ten in many other languages.
We plan to experimen-tally test our algorithm with available standard datasets in English.AcknowledgmentsThis research was supported by the grant of the Na-tional Research Council of Thailand, 2002.
Manythanks to Tan Sinthurahat (Thammasat University)for manual summarizing the data sets.ReferencesBanko, M., Mittal, V., Kantrowitz, M., and Goldstein, J.1999.
Generating extraction-based summaries fromhand-written summaries by aligning text spans.
InProceedings of PACLING?99.Buyukkokten, O., Garcia-Molina, H., and Paepcke, A.2001.
Seeing the whole in parts: Text summarizationfor web browsing on handheld devices.
WWW10.Chuang, W. T., and Yang, J.
2000.
Extracting sentencesegments for text summarization: A machine learningapproach.
In Proceedings of the 23rd ACM SIGIR,152?159.Edmundson, H. P. 1969.
New methods in automatic ex-traction.
Journal of the ACM, 16(2):264?285.Goldstein, J., Kantrowitz, M., Mittal, V., and Carbonell,J.
1999.
Summarizing text documents: Sentence se-lection and evaluation metrics.
In Proceedings of the22nd ACM SIGIR, 121?128.Hahn, U., and Mani, I.
2000.
The challenges of auto-matic summarization.
IEEE Computer, 33(11):29?35.Jaruskulchai, C., Khanthong, A., and Tantiprasongchai,W.
2003.
A Framework for Delivery of Thai Contentthrough Mobile Devices.
Closing Gaps in the DigitalDivide Regional Conference on Digital GMS.
AsianInstitute of Technology, 190?194.Jing, H., Barzilay, R., McKeown, K., and Elhadad, M.1998.
Summarization evaluation methods: Experi-ments and analysis.
AAAI Intelligent Text Summariza-tion Workshop, 60?68.Jing, H., and McKeown, K. 2000.
Cut and paste basedtext summarization.
In Proceedings of the 1st Confer-ence of the North American Chapter of the Associationfor Computational Linguistics.Kupiec, J., Pedersen, J., and Chen, F. 1995.
A train-able document summarizer.
In Proceedings of the 18thACM SIGIR, 68?73.Lam-Adesina, M., and Jones, G. J. F. 2001.
Applyingsummarization techniques for term selection in rele-vance feedback.
In Proceedings of the 24th ACM SI-GIR, 1?9.Luhn, H. P. 1959.
The automatic creation of literatureabstracts.
IBM Journal of Research and Development,159?165.Mani, I., Firmin, T., House, D., Klein, G., Sundheim,B., Hirschman, L. 1999.
The TIPSTER SUMMACText Summarization Evaluation.
In Proceedings ofEACL?99.Mani, I., and Maybury, M. T. 1999.
Advances in ac-tomatic text summarization.
MIT Press.Ohsawa, Y., Benson, N. E., and Yachida, M. 1998.
Key-Graph: Automatic indexing by cooccurrence graphbased on building construction metaphor.
In Proceed-ings of EAdvanced Digital Library Conference.Salton, G., and Buckley, C. 1988.
Term weighting ap-proaches in automatic text retrieval.
Information Pro-cessing and Management, 24(5):513?523.Salton, G., Singhal, A., Mitra, M., and Buckley, C. 1999.Automatic text structuring and summarization.
InMani, I. and Maybury, M.
(Eds.
), Advances in auto-matic text summarization.
MIT Press.Sornlertlamvanich, V. 1993.
Word segmentation for Thaiin machine translation system.
Machine Translation,National Electronics and Computer Technology Cen-ter, 50?56.
