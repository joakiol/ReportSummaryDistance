Proceedings of the ACL-HLT 2011 Student Session, pages 6?11,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsSentence Ordering Driven by Local and Global Coherencefor Summary GenerationRenxian ZhangDepartment of ComputingThe Hong Kong Polytechnic Universitycsrzhang@comp.polyu.edu.hkAbstractIn summarization, sentence ordering isconducted to enhance summary readability byaccommodating text coherence.
We propose agrouping-based ordering framework thatintegrates local and global coherence concerns.Summary sentences are grouped beforeordering is applied on two levels: group-leveland sentence-level.
Different algorithms forgrouping and ordering are discussed.
Thepreliminary results on single-document newsdatasets demonstrate the advantage of ourmethod over a widely accepted method.1 Introduction and BackgroundThe canonical pipeline of text summarizationconsists of topic identification, interpretation, andsummary generation (Hovy, 2005).
In the simplecase of extraction, topic identification andinterpretation are conflated to sentence selectionand concerned with summary informativeness.
Incomparison, summary generation addressessummary readability and a frequently discussedgeneration technique is sentence ordering.It is implicitly or explicitly stated that sentenceordering for summarization is primarily driven bycoherence.
For example, Barzilay et al (2002) uselexical cohesion information to model localcoherence.
A statistical model by Lapata (2003)considers both lexical and syntactic features incalculating local coherence.
More globally biasedis Barzilay and Lee?s (2004) HMM-based contentmodel, which models global coherence with worddistribution patterns.Whilst the above models treat coherence aslexical or topical relations, Barzilay and Lapata(2005, 2008) explicitly model local coherence withan entity grid model trained for optimal syntacticrole transitions of entities.Although coherence in those works is modeledin the guise of ?lexical cohesion?, ?topiccloseness?, ?content relatedness?, etc., fewpublished works simultaneously accommodatecoherence on the two levels: local coherence andglobal coherence, both of which are intriguingtopics in text linguistics and psychology.
Forsentences, local coherence means the well-connectedness between adjacent sentences throughlexical cohesion (Halliday and Hasan, 1976) orentity repetition (Grosz et al, 1995) and globalcoherence is the discourse-level relationconnecting remote sentences (Mann andThompson, 1995; Kehler, 2002).
An abundance ofpsychological evidences show that coherence onboth levels is manifested in text comprehension(Tapiero, 2007).
Accordingly, an apt sentenceordering scheme should be driven by suchconcerns.We also note that as sentence ordering is usuallydiscussed only in the context of multi-documentsummarization, factors other than coherence arealso considered, such as time and source sentenceposition in Bollegala et al?s (2006) ?agglomerativeordering?
approach.
But it remains an openquestion whether sentence ordering is non-trivialfor single-document summarization, as it has longbeen recognized as an actual strategy taken byhuman summarizers (Jing, 1998; Jing andMcKeown, 2000) and acknowledged early in workon sentence ordering for multi-documentsummarization (Barzilay et al, 2002).In this paper, we outline a grouping-basedsentence ordering framework that is driven by theconcern of local and global coherence.
Summarysentences are grouped according to theirconceptual relatedness before being ordered on twolevels: group-level ordering and sentence-levelordering, which capture global coherence and localcoherence in an integrated model.
As a preliminarystudy, we applied the framework to single-6document summary generation and obtainedinteresting results.The main contributions of this work are: (1) westress the need to channel sentence orderingresearch to linguistic and psychological findingsabout text coherence; (2) we propose a grouping-based ordering framework that integrates bothlocal and global coherence; (3) we find inexperiments that coherence-driven sentenceordering improves the readability of single-document summaries, for which sentence orderingis often considered trivial.In Section 2, we review related ideas andtechniques in previous work.
Section 3 providesthe details of grouping-based sentence ordering.The preliminary experimental results are presentedin Section 4.
Finally, Section 5 concludes thewhole paper and describes future work.2 Grouping-Based OrderingOur ordering framework is designed to captureboth local and global coherence.
Globally, weidentify related groups among sentences and findtheir relative order.
Locally, we strive to keepsentence similar or related in content close to eachother within one group.2.1 Sentence RepresentationAs summary sentences are isolated from theiroriginal context, we retain the important contentinformation by representing sentences as conceptvectors.
In the simplest case, the ?concept?
isequivalent to content word.
A drawback of thispractice is that it considers every content wordequally contributive to the sentence content, whichis not always true.
For example, in the newsdomain, entities realized as NPs are moreimportant than other concepts.To represent sentences as entity vectors, weidentify both common entities (as the head nounsof NPs) and named entities.
Two common entitiesare equivalent if their noun stems are identical orsynonymous.
Named entities are usually equatedby identity.
But in order to improve accuracy, wealso consider: 1) structural subsumption (one ispart of another); 2) hypernymy and holonymy (thenamed entities are in a superordinate-subordinateor part-whole relation).Now with summary sentence Si and m entities eik(k = 1 ?
m), Si = (wf(ei1), wf(ei2), ?, wf(eim)),where wf(eik) = wk?f(eik), f(eik) is the frequency ofeik and wk is the weight of eik.
We define wk = 1 ifeik is a common entity and wk = 2 if eik is a namedentity.
We give double weight to named entitiesbecause of their significance to news articles.
Afterall, a news story typically contains events, places,organizations, people, etc.
that denote the newstheme.
Other things being equal, two sentencessharing a mention of named entities arethematically closer than two sentences sharing amention of common entities.Alternatively, we can realize the ?concept?
as?event?
because events are prevalent semanticconstructs that bear much of the sentence contentin some domains (e.g., narratives and news reports).To represent sentences as event vectors, we canfollow Zhang et al?s (2010) method at the cost ofmore complexity.2.2 Sentence GroupingTo meet the global need of identifying sentencegroups, we develop two grouping algorithms byapplying graph-based operation and clustering.Connected Component Finding (CC)This algorithm treats grouping sentences asfinding connected components (CC) in a text graphTG = (V, E), where V represents the sentences andE the sentence relations weighted by cosinesimilarity.
Edges with weight < t, a threshold, areremoved because they represent poor sentencecoherence.The resultant graph may be disconnected, inwhich we find all of its connected components,using depth-first search.
The connectedcomponents are the groups we are looking for.Note that this method cannot guarantee that everytwo sentences in such a group are directly linked,but it does guarantee that there exists a pathbetween every sentence pair.Modified K-means Clustering (MKM)Observing that the CC method finds onlycoherent groups, not necessarily groups ofcoherent sentences, we develop a second algorithmusing clustering.
A good choice might be K-meansas it is efficient and outperforms agglomerativeclustering methods in NLP applications (Steibachet al, 2000), but the difficulty with theconventional K-means is the decision of K.Our solution is modified K-means (MKM) basedon (Wilpon and Rabiner, 1985).
Let?s denote7cluster i by CLi and cluster similarity by Sim(CLi)=, ( ( , ))im in i im inS S CLMin Sim S S?, where ( , )im inSim S S is theircosine.
The following illustrates the algorithm.1.
CL1 = all the sentence vectors;2.
Do the 1-means clustering by assigning all thevectors to CL1;3.
While at least 1 cluster has at least 2 sentences andMin(Sim(CLi)) <  t, do:3.1 If Sim(Sm, Sn) = Min(Sim(CLi)), create two newcentroids as Sm and Sn;3.2 Do the conventional K-means clustering untilclusters stabilize;The above algorithm stops iterating when eachcluster contains all above-threshold-similaritysentence pairs or only one sentence.
Unlike CC,MKM results in more strongly connected groups,or groups of coherence sentences.2.3 Ordering AlgorithmsAfter the sentences are grouped, ordering is to beconducted on two levels: group and sentence.Composed of closely related sentences, groupssimulate high-level textual constructs, such as?central event?, ?cause?, ?effect?, ?background?,etc.
for news articles, around which sentences aregenerated for global coherence.
For an intuitiveexample, all sentences about ?cause?
shouldimmediately precede all sentences about ?effect?
toachieve optimal readability.
We propose twoapproaches to group-level ordering.
1) If the groupsentences come from the same document, group(Gi) order is decided by the group-representingsentence (gi) order (  means ?precede?)
in the text.i j i jg g G G?2) Group order is decided in a greedy fashion inorder to maximize the connectedness betweenadjacent groups, thus enhancing local coherence.Each time a group is selected to achieve maximumsimilarity with the ordered groups and the firstordered group (G1) is selected to achievemaximum similarity with all the other groups.1 'arg max ( , ')G G GG Sim G G??
??
?1unordered groups 1arg max ( , )ii jG jG Sim G G??
??
?
(i > 1)where Sim(G, G?)
is the average sentence cosinesimilarity between G and G?.Within the ordered groups, sentence-levelordering is aimed to enhance local coherence byplacing conceptually close sentences next to eachother.
Similarly, we propose two approaches.
1) Ifthe sentences come from the same document, theyare arranged by the text order.
2)  Sentence order isgreedily decided.
Similar to the decision of grouporder, with ordered sentence Spi in group Gp:1 'arg max ( , ')pp S G S SS Sim S S?
??
??
?1unordered sentences in 1arg max ( , )pipi pjS G jS Sim S S??
??
?
(i > 1)Note that the text order is used as a commonheuristic, based on the assumption that thesentences are arranged coherently in the sourcedocument, locally and globally.3 Experiments and Preliminary ResultsCurrently, we have evaluated grouping-basedordering on single-document summarization, forwhich text order is usually considered sufficient.But there is no theoretical proof that it leads tooptimal global and local coherence that concernsus.
On some occasions, e.g., a news articleadopting the ?Wall Street Journal Formula?
(Richand Harper, 2007) where conceptually relatedsentences are placed at the beginning and the end,sentence conceptual relatedness does notnecessarily correlate with spatial proximity andthus selected sentences may need to be rearrangedfor better readability.
We are not aware of anypublished work that has empirically comparedalternative ways of sentence ordering for single-document summarization.
The experimental resultsreported below may draw some attention to thistaken-for-granted issue.3.1 Data and MethodWe prepared 3 datasets of 60 documents each, thefirst (D400) consisting of documents of about 400words from the Document UnderstandingConference (DUC) 01/02 datasets; the second(D1k) consisting of documents of about 1000words manually selected from popular Englishjournals such as The Wall Street Journal, TheWashington Post, etc; the third (D2k) consisting ofdocuments of about 2000 words from the DUC01/02 dataset.
Then we generated 100-wordsummaries for D400 and 200-word summaries forD1k and D2k.
Since sentence selection is not our8focus, the 180 summaries were all extractsproduced by a simple but robust summarizer builton term frequency and sentence position (Aone etal., 1999).Three human annotators were employed to eachprovide reference orderings for the 180 summariesand mark paragraph (of at least 2 sentences)boundaries, which will be used by one of theevaluation metrics described below.In our implementation of the grouping-basedordering, sentences are represented as entityvectors and the threshold t = ( ( ), )m nAvg Sim S S c?,the average sentence similarity in a groupmultiplied by a coefficient empirically decided onseparate held-out datasets of 20 documents foreach length category.
The ?group-representingsentence?
is the textually earliest sentence in thegroup.
We experimented with both CC and MKMto generate sentence groups and all the proposedalgorithms in 2.3 for group-level and sentence-level orderings, resulting in 8 combinations as testorderings, each coded in the format of ?Grouping(CC/MKM) / Group ordering (T/G) / Sentenceordering (T/G)?, where T and G represent the textorder approach and the greedy selection approachrespectively.
For example, ?CC/T/G?
meansgrouping with CC, group ordering with text order,and sentence ordering with the greedy approach.We evaluated the test orderings against the 3reference orderings  and compute the average(Madnani et al, 2007) by using 3 different metrics.The first metric is Kendall?s ?
(Lapata 2003,2006), which has been reliably used in orderingevaluations (Bollegala et al, 2006; Madnani et al,2007).
It measures ordering differences in terms ofthe number of adjacent sentence inversionsnecessary to convert a test ordering to the referenceordering.41 ( 1)mN N?
?
?
?In this formula, m represents the number ofinversions described above and N is the totalnumber of sentences.The second metric is the Average Continuity(AC) proposed by Bollegala et al (2006), whichcaptures the intuition that the quality of sentenceorderings can be estimated by the number ofcorrectly arranged continuous sentences.2lo AC (1/ ( 1 g( )) )k nnex Pp k ???
??
?In this formula, k is the maximum number ofcontinuous sentences, ?
is a small value in case Pn= 1.
Pn, the proportion of continuous sentences oflength n in an ordering, is defined as m/(N ?
n + 1)where m is the number of continuous sentences oflength n in both the test and reference orderingsand N is the total number of sentences.
Following(Bollegala et al, 2006), we set k = Min(4, N) and ?= 0.01.We also go a step further by considering onlythe continuous sentences in a paragraph marked byhuman annotators, because paragraphs are localmeaning units perceived by human readers and theorder of continuous sentences in a paragraph ismore strongly grounded than the order ofcontinuous sentences across paragraph boundaries.So in-paragraph sentence continuity is a betterestimation for the quality of sentence orderings.This is our third metric: Paragraph-level AverageContinuity (P-AC).2loP-AC g((1/ ( 1) ))k nnPexp Pk ????
?
?Here PPn = m?/(N ?
n + 1), where m?
is the numberof continuous sentences of length n in both the testordering and a paragraph of the reference ordering.All the other parameters are as defined in AC andPn.3.2 ResultsThe following tables show the results measured byeach metric.
For comparison, we also include a?Baseline?
that uses the text order.
For eachdataset, two-tailed t-test is conducted between thetop scorer and all the other orderings and statisticalsignificance (p < 0.05) is marked with *.?
AC P-ACBaseline 0.6573* 0.4452* 0.0630CC/T/T 0.7286 0.5688 0.0749CC/T/G 0.7149 0.5449 0.0714CC/G/T 0.7094 0.5449 0.0703CC/G/G 0.6986 0.5320 0.0689MKM/T/T 0.6735 0.4670* 0.0685MKM/T/G 0.6722 0.4452* 0.0674MKM/G/T 0.6710 0.4452* 0.0660MKM/G/G 0.6588* 0.4683* 0.0682Table 1: D400 Evaluation9?
AC P-ACBaseline 0.3276 0.0867* 0.0428*CC/T/T 0.3324 0.0979 0.0463*CC/T/G 0.3276 0.0923 0.0436*CC/G/T 0.3282 0.0944 0.0479*CC/G/G 0.3220 0.0893* 0.0428*MKM/T/T 0.3390 0.1152 0.0602MKM/T/G 0.3381 0.1130 0.0588MKM/G/T 0.3375 0.1124 0.0576MKM/G/G 0.3379 0.1124 0.0581Table 2: D1k Evaluation?
AC P-ACBaseline 0.3125* 0.1622 0.0213CC/T/T 0.3389 0.1683 0.0235CC/T/G 0.3281 0.1683 0.0229CC/G/T 0.3274 0.1665 0.0226CC/G/G 0.3279 0.1672 0.0226MKM/T/T 0.3125* 0.1634 0.0216MKM/T/G 0.3125* 0.1628 0.0215MKM/G/T 0.3125* 0.1630 0.0216MKM/G/G 0.3122* 0.1628 0.0215Table 3: D2k EvaluationIn general, our grouping-based ordering schemeoutperforms the baseline for news articles ofvarious lengths and statistically significantimprovement can be observed on each dataset.This result casts serious doubt on the widelyaccepted practice of taking the text order forsingle-document summary generation, which is amajor finding from our study.The three evaluation metrics give consistentresults although they are based on differentobservations.
The P-AC scores are much lowerthan their AC counterparts because of its strictparagraph constraint.Interestingly, applying the text order posterior tosentence grouping for group-level and sentence-level ordering leads to consistently optimalperformance, as the top scorers on each dataset arealmost all ?__/T/T?.
This suggests that the textualrealization of coherence can be sought in thesource document if possible, after the selectedsentences are rearranged.
It is in this sense that thegeneral intuition about the text order is justified.
Italso suggests that tightly knit paragraphs (groups),where the sentences are closely connected, play acrucial role in creating a coherence flow.
Shufflingthose paragraphs may not affect the finalcoherence1.1 I thank an anonymous reviewer for pointing this out.The grouping method does make a difference.While CC works best for the short and longdatasets (D400 and D2k), MKM is more effectivefor the medium-sized dataset D1k.
Whether thedifference is simply due to length orlinguistic/stylistic subtleties is an interesting topicfor in-depth study.4 Conclusion and Future WorkWe have established a grouping-based orderingscheme to accommodate both local and globalcoherence for summary generation.
Experimentson single-document summaries validate ourapproach and challenge the well accepted textorder by the summarization community.Nonetheless, the results do not necessarilypropagate to multi-document summarization, forwhich the same-document clue for ordering cannotapply directly.
Adapting the proposed scheme tomulti-document summary generation is theongoing work we are engaged in.
In the next step,we will experiment with alternative sentencerepresentations and ordering algorithms to achievebetter performance.We are also considering adapting moresophisticated coherence-oriented models, such as(Soricut and Marcu, 2006; Elsner et al, 2007), toour problem so as to make more interestingcomparisons possible.AcknowledgementsThe reported work was inspired by many talks withmy supervisor, Dr. Wenjie Li, who saw throughthis work down to every writing detail.
The authoris also grateful to many people for assistance.
YouOuyang shared part of his summarization work andhelped with the DUC data.
Dr. Li Shen, Dr. NaishiLiu, and three participants helped with theexperiments.
I thank them all.The work described in this paper was partiallysupported by Hong Kong RGC Projects (No.PolyU 5217/07E).10ReferencesAone, C., Okurowski, M. E., Gorlinsky, J., and Larsen,B.
1999.
A Trainable Summarizer with KnowledgeAcquired from Robust NLP Techniques.
In I. Maniand M. T. Maybury (eds.
), Advances in AutomaticText Summarization.
71?80.
Cambridge,Massachusetts: MIT Press.Barzilay, R., Elhadad, N., and McKeown, K. 2002.Inferring Strategies for Sentence Ordering inMultidocument News Summarization.
Journal ofArtificial Intelligence Research, 17: 35?55.Barzilay, R. and Lapata, M. 2005.
Modeling LocalCoherence: An Entity-based Approach.
InProceedings of the 43rd Annual Meeting of the ACL,141?148.
Ann Arbor.Barzilay, R. and Lapata, M. 2008.
Modeling LocalCoherence: An Entity-Based Approach.Computational Linguistics, 34: 1?34.Barzilay, R. and Lee L. 2004.
Catching the Drift:Probabilistic Content Models, with Applications toGeneration and Summarization.
In HLT-NAACL2004: Proceedings of the Main Conference.
113?120.Bollegala, D, Okazaki, N., and Ishizuka, M. 2006.
ABottom-up Approach to Sentence Ordering for Multi-document Summarization.
In Proceedings of the 21stInternational Conference on ComputationalLinguistics and 44th Annual Meeting of the ACL,385?392.
Sydney.Elsner, M., Austerweil, j.
& Charniak E. 2007.
?AUnified Local and Global Model for DiscourseCoherence?.
In Proceedings of NAACL HLT 2007,436-443.
Rochester, NY.Grosz, B. J., Aravind K. J., and Scott W. 1995.Centering: A framework for Modeling the LocalCoherence of Discourse.
Computational Linguistics,21(2):203?225.Halliday, M. A. K., and Hasan, R. 1976.
Cohesion inEnglish.
London: Longman.Hovy, E. 2005.
Automated Text Summarization.
In R.Mitkov (ed.
), The Oxford Handbook ofComputational Linguistics, pp.
583?598.
Oxford:Oxford University Press.Jing, H. 2000.
Sentence Reduction for Automatic TextSummarization.
In Proceedings of the 6th AppliedNatural Language Processing Conference, Seattle,WA, pp.
310?315.Jing, H., and McKeown, K. 2000.
Cut and Paste BasedText Summarization.
In Proceedings of the 1stNAACL, 178?185.Kehler, A.
2002.
Coherence, Reference, and the Theoryof Grammar.
Stanford, California: CSLI Publications.Lapata, M. 2003.
Probabilistic Text Structuring:Experiments with Sentence Ordering.
In Proceedingsof the Annual Meeting of ACL, 545?552.
Sapporo,Japan.Lapata, M. 2006.
Automatic evaluation of informationordering: Kendall?s tau.
Computational Linguistics,32(4):1?14.Madnani, N., Passonneau, R., Ayan, N. F., Conroy, J.M., Dorr, B. J., Klavans, J. L., O?leary, D. P., andSchlesinger, J. D. 2007.
Measuring Variability inSentence Ordering for News Summarization.
InProceedings of the Eleventh European Workshop onNatural Language Generation, 81?88.
Germany.Mann, W. C. and Thompson, S. 1988.
RhetoricalStructure Theory: Toward a Functional Theory ofText Organization.
Text, 8:243?281.Rich C., and Harper, C. 2007.
Writing and ReportingNews: A Coaching Method, Fifth Edition.
ThomasonLearning, Inc. Belmont, CA.Soricut, R. and Marcu D. 2006.
Discourse GenerationUsing Utility-Trained Coherence Models.
InProceedings of the COLING/ACL 2006 MainConference Poster Sessions, 803?810.Steibach, M., Karypis, G., and Kumar V. 2000.
AComparison of Document Clustering Techniques.Technical Report 00-034.
Department of ComputerScience and Engineering, University of Minnesota.Tapiero, I.
2007.
Situation Models and Levels ofCoherence: Towards a Definition of Comprehension.Mahwah, New Jersey: Lawrence Erlbaum Associates.Wilpon, J. G. and Rabiner, L. R. 1985.
A Modified K-means Clustering Algorithm for Use in IsolatedWord Recognition.
In IEEE Trans.
Acoustics, Speech,Signal Proc.
ASSP-33(3), 587?594.Zhang R., Li, W., and Lu, Q.
2010.
Sentence Orderingwith Event-Enriched Semantics and Two-LayeredClustering for Multi-Document News Summarization.In COLING 2010: Poster Volume, 1489?1497,Beijing.11
