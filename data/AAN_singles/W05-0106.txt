Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 32?36,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsMaking Hidden Markov Models More TransparentNashira Richard Lincoln?
and Marc Light??
?Linguistics Department?School of Library and Information Science?Computer Science DepartmentUniversity of IowaIowa, USA 52242{nashira-lincoln, marc-light}@uiowa.eduAbstractUnderstanding the decoding algorithm forhidden Markov models is a difficult taskfor many students.
A comprehensive un-derstanding is difficult to gain from staticstate transition diagrams and tables of ob-servation production probabilities.
Wehave built a number of visualizations de-picting a hidden Markov model for part-of-speech tagging and the operation of theViterbi algorithm.
The visualizations aredesigned to help students grasp the oper-ation of the HMM.
In addition, we havefound that the displays are useful as de-bugging tools for experienced researchers.1 IntroductionHidden Markov Models (HMMs) are an importantpart of the natural language processing toolkit andare often one of the first stochastic generation mod-els that students1 encounter.
The correspondingViterbi algorithm is also often the first exampleof dynamic programming that students encounter.Thus, HMMs provide an opportunity to start stu-dents on the correct path of understanding stochas-tic models, not simply treating them as black boxes.Unfortunately, static state transition diagrams, ta-bles of probability values, and lattice diagrams arenot enough for many students.
They have a generalidea of how a HMM works but often have common1The Introduction to Computational Linguistics course atthe University of Iowa has no prerequisites, and over half thestudents are not CS majors.misconceptions.
For example, we have found thatstudents often believe that as the Viterbi algorithmcalculates joint state sequence observation sequenceprobabilities, the best state sequence so far is alwaysa prefix of global best path.
This is of course false.Working a long example to show this is very tediousand thus text books seldom provide such examples.Even for practitioners, HMMs are often opaquein that the cause of a mis-tagging error is often leftuncharacterized.
A display would be helpful to pin-point why an HMM chose an incorrect state se-quence instead of the correct one.Below we describe two displays that attempt toremedy the above mentioned problems and we dis-cuss a Java implementation of these displays in thecontext of a part-of-speech tagging HMM (Kupiec,1992).
The system is freely available and has anXML model specification that allows models calcu-lated by other methods to be viewed.
(A standardmaximum likelihood estimation was implementedand can be used to create models from tagged data.A model is also provided.
)2 DisplaysFigure 1 shows a snapshot of our first display.
Itcontains three kinds of information: most likelypath for input, transition probabilities, and history ofmost likely prefixes for each observation index in theViterbi lattice.
The user can input text at the bottomof the display, e.g., Pelham pointed out that Geor-gia voters rejected the bill.
The system then runsViterbi and animates the search through all possiblestate sequences and displays the best state sequenceprefix as it works its way through the observation32Figure 1: The system?s main display.
Top pane: shows the state space and animates the derivation of themost likely path for ?Pelman pointed out that Georgia voters ...?
; Middle pane: a mouse-over-triggered bargraph of out transition probabilities for a state; Bottom pane: a history of most likely prefixes for eachobservation index in the Viterbi lattice.
Below the panes is the input text field.33Figure 2: Contrast display: The user enters a sequence on the top text field and presses enter, the sequenceis tagged and displayed in both the top and bottom text fields.
Finally, the user changes any incorrect tags inthe top text field and presses enter and the probability ratio bars are then displayed.34sequence from left to right (these are lines connect-ing the states in Figure 1).
At any point, the stu-dent can mouse-over a state to see probabilities fortransitions out of that state (this is the bar graph inFigure 1).
Finally, the history of most likely pre-fixes is displayed (this history appears below the bargraph in Figure 1).
We mentioned that students oftenfalsely believe that the most likely prefix is extendedmonotonically.
By seeing the path through the statesreconfigure itself in the middle of the observation se-quence and by looking at the prefix history, a studenthas a good chance of dispelling the false belief ofmonotonicity.The second display allows the user to contrast twostate sequences for the same observation sequence.See Figure 2.
For each contrasting state pairs, itshows the ratio of the corresponding transition toeach state and it shows the ratio of the generation ofthe observation conditioned on each state.
For exam-ple, in Figure 2 the transition DT?JJ is less likelythan DT?NNP.
The real culprit is generation proba-bility P(Equal|JJ) which is almost 7 times larger thanP(Equal|NNP).
Later in the sequence we see a simi-lar problem with generating opportunity from a NNPstate.
These generation probabilities seem to drownout any gains made by the likelihood of NNP runs.To use this display, the user types in a sentencein the box above the graph and presses enter.
TheHMM is used to tag the input.
The user then modi-fies (e.g., corrects) the tag sequence and presses en-ter and the ratio bars then appear.Let us consider another example: in Figure 2, themis-tagging of raises as a verb instead of a noun atthe end of the sentence.
The display shows us thatalthough NN?NNS is more likely than NN?VBZ,the generation probability for raises as a verb isover twice as high as a noun.
(If this pattern ofmis-taggings caused by high generation probabil-ity ratios was found repeatedly, we might considersmoothing these distributions more aggressively.
)3 ImplementationThe HMM part-of-speech tagging model andcorresponding Viterbi algorithm were implementedbased on their description in the updated version,http://www.cs.colorado.edu/?martin/SLP/updated.html , of chapter 8 of (Jurafskyand Martin, 2000).
A model was trained usingMaximum Likelihood from the UPenn Treebank(Marcus et al, 1993).
The input model file isencoded using XML and thus models built by othersystems can be read in and displayed.The system is implemented in Java and requires1.4 or higher to run.
It has been tested on Linux andApple operating systems.
We will release it under astandard open source license.4 Summary and future workStudents (and researchers) need to understandHMMs.
We have built a display that allow usersto probe different aspects of an HMM and watchViterbi in action.
In addition, our system providesa display that allows users to contrast state sequenceprobabilities.
To drive these displays, we have builta standard HMM system including parameter esti-mating and decoding and provide a part-of-speechmodel trained on UPenn Treebank data.
The systemcan also read in models constructed by other sys-tems.This system was built during this year?s offeringof Introduction to Computational Linguistics at theUniversity of Iowa.
In the Spring of 2006 it will bedeployed in the classroom for the first time.
We planon giving a demonstration of the system during alecture on HMMs and part-of-speech tagging.
A re-lated problem set using the system will be assigned.The students will be given several mis-tagged sen-tences and asked to analyze the errors and reporton precisely why they occurred.
A survey will beadministered at the end and improvements will bemade to the system based on the feedback provided.In the future we plan to implement Good-Turingsmoothing and a method for dealing with unknownwords.
We also plan to provide an additional displaythat shows the traditional Viterbi lattice figure, i.e.,observations listed left-to-right, possible states listedfrom top-to-bottom, and lines from left-to-right con-necting states at observation index i with the previ-ous states, i-1, that are part of the most likely statesequence to i.
Finally, we would like to incorpo-rate an additional display that will provide a visual-ization of EM HMM training.
We will use (Eisner,2002) as a starting point.35ReferencesJason Eisner.
2002.
An interactive spreadsheet for teach-ing the forward-backward algorithm.
In Proc.
of theACL 2002 Workshop on effective tools and method-ologies for teaching natural language processing andcomputational linguistics.Daniel Jurafsky and James H. Martin.
2000.
Speech andLanguage Processing: an introduction to natural lan-guage processing, and computational linguistics, andspeech recognition.
Prentice-Hall.J.
Kupiec.
1992.
Robust part-of-speech tagging usinga hidden markov model.
Computer Speech and Lan-guage, 6:225?242.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330, June.36
