Component-based multimodal dialog interfaces formobile knowledge creationGeorg NiklfeldTelecommunications ResearchCenter Vienna (ftw.)Maderstr.
1/91040 Vienna, Austrianiklfeld@ftw.atRobert Finanmobilkom austria AGObere Donaustrasse 291020 Vienna, Austriar.finan@mobilkom.atMichael PucherTelecommunications ResearchCenter Vienna (ftw.)Maderstr.
1/91040 Vienna, Austriapucher@ftw.atAbstractThis paper addresses two related top-ics: Firstly, it presents building-blocksfor flexible multimodal dialog inter-faces based on standardized compo-nents (VoiceXML, XML) to indicatethat thanks to well-supported standard-izations, mobile multimodal interfacesto heterogeneous data sources are be-coming ready for mass-market deploy-ment, provided that adequate modular-ization is respected.
Secondly, this isput in the perspective of a discussion ofknowledge management in firms, andthe paper argues that multimodal dia-log systems and the naturalized mobileaccess to company data they offer willtrigger a new knowledge managementpractice of importance for knowledge-intensive companies.1 IntroductionKnowledge management is concerned with pro-moting the creation and dissemination of knowl-edge in organizations.
A variety of technologies,in particular information technologies can supportthis process.
One way in which information tech-nology has contributed to knowledge manage-ment is through computer networks, which haveprovided individuals with easy access to informa-tion not stored in their own office.
The knowledgeworker was now able to remain seated at her deskand share data and information with other indi-viduals seated at desks thousands of kilometersaway.
Mobile data communication technologybrings another improvement, as access to the en-try points to the network is no longer confined tothe office: networking anytime, anywhere, as theslogan goes.
The next step in this process of re-moving access restrictions to information shouldtarget the user interface of the entry points, viz.the information interfaces.
For historical reasonstheir design is still inspired partly by the com-munication needs of machines, and certainly bythe office-bound scenario featuring large com-puter screens and keyboards.
Human beings inmovement however use speech for the exchangeof information along with visual representationssuch as text and graphics.
To bring such mul-timodal communication to mobile computer de-vices is an important issue for naturalizing in-formation access.
However, this paper arguesthat much beyond such principled considerations,multimodal interfaces are imperative for infor-mation access from mobile devices.
The paperpresents building-blocks and constraints for thedevelopment of such multimodal interfaces andsystems, and describes the emerging knowledgemanagement scenario that companies should un-derstand well in order to be prepared for this newstrategic technology.Section 2 develops the case for multimodal in-terfaces to mobile data services, e.g.
in the ap-proaching 3G telecommunication infrastructures.Section 3 presents our view on what are thebuilding-blocks for a technological infrastructurethat will foster ubiquitous multimodal interfacesto large numbers of data services in the businessenvironment.
Section 4 is more technically spe-cific and describes our proposal for a general soft-ware architecture that provides for easy imple-mentation of multimodal dialog interfaces.
Sec-tion 5 discusses the question of how the multi-modal interfaces can be mapped to heterogeneousdata sources.
With the technical picture in place,section 6 reviews the management theory conceptof knowledge management practices, to preparethe ground for section 7, which considers whethermultimodal interface techniques will trigger newknowledge management practices in companies.2 The importance of multimodalityHuman face-to-face communication is multi-modal, combining the acoustic channel with vi-sual and occasionally tactile channels.
Human be-ings are therefore well equipped to communicatemultimodally, and multimodal communication isperceived as natural.
In human computer inter-action, the use of spoken or written natural lan-guage poses complexities that have led to a dom-inance of visual interfaces consisting of text andusually 2D graphics.
Visual interfaces are veryefficient under some circumstances: written textallows rapid information uptake; visual interfaceson large screens allow to present large amountsof information simultaneously, with users able tofocus on bits that interest them without having toprocess all the rest in-depth; finally, visual inter-faces are preferred consistently for input and out-put of spatial information (Oviatt et al, 1997).
Invisual interfaces, text output can be used for in-formation for which graphical metaphors are notavailable, and text input can be used for unre-stricted content or inherently linguistic informa-tion such as names.Yet, considering data services on 3G mobiledevices, the following factors constitute obstaclesfor relying solely on visual interfaces, and implya real usefulness of added speech interface capa-bilities: The terminal devices where 3G data servicesrun will have small displays in terms of sizeand resolution, although a significant im-provement over current WAP phones is ex-pected.
Still, it will not be possible to mir-ror the user experience of today?s commer-cial web-sites in terms of the amount of in-formation that can be presented simultane-ously.
(Note also that the speed disadvantageof TTS is less problematic for small amountsof information.
) Although 3G terminals will be produced ina variety of form-factors, many devices willbe too small to provide comfortable alphanu-meric keyboards, relying instead on pointingdevices such as a pen/touch-screen combi-nation.
Without a keyboard, text input be-comes cumbersome, even when handwritingrecognition is provided.
Speech input is alogical alternative.
The input dilemma of3G devices is made yet more severe whereeven a pointing device is lacking, as in cur-rent mobile phones.
This WAP-like scenariomakes data services without speech supportso unattractive that we consider it unlikelythat large numbers of 3G devices intendedfor data access will be in this category. In mobile usage situations, visual access tothe display may be impossible in certain sit-uations, such as when driving a car.
Also,even where pointing-device and keyboardare provided, access to them is not possiblewhen a user has her hands busy for other ac-tivities.
A speech interface may still be us-able in such situations.We contend that the combination of these consid-erations takes the case for multimodal interfacesfor 3G services beyond a nice-to-have status tothat of a significant building-block that needs tobe put in place for successful deployment scenar-ios of 3G infrastructures to emerge.
This is alsothe motivation for application oriented researchlike the one described here, which attempts toidentify development models for multimodal in-terfaces that are feasible both technically and eco-nomically.3 Building-blocks for multimodal dialoginterfacesIt is our belief that a successful scenario wheremultimodal dialog interfaces become common-place in the business and consumer environmentsrequires that the development of a multimodal in-terface for an existing data service must be ac-complishable by software developers without aspecialized background in speech or natural lan-guage processing.
Developers that have such aspecialized background will likely be too hard tocome by for small run-of-the-mill projects.In 1999, the EU-sponsored DISC project un-dertook a comparison of seven toolkits for dia-log management in spoken dialog systems (DISC,1999).
The dialog management component is thecentral part of a dialog system and of particularimportance in a discussion about streamlined de-velopment processes for multimodal interfaces,because it is the bridge between the intelligentinterface technologies and the underlying appli-cation logic of application servers and databases.The DISC-survey finds platforms that offer rapidprototyping support, partly via integrated devel-opment environments.
As most of the toolkitsare shipped together with speech recognition andspeech synthesis engines, this frees applicationdevelopers to focus, firstly, on the dialog design,and secondly, on interfacing to the applicationcore.In May 2000, the VoiceXML Forum, a multi-party industry organization, submitted version 1.0of the VoiceXML standard to the W3C (W3C,2000c).
VoiceXML is a specification language forspoken dialogs including functional requirementson the platforms that implement the standard.
Thework done on the VoiceXML standard representsa major step forward from the state of affairs re-ported in the DISC-study for two reasons.
Firstly,being an internet standard, VoiceXML goes be-yond manufacturer-specific toolkits that are notcompatible with each other by providing a cred-ible set of target interfaces for all players in theindustry.
Second, it chooses XML and the as-sociated web-programming technology both as aformat for specification of the voice dialogs, andfor interfacing to the application logic.
The twomost important aspects for a standardized com-ponent framework for dialog systems have thusbeen brought in line with web technology, whichis what was needed to create that promise of aplatform for speech-enabled applications that iseasily accessible to developers with a general, andnot speech processing specific, background.Today one can add that VoiceXML has receivedimportant support from major players in the in-dustry who have made development platforms forVoiceXML-based applications available to devel-opers free of charge.
The only potential downsideof recent developments is that the platform manu-facturers have also included some proprietary el-ements in their implementations, making directtransfer of VoiceXML applications from one plat-form to another again impossible.While an early requirements document forVoiceXML explicitly treats multimodality in thecontext of the discussion on the inclusion ofDTMF input in the standard (W3C, 1999), thestandard is not written to cover general mul-timodal applications.
Subsequently, W3C hasdrafted a requirements document for a multi-modal dialog language (W3C, 2000b) and cur-rently a new working group on that topic is be-ing assembled.
At present however, the standardprovides no intentional support for general mul-timodal systems.
The most important drawbackthat we found in our attempts to nevertheless builda multimodal architecture around VoiceXML, isthat it is not possible to make an active voice di-alog running in a VoiceXML browser aware ofevents that occur outside the voice browser, e.g.
ata visual interface: VoiceXML neither allows forlinking in Java applets that could receive pushednotifications, nor does it provide any other inter-face for external events.
Our architecture for mul-timodal dialog systems based on VoiceXML isconsiderably influenced by this fact.With VoiceXML, a standard for voice compo-nents is available which seems ready for the mass-market.
What is needed in addition is a simplereference architecture that shows how functional-ity supported by VoiceXML can be integrated intoa system architecture for multimodal dialog inter-faces.4 ArchitectureIn a project at our institution that follows thelonger term goal to develop architectures for mul-timodal dialog systems for 3G telecommunica-tions infrastructures, for the benefit of our sup-porting partner companies from the telecommu-nications industry, we are currently developing anarchitecture that shall: use mainstream technolo-gies and standards as far as possible, to test theircapabilities and limitations; be general enough toscale from our first small prototypes to larger sys-tems that are close to market-readiness; providethe basis for usability research on multimodal in-terfaces.The architecture shall support a multimodal in-terface that shall combine a visual interface viaHTML and Java applets in a visual web browserwith a voice interface built using VoiceXML.Communication between the visual browser andthe voice browser is mediated via a central appli-cation server that is built using Java servlet tech-nology in combination with a web server.In (W3C, 2000b), three types of multimodal in-put are distinguished:1. sequential multimodal input is the simplesttype, where at each step of the interaction,either one or the other input modality is ac-tive, but never more than one simultane-ously;2. uncoordinated, simultaneous multimodal in-put allows concurrent activation of morethan one modality.
However, should the userprovide input on more than one modality,these informations are not integrated but willbe processed in isolation, in random order;3. coordinated, simultaneous multimodal inputexploits multimodality to the full, providingfor the integration of complementary inputsignals from different modalities into jointevents, based on timestamping.Because we cannot send events about the visualinterface to the dialogs in the voice browser (cf.section 3), we maintain that only the sequentialmultimodal input pattern can be properly realizedwith the current version of the VoiceXML stan-dard: The other patterns require that even whilethe voice interface is active, e.g.
listening for userspeech, it must be possible for the multimodal di-alog to change state based on inputs received fromthe visual interface.
In the sequential mode on theother hand, it is possible to deactivate the visualinterface whenever voice input is activated.In this case then, the choice of multimodalinteraction pattern is determined by features ofthe components used.
In many realistic applica-tion development efforts, the interaction patternwill be determined by user-level requirements thathave to be met.
Anyhow, the choice of multi-modal interaction pattern will certainly be a di-mension in which variation occurs.
For the pur-poses of the demonstrator development in ourproject, it was important to find a software archi-tecture that can remain stable across different pat-terns and across interface types.This can be accomplished by a modular orobject-oriented design which separates the centralapplication server into the following functions:visual communicator: a modular handler for thevisual interface;voice communicator: a modular handler for thevoice interface;transaction module: encapsulates the transac-tion needed for the application logic;multimodal integrator: handles all messageflows between the interface modules, andbetween the interface modules and thetransaction module.The resulting system architecture is shown inFig.
1.visual browser(html, applet)voice browser(VoiceXML)application server(servlet)visualcommunicatorvoicecommunicatormultimodalintegratortransactionmoduledatabasevisual IF voiceIFHTTPIPC HTTPFigure 1: Multimodal ArchitectureBoth the visual and the voice user interface arerealized by standardized components in line withgeneral web technologies (visual web browserand VoiceXML browser).
Within the applicationserver, the architecture stipulates a specialized in-terface handler for each modality.For example in our prototype, the voice com-municator prepares VoiceXML documents whichit puts on a web server that is associated to the ap-plication server.
Once the VoiceXML interpreterof the voice browser has been started (after a user-triggered event, e.g.
an incoming phone call),each of the VoiceXML documents processed bythe interpreter is programmed so as to terminatewith a load of a successor document from the webserver.
Our voice communicator simply preparesthese successor documents based on messages itreceives from the multimodal integrator.
Whenthe voice interface is not active, the prepared doc-uments contain just an idle loop that terminatesafter some time, e.g.
0.5 seconds.
When the mul-timodal integrator decides (based on user input)that a chunk of the interaction shall be performedvia voice, it sends a message indicating field la-bels and data-types (e.g.
the range of an enumer-ation) to the voice communicator, which insteadof an idle VoiceXML document now produces aVoiceXML dialog document that executes the re-spective voice dialog with the user and returns re-sults to the multimodal integrator.The visual communicator is designed similarlyto prepare HTML pages and applets for the visualbrowser.
In our prototype, the visual interface in-cludes controls that allow the user to explicitlyactivate the voice interface for a group of inputfields.
When this is done, the visual communica-tor deactivates all control elements on the visualinterface, and sends a message with the user re-quest for voice dialog to the multimodal integra-tor.The multimodal integrator is the only part inthe proposed architecture where information frommore than one modality is processed.
The waythis processing is done then defines the multi-modal interaction pattern.
To change the pattern,the architecture envisages that one implementa-tion of the multimodal integrator would simply bereplaced by another, without any changes to otherparts of the systems.
This of course presupposesthat the interfaces between the multimodal inte-grator and the interface handlers have been beendefined so generally that all occurring multimodalinteraction patterns are covered.
A description ofthis interface is for further study.In our view, these preliminary results show thatVoiceXML can be used to build simple, but nev-ertheless useful multimodal interfaces for typicaldata services for mobile access.
Once first im-plementations of the voice communicator and themultimodal integrator are available, it should be-come quite easy for the general web programmerto generate further multimodal interfaces for ex-isting data services.A simple working example for a multimodalinterface (not in the domain of knowledge man-agement) that we have already implemented isan interface to existing route-finder applicationsin the web for the city of Vienna.
It would behard to enter street names without a keyboard, sowe provide the choice to use speech for any ofthe input fields, but results of route-queries areinherently graphical, and therefore are displayedvisually.
We have developed the multimodal in-terface without access to the route-finder applica-tions themselves, but just by replicating the CGIrequests sent to the applications from their pro-vided visual browser interfaces, which demon-strates the potential of a modular architecture foreasy multimodal interface development.One drawback of the modularity of theVoiceXML standard itself is the resulting lackof support for some types of interface adaptiv-ity.
Information about environment characteris-tics that is easily obtainable in speech recogni-tion, such as the level of ambiance noise and con-fidence scores in speech recognition, are kept lo-cal in the speech recognition component used bythe VoiceXML browser (as part of the implemen-tation platform, which is not further consideredby the standard).
They are not accessible in theVoiceXML browser, and therefore neither in thevoice communicator, nor in the multimodal in-tegrator, where they could be used to influencemodality selection.
This shortcoming should beaddressed in future versions of VoiceXML.5 Mapping data to speech using XMLTo further illustrate the web-oriented technologyframework for multimodal dialog interfaces thatwe advocate, this section briefly reviews the useof the XML family of technologies for the map-ping of data to representations at a speech inter-face, although we do not have much new to sayon this.XML (W3C, 2000a) is a markup language thatcan be used to represent data and data schemata intext files.
Processors for XML-files are availablein common web-browsers and servers.
There ex-ist also associated transformation languages (e.g.XSLT) and style sheet languages (e.g.
XSL) thatmake it possible to associate presentation formatsto data that follows a defined schema.
Note thatVoiceXML itself is just one XML-schema, withdefined semantics.XML technologies are widely used to representdata that comes from databases or applications ina format that is universally interpretable in webinfrastructures.
A possible implementation of thevoice communicator of our architecture would bean XML-based translator that takes general XML-representations and produces VoiceXML dialogsthat play or query these data.
Such an approachwould reduce the development of a multimodaldialog interface to selection of a multimodal inter-action pattern and provision of the respective mul-timodal integrator.
It has to be said that automaticgeneration of good voice interfaces from arbitrarydata is certainly not easy.
However, if the com-plexity of each of the voice dialogs is kept low(in multimodal dialogs this is easier than in voice-only dialogs), the goal appears within reach.6 Knowledge Management PracticesWithin management theory literature, discussionsof knowledge management have often focused onthe discussion of different types of knowledge,such as ?tacit?
vs. ?explicit?
knowledge, and thepossibilities to convert between the types in orderto maximize usefulness to organizations (Polanyi,1958; Nonaka, 1994).
When the knowledge man-agement of individual organizations is studied, itis however difficult to identify and classify theknowledge held by an organization.This among others is the motivation for theproposal to study knowledge management prac-tices (KMPs) instead (Coombs and Hull, 1998b;Coombs and Hull, 1998a).
KMPs in a firm areregular activities by which knowledge is pro-cessed in some way, and which play an impor-tant role in shaping the knowledge base of the firmand making it available in the innovation process.They are distinctive ways in which a firm dealswith knowledge, and can be established by ques-tionnaire or interview studies with employees.Coombs and Hull (1998a) distinguish fivegroups of KMPs: KMPs located in the formalR&D management process; KMPs for managingintellectual property positions; KMPs for ?map-ping?
knowledge relationships; KMPs for serialtransfer of project experience; and KMPs contin-gent on information technology applications.
Inthe latter group the authors distinguish KMPs thatare supported by IT, but have a strong indepen-dent existence (e.g.
electronic patent watch bul-letins) from others that are triggered by innova-tions in IT, as is the case with across-site clusterbuilding among research staff that is triggered bythe ease of communication using email or intranetsolutions.The question that arises is whether mobile ac-cess to information is a new, IT-triggered KMP.This requires that such activities, presumably per-formed by employees that perform a significantpart of their work off-site, plays a significantrather than just anecdotal role in shaping theknowledge base of firms and their innovative po-tential.
Here are some thoughts on this topic: The mobile information access of today ismainly notebook-based.
Employees accesstheir company email while traveling andread company news on the intranet via se-cure internet connections.
This is essen-tially a quantitative improvement of intra-company networking. Field engineers can feed data observed ata customer installation to corporate appli-cation servers and use results immediatelyto make changes to the customer installa-tion.
This is a quantitative improvement ofresponse times. Professional mobile speech communicationis a ubiquitous phenomenon, which makesgroups whose individual members are highlymobile more cohesive.
A minor qualificationhere could be that topics of mobile speechcalls are usually administrative and do nottouch the core of the firms?
innovative pro-cesses.It seems that mobile access intensifies data andinformation use and the associated creation andsharing of knowledge, but that qualitatively newpractices are restricted to the IT departments thathave to plan and manage the supporting applica-tions.
These activities may be significant enoughto constitute new KMPs.
The next section con-siders the same issue for mobile multimodal in-terfaces.7 Outlook on mobile multimodalinterfacesWe assume that by 2004, business users will havesmart phones for 3G telecommunication networksthat will include speech input/output capabilitieslike on a telephone, and also colour displays ofapprox.
6x6cm size and 320x320 dots resolution,with touch-screen functionality.
Most users willnot have a comfortable keyboard for their devicewhen mobile, although some handwriting recog-nition technology will be available.We imagine that the following use cases are re-alistic:1 At a working lunch, a researcher discusseswork with a colleague.
He is reminded ofa relevant paper, but cannot recall the au-thors.
Handling knife, fork and 3G device inturns, he queries the library database of hisinstitution, entering keywords for the searchby voice and browsing through the resultslist using sometimes voice, sometimes thetouch-screen. Driving back to the airport on the highwayafter a meeting, a sales manager starts work-ing on a report, alternately dictating notesand voice-navigating through web-pages,and checking back on data on internal-accesspages of his company.
He checks the screenof his smart-phone occasionally for a quickglance to check progress. During the train ride home from work, aproject manager has been working on a re-source plan on her notebook, which has amobile internet connection.
When she has toget off and walk over to the bus stop, the planis nearly finished, so she decides to continue1Although further progress in ASR in noisy environ-ments is a precondition.
This is an important challenge formobile voice interfaces in general.work using the multimodal interface to thecorporate project planning application on hersmart phone.
She uploads the project plan tothe multimodal interface server of her com-pany and packs up the notebook.
She thenconnects to the multimodal interface serverfrom her smart phone and enters the remain-ing commands via voice while walking, andvia the visual interface once she has enteredthe bus and found a seat (being reluctant touse voice control in crowded places).What these imaginary examples demonstrateare firstly, new situations in which work whichrequires access to data and information resourcesis possible (intensifying knowledge creation), andsecondly, a reliance on multimodal interfacesfor tasks that probably would be performed viapurely visual interfaces in more office-like sit-uations.
This suggests a need for the generalcomponent-based multimodal interface develop-ment model presented earlier, to enable the IT de-partment of e.g.
the firm in the last example totailor multimodal interfaces for each of the appli-cations offered on the company network.Accordingly, we see a new KMP for multi-modal interfaces within the IT management func-tion of firms.
This new KMP is concerned withsupporting knowledge creation through the plan-ning and provision of multimodal interfaces tocompany data and information resources.
It re-quires decision-making on the following ques-tions:1.
What types of mobile access to company re-sources should be supported via telephonespeech, and what types via data services?2.
Of the data services, what services should besupported at notebooks, and what servicesat visual-only interfaces, speech-only inter-faces, and multimodal dialog interfaces atsmart phones, respectively?3.
Which multimodal interfaces can be devel-oped in-house, preferably using component-based methods, and what interfaces need tobe procured externally?Companies for which knowledge creationby mobile employees is strategically importantshould consider these questions carefully.8 ConclusionsUsing VoiceXML and XML technologies, thedevelopment of simple multimodal interfaces tocompany applications is today possible withoutinvolvement of significant speech and natural lan-guage processing expertise, by following archi-tecture models like the one proposed in this pa-per.
It is important for knowledge-intensive com-panies to define a stance on how to deal with thistechnology, because given the parameters of mo-bile data access, multimodal interfaces and the re-sulting improvements to mobile data and informa-tion access, which foster knowledge creation, willlikely become competitively relevant.Future work of our group will elaborate theproposed reference architecture, by providingprototype implementations of the visual commu-nicator, voice communicator, and multimodal in-tegrator for different multimodal interaction pat-terns, and by describing general interfaces be-tween them.The standardization work for a multimodal di-alog language to supersede VoiceXML will bringfurther improvements to the development modelsfor multimodal interfaces.AcknowledgementsThis work was supported within the Austriancompetence center program Kplus, and by thecompanies Alcatel, Connect Austria, Kapsch,Mobilkom Austria, and Nokia.ReferencesCoombs, R. and R. Hull.
1998.
?Knowledge Manage-ment Practices?
and path-dependency in innovation.Research Policy, 27(3):237?253.Coombs, R. and R. Hull.
1998.
Knowledge manage-ment practices for innovation: an audit tool for im-provement.
Technical report, CRIC, Univ.
Manch-ester.DISC.
1999.
Deliverable d2.7a: State-of-the-art sur-vey of dialogue management tools.
Technical re-port, Esprit Long-Term Research Concerted ActionNo.
24823.Nonaka, I.
1994.
A dynamic theory of organiza-tional knowledge creation.
Organization Science,5(1):14?17.Oviatt, S.L., A. DeAngeli, and K. Kuhn.
1997.
Inte-gration and synchronization of input modes duringmultimodal human-computer interaction.
In Proc.of CHI97, pages 415?422, New York.
ACM Press.Polanyi, M. 1958.
Personal Knowledge.
Univ.Chicago Press, Chicago.W3C.
1999.
Dialog requirements for voicemarkup languages.
Working draft 23 dec 1999.http://www.w3.org/TR/voice-dialog-reqs/.W3C.
2000a.
Extensible Markup Language (XML)1.0 (Second Edition).
Recommendation 6 oct 2000.http://www.w3.org/TR/2000/REC-xml-20001006.W3C.
2000b.
Multimodal requirements for voicemarkup languages.
Working draft 10 jul 2000.http://www.w3.org/TR/multimodal-reqs.W3C.
2000c.
Voice eXtensible MarkupLanguage (VoiceXML) version 1.0.http://www.w3.org/TR/2000/NOTE-voicexml-20000505/.
