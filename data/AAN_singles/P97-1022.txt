Fertility Models for Statistical Natural Language UnderstandingStephen De l la  P ie t ra  ?, Mark  Epste in ,  Sa l im Roukos ,  Todd WardIBM Thomas  J. Watson  Research  CenterP.O.
Box 218Yorktown Heights,  NY 10598, USA(*Now Wi th  Renaissance Technologies,  S tonybrook ,  NY, USA)sdella@rentec, tom\[meps/roukos/tward\] ?watson.
ibm.
comAbst ractSeveral recent efforts in statistical nat-ural language understanding (NLU) havefocused on generating clumps of Englishwords from semantic meaning concepts(Miller et al, 1995; Levin and Pierac-cini, 1995; Epstein et al, 1996; Epstein,1996).
This paper extends the IBM Ma-chine Translation Group's concept of fertil-ity (Brown et al, 1993) to the generationof clumps for natural anguage understand-ing.
The basic underlying intuition is thata single concept may be expressed in Eng-lish as many disjoint clump of words.
Wepresent wo fertility models which attemptto capture this phenomenon.
The first isa Poisson model which leads to appeal-ing computational simplicity.
The secondis a general nonparametric fertility model.The general model's parameters are boot-strapped from the Poisson model and up-dated by the EM algorithm.
These fertilitymodels can be used to impose clump fertil-ity structure on top of preexisting clumpgeneration models.
Here, we present re-sults for adding fertility structure to uni-gram, bigram, and headword clump gener-ation models on ARPA's Air Travel Infor-mation Service (ATIS\] domain.1 In t roduct ionThe goal of a natural anguage understanding (NLU)system is to interpret a user's request and respondwith an appropriate action.
We view this interpre-tation as translation from a natural language ex-pression, E, into an equivalent expression, F, inan unambigous formal language.
Typically, this for-mal language will be hand-crafted to enhance per-formance on some task-specific domain.
A statisti-cal NLU system translates a request E as the mostlikely formal expression ~' according to a probabilitymodel p,= are maxp(F\[E) --- are maxp(F, E).over  a l l  F over  a l l  FWe have previously built a fully automatic statis-tical NLU system (Epstein et al, 1996) based on thesource-channel factorization of the joint distributionp( f  , E)p( f  , E) = p(f)p(ZlF ).This factorization, which has proven effective inspeech recognition (Bahl, Jelinek, and Mercer,1983), partitions the joint probability into an a pri-ori intention model p(F), and a translation modelp(E\[F) which models how a user might phrase a re-quest F in English.For the ATIS task, our formal language is a mi-nor variant of the NL-Parse (Hemphill, Godfrey, andDoddington, 1990) used by ARPA to annotate theATIS corpus.
An example of a formal and naturallanguage pair is:?
F :  List flights from New Orleans to Memphisflying on Monday departing early_morning?
E: do you have any flights going to Memphisleaving New Orleans early Monday morningHere, the evidence for the formal language concept'early_morning' resides in the two disjoint clumps ofEnglish 'early' and 'morning'.
In this paper, we in-troduce the notion of concept fertility into our trans-lation models p(EIF ) to capture this effect and themore general linguistic phenomenon of embeddedclauses.
Basically, this entails augmenting the trans-lation model with terms of the form p(nlf), where nis the number of clumps generated by the formal an-guage word f. The resulting model can be trainedautomatically from a bilingual corpus of English andformal anguage sentence pairs.Other attempts at statistical NLU systems haveused various meaning representations such as con-cepts in the AT&T system (Levin and Pieraccini,1995) or initial semantic structure in the BBN sys-tem (Miller et al, 1995).
Both of these systems re-quire significant rule-based transformations to pro-duce disambiguated interpretations which are then168used to generate the SQL query for ATIS.
More re-cently, BBN has replaced handwritten rules with de-cision trees (Miller et al, 1996).
Moreover, both sys-tems were trained using English annotated by handwith segmentation and labeling, and both systemsproduce a semantic representation which is forcedto preserve the time order expressed in the Eng-lish.
Interestingly, both the AT&T and BBN sys-tems generate words within a clump according tobigram models.
Other statistical approachs to NLUinclude decision trees (Kuhn and Mori, 1995) andneural nets (Gorin et al, 1991).In earlier IBM translation systems (Brown et al,1993) each English word would be generated by,or "aligned to", exactly one formal language word.This mapping between the English and formal lan-guage expressions i called the "alignment".
In thesimplest case, the translation model is simply pro-portional to the product of word-pair translationprobabilities, one per element in the alignment.
Inthese models, the alignment provides all of the struc-ture in the translation model.
The alignment is a"hidden" quantity which is not annotated in thetraining data and must be inferred indirectly.
TheEM algorithm (Dempster, Laird, and Rubin, 1977)used to train such "hidden" models requires us tosum an expression over all possible alignments.These early models were developed for French toEnglish translation.
However, in NLU there is a fun-damental asymmetry between the natural anguageand the unambiguous formal language.
Most no-tably, one formal anguage word may frequently cor-respond to whole English phrases.
We added the"clump", an extra layer of structure, to accomodatethis phenomenon (Epstein et al, 1996).
In this para-digm, formal anguage words first generate a clump-ing, or partition, of the word slots of the Englishexpression.
Then, each clump is filled in accordingto a translation model as before.
The alignment isdefined between the formal anguage words and theclumps.
Then, both the alignment and the clumpingare hidden structures which must be summed overto train the models.Already, these models represent significantprogress.
They learn automatically from a bilin-gual corpus of English and formal language sen-tences.
They do not require linguistically knowl-edgeable xperts to tediously annotate a trainingcorpus.
Rather, they rely upon a group of trans-lators with significantly less linguistic knowledge toproduce a bilingual training corpus.
The fertilitymodels introduced below maintain these benefitswhile slightly improving performance.2 Fertility Clumping TranslationModelsThe rationale behind a clumping model is thatthe input English can be clumped or bracketed intophrases.
Each clump is then generated from a sin-gle formal anguage word using a translation model.The notion of what constitutes a natural clumpingdepends on the formal language.
For example, sup-pose the English sentence were:I want to fly to Memphis please.If the formal anguage for this sentence were:LIST FLIGHTS TO LOCATION,then the most plausible clumping would be:\[I want\] \[to fly\] \[to\] \[Memphis\] \[please\],for which we would expect "\[I want\]" and "\[please\]"to be generated from "LIST", "\[to fly\]" from"FLIGHTS", "\[to\]" from "TO, and "\[Memphis\]"from LOCATION.
Similarly, if the formal languagewere:LIST FLIGHTS DESTINATION_LOCthen the most natural clumping would be:\[I want\] \[to fly\] \[to Memphis\] \[please\],in which we would now expect "\[to Memphis\]" to begenerated by "DESTINATION_LOC".Although these ctumpings are perhaps the mostnatural, neither the clumping nor the alignment isannotated in our training data.
Instead, both thealignment and the clumping are viewed as "hidden"quantities for which all values are possible with someprobability.
The EM algorithm is used to produce amaximum likelihood estimate of the model parame-ters, taking into account all possible alignments andclumpings.In the discussion of fertility models we denote anEnglish sentence by E, which consists of I(E) words.Similarly, we denote the formal language by F, atuple of order g(F), whose individual elements aredenoted by fi.
A clumping for a sentence partitionsE into a tuple of clumps C. The number of clumpsin C is denoted by g(C), and is an integer in therange 1.. .g(E).
A particular clump is denoted byci, where i 6 {1...g(C)}.
The number of words inq is denoted by g(ci), cl begins at the first wordin the sentence, and ct(c) ends at the last word inthe sentence.
The clumps form a proper partitionof E. All the words in a clump c must align to thesame f. An alignment between E and F determineswhich f generates each clump of E in C. Similarly,A denotes the alignment, with g(A) = g(C), and theai denote the formal anguage word to which each ein c~ align.
The individual words in a clump c arerepresented by el ..-el(~).For all fertility models, the fundamental parame-ters are the joint probabilities p( E, C, A, F).
Sincethe clumping and alignment are hidden, to computethe probability that E is generated by F, one calcu-lates:p(E I f ) = Zp(E ,C ,  A IF)C,A1693 Genera l  and  Po isson  Fer t i l i tyIn the general fertility model, the translation prob-ability with "revealed" alignment and clumping isp(E,C,A \[ F) =1 t(P) t(c)Z--\[ 1-\[ P( n' \[ Y,)n,!
r I  p(c~- I Io,) (1)i=1 j= le(c)p(c I f)  = p(e(c) I f) 1 \ ]  p(e, I fc) (2)i=1where p(ni \[ fi) is the fertility probability of gen-erating n i clumps by formal word f~.
Note thatni = L. The factorial terms combine to give aninverse multinomial coefficient which is the uniformprobability distribution for the alignment A of F toC.It appears that the computation of the likelihood,which is the sum of e(F)(e(F) + productterms, is exponential.
Although dynamic program-ming can reduce the complexity, there remain anexponentially arge number of terms to evaluate ineach iteration of the EM algorithm.
We resort toa top-N approximation to the EM sum for the gen-eral model, summing over candidate clumpings andalignments proposed by the Poisson fertility modeldeveloped below.If one assumes that the fertility is modeled by thePoisson distribution with mean fertility ),:e-Xt )tf np(n I f )  - n!
(3)then a polynomial time training algorithm exists.The simplicity arises from the fortuitous cancella-tion of n!
between the Poisson distribution and theuniform alignment probability.
Substituting equa-tion 3 into equation 1 yields:p(E, C, A I F)1 t(F) t(C)= L-7 1 \ ]  ISo,) (4)i=1 j= lI t(F) ?
(C)= Lq 1-I e-X" 1\]  q(cj In , )  (5)i=i j=lIf) -- If), (6)where A: '~ has been absorbed into the effectiveclump score q(c I f).
In this form, it is particu-larly simple to explicitly sum over all alignments Ato obtain p(E, C \[ F) by repeated application of thedistributive law.
The resulting polynomial time ex-pressions are:1 t(f) L(C)p(E, C l F) = L--\[.
r I  e-X" 1\] 4(cj IF) (7)i= I  \]=iq(c I F) = q(c If) (8)\]EFThe q(C \[ F) values for all possible clumpingscan be calculated in O(e(E)2e(F)) time if the maxi-mum clump size is unbounded, and in O(e(E)I(F))if bounded.
The Viterbi decoding algorithm (For-ney, 1973) is used to calculate p(E I L,F) fromthese expressions.
The Viterbi algorithm producesa score which is the sum over all possible clump-ings for a fixed L. This score must then normal-ized by the exp(-X't(v)  z...~,=l AA)/L!
factor.
The EMcount accumulation is done using an adaptationof the Baum-Welch algorithm (Baum, 1972) whichsearches through the space of all possible ctumpings,first considering 1 clump, then 2, and so forth.Initial values for p(e \[ f) are bootstrapped fromModel 1 (Epstein et al, 1996) with the initial meanfertilities A/ set to 1.
We also fixed the maximumclump size at 5 words.
Empirically, we found it ben-eficial to hold the p(e I f) parameters fixed for 20iterations to allow the other parameters to train toreasonable values.
After training, the translationprobabilities and clump lengths are smoothed usingdeleted interpolation (Bahl, Jelinek, and Mercer,1983).Since we have been unable to find a polynomialtime algorithm to train the general fertility model,we use the Poisson model to "expose" the hiddenalignments.
The Poisson fertility model gives themost likely 1000 clumpings and alignments, whichare then restored according to the current generalfertility model parameters.
This gives fractionalcounts for each of the 1000 alignments, which arethen used to update the the general fertility modelparameters.4 Improved C lump Mode l ingIn both the Poisson and general fertility models, thecomputation ofp(clf ) in equation 2 uses a unigrammodel.
Each English word e~ is generated with prob-ability p(ei\[fc).
Two more powerful modeling tech-niques for modeling clump generation are n-gramlanguage models (Miller et al, 1995; Levin and Pier-accini, 1995; Epstein, 1996), and headword languagemodels (Epstein, 1996).
A bigram language modeluses:p(c l Y) =p(e(c) l f)p(el l bdy, f~)p(bdy l el(c), fc) xt(?
)1-Iv(e, t e,-1, fo)i=2where bdy is a special marker to delimit the begin-ning and end of the clump.A headword language model uses two unigrammodels, a headword model and a non-headwordmodel.
Each clump is required to have a headword.All other words are non-headwords.
The identity ofa clump's headword is hidden, hence it is necessary170Word ~ p (n = O)late 1.49 .00early 1.55 .00morning 1.40 .01afternoon 1.62 .00early_morning 2.50 .00= i).62.89.85.85.16p = 2) p >= 3).28 .10.03 .08.11 .03.12 .03.69 .15Table 1: Trained Poisson and General FertilityWordearlymorningListTop p(elf) Scoreearly .37an .22i .09morning .63in .12leaving .05the .21me .19show .18what .17please .04Top ph,ad(elf ) Scoreearly .68i .23day .06morning .75leaving .06flights .05show .49what .17give .07you .06list .05Top p,~onhe~d(elf ) Scorean .30flight .29would .10the .43in .37of .08me .45the .19all .12are .05please .05Table 2: Trained Translation Probabilities using Poisson FertilityTableModel DEC93 DEC93a1ClumpClump-HWClump-BGPoissonPoisson-HWPoisson-BGGeneralGeneral-HWGeneral-BG75.0074.7875.8976.7978.1278.1278.1279.9179.9173.2175.2277.0178.3578.1281.2581.2581.2582.5979.9183.043: Class A CAS on Patterns for DEC93171to sum over all possible headwords:p(c I f )  =I f )  ~?~i=1 j?i5 Example FertilitiesTo illustrate how well fertility captures imple casesof embedding, trained fertilities are shown in table 1for several formal language words denoting time in-tervals.
As expected, "early_morning" dominantlyproduces two clumps, but can produce either one orthree clumps with reasonable probability.
"morn-ing" and "afternoon" train to comparable f rtilitiesand preferentially generate a single clump.
Anotherinteresting case is the formal language token "List"which trains to a A of 0.62 indicating that it fre-quently generates no English text.
As a furthercheck, the A values for "from", "to", and the twospecial classed words "CITY-l" and "CITY-2" arenear 1, ranging between 0.96 and 1.17.Some trained translation probabilities are shownfor the unigram and headword models in table 2.The formal language words have captured reason-able English words for their most likely transla-tion or headword translation.
However, "early"and "morning" have fairly undesirable looking sec-ond and third choices.
The reason for this is thatthese undesirable words are frequently adjacent othe English words "early" and "morning"; hencethe training algorithm includes contributions withtwo word clumps containing these extraneous words.This is the price we pay for not using supervisedtraining data.
Intriguingly, the headword model ismore strongly biased towards the likely translationsand has a smoother tail than the unigram model.6 Resu l tsThe translation models were trained with 5627context-independent ATIS sentences and smoothedwith 600 sentences.
In addition, 3567 training sen-tences were manually aligned and included in a sep-arate training experiment.
This allows comparisonbetween an unannotated corpus and a partially an-notated one.We employ a trivial decoder and language modelsince our emphasis i on evaluating the performanceof different ranslation models.
Our decoder is a sim-ple pattern matcher.
That is, we accumulate the dif-ferent formal language patterns een in the trainingset, and score each of them on the test set.
The lan-guage model is just the unsmoothed unigram prob-ability distribution of the patterns.
This LM has a10% chance of not including a test pattern and itsuse leads to pessimistic performance stimates.
Amore general anguage model for ATIS is presentedin (Koppelman et al, 1995).
Answers are gener-ated by an SQL program which is a deterministicallyconstructed from the formal anguage of our system.The accuracy of these database answers is measuredusing ARPA's Common Answer Specification (CAS)metric.The results are presented in table 3 for ARPA'sDecember 1993 blind test set.
The column headedDEC93 reports results on unsupervised trainingdata, while the column entitled DEC93a contains theresults from using models trained on the partiallyannotated corpus.
The rows correspond to varioustranslation models.
Model 1 is the word-pair trans-lation model used in simple machine translation andunderstanding models (Brown et al, 1993; Epsteinet al, 1996).
The models labeled "Clump" use abasic clumped model without fertility.
The mod-els labeled "Poisson" and "General" use the Poissonand general fertility models presented in this paper.The "HW" and "BG" suffixes indicate the resultswhen p(e\[f) is computed with a headword or bigrammodel.The partially annotated corpus provides an in-crease in performance of about 2-3% for most mod-els.
For General-LM, results increased by 8-10%.The Poisson and general fertility models show a 2-5% gain in performance over the basic clump modelwhen using the partially annotated corpus.
This isa reduction of the error rate by 10-20%.
The unan-notated corpus also shows a comparable gain.Acknowledgement :  This work was sponsoredin part by ARPA and monitored by Fort HuachucaHJ1500-4309-0513.
The views and conclusions con-tained in this document should not be interpretedas representing the official policies of the U.S. Gov-ernment.Re ferencesBahl, Lalit R., Frederick Jelinek, and Robert L.Mercer.
1983.
A maximum likelihood approachto continuous peech recognition.
IEEE Trans-actions on Pattern Analysis and Machine Intelli-gence, PAMI-5(2):179-190, March.Baum, L.E.
1972.
An inequality and associatedmaximization technique in statistical estimationof probabilistic functions of a Markov process.
In-equalities, 3:1-8.Brown, Peter F., Stephen A. DellaPietra, Vincent J.DellaPietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation:Parameter estimation.
In Computational Linguis-tics, pages 19(2):263-311, June.Dempster, A.P., N.M. Laird, and D.B.
Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal StatisticalSociety, 39(B):1-38.172Epstein, M. 1996.
Statistical Source Channel Mod-els for Natural Language Understanding.
Ph.D.thesis, New York University, September.Epstein, M., K. Papineni, S. Roukos, T. Ward, andS.
Della Pietra.
1996.
Statistical natural lan-guage understanding using hidden clumpings.
InProceedings of the IEEE International Conferenceon Acoustics, Speech and Signal Processing, pages176-179, Atlanta, Georgia, May.Forney, G. David.
1973.
The viterbi algorithm.
Pro-ceedings of the IEEE, 61:268-278, March.Gorin, A., S. Levinson, A. Gertner, and E. Goldman.1991.
Adaptive acquisition of language.
Com-puter Speech and Language, 5:101-132.Hemphill, C., J. Godfrey, and G. Doddington.
1990.The ATIS spoken language systems pilot corpus.In Proceedings of the DARPA Speech and NaturalLanguage Workshop, pages 96-101, Hidden Valley,PA, June.
Morgan Kaufmann Publishers, Inc.Koppelman, J., S. Della Pietra, M. Epstein, andS.
Roukos.
1995.
A statistical approach to lan-guage modeling for the ATIS task.
In Proceedingsof the Spoken Language Systems Workshop, pages1785-1788, Madrid, Spain, September.Kuhn, R. and R. De Mori.
1995.
The application ofsemantic lassification trees to natural languageunderstanding.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 17(5):449-460,May.Levin, E. and R. Pieraccini.
1995.
Chronus, the nextgeneration.
In Proceedings of the Spoken Lan-guage Systems Workshop, pages 269-271, Austin,Texas, January.Miller, S., M. Bates, R. Bobrow, R. Ingria,J.
Makhoul, and R. Schwartz.
1995.
Recentprogress in hidden understanding models.
In Pro-ceedings of the Spoken Language Systems Work-shop, pages 276-279, Austin, Texas, January.Miller, S., D. Stallard, R. Bobrow, and R. Schwartz.1996.
A fully statistical approach to natural an-guage interfaces.
In Proceedings of the 34th An-nual Meeting of the Association for Computa-tional Linguistics, pages 55-61, Santa Cruz, CA,June.
Morgan Kaufmann Publishers, Inc.173
