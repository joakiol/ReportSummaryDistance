On Log-Likelihood-Ratios and the Significance of Rare EventsRobert C. MOOREMicrosoft ResearchOne Microsoft WayRedmond, WA 90052USAbobmoore@microsoft.comAbstractWe address the issue of judging the significance ofrare events as it typically arises in statistical natural-language processing.
We first define a general ap-proach to the problem, and we empirically com-pare results obtained using log-likelihood-ratios andFisher?s exact test, applied to measuring strength ofbilingual word associations.1 IntroductionSince it was first introduced to the NLP commu-nity by Dunning (1993), the G2 log-likelihood-ratiostatistic1 has been widely used in statistical NLPas a measure of strength of association, particularlylexical associations.
Nevertheless, its use remainscontroversial on the grounds that it may be unreli-able when applied to rare events.
For instance Ped-ersen, et al (1996) present data showing that signifi-cance values for rare bigrams estimated with G2 candiffer substantially from the true values as computedby Fisher?s exact test.
Although Dunning arguesthat G2 is superior to the chi-square statistic2 X2for dealing with rare events, Agresti (1990, p. 246)cites studies showing ?X2 is valid with smaller sam-ple sizes and more sparse tables than G2,?
and eitherX2 or G2 can be unreliable when expected frequen-cies of less than 5 are involved, depending on cir-cumstances.The problem of rare events invariably ariseswhenever we deal with individual words becauseof the Zipfian phenomenon that, typically, no mat-ter how large a corpus one has, most of the distinctwords in it will occur only a small number of times.For example, in 500,000 English sentences sampledfrom the Canadian Hansards data supplied for thebilingual word alignment workshop held at HLT-NAACL 2003 (Mihalcea and Pedersen, 2003), thereare 52,921 distinct word types, of which 60.5% oc-1Dunning did not use the name G2, but this appears to beits preferred name among statisticians (e.g., Agresti, 1990).2Following Agresti, we use X2 to denote the test statisticand ?2 to denote the distribution it approximates.cur five or fewer times, and 32.8% occur only once.The G2 statistic has been most often used in NLPas a measure of the strength of association betweenwords, but when we consider pairs of words, thesparse data problem becomes even worse.
If welook at the 500,000 French sentences correspond-ing to the English sentences described above, wefind 19,460,068 English-French word pairs that oc-cur in aligned sentences more often than would beexpected by chance, given their monolingual fre-quencies.
Of these, 87.9% occur together five orfewer times, and 62.4% occur together only once.Moreover, if we look at the expected number of oc-currences of these word pairs (which is the criteriaused for determining the applicability of the X2 orG2 significance tests), we find that 93.2% would beexpected by chance to have fewer than five occur-rences.
Pedersen et al (1996) report similar propor-tions for monolingual bigrams in the ACL/DCIWallStreet Journal corpus.
Any statistical measure thatis unreliable for expected frequencies of less than 5would be totally unusable with such data.2 How to Estimate Significance for RareEventsA wide variety of statistics have been used to mea-sure strength of word association.
In one paperalone (Inkpen and Hirst, 2002), pointwise mutualinformation, the Dice coefficient, X2, G2, andFisher?s exact test statistic were all computed andcombined to aid in learning collocations.
Despitethe fact that many of these statistics arise fromsignificance testing, the usual practice in applyingthem in NLP is to choose a threshold heuristicallyfor the value of the statistic being used and discardall the pairs below the threshold.
Indeed, Inkpenand Hirst say (p. 70) ?there is no principled way ofchoosing these thresholds.
?This may seem an odd statement about the mea-sures that arise directly from significance testing,but it is clear that if standard statistical tests are usednaively, the results make no sense in these applica-tions.
One might suppose that this is merely the re-sult of the statistics in question not being applicableto the rare events that predominate in NLP, but it iseasy to show this is not so.2.1 When is Something Seen Only OnceSignficant?Consider the case of two words that each occur onlyonce in a corpus, but happen to co-occur.
Con-ventional wisdom strongly advises suspicion of anyevent that occurs only once, yet it is easy to see thatapplying standard statistical methods to this casewill tend to suggest that it is highly significant, with-out using any questionable approximations at all.The question that significance tests for associa-tion, such as X2, G2, and Fisher?s exact test, aredesigned to answer is, given the sample size andthe marginal frequencies of the two items in ques-tion, what is the probability (or p-value) of seeing bychance as many or more joint occurrences as wereobserved?
In the case of a joint occurrence of twowords that each occur only once, this is trivial to cal-culate.
For instance, suppose an English word and aFrench word each occur only once in our corpus of500,000 aligned sentence pairs of Hansard data, butthey happen to occur together.
What is the proba-bility that this joint occurrence happened by chancealone?
We can suppose that the English word oc-curs in an arbitrary sentence pair.
The probabilitythat the French word, purely by chance, would oc-cur in the same sentence pair is clearly 1 in 500,000or 0.000002.
Since it is impossible to have morethan one joint occurrence of two words that eachhave only a single occurrence, 0.000002 is the exactp-value for the question we have asked.Clearly, however, one cannot assume that the as-sociation of these two words is 0.999998 certain onthis basis alone.
The problem is that there are somany possible singleton-singleton pairs, it is verylikely that some of them will occur jointly, purelyby chance.
This, too, is easy to calculate.
In our500,000 sentence pairs there are 17,379 Englishsingletons and 22,512 French singletons; so thereare 391,236,048 possible singleton-singleton pairs.For each pair, the probability of having a joint oc-currence by chance is 0.000002, so the expectednumber of chance joint occurrences of singleton-singleton pairs is 391, 236, 048 ?
0.000002, or ap-proximately 782.5.The question of whether a singleton-singletonpair is signficant or not then turns on how manysingleton-singleton pairs we observe.
If we see onlyabout 800, then they are not signficant, because thatis just about the number we would expect to see bychance.
In our corpus, however, we see far morethan that: 19,312.
Thus our best estimate of theproportion of the singleton-singleton pairs that aredue to chance is 782.5/19312 = 0.0405, which wecan think of as the ?expected noise?
in the singleton-singleton pairs.
Looked at another way, we can es-timate that at least 95.9% of the observed singleton-singleton pairs are not due to chance, which we canthink of as ?expected precision?.3 So, we concludethat, for this data, seeing two singletons together issignificant at the 0.05 level, but this is more than fiveorders of magnitude less significant than naive useof standard p-values would suggest.2.2 Generalizing the MethodIn the previous section, we used the p-value forthe observed joint frequency given the marginal fre-quencies and sample size as a our base statisticalmeasure.
We used this in an indirect way, however,that we could apply to any other measure of asso-ciation.
For example, for a joint occurrence of twosingletons in 500,000 samples, G2 is approximately28.24.
Therefore, if we wanted to use G2 as ourmeasure of association, we could compare the num-ber of word pairs expected by chance to have a G2score greater than or equal to 28.24 with the numberof word pairs observed to have a G2 score greaterthan or equal to 28.24, and compute expected noiseand precision just as we did with p-values.
In prin-ciple, we can do the same for any measure of as-sociation.
The worst that can happen is that if themeasure of association is not a good one (i.e., ifit assigns values randomly), the expected precisionwill not be very good no matter how high we set thethreshold.This means that we can, if we wish, use two dif-ferent statistics to estimate expected noise and pre-cision, one as a measure of association and oneto estimate the number of word pairs expected bychance to have a given level or higher of the asso-ciation measure.
In our experiments, we will use alikelihood-ratio-based score as the measure of asso-ciation, and contrast the results obtained using ei-ther a likelihood-ratio-based test or Fisher?s exacttest to estimate expectations.Computing the expected number of pairs with agiven association score or higher, for a large collec-3Using the binomial distribution we can calculate that thereis a 0.99 probability that there are no more than 848 singleton-singleton pairs by chance, and hence that there is a 0.99 prob-ability that at least 95.6% of the observed singleton-singletonpairs are not due to chance.
Since this differs hardly at all fromthe expected precision, and there is no a priori reason to thinkoverestimating precision is any worse than underestimating it,we will use expected values of noise and precision as our pri-mary metrics in the rest of the paper.for each observed C(x) {for each observed C(y) {possible pairs =|values of x with frequency C(x)| ?|values of y with frequency C(y)| ;C0(x, y) = int(C(x)C(y)/N) + 1 ;i = 1 ;loop: for each C(x, y) such thatC0(x, y) ?
C(x, y) ?
min(C(x), C(y)) {score = assoc(C(x, y), C(x), C(y), N ) ;if (score ?
threshold[i]) {prob = p-value(C(x, y), C(x), C(y), N ) ;expected pairs = prob ?
possible pairs ;while (score ?
threshold[i]) {expected count[i] += expected pairs ;if (i < number of thresholds) {i++ ;}else {exit loop ;}}}}}}Figure 1: Algorithm for Expected Counts.tion of word pairs having a wide range of marginalfrequencies, turns out to be somewhat tricky.
Wemust first compute the p-value for an associationscore and then multiply the p-value by the appro-priate number of word pairs.
But if the associationscore itself does not correlate exactly with p-value,the relationship between association score and p-value will vary with each combination of marginalfrequencies.4 Furthermore, even for a single com-bination of marginal frequencies, there is in generalno way to go directly from an association score tothe corresponding p-value.
Finally, until we havecomputed all the expected frequencies and observedfrequencies of interest, we don?t know which as-sociation score is going to correspond to a desiredlevel of expected precision.These complications can be accomodated as fol-lows: First compute the distinct marginal frequen-cies of the words that occur in the corpus (sepa-4We assume that for fixed marginals and sample size, andjoint frequencies higher than the expected joint frequency, theassociation score will increase monotonically with the joint fre-quency.
It is hard to see how any function without this prop-erty could be considered a measure of association (unless itdecreases monotonically as joint frequency increases).rately for English and French), and how many dis-tinct words there are for each marginal frequency.Next, choose a set of association score thresholdsthat we would like to know the expected precisionsfor.Accumulate the expected pair counts for eachthreshold by iterating through all possible combi-nations of observed marginals.
For each combina-tion, compute the association score for each possi-ble joint count (given the marginals and the sam-ple size), starting from the smallest one greater thanthe expected joint count C(x)C(y)/N (where C(x)and C(y) are the marginals and N is the samplesize).
Whenever the first association score greaterthan or equal to one of the thresholds is encountered,compute the associated p-value, multiply it by thenumber of possible word pairs corresponding to thecombination of marginals (to obtain the expectednumber of word pairs with the given marginals hav-ing that association score or higher), and add theresult to the accumluators for all the thresholds thathave just been passed.
Stop incrementing the pos-sible joint frequency when either the smaller of thetwo marginals is reached or the highest associationthreshold is passed.
(See Figure 1 for the details.
)At this point, we have computed the number ofword pairs that would be expected by chance aloneto have an association score equal to or greater thaneach of our thresholds.
Next we compute the num-ber of word pairs observed to have an associationscore equal to or greater than each of the thresh-olds.
The expected noise for each threshold is justthe ratio of the expected number of word pairs forthe threshold to the observed number of word pairsfor the threshold, and the expected precision is 1 mi-nus the expected noise.What hidden assumptions have we made thatcould call these estimates into question?
First, theremight not be enough data for the estimates of ex-pected and observed frequencies to be reliable.
Thisshould seldom be a problem in statistical NLP.
Forour 500,000 sentence pair corpus, the cumulativenumber of observed word pairs is in the tens ofthousands for for any association score for whichthe estimated noise level approaches or exceeds 1%,which yields confidence bounds that should be morethan adequate for most purposes (see footnote 3).A more subtle issue is that our method may over-estimate the expected pair counts, resulting in ex-cessively conservative estimates of precision.
Ourestimate of the number of pairs seen by chance for aparticular value of the association measure is basedon considering all possible pairs as nonassociated,which is a valid approximation only if the number2 log[p(y|x)C(x,y) ?
p(y|?x)C(?x,y) ?
p(?y|x)C(x,?y) ?
p(?y|?x)C(?x,?y)p(y)C(y) ?
p(?y)C(?y)](1)2 log[p(y|x)C(x,y) ?
p(y|?x)C(?x,y) ?
p(?y|x)C(x,?y) ?
p(?y|?x)C(?x,?y)p(y)C(x,y) ?
p(y)C(?x,y) ?
p(?y)C(x,?y) ?
p(?y)C(?x,?y)](2)2 log?x??{x,?x}?y??{y,?y}(p(y?|x?)p(y?))C(x?,y?)(3)2???x??{x,?x}?y??
{y,?y}C(x?, y?)
logp(y?|x?)p(y?)??
(4)2N???x??{x,?x}?y??
{y,?y}p(x?, y?)
logp(x?, y?)p(x?)p(y?)??
(5)Figure 2: Alternative Formulas for G2.of pairs having a significant positive or negative as-sociation is very small compared to the total numberof possible pairs.For the corpus used in this paper, this seems un-likely to be a problem.
The corpus contains 52,921distinct English words and 66,406 distinct Frenchwords, for a total of 3,514,271,926 possible wordpairs.
Of these only 19,460,068 have more than theexpected number of joint occurrences.
Since mostword pairs have no joint occurrences and far lessthan 1 expected occurrence, it is difficult to get ahandle on how many of these unseen pairs might benegatively associated.
Since we are measuring asso-ciation on the sentence level, however, it seems rea-sonable to expect fewer word pairs to have a signifi-cant negative association than a positive association,so 40,000,000 seems likely to be a upper bound onhow many word pairs are significantly nonindepen-dent.
This, however, is only about 1% of the totalnumber of possible word pairs, so adjusting for thepairs that might be significantly related would notmake an appreciable difference in our estimates ofexpected noise.
In applications where the signifi-cantly nonindependent pairs do make up a substan-tial proportion of the total possible pairs, an adjust-ment should be made to avoid overly conservativeestimates of precision.3 Understanding G2Dunning (1993) gives the formula for the statisticwe are calling G2 in a form that is very compact,but not necessarily the most illuminating:2 [ log L(p1, k1, n1) + log L(p2, k2, n2) ?log L(p, k1, n1) ?
log L(p, k2, n2) ],whereL(p, k, n) = pk(1 ?
p)n?k.The interpretation of the statistic becomes clearerif we re-express it in terms of frequencies and prob-abilities as they naturally arise in association prob-lems, as shown in a number of alternative formu-lations in Figure 2.
In these formulas, x and yrepresent two words for which we wish to esti-mate the strength of association.
C(y) and C(?y)are the observed frequencies of y occurring or notoccurring in the corpus; C(x, y), .
.
.
, C(?x,?y)are the joint frequencies of the different possiblecombinations of x and y occuring and not occur-ing; and p(y), p(?y), p(y|x), .
.
.
, p(?y|?x) are themaximum likelihood estimates of the correspondingmarginal and conditional probabilities.Formula 1 expresses G2 as twice the logarithmof a ratio of two estimates of the probability of asequence of observations of whether y occurs; oneestimate being conditioned on whether x occurs,and the other not.
The estimate in the numeratoris conditioned on whether x occurs, so the numera-tor is a product of four factors, one for each possi-ble combination of x occuring and y occuring.
Theoverall probability of the sequence is the productof each conditional probability of the occurrence ornonoccurrence of y conditioned on the occurrenceor nonoccurrence of x, to the power of the numberof times the corresponding combination occurs inthe sequence of observations.
The denominator isan estimate of the probability of the same sequence,based only on the marginal probability of y. Hencethe denominator is simply the product of the prob-abilty of y occuring, to the power of the number oftimes y occurs, and the probabilty of y not occur-ing, to the power of the number of times y fails tooccur.5The rest of Figure 2 consists of a sequence of mi-nor algebraic transformations that yield other equiv-alent formulas.
In Formula 2, we simply factorthe denominator into four factors corresponding tothe same combinations of occurrence and nonoc-currence of x and y as in the numerator.
Then,by introducing x?
and y?
as variables ranging overthe events of x and y occurring or not occurring,we can re-express the ratio as a doubly nestedproduct as shown in Formula 3.
By distributingthe log operation over all the products and expo-nentiations, we come to Formula 4.
Noting thatC(x?, y?)
= N ?
p(x?, y?)
(where N is the sam-ple size), and p(y?|x?)/p(y?)
times p(x?)/p(x?)
isp(x?, y?)/p(x?)p(y?
), we arrive at Formula 5.
Thiscan be immediately recognized as 2N times the for-mula for the (average) mutual information of tworandom variables,6 using the maximum likelihoodestimates for the probabilities involved.The near equivalence of G2 and mutual informa-tion is important for at least two reasons.
First, itgives us motivation for using G2 as a measure ofword association that is independent of whether itusable for determining significance.
Mutual infor-mation can be viewed as a measure of the informa-tion gained about whether one word will occur byknowing whether the other word occurs.
A priori,this is at least as plausible a measure of strength ofassociation as is the degree to which we should besurprised by the joint frequency of the two words.Thus even if G2 turns out to be bad for estimatingsignificance, it does not follow that it is therefore abad measure of strength of association.The second benefit of understanding the relationbetween G2 and mutual information is that it an-5The interpretation of G2 in terms of a likelihood ratio fora particular sequence of observations omits the binomial co-efficients that complicate the usual derivation in terms of allpossible sequences having the observed joint and marginal fre-quencies.
Since all such sequences have the same probabilityfor any given probability distributions, and the same numberof possible sequences are involved in both the numerator anddenominator, the binomial coefficients cancel out, yielding thesame likelihood ratio as for a single sequence.6After discovering this derivation, we learned that itis, in fact, an old result (Attneave, 1959), but it seemsto be almost unknown in statistical NLP.
The only ref-erence to it we have been able to find in the statisti-cal NLP ?literature?
is a comment in Pederson?s publicallydistributed Perl module for computing mutual information(http://search.cpan.org/src/TPEDERSE/Text-NSP-0.69/ Mea-sures/tmi.pm).
It can also be seen as a special case of a moregeneral result presented by Cover and Thomas (1991, p. 307,12.187?12.192), but otherwise we have not found it in any con-temporary textbook.swers the question of how to compare G2 scores asmeasures of strength of association, when they areobtained from corpora of different sizes.
Formula 5makes it clear that G2 scores will increase linearlywith the size of the corpus, assuming the relevantmarginal and conditional probabilities remain thesame.
The mutual information score is independentof corpus size under the same conditions, and thusoffers a plausible measure to be used across corporaof varying sizes.4 Computing Fisher?s Exact TestIn Section 2 we developed a general method of es-timating significance for virtually any measure ofassociation, given a way to estimate the expectednumber of pairs of items having a specified degreeof association or better, conditioned on the marginalfrequencies of the items composing the pair andthe sample size.
We noted that for some plausiblemeasures of association, the association metric it-self can be used to estimate the p-values needed tocompute the expected counts.
G2 is one such mea-sure, but it is questionable whether it is usable forcomputing p-values on the kind of data typical ofNLP applications.
We will attempt to answer thisquestion empirically, at least with respect to bilin-gual word association, by comparing p-values andexpected noise estimates derived from G2 to thosederived from a gold standard, Fisher?s exact test.In this test, the hypergeometric probability distri-bution is used to compute what the exact probabilityof a particular joint frequency would be if there wereno association between the events in question, giventhe marginal frequencies and the sample size.
Theonly assumption made is that all trials are indepen-dent.
The formula for this probability in our settingis:C(x)!
C(?x)!
C(y)!
C(?y)!N !
C(x, y)!
C(?x, y)!
C(x,?y)!
C(?x,?y)!The p-value for a given joint frequency is ob-tained by summing the hypergeometric probabilityfor that joint frequency and every more extremejoint frequency consistent with the marginal fre-quencies.
In our case ?more extreme?
means larger,since we are only interested in positive degrees ofassociation.7 Because it involves computing fac-torials of potentially large numbers and summingover many possible joint frequencies, this test hastraditionally been considered feasible only for rela-tively small sample sizes.
However, a number op-7The null hypothesis that we wish to disprove is that apair of words is either negatively associated or not associated;hence, a one-sided test is appropriate.timizations enable efficient estimation of p-valuesby Fisher?s exact test for sample sizes up to at least1011 on current ordinary desktop computers, wherethe limiting factor is the precision of 64-bit floatingpoint arithmetic rather than computation time.Some keys to efficient computation of Fisher?sexact test are:?
The logarithms of factorials of large numberscan be efficiently computed by highly accuratenumerical approximations of the gamma func-tion (Press et al, 1992, Chapter 6.1), based onthe identity n!
= ?
(n + 1).?
The following well-known recurrence relationfor the hypergeometric distribution:Pk =Ck?1(?x, y) Ck?1(x,?y)Ck(x, y) Ck(?x,?y)Pk?1makes it easy to calculate probabilities for a se-quence of consecutive joint frequencies, oncethe first one is obtained.
(The subscript k indi-cates parameters associated with the kth jointfrequency in the sequence.)?
The highest possible joint frequency will be thesmaller of the two marginal frequencies, so ifone of the marginals is small, few terms needto be summed.?
If we iterate from less extreme joint frequen-cies to more extreme joint frequencies, eachprobability in the summation will be smallerthan the one before.
If both the marginalsare large, the summation will often convergeto a constant value, given limited arithmeticprecision, long before the smaller marginal isreached, at which point we can stop the sum-mation.By taking advantage of these observations, plus afew other optimizations specific to our application,we are able to estimate the necessary expected jointfrequencies for our 500,000 sentence pair corpus in66.7 minutes using Fisher?s exact test, compared to57.4 minutes using an approximate estimate basedon likelihood ratios, a time penalty of only 16% forusing the exact method.5 Estimating P-Values withLog-Likelihood-RatiosThe usual way of estimating p-values from log-likelihood-ratios is to rely on the fact that the p-values for G2 asymtotically approach the well-understood ?2 distribution, as the sample size in-creases.
This is subject to the various caveats andconditions that we discussed in Section 1, however.Since we have the ability to compute all of the ex-act p-values for our corpus, we do not need to relyon the ?2 approximation to test whether we can uselog-likelihood-ratios to estimate p-values.
We canempirically measure whether there is any consis-tent relationship between log-likelihood-ratios andp-values, and if so, use it empirically to estimate p-values from log-likelihood-ratios without resortingto the ?2 approximation.
For all we know at thispoint, it may be possible to empirically predict p-values from G2 under conditions where the corre-spondence with ?2 breaks down.This means we can drop the heretofore mysteri-ous factor of 2 that has appeared in all the formulasfor G2, since this factor seems to have been intro-duced just to be able to read p-values directly fromstandard tables for the ?2 distribution.
To make itclear what we are doing, from this point on we willuse a statistic we will call LLR which we define tobe G2/2.To look for a relationship between LLR and p-values as computed by Fisher?s exact test, we firstcomputed both statistics for a various combinationsof joint frequency, marginal frequencies, and sam-ple sizes.
Exploratory data analysis suggested anear-linear relationship between LLR scores andthe negative of the natural logarithm of the p-values.To make sure the apparent relationship held for areal dataset, we computed the LLR scores and neg-ative log p-values for all 19,460,068 English-Frenchword pairs in our corpus with more joint occur-rences than expected by chance, and carried out aleast-squares linear regression, treating LLR scoreas the independent variable and negative log p-valueas the dependent variable, to see how well we canpredict p-values from LLR scores.
The results areas follows:slope: 1.00025intercept: 1.15226Pearson?s r2: 0.999986standard deviation: 0.552225With an r2 value that rounds off to five nines,LLR score proves to be a very good predictor ofthe negative log p-values over the range of valuesconsidered.
Moreover, with a slope of very close to1, the LLR score and negative log p-values are notmerely correlated, they are virtually the same exceptfor the small delta represented by the intercept.
Inother words,p-value ?
e?
(LLR+1.15)would seem to be not too bad an approximation.The standard deviation of 0.55, however, is atleast slightly worrying.
As a range of differences inthe logarithms of the predicted and actual p-values,it corresponds to a range of ratios between the pre-dicted and actual p-values from about 0.57 to 1.7.6 Estimating Noise in Bilingual WordAssociationFor our final experiment, we estimated the noise inthe bilingual word associations in our data by themethod of Section 2, using both Fisher?s exact testand LLR scores via our regression equation to es-timate expected pair counts.
In both cases, we useLLR scores as the measure of association.
We com-puted the cumulative expected noise for every inte-gral value of the LLR score from 1 through 20.To try to determine the best results we could getby using LLR scores to estimate expected noise inthe region where we would be likely to set a cut-off threshold, we recomputed the least-squares fit ofLLR and negative log p-value, using only data withLLR scores between 5 and 15.8 We obtained thefollowing values for the parameters of the regres-sion equation from the re-estimation:slope: 1.04179intercept: 0.793324Note that the re-estimated value of the interceptmakes it closer to the theoretical value, which is?
log(0.5) ?
0.69, since independence correspondsto an LLR score of 0 and a p-value of 0.5.The results of these experiments are summarizedin Table 1.
The first column shows the potentialLLR association score cut-offs, the second col-umn is the expected noise for each cut-off esti-mated by Fisher?s exact test, the third column givesthe noise estimates derived from p-values estimatedfrom LLR scores, and the fourth column shows theratio between the two noise estimates.
If we lookat the noise estimates based on our gold standard,Fisher?s exact test, we see that the noise level is be-low 1% above an LLR score of 11, and rises rapidlybelow that.
This confirms previous annecdotal ex-perience that an LLR score above 10 seems to be areliable indicator of a significant association.The comparison between the two noise estimatesindicates that the LLR score underestimates theamount of noise except at very high noise levels.It is worst when the LLR score cut-off equals 14,8This constitutes training on the test data for the sake of ob-taining an upper bound on what could be achieved using LLRscores.
Should we conclude that LLR scores look promisingfor this use, one would want to re-run the test training the re-gression parameters on held-out data.Fisher LLRCut-Off Noise Est Noise Est Ratio1 0.624 0.792 1.272 0.516 0.653 1.273 0.423 0.384 0.914 0.337 0.274 0.815 0.256 0.183 0.716 0.181 0.114 0.637 0.119 0.0650 0.558 0.0713 0.0338 0.479 0.0394 0.0159 0.4010 0.0205 0.00695 0.3411 0.00946 0.00260 0.2712 0.00432 0.000961 0.2213 0.00136 0.000221 0.1614 0.00137 0.000166 0.1215 3.52e-005 2.00e-005 0.5716 1.56e-005 8.02e-006 0.5117 6.82e-006 3.19e-006 0.4718 2.94e-006 1.24e-006 0.4219 1.24e-006 4.65e-007 0.3820 5.16e-007 1.72e-007 0.33Table 1: Word Association Noise Estimates.which happens to be just below the LLR score(14.122) for singleton-singleton pairs.
Since, fora given sample size, singleton-singleton pairs havethe lowest possible expected joint count, this isprobably the effect of known problems with estimat-ing p-values from likelihood ratios when expectedcounts are very small.7 ConclusionsWhen we use Fisher?s exact test to estimate p-values, our new method for estimating noise for col-lections of rare events seems to give results thatare quite consistent with our previous annecdotalexperience in using LLR scores as a measure ofword association.
Using likelihood ratios to esti-mate p-values introduces a substantial amount of er-ror, but not the orders-of-magnitude error that Dun-ning (1993) demonstrated for estimates that rely onthe assumption of a normal distribution.
However,since we have also shown that Fisher?s exact test canbe applied to this type of problem without a majorcomputational penalty, there seems to be no reasonto compromise in this regard.8 AcknowledgementsThanks to Ken Church, Joshua Goodman, DavidHeckerman, Mark Johnson, Chris Meek, Ted Peder-sen, and Chris Quirk for many valuable discussionsof the issues raised in this paper.
Thanks especiallyto Joshua Goodman for pointing out the existenceof fast numerical approximations for the factorialfunction, and to Mark Johnson for helping to trackdown previous results on the relationship betweenlog-likelihood-ratios and mutual information.ReferencesAlan Agresti.
1990.
Categorical Data Analysis.John Wiley & Sons, New York, New York.Fred Attneave.
1959.
Applications of InformationTheory to Psychology.
Holt, Rinehart and Win-ston, New York, New York.Thomas M. Cover and Joy A. Thomas.
1991.
El-ements of Information Theory.
John Wiley &Sons, New York, New York.Ted Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Computa-tional Linguistics, 19(1):61?74.Diana Z. Inkpen and Graeme Hirst.
2002.
Acquir-ing collocations for lexical choice between near-synonyms.
In Unsupervised Lexical Acquisition:Proceedings of the Workshop of the ACL SpecialInterest Group on the Lexicon (SIGLEX), pp.
67?76, Philadelphia, Pennsylvania.Rada Mihalcea and Ted Pedersen.
2003.
An eval-uation exercise for word alignment.
In Proceed-ings of the HLT-NAACL 2003 Workshop, Buildingand Using Parallel Texts: Data Driven MachineTranslation and Beyond, pp.
1?6, Edmonton, Al-berta.Ted Pedersen, Mehmet Kayaalp, and RebeccaBruce.
1996.
Significant Lexical Relationships.In Proceedings of the 13th National Conferenceon Artificial Intelligence, Portland, Oregon.William H. Press, Saul A. Teukolsky, William T.Vetterling, and Brian P. Flannery.
1992.
Numer-ical Recipies in C: The Art of Scientific Com-puting, Second Edition.
Cambridge UniversityPress, Cambridge, England.
