Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 95?99,Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational LinguisticsChinese Coreference Resolution via Ordered Filtering?Xiaotian Zhang1,2 Chunyang Wu1,2 Hai Zhao1,2?1Center for Brain-Like Computing and Machine Intelligence,Department of Computer Science and Engineering, Shanghai Jiao Tong University2MOE-Microsoft Key Laboratory for Intelligent Computing and Intelligent SystemsShanghai Jiao Tong Universityxtian.zh@gmail.com, chunyang506@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cnAbstractWe in this paper present the model for ourparticipation (BCMI) in the CoNLL-2012Shared Task.
This paper describes a purerule-based method, which assembles dif-ferent filters in a proper order.
Differentfilters handle different situations and thefiltering strategies are designed manually.These filters are assigned to different or-dered tiers from general to special cases.We participated in the Chinese and En-glish closed tracks, scored 51.83 and 59.24respectively.1 IntroductionIn this paper, we describes the approaches we u-tilized for our participation in the CoNLL-2012Shared Task.
This year?s shared task targets atmodeling coreference resolution for multiple lan-guages.
Following (Lee et al, 2011), we extend-s the methodology of deterministic coreferencemodel, using manually designed rules to rec-ognize expressions with corresponding entities.The deterministic coreference model (Raghu-?
This work was partially supported by the Na-tional Natural Science Foundation of China (Grant No.60903119 and Grant No.
61170114), the National Re-search Foundation for the Doctoral Program of Higher E-ducation of China under Grant No.
20110073120022, theNational Basic Research Program of China (Grant No.2009CB320901), the Science and Technology Commissionof Shanghai Municipality (Grant No.
09511502400), andthe European Union Seventh Framework Program (GrantNo.
247619).?
Corresponding author.nathan et al, 2010) has shown good perfor-mance in the shared task of CoNLL-2011.
Thiskind of model focuses on filtering with orderedtiers: One filter is applied at one time, fromhighest to lowest precision.
However, comparedwith learning approaches (Soon et al, 2001), s-ince effective rules are quite heterogeneous indifferent languages, several filtering methodsshould be redesigned when different languagesare considered.
We modified the original Stan-ford English coreference system1 to adapt to theChinese scenario.
For the English participation,we implemented the full strategies and interfaceof the semantic-based filters which are not ob-tained from the open source toolkit.The rest of this paper is organized as follows:In Section 2, we review the related work; In Sec-tion 3, we describe the detail of our model ofhandling coreference resolution in Chinese; Ex-periment results are reported in Section 4 andthe conclusion is presented in Section 5.2 Related WorkMany existing works have been published onlearning relation extractors via supervised (Soonet al, 2001) or unsupervised (Haghighi and K-lein, 2010; Poon and Domingos, 2008) approach-es.
For involving semantics, (Rahman and Ng,2011) proposed a coreference resolution modelwith world knowledge; By using word associa-tions, (Kobdani et al, 2011) showed its effec-tiveness to coreference resolution.
Compared1http://nlp.stanford.edu/software/dcoref.shtml95with machine learning methods, (Raghunathanet al, 2010) proposed rule-base models whichhave been witnessed good performance.Researchers began to work on Chinese coref-erence resolution at a comparatively late dateand most of them adopt a machine learningapproach.
(Guochen and Yunfei, 2005) basedtheir Chinese personal pronoun coreference res-olution system on decision trees and (Naiquan etal., 2009) realized a Chinese coreference resolu-tion system based on maximum entropy model.
(Weixuan et al, 2010) proposes a SVM-basedapproach to anaphora resolution of noun phrasesin Chinese and achieves the F-measure of 63.3%in the evaluation on ACE 2005.
(Guozhi et al,2011) presented a model for personal pronounsanaphora resolution based on corpus,which us-ing rule pretreatment combined with maximumentropy.3 Model for ChineseIn general, we adapt Stanford English corefer-ence system to Chinese by making necessarychanges.
The sketch of this deterministic modelis to extract mentions and relevant informationfirstly; then several manually designed rules, orfiltering sieves are applied to identify the corefer-ence.
Moreover, these sieves are utilized in a pre-designed order, which are sorted from highest tolowest precision.
The ordered filtering sieves arelisted in Table 1.Ordered Sieves1.
Mention Detection Sieve2.
Discourse Processing Sieve3.
Exact String Match Sieve4.
Relaxed String Match Sieve5.
Precise Constructs Sieve6.
Head Matching Sieves7.
Proper Head Word Match Sieve8.
Pronouns Sieve9.
Post-Processing SieveTable 1: Ordered filtering sieves for Chinese.
Modi-fied sieves are bold.We remove the semantic-based sieves due tothe resource constraints.
The simplified versionconsists of nine filtering sieves.
The bold onesin Table 1 are the modified sieves for Chinese.First of all, we adopt the head finding rules forChinese used in (Levy and Manning, 2003), andthis affects sieve 4, 6 and 7 which are all takeadvantage of the head words.
And our changeto other sieves are described as follows.?
Mention Detection Sieve: We in thissieve first extract all the noun phrases,pronouns (the words with part-of-speech(POS) tag PN), proper nouns (the word-s with POS tag NR) and named entities.Thus a mention candidate set is produced.We then refine this set by removing severaltypes of candidates listed as follows:1.
Themeasure words, a special word pat-tern in Chinese such as ?
??
(a yearof), ????
(a ton of).2.
Cardinals, percents and money.3.
A mention if a larger mention with thesame head word exists.?
Discourse Processing & PronounsSieve: In these two sieves, we adaptthe common pronouns to Chinese.
It in-cludes ?\?
(you), ???
(I or me),??
(heor him),???
(she or her),???
(it),?\??
(plural of ?you?
), ????
(we or us),???
(they, gender: male),????
(they, gender:female),????
(plural of ?it?)
and relativepronoun ?gC?
(self).
Besides these, weenrich the pronouns set by adding ?4?, ?4?
?, ?
T?
and ?T??
which are more oftento appear in spoken dialogs as first personpronouns and ?
s?
which is used to showrespect for ?you?
and the third person pro-noun ??
?.Besides, for mention processing of the originalsystem, whether a mention is singular or pluralshould be given.
Different from English POStags, in Chinese plural nouns couldn?t be distin-guished from single nouns in terms of the POS.Therefore, we add two rules to judge whether anoun is plural or not.?
A noun that ends with ???
(plural markerfor pronouns and a few animate nouns), and?
?
(and so on) is plural.96?
A noun phrase that involves the coordinat-ing conjunction words such as ?
??
(and)is plural.4 Experiments4.1 Modification for the English systemWe implement the semantic-similarity sievesproposed in (Lee et al, 2011) with the WordNet.These modifications consider the alias sieve andlexical chain sieve.
For the alias sieve, two men-tions are marked as aliases if they appear in thesame synset in WordNet.
For the lexical chainsieve, two mentions are marked as coreference iflinked by a WordNet lexical chain that traverseshypernymy or synonymy relations.4.2 Numerical ResultsLang.
Coref Anno.
R P FChBefore gold 87.78 40.63 55.55auto 80.37 38.95 52.47After gold 69.56 62.77 65.99auto 65.02 59.76 62.28EnBefore gold 93.65 42.32 58.30auto 88.84 40.17 55.32After gold 77.49 74.59 76.01auto 72.88 74.53 73.69Table 2: Performance of the mention detection com-ponent, before and after coreference resolution, withboth gold and auto linguistic annotations on devel-opment set.Lang.
R P FCh 61.11 62.12 61.61En 75.23 72.24 73.71Table 3: Performance of the mention detection com-ponent, after coreference resolution, with auto lin-guistic annotations on test set.Table 2 shows the performance of mention de-tection both before and after the coreference res-olution with gold and predicted linguistic anno-tations on development set.
The performance ofmention detection on test set is presented in Ta-ble 3.
The recall is much higher than the preci-sion so as to make sure less mentions are missed,Metric R P F1 avg F1ChMUC 50.02 49.64 49.8351.83BCUBED 65.81 65.50 65.66CEAF (M) 49.88 49.88 49.88CEAF (E) 40.39 43.47 41.88BLANC 67.12 65.83 66.45EnMUC 64.08 63.57 63.8259.24BCUBED 66.45 70.71 68.51CEAF (M) 57.24 57.24 57.24CEAF (E) 45.13 45.67 45.40BLANC 71.12 77.92 73.95Table 5: Results on the official test set (closed track).and because spurious mentions will be left as s-ingletons and removed at last, a low precisionwill not affect the final result.
The performanceof mention detection for Chinese is worse thanthat of English, and this is a direction for futureimprovement for Chinese.Our results on the development set for bothlanguages are listed in Table 4 and the officialtest results are in Table 5.
Avg F1 is the arith-metic mean of MUC, B3, and CEAFE.We further examine the performance by test-ing on different data types (broadcast con-versations, broadcast news, magazine articles,newswire, conversational speech, and web da-ta) of the development set, and the results areshown in Table 6.
The system do better on bn,mz, tc than bc, nw, wb for both Chinese andEnglish.
And it performs the worst on wb dueto a relative lower recall in mention detection.For Chinese, we also compare the performancewhen handling the three different mention types,proper nominal, pronominal, and other nominal.Table 7 shows the scores output by the officialscorer when only each kind of mentions are pro-vided in the keys file and response file each timeand both the quality of the coreference links a-mong the nominal of each mention type and thecorresponding performance of mention detectionare presented.
The performance of coreferenceresolution among proper nominal and pronomi-nal is significant higher than that of other nom-inal which highly coincides with the results inTable 6.97MUC BCUBED CEAF (E) avg F1Lang.
Setting R P F1 R P F1 R P F1ChAUTO 52.38 47.44 49.79 68.25 62.36 65.17 37.43 41.89 39.54 51.50GOLD 58.16 53.55 55.76 70.66 68.65 69.64 41.44 45.60 43.42 56.27GMB 63.60 87.63 73.70 62.71 88.32 73.34 74.08 42.83 54.28 67.11EnAUTO 64.24 64.95 64.59 68.22 73.16 70.60 47.03 46.29 46.66 60.61GOLD 67.45 66.94 67.20 69.76 73.62 71.64 47.86 48.42 48.14 62.33GMB 71.78 90.55 80.08 65.45 88.95 75.41 77.42 46.47 58.08 71.19Table 4: Results on the official development set (closed track).
GMB stands for Gold Mention BoundariesLang.
Anno.
bc bn mz nw pt tc wbCh AUTO 50.31 53.87 52.80 47.82 - 55.10 47.54GOLD 53.19 63.63 58.23 50.65 - 58.96 50.15En AUTO 59.26 62.40 63.17 57.57 65.24 60.91 56.88GOLD 60.34 64.51 64.36 59.71 67.07 62.44 58.47Table 6: Results (Avg F1) on different data types of the development set (closed track).Proper nominal Pronominal Other nominalData Type MD (Recall) avg F1 MD (Recall) avg F1 MD (Recall) avg F1bc 94.5 (550/582) 68.06 94.5 (1372/1452) 66.40 80.5 (1252/1555) 47.74bn 96.7 (1213/1254) 67.46 97.8 (264/270) 77.39 83.7 (1494/1786) 53.51mz 92.0 (526/572) 67.05 94.8 (91/96) 56.89 76.1 (834/1096) 53.68nw 91.4 (402/440) 67.44 90.6 (29/32) 83.54 51.0 (1305/2559) 44.86tc 100 (23/23) 95.68 84.5 (572/677) 61.96 71.2 (272/382) 53.88wb 93.2 (218/234) 72.23 95.9 (397/414) 72.55 77.1 (585/759) 43.37all 94.4 (2932/3105) 68.30 92.7 (2725/2941) 68.10 70.6 (5742/8137) 49.56Table 7: Results ( Recall of mention detection and Avg F1) on different data types and different mentiontypes of the development set with linguistic annotations (closed track).985 ConclusionWe presented the rule-base approach for the BC-MI?s participation in the shared task of CoNLL-2012.
We extend the work by (Lee et al, 2011)and modified several tiers to adapt to Chinese.Numerical results show the effectiveness in theevaluation for Chinese and English.
For theChinese scenario, we firstly show it is possibleto consider special POS-tags and common pro-nouns as indicators for improving the perfor-mance.
This work could be extended by involv-ing more feasible filtering tiers or utilizing someautomatic rule generating methods.ReferencesLi Guochen and Luo Yunfei.
2005.
?^`k?J????<?????).
(Personal pronouncoreference resolution in Chinese using a priorityselection strategy).
Journal of Chinese Informa-tion Processings, pages 24?30.Dong Guozhi, Zhu Yuquan, and Cheng Xianyi.
2011.Research on personal pronoun anaphora resolutionin chinese.
Application Research of Computers,28:1774?1776.Aria Haghighi and Dan Klein.
2010.
Coreferenceresolution in a modular, entity-centered model.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, HLT?10, pages 385?393, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Hamidreza Kobdani, Hinrich Schu?tze, MichaelSchiehlen, and Hans Kamp.
2011.
Bootstrappingcoreference resolution using word associations.
InProceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: HumanLanguage Technologies - Volume 1, HLT ?11, pages783?792, Stroudsburg, PA, USA.
Association forComputational Linguistics.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and DanJurafsky.
2011.
Stanford multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Proceedings of the Fifteenth Confer-ence on Computational Natural Language Learn-ing: Shared Task, pages 28?34, Portland, Oregon,USA, June.
Association for Computational Lin-guistics.Roger Levy and Christopher Manning.
2003.
Is itharder to parse chinese, or the chinese treebank?In Proceedings of the 41st Annual Meeting on As-sociation for Computational Linguistics - Volume1, ACL ?03, pages 439?446, Stroudsburg, PA, US-A.
Association for Computational Linguistics.Hu Naiquan, Kong Fang, Wang Haidong, and ZhouGuodong.
2009.
Realization on chinese corefer-ence resolution system based on maximum entropymodel.
Application Research of Computers, pages2948?2951.Hoifung Poon and Pedro Domingos.
2008.
Joint un-supervised coreference resolution with markov log-ic.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 650?659, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Karthik Raghunathan, Heeyoung Lee, SudarshanRangarajan, Nate Chambers, Mihai Surdeanu,Dan Jurafsky, and Christopher Manning.
2010.A multi-pass sieve for coreference resolution.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages492?501, Cambridge, MA, October.
Associationfor Computational Linguistics.Altaf Rahman and Vincent Ng.
2011.
Coreferenceresolution with world knowledge.
In Proceedingsof the 49th Annual Meeting of the Associationfor Computational Linguistics: Human LanguageTechnologies - Volume 1, HLT ?11, pages 814?824,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Y-ong Lim.
2001.
A machine learning approach tocoreference resolution of noun phrases.
Comput.Linguist., 27(4):521?544, December.Tan Weixuan, Kong Fang, Wang Haidong, and ZhouGuodong.
2010.
Svm-based approach to chineseanaphora resolution.
High Performance Comput-ing Technology, pages 30?36.99
