Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 170?181,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsGrowing Multi-Domain Glossaries from a Few Seedsusing Probabilistic Topic ModelsStefano Faralli and Roberto NavigliDipartimento di InformaticaSapienza Universita` di Roma{faralli,navigli}@di.uniroma1.itAbstractIn this paper we present a minimally-supervised approach to the multi-domain ac-quisition of wide-coverage glossaries.
We startfrom a small number of hypernymy rela-tion seeds and bootstrap glossaries from theWeb for dozens of domains using Probabilis-tic Topic Models.
Our experiments show thatwe are able to extract high-precision glos-saries comprising thousands of terms and def-initions.1 IntroductionDictionaries, thesauri and glossaries are usefulsources of information for students, scholars and ev-eryday readers, who use them to look up words ofwhich they either do not know, or have forgotten,the meaning.
With the advent of the Web an increas-ing number of dictionaries and technical glossarieshas been made available online, thereby speedingup the definition search process.
However, findingdefinitions is not always immediate, especially if thetarget term pertains to a specialized domain.
Indeed,not even well-known services such as Google Defineare able to provide definitions for scientific or tech-nical terms such as taxonomy learning or distant su-pervision in AI or figure-four leglock and suspendedsurfboard in wrestling.Domain-specific knowledge of a definitional na-ture is not only useful for humans, it is also use-ful for machines (Hovy et al 2013).
Examplesinclude Natural Language Processing tasks such asQuestion Answering (Cui et al 2007), Word SenseDisambiguation (Duan and Yates, 2010; Faralli andNavigli, 2012) and ontology learning (Velardi et al2013).
Unfortunately, most of the Web dictionar-ies and glossaries available online comprise just afew hundred definitions, and they therefore provideonly a partial view of a domain.
This is also thecase with manually compiled glossaries created bymeans of collaborative efforts, such as Wikipedia.1The coverage issue is addressed by online aggrega-tion services such as Google Define, which bring to-gether definitions from several online dictionaries.However, these services do not classify textual def-initions by domain: they just present the collecteddefinitions for all the possible meanings of a giventerm.In order to automatically obtain large domainglossaries, in recent years computational approacheshave been developed which extract textual defi-nitions from corpora (Navigli and Velardi, 2010;Reiplinger et al 2012) or the Web (Velardi et al2008; Fujii and Ishikawa, 2000).
The methods in-volving corpora start from a given set of terms (pos-sibly automatically extracted from a domain cor-pus) and then harvest textual definitions for theseterms from the input corpus using a supervised sys-tem.
Web-based methods, instead, extract text snip-pets from Web pages which match pre-defined lex-ical patterns, such as ?X is a Y?, along the linesof Hearst (1992).
These approaches typically per-form with high precision and low recall, becausethey fall short of detecting the high variability of thesyntactic structure of textual definitions.
To addressthe low-recall issue, recurring cue terms occurring1See http://en.wikipedia.org/wiki/Portal:Contents/Glossaries170within dictionary and encyclopedic resources can beautomatically extracted and incorporated into lexicalpatterns (Saggion, 2004).
However, this approach isterm-specific and does not scale to arbitrary termi-nologies and domains.The goal of the new approach outlined in this pa-per is to enable the automatic harvesting of large-scale, full-fledged domain glossaries for dozens ofdomains, an outcome which should be very use-ful for both human activities and automatic tasks.We present ProToDoG (Probabilistic Topics formulti-Domain Glossaries), a framework for growingmulti-domain glossaries which has three main nov-elties:i) minimal human supervision: a small set ofhypernymy relation seeds for each domain isused to bootstrap the multi-domain acquisitionprocess;ii) jointness: our approach harvests terms andglosses at the same time;iii) probabilistic topic models are leveraged fora simultaneous, high-precision multi-domainclassification of the extracted definitions, withsubstantial performance improvements overour previous work on glossary bootstrapping,i.e., GlossBoot (De Benedictis et al 2013).ProToDog is able to harvest definitions from theWeb and thus drop the requirement of large corporafor each domain.
Moreover, apart from the need toselect a few seeds, it avoids the use of training dataor manually defined sets of lexical patterns.
It is thusapplicable to virtually any language of interest.2 ProToDoGGiven a set of domains D = {d1, ..., dn}, for eachdomain d ?
D ProToDoG harvests a domain glos-sary Gd containing pairs of the kind (t, g) where tis a domain term and g is its textual definition, i.e.,gloss.
We show the pseudocode of ProToDoG in Al-gorithm 1.Step 1.
Initial seed selection: Algorithm 1 takesas input a set of domains D and, for each domaind ?
D, a small set of hypernymy relation seedsSd = {(t1, h1), .
.
.
, (t|Sd|, h|Sd|)}, where the seedAlgorithm 1 ProToDoGInput: the set of domains D,a set Sd of hypernymy seeds for each domaind ?
DOutput: a multi-domain glossary G1: k ?
12: repeat3: for each domain d ?
D do4: Gkd ?
?5: for each seed (tj , hj) ?
Sd do6: pages?
webSearch(tj , hj , ?glossary?
)7: Gkd ?
Gkd ?
extractGlossary(pages)8: end for9: end for10: create a topic model using glossaries from previ-ous iterations11: infer topic assignments for iteration-k glosses12: filter out non-domain glosses for each domain13: for each d ?
D do14: Sd ?
seedSelectionForNextIteration(Gkd)15: end for16: k ?
k + 117: until k > max18: for each domain d ?
D do19: recover filtered glosses into Gmax+1d20: Gd ?
?j=1,...,max+1Gjd21: end for22: return G = {(Gd, d) : d ?
D}pair (tj , hj) contains a term tj and its generalizationhj (e.g., (linux, operating system)).
This is the onlyhuman input to the entire glossary acquisition pro-cess.
The selection of the input seeds plays a keyrole in the bootstrapping process, in that the patternand gloss extraction process will be driven by them.The chosen hypernymy relations thus have to be astopical and representative as possible for the domainof interest (e.g., (compiler, computer program) is anappropriate pair for computer science, while (byte,unit of measurement) is not, as it might cause theextraction of out-of-domain glossaries of units andmeasures).The algorithm first sets the iteration counter k to1 (line 1) and starts the first iteration of the glos-sary bootstrapping process (lines 2-17), each involv-ing steps 2-4, described below.
After each iterationk, for each domain d we keep track of the set ofglosses Gkd acquired during that iteration.
After thelast iteration, we perform step (5) of gloss recovery(lines 18-21).171Step 2.
Web search and glossary extraction (lines3-9): For each domain d, we first initialize the do-main glossary for iteration k: Gkd := ?
(line 4).Then, for each seed pair (tj , hj) ?
Sd, we submitthe following query to a Web search engine: ?tj??hj?
glossary and collect the top-ranking resultsfor each query (line 6).2 Each resulting page is acandidate glossary for the domain d.We then call the extractGlossary function (line7) which extracts terms and glosses from the re-trieved pages as follows.
From each candidate page,we harvest all the text snippets s starting with tj andending with hj (e.g., ?linux</b> ?
an<i>operatingsystem?
), i.e., s = tj .
.
.
hj .
For each such text snip-pet s, we extract the following pattern instance:pL tj pM glosss(tj) pR,where:?
pM is the longest sequence of HTML tags andnon-alphanumeric characters between tj andthe glossary definition (e.g., ?</b> ??
between?linux?
and ?an?
in the above example);?
glosss(tj) is the gloss of tj obtained by mov-ing to the right of pM until we reach a non-formatting tag element (e.g., <span>, <p>,<div>), while ignoring formatting elementssuch as <b>, <i> and <a> which are typi-cally included within a definition sentence;?
pL and pR are the longest sequences of HTMLtags on the left of tj and the right of glosss(tj),respectively.For instance, given the HTML snippet?.
.
.<p><b>linux</b> ?
an <i>operatingsystem</i> developed by Linus Torvalds</p>.
.
.
?we extract the following pattern instance: pL =?<p><b>?, tj = ?linux?, pM = ?</b> ?
?,glosss(tj) = ?an <i>operating system</i>developed by Linus Torvalds?, pR =?</p>?.Then we generalize the above pattern instance byreplacing tj and glosss(tj) with *, obtaining:pL ?
pM ?
pR,For the above example, we obtain the followingpattern:2We use the Google Ajax API, which returns the 64 top-ranking search results.<p><b> * </b> ?
* </p>.We add the first sentence of the retrieved glossglosss(tj) to our glossary Gkd, i.e., Gkd := Gkd ?
{(tj , first(glosss(tj)))}, where first(g) returnsthe first sentence of gloss g. Finally, we look for ad-ditional pairs of terms/glosses in the Web page con-taining the snippet s by matching the page againstthe generalized pattern pL ?
pM ?
pR, and addingthem to Gkd.As a result of step (2), for each domain d ?
Dwe obtain a glossary Gkd for the terms discovered atiteration k.Step 3.
Topic modeling and gloss filtering (lines10-12): Unfortunately, not all (term, gloss) pairsin a glossaryGkd will pertain to the domain d. For in-stance, we might end up retrieving interdisciplinaryor even unrelated glossaries.
In order to address thisfuzziness, we model domains with a ProbabilisticTopic Model (PTM) (Blei et al 2003; Steyvers andGriffiths, 2007).
PTMs model a given text documentas a mixture of topics.
In our case topics are do-mains and we, first, create a topic model from thedomain glossaries acquired before the current iter-ation k, then, second, use the topic model to esti-mate the domain assignment of each new pair (term,gloss) in our glossaries Gkd, i.e., obtained at iterationk, third, filter out non-domain glosses.Creating the topic model (line 10): For a giveniteration k and domain d, we first define the ter-minology accumulated up until iteration k ?
1 forthat domain as the set T 1,k?1d :=?k?1j=1 Tjd , whereT jd is the set of terms acquired at iteration j, i.e.,T jd := {t : ?
(t, g) ?
Gjd}.3 Then we define:?
W :=?d?D T1,k?1d as the entire terminologyacquired up until iteration k?1 for all domains,i.e., the full set of terms independently of theirdomain;?
M :=?d?D?k?1j=1 Gjd as the multi-domainglossary acquired up until iteration k ?
1, i.e.,the full set of pairs (term, gloss) independentlyof their domain;43For the first iteration, i.e., when k = 1, we define T 1,0d :={t : ?
(t, g) ?
G1d}, i.e., we use the terminology resulting fromstep (2) of the first iteration.4For k = 1, M :=?d?D G1d.172?
Two count matrices, i.e., the word-domain ma-trix CWD and the gloss-domain matrix CMD,such that: CWDw,d counts the number of timesw ?W is assigned to domain d ?
D, i.e., it oc-curs in the glosses of domain d; CMD(t,g),d countsthe number of words in g assigned to domain d.At this point, as shown by Steyvers and Grif-fiths (2007), we can estimate the probability ?
(d)w forword w, and the probability ?
(t,g)d for a term/glosspair (t, g), of belonging to domain d:?
(d)w =CWDw,d + ?
?|W |w?=1 CWDw?,d + |W |?
; (1)?
(t,g)d =CMD(t,g),d + ?
?|D|d?=1 CMD(t,g),d?
+ |D|?
(2)where ?
and ?
are smoothing factors.5 The twoabove probabilities represent the core of our topicmodel of the domain knowledge acquired up untiliteration k ?
1.Probabilistic modeling of iteration-k glosses (line11): We now utilize the above topic model to es-timate the probabilities in Formulas 1 and 2 for thenewly acquired glosses at iteration k. To this end wedefine M ?
:=?d?DGkd as the union of the (term,gloss) pairs at iteration k and W ?
:=?d?D Tkd?Was the union of terms acquired at iteration k, but alsooccurring in W (i.e., the entire terminology up un-til iteration k ?
1).
Then we apply Gibbs sampling(Blei et al 2003; Phan et al 2008) to estimate theprobability of each pair (t, g) ?
M ?
of pertaining toa domain d by computing:??
(t,g)d =RM?D(t,g),d + ?
?|D|d?=1RM ?D(t,g),d?
+ |D|?
(3)where the gloss-domain matrixRM?D is initially de-fined by counting random domain assignments foreach word w?
in the bag of words of each (term,gloss) pair ?
M ?.
Next, the domain assignmentcounts in RM?D are iteratively updated using Gibbssampling.65As experienced by Steyvers and Griffiths (2007), the valuesof ?
= 50/|D| and ?
= 0.01 work well with many differenttext collections.6For the PTM part of ProToDoG we used the JGibbLDAFiltering out non-domain glosses (line 12): Now,for each domain d ?
D, for each pair (t, g) ?
Gkd wehave a probability ??
(t,g)d of belonging to d. We mark(t, g) as a non-domain item if ??
(t,g)d < ?, where ?
isa confidence threshold, or if ??
(t,g)d is not maximumamong all domains in D. Non-domain pairs are re-moved from Gkd and stored into a set Ad for possiblerecovery after the last iteration (see step (5)).Step 4.
Seed selection for next iteration (lines13-15): For each domain d ?
D, we now selectthe new set of hypernymy relation seeds to be usedto start the next iteration.
First, for each newly-acquired term/gloss pair (t, g) ?
Gkd, we automat-ically extract a candidate hypernym h from the tex-tual gloss g. To do this we use a simple heuristicwhich just selects the first content term in the gloss.7Then we sort all the glosses in Gkd by the number ofseed terms found in each gloss.
In the case of ties(i.e., glosses with the same number of seed terms),we further sort the glosses by ??
(t,g)d .
Finally we se-lect the (term, hypernym) pairs corresponding to the|Sd| top-ranking glosses as the new set of seeds forthe next iteration.Next, we increment k (line 16 of Algorithm 1)and if the maximum number of iterations is reachedwe jump to step (5).
Otherwise, we go back to step(2) of our glossary bootstrapping algorithm with thenew set of seeds Sd.Step 5.
Gloss recovery (line 19): After all iter-ations, the entire multi-domain terminology W (cf.step (3)) may contain several new terms which werenot present when a given gloss g was filtered out.So, thanks to the last-iteration topic model, the glossg might come back into play because its words arenow important cues for a domain.
To reassess thedomain pertinence of (term, gloss) pairs in Ad foreach d, we just reapply the entire step (3) by settingGmax+1d := Ad for each d ?
D. As a result, welibrary, a Java Implementation of Latent Dirichlet Allocation(LDA) using Gibbs Sampling for Parameter Estimation and In-ference, available at: http://jgibblda.sourceforge.net/7While more complex strategies could be devised, e.g.,lattice-based hypernym extraction (Navigli and Velardi, 2010),we found that this heuristic works well because, even when it isnot a hypernym, the first term acts as a cue word for the definedterm.173obtain an updated glossary Gmax+1d which containsall the recovered glosses.Final output: For each domain d ?
D the finaloutput of ProToDoG is a domain glossary Gd :=?j=1,...,max+1Gjd.
Finally the algorithm aggregatesall glossaries Gd into a multi-domain glossary G(line 22).3 Experimental Setup3.1 DomainsFor our experiments we selected 30 different do-mains ranging from Arts to Warfare, mostly follow-ing the domain classification of Wikipedia featuredarticles (full list at http://lcl.uniroma1.it/protodog).
The set includes several techni-cal domains, such as Chemistry, Geology, Meteorol-ogy, Mathematics, some of which are highly inter-disciplinary.
For instance, the Environment domaincovers terms from fields such as Chemistry, Biology,Law, Politics, etc.3.2 Gold StandardSince our evaluations required considerable humaneffort, in what follows we calculated all perfor-mances on a random set of 10 domains, shown in thetop row of Table 1.
For each of these 10 domains weselected well-reputed glossaries on the Web as goldstandards, including the Reuters glossary of finance,the Utah computing glossary and many others (fulllist at the above URL).
We show the size of our 10gold-standard datasets in Table 1.3.3 Evaluation measuresWe evaluated the quality of both terms and glosses,as jointly extracted by ProToDoG.3.3.1 TermsFor each domain we calculated coverage, extra-coverage and precision of the acquired terms T .Coverage is the ratio of extracted terms in T alsocontained in the gold standard T?
over the size of T?
.Extra-coverage is calculated as the ratio of the ad-ditional extracted terms in T \ T?
over the number ofgold standard terms T?
.
Finally, precision is the ra-tio of extracted terms in T deemed to be within thedomain.
To calculate precision we randomly sam-pled 5% of the retrieved terms and asked two humanannotators to manually tag their domain pertinence(with adjudication in case of disagreement; ?
= .62,indicating substantial agreement).
Note that by ran-domly sampling on the entire set T we calculate theprecision of both terms in T ?
T?
, i.e., in the goldstandard, and terms in T \ T?
, i.e., not in the goldstandard, but which are not necessarily outside thedomain.3.3.2 GlossesWe calculated the precision of the extractedglosses as the ratio of glosses which were both well-formed textual definitions and specific to the tar-get domain.
Precision was determined on a randomsample of 5% of the acquired glosses for each do-main.
The annotation was made by two annotators,with ?
= .675, indicating substantial agreement.The annotators were provided with specific guide-lines available on the ProToDoG Web site (see URLabove).3.4 ComparisonWe compared ProToDog against:?
BoW: a bag-of-words variant in which step(3) is replaced by a simple bag-of-words scor-ing approach which assigns a score to eachterm/gloss pair (t, g) ?
Gkd as follows:score(g) =|Bag(g) ?
T 1,k?1d ||Bag(g)|.
(4)where Bag(g) contains all content words ing.
At iteration k, we filter out those glosseswhose score(g) < ?, where ?
is a thresh-old tuned in the same manner as ?
(see Sec-tion 3.5).
This approach essentially implementsGlossBoot, our previous work on domain glos-sary bootstrapping (De Benedictis et al 2013).?
Wikipedia: since Wikipedia is the largestcollaborative resource, covering hundredsof fields of knowledge, we devised a simpleheuristic for producing multi-domain glos-saries from Wikipedia, so as to compare theirperformance against our gold standards.
Foreach target domain we manually selected one17445%50%55%60%65%70%75%80%85%2  4  6  8  10  12  14  16  18  20iterationBotanyFashionFigure 1: Harmonic mean of precision and coverage forBotany and Fashion (tuning domains) over 20 iterations(|Sd|=5, ?=0.03).or more Wikipedia categories representingthe domain (for instance, Category:Artsfor Arts, Category:Business for Fi-nance, etc.).
Then, for each domain d,we picked out all the Wikipedia pagestagged either with the categories selectedfor d or their direct subcategories (e.g.,Category:Creative works) or sub-subcategories (e.g., Category:Genres).From each page we extracted a (page title,gloss) pair, where the gloss was obtained byextracting the first sentence of the Wikipediapage, as done, e.g., in BabelNet (Navigli andPonzetto, 2012).
Since subcategories mighthave more parents and might thus belong tomultiple domains, we discarded pages assignedto more than 2 domains.3.5 Parameter tuningIn order to choose the optimal values of the parame-ters of ProToDoG (number |Sd| of seeds per domain,number max of iterations, and filtering threshold ?
)and BoW (?
threshold) we selected two extra do-mains, i.e., Botany and Fashion, not used in ourtests, together with the corresponding gold standardWeb glossaries.As regards the number of seeds, we defined aninitial pool of 10 seeds for each of the two tun-ing domains and studied the average performanceof 5 random sets of x seeds (from the initial pool),when x = 1, 3, 5, 7, 9.
As regards the number ofiterations, we explored all values between 1 and20.
Finally, for the filtering thresholds ?
and ?for ProToDoG PTM and its BoW variant, we triedvalues of ?
?
{0, 0.03, 0.06, .
.
.
, 0.6} and ?
?
{0, 0.05, 0.1, .
.
.
, 1.0}, respectively.Given the high number of possible parametervalue configurations, we first explored the entiresearch space automatically by calculating the cov-erage of ProToDoG PTM (and BoW) with each con-figuration against our tuning gold standards.
Thenwe identified as optimal candidates those ?fron-tier?
configurations for which, when moving froma lower-coverage configuration, coverage reached amaximum.
We then calculated the precision of eachoptimal candidate configuration by manually vali-dating a 3% random sample of the resulting glos-saries for the two tuning domains.
The optimal con-figuration for ProToDoG was |Sd| = 5, max = 5,?
= 0.03, while for BoW was ?
= 0.1.In Figure 1 we show the performance trend overiterations for our two tuning domains when |Sd| = 5and ?
= 0.03.
Performance is calculated as theharmonic mean of precision and coverage of the ac-quired glossary after each iteration, from 1 to 20.
Wecan see that after 5 iterations performance decreasesfor Botany (a highly interdisciplinary domain) dueto lower precision, while it remains stable for Fash-ion due to the lack of newly-acquired glosses.3.6 Seed SelectionFor each domain d we manually selected five seedhypernymy relations as the seed sets Sd input to Al-gorithm 1 (see Section 3.5).
The seeds were selectedby the authors on the basis of just two conditions: i)the seeds should cover different aspects of the do-main and, indeed, should identify the domain im-plicitly; ii) at least 10,000 results should be returnedby the search engine when querying it with the seedsplus the glossary keyword (see line 6 of Algo-rithm 1).
The seed selection was not fine-tuned (i.e.,it was not adjusted to improve performance), so itmight well be that better seeds would provide betterresults (see (Kozareva and Hovy, 2010a)).
However,such a study is beyond the scope of this paper.175ArtBusinessChemistryComputingEnvironmentFoodLawMusicPhysicsSportGold t/g 394 1777 164 421 713 946 180 218 315 146PTM t 4253 7370 2493 3412 3009 1526 1836 1647 3847 1696g 7386 9795 3841 4186 3552 2175 4141 2729 5197 2938BoW t 4012 7639 1174 3127 3644 1827 1773 1166 4471 1990g 5923 8999 1414 3662 4334 2601 4024 1249 6956 3425Wiki t,g 107.1k 48.4k 8137 32.0k 23.6k 5698 13.5k 84.1k 33.8k 267.5kTable 1: Size of the gold standard and the automatically-acquired glossaries for 10 of the 30 selected domains (t:number of terms, g: number of glosses).4 Results and Discussion4.1 TermsThe size of the extracted terminologies for the 10 do-mains after five iterations is reported in Table 1 (theoutput for all 30 domains is available at the aboveURL, cf.
Section 3.1).
ProToDoG PTM and its BoWvariant extract thousands of terms and glosses foreach domain, whereas the number of glosses ob-tained from Wikipedia (cf.
Section 3.4) varies de-pending upon the domain, from thousands to hun-dreds of thousands.
Note that there is no overlapbetween the glossaries extracted by ProToDoG andthe set of Wikipedia articles, since the latter are notorganized as glossaries.In Table 2 we show the percentage results interms of precision (P), coverage (C), and extra-coverage (X, see Section 3.3 for definitions) forProToDoG PTM and its BoW variant and for theWikipedia glossary.
With the exception of theFood domain, ProToDoG achieves the best pre-cision.
The Wikipedia glossary has fluctuatingprecision values, ranging between 25% and 90%,due to the heterogeneous nature of subcategories.ProToDog achieves the best coverage of gold stan-dard terms on 6 of the 10 domains, with the BoWvariant obtaining slightly higher coverage on 3 do-mains and +10% on the Food domain.
The cov-erage of Wikipedia glossaries, instead, with thesole exception of Sport, is much lower, despite theuse of (sub)subcategories (cf.
Section 3.4).
BothProToDoG PTM and BoW achieve very high extra-coverage percentages, meaning that they are able togo substantially beyond our domain gold standards,but it is the Wikipedia glossary which achieves thehighest extra-coverage values.
To get a better in-sight into the quality of extra-coverage we calcu-lated the percentage of named entities (i.e., encyclo-pedic) among the terms extracted by each of the dif-ferent approaches.
Comparing results across the (E)columns of Table 2 it can be seen that high percent-ages of the terms extracted by Wikipedia are namedentities, which is in marked contrast to the 0%-1%extracted by ProToDog.
This is as should be ex-pected for an encyclopedia, whose coverage focuseson people, places, brands, etc.
rather than concepts.To summarize, ProToDoG PTM outperforms bothBoW and Wikipedia in terms of precision, whileat the same time achieving both competitive cov-erage and extra-coverage.
The Wikipedia glossarysuffers from fluctuating precision values across do-mains and overly encyclopedic coverage of terms.4.2 GlossesWe show the results of gloss evaluation in Table 2(last two columns) for ProToDoG PTM and BoW(we do not report the precision values for Wikipedia,as they are slightly lower than those obtained forterms).
Precision ranges between 89% and 99%for ProToDoG PTM and between 82% and 97%for BoW.
We observe that these results are stronglycorrelated with the precision of the extracted terms(cf.
Table 2), because the retrieved glosses of do-main terms are usually in-domain too, and follow adefinitional style since they come from glossaries.Note, however, that the gloss precision could also be176terms glossesPTM BoW Wiki PTM BoWP C X E P C X E P C X E P PArt 92 26 1053 1 86 25 992 0 81 19 23.4k 67 93 87Business 95 41 374 0 90 43 387 0 37 15 2692 31 96 91Chemistry 99 77 1410 0 95 73 643 0 49 18 12.9k 3 98 96Computing 95 43 767 0 93 40 702 0 81 30 7506 36 96 94Environment 91 29 393 0 84 28 482 0 25 9 3302 12 89 82Food 91 21 1404 0 97 31 1621 0 81 9 3997 25 92 95Law 98 89 931 0 95 87 897 0 35 34 7406 16 99 97Music 94 98 660 0 93 84 453 0 90 50 37.1k 84 96 95Physics 97 43 1178 0 91 46 1373 0 68 25 10.6k 10 95 89Sport 98 22 1139 1 96 23 1339 1 87 44 178.2k 83 97 96Table 2: Precision (P), coverage (C), extra-coverage (X), encyclopedic (E) percentages after 5 iterations.ArtBusinessChemistryComputingEnvironmentFoodLawMusicPhysicsSportGoogle Define 76 80 93 86 88 91 96 96 98 84ProToDoG 27 41 81 40 37 19 85 98 47 27Table 3: Number of domain glosses (from a random sam-ple of 100 gold standard terms per domain) retrieved us-ing Google Define and ProToDoG.higher than term precision, thanks to many pertinentglosses being extracted for the same term (cf.
Table1).In Table 4 we show an excerpt of the multi-domain glossary extracted by ProToDoG for the Art,Business and Sport domains.5 Comparative Evaluation5.1 Comparison with Google DefineWe performed a comparison with Google Define,8a state-of-the-art definition search service.
Thisservice inputs a term query and outputs a list ofglosses.
First, we randomly sampled 100 termsfrom our gold standard for each domain.
Next, foreach domain, we manually calculated the fractionof terms for which at least one in-domain defini-tion was provided by Google Define and ProToDoG.8Accessible from Google search with the define: key-word.Table 3 shows the coverage results.
In this exper-iment, Google Define outperforms our system on9 of the 10 analyzed domains.
However, we notethat when searching for domain-specific knowledgeonly, Google Define: i) needs to know the domainterm to be defined in advance, while ProToDoGjointly acquires domain terms and glosses startingfrom just a few seeds; ii) does not discriminate be-tween glosses pertaining to the target domain andglosses pertaining to other fields or senses, whereasProToDog extracts terms and glosses specific to eachdomain of interest.5.2 Comparison with TaxoLearnWe also compared ProToDoG with the output ofa state-of-the-art taxonomy learning framework,called TaxoLearn (Navigli et al 2011).
We didthis because i) TaxoLearn extracts terms and glossesfrom domain corpora in order to create a domain tax-onomy; ii) it is one of the few systems which extractsboth terms and glosses from specialized corpora; iii)the extracted glossaries are available online.9 There-fore we compared the performance of ProToDoG ontwo domains for which glossaries were extracted byTaxoLearn, i.e.
AI and Finance.
The glossaries wereharvested from large collections of scholarly arti-cles.
For ProToDoG we selected 10 seeds to coverall the fields of AI, while for the financial domainwe selected the same 5 seeds used in the Business9http://ontolearn.org and http://lcl.uniroma1.it/taxolearn177Artrock art includes pictographs (designs painted on stone surfaces) and petroglyphs (designspecked or incised on stone surfaces).impressionism Late 19th-century French school dedicated to defining transitory visual impressionspainted directly from nature, with light and color of primary importance.point Regarding paper, a unit of thickness equating 1/1000 inch.Businesshyperinflation Extremely rapid or out of control inflation.interbank rate The rate of interest charged by a bank on a loan to another bank.points Amount of discount on a mortgage loan stated as a percentage; one point equals onepercent of the face amount of the loan; a discount of one point raises the net yield onthe loan by one-eighth of one percent.Sportgross score The actual number of strokes taken by a player for hole or round before the player?shandicap is deducted.obstructing preventing the opponent from going around a player by standing in the path of move-ment.points a team statistic indicating its degree of success, calculated as follows: 2 points for awin (3 in the 1994 World Cup), 1 point for a tie, 0 points for a loss.Table 4: An excerpt of the resulting multi-domain glossary obtained with ProToDoG.domain of our experiments above (cf.
Section 3).We show the number of extracted terms andglosses for ProToDoG and TaxoLearn in Table 5.We also show the precision values calculated on arandom sample of 5% of terms and glosses.
As canbe clearly seen, on both domains ProToDoG extractsa number of terms and glosses which is an orderof magnitude greater than those obtained by Tax-oLearn, while at the same time obtaining consider-ably higher precision.6 Related WorkCurrent approaches to automatic glossary acquisi-tion suffer from two main issues: i) the poor avail-ability of large domain-specific corpora from whichterms and glosses are extracted at different times;ii) the focus on individual domains.
ProToDog ad-dresses both issues by providing a joint multi-domain approach to term and glossary extraction.Among the approaches which extract unre-stricted textual definitions from open text, Fujii andIshikawa (2000) determine the definitional nature oftext fragments by using an n-gram model, whereasKlavans and Muresan (2001) apply pattern match-ing techniques at the lexical level guided by cuephrases such as ?is called?
and ?is defined as?.More recently, a domain-independent supervised ap-proach, named Word-Class Lattices (WCLs), waspresented which learns lattice-based definition clas-sifiers applied to candidate sentences containing theinput terms (Navigli and Velardi, 2010).
To avoidthe burden of manually creating a training dataset,definitional patterns can be extracted automatically.Faralli and Navigli (2013) utilized Wikipedia asa huge source of definitions and simple, yet ef-fective heuristics to automatically annotate them.Reiplinger et al(2012) experimented with two dif-ferent approaches for the acquisition of lexical-syntactic patterns.
The first approach bootstraps pat-terns from a domain corpus and then manually re-fines the acquired patterns.
The second approach, in-stead, automatically acquires definitional sentencesby using a more sophisticated syntactic and seman-tic processing.
The results show high precisionin both cases.
However, all the above approachesneed large domain corpora, the poor availability ofwhich hampers the creation of wide-coverage glos-saries for several domains.
To avoid the need touse a large corpus, domain terminologies can be ob-tained by using Doubly-Anchored Patterns (DAPs)178AI Finance# terms P # glosses P # terms P # glosses PProToDoG 4983 83% 5326 84% 7370 95% 9795 96%TaxoLearn 427 77% 834 79% 2348 86% 1064 88%Table 5: Number and precision of terms and glosses extracted by ProToDoG and TaxoLearn in the Artificial Intelli-gence (AI) and Finance domains.which, given a (term, hypernym) pair, extract fromthe Web sentences matching manually-defined pat-terns like ?<hypernym> such as <term>, and *?
(Kozareva and Hovy, 2010b).
This term extrac-tion process is further extended by harvesting newhypernyms using the corresponding inverse pat-terns (called DAP?1) like ?
* such as <term1>, and<term2>?.
Similarly to ProToDoG, this approachdrops the requirement of a domain corpus and startsfrom a small number of (term, hypernym) seeds.However, while DAPs have proven useful in the in-duction of domain taxonomies (Kozareva and Hovy,2010b), they cannot be applied to the glossary learn-ing task because the extracted sentences are not for-mal definitions.
In contrast, ProToDoG performsthe novel task of multi-domain glossary acquisitionfrom the Web by bootstrapping the extraction pro-cess with a few (term, hypernym) seeds.
Bootstrap-ping techniques (Brin, 1998; Agichtein and Gra-vano, 2000; Pas?ca et al 2006) have been success-fully applied to several tasks, including learning se-mantic relations (Pantel and Pennacchiotti, 2006),extracting surface text patterns for open-domainquestion answering (Ravichandran and Hovy, 2002),semantic tagging (Huang and Riloff, 2010) and un-supervised Word Sense Disambiguation (Yarowsky,1995).
ProToDoG synergistically integrates boot-strapping with probabilistic topic models so as tokeep the glossary acquisition process within the tar-get domains as much as possible.7 ConclusionsIn this paper we have presented ProToDoG, a new,minimally-supervised approach to multi-domainglossary acquisition.
Starting from a small set of hy-pernymy seeds which identify each domain of inter-est, we apply a bootstrapping approach which itera-tively obtains generalized patterns from Web glos-saries and then applies them to the extraction ofterm/gloss pairs.
To our knowledge, ProToDoG isthe first approach to large-scale probabilistic glos-sary learning which jointly acquires thousands ofterms and glosses for dozens of domains with mini-mal supervision.At the core of ProToDoG lies our glossary boot-strapping approach, thanks to which we can dropthe requirements of existing techniques such as theready availability of domain corpora, which often donot contain enough definitions (cf.
Table 5), and themanual definition of lexical patterns, which typicallyextract sentence snippets instead of formal glosses.ProToDoG will be made available to the re-search community.
Beyond the immediate usabil-ity of the output glossaries (we show an excerptin Table 4), we also wish to show the benefitof ProToDoG in gloss-driven approaches to taxon-omy learning (Navigli et al 2011; Velardi et al2013) and Word Sense Disambiguation (Duan andYates, 2010; Faralli and Navigli, 2012).
The 30-domain glossaries and gold standards created forour experiments are available from http://lcl.uniroma1.it/protodog.We remark that the terminologies covered withProToDoG are not only precise, but are also oneorder of magnitude greater than those covered inindividual online glossaries.
As future work, weplan to study the ability of ProToDoG to acquiredomain glossaries at different levels of specificity(i.e., domains vs. subdomains).
Finally, we willadapt ProToDoG to other languages, by translatingthe glossary keyword used in step (2), along thelines of (De Benedictis et al 2013).AcknowledgmentsThe authors gratefully acknowledgethe support of the ?MultiJEDI?
ERCStarting Grant No.
259234.179ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:extracting relations from large plain-text collections.In Proceedings of the 5th ACM conference on DigitalLibraries, pages 85?94, San Antonio, Texas, USA.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Sergey Brin.
1998.
Extracting patterns and relationsfrom the World Wide Web.
In Proceedings of theInternational Workshop on The World Wide Web andDatabases, pages 172?183, London, UK.Hang Cui, Min-Yen Kan, and Tat-Seng Chua.
2007.
Softpattern matching models for definitional question an-swering.
ACM Transactions on Information Systems,25(2):8.Flavio De Benedictis, Stefano Faralli, and Roberto Nav-igli.
2013.
GlossBoot: Bootstrapping MultilingualDomain Glossaries from the Web.
In Proceedings ofthe 51st Annual Meeting of the Association for Compu-tational Linguistics, pages 528?538, Sofia, Bulgaria.Weisi Duan and Alexander Yates.
2010.
Extractingglosses to disambiguate word senses.
In Proceedingsof Human Language Technologies: The 11th AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 627?635, Los Angeles, CA, USA.Stefano Faralli and Roberto Navigli.
2012.
A NewMinimally-supervised Framework for Domain WordSense Disambiguation.
In Proceedings of the 2012Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, pages 1411?1422, Jeju, Korea.Stefano Faralli and Roberto Navigli.
2013.
A JavaFramework for Multilingual Definition and HypernymExtraction.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics, Sys-tem Demonstrations, pages 103?108, Sofia, Bulgaria.Atsushi Fujii and Tetsuya Ishikawa.
2000.
Utilizingthe World Wide Web as an encyclopedia: extractingterm descriptions from semi-structured texts.
In Pro-ceedings of the 38th Annual Meeting on Associationfor Computational Linguistics, pages 488?495, HongKong.Marti A. Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 15th International Conference on ComputationalLinguistics, pages 539?545, Nantes, France.Eduard H. Hovy, Roberto Navigli, and Simone PaoloPonzetto.
2013.
Collaboratively built semi-structuredcontent and Artificial Intelligence: The story so far.Artificial Intelligence, 194:2?27.Ruihong Huang and Ellen Riloff.
2010.
Inducingdomain-specific semantic class taggers from (almost)nothing.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages275?285, Uppsala, Sweden.Judith Klavans and Smaranda Muresan.
2001.
Evalu-ation of the DEFINDER system for fully automaticglossary construction.
In Proceedings of the AmericanMedical Informatics Association (AMIA) Symposium,pages 324?328, Washington, D.C., USA.Zornitsa Kozareva and Eduard H. Hovy.
2010a.
Not allseeds are equal: Measuring the quality of text min-ing seeds.
In Proceedings of Human Language Tech-nologies: The 11th Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 618?626, Los Angeles, Cali-fornia, USA.Zornitsa Kozareva and Eduard H. Hovy.
2010b.
A semi-supervised method to learn and construct taxonomiesusing the web.
In Proceedings of Empirical Methodsin Natural Language Processing, pages 1110?1118,Cambridge, MA, USA.Roberto Navigli and Simone Paolo Ponzetto.
2012.
Ba-belNet: The automatic construction, evaluation andapplication of a wide-coverage multilingual semanticnetwork.
Artificial Intelligence, 193:217?250.Roberto Navigli and Paola Velardi.
2010.
LearningWord-Class Lattices for definition and hypernym ex-traction.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages1318?1327, Uppsala, Sweden.Roberto Navigli, Paola Velardi, and Stefano Faralli.2011.
A graph-based algorithm for inducing lexi-cal taxonomies from scratch.
In Proceedings of the22th International Joint Conference on Artificial Intel-ligence, pages 1872?1877, Barcelona, Spain.Marius Pas?ca, Dekang Lin, Jeffrey Bigham, Andrei Lif-chits, and Alpa Jain.
2006.
Names and similarities onthe Web: Fact extraction in the fast lane.
In Proceed-ings of the 21st International Conference on Computa-tional Linguistics and 44th Annual Meeting of the As-sociation for Computational Linguistics, pages 809?816, Sydney, Australia.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging Generic Patterns for Automatically Har-vesting Semantic Relations.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics (COLING-ACL), Syd-ney, Australia, pages 113?120, Sydney, Australia.Xuan-Hieu Phan, Le-Minh Nguyen, and SusumuHoriguchi.
2008.
Learning to classify short and sparsetext & web with hidden topics from large-scale datacollections.
In Proceedings of the 17th international180conference on World Wide Web, WWW ?08, pages 91?100, New York, NY, USA.Deepak Ravichandran and Eduard Hovy.
2002.
Learn-ing surface text patterns for a question answering sys-tem.
In Proceedings of the 40th Annual Meeting onAssociation for Computational Linguistics, pages 41?47, Philadelphia, PA, USA.Melanie Reiplinger, Ulrich Scha?fer, and Magdalena Wol-ska.
2012.
Extracting glossary sentences from schol-arly articles: A comparative evaluation of patternbootstrapping and deep analysis.
In Proceedings ofthe ACL-2012 Special Workshop on Rediscovering 50Years of Discoveries, pages 55?65, Jeju Island, Korea.Horacio Saggion.
2004.
Identifying definitions in textcollections for question answering.
In Proceedingsof the Fourth International Conference on LanguageResources and Evaluation, pages 1927?1930, Lisbon,Portugal.Mark Steyvers and Tom Griffiths, 2007.
ProbabilisticTopic Models.
Lawrence Erlbaum Associates.Paola Velardi, Roberto Navigli, and Pierluigi D?Amadio.2008.
Mining the web to create specialized glossaries.IEEE Intelligent Systems, 23(5):18?25.Paola Velardi, Stefano Faralli, and Roberto Navigli.2013.
OntoLearn Reloaded: A graph-based algorithmfor taxonomy induction.
Computational Linguistics,39(3):665?707.David Yarowsky.
1995.
Unsupervised Word Sense Dis-ambiguation rivaling supervised methods.
In Proceed-ings of the 33rd Annual Meeting of the Associationfor Computational Linguistics, pages 189?196, Cam-bridge, MA, USA.181
