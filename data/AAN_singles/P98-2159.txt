An Efficient Parallel Substrate for Typed Feature Structures onShared Memory Parallel MachinesNINOMIYA Takash i  t, TORISAWA Kentaro  t and TSUJ I I  Jun ' i ch i  t$tDepartment of Information ScienceGraduate School of Science, University of Tokyo*$CCL, UMIST, U.K.Abst rac tThis paper describes an efficient parallel systemfor processing Typed Feature Structures (TFSs)on shared-memory parallel machines.
We callthe system Parallel Substrate for TFS (PSTFS}.PSTFS is designed for parallel computing envi-ronments where a large number of agents areworking and communicating with each other.Such agents use PSTFS as their low-level mod-ule for solving constraints on TFSs and send-ing/receiving TFSs to/from other agents in anefficient manner.
From a programmers' pointof view, PSTFS provides a simple and unifiedmechanism for building high-level parallel NLPsystems.
The performance and the flexibility ofour PSTFS are shown through the experimentson two different ypes of parallel HPSG parsers.The speed-up was more than 10 times on bothparsers.1 In t roduct ionThe need for real-time NLP systems has beendiscussed for the last decade.
The difficulty inimplementing such a system is that people cannot use sophisticated but computationally ex-pensive methodologies.
However, if we couldprovide an efficient tool/environment for de-veloping parallel NLP systems, programmerswould have to be less concerned about the issuesrelated to efficiency of the system.
This becamepossible due to recent developments of parallelmachines with shared-memory architecture.We propose an efficient programming envi-ronment for developing parallel NLP systemson shared-memory parallel machines, called theParallel Substrate for Typed Feature Structures(PSTFS).
The environment is based on agent-based/object-oriented architecture.
In otherwords, a system based on PSTFS has manycomputational gents running on different pro-cessors in parallel; those agents communicatewith each other by using messages includingTFSs.
Tasks of the whole system, such as pars-* This research is partially founded by the project ofJSPS(JSPS-RFTF96P00502).Order~ !
Rep~Figure 1: Agent-based System with the PSTFSing or semantic processing, are divided into sev-eral pieces which can be simultaneously com-puted by several agents.Several parallel NLP systems have been de-veloped previously.
But most of them have beenneither efficient nor practical enough (Adriaensand Hahn, 1994).
On the other hand, ourPSTFS provides the following features.?
An efficient communication scheme formessages including Typed Feature Struc-tures (TFSs) (Carpenter, 1992).?
Efficient treatment of TFSs by an abstractmachine (Makino et al, 1998).Another possible way to develop arallel NLPsystems with TFSs is to use a full concurrentlogic programming language (Clark and Gre-gory, 1986; Ueda, 1985).
However, we have ob-served that it is necessary to control parallelismin a flexible way to achieve high-performance.
(Fixed concurrency in a logic programming lan-guage does not provide sufficient flexibility.
)Our agent-based architecture is suitable for ac-complishing such flexibility in parallelism.The next section discusses PSTFS from a pro-grammers' point of view.
Section 3 describesthe PSTFS architecture in detail.
Section 4 de-scribes the performance ofPSTFS on our HPSGparsers.968Const ra in t  So lver  Agentbegin-def init ionsme" \[FIRST F~a#~ \] na  ( \[.LAST Schlbevt J )", \[FIRST Joha~.
\ ] ,  namer \[LAST Bach J \]"concatenate .name(X,x  = \[FULL\[LAST~IRST ~12\[ \[\] j '  \]\['5\]) Y) iY = \[FIRST 1~\[LAST 212.Jend-def in l t lons(A) Description of CSAsAST Sch*beTt J 'IRST J oha .
.
\]AST Bach J '~'ULL \] g %T\]FULL (Joha*#, Bach) \]~FIRST J oha .
.
J\[LAST Bach(C) Values of F and RdeHne ?Gontrol Agent  #ame-concafe~a lor - ssbWhen a message s.1?e(z) arrives, do the followings,S := CSA ~ se lvo-csn J t ta int (concatenate_na~e(~,  ?
));return S;define #Gontrol Agent  %ome.comcafe~atorWhen a message selw arrives, do the followings,R := O;F := (CSA ~= selve-?oastr,int(name(?
)));i := O;forall z E F docreate  uarae-concat?~atoT'-Rmb age~| J~f i;N, ~= s*lve(x); i := i + 1;forellend for j := 0 to i doR := R U (Wai t - lor - resu l t ( J~f j ) ) ;forendreturn 77.;(B) Description of GAsFigure 2: Example concatenate_name2 Programmers '  V iewFrom a programmers" point of view, the PSTFSmechanism is quite simple and natural, whichis due to careful design for accomplishing high-performance and ease of progranmfing.Systems to be constructed on our PSTFS willinclude two different ypes of agents:?
Control Agents (CAs)?
Constraint Solver Agents (CSAs)As illustrated in Figure 1, CAs have overallcontrol of a system, including control of par-allelism, and they behave as masters of CSAs.CSAs modify TFSs according to the orders fromCAs.
Note that CAs can neither modify norgenerate TFSs by themselves.PSTFS has been implemented by combin-ing two existing programming languages: theconcurrent object-oriented programm, ng lan-guage ABCL/ f  (Taura, 1997) and the sequentialprogramming language LiLFeS (Makino et as.,1998).
CAs can be written in ABCL/f, whiledescription of CSAs can be mainly written inLiLFeS.Figure 2 shows an example of a part of thePSTFS code.
The task of this code is to con-catenate the first and the second name in agiven list.
One of the CAs is called name-concatenator.
This specific CA gathers pairs ofthe first and last name by asking a CSA with themessage solve-constraint( 'name(?)')
.
Whenthe CSA receives this message, the argument'name(?)'
is treated as a Prolog query inLiLFeS 1, according to the program of a CSA((A) of Figure 2).
There are several facts withthe predicate 'name'.
When the goal 'name(?
)'is processed by a CSA, all the possible answersdefined by these facts are returned.
The ob-tained pairs are stored in the variable F in thename-coneatenator ( (C) in  Figure 2).The next behavior of the name-eoncatenatoragent is to create CAs (name-concatenator-?F~) and to send the message so lve with ato each created CA running in parallel.The message contains one of the TFSs in F.Each name-concatenator-sub asks a CSA to con-catenate FIRST and LAST in a TFS.
Theneach CSA concatenates them using the defi-nite clause concatenate_name given in (A) ofFigure 2.
The result is returned to the name-concatenator-sub which had asked to do the job.Note that the name-concatenator-sub can askany of the existing CSAs.
All CSAs can basi-cally perform concatenation i  parallel and in-dependent way.
Then, the name-concatenatorwaits for the name-concatenator-sub to returnconcatenated names, and puts the return val-ues into the variable R.The CA name-concatenator c ntrols the over-all process.
It controls parallelism by creatingCAs and sending messages to them.
On tileother hand, all the operations on TFSs are per-formed by CSAs when they are asked by CAs.Suppose that one is trying to implement aparsing system based oil PSTFS.
The distinc-tion between CAs and CSAs roughly corre-sponds to the distinction between an abstractparsing schema nd application of phrase struc-ture rules.
Here, a parsing schema means ahigh-level description of a parsing algorithm inwhich the application of phrase structure rulesis regarded as an atomic operation or a sub-routine.
This distinction is a minor factor inwriting a sequential parser, but it has a majorimpact on a parallel environment.For instance, suppose that several distinctagents evoke applications of phrase structurerules against he same data simultaneously, andthe applications are accompanied with destruc-tive operations on the data.
This can cause ananomaly, since the agents will modify the orig.-inal data in unpredictable order and there isno way to keep consistency.
In order to avoidthis anomaly, one has to determine what is anatomic operation and provide a method to pre-vent the anomaly when atomic operations areevoked by several agents.
In our framework,any action taken by CSAs is viewed as suchan atomic operation and it is guaranteed thatno anomaly occurs even if CSAs concurrentlya LiLFeS supports definite clause programs, a TFSversion of Horn clauses.969Local Heapi~ii ii ~!~iiiiiiiiii: ~ .....Shared  Heap AreaPSTFSFigure 3: Inside of the PSTFSperform operations on the same data.
Thiscan be done by introducing copying of TFSs,which does not require any destructive opera-tions.
The details axe described in the next sec-tion.The other implication of the distinction be-tween CAs and CSAs is that this enables effi-cient communication between agents in a natu-ral way.
During parsing in HPSG, it is possiblethat TFSs with hundreds of nodes can be gen-erated.
Encoding such TFSs in a message andsending them in an efficient way are not triv-ial.
PSTFS provides a communication schemethat enables efficient sending/receiving of suchTFSs.
This becomes possible because of thedistinction of agents.
In other words, since CAscannot nmdify a TFS, CAs do not have to havea real image of TFSs.
When CSAs return theresults of computations to CAs, the CSAs sendonly an ID of a TFS.
Only when the ID is passedto other CSAs and they try to modify a TFSwith the ID, the actual transfer of the TFS'sreal image occurs.
Since the transfer is car-ried out only between CSAs, it can be directlyperformed using a low level representation ofTFSs used in CSAs in an efficient manner.
Notethat if CAs were to modify TFSs directly, thisscheme could not have been used.3 Arch i tec tureThis section explains the inner structure ofPSTFS focusing on the execution mechanism ofCSAs (See (Taura, 1997) for further detail onCAs).
A CSA is implemented by modifying theabstract machine for TFSs (i.e., LiAM), origi-nally designed for executing LiLFeS (Makino etal., 1998).The important constraint in designing the ex-ecution mechanism for CSAs is that TFSs gen-erated by CSAs must be kept unmodified.
Thisis because the TFSs must be used with severalagents in parallel.
If the TFS had been modi-fied by a CSA and if other agents did not knowthe fact, the expected results could not havebeen obtained.
Note that unification, which is(i) Copying from shared heapo" :ii ::i:.
iiiii:.i ~i:.iii~ i ::!~4~ii' .
::iii: ~:~~~i i  ~iii~ ~ ~!i i~l ~ i!~:~ :.Local HeapShared Heap:: :: ::.~ ~:i~ii i ?~ ii ii !
~ ::if i::i;ii~i /~::.:::.!!
ii i~ii~i~ : o .~.~ i~ii ii~i!!
i~ .....(ii) Computation on local heapR ~ ~ LocalHeapShared Heap(iii) Write resulting TFSs to shared heapLocal HeapFigure 4: Operation steps on PSTFSa major operation on TFSs, is a destructive op-eration, and modifications are likely to occurwhile executing CSAs.
Our execution mecha-nism handles this problem by letting CSAs copyTFSs generated by other CSAs at each time.Though this may not look like an efficient wayat first glance, it has been performed efficientlyby shared memory mechanisms and our copyingmethods.A CSA uses two different ypes of memoryareas as its heap:?
shared heap?
local heapA local heap is used for temporary operationsduring the computation inside a CSA.
A CSAcannot read/write local heap of other CSAs.
Ashared heap is used as a medium of commu-nication between CSAs, and it is realized ona shared memory.
When a CSA completes acomputation on TFSs, it writes the result ona shared heap.
Since the shared heap can beread by any CSAs, each CSA can read the re-sult performed by any other CSAs.
However,the portion of a shared heap that the CSA canwrite to is limited.
Any other CSA cannot writeon that portion.Next, we look at the steps performed by aCSA when it is asked by CAs with a message.970Note that the message only contains the IDs ofthe TFSs as described in the previous section.The IDs are realized as pointers on the sharedheap.1.
Copy TFSs pointed at by the IDs in themessage from the shared heap to the localheap of the CSA.
((i) in Figure 4.)2.
Process a query using LiAM and the localheap.
((ii) in Figure 4.)3.
If a query has an answer, the result iscopied to the portion of the shared heapwritable by the CSA.
Keep IDs on thecopied TFSs.
If there is no answer for thequery, go to Step 5.
((iii) in Figure 4.)4.
Evoke backtracking in LiAM and go to Step2.5.
Send the message, including the kept IDs,back to the CA that had asked the task.Note that, in step 3, the results of the compu-tation becomes readable by other CSAs.
Thisprocedure has the following desirable features.S imul taneous  Copy ing  An identical TFS ona shared heap can be copied by severalCSAs simultaneouslv.
This is due to ourshared memory mecilanism and the prop-erty of LiAM that copying does not haveany side-effect on TFSs 2.S imul taneous /Sa fe  Wr i t ing  CSAs canwrite on their own shared heap without thedanger of accidental modification by otherCSAs.Demand Dr iven  Copy ing  As described inthe previous section, the transfer of realimages of TFSs is performed only after theIDs of the TFSs reach to the CSAs requir-ing the TFSs.
Redundant copying/sendingof the TFSs' real image is reduced, and thetransfer is performed efficiently by mecha-nisms originally provided by LiAM.With efficient data transfer in shared-memorymachines, these features reduce the overhead ofparallelization.Note that copying in the procedures makesit possible to support non-determinism in NLPsystems.
For instance, during parsing, interme-diate parse trees must be kept.
In a chart pars-ing for a unification-based grammar, generated2Actually, this is not trivial.
Copying in Step 3 nor-malizes TFSs  and stores the TFSs  into a cont inuous re-gion on a shared heap.
TFSs stored in such a way canbe copied without  any side-effect.edges are kept untouched, and destructive oper-ations on the results must be done after copyingthem.
The copying of TFSs in the above stepsrealizes uch mechanisms in a natural way, as itis designed for efficient support for data sharingand destructive operations on shared heaps byparallel agents.4 App l i ca t ion  and  Per fo rmanceEva luat ionThis section describes two different types ofHPSG parsers implemented on PSTFS.
One isdesigned for our Japanese grammar and the al-gorithm is a parallel version of the CKY algo-rithm (Kasami, 1965).
The other is a parser foran ALE-style Grammar (Carpenter and Penn,1994).
The algorithms of both parsers are basedon parallel parsing algorithms for CFG (Ni-nomiya et al, 1997; Nijholt, 1994; Grishmanand Chitrao, 1988; Thompson, 1994).
Descrip-tions of both parsers are concise.
Both of themare written in less than 1,000 lines.
This showsthat our PSTFS can be easily used.
With thehigh performance of the parsers, this shows thefeasibility and flexibility of our PSTFS.For simplicity of discussion, we assume thatHPSG consists of lexical entries and ruleschemata.
Lexical entries can be regarded asTFSs assigned to each word.
A rule schema isa rule in the form of z --- abe.
.
,  where z. a. b. care TFSs.4.1 Paral le l  CKY-s ty le  HPSG Pars ingA lgor i thmA sequential CKY parser for CFG uses a datastructure called a triangular table.
Let Fi ~ de-note a cell in the triangular table.
Each cell Fi,~has a set of the non-terminal symbols in CF~that can generate the word sequence from thei + 1-th word to the j -th word in an input sen-tence.
The sequential CKY algorithm computeseach Fi,j according to a certain order.Our algorithm for a parallel CKY-style parserfor HPSG computes each Fi,j in parallel.
Notethat Fi,j contains TFSs covering the word se-quence from the i + 1-th word to the j-thword, not non-terminals.
We consider only therule schemata with a form of z ---* ab wherez,a,b are TFSs.
Parsing is started by a CAcalled PATCSCT?.
7)ATiSCT?
creates cell-agentsCi,j(O <_ i < j <_ n) and distributes them to pro-cessors on a parallel machine (Figure 5).
EachCi,j computes Fi,j in parallel.
More precisely,Ci,j(j - i = 1) looks up a dictionary and obtainslexical entries.
Ci,j( j  - i > 1) waits for the mes-sages including Fi,k and Fk,j for all k(i  < k < j )from other cell-agents.
When Ci,j receives Fi,kand Fk, j for an arbitrary k, Ci,j computes TFSsb~ appl3ing rule schemata to each members of971Figure 5: Correspondence b tween CKY matrixand agents: Ci,j correspond to the element of aCKY triangular matrixFi,k and Fkj.
The computed TFSs are consid-ered to be naothers of members of Fi,k and Fkjand they are added to Fi,j.
Note that these ap-plications of rule schemata re done in parallelin several CSAs 3.
Finally.
when computation ofFi (using Fi k and Fk j for all k(i < k < j))  iscompleted, Ci, d\]strlbutes Fi, to other agents ?
J .
.
3waiting for Fij.
Parsing \]s completed when thecomputation of F0 n is completed.We have done a series of experiments on ashared-memory parallel machine, SUN UltraEnterprise 10000 consisting of 64 nodes (eachnode is a 250 MHz UltraSparc) and 6 GByteshared memory.
The corpus consists of 879random sentences from the EDR Japanese cor-pus written in Japanese (average length of sen-tences is 20.8) 4 .
The grammar we used is anunderspecified Japanese HPSG grammar (Mit-suishi et al, 1998) consisting of 6 ID-schemataand 39 lexical entries (assigned to functionalwords) and 41 lexical-entry-templates (assignedto parts of speech)?
This grammar has wide cov-erage and high accuracy for real-world texts .Table 1 shows the result and comparison witha parser written in LiLFeS.
Figure 6 showsits speed-up.
From the Figure 6, we observethat the maximum speedup reaches up to 12.4times.
The average parsing time is 85 msec per3CSAs cannot be added dynamically in our imple-mentation.
So, to gain the maximum parallelism, weassigned a CSA to each processor.
Each Cij asks theCSA on the same processor to apply rule schemata.4We chose 1000 random sentences from the EDRJapanese corpus, and the used 897 sentences are all theparsable sentences by the grammar.5This grammar can generate parse trees for 82% of10000 sentences from the EDR Japanese corpus and thedependency accuracy is 78%.Number  oI I Av~ of Pars ing  T ime(msec)P rocessors  I PSTFS  I L i LFeS24820 13830 10640 9350 8560 135Table 1: Average parsing time per sentenceSpeed-up1412I086420,,I/.
/i i i i i10 20 30 40 50# of Wocessor lFigure 6: Speed-up of parsing time on parallelCKY-style HPSG parsersentence6?4.2 Char t -based  Para l le l  HPSGPars ing  A lgor i thm for ALEGrammarNext, we developed a parallel chart-basedHPSG parser for an ALE-style grammar.
Thealgorithm is based on a chart schema on whicheach agent throws active edges and inactiveedges containing a TFS.
When we regard therule schemata s a set of rewriting rules inCFG, this algorithm is exactly the same asthe Thompson's algorithm (Thompson, 1994)and similar to PAX (Matsumoto, 1987).
Themain difference between the chart-based parserand our CKY-style parser is that the ALE-styleparser supports a n-branching tree.A parsing process is started by a CA calledP.AT~S?T~.
It .creates word-position agents:Pk(0 < k < n), distributes them to parallelprocessors and waits for them to complete theirtasks.
The role of the word-position agent Pke Using 60 processors is worse than with 50 proces-sors.
In general, when the number of processes increasesto near or more than the number of existing processors,context switch between processes occurs frequently onshared-memory parallel machines (many people can usethe machines imultaneously).
We believe the cause forthe inefficiency when using 60 processors lies in such con-text switches.972~hort  Length  ~entences/ i  / k im beli .
.
.
.
.
andy to walka person whom he sees walkshe is seenhe persuades  her to walkDon n Length  ~entences( I )  e. person who sees  klm who sees sandy whom he tries tosee walks(2) a person who sees k im who sees sandy  who sees kim whomhe tries to see  walks(3) a person who sees k im who sees sandy  who sees k im whobel ieves  her  to  tend  to walk walksTable 2: Test corpus for parallel ALE-styleHPSG parser~hort  Length  ~entences"~umber  of Avg.
of Pars ing T lme(msec)Processors PSTFS  \] LiLFeS \] ALE10 16020 15630 12740 20550 14260 170~on~ Length  SentencesNumber  o\[ Avg.
of ParsinK T lme~msec)Processors  PSTFS  I LiLFeS l ALE110 1~013208 308~7 ~37U20 213930 177640 184150 190260 2052is to collect edges adjacent o the position k.A word-position agent has its own active edgesand inactive dges.
An active edge is in the form( i ,z --.
AoxB) ,  where A is a set of TFSs whichhave already been unified with an existing con-stituents, B is a set of TFSs which have notbeen unified yet, and x is the TFS which can beunified with the constituent in an inactive dgewhose left-side is in position k. Inactive edgesare in the form (k ,x , j ) ,  where k is  the left-sideposition of the constituent x and j is the right-side position of the constituent x.
That is, theset of all inactive edges whose left-side positionis k are collected by T'k.In our algorithm, ~k is always waiting for ei-ther an active edge or an inactive dge, and per-forms the following procedure when receiving anedge.?
When Pk receives an active edge (i,z - -A o xB),  7-)k preserve the edge and tries tofind the unifiable constituent with x fromthe set of inactive dges that :Pk has alreadyreceived.
If the unification succeeds, a newactive edge (i ,z ~ Ax o B) is created.
Ifthe dot in the new active edge reaches tothe end of RHS (i.e.
B = 0), a new inactiveedge is created and is sent to :Pi.
Otherwisethe new active edge is sent to :Pj.?
When Pk receives an inactive dge (k, x, j ) ,:Pk preserves the edge and tries to find theunifiable constituent on the right side ofthe dot from the set of active edges that:Pk has already received.
If the unificationsucceeds, a new active edge (i, z ---, Ax o B)is created.
If the dot in the new active edgereaches to the end of RHS (i.e.
B = 0), anew inactive edge is created and is sent to7:)i.
Otherwise the new active edge is sentto  ~Oj.As long as word-position-agents follow thesebehavior, they can run in parallel without anyother restriction.We have done a series of experiments in thesame machine settings as the experiments withTable 3: Average parsing time per sentenceSpeed-up1210!0 ~0 ,0 20  of P~ .
.
.
.
2 0 50 80Figure 7: Speed-up of parsing time on chart-based parallel HPSG parserthe CKY-style HPSG parser.
We measuredboth its speed up and real parsing time, andwe compared our parallel parser with the ALEsystem and a sequential parser on LiLFeS.
Thegrammar we used is a sample HPSG grammarattached to ALE system 7, which has 7 schemataand 62 lexical entries.
The test corpus weused in this experiment is shown in the Table2.
Results and comparison with other sequen-tial parsing systems are given in Table 3.
Itsspeedup is shown in Figure 7.
From the figure,we observe that the maximum speedup reachesup to 10.9 times and its parsing time is 1776msec per sentence.4.3 DiscussionIn both parsers, parsing time reaches a levelrequired by real-time applications, though weused computationally expensive grammar for-malisms, i.e.
HPSG with reasonable coverageand accuracy.
This shows the feasibility of our7This sample grammar is converted to LiLFeS stylehalf automatically.973P ~  ID40  - -3020100Processor  Status_=- - - - - - -  .=_....-_  - .
-=-- : _  _ - -  - - .
_ _  _ - -  .
~-.
: _ .
.
- -  .
.
- -  Swizch.~ : - _  _---.--.
: .
.
.~ .
.
.
.
- .
.o- - - - : -  - ~  : .
- - -  _:_.
|~.
.
m -I r I I616,12 616.14 616.16 616.18 (84C)Figure 8: Processors tatusframework for the goal to provide a parallel pro-gramming environment for real-time NLP.
Inaddition, our parallel HPSG parsers are con-siderably more efficient than other sequentialHPSG parsers.However, the speed-up is not proportional tothe number of processors.
We think that this isbecause the parallelism extracted in our parsingalgorithm is not enough.
Figure 8 shows the logof parsing Japanese sentences by the CKY-styleparser.
The black lines indicate when a proces-sor is busy.
One can see that many processorsare frequently idle.We think that this idle time does not sug-gest that parallel NLP systems are useless.
Onthe contrary, this suggest hat parallel NLP sys-tems have many possibilities.
If we introducesemantic processing for instance, overall pro-cessing time may not change because the idletime is used for semantic processing.
Anotherpossibility is the use of parallel NLP systems asa server.
Even if we feed several sentences at atime, throughput will not change, because theidle time is used for parsing different sentences.5 Conc lus ion  and  Future  WorkWe described PSTFS, a substrate for parallelprocessing of typed feature structures.
PSTFSserves as an efficient programming environmentfor implementing parallel NLP systems.
Wehave shown the feasibility and flexibility ofour PSTFS through the implementation f twoHPSG parsers.For the future, we are considering the use ofour HPSG parser on PSTFS for a speech recog-nition system, a Natural Language Interface orSpeech Machine Translation applications.R e f e r e n c e sAdriaens and Hahn, editors.
1994.
ParallelNatural Language Processing.
Ablex Publish-ing Corporation, New Jersey.Bob Carpenter and Gerald Penn.
1994.
ALE2.0 user's guide.
Technical report, CarnegieMellon University Laboratory for Computa-tional Linguistics, Pittsburgh, PA.Bob Carpenter.
1992.
The Logic of Typed Fea-ture Structures.
Cambridge University Press,Cambridge, England.K.
Clark and S. Gregory.
1986.
Parlog: Parallelprogramming in logic.
Journal of the A CMTransaction on Programming Languages andSyste ms, 8 ( 1):1-49.Ralph Grishman and Mehesh Chitrao.
1988.Evaluation of a parallel chart parser.
In Pro-ceedings of the second Conference on AppliedNatural Language Processing, pages 71-76.Association for Computational Linguistics.T.
Kasami.
1965.
An efficient recognition andsyntax algorithm for context-free languages.Technical Report AFCRL-65-758, Air ForceCambrige Research Lab., Bedford, Mass.Takaki Makino, Minoru Yoshida, Kentaro Tori-sawn, and Jun'ichi Tsujii.
1998.
LiLFeS- -  towards a practical HPSG parser.
InCOLING-A CL '98 Proceedings, August.Yuji Matsumoto.
1987.
A parallel parsing sys-tem for natural anguage analysis.
In Proceed-ings of 3rd International Conference on LogicProgramming, pages 396-409.Yutaka Mitsuishi, Kentaro Torisawa, andJun'ichi Tsujii.
1998.
HPSG-style underspec-ified Japanese grammar with wide coverage.In COLING-A CL'98 Proceedings, August.Anton Nijholt, 1994.
Parallel Natural LanguageProcessing, chapter Parallel Approaches toContext-Free Language Parsing, pages 135-167.
Ablex Publishing Corporation.Takashi Ninomiya, Kentaro Torisawa, KenjiroTaura, and Jun'ichi Tsujii.
1997.
A par-allel cky parsing algorithm on large-scaledistributed-memory parallel machines.
InPACLING '97, pages 223-231, September.Kenjiro Taura.
1997.
Efficient and ReusableImplementation of Fine-Grain Multithread-ing and Garbage Collection on Distributed-Memory Parallel Computers.
Ph.D. thesis,Department of Information Sciencethe, Uni-versity of Tokyo.Henry S. Thompson, 1994.
Parallel NaturalLanguage Processing, chapter Parallel Parsersfor Context-Free Grammars-Two Actual Im-plementations Comparesd, pages 168-187.Ablex Publishing Corporation.Kazunori Ueda.
1985.
Guarded horn clauses.Technical Report TR-103, ICOT.974
