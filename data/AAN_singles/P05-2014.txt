Proceedings of the ACL Student Research Workshop, pages 79?84,Ann Arbor, Michigan, June 2005. c?2005 Association for Computational LinguisticsDialogue Act Tagging for Instant Messaging Chat SessionsEdward IvanovicDepartment of Computer Science and Software EngineeringUniversity of MelbourneVictoria 3010, Australiaedwardi@csse.unimelb.edu.auAbstractInstant Messaging chat sessions are real-time text-based conversations which canbe analyzed using dialogue-act models.We describe a statistical approach formodelling and detecting dialogue acts inInstant Messaging dialogue.
This in-volved the collection of a small set oftask-based dialogues and annotating themwith a revised tag set.
We then dealt withsegmentation and synchronisation issueswhich do not arise in spoken dialogue.The model we developed combines naiveBayes and dialogue-act n-grams to obtainbetter than 80% accuracy in our taggingexperiment.1 IntroductionInstant Messaging (IM) dialogue has received rel-atively little attention in discourse modelling.
Thenovelty and popularity of IM dialogue and thesignificant differences between written and spokenEnglish warrant specific research on IM dialogue.We show that IM dialogue has some unique prob-lems and attributes not found in transcribed spokendialogue, which has been the focus of most work indiscourse modelling.
The present study addressesthe problems presented by these differences whenmodelling dialogue acts in IM dialogue.Stolcke et al (2000) point out that the use ofdialogue acts is a useful first level of analysis fordescribing discourse structure.
Dialogue acts arebased on the illocutionary force of an utterance fromspeech act theory, and represent acts such as asser-tions and declarations (Austin, 1962; Searle, 1979).This theory has been extended in dialogue acts tomodel the conversational functions that utterancescan perform.
Dialogue acts have been used to ben-efit tasks such as machine translation (Tanaka andYokoo, 1999) and the automatic detection of dia-logue games (Levin et al, 1999).
This deeper levelof discourse understanding may help replace or as-sist a support representative using IM dialogue bysuggesting responses that are more sophisticated andrealistic to a human dialogue participant.The unique problems and attributes exhibited byIM dialogue prohibit existing dialogue act classi-fication methods from being applied directly.
Wepresent solutions to some of these problems alongwith methods to obtain high accuracy in automateddialogue act classification.
A statistical discoursemodel is trained and then used to classify dialogueacts based on the observed words in an utterance.The training data are online conversations betweentwo people: a customer and a shopping assistant,which we collected and manually annotated.
Table 1shows a sample of the type of dialogue and discoursestructure used in this study.We begin by considering the preliminary issuesthat arise in IM dialogue, why they are problematicwhen modelling dialogue acts, and present their so-lutions in ?2.
With the preliminary problems solved,we investigate the dialogue act labelling task with adescription of our data in ?3.
The remainder of thepaper describes our experiment involving the train-ing of a naive Bayes model combined with a n-gramdiscourse model (?4).
The results of this model andevaluation statistics are presented in ?5.
?6 containsa discussion of the approach we used including itsstrengths, areas of improvement, and issues for fu-ture research followed by the conclusion in ?7.79Turn Msg Sec Speaker Message5 8 18 Customer [i was talking to mike and my browser crashed]U8:STATEMENT - [can you transfer me to himagain?
]U9:YES-NO-QUESTION5 9 7 Customer [he found a gift i wanted]U10:STATEMENT6 10 35 Sally [I will try my best to help you find the gift,]U11:STATEMENT [please let me know therequest]U12:REQUEST6 11 9 Sally [Mike is not available at this point of time]U13:STATEMENT7 12 1 Customer [but mike already found it]U14:STATEMENT [isn?t he there?
]U15:YES-NO-QUESTION8 13 8 Customer [it was a remote control car]U16:STATEMENT9 14 2 Sally [Mike is not available right now.
]U17:NO-ANSWER [I am here to assist you.
]U18:STATEMENT10 15 28 Sally [Sure Customer,]U19:RESPONSE-ACK [I will search for the remote control car.
]U20:STATEMENTTable 1: An example of unsynchronised messages occurring when a user prematurely assumes a turn isfinished.
Here, message (?Msg?)
12 is actually in response to 10, not 11 since turn 6 was sent as 2 messages:10 and 11.
We use the seconds elapsed (?Sec?)
since the previous message as part of a method to re-synchronise messages.
Utterance boundaries and their respective dialogue acts are denoted by Un.2 Issues in Instant Messaging DialogueThere are several differences between IM and tran-scribed spoken dialogue.
The dialogue act classifierdescribed in this paper is dependent on preprocess-ing tasks to resolve the issues discussed in this sec-tion.Sequences of words in textual dialogue aregrouped into three levels.
The first level is a Turn,consisting of at least one Message, which consistsof at least one Utterance, defined as follows:Turn: Dialogue participants normally take turnswriting.Message: A message is defined as a group of wordsthat are sent from one dialogue participant to theother as a single unit.
A single turn can span multi-ple messages, which sometimes leads to accidentalinterruptions as discussed in ?2.2.Utterance: This is the shortest unit we deal with andcan be thought of as one complete semantic unit?something that has a meaning.
This can be a com-plete sentence or as short as an emoticon (e.g.
?
:-)?to smile).Several lines from one of the dialogues in our cor-pus are shown as an example denoted with Turn,Message, and Utterance boundaries in Table 1.2.1 Utterance SegmentationBecause dialogue acts work at the utterance leveland users send messages which may contain morethan one utterance, we first need to segment the mes-sages by detecting utterance boundaries.
Messagesin our data were manually labelled with one or moredialogue act depending on the number of utteranceseach message contained.
Labelling in this fashionhad the effect of also segmenting messages into ut-terances based on the dialogue act boundaries.2.2 Synchronising Messages in IM DialogueThe end of a turn is not always obvious in typeddialogue.
Users often divide turns into multiplemessages, usually at clause or utterance boundaries,which can result in the end of a message being mis-taken as the end of that turn.
This ambiguity can leadto accidental turn interruptions which cause mes-sages to become unsynchronised.
In these caseseach participant tends to respond to an earlier mes-sage than the immediately previous one, making theconversation seem somewhat incoherent when readas a transcript.
An example of such a case is shownin Table 1 in which Customer replied to message 10with message 12 while Sally was still completingturn 6 with message 11.
If the resulting discourse isread sequentially it would seem that the customer ig-nored the information provided in message 11.
Thetime between messages shows that only 1 secondelapsed between messages 11 and 12, so message12 must in fact be in response to message 10.Message Mi is defined to be dependent on mes-sage Md if the user wrote Mi having already seenand presumably considered Md.
The importanceof unsynchronised messages is that they result inthe dialogue acts also being out of order, which is80problematic when using bigram or higher-order n-gram language models.
Therefore, messages arere-synchronised as described in ?3.2 before trainingand classification.3 The Dialogue Act Labelling TaskThe domain being modelled is the online shoppingassistance provided as part of the MSN Shoppingsite.
People are employed to provide live assistancevia an IM medium to potential customers who needhelp in finding items for purchase.
Several dialogueswere collected using this service, which were thenmanually labelled with dialogue acts and used totrain our statistical models.There were 3 aims of this task: 1) to obtain a re-alistic corpus; 2) to define a suitable set of dialogueact tags; and 3) to manually label the corpus usingthe dialogue act tag set, which is then used for train-ing the statistical models for automatic dialogue actclassification.3.1 Tag SetWe chose 12 tags by manually labelling the dialoguecorpus using tags that seemed appropriate from the42 tags used by Stolcke et al (2000) based on theDialog Act Markup in Several Layers (DAMSL) tagset (Core and Allen, 1997).
Some tags, such as UN-INTERPRETABLE and SELF-TALK, were eliminatedas they are not relevant for typed dialogue.
Tags thatwere difficult to distinguish, given the types of ut-terances in our corpus, were collapsed into one tag.For example, NO ANSWERS, REJECT, and NEGA-TIVE NON-NO ANSWERS are all represented by NO-ANSWER in our tag set.The Kappa statistic was used to compare inter-annotator agreement normalised for chance (Siegeland Castellan, 1988).
Labelling was carried outby three computational linguistics graduate studentswith 89% agreement resulting in a Kappa statistic of0.87, which is a satisfactory indication that our cor-pus can be labelled with high reliability using ourtag set (Carletta, 1996).A complete list of the 12 dialogue acts we used isshown in Table 2 along with examples and the fre-quency of each dialogue act in our corpus.Tag Example %STATEMENT I am sending you the page now 36.0THANKING Thank you for contacting us 14.7YES-NO-QUESTIONDid you receive the page?
13.9RESPONSE-ACK Sure 7.2REQUEST Please let me know how I canassist5.9OPEN-QUESTIONhow do I use the internationalversion?5.3YES-ANSWER yes, yeah 5.1CONVENTIONAL-CLOSINGBye Bye 2.9NO-ANSWER no, nope 2.5CONVENTIONAL-OPENINGHello Customer 2.3EXPRESSIVE haha, :-), grr 2.3DOWNPLAYER my pleasure 1.9Table 2: The 12 dialogue act labels with examplesand frequencies given as percentages of the totalnumber of utterances in our corpus.3.2 Re-synchronising MessagesThe typing rate is used to determine messagedependencies.
We calculate the typing rate bytime(Mi)?time(Md)length(Mi), which is the elapsed time be-tween two messages divided by the number of char-acters in Mi.
The dependent message Md may bethe immediately preceding message such that d =i ?
1 or any earlier message where 0 < d < i withthe first message being M1.
This algorithm is shownin Algorithm 1.Algorithm 1 Calculate message dependency formessage id?
irepeatd?
d?
1typing rate?
time(Mi)?time(Md)length(Mi)until typing rate < typing threshold or d = 1or speaker(Mi) = speaker(Md)The typing threshold in Algorithm 1 was calcu-lated by taking the 90th percentile of all observedtyping rates from approximately 300 messages thathad their dependent messages manually labelled re-sulting in a value of 5 characters per second.
Wefound that 20% of our messages were unsynchro-81nised, giving a baseline accuracy of automaticallydetecting message dependencies of 80% assumingthat Md = Mi?1.
Using the method described, weachieved a correct dependency detection accuracy of94.2%.4 Training on Speech ActsOur goal is to perform automatic dialogue act clas-sification of the current utterance given any previousutterances and their tags.
Given all available evi-dence E about a dialogue, the goal is to find thedialogue act sequence U with the highest posteriorprobability P (U |E) given that evidence.
To achievethis goal, we implemented a naive Bayes classifierusing bag-of-words feature representation such thatthe most probable dialogue act d?
given a bag-of-words input vector v?
is taken to be:d?
= argmaxd?DP (v?|d)P (d)P (v?
)(1)P (v?|d) ?n?j=1P (vj |d) (2)d?
= argmaxd?DP (d)n?j=1P (vj |d) (3)where vj is the jth element in v?, D denotes the set ofall dialogue acts and P (v?)
is constant for all d ?
D.The use of P (d) in Equation 3 assumes that dia-logue acts are independent of one another.
However,we intuitively know that if someone asks a YES-NO-QUESTION then the response is more likely to be aYES-ANSWER rather than, say, CONVENTIONAL-CLOSING.
This intuition is reflected in the bigramtransition probabilities obtained from our corpus.1To capture this dialogue act relationship wetrained standard n-gram models of dialogue act his-tory with add-one smoothing for the calculationof P (vj |d).
The bigram model uses the posteriorprobability P (d|H) rather than the prior probabilityP (d) in Equation 3, where H is the n-gram contextvector containing the previous dialogue act or previ-ous 2 dialogue acts in the case of the trigram model.1Due to space constraints, the dialogue act transition ta-ble has been omitted from this paper and is made available athttp://www.cs.mu.oz.au/?edwardi/papers/da transitions.htmlModel Min Max Mean Hit % PxBaseline ?
?
36.0% ?
?Likelihood 72.3% 90.5% 80.1% ?
?Unigram 74.7% 90.5% 80.6% 100 7.7Bigram 75.0% 92.4% 81.6% 97 4.7Trigram 69.5% 94.1% 80.9% 88 3.3Table 3: Mean accuracy of labelling utterances withdialogue acts using n-gram models.
Shown with hit-rate results and perplexities (?Px?
)5 Experimental ResultsEvaluation of the results was conducted via 9-foldcross-validation across the 9 dialogues in our cor-pus using 8 dialogues for training and 1 for testing.Table 3 shows the results of running the experimentwith various models replacing the prior probability,P (d), in Equation 3.
The Min, Max, and Meancolumns are obtained from the cross-validation tech-nique used for evaluation.
The baseline used for thistask was to assign the most frequently observed dia-logue act to each utterance, namely, STATEMENT.Omitting P (d) from Equation 3 such that onlythe likelihood (Equation 2) of the naive Bayes for-mula is used resulted in a mean accuracy of 80.1%.The high accuracy obtained with only the likelihoodreflects the high dependency between dialogue actsand the actual words used in utterances.
This de-pendency is represented well by the bag-of-wordsapproach.
Using P (d) to arrive at Equation 3 yieldsa slight increase in accuracy to 80.6%.The bigram model obtains the best result with81.6% accuracy.
This result is due to more accuratepredictions with P (d|H).
The trigram model pro-duced a slightly lower accuracy rate, partly due to alack of training data and to dialogue act adjacencypairs not being dependent on dialogue acts furtherremoved as discussed in ?4.In order to gauge the effectiveness of the bigramand trigram models in view of the small amount oftraining data, hit-rate statistics were collected duringtesting.
These statistics, presented in Table 3, showthe percentage of conditions that existed in the var-ious models.
Conditions that did not exist were notcounted in the accuracy measure during evaluation.The perplexities (Cover and Thomas, 1991) forthe various n-gram models we used are shown in82Table 3.
The biggest improvement, indicated by adecreased perplexity, comes when moving from theunigram to bigram models as expected.
However,the large difference between the bigram and trigrammodels is somewhat unexpected given the theory ofadjacency pairs.
This may be a result of insufficienttraining data as would be suggested by the lower tri-gram hit rate.6 Discussion and Future ResearchAs indicated by the Kappa statistics in ?3.1, la-belling utterances with dialogue acts can sometimesbe a subjective task.
Moreover, there are many pos-sible tag sets to choose from.
These two factorsmake it difficult to accurately compare various tag-ging methods and is one reason why Kappa statisticsand perplexity measures are useful.
The work pre-sented in this paper shows that using even the rel-atively simple bag-of-words approach with a naiveBayes classifier can produce very good results.One important area not tackled by this experimentwas that of utterance boundary detection.
Multipleutterances are often sent in one message, sometimesin one sentence, and each utterance must be tagged.Approximately 40% of the messages in our corpushave more than one utterance per message.
Utter-ances were manually marked in this experiment asthe study was focussed only on dialogue act classi-fication given a sequence of utterances.
It is rare,however, to be given text that is already segmentedinto utterances, so some work will be required toaccomplish this segmentation before automated di-alogue act tagging can commence.
Therefore, ut-terance boundary detection is an important area forfurther research.The methods used to detect dialogue acts pre-sented here do not take into account sentential struc-ture.
The sentences in (1) would thus be treatedequally with the bag-of-words approach.
(1) a. john has been to londonb.
has john been to londonWithout the punctuation (as is often the case with in-formal typed dialogue) the bag-of-words approachwill not differentiate the sentences, whereas if welook at the ordering of even the first two words wecan see that ?john has ...?
is likely to be a STATE-MENT whereas ?has john ...?
would be a question.
Itwould be interesting to research other types of fea-tures such as phrase structure or even looking at theorder of the first x words and the parts of speech ofan utterance to determine its dialogue act.Aspects of dialogue macrogame theory (DMT)(Mann, 2002) may help to increase tagging accu-racy.
In DMT, sets of utterances are grouped to-gether to form a game.
Games may be nested asin the following example:A: May I know the price range please?B: In which currency?A: $US pleaseB: 200?300Here, B has nested a clarification question whichwas required before providing the price range.
Thebigram model presented in this paper will incor-rectly capture this interaction as the sequence YES-NO-QUESTION, OPEN-QUESTION, STATEMENT,STATEMENT, whereas DMT would be able to ex-tract the nested question resulting in the correct pairsof question and answer sequences.Although other studies have attempted to auto-matically tag utterances with dialogue acts (Stolckeet al, 2000; Jurafsky et al, 1997; Kita et al, 1996) itis difficult to fairly compare results because the datawas significantly different (transcribed spoken dia-logue versus typed dialogue) and the dialogue actswere also different ranging from a set of 9 (Kita etal., 1996) to 42 (Stolcke et al, 2000).
It may be pos-sible to use a standard set of dialogue acts for a par-ticular domain, but inventing a set that could be usedfor all domains seems unlikely.
This is primarily dueto differing needs in various applications.
A super-set of dialogue acts that covers all domains wouldnecessarily be a large number of tags (at least the 42identified by Stolcke et al (2000)) with many tagsnot being appropriate for other domains.The best result from our dialogue act classifierwas obtained using a bigram discourse model result-ing in an average tagging accuracy of 81.6% (see Ta-ble 3).
Although this is higher than the results from13 recent studies presented by Stolcke et al (2000)with accuracy ranging from ?
40% to 81.2%, thetasks, data, and tag sets used were all quite different,so any comparison should be used as only a guide-line.837 ConclusionIn this paper, we have highlighted some unique char-acteristics in IM dialogue that are not found in tran-scribed spoken dialogue or other forms of writtendialogue such as e-mail; namely, utterance segmen-tation and message synchronisation.
We showed theproblem of unsynchronised messages can be readilysolved using a simple technique utilising the typing-rate and time stamps of messages.
We describeda method for high-accuracy dialogue act classifica-tion, which is an essential part for a deeper under-standing of dialogue.
In our experiments, the bi-gram model performed with the highest tagging ac-curacy which indicates that dialogue acts often oc-cur as adjacency pairs.
We also saw that the hightagging accuracy results obtained by the likelihoodfrom the naive Bayes model indicated the high cor-relation between the actual words and dialogue acts.The Kappa statistics we calculated indicate that ourtag set can be used reliably for annotation tasks.The increasing popularity of IM and automatedagent-based support services is ripe with new chal-lenges for research and development.
For example,IM provides the ability for an automated agent to askclarification questions.
Appropriate dialogue mod-elling will enable the automated agent to reliablydistinguish questions from statements.
More gener-ally, the rapidly expanding scope of online supportservices provides the impetus for IM dialogue sys-tems and discourse models to be developed further.Our findings have demonstrated the potential for di-alogue modelling for IM chat sessions, and opensthe way for a comprehensive investigation of thisnew application area.AcknowledgmentsWe thank Steven Bird, Timothy Baldwin, TrevorCohn, and the anonymous reviewers for their help-ful and constructive comments on this paper.
Wealso thank Vanessa Smith, Patrick Ye, and JeremyNicholson for annotating the data.ReferencesJohn L. Austin.
1962.
How to do Things with Words.Clarendon Press, Oxford.Jean Carletta.
1996.
Assessing agreement on classifica-tion tasks: the kappa statistic.
Computational Linguis-tics, 22(2):249?254.Mark Core and James Allen.
1997.
Coding dialogs withthe DAMSL annotation scheme.
Working Notes ofthe AAAI Fall Symposium on Communicative Actionin Humans and Machines, pages 28?35.Thomas M. Cover and Joy A. Thomas.
1991.
Elementsof Information Theory.
Wiley, New York.Daniel Jurafsky, Rebecca Bates, Noah Coccaro, RachelMartin, Marie Meteer, Klaus Ries, Elizabeth Shriberg,Andreas Stolcke, Paul Taylor, and Carol Van Ess-Dykema.
1997.
Automatic detection of discoursestructure for speech recognition and understanding.Proceedings of the 1997 IEEE Workshop on SpeechRecognition and Understanding, pages 88?95.Kenji Kita, Yoshikazu Fukui, Masaaki Nagata, andTsuyoshi Morimoto.
1996.
Automatic acquisitionof probabilistic dialogue models.
Proceedings of theFourth International Conference on Spoken Language,1:196?199.Lori Levin, Klaus Ries, Ann Thyme-Gobbel, and AlonLavie.
1999.
Tagging of speech acts and dialoguegames in spanish call home.
Towards Standards andTools for Discourse Tagging (Proceedings of the ACLWorkshop at ACL?99), pages 42?47.William Mann.
2002.
Dialogue macrogame theory.
Pro-ceedings of the 3rd SIGdial Workshop on Discourseand Dialogue, pages 129?141.John R. Searle.
1979.
Expression and Meaning: Studiesin the Theory of Speech Acts.
Cambridge UniversityPress, Cambridge, UK.Sidney Siegel and N. John Castellan, Jr. 1988.
Nonpara-metric statistics for the behavioral sciences.
McGraw-Hill, second edition.Andreas Stolcke, Noah Coccaro, Rebecca Bates, PaulTaylor, Carol Van Ess-Dykema, Klaus Ries, Eliza-beth Shriberg, Daniel Jurafsky, Rachel Martin, andMarie Meteer.
2000.
Dialogue act modeling forautomatic tagging and recognition of conversationalspeech.
Computational Linguistics, 26(3):339?373.Hideki Tanaka and Akio Yokoo.
1999.
An efficientstatistical speech act type tagging system for speechtranslation systems.
In Proceedings of the 37th con-ference on Association for Computational Linguistics,pages 381?388.
Association for Computational Lin-guistics.84
