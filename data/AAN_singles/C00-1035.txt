Aspects of Pattern-matching in Data-Oriented ParsingGuy De PauwCNTSUniversity of Antwert)Abst rac tData-Oriented Parsing (DOP) ranks mnong the best pars-ing schemes, pairing state-of-the art parsing accuracy tothe psycholinguistic insight that larger clmnks of syn-tactic structures are relevant grammatical and proba-bilistic units.
Parsing with the DOp-model~ however,seems to involve a lot of CPU cycles and a consider-able amomtt of double work, brought on by the conceptof multiple derivations, which is necessary for probabilis-tic processing, lint which is not convincingly related to aproper linguistic backbone.
It is however possible to re-interpret he poP-model as a pattern-matching model,which tries to maximize the size of the substructuresthat construct the parse, rather than the probability ofthe parse.
By emphasizing this memory-based aspect ofthe DoP-model, it is possible to do away with multiplederivations, opening up possibilities for efiqcient Viterbi-style optimizations, while still retaining acceptable pars-ing accuracy through enhanced context-sensitivity.1 I n t roduct ionThe machine learning paradigm of Memory-Based Learning, based on the assumpt ion  thatnew problems are solved by direct refbrence tostored experiences of previously solved prob-lems, has beest successfully applied to a numberof linguistic phenomena, such as part-of-speechtagging, NP-clmnking and stress acquisition(consult Daelemans (1999) for an overview).To solve these particular problems, linguisticinformation eeded to trigger the correct dis-ambiguation, is encoded in a linear featurevalue representation a d presented to a mem-ory based learner, such as TiMBL (Daelemanset al, 1999).Yet, many of the intricacies of the domain ofsyntax do not translate well to a linear repre-sentation, so that established MBL-methods arenecessarily limited to low-level syntactic analy-sis, like the atbrementioned NP-chunking task.Data Oriented Parsing (Bod, 1999), a state-of-the art natural language parsing system,translates very well to a Memory Based Learn-ing context.
This paper describes a re-interpretation of the soP-model, in which thepattern-match, infl aspects of tim model are ex-ploited, so that parses are analyzed by tryingto match a stew analysis to the largest possiblesubstructures recorded in memory.A short introduction to Data Oriented Pars-ing will be presented in Section 2, followed by anexplanation of the term pattern-matehin9 in thecontext of this paper.
Section 4 describes theexperimental setup and the corlms.
The parsingphase that precedes the disambiguation phasewill be outlined in Section 5 and a descriptionof the 3 disambiguating models, POFG, PMPGand the combined system PCFG@PMPG (:an befound in Sections 6, 7 and 8.2 Data  Or iented  Pars ingData Oriented Parsing, originally conceived byRemko Scha (Scha, 1990), has been successfullyapplied to syntactic natural language parsingby ll,ens Bod (1995), (1999).
The aim of DataOriented Parsing (henceforth DOP) is to developa per\[ormanee model of natural anguage, thatmodels language use rather than some type ofcompetence.
It adapts the psycholinguistic in-sight that language users analyze sentences us-ing previously registered constructions and thatnot only rewrite rules, but cornt)lete substruc-tures of any given depth cast be linguisticallyrelevant milts tbr parsing.2.1 ArehiteetureThe core of a DOP-system is its TREEBANK: anannotated corlms is used to induce all substruc-t, ures of arbitrary depth, together with their re-spective probabilities, which is a expressed by236SJ ~NP \111IPeterVPkilled NPi~ raccoo l lS NP \;PNP VP Peter killed NPNI'.~l raCCOOllFigure 1: Mult iple l)eriw~tionsits fl:equency in the TREEBANK relative to l;henuml)er of substructures with the Sanle root-node.Figure 1 shows the coral)|nation ol)eral;ionthat  is needed to tbrm the correct l)arse treefor the sentence Peter" killed a raccoon.
Given atreet)ank of substructures, the systcln tries tomatch the leftmost open nod(; of a substruc-ture |;hat is consistent with the parse tree, withthe top-node of another sul)structur(;, consistentwith the parse tree.Usually, ditferent conlt)inations of sul)struc-tllrO.s are possible, as is i~l(ti(:ated in Figure1: in the examl)le at the left-hand side thetree-structure (:an t)e built l)y (:o11111ining all S-structure wil;h a st)coiffed NP a.lld a flllly spe(:i-fled vp-structure.
The right example shows an-other possible Colnl)ination, where a parse tree is1)uilt t)y conll)ining the \]ninimal sut)s|;rltcl;ures.Nol;e that  t\]\]cse are (:(msisl;(mt wit\]l ol'dinaryrewrite-rules, such as s -+ NP VP.One t)artit:ul;~r 1)~trse tree may t;hus (:()\]lsist ()fseveral (lill.
(u'(ml; deriva, t io'n.s..To lind l;hc 1)rot) -al)ility (If ;I, (terivation, we lnultit)ly tim t)rot)a-1)ilities of the substructures thai; were used tol.
()rm the derivation.
To lind the t)robal)ility ofa parse, we must; in tlrilmit)le sum the t)rol)at)il-ities of all its deriw~tions.It is COlnl/utationally hardly tra(:tat)h; to COil-sider all deriw~tiolls t.()r each pars('.
SinceVITF, RBI ol)timization only su('(:ceds in findingthe most 1)robal)h'~ (teriw~tion as opposed to themost 1)robal)le l)arse, the MONTE CARLO al-gorithm is introduced as a proper al)proxima-tion I;hat randomly generates a large nlmfl)er ofderiw~tions.
The most prol)al/le l)arse is (:onsi(t-ered to be the parse that is most often observedin this derivation forest.2.2 Exper imenta l  Resu l t s  of  HOPThe basic 1)op-model, POP1, was testc,(t (111a manual ly edited version (if the ATIS-corlnlS(Marcus, Sant(lrini, and Marcinkiewicz, 199a).The syst;eln was trained on 603 Selltelmes (t)arl; -ofstmech tag sequelmes) and (;wfluated on a testset (if 75 SCld;ences.
Parse accuracy was used asan evahlation metric, expressing t;11(; percentageof sentences in the test set for which the tlarsel)rOl)osed by the system is COlnpletely identi-cal to the one in l;lle original eort)us, l)ifl'er-eat exl)erilnents were conducted in which max|-11111111 sul)structure size was varied.
Wi th  DoPl-lillfited to a sul)sl;ructure-size (If 1 (equiw~lenl;1;O a PCFG), t)arse accuracy is 47%.
hi the (/p-l;ima\] D()l'-mo(lel, in whi(:h sut)stru(:ture-siz(; is1lot limited, a 1)arse accuracy of 85% is (ll)-tni\]lc(t.2.3 Shor t  Assessment  o f  DOPDOI'I in its ot)tinlal fornl achieves a very highparse accuarcy.
The comt)utational costs of thesyste111, however, are equally high.
Bed (19951reported an average t/arse tilne of 3.5 hours 11(;1 .Sellte.n(:e. Even though (:urrent 1)arse tilne isrcl)ortc.d to l)e 11,or(; reasollal)le, tile oi)timalD()P algoril:lml in whi(:h n(/('onstr;dlts are madeon tll('~ size (1t' sut)structures, nlay not yet 1)etract;able for life-siz( ~.
COl'l)()ra.In a context-free grammar framework (con-sistent with \])()P l imited to a sutlstru(:tm:e-size(If 1), there is only (me way a t/arse tree cant)e t'ornmd (t'(/1: exalnl/le, the right hand side ofFigure \]), nleaning that  there is Olfly one del:iva-tioll for a given 1)arse tree.
This allows efficientVITEll.BI style Ol)tillfization.To elmo(le (:ontext-sellsitivity in the systeln,DOP is tbr(:ed to introduce multiple deriw~tiolls,so that repeatedly the same l)arse tree needs to1)e g(;lmrated, l)rillging at/(/ut a lot of COl l l \ ] ) l l ta , -tional overhead.Even though the use of larger syntactic coil-texts is highly relewmt fl'om a psycholinguisI,ict)oint-ofview, there is 11o explicit l)reference l)e-ing lnade t'(/1' larger substructures in the DOPnlodel.
While the MONTE CARLO optimizatiolxscheme nlaxinlizes the prot)ability of the (teriw>tions and seelns to 1)refer derivations nlade upof larger substructures, it; may 1)e ild;eresting to237Disambiguator Parse Accuracy (/562)PCFGPMPGI'CFG@PMP(I(a) Correct AnalysisSNP-SBJ VP,prpvbp  NPNP PP PPdt m~ in NP to NPI Il t I l  I) \ ]H I  t)% F \] Parse Accuracy oi* parsable sentences (/456) %373 66.4 83.0 I 373327 58.2 75.1 327402 71.5 85.2 402Table 1: Exl)erimental Results(b) PCFG-AnalysisSNP-SB J  VPIprpvbp NP PP PP81.871.788.2ell; mt in NP to NPI Ini lp nn  t)Figure 2: PCFG Error AnMysissee if we can make this assumption explicit.3 Pat tern -match ingWhen we look at natural anguage parsing fl:oma memory-based point of view, one might saythat a sentence is analyzed by looking u t) themost similar structure for the different analy-ses of that sentence in meinory.
The parsingsystem described in this paper tries to mimicthis 1)ehavior by interpreting the pop-model asa memory-t)ased model, in which analyses arebeing matched with syntactic patterns recordedin memory.
Similarity t)etween the proposedanalysis and tile patterns in memory is com-Imted according to:?
the number of patterns needed to constructa tree (to be minimized)?
the size of the patterns that are used toconstruct a tree (to be maximized)Tile nearest neighbor tbr a given analysis canbe defined as the derivation that shares thelargest amount of common nodes.4 The  exper imenta l  Setup10-tbld cross-validation was used to appropri-ately evaluate the algorithms, as tile dataset(see Section 4.1) is rather small.
Like DoPl thesystem is trained and tested on part-of-speechtag sequences.
In a first phase, a simple bottom-up chart parser, trained on the training parti-tions, was used to generate parse forests tbr the1)art-of speech tag sequences of the test parti-tion.
Next, the parse tbrests were sent to the 3algorithms (hencetbrth the disambiguators) toorder these parse forests, the first parse of theordered parse forest being the one proposed bythe disanfl)iguator.In this paper, 3 disambiguators are described:?
PCFG: siml)le Prol)abilistic Context-Free(~ra ln lnar?
PMPG:  the DOP approximation, Patten>Matching Probabilistic Grammar?
PCFGq-PMPG:  a combined system, inte-grating PCFG and PMPGThe evaluation metric used is pars(; accuracy,but also tile typical parser evaluation metric F-measure (precision/recall) is given ms a meansof reference to other systems.4.1 The CorpusThe ext)eriments were conducted oil all editedversion of tile ATIS-II-corpus (Marcus, San-torini, and Marcinkiewicz, 1993), which con-sists of 578 sentences.
Quite a lot of errors andinconsistencies were found, but not corrected,since we want our (probabilistic) system to be238l i l le to deal with this kind of noise.
Seman-ti(:ally oriented tlags like -TMP all(1 -Dill,, lllOSI;often used in conjmml;ion with l'p, have beenrenlove(t~ since l;here is no way of rel;rieving thiskind of semanti(: intbrmation from t;11(; t)art;-o5sl)ee(:h tags of the ATIS-(:ortms.
Synta(:ti(: flagslike -sILL on the other hand, \]lave 1)een main-taine(t. Internal relations (denoted by lllllllerictlags) were removed and tbr 1)ractical reasons,scntenee-lellgth was l imited 1;o 15 words max.The edited (:orl)us retained 562 sentences.5 Pars ingAs a first phase, a 1)ottom-ut) (:hart parseri)al"sed t;he test sol;.
This t)roved to t)e quitel)rol)lemati(:, since overall, 1()6 out of 562 sen-ten(:es (190/(0) could not 1)e t)arsed, (111(', to thesl,arsencss of the gramnmr, meanil,g I;ha(; l;heat)l)ropriate rewrite rule needed to (:onstru('l; the(:orre(:t t)~lrse tree tbr a senten(:c, in the test set,wasn't featured in the, in(tu(:ed grammar.
NP-annol ;at ; ion seem(~(t 1;o 1)(; t;lle lml, in (:aus(~ \]'or 11n-l)arsal)ility.
An NP like restriction code AP/57is repres(ml;ed 1)y the, rewrite rule:NP -~ NN NN sym sym sym C\]) CDHighly st)ccitt(: and tint stru(:tur(;s like theseare s(:ar(:e an(t are usually ll()t induced from thetraining set whell nee(h;d to parse the test set.On-going re, sear(:h tries 1;o iml)h;ln(ml; gl"am-mal;i(:a.1 SlnOothing ;ts :t soluti(m to |;his 1)rol)hml,but one might also (:onsid('a: genera.ling parsefol"eSi;S with an in(tep(mdent ~,;l"allllll;Ll', ilMu(:e(lfronl the entire (:orlms (training setq-t('~si;s(',l;) ora difl'erent corlms.
111 t)()th cases, however, wewould need to apply 1)robal)ilisti(" smoothing tobe al)le to assign t)rot)at)ilities to llllkllown s(;,l;llc-lures/rules.
Neither grammatical ,  nor t)rot)a-bilistic smoothing was imt)lemented in the (;ell-text of the exl)eriments, (les(:ril)ed in this 1)at)er.The sl/ars(mess of the grammar 1)roves t;o l)ea serious 1)otl;hme(:k fi)r pars(', a(:(:ura(:y, l imitingour (lisamlliguators t;o a maximuln tlarsc act:u-racy of 81%.6 PCFd-exper imentsa PCFG constru(:ts parse trees by using simplerewrite-rules.
The prot)al)ility of ~ parse tree(;~7tll })e (:omlml;ed l)y mull;it)lying the t)robat)ili-ties (1t" the.
rewrite-rules that w(~.re used to (:on-st;fuel; the t)ars(:.
Note that a l'CFd is i(h;nti(:altO DOP\]  whe l l  we l imit I;he maximum sul)Stl'UC-tures size to \], only Mlowing deriwd;ions of thetype found at the r ight-hand side of Figure 1.6.1 Exper imenta l  Resu l t sThe first line of Tat)le I shows the, rc, sull;s for thel'CF(~-(',xl)eriments: 66.4% parse accuracy is anadequate result for this baseline model.
We alsolook at l)arsc accuracy for parsable sentences(an estimal;e of the parse accuracy we 1nightget if we had a more suited parse forest gener-ator) and w(; notice that  we are able to a(:hievea 81.8% parse ae(:ur~my.
This is already quitehigh, trot on exmnining the parsed data, seriousand fluManmntal l imitations to the POPO-mo(lclcan be el)served6.2 Error Ana lys i sFigm'c 2, disl)lays the mosl; common tyl)c of mis-take mad(; l)y 1)CFG~S.
:\]'lit; (;orr0,cl; t )arse l;ree('ouht r(;i)res(mt an mlalysis for 1;11(; senten(:e:I ".
;ant o, fli.qht f rom \]h'us.scl.s to 2bronto.This examt)le shows thai; ~t PCFG h~ls a I;(~,n-dency to prctbr tlatter strueture, s over emt)edde, dstru(:t;ures.
This is a trivial effect of 1;11(; mathc-mat;it'll tbrmula used to conqml;e the t)rol)at)il -il;y of a I)arse-tr(;(;: emt/cdded structure requiremore r(;writ(' rules, adding more fat:tots to themultii)li(:ation , whi(:h will alm(/st ilw, vit~d)ly r(;-suit in :t lower l)rol)al)ilit;y.11; is all  1111J'()ri;llllal;e 1)r()I)(;rl;y of I'CFG~s t;hal;the mmfl)er of no(l(;s in the 1)atse tree is invers(~ly1)rot)ortiomd;e to il;s t)rol)al)ility.
()n(; might t)einclin(xl to n(n'malizc a parse tree's pr()bat)ilityrelative t(/the mnnt)er of nodes in the tree, but amore linguistically solmd alternative is at hand:the enhancenmnt of context sensii;ivity throughthe use of larger synl;tt(:ti(: (:ont(;xt; within t)arsetre(:s (:;/,11 make our disaml)iguat;or lnore rolmst.7 pMpo-exper imentsThe 1)att(;rn-Matching Prol)al)ilistie Gramnmris a memory-based interpretation of a \])OI'-model, in which a s(mtence is analyzed t)ymatching the largest, possible chunks of syn-t;acti(" strut:lure Oll the sentence.
To COml)ilet/~rse trees into pat, terns, all substructm'es illthe l;raining set are eneo(ted 1)y assigning l;hemspecific indexes, NP(o)345 e.g.
denotil~g a fullyspecified NP-sl;ruel;urc.
This apt)roa(:h was in-sl)ired 1)y Goodman (199(i), in which Goodman239unsuccessflflly uses a system of indexed parsetrees to transform DOP into aSl equivalent PCFG.The system of indexing (which is detailed in DePauw (2000)) used in tim experiments describedin this paper, is however specifically geared to-wards encoding contextual intbnnation i  parsetrees.Gives, an indexed training set, indexes canthen be matched on a test set parse tree in abottom-up fashion.
In the tbllowing example,boxed nodes indicate nodes that have been re-trieved from memory.Svpprp vbp \ [~dt nn iImlpIn this example we can see that an NP, con-sisting of a flflly specified embedded NP andl 'P, has l)een completely retrieved from men>ory, meaning that the NP in its entirety canbe observed in the training set.
However, novp was tbund that consists of a VBP and thatparticular NP.
Disambiguating with PMPG coil-sequently involves pruning all nodes retrievedfrolu i l lei l lory:SNP-SBJ VPvbp NPFinally, the probability for this pruned parsetree is computed in a pCFO-type manner, notadding the retrieved nodes to the product:P(parse) = P(s --+ NP-SBJ VP) .
P(vp --+ vb I) NP)7.1 Exper imenta l  Resu l tsThe results tbr the PMPG-exI)erinmnts can beibund on the second line of Table 1.
On somepartitions, PMPG pcrtbrmed insignificantly bet-ter than PCFG, but Table 1 shows that tile re-sults for the context sensitive scheme are muchworse.
58.2% overall parse accuracy and 71.7%parse accuracy on parsable sentences indicatesthat PMPG is *sot a valid approximation of DOP'Scontext-sensitivity.7.2 Error Analys isThe dramatic drop in parsing accuracy calls tbran error analysis of the parsed data.
Figure 3is a prototypical mistake PMPG has made.
Thecorrect analysis could represent a parse tree fora sentence like:What flights can I get f i rm Brussels to 2brvnto.The PMPG analysis would never have beenconsidered a likely candidate by a commonPCFG.
This particular sentence in fact was eftbrtlessly disambignated by the PCFG .
Yetthe fact that large chunks of tree-structure areretrieved Dora memory, make it the preferredparse for the PMPG.
We notice tbr instance thata large part of the sentence can be matchedon an SBAR structure, which has no relevancewhatsoever .Clearly, PMPG overestimates ubstructuresize as a feature for disambiguation.
It's inter-esting however to see that it is a working imple-mentation of context sensitivity, eagerly match-ing patterns from memory.
At the same time, ithas lost track of common-sense PCFG tactics, itis in the combination of the two that one mayfind a decent disambiguator and accurate im-plementation of context-sensitivity.8 A Combined  System (PMPG@PCFG)Table 1 showed that 81.8(/o of the time, a PCFGfinds the correct parse (Ibr t)arsable sentences),meaning that the correct parse is at the firstplace in the ordered parse tbrest.
99% of thetime, the correct parse can be tbund among the10 most probable parses in the ordered pars(;forest.
This opens up a myriad of possibili-ties tbr optin, ization.
One might for instanceuse a best-first strategy to generate only the 10best parses, significantly reducing parse and dis-ambiguation time.
An optimized isanNiguatormight theretbre include a preparatory phase inwtfich a common-sense PCFG retains the mostprobable parses, so that a nlore sophisticatedtbllow-up scheme ,teed not bother with sense-less analyses.In our experiments, we combined thecommon-sense logic of a PCFG and used itsoutput as the PMPG'8 input.
This is a well-established technique usually refi~rred to as sys-tent combination (see van Halteren, Zavrel, andDaelemans (1998) for an application of this240technique to I)art-ofst)ee('h tagging):II)CFGII, ,nos,: In'obable parso, s \[II'MI'(;I\]most In'obablc Imrse \[We art'.
also presented with th(', possibility toassign a weight to each algorithm's decision.The probability of a parse can the })e describedwith the following formula:I~/,(rewrito,-rule)iil)(\]m'""s'(O = (# non-inde, xed nodes),,The weight of ea(:h algorithm's (lc(:ision, aswell as the mnnt)er of 1HOSt )robM)h; parses thatm:e extrat)olated for the 1)attern-m~tt:hing al o-rithnq are parameters to 1)e optimized.
Futm:ework will include evaluation on a validation setto retrieve the ol)timal va, hles for these 1)aram-e, tcrs.8.1 Resu l tsThe third line in Tattle 1 shows that the com-1)ined system 1)ert'orlns better them either one,wit;h a parse accuracy of 71.5% and close I;o 90%1)~trs(; at:curacy on t)arsal)l(~ scnt(m(:es, whi(:h w(',(-nn consider an at)l)roximat;ion of results rc-porteA for DOP1.
Error annlysis shows thatthe combined system is ilMe, ed M)Ie to overt:ore(;difficulties of both Mgorithms.
The examtflo,in Figure 2 as well as the, ex~mlple in Figure3 were disanllfiguated correctly using the com-bined syst(;m9 Future  ResearchEven thoug\]l t\]le PMPG shows a lot of promisein its parse at:curacy, the following extensionsne, ed to be researched:Optimizing PMPG@PCFG for comtmta-tional etfieieney: the graph in Section 8shows a possible optimized parsing system,in which a pre-processing POF(I generatesthe n most likely candidates to 1)e extrap-olated tbr the actual disantbiguator.
Fullparse forests were generated for the exper-iments descrit)e,d in this paper, so that theefiiciency gain of such a system Calmot t)eprot)erly estimated.PMPG@PCFG as all approximation eeds tobe compm'ed to actual D()P~ by having DOPparse the data used in this experiment, andby having PMPG-I-I 'CFG parse the data usedin the exl)erilnents described in Bod (1999).The l)ottlelmck of the sparse grammar1)roblem prevents us from flflly exploitingthe disambiguating power of the pattern-matching algorithln.
The ORAEL-system(GRammar Adaptation, Evolution andLearning) that is currently being devel-olmd , tries to address the t)roblem of gram-matical spars(mess by using evolutionaryte(:lmiques to g('ncrate,, Ol)l;imizo, and com-l)lemeld, g~rallllllars.10 Conc lus ionsEven though l)()l'\] exhil)its outstanding pars-ing 1)eh~vior, the et|iciency of the model israthe, r problematic.
The introduction of mul-tit fie deriwd;ions causes a considerable amountof computational overhead.
Neither is it clearhow the concept of multiple deriwd;ions trans-lal;es to a t)sycholinguistic context: there is noproof thai; lmlguage users consider (titf'(;rcnt in-st~mtiations of th(; same parse, whmt decidingon the correct anMysis for a given sentence.A 1)M;tcrn-m~t:chil~g schcnm w~s 1)rcsenLcdthat tried to dis~mfl)iguate parse forests bytrying to maximize the size of the sul)strnc-tures that can 1)e retrie, ved from inoanory.This straightforward memory-based intert)rcta-tion yields sut)-standm'd parsing accuracy.
Butthe (:oml)ination of common-sense l)robal)ili-ties nnd enhanced context-sensitivity providesa workM)le t)arse forest disambiguator, indicat-ing that language users might exert a COml)lexcorot)libation of memory-based recollection tech-niques and stored statistical data to analyze ut-terances.ReferencesBod, R. 1995.
Enriching linguistics with stat:istics: Per-fornmnce models of natural anguage.
Dissertation,II,LC, Univcrsiteit wm Alnsterdanl.Bod, l{.ens.
1999.
Be, pond Grammar An E:rpericncc-Based ~lTu:ory of Language.
Cambridge, Fngland:Cambridge University Press.Daelcmans, W., J. Zavrcl, K. Van der Sloot, andA.
Van den Bosch.
19!)9.
TiMt3L: Tillmrg Memory241(a) Correct AnalysisSWHNP SQWHNP PP PPwdt mm xxx xxx /(b) PMPG Analysisvbp NP-SBJ VP'prpvb NP PP PPxxx in NP to NPI Innp mtpwdt fillsSNP-SBJ \ [~\ ]/ ~ nnp mlpxxxI NP-,XXX VIprpF igure  3: PMPG Er ror  Ana lys i sBased Learner, version 2.0, reference manual.
Tech-nical Report ILK-9901, ILK, Tilburg University.Daelemans, Walter.
1999.
Memory-based language pro-ccssing.
Journal for Ez'perimcntal nd Theoretical Ar-tificial Intelligence, 11:3:287 467.De Pauw, Guy.
2000.
Probabilistischc Parsers - Con-te~:tgcvocligheid cn Pattcrn-Matehin 9.
Antwerpen,Belgium: Antwerp Papers in Linguistics.Goodman, Joshua.
1996.
Efficient algorithms for parsingthe dop model.
In Proceedings of the Co@fence onEmpirical Methods in Natural Language Processing.pages 143 152.Marcus, M., B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large amlotatcd corpus of en-glish: The Petal Tl-eebank.
Computational Lingnis-ties, 19(2):313-330.Scha, R. 1990.
Taaltheorie en taaltectmologie: com-petence cn performance.
In Q.
A. M. dt Kortand G. L. J. Lcerdam, editors, Computcrtocpassin-.qcn in dc Nccrlandistick, LVVN-jaarboek.
LandelijkeVereniging van Ncerlandici.van Halteren, It., J. Zavrel, and W. Daclemans.
1998.Improving data-driven wordclass tagging by systemcombination.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguistics,Montr'eal, Quebec, Canada, pages 491-497, Montreal,Canada, August 10-14.242
