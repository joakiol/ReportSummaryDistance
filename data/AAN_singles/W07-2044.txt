Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 207?214,Prague, June 2007. c?2007 Association for Computational LinguisticsKU: Word Sense Disambiguation by SubstitutionDeniz YuretKoc?
UniversityIstanbul, Turkeydyuret@ku.edu.trAbstractData sparsity is one of the main factors thatmake word sense disambiguation (WSD)difficult.
To overcome this problem we needto find effective ways to use resources otherthan sense labeled data.
In this paper I de-scribe a WSD system that uses a statisticallanguage model based on a large unanno-tated corpus.
The model is used to evalu-ate the likelihood of various substitutes for aword in a given context.
These likelihoodsare then used to determine the best sense forthe word in novel contexts.
The resultingsystem participated in three tasks in the Se-mEval 2007 workshop.
The WSD of prepo-sitions task proved to be challenging for thesystem, possibly illustrating some of its lim-itations: e.g.
not all words have good sub-stitutes.
The system achieved promising re-sults for the English lexical sample and En-glish lexical substitution tasks.1 IntroductionA typical word sense disambiguation system istrained on a corpus of manually sense tagged text.Machine learning algorithms are then employed tofind the best sense for a word in a novel contextby generalizing from the training examples.
Thetraining data is costly to generate and inter-annotatoragreement is difficult to achieve.
Thus there is verylittle training data available: the largest single cor-pus of sense tagged text, SemCor, has 41,497 sensetagged words.
(Yuret, 2004) observed that approxi-mately half of the test instances do not match any ofthe contextual features learned from the training datafor an all words disambiguation task.
(Yarowsky andFlorian, 2002) found that each successive doublingof the training data only leads to a 3-4% error reduc-tion within their experimental range.Humans do not seem to be cursed with an expo-nential training data requirement to become profi-cient with the use of a word.
Dictionaries typicallycontain a definition and one or two examples of us-age for each sense.
This seems to be sufficient fora human to use the word correctly in contexts thatshare no surface features with the dictionary exam-ples.
The 108 waking seconds it takes a person tobecome proficient in a language does not seem suf-ficient to master all the words and their differentsenses.
We need models that do not require largeamounts of annotated text to perform WSD.What possible process can explain our proficiencywithout relying on a lot of labeled data?
Let us lookat a concrete example: The two most frequent sensesof the word ?board?
according to WordNet 3.0 (Fell-baum, 1998) are the ?committee?
sense, and the?plank?
sense.
When we hear a sentence like ?Therewas a board meeting?, it is immediately obvious thatthe first sense is intended.
One hypothesis is that acommon sense inference engine in your brain rulesout the second sense.
Maybe you visualize piecesof timber sitting around a meeting table and decidethat it is absurd.
Another hypothesis is that the planksense does not even occur to you because you hearthis sentence in the middle of a conversation aboutcorporate matters.
Therefore the plank sense is notpsychologically ?primed?.
Finally, maybe you sub-consciously perform a substitution and the sentence207?There was a plank meeting?
just sounds bad to yourlinguistic ?ear?.In this paper I will describe a system that judgespotential substitutions in a given context using a sta-tistical language model as a surrogate for the linguis-tic ?ear?.
The likelihoods of the various substitutesare used to select the best sense for a target word.The use of substitutes for WSD is not new.
(Lea-cock et al, 1998) demonstrated the use of relatedmonosemous words (monosemous relatives) to col-lect examples for a given sense from the Internet.
(Mihalcea, 2002) used the monosemous relativestechnique for bootstrapping the automatic acquisi-tion of large sense tagged corpora.
In both cases, thefocus was on collecting more labeled examples to besubsequently used with supervised machine learn-ing techniques.
(Martinez et al, 2006) extended themethod to make use of polysemous relatives.
Moreimportantly, their method places these relatives inthe context of the target word to query a search en-gine and uses the search results to predict the bestsense in an unsupervised manner.There are three areas that distinguish my systemfrom the previous work: (i) The probabilities forsubstitutes in context are determined using a statisti-cal language model rather than search hits on heuris-tically constructed queries, (ii) The set of substitutesare derived from multiple sources and optimized us-ing WSD performance as the objective function, and(iii) A probabilistic generative model is used to se-lect the best sense rather than typical machine learn-ing algorithms or heuristics.
Each of these areas isexplained further below.Probabilities for substitutes: Statistical languagemodeling is the art of determining the probability ofa sequence of words.
According to the model usedin this study, the sentence ?There was a committeemeeting?
is 17,629 times more likely than the sen-tence ?There was a plank meeting?.
Thus, a statis-tical language model can be used as a surrogate foryour inner ear that decides what sounds good andwhat sounds bad.
I used a language model based onthe Web 1T 5-gram dataset (Brants and Franz, 2006)which gives the counts of 1 to 5-grams in a web cor-pus of 1012 words.
The details of the Web1T modelare given in the Appendix.Given that I criticize existing WSD algorithms forusing too much data, it might seem hypocritical toemploy a data source with 1012 words.
In my de-fense, from an engineering perspective, an unanno-tated 1012 word corpus exists, whereas large sensetagged corpora do not.
From a scientific perspective,it is clear that no human ever comes close to expe-riencing 1012 words, but they do outperform simplen-gram language models based on that much data inpredicting the likelihood of words in novel contexts(Shannon, 1951).
So, even though we do not knowhow humans do it, we do know that they have theequivalent of a powerful statistical language modelin their heads.Selecting the best substitutes: Perhaps more im-portant for the performance of the system is the deci-sion of which substitutes to try.
We never thought ofusing ?monkey?
as a potential substitute for ?board?.One possibility is to use the synonyms in Word-Net which were selected such that they can be in-terchanged in at least some contexts.
However 54%of WordNet synsets do not have any synonyms.
Be-sides, synonymous words would not always help ifthey share similar ambiguities in meaning.
Substi-tutes that are not synonyms, on the other hand, maybe very useful such as ?hot?
vs. ?cold?
or ?car?vs.
?truck?.
In general we are looking for potentialsubstitutes that have a high likelihood of appearingin contexts that are associated with a specific senseof the target word.
The substitute selection methodused in this work is described in Section 3.Selecting the best sense: Once we have a lan-guage model and a set of substitutes to try, we needa decision procedure that picks the best sense of aword in a given context.
An unsupervised systemcan be designed to keep track of the sense associ-ated with each substitute based on the lexical re-source used.
However since I used multiple lexicalresources, and had training data available, I chose asupervised approach.
For each instance in the train-ing set, the likelihood of each substitute is deter-mined.
Then instances of a single sense are groupedtogether to yield a probability distribution over thesubstitutes for that sense.
When a test instance isencountered its substitute distribution is comparedto that of each sense to select the most appropriateone.
Section 2 describes the sense selection proce-dure in detail.208We could say each context is represented withthe likelihood it assigns to various substitutes ratherthan its surface features.
That way contexts that donot share any surface features can be related to eachother.Results: To summarize the results, in the WordSense Disambiguation of Prepositions Task, the sys-tem achieved 54.7% accuracy1 .
This is 15.1% abovethe baseline of picking the most frequent sensebut 14.6% below the best system.
In the CoarseGrained English Lexical Sample WSD Task, the sys-tem achieved 85.1% accuracy, which is 6.4% abovethe baseline of picking the most frequent sense and3.6% below the best system.
Finally, in the EnglishLexical Substitution Task, the system achieved thetop result for picking the best substitute for eachword.2 Sense Selection ProcedureConsider a target word w0 with n senses S ={s1, .
.
.
, sn}.
Let Cj = {cj1, cj2, .
.
.}
be the setof contexts in the training data where w0 has beentagged with sense sj .
The prior probability of asense sj will be defined as:P (sj) =|Cj |?nk=1 |Ck|Suppose we decide to use m substitutes W ={w1, .
.
.
, wm}.
The selection of the possible sub-stitutes is discussed in Section 3.
Let P (wi, c) de-note the probability of the context c where the targetword has been replaced with wi.
This probability isobtained from the Web1T language model.
The con-ditional probability of a substitute wi in a particularcontext c is defined as:P (wi|c) =P (wi, c)?w?W P (w, c)The conditional probability of a substitute wi fora particular sense sj is defined as:P (wi|sj) =1|Cj |?c?CjP (wi|c)1In all the tasks participated, the system submitted a uniqueanswer for each instance.
Therefore precision, recall, F-measure, and accuracy have the same value.
I will use the termaccuracy to represent them all.Given a test context ct, we would like to find outwhich sense sj it is most likely to represent:argmaxj P (sj |ct) ?
P (ct|sj)P (sj)To calculate the likelihood of the test contextP (ct|sj), we first find the conditional probabilitydistribution of the substitutes P (wi|ct), as describedabove.
Treating these probabilities as fractionalcounts we can express the likelihood as:P (ct|sj) ?
?w?WP (w|sj)P (w|ct)Thus we choose the sense that maximizes the pos-terior probability:argmaxjP (sj)?w?WP (w|sj)P (w|ct)3 Substitute Selection ProcedurePotential substitutes for a word were selected fromWordNet 3.0 (Fellbaum, 1998), and the Roget The-saurus (Thesaurus.com, 2007).When selecting the WordNet substitutes, the pro-gram considered all synsets of the target word andneighboring synsets accessible following a singlelink.
All words contained within these synsets andtheir glosses were considered as potential substi-tutes.When selecting the Roget substitutes, the programconsidered all entries that included the target word.By default, the entries that included the target wordas part of a multi word phrase and entries that hadthe wrong part of speech were excluded.I observed that the particular set of substitutesused had a large impact on the disambiguation per-formance in cross validation.
Therefore I spent aconsiderable amount of effort trying to optimize thesubstitute sets.
The union of the WordNet and Ro-get substitutes were first sorted based on their dis-criminative power measured by the likelihood ratioof their best sense:LR(wi) = maxjP (wi|sj)P (wi|sj)The following optimization algorithms were thenrun to maximize the leave-one-out cross validation(loocv) accuracy on the lexical sample WSD train-ing data.2091.
Each substitute was temporarily deleted and theresulting gain in loocv was noted.
The sub-stitute that led to the highest gain was perma-nently deleted.
The procedure was repeated un-til no further loocv gain was possible.2.
Each pair of substitutes were tried alone andthe pair that gave the highest loocv score waschosen as the initial list.
Other substitutes werethen greedily added to this list until no furtherloocv gain was possible.3.
Golden section search was used to find the idealcutoff point in the list of substitutes sorted bylikelihood ratio.
Substitutes below the cutoffpoint were deleted.None of these algorithms consistently gave thebest result.
Thus, each algorithm was run for eachtarget word and the substitute set that gave the bestloocv result was used for the final testing.
The loocvgain from using the optimized substitute sets insteadof the initial union of WordNet and Roget substi-tutes was significant.
For example the average gainwas 9.4% and the maximum was 38% for the En-glish Lexical Sample WSD task.4 English Lexical SubstitutionThe English Lexical Substitution Task (McCarthyand Navigli, 2007), for both human annotators andsystems is to replace a target word in a sentence withas close a word as possible.
It is different from thestandard WSD tasks in that there is no sense repos-itory used, and even the identification of a discretesense is not necessary.The task used a lexical sample of 171 words with10 instances each.
For each instance the humanannotators selected several substitutes.
There werethree subtasks: best: scoring the best substitute fora given item, oot: scoring the best ten substitutes fora given item, and mw: detection and identificationof multi-words.
The details of the subtasks and scor-ing can be found in (McCarthy and Navigli, 2007).My system participated in the first two subtasks.Because there is no training set, the supervisedoptimization of the substitute set using the algo-rithms described in Section 3 is not applicable.Based on the trial data, I found that the Roget substi-tutes work better than the WordNet substitutes mostBEST P R Mode P Mode Rall 12.90 12.90 20.65 20.65Further AnalysisNMWT 13.39 13.39 21.20 21.20NMWS 14.33 13.98 21.88 21.42RAND 12.67 12.67 20.34 20.34MAN 13.16 13.16 21.01 21.01OOT P R Mode P Mode Rall 46.15 46.15 61.30 61.30Further AnalysisNMWT 48.43 48.43 63.42 63.42NMWS 49.72 49.72 63.74 63.74RAND 47.80 47.80 62.84 62.84MAN 44.23 44.23 59.55 59.55Table 1: BEST and OOT results: P is precision, Ris recall, Mode indicates accuracy selecting the sin-gle preferred substitute when there is one, NMWTis the score without items identified as multi-words,NMWS is the score using only single word substi-tutes, RAND is the score for the items selected ran-domly, and MAN is the score for the items selectedmanually.of the time.
The antonyms in each entry and theentries that did not have the target word as the headwere filtered out to improve the accuracy.
Antonymshappen to be good substitutes for WSD, but not sogood for lexical substitution.For the final output of the system, the substituteswi in a context c were simply sorted by P (wi, c)which is calculated based on the Web1T languagemodel.In the best subtask the system achieved 12.9% ac-curacy, which is the top score and 2.95% above thebaseline.
The system was able to find the mode (asingle substitute preferred to the others by the anno-tators) in 20.65% of the cases when there was one,which is 5.37% above the baseline and 0.08% be-low the top score.
The top part of Table 1 givesthe breakdown of the best score, see (McCarthy andNavigli, 2007) for details.The low numbers here are partly a consequence ofthe scoring formula used.
Specifically, the score fora single item is bounded by the frequency of the bestsubstitute in the gold standard file.
Therefore, the210-0.4-0.200.20.40.60.810  100  1000  10000AccuracyabovebaselineNumber of training instancesFigure 1: Training set size vs. accuracy above base-line for the English lexical sample task.highest achievable score was not 100%, but 45.76%.A more intuitive way to look at the result may bethe following: Human annotators assigned 4.04 dis-tinct substitutes for each instance on average, andmy system was able to guess one of these as the bestin 33.73% of the cases.In the oot subtask the system achieved 46.15%accuracy, which is 16.45% above the baseline and22.88% below the top result.
The system was able tofind the mode as one of its 10 guesses in 61.30% ofthe cases when there was a mode, which is 20.73%above the best baseline and 4.96% below the topscore.
Unlike the best scores, 100% accuracy is pos-sible for oot.
Each item had 1 to 9 distinct substi-tutes in the gold standard, so an ideal system couldpotentially cover them all with 10 guesses.
The sec-ond part of Table 1 gives the breakdown of the ootscore.In conclusion, selecting substitutes based on astandard repository like Roget and ranking them us-ing the ngram language model gives a good base-line for this task.
To improve the performance alongthese lines we need better language models, and bet-ter substitute selection procedures.
Even the bestlanguage model will only tell us which words aremost likely to replace our target word, not whichones preserve the meaning.
Relying on reposito-ries like Roget for the purpose of substitute selectionseems ad-hoc and better methods are needed.5 English Lexical Sample WSDThe Coarse-Grained English Lexical Sample WSDTask (Palmer et al, 2007), provided training andtest data for sense disambiguation of 65 verbs and35 nouns.
On average there were 223 training and49 testing instances for each word tagged with anOntoNote sense tag (Hovy et al, 2006).
OntoNotesense tags are groupings of WordNet senses thatare more coarse-grained than traditional WN entries,and which have achieved on average 90% inter-annotator agreement.
The number of senses for aword ranged from 1 to 13 with an average of 3.6.I used substitute sets optimized for each word asdescribed in Section 3.
Then a single best sense foreach test instance was selected based on the modelgiven in Section 2.
The system achieved 85.05% ac-curacy, which is 6.39% above the baseline of pick-ing the most frequent sense and 3.65% below the topscore.These numbers seem higher than previous Sen-seval lexical sample tasks.
The best system inSenseval-3 (Mihalcea et al, 2004; Grozea, 2004)achieved 72.9% fine grained, 79.3% coarse grainedaccuracy.
Many factors may have played a role butthe most important one is probably the sense inven-tory.
The nouns and verbs in Senseval-3 had 6.1 finegrained and 4.5 coarse grained senses on average.The leave-one-out cross-validation result of mysystem on the training set was 83.21% with the un-filtered union of Roget and WordNet substitutes, and90.69% with the optimized subset.
Clearly there issome over-fitting in the substitute optimization pro-cess which needs to be improved.Table 2 details the performance on individualwords.
The accuracy is 88.67% on the nouns and81.02% on the verbs.
One can clearly see the rela-tion of the performance with the number of senses(decreasing) and the frequency of the first sense (in-creasing).
Interestingly no clear relation exists be-tween the training set size and the accuracy abovethe baseline.
Figure 1 plots the relationship betweentraining set size vs. the accuracy gain above the mostfrequent sense baseline.
This could indicate that thesystem peaks at a low training set size and general-izes well because of the language model.
However,it should be noted that each point in the plot rep-resents a different word, not experiments with the211same word at different training set sizes.
Thus thedifficulty of each word may be the overriding factorin determining performance.
A more detailed studysimilar to (Yarowsky and Florian, 2002) is needed toexplore the relationship in more detail.6 WSD of PrepositionsThe Word Sense Disambiguation of PrepositionsTask (Litkowski and Hargraves, 2007), providedtraining and test data for sense disambiguation of34 prepositions.
On average there were 486 train-ing and 234 test instances for each preposition.
Thenumber of senses for a word ranged from 1 to 20with an average of 7.4.The system described in Sections 2 and 3 wereapplied to this task as well.
WordNet does not haveinformation about prepositions, so most of the can-didate substitutes were obtained from Roget and ThePreposition Project (Litkowski, 2005).
After opti-mizing the substitute sets the system achieved 54.7%accuracy which is 15.1% above the most frequentsense baseline and 14.6% below the top result.
Un-fortunately there were only three teams that partic-ipated in this task.
The detailed breakdown of theresults can be seen in the second part of Table 2.The loocv result on the training data with the ini-tial unfiltered set of substitutes was 51.70%.
Opti-mizations described in Section 3 increased this to59.71%.
This increase is comparable to the onein the lexical substitution task.
The final result of54.7% shows signs of overfitting in the substitute se-lection process.The average gain above the baseline for preposi-tions (39.6% to 54.7%) is significantly higher thanthe English lexical sample task (78.7% to 85.1%).However the preposition numbers are generallylower compared to the nouns and verbs because theyare more ambiguous: the number of senses is higherand the first sense frequency is lower.Good quality substitutes are difficult to find forprepositions.
Unlike common nouns and verbs,common prepositions play unique roles in languageand are difficult to replace.
Open class words havesynonyms, hypernyms, antonyms etc.
that providegood substitutes: it is easy to come up with ?I atehalibut?
when you see ?I ate fish?.
It is not as easyto replace ?of?
in the phrase ?the president of thecompany?.
Even when there is a good substitute,e.g.
?over?
vs.
?under?, the two prepositions usuallyshare the exact same ambiguities: they can both ex-press a physical direction or a quantity comparison.Therefore the substitution based model presented inthis work may not be a good match for prepositiondisambiguation.7 Contributions and Future WorkA WSD method employing a statistical languagemodel was introduced.
The language model is usedto evaluate the likelihood of possible substitutes forthe target word in a given context.
Each context isrepresented with its preferences for possible substi-tutes, thus contexts with no surface features in com-mon can nevertheless be related to each other.The set of substitutes used for a word had a largeeffect on the performance of the resulting system.
Asubstitute selection procedure that uses the languagemodel itself rather than external lexical resourcesmay work better.I hypothesize that the model would be advanta-geous on tasks like ?all words?
WSD, where datasparseness is paramount, because it is able to linkcontexts with no surface features in common.
It canbe used in an unsupervised manner where the sub-stitutes and their associated senses can be obtainedfrom a lexical resource.
Work along these lines wasnot completed due to time limitations.Finally, there are two failure modes for the algo-rithm: either there are no good substitutes that dif-ferentiate the various senses (as I suspect is the casefor some prepositions), or the language model doesnot yield accurate preferences among the substitutesthat correspond to our intuition.
In the first case wehave to fall back on other methods, as the substi-tutes obviously are of limited value.
The correspon-dence between the language model and our intuitionrequires further study.Appendix: Web1T Language ModelThe Web 1T 5-gram dataset (Brants and Franz,2006) that was used to build a language model forthis work consists of the counts of word sequencesup to length 5 in a 1012 word corpus derived fromthe Web.
The data consists of mostly English wordsthat have been tokenized and sentence tagged.
To-212kens that appear less than 200 times and ngrams thatappear less than 40 times have been filtered out.I used a smoothing method loosely based on theone-count method given in (Chen and Goodman,1996).
Because ngrams with low counts are not in-cluded in the data I used ngrams with missing countsinstead of ngrams with one counts.
The missingcount is defined as:m(wi?1i?n+1) = c(wi?1i?n+1) ?
?wic(wii?n+1)where wii?n+1 indicates the n-word sequence end-ing with wi, and c(wii?n+1) is the count of this se-quence.
The corresponding smoothing formula is:P (wi|wi?1i?n+1) =c(wii?n+1) + (1 + ?n)m(wi?1i?n+1)P (wi|wi?1i?n+2)c(wi?1i?n+1) + ?nm(wi?1i?n+1)The parameters ?n > 0 for n = 2 .
.
.
5 was opti-mized on the Brown corpus to yield a cross entropyof 8.06 bits per token.
The optimized parameters aregiven below:?2 = 6.71, ?3 = 5.94, ?4 = 6.55, ?5 = 5.71ReferencesThorsten Brants and Alex Franz.
2006.
Web 1T 5-gramversion 1.
Linguistic Data Consortium, Philadelphia.LDC2006T13.Stanley F. Chen and Joshua Goodman.
1996.
An empir-ical study of smoothing techniques for language mod-eling.
In Proceedings of the 34th Annual Meeting ofthe ACL.Christiane Fellbaum, editor.
1998.
Wordnet: An Elec-tronic Lexical Database.
MIT Press.Cristian Grozea.
2004.
Finding optimal parameter set-tings for high performance word sense disambigua-tion.
In Proceedings of Senseval-3: The Third Inter-national Workshop on the Evaluation of Systems forthe Semantic Analysis of Text.Eduard H. Hovy, M. Marcus, M. Palmer, S. Pradhan,L.
Ramshaw, and R. Weischedel.
2006.
Ontonotes:The 90% solution.
In Proceedings of the Human Lan-guage Technology / North American Association ofComputational Linguistics conference (HLT-NAACL2006), New York, NY.
Short paper.Claudia Leacock, Martin Chodorow, and George A.Miller.
1998.
Using corpus statistics and wordnetrelations for sense identification.
Computational Lin-guistics, 24(1):147?166, March.Ken Litkowski and Orin Hargraves.
2007.
SemEval-2007 Task 06: Word sense disambiguation of preposi-tions.
In SemEval-2007: 4th International Workshopon Semantic Evaluations.K.
C. Litkowski.
2005.
The preposition project.
InProceedings of the Second ACL-SIGSEM Workshop onThe Linguistic Dimensions of Prepositions and theirUse in Computational Linguistics Formalisms and Ap-plications, Colchester, England, April.
University ofEssex.David Martinez, Eneko Agirre, and Xinglong Wang.2006.
Word relatives in context for word sense dis-ambiguation.
In Proceedings of the 2006 AustralasianLanguage Technology Workshop (ALTW 2006), pages42?50.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 Task 10: English lexical substitution task.
InSemEval-2007: 4th International Workshop on Se-mantic Evaluations.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The Senseval-3 English lexical sampletask.
In Proceedings of Senseval-3: The Third Inter-national Workshop on the Evaluation of Systems forthe Semantic Analysis of Text.Rada Mihalcea.
2002.
Bootstrapping large sense taggedcorpora.
In Proceedings of the 3rd InternationalConference on Languages Resources and EvaluationsLREC 2002, Las Palmas, Spain, May.Martha Palmer, Sameer Pradhan, and Edward Loper.2007.
SemEval-2007 Task 17: English lexical sample,English SRL and English all-words tasks.
In SemEval-2007: 4th International Workshop on Semantic Evalu-ations.Claude Elwood Shannon.
1951.
Prediction and entropyof printed English.
The Bell System Technical Journal,30:50?64.Thesaurus.com.
2007.
Roget?s NewMillenniumTMThesaurus, First Edition (v1.3.1).
Lexico Publishing Group, LLC.http://thesaurus.reference.com.David Yarowsky and Radu Florian.
2002.
Evaluatingsense disambiguation across diverse parameter spaces.Natural Language Engineering, 8(4):293?310.Deniz Yuret.
2004.
Some experiments with a NaiveBayes WSD system.
In ACL 2004 Senseval-3 Work-shop, Barcelona, Spain, July.213English Lexical Sample WSDlexelt trn/tst s mfs acc lexelt trn/tst s mfs accaffect.v 45/19 1 1.000 1.000 allow.v 108/35 2 0.971 0.971announce.v 88/20 2 1.000 1.000 approve.v 53/12 2 0.917 0.917area.n 326/37 3 0.703 0.838 ask.v 348/58 6 0.517 0.759attempt.v 40/10 1 1.000 1.000 authority.n 90/21 4 0.238 0.714avoid.v 55/16 1 1.000 1.000 base.n 92/20 5 0.100 0.650begin.v 114/48 4 0.562 0.792 believe.v 202/55 2 0.782 0.836bill.n 404/102 3 0.755 0.902 build.v 119/46 3 0.739 0.543buy.v 164/46 5 0.761 0.783 capital.n 278/57 4 0.965 0.982care.v 69/7 3 0.286 1.000 carrier.n 111/21 7 0.714 0.667cause.v 73/47 1 1.000 1.000 chance.n 91/15 4 0.400 0.667claim.v 54/15 3 0.800 0.800 come.v 186/43 10 0.233 0.372complain.v 32/14 2 0.857 0.857 complete.v 42/16 2 0.938 0.938condition.n 132/34 2 0.765 0.765 contribute.v 35/18 2 0.500 0.500defense.n 120/21 7 0.286 0.476 describe.v 57/19 3 1.000 1.000development.n 180/29 3 0.621 0.759 disclose.v 55/14 1 0.929 0.929do.v 207/61 4 0.902 0.934 drug.n 205/46 2 0.870 0.935effect.n 178/30 3 0.767 0.800 end.v 135/21 4 0.524 0.619enjoy.v 56/14 2 0.571 0.643 estimate.v 74/16 1 1.000 1.000examine.v 26/3 3 1.000 1.000 exchange.n 363/61 5 0.738 0.902exist.v 52/22 2 1.000 1.000 explain.v 85/18 2 0.889 0.944express.v 47/10 1 1.000 1.000 feel.v 347/51 3 0.686 0.765find.v 174/28 5 0.821 0.821 fix.v 32/2 5 0.500 0.500future.n 350/146 3 0.863 0.829 go.v 244/61 12 0.459 0.426grant.v 19/5 2 0.800 0.400 hold.v 129/24 8 0.375 0.542hope.v 103/33 1 1.000 1.000 hour.n 187/48 4 0.896 0.771improve.v 31/16 1 1.000 1.000 job.n 188/39 3 0.821 0.795join.v 68/18 4 0.389 0.556 keep.v 260/80 7 0.562 0.562kill.v 111/16 4 0.875 0.875 lead.v 165/39 6 0.385 0.513maintain.v 61/10 2 0.900 0.800 management.n 284/45 2 0.711 0.978move.n 270/47 4 0.979 0.979 need.v 195/56 2 0.714 0.857negotiate.v 25/9 1 1.000 1.000 network.n 152/55 3 0.909 0.836occur.v 47/22 2 0.864 0.864 order.n 346/57 7 0.912 0.930part.n 481/71 4 0.662 0.901 people.n 754/115 4 0.904 0.948plant.n 347/64 2 0.984 0.984 point.n 469/150 9 0.813 0.920policy.n 331/39 2 0.974 0.949 position.n 268/45 7 0.467 0.556power.n 251/47 3 0.277 0.766 prepare.v 54/18 2 0.778 0.833president.n 879/177 3 0.729 0.927 produce.v 115/44 2 0.750 0.750promise.v 50/8 2 0.750 0.750 propose.v 34/14 2 0.857 1.000prove.v 49/22 3 0.318 0.818 purchase.v 35/15 1 1.000 1.000raise.v 147/34 7 0.147 0.441 rate.n 1009/145 2 0.862 0.917recall.v 49/15 3 0.867 0.933 receive.v 136/48 2 0.958 0.958regard.v 40/14 3 0.714 0.643 remember.v 121/13 2 1.000 1.000remove.v 47/17 1 1.000 1.000 replace.v 46/15 2 1.000 1.000report.v 128/35 3 0.914 0.914 rush.v 28/7 2 1.000 1.000say.v 2161/541 5 0.987 0.987 see.v 158/54 6 0.444 0.574set.v 174/42 9 0.286 0.500 share.n 2536/525 2 0.971 0.973source.n 152/35 5 0.371 0.829 space.n 67/14 5 0.786 0.929start.v 214/38 6 0.447 0.447 state.n 617/72 3 0.792 0.819system.n 450/70 5 0.486 0.586 turn.v 340/62 13 0.387 0.516value.n 335/59 3 0.983 0.983 work.v 230/43 7 0.558 0.721AVG 222.8/48.5 3.6 0.787 0.851Preposition WSDlexelt trn/tst s mfs acc lexelt trn/tst s mfs accabout.p 710/364 6 0.885 0.934 above.p 48/23 5 0.609 0.522across.p 319/151 2 0.960 0.960 after.p 103/53 6 0.434 0.585against.p 195/92 6 0.435 0.793 along.p 364/173 3 0.954 0.954among.p 100/50 3 0.300 0.680 around.p 334/155 6 0.452 0.535as.p 173/84 1 1.000 1.000 at.p 715/367 12 0.425 0.662before.p 47/20 3 0.450 0.850 behind.p 138/68 4 0.662 0.676beneath.p 57/28 3 0.571 0.679 beside.p 62/29 1 1.000 1.000between.p 211/102 7 0.422 0.765 by.p 509/248 10 0.371 0.556down.p 332/153 3 0.438 0.647 during.p 81/39 2 0.385 0.564for.p 950/478 13 0.238 0.395 from.p 1204/578 16 0.279 0.415in.p 1391/688 13 0.362 0.436 inside.p 67/38 4 0.526 0.579into.p 604/297 8 0.451 0.539 like.p 266/125 7 0.768 0.808of.p 3000/1478 17 0.205 0.374 off.p 161/76 4 0.763 0.776on.p 872/441 20 0.206 0.469 onto.p 117/58 3 0.879 0.879over.p 200/98 12 0.327 0.510 round.p 181/82 7 0.378 0.512through.p 440/208 15 0.495 0.538 to.p 1182/572 10 0.322 0.579towards.p 214/102 4 0.873 0.873 with.p 1187/578 15 0.249 0.455AVG 486.3/238.1 7.4 0.397 0.547Table 2: English Lexical Sample and Preposition WSD Results: lexelt is the lexical item, trn/tst is thenumber of training and testing instances, s is the number of senses in the training set, mfs is the mostfrequent sense baseline, and acc is the final accuracy.214
