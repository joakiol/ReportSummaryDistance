Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2316?2325,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTransG : A Generative Model for Knowledge Graph EmbeddingHan Xiao, Minlie Huang?, Xiaoyan ZhuState Key Lab.
of Intelligent Technology and SystemsNational Lab.
for Information Science and TechnologyDept.
of Computer Science and TechnologyTsinghua University, Beijing 100084, PR China{aihuang, zxy-dcs}@tsinghua.edu.cnAbstractRecently, knowledge graph embedding,which projects symbolic entities and rela-tions into continuous vector space, has be-come a new, hot topic in artificial intelli-gence.
This paper proposes a novel gen-erative model (TransG) to address the is-sue of multiple relation semantics that arelation may have multiple meanings re-vealed by the entity pairs associated withthe corresponding triples.
The new modelcan discover latent semantics for a rela-tion and leverage a mixture of relation-specific component vectors to embed a facttriple.
To the best of our knowledge, thisis the first generative model for knowl-edge graph embedding, and at the firsttime, the issue of multiple relation seman-tics is formally discussed.
Extensive ex-periments show that the proposed modelachieves substantial improvements againstthe state-of-the-art baselines.1 IntroductionAbstract or real-world knowledge is always a ma-jor topic in Artificial Intelligence.
Knowledgebases such as Wordnet (Miller, 1995) and Free-base (Bollacker et al, 2008) have been shown veryuseful to AI tasks including question answering,knowledge inference, and so on.
However, tra-ditional knowledge bases are symbolic and logic,thus numerical machine learning methods can-not be leveraged to support the computation overthe knowledge bases.
To this end, knowledgegraph embedding has been proposed to project en-tities and relations into continuous vector spaces.Among various embedding models, there is a line?Correspondence authorof translation-based models such as TransE (Bor-des et al, 2013), TransH (Wang et al, 2014),TransR (Lin et al, 2015b), and other related mod-els (He et al, 2015) (Lin et al, 2015a).Figure 1: Visualization of TransE embedding vec-tors with PCA dimension reduction.
Four relations(a ?
d) are chosen from Freebase and Wordnet.A dot denotes a triple and its position is decidedby the difference vector between tail and head en-tity (t?
h).
Since TransE adopts the principle oft?
h ?
r, there is supposed to be only one clusterwhose centre is the relation vector r. However, re-sults show that there exist multiple clusters, whichjustifies our multiple relation semantics assump-tion.A fact of knowledge base can usually be rep-resented by a triple (h, r, t) where h, r, t indicatea head entity, a relation, and a tail entity, respec-tively.
All translation-based models almost followthe same principle hr+ r ?
trwhere hr, r, trin-2316dicate the embedding vectors of triple (h, r, t),with the head and tail entity vector projected withrespect to the relation space.In spite of the success of these models, noneof the previous models has formally discussedthe issue of multiple relation semantics that arelation may have multiple meanings revealed bythe entity pairs associated with the correspondingtriples.
As can be seen from Fig.
1, visualizationresults on embedding vectors obtained fromTransE (Bordes et al, 2013) show that, thereare different clusters for a specific relation,and different clusters indicate different latentsemantics.
For example, the relation HasPart hasat least two latent semantics: composition-relatedas (Table, HasPart, Leg) and location-relatedas (Atlantics, HasPart, NewYorkBay).
As onemore example, in Freebase, (Jon Snow, birthplace, Winter Fall) and (George R. R. Martin,birth place, U.S.) are mapped to schema /fic-tional universe/fictional character/place of birthand /people/person/place of birth respectively,indicating that birth place has different meanings.This phenomenon is quite common in knowledgebases for two reasons: artificial simplification andnature of knowledge.
On one hand, knowledgebase curators could not involve too many similarrelations, so abstracting multiple similar relationsinto one specific relation is a common trick.
Onthe other hand, both language and knowledgerepresentations often involve ambiguous infor-mation.
The ambiguity of knowledge means asemantic mixture.
For example, when we mention?Expert?, we may refer to scientist, businessmanor writer, so the concept ?Expert?
may be ambigu-ous in a specific situation, or generally a semanticmixture of these cases.However, since previous translation-based mod-els adopt hr+ r ?
tr, they assign only one trans-lation vector for one relation, and these models arenot able to deal with the issue of multiple relationsemantics.
To illustrate more clearly, as showedin Fig.2, there is only one unique representationfor relation HasPart in traditional models, thusthe models made more errors when embedding thetriples of the relation.
Instead, in our proposedmodel, we leverage a Bayesian non-parametric in-finite mixture model to handle multiple relation se-mantics by generating multiple translation compo-nents for a relation.
Thus, different semantics arecharacterized by different components in our em-bedding model.
For example, we can distinguishthe two clusters HasPart.1 or HasPart.2, wherethe relation semantics are automatically clusteredto represent the meaning of associated entity pairs.To summarize, our contributions are as follows:?
We propose a new issue in knowledge graphembedding, multiple relation semantics thata relation in knowledge graph may have dif-ferent meanings revealed by the associatedentity pairs, which has never been studiedpreviously.?
To address the above issue, we propose anovel Bayesian non-parametric infinite mix-ture embedding model, TransG.
The modelcan automatically discover semantic clustersof a relation, and leverage a mixture of multi-ple relation components for translating an en-tity pair.
Moreover, we present new insightsfrom the generative perspective.?
Extensive experiments show that our pro-posed model obtains substantial improve-ments against the state-of-the-art baselines.2 Related WorkTranslation-Based Embedding.
Existingtranslation-based embedding methods share thesame translation principle h + r ?
t and thescore function is designed as:fr(h, t) = ||hr+ r?
tr||22where hr, trare entity embedding vectors pro-jected in the relation-specific space.
TransE (Bor-des et al, 2013), lays the entities in the original en-tity space: hr= h, tr= t. TransH (Wang et al,2014) projects entities into a hyperplane for ad-dressing the issue of complex relation embedding:hr= h?w>rhwr, tr= t?w>rtwr.
To addressthe same issue, TransR (Lin et al, 2015b), trans-forms the entity embeddings by the same relation-specific matrix: hr= Mrh, tr= Mrt.
TransRalso proposes an ad-hoc clustering-based method,CTransR, where the entity pairs for a relationare clustered into different groups, and the pairsin the same group share the same relation vec-tor.
In comparison, our model is more elegantto address such an issue theoretically, and doesnot require a pre-process of clustering.
Further-more, our model has much better performancethan CTransR, as expected.
TransM (Fan et al,2317Figure 2: Visualization of multiple relation semantics.
The data are selected from Wordnet.
The dotsare correct triples that belong to HasPart relation, while the circles are incorrect ones.
The point coor-dinate is the difference vector between tail and head entity, which should be near to the centre.
(a) Thecorrect triples are hard to be distinguished from the incorrect ones.
(b) By applying multiple semanticcomponents, our proposed model could discriminate the correct triples from the wrong ones.2014) leverages the structure of the knowledgegraph via pre-calculating the distinct weight foreach training triple to enhance embedding.
KG2E(He et al, 2015) is a probabilistic embeddingmethod for modeling the uncertainty in knowledgegraph.There are many works to improve translation-based methods by considering other information.For instance, (Guo et al, 2015) aims at discov-ering the geometric structure of the embeddingspace to make it semantically smooth.
(Wang etal., 2014) focuses on bridging the gap betweenknowledge and texts, with a loss function forjointly modeling knowledge graph and text re-sources.
(Wang et al, 2015) incorporates the rulesthat are related with relation types such as 1-N andN-1.
PTransE (Lin et al, 2015a) takes into ac-count path information in knowledge graph.Since the previous models are point-wise mod-eling methods, ManifoldE (Xiao et al, 2016) pro-poses a novel manifold-based approach for knowl-edge graph embedding.
In aid of kernel tricks,manifold-based methods can improve embeddingperformance substantially.Structured & Unstructured Embedding.
Thestructured embedding model (Bordes et al, 2011)transforms the entity space with the head-specificand tail-specific matrices.
The score function isdefined as fr(h, t) = ||Mh,rh?Mt,rt||.
Ac-cording to (Socher et al, 2013), this model cannotcapture the relationship between entities.
Seman-tic Matching Energy (SME) (Bordes et al, 2012)(Bordes et al, 2014) can handle the correlationsbetween entities and relations by matrix productand Hadamard product.
The unstructured model(Bordes et al, 2012) may be a simplified versionof TransE without considering any relation-relatedinformation.
The score function is directly definedas fr(h, t) = ||h?
t||22.Neural Network based Embedding.
Sin-gle Layer Model (SLM) (Socher et al, 2013)applies neural network to knowledge graphembedding.
The score function is definedas fr(h, t) = u>rg(Mr,1h + Mr,2t) whereMr,1,Mr,2are relation-specific weight matri-ces.
Neural Tensor Network (NTN) (Socheret al, 2013) defines a very expressive scorefunction by applying tensor: fr(h, t) =u>rg(h>W?
?rt + Mr,1h + Mr,2t + br), whereuris a relation-specific linear layer, g(?)
is thetanh function, W ?
Rd?d?kis a 3-way tensor.Factor Models.
The latent factor models (Jenat-ton et al, 2012) (Sutskever et al, 2009) attemptto capturing the second-order correlations betweenentities by a quadratic form.
The score functionis defined as fr(h, t) = h>Wrt.
RESCAL is acollective matrix factorization model which is alsoa common method in knowledge base embedding(Nickel et al, 2011) (Nickel et al, 2012).3 Methods3.1 TransG: A Generative Model forEmbeddingAs just mentioned, only one single translation vec-tor for a relation may be insufficient to model mul-tiple relation semantics.
In this paper, we pro-pose to use Bayesian non-parametric infinite mix-2318ture embedding model (Griffiths and Ghahramani,2011).
The generative process of the model is asfollows:1.
For an entity e ?
E:(a) Draw each entity embedding mean vec-tor from a standard normal distributionas a prior: uev N (0,1).2.
For a triple (h, r, t) ?
?
:(a) Draw a semantic component from Chi-nese Restaurant Process for this relation:pir,m?
CRP (?).
(b) Draw a head entity embedding vec-tor from a normal distribution: h vN (uh, ?2hE).
(c) Draw a tail entity embedding vec-tor from a normal distribution: t vN (ut, ?2tE).
(d) Draw a relation embedding vector forthis semantics: ur,m= t?
h vN (ut?
uh, (?2h+ ?2t)E).where uhand utindicate the mean embeddingvector for head and tail respectively, ?hand ?tindicate the variance of corresponding entity dis-tribution respectively, and ur,mis the m-th com-ponent translation vector of relation r. ChineseRestaurant Process (CRP) is a Dirichlet Processand it can automatically detect semantic compo-nents.
In this setting, we obtain the score functionas below:P{(h, r, t)} ?Mr?m=1pir,mP(ur,m|h, t)=Mr?m=1pir,me?||uh+ur,m?ut||22?2h+?2t(1)where pir,mis the mixing factor, indicating theweight of i-th component and Mris the numberof semantic components for the relation r, whichis learned from the data automatically by the CRP.Inspired by Fig.1, TransG leverages a mixtureof relation component vectors for a specific re-lation.
Each component represents a specific la-tent meaning.
By this way, TransG could distin-guish multiple relation semantics.
Notably, theCRP could generate multiple semantic compo-nents when it is necessary and the relation seman-tic component number Mris learned adaptivelyfrom the data.Table 1: Statistics of datasetsData WN18 FB15K WN11 FB13#Rel 18 1,345 11 13#Ent 40,943 14,951 38,696 75,043#Train 141,442 483,142 112,581 316,232#Valid 5,000 50,000 2,609 5,908#Test 5,000 59,071 10,544 23,7333.2 Explanation from the GeometryPerspectiveSimilar to previous studies, TransG has geometricexplanations.
In the previous methods, when therelation r of triple (h, r, t) is given, the geometricrepresentations are fixed, as h + r ?
t. However,TransG generalizes this geometric principle to:m?
(h,r,t)= arg maxm=1...Mr(pir,me?||uh+ur,m?ut||22?2h+?2t)h + ur,m?(h,r,t)?
t (2)where m?
(h,r,t)is the index of primary compo-nent.
Though all the components contribute to themodel, the primary one contributes the most dueto the exponential effect (exp(?)).
When a triple(h, r, t) is given, TransG works out the index ofprimary component then translates the head entityto the tail one with the primary translation vector.For most triples, there should be only one com-ponent that have significant non-zero value as(pir,me?||uh+ur,m?ut||22?2h+?2t)and the others wouldbe small enough, due to the exponential decay.This property reduces the noise from the othersemantic components to better characterize mul-tiple relation semantics.
In detail, (t?
h) is al-most around only one translation vector ur,m?
(h,r,t)in TransG.
Under the condition m 6= m?
(h,r,t),(||uh+ur,m?ut||22?2h+?2t)is very large so that the expo-nential function value is very small.
This is whythe primary component could represent the corre-sponding semantics.To summarize, previous studies make transla-tion identically for all the triples of the same re-lation, but TransG automatically selects the besttranslation vector according to the specific seman-tics of a triple.
Therefore, TransG could focus onthe specific semantic embedding to avoid muchnoise from the other unrelated semantic compo-nents and result in promising improvements thanexisting methods.
Note that, all the components in2319Table 2: Evaluation results on link predictionDatasets WN18 FB15KMetricMean Rank HITS@10(%) Mean Rank HITS@10(%)Raw Filter Raw Filter Raw Filter Raw FilterUnstructured (Bordes et al, 2011) 315 304 35.3 38.2 1,074 979 4.5 6.3RESCAL (Nickel et al, 2012) 1,180 1,163 37.2 52.8 828 683 28.4 44.1SE(Bordes et al, 2011) 1,011 985 68.5 80.5 273 162 28.8 39.8SME(bilinear) (Bordes et al, 2012) 526 509 54.7 61.3 284 158 31.3 41.3LFM (Jenatton et al, 2012) 469 456 71.4 81.6 283 164 26.0 33.1TransE (Bordes et al, 2013) 263 251 75.4 89.2 243 125 34.9 47.1TransH (Wang et al, 2014) 401 388 73.0 82.3 212 87 45.7 64.4TransR (Lin et al, 2015b) 238 225 79.8 92.0 198 77 48.2 68.7CTransR (Lin et al, 2015b) 231 218 79.4 92.3 199 75 48.4 70.2PTransE (Lin et al, 2015a) N/A N/A N/A N/A 207 58 51.4 84.6KG2E (He et al, 2015) 362 348 80.5 93.2 183 69 47.5 71.5TransG (this paper) 357 345 84.5 94.9 152 50 55.9 88.2TransG have their own contributions, but the pri-mary one makes the most.3.3 Training AlgorithmThe maximum data likelihood principle is appliedfor training.
As to the non-parametric part, pir,mis generated from the CRP with Gibbs Sampling,similar to (He et al, 2015) and (Griffiths andGhahramani, 2011).
A new component is sampledfor a triple (h,r,t) with the below probability:P(mr,new) =?e?||h?t||22?2h+?2t+2?e?||h?t||22?2h+?2t+2+ P{(h, r, t)}(3)where P{(h, r, t)} is the current posterior prob-ability.
As to other parts, in order to better distin-guish the true triples from the false ones, we max-imize the ratio of likelihood of the true triples tothat of the false ones.
Notably, the embedding vec-tors are initialized by (Glorot and Bengio, 2010).Putting all the other constraints together, the finalobjective function is obtained, as follows:minuh,ur,m,utLL = ??(h,r,t)??ln(Mr?m=1pir,me?||uh+ur,m?ut||22?2h+?2t)+?(h?,r?,t?)???ln??Mr?m=1pir?,me?||uh?+ur?,m?ut?||22?2h?+?2t??
?+C(?r?RMr?m=1||ur,m||22+?e?E||ue||22)(4)where ?
is the set of golden triples and ?
?is theset of false triples.
C controls the scaling degree.E is the set of entities and R is the set of relations.Noted that the mixing factors pi and the variances?
are also learned jointly in the optimization.SGD is applied to solve this optimization prob-lem.
In addition, we apply a trick to control theparameter updating process during training.
Forthose very impossible triples, the update process isskipped.
Hence, we introduce a similar conditionas TransE (Bordes et al, 2013) adopts: the train-ing algorithm will update the embedding vectorsonly if the below condition is satisfied:P{(h, r, t)}P{(h?, r?, t?)}=?Mrm=1pir,me?||uh+ur,m?ut||22?2h+?2t?Mr?m=1pir?,me?||uh?+ur?,m?ut?||22?2h?+?2t??
Mre?
(5)where (h, r, t) ?
?
and (h?, r?, t?)
?
??.
?
con-trols the updating condition.As to the efficiency, in theory, the time com-plexity of TransG is bounded by a small constantM compared to TransE, that is O(TransG) =O(M ?
O(TransE)) where M is the number ofsemantic components in the model.
Note thatTransE is the fastest method among translation-based methods.
The experiment of Link Predic-tion shows that TransG and TransE would con-verge at around 500 epochs, meaning there is alsono significant difference in convergence speed.
Inexperiment, TransG takes 4.8s for one iteration onFB15K while TransR costs 136.8s and PTransE2320Table 3: Evaluation results on FB15K by mapping properties of relations(%)Tasks Predicting Head(HITS@10) Predicting Tail(HITS@10)Relation Category 1-1 1-N N-1 N-N 1-1 1-N N-1 N-NUnstructured (Bordes et al, 2011) 34.5 2.5 6.1 6.6 34.3 4.2 1.9 6.6SE(Bordes et al, 2011) 35.6 62.6 17.2 37.5 34.9 14.6 68.3 41.3SME(bilinear) (Bordes et al, 2012) 30.9 69.6 19.9 38.6 28.2 13.1 76.0 41.8TransE (Bordes et al, 2013) 43.7 65.7 18.2 47.2 43.7 19.7 66.7 50.0TransH (Wang et al, 2014) 66.8 87.6 28.7 64.5 65.5 39.8 83.3 67.2TransR (Lin et al, 2015b) 78.8 89.2 34.1 69.2 79.2 37.4 90.4 72.1CTransR (Lin et al, 2015b) 81.5 89.0 34.7 71.2 80.8 38.6 90.1 73.8PTransE (Lin et al, 2015a) 90.1 92.0 58.7 86.1 90.1 70.7 87.5 88.7KG2E (He et al, 2015) 92.3 93.7 66.0 69.6 92.6 67.9 94.4 73.4TransG (this paper) 93.0 96.0 62.5 86.8 92.8 68.1 94.5 88.8costs 1200.0s on the same computer for the samedataset.4 ExperimentsOur experiments are conducted on four publicbenchmark datasets that are the subsets of Word-net and Freebase, respectively.
The statistics ofthese datasets are listed in Tab.1.
Experimentsare conducted on two tasks : Link Prediction andTriple Classification.
To further demonstrate howthe proposed model approaches multiple relationsemantics, we present semantic component analy-sis at the end of this section.4.1 Link PredictionLink prediction concerns knowledge graph com-pletion: when given an entity and a relation, theembedding models predict the other missing en-tity.
More specifically, in this task, we predict tgiven (h, r, ?
), or predict h given (?, r, t).
TheWN18 and FB15K are two benchmark datasets forthis task.
Note that many AI tasks could be en-hanced by Link Prediction such as relation extrac-tion (Hoffmann et al, 2011).Evaluation Protocol.
We adopt the same proto-col used in previous studies.
For each testing triple(h, r, t), we corrupt it by replacing the tail t (or thehead h) with every entity e in the knowledge graphand calculate a probabilistic score of this corruptedtriple (h, r, e) (or (e, r, t)) with the score functionfr(h, e).
After ranking these scores in descend-ing order, we obtain the rank of the original triple.There are two metrics for evaluation: the averagedrank (Mean Rank) and the proportion of testingtriple whose rank is not larger than 10 (HITS@10).This is called ?Raw?
setting.
When we filter outthe corrupted triples that exist in the training, val-idation, or test datasets, this is the?Filter?
setting.If a corrupted triple exists in the knowledge graph,ranking it ahead the original triple is also accept-able.
To eliminate this case, the ?Filter?
settingis preferred.
In both settings, a lower Mean Rankand a higher HITS@10 mean better performance.Implementation.
As the datasets are the same,we directly report the experimental results of sev-eral baselines from the literature, as in (Bordeset al, 2013), (Wang et al, 2014) and (Lin et al,2015b).
We have attempted several settings onthe validation dataset to get the best configuration.For example, we have tried the dimensions of 100,200, 300, 400.
Under the ?bern.?
sampling strat-egy, the optimal configurations are: learning rate?
= 0.001, the embedding dimension k = 100,?
= 2.5, ?
= 0.05 on WN18; ?
= 0.0015,k = 400, ?
= 3.0, ?
= 0.1 on FB15K.
Notethat all the symbols are introduced in ?Methods?.We train the model until it converges.Results.
Evaluation results on WN18 andFB15K are reported in Tab.2 and Tab.31.
We ob-serve that:1.
TransG outperforms all the baselines obvi-ously.
Compared to TransR, TransG makesimprovements by 2.9% on WN18 and 26.0%on FB15K, and the averaged semantic com-ponent number on WN18 is 5.67 and that onFB15K is 8.77.
This result demonstrates cap-turing multiple relation semantics would ben-efit embedding.1Note that correctly regularized TransE can produce muchbetter performance than what were reported in the ogirinalpaper, see (Garc?
?a-Dur?an et al, 2015).2321Table 4: Different clusters in WN11 and FB13 relations.Relation Cluster Triples (Head, Tail)PartOfLocation (Capital of Utah, Beehive State), (Hindustan, Bharat) ...Composition (Monitor, Television), (Bush, Adult Body), (Cell Organ, Cell)...ReligionCatholicism (Cimabue, Catholicism), (St.Catald, Catholicism) ...Others (Michal Czajkowsk, Islam), (Honinbo Sansa, Buddhism) ...DomainRegionAbstract (Computer Science, Security System), (Computer Science, PL)..Specific (Computer Science, Router), (Computer Science, Disk File) ...ProfessionScientist (Michael Woodruf, Surgeon), (El Lissitzky, Architect)...Businessman (Enoch Pratt, Entrepreneur), (Charles Tennant, Magnate)...Writer (Vlad.
Gardin, Screen Writer), (John Huston, Screen Writer) ...2.
The model has a bad Mean Rank score on theWN18 dataset.
Further analysis shows thatthere are 24 testing triples (0.5% of the test-ing set) whose ranks are more than 30,000,and these few cases would lead to about 150mean rank loss.
Among these triples, thereare 23 triples whose tail or head entities havenever been co-occurring with the correspond-ing relations in the training set.
In one word,there is no sufficient training data for thoserelations and entities.3.
Compared to CTransR, TransG solves themultiple relation semantics problem muchbetter for two reasons.
Firstly, CTransR clus-ters the entity pairs for a specific relation andthen performs embedding for each cluster,but TransG deals with embedding and multi-ple relation semantics simultaneously, wherethe two processes can be enhanced by eachother.
Secondly, CTransR models a triple byonly one cluster, but TransG applies a mix-ture to refine the embedding.Our model is almost insensitive to the dimen-sion if that is sufficient.
For the dimensionsof 100, 200, 300, 400, the HITS@10 of TransGon FB15 are 81.8%, 84.0%, 85.8%, 88.2%, whilethose of TransE are 47.1%, 48.5%, 51.3%, 49.2%.4.2 Triple ClassificationIn order to testify the discriminative capability be-tween true and false facts, triple classification isconducted.
This is a classical task in knowledgebase embedding, which aims at predicting whethera given triple (h, r, t) is correct or not.
WN11and FB13 are the benchmark datasets for this task.Note that evaluation of classification needs nega-tive samples, and the datasets have already pro-vided negative triples.Figure 3: Accuracies of each relations in WN11for triple classification.
The right y-axis is thenumber of semantic components, corresponding tothe lines.Evaluation Protocol.
The decision process isvery simple as follows: for a triple (h, r, t), iffr(h, t) is below a threshold ?r, then positive; oth-erwise negative.
The thresholds {?r} are deter-mined on the validation dataset.Table 5: Triple classification: accuracy(%) for dif-ferent embedding methods.Methods WN11 FB13 AVG.LFM 73.8 84.3 79.0NTN 70.4 87.1 78.8TransE 75.9 81.5 78.7TransH 78.8 83.3 81.1TransR 85.9 82.5 84.2CTransR 85.7 N/A N/AKG2E 85.4 85.3 85.4TransG 87.4 87.3 87.4Implementation.
As all methods use the samedatasets, we directly re-use the results of differentmethods from the literature.
We have attemptedseveral settings on the validation dataset to find2322Figure 4: Semantic component number on WN18 (left) and FB13 (right).the best configuration.
The optimal configurationsof TransG are as follows: ?bern?
sampling, learn-ing rate ?
= 0.001, k = 50, ?
= 6.0, ?
= 0.1on WN11, and ?bern?
sampling, ?
= 0.002,k = 400, ?
= 3.0, ?
= 0.1 on FB13.Results.
Accuracies are reported in Tab.5 andFig.3.
The following are our observations:1.
TransG outperforms all the baselines remark-ably.
Compared to TransR, TransG improvesby 1.7% on WN11 and 5.8% on FB13, andthe averaged semantic component number onWN11 is 2.63 and that on FB13 is 4.53.
Thisresult shows the benefit of capturing multiplerelation semantics for a relation.2.
The relations, such as ?Synset Domain?
and?Type Of?, which hold more semantic com-ponents, are improved much more.
In com-parison, the relation ?Similar?
holds only onesemantic component and is almost not pro-moted.
This further demonstrates that cap-turing multiple relation semantics can benefitembedding.4.3 Semantic Component AnalysisIn this subsection, we analyse the number of se-mantic components for different relations and listthe component number on the dataset WN18 andFB13 in Fig.4.Results.
As Fig.
4 and Tab.
4 show, we have thefollowing observations:1.
Multiple semantic components are indeednecessary for most relations.
Except for re-lations such as ?Also See?, ?Synset Usage?and ?Gender?, all other relations have morethan one semantic component.2.
Different components indeed correspondto different semantics, justifying thetheoretical analysis and effectivenessof TransG.
For example, ?Profession?has at least three semantics: scientist-related as (ElLissitzky,Architect),businessman-related as(EnochPratt,Entrepreneur) and writer-related as (Vlad.Gardin, ScreenWriter).3.
WN11 and WN18 are different subsets ofWordnet.
As we know, the semantic compo-nent number is decided on the triples in thedataset.
Therefore, It?s reasonable that sim-ilar relations, such as ?Synset Domain?
and?Synset Usage?
may hold different semanticnumbers for WN11 and WN18.5 ConclusionIn this paper, we propose a generative Bayesiannon-parametric infinite mixture embedding model,TransG, to address a new issue, multiple relationsemantics, which can be commonly seen in knowl-edge graph.
TransG can discover the latent se-mantics of a relation automatically and leveragea mixture of relation components for embedding.Extensive experiments show our method achievessubstantial improvements against the state-of-the-art baselines.6 AcknowledgementsThis work was partly supported by the NationalBasic Research Program (973 Program) undergrant No.
2012CB316301/2013CB329403, theNational Science Foundation of China under grant2323No.
61272227/61332007, and the Beijing HigherEducation Young Elite Teacher Project.ReferencesKurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuringhuman knowledge.
In Proceedings of the 2008 ACMSIGMOD international conference on Managementof data, pages 1247?1250.
ACM.Antoine Bordes, Jason Weston, Ronan Collobert,Yoshua Bengio, et al 2011.
Learning structuredembeddings of knowledge bases.
In Proceedings ofthe Twenty-fifth AAAI Conference on Artificial Intel-ligence.Antoine Bordes, Xavier Glorot, Jason Weston, andYoshua Bengio.
2012.
Joint learning of wordsand meaning representations for open-text semanticparsing.
In International Conference on ArtificialIntelligence and Statistics, pages 127?135.Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.2013.
Translating embeddings for modeling multi-relational data.
In Advances in Neural InformationProcessing Systems, pages 2787?2795.Antoine Bordes, Xavier Glorot, Jason Weston, andYoshua Bengio.
2014.
A semantic matching en-ergy function for learning with multi-relational data.Machine Learning, 94(2):233?259.Miao Fan, Qiang Zhou, Emily Chang, andThomas Fang Zheng.
2014.
Transition-basedknowledge graph embedding with relational map-ping properties.
In Proceedings of the 28th PacificAsia Conference on Language, Information, andComputation, pages 328?337.Alberto Garc?
?a-Dur?an, Antoine Bordes, NicolasUsunier, and Yves Grandvalet.
2015.
Combiningtwo and three-way embeddings models for link pre-diction in knowledge bases.
CoRR, abs/1506.00999.Xavier Glorot and Yoshua Bengio.
2010.
Understand-ing the difficulty of training deep feedforward neuralnetworks.
In International conference on artificialintelligence and statistics, pages 249?256.Thomas L Griffiths and Zoubin Ghahramani.
2011.The indian buffet process: An introduction and re-view.
The Journal of Machine Learning Research,12:1185?1224.Shu Guo, Quan Wang, Bin Wang, Lihong Wang, andLi Guo.
2015.
Semantically smooth knowledgegraph embedding.
In Proceedings of ACL.Shizhu He, Kang Liu, Guoliang Ji, and Jun Zhao.2015.
Learning to represent knowledge graphswith gaussian embedding.
In Proceedings of the24th ACM International on Conference on Informa-tion and Knowledge Management, pages 623?632.ACM.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S Weld.
2011.
Knowledge-based weak supervision for information extractionof overlapping relations.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies-Volume 1, pages 541?550.
Association for Compu-tational Linguistics.Rodolphe Jenatton, Nicolas L Roux, Antoine Bordes,and Guillaume R Obozinski.
2012.
A latent fac-tor model for highly multi-relational data.
In Ad-vances in Neural Information Processing Systems,pages 3167?3175.Yankai Lin, Zhiyuan Liu, and Maosong Sun.
2015a.Modeling relation paths for representation learn-ing of knowledge bases.
Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP).
Association for Com-putational Linguistics.Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, andXuan Zhu.
2015b.
Learning entity and relation em-beddings for knowledge graph completion.
In Pro-ceedings of the Twenty-Ninth AAAI Conference onArtificial Intelligence.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2011.
A three-way model for collectivelearning on multi-relational data.
In Proceedings ofthe 28th international conference on machine learn-ing (ICML-11), pages 809?816.Maximilian Nickel, Volker Tresp, and Hans-PeterKriegel.
2012.
Factorizing yago: scalable machinelearning for linked data.
In Proceedings of the 21stinternational conference on World Wide Web, pages271?280.
ACM.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural ten-sor networks for knowledge base completion.
In Ad-vances in Neural Information Processing Systems,pages 926?934.Ilya Sutskever, Joshua B Tenenbaum, and RuslanSalakhutdinov.
2009.
Modelling relational data us-ing bayesian clustered tensor factorization.
In Ad-vances in neural information processing systems,pages 1821?1828.Zhen Wang, Jianwen Zhang, Jianlin Feng, and ZhengChen.
2014.
Knowledge graph embedding by trans-lating on hyperplanes.
In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,pages 1112?1119.2324Quan Wang, Bin Wang, and Li Guo.
2015.
Knowl-edge base completion using embeddings and rules.In Proceedings of the 24th International Joint Con-ference on Artificial Intelligence.Han Xiao, Minlie Huang, and Xiaoyan Zhu.
2016.From one point to a manifold: Knowledge graph em-bedding for precise link prediction.
In IJCAI.2325
