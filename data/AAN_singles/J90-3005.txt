WORKSHOP ON THE EVALUATION OF NATURAL LANGUAGEPROCESSING SYSTEMSMartha PalmerUnisys Center for Advanced Information TechnologyPaoli, PA 19301Tim FininUnisys Center for Advanced Information TechnologyPaoli, PA 193011 INTRODUCTIONIn the past few years, the computational linguistics re-search community has begun to wrestle with the problem ofhow to evaluate its progress in developing natural languageprocessing systems.
With the exception of natural languageinterfaces, there are few working systems in existence, andthey tend to focus on very different asks using equallydifferent techniques.
There has been little agreement in thefield about training sets and test sets, or about clearlydefined subsets of problems that constitute standards fordifferent levels of performance.
Even those groups thathave attempted a measure of self-evaluation have oftenbeen reduced to discussing a system's performance in isola-tion---comparing its current performance to its previousperformance rather than to another system.
As this technol-ogy begins to move slowly into the marketplace, the lack ofuseful evaluation techniques i becoming more and morepainfully obvious.In order to make progress in the difficult area of naturallanguage valuation, a Workshop on the Evaluation ofNatural Language Processing Systems was held on Decem-ber 7-9, 1988 at the Wayne Hotel in Wayne, Pennsylvania.The workshop was organized by Martha Palmer (Unisys),assisted by a program committee consisting of Beth Sund-heim (NOSC), Ed Hovy (ISI), Tim Finin (Unisys), LynnBates (BBN), and Mitch Marcus (Pennsylvania).
Approx-imately 50 people participated, rawn from universities,industry, and government.
The workshop received the gen-erous support of the Rome Air Defense Center, the Associ-ation of Computational Linguistics, the American Associa-tion of Artificial Intelligence, and Unisys Defense Systems.The workshop was organized along two basic premises.First, it should be possible to discuss ystem evaluation ingeneral without having to state whether the purpose of thesystem is "question-answering" or text processing."
Eval-uating a system requires the definition of an applicationtask in terms of input/output pairs that are equally applica-ble to question-answering, text processing, or generation.Second, there are two basic types of evaluation, black-boxevaluation, which measures system performance on a giventask in terms of well-defined input/output pairs, and glass-box evaluation, which examines the internal workings ofthe system.
For example, glass-box performance evaluationfor a system that is supposed to perform semantic andpragmatic analysis hould include the examination ofpred-icate-argument relations, referents, and temporal and causalrelations.
Since there are many different stages of develop-ment hat a natural language system passes through beforeit is in a state where black-box evaluation is even possible(see Figure 1), glass-box evaluation plays an especiallyimportant role in guiding the development a early stages.With these premises in mind, the workshop was struc-tured around the following three sessions: (i) defining thenotions of "black-box evaluation" and "glass-boxevaluation" and exploring their utility; (ii) defining criteriafor "black-box evaluation"; and (iii) defining criteria for"glass-box evaluation."
It was hoped that the workshopwould shed light on the following questions.?
What are valid measures of"black-box" performance??
What linguistic theories are relevant o developing testsuites??
How can we characterize efficiency??
What is a reasonable expectation for robustness??
What would constitute valid training sets and test sets??
How does all of this relate to measuring progress in thefield?2 BACKGROUNDBefore looking at the distinctions between "black-box" and"glass-box" evaluation, it is first necessary to examine thedevelopment of a natural anguage system a little moreclosely.
There are several different phases, and differenttypes of evaluation are required at each phase.
The variousphases are summarized in Figure 1.Computational Linguistics Volume 16, Number 3, September 1990 175Martha Palmer and Tim Finin Natural Language Processing SystemsNLP System Development Steps1.
Picking the application.2.
Characterising the necessary phenomena.3.
Selecting relevant theories, if available.4.
Developing and testing algorithms that implement hese theo-ries.5.
Implementing the first pass st the system, testing it, and idenotifying gaps in coverage.6.
Characterising the new phenomena that have been discovered,especially those having to do with interactions between compo-nents.7.
Fine-tuning algorithms to improve ef~ciency, nnd also replacingalgorithms e.s the characterization of the phenomena changes.8.
Second pass at implementation, to extend coverage to thesenew phenomena nd thus fill in the gaps.9.
Third pass at an implementation in which a focus is placed onissues of extensibility.10.
Fourth and final pass st the implementation in which the sys-tem moves into s production environment.
This stage paysspecial attention to issues of zobustness.Figure 1 There are a number of different stages inthe development ofa natural language processingsystem.
Different kinds of evaluations are requiredand/or possible at the different stages.Speaking very roughly, the development of a naturallanguage processing system is usually sparked by the needsof the particular application driving it, whether it be ques-tion answering, text processing, or machine translation.What has happened in the past is that, in examining therequirements of such an application, it has quickly becomeapparent that certain phenomena, such as pronoun refer-ence, are essential to the successful handling of that appli-cation.
It has also quickly become apparent that for manyof these phenomena, especially semantic and pragmaticones, past linguistic analysis has very little to offer in theway of categorization.
Even when it might offer a fairlyrigorous account of the phenomenon, as in the case ofsyntax, it has very little to say about useful algorithms forefficiently producing syntactic analyses and even less to sayabout interaction between different ypes of phenomena.So, almost before beginning implementation, a great dealof effort in the computational linguistics community mustof necessity be devoted to tasks that can rightly be seen asbelonging to theoretical linguistics.
The Discourse Canonthat Bonnie Webber prepared for the Mohonk Darpa Work-shop (1988) is an excellent example of the type of ground-work that must be done prior to serious attempts at imple-mentation, and must be continued throughout andsubsequent to said implementation.
The field needs manymore such "canons" for other semantic and pragmaticphenomena.Algorithm development is equally important, and canalso be carried out independently of or in parallel with animplementation.
We have several different algorithms forsyntactic parsing, and ways of comparing them (and waysof proving that they are all equivalent), but very fewalgorithms for semantics and pragmatics.Implementing an algorithm for use in an application is aseparzte stage of development.
Progress cannot, however,be measured in terms of accurate output until a system thatuses particular algorithms to handle particular phenomenahas been implemented.
In addition to methods for measur-ing performance of entire systems, we also need ways ofmeasuri!ng progress in characterizing phenomena and devel-oping algorithms that will contribute to system develop-ment.Once a system is up and running, the accuracy of itsoutput can then be measured.
The different ypes of theoutput ,:an be associated with the phenomena that have tobe handled in order to produce each type.
For example,consider a phrase from the trouble failure report domain(Ball 1989):Replaced interlock switch with new one.In order accurately to fill in the slot in the database fieldassocia~ed with the new-part-installed(-) relation, the "one"anaphora has to be correctly resolved, requiring a complexinteraction between semantic and pragmatic analysis.It is possible to have two systems that produce the sameoutput, but do it very differently.
This is where such issuesas effic!iency, extensibility, maintainability, and robustnesscome in.
A more efficient implementation, for example,may be able to support a larger, more complex domain.With a more general implementation, it should be easier toextend the scope of the system's domain or to port thesystem to an entirely new domain.
A system with a moreconve, nient or more robust interface will be easier to useand, one would suppose, used more often.
In a recent studycomparing Lotus HAL (Lotus with a restricted naturallanguage interface) to Lotus, not only was Lotus HALjudged to be more convenient to use, but the Lotus HALusers also had higher scores on the problem-solving tasks(Napier 1989).2.1 BLACK-BOX EVALUATIONBlack-box evaluation is primarily focused on "what a sys-tem does."
Ideally, it should be possible to measure perfor-mance based on well-defined I/0 pairs.
If accurate output isproduced with respect o particular input, then the systemis performing correctly.
In practice, this is more difficultthan it appears.
There is no consensus on how to evaluatethe correctness of semantic representations, so output hasto be in terms of some specific application task such asdatabzse answering or template fill (Sundheim 1989).
Thisallows for an astonishing amount of variation betweensystems, and makes it difficult to separate out issues ofcoverage of linguistic phenomena from robustness anderror' recovery (see Figure 2).In addition to the accuracy of the output, systems couldalso be evaluated in terms of their user-friendliness, modu-larity, portability, and maintainability.
How easy are theyto us.e, how well do they plug into other components, canthey be ported and maintained by someone who is not asystem expert?
In general, it should be possible to perform176 Computational Linguistics Volume 16, Number 3, September 1990Martha Palmer and Tim Finin Natural Language Processing SystemsFigure 2 A "black-box evaluation" is primarilyfocused on "what a system does."
It attempts omeasure the system performance ona given task interms of well-defined input/output pairs.black box evaluation without knowing anything about theinner workings of the system--the system can be seen as ablack box, and can be evaluated by system users.2.2 GLASS-BOX EVALUATIONIn contrast, glass-box evaluation attempts to look inside thesystem, and find ways of measuring how well the systemdoes something, rather than simply whether or not it doesit.
Glass-box evaluation should measure the system's cover-age of a particular linguistic phenomenon rset of phenom-ena and the data structures used to represent them (seeFigure 3).
And it also should be concerned with the effi-ciency of the algorithms being used.
Many of these testscould be performed only by a system builder.
They areespecially useful in attempting to measure progress when asystem is under development.
Glass-box evaluation shouldalso include an examination of relevant linguistic theoriesand how faithfully they are implemented.
If a linguistictheory does not deal with all of the data and has to bemodified by the developer, those modifications need beclearly documented, and the information relayed to theFigure 3 A "glass-box evaluation" addresses "howthe system works".
It attempts o look inside thesystem and find ways of measuring how well thesystem does something, rather than simplywhether or not it does it.theory's developer.
For example, as pointed out in BonnieWebber's (Penn) presentation, there is a distinction be-tween Tree Adjunction Grammar (TAG) as a linguistictheory and the several algorithms that have been used toimplement TAG parsers: Extended CKY parser, ExtendedEarley parser, Two-pass extended Earley parser based onlexicalized TAGs, and a DCG parser using lexicalizedTAGs.
There is also a distinction between Centering as atheory for resolving anaphoric pronouns (Joshi and Wein-stein 1981; Gross et al 1983), and the attempts to use acentering approach to resolving pronouns in an implemen-tation (Brennan et al 1987).In addition, one way of looking inside a system is to lookat the performance of one or more modules or components.Which components are obtained epends on the nature ofthe decomposition of the system.
NL systems are com-monly decomposed into functional modules (e.g., parser,semantic interpretation, lexical ookup, etc.
), each of whichperforms a specified task, and into analysis phases duringwhich different functions can be performed.
A black-boxevaluation of a particular component's performance couldbe seen as a form of glass-box evaluation.
For example,taking a component such as a parser and defining a test thatdepends on associating specified outputs for specified inputswould be a black-box evaluation of the parser.
Since it is anevaluation of a component that cannot by itself perform anapplication, and since it will give information about thecomponent's coverage that is independent of the coverageof any system in which it might be embedded, this can beseen as providing lass-box information for such an overallsystem.3 WORKSHOP FORMATThe workshop began with a set of presentations that dis-cussed evaluation methods from related fields: speech pro-cessing (Dave Pa l le t - -N IST) ,  machine translation(Jonathan Slocum--Symantec), and information retrieval(Dave Lewis--UMASS).
This was followed by a panel onreports on evaluations of natural anguage systems chairedby Lynn Bates (BBN), and including John Nerbonne(HP), Debbie Dahl (Unisys), Anatole Gershman (Cogni-tive Systems, Inc), and Dick Kitteridge (Odyssey Re-search, Inc).
After lunch Beth Sundheim (NOSC) pre-sented the workshop with the task for the afternoon workinggroups, which was to discuss black-box evaluation method-ologies.
The groups consisted of Message Understandingchaired by Ralph Grishman (NYU), Text Understandingchaired by Lynette Hirschman (Unisys), Database Ques-tion-Answering chaired by Harry Tennant (TI), DialogueUnderstanding chaired by Mitch Marcus (Penn), and Gen-eration chaired by Ed Hovy.
After reporting on the resultsof the working groups, the workshop met for a banquet,which included a demonstration of the Dragon speechrecognition system by Jim Baker.
The second day beganwith another presentation f a method of black-box evalua-tion applied to syntactic parsers, (i.e., glass-box evaluationComputational Linguistics Volume 16, Number 3, September 1990 177Martha Palmer and Tim Finin Natural Language Processing Systemswith respect o an entire system), by Fred Jelinek (IBM),and then moved on to an introduction of the topic of glassbox evaluation by Martha Palmer (Unisys).
A panel chairedby Jerry Hobbs (SRI), which included Mitch Marcus(Penn) and Ralph Weischedel (BBN), discussed necessarycharacteristics for corpora to serve as training sets and testsets for black-box evaluation of systems and components ofsystems.
The workshop then broke up into a new set ofworking groups to discuss a glass-box evaluation task intro-duced by Bonnie Webber (Penn): syntax, chaired by DickKitteredge (Odyssey Research), semantics, chaired byChristine Montgomery (Language Systems, Inc.), pragmat-ics and discourse, chaired by Candy Sidner (DEC), knowl-edge representation frameworks, chaired by Tim Finin, andsystems, chaired by Lynn Bates.
The final session wasdevoted to reports of the working groups and summariza-tion of results.4 BLACK-BOX EVALUATIONBeth Sundheim (NOSC) proposed a black box evaluationof message understanding systems consisting of a trainingset of 100 messages from a specific domain, and twoseparate test sets, one consisting of 20 messages and an-other of 10.
The performance was to be evaluated withrespect o a frame-filling task.
There was general agree-ment among the workshop participants that useful black-box evaluations can be done for the message understandingand database question-answering task domains.
It was alsoagreed that more general systems aimed at text understand-ing and dialogue understanding were not good candidatesfor black-box evaluation due to the nascent stage of theirdevelopment, although individual components from suchsystems might benefit from evaluation.
The workshop at-tendees were pleasantly surprised by the results of thegeneration group, which came up with a fairly concreteplan for comparing performance of generation systems,based on the message understanding proposal.
A perennialproblem with all of these proposals, with the exception ofthe message understanding proposal, is the lack of funding.Conferences and workshops need to be organized, systemsneed to be ported to the same domain so that they can becompared, etc., and there is very little financial support omake these things possible.4.1 MESSAGE UNDERSTANDING CONFERENCE IIBeth Sundheim's black-box evaluation was in fact carriedout last summer, June 1989, at MUCK II (Message Under-standing Conference II) with quite interesting results (Sund-heim 1989).It quickly became clear how important i was for systemsto be able to handle partial input, a characteristic normallyassociated with usability.
A system that could only handle60 percent of the linguistic phenomena, but could do that ina robust fashion could receive a higher accuracy ratingthan a system that was capable of handling 80 percent ofthe linguistic phenomena, but only under ideal circum-stances.
The overall system performance, including manyfeatures 'that are not directly related to natural anguageprocessing, was a more important factor in the scoring thanthe system's linguistic coverage.
Since these tests are in-tended 12o compare mature systems that are ready for endusers, this is entirely appropriate, and is exactly what theend u,;ers are interested in.
They are not concerned withhow the system arrives at an answer, but simply with theanswer !itself.
However, tests that could provide more infor-mation about how a system achieved its results could be ofmore real utility to the developers of natural languagesystems.5 GLASS-BOX EVALUATIONOne of the primary goals of glass-box evaluations should beproviding uidance to system developers--pinpointing gapsin coverage and imperfections in algorithms.
The glass-boxevaluation task for the workshop, as outlined by BonnieWebber (Penn), consisted of several stages.
The first stagewas to define for each area a range of items that should beevaluated.
The next stage was to determine which items inthe ranlge were suitable for evaluation and which were not.For those that could be evaluated, appropriate methodolo-gies (features and behaviors) and metrics (measures madeon those features and behaviors) were to be specified.
Foritems o:r areas that were not yet ready, there should be anattempt o specify the necessary steps for improving theirsuitability for evaluation.As explained in more detail below, the glass-box method-ology most commonly suggested by the working groups wasblack-box evaluation of a single component.
The area thatseemed the ripest for evaluation was syntax, with semanticsbeing the farthest away from the level of consensus re-quired for general evaluation standards.
Pragmatics anddiscourse heroically managed to specify a range of itemsand suggest a possible black-box evaluation methodologyfor a subset of those items.
Knowledge representationspecified subtopics with associated evaluation techniques.5.1 SYNTAXThe most clearly defined methodology belonged to thesyntax group, and has since taken shape in the form of theTreebank project, which follows many of the guidelinesoriginally suggested by Fred Jelinek.
This project will beable to evaluate syntactic parsers by comparing their out-put with respect o previously determined correct  parseinforrnation--a black-box evaluation of a single compo-nent, i.e., a parser.
The project has recently been estab-lished at the University of Pennsylvania under Mitch Mar-cus and is funded by DARPA, General Electric, and theAir Force.
The goal of the project is to collect a largeamount of data, both written language and spoken lan-guage, which will be divided into training sets and test sets.It involves annotating the data with a polytheoretic syntac-tic structure.
It has been agreed that the annotation in-cludes lexical class labels, bracketing, predicate argument178 Computational Linguistics Volume 16, Number 3, September 1990Martha Palmer and Tim Finin Natural Language Processing Systemsrelationships, and possibly reconstruction f control relation-ships, wh-gaps, and conjunction scope.
Eventually it wouldbe desirable to include co-reference anaphora, preposi-tional phrase attachment, and comparatives, although it isnot clear how to ensure consistent annotation.
People inter-ested in testing the parsing capability of their systemsagainst untried test data could deliver the parsers to the testsite with the ability to map their output into the form of thecorpus annotation for automatic testing.
The test resultscan be returned to parser developers with overall scores aswell as scores broken out by case, i.e., percentage of prepo-sitional phrase bracketings that are correct.5.2 SEMANTICSOne of the special difficulties in attempting to developglass-box evaluation techniques i the lack of agreementover the content of semantic representations.
Most peoplewill agree that predicate argument relations, temporalrelations, and modifiers (including prepositional phraseattachment) count as semantic phenomena, but will notagree on instructions for annotation or methodologies forevaluation.
This is partly because semantics draws on somany diverse areas.
People who are primarily interested inunderlying cognitive structures have been accused of ignor-ing relations to surface syntactic phenomena.
Logicianswho are concentrating on building formal tools have beenaccused of ignoring lexical and cognitive issues, and peopleconcerned with lexical semantics have been accused ofignoring everything but the dictionary.
Some day this willall be brought ogether in peace and harmony, but mean-while there are as many different styles of semantic repre-sentation as there are researchers in the field.
The onlypossible form of comparative valuation must be task-related.
Good performance on such a task might be due toall sorts of factors besides the quality of the semanticrepresentations, so it is not really an adequate discrimina-tor.In the Darpa Spoken Language Workshop in February1989, Martha Palmer suggested three likely steps towardachieving more of a consensus on semantic representations:1.
Agreement on characterization f phenomena.2.
Agreement on mappings from one style of semanticrepresentation to another.3.
Agreement on content of representations for a commondomain.An obvious choice for a common domain would be one ofthe MUCK domains, such as the OPREPS domain re-cently used for MUCK II.
There are several state-of-the-art systems that are performing the same task for the samedomain using quite different semantic representations.
Itwould be useful to take four of these systems, say NYU,SRI, Unisys, and GE, and compare a selected subset oftheir semantic representations i  depth.
It should be possi-ble to define a mapping from one style of semantic represen-tation to another and pinpoint the various strengths andweaknesses of the different approaches.
Another potentialchoice of domain is the Airline Guide domain.
The AirlineGuide task is a spoken language interface to the OfficialAirline Guide, where users can ask the system about flights,air fares, and other types of information about air travel.5.3 PRAGMATICS AND DISCOURSEThe group's basic premise was that they would need a largecorpus annotated with discourse phenomena.
This wouldallow them to evaluate the effect of individual componentsupon the system as a whole and upon other components,such as syntax and semantics.
It would also allow anindividual component's behavior to be observed.
They listedthe discourse phenomena shown below, with the ones forwhich precise annotation i structions could be given markedwith a *.
The others might take a bit more thought.
It wasagreed that the topics for a subsequent meeting wouldinclude experimenting with text annotations and designingtraining sets and test sets.?
turn taking* referring expressions, including anaphora, "do so," re-spectively?
multi-sentence t xt?
sensitivity to user's goals and plans?
model of user's beliefs, goals, intentions, etc.?
use of Gricean maxims?
use of speech acts?
interpretation and use of temporal and causal relation-ships* part/whole, member/set relationships?
vague predicate specification* determination of implicit arguments in predicate-argu-ment relationships?
metaphor and analogy?
schema matching?
varying depth of processing based on certain criteria?
focus of attention and saliency of entities* ellipsis?
style and social attitudes?
deixis5.4 KNOWLEDGE REPRESENTATIONFRAMEWORKSThis group began by pointing out that the knowledgerepresentation a d reasoning (KR&R) services providedfor natural language systems fall into two classes: (1)providing a meaning representation la guage (MRL); and(2) providing inferential services in support of syntactic,semantic, and pragmatic processing.
The group noted thatthe MRL class should probably be broadened to includelanguages for representing dialogues, lexical items, etc.
Inaddition, the group laid out a spectrum of activities, whichare included in a KR&R shown in Figure 4.The group suggested three evaluation methodologies.The first was aimed at evaluating a KR&R system's uit-ability as a meaning representation language.
One way toevaluate a potential MRL is to have a standard set ofComputational Linguistics Volume 16, Number 3, September 1990 179Martha Palmer and Tim Finin Natural Language Processing Systems?
theory  - lJ there an underlying theory which gives meaning to the xa&Rsystem?
What  is known about the expressiveness of the language and thecomputat ional  complexity of its reasoning??
l anguages  - How does the K~t&It system function as u practical languagefor expressing knowledge?
How easy or difficult is it to define certainconcepts or relations or to specify compututions??
sys tems - KIt&X systems are more than just nn implementation of snunderlying theory.
They require good development environments: knowl-edge acquisition tools, debugging tools, interface technology, integrationaids, etc.
How extensive and good is this environment??
bas ic  mode ls  - A KR&R system often comes with some basic, domain-independent modules or models, such as temporal  reasoning, spatial rea-soning, naive physics, etc.
Are such models avai lable and,  i f  they s~e, howextensive and detailed sac they?Figure 4 There are several dimensions along whicha knowledge representation a d reasoning systemmight be evaluated.natural anguage xpressions to try to express in the MRL.This provides an evaluator with some idea of the expressive-ness and conciseness of the KR&R system as an MRL.
Asecond evaluation methodology follows the "Consumer'sReports" paradigm and involves developing a checklist offeatures.
An extensive list of KR&R features could bedeveloped for each of the dimensions given in Figure 4.Scoring how well KR&R systems provide each of thesefeatures provides a way to compare different systems.
Thefinal evaluation technique is to hold a MUCK-like work-shop aimed at evaluating the performance of the NLPsystem's underlying KR&R system.
The group outlined aproposal for organizing a workshop to do an evaluation ofthe KR&R aspects of a natural anguage processing systembased on the MUCK Workshop models.6 WORKSHOP CONCLUSIONSSeveral concerete results came out of the workshop.
Inparticular, a consensus was reached on the black-box evalu-ation task for the second Message Understanding Confer-ence (MUCK II), and a consensus was also reached on thedesirability of a common corpus of annotated language,both written and spoken, that could be used for trainingand testing purposes.
Since the workshop, MUCK II hasbeen held with interesting and useful results, and theTreebank project at the University of Pennsylvania hasreceived funding and has begun.
This should eventuallylead to a more formalized testing and comparisons ofparsers.
Evaluation is becoming a more prevalent topic atNL  workshops, such as the one held at RADC in Septem-ber 1989, and the Darpa Spoken Language Community isworking hard to construct a general evaluation procedurefor the various contractors.
However, most of the otherspecific workshops uggested, such as Database Question-Answering, Generation, Knowledge Representation, andPragmatics and Discourse do not have any funding sourcesavailable.
The most difficult problems remain unresolved.There ,are still large classes of phenomena that have yet tobe characterized in a scholarly fashion, and we do not haveadequa.te methods for measuring progress of a systemunder development.A fundamental underlying snag is the difficulty in arriv-ing al; a consensus on the nature of semantic representation.If the community was in agreement on what the represen-tation of a sentence is supposed to be--whether it was asentence from a dialogue with an expert system, a sentencefragment from a tactical message, or a database query- -then the task of assessing a system's performance would bemuch more straightforward.
Given input X, does the sys-tem produce Y as an internal data structure?
Unfortu-nately, there are now as many Y's for X as there aresystems, so finding a reliable method of assessing a systemin isolation, or of comparing two systems, becomes muchmore clifficult.
It is necessary to define the evaluation interm:; of a task that is being performed (Sundheim 1989;Napier 1989).
Then the system's core with respect tonatural language competence becomes dependent on howwell 'the system as a whole can elicit information from theexpert system or the database, or can summarize theinformation in the message.
Task-oriented black-box evalu-ations are useful and valid, and are certainly of primaryconcern to the end users who need the information, and donot rea.lly care how it is produced.
But there are drawbacksin depending solely on this approach.
A system's capabili-ties cannot be measured or compared until it has beencompletely integrated with a target application.
For anyinteresting application, this requires a major investment ina dornain model and in a domain semantics, not to mentionall of tlhe application-specific needs around user friendlinessand informative displays of information, etc.
Designing thetask-oriented test can require a major investment as well(Sundlheim 1989).
This is an extremely expensive andtime--consuming enterprise that few organizations can in-dulge Jln.
The result is that there are very few systems thatare fully integrated with target applications in such a waythat an appropriate task-oriented evaluation can be per-formed.
There is no way to test whether or not a system issuitab'~te for a particular application without actually build-ing the application.
There are no accepted guidelines thatsystem developers can use to measure the progress beingmade by a fledgling system from month to month.
Grantedthat a task-oriented evaluation is necessary and sufficientfor a system that is ready for end-users, it does not solve theproblem of charting a system's progress along the waytoward a particular application.REFERENCESBall, Catherine N. 1989 Analyzing Explicitly-Structured Discourse inaLimited Domain: Trouble and Failure Reports.
Proceedings of theDARPA Speech and Natural Language Workshop Philadelphia, PA.Brennan, Susan E., Friedman, Marilyn W., and Pollard, Carl.
J.
1987 ACentering Approach to Pronouns.
Proceedings of the 25th AnnualMeeting of the Association for Computational Linguistics, 155-162.180 Computational Linguistics Volume 16, Number 3, September 1990Martha Palmer and Tim Finin Natural Language Processing SystemsGrosz, Barbara, Joshi, Aravind, and Weinstein, Scott.
1983 Providing aUnified Account of Definite Noun Phrases in Discourse.
Proceedings ofthe 21st Annual Meeting of the Association for Computational Linguis-tics, 44-50.Joshi, A. K., and Weinstein, S. 1981 Control of Inference: Centering.Proceedings of the 7th International Conference on Artificial Intelli-gence, 385-387.Napier, H. Albert.
1989 The Impact of a Restricted Natural LanguageInterface on Ease of Learning and Productivity.
Communications ofthe ACM, 32(10): 1190-1198.Sundheim, B. M. 1989 Plans for a Task-Oriented Evaluation of NaturalLanguage Understanding Systems.
Proceedings ofthe DARPA Speechand Natural Language Workshop.
197-202.Webber, Bonnie Lynn.
1988 Discourse Canon.
Presented at the MohonkDarpa Workshop, May, 1988.Computational Linguistics Volume 16, Number 3, September 1990 181
