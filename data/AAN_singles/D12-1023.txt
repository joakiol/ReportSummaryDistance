Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 244?255, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsMinimal Dependency Length in Realization RankingMichael White and Rajakrishnan RajkumarDepartment of LinguisticsThe Ohio State UniversityColumbus, OH, USA{mwhite,raja}@ling.osu.eduAbstractComprehension and corpus studies have foundthat the tendency to minimize dependencylength has a strong influence on constituent or-dering choices.
In this paper, we investigatedependency length minimization in the con-text of discriminative realization ranking, fo-cusing on its potential to eliminate egregiousordering errors as well as better match the dis-tributional characteristics of sentence order-ings in news text.
We find that with a state-of-the-art, comprehensive realization rank-ing model, dependency length minimizationyields statistically significant improvementsin BLEU scores and significantly reducesthe number of heavy/light ordering errors.Through distributional analyses, we also showthat with simpler ranking models, dependencylength minimization can go overboard, too of-ten sacrificing canonical word order to shortendependencies, while richer models manage tobetter counterbalance the dependency lengthminimization preference against (sometimes)competing canonical word order preferences.1 IntroductionIn this paper, we show that for the constituent or-dering problem in surface realization, incorporatinginsights from the minimal dependency length the-ory of language production (Temperley, 2007) into adiscriminative realization ranking model yields sig-nificant improvements upon a state-of-the-art base-line.
We demonstrate empirically using OpenCCG,our CCG-based (Steedman, 2000) surface realiza-tion system, the utility of a global feature encodingthe total dependency length of a given derivation.Although other works in the realization literaturehave used phrase length or head-dependent distancesin their models (Filippova and Strube, 2009; Velldaland Oepen, 2005; White and Rajkumar, 2009, i.a.
),to the best of our knowledge, this paper is the firstto use insights from the minimal dependency lengththeory directly and study their effects, both qualita-tively and quantitatively.The impetus for this paper was the discoverythat despite incorporating a sophisticated syntac-tic model borrowed from the parsing literature?including features with head-dependent distances atvarious scales?White & Rajkumar?s (2009) real-ization ranking model still often performed poorlyon weight-related decisions such as when to em-ploy heavy-NP shift.
Table 1 illustrates this point.In wsj 0034.9, the full model (incorporating numer-ous syntactic features) succeeds in reproducing thereference sentence, which is clearly preferable tothe rather awkward variant selected by the base-line model (using various n-gram models).
How-ever, in wsj 0013.16, the full model fails to shift thetemporal modifier for now next to the phrasal verbturned down, leaving it at the end of its very longverb phrase where it is highly ambiguous (with mul-tiple intervening attachment sites).
Conversely, inwsj 0044.3, the full model shifts before next to theverb, despite the NP cheating being very light, yield-ing a very confusing ordering given that before ismeant to be intransitive.The syntactic features in White & Rajku-mar?s (2009) realization ranking model are takenfrom Clark & Curran?s (2007) normal form model244wsj 0034.9 they fell into oblivion after the 1929 crash .FULL [same]BASELINE they fell after the 1929 crash into oblivion .wsj 0013.16 separately , the Federal Energy Regulatory Commission [V P turned down for now [NP a requestby Northeast [V P seeking approval of [NP its possible purchase of PS of New Hampshire]]]] .FULL separately , the Federal Energy Regulatory Commission [V P turned down [NP a request by North-east [V P seeking approval of [NP its possible purchase of PS of New Hampshire]]] for now] .wsj 0044.3 she had seen cheating before , but these notes were uncanny .FULL she had seen before cheating , but these notes were uncanny .Table 1: Examples of OpenCCG output with White & Rajkumar?s (2009) models?the first represents a successfulcase, the latter two egregious ordering errors(Table 3; see Section 3).
In this model, head-dependenct distances are considered in conjunc-tion with lexicalized and unlexicalized CCG deriva-tion steps, thereby appearing in numerous features.As such, the model takes into account the inter-action of dependency length with derivation steps,but in essence does not consider the main ef-fect of dependency length itself.
In this light,our investigation of dependency length minimiza-tion can be viewed as examining the question ofwhether realization ranking models can be mademore accurate?and in particular, avoid egregiousordering errors?by incorporating a feature to ac-count for the main effect of dependency length.It is important to observe at this point that de-pendency length minimization is more of a prefer-ence than an optimization objective, which must bebalanced against other order preferences at times.A closer reading of Temperley?s (2007) study re-veals that dependency length can sometimes runcounter to many canonical word order choices.
Acase in point is the class of examples involvingpre-modifying adjunct sequences that precede boththe subject and the verb.
Assuming that their par-ent head is the main verb of the sentence, a long-short sequence would minimize overall dependencylength.
However, in 613 examples found in the PennTreebank, the average length of the first adjunct was3.15 words while the second adjunct was 3.48 wordslong, thus reflecting a short-long pattern, as illus-trated in the Temperley p.c.
example in Table 2.Apart from these, Hawkins (2001) shows that argu-ments are generally located closer to the verb thanadjuncts.
Gildea and Temperley (2007) also suggestthat adverb placement might involve cases which goagainst dependency length minimization.
An exam-ination of 295 legitimate long-short post-verbal con-stituent orders (counter to dependency length) fromSection 00 of the Penn Treebank revealed that tem-poral adverb phrases are often involved in long-shortorders, as shown in wsj 0075.13 in Table 2.
In oursetup, the preference to minimize dependency lengthcan be balanced by features capturing preferencesfor alternate choices (e.g.
the argument-adjunct dis-tinction in our dependency ordering model, Table 4).Via distributional analyses, we show that while sim-pler realization ranking models can go overboardin minimizing dependency length, richer modelslargely succeed in overcoming this issue, while stilltaking advantage of dependency length minimiza-tion to avoid egregious ordering errors.2 Background2.1 Minimal Dependency LengthComprehension and corpus studies (Gibson, 1998;Gibson, 2000; Temperley, 2007) point to the ten-dency of production and comprehension systems toadhere to principles of dependency length minimiza-tion.
The idea of dependency length minimizationis based on Gibson?s (1998) Dependency LocalityTheory (DLT) of comprehension, which predictsthat longer dependencies are more difficult to pro-cess.
DLT predictions have been further validatedusing comprehension studies involving eye-trackingcorpora (Demberg and Keller, 2008).
DLT metricsalso correlate reasonably well with activation de-cay over time expressed in computational models of245Temperley (p.c.)
[In 1976], [as a film student at the Purchase campus of the State University of New York], Mr.Lane, shot ...wsj 0075.13 The Treasury also said it plans to sell [$ 10 billion] [in 36-day cash management bills] [onThursday].Table 2: Counter-examples to dependency length minimizationcomprehension (Lewis et al2006; Lewis and Va-sishth, 2005).Extending these ideas from comprehension, Tem-perley (2007) poses the question: Does languageproduction reflect a preference for shorter dependen-cies as well so as to facilitate comprehension?
Bymeans of a study of Penn Treebank data, Temperleyshows that English sentences do display a tendencyto minimize the sum of all their head-dependentdistances as illustrated by a variety of construc-tions.
Further, Gildea and Temperley (2007) reportthat random linearizations have higher dependencylengths compared to actual English, while an ?opti-mal?
algorithm (from the perspective of dependencylength minimization), which places dependents oneither sides of a head in order of increasing length,is closer to actual English.
Tily (2010) also appliesinsights from the above cited papers to show thatdependency length constitutes a significant pressuretowards language change.
For head-final languages(e.g., Japanese), dependency length minimizationresults in the ?long-short?
constituent ordering inlanguage production (Yamashita and Chang, 2001).More generally, Hawkins?s (1994; 2000) processingdomains, dependency length minimization and end-weight effects in constituent ordering (Wasow andArnold, 2003) are all very closely related.
The de-pendency length hypothesis goes beyond the predic-tions made by Hawkins?
Minimize Domains princi-ple in the case of English clauses with three post-verbal adjuncts: Gibson?s DLT correctly predictsthat the first constituent tends to be shorter than thesecond, while Hawkins?
approach does not makepredictions about the relative orders of the first twoconstituents.However, it would be very reductive to considerdependency length minimization as the sole factorin language production.
In fact, a large body ofprior work discusses a variety of other factors in-volved in language production.
These other prefer-ences are either correlated with dependency lengthor can override the minimal dependency length pref-erence.
Complexity (Wasow, 2002; Wasow andArnold, 2003), animacy (Snider and Zaenen, 2006;Branigan et al2008), information status consid-erations (Wasow and Arnold, 2003; Arnold et al2000), the argument-adjunct distinction (Hawkins,2001) and lexical bias (Wasow and Arnold, 2003;Bresnan et al2007) are a few prominent factors.More recently, Anttila et al2010) argued that theprinciple of end weight can be revised by calculat-ing weight in prosodic terms to provide more ex-planatory power.
As Temperley (2007) suggests,a satisactory model should combine insights frommultiple approaches, a theme which we investigatein this work by means of a rich feature set adaptedfrom the parsing and realization literature.
Our fea-ture design has been inspired by the conclusions ofthe above-cited works pertaining to the role of de-pendency length minimization in syntactic choicein conjuction with other factors influencing con-stituent order.
However, going beyond Temper-ley?s corpus study, we confirm the utility of incor-porating a feature for minimizing dependency lengthinto machine-learned models with hundreds of thou-sands of features found to be useful in previous pars-ing and realization work, and investigate the extentto which these features can counterbalance a de-pendency length minimization preference in caseswhere canonical word order considerations shouldprevail.2.2 Surface Realization with CombinatoryCategorial Grammar (CCG)We provide here a brief overview of CCG and theOpenCCG realizer; for further details, see the workscited below.CCG (Steedman, 2000) is a unification-basedcategorial grammar formalism defined almost en-tirely in terms of lexical entries that encode sub-246Feature Type ExampleLexCat + Word s/s/np + beforeLexCat + POS s/s/np + INRule sdcl ?
np sdcl\npRule + Word sdcl ?
np sdcl\np + boughtRule + POS sdcl ?
np sdcl\np + VBDWord-Word ?company, sdcl ?
np sdcl\np, bought?Word-POS ?company, sdcl ?
np sdcl\np, VBD?POS-Word ?NN, sdcl ?
np sdcl\np, bought?Word + ?w ?bought, sdcl ?
np sdcl\np?
+ dwPOS + ?w ?VBD, sdcl ?
np sdcl\np?
+ dwWord + ?p ?bought, sdcl ?
np sdcl\np?
+ dpPOS + ?p ?VBD, sdcl ?
np sdcl\np?
+ dpWord + ?v ?bought, sdcl ?
np sdcl\np?
+ dvPOS + ?v ?VBD, sdcl ?
np sdcl\np?
+ dvTable 3: Basic and dependency features from Clark &Curran?s (2007) normal form model; distances are in in-tervening words, punctuation marks and verbs, and arecapped at 3, 3 and 2, respectivelycategorization as well as syntactic features (e.g.number and agreement).
OpenCCG is a pars-ing/generation library which includes a hybridsymbolic-statistical chart realizer (White, 2006;White and Rajkumar, 2009).
The input to theOpenCCG realizer is a semantic graph, where eachnode has a lexical predication and a set of seman-tic features; nodes are connected via dependency re-lations.
Internally, such graphs are represented us-ing Hybrid Logic Dependency Semantics (HLDS),a dependency-based approach to representing lin-guistic meaning (Baldridge and Kruijff, 2002).
Al-ternative realizations are ranked using integrated n-gram or averaged perceptron scoring models.
In theexperiments reported below, the inputs are derivedfrom the gold standard derivations in the CCGbank(Hockenmaier and Steedman, 2007), and the outputsare the highest-scoring realizations found during therealizer?s chart-based search.13 Feature DesignIn the realm of paraphrasing using tree lineariza-tion, Kempen and Harbusch (2004) explore featureswhich have later been appropriated into classifica-tion approaches for surface realization (Filippovaand Strube, 2007).
Prominent features include in-1The realizer can also be run using inputs derived fromOpenCCG?s parser, though informal experiments suggest thatparse errors tend to decrease generation quality.formation status, animacy and phrase length.
In thecase of ranking models for surface realization, by farthe most comprehensive experiments involving lin-guistically motivated features are reported in workof Cahill for German realization ranking (Cahill etal., 2007; Cahill and Riester, 2009).
Apart fromlanguage model and Lexical Functional Grammar(LFG) c-structure and f -structure based features,Cahill also designed and incorporated features mod-eling information status considerations.The feature sets explored in this paper ex-tend those in previous work on realization rankingwith OpenCCG using averaged perceptron models(White and Rajkumar, 2009; Rajkumar et al2009;Rajkumar and White, 2010) to include more com-prehensive ordering features.
The feature classesare listed below, where DEPLEN, HOCKENMAIERand DEPORD are novel, and the rest are as in ear-lier OpenCCG models.
The inclusion of the DE-PORD features is intended to yield a model with asimilarly rich set of ordering features as Cahill andForster?s (2009) realization ranking model for Ger-man.
Except where otherwise indicated, features areinteger-valued, representing counts of occurrencesin a derivation.DEPLEN The total of the length between all se-mantic heads and dependents for a realization,where length is in intervening words2 exclud-ing punctuation.
For length purposes, collapsednamed entities were counted as a single word inthe experiments reported here.NGRAMS The log probabilities of the word se-quence scored using three different n-grammodels: a trigram word model, a trigramword model with named entity classes replac-ing words, and a trigram model over POS tagsand supertags.HOCKENMAIER As an extra component of thegenerative baseline, the log probability of thederivation according to (a reimplementation2We also experimented with two other definitions of depen-dency length described in the literature, namely (1) countingonly nouns and verbs to approximate counting by discourse ref-erents (Gibson, 1998) and (2) omitting function words to ap-proximate prosodic weight (Anttila et al2010); however, re-alization ranking accuracy was slightly worse than counting allnon-punctuation words.247Feature Type ExampleHeadBroadPos + Rel + Precedes + HeadWord + DepWord ?VB, Arg0, dep, wants, he?.
.
.
+ HeadWord + DepPOS ?VB, Arg0, dep, wants, PRP?.
.
.
+ HeadPOS + DepWord ?VB, Arg0, dep, VBZ, he?.
.
.
+ HeadWord + DepPOS ?VB, Arg0, dep, VBZ, PRP?HeadBroadPos + Side + DepWord1 + DepWord2 ?NN, left, an, important?.
.
.
+ DepWord1 + DepPOS2 ?NN, left, an, JJ?.
.
.
+ DepPOS1 + DepWord2 ?NN, left, DT, important?.
.
.
+ DepPOS1 + DepPOS2 ?NN, left, DT, JJ?.
.
.
+ Rel1 + Rel2 ?NN, left, Det, Mod?Table 4: Basic head-dependent and sibling dependent ordering featuresof) Hockenmaier?s (2003) generative syntacticmodel.DISCRIMINATIVE NGRAMS Sequences from eachof the n-gram models in the perceptron model.AGREEMENT Features for subject-verb and ani-macy agreement as well as balanced punctua-tion.C&C NF BASE The features from Clark & Cur-ran?s (2007) normal form model, listed in Ta-ble 3, minus the distance features.C&C NF DISTANCE The distance features fromthe C&C normal form model, where the dis-tance between a head and its dependent is mea-sured in intervening words, punctuation marksor verbs; caps of 3, 3 and 2 (resp.)
on thedistances have the effect of binning longer dis-tances.DEPORD Several classes of features for orderingheads and dependents as well as sibling depen-dents on the same side of the head.
The ba-sic features?using words, POS tags and de-pendency relations, grouped by the broad POStag of the head?are shown in Table 4.
Thereare also similar features using words and aword class (instead of words and POS tags),where the class is either the named entity class,COLOR for color words, PRO for pronouns,one of 60-odd suffixes culled from the web, orHYPHEN or CAP for hyphenated or capital-ized words.
Additionally, there are features fordetecting definiteness of an NP or PP (wherethe definiteness value is used in place of thePOS tag).Model # Alph Feats # Model FeatsGLOBAL 4 4DEPLEN-GLOBAL 5 5DEPORD-NONF 790,887 269,249DEPORD-NODIST 1,035,915 365,287DEPLEN-NODIST 1,035,916 366,094DEPORD-NF 1,173,815 431,226DEPLEN 1,173,816 428,775Table 6: Model sizes?number of features in alphabet foreach model (satisfying count cutoff of 5) along with num-ber active in model after 5 training epochs4 Evaluation4.1 Experimental ConditionsWe followed the averaged perceptron training proce-dure of White and Rajkumar (2009) with a couple ofupdates.
First, as noted earlier, we used a reimple-mentation of Hockenmaier?s (2003) generative syn-tactic model as an extra component of our genera-tive baseline; and second, only five epochs of train-ing were used, which was found to work as well asusing additional epochs on the development set.
Asin the earlier work, the models were trained on thestandard training sections (02?21) of an enhancedversion of the CCGbank, using a lexico-grammarextracted from these sections.The models tested in the experiments reported be-low are summarized in Table 5.
The three groupsof models are designed to test the impact of thedependency length feature when added to featuresets of increasing complexity.
In more detail,the GLOBAL and DEPLEN-GLOBAL models containdense features on entire derivations; their valuesare the log probabilities of the three n-gram mod-248Model Dep Ngram Hocken- Discr Agree- C&C NF C&C NF DepLen Mods maier Ngrams ment Base Dist OrdGLOBAL N Y Y N N N N NDEPLEN-GLOBAL Y Y Y N N N N NDEPORD-NONF N Y Y Y Y N N YDEPORD-NODIST N Y Y Y Y Y N YDEPLEN-NODIST Y Y Y Y Y Y N YDEPORD-NF N Y Y Y Y Y Y YDEPLEN Y Y Y Y Y Y Y YTable 5: Legend for experimental conditionsels used in the earlier work along with the Hock-enmaier model (and the dependency length feature,in DEPLEN-GLOBAL).
The second group is cen-tered on DEPORD-NODIST, which contains all fea-tures except the dependency length feature and thedistance features in Clark & Curran?s normal formmodel, which may indirectly capture some depen-dency length minimization preferences.
In additionto DEPLEN-NODIST?where the dependency lengthfeature is added?this group also contains DEPORD-NONF, which is designed to test (as a side compari-son) whether the Clark & Curran normal form basefeatures are still useful even when used in conjunc-tion with the new dependency ordering features.
Inthe final group, DEPORD-NF contains all the featuresexamined in this paper except the dependency lengthfeature, while DEPLEN contains all the features in-cluding the dependency length feature.
Note that thelearned weight of the total dependency length fea-ture was negative in each case, as expected.Table 6 shows the sizes of the various models.
Foreach model, the alphabet?whose size increases toover a million features?is the set of applicable fea-tures found to have discriminative value in at least 5training examples; from these, a subset are made ac-tive (i.e., take on a non-zero weight) through percep-tron updates when the feature value differs betweenthe model-best and oracle-best realization.4.2 BLEU ResultsFollowing the usual practice in the realization rank-ing, we first evaluate our results quantitatively us-ing exact matches and BLEU (Papineni et al2002),a corpus similarity metric developed for MT evalu-ation.
Realization results for the development andModel % Exact BLEU SignifSect 00GLOBAL 33.03 0.8292 -DEPLEN-GLOBAL 34.73 0.8345 ***DEPORD-NONF 42.33 0.8534 **DEPORD-NODIST 43.12 0.8560 -DEPLEN-NODIST 43.87 0.8587 ***DEPORD-NF 43.44 0.8590 -DEPLEN 44.56 0.8610 **Sect 23GLOBAL 34.75 0.8302 -DEPLEN-GLOBAL 34.70 0.8330 ***DEPORD-NODIST 41.42 0.8561 -DEPLEN-NODIST 42.95 0.8603 ***DEPORD-NF 41.32 0.8577 -DEPLEN 42.05 0.8596 **Table 7: Development (Section 00) & test (Section 23)set results?exact match percentage and BLEU scores,along with statistical significance of BLEU compared tothe unmarked model in each group (* = p < 0.1, ** =p < 0.05, *** = p < 0.01); significant within-groupwinners (at p < 0.05) are shown in boldtest sections appear in Table 7.
For all three modelgroups, the dependency length feature yields signif-icant increases in BLEU scores, even in compar-ison to the model (DEPORD-NF) containing Clark& Curran?s distance features in addition to the newdependency ordering features (as well as all otherfeatures but total dependency length).
The secondgroup additionally shows that the Clark & Currannormal form base features do indeed have a signif-icant impact on BLEU scores even when used with249Model % DL % DL DL SignifLower Greater MeanGOLD n.a.
n.a.
41.02 -GLOBAL 17.23 21.59 42.40 ***DEPLEN-GLOBAL 24.37 12.81 40.29 ***DEPORD-NONF 15.76 19.34 42.34 ***DEPORD-NODIST 14.58 19.06 42.03 ***DEPLEN-NODIST 17.75 14.82 40.87 n.s.DEPORD-NF 14.96 17.65 41.58 ***DEPLEN 16.28 14.78 40.97 n.s.Table 8: Dependency length compared to corpus?percentage of realizations with dependency length lessthan and greater than gold standard, along with meandependency length, whose significance is tested againstgold; 1671 development set (Section 00) complete real-izations analyzedthe new dependency ordering model, as DEPORD-NONF is significantly worse than DEPORD-NODIST(the impact of the distance features is evident in theincreases from the second group to the third group).As with the dev set, the dependency length featureyielded a significant increase in BLEU scores foreach comparison on the test set alFor each group, the statistical significance of thedifference in BLEU scores between a model and theunmarked model (-) is determined by bootstrap re-sampling (Koehn, 2004).3 Note that although thedifferences in BLEU scores are small, they endup being statistically significant because the mod-els frequently yield the same top scoring realiza-tion, and reliably deliver improvements in the caseswhere they differ.
In particular, note that DEPLENand DEPORD-NF agree on the best realization 81%of the time, while DEPLEN-NODIST and DEPORD-NODIST have 78.1% agreement, and DEPLEN-GLOBAL and GLOBAL show 77.4% agreement; bycomparison, DEPORD-NODIST and GLOBAL onlyagree on the best realization 51.1% of the time.4.3 Detailed AnalysesThe effect of the dependency length feature on thedistribution of dependency lengths is illustrated inTable 8.
The table shows the mean of the total de-pendency length of each realized derivation com-3Kudos to Kevin Gimpel for making his resamplingscripts available from http://www.ark.cs.cmu.edu/MT/paired_bootstrap_v13a.tar.gz.Model % Short % Long % Eq % Single/ Long / Short ConstitGOLD 25.25 4.87 4.08 65.79GLOBAL 23.15 7.86 3.94 65.04DEPLEN-GLOBAL 24.58 5.57 4.09 65.76DEPORD-NONF 23.13 6.61 4.03 66.23DEPORD-NODIST 23.38 6.52 3.94 66.15DEPLEN-NODIST 24.03 5.38 4.01 66.58DEPORD-NF 23.74 5.92 3.96 66.40DEPLEN 24.36 5.36 4.07 66.21Table 9: Distribution of various kinds of post-verbal con-stituents in the development set (Section 00); 4692 goldcases consideredpared to the corresponding gold standard derivation,as well as the number of derivations with greater andlower dependency length.
According to paired t-tests, the mean dependency lengths for the DEPLEN-NODIST and DEPLEN models do not differ signifi-cantly from the gold standard.
In contrast, the meandependency length of all the models that do not in-clude the dependency length feature does differ sig-nificantly (p < 0.001) from the gold standard.
Ad-ditionally, all these models have more realizationswith dependency length greater than the gold stan-dard, in comparison to the dependency length min-imizing models; this shows the efficacy of the de-pendency length feature in approximating the goldstandard.
Interestingly, the DEPLEN-GLOBAL modelsignificantly undershoots the gold standard on meandependency length, and has the most skewed dis-tribution of sentences with greater vs. lesser depen-dency length than the gold standard.Apart from studying dependency length directly,we also looked at one of the attested effects of de-pendency length minimization, viz.
the tendency toprefer short-long post-verbal constituents in produc-tion (Temperley, 2007).
The relative lengths of ad-jacent post-verbal constituents were computed andtheir distribution is shown in Table 9.
While cal-culating length, punctuation marks were excluded.Four kinds of constituents were found in the post-verbal domain.
For every verb, apart from singleconstituents and equal length constituents, short-long and long-short sequences were also observed.Table 9 demonstrates that for both the gold standardcorpus as well as the realizer models, short-longconstituents were more frequent than long-short orequal length constituents.
This follows the trend re-250Model % Light % Heavy Signif/ Heavy / LightGOLD 8.60 0.36 -GLOBAL 7.73 2.02 ***DEPLEN-GLOBAL 8.35 0.75 **DEPORD-NONF 7.98 1.15 ***DEPORD-NODIST 8.04 1.12 ***DEPLEN-NODIST 8.23 0.45 n.s.DEPORD-NF 8.26 0.71 **DEPLEN 8.36 0.51 n.s.Table 10: Distribution of heavy unequal constituents(length difference > 5) in Section 00; 4692 gold casesconsidered and significance tested against the gold stan-dard using a ?-square testported by previous corpus studies of English (Tem-perley, 2007; Wasow and Arnold, 2003).
The figuresreported here show the tendency of the DEPLEN*models to be closer to the gold standard than theother models, especially in the case of short-longconstituents.We also performed an analysis of relative con-stituent lengths focusing on light-heavy and heavy-light cases; specifically, we examined unequallength constituent sequences where the length dif-ference of the constituents was greater than 5, andthe shorter constituent was under 5 words.
Table 10shows the results.
Using a ?-square test, the distri-bution of heavy unequal length constituent counts inthe DEPLEN-NODIST and DEPLEN models does notsignificantly differ from that of the gold standard.
Incontrast, for all the other models, the counts do dif-fer significantly from the gold standard.4.4 ExamplesTable 11 shows examples of how the dependencylength feature (DEPLEN) affects the output even incomparison to a model (DEPORD) with a rich setof discriminative syntactic and dependency order-ing features, but no features directly targeting rel-ative weight.
In wsj 0015.7, the dependency lengthmodel produces an exact match, while the DEPORDmodel fails to shift the short temporal adverbial nextyear next to the verb, leaving a confusingly repeti-tive this year next year at the end of the sentence.In wsj 0020.1, the dependency length model pro-duces a nearly exact match with just an equally ac-ceptable inversion of closely watching.
By contrast,the DEPORD model mistakenly shifts the direct ob-ject South Korea, Taiwan and Saudia Arabia to theend of the sentence where it is difficult to under-stand following two very long intervening phrases.In wsj 0021.8, both models mysteriously put not infront of the auxiliary and leave out the complemen-tizer, but DEPORD also mistakenly leaves before atthe end of the verb phrase where it is again apt tobe interpreted as modifying the preceding verb.
Inwsj 0075.13, both models put the temporal modi-fier on Thursday in its canonical VP-final position,despite this order running counter to dependencylength minimization.
Finally, wsj 0014.2 shows acase where DEPORD is nearly an exact match (exceptfor a missing comma), but the dependency lengthmodel fronts the PP on the 12-member board, whereit is grammatical but rather marked (and not moti-vated in the discourse context).4.5 Interim DiscussionThe experiments show a consistent positive effect ofthe dependency length feature in improving BLEUscores and achieving a better match with the corpusdistributions of dependency length and short/longconstituent orders.
The results in Table 10 are partic-ulary encouraging, as they show that minimizing de-pendency length reduces the number of realizationsin which a heavy constituent precedes a light onedown to essentially the level of the corpus, therebyeliminating many realizations that can be expectedto have egregious errors like those shown in Ta-ble 11.Intriguingly, there is some evidence that a nega-tively weighted total dependency length feature cango too far in minimizing dependency length, in theabsence of other informative features to counterbal-ance it.
In particular, the DEPLEN-GLOBAL model inTable 8 has significantly lower dependency lengththan the corpus, but in the richer models with dis-criminative synactic and dependency ordering fea-tures, there are no significant differences.
It may stillbe though that additional features are necessary tocounteract the tendency towards dependency lengthminimization, for example to ensure that initial con-stituents play their intended role in establishing andcontinuing topics in discourse, as also observed inTable 11.251wsj 0015.7 the exact amount of the refund will be determined next year based on actual collections made untilDec.
31 of this year .DEPLEN [same]DEPORD the exact amount of the refund will be determined based on actual collections made until Dec. 31of this year next year .wsj 0020.1 the U.S. , claiming some success in its trade diplomacy , removed South Korea , Taiwan and SaudiArabia from a list of countries it is closely watching for allegedly failing to honor U.S. patents ,copyrights and other intellectual-property rights .DEPLEN the U.S. claiming some success in its trade diplomacy , removed South Korea , Taiwan and SaudiArabia from a list of countries it is watching closely for allegedly failing to honor U.S. patents ,copyrights and other intellectual-property rights .DEPORD the U.S. removed from a list of countries it is watching closely for allegedly failing to honor U.S.patents , copyrights and other intellectual-property rights , claiming some success in its trade diplo-macy , South Korea , Taiwan and Saudi Arabia .wsj 0021.8 but he has not said before that the country wants half the debt forgiven .DEPLEN but he not has said before ?
the country wants half the debt forgiven .DEPORD but he not has said ?
the country wants half the debt forgiven before .wsj 0075.13 The Treasury also said it plans to sell [$ 10 billion] [in 36-day cash management bills] [on Thurs-day].DEPLEN [same]DEPORD [same]wsj 0014.2 they succeed Daniel M. Rexinger , retired Circuit City executive vice president , and Robert R.Glauber , U.S. Treasury undersecretary , on the 12-member board .DEPORD they succeed Daniel M. Rexinger , retired Circuit City executive vice president , and Robert R.Glauber , U.S. Treasury undersecretary ?
on the 12-member board .DEPLEN on the 12-member board they succeed Daniel M. Rexinger , retired Circuit City executive vicepresident , and Robert R. Glauber , U.S. Treasury undersecretary .Table 11: Examples of realized output for full models with and without the dependency length feature4.6 Targeted Human EvaluationTo determine whether heavy-light ordering differ-ences often represent ordering errors (includingegregious ones), rather than simply representing ac-ceptable variation, we conducted a targeted humanevaluation on examples of this kind.
Specifically,for each of the DEPLEN* models and their corre-sponding models without the dependency length fea-ture, we chose the 25 sentences from the develop-ment section whose realizations exhibited the great-est difference in dependency length between siblingconstituents appearing in opposite orders, and askedtwo judges (not the authors) to choose which of thetwo realizations best expressed the meaning of thereference sentence in a grammatical and fluent way,with the choice forced (2AFC).
Table 12 shows theresults.
Agreement between the judges was high,Model % Preferred % Agr SignifGLOBAL 22 - -DEPLEN-GLOBAL 78 84 ***DEPORD-NODIST 24 - -DEPLEN-NODIST 76 92 ***DEPORD-NF 26 - -DEPLEN 74 96 ***Table 12: Targeted human evaluation?percentage of re-alizations preferred by two human judges in a 2AFC testamong the 25 development set sentences with the great-est differences in dependency length, with a binomial testfor significance252with only one disagreement on the realizations fromthe DEPLEN and DEPORD-NF models (involving anacceptable paraphrase in our judgment), and onlyfour disagreements on the DEPLEN-GLOBAL andGLOBAL realizations.
Pooling the judgments, thepreference for the DEPLEN* models was well abovethe chance level of 50% according to a binomial test(p < 0.001 in each case).
Inspecting the data our-selves, we found that many of the items did indeedinvolve egregious ordering errors that the DEPLEN*models managed to avoid.5 Related WorkAs noted in the introduction, to the best of ourknowledge this paper is the first to examine the im-pact of dependency length minimization on realiza-tion ranking.
While there have been quite a fewpapers to date reporting results on Penn Treebankdata, since the various systems make different as-sumptions regarding the specificity of their inputs,all but the most broad-brushed comparisons remainimpossible at present, and thus detailed studies suchas the present one can only be made within the con-text of different models for the same system.
Someprogress on this issue has been made in the con-text of the Generation Challenges Surface Realiza-tion Shared Task (Belz et al2011), but it remainsto be seen to what extent fair cross-system compar-isons using common inputs can be achieved.For (very) rough comparison purposes, Table 13lists our results in the context of those reported forvarious other systems on PTB Section 23.
As thetable shows, the OpenCCG scores are quite com-petitive, exceeded only by Callaway?s (2005) ex-tensively hand-crafted system as well as Bohnet etal.
?s (2011) system on shared task shallow inputs(-S), which performs much better than their sys-tem on deep inputs (-D) that more closely resembleOpenCCG?s.6 ConclusionsIn this paper, we have investigated dependencylength minimization in the context of realizationranking, focusing on its potential to eliminate egre-gious ordering errors as well as better match the dis-tributional characteristics of sentence orderings innews text.
When added to a state-of-the-art, com-System Coverage BLEU % ExactCallaway (05) 98.5% 0.9321 57.5Bohnet et al (11) 100% 0.8911OpenCCG (12) 97.1% 0.8596 42.1OpenCCG (09) 97.1% 0.8506 40.5Ringger et al04) 100% 0.836 35.7Bohnet et al (11) 100% 0.7943Langkilde-Geary (02) 83% 0.757 28.2Guo et al08) 100% 0.7440 19.8Hogan et al07) ?100% 0.6882OpenCCG (08) 96.0% 0.6701 16.0Nakanishi et al05) 90.8% 0.7733Table 13: PTB Section 23 BLEU scores and exact matchpercentages in the NLG literature (Nakanishi et al re-sults are for sentences of length 20 or less)prehensive realization ranking model, we showedthat including a dense, global feature for minimiz-ing total dependency length yields statistically sig-nificant improvements in BLEU scores and signif-icantly reduces the number of heavy-light orderingerrors.
Going beyond the BLEU metric, we alsoconducted a targeted human evaluation to confirmthe utility of the dependency length feature in mod-els of varying richness.
Interestingly, even with therichest model, in some cases we found that the de-pendency length feature still appears to go too far inminimizing dependency length, suggesting that fur-ther counter-balancing features?especially ones forthe sentence-initial position (Filippova and Strube,2009)?warrant investigation in future work.AcknowledgmentsThis work was supported in part by NSF grants no.IIS-1143635 and IIS-0812297.
We thank the anony-mous reviewers for helpful comments and discus-sion, and Scott Martin and Dennis Mehay for theirparticipation in the targeted human evaluation.253ReferencesArto Anttila, Matthew Adams, and Mike Speriosu.
2010.The role of prosody in the English dative alternation.Language and Cognitive Processes.Jennifer E. Arnold, Thomas Wasow, Anthony Losongco,and Ryan Ginstrom.
2000.
Heaviness vs. newness:The effects of structural complexity and discourse sta-tus on constituent ordering.
Language, 76:28?55.Jason Baldridge and Geert-Jan Kruijff.
2002.
CouplingCCG and Hybrid Logic Dependency Semantics.
InProc.
ACL-02.Anja Belz, Mike White, Dominic Espinosa, Eric Kow,Deirdre Hogan, and Amanda Stent.
2011.
Thefirst surface realisation shared task: Overview andevaluation results.
In Proceedings of the Genera-tion Challenges Session at the 13th European Work-shop on Natural Language Generation, pages 217?226, Nancy, France, September.
Association for Com-putational Linguistics.Bernd Bohnet, Simon Mille, Beno?
?t Favre, and Leo Wan-ner.
2011.
<stumaba >: From deep representation tosurface.
In Proceedings of the Generation ChallengesSession at the 13th European Workshop on NaturalLanguage Generation, pages 232?235, Nancy, France,September.
Association for Computational Linguis-tics.H Branigan, M Pickering, and M Tanaka.
2008.
Con-tributions of animacy to grammatical function assign-ment and word order during production.
Lingua,118(2):172?189.Joan Bresnan, Anna Cueni, Tatiana Nikitina, and R. Har-ald Baayen.
2007.
Predicting the Dative Alternation.Cognitive Foundations of Interpretation, pages 69?94.Aoife Cahill and Arndt Riester.
2009.
Incorporating in-formation status into generation ranking.
In Proceed-ings of, ACL-IJCNLP ?09, pages 817?825, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Aoife Cahill, Martin Forst, and Christian Rohrer.
2007.Designing features for parse disambiguation and real-isation ranking.
In Miriam Butt and Tracy HollowayKing, editors, Proceedings of the 12th InternationalLexical Functional Grammar Conference, pages 128?147.
CSLI Publications, Stanford.Charles Callaway.
2005.
The types and distributionsof errors in a wide coverage surface realizer evalua-tion.
In Proceedings of the 10th European Workshopon Natural Language Generation.Stephen Clark and James R. Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Vera Demberg and Frank Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109(2):193?210.Katja Filippova and Michael Strube.
2007.
Generatingconstituent order in German clauses.
In ACL 2007,Proceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics, June 23-30,2007, Prague, Czech Republic.
The Association forComputer Linguistics.Katja Filippova and Michael Strube.
2009.
Tree lin-earization in English: Improving language modelbased approaches.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, Companion Volume: ShortPapers, pages 225?228, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Edward Gibson.
1998.
Linguistic complexity: Localityof syntactic dependencies.
Cognition, 68:1?76.Edward Gibson.
2000.
Dependency locality theory:A distance-based theory of linguistic complexity.
InAlec Marantz, Yasushi Miyashita, and Wayne O?Neil,editors, Image, Language, brain: Papers from the FirstMind Articulation Project Symposium.
MIT Press,Cambridge, MA.Daniel Gildea and David Temperley.
2007.
Optimizinggrammars for minimum dependency length.
In Pro-ceedings of the 45th Annual Meeting of the Associationof Computational Linguistics, pages 184?191, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Yuqing Guo, Josef van Genabith, and Haifeng Wang.2008.
Dependency-based n-gram models for generalpurpose sentence realisation.
In Proc.
COLING-08.John A. Hawkins.
1994.
A Performance Theory of Orderand Constituency.
Cambridge University Press, NewYork.John A. Hawkins.
2000.
The relative order ofprepositional phrases in English: Going beyondmanner-place-time.
Language Variation and Change,11(03):231?266.John A. Hawkins.
2001.
Why are categories adjacent?Journal of Linguistics, 37:1?34.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and DependencyStructures Extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Julia Hockenmaier.
2003.
Data and models for statis-tical parsing with Combinatory Categorial Grammar.Ph.D.
thesis, University of Edinburgh.Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josefvan Genabith.
2007.
Exploiting multi-word unitsin history-based probabilistic generation.
In Proc.EMNLP-CoNLL.254Gerard Kempen and Karin Harbusch.
2004.
Generat-ing natural word orders in a semi-free word order lan-guage: Treebank-based linearization preferences forGerman.
In Alexander F. Gelbukh, editor, CICLing,volume 2945 of Lecture Notes in Computer Science,pages 350?354.
Springer.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.Irene Langkilde-Geary.
2002.
An empirical verificationof coverage and correctness for a general-purpose sen-tence generator.
In Proc.
INLG-02.R.
L. Lewis and S. Vasishth.
2005.
An activation-basedmodel of sentence processing as skilled memory re-trieval.
Cognitive Science, 29:1?45, May.Richard L. Lewis, Shravan Vasishth, and Julie Van Dyke.2006.
Computational principles of working memoryin sentence comprehension.
Trends in Cognitive Sci-ences, 10(10):447?454.Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic methods for disambiguation of anHPSG-based chart generator.
In Proc.
IWPT-05.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
ACL-02.Rajakrishnan Rajkumar and Michael White.
2010.
De-signing agreement features for realization ranking.In Coling 2010: Posters, pages 1032?1040, Beijing,China, August.
Coling 2010 Organizing Committee.Rajakrishnan Rajkumar, Michael White, and DominicEspinosa.
2009.
Exploiting named entity classes inCCG surface realization.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, Companion Volume: ShortPapers, pages 161?164, Boulder, Colorado, June.
As-sociation for Computational Linguistics.Eric Ringger, Michael Gamon, Robert C. Moore, DavidRojas, Martine Smets, and Simon Corston-Oliver.2004.
Linguistically informed statistical models ofconstituent structure for ordering in sentence realiza-tion.
In Proc.
COLING-04.Neal Snider and Annie Zaenen.
2006.
Animacy and syn-tactic structure: Fronted NPs in English.
In M. Butt,M.
Dalrymple, and T.H.
King, editors, Intelligent Lin-guistic Architectures: Variations on Themes by RonaldM.
Kaplan.
CSLI Publications, Stanford.Mark Steedman.
2000.
The Syntactic Process.
MITPress.David Temperley.
2007.
Minimization of dependencylength in written English.
Cognition, 105(2):300 ?333.Harry Tily.
2010.
The Role of Processing Complexityin Word Order Variation and Change.
Ph.D. thesis,Stanford University.Erik Velldal and Stefan Oepen.
2005.
Maximum entropymodels for realization ranking.
In Proc.
MT-SummitX.Thomas Wasow and Jennifer Arnold.
2003.
Post-verbalConstituent Ordering in English.
Mouton.Tom Wasow.
2002.
Postverbal Behavior.
CSLI Publica-tions, Stanford.Michael White and Rajakrishnan Rajkumar.
2009.
Per-ceptron reranking for CCG realization.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 410?419, Singapore,August.
Association for Computational Linguistics.Michael White.
2006.
Efficient Realization of Coordi-nate Structures in Combinatory Categorial Grammar.Research on Language & Computation, 4(1):39?75.Hiroko Yamashita and Franklin Chang.
2001.
?Longbefore short?
preference in the production of a head-final language.
Cognition, 81.255
