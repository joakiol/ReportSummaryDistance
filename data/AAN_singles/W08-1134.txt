NIL-UCM: Most-Frequent-Value-First Attribute Selection andBest-Scoring-Choice RealizationPablo Gerva?s, Raquel Herva?s, Carlos Leo?nNatural Interaction based on Language (NIL)Universidad Complutense de Madridc/ Profesor Jose?
Garc?
?a Santesmases s/n, 28040 Madrid, Spainpgervas@sip.ucm.es, raquelhb@fdi.ucm.es, cleon@fdi.ucm.es1 IntroductionThe NIL entry for the challenge has been con-structed upon the general architecture for develop-ing Natural Language Generation systems providedby the TAP project (Gerva?s, 2007).
TAP (Text Ar-ranging Pipeline) is a set of interfaces that definegeneric functionality for a pipeline of tasks orientedtoward natural language generation, from an initialconceptual input to surface realization as a string,with intervening stages of content planning and sen-tence planning.The TAP architecture considers three basic stages:content planning, sentence planning and surface re-alization.
Of these, the first stage is not relevant tothe challenge tasks.
The configuration choices ap-plied to the other two stages to adapt them to thechallenge tasks are described below.2 NIL-UCM-MFVF Entry for Task 1The NIL-UCM-MFVF for Task 1 applies a Most-Frequent-Value-First method for Attribute Selec-tion.
Of the five evaluation dimensions consideredin this challenge (Dice, MASI, accuracy, minimal-ity and uniqueness), this method has been designedto address explicitly only three: Dice, MASI anduniqueness.
Minimality was abandoned in view ofresults in previous challenges (Herva?s and Gerva?s,2007) that showed good minimality results tended toproduce low Dice scores.
We have also opted for notusing accuracy evaluation to fit the performance ofour system, since the corpus contains a wide rangeof style of reference and we are interested in pro-viding our system with only a subset of these thatensure correct identification.2.1 Most-Frequent-Value-First AttributeSelectionThe selection algorithm employed is an adapta-tion of the algorithm described in (Reiter and Dale,1992).
The original algorithm has been modified toallow for a dynamically changing list of preferredattributes, which determine the particular order inwhich attributes are considered to generate the dis-tinguishing expression.
This list is constructed dy-namically for each reference by computing the prob-ability of occurrence in the corpus of the particu-lar attribute-value pairs associated with the referent,and using those probabilities to rank them into a spe-cific list of preferred attributes.
The idea is that at-tributes should be considered in a particular orderdepending highly on their values.
For example, inthe people domain we have observed that almostthe 100% of the target entities that have beard (at-tribute has value 1) are referred using the attributehasBeard, but when this attribute has value 0 it isnever used.
For the hasHair attribute, the oppositeseems to be the case (mentioned only when lacking).The training data was studied to obtain the prob-ability of occurrence of an attribute given a certainvalue for it.
This probability was calculated usingFormula 1:probvali =?appsV alueInAttSet?appsV alueInTarget (1)For each possible value of each of the attributesof the domains, the sum of the appearances of thisvalue in the ATTRIBUTE-SET elements (appsVal-ueInAttSet) and the sum of the appearances of thisvalue in the attributes of all targets (appsValueInTar-get) are calculated.
The division of these two valuesis the probability of mentioning an attribute when ithas a specific value.215Dice MASI Accuracy Uniqueness MinimalityTrain.
Furniture 79,18% 56,95% 41,69% 100% 0%People 69,71% 42,41% 22,99% 100% 0%Both 74,80% 50,23% 34,81% 100% 0%Dev.
Furniture 77,55% 53,97% 41,25% 100% 0%People 70,86% 42,59% 22,06% 100% 0%Both 74,48% 48,75% 32,43% 100% 0%Table 1:Task 1 results for training and development dataSome examples of the results obtained are that theattribute hasGlasses is mentioned in the 60% ofthe situations when its value is 1, and in the 0% ofthe situations when its value is 0.
On the contrary,the attribute hasShirt is almost never mentioned(0.8% when its value is 1 and 0% with value 0).The only exception in the algorithm is the typeattribute for the people domain.
As every entity inthis domain is of type person, the attribute selectordoes not choose this attribute because no distractoris discarded by it.
However, the experiments haveshown us that in the corpus a lot of descriptions in-clude the type person even when it is redundant.Following this idea, our algorithm always includesthe type in the list of chosen attributes for the peo-ple domain.12.2 Obtained ResultsResults obtained over the training and developmentdata are shown in Table 1.
As can be seen com-paring both tables there are no surprises in the finalresults: the system gets similar results with both do-mains and with both the training and developmentdata.
These results confirm that the probability ofappearance of an attribute depending on its value ismore or less the same in the whole corpus.3 NIL-UCM-BSC Entry for Task 2The NIL-UCM-BSC for Task 2 applies a Best-Scoring-Choice approach to Realization.The realization tasks of the 2008 GRE challengerequired specific instantiations of the Referring Ex-1We have only recently discovered that the surprising differ-ence between NIL-UCM results for the people and the furnituredomains in the 2007 GRE challenge was the mostly due to ournot having taken this issue into account at the time.
The effectis noticeable only when the type attribute is redundant, as it isin the people domain.pression Generation, Syntactic Choice, and Lexical-ization stages of the Sentence Planning module ofTAP, and it draws on the SurReal (Gerva?s, 2006) sur-face realization module.
SurReal provides a Java im-plementation of the surface realization mechanismsof FUF described in Elhadad (Elhadad, 1993), op-erating over a grammar which follows the notationalconventions of the SURGE grammar in Elhadad (El-hadad and Robin, 1996), but it is not systemic in na-ture.
It currently has much smaller coverage than theoriginal, but quite sufficient to deal with the kind ofrealizations required for the challenge tasks.3.1 Realization Choices in the CorpusAn analysis of the domain was carried out to ascer-tain what the various alternatives required for real-ization were for the given corpus, both in terms ofhow to realize syntactically the different conceptsand what alternative lexicalizations should be con-sidered.
With respect to linguistic variation in theform of expression we have distinguished betweenchoices that give rise to different syntactic struc-tures (which we consider as syntactic choices) andchoices which give rise to the same syntactic struc-tures but with different lexical items (which we con-sider as lexical choices).With respect to the Referring Expression Genera-tion stage, the following issues required specific de-cisions.
The use of determiners is erratic.
Someexamples in the corpus use indefinite article, someuse definite articles, and some omit the determin-ers altogether.
The corpus shows many cases wherespatial expressions describing the location of refer-ents are used, many using different systems of refer-ence (north-south vs. top-bottom).
The use of par-ticular features of the object in its description, asin ?the desk with the drawers facing the viewer?
or?the chair with the seat facing away?.
Comparison216with all or some of the distractors are also used, ei-ther as adjuncts describing their position relative toother distractors, as in ?the blue fan next to the greenfan?, or as comparative adjectives used for particu-lar attributes, as in ?the largest red couch?
(and evencombinations of the two as in ?the smaller of the twoblue fans?).
Finally, there are samples in the corpusof use of ellipsis and ungrammatical expressions.The mention of particular features and the use ofcomparison would involve operating on more datathan are generated in task 1, and the current sub-mission is aimed to interconnection with task 2 foraddressing task 3.
The issue of ungrammaticality isimportant since it implies that there is an upper limitto the possible scores that the system may achieveover the corpus under the circumstances, totally un-related with the correctness of the generated expres-sions.With respect to Syntactic Choice, some attributesshow more than one possible option for syntac-tic realization.
The number of alternatives variesfrom color (?grey chair - chair that is gray?
), throughbeards (?with beard - with the beard - with whiskers- the bearded man - with a beard - with facial hair?
)to orientation (12 different syntactic alternatives forexpressing orientation: back).There are slight variations of Lexical Choice overthe corpus, as in ?sofa - couch - settee - loveseat?,?ventilator - fan - windmill?
or ?man - guy - bloke?
(for nouns) and ?large - big?
or ?small - little?
(foradjectives).
Because it has a significant impact onthe edit distance measure, it is also important to con-sider the existence of a large number of misspellingsin the corpus.
Finally, there are some conceptualmismatches in annotation, between the attributeset and the given realization in some cases (?purple- blue?, ?black and white - grey?,...
).3.2 Best Scoring Choice SolutionThe solution employed in the present submissionfor selecting among the features described aboveimplements straight forward realization rather thanchoice, in the sense in which (Cahill, 1998) uses theterms for lexicalization.
To implement real choicethe system would have to consider more than one al-ternative for a specific feature and to select one ofthem based on some criteria.
This has not been donein the present submission.
Instead, a single alterna-tive has been implemented for each feature, usingit consistently across all samples.
The selection ofwhich particular alternative to implement has beendone empirically to ensure the best possible scoreover the training corpus.3.3 Results and DiscussionResults obtained over the training and developmentdata are shown in Table 2.SE distance AccuracyTrain.
Furniture 4,26 14,15%People 5,43 9,12%Both 4,8 11,82%Dev.
Furniture 4,21 15%People 4,94 7,35%Both 4,54 11,48%Table 2:Task 2 results for training and development dataAn important point to consider with respect tothe current submission is whether a solution im-plementing real choice would have obtained bet-ter results.
Such a solution might have benefitedfrom the information that can be extracted from theANNOTATED-WORD-STRING to train a decisionprocedure on the various features.
This has not beenaddressed in the present submission more for lack oftime than lack of conviction on its merit.Addressing explicitly some of the possible con-structions that are described in section 3.1 may alsohave a positive effect on the results.4 NIL-UCM-FVBS Entry for Task 3The NIL-UCM-FVBS entry for Task 3 appliesa combination of the Most-Frequent-Value-Firstmethod for Attribute Selection and the Best-Scoring-Choice approach to Realization.The modular architecture of TAP has allowedeasy integration for Task 3 of the solution for at-tribute selection described in section 2, and the so-lution for realization described in section 3.4.1 Results and DiscussionResults obtained over the training and developmentdata are shown in Table 3.
Comparing both sets ofresults there are no surprises in the final results: thesystem gets similar results with both domains and217with both the training and development data.
Theseresults confirm that the probability of appearance ofan attribute depending on its value is more or lessthe same in the whole corpus.SE distance AccuracyTrain.
Furniture 5,03 5,03%People 6,11 5,47%Both 5,53 5,24%Dev.
Furniture 5,06 3,75%People 6,24 1,47%Both 5,60 2,70%Table 3:Task 3 results for training and development dataThe results obtained are a bit lower than the onesobtained by both the attribute selection and realiza-tion submodules separately.
This is not an unex-pected result.
Bad choices produced in the attributeselection are propagated through the realization, re-sulting in accumulated errors in the final evaluation.However, there are additional shortcomings thatarise from considering the general goal of task 3as a composition of task 2 over task 1.
The re-duction of the types of expression produced by hu-man subjects to a set of attributes involves in somecases a certain loss of information.
This is par-ticularly the case when the human-produced ex-pressions involve attributes for which additional in-formation is provided.
This can be seen if theANNOTATED-WORD-STRING is compared withthe actual attribute set generated for some of thehuman-produced expressions.
For instance, the cor-pus contains examples in which the hasBeard at-tribute has a nested attribute that indicates the beardis white.
Other examples provide color informationon pieces of clothing worn.
This information is lostto the realization stage if the data have to go throughtask 1, which reduces the available format to a set ofindividual unstructured attributes.Considering a version of task 3 that allowed fullrealization directly from input data as considered fortask 1, with no requirements on the stages of inter-mediate representation to be employed in the pro-cess, may result in a richer range of realizations, andpossibly in improved performance with respect tohuman evaluation.In more general terms, it seems that the corpusdoes contain adequate data for informing systemperformance at the level of sentence planning sub-tasks such as lexical choice or syntactic choice.
Nev-ertheless, some of the variations in the corpus, suchas the free use of determiners or the flexibility thatsubjects exhibit in the way they refer to the imagesdo introduce a certain ?noise?.
Instances of these oc-cur when human-produced descriptions involve in-tense forms of ellipsis, and agrammatical orderingof attributes.
Some of these might be reduced if a re-fined version of the corpus were produced with morecontrol on the experimental settings, to ensure thatsubjects either described the elements as images oras the things represented in the images, for instance.AcknowledgmentsThis research is funded by the Spanish Ministryof Education and Science (TIN2006-14433-C02-01project) and the UCM and the Direccio?n General deUniversidades e Investigacio?n of the CAM (CCG07-UCM/TIC-2803).ReferencesCahill, Lyne.
1998.
Lexicalisation in applied NLG sys-tems.
Technical Report ITRI-99-04.Elhadad, Michael.
1993.
Technical Report CUCS-038-91.
Columbia University.Elhadad, Michael and Robin, Jacques.
1996.
TechnicalReport 96-03.
Department of Computer Science, BenGurion University.Gerva?s, Pablo.
2006.
SurReal: a Surface Realiza-tion module.
Natural Interaction based on LanguageGroup Technical Report, Universidad Complutense deMadrid, Spain.Gerva?s, Pablo.
2007.
TAP: a Text Arranging Pipeline.Natural Interaction based on Language Group Tech-nical Report, Universidad Complutense de Madrid,Spain.Herva?s, Raquel and Gerva?s, Pablo.
2007.
NIL: AttributeSelection for Matching the Task Corpus Using Rela-tive Attribute Groupings Obtained from the Test Data.First NLG Challenge on Attribute Selection for Gener-ating Referring Expressions (ASGRE), UCNLG+MTWorkshop, Machine Translation Summit XI, Copen-hagen.Reiter, Ehud and Dale, Robert.
1992.
A fast algorithmfor the generation of referring expressions.
Proceed-ings of the 14th conference on Computational Linguis-tics, pp.
232-238.
Association for Computational Lin-guistics.218
