Proceedings of the 14th European Workshop on Natural Language Generation, pages 105?114,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsGenerating Natural Language Questions to Support Learning On-LineDavid Lindberg Fred PopowichSchool of Computing ScienceSimon Fraser UniversityBurnaby, BC, CANADAdll4,popowich@sfu.caJohn Nesbit Phil WinneFaculty of EducationSimon Fraser UniversityBurnaby, BC, CANADAnesbit,winne@sfu.caAbstractWhen instructors prepare learning materi-als for students, they frequently developaccompanying questions to guide learn-ing.
Natural language processing technol-ogy can be used to automatically generatesuch questions but techniques used havenot fully leveraged semantic informationcontained in the learning materials or thefull context in which the question genera-tion task occurs.
We introduce a sophisti-cated template-based approach that incor-porates semantic role labels into a systemthat automatically generates natural lan-guage questions to support online learn-ing.
While we have not yet incorporatedthe full learning context into our approach,our preliminary evaluation and evaluationmethodology indicate our approach is apromising one for supporting learning.1 IntroductionAmple research (e.g., Callender and McDaniel,2007) shows that learners learn more, and moredeeply, if they are prompted to examine theirlearning materials while and after they study.
Of-ten, these prompts consist of questions related tothe learning materials.
After reading a given pas-sage or section of text, learners are familiar withlearning exercises which consist of questions theyneed to answer.Questioning is one of the most common and in-tensively studied instructional strategies used byteachers (Rus and Graesser, 1989).
Questions em-bedded in text, or presented while learners arestudying text, are hypothesized to promote self-explanation which is known to increase compre-hension and enhance transfer of learning (e.g.,Rittle-Johnson, 2006).Traditionally, these questions have been con-structed by educators.
Recent research, though,has investigated how natural language processingtechniques can be used to automatically generatethese questions (Kalady et al 2010; Varga andHa, 2010; Ali et al 2010; Mannem et al 2010).While the automated approaches have generallyfocussed on syntactic features, we propose an ap-proach that also takes semantic features into ac-count, in conjunction with domain dependent anddomain independent templates motivated by ed-ucational research.
After introducing our ques-tion generation system, we will provide a prelimi-nary analysis of the performance of the system oneducational material, and then outline our futureplans to tailor the questions to the needs of spe-cific learners and specific learning outcomes.2 Question Generation from TextThe task of question generation (QG) from textcan be broadly divided into three (not entirely dis-joint) categories: syntax-based, semantics-based,and template-based.
Systems in the syntactic cat-egory often use elements of semantics and vice-versa.
A system we would call template-basedmust to some extent use syntactic and/or seman-tic information.
Regardless of the approach taken,systems must perform at least four tasks:1. content selection: picking spans of sourcetext (typically single sentences) from whichquestions can be generated2.
target identification: determining which spe-cific words and/or phrases should be askedabout3.
question formulation: determining the appro-priate question(s) given the content identified4.
surface form generation: producing the finalsurface-form realizationTask 2 need not always precede task 3; targetidentification can drive question formulation and105vice-versa.
A system constrained to generatingspecific kinds of questions will select only the tar-gets appropriate for those kinds of questions.
Con-versely, a system with broader generation capa-bilities might pick targets more freely and (ide-ally) generate only the questions that are appro-priate for those targets.
We consider the methodsused in performing tasks 2 and 4 to be the pri-mary discriminators in determining the categoryinto which a given method is best placed.
This isnot the only way one might classify a QG system.However, we believe this method allows us to bestcompare and contrast our approach with previousapproaches.Syntax-based methods comprise a large portionof the existing literature.
Kalady et al(2012),Varga and Ha (2010), Wolfe (1976), and Ali etal.
(2010) provide a sample of these methods.
Al-though each of these efforts has differed on a fewdetails, they have followed the same basic strat-egy: parse sentences using a syntactic parser, sim-plify complex sentences, identify key phrases, andapply syntactic transformation rules and questionword replacement.The methods we have labeled ?semantics-based?
use method(s) of target identification (task2) that are primarily semantic, using techniquessuch as semantic role labeling (SRL).
Given a sen-tence, a semantic role labeler identifies the pred-icates (relations and actions) along with the se-mantic entities associated with each predicate.
Se-mantic roles, as defined in PropBank (Palmer etal., 2005), include Arg0, Arg1, ..., Arg5, andArgA.
A set of modifiers is also defined and in-cludes ArgM-LOC (location), ArgM-EXT (ex-tent), ArgM-DIS (discourse), ArgM-ADV (adver-bial), ArgM-NEG (negation), ArgM-MOD (modalverb), ArgM-CAU (cause), ArgM-TMP (time),ArgM-PNC (purpose), ArgM-MNR (manner), andArgM-DIR (direction).
We adopt the shorterCoNLL SRL shared task naming conventions(Carreras and Ma`rquez, 2005) (e.g., A0 and AM-LOC).Mannem et al(2010), for example, introduce asemantics-based system that combines SRL withsyntactic transformations.
In the content selec-tion stage, a single sentence is first parsed witha semantic role labeler to identify potential tar-gets.
Targets are selected using simple selec-tion criteria.
Any of the predicate-specific se-mantic arguments (A0-A5), if present, are consid-ered valid targets.
Mannem et alfurther iden-tify modifiers AM-MNR, AM-PUNC, AM-CAU,AM-TMP, AM-LOC, and AM-DIS as potentialtargets.
These roles are used to generate addi-tional questions that cannot be attained using onlythe A0-A5 roles.
For example, AM-LOC can beused to generate a where question, and an AM-TMP can be used to generate a when question.
Af-ter targets have been identified, these, along withthe complete SRL parse of the sentence are passedto the question formulation stage.
Two heuristicsare used to rank the generated questions.
Ques-tions are ranked first by the depth of their predi-cate in the dependency parse of the original ques-tion.
This is based on the assumption that ques-tions arising from main clauses are more desir-able than those generated from deeper predicates.In the second stage, questions with the same rankare re-ranked according to the number of pronounsthey contain, with questions with fewer pronounshaving higher rank.One limitation of the syntax and semantics-based methods is that they generate questions byrearranging the surface form of sentences.
Ques-tion templates offer the ability to ask questions thatare not so tightly-coupled to the exact wording ofthe source text.
A question template is any pre-defined text with placeholder variables to be re-placed with content from the source text.
Ques-tion templates allow question generation systemsto leverage human expertise in language genera-tion.The template-based system of Cai et al(2006)uses Natural Language Generation Markup Lan-guage (NLGML), a language that can be used togenerate not only questions but any natural lan-guage expression.
NLGML uses syntactic patternmatching and semantic features for content selec-tion and question templates to guide question for-mulation and surface-form realization.
Note thata pattern need not specify a complete syntax tree.Additionally, patterns can impose semantic con-straints.
However, simple ?copy and paste?
tem-plates are not a panacea for surface-form real-ization.
Mechanisms for changing capitalizationof words and changing verb conjugation (whensource sentence verbs are to appear in the outputtext) need to be provided: NLGML provides somesuch functions.1063 Our ApproachWe develop a template-based framework for QG.The primary motivation for this decision is theability of a template-based approach to generatequestions that are not merely declarative to in-terrogative transformations.
We aim to addresssome of the limitations of the existing approachesoutlined in the previous section while leveragingsome of their strengths in novel ways.
We com-bine the benefits of a semantics-based approach,the most important of which is not being tightly-constrained by syntax, with the surface-form flex-ibility of a template-based approach.The data used to develop our approach was ob-tained from a collection of 25 documents preparedfor educational research purposes within the Fac-ulty of Education at SFU.
All hand-coded ruleswe describe below were motivated by patterns ob-served in this development data.
This collectionwas modeled after a high-school science curricu-lum on global warming, with vocabulary and dis-course appropriate for learners in that age group.Although the collection included a glossary of keyterms and their definitions, this resource was usedonly for evaluation purposes as described in Sec-tion 4.3.1 Semantic-based templatesPrevious template-based methods have used syn-tactic pattern matching, which does provide agreat deal of flexibility in specifying sentencesappropriate for generating certain types of ques-tions.
However, this flexibility comes at the ex-pense of generality.
As seen in Wyse and Piwek(2009), who use Stanford Tregex (Levy and An-drew, 2006) for pattern matching, the specificity ofsyntactic patterns can make it difficult to specifya syntactic pattern of the desired scope.
Further-more, semantically similar entities can span dif-ferent syntactic structures, and matching these re-quires either multiple patterns (in the case of Caiet al 2006) or a more complicated pattern (in thecase of Wyse and Piwek, 2009).If we want to develop templates that are se-mantically motivated, more flexible in terms ofthe content they successfully match, and more ap-proachable for non-technical users, we need tomove away from syntactic pattern matching.
In-stead, we match semantic patterns.
We define asemantic pattern as the SRL parse of a sentenceand the named entities (if any) contained withinthe span of each semantic role.
We use StanfordNER (Finkel et al 2005) for named entity recog-nition.
Figure 1 shows a sentence and its corre-sponding semantic pattern.
Notice this sentencehas two predicates, each with its own semantic ar-guments.
Each of these predicate-argument struc-tures is a distinct predicate frame.Figure 1: A sentence and its semantic patternEven the shallow semantics of SRL can identifythe semantically interesting portions of a sentence,and these semantically-meaningful substrings canspan a range of syntactic patterns.
Figure 2shows a clear example of this phenomenon.
Inthis example, we see two sentences expressingthe same semantic relationship between two con-cepts, namely, the fact that trapped heat causesthe Earth?s temperature to increase.
In one case,this causation is expressed in an adjective phrase,while the other uses a sentence-initial preposi-tional phrase.
The parse trees are generated usingthe Stanford Parser (Klein and Manning, 2003).The AM-CAU semantic role captures the cause inboth sentences.
It is impossible to accomplish thesame feat with a single NLGML pattern.
However,it is possible to capture both with a single Tregexpattern.The principle advantage of semantic patternmatching is that a single semantic pattern casts anarrow semantic net while casting a large syntacticnet.
This means fewer patterns need to be definedby the template author, and the patterns are morecompact.Our templates have three components: plain-text, slots, and slot options.
Plaintext forms the107Figure 2: Two different syntax subtrees subsumedby a single semantic roleskeleton into which semantically-meaningful sub-strings of a source sentence are inserted to create aquestion.
The only restrictions on the plaintext isthat it cannot contain any text that looks like a slotbut is not intended as one, and it cannot containthe character sequence used to delineate the plain-text from the slots appearing outside the plaintext.Aside from these restrictions, any desired text isvalid.Slots facilitate sentence and template matching.They accept specific semantic arguments, and canappear inside or outside the plaintext.
These pro-vide the semantic pattern against which a sourcesentence is matched.
A slot inside the plaintextacts as a variable to be replaced by the correspond-ing semantic role text from a matching sentence,while any slots appearing outside the plaintextserve only to provide additional pattern match-ing criteria.
The template author does not needto specify the complete semantic pattern in eachtemplate.
Instead, only the portions relevant to thedesired question need to be specified.
This is animportant point of contrast between our template-based approach vs. syntax and semantics-basedapproaches.
We can choose to generate questionsthat do not include any predicates from the sourcesentence but instead ask more abstract or generalquestions about other semantic constituents.
Webelieve these kinds of questions are better able toescape the realm of the factoid, because they arenot constrained to the actions and relations de-scribed by predicates.Slot options function much like NLGML func-tions and are of two types: modifiers and filters.Modifiers apply transformations to the role text in-serted into a slot, and filters enforce finer-grainedmatching criteria.
Predicate slots have their owndistinct set of options, while the other semanticroles share a common set of options.
A template?sslots and filters describe the necessary conditionsfor the template to be matched with a source sen-tence semantic pattern.3.2 Predicate slot optionsThe predicate filter options restrict the predicatesthat can match a predicate slot.
With no filteroptions specified, any predicate is considered amatch.
Table 1 shows the complete list of filters.Filter Descriptionbe predicate lemma must not be ?be?
!be predicate lemma must be ?be?
!have predicate lemma must not be ?have?Table 1: Predicate filtersThe selection of predicate filters might at firstseem oddly limited.
Failing to consider the func-tional differences between various types of verbs(particularly auxiliary and copula) would indeedproduce low-quality questions and should in factbe ignored in most cases.
For example, considerthe sentence ?Dinosaurs, along with many otheranimals, became extinct approximately 65 mil-lion years ago.?
A question such as ?What diddinosaurs, along with many other animals, be-come??
is not particularly useful.
We can rec-ognize copula predicates by their surrounding se-mantic pattern, so in the broad sense, we do notneed to adopt any copula-specific rules.The one exception to the above rule is any cop-ula whose lemma is be.
The be and !be filtersallow the presence or absence of such a predicateto be detected.
This capability is useful for tworeasons.
First, the presence of such a predicategives us an inexpensive way to generate defini-tion questions, even if the source text is not writ-ten in the form of a definition.
Although this willover-generate definition questions, non-predicate108filters can be used to add additional mitigatingconstraints.
Second, requiring the absence of sucha predicate allows us to actively avoid generat-ing certain kinds of ungrammatical or meaning-less questions.
Whether using one of these predi-cates results in ungrammatical questions dependson the wording of the underlying template, so weprovide the !be filter for the template author touse as needed.
Consider the sentence ?El Ninois caused when the westerly winds are unusuallyweak.?
Without the !be filter, one of our tem-plates would generate the question ?When can ElNino be??
Applying the !be filter prevents thisquestion from being generated.Like copula, auxiliary verbs are often not suit-able for question generation.
Fortunately, manyauxiliary verbs are also modal and are assignedthe label AM-MOD and so do not form predi-cate frames of their own.
Instead, they are in-cluded in the frame of the predicate they modify.In other cases auxiliary verbs are not modal, suchas in the sentence ?So far, scientists have not beenable to predict the long term effects of this wob-ble.?
In this case, the auxiliary have is treated asa separate predicate, but importantly, the span ofits A1 includes the predicate been.
We providea non-predicate filter to prevent generation whenthis overlap is present.The !have filter is motivated by the observa-tion that the predicate have can appear as a full,non-copula predicate (with an A0 and A1) but of-ten does not yield high-quality questions.
For ex-ample, consider the sentence ?This effect can havea large impact on the Earth?s climate.?
Without the!have filter, one of our templates would gener-ate the question ?What can this effect have??
Withthe !have filter, that template does not yield anyquestions from the given sentence.Predicate modifiers allow the template author toexplicitly force a change in conjugation.
See Ta-ble 2 for the complete set of predicate modifiers,where fps is an abbreviation for first person sin-gular, sps for second person singular, and so on.The lemma modifier can appear on its own.
How-ever, all other conjugation changes must specifyboth a tense and a person.
If no modifiers are used,the predicate is copied as-is from the source sen-tence.
Although perfect is an aspect rather than atense, MorphAdorner1, which we use to conjugatepredicates, defines it as a tense, so we have imple-1http://morphadorner.northwestern.edumented it as a tense filter.Modifier Tense Modifierlemma lemma (dictionary form) fpspres present spsprespart present participle tpspast past fpppastpart past participle sppperf perfect tpppastperf past perfectpastperfpart past perfect participleTable 2: Predicate modifiers3.3 Non-predicate slot optionsThe filters for non-predicate slots impose addi-tional syntactic and named entity restrictions onany matching role text.
As with predicates, theabsence of any non-predicate filters results in themere presence of the corresponding semantic rolebeing sufficient for matching.
See Table 3 for thecomplete list of non-predicate filters describing re-strictions on the role text (RT), role span (RS), andpredicate frame (PF) in terms of the semantic typeof named entities (and in some cases in terms ofnon-semantic features).Filter Descriptionnull PF must not contain this semantic role.
!nv RS must not contain a predicatedur RT must contain DURATIONdate RT must contain DATE!date RT must not contain a DATEloc RT must contain a LOCATION.ne RT must contain a named entitymisc RT must contain a MISCcomp RT must contain a comparison!comma RT must not contain a commasingular RT must be singularplural RT must be pluralTable 3: Non-predicate filtersThe choice of filters again requires some expla-nation.
The null and !nv filters were foreshad-owed above.
For slots appearing outside the tem-plate?s plaintext, the null filter explicitly requiresthat the corresponding semantic role not be presentin a source sentence semantic pattern.
An A0 slotpaired with the null filter is the mechanism al-luded to earlier that allows for the recognition ofcopula predicates without the need to examine thepredicate itself.
The !nv filter can be used to pre-vent ungrammatical questions.
We observe thatif a role span does include a predicate, resultingquestions are often ungrammatical due to the con-jugation of that predicate.
Applying this filter to109the A1 of a predicate prevents generation from apredicate frame whose predicate is a non-modalauxiliary verb.The named entity filters (dur, !dur, date,loc, ne, and misc) are those most relevant tothe corpus we have used to evaluate our approachand thus the easiest to experiment with effectively.Because named entities are used only for filtering,expanding the set of named entity filters is a trivialtask.The filters comp, !comma, singular, andplural are syntax-based filters.
With the ex-ception of !comma, these filters force the exam-ination of the part-of-speech (POS) tags to de-tect the desired features.
The singular andplural filters let templates be tailored to singu-lar and plural arguments in any desired way, be-yond simply selecting appropriate auxiliary verbs.The type of comparison we search for when thecomp filter is used is quite specific.
We searchfor phrases that describe conditions that are atypi-cal.
These can be seen in phrases such ?unusuallyweak,?
?unseasonably warm,?
?strangely absent,?and so on.
These phrases are present when a wordwhose POS tag is RB (adverb) is followed by aword whose tag is JJ (adjective).
Consider a sen-tence such as ?El Nino is caused when the westerlywinds are unusually weak.?
The comp filter allowsus to generate questions such as ?What data wouldindicate El Nino??
or ?How do the conditions thatcause El Nino differ from normal conditions??
Al-though this heuristic does produce both false pos-itives and false negatives, other syntactic featuressuch as comparative adverbs and comparative ad-jectives are less semantically constrained.
Furtherinvestigation is needed to determine more flexibleways to recognize descriptions of atypical condi-tions.We see two situations in which a comma ap-pears within the span of a single semantic role.The first situation occurs when a list of nouns isserving the role, such as in ?Climate change in-cludes changes in precipitation, temperature, andpressure.?
Here, ?changes in precipitation, temper-ature, and pressure?
is the A1 of the predicate in-cludes.
In cases where a question is only appro-priate for single concept (e.g.
temperature) ratherthan a set of concepts, the !comma filter pre-vents such a question from being generated fromthe sentence above.
This has implications for roletext containing appositives, the second situation inwhich a comma appears within a single role span.Such roles are rejected when !comma is used.This is not ideal, as removing appositives does notcause semantic roles to be lost from a semanticpattern.
Future work will address this problem.The non-predicate modifiers (Table 4) serve twopurposes: to create more fluent questions and toremove non-essential text.
Note that the -tpp,which forces the removal of trailing prepositionalphrases, can have undesired results when appliedto certain modifier roles, such as AM-LOC, AM-MNR, and AM-TMP, when they appear in the tem-plate plaintext.
These modifiers often contain onlya prepositional phrase, and in such cases, -tppwill result in an empty string being placed into thetemplate.Modifier Effect-lp If initial token is prep, remove it-tpp If RT ends with PP, remove PP-ldt If initial token is determiner, remove itTable 4: Non-predicate modifiers3.4 Our QG systemFigure 3 shows the architecture and data flow ofour QG system.
One of the most important thingsto observe about this architecture is that the tem-plates are an external input.
They are in no waycoupled to the system and can be modified asneeded without any system modifications.Compared to most other approaches, we per-form very little pre-processing.
Syntax-basedmethods in particular have been motivated to per-form sentence simplification, because their meth-ods are more likely to generate meaningful ques-tions from short, succinct sentences.
We have cho-sen not to perform any sentence simplification.This decision was motivated by the observationthat common methods of sentence simplificationcan eliminate useful semantic content.
For exam-ple, Kalady et al(2010) claim that prepositionalphrases are often not fundamental to the meaningof a sentence, so they remove them when simpli-fying a sentence.
However, as Figure 4 shows,a prepositional phrase can contain important se-mantic information.
In that example, removing theprepositional phrase causes temporal informationto be lost.One pre-processing step we do perform ispronominal anaphora resolution (Charniak and El-sner, 2009).
Even though we do not split com-110Figure 3: System architecture and data flowFigure 4: Semantic information can be lost dur-ing sentence simplification.
Removing the prepo-sitional phrase from the first sentence leaves thesimpler second sentence, but the AM-TMP modi-fier is lost.plex sentences and therefore do not create newsentences in which pronouns are separated fromtheir antecedents, this kind of anaphora resolutionremains an important step in limiting the numberof vague questions.Each source sentence is tokenized and anno-tated with POS tags, named entities, lemmata, andits SRL parse.
SRL is the cornerstone of our ap-proach.
We generate the SRL parse (Collobertet al 2011) in order to extract a set of predicateframes.
Questions are generated from individ-ual predicate frames rather than entire sentences(unless the sentence contains only one predicateframe).
Given a sentence, the semantic pattern ofeach of its predicate frames is compared againstthat of each template.
Algorithm 1 describes theprocess of matching a single predicate frame (pf )to a single template (t).
Although it is not stated inAlgorithm 1, the sentence-level tokenization, lem-mata, named entities and POS tags are checkedas needed according to the template?s slot filters.If a predicate frame and template are matched,they are passed to Algorithm 2, which fills tem-plate slots with role text to produce a question.Even in the absence of modifiers, all role text re-ceives some additional processing before being in-serted into its corresponding slot.
These additionalsteps include the removal of colons and the thingsthey introduce and the removal of text containedin parentheses.
We observe that these extra stepslead to questions that are more meaningful.Algorithm 1 patternsMatch(pf ,t)for all slot ?
t doif pf does not have slot.role thenif null 6?
slot.filters thenreturn falseend ifelsefor all filter ?
slot.filters doif pf.role does not match filter thenreturn falseend ifend forend ifend forreturn trueBecause we generate questions from predicateframes rather than entire sentences, two sentencesdescribing the same semantic entities might resultin duplicate questions.
To avoid duplicates wekeep only the first occurrence of a question.Using slots and filters, we can now create someinteresting templates and see the questions they111Algorithm 2 fillTemplate(t,pf )question text?
t.plaintextfor all slot ?
t.plaintext slots dorole text?
pf.role(slot.role).textfor all modifier ?
slot.modifiers doapplyModifier(role text,modifier)end forIn question text, replace slot with role textend forreturn question textyield.
Table 5 shows some templates (T) thatmatch the sentence in Figure 1 and the questions(Q) that result.
Although the questions that aregenerated are not answerable from the originalsentence, they were judged answerable from thesource document in our evaluation.
The full set oftemplates is provided in (Lindberg, 2013).As recently as 12,500 years ago, the Earth was in themidst of a glacial age referred to as the Last Ice Age.T: How would you describe [A2 -lp misc]?Q: How would you describe the Last Ice Age?T: Summarize the influence of [A1 -lp !comma !nv] onthe environment.Q: Summarize the influence of a glacial age on the envi-ronment.T: What caused [A2 -lp !nv misc]?
## [A0 null]Q: What caused the Last Ice Age?Table 5: A few sample templates and questions4 EvaluationThere remains no standard set of evaluation met-rics for assessing the quality of question gener-ation output.
Some present no evaluation at all(Wyse and Piwek, 2009; Stanescu et al 2008).Among those who do perform an evaluation, theredoes appear to be a consensus that some formof human evaluation is necessary.
Despite thisagreement in principle, approaches tend to divergethereafter.
There are differences in the evaluationcriteria and the evaluation procedure.Most previous efforts in QG have not gone be-yond manual evaluation.
While some have gonea step further and built models for ranking basedon the probability of a question being acceptable(Heilman and Smith, 2010), these models have nothad a strong basis in pedagogy.
While a questionthat is both syntactically and semantically well-formed is considered acceptable in some evalua-tion schemes, such questions can greatly outnum-ber the questions that we can reasonably expect astudent would want or have time to answer.
Weimplement a classifier that attempts to identify thequestions that are the most pedagogically useful.For our initial evaluation of the performance ofour QG system, we selected a subset of 10 doc-uments from the collection described in the previ-ous section.
On average, each document contained25 sentences.
From the 10 documents, our systemgenerated 1472 questions in total, an average of5.9 questions per sentence.
Due to the educationalnature of this material, we needed evaluators witheducational training rather than naive ones.
Ac-cordingly, the questions we generated were evalu-ated by a graduate student from the Faculty of Ed-ucation.
She was asked to give binary judgementsfor grammaticality, semantic validity, vagueness,answerability, and learning value.
For each ques-tion, two aspects of answerability were evaluated.The first aspect was whether the question was an-swerable from the source sentence from whichit was generated.
The second was whether thequestion was answerable given the source docu-ment as a whole.
The evaluator was given no pre-determined guidelines regarding the relationshipsamong the evaluation criteria (e.g., the influenceof vagueness and answerability on learning value).This aspect of the evaluation was left to her dis-cretion as an educator.
She found that 85% of thequestions were grammatical, with 66% of them ac-tually making sense.
It was determined that 14%of the questions were answerable from the sen-tence used to generate them, while 20% of themwere answerable from the document.
Finally, shedetermined that 17% of the questions had learn-ing value according to the prescribed learning out-comes for the curriculum being modeled.
Asidefrom performing this evaluation, the evaluator wasnot involved in this research.Given this evaluation, we then built a classi-fier which used logistic regression (L2 regular-ized log-likelihood) to classify on learning value.We used length, language model, SRL, named en-tity, glossary, and syntax features.
Length andlanguage model features measure the token countand grammaticality of a question and the sentencefrom which it was generated.
SRL features in-clude the token count of each semantic role in thegenerating predicate frame, whether each role isrequired by the matching template, and whethereach role?s text is used.
Named entity featuresindicate the presence of each of nine named en-tity types in both the source sentence and gener-ated question.
Glossary features note the number112of glossary terms that appear in a sentence andquestion and a measure of the average importanceof each term, which we calculated from a sim-ple in-terms-of graph (Winne and Hadwin, 2013)we constructed from the glossary.
This graph hasdirected edges between each glossary term andthe terms that appear in its gloss.
Syntax fea-tures identify the depth of the generating predi-cate frame in the source sentence and the POS tagof its predicate.
Without adding noise, the train-ing set had 217 questions with learning value and1101 questions without learning value.
The clas-sifier obtained precision and recall scores of 0.47and 0.22 respectively for questions with learningvalue, along with scores of 0.79 and 0.92 for ques-tions with no learning value.
We then added noiseto the training set by relabelling any grammati-cal question that made sense as having learningvalue.
This relabelling resulted in a training setof 778 questions with learning value and only 540questions without learning value.
The classifiertrained on this noisy set showed a precision scoreon learning value questions decreased to 0.29 buta dramatic increase in recall to 0.81.
For questionswith no learning value, the precision increasedslightly to 0.86 which was offset by a dramatic de-crease in recall to 0.38.
So when the system gener-ates a poor quality question, we have a high prob-ability of knowing that it is a poor question whichallows us to then filter or discard it.5 ConclusionsWe have shown how a template-based method,using predominately semantic information, canbe used to generate natural language questionsfor use in an on-line learning system.
Our tem-plates are based on semantic patterns, which casta wide syntactic net and a narrow semantic net.The template mechanism supports rich selectionaland generational capabilities, generating a largepool from which questions for learners can beselected.
A simple automated technique for se-lecting questions with learning value was intro-duced.
Although this automated technique showspromise for some applications, future investiga-tion into what constitutes a useful question in thecontext of a specific task and an individual learneris needed.
Some might argue that it is risky togenerate questions that cannot be answered fromthe source sentence from which they were gener-ated.
Although some questions are generated thatare not answered elsewhere in a document, thereis a benefit in learners being able to recognize thata particular question is not answerable.
Our futurework will expand both on the types of potentialquestions generated, and on the selection from theset of potential questions based on the informationan individual learner (a) knows, (b) has availablein a ?library?
of saved sources, (c) has operatedon while studying online (e.g., tagged), and (d)might find in the Internet.
To facilitate this furtherresearch, we will be integrating question genera-tion into the nStudy system (Hadwin et al 2010;Winne and Hadwin, 2013).
We will also be per-forming thorough user studies which will evalu-ate the generated questions from the learner?s per-spective in addition to the educator?s perspective.AcknowledgmentsThis research was supported by an Insight De-velopment Grant (#430-2012-044) from the So-cial Sciences and Humanities Research Councilof Canada and a Discovery Grant from the Nat-ural Sciences and Engineering Research Councilof Canada.
The authors are extremely grateful toKiran Bisra from the Faculty of Education for pro-viding information for the evaluation.
Finally, spe-cial thanks to the reviewers for their comments andsuggestions.ReferencesHusam Ali, Yllias Chali, and Sadid A Hasan.
2010.Automation of question generation from sentences.In Proceedings of QG2010: The Third Workshop onQuestion Generation, pages 58?67.Zhiqiang Cai, Vasile Rus, Hyun-Jeong Joyce Kim,Suresh C. Susarla, Pavan Karnam, and Arthur C.Graesser.
2006.
Nlgml: A markup languagefor question generation.
In Thomas Reeves andShirley Yamashita, editors, Proceedings of WorldConference on E-Learning in Corporate, Govern-ment, Healthcare, and Higher Education 2006,pages 2747?2752, Honolulu, Hawaii, USA, Octo-ber.
AACE.Aimee A. Callender and Mark A. McDaniel.
2007.The benefits of embedded question adjuncts for lowand high structure builders.
Journal Of EducationalPsychology (2007), pages 339?348.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduc-tion to the conll-2005 shared task: Semantic role la-beling.
In Proceedings of the Ninth Conference onComputational Natural Language Learning, pages152?164.
Association for Computational Linguis-tics.113Eugene Charniak and Micha Elsner.
2009.
Em worksfor pronoun anaphora resolution.
In Proceedingsof the 12th Conference of the European Chapterof the Association for Computational Linguistics,pages 148?156.
Association for Computational Lin-guistics.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.A.F.
Hadwin, M. Oshige, C.L.Z.
Gress, and P.H.Winne.
2010.
Innovative ways for using nstudyto orchestrate and research social aspects of self-regulated learning.
Computers in Human Behaviour(2010), pages 794?805.Michael Heilman and Noah A Smith.
2010.
Goodquestion!
statistical ranking for question genera-tion.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 609?617.
Association for Computational Lin-guistics.Saidalavi Kalady, Ajeesh Elikkottil, and Rajarshi Das.2010.
Natural language question generation usingsyntax and keywords.
In Proceedings of QG2010:The Third Workshop on Question Generation, pages1?10.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.Roger Levy and Galen Andrew.
2006.
Tregex and tsur-geon: tools for querying and manipulating tree datastructures.
In LREC 2006.David Lindberg.
2013.
Automatic question generationfrom text for self-directed learning.
Master?s thesis,Simon Fraser University, Canada.Prashanth Mannem, Rashmi Prasad, and Aravind Joshi.2010.
Question generation from paragraphs atupenn: Qgstec system description.
In Proceedingsof QG2010: The Third Workshop on Question Gen-eration, pages 84?91.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Bethany Rittle-Johnson.
2006.
Promoting transfer:Effects of self-explanation and direct instruction.Child Development (2006), pages 1?15.Vasile Rus and Arthur C Graesser.
1989.
Classroomquestioning.
In School improvement research series.Liana Stanescu, Cosmin Stoica Spahiu, Anca Ion, andAndrei Spahiu.
2008.
Question generation forlearning evaluation.
In Computer Science and In-formation Technology, 2008.
IMCSIT 2008.
Interna-tional Multiconference on, pages 509?513.
IEEE.Andrea Varga and Le An Ha.
2010.
Wlv: A ques-tion generation system for the qgstec 2010 task b.In Proceedings of QG2010: The Third Workshop onQuestion Generation, pages 80?83.Philip H Winne and Allyson F Hadwin.
2013. nstudy:Tracing and supporting self-regulated learning in theinternet.
In International handbook of metacog-nition and learning technologies, pages 293?308.Springer.John H Wolfe.
1976.
Automatic question gener-ation from text-an aid to independent study.
InACM SIGCUE Outlook, volume 10, pages 104?112.ACM.Brendan Wyse and Paul Piwek.
2009.
Generatingquestions from openlearn study units.
In AIED2009 Workshop Proceedings Volume 1: The 2ndWorkshop on Question Generation, 6-9 July 2009,Brighton, UK.114
