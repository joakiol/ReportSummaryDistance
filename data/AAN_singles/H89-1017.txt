THE MINDS SYSTEM:USING CONTEXT AND DIALOG TO ENHANCE SPEECH RECOGNIT IONSheryl R. Young 1Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTContextual knowledge has traditionally been used in multi-sentential textual understanding systems.
Incontrast, this paper describes a new approach toward using contextual, dialog-based knowledge forspeech recognition.
To demonstrate this approach, we have built MINDS, a system which uses contextualknowledge to predictively generate xpectations about he conceptual content that may be expressed in asystem user's next utterance.
These expectations are expanded to constrain the possible words which maybe matched from an incoming speech signal.
To prevent system rigidity and allow for diverse userbehavior, the system creates layered predictions which range from very specific to very general.
Eachtime new information becomes available from the ongoing dialog, MINDS generates a different set oflayered predictions for processing the next utterance.
The predictions contain constraints derived fromthe contextual, dialog level knowledge sources and each prediction is translated into a grammar usable byour speech recognizer, SPHINX.
Since speech recognizers use grammars to dictate legal word sequencesand to constrain the recognition process, the dynamically generated grammars reduce the number of wordcandidates considered by the recognizer.
The results demonstrate hat speech recognition accuracy isgreatly enhanced through the use of predictions.OVERVIEWOne of the primary problems in speech recognition research is effectively analyzing very large, complexsearch spaces.
As search space size increases, recognition accuracy decreases.
Previous research in thespeech recognition area illustrates that knowledge can compensate for search by constraining theexponential growth of a search space and thus increasing recognition accuracy \[12, 4, 8\].
The mostcommon approach to constraining a search space is to use a grammar.
The grammars used for speechrecognition dictate legal word sequences.
Normally they are used in a strict left to right fashion andembody syntactic and semantic onstraints on individual sentences.
These constraints are represented insome form of probabilisfic or semantic network which does not change from utterance to utterance \[2, 8\].Today, state-of-the-art speech recognizers can achieve word accuracy rates in excess of 95% when usinggrammars of perplexity 30 - 60.
As the number of word alternatives ateach point in time increases (or asperplexity increases) performance of these systems decreases rapidly.
Given this level of performance,recently researchers have begun using speech in computer problem solving applications.
Using speech asan input medium for computer applications has resulted in two important findings.
First, the grammarsnecessary to ensure some minimal coverage of a user's language have perplexities which are an order ofmagnitude larger than those used in today's high performing speech systems \[18\].
Second, the use ofspeech in problem solving tasks permits knowledge sources beyond the sentence level to be used tocompensate for the extra search entailed by the increased perplexities.
There are two reasons why higherlevel, contextual knowledge sources can be used to reduce the effective search space: first, the input doesnot consist of isolated, spoken sentences; econd, all input is goal directed.
There are many knowledgeIThis research was conducted with the help of Alexander G. Hauptmann, Wayne H. Ward, Edward T. Smith andPhilip Werner.
This research was sponsored by the Defense Advanced Research Projects Agency (DOD), ARPAOrder No.
5167, under contract number N00039-85-C-0163.
The views and conclusions contained in this documentare those of the authors and should not be interpreted as representing the official policies, either expressed orimplied, of the Defense Advanced Research Projects Agency or the US Government.131sources above the sentence l vel.
Some of these include inferring plans, using context across clausal andsentence boundaries, determining local and global constraints on utterances and dealing with definite andpronominal reference.
Work in the natural language community has shown that these knowledge sourcesare important for understanding language.
The representation a d use of goals and the plans toaccomplish them have received much attention in the artificial intelligence literature \[13, 14, 5\].Furthermore, natural language research as demonstrated that goals, plans and context are important forunderstanding implied and unexpected information as well as for providing helpful responses\[16, 3, 1, 11, 7\].While the importance of contextual, dialog-level knowledge sources has been recognized in the naturallanguage community, these knowledge sources have never been applied to the actual speech recognitionprocess.
In the past, contextual, dialog level knowledge sources were used in speech to either correctspeech recognition errors \[6\] or to disambiguate spoken input and perform inferences required forunderstanding \[10, 15, 17\].
In these systems, dialog knowledge was applied only to the output of therecognizer.In this paper, we describe the use of contextual, dialog based knowledge sources to reduce the effectivesearch space for words in a speech signal and report results which illustrate their effect on recognitionaccuracy.
Our approach uses predictions derived from the contextual knowledge sources to delimit hepossible content of an incoming utterance.
These knowledge sources are integrated into a robust speaker-independent, large-vocabulary speech recognition system.
The knowledge sources are used predictively,and are used in conjunction with traditional syntax and semantics to constrain the recognition process byeliminating large portions of the search space for the earliest acoustic phonetic analysis.
At each point ina dialog, we predict he concepts which are likely to occur.
The concepts are expanded into possible wordsequences which are combined with syntactic networks to produce asemantic grammar.
To avoid systemrigidity which could result from unfulfilled predictions, we generate predictions at different levels ofspecificity by using our knowledge sources opportunistically.
This results in a flexible robust systemwhich displays graceful recognition degradation.
Our approach is demonstrated in a system calledMINDS.
MINDS works in a resource management domain, featuring information obtained from adatabase of facts about ships in the United States Navy.
The basic problem scenario involves determiningwhat to do about a disabled ship which is performing a specific mission.
The user must determine theimpact of the damage on the mission and then determine whether to replace the ship, replace the damagedequipment, delay the mission, etc.
These decisions are made based upon the availability of other ships,which is determined by querying the database.The paper begins with a description of the knowledge sources used in the MINDS system and theirrepresentation.
Then the use of the knowledge sources by the recognizer is addressed.
Finally, resultswhich show the effect of the knowledge sources on speech recognition accuracy are presented.KNOWLEDGE SOURCES AND THEIR REPRESENTATIONThe MINDS system relies upon four knowledge sources to generate layered predictions about he contentof an incoming utterance.
These are dialog structure, general world knowledge, a domain model or modelof task semantics, and models of individual users.DIALOG STRUCTUREOne of the ideas underlying the MINDS system is that tracking all information communicated (userquestions and database answers) enables asystem to infer a set of user goals and possible problem solvingplans for accomplishing these goals.
Once goals and plans are identified, progress can be tracked and thesystem can generate predictions about what goal or plan steps could be executed next.
In the conventionof Newell and Simon (1972) MINDS represents goals and plans as a hierarchically organized AND-ORtree.
This tree represents all possible abstract goals a user may have during a dialog.
For example, in thedomain of dealing with disabled ships, a goal would be finding a replacement ship.
Each node in the treeis characterized by the possible subgoals into which it can be decomposed and a set of domain conceptsinvolved in trying to achieve the goal.
The concepts associated with each node can be single or multipleuse as well as optional or mutually exclusive.
The rationale for representing the combinations ofconceptswhich may be involved in trying to achieve a goal or plan step is that speech systems are guided by132grammars.
If one can identify possible concepts, the traditional understanding process can be reversedand word sequences which express the concepts can be output.The goal tree not only defines the goals, subgoals and domain concepts, but also the traversal optionsavailable to the user.
Additionally, the dialog structure permits constraints derived from previouslycommunicated information to be propagated to related portions of the tree.
These constraints restricteither the concepts associated with various goals or the expansion of concepts associated with goals.Thus, by tracking progress through the goal tree, one can identify a list of possible next states and usethem to generate a set of possible concepts which could be spoken in the next utterance.TASK SEMANTICSThe second important knowledge source is a knowledge base of domain concepts.
In this data structurewe represent all domain objects and their attributes.
The representation uses standard frame languagewhich allows inheritance and multiple relations between frames and frame slots.
The domain conceptsrepresent everything that can be expressed by a user as well as all the relations and interrelations anddefault assumptions about domain objects.
Each utterance can be mapped into a combination of domainconcepts which constitute the meaning of the utterance.
The domain theory or task semantics are used toperform inferencing, as well as to restrict he concepts and concept expansions that are associated withvarious nodes in the goal tree.
For example, ira helicopter is disabled, it is possible for the user to locatea replacement helicopter as opposed to locating a replacement ship, while it is not possible to borrowequipment ira radar device is damaged.GENERAL WORLD KNOWLEDGEOur third knowledge source is a collection of domain independent, general world knowledge sources thatare represented asa procedures and rules.
In the MINDS system, this knowledge includes determinationof what objects are in "focus" and are relevant, procedures for determining which constraints arepropagated to additional nodes in the goal tree given their relative embedding.
MINDS also hasprocedures for determining which objects or attributes could be used in an incomplete sentence in the nextutterance, rules for detecting when a plan fails and principles for determining when a clarification dialogcan be pursued as well as its content.
Additionally, procedures are included for determining the types ofanaphoric references which can be used as well as the object available for such reference.
Theseknowledge based procedures are intended to limit the concepts which can be expressed in a nextutterance, to limit the syntactic methods of expressing concepts and to limit concept expansions.
Thus theset of concepts associated with a particular state in the goal tree is dynamically computed using the abovedescribed rules and procedures.
The restrictions on concept expansions are computed each time a conceptis predicted to be included in a future utterance.USER KNOWLEDGEKnowledge about individual user's domain knowledge is contained in a user model.
Specifically, the usermodels contain domain concepts and relations between domain concepts that a user knows.
These modelsare represented as control structures attached to individual goal nodes in the goal tree.
The controlstructures further efine goal tree traversal options by specifying mutually exclusive goal states as well asoptional goal states for a particular user.
Essentially they specify what information can be inferred fromother information if a user knows certain facts, and what information a user is unlikely to ask when theconcepts are foreign to the individual user.USING THE KNOWLEDGEThe MINDS system uses the above described knowledge sources to dynamically derive constraints uponan incoming utterance.
The basic processing loop involves analyzing the last utterance and databaseresponse to determine their significance and to track progress in the goal tree.
Any constraints which canbe inferred from the input information is stored and later propagated where appropriate.
Next, the systemdetermines the possible next goals the user might pursue given current positions in the goal tree.
The listof possible next moves includes not only reasonable extensions to existing moves, but also clarificationsand returns to previously abandoned goals.
Once possible next goal states are determined, the constraints133upon their associated concepts and their expansions are computed.
Finally, the set of layered predictionsare expanded into grammars and used to guide the search for words in an incoming speech signal.
In thissection we review the procedures involved in generating conceptual predictions and using the predictionsto guide recognition.PREDICTION GENERATIONThe prediction generation process involves both processing the last utterance to update the record ofwhich portions of the goal tree have been visited and/or completed and determining what options areavailable for the user to pursue next.To process an input utterance and its database response, we first try to determine which goal states aretargeted by the present interaction.
Determination f activated goal states, or inferring a user's plan is byno means unambiguous.
During one interaction many goals may be completed and many new goal statesmay be initiated.
Similarly, it is possible that a previously assumed goal state is not being pursued by theuser.
To deal with these ambiguities, we use a number of algorithms.
Goals that have just beencompleted by this interaction and are consistent with previous plan steps are preferred.
If no goals havebeen completed, we prefer the activated goal states which are most likely to follow, given the active goalsat higher, more abstract levels in the hierarchy.
Based on this information we select he next set of plansteps which are most likely to be executed.
This usually constitutes the second most specific set ofpredictions.
The set of plan steps and actions are further pruned, if possible by applying any userknowledge represented in the control schemas attached to the goal states.
The concepts associated withthis set of information are used to generate the most specific layer of predictions.
To generate additionallayers of predictions beyond the two most specific described above, we maintain a list of all incomplete,active goal states -- regardless of their relative embedding the .the hierarchy.
These goal states areassessed to determine possible next moves and then used to generate additional, ess restrictive layers ofpredictions.
This procedure continues until all active goals are incorporated into a prediction set.
Thus,goals are layered by the amount of constraint they provide as well as the reliability of the knowledgesources used to generate them.
Hence, the least restrictive set of goals includes all domain concepts.Once the prediction layers have been determined, restrictions on the concepts associated with each of thepossible goal states are computed from the task semantics and procedures for applying prior context suchas given their placement in the goal tree, and the general world knowledge procedures for propagatingconstraints and determining focus.
Next, focus is used to determine objects and attributes available forreferences and use of pronominal references.
Finally, objects and attributes available for inclusion in apartial, elliptical utterance are determined.
This information is then used to generate the grammars andlexicons used by the speech recognizer, as described below.PREDICTION EXPANSION AND USEThe idea behind the MINDS system is to use dialog knowledge to reduce the amount of search performedby a speech recognizer and thereby reduce the number of recognition errors caused by ambiguity andword confusion.
Thus, once the layered predictions have been generated, they must be expanded into aform which can be used to guide the speech recognition module.
Since the prediction layers arecomposed of sets of abstract concepts, we need to expand or translate these into sentence fragments orword sequences that signify the appropriate conceptual meaning.
Additionally, since speech recognizerscan be guided by a semantic grammar, we actually expand each layer of the predictions into adynamically generated semantic grammar composed of different, precompiled rewrite roles.
Becauseconcepts themselves are also restricted by prior context, it is also necessary to supplement each grammarwith a lexicon.
For example, a rewrite rule may allow any shipname but context may restrict theshipnames to include only a few, such as Whipple and Fox.
In this case, the lexicon would only includethe shipnames Whipple and Fox.Once the predictions have been expanded into a semantic grammar, we use the grammar to guide thespeech recognition system, which in this case is a modified version of the SPHNIX system \[9\].
Duringrecognition, the speech module performs a time synchronous beam search.
We trace through the activenodes in each part of the finite state semantic grammar to control the word transitions.
As the search exitsa word, it forms a set of words to transit o given the successor states in the finite state network.
The134recognizer uses the grammars in order of most specific first.
If no string of words is found that exceeds apredetermined goodness score, the signal is reprocessed with a less constraining grammar.
This processcontinues until an acceptable sequence of words is found.EVALUATIONTo test the ability of our layered predictions to both reduce search space and to improve speechrecognition performance, we used an independent test set.
This means that the utterances processed bythe system were never before seen by the system or its developers.
Additionally, the test set did notinclude any clarification dialogs.
We used ten speakers (8 male, 2 female) who had not been used to trainthe recognizer.
Each speaker read 20 sentences from adapted (to be consistent with the CMU database)versions of three test scenarios provided by the Navy.
Each of these utterances was recorded.
The speechrecordings were then run through the SPHINX recognition system in two conditions:?
using the system grammar (all legal sentences)?
using the grammar f om the successful prediction layer merged with all unsuccessful layersThe results can be seen in Table 1.
As can be seen, the system performed significantly better with theRecognition Performance'Constraints Used: Grammar Predictions242.4 18.3 Test Set PerplexityWord Accuracy 82.1 96.5Semantic Accuracy 85% 100%Insertions 0.0% 0.5%Deletions 8.5% 1.6%Substitutions 9.4% 1.4%predictions.
Error rate decreased by a factor of five.
Perhaps more important, however, is the nature ofthe errors.
In the "layered predictions" condition, 89 percent of the insertions and deletions were the word"the".
Additionally, 67 percent of the substitutions were "his" for "its".
Furthermore, none of the errorsin the "layered predictions" condition resulted in an incorrect database query.
Because both our databaseand the Navy's database shared the same fields and were implemented using Informix TM, we coulddirectly assess the accuracy of the SQL database queries to Informix.
Hence, semantic accuracy, definedas a correct database query, was 100% in the "layered prediction" condition.
Finally, we assessed thepercentage of false alarms, where the recognizer output a sequence of words deemed acceptable from aprediction layer which did not contain a correct parse of the speech input.
For the 30 utterances whichcould not be parsed at the most specific prediction layer, there were no false alarms.SUMMARYThe MINDS system was built to demonstrate the feasibility of using contextual, dialog-level knowledgesources to constrain the exponential growth of a search space and hence increase speech recognitionaccuracy.
The results of our studies using the system indicate that such knowledge sources are effectivefor dynamically reducing the number of word candidates a speech recognition system must consider whenanalyzing a speech signal.
As we move towards robust, interactive problem solving environments wherespeech is a primary input medium, use of these knowledge sources could prove important for enhancingsystem performance.REFERENCES1.
Allen, J. F. and Perrault, C. R. "Analyzing Intention in Utterances".
Artificial Intelligence 15, 3(1980), 143-178.1352.
Borghesi, L. and Favareto, C. Flexible Parsing of Discretely Uttered Sentences.
COLING-82,Association for Computational Linguistics, Prague, July, 1982, pp.
37 - 48.3.
Cohen, P. R. and Perrault, C. R. "Elements of a Plan-Based Theory of Speech Acts".
CognitiveScience 3 (1979), 177-212.4.
Erman, L.D.
and Lesser, V.R.
The Hearsay-II Speech Understanding System: A Tutorial.
In Lea,W.A., Ed., Trends in Speech Recognition, Prentice-Hall, Englewood Cliffs, NJ, 1980, pp.
340 - 360.5.
Fikes, R. E. and Nilsson, N. J.
"STRIPS: A New Approach to the Application of Theorem Proving toProblem Solving".
Artificial Intelligence 2 (1971), 189-208.6.
Fink, P. K. and Biermann, A. W. "The Correction of Ill-Formed Input Using History-BasedExpectation With Applications to Speech Understanding".
Computational Linguistics 12 (1986), 13-36.7.
Grosz, B. J. and Sidner, C. L. "Attention, Intentions and the Structure of Discourse".
ComputationLinguistics 12 (1986), 175-204.8.
Kimball, O., Price, P., Roucos, S., Schwartz, R., Kubala, F., Chow, Y.-L., Haas, A., Krasner, M. andMakhoul, J.
Recognition Performance and Grammatical Constraints.
Proceedings of the DARPA SpeechRecognition Workshop, Science Applications International Corporation Report Number SAIC-86/1546,1986, pp.
53 - 59.9.
Lee, K.F., Hon, H.W.
and Reddy, R. "An Overview of the SPHINX Speech Recognition System".IEEE Transactions on Acoustics, Speech and Signal Processing in press (1989),.10.
Levinson, S. E. and Rabiner, L. R. "A Task-Oriented Conversational Mode Speech UnderstandingSystem".
Bibliotheca Phonetica 12 (1985), 149-196.11.
Litman, D. J. and Allen, J.F.
"A Plan Recognition Model for Subdialogues in Conversation".Cognitive Science 11 (1987), 163-200.12.
Lowerre, B. and Reddy, R. The Harpy Speech Understanding System.
In Lea, W.A., Ed., Trends inSpeech Recognition, Prentice-Hall, Englewood Cliffs, NJ, 1980, pp.
340 - 360.13.
Newell, A. and Simon, H. A.. Human Problem Solving.
New Jersey: Prentice-Hall, 1972.14.
Sacerdoti, E. D. "Planning in a Hierarchy of Abstraction Spaces".
Artificial Intelligence 5, 2 (1974),115-135.15.
Walker, D.E.
SRI Research on Speech Understanding.
In Lea, W.A., Ed., Trends in SpeechRecognition, Prentice-Hall, Englewood Cliffs, NJ, 1980, pp.
294 - 315.16.
Wilensky, R.. Planning and Understanding.
Addison Wesley, Reading, MA, 1983.17.
Woods, W.A., Bates, M., Brown, G., Bruce, B., Cook, C., Klovstad, J., Makhoul, J., Nash-Webber,B., Schwartz, R., Wolf, J., and Zue, V. Speech Understanding Systems - Final Technical Report.
Tech.Rept.
3438, Bolt, Beranek, and Newman, Inc., Cambridge, MA, 1976.18.
Young, S. R., Hauptmann, A. G., Ward, W. H., Smith, E. T. and Wemer, P. "High Level KnowledgeSources in Usable Speech Recognition Systems".
Communications of the ACM 32, 2 (1989),.136
