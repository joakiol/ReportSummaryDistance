A Model For Generating Better ExplanationsPeter van BeekDepartment of Computer ScienceUniversity of WaterlooWaterloo, OntarioCANADA N2L 3G1AbstractPrevious work in generating explanations from advice-giving systems has demonstrated that a cooperative sys-tem can and should infer the immediate goals and plansof an utterance (or discourse segment) and formulate aresponse in light of these goals and plans.
The claim ofthis paper is that a cooperative response may also haveto address a user's overall goals, plans, and preferencesamong those goals and plans.
An algorithm is intro-duced that generates user-specific responses by reasoningabout the goals, plans and preferences hypothesizedabout a user.1.
IntroductionWhat constitutes a good response?
There is generalagreement that a correct, direct response to a questionmay, under certain circumstances, be inadequate.
Pre-vious work has emphasized that a good response shouldbe formulated in light of the user's immediate goals andplans as inferred from the utterance (or discourse seg-ment).
Thus, a good response may also have to (i)assure the user that his underlying goal was consideredin arriving at the response (McKeown, Wish, andMatthews 1985); (ii) answer a query that results froman inappropriate plan indirectly by responding to theunderlying goal of the query (Pollack 1986); (iii) pro-vide additional information aimed at preventing the" userfrom drawing false conclusions because of violatedexpectations of how an expert would respond (Joshi,Webber, and Weischedel 1984a, 1984b).The claim of this paper is that a cooperative responsecan (and should) also address a user's overall goals,plans, and preferences among those goals and plans.We wish to show that an advice seeker may also expectthe expert to respond in light of, not only the immediategoals and plans of the user as expressed in a query, butalso in light of (i) previously expressed goals or prefer-ences, (ii) goals that may be inferred or known from theuser's background, and (iii) domain goals the user maybe expected to hold.
If the expert's response does notconsider these latter type of goals the result may misleador confuse the user and, at the least, will not becooperative.As one example, consider the following exchangebetween a student and student-advisor system.User: Can I enroll in CS 375 (NumericalAnalysis)?System: Yes, but CS 375 does involve a lot of FOR-TRAN programming.
You may find Eng353 (Technical Writing) and CS 327 (AI) tobe useful courses.The user hopes to enroll in a particular course to helpfulfill his elective requirements.
But imagine that inthe past the student has told the advisor that he hasstrong feelings about not using FORTRAN as a pro-gramming language.
If the student-advisor gives thesimple response of "Yes" and the student subsequentlyenrolls in the course and finds out that it involves heavydoses of FORTRAN programming, the student willprobably have justifiably bad feelings about the student-advisor.
The better response shown takes into accountwhat is known about the user's preferences.
Thus thesystem must check if the user's plan as expressed in hisquery is compatible with previously expressed goals ofthe user.
The system can be additionally cooperative byoffering alternatives that are compatible with the user'spreferences and also help towards the user's intendedgoal of choosing an elective (see response).Our work should be seen as an extension of theapproach of Joshi, Webber, and Weischedel (1984a,1984b; hereafter eferred to as Joshi).
Joshi's approach,however, involves only the stated and intended (orunderlying) goal of the query, which, as the aboveexample illustrates, can be inadequate for avoidingmisleading responses.
Further, a major claim of Joshi isthat a system must recognize when a user's plan (asexpressed in a query) is sub-optimal and provide a betteralternative.
However, Joshi leaves unspecified how thiscould be done.
We present an algorithm that producesgood responses by abstractly reasoning about the overallgoals and plans hypothesized of a user.
An explicitmodel of the user is maintained to track the goals,plans, and preferences of the user and also to recordsome of the background of the user pertinent to thedomain.
Together these provide a more general,extended method of computing non-misleading215responses.
Along with new cases where a response mustbe modified to not be misleading, we show how thecases enumerated in (Joshi 1984a) can be effectivelycomputed given the model of the user.
We also showhow the user model allows us to compare alternativesand select the better one, all with regards to a specificuser, and how the algorithm allows the responses to becomputed in a domain independent manner.
In sum-mar),, computing a response requires, among otherthings, the ability to provide a correct, direct answer toa query; explain the failure of a query; compute betteralternatives to a user's plan as expressed in a query; andrecognize when a direct response should be modifiedand make the appropriate modification.2.
The User  Mode lOur model requires a database of domain dependentplans and goals.
We assume that the goals of the userin the immediate discourse are available by methodssuch as specified in (Allen 1983; Carberry 1983; Litmanand Allen 1984; Pollack 1984, 1986).
The model of auser contains, in addition to the user's immediatediscourse goals, fiis background, higher domain goals,and plans specifying how the higher domain goals willbe accomplished.
In the student-advisor domain, forexample, the user model will initially contain somedefault goals that the user can be expected to hold, suchas avoiding failing marks on his permanent record.
Itwill also contain those goals of the user that can beinferred or known from the system's knowledge of theuser's background, such as the attainment of a degree.
"New goals and plans will be added to the model (e.g.the student's preferences or intentions) as they arederived from the discourse.
For example, if the userdisplays or mentions a predilection for numericalanalysis courses this would be installed in the user modelas a goal to be achieved.3.
The A lgor i thmExplanations and predictions of people's choices ineveryday life are often founded on the assumption ofhuman rationality.
Allen's (1983) work in recognizingintentions from natural language utterances makes theassumption that "people are rational agents who are'capable of forming and executing plans to achieve theirgoals" (see also Cohen and Levesque 1985).
Our algo-.
'-ithm reasons about the user's goals and plans accordingto some postulated guiding principles of action to whicha reasonable agent will try to adhere in decidingbetween competing goals and methods for achievingthose goals.
If the user does not "live up" to these prin-ciples, the response generated by the algorithm willinclude how the principles are violated and also somealternatives that are better (if they exist) because theydo not violate the principles.
Some of these principleswill be made explicit in the following description of thealgorithm (see van Beek 1986 for a more completedescription).The algorithm begins by checking whether the user'squery (e.g.
"Can I enroll in CS 375?")
is possible or notpossible (refer to figure 1).
If the query is not possible,the user is informed and the explanation includes thereasons for the failure (step 1.0 of algorithm).
Alterna-tive plans that are possible and help achieve the user'sintended goal are searched for and presented to theuser.
But before presenting any alternative, the algo-rithm, to not mislead the user, ensures that the alterna-five is compatible with the higher domain goals of theuser (step 1.1).If the query is possible, control passes to step 2.0,where the next step is to determine whether the statedgoal does, as the user believes, help achieve theintended goal.
Given that the user presents a plan thathe believes will accomplish his intended goals, the sys-tem must check if the plan succeeds in its intentions(step 2.1 of algorithm).
As is shown in the algorithm, ifthe relationship does not hold or the plan is not execut-able, the user should be informed.
Here it is possible toprovide additional unrequested information necessary toachieve the goal (cf.
Allen 1983).In planning a response, the system should ensure thatthe current goals, as expressed in the user's queries, arecompatible with the user's higher domain goals (step 2.2in algorithm).
For example, a plan that leads to theattainment of one goal may cause the non-attainment ofanother such as when a previously formed plan becomesinvalid or a subgoal becomes impossible to achieve.
Auser may expect to be informed of such consequences,particularly if the goal that cannot now be attained is agoal the user values highly.The system can be additionally cooperative by sug-gesting better alternatives if they exist (step 2.3 in algo-rithm).
Furthermore, both the definitions of better andpossible alternatives are relative to a particular user.
Inparticular, if a user has several compatible goals, heshould adopt the plan that will contribute to the greatestnumber of his goals.
As well, those goals that arevalued absolutely higher than other goals, are the goalsto be achieved.
A user should seek plans of action thatwill satisfy those goals, and plans to satisfy his othergoals should be adopted only if they are compatible withthe satisfaction of those goals he values most highly.216(1.0)(1.1)(1.2)(2.0)(2.1)(2.2)(2.3)Check if original query is possible.Case 1: { Original query fails }Message: No, \[query\] is not possible because ...If ( 3 alternatives that help achieve the intended goal andare compatible with the higher domain goals ) thenMessage: However, you can \[alternatives\]ElseMessage: No alternativesCase 2: { Original query succeeds }Message: Yes, \[query\] is possible.If not ( intended goal ) thenMessage: Warn user that intended goal does not hold and explain why.If ( 3 alternatives that do help achieve the intended goal andare also compatible with the higher domain goals ) thenMessage: However, you can \[alternatives\]ElseMessage: No alternativesElse If ( stated goal of query is incompatible with the higherdomain goals ) thenMessage: Warn user of incompatibility.If ( 3 alternatives that are compatible with the higher domaingoals and also help achieve the intended goal ) thenMessage: However, you can \[alternatives\]ElseMessage: No alternativesElse If ( 3 alternatives that also meet intended goal but arebetter than the stated goal of the query ) thenMessage: There is a better way ...Else{ No action }Figure 1: Explanation Algorithm4.
An ExampleUntil now we have discussed a model for generatingbetter, user-specific explanations.
A test version of thismodel has been implemented in a student-advisordomain using Waterloo UNIX Prolog.
Below wepresent an example to illustrate how the algorithm andthe model of the user work together to produce theseresponses and to illustrate some of the details of theimplementation.Given a query by the user, the system determineswhether the stated goal of the query is possible or notpossible and whether the stated goal will help achievethe intended goal.
In the hypothetical situation shownin figure 2, the stated goal of enrolling in CS572 is pos-sible and the intended goal of taking a numericalanalysis course is satisfied 1.
The system then considersthe background of the user (e.g.
the courses taken), thebackground of the domain (e.g.
what courses areoffered) and a query from the user (e.g, "Can I enroll inCS572?
"), and ensures that the goal of the query is com-patible with the attainment of the overall domain goal.In this example, the user's stated goal of enrolling ina particular course is incompatible with the user's higherI Recall that we are assuming the stated and intended goalsare supplied to our model.
This particular intended goal, hy-pothetically inferred from the stated goal and previousdiscourse, was chosen to illustrate the use of the stated, in-tended, and domain goals in forming a best response.
Tilecase of a conflict between stated and intended goal would behandled in a similar fashion to the conflict be~'een stated anddomain goal, shown in this example.217Scenario:The user asks about enrolling in a 500 level course.Only a certain number of 500 level courses can becredited towards a degree and the user has alreadytaken that number of 500 level courses.Stated goal:Intended goal:Domain goal:Enroll in the course.Take a numerical analysis course.Get a degree.User:Can I enroll in CS 572 (Linear Algebra)?System:Yes, but it will not get you further towards yourdegree since you have already met your 500 levelrequirement.
Some useful courses would be CS 673(Linear Programming) and CS 674 (Approximation).Figure 2: Example from student advisor domaindomain goal of achieving a degree because severalpreconditions fail.
That is, given the background of theuser the goal of the query to enroll in CS572 will nothelp achieve the domain goal.
Knowledge of the incom-patibility and the failed preconditions are used to form?
the first sentence of the system's response.To suggest better alternatives, the system goes into aplanning stage.
There is stored in the system a generalplan for accomplishing the higher domain goal of theuser.
This plan is necessarily incomplete and is used bythe system to track the user by instantiating the planaccording to the user's particular case.
The system con-siders alternative plans to achieve the user's intendedgoal that are compatible with the domain goal.
For thisparticular example, the system discovers other coursesthe user can add that will help achieve the higher goal.To actually generate better alternatives and to checkwhether the user's stated goal is compatible with theuser's domain goal, a module of the implemented sys-tem is a Horn clause theorem prover, built on top ofWaterloo Unix Prolog, with the feature that it records ahistory of the deduction.
The theorem prover generatespossible alternative plans by performing deduction onthe goal at the level of the user's query.
That is, thegoal is "proven" given the "actions" (e.g.
enroll in acourse) and the "constraints" (e.g.
prerequisites of thecourse were taken) of the domain.
In the example offigure 2, the expert system has the following Hornclauses in its knowledge base:course (cs673, numerical)course (cs674.
numerical)Figure 3 shows a portion of the simplified domain planfor getting a degree.
Consider the first clause of thecounts_.for_credit predicate.
This clause states that acourse will count for credit if it is a 500 level course andfewer than two 500 level course have already beencounted for credit (since in our hypothetical world, atmost two 500 level courses can be counted for credittowards a degree).
The second clause is similar.
Itstates the conditions under which a a 600 level coursecan be counted for credit.get_degree(Student, Action) <-receive_credit(Student, Course, Action);getdegree(Student ,  \[\]);receive credit (Student, Course, Action) <-counts_for_credit (Student, Course),enrolled (Student, Course, credit, Action),dowork  (Student, Course),passing_grade (Student, Course);receive_credit (Student, Course, Action) <-enrolled (Student, Course, credit, \[\]),enrolled (Student, Course, incomplete, Action),complete_work (Student, Course),passing_grade (Student, Course);counts_for_credit (Student, Course) <-is_500_level (Course).500_level_taken (Student, N), It (N, 2);counts for credit (Student, Course) <-is_600_level (Course).600_level_taken (Student, N), It (N, 5);Figure 3: Simplified domain plan for course domain.The domain plan is then employed to generate anappropriate response.
The clauses can be used in twoways: (i) to return an action that will help achieve agoal and (ii) to check whether a particular action is apossible step in a plan to achieve a goal.
In the firstuse, the Action parameter is uninstantiated (a variable),the theorem prover is applied to the clause, and, as aresult, the Action parameter is instantiated with anaction the user could perform towards achieving hisgoal.
In the second case, the Action parameter is boundto a particular action and then the theorem prover isapplied.
If the proof succeeds, the particular action is avalid step in a plan; if the proof fails, it is not valid and218the history of the deduction will show why.
In thisexample, enrolling in CS673 is a valid step in a plan forachieving a degree.Recall that the system will generate alternative planseven if the user's query is a valid plan in an attempt ofind a better solution for the user.
The (possibly) multi-ple alternative plans are then potential candidates forpresenting to the user.
These candidates are pruned byranking them according to the heuristic of "which planwould get the user further towards his goals".
Thus, thebetter alternatives are the ones that help satisfy multiplegoa ls  or multiple subgoals 2.
One way in which the sys-tem can reduce alternatives is to employ previouslyderived goals of the user such as those that indicate cer-tain preferences or interests.
In the course domain, forinstance, the user may prefer taking numerical analysiscourses.
For the example in figure 2, the suggestedalternatives of CS673 and CS674 help towards the user'sgoal of getting a degree and the user's goal of takingnumerical analysis courses and so are preferable 3.5.
Joshi RevisitedThe discussion in the previous section showed howour model can recognize when a user's plan is incompa-tible with his domain goals and present better alternativeplans that are user-specific.
Here we present examplesof how our model can generate the responsesenumerated by Joshi.
The examples further illustratehow the addition of the user's overall goals allows us tocompare and select better alternatives to a user's plan.Figure 4 shows two different responses to the samequestion: "Can I drop CS 577?"
The student asking thequestion is doing poorly in the course and wishes to dropit to avoid failing it.
The goals of the query are passedto the Prolog implementation and the response gen-erated depends on these goals, the information in themodel of the user, and on external conditions such asdeadlines for changing status in a course.
For examplepurposes, the domain information is read in from a file(e.g.
consult(example_l)).
Figure 3 shows the clausalrepresentation of the domain goals and plans used inthis example (the representations for the goal of avoid-ing a failing mark are not shown but are similar).2 Part of our purpose is to characterize domain independcntcriteria for "bettemess".
Domain dependent knowledge couldalso be used to further educe the alternatives displayed to theuser.
For example, in the course domain a rule of the form:"A mandatory, course is preferable to a non-mandatorycourse", may help eliminate presentation fcertain options.3 Note that in this example the user's intended goal also in-dicates a preference.
Other user preferences may have beenpreviously specificed: these would be used to influence theresponse in a similar faslfion.%% Can Ariadne drop CS 577?%?consult(example_l);?
query(changestatus(ar iadne,  577, credit, nil),not fail(ariadne, 577, Action));Yes, change_status(ariadne, 577, credit, nil) is possible.But, not fail(ariadne, 577, _461) is not achieved since...is_failing(ariadne, 577)However, you can ...change_status(ariadne, 577, credit, incomplete)This will also help towards receive_credit%% Can Andrew drop CS 577?%?consult (exam pie...2);query(changestatus(andrew, 577  credit, nil),not_fail(andrew, 577, Action));Yes, changestatus(andrew, 577, credit, nil) is possible.But, there is a better way ...change_status(andrew, 577, credit, incomplete)Because this will also help towards receive_creditFigure 4: Sample responsesExample 1: In this example, the stated goal is possible,but it fails in its intention (dropping the course doesn'tenable the student to avoid failing the course).
This iscase 2.1 of the algorithm.
The system now looks foralternatives that will help achieve the student's intendedgoal and determines that two alternative plans are possi-ble: the student could either change to audit status ortake an incomplete in the course.
The plan to take anincomplete is presented to the user because it is con-sidered the best of the two alternatives; it will allow thestudent to still achieve another of his goals: receivingcredit for the course.Example 2: Here the query is possible (the student candrop the course) and is successful in its intention (drop-ping the course does enable the student to avoid failingthe course).
The system now looks for a better alterna-tive to the student's plan of dropping the course (case2.3 of algorithm) and determines an alternative thatachieves the intended goal of not failing the course butalso achieves another of the student's domain goals:receiving credit for the course.
This better alternative isthen presented to the student.2196.
Future Work and ConclusionFuture work should include incorporation of existingmethods for inferring the user's goals from an utteranceand also should include a component for mappingbetween the Horn clause representation used by the pro-gram and the English surface form.An interesting next step would be to investigate com-bining the present work with methods for varying anexplanation from an expert system according to theuser's knowledge of the domain.
In some domains it isdesirable for an expert system to support explanationsfor users with widely diverse backgrounds.
To providethis support an expert system should also tailor the con-tent of its explanations according to the user'sknowledge of the domain.
An expert system currentlybeing developed for the diagnosis of a child's learningdisabilities and the recommendation f a remedial pro-gram provides a good example (Jones and Poole 1985).Psychologists, administrators, teachers, and parents areall potential audiences for explanations.
As well,members within each of these groups will have varyinglevels of expertise in educational diagnosis.
Cohen andJones (1986; see also van Beck and Cohen) suggest hatthe user model begin with default assumptions based onthe user's group and be updated as information isexchanged in the dialogue.
In formulating a response,the system determines the information relevant toanswering the query and includes that portion of theinformation believed to be outside of the user'sknowledge.We have argued that, in generating explanations, wecan and should consider the user's goals, plans forachieving goals, and preferences among these goals andplans.
Our implementation has supported the claim thatthis approach is useful in an expert advice-givingenvironment where the user and the system workcooperatively towards common goals through the dialo-gue and the user's utterances may be viewed as actionsin plans for achieving those goals.
We believe thepresent work is a small but nevertheless worthwhile steptowards better and user-specific explanations from expertsystems.7.
AcknowledgementsThis paper is based on thesis work done under thesupervision of Robin Cohen, to whom I offer my thanksfor her guidance and encouragement.
Financial supportis acknowledged from the Natural Sciences andEngineering .Research Council of Canada and theUniversity of Waterloo.8.
ReferencesAllen, J. F., 1983, "Recognizing Intentions fromNatural Language Utterances," in ComputationalModels of Discourse, Ed.
M. Brady and R. C.Berwick, Cambridge: MIT Press.Carberry, S., 1983, 'Tracking User Goals in anInformation-Seeking Environment," Proceedings ofNational Conference on Artificial Intelligence, Wash-ington, D.C.Cohen, P. R. and Levesque, H. J., 1985, "Speech Actsand Rationality," Proceedings of ACL-85, Chicago,Ill.Cohen, R. and Jones, M., 1986, "Incorporating UserModels into Expert Systems for Educational Diag-nosis," Department of Computer Science ResearchReport CS-86-37, University of Waterloo, Waterloo,Ont.Jones, M. and Poole, D., 1985, "An Expert System forEducational Diagnosis Based on Default Logic,"Proceedings of the Fifth International Conference onExpert Systems.
and Their Applications, Avignon,France.Joshi, A., Webber, B., and Weischedel, R., 1984a,"Living up to Expectations: Computing ExpertResponses," Proceedings of AAAI-84, Austin, Tex.Joshi, A., Webber, B., and Weischedel, R., 1984b,"Preventing False Inferences," Proceedings ofCOLING-84, lOth International Conference on Compu-tational Linguistics, Stanford, Calif.Litman, D. J. and Allen, J. F., 1984, "A Plan Recogni-tion Model for Subdialogue in Conversations,"University of Rochester Technical Report 141,Rochester, N.Y.McKeown, K. R., Wish, M., and Matthews K., 1985,"Tailoring Explanations for the User," Proceedings ofIJCAI-85, Los Angeles, Calif.Pollack, M. E., 1984.
"Good Answers to Bad Questions:Goal Inference in Expert Advice-Giving," Proceed-ings of CSCSI-84, London, Ont.Pollack, M. E., 1986, "A Model of Plan Inference thatDistinguishes Between the Beliefs of Actors andObservers," Proceedings of ACL-86, New York, N.Y.van Beck, P., 1986, "A Model for User-Specific Expla-nations from Expert Systems/' M. Math thesis, pub-lished as Department of Computer Science ResearchReport CS-86-42, University of Waterloo, Waterloo,Ont.van Beck, P. and Cohen, R., 1986, ''Towards User-Specific Explanations from Expert Systems," Proceed-ings of CSCSI-86, Montreal, Quc.220
