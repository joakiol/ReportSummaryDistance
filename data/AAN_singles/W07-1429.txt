Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 177?184,Prague, June 2007. c?2007 Association for Computational LinguisticsBiology Based Alignments of Paraphrases for Sentence CompressionJoa?o CordeiroCLT and BioinformaticsUniversity of Beira InteriorCovilha?, Portugaljpaulo@di.ubi.ptGa?el DiasCLT and BioinformaticsUniversity of Beira InteriorCovilha?, Portugalddg@di.ubi.ptGuillaume CleuziouLIFO - University of Orle?ansOrle?ans, Franceguillaume.cleuziou@univ-orleans.frAbstract1 In this paper, we present a study for ex-tracting and aligning paraphrases in the con-text of Sentence Compression.
First, we jus-tify the application of a new measure for theautomatic extraction of paraphrase corpora.Second, we discuss the work done by (Barzi-lay & Lee, 2003) who use clustering of para-phrases to induce rewriting rules.
We willsee, through classical visualization method-ologies (Kruskal & Wish, 1977) and exhaus-tive experiments, that clustering may not bethe best approach for automatic pattern iden-tification.
Finally, we will provide some re-sults of different biology based methodolo-gies for pairwise paraphrase alignment.1 IntroductionSentence Compression can be seen as the removalof redundant words or phrases from an input sen-tence by creating a new sentence in which the gistof the original meaning of the sentence remains un-changed.
Sentence Compression takes an impor-tant place for Natural Language Processing (NLP)tasks where specific constraints must be satisfied,such as length in summarization (Barzilay & Lee,2002; Knight & Marcu, 2002; Shinyama et al, 2002;Barzilay & Lee, 2003; Le Nguyen & Ho, 2004;Unno et al, 2006), style in text simplification (Marsi& Krahmer, 2005) or sentence simplification forsubtitling (Daelemans et al, 2004).1Project partially funded by Portuguese FCT (Reference:POSC/PLP/57438/2004)Generally, Sentence Compression involves per-forming the following three steps: (1) Extractionof paraphrases from comparable corpora, (2) Align-ment of paraphrases and (3) Induction of rewritingrules.
Obviously, each of these steps can be per-formed in many different ways going from totallyunsupervised to totally supervised.In this paper, we will focus on the first two steps.In particular, we will first justify the application ofa new measure for the automatic extraction of para-phrase corpora.
Second, we will discuss the workdone by (Barzilay & Lee, 2003) who use cluster-ing of paraphrases to induce rewriting rules.
Wewill see, through classical visualization methodolo-gies (Kruskal & Wish, 1977) and exhaustive ex-periments, that clustering may not be the best ap-proach for automatic pattern identification.
Finally,we will provide some results of different biologybased methodologies for pairwise paraphrase align-ment.2 Related WorkTwo different approaches have been proposed forSentence Compression: purely statistical method-ologies (Barzilay & Lee, 2003; Le Nguyen & Ho,2004) and hybrid linguistic/statistic methodologies(Knight & Marcu, 2002; Shinyama et al, 2002;Daelemans et al, 2004; Marsi & Krahmer, 2005;Unno et al, 2006).As our work is based on the first paradigm, wewill focus on the works proposed by (Barzilay &Lee, 2003) and (Le Nguyen & Ho, 2004).
(Barzilay & Lee, 2003) present a knowledge-leanalgorithm that uses multiple-sequence alignment to177learn generate sentence-level paraphrases essentiallyfrom unannotated corpus data alone.
In contrast to(Barzilay & Lee, 2002), they need neither paral-lel data nor explicit information about sentence se-mantics.
Rather, they use two comparable corpora.Their approach has three main steps.
First, work-ing on each of the comparable corpora separately,they compute lattices compact graph-based repre-sentations to find commonalities within groups ofstructurally similar sentences.
Next, they identifypairs of lattices from the two different corpora thatare paraphrases of each other.
Finally, given an inputsentence to be paraphrased, they match it to a latticeand use a paraphrase from the matched lattices mateto generate an output sentence.
(Le Nguyen & Ho, 2004) propose a new sentence-reduction algorithm that do not use syntactic pars-ing for the input sentence.
The algorithm is an ex-tension of the template-translation algorithm (one ofexample-based machine-translation methods) via in-novative employment of the Hidden Markov model,which uses the set of template rules learned from ex-amples.In particular, (Le Nguyen & Ho, 2004) do notpropose any methodology to automatically extractparaphrases.
Instead, they collect a corpus by per-forming the decomposition program using news andtheir summaries.
After correcting them manually,they obtain more than 1,500 pairs of long and re-duced sentences.
Comparatively, (Barzilay & Lee,2003) propose to use the N-gram Overlap metricto capture similarities between sentences and auto-matically create paraphrase corpora.
However, thischoice is arbitrary and mainly leads to the extractionof quasi-exact or exact matching pairs.
For that pur-pose, we introduce a new metric, the Sumo-Metric.Unlike (Le Nguyen & Ho, 2004), one interestingidea proposed by (Barzilay & Lee, 2003) is to clus-ter similar pairs of paraphrases to apply multiple-sequence alignment.
However, once again, thischoice is not justified and we will see by classi-cal visualization methodologies (Kruskal & Wish,1977) and exhaustive experiments by applying dif-ferent clustering algorithms, that clustering may notbe the best approach for automatic pattern identifi-cation.
As a consequence, we will study global andlocal biology based sequence alignments comparedto multi-sequence alignment that may lead to betterresults for the induction of rewriting rules.3 Paraphrase Corpus ConstructionParaphrase corpora are golden resources for learningmonolingual text-to-text rewritten patterns.
How-ever, such corpora are expensive to construct manu-ally and will always be an imperfect and biased rep-resentation of the language paraphrase phenomena.Therefore, reliable automatic methodologies able toextract paraphrases from text and subsequently cor-pus construction are crucial, enabling better patternidentification.
In fact, text-to-text generation is aparticularly promising research direction given thatthere are naturally occurring examples of compara-ble texts that convey the same information but arewritten in different styles.
Web news stories are anobvious example.
Thus, presented with such texts,one can pair sentences that convey the same infor-mation, thereby building a training set of rewritingexamples i.e.
a paraphrase corpus.3.1 Paraphrase IdentificationA few unsupervised metrics have been applied toautomatic paraphrase identification and extraction(Barzilay & Lee, 2003; Dolan & Brockett, 2004).However, these unsupervised methodologies show amajor drawback by extracting quasi-exact2 or evenexact match pairs of sentences as they rely on clas-sical string similarity measures such as the Edit Dis-tance in the case of (Dolan & Brockett, 2004) andword N-gram overlap for (Barzilay & Lee, 2003).Such pairs are clearly useless.More recently, (Anonymous, 2007) proposed anew metric, the Sumo-Metric specially designedfor asymmetrical entailed pairs identification, andproved better performance over previous establishedmetrics, even in the specific case when tested withthe Microsoft Paraphrase Research Corpus (Dolan& Brockett, 2004).
For a given sentence pair, hav-ing each sentence x and y words, and with ?
exclu-sive links between the sentences, the Sumo-Metric isdefined in Equation 1 and 2.2Almost equal strings, for example: Bush said America isaddicted to oil.
and Mr. Bush said America is addicted to oil.178S(Sa, Sb) =8>><>>:S(x, y, ?)
if S(x, y, ?)
< 1.00 if ?
= 0e?k?S(x,y,?)
otherwise(1)whereS(x, y, ?)
= ?
log2(x? )
+ ?
log2(y? )
(2)with ?, ?
?
[0, 1] and ?+ ?
= 1.
(Anonymous, 2007) show that the Sumo-Metricoutperforms all state-of-the-art metrics over alltested corpora.
In particular, it shows systematicallybetter F-Measure and Accuracy measures over allother metrics showing an improvement of (1) at least2.86% in terms of F-Measure and 3.96% in termsof Accuracy and (2) at most 6.61% in terms of F-Measure and 6.74% in terms of Accuracy comparedto the second best metric which is also systemati-cally the word N-gram overlap similarity measureused by (Barzilay & Lee, 2003).3.2 ClusteringLiterature shows that there are two main reasons toapply clustering for paraphrase extraction.
On onehand, as (Barzilay & Lee, 2003) evidence, clustersof paraphrases can lead to better learning of text-to-text rewriting rules compared to just pairs of para-phrases.
On the other hand, clustering algorithmsmay lead to better performance than stand-alonesimilarity measures as they may take advantage ofthe different structures of sentences in the cluster todetect a new similar sentence.However, as (Barzilay & Lee, 2003) do not pro-pose any evaluation of which clustering algorithmshould be used, we experiment a set of clustering al-gorithms and present the comparative results.
Con-trarily to what expected, we will see that clusteringis not a worthy effort.Instead of extracting only sentence pairs from cor-pora3, one may consider the extraction of paraphrasesentence clusters.
There are many well-known clus-tering algorithms, which may be applied to a cor-pus sentence set S = {s1, ..., sn}.
Clustering im-plies the definition of a similarity or (distance) ma-trix An?n, where each each element aij is the simi-larity (distance) between sentences si and sj .3A pair may be seen as a cluster with only two elements.3.2.1 Experimental ResultsWe experimented four clustering algorithms on acorpus of web news stories and then three humanjudges manually cross-classified a random sampleof the generated clusters.
They were asked to clas-sify a cluster as a ?wrong cluster?
if it contained atleast two sentences without any entailment relationbetween them.
Results are shown in the next table 1.Table 1: Precision of clustering algorithmsBASE S-HAC C-HAC QT EM0.618 0.577 0.569 0.640 0.489The ?BASE?
column is the baseline, where theSumo-Metric was applied rather than clustering.Columns ?S-HAC?
and ?C-HAC?
express the re-sults for Single-link and Complete-link Hierarchi-cal Agglomerative Clustering (Jain et al, 1999).The ?QT?
column shows the Quality Threshold al-gorithm (Heyer et al, 1999) and the last column?EM?
is the Expectation Maximization clustering al-gorithm (Hogg et al, 2005).One main conclusion, from table 1 is that cluster-ing tends to achieve worst results than simple para-phrase pair extraction.
Only the QT achieves betterresults, but if we take the average of the four cluster-ing algorithms it is equal to 0.568, smaller than the0.618 baseline.
Moreover, these results with the QTalgorithm were applied with a very restrictive valuefor cluster attribution as it is shown in table 2 withan average of almost two sentences per cluster.Table 2: Figures about clustering algorithmsAlgorithm # Sentences/# ClustersS-HAC 6,23C-HAC 2,17QT 2,32EM 4,16In fact, table 2 shows that most of the clustershave less than 6 sentences which leads to questionthe results presented by (Barzilay & Lee, 2003) whoonly keep the clusters that contain more than 10 sen-tences.
In fact, the first conclusion is that the num-ber of experimented clusters is very low, and moreimportant, all clusters with more than 10 sentencesshowed to be of very bad quality.The next subsection will reinforce the sight that179clustering is a worthless effort for automatic para-phrase corpora construction.3.2.2 VisualizationIn this subsection, we propose a visual analy-sis of the different similarity measures tested pre-viously: the Edit Distance (Levenshtein, 1966), theBLEU metric (Papineni et al, 2001), the word N-gram overlap and the Sumo-Metric.
The goal of thisstudy is mainly to give the reader a visual interpre-tation about the organization each measure induceson the data.To perform this study, we use a MultidimensionalScaling (MDS) process which is a traditional dataanalysis technique.
MDS (Kruskal & Wish, 1977)allows to display the structure of distance-like datainto an Euclidean space.Since the only available information is a similar-ity in our case, we transform similarity values intodistance values as in Equation 3.dij = (sii ?
2sij + sjj)1/2 (3)This transformation enables to obtain a (pseudo)distance measure satisfying properties like minimal-ity, identity and symmetry.
On a theoretical pointof view, the measure we obtain is a pseudo-distanceonly, since triangular inequality is not necessary sat-isfied.
In practice, the projection space we build withthe MDS from such a pseudo-distance is sufficient tohave an idea about whether data are organized intoclasses.We perform the MDS process on 500 sentences4randomly selected from the Microsoft ResearchParaphrase Corpus.
In particular, the projection overthe three first eigenvectors (or proper vectors) pro-vides the best visualization where data are clearlyorganized into several classes (at least two classes).The obtained visualizations (Figure 1) show dis-tinctly that no particular data organization can bedrawn from the used similarity measures.
Indeed,we observe only one central class with some ?satel-lite?
data randomly placed around the class.The last observation allows us to anticipate on theresults we could obtain with a clustering step.
First,clustering seems not to be a natural way to manage4The limitation to 500 data is due to computation costs sinceMDS requires the diagonalization of the square similarity ordistance matrix.such data.
Then, according to the clustering methodused, several types of clusters can be expected: verysmall clusters which contain ?satellite?
data (prettyrelevant) or large clusters with part of the main cen-tral class (pretty irrelevant).
These results confirmthe observed figures in the previous subsection andreinforce the sight that clustering is a worthless ef-fort for automatic paraphrase corpora construction,contrarily to what (Barzilay & Lee, 2003) suggest.4 Biology Based AlignmentsSequence alignments have been extensively ex-plored in bioinformatics since the beginning of theHuman Genome Project.
In general, one wants toalign two sequences of symbols (genes in Biology)to find structural similarities, differences or transfor-mations between them.In NLP, alignment is relevant in sub-domainslike Text Generation (Barzilay & Lee, 2002).
Inour work, we employ alignment methods for align-ing words between two sentences, which are para-phrases.
The words are the base blocks of our se-quences (sentences).There are two main classes of pairwise align-ments: the global and local classes.
In the firstone, the algorithms try to fully align both sequences,admitting gap insertions at a certain cost, while inthe local methods the goal is to find pairwise sub-alignments.
How suitable each algorithm may beapplied to a certain problem is discussed in the nexttwo subsections.4.1 Global AlignmentThe well established and widely used Needleman-Wunsch algorithm for pairwise global sequencealignment, uses dynamic programming to find thebest possible alignment between two sequences.
It isan optimal algorithm.
However, it reveals space andtime inefficiency as sequence length increases, sincean m ?
n matrix must be maintained and processedduring computations.
This is the case with DNA se-quence alignments, composed by many thousands ofnucleotides.
Therefore, a huge optimization effortwere engaged and new algorithms appeared like k-tuple, not guaranteeing to find optimal alignmentsbut able to tackle the complexity problem.In our alignment tasks, we do not have these com-180plexity obstacles, because in our corpora the meanlength of a sentence is equal to 20.9 words, whichis considerably smaller than in a DNA sequence.Therefore an implementation of the Needleman-Wunsch algorithm has been used to generate optimalglobal alignments.The figure 2 exemplifies a global word alignmenton a paraphrase pair.4.2 Local AlignmentThe Smith-Waterman (SW) algorithm is similar tothe Needleman Wunsch (NW) one, since dynamicprogramming is also followed hence denoting thesimilar complexity issues, to which our alignmenttask is immune.
The main difference is that SWseeks optimal sub-alignments instead of a globalalignment and, as described in the literature, itis well tailored for pairs with considerable differ-ences5, in length and type.
In table 3 we exemplifythis by showing two character sequences6 where onemay clearly see that SW is preferable:N Char.
Sequences Alignments1 ABBAXYTRVRVTTRVTR XYTRVFWHWWHGWGFXYTVWGF XYT-V2 ABCDXYDRQR AB-CDDQZZSTABZCD ABZCDTable 3: Preferable local alignment cases.Remark that in the second pair, only the maximallocal sub-alignment is shown.
However, there ex-ists another sub-alignment: (DRQ, D-Q).
This meansthat local alignment may be tuned to generate notonly the maximum sub-alignment but a set of sub-alignments that satisfy some criterium, like havingalignment value greater than some minimum thresh-old.
In fact, this is useful in our word alignmentproblem and were experimented by adapting theSmith Waterman algorithm.4.3 Dynamic AlignmentAccording to the previous two subsections, wheretwo alignment strategies were presented, a natu-ral question rises: which alignment algorithm touse for our problem of inter-sentence word align-ment?
Initially, we thought to use only the global5With sufficient similar sequences there is no difference be-tween NW and SW.6As in DNA subsequences and is same for word sequences.alignment Needleman Wunsch algorithm, since acomplete inter-sentence word alignment is obtained.However, we noticed that this strategy is unappro-priate for certain pairs, specially when there are syn-tactical alternations, like in the next example:During his magnificent speech, :::the ::::::::president:::::::::remarkably::::::praised::::IBM::::::::research.
:::The::::::::president:::::::praised::::IBM::::::::research, during hisspeech.If a global alignment is applied for such a pair, thenweird alignments will be generated, like the one thatis shown in the next representation (we use charactersequences for space convenience and try to preservethe word first letter, from the previous example):D H M S T P R Q I S _ _ __ _ _ _ T P _ Q I S D H SHere it would be more adequate to apply local align-ment and extract all relevant sub-alignments.
In thiscase, two sub-alignments would be generated:|D H M S| |T P R P I R||D H _ S| |T P _ P I R|Therefore, for inter-paraphrase word alignments,we propose a dynamic algorithm which chooses thebest alignment to perform: global or local.
To com-pute this pre-scan, we regard the notion of link-crossing between sequences as illustrated in the fig-ure 3, where the 4 crossings are signalized with thesmall squares.It is easily verifiable that the maximum numberof crossings, among two sequences with n exclusivelinks in between is equal to ?
= 12 ?
n ?
(n ?
1).We suggest that if a fraction of these crossings holds,for example 0.4 ?
?
or 0.5 ?
?, then a local align-ment should be used.
Remark that the more this frac-tion tends to 1.0 the more unlikely it is to use globalalignment.Crossings may be calculated by taking index pairs?xi, yi?
to represent links between sequences, wherexi and yi are respectively the first and second se-quence indexes, for instance in figure 3 the ?U?link has pair ?5, 1?.
It is easily verifiable that twolinks ?xi, yi?
and ?xj , yj?
have a crossing point if:(xi ?
xj) ?
(yi ?
yj) < 0.4.4 Alignment with Similarity MatrixIn bioinformatics, DNA sequence alignment algo-rithms are usually guided by a scoring function, re-lated to the field of expertise, that defines what is181the mutation probability between nucleotides.
Thesescoring functions are defined by PAM7 or BLO-SUM8 matrices and encode evolutionary approx-imations regarding the rates and probabilities ofamino acid mutations.
Different matrices might pro-duce different alignments.Subsequently, this motivated the idea of model-ing word mutation.
It seems intuitive to allow sucha word mutation, considering the possible relation-ships that exit between words: lexical, syntacticalor semantic.
For example, it seems evident that be-tween spirit and spiritual there exists a stronger rela-tion (higher mutation probability) than between spir-itual and hamburger.A natural possibility to choose a word muta-tion representation function is the Edit-distance(Levenshtein, 1966) (edist(.,.))
as a negative re-ward for word alignment.
For a given word pair?wi, wj?, the greater the Edit-distance value, themore unlikely the word wi will be aligned withword wj .
However, after some early experimentswith this function, it revealed to lead to some prob-lems by enabling alignments between very differ-ent words, like ?total, israel?, ?fire,made?
or?troops,members?, despite many good alignmentsalso achieved.
This happens because the Edit-distance returns relatively small values, unable tosufficiently penalize different words, like the oneslisted before, to inhibit the alignment.
In bioinfor-matics language, it means that even for such pairsthe mutation probability is still high.
Another prob-lem of the Edit-distance is that it does not distin-guish between long and small words, for instancethe pairs ?in, by?
and ?governor, governed?
haveboth the Edit-distance equals to 2.As a consequence, we propose a new func-tion (Equation 4) for word mutation penaliza-tion, able to give better answers for the men-tioned problems.
The idea is to divide the Edit-distance value by the length of the normalized9maximum common subsequence maxseq(., .)
be-tween both words.
For example, the longestcommon subsequence for the pair ?w1, w2?
=?reinterpretation, interpreted?
is ?interpret?,7Point Access Mutation.8Blocks Substitution Matrices.9The length of the longest common subsequence divided bythe word with maximum length value.with length equal to 9 and maxseq(w1, w2) =9max{16,11} = 0.5625costAlign(wi, wj) = ?
edist(wi, wj)?+maxseq(wi, wj) (4)where ?
is a small value10 that acts like a?safety hook?
against divisions by zero, whenmaxseq(wi, wj) = 0.word 1 word 2 -edist costAlignrule ruler -1 -1.235governor governed -2 -2.632pay paying -3 -5.882reinterpretation interpreted -7 -12.227hamburger spiritual -9 -74.312in by -2 -200.000Table 4: Word mutation functions comparision.Remark that with the costAlign(., .)
scoringfunction the problems with pairs like ?in, by?
simplyvanish.
The smaller the words, the more constrainedthe mutation will be.5 Experiments and Results5.1 Corpus of ParaphrasesTo test our alignment method, we used two typesof corpora.
The first is the ?DUC 2002?
corpus(DUC2002) and the second is automatically ex-tracted from related web news stories (WNS) auto-matically extracted.
For both original corpora, para-phrase extraction has been performed by using theSumo-Metric and two corpora of paraphrases wereobtained.
Afterwards the alignment algorithm wasapplied over both corpora.5.2 Quality of Dynamic AlignmentWe tested the proposed alignment methods by givinga sample of 201 aligned paraphrase sentence pairsto a human judge and ask to classify each pair ascorrect, acorrect11, error 12, and merror13.
We alsoasked to classify the local alignment choice14 as ad-equate or inadequate.
The results are shown in thenext table:10We take ?
= 0.01.11Almost correct - minor errors exist12With some errors.13With many errors14Global or local alignment.182Global Localnot para correct acorrect error merror adequate31 108 28 12 8 12/1415.5% 63.5% 16.5% 7.1% 4.7% 85.7%Table 5: Precision of alignments.For global alignments15 we have 11.8% pairs withrelevant errors and 85.7% (12 from 14) of all lo-cal alignment decisions were classified as adequate.The not para column shows the number of falseparaphrases identified, revealing a precision value of84.5% for the Sumo-Metric.6 Conclusion and Future WorkA set of important steps toward automatic construc-tion of aligned paraphrase corpora are presented andinherent relevant issues discussed, like clusteringand alignment.
Experiments, by using 4 algorithmsand through visualization techniques, revealed thatclustering is a worthless effort for paraphrase cor-pora construction, contrary to the literature claims(Barzilay & Lee, 2003).
Therefore simple para-phrase pair extraction is suggested and by usinga recent and more reliable metric (Sumo-Metric)(Anonymous, 2007) designed for asymmetrical en-tailed pairs.
We also propose a dynamic choosing ofthe alignment algorithm and a word scoring functionfor the alignment algorithms.In the future we intend to clean the automaticconstructed corpus by introducing syntactical con-straints to filter the wrong alignments.
Our next stepwill be to employ Machine Learning techniques forrewriting rule induction, by using this automaticallyconstructed aligned paraphrase corpus.ReferencesBarzilay R. and Lee L. 2002.
Bootstrapping LexicalChoice via Multiple-Sequence Alignment.
Proceed-ings of the Conference on Empirical Methods in Nat-ural Language Processing, (EMNLP), 164-171.Barzilay, R., and Lee, L. 2003.
Learning to para-phrase: An unsupervised approach using multiple-sequence alignment.
Proceedings of HLT-NAACL.15Percentage are calculated by dividing by 170 (201 ?
31)the number of true paraphrases that exists.Dolan W.B.
and Brockett C. 2004.
Unsupervised con-struction of large paraphrase corpora: Exploitingmassively parallel news sources.
Proceedings of 20thInternational Conference on Computational Linguis-tics (COLING 2004).Anonymous 2007.
Learning Paraphrases from WNSCorpora.
Proceedings of 20th International FLAIRSConference.
AAAI Press.
Key West, Florida.Daelemans W., Hothker A., and Tjong E. 2004.
Auto-matic Sentence Simplification for Subtitling in Dutchand English.
In Proceedings of LREC 2004, Lisbon,Portugal.Heyer L.J., Kruglyak S. and Yooseph S. 1999.
ExploringExpression Data: Identification and Analysis of Coex-pressed Genes.
Genome Research, 9:1106-1115.Hogg R., McKean J., and Craig A.
2005 Introductionto Mathematical Statistics.
Upper Saddle River, NJ:Pearson Prentice Hall, 359-364.Jain A., Murty M. and Flynn P. Data clustering: a review.ACM Computing Surveys, 31:264-323Knight K. and Marcu D. 2002.
Summarization beyondsentence extraction: A probabilistic approach to sen-tence compression.
Artificial Intelligence, 139(1):91-107.Kruskal J.
B. and Wish M. 1977.
Multidimensional Scal-ing.
Sage Publications.
Beverly Hills.
CA.Le Nguyen M., Horiguchi S., A. S., and Ho B. T. 2004.Example-based sentence reduction using the hiddenmarkov model.
ACM Transactions on Asian LanguageInformation Processing (TALIP), 3(2):146-158.Levenshtein V. 1966.
Binary Codes Capable of Cor-recting Deletions, Insertions, and Reversals.
SovietPhysice-Doklady, 10:707-710.Marsi E. and Krahmer E. 2005.
Explorations in sentencefusion.
In Proceedings of the 10th European Work-shop on Natural Language Generation.Papineni K., Roukos S., Ward T., Zhu W.-J.
2001.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
IBM Research Report RC22176.Shinyama Y., Sekine S., and Sudo K. 2002.
Auto-matic Paraphrase Acquisition from News Articles.
SaoDiego, USA.Unno Y., Ninomiya T., Miyao Y. and Tsujii J.
2006.Trimming CFG Parse Trees for Sentence Compres-sion Using Machine Learning Approaches.
In the Pro-ceedings of the COLING/ACL 2006 Main ConferencePoster Sessions.183-0.2-0.15-0.1-0.0500.050.10.150.2-0.25-0.2-0.15-0.1-0.05 00.05 0.1 0.150.2-0.3-0.2-0.10 0.10.2 0.3Edit Distance-0.35-0.3-0.25-0.2-0.15-0.1-0.0500.050.10.150.2-0.2-0.15-0.1-0.050 0.050.1 0.15-0.2-0.15-0.1-0.050 0.050.1 0.150.2Word N-Gram Family-0.25-0.2-0.15-0.1-0.0500.050.10.150.20.25-0.2-0.15-0.1-0.05 00.05 0.1 0.150.2-0.2-0.15-0.1-0.05 00.05 0.10.15 0.2BLEU Metric-0.04-0.0200.020.04 -0.04-0.02  00.02 0.04-0.04-0.020 0.020.04Sumo-MetricFigure 1: MDS on 500 sentences with the Edit Distance (top left), the BLEU Metric (top right), the WordN-Gram Family (bottom left) and the Sumo-Metric (bottom right).To the horror of their television fans , Miss Ball and Arnaz were divorced in 1960.__ ___ ______ __ _____ __________ ____ _ ____ Ball and Arnaz ____ divorced in 1960.Figure 2: Global aligned words in a paraphrase pair.Figure 3: Crossings between a sequence pair.184
