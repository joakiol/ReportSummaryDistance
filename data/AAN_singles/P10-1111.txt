Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1087?1097,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsHard Constraints for Grammatical Function LabellingWolfgang SeekerUniversity of StuttgartInstitut fu?r Maschinelle Sprachverarbeitungseeker@ims.uni-stuttgart.deInes RehbeinUniversity of SaarlandDep.
for Comp.
Linguistics & Phoneticsrehbein@coli.uni-sb.deJonas KuhnUniversity of StuttgartInstitut fu?r Maschinelle Sprachverarbeitungjonas@ims.uni-stuttgart.deJosef van GenabithDublin City UniversityCNGL and School of Computingjosef@computing.dcu.ieAbstractFor languages with (semi-) free word or-der (such as German), labelling gramma-tical functions on top of phrase-structuralconstituent analyses is crucial for makingthem interpretable.
Unfortunately, moststatistical classifiers consider only localinformation for function labelling and failto capture important restrictions on thedistribution of core argument functionssuch as subject, object etc., namely thatthere is at most one subject (etc.)
perclause.
We augment a statistical classifierwith an integer linear program imposinghard linguistic constraints on the solutionspace output by the classifier, capturingglobal distributional restrictions.
We showthat this improves labelling quality, in par-ticular for argument grammatical func-tions, in an intrinsic evaluation, and, im-portantly, grammar coverage for treebank-based (Lexical-Functional) grammar ac-quisition and parsing, in an extrinsic eval-uation.1 IntroductionPhrase or constituent structure is often regarded asan analysis step guiding semantic interpretation,while grammatical functions (i. e. subject, object,modifier etc.)
provide important information rele-vant to determining predicate-argument structure.In languages with restricted word order (e. g.English), core grammatical functions can oftenbe recovered from configurational information inconstituent structure analyses.
By contrast, sim-ple constituent structures are not sufficient for lessconfigurational languages, which tend to encodegrammatical functions by morphological means(Bresnan, 2001).
Case features, for instance, canbe important indicators of grammatical functions.Unfortunately, many of these languages (includingGerman) exhibit strong syncretism where morpho-logical cues can be highly ambiguous with respectto functional information.Statistical classifiers have been successfullyused to label constituent structure parser outputwith grammatical function information (Blahetaand Charniak, 2000; Chrupa?a and Van Genabith,2006).
However, as these approaches tend touse only limited and local context informationfor learning and prediction, they often fail to en-force simple yet important global linguistic con-straints that exist for most languages, e. g. thatthere will be at most one subject (object) per sen-tence/clause.1?Hard?
linguistic constraints, such as these,tend to affect mostly the ?core grammatical func-tions?, i. e. the argument functions (rather thane.
g. adjuncts) of a particular predicate.
As thesefunctions constitute the core meaning of a sen-tence (as in: who did what to whom), it is impor-tant to get them right.
We present a system thatadds grammatical function labels to constituentparser output for German in a postprocessing step.We combine a statistical classifier with an inte-ger linear program (ILP) to model non-violableglobal linguistic constraints, restricting the solu-tion space of the classifier to those labellings thatcomply with our set of global constraints.
Thereare, of course, many other ways of including func-tional information into the output of a syntacticparser.
Klein and Manning (2003) show that merg-ing some linguistically motivated function labelswith specific syntactic categories can improve theperformance of a PCFG model on Penn-II En-1Coordinate subjects/objects form a constituent that func-tions as a joint subject/object.1087glish data.2 Tsarfaty and Sim?aan (2008) presenta statistical model (Relational-Realizational Pars-ing) that alternates between functional and config-urational information for constituency tree pars-ing and Hebrew data.
Dependency parsers likethe MST parser (McDonald and Pereira, 2006) andMalt parser (Nivre et al, 2007) use function labelsas core part of their underlying formalism.
In thispaper, we focus on phrase structure parsing withfunction labelling as a post-processing step.Integer linear programs have already been suc-cessfully used in related fields including semanticrole labelling (Punyakanok et al, 2004), relationand entity classification (Roth and Yih, 2004), sen-tence compression (Clarke and Lapata, 2008) anddependency parsing (Martins et al, 2009).
Earlywork on function labelling for German (Brants etal., 1997) reports 94.2% accuracy on gold data (avery early version of the TiGer Treebank (Brantset al, 2002)) using Markov models.
Klenner(2007) uses a system similar to ?
but more re-stricted than ?
ours to label syntactic chunks de-rived from the TiGer Treebank.
His research fo-cusses on the correct selection of predefined sub-categorisation frames for a verb (see also Klenner(2005)).
By contrast, our research does not involvesubcategorisation frames as an external resource,instead opting for a less knowledge-intensive ap-proach.
Klenner?s system was evaluated on goldtreebank data and used a small set of 7 dependencylabels.
We show that an ILP-based approach canbe scaled to a large and comprehensive set of 42labels, achieving 97.99% label accuracy on goldstandard trees.
Furthermore, we apply the sys-tem to automatically parsed data using a state-of-the-art statistical phrase-structure parser with a la-bel accuracy of 94.10%.
In both cases, the ILP-based approach improves the quality of argumentfunction labelling when compared with a non-ILP-approach.
Finally, we show that the approachsubstantially improves the quality and coverage(from 93.6% to 98.4%) of treebank-based Lexical-Functional Grammars for German over previouswork in Rehbein and van Genabith (2009).The paper is structured as follows: Section 2presents basic data demonstrating the challengespresented by German word order and case syn-cretism for the function labeller.
Section 3 de-2Table 6 shows that for our data a model with mergedcategory and function labels (but without hard constraints!
)performs slightly worse than the ILP approach developed inthis paper.scribes the labeller including the feature model ofthe classifier and the integer linear program usedto pick the correct labelling.
The evaluation part(Section 4) is split into an intrinsic evaluation mea-suring the quality of the labelling directly usingthe German TiGer Treebank (Brants et al, 2002),and an extrinsic evaluation where we test the im-pact of the constraint-based labelling on treebank-based automatic LFG grammar acquisition.2 DataUnlike English, German exhibits a relatively freeword order, i. e. in main clauses, the verb occu-pies second position (the last position in subor-dinated clauses) and arguments and adjuncts canbe placed (fairly) freely.
The grammatical func-tion of a noun phrase is marked morphologicallyon its constituting parts.
Determiners, pronouns,adjectives and nouns carry case markings and inorder to be well-formed, all parts of a noun phrasehave to agree on their case features.
German usesa nominative?accusative system to mark predicatearguments.
Subjects are marked with nominativecase, direct objects carry accusative case.
Further-more, indirect objects are mostly marked with da-tive case and sometimes genitive case.
(1) Der Lo?weNOMthe liongibtgivesdem WolfDATthe wolfeinen Besen.ACCa broomThe lion gives a broom to the wolf.
(1) shows a sentence containing the ditransi-tive verb geben (to give) with its three arguments.Here, the subject is unambiguously marked withnominative case (NOM), the indirect object withdative case (DAT) and the direct object with ac-cusative case (ACC).
(2) shows possible word or-ders for the arguments in this sentence.3(2) Der Lo?we gibt einen Besen dem Wolf.Dem Wolf gibt der Lo?we einen Besen.Dem Wolf gibt einen Besen der Lo?we.Einen Besen gibt der Lo?we dem Wolf.Einen Besen gibt dem Wolf der Lo?we.Since all permutations of arguments are possi-ble, there is no chance for a statistical classifier todecide on the correct function of a noun phrase byits position alone.
Introducing adjuncts to this ex-ample makes matters even worse.3Note that although (apart from the position of the finiteverb) there are no syntactic restrictions on the word order,there are restrictions pertaining to phonological or informa-tion structure.1088Case information for a given noun phrase cangive a classifier some clue about the correct ar-gument function, since functions are strongly re-lated to case values.
Unfortunately, the Germancase system is complex (see Eisenberg (2006) fora thorough description) and exhibits a high degreeof case syncretism.
(3) shows a sentence whereboth argument NPs are ambiguous between nom-inative or accusative case.
In such cases, addi-tional semantic or contextual information is re-quired for disambiguation.
A statistical classifier(with access to local information only) runs a highrisk of incorrectly classifying both NPs as sub-jects, or both as direct objects or even as nominalpredicates (which are also required to carry nom-inative case).
This would leave us with uninter-pretable results.
Uninterpretability of this kind canbe avoided if we are able to constrain the numberof subjects and objects globally to one per clause.4(3) Das SchafNOM/ACCthe sheepsiehtseesdas Ma?dchen.NOM/ACCthe girlEITHER The sheep sees the girlOR The girl sees the sheep.3 Grammatical Function LabellingOur function labeller was developed and tested onthe TiGer Treebank (Brants et al, 2002).
TheTiGer Treebank is a phrase-structure and gram-matical function annotated treebank with 50,000newspaper sentences from the Frankfurter Rund-schau (Release 2, July 2006).
Its overall anno-tation scheme is quite flat to account for the rel-atively free word order of German and does notallow for unary branching.
The annotations usenon-projective trees modelling long distance de-pendencies directly by crossing branches.
Wordsare lemmatised and part-of-speech tagged with theStuttgart-Tu?bingen Tag Set (STTS) (Schiller et al,1999) and contain morphological annotations (Re-lease 2).
TiGer uses 25 syntactic categories and aset of 42 function labels to annotate the grammat-ical function of a phrase.The function labeller consists of two main com-ponents, a maximum entropy classifier and an in-teger linear program.
This basic architecture wasintroduced by Punyakanok et al (2004) for thetask of semantic role labelling and since then hasbeen applied to different NLP tasks without signif-icant changes.
In our case, its input is a bare tree4Although the classifier may, of course, still identify thewrong phrase as subject or object.structure (as obtained by a standard phrase struc-ture parser) and it outputs a tree structure whereevery node is labelled with the grammatical rela-tion it bears to its mother node.
For each possi-ble label and for each node, the classifier assignsa probability that this node is labelled by this la-bel.
This results in a complete probability distri-bution over all labels for each node.
An integerlinear program then tries to find the optimal over-all tree labelling by picking for each node the labelwith the highest probability without violating anyof its constraints.
These constraints implement lin-guistic rules like the one-subject-per-sentence rulementioned above.
They can also be used to cap-ture treebank particulars, such as for example thatpunctuation marks never receive a label.3.1 The Feature ModelMaximum entropy classifiers have been used in awide range of applications in NLP for a long time(Berger et al, 1996; Ratnaparkhi, 1998).
Theyusually give good results while at the same timeallowing for the inclusion of arbitrarily complexfeatures.
They also have the advantage that theydirectly output probability distributions over theirset of labels (unlike e. g. SVMs).The classifier uses the following features:?
the lemma (if terminal node)?
the category (the POS for terminal nodes)?
the number of left/right sisters?
the category of the two left/right sisters?
the number of daughters?
the number of terminals covered?
the lemma of the left/right corner terminal?
the category of the left/right corner terminal?
the category of the mother node?
the category of the mother?s head node?
the lemma of the mother?s head node?
the category of the grandmother node?
the category of the grandmother?s head node?
the lemma of the grandmother?s head node?
the case features for noun phrases?
the category for PP objects?
the lemma for PP objects (if terminal node)These features are also computed for the headof the phrase, determined using a set of head-finding rules in the style of Magerman (1995)adapted to TiGer.
For lemmatisation, we use Tree-Tagger (Schmid, 1994) and case features of noun1089phrases are obtained from a full German morpho-logical analyser based on (Schiller, 1994).
If anoun phrase consists of a single word (e. g. pro-nouns, but also bare common nouns and propernouns), all case values output by the analyser areused to reflect the case syncretism.
For multi-wordnoun phrases, the case feature is computed by tak-ing the intersection of all case-bearing words in-side the noun phrase, i. e. determiners, pronouns,adjectives, common nouns and proper nouns.
If,for some reason (e.g., due to a bracketing error inphrase structure parsing), the intersection turns outto be empty, all four case values are assigned to thephrase.53.2 Constrained OptimisationIn the second step, a binary integer linear pro-gram is used to select those labels that optimise thewhole tree labelling.
A linear program consists ofa linear objective function that is to be maximised(or minimised) and a set of constraints which im-pose conditions on the variables of the objectivefunction (see (Clarke and Lapata, 2008) for a shortbut readable introduction).
Although solving a lin-ear program has polynomial complexity, requiringthe variables to be integral or binary makes find-ing a solution exponentially hard in the worst case.Fortunately, there are efficient algorithms whichare capable of handling a large number of vari-ables and constraints in practical applications.6For the function labeller, we define the set ofbinary variables V = N ?
L to be the crossprod-uct of the set of nodes N and the set of labels L.Setting a variable xn,l to 1 means that node n islabelled by label l. Every variable is weighted bythe probability wn,l = P (l|f(n)) which the clas-sifier has assigned to this node-label combination.The objective function that we seek to optimise isdefined as the sum over all weighted variables:max?n?N?l?Lwn,lxn,l (4)Since we want every node to receive exactly one5We decided to train the classifier on automaticallyassigned and possibly ambiguous morphological informa-tion instead of on the hand-annotated and manually disam-biguated morphological information provided by TiGer be-cause we want the classifier to learn the German case syn-cretism.
This way, the classifier will perform better when pre-sented with unseen data (e.g.
from parser output) for whichno hand-annotated morphological information is available.6See lpsolve (http://lpsolve.sourceforge.net/) or GLPK(http://www.gnu.org/software/glpk/glpk.html) for open-source implementationslabel, we add a constraint that for every node n,exactly one of its variables is set to 1.?l?Lxn,l = 1 (5)Up to now, the whole system is doing exactlythe same as an ordinary classifier that always takesthe most probable label for each node.
We willnow add additional global and local linguistic con-straints.7The first and most important constraint restrictsthe number of each argument function (as opposedto modifier functions) to at most one per clause.Let D ?
N ?
N be the direct dominance rela-tion between the nodes of the current tree.
For ev-ery node n with category S (sentence) or VP (verbphrase), at most one of its daughters is allowedto be labelled SB (subject).
The single-subject-function condition is defined as:cat(n) ?
{S, V P} ????n,m?
?Dxm,SB ?
1 (6)Identical constraints are added for labels OA,OA2, DA, OG, OP, PD, OC, EP.8We add further constraints to capture the follow-ing linguistic restrictions:?
Of all daughters of a phrase, only one is allowedto be labelled HD (head).??n,m?
?Dxm,HD ?
1 (7)?
If a noun phrase carries no case feature for nom-inative case, it cannot be labelled SB, PD or EP.case(n) 6= nom ???l?
{SB,PD,EP}xn,l = 0(8)?
If a noun phrase carries no case feature for ac-cusative case, it cannot be labelled OA or OA2.?
If a noun phrase carries no case feature for da-tive case, it cannot be labelled DA.?
If a noun phrase carries no case feature for gen-itive case, it cannot be labelled OG or AG9.7Note that some of these constraints are language specificin that they represent linguistic facts about German and donot necessarily hold for other languages.
Furthermore, theconstraints are treebank specific to a certain degree in thatthey use a TiGer-specific set of labels and are conditioned onTiGer-specific configurations and categories.8SB = subject, OA = accusative object, OA2 = sec-ond accusative object, DA = dative, OG = genitive object,OP = prepositional object, PD = predicate, OC = clausal ob-ject, EP = expletive es9AG = genitive adjunct1090Unlike Klenner (2007), we do not use prede-fined subcategorization frames, instead letting thestatistical model choose arguments.In TiGer, sentences whose main verbs areformed from auxiliary-participle combinations,are annotated by embedding the participle underan extra VP node and non-subject arguments aresisters to the participle.
Therefore we add an ex-tension of the constraint in (6) to the constraint setin order to also include the daughters of an embed-ded VP node in such a case.Because of the particulars of the annotationscheme of TiGer, we can decide some labels inadvance.
As mentioned before, punctuation doesnot get a label in TiGer.
We set the label for thosenodes to ??
(no label).
Other examples are:?
If a node?s category is PTKVZ (separated verbparticle), it is labeled SVP (separable verb par-ticle).cat(n) = PTKV Z ??
xn,SV P = 1 (9)?
If a node?s category is APPR, APPRART,APPO or APZR (prepositions), it is labeled AC(adpositional case marker).?
All daughters of an MTA node (multi-tokenadjective) are labeled ADC (adjective compo-nent).These constraints are conditioned on part-of-speech tags and require high POS-tagging accu-racy (when dealing with raw text).Due to the constraints imposed on the classifi-cation, the function labeller can no longer assigntwo subjects to the same S node.
Faced with twonodes whose most probable label is SB, it has todecide on one of them taking the next best label forthe other.
This way, it outputs the optimal solutionwith respect to the set of constraints.
Note that thisrequires the feature model not only to rank the cor-rect label highest but also to provide a reasonableranking of the other labels as well.4 EvaluationWe conducted a number of experiments using1,866 sentences of the TiGer Dependency Bank(Forst et al, 2004) as our test set.
The TiGerDB isa part of the TiGer Treebank semi-automaticallyconverted into a dependency representation.
Weuse the manually labelled TiGer trees correspond-ing to the sentences in the TiGerDB for assessingthe labelling quality in the intrinsic evaluation, andthe dependencies from TiGerDB for assessing thequality and coverage of the automatically acquiredLFG resources in the extrinsic evaluation.In order to test on real parser output, the testset was parsed with the Berkeley Parser (Petrov etal., 2006) trained on 48k sentences of the TiGercorpus (Table 1), excluding the test set.
Since theBerkeley Parser assumes projective structures, thetraining data and test data were made projective byraising non-projective nodes in the tree (Ku?bler,2005).precision 83.60 recall 82.81f-score 83.20 tagging acc.
97.97Table 1: evalb unlabelled parsing scores on test set for Berke-ley Parser trained on 48,000 sentences (sentence length?
40)The maximum entropy classifier of the func-tion labeller was trained on 46,473 sentences ofthe TiGer Treebank (excluding the test set) whichyields about 1.2 million nodes as training samples.For training the Maximum Entropy Model, weused the BLMVM algorithm (Benson and More,2001) with a width factor of 1.0 (Kazama and Tsu-jii, 2005) implemented in an open-source C++ li-brary from Tsujii Laboratory.10 The integer linearprogram was solved with the simplex algorithm incombination with a branch-and-bound method us-ing the freely available GLPK.114.1 Intrinsic EvaluationIn the intrinsic evaluation, we measured the qual-ity of the labelling itself.
We used the nodespan evaluation method of (Blaheta and Char-niak, 2000) which takes only those nodes into ac-count which have been recognised correctly by theparser, i.e.
if there are two nodes in the parse andthe reference treebank tree which cover the sameword span.
Unlike Blaheta and Charniak (2000)however, we do not require the two nodes to carrythe same syntactic category label.12Table 2 shows the results of the node span eval-uation.
The labeller achieves close to 98% labelaccuracy on gold treebank trees which shows thatthe feature model captures the differences betweenthe individual labels well.
Results on parser outputare about 4 percentage points (absolute) lower asparsing errors can distort local context features forthe classifier even if the node itself has been parsed10http://www-tsujii.is.s.u-tokyo.ac.jp/?tsuruoka/maxent/11http://www.gnu.org/software/glpk/glpk.html12We also excluded the root node, all punctuation marksand both nodes in unary branching sub-trees from evaluation.1091correctly.
The addition of the ILP constraints im-proves results only slightly since the constraintsaffect only (a small number of) argument labelswhile the evaluation considers all 40 labels occur-ring in the test set.
Since the constraints restrict theselection of certain labels, a less probable label hasto be picked by the labeller if the most probableis not available.
If the classifier is ranking labelssensibly, the correct label should emerge.
How-ever, with an incorrect ranking, the ILP constraintsmight also introduce new errors.label accuracy error red.without constraintsgold 44689/45691 = 97.81% ?parser 40578/43140 = 94.06% ?with constraintsgold 44773/45691 = 97.99%* 8.21%parser 40593/43140 = 94.10% 0.68%Table 2: label accuracy and error reduction (all labels) fornode span evaluation, * statistically significant, sign test, ?
=0.01 (Koo and Collins, 2005)As the main target of the constraint set are argu-ment functions, we also tested the quality of argu-ment labels.
Table 3 shows the node span evalua-tion in terms of precision, recall and f-score for ar-gument functions only, with clear statistically sig-nificant improvements.prec.
rec.
f-scorewithout constraintsgold standard 92.41 91.86 92.13parser output 88.14 86.43 87.28with constraintsgold standard 94.31 92.76 93.53*parser output 89.51 86.73 88.09*Table 3: node span results for the test set, argument functionsonly (SB, EP, PD, OA, OA2, DA, OG, OP, OC), * statisticallysignificant, sign test, ?
= 0.01 (Koo and Collins, 2005)For comparison and to establish a highly com-petitive baseline, we use the best-scoring systemin (Chrupa?a and Van Genabith, 2006), trained andtested on exactly the same data sets.
This purelystatistical labeller achieves accuracy of 96.44%(gold) and 92.81% (parser) for all labels, and f-scores of 89.88% (gold) and 84.98% (parser) forargument labels.
Tables 2 and 3 show that our sys-tem (with and even without ILP constraints) com-prehensively outperforms all corresponding base-line scores.The node span evaluation defines a correct la-belling by taking only those nodes (in parser out-put) into account that have a corresponding nodein the reference tree.
However, as this restricts at-tention to correctly parsed nodes, the results aresomewhat over-optimistic.
Table 4 provides theresults obtained from an evalb evaluation of thesame data sets.13 The gold standard scores arehigh confirming our previous findings about theperformance of the function labeller.
However,the results on parser output are much worse.
Theevaluation scores are now taking the parsing qual-ity into account (Table 1).
The considerable dropin quality between gold trees and parser outputclearly shows that a good parse tree is an impor-tant prerequisite for reasonable function labelling.This is in accordance with previous findings byPunyakanok et al (2008) who emphasise the im-portance of syntactic parsing for the closely re-lated task of semantic role labelling.prec.
rec.
f-scorewithout constraintsgold standard 95.94 95.94 95.94parser output 76.27 75.55 75.91with constraintsgold standard 96.21 96.21 96.21parser output 76.36 75.64 76.00Table 4: evalb results for the test set4.1.1 Subcategorisation FramesEarly on in the paper we mention that, unlike e. g.Klenner (2007), we did not include predefinedsubcategorisation frames into the constraint set,but rather let the joint statistical and ILP modelsdecide on the correct type of arguments assignedto a verb.
The assumption is that if one uses prede-fined subcategorisation frames which fix the num-ber and type of arguments for a verb, one runs therisk of excluding correct labellings due to missingsubcat frames, unless a very comprehensive andhigh quality subcat lexicon resource is available.In order to test this assumption, we run an addi-tional experiment with about 10,000 verb framesfor 4,508 verbs, which were automatically ex-tracted from our training section.
Following Klen-ner (2007), for each verb and for each subcat framefor this verb attested at least once in the trainingdata, we introduce a new binary variable fn tothe ILP model representing the n-th frame (for theverb) weighted by its frequency.We add an ILP constraint requiring exactly oneof the frames to be set to one (each verb has to havea subcat frame) and replace the ILP constraint in(6) by:13Function labels were merged with the category symbols.1092??n,m?
?Dxm,SB ?
?SB?fifi = 0 (10)This constraint requires the number of subjectsin a phrase to be equal to the number of selected14verb frames that require a subject.
As each verbis constrained to ?select?
exactly one subcat frame(see additional ILP constraint above), there is atmost one subject per phrase, if the frame in ques-tion requires a subject.
If the selected frame doesnot require a subject, then the constraint blocks theassignment of subjects for the entire phrase.
Thesame was done for the other argument functionsand as before we included an extension of this con-straint to cover embedded VPs.
For unseen verbs(i.e.
verbs not attested in the training set) we keepthe original constraints as a back-off.prec.
rec.
f-scoreall labels (cmp.
Table 2)gold standard 97.24 97.24 97.24parser output 93.43 93.43 93.43argument functions only (cmp.
Table 3)gold standard 91.36 90.12 90.74parser output 86.64 84.38 85.49Table 5: node span results for the test set using constraintswith automatically extracted subcat framesTable 5 shows the results of the test set nodespan evaluation when using the ILP system en-hanced with subcat frames.
Compared to Tables 2and 3, the results are clearly inferior, and particu-larly so for argument grammatical functions.
Thisseems to confirm our assumption that, given ourdata, letting the joint statistical and ILP model de-cide argument functions is superior to an approachthat involves subcat frames.
However, and impor-tantly, our results do not rule out that a more com-prehensive subcat frame resource may in fact re-sult in improvements.4.2 Extrinsic EvaluationOver the last number of years, treebank-baseddeep grammar acquisition has emerged as anattractive alternative to hand-crafting resourceswithin the HPSG, CCG and LFG paradigms(Miyao et al, 2003; Clark and Hockenmaier,2002; Cahill et al, 2004).
While most of the ini-tial development work focussed on English, morerecently efforts have branched to other languages.Below we concentrate on LFG.14The variable representing this frame has been set to 1.Lexical-Functional Grammar (Bresnan, 2001)is a constraint-based theory of grammar with min-imally two levels of representation: c(onstituent)-structure and f(unctional)-structure.
C-structure(CFG trees) captures language specific surfaceconfigurations such as word order and the hier-archical grouping of words into phrases, whilef-structure represents more abstract (and some-what more language independent) grammatical re-lations (essentially bilexical labelled dependencieswith some morphological and semantic informa-tion, approximating to basic predicate-argumentstructures) in the form of attribute-value struc-tures.
F-structures are defined in terms of equa-tions annotated to nodes in c-structure trees (gram-mar rules).
Treebank-based LFG acquisition wasoriginally developed for English (Cahill, 2004;Cahill et al, 2008) and is based on an f-structureannotation algorithm that annotates c-structuretrees (from a treebank or parser output) withf-structure equations, which are read off of the treeand passed on to a constraint solver producing anf-structure for the given sentence.
The Englishannotation algorithm (for Penn-II treebank-styletrees) relies heavily on configurational and catego-rial information, translating this into grammaticalfunctional information (subject, object etc.)
rep-resented at f-structure.
LFG is ?functional?
in themathematical sense, in that argument grammaticalfunctions have to be single valued (there cannot betwo or more subjects etc.
in the same clause).
Infact, if two or more values are assigned to a singleargument grammatical function in a local tree, theLFG constraint solver will produce a clash (i. e.it will fail to produce an f-structure) and the sen-tence will be considered ungrammatical (in otherwords, the corresponding c-structure tree will beuninterpretable).Rehbein (2009) and Rehbein and van Genabith(2009) develop an f-structure annotation algorithmfor German based on the TiGer treebank resource.Unlike the English annotation algorithm and be-cause of the language-particular properties of Ger-man (see Section 2), the German annotation al-gorithm cannot rely on c-structure configurationalinformation, but instead heavily uses TiGer func-tion labels in the treebank.
Learning function la-bels is therefore crucial to the German LFG an-notation algorithm, in particular when parsing rawtext.
Because of the strong case syncretism in Ger-man, traditional classification models using local1093information only run the risk of predicting mul-tiple occurences of the same function (subject,object etc.)
at the same level, causing featureclashes in the constraint solver with no f-structurebeing produced.
Rehbein (2009) and Rehbeinand van Genabith (2009) identify this as a majorproblem resulting in a considerable loss in cov-erage of the German annotation algorithm com-pared to English, in particular for parsing raw text,where TiGer function labels have to be supplied bya machine-learning-based method and where thecoverage of the LFG annotation algorithm dropsto 93.62% with corresponding drops in recall andf-scores for the f-structure evaluations (Table 6).Below we test whether the coverage problemscaused by incorrect multiple assignments of gram-matical functions can be addressed using the com-bination of classifier with ILP constraints devel-oped in this paper.
We report experiments whereautomatically parsed and labelled data are handedover to an LFG f-structure computation algorithm.The f-structures produced are converted into adependency triple representation (Crouch et al,2002) and evaluated against TiGerDB.cov.
prec.
rec.
f-scoreupper bound 99.14 85.63 82.58 84.07without constraintsgold 95.82 84.71 76.68 80.49parser 93.41 79.70 70.38 74.75with constraintsgold 99.30 84.62 82.15 83.37parser 98.39 79.43 75.60 77.47Rehbein 2009parser 93.62 79.20 68.86 73.67Table 6: f-structure evaluation results for the test set againstTigerDBTable 6 shows the results of the f-structureevaluation against TiGerDB, with 84.07% f-scoreupper-bound results for the f-structure annotationalgorithm on the original TiGer treebank treeswith hand-annotated function labels.
Using thefunction labeller without ILP constraints results indrastic drops in coverage (between 4.5% and 6.5%points absolute) and hence recall (6% and 12%)and f-score (3.5% and 9.5%) for both gold treesand parser output (compared to upper bounds).By contrast, with ILP constraints, the loss in cov-erage observed above almost completely disap-pears and recall and f-scores improve by between4.4% and 5.5% (recall) and 3% (f-score) abso-lute (over without ILP constraints).
For compar-ison, we repeated the experiment using the best-scoring method of Rehbein (2009).
Rehbein trainsthe Berkeley Parser to learn an extended categoryset, merging TiGer function labels with syntacticcategories, where the parser outputs fully-labelledtrees.
The results show that this approach suf-fers from the same drop in coverage as the classi-fier without ILP constraints, with recall about 7%and f-score about 4% (absolute) lower than for theclassifier with ILP constraints.Table 7 shows the dramatic effect of the ILPconstraints on the number of sentences in the testset that have multiple argument functions of thesame type within the same clause.
With ILP con-straints, the problem disappears and therefore, lessfeature-clashes occur during f-structure computa-tion.no constraints constraintsgold 185 0parser 212 0Table 7: Number of sentences in the test set with doubly an-notated argument functionsIn order to assess whether ILP constraints helpwith coverage only or whether they affect the qual-ity of the f-structures as well, we repeat the experi-ment in Table 6, however this time evaluating onlyon those sentences that receive an f-structure, ig-noring the rest.
Table 8 shows that the impact ofILP constraints on quality is much less dramaticthan on coverage, with only very small variationsin precison, recall and f-scores across the board,and small increases over Rehbein (2009).cov.
prec.
rec.
f-scoreno constr.
93.41 79.70 77.89 78.79constraints 98.39 79.43 77.85 78.64Rehbein 93.62 79.20 76.43 77.79Table 8: f-structure evaluation results for parser output ex-cluding sentences without f-structuresEarly work on automatic LFG acquisition andparsing for German is presented in Cahill et al(2003) and Cahill (2004), adapting the EnglishAnnotation Algorithm to an earlier and smallerversion of the TiGer treebank (without morpho-logical information) and training a parser to learnmerged Tiger function-category labels, and report-ing 95.75% coverage and an f-score of 74.56%f-structure quality against 2,000 gold treebanktrees automatically converted into f-structures.Rehbein (2009) uses the larger Release 2 of thetreebank (with morphological information) report-ing 77.79% f-score and coverage of 93.62% (Ta-1094ble 8) against the dependencies in the TiGerDBtest set.
The only rule-based approach to GermanLFG-parsing we are aware of is the hand-craftedGerman grammar in the ParGram Project (Buttet al, 2002).
Forst (2007) reports 83.01% de-pendency f-score evaluated against a set of 1,497sentences of the TiGerDB.
It is very difficult tocompare results across the board, as individual pa-pers use (i) different versions of the treebank, (ii)different (sections of) gold-standards to evaluateagainst (gold TiGer trees in TigerDB, the depen-dency representations provided by TigerDB, auto-matically generated gold-standards etc.)
and (iii)different label/grammatical function sets.
Further-more, (iv) coverage differs drastically (with thehand-crafted LFG resources achieving about 80%full f-structures) and finally, (v) some of the gram-mars evaluated having been used in the generationof the gold standards, possibly introducing a biastowards these resources: the German hand-craftedLFG was used to produce TiGerDB (Forst et al,2004).
In order to put the results into some per-spective, Table 9 shows an evaluation of our re-sources against a set of automatically generatedgold standard f-structures produced by using thef-structure annotation algorithm on the originalhand-labelled TiGer gold trees in the section cor-responding to TiGerDB: without ILP constraintswe achieve a dependency f-score of 84.35%, withILP constraints 87.23% and 98.89% coverage.cov.
prec.
rec.
f-scorewithout constraintsgold 95.24 97.76 90.93 94.22parser 93.35 88.71 80.40 84.35with constraintsgold 99.30 97.66 97.33 97.50parser 98.89 88.37 86.12 87.23Table 9: f-structure evaluation results for the test set againstautomatically generated goldstandard (1,850 sentences)5 ConclusionIn this paper, we addressed the problem of assign-ing grammatical functions to constituent struc-tures.
We have proposed an approach to grammat-ical function labelling that combines the flexibil-ity of a statistical classifier with linguistic expertknowledge in the form of hard constraints imple-mented by an integer linear program.
These con-straints restrict the solution space of the classifierby blocking those solutions that cannot be correct.One of the strengths of an integer linear programis the unlimited context it can take into accountby optimising over the entire structure, providingan elegant way of supporting classifiers with ex-plicit linguistic knowledge while at the same timekeeping feature models small and comprehensi-ble.
Most of the constraints are direct formaliza-tions of linguistic generalizations for German.
Ourapproach should generalise to other languages forwhich linguistic expertise is available.We evaluated our system on the TiGer corpusand the TiGerDB and gave results on gold stan-dard trees and parser output.
We also appliedthe German f-structure annotation algorithm tothe automatically labelled data and evaluated thesystem by measuring the quality of the resultingf-structures.
We found that by using the con-straint set, the function labeller ensures the inter-pretability and thus the usefulness of the syntac-tic structure for a subsequently applied processingstep.
In our f-structure evaluation, that means, thef-structure computation algorithm is able to pro-duce an f-structure for almost all sentences.AcknowledgementsThe first author would like to thank Gerlof Boumafor a lot of very helpful discussions.
We wouldlike to thank our anonymous reviewers for de-tailed and helpful comments.
The research wassupported by the Science Foundation Ireland SFI(Grant 07/CE/I1142) as part of the Centre forNext Generation Localisation (www.cngl.ie) andby DFG (German Research Foundation) throughSFB 632 Potsdam-Berlin and SFB 732 Stuttgart.ReferencesSteven J. Benson and Jorge J.
More.
2001.
A limitedmemory variable metric method in subspaces andbound constrained optimization problems.
Techni-cal report, Argonne National Laboratory.Adam L. Berger, Vincent J.D.
Pietra, and Stephen A.D.Pietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational linguis-tics, 22(1):71.Don Blaheta and Eugene Charniak.
2000.
Assigningfunction tags to parsed text.
In Proceedings of the1st North American chapter of the Association forComputational Linguistics conference, pages 234 ?240, Seattle, Washington.
Morgan Kaufmann Pub-lishers Inc.Thorsten Brants, Wojciech Skut, and Brigitte Krenn.1997.
Tagging grammatical functions.
In Proceed-ings of EMNLP, volume 97, pages 64?74.1095Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Proceedings of the Workshop on Tree-banks and Linguistic Theories, page 2441.Joan Bresnan.
2001.
Lexical-Functional Syntax.Blackwell Publishers.Miriam Butt, Helge Dyvik, Tracy Halloway King, Hi-roshi Masuichi, and Christian Rohrer.
2002.
Theparallel grammar project.
In COLING-02 on Gram-mar engineering and evaluation-Volume 15, volumepages, page 7.
Association for Computational Lin-guistics.Aoife Cahill, Martin Forst, Mairead McCarthy, RuthODonovan, Christian Rohrer, Josef van Genabith,and Andy Way.
2003.
Treebank-based multilingualunification-grammar development.
In Proceedingsof the Workshop on Ideas and Strategies for Multi-lingual Grammar Development at the 15th ESSLLI,page 1724.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josefvan Genabith, and Andy Way.
2004.
Long-distance dependency resolution in automatically ac-quired wide-coverage PCFG-based LFG approxima-tions.
Proceedings of the 42nd Annual Meetingon Association for Computational Linguistics - ACL?04, pages 319?es.Aoife Cahill, Michael Burke, Ruth O?Donovan, StefanRiezler, Josef van Genabith, and Andy Way.
2008.Wide-Coverage Deep Statistical Parsing Using Au-tomatic Dependency Structure Annotation.
Compu-tational Linguistics, 34(1):81?124, Ma?rz.Aoife Cahill.
2004.
Parsing with Automatically Ac-quired, Wide-Coverage, Robust, Probabilistic LFGApproximations.
Ph.D. thesis, Dublin City Univer-sity.Grzegorz Chrupa?a and Josef Van Genabith.
2006.Using machine-learning to assign function labelsto parser output for Spanish.
In Proceedings ofthe COLING/ACL main conference poster session,page 136143, Sydney.
Association for Computa-tional Linguistics.Stephen Clark and Judith Hockenmaier.
2002.
Evalu-ating a wide-coverage CCG parser.
In Proceedingsof the LREC 2002, pages 60?66.James Clarke and Mirella Lapata.
2008.
Global in-ference for sentence compression an integer linearprogramming approach.
Journal of Artificial Intelli-gence Research, 31:399?429.Richard Crouch, Ronald M. Kaplan, Tracy HallowayKing, and Stefan Riezler.
2002.
A comparison ofevaluation metrics for a broad-coverage stochasticparser.
In Proceedings of LREC 2002 Workshop,pages 67?74, Las Palmas, Canary Islands, Spain.Peter Eisenberg.
2006.
Grundriss der deutschenGrammatik: Das Wort.
J.B. Metzler, Stuttgart, 3edition.Martin Forst, Nu?ria Bertomeu, Berthold Crysmann,Frederik Fouvry, Silvia Hansen-Shirra, and ValiaKordoni.
2004.
Towards a dependency-based goldstandard for German parsers The TiGer DependencyBank.
In Proceedings of the COLING Workshopon Linguistically Interpreted Corpora (LINC ?04),Geneva, Switzerland.Martin Forst.
2007.
Filling Statistics with LinguisticsProperty Design for the Disambiguation of GermanLFG Parses.
In Proceedings of ACL 2007.
Associa-tion for Computational Linguistics.Jun?Ichi Kazama and Jun?Ichi Tsujii.
2005.
Maxi-mum entropy models with inequality constraints: Acase study on text categorization.
Machine Learn-ing, 60(1):159194.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of ACL2003, pages 423?430, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Manfred Klenner.
2005.
Extracting Predicate Struc-tures from Parse Trees.
In Proceedings of theRANLP 2005.Manfred Klenner.
2007.
Shallow dependency label-ing.
In Proceedings of the ACL 2007 Demo andPoster Sessions, page 201204, Prague.
Associationfor Computational Linguistics.Terry Koo and Michael Collins.
2005.
Hidden-variable models for discriminative reranking.
InProceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing - HLT ?05, pages 507?514, Mor-ristown, NJ, USA.
Association for ComputationalLinguistics.Sandra Ku?bler.
2005.
How Do Treebank AnnotationSchemes Influence Parsing Results?
Or How Not toCompare Apples And Oranges.
In Proceedings ofRANLP 2005, Borovets, Bulgaria.David M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of the 33rd an-nual meeting on Association for Computational Lin-guistics, page 276283, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics Morristown,NJ, USA.Andre?
F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formu-lations for dependency parsing.
In Proceedings ofACL 2009.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proceedings of EACL, volume 6.Yusuke Miyao, Takashi Ninomiya, and Jun?ichi Tsujii.2003.
Probabilistic modeling of argument structuresincluding non-local dependencies.
In Proceedingsof the Conference on Recent Advances in NaturalLanguage Processing RANLP 2003, volume 2.1096Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, Gu?lsen Eryigit, Sandra Ku?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135, Januar.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the ACL- ACL ?06, pages 433?440, Morristown, NJ, USA.Association for Computational Linguistics.Vasin Punyakanok, Wen-Tau Yih, Dan Roth, and DavZimak.
2004.
Semantic role labeling via integerlinear programming inference.
In Proceedings ofthe 20th international conference on ComputationalLinguistics - COLING ?04, Morristown, NJ, USA.Association for Computational Linguistics.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.The Importance of Syntactic Parsing and Inferencein Semantic Role Labeling.
Computational Linguis-tics, 34(2):257?287, Juni.Adwait Ratnaparkhi.
1998.
Maximum Entropy Modelsfor Natural Language Ambiguity Resolution.
Ph.D.thesis, University of Pennsylvania.Ines Rehbein and Josef van Genabith.
2009.
Auto-matic Acquisition of LFG Resources for German-As Good as it gets.
In Miriam Butt and Tracy Hol-loway King, editors, Proceedings of LFG Confer-ence 2009.
CSLI Publications.Ines Rehbein.
2009.
Treebank-based grammar acqui-sition for German.
Ph.D. thesis, Dublin City Uni-versity.Dan Roth and Wen-Tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Proceedings of CoNNL 2004.Anne Schiller, Simone Teufel, and Christine Sto?ckert.1999.
Guidelines fu?r das Tagging deutscherTextcorpora mit STTS (Kleines und gro?es Tagset).Technical Report August, Universita?t Stuttgart.Anne Schiller.
1994.
Dmor - user?s guide.
Technicalreport, University of Stuttgart.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofInternational Conference on New Methods in Lan-guage Processing, volume 12.
Manchester, UK.Reut Tsarfaty and Khalil Sima?an.
2008.
Relational-realizational parsing.
In Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics - COLING ?08, pages 889?896, Morristown, NJ,USA.
Association for Computational Linguistics.1097
