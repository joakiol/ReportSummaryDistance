Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 9?17,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsViterbi Training Improves Unsupervised Dependency ParsingValentin I. SpitkovskyComputer Science DepartmentStanford University and Google Inc.valentin@cs.stanford.eduHiyan AlshawiGoogle Inc.Mountain View, CA, 94043, USAhiyan@google.comDaniel Jurafsky and Christopher D. ManningDepartments of Linguistics and Computer ScienceStanford University, Stanford, CA, 94305, USAjurafsky@stanford.edu and manning@cs.stanford.eduAbstractWe show that Viterbi (or ?hard?)
EM iswell-suited to unsupervised grammar in-duction.
It is more accurate than standardinside-outside re-estimation (classic EM),significantly faster, and simpler.
Our ex-periments with Klein and Manning?s De-pendency Model with Valence (DMV) at-tain state-of-the-art performance ?
44.8%accuracy on Section 23 (all sentences) ofthe Wall Street Journal corpus ?
withoutclever initialization; with a good initial-izer, Viterbi training improves to 47.9%.This generalizes to the Brown corpus,our held-out set, where accuracy reaches50.8% ?
a 7.5% gain over previous bestresults.
We find that classic EM learns bet-ter from short sentences but cannot copewith longer ones, where Viterbi thrives.However, we explain that both algorithmsoptimize the wrong objectives and provethat there are fundamental disconnects be-tween the likelihoods of sentences, bestparses, and true parses, beyond the well-established discrepancies between likeli-hood, accuracy and extrinsic performance.1 IntroductionUnsupervised learning is hard, often involving dif-ficult objective functions.
A typical approach isto attempt maximizing the likelihood of unlabeleddata, in accordance with a probabilistic model.Sadly, such functions are riddled with local op-tima (Charniak, 1993, Ch.
7, inter alia), since theirnumber of peaks grows exponentially with in-stances of hidden variables.
Furthermore, a higherlikelihood does not always translate into superiortask-specific accuracy (Elworthy, 1994; Merialdo,1994).
Both complications are real, but we willdiscuss perhaps more significant shortcomings.We prove that learning can be error-prone evenin cases when likelihood is an appropriate mea-sure of extrinsic performance and where globaloptimization is feasible.
This is because a keychallenge in unsupervised learning is that the de-sired likelihood is unknown.
Its absence renderstasks like structure discovery inherently under-constrained.
Search-based algorithms adopt sur-rogate metrics, gambling on convergence to the?right?
regularities in data.
Their wrong objec-tives create cases in which both efficiency and per-formance improve when expensive exact learningtechniques are replaced by cheap approximations.We propose using Viterbi training (Brownet al, 1993), instead of inside-outside re-estimation (Baker, 1979), to induce hierarchicalsyntactic structure from natural language text.
Ourexperiments with Klein and Manning?s (2004) De-pendency Model with Valence (DMV), a popularstate-of-the-art model (Headden et al, 2009; Co-hen and Smith, 2009; Spitkovsky et al, 2009),beat previous benchmark accuracies by 3.8% (onSection 23 of WSJ) and 7.5% (on parsed Brown).Since objective functions used in unsupervisedgrammar induction are provably wrong, advan-tages of exact inference may not apply.
It makessense to try the Viterbi approximation ?
it is alsowrong, only simpler and cheaper than classic EM.As it turns out, Viterbi EM is not only faster butalso more accurate, consistent with hypotheses ofde Marcken (1995) and Spitkovsky et al (2009).We begin by reviewing the model, standard datasets and metrics, and our experimental results.
Af-ter relating our contributions to prior work, wedelve into proofs by construction, using the DMV.9Corpus Sentences POS Tokens Corpus Sentences POS TokensWSJ1 159 159 WSJ13 12,270 110,760WSJ2 499 839 WSJ14 14,095 136,310WSJ3 876 1,970 WSJ15 15,922 163,715WSJ4 1,394 4,042 WSJ20 25,523 336,555WSJ5 2,008 7,112 WSJ25 34,431 540,895WSJ6 2,745 11,534 WSJ30 41,227 730,099WSJ7 3,623 17,680 WSJ35 45,191 860,053WSJ8 4,730 26,536 WSJ40 47,385 942,801WSJ9 5,938 37,408 WSJ45 48,418 986,830WSJ10 7,422 52,248 WSJ100 49,206 1,028,054WSJ11 8,856 68,022 Section 23 2,353 48,201WSJ12 10,500 87,750 Brown100 24,208 391,796 5 10 15 20 25 30 35 40 4551015202530354045Thousandsof SentencesThousandsof Tokens 100200300400500600700800900WSJkFigure 1: Sizes of WSJ{1, .
.
.
, 45, 100}, Section 23 of WSJ?
and Brown100 (Spitkovsky et al, 2009).NNS VBD IN NN ?Payrolls fell in September .P = (1?0z }| {PSTOP(?, L, T)) ?
PATTACH(?, L, VBD)?
(1?
PSTOP(VBD, L, T)) ?
PATTACH(VBD, L, NNS)?
(1?
PSTOP(VBD, R, T)) ?
PATTACH(VBD, R, IN)?
(1?
PSTOP(IN, R, T)) ?
PATTACH(IN, R, NN)?
PSTOP(VBD, L, F) ?
PSTOP(VBD, R, F)?
PSTOP(NNS, L, T) ?
PSTOP(NNS, R, T)?
PSTOP(IN, L, T) ?
PSTOP(IN, R, F)?
PSTOP(NN, L, T) ?
PSTOP(NN, R, T)?
PSTOP(?, L, F)| {z }1?
PSTOP(?, R, T)| {z }1.Figure 2: A dependency structure for a short sen-tence and its probability, as factored by the DMV,after summing out PORDER (Spitkovsky et al, 2009).2 Dependency Model with ValenceThe DMV (Klein and Manning, 2004) is a single-state head automata model (Alshawi, 1996) overlexical word classes {cw} ?
POS tags.
Its gener-ative story for a sub-tree rooted at a head (of classch) rests on three types of independent decisions:(i) initial direction dir ?
{L, R} in which to attachchildren, via probability PORDER(ch); (ii) whether toseal dir, stopping with probability PSTOP(ch, dir, adj),conditioned on adj ?
{T, F} (true iff consideringdir?s first, i.e., adjacent, child); and (iii) attach-ments (of class ca), according to PATTACH(ch, dir, ca).This produces only projective trees.
A root token?
generates the head of a sentence as its left (andonly) child.
Figure 2 displays a simple example.The DMV lends itself to unsupervised learn-ing via inside-outside re-estimation (Baker, 1979).Viterbi training (Brown et al, 1993) re-estimateseach next model as if supervised by the previousbest parse trees.
And supervised learning fromreference parse trees is straight-forward, sincemaximum-likelihood estimation reduces to count-ing: P?ATTACH(ch, dir, ca) is the fraction of children ?those of class ca ?
attached on the dir side of ahead of class ch; P?STOP(ch, dir, adj = T), the frac-tion of words of class ch with no children on thedir side; and P?STOP(ch, dir, adj = F), the ratio1 of thenumber of words of class ch having a child on thedir side to their total number of such children.3 Standard Data Sets and EvaluationThe DMV is traditionally trained and tested oncustomized subsets of Penn English Treebank?sWall Street Journal portion (Marcus et al, 1993).Following Klein and Manning (2004), we be-gin with reference constituent parses and com-pare against deterministically derived dependen-cies: after pruning out all empty sub-trees, punc-tuation and terminals (tagged # and $) not pro-nounced where they appear, we drop all sentenceswith more than a prescribed number of tokensremaining and use automatic ?head-percolation?rules (Collins, 1999) to convert the rest, as is stan-dard practice.
We experiment with WSJk (sen-tences with at most k tokens), for 1 ?
k ?
45, andSection 23 of WSJ?
(all sentence lengths).
Wealso evaluate on Brown100, similarly derived fromthe parsed portion of the Brown corpus (Francisand Kucera, 1979), as our held-out set.
Figure 1shows these corpora?s sentence and token counts.Proposed parse trees are judged on accuracy: adirected score is simply the overall fraction of cor-rectly guessed dependencies.
Let S be a set ofsentences, with |s| the number of terminals (to-1The expected number of trials needed to get oneBernoulli(p) success is n ?
Geometric(p), with n ?
Z+,P(n) = (1 ?
p)n?1p and E(n) = p?1; MoM and MLEagree, p?
= (# of successes)/(# of trials).105 10 15 20 25 30 35 4010203040506070OracleAd-Hoc?UninformedWSJkDirectedDependencyAccuracyonWSJ40(a) %-Accuracy for Inside-Outside (Soft EM)5 10 15 20 25 30 35 4010203040506070OracleAd-Hoc?
UninformedWSJk(training on all WSJ sentences up to k tokens in length)DirectedDependencyAccuracyonWSJ40(b) %-Accuracy for Viterbi (Hard EM)5 10 15 20 25 30 35 4050100150200350400OracleAd-Hoc?UninformedWSJkIterationstoConvergence(c) Iterations for Inside-Outside (Soft EM)5 10 15 20 25 30 35 4050100150200OracleAd-Hoc?UninformedWSJkIterationstoConvergence(d) Iterations for Viterbi (Hard EM)Figure 3: Directed dependency accuracies attained by the DMV, when trained on WSJk, smoothed, thentested against a fixed evaluation set, WSJ40, for three different initialization strategies (Spitkovsky et al,2009).
Red, green and blue graphs represent the supervised (maximum-likelihood oracle) initialization,a linguistically-biased initializer (Ad-Hoc?)
and the uninformed (uniform) prior.
Panel (b) shows resultsobtained with Viterbi training instead of classic EM ?
Panel (a), but is otherwise identical (in both, eachof the 45 vertical slices captures five new experimental results and arrows connect starting performancewith final accuracy, emphasizing the impact of learning).
Panels (c) and (d) show the correspondingnumbers of iterations until EM?s convergence.kens) for each s ?
S. Denote by T (s) the setof all dependency parse trees of s, and let ti(s)stand for the parent of token i, 1 ?
i ?
|s|, int(s) ?
T (s).
Call the gold reference t?
(s) ?
T (s).For a given model of grammar, parameterized by?, let t??
(s) ?
T (s) be a (not necessarily unique)likeliest (also known as Viterbi) parse of s:t??
(s) ?
{arg maxt?T (s)P?
(t)};then ?
?s directed accuracy on a reference set R is100% ?
?s?R?|s|i=1 1{t?
?i (s)=t?i (s)}?s?R |s|.4 Experimental Setup and ResultsFollowing Spitkovsky et al (2009), we trained theDMV on data sets WSJ{1, .
.
.
, 45} using three ini-tialization strategies: (i) the uninformed uniformprior; (ii) a linguistically-biased initializer, Ad-Hoc?
;2 and (iii) an oracle ?
the supervised MLEsolution.
Standard training is without smoothing,iterating each run until successive changes in over-all per-token cross-entropy drop below 2?20 bits.We re-trained all models using Viterbi EMinstead of inside-outside re-estimation, exploredLaplace (add-one) smoothing during training, andexperimented with hybrid initialization strategies.2Ad-Hoc?
is Spitkovsky et al?s (2009) variation on Kleinand Manning?s (2004) ?ad-hoc harmonic?
completion.115 10 15 20 25 30 35 4010203040506070OracleAd-Hoc?UninformedBaby StepsWSJkDirectedDependencyAccuracyonWSJ40(a) %-Accuracy for Inside-Outside (Soft EM)5 10 15 20 25 30 35 4010203040506070OracleAd-Hoc?
UninformedBaby StepsWSJkDirectedDependencyAccuracyonWSJ40(b) %-Accuracy for Viterbi (Hard EM)Figure 4: Superimposes directed accuracies attained by DMV models trained with Laplace smoothing(brightly-colored curves) over Figure 3(a,b); violet curves represent Baby Steps (Spitkovsky et al, 2009).4.1 Result #1: Viterbi-Trained ModelsThe results of Spitkovsky et al (2009), testedagainst WSJ40, are re-printed in Figure 3(a); ourcorresponding Viterbi runs appear in Figure 3(b).We observe crucial differences between the twotraining modes for each of the three initializationstrategies.
Both algorithms walk away from thesupervised maximum-likelihood solution; how-ever, Viterbi EM loses at most a few points ofaccuracy (3.7% at WSJ40), whereas classic EMdrops nearly twenty points (19.1% at WSJ45).
Inboth cases, the single best unsupervised result iswith good initialization, although Viterbi peaksearlier (45.9% at WSJ8) and in a narrower range(WSJ8-9) than classic EM (44.3% at WSJ15;WSJ13-20).
The uniform prior never quite gets offthe ground with classic EM but manages quite wellunder Viterbi training,3 given sufficient data ?
iteven beats the ?clever?
initializer everywhere pastWSJ10.
The ?sweet spot?
at WSJ15 ?
a neigh-borhood where both Ad-Hoc?
and the oracle ex-cel under classic EM ?
disappears with Viterbi.Furthermore, Viterbi does not degrade with more(complex) data, except with a biased initializer.More than a simple efficiency hack, Viterbi EMactually improves performance.
And its benefits torunning times are also non-trivial: it not only skipscomputing the outside charts in every iteration butalso converges (sometimes an order of magnitude)3In a concurrently published related work, Cohen andSmith (2010) prove that the uniform-at-random initializer is acompetitive starting M-step for Viterbi EM; our uninformedprior consists of uniform multinomials, seeding the E-step.faster than classic EM (see Figure 3(c,d)).44.2 Result #2: Smoothed ModelsSmoothing rarely helps classic EM and hurts inthe case of oracle training (see Figure 4(a)).
WithViterbi, supervised initialization suffers much less,the biased initializer is a wash, and the uninformeduniform prior generally gains a few points of ac-curacy, e.g., up 2.9% (from 42.4% to 45.2%, eval-uated against WSJ40) at WSJ15 (see Figure 4(b)).Baby Steps (Spitkovsky et al, 2009) ?
iterativere-training with increasingly more complex datasets, WSJ1, .
.
.
,WSJ45 ?
using smoothed Viterbitraining fails miserably (see Figure 4(b)), due toViterbi?s poor initial performance at short sen-tences (possibly because of data sparsity and sen-sitivity to non-sentences ?
see examples in ?7.3).4.3 Result #3: State-of-the-Art ModelsSimply training up smoothed Viterbi at WSJ15,using the uninformed uniform prior, yields 44.8%accuracy on Section 23 of WSJ?, already beatingprevious state-of-the-art by 0.7% (see Table 1(A)).Since both classic EM and Ad-Hoc?
initializerswork well with short sentences (see Figure 3(a)),it makes sense to use their pre-trained models toinitialize Viterbi training, mixing the two strate-gies.
We judged all Ad-Hoc?
initializers againstWSJ15 and found that the one for WSJ8 mini-mizes sentence-level cross-entropy (see Figure 5).This approach does not involve reference parse4For classic EM, the number of iterations to convergenceappears sometimes inversely related to performance, givingcredence to the notion of early termination as a regularizer.12Model Incarnation WSJ10 WSJ20 WSJ?DMV Bilingual Log-Normals (tie-verb-noun) (Cohen and Smith, 2009) 62.0 48.0 42.2 Brown100Less is More (Ad-Hoc?
@15) (Spitkovsky et al, 2009) 56.2 48.2 44.1 43.3A.
Smoothed Viterbi Training (@15), Initialized with the Uniform Prior 59.9 50.0 44.8 48.1B.
A Good Initializer (Ad-Hoc?
?s @8), Classically Pre-Trained (@15) 63.8 52.3 46.2 49.3C.
Smoothed Viterbi Training (@15), Initialized with B 64.4 53.5 47.8 50.5D.
Smoothed Viterbi Training (@45), Initialized with C 65.3 53.8 47.9 50.8EVG Smoothed (skip-head), Lexicalized (Headden et al, 2009) 68.8Table 1: Accuracies on Section 23 of WSJ{10, 20,? }
and Brown100 for three recent state-of-the-artsystems, our initializer, and smoothed Viterbi-trained runs that employ different initialization strategies.5 10 15 20 25 30 35 40 454.55.05.5WSJkbptlowest cross-entropy (4.32bpt) attained at WSJ8x-Entropy h (in bits per token) on WSJ15Figure 5: Sentence-level cross-entropy on WSJ15for Ad-Hoc?
initializers of WSJ{1, .
.
.
, 45}.trees and is therefore still unsupervised.
Using theAd-Hoc?
initializer based on WSJ8 to seed classictraining at WSJ15 yields a further 1.4% gain in ac-curacy, scoring 46.2% on WSJ?
(see Table 1(B)).This good initializer boosts accuracy attainedby smoothed Viterbi at WSJ15 to 47.8% (see Ta-ble 1(C)).
Using its solution to re-initialize train-ing at WSJ45 gives a tiny further improvement(0.1%) on Section 23 of WSJ?
but bigger gainson WSJ10 (0.9%) and WSJ20 (see Table 1(D)).Our results generalize.
Gains due to smoothedViterbi training and favorable initialization carryover to Brown100 ?
accuracy improves by 7.5%over previous published numbers (see Table 1).55 Discussion of Experimental ResultsThe DMV has no parameters to capture syntacticrelationships beyond local trees, e.g., agreement.Spitkovsky et al (2009) suggest that classic EMbreaks down as sentences get longer precisely be-cause the model makes unwarranted independenceassumptions.
They hypothesize that the DMV re-serves too much probability mass for what shouldbe unlikely productions.
Since EM faithfully al-locates such re-distributions across the possibleparse trees, once sentences grow sufficiently long,this process begins to deplete what began as like-lier structures.
But medium lengths avoid a floodof exponentially-confusing longer sentences (and5In a sister paper, Spitkovsky et al (2010) improve perfor-mance by incorporating parsing constraints harvested fromthe web into Viterbi training; nevertheless, results presentedin this paper remain the best of models trained purely on WSJ.the sparseness of unrepresentative shorter ones).6Our experiments corroborate this hypothesis.First of all, Viterbi manages to hang on to su-pervised solutions much better than classic EM.Second, Viterbi does not universally degrade withmore (complex) training sets, except with a biasedinitializer.
And third, Viterbi learns poorly fromsmall data sets of short sentences (WSJk, k < 5).Viterbi may be better suited to unsupervisedgrammar induction compared with classic EM, butneither is sufficient, by itself.
Both algorithmsabandon good solutions and make no guaranteeswith respect to extrinsic performance.
Unfortu-nately, these two approaches share a deep flaw.6 Related Work on Improper ObjectivesIt is well-known that maximizing likelihood may,in fact, degrade accuracy (Pereira and Schabes,1992; Elworthy, 1994; Merialdo, 1994).
de Mar-cken (1995) showed that classic EM suffers froma fatal attraction towards deterministic grammarsand suggested a Viterbi training scheme as a rem-edy.
Liang and Klein?s (2008) analysis of errorsin unsupervised learning began with the inappro-priateness of the likelihood objective (approxima-tion), explored problems of data sparsity (estima-tion) and focused on EM-specific issues related tonon-convexity (identifiability and optimization).Previous literature primarily relied on experi-mental evidence.
de Marcken?s analytical result isan exception but pertains only to EM-specific lo-cal attractors.
Our analysis confirms his intuitionsand moreover shows that there can be global pref-erences for deterministic grammars ?
problemsthat would persist with tractable optimization.
Weprove that there is a fundamental disconnect be-tween objective functions even when likelihood isa reasonable metric and training data are infinite.6Klein and Manning (2004) originally trained the DMVon WSJ10 and Gillenwater et al (2009) found it useful to dis-card data from WSJ3, which is mostly incomplete sentences.137 Proofs (by Construction)There is a subtle distinction between three differ-ent probability distributions that arise in parsing,each of which can be legitimately termed ?likeli-hood?
?
the mass that a particular model assignsto (i) highest-scoring (Viterbi) parse trees; (ii) thecorrect (gold) reference trees; and (iii) the sen-tence strings (sums over all derivations).
A classicunsupervised parser trains to optimize the third,makes actual parsing decisions according to thefirst, and is evaluated against the second.
Thereare several potential disconnects here.
First of all,the true generative model ??
may not yield thelargest margin separations for discriminating be-tween gold parse trees and next best alternatives;and second, ??
may assign sub-optimal mass tostring probabilities.
There is no reason why an op-timal estimate ??
should make the best parser orcoincide with a peak of an unsupervised objective.7.1 The Three Likelihood ObjectivesA supervised parser finds the ?best?
parameters??
by maximizing the likelihood of all referencestructures t?
(s) ?
the product, over all sentences,of the probabilities that it assigns to each such tree:?
?SUP = arg max?L(?)
= arg max??sP?(t?
(s)).For the DMV, this objective function is convex ?its unique peak is easy to find and should matchthe true distribution ??
given enough data, barringpractical problems caused by numerical instabilityand inappropriate independence assumptions.
It isoften easier to work in log-probability space:?
?SUP = arg max?
logL(?
)= arg max?
?s log P?(t?
(s)).Cross-entropy, measured in bits per token (bpt),offers an interpretable proxy for a model?s quality:h(?)
= ?
?s lg P?(t?
(s))?s |s|.Clearly, arg max?
L(?)
= ?
?SUP = arg min?
h(?
).Unsupervised parsers cannot rely on referencesand attempt to jointly maximize the probability ofeach sentence instead, summing over the probabil-ities of all possible trees, according to a model ?:?
?UNS = arg max?
?slog?t?T (s)P?(t)?
??
?P?
(s).This objective function is not convex and in gen-eral does not have a unique peak, so in practice oneusually settles for ?
?UNS ?
a fixed point.
There is noreason why ?
?SUP should agree with ?
?UNS, which isin turn (often badly) approximated by ?
?UNS, in ourcase using EM.
A logical alternative to maximiz-ing the probability of sentences is to maximize theprobability of the most likely parse trees instead:7?
?VIT = arg max?
?slog P?(t??
(s)).This 1-best approximation similarly arrives at ?
?VIT,with no claims of optimality.
Each next model isre-estimated as if supervised by reference parses.7.2 A Warm-Up Case: Accuracy vs.
?
?SUP 6= ?
?A simple way to derail accuracy is to maximizethe likelihood of an incorrect model, e.g., one thatmakes false independence assumptions.
Considerfitting the DMV to a contrived distribution ?
twoequiprobable structures over identical three-tokensentences from a unary vocabulary { a?
}:(i) x xa?
a?
a?
; (ii) y ya?
a?
a?.There are six tokens and only two have childrenon any given side, so adjacent stopping MLEs are:P?STOP( a?, L, T) = P?STOP( a?, R, T) = 1 ?26 =23 .The rest of the estimated model is deterministic:P?ATTACH(?, L, a?)
= P?ATTACH( a?, ?, a?)
= 1and P?STOP( a?, ?, F) = 1,since all dependents are a?
and every one is anonly child.
But the DMV generates left- and right-attachments independently, allowing a third parse:(iii) x ya?
a?
a?.It also cannot capture the fact that all structures arelocal (or that all dependency arcs point in the samedirection), admitting two additional parse trees:(iv) a?
xa?
a?
; (v) ya?
a?
a?.Each possible structure must make four (out of six)adjacent stops, incurring identical probabilities:P?STOP( a?, ?, T)4 ?
(1 ?
P?STOP( a?, ?, T))2 =2436 .7It is also possible to use k-best Viterbi, with k > 1.14Thus, the MLE model does not break symmetryand rates each of the five parse trees as equallylikely.
Therefore, its expected per-token accuracyis 40%.
Average overlaps between structures (i-v)and answers (i,ii) are (i) 100% or 0; (ii) 0 or 100%;and (iii,iv,v) 33.3%: (3+3)/(5?3) = 2/5 = 0.4.A decoy model without left- or right-branching,i.e., P?STOP( a?, L, T) = 1 or P?STOP( a?, R, T) = 1,would assign zero probability to some of the train-ing data.
It would be forced to parse every instanceof a?
a?
a?
either as (i) or as (ii), deterministically.Nevertheless, it would attain a higher per-token ac-curacy of 50%.
(Judged on exact matches, at thegranularity of whole trees, the decoy?s guaranteed50% accuracy clobbers the MLE?s expected 20%.
)Our toy data set could be replicated n-fold with-out changing the analysis.
This confirms that, evenin the absence of estimation errors or data sparsity,there can be a fundamental disconnect betweenlikelihood and accuracy, if the model is wrong.87.3 A Subtler Case: ??
= ?
?SUP vs.
?
?UNS vs.
?
?VITWe now prove that, even with the right model,mismatches between the different objective like-lihoods can also handicap the truth.
Our calcula-tions are again exact, so there are no issues withnumerical stability.
We work with a set of param-eters ??
already factored by the DMV, so that itsproblems could not be blamed on invalid indepen-dence assumptions.
Yet we are able to find anotherimpostor distribution ??
that outshines ?
?SUP = ??
onboth unsupervised metrics, which proves that thetrue models ?
?SUP and ??
are not globally optimal,as judged by the two surrogate objective functions.This next example is organic.
We began withWSJ10 and confirmed that classic EM abandonsthe supervised solution.
We then iteratively dis-carded large portions of the data set, so long asthe remainder maintained the (un)desired effect ?EM walking away from its ??SUP.
This procedureisolated such behavior, arriving at a minimal set:NP : NNP NNP ??
Marvin Alisky.S : NNP VBD ?
(Braniff declined).NP-LOC : NNP NNP ?Victoria, Texas8And as George Box quipped, ?Essentially, all models arewrong, but some are useful?
(Box and Draper, 1987, p. 424).This kernel is tiny, but, as before, our analysis isinvariant to n-fold replication: the problem cannotbe explained away by a small training size ?
itpersists even in infinitely large data sets.
And so,we consider three reference parse trees for two-token sentences over a binary vocabulary { a?, z?
}:(i) xa?
a?
; (ii) xa?
z?
; (iii) ya?
a?.One third of the time, z?
is the head; only a?
canbe a child; and only a?
has right-dependents.
Trees(i)-(iii) are the only two-terminal parses generatedby the model and are equiprobable.
Thus, thesesentences are representative of a length-two re-striction of everything generated by the true ??
:PATTACH(?, L, a?)
=23 and PSTOP( a?, ?, T) =45 ,since a?
is the head two out of three times, andsince only one out of five a?
?s attaches a child oneither side.
Elsewhere, the model is deterministic:PSTOP( z?, L, T) = 0;PSTOP(?, ?, F) = PSTOP( z?, R, T) = 1;PATTACH( a?, ?, a?)
= PATTACH( z?, L, a?)
= 1.Contrast the optimal estimate ?
?SUP = ??
with thedecoy fixed point9 ??
that is identical to ?
?, exceptP?STOP( a?, L, T) =35 and P?STOP( a?, R, T) = 1.The probability of stopping is now 3/5 on the leftand 1 on the right, instead of 4/5 on both sides ???
disallows a?
?s right-dependents but preserves itsoverall fertility.
The probabilities of leaves a?
(nochildren), under the models ?
?SUP and ?
?, are:P?
( a?)
= P?STOP( a?, L, T)?P?STOP( a?, R, T) =(45)2and P?
( a?)
= P?STOP( a?, L, T)?P?STOP( a?, R, T) =35 .And the probabilities of, e.g., structurexa?
z?, are:P?ATTACH(?, L, z?)
?
P?STOP( z?, R, T)?
(1 ?
P?STOP( z?, L, T)) ?
P?STOP( z?, L, F)?
P?ATTACH( z?, L, a?)
?
P?
( a?
)9The model estimated from the parse trees induced by ?
?over the three sentences is again ?
?, for both soft and hard EM.15= P?ATTACH(?, L, z?)
?
P?
( a?)
=13 ?1625and P?ATTACH(?, L, z?)
?
P?
( a?)
=13 ?35 .Similarly, the probabilities of all four possibleparse trees for the two distinct sentences, a?
a?
anda?
z?, under the two models, ?
?SUP = ??
and ?
?, are:?
?SUP = ??
??xa?
z?
13` 1625?= 13` 35?=1675 = 0.213 15 = 0.2ya?
z?
0 0xa?
a?
23` 45?
`1?
45?
` 1625?= 23`1 ?
35?
` 35?=1281875 = 0.06826 425 = 0.16ya?
a?
0.06826 0To the three true parses, ?
?SUP assigns probability(1675) ( 1281875)2 ?
0.0009942 ?
about 1.66bpt; ?
?leaves zero mass for (iii), corresponding to a larger(infinite) cross-entropy, consistent with theory.So far so good, but if asked for best (Viterbi)parses, ?
?SUP could still produce the actual trees,whereas ??
would happily parse sentences of (iii)and (i) the same, perceiving a joint probability of(0.2)(0.16)2 = 0.00512 ?
just 1.27bpt, appear-ing to outperform ?
?SUP = ??!
Asked for sentenceprobabilities, ??
would remain unchanged (it parseseach sentence unambiguously), but ?
?SUP would ag-gregate to(1675) (2 ?
1281875)2 ?
0.003977, improv-ing to 1.33bpt, but still noticeably ?worse?
than ?
?.Despite leaving zero probability to the truth, ?
?beats ??
on both surrogate metrics, globally.
Thisseems like an egregious error.
Judged by (extrin-sic) accuracy, ??
still holds its own: it gets fourdirected edges from predicting parse trees (i) and(ii) completely right, but none of (iii) ?
a solid66.7%.
Subject to tie-breaking, ??
is equally likelyto get (i) and/or (iii) entirely right or totally wrong(they are indistinguishable): it could earn a perfect100%, tie ?
?, or score a low 33.3%, at 1:2:1 odds,respectively ?
same as ??
?s deterministic 66.7%accuracy, in expectation, but with higher variance.8 Discussion of Theoretical ResultsDaume?
et al (2009) questioned the benefits of us-ing exact models in approximate inference.
In ourcase, the model already makes strong simplifyingassumptions and the objective is also incorrect.
Itmakes sense that Viterbi EM sometimes works,since an approximate wrong ?solution?
could, bychance, be better than one that is exactly wrong.One reason why Viterbi EM may work well isthat its score is used in selecting actual outputparse trees.
Wainwright (2006) provided strongtheoretical and empirical arguments for using thesame approximate inference method in trainingas in performing predictions for a learned model.He showed that if inference involves an approxi-mation, then using the same approximate methodto train the model gives even better performanceguarantees than exact training methods.
If our taskwere not parsing but language modeling, wherethe relevant score is the sum of the probabilitiesover individual derivations, perhaps classic EMwould not be doing as badly, compared to Viterbi.Viterbi training is not only faster and more accu-rate but also free of inside-outside?s recursion con-straints.
It therefore invites more flexible model-ing techniques, including discriminative, feature-rich approaches that target conditional likelihoods,essentially via (unsupervised) self-training (Clarket al, 2003; Ng and Cardie, 2003; McClosky etal., 2006a; McClosky et al, 2006b, inter alia).Such ?learning by doing?
approaches may berelevant to understanding human language ac-quisition, as children frequently find themselvesforced to interpret a sentence in order to inter-act with the world.
Since most models of humanprobabilistic parsing are massively pruned (Juraf-sky, 1996; Chater et al, 1998; Lewis and Vasishth,2005, inter alia), the serial nature of Viterbi EM?
or the very limited parallelism of k-best Viterbi?
may be more appropriate in modeling this taskthan the fully-integrated inside-outside solution.9 ConclusionWithout a known objective, as in unsupervisedlearning, correct exact optimization becomes im-possible.
In such cases, approximations, althoughliable to pass over a true optimum, may achievefaster convergence and still improve performance.We showed that this is the case with Viterbitraining, a cheap alternative to inside-outside re-estimation, for unsupervised dependency parsing.We explained why Viterbi EM may be partic-ularly well-suited to learning from longer sen-tences, in addition to any general benefits to syn-chronizing approximation methods across learn-ing and inference.
Our best algorithm is sim-pler and an order of magnitude faster than clas-sic EM.
It achieves state-of-the-art performance:3.8% higher accuracy than previous published best16results on Section 23 (all sentences) of the WallStreet Journal corpus.
This improvement general-izes to the Brown corpus, our held-out evaluationset, where the same model registers a 7.5% gain.Unfortunately, approximations alone do notbridge the real gap between objective functions.This deeper issue could be addressed by drawingparsing constraints (Pereira and Schabes, 1992)from specific applications.
One example of suchan approach, tied to machine translation, is syn-chronous grammars (Alshawi and Douglas, 2000).An alternative ?
observing constraints induced byhyper-text mark-up, harvested from the web ?
isexplored in a sister paper (Spitkovsky et al, 2010),published concurrently.AcknowledgmentsPartially funded by NSF award IIS-0811974 and by the AirForce Research Laboratory (AFRL), under prime contractno.
FA8750-09-C-0181; first author supported by the Fan-nie & John Hertz Foundation Fellowship.
We thank An-gel X. Chang, Mengqiu Wang and the anonymous reviewersfor many helpful comments on draft versions of this paper.ReferencesH.
Alshawi and S. Douglas.
2000.
Learning dependencytransduction models from unannotated examples.
InRoyal Society of London Philosophical Transactions Se-ries A, volume 358.H.
Alshawi.
1996.
Head automata for speech translation.
InProc.
of ICSLP.J.
K. Baker.
1979.
Trainable grammars for speech recogni-tion.
In Speech Communication Papers for the 97th Meet-ing of the Acoustical Society of America.G.
E. P. Box and N. R. Draper.
1987.
Empirical Model-Building and Response Surfaces.
John Wiley.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19.E.
Charniak.
1993.
Statistical Language Learning.
MITPress.N.
Chater, M. J. Crocker, and M. J. Pickering.
1998.
Therational analysis of inquiry: The case of parsing.
InM.
Oaksford and N. Chater, editors, Rational Models ofCognition.
Oxford University Press.S.
Clark, J. Curran, and M. Osborne.
2003.
BootstrappingPOS-taggers using unlabelled data.
In Proc.
of CoNLL.S.
B. Cohen and N. A. Smith.
2009.
Shared logistic nor-mal distributions for soft parameter tying in unsupervisedgrammar induction.
In Proc.
of NAACL-HLT.S.
B. Cohen and N. A. Smith.
2010.
Viterbi training forPCFGs: Hardness results and competitiveness of uniforminitialization.
In Proc.
of ACL.M.
Collins.
1999.
Head-Driven Statistical Models for Nat-ural Language Parsing.
Ph.D. thesis, University of Penn-sylvania.H.
Daume?, III, J. Langford, and D. Marcu.
2009.
Search-based structured prediction.
Machine Learning, 75(3).C.
de Marcken.
1995.
Lexical heads, phrase structure andthe induction of grammar.
In WVLC.D.
Elworthy.
1994.
Does Baum-Welch re-estimation helptaggers?
In Proc.
of ANLP.W.
N. Francis and H. Kucera, 1979.
Manual of Informationto Accompany a Standard Corpus of Present-Day EditedAmerican English, for use with Digital Computers.
De-partment of Linguistic, Brown University.J.
Gillenwater, K. Ganchev, J. Grac?a, B. Taskar, andF.
Pereira.
2009.
Sparsity in grammar induction.
InNIPS: Grammar Induction, Representation of Languageand Language Learning.W.
P. Headden, III, M. Johnson, and D. McClosky.
2009.Improving unsupervised dependency parsing with richercontexts and smoothing.
In Proc.
of NAACL-HLT.D.
Jurafsky.
1996.
A probabilistic model of lexical and syn-tactic access and disambiguation.
Cognitive Science, 20.D.
Klein and C. D. Manning.
2004.
Corpus-based inductionof syntactic structure: Models of dependency and con-stituency.
In Proc.
of ACL.R.
L. Lewis and S. Vasishth.
2005.
An activation-basedmodel of sentence processing as skilled memory retrieval.Cognitive Science, 29.P.
Liang and D. Klein.
2008.
Analyzing the errors of unsu-pervised learning.
In Proc.
of HLT-ACL.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19(2).D.
McClosky, E. Charniak, and M. Johnson.
2006a.
Effec-tive self-training for parsing.
In Proc.
of NAACL-HLT.D.
McClosky, E. Charniak, and M. Johnson.
2006b.
Rerank-ing and self-training for parser adaptation.
In Proc.
ofCOLING-ACL.B.
Merialdo.
1994.
Tagging English text with a probabilisticmodel.
Computational Linguistics, 20(2).V.
Ng and C. Cardie.
2003.
Weakly supervised natural lan-guage learning without redundant views.
In Proc.
of HLT-NAACL.F.
Pereira and Y. Schabes.
1992.
Inside-outside reestimationfrom partially bracketed corpora.
In Proc.
of ACL.V.
I. Spitkovsky, H. Alshawi, and D. Jurafsky.
2009.
BabySteps: How ?Less is More?
in unsupervised dependencyparsing.
In NIPS: Grammar Induction, Representation ofLanguage and Language Learning.V.
I. Spitkovsky, D. Jurafsky, and H. Alshawi.
2010.
Profit-ing from mark-up: Hyper-text annotations for guided pars-ing.
In Proc.
of ACL.M.
J. Wainwright.
2006.
Estimating the ?wrong?
graphicalmodel: Benefits in the computation-limited setting.
Jour-nal of Machine Learning Research, 7.17
