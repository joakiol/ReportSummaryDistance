Proceedings of the NAACL-HLT 2007 Doctoral Consortium, pages 33?36,Rochester, April 2007. c?2007 Association for Computational LinguisticsCombining Evidence for Improved Speech RetrievalJ.
Scott OlssonDepartment of MathematicsUniversity of MarylandCollege Park, MD 20742olsson@math.umd.eduAbstractThe goal of my dissertation research isto investigate the combination of new ev-idence sources for improving informa-tion retrieval on speech collections.
Theutility of these evidence sources is ex-pected to vary depending on how well theyare matched to a collection?s domain.
Ioutline several new evidence sources forspeech retrieval, situate them in the con-text of this domain dependency, and de-tail several methods for their combinationwith speech recognition output.
Secondly,I highlight completed and proposed workfor the production of this evidence.1 Introduction and GoalEarly research in spoken document retrieval (SDR)was spurred by a new way to overcome the highcost of producing metadata (e.g., human assignedtopic labels) or manual transcripts for spoken doc-uments: large vocabulary continuous speech recog-nition.
In this sense, SDR research has always beenabout making do with the available evidence.
Withthe advent of automatic speech recognition (ASR),this available evidence simply grew from being onlyexpensive human annotations to comparatively low-cost machine producible transcripts.But today even more evidence is available for re-trieving speech: (1) Using ASR text as input fea-tures, text classification can be applied to spokendocument collections to automatically produce topiclabels; (2) vocabulary independent spoken term de-tection (STD) systems have been developed whichcan search for query words falling outside of anASR system?s fixed vocabulary.
These evidencesources can be thought of as two bookends to thespectrum of domain dependence and independence.On one end, topic labels can significantly improveretrieval performance but require the creation ofa (presumably domain-dependent) topic thesaurusand training data.
Furthermore, classification accu-racy will be poor if the ASR system?s vocabulary isbadly matched to the collection?s speech (e.g., weshouldn?t expect a classifier to sensibly hypothesizeautomotive topics if the ASR system can not out-put words about cars or driving).
On the other end,STD systems offer the most promise precisely whenthe ASR system?s vocabulary is poorly matched tothe domain.
If the ASR system?s vocabulary alreadyincludes every word in the domain, after all, STDcan hardly be expected to help.The primary goal of this dissertation is (1) to ex-plore the combination of these new evidence sourceswith the features available in ASR transcripts orword lattices for SDR and (2) to determine theirsuitability in various domain-matching conditions.Secondarily, I?ll explore improving the productionof these new resources themselves (e.g., by classify-ing with temporal domain knowledge or more robustterm detection methods).Research in SDR has been inhibited by the ab-sence of suitable test collections.
The recently avail-able MALACH collection of oral history data will,in large part, make this dissertation research possible(Oard et al, 2004).
The MALACH test collection33contains about 1,000 hours of conversational speechfrom 400 interviews with survivors of the Holo-caust1.
The interviews are segmented into 8,104documents with topic labels manually assigned froma thesaurus of roughly 40,000 descriptors.
Thecollection includes relevance assessments for morethan 100 topics and has been used for several yearsin CLEF?s cross-language speech retrieval (CLSR)track (Oard et al, 2006).Participants in the CLEF CLSR evaluations havealready begun investigating evidence combinationfor SDR, through the use of automatic topic labels?although label texts are presently only used as an ad-ditional field for indexing.
In monolingual Englishtrials, this topic classification represents a significanteffort both in time and money (i.e., to produce train-ing data), so that these evidence combination studieshave so far been rather domain dependent.
Partici-pants have also been using what are probably un-naturally good ASR transcripts.
The speech is emo-tional, disfluent, heavily accented, and focused on asomewhat rare topic, such that the ASR system re-quired extensive tuning and adaptation to producethe current word error rate of approximately 25%.In this setting, we?d expect STD output and topic la-bels to have low and high utility, respectively.
Toinvestigate the domain mismatch case, I will applyan off-the-shelf ASR system to produce new, com-paratively poor, transcripts of the collection.
In thissetting, we?d expect STD output and topic labels toinstead have high and low utility, respectively.2 Proposed Combination SolutionsI will investigate improving SDR performance inboth the poorly and well matched domain conditionsthrough: (1) multiple approaches for utilizing auto-matically produced topic labels and (2) the utiliza-tion of STD output.Throughout this paper, completed work will bedenoted with a ??
?, while proposed (non-complete,future) work will be denoted with a ??
?.1This is only a small subset of the entire MALACH col-lection, which contains roughly 116,000 hours of speech from52,000 interviews in 32 languages.
This additional data alsoprovides training examples for classification.2.1 Speech Classification for SDRI outline three methods of incorporating evidencefrom automatic classification for speech retrieval.Creating Additional Indexable Text?The simplest way to combine classification andspeech retrieval is to use the topic labels associ-ated with the classes as indexable text.
As a par-ticipant on the MALACH project, I produced theseautomatic topic labels (?keywords?)
for the collec-tion?s speech segments.
These keywords were usedin this way in both years of the CLEF CLSR track.For a top system in the track, using solely automat-ically produced data (e.g., ASR transcripts and key-word text), indexing keyword text gave a relativeimprovement in mean average precision of 40.6%over an identical run without keywords (Alzghooland Inkpen, 2007).Runtime Query Classification for SDR?Simply using keyword text as an indexing fieldis probably suboptimal because information seek-ers don?t necessarily speak the same language asthe thesaurus constructors.
An alternative is to clas-sify the queries themselves at search time and to usethese label assignments to rank the documents.
Wemight expect this to be superior, insofar as infor-mation seekers use language more like interviewees(from which classification features are drawn) thanlike thesaurus builders.Class Guided Document Expansion?A third option for using classification output isas seed text for document expansion.
The intuitionhere is that ASR text may be a strong predictor fora particular class label even if the ASR contains fewterms which a user might consider for a query.
Inthis sense, the class label text may represent a moresemantically dense representation of the segment?stopical content.
This denser representation may thenbe a superior starting source for document centeredterm expansion.2.2 Unconstrained Term Detection for SDR?It is not yet clear how best to combine a STD andtopical relevance IR system.
One difficulty is thatIR systems count words (or putative occurrences ofwords from an ASR system), while STD systems34report a score proportional to the confidence that aword occurs in the audio.
As a solution, I proposenormalizing the STD system?s score for OOV queryterms by a function of the STD system?s score onputative occurrences of in-vocabulary terms.
Theintuition here is that the ASR transcript is roughlya ground truth representation of in-vocabulary termoccurrences and the score on OOV query termsought to reflect the STD system?s confidence in pre-diction (which can be modeled from the STD sys-tem?s score on ?ground truth?
in-vocabulary termoccurrences).
In this way, the presence or absence ofin-vocabulary terms and their associated STD confi-dence scores can be used to learn a normalizer forthe STD system?s scores.3 Producing the EvidenceIn this section, I highlight both completed and pro-posed work to improve the production of evidencefor combination.3.1 Classifying with Temporal Evidence?In spoken document collections, features beyondmerely the automatically transcribed words may ex-ist.
Consider, for example, the oral history data con-tained in the MALACH collection.
Each interviewin this collection can be thought of as a time orderedset of spoken documents, produced by the guidedinterview process.
These documents naturally arisein this context, and this temporal information can beused to improve classification accuracy.This work has so far focused on MALACH data,although we expect the methods to be generally ap-plicable to speech collections.
For example, the top-ical content of a television episode may often bea good predictor of the subsequent episode?s topic.Likewise, topics in radio, television, and podcastsmay tend to be seasonally dependent (based on Hol-idays, recurring political or sporting events, etc.
).Time-shifted classification?
One source of tem-poral information in the MALACH data is the fea-tures associated with temporally adjacent segments.Terms may be class-predictive for not only theirown segment, but for the subsequent segments aswell.
This intuition may be easily captured by a timeshifted classification (TSC) scheme.
In TSC, eachtraining segment is labeled with the subsequent seg-ment?s labels.
During classification, each test seg-ment is used to assign labels to its subsequent seg-ment.Temporal label weighting?
We can also benefitfrom non-local temporal information about a seg-ment.
For example, because interviewees were in-structed to relate their story in chronological order,we are more likely to find a discussion of childhoodat an interview?s beginning than at its end.
We canestimate the joint probability of labels and segmenttimes on held-out data and use this to bias new labelassignments.
We call this approach temporal labelweighting (TLW).In Olsson and Oard (2007), we showed that acombined TSC and TLW approach on MALACHdata yields significant improvements on two sep-arate label assignment tasks: conceptual and geo-graphic thesaurus terms, with relative improvementsin mean average precision of 8.0% and 14.2% re-spectively.3.2 Classifying across languages?In multilingual collections, training data for meta-data creation may not be available for a particularlanguage?a good example of domain mismatch.
Ifhowever, training examples are available in a sec-ond language, the metadata may still be producedthrough cross-language text classification.
In Ols-son (2005), we used a probabilistic Czech-Englishdictionary to transform Czech document vectors intoan English vector space before classifying them withk-Nearest Neighbors and English training exam-ples.
In this study, the cross-language performanceachieved 73% of the monolingual English baselineon conceptual topic assignment.3.3 Vocabulary Independent Spoken UtteranceRetrieval?In Olsson (2007), we examined a low resource ap-proach to utterance retrieval using the expected pos-terior count of n-grams in phonetic lattices as index-ing units.
A query?s phone subsequences are thenextracted and matched against the index to producea ranking on the lattices.
Against a 1-best phonesequence baseline, the approach was shown to sig-nificantly improve the mean average precision of re-trieved utterances on five human languages.353.4 Improving Spoken Term Detection?Phonetic lattices improve spoken term detection per-formance by more accurately encoding the recog-nizer?s uncertainty in prediction.
Even so, a cor-rect lattice may not always contain a path withthe query?s entire phone sequence.
This is so notonly because of practical constraints on the size(i.e., depth) of the lattice, but also because speak-ers don?t always pronounce words with dictionaryprecision.
We?d like to allow approximate matchingof a query?s phone sequence with the phonetic lat-tices, and to do this as quickly as possible.
This timerequirement will prevent us from linearly scanningthrough lattices for near matches.
I am currently in-vestigating two solutions to this problem: phoneticquery degradation and query expansion.Phonetic query degradation?
The idea in pho-netic query degradation is to build an error model forthe phone recognition system and to then degradethe query phone sequence such that it, hopefully,will more closely resemble recognized sequences.This approach incurs only a very slight cost in timeand is query independent (in the sense that any termcan be pushed through the degradation model?not,for example, only terms for which we can find rec-ognized examples).Phonetic query expansion?
The idea of phoneticquery expansion is, again, to transform the cleanphone sequence of the query into the degraded formhypothesized by a recognizer.
Instead of using adegradation model however, we simply run a firstpass at STD with the non-degraded query term anduse the putative occurrences to learn new, alterna-tive, degraded forms for a second search pass.
Thiscan be thought of as blind relevance feedback orquery by (putative) example.The advantage of this approach is that we arenot required to explicitly model the degradation pro-cess.
Disadvantages are that we (1) require exam-ples which may not be available and (2) assume thatthe degradation process is well represented by onlya few examples.4 ContributionsThis dissertation will significantly contribute tospeech retrieval research in several ways.Can we improve SDR by evidence combination?By exploring evidence combination, this dissertationwill advance the state of the art in speech retrievalsystems and their applicability to diverse domains.
Iwill investigate multiple methods for combining theevidence presented by both STD and classificationsystems with conventional ASR output (transcriptsor word lattices).
This work will develop upon pre-vious research which studied, in depth, the use ofonly one evidence source, e.g., (Ng, 2000).Can evidence combination decrease domain de-pendency?
I will investigate how combining evi-dence sources can increase their applicability to newcontent domains.
This will include, for example, un-derstanding how (vocabulary independent) STD sys-tems can be paired with fixed vocabulary ASR.How can these evidence sources be improved?Lastly, I will explore how these new evidencesources may themselves be improved.
This will in-clude utilizing temporal domain knowledge for clas-sification and improving the robustness of phone-based STD systems.ReferencesM.
Alzghool and D. Inkpen.
Experiments for the CrossLanguage Spoken Retrieval Task at CLEF 2006.
InNot yet published.K.
Ng.
2000.
Subword-based approaches for spokendocument retrieval.
MIT dissertation.D.W.
Oard, et al Building an Information Retrieval TestCollection for Spontaneous Conversational Speech.
InProceedings of SIGIR?04.D.W.
Oard, et al 2006.
Evaluation of Multilingual andMulti-modal Information Retrieval.
In Seventh Work-shop of the Cross-Language Evaluation Forum, Ali-cante, Spain.
Selected Papers Series: Lecture Notes inComputer Science.J.S.
Olsson and D.W. Oard.
2007.
Improving Text Clas-sification for Oral History Archives with Temporal Do-main Knowledge.
In Not yet published.J.S.
Olsson, et al Cross-language text classification.
InProceedings of SIGIR?05.J.S.
Olsson, et al Fast Unconstrained Audio Search inNumerous Human Languages.
In ICASSP?07.36
