Maximum Entropy Modeling in Sparse Semantic Tagging ?Jia CuiCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD, 21210, USAcuijia@jhu.eduDavid GuthrieDepartment of Computer ScienceUniversity of SheffieldSheffield, S1 4DP, UKD.Guthrie@dcs.shef.ac.ukAbstractIn this work, we are concerned with a coarsegrained semantic analysis over sparse data, whichlabels all nouns with a set of semantic categories.To get the benefit of unlabeled data, we proposea bootstrapping framework with Maximum En-tropy modeling (MaxEnt) as the statistical learn-ing component.
During the iterative tagging pro-cess, unlabeled data is used not only for betterstatistical estimation, but also as a medium to in-tegrate non-statistical knowledge into the modeltraining.
Two main issues are discussed in thispaper.
First, Association Rule principles are sug-gested to guide MaxEnt feature selections.
Sec-ond, to guarantee the convergence of the boot-strapping process, three adjusting strategies areproposed to soft tag unlabeled data.1 IntroductionSemantic analysis is an open research field in natural lan-guage processing.
Two major research topics in this field areNamed Entity Recognition (NER) (N. Wacholder and Choi,1997; Cucerzan and Yarowsky, 1999) and Word Sense Dis-ambiguation (WSD) (Yarowsky, 1995; Wilks and Steven-son, 1999).
NER identifies different kinds of names such as?person?,?location?
or ?date?, while WSD distinguishes thesenses of ambiguous words.
For example, ?bank?
can be la-beled as ?financial institution?
or ?edge of a river?.
Our taskof semantic analysis has a more general purpose, taggingall nouns with one semantic label set.
Compared with NE,which only considers names, our task concerns all nouns.Unlike WSD, in which every ambiguous word has its ownrange of sense set, our task aims at another set of semanticlabels, shared by all nouns.
The motivation behind this workis that a semantic category assignment with reliable perfor-mance can contribute to a number of applications includingstatistical machine translation and sub-tasks of informationextraction.
?This work was supported in part by NSF grant numbers IIS-0121285.
Any opinions, fndings and conclusions or recommenda-tions expressed in this material are the authors?
and do not neces-sarily reflect those of the sponsor.The semantic categories adopted in this paper come fromthe Longman Dictionary.
These categories are neither par-allel to nor independent of each other.
One category maydenote a concept which is a subset of that of another.
Exam-ples of the category structures are illustrated in Figure 1.Figure 1: Structure of some semantic categories used in theLangman dictionary.Maximum Entropy (MaxEnt) principle has been success-fully applied in many classification and tagging tasks (Rat-naparkhi, 1996; K. Nigam and A.McCallum, 1999; A. Mc-Callum and Pereira, 2000).
We use MaxEnt modeling as thelearning component.
A major issue in MaxEnt training ishow to select proper features and determine the feature tar-gets (Berger et al, 1996; Jebara and Jaakkola, 2000).
Todiscover useful features, we exploit the concept of Associ-ation Rules (AR) (R. Agrawal and Swami, 1993; Srikantand Agrawal, 1997), which is originally proposed in DataMining research field to identify frequent itemsets in a largedatabase.Like many other classification tasks, human-annotateddata for semantic analysis is expensive and limited, whilea large amount of unlabeled data is easily obtained.
Manyresearchers (Blum and Mitchell, 1998; K. Nigam andMitchell, 2000; Corduneanu and Jaakkola, 2002) have at-tempted to improve performance with unlabeled data.
Inthis paper, we also propose a framework to bootstrap withunlabeled data.
Fractional counts are assigned to unlabeledinstances based on current model and accessible knowledgesources.
Pooled with human-annotated data, unlabeled datacontributes to the next MaxEnt model.We begin with an introduction of our bootstrappingframework and MaxEnt training.
An interesting MaxEntpuzzle is presented, with its derivation showing possible di-rections of utilizing unlabeled data.
Then the feature selec-tion criterion guided by AR is discussed as well as the indi-cator selection (section 3).
We discuss initialization meth-ods for the unlabeled data.
Strategies to guarantee conver-gence of bootstrapping process and approaches of integrat-ing non-statistical knowledge are proposed in section 4.
Fi-nally, experimental results are presented along with conclu-sions and future work.2 Bootstrapping and the MaxEnt puzzleAn instance in the corpus includes the headword, which isthe noun to be labeled, and its context.
To integrating the un-labeled instances in the training process, we propose a tag-ging method called soft tagging.
Unlike the normal tagging,in which each instance is assigned only one label, soft tag-ging allows one instance to be assigned several labels witheach of them being associated with a fractional credit.
Allcredits assigned to one instance should sum up to 1.
Forexample, a raw instance with the headword ?club?
can besoft tagged as (movable J:0.3, not-movable N:0.3, collectiveU:0.4).
Once all auxiliary instances have been assigned se-mantic labels, we pool them together with human-annotateddata to select useful features and set up feature expectations.Then a log-linear model is trained for several iterations tomaximize likelihood of the whole corpora.
With the updatedMaxEnt model, unlabeled data will be soft tagged again.The whole process is repeated until convergence conditionis satisfied.
This framework is illustrated in Figure 2.In building a mexent model, unlabeled data contributesdifferently to the feature selection and target estimationcompared to the human-annotated data.
While human-annotated instances never change, tags in unlabeled datakeep updating according to the new MaxEnt model in eachbootstrapping iteration, which might lead to a different fea-ture set.Figure 2: The bootstrapping frameworkBefore presenting the MaxEnt puzzle, let?s first considera regular MaxEnt formulation.
Let l be a label and x bean instance.
Let kh(l, x) be the h-th binary feature func-tion which is equal to 1 if (l, x) activates the h-th featureand 0 otherwise.
P (l, x) is given by the model, denoting theprobability of observing both l and x. P?
(l, x) is the empir-ical frequency of (l, x) in the training data.
The constraintassociated with feature kh(l, x) is represented as:?l,xP (l,x) kh(l,x) =?l,xP?
(l,x) kh(l,x) (1)In practice, we are interested in conditional modelsP (l|x), which assign probability mass to a label set givensupporting evidence x.
And we approximate P (x) withP?
(x) .
Consequently,?xP?
(x)?lP (l|x) kh(l,x) =?l,xP?
(l,x) kh(l,x) (2)Next, we show how the effect of unlabeled data disap-pears during the bootstrapping in a restricted situation.
Wemake the following assumptions:1.
The feature targets are the raw frequencies in the train-ing data.
No smoothing is applied.2.
During the bootstrapping, we maintain a fixed featureset, while the feature targets might change.3.
The fractional credit assigned to label l and instance xin the t+ 1 iteration is P t(l|x).Let N be the total number of instance(1, ?
?
?
, N ) in la-beled data and M be the total number of instance(N +1, ?
?
?
, N + M ) in unlabeled data.
Let ?
be the weight as-signed to unlabeled instances.
The new constraint for theh-th feature function can be rewritten as:1N + ?MN?i=1?lP t+1(l|xi) kh(l,xi) (3)+ ?N + ?MN+M?i=N+1?lP t+1(l|xi) kh(l,xi)= 1N + ?MN?i=1kh(li,xi)+ ?N + ?MN+M?i=N+1?lP t(l|xi) kh(l,xi)where t is the index of the bootstrap iterations.
li is thehuman-annotated label attached to the ith instance.When the procedure converges, P t(l|xi) = P t+1(l|xi),the second parts on both sides will cancel each other.
Fi-nally, the constraint will turn out to be:N?i=1?lP t+1(l|xi) kh(l,xi) =N?i=1kh(li,xi) (4)which includes statistics only from labeled data.
If the fea-ture set stays the same, unlabeled data will make no con-tribution to the whole constraint set.
A model which cansatisfy Equation 3 must be able to satisfy Equation 4.
If un-labeled instances satisfy the same distribution as labeled in-stances, given the same set of constraints, the model trainedon only labeled data would be equivalent to the one trainedon both.The above derivation shows that with the three restric-tions, the soft tagged instances make no contribution to theconstraint set, which is counter-intuitive.
That is why wecall it a ?MaxEnt Puzzle?.
Consequently, to utilize unla-beled data, we should break the three restrictions: to re-select feature set after each bootstrapping iteration, or adjustthe soft tagging results given by the model.3 Feature selectionBerger et al(1996) proposed an iterative procedure ofadding news features to feature set driven by data.
Wepresent a simple and effective approach using some statis-tical heuristics for feature selection.There are two kinds of features in MaxEnt modeling:marginal features and conditional features.
Marginal fea-tures represent the priors of the semantic categories.
Condi-tional features are composed of an indicator and a label.
Anindicator can be a single word, an array of words or a wordcluster.
Similarly, a label can be either a single semanticcategory or a set of categories.One major motivation of combining unlabeled data withlabeled data in the training process is that unlabeled datacan provide a large pool of valuable feature candidates aswell as more statistical information.
An (indicator, label)pair may appear only once in labeled data, but it may oc-cur very frequently in soft tagged data, it might be selectedas a feature during the bootstrapping.
Similarly, a featureselected only from human-annotated data may become rel-atively infrequent when the soft tagged instances are added,thus being eliminated from the feature set.3.1 IndicatorsA good indicator should have an obvious preference for def-inite semantic categories.
For instance, ?good?,?great?
areweak indicators because they can modify almost all seman-tic categories; ?drinkable?
and ?light-weighted?
are usuallyassociated with ?liquid (L)?
and ?movable (J)?
respectively,thus are strong indicators.
The quality of an indicator can bemeasured by the entropy of the semantic categories it mod-ifies.
The lower the entropy is, the stronger preference theindicator has.To overcome the data sparsity problem, several indica-tors can be clustered together to form a new single indica-tor.
There are two possible ways of clustering.
One is hardclustering, which divides all words into non-overlappingclasses; The other is soft clustering, which permits one wordto belong to different classes.
Usually, words with similarsemantic preferences are clustered together.3.2 Association rules in feature selectionAn indicator can be combined with different labels to formdifferent features.
But not all pairs including good indica-tors are good features.
In a statistical learning algorithm,a good feature should first be statistically reliable, which isconsidered in most of the MaxEnt modeling implementa-tions.
However, with this single requirement, it is possiblethat (x, l) is a feature if both x and l are frequent, but thechance of seeing label l is tiny given the presence of indica-tor x.
More constraints are necessary to pinpoint indicativefeatures.
We exploit Association Rule (AR) principles to putmore restrictions in feature selecting.Let x be an indicator and l be a label.
count(x, l) de-notes the number of instances which include indicator x andare attached with label l in the training corpus.
N is the to-tal number of instances.
We define three measurements tocharacterize the goodness of a given (x, l) pair from threedifferent points.
We put thresholds on each of them.support(x, l) denotes how frequently indicator x and la-bel l occur together.support(x, l) = count(x, l)N (5)confidence(x, l) denotes how likely it is to observe labell when indicator x is at present.confidence(x, l) = count(x, l)count(x) (6)improvement(x, l) shows how much more likely to seelabel l when indicator x is observed relative to the overallprobability of seeing l.improvement(x, l) = count(x, l)/count(x)count(l)/N (7)The concept of improvement can be extended to eachpart of the indicators.
Let S be the feature set.improvement(x, l) = minx?
?x,(x?,l)?Sconfidenc(x, l)confidenc(x?, l)(8)For instance, assume the improvement threshold is 1.Let x=x1x2, if confidence(x1, l) > confidence(x1x2, l),then (x1x2, l) will be ignored and only (x1, l) is selectedas a feature; otherwise, only (x1x2, l) remains and (x1, l)is ignored.
This idea can be applied to remove embeddedfeatures, reducing the redundancy in the feature set.The assumption behind those criteria is that most featuresshould be positive, i.e., given feature (x,l), the existence ofindicator x always increase the odds of seeing label l, otherthan decrease it.4 Soft taggingInitialization of unlabeled instances is important for ourbootstrapping framework, because its effect will be carriedon from one iteration to another.
Even if the auxiliary data(unlabeled data included in training) has no effect on theconstraint set under some circumstances, the soft taggedauxiliary data is still a part of the training corpus, the like-lihood of which is to be maximized in the MaxEnt training.On one hand, we wish the initialization to be as close aspossible to the real value, of which we have no idea exceptfor the knowledge from labeled data; on the other hand, wewish the tagging of the auxiliary data could be slightly dif-ferent from the model trained only on labeled data, to avoidconverging to this model itself.As we have seen in section 2, using the model generatedin the previous iteration directly to soft tag the auxiliary datais one of the causes diluting the effect of the auxiliary data.Therefore, we propose to adjust the soft tagging results bynon-statistical knowledge or through fixing some of the tags.4.1 Initialization of unlabeled dataInspired by the MaxEnt puzzle, we realize the importanceof additional knowledge sources.
To assign inital fractionalcredits to unlabeled instances, we look up a dictionary tonarrow down choices.
In particular,1.
If the headword of the instance has a unique semanticlabel choice according to the dictionary, it gets credit 1for this label and 0 for others.2.
If the headword has been observed in labeled data andit has more than one possible labels in the dictionary, itis initialized as follows:Let w be the headword and l be a label.
m is the to-tal number of permitted labels for w according to thedictionary.
Let L(w) be the set of (w, l) pairs in la-beled data and D(w) be the set of (w, l) permitted inthe dictionary.If D(w) = L(w), thencredit(w, l) ={count(w,l)count(w) if (w, l) ?
L(w)0 otherwiseOtherwise, we add one and renormalize:credit(w, l) ={count(w,l)+1count(w)+m if (w, l) ?
D(w)0 otherwise3.
If the headword never occurs in human-annotated data,we initialize the credits with the flat distribution.credit(w, l) ={ 1m if (w, l) ?
D(w)0 otherwise4.2 Adjust credits according to their distributionsConsidering the bootstrapping process as a tug-of-war be-tween labeled and unlabed data, with the flag denoting themodel.
In each bootstrapping iteration, tags of the auxil-iary data are updated in accordance with the model.
Unla-beled data is moving towards the labeled data while the latterstands still.
One way to prevent the auxiliary data from be-ing dragged completely to the point of the labeled data is tonail down part of the auxiliary data tags.
For example, sup-pose an instance gets credits (0.8,0.15,0.05).
Since the firstcategory is much more likely than the others, we can fix thecredits as (1,0,0) for ever.
Another choice would be to re-move the least probable category, i.e., changing the creditsto (0.84,0.16,0).
An even more radical method is to removethe whole instance from the auxiliary data if it has no strongsemantic preferences.Here are the details of these adjusting strategies:rmlow set the least probable category with credit zero andrenormalize the remaining credits.
Whether to applythis action or not can depend on thresholding on themininal credit, the ratio of the minimal credit to themaximal credit, or the entropy of the credit distribution.kphigh assign all credit 1 to the most probable category.Possible thresholds can be set for the maximal credit,the ratio of the maximal credit to the second maximalcredit, or the entropy of the credit distribution.rmevent remove the instance if it has a flat credit distri-bution after several iterations.
Possible thresholds canbe set for the maximal credit, the ratio of the maximalcredit to the minimal credit, or the entropy of the creditdistribution.The effect of those actions is permanent.
That is, if a cat-egory is forbidden (set to zero) for one instance, it will beforbidden for this instance in the following iterations.
Sim-ilarly, if an instance is removed, it will never be used in thetraining process again.4.3 Adjust credits with non-statistical knowledgeNon-statistical knowledge is the knowlege which is not ob-tainable from statistics in a sparse data set, but rather fromother resources like a dictionary or WordNet.
This kind ofknowledge may not be expressed in labeled data, thereforecould not be used to form features.
For instance, we can ob-tain easily from a dictionary that word ?long-tailed?
is usedto modify animals.
But if ?long-tailed?
is rarely observed inthe training data, it will not not selected as a feature, thusbeing ignored by the model.To take advantage of non-statistical knowledge, we use itto adjust soft tagging results.
One possibility is pruning theillegal categories with a dictionary.
By doing so, this kindof knowledge can affect the MaxEnt modeling indirectly.Similarly, the preference of a context word could also beused to adjust the credit distributions.5 Experiments5.1 CorpusThe corpus used in our experiments is provided by SheffieldUniversity.
The human-annotated data is divided into train-ing (SHD) and test (Blind).
SHD contains 197K instancesand Blind contains 13K instances.The Longman dictionary provides 36 semantic cate-gories, among which only 21 most frequently used cate-gories with the exception of ?unknown (Z)?
are used in ourexperiments.
The distribution of the semantic categories isfar from uniform.
Semantic category ?abstract (T)?
aloneforms 60% (126K) of the human-annotated instances.
Thehistogram of the other categories is shown in Figure 3.The inter-annotator agreement for the labeled data is 95%with exact match.
Even though some semantic categoriesform a hierarchical structure, we always assume that humanannotators chose the most specific categories.
Evaluation ofthe classification error rate (CER) in the following experi-ments also uses exact match only.5.2 Feature selection with Association Rule principlesThe first group of experiments are designed to test differentindicators as well as the feature selection strategies, withoutusing unlabeled instances or bootstrapping.
The indicatorsand feature selection thresholds which result in the best per-formance are used in the bootstrapping experiments.Figure 3: The number of instances labeled with semanticcategories other than TSince 96% of the headwords in Blind are also in SHD,headwords are the most effective indicators.
Using head-words alone can get 9.47% classification error rate.
Otherindicators investigated include adjectives, which modifyheadwords, and their clusters.
As we mentioned before,clustering can alleviate data sparseness problem.
So wegroup headwords and adjectives.
Headword clusters are softclusters.
The ambiguous headwords can belong to severalclusters as long as the corresponding categories of thoseclusters are permitted for these headwords in the dictionary.Adjectives are hard clustered.
We discard general commonadjectives according to their entropies.
We keep only thosewith strong semantic preferences and assign them to theclusters corresponding to their best preferences.We also include compound features such as headword-adjective, headword-adjectivecluster and other combina-tions.Table 1 shows the experimental results with all typesof indicators which are mentioned above, and comparedthe results with the baseline.
For different types of fea-tures, we set up different thresholds.
In config-1, only thesupport thresholds are used.
From config-2 to config-5,we raise confidence and improvement thresholds grad-ually to lower the number of features.
The CER resultsshow that with proper thresholds, using confidence andimprovement not only helps decreasing the feature set sizeby as much as 26%, but also improves the performance.The last column of the table 1 shows the p-value of sig-nificnce test between each configuration and its precedence.Clearly, config 1,2,and 3 perform similar to each other,though they are significant better than using headwords only.And config 4 and 5 are significant better than config 3.5.3 BootstrappingIn this group of experiments, we use Blind as the auxiliarydata in the bootstrapping process.
Blind data is initializedwith the methods mentioned in section 4.
No human labelsin Blind are included in the training process.
The weightfor the auxiliary data ?
is set to 1 in our experiments.
TheLongman dictionary is always used in soft tagging to elimi-nate the illegal categories.The bootstrapping process will stop when the constraintset does not change.
Since the feature set is re-selected inconfiguration # of features CER(%) p-valueheadwords only 12514 9.47 -config-1 49861 8.18 < 1.0e?12config-2 44141 8.18 0.53config-3 39136 7.94 0.034config-4 36631 7.62 < 1.0e?3config-5 32214 7.64 0.65Table 1: Classification Error Rates (CER) with different fea-ture setseach iteration, the convergence of the iterative steps is diffi-cult to achieve.
Fluctuations might be observed.
With strate-gies proposed in 4.2, more and more auxiliary instances canbe fixed with one semantic category.
The whole process willstop at some point.Figure 4 plots the CER in the first 20 iterations for dif-ferent setups.
It shows adding Blind into the bootstrappingprocess can lower the error rate to 7.37%.
Without any softtagging adjustment (normal), the training process does notstop and the CER drops at first but then climbs up later.Using entropy as thresholds is an efficient way to en-sure the convergence for both rmlow and kphigh.
Actionrmevent can also keep the results from getting much worseduring bootstrapping.Figure 4: Error rate in each iteration for different soft tag-ging strategies6 ConclusionsWe introduce a general semantic analysis task, which labelsall nouns with a unique set of semantic categories.
In orderto get benefitted from unlabeled data, we propose a boot-strapping framework with MaxEnt modeling as the mainlearning component.
Unlabeled data, after soft tagging, canbe combined with labeled data to provide evidence for Max-Ent feature selection and target estimation.
Moreover, non-statistical knowledge can affect the modeling indirectly byadjusting the soft tagging results of the auxiliary data.For MaxEnt training, we suggest using Association Ruleprinciples to control the feature selection.
The intuition be-hind these criteria is that the very frequent, strongly prefered(indicator,label) pairs are good enough for model training.Using AR principles not only decreases the size of the fea-ture set dramatically, but also improves the performances.But there is no good way to set the AR thresholds properlyyet.With the feature set changing in each iteration, the con-vergence of the bootstrapping is hard to guarantee.
Bysharpening the soft tagging results through removing theleast probable label or keeping only the most probable la-bel, we can speed up convergence.7 Future workAt present, only headwords and adjectives are used to com-pose indicators.
We plan to incorporate linguistically en-riched features.
Using parsing, we can locate verbs and usetheir relationships with headwords as new indicators.
An-other type of potential indicators are the subject codes of theheadwords.
Subject codes represent finer grained semanticclassifications.
Some of the instances in human-annotateddata have been marked with subject codes.
There are a to-tal of 320 subject codes, such as ?architecture?,?agriculture?and ?business?.
We believe that knowing the subject codesof the headwords will help to decrease the entropy of theheadword senses.We have a very large data set BNC corpus, which comesfrom the same source as the labeled data we use.
BNC datais composed of 1.2M unambiguous instances, 1.4M seeninstances (ambiguous instances with headwords being ob-served in SHD) and 0.4M unseen instances (ambiguous in-stances with headwords never being observed in SHD).
Inthe future, we will try bootstrapping with BNC data.Alternative indicator clustering techniques will be ex-plored too.
We have used entropy as a heuristic in the ex-periments; an alternative heuristic that can be employed ismutual information.We have provided a framework to utilize non-statisticalknowledge.
In addition to using a dictionary to limit thechoices of unlabeled data, we can obtain plenty of informa-tion about word sense preferences from the WordNet for softtagging adjustment.The bootstrapping process is closely related toExpectation-Maximization procedure, in which softtagging can be ragarded as the E-step.
In many practicalEM implementations, however, updating at the E-step doesnot use the exact theoretical value.
The modification takenin the E-step can be a linear combination of the old valueand the new calculated one.
Similar re-estimation strategiescan be applied in our work.
A theoretical descriptionof the relationship between EM and soft tagging wouldpotentially be able to identify convergency properties of thebootstrapping framework.AcknowledgmentsThe authors would like to thank Prof. Frederick Jelinek andDr.
Louise Guthrie and all other members of the Seman-tic group in 2003 JHU Summer Research Workshop.
Theyare also grateful to the anonymous reviewers for their veryinsightful comments and suggestions.ReferencesD.
Freitag A. McCallum and F. Pereira.
2000.
Maximumentropy markov models for information extraction andsegmentation.
In Proc.
17th International Conf.
on Ma-chine Learning, pages 591?598.A.
L. Berger, S. D. Pietra, and V. J. D. Pietra.
1996.
A max-imum entropy approach to natural language processing.Computational Linguistics, 22(1):39?71.A.
Blum and T. Mitchell.
1998.
Combining labeled andunlabeled data with co-training.
In COLT: Proceedingsof the Workshop on Computational Learning Theory.A.
Corduneanu and T. Jaakkola.
2002.
Continuation meth-ods for mixing heterogeneous sources.
In Proceedingsof the Eighteenth Annual Conference on Uncertainty inArtificial Intelligence.S.
Cucerzan and D. Yarowsky.
1999.
Language indepen-dent named entity recognition combining morphologicaland contextual evidence.
In joint SIGDAT Conferenceon Empirical Methods in NLP and Very Large Corpora,pages 90?99.T.
Jebara and T. Jaakkola.
2000.
Feature selection and dual-ities in maximum entropy discrimination.
In UncertainityIn Artificial Intellegence.J.Lafferty K. Nigam and A.McCallum.
1999.
Using maxi-mum entropy for text classification.
In IJCAI-99 Work-shop on Machine Learning for Information Filtering,pages 61?67.S.
Thrun K. Nigam, A. K. McCallum and T. M. Mitchell.2000.
Text classification from labeled and unlabeled doc-uments using em.
Machine Learning, 39(2-3):103?134.Y.
Ravin N. Wacholder and M. Choi.
1997.
Disambiguationof proper names in text.
In Proceedings of Fifth Con-ference on Applied Natural Language Processing, pages202?208.T.
Imielinski R. Agrawal and A. N. Swami.
1993.
Miningassociation rules between sets of items in large databases.In Peter Buneman and Sushil Jajodia, editors, Proceed-ings of the 1993 ACM SIGMOD International Conferenceon Management of Data, pages 207?216, Washington,D.C., 26?28 .A.
Ratnaparkhi.
1996.
A maximum entropy model forpart of speech tagging.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Processing,pages 133?142.R.
Srikant and R. Agrawal.
1997.
Mining generalized as-sociation rules.
Future Generation Computer Systems,13(2?3):161?180.Y.
Wilks and M. Stevenson.
1999.
The grammar of sense:Using part-of-speech tags as a first step in semantic dis-ambiguation.
Journal of Natural Language Engineering,4(3).D.
Yarowsky.
1995.
Unsupervised word sense disambigua-tion rivaling supervised methods.
In Meeting of the Asso-ciation for Computational Linguistics, pages 189?196.
