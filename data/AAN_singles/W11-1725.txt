Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, ACL-HLT 2011, pages 189?195,24 June, 2011, Portland, Oregon, USA c?2011 Association for Computational LinguisticsSentimatrix ?
Multilingual Sentiment Analysis ServiceAlexandru-Lucian G?nsc?1, Emanuela Boro?1, Adrian Iftene1, Diana Trandab?
?1,Mihai Toader2, Marius Cor?ci2, Cenel-Augusto Perez1, Dan Cristea1, 31?Al.
I. Cuza?
University, Faculty of Computer Science, Iasi, Romania2Intelligentics, Cluj-Napoca, Romania3Institute of Computer Science, Romanian Academy, Iasi, Romania{lucian.ginsca, emanuela.boros, adiftene, dtrandabat, augusto.perez,dcristea}@info.uaic.ro, {mtoader, marius}@intelligentics.roAbstractThis paper describes the preliminary resultsof a system for extracting sentimentsopinioned with regard with named entities.It also combines rule-based classification,statistics and machine learning in a newmethod.
The accuracy and speed ofextraction and classification are crucial.The service oriented architecture permitsthe end-user to work with a flexibleinterface in order to produce applicationsthat range from aggregating consumerfeedback on commercial products tomeasuring public opinion on politicalissues from blog and forums.
Theexperiment has two versions available fortesting, one with concrete extraction resultsand sentiment calculus and the other withinternal metrics validation results.1 MotivationNowadays, big companies and organizations spendtime and money in order to find users?
opinionsabout their products, the impact of their marketingdecisions, or the overall feeling about their supportand maintenance services.
This analysis helps inthe process of establishing new trends and policiesand determines in which areas investments must bemade.
One of the focuses of our work is helpingcompanies build such analysis in the context ofusers?
sentiment identification.
Therefore, thecorpus we work on consists of articles ofnewspapers, blogs, various entries of forums, andposts in social networks.Sentiment analysis, i.e.
the analysis andclassification of the opinion expressed by a text onits subject matter, is a form of informationextraction from text, which recently focused a lotof research and growing commercial interest.This paper describes Sentimatrix, a sentimentanalysis service, doing sentiment extraction andassociating these analyses with named entities, indifferent languages.
We seek to explore howsentiment analysis methods perform acrosslanguages, especially Romanian.
The mainapplications that this system experiments with aremonitoring the Internet before, during and after acampaign/message release and obtaining consumerfeedback on different topics/products.In Section 2 we briefly discuss a state of the artin sentiment analysis, the system?s architecture isdescribed in Section 3 and in Section 4 we focuson identifying opinions on Romanian.Subsequently, we present the experiment results,analysis and discussion in Sections 5 and 6.
Futurework and conclusions are briefly described inSection 7.2 Sentimatrix compared with state-of-the-artA comprehensive state of the art in the field ofsentiment analysis, together with potentialapplications of such opinion identification tools, ispresented in (Pang and Lee, 2008).Starting from the early 1990s, the research onsentiment-analysis and point of views generallyassumed the existence of sub-systems for rathersophisticated NLP tasks, ranging from parsing tothe resolution of pragmatic ambiguities (Hearst,1992; Wiebe 1990 and 1994).
In Sentimatrix, inorder to identify the sentiment a user expressesabout a specific product or company, the companyname must be first identified in the text.
Named189entity recognition (NER) systems typically uselinguistic grammar-based techniques or statisticalmodels (an overview is presented in (Nadeau andSatoshi Sekine.
2007)).
Hand-crafted grammar-based systems typically obtain better precision, butat the cost of lower recall and months of work byexperienced computational linguists.
Besides, thetask is hard to adapt to new domains.
Varioussentiment types and levels have been considered,starting from the ?universal?
six level of emotionsconsidered in (Ovesdotter Alm, 2005; Liu et al,2003; Subasic and Huettner, 2001): anger, disgust,fear, happiness, sadness, and surprise.
ForSentimatrix, we adapted this approach to fivelevels of sentiments: strong positive, positive,neutral, negative and strong negative.The first known systems relied on relativelyshallow analysis based on manually builtdiscriminative word lexicons (Tong 2001), used toclassify a text unit by trigger terms or phrasescontained in a lexicon.
The lack of sufficientamounts of sentiment annotated corpora led theresearchers to incorporate learning componentsinto their sentiment analysis tools, usuallysupervised classification modules, (e.g.,categorization according to affect), as initiated in(Wiebe and Bruce 1995).Much of the literature on sentiment analysis hasfocused on text written in English.
Sentimatrix isdesigned to be, as much as possible, languageindependent, the resources used being easilyadaptable for any language.Some of the most known tools availablenowadays for NER and Opinion Mining are:Clarabridge (www.clarabridge.com), RavenPack(ravenpack.com), Lexalytics (www.lexalytics.com)OpenAmplify (openamplify.com), Radian6(www.radian6.com), Limbix (lymbix.com), butcompanies like Google, Microsoft, Oracle, SAS,are also deeply involved in this task.3 System componentsIn Figure 1, the architecture and the main modulesof our system are presented: preprocessing, namedentity extraction and opinion identification(sentiment extraction per fragment).The final production system is based on serviceoriented architecture in order to allow usersflexible customization and to enable an easier wayfor marketing technology.
Each module of thesystem (Segmenter, Tokenizer, Language Detector,Entity Extractor, and Sentiment Extractor) can beexposed in a user-friendly interface.Figure 1.
System architecture3.1 PreprocessingThe preprocessing phase is made out of a textsegmentator and a tokenizer.
Given a text, wedivide it into paragraphs, every paragraph is splitinto sentences, and every phrase is tokenized.
Eachtoken is annotated with two pieces of information:its lemma (for Romanian it is obtained from ourresource with 76,760 word lemmas correspondingto 633,444 derived forms) and the normalized form(translated into the proper diacritics1).3.2 Language DetectionLanguage detection is a preprocessing stepproblem of classifying a sample of charactersbased on its features (language-specific models).Currently, the system supports English, Romanianand Romanian without Diacritics.
This step isneeded in order to correctly identify a sentiment ora sentiment modifier, as the named entity detectiondepends on this.
We combined three methods for1In Romanian online texts, two diacritics are commonly used,but only one is accepted by the official grammar.190identifying the language: N-grams detection,strictly 3-grams detection and lemma correction.The 3-grams classification method uses corpusfrom Apache Tika for several languages.
TheRomanian 3-gram profile for this method wasdeveloped from scratch, using our articles archive.The language detection in this case performssimple distance measurement between everylanguage profile that we have and the testdocument profile.
The N-grams classificationmethod implies, along with computing frequencies,a posterior Naive Bayes implementation.
The thirdmethod solves the problematic issue of shortphrases language detection and it implies lookingthrough the lemmas of several words to obtain thespecificity of the test document.3.3 Named Entity RecognitionThe Named Entity Recognition component forRomanian language is created using linguisticgrammar-based techniques and a set of resources.Our component is based on two modules, thenamed entity identification module and the namedentity classification module.
After the named entitycandidates are marked for each input text, eachcandidate is classified into one of the consideredcategories, such as Person, Organization, Place,Country, etc.Named Entity Extraction: After the pre-processing step, every token written with a capitalletter is considered to be a named entity candidate.For tokens with capital letters which are the firsttokens in phrases, we consider two situations:1. this first token of a phrase is in our stop wordlist (in this case we eliminate it from thenamed entities candidate list),2. the first token of a phrase is in our commonword list.
In the second situation there areconsidered two cases:a. this common word is followed by lowercasewords (then we check if the common wordcan be found in the list of trigger words, likeuniversity, city, doctor, etc.),b.
this common word is followed by uppercasewords (in this case the first word of thesentence is kept in the NEs candidate list,and in a further step it will be decided if itwill be combined with the following word inorder to create a composed named entity).Named Entities Classification: In theclassification process we use some of rules utilizedin the unification of NEs candidates along with theresource of NEs and several rules specificallytailored for classification.
Thus, after all NEs in theinput text are identified and, if possible, compoundNEs have been created, we apply the followingclassification rules: contextual rules (usingcontextual information, we are able to classifycandidate NEs in one of the categoriesOrganization, Company, Person, City and Countryby considering a mix between regular expressionsand trigger words) and resource-based rules (if notriggers were found to indicate what type of entitywe have, we start searching our databases for thecandidate entity).Evaluation: The system?s Upper Bound and itsperformance in real context are evaluated for eachof the two modules (identification andclassification) and for each named entity type.
Thefirst part of the evaluation shows an upper boundof 95.76% for F-measure at named entityextraction and 95.71% for named entityclassification.
In real context the evaluation showsa value of 90.72% for F-measure at named entityextraction and a value of 66.73% for named entityclassification.
The results are very promising, andthey are being comparable with the existingsystems for Romanian, and even better for Personrecognition.4 Identify users opinions on Romanian4.1 ResourcesIn such a task as sentiment identification, linguisticresources play a very important role.
The coreresource is a manually built list of words andgroups of words that semantically signal a positiveor a negative sentiment.
From now on, we willrefer to such a word or group of words as?sentiment trigger?.
Certain weights have beenassigned to these words after multiple revisions.The weights vary from -3, meaning strong negativeto +3, which translates to a strong positive.
Thereare a total of 3,741 sentiment triggers distributed toweight groups as can be observed in Figure 2.
Thetriggers are lemmas, so the real number of wordsthat can be identified as having a sentiment valueis much higher.191This list is not closed and it suffers modifications,especially by adding new triggers, but in certaincases, if a bad behavior is observed, the weightsmay also be altered.Figure 2.
Number of sentiment words by weight groupsWe define a modifier as a word or a group ofwords that can increase or diminish the intensity ofa sentiment trigger.
We have a manually built listof modifiers.
We consider negation words a specialcase of modifiers that usually have a greater impacton sentiment triggers.
So, we also built a small listof negation words.4.2 FormalismGeneral definitions: We define a sentimentsegment as follows: = (, , 		)sSG is a tuple in which the first two elements areoptional.Let NL be the set of negation words that we use,ML the set of modifiers and TL the set of sentimenttriggers.
We define two partially ordered sets: = (, ?), ?	 	?
!
 		and 	% = (%, ?%), ?	% 	?
!
We consider ? and ?% are two binary relationsthat order sentiment segments based on theirweights.
The weights give a numericrepresentation of how strong or weak is thesentiment expressed by the sentiment segment.
Forinstance, if we have sSG1, sSG2, sSG3 with the weights1, 2, 3 and sSG4, sSG5, sSG6 with the weights 4, 5, 6,then sSG1 ?+ sSG2 ?+  sSG3 and sSG4 ?- sSG5 ?-  sSG6.We define a weight function, weightS: S ?
R,over the set of sentiment segments that returns areal number representing the global weight thattakes into consideration the effect of the negationwords and modifiers on the sentiment trigger.Global sentiment computation: In this section,we will describe how the cumulative value of asentiment segment, expressed by the weightS, iscomputed.At the base of a sentiment segment stands thegiven weight of the sentiment trigger that is part ofthe general segment.
Besides that, modifiers andnegation words have a big impact.
For example,consider the following three sentences.1.
John is a good person.2.
John is a very good person.3.
John is the best.In the first one, a positive sentiment is expressedtowards John.
In the second one, we also have apositive sentiment, but it has a bigger power and inthe third one the sentiment has the strongestintensity.We distinguish two separate cases in whichnegation appears.
The first one is when thenegation word is associated with a sentimenttrigger and it changes a positive one into a negativetrigger and vice versa; and the second one refers tothe case in which the negation affects a triggeraccompanied by a modifier.
We illustrate thesesituations in the following examples.A1.
John is a good person.A2.
John is not a good person.B1.
John is the best.B2.
John is not the best.If we assign the weight +2 to good in the A1sentence, it is safe to say that in A2, not good willhave the weight -2.
From a semantic perspective,we have the antonym relation: good ?
?
good andthe synonym relation ?
good = bad).On the other hand, in the B2 example, not thebest is not the same as the worst, the antonym ofthe best.
In this case, we consider not the best to besomewhere between good and the best.
We give amore detailed description of this kind of orderingin the formalisms section.Entity sentiment computation: Let E denote anamed entity and Sent a sentence.
We define thesentiment value, sv, of an entity E in a sentence192Sent as the general sentiment expressed towards Ein Sent.
This value is a real number and is thecumulative effect of all the sentiment segment?sweights in that sentence.Let SSent be the set of all sentiment segments inthe sentence Sent and distance(E, sSG) the numberof tokens between E and sSG.
The expression forcomputing the sentiment value of an entity in asentence is given below:sv(E, Sent) 	= 	?weightS(s12)ln	1 + distance(E, s12)789	?18;<= |S1?
@A|The sv for an entity E in a larger text will be thesum of the sentiment values for E in everysentence of the text.4.3 EvaluationFor testing our system, we were interested in twoaspects: how well does it recognize sentimentsegments and how accurate is the semanticmeaning given by the system compared to the oneattributed by a person.
More than that, wedissected the sentiment segment and analyzed thesystem?s performance on finding sentimenttriggers and modifiers.Evaluation resources: Finding or developingclean resources is the most difficult part of theevaluation task.
We used 100 complex sentencesselected from news articles that were manuallyannotated as a gold standard.
Despite the smallnumber of sentences, they were specially thoughtto capture a large number of situations.Evaluation methods: We used precision, awidely known information retrieval metric andother measures that we developed for this task,such as a relaxed precision and deviation mean.We provide below a more detailed description ofthese metrics.We computed the precision for sentimentsegments, sentiment triggers and modifiers asfollows:BCDEDFG	 #	II	J	#K	J	 , 	?	L	 ?
{			,	 			,	}For the weight associated with the sentimentsegment, we use two types of precision: an exactmatch precision, Pweight in which we considered afound weight to be correct if it is equal to theweight given in the gold corpus and a relaxedprecision, RPweight.
We computed these metricsonly on the correctly identified segments.
Let CSbe the set of correctly identified segments, wF theweight of the sentiment segment returned by oursystem and wG the weight of the sentiment segmentfrom the gold corpus.OPBEQRD	 = 		?
SK!I?
()TUV	?W |X| ,?	SK!I?
() = 	 Y1,			|Z ?
| < 1.50,?										 `The RPweight measure is important because theweights given to a sentiment segment can differfrom one person to another and, by using thismetric, we allow our system to make smallmistakes.Besides the sentiment segments, we also testedthe sentiment values of entities.
For this task, weused four metrics.
The first one is a relaxedprecision measure for the sentiment valuescomputed for the entities.
Let SSV be the set of thesentiment values returned by the system, svF thesentiment value found by the system and svG thesentiment value specified in the gold corpus.OTa	 = 		?
SK!I?
(bZ)Tac?de |Ta| ,?	SK!I?
(bZ) = 	 Y1,			|bZ ?
b| ?
0.50,?													  `The last three metrics address the problem ofhow far the sentiment values are returned by thesystem from those considered correct by a humanannotator.
We called these measures sv positivedeviation, Dsv+, which takes into account onlypositive sentiment values, sv negative deviation,Dsv-, which takes into account only negativesentiment values and sv general deviation, Dsv+-, anaverage of the first two.fTa =	?
|bg ?
bh|bg?b+|b+|193SSV+ is the set of positive sentiment values foundby the system.
Dsv- is calculated in a similarmanner as Dsv+.5 ResultsThe results were obtained using the manuallyannotated sentences presented in the Evaluationresources section.
Out of those sentences, 58%contain entities and 42% contain only sentimentsegments.
The entity-related metrics could beapplied only on the first type of sentences.
Theresults can be observed in Figure 3.Figure 3.
Precision metrics resultsIn Figure 3, P_ss = Psentiment segment, P_st  =  Psentimenttrigger, P_m = Pmodifier and the rest of the metricshave the same meaning as defined in the evaluationmethods section.Figure 4.
Deviation metrics resultsIn Figure 4, we show the results of the metrics thatfollow the sentiment value deviation.6 DiscussionThe main problem encountered is the contexts inwhich the opinions that we identify appear.
It ispossible that the same trigger has a positivemeaning in a context, and in another context to benegative.
For example, ?scade TVA?
(En: ?reduceVAT?)
which is positive, compared to "scadsalariile?
(En: ?reduce salaries?)
which is negative.In these cases the trigger ?scade?
(En: reduce) canlead to opposing opinions.
As for ?inchide fabrica?
(En: ?close the plant?
), that has a negative contextcompared to ?inchide infractorul?
(En: ?close theoffender?)
which is positive.Another problem in quantifying the sentimentsand opinions is related to numerical values that weidentify in the text.
For example ?15 profesoriprotesteaza?
(En: ?15 teachers protest?)
comparedto ?2.000.000 de profesori protesteaza?
(En:?2,000,000 teachers protest?).
In both cases wehave negative sentiments, but it is clear that thesecond case has even a stronger sense due to thelarge number of people who participate in theprotest.
If in the first case it seems to be a localissue, at the school, in the second case, it seems tobe a general problem that is seen nationwide.7 Conclusion and Future WorkThis paper introduces the Sentimatrix system.
Themain components of the system are dedicated toidentifying named entities, opinions andsentiments.
Preliminary evaluation show promisingresults.Future work includes completing the resourceslists with entities, sentiment triggers and modifiers.As we have seen in the tests, rapid improvementscan be achieved by taking into considerationmodifiers such as ?daca?, ?posibil?, ?ar putea?
(En: ?if?, ?possible?, ?could?)
which have theeffect of lowering the intensity of opinions andsentiments.
Also, we intend to build a bigger goldcorpus to evaluate sentiments by using a semi-automatic approach (at first the system generatesannotation, which is later to be validates andcompleted by a human annotator).AcknowledgmentsThe research presented in this paper is partiallyfunded by the Sectoral Operational Program forHuman Resources Development through theproject ?Development of the innovation capacityand increasing of the research impact through post-doctoral programs?
POSDRU/89/1.5/S/49944.194ReferencesBo Pang, Lillian Lee.
2008.
Opinion Mining andSentiment Analysis, Found.
Trends Inf.
Retr., Vol.
2,No.
1?2.
(January 2008), pp.
1-135Cecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.2005.
Emotions from text: machine learning for text-based emotion prediction.
In Proceedings of theHuman Language Technology Conference and theConference on Empirical Methods in NaturalLanguage Processing (HLT/EMNLP).David Nadeau and Satoshi Sekine.
2007.
A survey ofnamed entity recognition and classification,Linguisticae Investigationes 30, no.
1, 3{26,Publisher: John Benjamin?s Publishing CompanyDavid Nadeau.
2007.
Semi-supervised named entityrecognition: Learning to recognize 100 entity typeswith little supervision, PhD Thesis.Ellen Riloff, Janyce Wiebe, and William Phillips.
2005.Exploiting subjectivity classification to improveinformation extraction.
In Proceedings of AAAI,pages 1106?1111.Hugo Liu, Henry Lieberman, and Ted Selker.
2003.Amodel of textual affect sensing using real-worldknowledge.
In Proceedings of Intelligent UserInterfaces (IUI), pages 125?132.Iadh Ounis, Maarten de Rijke, Craig Macdonald, GiladMishne, and Ian Soboroff.
2006.
Overview of theTREC-2006 Blog Track.
In Proceedings of the 15thText REtrieval Conference (TREC 2006).Janyce M. Wiebe 1990.
Identifying subjectivecharacters in narrative.
In Proceedings of theInternational Conference on ComputationalLinguistics (COLING), pages 401?408.Janyce M. Wiebe.
1994.
Tracking point of view innarrative.
Computational Linguistics, 20(2):233?287.Janyce Wiebe and Rebecca Bruce.
1995.
Probabilisticclassifiers for tracking point of view.
In Proceedingsof the AAAI Spring Symposium on EmpiricalMethods in Discourse Interpretation and Generation,pages 181?187.Marti Hearst.
1992.
Direction-based text interpretationas an information access refinement.
In Paul Jacobs,editor, Text-Based Intelligent Systems, pages 257?274.
Lawrence Erlbaum Associates.Namrata Godbole, Manjunath Srinivasaiah, and StevenSkiena.
2007.
Large-scale sentiment analysis fornews and blogs.
In Proceedings of the InternationalConference on Weblogs and Social Media (ICWSM).Pero Subasic and Alison Huettner.
2001.
Affect analysisof text using fuzzy semantic typing.
IEEETransactions on Fuzzy Systems, 9(4):483?496.Richard M. Tong.
2001.
An operational system fordetecting and tracking opinions in on-line discussion.In Proceedings of the Workshop on Operational TextClassification (OTC).Scurtu V., Stepanov E., Mehdad, Y.
2009.
Italiannamed entity recognizer participation in NERtask@evalita 09, 2009.195
