SquibsStable Classification of Text GenresPhilipp Petrenz?University of EdinburghBonnie Webber?
?University of EdinburghEvery text has at least one topic and at least one genre.
Evidence for a text?s topic and genrecomes, in part, from its lexical and syntactic features?features used in both Automatic TopicClassification and Automatic Genre Classification (AGC).
Because an ideal AGC system shouldbe stable in the face of changes in topic distribution, we assess five previously published AGCmethods with respect to both performance on the same topic?genre distribution on which theywere trained and stability of that performance across changes in topic?genre distribution.
Ourexperiments lead us to conclude that (1) stability in the face of changing topical distributionsshould be added to the evaluation critera for new approaches to AGC, and (2) part-of-speechfeatures should be considered individually when developing a high-performing, stable AGCsystem for a particular, possibly changing corpus.1.
IntroductionThis article concerns Automated Genre Classification (AGC).
Genre has a range ofdefinitions, but for Language Technology, a good one is a class of documents that sharea communicative purpose (e.g., Kessler, Nunberg, and Schu?tze 1997).
Although com-municative purpose may be difficult to recognize without document understanding,researchers have found low-level features of texts to correlate with genre, making it auseful proxy.AGC can directly benefit Information Retrieval (Freund, Clarke, and Toms 2006),where users may want documents that serve a particular communicative purpose(instructions, reviews, user guides, etc.).
AGC can also benefit Language Technologyindirectly, where differences in the low-level properties that correlate with genre mayimpact system performance.
For example, if a part?of?speech (PoS) tagger or StatisticalMachine Translation system trained on a corpus of editorials was then used for PoStagging or translating a corpus of letters to the editor, it would benefit from the knowledgethat inter alia the likelihood of the word ?states?
being a verb is considerably higher inletters (?20%) than in editorials (?2%).1?
University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK.E-mail: p.petrenz@sms.ed.ac.uk.??
University of Edinburgh, 10 Crichton Street, Edinburgh EH8 9AB, UK.E-mail: bonnie.webber@ed.ac.uk.1 This holds in the NYT Annotated Corpus, whether the topics are as different as Health and Defense.Submission received: 23 July 2010; revised submission received: 15 October 2010; accepted for publication:8 December 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 2Genre differs from topic, which is what a text is about.
Theoretically, a text from anygiven genre can be about any given topic (Finn and Kushmerick 2006), yet it is clear thatco-variances exist between genre and topic, with some genre?topic combinations morelikely than others (cf.
fiction vs. news reports about dragons).Because both genre classification and topic classification exploit low-level featuresof text as a basis for their predictions, a feature indicative of topic might benefit a genreclassifier through correlations in the training corpus.
However, if the topics addressed indifferent genres can change unpredictably over time, such correlated features can thenharm performance.
Although domain adaptation techniques might remedy this, theytypically require extensive data in the target domain, and the remedy may fail as soonas the distribution changes again.To date, the correlation between topic and genre has not been quantified, nor hasthe extent to which it may change in an actively growing corpus.
In order to motivateresearch on stability in AGC,2 we analyzed a large, publicly available newspaper corpusand found that (1) genres and topics do correlate substantially and (2) these correlationsvary substantially over time.3This squib makes two points: (1) Low-level features that correlate with topic candegrade the performance of AGC systems and are best removed unless the genre?topicdistribution is guaranteed to be fixed, and (2) PoS features should not be lumped to-gether in AGC because they have different correlations with genre and topic.
Althoughthe experiments used to make these points reflect an extreme situation?a completechange in genre?topic distribution?they do allow us to make these points convincingly.2.
MethodThe data for our experiments come from the New York Times Annotated Corpus (NYTAC)(Sandhaus 2008), covering 21 years of publication (1987?2007) and more than 1.8 millionarticles.4 Articles are richly annotated with meta-data, including fields whose values canbe used to infer their genre and topic.Genre: Two meta-data fields are related to the notion of genre as communicative pur-pose: Types of Material and Taxonomic Classifier.
The former, appearing with 41.5% ofarticles, specifies the editorial category of an article.
Usually the field has a single value,sometimes more than one.
Although these values are not drawn from a fixed set, theycan be used to infer genre after spelling errors are corrected (e.g., from Reivew to Review)and similar values are merged (e.g., Editorial, editorial, Op-Ed, and Editors?
Note).The values in the second genre-related field (Taxonomic Classifier) are drawn froma hierarchy, some of whose divisions indicate the section of the newspaper in whicha document appears.
In total, 99.5% of documents in the corpus contain a TaxonomicClassifier field, with an average of 4.5 values per article.
Although the hierarchy variesin depth, its second level comprises a set of four fairly high level genres?that is, Top/Classifieds, Top/Features, Top/News, and Top/Opinion.Because the Types of Material field did not include news reports, we used the Taxo-nomic Classifier field to recognize documents from this genre.
Specifically, we considered2 The term stability is used in machine learning to describe the repeatability of a learner?s results (cf.Turney 1995).
Here, we use it to describe the robustness of a method to (topical) domain changes orchanges in the topic?genre distribution.3 The results of our analysis can be found at http://homepages.inf.ed.ac.uk/s0895822/SCTG/.4 www.ldc.upenn.edu, Catalog Id=LDC2008T19.386Petrenz and Webber Stable Classification of Text GenresTable 1Genre (columns) and topic (rows) distribution in the data sets used in the experiments.Training set(3 ?
4,309 = 12,927 texts)News Edit.
LttEEdu xDef xMed xTest set 1(3 ?
2,155 = 6,465 texts)News Edit.
LttEEdu xDef xMed xTest set 2(6 ?
2,285 = 13,710 texts)News Edit.
LttEEdu x xDef x xMed x xany document with no Types of Material tag as a news report, if at least one of its TaxonomicClassifier values started with Top/News.Topic: Topic descriptors were drawn from the General Online Descriptors meta-data field.The field appears with 79.7% of documents, with 3.3 descriptors per document onaverage.5 Whereas General Online Descriptors are structured in a hierarchy, a documenttagged with the more specific United States Politics and Government will also typically betagged with the less-specific (i.e., closer to the root) value Politics and Government, butnot vice versa.Framework: Our experiments use news reports, editorials, and letters as target variablesbecause similar classes have been used elsewhere in AGC research (e.g., Finn andKushmerick 2006; Karlgren and Cutting 1994).
For topics, we chose three that occurfrequently and that were distinct from each other, in order to maximize differences inthe formal cues used by classifiers.
We based distinctiveness on the percentage overlapof topic tags in the corpus: Topics were taken to be distinct if (for our three chosengenres) fewer than 5% of texts that had one of the tags had another of them.
Thedegree of overlap in the three topic tags we chose, ?Education and Schools?, ?Armament,Defense and Military Forces?, and ?Medicine and Health?, ranges from 0.5% to 2.9%.For comparison, the degree of overlap of the pair ?Politics and Government?
and?International Relations?
ranges from 28.4% to 38.1% for our three genres.
For all ex-periments, we only used texts which were unambiguously about Education, Medicine,or Defense.
The small proportion of documents with more than one of these tags wasignored.In order to examine the impact on system performance of a complete shift intopics, we varied the topical distribution of the test sets with respect to the trainingset.
The training set consisted of 12,927 texts: News reports (News) about Education (Edu),Editorials (Edit.)
about Defense (Def), and Letters to the Editor (LttE) about Medicine (Med).
(This pairing yields more articles in the corpus than any of the five other possiblecombinations.)
Table 1 shows the genre?topic distribution of both the training set andthe two test sets used in these experiments.
The first test set (6,465 articles) had thesame distribution as the training set.
In the second test set (13,710 articles), genre?topicpairings were inverted (see Table 1).
All sets were balanced with an equal number oftexts for each genre?topic combination (where not zero).
The difference in the size of thetest sets reflects the number of articles available with the desired topic?genre pairing.The training set and test set 1 were created by a random 2:1 split within each genre class.Because test set 2 comes from a different distribution than the training set, we report5 Genre-related differences in the number of General Online Descriptors are described further athttp://homepages.inf.ed.ac.uk/s0895822/SCTG/.387Computational Linguistics Volume 37, Number 2results on these large holdout sets rather than performing cross-validation.
We inferredconfidence intervals by assuming that the number of misclassifications is approximatelynormally distributed with mean ?
= e ?
n and standard deviation ?
=??
?
(1 ?
e),where e is the percentage of misclassified instances and n is the size of the test set.We took two classification results to differ significantly only if their 95% confidenceintervals (i.e., ?
?
1.96 ?
?)
did not overlap.3.
Assessing Performance on Static and Altered Genre?Topic DistributionsTo make our first point?that low-level features that correlate with topic can degradethe performance of AGC systems and are best removed unless the genre?topic dis-tribution is fixed?we show how five published approaches to AGC perform in ourexperimental framework (Section 3).
The choice of methods was partly motivated bythe study by Finn and Kushmerick (2006), which compares bag-of-words, PoS frequen-cies, and text statistics as document representations in genre classification tasks acrosstopical domains.
The methods we assessed were chosen so that all these features wererepresented to different degrees.
All were implemented on the same platform (Petrenz2009).KC: Karlgren and Cutting (1994) use a small set of textual features and discriminantanalysis to predict genres.
Most of these features involve either PoS frequencies or textstatistics.
Counts based on the fixed length of texts used in their experiments wereadjusted to represent frequencies rather than absolute counts.KNS/KNSPOS: Kessler, Nunberg, and Schu?tze (1997) predict genre based on surfacecues.
Because the paper gives few details about the specific features they use, wecommunicated with the authors directly.
The list they gave us included features thatrequire PoS tagging.
As their published experiments do not make use of such features,we included two versions of their method, one version with PoS-based features and onewithout.
6FCT: Freund, Clarke, and Toms (2006) predict genre using a support vector machine ona simple bag-of-words representation of a text.
This feature set is not filtered using stopwords or other techniques.FMOG: Feldman et al (2009) use part-of-speech histograms and principal componentanalysis to construct features.
The authors classify genres using the QDA and NaiveBayes algorithms.
We followed their decision to compute histograms on a sliding win-dow of five PoS tags.SWM: Sharoff, Wu, and Markert (2010) found a character n-gram based feature setto perform better than PoS n-grams and word n-grams in extensive AGC experi-ments using nine data collections and different choices of n. Although variable lengthn-grams as features for genre classification had previously been proposed by Kanarisand Stamatatos (2007), we followed Sharoff, Wu, and Markert in using fixed length4-grams, which they found to yield higher accuracies.Although a variety of Machine Learning (ML) methods were used in these ap-proaches, here we just used the SVM implementation by Joachims (1999) because otherML methods produced similar, albeit poorer, results (Petrenz 2009).
For PoS tagging, weused the Stanford maximum entropy tagger described in Toutanova et al (2003).6 The features we used are listed at http://homepages.inf.ed.ac.uk/s0895822/SCTG/.388Petrenz and Webber Stable Classification of Text GenresFigure 1Classification accuracy of six different genre classification methods (cf.
Section 3).
Chart showsthe percentage of correctly classified instances in both test sets (TS 1/2, cf.
Table 1), with theconfidence interval boundaries given in parentheses.Figure 1 shows the results of training and testing on the same genre?topic distribu-tion (test set 1).
These results confirm the findings of SWM that binary character n-gramsare good features in AGC.
Both their approach and the bag-of-words approach used byFCT significantly outperform all other methods when the genre?topic distribution is thesame for training and testing.A different picture emerges from the second test set, however, whose genre?topicdistributions differ from the training set.
It shows that some feature sets owe their goodresults on test set 1 to the strong correlation between topics and genres.
The performanceof both SWM and FCT is significantly worse, with the latter even worse than the 33.3%that a random guess classifier would achieve in this balanced 3-class classification task.The performance drop for both KC and KNS is slight but still significant.
WhereasKNSPOS had significantly outperformed KNS on test set 1, its performance is signifi-cantly worse than that of KNS on test set 2.
(More on this shortly.
)Some of these results are not surprising: As bag-of-words and character n-grams re-flect lexical differences of texts, systems that rely on them (SWM and FCT) will be misledby a major change in genre?topic distribution.
Similar findings were reported in Finnand Kushmerick (2006) for bag-of-words and in (SWM) for character n-grams, althoughno explicit tests with topical distributions were carried out in the latter.
More surprisingare the results involving PoS tags: Two of the methods that used PoS tags?FMOGand KNSPOS?suffered when the genre?topic distribution changed, even though PoSfrequencies had previously been reported to perform well as a feature set when used innew topical domains (Finn and Kushmerick 2006).
However, the PoS frequencies in theKC feature set did not seem to harm stability much.
This brings us to the second pointof this squib.4.
Impact of PoS Features on Performance and StabilityWe justify our second point?that PoS features should not be lumped together inAGC because they have different correlations with genre and topic?through a set ofexperiments that assess the effect of adding PoS features to a set of basic non-PoS389Computational Linguistics Volume 37, Number 2Table 2Prediction accuracies for the baseline feature set and the same set with all 36 PoS tags added.Test set 1 Test set 213 surface features (Baseline) 72.9% (?
1.1%) 70.8% (?
0.8%)13 surface + 36 PoS features 87.1% (?
0.8%) 72.5% (?
0.7%)features similar to those used earlier by Karlgren and Cutting (1994), here normalized bydocument length (in words).
These basic non-PoS features include character count perdocument, sentence count per document, average character count per sentence, averageword count per sentence, average character count per word, type/token ratio, frequencyof long words (ones with more than 6 letters), and the frequencies of the words therefore,I, me, it, that, and which.The texts were PoS-tagged (Toutanova et al 2003), using the same tag set as in thePenn Treebank (Marcus, Marcinkiewicz, and Santorini 1993).
All 36 non-punctuationtags were used, and counts of PoS-tags were normalized by document length.Each experiment involved the 13 non-PoS features and a single PoS frequencyfeature (36 sets).
Comparing performance with that on the non-PoS features alone (asa baseline) demonstrated the effect of adding each PoS frequency feature on classifieraccuracy and stability.7 All 36 feature sets as well as the baseline set were trained onthe same training set.
They were then tested on both test sets described in Section 2.
Asbefore, we use the same Support Vector Machines (SVM) classifier in all experiments,with the set of features as the experimental variable.Table 2 shows both the accuracy of the baseline system on the two test sets as well asthe accuracy of a system with the baseline features plus all 36 PoS features.
Recall fromTable 1 that the genre?topic distribution for test set 1 is the same as in the training set,whereas in test set 2 it is different.
The first thing to note in Table 2 is that the classifierperforms significantly better on the larger set of 49 features than on the smaller set of13 basic features when the genre?topic distribution is not altered (column 2).
When it isaltered (column 3), the losses are less severe on our baseline than on the set that includesPoS features.
This can be explained by looking at the contributions of each feature.Figure 2 shows how accuracy changes when each PoS feature is added individuallyto the basic set of non-PoS features.
To highlight the most interesting results, we onlyshow features which cause a deviation of more than 1% from the baseline for at leastone of the two test sets.8 When the topic?genre distribution remains the same (i.e., testset 1), PoS frequencies appear to have a positive impact on prediction accuracy: Whenadded to the basic feature set, accuracy increases.
This is especially true for the tags VBD(past tense verb), JJ (adjective), RB (adverb), NN (singular noun), VB (base form verb),and NNP (plural proper noun).The same is not true when the topic?genre distribution is changed (test set 2).Figure 2 shows that the PoS tags NN (singular noun), NNS (plural noun), and NNPS(plural proper noun) all have a toxic effect when added to the baseline set.
This meansthe NN, NNS, and NNPS frequency features improve accuracy in stable conditions,whereas they severely harm it as topics change.
This is not good for stability.
A similar,7 The goal was not to select the best subset of PoS features?for that one would use a different method?butrather to show precisely how PoS features differ from each other with respect to AGC stability.8 The full results can be found at http://homepages.inf.ed.ac.uk/s0895822/SCTG/.390Petrenz and Webber Stable Classification of Text GenresFigure 2Deviation in percentage of correctly classified instances from the baseline feature set (cf.Table 2) for each added PoS tag frequency and test set (TS 1/2, cf.
Table 1).
CC = coordinatingconjunction; CD = cardinal number; JJ = adjective; MD = modal; NN = singular noun; NNP =proper noun; NNPS = plural proper noun; NNS = plural noun; RB = adverb; TO = to; VB =base form verb; VBD = past tense verb; VBZ = third-person singular present verb.if somewhat weaker effect, can be observed for the CD (cardinal number) frequency.Other PoS tags like CC (coordinating conjunction), RB (adverb), and VB (base formverb) increase predictive power while not impairing stability.
Even more interesting arethe results for the tags VBD (past tense verb) and VBZ (third-person singular presentverb).
Adding these features eliminates the significant difference between accuracies ontest set 1 and 2, which we observed on the baseline feature set.This makes sense if we consider how different genres and topics vary in their useof different parts of speech.
In our data sets, for example, the frequency of plural noun(NNS) varies more by topic (on average, 8.0% of words are NNS in texts on Educationand Schools, 7.6% in texts on Medicine and Health, and 6.3% in texts on Defense and MilitaryForces) than it does by genre (on average, 7.3% of words are NNS in news reports, 7.1%in letters, and 7.5% in editorials).
The opposite holds for past tense verbs (VBD): Theiraverage frequency varies less by topic (2.9%, 2.9%, and 3.3%) than by genre (4.8%, 1.8%,and 2.5%).The odd result here is that singular proper noun (NNP) frequencies do not impairclassifier stability: Unlike NN, NNS, and NNPS frequencies, they are topic-independent.This is because the most commonly tagged singular proper nouns in news reports aretitles such as ?Mr.
?, ?Ms.
?, ?Dr.
?, etc., regardless of the topic.
The frequency of titlesamong proper nouns is much lower in editorials and even lower in letters, across allthree topical domains.
NNP frequency in news reports is increased by the fact that titlesare usually followed by one or more names, which are also singular proper nouns.
Weassume that this is the reason that the fluctuation between NNP frequencies is relativelylow across topics (for the three topics and genres we examined) and hence a stablecontributor to genre prediction.Note that we are not making any claim about whether these specific results (e.g.,that NN frequencies are bad for stability) hold for settings with different genres andtopics.
Rather, our point is that PoS tags should not be included or excluded wholesale391Computational Linguistics Volume 37, Number 2for AGC.
If one is going to the expense of PoS-tagging texts, only a subset of PoS tagsshould be used as features in AGC in order to maintain performance across changes inthe topical distribution.5.
ConclusionOur results suggest that prediction accuracy on a static topic distribution should notbe the sole basis for assessing the quality of Automatic Genre Classification sys-tems: In particular, approaches that perform well on a static topic distribution can beseverely impacted by changes in topical distributions.
The notion of topic indepen-dence for features has rarely been explored in the literature on genre classification.Nevertheless, this is an important issue, especially in dynamic environments like theWorld Wide Web, where new topics emerge rapidly and unpredictably.
In topicaldomains with little or no labeled data to train on, instability can impede any usefulapplication of classifiers.
Because of this, we believe that stability should join accu-racy as a criterion for assessing any new developments in genre classification.
To thisend, we introduced a cross-product methodology in Section 2 as a way of assessingstability.Our results also suggest that, where the cost of PoS-tagging is acceptable, selectiveuse of PoS-based features can yield high performance that is stable even when topicaldistribution differs from training to test sets.
Although we have not identified a set ofPoS-based features that supports classification among an arbitrary set of genres, wecan say that it is crucial to evaluate and select PoS-based features carefully in order toachieve genre classification which is as topic-independent as possible.AcknowledgmentsWe would like to thank Katja Markert,Mark Steedman, Maria Wolters, and fouranonymous reviewers, each of whom gaveus many valuable comments on earlierdrafts of this article.ReferencesFeldman, S., M. A. Marin, M. Ostendorf,and M. R. Gupta.
2009.
Part-of-speechhistograms for genre classification oftext.
In Proceedings of the 2009 IEEEInternational Conference on Acoustics,Speech and Signal Processing,pages 4781?4784, Washington, DC.Finn, Aidan and Nicholas Kushmerick.2006.
Learning to classify documentsaccording to genre.
Journal of theAmerican Society for Information Scienceand Technology, 57(11):1506?1518.Freund, Luanne, Charles L. A. Clarke,and Elaine G. Toms.
2006.
Towardsgenre classification for IR in theworkplace.
In Proceedings of the 1stInternational Conference on InformationInteraction in Context, pages 30?36,New York, NY.Joachims, Thorsten.
1999.
Makinglarge-scale support vector machinelearning practical.
In B. Scho?lkopf,C.
J. C. Burges, and A. J. Smola, editors,Advances in Kernel Methods: Support VectorLearning.
MIT Press, Cambridge, MA,pages 169?184.Kanaris, Ioannis and Efstathios Stamatatos.2007.
Webpage genre identification usingvariable-length character n-grams.
InProceedings of the 19th IEEE InternationalConference on Tools with AI, pages 3?10,Washington, DC.Karlgren, Jussi and Douglass Cutting.1994.
Recognizing text genres withsimple metrics using discriminantanalysis.
In Proceedings of the 15thConference on Computational Linguistics,pages 1071?1075, Morristown, NJ.Kessler, Brett, Geoffrey Nunberg, andHinrich Schu?tze.
1997.
Automaticdetection of text genre.
In Proceedings ofthe 35th Annual Meeting of the Associationfor Computational Linguistics, pages 32?38,Morristown, NJ.Marcus, Mitchell P., Mary AnnMarcinkiewicz, and Beatrice Santorini.1993.
Building a large annotated corpus of392Petrenz and Webber Stable Classification of Text GenresEnglish: The Penn treebank.
ComputationalLinguistics, 19(2):313?330.Petrenz, Philipp.
2009.
Assessing approachesto genre classification.
M.Sc.
thesis, Schoolof Informatics, University of Edinburgh.Sandhaus, Evan.
2008.
New York Timescorpus: Corpus overview.
LDC catalogueentry LDC2008T19.Sharoff, Serge, Zhili Wu, and KatjaMarkert.
2010.
The Web Library ofBabel: Evaluating genre collections.In Proceedings of the Seventh Conferenceon International Language Resourcesand Evaluation, pages 3063?3070,Valletta.Toutanova, Kristina, Dan Klein,Christopher D. Manning, and YoramSinger.
2003.
Feature-rich part-of-speechtagging with a cyclic dependencynetwork.
In Proceedings of the 2003Conference of the North American Chapter ofthe ACL and Human Language Technology,pages 173?180, Morristown, NJ.Turney, Peter.
1995.
Technical note: Biasand the quantification of stability.Machine Learning, 20(1?2):23?33.393
