Three New Probabi l is t ic  Mode lsfor Dependency  Parsing: An  Exploration*J ason  M.  E i snerCIS  Depar tment ,  Un ivers i ty  of  Pe lmsy lva i f ia .200 S. 33rd  St. ,  Ph i lade lph ia ,  PA  19104-6"{89, USAj eisner@linc, cis.
upenn, eduAbstractAlter presenting a novel O(n a) parsing al-gorithm for dependency grammar, we de-velop three contrasting ways to stochasticizeit.
We propose (a) a lexical atfinity mode\]where words struggle to modify each other,(b) a sense tagging model where words tluc-tuate randomly in their selectional prefer-ences, and (e) a. generative model wherethe speaker fleshes ()tit each word's syntacticand concep{.ual structure without regard tothe implications :for the hearer.
W(!
also givepreliminary empirical results from evaluat-ing the three models' p;Lrsing performanceon annotated Wall Street Journal trMningtext (derived fi'om the Penn Treebank).
inthese results, the generative model performssignificantly better than the others, anddoes about equally well at assigning pa.rt-of-speech tags.1 In t roduct ionIn recent years, the statistical parsing communityhas begun to reach out; for syntactic formalismsthat recognize the individuality of words, l,inkgrammars (Sleator and 'Pemperley, 1991) and lex-icalized tree-adjoining ranunars (Schabes, 1992)have now received stochastic treatments.
Otherresearchers, not wishing to abandon context-fleegrammar (CI"G) but disillusioned with its lexica\]blind spot, have tried to re-parameterize stochas-tic CI"G in context-sensitive ways (Black et al,1992) or have augmented the formalism with lex-ical headwords (Magerman, 1995; Collins, 11996).In this paper, we 1)resent a \[lexible l)robat)ilisticparser that simultaneously assigns both part-of-sl)eech tags and a bare-bones dependency struc-ture (illustrate.d in l!
'igure 1).
The choice o t 'asimple syntactic structure is deliberate: we wouldlike to ask some basic questions about where h'x-ical relationships al)pear and how best, to exploit*This materia.l is based upon work supported un-der a National Science I%undation Graduate Fellow-ship, and has benefited greatly from discussions withMike Collins, Dan M(:lame(l, Mitch Marcus and Ad-wait Ratnaparkhi.
(a) Tile man in the coiner  taught his dachsht , ld  IO play gol f  I';OSDT NN IN DT NN VBD PP.P$ NN TO VH NN/?
man N ~.. phty~ j J - y ,  .%(b) The ill __ ~ / .~dachshund It) gol f. ) f  COfllel hisfileFigure 1: (a) A bare-l>ones dependen(-y parse.
\]'\]a<:hword points to a single t)arent, the word it modities;the head of the sentence points to the EOS (end-of:sentence) ma.rk.
Crossing links and cycles arc not al-lowed.
(b) Constituent structure and sub(:ategoriza-tion may be highlighted by displaying the same de-pendencies as a lexical tree.them.
It is uscflfl to look into thes0 basic ques-tions before trying to tine-tmm the performance ofsystems whose behavior is harder to understand.
1The main contribution of' the work is to I)ro-pose three distin('t, lexiealist hyl)otheses abou(.
(,heprobability space underlying seHl\]ence structure.We il\]ustrate how each hypothesis is (:xl)ressed ina depemteney framework, and how each can beused to guide our parser toward its favored so-lution.
Finally, we point to experimental resul(;sthat compare the three hypotheses' parsing per-formance on sentences fi:om the Wall ,b'treel dour-hal.
\ ] 'he parser is trained on an annol,ated corpus;no hand-written grammar  is required.2 Probabilistic DependenciesIt cannot be emphasized too strongly that a gram-marital rcprcsentalion (de4)endency parses, tag se-quen(-es, phrase-structure trees) does not entailany particular probability model.
In principle, onecouht model the distribution of dependency l)arsesl()ur novel parsing algorithm a/so rescues dependency from certain criticisins: "l)ependency granl-mars .
.
.are not lexicM, and (as far ~ as we know) lacl(a parsing algorithm of efficiency compara.ble to linkgrammars."
(LMferty et ;LI., 1992, p. 3)340in any uuml)er of sensible or perverse ways.
'l'h(~choice of l;he right model is not a priori (A)vious.One way to huild a l)robabilistie g rammar  is tospecify what sequences of moves (such as shift an(/reduce) a parser is likely to make.
It is reasonableto expect a given move to be correct about asoften on test data.
as on training data.
This istire philosophy behind stochastic CF(I  (aelinek eta1.1992), "history-based" phrase-structure parsing(I-~lack et al, 1992), +m(I others.IIowever, i)rol)ability models derived fromparsers sotnetimes focus on i,lci(lental prope.rtiesof the data.
This utW be the case for (l,alli'.rty etM., 1992)'s model for link grammar,  l\[' we were toadapt their top-(h)wn stochastic parsing str~tegyto the rather similar case of depen(lency gram-mar, we would find their elementary probabil it iestabulat ing only non-intuitive aspects of the parsestructure:Pr(word j is the r ightmost pre-k chihl of word i\] i is a right-sl)ine st, rid, descendant of one of theleft children of a token of word k, or else i is theparent of k, and i l)re(;edes j t)recerles k).
:eWhile it is dear ly  necessary to decide whether jis a child of i, conditioning that (Iccision as alrovemay not reduce its test entropy as mneh as a tnorelinguistically perspienous condition woul(/.We believe it is ffttil,\['u\[ to de.sign prol>al)ilitymodels indel)en(letrtly of tit(' pa.rser.
In this see-lion, we will outline the three+ lexicalist, linguis-tically perspicuous, qualitatiw~ly different modelsthat we have (leveloped a, nd tested.2.1 Mode l  A:  Bigram lex iea l  a f f in i t iesN-gram ta t ters  like (Church, 1988; .lelinek 1985;Kupiec 1992; Merialdo 1990) take the followingview of \]row ~/, tagged sentctrce enters the worhl.I"irst, a se.
(tuenee of tags is g('nexate.d aecordittg toa Markov l)rocess, with t.h(' random choice of e~chtag conditioned ou the previous two tags.
Second,a word is choseu conditional on each tag.Since our sentences have links as well as tagsand words, suppose that afl;er the words are in-serte(l, each senl;ence passes through a third stepthat looks at each pair of words and ran(lotnly de-cides whether to link them.
For the resulting sen-tences to resemble real tort)era, the.
probabil itythat word j gets linked to word i should b(' le:~:i-(:ally scnsilivc: it should depend on the (tag,word)pairs at both i and j.
'Fhe probabil ity of drawing a given parsed sen-(once froln the+ populat ion may then be expressed2This correspouds to l,Mi'erty el, al.
's central st~ttis-tk: (p. 4), l ' r (m +-I L, le, l,r), in the case where i'spa.rent is to the left el i. i , j ,  k correspond to L, W, Rrespectively.
Owing to the particular re(:ursiw~ strat-egy the p~trscr uses to bre+tk up the s(!\[tl,(?n(:e, thestatistic would be measured ~ttld utilized only underthe condition (lescribed above.
(a) Ihe \[nice of Ihc sRu:k 1%11I)T NN IN I)1' NN VIII)(b) tile price uf .
the stock R'II\]YI" NN IN I)T NN Viii)t,'igure 3: (++)Th(, ,:orrect parse.
(b) A cotnmon cr,orif the model ignores arity.as (1) in \[,'igure 2, where the random wMableLij G {0, 1} is t iff word i is the parent of word j.Expression (1) assigns a probabi l i ty to e.verypossible tag-a.nd-l ink-annotated string, and thesel)robabilities unl to one.
Many or the annotatedstrings exhibit violations such as crossing linksand multiple parents which, i f theywcrea l lowed,wouhl let al the words express their lexical prefe.r-ences independently and situttlta.ne:ously.
We SiAl)-ulate that the model discards fl'om the popula+tiontiny illegal structures that it generates; they do notappear in either training or test data.
Therefore,the parser described below \[inds the likeliest le-gal structure: it maximizes the lexical preferencesof ( l)  within the few hard liuguistic coush'ainlsitnlrosed by the del)endency formalism.In practice, solrre generalization or "coarsen-lug" of the conditionM probabil it ies in (1) heapsto avoid tile e.ll~ets of undertrMning.
For exalH-ph'., we folk)w standard prn(-tice (Church, 1988) inn-gram tagging hy using (3) to al)proxitllate thelit'st term in (2).
I)ecisions al)out how much coars-enittg t,o lie are+ o1' great pra(-t, ieal interest, b ut t hey(lel)etM on the training corpus an(l tnay l)e olnit-ted from a eonc<'.t)tuM discussion of' the model.
'Fhe model in ( I)  can be improved; it does not(:aptrlr(" the fact that words have arities.
For ex-+Unl)h.' , lh.e price of lh.c sleek fell (l"igure 3a) willtyl>ically 1)e nlisanalyzed under this model.
Sincestocks often fall, .sleek has a greater affinity f<>r fl:llthan lbr @ llen<:e stock (as w<'.ll as price) will en<ltt\[) t>ointittg to the verl> ./'(ell ( lqgure 31>), result, h itin a double subject for JNI and \[eavitlg of childless.
'l'o Cal)i.nre word aril, ies an(l othe+r stil>cal,<,gr)riza-lion I'aets, we must recognize that the.
chihh:ert ofa word like J~ll are not in(le4)ende.nt of each other.
'File sohttion is to nlodi/'y (t) slightly, furtherconditioning l,lj on the number and/or  type ofchildren of i that already sit between i and j. Thismeans that in I, he parse of Figure 3b, the link price-+ \]?~11 will be sensitive to the fact that fell alreadyhas a ok)set chihl tagged as a noun (NN) .
Specif-ically, tire price --+ fell link will now be stronglydisfavored in Figure '3b, since verbs rarely Lalw~two N N del)endents to the left.
By COllt;rast, price--> fell is unobjectionable in l!
'igure 3a, renderingthat parse more probable.
(This change (;an berellected in the conceptual model, by stating thattire l,ij decisions are Hla(le ill increasing order oflink length l i - - J l  and are no longer indepen(lent.
)2.2  Mode l  B: Se leet iona l  i ) re fe rencesIn a legal dependency l)axse, every word exceptfor the head of the setrtence (tile EOS mark)  has341Pr'(words, tags, links) =/ ' , ' (words ,  tags).
Pr(link presences and absences I words, tags) (1)I-\[ I t om(i + 1), twom(i + 2)).
I \ ]  I two,.d(i), two,'dO)) ('e)l< i<n l <_i,j <nl'v(tword(i) \] tword(i + 1), tword(i + 2)) ~ l','(tag(i) I tag(i + 1), tag(i + 2)).
P,'(word(i) I tag(/)) (a)Pr(words, tags, links) c~ Pr(words, tags, preferences) =/ ' r (words ,  tags).
Pr(preferences \] words, t~gs) (4)\]-I l',.
(twom(i) I two d(i + 1), t o,'d(i + 2)).
H I two,.d(i))1 <i<n t< i<n/ 1 +#r ight -k ids( i )  '~Pv(words, t+gs, links)= I I  { 1-\[ P,.
(two,.d(kid+(i))I t,gj +dd+_,(i) ),t+o,'d(i))l< i<n \c=-( \ ] -k#lef t+kids( i ) ) ,eT~0 kid~q_ 1 if c < 0Figure 2: tligh-level views of model A (formuhrs I 3); model l:l (forinul;t 4); and model C (lbrmula, 5).
If i andj are tokens, then tword(i) represents he pair (tag(i), word(i)), and L,j C {0, 1} i~ ~ ill" i is the p~m:nt of j.exactly one parent.
Rather than having the modelselect a subset of the ~2 possible links, as inmodel A, and then discard the result unless eachword has exactly one parent, we might restrict themodel to picking out one parent per word to be-gin with.
Model B generates a sequence of taggedwords, then specifies a parent or more precisely,a type of parent for each word j.Of course model A also ends up selecting a par-ent tbr each word, but its calculation plays carefulpolitics with the set of other words that happen toappear: in the senterl(;C: word j considers both thebenefit of selecting i as a parent, and the costs ofspurning all the other possible parents/ ' .Model Btakes an appro;~ch at the opposite extreme, andsimply has each word blindly describe its idealparent.
For example, price in Figure 3 might in-sist (with some probability) that it "depend on averb to my right."
To capture arity, words proba-bilistically specify their ideal children as well: fellis highly likely to want only one noun to its left.The form and coarseness of such specifications ia parameter of the model.When a word stochastically chooses one set ofrequirements on its parents and children, it ischoosing what a link grammarian would call a dis-juuct (set of selectional preferences) for the word.We may thus imagine generating a Markov se-quence of tagged words as before, and then in-dependently "sense tagging" each word with adisjunct, a Choosing all the disjuncts does notquite specify a parse, llowever, if the disjunctsare sufficiently specific, it specifies at most oneparse.
Some sentences generated in this way areillegal because their disjuncts cannot be simulta-neously satisfied; as in model A, these sentencesare said to be removed fi'om the population, andthe probabilities renormalized.
A likely parse istherefore one that allows a likely and consistentaln our implementation, the distribution over pos-sible disjuncts is given by a pair of Markov processes,as in model C.set of sells(', tags; its probability in the populationis given in (4).2.3 Mode l  C: Recurs ive  generat ionThe final model we prol)ose is a generat ionmodel, as opposed l;o the comprehens ion  mo(l-els A and B (and to other comprehension modc, lssuch as (l,afferty et al, 1992; Magerman, 1995;Collins, 1996)).
r\]'he contrast recalls an ohl debateover spoken language, as to whether its propertiesare driven by hearers' acoustic needs (coml)rehen-sion) or speakers' articulatory needs (generation).Models A and B suggest that spe~kers producetext in such a way that the grammatical relationscan be easily decoded by a listener, given words'preferences to associate with each other and tags'preferences to follow each other.
But model C saysthat speakers' primary goal is to flesh out the syntactic and conceptual structure \['or each word theyutter, surrounding it with arguments, modifiers,and flmction words as appropriate.
According tomodel C, speakers hould not hesitate to add ex-tra prepositionM phrases to a noun, even if thislengthens ome links that are ordinarily short, orleads to tagging or attachment mzJ)iguities.The generation process is straightforward.
Eachtime a word i is added, it generates a Markovsequence of (tag,word) pairs to serve, as its leftchildren, and an separate sequence of (tag,word)pairs as its right children.
Each Markov process,whose probabilities depend on the word i and itstag, begins in a speciM STAI{T state; the symbolsit generates are added as i's children, from closestto farthest, until it re~ches the STOP state, q'heprocess recurses for each child so generated.
Thisis a sort of lexicalized context-free model.Suppose that the Markov process, when gemcrating a child, remembers just the tag of thechild's most recently generated sister, if any.
Thenthe probability of drawing a given parse fi'om thepopulation is (5), where kid(i, c) denotes the cth-closest right child of word i, and where kid(i, O) =START and kid(i, 1 + #,'ight-kids(i)) = STOP.342(a)(b)dachshund ovcr  there  can  rea l ly  phtydachshund ow: r  there  can  rea l ly  p layI,'igure 4: Spans \])~u'ticipa, ting, in the (:orru(:l. i)a, rsc of7'h, at dachs/*und o'+wr there c(+u vcalhl ph+g golf~.
(st)has one pa,rcnt, lcss cndwor(I; its sul)sl)+tn (b) lists two.
(c < 0 in(h'xes l('ft children,) 'Fhis may bcthought o\[" as a, non-linca.r l;rigrrmt model, whereeach t;agg('d woM is genera, l,ed 1)ascd on the l)a.r('nl, 1,~gg(:d wor(l and ;t sistx'r tag.
'l'he links in theparse serve Lo pick o,tt; t, he r('Jev;mt t,rit:;t+a,n~s, anda.rc' chosen 1;o g('t; l,rigrams t, lml, ot)l, imiz(~ t, hc glohMt,a,gging.
'l'tt;tl; the liuks also ha.t)l)en t;o ;ulnot,;:d,('.useful setnant;ic rela, tions is, from this t>crsl)ective,quil.e a(-cidcn{,a,l.Note that  the revised v(',rsiol~ of ulo(h:t A usesprol)a, bilit, ics / "@ink  to chihl I child, I)arenl,,closer-('hihh:en), where n.)(le\] (; uses l 'v( l ink 1,ochild \] parent,, eloscr-chil(h'en).
'l'his is I)c(:;,.t~semodel A assunw.s 1,lu~l, I,h('.
(:hild was i)reviouslygencrat, ed I)y a lin(;a,r l)roc('ss, aml all t;hal, is nec-ess+u'y is t,o l i .
k  1,o it,.
Model (~ a, cl,ually g(,n(;ral,est, he chihl in the process o\[' l iuking to il,.3 Bot tom- \ [ ) i )  Dependency  Pars inglu this sec.tAon we sket(:h our dependel .
'y  l)m'sing;dg;oril, hnl: ~ novel dytmJni('.-l)rogr;mJndng m('.l,hod1,o assetnhle l, he mosl, l>rol)a,ble+ i)a.rse From the bet,-tom Ul).
The a lgor i@m ++(l(Is one link at a l, ime,nmking il; easy to mul t ip ly  oul, the hie(lois' l)rolmhility l'a(:t, ors.
It, also enforces I,hc special direcLion;dil,y requiremenl~s of dependency gra.nnnar,1;he l)rohibitions on cycles mM nlultiple par('nl,s.
4'\['\]10 liic.t\]tod llsed is s imilar t;o t ie  C K Y met.hodof cont.exl,-fr('e l)~rsing, which combines aJIMys(:sof shorl, er substr ings into analys<:s of progressivelylonger ones.
Mult iple a.na.lyses It;we l, hc s~tnms ignature  if t;hey are indistinguishal>le i , theirM)il ity to (;Otlll)ill(?
wit,h other analyses; if so, theparser disca,rds all but, the higlmsl,-scoring one.CI, :Y t'cquit',;s ()(?,.
:t~ ~) t.i,,,,' +utd O(,,.
:'.~) sp+.
'.,;,where n is the lenglih of 1,he s(mtcn(:c and ,s is a,nUpl)(;r bouiM on signal;ures 1)er subsl;ring.Let us consider dependency parsing in t;hisf ramework.
()he mighl; guess that each substa'ing;mMysis shottld bc t+ lcxicM tree ;+ tagged he;ul-word plus aJl Icxical sulfl;rees dependc'nt, uponi/,.
(See l"igure 111.)
l lowew, r, if a. o:/tst,il, cnt s?
11,Mmled depend(reties a,re possible, a.nd a minorva,ria, nt ha.ndles the sitnplcr (:~tse of link gra.tnltl;-u', hi-deed, abstra.ctly, the a.lgorithm rescmbies ;t c\](,.aamr,bottom-up vcrsiou of the top-down link gr~tmm~trpa,rser develol)ed independently by (l,Ml:'crty et aJ.,1992).. .
.
.
.
.
~ fz_  .
.
.
.
.
.
.
~ ~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.+ _~.._._ .
.
.
.
.
.
.
,%i.y -  - ....<?
?
?
?
?
?
?~o dI a (loll s}fl!Slm,,) Ji, b(_right subspan)  ',I " ig l l l 'e 5' The  ass,:,mbly of a span c from two sm:LIIcrspaus (a a,nd b) ~tml a cove.ring link.
Only .
is miuimal.probabil ist ic behavior depends on iL~ he.adword(;he lcxicMisL hypoiJmsis titan dilt'erent;ly hc~:uhxla.na.lyses need dilt'erenI; sigmrtures.
There a.re al.lca+sl, k of t,hcsc for a, s/ ibst;rhl,~ Of le..e;IJI k, whenceGe houn(t ,,~ :: t: = ~(u) ,  giving: ;i l, illm COml)lex-it,y of t l ( , s ) .
((~ollins, 19.%)uses t,his t~(,'.
"-')a, lgoril, lml (lireclJy (t,ogel,h('r wil, h l)runiug;).\'\% I)rOl)OSe a,u aJl,ermtl, ive a,I)l)roa.
('h l, ha, I, I)reserves the OOP) hound, hls~ca(t of analyzing sul)st,ri.gs as lcxical t, rees that, will be l inked t, ogoJ,herin(,o la, rgcr h'~xica, I l, rees, t, lic I)arsc, r wil l  ana, lyzeI,hc'ln a,s uon-const,itm'.nt, sl)a:n.s t;haJ, will he coucat;cm~t,ed into larger spans.
A Sl)a,n cousisl;s el'> :~ ;.t.i{.
:e,l<; words; l,;~gs I'or a,ll these words cx(:el)l, possibly the last; ;t list, of all de l .
'mle .cy  \] i ,  ksmuong the words in l, hc Sl)an; and l)erha, l)S s()lueother inl'ornml,ic, n carried a, long in t;lu, siren's sig-naJ, mc.
No cych's, n,ull, iph' l)a, rcnts, or (','ossi,tgliul.
:s are Mlowed in the Sl)a.u, and each Jut,re'halword  of' l, he Sl>ml must ha, vc ~ Ira.rein iu the q);m+Two sl>a, ns at<> illustraJ,ed in I"igure d, 'l'hese di-a,gra.nts a, rc I,yl)ica,l: a, Sl)a,n el" a (Icpendct.
:y l)a+rscmay consist, of oil,her a I)a+rcn(,less endword  andsome o\[' its des(:cn<hmt,s on one side (l"igtu'c 4a),or two parent, less cndwords, wi th a.ll t,he right &"s('(mda, nLs oF(me and all l;hc M'I, dcscen(hml,s of I, Ii(~el, her (lq,e;urc 4b).
'1'tl(.'
im, uilAon is I, haJ, L\]le.
illl,('A'hal part; of a, span is gra, nmmtica l ly  iuert: excel)l,Ior tit(', cmlwords dachsh, u~td mid play, l;hc struclure o1' ea,ch span is irrelewml, I,o t,\]1(; Sl>Cm's al)i l ityt,o cotnbinc iu ful,ure, so sl)a, ns with different inter-1ml strucl, tu'e ca,n colnlmte to bc t;hc I)est,-scoringspan wil, h a, lm,rticula,r signal;urc.117 sl)an a, ctMs on t,he saanc word i l;\[ha, l, st,al'l,sspan b, t,h(;n law I)a,rs(er tries l;o c(>ml>ine I,hc l, wospans I)y cove, red - ( - (mvatcnat ion  (l"igur(; 5).The I,wo Col)ies of word i arc idc.nt, i\[ied, a, fl,erwhich a M'l,waM or r ightwaM cove\]\[ ' ing l ink  isol)l;ionMly added I)ct,wceu t,h(' c.dwor(ts of t,h0.
,.>.vsf)a,n.
Any tlepcudcncy parse ca, n I)c built  Ill:) hyeovered-coitca, tena, i;ion.
When the l)a,rser covcrcd-('O\]lCaJ,enat,cs (~ trod b, it, ol)l, ains up to IJtrce newSlmUS (M't, wa, rd, right,war(I, and no coveritlg \]ink).The <'o',,ered-(:oncaJ,cnal,ion of (+ a.nd b, I'ornfing(', is 1)arrcd unh;ss it, tricots terra, in simple test;s:?
.
must, I)e min ima l  (not ,  itself expressihle ++s aconcaLenal,ion of narrower spaus).
This prcvenLsus from assend>ling c in umlt iple ways.?
Since tim overlapping word will bc int;ertta,l to c,it; Illll81\[, ha, ve ?g parenl; in cxa,(;L\]y oile of a told b.343H Pr(tword(i) I tword(i + 1), tword(i + 2)).
H Pr(i has peels that j satisfies I tword(i), tword(j)) (6)k<_i<g k<i,j<g with i,j linkedH Pr(Lij ItW?rd(i)' tword(j), tag'(next-closest-kid(i))).
H Pr(LiJ ItW?rd(i)' tword(j),...) (7)k<_i,j<g with i,j linked k<i<(, ( j<k  or ~.<j)?
c must not be given a covering link if either theleftmost word of a or the rightmost word of b hasa parent.
(Violating this condition leads to eithermultiple parents or link cycles.
)Any sufficiently wide span whose left endwordhas a parent is a legal parse, rooted at the EOSmark (Figure 1).
Note that a span's signaturemust specify whether its endwords have parents.4 Bottom-Up ProbabilitiesIs this one parser really compatible with all threeprobability models?
Yes, but for each model, wemust provide a way to keep tr~tck of probabilitiesas we parse.
Bear in mind that models A, B, andC do not themselves pecify probabilities for allspans; intrinsically they give only probabilities forsentences.Mode l  C. Define each span's score to be theproduct of all probabilities of links within thespan.
(The link to i from its eth child is asso-ciated with the probability P r ( .
.
. )
in (5).)
Whenspans a and b are combined and one more link isadded, it is easy to compute the resulting span'sscore: score(a), score(b)./?r(covering l ink))When a span constitutes a parse of the wholeinput sentence, its score as just computed provesto be the parse probability, conditional on the treeroot EOS, under model C. The highest-probabilityparse can therefore be built by dynamic program-ming, where we build and retain the highest-scoring span of each signature.Mode l  B.
Taking the Markov process to gen-erate (tag,word) pairs from right to left, we let (6)define the score of a span from word k to word (?.The first product encodes the Markovian proba-bility that the (tag,word) pairs k through g -  1 areas claimed by the span, conditional on the appear-ance of specific (tag,word) pairs at g, ~+1.
~ Again,scores can be easily updated when spans combine,and the probability of a complete parse P, dividedby the total probability of all parses that succeedin satisfying lexical preferences, is just P 's  score.Mode l  A.
Finally, model A is scored the sameas model B, except for the second factor in (6),SThe third factor depends on, e.g., kid(i,c- 1),which we recover fl'om the span signature.
Also, mat-ters are complicated slightly by the probabilities asso-ciated with the generation of STOP.6Different k-g spans have scores conditioned on dif-ferent hypotheses about tag(g) and tag(g + 1); theirsignatures are correspondingly different.
Under modelB, a k-.g span may not combine with an 6-~n spanwhose tags violate its assumptions about g and g + 1.11 A I ~1 c I c' T -  x I~,~o1~  1.o INon-punt  88.9 89.8 89.6 89.
'1 89.8 77.JNouns 90.1 89.8 90.2 90.4 90.0 S(;.2I,ex verbs 74.6 75.9 7.
"/.3 75.8 73.3 67.5'Fable t: Results of preliminary experiments: Per-centage of tokens correctly tagged by each model.which is replaced by the less obvious expression in(7).
As usual, scores can be constructed from thebottom up (though tword(j) in the second factorof (7) is not available to the algorithm, j beingoutside the span, so we back off to word(j)).5 Empirical ComparisonWe have undertaken a careful study to comparethese models' success at generalizing from train-ing data to test data.
Full results on a moderatecorpus of 25,000+ tagged, dependency-annotatedWall Street Journal sentences, discussed in (Eis-ner, 1996), were not complete hi; press time.
How-ever, Tables 1 2 show pilot results for a small setof data drawn from that corpus.
(The full resnltsshow substantially better performance, e.g., 93%correct tags and 87% correct parents fbr model C,but appear qualitatively similar.
)The pilot experiment was conducted on a subsetof 4772 of the sentences comprising 93,a~0 wordsand punctuation marks.
The corpus was derivedby semi-automatic means from the Penn Tree-bank; only sentences without conjunction wereavailable (mean length=20, max=68).
A ran-domly selected set of 400 sentences was set asidefor testing all models; the rest were used to esti-mate the model parameters.
In the pilot (unlikethe full experiment), the parser was instructed to"back oil"' from all probabilities with denomina-tors < 10.
For this reason, the models were insen-sitive to most lexical distinctions.In addition to models A, B, and C, describedabove, the pilot experiment evaluated two othermodels for comparison.
Model C' was a versionof model C that ignored lexical dependencies be-tween parents and children, considering only de-pendencies between a parent's tag and a child'stag.
This model is similar to the model nsed bystochastic CFG.
Model X did the same n-gramtagging as models A and B (~.
= 2 for the prelim-inary experiment, rather than n = 3), but did notassign any links.Tables 1 -2 show the percentage of raw tokensthat were correctly tagged by each model, as wellas the proportion that were correctly attached to344All tokonsNtlll-llllncNOLIn817~1 verbs\[ A t~-  - (' C r -\[ L~5.
.
,~  r 8 .1S~, ,a .~ 47.3 ~l  r~ sA rr.~ I '~  1~ ~ - L 4 0 : , < ~ A _  - ~ ~_'l'~d)le 2: \]{.csults of preli ininary (,Xl)crimcnts: Per.contage of tokens corrc0Lly attached Lo their par-onl;s by each model.their parents.
Per tagging, baseline per\[ol:lnanceWa, S I/leaSlli'ed by assigniug each word ill the testset its most frequent tag (i\[' any) \['roiii the train-lug set.
Thc iinusually low I)aseliue t)crJ'orillanceI:esults \['l'Olll kL conil)iuation of ;t sHiaJl l>ilot Lr;~ill-ing set and ;t Inil(lly (~xten(|e(I t~g set.
7 \Vc ol)served that hi the ka.ining set, detei:lniners n-lostcolrinlonly pointed t.o the following; word, so as aparsing baseline, we linked every test dctcrnihlerto the following word; likewise, wc linked everytest prcpositior, to the preceding word, and so ()11,The l ' Jatter l lS  in the preliuli/lary data ~ti'e strik-ing, with w:rbs showing up as all aFea el (lil\[iculty,alld with SOllle \ ] t lodc i s  cl<;arly farillg bctter I,\[I;tllother.
The siinplcst and \['astest uiodel, the l'(~cur--siw ~, generation uiodel (7, did easily i.he bcsl.
,jobof <'i-q)turing the dependency s/.ructurc ('l'able 2).It misattachcd t.hc fewest words, both overall audin each categol:y.
This suggcsts that sut)eategjorization 1)rcferc\[lccs the only I'~Lctor ('onsideredby model (J I)lay a substantial role in I;he sti:uc-lure of Trcebank scntcn(-cs.
(lndccd, tii(; erl;ors illmodel I~, wliich pe:l:forHled worst across the bO~Lr(l,were very frequently arity erl:ors, where ttie desireof a chihl to ~Ltta(:h LO a 1)articular parent over-.calne the rchi(:i;ail(;e of tile \[)areiit to a(:(-el)t uiorechildren.
)A good deal of the l,arsi0_g Sll(;(',ess of inoclel (7seems to h~ve arisen from its k/iowle(lgc, of individ--tiff.
words, as we cxpe(:ted.
This is showfi by thevastly inl~rior l)Cl;forniaH('e o\[' I;}lc control, model(ft. On l;he ot\]ier hand, I)oth (7 an(l (J' were conl-petitivc with t\[10 oth0r UlOdCiS i~l; tagging.
Thisshows that a t~Lg can 1)e predicted ~d)out as well\['rolri Lhe tags of its putative p;Lrel,t ;rod sil)\]in<gas  it ('an fiX)ill the \[~ags O\[" str ing-a( l ja( :cnt  words,eVell when there is ('onsideral)le /;l:OF ill dcterinin--ing the parent and s\[bling.6 Conc lus ionsI~arc-bories dependency grammar which requires1lO Ihik labels> no ~ral f l i i ia i ' ,  and ItO fll~S tOlirlderstand iS a clean tcstbcd for studying thelexical a\[liniLies of words.
Wc believe filial; thisiS all i l l ,per ,a l l ,  line of ilivcstigative research> ollethat is likely to produce both useful parsing toolsand signilicaut insights ~tboilt language niodeling.7We l lsed distinctive t~tgs for a,uxi\[ia,ry verbs  ;-I, ll(Ifor words being used as noun modifiers (e.g., partici-ples), bec<xuse they ha.ve very ditferent subca.tcgoriz~>lion fra.mes.As a lirst step in the study of lexicM a@n-ity, we asked whether there was a "natural" wayto stochasticize such ~ siint)le formMism a.s de-pendency, hi f~ct, wc have now exhibited threepromising types of lnodel for this simple problem.Further, we have develol)cd a novel parsing algo-r ithm to compare thesc hyt)otheses, with resultstim, so far favor the spe;tker-oriented model C,eveu in written, edited Wall Slrcet dournal I~cxt.To our knowledge, the relative merits of speakeroricn/,cd V(~l'SilS hcarer-orienl,ed probed)ills,it syn-l.iL?
ino(h;Is iiave uoL been investigated l)efore.ll, e fe l 'e l l cesEzra Bla.
(:k, Fred ,lelinck, et a.1.
1992.
Towards history-ba,sed gramnl~u:s: using richer mod(,.ls \[br probabilis-tic i,~trsing.
\[u Fifth I)AI~,FA Worksh.op ou ?
'pecchand Natural Language, Arden (7onfcrcn(:c Ceutcr,llnrrim~m, New York, Febrl,u'y.\['(enne.th W. (3mr(:h. 1988.
A stochastic parts pro-gi:ntll a, nd noun l)hra,se parser for unrestri(:tcd text.In /'roe, of the 2rid (;onf.
on Applied Natural Lan-g'uage lJroccssing, 136 148, Austin, TX.
Asso(:i~Lti(,nfor ('~omput~Ltimml l,inguistics, Morristowu, N.I.Mi(:ha.el ./.
(',ollins.
1996.
A new statistical parserbased on bigr~un lexi(:~fl del)cndeucies, h, l~rocc.cd -iTtfJS of tit(; 24th A CL, S~l, nt~,, (~171'Z, (\]A, July.Ja.sol!
1'3isner.
199(;.
An empirical (:omp~H'ison f prob-~dfility nlodcls for dependeucy gl:a, lnnlaJ:.
Teehnic;dILeport IRCS 96 11, University of PennsylvaJtilt.I!
'red .felinck.
1985.
M~rkov sour(:e modeliug of textgener~Ltiou.
In .I.
Skwirzinski, editor, hnpact of IS"o-tossing 7~chniques ou (;ommunication, /)ordrc(:ht,l"red Jelinek, ,lohn 1).
l,Mferty, aml Robert 1, Mercer.I.?)92.
I\]~si(: niethods el prob~dfilistic context-fre(,.~INI,I"I'IILI7S.
lit ?
'pccch tlccoqnition and U~zdcrstand-ing: l?ecent Advances, Trends, and Applications..I.
Knpie.c.
1!392.
I{obust l)arDof-speech ta.gging us-ing a. hidden Ma, rkov model.
(7omputcr ?
'pccch .ridLanguage, 6..\]ohu t,~Lfferty, I)~ufiel Sle~ttor, ~uid I)~vy '\['cmperley.1992.
(~ramm~LticaJ trigr~mm: A prob~bilistic modelof link gr~mnnar In 15"oc.
of the AAAI  Conf.
ont)robabilistic Approaches to Natural Language, Oct.l);wid M~tgerul~n.
1l!)95.
St~ttisti(:~d decision-treemodels for p~u'sing, in Proceedings of the 33rd An-'nual Meeting of the A CL, l~oston, MA.Igor A, Mel'(:uk.
1988. l)cpcndcncy Syntax: 7?worgand l'racticc.
St~te University of New York Press.IL Meria.hlo.
1990.
Tagging text with ;L probabilisticmodel, lu l~rocccdinw of the IBM Natural Language.17'L, Paris, Fra.nce, pp.
161-172.Yves S(:ha.bes.
L992.
Stochastic lexi(:alized tree-~tdjoining gra.mmars, lit l'rocccdings of C()lHNG'-92, Na.nl.es, I)')'auce, .lnly.I)nniel Sleator and Daxy Tcmperlcy.
1991.
Pro:singI",nglish with ~t I,iuk (h:,~mm~m Te(:hnicifl reportCM U.-('S-91-196.
(iS Dept., C~m,egic Melk)n tl uiv.345
