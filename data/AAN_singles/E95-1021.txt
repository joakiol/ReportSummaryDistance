Tagging F rench-compar ing  a stat ist ical  and a constra int -based methodJean-Pierre Chanod and Pas i  TapanainenRank  Xerox  Research  Cent re ,  Grenob le  Laboratory6, chemin  de Mauper tu is ,  38240 Mey lan ,  F ranceJean.
Pierre.
Chanod, Pasi.
TapanainenOxerox.
frAbst ractIn this paper we compare two compet-ing approaches to part-of-speech tagging,statistical and constraint-based disam-biguation, using French as our test lan-guage.
We imposed a time limit on ourexperiment: the amount of time spenton the design of our constraint systemwas about the same as the time we usedto train and test the easy-to-implementstatistical model.
We describe the twosystems and compare the results.
Theaccuracy of the statistical method is rea-sonably good, comparable to taggers forEnglish.
But the constraint-based taggerseems to be superior even with the lim-ited time we allowed ourselves for ruledevelopment.1 Overv iewIn this paper 1 we compare two competing ap-proaches to part-of-speech tagging, statistical andconstraint-based disambiguation, using French asour test language.
The process of tagging consistsof three stages: tokenisation, morphological nal-ysis and disambiguation.
The two taggers includethe same tokeniser and morphological nalyser.The tokeniser uses a finite-state transducer thatreads the input and outputs a token whenever ithas read far enough to be sure that a token isdetected.
The morphological analYser containsa transducer lexicon.
It produces all the legiti-mate tags for words that appear in the lexicon.If a word is not in the lexicon, a guesser is con-sulted.
The guesser employs another finite-statetransducer.
It reads a token and prints out a setof tags depending on prefixes, inflectional infor-mation and productive ndings that it finds.We make even more use of transducers in theconstraint-based tagger.
The tagger reads onesentence at a time, a string of words and alterna-tive tags, feeds them to the grammatical transduc-1There is a \]onger version (17 pages) of this paperin (Chanod and Tapanainen, 1994)ers that remove all but one alternative tag from allthe words on the basis of contextual information.If all the transducers described above (to-keniser, morphological analyser and disambigua-tot) could be composed together, we would getone single transducer that transforms a raw inputtext to a fully disambiguated output.The statistical method contains the same to-keniser and morphological nalyser.
The disam-biguation method is a conventional one: a hiddenMarkov model.2 Morpho log ica l  ana lys i s  andguess ingThe morphological nalyser is based on a lexicaltransducer (Karttunen et al, 1992).
The trans-ducer maps each inflected surface form of a wordto its canonical exical form followed by the ap-propriate morphological tags.Words not found in the lexicon are analysed bya separate finite-state transducer, the guesser.
Wedeveloped a simple, extremely compact and effi-cient guesser for French.
It is based on the gen-eral assumption that neologisms and uncommonwords tend to follow regular inflectional patterns.The guesser is thus based on productive ndings(like merit for adverbs, ible for adjectives, er forverbs).
A given ending may of course point tovarious categories, e.g.
er identifies nouns as wellas verbs due to possible borrowings from English.3 The  s ta t i s t i ca l  mode lWe use the Xerox part-of-speech tagger (Cuttinget al, 1992), a statistical tagger made at the XeroxPalo Alto Research Center.3.1 TrainingThe Xerox tagger is claimed (Cutting el al., 1992)to be adaptable and easily trained; only a lexiconand suitable amount of untagged text is required.A new language-specific tagger can therefore bebuilt with a minimal amount of work.
We startedour project by doing so.
We took our lexiconwith the new tagset, a corpus of French text, and149trained the tagger.
We ran the tagger on anothertext and counted the errors.
The result was notgood; 13 % of the words were tagged incorrectly.The tagger does not require a tagged corpus fortraining, but two types of biases can be set to tellthe tagger what is correct and what is not: symbolbiases and transition biases.
The symbol biasesdescribe what is likely in a given ambiguity class.They represent kinds of lexical probabilities.
Thetransition biases describe the likelihood of varioustag pairs occurring in succession.
The biases serveas initial values before training.We spent approximately one man-month writ-ing biases and tuning the tagger.
Our training cor-pus was rather small, because the training had tobe repeated frequently.
When it seemed that theresults could not be further improved, we testedthe tagger on a new corpus.
The eventual resultwas that 96.8 % of the words in the corpus weretagged correctly.
This result is about the same asfor statistical tuggers of English.3.2 Mod i fy ing  the biasesA 4 % error rate is not generally considered a neg-ative result for a statistical tagger, but some ofthe errors are serious.
For example, a sequence ofdeterminer.., noun.., noun/verb...preposition is fre-quently disambiguated in the wrong way, e.g.
Le~rain part ~t cinq heures (The ~rain leaves a~ 5o'clock).
The word part is ambiguous between anoun and a verb (singular, third person), and itis disambiguated incorrectly.
The tagger seems toprefer the noun reading between a singular nounand a preposition.One way to resolve this is to write new biases.We added two new ones.
The first one says thata singular noun is not likely to be followed by anoun (this is not always true but we could callthis a tendency).
The second states that a sin-gular noun is likely to be followed by a singular,third-person verb.
The result was that the prob-lematic sentence was disambiguated correctly, butthe changes had a bad side effect.
The overall er-ror rate of the tagger increased by over 50 %.
Thisillustrates how difficult it is to write good biases.Getting a correct result for a particular sentencedoes not necessarily increase the overall successrate.4 The constraint-based model4.1 A two- level  mode l  for  taggingIn the constraint-based tagger, the rules are rep-resented as finite-state transducers.
The trans-ducers are composed with the sentence in a se-quence.
Each transducer may remove, or in prin-ciple it may also change, one or more readings ofthe words.
After all the transducers have beenapplied, each word in the sentence has only oneanalysis.Our constraint-based tagger is based on tech-niques that were originally developed for mor-phological analysis.
The disambiguation rules aresimilar to phonological rewrite rules (Kaplan andKay, 1994), and the parsing algorithm is similarto the algorithm for combining the morphologicalrules with the lexicon (Karttunen, 1994).The tagger has a close relative in (Koskenniemi,1990; Koskenniemi et al, 1992; Voutilalnen andTapanainen, 1993) where the rules are representedas finite-state machines that are conceptually in-tersected with each other.
In this tagger the dis-ambiguation rules are applied in the same man-ner as the morphological rules in (Koskenniemi,1983).
Another relative is represented in (Rocheand Schabes, 1994) which uses a single finite-state transducer to transform one tag into an-other.
A constraint-based system is also presentedin (Karlsson, 1990; Karlsson et al, 1995).
Relatedwork using finite-state machines has been doneusing local grammars (Roche, 1992; Silberztein,1993; Laporte, 1994)'.4.2 Writing the ru les4.2.1 S tudy ing  ambigu i t iesOne quick experiment that motivated the build-ing of the constraint-based model was the follow-ing: we took a million words of newspaper textand ranked ambiguous words by frequency.
Wefound that a very limited set of word forms coversa large part of the total ambiguity.
The 16 mostfrequent ambiguous word forms 2 account for 50 %of all ambiguity.
Two thirds of the ambiguity aredue to the 97 most frequent ambiguous words 3.Another interesting observation is that themost frequent ambiguous words are usuallywords which are in general corpus-independent,i.e.
words that belong to closed classes (determin-ers, prepositions, pronouns, conjunctions), auxil-iaries, common adverbials or common verbs, likefaire (to do, to make).
The first corpus-specificword is in the 41st position.4.2.2 Principled rulesFor the most frequent ambiguous word forms,one may safely define principled contextual re-strictions to resolve ambiguities.
This is in par-ticular the case for clitic/determiner ambiguitiesattached to words like le or la.
Our rule says thatclitic pronouns are attached to a verb and deter-miners to a noun with possibly an unrestrictednumber of premodifiers.
This is a good startingpoint although some ambiguity remains as in la2Namely de, la, le, les, des, en, du, un, a, duns,une, pus, est, plus, Le, son3 A similar experiment shows that in the Brown cor-pus 63 word forms cover 50 % of all the ambiguity, andtwo thirds of the ambiguity is covered by 220 wordforms.150place, which can be read as a determiner-noun orclitic-verb sequence.Some of the very frequent words have categoriesthat are rare, for instance the auxiliary forms aand est can also be nouns and the pronoun celais also a very rare verb form.
In such a case, werestrict the use of the rarest categories to con-texts where the most frequent reading is not atall possible, otherwise the most frequent readingis preferred.
For instance, the word avions may bea noun or an auxiliary verb.
We prefer the nounreading and accept the verb reading only whenthe first-person pronoun nous appears in the leftcor/text, e.g.
as in nous ne les avions pas (we didnot have them).This means that the tagger errs only when arare reading should be chosen in a context wherethe most common reading is still acceptable.
Thismay never actually occur, depending on how accu-rate the contextual restrictions are.
It can even bethe case that discarding the rare readings wouldnot induce a detectable loss in accuracy, e.g.
inthe conflict between cela as a pronoun and as averb.
The latter is a rarely used tense of a ratherliterary verb.The principled rules do not require any taggedcorpus, and should be thus corpus-independent.The rules are based on a short list of extremelycommon words (fewer than 100 words).4.2.3 Heur i s t i csThe rules described above are certainly not suf-ficient to provide full disambiguation, even if oneconsiders only the most ambiguous word forms.We need more rules for cases that the principledrules do not disambiguate.Some ambiguity is extremely difficult to resolveusing the information available.
A very problem-atic case is the word des, which can either be a de-terminer, Jean mange des pommes (Jean eats ap-ples) or an amalgamated preposition-determiner,as in Jean aime le bruit des vagues (Jean likes thesound of waves).Proper treatment of such an ambiguity wouldrequire verb subcategorisation a d a description ofcomplex coordinations of noun and prepositionalphrases.
This goes beyond the scope of both thestatistical and the constraint-based taggers.
Forsuch cases we introduce ad-hoc heuristics.
Someare quite reasonable, e.g.
the determiner readingof des is preferred at the begining of a sentence.Some are more or less arguable, e.g.
the preposi-tional reading is preferred after a noun.One may identify various contexts in which ei-ther the noun or the adjective can be preferred.Such contextual restrictions (Chanod, 1993) arenot always true, but may be considered reason-able for resolving the ambiguity.
For instance, inthe case of two successive noun/adjective ambigu-ities like le franc fort (the strong franc or the frankfort), we favour the noun-adjective sequence x-cept when the first word is a common prenominaladjective such as bon, petit, grand, premier, ... asin le petit fort (the small fort) or even le bon petit(the good little one).4.2.4 Non-contextua l  ru lesOur heuristics do not resolve all the ambigu-ity.
To obtain the fully unambiguous result wemake use of non-contextual heuristics.
The non-contextual rules may be thought of as lexical prob-abilities.
We guess what the most probable tagis in the remaining ambiguities.
For instance,preposition is preferred to adjective, pronoun ispreferred to past participle, etc.
The rules are ob-viously not very reliable, but they are needed onlywhen the previous rules fail to fully disambiguate.4.2.5 Current  ru lesThe current system contains 75 rules, consistingof:?
39 reliable contextual rules dealing mostlywith frequent ambiguous words.?
25 rules describing heuristics with various de-grees of linguistic generality.?
11 non-contextual rules for the remaining am-biguities.The rules were constructed in less than onemonth, on the basis of 50 newspaper sentences.All the rules are currently represented by 11 trans-ducers.5 The results5.1 Test  AFor evaluation, we used a corpus totally unrelatedto the development corpus.
It contains 255 sen-tences (5752 words) randomly selected from a cor-pus of economic reports.
About 54 % of the wordsare ambiguous.
The text is first tagged manuallywithout using the disambiguators, and the outputof the tagger is then compared to the hand-taggedresult.If we apply all the rules, we get a fully disam-biguated result with an error rate of only 1.3 %.This error rate is much lower than the one we getusing the hidden Markov model (3.2 %).
See Fig-ure 1.We can also restrict he tagger to using only themost reliable rules.
Only 10 words lose the cor-rect tag when almost 2000 out of 3085 ambiguouswords are disambiguated.
Among the remaining1136 ambiguous words about 25 % of the ambigu-ity is due to determiner/preposition ambiguities(words like dn and des), 30 % are adjective/nounambiguities and 18 % are noun/verb ambiguities.If we use both the principled and heuristic rules,the error rate is 0.52 % while 423 words remainambiguous.
The non-contextual rules that elim-inate the remaining 423 ambiguities produce an151error rate(correctness)Lexicon + Guesser 0.03 % (99.97 %) 54 %Hidden Markov model 3.2 % (96.8 %) 0 %Principled rules 0.17 % (99.83 %) 20 %Principled and heuristic rules 0.52 % (99.48 %) I 7 %All the rules I 1.3% (98.7 %) I 0%remainingambiguitytag / word1.641.001.241.091.00Figure 1: The result in the test sampleadditional 43 errors.
Overall, 98.7 % of the wordsreceive the correct tag.5.2 Test  BWe also tested the tuggers with more difficult text.The 12 000 word sample of newspaper text hastypos and proper names 4 that match an existingword in the lexicon.
Problems of the latter typeare relatively rare but this sample was exceptional.Altogether the lexicon mismatches produced 0.5 %errors to the input of the tuggers.
The results areshown in Figure 2.
This text also seems to begenerally more difficult to parse than the first one.5.3 Combinat ion  of  the  tuggersWe also tried combining the tuggers, using firstthe rules and then the statistics (a similar ap-proach was also used in (Tapanainen and Vouti-lainen, 1994)).
We evaluated the results obtainedby the following sequence of operations:1) Running the constraint-based tagger withoutthe final, non-contextual rules.2) Using the statistical disambiguator indepen-dently.
We select the tag proposed by thestatistical disambiguator if it is not removedduring step 1.3) Solving the remaining ambiguities by run-ning the final non-contextual rules of theconstraint-based tagger.
This last step en-sures that one gets a fully disambiguatedtext.
Actually only about 0.5 % of words werenot fully disambiguated after step 2.We used the test sample B.
After the first step,1400 words out of 12 000 remain ambiguous.
Theprocess of combining the three steps describedabove eventually leads to more errors than run-ning the constraint-based tagger alone.
The sta-tistical tagger introduces 220 errors on the 1400words that remain ambiguous after step 1.
Incomparison, the final set of non-contextual rulesintroduces around 150 errors on the same set of1400 words.
We did not expect this result.
Onepossible explanation for the superior performanceof the final non-contextual rules is that they aremeant to apply after the previous rules failed todisambiguate the word.
This is in itself useful4like Bats, Botta, Ddrnis, Ferrasse, Hersant, ...information.
The final heuristics favour tags thathave survived all conditions that restrict heir use.For instance, the contextual rules define variouscontexts where the preposition tag for des is pre-ferred.
Therefore, the final heuristics favours thedeterminer reading for des.6 Analysis of er rors6.1 Er rors  of  p r inc ip led  and  heur i s t i crulesLet us now consider what kind of errors the con-straint-based tagger produced.
We do not dealwith errors produced by the last set of rules, thenon-contextual rules, because it is already knownthat they are not very accurate.
To make thetagger better, they should be replaced by writingmore accurate heuristic rules.We divide the errors into three categories: (1)errors due to multi-word expressions, (2) errorsthat should/could be resolved and (3) errors thatare hard to resolve by using the information thatis available.Thef irst  group (15 errors), the multi-word ex-pressions, are difficult for the syntax-based rulesbecause in many cases the expression does not fol-low any conventional syntactic structure, or thestructure may be very rare.
In multi-word expres-sions some words also have categories that maynot appear anywhere lse.
The best way to han-dle them is to lexicalise these expressions.
Whena possible expression is recognised we can eithercollapse it into one unit or leave it otherwise in-tact except that the most "likely" interpretationis marked.The biggest group (41 errors) contains errorsthat could have been resolved correctly but werenot.
The reason for this is obvious: only a rela-tively small amount of time was allowed for writ-ing the rules.
In addition, the rules were con-structed on the basis of a rather small set of ex-ample sentences.
Therefore, it would be very sur-prising if such errors did not appear in the testsample taken from a different source.
The errorsare the following:?
The biggest subgroup has 19 errors that re-quire modifications to existing rules.
Ourrules were meant o handle such cases but fail152 'error rate remaining tag / word(correctness) ambiguityLexicon + Guesser 0.5 % (99.5 %) 48 % 1.59Hidden Markov model 5.0 % (95.0 %) 0 % 1.00Principled rules I 0.8 % (99.2 %) 23 % 1.29Principled and heuristic rules \] 1.3 % (98.7 %) 12 % 1.14All the rules \[ 2.5 % (97.5 %) 0 % 1.00Figure 2: The result in a difficult test sample with many lexicon mismatchesto do so correctly in some sentences.
Oftenonly a minor correction is needed.?
Some syntactic constructions, or word se-quences, were omitted.
This caused 7 er-rors which could easily be avoided by writ-ing more rules.
For instance, a constructionlike "preposition ?
clitic + finite verb" wasnot forbidden.
The phrase h l'est was anal-ysed in this way while the correct analysis is"preposition ?
determiner + noun".?
Sometimes a little bit of extra lexical infor-mation is required.
Six errors would requiremore information or the kind of refinement inthe tag inventory that would not have beenappropriate for the statistical tagger.?
Nine errors could be avoided by refining ex-isting heuristics, especially by taking into ac-count exceptions for specific words like point,pendant and devant.The remaining errors (28 errors) constitute theprice we pay for using the heuristics.
Removingthe rules which fail would cause a lot of ambiguityto remain.
The errors are the following:?
Fifteen errors are due to the heuristics for deand des.
There is little room for improvementat this level of description (see Chapter 4.2.3).However, the current, simple heuristics fullydisambiguate 850 instances of de and des outof 914 i.e.
92 % of all the occurrences wereparsed with less than a 2 % error rate.?
Six errors involve noun-adjective ambiguitiesthat are difficult to solve, for instance, in asubject or object predicate position.?
Seven errors seem to be beyond reach forvarious reasons: long coordination , rare con-structions, etc.
An example is les boltes (theboxes) where les is wrongly tagged in the testsample because the noun form is misspelledas boites, which is identified only as a verb bythe lexicon.6.2 D i f ference between the  taggersWe also investigated how the errors compare be-tween the two taggers.
Here we used the fullydisambiguated outputs of the taggers.
The errorsbelong mainly to three classes:* Some errors appear predominantly with thestatistical tagger and almost never with theconstraint-based tagger.
This is particularlythe case with the ambiguity between past par-ticiples and adjectives.?
Some errors are common to both taggers, theconstraint-based tagger generally being moreaccurate (often with a ratio of I to 2).
Theseerrors cover ambiguities that are known to bedifficult to handle in general, such as the al-ready mentioned eterminer/preposition am-biguity.?
Finally, there are errors that are specific tothe constraint-based tagger.
They are of-ten related to errors that could be correctedwith some extra work.
They are relativelyinfrequent, thus the global accuracy of theconstraint-based tagger emains higher.The first two classes of errors are generally dif-ficult to correct.
The easiest way to improve theconstraint-based tagger is to concentrate on thefinal class.
As we mentioned earlier, it is notvery easy to change the behaviour of the statisticaltagger in one place without some side-effects else-where.
This means that the errors of the first classare probably easiest to resolve by means otherthan statistics.The first class is quite annoying for the statisti-cal parser because it contains errors that are intu-itively very clear and resolvable, but which are farbeyond the limits of the current statistical tagger.We can take an easy sentence to demonstrate his:Je ne le pense pas.
I do not think so.Tune le penses pas.
You do not think so.Il ne le pense pas.
He does not think so.The verb pense is ambiguous 5 in the first person orin the third person.
It is usually easy to determinethe person just by checking the personal pronounnearby.
For a human or a constraint-based taggerthis is an easy task, for a statistical tagger it is not.There are two words between the pronoun and theverb that do not carry any information about theperson.
The personal pronoun may thus be toofar from the verb because bi-gram models can seebackward no farther than le, and tri-gram modelsSThat is not case with all the French verbs, e.g.
Jecrois and //croit.153no farther than ne le.Also, as mentioned earlier, resolving the adjec-tive vs. past participle ambiguity is much harder,if the tagger does not know whether there is anauxiliary verb in the sentence or not.7 Conc lus ionWe have presented two taggers for french: a sta-tistical one and a constraint-based one.There are two ways to train the statisticaltagger: from a tagged corpus or using a self-organising method that does not need a taggedcorpus.
We had a strict time limit of one monthfor doing the tagger and no tagged corpus wasavailable.
This is a short time for the manual tag-ging of a corpus and for the training of the tag-ger.
It would be risky to spend, say, three weeksfor writing a corpus, and only one week for train-ing.
The size of corpus would have to be limited,because it should be also checked.We selected the Xerox tagger that learns froman untagged corpuS.
The task was not as straigth-forward as we thought.
Without human assistancein the training the result was not impressive, andwe had to spend much time tuning the taggerand guiding the learning process.
In a month weachieved 95-97 % accuracy.The training process of a statistical tagger re-quires some time because the linguistic informa-tion has to be incorporated into the tagger oneway or another, it cannot be obtained for freestarting from null.
Because the linguistic infor-mation is needed, we decided to encode the infor-mation in a more straightforward way, as explicitlinguistic disambiguation rules.
It has been ar-gued that statistical taggers are superior to rule-based/hand-coded ones because of better accu-racy and better adaptabil ity (easy to train).
Inour experiment, both claims turned out to bewrong.For the constraint-based tagger we set onemonth time limit for writing the constraints byhand.
We used only linguistic intuition and a verylimited set of sentences to write the 75 constraints.We formulated constraints of different accuracy.Some of the constraints are almost 100 % accu-rate, some of them just describe tendencies.Finally, when we thought that the rules weregood enough, we took two text samples from dif-ferent sources and tested both the taggers.
Theconstraint-based tagger made several naive errorsbecause we had forgotten, miscoded or ignoredsome linguistic phenomena, but still, it made onlyhalf of the errors that the statistical one made.A big difference between the taggers is that thetuning of the statistical tagger is very subtle i.e.
itis hard to predict the effect of tuning the param-eters of the system, whereas the constraint-basedtagger is very straightforward to correct.Our general conclusion is that the hand-codedconstraints perform better than the statistical tag-ger and that we can still refine them.
The mostimportant of our findings is that writing con-straints that contain more linguistic informationthan the current statisticM model does not takemuch time.ReferencesJean-Pierre Chanod.
Probl~mes de robustesse nanalyse syntaxique.
In Acres de la conf6renceInformatique t langue n alurelle.
IRIN, Univer-sit@ de Nantes, 1993.Jean-Pierre Chanod and Past Tapanainen.
Statis-tical and Constraint-based Taggers for French.Technical report MLTT-016, Rank Xerox Re-search Centre, Grenoble, 1994.Doug Cutting, Julian Kupiec, Jan Pedersen andPenelope Sibun.
A Practical Part-of-SpeechTagger.
In Third Conference on Applied Natu-ral Language Processing.
pages 133-140.
Trento,1992.Ron Kaplan and Martin Kay.
Regular Modelsof Phonological Rule Systems.
ComputationalLinguistics Vol.
20, Number 3, pages 331-378.Fred Karlsson.
Constraint Grammar as a Frame-work for Parsing Running Text.
In proceedingsof Coling-90.
Papers presented to the 13th In-ternational Conference on Computational Lin-guistics.
Vol.
3, pages 168-173.
Helsinki, 1990.Fred Karlsson, Atro Voutilainen, Juha Reikkil~and Arto Antti la (eds.).
Constraint Grammar:a Language-Independent System for ParsingUnrestricted Text.
Mouton de Gruyter, Berlin,1995.Lauri Karttunen.
Constructing LexicM Trans-ducers.
In proceedings of Coling-94.
The fif-leenth International Conference on Computa-tional Linguistics.
Vol I, pages 406-411.
Kyoto,1994.Lauri Karttunen, Ron Kaplan and Annie Zae-nen.
Two-level morphology with composition.In proceedings of Colin9-92.
The fourteenth In-ternational Conference on Computational Lin-guistics.
Vol I, pages 141-148.
Nantes, 1992.Kimmo Koskenniemi.
Two-level morphology.A general computational model for word-formrecognition and production.
University ofHelsinki, 1983.Kimmo Koskenniemi.
Finite-state parsing anddisambiguation.
In proceedings of Coling-90.Papers presented to the 131h International Con-ference on Computational Linguistics.
Vol.
2,pages 229-232.
Helsinki, 1990.Kimmo Koskenniemi, Past Tapanainen and AtroVoutilainen.
Compiling and using finite-state154syntactic rules.
In proceedings of Coling-92.The fourteenth International Conference onComputational Linguistics.
Vol.
I, pages 156-162.
Nantes, 1992.Eric Laporte.
Experiences in Lexical Disambigua-tion Using Local Grammars.
In Third Interna-tional Conference on Computational Lexicogra-phy.
pages 163-172.
Budapest, 1994.Emmanuel Roche.
Text Disambiguation byfinite-state automata, n algorithm and experimentson corpora.
In proceedings of Coling-92.
Thefourteenth International Conference on Com-putational Linguistics.
Vol III, pages 993-997.Nantes, 1992.Emmanuel Roche and Yves Schabes.
Deter-ministic part-of-speech tagging with finite-statetransducers.
Technical report TR-94-07, Mit-subishi Electric Research Laboratories, Cam-bridge, USA.Max Silberztein.
Dictionnaires ~lectroniques etanalyse automatique de textes.
Le systkme IN-TEX.
Masson, Paris, 1993.Pasi Tapanainen and Atro Voutilainen.
Taggingaccurately - Don't guess if you know.
In Fourth.Conference on Applied Natural Language Pro-cessing, pages 47-52.
Stuttgart, 1994.Atro Voutilainen and Pasi Tapanainen.
Ambi-guity resolution in a reductionistic parser.
InSixth Conference of the European Chapter of theACL.
pages 394-403.
Utrecht, 1993.A The restricted tag setIn this appendix the tag set is represented.
Be-sides the following tags, there may also be someword-specific tags like PREP-DE, which is thepreposition reading for words de, des and du,i.e.
word de is initially ambiguous between PREP-DE and PC.
This information is mainly forthe statistical tagger to deal with, for instance,different prepositions in a different way.
Theconstraint-based tagger does not need this becauseit has direct access to word forms anyway.
Af-ter disambiguation, the word-specific tags may becleaned.
The tag PREP-DE is changed back intoPREP, to reduce the redundant information.?
DET-SG: Singular determiner e.g.
le, la,mon, ma.
This covers masculine as well asfeminine forms.
Sample sentence: L_ee chiendort dans l__a cuisine.
(The dog is sleeping inthe kitchen).?
DET-PL  Plural determiner e.g.
les, mes.This covers masculine as well as feminineforms.
Sample sentence: Les enfants jouentavec mes livres.
(The children are playingwith my books.)?
ADJ - INV Adjective invariant in numbere.g.
heureux.
Sample sentence: Le chien estheureux quand les enfan'ts ont heureux.
(Thedog is happy when the children are happy.)?
ADJ-SG Singular adjective .g.
gentil, gen-tille.
This covers masculine as well as fem-inine forms.
Sample sentence: Le chien estgentil.
(The dog is nice.)?
ADJ -PL  Plural adjective .g.
gentils, gen-tilles.
This covers masculine as well as femi-nine forms.
Sample sentence: Ces chiens sontgentils.
(These dogs are nice.)?
NOUN-INV Noun invariant in number e.g.souris, Frangais.
This covers masculine aswell as feminine forms.
Sample sentence: Lessouris dansent.
(The mice are dancing.)?
NOUN-SG Singular noun e.g.
chien, fleur.This covers masculine as well as feminineforms.
Sample sentence: C'est une jolie fleur.
(It is a nice flower.)?
NOUN-PL Plural noun e.g.
chiens, fleurs.This covers masculine as well as feminineforms.
Sample sentence: Nous aimons lesfleurs.
(We like flowers.)?
VAUX-INF Auxiliary verb, infinitive ~tre,avoir.
Sample sentence: Le chien vient d'Etrepuni.
(The dog has just been punished.)?
VAUX-PRP Auxiliary verb, present par-ticiple grant, ayant.?
VAUX-PAP Auxiliary verb, past participlee.g.
dtd, eu.
Sample sentence: Le thdor~mea ~t__d dmontrd.
(The theorem has beenproved.)?
VAUX-P1P2 Auxiliary verb, covers any 1stor 2nd person form, regardless of number,tense or mood, e.g.
1st person singularpresent indicative, 2nd person plural impera-tive: ai, soyons, es.
Sample sentence: Tu e_ssfort.
(You are strong.)?
VAUX-P3SG Auxiliary verb, covers any3rd person singular form e.g.
avait, sera,es.
Sample sentence: Elle es._tt forte.
(She isstrong.)?
VAUX-P3PL Auxiliary verb, covers any3rd person plural form e.g.
ont, seront,avaient.
Sample sentence: Elles avaientdormi.
(They had slept.)?
VERB- INF Infinitive verb e.g.
danser,finir, dormir.
Sample sentence: Le chienaime dormir.
(The dog enjoys leeping.)?
VERB-PRP  Present participlee.g.
dansant, finissant, aboyant.
Sample sen-tence: Le chien arrive en aboyant.
(The dogis coming and it is barking.)?
VERB-P IP2  Any 1st or 2nd person verbform, regardless of number, tense or moode.g.
1st person singular present indicative,1552nd pers plural imperative: chante, finissons.Sample sentence: Je chante.
(I sing.)?
VERB-P3SG Any 3rd person singular verbform e.g.
chanlera, finil, aboie.
Sample sen-tence: ge chien aboie.
(The dog is barking.)?
VERB-P3PL  Any 3rd person plural verbform e.g.
chanleront, finissen$, aboient.
Sam-ple sentence: Les chiens aboient.
(The dogsare barking.)?
PAP- INV Past participle invariant in num-ber e.g.
surpris.
Sample sentence: Le chienm'a surpris.
(The dog surprised me.)?
PAP-SG Singular past participle e.g.
fini,finie.
This covers masculine as well as femi-nine forms.
Sample sentence: La journge estfinie.
(The day is over.)?
PAP-PL  Plural past participle e.g.
finis,finies.
This covers masculine as well as fem-inine forms.
Sample sentence: Les travauxsont finis.
(The work is finished.)?
PC  Non-nominative clitic pronoun such asme, le.
Sample sentence: It me l'a donnL(He gave it to me.)?
PRON 3rd person pronoun, relative pro-nouns excluded, e.g.
il, elles, chacun.
Samplesentence: I__l a parle ~ chacun.
(He spoke toevery person.)?
PRON-P1P2 1st or 2nd person pronoune.g.
je, ?u, nous.
Sample sentence: Est-ceque t_uu viendras avec moi?
(Will you comewith me?)?
VOIC ILA  Reserved for words voici andvoile.
Sample sentence: Voici mon chien.
(Here is my dog.)?
ADV Adverbs e.g.
finalement.
Sample sen-tence: Le jour es?
finalement arrivd.
(Theday has finally come.)?
NEG Negation particle.
Reserved for theword ne.
Sample sentence: Le chien n_~e dor?pas.
(The dog is not sleeping.)?
PREP  Preposition e.g.
dans.
Sample sen-tence: Le chien dor?
dans la cuisine.
(Thedog sleeps in the kitchen.
)For statistical taggers this group may be di-vided into subgroups for different prepositiongroups, like PREP-DE, PREP-A, etc.?
CONN Connector.
This class includes coor-dinating conjuctions uch as el, subordinateconjunctions uch as lorsque, relative or in-terrogative pronouns such as lequel.
Wordslike comme or que which have very specialbehaviour are not coded as CONN. Samplesentence: Le chien e___t le chat dorment quandil pleut.
(The dog and the cat sleep when itrains.
)For statistical taggers this group may be di-vided into subgroups for different connectors,like CONN-ET, CONN-Q, etc.?
COMME Reserved for M1 instances of theword comme.
Sample sentence: Il jouecomme un enfant.
(He plays like a child.)?
CONJQUE Reserved for all instances of theword que.?
NUM Numeral e.g.
12,7, 120/98, 34+0.7.?
HEURE String representing time e.g.12h24, 12:45:00.M ISC Miscellaneous words, such as: inter-jectiorr oh, salutation bonjour, onomatopoeiamiaou, wordparts i.e.
words that only existas part of a multi-word expression, such aspriori, as part of a priori.?
CM Comma.?
PUNCT Punctuation other than comma.156
