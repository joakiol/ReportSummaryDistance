Shallow Parsing with Conditional Random FieldsFei Sha and Fernando PereiraDepartment of Computer and Information ScienceUniversity of Pennsylvania200 South 33rd Street, Philadelphia, PA 19104(feisha|pereira)@cis.upenn.eduAbstractConditional random fields for sequence label-ing offer advantages over both generative mod-els like HMMs and classifiers applied at eachsequence position.
Among sequence labelingtasks in language processing, shallow parsinghas received much attention, with the devel-opment of standard evaluation datasets and ex-tensive comparison among methods.
We showhere how to train a conditional random field toachieve performance as good as any reportedbase noun-phrase chunking method on theCoNLL task, and better than any reported sin-gle model.
Improved training methods basedon modern optimization algorithms were crit-ical in achieving these results.
We present ex-tensive comparisons between models and train-ing methods that confirm and strengthen pre-vious results on shallow parsing and trainingmethods for maximum-entropy models.1 IntroductionSequence analysis tasks in language and biology are of-ten described as mappings from input sequences to se-quences of labels encoding the analysis.
In language pro-cessing, examples of such tasks include part-of-speechtagging, named-entity recognition, and the task we shallfocus on here, shallow parsing.
Shallow parsing iden-tifies the non-recursive cores of various phrase types intext, possibly as a precursor to full parsing or informa-tion extraction (Abney, 1991).
The paradigmatic shallow-parsing problem is NP chunking, which finds the non-recursive cores of noun phrases called base NPs.
Thepioneering work of Ramshaw and Marcus (1995) in-troduced NP chunking as a machine-learning problem,with standard datasets and evaluation metrics.
The taskwas extended to additional phrase types for the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000),which is now the standard evaluation task for shallowparsing.Most previous work used two main machine-learningapproaches to sequence labeling.
The first approach re-lies on k-order generative probabilistic models of pairedinput sequences and label sequences, for instance hiddenMarkov models (HMMs) (Freitag and McCallum, 2000;Kupiec, 1992) or multilevel Markov models (Bikel et al,1999).
The second approach views the sequence labelingproblem as a sequence of classification problems, one foreach of the labels in the sequence.
The classification re-sult at each position may depend on the whole input andon the previous k classifications.
1The generative approach provides well-understoodtraining and decoding algorithms for HMMs and moregeneral graphical models.
However, effective genera-tive models require stringent conditional independenceassumptions.
For instance, it is not practical to make thelabel at a given position depend on a window on the in-put sequence as well as the surrounding labels, since theinference problem for the corresponding graphical modelwould be intractable.
Non-independent features of theinputs, such as capitalization, suffixes, and surroundingwords, are important in dealing with words unseen intraining, but they are difficult to represent in generativemodels.The sequential classification approach can handlemany correlated features, as demonstrated in work onmaximum-entropy (McCallum et al, 2000; Ratnaparkhi,1996) and a variety of other linear classifiers, includingwinnow (Punyakanok and Roth, 2001), AdaBoost (Ab-ney et al, 1999), and support-vector machines (Kudo andMatsumoto, 2001).
Furthermore, they are trained to min-imize some function related to labeling error, leading tosmaller error in practice if enough training data are avail-able.
In contrast, generative models are trained to max-imize the joint probability of the training data, which is1Ramshaw and Marcus (1995) used transformation-basedlearning (Brill, 1995), which for the present purposes can betought of as a classification-based method.Edmonton, May-June 2003Main Papers , pp.
134-141Proceedings of HLT-NAACL 2003not as closely tied to the accuracy metrics of interest if theactual data was not generated by the model, as is alwaysthe case in practice.However, since sequential classifiers are trained tomake the best local decision, unlike generative mod-els they cannot trade off decisions at different positionsagainst each other.
In other words, sequential classifiersare myopic about the impact of their current decisionon later decisions (Bottou, 1991; Lafferty et al, 2001).This forced the best sequential classifier systems to re-sort to heuristic combinations of forward-moving andbackward-moving sequential classifiers (Kudo and Mat-sumoto, 2001).Conditional random fields (CRFs) bring together thebest of generative and classification models.
Like classi-fication models, they can accommodate many statisticallycorrelated features of the inputs, and they are trained dis-criminatively.
But like generative models, they can tradeoff decisions at different sequence positions to obtain aglobally optimal labeling.
Lafferty et al (2001) showedthat CRFs beat related classification models as well asHMMs on synthetic data and on a part-of-speech taggingtask.In the present work, we show that CRFs beat all re-ported single-model NP chunking results on the standardevaluation dataset, and are statistically indistinguishablefrom the previous best performer, a voting arrangement of24 forward- and backward-looking support-vector clas-sifiers (Kudo and Matsumoto, 2001).
To obtain theseresults, we had to abandon the original iterative scal-ing CRF training algorithm for convex optimization al-gorithms with better convergence properties.
We providedetailed comparisons between training methods.The generalized perceptron proposed by Collins(2002) is closely related to CRFs, but the best CRF train-ing methods seem to have a slight edge over the general-ized perceptron.2 Conditional Random FieldsWe focus here on conditional random fields on sequences,although the notion can be used more generally (Laf-ferty et al, 2001; Taskar et al, 2002).
Such CRFs defineconditional probability distributions p(Y |X) of label se-quences given input sequences.
We assume that the ran-dom variable sequences X and Y have the same length,and use x = x1 ?
?
?xn and y = y1 ?
?
?
yn for the genericinput sequence and label sequence, respectively.A CRF on (X, Y ) is specified by a vector f of localfeatures and a corresponding weight vector ?.
Each localfeature is either a state feature s(y, x, i) or a transitionfeature t(y, y?, x, i), where y, y?
are labels, x an inputsequence, and i an input position.
To make the notationmore uniform, we also writes(y, y?, x, i) = s(y?, x, i)s(y, x, i) = s(yi, x, i)t(y, x, i) ={t(yi?1, yi, x, i) i > 10 i = 1for any state feature s and transition feature t. Typically,features depend on the inputs around the given position,although they may also depend on global properties of theinput, or be non-zero only at some positions, for instancefeatures that pick out the first or last labels.The CRF?s global feature vector for input sequence xand label sequence y is given byF (y, x) =?if(y, x, i)where i ranges over input positions.
The conditionalprobability distribution defined by the CRF is thenp?
(Y |X) =exp ?
?
F (Y , X)Z?(X)(1)whereZ?
(x) =?yexp ?
?
F (y, x)Any positive conditional distribution p(Y |X) that obeysthe Markov propertyp(Yi|{Yj}j 6=i, X) = p(Yi|Yi?1, Yi+1, X)can be written in the form (1) for appropriate choice offeature functions and weight vector (Hammersley andClifford, 1971).The most probable label sequence for input sequencex isy?
= argmaxyp?
(y|x) = argmaxy?
?
F (y, x)because Z?
(x) does not depend on y. F (y, x) decom-poses into a sum of terms for consecutive pairs of labels,so the most likely y can be found with the Viterbi algo-rithm.We train a CRF by maximizing the log-likelihood of agiven training set T = {(xk, yk)}Nk=1, which we assumefixed for the rest of this section:L?
=?k log p?
(yk|xk)=?k [?
?
F (yk, xk) ?
log Z?
(xk)]To perform this optimization, we seek the zero of the gra-dient?L?
=?k[F (yk, xk) ?
Ep?
(Y |xk)F (Y , xk)] (2)In words, the maximum of the training data likelihoodis reached when the empirical average of the global fea-ture vector equals its model expectation.
The expectationEp?
(Y |x)F (Y , x) can be computed efficiently using avariant of the forward-backward algorithm.
For a givenx, define the transition matrix for position i asMi[y, y?]
= exp ?
?
f(y, y?, x, i)Let f be any local feature, fi[y, y?]
= f(y, y?, x, i),F (y, x) =?i f(yi?1, yi, x, i), and let ?
denotecomponent-wise matrix product.
ThenEp?
(Y |x)F (Y , x) =?yp?
(y|x)F (y, x)=?i?i?1(fi ?
Mi)?>iZ?(x)Z?
(x) = ?n ?
1>where ?i and ?i the forward and backward state-costvectors defined by?i ={?i?1Mi 0 < i ?
n1 i = 0?>i ={Mi+1?>i+1 1 ?
i < n1 i = nTherefore, we can use a forward pass to compute the ?iand a backward bass to compute the ?i and accumulatethe feature expectations.To avoid overfitting, we penalize the likelihood witha spherical Gaussian weight prior (Chen and Rosenfeld,1999):L??
=?k[?
?
F (yk, xk) ?
log Z?(xk)]?
??
?22?2 + constwith gradient?L??
=?k[F (yk, xk) ?
Ep?
(Y |xk)F (Y , xk)]?
?
?23 Training MethodsLafferty et al (2001) used iterative scaling algorithmsfor CRF training, following earlier work on maximum-entropy models for natural language (Berger et al, 1996;Della Pietra et al, 1997).
Those methods are very sim-ple and guaranteed to converge, but as Minka (2001) andMalouf (2002) showed for classification, their conver-gence is much slower than that of general-purpose convexoptimization algorithms when many correlated featuresare involved.
Concurrently with the present work, Wal-lach (2002) tested conjugate gradient and second-ordermethods for CRF training, showing significant trainingspeed advantages over iterative scaling on a small shal-low parsing problem.
Our work shows that precon-ditioned conjugate-gradient (CG) (Shewchuk, 1994) orlimited-memory quasi-Newton (L-BFGS) (Nocedal andWright, 1999) perform comparably on very large prob-lems (around 3.8 million features).
We compare thosealgorithms to generalized iterative scaling (GIS) (Dar-roch and Ratcliff, 1972), non-preconditioned CG, andvoted perceptron training (Collins, 2002).
All algorithmsexcept voted perceptron maximize the penalized log-likelihood: ??
= argmax?
L??.
However, for ease ofexposition, this discussion of training methods uses theunpenalized log-likelihood L?.3.1 Preconditioned Conjugate GradientConjugate-gradient (CG) methods have been shown tobe very effective in linear and non-linear optimization(Shewchuk, 1994).
Instead of searching along the gra-dient, conjugate gradient searches along a carefully cho-sen linear combination of the gradient and the previoussearch direction.CG methods can be accelerated by linearly trans-forming the variables with preconditioner (Nocedal andWright, 1999; Shewchuk, 1994).
The purpose of the pre-conditioner is to improve the condition number of thequadratic form that locally approximates the objectivefunction, so the inverse of Hessian is reasonable precon-ditioner.
However, this is not applicable to CRFs for tworeasons.
First, the size of the Hessian is dim(?
)2, lead-ing to unacceptable space and time requirements for theinversion.
In such situations, it is common to use insteadthe (inverse of) the diagonal of the Hessian.
However inour case the Hessian has the formH?
def= ?2L?= ?
?k{E [F (Y , xk) ?
F (Y , xk)]?EF (Y , xk) ?
EF (Y , xk)}where the expectations are taken with respect top?
(Y |xk).
Therefore, every Hessian element, includ-ing the diagonal ones, involve the expectation of a prod-uct of global feature values.
Unfortunately, computingthose expectations is quadratic on sequence length, as theforward-backward algorithm can only compute expecta-tions of quantities that are additive along label sequences.We solve both problems by discarding the off-diagonalterms and approximating expectation of the square of aglobal feature by the expectation of the sum of squares ofthe corresponding local features at each position.
The ap-proximated diagonal term Hf for feature f has the formHf = Ef(Y , xk)2??i??
?y,y?Mi[y, y?]Z?
(x)f(Y , xk)?
?2If this approximation is semidefinite, which is trivial tocheck, its inverse is an excellent preconditioner for earlyiterations of CG training.
However, when the model isclose to the maximum, the approximation becomes un-stable, which is not surprising since it is based on fea-ture independence assumptions that become invalid asthe weights of interaction features move away from zero.Therefore, we disable the preconditioner after a certainnumber of iterations, determined from held-out data.
Wecall this strategy mixed CG training.3.2 Limited-Memory Quasi-NewtonNewton methods for nonlinear optimization use second-order (curvature) information to find search directions.As discussed in the previous section, it is not practi-cal to obtain exact curvature information for CRF train-ing.
Limited-memory BFGS (L-BFGS) is a second-ordermethod that estimates the curvature numerically fromprevious gradients and updates, avoiding the need foran exact Hessian inverse computation.
Compared withpreconditioned CG, L-BFGS can also handle large-scaleproblems but does not require a specialized Hessian ap-proximations.
An earlier study indicates that L-BFGSperforms well in maximum-entropy classifier training(Malouf, 2002).There is no theoretical guidance on how much infor-mation from previous steps we should keep to obtainsufficiently accurate curvature estimates.
In our exper-iments, storing 3 to 10 pairs of previous gradients andupdates worked well, so the extra memory required overpreconditioned CG was modest.
A more detailed descrip-tion of this method can be found elsewhere (Nocedal andWright, 1999).3.3 Voted PerceptronUnlike other methods discussed so far, voted perceptrontraining (Collins, 2002) attempts to minimize the differ-ence between the global feature vector for a training in-stance and the same feature vector for the best-scoringlabeling of that instance according to the current model.More precisely, for each training instance the methodcomputes a weight update?t+1 = ?t + F (yk, xk) ?
F (y?k, xk) (3)in which y?k is the Viterbi pathy?k = argmaxy?t ?
F (y,xk)Like the familiar perceptron algorithm, this algorithm re-peatedly sweeps over the training instances, updating theweight vector as it considers each instance.
Instead oftaking just the final weight vector, the voted perceptronalgorithm takes the average of the ?t.
Collins (2002) re-ported and we confirmed that this averaging reduces over-fitting considerably.4 Shallow ParsingFigure 1 shows the base NPs in an example sentence.
Fol-lowing Ramshaw and Marcus (1995), the input to theNP chunker consists of the words in a sentence anno-tated automatically with part-of-speech (POS) tags.
Thechunker?s task is to label each word with a label indi-cating whether the word is outside a chunk (O), startsa chunk (B), or continues a chunk (I).
For example,the tokens in first line of Figure 1 would be labeledBIIBIIOBOBIIO.4.1 Data PreparationNP chunking results have been reported on two slightlydifferent data sets: the original RM data set of Ramshawand Marcus (1995), and the modified CoNLL-2000 ver-sion of Tjong Kim Sang and Buchholz (2000).
Althoughthe chunk tags in the RM and CoNLL-2000 are somewhatdifferent, we found no significant accuracy differencesbetween models trained on these two data sets.
There-fore, all our results are reported on the CoNLL-2000 dataset.
We also used a development test set, provided byMichael Collins, derived from WSJ section 21 taggedwith the Brill (1995) POS tagger.4.2 CRFs for Shallow ParsingOur chunking CRFs have a second-order Markov depen-dency between chunk tags.
This is easily encoded bymaking the CRF labels pairs of consecutive chunk tags.That is, the label at position i is yi = ci?1ci, where ci isthe chunk tag of word i, one of O, B, or I.
Since Bmust beused to start a chunk, the label OI is impossible.
In addi-tion, successive labels are constrained: yi?1 = ci?2ci?1,yi = ci?1ci, and c0 = O.
These contraints on the modeltopology are enforced by giving appropriate features aweight of ?
?, forcing all the forbidden labelings to havezero probability.Our choice of features was mainly governed by com-puting power, since we do not use feature selection andall features are used in training and testing.
We use thefollowing factored representation for featuresf(yi?1, yi, x, i) = p(x, i)q(yi?1, yi) (4)where p(x, i) is a predicate on the input sequence x andcurrent position i and q(yi?1, yi) is a predicate on pairsof labels.
For instance, p(x, i) might be ?word at posi-tion i is the?
or ?the POS tags at positions i ?
1, i areRockwell International Corp. ?s Tulsa unit said it signed a tentative agreement extendingits contract with Boeing Co. to provide structural parts for Boeing ?s 747 jetliners .Figure 1: NP chunksq(yi?1, yi) p(x, i)yi = y trueyi = y, yi?1 = y?c(yi) = cyi = y wi = wor wi?1 = wc(yi) = c wi+1 = wwi?2 = wwi+2 = wwi?1 = w?, wi = wwi+1 = w?, wi = wti = tti?1 = tti+1 = tti?2 = tti+2 = tti?1 = t?, ti = tti?2 = t?, ti?1 = tti = t?, ti+1 = tti+1 = t?, ti+2 = tti?2 = t?
?, ti?1 = t?, ti = tti?1 = t?
?, ti = t?, ti+1 = tti = t?
?, ti+1 = t?, ti+2 = tTable 1: Shallow parsing featuresDT, NN.?
Because the label set is finite, such a factoringof f(yi?1, yi, x, i) is always possible, and it allows eachinput predicate to be evaluated just once for many fea-tures that use it, making it possible to work with millionsof features on large training sets.Table 1 summarizes the feature set.
For a given po-sition i, wi is the word, ti its POS tag, and yi its label.For any label y = c?c, c(y) = c is the correspondingchunk tag.
For example, c(OB) = B.
The use of chunktags as well as labels provides a form of backoff fromthe very small feature counts that may arise in a second-order model, while allowing significant associations be-tween tag pairs and input predicates to be modeled.
Tosave time in some of our experiments, we used only the820,000 features that are supported in the CoNLL train-ing set, that is, the features that are on at least once.
Forour highest F score, we used the complete feature set,around 3.8 million in the CoNLL training set, which con-tains all the features whose predicate is on at least once inthe training set.
The complete feature set may in princi-ple perform better because it can place negative weightson transitions that should be discouraged if a given pred-icate is on.4.3 Parameter TuningAs discussed previously, we need a Gaussian weight priorto reduce overfitting.
We also need to choose the num-ber of training iterations since we found that the best Fscore is attained while the log-likelihood is still improv-ing.
The reasons for this are not clear, but the Gaussianprior may not be enough to keep the optimization frommaking weight adjustments that slighly improve traininglog-likelihood but cause large F score fluctuations.
Weused the development test set mentioned in Section 4.1 toset the prior and the number of iterations.4.4 Evaluation MetricThe standard evaluation metrics for a chunker are preci-sion P (fraction of output chunks that exactly match thereference chunks), recall R (fraction of reference chunksreturned by the chunker), and their harmonic mean, theF1 score F1 = 2 ?
P ?
R/(P + R) (which we call justF score in what follows).
The relationships between Fscore and labeling error or log-likelihood are not direct,so we report both F score and the other metrics for themodels we tested.
For comparisons with other reportedresults we use F score.4.5 Significance TestsIdeally, comparisons among chunkers would control forfeature sets, data preparation, training and test proce-dures, and parameter tuning, and estimate the statisticalsignificance of performance differences.
Unfortunately,reported results sometimes leave out details needed foraccurate comparisons.
We report F scores for comparisonwith previous work, but we also give statistical signifi-cance estimates using McNemar?s test for those methodsthat we evaluated directly.Testing the significance of F scores is tricky becausethe wrong chunks generated by two chunkers are notdirectly comparable.
Yeh (2000) examined randomizedtests for estimating the significance of F scores, and inparticular the bootstrap over the test set (Efron and Tib-shirani, 1993; Sang, 2002).
However, bootstrap variancesin preliminary experiments were too high to allow anyconclusions, so we used instead a McNemar paired teston labeling disagreements (Gillick and Cox, 1989).Model F scoreSVM combination 94.39%(Kudo and Matsumoto, 2001)CRF 94.38%Generalized winnow 93.89%(Zhang et al, 2002)Voted perceptron 94.09%MEMM 93.70%Table 2: NP chunking F scores5 ResultsAll the experiments were performed with our Java imple-mentation of CRFs,designed to handle millions of fea-tures, on 1.7 GHz Pentium IV processors with Linux andIBM Java 1.3.0.
Minor variants support voted perceptron(Collins, 2002) and MEMMs (McCallum et al, 2000)with the same efficient feature encoding.
GIS, CG, andL-BFGS were used to train CRFs and MEMMs.5.1 F ScoresTable 2 gives representative NP chunking F scores forprevious work and for our best model, with the com-plete set of 3.8 million features.
The last row of the tablegives the score for an MEMM trained with the mixed CGmethod using an approximate preconditioner.
The pub-lished F score for voted perceptron is 93.53% with a dif-ferent feature set (Collins, 2002).
The improved resultgiven here is for the supported feature set; the completefeature set gives a slightly lower score of 94.07%.
Zhanget al (2002) reported a higher F score (94.38%) with gen-eralized winnow using additional linguistic features thatwere not available to us.5.2 Convergence SpeedAll the results in the rest of this section are for the smallersupported set of 820,000 features.
Figures 2a and 2bshow how preconditioning helps training convergence.Since each CG iteration involves a line search that mayrequire several forward-backward procedures (typicallybetween 4 and 5 in our experiments), we plot the progressof penalized log-likelihood L??
with respect to the num-ber of forward-backward evaluations.
The objective func-tion increases rapidly, achieving close proximity to themaximum in a few iterations (typically 10).
In contrast,GIS training increases L??
rather slowly, never reachingthe value achieved by CG.
The relative slowness of it-erative scaling is also documented in a recent evaluationof training methods for maximum-entropy classification(Malouf, 2002).
In theory, GIS would eventually con-verge to the L??
optimum, but in practice convergencemay be so slow that L??
improvements may fall belownumerical accuracy, falsely indicating convergence.training method time F score L??Precond.
CG 130 94.19% -2968Mixed CG 540 94.20% -2990Plain CG 648 94.04% -2967L-BFGS 84 94.19% -2948GIS 3700 93.55% -5668Table 3: Runtime for various training methodsnull hypothesis p-valueCRF vs. SVM 0.469CRF vs. MEMM 0.00109CRF vs. voted perceptron 0.116MEMM vs. voted perceptron 0.0734Table 4: McNemar?s tests on labeling disagreementsMixed CG training converges slightly more slowlythan preconditioned CG.
On the other hand, CG withoutpreconditioner converges much more slowly than bothpreconditioned CG and mixed CG training.
However, itis still much faster than GIS.
We believe that the superiorconvergence rate of preconditioned CG is due to the useof approximate second-order information.
This is con-firmed by the performance of L-BFGS, which also usesapproximate second-order information.2Although there is no direct relationship between Fscores and log-likelihood, in these experiments F scoretends to follow log-likelihood.
Indeed, Figure 3 showsthat preconditioned CG training improves test F scoresmuch more rapidly than GIS training.Table 3 compares run times (in minutes) for reaching atarget penalized log-likelihood for various training meth-ods with prior ?
= 1.0.
GIS is the only method that failedto reach the target, after 3,700 iterations.
We cannot placethe voted perceptron in this table, as it does not opti-mize log-likelihood and does not use a prior.
However,it reaches a fairly good F-score above 93% in just twotraining sweeps, but after that it improves more slowly, toa somewhat lower score, than preconditioned CG train-ing.5.3 Labeling AccuracyThe accuracy rate for individual labeling decisions isover-optimistic as an accuracy measure for shallow pars-ing.
For instance, if the chunk BIIIIIII is labled asOIIIIIII, the labeling accuracy is 87.5%, but recall is0.
However, individual labeling errors provide a moreconvenient basis for statistical significance tests.
One2Although L-BFGS has a slightly higher penalized log-likelihood, its log-likelihood on the data is actually lower thanthat of preconditioned CG and mixed CG training.6 56 106 156 206 256?35000?30000?25000?20000?15000?10000?50000# of Forward?backward evaluationsPenalizedLog?likelihoodComparison of Fast Training Algorithms for CRFPreconditioned CGMixed CG TrainingL?BFGS(a) L??
: CG (precond., mixed), L-BFGS0 50 100 150 200 250 300 350 400 450 500?200000?180000?160000?140000?120000?100000?80000?60000?40000?200000# of Forward?backward evaluationsPenalizedLog?likelihoodComparison of CG Methods to GISPreconditioned CGCG w/o PreconditionerGIS(b) L??
: CG (precond., plain), GISFigure 2: Training convergence for various methods0 50 100 150 200 250 300 350 400 450 5000.450.50.550.60.650.70.750.80.850.90.95# of Forward?backward evaluationsFscoreComparison of CG Methods to GISPreconditioned CGCG w/o PreconditionerGISFigure 3: Test F scores vs. training timesuch test is McNemar test on paired observations (Gillickand Cox, 1989).With McNemar?s test, we compare the correctness ofthe labeling decisions of two models.
The null hypothesisis that the disagreements (correct vs. incorrect) are due tochance.
Table 4 summarizes the results of tests betweenthe models for which we had labeling decisions.
Thesetests suggest that MEMMs are significantly less accurate,but that there are no significant differences in accuracyamong the other models.6 ConclusionsWe have shown that (log-)linear sequence labeling mod-els trained discriminatively with general-purpose opti-mization methods are a simple, competitive solution tolearning shallow parsers.
These models combine the bestfeatures of generative finite-state models and discrimina-tive (log-)linear classifiers, and do NP chunking as wellas or better than ?ad hoc?
classifier combinations, whichwere the most accurate approach until now.
In a longerversion of this work we will also describe shallow pars-ing results for other phrase types.
There is no reason whythe same techniques cannot be used equally successfullyfor the other types or for other related tasks, such as POStagging or named-entity recognition.On the machine-learning side, it would be interest-ing to generalize the ideas of large-margin classificationto sequence models, strengthening the results of Collins(2002) and leading to new optimal training algorithmswith stronger guarantees against overfitting.On the application side, (log-)linear parsing modelshave the potential to supplant the currently dominantlexicalized PCFG models for parsing by allowing muchricher feature sets and simpler smoothing, while avoid-ing the label bias problem that may have hindered earlierclassifier-based parsers (Ratnaparkhi, 1997).
However,work in that direction has so far addressed only parsereranking (Collins and Duffy, 2002; Riezler et al, 2002).Full discriminative parser training faces significant algo-rithmic challenges in the relationship between parsing al-ternatives and feature values (Geman and Johnson, 2002)and in computing feature expectations.AcknowledgmentsJohn Lafferty and Andrew McCallum worked with thesecond author on developing CRFs.
McCallum helpedby the second author implemented the first conjugate-gradient trainer for CRFs, which convinced us that train-ing of large CRFs on large datasets would be practical.Michael Collins helped us reproduce his generalized per-cepton results and compare his method with ours.
ErikTjong Kim Sang, who has created the best online re-sources on shallow parsing, helped us with details of theCoNLL-2000 shared task.
Taku Kudo provided the out-put of his SVM chunker for the significance test.ReferencesS.
Abney.
Parsing by chunks.
In R. Berwick, S. Abney, andC.
Tenny, editors, Principle-based Parsing.
Kluwer Aca-demic Publishers, 1991.S.
Abney, R. E. Schapire, and Y.
Singer.
Boosting applied totagging and PP attachment.
In Proc.
EMNLP-VLC, NewBrunswick, New Jersey, 1999.
ACL.A.
L. Berger, S. A. Della Pietra, and V. J. Della Pietra.
A maxi-mum entropy approach to natural language processing.
Com-putational Linguistics, 22(1), 1996.D.
M. Bikel, R. L. Schwartz, and R. M. Weischedel.
An algo-rithm that learns what?s in a name.
Machine Learning, 34:211?231, 1999.L.
Bottou.
Une Approche the?orique de l?Apprentissage Con-nexionniste: Applications a` la Reconnaissance de la Parole.PhD thesis, Universite?
de Paris XI, 1991.E.
Brill.
Transformation-based error-driven learning and naturallanguage processing: a case study in part of speech tagging.Computational Linguistics, 21:543?565, 1995.S.
F. Chen and R. Rosenfeld.
A Gaussian prior for smoothingmaximum entropy models.
Technical Report CMU-CS-99-108, Carnegie Mellon University, 1999.M.
Collins.
Discriminative training methods for hidden Markovmodels: Theory and experiments with perceptron algo-rithms.
In Proc.
EMNLP 2002.
ACL, 2002.M.
Collins and N. Duffy.
New ranking algorithms for parsingand tagging: Kernels over discrete structures, and the votedperceptron.
In Proc.
40th ACL, 2002.J.
N. Darroch and D. Ratcliff.
Generalized iterative scaling forlog-linear models.
The Annals of Mathematical Statistics, 43(5):1470?1480, 1972.S.
Della Pietra, V. Della Pietra, and J. Lafferty.
Inducing fea-tures of random fields.
IEEE PAMI, 19(4):380?393, 1997.B.
Efron and R. J. Tibshirani.
An Introduction to the Bootstrap.Chapman & Hall/CRC, 1993.D.
Freitag and A. McCallum.
Information extraction withHMM structures learned by stochastic optimization.
InProc.
AAAI 2000, 2000.S.
Geman and M. Johnson.
Dynamic programming for parsingand estimation of stochastic unification-based grammars.
InProc.
40th ACL, 2002.L.
Gillick and S. Cox.
Some statistical issues in the compairsonof speech recognition algorithms.
In International Confer-ence on Acoustics Speech and Signal Processing, volume 1,pages 532?535, 1989.J.
Hammersley and P. Clifford.
Markov fields on finite graphsand lattices.
Unpublished manuscript, 1971.T.
Kudo and Y. Matsumoto.
Chunking with support vector ma-chines.
In Proc.
NAACL 2001.
ACL, 2001.J.
Kupiec.
Robust part-of-speech tagging using a hiddenMarkov model.
Computer Speech and Language, 6:225?242,1992.J.
Lafferty, A. McCallum, and F. Pereira.
Conditional randomfields: Probabilistic models for segmenting and labeling se-quence data.
In Proc.
ICML-01, pages 282?289, 2001.R.
Malouf.
A comparison of algorithms for maximum entropyparameter estimation.
In Proc.
CoNLL-2002, 2002.A.
McCallum, D. Freitag, and F. Pereira.
Maximum entropyMarkov models for information extraction and segmentation.In Proc.
ICML 2000, pages 591?598, Stanford, California,2000.T.
P. Minka.
Algorithms for maximum-likelihood logistic re-gression.
Technical Report 758, CMU Statistics Department,2001.J.
Nocedal and S. J. Wright.
Numerical Optimization.
Springer,1999.V.
Punyakanok and D. Roth.
The use of classifiers in sequentialinference.
In NIPS 13, pages 995?1001.
MIT Press, 2001.L.
A. Ramshaw and M. P. Marcus.
Text chunking usingtransformation-based learning.
In Proc.
Third Workshop onVery Large Corpora.
ACL, 1995.A.
Ratnaparkhi.
A maximum entropy model for part-of-speechtagging.
In Proc.
EMNLP, New Brunswick, New Jersey,1996.
ACL.A.
Ratnaparkhi.
A linear observed time statistical parserbased on maximum entropy models.
In C. Cardie andR.
Weischedel, editors, EMNLP-2.
ACL, 1997.S.
Riezler, T. H. King, R. M. Kaplan, R. Crouch, J. T.Maxwell III, and M. Johnson.
Parsing the Wall Street Journalusing a lexical-functional grammar and discriminative esti-mation techniques.
In Proc.
40th ACL, 2002.E.
F. T. K. Sang.
Memory-based shallow parsing.
Journal ofMachine Learning Research, 2:559?594, 2002.J.
R. Shewchuk.
An introduction to the conjugate gradientmethod without the agonizing pain, 1994.
URL http://www-2.cs.cmu.edu/?jrs/jrspapers.html#cg.B.
Taskar, P. Abbeel, and D. Koller.
Discriminative probabilis-tic models for relational data.
In Eighteenth Conference onUncertainty in Artificial Intelligence, 2002.E.
F. Tjong Kim Sang and S. Buchholz.
Introduction to theCoNLL-2000 shared task: Chunking.
In Proc.
CoNLL-2000,pages 127?132, 2000.H.
Wallach.
Efficient training of conditional random fields.
InProc.
6th Annual CLUK Research Colloquium, 2002.A.
Yeh.
More accurate tests for the statistical significance ofresult differences.
In COLING-2000, pages 947?953, Saar-bruecken, Germany, 2000.T.
Zhang, F. Damerau, and D. Johnson.
Text chunking basedon a generalization of winnow.
Journal of Machine LearningResearch, 2:615?637, 2002.
