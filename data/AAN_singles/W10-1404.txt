Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 31?39,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsApplication of Different Techniques to Dependency Parsing of BasqueKepa Bengoetxea Koldo GojenolaIXA NLP Group IXA NLP GroupUniversity of the Basque Country University of the Basque CountryTechnical School of Engineering, Bilbao,Plaza La Casilla 3, 48012, BilbaoTechnical School of Engineering, Bilbao,Plaza La Casilla 3, 48012, Bilbaokepa.bengoetxea@ehu.es koldo.gojenola@ehu.esAbstractWe present a set of experiments on depend-ency parsing of the Basque Dependency Tree-bank (BDT).
The present work has examinedseveral directions that try to explore the richset of morphosyntactic features in the BDT: i)experimenting the impact of morphologicalfeatures, ii) application of dependency treetransformations, iii) application of a two-stageparsing scheme (stacking), and iv) combina-tions of the individual experiments.
All thetests were conducted using MaltParser (Nivreet al, 2007a), a freely available and state ofthe art dependency parser generator.1 IntroductionThis paper presents several experiments performedon dependency parsing of the Basque DependencyTreebank (BDT, Aduriz et al, 2003).
Basque canbe briefly described as a morphologically rich lan-guage with free constituent order of the main sen-tence elements with respect to the main verb.This work has been developed in the context ofdependency parsing exemplified by the CoNLLShared Task on Dependency Parsing in years 2006and 2007 (Nivre et al, 2007b), where several sys-tems competed analyzing data from a typologicallyvaried range of 19 languages.
The treebanks for alllanguages were standardized using a previouslyagreed CoNLL-X format (see Figure 1).
An earlyversion of the BDT (BDT I) was one of the evalu-ated treebanks, which will allow a comparison withour results.
One of the conclusions of the CoNLL2007 workshop (Nivre et al, 2007a) was that thereis a class of languages, those that combine a rela-tively free word order with a high degree of inflec-tion, that obtained the worst scores.
This asks forthe development of new methods and algorithmsthat will help to reach the parsing performance ofthe more studied languages, as English.In this work, we will take the opportunity ofhaving a new fresh version of the BDT, (BDT IIhenceforth), which is the result of an extension(three times bigger than the original one), and itsredesign (see section 3.2).
Using MaltParser, afreely available and state of the art dependencyparser for all the experiments (Nivre et al, 2007a),this paper will concentrate on the application ofdifferent techniques to the task of parsing this newtreebank, with the objective of giving a snapshotthat can show the expected gains of each tech-nique, together with some of their combinations.Some of the techniques have already been evalu-ated with other languages/treebanks or BDT I,while others have been adapted or extended to dealwith specific aspects of the Basque language or theBasque Treebank.
We will test the following:?
Impact of rich morphology.
Although manysystems performed feature engineering on theBDT at CoNLL 2007, providing a strongbaseline, we will take a step further to im-prove parsing accuracy taking into account theeffect of specific morphosyntactic features.?
Application of dependency-tree transforma-tions.
Nilsson et al (2007) showed that theycan increase parsing accuracy across lan-guages/treebanks.
We have performed similarexperiments adapted to the specific propertiesof Basque and the BDT.?
Several works have tested the effect of using atwo-stage parser (Nivre and McDonald, 2008;Martins et al, 2008), where the second parsertakes advantage of features obtained by thefirst one.
Similarly, we will experiment the31addition of new features to the input of thesecond-stage parser, in the form of morpho-syntactic features propagated through the firstparser?s dependency tree and also as the addi-tion of contextual features (such as categoryor dependency relation of parent, grandparent,and descendants).?
Combinations of the individual experiments.The rest of the paper is organized as follows.After presenting related work in section 2, section3 describes the main resources used in this work.Next, section 4 will examine the details of the dif-ferent experiments to be performed, while section5 will evaluate their results.
Finally, the last sectionoutlines our main conclusions.2 Related workUntil recently, many works on treebank parsinghave been mostly dedicated to languages with poormorphology, as exemplified by the Penn EnglishTreebank.
As the availability of treebanks for typo-logically different languages has increased, therehas been a growing interest towards research onextending the by now standard algorithms andmethods to the new languages and treebanks (Tsar-faty et al, 2009).
For example, Collins et al (1999)adapted Collins?
parser to Czech, a highly-inflected language.
Cowan and Collins (2005) ap-ply the same parser to Spanish, concluding that theinclusion of morphological information improvesthe analyzer.
Eryi?it et al (2008) experiment theuse of several types of morphosyntactic informa-tion in Turkish, showing how the richest the in-formation improves precision.
They also show thatusing morphemes as the unit of analysis (instead ofwords) gets better results, as a result of the aggluti-native nature of Turkish, where each wordformcontains several morphemes that can be individu-ally relevant for parsing.
Goldberg and Tsarfaty(2008) concluded that an integrated model of mor-phological disambiguation and syntactic parsing inHebrew Treebank parsing improves the results of apipelined approach.
This is in accord with our ex-periment of dividing words into morphemes andtransforming the tree accordingly (see section 4.2).Since the early times of treebank-based parsingsystems, a lot of effort has been devoted to aspectsof preprocessing trees in order to improve the re-sults (Collins, 1999).
When applied to dependencyparsing, several works (Nilsson et al, 2007; Ben-goetxea and Gojenola, 2009a) have concentratedon modifying the structure of the dependency tree,changing its original shape.
For example, Nilssonet al (2007) present the application of pseudopro-jective, verbal group and coordination transforma-tions to several languages/treebanks usingMaltParser, showing that they improve the results.Another interesting research direction has exam-ined the application of a two-stage parser, wherethe second parser tries to improve upon the resultof a first parser.
For example, Nivre and McDonald(2008) present the combination of two state of theart dependency parsers feeding each another,showing that there is a significant improvementover the simple parsers.
This experiment can beseen as an instance of stacked learning, which wasalso tested on dependency parsing of several lan-guages in (Martins et al, 2008) with significantimprovements over the base parser.3 ResourcesThis section will describe the main resources thathave been used in the experiments.
First, subsec-Index Word   Lemma   Category Subcategory Features  Head Dependency1 etorri   etorri   V  V  _   3 coord2 dela   izan     AUXV AUXV  REL:CMP|SUBJ:3S 1 auxmod3 eta   eta     CONJ CONJ  _   6 ccomp_obj4 joan   joan     V  V  _   3 coord5 dela   izan     AUXV AUXV  REL:CMP|SUBJ:3S 4 auxmod6 esan   esan     V  V  _   0 ROOT7 zien   *edun    AUXV AUXV  SUBJ:3S|OBJ:3P 6 auxmod8 mutilak  mutil   NOUN NOUN_C  CASE:ERG|NUM:S 6 ncsubj9 .
.
PUNT PUNT_PUNT _   8 PUNCFigure 1: Example of a BDT sentence in the CoNLL-X format(V = main verb, AUXV = auxiliary verb, CONJ = conjunction, REL = subordinated clause, CMP = completive, ccomp_obj =clausal complement object, ERG = ergative, SUBJ:3S: subject in 3rd person sing., OBJ:3P: object in 3rd person pl, coord =coordination, auxmod = auxiliary, ncsubj = non-clausal subject, ncmod = non-clausal modifier).32tion 3.1 will describe the Basque DependencyTreebank, which has increased its size from 55,469tokens in its original version to more than 150,000,while subsection 3.2 will present the main charac-teristics of MaltParser, a state of the art and data-driven dependency parser.3.1 The Basque Dependency TreebankBasque can be described as an agglutinative lan-guage that presents a high power to generate in-flected word-forms, with free constituent order ofsentence elements with respect to the main verb.The BDT can be considered a pure dependencytreebank from its original design, due mainly to thesyntactic characteristics of Basque.
(1) Etorri  dela  eta joan  dela   esan  zien mutilakcome  that-has and go  that-has tell did  boy-theThe boy told them that he has come and goneFigure 1 contains an example of a sentence (1),annotated in the CoNLL-X format.
The text is or-ganized in eight tab-separated columns: word-number, form, lemma, category, subcategory, mor-phological features, and the dependency relation(headword + dependency).
The information in Fig-ure 1 has been simplified due to space reasons, astypically the Features column will contain manymorphosyntactic1 features (case, number, type ofsubordinated sentence, ?
), which are relevant forparsing.
The first version of the Basque Depend-ency Treebank contained 55,469 tokens forming3,700 sentences (Aduriz et al, 2003).
This tree-bank was used as one of the evaluated treebanks inthe CoNLL 2007 Shared Task on DependencyParsing (Nivre et al, 2007b).
Our work will makeuse of the second version of the BDT (BDT II),which is the consequence of a process of extensionand redesign of the original requirements:?
The new version contains 150,000 tokens(11,225 sentences), a three-fold increase.?
The new design considered that all the de-pendency arcs would connect sentence tokens.In contrast, the original annotation containedempty nodes, especially when dealing with el-lipsis and some kinds of coordination.
As aresult, the number of non-projective arcs di-1 We will use the term morphosyntactic to name the set offeatures attached to each word-form, which by the agglutina-tive nature of Basque correspond to both morphology andsyntax.minished from 2.9% in the original treebankto 1.3% in the new version.?
The annotation follows a stand-off markupapproach, inspired on TEI-P4 (Artola et al,2005).
There was a conversion process from aset of interconnected XML files to theCoNLL-X format of the present experiments.Although the different characteristics and size ofthe two treebank versions do not allow a strictcomparison, our preliminary experiments showedthat the results on both treebanks were similar re-garding our main evaluation criterion (LabeledAttachment Score, or LAS).
In the rest of the paperwe will only use the new BDT II.3.2 MaltParserMaltParser (Nivre et al 2007a) is a state of the artdependency parser that has been successfully ap-plied to typologically different languages and tree-banks.
While several variants of the base parserhave been implemented, we will use one of itsstandard versions (MaltParser version 1.3).
Theparser obtains deterministically a dependency treein linear-time in a single pass over the input usingtwo main data structures: a stack of partially ana-lyzed items and the remaining input sequence.
Todetermine the best action at each parsing step, theparser uses history-based feature models and dis-criminative machine learning.
In all the followingexperiments, we will make use of a SVM classi-fier.
The specification of the configuration used forlearning can in principle include any kind of col-umn in Figure 1 (such as word-form, lemma, cate-gory, subcategory or morphological features),together with a feature function.
This means that alearning model can be described as a series of(column, function) pairs, where column representsthe name of a column in Figure 1, and functionmakes reference to the parser?s main data struc-tures.
For example, the two pairs (Word, Stack[0]),and (Word, Stack[1]) represent two features thatcorrespond to the word-forms on top and next totop elements of the stack, respectively, while(POSTAG, Input[0]) represents the POS categoryof the first token in the remaining input sequence.4 ExperimentsThe following subsections will present three typesof techniques that will be tested with the aim of33improving the results of the syntactic analyzer.Subsection 4.1 presents the process of fine-tuningthe rich set of available morphosyntactic features.Then, 4.2 will describe the application of threetypes of tree transformations, while subsection 4.3will examine the application of propagating syntac-tic features through a first-stage dependency tree, aprocess that can also be seen as an application ofstacked learning, as tested in (Nivre and McDon-ald, 2008; Martins et al, 2008)4.1 Feature engineeringThe original CoNLL-X format uses 10 differentcolumns (see Figure 12), grouping the full set ofmorphosyntactic features in a single column.
Wewill experiment the effect of individual features,following two steps:?
First, we tested the effect of incorporatingeach individual lexical feature, concludingthat there were two features that individuallygave significant performance increases.
Theywere syntactic case, which is relevant formarking a word?s syntactic function (or,equivalently, the type of dependency relation),and subordination type (REL henceforth).This REL feature appears in verb-ending mor-phemes that specify a type of subordinatedsentence, such as in relative, completive, orindirect interrogative clauses.
The feature is,therefore, relevant for establishing the mainstructure of a sentence, helping to delimitmain and subordinated clauses, and it is alsocrucial for determining the dependency rela-tion between the subordinated sentence andthe main verb (head).?
Then, we separated these features in two in-dependent columns, grouping the remainingfeatures under the Features column.
This way,Maltparser?s learning specification can bemore fine-grained, in terms of three morpho-syntactic feature sets (CASE, REL and therest, see Table 2).This will allow us testing learning models withdifferent configurations for each column, insteadof treating the full set of features as a whole.
So,we will have the possibility of experimenting with2 As a matter of fact, Figure 1 only shows 8 columns, althoughthe CoNLL-X format includes two additional columns for theprojective head (PHEAD) and projective dependency relation(PDEPREL), which have not been used in our work.richer contexts (that is, advancing the Stack and/orInput3 functions for each feature).4.2 Tree transformationsTree transformations have long been applied withthe objective of improving parsing results (Collins,1999; Nilsson et al, 2007).
The general processconsists of the following steps:?
Apply tree transformations to the treebank?
Train the system on the modified treebank?
Apply the parser to the test set?
Apply the inverse transformations?
Evaluate the result on the original treebankWe will test three different tree transformations,which had already been applied to the Treebank(BDT I) (Bengoetxea and Gojenola, 2009a):?
Projectivization (TP).
This is a language inde-pendent transformation already tested in sev-eral languages (Nivre and Nilsson, 2005).This transformation is totally language inde-pendent, and can be considered a standardtransformation.
Its performance on the firstversion of BDT had been already tested (Hallet al, 2007), giving significant improvementsThis is in accordance with BDT I having a2.9% of non-projective arcs.?
Coordination (TC).
The transformation on co-ordinated sentences can be considered general(Nilsson et al, 2007) but it is also languagedependent, as it depends on the specific con-figurations present in each language, mainlythe set of coordination conjunctions and thetypes of elements that can be coordinated, to-gether with their morphosyntactic properties(such as head initial or final).
Coordination inBDT (both versions) is annotated in the socalled Prague Style (PS, see Figure 2), wherethe conjunction is taken as the head, and the3 Maltparser allows a rich set of functions to be specified foreach column.
In our experiments we mainly used the Stackand Input functions, which allow the inspection of the contentsof the top elements of the Stack (Stack[0], Stack[1], ?)
or thecurrently unanalyzed input sequence (Input[0], Input [1], ?
).C1 C2  S C3   C1 C2 S C3  C1 C2 S C3(PS) (MS) (MS-sym)Figure 2.
Dependency structures for coordination.34conjuncts depend on it.
Nilsson et al (2007)advocate the Mel?cuk style (MS) for parsingCzech, taking the first conjunct as the head,and creating a chain where each element de-pends on the preceding one.
Basque is a headfinal language, where many important syntac-tic features, like case or subordinating mor-phemes are located at the end of constituents.For that reason, Bengoetxea and Gojenola(2009a) proposed MS-sym, a symmetricvariation of MS in which the coordinatedelements will be dependents of the last con-junct (which will be the head, see Figure 2).?
Transformation of subordinated sentences(TS).
They are formed in Basque by attachingthe corresponding morphemes to the auxiliaryverbs.
However, in BDT (I and II) the verbalelements are organized around the main verb(semantic head) while the syntactic headcorresponds to the subordination morpheme,which appears usually attached to theauxiliary.
Its main consequence is that theelements bearing the relevant information forparsing are situated far in the tree with respectto their head.
In Figure 3, we see that themorpheme ?la, indicating a subordinatedcompletive sentence, appears down in the tree,and this could affect the correct attachment tothe main verb (esan).
Figure 4 shows theeffect of transforming the original tree inFigure 3.
The subordination morpheme (-la) isseparated from the auxiliary verb (da), and is?promoted?
as the syntactic head of  thesubordinated sentence.
New arcs are createdfrom the main verb (etorri) to the morpheme(which is now the head), and also a newdependency relation (SUB).Overall, the projectivization transformation (TP)is totally language-independent.
TC (coordination)can be considered in the middle, as it depends onthe general characteristics of the language.
Finally,the transformation of subordinated sentences (TS)is specific to the treebank and intrinsecally linkedto the agglutinative nature of Basque.
Bengoetxeaand Gojenola (2009a) also found that the order oftransformations can be relevant.
Their best system,after applying all the transformations, obtained a76.80% LAS on BDT I (2.24% improvement overa baseline of 74.52%) on the test set.
We includethese already evaluated transformations in the pre-sent work with two objectives in mind:?
We want to test its effect on BDT II, 3 timeslarger than BDT I, and also with a lowerproportion of non-projective arcs (1.3%).?
We are also interested in testing itscombination with the rest of the techniques(see subsections 4.1 and 4.3).4.3 Two-stage parsing (stacking)Bengoetxea and Gojenola (2009b) tested the effectof propagating several morphosyntactic featurevalues after a first parsing phase, as in classicalunification-based grammars, as a means of propa-gating linguistic information through syntax trees.They applied three types of feature propagation ofthe morphological feature values: a) from auxiliaryverbs to the main verb (verb groups) b) propaga-tion of case and number from post-modifiers to thehead noun (noun phrases) c) from the last conjunctto the conjunction (coordination).
This was donemainly because Basque is head final, and relevantfeatures are located at the end of constituents.Nivre and McDonald (2008) present anapplication of stacked learning to dependencyparsing, in which a second predictor is trained toimprove the performance of the first.
Martins et al(2008) specify the following steps:?
Split training data D into L partitions D1, ...
,DL.?
Train L instances of the level 0 parser in thefollowing way: the l-th instance, gl, is trainedauxmodccomp_objFigure 4.
Effect of applying the transformation onsubordinated sentences to the tree in Figure 3(dotted lines represent the modified arcs).Etorri   da    +la  esan   ducome      has+he  that  told     did+heV       AUXV+3S  COMPL   V      AUXVSUB auxmodauxmodccomp_objauxmodFigure 3.
Dependency tree for the sentence Etorridela esan du (He told that he would come).Etorri    da+la     esan   ducome      has+he+that   told    did+heV       AUXV+3S+COMPL   V       AUXV35on D?l = D \ Dl.
Then use gl to outputpredictions for the (unseen) partition Dl.
Atthe end, we have an augmented dataset D* = D+ new set of stacked/propagated features.?
Train the level 0 parser g on the originaltraining data D.?
Train the level 1 parser on the augmentedtraining data D*.In our tests, it was enough with two partitions (L= 2), as experiments with L > 2 did not give anysignificant improvement.
Figure 5 shows the typesof information that can be added to each targetelement.
The token X can take several kinds ofinformation from its children (A and B) or his par-ent (H).
The information that is propagated canvary, including part of speech, morphosyntacticfeatures or the dependency relations between Xand its children/parent.
We can roughly classify thestacked features in two different sets:?
Linguistic features (feature passing), such ascase or number, which are propagatedapplying linguistic principles, such as ?thesyntactic case is propagated from thedependents towards the head of NPs andpostpositional phrases?.
The idea is topropagate several morphosyntactic features(case, number, ?)
from dependents to  heads.?
Parser features.
They will be based solely ondifferent dependency tree configurations (seeFigure 5), similarly to (Nivre and McDonald,2008; Martins et al, 2008).
Among them, wewill test the inclusion of several features(dependency relation, category andmorphosyntactic features) from the following:parent, grandparent, siblings, and children.In the present work, we have devised the follow-ing experiments:?
We will test the effect of propagatinglinguistic features on the new BDT II.
Incontrast to (Bengoetxea and Gojenola,2009b), who used the enriched gold data as D*directly, we will test Martins et al?s proposal,in which the level 1 parser will be able tolearn on the errors of the level 0 parser.?
We will extend these experiments with the useof different parser features (Nivre andMcDonald, 2008; Martins et al, 2008).4.4 CombinationFinally, we will combine the different techniques.An important point is to determine whether thetechniques are independent (and accumulative) orit could also be that they can serve as alternativetreatments to deal with the same phenomena.5 EvaluationBDT I was used at the CoNLL 2007 Shared Task,where many systems competed on it (Nivre et al,2007b).
We will use Labeled Attachment Score(LAS) as the evaluation measure: the percentage ofcorrect arcs (both dependency relation and head)over all arcs, with respect to the gold standard.
Ta-ble 1 shows the best CoNLL 2007 results on BDTI.
The best system obtained a score of 76.94%,combining six variants of MaltParser, and compet-ing with 19 systems.
Carreras (2007) and Titov andHenderson (2007) obtained the second and thirdpositions, respectively.
We consider the last twolines in Table 1 as our baselines, which consist inapplying a single MaltParser version (Hall et al,2007), that obtained the fifth position at CoNLL2007.
Although Hall et al (2007) applied the pro-jectivization transformation (TP), we will not use itin our baseline because we want to evaluate theeffect of multiple techniques over a base parser.Although we could not use the subset of BDT IIcorresponding to BDT I, we run4 a test with a setof sentences the size of BDT I.
As could be ex-4 For space reasons, we do not specify details of the algorithmand the parameters.
These data can be obtained, together withthe BDT II data, from any of the authors.System LASNivre et al 2007b (MaltParser,combined)76.94%Carreras, 2007 75.75%Titov and Henderson, 2007 75.49%CoNLL07Hall et al, 2007 (MaltParser(single parser) + pseudoprojec-tive transformation)74.99%BDT IMaltParser (single parser) 74.52%BDT I size 74.83% BDT II  MaltParser (singleparser)  Baseline 77.08%Table 1.
Top LAS scores for Basque dependency parsing.d2d1d3Figure 5.
Stacked features.
X can take severalfeatures from its descendants (dependency arcsd2 and d3) or his head (d1).A            X           B    H36pected, the three-fold increase in the new treebankgives a 2.35% improvement over BDT I.For evaluation, we divided the treebank in threesets, corresponding to training, development, andtest (80%, 10%, and 10%, respectively).
All theexperiments were done on the development set,leaving the best systems for the final test.5.1 Single systemsTable 3 shows the results for the basic systemsemploying each of the techniques advanced in Sec-tion 4.
As a first result, we see that a new step ofreengineering MaltParser?s learning configurationwas rewarding (see row 2 in Table 3), as morpho-syntactic features were more finely specified withrespect to the most relevant features.
Table 2 pre-sents the baseline and the best learning model5.
Wesee that advancing the input lookahead for CASEand REL gives an increase of 0.82 points.Looking at the transformations (rows 3 to 7), thenew Treebank BDT II obtains results similar tothose described in (Bengoetxea and Gojenola,2009a).
As could be expected from the reductionof non-projective arcs (from 2.9% to 1.3%), thegains of TP are proportionally lower than in BDT I.Also, we can observe that TS alone worsens thebaseline, but it gives the best results when com-bined with the rest (rows 6 and 7).
This can be ex-plained because TS creates new non-projectivearcs, so it is effective only if TP is applied later.The transformation on coordination (TC) alonedoes not get better results, but when combinedwith TP and TS gives the best results.Applying feature propagation and stacking (seerows 9-17), we can see that most of the individualtechniques (rows 9-14) give improvements overthe baseline.
When combining what we defined as5 This experiment was possible due to the fact that Malt-Parser?s functionality was extended, allowing the specificationof new columns/features, as the first versions of MaltParseronly permitted a single column that included all the features.linguistic features (those morphosyntactic featurespropagated by the application of three linguisticprinciples), we can see that their combinationseems accumulative (row 15).
The parser featuresalso give a significant improvement individually(rows 12-14), but, when combined either amongthemselves (row 16) or with the linguistic features(row 17), their effect does not seem to be additive.5.2 Combined systemsAfter getting significant improvements on the indi-vidual techniques and some of their combinations,we took a further step to integrate different tech-niques.
An important aspect that must be taken intoaccount is that the combination is not trivial all thetimes.
For example, we have seen (section 5.1) thatcombinations of the three kinds of tree transforma-tions must be defined having in mind the possibleside-effects of any previous transformation.
Whencombining different techniques, care must be takento avoid any incompatibility.
For that reason weonly tested some possibilities.
Rows 18-21 showsome of the combined experiments.
Combinationof feature optimization with the pseudoprojectivetransformation yields an accumulative improve-ment (row 18).
However, the combination of allthe tree transformations with FO (row 19) does notaccumulate.
This can be due to the fact that featureoptimization already cancelled the effect of thetransformation on coordination and subordinatedsentences, or otherwise it could also need a betterexploration of their interleaved effect.
Finally, row21 shows that feature optimization, the pseudopro-jective transformation and feature propagation arealso accumulative, giving the best results.
The rela-tions among the rest of the transformations deservefuture examination, as the results do not allow usto extract a precise conclusion.6 Conclusions and future workWe studied several proposals for improving a base-line system for parsing the Basque Treebank.
Allthe results were evaluated on the new version,BDT II, three times larger than the previous one.We have obtained the following main results:?
Using rich morphological features.
We haveextended previous works, giving a finergrained description of morphosyntacticfeatures on the learner?s configuration,Stack[0] Input[0] Input[1] Input[2]1 Features + +CASE + + +REL + + + +2Features(rest)+ +Table 2.
Learning configurations for morphosyntactic fea-tures (1 = best model for the whole set of features.2 = best model when specializing features).37showing that it can significantly improve theresults.
In particular, differentiating case andthe type of subordinated sentence gives thebest LAS increase (+0.82%).?
Tree transformations.
We have replicated theset of tree transformations that were tested inthe old treebank (Bengoetxea and Gojenola2009a).
Two of the transformations(projectivization and coordination) can beconsidered language independent, while thetreatment of subordination morphemes isrelated to the morphological nature of Basque.?
Feature propagation.
We have experimentedthe effect of a stacked learning scheme.
Someof the stacked features were language-independent, as in (Nivre and McDonald.2008), but we have also applied ageneralization of the stacking mechanism to amorphologically rich language, as some of thestacked features are morphosyntactic features(such as case and number) which werepropagated through a first stage dependencytree by the application of linguistic principles(noun phrases, verb groups and coordination).?
Combination of techniques.
Although severalof the combined approaches are accumulativewith respect to the individual systems, someothers do not give a improvement over thebasic systems.
A careful study must beconducted to investigate whether theapproaches are exclusive or complementary.For example, the transformation onsubordinated sentences and featurepropagation on verbal groups seem to beattacking the same problem, i. e., the relationsbetween main and subordinated sentences.
Inthis respect, they can be viewed as alternativeapproaches to dealing with these phenomena.The results show that the application of thesetechniques can give noticeable results, getting anoverall improvement of 1.90% (from 77.08% until78.98%), which can be roughly comparable to theeffect of doubling the size of the treebank (see thelast two lines of Table 1).AcknowledgementsThis research was supported by the Department ofIndustry of the Basque Government (IE09-262)and the University of the Basque Country(GIU09/19).
Thanks to Joakim Nivre and his teamfor their support using Maltparser and his fruitfulsuggestion about the use of stacked features.Row System LASBaseline 1  77.08%Feature optimization 2 FO *77.90% (+0.82)3 TP **77.92% (+0.84)4 TS 75.95% (-1.13)5 TC 77.05% (-0.03)6 TS + TP **78.41% (+1.33)Transformations7 TS + TC + TP **78.59% (+1.51)9 SVG **77.68%  (+0.60)10 SNP 77.17% (+0.09)11 SC 77.40% (+0.32)12 SP *77.70% (+0.62)13 SCH *77.80% (+0.72)14 SGP 77.37% (+0.29)15 SVG + SNP + SC **78.22% (+1.14)16 SP + SCH **77.96% (+0.88)SingletechniqueStacking17 SVG + SNP + SC + SP + SCH **78.44% (+1.36)18 FO + TP **78.78% (+1.70)19 FO + TS + TC + TP **78.47% (+1.39)20 TP + SVG + SNP + SC **78.56% (+1.48)Combination21 FO + TP + SVG + SNP + SC **78.98% (+1.90)Table 3.
Evaluation results.
(FO: feature optimization; TP TC TS: Pseudo-projective, Coordination and Subordinated sentence transformations;SVG, SNP, SC: Stacking (feature passing) on Verb Groups, NPs  and Coordination;SP, SCH, SGP: Stacking (category, features and dependency) on Parent, CHildren and GrandParent;*: statistically significant in McNemar's test, p < 0.005; **: statistically significant, p < 0.001)38ReferencesItziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arrio-la, Aitziber Atutxa, Arantza Diaz de Ilarraza, AitzpeaGarmendia and Maite Oronoz.
2003.
Construction ofa Basque dependency treebank.
Treebanks and Lin-guistic Theories.Xabier Artola, Arantza  D?az de Ilarraza, Nerea Ezei-za, Koldo Gojenola, Gorka Labaka, Aitor Sologais-toa, Aitor Soroa.
2005.
A framework forrepresenting and managing linguistic annotationsbased on typed feature structures.
Proceedings of theInternational Conference on Recent Advances inNatural Language Processing, RANLP 2005.Kepa Bengoetxea and Koldo Gojenola.
2009a.
Explor-ing Treebank Transformations in Dependency Pars-ing.
Proceedings of the International Conference onRecent Advances in Natural Language Processing,RANLP?2009.Kepa Bengoetxea and Koldo Gojenola.
2009b.
Applica-tion of feature propagation to dependency parsing.Proceedings of the International Workshop on Pars-ing Technologies (IWPT?2009).Xavier Carreras.
2007.
Experiments with a high-orderprojective dependency parser.
In Proceedings of theCoNLL 2007 Shared Task (EMNLP-CoNLL).Shay B. Cohen and Noah A. Smith.
2007.
Joint Mor-phological and Syntactic Disambiguation.
In Pro-ceedings of the CoNLL 2007 Shared Task.Michael Collins, Jan Hajic, Lance Ramshaw and Chris-toph Tillmann.
1999.
A Statistical Parser for Czech.Proceedings of ACL.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
PhD Dissertation,University of Pennsylvania..Brooke Cowan and Michael Collins.
2005.
Morphologyand Reranking for the Statistical Parsing of Span-ish.
In Proceedings of EMNLP 2005.G?lsen Eryi?it, Joakim Nivre and Kemal Oflazer.
2008.Dependency Parsing of Turkish.
ComputationalLinguistics, Vol.
34 (3).Yoav Goldberg and Reut Tsarfaty.
2008.
A Single Gen-erative Model for Joint Morphological Segmenta-tion and Syntactic Parsing.
Proceedings of ACL-HLT 2008, Colombus, Ohio, USA.Johan Hall, Jens Nilsson, Joakim Nivre, G?lsen Eryigit,Be?ta Megyesi, Mattias Nilsson and Markus Saers.2007.
Single Malt or Blended?
A Study in Multilin-gual Parser Optimization.
Proceedings of the CoNLLShared Task EMNLP-CoNLL.Andr?
F. T. Martins, Dipanjan Das, Noah A. Smith,Eric P. Xing.
2008.
Stacking Dependency Parsing.Proceedings of EMNLP-2008.Jens Nilsson, Joakim Nivre and Johan Hall.
2007.
Gen-eralizing Tree Transformations for Inductive De-pendency Parsing.
Proceedings of the 45thConference of the ACL.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.Joakim Nivre, Johan Hall, Jens Nilsson, Chanev A.,G?lsen Eryi?it, Sandra K?bler, Marinov S., andEdwin Marsi.
2007a.
MaltParser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering.Joakim Nivre, Johan Hall, Sandra K?bler, RyanMcDonald, Jens Nilsson,  Sebastian Riedel andDeniz Yuret.
2007b.
The CoNLL 2007 Shared Taskon Dependency Parsing.
Proceedings of EMNLP-CoNLL.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency pars-ers.
Proceedings of ACL-2008.Ivan Titov and James Henderson.
2007.
Fast and robustmultilingual dependency parsing with a generativelatent variable model.
In Proceedings of the CoNLL2007 Shared Task (EMNLP-CoNLL).Reut Tsarfaty, Khalil Sima?an, and Remko Scha.
2009.An Alternative to Head-Driven Approaches forParsing a (Relatively) Free Word-Order Language.Proceedings of EMNLP.39
