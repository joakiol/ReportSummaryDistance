Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 19?26,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPWhitepaper of NEWS 2009 Machine Transliteration Shared Task?Haizhou Li?, A Kumaran?, Min Zhang?
and Vladimir Pervouchine?
?Institute for Infocomm Research, A*STAR, Singapore 138632{hli,mzhang,vpervouchine}@i2r.a-star.edu.sg?Multilingual Systems Research, Microsoft Research IndiaA.Kumaran@microsoft.comAbstractTransliteration is defined as phonetictranslation of names across languages.Transliteration of Named Entities (NEs)is necessary in many applications, suchas machine translation, corpus alignment,cross-language IR, information extractionand automatic lexicon acquisition.
Allsuch systems call for high-performancetransliteration, which is the focus of theshared task in the NEWS 2009 workshop.The objective of the shared task is to pro-mote machine transliteration research byproviding a common benchmarking plat-form for the community to evaluate thestate-of-the-art technologies.1 Task DescriptionThe task is to develop machine transliteration sys-tem in one or more of the specified language pairsbeing considered for the task.
Each language pairconsists of a source and a target language.
Thetraining and development data sets released foreach language pair are to be used for developinga transliteration system in whatever way that theparticipants find appropriate.
At the evaluationtime, a test set of source names only would be re-leased, on which the participants are expected toproduce a ranked list of transliteration candidatesin another language (i.e.
n-best transliterations),and this will be evaluated using common metrics.For every language pair the participants must sub-mit one run that uses only the data provided by theNEWS workshop organisers in a given languagepair (designated as ?standard?
runs).
Users maysubmit more runs (?non-standard?)
for each lan-guage pair that uses other data than those providedby the NEWS 2009 workshop; such runs would beevaluated and reported separately.
?http://www.acl-ijcnlp-2009.org/workshops/NEWS2009/2 Important DatesResearch paper submission deadline 1 May 2009Shared taskRegistration opens 16 Feb 2009Registration closes 9 Apr 2009Release Training/Development Data 16 Feb 2009Release Test Data 10 Apr 2009Results Submission Due 14 Apr 2009Results Announcement 29 Apr 2009Task (short) Papers Due 3 May 2009For all submissionsAcceptance Notification 1 Jun 2009Camera-Ready Copy Deadline 7 Jun 2009Workshop Date 7 Aug 20093 Participation1.
Registration (16 Feb 2009)(a) NEWS Shared Task opens for registra-tion.
(b) Prospective participants are to register tothe NEWS Workshop homepage.2.
Training & Development Data (16 Feb 2009)(a) Registered participants are to obtaintraining and development data from theShared Task organiser and/or the desig-nated copyright owners of databases.3.
Evaluation Script (16 Mar 2009)(a) A sample test set and expected user out-put format are to be released.
(b) An evaluation script, which runs on theabove two, is to be released.
(c) The participants must make sure thattheir output is produced in a way thatthe evaluation script may run and pro-duce the expected output.19(d) The same script (with held out test dataand the user outputs) would be used forfinal evaluation.4.
Test data (10 April 2009)(a) The test data would be released on 10Apr 2009, and the participants have amaximum of 4 days to submit their re-sults in the expected format.
(b) Only 1 ?standard?
run must be submit-ted from every group on a given lan-guage pair; more ?non-standard?
runs (0to 4) may be submitted.
In total, maxi-mum 5 runs (1 ?standard?
run plus up to4 ?non-standard?
runs) can be submit-ted from each group on a registered lan-guage pair.
(c) Any runs that are ?non-standard?
mustbe tagged as such.
(d) The test set is a list of names in sourcelanguage only.
Every group will pro-duce and submit a ranked list of translit-eration candidates in another languagefor each given name in the test set.Please note that this shared task is a?transliteration generation?
task, i.e.,given a name in a source language oneis supposed to generate one or moretransliterations in a target language.
Itis not the task of ?transliteration discov-ery?, i.e., given a name in the source lan-guage and a set of names in the targetlanguage evaluate how to find the ap-propriate names from the target set thatare transliterations of the given sourcename.5.
Results (29 April 2009)(a) On 29 April 2009, the evaluation resultswould be announced and will be madeavailable on the Workshop website.
(b) Note that only the scores (in respectivemetrics) of the participating systems oneach language pairs would be published,and no explicit ranking of the participat-ing systems would be published.
(c) Note that this is a shared evaluation taskand not a competition; the results aremeant to be used to evaluate systems oncommon data set with common metrics,and not to rank the participating sys-tems.
While the participants can cite theperformance of their systems (scores onmetrics) from the workshop report, theyshould not use any ranking informationin their publications.
(d) Further, all participants should agree notto reveal identities of other participantsin any of their publications unless youget permission from the other respectiveparticipants.
If the participants wantto remain anonymous in publishedresults, they should inform the or-ganisers (mzhang@i2r.a-star.edu.sg,a.kumaran@microsoft.com), at the timeof registration.
Note that the results oftheir systems would still be published,but with the participant identitiesmasked.
As a result, in this case, yourorganisation name will still appear inthe web site as one of participants, but itis not linked explicitly with your results.6.
Short Papers on Task (3 May 2009)(a) Each submitting site is required to sub-mit a 4-page system paper (short paper)for its submissions, including their ap-proach, data used and the results on ei-ther test set or development set or by n-fold cross validation on training set.
(b) All system short papers will be includedin the proceedings.
Selected short pa-pers will be presented orally in theNEWS 2009 workshop.
Reviewers?comments for all system short papersand the acceptance notification for thesystem short papers for oral presentationwould be announced on 1 June 2009 to-gether with that of other papers.
(c) All registered participants are requiredto register and attend the workshop tointroduce your work.
(d) All paper submission and reviewwill be managed electronicallythrough https://www.softconf.com/acl-ijcnlp09/NEWS/.4 Languages InvolvedThe tasks are to transliterate personal names orplace names from a source to a target language assummarised in Table 1.20Source language Target language Data Owner Approx.
Data Size Task IDEnglish Chinese Institute for Infocomm Research 30K EnChEnglish Japanese Katakana CJK Institute 25K EnJaEnglish Korean Hangul CJK Institute 7K EnKoJapanese name (in English) Japanese Kanji CJK Institute 20K JnJkEnglish Hindi Microsoft Research India 15K EnHiEnglish Tamil Microsoft Research India 15K EnTaEnglish Kannada Microsoft Research India 15K EnKaEnglish Russian Microsoft Research India 10K EnRuTable 1: Source and target languages for the shared task on transliteration.The names given in the training sets for Chi-nese, Japanese and Korean languages are Westernnames and their CJK transliterations; the JapaneseName (in English)?
Japanese Kanji data set con-sists only of native Japanese names.
The Indic dataset (Hindi, Tamil, Kannada) consists of a mix ofIndian and Western names.English?
ChineseTimothy????English?
Japanese KatakanaHarrington??????English?
Korean HangulBennett ?
?
?Japanese name in English?
Japanese KanjiAkihiro???English?
HindiSan Francisco ?
????????????????English?
TamilLondon ?
??????English?
KannadaTokyo ?
??????English?
RussianMoscow ?
?????
?5 Standard DatabasesTraining Data (Parallel)Paired names between source and target lan-guages; size 5K ?
40K.Training Data is used for training a basictransliteration system.Development Data (Parallel)Paired names between source and target lan-guages; size 1K ?
2K.Development Data is in addition to the Train-ing data, which is used for system fine-tuningof parameters in case of need.
Participantsare allowed to use it as part of training data.Testing DataSource names only; size 1K ?
3K.This is a held-out set, which would be usedfor evaluating the quality of the translitera-tions.1.
Participants will need to obtain licenses fromthe respective copyright owners and/or agreeto the terms and conditions of use that aregiven on the downloading website (Li et al,2004; Kumaran and Kellner, 2007; MSRI,2009; CJKI, 2009).
NEWS 2009 will pro-vide the contact details of each individualdatabase.
The data would be provided in Uni-code UTF-8 encoding, in XML format; theresults are expected to be submitted in XMLformat.
The XML formats will be announcedat the workshop website.2.
The data are provided in 3 sets as describedabove.3.
Name pairs are distributed as-is, as providedby the respective creators.
(a) While the databases are mostly man-ually checked, there may be still in-consistency (that is, non-standard usage,region-specific usage, errors, etc.)
or in-completeness (that is, not all right varia-tions may be covered).
(b) The participants may use any method tofurther clean up the data provided.i.
If they are cleaned up manually, weappeal that such data be providedback to the organisers for redistri-bution to all the participating groupsin that language pair; such sharingbenefits all participants, and further21ensures that the evaluation providesnormalisation with respect to dataquality.ii.
If automatic cleanup were used,such cleanup would be considered apart of the system fielded, and hencenot required to be shared with allparticipants.4.
We expect that the participants to use only thedata (parallel names) provided by the SharedTask for transliteration task for a ?standard?run to ensure a fair evaluation.
One such run(using only the data provided by the sharedtask) is mandatory for all participants for agiven language pair that they participate in.5.
If more data (either parallel names data ormonolingual data) were used, then all suchruns using extra data must be marked as?non-standard?.
For such ?non-standard?runs, it is required to disclose the size andcharacteristics of the data used in the systempaper.6.
A participant may submit a maximum of 5runs for a given language pair (including themandatory 1 ?standard?
run).6 Paper FormatPaper submissions to NEWS 2009 should followthe ACL-IJCNLP-2009 paper submission policy,including paper format, blind review policy and ti-tle and author format convention.
Full papers (re-search paper) are in two-column format withoutexceeding eight (8) pages of content plus one extrapage for references and short papers (task paper)are also in two-column format without exceedingfour (4) pages, including references.
Submissionmust conform to the official ACL-IJCNLP-2009style guidelines.
For details, please refer to thewebsite2.7 Evaluation MetricsWe plan to measure the quality of the translitera-tion task using the following 6 metrics.
We acceptup to 10 output candidates in a ranked list for eachinput entry.Since a given source name may have multiplecorrect target transliterations, all these alternativesare treated equally in the evaluation.
That is, any2http://www.acl-ijcnlp-2009.org/main/authors/stylefiles/index.htmlof these alternatives are considered as a correcttransliteration, and the first correct transliterationin the ranked list is accepted as a correct hit.The following notation is further assumed:N : Total number of names (sourcewords) in the test setni : Number of reference transliterationsfor i-th name in the test set (ni ?
1)ri,j : j-th reference transliteration for i-thname in the test setci,k : k-th candidate transliteration (systemoutput) for i-th name in the test set(1 ?
k ?
10)Ki : Number of candidate transliterationsproduced by a transliteration system1.
Word Accuracy in Top-1 (ACC) Alsoknown as Word Error Rate, it measures correct-ness of the first transliteration candidate in the can-didate list produced by a transliteration system.ACC = 1 means that all top candidates are cor-rect transliterations i.e.
they match one of the ref-erences, and ACC = 0 means that none of the topcandidates are correct.ACC =1NN?i=1{1 if ?ri,j : ri,j = ci,1;0 otherwise}(1)2.
Fuzziness in Top-1 (Mean F-score) Themean F-score measures how different, on average,the top transliteration candidate is from its closestreference.
F-score for each source word is a func-tion of Precision and Recall and equals 1 when thetop candidate matches one of the references, and0 when there are no common characters betweenthe candidate and any of the references.Precision and Recall are calculated based on thelength of the Longest Common Subsequence be-tween a candidate and a reference:LCS(c, r) =12(|c|+ |r| ?
ED(c, r)) (2)where ED is the edit distance and |x| is the lengthof x.
For example, the longest common subse-quence between ?abcd?
and ?afcde?
is ?acd?
andits length is 3.
The best matching reference, thatis, the reference for which the edit distance hasthe minimum, is taken for calculation.
If the bestmatching reference is given byri,m = argminj(ED(ci,1, ri,j)) (3)22then Recall, Precision and F-score for i-th wordare calculated asRi =LCS(ci,1, ri,m)|ri,m|(4)Pi =LCS(ci,1, ri,m)|ci,1|(5)Fi = 2Ri ?
PiRi + Pi(6)?
The length is computed in distinct Unicodecharacters.?
No distinction is made on different charactertypes of a language (e.g., vowel vs. conso-nants vs. combining diereses?
etc.)3.
Mean Reciprocal Rank (MRR) Measurestraditional MRR for any right answer produced bythe system, from among the candidates.
1/MRRtells approximately the average rank of the correcttransliteration.
MRR closer to 1 implies that thecorrect answer is mostly produced close to the topof the n-best lists.RRi ={minj 1j if ?ri,j , ci,k : ri,j = ci,k;0 otherwise}(7)MRR =1NN?i=1RRi (8)4.
MAPref Measures tightly the precision in then-best candidates for i-th source name, for whichreference transliterations are available.
If all ofthe references are produced, then the MAP is 1.Let?s denote the number of correct candidates forthe i-th source word in k-best list as num(i, k).MAPref is then given byMAPref =1NN?i1ni(ni?k=1num(i, k))(9)5.
MAP10 measures the precision in the 10-bestcandidates for i-th source name provided by thecandidate system.
In general, the higher MAP10is, the better is the quality of the transliterationsystem in capturing the multiple references.
Notethat the number of reference transliterations maybe more or less than 10.
If the number of refer-ence transliterations is below 10, then MAP10 cannever be equal to 1.
Only if the number of ref-erence transliterations for every source word is atleast 10, then MAP10 could possibly be equal to 1.MAP10 =1NN?i=1110(10?k=1num(i, k))(10)Note that in general MAPm measures the ?good-ness in m-best?
candidate list.
We use m = 10because we have asked the systems to produce upto 10 candidates for every source name in the testset.6.
MAPsys Measures the precision in the topKi-best candidates produced by the system for i-th source name, for which ni reference translit-erations are available.
This measure allows thesystems to produce variable number of translitera-tions, based on their confidence in identifying andproducing correct transliterations.
If all of the nireferences are produced in the top-ni candidates(that is, Ki = ni, and all of them are correct), thenthe MAPsys is 1.MAPsys =1NN?i=11Ki(Ki?k=1num(i, k))(11)8 Contact UsIf you have any questions about this share task andthe database, please email toDr.
Haizhou LiInstitute for Infocomm Research (I2R),A*STAR1 Fusionopolis Way#08-05 South Tower, ConnexisSingapore 138632hli@i2r.a-star.edu.sgDr.
A. KumaranMicrosoft Research IndiaScientia, 196/36, Sadashivnagar 2nd MainRoadBangalore 560080 INDIAa.kumaran@microsoft.comMr.
Kurt EasterwoodThe CJK Dictionary Institute (CJK Data)Komine Building (3rd & 4th floors)34-14, 2-chome, Tohoku, Niiza-shiSaitama 352-0001 JAPANakurt@cjki.org23ReferencesCJKI.
2009.
CJK Institute.
http://www.cjk.org/.A Kumaran and T. Kellner.
2007.
A generic frame-work for machine transliteration.
In Proc.
SIGIR,pages 721?722.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.In Proc.
42nd ACL Annual Meeting, pages 159?166,Barcelona, Spain.MSRI.
2009.
Microsoft Research India.http://research.microsoft.com/india.Appendix A: Training/Development Data?
File Naming Conventions:NEWS09 train XXYY nnnn.xmlNEWS09 dev XXYY nnnn.xmlNEWS09 test XXYY nnnn.xml?
XX: Source Language?
YY: Target Language?
nnnn: size of parallel/monolingualnames (?25K?, ?10000?, etc)?
File formats:All data will be made available in XML for-mats (Figure 1).?
Data Encoding Formats:The data will be in Unicode UTF-8 encod-ing files without byte-order mark, and in theXML format specified.Appendix B: Submission of Results?
File Naming Conventions:NEWS09 result XXYY gggg nn descr.xml?
XX: Source Language?
YY: Target Language?
gggg: Group ID?
nn: run ID.
Note that run ID ?1?
stands for ?stan-dard?
run where only the provided data are al-lowed to be used.
Run ID ?2?5?
means ?non-standard?
run where additional data can be used.?
descr: Description of the run.?
File formats:All data will be made available in XML formats (Fig-ure 2).?
Data Encoding Formats:The results are expected to be submitted in UTF-8 en-coded files without byte-order mark only, and in theXML format specified.24<?xml version="1.0" encoding="UTF-8"?><TransliterationCorpusCorpusID = "NEWS2009-Train-EnHi-25K"SourceLang = "English"TargetLang = "Hindi"CorpusType = "Train|Dev"CorpusSize = "25000"CorpusFormat = "UTF8"><Name ID=?1?><SourceName>eeeeee1</SourceName><TargetName ID="1">hhhhhh1_1</TargetName><TargetName ID="2">hhhhhh1_2</TargetName>...<TargetName ID="n">hhhhhh1_n</TargetName></Name><Name ID=?2?><SourceName>eeeeee2</SourceName><TargetName ID="1">hhhhhh2_1</TargetName><TargetName ID="2">hhhhhh2_2</TargetName>...<TargetName ID="m">hhhhhh2_m</TargetName></Name>...<!-- rest of the names to follow -->...</TransliterationCorpus>Figure 1: File: NEWS2009 Train EnHi 25K.xml25<?xml version="1.0" encoding="UTF-8"?><TransliterationTaskResultsSourceLang = "English"TargetLang = "Hindi"GroupID = "Trans University"RunID = "1"RunType = "Standard"Comments = "HMM Run with params: alpha=0.8 beta=1.25"><Name ID="1"><SourceName>eeeeee1</SourceName><TargetName ID="1">hhhhhh11</TargetName><TargetName ID="2">hhhhhh12</TargetName><TargetName ID="3">hhhhhh13</TargetName>...<TargetName ID="10">hhhhhh110</TargetName><!-- Participants to provide theirtop 10 candidate transliterations --></Name><Name ID="2"><SourceName>eeeeee2</SourceName><TargetName ID="1">hhhhhh21</TargetName><TargetName ID="2">hhhhhh22</TargetName><TargetName ID="3">hhhhhh23</TargetName>...<TargetName ID="10">hhhhhh110</TargetName><!-- Participants to provide theirtop 10 candidate transliterations --></Name>...<!-- All names in test corpus to follow -->...</TransliterationTaskResults>Figure 2: Example file: NEWS2009 EnHi TUniv 01 StdRunHMMBased.xml26
