Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 79?87,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPThe GREC Main Subject Reference Generation Challenge 2009:Overview and Evaluation ResultsAnja Belz Eric KowNLT GroupUniversity of BrightonBrighton BN2 4GJ, UK{asb,eykk10}@bton.ac.ukJette ViethenCentre for LTMacquarie UniversitySydney NSW 2109jviethen@ics.mq.edu.auAlbert GattComputing ScienceUniversity of AberdeenAberdeen AB24 3UE, UKa.gatt@abdn.ac.ukAbstractThe GREC-MSR Task at Generation Chal-lenges 2009 required participating systemsto select coreference chains to the mainsubject of short encyclopaedic texts col-lected from Wikipedia.
Three teams sub-mitted one system each, and we addition-ally created four baseline systems.
Sys-tems were tested automatically using ex-isting intrinsic metrics.
We also evaluatedsystems extrinsically by applying corefer-ence resolution tools to the outputs andmeasuring the success of the tools.
In ad-dition, systems were tested in an intrinsicevaluation involving human judges.
Thisreport describes the GREC-MSR Task andthe evaluation methods applied, gives briefdescriptions of the participating systems,and presents the evaluation results.1 IntroductionThe GREC-MSR Task is about how to generate ap-propriate references to an entity in the context of apiece of discourse longer than a sentence.
Ratherthan requiring participants to generate referringexpressions from scratch, the GREC-MSR data pro-vides sets of possible referring expressions for se-lection.
This was the second time we ran a sharedtask using the GREC-MSR data (following a firstrun in 2008).
The task definition was again keptfairly simple, but in the 2009 round the main aimfor participating systems was to select an appro-priate word string to serve as a referring expres-sion, whereas in 2008 it was to select an appropri-ate type of referring expression (name, commonnoun, pronoun, or empty reference).The immediate motivating application contextfor the GREC-MSR Task is the improvement of ref-erential clarity and coherence in extractive sum-maries by regenerating referring expressions inthem.
There has recently been a small flurryof work in this area (Steinberger et al, 2007;Nenkova, 2008).
In the longer term, the GREC-MSR Task is intended to be a step in the directionof the more general task of generating referentialexpressions in discourse context.The GREC-MSR data is an extension of theGREC 1.0 Corpus which had about 1,000 texts inthe subdomains of cities, countries, rivers and peo-ple (Belz and Varges, 2007a).
For the purpose ofthe GREC-MSR shared task, an additional 1,000texts in the new subdomain of mountain texts wereobtained and a new XML annotation scheme (Sec-tion 2.2) was developed.Team System NameUniversity of Delaware UDelICSI, Berkeley ICSI-CRFJadavpur University JUNLGTable 1: GREC-MSR?09 participating teams.Nine teams from seven countries registered forGREC-MSR?09, of which three teams (Table 1)submitted one system each.1 Participants had tosubmit their system reports before downloadingtest data inputs, and had to submit test data out-puts within 48 hours of downloading the test datainputs.
In addition to the participants?
systems,we also used the corpus texts themselves as ?sys-tem?
outputs, and created 4 baseline systems; weevaluated the resulting 8 systems using a range ofintrinsic and extrinsic evaluation methods (for de-tails see Sections 5 and 6).
This report presents theresults of all evaluations (Section 6), along withdescriptions of GREC-MSR data and task (Sec-tion 2), test sets (Section 3), evaluation methods(Section 4), and participating systems (Section 5).2 Data and TaskThe GREC Corpus (version 2.0) consists of about2,000 texts in total, all collected from introduc-1One team submitted by the original deadline (Jan. 2009),one by the revised deadline (1 June 2009), one slightly later.79tory sections in Wikipedia articles, in five differentsubdomains (cities, countries, rivers, people andmountains).
In each text, three broad categoriesof Main Subject Reference (MSR)2 have been an-notated, resulting in a total of about 13,000 anno-tated REs.
The GREC-MSR shared task version ofthe corpus was randomly divided into 90% train-ing data (of which 10% were randomly selected asdevelopment data) and 10% test data.
Participantsused the training data in developing their systems,and (as a minimum requirement) reported resultson the development data.2.1 Types of referential expression annotatedThree broad categories of main subject referringexpressions (MSREs) are annotated in the GRECcorpus3 ?
subject NPs, object NPs, and geni-tive NPs and pronouns which function as subject-determiners within their matrix NP.
These cate-gories of referring expressions (RE) are relativelystraightforward to identify and to achieve highinter-annotator agreement on (complete agree-ment among four annotators in 86% of MSRs), andaccount for most cases of overt main subject refer-ence in the GREC texts.
The annotators were askedto identify subject, object and genitive subject-determiners and decide whether or not they referto the main subject of the text.
More detail is pro-vided in Belz and Varges (2007b).In addition to the above, relative pronouns insupplementary relative clauses (as opposed to in-tegrated relative clauses, Huddleston and Pullum,2002, p. 1058) were annotated, e.g.
:(1) Stoichkov is a football manager and former strikerwho was a member of the Bulgaria national team thatfinished fourth at the 1994 FIFA World Cup.We also annotated ?non-realised?
subject MSREsin those cases of VP coordination where an MSREis the subject of the coordinated VPs, e.g.
:(2) He stated the first version of the Law of conservationof mass, introduced the Metric system, andhelped to reform chemical nomenclature.The motivation for annotating the approximateplace where the subject NP would be if it wererealised (the gap-like underscores above) is thatfrom a generation perspective there is a choice tobe made about whether to realise the subject NP inthe second and third coordinates or not.2The main subject of a Wikipedia article is simply taken tobe given by its title, e.g.
in the cities domain the main subject(and title) of one text is London.3In terminology and view of grammar the annotations relyheavily on Huddleston and Pullum (2002).2.2 XML formatFigure 1 is one of the texts distributed in theGREC-MSR training/development data set.
TheREF element indicates a reference, in the sense of?an instance of referring?
(which could, in princi-ple, be realised by gesture or graphically, as wellas by a string of words, or a combination of these).REFs have three attributes: ID, a unique refer-ence identifier; SEMCAT, the semantic category ofthe referent, ranging over city, country, river,person, mountain; and SYNCAT, the syntactic cate-gory required of referential expressions for the ref-erent in this discourse context (np-obj, np-subj,subj-det).
A REF is composed of one REFEX ele-ment (the ?selected?
referential expression for thegiven reference; in the training/development datatexts it is simply the referential expression foundin the corpus) and one ALT-REFEX element whichin turn is a list of REFEXs which are possible alter-native referential expressions (see following sec-tion).REFEX elements have four attributes.
TheHEAD attribute has the possible values nominal,pronoun, and rel-pron; the CASE attribute hasthe possible values nominative, accusative andgenitive for pronouns, and plain and genitivefor nominals.
The binary-valued EMPHATIC at-tribute indicates whether the RE is emphatic; inthe GREC-MSR corpus, the only type of RE thathas EMPHATIC=yes is one which incorporates a re-flexive pronoun used emphatically (e.g.
India it-self ).
The REG08-TYPE attribute indicates basic REtype.
The choice of types is motivated by the hy-pothesis that one of the most basic decisions to betaken in RE selection for named entities is whetherto use an RE that includes a name, such as Mod-ern India (the corresponding REG08-TYPE valueis name); whether to go for a common-noun RE,i.e.
with a category noun like country as the head(common); whether to use a pronoun (pronoun); orwhether it can be left unrealised (empty).2.3 The GREC-MSR TaskThe task for participating systems was to developa method for selecting one of the REFEXs in theALT-REFEX list, for each REF in each TEXT in thetest sets.
The test data inputs were identical tothe training/development data, except that REF el-ements contained only an ALT-REFEX list, not thepreceding ?selected?
REFEX.
ALT-REFEX lists aregenerated for each text by an automatic method80<?xml version="1.0" encoding="utf-8"?><!DOCTYPE TEXT SYSTEM "reg08-grec.dtd"><TEXT ID="36"><TITLE>Jean Baudrillard</TITLE><PARAGRAPH><REF ID="36.1" SEMCAT="person" SYNCAT="np-subj"><REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX><ALT-REFEX><REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="plain">Jean Baudrillard</REFEX><REFEX REG08-TYPE="name" EMPHATIC="yes" HEAD="nominal" CASE="plain">Jean Baudrillard himself</REFEX><REFEX REG08-TYPE="empty">_</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="nominative">he</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="pronoun" CASE="nominative">he himself</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="nominative">who</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="yes" HEAD="rel-pron" CASE="nominative">who himself</REFEX></ALT-REFEX></REF>(born June 20, 1929) is a cultural theorist, philosopher, political commentator,sociologist, and photographer.<REF ID="36.2" SEMCAT="person" SYNCAT="subj-det"><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">His</REFEX><ALT-REFEX><REFEX REG08-TYPE="name" EMPHATIC="no" HEAD="nominal" CASE="genitive">Jean Baudrillard?s</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="pronoun" CASE="genitive">his</REFEX><REFEX REG08-TYPE="pronoun" EMPHATIC="no" HEAD="rel-pron" CASE="genitive">whose</REFEX></ALT-REFEX></REF>work is frequently associated with postmodernism and post-structuralism.</PARAGRAPH></TEXT>Figure 1: Example text from the GREC-MSR Training Data.which collects all the (manually annotated) MSREsin a text including the title, and adds several de-faults: pronouns and reflexive pronouns in all sub-domains; and category nouns (e.g.
the river), inall subdomains except people.
The main objec-tive in the 2009 GREC-MSR Task was to get theword strings contained in REFEXs right (whereasin REG?08 it was the REG08-TYPE attributes).3 Test Data1.
Test Set C-1: a randomly selected 10% sub-set (183 texts) of the GREC corpus (with the sameproportions of texts in the 5 subdomains as in thetraining/testing data).2.
Test Set C-2: the same subset of texts as in C-1; however, for C-2 we did not use the MSREs inthe corpus, but replaced them with human-selectedalternatives.
These were obtained in an online ex-periment as described in Belz & Varges (2007a)where subjects selected MSREs in a setting that du-plicated the conditions in which the participatingsystems in the GREC-MSR Task make selections.4We obtained three versions of each text, where ineach version all MSREs were selected by the sameperson.
The motivation for this version of Test SetC was that having several human-produced chainsof MSREs to compare the outputs of participating(?peer?)
systems against is more reliable than hav-ing one only; and that Wikipedia texts are edited4The experiment can be tried out here: http://www.nltg.brighton.ac.uk/home/Anja.Belz/TESTDRIVE/by multiple authors which sometimes adverselyaffects MSR chains; we wanted to have additionalreference texts where all references are selected bya single author.3.
Test Set L: 74 Wikipedia introductory textsfrom the subdomain of lakes (there were no laketexts in the training/development set).4.
Test Set P: 31 short encyclopaedic texts inthe same 5 subdomains as in the GREC corpus,in approximately the same proportions as in thetraining/testing data, but of different origin.
Wetranscribed these texts from printed encyclopae-dias published in the 1980s which are not avail-able in electronic form.
The texts in this set aremuch shorter and more homogeneous than theWikipedia texts, and the sequences of MSRs fol-low very similar patterns.
It seems likely that it isthese properties that have resulted in better scoresoverall for Test Set P than for the other test setsin both the 2008 and 2009 runs of the GREC-MSRtask (for the latter, see Section 6).Each test set was designed to test peer systemsfor generalisation to different kinds of unseen data.Test Set C tests for generalisation to unseen ma-terial from the same corpus and the same subdo-mains as the training set; Test Set L tests for gen-eralisation to unseen material from the same cor-pus but different subdomain; and Test Set P forgeneralisation to a different corpus but the samesubdomains.814 Evaluation methods4.1 Automatic intrinsic evaluations5Accuracy of REFEX word strings: when com-puted against test sets (C-1, L and P), Word StringAccuracy is simply the proportion of REFEX wordstrings selected by a participating system that areidentical to the one in the corpus.
When computedagainst test set C-2, which has three versions ofeach text, Word String Accuracy is computed asfollows: first the number of correct REFEX wordstrings is computed at the text level for each of thethree versions of a text and the maximum of theseis determined; then the maximum text-level num-bers are summed and divided by the total numberof REFs in all the texts, which gives the globalWord String Accuracy score.
The rationale be-hind computing the Word String Accuracy scoresin this way for multiple-RE test sets (maximisingscores on RE chains rather than individual REs) isthat an RE is not good or bad in its own right, butdepends on other MSREs in the same text.Accuracy of REG08-Type: similarly to WordString Accuracy above, when computed againsttest sets C-1, L and P, REG08-Type Accuracy is theproportion of REFEXs selected by a participatingsystem that have a REG08-TYPE value identical tothe one in the corpus.
When computed against testset C-2, first the number of correct REG08-Types iscomputed at the text level for each of the three ver-sions of a corpus text and the maximum of theseis determined; then the maximum text-level num-bers are summed and divided by the total num-ber of REFs in all the texts, which gives the globalREG08-Type Accuracy score.String-edit distance metrics: String-edit dis-tance (SE) is straightforward Levenshtein distancewith a substitution cost of 2 and insertion/deletioncost of 1.
We also used a length-normalised ver-sion of string-edit distance (denoted ?norm.
SE?
inresults tables below).
For test sets C-1, L and P,the global score is simply the mean of all RE-levelscores.
For Test Set C-2, the global score is themean of the mean of the three text-level scores.Other metrics: BLEU is a precision metricfrom machine translation that assesses peer trans-lations in terms of the proportion of word n-grams5For GREC-MSR?09 we updated the tool that computes allautomatic intrinsic scores and in the course of this eliminateda character encoding issue; as a result the results for baselinesystems and corpus texts reported here are on the whole veryslightly higher than those reported for GREC-MSR?08.
(n ?
4 is standard) they share with several ref-erence translations.
We used BLEU-3 rather thanthe more standard BLEU-4 because most REs inthe corpus are less than 4 tokens long.
We alsoused the NIST version of BLEU which weights infavour of less frequent n-grams.
In both cases,we assessed just the MSREs selected by peer sys-tems (leaving out the surrounding text), and com-puted scores globally (rather than averaging overRE-level scores), as this is standard for these met-rics.
BLEU, and NIST are designed to work withone or multiple reference texts, so we did not needto use a different method for Test Set C-2.4.2 Automatic extrinsic evaluationAs in GREC-MSR?08, we used an automatic ex-trinsic evaluation method based on coreferenceresolution performance.6 The basic idea is that itseems likely that badly chosen reference chains af-fect the ability to resolve REs in automatic coref-erence resolution tools which will tend to performworse with poorly selected MSR reference chains.To counteract the possibility of results being afunction of a specific coreference resolution algo-rithm or tool, we used two different resolvers?those included in LingPipe7 and OpenNLP (Mor-ton, 2005)?and averaged results.There does not appear to be a single standardevaluation metric in the coreference resolutioncommunity, so we opted to use three: MUC-6(Vilain et al, 1995), CEAF (Luo, 2005), and B-CUBED (Bagga and Baldwin, 1998), which seemto be the most widely accepted metrics.
All threemetrics compute Recall, Precision and F-Scoreson aligned gold-standard and resolver-tool coref-erence chains.
They differ in how the alignmentis obtained and what components of coreferencechains are counted for calculating scores.
Resultsfor the automatic extrinsic evaluations are reportedbelow in terms of the F-Scores from these threemetrics, as well as in terms of their mean.4.3 Human intrinsic evaluationThe intrinsic human evaluation involved 24 ran-domly selected items from Test Set C and outputsfor these produced by peer and basline systems as6However, for GREC?09 we overhauled the tool; the cur-rent version no longer uses JavaRAP, and uses the most recentversions of the other resolvers; the GREC-MSR?08 and GREC-MSR?09 results for this method are not entirely comparablefor this reason.7http://alias-i.com/lingpipe/82Figure 2: Example of text presented in human intrinsic evaluation of GREC-MSR systems.well as those found in the original corpus texts(8 systems in total).
We used a Repeated LatinSquares design which ensures that each subjectsees the same number of outputs from each sys-tem and for each test set item.
There were three8x8 squares, and a total of 576 individual judg-ments in this evaluation (72 per system: 3 criteriax 3 articles x 8 evaluators).We recruited 8 native speakers of English fromamong post-graduate students currently doing alinguistics-related degree at University CollegeLondon (UCL) and University of Sussex.Following detailed instructions, subjects didtwo practice examples, followed by the 24 textsto be evaluated, in random order.
Subjects carriedout the evaluation over the internet, at a time andplace of their choosing.
They were allowed to in-terrupt and resume the experiment (though discor-ouged from doing so).
According to self-reportedtimings, subjects took between 25 and 45 minutesto complete the evaluation (not counting breaks).Figure 2 shows what subjects saw during theevaluation of an individual text.
All references tothe MS are highlighted in yellow, and the task is toevaluate the quality of the REs in terms of three cri-teria which were explained in the introduction asfollows (the wording of the explanations of Crite-ria 1 and 3 were taken from the DUC evaluations):1.
Referential Clarity: It should be easy to identify whoor what the referring expressions in the text are refer-ring to.
If a person or other entity is mentioned, itshould be clear what their role in the story is.
So, a ref-erence would be unclear if an entity is referenced, buttheir identity or relation to the story remains unclear.2.
Fluency: A referring expression should ?read well?, i.e.it should be written in good, clear English, and the useof titles and names etc.
should seem natural.
Note thatthe Fluency criterion is independent of the ReferentialClarity criterion: a reference can be perfectly clear, yetnot be fluent.3.
Structure and Coherence: The text should be wellstructured and well organised.
The text should not justbe a heap of related information, but should build fromsentence to sentence to a coherent body of informationabout a topic.
This criterion too is independent of theothers.Subjects selected evaluation scores by movingsliders (see Figure 2) along scales ranging from 1to 5.
Slider pointers started out in the middle ofthe scale (3).
These were continuous scales andwe recorded scores with one decimal place (e.g.3.2).
The meaning of the numbers was explainedin terms of integer scores (1=very poor, 2=poor,3=neither poor nor good, 4=good, 5=very good).5 SystemsBase-rand, Base-freq, Base-1st, Base-name:Baseline system Base-rand selects one of theREFEXs at random.
Base-freq selects the REFEXthat is the overall most frequent given the SYNCATand SEMCAT of the reference.
Base-1st al-ways selects the REFEX which appears first inthe ALT-REFEX list; and Base-name selects theshortest REFEX with attributes REG08-TYPE=name,HEAD=nominal and EMPHATIC=no.88Attributes are considered in this order.
If for one at-tribute, the right value is not found, the process ignores thatattribute and moves on the next one.83UDel: The UDel system consists of a prepro-cessing component performing sentence segmen-tation and identification of non-referring occur-rences of main subject (MS) names, an RE typeselection component (two C5.0 decision trees, oneoptimised for people and mountains, the other forthe other subdomains), and a word string selec-tion component.
The RE type selection decisiontrees use the following features: is the MS the sub-ject of the current, preceding and preceding butone sentence; was the last MSR in subject position;are there interfering references to other entities be-tween the current and the previous MSR; distanceto preceding non-referring occurrences of an MSname; sentence and reference IDs; other featuresindicating whether the reference occurred beforeand after certain words and punctuation marks.Given a selected RE type, the word-string selec-tion component selects the longest non-emphaticname for the first named reference in an article,and the shortest for subsequent named references;for other types, the first matching word-string isused, backing off to pronoun or name.ICSI-CRF: The ICSI-CRF system construes theGREC-MSR task as a sequence labelling task anddetermines the most likely current label given pre-ceding labels using a Conditional Random Fieldmodel trained using the follow features for the cur-rent, preceding and preceding but one MSR: pre-ceding and following word unigram and bigram;suffix of preceding and following word; precedingand following punctuation; reference ID; is this isthe beginning of a paragraph.
If more than one la-bel remains, the last in the list of possible REs inthe GREC-MSR data is selected.JUNLG: The JUNLG system is based on co-occurrence statistics between REF feature sets andREFEX feature sets as found in the GREC-MSR data.REF feature sets were augmented by a paragraphcounter and a within-paragraph REF counter.
Foreach given set of REF features, the system selectsthe most frequent REFEX feature set (as determinedfrom co-occurrence counts in the training data).
Ifthe current set of possible REFEXs does not includea REFEX with the selected feature set, then the sec-ond most likely feature set is selected.
Severalhand-coded default rules override the frequency-based selections, e.g.
if the preceding word is aconjunction, and the current SYNCAT is np-subj,then the REG08-Type is empty.6 ResultsThis section presents the results of all evalua-tion methods described in Section 4.
We startwith Word String Accuracy, the intrinsic auto-matic metric which participating teams were toldwas going to be the chief evaluation method, fol-lowed by REG08-Type Accuracy and other intrin-sic automatic metrics (Section 6.2), the intrinsichuman evaluation (Section 6.3) and the extrinsicautomatic evaluation (Section 6.4).System Word String Acc.
REG08-Type Acc.
Norm.
Edit Dist.ICSI-CRF 0.67 0.75 0.28UDel 0.6357 0.7027 0.3383JUNLG 0.532 0.62 0.421Table 2: Self-reported evaluation scores for devel-opment set.6.1 Word String AccuracyParticipants computed Word String Accuracy forthe development set (97 texts) themselves, usingan evaluation tool provided by us.
These scoresare shown in column 2 of Table 2, and are alsoincluded in the participants?
reports in this vol-ume.
Corresponding results for test set C-1 areshown in column 2 of Table 3.
Surprisingly, WordString Accuracy results on the test data are better(than on the development data) for the UDel andJUNLG systems.
Also included in this table are re-sults for the four baseline systems, and it is clearthat selecting the most frequent word string givenSEMCAT and SYNCAT (as done by the Base-freq sys-tem) provides a strong baseline.The other two parts of Table 3 contain results fortest sets L and P. As expected, results for Test Set Lare lower than for Test Set C-1, because in additionto consisting of unseen texts (like C-1), Test Set Lis also from an unseen subdomain (unlike C-1).The Word String Accuracy results for Test Set Pare higher than for any other set, probably for thereasons discussed at the end of Section 3.For each test set in Table 3 we carried out aunivariate ANOVA with System as the fixed factor,?Number of REFEXs in a text?
as a random factor,and Word String Accuracy as the dependent vari-able.
We found significant main effects of Sys-tem on Word String Accuracy at p < .001 in thecase of all three test sets (C-1: F(7,1272) = 90.058;L: F(7,440) = 44.139; P: F(7,168) = 21.991).9The columns containing capital letters in Table 39We included the corpus texts themselves in the analysis,hence 7 degrees of freedom (8 systems).84Test Set C-1 Test Set L Test Set PUDel 67.68 A UDel 52.89 A UDel 77.16 AICSI-CRF 62.98 A JUNLG 50.80 A ICSI-CRF 72.22 AJUNLG 61.94 A ICSI-CRF 49.20 A JUNLG 71.60 ABase-freq 47.05 B Base-name 21.06 B Base-freq 53.09 BBase-name 28.74 C Base-freq 20.74 B Base-name 27.78 CBase-1st 28.26 C Base-1st 20.74 B Base-1st 27.16 CBase-rand 18.95 D Base-rand 15.11 B Base-rand 18.52 CTable 3: Word String Accuracy scores against Test Sets C-1, L and P; homogeneous subsets (Tukey HSD,alpha = .05) for each test set (systems that do not share a letter are significantly different).System Word String Accuracy for multiple-RE Test Set C-2All Cities Countries Rivers People MountainsCorpus 71.58 A 65.25 69.11 76.47 80.40 66.87UDel 70.22 A B 68.09 71.20 76.47 76.63 64.84JUNLG 64.57 B C 54.61 51.83 73.53 71.86 65.85ICSI-CRF 63.69 C 58.87 56.54 64.71 72.11 60.98Base-freq 57.01 D 51.06 57.07 58.82 63.82 53.05Base-name 40.21 E 51.06 46.07 29.41 29.90 43.90Base-1st 39.65 E 47.52 41.88 38.24 25.63 47.97Base-rand 26.99 F 28.37 29.32 23.53 21.61 30.28Table 4: Word String Accuracy scores against Test Set C-2 for complete set and for subdomains; homo-geneous subsets (Tukey HSD, alpha = .05) for complete set only (systems that do not share a letter aresignificantly different).show the homogeneous subsets of systems as de-termined by post-hoc Tukey HSD comparisons ofmeans.
Systems whose Word String Accuracyscores are not significantly different (at the .05level) share a letter.The results for Word String Accuracy com-puted against Test Set C-2 are shown in Table 4.These should be considered the chief results of theGREC-MSR?09 Task evaluations, as stated in theparticipants?
guidelines.
Here too we performeda univariate ANOVA with System as the fixed fac-tor, Number of REFEXs as the random factor andWord String Accuracy as the dependent variable.There was a significant main effect of System(F(7,1272) = 74.892, p < .001).
We compared themean scores with Tukey?s HSD.
As can be seenfrom the resulting homogeneous subsets, there isno significant difference between the corpus texts(C-1) and the UDel system, but also there is nosignificant difference between the latter and theJUNLG system.
In this analysis, all peer systemsoutperform all baselines; the Base-freq baselineoutperforms all other baselines; and Base-nameand Base-1st outperform the random baseline.Overall, there is a marked improvement in WordString Accuracy compared to GREC-MSR?08where peer systems?
scores ranged from 50.72 to65.61.6.2 Other automatic intrinsic metricsIn addition to the chief evaluation measure re-ported on in the preceding section, we computedREG08-Type Accuracy and the string similaritymetrics described in Section 4.1.
The resultingscores for Test Set C-2 are shown in Table 5 (re-call that in Test Set C-2 corpus texts are evalu-ated against 3 texts with human-selected alterna-tive REs).
The corpus texts again receive the bestscores across the board.
Ranks for peer systemsare very similar to those reported in the last sec-tion.We performed a univariate ANOVA with Sys-tem as the fixed factor, Number of REFEXs as therandom factor, and REG08-Type Accuracy as thedependent variable.
The main effect of Systemwas F(7,1272) = 75.040, p < .001; the homoge-neous subsets resulting from the Tukey HSD post-hoc analysis are shown in columns 3?5 of Table 5.The differences between the scores of the peer sys-tems and the corpus texts were not found to be sig-nificant.6.3 Human-assessed intrinsic measuresTable 6 shows the results of the human intrinsicevaluation.
In each of the three parts of the ta-ble (showing the results for Fluency, Clarity andCoherence, respectively) systems are ordered interms of their mean scores (shown in the secondcolumn of each part of the table).
We first es-tablished that the main effect of Evaluator wasweak (F between 2.1 and 2.6) on Fluency, Clar-ity and Coherence, and only of borderline signifi-cance (just below .05); and that the interaction be-tween System and Evaluator was very weak and85System Other similarity measures for Triple-RE Test Set C-2REG08-Type BLEU-3 NIST SE norm.
SECorpus 79.30 A 0.77 5.60 1.04 0.34UDel 77.71 A 0.74 5.32 1.11 0.37JUNLG 75.40 A 0.53 4.69 1.34 0.40ICSI-CRF 75.16 A 0.54 4.68 1.32 0.41Base-freq 62.50 B 0.54 4.30 1.93 0.50Base-name 51.04 C 0.46 4.76 1.80 0.63Base-1st 50.32 C 0.39 4.42 1.93 0.63Base-rand 48.09 C 0.26 3.02 2.30 0.72Table 5: REG08-Type Accuracy, BLEU, NIST and string-edit scores, computed on test set C-2 (systemsin order of REG08-Type Accuracy); homogeneous subsets (Tukey HSD, alpha = .05) for REG08-TypeAccuracy only (systems that do not share a letter are significantly different).Fluency Clarity CoherenceCorpus 4.43 A Base-name 4.62 A Corpus 4.40 AUDel 4.27 A Corpus 4.56 A JUNLG 4.33 AJUNLG 4.26 A JUNLG 4.50 A UDel 4.27 A BICSI-CRF 4.15 A B ICSI-CRF 4.45 A ICSI-CRF 4.02 A BBase-freq 3.33 B C UDel 4.35 A Base-freq 3.96 A BBase-name 2.84 C D Base-1st 4.27 A Base-name 3.85 A BBase-1st 2.76 C D Base-freq 4.10 A Base-1st 3.7 A BBase-rand 2.15 D Base-rand 3.18 B Base-rand 3.46 BTable 6: Clarity, Fluency and Coherence scores (with homogeneous subsets) for all systems.not significant in the case of Clarity and Coher-ence, and borderline significant in the case of Flu-ency.
We then ran a (non-factorial) multivariateANOVA, with Fluency, Coherence and Clarity asthe dependent variables, and (just) System as thefixed factor.
The main effect of System was asfollows: Fluency: F(7,128) = 20.444, p < 0.001;Clarity: F(7,128) = 5.248, p < 0.001; Coherence:F(7,128) = 2.680, p < 0.012.
The homogeneoussubsets resulting from a post-hoc Tukey analysisare shown in the letter columns in Table 6.The effect of System was strongest on Fluency;here, the system ranks are also the same as forWord String Accuracy and REG08-Type Accuracyfor Test Set C-2.
This, together with the fairamount of significant differences found, indicatesthat the evaluators were able to make sense of theFluency criterion and that there were interestingdifferences between systems under this criterion.However, differences between the three peer sys-tems were not significant.For Clarity, there were no significant differ-ences among the peer systems and non-randombaseline systems; all of these were significantlybetter than the random baseline.
Base-name hadthe highest mean Clarity score, possibly becausealways chosing the name of an entity when refer-ring to it ensures high referential clarity.The Coherence results are perhaps the most dif-ficult to interpret.
Both the main effect of Systemon Coherence and its significance were weakerthan for Fluency and Clarity.
Only two signifi-cant pairwise differences were found: Corpus andJUNLG were better than the random baseline.
Thesystem ranks are roughly the same as for Fluency,but the mean scores cover a smaller range (from3.46 to 4.4) than in the case of either of the othertwo criteria.
Overall, the Coherence results proba-bly indicate that the evaluators found it somewhatdifficult to make sense of the Coherence criterion.Computing Pearson?s r for the three criteriaon individual (text-level) scores showed that therewere only moderate correlations between them (allaround r = 0.5) which were all significant at?
= 0.05.
This gives some indication that theevaluators were able to assess the three criteria in-dependently from each other.6.4 Automatic extrinsic measuresWe fed the outputs of all eight systems throughthe two coreference resolvers, and computed meanMUC, CEAF and B-CUBED F-Scores as describedin Section 4.2.
The second column in Table 7shows the mean of these three F-Scores, to givea single overall result for this evaluation method.A univariate ANOVA with mean F-Score as the de-pendent variable and System as the fixed factorrevealed a significant main effect of System onmean F-Score (F(7,1456) = 73.061, p < .001).A post-hoc comparison of the means (Tukey HSD,alpha = .05) found the significant differences in-dicated by the homogeneous subsets in columns3?4 (Table 7).
The numbers shown in the lastthree columns are the separate MUC, CEAF and B-CUBED F-Scores for each system, averaged overthe two resolver tools.
ANOVAs revealed the fol-86lowing effects of System on the separate scoringmethods: on CEAF F(7,1456) = 43.471, p < .001;on MUC: F(7,1456) =, p < .001; on B-CUBED:F(7,1456) = 38.574, p < .001.
All three scor-ing methods separately and their mean yielded thesame significant differences (as shown in columns3?4 of Table 7).The three F-Score measures (MUC, CEAF and B-CUBED) are all significantly correlated (p < .001,2-tailed).
However it is not a strong correlation,with Pearson?s correlation coefficient around 0.5.System (MUC+CEAF+B3)/3 MUC CEAF B3Base-name 65.19 A 62.35 63.14 70.06Base-1st 63.77 A 59.95 62.08 69.28Base-freq 63.14 A 59.08 62.04 68.3UDel 46.19 B 34.85 46.86 56.86ICSI-CRF 44.47 B 31.61 45.58 56.21JUNLG 44.19 B 31.27 45.21 56.10Base-rand 42.99 B 30.24 43.04 55.7Corpus 42.52 B 29.53 43.57 54.47Table 7: MUC, CEAF and B-CUBED F-Scores forall systems; homogeneous subsets (Tukey HSD),alpha = .05, for mean of F-Scores.6.5 CorrelationsWhen assessed on the system-level scores and us-ing Pearson?s r, all evaluation methods above werestrongly and significantly correlated with eachother (at the 0.01 level, 2-tailed), with the fol-lowing exceptions.
Clarity was not significantlycorrelated with any of the other methods exceptNIST (r = .902, p < .01); apart from this, NISTwas only correlated with Word String Accuracy ontest set C-2, with non-normalised string-edit dis-tance, Fluency and Coherence, moreover all at theweaker 0.05 level.
Finally, the extrinsic methodwas not correlated with any of the intrinsic meth-ods (and in fact showed signs of being negativelycorrelated with all of them except Clarity).7 Concluding RemarksThe GREC-MSR Task is still a relatively new tasknot only for an NLG shared-task challenge, but alsoas a research task in general (post-processing ex-tractive summaries in order to improve their qual-ity seems to be just taking off as a research sub-field).
There was substantial interest in the GREC-MSR Task this year (as indicated by the nine teamsthat originally registered).
However, only threeteams were ultimately able to participate.We continued the traditions of previous NLGshared tasks in that we used a wide range of eval-uation metrics to obtain a well-rounded view ofthe quality of the participating systems.
This in-cluded intrinsic human evaluations for the firsttime.
However, we decided against an extrinsichuman evaluation this year, given time constraintsas well as the fact that this evaluation type yieldedbarely any significant results last year.Overall, there was an improvement in systemperformance compared to last year, to the pointwhere the performance of the top system wasbarely distinguishable from the human topline.We are not currently planning to run the GREC-MSR task again next year.AcknowledgmentsMany thanks to the UCL and Sussex students whoparticipated in the intrinsic evaluation experiment.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scor-ing coreference chains.
In Proceedings of the Lin-guistic Coreference Workshop at LREC?98, pages563?566.A.
Belz and S. Varges.
2007a.
Generation of repeatedreferences to discourse entities.
In Proceedings ofENLG?07, pages 9?16.A.
Belz and S. Varges.
2007b.
The GREC corpus:Main subject reference in context.
Technical ReportNLTG-07-01, University of Brighton.R.
Huddleston and G. Pullum.
2002.
The CambridgeGrammar of the English Language.
Cambridge Uni-versity Press.X.
Luo.
2005.
On coreference resolution performancemetrics.
Proc.
of HLT-EMNLP, pages 25?32.T.
Morton.
2005.
Using Semantic Relations to ImproveInformation Retrieval.
Ph.D. thesis, University ofPensylvania.A.
Nenkova.
2008.
Entity-driven rewrite for multi-document summarization.
In Proceedings of IJC-NLP?08.L.
Qiu, M. Kan, and T.-S. Chua.
2004.
A public refer-ence implementation of the rap anaphora resolutionalgorithm.
In Proceedings of LREC?04, pages 291?294.J.
Steinberger, M. Poesio, M. Kabadjov, and K. Jezek.2007.
Two uses of anaphora resolution in summa-rization.
Information Processing and Management:Special issue on Summarization, 43(6):1663?1680.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic corefer-ence scoring scheme.
Proceedings of MUC-6, pages45?52.87
