Proceedings of the 5th Workshop on Important Unresolved Matters, pages 9?16,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsPerceptron Training for a Wide-Coverage Lexicalized-Grammar ParserStephen ClarkOxford University Computing LaboratoryWolfson Building, Parks RoadOxford, OX1 3QD, UKstephen.clark@comlab.ox.ac.ukJames R. CurranSchool of Information TechnologiesUniversity of SydneyNSW 2006, Australiajames@it.usyd.edu.auAbstractThis paper investigates perceptron trainingfor a wide-coverage CCG parser and com-pares the perceptron with a log-linear model.The CCG parser uses a phrase-structure pars-ing model and dynamic programming in theform of the Viterbi algorithm to find thehighest scoring derivation.
The difficulty inusing the perceptron for a phrase-structureparsing model is the need for an efficient de-coder.
We exploit the lexicalized nature ofCCG by using a finite-state supertagger todo much of the parsing work, resulting ina highly efficient decoder.
The perceptronperforms as well as the log-linear model; ittrains in a few hours on a single machine;and it requires only a few hundred MB ofRAM for practical training compared to 20GB for the log-linear model.
We also inves-tigate the order in which the training exam-ples are presented to the online perceptronlearner, and find that order does not signifi-cantly affect the results.1 IntroductionA recent development in data-driven parsing is theuse of discriminative training methods (Riezler etal., 2002; Taskar et al, 2004; Collins and Roark,2004; Turian and Melamed, 2006).
One popular ap-proach is to use a log-linear parsing model and max-imise the conditional likelihood function (Johnsonet al, 1999; Riezler et al, 2002; Clark and Curran,2004b; Malouf and van Noord, 2004; Miyao andTsujii, 2005).
Maximising the likelihood involvescalculating feature expectations, which is computa-tionally expensive.
Dynamic programming (DP) inthe form of the inside-outside algorithm can be usedto calculate the expectations, if the features are suf-ficiently local (Miyao and Tsujii, 2002); however,the memory requirements can be prohibitive, es-pecially for automatically extracted, wide-coveragegrammars.
In Clark and Curran (2004b) we use clus-ter computing resources to solve this problem.Parsing research has also begun to adopt discrim-inative methods from the Machine Learning litera-ture, such as the perceptron (Freund and Schapire,1999; Collins and Roark, 2004) and the large-margin methods underlying Support Vector Ma-chines (Taskar et al, 2004; McDonald, 2006).Parser training involves decoding in an iterative pro-cess, updating the model parameters so that the de-coder performs better on the training data, accord-ing to some training criterion.
Hence, for efficienttraining, these methods require an efficient decoder;in fact, for methods like the perceptron, the updateprocedure is so trivial that the training algorithm es-sentially is decoding.This paper describes a decoder for a lexicalized-grammar parser which is efficient enough for prac-tical discriminative training.
We use a lexicalizedphrase-structure parser, the CCG parser of Clark andCurran (2004b), together with a DP-based decoder.The key idea is to exploit the properties of lexi-calized grammars by using a finite-state supertag-ger prior to parsing (Bangalore and Joshi, 1999;Clark and Curran, 2004a).
The decoder still usesthe CKY algorithm, so the worst case complexity of9the parsing is unchanged; however, by allowing thesupertagger to do much of the parsing work, the effi-ciency of the decoder is greatly increased in practice.We chose the perceptron for the training algo-rithm because it has shown good performance onother NLP tasks; in particular, Collins (2002) re-ported good performance for a perceptron taggercompared to a Maximum Entropy tagger.
LikeCollins (2002), the decoder is the same for both theperceptron and the log-linear parsing models; theonly change is the method for setting the weights.The perceptron model performs as well as the log-linear model, but is considerably easier to train.Another contribution of this paper is to advancewide-coverage CCG parsing.
Previous discrimina-tive models for CCG (Clark and Curran, 2004b) re-quired cluster computing resources to train.
In thispaper we reduce the memory requirements from 20GB of RAM to only a few hundred MB, but with-out greatly increasing the training time or reducingparsing accuracy.
This provides state-of-the-art CCGparsing with a practical development environment.More generally, this work provides a practicalenvironment for experimenting with discriminativemodels for phrase-structure parsing; because thetraining time for the CCG parser is relatively short(a few hours), experiments such as comparing alter-native feature sets can be performed.
As an example,we investigate the order in which the training exam-ples are presented to the perceptron learner.
Sincethe perceptron training is an online algorithm ?
up-dating the weights one training sentence at a time?
the order in which the data is processed affectsthe resulting model.
We consider random ordering;presenting the shortest sentences first; and present-ing the longest sentences first; and find that the orderdoes not significantly affect the final results.We also use the random orderings to investigatemodel averaging.
We produced 10 different models,by randomly permuting the data, and averaged theweights.
Again the averaging was found to have noimpact on the results, showing that the perceptronlearner ?
at least for this parsing task ?
is robustto the order of the training examples.The contributions of this paper are as follows.First, we compare perceptron and log-linear parsingmodels for a wide-coverage phrase-structure parser,the first work we are aware of to do so.
Second,we provide a practical framework for developingdiscriminative models for CCG, reducing the mem-ory requirements from over 20 GB to a few hundredMB.
And third, given the significantly shorter train-ing time compared to other discriminative parsingmodels (Taskar et al, 2004), we provide a practicalframework for investigating discriminative trainingmethods more generally.2 The CCG ParserClark and Curran (2004b) describes the CCG parser.The grammar used by the parser is extracted fromCCGbank, a CCG version of the Penn Treebank(Hockenmaier, 2003).
The grammar consists of 425lexical categories, expressing subcategorisation in-formation, plus a small number of combinatory ruleswhich combine the categories (Steedman, 2000).
AMaximum Entropy supertagger first assigns lexicalcategories to the words in a sentence, which arethen combined by the parser using the combinatoryrules and the CKY algorithm.
A log-linear modelscores the alternative parses.
We use the normal-form model, which assigns probabilities to singlederivations based on the normal-form derivations inCCGbank.
The features in the model are definedover local parts of the derivation and include word-word dependencies.
A packed chart representationallows efficient decoding, with the Viterbi algorithmfinding the most probable derivation.The supertagger is a key part of the system.
Ituses a log-linear model to define a distribution overthe lexical category set for each word and the previ-ous two categories (Ratnaparkhi, 1996) and the for-ward backward algorithm efficiently sums over allhistories to give a distibution for each word.
Thesedistributions are then used to assign a set of lexicalcategories to each word (Curran et al, 2006).Supertagging was first defined for LTAG (Banga-lore and Joshi, 1999), and was designed to increaseparsing speed for lexicalized grammars by allow-ing a finite-state tagger to do some of the parsingwork.
Since the elementary syntactic units in a lexi-calized grammar ?
in LTAG?s case elementary treesand in CCG?s case lexical categories ?
contain a sig-nificant amount of grammatical information, com-bining them together is easier than the parsing typi-cally performed by phrase-structure parsers.
Hence10Bangalore and Joshi (1999) refer to supertagging asalmost parsing.Supertagging has been especially successful forCCG: Clark and Curran (2004a) demonstrates theconsiderable increases in speed that can be obtainedthrough use of a supertagger.
The supertagger in-teracts with the parser in an adaptive fashion.
Ini-tially the supertagger assigns a small number of cat-egories, on average, to each word in the sentence,and the parser attempts to create a spanning analysis.If this is not possible, the supertagger assigns morecategories, and this process continues until a span-ning analysis is found.
The number of categories as-signed to each word is determined by a parameter ?in the supertagger: all categories are assigned whoseforward-backward probabilities are within ?
of thehighest probability category (Curran et al, 2006).Clark and Curran (2004a) also shows how the su-pertagger can reduce the size of the packed charts toallow discriminative log-linear training.
However,even with the use of a supertagger, the packed chartsfor the complete CCGbank require over 20 GB ofRAM.
Reading the training instances into memoryone at a time and keeping a record of the relevantfeature counts would be too slow for practical de-velopment, since the log-linear model requires hun-dreds of iterations to converge.
Hence the packedcharts need to be stored in memory.
In Clark andCurran (2004b) we use a cluster of 45 machines, to-gether with a parallel implementation of the BFGStraining algorithm, to solve this problem.The need for cluster computing resources presentsa barrier to the development of further CCG pars-ing models.
Hockenmaier and Steedman (2002) de-scribe a generative model for CCG, which only re-quires a non-iterative counting process for training,but it is generally acknowledged that discrimina-tive models provide greater flexibility and typicallyhigher performance.
In this paper we propose theperceptron algorithm as a solution.
The perceptronis an online learning algorithm, and so the param-eters are updated one training instance at a time.However, the key difference compared with the log-linear training is that the perceptron converges inmany fewer iterations, and so it is practical to readthe training instances into memory one at a time.The difficulty in using the perceptron for trainingphrase-structure parsing models is the need for anefficient decoder (since perceptron training essen-tially is decoding).
Here we exploit the lexicalizednature of CCG by using the supertagger to restrict thesize of the charts over which Viterbi decoding is per-formed, resulting in an extremely effcient decoder.In fact, the decoding is so fast that we can estimate astate-of-the-art discriminative parsing model in onlya few hours on a single machine.3 Perceptron TrainingThe parsing problem is to find a mapping from a setof sentences x ?
X to a set of parses y ?
Y .
Weassume that the mapping F is represented through afeature vector ?
(x, y) ?
Rd and a parameter vector?
?
Rd in the following way (Collins, 2002):F (x) = argmaxy?GEN(x)?
(x, y) ?
?
(1)where GEN(x) denotes the set of possible parses forsentence x and ?
(x, y) ?
?
=?i ?i?i(x, y) is theinner product.
The learning task is to set the parame-ter values (the feature weights) using the training setas evidence, where the training set consists of ex-amples (xi, yi) for 1 ?
i ?
N .
The decoder is analgorithm which finds the argmax in (1).In this paper, Y is the set of possible CCG deriva-tions and GEN(x) enumerates the set of derivationsfor sentence x.
We use the same feature representa-tion ?
(x, y) as in Clark and Curran (2004b), to allowcomparison with the log-linear model.
The featuresare defined in terms of local subtrees in the deriva-tion, consisting of a parent category plus one ortwo children.
Some features are lexicalized, encod-ing word-word dependencies.
Features are integer-valued, counting the number of times some configu-ration occurs in a derivation.GEN(x) is defined by the CCG grammar, plus thesupertagger, since the supertagger determines howmany lexical categories are assigned to each wordin x (through the ?
parameter).
Rather than try torecreate the adaptive supertagging described in Sec-tion 2 for training, we simply fix the the value of ?
sothat GEN(x) is the set of derivations licenced by thegrammar for sentence x, given that value.
?
is nowa parameter of the training process which we deter-mine experimentally using development data.
The ?parameter can be thought of as determining the setof incorrect derivations which the training algorithm11uses to ?discriminate against?, with a smaller valueof ?
resulting in more derivations.3.1 Feature ForestsThe same decoder is used for both training and test-ing: the Viterbi algorithm.
However, the packedrepresentation of GEN(x) in each case is different.When running the parser, a lot of grammatical in-formation is stored in order to produce linguisticallymeaningful output.
For training, all that is requiredis a packed representation of the features on eachderivation in GEN(x) for each sentence in the train-ing data.
The feature forests described in Miyao andTsujii (2002) provide such a representation.Clark and Curran (2004b) describe how a set ofCCG derivations can be represented as a feature for-est.
The feature forests are created by first buildingpacked charts for the training sentences, and thenextracting the feature information.
Packed chartsgroup together equivalent chart entries.
Entries areequivalent when they interact in the same mannerwith both the generation of subsequent parse struc-ture and the numerical parse selection.
In prac-tice, this means that equivalent entries have the samespan, and form the same structures and generate thesame features in any further parsing of the sentence.Back pointers to the daughters indicate how an indi-vidual entry was created, so that any derivation canbe recovered from the chart.A feature forest is essentially a packed chart withonly the feature information retained (see Miyao andTsujii (2002) and Clark and Curran (2004b) for thedetails).
Dynamic programming algorithms can beused with the feature forests for efficient estimation.For the log-linear parsing model in Clark and Cur-ran (2004b), the inside-outside algorithm is used tocalculate feature expectations, which are then usedby the BFGS algorithm to optimise the likelihoodfunction.
For the perceptron, the Viterbi algorithmfinds the features corresponding to the highest scor-ing derivation, which are then used in a simple addi-tive update process.3.2 The Perceptron AlgorithmThe training algorithm initializes the parameter vec-tor as all zeros, and updates the vector by decodingthe examples.
Each feature forest is decoded withthe current parameter vector.
If the output is incor-Inputs: training examples (xi, yi)Initialisation: set ?
= 0Algorithm:for t = 1..T , i = 1..Ncalculate zi = argmaxy?GEN(xi) ?
(xi, y) ?
?if zi 6= yi?
= ?
+?
(xi, yi)?
?
(xi, zi)Outputs: ?Figure 1: The perceptron training algorithmrect, the parameter vector is updated by adding thefeature vector of the correct derivation and subtract-ing the feature vector of the decoder output.
Train-ing typically involves multiple passes over the data.Figure 1 gives the algorithm, where N is the numberof training sentences and T is the number of itera-tions over the data.For all the experiments in this paper, we used theaveraged version of the perceptron.
Collins (2002)introduced the averaged perceptron, as a way of re-ducing overfitting, and it has been shown to performbetter than the non-averaged version on a number oftasks.
The averaged parameters are defined as fol-lows: ?s =?t=1...T,i=1...N ?t,is /NT where ?t,is isthe value of the sth feature weight after the tth sen-tence has been processed in the ith iteration.A naive implementation of the averaged percep-tron updates the accumulated weight for each fea-ture after each example.
However, the number offeatures whose values change for each example is asmall proportion of the total.
Hence we use the al-gorithm described in Daume III (2006) which avoidsunnecessary calculations by only updating the accu-mulated weight for a feature fs when ?s changes.4 ExperimentsThe feature forests were created as follows.
First,the value of the ?
parameter for the supertagger wasfixed (for the first set of experiments at 0.004).
Thesupertagger was then run over the sentences in Sec-tions 2-21 of CCGbank.
We made sure that ev-ery word was assigned the correct lexical categoryamong its set (we did not do this for testing).
Thenthe parser was run on the supertagged sentences, us-ing the CKY algorithm and the CCG combinatoryrules.
We applied the same normal-form restrictionsused in Clark and Curran (2004b): categories can12only combine if they have been seen to combine inSections 2-21 of CCGbank, and only if they do notviolate the Eisner (1996a) normal-form constraints.This part of the process requires a few hundred MBof RAM to run the parser, and takes a few hours forSections 2-21 of CCGbank.
Any further trainingtimes or memory requirements reported do not in-clude the resources needed to create the forests.The feature forests are extracted from the packedchart representation used in the parser.
We only usea feature forest for training if it contains the correctderivation (according to CCGbank).
Some forestsdo not have the correct derivation, even though weensure the correct lexical categories are present, be-cause the grammar used by the parser is missingsome low-frequency rules in CCGbank.
The to-tal number of forests used for the experiments was35,370 (89% of Sections 2-21) .
Only features whichoccur at least twice in the training data were used,of which there are 477,848.
The complete set offorests used to obtain the final perceptron results inSection 4.1 require 21 GB of disk space.The perceptron is an online algorithm, updatingthe weights after each forest is processed.
Each for-est is read into memory one at a time, decoding isperformed, and the weight values are updated.
Eachforest is discarded from memory after it has beenused.
Constantly reading forests off disk is expen-sive, but since the perceptron converges in so fewiterations the training times are reasonable.In contrast, log-linear training takes hundreds ofiterations to converge, and so it would be impracticalto keep reading the forests off disk.
Also, since log-linear training uses a batch algorithm, it is more con-venient to keep the forests in memory at all times.In Clark and Curran (2004b) we use a cluster of 45machines, together with a parallel implementationof BFGS, to solve this problem, but need up to 20 GBof RAM.The feature forest representation, and our imple-mentation of it, is so compact that the perceptrontraining requires only 20 MB of RAM.
Since the su-pertagger has already removed much of the practicalparsing complexity, decoding one of the forests isextremely quick, and much of the training time istaken with continually reading the forests off disk.However, the training time for the perceptron is stillonly around 5 hours for 10 iterations.model RAM iterations time (mins)perceptron 20 MB 10 312log-linear 19 GB 475 91Table 1: Training requirements for the perceptronand log-linear modelsTable 1 compares the training for the perceptronand log-linear models.
The perceptron was run for10 iterations and the log-linear training was run toconvergence.
The training time for 10 iterations ofthe perceptron is longer than the log-linear training,although the results in Section 4.1 show that the per-ceptron typically converges in around 4 iterations.The striking result in the table is the significantlysmaller memory requirement for the perceptron.4.1 ResultsTable 2 gives the first set of results for the averagedperceptron model.
These were obtained using Sec-tion 00 of CCGbank as development data.
Gold-standard POS tags from CCGbank were used for allthe experiments.
The parser provides an analysis for99.37% of the sentences in Section 00.
The F-scoresare based only on the sentences for which there isan analysis.
Following Clark and Curran (2004b),accuracy is measured using F-score over the gold-standard predicate-argument dependencies in CCG-bank.
The table shows that the accuracy increasesinitially with the number of iterations, but convergesquickly after only 4 iterations.
The accuracy afteronly one iteration is also surprisingly high.Table 3 compares the accuracy of the perceptronand log-linear models on the development data.
LPis labelled precision, LR is labelled recall, and CATis the lexical category accuracy.
The same featureforests were used for training the perceptron andlog-linear models, and the same parser and decodingalgorithm were used for testing, so the results for thetwo models are directly comparable.
The only dif-ference in each case was the weights file used.1The table also gives the accuracy for the percep-tron model (after 6 iterations) when a smaller valueof the supertagger ?
parameter is used during the1Both of these models have parameters which have beenoptimised on the development data, in the log-linear case theGaussian smoothing parameter and in the perceptron case thenumber of training iterations.13iteration 1 2 3 4 5 6 7 8 9 10F-score 85.87 86.28 86.33 86.49 86.46 86.51 86.47 86.52 86.53 86.54Table 2: Accuracy on the development data for the averaged perceptron (?
= 0.004)model LP LR F CATlog-linear?=0.004 87.02 86.07 86.54 93.99perceptron?=0.004 87.11 85.98 86.54 94.03perceptron?=0.002 87.25 86.20 86.72 94.08Table 3: Comparison of the perceptron and log-linear models on the development dataforest creation (with the number of training itera-tions again optimised on the development data).
Asmaller ?
value results in larger forests, giving moreincorrect derivations for the training algorithm to?discriminate against?.
Increasing the size of theforests is no problem for the perceptron, since thememory requirements are so modest, but this wouldcause problems for the log-linear training which isalready highly memory intensive.
The table showsthat increasing the number of incorrect derivationsgives a small improvement in performance for theperceptron.Table 4 gives the accuracies for the two modelson the test data, Section 23 of CCGbank.
Here thecoverage of the parser is 99.63%, and again the ac-curacies are computed only for the sentences withan analysis.
The figures for the averaged perceptronwere obtained using 6 iterations, with ?
= 0.002.The perceptron slightly outperforms the log-linearmodel (although we have not carried out signifi-cance tests).
We justify the use of different ?
valuesfor the two models by arguing that the perceptron ismuch more flexible in terms of the size of the train-ing forests it can handle.Note that the important result here is that the per-ceptron model performs at least as well as the log-linear model.
Since the perceptron is considerablyeasier to train, this is a useful finding.
Also, sincethe log-linear parsing model is a Conditional Ran-dom Field (CRF), the results suggest that the percep-tron should be compared with a CRF for other tasksfor which the CRF is considered to give state-of-the-art results.model LP LR F CATlog-linear?=0.004 87.39 86.51 86.95 94.07perceptron?=0.002 87.50 86.62 87.06 94.08Table 4: Comparison of the perceptron and log-linear models on the test data5 Order of Training ExamplesAs an example of the flexibility of our discrimina-tive training framework, we investigated the order inwhich the training examples are presented to the on-line perceptron learner.
These experiments were par-ticularly easy to carry out in our framework, sincethe 21 GB file containing the complete set of trainingforests can be sampled from directly.
We stored theposition on disk of each of the forests, and selectedthe forests one by one, according to some order.The first set of experiments investigated orderingthe training examples by sentence length.
Buttery(2006) found that a psychologically motivated Cate-gorial Grammar learning system learned faster whenthe simplest linguistic examples were presented first.Table 5 shows the results both when the shortest sen-tences are presented first and when the longest sen-tences are presented first.
Training on the longestsentences first provides the best performance, but isno better than the standard ordering.For the random ordering experiments, forestswere randomly sampled from the complete 21 GBtraining file on disk, without replacement.
Thenew forests file was then used for the averaged-perceptron training, and this process was repeated9 times.The number of iterations for each training run wasoptimised in terms of the accuracy of the resultingmodel on the development data.
There was littlevariation among the models, with the best modelscoring 86.84% F-score on the development dataand the worst scoring 86.63%.
Table 6 shows thatthe performance of this best model on the test datais only slightly better than the model trained usingthe CCGbank ordering.14iteration 1 2 3 4 5 6Standard order 86.14 86.30 86.53 86.61 86.69 86.72Shortest first 85.98 86.41 86.57 86.56 86.54 86.53Longest first 86.25 86.48 86.66 86.72 86.74 86.75Table 5: F-score of the averaged perceptron on the development data for different data orderings (?
= 0.002)perceptron model LP LR F CATstandard order 87.50 86.62 87.06 94.08best random order 87.52 86.72 87.12 94.12averaged 87.53 86.67 87.10 94.09Table 6: Comparison of various perceptron modelson the test dataFinally, we used the 10 models (including themodel from the original training set) to investigatemodel averaging.
Corston-Oliver et al (2006) mo-tivate model averaging for the perceptron in termsof Bayes Point Machines.
The averaged percep-tron weights resulting from each permutation of thetraining data were simply averaged to produce a newmodel.
Table 6 shows that the averaged model againperforms only marginally better than the originalmodel, and not as well as the best-performing ?ran-dom?
model, which is perhaps not surprising giventhe small variation among the performances of thecomponent models.In summary, the perceptron learner appears highlyrobust to the order of the training examples, at leastfor this parsing task.6 Comparison with Other WorkTaskar et al (2004) investigate discriminative train-ing methods for a phrase-structure parser, and alsouse dynamic programming for the decoder.
The keydifference between our work and theirs is that theyare only able to train on sentences of 15 words orless, because of the expense of the decoding.There is work on discriminative models for de-pendency parsing (McDonald, 2006); since thereare efficient decoding algorithms available (Eisner,1996b), complete resources such as the Penn Tree-bank can used for estimation, leading to accurateparsers.
There is also work on discriminative mod-els for parse reranking (Collins and Koo, 2005).
Themain drawback with this approach is that the correctparse may get lost in the first phase.The existing work most similar to ours is Collinsand Roark (2004).
They use a beam-search decoderas part of a phrase-structure parser to allow practicalestimation.
The main difference is that we are ableto store the complete forests for training, and canguarantee that the forest contains the correct deriva-tion (assuming the grammar is able to generate itgiven the correct lexical categories).
The downsideof our approach is the restriction on the locality ofthe features, to allow dynamic programming.
Onepossible direction for future work is to compare thesearch-based approach of Collins and Roark withour DP-based approach.In the tagging domain, Collins (2002) comparedlog-linear and perceptron training for HMM-styletagging based on dynamic programming.
Our workcould be seen as extending that of Collins since wecompare log-linear and perceptron training for a DP-based wide-coverage parser.7 ConclusionInvestigation of discriminative training methods isone of the most promising avenues for breakingthe current bottleneck in parsing performance.
Thedrawback of these methods is the need for an effi-cient decoder.
In this paper we have demonstratedhow the lexicalized nature of CCG can be used todevelop a very efficient decoder, which leads to apractical development environment for discrimina-tive training.We have also provided the first comparison of aperceptron and log-linear model for a wide-coveragephrase-structure parser.
An advantage of the percep-tron over the log-linear model is that it is consider-ably easier to train, requiring 1/1000th of the mem-ory requirements and converging in only 4 iterations.Given that the global log-linear model used here(CRF) is thought to provide state-of-the-art perfor-mance for many NLP tasks, it is perhaps surprising15that the perceptron performs as well.
The evalua-tion in this paper was based solely on CCGbank, butwe have shown in Clark and Curran (2007) that theCCG parser gives state-of-the-art performance, out-performing the RASP parser (Briscoe et al, 2006)by over 5% on DepBank.
This suggests the need formore comparisons of CRFs and discriminative meth-ods such as the perceptron for other NLP tasks.AcknowledgementsJames Curran was funded under ARC Discoverygrants DP0453131 and DP0665973.ReferencesSrinivas Bangalore and Aravind Joshi.
1999.
Supertagging:An approach to almost parsing.
Computational Linguistics,25(2):237?265.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.
Thesecond release of the RASP system.
In Proceedings ofthe Interactive Demo Session of COLING/ACL-06, Sydney,Austrailia.Paula Buttery.
2006.
Computational models for first languageacquisition.
Technical Report UCAM-CL-TR-675, Univer-sity of Cambridge Computer Laboratory.Stephen Clark and James R. Curran.
2004a.
The importance ofsupertagging for wide-coverage CCG parsing.
In Proceed-ings of COLING-04, pages 282?288, Geneva, Switzerland.Stephen Clark and James R. Curran.
2004b.
Parsing the WSJusing CCG and log-linear models.
In Proceedings of the42nd Meeting of the ACL, pages 104?111, Barcelona, Spain.Stephen Clark and James R. Curran.
2007.
Formalism-independent parser evaluation with CCG and DepBank.
InProceedings of the 45th Annual Meeting of the ACL, Prague,Czech Republic.Michael Collins and Terry Koo.
2005.
Discriminative rerank-ing for natural language parsing.
Computational Linguistics,31(1):25?69.Michael Collins and Brian Roark.
2004.
Incremental parsingwith the perceptron algorithm.
In Proceedings of the 42ndMeeting of the ACL, pages 111?118, Barcelona, Spain.Michael Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments with per-ceptron algorithms.
In Proceedings of the 40th Meeting ofthe ACL, Philadelphia, PA.S.
Corston-Oliver, A. Aue, K. Duh, and E. Ringger.
2006.
Mul-tilingual dependency parsing using bayes point machines.
InProceedings of HLT/NAACL-06, New York.James R. Curran, Stephen Clark, and David Vadas.
2006.Multi-tagging for lexicalized-grammar parsing.
In Proceed-ings of COLING/ACL-06, pages 697?704, Sydney, Aus-trailia.Jason Eisner.
1996a.
Efficient normal-form parsing for Com-binatory Categorial Grammar.
In Proceedings of the 34thMeeting of the ACL, pages 79?86, Santa Cruz, CA.Jason Eisner.
1996b.
Three new probabilistic models for de-pendency parsing: An exploration.
In Proceedings of the16th COLING Conference, pages 340?345, Copenhagen,Denmark.Yoav Freund and Robert E. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
Machine Learn-ing, 37(3):277?296.Julia Hockenmaier and Mark Steedman.
2002.
Generativemodels for statistical parsing with Combinatory CategorialGrammar.
In Proceedings of the 40th Meeting of the ACL,pages 335?342, Philadelphia, PA.Julia Hockenmaier.
2003.
Data and Models for StatisticalParsing with Combinatory Categorial Grammar.
Ph.D. the-sis, University of Edinburgh.Mark Johnson, Stuart Geman, Stephen Canon, Zhiyi Chi, andStefan Riezler.
1999.
Estimators for stochastic ?unification-based?
grammars.
In Proceedings of the 37th Meeting of theACL, pages 535?541, University of Maryland, MD.Robert Malouf and Gertjan van Noord.
2004.
Wide coverageparsing with stochastic attribute value grammars.
In Pro-ceedings of the IJCNLP-04 Workshop: Beyond shallow anal-yses - Formalisms and statistical modeling for deep analyses,Hainan Island, China.Ryan McDonald.
2006.
Discriminative Training and SpanningTree Algorithms for Dependency Parsing.
Ph.D. thesis, Uni-versity of Pennsylvania.Yusuke Miyao and Jun?ichi Tsujii.
2002.
Maximum entropyestimation for feature forests.
In Proceedings of the HumanLanguage Technology Conference, San Diego, CA.Yusuke Miyao and Jun?ichi Tsujii.
2005.
Probabilistic dis-ambiguation models for wide-coverage HPSG parsing.
InProceedings of the 43rd meeting of the ACL, pages 83?90,University of Michigan, Ann Arbor.Adwait Ratnaparkhi.
1996.
A maximum entropy part-of-speech tagger.
In Proceedings of the EMNLP Conference,pages 133?142, Philadelphia, PA.Stefan Riezler, Tracy H. King, Ronald M. Kaplan, RichardCrouch, John T. Maxwell III, and Mark Johnson.
2002.Parsing the Wall Street Journal using a Lexical-FunctionalGrammar and discriminative estimation techniques.
In Pro-ceedings of the 40th Meeting of the ACL, pages 271?278,Philadelphia, PA.Mark Steedman.
2000.
The Syntactic Process.
The MIT Press,Cambridge, MA.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Manning.2004.
Max-margin parsing.
In Proceedings of the EMNLPconference, pages 1?8, Barcelona, Spain.Joseph Turian and I. Dan Melamed.
2006.
Advances in dis-criminative parsing.
In Proceedings of COLING/ACL-06,pages 873?880, Sydney, Australia.16
