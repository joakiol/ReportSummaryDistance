Automated Construction of Database Interfaces: IntegratingStatistical and Relational Learning for Semantic ParsingLappoon R .
Tang and  Raymond J .
MooneyDepartment of Computer SciencesUniversity of Texas at AustinAustin, TX 78712-1188{rupert, mooney}@cs, utexas, eduAbst ractThe development of natural language inter-faces (NLI's) for databases has been a chal-lenging problem in natural anguage process-ing (NLP) since the 1970's.
The need forNLI's has become more pronounced due to thewidespread access to complex databases nowavailable through the Internet.
A challengingproblem for empirical NLP is the automatedacquisition of NLI's from training examples.We present a method for integrating statisti-cal and relational learning techniques for thistask which exploits the strength of both ap-proaches.
Experimental results from three dif-ferent domains uggest that such an approachis more robust than a previous purely logic-based approach.1 In t roduct ionWe use the term semantic parsing to referto the process of mapping a natural anguagesentence to a structured meaning representa-tion.
One interesting application of semanticparsing is building natural language interfacesfor online databases.
The need for such appli-cations is growing since when information isdelivered through the Internet, most users donot know the underlying database access lan-guage.
An example of such an interface thatwe have developed is shown in Figure 1.Traditional (rationalist) approaches to con-structing database interfaces require an ex-pert to hand-craft an appropriate semanticparser (Woods, 1970; Hendrix et al, 1978).However, such hand-crafted parsers are timeconsllming to develop and suffer from prob-lems with robustness and incompleteness evenfor domain specific applications.
Neverthe-less, very little research in empirical NLP hasexplored the task of automatically acquiringsuch interfaces from annotated training ex-amples.
The only exceptions of which weare aware axe a statistical approach to map-ping airline-information queries into SQL pre-sented in (Miller et al, 1996), a probabilisticdecision-tree method for the same task de-scribed in (Kuhn and De Mori, 1995), andan approach using relational learning (a.k.a.inductive logic programming, ILP) to learn alogic-based semantic parser described in (Zelleand Mooney, 1996).The existing empirical systems for this taskemploy either a purely logical or purely sta-tistical approach.
The former uses a deter-ministic parser, which can suffer from someof the same robustness problems as rational-ist methods.
The latter constructs a prob-abilistic grammar, which requires supplyinga sytactic parse tree as well as a semanticrepresentation foreach training sentence, andrequires hand-crafting a small set of contex-tual features on which to condition the pa-rameters of the model.
Combining relationaland statistical approaches can overcome theneed to supply parse-trees and hand-craftedfeatures while retaining the robustness of sta-tistical parsing.
The current work is basedon the CHILL logic-based parser-acquisitionframework (Zelle and Mooney, 1996), retain-ing access to the complete parse state for mak-ing decisions, but building a probabilistic re-lational model that allows for statistical pars-ing-2 Overv iew of  the  ApproachThis section reviews our overall approachusing an interface developed for a U.S.Geography database (Geoquery) as asample application (ZeUe and Mooney,1996) which is available on the Web (seehl:tp://gvg, cs .
u tezas ,  edu/users/n~./geo .html).2.1 Semant ic  Representat ionFirst-order logic is used as a semantic repre-sentation language.
CHILL has also been ap-plied to a restaurant database in which thelogical form resembles SQL, and is translated133Damba~QUERY YOU PO~TED:all a goo~ ~ z~caL~ ~m ~o ~.t~o'PRE~UI.T:~ o o a ~ e ~  I,~ p .~.
, .~r  ~,~o~o ~ J~u~,,o~ ",,~u.,.
p~o ~.~.
, ,?Bo  ~ ~,.~.o ,~.~o ~.~ Ia~ooo~z~u~r~ ~ ~rr~r  ~,~o~.~o ~ ITHE SOL GENERATED:~n0~t ~.K~t  ~Fo, LOCAnONSC~*t l~rOJ t~ ~ Z3 AgOFigure 1: Screenshots ofa Learned Web-based NL Database Interfaceautomatically into SQL (see Figure 1).
Weexplain the features of the Geoquery repre-sentation language through a sample query:Input: "W'hat is the largest city in Texas?
"Quc~'y: a nswer(C,largest(C,(city(C),loc(C,S),const (S,stateid (texas))))).Objects are represented as logical terms andare typed with a semantic category usinglogical functions applied to possibly ambigu-ous English constants (e.g.
stateid(Mississippi),riverid(Mississippi)).
Relationships between ob-jects are expressed using predicates; for in-stance, Ioc(X,Y) states that X is located in Y.We also need to handle quantifiers uchas 'largest'.
We represent these using meta-predicates for which at least one argument isaconjunction ofliterals.
For example, largest(X,Goal) states that the object X satisfies Goaland is the largest object that does so, usingthe appropriate measure of size for objects ofits type (e.g.
area for states, population forcities).
Finally, an nn.qpeci~ed object requiredas an argument to a predicate can appear else-where in the sentence, requiring the use of thepredicate const(X,C) to bind the variable X tothe constant C. Some other database queries(or training examples) for the U.S. Geographydomain are shown below:What is the capital of Texas?a nswer(C,(ca pital(C,S),const(S,stateid (texas)))).What state has the most rivers running through it?a nswer(S,most (S,R,(state(S),rlver(R),traverse(R,S)))).2.2 Parsing ActionsOur semantic parser employs a shift-reducearchitecture that maintains a stack of pre-viously built semantic constituents and abuffer of remaining words in the input.
Theparsing actions are automatically generatedfrom templates given the training data.
Thetemplates are INTRODUCE, COREF_VABS,DROP_CON J, LIFT_CON J, and SttIFT.
IN-TRODUCE pushes apredicate onto the stackbased on a word appearing in the input andinformation about its possible meanings inthe lexicon.
COREF_VARS binds two argu-ments of two different predicates on the stack.DROP_CONJ  (or L IFT_CON J) takes a pred-icate on the stack and puts it into one of thearguments of a meta-predicate on the stack.SH IFT  simply pushes a word from the inputbuffer onto the stack.
The parsing actions aretried in exactly this order.
The parser alsorequires a lexicon to map phrases in the in-put into specific predicates, this lexicon canalso be learned automatically from the train-ing data (Thompson and Mooney, 1999).Let's go through a simple trace of parsingthe request "What is the capital of Texas?
"A lexicon that maps 'capital' to 'capital(_,_)'and 'Texas' to 'const(_,stateid(texas))' su.~ces134here.
Interrogatives like "what" may bemapped to predicates in the lexicon if neces-sary.
The parser begins with an initial stackand a buffer holding the input sentence.
Eachpredicate on the parse stack has an attachedbuffer to hold the context in which it wasintroduced; words from the input sentenceare shifted onto this buffer during parsing.The initial parse state is shown below:Parse Stack: \[answer(_,_):O\]Input Buffer: \[what,is,the,ca pital,of,texas,?\]Since the first three words in the inputbuffer do not map to any predicates, threeSHIFT actions are performed.
The next is anINTRODUCE as 'capital' is at the head ofthe input buffer:Parse Stack: \[capital(_,_): O, answer(_,_):\[the,is,what\]\]Input Buffer: \[capital,of,texas,?\]The next action is a COREF_VARS thatbinds the first argument of capital(_,_) withthe first argument of answer(_,_).Parse Stack: \[capital(C,_): O, answer(C,_):\[the,is,what\]\]Input Buffer: \[capital,of,texas,?\]The next sequence of steps axe two SHIFT's,an INTRODUCE,  and then a COR.EF_VARS:Parse Stack: \[const(S,stateid(texas)): 0'ca pital(C,S):\[of, ca pital\],answer(C,_):\[the,is,what~Input Buffer: \[texas,?\]The last four steps are two DROP_CONJ'sfollowed by two SHIFT's:Parse Stack: \[answer(C, (capital(C,S),const(S,stateld(texas)))):\[?,texas,of, ca pital,the,is,what\]\]Input Buffer: I\]This is the final state and the logical query isextracted from the stack.2.3 Learning Control RulesThe initially constructed parser has no con-straints on when to apply actions, and istherefore overly general and generates n11rner-ous spurious parses.
Positive and negative ex-amples are collected for each action by parsingeach tralnlng example and recordlng the parsestates encountered.
Parse states to which anaction should be applied (i.e.
the action leadsto building the correct semantic representa-tion) are labeled positive examples for thataction.
Otherwise, a parse state is labeled anegative xample for an action if it is a posi-tive example for another action below the cur-rent one in the ordered list of actions.
Controlconditions which decide the correct action fora given parse state axe learned for each actionfrom these positive and negative xamples.The initial CHILL system used ILP (Lavracand Dzeroski, 1994) to learn Prolog controlrules and employed eterministic parsing, us-ing the learned rules to decide the appropriateparse action for each state.
The current ap-proach learns a model for estimating the prob-ability that each action should be applied toa given state, and employs tatistical parsing(Manning and Schiitze, 1999) to try to findthe overall most probable parse, using beamsearch to control the complexity.
The advan-tage of ILP is that it can perform inductionover the logical description of the completeparse state without he need to pre-engineer afixed set of features (which vary greatly fromone domain to another) that are relevant omaking decisions.
We maintain this advan-tage by using ILP to learn a committee ofhypotheses, and basing probability estimateson a weighted vote of them (Ali and Pazzani,1996).
We believe that using such a proba-bilistic relational model (Getoor and Jensen,2000) combines the advantages of frameworksbased on first-order logic and those based onstandard statistical techniques.3 The  TABULATE ILP  MethodThis section discusses the ILP method used tobuild a committee of logical control hypothe-ses for each action.3.1 The Basic TABULATE AlgorithmMost ILP methods use a set-covering methodto learn one clause (rule) at a time and con-struct clauses using either a strictly top-down(general to specific) or bottom-up (specific togeneral) search through the space of possi-ble rules (Lavrac and Dzeroski, 1994).
TAB-ULATE, 1 on the other hand, employs bothbottom-up and top-down methods to con-struct potential clauses and searches throughthe hypothesis pace of complete logic pro-grams (i.e.
sets of clauses called theories).
Ituses beam search to find a set of alternativehypotheses guided by a theory evaluation met-ric discussed below.
The search starts withaTABULATB stands for Top-doera And Bottom-UpcLAuse construction urith Theory Evaluation.135Procedure TabulateInput:t(X,,.
.
.
,Xn): the target concept to learn~+: the (B examples~-: the (9 examplesOutput:Q: a queue of learned theoriesTheoryo := {E '?
'-I E E ~+} /* the initial theory */T(No) := Theoryo /* theory of node No */C(No) := empty /* the clause being built */Q := \[No\] /* the search queue */RepeatCO ?Fo_._X each search node Ni E Q D__qC(Ni) = empty or C(Ni) = fail ThenPairs := sampling of S pairs of clauses from T(N~)Find LGG G in Pairs with the greatest cover in ~+Ri := Refine_Clause(t(X1,... ,Xn) +-) URefine_Clause( G ~--)ElseR4 := Reflne_Clause(C(Ni))End I fI f  Ri ---- ?
ThenCQ, := {(T(N,), fail)}ElseCQi := {(Coraplete(T(N,), Gj, ~+), neztj) \[for each G~ E ,,~, next~ = empty if Gjsatisfies the noise criteria; otherwise, G$}End I fCQ :=  CQ u CQ~End ForQ := the B best nodes from Q U CQranked by metric MUnt i l  terminatlon-criteria-satisfiedReturn QEnd ProcedureFigure 2: The TABULATE algorithmthe most specific hypothesis (the set of posi-tive examples each represented as a separateclause).
Each iteration of the loop attemptsto refine each of the hypotheses in the currentsearch queue.
There are two cases in each it-eration: 1) an existing clause in a theory isrefined or 2) a new clause is begun.
Clausesare learned using both top-down specialiT.~-tion using a method similar to FOIL (Quin-lan, 1990) and bottom-up generalization usingLeast General Generalizations (LGG's).
Ad-vantages of combining both ILP approacheswere explored in CHILLIN (ZeUe and Mooney,1994), an ILP method which motivated thedesign of TABULATE.
An outline of the TAB-ULATE algorithm is given in Figure 2.A noise-handling criterion is used to de-cide when an individual clause in a hypoth-esis is sufficiently accurate to be permanentlyretained.
There are three possible outcomesin a refinement: 1) the current clause satisfiesthe noise-handling criterion and is simply re-turned (nextj is set to empty), 2) the currentclause does not satisfy the noise-handling cri-teria and all possible refinements are returned(neztj is set to the refined clause), and 3)the current clause does not satisfy the noise-handling criterion but there are no further e-finements (neztj is set to fai O.
If the refine-ment is a new clause, clauses in the currenttheory subs-reed by it are removed.
Oth-erwise, it is a specialization of an existingclause.
Positive examples that are not cov-ered by the resulting theory, due to special-izing the clause, are added back into theoryas individual clauses.
Hence, the theory is al-ways maintained complete (i.e.
covering allpositive examples).
These final steps are per-formed in the Complete procedure.The termination criterion checks for twoconditions.
The first is satisfied if the nextsearch queue does not improve the sum ofthe metric score over all hypotheses in thequeue.
Second, there is no clause currentlybeing built for each theory in the search queueand the last finished clause of each theory sat-isfies the noise-handling criterion.
Finally, acommittee of hypotheses found by the algo-rithm is returned.3.2  Compress ion  and  AccuracyThe goal of the search is to find accurateand yet simple hypotheses.
We measure accu-racy using the m-estimate (Cestnik, 1990), asmoothed measure of accuracy on the trainingdata which in the case of a two-class problemis defined as:accuracy(H) s + m.  p+ = (1)n ,-I- rrtwhere s is the n-tuber of positive examplescovered by the hypothesis H,  n is the totalnumber of examples covered, p+ is the priorprobability of the class (9, and m is a smooth-ing parameter.We measure theory complexity using a met-ric slmi\]ar to that introduced in (Muggletonand Buntine, 1988).
The size of a Clause hav-ing a Head and a Body is defined as follows(ts="term size" and ar="ar i ty ' ) :size(Clause) = 1 + ts(Head) + ts(Body) (2)136I 1 T is a variablets(T) = 2 r ~,, ?o~t2 + ts(argi(T))(3)The size of a clause is roughly the n,,mber ofvariables, constants, or predicate symbols itcontains.
The size of a theory is the sum ofthe sizes of its clauses.
The metric M(H) usedas the search heuristic is defined as:M(H) = accuracy(H) + Clog 2 size(H) (4)where C is a constant used to control the rel-ative weight of accuracy vs. complexity.
Weass~,me that the most general hypothesis i asgood as the most specific hypothesis; thus, Cis determined to be:C = EbSt -- EtSb (5)&-&where Et, Eb are the accuracy estimates of themost general and most specific hypotheses re-spectively, and St, Sb are their sizes.3.3 Noise Handl ingA clause needs no further refinement when itmeets the following criterion (as in RIPPER(Cohen, 1995)):P -.__.2_ > (6)p+nwhere p is the number of positive examplescovered by the clause, n is the number of neg-ative examples covered and -1  </~ _< 1 is aparameter.
The value of ~ is decreased when-ever the sum of the metric over the hypothesesin the queue does not improve although someof them still have ,nflni~hed or failed clauses.4 Statistical Semantic Parsing4.1 The  Pars ing  Mode lA parser is a relation Parser C_ Sentences xQueries where Sentences and Queries arethe sets of natural language sentences anddatabase queries respectively.
Given a sen-tence I ?
Sentences, the set Q(1) = {q ?Queries I (l, q) ?
Parser} is the set of queriesthat are correct interpretations of I.A parse state consists of a stack of lexical-ized predicates and a list of words from theinput sentence.
S is the set of states reach-able by the parser.
Suppose our learned parserhas n different parsing actions, the ith ac-tion a / i s  a function a/(s) : ISi -+ OSi whereISi G S is the set of states to which the ac-tion is applicable and OSi C_ S is the set ofstates constructed by the action.
The functionao(l) : Sentences ~ IniS maps each sentence lto a corresponding unique initial parse state inIn/S C_ S. A state is called afinalstate if thereexists no parsing action applicable to it.
Thepartial function a,+l(s) : FS ~ Queries isdefined as the action that retrieves the queryfrom the final state s 6 FS C S if one exists.Some final states may not "contain" a query(e.g.
when the parse stack contain.q predicateswith unbound ~rariables) and therefore it is apartial function.
When the parser meets sucha final state, it reports a failure.A path is a finite sequence of parsing ac-tions.
Given a sentence 1, a good state s isone such that there exists a path from it to aquery q 6 Q(1).
Otherwise, it is a bad state.The set of parse states can be uniquely dividedinto the set of good states and the set of badstates given l and Parser.
S + and S-  are thesets of good and bad states respectively.Given a sentence l, the goal is to constructthe query ~ such that= argmqaX P(q ?
Q(l) \ [ l  ~ q) (7)where I ~ q means a path exists from l to q.Now, we need to estimate P(q ?
Q(1) I l =-~q).
First, we notice that:P(q ?
Q(1) \[l =~.
q) ---- (8)P(s ?
FS + I I ~ s and an+l(S) ---- q)where FS + = FS N S +.
For notational con-venience we drop the conditions and denotethe above probabilities as P(q ?
Q(l)) andP(s ?
FS +) respectively, assuming these con-ditions in the following discussion.
The equa-tion states that the probability that a givenquery is a correct meaning for I is the same asthe probability that the final state (reachedby parsing l) is a good state.
We need to de-termine in general the probability of having agood resulting parse state.
Given any parsestate s i at the j th  step of parsing and an ac-tion ai such that si+1 = a/(sj), we have:PCsi+1 ?
(9)pCsj+l e o& + I ?
x&+)pCs  ?
x& +) +P(Si+l ?
OSi + I sj ?
ISi+)P(sj ?~ ISi +)where IS~ = ISi N S + and OS~ = OS~ N S +.Since no parsing action can produce a good137parse state from a bad one, the second termis zero.
Now, we are ready to derive P(q ?Q(l)).
Suppose q = an+l(Sm), we have:P(q 6 Q(l)) (10)= P(s~ ?
F~).
.
.= P(s,n ?
FS  + l sm-1 ?/St,_a)...P(s~ ?
OS~_, I sj-1 ?
I s~_,) .
.
.P(s2 ?
Ob~, Is1 ?
IS~, )P ( ' I  ?
IS~,)where ak denotes the index of which actionis applied at the kth step.
We assume that= P(sl  ?
I~aa) ~ 0 (which may not be truein general).
Now, we havem--IP(q 6 Q(l)) = 7 I I  P(sj+I ?
O~ l sj ?
IS~-3).i=l(11)Next we describe how we estimate the proba-bi l i~ of the goodness of each action in a givenstate (P(~(s) ?
o$  I s ?
I~) ) .
We n~not estimate 7 since its value does not affectthe outcome of equation (7).4.2 Es t imat ing  Probab i l i t i es  forPars ing  Act ionsThe committee of hypotheses learned by TAB-ULATE is used to est imate the probability thata particular action is a good one to apply to agiven parse state.
Some hypotheses are more"important" than others in the sense that theycarry more weight in the decision.
A weight-ing parameter is also included to lower theprobability estimate of actions that appearfm'ther down the decision list.
For actions aiwhere 1 < i < n - 1:P(ai(s) ?
o~ Is ?
Is7-) =.po,(i)-I ~ AkP(~Cs) 60b~ ~ I h~)hk~H~(12)where s is a given parse state, pos(i) is theposition of the action ai in the list of ac-tions applicable to state s, Ak and 0 < /~ <1 are weighting parameters, z Hi is the setof hypotheses learned for the action ai, and~k A~ = 1.To estimate the probability for the last ac-tion an, we devise a simple test that checksif the maximum of the set A(s) of proba-bility estimates for the subset of the actions2p is set to  0.95 for all the experiments performed.
{a l , .
.
.
,  an-l} applicable to s is less than orequal to a threshold a.
If A(s) is empty, weassume the maxlrn,,rn is zero.
More precisely,PCa.Cs) ?
os~ Is ?
xs~) ={ ,c..(,)~os~) if maxCACs)) < ~(,~IS~)0 otherwise(13)where a is the threshold, 3 c(an(s) ?
Ob~) isthe count of the number of good states pro-duced by the last action, and c(s ?
IS~) is thecount of the number of good states to whichthe last action is applicable.Now, let's discuss how P(ai(s) ?
OS~  I hk)and Ak are estimated.
If hk ~ s (i.e.
hk coverss), we havePCai(s) ?
o~ I hk) = pc + O " ncPc -t- nc(14)where Pc and ne are the number of positiveand negative xamples covered by hk respec-tively.
Otherwise, if h~ ~= s (i.e.
hk does notcover s), we havePCai(s) ?
OS 7" I hk) -- p" + 8 .n , ,Pu +nu(15)where Pu and nu are the n,,rnber of positiveand negative xamples rejected by hk respec-tively.
/9 is the probability that a negativeexample is mislabelled and its value can beestimated given # (in equation (6)) and thetotal nnrnber of positive and negative xam-ples.One could use a variety of linear combina-tion methods to estimate the weights Ak (e.g.Bayesian combination (Buntine, 1990)).
How-ever, we have taken a simple approach andweighted hypotheses based on their relativesimplicity:size(hk) -1Ak = ~.lHd size(hi)_1" (16) z-d=l4.3 Search ing  for a ParseTo find the most probably correct parse, theparser employs a beam search.
At each step,the parser finds all of the parsing actions ap-plicable to each parse state on the queue andcalculates the probability of goodness of eachof them using equations (12) and (13).
It thenSThe threshold is set to 0.5 for all the experimentsperformed.138computes the probability that the resultingstate of each possible action is a good stateusing equation (11), sorts the queue of possi-ble next states accordingly, and keeps the bestB options.
The parser stops when a completeparse is found on the top of the parse queueor a failure is reported.5 Exper imenta l  Resu l t s5.1 The DomainsThree different domains are used to demon-strate the performance of the new approach.The first is the U.S. Geography domain.The database contains about 800 facts aboutU.S.
states like population, area, capital city,neighboring states, major rivers, major cities,and so on.
A hand-crafted parser, GEOBASEwas previously constructed for this domain asa demo product for Turbo Prolog.
The secondapplication is the restaurant query system il-lustrated in Figure 1.
The database containsinformation about thousands of restaurantsin Northern California, including the name ofthe restaurant, its location, its specialty, and aguide-book rating.
The third domain consistsof a set of 300 computer-related jobs automat-ically extracted from postings to the USENETnewsgroup aust in .
jobs .
The database con-talus the following information: the job title,the company, the recruiter, the location, thesalary, the languages and platforms used, andrequired or desired years of experience and de-grees.5.2 Exper imenta l  Des ignThe geography corpus contains 560 questions.Approximately 100 of these were collectedfrom a log of questions ubmitted to the website and the rest were collected in studies in-volving students in undergraduate classes atour university.
We also included results for thesubset of 250 sentences originally used in theexperiments reported in (Zelle and Mooney,1996).
The remaining questions were specif-icaUy collected to be more complex than theoriginal 250, and generally require one or moremeta-predicates.
The restaurant corpus con-taln~ 250 questions automatically generatedfrom a hand-built grammar  Constructed to re-flect typical queries in this domain.
The jobcorpus contains 400 questions automaticallygenerated in a similar fashion.
The beamwidth for TABULATE was set~ to five for all thedomains.
The deterministic parser used onlythe best hypothesis found.
The experimentswere conducted using 10-fold cross validation.For each domain, the average recall (a.k.a.accuracy) and precision of the parser on dis-joint test data are reported where:of correct queries producedRecal l  =of test sentencesPrec is ion = # of correct queries produced# of complete parses produced"A complete parse is one which contains an ex-ecutable query (which could be incorrect).
Aquery is considered correct if it produces thesame answer set as the gold-standard querysupplied with the corpus.5.3 Resu l tsThe results are presented in Table 1 and Fig-ure 3.
By switching from deterministic toprobabilistic parsing, the system increased thenumber of correct queries it produced.
Re-call increases almost monotonically with pars-ing beam width in most of the domains.
I_m-provement is most apparent in the Jobs do-maln where probabilistic parsing signiBcantlyoutperformed the deterministic system (80%vs 68%).
However, using a beam width ofone (and thus the probabilistic parser picksonly the best action) results in worse perfor-mance than using the original purely logic-based determlni~tic parser.
This suggests thatthe probability esitmates could be improvedsince overall they are not indicating the sin-gle best action as well as a non-probabilisticapproach.
Precision of the system decreasedwith beam width, but not signi~cantly exceptfor the larger Geography corpus.
Since thesystem conducts a more extensive search fora complete parse, it risks increasing the num-ber of incorrect as well as correct parses.
Theimportance of recall vs. precision depends onthe relative cost of providing an incorrect an-swer versus no answer at all.
Individual ap-plications may require mphasizing one or theother.All of the experiments were run on a167MHz UltraSparc work station under Sic-stus Prolog.
Although results on the parsingtime of the different systems are not formallyreported here, it was noted that the differencebetween using a beam width of three and theoriginal system was less than two seconds inall domains but increased to a~r0und twentyseconds when using a beam width of twelve.However, the current Prolog implementationis not highly optimized.139Parsers \ Corpora Geo250 Geo560 Jobs400 Rest250Prob-Parser(12)Prob-Parser(8)Prob-Parser(5)Prob-Parser(3)Prob-Parser(1)TABULATEOriginal CHILLHand-Built ParserR P80.40 88.1679.60 86.9078.40 87.1177.60 87.3967.60 90.3775.60 92.6568.50 97.6556.00 96.40R P71.61 78.9471.07 79.7670.00 79.5169.11 79.3062.86 82.0569.29 89.81I~ P80.50 86.5678.75 86.5474.25 86.5970.50 87.3134.25 85.6368.50 87.54R P99.20 99.6099.20 99.6099.20 99.6099.20 99.6099.20 99.6099.20 99.60Table 1: Results For All Domains: R = % Recall and P = % Precision.
Prob-Parser(B) isthe probabilistic parser using a beam width of B. TABULATE is CHILL using the TABULATEinduction algorithm with determ;nistic parsing.100 ~.9O8070605040300Geo250 ,Geo560 - - -?- - -Jobs400-  ~*Rest50  -~-10095908580i i i i / 752 4 6 8 lO 12B~n ,~ZeGeo250 ,Geo560 - - -x- - -Job?,400 ~ -Rest250 - - - c - -~ - ~  .........x, ,  ...i i i /2 4 6 8 10 12Seam S?~Figure 3: The recall and precision of the parser using various beam widths in the differentdomainsWhile there was an overall improvement inrecall using the new approach, its performancevaried signiGcantly from dom~;~ to domain.As a result, the recall did not always improvedramatically by using a larger beam width.Domain factors possibly affecting the perfor-mance are the quality of the lexicon, the rel-ative amount of data available for calculat-ing probability estimates, and the problem of'~parser incompleteness" with respect o thetest data (i.e.
there is not a path from a sen-tence to a correct query which happens when'7 = 0).
The performance of all systems werebasically equivalent in the restaurant domain,where they were near perfect in both recalland precision.
This is because this corpus isrelatively easier given the restricted range ofpossible questions due to the limited informa-tion available about each restaurant.
The sys-tems achieved > 90% in recall and precisiongiven only roughly 30% of the training datain this domain.
Finally, GEOBASE perfomedthe worst on the original geography queries,since it is difficult to hand-crat~ a parser thathandles a sn~cient variety of questions.6 Conc lus ionA probabilistic framework for semantic shift-reduce parsing was presented.
A new ILPlearning system was also introduced whichlearns multiple hypotheses.
These two tech-niques were integrated to learn semanticparsers for building NLI's to on|ine databases.Experimental results were presented thatdemonstrate that such an approach outper-forms a purely logical approach in terms ofthe accuracy of the learned parser.7 Ack~nowledgementsThis research was supported by a grant fromthe Daimler-Chrysler Research and Technol-ogy Center and by the National Science Foun-dation under grant n~I-9704943.140ReferencesK.
Ali and M. Pazzani.
1996.
Error reductionthrough learning multiple descriptions.
Ma-chine Learning Journal, 24:3:100--132.W.
Buntine.
1990.
A theory of learning classifica-tion rules.
Ph.D. thesis, University of Technol-ogy, Sydney, Australia.B.
Cestnik.
1990.
Estimating probabilities: A cru-cial task in machine learning.
In Proceedings ofthe Ninth European Conference on Artificial In-teUigence, pages 147-149, Stockholm, Sweden.W.
W. Cohen.
1995.
Fast effective rule induc-tion.
In Proceedings of the Twelfth Interna-tional Conference on Machine Learning, pages115-123.L.
Getoor and D. Jensen, editors.
2000.
Papersfrom the AAA1 Workshop on Learning Statis-tical Models from Relational Data, Austin, TX.AAAI Press.G.
G. Hendrix, E. Sacerdoti, D. Sagalowicz, andJ.
Slocum.
1978.
Developing a natural languageinterface to complex data.
AGM Transactionson Database Systems, 3(2):105-147.R.
Knhn and R. De Mori.
1995.
The application ofsemantic classification trees to natural languageunderstanding.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 17(5):449-.-460.N.
Lavrac and S. Dzeroski.
1994.
Inductive LogicProgramming: Techniques and Applications.Ellis Horwood.C.
D. Mauning and H. Sch/itze.
1999.
Founda-tions of Statistical Natural Language Process-ing.
MIT Press, Cambridge, MA.Scott Miller, David StaUard, Robert Bobrow, andRichard Schwartz.
1996.
A fully statistical ap-proach to natural anguage interfaces.
In Pro-ceedings of the 34th Annual Meeting of the As-sociation for Computational Linguistics, pages55-61, Santa Cruz, CA.S.
Muggleton and W. Buntine.
1988.
Machineinvention of first-order predicates by invertingresolution.
In Proceedings of the Fifth Interna-tional Conference on Machine Learning, pages339--352, Ann Arbor, MI, June.J.
tL Q,inlan.
1990.
Learning logical definitionsfrom relations.
Machine Learning, 5(3):239-266.C.
A. Thompson and R. J. Mooney.
1999.
Au-tomatic construction of semantic lexicons forlearning natural anguage interfaces.
In Pro-ceedings of the Sixteenth National Conferenceon Artificial Intelligence, pages 487-493, Or-lando, FL, July.W.
A.
Woods.
1970.
Transition network gram-mars for natural language analysis.
Communi-cations of the Association for Computing Ma-chinery, 13:591-606.J.
M. Zelle and R. J. Mooney.
1994.
Combin-ing top-down and bottom-up methods in indue-tive logic programming.
In Proceedings of theEleventh International Conference on MachineLearning, pages 343--351, New Brunswick, NJ,July.J.
M. Zelle and tL J. Mooney.
1996.
Learningto parse database queries using inductive logicprogramming.
In Proceedings of the ThirteenthNational Conference on Artificial Intelligence,pages 1050--1055, Portland, OR, August.141
