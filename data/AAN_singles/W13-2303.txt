Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 19?27,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsUtilizing State-of-the-art Parsers to Diagnose Problems in TreebankAnnotation for a Less Resourced LanguageQuy T. NguyenUniversity of InformationTechnology, Ho Chi Minh Cityquynt@uit.edu.vnNgan L.T.
NguyenNational Instituteof Informatics, Tokyongan@nii.ac.jpYusuke MiyaoNational Instituteof Informatics, Tokyoyusuke@nii.ac.jpAbstractThe recent success of statistical pars-ing methods has made treebanks becomeimportant resources for building goodparsers.
However, constructing high-quality annotated treebanks is a challeng-ing task.
We utilized two publicly avail-able parsers, Berkeley and MST parsers,for feedback on improving the quality ofpart-of-speech tagging for the VietnameseTreebank.
Analysis of the treebank andparsing errors revealed how problems withthe Vietnamese Treebank influenced theparsing results and real difficulties of Viet-namese parsing that required further im-provements to existing parsing technolo-gies.1 IntroductionTreebanks, corpora annotated with syntactic struc-tures, have become more and more importantfor language processing.
The Vietnamese Tree-bank (VTB) has been built as part of the nationalproject ?Vietnamese language and speech process-ing (VLSP)?
to strengthen automatic processing ofthe Vietnamese language (Nguyen et al 2009).However, when we trained the Berkeley parser(Petrov et al 2006) in our preliminary experimentwith VTB and evaluated it using the corpus, theparser only achieved an F-score of 72.1%.
Thispercentage was far lower than the state-of-the-artperformance reported for the Berkeley parser onthe English Penn Treebank of 90.2% (Petrov etal., 2006).
There are two possible reasons for this.First, the quality of VTB is not good enough toconstruct a good parser that included the quality ofthe annotation scheme, the annotation guidelines,and the annotation process.
Second, parsing Viet-namese is a difficult problem on its own, and weneed to seek new solutions to this.Nguyen et al(2012) proposed methods ofimproving the annotations of word segmentation(WS) for VTB.
They also evaluated different WScriteria in two applications, i.e., machine trans-lation and text classification.
This paper focuseson improving the quality of parts-of-speech (POS)annotations by using state-of-the-art parsers toprovide feedback for this process.The difficulties with Vietnamese POS tag-ging have been recognized by many researchers(Nghiem et al 2008; Le et al 2010).
There is lit-tle consensus as to the methodology for classifyingwords.
Polysemous words, words with the samesurface form but having different meanings andgrammar functions, are very popular in the Viet-namese language.
For example, the word ?c?
?can be a noun that means neck/she, or an adjec-tive that means ancient depending on the context.This characteristic makes it difficult to tag POSsfor Vietnamese, both manually and automatically.The rest of this paper is organized as follows:a brief introduction to VTB and its annotationschemes are provided in Section 2.
Then, previ-ous work is summarized in Section 3.
Section 4describes our methods of detecting and correctinginconsistencies in POSs in the VTB corpus.
Eval-uations of these methods are described in Section5.
Finally, Section 6 explains our evaluations ofthe Berkeley parser and Minimum-Spanning Tree(MST) parser on different versions of the VTBcorpus, which were created by using detected in-consistencies.
These results from evaluations areconsidered to be a way of measuring the effectof automatically detected and corrected inconsis-tencies.
We could observe difficulties with Viet-namese that affected the quality of parsers by ana-lyzing the results from parsing.Our experiences in using state-of-the-art parsersfor treebank annotation, which are presented inthis paper, should not only benefit the Vietnameselanguage, but also other languages with similar19Label Name ExampleN Common noun nh?n d?n {people}Np Proper noun Vi?t Nam {Vietnam}Nc Classifer noun con, c?i, b?c {*}Nu Unit noun m?t {meter}V Verb ng?i {sit}A Adjective t?t {good}P Pronoun t?i {I}, h?n {he}L Determiner m?i {every}, nh?ng {*}M Number m?t {one}R Adverb ?
?, s?, ?ang {*}E Preposition tr?n {on}C Conjunction tuy nhi?n {however}I Exclamation ?i, chao, a ha {*}T Particle ?, ?y, ch?ng {*}B Foreign word internet, emailY Abbreviation APEC, WTO, HIVS Affix b?t, v?, ?a {*}X OtherTable 1: VTB part-of-speech tag setcharacteristics.2 Brief introduction to VTBThe VTB corpus contains 10.433 sentences(274.266 tokens), semi-manually annotated withthree layers of WS, POS tagging, and bracketing.The first annotation is produced for each annota-tion layer by using automatic tools.
Then, the an-notators revise these data.
The WS and POS an-notation schemes were introduced by Nguyen etal.
(2012).
This section briefly introduces POS tagset and a bracketing annotation scheme.VTB specifies the 18 different POS tags sum-marized in Table 1 (Nguyen et al 2010a).
Eachunit in this table goes with several example words.English translations of these words are included inbraces.
However, as we could not find any appro-priate English translations for some words, theseempty translations have been denoted by asterisks(*).The VTB corpus is annotated with three syn-tactic tag types: constituency tags, functionaltags, and null-element tags.
There are 18 con-stituency tags in VTB.
The functional tags areused to enrich information for syntactic trees, suchas where functional tag ?SUB?
is combined withconstituency tag ?NP?, which is presented as ?NP-SUB?
to indicate this noun phrase is a subject.There are 17 functional tags in VTB.
The headword of a phrase is annotated with functional tag?H?.The phrase structures of Vietnamese includethree positions: <pre-head>, <head>, and <post-head> (Vietnamese grammar, 1983; Nguyen et al2010c).
The head word of the phrase is in the<head> position.
The words that are in the <pre-head> and <post-head> positions are modifiers ofthe head word.There is a special type of noun in Vietnamesethat we have called Nc-noun in this paper.
Nc-nouns can be classifier nouns or common nounsdepending on their modifiers.
For example, theNc-noun ?con?
is a classifier noun if its modifieris the word ?c?
{fish}?
(?con c?
?, which meansa specific fish, similar to ?the fish?
in English).However, the Nc-noun ?con {child}?
is a commonnoun if its modifier is the word ?gh??
(?con gh?
?,which means ?stepchild?
in English).
We foundthat Nc-nouns always appeared in the head posi-tions of noun phrases by investigating the VTBcorpus.
There is currently little consensus as tothe methodology for annotating Nc-nouns (Hoang,1998; Nguyen et al 2010b; Nguyen et al 2010a).3 Summarization of previous workNguyen et al(2012) described methods of detect-ing and correcting WS inconsistencies in the VTBcorpus.
These methods focused on two types ofWS inconsistency, variation and structural incon-sistency, which are defined below.Variation inconsistency: is a sequence of tokensthat has more than one way of being segmented inthe corpus.Structural inconsistency: occurs when differentsequences have similar structures, and thus shouldbe split in the same way, but are segmented in dif-ferent ways in the corpus.
Nguyen et al(2012)pointed out three typical cases of structural in-consistency that were analyzed as classifier nouns(Nc), affixes (S), and special characters.Nguyen et al(2012) analyzed N-gram se-quences and phrase structures to detect WS in-consistencies.
Then, the detected WS inconsis-tencies were classified into several patterns of in-consistencies, parts of which were manually fixedto improve the quality of the corpus.
The restwere used to create different versions of the VTBcorpus.
These data sets were evaluated on auto-matic WS and its applications to text classificationand English-Vietnamese statistical machine trans-lations to find appropriate criteria for automaticWS and its applications.Their experiments revealed that theVAR_FREQ data set achieved excellent re-sults in these applications.
The VAR_FREQ data20set was the original VTB corpus with manuallycorrected structural inconsistencies in specialcharacters and selected segmentations with higherfrequencies in all detected variations.
There-fore, we used the VAR_FREQ data set in ourexperiments.4 Methods of detecting and correctinginconsistencies in POS annotationsWe propose two kinds of methods of detectingand correcting inconsistencies.
They correspondto two different types of POS inconsistency thatwe call multi-POS inconsistency (MI) and Nc in-consistency (NcI), which are defined as follows.Multi-POS inconsistency: is a word that is notNc-noun and has more than one POS tag at eachposition in each phrase category.Nc inconsistency: is a sequence of Nc-noun andmodifier, in which Nc-noun has more than oneway of POS annotation in the VTB corpus.We separated the POS inconsistencies into thesetwo types of inconsistencies because Nc-nounsare special types of words in Vietnamese.
Themethods of detecting and correcting NcIs werelanguage-specific methods developed based on thecharacteristics of Vietnamese.
However, as themethods for MIs are rather general, they can beapplied to other languages.4.1 General method for multi-POSinconsistenciesDetection method (MI_DM)Our main problem was to distinguish MIsfrom polysemous words, since polysemous wordsshould not be considered inconsistent annotations.Our method was based on the position of words inphrases and phrase categories.
This idea resultedfrom the observation that polysemous words havemany POS tags; however, each word usually hasonly one true POS tag at each position in eachphrase category.
For example, when a phrase cat-egory is a verb phrase, the word ?can?
in the pre-head position of the verb phrase ?
(VP (MD can)(VB can))?
should be a modal, but the word ?can?in the head position should be a verb.
Further, theword ?cut?
in the head position of a noun phrase?
(NP (DT a) (JJ further) (NN cut))?
should be anoun, but the word ?cut?
in the head position ofthe verb phrase ?
(VP (VB cut) (NP (NNS costs)))?should be a verb.
This may be more frequent inVietnamese because it is not an inflectional lan-guage i.e., the word form does not change accord-ing to tenses, word categories (e.g., nouns, verbs,and adjectives), or number (singular and plural).The method involved three steps.
First, weextracted words in the same position for eachphrase category.
Second, we counted the num-ber of different POS tags of each word.
Wordsthat had more than one POS tag were determinedto be multi-POS inconsistencies.
For example, inthe following two preposition phrases, ?
(PP (E-H c?a) (P ch?ng_t?i1)) {of us}?
and ?
(PP (C-Hc?a) (P h?i_ngh?))
{of conference}?, the words?c?a {of}?
appear at the head positions of bothphrases, but they are annotated with different POStags, preposition (E) and conjunction (C).
There-fore, they are MIs according to our method.It should be noted that this method was appliedto words that were direct children of a phrase.Embedded phrases, such as ?
(PP (E c?a) (Pch?ng_t?i))?
in ?
(NP (M hai) (Nc-H con) (N m?o)(PP (E c?a) (P ch?ng_t?i))) {our two cats}?, wereconsidered separately.Correction method (MI_CM)A multi-POS inconsistency detected with theMI_DM method is denoted by ?w|P1-f1|P2-f2|...|Pn-fn AC?, where Pi (i = 1, 2, ..., n) is a POStag of word w, fi is the frequency of POS tag Pi,and AC is applying condition of w. Our methodof correcting the POS tag for POS inconsistency?w|P1-f1|P2-f2|...|Pn-fn AC?
involves two steps.First, we select the POS tag with the highest fre-quency of all POS tags of ?w|P1-f1|P2-f2|...|Pn-fnAC?
(Pmax).
Second, we replace POS tags Pi ofall instances (w|Pi) satisfying condition AC withPOS tag Pmax.
For MIs, the AC of word w is itsphrase category and position in the phrase.For example, ?to?n b?|L-27|P-2?
is a multi-POS inconsistency in the pre-head position of anoun phrase.
The frequency of POS tag ?L?
is 27and the frequency of POS tag ?P?
is 2.
There-fore, ?L?
is the POS tag that was selected by theMI_CM method.
We replace all POS tags Pi ofinstances ?to?n b?|Pi?
in the pre-head positionsof noun phrases with POS tag ?L?.4.2 Language-specific method for classifiernounsDetection methodAs mentioned in Section 2, an Nc-noun can be1We used underscore ?_?
to link syllables of Vietnamesecompound words.21annotated with POS tag ?Nc?
or ?N?
dependingon the modifier that follows that Nc-noun.
Ana-lyzing the VTB corpus revealed that Nc-nouns hadtwo characteristics.
First, an Nc-noun that is fol-lowed by the same word at each occurrence is usu-ally annotated with the same POS tag.
Second, anNc-noun that is followed by a phrase or nothing ateach occurrence is annotated with the same POStag.
Based on these two cases, we propose twomethods of detecting NcIs, which we have calledNcI_DM1 and NcI_DM2.
They are described be-low.NcI_DM1: We counted Nc-nouns in VTB thathad two or more ways of POS annotation, satis-fying the condition that Nc-nouns are followed bya phrase or nothing.
For example, the Nc-noun?con?
in ?
(NP (M 2) (N-H con)) {2 children}?
isfollowed by nothing or it is followed by a prepo-sitional phrase as in ?
(NP (L c?c) (N-H con) (PP(E-H c?a) (P t?i))) {my children}?.NcI_DM2: We counted two-gram sequencesbeginning with an Nc-noun in VTB that had twoor more ways of POS annotation of the Nc-noun,satisfying the conditions that two tokens were allin the same phrase and and they all had the samedepth in a phrase.
For example, the Nc-noun?con?
in the two-gram ?con g?i {daughter}?
wassometimes annotated ?Nc?, and sometimes anno-tated ?N?
in VTB; in addition, as ?con?
and ?g?i?in the structure ?
(NP (Nc-H con) (N g?i) (PP (E-H c?a) (P t?i))) {my daughter}?
were in the samephrase and have the same depth, ?con?
was anNcI.Correction methodWe denoted NcIs with ?w|P1-f1|P2-f2|...|Pn-fnAC?
similarly to MIs.
We also replaced the POStag of Nc-nouns with the highest frequency tag.The only differences were the applying conditionsthat varied according to the previous two cases ofNcIs.?
For Nc inconsistencies detected by theNcI_DM1 method, AC is defined as follows:w is an Nc-noun that is followed by nothingor a phrase.?
For Nc inconsistencies detected by theNcI_DM2 method, AC is defined as follows:w is an Nc-noun that must be followed by aword, m.5 Results and evaluationWe detected and corrected MIs and NcIs basedon the two data sets, ORG and VAR_FREQ.
TheORG data set was the original VTB corpus andVAR_FREQ was the original corpus with modifi-cations to WS annotation.
This setting was madesimilar to that used by Nguyen et al(2012) toenable comparison.There are a total of 128,871 phrases in the VTBcorpus.
The top five types of phrases are nounphrases (NPs) (representing 49.6% of the totalnumber of phrases), verb phrases (VPs), preposi-tional phrases (PPs), adjectival phrases (ADJPs),and quantity phrases (QPs), representing 99.1% ofthe total number of phrases in the VTB corpus.
Weanalyzed the VTB corpus based on these five typesof phrases.5.1 Results for detected POS inconsistenciesTables 2 and 3 show the overall statistics forMIs and NcIs for each phrase category.
The sec-ond and third columns in these tables indicate thenumbers of inconsistencies and their instances thatwere detected in the ORG data set.
The fourth andfifth columns indicate the numbers of inconsisten-cies and their instances that were detected in theVAR_FREQ data set.
The rows in Table 3 indicatethe number of NcIs and the number of instancesdetected with the NcI_DM1 and NcI_DM2 meth-ods.According to Table 2, most of the MIs occurredin noun phrases, representing more than 72% ofthe total number of MIs.
All NcIs in Table 3 arealso in noun phrases.
There are two possible rea-sons for this.
First, noun phrases represent the ma-jority of phrases in VTB (represent 49.6% of thetotal number of phrases in the VTB corpus).
Sec-ond, nouns are sub-divided into many other types(common noun (N), classifier noun (Nc), propernoun (Np), and unit noun (Nu)) (mentioned in Sec-tion 2), which may confuse annotators in anno-tating POS tags for nouns.
In addition, the highnumber of NcIs in Table 3 indicate that it is diffi-cult to distinguish between Nc and other types ofnouns.
Therefore, we need to have clearer annota-tion guidelines for this.5.2 Evaluation of methods to detect andcorrect inconsistenciesWe estimated the accuracy of our methods whichdetected and corrected inconsistencies in POS tag-22PhraseORG VAR_FREQInc Ins Inc InsNP 792 28,423 752 27,067VP 221 10,158 139 10,110ADJP 64 1,302 61 1,257QP 4 22 4 22PP 14 5,649 13 5,628Total 1,095 45,554 969 44,084Table 2: Statistics for multi-POS inconsistenciesfor each phrase category in VTB.
Number of In-consistencies (Inc) and Number of Instances (Ins).Detection methodORG VAR_FREQInc Ins Inc InsNcI_DM1 52 3,801 51 3,792NcI_DM2 338 2,468 326 2,412Total 390 6,269 377 6,204Table 3: Statistics for Nc inconsistencies in headpositions of noun phrases in VTB.ging by manually inspecting inconsistent annota-tions.
We manually inspected the two data setsof ORG_EVAL and ORG_POS_EVAL.
To cre-ate ORG_EVAL, we randomly selected 100 sen-tences which contained instances of POS incon-sistencies in the ORG data set.
ORG_EVAL con-tained 459 instances of 157 POS inconsistencies.ORG_POS_EVAL was the ORG_EVAL data setwith corrections made to multi-POS inconsisten-cies and Nc inconsistencies with our methods ofcorrection above.Detection: We manually checked POS incon-sistencies and found that 153 cases out of 157 POSinconsistencies (97.5%) were actual inconsisten-cies.
There were four cases that our method de-tected as multi-POS inconsistencies, but they wereactually ambiguities in Vietnamese POS tagging.They were polysemous words whose meaningsand POS tags depended on surrounding words, butdid not depend on their positions in phrases.
Forexample, the word ?s?ng?
in the post-head posi-tions of the verb phrases VP1 and VP2 below, canbe a noun that means morning in English, or it canbe an adjective that means bright, depending onthe preceding verb.VP1: (VP (V-H th?p) (A s?ng) {lighten bright}VP2: (VP (V-H ?i) (N s?ng) {go in the morning}Correction: Table 4 shows results of com-parison of the POS tags for 459 instances inORG_EVAL and those in ORG_POS_EVAL.These results indicate that there are instanceswhose POS tags are incorrect in ORG_EVALbut correct in ORG_POS_EVAL (the third rowORG_EVAL ORG_POS_EVAL No.
of Instancescorrect correct 404incorrect correct 41correct incorrect 11incorrect incorrect 3Total 459Table 4: Comparison of POS tags for 459instances in ORG_EVAL with those inORG_POS_EVAL.PoPOS Counts ExamplesNc-N 385 ng?
?i {the, person}N-V 186 m?t m?t {loss}N-Np 176 H?i {association}N-A 144 kh?
kh?n {difficult}V-A 92 ph?i {must, right}Table 5: Top five pairs of confusing POS tags.in Table 4), and there are instances whose POStags are correct in ORG_EVAL but incorrect inORG_POS_EVAL (the fourth row in Table 4).The results in Table 4 indicate that, the numberof correct POS tags in ORG_POS_EVAL (445 in-stances, representing 96.9% of the total number ofinstances) is higher than that in ORG_EVAL (415instances, representing 90.4% of the total numberof instances).
This means our methods of correct-ing inconsistencies in POS tagging improved thequality of treebank annotations.5.3 Analysis of detected inconsistenciesWe analyzed the detected POS inconsistencies tofind the reasons for inconsistent POS annotations.We classified the detected POS inconsistencies ac-cording to pairs of their POS tags.
There werea total of 85 patterns of pairs of POS tags.
Ta-ble 5 lists the top five confusing patterns (PoPOS),their counts of inconsistencies (Counts), and ex-amples.
It also seemed to be extremely confus-ing for the annotators to distinguish types of nouns(Nc and N, and N and Np) and distinguish nounsfrom other types of words (such as verbs, adjec-tives, and pronouns).We investigated POS inconsistencies and theannotation guidelines (Nguyen et al 2010b;Nguyen et al 2010a; Nguyen et al 2010c) tofind why common nouns were sometimes taggedas classifier nouns and vice versa, and verbs weresometimes tagged as common nouns and viceversa, and so on.
We found that these POS in-consistencies belonged to polysemous words thatwere difficult to tag.The difficulties with tagging polysemous words23were due to four main reasons: (1) The POS of apolysemous word changes according to the func-tion of that polysemous word in each phrase cate-gory or changes according to the meaning of sur-rounding words.
Although polysemous words areannotated with different POS tags, they do notchange their word form.
(2) The way polysemouswords are tagged according to their context is notcompletely clear in the POS tagging guidelines.
(3) Annotators referred to a dictionary that hadbeen built as part of the VLSP project (Nguyen etal., 2009) (VLSP dictionary) to annotate the VTBcorpus.
However, this dictionary lacked variouswords and did not cover all contexts for the words.For example, ?h?n {more than}?
in Vietnamese isan adjective when it is the head word of an adjec-tival phrase, but ?h?n {over}?
is an adverb when itis the modifier of a quantifier noun (such as ?h?n200 sinh vi?n {over 200 students}?).
However, theVLSP dictionary only considered ?h?n?
to be anadjective (?t?i h?n n?
hai tu?i {I am more thanhim two years old}?).
No cases where ?h?n?
wasan adverb were mentioned in this dictionary.
(4)There are several overlapping but conflicting in-structions across the annotation guidelines for dif-ferent layers of the treebank.
For example, thecombinations of affixes and words they modify tocreate compound words are clear in the WS guide-lines, but POS tagging guidelines treat affixes aswords and they are annotated as POS tags ?S?.For words modifying quantifier nouns, such as?h?n and g?n {over and about}?, the POS taggingguidelines treat them as adjectives, but the brack-eting guidelines treat them as adverbs.
Therefore,our method detected multi-POS inconsistencies as?h?n|A-135|R-51?, ?g?n|A-102|R-5?
at the pre-head positions of noun phrases.
Since the frequen-cies of the adjective tags were greater than those ofadverb tags (fA > fR), these words were automati-cally assigned to adjective POS tags (A) accordingto our method of correction.
These were POS in-consistencies that our method of correction couldnot be applied to, because the frequency of incor-rect POS tags was higher than that of actual POStags.6 Evaluation of state-of-the-art parserson VTBWe carried out experiments to evaluate two pop-ular parsers, a syntactic parser and a dependencyparser, on different versions of the VTB corpus.Some of these data sets were made the same as thedata settings for WS in Nguyen et al(2012).
Theother data sets contained changes in POS annota-tions following our methods of correcting incon-sistencies presented in Section 4.
We could ob-serve how the problems with WS and POS tag-ging influenced the quality of Vietnamese parsingby analyzing the parsing results.6.1 Experimental settingsData.
Nine configurations of the VTB corpuswere created as follows:?
ORG: The original VTB corpus.?
BASE, STRUCT_AFFIX, STRUCT_NC,VAR_SPLIT, VAR_COMB, andVAR_FREQ correspond to different set-tings for WS described in Nguyen etal.
(2012).?
ORG_POS: The ORG data set with correc-tions for multi-POS inconsistencies and Ncinconsistencies by using the methods in Sec-tion 4.1 and 4.2.?
VAR_FREQ_POS: The VAR_FREQ data setwith corrections for multi-POS inconsisten-cies and Nc inconsistencies by using themethods in Section 4.1 and 4.2.Each of the nine data sets was randomly splitinto two subsets for training and testing our parsermodels.
The training set contained 9,443 sen-tences, and the testing set contained 1,000 sen-tences.ToolsWe used the Berkeley parser (Petrov et al2006) to evaluate the syntactic parser on VTB.This parser has been used in experiments in En-glish, German, and Chinese and achieved an F1 of90.2% on the English Penn Treebank.We used the conversion tool built by Johans-son et al(2007) to convert VTB into dependencytrees.We used the MST parser to evaluate the depen-dency parsing on VTB.
This parser was evaluatedon the English Penn Treebank (Mcdonald et al2006a) and 13 other languages (Mcdonald et al2006b).
Its accuracy achieved 90.7% on the En-glish Penn Treebank.We made use of the bracket scoring programEVALB, which was built by Sekine et al(1997),24Data sets Bracketing F-measuresORG 72.10BASE 72.20STRUCT_AFFIX 72.60STRUCT_NC 71.92VAR_SPLIT 72.03VAR_COMB 72.46VAR_FREQ 72.34ORG_POS 72.72VAR_FREQ_POS 73.21Table 6: Bracketing F-measures of Berkeleyparser on nine configurations of VTB corpus.Data set UA LAORG 50.51 46.14BASE 53.90 50.14STRUCT_AFFIX 54.00 50.25STRUCT_NC 53.88 49.96VAR_SPLIT 53.95 50.14VAR_COMB 53.93 50.27VAR_FREQ 54.21 50.41ORG_POS 54.20 50.37VAR_FREQ_POS 57.87 53.19Table 7: Dependency accuracy of MSTParser onnine configurations of VTB corpus.
UnlabeledAccuracy (UA), Labeled Accuracy (LA).to evaluate the performance of the Berkeley parser.As an evaluation tool was included in the MSTparser tool, we used it to evaluate the MST parser.6.2 Experimental resultsThe bracketing F-measures of the Berkeley parseron nine configurations of the VTB corpus arelisted in Table 6.
The dependency accuracies ofthe MST parser on nine configurations of the VTBcorpus are shown in Table 7.
These results indicatethat the quality of the treebank strongly affectedthe quality of the parsers.According to Table 6, all modifications to WSinconsistencies improved the performance of theBerkeley parser except for STRUCT_NC andVAR_SPLIT.
More importantly, the ORG_POSmodel achieved better results than the ORGmodel, and the VAR_FREQ_POS model achievedbetter results than the VAR_FREQ model, whichindicates that the modifications to POS inconsis-tencies improved the performance of the Berkeleyparser.
The VAR_FREQ_POS model scored 1.11point higher than ORG, which is a significant im-provement.Dependency accuracies of the MST parserin Table 7 indicate that all modifications toPOS inconsistencies improved the performanceof the MST parser.
All modifications to WSAPSs CCTs and FreqA M N NP-79|ADJP-27A V VP-56|ADJP-78|NP-2Table 8: Examples of ambiguous POS sequences(APSs), their CCTs, and frequency of each CCT(Freq)inconsistencies also improved the performanceof the MST parser except for STRUCT_NC.The VAR_FREQ_POS model scored 7.36 pointshigher than ORG, which is a significant improve-ment.6.3 Analysis of parsing resultsThe results for the Berkeley parser and MSTparser trained on the POS-modified versions ofVTB were better than those trained on the origi-nal VTB corpus, but they were still much lowerthan the performance of the same parsers onthe English language.
We analyzed error basedon the output data of the best parsing results(VAR_FREQ_POS) for the Berkeley parser, andfound that the unmatched annotations betweengold and test data were caused by ambiguous POSsequences in the VTB corpus.An ambiguous POS sequence is a sequence ofPOS tags that has two or more constituency tags.For example, there are the verb phrase ?
(VP (R?ang) (A c?m_c?i) (V l?m)) {* (be) painstak-ingly doing}?
and the adjectival phrase ?
(ADJP (Rr?t) (A d?)
(V th?c_hi?n)) {very easy (to) imple-ment}?
in the training data of VAR_FREQ_POS.As these two phrases have the same POS sequence?R A V?, ?R A V?
is an ambiguous POS se-quence, and VP and ADJP are confusing con-stituency tags (CCTs).
We found 42,373 occur-rences of 213 ambiguous POS sequences (repre-senting 37.02% of all phrases) in the training dataof VAR_FREQ_POS.
We also found 1,065 oc-currences of 13 ambiguous POS sequences in theparsing results for VAR_FREQ_POS.
Some ex-amples of ambiguous POS sequences, their CCTs,and the number of occurrences of each CCT in thetraining data of VAR_FREQ_POS are listed in Ta-ble 8.We classified the detected ambiguous POS se-quences according to pairs of different CCTs tofind the reasons for ambiguity in each pair.
Therewere a total of 42 pairs of CCTs, whose top threepairs, along with their counts of types of am-biguous POS sequences, and examples of ambigu-25Pairs of CCTs Counts ExamplesNP-VP 61 P V N, ...VP-ADJP 54 R A V, A V N, ...ADJP-NP 52 A M N, ...Table 9: Top three pairs of confusing constituencytagsPairs of CCTs 1 2NP-VP M, L ,R ,V N, R, M, P, AVP-ADJP A, R N, RADJP-NP N, R R, M, A, LTable 10: Statistics for POS tags at pre-head posi-tion of each phrase category.ous POS sequences are listed in Table 9.
Weextracted different POS tags at each position ofeach phrase category for each pair of CCTs, basedon the ambiguous POS sequences.
For example,the third row in Table 9 has ?R A V?
and ?A VN?, which are two ambiguous POS sequences thatwere sometimes annotated as VP and sometimesannotated as ADJP.
The different POS tags thatwere extracted from the pre-head positions of VPsbased on these two POS sequences were ?R, A?and ?R?
was the POS tag that was extracted fromthe pre-head positions of ADJPs based on thesetwo POS sequences.
These POS tags are importantclues to finding reasons for ambiguities in POS se-quences.Table 10 summarizes the extracted POS tags atpre-head positions for the top three pairs of CCTs.For example, the POS tags in row NP-VP and col-umn 1 are in the pre-head positions of NP and thePOS tags in row NP-VP and column 2 are in thepre-head positions of VP.
By comparing these re-sults with the structures of the pre-head positionsof phrase categories in VTB bracketing guidelines(Nguyen et al 2010c), we found many cases thatwere not annotated according to instructions in theVTB bracketing guidelines, such as those accord-ing to Table 10, where an adjective (A) is in thepre-head position of VP, but according to the VTBbracketing guidelines, the structure of the pre-headposition of VB only includes adverb (R).We investigated cases that had not been anno-tated according to the guidelines, and found twopossible reasons that caused ambiguous POS se-quences.
First, although our methods improvedthe quality of the VTB corpus, some POS anno-tation errors remained in the VTB corpus.
ThesePOS annotation errors were cases to which ourmethods could not be applied (mentioned in Sec-tion 5).
Second, there were ambiguities in POSsequences caused by Vietnamese characteristics,such as the adjectival phrase ?
(ADJP (R ?ang)(N ng?y_?
?m) (A ?au_?
?n)) {* day-and-nightpainful}?
and the noun phrase ?
(NP (R c?ng) (Nsinh_vi?n) (A gi?i)) {also good student}?
that hadthe same POS sequence of ?R N A?.Therefore, POS annotation errors need to beeliminated from the VTB corpus to further im-prove its quality and that of the Vietnamese parser.We not only need to eliminate overlapping butconflicting instructions, which were mentioned inSection 5.3, from the guidelines, but we also haveto complete annotation instructions for cases thathave not been treated (or not been clearly treated)in the guidelines.
We may also need to improvePOS tag set because adverbs modifying adjectives,verbs and nouns are all presently tagged as ?R?,which caused ambiguous POS sequences, such asthe ambiguous POS sequence ?R N A?
mentionedabove.
If we use different POS tags for the adverb?
?ang?, which modifies the adjective ?
?au ?
?n{painful}?, and the adverb ?c?ng?, which modi-fies the noun ?sinh vi?n {student}?, we can elimi-nate ambiguous POS sequences in these cases.7 ConclusionWe proposed several methods of improving thequality of the VTB corpus.
Our manual evalua-tion revealed that our methods improved the qual-ity of the VTB corpus by 6.5% with correct POStags.
Analysis of inconsistencies and the annota-tion guidelines suggested that: (1) better instruc-tions should be added to the VTB guidelines tohelp annotators to distinguish difficult POS tags,(2) overlapping but conflicting instructions shouldbe eliminated from the VTB guidelines, and (3)annotations that referred to dictionaries should beavoided.To the best of our knowledge, this paper is thefirst report on evaluating state-of-the-art parsersused on the Vietnamese language.
The results ob-tained from evaluating these two parsers were usedas feedback to improve the quality of treebank an-notations.
We also thoroughly analyzed the pars-ing output, which revealed challenging issues intreebank annotations and in the Vietnamese pars-ing problem itself.26ReferencesAnna M. D. Sciullo and Edwin Williams.
1987.
Onthe definition of word.
The MIT Press.Fei Xia.
2000.
The part-of-speech tagging guidelinesfor the penn chinese treebank (3.0).Minh Nghiem, Dien Dinh and Mai Nguyen.
2008.
Im-proving Vietnamese POS tagging by integrating arich feature set and Support Vector Machines.
Pro-ceedings of RIVF 2008, pages: 128?133.Phe Hoang.
1998.
Vietnamese Dictionary.
Scientific& Technical Publishing.Phuong H. Le, Azim Roussanaly, Huyen T. M. Nguyenand Mathias Rossignol.
2010.
An empirical study ofmaximum entropy approach for part-of-speech tag-ging of Vietnamese texts.
Proceedings of TALN2010 Conference.
Montreal, Canada.Quy T. Nguyen, Ngan L.T.
Nguyen and Yusuke Miyao.2012.
Comparing Different Criteria for VietnameseWord Segmentation.
Proceedings of 3rd Workshopon South and Southeast Asian Natural LanguageProcessing (SANLP), pages: 53?68.Richard Johansson and Pierre Nugues.
2007.
ExtendedConstituent-to-dependency Conversion for English.Proceedings of NODALIDA, Tartu, Estonia, pages:105?112.Ryan Mcdonald and Fernando Pereira.
2006a.
On-line Learning of Approximate Dependency ParsingAlgorithms.
Proceedings of 11th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics: EACL 2006, pages: 81?88.Ryan Mcdonald, Kevin Lerman and Fernando Pereira.2006b.
Multilingual Dependency Analysis witha Two-Stage Discriminative Parser.
Proceedingsof Tenth Conference on Computational NaturalLanguage Learning (CoNLL-X), Bergan, Norway,pages: 216?220.Slav Petrov, Leon Barrett, Romain Thibaux and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
Proceedings of 21st In-ternational Conference on Computational Linguis-tics and the 44th annual meeting of the Associationfor Computational Linguistics, pages: 433?440.Thai P. Nguyen, Luong X.
Vu and Huyen T.M.
Nguyen.2010a.
VTB part-of-speech tagging guidelines.Thai P. Nguyen, Luong X.
Vu and Huyen T.M.
Nguyen.2010b.
VTB word segmentation guidelines.Thai P. Nguyen, Luong X.
Vu, Huyen T.M.
Nguyen,Hiep V. Nguyen and Phuong H. Le.
2009.
Build-ing a large syntactically-annotated corpus of Viet-namese.
Proceedings of Third Linguistic Annota-tion Workshop, pages: 182?185.Thai P. Nguyen, Luong X.
Vu, Huyen T.M.
Nguyen,Thu M. Dao, Ngoc T.M.
Dao and Ngan K. Le.2010c.
VTB bracketing guidelines.Vietnamese grammar.
1983.
Social Sciences Publish-ers.27
