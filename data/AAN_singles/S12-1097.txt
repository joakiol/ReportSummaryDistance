First Joint Conference on Lexical and Computational Semantics (*SEM), pages 655?661,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUniversity Of Sheffield: Two Approaches to Semantic Text SimilaritySam Biggins, Shaabi Mohammed, Sam Oakley,Luke Stringer, Mark Stevenson and Judita PriessDepartment of Computer ScienceUniversity of SheffieldSheffieldS1 4DP, UK{aca08sb, aca08sm, coa07so, aca08ls,r.m.stevenson, j.preiss}@shef.ac.ukAbstractThis paper describes the University ofSheffield?s submission to SemEval-2012 Task6: Semantic Text Similarity.
Two approacheswere developed.
The first is an unsupervisedtechnique based on the widely used vectorspace model and information from WordNet.The second method relies on supervised ma-chine learning and represents each sentence asa set of n-grams.
This approach also makesuse of information from WordNet.
Resultsfrom the formal evaluation show that both ap-proaches are useful for determining the simi-larity in meaning between pairs of sentenceswith the best performance being obtained bythe supervised approach.
Incorporating infor-mation from WordNet alo improves perfor-mance for both approaches.1 IntroductionThis paper describes the University of Sheffield?ssubmission to SemEval-2012 Task 6: Semantic TextSimilarity (Agirre et al, 2012).
The task is con-cerned with determining the degree of semanticequivalence between a pair of sentences.Measuring the similarity between sentences is animportant problem that is relevant to many areasof language processing, including the identificationof text reuse (Seo and Croft, 2008; Bendersky andCroft, 2009), textual entailment (Szpektor et al,2004; Zanzotto et al, 2009), paraphrase detection(Barzilay and Lee, 2003; Dolan et al, 2004), In-formation Extraction/Question Answering (Lin andPantel, 2001; Stevenson and Greenwood, 2005), In-formation Retrieval (Baeza-Yates and Ribeiro-Neto,1999), short answer grading (Pulman and Sukkarieh,2005; Mohler and Mihalcea, 2009), recommenda-tion (Tintarev and Masthoff, 2006) and evaluation(Papineni et al, 2002; Lin, 2004).Many of the previous approaches to measuring thesimilarity between texts have relied purely on lexi-cal matching techniques, for example (Baeza-Yatesand Ribeiro-Neto, 1999; Papineni et al, 2002; Lin,2004).
In these approaches the similarity of texts iscomputed as a function of the number of matchingtokens, or sequences of tokens, they contain.
How-ever, this approach fails to identify similarities whenthe same meaning is conveyed using synonymousterms or phrases (for example, ?The dog sat on themat?
and ?The hound sat on the mat?)
or when themeanings of the texts are similar but not identical(for example, ?The cat sat on the mat?
and ?A dogsat on the chair?
).Significant amounts of previous work on textsimilarity have focussed on comparing the mean-ings of texts longer than a single sentence, such asparagraphs or documents (Baeza-Yates and Ribeiro-Neto, 1999; Seo and Croft, 2008; Bendersky andCroft, 2009).
The size of these texts means thatthere is a reasonable amount of lexical items in eachdocument that can be used to determine similarityand failing to identify connections between relatedterms may not be problematic.
The situation is dif-ferent for the problem of semantic text similaritywhere the texts are short (single sentences).
Thereare fewer lexical items to match in this case, makingit more important that connections between relatedterms are identified.
One way in which this infor-mation has been incorporated in NLP systems has655been to make use of WordNet to provide informa-tion about similarity between word meanings, andthis approach has been shown to be useful for com-puting text similarity (Mihalcea and Corley, 2006;Mohler and Mihalcea, 2009).This paper describes two approaches to the se-mantic text similarity problem that use WordNet(Miller et al, 1990) to provide information aboutrelations between word meanings.
The two ap-proaches are based on commonly used techniquesfor computing semantic similarity based on lexicalmatching.
The first is unsupervised while the otherrequires annotated data to train a learning algorithm.Results of the SemEval evaluation show that the su-pervised approach produces the best overall resultsand that using the information provided by WordNetleads to an improvement in performance.The remainder of this paper is organised as fol-lows.
The next section describes the two approachesfor computing semantic similarity between pairs ofsentences that were developed.
The system submit-ted for the task is described in Section 3 and its per-formance in the official evaluation in Section 4.
Sec-tion 5 contains the conclusions and suggestions forfuture work.2 Computing Semantic Text SimilarityTwo approaches for computing semantic similar-ity between sentences were developed.
The firstmethod, described in Section 2.1, is unsupervised.
Ituses an enhanced version of the vector space modelby calculating the similarity between word senses,and then finding the distances between vectors con-structed using these distances.
The second method,described in Section 2.2, is based on supervised ma-chine learning and compares sentences based on theoverlap of the n-grams they contain.2.1 Vector Space ModelThe first approach is inspired by the vector spacemodel (Salton et al, 1975) commonly used to com-pare texts in Information Retrieval and Natural Lan-guage Processing (Baeza-Yates and Ribeiro-Neto,1999; Manning and Schu?tze, 1999; Jurafsky andMartin, 2009).2.1.1 Creating vectorsEach sentence is tokenised, stop words removedand the remaining words lemmatised using NLTK(Bird et al, 2009).
(The WordPunctTokenizerand WordNetLemmatizer are applied.)
Binaryvectors are then created for each sentence.The similarity between sentences can be com-puted by comparing these vectors using the cosinemetric.
However, this does not take account ofwords with similar meanings, such as ?dog?
and?hound?
in the sentences ?The dog sat on the mat?and ?The hound sat on the mat?.
To take accountof these similarities WordNet-based similarity mea-sures are used (Patwardhan and Pedersen, 2006).Any terms that occur in only one of the sentencesdo not contribute to the similarity score since theywill have a 0 value in the binary vector.
Any wordswith a 0 value in one of the binary vectors are com-pared with all of the words in the other sentence andthe similarity values computed.
The highest similar-ity value is selected and use to replace the 0 valuein that vector, see Figure 1.
(If the similarity scoreis below the set threshold of 0.5 then the similarityvalue is not used and in these cases the 0 value re-mains unaltered.)
This substitution of 0 values in thevectors ensures that similarity between words can betaken account of when computing sentence similar-ity.Figure 1: Determining word similarity values forvectorsVarious techniques were explored for determiningthe similarity values between words.
These are de-scribed and evaluated in Section 2.1.3.2.1.2 Computing Sentence SimilarityThe similarity between two sentences is com-puted using the cosine metric.
Since the cosine met-ric is a distance measure, which returns a score of 0for identical vectors, its complement is used to pro-656duce the similarity score.
This score is multiplied by5 in order to generate a score in the range requiredfor the task.2.1.3 Computing Word SimilarityThe similarity values for the vectors are computedby first disambiguating each sentence and then ap-plying a similarity measure.
Various approaches forcarrying out these tasks were explored.Word Sense Disambiguation Two simple andcommonly used techniques for Word SenseDisambiguation were applied.Most Frequent Sense (MFS) simply selectsthe first sense in WordNet, i.e., the mostcommon occurring sense for the word.This approach is commonly used as abaseline for word sense disambiguation(McCarthy et al, 2004).Lesk (1986) chooses a synset by comparing itsdefinition against the sentence and select-ing the one with the highest number ofwords in common.Similarity measures WordNet-based similaritymeasures have been found to perform wellwhen used in combination with text similaritymeasures (Mihalcea and Corley, 2006) andseveral of these were compared.
Implementa-tions of these measures from the NLTK (Birdet al, 2009) were used.Path Distance uses the length of the shortestpath between two senses to determine thesimilarity between them.Leacock and Chodorow (1998) expand uponthe path distance similarity measure byscaling the path length by the maximumdepth of the WordNet taxonomy.Resnik (1995) makes use of techniques fromInformation Theory.
The measure of re-latedness between two concepts is basedon the Information Content of the LeastCommon Subsumer.Jiang and Conrath (1997) also uses the In-formation Content of the two inputsynsets.Lin (1998) uses the same values as Jiang andConrath (1997) but takes the ratio of theshared information content to that of theindividual concepts.Results produced by the various combinations ofword sense disambiguation strategy and similaritymeasures are shown in Table 1.
This table showsthe Pearson correlation of the system output with thegold standard over all of the SemEval training data.The row labelled ?Binary?
shows the results usingbinary vectors which are not augmented with anysimilarity values.
The remainder of the table showsthe performance of each of the similarity measureswhen the senses are selected using the two wordsense disambiguation algorithms.Metric MFS LeskBinary 0.657Path Distance 0.675 0.669Leacock and Chodorow (1998) 0.087 0.138Resnik (1995) 0.158 0.153Jiang and Conrath (1997) 0.435 0.474Lin (1998) 0.521 0.631Table 1: Performance of Vector Space Model us-ing various disambiguation strategies and similaritymeasuresThe results in this table show that the only simi-larity measure that leads to improvement above thebaseline is the path measure.
When this is appliedthere is a modest improvement over the baseline foreach of the word sense disambiguation algorithms.However, all other similarity measures lead to a dropin performance.
Overall there seems to be little dif-ference between the performance of the two wordsense disambiguation algorithms.
The best perfor-mance is obtained using the paths distance and MFSdisambiguation.Table 2 shows the results of the highest scoringmethod broken down by the individual corpora usedfor the evaluation.
There is a wide range between thehighest (0.726) and lowest (0.485) correlation scoreswith the best performance being obtained for theMSRvid corpus which contains short, simple sen-tences.657Metric CorrelationMSRpar 0.591MSRvid 0.726SMTeuroparl 0.485Table 2: Correlation scores across individual cor-pora using Path Distance and Most Frequent Sense.2.2 Supervised Machine LearningFor the second approach the sentences are repre-sented as sets of n-grams of varying length, a com-mon approach in text comparison applications whichpreserves some information about the structure ofthe document.
However, like the standard vectorspace model (Section 2.1) this technique also fails toidentify similarity between texts when an alternativechoice of lexical item is used to express the same,or similar, meaning.
To avoid this problem Word-Net is used to generate sets of alternative n-grams.After the n-grams have been generated for each sen-tence they are augmented with semantic alternativescreated using WordNet (Section 2.2.1).
The overlapscores between the n-grams from the two sentencesare used as features for a supervised learning algo-rithm (Section 2.2.2).2.2.1 Generating n-gramsPreprocessing is carried out using NLTK.
Eachsentence is tokenised, lemmatised and stop wordsremoved.
A set of n-grams are then extracted fromeach sentence.
The set of n-grams for the sentenceS is referred to as So.For every n-gram in So a list of alternative n-grams is generated using WordNet.
Each item inthe n-gram is considered in turn and checked to de-termine whether it occurs in WordNet.
If it doesthen a set of alternative lexical items is constructedby combining all terms that are found in all synsetscontaining that item as well as their immediate hy-pernyms and hyponyms of the terms.
An additionaln-gram is created for each item in this set of alterna-tive lexical items by substituting each for the origi-nal term.
This set of expanded n-grams is referred toas Sa.2.2.2 Sentence ComparisonOverlap metrics to determine the similarity be-tween the sets of n-grams are used to create featuresfor the learning algorithm.
For two sentences, S1and S2, four sets of n-grams are compared: S1o,S2o, S1a and S2a (i.e., the n-grams extracted di-rectly from sentences S1 and S2 as well as the mod-ified versions created using WordNet).The n-grams that are generated using WordNet(Sa) are not as important as the original n-grams(So) for determining the similarity between sen-tences and this is accounted for by generating threedifferent scores reflecting the overlap between thetwo sets of n-grams for each sentence.
These scorescan be expressed using the following equations:|S1o ?
S2o|?|S1o| ?
|S2o|(1)|(S1o ?
S2a)?
(S2o ?
S1a)|?|(S1o ?
S2a)| ?
|(S2o ?
S1a)|(2)|S1a ?
S2a|?|S1a| ?
|S2a|(3)Equation 1 is the cosine measure applied to thetwo sets of original n-grams, equation 2 comparesthe original n-grams in each sentence with the alter-native n-grams in the other while equation 3 com-pares the alternative n-grams with each other.Other features are used in addition to these sim-ilarity scores: the mean length of S1 and S2, thedifference between the lengths of S1 and S2 and thecorpus label (indicating which part of the SemEvaltraining data the sentence pair was drawn from).
Wefound that these additional features substantially in-crease the performance of our system, particularlythe corpus label.3 University of Sheffield?s entry for Task 6Our entry for this task consisted of three runs usingthe two approaches described in Section 2.Run 1: Vector Space Model (VS) The first runused the unsupervised vector space approach (Sec-tion 2.1).
Comparison of word sense disambiguationstrategies and semantic similarity measures on thetraining data showed that the best results were ob-tained using the Path Distance Measure combined658with the Most Frequent Sense approach (see Ta-bles 1 and 2) and these were used for the officialrun.
Post evaluation analysis also showed that thisstrategy produced the best performance on the testdata.Run 2: Machine Learning (NG) The secondrun used the supervised machine learning approach(Section 2.2.2).
The various parameters used bythis approach were explored using 10-fold cross-validation applied to the SemEval training data.
Wevaried the lengths of the n-grams generated, exper-imented with various pre-processing strategies andmachine learning algorithms.
The best performancewas obtained using short n-grams, unigrams and bi-grams, and these were used for the official run.
In-cluding longer n-grams did not lead to any improve-ment in performance but created significant com-putational cost due to the number of alternative n-grams that were created using WordNet.
Whenthe pre-processing strategies were compared it wasfound that the best performance was obtained by ap-plying both stemming and stop word removal beforecreating n-grams and this approach was used in theofficial run.
The Weka1 LinearRegression al-gorithm was used for the official run and a singlemodel was created by training on all of the data pro-vided for the task.Run 3: Hybrid (VS + NG) The third run is ahybrid combination of the two methods.
The su-pervised approach (NG) was used for the three datasets that had been made available in the training data(MSRpar, MSRvid and SMT-eur) while the vectorspace model (VS) was used for the other two datasets.
This strategy was based on analysis of perfor-mance of the two approaches on the training data.The NG approach was found to provide the bestperformance.
However it was sensitive to the dataset from which the training data was obtained fromwhile VS, which does not require training data, ismore robust.A diagram depicting the various components ofthe submitted entry is shown in Figure 2.4 EvaluationThe overall performance (ALLnrm) of NG, VG andthe hybrid systems is significantly higher than the1http://www.cs.waikato.ac.nz/ml/weka/Figure 2: System Digram for entryofficial baseline (see Table 3).
The table also in-cludes separate results for each of the evaluationcorpora (rows three to seven): the unsupervised VSmodel performance is significantly higher than thebaseline (p-value of 0.06) over all corpus types, as isthat of the hybrid model.However, the performance of the supervised NGmodel is below the baseline for the (unseen in train-ing data) SMT-news corpus.
Given a pair of sen-tences from an unknown source, the algorithm em-ploys a model trained on all data combined (i.e.,omits the corpus information), which may resemblethe input (On-WN) or it may not (SMT-news).After stoplist removal, the average sentencelength within MSRvid is 4.5, whereas it is 6.0 and6.9 in MSRpar and SMT-eur respectively, and thusthe last two corpora are expected to form better train-ing data for each other.
The overall performance onthe MSRvid data is higher than for the other cor-pora, which may be due to the small number of ad-jectives and the simpler structure of the shorter sen-tences within the corpus.The hybrid system, which selects the supervisedsystem (NG)?s output when the test sentence pairis drawn from a corpus within the training data659Corpus Baseline Vector Space (VS) Machine Learning (NG) Hybrid (NG+VS)ALL .3110 .6054 .7241 .6485ALLnrm .6732 .7946 .8169 .8238MSRpar .4334 .5460 .5166 .5166MSRvid .2996 .7241 .8187 .8187SMT-eur .4542 .4858 .4859 .4859On-WN .5864 .6676 .6390 .6676SMT-news .3908 .4280 .2089 .4280Table 3: Correlation scores from official SemEval resultsRank (/89) Rank Ranknrm RankMeanBaseline 87 85 70Vector Space (VS) 48 44 29Machine Learning (NG) 17 18 37Hybrid 34 15 20Table 4: Ranks from official SemEval resultsand selects the unsupervised system (VS)?s answerotherwise, outperforms both systems in combina-tion.
Contrary to expectations, the supervised sys-tem did not always outperform VS on phrases basedon training data ?
the performance of VS on MSR-par, with its long and complex sentences, provedto be slightly higher than that of NG.
However, theunsupervised system was clearly the correct choicewhen the source was unknown.5 Conclusion and Future WorkTwo approaches for computing semantic similaritybetween sentences were explored.
The first, unsu-pervised approach, uses a vector space model andcomputes similarity between sentences by compar-ing vectors while the second is supervised and rep-resents the sentences as sets of n-grams.
Bothapproaches used WordNet to provide informationabout similarity between lexical items.
Results fromevaluation show that the supervised approach pro-vides the best results on average but also that per-formance of the unsupervised approach is better forsome data sets.
The best overall results for the Se-mEval evaluation were obtained using a hybrid sys-tem that attempts to choose the most suitable ap-proach for each data set.The results reported here show that the semantictext similarity task can be successfully approachedusing lexical overlap techniques augmented withlimited semantic information derived from Word-Net.
In future, we would like to explore whetherperformance can be improved by applying deeperanalysis to provide information about the structureand semantics of the sentences being compared.
Forexample, parsing the input sentences would providemore information about their structure than can beobtained by representing them as a bag of words orset of n-grams.
We would also like to explore meth-ods for improving performance of the n-gram over-lap approach and making it more robust to differentdata sets.AcknowledgementsThis research has been supported by a Google Re-search Award.ReferencesE.
Agirre, D. Cer, M Diab, and A. Gonzalez-Agirre.2012.
Semeval-2012 task 6: A pilot on semantic tex-tual similarity.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation (SemEval 2012), inconjunction with the First Joint Conference on Lexicaland Computational Semantics (*SEM 2012).R.
Baeza-Yates and B. Ribeiro-Neto.
1999.
Modern In-formation Retrieval.
Addison Wesley Longman Lim-ited, Essex.660R.
Barzilay and L. Lee.
2003.
Learning to paraphrase:An unsupervised approach using multiple-sequencealignment.
In Proceedings of the 2003 Human Lan-guage Technology Conference of the North AmericanChapter of the Association for Computational Linguis-tics.M.
Bendersky and W.B.
Croft.
2009.
Finding text reuseon the web.
In Proceedings of the Second ACM Inter-national Conference on Web Search and Data Mining,pages 262?271.
ACM.S.
Bird, E. Klein, and E. Loper.
2009.
Natural LanguageProcessing with Python.
O?Reilly.B.
Dolan, C. Quirk, and C. Brockett.
2004.
Unsuper-vised construction of large paraphrase corpora: Ex-ploiting massively parallel news sources.
In Proceed-ings of Coling 2004, pages 350?356, Geneva, Switzer-land.J.J.
Jiang and D.W. Conrath.
1997.
Semantic SimilarityBased on Corpus Statistics and Lexical Taxonomy.
InInternational Conference Research on ComputationalLinguistics (ROCLING X).D.
Jurafsky and J. Martin.
2009.
Speech and LanguageProcessing.
Pearson, second edition.C.
Leacock and M. Chodorow, 1998.
Combining localcontext and WordNet similarity for word sense identi-fication, pages 305?332.
In C. Fellbaum (Ed.
), MITPress.M.
Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: how to tell a pine conefrom an ice cream cone.
In Proceedings of ACM SIG-DOC Conference, pages 24?26, Toronto, Canada.D.
Lin and P. Pantel.
2001.
Discovery of interence rulesfor question answering.
Natural Language Engineer-ing, 7(4):343?360.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In In Proceedings of the 15th InternationalConference on Machine Learning, pages 296?304.C.
Lin.
2004.
Rouge: A package for automatic evalu-ation of summaries.
In Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop, pages 74?81, Barcelona, Spain, July.C.
Manning and H. Schu?tze.
1999.
Foundations ofStatistical Natural Language Processing.
MIT Press,Cambridge, MA.D.
McCarthy, R. Koeling, J. Weeds, and J. Carroll.
2004.Finding predominant senses in untagged text.
In Pro-ceedings of the 42nd Annual Meeting of the Associa-tion for Computational Lingusitics (ACL-2004), pages280?287, Barcelona, Spain.R.
Mihalcea and C. Corley.
2006.
Corpus-based andknowledge-based measures of text semantic similarity.In In AAAI06, pages 775?780.G.A.
Miller, R. Beckwith, C. Fellbaum, D. Gross, andK.J.
Miller.
1990.
WordNet: An On-line Lexi-cal Database.
International Journal of Lexicography,3(4):235?312.M.
Mohler and R. Mihalcea.
2009.
Text-to-text seman-tic similarity for automatic short answer grading.
InProceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), pages 567?575,Athens, Greece.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of 40th Annual Meeting ofthe Association for Computational Linguistics, pages311?318, Philadelphia, Pennsylvania, USA.S.
Patwardhan and T. Pedersen.
2006.
Using WordNet-based context vectors to estimate the semantic related-ness of concept.
In Proceedings of the workshop on?Making Sense of Sense: Bringing Psycholinguisticsand Computational Linguistics Together?
held in con-junction with the EACL 2006, pages 1?8.S.G.
Pulman and J.Z.
Sukkarieh.
2005.
Automaticshort answer marking.
In Proceedings of the SecondWorkshop on Building Educational Applications Us-ing NLP, pages 9?16, Ann Arbor, Michigan.P.
Resnik.
1995.
Using information content to evaluatesemantic similarity in a taxonomy.
In In Proceedingsof the 14th International Joint Conference on ArtificialIntelligence, pages 448?453.G.
Salton, A. Wong, and C. S. Yang.
1975.
A vectorspace model for automatic indexing.
Commun.
ACM,18(11):613?620.J.
Seo and W.B.
Croft.
2008.
Local text reuse detection.In Proceedings of the 31st Annual International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 571?578.M.
Stevenson and M. Greenwood.
2005.
A SemanticApproach to IE Pattern Induction.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL-05), pages 379?386, AnnArbour, MI.I.
Szpektor, H. Tanev, I. Dagan, and B. Coppola.
2004.Scaling web-based acquisition of entailment relations.In Proceedings of the 2004 Conference on EmpiricalMethods in Natural Language Processing, pages 41?48, Barcelona, Spain.N.
Tintarev and J. Masthoff.
2006.
Similarity fornews recommender systems.
In In Proceedings of theAH?06 Workshop on Recommender Systems and Intel-ligent User Interfaces.F.M.
Zanzotto, M. Pennacchiotti, and A. Moschitti.2009.
A machine learning approach to textual entail-ment recognition.
Natural Language Engineering, 15-04:551?582.661
