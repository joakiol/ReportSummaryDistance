Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 594?602,Sydney, July 2006. c?2006 Association for Computational LinguisticsBroad-Coverage Sense Disambiguation and Information Extraction witha Supersense Sequence Tagger?Massimiliano CiaramitaInst.
of Cognitive Science and TechnologyItalian National Research Councilm.ciaramita@istc.cnr.itYasemin AltunToyota Technological Instituteat Chicagoaltun@tti-c.orgAbstractIn this paper we approach word sensedisambiguation and information extrac-tion as a unified tagging problem.
Thetask consists of annotating text with thetagset defined by the 41 Wordnet super-sense classes for nouns and verbs.
Sincethe tagset is directly related to Wordnetsynsets, the tagger returns partial wordsense disambiguation.
Furthermore, sincethe noun tags include the standard namedentity detection classes ?
person, location,organization, time, etc.
?
the tagger, asa by-product, returns extended named en-tity information.
We cast the problem ofsupersense tagging as a sequential label-ing task and investigate it empirically witha discriminatively-trained Hidden MarkovModel.
Experimental evaluation on themain sense-annotated datasets available,i.e., Semcor and Senseval, shows consid-erable improvements over the best known?first-sense?
baseline.1 IntroductionNamed entity recognition (NER) is the most stud-ied information extraction (IE) task.
NER typi-cally focuses on detecting instances of ?person?,?location?, ?organization?
names and optionallyinstances of ?miscellaneous?
or ?time?
categories.The scalability of statistical NER allowed re-searchers to apply it successfully on large col-lections of newswire text, in several languages,and biomedical literature.
Newswire NER per-formance, in terms of F-score, is in the upper?The first author is now at Yahoo!
Research.
The tag-ger described in this paper is free software and can be down-loaded from http://www.loa-cnr.it/ciaramita.html.80s (Carreras et al, 2002; Florian et al, 2003),while Bio-NER accuracy ranges between the low70s and 80s, depending on the data-set used fortraining/evaluation (Dingare et al, 2005).
Oneshortcoming of NER is its over-simplified onto-logical model, leaving instances of other poten-tially informative categories unidentified.
Hence,the utility of named entity information is limited.In addition, instances to be detected are mainly re-stricted to (sequences of) proper nouns.Word sense disambiguation (WSD) is the taskof deciding the intended sense for ambiguouswords in context.
With respect to NER, WSDlies at the other end of the semantic tagging spec-trum, since the dictionary defines tens of thou-sand of very specific word senses, including NERcategories.
Wordnet (Fellbaum, 1998)1, possiblythe most used resource for WSD, defines wordsenses for verbs, common and proper nouns.
Wordsense disambiguation, at this level of granularity,is a complex task which resisted all attempts ofrobust broad-coverage solutions.
Many distinc-tions are too subtle to be captured automatically,and the magnitude of the class space ?
severalorders larger than NER?s ?
makes it hard to ap-proach the problem with sophisticated, but scal-able, machine learning methods.
Lastly, even ifthe methods would scale up, there are not enoughmanually tagged data, at the word sense level, fortraining a model.
The performance of state ofthe art WSD systems on realistic evaluations isonly comparable to the ?first sense?
baseline (cf.Section 5.3).
Notwithstanding much research, thebenefits of disambiguated lexical information forlanguage processing are still mostly speculative.This paper presents a novel approach to broad-1When referring to Wordnet, throughout the paper, wemean Wordnet version 2.0.594NOUNSSUPERSENSE NOUNS DENOTING SUPERSENSE NOUNS DENOTINGact acts or actions object natural objects (not man-made)animal animals quantity quantities and units of measureartifact man-made objects phenomenon natural phenomenaattribute attributes of people and objects plant plantsbody body parts possession possession and transfer of possessioncognition cognitive processes and contents process natural processescommunication communicative processes and contents person peopleevent natural events relation relations between people or things or ideasfeeling feelings and emotions shape two and three dimensional shapesfood foods and drinks state stable states of affairsgroup groupings of people or objects substance substanceslocation spatial position time time and temporal relationsmotive goals Tops abstract terms for unique beginnersVERBSSUPERSENSE VERBS OF SUPERSENSE VERBS OFbody grooming, dressing and bodily care emotion feelingchange size, temperature change, intensifying motion walking, flying, swimmingcognition thinking, judging, analyzing, doubting perception seeing, hearing, feelingcommunication telling, asking, ordering, singing possession buying, selling, owningcompetition fighting, athletic activities social political and social activities and eventsconsumption eating and drinking stative being, having, spatial relationscontact touching, hitting, tying, digging weather raining, snowing, thawing, thunderingcreation sewing, baking, painting, performingTable 1.
Nouns and verbs supersense labels, and short description (from the Wordnet documentation).coverage information extraction and word sensedisambiguation.
Our goal is to simplify the disam-biguation task, for both nouns and verbs, to a levelat which it can be approached as any other taggingproblem, and can be solved with state of the artmethods.
As a by-product, this task includes andextends NER.
We define a tagset based on Word-net?s lexicographers classes, or supersenses (Cia-ramita and Johnson, 2003), cf.
Table 1.
The sizeof the supersense tagset alows us to adopt a struc-tured learning approach, which takes local depen-dencies between labels into account.
To this ex-tent, we cast the supersense tagging problem as asequence labeling task and train a discriminativeHidden Markov Model (HMM), based on that ofCollins (2002), on the manually annotated Semcorcorpus (Miller et al, 1993).
In two experimentswe evaluate the accuracy of the tagger on the Sem-cor corpus itself, and on the English ?all words?Senseval 3 shared task data (Snyder and Palmer,2004).
The model outperforms remarkably thebest known baseline, the first sense heuristic ?
tothe best of our knowledge, for the first time on themost realistic ?all words?
evaluation setting.The paper is organized as follows.
Section 2introduces the tagset, Section 3 discusses relatedwork and Section 4 the learning model.
Section 5reports on experimental settings and results.
InSection 6 we summarize our contribution and con-sider directions for further research.2 Supersense tagsetWordnet (Fellbaum, 1998) is a broad-coveragemachine-readable dictionary which includes11,306 verbs mapped to 13,508 word senses,called synsets, and 114,648 common and propernouns mapped to 79,689 synsets.
Each noun orverb synset is associated with one of 41 broadsemantic categories, in order to organize thelexicographer?s work of updating and managingthe lexicon (see Table 1).
Since each lexicog-rapher category groups together many synsetsthey have been also called supersenses (Ciaramitaand Johnson, 2003).
There are 26 supersensesfor nouns, 15 for verbs.
This coarse-grainedontology has a number of attractive features, forthe purpose of natural language processing.
First,the small size of the set makes it possible to builda single tagger which has positive consequenceson robustness.
Second, classes, although fairlygeneral, are easily recognizable and not tooabstract or vague.
More importantly, similar wordsenses tend to be merged together.As an example, Table 2 summarizes all sensesof the noun ?box?.
The 10 synsets are mappedto 6 supersenses: ?artifact?, ?quantity?, ?shape?,?state?, ?plant?, and ?act?.
Three similar senses(2), (7) and (9), and the probably related (8), aremerged in the ?artifact?
supersense.
This processcan help disambiguation because it removes sub-5951.
{box} (container) ?he rummaged through a box ofspare parts?
- n.artifact2.
{box, loge} (private area in a theater or grandstandwhere a small group can watch the performance) ?theroyal box was empty?
- n.artifact3.
{box, boxful} (the quantity contained in a box) ?hegave her a box of chocolates?
- n.quantity4.
{corner, box} (a predicament from which a skillful orgraceful escape is impossible) ?his lying got him into atight corner?
- n.state5.
{box} (a rectangular drawing) ?the flowchart containedmany boxes?
- n.shape6.
{box, boxwood} (evergreen shrubs or small trees) -n.plant7.
{box} (any one of several designated areas on a ballfield where the batter or catcher or coaches are posi-tioned) ?the umpire warned the batter to stay in the bat-ter?s box?
- n.artifact8.
{box, box seat} (the driver?s seat on a coach) ?an armedguard sat in the box with the driver?
- n.artifact9.
{box} (separate partitioned area in a public place for afew people) ?the sentry stayed in his box to avoid thecold?
- n.artifact10.
{box} (a blow with the hand (usually on the ear)) ?Igave him a good box on the ear?
- n.actTable 2.
The noun ?box?
in Wordnet: each line lists onesynset, the set of synonyms, a definition, an optionalexample sentence, and the supersense label.tle distinctions, which are hard to discriminate andincrease the size of the class space.
One possi-ble drawback is that senses which one might wantto keep separate, e.g., the most common sensebox/container (1), can be collapsed with others.One might argue that all ?artifact?
senses sharesemantic properties which differentiate them fromthe other senses and can support useful semanticinferences.
Unfortunately, there are no general so-lutions to the problem of sense granularity.
How-ever, major senses identified by Wordnet are main-tained at the supersense level.
Hence, supersense-disambiguated words are also, at least partially,synset-disambiguated.Since Wordnet includes both proper and com-mon nouns, the new tagset suggests an extendednotion of named entity.
As well as the usualNER categories, ?person?, ?group?, ?location?,and ?time?2, supersenses include categories suchas artifacts, which can be fairly frequent, but usu-ally neglected.
To a greater extent than in stan-dard NER, research in Bio-NER has focused onthe adoption of richer ontologies for informationextraction.
Genia (Ohta et al, 2002), for exam-ple, is an ontology of 46 classes ?
with annotated2The supersense category ?group?
is rather a superordi-nate of ?organization?
and has wider scope.corpus ?
designed for supporting information ex-traction in the molecular biology domain.
In addi-tion, there is growing interest for extracting rela-tions between entities, as a more useful type of IE(cf.
(Rosario and Hearst, 2004)).Supersense tagging is inspired by similar con-siderations, but in a domain-independent setting;e.g., verb supersenses can label semantic interac-tions between nominal concepts.
The followingsentence (Example 1), extracted from the data ?further described in Section 5.1 ?
shows the infor-mation captured by the supersense tagset:(1) Clara Harrisn.person, one of theguestsn.person in the boxn.artifact, stoodupv.motion and demandedv.communicationwatern.substance.As Example 1 shows there is more informationthat can be extracted from a sentence than justthe names; e.g.
the fact that ?Clara Harris?
andthe following ?guests?
are both tagged as ?person?might suggest some sort of co-referentiality, whilethe coordination of verbs of motion and commu-nication, as in ?stood up and demanded?, might beuseful for language modeling purposes.
In such asetting, structured learning methods, e.g., sequen-tial, can help tagging by taking the senses of theneighboring words into account.3 Related WorkSequential models are common in NER, POS tag-ging, shallow parsing, etc..
Most of the work inWSD, instead, has focused on labeling each wordindividually, possibly revising the assignments ofsenses at the document level; e.g., following the?one sense per discourse?
hypothesis (Gale et al,1992).
Although it seems reasonable to assumethat occurrences of word senses in a sentence canbe correlated, hence that structured learning meth-ods could be successful, there has not been muchwork on sequential WSD.
Segond et al (1997) arepossibly the first to have applied an HMM tag-ger to semantic disambiguation.
Interestingly, tomake the method more tractable, they also usedthe supersense tagset and estimated the model onSemcor.
By cross-validation they show a markedimprovement over the first sense baseline.
How-ever, in (Segond et al, 1997) the tagset is used dif-ferently, by defining equivalence classes of wordswith the same set of senses.
From a similar per-spective, de Loupy et al (de Loupy et al, 1998)596also investigated the potential advantages of usingHMMs for disambiguation.
More recently, vari-ants of the generative HMM have been applied toWSD (Molina et al, 2002; Molina et al, 2004)and evaluated also on Senseval data, showing per-formance comparable to the first sense baseline.Previous work on prediction at the supersenselevel (Ciaramita and Johnson, 2003; Curran, 2005)has focused on lexical acquisition (nouns exclu-sively), thus aiming at word type classificationrather than tagging.
As far as applications are con-cerned, it has been shown that supersense infor-mation can support supervised WSD, by provid-ing a partial disambiguation step (Ciaramita et al,2003).
In syntactic parse re-ranking supersenseshave been used to build useful latent semantic fea-tures (Koo and Collins, 2005).
We believe thatsupersense tagging has the potential to be useful,in combination with other sources of informationsuch as part of speech, domain-specific NER mod-els, chunking or shallow parsing, in tasks suchas question answering and information extractionand retrieval, where large amounts of text needto be processed.
It is also possible that this kindof shallow semantic information can help build-ing more sophisticated linguistic analysis as in fullsyntactic parsing and semantic role labeling.4 Sequence TaggingWe take a sequence labeling approach to learn-ing a model for supersense tagging.
Our goal isto learn a function from input vectors, the obser-vations from labeled data, to response variables,the supersense labels.
POS tagging, shallow pars-ing, NP-chunking and NER are all examples ofsequence labeling tasks in which performance canbe significantly improved by optimizing the choiceof labeling over whole sequences of words, ratherthan individual words.
The limitations of the gen-erative approach to sequence tagging, i. e. HiddenMarkov Models, have been overcome by discrim-inative approaches proposed in recent years (Mc-Callum et al, 2000; Lafferty et al, 2001; Collins,2002; Altun et al, 2003).
In this paper we applyperceptron trained HMMs originally proposed in(Collins, 2002).4.1 Perceptron-trained HMMHMMs define a probabilistic model for observa-tion/label sequences.
The joint model of an obser-vation/label sequence (x,y), is defined as:P (y,x) =?iP (yi|yi?1)P (xi|yi)), (2)where yi is the ith label in the sequence and xi isthe ith word.
In the NLP literature, a common ap-proach is to model the conditional distribution oflabel sequences given the label sequences.
Thesemodels have several advantages over generativemodels, such as not requiring questionable inde-pendence assumptions, optimizing the conditionallikelihood directly and employing richer featurerepresentations.
This task can be represented aslearning a discriminant function F : X ?Y ?
IR,on a training data of observation/label sequences,where F is linear in a feature representation ?
de-fined over the joint input/output spaceF (x,y;w) = ?w,?(x,y)?.
(3)?
is a global feature representation, mapping each(x,y) pair to a vector of feature counts ?
(x,y) ?IRd, where d is the total number of features.
Thisvector is given by?
(x,y) =d?i=1|y|?j=1?i(yj?1, yj ,x).
(4)Each individual feature ?i typically represents amorphological, contextual, or syntactic property,or also the inter-dependence of consecutive la-bels.
These features are described in detail in Sec-tion 4.2.
Given an observation sequence x, wemake a prediction by maximizing F over the re-sponse variables:fw(x) = argmaxy?YF (x,y;w).
(5)This involves computing the Viterbi decoding withrespect to the parameter vector w ?
IRd.
Thecomplexity of the Viterbi algorithm scales linearlywith the length of the sequence.There are different ways of estimatingw for thedescribed model.
We use the perceptron algorithmfor sequence tagging (Collins, 2002).
The per-ceptron algorithm focuses on minimizing the errorrate, without involving any normalization factors.This property makes it very efficient which is a de-sirable feature in a task dealing with a large tagsetsuch as ours.
Additionally, the performance ofperceptron-trained HMMs is very competitive ona number of tasks; e.g., in shallow parsing, where597Algorithm 1 Hidden Markov average perceptronalgorithm.1: Initialize w0 = ~02: for t = 1...., T do3: Choose xi4: Compute y?
= argmaxy?Y F (xi,y;w)5: if yi 6= y?
then6: wt+1 ?
wt + ?(xi,yi)?
?
(xi, y?
)7: end if8: w = 1T?twt9: end for10: return wthe perceptron performance is comparable to thatof Conditional Random Field models (Sha andPereira, 2003), The tendency to overfit of the per-ceptron can be mitigated in a number of ways in-cluding regularization and voting.
Here we applyaveraging and straightforwardly extended Collinsalgorithm, summarized in Algorithm 1.4.2 FeaturesWe used the following combination ofspelling/morphological and contextual fea-tures.
For each observed word xi in the data ?extracts the following features:1.
Words: xi, xi?1, xi?2, xi+1, xi+2;2.
First sense: supersense baseline predictionfor xi, fs(xi), cf.
Section 5.3;3.
Combined (1) and (2): xi + fs(xi);4.
Pos: posi (the POS of xi), posi?1, posi?2,posi+1, posi+2, posi[0], posi?1[0], posi?2[0],posi+1[0], posi+2[0], pos commi if xi?s POStags is ?NN?
or ?NNS?
(common nouns), andpos propi if xi?s POS is ?NNP?
or ?NNPS?
(proper nouns);5.
Word shape: sh(xi), sh(xi?1), sh(xi?2),sh(xi+1), sh(xi+2), where sh(xi) is asdescribed below.
In addition shi = lowif the first character of xi is lowercase,shi = cap brk if the first character of xi is up-percase and xi?1 is a full stop, question orexclamation mark, or xi is the first word ofthe sentence, shi = cap nobrk otherwise;6.
Previous label: supersense label yi?1.Word features (1) are morphologically simplifiedusing the morphological functions of the Word-net library.
The first sense feature (2) is the labelpredicted for xi by the baseline model, cf.
Sec-tion 5.3.
POS labels (4) were generated usingBrants?
TnT tagger (Brants, 2002).
POS featuresof the form posi[0] extract the first character fromthe POS label, thus providing a simplified repre-sentation of the POS tag.
Finally, word shape fea-tures (5) are regular expression-like transforma-tion in which each character c of a string s is sub-stituted with X if c is uppercase, if lowercase, cis substituted with x, if c is a digit it is substitutedwith d and left as it is otherwise.
In addition eachsequence of two or more identical characters c issubstituted with c?.
For example, for s = ?MerrillLynch& Co.?, sh(s) = Xx ?
Xx ?&Xx..Exploratory experiments with richer featuresets, including syntactic information, affixes, andtopic labels associated with words, did not resultin improvements in terms of performance.
Whilemore experiments are needed to investigate theusefulness of other sources of information, the fea-ture set described above, while basic, offers goodgeneralization properties.5 Experiments5.1 DataWe experimented with the following data-sets3.The Semcor corpus (Miller et al, 1993), a frac-tion of the Brown corpus (Kuc?era and Francis,1967) which has been manually annotated withWordnet synset labels.
Named entities of the cat-egories ?person?, ?location?
and ?group?
are alsoannotated.
The original annotation with Wordnet1.6 synset IDs has been converted to the most re-cent version 2.0 of Wordnet.
Semcor is dividedin three parts: ?brown1?
and ?brown2?, here re-ferred to as ?SEM?, in which nouns, verbs, adjec-tives and adverbs are annotated.
In addition, thesection ?brownv?, ?SEMv?
here, contains annota-tions only for verbs.
We also experimented withthe Senseval-3 English all-words tasks data (Sny-der and Palmer, 2004), here called ?SE3?.
TheSenseval all-words task evaluates the performanceof WSD systems on all open class words in com-plete documents.
The Senseval-3 data consists oftwo Wall Street Journal Articles, ?wsj 1778?
and3These datasets are available in a con-sistent format and can be downloaded fromhttp://www.cs.unt.edu/ rada/downloads.html598DatasetCounts SE3 SEM SEMvSentences 300 20,138 17,038Tokens 5,630 434,774 385,546Supersenses 1,617 135,135 40,911Verbs 725 47,710 40,911Nouns 892 87,425 0Avg-poly-N-WS 4.66 4.41 4.33Avg-poly-N-SS 2.86 2.75 2.66Avg-poly-V-WS 11.17 10.87 11.05Avg-poly-V-SS 4.20 4.11 4.16Table 3.
Statistics of the datasets.
The row ?Super-senses?
lists the number of instances of supersenselabels, partitioned, in the following two rows, betweenverb and noun supersense labels.
The lowest four rowssummarize average polysemy figures at the synset andsupersense level for both nouns and verbs.
?wsj 1695?, and a fiction excerpt, ?cl 23?, fromthe unannotated portion of the Brown corpus.
Ta-ble 3 summarizes a few statistics about the compo-sition of the datasets.
The four lower rows reportthe average polysemy of nouns (?N?)
and verbs(?V?
), in each dataset, both at the synset level(?WS?)
and supersense (?SS?)
level.
The averagenumber of senses decreases significantly when themore general sense inventory is considered.We substituted the corresponding supersense toeach noun and verb synset in all three data-sets:SEM, SEMv and SE3.
All other tokens werelabeled ?0?.
The supersense label ?noun.Tops?refers to 45 synsets which lie at the very topof the Wordnet noun hierarchy.
Some of thesesynsets are expressed by very general nouns suchas ?biont?, ?benthos?, ?whole?, and ?nothing?.However, others undoubtedly refer to other super-senses, for which they provide the label, such as?food?, ?person?, ?plant?
or ?animal?.
Since thesenouns tend to be fairly frequent, it is confusingand inconsistent to label them ?noun.Tops?
; e.g.,nouns such as ?chowder?
and ?Swedish meatball?would be tagged as ?noun.food?, but the noun?food?
would be tagged as ?noun.Tops?.
For thisreason, in all obvious cases, we substituted the?noun.Tops?
label with the more specific super-sense label for the noun4.The SEMv dataset only includes supersense la-bels for verbs.
In order to avoid unwanted falsenegatives, that is, thousands of nouns labeled ?0?,4The nouns which are left with the ?noun.Top?
label are:entity, thing, anything, something, nothing, object, livingthing, organism, benthos, heterotroph, life, and biont.we applied the following procedure.
Rather thanusing the full sentences from the SEMv dataset,from each sentence we generated the fragments in-cluding a verb but no common or proper nouns;e.g., from a sentence such as ?Karns?
ruling per-tainedverb.stative to eight of the 10 cases.?
only thefragment ?pertainedverb.stative to eight of the 10?is extracted and used for training.Sometimes more than one label is assigned toa word, in all data-sets.
In these cases we adoptedthe heuristic of only using the first label in the dataas the correct synset/supersense.
We leave the ex-tension of the tagger to the multilabel case for fu-ture research.
As for now, we can expect that thissolution will simply lower, somewhat, both thebaseline and the tagger performance.
Finally, weadopted a beginning (B) and continuation of entity(I) plus no label (0), encoding; i.e., the actual classspace defines 83 labels.5.2 SetupThe supersense tagger was trained on the Semcordatasets SEM and SEMv.
The only free parame-ter to set in evaluation is the number of iterationsto perform T (cf.
Algorithm 1).
We evaluated themodel?s accuracy on Semcor by splitting the SEMdata randomly in training, development and evalu-ation.
In a 5-fold cross-validation setup the taggerwas trained on 4/5 of the SEM data, the remain-ing data was split in two halves, one used to fix Tthe other for evaluating performance on test.
Thefull SEMv data was always added to the trainingportion of SEM.
We also evaluated the model onthe Senseval-3 data, using the same value for T setby cross-validation on the SEM data5.
The order-ing of the training instances is randomized acrossdifferent runs, therefore the algorithm outputs dif-ferent results after each run, even if the evaluationset is fixed, as is the case for the Senseval evalu-ation.
The variance in the results on the SE3 datawas measured in this way.5.3 Baseline taggerThe first sense baseline is the supersense of themost frequent synset for a word, according toWordnet?s sense ranking.
This baseline is verycompetitive inWSD tasks, and it is extremely hardto improve upon even slightly.
In fact, the baselinehas been proposed as a good alternative to WSD5On average T is equal to 12 times the size of the trainingdata.599Semcor Senseval-3Method Recall Precision F-score [?]
Recall Precision F-score [?
]Rand 42.99 38.17 40.44 42.09 35.84 38.70Baseline 69.25 63.90 66.47 68.65 60.10 64.09Supersense-Tagger 77.71 76.65 77.18 0.45 73.74 67.60 70.54 0.21Table 4.
Summary of results for random and first sense baselines and supersense tagger, ?
is the standard errorcomputed on the five trials results.altogether (cf.
(McCarthy et al, 2004)).
For thisreason we include the first sense prediction as oneof the features of our tagging model.We apply the heuristic as follows.
First, in eachsentence, we identify the longest sequence whichhas an entry in Wordnet as either noun or verb.We carry out this step using the Wordnet?s libraryfunctions, which perform also morphological sim-plification.
Hence, in Example 1 the entry ?standup?
is detected, although also ?stand?
has an en-try in Wordnet.
Then, each word identified inthis way is assigned its most frequent sense ?
theonly one available if the word is unambiguous.
Toreduce the number of candidate supersenses wedistinguish between common and proper nouns;e.g.
?Savannah?
(city/river) is distinguished from?savannah?
(grassland).
This method improvesslightly the accuracy of the baseline which doesnot distinguish between different types of nouns.5.4 ResultsTable 4 summarizes overall performance6.
Thefirst line shows the accuracy of a baseline whichassigns possible supersenses of identified words atrandom.
The second line shows the performanceof the first sense baseline (cf.
Section 5.3), themarked difference between the two is a measure ofthe robustness of the first sense heuristic.
On theSemcor data the tagger improves over the base-line by 10.71%, 31.19% error reduction, whileon Senseval-3 the tagger improves over the base-line by 6.45%, 17.96% error reduction.
We canput these results in context, although indirectly,by comparison with the results of the Senseval-3 all words task systems.
There, with a base-line of 62.40%, only 4 out of 26 systems per-formed above the baseline, with the two best sys-tems (Mihalcea and Faruque, 2004; Decadt et al,2004) achieving an F-score of 65.2% (2.8% im-provement, 7.45% error reduction).
The systembased on the HMM tagger (Molina et al, 2004),6Scoring was performed with a re-implementation of the?conlleval?
script .achieved an F-score of 60.9%.
The supersensetagger improves mostly on precision, while alsoimproving on recall.
Overall the tagger achievesF-scores between 70.5 and 77.2%.
If we comparethese figures with the accuracy of NER taggersthe results are very encouraging.
Given the con-siderably larger ?
one order of magnitude ?
classspace some loss has to be expected.
Experimentswith augmented tagsets in the biomedical domainalso show performance loss with respect to smallertagsets; e.g., Kazama et al (2002) report an F-score of 56.2% on a tagset of 25 Genia classes,compared to the 75.9% achieved on the simplestbinary case.
The sequence fragments from SEMvcontribute about 1% F-score improvement.Table 5 focuses on subsets of the evaluation.The upper part summarizes the results on Sem-cor for the classes comparable to standard NER?s:?person?, ?group?, ?location?
and ?time?.
How-ever, these categories here are composed of com-mon nouns as well as proper names/named enti-ties.
On this four tags the tagger achieves an aver-age 82.46% F-score, not too far from NER results.The lower portion of Table 5 summarizes the re-sults on the five most frequent noun and verb su-persense labels on the Senseval-3 data, providingmore specific evidence for the supersense tagger?sdisambiguation accuracy.
The tagger outperformsthe first sense baseline on all categories, with theexception of ?verb.cognition?
and ?noun.person?.The latter case has a straightforward explanation,named entities (e.g., ?Phil Haney?, ?Chevron?
or?Marina District?)
are not annotated in the Sense-val data, while they are in Semcor.
Hence the tag-ger learns a different model for nouns than the oneused to annotate the Senseval data.
Because of thisdiscrepancy the tagger tends to return false posi-tives for some categories.
In fact, the other nouncategories on which the tagger performs poorly inSE3 are ?group?
and ?location?
(baseline 52.10tagger 44.72 and baseline 47.62% tagger 47.54%F-score).
Naturally, the lower performance onSenseval is also explained by the fact that the eval-600NER supersenses in SemcorSupersense-Tagger BaselineSupersense # Supersenses R P F R P Fn.person 1526 92.04 87.94 89.94 56.29 77.35 65.16n.group 665 75.38 79.56 77.40 62.42 66.81 64.54n.location 459 77.21 75.37 76.25 67.88 63.33 65.53n.time 412 88.36 84.30 86.27 78.26 83.88 80.985 most frequent verb supersenses in Senseval-3Supersense # Supersenses R P F R P Fv.stative 184 80.33 81.30 80.81 72,83 63.81 68.02v.communication 88 77.53 83.36 80.33 71.91 74.42 73.14v.motion 81 69.63 64.54 66.98 58.02 60.26 59.12v.cognition 61 73.44 67.91 70.56 75.41 71.87 73.60v.change 60 68.33 67.47 67.89 56.67 57.63 57.145 most frequent noun supersenses in Senseval-3Supersense # Supersenses R P F R P Fn.person 148 92.24 60.49 73.06 89.12 79.39 83.97n.artifact 131 80.91 77.73 79.29 74.24 75.97 75.10n.act 96 61.46 72.37 66.45 58.33 65.12 61.54n.cognition 67 45.80 52.87 49.06 49.28 46.58 47.89n.event 60 70.33 89.83 78.87 71.67 75.44 73.50Table 5.
Summary of results of baseline and tagger on selected subsets of labels: NER categories evaluated onSemcor (upper section), and 5 most frequent verb (middle) and noun (bottom) categories evaluated on Senseval.uation comes from different sources than training.6 ConclusionsIn this paper we presented a novel approach tobroad-coverage word sense disambiguation andinformation extraction.
We defined a tagset basedon Wordnet supersenses, a much simpler and gen-eral semantic model than Wordnet which, how-ever, preserves significant polysemy informationand includes standard named entity recognitioncategories.
We showed that in this framework it ispossible to perform accurate broad-coverage tag-ging with state of the art sequence learning meth-ods.
The tagger considerably outperformed themost competitive baseline on both Semcor andSenseval data.
To the best of our knowledge the re-sults on Senseval data provide the first convincingevidence of the possibility of improving by con-siderable amounts over the first sense baseline.We believe both the tagset and the structuredlearning approach contribute to these results.
Thesimplified representation obviously helps by re-ducing the number of possible senses for eachword (cf.
Table 3).
Interestingly, the relative im-provement in performance is not as large as therelative reduction in polysemy.
This indicates thatsense granularity is only one of the problems inWSD.
More needs to be understood concerningsources of information, and processes, that affectword sense selection in context.
As far as the tag-ger is concerned, we applied the simplest featurerepresentation, more sophisticated features can beused, e.g., based on kernels, which might con-tribute significantly by allowing complex featurecombinations.
These results also suggest new di-rections of research within this model.
In partic-ular, the labels occurring in each sequence tendto coincide with predicates (verbs) and arguments(nouns and named entities).
A sequential depen-dency model might not be the most accurate atcapturing the grammatical dependencies betweenthese elements.
Other conditional models, e.g.,designed on head to head, or similar, dependen-cies could prove more appropriate.Another interesting issue is the granularity ofthe tagset.
Supersenses seem more practical thensynsets for investigating the impact of broad-coverage semantic tagging, but they define a verysimplistic ontological model.
A natural evolutionof this kind of approach might be one which startsby defining a semantic model at an intermediatelevel of abstraction (cf.
(Ciaramita et al, 2005)).601ReferencesY.
Altun, T. Hofmann, and M. Johnson.
2003.Discriminative Learning for Label Sequences viaBoosting.
In Proceedings of NIPS 2003.T.
Brants.
2002.
TnT - A Statistical Part-of-SpeechTagger.
In Proceedings of ANLP 2000.X.
Carreras, L. Marquez, and L. Padro.
2002.
NamedEntity Extraction Using AdaBoost.
In Proceedingsof CONLL 2002.M.
Ciaramita and M. Johnson.
2003.
Supersense Tag-ging of Unknown Nouns in WordNet.
In Proceed-ings of EMNLP 2003.M.
Ciaramita, T. Hofmann, and M. Johnson.
2003.
Hi-erarchical Semantic Classification: Word Sense Dis-ambiguation with World Knowledge.
In Proceed-ings of IJCAI 2003.M.
Ciaramita, S. Sloman, M. Johnson, and E. Upfal.2005.
Hierarchical Preferences in a Broad-CoverageLexical Taxonomy.
In Proceedings of CogSci 2005.M.
Collins.
2002.
Discriminative Training Methodsfor Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In Proceedingsof EMNLP 2002, pages 1?8.J.
Curran.
2005.
Supersense Tagging of UnknownNouns Using Semantic Similarity.
In Proceedingsof ACL 2005, pages 26?33.C.
de Loupy, M. El-Beze, and P.F.
Marteau.
1998.Word Sense Disambiguation Using HMM Tagger.In Proceedings of LREC 1998, pages 1255?1258.B.
Decadt, V. Hoste, W. Daelemans, and A. van derBosch.
2004.
GAMBL, Genetic Algorithm Opti-mization of Memory-Based WSD.
In Proceedingsof SENSEVAL-3/ACL 2004.S.
Dingare, M. Nissim, J. Finkel, C. Manning, andC.
Grover.
2005.
A System for Identifying NamedEntities in Biomedical Text: How Results fromTwo Evaluations Reflect on Both the System andthe Evaluations.
Comparative and Functional Ge-nomics, 6:77?85.C.
Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge.R.
Florian, A. Ittycheriah, H. Jing, and T. Zhang.
2003.Named Entity Extraction through Classifier Combi-nation.
In Proceedings of CONLL 2003.W.
Gale, K. Church, and D. Yarowsky.
1992.
OneSense per Discourse.
In Proceedings of the DARPAWorkshop on Speech and Natural Language.J.
Kazama, T. Makino, Y. Ohta, and J. Tsujii.
2002.Tuning Support Vector Machines for BiomedicalNamed Entity Recognition.
In Proceedings of theWorkshop on Natural Language Processing in theBiomedical Domain (ACL 2002).T.
Koo and M. Collins.
2005.
Hidden-Variable Mod-els for Discriminative Reranking.
In Proceedings ofEMNLP 2005.H.
Kuc?era and W. Francis.
1967.
Computational Anal-ysis of Present-Day American English.
Brown Uni-versity Press, Providence, RI.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proceed-ings of ICML 2001, pages 282?289.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Max-imum Entropy Markov Models for Information Ex-traction and Segmentation.
In Proceedings of ICML2000, pages 591?598.D.
McCarthy, R. Koeling, and J. Carroll.
2004.
Find-ing Predominant Senses in Untagged Text.
In Pro-ceedings of ACL 2004.R.
Mihalcea and E. Faruque.
2004.
SenseLearner:Minimally Supervised Word Sense Disambiguationfor All Words in Open Text.
In Proceedings ofSENSEVAL-3/ACL 2004.G.A.
Miller, C. Leacock, T. Randee, and R. Bunker.1993.
A Semantic Concordance.
In Proceedings ofthe 3 DARPA Workshop on Human Language Tech-nology, pages 303?308.A.
Molina, F. Pla, and E. Segarra.
2002.
A Hid-den Markov Model Approach to Word Sense Dsi-ambiguation.
In Proceedings of IBERAMIA 2002.A.
Molina, F. Pla, and E. Segarra.
2004.
WSD SystemBased on Specialized Hidden Markov Model (upv-shmm-eaw).
In Proceedings of SENSEVAL-3/ACL2004.Y.
Ohta, Y. Tateisi, J. Kim, H. Mima, and J. Tsujii.2002.
The GENIA Corpus: An Annotated ResearchAbstract Corpus in the Molecular Biology Domain.In Proceedings of HLT 2002.B.
Rosario and M. Hearst.
2004.
Classifying Seman-tic Relations in Bioscience Text.
In Proceedings ofACL 2004).F.
Segond, A. Schiller, G. Grefenstette, and J.P.Chanod.
1997.
An Experiment in Semantic TaggingUsing Hidden Markov Model.
In Proceedings of theWorkshop on Automatic Information Extraction andBuilding of Lexical Semantic Resources (ACL/EACL1997), pages 78?81.F.
Sha and F. Pereira.
2003.
Shallow Parsing withConditional Random Fields.
In Proceedings of HLT-NAACL 2003, pages 213?220.B.
Snyder and M. Palmer.
2004.
The english All-Words Tasks.
In Proceedings of SENSEVAL-3/ACL2004.602
