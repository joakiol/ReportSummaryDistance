Proceedings of NAACL HLT 2007, pages 57?64,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsDirect Translation Model 2Abraham Ittycheriah and Salim RoukosIBM T.J. Watson Research Center1101 Kitchawan RoadYorktown Heights, NY 10598{abei,roukos}@us.ibm.comAbstractThis paper presents a maximum entropy ma-chine translation system using a minimal setof translation blocks (phrase-pairs).
Whilerecent phrase-based statistical machine trans-lation (SMT) systems achieve significant im-provement over the original source-channel sta-tistical translation models, they 1) use a largeinventory of blocks which have significant over-lap and 2) limit the use of training to just afew parameters (on the order of ten).
In con-trast, we show that our proposed minimalistsystem (DTM2) achieves equal or better per-formance by 1) recasting the translation prob-lem in the traditional statistical modeling ap-proach using blocks with no overlap and 2) re-lying on training most system parameters (onthe order of millions or larger).
The new modelis a direct translation model (DTM) formu-lation which allows easy integration of addi-tional/alternative views of both source and tar-get sentences such as segmentation for a sourcelanguage such as Arabic, part-of-speech of bothsource and target, etc.
We show improvementsover a state-of-the-art phrase-based decoder inArabic-English translation.1 IntroductionStatistical machine translation takes a source se-quence, S = [s1 s2 .
.
.
sK ], and generates a targetsequence, T ?
= [t1 t2 .
.
.
tL], by finding the mostlikely translation given by:T ?
= argmaxTp(T |S).1.1 Block selectionRecent statistical machine translation (SMT) al-gorithms generate such a translation by incorpo-rating an inventory of bilingual phrases (Och andNey, 2000).
A m-n phrase-pair, or block, is a se-quence of m source words paired with a sequenceof n target words.
The inventory of blocks in cur-rent systems is highly redundant.
We illustrate theredundancy using the example in Table 1 whichlljnpAlmrkzypllHzbAl$ywEyAlSynythePolitburooftheCentralCommitteeoftheChineseCommunistPartyAlmktbAlsyAsyFigure 1: Example of Arabic snipet and alignmentto its English translation.shows a set of phrases that cover the two-wordArabic fragment ?lljnp Almrkzyp?
whose align-ment and translation is shown in Figure 1.
Onenotices the significant overlap between the vari-ous blocks including the fact the output target se-quence ?of the central committee?
can be pro-duced in at least two different ways: 1) as 2-4 block?lljnp Almrkzyp | of the central committee?
cov-ering the two Arabic words, or 2) by using the 1-3 block ?Almrkzyp | of the central?
followed bycovering the first Arabic word with the 1-1 block?lljnp | committee?.
In addition, if one adds onemore word to the Arabic fragment in the third posi-tion such as the block ?AlSyny | chinese?
the over-lap increases significantly and more alternate possi-bilities are available to produce an output such asthe ?of the central chinese committee.
?In this work, we propose to only use 1-n blocks andavoid completely the redundancy obtained by the useof m-n blocks for m > 1 in current phrase-based sys-tems.
We discuss later how by defining appropriatefeatures in the translation model, we capture the im-portant dependencies required for producing n-longfragments for an m-word input sequence includingthe reordering required to produce more fluent out-put.
So in Table 1 only the blocks corresponding toa single Arabic word are in the block inventory.
Todifferentiate this work from previous approaches in57lljnp Almrkzypcommittee centralof the commission the centralcommission of the centralof the committee of centralthe committee and the centralof the commission on and centralthe commission , centralcommittee of ?s central.
.
.
.
.
.of the central committee(11)of the central committee of (11)the central committee of (8)central committee(7)committee central (2)central committee , (2).
.
.Table 1: Example Arabic-English blocks showingpossible 1-n and 2-n blocks ranked by frequency.Block count is given in () for 2-n blocks.direct modeling for machine translation, we call ourcurrent approach DTM2 (Direct Translation Model2).1.2 Statistical modeling for translationEarlier work in statistical machine translation(Brown et al, 1993) is based on the ?noisy-channel?formulation whereT ?
= arg maxTp(T |S) = argmaxTp(T )p(S|T ) (1)where the target language model p(T ) is further de-composed asp(T ) ?
?ip(ti|ti?1, .
.
.
, ti?k+1)where k is the order of the language model and thetranslation model p(S|T ) has been modeled by asequence of five models with increasing complexity(Brown et al, 1993).
The parameters of each of thetwo components are estimated using Maximum Like-lihood Estimation (MLE).
The LM is estimated bycounting n-grams and using smoothing techniques.The translation model is estimated via the EM algo-rithm or approximations that are bootstrapped fromthe previous model in the sequence as introduced in(Brown et al, 1993).
As is well known, improvedresults are achieved by modifying the Bayes factor-ization in Equation 1 above by weighing each distri-bution differently as in:p(T |S) ?
p?
(T )p1??
(S|T ) (2)This is the simplest MaxEnt1 model that uses twofeature functions.
The parameter ?
is tuned on adevelopment set (usually to improve an error met-ric instead of MLE).
This model is a special caseof the Direct Translation Model proposed in (Pap-ineni et al, 1997; Papineni et al, 1998) for languageunderstanding; (Foster, 2000) demostrated perplex-ity reductions by using direct models; and (Och andNey, 2002) employed it very successfully for languagetranslation by using about ten feature functions:p(T |S) = 1Z exp?i?i?i(S, T )Many of the feature functions used for translation areMLE models (or smoothed variants).
For example,if one uses ?1 = log(p(T )) and ?2 = log(p(S|T )) weget the model described in Equation 2.
Most phrase-based systems, including the baseline decoder usedin this work use feature functions:?
a target word n-gram model (e.g., n = 5),?
a target part-of-speech n-gram model (n ?
5),?
various translation models such as a block in-ventory with the following three varieties: 1) theunigram block count, 2) a model 1 score p(si|ti)on the phrase-pair, and 3)a model 1 score forthe other direction p(ti|si),?
a target word count penalty feature |T |,?
a phrase count feature,?
a distortion model (Al-Onaizan and Papineni,2006).The weight vector ?
is estimated by tuning on arather small (as compared to the training set used todefine the feature functions) development set usingthe BLEU metric (or other translation error met-rics).
Unlike MaxEnt training, the method (Och,2003) used for estimating the weight vector for BLEUmaximization are not computationally scalable for alarge number of feature functions.2 Related WorkMost recent state-of-the-art machine translation de-coders have the following aspects that we improveupon in this work: 1) block style, and 2) model pa-rameterization and parameter estimation.
We dis-cuss each item next.1The subfields of log-linear models, exponential fam-ily, and MaxEnt describe the equivalent techniques fromdifferent perspectives.582.1 Block styleIn order to extract phrases from alignments availablein one or both directions, most SMT approaches usea heuristic such as union, intersection, inverse pro-jection constraint, etc.
As discussed earlier, theseapproaches result in a large overlap between the ex-tracted blocks (longer blocks overlap with all theshorter subcomponents blocks).
Also, slightly re-stating the advantages of phrase-pairs identified in(Quirk and Menezes, 2006), these blocks are effec-tive at capturing context including the encoding ofnon-compositional phrase pairs, and capturing localreordering, but they lack variables (e.g.
embeddingbetween ne .
.
.
pas in French), have sparsity prob-lems, and lack a strategy for global reordering.
Morerecently, (Chiang, 2005) extended phrase-pairs (orblocks) to hierarchical phrase-pairs where a grammarwith a single non-terminal allows the embedding ofphrases-pairs, to allow for arbitrary embedding andcapture global reordering though this approach stillhas the high overlap problem.
However, in (Quirkand Menezes, 2006), the authors investigate mini-mum translation units (MTU) which is a refinementover a similar approach by (Banchs et al, 2005)to eliminate the overlap issue.
The MTU approachpicks all the minimal blocks subject to the conditionthat no word alignment link crosses distinct blocks.They do not have the notion of a block with a vari-able (a special case of the hierarchical phrase-pairs)that we employ in this work.
They also have a weak-ness in the parameter estimation method; they relyon an n-gram language model on blocks which inher-ently requires a large bilingual training data set.2.2 Estimating Model ParametersMost recent SMT systems use blocks (i.e.
phrase-pairs) with a few real valued ?informative?
featureswhich can be viewed as an indicator of how proba-ble the current translation is.
As discussed in Sec-tion 1.2, these features are typically MLE models(e.g.
block translation, Model 1, language model,etc.)
whose scores are log-linearly combined usinga weight vector, ?f where f is a particular feature.The ?f are trained using a held-out corpus usingmaximum BLEU training (Och, 2003).
This methodis only practical for a small number of features; typ-ically, the number of features is on the order of 10 to20.Recently, there have been several discriminativeapproaches at training large parameter sets includ-ing (Tillmann and Zhang, 2006) and (Liang et al,2006).
In (Tillmann and Zhang, 2006) the modelis optimized to produce a block orientation and thetarget sentence is used only for computing a sentencelevel BLEU.
(Liang et al, 2006) demonstrates a dis-criminatively trained system for machine translationthat has the following characteristics: 1) requires avarying update strategy (local vs. bold) dependingon whether the reference sentence is ?reachable?
ornot, 2) uses sentence level BLEU as a criterion for se-lecting which output to update towards, and 3) onlytrains on limited length (5-15 words) sentences.So both methods fundamentally rely on a priordecoder to produce an ?N-best?
list that is used tofind a target (using max BLEU) for the training al-gorithm.
The methods to produce an ?N-best?
listtend to be not very effective since most alternativetranslations are minor differences from the highestscoring translation and do not typically include thereference translation (particularly when the systemmakes a large error).In this paper, the algorithm trains on all sentencesin the test-specific corpus and crucially, the algo-rithm directly uses the target translation to updatethe model parameters.
This latter point is a criticaldifference that contrasts to the major weakness of thework of (Liang et al, 2006) which uses a top-N list oftranslations to select the maximum BLEU sentenceas a target for training (so called local update).3 A Categorization of Block StylesIn (Brown et al, 1993), multi-word ?cepts?
(whichare realized in our block concept) are discussed andthe authors state that when a target sequence issufficiently different from a word by word transla-tion, only then should the target sequence shouldbe promoted to a cept.
This is in direct oppositionto phrase-based decoders which utilize all possiblephrase-pairs and limit the number of phrases onlydue to practical considerations.
Following the per-spective of (Brown et al, 1993), a minimal set ofphrase blocks with lengths (m, n) where either m orn must be greater than zero results in the followingtypes of blocks:1. n = 0, source word producing nothing in thetarget language (deletion block),2. m = 0, spontaneous target word (insertionblock),3. m = 1 and n ?
1, a source word producing ntarget words including the possibility of a vari-able (denoted by X) which is to be filled withother blocks from the sentence (the latter casecalled a discontiguous block)4. m ?
1 and n = 1, a sequence of source wordsproducing a single target words including thepossibility of a variable on the source side (as inthe French ne...pas translating into not, calledmulti-word singletons) in the source sequence595.
m > 1 and n > 1, a non-compositional phrasetranslationIn this paper, we restrict the blocks to Types 1 and 3.From the example in Figure 1, the following blocksare extracted:?
lljnp ?
of the X Committee?
Almrkzyp ?
Central?
llHzb ?
of the X Party?
Al$ywEy ?
Communist?
AlSyny ?
Chinese.These blocks can now be considered more ?general?and can be used to generate more phrases comparedto the blocks shown in Table 1.
These blocks whenutilized independently of the remainder of the modelperform very poorly as all the advantages of blocksare absent.
These advantages are obtained using thefeatures to be described below.
Also, we store with ablock additional information such as: (a) alignmentinformation, and (b) source and target analysis.
Thetarget analysis includes part of speech and for eachtarget string a list of part of speech sequences arestored along with their corpus frequencies.The first alignment shown in Figure 1 is an exam-ple of a Type 5 non-compositional block; althoughthis is not currently addressed by the decoder, weplan to handle such blocks in the future.4 AlgorithmA classification problem can be considered as a map-ping from a set of histories, S, into a set of futures,T .
Traditional classification problems deal with asmall finite set of futures usually no more than a fewthousands of classes.Machine translation can be cast into the sameframework with a much larger future space.
In con-trast to the current global models, we decompose theprocess into a sequence of steps.
The process beginsat the left edge of a sentence and for practical rea-sons considers a window of source words that couldbe translated.
The first action is to jump a distance,j to a source position and to produce a target string,t corresponding to the source word at that position.The process then marks the source position as hav-ing been visited and iterates till all source words havebeen visited.
The only wrinkle in this relatively sim-ple process is the presence of a variable in the tar-get sequence.
In the case of a variable, the sourceposition is marked as having been partially visited.When a partially visited source position is visitedagain, the target string to the right of the variable isoutput and the process is iterated.
The distortion orjump from the previously translated source word, jin training can vary widely due to automatic sentencealignment that is used to create the parallel corpus.To limit the sparseness created by these longer jumpswe cap the jump to a window of source words (-5 to 5words) around the last translated source word; jumpsoutside the window are treated as being to the edgeof the window.We combine the above translation model with an-gram language model as inp(T, j|S) =?ip(ti, j|si)?
?i?LMp(ti|ti?1, .
.
.
, ti?n)+?TMp(ti, j|si)This mixing allows the use of language model builtfrom a very large monolingual corpus to be used witha translation model which is built from a smallerparallel corpus.
In the rest of this paper, we areconcerned only with the translation model.The minimum requirements for the algorithm are(a) parallel corpus of source and target languagesand (b) word-alignments.
While one can use theEM algorithm to train this hidden alignment model(the jump step), we use Viterbi training, i.e.
we usethe most likely alignment between target and sourcewords in the training corpus to estimate this model.We assume that each sentence pair in the trainingcorpus is word-aligned (e.g.
using a MaxEnt aligner(Ittycheriah and Roukos, 2005) or an HMM aligner(Ge, 2004)).
The algorithm performs the followingsteps in order to train the maximum entropy model:(a) block extraction, (b) feature extraction, and (c)parameter estimation.
Each of the first two stepsrequires a pass over the training data and param-eter estimation requires typically 5-10 passes overthe data.
(Della Pietra et al, 1995) documents theImproved Iterative Scaling (IIS) algorithm for train-ing maximum entropy models.
When the system isrestricted to 1-N type blocks, the future space in-cludes all the source word positions that are withinthe skip window and all their corresponding blocks.The training algorithm at the parameter estimationstep can be concisely stated as:1.
For each sentence pair in the parallel corpus,walk the alignment in source word order.2.
At each source word, the alignment identifies the?true?
block.3.
Form a window of source words and allow allblocks at source words to generate at this gen-eration point.604.
Apply the features relevant to each block andcompute the probability of each block.5.
Form the MaxEnt polynomials(Della Pietra etal., 1995) and solve to find the update for eachfeature.We will next discuss the prior distribution used inthe maximum entropy model, the block extractionmethod and the feature generation method and dis-cuss differences with a standard phrase based de-coder.4.1 Prior DistributionMaximum entropy models are of the form,p(t, j|s) = p0(t, j|s)Z exp?i?i?i(t, j, s)where p0 is a prior distribution, Z is a normalizingterm, and ?i(t, j, s) are the features of the model.The prior distribution can contain any informationwe know about our future and in this work we utilizethe normalized phrase count as our prior.
Strictly,the prior has to be uniform on the set of futures tobe a ?maximum?
entropy algorithm and choices ofother priors result in minimum divergence models.We refer to both as a maximum entropy models.The practical benefit of using normalized phrasecount as the prior distribution is for rare transla-tions of a common source words.
Such a translationblock may not have a feature due to restrictions inthe number of features in the model.
Utilizing thenormalized phrase count prior, the model is still ableto penalize such translations.
In the best case, a fea-ture is present in the model and the model has thefreedom to either boost the translation probabilityor to further reduce the prior.4.2 Block ExtractionSimilar to phrase decoders, a single pass is madethrough the parallel corpus and for each source word,the target sequence derived from the alignmentsis extracted.
The ?Inverse Projection Constraint?,which requires that the target sequence be alignedonly to the source word or phrase in question, is thenchecked to ensure that the phrase pair is consistent.A slight relaxation is made to the traditional targetsequence in that variables are allowed if the length oftheir span is 3 words or less.
The length restriction isimposed to reduce the effect of alignment errors.
Anexample of blocks extracted for the romanized ara-bic words ?lljnp?
and ?Almrkzyp?
are shown Figure 2,where on the left side are shown the unsegmentedArabic words, the segmented Arabic stream and thecorresponding Arabic part-of-speech.
On the right,the target sequences are shown with the most fre-quently occuring part-of-speech and the corpus countof this block.The extracted blocks are pruned in order to min-imize alignment problems as well as optimize thespeed during decoding.
Blocks are pruned if theircorpus count is a factor of 30 times smaller than themost frequent target sequence for the same sourceword.
This results in about 1.6 million blocks froman original size of 3.2 million blocks (note this ismuch smaller than the 50 million blocks or so thatare derived in current phrase-based systems).4.3 FeaturesThe features investigated in this work are binaryquestions about the lexical context both in the sourceand target streams.
These features can be classi-fied into the following categories: (a) block internalfeatures, and (b) block context features.
Featurescan be designed that are specific to a block.
Suchfeatures are modeling the unigram phrase count ofthe block, which is information already present inthe prior distribution as discussed above.
Featureswhich are less specific are tied across many transla-tions of the word.
For example in Figure 2, the pri-mary translation for ?lljnp?
is ?committee?
and occurs920 times across all blocks extracted from the corpus;the final block shown which is ?of the X committee?occurs only 37 times but employs a lexical feature?lljnp committee?
which fires 920 times.4.3.1 Lexical FeaturesLexical features are block internal features whichexamine a source word, a target word and the jumpfrom the previously translated source word.
As dis-cussed above, these are shared across blocks.4.3.2 Lexical Context FeaturesContext features encode the context surroundinga block by examining the previous and next sourceword and the previous two target words.
Unlike atraditional phrase pair, which encodes all the infor-mation lexically, in this approach we define in Ta-ble 2, individual feature types to examine a por-tion of the context.
One or more of these featuresmay apply in each instance where a block is relevant.The previous source word is defined as the previouslytranslated source word, but the next source word isalways the next word in the source string.
At train-ing time, the previously translated source word isfound by finding the previous target word and utiliz-ing the alignment to find the previous source word.If the previous target word is unaligned, no contextfeature is applied.61committee/NN (613)of the commission/IN DT NN (169)the committee/DT NN (136)commission/NN (135)of the committee/IN DT NN (134)the commission/DT NN (106)of the HOLE committee/IN DT -1 NN(37)central/NNP (731)the central/DT JJ (504)of the central/IN DT NNP(64)the cia/DT NNP (58)AlmrkzypAl# mrkzy +pDET ADJ NSUFF_FEM_SGlljnpl# ljn +pPREP NOUN NSUFF_FEM_SGFigure 2: Extracted blocks for ?lljnp?
and ?Almrkzyp?.Feature Name Feature variablesSRC LEFT source left, source word,target wordSRC RIGHT source right, source word,target wordSRC TGT LEFT source left, target left,source word, target wordSRC TGT LEFT 2 source left, target left,target left 2, source word,target wordTable 2: Context Feature Types4.3.3 Arabic Segmentation FeaturesAn Arabic segmenter produces morphemes; inArabic, prefixes and suffixes are used as prepositions,pronouns, gender and case markers.
This produces asegmentation view of the arabic source words (Lee etal., 2003).
The features used in the model are formedfrom the Cartesian product of all segmentation to-kens with the English target sequence produced bythis source word or words.
However, prefixes andsuffixes which are specific in translation are limitedto their English translations.
For example the pre-fix ?Al#?
is only allowed to participate in a featurewith the English word ?the?
and similarly ?the?
is notallowed to participate in a feature with the stem ofthe Arabic word.
These restrictions limit the num-ber of features and also reduce the over fitting by themodel.4.3.4 Part-of-speech FeaturesPart-of-speech taggers were run on each language:the English part of speech tagger is a MaxEnt tag-ger built on the WSJ corpus and on the WSJ testset achieves an accuracy of 96.8%; the Arabic partof speech tagger is a similar tagger built on the Ara-bic tree bank and achieves an accuracy of 95.7% onautomatically segmented data.
The part of speechfeature type examines the source and target as wellas the previous target and the corresponding previ-ous source part of speech.
A separate feature typeexamines the part of speech of the next source wordwhen the target sequence has a variable.4.3.5 Coverage FeaturesThese features examine the coverage status of thesource word to the left and the source word to theright.
During training, the coverage is determinedby examining the alignments; the source word to theleft is uncovered if its target sequence is to the rightof the current target sequence.
Since the model em-ploys binary questions and predominantly the sourceword to the left is already covered and the rightsource word is uncovered, these features fire only ifthe left is open or if the right is closed in order tominimize the number of features in the model.5 Translation DecoderA beam search decoder similar to phrase-based sys-tems (Tillmann and Ney, 2003) is used to translatethe Arabic sentence into English.
These decodershave two parameters that control their search strat-egy: (a) the skip length (how many positions are al-lowed to be untranslated) and (b) the window width,which controls how many words are allowed to beconsidered for translation.
Since the majority of theblocks employed in this work do not encode local re-ordering explicitly, the current DTM2 decoder usesa large skip (4 source words for Arabic) and triesall possible reorderings.
The primary difference be-tween a DTM2 decoder and standard phrase baseddecoders is that the maximum entropy model pro-vides a cost estimate of producing this translationusing the features described in previous sections.
An-other difference is that the DTM2 decoder handlesblocks with variables.
When such a block is pro-posed, the initial target sequence is first output andthe source word position is marked as being partiallyvisited and an index into which segment was gener-ated is kept for completing the visit at a later time.Subsequent extensions of this path can either com-plete this visit or visit other source words.
On asearch path, we make a further assumption that only62one source position can be in a partially visited stateat any point.
This greatly reduces the search taskand suffices to handle the type of blocks encounteredin Arabic to English translation.6 ExperimentsThe UN parallel corpus and the LDC news corporareleased as training data for the NIST MT06 eval-uation are used for all evaluations presented in thispaper.
A variety of test corpora are now availableand we use MT03 as development test data, andtest results are presented on MT05.
Results obtainedon MT06 are from a blind evaluation.
For Arabic-English, the NIST MT06 training data contains 3.7Msentence pairs from the UN from 1993-2002 and 100Ksentences pairs from news sources.
This representsthe universe of training data, but for each test setwe sample this corpus to train efficiently while alsoobserving slight gains in performance.
The traininguniverse is time sorted and the most recent corporaare sampled first.
Then for a given test set, we obtainthe first 20 instances of n-grams from the test thatoccur in the training universe and the resulting sam-pled sentences then form the training sample.
Thecontribution of the sampling technique is to producea smaller training corpus which reduces the compu-tational load; however, the sampling of the universeof sentences can be viewed as test set domain adapta-tion which improves performance and is not strictlydone due to computational limitations2.
The 5-gramlanguage model is trained from the English Gigawordcorpus and the English portion of the parallel corpusused in the translation model training.The baseline decoder is a phrase-based decoderthat employs n-m blocks and uses the same test setspecific training corpus described above.6.1 Feature Type ExperimentsThere are 15 individual feature types utilized in thesystem, but in order to be brief we present the re-sults by feature groups (see Table 3): (a) lexical, (b)lexical context, (c) segmentation, (d) part-of-speech,and (e) coverage features.
The results show im-provements with the addition of each feature set, butthe part-of-speech features and coverage features arenot statistically significant improvements.
The morecomplex features based on Arabic segmentation andEnglish part-of-speech yield a small improvement of0.5 BLEU points over the model with only lexicalcontext.2Recent results indicate that test set adaptation bytest set sampling of the training corpus achieves a casedBleu of 53.26 on MT03 whereas a general system trainedon all data achieves only 51.02Verb Placement 3Missing Word 5Extra Word 5Word Choice 26Word Order 3Other error 1Total 43Table 4: Errors on last 25 sentences of MT-03.7 Error Analysis and DiscussionWe analyzed the errors in the last 25 sentences of theMT-03 development data using the broad categoriesshown in Table 4.
These error types are not indepen-dent of each other; indeed, incorrect verb placementis just a special case of the word order error typebut for this error analysis for each error we take thefirst category available in this list.
Word choice er-rors can be a result of (a) rare words with few, orincorrect, or no translation blocks (4 times) or (b)model weakness3 (22 times).
In order to address themodel weakness type of errors, we plan on investigat-ing feature selection using a language model prior.As an example, consider an arabic word which pro-duces both ?the?
(due to alignment errors) and ?theconduct?.
An n-gram LM has very low cost for theword ?the?
but a rather high cost for content wordssuch as ?conduct?.
Incorporating the LM model as aprior should help the maximum entropy model focusits weighting on the content word to overcome theprior information.8 Conclusion and Future WorkWe have presented a complete direct translationmodel with training of millions of parameters basedon a set of minimalist blocks and demonstrated theability to retain good performance relative to phrasebased decoders.
Tied features minimize the num-ber of parameters and help avoid the sparsity prob-lems associated with phrase based decoders.
Uti-lizing language analysis of both the source and tar-get languages adds 0.8 BLEU points on MT-03, and0.4 BLEU points on MT-05.
The DTM2 decoderachieved a 1.7 BLEU point improvement over thephrase based decoder on MT-06.
In this work, wehave restricted the block types to only single sourceword blocks.
Many city names and dates in Ara-bic can not be handled by such blocks and in futurework we intend to investigate the utilization of morecomplex blocks as necessary.
Also, the DTM2 de-coder utilized the LM component independently of3The word occurred with the correct translation inthe phrase library with a count more than 10 and yet thesystem used an incorrect translation.63Feature Types # of feats MT-03 MT-05 MT-06(MT03)Training SizeNum.
of Sentences 197K 267K 279KPhrase-based Decoder 51.20 49.06 36.92DTM2 DecoderLex Feats a 439,582 49.70 48.37+Lex Context b 2,455,394 50.45 49.61+Seg Feats c 2,563,338 50.97 49.96+POS Feats d 2,608,352 51.27 49.93+Cov Feats e 2,783,813 51.19 50.00 38.61Table 3: Bleu scores on MT03-MT06.the translation model; however, in future work weintend to investigate feature selection using the lan-guage model as a prior which should result in muchsmaller systems.9 AcknowledgementsThis work was partially supported by the Department ofthe Interior, National Business Center under contract No.NBCH2030001 and Defense Advanced Research ProjectsAgency under contract No.
HR0011-06-2-0001.
Theviews and findings contained in this material are thoseof the authors and do not necessarily reflect the positionor policy of the U.S. government and no official endorse-ment should be inferred.
This paper owes much to thecollaboration of the Statistical MT group at IBM.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distortionmodels for statistical machine translation.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and 44th Annual Meeting of theACL, pages 529?536, Sydney, Australia.Rafael Banchs, Josep M. Crego, Adria` de Gispert, Pa-trik Lambert, and Jose?
B. Marino.
2005.
Statisticalmachine translation of euparl data by using bilingualn-grams.
In Proc.
of the ACL Workshop on Buildingand Using Parallel Texts, pages 133?136, Ann Arbor,Michigan, USA.Peter F. Brown, Vincent J. Della Pietra, StephenA.
Della Pietra, and Robert L. Mercer.
1993.The Mathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguistics,19(2):263?311.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the ACL, pages 263?270,Ann Arbor, Michigan, June.Stephen Della Pietra, Vincent Della Pietra, and JohnLafferty.
1995.
Inducing features of random fields.Technical Report, Department of Computer Science,Carnegie-Mellon University, CMU-CS-95-144.George Foster.
2000.
A maximum entropy/minimumdivergence translation model.
In 38th Annual Meetingof the ACL, pages 45?52, Hong Kong.Niyu Ge.
2004.
Improvement in Word Alignments.
Pre-sentation given at DARPA/TIDES MT workshop.Abraham Ittycheriah and Salim Roukos.
2005.
A maxi-mum entropy word aligner for arabic-english machinetranslation.
In HLT ?05: Proceedings of the HLT andEMNLP, pages 89?96.Young-Suk Lee, Kishore Papineni, and Salim Roukos.2003.
Language model based arabic word segmenta-tion.
In 41st Annual Meeting of the ACL, pages 399?406, Sapporo, Japan.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein, andBen Taskar.
2006.
An end-to-end discriminative ap-proach to machine translation.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the ACL, pages761?768, Sydney, Australia.Franz Josef Och and Hermann Ney.
2000.
Statisticalmachine translation.
In EAMT Workshop, pages 39?46, Ljubljana, Slovenia.Franz-Josef Och and Hermann Ney.
2002.
DiscriminativeTraining and Maximum Entropy Models for StatisticalMachine Translations.
In 40th Annual Meeting of theACL, pages 295?302, Philadelphia, PA, July.Franz Josef Och.
2003.
Minimum error rate training inStatistical Machine Translation.
In 41st Annual Meet-ing of the ACL, pages 160?167, Sapporo, Japan.Kishore Papineni, Salim Roukos, and R. T. Ward.1997.
Feature-based language understanding.
In EU-ROSPEECH, pages 1435?1438, Rhodes,Greece.Kishore Papineni, Salim Roukos, and R. T. Ward.
1998.Maximum likelihood and discriminative training of di-rect translation models.
In International Conf.
onAcoustics, Speech and Signal Processing, pages 189?192, Seattle, WA.Chris Quirk and Arul Menezes.
2006.
Do we needphrases?
challenging the conventional wisdom in sta-tistical machine translation.
In Proceedings of the Hu-man Language Technology Conference of the NAACL,pages 9?16, New York, NY, USA.Christoph Tillmann and Hermann Ney.
2003.
Word re-ordering and a dynamic programming beam search al-gorithm for Statistical Machine Translation.
29(1):97?133.Christoph Tillmann and Tong Zhang.
2006.
A discrimi-native global training algorithm for statistical mt.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meetingof the ACL, pages 721?728, Sydney, Australia.64
