Proceedings of the GEMS 2011 Workshop on Geometrical Models of Natural Language Semantics, EMNLP 2011, pages 62?66,Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational LinguisticsExperimenting with Transitive Verbs in a DisCoCatEdward GrefenstetteUniversity of OxfordDepartment of Computer ScienceWolfson Building, Parks RoadOxford OX1 3QD, UKedward.grefenstette@cs.ox.ac.ukMehrnoosh SadrzadehUniversity of OxfordDepartment of Computer ScienceWolfson Building, Parks RoadOxford OX1 3QD, UKmehrs@cs.ox.ac.ukAbstractFormal and distributional semantic modelsoffer complementary benefits in modelingmeaning.
The categorical compositional dis-tributional model of meaning of Coecke et al(2010) (abbreviated to DisCoCat in the title)combines aspects of both to provide a gen-eral framework in which meanings of words,obtained distributionally, are composed usingmethods from the logical setting to form sen-tence meaning.
Concrete consequences ofthis general abstract setting and applications toempirical data are under active study (Grefen-stette et al, 2011; Grefenstette and Sadrzadeh,2011).
In this paper, we extend this study byexamining transitive verbs, represented as ma-trices in a DisCoCat.
We discuss three ways ofconstructing such matrices, and evaluate eachmethod in a disambiguation task developed byGrefenstette and Sadrzadeh (2011).1 BackgroundThe categorical distributional compositional modelof meaning of Coecke et al (2010) combines themodularity of formal semantic models with the em-pirical nature of vector space models of lexical se-mantics.
The meaning of a sentence is defined tobe the application of its grammatical structure?represented in a type-logical model?to the kro-necker product of the meanings of its words, ascomputed in a distributional model.
The concreteand experimental consequences of this setting, andother models that aim to bring together the log-ical and distributional approaches, are active top-ics in current natural language semantics research,e.g.
see (Grefenstette et al, 2011; Grefenstette andSadrzadeh, 2011; Clark et al, 2010; Baroni andZamparelli, 2010; Guevara, 2010; Mitchell and La-pata, 2008).In this paper, we focus on our recent concrete Dis-CoCat model (Grefenstette and Sadrzadeh, 2011)and in particular on nouns composed with transitiveverbs.
Whereby the meaning of a transitive sentence?sub tverb obj?
is obtained by taking the component-wise multiplication of the matrix of the verb withthe kronecker product of the vectors of subject andobject:????????
?sub tverb obj = tverb (??sub??
?obj) (1)In most logical models, transitive verbs are modeledas relations; in the categorical model the relationalnature of such verbs gets manifested in their ma-trix representation: if subject and object are each r-dimensional row vectors in some space N , the verbwill be a r ?
r matrix in the space N ?
N .
Thereare different ways of learning the weights of this ma-trix.
In (Grefenstette and Sadrzadeh, 2011), we de-veloped and implemented one such method on thedata from the British National Corpus.
The matrix ofeach verb was constructed by taking the sum of thekronecker products of all of the subject/object pairslinked to that verb in the corpus.
We refer to thismethod as the indirect method.
This is because theweight cij is obtained from the weights of the sub-ject and object vectors (computed via co-occurrencewith bases ?
?n i and?
?n j respectively), rather than di-rectly from the context of the verb itself, as wouldbe the case in lexical distributional models.
Thisconstruction method was evaluated against an exten-62sion of Mitchell and Lapata (2008)?s disambiguationtask from intransitive to transitive sentences.
Weshowed and discussed how and why our method,which is moreover scalable and respects the gram-matical structure of the sentence, resulted in betterresults than other known models of semantic vectorcomposition.As a motivation for the current paper, note thatthere are at least two different factors at work inEquation (1): one is the matrix of the verb, denotedby tverb, and the other is the kronecker product ofsubject and object vectors?
?sub ???obj.
Our model?smathematical formulation of composition prohibitsus from changing the latter kronecker product, butthe ?content?
of the verb matrices can be builtthrough different procedures.In recent work we used a standard lexical distri-butional model for nouns and engineered our verbsto have a more sophisticated structure because ofthe higher dimensional space they occupy.
In par-ticular, we argued that the resulting matrix of theverb should represent ?the extent according to whichthe verb has related the properties of subjects to theproperties of its objects?, developed a general proce-dure to build such matrices, then studied their em-pirical consequences.
One question remained open:what would be the consequence of starting from thestandard lexical vector of the verb, then encodingit into the higher dimensional space using different(possibly ad-hoc but nonetheless interesting) mathe-matically inspired methods.In a nutshell, the lexical vector of the verb is de-noted by??
?tverb and similar to vectors of subject andobject, it is an r-dimensional row vector.
Since thekronecker product of subject and object (??sub??
?obj)is r ?
r, in order to make??
?tverb applicable in Equa-tion 1, i.e.
to be able to substitute it for tverb, weneed to encode it into a r ?
r matrix in the N ?
Nspace.
In what follows, we investigate the empiricalconsequences of three different encodings methods.2 From Vectors to MatricesIn this section, we discuss three different ways of en-coding r dimensional lexical verb vectors into r?
rverb matrices, and present empirical results for each.We use the additional structure that the kroneckerproduct provides to represent the relational natureof transitive verbs.
The results are an indication thatthe extra information contained in this larger spacecontributes to higher quality composition.One way to encode an r-dimensional vector as ar ?
r matrix is to embed it as the diagonal of thatmatrix.
It remains open to decide what the non-diagonal values should be.
We experimented with0s and 1s as padding values.
If the vector of the verbis [c1, c2, ?
?
?
, cr] then for the 0 case (referred to as0-diag) we obtain the following matrix:tverb =????
?c1 0 ?
?
?
00 c2 ?
?
?
0....... .
....0 0 .
.
.
cr????
?For the 1 case (referred to as 1-diag) we obtain thefollowing matrix:tverb =????
?c1 1 ?
?
?
11 c2 ?
?
?
1....... .
....1 1 .
.
.
cr????
?We also considered a third case where the vector isencoded into a matrix by taking the kronecker prod-uct of the verb vector with itself:tverb =???tverb???
?tverbSo for??
?tverb = [c1, c2, ?
?
?
, cr] we obtain the follow-ing matrix:tverb =????
?c1c1 c1c2 ?
?
?
c1crc2c1 c2c2 ?
?
?
c2cr....... .
....crc1 crc2 ?
?
?
crcr????
?3 Degrees of synonymity for sentencesThe degree of synonymity between meanings oftwo sentences is computed by measuring their ge-ometric distance.
In this work, we used the co-sine measure.
For two sentences ?sub1 tverb1 obj1?and ?sub2 tverb2 obj2?, this is obtained by takingthe Frobenius inner product of??????????
?sub1 tverb1 obj1 and??????????
?sub2 tverb2 obj2.
The use of Frobenius productrather than the dot product is because the calcula-tion in Equation (1) produces matrices rather thanrow vectors.
We normalized the outputs by the mul-tiplication of the lengths of their corresponding ma-trices.634 ExperimentIn this section, we describe the experiment used toevaluate and compare these three methods.
The ex-periment is on the dataset developed in (Grefenstetteand Sadrzadeh, 2011).Parameters We used the parameters described byMitchell and Lapata (2008) for the noun and verbvectors.
All vectors were built from a lemmatisedversion of the BNC.
The noun basis was the 2000most common context words, basis weights werethe probability of context words given the targetword divided by the overall probability of the con-text word.
These features were chosen to enableeasy comparison of our experimental results withthose of Mitchell and Lapata?s original experiment,in spite of the fact that there may be more sophisti-cated lexical distributional models available.Task This is an extension of Mitchell and Lap-ata (2008)?s disambiguation task from intransitiveto transitive sentences.
The general idea behindthe transitive case (similar to the intransitive one) isas follows: meanings of ambiguous transitive verbsvary based on their subject-object context.
For in-stance the verb ?meet?
means ?satisfied?
in the con-text ?the system met the criterion?
and it means?visit?, in the context ?the child met the house?.Hence if we build meaning vectors for these sen-tences compositionally, the degrees of synonymityof the sentences can be used to disambiguate themeanings of the verbs in them.Suppose a verb has two meanings a and b andthat it has occurred in two sentences.
Then if inboth of these sentences it has its meaning a, the twosentences will have a high degree of synonymity,whereas if in one sentence the verb has meaning aand in the other meaning b, the sentences will havea lower degree of synonymity.
For instance ?the sys-tem met the criterion?
and ?the system satisfied thecriterion?
have a high degree of semantic similarity,and similarly for ?the child met the house?
and ?thechild visited the house?.
This degree decreases forthe pair ?the child met the house?
and ?the child sat-isfied the house?.Dataset The dataset is built using the same guide-lines as Mitchell and Lapata (2008), using transi-tive verbs obtained from CELEX1 paired with sub-jects and objects.
We first picked 10 transitive verbsfrom the most frequent verbs of the BNC.
For eachverb, two different non-overlapping meanings wereretrieved, by using the JCN (Jiang Conrath) infor-mation content synonymity measure of WordNet toselect maximally different synsets.
For instance for?meet?
we obtained ?visit?
and ?satisfy?.
For eachoriginal verb, ten sentences containing that verb withthe same role were retrieved from the BNC.
Exam-ples of such sentences are ?the system met the crite-rion?
and ?the child met the house?.
For each suchsentence, we generated two other related sentencesby substituting their verbs by each of their two syn-onyms.
For instance, we obtained ?the system sat-isfied the criterion?
and ?the system visited the cri-terion?
for the first meaning and ?the child satisfiedthe house?
and ?the child visited the house?
for thesecond meaning .
This procedure provided us with200 pairs of sentences.The dataset was split into four non-identical sec-tions of 100 entries such that each sentence appearsin exactly two sections.
Each section was given toa group of evaluators who were asked to assign asimilarity score to simple transitive sentence pairsformed from the verb, subject, and object providedin each entry (e.g.
?the system met the criterion?from ?system meet criterion?).
The scoring scale forhuman judgement was [1, 7], where 1 was most dis-similar and 7 most identical.Separately from the group annotation, each pair inthe dataset was given the additional arbitrary classi-fication of HIGH or LOW similarity by the authors.Evaluation Method To evaluate our methods, wefirst applied our formulae to compute the similar-ity of each phrase pair on a scale of [0, 1] and thencompared it with human judgement of the samepair.
The comparison was performed by measuringSpearman?s ?, a rank correlation coefficient rangingfrom -1 to 1.
This provided us with the degree ofcorrelation between the similarities as computed byour model and as judged by human evaluators.Following Mitchell and Lapata (2008), we alsocomputed the mean of HIGH and LOW scores.However, these scores were solely based on the au-thors?
personal judgements and as such (and on their1http://celex.mpi.nl/64own) do not provide a very reliable measure.
There-fore, like Mitchell and Lapata (2008), the modelswere ultimately judged by Spearman?s ?.The results are presented in table 4.
The additiveand multiplicative rows have, as composition oper-ation, vector addition and component-wise multipli-cation.
The Baseline is from a non-compositionalapproach; it is obtained by comparing the verb vec-tors of each pair directly and ignoring their subjectsand objects.
The UpperBound is set to be inter-annotator agreement.Model High Low ?Baseline 0.47 0.44 0.16Add 0.90 0.90 0.05Multiply 0.67 0.59 0.17CategoricalIndirect matrix 0.73 0.72 0.210-diag matrix 0.67 0.59 0.171-diag matrix 0.86 0.85 0.08v ?
v matrix 0.34 0.26 0.28UpperBound 4.80 2.49 0.62Table 1: Results of compositional disambiguation.The indirect matrix performed better than thevectors encoded in diagonal matrices padded with0 and 1.
However, surprisingly, the kronecker prod-uct of this vector with itself provided better resultsthan all the above.
The results were statistically sig-nificant with p < 0.05.5 Analysis of the ResultsSuppose the vector of subject is [s1, s2, ?
?
?
, sr] andthe vector of object is?
?obj = [o1, o2, ?
?
?
, or], thenthe matrix of??sub??
?obj is:????
?s1o1 s1o2 ?
?
?
s1ors2o1 s2o2 ?
?
?
s2or...sro1 sro2 ?
?
?
sror????
?After computing Equation (1) for each generationmethod of tverb, we obtain the following three ma-trices for the meaning of a transitive sentence:0-diag :????
?c1s1o1 0 ?
?
?
00 c2s2o2 ?
?
?
0....... .
....0 0 ?
?
?
crsror????
?This method discards all of the non-diagonal infor-mation about the subject and object, for examplethere is no occurrence of s1o2, s2o1, etc.1-diag :????
?c1s1o1 s1o2 ?
?
?
s1ors2o1 c2s2o2 ?
?
?
s2or....... .
....sro1 sro2 ?
?
?
crsror????
?This method conserves the information about thesubject and object, but only applies the informationof the verb to the diagonals: s1 and o2, s2 and o1,etc.
are never related to each other via the verb.v ?
v :????
?c1c1s1o1 c1c2s1o2 ?
?
?
c1crs1orc2c1s2o1 c2c2s2o2 ?
?
?
c2crs2or....... .
....crc1sro1 crc2sro2 ?
?
?
crcrsror????
?This method not only conserves the informationof the subject and object, but also applies to themall of the information encoded in the verb.
Thesedata propagate to Frobenius products when comput-ing the semantic similarity of sentences and justifythe empirical results.The unexpectedly good performance of the v ?
vmatrix relative to the more complex indirect methodis surprising, and certainly demands further inves-tigation.
What is sure is that they each draw upondifferent aspects of semantic composition to providebetter results.
There is certainly room for improve-ment and empirical optimisation in both of theserelation-matrix construction methods.Furthermore, the success of both of these meth-ods relative to the others examined in Table 1 showsthat it is the extra information provided in the matrix(rather than just the diagonal, representing the lexi-cal vector) that encodes the relational nature of tran-sitive verbs, thereby validating in part the require-ment suggested in Coecke et al (2010) and Grefen-stette and Sadrzadeh (2011) that relational word vec-tors live in a space the dimensionality of which be afunction of the arity of the relation.65ReferencesH.
Alshawi (ed).
1992.
The Core Language Engine.MIT Press.M.
Baroni and R. Zamparelli.
2010.
Nouns are vectors,adjectives are matrices.
Proceedings of Conferenceon Empirical Methods in Natural Language Processing(EMNLP).D.
Clarke, R. Lutz and D. Weir.
2010.
SemanticComposition with Quotient Algebras.
Proceedingsof Geometric Models of Natural Language Semantics(GEMS-2010).S.
Clark and S. Pulman.
2007.
Combining Symbolicand Distributional Models of Meaning.
Proceedingsof AAAI Spring Symposium on Quantum Interaction.AAAI Press.B.
Coecke, M. Sadrzadeh and S. Clark.
2010.
Mathemat-ical Foundations for Distributed Compositional Modelof Meaning.
Lambek Festschrift.
Linguistic Analysis36, 345?384.
J. van Benthem, M. Moortgat and W.Buszkowski (eds.).J.
Curran.
2004.
From Distributional to Semantic Simi-larity.
PhD Thesis, University of Edinburgh.K.
Erk and S. Pado?.
2004.
A Structured Vector SpaceModel for Word Meaning in Context.
Proceedingsof Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), 897?906.G.
Frege 1892.
U?ber Sinn und Bedeutung.
Zeitschriftfu?r Philosophie und philosophische Kritik 100.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930-1955.
Studies in Linguistic Analysis.E.
Grefenstette, M. Sadrzadeh, S. Clark, B. Coecke,S.
Pulman.
2011.
Concrete Compositional SentenceSpaces for a Compositional Distributional Model ofMeaning.
International Conference on ComputationalSemantics (IWCS?11).
Oxford.E.
Grefenstette, M. Sadrzadeh.
2011.
Experimental Sup-port for a Categorical Compositional DistributionalModel of Meaning.
Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing.G.
Grefenstette.
1994.
Explorations in Automatic The-saurus Discovery.
Kluwer.E.
Guevara.
2010.
A Regression Model of Adjective-Noun Compositionality in Distributional Semantics.Proceedings of the ACL GEMS Workshop.Z.
S. Harris.
1966.
A Cycling Cancellation-Automatonfor Sentence Well-Formedness.
International Compu-tation Centre Bulletin 5, 69?94.R.
Hudson.
1984.
Word Grammar.
Blackwell.J.
Lambek.
2008.
From Word to Sentence.
Polimetrica,Milan.T.
Landauer, and S. Dumais.
2008.
A solution to Platosproblem: The latent semantic analysis theory of ac-quisition, induction, and representation of knowledge.Psychological review.C.
D. Manning, P. Raghavan, and H. Schu?tze.
2008.
In-troduction to information retrieval.
Cambridge Uni-versity Press.J.
Mitchell and M. Lapata.
2008.
Vector-based mod-els of semantic composition.
Proceedings of the 46thAnnual Meeting of the Association for ComputationalLinguistics, 236?244.R.
Montague.
1974.
English as a formal language.
For-mal Philosophy, 189?223.J.
Nivre 2003.
An efficient algorithm for projectivedependency parsing.
Proceedings of the 8th Interna-tional Workshop on Parsing Technologies (IWPT).J.
Saffron, E. Newport, R. Asling.
1999.
Word Segmenta-tion: The role of distributional cues.
Journal of Mem-ory and Language 35, 606?621.H.
Schuetze.
1998.
Automatic Word Sense Discrimina-tion.
Computational Linguistics 24, 97?123.P.
Smolensky.
1990.
Tensor product variable bindingand the representation of symbolic structures in con-nectionist systems.
Computational Linguistics 46, 1?2, 159?216.M.
Steedman.
2000.
The Syntactic Process.
MIT Press.D.
Widdows.
2005.
Geometry and Meaning.
Universityof Chicago Press.L.
Wittgenstein.
1953.
Philosophical Investigations.Blackwell.66
