Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 76?81,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSemEval-2010 Task 2: Cross-Lingual Lexical SubstitutionRavi SinhaUniversity of North Texasravisinha@unt.eduDiana McCarthyUniversity of Sussexdianam@sussex.ac.ukRada MihalceaUniversity of North Texasrada@cs.unt.eduAbstractIn this paper we describe the SemEval-2010 Cross-Lingual Lexical Substitution task,which is based on the English Lexical Substi-tution task run at SemEval-2007.
In the En-glish version of the task, annotators and sys-tems had to find an alternative substitute wordor phrase for a target word in context.
In thispaper we propose a task where the target wordand contexts will be in English, but the substi-tutes will be in Spanish.
In this paper we pro-vide background and motivation for the taskand describe how the dataset will differ froma machine translation task and previous wordsense disambiguation tasks based on paralleldata.
We describe the annotation process andhow we anticipate scoring the system output.We finish with some ideas for participatingsystems.1 IntroductionThe Cross-Lingual Lexical Substitution task isbased on the English Lexical Substitution task run atSemEval-2007.
In the 2007 English Lexical Substi-tution Task, annotators and systems had to find an al-ternative substitute word or phrase for a target wordin context.
In this cross-lingual task the target wordand contexts will be in English, but the substituteswill be in Spanish.An automatic system for cross-lingual lexical sub-stitution would be useful for a number of applica-tions.
For instance, such a system could be usedto assist human translators in their work, by provid-ing a number of correct translations that the humantranslator can choose from.
Similarly, the systemcould be used to assist language learners, by pro-viding them with the interpretation of the unknownwords in a text written in the language they are learn-ing.
Last but not least, the output of a cross-linguallexical substitution system could be used as input toexisting systems for cross-language information re-trieval or automatic machine translation.2 Background: The English LexicalSubstitution TaskThe English Lexical substitution task (hereafter re-ferred to as LEXSUB) was run at SemEval-2007 fol-lowing earlier ideas on a method of testing WSDsystems without predetermining the inventory (Mc-Carthy, 2002).
The issue of which inventory is ap-propriate for the task has been a long standing is-sue for debate, and while there is hope that coarse-grained inventories will allow for increased systemperformance (Ide and Wilks, 2006) we do not yetknow if these will make the distinctions that willmost benefit practical systems (Stokoe, 2005) or re-flect cognitive processes (Kilgarriff, 2006).
LEXSUBwas proposed as a task which, while requiring con-textual disambiguation, did not presuppose a spe-cific sense inventory.
In fact, it is quite possible touse alternative representations of meaning (Schu?tze,1998; Pantel and Lin, 2002).The motivation for a substitution task was that itwould reflect capabilities that might be useful fornatural language processing tasks such as paraphras-ing and textual entailment, while only focusing onone aspect of the problem and therefore not requir-ing a complete system that might mask system capa-bilities at a lexical level and at the same time make76participation in the task difficult for small researchteams.The task required systems to produce a substituteword for a word in context.
For example a substituteof tournament might be given for the second oc-currence of match (shown in bold) in the followingsentence:The ideal preparation would be a light mealabout 2-2 1/2 hours pre-match, followed by awarm-up hit and perhaps a top-up with extra fluidbefore the match.In LEXSUB, the data was collected for 201 wordsfrom open class parts-of-speech (PoS) (i.e.
nouns,verbs, adjectives and adverbs).
Words were selectedthat have more than one meaning with at least onenear synonym.
Ten sentences for each word wereextracted from the English Internet Corpus (Sharoff,2006).
There were five annotators who annotatedeach target word as it occurred in the context of asentence.
The annotators were each allowed to pro-vide up to three substitutes, though they could alsoprovide a NIL response if they could not come upwith a substitute.
They had to indicate if the targetword was an integral part of a multiword.A development and test dataset were provided,but no training data.
Any system that relied on train-ing data, such as sense annotated corpora, had to useresources available from other sources.
The task hadeight participating teams.
Teams were allowed tosubmit up to two systems and there were a total often different systems.
The scoring was conductedusing recall and precision measures using:?
the frequency distribution of responses fromthe annotators and?
the mode of the annotators (the most frequentresponse).The systems were scored using their best guess aswell as an out-of-ten score which allowed up to 10attempts.
1 The results are reported in McCarthy andNavigli (2007) and in more detail in McCarthy andNavigli (in press).1The details are available athttp://nlp.cs.swarthmore.edu/semeval/tasks/task10/task10documentation.pdf.3 Motivation and Related WorkWhile there has been a lot of discussion on the rel-evant sense distinctions for monolingual WSD sys-tems, for machine translation applications there isa consensus that the relevant sense distinctions arethose that reflect different translations.
One earlyand notable work was the SENSEVAL-2 JapaneseTranslation task (Kurohashi, 2001) that obtained al-ternative translation records of typical usages of atest word, also referred to as a translation mem-ory.
Systems could either select the most appropri-ate translation memory record for each instance andwere scored against a gold-standard set of annota-tions, or they could provide a translation that wasscored by translation experts after the results weresubmitted.
In contrast to this work, we propose toprovide actual translations for target instances in ad-vance, rather than predetermine translations usinglexicographers or rely on post-hoc evaluation, whichdoes not permit evaluation of new systems after thecompetition.Previous standalone WSD tasks based on paralleldata have obtained distinct translations for senses aslisted in a dictionary (Ng and Chan, 2007).
In thisway fine-grained senses with the same translationscan be lumped together, however this does not fullyallow for the fact that some senses for the samewords may have some translations in common butalso others that are not.
An example from Resnikand Yarowsky (2000) (table 4 in that paper) is thefirst two senses from WordNet for the noun interest:WordNet sense Spanish Translationmonetary e.g.
on loan intere?s, re?ditostake/share intere?s,participacio?nFor WSD tasks, a decision can be made to lumpsenses with such overlap, or split them using the dis-tinctive translation and then use the distinctive trans-lations as a sense inventory.
This sense inventory isthen used to collect training from parallel data (Ngand Chan, 2007).
We propose that it would be in-teresting to collect a dataset where the overlap intranslations for an instance can remain and that thiswill depend on the token instance rather than map-ping to a pre-defined sense inventory.
Resnik andYarowsky (2000) also conducted their experimentsusing words in context, rather than a predefined77sense-inventory as in (Ng and Chan, 2007; Chan andNg, 2005), however in these experiments the anno-tators were asked for a single preferred translation.We intend to allow annotators to supply as manytranslations as they feel are equally valid.
This willallow us to examine more subtle relationships be-tween usages and to allow partial credit to systemswhich get a close approximation to the annotators?translations.
Unlike a full blown machine transla-tion task (Carpuat and Wu, 2007), annotators andsystems will not be required to translate the wholecontext but just the target word.4 The Cross-Lingual Lexical SubstitutionTaskHere we discuss our proposal for a Cross-LingualLexical Substitution task.
The task will follow LEX-SUB except that the annotations will be translationsrather than paraphrases.Given a target word in context, the task is to pro-vide several correct translations for that word in agiven language.
We will use English as the sourcelanguage and Spanish as the target language.
Mul-tiwords are ?part and parcel?
of natural language.For this reason, rather than try and filter multiwords,which is very hard to do without assuming a fixedinventory, 2 we will ask annotators to indicate wherethe target word is part of a multiword and what thatmultiword is.
This way, we know what the substitutetranslation is replacing.We will provide both development and test sets,but no training data.
As for LEXSUB, any sys-tems requiring data will need to obtain it from othersources.
We will include nouns, verbs, adjectivesand adverbs in both development and test data.
Un-like LEXSUB, the annotators will be told the PoS ofthe current target word.4.1 AnnotationWe are going to use four annotators for our task, allnative Spanish speakers from Mexico, with a highlevel of proficiency in English.
The annotation in-terface is shown in figure 1.
We will calculate inter-tagger agreement as pairwise agreement between2The multiword inventories that do exist are far from com-plete.sets of substitutes from annotators, as was done inLEXSUB.4.2 An ExampleOne significant outcome of this task is that therewill not necessarily be clear divisions between us-ages and senses because we do not use a predefinedsense inventory, or restrict the annotations to dis-tinctive translations.
This will mean that there canbe usages that overlap to different extents with eachother but do not have identical translations.
An ex-ample from our preliminary annotation trials is thetarget adverb severely.
Four sentences are shown infigure 2 with the translations provided by one an-notator marked in italics and {} braces.
Here, allthe token occurrences seem related to each other inthat they share some translations, but not all.
Thereare sentences like 1 and 2 that appear not to haveanything in common.
However 1, 3, and 4 seem tobe partly related (they share severamente), and 2, 3,and 4 are also partly related (they share seriamente).When we look again, sentences 1 and 2, though notdirectly related, both have translations in commonwith sentences 3 and 4.4.3 ScoringWe will adopt the best and out-of-ten precision andrecall scores from LEXSUB.
The systems can supplyas many translations as they feel fit the context.
Thesystem translations will be given credit dependingon the number of annotators that picked each trans-lation.
The credit will be divided by the number ofannotator responses for the item and since for thebest score the credit for the system answers for anitem is also divided by the number of answers thesystem provides, this allows more credit to be givento instances where there is less variation.
For thatreason, a system is better guessing the translationthat is most frequent unless it really wants to hedgeits bets.
Thus if i is an item in the set of instancesI , and Ti is the multiset of gold standard translationsfrom the human annotators for i, and a system pro-vides a set of answers Si for i, then the best scorefor item i will be:best score(i) =?s?Si frequency(s ?
Ti)|Si| ?
|Ti|(1)78Figure 1: The Cross-Lingual Lexical Substitution Interface1.
Perhaps the effect of West Nile Virus is sufficient to extinguish endemic birds already severely stressedby habitat losses.
{fuertemente, severamente, duramente, exageradamente}2.
She looked as severely as she could muster at Draco.
{rigurosamente, seriamente}3.
A day before he was due to return to the United States Patton was severely injured in a road accident.
{seriamente, duramente, severamente}4.
Use market tools to address environmental issues , such as eliminating subsidies for industries thatseverely harm the environment, like coal.
{peligrosamente, seriamente, severamente}5.
This picture was severely damaged in the flood of 1913 and has rarely been seen until now.
{altamente,seriamente, exageradamente}Figure 2: Translations from one annotator for the adverb severelyPrecision is calculated by summing the scores foreach item and dividing by the number of items thatthe system attempted whereas recall divides the sumof scores for each item by |I|.
Thus:best precision =?i best score(i)|i ?
I : defined(Si)| (2)best recall =?i best score(i)|I| (3)The out-of-ten scorer will allow up to ten systemresponses and will not divide the credit attributedto each answer by the number of system responses.This allows the system to be less cautious and forthe fact that there is considerable variation on thetask and there may be cases where systems select aperfectly good translation that the annotators had notthought of.
By allowing up to ten translations in theout-of-ten task the systems can hedge their bets tofind the translations that the annotators supplied.oot score(i) =?s?Si frequency(s ?
Ti)|Ti|(4)oot precision =?i oot score(i)|i ?
I : defined(Si)| (5)79oot recall =?i oot score(i)|I| (6)We will refine the scores before June 2009 whenwe will release the development data for this cross-lingual task.
We note that there was an issue that theoriginal LEXSUB out-of-ten scorer allowed dupli-cates (McCarthy and Navigli, in press).
The effectof duplicates is that systems can get inflated scoresbecause the credit for each item is not divided bythe number of substitutes and because the frequencyof each annotator response is used.
McCarthy andNavigli (in press) describe this oversight, identifythe systems that had included duplicates and explainthe implications.
For our task there is an option forthe out-of-ten score.
Either:1. we remove duplicates before scoring or,2.
we allow duplicates so that systems can boosttheir scores with duplicates on translations withhigher probabilityWe will probably allow duplicates but make thisclear to participants.We may calculate additional best and out-of-tenscores against the mode from the annotators re-sponses as was done in LEXSUB, but we have notdecided on this yet.
We will not run a multiwordtask, but we will use the items identified as multi-words as an optional filter to the scoring i.e.
to seehow systems did without these items.We will provide baselines and upper-bounds.5 SystemsIn the cross-lingual LEXSUB task, the systems willhave to deal with two parts of the problem, namely:1. candidate collection2.
candidate selectionThe first sub-task, candidate collection, refers toconsulting several resources and coming up with alist of potential translation candidates for each tar-get word and part of speech.
We do not provide anyinventories, as with the original LEXSUB task, andthus leave this task of coming up with the most suit-able translation list (in contrast to the synonym listrequired for LEXSUB) to the participants.
As wasobserved with LEXSUB, it is our intuition that thequality of this translation list that the systems comeup with will determine to a large extent how wellthe final performance of the system will be.
Partici-pants are free to use any ideas.
However, a few pos-sibilities might be to use parallel corpora, bilingualdictionaries, a translation engine that only translatesthe target word, or a machine translation system thattranslates the entire sentences.
Several of the bilin-gual dictionaries or even other resources might becombined together to come up with a comprehen-sive translation candidate list, if that seems to im-prove performance.The second phase, candidate selection, concernsfitting the translation candidates in context, and thuscoming up with a ranking as to which translationsare the most suitable for each instance.
The highestranking candidate will be the output for best, and thelist of the top 10 ranking candidates will be the out-put for out-of-ten.
Again, participants are free to usetheir creativity in this, while a range of possible al-gorithms might include using a machine translationsystem, using language models, word sense disam-biguation models, semantic similarity-based tech-niques, graph-based models etc.
Again, combina-tions of these might be used if they are feasible asfar as time and space are concerned.We anticipate a minor practical issue to come upwith all participants, and that is the issue of differentcharacter encodings, especially when using bilin-gual dictionaries from the Web.
This is directly re-lated to the issue of dealing with characters with di-acritics, and in our experience not all available soft-ware packages and programs are able to handle dia-critics and different character encodings in the sameway.
This issue is inherent in all cross-lingual tasks,and we leave it up to the discretion of the partici-pants to effectively deal with it.6 Post Hoc IssuesIn LEXSUB a post hoc evaluation was conducted us-ing fresh annotators to ensure that the substitutesthe systems came up with were not typically bet-ter than those of the original annotators.
This wasdone as a sanity check because there was no fixedinventory for the task and there will be a lot of varia-80tion in the task and sometimes the systems might dobetter than the annotators.
The post hoc evaluationdemonstrated that the post hoc annotators typicallypreferred the substitutes provided by humans.We have not yet determined whether we will runa post hoc evaluation because of the costs of do-ing this and the time constraints.
Another option isto reannotate a portion of our data using a new setof annotators but restricting them to the translationssupplied by the initial set of annotations and othertranslations from available resources.
This would beworthwhile but it could be done at any stage whenfunds permit because we do not intend to supply aset of candidate translations to the annotators sincewe wish to evaluate candidate collection as well ascandidate selection.7 ConclusionsIn this paper we have outlined the cross-lingual lex-ical substitution task to be run under the auspicesof SemEval-2010.
The task will require annotatorsand systems to find translations for a target word incontext.
Unlike machine translation tasks, the wholetext is not translated and annotators are encouragedto supply as many translations as fit the context.
Un-like previous WSD tasks based on parallel data, be-cause we allow multiple translations and because wedo not restrict translations to those that provide clearcut sense distinctions, we will be able to use thedataset collected to investigate more subtle represen-tations of meaning.8 AcknowledgementsThe work of the first and third authors has been partiallysupported by a National Science Foundation CAREERaward #0747340.
The work of the second author has beensupported by a Royal Society UK Dorothy Hodgkin Fel-lowship.ReferencesMarine Carpuat and Dekai Wu.
2007.
Improving statisti-cal machine translation using word sense disambigua-tion.
In Proceedings of the Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL 2007), pages 61?72, Prague, Czech Republic,June.
Association for Computational Linguistics.Yee Seng Chan and Hwee Tou Ng.
2005.
Word sensedisambiguation with distribution estimation.
In Pro-ceedings of the 19th International Joint Conference onArtificial Intelligence (IJCAI 2005), pages 1010?1015,Edinburgh, Scotland.Nancy Ide and Yorick Wilks.
2006.
Making sense aboutsense.
In Eneko Agirre and Phil Edmonds, editors,Word Sense Disambiguation, Algorithms and Applica-tions, pages 47?73.
Springer.Adam Kilgarriff.
2006.
Word senses.
In EnekoAgirre and Phil Edmonds, editors, Word Sense Disam-biguation, Algorithms and Applications, pages 29?46.Springer.Sadao Kurohashi.
2001.
SENSEVAL-2 japanese transla-tion task.
In Proceedings of the SENSEVAL-2 work-shop, pages 37?44.Diana McCarthy and Roberto Navigli.
2007.
SemEval-2007 task 10: English lexical substitution task.
In Pro-ceedings of the 4th International Workshop on Seman-tic Evaluations (SemEval-2007), pages 48?53, Prague,Czech Republic.Diana McCarthy and Roberto Navigli.
in press.
The en-glish lexical substitution task.
Language Resourcesand Evaluation Special Issue on Computational Se-mantic Analysis of Language: SemEval-2007 and Be-yond.Diana McCarthy.
2002.
Lexical substitution as a task forwsd evaluation.
In Proceedings of the ACL Workshopon Word Sense Disambiguation: Recent Successes andFuture Directions, pages 109?115, Philadelphia, USA.Hwee Tou Ng and Yee Seng Chan.
2007.
SemEval-2007 task 11: English lexical sample task viaEnglish-Chinese parallel text.
In Proceedings of the4th International Workshop on Semantic Evaluations(SemEval-2007), pages 54?58, Prague, Czech Repub-lic.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of ACM SIGKDDConference on Knowledge Discovery and Data Min-ing, pages 613?619, Edmonton, Canada.Philip Resnik and David Yarowsky.
2000.
Distinguish-ing systems and distinguishing senses: New evaluationmethods for word sense disambiguation.
Natural Lan-guage Engineering, 5(3):113?133.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Serge Sharoff.
2006.
Open-source corpora: Using thenet to fish for linguistic data.
International Journal ofCorpus Linguistics, 11(4):435?462.Christopher Stokoe.
2005.
Differentiating homonymyand polysemy in information retrieval.
In Proceedingsof the joint conference on Human Language Technol-ogy and Empirical methods in Natural Language Pro-cessing, pages 403?410, Vancouver, B.C., Canada.81
