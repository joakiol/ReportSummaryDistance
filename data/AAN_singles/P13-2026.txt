Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 143?147,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAutomated Pyramid Scoring of Summaries using Distributional SemanticsRebecca J. Passonneau?
and Emily Chen?
and Weiwei Guo?
and Dolores Perin?
?Center for Computational Learning Systems, Columbia University?Department of Computer Science, Columbia University?Teachers College, Columbia University(becky@ccls.|ec2805@|weiwei@cs.
)columbia.edu, perin@tc.eduAbstractThe pyramid method for content evaluation of auto-mated summarizers produces scores that are shownto correlate well with manual scores used in edu-cational assessment of students?
summaries.
Thismotivates the development of a more accurate auto-mated method to compute pyramid scores.
Of threemethods tested here, the one that performs best re-lies on latent semantics.1 IntroductionThe pyramid method is an annotation and scor-ing procedure to assess semantic content of sum-maries in which the content units emerge fromthe annotation.
Each content unit is weightedby its frequency in human reference summaries.It has been shown to produce reliable rank-ings of automated summarization systems, basedon performance across multiple summarizationtasks (Nenkova and Passonneau, 2004; Passon-neau, 2010).
It has also been applied to assessmentof oral narrative skills of children (Passonneau etal., 2007).
Here we show its potential for assess-ment of the reading comprehension of communitycollege students.
We then present a method to au-tomate pyramid scores based on latent semantics.The pyramid method depends on two phases ofmanual annotation, one to identify weighted con-tent units in model summaries written by profi-cient humans, and one to score target summariesagainst the models.
The first annotation phaseyields Summary Content Units (SCUs), sets oftext fragments that express the same basic content.Each SCU is weighted by the number of modelsummaries it occurs in.Figure 1 illustrates a Summary Content Unittaken from pyramid annotation of five model sum-maries of an elementary physics text.
The ele-ments of an SCU are its index; a label, created bythe annotator; contributors (Ctr.
), or text fragmentsfrom the model summaries; and the weight (Wt.
),corresponding to the number of contributors fromdistinct model summaries.
Four of the five modelIndex 105Label Matter is what makes up all objects or substancesCtr.
1 Matter is what makes up all objects or substancesCtr.
2 matter as the stuff that all objects and substancesin the universe are made ofCtr.
3 Matter is identified as being present everywhereand in all substancesCtr.
4 Matter is all the objects and substances around usWt.
4Figure 1: A Summary Content Unit (SCU)summaries contribute to SCU 105 shown here.The four contributors have lexical items in com-mon (matter, objects, substances), and many dif-ferences (makes up, being present).
SCU weights,which range from 1 to the number of model sum-maries M , induce a partition on the set of SCUsin all summaries into subsets Tw, w ?
1, .
.
.
,M .The resulting partition is referred to as a pyramidbecause, starting with the subset for SCUs withweight 1, each next subset has fewer SCUs.To score new target summaries, they are firstannotated to identify which SCUs they express.Application of the pyramid method to assessmentof student reading comprehension is impracticalwithout an automated method to annotate targetsummaries.
Previous work on automated pyramidscores of automated summarizers performs wellat ranking systems on many document sets, butis not precise enough to score human summariesof a single text.
We test three automated pyramidscoring procedures, and find that one based on dis-tributional semantics correlates best with manualpyramid scores, and has higher precision and re-call for content units in students?
summaries thanmethods that depend on string matching.2 Related WorkThe most prominent NLP technique applied toreading comprehension is LSA (Landauer and Du-mais, 1997), an early approach to latent semanticanalysis claimed to correlate with reading compre-hension (Foltz et al, 2000).
More recently, LSA143has been incorporated with a suite of NLP metricsto assess students?
strategies for reading compre-hension using think-aloud protocols (Boonthum-Denecke et al, 2011).
The resulting tool, and sim-ilar assesment tools such as Coh-Metrix, assessaspects of readability of texts, such as coherence,but do not assess students?
comprehension throughtheir writing (Graesser et al, 2004; Graesser et al,2011).
E-rater is an automated essay scorer forstandardized tests such as GMAT that also relieson a suite of NLP techniques (Burstein et al, 1998;Burstein, 2003).
The pyramid method (Nenkovaand Passonneau, 2004), was inspired in part bywork in reading comprehension that scores con-tent using human annotation (Beck et al, 1991).An alternate line of research attempts to repli-cate human reading comprehension.
An auto-mated tool to read and answer questions relies onabductive reasoning over logical forms extractedfrom text (Wellner et al, 2006).
One of the perfor-mance issues is resolving meanings of words: re-moval of WordNet features degraded performance.The most widely used automated content evalu-ation is ROUGE (Lin, 2004; Lin and Hovy, 2003).It relies on model summaries, and depends onngram overlap measures of different types.
Be-cause of its dependence on strings, it performs bet-ter with larger sets of model summaries.
In con-trast to ROUGE, pyramid scoring is robust with asfew as four or five model summaries (Nenkova andPassonneau, 2004).
A fully automated approachto evaluation for ranking systems that requires nomodel summaries incorporates latent semantic dis-tributional similarities across words (Louis andNenkova, 2009).
The authors note, however, itdoes not perform well on individual summaries.3 Criteria for Automated ScoringPyramid scores of students?
summaries correlatewell with a manual main ideas score developedfor an intervention study with community collegefreshmen who attended remedial classes (Perin etal., In press).
Twenty student summaries by stu-dents who attended the same college and took thesame remedial course were selected from a largerset of 322 that summarized an elementary physicstext.
All were native speakers of English, andscored within 5 points of the mean reading scorefor the larger sample.
For the intervention study,student summaries had been assigned a score torepresent how many main ideas from the sourcetext were covered (Perin et al, In press).
Inter-rater reliability of the main ideas score, as givenby the Pearson correlation coefficient, was 0.92.One of the co-authors created a model pyra-mid from summaries written by proficient Mastersof Education students, annotated 20 target sum-maries against this pyramid, and scored the re-sult.
The raw score of a target summary is thesum of its SCU weights.
Pyramid scores havebeen normalized by the number of SCUs in thesummary (analogous to precision), or the averagenumber of SCUs in model summaries (analogousto recall).
We normalized raw scores as the aver-age of the two previous normalizations (analogousto F-measure).
The resulting scores have a highPearson?s correlation of 0.85 with the main ideascore (Perin et al, In press) that was manually as-signed to the students?
summaries.To be pedagogically useful, an automatedmethod to assign pyramid scores to students?
sum-maries should meet the following criteria: 1) reli-ably rank students?
summaries of a source text, 2)assign correct pyramid scores, and 3) identify thecorrect SCUs.
A method could do well on crite-rion 1 but not 2, through scores that have uniformdifferences from corresponding manual pyramidscores.
Also, since each weight partition will havemore than one SCU, it is possible to produce thecorrect numeric score by matching incorrect SCUsthat have the correct weights.
Our method meetsthe first two criteria, and has superior performanceon the third to other methods.4 Approach: Dynamic ProgrammingPrevious work observed that assignment of SCUsto a target summary can be cast as a dynamicprogramming problem (Harnly et al, 2005).
Themethod presented there relied on unigram overlapto score the closeness of the match of each eli-gible substring in a summary against each SCUin the pyramid.
It returned the set of matchesthat yielded the highest score for the summary.It produced good rankings across summarizationtasks, but assigned scores much lower than thoseassigned by humans.
Here we extend the DP ap-proach in two ways.
We test two new semantictext similarities, a string comparison method and adistributional semantic method, and we present ageneral mechanism to set a threshold value for anarbitrary computation of text similarity.Unigram overlap ignores word order, and can-not consider the latent semantic content of astring, only the observed unigram tokens.
To144take order into account, we use Ratcliff/Obershelp(R/O), which measures overlap of common sub-sequences (Ratcliff and Metzener, 1988).
To takethe underlying semantics into account, we use co-sine similarity of 100-dimensional latent vectorsof the candidate substrings and of the textual com-ponents of the SCU (label and contributors).
Be-cause the algorithm optimizes for the total sum ofall SCUs, when there is no threshold similarity tocount as a match, it favors matching shorter sub-strings to SCUs with higher weights.
Therefore,we add a threshold to the algorithm, below whichmatches are not considered.
Because each sim-ilarity metric has different properties and distri-butions, a single absolute value threshhold is notcomparable across metrics.
We present a methodto set comparable thresholds across metrics.4.1 Latent Vector RepresentationsTo represent the semantics of SCUs and candidatesubstrings of target summaries, we applied the la-tent vector model of Guo and Diab (2012).1 Guoand Diab find that it is very hard to learn a 100-dimension latent vector based only on the lim-ited observed words in a short text.
Hence theyinclude unobserved words that provide thousandsmore features for a short text.
This produces moreaccurate results for short texts, which makes themethod suitable for our problem.
Weighted ma-trix factorization (WMF) assigns a small weightfor missing words so that latent semantics dependslargely on observed words.A 100-dimension latent vector representationwas learned for every span of contiguous wordswithin sentence bounds in a target summary, forthe 20 summaries.
The training data was selectedto be domain independent, so that our model couldbe used for summaries across domains.
Thus weprepared a corpus that is balanced across topicsand genres.
It is drawn from from WordNet sensedefinitions, Wiktionary sense definitions, and theBrown corpus.
It yields a co-occurrence matrixM of unique words by sentences of size 46,619?
393,666.
Mij holds the TF-IDF value of wordwi in sentence sj .
Similarly, the contributorsto and the label for an SCU were given a 100-dimensional latent vector representation.
Theserepresentations were then used to compare candi-dates from a summary to SCUs in the pyramid.1http://www.cs.columbia.edu/?weiwei/code.html#wtmf.4.2 Three Comparison MethodsAn SCU consists of at least two text strings: theSCU label and one contributor.
As in Harnly etal.
(2005), we use three similarity comparisonsscusim(X,SCU), where X is the target summarystring.
When the comparison parameter is set tomin (max, or mean), the similarity of X toeach SCU contributor and the label is computedin turn, and the minimum (max, or mean) is re-turned.4.3 Similarity ThresholdsWe define a threshold parameter for a target SCUto match a pyramid SCU based on the distributionsof scores each similarity method gives to the targetSCUs identified by the human annotator.
Annota-tion of the target summaries yielded 204 SCUs.The similarity score being a continuous randomvariable, the empirical sample of 204 scores isvery sparse.
Hence, we use a Gaussian kernel den-sity estimator to provide a non-parametric estima-tion of the probability densities of scores assignedby each of the similarity methods to the manuallyidentified SCUs.
We then select five threshold val-ues corresponding to those for which the inversecumulative density function (icdf) is equal to 0.05,0.10, 0.15, 0.20 and 0.25.
Each threshold rep-resents the probability that a manually identifiedSCU will be missed.5 ExperimentThe three similarity computations, three methodsto compare against SCUs, and five icdf thresh-olds yield 45 variants, as shown in Figure 2.
Eachvariant was evaluated by comparing the unnormal-ized automated variant, e.g., Lvc, max, 0.64 (its0.15 icdf) to the human gold scores, using each ofthe evaluation metrics described in the next sub-section.
To compute confidence intervals for theevaluation metrics for each variant, we use boot-strapping with 1000 samples (Efron and Tibshi-rani, 1986).To assess the 45 variants, we compared theirscores to the manual scores.
We also comparedthe sets of SCUs retrieved.
By our criterion 1), anautomated score that correlates well with manualscores for summaries of a given text could be used(3 Similarities) ?
(3 Comparisons) ?
(5 Thresholds) = 45(Uni, R/O, Lvc) ?
(min, mean, max) ?
(0.05, .
.
.
, 0.25)Figure 2: Notation used for the 45 variants145Variant (with icdf) P (95% conf.
), rank S (95% conf.
), rank K (95% conf.
), rank ?
Diff.
T testLVc, max, 0.64 (0.15) 0.93 (0.94, 0.92), 1 0.94 (0.93, 0.97), 1 0.88 (0.85, 0.91), 1 49.9 15.65 0.0011R/O, mean, 0.23 (0.15) 0.92 (0.91, 0.93), 3 0.93 (0.91,0.95), 2 0.83 (0.80, 0.86), 3 49.8 15.60 0.0012R/O, mean, 0.26 (0.20) 0.92 (0.90, 0.93), 4 0.92 (0.90, 0.94) 4 0.80 (0.78, 0.83), 5 47.7 13.45 0.0046LVc, max, 0.59 (0.10) 0.91 (0.89, 0.92), 8 0.93 (0.91, 0.95) 3 0.83 (0.80, 0.87), 2 52.7 18.50 0.0002LVc, min, 0.40 (0.20) 0.92 (0.90,0.93), 2 0.87 (0.84, 0.91) 11 0.74 (0.69, 0.79), 11 37.5 3.30 0.4572Table 1: Five variants from the top twelve of all correlations, with confidence interval and rank (P=Pearson?s, S=Spearman,K=Kendall?s tau), mean summed SCU weight, difference of mean from mean gold score, T test p-value.to indicate how well students rank against otherstudents.
We report several types of correlationtests.
Pearsons tests the strength of a linear cor-relation between the two sets of scores; it will behigh if the same order is produced, with the samedistance between pairs of scores.
The Spearmanrank correlation is said to be preferable for ordi-nal comparisons, meaning where the unit intervalis less relevant.
Kendall?s tau, an alternative rankcorrelation, is less sensitive to outliers and moreintuitive.
It is the proportion of concordant pairs(pairs in the same order) less the proportion of dis-cordant pairs.
Since correlations can be high whendifferences are uniform, we use Student?s T to testwhether differences score means statistically sig-nificant.
Criterion 2) is met if the correlations arehigh and the means are not significantly different.6 ResultsThe correlation tests indicate that several variantsachieve sufficiently high correlations to rank stu-dents?
summaries (criterion 2).
On all correla-tion tests, the highest ranking automated methodis LVc, max, 0.64; this similarity threshold corre-sponds to the 0.15 icdf.
As shown in Table 1, thePearson correlation is 0.93.
Note, however, that itis not significantly higher than many of its com-petitors.
LVc, min, 0.40 did not rank as highly forSpeaman and Kendall?s tau correlations, but theStudent?s T result in column 3 of Table 1 showsthat this is the only variant in the table that yieldsabsolute scores that are not significantly differentfrom the human annotated scores.
Thus this vari-ant best balances criteria 1 and 2.The differences in the unnormalized score com-puted by the automated systems from the score as-signed by human annotation are consistently posi-tive.
Inspection of the SCUs retrieved by each au-tomated variant reveals that the automated systemslean toward the tendency to identify false posi-tives.
This may result from the DP implementationdecision to maximize the score.
To get a measureof the degree of overlap between the SCUs thatwere selected automatically versus manually (cri-terion 4), we computed recall and precision for thevarious methods.
Table 2 shows the mean recalland precision (with standard deviations) across allfive thresholds for each combination of similaritymethod and method of comparison to the SCU.The low standard deviations show that the recalland precision are relatively similar across thresh-olds for each variant.
The LVc methods outper-form R/O and unigram overlap methods, particu-larly for the precision of SCUs retrieved, indicat-ing the use of distributional semantics is a supe-rior approach for pyramid summary scoring thanmethods based on string matching.The unigram overlap and R/O methods show theleast variation across comparison methods (min,mean, max).
LVc methods outperform them, onprecision (Table 2).
Meeting all three criteria isdifficult, and the LVc method is clearly superior.7 ConclusionWe extended a dynamic programming frame-work (Harnly et al, 2005) to automate pyramidscores more accurately.
Improvements resultedfrom principled thresholds for similarity, and froma vector representation (LVc) to capture the latentsemantics of short spans of text (Guo and Diab,2012).
The LVc methods perform best at all threecriteria for a pedagogically useful automatic met-ric.
Future work will address how to improve pre-cision and recall of the gold SCUs.AcknowledgementsWe thank the reviewers for very valuable insights.Variant ?
Recall (std) ?
Precision (std) F scoreUni, min 0.69 (0.08) 0.35 (0.02) 0.52Uni, max 0.70 (0.03) 0.35 (0.04) 0.53Uni, mean 0.69 (0.02) 0.39 (0.04) 0.54R/O, min 0.69 (0.08) 0.34 (0.01) 0.51R/O, max 0.72 (0.03) 0.33 (0.04) 0.52R/O, mean 0.71 (0.06) 0.38 (0.02) 0.54LVc, min 0.61 (0.03) 0.38 (0.04) 0.49LVc, max 0.74 (0.06) 0.48 (0.01) 0.61LVc, mean 0.75 (0.06) 0.50 (0.02) 0.62Table 2: Recall and precision for SCU selection146ReferencesIsabel L. Beck, Margaret G. McKeown, Gale M. Sina-tra, and Jane A. Loxterman.
1991.
Revising socialstudies text from a text-processing perspective: Ev-idence of improved comprehensibility.
Reading Re-search Quarterly, pages 251?276.Chutima Boonthum-Denecke, Philip M. McCarthy,Travis A. Lamkin, G. Tanner Jackson, Joseph P.Maglianoc, and Danielle S. McNamara.
2011.
Au-tomatic natural language processing and the de-tection of reading skills and reading comprehen-sion.
In Proceedings of the Twenty-Fourth Interna-tional Florida Artificial Intelligence Research Soci-ety Conference, pages 234?239.Jill Burstein, Karen Kukich, Susanne Wolff, ChiLu, Martin Chodorow, Lisa Braden-Harder, andMary Dee Harris.
1998.
Automated scoring us-ing a hybrid feature identification technique.
In Pro-ceedings of the 36th Annual Meeting of the Associ-ation for Computational Linguistics and 17th Inter-national Conference on Computational Linguistics,pages 206?210, Montreal, Quebec, Canada, August.Association for Computational Linguistics.Jill Burstein.
2003.
The e-rater R?scoring engine: Au-tomated essay scoring with natural language pro-cessing.
In M. D. Shermis and J. Burstein, editors,Automated Essay Scoring: A Cross-disciplinaryPerspective.
Lawrence Erlbaum Associates, Inc.,Hillsdale, NJ.Bradley Efron and Robert Tibshirani.
1986.
Boot-strap methods for standard errors, confidence inter-vals, and other measures of statistical accuracy.
Sta-tistical Science, 1:54?77.Peter W. Foltz, Sara Gilliam, and Scott Kendall.
2000.Supporting content-based feedback in on-line writ-ing evaluation with LSA.
Interactive Learning En-vironments, 8:111?127.Arthur C. Graesser, Danielle S. McNamara, Max M.Louwerse, and Zhiqiang Cai.
2004.
Coh-Metrix:Analysis of text on cohesion and language.
Behav-ior Research Methods, Instruments, and Computers,36:193202.Arthur C. Graesser, Danielle S. McNamara, andJonna M. Kulikowich.
2011.
Coh-Metrix: Provid-ing multilevel analyses of text characteristics.
Edu-cational Researcher, 40:223?234.Weiwei Guo and Mona Diab.
2012.
Modeling sen-tences in the latent space.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics (ACL), pages 864?872.Aaron Harnly, Ani Nenkova, Rebecca J. Passonneau,and Owen Rambow.
2005.
Automation of summaryevaluation by the Pyramid Method.
In Recent Ad-vances in Natural Language Processing (RANLP),pages 226?232.Thomas K Landauer and Susan T. Dumais.
1997.
Asolution to Plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological review,pages 211?240.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL), pages 71?78.Chin-Yew Lin.
2004.
ROUGE: A package for au-tomatic evaluation of summaries.
In Proceedingsof the Human Language Technology Conferenceof the North American Chapter of the Associationfor Computational Linguistics (HLT-NAACL), pages463?470.Annie Louis and Ani Nenkova.
2009.
Evaluating con-tent selection in summarization without human mod-els.
In Proceedings of the 2009 Conference on Em-pirical Methods in Natural Language Processing,pages 306?314, Singapore, August.
Association forComputational Linguistics.Ani Nenkova and Rebecca J. Passonneau.
2004.Evaluating content selection in summarization: ThePyramid Method.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 145?152.Rebecca J. Passonneau, Adam Goodkind, and ElenaLevy.
2007.
Annotation of children?s oral narra-tions: Modeling emergent narrative skills for com-putational applications.
In Proceedings of the Twen-tieth Annual Meeting of the Florida Artificial Intel-ligence Research Society (FLAIRS-20), pages 253?258.
AAAI Press.Rebecca Passonneau.
2010.
Formal and functional as-sessment of the Pyramid Method for summary con-tent evaluation.
Natural Language Engineering, 16.D.
Perin, R. H. Bork, S. T. Peverly, and L. H. Mason.In press.
A contextualized curricular supplement fordevelopmental reading and writing.
Journal of Col-lege Reading and Learning.J.
W. Ratcliff and D. Metzener.
1988.
Pattern match-ing: the Gestalt approach.Ben Wellner, Lisa Ferro, Warren R. Greiff, and LynetteHirschman.
2006.
Reading comprehension tests forcomputer-based understanding evaluation.
NaturalLanguage Engineering, 12(4):305?334.147
