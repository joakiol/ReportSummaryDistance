Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 223?231,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsSpectral Learning of Latent-Variable PCFGsShay B. Cohen1, Karl Stratos1, Michael Collins1, Dean P. Foster2, and Lyle Ungar31Dept.
of Computer Science, Columbia University2Dept.
of Statistics/3Dept.
of Computer and Information Science, University of Pennsylvania{scohen,stratos,mcollins}@cs.columbia.edu, foster@wharton.upenn.edu, ungar@cis.upenn.eduAbstractWe introduce a spectral learning algorithm forlatent-variable PCFGs (Petrov et al, 2006).Under a separability (singular value) condi-tion, we prove that the method provides con-sistent parameter estimates.1 IntroductionStatistical models with hidden or latent variables areof great importance in natural language processing,speech, and many other fields.
The EM algorithm isa remarkably successful method for parameter esti-mation within these models: it is simple, it is oftenrelatively efficient, and it has well understood formalproperties.
It does, however, have a major limitation:it has no guarantee of finding the global optimum ofthe likelihood function.
From a theoretical perspec-tive, this means that the EM algorithm is not guar-anteed to give consistent parameter estimates.
Froma practical perspective, problems with local optimacan be difficult to deal with.Recent work has introduced polynomial-timelearning algorithms (and consistent estimation meth-ods) for two important cases of hidden-variablemodels: Gaussian mixture models (Dasgupta, 1999;Vempala and Wang, 2004) and hidden Markov mod-els (Hsu et al, 2009).
These algorithms use spec-tral methods: that is, algorithms based on eigen-vector decompositions of linear systems, in particu-lar singular value decomposition (SVD).
In the gen-eral case, learning of HMMs or GMMs is intractable(e.g., see Terwijn, 2002).
Spectral methods finessethe problem of intractibility by assuming separabil-ity conditions.
For example, the algorithm of Hsuet al (2009) has a sample complexity that is polyno-mial in 1/?, where ?
is the minimum singular valueof an underlying decomposition.
These methods arenot susceptible to problems with local maxima, andgive consistent parameter estimates.In this paper we derive a spectral algorithmfor learning of latent-variable PCFGs (L-PCFGs)(Petrov et al, 2006; Matsuzaki et al, 2005).
Ourmethod involves a significant extension of the tech-niques from Hsu et al (2009).
L-PCFGs have beenshown to be a very effective model for natural lan-guage parsing.
Under a separation (singular value)condition, our algorithm provides consistent param-eter estimates; this is in contrast with previous work,which has used the EM algorithm for parameter es-timation, with the usual problems of local optima.The parameter estimation algorithm (see figure 4)is simple and efficient.
The first step is to takean SVD of the training examples, followed by aprojection of the training examples down to a low-dimensional space.
In a second step, empirical av-erages are calculated on the training example, fol-lowed by standard matrix operations.
On test ex-amples, simple (tensor-based) variants of the inside-outside algorithm (figures 2 and 3) can be used tocalculate probabilities and marginals of interest.Our method depends on the following results:?
Tensor form of the inside-outside algorithm.Section 5 shows that the inside-outside algorithm forL-PCFGs can be written using tensors.
Theorem 1gives conditions under which the tensor form calcu-lates inside and outside terms correctly.?
Observable representations.
Section 6 showsthat under a singular-value condition, there is an ob-servable form for the tensors required by the inside-outside algorithm.
By an observable form, we fol-low the terminology of Hsu et al (2009) in referringto quantities that can be estimated directly from datawhere values for latent variables are unobserved.Theorem 2 shows that tensors derived from the ob-servable form satisfy the conditions of theorem 1.?
Estimating the model.
Section 7 gives an al-gorithm for estimating parameters of the observablerepresentation from training data.
Theorem 3 gives asample complexity result, showing that the estimatesconverge to the true distribution at a rate of 1/?Mwhere M is the number of training examples.The algorithm is strikingly different from the EMalgorithm for L-PCFGs, both in its basic form, andin its consistency guarantees.
The techniques de-223veloped in this paper are quite general, and shouldbe relevant to the development of spectral methodsfor estimation in other models in NLP, for exam-ple alignment models for translation, synchronousPCFGs, and so on.
The tensor form of the inside-outside algorithm gives a new view of basic calcula-tions in PCFGs, and may itself lead to new models.2 Related WorkFor work on L-PCFGs using the EM algorithm, seePetrov et al (2006), Matsuzaki et al (2005), Pereiraand Schabes (1992).
Our work builds on meth-ods for learning of HMMs (Hsu et al, 2009; Fos-ter et al, 2012; Jaeger, 2000), but involves sev-eral extensions: in particular in the tensor form ofthe inside-outside algorithm, and observable repre-sentations for the tensor form.
Balle et al (2011)consider spectral learning of finite-state transducers;Lugue et al (2012) considers spectral learning ofhead automata for dependency parsing.
Parikh et al(2011) consider spectral learning algorithms of tree-structured directed bayes nets.3 NotationGiven a matrix A or a vector v, we write A?
or v?for the associated transpose.
For any integer n ?
1,we use [n] to denote the set {1, 2, .
.
.
n}.
For anyrow or column vector y ?
Rm, we use diag(y) torefer to the (m?m) matrix with diagonal elementsequal to yh for h = 1 .
.
.
m, and off-diagonal ele-ments equal to 0.
For any statement ?, we use [[?
]]to refer to the indicator function that is 1 if ?
is true,and 0 if ?
is false.
For a random variable X, we useE[X] to denote its expected value.We will make (quite limited) use of tensors:Definition 1 A tensor C ?
R(m?m?m) is a set ofm3 parameters Ci,j,k for i, j, k ?
[m].
Given a ten-sor C , and a vector y ?
Rm, we define C(y) to bethe (m ?
m) matrix with components [C(y)]i,j =?k?[m]Ci,j,kyk.
Hence C can be interpreted as afunction C : Rm ?
R(m?m) that maps a vectory ?
Rm to a matrix C(y) of dimension (m?m).In addition, we define the tensor C?
?
R(m?m?m)for any tensor C ?
R(m?m?m) to have values[C?
]i,j,k = Ck,j,iFinally, for vectors x, y, z ?
Rm, xy?z?
is thetensor D ?
Rm?m?m where Dj,k,l = xjykzl (thisis analogous to the outer product: [xy?
]j,k = xjyk).4 L-PCFGs: Basic DefinitionsThis section gives a definition of the L-PCFG for-malism used in this paper.
An L-PCFG is a 5-tuple(N ,I,P,m, n) where:?
N is the set of non-terminal symbols in thegrammar.
I ?
N is a finite set of in-terminals.P ?
N is a finite set of pre-terminals.
We assumethat N = I ?
P, and I ?
P = ?.
Hence we havepartitioned the set of non-terminals into two subsets.?
[m] is the set of possible hidden states.?
[n] is the set of possible words.?
For all a ?
I , b ?
N , c ?
N , h1, h2, h3 ?
[m],we have a context-free rule a(h1) ?
b(h2) c(h3).?
For all a ?
P, h ?
[m], x ?
[n], we have acontext-free rule a(h) ?
x.Hence each in-terminal a ?
I is always the left-hand-side of a binary rule a ?
b c; and each pre-terminal a ?
P is always the left-hand-side of arule a ?
x.
Assuming that the non-terminals inthe grammar can be partitioned this way is relativelybenign, and makes the estimation problem cleaner.We define the set of possible ?skeletal rules?
asR = {a ?
b c : a ?
I, b ?
N , c ?
N}.
Theparameters of the model are as follows:?
For each a?
b c ?
R, and h ?
[m], we havea parameter q(a ?
b c|h, a).
For each a ?
P,x ?
[n], and h ?
[m], we have a parameterq(a ?
x|h, a).
For each a ?
b c ?
R, andh, h?
?
[m], we have parameters s(h?|h, a ?
b c)and t(h?|h, a?
b c).These definitions give a PCFG, with rule proba-bilitiesp(a(h1) ?
b(h2) c(h3)|a(h1)) =q(a?
b c|h1, a)?
s(h2|h1, a?
b c)?
t(h3|h1, a?
b c)and p(a(h) ?
x|a(h)) = q(a?
x|h, a).In addition, for each a ?
I , for each h ?
[m], wehave a parameter ?
(a, h) which is the probability ofnon-terminal a paired with hidden variable h beingat the root of the tree.An L-PCFG defines a distribution over parse treesas follows.
A skeletal tree (s-tree) is a sequence ofrules r1 .
.
.
rN where each ri is either of the forma ?
b c or a ?
x.
The rule sequence formsa top-down, left-most derivation under a CFG withskeletal rules.
See figure 1 for an example.A full tree consists of an s-tree r1 .
.
.
rN , togetherwith values h1 .
.
.
hN .
Each hi is the value for224S1NP2D3theN4dogVP5V6sawP7himr1 = S ?
NP VPr2 = NP ?
D Nr3 = D ?
ther4 = N ?
dogr5 = VP ?
V Pr6 = V ?
sawr7 = P ?
himFigure 1: An s-tree, and its sequence of rules.
(For con-venience we have numbered the nodes in the tree.
)the hidden variable for the left-hand-side of rule ri.Each hi can take any value in [m].Define ai to be the non-terminal on the left-hand-side of rule ri.
For any i ?
{2 .
.
.
N} define pa(i)to be the index of the rule above node i in the tree.Define L ?
[N ] to be the set of nodes in the treewhich are the left-child of some parent, and R ?
[N ] to be the set of nodes which are the right-child ofsome parent.
The probability mass function (PMF)over full trees is thenp(r1 .
.
.
rN , h1 .
.
.
hN ) = ?
(a1, h1)?N?i=1q(ri|hi, ai)?
?i?Ls(hi|hpa(i), rpa(i))?
?i?Rt(hi|hpa(i), rpa(i)) (1)The PMF over s-trees is p(r1 .
.
.
rN ) =?h1...hN p(r1 .
.
.
rN , h1 .
.
.
hN ).In the remainder of this paper, we make use of ma-trix form of parameters of an L-PCFG, as follows:?
For each a?
b c ?
R, we define Qa?b c ?Rm?m to be the matrix with values q(a ?
b c|h, a)for h = 1, 2, .
.
.
m on its diagonal, and 0 values forits off-diagonal elements.
Similarly, for each a ?
P,x ?
[n], we define Qa?x ?
Rm?m to be the matrixwith values q(a ?
x|h, a) for h = 1, 2, .
.
.
m on itsdiagonal, and 0 values for its off-diagonal elements.?
For each a ?
b c ?
R, we define Sa?b c ?Rm?m where [Sa?b c]h?,h = s(h?|h, a?
b c).?
For each a ?
b c ?
R, we define T a?b c ?Rm?m where [T a?b c]h?,h = t(h?|h, a?
b c).?
For each a ?
I , we define the vector ?a ?
Rmwhere [?a]h = ?
(a, h).5 Tensor Form of the Inside-OutsideAlgorithmGiven an L-PCFG, two calculations are central:Inputs: s-tree r1 .
.
.
rN , L-PCFG (N , I,P ,m, n), parameters?
Ca?b c ?
R(m?m?m) for all a?
b c ?
R?
c?a?x ?
R(1?m) for all a ?
P , x ?
[n]?
c1a ?
R(m?1) for all a ?
I.Algorithm: (calculate the f i terms bottom-up in the tree)?
For all i ?
[N ] such that ai ?
P , f i = c?ri?
For all i ?
[N ] such that ai ?
I, f i = f?Cri(f?)
where?
is the index of the left child of node i in the tree, and ?is the index of the right child.Return: f1c1a1 = p(r1 .
.
.
rN)Figure 2: The tensor form for calculation of p(r1 .
.
.
rN ).1.
For a given s-tree r1 .
.
.
rN , calculatep(r1 .
.
.
rN ).2.
For a given input sentence x = x1 .
.
.
xN , cal-culate the marginal probabilities?
(a, i, j) =??
?T (x):(a,i,j)??p(?
)for each non-terminal a ?
N , for each (i, j)such that 1 ?
i ?
j ?
N .Here T (x) denotes the set of all possible s-trees forthe sentence x, and we write (a, i, j) ?
?
if non-terminal a spans words xi .
.
.
xj in the parse tree ?
.The marginal probabilities have a number of uses.Perhaps most importantly, for a given sentence x =x1 .
.
.
xN , the parsing algorithm of Goodman (1996)can be used to findarg max?
?T (x)?(a,i,j)???
(a, i, j)This is the parsing algorithm used by Petrov et al(2006), for example.
In addition, we can calcu-late the probability for an input sentence, p(x) =??
?T (x) p(?
), as p(x) =?a?I ?
(a, 1, N).Variants of the inside-outside algorithm can beused for problems 1 and 2.
This section introduces anovel form of these algorithms, using tensors.
Thisis the first step in deriving the spectral estimationmethod.The algorithms are shown in figures 2 and 3.
Eachalgorithm takes the following inputs:1.
A tensor Ca?b c ?
R(m?m?m) for each rulea?
b c.2.
A vector c?a?x ?
R(1?m) for each rule a?
x.2253.
A vector c1a ?
R(m?1) for each a ?
I .The following theorem gives conditions underwhich the algorithms are correct:Theorem 1 Assume that we have an L-PCFG withparameters Qa?x, Qa?b c, T a?b c, Sa?b c, ?a, andthat there exist matrices Ga ?
R(m?m) for all a ?N such that each Ga is invertible, and such that:1.
For all rules a?
b c, Ca?b c(y) =GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?12.
For all rules a?
x, c?a?x = 1?Qa?x(Ga)?13.
For all a ?
I , c1a = Ga?aThen: 1) The algorithm in figure 2 correctly com-putes p(r1 .
.
.
rN ) under the L-PCFG.
2) The algo-rithm in figure 3 correctly computes the marginals?
(a, i, j) under the L-PCFG.Proof: See section 9.1.6 Estimating the Tensor ModelA crucial result is that it is possible to directly esti-mate parameters Ca?b c, c?a?x and c1a that satisfy theconditions in theorem 1, from a training sample con-sisting of s-trees (i.e., trees where hidden variablesare unobserved).
We first describe random variablesunderlying the approach, then describe observablerepresentations based on these random variables.6.1 Random Variables Underlying the ApproachEach s-tree with N rules r1 .
.
.
rN has N nodes.
Wewill use the s-tree in figure 1 as a running example.Each node has an associated rule: for example,node 2 in the tree in figure 1 has the rule NP?
D N.If the rule at a node is of the form a?
b c, then thereare left and right inside trees below the left child andright child of the rule.
For example, for node 2 wehave a left inside tree rooted at node 3, and a rightinside tree rooted at node 4 (in this case the left andright inside trees both contain only a single rule pro-duction, of the form a ?
x; however in the generalcase they might be arbitrary subtrees).In addition, each node has an outside tree.
Fornode 2, the outside tree isSNP VPVsawPhimInputs: Sentence x1 .
.
.
xN , L-PCFG (N , I,P ,m, n), param-eters Ca?b c ?
R(m?m?m) for all a?
b c ?
R, c?a?x ?R(1?m) for all a ?
P , x ?
[n], c1a ?
R(m?1) for all a ?
I.Data structures:?
Each ?a,i,j ?
R1?m for a ?
N , 1 ?
i ?
j ?
N is arow vector of inside terms.?
Each ?a,i,j ?
Rm?1 for a ?
N , 1 ?
i ?
j ?
N is acolumn vector of outside terms.?
Each ?
(a, i, j) ?
R for a ?
N , 1 ?
i ?
j ?
N is amarginal probability.Algorithm:(Inside base case) ?a ?
P , i ?
[N ], ?a,i,i = c?a?xi(Inside recursion) ?a ?
I, 1 ?
i < j ?
N,?a,i,j =j?1?k=i?a?b c?c,k+1,jCa?b c(?b,i,k)(Outside base case) ?a ?
I, ?a,1,n = c1a(Outside recursion) ?a ?
N , 1 ?
i ?
j ?
N,?a,i,j =i?1?k=1?b?c aCb?c a(?c,k,i?1)?b,k,j+N?k=j+1?b?a cCb?a c?
(?c,j+1,k)?b,i,k(Marginals) ?a ?
N , 1 ?
i ?
j ?
N,?
(a, i, j) = ?a,i,j?a,i,j =?h?
[m]?a,i,jh ?a,i,jhFigure 3: The tensor form of the inside-outside algorithm,for calculation of marginal terms ?
(a, i, j).The outside tree contains everything in the s-treer1 .
.
.
rN , excluding the subtree below node i.Our random variables are defined as follows.First, we select a random internal node, from a ran-dom tree, as follows:?
Sample an s-tree r1 .
.
.
rN from the PMFp(r1 .
.
.
rN ).
Choose a node i uniformly at ran-dom from [N ].If the rule ri for the node i is of the form a?
b c,we define random variables as follows:?
R1 is equal to the rule ri (e.g., NP ?
D N).?
T1 is the inside tree rooted at node i. T2 is theinside tree rooted at the left child of node i, and T3is the inside tree rooted at the right child of node i.?
H1,H2,H3 are the hidden variables associatedwith node i, the left child of node i, and the rightchild of node i respectively.226?
A1, A2, A3 are the labels for node i, the leftchild of node i, and the right child of node i respec-tively.
(E.g., A1 = NP, A2 = D, A3 = N.)?
O is the outside tree at node i.?
B is equal to 1 if node i is at the root of the tree(i.e., i = 1), 0 otherwise.If the rule ri for the selected node i is ofthe form a ?
x, we have random vari-ables R1, T1,H1, A1, O,B as defined above, butH2,H3, T2, T3, A2, and A3 are not defined.We assume a function ?
that maps outside trees oto feature vectors ?
(o) ?
Rd?
.
For example, the fea-ture vector might track the rule directly above thenode in question, the word following the node inquestion, and so on.
We also assume a function ?that maps inside trees t to feature vectors ?
(t) ?
Rd.As one example, the function ?
might be an indica-tor function tracking the rule production at the rootof the inside tree.
Later we give formal criteria forwhat makes good definitions of ?
(o) of ?(t).
Onerequirement is that d?
?
m and d ?
m.In tandem with these definitions, we assume pro-jection matices Ua ?
R(d?m) and V a ?
R(d?
?m)for all a ?
N .
We then define additional randomvariables Y1, Y2, Y3, Z asY1 = (Ua1)??
(T1) Z = (V a1)??
(O)Y2 = (Ua2)??
(T2) Y3 = (Ua3)??
(T3)where ai is the value of the random variable Ai.Note that Y1, Y2, Y3, Z are all in Rm.6.2 Observable RepresentationsGiven the definitions in the previous section, ourrepresentation is based on the following matrix, ten-sor and vector quantities, defined for all a ?
N , forall rules of the form a?
b c, and for all rules of theform a?
x respectively:?a = E[Y1Z?|A1 = a]Da?b c = E[[[R1 = a?
b c]]Y3Z?Y ?2 |A1 = a]d?a?x = E[[[R1 = a?
x]]Z?|A1 = a]Assuming access to functions ?
and ?, and projec-tion matrices Ua and V a, these quantities can be es-timated directly from training data consisting of aset of s-trees (see section 7).Our observable representation then consists of:Ca?b c(y) = Da?b c(y)(?a)?1 (2)c?a?x = d?a?x(?a)?1 (3)c1a = E [[[A1 = a]]Y1|B = 1] (4)We next introduce conditions under which thesequantities satisfy the conditions in theorem 1.The following definition will be important:Definition 2 For all a ?
N , we define the matricesIa ?
R(d?m) and Ja ?
R(d?
?m) as[Ia]i,h = E[?i(T1) | H1 = h,A1 = a][Ja]i,h = E[?i(O) | H1 = h,A1 = a]In addition, for any a ?
N , we use ?a ?
Rm todenote the vector with ?ah = P (H1 = h|A1 = a).The correctness of the representation will rely onthe following conditions being satisfied (these areparallel to conditions 1 and 2 in Hsu et al (2009)):Condition 1 ?a ?
N , the matrices Ia and Ja areof full rank (i.e., they have rank m).
For all a ?
N ,for all h ?
[m], ?ah > 0.Condition 2 ?a ?
N , the matrices Ua ?
R(d?m)and V a ?
R(d?
?m) are such that the matrices Ga =(Ua)?Ia and Ka = (V a)?Ja are invertible.The following lemma justifies the use of an SVDcalculation as one method for finding values for Uaand V a that satisfy condition 2:Lemma 1 Assume that condition 1 holds, and forall a ?
N define?a = E[?
(T1) (?(O))?
|A1 = a] (5)Then if Ua is a matrix of the m left singular vec-tors of ?a corresponding to non-zero singular val-ues, and V a is a matrix of the m right singular vec-tors of ?a corresponding to non-zero singular val-ues, then condition 2 is satisfied.Proof sketch: It can be shown that ?a =Iadiag(?a)(Ja)?.
The remainder is similar to theproof of lemma 2 in Hsu et al (2009).The matrices ?a can be estimated directly from atraining set consisting of s-trees, assuming that wehave access to the functions ?
and ?.We can now state the following theorem:227Theorem 2 Assume conditions 1 and 2 are satisfied.For all a ?
N , define Ga = (Ua)?Ia.
Then underthe definitions in Eqs.
2-4:1.
For all rules a?
b c, Ca?b c(y) =GcT a?b cdiag(yGbSa?b c)Qa?b c(Ga)?12.
For all rules a?
x, c?a?x = 1?Qa?x(Ga)?1.3.
For all a ?
N , c1a = Ga?aProof: The following identities hold (see sec-tion 9.2):Da?b c(y) = (6)GcT a?b cdiag(yGbSa?b c)Qa?b cdiag(?a)(Ka)?d?a?x = 1?Qa?xdiag(?a)(Ka)?
(7)?a = Gadiag(?a)(Ka)?
(8)c1a = Gapia (9)Under conditions 1 and 2, ?a is invertible, and(?a)?1 = ((Ka)?)?1(diag(?a))?1(Ga)?1.
Theidentities in the theorem follow immediately.7 Deriving Empirical EstimatesFigure 4 shows an algorithm that derives esti-mates of the quantities in Eqs 2, 3, and 4.
Asinput, the algorithm takes a sequence of tuples(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for i ?
[M ].These tuples can be derived from a training setconsisting of s-trees ?1 .
.
.
?M as follows:?
?i ?
[M ], choose a single node ji uniformly atrandom from the nodes in ?i.
Define r(i,1) to be therule at node ji.
t(i,1) is the inside tree rooted at nodeji.
If r(i,1) is of the form a?
b c, then t(i,2) is theinside tree under the left child of node ji, and t(i,3)is the inside tree under the right child of node ji.
Ifr(i,1) is of the form a ?
x, then t(i,2) = t(i,3) =NULL.
o(i) is the outside tree at node ji.
b(i) is 1 ifnode ji is at the root of the tree, 0 otherwise.Under this process, assuming that the s-trees?1 .
.
.
?M are i.i.d.
draws from the distributionp(?)
over s-trees under an L-PCFG, the tuples(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) are i.i.d.
drawsfrom the joint distribution over the random variablesR1, T1, T2, T3, O,B defined in the previous section.The algorithm first computes estimates of the pro-jection matrices Ua and V a: following lemma 1,this is done by first deriving estimates of ?a,and then taking SVDs of each ?a.
The matricesare then used to project inside and outside treest(i,1), t(i,2), t(i,3), o(i) down to m-dimensional vec-tors y(i,1), y(i,2), y(i,3), z(i); these vectors are used toderive the estimates of Ca?b c, c?a?x, and c1a.We now state a PAC-style theorem for the learningalgorithm.
First, for a given L-PCFG, we need acouple of definitions:?
?
is the minimum absolute value of any elementof the vectors/matrices/tensors c1a, d?a?x, Da?b c,(?a)?1.
(Note that ?
is a function of the projec-tion matrices Ua and V a as well as the underlyingL-PCFG.)?
For each a ?
N , ?a is the value of the m?thlargest singular value of ?a.
Define ?
= mina ?a.We then have the following theorem:Theorem 3 Assume that the inputs to the algorithmin figure 4 are i.i.d.
draws from the joint distributionover the random variables R1, T1, T2, T3, O,B, un-der an L-PCFG with distribution p(r1 .
.
.
rN ) overs-trees.
Define m to be the number of latent statesin the L-PCFG.
Assume that the algorithm in fig-ure 4 has projection matrices U?a and V?
a derived asleft and right singular vectors of ?a, as defined inEq.
5.
Assume that the L-PCFG, together with U?aand V?
a, has coefficients ?
> 0 and ?
> 0.
In addi-tion, assume that all elements in c1a, d?a?x, Da?b c,and ?a are in [?1,+1].
For any s-tree r1 .
.
.
rN de-fine p?
(r1 .
.
.
rN ) to be the value calculated by thealgorithm in figure 3 with inputs c?1a, c?
?a?x, C?a?b cderived from the algorithm in figure 4.
Define R tobe the total number of rules in the grammar of theform a?
b c or a ?
x.
Define Ma to be the num-ber of training examples in the input to the algorithmin figure 4 where ri,1 has non-terminal a on its left-hand-side.
Under these assumptions, if for all aMa ?128m2( 2N+1?1 + ??
1)2 ?2?4log(2mR?)Then1?
?
?????p?
(r1 .
.
.
rN )p(r1 .
.
.
rN )?????
1 + ?A similar theorem (omitted for space) states that1?
?
??????(a,i,j)?(a,i,j)????
1 + ?
for the marginals.The condition that U?a and V?
a are derived from?a, as opposed to the sample estimate ?
?a, followsFoster et al (2012).
As these authors note, similartechniques to those of Hsu et al (2009) should be228applicable in deriving results for the case where ?
?ais used in place of ?a.Proof sketch: The proof is similar to that of Fosteret al (2012).
The basic idea is to first show thatunder the assumptions of the theorem, the estimatesc?1a, d?
?a?x, D?a?b c, ?
?a are all close to the underlyingvalues being estimated.
The second step is to showthat this ensures that p?(r1...rN?
)p(r1...rN? )
is close to 1.The method described of selecting a single tuple(r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i)) for each s-tree en-sures that the samples are i.i.d., and simplifies theanalysis underlying theorem 3.
In practice, an im-plementation should most likely use all nodes in alltrees in training data; by Rao-Blackwellization weknow such an algorithm would be better than theone presented, but the analysis of how much betterwould be challenging.
It would almost certainly leadto a faster rate of convergence of p?
to p.8 DiscussionThere are several potential applications of themethod.
The most obvious is parsing with L-PCFGs.1 The approach should be applicable in othercases where EM has traditionally been used, for ex-ample in semi-supervised learning.
Latent-variableHMMs for sequence labeling can be derived as spe-cial case of our approach, by converting tagged se-quences to right-branching skeletal trees.The sample complexity of the method depends onthe minimum singular values of ?a; these singularvalues are a measure of how well correlated ?
and?
are with the unobserved hidden variable H1.
Ex-perimental work is required to find a good choice ofvalues for ?
and ?
for parsing.9 ProofsThis section gives proofs of theorems 1 and 2.
Dueto space limitations we cannot give full proofs; in-stead we provide proofs of some key lemmas.
Along version of this paper will give the full proofs.9.1 Proof of Theorem 1First, the following lemma leads directly to the cor-rectness of the algorithm in figure 2:1Parameters can be estimated using the algorithm infigure 4; for a test sentence x1 .
.
.
xN we can firstuse the algorithm in figure 3 to calculate marginals?
(a, i, j), then use the algorithm of Goodman (1996) to findargmax?
?T (x)?(a,i,j)??
?
(a, i, j).Inputs: Training examples (r(i,1), t(i,1), t(i,2), t(i,3), o(i), b(i))for i ?
{1 .
.
.M}, where r(i,1) is a context free rule; t(i,1),t(i,2) and t(i,3) are inside trees; o(i) is an outside tree; andb(i) = 1 if the rule is at the root of tree, 0 otherwise.
A function?
that maps inside trees t to feature-vectors ?
(t) ?
Rd.
A func-tion ?
that maps outside trees o to feature-vectors ?
(o) ?
Rd?
.Algorithm:Define ai to be the non-terminal on the left-hand side of ruler(i,1).
If r(i,1) is of the form a?
b c, define bi to be the non-terminal for the left-child of r(i,1), and ci to be the non-terminalfor the right-child.
(Step 0: Singular Value Decompositions)?
Use the algorithm in figure 5 to calculate matrices U?a ?R(d?m) and V?
a ?
R(d?
?m) for each a ?
N .
(Step 1: Projection)?
For all i ?
[M ], compute y(i,1) = (U?ai)??(t(i,1)).?
For all i ?
[M ] such that r(i,1) is of the forma?
b c, compute y(i,2) = (U?bi)??
(t(i,2)) and y(i,3) =(U?ci)??(t(i,3)).?
For all i ?
[M ], compute z(i) = (V?
ai)??(o(i)).
(Step 2: Calculate Correlations)?
For each a ?
N , define ?a = 1/?Mi=1[[ai = a]]?
For each rule a?
b c, compute D?a?b c = ?a ?
?Mi=1[[r(i,1) = a?
b c]]y(i,3)(z(i))?(y(i,2))??
For each rule a ?
x, compute d?
?a?x = ?a ?
?Mi=1[[r(i,1) = a?
x]](z(i))??
For each a ?
N , compute ?
?a = ?a ?
?Mi=1[[ai = a]]y(i,1)(z(i))?
(Step 3: Compute Final Parameters)?
For all a?
b c, C?a?b c(y) = D?a?b c(y)(??a)?1?
For all a?
x, c?
?a?x = d??a?x(??a)?1?
For all a ?
I, c?1a =?Mi=1[[ai=a and b(i)=1]]y(i,1)?Mi=1[[b(i)=1]]Figure 4: The spectral learning algorithm.Inputs: Identical to algorithm in figure 4.Algorithm:?
For each a ?
N , compute ?
?a ?
R(d?
?d) as?
?a =?Mi=1[[ai = a]]?(t(i,1))(?(o(i)))?
?Mi=1[[ai = a]]and calculate a singular value decomposition of ??a.?
For each a ?
N , define U?a ?
Rm?d to be a matrix of the leftsingular vectors of ?
?a corresponding to the m largest singularvalues.
Define V?
a ?
Rm?d?
to be a matrix of the right singularvectors of ?
?a corresponding to the m largest singular values.Figure 5: Singular value decompositions.229Lemma 2 Assume that conditions 1-3 of theorem 1are satisfied, and that the input to the algorithm infigure 2 is an s-tree r1 .
.
.
rN .
Define ai for i ?
[N ]to be the non-terminal on the left-hand-side of ruleri, and ti for i ?
[N ] to be the s-tree with rule riat its root.
Finally, for all i ?
[N ], define the rowvector bi ?
R(1?m) to have componentsbih = P (Ti = ti|Hi = h,Ai = ai)for h ?
[m].
Then for all i ?
[N ], f i = bi(G(ai))?1.It follows immediately thatf1c1a1 = b1(G(a1))?1Ga1?a1 = p(r1 .
.
.
rN )This lemma shows a direct link between the vec-tors f i calculated in the algorithm, and the terms bih,which are terms calculated by the conventional in-side algorithm: each f i is a linear transformation(through Gai) of the corresponding vector bi.Proof: The proof is by induction.First consider the base case.
For any leaf?i.e., forany i such that ai ?
P?we have bih = q(ri|h, ai),and it is easily verified that f i = bi(G(ai))?1.The inductive case is as follows.
For all i ?
[N ]such that ai ?
I , by the definition in the algorithm,f i = f?Cri(f?
)= f?Ga?T ridiag(f?Ga?Sri)Qri(Gai)?1Assuming by induction that f?
= b?(G(a?
))?1 andf?
= b?(G(a?
))?1, this simplifies tof i = ?rdiag(?l)Qri(Gai)?1 (10)where ?r = b?T ri , and ?l = b?Sri .
?r is a rowvector with components ?rh =?h??
[m] b?h?Trih?,h =?h??
[m] b?h?t(h?|h, ri).
Similarly, ?l is a row vectorwith components equal to ?lh =?h??
[m] b?h?Srih?,h =?h??
[m] b?h?s(h?|h, ri).
It can then be verified that?rdiag(?l)Qri is a row vector with componentsequal to ?rh?lhq(ri|h, ai).But bih = q(ri|h, ai)?(?h??
[m] b?h?t(h?|h, ri))?(?h??
[m] b?h?s(h?|h, ri))= q(ri|h, ai)?rh?lh, hence?rdiag(?l)Qri = bi and the inductive case followsimmediately from Eq.
10.Next, we give a similar lemma, which implies thecorrectness of the algorithm in figure 3:Lemma 3 Assume that conditions 1-3 of theorem 1are satisfied, and that the input to the algorithm infigure 3 is a sentence x1 .
.
.
xN .
For any a ?
N , forany 1 ?
i ?
j ?
N , define ?
?a,i,j ?
R(1?m) to havecomponents ?
?a,i,jh = p(xi .
.
.
xj|h, a) for h ?
[m].In addition, define ?
?a,i,j ?
R(m?1) to have compo-nents ?
?a,i,jh = p(x1 .
.
.
xi?1, a(h), xj+1 .
.
.
xN ) forh ?
[m].
Then for all i ?
[N ], ?a,i,j = ?
?a,i,j(Ga)?1and ?a,i,j = Ga?
?a,i,j .
It follows that for all (a, i, j),?
(a, i, j) = ??a,i,j(Ga)?1Ga?
?a,i,j = ?
?a,i,j ??a,i,j=?h?
?a,i,jh ?
?a,i,jh =??
?T (x):(a,i,j)??p(?
)Thus the vectors ?a,i,j and ?a,i,j are linearly re-lated to the vectors ?
?a,i,j and ?
?a,i,j , which are theinside and outside terms calculated by the conven-tional form of the inside-outside algorithm.The proof is by induction, and is similar to theproof of lemma 2; for reasons of space it is omitted.9.2 Proof of the Identity in Eq.
6We now prove the identity in Eq.
6, used in the proofof theorem 2.
For reasons of space, we do not givethe proofs of identities 7-9: the proofs are similar.The following identities can be verified:P (R1 = a?
b c|H1 = h,A1 = a) = q(a?
b c|h, a)E [Y3,j|H1 = h,R1 = a?
b c] = Ea?b cj,hE [Zk|H1 = h,R1 = a?
b c] = Kak,hE [Y2,l|H1 = h,R1 = a?
b c] = F a?b cl,hwhere Ea?b c = GcT a?b c, F a?b c = GbSa?b c.Y3, Z and Y2 are independent when conditionedon H1, R1 (this follows from the independence as-sumptions in the L-PCFG), henceE [[[R1 = a?
b c]]Y3,jZkY2,l | H1 = h,A1 = a]= q(a?
b c|h, a)Ea?b cj,h Kak,hF a?b cl,hHence (recall that ?ah = P (H1 = h|A1 = a)),Da?b cj,k,l = E [[[R1 = a?
b c]]Y3,jZkY2,l | A1 = a]=?h?ahE [[[R1 = a?
b c]]Y3,jZkY2,l | H1 = h,A1 = a]=?h?ahq(a?
b c|h, a)Ea?b cj,h Kak,hF a?b cl,h (11)from which Eq.
6 follows.230Acknowledgements: Columbia University gratefully ac-knowledges the support of the Defense Advanced Re-search Projects Agency (DARPA) Machine Reading Pro-gram under Air Force Research Laboratory (AFRL)prime contract no.
FA8750-09-C-0181.
Any opinions,findings, and conclusions or recommendations expressedin this material are those of the author(s) and do not nec-essarily reflect the view of DARPA, AFRL, or the USgovernment.
Shay Cohen was supported by the NationalScience Foundation under Grant #1136996 to the Com-puting Research Association for the CIFellows Project.Dean Foster was supported by National Science Founda-tion grant 1106743.ReferencesB.
Balle, A. Quattoni, and X. Carreras.
2011.
A spec-tral learning algorithm for finite state transducers.
InProceedings of ECML.S.
Dasgupta.
1999.
Learning mixtures of Gaussians.
InProceedings of FOCS.Dean P. Foster, Jordan Rodu, and Lyle H. Ungar.2012.
Spectral dimensionality reduction for hmms.arXiv:1203.6130v1.J.
Goodman.
1996.
Parsing algorithms and metrics.
InProceedings of the 34th annual meeting on Associ-ation for Computational Linguistics, pages 177?183.Association for Computational Linguistics.D.
Hsu, S. M. Kakade, and T. Zhang.
2009.
A spec-tral algorithm for learning hidden Markov models.
InProceedings of COLT.H.
Jaeger.
2000.
Observable operator models for discretestochastic time series.
Neural Computation, 12(6).F.
M. Lugue, A. Quattoni, B. Balle, and X. Carreras.2012.
Spectral learning for non-deterministic depen-dency parsing.
In Proceedings of EACL.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Proba-bilistic CFG with latent annotations.
In Proceedingsof the 43rd Annual Meeting on Association for Com-putational Linguistics, pages 75?82.
Association forComputational Linguistics.A.
Parikh, L. Song, and E. P. Xing.
2011.
A spectral al-gorithm for latent tree graphical models.
In Proceed-ings of The 28th International Conference on MachineLearningy (ICML 2011).F.
Pereira and Y. Schabes.
1992.
Inside-outside reesti-mation from partially bracketed corpora.
In Proceed-ings of the 30th Annual Meeting of the Association forComputational Linguistics, pages 128?135, Newark,Delaware, USA, June.
Association for ComputationalLinguistics.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and 44thAnnual Meeting of the Association for ComputationalLinguistics, pages 433?440, Sydney, Australia, July.Association for Computational Linguistics.S.
A. Terwijn.
2002.
On the learnability of hiddenmarkov models.
In Grammatical Inference: Algo-rithms and Applications (Amsterdam, 2002), volume2484 of Lecture Notes in Artificial Intelligence, pages261?268, Berlin.
Springer.S.
Vempala and G. Wang.
2004.
A spectral algorithm forlearning mixtures of distributions.
Journal of Com-puter and System Sciences, 68(4):841?860.231
