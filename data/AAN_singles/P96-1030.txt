FAST PARSING US ING PRUNING AND GRAMMARSPECIAL IZAT IONManny Rayner  and  Dav id  Car terSRI Internat ionalSuite 23, Millers YardCambridge CB2 1RQUnited K ingdommanny@cam, sri.
com, dmc~cam, sri.
comAbst rac tWe show how a general grammar maybe automatically adapted for fast parsingof utterances from a specific domain bymeans of constituent pruning and grammarspecialization based on explanation-basedlearning.
These methods together give anorder of magnitude increase in speed, andthe coverage loss entailed by grammar spe-cialization is reduced to approximately halfthat reported in previous work.
Experi-ments described here suggest that the lossof coverage has been reduced to the pointwhere it no longer causes ignificant perfor-mance degradation i the context of a realapplication.1 In t roduct ionSuppose that we have a general grammar for En-glish, or some other natural anguage; by this, wemean a grammar which encodes most of the impor-tant constructions in the language, and which is in-tended to be applicable to a large range of differentdomains and applications.
The basic question at-tacked in this paper is the following one: can such agrammar be concretely useful if we want to processinput from a specific domain?
In particular, how cana parser that uses a general grammar achieve a levelof efficiency that is practically acceptable?The central problem is simple to state.
By thevery nature of its construction, a general grammarallows a great many theoretically valid analyses ofalmost any non-trivial sentence.
However, in thecontext of a specific domain, most of these will be ex-tremely implausible, and can in practice be ignored.If we want efficient parsing, we want to be able tofocus our search on only a small portion of the spaceof theoretically valid grammatical nalyses.One possible solution is of course to dispensewith the idea of using a general grammar, and sim-ply code a new grammar for each domain.
Manypeople do this, but one cannot help feeling thatsomething is being missed; intuitively, there aremany domain-independent grammatical constraints,which one would prefer only to need to code once.In the last ten years, there have been a numberof attempts to find ways to automatically adapt ageneral grammar and/or parser to the sub-languagedefined by a suitable training corpus.
For exam-ple, (Briscoe and Carroll, 1993) train an LR parserbased on a general grammar to be able to distin-guish between likely and unlikely sequences of pars-ing actions; (Andry et al, 1994) automatically infersortal constraints, that can be used to rule out oth-erwise grammatical constituents; and (Grishman etal., 1984) describes methods that reduce the size of ageneral grammar to include only rules actually use-ful for parsing the training corpus.The work reported here is a logical continuationof two specific strands of research aimed in this gen-eral direction.
The first is the popular idea of sta-tistical tagging e.g.
(DeRose, 1988; Cutting et al,1992; Church, 1988).
Here, the basic idea is thata given small segment S of the input string mayhave several possible analyses; in particular, if Sis a single word, it may potentially be any one ofseveral parts of speech.
However, if a substantialtraining corpus is available to provide reasonable es-timates of the relevant parameters, the immediatecontext surrounding S will usually make most of thelocally possible analyses of S extremely implausible.In the specific case of part-of-speech tagging, it iswell-known (DeMarcken, 1990) that a large propor-tion of the incorrect tags can be eliminated "safely"~i.e.
with very low risk of eliminating correct tags.In the present paper, the statistical tagging idea isgeneralized to a method called "constituent prun-ing"; this acts on local analyses of phrases normally223larger than single-word units.Constituent pruning is a bottom-up approach,and is complemented by a second, top-down,method based on Explanation-Based Learning (EBL;(Mitchell et al, 1986; van Harmelen and Bundy,1988)).
This part of the paper is essentially an exten-sion and generalization of the line of work describedin (Rayner, 1988; Rayner and Samuelsson, 1990;Samuelsson and Rayner, 1991; Rayner and Samuels-son, 1994; Samuelsson, 1994b).
Here, the basic ideais that grammar rules tend in any specific domain tocombine much more frequently in some ways thanin others.
Given a sufficiently large corpus parsedby the original, general, grammar, it is possible toidentify the common combinations ofgrammar rulesand "chunk" them into "macro-rules".
The result isa "specialized" grammar; this has a larger number ofrules, but a simpler structure, allowing it in practiceto be parsed very much more quickly using an LR-based method (Samuelsson, 1994a).
The coverageof the specialized grammar is a strict subset of thatof the original grammar; thus any analysis producedby the specialized grammar is guaranteed to be validin the original one as well.
The practical utility ofthe specialized grammar is largely determined by theloss of coverage incurred by the specialization pro-cess.The two methods, constituent pruning and gram-mar specialization, are combined as follows.
Therules in the original, general, grammar are dividedinto two sets, called phrasal and non-phrasal respec-tively.
Phrasal rules, the majority of which definenon-recursive noun phrase constructions, are usedas they are; non-phrasal rules are combined usingEBL into chunks, forming a specialized grammarwhich is then compiled further into a set of LR-tables.
Parsing proceeds by interleaving constituentcreation and deletion.
First, the lexicon and mor-phology rules are used to hypothesize word analyses.Constituent pruning then removes all sufficiently un-likely edges.
Next, the phrasal rules are appliedbottom-up, to find all possible phrasal edges, afterwhich unlikely edges are again pruned.
Finally, thespecialized grammar is used to search for full parses.The scheme is fully implemented within a version ofthe Spoken Language Translator system (Rayner etal., 1993; Agniis et al, 1994), and is normally appliedto input in the form of small lattices of hypothesesproduced by a speech recognizer.The rest of the paper is structured as fol-lows.
Section 2 describes the constituent pruningmethod.
Section 3 describes the grammar special-ization method, focusing on how the current workextends and improves on previous results.
Section 4describes experiments where the constituent prun-ing/grammar specialization method was used on setsof previously unseen speech data.
Section 5 con-cludes and sketches further directions for research,which we are presently in the process of investigat-ing.2 Const i tuent  P run ingBefore both the phrasal and full parsing stages, theconstituent table (henceforth, the chart) is prunedto remove dges that are relatively unlikely to con-tribute to correct analyses.For example, after the string "Show flight D Lthree one two" is lexically analysed, edges for "D"and "L" as individual characters are pruned becauseanother edge, derived from a lexical entry for "DL" as an airline code, is deemed far more plausible.Similarly, edges for "one" as a determiner and asa noun are pruned because, when flanked by twoother numbers, "one" is far more likely to functionas a number.Phrasal parsing then creates a number of newedges, including one for "flight D L three one two" asa noun phrase.
This edge is deemed far more likelyto serve as the basis for a correct full parse thanany of the edges spanning substrings of this phrase;those edges, too, are therefore pruned.
As a result,full parsing is very quick, and only one analysis (thecorrect one) is produced for the sentence.
In the ab-sence of pruning, processing takes over eight timesas long and produces 37 analyses in total.2.1 The prun ing  a lgor i thmOur algorithm estimates the probability of correct-ness of each edge: that is, the probability that theedge will contribute to the correct full analysis of thesentence (assuming there is one), given certain lex-ical and/or syntactic information about it.
Valueson each criterion (selection of pieces of information)are derived from training corpora by maximum like-lihood estimation followed by smoothing.
That is,our estimate for the probability that an edge withproperty P is correct is (modulo smoothing) simplythe number of times edges with property P occur incorrect analyses in training divided by the numberof times such edges are created uring the analysisprocess in training.The current criteria are:?
The left bigram score: the probability of correct-ness of an edge considering only the followingdata about it:- its tag (corresponding to its major categorysymbol plus, for a few categories, ome ad-224ditional distinctions derived from featurevalues);- for a lexical edge, its word or semanticword class (words with similar distribu-tions, such as city names, are grouped intoclasses to overcome data sparseness); or fora phrasal edge, the name of the final (top-most) grammar ule that was used to createit;- the tag of a neighbouring edge immediatelyto its left.
I f  there are several eft neigh-bours, the one giving the highest probabil-ity is used.?
The right bigram score: as above, but consider-ing right neighbours.?
The unigram score: the probability of correct-ness of an edge considering only the tree ofgrammar ules, with words or word classes atthe leaves, that gave rise to it.
For a lexicaledge, this reduces to its word or word class, andits tag.Other criteria, such as trigrams and finer-grainedtags, are obviously worth investigating, and couldbe applied straightforwardly within the frameworkdescribed here.The minimum score derived from any of the crite-ria applied is deemed initially to be the score of theconstituent.
That is, an assumption of full statis-tical dependence (Yarowsky, 1994), rather than themore common full independence, is made3 Whenllf events El, E2,..., E,~ are fully independent, henthe joint probability P(E1 A ... A En) is the product ofP(EI)...P(En), but if they are maximally dependent,it is the minimum of these values.
Of course, neitherassumption is any more than an approximation to thetruth; but assuming dependence has the advantage thatthe estimate of the joint probability depends much lessstrongly on n, and so estimates for alternative jointevents can be directly compared, without any possiblytricky normalization, even if they are composed of dif-ferent numbers of atomic events.
This property is de-sirable: different (sub-)paths through a chart may spandifferent numbers of edges, and one can imagine evalu-ation criteria which are only defined for some kinds ofedge, or which often duplicate information supplied byother criteria.
Taking minima means that the pruning ofan edge results from it scoring poorly on one criterion,regardless of other, possibly good scores assigned to it byother criteria.
This fits in with the fact that on the basisof local information alone it is not usually possibly topredict with confidence that a particular edge is highlylikely to contribute to the correct analysis (since globalfactors will also be important) but it often is possible tospot highly unlikely edges.
In other words, our trainingprocedure yields far more probability estimates close tozero than close to one.recognizer output is being processed, however, theestimate from each criterion is in fact multiplied bya further estimate derived from the acoustic score ofthe edge: that is, the score assigned by the speechrecognizer to the best-scoring sentence hypothesiscontaining the word or word string for the edge inquestion.
Multiplication is used here because acous-tic and lexicosyntactic likelihoods for a word or con-stituent would appear to be more nearly fully inde-pendent han fully dependent, being based on verydifferent kinds of information.Next, account is taken of the connectivity of thechart.
Each vertex of the chart is labelled with thescore of the best path through the chart that vis-its that vertex.
In accordance with the dependenceassumption, the score of a path is defined as the min-imum of the scores of its component edges.
Then thescore of each edge is recalculated to be the minimumof its existing score and the scores of its start andend vertices, on the grounds that a constituent, how-ever intrinsically plausible, is not worth preservingif it does not occur on any plausible paths.Finally, a pruning threshold is calculated as thescore of the best path through the chart multipliedby a certain fraction.
For the first pruning phasewe use 1/20, and for the second, 1/150, althoughperformance is not very sensitive to this.
Any con-stituents coring less than the threshold are prunedout.2.2 Re la t ion  to  o ther  p run ing  methodsAs the example above suggests, judicious pruningof the chart at appropriate points can greatly re-strict the search space and speed up processing.
Ourmethod has points of similarity with some very re-cent work in Constraint Grammar  2 and is an alter-native to several other, related schemes.Firstly, a remarked earlier, it generalizes tagging:it not only adjudicates between possible labels forthe same word, but can also use the existence ofa constituent over one span of the chart as justifi-cation for pruning another constituent over anotherspan, normally a subsumed one, as in the "D L" ex-ample.
This is especially true in the second stage ofpruning, when many constituents of different lengthshave been created.
Furthermore, it applies equallywell to lattices, rather than strings, of words, andcan take account of acoustic plausibility as well assyntactic onsiderations.Secondly, our method is related to beam search(Woods, 1985).
In beam search, incomplete parsesof an utterance are pruned or discarded when, on2Ghrister Samuelsson, personal communication, 8thApril 1996; see (Karlsson et al, 1995) for background.225some criterion, they are significantly less plausi-ble than other, competing parses.
This pruning isfully interleaved with the parsing process.
In con-trast, our pruning takes place only at certain points:currently before parsing begins, and between thephrasM and full parsing stages.
Potentially, as withany generate-and-test algorithm, this can mean effi-ciency is reduced: some paths will be explored thatcould in principle be pruned earlier.
However, asthe results in section 4 below will show, this is notin practice a serious problem, because the secondpruning phase greatly reduces the search space inpreparation for the potentially inefficient full parsingphase.
Our method has the advantage, compared tobeam search, that there is no need for any particu-lar search order to be followed; when pruning takesplace, all constituents hat could have been foundat the stage in question are guaranteed already toexist.Thirdly, our method is a generalization of thestrategy employed by (McCord, 1993).
McCord in-terleaved parsing with pruning in the same way asus, but only compared constituents over the samespan and with the same major category.
Our com-parisons are more global and therefore can result inmore effective pruning.3 Grammar  spec ia l i za t ionAs described in Section 1 above, the non-phrasalgrammar rules are subjected to two phases of pro-cessing.
In the first, "EBL learning" phase, a parsedtraining corpus is used to identify "chunks" of rules,which are combined by the EBL algorithm into sin-gle macro-rules.
In the second phase, the resultingset of "chunked" rules is converted into LR tableform, using the method of (Samuelsson, 1994a).There are two main parameters that can be ad-justed in the EBL learning phase.
Most simply, thereis the size of the training corpus; a larger trainingcorpus means asmaller loss of coverage due to gram-mar specialization.
(Recall that grammar special-ization in general trades coverage for speed).
Sec-ondly, there is the question of how to select he rule-chunks that will be turned into macro-rules.
At onelimit, the whole parse-tree for each training exam-ple is turned into a single rule, resulting in a special-ized grammar all of whose derivations are completely"flat".
These grammars can be parsed extremelyquickly, but the coverage loss is in practice unac-ceptably high, even with very large training corpora.At the opposite extreme, each rule-chunk consistsof a single rule-application; this yields a specializedgrammar identical to the original one.
The challengeis to find an intermediate solution, which specializesthe grammar non-triviMly without losing too muchcoverage.Several attempts to find good "chunking crite-ria" are described in the papers by Rayner andSamuelsson quoted above.
In (Rayner and Samuels-son, 1994), a simple scheme is given, which createsrules corresponding tofour possible units: full utter-ances, recursive NPs, PPs, and non-recursive NPs.A more elaborate scheme is given in (Samuelsson,1994b), where the "chunking criteria" are learnedautomatically b  an entropy-minimization method;the results, however, do not appear to improve onthe earlier ones.
In both cases, the coverage lossdue to grammar specialization was about 10 to 12%using training corpora with about 5,000 examples.In practice, this is still unacceptably high for mostapplications.Our current scheme is an extension of the one from(Rayner and Samuelsson, 1994), where the rule-chunks are trees of non-phrasal rules whose rootsand leaves are categories of the following possibletypes: full utterances, utterance units, imperativeVPs, NPs, relative clauses, VP modifiers and PPs.The resulting specialized grammars are forced to benon-recursive, with derivations being a maximum ofsix levels deep.
This is enforced by imposing thefollowing dominance hierarchy between the possiblecategories:utterance > utterance_unit > imperative_VP> NP > {tel, VP_modifier} > PPThe precise definition of the rule-chunking criteria isquite simple, and is reproduced in the appendix.Note that only the non-phrasal rules are used asinput to the chunks from which the specialized gram-mar rules are constructed.
This has two importantadvantages.
Firstly, since all the phrasal rules areexcluded from the speciMization process, the cov-erage loss associated with missing combinations ofphrasal rules is eliminated.
As the experiments inthe next section show, the resulting improvement isquite substantial.
Secondly, and possibly even moreimportantly, the number of specialized rules pro-duced by a given training corpus is approximatelyhalved.
The most immediate consequence is thatmuch larger training corpora can be used before thespecialized grammars produced become too large tobe handled by the LR table compiler.
If both phrasaland non-phrasal rules are used, we have been unableto compile tables for rules derived from training setsof over 6,000 examples (the process was killed afterrunning for about six hours on a Sun Sparc 20/HS21,SpecINT92=131.2).
Using only non-phrasal rules,compilation of the tables for a 15,000 example train-226ing set required less than two CPU-hours on thesame machine.4 Exper imentsThis section describes a number of experiments car-ried out to test the utility of the theoretical ideaspresented above.
The basic corpus used was a setof 16,000 utterances from the Air Travel Planning(ATIS; (Hemphill et al, 1990)) domain.
All of theseutterances were available in text form; 15,000 ofthem were used for training, with 1,000 held out fortest purposes.
Care was taken to ensure not just thatthe utterances themselves, but also the speakers ofthe utterances were disjoint between test and train-ing data; as pointed out in (Rayner et al, 1994a),failure to observe these precautions can result in sub-stantial spurious improvements in test data results.The 16,000 sentence corpus was analysed by theSRI Core Language Engine (Alshawi (ed), 1992), us-ing a lexicon extended to cover the ATIS domain(Rayner, 1994).
All possible grammatical nalysesof each utterance were recorded, and an interactivetool was used to allow a human judge to identifythe correct and incorrect readings of each utterance.The judge was a first-year undergraduate studentwith a good knowledge of linguistics but no priorexperience with the system; the process of judgingthe corpus took about wo and a half person-months.The input to the EBL-based grammar-specializationprocess was limited to readings of corpus utterancesthat had been judged correct.
When utterances hadmore than one correct reading, apreference heuristicwas used to select he most plausible one.Two sets of experiments were performed.
In thefirst, increasingly arge portions of the training setwere used to train specialized grammars.
The cov-erage loss due to grammar specialization was thenmeasured on the 1,000 utterance test set.
The ex-periment was carried out using both the chunkingcriteria from (Rayner and Samuelsson, 1994) (the"Old" scheme), and the chunking criteria describedin Section 3 above (the "New" scheme).
The resultsare presented in Table 1.The second set of experiments ested more di-rectly the effect of constituent pruning and gram-mar specialization on the Spoken Language Transla-tor's speed and coverage; in particular, coverage wasmeasured on the real task of translating English intoSwedish, rather than the artificial one of producing acorrect QLF analysis.
To this end, the first 500 test-set utterances were presented in the form of speechhypothesis lattices derived by aligning and conflat-ing the top five sentence strings produced by a ver-sion of the DECIPHER (TM) recognizer (MurveitExamples10025050010003000500070001100015000Old schemeRules Loss100 47.8%181 37.6%281 27.6%432 22.7%839 14.9%1101 11.2%1292 10.4%1550 9.8%1819 8.7%New schemeRules Loss69 35.5%126 21.8%180 14.7%249 10.8%455 7.8%585 6.6%668 62%808 5.8%937 5.0%Table 1: EBL rules and EBL coveragenumber of training examplesloss againstet al, 1993).
The lattices were analysed by four dif-ferent versions of the parser, exploring the differentcombinations of turning constituent pruning on oroff, and specialized versus unspecialized grammars.The specialized grammar used the "New" scheme,and had been trained on the full training set.
Ut-terances which took more than 90 CPU seconds toprocess were timed out and counted as failures.The four sets of outputs from the parser were thentranslated into Swedish by the SLT transfer and gen-eration mechanism (Agn~ et al, 1994).
Finally,the four sets of candidate translations were pairwisecompared in the cases where differing translationshad been produced.
We have found this to be aneffective way of evaluating system performance.
Al-though people differ widely in their judgements ofwhether a given translation can be regarded as "ac-ceptable", it is in most cases surprisingly easy tosay which of two possible translations i  preferable.The last two tables summarize the results.
Table 2gives the average processing times per input latticefor each type of processing (times measured run-ning SICStus Prolog 3#3 on a SUN Sparc 20/HS21),showing how the time is divided between the variousprocessing phases.
Table 3 shows the relative scoresof the four parsing variants, measured according tothe "preferable translation" criterion.5 Conc lus ions  and  fu r ther  d i rec t ionsTable 2 indicates that EBL and pruning each makeprocessing about hree times faster; the combinationof both gives a factor of about nine.
In fact, as thedetailed breakdown shows, even this underestimatesthe effect on the main parsing phase: when bothpruning and EBL are operating, processing times forother components (morphology, pruning and prefer-ences) become the dominant ones.
As we have so227E- E-t- E- E+p- p- P?
P+Morph/lex lookup 0.53 0.54 0.54 0.49Phrasal parsing 0.27 0.28 0.14 0.14Pruning - - 0.57 0.56Full parsing 12.42 2.61 3.04 0.26Preferences 3.63 1.57 1.27 0.41TOTALTable 2: Breakdown of average time spent on eachprocessing phase for each type of processing (secondsper utterance)E- E+ E- E-t-P- P- P-t- P+E - /P -  12-24 25-63 24-65E+/P -  24-12 31-50 26-47E- /P+ 63-25 50-31 5-8E+/P+ 65-24 47-26 8-5Table 3: Comparison between translation results onthe four different analysis alternatives, measured onthe 500-utterance t st set.
The entry for a givenrow and column holds two figures, showing respec-tively the number of examples where the "row" vari-ant produced a better translation than the "col-umn" variant and the number where it produced aworse one.
Thus for example "EBL+/pruning+"was better than "EBL-/pruning-" on 65 examples,and worse on 24.far expended little effort on optimizing these phasesof processing, it is reasonable to expect substantialfurther gains to be possible.Even more interestingly, Table 3 shows that realsystem performance, in terms of producing a goodtranslation, is significantly improved by pruning, andis not degraded by grammar specialization.
(Theslight improvement in coverage with EBL on is notstatistically significant).
Our interpretation f theseresults is that the technical loss of grammar cover-age due to the specialization and pruning processesis more than counterbalanced by two positive ffects.Firstly, fewer utterances time out due to slow pro-cessing; secondly, the reduced space of possible anal-yses means that the problem of selecting betweendifferent possible analyses of a given utterance be-comes easier.To sum up, the methods presented here demon-strate that it is possible to use the combined pruningand grammar specialization method to speed up thewhole analysis phase by nearly an order of magni-tude, without incurring any real penalty in the formof reduced coverage.
We find this an exciting andsignificant result, and are further continuing our re-search in this area during the coming year.
In thelast two paragraphs we sketch some ongoing work.All the results presented above pertain to Englishonly.
The first topic we have been investigating isthe application of the methods described here toprocessing of other languages.
Preliminary exper-iments we have carried out on the Swedish versionof the CLE (Gamb~ick and Rayner 1992) have beenencouraging; using exactly the same pruning meth-ods and EBL chunking criteria as for English, weobtain comparable speed-ups.
The loss of coveragedue to grammar specialization also appears compa-rable, though we have not yet had time to do thework needed to verify this properly.
We intend todo so soon, and also to repeat he experiments onthe French version of the CLE (Rayner, Carter andBouillon, 1996).The second topic is a more radical departure, andcan be viewed as an attempt o make interleavingof parsing and pruning the basic principle underly-ing the CLE's linguistic analysis process.
Exploitingthe "stratified" nature of the EBL-specialized gram-mar, we group the chunked rules by level, and applythem one level at a time, starting at the bottom.After each level, constituent pruning is used to elim-inate unlikely constituents.
The intent is to achievea trainable robust parsing model, which can returna useful partial analysis when no single global analy-sis is found.
An initial implementation exists, and iscurrently being tested; preliminary results here arealso very positive.
We expect o be able to reporton this work more fully in the near future.AcknowledgementsThe work reported in this paper was funded byTelia Research AB.
We would like to thank ChristerSamuelsson for making the LR compiler available tous, Martin Keegan for patiently judging the resultsof processing 16,000 ATIS utterances, and Steve Pul-man and Christer Samuelsson for helpful comments.Re ferencesAgn~, M-S., Alshawi, H., Bretan, I., Carter, D.M.Ceder, K., Collins, M., Crouch, R., Digalakis, V.,Ekholm, B., Gamb~ick, B., Kaja, J., Karlgren, J.,Lyberg, B., Price, P., Pulman, S., Rayner, M.,Samuelsson, C. and Svensson, T. 1994.
Spoken228Language Translator: First Year Report.
SRItechnical report CRC-0433Alshawi, H.
(ed.)
1992.
The Core Language Engine.MIT Press.Andry, F., M. Gawron, J. Dowding, and R. Moore.1994.
A Tool for Collecting Domain Depen-dent Sortal Constraints From Corpora.
Proc.COLING-94, Kyoto.Briscoe, Ted, and John Carroll.
1993.
Gener-alized Probabilistic LR Parsing of Natural Lan-guage (Corpora) with Unification-Based Gram-mars.
Computational Linguistics, 19:1, pp.
25-60.Church, Ken.
1988.
A stochastic parts program andnoun phrase parser for unrestricted text.
Proc.1st ANLP, Austin, Tx., pp.
136-143.Cutting, D., J. Kupiec, J. Pedersen and P. Sibun.1992.
A Practical Part-of-Speech Tagger Proc.3rd ANLP, Trento, Italy, pp.
133-140.DeMarcken, C.G.
1990.
Parsing the LOB CorpusProc.
28th ACL, Pittsburgh, Pa., pp.
243-251DeRose, Steven.
1988.
Grammatical Category Dis-ambiguation by Statistical Optimization.
Compu-tational Linguistics 14, pp.
31-39Gamb~ick, Bj6rn, and Manny Rayner.
"The SwedishCore Language Engine".
Proc.
3rd Nordic Con-ference on Text Comprehension in Man and Ma-chine, LinkSping, Sweden.
Also SRI Technical Re-port CRC-025.Grishman, R., N. Nhan,E.
Marsh and L. Hirschmann.
1984.
AutomatedDetermination f Sublanguage Usage.
Proc.
22ndCOLING, Stanford, pp.
96-100.van Harmelen, Frank, and Alan Bundy.
1988.Explanation-Based Generalization = Partial Eval-uation (Research Note) Artificial Intelligence 36,pp.
401-412.Hemphill, C.T., J.J. Godfrey and G.R.
Doddington.1990.
The ATIS Spoken Language Systems pilotcorpus.
Proc.
DARPA Speech and Natural Lan-guage Workshop, Hidden Valley, Pa., pp.
96-101.Karlsson, F., A. Voutilainen, J. Heikkil/i andA.
Anttila (eds).
1995.
Constraint Grammar.Mouton de Gruyer, Berlin, New York.McCord, M. 1993.
Heuristics for Broad-Coverage Natural Language Parsing.
Proc.
1stARPA Workshop on Human Language Technol-ogy, Princeton, NJ.
Morgan Kaufmann.aAll SRI Cambridge technical reports are availablethrough WWW from http://www, cam.
s r i .
cornMitchell, T., R. Keller, and S. Kedar-Cabelli.
1986.Explanation-Based Generalization: a UnifyingView.
Machine Learning 1:1, pp.
47-80.Murveit, H., Butzberger, J., Digalakis, V. and Wein-traub, M. 1993.
Large Vocabulary Dictation us-ing SRI's DECIPHER(TM) Speech RecognitionSystem: Progressive Search Techniques.
Proc.
In-ter.
Conf.
on Acoust., Speech and Signal, Min-neapolis, Mn.Rayner, M. 1988.
Applying Explanation-BasedGeneralization to Natural-Language Processing.Proc.
the International Conference on Fifth Gen-eration Computer Systems, Kyoto, pp.
1267-1274.Rayner, M. 1994.
Overview of English LinguisticCoverage.
In (Agn/is et al, 1994)Rayner, M., Alshawi, H., Bretan, I., Carter, D.M.,Digalakis, V., Gamb/ick, B., Kaja, J., Karlgren, J.,Lyberg, B., Price, P., Pulman, S. and Samuels-son, C. 1993.
A Speech to Speech Translation S~'s-tem Built From Standard Components.
Proc.lstARPA workshop on Human Language Technol-ogy, Princeton, NJ.
Morgan Kaufmann.
Also SRITechnical Report CRC-031.Rayner, M., D. Carter and P. Bouillon.
1996.Adapting the Core Language Engine to Frenchand Spanish.
Proc.
NLP-IA, Moncton, NewBrunswick.
Also SRI Technical Report CRC-061.Rayner, M., D. Carter, V. Digalakis and P. Price.1994.
Combining Knowledge Sources to Re-order N-Best Speech Hypothesis Lists.
Proc.2ndARPA workshop on Human Language Technol-ogy, Princeton, NJ., pp.
217-221.
Morgan Kauf-mann.
Also SRI Technical Report CRC-044.Rayner.
M., and C. Samuelsson.
1990.
UsingExplanation-Based Learning to Increase Perfor-mance in a Large NL Query System.
Proc.DARPA Speech and Natural Language Workshop,June 1990, pp.
251-256.
Morgan Kaufmann.Rayner, M., and C. Samuelsson.
1994.
Corpus-Based Grammar Specialization for Fast Analysis.In (Agn/is et al, 1994)Samuelsson, C. 1994.
Notes on LR Parser Design.Proc.
COLING-94, Kyoto, pp.
386-390.Samuelsson, C. 1994.
Grammar Specializationthrough Entropy Thresholds.
Proc.
ACL-94, LasCruces, NM, pp.
188-195.Samuelsson, C., and M. Rayner.
1991.
QuantitativeEvaluation of Explanation-Based Learning as anOptimization Tool for a Large-Scale Natural Lan-guage System.
Proc.
12th IJCAI, Sydney, pp.
609-615.229Woods, W. 1985.
Language Processing for SpeechUnderstanding.
Computer Speech Processing, W.Woods and F. Fallside (eds), Prentice-Hall Inter-national.Yarowsky, D. 1994.
Decision Lists for Lexical Ambi-guity Resolution.
Proc.
ACL-94, Las Cruces, NM,pp.
88-95.Append ix :  de f in i t ion  o f  the  "New"chunk ing  ru lesThis appendix defines the "New" chunking rules re-ferred to in Sections 3 and 4.
There are seven typesof non-phrasal constituent in the specialised gram-mar.
We start by describing each type of constituentthrough examples.Ut terance :  The top category.Ut terance_un i t :  Utterance_  uni ts  are minimalsyntactic units capable of standing on their own:for example, declarative clauses, questions, NPsand PPs.
Utterances may consist of morethan one ut terance_un i t .
The following is anu t terance  containing two utterance_uni ts :"\[Flights to Boston on Monday\] \[please showme the cheapest ones.\]"Imperat lve_VP:  Since imperative verb phrasesare very common in the corpus, we make thema category of their own in the specialised gram-mar.
To generalise over possible addition ofadverbials (in particular, "please" and "now"),we define the imperative_vp category so as toleave the adverbials outside.
Thus the brack-eted portion of the following utterance is animperative_vp: "That's fine now \[give me thefares for those flights\]"Non_phrasalANP:  All NPs which are not pro-duced entirely by phrasal rules.
The followingare all non_phrasal~Ps: "Boston and Denver","Flights on Sunday morning", "Cheapest farefrom Boston to Denver", "The meal I'd get onthat flight"Reh  Relative clauses.VP.
.modi f ier :  VPs appearing as NP postmodifiers.The bracketed portions of the following areVP_modifiers: "Delta flights \[arriving afterseven P M\] .
.
.
.
All flights tomorrow \[ordered byarrival time\]"PP :  The CLE grammar treats nominal temporaladverbials, sequences of PPs, and "A to B"constructions as PPs (cf (Rayner, 1994)).
Thefollowing are examples of PPs: "Tomorrow af-ternoon", "From Boston to Dallas on Friday","Denver to San Francisco Sunday"We can now present the precise criteria which de-termine the chunks of rules composed to form eachtype of constituent.
For each type of constituent inthe specialised grammar, the chunk is a subtree ex-tracted from the derivation tree of a training exam-ple (cf (Rayner and Samuelsson, 1994)); we specifythe roots and leaves of the relevant subtrees.
Theterm "phrasal tree" will be used to mean a deriva-tion tree all of whose rule-applications are phrasalrules.Ut terance :  The root of the chunk is the root ofthe original tree.
The leaves are the nodes re-suiting from cutting at maximal subtrees forut terance_uni ts ,  non_phrasal_ups pps, andmaximal phrasal subtrees.Ut terance_un i t :  The root is the root of amaximal subtree for a constituent of typeut terance_un i t .
The leaves are the nodes re-sulting from cutting at maximal subtrees forimperative_vps, nps, and pps, and maximalphrasal subtrees.Imperat lve_VP:  The root is the root of a maxi-mal subtree under an application of the S --~VP rule whose root is not an application of anadverbial modification rule.
The leaves are thenodes resulting from cutting at maximal sub-trees for non_phrasal_np, and pp, and maximalphrasal subtrees.Non_phrasa l_NP:  The root is the root of a max-imal non-phrasal subtree for a constituent oftype np.
The leaves are the nodes result-ing from cutting at maximal subtrees for re1,vp_modif ier,  and pp, and maximal phrasalsubtrees.Reh The root is the root of a maximal subtree fora constituent of type re1.
The leaves are thenodes resulting from cutting at maximal sub-trees for pp, and maximal phrasal subtrees.VP.anodi f ier :  The root is the root of a vp subtreeimmediately dominated by an application of theNP --+ NP VP rule.
The leaves are the nodes re-sulting from cutting at maximal subtrees for pp,and maximal phrasal subtrees.PP :  The root is the root of a maximal non-phrasalsubtree for a constituent of type pp.
The leavesare the nodes resulting from cutting at maximalphrasal subtrees.230
