Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 7?12,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsBiases in Predicting the Human Language ModelAlex B. FineUniversity of Illinois at Urbana-Champaignabfine@illinois.eduAustin F. FrankRiot Gamesaufrank@riotgames.comT.
Florian JaegerUniversity of Rochesterfjaeger@bcs.rochester.eduBenjamin Van DurmeJohns Hopkins Universityvandurme@cs.jhu.eduAbstractWe consider the prediction of three hu-man behavioral measures ?
lexical deci-sion, word naming, and picture naming ?through the lens of domain bias in lan-guage modeling.
Contrasting the predic-tive ability of statistics derived from 6 dif-ferent corpora, we find intuitive resultsshowing that, e.g., a British corpus over-predicts the speed with which an Amer-ican will react to the words ward andduke, and that the Google n-grams over-predicts familiarity with technology terms.This study aims to provoke increased con-sideration of the human language modelby NLP practitioners: biases are not lim-ited to differences between corpora (i.e.?train?
vs.
?test?
); they can exist as wellbetween corpora and the intended user ofthe resultant technology.1 IntroductionComputational linguists build statistical languagemodels for aiding in natural language processing(NLP) tasks.
Computational psycholinguists buildsuch models to aid in their study of human lan-guage processing.
Errors in NLP are measuredwith tools like precision and recall, while errors inpsycholinguistics are defined as failures to modela target phenomenon.In the current study, we exploit errors of the lat-ter variety?failure of a language model to predicthuman performance?to investigate bias acrossseveral frequently used corpora in computationallinguistics.
The human data is revealing becauseit trades on the fact that human language process-ing is probability-sensitive: language processingreflects implicit knowledge of probabilities com-puted over linguistic units (e.g., words).
For ex-ample, the amount of time required to read a wordvaries as a function of how predictable that word is(McDonald and Shillcock, 2003).
Thus, failure ofa language model to predict human performancereveals a mismatch between the language modeland the human language model, i.e., bias.Psycholinguists have known for some time thatthe ability of a corpus to explain behavior dependson properties of the corpus and the subjects (cf.Balota et al (2004)).
We extend that line of workby directly analyzing and quantifying this bias,and by linking the results to methodological con-cerns in both NLP and psycholinguistics.Specifically, we predict human data fromthree widely used psycholinguistic experimentalparadigms?lexical decision, word naming, andpicture naming?using unigram frequency esti-mates from Google n-grams (Brants and Franz,2006), Switchboard (Godfrey et al, 1992), spokenand written English portions of CELEX (Baayenet al, 1995), and spoken and written portionsof the British National Corpus (BNC Consor-tium, 2007).
While we find comparable overallfits of the behavioral data from all corpora un-der consideration, our analyses also reveal spe-cific domain biases.
For example, Google n-grams overestimates the ease with which humanswill process words related to the web (tech, code,search, site), while the Switchboard corpus?acollection of informal telephone conversations be-tween strangers?overestimates how quickly hu-mans will react to colloquialisms (heck, darn) andbackchannels (wow, right).7Figure 1: Pairwise correlations between log frequency es-timates from each corpus.
Histograms show distribution overfrequency values from each corpus.
Lower left panels givePearson (top) and Spearman (bottom) correlation coefficientsand associated p-values for each pair.
Upper right panels plotcorrelations2 Fitting Behavioral Data2.1 DataPairwise Pearson correlation coefficients for logfrequency were computed for all corpora underconsideration.
Significant correlations were foundbetween log frequency estimates for all pairs (Fig-ure 1).
Intuitive biases are apparent in the corre-lations, e.g.
: BNCw correlates heavily with BNCs(0.91), but less with SWBD (0.79), while BNCscorrelates more with SWBD (0.84).1Corpus Size (tokens)Google n-grams (web release) ?
1 trillionBritish National Corpus (written, BNCw) ?
90 millionBritish National Corpus (spoken, BNCs) ?
10 millionCELEX (written, CELEXw) ?
16.6 millionCELEX (spoken, CELEXs) ?
1.3 millionSwitchboard (Penn Treebank subset 3) ?
800,000Table 1: Summary of the corpora under consideration.2.2 ApproachWe ask whether domain biases manifest as sys-tematic errors in predicting human behavior.
Logunigram frequency estimates were derived fromeach corpus and used to predict reaction times(RTs) from three experiments employing lexical1BNCw and BNCs are both British, while BNCs andSWBD are both spoken.decision (time required by subjects to correctlyidentify a string of letters as a word of English(Balota et al, 1999)); word naming (time requiredto read aloud a visually presented word (Spielerand Balota, 1997); (Balota and Spieler, 1998));and picture naming (time required to say a pic-ture?s name (Bates et al, 2003)).
Previous workhas shown that more frequent words lead to fasterRTs.
These three measures provide a strong testfor the biases present in these corpora, as theyspan written and spoken lexical comprehensionand production.To compare the predictive strength of log fre-quency estimates from each corpus, we fit mixedeffects regression models to the data from eachexperiment.
As controls, all models included (1)mean log bigram frequency for each word, (2)word category (noun, verb, etc.
), (3) log mor-phological family size (number of inflectional andderivational morphological family members), (4)number of synonyms, and (5) the first principalcomponent of a host of orthographic and phono-logical features capturing neighborhood effects(type and token counts of orthographic and phono-logical neighbors as well as forward and backwardinconsistent words; (Baayen et al, 2006)).
Mod-els of lexical decision and word naming includedrandom intercepts of participant age to adjust fordifferences in mean RTs between old (mean age= 72) vs. young (mean age = 23) subjects, givendifferences between younger vs. older adults?
pro-cessing speed (cf.
(Ramscar et al, 2014)).
(Allparticipants in the picture naming study were col-lege students.
)2.3 ResultsFor each of the six panels corresponding to fre-quency estimates from a corpus A, Figure 2 givesthe ?2value resulting from the log-likelihood ra-tio of (1) a model containing A and an estimatefrom one of the five remaining corpora (given onthe x axis) and (2) a model containing just the cor-pus indicated on the x axis.
Thus, for each panel,each bar in Figure 2 shows the explanatory powerof estimates from the corpus given at the top of thepanel after controlling for estimates from each ofthe other corpora.Model fits reveal intuitive, previously undocu-mented biases in the ability of each corpus to pre-dict human data.
For example, corpora of BritishEnglish tend to explain relatively little after con-8trolling for other British corpora in modeling lexi-cal decision RTs (yellow).
Similarly, Switchboardprovides relatively little explanatory power overthe other corpora in predicting picture namingRTs (blue bars), possibly because highly image-able nouns and verbs frequent in everyday interac-tions are underrepresented in telephone conversa-tions between people with no common visual ex-perience.
In other words, idiosyncratic facts aboutthe topics, dialects, etc.
represented in each cor-pus lead to systematic patterns in how well eachcorpus can predict human data relative to the oth-ers.
In some cases, the predictive value of onecorpus after controlling for another?apparentlyfor reasons related to genre, dialect?can be quitelarge (cf.
the ?2difference between a model withboth Google and Switchboard frequency estimatescompared to one with only Switchboard [top rightyellow bar]).In addition to comparing the overall predictivepower of the corpora, we examined the wordsfor which behavioral predictions derived from thecorpora deviated most from the observed behav-ior (word frequencies strongly over- or under-estimated by each corpora).
First, in Table 2 wegive the ten words with the greatest relative differ-ence in frequency for each corpus pair.
For exam-ple, fife is deemed more frequent according to theBNC than to Google.2These results suggest that particular corporamay be genre-biased in systematic ways.
For in-stance, Google appears to be biased towards termi-nology dealing with adult material and technology.Similarly, BNCw is biased, relative to Google, to-wards Britishisms.
For these words in the BNCand Google, we examined errors in predicted lexi-cal decision times.
Figure 3 plots errors in the lin-ear model?s prediction of RTs for older (top) andyounger (bottom) subjects.The figure shows a positive correlation betweenhow large the difference is between the lexical de-cision RT predicted by the model and the actu-ally observed RT, and how over-estimated the logfrequency of that word is in the BNC relative toGoogle (left panel) or in Google relative to theBNC (right panel).
The left panel shows that BNCproduces a much greater estimate of the log fre-2Surprisingly, fife was determined to be one of the wordswith the largest frequency asymmetry between Switchboardand the Google n-grams corpus.
This was a result of lower-casing all of the words in in the analyses, and the fact thatBarney Fife was mentioned several times in the BNC.quency of the word lee relative to Google, whichleads the model to predict a lower RT for this wordthan is observed (i.e., the error is positive; thoughnote that the error is less severe for older relative toyounger subjects).
By contrast, the asymmetry be-tween the two corpora in the estimated frequencyof sir is less severe, so the observed RT deviatesless from the predicted RT.
In the right panel, wesee that Google assigns a much greater estimateof log frequency to the word tech than the BNC,which leads a model predicting RTs from Google-derived frequency estimates to predict a far lowerRT for this word than observed.3 DiscussionResearchers in computational linguistics often as-sume that more data is always better than lessdata (Banko and Brill, 2001).
This is true in-sofar as larger corpora allow computational lin-guists to generate less noisy estimates of the av-erage language experience of the users of compu-tational linguistics applications.
However, corpussize does not necessarily eliminate certain types ofbiases in estimates of human linguistic experience,as demonstrated in Figure 3.Our analyses reveal that 6 commonly used cor-pora fail to reflect the human language model invarious ways related to dialect, modality, and otherproperties of each corpus.
Our results point toa type of bias in commonly used language mod-els that has been previously overlooked.
This biasmay limit the effectiveness of NLP algorithms in-tended to generalize to a linguistic domains whosestatistical properties are generated by humans.For psycholinguists these results support an im-portant methodological point: while each corpuspresents systematic biases in how well it predictshuman behavior, all six corpora are, on the whole,of comparable predictive value and, specifically,the results suggest that the web performs as wellas traditional instruments in predicting behavior.This has two implications for psycholinguistic re-search.
First, as argued by researchers such asLew (2009), given the size of the Web compared toother corpora, research focusing on low-frequencylinguistic events?or requiring knowledge of thedistributional characteristics of varied contexts?is now more tractable.
Second, the viability ofthe web in predicting behavior opens up possibil-ities for computational psycholinguistic researchin languages for which no corpora exist (i.e., most9CELEX written BNC written GoogleCELEX spoken BNC spoken Switchboard04080120010203040010203001020300102030400510CELEXwrittenBNC writtenGoogleCELEXspokenBNC spokenSwitchboardCELEXwrittenBNC writtenGoogleCELEXspokenBNC spokenSwitchboardCELEXwrittenBNC writtenGoogleCELEXspokenBNC spokenSwitchboardComparison?
?2tasklexical decisionpicture namingword namingPairwise model comparisonsFigure 2: Results of log likelihood ratio model comparisons.
Large values indicate that the reference predictor (panel title)explained a large amount of variance over and above the predictor given on the x-axis.Google and BNC writtenStandardized difference scoreErrorin linearmodelcent damedoledukefifeglengodgulf hallhankkingleelord marchnickprimeprincesir wardcent damedoledukefifeglengodgulfhallhank kingleelord marchnickprimeprincesir wardassbinbugbutt cartchat clickcodedarndendialdikefileflip gayheckhop hunklink logmailmap pagepeeprepprintquoteranchscriptsearchselfsexsiteskipslotstoresucktagtechteensthreadtiretoetwainwebwhizwowzipassbinbugbuttcartchat clickcodedarndendialdikefileflipgayheckhop hunklink log mailmap pagepeepr pprint quoteranchscriptsearchselfsexsiteskipslotstore sucktagtechteensthreadtiretoetwain webwhizwowzip-0.10.00.10.20.30.4-0.10.00.10.20.30.4oldyoung-3.5 -3.0 -2.5 2.5 3.0 3.5 4.0 4.5 5.0 5.5Google < BNC written Google > BNC writtengoog.f-4-202Figure 3: Errors in the linear model predicting lexical decision RTs from log frequency are plotted against the standardizeddifference in log frequency in the Google n-grams corpus versus the written portion of the BNC.
Top and bottom panels showerrors for older and younger subjects, respectively.
The left panel plots words with much greater frequency in the writtenportion of the BNC relative to Google; the right panel plots words occurring more frequently in Google.
Errors in the linearmodel are plotted against the standardized difference in log frequency across the corpora, and word color encodes the degree towhich each word is more (red) or less (blue) frequent in Google.
That the fit line in each graph is above 0 in the y-axis meansthat on average these biased words in each domain are being over-predicted, i.e., the corpus frequencies suggest humans willreact (sometimes much) faster than they actually did in the lab.10Greater Lesser Top-10google bnc.s web, ass, gay, tire, text, tool, code, woe, site, zipgoogle bnc.w ass, teens, tech, gay, bug, suck, site, cart, log, searchgoogle celex.s teens, cart, gay, zip, mail, bin, tech, click, pee, sitegoogle celex.w web, full, gay, bin, mail, zip, site, sake, ass, loggoogle swbd gay, thread, text, search, site, link, teens, seek, post, sexbnc.w google fife, lord, duke, march, dole, god, cent, nick, dame, draughtbnc.w bnc.s pact, corps, foe, tract, hike, ridge, dine, crest, aide, whimbnc.w celex.s staff, nick, full, waist, ham, lap, knit, sheer, bail, marchbnc.w celex.w staff, lord, last, nick, fair, glen, low, march, should, westbnc.w swbd rose, prince, seek, cent, text, clause, keen, breach, soul, risecelex.s google art, yes, pound, spoke, think, mean, say, thing, go, drovecelex.s bnc.s art, hike, pact, howl, ski, corps, peer, spoke, jazz, arecelex.s bnc.w art, yes, dike, think, thing, sort, mean, write, pound, lotcelex.s celex.w yes, sort, thank, think, jazz, heck, tape, well, fife, getcelex.s swbd art, cell, rose, spoke, aim, seek, shall, seed, text, knightcelex.w google art, plod, pound, shake, spoke, dine, howl, sit, say, draughtcelex.w bnc.s hunch, stare, strife, hike, woe, aide, rout, yell, glaze, fleecelex.w bnc.w dike, whiz, dine, shake, grind, jerk, whoop, say, are, cramcelex.w celex.s wrist, pill, lawn, clutch, stare, spray, jar, shark, plead, horncelex.w swbd art, rose, seek, aim, rise, burst, seed, cheek, grin, lipswbd google mow, kind, lot, think, fife, corps, right, cook, sort, doswbd bnc.s creek, mow, guess, pact, strife, tract, hank, howl, foe, napswbd bnc.w stuff, whiz, tech, lot, kind, creek, darn, dike, bet, kidswbd celex.s wow, sauce, mall, deck, full, spray, flute, rib, guy, bunchswbd celex.w heck, guess, right, full, stuff, lot, last, well, guy, fairTable 2: Examples of words with largest difference in z-transformed log frequencies (e.g., the relative frequencies of fife,lord, and duke, in the BNC are far greater than in Google).languages).
This furthers the arguments of the ?theweb as corpus?
community (Kilgarriff and Grefen-stette, 2003) with respect to psycholinguistics.Finally, combining multiple sources of fre-quency estimates is one way researchers may beable to reduce the prediction bias from any sin-gle corpus.
This relates to work in automaticallybuilding domain specific corpora (e.g., Moore andLewis (2010), Axelrod et al (2011), Daum?e IIIand Jagarlamudi (2011), Wang et al (2014), Gaoet al (2002), and Lin et al (1997)).
Those effortsfocus on building representative document collec-tions for a target domain, usually based on a seedset of initial documents.
Our results prompt thequestion: can one use human behavior as the tar-get in the construction of such a corpus?
Con-cretely, can we build corpora by optimizing an ob-jective measure that minimizes error in predictinghuman reaction times?
Prior work in building bal-anced corpora used either rough estimates of theratio of genre styles a normal human is exposed todaily (e.g., the Brown corpus (Kucera and Fran-cis, 1967)), or simply sampled text evenly acrossgenres (e.g., COCA: the Corpus of ContemporaryAmerican English (Davies, 2009)).
Just as lan-guage models have been used to predict readinggrade-level of documents (Collins-Thompson andCallan, 2004), human language models could beused to predict the appropriateness of a documentfor inclusion in an ?automatically balanced?
cor-pus.4 ConclusionWe have shown intuitive, domain-specific biasesin the prediction of human behavioral measuresvia corpora of various genres.
While some psy-cholinguists have previously acknowledged thatdifferent corpora carry different predictive power,this is the first work to our knowledge to system-atically document these biases across a range ofcorpora, and to relate these predictive errors to do-main bias, a pressing issue in the NLP community.With these results in hand, future work may nowconsider the automatic construction of a ?prop-erly?
balanced text collection, such as originallydesired by the creators of the Brown corpus.AcknowledgmentsThe authors wish to thank three anonymous ACLreviewers for helpful feedback.
This researchwas supported by a DARPA award (FA8750-13-2-0017) and NSF grant IIS-0916599 to BVD, NSFIIS-1150028 CAREER Award and Alfred P. SloanFellowship to TFJ, and an NSF Graduate ResearchFellowship to ABF.11ReferencesA.
Axelrod, X.
He, and J. Gao.
2011.
Domain adap-tation via pseudo in-domain data selection.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP 11).R.
H. Baayen, R. Piepenbrock, and L. Gulikers.
1995.The CELEX Lexical Database (Release 2).
Linguis-tic Data Consortium, Philadelphia.R.
H. Baayen, L. F. Feldman, and R. Schreuder.2006.
Morphological influences on the recognitionof monosyllabic monomorphemic words.
Journal ofMemory and Language, 53:496?512.D.
A. Balota and D. H. Spieler.
1998.
The utility ofitem-level analyses in model evaluation: A reply toSeidenberg & Plaut (1998).
Psychological Science.D.
A. Balota, M. J. Cortese, and M. Pilotti.
1999.
Item-level analyses of lexical decision performance: Re-sults from a mega-study.
In Abstracts of the 40th An-nual Meeting of the Psychonomics Society, page 44.D.
Balota, M. Cortese, S. Sergent-Marshall, D. Spieler,and M. Yap.
2004.
Visual word recognition forsingle-syllable words.
Journal of Experimental Psy-chology:General, (133):283316.M.
Banko and E. Brill.
2001.
Mitigating the paucity ofdata problem.
Human Language Technology.E.
Bates, S. D?Amico, T. Jacobsen, A. Szkely, E. An-donova, A. Devescovi, D. Herron, CC Lu, T. Pech-mann, C. Plh, N. Wicha, K. Federmeier, I. Gerd-jikova, G. Gutierrez, D. Hung, J. Hsu, G. Iyer,K.
Kohnert, T. Mehotcheva, A. Orozco-Figueroa,A.
Tzeng, and O. Tzeng.
2003.
Timed picture nam-ing in seven languages.
Psychonomic Bulletin & Re-view, 10(2):344?380.BNC Consortium.
2007.
The British National Corpus,version 3 (BNC XML Edition).
Distributed by Ox-ford University Computing Services on behalf of theBNC Consortium.T.
Brants and A. Franz.
2006.
Web 1T 5-gram Version1.
Linguistic Data Consortium (LDC).Kevyn Collins-Thompson and James P. Callan.
2004.A language modeling approach to predicting readingdifficulty.
In HLT-NAACL, pages 193?200.H.
Daum?e III and J. Jagarlamudi.
2011.
Domainadaptation for machine translation by mining unseenwords.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies (ACL-HLT 11).M.
Davies.
2009.
The 385+ million word corpus ofcontemporary american english (19902008+): De-sign, architecture, and linguistic insights.
Inter-national Journal of Corpus Linguistics, 14(2):159?190.J.
Gao, J. Goodman, M. Li, and K. F. Lee.
2002.
To-ward a unified approach to statistical language mod-eling for chinese.
In Proceedings of the ACM Trans-actions on Asian Language Information Processing(TALIP 02).J.
Godfrey, E. Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone Speech Corpus forResearch and Development.
In Proceedings ofICASSP-92, pages 517?520.A.
Kilgarriff and G. Grefenstette.
2003.
Introductionto the special issue on the web as corpus.
Computa-tional Linguistics, 29(3):333?348.H.
Kucera and W.N.
Francis.
1967.
Computationalanalysis of present-day american english.
provi-dence, ri: Brown university press.R.
Lew, 2009.
Contemporary Corpus Linguistics,chapter The Web as corpus versus traditional cor-pora: Their relative utility for linguists and languagelearners, pages 289?300.
London/New York: Con-tinuum.S.
C. Lin, C. L. Tsai, L. F. Chien, K. J. Chen, andL.
S. Lee.
1997.
Chinese language model adapta-tion based on document classification and multipledomain-specific language models.
In Proceedingsof the 5th European Conference on Speech Commu-nication and Technology.S.A.
McDonald and R.C.
Shillcock.
2003.
Eyemovements reveal the on-line computation of lexicalprobabilities during reading.
Psychological science,14(6):648?52, November.R.
C. Moore and W. Lewis.
2010.
Intelligent selectionof language model training data.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics (ACL 10).M.
Ramscar, P. Hendrix, C. Shaoul, P. Milin, and R. H.Baayen.
2014.
The myth of cognitive decline: non-linear dynamics of lifelong learning.
Topics in Cog-nitive Science, 32:5?42.D.
H. Spieler and D. A. Balota.
1997.
Bringing com-putational models of word naming down to the itemlevel.
6:411?416.L.
Wang, D.F.
Wong, L.S.
Chao, Y. Lu, and J. Xing.2014.
A systematic comparison of data selectioncriteria for smt domain adaptation.
The ScientificWorld Journal.12
