Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1066?1076,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsMovie Script Summarization as Graph-based Scene ExtractionPhilip John Gorinski and Mirella LapataInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh10 Crichton Street, Edinburgh EH8 9ABP.J.Gorinski@sms.ed.ac.uk, mlap@inf.ed.ac.ukAbstractIn this paper we study the task of moviescript summarization, which we argue couldenhance script browsing, give readers a roughidea of the script?s plotline, and speed up read-ing time.
We formalize the process of gen-erating a shorter version of a screenplay asthe task of finding an optimal chain of scenes.We develop a graph-based model that selectsa chain by jointly optimizing its logical pro-gression, diversity, and importance.
Humanevaluation based on a question-answering taskshows that our model produces summarieswhich are more informative compared to com-petitive baselines.1 IntroductionEach year, about 50,000 screenplays are registeredwith the WGA1, the Writers Guild of America.
Onlya fraction of these make it through to be consideredfor production and an even smaller fraction to thebig screen.
How do producers and directors navigatethrough this vast number of scripts available?
Typ-ically, production companies, agencies, and studioshire script readers, whose job is to analyze screen-plays that come in, sorting the hopeful from thehopeless.
Having read the script, a reader will gen-erate a coverage report consisting of a logline (oneor two sentences describing the story in a nutshell),a synopsis (a two- to three-page long summary ofthe script), comments explaining its appeal or prob-lematic aspects, and a final verdict as to whether thescript merits further consideration.
A script excerpt1The WGA is a collective term representing US TV and filmwriters.We can?t get a good glimpse of his face, buthis body is plump, above average height; heis in his mid 30?s.
Together they easilylift the chair into the truck.MAN (O.S.
)Let?s slide it up, you mind?CUT TO:INT.
THE PANEL TRUCK - NIGHTHe climbs inside the truck, ducking under asmall hand winch, and grabs the chair.
Shehesitates again, but climbs in after him.MANAre you about a size 14?CATHERINE(surprised)What?Suddenly, in the shadowy dark, he clubs herover the back of her head with his cast.Figure 1: Excerpt from ?The Silence of the Lambs?.The scene heading INT.
THE PANEL TRUCK - NIGHTdenotes that the action takes place inside the panel truckat night.
Character cues (e.g., MAN, CATHERINE) prefacethe lines the actors speak.
Action lines describe what thecamera sees (e.g., We can?t get a good glimpse ofhis face, but his body.
.
.
).from ?Silence of the Lambs?, an American thrillerreleased in 1991, is shown in Figure 1.Although there are several screenwriting tools forauthors (e.g., Final Draft is a popular applicationwhich automatically formats scripts to industry stan-dards, keeps track of revisions, allows insertion ofnotes, and writing collaboratively online), there is alack of any kind of script reading aids.
Features ofsuch a tool could be to automatically grade the qual-ity of the script (e.g., thumbs up or down), generate1066synopses and loglines, identify main characters andtheir stories, or facilitate browsing (e.g., ?show meevery scene where there is a shooting?).
In this pa-per we explore whether current NLP technology canbe used to address some of these tasks.
Specifically,we focus on script summarization, which we con-ceptualize as the process of generating a shorter ver-sion of a screenplay, ideally encapsulating its mostinformative scenes.
The resulting summaries canbe used to enhance script browsing, give readers arough idea of the script?s content and plotline, andspeed up reading time.So, what makes a good script summary?
Accord-ing to modern film theory, ?all films are about noth-ing ?
nothing but character?
(Monaco, 1982).
Be-yond characters, a summary should also highlightmajor scenes representative of the story and its pro-gression.
With this in mind, we define a script sum-mary as a chain of scenes which conveys a narrativeand smooth transitions from one scene to the next.At the same time, a good chain should incorporatesome diversity (i.e., avoid redundancy), and focuson important scenes and characters.
We formalizethe problem of selecting a good summary chain us-ing a graph-theoretic approach.
We represent scriptsas (directed) bipartite graphs with vertices corre-sponding to scenes and characters, and edge weightsto their strength of correlation.
Intuitively, if twoscenes are connected, a random walk starting fromone would reach the other frequently.
We find achain of highly connected scenes by jointly optimiz-ing logical progression, diversity, and importance.Our contributions in this work are three-fold: weintroduce a novel summarization task, on a new textgenre, and formalize scene selection as the problemof finding a chain that represents a film?s story; wepropose several novel methods for analyzing scriptcontent (e.g., identifying important characters andtheir interactions); and perform a large-scale humanevaluation study using a question-answering task.Experimental results show that our method producessummaries which are more informative compared toseveral competitive baselines.2 Related WorkComputer-assisted analysis of literary text has a longhistory, with the first studies dating back to the1960s (Mosteller and Wallace, 1964).
More re-cently, the availability of large collections of dig-itized books and works of fiction has enabled re-searchers to observe cultural trends, address ques-tions about language use and its evolution, studyhow individuals rise to and fall from fame, performgender studies, and so on (Michel et al, 2010).
Mostexisting work focuses on low-level analysis of wordpatterns, with a few notable exceptions.
Elson et al(2010) analyze 19th century British novels by con-structing a conversational network with vertices cor-responding to characters and weighted edges corre-sponding to the amount of conversational interac-tion.
Elsner (2012) analyzes characters and theiremotional trajectories, whereas Nalisnick and Baird(2013) identify a character?s enemies and allies inplays based on the sentiment of their utterances.Other work (Bamman et al, 2013, 2014) automat-ically infers latent character types (e.g., villains orheroes) in novels and movie plot summaries.Although we are not aware of any previous ap-proaches to summarize screenplays, the field ofcomputer vision is rife with attempts to summa-rize video (see Reed 2004 for an overview).
Mosttechniques are based on visual information and relyon low-level cues such as motion, color, or audio(e.g., Rasheed et al 2005).
Movie summarization isa special type of video summarization which posesmany challenges due to the large variety of filmstyles and genres.
A few recent studies (Weng et al,2009; Lin et al, 2013) have used concepts from so-cial network analysis to identify lead roles and rolecommunities in order to segment movies into scenes(containing one or more shots) and create more in-formative summaries.
A surprising fact about thisline of work is that it does not exploit the moviescript in any way.
Characters are typically identifiedusing face recognition techniques and scene bound-aries are presumed unknown and are automaticallydetected.
A notable exception are Sang and Xu(2010) who generate video summaries for movies,while taking into account character interaction fea-tures which they estimate from the correspondingscreenplay.Our own approach is inspired by work in ego-centric video analysis.
An egocentric video offersa first-person view of the world and is captured froma wearable camera focusing on the user?s activities,1067# Movies AvgLines AvgScenes AvgCharsDrama 665 4484.53 79.77 60.94Thriller 451 4333.10 91.84 52.59Comedy 378 4303.02 66.13 57.51Action 288 4255.56 101.82 59.99Figure 2: ScriptBase corpus statistics.
Movies can havemultiple genres, thus numbers do not add up to 1,276.social interactions, and interests.
Lu and Grauman(2013) present a summarization model which ex-tracts subshot sequences while finding a balance ofimportant subshots that are both diverse and providea natural progression through the video, in terms ofprominent visual objects (e.g., bottle, mug, televi-sion).
We adapt their technique to our task, and showhow to estimate character-scene correlations basedon linguistic analysis.
We also interpret moviesas social networks and extract a rich set of fea-tures from character interactions and their sentimentwhich we use to guide the summarization process.3 ScriptBase: A Movie Script CorpusWe compiled ScriptBase, a collection of1,276 movie scripts, by automatically crawlingweb-sites which host or link entire movie scripts(e.g., imsdb.com).
The retrieved scripts were thencross-matched against Wikipedia2and IMDB3andpaired with corresponding user-written summaries,plot sections, loglines and taglines (taglines areshort snippets used by marketing departmentsto promote a movie).
We also collected meta-information regarding the movie?s genre, its actors,the production year, etc.
ScriptBase contains moviescomprising 23 genres; each movie is on averageaccompanied by 3 user summaries, 3 loglines, and3 taglines.
The corpus spans years 1909?2013.Some corpus statistics are shown in Figure 2.The scripts were further post-processed with theStanford CoreNLP pipeline (Manning et al, 2014)to perform tagging, parsing, named entity recogni-tion and coreference resolution.
They were also an-notated with semantic roles (e.g., ARG0, ARG1),using the MATE tools (Bj?orkelund et al, 2009).Our summarization experiments focused on come-dies and thrillers.
We randomly selected 30 movies2http://en.wikipedia.org3http://www.imdb.com/s1 s2 s3 s4 s5 s6 s7...s1 s2 s3 s4 s5 s6 s7...//Figure 3: Example of consecutive chain (top).
Squaresrepresent scenes in a screenplay.
The bottom chain wouldnot be allowed, since the connection between s3 and s5makes it non-consecutive.for training/development and 65 movies for testing.4 The Scene Extraction ModelAs mentioned earlier, we define script summariza-tion as the task of selecting a chain of scenes rep-resenting the movie?s most important content.
Weinterpret the term scene in the screenplay sense.
Ascene is a unit of action that takes place in one loca-tion at one time (see Figure 1).
We therefore neednot be concerned with scene segmentation; sceneboundaries are clearly marked, and constitute the ba-sic units over which our model operates.Let M = (S,C) represent a screenplay consist-ing of a set S = {s1,s2, .
.
.
,sn} of scenes, and a setC = {c1, .
.
.
,cm} of characters.
We are interested infinding a list S?= {si, .
.
.sk} of ordered, consecutivescenes subject to a compression rate m (see the ex-ample in Figure 3).
A natural interpretation of m inour case is the percentage of scenes from the orig-inal script retained in the summary.
The extractedchain should contain (a) important scenes (i.e., crit-ical for comprehending the story and its develop-ment); (b) diverse scenes that cover different as-pects of the story; and (c) scenes which highlightthe story?s progression from beginning to end.
Wetherefore find the chain S?maximizing the objectivefunction Q(S?)
which is the weighted sum of threeterms: the story progression P, scene diversity D,and scene importance I:S?= argmaxS??SQ(S?)
(1)Q(S?)
= ?PP(S?)+?DD(S?)+?II(S?)
(2)In the following, we define each of the three terms.1068scene 1 scene 2 scene 3 scene 4char 1 char 2 char 3 char 40.30.10.20.040.10.050.070.010.110.20.040.30.070.2Figure 4: Example of a bipartite graph, connecting amovie?s scenes with participating characters.Scene-to-scene Progression The first term in theobjective is responsible for selecting chains repre-senting a logically coherent story.
Intuitively, thismeans that if our chain includes a scene where acharacter commits an action, then scenes involvingaffected parties or follow-up actions should also beincluded.
We operationalize this idea of progressionin a story in terms of how strongly the characters ina selected scene siinfluence the transition to the nextscene si+1:P(S?)
=|S?|?1?i=0?c?CiINF(si,si+1|c) (3)We represent screenplays as weighted, bipartitegraphs connecting scenes and characters:B = (V,E) : V =C?SE = {(s,c,ws,c)|s ?
S, c ?C, ws,c?
[0,1]}?
{(c,s,wc,s)|c ?C, s ?
S, wc,s?
[0,1]}The set of vertices V corresponds to the union ofcharacters C and scenes S. We therefore add tothe bipartite graph one node per scene and onenode per character, and two directed edges for eachscene-character and character-scene pair.
An exam-ple of a bipartite graph is shown in Figure 4.
Wefurther assume that two scenes siand si+1are tightlyconnected in such a graph if a random walk withrestart (RWR; Tong et al 2006; Kim et al 2014)which starts in sihas a high probability of endingin si+1.In order to calculate the random walk stationarydistributions, we must estimate the weights betweena character and a scene.
We are interested in howimportant a character is generally in the movie, andspecifically in a particular scene.
For wc,s, we con-sider the probability of a character being important,i.e., of them belonging to the set of main characters:wc,s= P(c ?
main(M)), ?
(c,s,wc,s) ?
E (4)where P(c ?main(M)) is some probability score as-sociated with c being a main character in script M.For ws,c, we take the number of interactions a char-acter is involved in relative to the total number ofinteractions in a specific scene as indicative of thecharacter?s importance in that scene.
Interactions re-fer to conversational interactions as well as relationsbetween characters (e.g., who does what to whom):ws,c=?c??Csinter(c,c?
)?c1,c2?Csinter(c1,c2), ?
(s,c,ws,c) ?
E (5)We defer discussion of how we model probabil-ity P(c ?Main(M)) and obtain interaction counts toSection 5.
Weights ws,cand wc,sare normalized:ws,c=ws,c?
(s,c?,w?s,c)w?s,c, ?
(s,c,ws,c) ?
E (6)wc,s=wc,s?
(c,s?,w?c,s)w?c,s, ?
(c,s,wc,s) ?
E (7)We calculate the stationary distributions of a ran-dom walk on a transition matrix T , enumerating overall vertices v (i.e., characters and scenes) in the bi-partite graph B:T (i, j) ={wi, jif (vi,vj,wi, j?
EB)0 otherwise(8)We measure the influence individual characters haveon scene-to-scene transitions as follows.
The sta-tionary distribution rkfor a RWR walker starting atnode k is a vector that satisfies:rk= (1?
?
)Trk+ ?ek(9)where T is the transition matrix of the graph, ekis aseed vector, with all elements 0, except for element kwhich is set to 1, and ?
is a restart probability param-eter.
In practice, our vectors rkand ekare indexed bythe scenes and characters in a movie, i.e., they havelength |S|+ |C|, and their nthelement correspondseither to a known scene or character.
In cases where1069graphs are relatively small, we can compute r di-rectly4by solving:rk= ?(I?
(1?
?
)T )?1ek(10)The lth element of r then equals the probability ofthe random walker being in state l in the stationarydistribution.
Let rckbe the same as rk, but with thecharacter node c of the bipartite graph being turnedinto a sink, i.e., all entries for c in the transitionmatrix T are 0.
We can then define how a singlecharacter influences the transition between scenes siand si+1as:INF(si,si+1|c) = rsi[si+1]?
rcsi[si+1] (11)where rsi[si+1] is shorthand for that element in thevector rsithat corresponds to scene si+1.
We usethe INF score directly in Equation (3) to determinethe progress score of a candidate chain.Diversity The diversity term D(S?)
in our objec-tive should encourage chains which consist of moredissimilar scenes, thereby avoiding redundancy.
Thediversity of chain S?is the sum of the diversities ofits successive scenes:D(S?)
=|S?|?1?i=1d(si,si+1) (12)The diversity d(si,si+1) of two scenes siand si+1is estimated taking into account two factors: (a) dothey have any characters in common, and (b) doesthe sentiment change from one scene to the next:d(si,si+1) =dchar(si,si+1)+dsen(si,si+1)2(13)where dchar(si,si+1) and dsen(si,si+1) respectivelydenote character and sentiment similarity betweenscenes.
Specifically, dchar(si,si+1) is the relativecharacter overlap between scenes siand si+1:dchar(si,si+1) = 1?|Csi?Csi+1||Csi?Csi+1|(14)dcharwill be 0 if two scenes share the same charac-ters and 1 if no characters are shared.
Analogously,4We could also solve for r recursively which would bepreferable for large graphs, since the performed matrix inver-sion is computationally expensive.we define dsen, the sentiment overlap between twoscenes as:dsen(si,si+1) =1?k ?di f (si,si+1)k?
k ?di f (si,si+1)+1(15)di f (si,si+1) =11+ |sen(si)?
sen(si+1)|(16)where the sentiment sen(s) of scene s is the aggre-gate sentiment score of all interactions in s:sen(s) =?c,c??Cssen(inter(c,c?))
(17)We explain how interactions and their sentiment arecomputed in Section 5.
Again, dsenis larger if twoscenes have a less similar sentiment.
di f (si,si+1)becomes 1 if the sentiments are identical, andincreasingly smaller for more dissimilar senti-ments.
The sigmoid-like function in Equation (15)scales dsenwithin range [0,1] to take smaller valuesfor larger sentiment differences (factor k adjusts thecurve?s smoothness).Importance The score I(S?)
captures whethera chain contains important scenes.
We defineI(S?)
as the sum of all scene-specific importancescores imp(si) of scenes contained in the chain:I(S?)
=|S?|?i=1imp(si) (18)The importance imp(si) of a scene siis the ratio oflead to support characters within that scene:imp(si) =?c: c?Csi?c?main(M)1?c: c?Csi1(19)where Csiis the set of characters present in scene si,and main(M) is the set of main characters in themovie.5I(si) is 0 if a scene does not contain anymain characters, and 1 if it contains only main char-acters (see Section 5 for how main(M) is inferred).Optimal Chain Selection We use Linear Pro-gramming to efficiently find a good chain.
The ob-jective is to maximize Equation (2), i.e., the sumof the terms for progress, diversity and importance,5Whether scenes are important if they contain many maincharacters is an empirical question in its own right.
For ourpurposes, we assume that this relation holds.1070subject to their weights ?.
We add a constraint corre-sponding to the compression rate, i.e., the number ofscenes to be selected and enforce their linear orderby disallowing non-consecutive combinations.
Weuse GLPK6to solve the linear problem.5 ImplementationIn this section we discuss several aspects of the im-plementation of the model presented in the previoussection.
We explain how interactions are extractedand how sentiment is calculated.
We also present ourmethod for identifying main characters and estimat-ing the weights ws,cand wc,sin the bipartite graph.Interactions The notion of interaction underliesmany aspects of the model defined in the previoussection.
For instance, interaction counts are requiredto estimate the weights ws,cin the bipartite graph ofthe progression term (see Equation (5)), and in defin-ing diversity (see Equations (15)?(17)).
As we shallsee below, interactions are also important for identi-fying main characters in a screenplay.We use the term interaction to refer to conversa-tions between two characters, as well as their rela-tions (e.g., if a character kills another).
For con-versational interactions, we simply need to iden-tify the speaker generating an utterance and the lis-tener.
Speaker attribution comes for free in ourcase, as speakers are clearly marked in the text (seeFigure 1).
Listener identification is more involved,especially when there are multiple characters in ascene.
We rely on a few simple heuristics.
We as-sume that the previous speaker in the same scene,who is different from the current speaker, is the lis-tener.
If there is no previous speaker, we assumethat the listener is the closest character mentioned inthe speaker?s utterance (e.g., via a coreferring propername or a pronoun).
In cases where we cannot finda suitable listener, we assume the current speaker isthe listener.We obtain character relations from the output ofa semantic role labeler.
Relations are denoted byverbs whose ARG0 and ARG1 roles are charac-ter names.
We extract relations from the dialoguebut also from scene descriptions.
For example,in Figure 1 the description Suddenly, [...] he6https://www.gnu.org/software/glpk/clubs her over the head contains the relationclubs(MAN,CATHERINE).
Pronouns are resolved totheir antecedent using the Stanford coreference res-olution system (Lee et al, 2011).Sentiment We labeled lexical items in screenplayswith sentiment values using the AFINN-96 lexi-con (Nielsen, 2011), which is essentially a list ofwords scored with sentiment strength within therange [?5,+5].
The list also contains obscene words(which are often used in movies) and some Internetslang.
By summing over the sentiment scores of in-dividual words, we can work out the sentiment of aninteraction between two characters, the sentiment ofa scene (see Equation (17)), and even the sentimentbetween characters (e.g., who likes or dislikes whomin the movie in general).Main Characters The progress term in our sum-marization objective crucially relies on charactersand their importance (see the weight wc,sin Equa-tion (4)).
Previous work (Weng et al, 2009; Linet al, 2013) extracts social networks where nodescorrespond to roles in the movie, and edges to theirco-occurrence.
Leading roles (and their communi-ties) are then identified by measuring their centralityin the network (i.e., number of edges terminating ina given node).It is relatively straightforward to obtain a so-cial network from a screenplay.
Formally, for eachmovie we define a weighted and undirected graph:G = {C,E}, : C = {c1, .
.
.cn},E = {(ci,cj,w)|ci,cj?C, w ?
N>0}where vertices correspond to movie characters7,and edges denote character-to-character interac-tions.
Figure 5 shows an example of a social net-work for ?The Silence of the Lambs?.
Due to lackof space, only main characters are displayed, how-ever the actual graph contains all characters (42 inthis case).
Importantly, edge weights are not nor-malized, but directly reflect the strength of associa-tion between different characters.We do not solely rely on the social net-work to identify main characters.
We esti-mate P(c ?
main(M)), the probability of c being aleading character in movie M, using a Multi Layer7We assume one node per speaking role in the script.1071Mr.
GumbCatherineDr.
LecterChiltonClarice CrawfordSen.
Martin4805313316 37716454424113139Figure 5: Social network for ?The Silence of the Lambs?
;edge weights correspond to absolute number of interac-tions between nodes.Perceptron (MLP) and several features pertaining tothe structure of the social network and the script textitself.
A potential stumbling block in treating char-acter identification as a classification task is obtain-ing training data, i.e., a list of main characters foreach movie.
We generate a gold-standard by assum-ing that the characters listed under Wikipedia?s Castsection (or an equivalent section, e.g., Characters)are the main characters in the movie.Examples of the features we used for the clas-sification task include the barycenter of a charac-ter (i.e., the sum of its distance to all other charac-ters), PageRank (Page et al, 1999), an eigenvector-based centrality measure, absolute/relative interac-tion weight (the sum of all interactions a character isinvolved in, divided by the sum of all interactions inthe network), absolute/relative number of sentencesuttered by a character, number of times a charac-ter is described by other characters (e.g., He is amonster or She is nice), number of times a char-acter talks about other characters, and type-token-ratio of sentences uttered by the character (i.e., rateof unique words in a character?s speech).
Usingthese features, the MLP achieves an F1 of 79.0% onthe test set.
It outperforms other classification meth-ods such as Naive Bayes or logistic regression.
Us-ing the full-feature set, the MLP also obtains perfor-mance superior to any individual measure of graphconnectivity.Aside from Equation (4), lead characters also ap-pear in Equation (19), which determines scene im-portance.
We assume a character c ?
main(M) if itis predicted by the MLP with a probability ?
0.5.6 Experimental SetupGold Standard Chains The development andtuning of the chain extraction model presented inSection 4 necessitates access to a gold standard ofkey scene chains representing the movie?s most im-portant content.
Our experiments concentrated on asample of 95 movies (comedies and thrillers) fromthe ScriptBase corpus (Section 3).
Performing thescene selection task for such a big corpus manuallywould be both time consuming and costly.
Instead,we used distant supervision based on Wikipedia toautomatically generate a gold standard.Specifically, we assume that Wikipedia plots arerepresentative of the most important content in amovie.
Using the alignment algorithm presentedin Nelken and Shieber (2006), we align script sen-tences to Wikipedia plot sentences and assume thatscenes with at least one alignment are part of thegold chain of scenes.
We obtain many-to-manyalignments using features such as lemma overlapand word stem similarity.
When evaluated on fourmovies8(from the training set) whose content wasmanually aligned to Wikipedia plots, the alignerachieved a precision of .53 at a recall rate of .82 atdeciding whether a scene should be aligned.
Scenesare ranked according to the number of alignmentsthey contain.
When creating gold chains at differ-ent compression rates, we start with the best-rankedscenes and then successively add lower ranked onesuntil we reach the desired compression rate.System Comparison In our experiments we com-pared our scene extraction model (SceneSum)against three baselines.
The first baseline was basedon the minimum overlap (MinOv) of characters inconsecutive scenes and corresponds closely to thediversity term in our objective.
The second base-line was based on the maximum overlap (MaxOv) ofcharacters and approximates the importance term inour objective.
The third baseline selects scenes atrandom (averaged over 1,000 runs).
Parameters forour models were tuned on the training set, weightsfor the terms in the objective were optimized to thefollowing values: ?P= 1.0, ?D= 0.3, and ?I= 0.1.We set the restart probability of our random walker8?Cars 2?, ?Shrek?, ?Swordfish?, and ?The Silence of theLambs?.10721.
Why does Trevor leave New York and where doeshe move to?2.
What is KOS, who is their leader, and why is heattending high school?3.
What happened to Cesar?s finger, how did heeventually die?4.
Who killed Benny and how does Ellen find out?5.
Who is Rita and what becomes of her?Table 1: Questions for the movie ?One Eight Seven?.to ?
= 0.5, and the sigmoid scaling factor in our di-versity term to k =?1.2.Evaluation We assessed the output of our model(and comparison systems) automatically against thegold chains described above.
We performed ex-periments with compression rates in the range of10% to 50% and measured performance in termsof F1.
In addition, we also evaluated the quality ofthe extracted scenes as perceived by humans, whichis necessary, given the approximate nature of ourgold standard.
We adopted a question-answering(Q&A) evaluation paradigm which has been usedpreviously to evaluate summaries and documentcompression (Morris et al, 1992; Mani et al, 2002;Clarke and Lapata, 2010).
Under the assumptionthat the summary is to function as a replacement forthe full script, we can measure the extent to whichit can be used to find answers to questions whichhave been derived from the entire script and are rep-resentative of its core content.
The more questionsa hypothetical system can answer, the better it is atsummarizing the script as a whole.Two annotators were independently instructed toread scripts (from our test set) and create Q&A pairs.The annotators generated questions relating to theplot of the movie and the development of its charac-ters, requiring an unambiguous answer.
They com-pared and revised their Q&A pairs until a commonagreed-upon set of five questions per movie wasreached (see Table 1 for an example).
In addition,for every movie we asked subjects to name the maincharacters, and summarize its plot (in no more thanfour sentences).
Using Amazon Mechanical Turk(AMT)9, we elicited answers for eight scripts (fourcomedies and thrillers) in four summarization con-9https://www.mturk.com/10% 20% 30% 40% 50%MaxOv 0.40 0.50 0.58 0.64 0.71MinOv 0.13 0.27 0.40 0.53 0.66SceneSum 0.23 0.37 0.50 0.60 0.68Random 0.10 0.20 0.30 0.40 0.50Table 2: Model performance on automatically generatedgold standard (test set) at different compression rates.ditions: using our model, the two baselines basedon minimum and maximum character overlap, andthe random system.
All models were assessed at thesame compression rate of 20% which seems realis-tic in an actual application environment, e.g., com-puter aided summarization.
The scripts were prese-lected in an earlier AMT study where participantswere asked to declare whether they had seen themovies in our test set (65 in total).
We chose thescreenplays which had received the least viewingsso as to avoid eliciting answers based on familiar-ity with the movie.
A total of 29 participants, allself-reported native English speakers, completed theQ&A task.
The answers provided by the subjectswere scored against an answer key.
A correct an-swer was marked with a score of one, and zero oth-erwise.
In cases where more answers were requiredper question, partial scores were awarded to eachcorrect answer (e.g., 0.5).
The score for a summaryis the average of its question scores.7 ResultsTable 2 shows the performance of SceneSum, ourscene extraction model, and the three comparisonsystems (MaxOv, MinOv, Random) on the auto-matic gold standard at five compression rates.
Ascan be seen, MaxOv performs best in terms of F1,followed by SceneSum.
We believe this is an ar-tifact due to the way the gold standard was cre-ated.
Scenes with large numbers of main charac-ters are more likely to figure in Wikipedia plot sum-maries and will thus be more frequently aligned.
Achain based on maximum character overlap will fo-cus on such scenes and will agree with the gold stan-dard better compared to chains which take additionalscript properties into account.We further analyzed the scenes selected by Sce-neSum and the comparison systems with respect totheir position in the script.
Table 3 shows the av-1073Beginning Middle EndMaxOv 33.95 34.89 31.16MinOv 34.30 33.91 31.80SceneSum 35.30 33.54 31.16Random 34.30 33.91 31.80Table 3: Average percentage of scenes taken from thebeginning, middle and ends of movies, on automatic goldstandard test set.erage percentage of scenes selected from the be-ginning, middle, and end of the movie (based onan equal division of the number of scenes in thescreenplay).
As can be seen, the number of se-lected scenes tends to be evenly distributed acrossthe entire movie.
SceneSum has a slight bias to-wards the beginning of the movie which is probablynatural, since leading characters appear early on, aswell as important scenes introducing essential storyelements (e.g., setting, points of view).The results of our human evaluation study aresummarized in Table 4.
We observe that SceneSumsummaries are overall more informative comparedto those created by the baselines.
In other words,AMT participants are able to answer more ques-tions regarding the story of the movie when readingSceneSum summaries.
In two instances (?A Night-mare on Elm Street 3?
and ?Mumford?
), the over-lap models score better, however, in this case themovies largely consist of scenes with the same char-acters and relatively little variation (?A Nightmareon Elm Street 3?
), or the camera follows the mainlead in his interactions with other characters (?Mum-ford?).
Since our model is not so character-centric,it might be thrown off by non-character-based termsin its objective, leading to the selection of unfavor-able scenes.
Table 4 also presents a break down ofthe different types of questions answered by our par-ticipants.
Again, we see that in most cases a largerpercentage is answered correctly when reading Sce-neSum summaries.Overall, we observe that SceneSum extractschains which encapsulate important movie contentacross the board.
We should point out that al-though our movies are broadly classified as come-dies and thrillers, they have very different structureand content.
For example, ?Little Athens?
has avery loose plotline, ?Living in Oblivion?
has multi-Movies MaxOv MinOv SceneSum RandomNightmare 3 69.18 74.49 60.24 56.33Little Athens 34.92 31.75 36.90 33.33Living in Oblivion 40.95 35.00 60.00 30.24Mumford 72.86 60.00 30.00 54.29One Eight Seven 47.30 38.89 67.86 30.16Anniversary Party 45.39 56.35 62.46 37.62We Own the Night 28.57 32.14 52.86 28.57While She Was Out 72.86 75.71 85.00 45.71All Questions 51.51 50.54 56.91 39.53Five Questions 51.00 53.13 57.38 36.88Plot Question 60.00 56.88 73.75 55.00Characters Question 45.54 37.34 37.75 31.29Table 4: Percentage of questions answered correctly.ple dream sequences, whereas ?While She was Out?contains only a few characters and a series of im-portant scenes towards the end.
Despite this variety,SceneSum performs consistently better in our task-based evaluation.8 ConclusionsIn this paper we have developed a graph-basedmodel for script summarization.
We formalizedthe process of generating a shorter version of ascreenplay as the task of finding an optimal chainof scenes, which are diverse, important, and ex-hibit logical progression.
A large-scale evaluationbased on a question-answering task revealed that ourmethod produces more informative summaries com-pared to several baselines.
In the future, we planto explore model performance in a wider range ofmovie genres as well as its applicability to otherNLP tasks (e.g., book summarization or event ex-traction).
We would also like to automatically deter-mine the compression rate which should presumablyvary according to the movie?s length and content.
Fi-nally, our long-term goal is to be able to generateloglines as well as movie plot summaries.Acknowledgments We would like to thank RikSarkar, Jon Oberlander and Annie Louis for theirvaluable feedback.
Special thanks to Bharat Am-bati, Lea Frermann, and Daniel Renshaw for theirhelp with system evaluation.ReferencesBamman, David, Brendan O?Connor, and Noah A.Smith.
2013.
Learning Latent Personas of1074Film Characters.
In Proceedings of the 51stAnnual Meeting of the Association for Compu-tational Linguistics.
Sofia, Bulgaria, pages 352?361.Bamman, David, Ted Underwood, and A. NoahSmith.
2014.
A Bayesian Mixed Effects Modelof Literary Character.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics.
Baltimore, MD, USA, pages370?379.Bj?orkelund, Anders, Love Hafdell, and PierreNugues.
2009.
Multilingual Semantic Role La-beling.
In Proceedings of the 13th Conferenceon Computational Natural Language Learning:Shared Task.
Boulder, Colorado, pages 43?48.Clarke, James and Mirella Lapata.
2010.
DiscourseConstraints for Document Compression.
Compu-tational Linguistics 36(3):411?441.Elsner, Micha.
2012.
Character-based kernels fornovelistic plot structure.
In Proceedings of the13th Conference of the European Chapter of theAssociation for Computational Linguistics.
Avi-gnon, France, pages 634?644.Elson, David K., Nicholas Dames, and Kathleen R.McKeown.
2010.
Extracting Social Networksfrom Literary Fiction.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics.
Uppsala, Sweden, pages 138?147.Kim, Jun-Seong, Jae-Young Sim, and Chang-SuKim.
2014.
Multiscale Saliency Detection Us-ing Random Walk With Restart.
IEEE Transac-tions on Circuits and Systems for Video Technol-ogy 24(2):198?210.Lee, Heeyoung, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and DanJurafsky.
2011.
Stanford?s Multi-Pass SieveCoreference Resolution System at the CoNLL-2011 Shared Task.
In Proceedings of the 15thConference on Computational Natural LanguageLearning: Shared Task.
Portland, OR, USA,pages 28?34.Lin, C., C. Tsai, L. Kang, and Weisi Lin.
2013.Scene-Based Movie Summarization via Role-Community Networks.
IEEE Transactionson Circuits and Systems for Video Technology23(11):1927?1940.Lu, Zheng and Kristen Grauman.
2013.
Story-Driven Summarization for Egocentric Video.
InProceedings of the 2013 IEEE Conference onComputer Vision and Pattern Recognition.
Port-land, OR, USA, pages 2714?2721.Mani, Inderjeet, Gary Klein, David House, LynetteHirschman, Therese Firmin, and Beth Sundheim.2002.
SUMMAC: A Text Summarization Evalua-tion.
Natural Language Engineering 8(1):43?68.Manning, Christopher, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP NaturalLanguage Processing Toolkit.
In Proceedings of52nd Annual Meeting of the Association for Com-putational Linguistics: System Demonstrations.pages 55?60.Michel, Jean-Baptiste, Yuan Kui Shen, avivaPresser Aiden, Adrian Veres, Matthew K. Gray,The Google Books Team, Joseph P. Pickett, DaleHoiberg, Dan Clancy, Peter Norvig, Jon Orwant,Steven Pinker, Martin A. Nowak, and Erez Liber-man Aiden.
2010.
Quantitative Analysis of Cul-ture Using Millions of Digitized Books.
Science331(6014):176?182.Monaco, James.
1982.
How to Read a Film: TheArt, Technology, Language, History and Theory ofFilm and Media.
OUP, New York, NY, USA.Morris, A., G. Kasper, and D. Adams.
1992.The Effects and Limitations of Automated TextCondensing on Reading Comprehension Perfor-mance.
Information Systems Research 3(1):17?35.Mosteller, Frederick and David Wallace.
1964.
In-ference and Dispituted Authorship: The Federal-ists.
Addison-Wesley, Boston, MA, USA.Nalisnick, T. Eric and S. Henry Baird.
2013.Character-to-Character Sentiment Analysis inShakespeare?s Plays.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics.
Sofia, Bulgaria, pages 479?483.Nelken, Rani and Stuart Shieber.
2006.
TowardsRobust Context-Sensitive Sentence Alignment forMonolingual Corpora.
In Proceedings of the 11thConference of the European Chapter of the As-sociation for Computational Linguistics.
Trento,Italy, pages 161?168.1075Nielsen, Finn Arup.
2011.
A new ANEW: Eval-uation of a word list for sentiment analysis inmicroblogs.
In Proceedings of the ESWC2011Workshop on ?Making Sense of Microposts?
: BigThings Come in Small Packages.
Heraklion,Crete, pages 93?98.Page, Lawrence, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The pagerank citationranking: Bringing order to the web.
Technical Re-port 1999-66, Stanford InfoLab.
Previous numberSIDL-WP-1999-0120.Rasheed, Z., Y. Sheikh, and M. Shah.
2005.
On theUse of Computable Features for Film Classifica-tion.
IEEE Transactions on Circuits and Systemsfor Video Technology 15(1):52?64.Reed, Todd, editor.
2004.
Digital Image SequenceProcessing.
Taylor & Francis.Sang, Jitao and Changsheng Xu.
2010.
Character-based Movie Summarization.
In Proceedingsof the International Conference on Multimedia.Firenze, Italy, pages 855?858.Tong, Hanghang, Christos Faloutsos, and Jia-YuPan.
2006.
Fast Random Walk with Restart andIts Applications.
In Proceedings of the Sixth In-ternational Conference on Data Mining.
HongKong, pages 613?622.Weng, Chung-yi, Wei-Ta Chu, and Ja ling Wu.
2009.Rolenet: Movie Analysis from the perspective ofSocial Networks.
IEEE Transactions on Multime-dia 11(2):256?271.1076
