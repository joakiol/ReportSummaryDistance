Proceedings of the ACL 2010 Conference Short Papers, pages 86?91,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsTackling Sparse Data Issue in Machine Translation Evaluation ?Ondr?ej Bojar, Kamil Kos, and David Marec?ekCharles University in Prague, Institute of Formal and Applied Linguistics{bojar,marecek}@ufal.mff.cuni.cz, kamilkos@email.czAbstractWe illustrate and explain problems ofn-grams-based machine translation (MT)metrics (e.g.
BLEU) when applied tomorphologically rich languages such asCzech.
A novel metric SemPOS basedon the deep-syntactic representation of thesentence tackles the issue and retains theperformance for translation to English aswell.1 IntroductionAutomatic metrics of machine translation (MT)quality are vital for research progress at a fastpace.
Many automatic metrics of MT quality havebeen proposed and evaluated in terms of correla-tion with human judgments while various tech-niques of manual judging are being examined aswell, see e.g.
MetricsMATR08 (Przybocki et al,2008)1, WMT08 and WMT09 (Callison-Burch etal., 2008; Callison-Burch et al, 2009)2.The contribution of this paper is twofold.
Sec-tion 2 illustrates and explains severe problems of awidely used BLEU metric (Papineni et al, 2002)when applied to Czech as a representative of lan-guages with rich morphology.
We see this as aninstance of the sparse data problem well knownfor MT itself: too much detail in the formal repre-sentation leading to low coverage of e.g.
a transla-tion dictionary.
In MT evaluation, too much detailleads to the lack of comparable parts of the hy-pothesis and the reference.?
This work has been supported by the grants EuroMa-trixPlus (FP7-ICT-2007-3-231720 of the EU and 7E09003of the Czech Republic), FP7-ICT-2009-4-247762 (Faust),GA201/09/H057, GAUK 1163/2010, and MSM 0021620838.We are grateful to the anonymous reviewers for further re-search suggestions.1http://nist.gov/speech/tests/metricsmatr/2008/results/2http://www.statmt.org/wmt08 and wmt090.06 0.08 0.10 0.12 0.140.40.6 bcu-bojarbgooglebuedinbeurotranxpbpctransbcu-tectomtBLEURankFigure 1: BLEU and human ranks of systems par-ticipating in the English-to-Czech WMT09 sharedtask.Section 3 introduces and evaluates some newvariations of SemPOS (Kos and Bojar, 2009), ametric based on the deep syntactic representationof the sentence performing very well for Czech asthe target language.
Aside from including depen-dency and n-gram relations in the scoring, we alsoapply and evaluate SemPOS for English.2 Problems of BLEUBLEU (Papineni et al, 2002) is an establishedlanguage-independent MT metric.
Its correlationto human judgments was originally deemed high(for English) but better correlating metrics (esp.for other languages) were found later, usually em-ploying language-specific tools, see e.g.
Przy-bocki et al (2008) or Callison-Burch et al (2009).The unbeaten advantage of BLEU is its simplicity.Figure 1 illustrates a very low correlation to hu-man judgments when translating to Czech.
Weplot the official BLEU score against the rank es-tablished as the percentage of sentences where asystem ranked no worse than all its competitors(Callison-Burch et al, 2009).
The systems devel-oped at Charles University (cu-) are described inBojar et al (2009), uedin is a vanilla configurationof Moses (Koehn et al, 2007) and the remainingones are commercial MT systems.In a manual analysis, we identified the reasonsfor the low correlation: BLEU is overly sensitiveto sequences and forms in the hypothesis matching86Con- Errorfirmed Flags 1-grams 2-grams 3-grams 4-gramsYes Yes 6.34% 1.58% 0.55% 0.29%Yes No 36.93% 13.68% 5.87% 2.69%No Yes 22.33% 41.83% 54.64% 63.88%No No 34.40% 42.91% 38.94% 33.14%Total n-grams 35,531 33,891 32,251 30,611Table 1: n-grams confirmed by the reference andcontaining error flags.the reference translation.
This focus goes directlyagainst the properties of Czech: relatively freeword order allows many permutations of wordsand rich morphology renders many valid wordforms not confirmed by the reference.3 Theseproblems are to some extent mitigated if severalreference translations are available, but this is of-ten not the case.Figure 2 illustrates the problem of ?sparse data?in the reference.
Due to the lexical and morpho-logical variance of Czech, only a single word ineach hypothesis matches a word in the reference.In the case of pctrans, the match is even a falsepositive, ?do?
(to) is a preposition that should beused for the ?minus?
phrase and not for the ?endof the day?
phrase.
In terms of BLEU, both hy-potheses are equally poor but 90% of their tokenswere not evaluated.Table 1 estimates the overall magnitude of thisissue: For 1-grams to 4-grams in 1640 instances(different MT outputs and different annotators) of200 sentences with manually flagged errors4, wecount how often the n-gram is confirmed by thereference and how often it contains an error flag.The suspicious cases are n-grams confirmed bythe reference but still containing a flag (false posi-tives) and n-grams not confirmed despite contain-ing no error flag (false negatives).Fortunately, there are relatively few false posi-tives in n-gram based metrics: 6.3% of unigramsand far fewer higher n-grams.The issue of false negatives is more serious andconfirms the problem of sparse data if only onereference is available.
30 to 40% of n-grams donot contain any error and yet they are not con-3Condon et al (2009) identify similar issues when eval-uating translation to Arabic and employ rule-based normal-ization of MT output to improve the correlation.
It is beyondthe scope of this paper to describe the rather different natureof morphological richness in Czech, Arabic and also otherlanguages, e.g.
German or Finnish.4The dataset with manually flagged errors is available athttp://ufal.mff.cuni.cz/euromatrixplus/firmed by the reference.
This amounts to 34% ofrunning unigrams, giving enough space to differ inhuman judgments and still remain unscored.Figure 3 documents the issue across languages:the lower the BLEU score itself (i.e.
fewer con-firmed n-grams), the lower the correlation to hu-man judgments regardless of the target language(WMT09 shared task, 2025 sentences per lan-guage).Figure 4 illustrates the overestimation of scorescaused by too much attention to sequences of to-kens.
A phrase-based system like Moses (cu-bojar) can sometimes produce a long sequence oftokens exactly as required by the reference, lead-ing to a high BLEU score.
The framed wordsin the illustration are not confirmed by the refer-ence, but the actual error in these words is verysevere for comprehension: nouns were used twiceinstead of finite verbs, and a misleading transla-tion of a preposition was chosen.
The output bypctrans preserves the meaning much better despitenot scoring in either of the finite verbs and produc-ing far shorter confirmed sequences.3 Extensions of SemPOSSemPOS (Kos and Bojar, 2009) is inspired by met-rics based on overlapping of linguistic features inthe reference and in the translation (Gime?nez andMa?rquez, 2007).
It operates on so-called ?tec-togrammatical?
(deep syntactic) representation ofthe sentence (Sgall et al, 1986; Hajic?
et al, 2006),formally a dependency tree that includes only au-tosemantic (content-bearing) words.5 SemPOS asdefined in Kos and Bojar (2009) disregards thesyntactic structure and uses the semantic part ofspeech of the words (noun, verb, etc.).
There are19 fine-grained parts of speech.
For each semanticpart of speech t, the overlapping O(t) is set to zeroif the part of speech does not occur in the referenceor the candidate set and otherwise it is computedas given in Equation 1 below.5We use TectoMT (Z?abokrtsky?
and Bojar, 2008),http://ufal.mff.cuni.cz/tectomt/, for the lin-guistic pre-processing.
While both our implementation ofSemPOS as well as TectoMT are in principle freely avail-able, a stable public version has yet to be released.
Our plansinclude experiments with approximating the deep syntacticanalysis with a simple tagger, which would also decrease theinstallation burden and computation costs, at the expense ofaccuracy.87SRC Prague Stock Market falls to minus by the end of the trading dayREF praz?ska?
burza se ke konci obchodova?n??
propadla do minusucu-bojar praha stock market klesne k minus na konci obchodn?
?ho dnepctrans praha trh cenny?ch pap??ru?
pada?
minus do konce obchodn?
?ho dneFigure 2: Sparse data in BLEU evaluation: Large chunks of hypotheses are not compared at all.
Only asingle unigram in each hypothesis is confirmed in the reference.-0.2 00.20.40.60.8 1  0.050.10.150.20.250.3CorrelationBLEUscorecs-ende-enes-enfr-enhu-enen-csen-deen-esen-frFigure 3: BLEU correlates with its correlation to human judgments.
BLEU scores around 0.1 predictlittle about translation quality.O(t) =?i?I?w?ri?cimin(cnt(w, t, ri), cnt(w, t, ci))?i?I?w?ri?cimax(cnt(w, t, ri), cnt(w, t, ci))(1)The semantic part of speech is denoted t; ciand ri are the candidate and reference translationsof sentence i, and cnt(w, t, rc) is the number ofwordsw with type t in rc (the reference or the can-didate).
The matching is performed on the level oflemmas, i.e.
no morphological information is pre-served in ws.
See Figure 5 for an example; thesentence is the same as in Figure 4.The final SemPOS score is obtained by macro-averaging over all parts of speech:SemPOS =1|T |?t?TO(t) (2)where T is the set of all possible semantic partsof speech types.
(The degenerate case of blankcandidate and reference has SemPOS zero.
)3.1 Variations of SemPOSThis section describes our modifications of Sem-POS.
All methods are evaluated in Section 3.2.Different Classification of AutosemanticWords.
SemPOS uses semantic parts of speechto classify autosemantic words.
The tectogram-matical layer offers also a feature called Functordescribing the relation of a word to its governorsimilarly as semantic roles do.
There are 67functor types in total.Using Functor instead of SemPOS increases thenumber of word classes that independently requirea high overlap.
For a contrast we also completelyremove the classification and use only one globalclass (Void).Deep Syntactic Relations in SemPOS.
InSemPOS, an autosemantic word of a class is con-firmed if its lemma matches the reference.
We uti-lize the dependency relations at the tectogrammat-ical layer to validate valence by refining the over-lap and requiring also the lemma of 1) the parent(denoted ?par?
), or 2) all the children regardless oftheir order (denoted ?sons?)
to match.Combining BLEU and SemPOS.
One of themajor drawbacks of SemPOS is that it completelyignores word order.
This is too coarse even forlanguages with relatively free word order likeCzech.
Another issue is that it operates on lemmasand it completely disregards correct word forms.Thus, a weighted linear combination of SemPOSand BLEU (computed on the surface representa-tion of the sentence) should compensate for this.For the purposes of the combination, we computeBLEU only on unigrams up to fourgrams (denotedBLEU1, .
.
.
, BLEU4) but including the brevitypenalty as usual.
Here we try only a few weightsettings in the linear combination but given a held-out dataset, one could optimize the weights for thebest performance.88SRC Congress yields: US government can pump 700 billion dollars into banksREF kongres ustoupil : vla?da usa mu?z?e do bank napumpovat 700 miliard dolaru?cu-bojar kongres vy?nosy : vla?da usa mu?z?e c?erpadlo 700 miliard dolaru?
v banka?chpctrans kongres vyna?s???
: us vla?da mu?z?e c?erpat 700 miliardu dolaru?
do bankFigure 4: Too much focus on sequences in BLEU: pctrans?
output is better but does not score well.BLEU gave credit to cu-bojar for 1, 3, 5 and 8 fourgrams, trigrams, bigrams and unigrams, resp., butonly for 0, 0, 1 and 8 n-grams produced by pctrans.
Confirmed sequences of tokens are underlined andimportant errors (not considered by BLEU) are framed.REF kongres/n ustoupit/v :/n vla?da/n usa/n banka/n napumpovat/v 700/n miliarda/n dolar/ncu-bojar kongres/n vy?nos/n :/n vla?da/n usa/n moci/v c?erpadlo/n 700/n miliarda/n dolar/n banka/npctrans kongres/n vyna?s?et/v :/n us/n vla?da/n c?erpat/v 700/n miliarda/n dolar/n banka/nFigure 5: SemPOS evaluates the overlap of lemmas of autosemantic words given their semantic part ofspeech (n, v, .
.
.
).
Underlined words are confirmed by the reference.SemPOS for English.
The tectogrammaticallayer is being adapted for English (Cinkova?
et al,2004; Hajic?
et al, 2009) and we are able to use theavailable tools to obtain all SemPOS features forEnglish sentences as well.3.2 Evaluation of SemPOS and FriendsWe measured the metric performance on data usedin MetricsMATR08, WMT09 and WMT08.
Forthe evaluation of metric correlation with humanjudgments at the system level, we used the Pearsoncorrelation coefficient ?
applied to ranks.
In caseof a tie, the systems were assigned the average po-sition.
For example if three systems achieved thesame highest score (thus occupying the positions1, 2 and 3 when sorted by score), each of themwould obtain the average rank of 2 = 1+2+33 .When correlating ranks (instead of exact scores)and with this handling of ties, the Pearson coeffi-cient is equivalent to Spearman?s rank correlationcoefficient.The MetricsMATR08 human judgments includepreferences for pairs of MT systems saying whichone of the two systems is better, while the WMT08and WMT09 data contain system scores (for up to5 systems) on the scale 1 to 5 for a given sentence.We assigned a human ranking to the systems basedon the percent of time that their translations werejudged to be better than or equal to the translationsof any other system in the manual evaluation.
Weconverted automatic metric scores to ranks.Metrics?
performance for translation to Englishand Czech was measured on the following test-sets (the number of human judgments for a givensource language in brackets):To English: MetricsMATR08 (cn+ar: 1652),WMT08 News Articles (de: 199, fr: 251),WMT08 Europarl (es: 190, fr: 183), WMT09(cz: 320, de: 749, es: 484, fr: 786, hu: 287)To Czech: WMT08 News Articles (en: 267),WMT08 Commentary (en: 243), WMT09(en: 1425)The MetricsMATR08 testset contained 4 refer-ence translations for each sentence whereas the re-maining testsets only one reference.Correlation coefficients for English are shownin Table 2.
The best metric is Voidpar closely fol-lowed by Voidsons.
The explanation is that Voidcompared to SemPOS or Functor does not losepoints by an erroneous assignment of the POS orthe functor, and that Voidpar profits from check-ing the dependency relations between autoseman-tic words.
The combination of BLEU and Sem-POS6 outperforms both individual metrics, but incase of SemPOS only by a minimal difference.Additionally, we confirm that 4-grams alone havelittle discriminative power both when used as ametric of their own (BLEU4) as well as in a lin-ear combination with SemPOS.The best metric for Czech (see Table 3) is a lin-ear combination of SemPOS and 4-gram BLEUclosely followed by other SemPOS and BLEUncombinations.
We assume this is because BLEU4can capture correctly translated fixed phrases,which is positively reflected in human judgments.Including BLEU1 in the combination favors trans-lations with word forms as expected by the refer-6For each n ?
{1, 2, 3, 4}, we show only the best weightsetting for SemPOS and BLEUn.89Metric Avg Best WorstVoidpar 0.75 0.89 0.60Voidsons 0.75 0.90 0.54Void 0.72 0.91 0.59Functorsons 0.72 1.00 0.43GTM 0.71 0.90 0.544?SemPOS+1?BLEU2 0.70 0.93 0.43SemPOSpar 0.70 0.93 0.301?SemPOS+4?BLEU3 0.70 0.91 0.264?SemPOS+1?BLEU1 0.69 0.93 0.43NIST 0.69 0.90 0.53SemPOSsons 0.69 0.94 0.40SemPOS 0.69 0.95 0.302?SemPOS+1?BLEU4 0.68 0.91 0.09BLEU1 0.68 0.87 0.43BLEU2 0.68 0.90 0.26BLEU3 0.66 0.90 0.14BLEU 0.66 0.91 0.20TER 0.63 0.87 0.29PER 0.63 0.88 0.32BLEU4 0.61 0.90 -0.31Functorpar 0.57 0.83 -0.03Functor 0.55 0.82 -0.09Table 2: Average, best and worst system-level cor-relation coefficients for translation to English fromvarious source languages evaluated on 10 differenttestsets.ence, thus allowing to spot bad word forms.
Inall cases, the linear combination puts more weighton SemPOS.
Given the negligible difference be-tween SemPOS alone and the linear combinations,we see that word forms are not the major issue forhumans interpreting the translation?most likelybecause the systems so far often make more im-portant errors.
This is also confirmed by the obser-vation that using BLEU alone is rather unreliablefor Czech and BLEU-1 (which judges unigramsonly) is even worse.
Surprisingly BLEU-2 per-formed better than any other n-grams for reasonsthat have yet to be examined.
The error metricsPER and TER showed the lowest correlation withhuman judgments for translation to Czech.4 ConclusionThis paper documented problems of single-reference BLEU when applied to morphologicallyrich languages such as Czech.
BLEU suffers froma sparse data problem, unable to judge the qualityof tokens not confirmed by the reference.
This isconfirmed for other languages as well: the lowerthe BLEU score the lower the correlation to hu-man judgments.We introduced a refinement of SemPOS, anautomatic metric of MT quality based on deep-syntactic representation of the sentence tacklingMetric Avg Best Worst3?SemPOS+1?BLEU4 0.55 0.83 0.142?SemPOS+1?BLEU2 0.55 0.83 0.142?SemPOS+1?BLEU1 0.53 0.83 0.094?SemPOS+1?BLEU3 0.53 0.83 0.09SemPOS 0.53 0.83 0.09BLEU2 0.43 0.83 0.09SemPOSpar 0.37 0.53 0.14Functorsons 0.36 0.53 0.14GTM 0.35 0.53 0.14BLEU4 0.33 0.53 0.09Void 0.33 0.53 0.09NIST 0.33 0.53 0.09Voidsons 0.33 0.53 0.09BLEU 0.33 0.53 0.09BLEU3 0.33 0.53 0.09BLEU1 0.29 0.53 -0.03SemPOSsons 0.28 0.42 0.03Functorpar 0.23 0.40 0.14Functor 0.21 0.40 0.09Voidpar 0.16 0.53 -0.08PER 0.12 0.53 -0.09TER 0.07 0.53 -0.23Table 3: System-level correlation coefficients forEnglish-to-Czech translation evaluated on 3 differ-ent testsets.the sparse data issue.
SemPOS was evaluated ontranslation to Czech and to English, scoring betterthan or comparable to many established metrics.ReferencesOndr?ej Bojar, David Marec?ek, Va?clav Nova?k, Mar-tin Popel, Jan Pta?c?ek, Jan Rous?, and Zdene?kZ?abokrtsky?.
2009.
English-Czech MT in 2008.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, Athens, Greece, March.
Asso-ciation for Computational Linguistics.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the Third Workshop on Statisti-cal Machine Translation, pages 70?106, Columbus,Ohio, June.
Association for Computational Linguis-tics.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009workshop on statistical machine translation.
In Pro-ceedings of the Fourth Workshop on Statistical Ma-chine Translation, Athens, Greece.
Association forComputational Linguistics.Silvie Cinkova?, Jan Hajic?, Marie Mikulova?, Lu-cie Mladova?, Anja Nedoluz?ko, Petr Pajas, JarmilaPanevova?, Jir???
Semecky?, Jana S?indlerova?, JosefToman, Zden?ka Ures?ova?, and Zdene?k Z?abokrtsky?.2004.
Annotation of English on the tectogram-matical level.
Technical Report TR-2006-35,U?FAL/CKL, Prague, Czech Republic, December.90Sherri Condon, Gregory A. Sanders, Dan Parvaz, AlanRubenstein, Christy Doran, John Aberdeen, andBeatrice Oshika.
2009.
Normalization for Auto-mated Metrics: English and Arabic Speech Transla-tion.
In MT Summit XII.Jesu?s Gime?nez and Llu?
?s Ma?rquez.
2007.
Linguis-tic Features for Automatic Evaluation of Heteroge-nous MT Systems.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages256?264, Prague, June.
Association for Computa-tional Linguistics.Jan Hajic?, Silvie Cinkova?, Kristy?na C?erma?kova?, Lu-cie Mladova?, Anja Nedoluz?ko, Petr Pajas, Jir???
Se-mecky?, Jana S?indlerova?, Josef Toman, Kristy?naToms?u?, Mate?j Korvas, Magdale?na Rysova?, Kater?inaVeselovska?, and Zdene?k Z?abokrtsky?.
2009.
PragueEnglish Dependency Treebank 1.0.
Institute of For-mal and Applied Linguistics, Charles University inPrague, ISBN 978-80-904175-0-2, January.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan S?te?pa?nek, Jir???
Havelka,Marie Mikulova?, Zdene?k Z?abokrtsky?, and MagdaS?evc???kova?
Raz??mova?.
2006.
Prague DependencyTreebank 2.0.
LDC2006T01, ISBN: 1-58563-370-4.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In ACL 2007, Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics Companion Volume Proceedings of the Demoand Poster Sessions, pages 177?180, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Kamil Kos and Ondr?ej Bojar.
2009.
Evaluation of Ma-chine Translation Metrics for Czech as the TargetLanguage.
Prague Bulletin of Mathematical Lin-guistics, 92.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In ACL 2002,Proceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics, pages 311?318, Philadelphia, Pennsylvania.M.
Przybocki, K. Peterson, and S. Bronsart.
2008.
Of-ficial results of the NIST 2008 ?Metrics for MA-chine TRanslation?
Challenge (MetricsMATR08).Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence and Its Semanticand Pragmatic Aspects.
Academia/Reidel Publish-ing Company, Prague, Czech Republic/Dordrecht,Netherlands.Zdene?k Z?abokrtsky?
and Ondr?ej Bojar.
2008.
TectoMT,Developer?s Guide.
Technical Report TR-2008-39,Institute of Formal and Applied Linguistics, Facultyof Mathematics and Physics, Charles University inPrague, December.91
