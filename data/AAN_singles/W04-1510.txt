Extensible Dependency Grammar: A New MethodologyRalph DebusmannProgramming Systems LabSaarland UniversityPostfach 15 11 5066041 Saarbru?ckenGermanyrade@ps.uni-sb.deDenys Duchier?Equipe CalligrammeLORIA ?
UMR 7503Campus Scientifique, B. P. 23954506 Vand?uvre le`s Nancy CEDEXFranceduchier@loria.frGeert-Jan M. KruijffComputational LinguisticsSaarland UniversityPostfach 15 11 5066041 Saarbru?ckenGermanygj@coli.uni-sb.deAbstractThis paper introduces the new grammar formalismof Extensible Dependency Grammar (XDG), andemphasizes the benefits of its methodology of ex-plaining complex phenomena by interaction of sim-ple principles on multiple dimensions of linguis-tic description.
This has the potential to increasemodularity with respect to linguistic description andgrammar engineering, and to facilitate concurrentprocessing and the treatment of ambiguity.1 IntroductionWe introduce the new grammar formalism of Exten-sible Dependency Grammar (XDG).
In XDG, com-plex phenomena arise out of the interaction of sim-ple principles on multiple dimensions of linguis-tic description.
In this paper, we point out howthis novel methodology positions XDG in betweenmulti-stratal approaches like LFG (Bresnan and Ka-plan, 1982) and MTT (Mel?c?uk, 1988), see also(Kahane, 2002), and mono-stratal ones like HPSG(Pollard and Sag, 1994), attempting to combinetheir benefits and avoid their problems.It is the division of linguistic analyses into dif-ferent dimensions which makes XDG multi-stratal.On the other, XDG is mono-stratal in that its princi-ples interact to constrain all dimensions simultane-ously.
XDG combines the benefits of these two po-sitions, and attempts to circumvent their problems.From multi-stratal approaches, XDG adopts a highdegree of modularity, both with respect to linguis-tic description as well as for grammar engineering.This also facilitates the statement of cross-linguisticgeneralizations.
XDG avoids the problem of placingtoo high a burden on the interfaces, and allows in-teractions between all and not only adjacent dimen-sions.
From mono-stratal approaches, XDG adoptsa high degree of integration, facilitating concurrentprocessing and the treatment of ambiguity.
At thesame time, XDG does not lose its modularity.XDG is a descendant of Topological Depen-dency Grammar (TDG) (Duchier and Debusmann,2001), pushing the underlying methodology furtherby generalizing it in two aspects:?
number of dimensions: two in TDG (ID andLP), arbitrary many in XDG?
set of principles: fixed in TDG, extensibleprinciple library in XDGThe structure of this paper is as follows: In ?2, weintroduce XDG and the XDG solver used for pars-ing and generation.
In ?3, we introduce a numberof XDG principles informally, before making use ofthem in an idealized example grammar in ?4.
In ?5we argue why XDG has the potential to be an im-provement over multi-stratal and mono-stratal ap-proaches, before we conclude in ?6.2 Extensible Dependency GrammarIn this section, we introduce XDG formally andmention briefly the constraint-based XDG solver forparsing and generation.2.1 FormalizationFormally, an XDG grammar is built up of dimen-sions, a lexicon and principles, and characterizes aset of well-formed analyses.A dimension is a tuple D = (Lab,Fea,Val,Pri) ofa set Lab of edge labels, a set Fea of features, a setVal of feature values, and a set of one-dimensionalprinciples Pri.
A lexicon for the dimension D is aset Lex ?
Fea ?
Val of total feature assignmentscalled lexical entries.
An analysis on dimensionD is a triple (V,E,F) of a set V of nodes, a setE ?
V ?V ?Lab of directed labeled edges, and anassignment F : V ?
(Fea ?
Val) of lexical entriesto nodes.
V and E form a graph.
We write AnaD forthe set of all possible analyses on dimension D. Theprinciples characterize subsets of AnaD.
We assumethat the elements of Pri are finite representations ofsuch subsets.An XDG grammar ((Labi,Feai,Vali,Prii)ni=1,Pri,Lex) consists of n dimensions, multi-dimensionalprinciples Pri, and a lexicon Lex.
An XDG analysis(V,Ei,Fi)ni=1 is an element of Ana = Ana1 ?
??
?
?Anan where all dimensions share the same set ofnodes V .
We call a dimension of a grammar gram-mar dimension.Multi-dimensional principles specify subsets ofAna, i.e.
of tuples of analyses for the individual di-mensions.
The lexicon Lex ?
Lex1???
?
?Lexn con-strains all dimensions at once, thereby synchroniz-ing them.
An XDG analysis is licensed by Lex iff(F1(v), .
.
.
,Fn(v)) ?
Lex for every node v ?V .In order to compute analyses for a given input,we employ a set of input constraints (Inp), whichagain specify a subset of Ana.
XDG solving thenamounts to finding elements of Ana that are licensedby Lex, and consistent with Inp and Pri.
The inputconstraints determine whether XDG solving is to beused for parsing or generation.
For parsing, theyspecify a sequence of words, and for generation, amultiset of semantic literals.2.2 SolverXDG solving has a natural reading as a constraintsatisfaction problem (CSP) on finite sets of integers,where well-formed analyses correspond to the solu-tions of the CSP (Duchier, 2003).
We have imple-mented an XDG solver using the Mozart-Oz pro-gramming system.XDG solving operates on all dimensions concur-rently.
This means that the solver can infer informa-tion about one dimension from information on an-other, if there is either a multi-dimensional principlelinking the two dimensions, or by the synchroniza-tion induced by the lexical entries.
For instance, notonly can syntactic information trigger inferences insyntax, but also vice versa.Because XDG allows us to write grammars withcompletely free word order, XDG solving is anNP-complete problem (Koller and Striegnitz, 2002).This means that the worst-case complexity of thesolver is exponential.
The average-case complexityof many smaller-scale grammars that we have ex-perimented with seems polynomial, but it remainsto be seen whether we can scale this up to large-scale grammars.3 PrinciplesThe well-formedness conditions of XDG analy-ses are stipulated by principles.
Principles areparametrizable, e.g.
by the dimensions on whichthey are applied, or by lexical features.
They canbe lexicalized or non-lexicalized, and can be one-dimensional or multi-dimensional.
Principles aretaken from an extensible principle library, and weintroduce some of the most important principles inthe following.3.1 Tree principletree(i) The analysis on dimension i must be a tree.The tree principle is non-lexicalized andparametrized by the dimension i.3.2 Dag principledag(i) The analysis on dimension i must be a di-rected acyclic graph.The dag principle is non-lexicalized andparametrized by the dimension i.3.3 Valency principlevalency(i, ini,outi) All nodes on dimension i mustsatisfy their in and out specifications.The valency principle is lexicalized and servesto lexically describe dependency graphs.
It isparametrized by the dimension i, the in specificationini and the out specification outi.
For each node, inistipulates the licensed incoming edges, and outi thelicensed outgoing edges.In the example grammar lexicon part in Figure 1below, the in specification is inID and outID is theout specification on the ID dimension.
For the com-mon noun Roman, the in specification licenses zeroor one incoming edges labeled subj, and zero or oneincoming edges labeled obj ({subj?,obj?
}), i.e.
itcan be either a subject or an object.
The out specifi-cation requires precisely one outgoing edge labeleddet ({det!
}), i.e.
it requires a determiner.3.4 Government principlegovernment(i,casesi,governi) All edges in dimen-sion i must satisfy the government specification ofthe mother.The government principle is lexicalized.
Its pur-pose is to constrain the case feature of a depen-dent.1 It is parametrized by the dimension i, thecases specification casesi and the government spec-ification govern.
cases assigns to each word a set ofpossible cases, and govern a mapping from labels tosets of cases.In Figure 1, the cases specification for the deter-miner den is {acc} (i.e.
it can only be accusative).By its government specification, the finite verb ver-sucht requires its subject to exhibit nominative case(subj 7?
{nom}).3.5 Agreement principleagreement(i,casesi,agreei) All edges in dimensioni must satisfy the agreement specification of themother.1We restrict ourselves to the case feature only for simplicity.In a fully-fledged grammar, the government principle would beused to constrain also other morphological aspects like number,person and gender.The agreement principle is lexicalized.
Its pur-pose is to enforce the case agreement of a daugh-ter.2 It is parametrized by dimension i, the lexicalcases specification casesi, assigning to each word aset of possible cases, and the agreement specifica-tion agreei, assigning to each word a set of labels.As an example, in Figure 1, the agreement spec-ification for the common noun Roman is {det}, i.e.the case of the common noun must agree with itsdeterminer.3.6 Order principle.order(i,oni,?i) On dimension i, 1) each node mustsatisfy its node labels specification, 2) the order ofthe daughters of each node must be compatible with?i, and 3) the node itself must be ordered correctlywith respect to its daughters (using its node label).The order principle is lexicalized.
It isparametrized by the dimension i, the node labelsspecification oni mapping each node to set of labelsfrom Labi, and the total order ?i on Labi.Assuming the node labels specification given inFigure 2, and the total order in (5), the tree in (11)satisfies the order principle.3 For instance for thenode versucht: 1) The node label of versucht is lbf,satisfying the node labels specification.
2) The orderof the daughters Roman (under the edge labeled vf),Peter (mf) and lesen (rbf) is compatible with thetotal order prescribing vf ?
mf ?
rbf.
3) The nodeversucht itself is ordered correctly with respect to itsdaughters (the total order prescribes vf ?
lbf ?mf).3.7 Projectivity principleprojectivity(i) The analysis on dimension i must beprojective.The projectivity principle is non-lexicalized.
Itspurpose is to exclude non-projective analyses.4 It isparametrized by dimension i.3.8 Climbing principleclimbing(i, j) The graph on dimension i must beflatter than the graph on dimension j.The climbing principle is non-lexicalized andtwo-dimensional.
It is parametrized by the two di-mensions i and j.For instance, the tree in (11) is flatter than thecorresponding tree in (10).
This concept was intro-duced as lifting in (Kahane et al, 1998).2Again, we restrict ourselves to case for simplicity.3The node labels are defined in (2) below.4The projectivity principle of course only makes sense incombination with the order principle.3.9 Linking principlelinking(i, j, linki, j) All edges on dimension i mustsatisfy the linking specification of the mother.The linking principle is lexicalized and two-dimensional.
It is parametrized by the two dimen-sions i and j, and by the linking specification linki, j ,mapping labels from Labi to sets of labels fromLab j .
Its purpose is to specify how dependents ondimension i are realized by (or linked to) dependentson dimension j.In the lexicon part in Figure 3, the linking spec-ification for the transitive verb lesen requires thatits agent on the PA dimension must be realized by asubject (ag 7?
{subj}), and the patient by an object(pat 7?
{obj}).The linking principle is oriented.
Symmetriclinking could be gained simply by using the linkingprinciple twice (in both directions).4 Example grammarIn this section, we elucidate XDG with an examplegrammar fragment for German.
With it, we demon-strate three aspects of the methodology of XDG:?
How complex phenomena such as topicaliza-tion and control arise by the interaction of sim-ple principles on different dimensions of lin-guistic description.?
How the high degree of integration helps to re-duce ambiguity.?
How the high degree of modularity facilitatesthe statement of cross-linguistic generaliza-tions.Note that this grammar fragment is an idealized ex-ample, and does not make any claims about XDG asa grammar theory.
Its purpose is solely to substan-tiate our points about XDG as a framework.
More-over, the grammar is fully lexicalized for simplicity.However, XDG of course allows the grammar writerto formulate lexical abstractions using inheritance(like in HPSG) or crossings (Candito, 1996).4.1 DimensionsThe grammar fragment make use of two dimen-sions: Immediate Dominance (ID) and LinearPrecedence (LP).
The models on the ID dimensionare unordered, syntactic dependency trees whoseedge labels correspond to syntactic functions likesubject and object.
On the LP dimension, the mod-els are ordered, projective topological dependencytrees whose edge labels are topological fields likeVorfeld and Mittelfeld.4.2 LabelsThe set LabID of labels on the ID dimension is:LabID = {det,subj,obj,vinf ,part} (1)These correspond resp.
to determiner, subject, ob-ject, infinitive verbal complement, and particle.The set LabLP of labels on the LP dimension is:LabLP = {detf,nounf,vf, lbf,mf,partf, rbf} (2)Corresponding resp.
to determiner field, noun field,Vorfeld, left bracket field, Mittelfeld, particle field,and right bracket field.4.3 PrinciplesOn the ID dimension, we make use of the followingone-dimensional principles:tree(ID)valency(ID, inID,outID)government(ID,casesID,governID)agreement(ID,casesID,agreeID)(3)The LP dimension uses the following principles:tree(LP)valency(LP, inLP,outLP)order(LP,onLP,?LP)projectivity(LP)(4)where the total order ?LP is defined as:detf ?
nounf ?
vf ?
lbf ?
mf ?
partf ?
rbf (5)We make use of the following multi-dimensionalprinciples:climbing(LP, ID)linking(LP, ID) (6)4.4 LexiconWe split the lexicon into two parts.
The ID and LPparts are displayed resp.
in Figure 15 and Figure 2.The LP part includes also the linking specificationfor the LP,ID-application of the linking principle.64.5 Government and agreementOur first example is the following sentence:Peter versucht einen Roman zu lesen.Peter tries aacc novel to read.Peter tries to read a novel.
(7)We display the ID analysis of the sentence below:.Peter versucht einen Roman zu lesensubj vinfpartobjdet(8)5Here, stands for ?don?t care?, this means e.g.
for the verbversucht that it has unspecified case.6We do not make use of the linking specification for theGerman grammar fragment (the mappings are all empty), butwe will do so as we switch to Dutch in ?4.8 below.Here, Peter is the subject of versucht.
lesen is the in-finitival verbal complement of versucht, zu the parti-cle of lesen, and Roman the object of lesen.
Finally,einen is the determiner of Roman.Under our example grammar, the sentence is un-ambiguous, i.e.
the given ID tree is the only possibleone.
Other ID trees are ruled out by the interactionof the principles on the ID dimension.
For instance,the government and agreement principles conspireto rule out the reading where Roman is the subject ofversucht (and Peter the object).
How?
By the agree-ment principle, Roman must be accusative, since itagrees with its accusative determiner einen.
By thegovernment principle, the subject of versucht mustbe nominative, and the object of lesen accusative.Thus Roman, by virtue of being accusative, cannotbecome the subject of versucht.
The only other op-tion for it is to become the object of lesen.
Conse-quently, Peter, which is unspecified for case, mustbecome the subject of versuchen (versuchen musthave a subject by the valency principle).4.6 TopicalizationOur second example is a case of topicalization,where the object has moved into the Vorfeld, to theleft of the finite verb:Einen Roman versucht Peter zu lesen.
(9)Here is the ID tree and the LP tree analysis:.Einen Roman versucht Peter zu lesensubj vinfpartobjdet(10).Einen Roman versucht Peter zu lesendetfnounflbfnounfpartfrbfvfdetfmf rbfpartf(11)The ID tree analysis is the same as before, exceptthat the words are shown in different positions.
Inthe LP tree, Roman is in the Vorfeld of versucht, Pe-ter in the Mittelfeld, and lesen in the right bracketfield.
versucht itself is (by its node label) in the leftbracket field.
Moreover, Einen is in the determinerfield of Roman, and zu in the particle field of lesen.Again, this is an example demonstrating howcomplex phenomena (here: topicalization) are ex-plained by the interaction of simple principles.
Top-icalization does not have to explicitly taken care of,it is rather a consequence of the interacting princi-ples.
Here, the valency, projectivity and climbinginID outID casesID governID agreeIDden {det?}
{} {acc} {} {}Roman {subj?,obj?}
{det!}
{nom,dat,acc} {} {det}Peter {subj?,obj?}
{} {nom,dat,acc} {} {}versucht {} {subj!,vinf!}
{subj 7?
{nom}} {}zu {part?}
{} {} {}lesen {vinf?}
{obj!}
{obj 7?
{acc}} {}Figure 1: Lexicon for the example grammar fragment, ID partinLP outLP onLP linkLP,IDden {detf?}
{} {detf} {}Roman {vf?,mf?}
{detf!}
{nounf} {}Peter {vf?,mf?}
{} {nounf} {}versucht {} {vf?,mf?, rbf?}
{lbf} {}zu {partf?}
{} {partf} {}lesen {rbf?}
{} {rbf} {}Figure 2: Lexicon for the example grammar fragment, LP partprinciples conspire to bring about the ?climbing up?of the NP Einen Roman from being the daughter oflesen in the ID tree to being the daughter of versuchtin the LP tree: The out specification of lesen doesnot license any outgoing edge.
Hence, Roman mustbecome the daughter of another node.
The only pos-sibility is versucht.
The determiner Einen must thenalso ?climb up?
because Roman is its only possi-ble mother.
The result is an LP tree which is flat-ter with respect to the ID tree.
The LP tree is alsoprojective.
If it were not be flatter, then it wouldbe non-projective, and ruled out by the projectivityprinciple.4.7 Negative exampleOur third example is a negative example, i.e.
an un-grammatical sentence:?Peter einen Roman versucht zu lesen.
(12)This example is perfectly legal on the unordered IDdimension, but has no model on the LP dimension.Why?
Because by its LP out specification, the finiteverb versucht allows only one dependent to the leftof it (in its Vorfeld), and here we have two.
Theinteresting aspect of this example is that althoughwe can find a well-formed ID tree for it, this ID treeis never actually generated.
The interactions of theprinciples, viz.
here of the principles on the LP di-mension, rule out the sentence before any full IDanalysis has been found.4.8 From German to DutchFor the fourth example, we switch from German toDutch.
We will show how to use the lexicon to con-cisely capture an important cross-linguistic general-ization.
We keep the same grammar as before, butwith two changes, arising from the lesser degree ofinflection and the higher reliance on word order inDutch:?
The determiner een is not case-marked butcan be either nominative, dative or accusative:casesID = {nom,dat,acc}.?
The Vorfeld of the finite verb probeert cannotbe occupied by an object (but only by an ob-ject): linkLP,ID = {vf 7?
{subj}}.7Now to the example, a Dutch translation of (7):Peter probeert een roman te lezen.Peter tries a novel to read.Peter tries to read a novel.
(13)We get only one analysis on the ID dimension,where Peter is the subject and roman the object.An analysis where Peter is the object of lezen androman the subject of probeert is impossible, as inthe German example.
The difference is, however,how this analysis is excluded.
In German, the ac-cusative inflection of the determiner einen triggeredthe agreement and the government principle to ruleit out.
In Dutch, the determiner is not inflected.The unwanted analysis is excluded on the groundsof word order instead: By the linking principle, theVorfeld of probeert must be filled by a subject, andnot by an object.
That means that Peter in the Vor-feld (to the left of probeert) must be a subject, andconsequently, the only other choice for roman is thatit becomes the object of lezen.4.9 Predicate-Argument StructureGoing towards semantics, we extend the grammarwith another dimension, Predicate-Argument Struc-ture (PA), where the models are not trees but di-rected acyclic graphs (dags), to model re-entrancies7Of course, this is an idealized assumption.
In fact, giventhe right stress, the Dutch Vorfeld can be filled by objects.e.g.
caused by control constructions.
Thanks to themodularity of XDG, the PA part of the grammar isthe same for German and Dutch.The set LabPA of labels on the PA dimension is:LabPA = {ag,pat,prop} (14)Corresponding resp.
to agent, patient and proposi-tion.The PA dimension uses the following one-dimensional principles:dag(PA)valency(PA, inPA,outPA) (15)Note that we re-use the valency principle again, aswe did on the ID and LP dimensions.And also the following multi-dimensional princi-ples:climbing(ID, PA)linking(PA, ID) (16)Here, we re-use the climbing and linking princi-ples.
That is, we state that the ID tree is flatterthan the corresponding PA dag.
This captures rais-ing and control, where arguments of embedded infi-nite verbs can ?climb up?
and become arguments ofa raising or control verb, in the same way as syntac-tic arguments can ?climb up?
from ID to LP.
We usethe linking principle to specify how semantic argu-ments are to be realized syntactically (e.g.
the agentas a subject etc.).
We display the PA part of the lex-icon in Figure 3.8Here is an example PA dag analysis of examplesentence (7):.Peter versucht einen Roman zu lesenagproppatag(17)Here, Peter is the agent of versucht, and also theagent of lesen.
Furthermore, lesen is a propositiondependent of versucht, and Roman is the patient oflesen.Notice that the PA dag is indeed a dag and not atree since Peter has two incoming edges: It is simul-taneously the agent of versucht and of lesen.
Thisis enforced by by the valency principle: Both ver-sucht and lesen require an agent.
Peter is the onlyword which can be the agent of both, because it is asubject and the agents of versucht and lesen mustbe subjects by the linking principle.
The climb-ing principle ensures that predicate arguments can8Notice that we specify linking lexically, allowing us tocapture deviations from the typical linking patterns.
Still, wecan also accommodate linking generalizations using lexical ab-stractions.be ?raised?
on the ID structure with respect to thePA structure.
Again, this example demonstrates thatXDG is able to reduce a complex phenomenon suchas control to the interaction of per se fairly simpleprinciples such as valency, climbing and linking.5 ComparisonThis section includes a more in-depth comparisonof XDG with purely multi- and mono-stratal ap-proaches.Contrary to multi-stratal approaches like LFG orMTT, XDG is more integrated.
For one, it placesa lighter burden the interfaces between the dimen-sions.
In LFG for instance, the ?
-mapping from c-structure to f-structure is rather specific, and has tobe specifically adapted to new c-structures, e.g.
inorder to handle a new construction with a differentword order.
That is, not only the grammar rules forthe c-structure need to be adapted, but also the inter-face between c- and f-structure.
In XDG, complexphenomena arise out of the interaction of simple,maximally general principles.
To accommodate thenew construction, the grammar would ideally onlyneed to be adapted on the word order dimension.Furthermore, XDG allows interactions of rela-tional constraints between all dimensions, not onlybetween adjacent ones (like c- and f-structure),and in all directions.
For one, this gets us bi-directionality for free.
Secondly, the interactionsof XDG have the potential to help greatly in reduc-ing ambiguity.
In multi-stratal approaches, ambigu-ity must be duplicated throughout the system.
E.g.suppose there are two candidate c-structures in LFGparsing, but one is ill-formed semantically.
Thenthey can only be ruled out after duplicating the am-biguity on the f-structure, and then filtering out theill-formed structure on the semantic ?
-structure.
InXDG on the other hand, the semantic principles canrule out the ill-formed analysis much earlier, typ-ically on the basis of a partial syntactic analysis.Thus, ill-formed analyses are never duplicated.Contrary to mono-stratal ones, XDG is moremodular.
For one, as (Oliva et al, 1999) note,mono-stratal approaches like HPSG usually giveprecedence to the syntactic tree structure, whileputting the description of other aspects of the anal-ysis on the secondary level only, by means of fea-tures spread over the nodes of the tree.
As a result,it becomes a hard task to modularize grammars.
Be-cause syntax is privileged, the phenomena ascribingto semantics cannot be described independently, andwhenever the syntax part of the grammar changes,the semantics part needs to be adapted.
In XDG, nodimension is privileged to another.
Semantic phe-inPA outPA linkPA,IDden {} {} {}Roman {ag?,pat?}
{} {}Peter {ag?,pat?}
{} {}versucht {} {ag!,prop!}
{ag 7?
{subj},prop 7?
{vinf}}zu {} {} {}lesen {prop?}
{ag!,pat!}
{ag 7?
{subj},pat 7?
{obj}}Figure 3: Lexicon of the example grammar fragment, PA partnomena can be described much more independentlyfrom syntax.
This facilitates grammar engineering,and also the statement of cross-linguistic general-izations.
Assuming that the semantics part of agrammar stay invariant for most natural languages,in order to accommodate a new language, ideallyonly the syntactic parts would need to be changed.6 ConclusionIn this paper, we introduced the XDG grammarframework, and emphasized that its new methodol-ogy places it in between the extremes of multi- andmono-stratal approaches.
By means of an idealizedexample grammar, we demonstrated how complexphenomena are explained as arising from the in-teraction of simple principles on numerous dimen-sions of linguistic description.
On the one hand, thismethodology has the potential to modularize lin-guistic description and grammar engineering, andto facilitate the statement of linguistic generaliza-tions.
On the other hand, as XDG is a inherentlyconcurrent architecture, inferences from any dimen-sion can help reduce the ambiguity on others.XDG is a new grammar formalism, and still hasmany open issues.
Firstly, we need to continue workon XDG as a framework.
Here, one important goalis to find out what criteria we can give to restrict theprinciples.
Secondly, we need to evolve the XDGgrammar theory, and in particular the XDG syntax-semantics interface.
Thirdly, for practical use, weneed to improve our knowledge about XDG solv-ing (i.e.
parsing and generation).
So far, our onlygood results are for smaller-scale handwritten gram-mars, and we have not good results yet for larger-scale grammars induced from treebanks (NEGRA,PDT) or converted from other grammar formalisms(XTAG).
Finally, we need to incorporate statisticsinto the picture, e.g.
to guide the search for solu-tions, in the vein of (Dienes et al, 2003).ReferencesJoan Bresnan and Ronald Kaplan.
1982.
Lexical-functional grammar: A formal system for gram-matical representation.
In Joan Bresnan, editor,The Mental Representation of Grammatical Re-lations, pages 173?281.
The MIT Press, Cam-bridge/USA.Marie-He`le?ne Candito.
1996.
A principle-based hi-erarchical representation of LTAG.
In Proceed-ings of COLING 1996, Kopenhagen/DEN.Peter Dienes, Alexander Koller, and MarcoKuhlmann.
2003.
Statistical A* DependencyParsing.
In Prospects and Advances in the Syn-tax/Semantics Interface, Nancy/FRA.Denys Duchier and Ralph Debusmann.
2001.Topological dependency trees: A constraint-based account of linear precedence.
In Proceed-ings of ACL 2001, Toulouse/FRA.Denys Duchier.
2003.
Configuration of labeledtrees under lexicalized constraints and principles.Research on Language and Computation, 1(3?4):307?336.Sylvain Kahane, Alexis Nasr, and Owen Ram-bow.
1998.
Pseudo-projectivity: a polynomi-ally parsable non-projective dependency gram-mar.
In 36th Annual Meeting of the Associa-tion for Computational Linguistics (ACL 1998),Montre?al/CAN.Sylvain Kahane.
2002.
Grammaire d?UnificationSens-Texte: Vers un mode`le mathe?matique ar-ticule?
de la langue.
Universite?
Paris 7.
Docu-ment de synthe`se de l?habilitation a` diriger lesrecherches.Alexander Koller and Kristina Striegnitz.
2002.Generation as dependency parsing.
In Proceed-ings of ACL 2002, Philadelphia/USA.Igor Mel?c?uk.
1988.
Dependency Syntax: Theoryand Practice.
State Univ.
Press of New York, Al-bany/USA.Karel Oliva, M. Andrew Moshier, and SabineLehmann.
1999.
Grammar engineering for thenext millennium.
In Proceedings of the 5th Natu-ral Language Processing Pacific Rim Symposium1999 ?Closing the Millennium?, Beijing/CHI.Tsinghua University Press.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven Phrase Structure Grammar.
University ofChicago Press, Chicago/USA.
