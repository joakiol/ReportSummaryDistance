Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1159?1168,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAdaptive Parser-Centric Text NormalizationCongle Zhang?Dept of Computer Science and EngineeringUniversity of Washington, Seattle, WA 98195, USAclzhang@cs.washington.eduTyler Baldwin Howard Ho Benny Kimelfeld Yunyao LiIBM Research - Almaden650 Harry Road, San Jose, CA 95120, USA{tbaldwi,ctho,kimelfeld,yunyaoli}@us.ibm.comAbstractText normalization is an important firststep towards enabling many Natural Lan-guage Processing (NLP) tasks over infor-mal text.
While many of these tasks, suchas parsing, perform the best over fullygrammatically correct text, most existingtext normalization approaches narrowlydefine the task in the word-to-word sense;that is, the task is seen as that of mappingall out-of-vocabulary non-standard wordsto their in-vocabulary standard forms.
Inthis paper, we take a parser-centric viewof normalization that aims to convert rawinformal text into grammatically correcttext.
To understand the real effect of nor-malization on the parser, we tie normal-ization performance directly to parser per-formance.
Additionally, we design a cus-tomizable framework to address the oftenoverlooked concept of domain adaptabil-ity, and illustrate that the system allows fortransfer to new domains with a minimalamount of data and effort.
Our experimen-tal study over datasets from three domainsdemonstrates that our approach outper-forms not only the state-of-the-art word-to-word normalization techniques, but alsomanual word-to-word annotations.1 IntroductionText normalization is the task of transforming in-formal writing into its standard form in the lan-guage.
It is an important processing step for awide range of Natural Language Processing (NLP)tasks such as text-to-speech synthesis, speechrecognition, information extraction, parsing, andmachine translation (Sproat et al, 2001).
?This work was conducted at IBM.The use of normalization in these applicationsposes multiple challenges.
First, as it is most oftenconceptualized, normalization is seen as the taskof mapping all out-of-vocabulary non-standardword tokens to their in-vocabulary standard forms.However, the scope of the task can also be seen asmuch wider, encompassing whatever actions arerequired to convert the raw text into a fully gram-matical sentence.
This broader definition of thenormalization task may include modifying punc-tuation and capitalization, and adding, removing,or reordering words.
Second, as with other NLPtechniques, normalization approaches are often fo-cused on one primary domain of interest (e.g.,Twitter data).
Because the style of informal writ-ing may be different in different data sources,tailoring an approach towards a particular datasource can improve performance in the desired do-main.
However, this is often done at the cost ofadaptability.This work introduces a customizable normal-ization approach designed with domain transfer inmind.
In short, customization is done by provid-ing the normalizer with replacement generators,which we define in Section 3.
We show that theintroduction of a small set of domain-specific gen-erators and training data allows our model to out-perform a set of competitive baselines, includingstate-of-the-art word-to-word normalization.
Ad-ditionally, the flexibility of the model also allows itto attempt to produce fully grammatical sentences,something not typically handled by word-to-wordnormalization approaches.Another potential problem with state-of-the-artnormalization is the lack of appropriate evaluationmetrics.
The normalization task is most frequentlymotivated by pointing to the need for clean textfor downstream processing applications, such assyntactic parsing.
However, most studies of nor-malization give little insight into whether and towhat degree the normalization process improves1159the performance of the downstream application.For instance, it is unclear how performance mea-sured by the typical normalization evaluation met-rics of word error rate and BLEU score (Pap-ineni et al, 2002) translates into performance ona parsing task, where a well placed punctuationmark may provide more substantial improvementsthan changing a non-standard word form.
To ad-dress this problem, this work introduces an eval-uation metric that ties normalization performancedirectly to the performance of a downstream de-pendency parser.The rest of this paper is organized as follows.In Section 2 we discuss previous approaches tothe normalization problem.
Section 3 presentsour normalization framework, including the actualnormalization and learning procedures.
Our in-stantiation of this model is presented in Section 4.In Section 5 we introduce the parser driven eval-uation metric, and present experimental results ofour model with respect to several baselines in threedifferent domains.
Finally, we discuss our exper-imental study in Section 6 and conclude in Sec-tion 7.2 Related WorkSproat et al (2001) took the first major look atthe normalization problem, citing the need for nor-malized text for downstream applications.
Unlikelater works that would primarily focus on specificnoisy data sets, their work is notable for attempt-ing to develop normalization as a general processthat could be applied to different domains.
The re-cent rise of heavily informal writing styles such asTwitter and SMS messages set off a new round ofinterest in the normalization problem.Research on SMS and Twitter normalization hasbeen roughly categorized as drawing inspirationfrom three other areas of NLP (Kobus et al, 2008):machine translation, spell checking, and automaticspeech recognition.
The statistical machine trans-lation (SMT) metaphor was the first proposed tohandle the text normalization problem (Aw et al,2006).
In this mindset, normalizing SMS can beseen as a translation task from a source language(informal) to a target language (formal), which canbe undertaken with typical noisy channel basedmodels.
Work by Choudhury et al (2007) adoptedthe spell checking metaphor, casting the problemin terms of character-level, rather than word-level,edits.
They proposed an HMM based model thattakes into account both grapheme and phonemeinformation.
Kobus et al (2008) undertook ahybrid approach that pulls inspiration from boththe machine translation and speech recognitionmetaphors.Many other approaches have been examined,most of which are at least partially reliant onthe above three metaphors.
Cook and Steven-son (2009) perform an unsupervised method,again based on the noisy channel model.
Pen-nell and Liu (2011) developed a CRF tagger fordeletion-based abbreviation on tweets.
Xue etal.
(2011) incorporated orthographic, phonetic,contextual, and acronym expansion factors to nor-malize words in both Twitter and SMS.
Liu etal.
(2011) modeled the generation process fromdictionary words to non-standard tokens under anunsupervised sequence labeling framework.
Hanand Baldwin (2011) use a classifier to detect ill-formed words, and then generate correction can-didates based on morphophonemic similarity.
Re-cent work has looked at the construction of nor-malization dictionaries (Han et al, 2012) and onimproving coverage by integrating different hu-man perspectives (Liu et al, 2012).Although it is almost universally used as a mo-tivating factor, most normalization work does notdirectly focus on improving downstream appli-cations.
While a few notable exceptions high-light the need for normalization as part of text-to-speech systems (Beaufort et al, 2010; Pennelland Liu, 2010), these works do not give any di-rect insight into how much the normalization pro-cess actually improves the performance of thesesystems.
To our knowledge, the work presentedhere is the first to clearly link the output of a nor-malization system to the output of the downstreamapplication.
Similarly, our work is the first to pri-oritize domain adaptation during the new wave oftext message normalization.3 ModelIn this section we introduce our normalizationframework, which draws inspiration from our pre-vious work on spelling correction for search (Baoet al, 2011).3.1 Replacement GeneratorsOur input the original, unnormalized text, repre-sented as a sequence x = x1, x2, .
.
.
, xn of tokensxi.
In this section we will use the following se-1160quence as our running example:x = Ay1 woudent2 of3 see4 ?em5where space replaces comma for readability, andeach token is subscripted by its position.
Given theinput x, we apply a series of replacement genera-tors, where a replacement generator is a functionthat takes x as input and produces a collection ofreplacements.
Here, a replacement is a statementof the form ?replace tokens xi, .
.
.
, xj?1 with s.?More precisely, a replacement is a triple ?i, j, s?,where 1 ?
i ?
j ?
n + 1 and s is a sequence oftokens.
Note that in the case where i = j, the se-quence s should be inserted right before xi; and inthe special case where s is empty, we simply deletexi, .
.
.
, xj?1.
For instance, in our running exam-ple the replacement ?2, 3,would not?
replacesx2 = woudent with would not; ?1, 2,Ay?
re-places x1 with itself (hence, does not change x);?1, 2, ?
(where  is the empty sequence) deletesx1; ?6, 6,.?
inserts a period at the end of the se-quence.The provided replacement generators can be ei-ther generic (cross domain) or domain-specific, al-lowing for domain customization.
In Section 4,we discuss the replacement generators used in ourempirical study.3.2 Normalization GraphGiven the input x and the set of replacements pro-duced by our generators, we associate a uniqueBoolean variable Xr with each replacement r. Asexpected, Xr being true means that the replace-ment r takes place in producing the output se-quence.Next, we introduce dependencies among vari-ables.
We first discuss the syntactic consistencyof truth assignments.
Let r1 = ?i1, j1, s1?
andr2 = ?i2, j2, s2?
be two replacements.
We saythat r1 and r2 are locally consistent if the inter-vals [i1, j1) and [i2, j2) are disjoint.
Moreover,we do not allow two insertions to take place atthe same position; therefore, we exclude [i1, j1)and [i2, j2) from the definition of local consistencywhen i1 = j1 = i2 = j2.
If r1 and r2 are locallyconsistent and j1 = i2, then we say that r2 is aconsistent follower of r1.A truth assignment ?
to our variables Xr issound if every two replacements r and r?
with?
(Xr) = ?(Xr?)
= true are locally consis-tent.
We say that ?
is complete if every tokenof x is captured by at least one replacement rwith ?
(Xr) = true.
Finally, we say that ?is legal if it is sound and complete.
The out-put (normalized sequence) defined by a legal as-signment ?
is, naturally, the concatenation (fromleft to right) of the strings s in the replacementsr = ?i, j, s?
with ?
(Xr) = true.
In Fig-ure 1, for example, if the nodes with a greyshade are the ones associated with true vari-ables under ?, then the output defined by ?
isI would not have seen them.Our variables carry two types of interdependen-cies.
The first is that of syntactic consistency: theentire assignment is required to be legal.
The sec-ond captures correlation among replacements.
Forinstance, if we replace of with have in our run-ning example, then the next see token is morelikely to be replaced with seen.
In this work,dependencies of the second type are restricted topairs of variables, where each pair corresponds toa replacement and a consistent follower thereof.The above dependencies can be modeled over astandard undirected graph using Conditional Ran-dom Fields (Lafferty et al, 2001).
However, thegraph would be complex: in order to model lo-cal consistency, there should be edges between ev-ery two nodes that violate local consistency.
Sucha model renders inference and learning infeasi-ble.
Therefore, we propose a clearer model by adirected graph, as illustrated in Figure 1 (wherenodes are represented by replacements r insteadof the variables Xr, for readability).
To incorpo-rate correlation among replacements, we introducean edge from Xr to Xr?
whenever r?
is a consis-tent follower of r. Moreover, we introduce twodummy nodes, start and end, with an edge fromstart to each variable that corresponds to a prefixof the input sequence x, and an edge from eachvariable that corresponds to a suffix of x to end.The principal advantage of modeling the depen-dencies in such a directed graph is that now, the le-gal assignments are in one-to-one correspondencewith the paths from start to end; this is a straight-forward observation that we do not prove here.We appeal to the log-linear model formulationto define the probability of an assignment.
Theconditional probability of an assignment ?, givenan input sequence x and the weight vector ?
=?
?1, .
.
.
, ?k?
for our features, is defined as p(?
|1161?1, 2,I?end?2, 4,would not have?
?1, 2,Ay?
?5, 6,them?
?4, 5,seen?
?2, 3,would?
?4, 6,see him?
?3, 4,of?start?6, 6, .
?Figure 1: Example of a normalization graph; thenodes are replacements generated by the replace-ment generators, and every path from start to endimplies a legal assignmentx,?)
= 0 if ?
is not legal, and otherwise,p(?
| x,?)
= 1Z(x)?X?Y ?
?exp(?j?j?j(X,Y,x)) .Here, Z(x) is the partition function, X ?
Y ?
?refers to an edge X ?
Y with ?
(X) = true and?
(Y ) = true, and ?1(X,Y,x), .
.
.
, ?k(X,Y,x)are real valued feature functions that are weightedby ?1, .
.
.
, ?k (the model?s parameters), respec-tively.3.3 InferenceWhen performing inference, we wish to selectthe output sequence with the highest probability,given the input sequence x and the weight vector?
(i.e., MAP inference).
Specifically, we want anassignment ??
= arg max?
p(?
| x,?
).While exact inference is computationally hardon general graph models, in our model it boilsdown to finding the longest path in a weightedand acyclic directed graph.
Indeed, our directedgraph (illustrated in Figure 1) is acyclic.
We as-sign the real value ?j ?j?j(X,Y,x) to the edgeX ?
Y , as the weight.
As stated in Section 3.2,a legal assignment ?
corresponds to a path fromstart to end; moreover, the sum of the weights onthat path is equal to log p(?
| x,?)
+ logZ(x).In particular, a longer path corresponds to an as-signment with greater probability.
Therefore, wecan solve the MAP inference within our model byfinding the weighted longest path in the directedacyclic graph.
The algorithm in Figure 2 summa-rizes the inference procedure to normalize the in-put sequence x.Input:1.
A sequence x to normalize;2.
A weight vector ?
= ?
?1, .
.
.
, ?k?.Generate replacements: Apply all replace-ment generators to get a set of replacements r,each r is a triple ?i, j, s?.Build a normalization graph:1.
For each replacement r, create a node Xr.2.
For each r?
and r, create an edge Xr toXr?
if r?
is a consistent follower of r.3.
Create two dummy nodes start and end,and create edges from start to all prefixnodes and end to all suffix nodes.4.
For each edge X ?
Y , compute the fea-tures ?j(X,Y,x), and weight the edge by?j ?j?j(X,Y,x).MAP Inference: Find a weighted longest pathP from start to end, and return ?
?, where??
(Xr) = true iff Xr ?
P .Figure 2: Normalization algorithm3.4 LearningOur labeled data consists of pairs (xi,ygoldi ),where xi is an input sequence (to normalize) andygoldi is a (manually) normalized sequence.
Weobtain a truth assignment ?goldi from each ygoldiby selecting an assignment ?
that minimizes theedit distance between ygoldi and the normalizedtext implied by ?
:?goldi = arg min?
DIST(y(?
),ygoldi ) (1)Here, y(?)
denotes the normalized text implied by?, and DIST is a token-level edit distance.
Weapply a simple dynamic-programming algorithmto compute ?goldi .
Finally, the items in our trainingdata are the pairs (xi, ?goldi ).Learning over similar models is commonlydone via maximum likelihood estimation:L(?)
= log?ip(?i = ?goldi | xi,?
)Taking the partial derivative gives the following:?i(?j(?goldi ,xi)?
Ep(?i|xi,?
)?j(?i,xi))where ?j(?,x) = ?X?Y ?j(X,Y,x), that is,the sum of values for the jth feature along the1162Input:1.
A set {(xi,ygoldi )}ni=1 of sequences andtheir gold normalization;2.
Number T of iterations.Initialization: Initialize each ?j as zero, andobtain each ?goldi according to (1).Repeat T times:1.
Infer each ?
?i from xi using the current ?;2.
?j ?
?j+?i(?j(?goldi ,xi)??j(?
?i ,xi))for all j = 1, .
.
.
, k.Output: ?
= ?
?1, .
.
.
, ?k?Figure 3: Learning algorithmpath defined by ?, andEp(?i|xi,?
)?j(?i,xi) is theexpected value of that sum (over all legal assign-ments ?i), assuming the current weight vector.How to efficiently computeEp(?i|xi,?
)?j(?i,xi) in our model is un-clear; naively, it requires enumerating all legalassignments.
We instead opt to use a moretractable perceptron-style algorithm (Collins,2002).
Instead of computing the expectation,we simply compute ?j(?
?i ,xi), where ?
?i is theassignment with the highest probability, generatedusing the current weight vector.
The result is then:?i(?j(?goldi ,xi)?
?j(?
?i ,xi))Our learning applies the following two steps it-eratively.
(1) Generate the most probable sequencewithin the current weights.
(2) Update the weightsby comparing the path generated in the previousstep to the gold standard path.
The algorithm inFigure 3 summarizes the procedure.4 InstantiationIn this section, we discuss our instantiation of themodel presented in the previous section.
In partic-ular, we describe our replacement generators andfeatures.4.1 Replacement GeneratorsOne advantage of our proposed model is thatthe reliance on replacement generators allows forstrong flexibility.
Each generator can be seen as ablack box, allowing replacements that are createdheuristically, statistically, or by external tools to beincorporated within the same framework.Generator From Toleave intact good goodedit distance bac backlowercase NEED needcapitalize it ItGoogle spell disspaear disappearcontraction wouldn?t would notslang language ima I am going toinsert punctuation  .duplicated punctuation !?
!delete filler lmao Table 1: Example replacement generatorsTo build a set of generic replacement generatorssuitable for normalizing a variety of data types, wecollected a set of about 400 Twitter posts as devel-opment data.
Using that data, a series of gener-ators were created; a sample of them are shownin Table 1.
As shown in the table, these gener-ators cover a variety of normalization behavior,from changing non-standard word forms to insert-ing and deleting tokens.4.2 FeaturesAlthough the proposed framework supports realvalued features, all features in our system are bi-nary.
In total, we used 70 features.
Our feature setpulls information from several different sources:N-gram: Our n-gram features indicate the fre-quency of the phrases induced by an edge.
Thesefeatures are turned into binary ones by bucketingtheir log values.
For example, on the edge from?1, 2,I?
to ?2, 3,would?
such a feature will indi-cate whether the frequency of I would is overa threshold.
We use the Corpus of ContemporaryEnglish (Davies, 2008 ) to produce our n-gram in-formation.Part-of-speech: Part-of-speech informationcan be used to produce features that encouragecertain behavior, such as avoiding the deletion ofnoun phrases.
We generate part-of-speech infor-mation over the original raw text using a Twit-ter part-of-speech tagger (Ritter et al, 2011).
Ofcourse, the part-of-speech information obtainedthis way is likely to be noisy, and we expect ourlearning algorithm to take that into account.Positional: Information from positions is usedprimarily to handle capitalization and punctuationinsertion, for example, by incorporating featuresfor capitalized words after stop punctuation or theinsertion of stop punctuation at the end of the sen-tence.Lineage: Finally, we include binary features1163that indicate which generator spawned the replace-ment.5 EvaluationIn this section, we present an empirical study ofour framework.
The study is done over datasetsfrom three different domains.
The goal is to eval-uate the framework in two aspects: (1) usefulnessfor downstream applications (specifically depen-dency parsing), and (2) domain adaptability.5.1 Evaluation MetricsA few different metrics have been used to evaluatenormalizer performance, including word error rateand BLEU score.
While each metric has its prosand cons, they all rely on word-to-word matchingand treat each word equally.
In this work, we aimto evaluate the performance of a normalizer basedon how it affects the performance of downstreamapplications.
We find that the conventional metricsare not directly applicable, for several reasons.
Tobegin with, the assumption that words have equalweights is unlikely to hold.
Additionally, thesemetrics tend to ignore other important non-wordinformation such as punctuation or capitalization.They also cannot take into account other aspectsthat may have an impact on downstream perfor-mance, such as the word reordering as seen in theexample in Figure 4.
Therefore, we propose a newevaluation metric that directly equates normaliza-tion performance with the performance of a com-mon downstream application?dependency pars-ing.To realize our desired metric, we apply the fol-lowing procedure.
First, we produce gold standardnormalized data by manually normalizing sen-tences to their full grammatically correct form.
Inaddition to the word-to-word mapping performedin typical normalization gold standard generation,this annotation procedure includes all actions nec-essary to make the sentence grammatical, such asword reordering, modifying capitalization, and re-moving emoticons.
We then run an off-the-shelfdependency parser on the gold standard normal-ized data to produce our gold standard parses.
Al-though the parser could still produce mistakes onthe grammatical sentences, we feel that this pro-vides a realistic benchmark for comparison, as itrepresents an upper bound on the possible perfor-mance of the parser, and avoids an expensive sec-ond round of manual annotation.Test Gold SVOI kinda wanna getipad NEWI kind of want toget a new iPad.verb(get) verb(want)verb(get)precisionv = 11recallv = 12subj(get,I)subj(get,wanna)obj(get,NEW)subj(want,I)subj(get,I)obj(get,iPad)precisionso = 13recallso = 13Figure 4: The subjects, verbs, and objects identi-fied on example test/gold text, and correspondingmetric scoresTo compare the parses produced over automati-cally normalized data to the gold standard, we lookat the subjects, verbs, and objects (SVO) identi-fied in each parse.
The metric shown in Equa-tions (2) and (3) below is based on the identifiedsubjects and objects in those parses.
Note that SOdenotes the set of identified subjects and objectswhereas SOgold denotes the set of subjects andobjects identified when parsing the gold-standardnormalization.precisionso =|SO ?
SOgold||SO | (2)recallso = |SO ?
SOgold||SOgold|(3)We similarly define precisionv and recallv, wherewe compare the set V of identified verbs to V goldof those found in the gold-standard normalization.An example is shown in Figure 4.5.2 ResultsTo establish the extensibility of our normaliza-tion system, we present results in three differentdomains: Twitter posts, Short Message Service(SMS) messages, and call-center logs.
For Twitterand SMS messages, we used established datasetsto compare with previous work.
As no estab-lished call-center log dataset exists, we collectedour own.
In each case, we ran the proposed systemwith two different configurations: one using onlythe generic replacement generators presented inSection 4 (denoted as generic), and one that addsadditional domain-specific generators for the cor-responding domain (denoted as domain-specific).All runs use ten-fold cross validation for trainingand evaluation.
The Stanford parser1 (Marneffeet al, 2006) was used to produce all dependency1Version 2.0.4, http://nlp.stanford.edu/software/lex-parser.shtml1164parses.
We compare our system to the followingbaseline solutions:w/oN: No normalization is performed.Google: Output of the Google spell checker.w2wN: The output of the word-to-word normal-ization of Han and Baldwin (2011).
Not availablefor call-center data.Gw2wN: The manual gold standard word-to-word normalizations of previous work (Choud-hury et al, 2007; Han and Baldwin, 2011).
Notavailable for call-center data.Our results use the metrics of Section 5.1.5.2.1 TwitterTo evaluate the performance on Twitter data, weuse the dataset of randomly sampled tweets pro-duced by (Han and Baldwin, 2011).
Because thegold standard used in this work only providedword mappings for out-of-vocabulary words anddid not enforce grammaticality, we reannotated thegold standard data2.
Their original gold standardannotations were kept as a baseline.To produce Twitter-specific generators, we ex-amined the Twitter development data collected forgeneric generator production (Section 4).
Thesegenerators focused on the Twitter-specific notionsof hashtags (#), ats (@), and retweets (RT).
Foreach case, we implemented generators that al-lowed for either the initial symbol or the entire to-ken to be deleted (e.g., @Hertz to Hertz, @Hertzto ).The results are given in Table 2.
As shown,the domain-specific generators yielded perfor-mance significantly above the generic ones and allbaselines.
Even without domain-specific genera-tors, our system outperformed the word-to-wordnormalization approaches.
Most notably, boththe generic and domain-specific systems outper-formed the gold standard word-to-word normal-izations.
These results validate the hypothesis thatsimple word-to-word normalization is insufficientif the goal of normalization is to improve depen-dency parsing; even if a system could produceperfect word-to-word normalization, it would pro-duce lower quality parses than those produced byour approach.2Our results and the reannotations of the Twitter and SMSdata are available at https://www.cs.washington.edu/node/9091/System Verb Subject-ObjectPre Rec F1 Pre Rec F1w/oN 83.7 68.1 75.1 31.7 38.6 34.8Google 88.9 78.8 83.5 36.1 46.3 40.6w2wN 87.5 81.5 84.4 44.5 58.9 50.7Gw2w 89.8 83.8 86.7 46.9 61.0 53.0generic 91.7 88.9 90.3 53.6 70.2 60.8domain specific 95.3 88.7 91.9 72.5 76.3 74.4Table 2: Performance on Twitter dataset5.2.2 SMSTo evaluate the performance on SMS data, we usethe Treasure My Text data collected by Choud-hury et al (2007).
As with the Twitter data, theword-to-word normalizations were reannotated toenforce grammaticality.
As a replacement genera-tor for SMS-specific substitutions, we used a map-ping dictionary of SMS abbreviations.3 No furtherSMS-specific development data was needed.Table 3 gives the results on the SMS data.
TheSMS dataset proved to be more difficult than theTwitter dataset, with the overall performance ofevery system being lower.
While this drop of per-formance may be a reflection of the difference indata styles between SMS and Twitter, it is alsolikely a product of the collection methodology.The collection methodology of the Treasure MyText dataset dictated that every message must haveat least one mistake, which may have resulted in adataset that was noisier than average.Nonetheless, the trends on SMS data mirrorthose on Twitter data, with the domain-specificgenerators achieving the greatest overall perfor-mance.
However, while the generic setting stillmanages to outperform most baselines, it did notoutperform the gold word-to-word normalization.In fact, the gold word-to-word normalization wasmuch more competitive on this data, outperform-ing even the domain-specific system on verbsalone.
This should not be seen as surprising, asword-to-word normalization is most likely to bebeneficial for cases like this where the proportionof non-standard tokens is high.It should be noted that the SMS dataset as avail-able has had all punctuation removed.
While thismay be appropriate for word-to-word normaliza-tion, this preprocessing may have an effect on theparse of the sentence.
As our system has the abil-ity to add punctuation but our baseline systems donot, this has the potential to artificially inflate ourresults.
To ensure a fair comparison, we manually3http://www.netlingo.com/acronyms.php1165System Verb Subject-ObjectRec Pre F1 Rec Pre F1w/oN 76.4 48.1 59.0 19.5 21.5 20.4Google 85.1 61.6 71.5 22.4 26.2 24.1w2wN 78.5 61.5 68.9 29.9 36.0 32.6Gw2wN 87.6 76.6 81.8 38.0 50.6 43.4generic 86.5 77.4 81.7 35.5 47.7 40.7domain specific 88.1 75.0 81.0 41.0 49.5 44.8Table 3: Performance on SMS datasetSystem Verb Subject-ObjectPre Rec F1 Pre Rec F1w/oN 98.5 97.1 97.8 69.2 66.1 67.6Google 99.2 97.9 98.5 70.5 67.3 68.8generic 98.9 97.4 98.1 71.3 67.9 69.6domain specific 99.2 97.4 98.3 87.9 83.1 85.4Table 4: Performance on call-center datasetadded punctuation to a randomly selected smallsubset of the SMS data and reran each system.This experiment suggested that, in contrast to thehypothesis, adding punctuation actually improvedthe results of the proposed system more substan-tially than that of the baseline systems.5.2.3 Call-CenterAlthough Twitter and SMS data are unmistakablydifferent, there are many similarities between thetwo, such as the frequent use of shorthand wordforms that omit letters.
The examination of call-center logs allows us to examine the ability of oursystem to perform normalization in more disparatedomains.
Our call-center data consists of text-based responses to questions about a user?s expe-rience with a call-center (e.g., their overall satis-faction with the service).
We use call-center logsfrom a major company, and collect about 150 re-sponses for use in our evaluation.
We collectedan additional small set of data to develop our call-center-specific generators.Results on the call-center dataset are in Table 4.As shown, the raw call-center data was compar-atively clean, resulting in higher baseline perfor-mance than in other domains.
Unlike on previ-ous datasets, the use of generic mappings onlyprovided a small improvement over the baseline.However, the use of domain-specific generatorsonce again led to significantly increased perfor-mance on subjects and objects.6 DiscussionThe results presented in the previous section sug-gest that domain transfer using the proposed nor-malization framework is possible with only asmall amount of effort.
The relatively modestset of additional replacement generators includedin each data set alowed the domain-specific ap-proaches to significantly outperform the genericapproach.
In the call-center case, performance im-provements could be seen by referencing a verysmall amount of development data.
In the SMScase, the presence of a domain-specific dictionaryallowed for performance improvements withoutthe need for any development data at all.
It islikely, though not established, that employing fur-ther development data would result in further per-formance improvements.
We leave further investi-gation to future work.The results in Section 5.2 establish a point thathas often been assumed but, to the best of ourknowledge, has never been explicitly shown: per-forming normalization is indeed beneficial to de-pendency parsing on informal text.
The parse ofthe normalized text was substantially better thanthe parse of the original raw text in all domains,with absolute performance increases ranging fromabout 18-25% on subjects and objects.
Further-more, the results suggest that, as hypothesized,preparing an informal text for a parsing task re-quires more than simple word-to-word normaliza-tion.
The proposed approach significantly outper-forms the state-of-the-art word-to-word normal-ization approach.
Perhaps most interestingly, theproposed approach performs on par with, and inseveral cases superior to, gold standard word-to-word annotations.
This result gives strong evi-dence for the conclusion that parser-targeted nor-malization requires a broader understanding of thescope of the normalization task.While the work presented here gives promis-ing results, there are still many behaviors foundin informal text that prove challenging.
Onesuch example is the word reordering seen in Fig-ure 4.
Although word reordering could be incor-porated into the model as a combination of a dele-tion and an insertion, the model as currently de-vised cannot easily link these two replacementsto one another.
Additionally, instances of re-ordering proved hard to detect in practice.
Assuch, no reordering-based replacement generatorswere implemented in the presented system.
An-other case that proved difficult was the insertionof missing tokens.
For instance, the informalsentence ?Day 3 still don?t freaking1166feel good!:(?
could be formally renderedas ?It is day 3 and I still do notfeel good!?.
Attempts to address missing to-kens in the model resulted in frequent false pos-itives.
Similarly, punctuation insertion proved tobe challenging, often requiring a deep analysisof the sentence.
For example, contrast the sen-tence ?I?m watching a movie I don?tknow its name.?
which would benefit frominserted punctuation, with ?I?m watching amovie I don?t know.
?, which would not.We feel that the work presented here provides afoundation for future work to more closely exam-ine these challenges.7 ConclusionsThis work presents a framework for normalizationwith an eye towards domain adaptation.
The pro-posed framework builds a statistical model over aseries of replacement generators.
By doing so, itallows a designer to quickly adapt a generic modelto a new domain with the inclusion of a small set ofdomain-specific generators.
Tests over three dif-ferent domains suggest that, using this model, onlya small amount of domain-specific data is neces-sary to tailor an approach towards a new domain.Additionally, this work introduces a parser-centric view of normalization, in which the per-formance of the normalizer is directly tied to theperformance of a downstream dependency parser.This evaluation metric allows for a deeper under-standing of how certain normalization actions im-pact the output of the parser.
Using this met-ric, this work established that, when dependencyparsing is the goal, typical word-to-word normal-ization approaches are insufficient.
By taking abroader look at the normalization task, the ap-proach presented here is able to outperform notonly state-of-the-art word-to-word normalizationapproaches but also manual word-to-word annota-tions.Although the work presented here establishedthat more than word-to-word normalization wasnecessary to produce parser-ready normalizations,it remains unclear which specific normalizationtasks are most critical to parser performance.
Weleave this interesting area of examination to futurework.AcknowledgmentsWe thank the anonymous reviewers of ACL forhelpful comments and suggestions.
We also thankIoana R. Stanoi for her comments on a prelim-inary version of this work, Daniel S. Weld forhis support, and Alan Ritter, Monojit Choudhury,Bo Han, and Fei Liu for sharing their tools anddata.
The first author is partially supported by theDARPA Machine Reading Program under AFRLprime contract numbers FA8750-09-C-0181 andFA8750-09-C-0179.
Any opinions, findings, con-clusions, or recommendations expressed in thismaterial are those of the authors and do not neces-sarily reflect the views of DARPA, AFRL, or theUS government.
This work is a part of IBM?s Sys-temT project (Chiticariu et al, 2010).ReferencesAiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for sms text normal-ization.
In ACL, pages 33?40.Zhuowei Bao, Benny Kimelfeld, and Yunyao Li.
2011.A graph approach to spelling correction in domain-centric search.
In ACL, pages 905?914.Richard Beaufort, Sophie Roekhaut, Louise-Ame?lieCougnon, and Ce?drick Fairon.
2010.
A hybridrule/model-based finite-state framework for normal-izing sms messages.
In ACL, pages 770?779.Laura Chiticariu, Rajasekar Krishnamurthy, YunyaoLi, Sriram Raghavan, Frederick Reiss, and Shivaku-mar Vaithyanathan.
2010.
SystemT: An algebraicapproach to declarative information extraction.
InACL, pages 128?137.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structureof texting language.
IJDAR, 10(3-4):157?174.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and ex-periments with perceptron algorithms.
In EMNLP,pages 1?8.Paul Cook and Suzanne Stevenson.
2009.
An unsu-pervised model for text message normalization.
InCALC, pages 71?78.Mark Davies.
2008-.
The corpus of contempo-rary american english: 450 million words, 1990-present.
Avialable online at: http://corpus.byu.edu/coca/.Bo Han and Timothy Baldwin.
2011.
Lexical normali-sation of short text messages: Makn sens a #twitter.In ACL, pages 368?378.1167Bo Han, Paul Cook, and Timothy Baldwin.
2012.
Au-tomatically constructing a normalisation dictionaryfor microblogs.
In EMNLP-CoNLL, pages 421?432.Catherine Kobus, Franc?ois Yvon, and Ge?raldineDamnati.
2008.
Normalizing SMS: are twometaphors better than one?
In COLING, pages 441?448.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In ICML, pages 282?289.Fei Liu, Fuliang Weng, Bingqing Wang, and Yang Liu.2011.
Insertion, deletion, or substitution?
normal-izing text messages without pre-categorization norsupervision.
In ACL, pages 71?76.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012.
Abroad-coverage normalization system for social me-dia language.
In ACL, pages 1035?1044.Marie-Catherine De Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC-06, pages 449?454.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In ACL, pages 311?318.Deana Pennell and Yang Liu.
2010.
Normalization oftext messages for text-to-speech.
In ICASSP, pages4842?4845.Deana Pennell and Yang Liu.
2011.
A character-levelmachine translation approach for normalization ofSMS abbreviations.
In IJCNLP, pages 974?982.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in Tweets: An ex-perimental study.
In EMNLP, pages 1524?1534.Richard Sproat, Alan W. Black, Stanley F. Chen,Shankar Kumar, Mari Ostendorf, and ChristopherRichards.
2001.
Normalization of non-standardwords.
Computer Speech & Language, 15(3):287?333.Zhenzhen Xue, Dawei Yin, and Brian D. Davison.2011.
Normalizing microtext.
In Analyzing Micro-text, volume WS-11-05 of AAAI Workshops.1168
