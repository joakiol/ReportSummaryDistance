Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 523?532,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA Japanese Predicate Argument Structure Analysis using Decision ListsHirotoshi Taira, Sanae Fujita, Masaaki NagataNTT Communication Science Laboratories2-4, Hikaridai, Seika-cho,Keihanna Science City,Kyoto 619-0237, Japan{{taira,sanae}@cslab.kecl, nagata.masaaki@lab}.ntt.co.jpAbstractThis paper describes a new automatic methodfor Japanese predicate argument structureanalysis.
The method learns relevant featuresto assign case roles to the argument of the tar-get predicate using the features of the wordslocated closest to the target predicate undervarious constraints such as dependency types,words, semantic categories, parts of speech,functional words and predicate voices.
Weconstructed decision lists in which these fea-tures were sorted by their learned weights.
Us-ing our method, we integrated the tasks of se-mantic role labeling and zero-pronoun iden-tification, and achieved a 17% improvementcompared with a baseline method in a sen-tence level performance analysis.1 IntroductionRecently, predicate argument structure analysis hasattracted the attention of researchers because thisinformation can increase the precision of text pro-cessing tasks, such as machine translation, informa-tion extraction (Hirschman et al, 1999), questionanswering (Narayanan and Harabagiu, 2004) (Shenand Lapata, 2007), and summarization (Melli etal., 2005).
In English predicate argument structureanalysis, large corpora such as FrameNet (Fillmoreet al, 2001), PropBank (Palmer et al, 2005) andNomBank (Meyers et al, 2004) have been createdand utilized.
Recently, the GDA Corpus (Hashida,2005), Kyoto Text Corpus Ver.4.0 (Kawahara et al,2002) and NAIST Text Corpus (Iida et al, 2007)were constructed in Japanese, and these corporahave become the target of an automatic Japanesepredicate argument structure analysis system.
Weconducted Japanese predicate argument structure(PAS) analysis for the NAIST Text Corpus, whichis the largest of these three corpora, and, as far aswe know, this is the first time PAS analysis has beenconducted for whole articles of the corpus.The NAIST Text Corpus has the following char-acteristics, i) semantic roles for both predicates andevent nouns are annotated in the corpus, ii) three ma-jor case roles,1 namely the ga, wo and ni-cases inJapanese are annotated for the base form of pred-icates and event nouns, iii) both the case roles insentences containing the target predicates and thoseoutside the sentences (zero-pronouns) are annotated,and iv) coreference relations are also annotated.As regards i), recently there has been an increasein the number of papers dealing with nominalizedpredicates (Pradhan et al, 2004) (Jiang and Ng,2006) (Xue, 2006) (Liu and Ng, 2007).
For exam-ple, ?trip?
in the sentence ?During my trip to Italy, Imet him.?
refers not only to the event ?I met him?but also to the event ?I traveled to Italy.?
As in thisexample, nouns sometimes have argument structuresreferring to an event.
Such nouns are called eventnouns (Komachi et al, 2007) in the NAIST TextCorpus.
At the same time, the problems related tocompound nouns are also important.
In Japanese, acompound noun sometimes simultaneously containsboth an event noun and its arguments.
For example,the compound noun, ?????
(corporate buyout)?contains an event noun ???
(buyout)?
and its ac-cusative, ???
(corporate).?
However, compound1Kyoto Text Corpus has about 15 case roles.523nouns provide no information about syntactic de-pendency or about case markers, so it is difficult tospecify the predicate-argument structure.
Komachiet al investigated the argument structure of eventnouns using the co-occurrence of target nouns andtheir case roles in the same sentence (Komachi etal., 2007).
In these approaches, predicates and eventnouns are dealt with separately.
Here, we try tounify these different argument structures using de-cision lists.As regards ii), for example, in the causative sen-tence, ????????????????
(Marymakes Tom fix dinner),?
the basic form of thecausative verb, ?????
(make fix)?
is ???
(fix),?and its nominative is ???
(Tom)?
and the ac-cusative case role (wo-case) is ???
(dinner),?
al-though the surface case particle is ni (dative).
Wemust deal with syntactic transformations in passive,causative, and benefactive constructions when ana-lyzing the corpus.As regards iii) and iv), in Japanese, zero pronounsoften occur, especially when the argument has al-ready been mentioned in previous sentences.
Therehave been many studies of zero-pronoun identifica-tion (Walker et al, 1994) (Nakaiwa, 1997) (Iida etal., 2006).In this paper, we present a general procedure forhandling both the case role assignment of predicatesand event nouns, and zero-pronoun identification.We use the decision list learning of rules to find theclosest words with various constraints, because withdecision lists the readability of learned lists is highand the learning is fast.The rest of this paper is organized as follows.
Wedescribe the NAIST Text Corpus, which is our tar-get corpus in Section 2.
We describe our proposedmethod in Section 3.
The result of experiments us-ing the NAIST Text Corpus and our method are re-ported in Section 4 and our conclusions are providedin Section 5.2 NAIST Text CorpusIn the NAIST Text Corpus, three major obligatoryJapanese case roles are annotated, namely the ga-case (nominative or subjective case), the wo-case(accusative or direct object) and the ni-case (da-tive or in-direct object).
The NAIST Text Corpusis based on the Kyoto Text Corpus Ver.
3.0, whichcontains 38,384 sentences in 2,929 texts taken fromnews articles and editorials in a Japanese newspaper,the ?Mainichi Shinbun?.We divided these case roles into four types by lo-cation in the article as in (Iida et al, 2006), i) thecase role depends on the predicate or the predicatedepends on the case role in the intra-sentence (?de-pendency relations?
), ii) the case role does not de-pend on the predicate and the predicate does not de-pend on the case role in the intra-sentence (?zero-anaphoric (intra-sentential)?
), iii) the case role isnot in the sentence containing the predicate (?zero-anaphoric (inter-sentential)?
), and iv) the case roleand the predicate are in the same phrase (?in samephrase?).
Here, we do not deal with exophora.We show the distribution of the above four typesin test samples in our split of the NAIST TextCorpus in Tables 1 and 2.
In predicates, the?dependency relations?
type in the wo-case andthe ni-case occur frequently.
In event nouns,the ?zero-anaphoric (intra-sentential)?
and ?zero-anaphoric (inter-sentential)?
types in the ga-case oc-cur frequently.
With respect to the ?in same phrase?type, the wo-case occurs frequently.3 Predicate Argument Structure Analysisusing Features of Closest WordsIn this section, we describe our algorithm.
In thealgorithm, we used various constraints when search-ing for the words located closest to the target predi-cate.
We described these constraints as features withthe direct products of dependency types (ic, oc, ga c,wo c, ni c, sc, nc, fw and bw), generalization levels(words, semantic categories, parts of speech), func-tional words and voices.3.1 Dependency TypesIn Japanese, the functional words in a phrase (Bun-setsu in Japanese) and the interdependency of bun-setsu phrases are important for determining thepredicate argument structure.
In accordance withthe character of the dependency between the caseroles and the predicates or event nouns, we dividedJapanese word dependency into the following seventypes that cover all dependency types in Japanese.Additionally, we use two optional dependency types.524Table 1: Distribution of case roles for predicates (Test Data)predicatega (Nominative) wo (Accusative) ni (Dative)all 15,996 (100.00%) 8,348 (100.00%) 4,871 (100.00%)dependency relations 9,591 ( 59.96%) 7,184 ( 86.06%) 4,276 ( 87.78%)zero-anaphoric (intra-sentential) 3,856 ( 24.11%) 870 ( 10.42%) 360 ( 7.39%)zero-anaphoric (inter-sentential) 2,496 ( 15.60%) 225 ( 2.70%) 132 ( 2.71%)in same phrase 53 ( 0.33%) 69 ( 0.83%) 103 ( 2.11%)Table 2: Distribution of case roles for event nouns (Test Data)event nounga (Nominative) wo (Accusative) ni (Dative)all 4,099 (100.00%) 2,314 (100.00%) 423 (100.00%)dependency relations 977 (23.84%) 648 (28.00%) 105 (24.82%)zero-anaphoric (intra-sentential) 1,672 (40.79%) 348 (15.04%) 135 (31.91%)zero-anaphoric (inter-sentential) 1,040 (25.37%) 165 (7.13%) 44 (10.40%)in same phrase 410 (10.00%) 1,153 (49.83%) 139 (32.86%)Figure 1: Type ic3.1.1 Incoming Connection Type (ic)With this type, the target case role is the head-word of a bunsetsu phrase and the case role phrasedepends on the target predicate phrase (Figure 1).3.1.2 Outgoing Connection Type (oc)With this type, the target case role is the headwordof a phrase and a phrase containing a target predicateor event noun depends on the case role phrase (Fig-ure 2).Figure 2: Type oc525Figure 3: Type scFigure 4: Type ga c, wo c, ni c3.1.3 ?Within the Same Phrase?
Type (sc)With this type, the target case role and the targetpredicate or event noun are in the same phrase (Fig-ure 3).3.1.4 ?Connection into Other Case role Types(ga c, wo c, ni c)With these types, a phrase containing the targetcase role depends on a phrase containing anotherpredetermined case role (Figure 4).
We use the terms?ga c?, ?wo c?
and ?ni c?
when the predeterminedcase roles are the ga-case, wo-case and ni-case, re-spectively.Figure 5: Type nc3.1.5 Non-connection Type (nc)With this type, a phrase containing the target caserole and a phrase containing the target predicate orevent noun are in the same article, but these phrasesdo not depend on each other (Figure 5).3.1.6 Optional Type (fw and bw)Type fw and bw stand for ?forward?
and ?back-ward?
types, respectively.
Type fw means the wordlocated closest to the target predicate or event nounwithout considering functional words or voices.With fw, the word is located between the top of thearticle containing the target predicate and the targetpredicate or event noun.
Similarly, type bw meansthe word located closest to the target predicate ornoun, which is located between the targeted predi-cate or event noun, and the tail of the article con-taining the predicate.3.2 Generalization LevelsWe used three levels of generalization for every caserole candidate, that is, word, semantic category, andpart of speech.
Every word is annotated with a partof speech in the Kyoto Text Corpus, and we usedthese annotations.
With regard to semantic cate-gories, we annotated every word with a semanticcategory based on a Japanese thesaurus, NihongoGoi Taikei.
The thesaurus consists of a hierarchyof 2,710 semantic classes, defined for over 264,312nouns, with a maximum depth of twelve (Ikehara etal., 1997).
We mainly used the semantic classes of526Figure 6: Top 3 levels of the Japanese thesaurus, ?Ni-hongo Goi Taikei?the third level, and partly the fourth level, which aresimilar to semantic roles.
We show the top three lev-els of the Nihongo Goi Taikei common noun the-saurus in Figure 6.
We annotated the words withtheir semantic category by hand.3.3 Functional Word and VoiceWe used a functional word in the phrase containingthe target case role and active and passive voices forthe predicate as base features.3.4 Training AlgorithmThe training algorithm used for our method is shownin Figure 7.
First, the algorithm constructs featuresthat search for the words located closest to the tar-get predicate under various constraints.
Next, thealgorithm learns by using linear Support Vector Ma-chines (SVMs) (Vapnik, 1995).
SVMs learn effec-tive features by the one vs. rest method for everycase role.
We used TinySVM 2 as an SVM imple-mentation.
Moreover, we construct decision listssorted by weight from linear SVMs.
Finally, the al-gorithm calculates the existing probabilities of caseroles for every predicate or event noun.
This step2http://chasen.org/t?aku/software/TinySVM/produces the criterion that decides whether or notwe will determine the case roles when there is no in-terdependency between the case role candidate andthe predicate.Our split of the NAIST Text Corpus has only62,264 training samples for 2,874 predicates, and wepredict that there will be a shortage of training sam-ples when adopting traditional learning algorithms,such as learning algorithms using entropy.
So, weused SVMs with a high generalization capability tolearn the decision lists.3.5 Test AlgorithmThe test algorithm of our method is shown in Fig-ure 8.
In the test phase, we analyzed test samplesusing decision lists and the existing probabilities ofcase roles learned in the training phase.
In step 1, wedetermined case roles using a decision list consistingof features exhibiting case role and predicate inter-dependency, that is, ic, oc, ga c, wo c, and ni c. Thisis because there are many cases in Japanese wherethe syntactic constraint is stronger than the seman-tic constraint when we determine the case roles.
Instep 2, we determined case roles using a decision listof sc (?in same phrase?)
for the case roles that werenot determined in step 1.
This step was mainly forevent nouns.
Japanese event nouns frequently formcompound nouns that contain case roles.
In step 3,we decided whether or not to proceed to the nextstep by using the existing probabilities of case roles.If the probability was less than a certain threshold(50%), then the algorithm stopped.
In step 4, we de-termined case roles using a decision list of the fea-tures that have no interdependency, that is, nc, fwand bw.
This step will be executed when the targetcase role is syntactically necessary and determinedby the co-occurrence of the case roles and predicateor event noun without syntactic clues, such as de-pendency, functional words and voices.4 Experimental Results4.1 Experimental SettingWe performed our experiments using the NAISTText Corpus 1.4?
(Iida et al, 2007).
We used49,527 predicates and 12,737 event nouns from arti-cles published from January 1st to January 11th andthe editorials from January to August as training ex-527for each predicate pi in all predicates appeared in the training corpus dofeature list(pi) = {} ; n ?
0clear (x, y)for each instance pij of pi, in the training corpus doClear order() for all featuresaij ?
the article including pijWij ?
the number of words in aijpred index ?
the word index of pij in aijfor (m = pred index?
1; m ?
1; m??)
don + +dep type = get dependency type(wm, pij)if dep type == ?ic?, ?nc?, ?ga c?, ?wo c?
or ?ni c?
then inc order(n, dep type, wm, pij)else if dep type == ?sc?
then inc order(n, dep type, ?
?, ??
)endifinc order(n, ?fw?, ?
?, ??
)if wm is the ga-case role then yn,ga ?
1 else yn,ga ?
0if wm is the wo-case role then yn,wo ?
1 else yn,wo ?
0if wm is the ni-case role then yn,ni ?
1 else yn,ni ?
0end forfor (m = pred index + 1; m ?
Wij ; m + +) don + +dep type = get dependency type(wm, pij)if dep type == ?oc?, ?nc?, ?ga c?, ?wo c?
or ?ni c?
then inc order(n, dep type, wm, pij)else if dep type == ?sc?
then inc order(n, dep type, ?
?, ??
)endifinc order(n, ?bw?, ?
?, ??
)if wm is the ga-case role then yn,ga ?
1 else yn,ga ?
0if wm is the wo-case role then yn,wo ?
1 else yn,wo ?
0if wm is the ni-case role then yn,ni ?
1 else yn,ni ?
0end forend forLearn linear SVMs using (x1, y1,ga), ..., (xn, yn,ga)Learn linear SVMs using (x1, y1,wo), ..., (xn, yn,wo)Learn linear SVMs using (x1, y1,ni), ..., (xn, yn,ni)Make the decision list for pi, sorting features by weight.Calculate the existing probabilities of case roles for pi.end forprocedure get dependency type(wm, pij)if phrase(wm) depends on phrase(pij) then return ?ic?else if phrase(pij) depends on phrase(wm) then return ?oc?else if phrase(wm) depends on phrase(pga) then return ?ga c?else if phrase(wm) depends on phrase(pwo) then return ?wo c?else if phrase(wm) depends on phrase(pni) then return ?ni c?else if phrase(wm) equals phrase(pij) then return ?sc?else return ?nc?end procedureprocedure inc order(n, dep type, func, voice)Set a feature fw = (wm, dep type, func, voice) ; order(fw)++ ; if order(fw) == 1 then xn,fw ?
1Set a feature fs = (sem(wm), dep type, func, voice) ; order(fs)++ ; if order(fs) == 1 then xn,fs ?
1Set a feature fp = (pos(wm), dep type, func, voice) ; order(fp)++ ; if order(fp) == 1 then xn,fp ?
1feature list(pi) ?
feature list(pi)?
{fw, fs, fp}end procedureFigure 7: Training algorithm528Step 1.
Determine case roles using a decision list concerning ic, oc, ga c, wo c and ni c.Step 2.
Determine case roles using a decision list concerning sc for undetermined case roles inStep.1.Step 3.
If the existing probability of case roles < 50 % then the program ends.Step 4.
Determine case roles using a decision list concerning nc, fw and bw types.Figure 8: Test algorithmamples.
We used 11,023 predicates and 3,161 eventnouns from articles published on January 12th and13th and the September editorials as developmentexamples.
And we used 19,501 predicate and 5,276event nouns from articles dated January 14th to 17thand editorials dated October to December as test ex-amples.
This is a typical way to split the data.We used the annotations in the Kyoto Text Corpusas the interdependency of bunsetsu phrases.
We usedboth individual and multiple words as case roles.
Weused the phrase boundaries annotated in the NAISTText Corpus in the training phase, and used thoseannotated automatically by our system using POSsand simple rules in the test phase.
The accuracy ofthe automatic annotation is about 90%.4.2 Baseline MethodTo evaluate our algorithm, we conducted experi-ments using a baseline method.
With the method,we used only nouns that depended on predicates orevent nouns as case role candidates.
If the functionalword (post-positional case) in the phrase is ?ga?,?wo?and ?ni?, we determined the ga-case, wo-case, or ni-case for the candidates.
Next, as regards event nounsin compound nouns, if there was another word in acompound noun containing an event noun and it co-occurred with the event noun as a case role with ahigher probability in the training samples, then theword was selected for the case role.4.3 Entropy MethodThe conventional approach for making decision listsutilizes the entropy of samples selected by therules (Yarowsky, 1994) (Goodman, 2002).
We per-formed comparative experiments using Yarowsky?sentropy algorithm (Yarowsky, 1994).Table 3: Existing probabilities of case roles for predicatesand event nounsPredicate Existing Probabilityor Event Noun ga (NOM) wo (ACC) ni (DAT)??
(use) 44.72% 82.92% 5.33%??
(negotiation) 77.41% 30.70% 0.00%??
(participation) 87.09% 0.00% 72.46%???
(based on) 81.89% 0.00% 100.00%4.4 Overall ResultsThe overall results are shown in Table 7.
Here, ?en-tropy?
indicates Yarowsky?s algorithm, which usesentropy (Yarowsky, 1994).
Throughout the test data,the F-measure (%) of our method exceeded that ofthe baseline system and the ?entropy?
system.
Withthe ga-case (nominative) in particular, the F-measureincreased 9 points.Table 3 shows some examples of the existingprobabilities of case roles for predicates or eventnouns.
When the probabilities are extreme valuessuch as the ni-case (dative) of??
(negotiation), thewo-case (accusative) of??
(participation), and thewo-case and ni-base of ???
(based on), we candecide to fill the targeted case role or not with highprecision.
However, it is difficult to decide to fillthe targeted case role or not when the probability isclose to 50 percent as in the ga-case of??
(use).We show the learned decision list of the ic type(the case role depends on the predicate or eventnoun), sc type (in the same phrase) and the othertypes for event noun??
(negotiation) in Tables 4, 5and 6, respectively.
Here, ?word?
in the ?level?column means ?base form of predicate?
and ?sem?means ?semantic category of predicate.?
In the icand sc type decision lists, features with semanticcategories, such as ?REGION?, ?LOCATION?
and?EVENT?, occupy a higher order.
In contrast, inthe list of the other types, the features that occupythe higher order are the features of the word base529Table 4: Decision list for ic type of event noun??
(negotiation)order case dep type level head word functional voice weightword1 ga ic word ????????
(North Korea) ?
(of) active 0.98202 ga ic sem ??
(REGION) ?
(of) active 0.63813 ga ic word ????
(both Japan and U.S.) ?
(of) active 0.55024 wo ic word ??????
(establishment of joint ventures) ?
(of) active 0.52885 wo ic word ??????
(telecommunications) ?
(of) active 0.41426 wo ic word ????????
(North Korea) ??
(for) active 0.31687 wo ic word ??
(ACTION) ?
(of) active 0.30838 ga ic sem ????
(OOV NOUN) ?
(of) active 0.29399 wo ic word ?????????
(car and auto parts sector) ?
(of) active 0.277510 wo ic sem ?
(LOCATION) ?
(of) active 0.2471Table 5: Decision list for sc type of event noun??
(negotiation)order case dep type level head word weight1 wo sc sem ??
(EVENT) 1.17382 wo sc word ??
(arrangement) 1.00003 ga sc word ????
(airline of Japan and China) 0.93924 wo sc sem ??
(MENTAL STATE) 0.89585 ga sc word ??????????
(financial services of Japan and U.S.) 0.83716 wo sc word ????
(contract extension) 0.78707 wo sc word ??
(joint venture) 0.78658 wo sc word ?????
(intellectual property rights) 0.72249 wo sc word ???????
(car and auto parts) 0.719610 ga sc word ??
(Japan and North Korea) 0.6771Table 6: Decision list for other types of event noun??
(negotiation)order case dep type level head word functional word voice weight1 ga fw word ??
(Japan and U.S.) 1.99542 ga fw word ??
(Taiwan) 1.99523 ga fw word ??
(U.S. and North Korea) 1.49794 ga fw word ??
(U.K. and China) 1.17735 ga nc word ??
(both nations) ?
(TOP) active 1.13796 wo fw word ?????
(diplomatic normalization) 1.00007 ga bw word ??
(U.S. and North Korea) 1.00008 ga fw word ??
(capital and labor) 1.00009 wo fw word ?????
(automotive area) 1.000010 ga nc word ??
(both sides) ?
(TOP) active 1.0000Table 7: Overall results for NAIST Text Corpus (F-measure(%))training data test datasentence ga (NOM) wo (ACC) ni (DAT) sentence ga (NOM) wo (ACC) ni (DAT)baseline 25.32 32.58 74.51 82.70 21.34 30.08 69.48 76.62entropy 73.46 89.53 92.72 91.09 33.10 45.67 73.28 77.77our method 64.81 86.76 92.52 92.20 38.06 55.07 75.82 80.45530Table 8: Results for predicates in test sets (F-measure(%))baseline / our methodga (Nominative) wo (Accusative) ni (Dative)all 34.44 / 57.40 77.00 / 79.50 79.83 / 83.15dependency relations 51.96 / 75.53 85.42 / 88.20 81.83 / 89.51zero-anaphoric (intra-sentential) 0.00 / 30.15 0.00 / 11.41 0.00 / 3.66zero-anaphoric (inter-sentential) 1.85 / 23.45 3.00 / 9.32 0.00 / 11.76in same phrase 0.00 / 75.00 0.00 / 51.78 0.00 / 84.65Table 9: Results for event nouns (F-measure(%))baseline / our methodga (Nominative) wo (Accusative) ni (Dative)all 11.05 / 45.64 32.30 / 61.80 20.85 / 38.88dependency relations 12.98 / 68.01 25.00 / 62.46 40.00 / 56.05zero-anaphoric (intra-sentential) 0.00 / 36.19 0.00 / 20.46 0.00 / 6.62zero-anaphoric (inter-sentential) 1.40 / 23.25 1.06 / 10.37 0.00 / 3.51in same phrase 58.76 / 78.93 47.44 / 77.96 28.91 / 58.13form.
This means local knowledge of relations be-tween case roles and predicates or event nouns inthe word level is more important than semantic levelknowledge.4.5 Results for Predicates in Test SetsWe show the results we obtained for predicates inTable 8.
The results reveal that our method is supe-rior to the baseline system.
Our algorithm is partic-ularly effective in the ga-case.4.6 Results for Event Nouns in Test SetsWe show the results we obtained for event nouns inTable 9.
This also shows that our method is superiorto the baseline system.
The precision with sc typeis high and our method is effective as regards eventnouns.5 ConclusionWe presented a new method for Japanese automaticpredicate argument structure analysis using deci-sion lists based on the features of the words locatedclosest to the target predicate under various con-straints.
The method learns the relative weights ofthese different features for case roles and ranks themusing decision lists.
Using our method, we inte-grated the knowledge of case role determination andzero-pronoun identification, and generally achieveda high precision in Japanese PAS analysis.
In par-ticular, we can extract knowledge at various levelsfrom the corpus for event nouns.
In future, we willuse richer constraints and research better ways ofdistinguishing whether or not cases are obligatory.AcknowledgmentsWe thank Ryu Iida and Yuji Matsumoto of NAISTfor the definitions of the case roles in the NAISTText Corpus and functional words, and FranklinChang for valuable comments.ReferencesCharles J. Fillmore, Charles Wooters, and Collin F.Baker.
2001.
Building a large lexical databank whichprovides deep semantics.
In Proc.
of the Pacific AsianConference on Language, Information and Computa-tion (PACLING).Joshua Goodman.
2002.
An incremental decisionlist learner.
In Proc.
of the ACL-02 Conferenceon Empirical Methods in Natural Language Process-ing(EMNLP02), pages 17?24.Kouichi Hashida.
2005.
Global document annotation(GDA) manual.
http://i-content.org/GDA/.Lynette Hirschman, Patricia Robinson, Lisa Ferro, NancyChinchor, Erica Brown, Ralph Grishman, and BethSundheim.
1999.
Hub-4 Event?99 general guidelines.Ryu Iida, Kentaro Inui, and Yuji Matsumoto.
2006.
Ex-ploiting syntactic patterns as clues in zero-anaphoraresolution.
In Proc.
of the 21st International Confer-531ence on Computational Linguistics and 44th AnnualMeeting of the ACL, pages 625?632.Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji Mat-sumoto.
2007.
Annotating a Japanese text corpuswith predicate-argument and coreference relations.
InProc.
of ACL 2007 Workshop on Linguistic Annota-tion, pages 132?139.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentaro Ogura, YoshifumiOoyama, and Yoshihiko Hayashi.
1997.
Nihongo GoiTaikei, A Japanese Lexicon.
Iwanami Shoten, Tokyo.Zheng Ping Jiang and Hwee Tou Ng.
2006.
Semanticrole labeling of NomBank: A maximum entropy ap-proach.
In Proc.
of the Conference on Empirical Meth-ods in Natural Language Processing.Daisuke Kawahara, Sadao Kurohashi, and KoichiHashida.
2002.
Construction of a Japanese relevance-tagged corpus (in Japanese).
Proc.
of the 8th AnnualMeeting of the Association for Natural Language Pro-cessing, pages 495?498.Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Mat-sumoto.
2007.
Learning-based argument structureanalysis of event-nouns in Japanese.
In Proc.
of theConference of the Pacific Association for Computa-tional Linguistics (PACLING), pages 120?128.Chang Liu and Hwee Tou Ng.
2007.
Learning predictivestructures for semantic role labeling of NomBank.
InProc.
of the 45th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 208?215.Gabor Melli, Yang Wang, Yudong Liu, Mehdi M.Kashani, Zhongmin Shi, Baohua Gu, Anoop Sarkar,and Fred Popowich.
2005.
Description of SQUASH,the SFU question answering summary handler for theDUC-2005 summarization task.
In Proc.
of DUC2005.Adam Meyers, Ruth Reeves, Catherine Macleod, RachelSzekely, Veronika Zielinska, Brian Young, and RalphGrishman.
2004.
The NomBank project: An interimreport.
In Proc.
of HLT-NAACL 2004 Workshop onFrontiers in Corpus Annotation.Hiromi Nakaiwa.
1997.
Automatic identification of zeropronouns and their antecedents within aligned sen-tence pairs.
In Proc.
of the 3rd Annual Meeting ofthe Association for Natural Language Processing (inJapanese).Srini Narayanan and Sanda Harabagiu.
2004.
Questionanswering based on semantic structures.
In Proc.
ofthe 20th International Conference on ComputationalLinguistics (COLING).M.
Palmer, P. Kingsbury, and D. Gildea.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Sameer Pradhan, Waybe Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2004.
Shallow seman-tic parsing using support vector machines.
In Proc.of the Human Language Technology Conference/NorthAmerican Chapter of the Association of Computa-tional Linguistics HLT/NAACL 2004.Dan Shen and Mirella Lapata.
2007.
Using semanticroles to improve question answering.
In Proc.
of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP/CoNLL), pages 12?21.V.
Vapnik.
1995.
The Nature of Statistical Learning The-ory.
Springer-Verlag, New York.M.
Walker, M. Iida, and S. Cote.
1994.
Japanese dis-course and the process of centering.
ComputationalLinguistics, 20(2):193?233.Nianwen Xue.
2006.
Semantic role labeling of nomi-nalized predicates in Chinese.
In Proc.
of the HLT-NAACL, pages 431?438.David Yarowsky.
1994.
Decision lists for lexical am-biguity resolution: Application to accent restorationin Spanish and French.
In Proc.
of the 32nd AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 88?95.532
