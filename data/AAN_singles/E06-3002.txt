What Humour Tells Us About Discourse TheoriesArjun KarandeIndian Institute of Technology KanpurKanpur 208016, Indiaarjun@iitk.ac.inAbstractMany verbal jokes, like garden path sen-tences, pose difficulties to models of dis-course since the initially primed interpre-tation needs to be discarded and a new onecreated based on subsequent statements.The effect of the joke depends on thefact that the second (correct) interpretationwas not visible earlier.
Existing modelsof discourse semantics in principle gen-erate all interpretations of discourse frag-ments and carry these until contradicted,and thus the dissonance criteria in humourcannot be met.
Computationally, main-taining all possible worlds in a discourseis very inefficient, thus computing only themaximum-likelihood interpretation seemsto be a more efficient choice on average.In this work we outline a probabilisticlexicon based lexical semantics approachwhich seems to be a reasonable constructfor discourse in general and use some ex-amples from humour to demonstrate itsworking.1 IntroductionConsider the following :(1) I still miss my ex-wife, but my aim isimproving.
(2) The horse raced past the barn fell.In a discourse structure common to many jokes,the first part of (1) has a default set of interpre-tations, say P1, for which no consistent interpre-tation can be found when the second part of thejoke is uttered.
After a search, the listener reachesP2P1J2J1time tTPI still miss my ex-wife, but my aim is improvingsearchgapFigure 1: Cognitive model of destructive disso-nance as in joke (1).
The initial sentence primesthe possible world P1 where miss is taken in anemotional sense.
After encountering the word aimthis is destroyed and eventually a new world P2arises where miss is taken in the physical sense.the alternate set of interpretations P2 (Figure 1).A similar process holds for garden path sentencessuch as (2), where the default interpretation cre-ated in the first part (upto the word barn) has to bediscarded when the last part is heard.
The searchinvolved in identifying the second interpretationis an important indicator of human communica-tion, and linguistic impairment such as autism of-ten leads to difficulty in identifying jokes.Yet, this aspect of discourse is not sufficientlyemphasized in most computational work.
Cog-nitively, this is a form of dissonance, a violationof expectation.
However, unlike some forms ofdissonance which may be constructive, leading tometaphoric or implicature shifts, where part of theoriginal interpretation may be retained, these dis-course structures are destructive, and the origi-nal interpretation has to be completely abandoned,and a new one searched out (Figure 2).
Oftenthis is because the default interpretation involvesa sense-association that has very high coherencein the immediate context, but is nullified by later31P1 P2 P1 P2 P2P1P1 P2(a) (b)(d)(c)Figure 2: Cognitive Dissonance in Discourse (a-c)can be Constructive, where the interpretation P1does not disappear completely after the dissonantutterance, or (d) Destructive, where P2 has to bearrived at afresh and P1 is destroyed completely.utterances.While humour may involve a number of othermechanisms such as allusion or stereotypes (Shi-bles, 1989; Gruner, 1997), a wide class of ver-bal humour exhibits destructive dissonance.
Fora joke to work, the resulting interpretation mustresult in an incongruity, what (Freud, 1960) callsan ?energy release?
that breaks the painful barrierswe have around forbidden thoughts.Part of the difficulty in dealing with such shiftsis that it requires a rich model of discourse se-mantics.
Computational theories such as theGeneral Theory of Verbal Humour (Attardo andRaskin, 1991) have avoided this difficult prob-lem by adopting extra-linguistic knowledge in theform of scripts, which encode different opposi-tions that may arise in jokes.
Others (Minsky,1986) posit a general mechanism without con-sidering specifics.
Other models in computationhave attempted to generate jokes using templates(Attardo and Raskin, 1994; Binsted and Ritchie,1997) or recognize jokes using machine learningmodels (Mihalcea and Strapparava, 2005).Computationally, the fact that other less likelyinterpretations such as P2 are not visible initially,may also result in considerably efficiency in morecommon situations, where ambiguities are notgenerated to begin with.
For example, in joke(1) the interpretation after reading the first clause,has the word miss referring to the abstract act ofmissing a dear person.
After hearing the punchline, somewhere around the word aim, (the triggerpoint TP ), we have to revise our interpretation toone where miss is used in a physical sense, as inshooting a target.
Then, the forbidden idea of hurt-ing ex-wives generates the humour.
By hiding thismeaning, the mechanism of destructive dissonanceenables the surprise which is the crux of the joke.In the model proposed here, no extra-linguisticsources of knowledge are appealed to.
LexicalSemantics proposes rich inter-relations encodingknowledge within the lexicon itself (Pustejovksy,1995; Jackendoff, 1990), and this work consid-ers the possibility that such lexicons may eventu-ally be able to carry discourse interpretations aswell, to the level of handling situations such as thedestructive transition from a possible-world P1 topossible world P2.
Clearly, a desideratum in sucha system would be that P1 would be the preferredinterpretation from the outset, so much so that P2,which is in principle compatible with the joke, isnot even visible in the first part of the joke.
Itwould be reasonable to assume that such an inter-pretation may be constructed as a ?Winner TakeAll?
measure using probabilistic inter-relations inthe lexicon, built up based on usage frequencies.This would differ from existing theories of dis-course in several ways, as will be illustrated in thefollowing sections.2 Models of DiscourseFormal semantics (Montague, 1973) looked at log-ical structures, but it became evident that lan-guage builds up on what is seemingly semanticincompatibility, particularly in Gricean Implica-ture (Grice, 1981).
It became necessary to lookat the relations that describe interactions betweensuch structures.
(Hobbs, 1985) introduces an earlytheory of discourse and the notion of coherencerelations, which are applied recursively on dis-course segments.
Coherence relations, such asElaboration, Explanation and Contrast, are rela-tions between discourse units that bind segmentsof text into one global structure.
(Grosz and Sid-ner, 1986) incorporates two more important no-tions into its model - the idea of intention and fo-cus.
The Rhetorical Structure Theory, introducedin (Mann and Thompson, 1987), binds text spanswith rhetorical relations, which are discourse con-nectives similar to coherence relations.The Discourse Representation Theory (DRT)(Kamp, 1984) computes inter-sentential anaphoraand attempts to maintain text cohesion throughsets of predicates, termed Discourse Representa-tion Structures (DRSs), that represent discourse32No one doesHe can still walk by himselfExplanationWho supports Gorbachev?Question-answerpairFigure 3: Rhetorical Relations for joke (3)units.
A Principal DRS accumulates informationcontained in the text, and forms the basis for re-solving anaphora and discourse referents.By marrying DRT to a rich set of rhetoricalrelations, Segmented Discourse RepresentationTheory (SDRT) (Lascarides and Asher, 2001)attempts to to create a dynamic framework thattries to bridge the semantic-pragmatic interface.It consists of three components - UnderspecifiedLogical Formulae (ULF), Rhetorical Relationsand Glue Logic.
Semantic representation inthe ULF acts as an interface to other levels.Information in discourse units is represented bya modified version of DRS, called SegmentedDiscourse Representation Structures (SDRSs).SDRSs are connected through rhetorical relations,which posit relationships on SDRSs to bind them.To illustrate, consider the discourse in (3):(3) Who supports Gorbachev?
No one does,he can still walk by himself!The rhetorical relations over the discourse areshown in Figure 3.
Here, Explanation inducessubordination and implies that the content of thesubordinate SDRSs work on further qualifying theprincipal SDRS, while Question-Answer Pair in-duces coordination.
Rhetorical relations thus con-nect semantic units together to formalize the flowin a discourse.
SDRT?s Glue Logic then runs se-quentially on the ULF and rhetorical relations toreduce underspecification and disambiguation andderive inferences through the discourse.
The wayinferencing is done is similar to DRT, with the ad-ditional constraints that rhetorical relations spec-ify.A point to note is SDRT?s Maximum Dis-course Coherence (MDC) Principle.
This princi-ple is used to resolve ambiguity in interpretationby maximizing discourse coherence to obtain thePragmatically Preferred interpretation.
There arethree conditions on which MDC works: (a) Themore rhetorical relations there are between twounits, the more coherent the discourse.
(b) Themore anaphorae that are resolved, the more coher-ent the discourse.
(c) Some rhetorical relationscan be measured for coherence as well.
For ex-ample, the coherence of Contrast depends on howdissimilar its connected prepositions are.
SDRTuses rhetorical relations and MDC to resolve lex-ical and semantic ambiguities.
For example, inthe utterance ?John bought an apartment.
But herented it?, the sense of rented is that of rentingout, and that is resolved in SDRT because the wordbut cues the relation Contrast, which prefers an in-terpretation that maximizes semantic contrast be-tween its connectives.Glue logic works by iteratively extracting sub-sets of inferences through the flow of the dis-course.
This is discussed in more detail later.2.1 Lexicons for Discourse modelingPustejovsky?s Generative Lexicon (GL) model(Pustejovksy, 1995) outlines an ambitious attemptto formulate a lexical semantics framework thatcan handle the unboundedness of linguistic ex-pressions by providing a rich semantic structure,a principled ontology of concepts (called qualia),and a set of generative devices in which partici-pants in a phrase or sentence can influence eachother?s semantic properties.The ontology of concepts in GL is hierarchi-cal, and concepts that exhibit similar behaviourare grouped together into subsystems called Lexi-cal Conceptual Paradigms (LCP).
As an example,the GL structure for door is an LCP that representsboth the use of door as a physical object such as in?he knocked on the door?, as well as an aperturelike in ?he entered the door?.In this work, we extend the GL structures to in-corporate likelihood measures in the ontology andthe event structure relations.
The ProbabilisticQualia Structure, which outlines the ontologicalhierarchy of a lexical item, also encodes frequencyinformation.
Every time the target word appearstogether with an ontologically connected concept,the corresponding qualia features are strength-ened.
This results in a probabilistic model ofqualia features, which can in principle determine33that a book has read as its maximally likely telicrole, but that in the context of the agent being theauthor, write becomes more likely.Generative mechanisms work on this semanticstructure to capture systematic polysemy in termsof type shifts.
Thus Type Coercion enforces se-mantic constraints on the arguments of a predicate.For example, ?He enjoyed the book?
is coerced to?He enjoyed reading the book?
since enjoy requiresan activity, which is taken as the telic role of theargument, i.e.
that of book.
Co-composition con-strains the type-shifting of the predicate by its ar-guments.
An example is the difference between?bake a cake?
(creating a new object) versus ?bakebeans?
(state change).
Finally, Selective Bindingtype-shifts a modifier based on the head.
For ex-ample, in ?old man?
and ?old book?, the propertybeing modified by old is shifted from physical-ageto information-recency.To accommodate for likelihoods in generativemechanisms, we need to incorporate conditionalprobabilities between the lexical and ontologicalentries that the mechanisms work on.
These prob-abilities can be stored within the lexicon itself orintegrated into the generative mechanisms.
In ei-ther case, mechanisms like Type Coercion shouldno longer exhibit a default behaviour - the coer-cion must change based on frequency of occur-rence and context.3 The Analysis of HumourThe General Theory of Verbal Humour (GTVH),introduced earlier, is a well-known computationalmodel of humour.
It uses the notion of scriptsto account for the opposition in jokes.
It modelshumour as two opposing and overlapping scriptsput together in a discourse, one of which isapparent and the other hidden from the reader tilla trigger point, when the hidden script suddenlysurfaces, generating humour.
However, the notionof scripts implies that there is a script for everyoccasion, which severely limits the theory.
On theother hand, models of discourse are more generaland do not require scripts.
However, they lack themechanism needed to capture such oppositions.In addition to joke (3), consider:(4) Two guys walked into a bar.
The thirdone ducked.The humour in joke (4) results from the polyse-mous use of the word bar.
The first sentence leadsus to believe that bar is a place where one drinks,but the second sentence forces us to revise our in-terpretation to mean a solid object.
GTVH woulduse the DRINKING BAR script before the triggerand the COLLISION script after.
Joke (3), quotedin Raskin?s work as well, contains an obvious op-position.
The first sentence invokes the sense ofsupport being that of political support.
The secondsentence introduces the opposition, and the mean-ing of support is changed to that of physical sup-port.In all examples discussed so far, the keyobservations are that (i) a single inference isprimed by the reader, (ii) this primary inferencesuppresses other inferences until (iii) a triggerpoint is reached.To formalize the unfolding of a joke, we re-fer back to Figure 1.
Let t be a point along thetimeline.
When t < TP , both P1 and P2 are com-patible, and the possible world is P = P1 ?
P2.P1 is the preferred interpretation and P2 is hidden.When t = TP , J2 is introduced, and P1 becomesincompatible with P2, and P1 may also losecompatibility with J2.
P2 now surfaces as thepreferred inference.
The reader has to invoke asearch to find P2, which is represented by thesearch gap.A possible world Pi = {qi1, qi2, .
.
.
, qik}where qmn is an inference.
Two worlds Pi and Pjare incompatible if there exists any pair of sets ofinferences whose intersection is a contradiction.i.e.Pi is said to be incompatible withPj iff ?
{qi1, qi2, .
.
.
, qik} ?
Pi ??
{qj1, qj2, .
.
.
, qjl} ?
Pj such that{qi1 ?
qi2 ?
.
.
.
qik ?
qj1 ?
qj2 ?
.
.
.
qjl} ?
F .They are said to be compatible if no such subsetsexist.We now explore in detail why compositionaldiscourse models fail to handle the mechanisms ofhumour.3.1 Beyond Scripts - Why Verbal HumourShould Be Winner Take AllAn argument against the approach of existing dis-course models like SDRT concerns their iterativeinferencing.
At each point in the process of infer-34encing, SDRT?s Glue Logic carries over all inter-pretations possible within its constraints as a set.MDC ranks contending inferences, allowing lesspreferred inferences to be discarded, and the resultof this process is a subset of the input to it.
Con-trasting inferences can coexist through underspec-ification, and the contrast is resolved when one ofthem loses compatibility.
This is cognitively un-likely; (Miller, 1956) has shown that the humanbrain actively retains only around seven units ofinformation.
With such a limited working mem-ory, it is not cognitively feasible to model dis-course analysis in this manner.
Cognitive modelsworking with limited-capacity short-term memorylike in (Lewis, 1996) support the same intuition.Thus, a better approach would be a Winner TakeAll (WTA) approach, where the most likely inter-pretation, called the winner, suppresses all otherinterpretations as we move through the discourse.The model must be revised to reflect new contextsif they are incompatible with the existing model.Let us now explore this with respect to joke(3).
There is a Question-Answer relation betweenthe first sentence and the next two.
The semanticrepresentation for the first sentence alone is:?x(support(x,Gorbachev)), x =?The x =?
indicates a missing referent forwho.
Using GL, it is not difficult to resolve thesense of support to mean that of political support.To elaborate, the lexical entry of Gorbachevis an LCP of two senses - that of the head ofgovernment and that of an animate, as shown:??????
?GorbachevARGSTR =[ARG1 =x: manARG2 =y: head of govtD-ARG3 =z: community]QUALIA =[human.president lcpFORMAL = p(x, y)TELIC = govern(y, z)]??????
?The two senses of support applicable in thiscontext are that of physical support and of politicalsupport.
We use abstract support as a generaliza-tion of the political sense.
The analysis of the firstsentence alone would allow for both these possi-bilities:??????
?supportabsARGSTR =[ARG1 =x: animateARG2 =y: abstract entity]EVENTSTR =[E1 = e1 : process]QUALIA =[FORMAL = supportabs act(e1, x, y)AGENTIVE =...]?????????????
?supportphyARGSTR =[ARG1 =x: physical entityARG2 =y: physical entity]EVENTSTR =[E1 = e1 : process]QUALIA =[FORMAL = supportphy act(e1, x, y)AGENTIVE =...]??????
?Thus, after the first sentence, the sense ofsupport includes both senses, i.e.
support ?
{supportabs, supportphy}.We then come across the second sentence andestablish the semantic representation for it, aswell as establish rhetorical relations.
We findthat the sentence contains walk(z).
SDRT?sRight Frontier Rule resolves the referent he toGorbachev.
Also, the clause ?no one does?resolves the referent x to null.
Thus, we get:walk(Gorbachev) ?
support(null,Gorbachev)Now consider the lexical entry for walk:????
?walkARGSTR =[ARG1 =x: animate]EVENTSTR =[E1 = e1 : process]QUALIA =[FORMAL = walk act(e1, x)AGENTIVE = walk begin(e1, x)]????
?The action walk requires an animate argument.Since walk(Gorbachev) is true, the sense of sup-port in the previous sentence is restricted to meanphysical support, i.e.
support = supportphy,since only supportphy can take an animate argu-ment as its object - the abstract entity require-ment of supportabs causes it to be ruled out, end-ing at a final inference.The change of sense for support is key to thegeneration of humour, but SDRT fails to recog-nize the shift since it neither has any primingmechanism nor revision of models built into it.It merely works by restricting the possible infer-ences as more information becomes available.
Re-ferring to Figure 1 again, SDRT will only accountfor the refinement of possible worlds from P1?P2to P2.
It will not be able to account for the primingof either Pi, which is required.4 A Probabilistic Semantic LexiconWe now introduce a WTA model under whichpriming could be well accounted for.
We wouldlike a model under which a single interpretation ismade at each point in the analysis.
We want a set35of possible worlds P such that:J1 ?
?WTA P ={p : p is a world consistent with J1}WTA ensures that only the prime world P ischosen by J1.
When J2 is analyzed, no worldp ?
P can satisfy J2, i.e:?p ?
P,?J2 ??
pIn this case, we need to backtrack and findanother set P ?
that satisfies both J1 and J2, i.e:(J1, J2) ?
?WTA P ?In Figure 1, P = P1 and P ?
= P2.The most appropriate way to achieve this isto include the priming in the lexicon itself.
Wepresent a lexical structure where senses of com-positional units are attributed with a probability ofoccurrence approximated by its frequency count.The probability of a composition can then be cal-culated from the individual probabilities.
Thehighest probability is primed.
Thus, at every pointin the discourse, only one inference emerges asprimary and suppresses all other inferences.
Asan example, the proposed structure for Gorbachevis presented below:????????
?GorbachevARGSTR =[ARG1 =x: manARG2 =y: head of govtD-ARG3 =z: community]QUALIA =??
?FORMAL = p(x, y)p(man) = p1p(head_of_govt) = p2...???????????
?Instead of using the concept of an LCP as inclassical GL, we assign probabilities to each senseencountered.
These probabilities can then facili-tate priming.To add weight to the argument with empiricaldata, we use WordNet (Fellbaum, 1998), built onthe British National Corpus, as an approximationfor frequency counts.
We find thatP (supportabs) = 0.59 andP (supportphy) = 0.36.Similarly, for the notion of Gorbachev, it isplausible to assume that Gorbachev as head ofgovernment is more meaningful for most of us,rather than just another old man.
In order to makean inference after the first sentence, we need tosearch for the correct interpretation, i.e.
we needto find argmaxi,j(P (supporti/Gorbachevj)),which intuitively should beP (supportabs/head of govt).
Making asimilar analysis as in the previous section,the second sentence should violate the firstassumption, since walk(Gorbachev) can-not be true (since P (abstract entity) = 0).Thus, we need to revise our inference, mov-ing back to the first sentence and choosingmax(P (supporti/Gorbachevj)) that is compati-ble with the second sentence.
This turns out to beP (supportphy/animate).
Thus, the distinct shiftbetween inferences is captured in the course ofanalysis.
Cognitive studies such as the studies onGarden Path Sentences strengthen this approachto analysis.
(Lewis, 1996), for example, presentsa model that predicts cognitive observations withvery limited working memory.Storing the inter-lexical conditional proba-bilities is also an issue, as mentioned ear-lier.
Where, for example, do we storeP (supporti/Gorbachevj)?
One possible ap-proach would be to store them with either lexicalitem.
A better approach would be to bestow the re-sponsibility of calculating these probabilities uponthe generative mechanisms of the semantic lexiconwhenever possible.Let us now analyze joke (1) under the prob-abilistic framework.
Again, approximations forprobability of occurrence will be taken fromWordNet.
The entry for wife in WordNet lists justone sense, and so we assign a probability of 1 to itin its lexical entry:?????
?wifeARGSTR =[ARG1 =x: womanD-ARG2 =y: man]QUALIA =[FORMAL = husband(x) = yAGENTIVE = marriage(x, y)p(woman) = 1]?????
?The humour is generated due to the lexical am-biguity of miss.
We list the lexical entries of thetwo senses of miss that apply in this context - thefirst being an abstract emotional state and the otherbeing a physical process.36But my aim is improvingI still miss my ex-wifeContrast, ParallelFigure 4: Rhetorical relations for joke (1)??????
?missabsARGSTR =[ARG1 =x: animateARG2 =y: entity]EVENTSTR =[E1 = e1 : state]QUALIA =[FORMAL = missabs act(e1, x, y)AGENTIVE =...]???????????????????
?missphyARGSTR =[ARG1 =x: physical entityARG2 =y: physical entityD-ARG1 =z: trajector]EVENTSTR =??
?E1 = e1 : processE2 = e2 : stateRESTR2 =<?HEAD2 = e2??
?QUALIA =[FORMAL = missedphy act(e2, x, y, z)AGENTIVE = shoot(e1, x, y, z)]????????????
?The Rhetorical Relations for joke (1) arepresented in Figure 4.
After parsing the firstsentence, the logical representation obtained is:?e1?e2?e3?x?y(wife(e1, x, y) ?divorce(e2, x, y)?miss(e3, x, y)?e1 < e2 < e3)To arrive at a prime inference, note that thesemantic types of the arguments of bothsenses of miss are exclusive, and henceP (physical entity/missphy) = 1 andP (entity/missabs) = 1.
Thus, using BayesTheorem, to compare P (missabs/entity) andP (missphy/physical entity), it is sufficient tocompare P (missabs) and P (missphy).
FromWordNet,P (missabs) = 0.22 andP (missphy) = 0.06.Thus, the primed inference has miss =missabs.
The second sentence has the followinglogical representation:?x(?goodness(aim(x)) > 0)This simply means that a measure of theaim, called goodness, is undergoing a positivechange.
The word but is a cue for a Contrastrelation between the two sentences, while thediscourse suggests Parallelism.
The two senses ofaim compatible with the first sentence are aimabs,which is synonymous to goal, and aimphy,referring to the physical sense of missing.
Wenow need to consider P (aimabs/missabs) andP (aimphy/missphy).
The semantic constraintsof the rhetorical relation Contrast ensures thatthe second is more coherent, i.e.
it is moreprobable that the contrast of physical aim get-ting better is more coherent with the physicalsense of miss, and we expect this to be re-flected in usage frequency as well.
ThereforeP (aimabs/missabs) < P (aimphy/missphy),and we need to shift our inference and makemiss = missphy.As a final assertion of the probabilistic ap-proach, consider:(5) You can lead a child to college, but youcannot make him think.The incongruity in joke (5) does not result froma syntactical or semantic ambiguity at all, and yetit induces dissonance.
The dissonance is not aresult of compositionality, but due to the accessof a whole linguistic structure, i.e.
we recall thefamiliar proverb ?You can lead a horse to waterbut you cannot make it drink?, and the deviationfrom the recognizable structure causes the viola-tion of our expectations.
Thus, access is not re-stricted to the lexical level; we seem to store andaccess bigger units of discourse if encountered fre-quently enough.
The only way to do justice to thisjoke would be to encode the entire sentential struc-ture directly into the lexicon.
Our model will nowalso consider these larger chunks, whose meaningis specified atomically.
The dissonance will nowcome from the semantic difference between theaccessed expression and the one under analysis.5 ConclusionWe have examined the mechanisms behind verbalhumour and shown how existing discourse mod-els are inadequate at capturing the mechanismsof humour.
We have proposed a probabilistic37WTA model based on lexical frequency distribu-tions that is more capable at handling humour, andis based on the notion of expectation and disso-nance.It would be interesting now to find necessaryand sufficient conditions under this framework forhumour to be generated.
Although the aboveframework can identify incongruity in humour dis-course, the same mechanisms are used and indeedare often integral to other forms of literature.
Po-ems, for example, often rely on such mechanisms.Are Freudian thoughts the key to separating hu-mour from the rest, or is it a result of the inten-tional misleading done by the speaker of a joke?Also, it would be very interesting to find an empir-ical link between the extent of incongruity in jokesin our framework and the way people respond tothem.Finally, a very interesting question is the acqui-sition of the lexicon under such a model.
How arelexical semantic models learned by the languageacquirer probabilistically?
An exploration of thequestion might result in a cognitively sound com-putational model for acquisition.ReferencesSalvatore Attardo and Victor Raskin.
1991.
Script the-ory revis(it)ed: Joke similarity and joke representa-tion model.
4(3):293?347.Savatore Attardo and Victor Raskin.
1994.
Non-literalness and non-bona-fide in language: An ap-proach to formal and computational treatments ofhumor.
volume 2, pages 31?69.Kim Binsted and Graeme Ritchie.
1997.
Computa-tional rules for generating punning riddles.
HU-MOR - International Journal of Humor Research,10(1):25?76.Christiane Fellbaum.
1998.
WordNet - An ElectronicLexical Database.
MIT Press.Sigmund Freud.
1960.
Jokes and their relation to theunconscious.
The Standard Edition of the CompletePsychological Works of Sigmund Freud.Herbet Paul Grice.
1981.
Presupposition and conver-sational implicature.
In Radical Pragmatics, pages183?197.
New York: Academic Press.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 12(3):175?204.Charles R. Gruner.
1997.
The Game of Humor:A Comprehensive Theory of Why We Laugh.
NJ:Transaction Publishers.Jerry R Hobbs.
1985.
On the coherence and structureof discourse.
Technical Report CSLI-85-37, Centerfor the Study of Language and Information, StanfordUniversity.Ray S. Jackendoff.
1990.
Semantic Structures.
MITPress.H.
Kamp.
1984.
A theory of truth and semantic rep-resentation.
In J. Groenendijk, T. M. V. Janssen,and M. Stokhof, editors, Truth, Interpretation andInformation: Selected Papers from the Third Ams-terdam Colloquium, pages 1?41.
Foris Publications,Dordrecht.Asher Lascarides and Nicolas Asher.
2001.
Seg-mented discourse representation theory: Dynamicsemantics with discourse structure.
ComputingMeaning, 3.Richard L. Lewis.
1996.
Interference in short-termmemory: The magical number two (or three) in sen-tence processing.
Journal of Psycholinguistic Re-search, 25(1):93 ?
115.William C. Mann and Sandra A. Thompson.
1987.Rhetorical structure theory: A theory of text orga-nization.
Technical Report ISI/RS-87-190, Centerfor the Study of Language and Information, Stan-ford University.Rada Mihalcea and Carlo Strapparava.
2005.
Makingcomputers laugh: Investigations in automatic humorrecognition.
In Joint Conference on Human Lan-guage Technology / Empirical Methods in NaturalLanguage Processing (HLT/EMNLP).George A. Miller.
1956.
The magical number seven,plus or minus two: Some limits on our capacityfor processing information.
Psychological Review,63:81?97.Marvin Minsky.
1986.
The Society of Mind.
Simonand Schuster.Richard Montague.
1973.
The proper treatment ofquantification in ordinary English.
In Approachesto Natural Language, pages 221?242.
D. Reidel.James Pustejovksy.
1995.
The Generative Lexicon.MIT Press.Warren Shibles.
1989.
Humor reference guide.http://facstaff.uww.edu/shiblesw/humorbook.38
