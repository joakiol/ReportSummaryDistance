Processing Homonyms in the Kana-to-Kanji ConversionMasahito TakahashiFukuoka University8-19-1, Nanakuma,Jonan-ku,  Fukuoka,814-01, Japantakahasi@helio.t l .fukuoka-u.ac.
jpTsuyoshi ShinchuFukuoka University8-19-1, Nanakulna,aonan-ku,  Fukuoka,814-01, Japanshinchu((}helio.tl.fukuoka-u.~u:.jpKen j i  Yosh imuraFukuoka University8-19-1, Nanakuma,Jonan-ku,  Fukuoka,814-01, Jal)anyosilnu ra<?~tlsun, tl.fukuoka-u.ac.
jpKosho ShudoFukuoka University8-19-1, Nanakuma,Jonan-ku,  Fukuoka,814-01, Jut)anshudo (~tlsun.tl.fukuoka-u.ac.jt)AbstractThis paper  I)roI)oses two new meth-ods to identify the correct meaning ofJapanese honmnyms in text based on tilei l oun :verb  co-occI l r renc(  ~, ill a sentencewhich (:an be obtained easily from cor-pora.
The first method uses the nearco-occurrence data sets, which are con-structed from the above (:o-occurrencerelation, to select the most fe~Lsible wordamong homonyms in the s(:ol)e of a sea>tence.
Ti le se(:ond uses the flu' co-occurrence data sets, which are con-s t rut ted  dynamica l ly  fl'om the near  co-occurrence data sets  in the course ofprocessing input sentences, to select themost feasible word among homonyms illthe s(:ope of a sequence of sentences.
Anexper iment of kana- to -kanf i (phonogran>to- ideograph) conversion has shown thatthe conversion is carried out at the ac-curacy rate of 79.6% per word by thefirst method.
This accuracy rate of ourmethod is 7.4% higher than that  of theordinary method based on the word oc-currence frequency.1 In t roduct ionProcessing hontonynLs, i.e.
identifying the correctmeaning of homonyms in text, is one of the mostimpor tant  phases of kana- to -kan j i  conversion, cur-rently the most popular  method for int)uttingJapanese characters into a computer .
Recently,severM new methods fi)r processing homonyms,based on neural networks(Kol)ayashi,1992) or tileco-occurrence relation of words(Yamamot<),1992), have been proposed.
These methods apl)ly tothe co-occurrence relation of words not only ina sentence  but  also ill a sequence of sentellC(~s.It appears  impra<:ticat)le to prepare a neural net-work for co-oecurren(:e data  large enough to han-dle 50,000 to 100,000 Japanese words.In this 1)aper, we propose two uew methods forprocessing Japanese homonyms based on the (:o-occurrence relation between a noun and a verb illa sentence .
We have defined two co-occurrencedata  sets.
One is a set of nouns ~compan ied  by acase marking particle, e~:h element of which has aset of co-occurring w~rbs in a sentence.
The otheris a set of verbs accompanied by a case markingpartMe,  each element of which has a set of co-occurring nouns in a sentence.
We (:all these tv~oco-occllrrence data  sets near  co-occur rence  datasets.
Thereafter,  we apply the data  sets to the1)ro<:essing of holuonylns.
Two strategies are usedto al>l)roach the problem.
The first uses the nearco-occurrence data sets to select the most feasibleword among homonyms in the scope of a sentence.The aim is to evaluate the possible existen<-e of anear  co-occurrence relat ion,  or co-occurrence re-lat ion betweeu a noun and a verb within a sen-tence.
The second ewfluates the possibh' existenceof a fa r  co-occurrence relat ion,  referring to a co-occurrence relation among words in different sen-tences.
This is achieved by construct ing f i tr  co-occurrence data sets from near  co-occurrence datasets  in the course of processing input sentences.2 Co-occur rence  data  setsThe near co-occurrence data sets are (lefined.The first near co-occurrence data set is the setEN ........ each element of which(n) is a tr iplet con-sisting of a noun, a case marking par tMe,  and aset of w~rl)s which co-occur with that  noun andl)artMe pair  in a sentence, as follows:n = (noun,  par  t ic le,  {(Vl, kl ), (v2, ~;2),"" })Ill this descript ion, par t i c le  is a Japanese casemarking particle, such as 7)'-'; (nominat ive case),(ac(:usative case), or tC (dative case), vi ( i  =1,2 , .
.
- )  is a verb, and k i ( i  ---- 1,2 , .
.
. )
is the fre-quency of occurren(:e of the combinat ion noun,par t i c le  and vl, which is del;ermined in the courseof construct ing EN ...... fi'om corpora.
The follow-ing are examl)les of the elements of EN ......
.
(g\[~ (rain), 7)~ (nominative case),{(~7~ (fal l ) ,10),( lk~2e(stop),3) ,  .
.
})(~( ra in ) ,  ~ (accusative case),{(~JT~-~Xa (take precautions),3), .
.
})1135The second near co-occurrence data set is theset By, ...... each element of which(v) is a tripletconsisting of a verb, a case marking partMe, anda set of nouns which co-occur with that verb andparticle pair in a sentence, as follows:v = (verb,particle, {(nt, ll), (n2,12),"' "})In this description, particle is a Japanese casemarking particle, ni(i = 1,2, .
.
. )
is a noun, andli(i : 1 ,2 , "  ") is the frequency of occurrence ofthe combination verb, particle and hi.
The follow-ing are examples of the elements of Ev, ..... .
Ev,~,~,.can be constructed fi'om E~ ..... , and vice versa.
(}Yo ( fa l l ) ,  7~ (nominat ive case) ,{(~\]~ ( ra in)  ,10) , (~  (snow) ,8) ,  .
.
})(~  (fall), }C (dative case),{(JIL'J'\]'\] (Kyushu), 1),  .
.
})3 P rocess ing  homonyms in  as imp le  sentenceUsing the near co-occurrcncc data set.s, the mostfeasible word among possible homonynls can 1)eselected within the scope of a sentence.
Our hy-pothesis tates that the most feasible noun o1' com-bination of nouns has the largest nulnber of verbswith which it can co-occur ill a sentence.The structure of an input Japanese sentencewritten in kana-characters can be simplified as fol-lows:N~ .
P~,N2 .
P=, .
.
.
,N , , ,  .
p,,,, vwhere Ni(i = 1 ,2 , .
.
.
,m)  is a noun, l ' i(i =1,2 , .
.
.
,m)  is a particle and V is a verb.3.1 ProcedureFollowing is the procedure for finding the mostfeasible combination of words for an input kana-string which has the above simplified Japanesesentence structure.
This procedure can also ac-cept an input kana-string which does not includea final position verb.S tep l  Let m = 0 and Ti = e(i = 1,2, .
.
. )
.Step2 If an input kana-string is null, go to Step4.Otherwise read one block of kana-string, thatis N ?
P or V, fi'om tile left side of the in-put kana-string.
And delete the one blockof kana-string fi'om the left side of the inlmtkana-string.Step3 Find all homonyinic ka,q/-variantsWk(k = 1,2, .
.
. )
for tile kana-string N or Vwhich is read in Step2.Increase m by 1.For each Wk(k = 1, 2,. .
.)
:1.
If W~ is a noun, retrieve (W~, P, V~,) fromtile near co-occurrence data ,set ~N .
.
.
.
.
.
.and add the doublet (W~, V~) to T,~.2.
If W~ is a verb, add the doublet(w~, {(w~,0)}) to T,,,.Go to Step2.S tep4 From Ti(i = 1 ,2 , .
.
.
,m) ,  find the combi-nation:(Wl, Vi)(W~, V.2),.-', (Win, Vm)(w ,  ~)  ~ T~(i = 1, 2 , .
.
.
,  ,,~)which has the largest value ofI N(v,, v.~,..., <,,) I.
Where the functionf-\](v,, v2 , .
.
.
,  v,,,) is deiilled as Mlows.
('\](Vl, v.~,..., v.D = {(v, y~.
k~) Ii=1(~, k,) e v, A. .
.
A (v, k,,,) c v,,,}And \] \["1(1/1, V x,..., V,,<) i is defined:I N(vl,  v.~,..., v,,,) I-- ~ k(~.k)6n(v, ,v.2 .....vm)The sequence of words WI ,W2, .
.
.
,W,~ isthe most feasible conibination of homonymicka'aji-w, riants for tile ini)ut kana-string.3.2 An  example  of  p rocess ing  homonymsin a s imple  sentenceFollowing is an example of homonynl processingnsiug the abow,, procedures.For the input kana-string"7~a~{C_l~ b,~e (kawa ni hashi o)""D~ (~a.~a)" means a riw'.r dud <'~ b (ha~h0"nleans a bridge. "
\ ]0~ (kawa)" and "~ 1_, (hashi)"both have honionyniic kanji-variants:ho l l lonyn ls  of  "7~a~\] " ) (\]gawa)" : )\[\] (river))59.
(leather)hon louyu ls  of "}'k'\[ 1~ (hashi)" : ~ (bridge)~ (chopsticks)The near co-occurrence data for ")il (river)" and"~ (leather)" followed by the particle "~:-(dativecase)" and tile near co-occurrence data for "~}(1,ridge)" and ".~ (chopsticks)" followed by tilet)article "~ (accusative case)" are shown below.
()ll ( r i ver ) ,  }C,{({-~< (go) ,8 ) ,(~-~70 (bu i ld ) ,6 ) ,('\]~:J- (drop) ,  5) })(~  ( leather ) ,  ~Y-,{("~Xa (pa in t ) ,6 ) ,(~\]~7o ( touch) ,3 )})(~(br idge) ,  "~,{(~7o (walk across ) ,9 ) ,( ~  (build),7),('~j- (drop) ,4) })(~(chopsticks), ~ ,{(~ (use),7),('~J- (drop),3)})Following tile procedure, the.
resultant frequencyvalues are as follows:)il~c ~ 8{i ~9-}~ -~ 0{}Therefore, the nlost feasible combination of wordsis "),l (river) }:- ~ (bridge) ~.
"11364 An exper iment  on processinghomonyms in a s imple senten(:e4.1 Prepar ing  a d ic t ionary  and aco-occurrence data file4.1.1 a noun f i leA noun file in<:luding 323 nOUllS, whi(:h con-sists oi" 190 nouns extra.l-ted front text concern-ing (:urrent topics ~til(t heir 133 holnol ly lns,  waspret)~m'd.4.1.2 a co-occurrence data fi lei (:o-occurrence (l~(;;~ ill(', was l)repa.r('d.
Therecord format of the file is specitied as folh)ws:\[I1()1111, C~:K'-le marking 1)~Lrticle, verl), tlwfrequency of occurrence\]wher(; case marking t)~trti(:le is (:hosen from 8 kindsof part ic les ,  i~mlely, "7)~"," ~" ," ~" ,  "~'-" ," &" ," 7) ~5"," J:.
9","'~".It includes 25,665 re.cords of co-o(:curr(mce re-l~d;ion (79 records per noun) for (;he nouns in thenollIl file by inerging 11,294 re(:ords fron~ EDRCo-o(:(:urrence Di(:tionary(EDR.,1994) with 15,856records from handmade simI)le sentences.4.1.3 an i nput  f i le and an answer  f i leAn intmt file for ml exl )er i lnent  , whi(:h in(:hules1,1.29 silnple sentences writ ten in ks'as a.ll)haJ)et ,and an ~mswer tile, which includes the same 1,129sentences wr i t ten in kanji  (:hzLra(:l;ers, were l)re-pared, i lere, every noun of the sel~ten(:es in thefiles was chosen fl'om the nOUl~ file.4.1.4 a word d ic t ionaryA word dict ionary, which consists of 323 lmunsin the llOUll file ;tlld 23,912 verbs in a, ,\]at)aJlesedict ionary for kana-to-kanfi  conversion ', w~Ls l)re -1)~r(~(1.
It is use(1 to find all honmnymi(: ka, nji-varimd;s for each noun or verb of the Sellt(Hices inthe input lilt.4.2 Exper iment  resu l t sAn exl)eriment on processing homonylns in at sim-1)le sentence was carried out.
In this exper-iment, kana-to-kanj i  conversion was N)plied toe.ach of tim sentences, or the inlmt kana-striugs,ill the al)ove input file ~md the 'near eo-occwrrc'acedata  sets  wer('.
(:onstrlt(:t;ed froll~ I;he ~d)ove co-o(:currence (la.t;t ill(.'.
Tal) lel  shows the resul(,sof kana-to-ka'aji (:onversion in the fofh)wing twocases, in tile first (n~se, ~LII inl)ut ks, us-str ing doesnot include ~L fired posit ion ver\]).
It lIl(~a.llS I;haJ;each verl) of the kana..strings ill the input file ixneglecte(l .
In the s(~,COll(l CILSe, a.ll till)Ill. \]go,'//,a-str ing includes a final posit ion verb.
The ('.xl)eri-Ill(Jilt h;ts showll  tfia?
the (:onversiou ix carried Olll;~t the accura.
(:y ra.te of 79.6% per word, wher(~the ('onversion r~te is 93.1% per word, in the first~This dictionary was m~Me by A1 Soft; Co.(:a.se.
\[n the stone way, the a(:curacy rate is 93.8%1)('r word, where the conversion r~Lte is 14.5% perword, in tile se(:ond (:a.se.
And then, we.
~flso (:on-(lu(:ted the sCLme exper iment by using the method1)ase(l on the.
word oc(:un'ence frequency to (:om-1)~m~ our iue(;hod with an or(lin~ry inethod.
It hasshowu that  the accuracy rate is 72.2% per wordin the tirst case, ~md 77.8% per woM in the sec-on(l (:~Lse.
We (:a,n lind tile ac(:ur;~cy r;Lte I)y ourmet;ho(l is 7.4% higher it, the first case ~md 16.9%higher ill the secolM case ('oml)~u'ed with the or-(lilmry m(~thod, it  is clarified th~Lt our method islnore etlective than the ordimLry method based onthe word O('Clll'relH:(*.
fre(l/tellcy.5 An approximat ion of the farco-occurrence relationW(, (:~m a,1)l)roxilua?e tile far co-occ'arrc,nce re-|at'ion, which is (:o-o(:curren(:e relation amongwords ill ~L seqllel lce of Sellt(~ll(zes, frolil 'li, C(l,'F co-occu'rrc'nce data sets.
The fivr co-occurrence datasets m'c descril)ed ~s follows:EN, .... = { (,,,,, t, ), (',,,~, t~) , .
.
.
,  ( ' ,<,  t , .
)}~,5 .... = ~(~,,., ' ,,), ( '~ , ' , ,~) , .
.
.
,  (< ,  'a,~)}where n i ( i  = 1 ,2 , .
.
.
,  1,,) is a noun,  t i is the 1)ri-or i ty wdue of hi, vi( i  = 1, 2 , .
.
.
, I v )  is ~t verb andui is the prior ity wdue of vi.The l)ro(:(~(lure for 1)roducing the f iw co-oceurre'n,c(', data .sets is:S tep l  Clem: the fa,'r co-oceu'r'rcnce data .sets.E~# .... = eS tep2  After e~tch fixing of noun N ,mmng homo-nyms in tim process of ka'ua-to-kanji conver-sioll, rClll:W the fivr co-occu ' r rencc  data  8?
;t,s2Nj .... ~md Ev~,,.
by folh)wing these steps:1.
(',ha,lJge all pr ior i ty values of ti(i =1, ~, .
.
.
,  t,,) i,~ ~h,, ~t  ~, , , , .
~,, f ( td  (forexmnl)le, f ( t l )  = 0.95ti).
This processis intended to de(:rease pr ior i ty with the1);tss~tge of time.TM)le 1: Exl)erinmnt results oi1 pl'o(:essing homo-l lyl l lS ill a. silnl)h~ SlHltellce,Co l lvers ion  r~tl, e1)er Selit(~llC(~Ac(:ur~t(:y ra.te1)el: S(~lltellceConversion r~d;el)er wordA(:cura.
(:y rzd;el)er wordFor  sell(;elIces For sentel lceswithout a verb with a verb1053/1129 1661~9(93.3%) (14.7%)663/1053 1361~66(63.0%) (81.9%)215502315 500/3444(93.1%) (14.~%)1716/2155 469/~O(79.6%) (93.8%)11372.
Change all priority values of ui(i =1, 2~,-.-,1,) in the set Evj,,, to f (u i )  aswell.3.
Let N be the noun determined inthe process of kana-to-kanji conversion.Find M1k,)(i = 1, 2 , .
.
.
,  q)which co-occur with the noun N followedby any particle, in the near co-occurrencedata set EN ..... .
Add new elements(v ,  g(k,))(i = 1, 2 , .
.
.
,  q)to the set Evj,,,.
If an element with thesame verb vi already exists in Evs~,,., addthe value g(k,) to the priority vMue ofthat element instead of the new element.Here, g(k,) is a function for convertingfl'equency of occurrence to priority value.For example,g(ki) = 1 - (1/k,)4.
Let vi be the verb described in the pre-vious step.
Find all(nj, l j ) ( j  = 1, 2 , .
.
.
,  q)which co-occur with the verb v, and anyparticle in the near co-occurrence dataset P~v...~.
Add new elements(nj, h(ki, l j ) )( j  = 1,2 , .
.
.
,  q)to the set ENj ....
If an element with thesame noun nj already exists in Egs~,~, ,add the value h(ki, lj) to the priorityvalue of that element instead of the newelement.
Here, h(ki, lj) is a flmctionfor converting frequency of occurrence topriority value.
For example,h(k ,  b)  = g(k,)(1 - (1/l j))6 Process ing  homonyms in asequence  of  sentencesUsing the Jar co-occurrence data sets definedin the previous section, the most feasible wordamong homonyms can be selected in the scope ofa sequence of sentences according to the followingtwo  CaSeS.Case l  An input word written in t~ana-charactersis a 1101111.Case2 An input word written in kaua-charactersis a verb.6.1 P rocedure  for ease lStep l  Find set Sn:S,  = {(N , ,T t ) , (N2 ,  T2), .
.
.
}where Ni(i  : 1, 2, .
.
. )
is a homonynfic l~a'nfi-variant for the input word written in kana-characters and T, is the priority vMue forhomonym N,, which can be retrieved fromthe Jar co-occurrence data set ENid,..Step2 The noun Ni which has the greatest T,priority value in Sn is tile most feasible nounfor the input word written in kana-chara~ters.6.2 P rocedure  for case2Step1 Find set S,:sv =Here, Vj(j = 1 ,2 , .
.
. )
i s  a homo,tymic kanji-variant for the input word written in kana-characters and Uj is the priority value forhomonym Vj , which can be retrieved fromthe far co-occurrence data set Evj ....Step2 The verb Iv) which has the greatest Uj pri-ority wflue in S, is the most feasible verb forthe input word written in kana-characters.7 Conc lus ionWe have proposed two new methods for processingJapanese homonyms based on the co-occurrencerelation between a noun and a verb in a sentencewhich can l)e obtained easily from corpora.
Usingthese inethods, we can evMuate the co-occurrencerelation of words in a simple sentence by using thenear co-occurrence data sets obtained from cor-pora.
We can Mso evaluate the co-occurrence r la-tion of words in different sentences by using the farco-occurrence data sets constructed from the nearco-occurrence data sets in the course of process-iug input sentences.
The far co-occurrence datasets are based on the proposition that it is morepractical to maintain a relatively smM1 amountof data on the semantic relations between words,being changed dynamically in the course of pro-cessing, than to maintain a huge universal "the-saurus" data base, which does not appear to havebeen built successfldly.An experiment of l~ana-to-kanji conversion bythe first method for 1,129 input simple sentenceshas shown that the conversion is carried out in93.1% per word and the accuracy rate is 79.6%per word.
It  is clarified that the first method ismore effective than the ordinary method base~l onthe word occurrence frequency.In the next stage of our study, we intend toewfluate the second method based on the.far co-occurrence data sets by conducting experiments.Re ferencesKobayashi, T., et al 1992.
Realization ofKana-to-Kanji Conversion Using Neural Net-works.
Toshiba Review, Vol.47, No.
l l ,  pages 868-870, Japan.Yamamoto, K., et al 1992.
Kana-to-KanjiConversion Using Co-occurrence Groups.
Proc.of 4\]~th Confi:reuce of IPSJ, 4p-l l ,  pages 189-190,Japan.EDR.
1994.
Co-occurrence Dictionary Vet.2,TR-043, Japan.1138
