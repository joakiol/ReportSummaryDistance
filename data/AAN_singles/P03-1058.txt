Exploiting Parallel Texts for Word Sense Disambiguation:An Empirical StudyHwee Tou NgBin WangYee Seng ChanDepartment of Computer ScienceNational University of Singapore3 Science Drive 2, Singapore 117543{nght, wangbin, chanys}@comp.nus.edu.sgAbstractA central problem of word sense disam-biguation (WSD) is the lack of manuallysense-tagged data required for supervisedlearning.
In this paper, we evaluate an ap-proach to automatically acquire sense-tagged training data from English-Chineseparallel corpora, which are then used fordisambiguating the nouns in theSENSEVAL-2 English lexical sampletask.
Our investigation reveals that thismethod of acquiring sense-tagged data ispromising.
On a subset of the most diffi-cult SENSEVAL-2 nouns, the accuracydifference between the two approaches isonly 14.0%, and the difference could nar-row further to 6.5% if we disregard theadvantage that manually sense-taggeddata have in their sense coverage.
Ouranalysis also highlights the importance ofthe issue of domain dependence in evalu-ating WSD programs.1 IntroductionThe task of word sense disambiguation (WSD) isto determine the correct meaning, or sense of aword in context.
It is a fundamental problem innatural language processing (NLP), and the abilityto disambiguate word sense accurately is importantfor applications like machine translation, informa-tion retrieval, etc.Corpus-based, supervised machine learningmethods have been used to tackle the WSD task,just like the other NLP tasks.
Among the variousapproaches to WSD, the supervised learning ap-proach is the most successful to date.
In this ap-proach, we first collect a corpus in which eachoccurrence of an ambiguous word w has beenmanually annotated with the correct sense, accord-ing to some existing sense inventory in a diction-ary.
This annotated corpus then serves as thetraining material for a learning algorithm.
Aftertraining, a model is automatically learned and it isused to assign the correct sense to any previouslyunseen occurrence of w in a new context.While the supervised learning approach hasbeen successful, it has the drawback of requiringmanually sense-tagged data.
This problem is par-ticular severe for WSD, since sense-tagged datamust be collected separately for each word in alanguage.One source to look for potential training datafor WSD is parallel texts, as proposed by Resnikand Yarowsky (1997).
Given a word-aligned paral-lel corpus, the different translations in a target lan-guage serve as the ?sense-tags?
of an ambiguousword in the source language.
For example, somepossible Chinese translations of the English nounchannel are listed in Table 1.
To illustrate, if thesense of an occurrence of the noun channel is ?apath over which electrical signals can pass?, thenthis occurrence can be translated as ????
in Chi-nese.WordNet1.7 sense idLumpedsense idChinese translations WordNet 1.7 English sense descriptions1 1 ??
A path over which electrical signals can pass2 2 ??
??
???
A passage for water3 3 ?
A long narrow furrow4 4 ??
A relatively narrow body of water5 5 ??
A means of communication or access6 6 ??
A bodily passage or tube7 1 ??
A television station and its programsTable 1: WordNet 1.7 English sense descriptions, the actual lumped senses, and Chinese translationsof the noun channel used in our implemented approachParallel corpora Size of English texts (inmillion words (MB))Size of Chinese texts (inmillion characters (MB))Hong Kong News 5.9 (39.4) 10.7 (22.8)Hong Kong Laws 7.0 (39.8) 14.0 (22.6)Hong Kong Hansards 11.9 (54.2) 18.0 (32.4)English translation of Chinese Treebank 0.1 (0.7) 0.2 (0.4)Xinhua News 3.6 (22.9) 7.0 (17.0)Sinorama 3.2 (19.8) 5.3 (10.2)Total 31.7 (176.8) 55.2 (105.4)Table 2: Size of English-Chinese parallel corporaThis approach of getting sense-tagged corpusalso addresses two related issues in WSD.
Firstly,what constitutes a valid sense distinction carriesmuch subjectivity.
Different dictionaries define adifferent sense inventory.
By tying sense distinc-tion to the different translations in a target lan-guage, this introduces a ?data-oriented?
view tosense distinction and serves to add an element ofobjectivity to sense definition.
Secondly, WSD hasbeen criticized as addressing an isolated problemwithout being grounded to any real application.
Bydefining sense distinction in terms of different tar-get translations, the outcome of word sense disam-biguation of a source language word is theselection of a target word, which directly corre-sponds to word selection in machine translation.While this use of parallel corpus for word sensedisambiguation seems appealing, several practicalissues arise in its implementation:(i) What is the size of the parallel corpusneeded in order for this approach to be able to dis-ambiguate a source language word accurately?
(ii) While we can obtain large parallel corporain the long run, to have them manually word-aligned would be too time-consuming and woulddefeat the original purpose of getting a sense-tagged corpus without manual annotation.
How-ever, are current word alignment algorithms accu-rate enough for our purpose?
(iii) Ultimately, using a state-of-the-art super-vised WSD program, what is its disambiguationaccuracy when it is trained on a ?sense-tagged?corpus obtained via parallel text alignment, com-pared with training on a manually sense-taggedcorpus?Much research remains to be done to investi-gate all of the above issues.
The lack of large-scaleparallel corpora no doubt has impeded progress inthis direction, although attempts have been made tomine parallel corpora from the Web (Resnik,1999).However, large-scale, good-quality parallelcorpora have recently become available.
For ex-ample, six English-Chinese parallel corpora arenow available from Linguistic Data Consortium.These parallel corpora are listed in Table 2, with acombined size of 280 MB.
In this paper, we ad-dress the above issues and report our findings, ex-ploiting the English-Chinese parallel corpora inTable 2 for word sense disambiguation.
We evalu-ated our approach on all the nouns in the Englishlexical sample task of SENSEVAL-2 (Edmondsand Cotton, 2001; Kilgarriff 2001), which used theWordNet 1.7 sense inventory (Miller, 1990).
Whileour approach has only been tested on English andChinese, it is completely general and applicable toother language pairs.22.12.22.32.4ApproachOur approach of exploiting parallel texts for wordsense disambiguation consists of four steps: (1)parallel text alignment (2) manual selection of tar-get translations (3) training of WSD classifier (4)WSD of words in new contexts.Parallel Text AlignmentIn this step, parallel texts are first sentence-alignedand then word-aligned.
Various alignment algo-rithms (Melamed 2001; Och and Ney 2000) havebeen developed in the past.
For the six bilingualcorpora that we used, they already come with sen-tences pre-aligned, either manually when the cor-pora were prepared or automatically by sentence-alignment programs.
After sentence alignment, theEnglish texts are tokenized so that a punctuationsymbol is separated from its preceding word.
Forthe Chinese texts, we performed word segmenta-tion, so that Chinese characters are segmented intowords.
The resulting parallel texts are then input tothe GIZA++ software (Och and Ney 2000) forword alignment.In the output of GIZA++, each English wordtoken is aligned to some Chinese word token.
Thealignment result contains much noise, especiallyfor words with low frequency counts.Manual Selection of Target TranslationsIn this step, we will decide on the sense classes ofan English word w that are relevant to translating winto Chinese.
We will illustrate with the nounchannel, which is one of the nouns evaluated in theEnglish lexical sample task of SENSEVAL-2.
Werely on two sources to decide on the sense classesof w:(i) The sense definitions in WordNet 1.7, whichlists seven senses for the noun channel.
Twosenses are lumped together if they are translated inthe same way in Chinese.
For example, sense 1 and7 of channel are both translated as ????
in Chi-nese, so these two senses are lumped together.
(ii) From the word alignment output ofGIZA++, we select those occurrences of the nounchannel which have been aligned to one of theChinese translations chosen (as listed in Table 1).These occurrences of the noun channel in the Eng-lish side of the parallel texts are considered to havebeen disambiguated and ?sense-tagged?
by the ap-propriate Chinese translations.
Each such occur-rence of channel together with the 3-sentencecontext in English surrounding channel then formsa training example for a supervised WSD programin the next step.The average time taken to perform manual se-lection of target translations for one SENSEVAL-2English noun is less than 15 minutes.
This is a rela-tively short time, especially when compared to theeffort that we would otherwise need to spend toperform manual sense-tagging of training exam-ples.
This step could also be potentially automatedif we have a suitable bilingual translation lexicon.Training of WSD ClassifierMuch research has been done on the best super-vised learning approach for WSD (Florian andYarowsky, 2002; Lee and Ng, 2002; Mihalcea andMoldovan, 2001; Yarowsky et al, 2001).
In thispaper, we used the WSD program reported in (Leeand Ng, 2002).
In particular, our method made useof the knowledge sources of part-of-speech, sur-rounding words, and local collocations.
We usedna?ve Bayes as the learning algorithm.
Our previ-ous research demonstrated that such an approachleads to a state-of-the-art WSD program with goodperformance.WSD of Words in New ContextsGiven an occurrence of w in a new context, wethen used the na?ve Bayes classifier to determinethe most probable sense of w.noun No.
ofsensesbeforelumpingNo.
ofsensesafterlumpingM1 P1 P1-BaselineM2 M3 P2 P2-Baselinechild 4 1 - - - - - - -detention 2 1 - - - - - - -feeling 6 1 - - - - - - -holiday 2 1 - - - - - - -lady 3 1 - - - - - - -material 5 1 - - - - - - -yew 2 1 - - - - - - -bar 13 13 0.619 0.529 0.500 - - - -bum 4 3 0.850 0.850 0.850 - - - -chair 4 4 0.887 0.895 0.887 - - - -day 10 6 0.921 0.907 0.906 - - - -dyke 2 2 0.893 0.893 0.893 - - - -fatigue 4 3 0.875 0.875 0.875 - - - -hearth 3 2 0.906 0.844 0.844 - - - -mouth 8 4 0.877 0.811 0.846 - - - -nation 4 3 0.806 0.806 0.806 - - - -nature 5 3 0.733 0.756 0.522 - - - -post 8 7 0.517 0.431 0.431 - - - -restraint 6 3 0.932 0.864 0.864 - - - -sense 5 4 0.698 0.684 0.453 - - - -stress 5 3 0.921 0.921 0.921 - - - -art 4 3 0.722 0.494 0.424 0.678 0.562 0.504 0.424authority 7 5 0.879 0.753 0.538 0.802 0.800 0.709 0.538channel 7 6 0.735 0.487 0.441 0.715 0.715 0.526 0.441church 3 3 0.758 0.582 0.573 0.691 0.629 0.609 0.572circuit 6 5 0.792 0.457 0.434 0.683 0.438 0.446 0.438facility 5 3 0.875 0.764 0.750 0.874 0.893 0.754 0.750grip 7 7 0.700 0.540 0.560 0.655 0.574 0.546 0.556spade 3 3 0.806 0.677 0.677 0.790 0.677 0.677 0.677Table 3: List of 29 SENSEVAL-2 nouns, their number of senses, and various accuracy figures3 An Empirical StudyWe evaluated our approach to word sense disam-biguation on all the 29 nouns in the English lexicalsample task of SENSEVAL-2 (Edmonds and Cot-ton, 2001; Kilgarriff 2001).
The list of 29 nouns isgiven in Table 3.
The second column of Table 3lists the number of senses of each noun as given inthe WordNet 1.7 sense inventory (Miller, 1990).We first lump together two senses s1 and s2 of anoun if s1 and s2 are translated into the same Chi-nese word.
The number of senses of each nounafter sense lumping is given in column 3 of Table3.
For the 7 nouns that are lumped into one sense(i.e., they are all translated into one Chinese word),we do not perform WSD on these words.
The aver-age number of senses before and after sense lump-ing is 5.07 and 3.52 respectively.After sense lumping, we trained a WSD classi-fier for each noun w, by using the lumped senses inthe manually sense-tagged training data for w pro-vided by the SENSEVAL-2 organizers.
We thentested the WSD classifier on the officialSENSEVAL-2 test data (but with lumped senses)for w. The test accuracy (based on fine-grainedscoring of SENSEVAL-2) of each noun obtained islisted in the column labeled M1 in Table 3.We then used our approach of parallel textalignment described in the last section to obtain thetraining examples from the English side of the par-allel texts.
Due to the memory size limitation ofour machine, we were not able to align all six par-allel corpora of 280MB in one alignment run ofGIZA++.
For two of the corpora, Hong Kong Han-sards and Xinhua News, we gathered all Englishsentences containing the 29 SENSEVAL-2 nounoccurrences (and their sentence-aligned Chinesesentence counterparts).
This subset, together withthe complete corpora of Hong Kong News, HongKong Laws, English translation of Chinese Tree-bank, and Sinorama, is then given to GIZA++ toperform one word alignment run.
It took about 40hours on our 2.4 GHz machine with 2 GB memoryto perform this alignment.After word alignment, each 3-sentence contextin English containing an occurrence of the noun wthat is aligned to a selected Chinese translationthen forms a training example.
For eachSENSEVAL-2 noun w, we then collected trainingexamples from the English side of the parallel textsusing the same number of training examples foreach sense of w that are present in the manuallysense-tagged SENSEVAL-2 official training cor-pus (lumped-sense version).
If there are insuffi-cient training examples for some sense of w fromthe parallel texts, then we just used as many paral-lel text training examples as we could find for thatsense.
We chose the same number of training ex-amples for each sense as the official training dataso that we can do a fair comparison between theaccuracy of the parallel text alignment approachversus the manual sense-tagging approach.After training a WSD classifier for w with suchparallel text examples, we then evaluated the WSDclassifier on the same official SENSEVAL-2 testset (with lumped senses).
The test accuracy of eachnoun obtained by training on such parallel texttraining examples (averaged over 10 trials) is listedin the column labeled P1 in Table 3.The baseline accuracy for each noun is alsolisted in the column labeled ?P1-Baseline?
in Table3.
The baseline accuracy corresponds to alwayspicking the most frequently occurring sense in thetraining data.Ideally, we would hope M1 and P1 to be closein value, since this would imply that WSD basedon training examples collected from the paralleltext alignment approach performs as well as manu-ally sense-tagged training examples.
Comparingthe M1 and P1 figures, we observed that there is aset of nouns for which they are relatively close.These nouns are: bar, bum, chair, day, dyke, fa-tigue, hearth, mouth, nation, nature, post, re-straint, sense, stress.
This set of nouns is relativelyeasy to disambiguate, since using the most-frequently-occurring-sense baseline would havedone well for most of these nouns.The parallel text alignment approach workswell for nature and sense, among these nouns.
Fornature, the parallel text alignment approach givesbetter accuracy, and for sense the accuracy differ-ence is only 0.014 (while there is a relatively largedifference of 0.231 between P1 and P1-Baseline ofsense).
This demonstrates that the parallel textalignment approach to acquiring training examplescan yield good results.For the remaining nouns (art, authority, chan-nel, church, circuit, facility, grip, spade), theaccuracy difference between M1 and P1 is at least0.10.
Henceforth, we shall refer to this set of 8nouns as ?difficult?
nouns.
We will give an analy-sis of the reason for the accuracy difference be-tween M1 and P1 in the next section.44.1AnalysisSense-Tag Accuracy of Parallel TextTraining ExamplesTo see why there is still a difference between theaccuracy of the two approaches, we first examinedthe quality of the training examples obtainedthrough parallel text alignment.
If the automati-cally acquired training examples are noisy, thenthis could account for the lower P1 score.The word alignment output of GIZA++ con-tains much noise in general (especially for the lowfrequency words).
However, note that in our ap-proach, we only select the English word occur-rences that align to our manually selected Chinesetranslations.
Hence, while the complete set of wordalignment output contains much noise, the subsetof word occurrences chosen may still have highquality sense tags.Our manual inspection reveals that the annota-tion errors introduced by parallel text alignmentcan be attributed to the following sources:(i) Wrong sentence alignment: Due to errone-ous sentence segmentation or sentence alignment,the correct Chinese word that an English word wshould align to is not present in its Chinese sen-tence counterpart.
In this case, word alignment willalign the wrong Chinese word to w.(ii) Presence of multiple Chinese translationcandidates: Sometimes, multiple and distinct Chi-nese translations appear in the aligned Chinesesentence.
For example, for an English occurrencechannel, both ????
(sense 1 translation) and ????
(sense 5 translation) happen to appear in thealigned Chinese sentence.
In this case, wordalignment may erroneously align the wrong Chi-nese translation to channel.
(iii) Truly ambiguous word: Sometimes, a wordis truly ambiguous in a particular context, and dif-ferent translators may translate it differently.
Forexample, in the phrase ?the church meeting?,church could be the physical building sense (??
), or the institution sense (??
).
In manualsense tagging done in SENSEVAL-2, it is possibleto assign two sense tags to church in this case, butin the parallel text setting, a particular translatorwill translate it in one of the two ways (??
or ??
), and hence the sense tag found by parallel textalignment is only one of the two sense tags.By manually examining a subset of about 1,000examples, we estimate that the sense-tag error rateof training examples (tagged with lumped senses)obtained by our parallel text alignment approach isless than 1%, which compares favorably with thequality of manually sense tagged corpus preparedin SENSEVAL-2 (Kilgarriff, 2001).4.2 Domain Dependence and InsufficientSense CoverageWhile it is encouraging to find out that the par-allel text sense tags are of high quality, we are stillleft with the task of explaining the difference be-tween M1 and P1 for the set of difficult nouns.
Ourfurther investigation reveals that the accuracy dif-ference between M1 and P1 is due to the followingtwo reasons: domain dependence and insufficientsense coverage.Domain Dependence The accuracy figure ofM1 for each noun is obtained by training a WSDclassifier on the manually sense-tagged trainingdata (with lumped senses) provided bySENSEVAL-2 organizers, and testing on the cor-responding official test data (also with lumpedsenses), both of which come from similar domains.In contrast, the P1 score of each noun is obtainedby training the WSD classifier on a mixture of sixparallel corpora, and tested on the officialSENSEVAL-2 test set, and hence the training andtest data come from dissimilar domains in thiscase.Moreover, from the ?docsrc?
field (which re-cords the document id that each training or testexample originates) of the official SENSEVAL-2training and test examples, we realized that thereare many cases when some of the examples from adocument are used as training examples, while therest of the examples from the same document areused as test examples.
In general, such a practiceresults in higher test accuracy, since the test exam-ples would look a lot closer to the training exam-ples in this case.To address this issue, we took the officialSENSEVAL-2 training and test examples of eachnoun w and combined them together.
We then ran-domly split the data into a new training and a newtest set such that no training and test examplescome from the same document.
The number oftraining examples in each sense in such a newtraining set is the same as that in the official train-ing data set of w.A WSD classifier was then trained on this newtraining set, and tested on this new test set.
Weconducted 10 random trials, each time splitting intoa different training and test set but ensuring thatthe number of training examples in each sense (andthus the sense distribution) follows the officialtraining set of w. We report the average accuracyof the 10 trials.
The accuracy figures for the set ofdifficult nouns thus obtained are listed in the col-umn labeled M2 in Table 3.We observed that M2 is always lower in valuecompared to M1 for all difficult nouns.
This sug-gests that the effect of training and test examplescoming from the same document has inflated theaccuracy figures of SENSEVAL-2 nouns.Next, we randomly selected 10 sets of trainingexamples from the parallel corpora, such that thenumber of training examples in each sense fol-lowed the official training set of w. (When therewere insufficient training examples for a sense, wejust used as many as we could find from the paral-lel corpora.)
In each trial, after training a WSDclassifier on the selected parallel text examples, wetested the classifier on the same test set (fromSENSEVAL-2 provided data) used in that trial thatgenerated the M2 score.
The accuracy figures thusobtained for all the difficult nouns are listed in thecolumn labeled P2 in Table 3.Insufficient Sense Coverage We observed thatthere are situations when we have insufficienttraining examples in the parallel corpora for someof the senses of some nouns.
For instance, no oc-currences of sense 5 of the noun circuit (racingcircuit, a racetrack for automobile races) could befound in the parallel corpora.
To ensure a fairercomparison, for each of the 10-trial manuallysense-tagged training data that gave rise to the ac-curacy figure M2 of a noun w, we extracted a newsubset of 10-trial (manually sense-tagged) trainingdata by ensuring adherence to the number of train-ing examples found for each sense of w in the cor-responding parallel text training set that gave riseto the accuracy figure P2 for w. The accuracy fig-ures thus obtained for the difficult nouns are listedin the column labeled M3 in Table 3.
M3 thus gavethe accuracy of training on manually sense-taggeddata but restricted to the number of training exam-ples found in each sense from parallel corpora.4.356DiscussionThe difference between the accuracy figures ofM2 and P2 averaged over the set of all difficultnouns is 0.140.
This is smaller than the differenceof 0.189 between the accuracy figures of M1 andP1 averaged over the set of all difficult nouns.
Thisconfirms our hypothesis that eliminating the possi-bility that training and test examples come fromthe same document would result in a fairer com-parison.In addition, the difference between the accuracyfigures of M3 and P2 averaged over the set of alldifficult nouns is 0.065.
That is, eliminating theadvantage that manually sense-tagged data have intheir sense coverage would reduce the performancegap between the two approaches from 0.140 to0.065.
Notice that this reduction is particularly sig-nificant for the noun circuit.
For this noun, the par-allel corpora do not have enough training examplesfor sense 4 and sense 5 of circuit, and these twosenses constitute approximately 23% in each of the10-trial test set.We believe that the remaining difference of0.065 between the two approaches could be attrib-uted to the fact that the training and test examplesof the manually sense-tagged corpus, while notcoming from the same document, are however stilldrawn from the same general domain.
To illustrate,we consider the noun channel where the differencebetween M3 and P2 is the largest.
For channel, itturns out that a substantial number of the trainingand test examples contain the collocation ?Channeltunnel?
or ?Channel Tunnel?.
On average, about9.8 training examples and 6.2 test examples con-tain this collocation.
This alone would have ac-counted for 0.088 of the accuracy differencebetween the two approaches.That domain dependence is an important issueaffecting the performance of WSD programs hasbeen pointed out by (Escudero et al, 2000).
Ourwork confirms the importance of domain depend-ence in WSD.As to the problem of insufficient sense cover-age, with the steady increase and availability ofparallel corpora, we believe that getting sufficientsense coverage from larger parallel corpora shouldnot be a problem in the near future for most of thecommonly occurring words in a language.Related WorkBrown et al (1991) is the first to have exploredstatistical methods in word sense disambiguation inthe context of machine translation.
However, theyonly looked at assigning at most two senses to aword, and their method only asked a single ques-tion about a single word of context.
Li and Li(2002) investigated a bilingual bootstrapping tech-nique, which differs from the method we imple-mented here.
Their method also does not require aparallel corpus.The research of (Chugur et al, 2002) dealt withsense distinctions across multiple languages.
Ide etal.
(2002) investigated word sense distinctions us-ing parallel corpora.
Resnik and Yarowsky (2000)considered word sense disambiguation using mul-tiple languages.
Our present work can be similarlyextended beyond bilingual corpora to multilingualcorpora.The research most similar to ours is the work ofDiab and Resnik (2002).
However, they used ma-chine translated parallel corpus instead of humantranslated parallel corpus.
In addition, they used anunsupervised method of noun group disambigua-tion, and evaluated on the English all-words task.ConclusionIn this paper, we reported an empirical study toevaluate an approach of automatically acquiringsense-tagged training data from English-Chineseparallel corpora, which were then used for disam-biguating the nouns in the SENSEVAL-2 Englishlexical sample task.
Our investigation reveals thatthis method of acquiring sense-tagged data is pro-mising and provides an alternative to manual sensetagging.AcknowledgementsThis research is partially supported by a researchgrant R252-000-125-112 from National Universityof Singapore Academic Research Fund.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1991.
Word-sense disambiguation using statistical methods.
InProceedings of the 29th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 264-270.Irina Chugur, Julio Gonzalo, and Felisa Verdejo.
2002.Polysemy and sense proximity in the Senseval-2 testsuite.
In Proceedings of the ACL SIGLEX Workshopon Word Sense Disambiguation: Recent Successesand Future Directions, pages 32-39.Mona Diab and Philip Resnik.
2002.
An unsupervisedmethod for word sense tagging using parallel cor-pora.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics, pages255-262.Philip Edmonds and Scott Cotton.
2001.
SENSEVAL-2:Overview.
In Proceedings of the Second Interna-tional Workshop on Evaluating Word SenseDisambiguation Systems (SENSEVAL-2), pages 1-5.Gerard Escudero, Lluis Marquez, and German Rigau.2000.
An empirical study of the domain dependenceof supervised word sense disambiguation systems.
InProceedings of the Joint SIGDAT Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora, pages 172-180.Radu Florian and David Yarowsky.
2002.
Modelingconsensus: Classifier combination for word sensedisambiguation.
In Proceedings of the 2002 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 25-32.Nancy Ide, Tomaz Erjavec, and Dan Tufis.
2002.
Sensediscrimination with parallel corpora.
In Proceedingsof the ACL SIGLEX Workshop on Word Sense Dis-ambiguation: Recent Successes and Future Direc-tions, pages 54-60.Adam Kilgarriff.
2001.
English lexical sample task de-scription.
In Proceedings of the Second InternationalWorkshop on Evaluating Word Sense Disambigua-tion Systems (SENSEVAL-2), pages 17-20.Yoong Keok Lee and Hwee Tou Ng.
2002.
An empiri-cal evaluation of knowledge sources and learning al-gorithms for word sense disambiguation.
InProceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing, pages 41-48.Cong Li and Hang Li.
2002.
Word translation disam-biguation using bilingual bootstrapping.
In Proceed-ings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 343-351.I.
Dan Melamed.
2001.
Empirical Methods for Exploit-ing Parallel Texts.
MIT Press, Cambridge.Rada F. Mihalcea and Dan I. Moldovan.
2001.
Patternlearning and active feature selection for word sensedisambiguation.
In Proceedings of the Second Inter-national Workshop on Evaluating Word Sense Dis-ambiguation Systems (SENSEVAL-2), pages 127-130.George A. Miller.
(Ed.)
1990.
WordNet: An on-linelexical database.
International Journal of Lexicogra-phy, 3(4):235-312.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting of the Association for Computa-tional Linguistics, pages 440-447.Philip Resnik.
1999.
Mining the Web for bilingual text.In Proceedings of the 37th Annual Meeting of the As-sociation for Computational Linguistics, pages 527-534.Philip Resnik and David Yarowsky.
1997.
A perspec-tive on word sense disambiguation methods and theirevaluation.
In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with Lexical Semantics:Why, What, and How?, pages 79-86.Philip Resnik and David Yarowsky.
2000.
Distinguish-ing systems and distinguishing senses: New evalua-tion methods for word sense disambiguation.
NaturalLanguage Engineering, 5(2):113-133.David Yarowsky, Silviu Cucerzan, Radu Florian,Charles Schafer, and Richard Wicentowski.
2001.The Johns Hopkins SENSEVAL2 system descrip-tions.
In Proceedings of the Second InternationalWorkshop on Evaluating Word Sense Disambigua-tion Systems (SENSEVAL-2), pages 163-166.
