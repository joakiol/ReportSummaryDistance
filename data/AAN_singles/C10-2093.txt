Coling 2010: Poster Volume, pages 810?818,Beijing, August 2010Varro: An Algorithm and Toolkit for Regular Structure Discovery inTreebanksScott MartensCentrum voor Computerlingu?
?stiek, KU Leuvenscott@ccl.kuleuven.beAbstractThe Varro toolkit is a system for identi-fying and counting a major class of reg-ularity in treebanks and annotated nat-ural language data in the form of tree-structures: frequently recurring unorderedsubtrees.
This software has been designedfor use in linguistics to be maximallyapplicable to actually existing treebanksand other stores of tree-structurable nat-ural language data.
It minimizes mem-ory use so that moderately large treebanksare tractable on commonly available com-puter hardware.
This article introducescondensed canonically ordered trees as adata structure for efficiently discoveringfrequently recurring unordered subtrees.1 CreditsThis research is supported by the AMASS++Project1 directly funded by the Institute for thePromotion of Innovation by Science and Technol-ogy in Flanders (IWT) (SBO IWT 060051).2 IntroductionTreebanks and similarly enhanced corpora are in-creasingly available for research, but these morecomplex structures are resistant to the techniquesused in NLP for the statistical analysis of strings.This paper introduces a new treebank analysissuite Varro, named after Roman philologist Mar-cus Terentius Varro (116 BC-27 BC), who madelinguistic regularity and irregularity central to his1http://www.cs.kuleuven.be/?liir/projects/amass/philosophy of language in De Lingua Latina.
(Harris and Taylor, 1989)The Varro toolkit focuses on a general problemin performing statistical analyses on treebanks:identifying, counting and extracting the distribu-tions of frequently recurring unordered subtreesin treebanks.
From this base, it is possible to con-struct more linguistically motivated schemes forperforming treebank analysis.
Complex statisticalanalyses are constructed from knowledge aboutfrequency and distribution, so this constitutes alow level task on top of which higher level analy-ses can be performed.An algorithm that can efficiently extract fre-quently recurring subtrees from treebanks has anumber of immediate applications in computa-tional linguistics:?
Speeding up treebank search algorithms likeTgrep2.
(Rohde, 2001)?
Rule discovery for tree transducers used inparsing and machine translation.
(Knight andGraehl, 2005; Knight, 2007)?
Generalizing lexical statistics techniques inNLP ?
e.g., collocation ?
to a broader arrayof linguistic structures.
(Sinclair, 1991)?
Efficiently identifying useful features for treekernel methods.
(Moschitti, 2006)3 Theory and Previous WorkFor the purposes of this paper, a treebank is anycollection of disjoint labeled trees.
While in prac-tice this mostly means parsed natural languagesentences, the approach described here is equallyapplicable to other kinds of data, including seman-tic feature structures, morphological analyses, and810doubtless many other kind of linguistically moti-vated structures.
Figure 1 is an example of a parsetree from a Dutch-language treebank.Figure 1: A tree from the Europarl Dutch cor-pus.
(Koehn, 2005) It has been parsed and labeledautomatically by the Alpino parser.
(van Noord,2006) A word-for-word translation is ?It also hasa legal reason.?
(?
?There is also a legal reason(for that).?
)In this paper, we are concerned with identify-ing and counting frequent induced unordered sub-trees in treebanks.
The term subtree has a numberof definitions, but this paper will follow the ter-minology of Chi et al (2004).
Figure 2 containsthree examples of induced unordered subtrees ofthe tree in Figure 1.
Note that the ordering ofthe vertices in the subtrees is different from thatof Figure 1.
This is what makes them unorderedsubtrees.
Induced subtrees are more formally de-scribed in Section 4.3.1 AprioriThe research builds on frequent subtree discov-ery algorithms based on the well-known Apri-ori algorithm, which is used to discover fre-quent itemsets in databases.
(Agrawal et al,1993) As a brief summary of Apriori, con-sider a collection of ordered itemsets C ={{a, b, c}, {a, b, d}, {b, c, d, e}}.
Apriori discov-ers all the subsets of those elements that appear atleast some user-determined ?
times.
As an exam-ple, let us set ?
= 2, and then count the numberof times each unique item appears in C. Any sin-gle element in C that appears less than two timescannot be a member of a set of elements that ap-(a)(b) (c)Figure 2: Three induced unordered subtrees of thetree in Figure 1pears at least ?
times (since ?
= 2), so thoseare rejected.
Each of the remaining set elements{a, b, c, d} is extended by counting the numberof two-element sets that include it and some el-ement to the right in the ordered itemsets in C.For b, these are {{b, c}, {b, d}, {b, e}}.
Of thisset, only those that appear at least ?
times are re-tained: {{b, c}, {b, d}}.
This process is repeatedfor size three sets, and iterated over and over forincreasingly large subsets, until there are no ex-tensions that appear at least ?
times.
This wholeprocedure is then repeated for each unique item.Finally, Apriori will have extracted and countedall itemsets that appear at least ?
times in C.Extending Apriori to frequent subtree dis-covery dates to the work of Zaki (2002) andAsai et al (2002).
Chi et al (2004) summa-rizes much of this line of research.
In Apriori,larger and less frequent itemsets are discovered811(a) (b) (c)(d)Figure 3: 3(b) and 3(c) are a subtrees of 3(a).
Thesubtrees in 3(d) are possible extensions to 3(b),while 3(c) is not.and counted by adding items to shorter and morefrequent ones.
This extends naturally to trees byinitially locating and counting all the one-vertextrees in a treebank, and then constructing largertrees by adding vertices and edges to their rightsides.In Figure 3, subtree 3(b) has as valid extensionssubtrees 3(d), all of which extend 3(b) to the right.An extension like subtree 3(c), which adds a nodeto the left of the rightmost node of 3(b), is not avalid extension.3.2 Treebank applicationsApplying these algorithms to natural languagetreebanks, however, presents a number of chal-lenges.The approach described above, because it con-structs and tests subtrees by moving from left toright, is well-suited to finding ordered subtrees.However, this paper will consider unordered sub-trees as better motivated linguistically.
Word or-der is not completely fixed in any language, andcan be very free in many important contexts.But there are other problems as well.
Apriori-style algorithms have the general property thattheir run-time is proportionate to the size of theoutput.
Given a data-set D and a user-determinedminimum frequency threshold ?, this class of so-lution outputs all the patterns that appear at least?
times in D. If D contains n patterns that ap-pear at least ?
times, P = {p1, p2, ..., pn};?pi ?P : freq(pi) ?
?, then the time necessary toidentify and count all the patterns in P is pro-portionate to?ni=1 freq(pi).
In weakly corre-lated data, this is a very efficient method of find-ing patterns.
In highly correlated data, however,the number of patterns present can become pro-hibitively large and extend run-time to unaccept-able lengths, especially for small ?
or large data-sets.
Each frequent pattern may have any numberof sub-patterns, each of which is also frequent andmust be separately counted.If we identify patterns with subtrees, a subtreewith n vertices will, depending on its structure,have a minimum of n(n ?
1) and a maximumof (n ?
1)!
+ 1 subtrees.
If each of those sub-trees is also a pattern that must be counted, thenruntime grows very rapidly even for very smalldata-sets.
Since natural language data is highlycorrelated, simple subtree-discovery extensions ofApriori, like those proposed in (Zaki, 2002) and(Asai, 2002), are not feasible for linguistic use.
Asreported in Martens (2009b), run-times becomeintractably long very quickly as data size increasesfor really existing treebanks.However, there are compact representations offrequent patterns that are better suited to highly-correlated data and which can be efficiently dis-covered by modified Apriori schemes.
This pa-per will only address one such representation: fre-quent closures.
(Boulicaut and Bykowski, 2000)Frequent closures are widely used in subtree dis-covery and have an intuitive meaning when dis-cussing natural language.Given a treebank D, and a tree T that has a sup-port of freq(T ) = ?, then T is closed if there isno supertree T ?
?
T where freq(T ?)
= ?.
In Fig-ure 3, if subtree 3(c) is as frequent in some tree-bank as 3(b), then 3(b) is not a closed subtree, norcan any further extension of it to the right be aclosed subtree.As a natural language example, given a corpus812of English sentences, let us assume we have founda pattern of the form ?NP make up NP to VP?,such as in ?He has made up his mind to study lin-guistics.?
If every time this pattern appears in thecorpus, the second NP contains ?mind?, then thepattern is not closed.
A larger pattern appears justas often and in exactly the same places.This makes the notion of frequent closed sub-tree discovery a generalization of collocation andcoligation - well known in corpus-based lexicog-raphy - to arbitrary tree structures.
(Sinclair,1991) J.R. Firth famously said, ?You shall knowa word by the company it keeps.?
(Firth, 1957)Frequent subtree discovery tells us exactly whatcompany entire linguistic structures keep.3.3 Efficient closed subtree discoveryChi et al (2005a) outlines a general method for ef-ficiently finding frequent closed subtrees withoutfinding all frequent subtrees first.
Their approachrequires each subtree found to be aligned with itssupertree before checking for closure and exten-sions.
However, the alignment between a subtreeand its supertree - the map from subtree verticesto supertree vertices - is not necessarily unique.
Asubtree may have a number of possible alignmentswith its supertree, even if one or more of the ver-tex alignments is specificed, as shown in Figure 4,which uses an example from the hand-correctedAlpino Treebank of Dutch.2This can only be avoided by adding a restrictionto trees: the combination of edge and vertex labelsfor each child of a vertex must be unique.
Thisguarantees that specifying just one vertex in thealignment of a subtree to its supertree is enoughto determine the entire unique mapping, but it isincompatible with most linguistic theories.
Pro-cesses like tree binarization can meet this require-ment, but only with some loss of generality: Somefrequent closed subtrees in a collection of treeslike Figure 4(a) will no longer be frequent, or willbe less frequent, in a collection of binary trees.Martens (2009a) describes an alternativemethod of checking for closure which does notrequire alignment and can, consequently, be muchfaster.
It has, however, two drawbacks: First,it does not find all frequent closed unordered2http://www.let.rug.nl/vannoord/trees/subtrees.
Figure 5 shows the kind of tree wherethat approach is unable to correctly identify andcount an unordered subtree.
Second, it requires agreat deal more memory than solutions that aligneach subtree discovered and check directly forclosure, and is therefore of limited use with verylarge corpora.4 DefinitionsA fully-labeled rooted tree is a rooted tree inwhich each vertex and each edge has a label: T :=?V,E, LV , LE?, where V is the set of vertices, Eis the set of edges, LV is a map LV : V ?
LVfrom the vertices to a set of labels; and similarlyLE maps the edges to labels LE : E ?
LE .
Wewill designate an edge e connecting vertex v1 toits child v2 by the notation e = ?v1, v2?.
LV andLE constitute collectively the lexicon.
Figure 1 isan example of a fully-labelled, rooted tree froma Dutch-language treebank.
This formalizationis broadly applicable to all linguistic formalismswhose structures are tree-based or can be con-verted one-to-one into trees without loss of gener-ality.
This may require some degree of restructur-ing of the tree formats used in particular linguistictheories.
For example, in many formal linguistictheories, labels are not atomic symbols, but mayhave many parts or even whole structured featuresets.
In general, these can be mapped to trees withatomic labels by inserting additional vertices, orby taking advantage of edge labelling.The algorithm described here is insufficientfor formal structures that require more powerfulgraph formalisms like directed acyclic graphs.The relations parent, child and sibling are takenhere in their ordinary sense in discussing trees.
InFigure 1, the vertex labeled adv is a child of thevertex labeled smain, the parent of the vertex la-beled ook, and a sibling of the vertex labeled verband the two vertices labeled np.
To simplify defi-nitions, the operator label(x) will indicate the la-bel of vertex or edge x.An induced unordered subtree is a connectedsubset of the vertices of some tree that preservesthe vertex and edge labels and the parent-child re-lations of that tree but need not preserve the or-dering of siblings.
Given a fully-labeled tree T :=?VT , ET , LVT , LET ?, an induced subtree S of T is813(a) (b)Figure 4: In 4(a) is a Dutch phrase conjoining multiple nouns.
It translates as ?police work, recreation,planning and court activities?.
4(b) has six unique unordered alignments with 4(a).
(a) (b) (c)Figure 5: Subtree 5(c) is an unordered subtree of both 5(a) and 5(b), but the algorithm described inMartens (2009a) is unable to capture this in all cases.a fully-labeled tree S := ?VS , ES , LVS , LES ?
forwhich there is an injection M : VS ?
VT fromthe vertices of S to some subset of the vertices ofT , and for which:?v ?
VS :a. label(v) = label(M(v))b. e = ?parent(v), v?
?
ES ?e?
= ?M(parent(v)),M(v)?
?
ETc.
label(e) = label(e?
)See Figures 1 and 2 for examples of subtrees ofa particular tree.We will further define all subtrees that are iden-tical except in the ordering of their vertices to beunordered isomorphic.
If a tree T is a subtree oftree T ?, we will follow set notation by denotingthis relation as T ?
T ?.4.1 Canonical OrderingUsing canonical orderings to solve frequent un-ordered subtree problems was first proposed inLuccio et al (2001) and expanded by otherresearchers in frequent subtree discovery tech-niques, notably in Chi et al (2005b).
Since theApriori-style approaches described in Section 3.1are suited only to finding subtrees whose verticesappear in a particular order, this paper will de-scribe a mechanism for converting fully-labeledtrees into canonical forms that guarantee that allinstances of any unordered subtree will have anidentical order to their vertices.We must first define a strict total ordering oververtex and edge labels.
Given lexica for the edgeand vertex labels, LE and LV respectively, wedefine a strict total ordering on each such that?li, lj ?
L either li ?
lj or li  lj or li = ljand if li ?
lj and lj ?
lk, then li ?
lk.In a collection of fully-labeled trees, ev-ery vertex v that is not the root of sometree can be associated with a full la-bel which is the pair fullLabel(v) =?label(?parent(v), v?
), label(v)?, containingthe label of the edge leading to its parent and thelabel of the vertex itself.
For any pair of verticeswhere the edge to their parent is different, we814order the vertices by the order of those edges.Where the edges are the same, we order themby the ordering of their vertex labels.
Wherewe have two sibling vertices vi and vj such thatfullLabel(vi) = fullLabel(vj), we recursivelyorder the descendants of vi and vj , and thencompare them.
In this way, two nodes can onlyhave an undefined order if they have both exactlythe same full labels and identical descendants.A canonically ordered tree is a tree T :=?VT , ET , LVT , LET ?, where for each v ?
VT , thechildren of v are ordered in just that fashion.4.2 Condensed treesA condensed tree is a fully-labeled tree T :=?VT , ET , LVT , LET ?
with two additional proper-ties:a.
Each vertex v ?
V is associated to a list ofindices parentIndex(v) = {i1, i2, ..., in},which we will call its parent index.
Each en-try i1, i2, ..., in is a non-negative integer.b.
No vertex v ?
V has two children with thesame full label.Condensed trees are constructed from non-condensed trees as follows:Given a tree T := ?V,E, LV , LE?, we firstcanonically order it, as described in the previoussection.
Then, we attach a parent index to eachvertex v ?
V which is not the root of T .
The ini-tial parent index of each node consists of a singlezero.We then traverse the vertices of the now or-dered tree T in breadth-first order from the theroot downwards and from left to right.
Givensome vj ?
V , if it has no sibling to its right,or if the sibling to its immediate right has a dif-ferent vertex label or a different edge label onthe edge to its parent, we do nothing.
Other-wise, if vj has a sibling to its immediate rightvi with the same full label, we set `i to the sizeof parentIndex(vi), and then we append theparentIndex(vj) to parentIndex(vi).
Then, wetake the children of vj , and for each one, we incre-ment each value in its parent index by `i, and theninsert it under vi as one of vi?s children.
We deletevj and then we reorder the children of vi into thecanonical order defined in Section 4.1.This is performed in breadth-first order over T .The result is guaranteed to be a tree where eachvertex never has two children with the same edgeand vertex labels.
Figure 6 shows how the treesin Figure 5 look after they are converted into con-densed trees.
We will denote condensed trees asT = cond(T ), to indicate that T has been con-structed from T .If two non-condensed trees are unordered iso-morphic, then their condensed forms will be iden-tical, including in their vertex orderings and par-ent indexes.
If two condensed trees are identical,then the non-condensed trees from which they areconstructed are always unordered isomorphic.Each vertex v of a condensed tree T = cond(T )has a parent index containing some number of en-tries corresponding to a set of vertices in non-condensed tree T .
We will designate that set asorig(v), a subset of the vertices in T .
Given acondensed tree vertex v and its parent p, if the sizeof orig(p) is larger than one, then the vertices in vmay have different parents in T .
We can interpretthe integers in the parent index of each condensedtree vertex as indicating which parent each mem-ber of orig(v) has.In this way, given T = cond(T ), there is aone-to-one mapping from the vertices of T to apair ?v, i?
consisting of some vertex in T and anindex to an entry in its parent index.
If somevertex v in T maps to ?v, i?, then all the chil-dren of v, c ?
children(v) map to pairs ?c, j?such that parent(c) = v and the jth entry inparentIndex(c) is i.
We can use this to defineparent-child operations over condensed trees thatperfectly match parent-child operations in non-condensed ones.We will define a skeleton tree as a con-densed tree stripped of its parent indices, anddenote it as skel(T).
Note that for any non-condensed tree T and any non-condensed sub-tree S ?
T , skel(cond(T )) will always containskel(cond(S)) as an ordered subtree, includingin cases like Figure 5, as shown in Figure 6.4.3 AlignmentAn alignment of a condensed subtree S with acondensed tree T has two parts:815(a) (b) (c)Figure 6: The trees in Figure 5 transformed into their condensed equivalents, with their parent arrays.Note that 6(c) is visibly an ordered subtree of both 6(a) and 6(b) if you ignore the parent arrays.a.
Skeleton Alignment:An injection M : VS ?
VT from the verticesof S to the vertices of T.b.
Index Alignment:For each vertex vS ?
VS, a bipartite map-ping from the vertices in orig(vS) to the ver-tices in orig(M(vS)).The first part is an alignment of skel(S) withskel(T).
Given an alignment from the root of Sto some vertex in T, this can be performed in timeproportionate, in the worst case, to the number ofvertices in skel(T).
If all the parent indices of thealigned vertices in the subtree and supertree haveonly one index in them, then the index alignmentis trivial and the alignment of S to T is complete.In other cases, index alignment is non-trival.
The method here draws on the proce-dure for unordered subtree alignment proposed byKilpela?inen (1992).
In the worst case, it resolvesto the same algorithm, but can perform better onthe average because of the structure of condensedtrees.Alignment proceeds from the bottom-up, start-ing with the leaves of S. If vertex s is a leaf ofS and is aligned to some vertex t in T, then weinitially assume any member of orig(s) can mapto any member of orig(t).
We then proceed up-wards in S, checking each vertex s in S to find amapping from orig(s) to orig(t) such that if somes ?
orig(s) can be mapped to some t ?
orig(t),then the children of s can be mapped to childrenof t.Once we reach the root of S, we proceed backdownwards, removing those mappings from eachorig(s) to its corresponding orig(t) that are im-possible because their parents do not align.The remaining index alignments must still bechecked to verify that each one can form a partof a one-to-one mapping from orig(s) to orig(t).This is equivalent to finding a maximal bipar-tite matching from orig(s) to orig(t) for eachpossible alignment from orig(s) to orig(t).
Bi-partite matching is a problem with a numberof well-documented solutions.
(Dijkstra (1959),Lova?sz (1986), among others)5 AlgorithmHaving outlined condensed trees and how to alignthem, we can build an algorithm for extracting allfrequent closed unordered subtrees from a tree-bank of condensed trees, given a minimum fre-quency threshold ?.
Space restrictions precludea full formal description of the algorithm, but itclosely follows the general outline for closed treediscovery schemes advanced by Chi et al (2005a):1.
Pass through the treebank collecting all thesubtrees that consist of a single vertex labeland all their locations.2.
Remove those that appear less than ?
times.3.
Loop over each remaining subtree, aligningit to each place it appears in the treebank4.
Collect all the possible extensions, creating anew list of two vertex subtrees and all theirlocations.5.
Use the extensions to the left of the rightmostvertex in each alignment to check if the sub-tree is closed to the left, and reject it if it isnot.6.
Use the extensions to the right of the right-most vertex to check if the subtree is closedto the right, and output it if it is.8167.
Retain the extensions to the right of the right-most vertex and their locations if those exten-sions appear at least ?
times.8.
Repeat for those subtrees.6 Implementation and PerformanceThe Varro toolkit implements condensed trees andthe algorithm described above in Python 3.1 andhas been applied to treebanks as large as severalhundred thousand sentences.
The software andsource code is available from sourceforge.net3 andincludes a small treebank of parsed Latin textsprovided by the Perseus Digital Library.
(Bam-man and Crane, 2007)The worst case memory performance of this al-gorithm is O(nm) where n is the number of ver-tices in the treebank and m is the largest frequentsubtree found in it.
However, only the most patho-logically structured treebank could come close tothis ceiling, and in practice, the current implemen-tation has so far never used as much twice thememory required to store the original treebank.The runtime performance is, as described inSection 3.2, proportionate to the size of the out-put.
However, aligning each occurrence of eachsubtree adds an additional factor.
Given a con-densed subtree S and its condensed supertreeT containing size(T) vertices, and one alreadyaligned vertex, the worst case alignment time isO(size(T)2.5), but only a highly pathological treestructure would approach this.
The best casealignment time is O(size(S)).
Therefore, it al-ways takes more time to align larger subtrees, andsince larger subtrees are less frequent than smallerones, setting lower minimum frequency thresh-olds increases the average time required to processa subtree.Processing even the small Alpino Treebankproduces very large numbers of frequent closedsubtrees.
After removing punctuation and the to-kens themselves, leaving just parts-of-speech andconsituency labels - the Alpino treebank?s 7137sentences are reduced to 206,520 vertices.
Withinthis small set, Varro took 1252 seconds to find7307 frequent closed subtrees that appear at least100 times.
This is both considerably more sub-3http://varro.sourceforge.net/trees than reported by Martens (2009b) on thesame data and considerably more time.Speed and memory performance are the majorpractical issues in this line of research.
Choos-ing to design Varro with memory footprint mini-mization in mind is a source of some performancebottlenecks.
Using Python also takes a heavy tollon speed and a C++ implementation is planned.The fast alignment-free closure checking schemein Martens (2009b) can also be implemented us-ing condensed trees.
On small treebanks this willimprove speed without loss of precision, but haslimited applicability to large treebanks.7 ConclusionsThe trade-off between memory usage, run-timeand completeness for this kind of algorithm ispunitive.
The user must balance very long run-times against excessive memory usage if theywant to accurately count all frequent unorderedinduced subtrees.
The Varro toolkit is designed tomake it possible to choose what tradeoffs to make.Since any subtree can be extended and checked forclosure independently of other subtrees, Varro caneasily implement heuristics designed to further re-duce the number of subtrees extracted.
We believethe future of this line of research lies in large partin that direction and hope that public release ofVarro will aid in its development.We have also discovered that there is a verystrong relationship between the concision andconsistency of linguistic formalisms and Varro?sperformance.
We restructured the Alpino databy promoting the head of each constituent, cre-ating dependency-style trees along the lines de-scribed by Tesnie`re (1959) and Mel?c?uk (1988).This reduced the number of subtrees found by50%-60% and reduced run-times consistently by60%-70% across a range of minimum frequencythresholds and treebank sizes.
As a general rule,increasing the degree of linguistic abstraction in-creases the number of frequent subtrees, and con-sequently slows Varro down dramatically.
Iden-tifying linguistic formalisms that lend themselvesto efficient and productive subtree discovery is an-other significant direction for this research, andone with immediate impact on other areas in lin-guistics.817ReferencesAgrawal, Rakesh, Tomasz Imielinski and Arun Swami.1993.
Mining association rules between sets ofitems in large databases.
Proceedings of the 1993ACM SIGMOD International Conference on Man-agement of Data, pp.
207?216.Asai, Tatsuya, Kenji Abe, Shinji Kawasoe, HirokiArimura, Hiroshi Sakamoto and Setsuo Arikawa.2002.
Efficient substructure discovery from largesemi-structured data.
Proceedings of the SecondSIAM International Conference on Data Mining,158?174.Bamman, David and Gregory Crane.
2007.The Latin Dependency Treebank in a CulturalHeritage Digital Library.
Proceedings of theWorkshop on Language Technology for Cul-tural Heritage Data, LaTeCH 2007: pp.
33?40.http://nlp.perseus.tufts.edu/syntax/treebank/Boulicaut, J.-F. and A. Bykowski.
2000.
Frequentclosures as a concise representation for binary datamining.
Knowledge discovery and data mining:current issues and new applications, PAKDD 2000:pp.
62?73.Chi, Yun, Richard R. Muntz, Siegfried Nijssen andJoost N. Kok.
2004.
Frequent Subtree Mining - AnOverview.
Fundamenta Informaticae, 66(1-2):161?198.Chi, Yun, Yi Xia, Yirong Yang and Richard R. Muntz.2005a.
Mining Closed and Maximal FrequentSubtrees from Databases of Labeled Rooted Trees.IEEE Transactions on Knowledge and Data Engi-neering, 17(2):190?202.Chi, Yun, Yi Xia, Yirong Yang and Richard R. Muntz.2005b .
Canonical forms for labelled trees and theirapplications in frequent subtree mining.
Knowledgeand Information Systems, 8(2):203?234.Dijkstra, E. W. 1959.
A note on two problems inconnexion with graphs.
Numerische Mathematik1:269?271.Firth, J.R. 1957.
Papers in Linguistics.
London: OUP.Harris, Roy and Talbot J. Taylor.
1989/1997.
Varro onLinguistic Regularity.
In Harris and Taylor, Land-marks in Linguistic Thought I: The Western Tra-dition from Socrates to Saussure.
2nd ed.
London:Routledge.
pp.
47-59.Kilpela?inen, Pekka.
1992.
Tree Matching Problemswith Applications to Structured Text Databases.PhD dissertation.
Univ.
Helsinki, Dept.
of ComputerScience.Knight, Kevin.
2007.
Capturing practical naturallanguage transformations.
Machine Translation,21:121?133.Knight, Kevin and Graehl, Jonathan.
2005.
AnOverview of Probabilistic Tree Transducers for Nat-ural Language Processing.
Proceedings of the 6thCICLing, 1?24.Koehn, Philipp.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
Proceedings of the10th Machine Translation Summit, 79?86.Lova?sz, La?szlo?
and M.D.
Plummer.
1986.
MatchingTheory.
Amsterdam: Elsevier Science.Luccio, Fabrizio, Antonio Enriquez, Pablo Rieumontand Linda Pagli.
2001.
Exact Rooted SubtreeMatching in Sublinear Time.
Universita` Di PisaTechnical Report TR-01-14.Moschitti, Alessandro.
Making tree kernels practicalfor natural language learning.
Proceedings of the11th Conference of the European Association forComputational Linguistics (EACL 2006), 113?120.Mel?c?uk, Igor A.
1988.
Dependency syntax: Theoryand practice.
Albany, NY: SUNY Press.Martens, Scott.
2009a.
Frequent Structure Discoveryin Treebanks.
Proceedings of the 19th Computa-tional Linguistics in the Netherlands (CLIN 19).Martens, Scott 2009b.
Quantitative analysis oftreebanks using frequent subtree mining methods.Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing(TextGraphs-4), 84?92.Rohde, Douglas.
2001.
Tgrep2 User Manual.http://tedlab.mit.edu/?dr/Tgrep2Sinclair, John.
1991.
Corpus, Concordance, Colloca-tion.
Oxford: OUP.Tesnie`re, Lucien.
1959.
?Ele?ments de syntaxe struc-turale.
Paris: ?Editions Klincksieck.van Noord, Gertjan.
2006.
At last parsing is nowoperational.
Verbum Ex Machina.
Actes de la13e confe?rence sur le traitement automatique deslangues naturelles (TALN6), 20?42.Mohammed J. Zaki.
2002.
Efficiently mining fre-quent trees in a forest.
Proceedings of the 8th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, 1021?1035.818
