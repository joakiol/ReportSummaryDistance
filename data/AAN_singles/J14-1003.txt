Random Walks for Knowledge-BasedWord Sense DisambiguationEneko Agirre?IXA NLP groupUniversity of the Basque CountryOier Lo?pez de Lacalle?
?University of EdinburghIKERBASQUEBasque Foundation for ScienceAitor Soroa?IXA NLP groupUniversity of the Basque CountryWord Sense Disambiguation (WSD) systems automatically choose the intended meaning of aword in context.
In this article we present a WSD algorithm based on random walks over largeLexical Knowledge Bases (LKB).
We show that our algorithm performs better than other graph-based methods when run on a graph built from WordNet and eXtended WordNet.
Our algorithmand LKB combination compares favorably to other knowledge-based approaches in the literaturethat use similar knowledge on a variety of English data sets and a data set on Spanish.
We includea detailed analysis of the factors that affect the algorithm.
The algorithm and the LKBs used arepublicly available, and the results easily reproducible.1.
IntroductionWord Sense Disambiguation (WSD) is a key enabling technology that automaticallychooses the intended sense of a word in context.
It has been the focus of intensiveresearch since the beginning of Natural Language Processing (NLP), and more recentlyit has been shown to be useful in several tasks such as parsing (Agirre, Baldwin, andMartinez 2008; Agirre et al.
2011), machine translation (Carpuat and Wu 2007; Chan,Ng, and Chiang 2007), information retrieval (Pe?rez-Agu?era and Zaragoza 2008; Zhongand Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and?
Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: e.agirre@ehu.es.??
IKERBASQUE, Basque Foundation for Science, 48011, Bilbao, Basque Country.E-mail: oier.lopezdelacalle@gmail.com.?
Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country.
E-mail: a.soroa@ehu.es.Submission received: 30 July 2011; Revised submission received: 15 November 2012; Accepted for publication:17 February 2013.doi:10.1162/COLI a 00164?
2014 Association for Computational LinguisticsComputational Linguistics Volume 40, Number 1summarization (Barzilay and Elhadad 1997).
WSD is considered to be a key step inorder to approach language understanding beyond keyword matching.The best performing WSD systems are currently those based on supervised learn-ing, as attested in public evaluation exercises (Snyder and Palmer 2004; Pradhan et al.2007), but they need large amounts of hand-tagged data, which is typically very ex-pensive to produce.
Contrary to lexical-sample exercises (where plenty of training andtesting examples for a handful of words are provided), all-words exercises (whichcomprise all words occurring in a running text, and where training data is more scarce)show that only a few systems beat the most frequent sense (MFS) heuristic, with smalldifferences.
For instance, the best system in SensEval-3 scored 65.2 F1, compared to 62.4(Snyder and Palmer 2004).
The best current state-of-the-art WSD system (Zhong and Ng2010), outperforms the MFS heuristic by 5% to 8% in absolute F1 scores on the SensEvaland SemEval fine-grained English all words tasks.The causes of the small improvement over the MFS heuristic can be found in therelatively small amount of training data available (sparseness) and the problems thatarise when the supervised systems are applied to different corpora from that used totrain the system (corpus mismatch) (Ng 1997; Escudero, Ma?rquez, and Rigau 2000).Note that most of the supervised systems for English are trained over SemCor (Milleret al.
1993), a half-a-million word subset of the Brown Corpus made available from theWordNet team, and DSO (Ng and Lee 1996), comprising 192,800 word occurrences fromthe Brown and WSJ corpora corresponding to the 191 most frequent nouns and verbs.Several researchers have explored solutions to sparseness.
For instance, Chan and Ng(2005) present an unsupervised method to obtain training examples from bilingual data,which was used together with SemCor and DSO to train one of the best performingsupervised systems to date (Zhong and Ng 2010).In view of the problems of supervised systems, knowledge-based WSD is emergingas a powerful alternative.
Knowledge-based WSD systems exploit the information ina lexical knowledge base (LKB) to perform WSD.
They currently perform below su-pervised systems on general domain data, but are attaining performance close or aboveMFS without access to hand-tagged data (Ponzetto and Navigli 2010).
In this sense, theyprovide a complementary strand of research which could be combined with supervisedmethods, as shown for instance in Navigli (2008).
In addition, Agirre, Lo?pez de Lacalle,and Soroa (2009) show that knowledge-based WSD systems can outperform supervisedsystems in a domain-specific data set, where MFS from general domains also fails.
Inthis article, we will focus our attention on knowledge-based methods.Early work for knowledge-based WSD was based on measures of similaritybetween pairs of concepts.
In order to maximize pairwise similarity for a sequence ofn words where each has up to k senses, the algorithms had to consider up to kn sensesequences.
Greedy methods were often used to avoid the combinatorial explosion(Patwardhan, Banerjee, and Pedersen 2003).
As an alternative, graph-based methodsare able to exploit the structural properties of the graph underlying a particular LKB.These methods are able to consider all possible combinations of occurring senses on aparticular context, and thus offer a way to analyze efficiently the inter-relations amongthem, gaining much attention in the NLP community (Mihalcea 2005; Navigli andLapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata 2010).The nodes in the graph represent the concepts (word senses) in the LKB, and edgesin the graph represent relations between them, such as subclass and part-of.
Networkanalysis techniques based on random walks like PageRank (Brin and Page 1998) canthen be used to choose the senses that are most relevant in the graph, and thus outputthose senses.58Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDIn order to deal with large knowledge bases containing more than 100,000 con-cepts (Fellbaum 1998), previous algorithms had to extract subsets of the LKB (Navigliand Lapata 2007, 2010) or construct ad hoc graphs for each context to be dis-ambiguated (Mihalcea 2005; Sinha and Mihalcea 2007).
An additional reason for theuse of custom-built subsets of ad hoc graphs for each context is that if we were usinga centrality algorithm like PageRank over the whole graph, it would choose the mostimportant senses in the LKB regardless of context, limiting the applicability of thealgorithm.
For instance, the word coach is ambiguous at least between the ?sportscoach?
and the ?transport service?
meanings, as shown in the following examples:(1) Nadal is sharing a house with his uncle and coach, Toni, and his physical trainer,Rafael Maymo.
(2) Our fleet comprises coaches from 35 to 58 seats.If we were to run a centrality algorithm over the whole LKB, with no context, thenwe would always assign coach to the same concept, and we would thus fail to correctlydisambiguate either one of the given examples.The contributions of this article are the following: (1) A WSD method based on ran-dom walks over large LKBs.
The algorithm outperforms other graph-based algorithmswhen using a LKB built from WordNet and eXtended WordNet.
The algorithm andLKB combination compares favorably to the state-of-the-art in knowledge-based WSDon a wide variety of data sets, including four English and one Spanish data set.
(2) Adetailed analysis of the factors that affect the algorithm.
(3) The algorithm together withthe corresponding graphs are publicly available1 and can be applied easily to senseinventories and knowledge bases different from WordNet.The algorithm for WSD was first presented in Agirre and Soroa (2009).
In this article,we present further evaluation on two more recent data sets, analyze the parameters andoptions of the system, compare it to the state of the art, and discuss the relation of ouralgorithm with PageRank and the MFS heuristic.2.
Related WorkTraditional knowledge-based WSD systems assign a sense to an ambiguous word bycomparing each of its senses with those of the surrounding context.
Typically, some se-mantic similarity metric is used for calculating the relatedness among senses (Lesk 1986;Patwardhan, Banerjee, and Pedersen 2003).
The metric varies between counting wordoverlaps between definitions of the words (Lesk 1986) to finding distances betweenconcepts following the structure of the LKB (Patwardhan, Banerjee, and Pedersen 2003).Usually the distances are calculated using only hierarchical relations on the LKB (Sussna1993; Agirre and Rigau 1996).
Combining both intuitions, Jiang and Conrath (1997)present a metric that combines statistics from corpus and a lexical taxonomy structure.One of the major drawbacks of these approaches stems from the fact that senses arecompared in a pairwise fashion and thus the number of computations grows exponen-tially with the number of words?that is, for a sequence of n words where each hasup to k senses they need to consider up to kn sense sequences.
Although alternativeslike simulated annealing (Cowie, Guthrie, and Guthrie 1992) and conceptual density1 http://ixa2.si.ehu.es/ukb.59Computational Linguistics Volume 40, Number 1(Agirre and Rigau 1996) were tried, most of the knowledge-based WSD at the time wasdone in a suboptimal word-by-word greedy process, namely, disambiguating wordsone at a time (Patwardhan, Banerjee, and Pedersen 2003).
Still, some recent work onfinding predominant senses in domains has applied such similarity-based techniqueswith success (McCarthy et al.
2007).Recently, graph-based methods for knowledge-based WSD have gained much at-tention in the NLP community (Mihalcea 2005; Navigli and Velardi 2005; Navigli andLapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata2010).
These methods use well-known graph-based techniques to find and exploit thestructural properties of the graph underlying a particular LKB.
Graph-based techniquesconsider all the sense combinations of the words occurring on a particular context atonce, and thus offer a way to analyze the relations among them with respect to thewhole graph.
They are particularly suited for disambiguating words in the sequence,and they manage to exploit the interrelations among the senses in the given context.In this sense, they provide a principled solution to the exponential explosion problemmentioned before, with excellent performance.Graph-based WSD is performed over a graph composed of senses (nodes) andrelations between pairs of senses (edges).
The relations may be of several types (lexico-semantic, cooccurrence relations, etc.)
and may have some weight attached to them.
Allthe methods reviewed in this section use some version of WordNet as a LKB.
Apartfrom relations in WordNet, some authors have used semi-automatic and fully auto-matic methods to enrich WordNet with additional relations.
Mihalcea and Moldovan(2001) disambiguated WordNet glosses in a resource called eXtended WordNet.
Thedisambiguated glosses have been shown to improve results of a graph-based system(Agirre and Soroa 2008), and we have also used them in our experiments.
Navigli andVelardi (2005) enriched WordNet with cooccurrence relations semi-automatically andshowed that those relations are effective in a number of graph-based WSD systems(Navigli and Velardi 2005; Navigli and Lapata 2007, 2010).
More recently, Cuadros andRigau (2006, 2007, 2008) learned automatically so-called KnowNets, and showed thatthe new provided relations improved WSD performance when plugged into a simplevector-based WSD system.
Finally, Ponzetto and Navigli (2010) have acquired relationsautomatically from Wikipedia, released as WordNet++, and have shown that they arebeneficial in a graph-based WSD algorithm.
All of these relations are publicly availablewith the exception of Navigli and Velardi (2005), but note that the system is availableon-line.2Disambiguation is typically performed by applying a ranking algorithm over thegraph, and then assigning the concepts with highest rank to the corresponding words.Given the computational cost of using large graphs like WordNet, most researchers usesmaller subgraphs built on-line for each target context.
The main idea of the subgraphmethod is to extract the subgraph whose vertices and relations are particularly relevantfor the set of senses from a given input context.
The subgraph is then analyzed and themost relevant vertices are chosen as the correct senses of the words.The TextRank algorithm for WSD (Mihalcea 2005) creates a complete weightedgraph (e.g., a graph in which every pair of distinct vertices is connected by a weightededge) formed by the synsets of the words in the input context.
The weight of the linksjoining two synsets is calculated by executing Lesk?s algorithm (Lesk 1986) betweenthem?that is, by calculating the overlap between the words in the glosses of the2 http://lcl.uniroma1.it/ssi.60Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDcorresponding senses.
Once the complete graph is built, a random walk algorithm(PageRank) is executed over it and words are assigned to the most relevant synset.In this sense, PageRank is used as an alternative to simulated annealing to find theoptimal pairwise combinations.
This work is extended in Sinha and Mihalcea (2007),using a collection of semantic similarity measures when assigning a weight to the linksacross synsets.
They also compare different graph-based centrality algorithms to rankthe vertices of the complete graph.
They use different similarity metrics for differentPOS types and a voting scheme among the centrality algorithm ranks.In Navigli and Velardi (2005), the authors develop a knowledge-based WSD methodbased on lexical chains called structural semantic interconnections (SSI).
Although thesystem was first designed to find the meaning of the words in WordNet glosses, theauthors also apply the method for labeling each word in a text sequence.
Given atext sequence, SSI first identifies monosemous words and assigns the correspondingsynset to them.
Then, it iteratively disambiguates the rest of the terms by selectingthe senses that get the strongest interconnection with the synsets selected so far.
Theinterconnection is calculated by searching for paths on the LKB, constrained by somehand-made rules of possible semantic patterns.In Navigli and Lapata (2007, 2010), the authors perform a two-stage process forWSD.
Given an input context, the method first explores the whole LKB in order to finda subgraph that is particularly relevant for the words of the context.
The subgraph iscalculated by applying a depth-first search algorithm over the LKB graph for everyword sense occurring in a context.
Then, they study different graph-based centralityalgorithms for deciding the relevance of the nodes on the subgraph.
As a result, everyword of the context is attached to the highest ranking concept among its possible senses.The best results were obtained by a simple algorithm like choosing the concept for eachword with the largest degree (number of edges) and by PageRank (Brin and Page 1998).We reimplemented their best methods in order to compare our algorithm with theirs onthe same setting (cf.
Section 6.3).
In later work (Ponzetto and Navigli 2010) the authorsapply a subset of their methods to an enriched WordNet with additional relations fromWikipedia, improving their results for nouns.Tsatsaronis, Vazirgiannis, and Androutsopoulos (2007) and Agirre and Soroa (2008)also use such a two-stage process.
They build the graph as before, but using breadth-first search.
The first authors apply a spreading activation algorithm over the subgraphfor node ranking, while the second use PageRank.
In later work (Tsatsaronis, Varlamis,and N?rva?g 2010) spreading activation is compared with PageRank and other centralitymeasures like HITS (Kleinberg 1998), obtaining better results than in their previouswork.This work departs from earlier work in its use of the full graph, and its ability toinfuse context information when computing the importance of nodes in the graph.
Forthis, we resort to an extension of the PageRank algorithm (Brin and Page 1998), calledPersonalized PageRank (Haveliwala 2002), which tries to bias PageRank using a set ofrepresentative topics and thus capture more accurately the notion of importance withrespect to a particular topic.
In our case, we initialize the random walk with the wordsin the context of the target word, and thus we obtain a context-dependent PageRank.We will show that this method is indeed effective for WSD.
Note that in order to useother centrality algorithms (e.g., HITS [Kleinberg 1998]), previous authors had to builda subgraph first.
In principle, those algorithms could be made context-dependent whenusing the full graph and altering their formulae, but we are not aware of such variations.Random walks over WordNet using Personalized PageRank have been also usedto measure semantic similarity between two words (Hughes and Ramage 2007; Agirre61Computational Linguistics Volume 40, Number 1et al.
2009).
In those papers, the random walks are initialized with a single word,whereas we use all content words in the context.
The results obtained by the authors,especially in the latter paper, are well above other WordNet-based methods.Most previous work on knowledge-based WSD has presented results on one or twogeneral domain corpora for English.
We present our results on four general domaindata sets for English and a Spanish data set (Ma`rquez et al.
2007).
Alternatively, someresearchers have applied knowledge-based WSD to specific domains, using differentmethods to adapt the method to the particular test domain.
In Agirre, Lo?pez de Lacalle,and Soroa (2009) and Navigli et al.
(2011), the authors apply our Personalized PageRankmethod to a domain-specific corpus with good results.
Ponzetto and Navigli (2010) alsoapply graph-based algorithms to the same domain-specific corpus.3.
WordNetMost WSD work uses WordNet as the sense inventory of choice.
WordNet (Fellbaum1998) is a freely available3 lexical database of English, which groups nouns, verbs,adjectives, and adverbs into sets of synonyms, each expressing a distinct concept (calledsynset in WordNet parlance).
For instance, coach has five nominal senses and two verbalsenses, which correspond to the following synsets:<coach#n1, manager#n2, handler#n3><coach#n2, private instructor#n1, tutor#n1><coach#n3, passenger car#n1, carriage#n1><coach#n4, four-in-hand#n2, coach-and-four#n1><coach#n5, bus#n1, autobus#n1, charabanc#n1,double-decker#n1,jitney#n1 .
.
.><coach#v1, train#v7><coach#v2>In these synsets coach#n1 corresponds to the first nominal sense of coach, coach#v1 corre-sponds to the first verbal sense, and so on.
Each of the senses of coach corresponds to adifferent synset, and each synset contains several words with different sense numbers.For instance, the first nominal sense of coach has two synonyms: manager in its secondsense and handler in its third sense.
As a synset can be identified by any of its wordsin a particular sense number, we will use a word and sense number to represent thefull concept.
Each synset has a descriptive gloss (e.g., a carriage pulled by four horses withone driver for coach#n4, or drive a coach for coach#v2).
The examples correspond to thecurrent version of WordNet (3.1), but the sense differences have varied across differentversions.
There exist automatic mappings across versions (Daude, Padro, and Rigau2000), but they contain small errors.
In this article we will focus on WordNet versions1.7 and 2.1, which have been used to tag the evaluation data sets used in this article(cf.
Section 6).The synsets in WordNet are interlinked with conceptual-semantic and lexical rela-tions.
Examples of conceptual-semantic relations are hypernymy, which corresponds tothe superclass or is-a relation, and holonymy, the part-of relation.
Figure 1 shows twosmall regions of the graph around three synsets of the word coach, including severalconceptual-semantic relations and lexical relations.
For example, the figure shows thatconcept trainer#n1 is a coach#n1 (hypernymy relation), and that seat#n1 is a part ofcoach#n5 (holonymy relation).
The figure only shows a small subset of the relations3 http://wordnet.princeton.edu.62Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDFigure 1Example showing three senses of coach, with links to related concepts.for three synsets of coach.
If we were to show the relations of the rest of the synsetsin WordNet we would end up with a densely connected graph, where one can go fromone synset to another following the semantic relations.
In addition to purely conceptual-semantic relations which hold between synsets, there are also lexical relations whichhold between specific senses.
For instance, angry#a2 is the antonym of calm#a2 and aderivation relation exists between handler#n3 and handle#v6, meaning that handler is aderived form of handle and that the third nominal sense of handler is related to the sixthverbal sense of handle.
Although lexical relations hold only between two senses, we gen-eralize to the whole synset.
This generalization captures the notion that if handler#n3 isrelated by derivation to handle#v6, then coach#n1 is also semantically related to handle#v6(as shown in Figure 1).In addition to these relations, we also use the relation between each synset andthe words in the glosses.
Most of the words in the glosses have been manually asso-ciated with their corresponding senses, and we can thus produce a link between thesynset being glossed, and the synsets of each of the words in the gloss.
For instance,following one of the given glosses, a gloss relation would be added between coach#v2and drive#v2.
The gloss relations were not available prior to WordNet 3.0, and wethus used automatically disambiguated glosses for WordNet 1.7 and WordNet 2.1, asmade freely available in the eXtended WordNet (Mihalcea and Moldovan 2001).
Notealso that the eXtended WordNet provided about 550,000 relations, whereas the disam-biguated glosses made available with WordNet 3.0 provide around 339,000 relations.We compare the performance of XWN relations and WordNet 3.0 gloss relations inSection 6.4.4.Table 1 summarizes the most relevant relations (with less frequent relationsgrouped as ?other?).
The table also lists how we grouped the relations, and the overallcounts.
Note that inverse relations are not counted, as their numbers equal those of theoriginal relation.
In Section 6.4.5 we report the impact of the relations in the behaviorof the system.
Overall, the graph for WordNet 1.7 has 109, 359 vertices (concepts) and620, 396 edges (relations between concepts).
Note that there is some overlap betweenXWN and other types of relations.
For instance, the hypernym of coach#n4 is carriage#n2,which is also present in its gloss.
Note that most of the relation types relate conceptsfrom the same part of speech, with the exception of derivation and XWN.63Computational Linguistics Volume 40, Number 1Table 1Relations and their inverses in WordNet 1.7, how we grouped them, and overall counts.
XWNrefers to relations from the disambiguated glosses in eXtended WordNet.relation inverse group countshypernymy hyponymy TAX 89,078derivation derivation REL 28,866holonymy meronymy MER 21,260antonymy antonymy ANT 7,558other other REL 3,134xwn xwn?1 XWN 551,551Finally, we have also used the Spanish WordNet (Atserias, Rigau, and Villarejo2004).
In addition to the native relations, we also added relations from the eXtendedWordNet.
All in all, it contains 105, 501 vertices and 623, 316 relations.3.1 Representing WordNet as a GraphAn LKB such as WordNet can be seen as a set of concepts and relations among them,plus a dictionary, which contains the list of words (typically word lemmas) linked tothe corresponding concepts (senses).
WordNet can be thus represented as a graph G =(V, E).
V is the set of nodes, where each node represents one concept (vi ?
V), and Eis the set of edges.
Each relation between concepts vi and vj is represented by an edgeei,j ?
E. We ignore the relation type of the edges.
If two WordNet relations exist betweentwo nodes, we only represent one edge, and ignore the type of the relation.
We chose touse undirected relations between concepts, because most of the relations are symmetricand have their inverse counterpart (cf.
Section 3), and in preliminary work we failed tosee any effect using directed relations.In addition, we also add vertices for the dictionary words, which are linked totheir corresponding concepts by directed edges (cf.
Figure 1).
Note that monosemouswords will be related to just one concept, whereas polysemous words may be attachedto several.
Section 5.2 explains the reason for using directed edges, and also mentionsan alternative to avoid introducing these vertices.4.
PageRank and Personalized PageRankThe PageRank random walk algorithm (Brin and Page 1998) is a method for rankingthe vertices in a graph according to their relative structural importance.
The main ideaof PageRank is that whenever a link from vi to vj exists in a graph, a vote from node ito node j is produced, and hence the rank of node j increases.
In addition, the strengthof the vote from i to j also depends on the rank of node i: The more important nodei is, the more strength its votes will have.
Alternatively, PageRank can also be viewedas the result of a random walk process, where the final rank of node i represents theprobability of a random walk over the graph ending on node i, at a sufficiently largetime.Let G be a graph with N vertices v1, .
.
.
, vN and di be the outdegree of node i; let Mbe a N ?
N transition probability matrix, where Mji = 1di if a link from i to j exists, and64Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDzero otherwise.
Then, the calculation of the PageRank Vector P over G is equivalent toresolving Equation (1).P = cMP+ (1 ?
c)v (1)In the equation, v is a N ?
1 stochastic vector and c is the so-called dampingfactor, a scalar value between 0 and 1.
The first term of the sum on the equationmodels the voting scheme described in the beginning of the section.
The second termrepresents, loosely speaking, the probability of a surfer randomly jumping to any node(e.g., without following any paths on the graph).
The damping factor, usually set inthe [0.85..0.95] range, models the way in which these two terms are combined ateach step.The second term in Equation (1) can also be seen as a smoothing factor that makesany graph fulfill the property of being aperiodic and irreducible, and thus guaranteesthat the PageRank calculation converges to a unique stationary distribution.In the traditional PageRank formulation the vector v is a stochastic normalizedvector whose element values are all 1N , thus assigning equal probabilities to all nodesin the graph in the case of random jumps.
However, as pointed out by Haveliwala(2002), the vector v can be non-uniform and assign stronger probabilities to certainkinds of nodes, effectively biasing the resulting PageRank vector to prefer these nodes.For example, if we concentrate all the probability mass on a unique node i, all randomjumps on the walk will return to i and thus its rank will be high; moreover, the high rankof i will make all the nodes in its vicinity also receive a high rank.
Thus, the importanceof node i given by the initial distribution of v spreads along the graph on successiveiterations of the algorithm.
As a consequence, the P vector can be seen as representingthe relevance of every node in the graph from the perspective of node i.In this article, we will use Static PageRank to refer to the case when a uniformv vector is used in Equation (1); and whenever a modified v is used, we will call itPersonalized PageRank.
The next section shows how we define a modified v.PageRank is actually calculated by applying an iterative algorithm that computesEquation (1) successively until convergence below a given threshold is achieved, oruntil a fixed number of iterations are executed.
Following usual practice, we used adamping value of 0.85 and finish the calculations after 30 iterations (Haveliwala 2002;Langville and Meyer 2003; Mihalcea 2005).
Some preliminary experiments with higheriteration counts showed that although sometimes the node ranks varied, the relativeorder among particular word synsets remained stable after the initial iterations (cf.Section 6.4 for further details).
Note that, in order to discard the effect of danglingnodes (i.e., nodes without outlinks) one would need to slightly modify Equation (1)following Langville and Meyer (2003).4 This modification is not necessary for WordNet,as it does not have dangling nodes.5.
Random Walks for WSDWe tested two different methods to apply random walks to WSD.4 The equation becomes P = cMP + (ca + (1 ?
c)e)v, where ai = 1 if node i is a dangling node,and 0 otherwise, and e is a vector of all ones.65Computational Linguistics Volume 40, Number 15.1 Static PageRank, No ContextIf we apply traditional PageRank over the whole WordNet, we get a context-independent ranking of word senses.
All concepts in WordNet get ranked accordingto their PageRank value.
Given a target word, it suffices to check which is the relativeranking of its senses, and the WSD system would output the one ranking highest.
Wecall this application of PageRank to WSD Static PageRank STATIC for short, as it doesnot change with the context, and we use it as a baseline.As the PageRank measure over undirected graphs for a node is closely relatedto the degree of the node, the Static PageRank returns the most predominant senseaccording to the number of relations the senses have.
We think that this is closely relatedto the Most Frequent Sense attested in general corpora, as the lexicon builders wouldtend to assign more relations to the most predominant sense.
In fact, our results (cf.Section 6.4.5) show that this is indeed the case for the English WordNet.5.2 Personalized PageRank, Using ContextStatic PageRank is independent of context, but this is not what we want in a WSDsystem.
Given an input piece of text we want to disambiguate all content words inthe input according to the relationships among them.
For this we can use PersonalizedPageRank (PPR for short) over the whole WordNet graph.Given an input text (e.g., a sentence), we extract the list Wi i = 1 .
.
.m of contentwords (i.e., nouns, verbs, adjectives, and adverbs) that have an entry in the dictionary,and thus can be related to LKB concepts.
As a result of the disambiguation process,every LKB concept receives a score.
Then, for each target word to be disambiguated, wejust choose its associated concept in G with maximum score.In order to apply Personalized PageRank over the LKB graph, the context wordsare first inserted into the graph G as nodes, and linked with directed edges to theirrespective concepts.
Then, the Personalized PageRank of the graph G is computed byconcentrating the initial probability mass uniformly over the newly introduced wordnodes.
As the words are linked to the concepts by directed edges, they act as sourcenodes injecting mass into the concepts they are associated with, which thus becomerelevant nodes, and spread their mass over the LKB graph.
Therefore, the resultingPersonalized PageRank vector can be seen as a measure of the structural relevance ofLKB concepts in the presence of the input context.Making the edges from words to concepts directed is important, as the use ofundirected edges will move part of the probability mass in the concepts to the wordnodes.
Note the contrast with the edges representing relations between concepts, whichare undirected (cf.
Section 3.1).Alternatively, we could do without the word nodes, concentrating the initial prob-ability mass on the senses of the words under consideration.
Such an initialization overthe graph with undirected edges between synset nodes is equivalent to initializingthe walk on the words in a graph with undirected edges between synset nodes anddirected nodes from words to synsets.
We experimentally checked that the resultsof both alternatives are indistinguishable.
Although the alternative without nodes ismarginally more efficient, we keep the word nodes as they provide a more intuitive andappealing formalization.One problem with Personalized PageRank is that if one of the target words hastwo senses that are related by semantic relations, those senses reinforce each other, andcould thus dampen the effect of the other senses in the context.
Although one could66Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDremove direct edges between competing senses from the graph, it is quite rare that thosesenses are directly linked, and usually a path with several edges is involved.
With thisobservation in mind we devised a variant called word-to-word heuristic (PPRw2w forshort), where we run Personalized PageRank separately for each target word in thecontext, that is, for each target word Wi, we concentrate the initial probability massin the senses of the rest of the words in the context of Wi, but not in the senses ofthe target word itself, so that context words increase their relative importance in thegraph.
The main idea of this approach is to avoid biasing the initial score of conceptsassociated with target word Wi, and let the surrounding words decide which conceptassociated with Wi has more relevance.
Contrary to the previous approach, PPRw2w doesnot disambiguate all target words of the context in a single run, which makes it lessefficient (cf.
Section 6.4).Figure 2 illustrates the disambiguation of a sample sentence.
The STATIC method(not shown in the figure) would choose the synset coach#n1 for the word coach becauseit is related to more concepts than other senses, and because those senses are relatedto concepts that have a high degree (for instance, sport#1).
The PPR method (left sideof Figure 2) concentrates the initial mass on the content words in the example.
Afterrunning the iterative algorithm, the system would return coach#n1 as the result for thetarget word coach.
Although the words in the sentence clearly indicate that the correctsynset in this sentence corresponds to coach#n5, the fact that teacher#n1 is related totrainer#n1 in WordNet causes both coach#n2 and coach#n1 to reinforce each other, andmake their pagerank higher.
The right side of Figure 2 depicts the PPRw2w method,where the word coach is not activated.
Thus, there is no reinforcement between coachsenses, and the method would correctly choose coach#n5 as the proper synset.6.
EvaluationWSD literature has used several measures for evaluation.
Precision is the percentage ofcorrectly disambiguated instances divided by the number of instances disambiguated.Some systems don?t disambiguate all instances, and thus the precision can be higheven if the system disambiguates a handful of instances.
In our case, when a word hasFigure 2Portion of WordNet to illustrate the disambiguation of coach in the sentence Our fleet comprisescoaches from 35 to 58 seats.
Each word in the sentence (shown partially) is linked to all its synsets.The path between trainer#n1 and teacher#1 is omitted for brevity (see Figure 1).
The left partshows the PPR method, and the right part shows the PPRw2w method.67Computational Linguistics Volume 40, Number 1two senses with the same PageRank value, our algorithm does not return anything,because it abstains from returning a sense in the case of ties.
In contrast, recall measuresthe percentage of correctly disambiguated instances divided by the total number ofinstances to be disambiguated.
This measure penalizes systems that are unable to returna solution for all instances.
Finally, the harmonic mean between precision and recall(F1) combines both measures.
F1 is our main measure of evaluation, as it provides abalanced measure between the two extremes.
Note that a system that returns a solutionfor all instances would have equal precision, recall, and F1 measures.In our experiments we build a context of at least 20 content words for each sentenceto be disambiguated, taking the sentences immediately before and after it in the case thatthe original sentence was too short.
The parameters for the PageRank algorithm wereset to 0.85 and 30 iterations following standard practice (Haveliwala 2002; Langvilleand Meyer 2003; Mihalcea 2005).
The post hoc impact of those and other parametershas been studied in Section 6.4.The general domain data sets used in this work are the SensEval-2 (S2AW) (Snyderand Palmer 2004), SensEval-3 (S3AW) (Palmer et al.
2001), and SemEval-2007 fine-grained (S07AW) (Palmer et al.
2001; Snyder and Palmer 2004; Pradhan et al.
2007)and coarse grained all-words data sets (S07CG) (Navigli, Litkowski, and Hargraves2007).
All data sets have been produced similarly: A few documents were selected fortagging, at least two annotators tagged nouns, verbs, adjectives, and adverbs, inter-tagger agreement was measured, and the discrepancies between taggers were solved.The first two data sets are labeled with WordNet 1.7 tags, the third uses WordNet2.1 tags, and the last one uses coarse-grained senses that group WordNet 2.1 senses.We run our system using WordNet 1.7 relations and senses for the first two data sets,and WordNet 2.1 for the other two.
Section 6.4.3 explores the use of WordNet 3.0 andcompares the performance with the use of other versions.Regarding the coarse senses in S07CG, we used the mapping from WordNet 2.1senses made available by the authors of the data set.
In order to return coarse grained-senses, we run our algorithm on fine-grained senses, and aggregate the scores forall senses that map to the same coarse-grained sense.
We finally choose the coarse-grained sense with the highest score.The data sets used in this article contain polysemous and monosemous words,as customary; the percentage of monosemous word occurrences in the S2AW, S3AW,S07AW, and S07CG data sets are 20.7%, 16.9%, 14.4%, and 29.9%, respectively.6.1 ResultsTable 2 shows the results as F1 of our random walk WSD systems over these datasets.
We detail overall results, as well as results per part of speech, and whether thereis any statistical difference with respect to the best result on each column.
Statisticalsignificance is obtained using the paired bootstrap resampling method (Noreen 1989),p < 0.01.The table shows that PPRw2w is consistently the best method in three data sets.
Allin all the differences are small, and in one data set STATIC obtains the best results.
Thedifferences with respect to the best system overall are always statistically significant.In fact, it is remarkable that a simple non-contextual measure like STATIC performs sowell, without the need for building subgraphs or any other manipulation.
Section 6.4.6will show that in some circumstances the performance of STATIC is much lower andanalyzes the reasons for this drop.
Regarding the use of the word-to-word heuristic, itconsistently provides slightly better results than PPR in all four data sets.
An analysis of68Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDTable 2Results on English data sets (F1).
Best results in each column in bold.
?
Statistically significantwith respect to the best result in each column.S2AW - SensEval-2 All-WordsMethod All N V Adj.
Adv.PPR 58.7?
71.8 35.0 58.9 69.8PPRw2w 59.7 70.3 40.3 59.8 72.9STATIC 58.0?
66.5 40.2 59.8 72.5S3AW - SensEval-3 All-WordsMethod All N V Adj.
Adv.PPR 57.3?
63.7 47.5 61.3 96.3PPRw2w 57.9 65.3 47.2 63.6 96.3STATIC 56.5?
62.5 47.1 62.8 96.3S07AW - SemEval 2007 All-WordsMethod All N V Adj.
Adv.PPR 39.7?
51.6 34.6 ?
?PPRw2w 41.7?
56.0 35.3 ?
?STATIC 43.0 56.0 37.3 ?
?S07CG - SemEval 2007 Coarse-grained All-WordsMethod All N V Adj.
Adv.PPR 78.1?
78.3 73.8 84.0 78.4PPRw2w 80.1 83.6 71.1 83.1 82.3STATIC 79.2?
81.0 72.4 82.9 82.8the performance according to the POS shows that PPRw2w performs better particularlyon nouns, but there does not seem to be a clear pattern for the rest.
In the rest of thearticle, we will only show the overall results, omitting those for all POS, in order not toclutter the result tables.Our algorithms do not always return an answer, and thus the precision is higherthan the F1 measure.
For instance, in S2AW the percentage of instances that get ananswer ranges between 95.4% and 95.6% for PPR, PPRw2w, and STATIC.
The precisionfor PPRw2w in S2AW is 61.1%, the recall is 58.4%, and F1 is 59.7%.
This pattern of slightlyhigher values for precisions, lower values for recall, and F1 in between is repeated forall data sets, POS, and data sets.
The percentage of instances that get an answer for theother data sets is higher, ranging between 98.1% in S3AW and 99.9% in S07CG.6.2 Comparison to State-of-the-Art SystemsIn this section we compare our results with the WSD systems described in Section 2, aswell as the top performing supervised systems at competition time and other unsuper-vised systems that improved on them.
Note that we do not mention all unsupervisedsystems participating in the competitions, but we do select the top performing ones.
Allresults in Table 3 are given as overall F1 for all Parts of Speech, but we also report F1 fornouns in the case of S07CG, where Ponz10 (Ponzetto and Navigli 2010) reported very69Computational Linguistics Volume 40, Number 1high results, but only for nouns.
Note that the systems reported here and our systemmight use different context sizes.For easier reference, Table 3 uses a shorthand for each system, whereas the text inthis section includes the shorthand and the full reference the first time the shorthand isused.
The shorthand uses the first letters of the first author followed by the year of thepaper, except for systems which participated in SensEval and SemEval, where we usetheir acronym.
Most systems in the table have been presented in Section 2, with a fewexceptions that will be presented this section.The results in Table 3 confirm that our system (PPRw2w) performs on the state-of-the-art of knowledge-based and unsupervised systems, with two exceptions:(1) Nav10 (Navigli and Lapata 2010) obtained better results on S07AW.We will compare both systems in more detail below, and also includea reimplementation in the next subsection which shows that, whenusing the same LKB, our method obtains better results.
(2) Although not reported in the table, an unsupervised system usingautomatically acquired training examples from bilingual data (Chan andNg 2005) obtained very good results on S2AW nouns (77.2 F1, comparedwith our 70.3 F1 in Table 2).
The automatically acquired training examplesare used in addition to hand-annotated data in Zhong10 (Zhong and Ng2010), also reported in the table (see below).We report the best unsupervised systems in S07AW and S07CG on the same row.JU-SKNSB (Naskar and Bandyopadhyay 2007) is a system based on an extended versionTable 3Comparison with state-of-the-art results (F1).
The top rows report knowledge-based andunsupervised systems, followed by our system (PPRw2w).
Below we report systems that useannotated data to some degree: (1) MFS or counts from hand-annotated corpora, (2) fullysupervised systems, including the best supervised participants in each exercise.
Best resultamong unsupervised systems in each column is shown in bold.
Please see text for referencesof each system.System S2AW S3AW S07AW S07CG (N)Mih05 54.2 52.2Sinha07 57.6 53.6Tsatsa10 58.8 57.4Agirre08 56.8Nav10 52.9 43.1JU-SKNSB / TKB-UO 40.2 70.2 (70.8)Ponz10 (79.4)PPRw2w 59.7 57.9 41.7 80.1 (83.6)MFS(1) 60.1 62.3 51.4 78.9 (77.4)IRST-DDD-00(1) 58.3Nav05(1) / UOR-SSI(1) 60.4 83.2 (84.1)BESTsup (2) 68.6 65.2 59.1 82.5 (82.3)Zhong10(2) 68.2 67.6 58.3 82.670Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDof the Lesk algorithm (Lesk 1986), evaluated on S07AW.
TKB-UO (Anaya-Sa?nchez,Pons-Porrata, and Berlanga-Llavori 2007), which was evaluated in S07CG, clustersWordNet senses and uses so-called topic signatures based on WordNet information fordisambiguation.
IRST-DDD-00 (Strapparava, Gliozzo, and Giuliano 2004) is a systembased on WordNet domains which leverages on large unannotated corpora.
Theyobtained excellent results, but their calculation of scores takes into account synsetprobabilities from SemCor, and the system can thus be considered to use some degreeof supervision.
We consider that systems which make use of information derived fromhand-annotated corpora need to be singled out as having some degree of supervision.This includes systems using the MFS heuristic, as it is derived from hand-annotatedcorpora.
In the case of the English WordNet, the use of the first sense also falls in thiscategory, as the order of senses in WordNet is based on sense counts in hand-annotatedcorpora.
Note that for wordnets in other languages, hand-annotated corpus is scarce,and thus our main results do not use this information.
Section 6.4.7 analyzes the resultsof our system when combined with this information.Among supervised systems, the best supervised systems at competition time arereported in a single row (Mihalcea 2002; Decadt et al.
2004; Chan, Ng, and Zhong2007; Tratz et al.
2007).
We also report Zhong10 (Zhong and Ng 2010), which is a freelyavailable supervised system giving some of the strongest results in WSD.We will now discuss in detail the systems that are most similar to our own.
We firstreview the WordNet versions and relations used by each system.
Mih05 (Mihalcea 2005)and Sinha07 (Sinha and Mihalcea 2007) apply several similarity methods, which useWordNet information from versions 1.7.1 and 2.0, respectively, including all relationsand the text in the glosses.5 Tsatsa10 (Tsatsaronis, Varlamis, and N?rva?g 2010) usesWordNet 2.0.
Agirre08 (Agirre and Soroa 2008) experimented with several LKBs formedby combining relations from different sources and versions, including WordNet 1.7 andeXtended WordNet.
Nav05 and Nav10 (Navigli and Velardi 2005; Navigli and Lapata2010) use WordNet 2.0, enriched with manually added co-occurrence relations whichare not publicly available.We can see in Table 3 that the combination of Personalized PageRank and LKBpresented in this article outperforms both Mih05 and Sinha07.
In order to factorout the difference in the WordNet version, we performed experiments using WN2.1and eXtended WordNet, yielding 58.7 and 56.5 F1 for S2AW and S3AW, respectively.Although a head-to-head comparison is not possible, the systems use similar informa-tion: Although they use glosses, our algorithm cannot directly use the glosses, and thuswe use disambiguated glosses as delivered in eXtended WordNet.
All in all the resultssuggest that analyzing the LKB structure as a graph is preferable to computing pairwisesimilarity measures over synsets to build a custom graph and then applying graphmeasures.
The results of various in-house experiments replicating Mih05 also confirmedthis observation.
Note also that our methods are simpler than the combination strategyused in Sinha07.Nav05 (Navigli and Velardi 2005) uses a knowledge-based WSD method based onlexical chains called structural semantic interconnections (SSI).
The SSI method wasevaluated on the SensEval-3 data set, as shown in row Nav05 in Table 3.
Note thatthe method labels an instance with the MFS of the word if the algorithm producesno output for that instance, which makes comparison to our system unfair, especiallygiven the fact that the MFS performs better than SSI.
In fact, it is not possible to separate5 Personal communication.71Computational Linguistics Volume 40, Number 1the effect of SSI from that of the MFS, and we thus report it as using some degree ofsupervision in the table.
A variant of the algorithm called UOR-SSI (Navigli, Litkowski,and Hargraves 2007) (reported in the same row) used a manually added set of 70,000relations and obtained the best results in S07CG out-of-competition,6 even better thanthe best supervised method.
Reimplementing SSI is not trivial, so we did not check theperformance of a variant of SSI that does not use MFS and that uses the same LKB asour method.
Section 6.4.7 analyzes the results of our system when combined with MFSinformation.Agirre08 (Agirre and Soroa 2008) uses breadth-first search to extract subgraphs ofthe WordNet graph for each context to be disambiguated, and then applies PageRank.Our better results seem to indicate that using the full graph instead of those subgraphswould perform better.
In order to check whether the better results are due to differencesin the information used, the next subsection presents the results of our reimplementa-tion of the systems using the same information as our full-graph algorithm.Tsatsa10 (Tsatsaronis, Vazirgiannis, and Androutsopoulos 2007; Tsatsaronis,Varlamis, and N?rva?g 2010) also builds the graph using breadth-first search, butweighting each type of edge differently, and using graph-based measures that take intoaccount those weights.
This is in contrast to the experiments performed in this articlewhere edges have no weight, and is an interesting avenue for future work.Nav10 (Navigli and Lapata 2010) first builds a subgraph of WordNet composed ofpaths between synsets using depth-first search and then applies a set of graph centralityalgorithms.
The best results are obtained using the degree of the nodes, and they presenttwo variants, depending on how they treat ties: Either they return a sense at random, orthey return the most frequent sense.
For fair comparison to our system (which does notuse MFS as a back-off), Table 3 reports the former variant as Nav10.
This system is betterthan ours in one data set and worse in another.
They use 60,000 relations that are notpublicly available, but they do not use eXtended WordNet relations.
In order to checkwhether the difference in performance is due to the relations used or the algorithm,the next subsection presents a reimplementation of their best graph-based algorithmsusing the same LKB as we do.
In earlier work (Navigli and Lapata 2007) they test asimilar system on S3AW, but report results only for nouns, verbs, and adjectives (F1 of61.9, 36.1, and 62.8, respectively), all of which are below the results of our system (cf.Table 2).In Ponz10 (Ponzetto and Navigli 2010) the authors apply the same techniques as inNav10 to a new resource called WordNet++.
They report results for nouns using degreeon subgraphs for the S07CG data set, as shown in Table 3.
Their F1 on nouns is 79.4,lower than our results using our LKB.6.3 Comparison with Related AlgorithmsThe previous section shows that our algorithm when applied to a LKB built fromWordNet and eXtended WordNet outperforms other knowledge-based systems in allcases but one system in one data set.
In this section we factor out algorithm and LKB,and present the results of other graph-based methods for WSD using the same WordNetversions and relations as in the previous section.
As we mentioned in Section 2, oursis the only method using the full WordNet graph.
Navigli and Lapata (2010) andPonzetto and Navigli (2010) build a custom graph based on the relations in WordNet6 The task was co-organized by the authors.72Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDas follows: For each sense si of each word in the context, a depth-first search (DFS forshort) is conducted through the WordNet graph starting in si until another sense sj of aword in the context is found or maximum distance is reached.
The maximum distancewas set by the authors to 6.
All nodes and edges between si and sj, inclusive, are added tothe subgraph.
Graph-based measures are then used to select the output senses for eachtarget word, with degree and PageRank yielding the best results.
In closely related work,Agirre and Soroa (2008) and Tsatsaronis, Varlamis, and N?rva?g (2010) use breadth-firstsearch (BFS) over the whole graph, and keep all paths connecting senses.
Note thatunlike the dfs approach, bfs does not require any threshold.
The subgraphs obtainedby each of these methods are slightly different.We reimplemented both strategies, namely, DFS with threshold 6 and BFS with nothreshold.
Table 4 shows the overall results of degree and PageRank for both kinds ofsubgraphs.
DFS yields slightly better results than BFS but PPRw2w is best in all four datasets, with statistical significance.In addition, we run PPR and PPRw2w on DFS and BFS subgraphs, and obtained betterresults than degree and PageRank in all data sets.
DFS with PPR and DFS with PPRw2ware best in S3AW and S07AW, respectively, although the differences with PPRw2w are notstatistically significant.
PPRw2w on the full graph is best in two data sets, with statisticalsignificance.From these results we can conclude that PPR and PPRw2w yield the best resultsalso for subgraphs.
Regarding the use of the full graph with respect to DFS or BFS,the performances for PPRw2w are very similar, but using the full graph gives a smalladvantage.
Section 6.4.5 provides an analysis of efficiency.6.4 Analysis of Performance FactorsThe behavior of the WSD system is influenced by a set of parameters that can yielddifferent results.
In our main experiments we did not perform any parameter tuning;we just used some default values which were found to be useful according to previouswork.
In this section we perform a post hoc analysis of several parameters on the generalperformance of the system, reporting F1 on a single data set, S2AW.Table 4Results for subgraph methods compared with our method (F1).
In the Reference column wemention the reference system that we reimplemented.
Best results in each column in bold.
?Statistically significant with respect to the best result in each column.
0 No significant difference.Reference S2AW S3AW S07AW S07CGDFSdegree Nav10, Ponz10 58.4?
56.4?
40.3?
79.4?BFSdegree 57.9?
56.5?
39.9?
79.2?DFSPageRank Nav10 58.2?
56.4?
39.9?
79.6?BFSPageRank Agirre08 57.7?
56.7?
39.7?
79.4?DFSPPR 59.3?
58.2 41.40 78.1?BFSPPR 58.80 57.50 41.20 78.8?DFSPPRw2w 58.70 58.00 41.2?
79.7?BFSPPRw2w 58.1?
57.90 41.9 79.5?PPRw2w 59.7 57.90 41.70 80.173Computational Linguistics Volume 40, Number 15556575859600 5 10 15 20 25 30Iterations  Figure 3Convergence according to number of PageRank iterations (F1 on S2AW).6.4.1 PageRank Parameters.
The PageRank algorithm has two main parameters, theso-called damping factor and the number of iterations (or, conversely, the convergencethreshold), which we set as 0.85 and 30, respectively (cf.
Section 4).
Figure 3 depictsthe effect of varying the number of iterations.
It shows that the algorithm convergesvery quickly: One sole iteration yields relatively high performance, and 20 iterationsare enough to achieve convergence.
Note also that the performance is in the [58.0, 58.5]range for iterations over 5.
Note that we use the same range of F1 for the y axis of Figures3, 4, and 5 for easier comparison.Figure 4 shows the effect of varying the damping factor.
Note that a dampingfactor of zero means that the PageRank value coincides with the initial probabilitydistribution.
Given the way we initialize the distribution (c.f.
Section 5.2), it would meanthat the algorithm is not able to disambiguate the target words.
Thus, the initial valueon Figure 4 corresponds to a damping factor of 0.001.
On the other hand, a dampingfactor of 1 yields to the same results as the STATIC method (c.f.
Section 5.1).
The bestvalue is attained with 0.90, with similar values around it (less than 0.5 absolute points in5556575859600 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Damping factorFigure 4Varying the damping factor (F1 on S2AW).74Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD5556575859605 10 15 20 25 30 35 40 45 50Context size  Figure 5Varying the context size (F1 on S2AW).variation), in agreement with previous results which preferred values in the 0.85...0.95interval (Haveliwala 2002; Langville and Meyer 2003; Mihalcea 2005).6.4.2 Size of Context Window.
Figure 5 shows the performance of the system when tryingdifferent context windows for the target words.
The best context size is for windowsof 20 content words, with less than 0.5 absolute point losses for windows in the [5, 25]range.6.4.3 Using Different WordNet Versions.
There has been little research on the best strat-egy to use when dealing with data sets and resources attached to different WordNetversions.
Table 5 shows the results for the four data sets used in this study when usingdifferent WordNet versions.
Two of the data sets (S2AW and S3AW) were tagged withsenses from version 1.7, S07AW with senses from version 2.1, and S07CG with coarsesenses built on version 2.1 senses.Given the fact that WordNet 3.0 is a more recent version that includes more rela-tions, one would hope that using it would provide the best results (Cuadros and Rigau2008; Navigli and Lapata 2010).
We built a graph analogous to the ones for versions1.7 and 2.1, but using the hand-disambiguated glosses instead of eXtended WordNetglosses.
We used freely available mappings (Daude, Padro, and Rigau 2000)7 to convertour eXtended WordNet relations to 3.0, and then the WordNet 3.0 sense results to thecorresponding version.
In addition, we also tested WN1.7 on S07AW and S07CG, andWN2.1 on S2AW and S3AW, also using the mappings from Daude, Padro, and Rigau(2000).Table 5 shows that the best results are obtained using our algorithm on the sameWordNet version as used in the respective data set.
When testing on data sets taggedwith WordNet 1.7, similar results are obtained using 2.1 or 3.0.
When testing on data setsbased on 2.1, 3.0 has a small lead over 1.7.
In any case, the differences are small rangingfrom 1.4 absolute points to 0.5 points.
All in all, it seems that the changes introduced7 http://nlp.lsi.upc.edu/tools/download-map.php.75Computational Linguistics Volume 40, Number 1Table 5Comparing WordNet versions.
Best result in each row in bold.Data set version 1.7 + xwn 2.1 + xwn 3.0 + xwnS2AW 1.7 59.7 58.7 58.4S3AW 1.7 57.9 56.5 56.8S07AW 2.1 40.7 41.7 40.9S07CG 2.1 coarse 79.6 80.1 79.6by different versions slightly deteriorate the results, and the best strategy is to use thesame WordNet version as was used for tagging.6.4.4 Using xwn vs. WN3.0 Gloss Relations.
WordNet 3.0 was released with an accom-panying data set comprising glosses where some of the words had been manuallydisambiguated.
In Table 6 we present the results of using these glosses with the WN3.0graph, showing that the results are lower than using XWN relations.
We also checkedthe use of WN3.0 gloss relation with other WordNet versions, and the results usingXWN were always slightly better.
We hypothesize that the better results for XWN aredue to the amount of relations, with XWN holding 61% more relations than WN3.0glosses.
Still, the best relations are obtained with the combination of both kinds of glossrelations.6.4.5 Analysis of Relations.
Previous results were obtained using all the relationsof WordNet and taking eXtended WordNet relations into account.
In this sectionwe analyze the effect of the relation types on the whole process, following therelation groups presented in Table 1.
Table 7 shows the results when using differentcombinations over relation types.
The eXtended WordNet XWN relations appear themost valuable when performing random walk WSD, as their performance is as goodas when using the whole graph, and they produce a large drop when ablated from thegraph.
Ignoring antonymy relations produces a small improvement, but the differencesbetween using all the relations, eliminating antonyms, and using XWN relations onlyare too small to draw any further conclusions.
It seems that given the XWN relations(the most numerous, cf.
Section 3.1), our algorithm is fairly robust to the addition ordeletion of other kinds of relations (less numerous).6.4.6 Behavior with Respect to STATIC and MFS.
The high results of the very simple STATICmethod (PageRank with no context) seems to imply that there is no need to use contextfor disambiguation.
Our intuition was that the synsets which correspond to the mostTable 6Comparing XWN and WN3.0 gloss relations, separately and combined.
Best result in each row inbold.Data set 3.0 + XWN 3.0 + gloss 3.0 + XWN + glossS2AW 58.4 58.1 58.8S3AW 56.8 51.7 56.1S07AW 40.9 38.8 42.2S07CG 79.6 78.9 80.276Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDTable 7Analysis of relation types.
The first column shows the performance using just that relation type.The second shows the combination of TAX and each type.
The last column shows all relationsexcept the corresponding type.relation single + TAX ablationTAX 37.4 ?
59.9ANT 19.1 42.1 59.9MER 23.4 36.4 59.6REL 35.4 46.1 59.6XWN 59.9 59.8 47.1Reference system (all relations) 59.7frequent senses would get more relations.
We thus computed the correlation betweensystems, gold tags, and MFS.
In order to make the correlation results comparable tothe figures used on evaluation, we use the number of times both sets of results agree,divided by the number of results returned by the first system.
Table 8 shows such amatrix of pairwise correlations.
If we take the row of gold tags, the results reflect theperformance of each system (the precision).
In the case of MFS, the column shows thatSTATIC has a slightly larger correlation with the MFS than the other two methods.
Thematrix also shows that all our three methods agree more than 80% of the time, with PPRand STATIC having a relatively smaller agreement.In contrast, related work using the same techniques over domain-specific words(Agirre, Lo?pez de Lacalle, and Soroa 2009) shows that the results of our PersonalizedPageRank models departs significantly from MFS and STATIC.
Table 9 shows the resultsof the three techniques on the three subcorpora that constitute the evaluation data setpublished in Koeling, McCarthy, and Carroll (2005).
This data set consists of examplesretrieved from the Sports and Finance sections of the Reuters corpus, and also from thebalanced British National Corpus (BNC), which is used as a general domain contrastcorpus.Applying PageRank over the entire WordNet graph yields low results, very similarto those of MFS, and below those of Personalized PageRank.
This confirms that STATICPageRank is closely related to MFS, as we hypothesized in Section 5.1 and showed inTable 8 for the other general domain data sets.
Whereas the results of PPRw2w are verysimilar in the general-domain BNC, PPRw2w departs from STATIC and MFS with 30 and20 points of difference in the domain-specific Sports and Finance corpora.
These resultsare highly relevant, because they show that PPR is able to effectively use contextualinformation, and depart from the MFS and STATIC baselines.Table 8Correlation between systems, gold tags, and MFS.Gold MFS PPR PPRw2w STATICGold 100.0 61.3 58.6 59.7 57.8MFS 60.1 100.0 79.8 79.0 81.3PPR 57.4 79.8 100.0 86.8 82.8PPRw2w 58.4 79.0 86.8 100.0 86.4STATIC 56.7 81.4 82.8 86.4 100.077Computational Linguistics Volume 40, Number 1Table 9Results on three subcorpora as reported in Agirre, Lo?pez de Lacalle, and Soroa (2009),where Sports and Finance are domain-specific.
Best results on each column in bold.System BNC Sports FinanceMFS 34.9 19.6 37.1STATIC 36.6 20.1 39.6PPRw2w 37.7 51.5 59.3Table 10Combination with MFS (F1).
The first two rows correspond to our system with and withoutinformation from MFS.
Below that we report systems that also use MFS.
Best results in eachcolumn in bold.System S2AW S3AW S07AW S07CG (N)PPRw2w 59.7 57.9 41.7 80.1 (83.6)PPRw2w MFS 62.6 63.0 48.6 81.4 (82.1)MFS 60.1 62.3 51.4 78.9 (77.4)IRST-DDD-00 58.3Nav05 / UOR-SSI 60.4 83.2 (84.1)Ponz10 81.7 (85.5)6.4.7 Combination with MFS.
As mentioned in Section 6.2, we have avoided using anyinformation regarding sense frequencies from annotated corpora, as this informationis not always available for all wordnets.
In this section we report the results of ouralgorithm when taking into account prior probabilities of senses taken from sensecounts.
We used the sense counts provided with WordNet in the index.sense file.8 Inthis setting, the edges linking words and their respective senses are weighted accordingto the prior probabilities of those senses, instead of uniform weights as in Section 5.2.Table 10 shows that results when using priors from MFS improve over the resultsof the original PPRw2w in all data sets.
The improvement varies across parts of speech,and, for instance, the results for nouns in S07CG are worse (shown in rightmost columnof Table 10).
In addition, the results for PPRw2w when using MFS information improveover MFS in all cases except for S07AW.The table also reports the best systems that do use MFS (see Section 6.3 for detailedexplanations).
For S2AW and S07AW we do not have references to related systems.For S3AW we can see that our system performs best.
In the case of S07CG, UOR-SSIreports better results than our system.
Finally, the final row reports their system whencombined with MFS information as back-off (Ponzetto and Navigli 2010), which alsoattains better results than our system.
We tried to use a combination method similar totheirs, but did not manage to improve results.6.4.8 Efficiency of Full Graphs vs. Subgraphs.
Given the very close results of our algorithmwhen using full graphs and subgraphs (cf.
Section 6.3), we studied the efficiency of each.We benchmarked several graph-based methods on the S2AW data set, which comprises8 http://wordnet.princeton.edu/wordnet/man/senseidx.5WN.html.78Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD2,473 instances to be disambiguated.
All tests were done on a multicore computer with16 GB of memory using a single 2.66 GHz processor.
When using the full graph PPRdisambiguates full sentences in one go at 684 instances per minute, whereas PPRw2wdisambiguates one word at a time, 70 instances per minute.
The DFS subgraphs providebetter performance than PPRw2w, 228 instances per minute when using degree, withmarginally slower performance when using PPRw2w (210 instances per minute).
TheBFS subgraph is slowest, with around 20 instances per minute.
The memory footprint ofusing the full graph algorithm is small, just 270 MB, so several processes can be run ona multiprocessor machine easily.All in all, there is a tradeoff in performance and speed, with PPRw2w on the fullgraph providing better results at the cost of some speed, and PPR on the full graphproviding the best speed at the cost of worse performance.
Using DFS with PPRw2wlays in between and is also a good alternative, and its speed can be improved usingpre-indexed paths.6.5 Experiments on SpanishOur WSD algorithm can be applied over non-English texts, provided that a LKB for thisparticular language exists.
We have applied our random walk algorithms to the SpanishWordNet (Atserias, Rigau, and Villarejo 2004), using the SemEval-2007 Task 09 data setas evaluation gold standard (Ma`rquez et al.
2007).
The data set contains examples of the150 most frequent nouns in the CESS-ECE corpus, manually annotated with SpanishWordNet synsets.
It is split into a train and test part, and has an ?all words?
shape(i.e., input consists of sentences, each one having at least one occurrence of a targetnoun).
We ran the experiment over the test part (792 instances), and used the train partfor calculating the MFS heuristic.
The results in Table 11 are consistent with those forEnglish, with our algorithms approaching MFS performance, and PPRw2w yielding thebest results.
Note that for this data set the supervised algorithm could barely improveover the MFS, which performs very well, suggesting that in this particular data set thesense distributions are highly skewed.Finally, we also show results for the first sense in the Spanish WordNet.
In theSpanish WordNet the order of the senses of a word has been assigned directly bythe lexicographer (Atserias, Rigau, and Villarejo 2004), as there is no informationof sense frequency from hand-annotated corpora.
This is in contrast to the EnglishWordNet, where the senses are ordered according to their frequency in annotatedTable 11Results as F1 on the Spanish SemEval07 data set, including first sense, MFS, and the bestsupervised system in the competition.
?
Statistically significant difference with respect tothe best of our results (in bold).Method Acc.PPR 78.4?PPRw2w 79.3STATIC 76.5?First sense 66.4?MFS 84.6?BEST 85.1?79Computational Linguistics Volume 40, Number 1corpora (Fellbaum 1998), and reflects the status on most other wordnets.
In this case,our algorithm clearly improves over the first sense in the dictionary.7.
ConclusionsIn this article we present a method for knowledge-based Word Sense Disambiguationbased on random walks over relations in a LKB.
Our algorithm uses the full graphof WordNet efficiently, and performs better than PageRank or degree on subgraphs(Navigli and Lapata 2007; Agirre and Soroa 2008; Navigli and Lapata 2010; Ponzettoand Navigli 2010).
We also show that our combination of method and LKB built fromWordNet and eXtended WordNet compares favorably to other knowledge-basedsystems using similar information sources (Mihalcea 2005; Sinha and Mihalcea2007; Tsatsaronis, Vazirgiannis, and Androutsopoulos 2007; Tsatsaronis, Varlamis, andN?rva?g 2010).
Our analysis shows that Personalized PageRank yields similar resultswhen using subgraphs and the full graph, with a trade-off between speed and perfor-mance, where Personalized PageRank over the full graph is fastest, its word-to-wordvariant slowest, and Personalized PageRank over the subgraph lies in between.We also show that the algorithm can be easily ported to other languages with goodresults, with the only requirement of having a wordnet.
Our results improve over thefirst sense of the Spanish dictionary.
This is particularly relevant for wordnets otherthan English.
For the English WordNet the senses of a word are ordered according to thefrequency of the senses in hand-annotated corpora, and thus the first sense is equivalentto the Most Frequent Sense, but this information is not always available for languagesthat lack large-scale hand-annotated corpora.We have performed an extensive analysis, showing the behavior according to theparameters of PageRank, and studying the impact of different relations and WordNetversions.
We have also analyzed the relation between our PPR algorithm, MFS, andSTATIC PageRank.
In general domain corpora they get similar results, close to theperformance of the MFS learned from SemCor, but the results reported on domain-specific data sets (Agirre, Lo?pez de Lacalle, and Soroa 2009) show that PPR is able tomove away from the MFS and STATIC and improve over them, indicating that PPR is ableto effectively use contextual information, and depart from MFS and STATIC PageRank.The experiments in this study are readily reproducible, as the algorithm and theLKBs are publicly available.9 The system can be applied easily to sense inventories andknowledge bases different from WordNet.In the future we would like to explore methods to incorporate global weights of theedges in the random walk calculations (Tsatsaronis, Varlamis, and N?rva?g 2010).
Giventhe complementarity of the WordNet++ resource (Ponzetto and Navigli 2010) and ouralgorithm, it would be very interesting to explore the combination of both, as well asthe contribution of other WordNet related resources (Cuadros and Rigau 2008).AcknowledgmentsWe are deeply indebted to the reviewers,who greatly helped to improve thearticle.
Our thanks to Rob Koeling andDiana McCarthy for kindly providingthe data set, thesauri, and assistance,and to Roberto Navigli and SimonePonzetto for clarifying the method tomap to coarse-grained senses.
Thiswork has been partially funded by theEducation Ministry (project KNOW2TIN2009-15049-C03-01).9 http://ixa2.si.ehu.es/ukb.80Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDReferencesAgirre, E., E. Alfonseca, K. Hall, J. Kravalova,M.
Pasca, and A. Soroa.
2009.
A studyon similarity and relatedness usingdistributional and WordNet-basedapproaches.
In Proceedings of the NorthAmerican Chapter of the Association forComputational Linguistics - Human LanguageTechnologies conference (NAACL/HLT?09),pages 19?27, Boulder, CO.Agirre, E., T. Baldwin, and D. Martinez.2008.
Improving parsing and PPattachment performance with senseinformation.
In Proceedings of the46th Annual Meeting of the Association forComputational Linguistics (ACL/HLT?08),pages 317?325, Columbus, OH.Agirre, E., K. Bengoetxea, K. Gojenola,and J. Nivre.
2011.
Improvingdependency parsing with semanticclasses.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies(ACL/HLT?11), pages 699?703,Portland, OR.Agirre, E., O.
Lo?pez de Lacalle, and A. Soroa.2009.
Knowledge-based WSD on specificdomains: Performing better than genericsupervised WSD.
In Proceedings of the19th International Joint Conference onArtificial Intelligence (IJCAI?09),pages 1,501?1,506, Pasadena, CA.Agirre, E. and G. Rigau.
1996.
Word sensedisambiguation using conceptual density.In Proceedings of the 16th InternationalConference on Computational Linguistics(COLING?96), pages 16?22, Copenhagen.Agirre, E. and A. Soroa.
2008.
Using themultilingual central repository forgraph-based word sense disambiguation.In Proceedings of the 6th Conference onLanguage Resources and Evaluation(LREC ?08), pages 1,388?1,392, Marrakesh.Agirre, E. and A. Soroa.
2009.
PersonalizingPageRank for word sense disambiguation.In Proceedings of the 12th Conference of theEuropean Chapter of the Association forComputational Linguistics (EACL?09),pages 33?41, Athens.Anaya-Sa?nchez, H., A. Pons-Porrata, andR.
Berlanga-Llavori.
2007.
TKB-UO: Usingsense clustering for WSD.
In Proceedings ofthe 4th International Workshop on SemanticEvaluations (SemEval-2007) in conjunctionwith ACL, pages 322?325, Prague.Atserias, J., G. Rigau, and L. Villarejo.
2004.Spanish wordnet 1.6: Porting the Spanishwordnet across Princeton versions.In Proceedings of the 4th Conference onLanguage Resources and Evaluation(LREC?04), pages 161?164, Lisbon.Barzilay, R. and M. Elhadad.
1997.
Usinglexical chains for text summarization.In Proceedings of the ACL Workshop onIntelligent Scalable Text Summarization,pages 10?17, New York, NY.Brin, S. and L. Page.
1998.
The anatomyof a large-scale hypertextual Web searchengine.
Computer Networks and ISDNSystems, 30(1-7):107?117.Carpuat, M. and D. Wu.
2007.
Improvingstatistical machine translation using wordsense disambiguation.
In Proceedings ofthe Joint Conference on Empirical Methodsin Natural Language Processing andComputational Natural Language Learning(EMNLP/CoNLL?07), pages 61?72, Prague.Chan, Y. S. and H. T. Ng.
2005.
Scalingup word sense disambiguation viaparallel texts.
In Proceedings of the20th National Conference on ArtificialIntelligence (AAAI?05), pages 1,037?1,042,Pittsburgh, PA.Chan, Y. S., H. T. Ng, and D. Chiang.2007.
Word sense disambiguationimproves statistical machine translation.In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics(ACL?07), pages 33?40, Prague.Chan, Y. S., H. T. Ng, and Z. Zhong.
2007.NUS-PT: Exploiting parallel texts forword sense disambiguation in the Englishall-words tasks.
In Proceedings of the4th International Workshop on SemanticEvaluations (SemEval-2007) in conjunctionwith ACL, pages 253?256, Prague.Cowie, J., J. Guthrie, and L. Guthrie.
1992.Lexical disambiguation using simulatedannealing.
In Proceedings of the Workshopon Speech and Natural Language (HLT?91),pages 238?242, Morristown, NJ.Cuadros, M. and G. Rigau.
2006.
Qualityassessment of large scale knowledgeresources.
In Proceedings of Joint SIGDATConference on Empirical Methods in NaturalLanguage Processing (EMNLP?06),pages 534?541, Sydney.Cuadros, M. and G. Rigau.
2007.Semeval-2007 task 16: Evaluation ofwide coverage knowledge resources.In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations(SemEval-2007) in conjunction with ACL,pages 81?86, Prague.Cuadros, M. and G. Rigau.
2008.
Knownet:Using topic signatures acquired from theWeb for building automatically highlydense knowledge bases.
In Proceedings81Computational Linguistics Volume 40, Number 1of the 22nd International Conference onComputational Linguistics (COLING?08),pages 71?84, Manchester.Daude, J., L. Padro, and G. Rigau.
2000.Mapping WordNets using structuralinformation.
In Proceedings of the 38thAnnual Meeting of the Association forComputational Linguistics (ACL?00),pages 504?511, Hong Kong.Decadt, B., V. Hoste, W. Daelemans, andA.
Van Den Bosch.
2004.
GAMBL, geneticalgorithm optimization of memory-basedWSD.
In Proceedings of SENSEVAL-3 ThirdInternational Workshop on Evaluation ofSystems for the Semantic Analysis of Text,pages 108?112, Barcelona.Escudero, G., L. Ma?rquez, and G. Rigau.2000.
An empirical study of the domaindependence of supervised word sensedisambiguation systems.
Proceedings ofthe Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processingand Very Large Corpora (EMNLP/VLC?00),pages 172?180, Hong Kong.Fellbaum, C., editor.
1998.
WordNet: AnElectronic Lexical Database and Some of ItsApplications.
MIT Press, Cambridge, MA.Haveliwala, T. H. 2002.
Topic-sensitivePageRank.
In Proceedings of the11th International Conference on WorldWide Web (WWW?02), pages 517?526,New York, NY.Hughes, T. and D. Ramage.
2007.
Lexicalsemantic relatedness with random graphwalks.
In Proceedings of the Joint Conferenceon Empirical Methods in Natural LanguageProcessing and Computational NaturalLanguage Learning (EMNLP/CoNLL?07),pages 581?589, Prague.Jiang, J. J. and D. W. Conrath.
1997.
Semanticsimilarity based on corpus statistics andlexical taxonomy.
In Proceedings of theInternational Conference on Research inComputational Linguistics, pages 19?33,Taiwan.Kleinberg., J. M. 1998.
Authoritativesources in a hyperlinked environment.In Proceedings of the Ninth AnnualACM-SIAM Symposium on DiscreteAlgorithms (SODA?98), pages 668?677,Philadelphia, PA.Koeling, R., D. McCarthy, and J. Carroll.2005.
Domain-specific sense distributionsand predominant sense acquisition.In Proceedings of the Human LanguageTechnology Conference and Conference onEmpirical Methods in Natural LanguageProcessing (HLT/EMNLP?05),pages 419?426, Ann Arbor, MI.Langville, A. N. and C. D. Meyer.
2003.Deeper inside PageRank.
InternetMathematics, 1(3):335?380.Lesk, M. 1986.
Automatic sensedisambiguation using machine readabledictionaries: How to tell a pine cone froman ice cream cone.
In Proceedings of the5th Annual International Conference onSystems Documentation (SIGDOC?86),pages 24?26, New York, NY.Ma`rquez, L., M. A. Villarejo, T.
Mart?
?, andM.
Taule?.
2007.
SemEval-2007 Task 09:Multilevel semantic annotation ofCatalan and Spanish.
In Proceedings of the4th International Workshop on SemanticEvaluations (SemEval-2007) in conjunctionwith ACL, pages 42?47, Prague.McCarthy, D., R. Koeling, J. Weeds,and J. Carroll.
2007.
Unsupervisedacquisition of predominant word senses.Computational Linguistics, 33(4):553?590.Mihalcea, R. 2002.
Word sensedisambiguation with pattern learningand automatic feature selection.
NaturalLanguage Engineering, 8:343?358.Mihalcea, R. 2005.
UnsupervisedLarge-vocabulary word sensedisambiguation with graph-basedalgorithms for sequence data labeling.In Proceedings of the Conference onHuman Language Technology andEmpirical Methods in Natural LanguageProcessing (HLT?05), pages 411?418,Morristown, NJ.Mihalcea, R. and D. I. Moldovan.
2001.eXtended WordNet: Progress report.In Proceedings of the NAACL Workshopon WordNet and Other Lexical Resources,pages 95?100, Pittsburgh, PA.Miller, G. A., C. Leacock, R. Tengi, and R.Bunker.
1993.
A semantic concordance.In Proceedings of the Workshop onHuman Language Technology (HLT?93),pages 303?308, Plainsboro, NJ.Naskar, S. K. and S. Bandyopadhyay.2007.
JU-SKNSB: Extended WordNetbased WSD on the English all-wordstask at SemEval-1.
In Proceedings of the4th International Workshop on SemanticEvaluations (SemEval-2007) in conjunctionwith ACL, pages 203?206, Prague.Navigli, R. 2008.
A structural approachto the automatic adjudication of wordsense disagreements.
Natural LanguageEngineering, 14(4):547?573.Navigli, R. and M. Lapata.
2007.
Graphconnectivity measures for unsupervisedword sense disambiguation.
In Proceedingsof the 17th International Joint Conference82Agirre, Lo?pez de Lacalle, and Soroa Random Walks for Knowledge-Based WSDon Artificial Intelligence (IJCAI?07),pages 1,683?1,688, Hyderabad.Navigli, R. and M. Lapata.
2010.An experimental study of graphconnectivity for unsupervised wordsense disambiguation.
IEEE Transactionson Pattern Analysis and MachineIntelligence, 32(4):678?692.Navigli, R., K. C. Litkowski, andO.
Hargraves.
2007.
SemEval-2007 Task 07:Coarse-grained English all-words task.In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations(SemEval-2007) in conjunction with ACL,pages 30?35, Prague.Navigli, R. and P. Velardi.
2005.
Structuralsemantic interconnections: A knowledge-based approach to word sensedisambiguation.
IEEE Transactions onPattern Analysis and Machine Intelligence,27(7):1,075?1,086.Navigli, Roberto, Stefano Faralli, Aitor Soroa,Oier Lo?pez de Lacalle, and Eneko Agirre.2011.
Two birds with one stone: Learningsemantic models for text categorizationand word sense disambiguation.In Proceedings of CIKM, pages 2,317?2,320,Glasgow.Ng, H. T. and H. B. Lee.
1996.
Integratingmultiple knowledge sources todisambiguate word sense: An exemplar-based approach.
In Proceedings of the34th Annual Meeting of the Association forComputational Linguistics, ACL ?96,pages 40?47, Stroudsburg, PA.Ng, T. H. 1997.
Getting serious aboutword sense disambiguation.In Proceedings of the ACL SIGLEXWorkshop on Tagging Text with LexicalSemantics: Why, What, and How?,pages 1?7, Washington, DC.Noreen, E. W. 1989.
Computer-IntensiveMethods for Testing Hypotheses.John Wiley & Sons.Palmer, M., C. Fellbaum, S. Cotton, L. Delfs,and H. T. Dang.
2001.
English tasks:All-words and verb lexical sample.In Proceedings of SENSEVAL-2: SecondInternational Workshop on EvaluatingWord Sense Disambiguation Systems,pages 21?24, Toulouse.Patwardhan, S., S. Banerjee, andT.
Pedersen.
2003.
Using measures ofsemantic relatedness for word sensedisambiguation.
In Proceedings of theFourth International Conference onIntelligent Text Processing andComputational Linguistics (CICLING-03),pages 241?257, Mexico City.Pe?rez-Agu?era, J. R. and H. Zaragoza.
2008.UCM-Y!R at CLEF 2008 Robust and WSDtasks.
In Proceedings of the 9th CrossLanguage Evaluation Forum Workshop(CLEF?08), pages 138?145, Aarhus.Ponzetto, S. P. and R. Navigli.
2010.Knowledge-rich word sensedisambiguation rivaling supervisedsystem.
In Proceedings of the 48th AnnualMeeting of the Association for ComputationalLinguistics (ACL?10), pages 1,522?1,531,Uppsala.Pradhan, S., E. Loper, D. Dligach, andM.
Palmer.
2007.
SemEval-2007 Task-17:English lexical sample SRL and all words.In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations(SemEval-2007) in conjunction with ACL,pages 87?92, Prague.Sinha, R. and R. Mihalcea.
2007.Unsupervised graph-based word sensedisambiguation using measures of wordsemantic similarity.
In Proceedings of theIEEE International Conference on SemanticComputing (ICSC 2007), pages 363?369,Irvine, CA.Snyder, B. and M. Palmer.
2004.
TheEnglish all-words task.
In Proceedings ofSENSEVAL-3 Third International Workshopon Evaluation of Systems for the SemanticAnalysis of Text, pages 41?43, Barcelona.Strapparava, C., A. Gliozzo, and C. Giuliano.2004.
Pattern abstraction and termsimilarity for word sense disambiguation:IRST at SENSEVAL-3.
In Proceedings ofSENSEVAL-3 Third International Workshopon Evaluation of Systems for the SemanticAnalysis of Text, pages 229?234, Barcelona.Surdeanu, M., M. Ciaramita, andH.
Zaragoza.
2008.
Learning to rankanswers on large online QA collections.In Proceedings of the 46th Annual Meetingof the Association for ComputationalLinguistics (ACL/HLT?08), pages 719?727,Columbus, OH.Sussna, M. 1993.
Word sense disambiguationfor free-text indexing using a massivesemantic network.
In Proceedings of theSecond International Conference onInformation and Knowledge Management(CIKM?93), pages 67?74, New York, NY.Tratz, S., A. Sanfilippo, M. Gregory,A.
Chappell, C. Posse, and P. Whitney.2007.
PNNL: A supervised maximumentropy approach to word sensedisambiguation.
In Proceedings of the4th International Workshop on SemanticEvaluations (SemEval-2007) in conjunctionwith ACL, pages 264?267, Prague.83Computational Linguistics Volume 40, Number 1Tsatsaronis, G., I. Varlamis, and K. N?rva?g.2010.
An experimental study onunsupervised graph-based wordsense disambiguation.
In Proceedingsof the 11th International Conference onComputational Linguistics and IntelligentText Processing (CICLing?10),pages 184?198, Iasi.Tsatsaronis, G., M. Vazirgiannis, andI.
Androutsopoulos.
2007.
Word sensedisambiguation with spreading activationnetworks generated from thesauri.In Proceedings of the 17th InternationalJoint Conference on Artificial Intelligence(IJCAI?07), pages 1,725?1,730, Hyderabad.Zhong, Z. and H. T. Ng.
2010.
It makessense: A wide-coverage word sensedisambiguation system for free text.In Proceedings of the ACL 2010 SystemDemonstrations, pages 78?83, Uppsala.Zhong, Z. and H. T. Ng.
2012.
Word sensedisambiguation improves informationretrieval.
In Proceedings of the 50th AnnualMeeting of the Association for ComputationalLinguistics (Volume 1: Long Papers),pages 273?282, Jeju Island.84
