Proceedings of the Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 12?19,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatic Identification of Non-Compositional Multi-Word Expressionsusing Latent Semantic AnalysisGraham KatzInstitute of Cognitive ScienceUniversity of Osnabru?ckgkatz@uos.deEugenie GiesbrechtInstitute of Cognitive ScienceUniversity of Osnabru?ckegiesbre@uos.deAbstractMaking use of latent semantic analy-sis, we explore the hypothesis that lo-cal linguistic context can serve to iden-tify multi-word expressions that have non-compositional meanings.
We propose thatvector-similarity between distribution vec-tors associated with an MWE as a wholeand those associated with its constitutentparts can serve as a good measure of thedegree to which the MWE is composi-tional.
We present experiments that showthat low (cosine) similarity does, in fact,correlate with non-compositionality.1 IntroductionIdentifying non-compositional (or idiomatic)multi-word expressions (MWEs) is an importantsubtask for any computational system (Sag et al,2002), and significant attention has been paidto practical methods for solving this problem inrecent years (Lin, 1999; Baldwin et al, 2003;Villada Moiro?n and Tiedemann, 2006).
Whilecorpus-based techniques for identifying collo-cational multi-word expressions by exploitingstatistical properties of the co-occurrence of thecomponent words have become increasinglysophisticated (Evert and Krenn, 2001; Evert,2004), it is well known that mere co-occurrencedoes not well distinguish compositional fromnon-compositional expressions (Manning andSchu?tze, 1999, Ch.
5).While expressions which may potentially haveidiomatic meanings can be identified using variouslexical association measures (Evert and Krenn,2001; Evert and Kermes, 2003), other techniquesmust be used to determining whether or not a par-ticular MWE does, in fact, have an idiomatic use.In this paper we explore the hypothesis that thelocal linguistic context can provide adequate cuesfor making this determination and propose onemethod for doing this.We characterize our task on analogy with word-sense disambiguation (Schu?tze, 1998; Ide andVe?ronis, 1998).
As noted by Schu?tze, WSDinvolves two related tasks: the general task ofsense discrimination?determining what sensesa given word has?and the more specific taskof sense selection?determining for a particularuse of the word in context which sense was in-tended.
For us the discrimination task involvesdetermining for a given expression whether it hasa non-compositional interpretation in addition toits compositional interpretation, and the selec-tion task involves determining in a given context,whether a given expression is being used compo-sitionally or non-compostionally.
The German ex-pression ins Wasser fallen, for example, has a non-compositional interpretation on which it means ?tofail to happen?
(as in (1)) and a compositional in-terpretation on which it means ?to fall into water(as in (2)).1(1) Das Kind war beim Baden von einer Luftma-tratze ins Wasser gefallen.
?The child had fallen into the water from an aair matress while swimming?
(2) Die Ero?fnung des Skateparks ist ins Wassergefallen.
?The opening of the skatepark was cancelled?The discrimination task, then, is to identify insWasser fallen as an MWE that has an idiomaticmeaning and the selection task is to determine that1Examples taken from a newspaper corpus of the GermanSu?ddeutsche Zeitung (1994-2000)12in (1) it is the compositional meaning that is in-tended, while in (2) it is the non-compositionalmeaning.Following Schu?tze (1998) and Landauer & Du-mais (1997) our general assumption is that themeaning of an expression can be modelled interms of the words that it co-occurs with: itsco-occurrence signature.
To determine whethera phrase has a non-compositional meaning wecompute whether the co-occurrence signature ofthe phrase is systematically related to the co-occurrence signatures of its parts.
Our hypoth-esis is that a systematic relationship is indica-tive of compositional interpretation and lack ofa systematic relationship is symptomatic of non-compositionality.
In other words, we expect com-positional MWEs to appear in contexts more sim-ilar to those in which their component words ap-pear than do non-compositional MWEs.In this paper we describe two experiments thattest this hypothesis.
In the first experiment weseek to confirm that the local context of a knownidiom can reliably distinguish idiomatic uses fromnon-idiomatic uses.
In the second experiment weattempt to determine whether the difference be-tween the contexts in which an MWE appears andthe contexts in which its component words appearcan indeed serve to tell us whether the MWE hasan idiomatic use.In our experiments we make use of lexical se-mantic analysis (LSA) as a model of context-similarity (Deerwester et al, 1990).
Since thistechnique is often used to model meaning, we willspeak in terms of ?meaning?
similiarity.
It shouldbe clear, however, that we are only using the LSAvectors?derived from context of occurrence in acorpus?to model meaning and meaning composi-tion in a very rough way.
Our hope is simply thatthis rough model is sufficient to the task of identi-fying non-compositional MWEs.2 Previous workRecent work which attempts to discriminatebetween compositional and non-compositionalMWEs include Lin (1999), who used mutual-information measures identify such phrases, Bald-win et al (2003), who compare the distributionof the head of the MWE with the distribution ofthe entire MWE, and Vallada Moiro?n & Tiede-mann (2006), who use a word-alignment strat-egy to identify non-compositional MWEs makinguse of parallel texts.
Schone & Jurafsky (2001)applied LSA to MWE identification, althoughtthey did not focus on distinguishing compositionalfrom non-compositional MWEs.Lin?s goal, like ours, was to discriminate non-compositional MWEs from compositional MWEs.His method was to compare the mutual informa-tion measure of the constituents parts of an MWEwith the mutual information of similar expressionsobtained by substituting one of the constituentswith a related word obtained by thesaurus lookup.The hope was that a significant difference betweenthese measures, as in the case of red tape (mutualinformation: 5.87) compared to yellow tape (3.75)or orange tape (2.64), would be characteristic ofnon-compositional MWEs.
Although intuitivelyappealing, Lin?s algorithm only achieves precisionand recall of 15.7% and 13.7%, respectively (ascompared to a gold standard generate from an id-iom dictionary?but see below for discussion).Schone & Jurafsky (2001) evaluated a num-ber of co-occurrence-based metrics for identify-ing MWEs, showing that, as suggested by Lin?sresults, there was need for improvement in thisarea.
Since LSA has been used in a numberof meaning-related language tasks to good ef-fect (Landauer and Dumais, 1997; Landauer andPsotka, 2000; Cederberg and Widdows, 2003),they had hoped to improve their results by identifynon-compositional expressions using a methodsimilar to that which we are exploring here.
Al-though they do not demonstrate that this methodactually identifies non-compositional expressions,they do show that the LSA similarity techniqueonly improves MWE identification minimally.Baldwin et al, (2003) focus more narrowlyon distinguishing English noun-noun compoundsand verb-particle constructions which are com-positional from those which are not composi-tional.
Their approach is methodologically similarto ours, in that they compute similarity on the ba-sis of contexts of occurrance, making use of LSA.Their hypothesis is that high LSA-based similar-ity between the MWE and each of its constituentparts is indicative of compositionality.
They evalu-ate their technique by assessing the correlation be-tween high semantic similarity of the constituentsof an MWE to the MWE as a whole with the like-lihood that the MWE appears in WordNet as a hy-ponym of one of the constituents.
While the ex-pected correlation was not attested, we suspect this13to be more an indication of the inappropriatenessof the evaluation used than of the faultiness of thegeneral approach.Lin, Baldwin et al, and Schone & Jurafsky, alluse as their gold standard either idiom dictionariesor WordNet (Fellbaum, 1998).
While Schone &Jurafsky show that WordNet is as good a standardas any of a number of machine readable dictionar-ies, none of these authors shows that the MWEsthat appear in WordNet (or in the MRDs) are gen-erally non-compositional, in the relevant sense.
Asnoted by Sag et al (2002) many MWEs are sim-ply ?institutionalized phrases?
whose meaningsare perfectly compositional, but whose frequencyof use (or other non-linguistic factors) make themhighly salient.
It is certainly clear that manyMWEs that appear in WordNet?examples beinglaw student, medical student, college man?areperfectly compositional semantically.Zhai (1997), in an early attempt to applystatistical methods to the extraction of non-compositional MWEs, made use of what we taketo be a more appropriate evaluation metric.
In hiscomparison among a number of different heuris-tics for identifying non-compositional noun-nouncompounds, Zhai did his evaluation by applyingeach heuristic to a corpus of items hand-classifiedas to their compositionality.
Although Zhai?s clas-sification appears to be problematic, we take thisto be the appropirate paradigm for evaluation inthis domain, and we adopt it here.3 ProceedureIn our work we made use of the Word Spacemodel of (semantic) similiarty (Schu?tze, 1998)and extended it slightly to MWEs.
In this frame-work, ?meaning?
is modeled as an n-dimensionalvector, derived via singular value decomposition(Deerwester et al, 1990) from word co-occurrencecounts for the expression in question, a techniquefrequently referred to as Latent Semantic Analysis(LSA).
This kind of dimensionality reduction hasbeen shown to improve performance in a numberof text-based domains (Berry et al, 1999).For our experiments we used a local Germannewspaper corpus.2 We built our LSA modelwith the Infomap Software package.3, using the1000 most frequent words not on the 102-word2Su?ddeutsche Zeitung (SZ) corpus for 2003 with about 42million words.3Available from infomap.stanford.edu.Figure 1: Two dimensional Word Spacehand-generated stop list as the content-bearing di-mension words (the columns of the matrix).
The20,000 most frequent content words were assignedrow values by counting occurrences within a 30-word window.
SVD was used to reduce the di-mensionality from 1000 to 100, resulting in 100dimensional ?meaning?-vectors for each word.
Inour experiments, MWEs were assigned meaning-vectors as a whole, using the same proceedure.For meaning similarity we adopt the standard mea-sure of cosine of the angle between two vectors(the normalized correlation coefficient) as a met-ric (Schu?tze, 1998; Baeza-Yates and Ribeiro-Neto,1999).
On this metric, two expressions are takento be unrelated if their meaning vectors are orthog-onal (the cosine is 0) and synonymous if their vec-tors are parallel (the cosine is 1).Figure 1 illustrates such a vector space in twodimensions.
Note that the meaning vector forLo?ffel ?spoon?
is quite similar to that for es-sen ?to eat?
but distant from sterben ?to die?,while the meaning vector for the MWE den Lo?ffelabgeben is close to that for sterben.
Indeed denLo?ffel abgeben, like to kick the bucket, is a non-compositional idiom meaning ?to die?.While den Lo?ffel abgeben is used almost ex-clusively in its idiomatic sense (all four occur-rences in our corpus), many MWEs are used reg-ularly in both their idiomatic and in their literalsenses.
About two thirds of the uses of the MWEins Wasser fallen in our corpus are idiomatic uses,and the remaing one third are literal uses.
Inour first experiment we tested the hypothesis thatthese uses could reliably be distinguished usingdistribution-based models of their meaning.143.1 Experiment IFor this experiment we manually annotated the67 occurrences of ins Wasser fallen in our cor-pus as to whether the expression was used com-positionally (literally) or non-compositionally (id-iomatically).4 Marking this distinction we gen-erate an LSA meaning vectors for the composi-tional uses and an LSA meaning vector for thenon-compositional uses of ins Wasser fallen.
Thevectors turned out, as expected, to be almost or-thogonal, with a cosine of the angle between themof 0.02.
This result confirms that the linguis-tic contexts in which the literal and the idiomaticuse of ins Wasser fallen appear are very differ-ent, indicating?not surprisingly?that the seman-tic difference between the literal meaning and theidiomatic meaning is reflected in the way thesethese phrases are used.Our next task was to investigate whether thisdifference could be used in particular cases to de-termine what the intended use of an MWE in aparticular context was.
To evaluate this, we did a10-fold cross-validation study, calculating the lit-eral and idiomatic vectors for ins Wasser fallen onthe basis of the training data and doing a simplenearest neighbor classification of each mememberof the test set on the basis of the meaning vectorscomputed from its local context (the 30 word win-dow).
Our result of an average accurace of 72%for our LSA-based classifier far exceeds the sim-ple maximum-likelihood baseline of 58%.In the final part of this experiment we comparedthe meaning vector that was computed by sum-ming over all uses of ins Wasser fallen with theliteral and idiomatic vectors from above.
Since id-iomatic uses of ins Wasser fallen prevail in the cor-pus (2/3 vs. 1/3), it is not surprisingly that the sim-ilarity to the literal vector (0.0946) is much thansimilarity to the idiomatic vector (0.3712).To summarize Experiment I, which is a vari-ant of a supervised phrase sense disambiguationtask, demonstrates that we can use LSA to distin-guish between literal and the idiomatic usage of anMWE by using local linguistic context.4This was a straightforward task; two annotators anno-tated independently, with very high agreement?kappa scoreof over 0.95 (Carletta, 1996).
Occurrences on which the an-notators disagreed were thrown out.
Of the 64 occurrenceswe used, 37 were idiomatic and 27 were literal.3.2 Experiment IIIn our second experiment we sought to makeuse of the fact that there are typically cleardistributional difference between compositionaland non-compositional uses of MWEs to deter-mine whether a given MWE indeed has non-compositional uses at all.
In this experi-ment we made use of a test set of GermanPreposition-Noun-Verb ?collocation candidate?database whose extraction is described by Krenn(2000) and which has been made available elec-tronically.5 From this database only word com-binations with frequency of occurrence more than30 in our test corpus were considered.
Our taskwas to classify these 81 potential MWEs accord-ing whether or not thay have an idiomatic mean-ing.To accomplish this task we took the followingapproach.
We computed on the basis of the dis-tribution of the components of the MWE an esti-mate for the compositional meaning vector for theMWE.
We then compared this to the actual vec-tor for the MWE as a whole, with the expecta-tion MWEs which indeed have non-compositinoaluses will be distinguished by a relatively low vec-tor similarity between the estimated compositionalmeaning vector and the actual meaning vector.In other words small similarity values should bediagnostic for the presense of non-compositinoaluses of the MWE.We calculated the estimated compositionalmeaning vector by taking it to be the sum of themeaning vector of the parts, i.e., the compositionalmeaning of an expression w1w2 consisting of twowords is taken to be sum of the meaning vectorsfor the constituent words.6 In order to maximizethe independent contribution of the constituentwords, the meaning vectors for these words werealways computed from contexts in which they ap-pear alone (that is, not in the local context of theother constituent).
We call the estimated composi-tional meaning vector the ?composed?
vector.7The comparisons we made are illustrated in Fig-ure 2, where vectors for the MWE auf die Streckebleiben ?to fall by the wayside?
and the wordsStrecke ?route?
and bleiben ?to stay?
are mapped5Available as an example data collection in UCS-Toolkit5 from www.collocations.de.6For all our experiments we consider only two-word com-binations.7Schone & Jurafsky (2001) explore a few modest varia-tions of this estimate.15Figure 2: Composed versus Multi-Wordinto two dimensions8.
(the words Autobahn ?high-way?
and eigensta?ndig ?independent?
are given forcomparison).
Here we see that the linear com-bination of the component words of the MWE isclearly distinct from that of the MWE as a whole.As a further illustration of the difference be-tween the composed vector and the MWE vector,in Table 2 we list the words whose meaning vectoris most similar to that of the MWE auf dis Streckebleiben along with their similarity values, and inTable 3 we list those words whose meaning vec-tor is most similar to the composed vector.
Thesemantic differences among these two classes arereadily apparent.folgerung ?consequence?
0.769663eigensta?ndig ?independent?
0.732372langfristiger ?long-term?
0.731411herbeifu?hren ?to effect?
0.717294ausnahmefa?lle ?exceptions?
0.704939Table 1: auf die Strecke bleibenstrecken ?to lengthen?
0.743309fahren ?to drive?
0.741059laufen ?to run?
0.726631fahrt ?drives?
0.712352schlie?en ?to close?
0.704364Table 2: Strecke+bleibenWe recognize that the composed vector isclearly nowhere near a perfect model of compo-sitional meaning in the general case.
This can beillustrated by considering, for example, the MWEfire breathing.
This expression is clearly com-positional, as it denotes the process of producing8The preposition auf and the article die are on the stop listcombusting exhalation, exactly what the seman-tic combination rules of the English would pre-dict.
Nevertheless the distribution of fire breath-ing is quite unrelated to that of its constituentsfire and breathing ( the former appears frequentlywith dragon and circus while the later appear fre-quently with blaze and lungs, respectively).
De-spite these principled objections, the composedvector provides a useful baseline for our investiga-tion.
We should note that a number of researchersin the LSA tradition have attempted to providemore compelling combinatory functions to cap-ture the non-linearity of linguistic compositionalinterpretation (Kintsch, 2001; Widdows and Pe-ters, 2003).As a check we chose, at random, a number ofsimple clearly-compositional word combinations(not from the candidate MWE list).
We expectedthat on the whole these would evidence a very highsimilarity measure when compared with their as-sociated composed vector, and this is indeed thecase, as shown in Table 1.
We also comparedvor Gericht verantworten 0.80735103?to appear in court?im Bett liegen 0.76056000?to lie in bed?aus Gefa?ngnis entlassen 0.66532673?dismiss from prison?Table 3: Non-idiomatic phrasesthe literal and non-literal vectors for ins Wasserfallen from the first experiment with the composedvector, computed out of the meaning vectors forWasser and for fallen.9 The difference isn?t large,but nevertheless the composed vector is more sim-ilar to the literal vector (cosine of 0.2937) than tothe non-literal vector (cosine of 0.1733).Extending to the general case, our task was tocompare the composed vector to the actual vec-tor for all the MWEs in our test set.
The result-ing cosine similarity values range from 0.01 to0.80.
Our hope was that there would be a similar-ity threshold for distinguishing MWEs that havenon-compositional interpretations from those thatdo not.
Indeed of the MWEs with a similarity val-ues of under 0.1, just over half are MWEs whichwere hand-annotated to have non-literal uses.10 It9The preposition ins is on the stop list and plays no rolein the computation.10The similarity scores for the entire test set are given in16is clear then that the technique described is, primafacie, capable of detecting idiomatic MWEs.3.3 Evaluation and DiscussionTo evaluate the method, we used the careful man-ual annotation of the PNV database described byKrenn (2000) as our gold standard.
By adopt-ing different threshholds for the classification de-cision, we obtained a range of results (trading offprecision and recall).
Table 4 illustrates this range.The F-score measure is maximized in our ex-periments by adopting a similarity threshold of0.2.
This means that MWEs which have a mean-ing vector whose cosine is under this value whencompared with with the combined vector shouldbe classified as having a non-literal meaning.To compare our method with that proposed byBaldwin et al (2003), we applied their methodto our materials, generating LSA vectors for thecomponent content words in our candidate MWEsand comparing their semantic similarity to theMWEs LSA vector as a whole, with the expecta-tion being that low similarity between the MWE asa whole and its component words is indication ofthe non-compositionality of the MWE.
The resultsare given in Table 5.It is clear that while Baldwin et al?s expectationis borne out in the case of the constituent noun(the non-head), it is not in the case of the con-stituent verb (the head).
Even in the case of thenouns, however, the results are, for the most part,markedly inferior to the results we achieved usingthe composed vectors.There are a number of issues that complicatethe workability of the unsupervised technique de-scribed here.
We rely on there being enoughnon-compositional uses of an idiomatic MWE inthe corpus that the overall meaning vector for theMWE reflects this usage.
If the literal meaningis overwhelmingly frequent, this will reduce theeffectivity of the method significantly.
A secondproblem concerns the relationship between the lit-eral and the non-literal meaning.
Our techniquerelies on these meaning being highly distinct.
Ifthe meanings are similar, it is likely that local con-text will be inadequate to distinguish a composi-tional from a non-compositional use of the expres-sion.
In our investigation it became apparent, infact, that in the newspaper genre, highly idiomaticexpressions such as ins Wasser fallen were oftenAppendix I.used in their idiomatic sense (apparently for hu-morous effect) particularly frequently in contextsin which elements of the literal meaning were alsopresent.114 ConclusionTo summarize, in order to classify an MWE asnon-compositional, we compute an approximationof its compositional meaning and compare thiswith the meaning of the expression as it is usedon the whole.
One of the obvious improvementsto the algorithm could come from better mod-els for simulating compositional meaning.
A fur-ther issue that can be explored is whether linguis-tic preprocessing would influence the results.
Weworked only on raw text data.
There is some ev-idence (Baldwin et al, 2003) that part of speechtagging might improve results in this kind of task.We also only considered local word sequences.Certainly some recognition of the syntactic struc-ture would improve results.
These are, however,more general issues associated with MWE pro-cessing.Rather promising results were attained usingonly local context, however.
Our study showsthat the F-score measure is maximized by takingas threshold for distinguishing non-compositionalphrases from compositional ones a cosine simi-larity value somewhere between 0.1-0.2.
An im-portant point to be explored is that compositional-ity appears to come in degrees.
As Bannard andLascarides (2003) have noted, MWEs ?do not fallcleanly into the binary classes of compositionaland non-compositional expressions, but populatea continuum between the two extremes.?
Whileour experiment was designed to classify MWEs,the technique described here, of course, providesa means, if rather a blunt one, for quantifying thedegreee of compositonality of an expression.ReferencesRicardo A. Baeza-Yates and Berthier A. Ribeiro-Neto.1999.
Modern Information Retrieval.
ACM Press /Addison-Wesley.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model11One such example from the SZ corpus:Der Auftakt wa?re allerdings fast ins Wasser gefallen, weil eingeplatzter Hydrant eine fu?nfzehn Meter hohe Wasserfonta?nein die Luft schleuderte.
?The prelude almost didn?t occur, because a burst hydrantshot a fifteen-meter high fountain into the sky.
?17cos < 0.1 cos < 0.2 cos < 0.3 cos < 0.4 cos < 0.5Precision 0.53 0.39 0.29 0.22 0.21Recall 0.42 0.63 0.84 0.89 0.95F-measure 0.47 0.48 0.43 0.35 0.34Table 4: Evaluation of Various Similarity Thresholdscos < 0.1 cos < 0.2 cos < 0.3 cos < 0.4 cos < 0.5Verb F-measure 0.21 0.16 0.29 0.26 0.27Noun F-measure 0.28 0.51 0.43 0.39 0.33Table 5: Evaluation of Method of Baldwin et al (2003)of multiword expression decomposability.
In Pro-ceedings of the ACL-2003 Workshop on MultiwordExpressions: Analysis, Acquisition and Treatment,pages 89?96, Sapporo, Japan.Colin Bannard, Timothy Baldwin, and Alex Las-carides.
2003.
A statistical approach to the seman-tics of verb-particles.
In Proceedings of the ACL-2003 Workshop on Multiword Expressions: Analy-sis, Acquisition and Treatment, pages 65?72, Sap-poro, Japan.Michael W. Berry, Zlatko Drmavc, and Elisabeth R.Jessup.
1999.
Matrices, vector spaces, and infor-mation retrieval.
SIAM Review, 41(2):335?362.Jean Carletta.
1996.
Assessing agreement on classi-fication tasks: The kappa statistic.
ComputationalLinguistics, 22(2):249?254.Scott Cederberg and Dominic Widdows.
2003.
UsingLSA and noun coordination information to improvethe precision and recall of automatic hyponymy ex-traction.
In In Seventh Conference on Computa-tional Natural Language Learning, pages 111?118,Edmonton, Canada, June.Scott C. Deerwester, Susan T. Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society of Information Science,41(6):391?407.Stefan Evert and Hannah Kermes.
2003.
Experi-ments on candidate data for collocation extraction.In Companion Volume to the Proceedings of the 10thConference of The European Chapter of the Associ-ation for Computational Linguistics, pages 83?86,Budapest, Hungary.Stefan Evert and Brigitte Krenn.
2001.
Methods forthe qualitative evaluation of lexical association mea-sures.
In Proceedings of the 39th Annual Meetingof the Association for Computational Linguistics,pages 188?195, Toulouse, France.Stefan Evert.
2004.
The Statistics of Word Cooccur-rences: Word Pairs and Collocations.
Ph.D. thesis,University of Stuttgart.Christiane Fellbaum.
1998.
WordNet, an electroniclexical database.
MIT Press, Cambridge, MA.Nancy Ide and Jean Ve?ronis.
1998.
Word sense dis-ambiguation: The state of the art.
ComputationalLinguistics, 14(1).Walter Kintsch.
2001.
Predication.
Cognitive Science,25(2):173?202.Brigitte Krenn.
2000.
The Usual Suspects: Data-Oriented Models for Identification and Representa-tion of Lexical Collocations.
Dissertations in Com-putational Linguistics and Language Technology.German Research Center for Artificial Intelligenceand Saarland University, Saarbru?cken, Germany.Thomas K. Landauer and Susan T. Dumais.
1997.A solution to plato?s problem: The latent seman-tic analysis theory of the acquisition, induction, andrepresentation of knowledge.
Psychological Review,104:211?240.Thomas K. Landauer and Joseph Psotka.
2000.
Sim-ulating text understanding for educational applica-tions with latent semantic analysis: Introduction toLSA.
Interactive Learning Environments, 8(2):73?86.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of the 37thAnnual Meeting of the Association for Computa-tional Linguistics, pages 317?324, College Park,MD.Christopher D. Manning and Hinrich Schu?tze.
1999.Foundations of Statistical NaturalLanguage Pro-cessing.
The MIT Press, Cambridge, MA.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann A.Copestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for NLP.
In Pro-ceedings of the 3rd International Conferences onIntelligent Text Processing and Computational Lin-guistics, pages 1?15.Patrick Schone and Daniel Jurafsky.
2001.
Isknowledge-free induction of multiword unit dictio-nary headwords a solved problem?
In Proceedings18of Empirical Methods in Natural Language Process-ing, Pittsburgh, PA.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?124.Begon?a Villada Moiro?n and Jo?rg Tiedemann.
2006.Identifying idiomatic expressions using automaticword-alignment.
In Proceedings of the EACL 2006Workshop on Multiword Expressions in a Multilin-gual Context, Trento, Italy.Dominic Widdows and Stanley Peters.
2003.
Wordvectors and quantum logic: Experiments with nega-tion and disjunction.
In Eighth Mathematics of Lan-guage Conference, pages 141?150, Bloomington,Indiana.Chengxiang Zhai.
1997.
Exploiting context to iden-tify lexical atoms ?
a statistical view of linguisticcontext.
In Proceedings of the International and In-terdisciplinary Conference on Modelling and UsingContext (CONTEXT-97), pages 119?129.APPENDIXSimilarity (cosine) values for the combined andthe MWE vector.
Uppercase entries are thosehand-annotated as being MWEs which have an id-iomatic interpretation.Word Combinations Cosines(vor) gericht verantworten 0.80735103(in) bett liegen 0.76056000(aus) gefa?ngnis entlassen 0.66532673(zu) verfu?ung stellen 0.60310321(aus) haft entlassen 0.59105617(um) prozent steigern 0.55889772(ZU) KASSE BITTEN 0.526331(auf) prozent sinken 0.51281725(IN) TASCHE GREIFEN 0.49350031(zu) verfu?gung stehen 0.49236563(auf) prozent steigen 0.47422122(um) prozent zulegen 0.47329672(in) betrieb gehen 0.47262171(unter) druck geraten 0.44377297(in) deutschland leben 0.44226071(um) prozent steigen 0.41498688(in) rechnung stellen 0.40985534(von) prozent erreichen 0.39407666(auf) markt kommen 0.38740534(unter) druck setzen 0.37822936(in) vergessenheit geraten 0.36654168(um) prozent sinken 0.36600216(in) rente gehen 0.36272313(zu) einsatz kommen 0.3562527(zu) schule gehen 0.35595884(in) frage stellen 0.35406327(in) frage kommen 0.34714701(in) luft sprengen 0.34241143(ZU) GESICHT BEKOMMEN 0.34160325(vor) gericht ziehen 0.33405685(in) gang setzen 0.33231573(in) anspruch nehmen 0.32217044(auf) prozent erho?hen 0.31574088(um) prozent wachsen 0.3151615(in) empfang nehmen 0.31420746(fu?r) sicherheit sorgen 0.30230156(zu) ausdruck bringen 0.30001438(IM) MITTELPUNKT STEHEN 0.29770654(zu) ruhe kommen 0.29753093(IM) AUGE BEHALTEN 0.2969367(in) urlaub fahren 0.29627064(in) kauf nehmen 0.2947628(in) pflicht nehmen 0.29470704(in) ho?he treiben 0.29450525(in) kraft treten 0.29311349(zu) kenntnis nehmen 0.28969961(an) start gehen 0.28315812(auf) markt bringen 0.2800427(in) ruhe standgehen 0.27575604(bei) prozent liegen 0.27287073(um) prozent senken 0.26506203(UNTER) LUPE NEHMEN 0.2607078(zu) zug kommen 0.25663165(zu) ende bringen 0.25210009(in) brand geraten 0.24819525( ?UBER) B ?UHNE GEHEN 0.24644366(um) prozent erho?hen 0.24058016(auf) tisch legen 0.23264335(auf) bu?hne stehen 0.23136641(auf) idee kommen 0.23097735(zu) ende gehen 0.20237252(auf) spiel setzen 0.20112171(IM) VORDERGRUND STEHEN 0.18957473(IN) LEERE LAUFEN 0.18390151(zu) opfer fallen 0.17724105(in) gefahr geraten 0.17454816(in) angriff nehmen 0.1643926(auer) kontrolle geraten 0.16212899(IN) HAND NEHMEN 0.15916243(in) szene setzen 0.15766861(ZU) SEITE STEHEN 0.14135151(zu) geltung kommen 0.13119923(in) geschichte eingehen 0.12458956(aus) ruhe bringen 0.10973377(zu) fall bringen 0.10900036(zu) wehr setzen 0.10652383(in) griff bekommen 0.10359659(auf) tisch liegen 0.10011075(IN) LICHTER SCHEINEN 0.08507655(zu) sprache kommen 0.08503791(IM) STICH LASSEN 0.0735844(unter) beweis stellen 0.06064519(IM) WEG STEHEN 0.05174435(AUS) FUGEN GERATEN 0.05103952(in) erinnerung bleiben 0.04339438(ZU) WORT KOMMEN 0.03808749(AUF) STRA?E GEHEN 0.03492515(AUF) STRECKE BLEIBEN 0.03463844(auer) kraft setzen 0.0338813(AUF) WEG BRINGEN 0.03122951(zu) erfolg fu?hren 0.02882997(in) sicherheit bringen 0.02862914(in) erfu?hlung gehen 0.01515792(in) zeitung lesen 0.0035459819
