Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1702?1712,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsText Categorization as a Graph Classification ProblemFran?ois Rousseau Emmanouil KiagiasLIX, ?cole Polytechnique, FranceMichalis VazirgiannisAbstractIn this paper, we consider the task oftext categorization as a graph classifica-tion problem.
By representing textual doc-uments as graph-of-words instead of his-torical n-gram bag-of-words, we extractmore discriminative features that corre-spond to long-distance n-grams throughfrequent subgraph mining.
Moreover, bycapitalizing on the concept of k-core, wereduce the graph representation to its dens-est part ?
its main core ?
speeding up thefeature extraction step for little to no costin prediction performances.
Experimentson four standard text classification datasetsshow statistically significant higher accu-racy and macro-averaged F1-score com-pared to baseline approaches.1 IntroductionThe task of text categorization finds applicationsin a wide variety of domains, from news filter-ing and document organization to opinion miningand spam detection.
With the ever-growing quan-tity of information available online nowadays, itis crucial to provide effective systems capable ofclassifying text in a timely fashion.
Comparedto other application domains of classification, itsspecificity lies in its high number of features, itssparse feature vectors and its skewed multiclassscenario.
For instance, when dealing with thou-sands of news articles, it is not uncommon to havemillions of n-gram features, only a few hundredsactually present in each document and tens of classlabels ?
some of them with thousands of articlesand some others will only a few hundreds.
Theseparticularities have to be taken into account whenenvisaging a different representation for a docu-ment and in our case when considering the task asa graph classification problem.Graphs are powerful data structures that areused to represent complex information about en-tities and interaction between them and we thinktext makes no exception.
Historically, followingthe traditional bag-of-words representation, uni-grams have been considered as the natural featuresand later extended to n-grams to capture someword dependency and word order.
However, n-grams correspond to sequences of words and thusfail to capture word inversion and subset match-ing (e. g., ?article about news?
vs. ?news article?
).We believe graphs can help solve these issues likethey did for instance with chemical compoundswhere repeating substructure patterns are good in-dicators of belonging to one particular class, e. g.,predicting carcinogenicity in molecules (Helma etal., 2001).
Graph classification has received alot of attention this past decade and various tech-niques have been developed to deal with the taskbut rarely applied on textual data and at its scale.In our work, we explored a graph representationof text, namely graph-of-words, to challenge thetraditional bag-of-words representation and helpbetter classify textual documents into categories.We first trained a classifier using frequent sub-graphs as features for increased effectiveness.
Wethen reduced each graph-of-words to its main corebefore mining the features for increased efficiency.Finally, we also used this technique to reduce thetotal number of n-gram features considered in thebaselines for little to no loss in prediction perfor-mances.The rest of the paper is organized as follows.Section 2 provides a review of the related work.Section 3 defines the preliminary concepts uponwhich our work is built.
Section 4 introduces theproposed approaches.
Section 5 describes the ex-perimental settings and presents the results we ob-tained on four standard datasets.
Finally, Section6 concludes our paper and mentions future workdirections.17022 Related workIn this section, we present the related work in textcategorization, graph classification and the com-bination of the two fields like in our case.2.1 Text categorizationText categorization, a.k.a.
text classification, cor-responds to the task of automatically predictingthe class label of a given textual document.
Werefer to (Sebastiani, 2002) for an in-depth re-view of the earliest works in the field and (Ag-garwal and Zhai, 2012) for a survey of the morerecent works that capitalize on additional meta-information.
We note in particular the seminalwork of Joachims (1998) who was the first to pro-pose the use of a linear SVM with TF?IDF termfeatures for the task.
This approach is one of thestandard baselines because of its simplicity yet ef-fectiveness (unsupervised n-gram feature miningfollowed by standard supervised learning).
An-other popular approach is the use of Naive Bayesand its multiple variants (McCallum and Nigam,1998), in particular for the subtask of spam de-tection (Androutsopoulos et al, 2000).
Finally,there are a couple of works such as (Hassan etal., 2007) that used the graph-of-words representa-tion to propose alternative weights for the n-gramfeatures but still without considering the task as agraph classification problem.2.2 Graph classificationGraph classification corresponds to the task of au-tomatically predicting the class label of a givengraph.
The learning part in itself does not differfrom other supervised learning problems and mostproposed methods deal with the feature extrac-tion part.
They fall into two main categories: ap-proaches that consider subgraphs as features andgraph kernels.2.2.1 Subgraphs as featuresThe main idea is to mine frequent subgraphs anduse them as features for classification, be it withAdaboost (Kudo et al, 2004) or a linear SVM(Deshpande et al, 2005).
Indeed, most datasetsthat were used in the associated experiments cor-respond to chemical compounds where repeatingsubstructure patterns are good indicators of be-longing to one particular class.
Some populargraph pattern mining algorithms are gSpan (Yanand Han, 2002), FFSM (Huan et al, 2003) andGaston (Nijssen and Kok, 2004).
The number offrequent subgraphs can be enormous, especiallyfor large graph collections, and handling such afeature set can be very expensive.
To overcomethis issue, recent works have proposed to retainor even only mine the discriminative subgraphs,i.
e. features that contribute to the classificationdecision, in particular gBoost (Saigo et al, 2009),CORK (Thoma et al, 2009) and GAIA (Jin etal., 2010).
However, when experimenting, gBoostdid not converge on our larger datasets whileGAIA and CORK consider subgraphs of nodesize at least 2, which exclude unigrams, result-ing in poorer performances.
Moreover, all theseapproaches have been developed for binary clas-sification, which meant mining features as manytimes as the number of classes instead of just once(one-vs-all learning strategy).
In this paper, wetackle the scalability issue differently through anunsupervised feature selection approach to reducethe size of the graphs and a fortiori the number offrequent subgraphs.2.2.2 Graph kernelsG?rtner et al (2003) proposed the first kernels be-tween graphs (as opposed to previous kernels ongraphs, i. e. between nodes) based on either ran-dom walks or cycles to tackle the problem of clas-sification between graphs.
In parallel, the idea ofmarginalized kernels was extended to graphs byKashima et al (2003) and by Mah?
et al (2004).We refer to (Vishwanathan et al, 2010) for an in-depth review of the topic and in particular its lim-itations in terms of number of unique node labels,which make them unsuitable for our problem astested in practice (limited to a few tens of uniquelabels compared to hundreds of thousands for us).2.3 Similar worksThe work of Markov et al (2007) is perhaps theclosest to ours since they also perform subgraphfeature mining on graph-of-words representationsbut with non-standard datasets and baselines.
Theworks of Jiang et al (2010) and Arora et al (2010)are also related but their representations are dif-ferent and closer to parse and dependency treesused as base features for text categorization byKudo and Matsumoto (2004) and Matsumoto et al(2005).
Moreover, they do not discuss the choiceof the support value, which controls the total num-ber of features and can potentially lead to millionsof subgraphs on standard datasets.17033 Preliminary conceptsIn this section, we introduce the preliminary con-cepts upon which our work is built.3.1 Graph-of-wordsWe model a textual document as a graph-of-words,which corresponds to a graph whose vertices rep-resent unique terms of the document and whoseedges represent co-occurrences between the termswithin a fixed-size sliding window.
The under-lying assumption is that all the words present ina document have some undirected relationshipswith the others, modulo a window size outside ofwhich the relationship is not considered.
This rep-resentation was first used in keyword extractionand summarization (Ohsawa et al, 1998; Mihal-cea and Tarau, 2004) and more recently in ad hocIR (Blanco and Lioma, 2012; Rousseau and Vazir-giannis, 2013).
We refer to (Blanco and Lioma,2012) for an in-depth review of the graph repre-sentations of text in NLP.system softwarimplement disciplinsciencspantopictheoretstudiranglimitalgorithmissupractichardwarcomputAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.hide meFigure 1: Graph-of-words representation of a tex-tual document ?
in bold font, its main core.Figure 1 illustrates the graph-of-words repre-sentation of a textual document.
The vertices cor-respond to the remaining terms after standard pre-processing steps have been applied (tokenization,stop word removal and stemming).
The undirectededges were drawn between terms co-occurringwithin a sliding window over the processed text ofsize 4, value consistently reported as working wellin the references aforementioned and validated inour experiments.
Edge direction was used by Fil-ippova (2010) so as to extract valid sentences butnot here in order to capture some word inversion.Note that for small-enough window sizes(which is typically the case in practice), we canconsider that two terms linked represent a long-distance bigram (Bassiou and Kotropoulos, 2010),if not a bigram.
Furthermore, by extending thedenomination, we can consider that a subgraphof size n is a long-distance n-gram, if not an n-gram.
Indeed, the nodes belonging to a subgraphdo not necessarily appear in a sequence in the doc-ument like for a n-gram.
Moreover, this enables usto ?merge?
together n-grams that share the sameterms but maybe not in the same order.
In the ex-periments, by abusing the terminology, we will re-fer to them as n-grams to adopt a common termi-nology with the baseline approaches.3.2 Node/edge labels and subgraph matchingIn graph classification, it is common to introducea node labeling function ?
to map a node id to itslabel.
For instance, consider the case of chemi-cal compounds (e. g., the benzene C6H6).
Then inits graph representation (its ?structural formula?
),it is crucial to differentiate between the multiplenodes labeled the same (e. g., C or H).
In the caseof graph-of-words, node labels are unique insidea graph since they represent unique terms of thedocument and we can therefore omit these func-tions since they are injective in our case and wecan substitute node ids for node labels.
In partic-ular, the general problem of subgraph matching,which defines an isomorphism between a graphand a subgraph and is NP-complete (Garey andJohnson, 1990), can be reduced to a polynomialproblem when node labels are unique.
In our ex-periments, we used the standard algorithm VF2developed by Cordella et al (2001).3.3 K-core and main coreSeidman (1983) defined the k-core of a graph asthe maximal connected subgraph whose verticesare at least of degree k within the subgraph.
Thenon-empty k-core of largest k is called the maincore and corresponds to the most cohesive set(s)of vertices.
The corresponding value of k may dif-fer from one graph to another.
Batagelj and Za-ver?nik (2003) proposed an algorithm to extractthe main core of an unweighted graph in time lin-ear in the number of edges, complexity similarin our case to the other NLP preprocessing steps.Bold font on Figure 1 indicates that a vertex be-longs to the main core of the graph.17044 Graph-of-words classificationIn this section, we present our work and the sev-eral approaches we explored, from unsupervisedfeature mining using gSpan to propose more dis-criminative features than standard n-grams to un-supervised feature selection using k-core to reducethe total number of subgraph and n-gram features.4.1 Unsupervised feature mining using gSpanWe considered the task of text categorization as agraph classification problem by representing tex-tual documents as graph-of-words and then ex-tracting subgraph features to train a graph classi-fier.
Each document is a separate graph-of-wordsand the collection of documents thus correspondsto a set of graphs.
Therefore, for larger datasets,the total number of graphs increases but not theaverage graph size (the average number of uniqueterms in a text), assuming homogeneous datasets.Because the total number of unique node la-bels corresponds to the number of unique termsin the collection in our case, graph kernels arenot suitable for us as verified in practice using theMATLAB code made available by Shervashidze(2009).
We therefore only explored the meth-ods that consider subgraphs as features.
Repeat-ing substructure patterns between graphs are intu-itively good candidates for classification since, atleast for chemical compounds, shared subparts ofmolecules are good indicators of belonging to oneparticular class.
We assumed it would the same fortext.
Indeed, subgraphs of graph-of-words corre-spond to sets of words co-occurring together, justnot necessarily always as the same sequence likefor n-grams ?
it can be seen as a relaxed definitionof a n-gram to capture additional variants.We used gSpan (graph-based Substructurepattern (Yan and Han, 2002)) as frequent sub-graph miner like (Jiang et al, 2010; Arora et al,2010) mostly because of its fast available C++implementation from gBoost (Saigo et al, 2009).Briefly, the key idea behind gSpan is that in-stead of enumerating all the subgraphs and test-ing for isomorphism throughout the collection, itfirst builds for each graph a lexicographic orderof all the edges using depth-first-search (DFS)traversal and assigns to it a unique minimum DFScode.
Based on all these DFS codes, a hierarchicalsearch tree is constructed at the collection-level.By pre-order traversal of this tree, gSpan discov-ers all frequent subgraphs with required support.Consider the set of all subgraphs in the collec-tion of graphs, which corresponds to the set of allpotential features.
Note that there may be overlap-ping (subgraphs sharing nodes/edges) and redun-dant (subgraphs included in others) features.
Be-cause its size is exponential in the number of edges(just like the number of n-grams is exponential inn), it is common to only retain/mine the most fre-quent subgraphs (again just like for n-grams with aminimum document frequency (F?rnkranz, 1998;Joachims, 1998)).
This is controlled via a param-eter known as the support, which sets the mini-mum number of graphs in which a given subgraphhas to appear to be considered as a feature, i. e.the number of subgraph matches in the collection.Here, since node labels are unique inside a graph,we do not have to consider multiple occurrencesof the same subgraph in a given graph.
The lowerthe support, the more features selected/consideredbut the more expensive the mining and the training(not only in time spent for the learning but also forthe feature vector generation).4.2 Unsupervised support selectionThe optimal value for the support can be learnedthrough cross-validation so as to maximize theprediction accuracy of the subsequent classifier,making the whole feature mining process super-vised.
But if we consider that the classifier canonly improve its goodness of fit with more fea-tures (the sets of features being nested as the sup-port varies), it is likely that the lowest support willlead to the best test accuracy; assuming subse-quent regularization to prevent overfitting.
How-ever, this will come at the cost of an exponentialnumber of features as observed in practice.
In-deed, as the support decreases, the number of fea-tures increases slightly up until a point where itincreases exponentially, which makes both the fea-ture vector generation and the learning expensive,especially with multiple classes.
Moreover, weobserved that the prediction performances did notbenefit that much from using all the possible fea-tures (support of 1) as opposed to a more manage-able number of features corresponding to a highersupport.
Therefore, we propose to select the sup-port using the so-called elbow method.
This is anunsupervised empirical method initially developedfor selecting the number of clusters in k-means(Thorndike, 1953).
Figure 3 (upper plots) in Sec-tion 5 illustrates this process.17054.3 Considered classifiersIn text categorization, standard baseline classifiersinclude k-nearest neighbors (kNN) (Larkey andCroft, 1996), Naive Bayes (NB) (McCallum andNigam, 1998) and linear Support Vector Machines(SVM) (Joachims, 1998) with the latter perform-ing the best on n-gram features as verified in ourexperiments.
Since our subgraph features corre-spond to ?long-distance n-grams?, we used linearSVMs as our classifiers in all our experiments ?the goal of our work being to explore and proposebetter features rather than a different classifier.4.4 Multiclass scenarioIn standard binary graph classification (e. g., pre-dicting chemical compounds?
carcinogenicity aseither positive or negative (Helma et al, 2001)),feature mining is performed on the whole graphcollection as we expect the mined features to beable to discriminate between the two classes (thusproducing a good classifier).
However, for thetask of text categorization, there are usually morethan two classes (e. g., 118 categories of news ar-ticles for the Reuters-21578 dataset) and with askewed class distribution (e. g., a lot more newsrelated to ?acquisition?
than to ?grain?).
There-fore, a single support value might lead to someclasses generating a tremendous number of fea-tures (e. g., hundreds of thousands of frequent sub-graphs) and some others only a few (e. g., a fewhundreds subgraphs) resulting in a skewed andnon-discriminative feature set.
To include dis-criminative features for these minority classes, wewould need an extremely low support resultingin an exponential number of features because ofthe majority classes.
For these reasons, we de-cided to mine frequent subgraphs per class usingthe same relative support (%) and then aggregat-ing each feature set into a global one at the cost ofa supervised process (but which still avoids cross-validated parameter tuning).
This was not neededfor the tasks of spam detection and opinion miningsince the corresponding datasets consist of onlytwo balanced classes.4.5 Main core mining using gSpanSince the main drawback of mining frequent sub-graphs for text categorization rather than chemicalcompound classification is the very high numberof possible subgraphs because of the size of thegraphs and the total number of graphs (more than10x in both cases), we thought of ways to reducethe graphs?
sizes while retaining as much classifi-cation information as possible.The graph-of-words representation is designedto capture dependency between words, i. e. de-pendency between features in the context of ma-chine learning but at the document-level.
Ini-tially, we wanted to capture recurring sets of words(i. e. take into account word inversion and sub-set matching) and not just sequences of words likewith n-grams.
In terms of subgraphs, this meanswords that co-occur with each other and form adense subgraph as opposed to a path like for a n-gram.
Therefore, when reducing the graphs, weneed to keep their densest part(s) and that is whywe considered extracting their main cores.
Com-pared to other density-based algorithms, retainingthe main core of a graph has the advantage of be-ing linear in the number of edges, i. e. in the num-ber of unique terms in a document in our case (thenumber of edges is at most the number of nodestimes the fixed size of the sliding window, a smallconstant in practice).4.6 Unsupervised n-gram feature selectionSimilarly to (Hassan et al, 2007) that used graph-of-words to propose alternative weights for the n-gram features, we can capitalize on main core re-tention to still extract binary n-gram features forclassification but considering only the terms be-longing to the main core of each document.
Be-cause some terms never belong to any main coreof any document, the dimension of the overall fea-ture space decreases.
Additionally, since a docu-ment is only represented by a subset of its originalterms, the number of non-zero feature values perdocument also decreases, which matters for SVM,even for the linear kernel, when considering thedual formulation or in the primal with more recentoptimization techniques (Joachims, 2006).Compared to most existing feature selectiontechniques in the field (Yang and Pedersen, 1997),it is unsupervised and corpus-independent as itdoes not rely on any labeled data like IG, MIor ?2nor any collection-wide statistics like IDF,which can be of interest for large-scale text cate-gorization in order to process documents in paral-lel, independently of each other.
In some sense,it is similar to what ?zg?r et al (2005) proposedwith corpus-based and class-based keyword selec-tion for text classification except that we use heredocument-based keyword selection following theapproach from Rousseau and Vazirgiannis (2015).17065 ExperimentsIn this section we present the experiments we con-ducted to validate our approaches.5.1 DatasetsWe used four standard text datasets: two for multi-class document categorization (WebKB and R8),one for spam detection (LingSpam) and one foropinion mining (Amazon) so as to cover all themain subtasks of text categorization:?
WebKB: 4 most frequent categories amonglabeled webpages from various CS depart-ments ?
split into 2,803 for training and 1,396for test (Cardoso-Cachopo, 2007, p.
39?41).?
R8: 8 most frequent categories of Reuters-21578, a set of labeled news articles from the1987 Reuters newswire ?
split into 5,485 fortraining and 2,189 for test (Debole and Se-bastiani, 2005).?
LingSpam: 2,893 emails classified as spamor legitimate messages ?
split into 10 sets for10-fold cross validation (Androutsopoulos etal., 2000).?
Amazon: 8,000 product reviews over fourdifferent sub-collections (books, DVDs, elec-tronics and kitchen appliances) classified aspositive or negative ?
split into 1,600 fortraining and 400 for test each (Blitzer et al,2007).5.2 ImplementationWe developed our approaches mostly in Pythonusing the igraph library (Csardi and Nepusz,2006) for the graph representation and main coreextraction.
For unsupervised subgraph featuremining, we used the C++ implementation ofgSpan from gBoost (Saigo et al, 2009).
Finallyfor classification and standard n-gram text catego-rization we used scikit (Pedregosa et al, 2011),a standard Python machine learning library.5.3 Evaluation metricsTo evaluate the performance of our proposed ap-proaches over standard baselines, we computed onthe test set both the micro- and macro-average F1-score.
Because we are dealing with single-labelclassification, the micro-average F1-score corre-sponds to the accuracy and is a measure of theoverall prediction effectiveness (Manning et al,Dataset # subgraphs before # subgraphs after reductionWebKB 30,868 10,113 67 %R8 39,428 11,373 71 %LingSpam 54,779 15,514 72 %Amazon 16,415 8,745 47 %Dataset # n-grams before # n-grams after reductionWebKB 1,849,848 735,447 60 %R8 1,604,280 788,465 51 %LingSpam 2,733,043 1,016,061 63 %Amazon 583,457 376,664 35 %Table 1: Total number of features (n-grams or sub-graphs) vs. number of features present only inmain cores along with the reduction of the dimen-sion of the feature space on all four datasets.2008, p. 281).
Conversely, the macro-average F1-score takes into account the skewed class label dis-tributions by weighting each class uniformly.
Thestatistical significance of improvement in accuracyover the n-gram SVM baseline was assessed us-ing the micro sign test (p < 0.05) (Yang and Liu,1999).
For the Amazon dataset, we report the av-erage of each metric over the four sub-collections.5.4 ResultsTable 2 shows the results on the four considereddatasets.
The first three rows correspond to thebaselines: unsupervised n-gram feature extrac-tion and then supervised learning using kNN, NB(Multinomial but Bernoulli yields similar results)and linear SVM.
The last three rows correspond toour approaches.In our first approach, denoted as ?gSpan +SVM?, we mine frequent subgraphs (gSpan) asfeatures and then train a linear SVM.
These fea-tures correspond to long-distance n-grams.
Thisleads to the best results in text categorization onalmost all datasets (all if we compare to baselinemethods), in particular on multiclass documentcategorization (R8 and WebKB).In our second approach, denoted as ?MC +gSpan + SVM?, we repeat the same procedureexcept that we mine frequent subgraphs (gSpan)from the main core (MC) of each graph-of-wordsand then train an SVM on the resulting features.Main cores can vary from 1-core to 12-core de-pending on the graph structure, 5-core and 6-corebeing the most frequent (more than 60%).
Thisyields results similar to the SVM baseline for afaster mining and training compared to gSpan +SVM.
Table 1 (upper table) shows the reductionin the dimension of the feature space and we see1707Table 2: Test accuracy and macro-average F1-score on four standard datasets.
Bold font marks the bestperformance in a column.
*indicates statistical significance at p < 0.05 using micro sign test with regardsto the SVM baseline of the same column.
MC corresponds to unsupervised feature selection using themain core of each graph-of-words to extract n-gram and subgraph features.
gSpan mining support valuesare 1.6% (WebKB), 7% (R8), 4% (LingSpam) and 0.5% (Amazon).MethodDataset WebKB R8 LingSpam AmazonAccuracy F1-score Accuracy F1-score Accuracy F1-score Accuracy F1-scorekNN (k=5) 0.679 0.617 0.894 0.705 0.910 0.774 0.512 0.644NB (Multinomial) 0.866 0.861 0.934 0.839 0.990 0.971 0.768 0.767linear SVM 0.889 0.871 0.947 0.858 0.991 0.973 0.792 0.790gSpan + SVM 0.912*0.882 0.955*0.864 0.991 0.972 0.798*0.795MC + gSpan + SVM 0.901*0.871 0.949*0.858 0.990 0.973 0.800*0.798MC + SVM 0.872 0.863 0.937 0.849 0.990 0.972 0.786 0.774# non-zero n-gram feature values before unsupervised feature selection050100150200250# documents0 1000 2000 3000 4000 5000# non-zero n-gram feature values after unsupervised feature selection050100150200250# documentsFigure 2: Distribution of non-zero n-gram featurevalues before and after unsupervised feature selec-tion (main core retention) on R8 dataset.that on average less than 60% of the subgraphs arekept for little to no cost in prediction effectiveness.In our final approach, denoted as ?MC + SVM?,we performed unsupervised feature selection bykeeping the terms appearing in the main core (MC)of each document?s graph-of-words representationand then extracted standard n-gram features.
Ta-ble 1 (lower table) shows the reduction in the di-mension of the feature space and we see that on av-erage less than half the n-grams remain.
Figure 2shows the distribution of non-zero features beforeand after the feature selection on the R8 dataset.Similar changes in distribution can be observed onthe other datasets, from a right-tail Gaussian to apower law distribution as expected from the maincore retention.
Table 2 shows that the main coreretention has little to no cost in accuracy and F1-score but can reduce drastically the feature spaceand the number of non-zero values per document.1 2 3 4support (%)050k100k150k200k250k# features5 6 7 8 9 10 11 12 13support (%)1 2 3 4support (%)0.850.900.951.00accuracy5 6 7 8 9 10 11 12 13support (%)Figure 3: Number of subgraph features/accuracyin test per support (%) on WebKB (left) and R8(right) datasets: in black, the selected supportvalue chosen via the elbow method and in red, theaccuracy in test for the SVM baseline.5.5 Unsupervised support selectionFigure 3 above illustrates the unsupervised heuris-tic (elbow method) we used to select the supportvalue, which corresponds to the minimum numberof graphs in which a subgraph has to appear to beconsidered frequent.
We noticed that as the sup-port decreases, the number of features increasesslightly up until a point where it increases expo-nentially.
This support value, highlighted in blackon the figure and chosen before taking into ac-count the class label, is the value we used in ourexperiments and for which we report the results inTable 1 and 2.
The lower plots provide evidence17081-grams 2-grams 3-grams 4-grams 5-grams 6-grams020406080100# features (%)baselinegSpanMC + gSpanFigure 4: Distribution of n-grams (standard andlong-distance ones) among all the features on We-bKB dataset.that the elbow method helps selecting in an unsu-pervised manner a support that leads to the best orclose to the best accuracy.5.6 Distribution of mined n-gramsIn order to gain more insights on why the long-distance n-grams mined with gSpan result in bet-ter classification performances than the baseline n-grams, we computed the distribution of the num-ber of unigrams, bigrams, etc.
up to 6-grams in thetraditional feature set and ours (Figure 4) as wellas in the top 5% features that contribute the mostto the classification decision of the trained SVM(Figure 5).
Again, a long-distance n-gram corre-sponds to a subgraph of size n in a graph-of-wordsand can be seen as a relaxed definition of the tra-ditional n-gram, one that takes into account wordinversion for instance.
To obtain comparable re-sults, we considered for the baseline n-grams witha minimum document frequency equal to the sup-port.
Otherwise, by definition, there are at least asmany bigrams as there are unigrams and so forth.Figure 4 shows that our approaches mine waymore n-grams than unigrams compared to thebaseline.
This happens because with graph-of-words a subgraph of size n corresponds to a setof n terms while with bag-of-words a n-gram cor-responds to a sequence of n terms.
Note that evenwhen restricting the subgraphs to the main cores,there are still more higher order n-grams mined.Figure 5 shows that the higher order n-gramsstill contribute indeed to the classification deci-sion and in higher proportion than with the base-line, even when restricting to the main cores.
For1-grams 2-grams 3-grams 4-grams 5-grams 6-grams020406080100# features (%)baseline SVMgSpan + SVMMC + gSpan + SVMFigure 5: Distribution of n-grams (standard andlong-distance ones) among the top 5% most dis-criminative features for SVM on WebKB dataset.instance, on the R8 dataset, {bank, base, rate}was a discriminative (top 5% SVM features) long-distance 3-gram for the category ?interest?
andoccurred in documents in the form of ?barclaysbank cut its base lending rate?, ?midland bankmatches its base rate?
and ?base rate of natwestbank dropped?, pattern that would be hard to cap-ture with traditional n-gram bag-of-words.5.7 TimingWith an Intel Core i5-3317U clocking at 2.6GHzand 8GB of RAM, mining the subgraph featureswith gSpan takes on average 30s for the selectedsupport.
It can take several hours with lower sup-port and goes down to 5s using the main cores.6 ConclusionIn this paper, we tackled the task of text cate-gorization by representing documents as graph-of-words and then considering the problem as agraph classification one.
We were able to extractmore discriminative features that correspond tolong-distance n-grams through frequent subgraphmining.
Experiments on four standard datasetsshow statistically significant higher accuracy andmacro-averaged F1-score compared to baselines.To the best of our knowledge, graph classifi-cation has never been tested at that scale ?
thou-sands of graphs and tens of thousands of uniquenode labels ?
and also in the multiclass scenario.For these reasons, we could not capitalize on allstandard methods.
In particular, we believe newkernels that support a very high number of uniquenode labels could yield even better performances.1709ReferencesCharu C. Aggarwal and ChengXiang Zhai.
2012.
ASurvey of Text Classification Algorithms.
In MiningText Data, pages 163?222.Ion Androutsopoulos, John Koutsias, Konstantinos V.Chandrinos, George Paliouras, and Constantine D.Spyropoulos.
2000.
An Evaluation of NaiveBayesian Anti-Spam Filtering.
In Proceedings ofthe Workshop on Machine Learning in the New In-formation Age, 11th European Conference on Ma-chine Learning, pages 9?17.Shilpa Arora, Elijah Mayfield, Carolyn Penstein-Ros?,and Eric Nyberg.
2010.
Sentiment ClassificationUsing Automatically Extracted Subgraph Features.In Proceedings of the NAACL HLT 2010 Workshopon Computational Approaches to Analysis and Gen-eration of Emotion in Text, CAAGET ?10, pages131?139.Nikoletta Bassiou and Constantine Kotropoulos.
2010.Word Clustering Using PLSA Enhanced with LongDistance Bigrams.
In Proceedings of the 20th Inter-national Conference on Pattern Recognition, ICPR?10, pages 4226?4229.Vladimir Batagelj and Matja?
Zaversnik.
2003.An O(m) Algorithm for Cores Decomposition ofNetworks.
The Computing Research Repository(CoRR), cs.DS/0310049.Roi Blanco and Christina Lioma.
2012.
Graph-basedterm weighting for information retrieval.
Informa-tion Retrieval, 15(1):54?92.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boomboxes andblenders: Domain adaptation for sentiment classi-fication.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,ACL ?07, pages 440?447.Ana Cardoso-Cachopo.
2007.
Improving Methods forSingle-label Text Categorization.
Ph.D. thesis, Insti-tuto Superior T?cnico, Universidade de Lisboa, Lis-bon, Portugal.Luigi Pietro Cordella, Pasquale Foggia, Carlo Sansone,and Mario Vento.
2001.
An improved algorithm formatching large graphs.
In Proceedings of the 3rdIAPR-TC15 Workshop on Graph-based Representa-tions in Pattern Recognition, pages 149?159.Gabor Csardi and Tamas Nepusz.
2006.
The igraphsoftware package for complex network research.
In-terJournal, Complex Systems, 1695(5):1?9.Franca Debole and Fabrizio Sebastiani.
2005.
AnAnalysis of the Relative Hardness of Reuters-21578Subsets: Research Articles.
Journal of the Ameri-can Society for Information Science and Technology,56(6):584?596.Mukund Deshpande, Michihiro Kuramochi, NikilWale, and George Karypis.
2005.
Fre-quent Substructure-Based Approaches for Classi-fying Chemical Compounds.
IEEE Transactionson Knowledge and Data Engineering, 17(8):1036?1050.Katja Filippova.
2010.
Multi-sentence Compression:Finding Shortest Paths in Word Graphs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, COLING ?10, pages 322?330.Johannes F?rnkranz.
1998.
A study using n-gramfeatures for text categorization.
Technical ReportOEFAI-TR-98-30, Austrian Research Institute forArtificial Intelligence.Michael R. Garey and David S. Johnson.
1990.
Com-puters and Intractability; A Guide to the Theory ofNP-Completeness.
W. H. Freeman & Co.Thomas G?rtner, Peter Flach, and Stefan Wrobel.2003.
On graph kernels: Hardness results andefficient alternatives.
In Proceedings of the An-nual Conference on Computational Learning The-ory, COLT ?03, pages 129?143.Samer Hassan, Rada Mihalcea, and Carmen Banea.2007.
Random-Walk Term Weighting for ImprovedText Classification.
In Proceedings of the Interna-tional Conference on Semantic Computing, ICSC?07, pages 242?249.Christoph Helma, Ross D. King, Stefan Kramer,and Ashwin Srinivasan.
2001.
The predictivetoxicology challenge 2000?2001.
Bioinformatics,17(1):107?108.Jun Huan, Wei Wang, and Jan Prins.
2003.
EfficientMining of Frequent Subgraphs in the Presence ofIsomorphism.
In Proceedings of the 3rd IEEE In-ternational Conference on Data Mining, ICDM ?03,pages 549?552.Chuntao Jiang, Frans Coenen, Robert Sanderson, andMichele Zito.
2010.
Text classification using graphmining-based feature extraction.
Knowledge-BasedSystems, 23(4):302?308.Ning Jin, Calvin Young, and Wei Wang.
2010.
GAIA:graph classification using evolutionary computation.In Proceedings of the 2010 ACM SIGMOD interna-tional conference on Management of data, SIGMOD?10, pages 879?890.Thorsten Joachims.
1998.
Text categorization withSupport Vector Machines: Learning with many rel-evant features.
In Proceedings of the 10th EuropeanConference on Machine Learning, ECML ?98, pages137?142.Thorsten Joachims.
2006.
Training Linear SVMsin Linear Time.
In Proceedings of the 12th ACMSIGKDD international conference on KnowledgeDiscovery and Data mining, KDD ?06, pages 217?226.1710Hisashi Kashima, Koji Tsuda, and Akihiro Inokuchi.2003.
Marginalized kernels between labeled graphs.In Proceedings of the 20th International Conferenceon Machine Learning, volume 3 of ICML ?03, pages321?328.Taku Kudo and Yuji Matsumoto.
2004.
A Boosting Al-gorithm for Classification of Semi-Structured Text.In Proceedings of the 9th Conference on EmpiricalMethods in Natural Language Processing, volume 4of EMNLP ?04, pages 301?308.Taku Kudo, Eisaku Maeda, and Yuji Matsumoto.
2004.An application of boosting to graph classification.In Advances in Neural Information Processing Sys-tems 17, NIPS ?04, pages 729?736.Leah S. Larkey and W. Bruce Croft.
1996.
CombiningClassifiers in Text Categorization.
In Proceedingsof the 19th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, SIGIR ?96, pages 289?297.Pierre Mah?, Nobuhisa Ueda, Tatsuya Akutsu, Jean-Luc Perret, and Jean-Philippe Vert.
2004.
Exten-sions of marginalized graph kernels.
In Proceed-ings of the 21st International Conference on Ma-chine Learning, ICML ?04, pages 70?78.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, New York,NY, USA.Alex Markov, Mark Last, and Abraham Kandel.
2007.Fast Categorization of Web Documents Representedby Graphs.
In Advances in Web Mining and WebUsage Analysis, number 4811 in Lecture Notes inArtificial Intelligence, pages 56?71.Shotaro Matsumoto, Hiroya Takamura, and ManabuOkumura.
2005.
Sentiment Classification UsingWord Sub-sequences and Dependency Sub-trees.
InProceedings of the 9th Pacific-Asia Conference onAdvances in Knowledge Discovery and Data Min-ing, PAKDD ?05, pages 301?311.Andrew McCallum and Kamal Nigam.
1998.
A com-parison of event models for Naive Bayes text classi-fication.
In Proceedings of the AAAI workshop onlearning for text categorization, AAAI ?98, pages41?48.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing Order into Texts.
In Proceedings of the 9thConference on Empirical Methods in Natural Lan-guage Processing, EMNLP ?04, pages 404?411.Siegfried Nijssen and Joost N. Kok.
2004.
A Quick-start in Frequent Structure Mining Can Make a Dif-ference.
In Proceedings of the 10th ACM SIGKDDinternational conference on Knowledge Discoveryand Data mining, KDD ?04, pages 647?652.Yukio Ohsawa, Nels E. Benson, and MasahikoYachida.
1998.
KeyGraph: Automatic Indexing byCo-occurrence Graph Based on Building Construc-tion Metaphor.
In Proceedings of the Advances inDigital Libraries Conference, ADL ?98, pages 12?18.Arzucan ?zg?r, Levent ?zg?r, and Tunga G?ng?r.2005.
Text Categorization with Class-based andCorpus-based Keyword Selection.
In Proceedingsof the 20th International Conference on Computerand Information Sciences, ISCIS ?05, pages 606?615.Fabian Pedregosa, Ga?l Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
The Journal of Machine LearningResearch, 12:2825?2830.Fran?ois Rousseau and Michalis Vazirgiannis.
2013.Graph-of-word and TW-IDF: New Approach to AdHoc IR.
In Proceedings of the 22nd ACM inter-national conference on Information and knowledgemanagement, CIKM ?13, pages 59?68.Fran?ois Rousseau and Michalis Vazirgiannis.
2015.Main Core Retention on Graph-of-words for Single-Document Keyword Extraction.
In Proceedings ofthe 37th European Conference on Information Re-trieval, ECIR ?15, pages 382?393.Hiroto Saigo, Sebastian Nowozin, Tadashi Kadowaki,Taku Kudo, and Koji Tsuda.
2009. gBoost: a math-ematical programming approach to graph classifica-tion and regression.
Machine Learning, 75(1):69?89.Fabrizio Sebastiani.
2002.
Machine Learning in Au-tomated Text Categorization.
ACM Computing Sur-veys, 34(1):1?47.Stephen B. Seidman.
1983.
Network structure andminimum degree.
Social Networks, 5:269?287.Nino Shervashidze.
Visited on 30/05/2015.
Graph ker-nels.
http://www.di.ens.fr/~shervashidze/code.html.Marisa Thoma, Hong Cheng, Arthur Gretton, Ji-awei Han, Hans-Peter Kriegel, Alexander J. Smola,Le Song, Philip S. Yu, Xifeng Yan, and Karsten M.Borgwardt.
2009.
Near-optimal Supervised FeatureSelection among Frequent Subgraphs.
In Proceed-ings of the SIAM International Conference on DataMining, SDM ?09, pages 1076?1087.Robert Thorndike.
1953. Who belongs in the family?Psychometrika, 18(4):267?276.S.
V. N. Vishwanathan, Nicol N. Schraudolph, RisiKondor, and Karsten M. Borgwardt.
2010.
Graphkernels.
Journal of Machine Learning Research,11:1201?1242.1711Xifeng Yan and Jiawei Han.
2002. gspan: Graph-based substructure pattern mining.
In Proceedingsof the 2nd IEEE International Conference on DataMining, ICDM ?02, pages 721?724.Yiming Yang and Xin Liu.
1999.
A Re-examination ofText Categorization Methods.
In Proceedings of the22nd annual international ACM SIGIR conferenceon Research and development in information re-trieval, SIGIR ?99, pages 42?49.Yiming Yang and J. O. Pedersen.
1997.
A Compar-ative Study on Feature Selection in Text Catego-rization.
In Proceedings of the 14th InternationalConference on Machine Learning, ICML ?97, pages412?420.1712
