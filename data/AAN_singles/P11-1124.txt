Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1239?1248,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsConsistent Translation using Discriminative Learning:A Translation Memory-inspired Approach?Yanjun Ma?
Yifan He?
Andy Way?
Josef van Genabith??
Baidu Inc., Beijing, Chinayma@baidu.com?Centre for Next Generation LocalisationSchool of Computing, Dublin City University{yhe,away,josef}@computing.dcu.ieAbstractWe present a discriminative learning methodto improve the consistency of translations inphrase-based Statistical Machine Translation(SMT) systems.
Our method is inspired byTranslation Memory (TM) systems which arewidely used by human translators in industrialsettings.
We constrain the translation of an in-put sentence using the most similar ?transla-tion example?
retrieved from the TM.
Differ-ently from previous research which used sim-ple fuzzy match thresholds, these constraintsare imposed using discriminative learning tooptimise the translation performance.
We ob-serve that using this method can benefit theSMT system by not only producing consis-tent translations, but also improved translationoutputs.
We report a 0.9 point improvementin terms of BLEU score on English?Chinesetechnical documents.1 IntroductionTranslation consistency is an important factorfor large-scale translation, especially for domain-specific translations in an industrial environment.For example, in the translation of technical docu-ments, lexical as well as structural consistency is es-sential to produce a fluent target-language sentence.Moreover, even in the case of translation errors, con-sistency in the errors (e.g.
repetitive error patterns)are easier to diagnose and subsequently correct bytranslators.
?This work was done while the first author was in the Cen-tre for Next Generation Localisation at Dublin City University.In phrase-based SMT, translation models and lan-guage models are automatically learned and/or gen-eralised from the training data, and a translation isproduced by maximising a weighted combination ofthese models.
Given that global contextual informa-tion is not normally incorporated, and that trainingdata is usually noisy in nature, there is no guaran-tee that an SMT system can produce translations ina consistent manner.On the other hand, TM systems ?
widely used bytranslators in industrial environments for enterpriselocalisation by translators ?
can shed some light onmitigating this limitation.
TM systems can assisttranslators by retrieving and displaying previouslytranslated similar ?example?
sentences (displayed assource-target pairs, widely called ?fuzzy matches?
inthe localisation industry (Sikes, 2007)).
In TM sys-tems, fuzzy matches are retrieved by calculating thesimilarity or the so-called ?fuzzy match score?
(rang-ing from 0 to 1 with 0 indicating no matches and 1indicating a full match) between the input sentenceand sentences in the source side of the translationmemory.When presented with fuzzy matches, translatorscan then avail of useful chunks in previous transla-tions while composing the translation of a new sen-tence.
Most translators only consider a few sen-tences that are most similar to the current input sen-tence; this process can inherently improve the con-sistency of translation, given that the new transla-tions produced by translators are likely to be similarto the target side of the fuzzy match they have con-sulted.Previous research as discussed in detail in Sec-1239tion 2 has focused on using fuzzy match score asa threshold when using the target side of the fuzzymatches to constrain the translation of the inputsentence.
In our approach, we use a more fine-grained discriminative learning method to determinewhether the target side of the fuzzy matches shouldbe used as a constraint in translating the input sen-tence.
We demonstrate that our method can consis-tently improve translation quality.The rest of the paper is organized as follows:we begin by briefly introducing related research inSection 2.
We present our discriminative learningmethod for consistent translation in Section 3 andour feature design in Section 4.
We report the exper-imental results in Section 5 and conclude the paperand point out avenues for future research in Section6.2 Related ResearchDespite the fact that TM and MT integration haslong existed as a major challenge in the localisationindustry, it has only recently received attention inmain-stream MT research.
One can loosely combineTM and MT at sentence (called segments in TMs)level by choosing one of them (or both) to recom-mend to the translators using automatic classifiers(He et al, 2010), or simply using fuzzy match scoreor MT confidence measures (Specia et al, 2009).One can also tightly integrate TM with MT at thesub-sentence level.
The basic idea is as follows:given a source sentence to translate, we firstly usea TM system to retrieve the most similar ?example?source sentences together with their translations.
Ifmatched chunks between input sentence and fuzzymatches can be detected, we can directly re-use thecorresponding parts of the translation in the fuzzymatches, and use an MT system to translate the re-maining chunks.As a matter of fact, implementing this idea ispretty straightforward: a TM system can easily de-tect the word alignment between the input sentenceand the source side of the fuzzy match by retracingthe paths used in calculating the fuzzy match score.To obtain the translation for the matched chunks, wejust require the word alignment between source andtarget TM matches, which can be addressed usingstate-of-the-art word alignment techniques.
Moreimportantly, albeit not explicitly spelled out in pre-vious work, this method can potentially increase theconsistency of translation, as the translation of newinput sentences is closely informed and guided (orconstrained) by previously translated sentences.There are several different ways of using thetranslation information derived from fuzzy matches,with the following two being the most widelyadopted: 1) to add these translations into a phrasetable as in (Bic?ici and Dymetman, 2008; Simard andIsabelle, 2009), or 2) to mark up the input sentenceusing the relevant chunk translations in the fuzzymatch, and to use an MT system to translate the partsthat are not marked up, as in (Smith and Clark, 2009;Koehn and Senellart, 2010; Zhechev and van Gen-abith, 2010).
It is worth mentioning that translationconsistency was not explicitly regarded as their pri-mary motivation in this previous work.
Our researchfollows the direction of the second strand given thatconsistency can no longer be guaranteed by con-structing another phrase table.However, to categorically reuse the translationsof matched chunks without any differentiation couldgenerate inferior translations given the fact that thecontext of these matched chunks in the input sen-tence could be completely different from the sourceside of the fuzzy match.
To address this problem,both (Koehn and Senellart, 2010) and (Zhechev andvan Genabith, 2010) used fuzzy match score as athreshold to determine whether to reuse the transla-tions of the matched chunks.
For example, (Koehnand Senellart, 2010) showed that reusing these trans-lations as large rules in a hierarchical system (Chi-ang, 2005) can be beneficial when the fuzzy matchscore is above 70%, while (Zhechev and van Gen-abith, 2010) reported that it is only beneficial to aphrase-based system when the fuzzy match score isabove 90%.Despite being an informative measure, usingfuzzy match score as a threshold has a number oflimitations.
Given the fact that fuzzy match scoreis normally calculated based on Edit Distance (Lev-enshtein, 1966), a low score does not necessarilyimply that the fuzzy match is harmful when usedto constrain an input sentence.
For example, inlonger sentences where fuzzy match scores tend tobe low, some chunks and the corresponding trans-lations within the sentences can still be useful.
On1240the other hand, a high score cannot fully guaranteethe usefulness of a particular translation.
We addressthis problem using discriminative learning.3 Constrained Translation withDiscriminative Learning3.1 Formulation of the ProblemGiven a sentence e to translate, we retrieve the mostsimilar sentence e?
from the translation memory as-sociated with target translation f ?.
The m com-mon ?phrases?
e?m1 between e and e?
can be iden-tified.
Given the word alignment information be-tween e?
and f ?, one can easily obtain the corre-sponding translations f?
?m1 for each of the phrases ine?m1 .
This process can derive a number of ?phrasepairs?
< e?m, f?
?m >, which can be used to specifythe translations of the matched phrases in the inputsentence.
The remaining words without specifiedtranslations will be translated by an MT system.For example, given an input sentence e1e2 ?
?
?eiei+1 ?
?
?
eI , and a phrase pair < e?, f?
?
>, e?
=eiei+1, f?
?
= f ?jf?j+1 derived from the fuzzy match,we can mark up the input sentence as:e1e2 ?
?
?
<tm=?f ?jf ?j+1?> eiei+1 < /tm> ?
?
?
eI .Our method to constrain the translations usingTM fuzzy matches is similar to (Koehn and Senel-lart, 2010), except that the word alignment betweene?
and f ?
is the intersection of bidirectional GIZA++(Och and Ney, 2003) posterior alignments.
We usethe intersected word alignment to minimise the noiseintroduced by word alignment of only one directionin marking up the input sentence.3.2 Discriminative LearningWhether the translation information from the fuzzymatches should be used or not (i.e.
whether the inputsentence should be marked up) is determined usinga discriminative learning procedure.
The translationinformation refers to the ?phrase pairs?
derived us-ing the method described in Section 3.1.
We castthis problem as a binary classification problem.3.2.1 Support Vector MachinesSVMs (Cortes and Vapnik, 1995) are binary classi-fiers that classify an input instance based on decisionrules which minimise the regularised error functionin (1):minw,b,?12wT w + Cl?i=1?is.
t.
yi(wT?
(xi) + b) > 1?
?i?i > 0(1)where (xi, yi) ?
Rn ?
{+1,?1} are l training in-stances that are mapped by the function ?
to a higherdimensional space.
w is the weight vector, ?
is therelaxation variable and C > 0 is the penalty param-eter.Solving SVMs is viable using a kernel functionK in (1) with K(xi, xj) = ?(xi)T?(xj).
We per-form our experiments with the Radial Basis Func-tion (RBF) kernel, as in (2):K(xi, xj) = exp(?
?||xi ?
xj ||2), ?
> 0 (2)When using SVMs with the RBF kernel, we havetwo free parameters to tune on: the cost parameterC in (1) and the radius parameter ?
in (2).In each of our experimental settings, the param-eters C and ?
are optimised by a brute-force gridsearch.
The classification result of each set of pa-rameters is evaluated by cross validation on thetraining set.The SVM classifier will thus be able to predictthe usefulness of the TM fuzzy match, and deter-mine whether the input sentence should be markedup using relevant phrase pairs derived from the fuzzymatch before sending it to the SMT system for trans-lation.
The classifier uses features such as the fuzzymatch score, the phrase and lexical translation prob-abilities of these relevant phrase pairs, and addi-tional syntactic dependency features.
Ideally theclassifier will decide to mark up the input sentenceif the translations of the marked phrases are accuratewhen taken contextual information into account.
Aslarge-scale manually annotated data is not availablefor this task, we use automatic TER scores (Snoveret al, 2006) as the measure for training data annota-tion.We label the training examples as in (3):y ={+1 if TER(w. markup) < TER(w/o markup)?1 if TER(w/o markup) ?
TER(w. markup)(3)Each instance is associated with a set of featureswhich are discussed in more detail in Section 4.12413.2.2 Classification Confidence EstimationWe use the techniques proposed by (Platt, 1999) andimproved by (Lin et al, 2007) to convert classifica-tion margin to posterior probability, so that we caneasily threshold our classifier (cf.
Section 5.4.2).Platt?s method estimates the posterior probabilitywith a sigmoid function, as in (4):Pr(y = 1|x) ?
PA,B(f) ?11 + exp(Af + B)(4)where f = f(x) is the decision function of the esti-mated SVM.
A and B are parameters that minimisethe cross-entropy error function F on the trainingdata, as in (5):minz=(A,B)F (z) = ?l?i=1(tilog(pi) + (1 ?
ti)log(1?
pi)),where pi = PA,B(fi), and ti ={N++1N++2 if yi = +11N?+2 if yi = ?1(5)where z = (A,B) is a parameter setting, andN+ and N?
are the numbers of observed positiveand negative examples, respectively, for the label yi.These numbers are obtained using an internal cross-validation on the training set.4 Feature SetThe features used to train the discriminative classi-fier, all on the sentence level, are described in thefollowing sections.4.1 The TM FeatureThe TM feature is the fuzzy match score, which in-dicates the overall similarity between the input sen-tence and the source side of the TM output.
If theinput sentence is similar to the source side of thematching segment, it is more likely that the match-ing segment can be used to mark up the input sen-tence.The calculation of the fuzzy match score itself isone of the core technologies in TM systems, andvaries among different vendors.
We compute fuzzymatch cost as the minimum Edit Distance (Leven-shtein, 1966) between the source and TM entry, nor-malised by the length of the source as in (6), asmost of the current implementations are based onedit distance while allowing some additional flexi-ble matching.hfm(e) = minsEditDistance(e, s)Len(e)(6)where e is the sentence to translate, and s is thesource side of an entry in the TM.
For fuzzy matchscores F , hfm roughly corresponds to 1?
F .4.2 Translation FeaturesWe use four features related to translation probabil-ities, i.e.
the phrase translation and lexical probabil-ities for the phrase pairs < e?m, f?
?m > derived us-ing the method in Section 3.1.
Specifically, we usethe phrase translation probabilities p(f?
?m|e?m) andp(e?m|f?
?m), as well as the lexical translation prob-abilities plex(f?
?m|e?m) and plex(e?m|f?
?m) as calcu-lated in (Koehn et al, 2003).
In cases where mul-tiple phrase pairs are used to mark up one singleinput sentence e, we use a unified score for eachof the four features, which is an average over thecorresponding feature in each phrase pair.
The intu-ition behind these features is as follows: phrase pairs< e?m, f?
?m > derived from the fuzzy match shouldalso be reliable with respect to statistically producedmodels.We also have a count feature, i.e.
the number ofphrases used to mark up the input sentence, and abinary feature, i.e.
whether the phrase table containsat least one phrase pair < e?m, f?
?m > that is used tomark up the input sentence.4.3 Dependency FeaturesGiven the phrase pairs < e?m, f?
?m > derived fromthe fuzzy match, and used to translate the corre-sponding chunks of the input sentence (cf.
Sec-tion 3.1), these translations are more likely to be co-herent in the context of the particular input sentenceif the matched parts on the input side are syntacti-cally and semantically related.For matched phrases e?m between the input sen-tence and the source side of the fuzzy match, we de-fine the contextual information of the input side us-ing dependency relations between words em in e?mand the remaining words ej in the input sentence e.We use the Stanford parser to obtain the depen-dency structure of the input sentence.
We adda pseudo-label SYS PUNCT to punctuation marks,whose governor and dependent are both the punc-tuation mark.
The dependency features designed tocapture the context of the matched input phrases e?mare as follows:1242Coverage features measure the coverage of de-pendency labels on the input sentence in order toobtain a bigger picture of the matched parts in theinput.
For each dependency label L, we consider itshead or modifier as covered if the corresponding in-put word em is covered by a matched phrase e?m.Our coverage features are the frequencies of gov-ernor and dependent coverage calculated separatelyfor each dependency label.Position features identify whether the head andthe tail of a sentence are matched, as these are thecases in which the matched translation is not af-fected by the preceding words (when it is the head)or following words (when it is the tail), and is there-fore more reliable.
The feature is set to 1 if this hap-pens, and to 0 otherwise.
We distinguish among thepossible dependency labels, the head or the tail ofthe sentence, and whether the aligned word is thegovernor or the dependent.
As a result, each per-mutation of these possibilities constitutes a distinctbinary feature.The consistency feature is a single feature whichdetermines whether matched phrases e?m belong toa consistent dependency structure, instead of beingdistributed discontinuously around in the input sen-tence.
We assume that a consistent structure is lessinfluenced by its surrounding context.
We set thisfeature to 1 if every word in e?m is dependent on an-other word in e?m, and to 0 otherwise.5 Experiments5.1 Experimental SetupOur data set is an English?Chinese translation mem-ory with technical translation from Symantec, con-sisting of 87K sentence pairs.
The average sentencelength of the English training set is 13.3 words andthe size of the training set is comparable to the largerTMs used in the industry.
Detailed corpus statisticsabout the training, development and test sets for theSMT system are shown in Table 1.The composition of test subsets based on fuzzymatch scores is shown in Table 2.
We can see thatsentences in the test sets are longer than those in thetraining data, implying a relatively difficult trans-lation task.
We train the SVM classifier using thelibSVM (Chang and Lin, 2001) toolkit.
The SVM-Train Develop TestSENTENCES 86,602 762 943ENG.
TOKENS 1,148,126 13,955 20,786ENG.
VOC.
13,074 3,212 3,115CHI.
TOKENS 1,171,322 10,791 16,375CHI.
VOC.
12,823 3,212 1,431Table 1: Corpus StatisticsScores Sentences Words W/S(0.9, 1.0) 80 1526 19.0750(0.8, 0.9] 96 1430 14.8958(0.7, 0.8] 110 1596 14.5091(0.6, 0.7] 74 1031 13.9324(0.5, 0.6] 104 1811 17.4135(0, 0.5] 479 8972 18.7307Table 2: Composition of test subsets based on fuzzymatch scorestraining and validation is on the same training sen-tences1 as the SMT system with 5-fold cross valida-tion.The SVM hyper-parameters are tuned using thetraining data of the first fold in the 5-fold cross val-idation via a brute force grid search.
More specifi-cally, for parameter C in (1), we search in the range[2?5, 215], while for parameter ?
(2) we search in therange [2?15, 23].
The step size is 2 on the exponent.We conducted experiments using a standard log-linear PB-SMT model: GIZA++ implementation ofIBM word alignment model 4 (Och and Ney, 2003),the refinement and phrase-extraction heuristics de-scribed in (Koehn et al, 2003), minimum-error-rate training (Och, 2003), a 5-gram language modelwith Kneser-Ney smoothing (Kneser and Ney, 1995)trained with SRILM (Stolcke, 2002) on the Chineseside of the training data, and Moses (Koehn et al,2007) which is capable of handling user-specifiedtranslations for some portions of the input during de-coding.
The maximum phrase length is set to 7.5.2 EvaluationThe performance of the phrase-based SMT systemis measured by BLEU score (Papineni et al, 2002)and TER (Snover et al, 2006).
Significance test-1We have around 87K sentence pairs in our training data.However, for 67.5% of the input sentences, our MT system pro-duces the same translation irrespective of whether the input sen-tence is marked up or not.1243ing is carried out using approximate randomisation(Noreen, 1989) with a 95% confidence level.We also measure the quality of the classificationby precision and recall.
Let A be the set of pre-dicted markup input sentences, and B be the setof input sentences where the markup version has alower TER score than the plain version.
We stan-dardly define precision P and recall R as in (7):P =|A?B||A| , R =|A?B||B| (7)5.3 Cross-fold translationIn order to obtain training samples for the classifier,we need to label each sentence in the SMT trainingdata as to whether marking up the sentence can pro-duce better translations.
To achieve this, we translateboth the marked-up versions and plain versions ofthe sentence and compare the two translations usingthe sentence-level evaluation metric TER.We do not make use of additional training data totranslate the sentences for SMT training, but insteaduse cross-fold translation.
We create a new trainingcorpus T by keeping 95% of the sentences in theoriginal training corpus, and creating a new test cor-pus H by using the remaining 5% of the sentences.Using this scheme we make 20 different pairs of cor-pora (Ti,Hi) in such a way that each sentence fromthe original training corpus is in exactly one Hi forsome 1 ?
i ?
20.
We train 20 different systemsusing each Ti, and use each system to translate thecorresponding Hi as well as the marked-up versionof Hi using the procedure described in Section 3.1.The development set is kept the same for all systems.5.4 Experimental Results5.4.1 Translation ResultsTable 3 contains the translation results of the SMTsystem when we use discriminative learning to markup the input sentence (MARKUP-DL).
The first row(BASELINE) is the result of translating plain testsets without any markup, while the second row isthe result when all the test sentences are markedup.
We also report the oracle scores, i.e.
the up-perbound of using our discriminative learning ap-proach.
As we can see from this table, we obtain sig-nificantly inferior results compared to the the Base-line system if we categorically mark up all the in-TER BLEUBASELINE 39.82 45.80MARKUP 41.62 44.41MARKUP-DL 39.61 46.46ORACLE 37.27 48.32Table 3: Performance of Discriminative Learning (%)put sentences using phrase pairs derived from fuzzymatches.
This is reflected by an absolute 1.4 pointdrop in BLEU score and a 1.8 point increase in TER.On the other hand, both the oracle BLEU and TERscores represent as much as a 2.5 point improve-ment over the baseline.
Our discriminative learningmethod (MARKUP-DL), which automatically clas-sifies whether an input sentence should be markedup, leads to an increase of 0.7 absolute BLEU pointsover the BASELINE, which is statistically signifi-cant.
We also observe a slight decrease in TER com-pared to the BASELINE.
Despite there being muchroom for further improvement when compared to theOracle score, the discriminative learning method ap-pears to be effective not only in maintaining transla-tion consistency, but also a statistically significantimprovement in translation quality.5.4.2 Classification Confidence ThresholdingTo further analyse our discriminative learning ap-proach, we report the classification results on the testset using the SVM classifier.
We also investigate theuse of classification confidence, as described in Sec-tion 3.2.2, as a threshold to boost classification pre-cision if required.
Table 4 shows the classificationand translation results when we use different con-fidence thresholds.
The default classification con-fidence is 0.50, and the corresponding translationresults were described in Section 5.4.1.
We inves-tigate the impact of increasing classification confi-dence on the performance of the classifier and thetranslation results.
As can be seen from Table 4,increasing the classification confidence up to 0.70leads to a steady increase in classification precisionwith a corresponding sacrifice in recall.
The fluc-tuation in classification performance has an impacton the translation results as measured by BLEU andTER.
We can see that the best BLEU as well as TERscores are achieved when we set the classificationconfidence to 0.60, representing a modest improve-1244Classification Confidence0.50 0.55 0.60 0.65 0.70 0.75 0.80BLEU 46.46 46.65 46.69 46.59 46.34 46.06 46.00TER 39.61 39.46 39.32 39.36 39.52 39.71 39.71P 60.00 68.67 70.31 74.47 72.97 64.28 88.89R 32.14 29.08 22.96 17.86 13.78 9.18 4.08Table 4: The impact of classification confidence thresholdingment over the default setting (0.50).
Despite thehigher precision when the confidence is set to 0.7,the dramatic decrease in recall cannot be compen-sated for by the increase in precision.We can also observe from Table 4 that the recallis quite low across the board, and the classificationresults become unstable when we further increasethe level of confidence to above 0.70.
This indicatesthe degree of difficulty of this classification task, andsuggests some directions for future research as dis-cussed at the end of this paper.5.4.3 Comparison with Previous WorkAs discussed in Section 2, both (Koehn and Senel-lart, 2010) and (Zhechev and van Genabith, 2010)used fuzzy match score to determine whether the in-put sentences should be marked up.
The input sen-tences are only marked up when the fuzzy matchscore is above a certain threshold.
We present theresults using this method in Table 5.
From this ta-Fuzzy Match Scores0.50 0.60 0.70 0.80 0.90BLEU 45.13 45.55 45.58 45.84 45.82TER 40.99 40.62 40.56 40.29 40.07Table 5: Performance using fuzzy match score for classi-ficationble, we can see an inferior performance compared tothe BASELINE results (cf.
Table 3) when the fuzzymatch score is below 0.70.
A modest gain can onlybe achieved when the fuzzy match score is above0.8.
This is slightly different from the conclusionsdrawn in (Koehn and Senellart, 2010), where gainsare observed when the fuzzy match score is above0.7, and in (Zhechev and van Genabith, 2010) wheregains are only observed when the score is above 0.9.Comparing Table 5 with Table 4, we can see thatour classification method is more effective.
Thisconfirms our argument in the last paragraph of Sec-tion 2, namely that fuzzy match score is not informa-tive enough to determine the usefulness of the sub-sentences in a fuzzy match, and that a more compre-hensive set of features, as we have explored in thispaper, is essential for the discriminative learning-based method to work.FM Scores w. markup w/o markup[0,0.5] 37.75 62.24(0.5,0.6] 40.64 59.36(0.6,0.7] 40.94 59.06(0.7,0.8] 46.67 53.33(0.8,0.9] 54.28 45.72(0.9,1.0] 44.14 55.86Table 6: Percentage of training sentences with markupvs without markup grouped by fuzzy match (FM) scorerangesTo further validate our assumption, we analysethe training sentences by grouping them accord-ing to their fuzzy match score ranges.
For eachgroup of sentences, we calculate the percentage ofsentences where markup (and respectively withoutmarkup) can produce better translations.
The statis-tics are shown in Table 6.
We can see that for sen-tences with fuzzy match scores lower than 0.8, moresentences can be better translated without markup.For sentences where fuzzy match scores are withinthe range (0.8, 0.9], more sentences can be bettertranslated with markup.
However, within the range(0.9, 1.0], surprisingly, actually more sentences re-ceive better translation without markup.
This indi-cates that fuzzy match score is not a good measure topredict whether fuzzy matches are beneficial whenused to constrain the translation of an input sentence.5.5 Contribution of FeaturesWe also investigated the contribution of our differ-ent feature sets.
We are especially interested inthe contribution of dependency features, as they re-1245Example 1w/o markup after policy name , type the name of the policy ( it shows new host integritypolicy by default ) .Translation ?????????????????
(?????
?????????????w.
markup after policy name <tm translation=?????????????
????
????????
?>, type the name of the policy ( it shows new hostintegrity policy by default ) .< /tm>Translation ?????????????????????
????
???????
?Reference ?????????????????????
????
???????
?Example 2w/o markup changes apply only to the specific scan that you select .Translation ??????????????w.
markup changes apply only to the specific scan that you select <tm translation=??
?>.< /tm>Translation ??????????????
?Reference ??????????????
?flect whether translation consistency can be capturedusing syntactic knowledge.
The classification andTER BLEU P RTM+TRANS 40.57 45.51 52.48 27.04+DEP 39.61 46.46 60.00 32.14Table 7: Contribution of Features (%)translation results using different features are re-ported in Table 7.
We observe a significant improve-ment in both classification precision and recall byadding dependency (DEP) features on top of TMand translation features.
As a result, the translationquality also significantly improves.
This indicatesthat dependency features which can capture struc-tural and semantic similarities are effective in gaug-ing the usefulness of the phrase pairs derived fromthe fuzzy matches.
Note also that without includingthe dependency features, our discriminative learningmethod cannot outperform the BASELINE (cf.
Ta-ble 3) in terms of translation quality.5.6 Improved TranslationsIn order to pinpoint the sources of improvements bymarking up the input sentence, we performed somemanual analysis of the output.
We observe that theimprovements can broadly be attributed to two rea-sons: 1) the use of long phrase pairs which are miss-ing in the phrase table, and 2) deterministically usinghighly reliable phrase pairs.Phrase-based SMT systems normally impose alimit on the length of phrase pairs for storage andspeed considerations.
Our method can overcomethis limitation by retrieving and reusing long phrasepairs on the fly.
A similar idea, albeit from a dif-ferent perspective, was explored by (Lopez, 2008),where he proposed to construct a phrase table on thefly for each sentence to be translated.
Differentlyfrom his approach, our method directly translatespart of the input sentence using fuzzy matches re-trieved on the fly, with the rest of the sentence trans-lated by the pre-trained MT system.
We offer somemore insights into the advantages of our method bymeans of a few examples.Example 1 shows translation improvements byusing long phrase pairs.
Compared to the refer-ence translation, we can see that for the underlinedphrase, the translation without markup contains (i)word ordering errors and (ii) a missing right quota-tion mark.
In Example 2, by specifying the transla-tion of the final punctuation mark, the system cor-rectly translates the relative clause ?that you select?.The translation of this relative clause is missingwhen translating the input without markup.
Thisimprovement can be partly attributed to the reduc-tion in search errors by specifying the highly reliabletranslations for phrases in an input sentence.6 Conclusions and Future WorkIn this paper, we introduced a discriminative learn-ing method to tightly integrate fuzzy matches re-trieved using translation memory technologies withphrase-based SMT systems to improve translationconsistency.
We used an SVM classifier to predictwhether phrase pairs derived from fuzzy matchescould be used to constrain the translation of an in-1246put sentence.
A number of feature functions includ-ing a series of novel dependency features were usedto train the classifier.
Experiments demonstratedthat discriminative learning is effective in improvingtranslation quality and is more informative than thefuzzy match score used in previous research.
We re-port a statistically significant 0.9 absolute improve-ment in BLEU score using a procedure to promotetranslation consistency.As mentioned in Section 2, the potential improve-ment in sentence-level translation consistency us-ing our method can be attributed to the fact thatthe translation of new input sentences is closely in-formed and guided (or constrained) by previouslytranslated sentences using global features such asdependencies.
However, it is worth noting thatthe level of gains in translation consistency is alsodependent on the nature of the TM itself; a self-contained coherent TM would facilitate consistenttranslations.
In the future, we plan to investigatethe impact of TM quality on translation consistencywhen using our approach.
Furthermore, we will ex-plore methods to promote translation consistency atdocument level.Moreover, we also plan to experiment withphrase-by-phrase classification instead of sentence-by-sentence classification presented in this paper,in order to obtain more stable classification results.We also plan to label the training examples usingother sentence-level evaluation metrics such as Me-teor (Banerjee and Lavie, 2005), and to incorporatefeatures that can measure syntactic similarities intraining the classifier, in the spirit of (Owczarzak etal., 2007).
Currently, only a standard phrase-basedSMT system is used, so we plan to test our methodon a hierarchical system (Chiang, 2005) to facilitatedirect comparison with (Koehn and Senellart, 2010).We will also carry out experiments on other data setsand for more language pairs.AcknowledgmentsThis work is supported by Science Foundation Ire-land (Grant No 07/CE/I1142) and part funded underFP7 of the EC within the EuroMatrix+ project (grantNo 231720).
The authors would like to thank thereviewers for their insightful comments and sugges-tions.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72, Ann Arbor, MI.Ergun Bic?ici and Marc Dymetman.
2008.
Dynamictranslation memory: Using statistical machine trans-lation to improve translation memory.
In Proceedingsof the 9th Internation Conference on Intelligent TextProcessing and Computational Linguistics (CICLing),pages 454?465, Haifa, Israel.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.David Chiang.
2005.
A hierarchical Phrase-Based modelfor Statistical Machine Translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 263?270, AnnArbor, MI.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vector networks.
Machine learning, 20(3):273?297.Yifan He, Yanjun Ma, Josef van Genabith, and AndyWay.
2010.
Bridging SMT and TM with translationrecommendation.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 622?630, Uppsala, Sweden.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing, volume 1,pages 181?184, Detroit, MI.Philipp Koehn and Jean Senellart.
2010.
Convergence oftranslation memory and statistical machine translation.In Proceedings of AMTA Workshop on MT Researchand the Translation Industry, pages 21?31, Denver,CO.Philipp Koehn, Franz Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceedingsof the 2003 Human Language Technology Conferenceand the North American Chapter of the Associationfor Computational Linguistics, pages 48?54, Edmon-ton, AB, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for Statistical Machine Translation.
In Pro-ceedings of the 45th Annual Meeting of the Associ-ation for Computational Linguistics Companion Vol-1247ume Proceedings of the Demo and Poster Sessions,pages 177?180, Prague, Czech Republic.Vladimir Iosifovich Levenshtein.
1966.
Binary codes ca-pable of correcting deletions, insertions, and reversals.Soviet Physics Doklady, 10(8):707?710.Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2007.A note on platt?s probabilistic outputs for support vec-tor machines.
Machine Learning, 68(3):267?276.Adam Lopez.
2008.
Tera-scale translation models viapattern matching.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 505?512, Manchester, UK, August.Eric W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses: An Introduction.
Wiley-Interscience, New York, NY.Franz Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Com-putational Linguistics, 29(1):19?51.Franz Och.
2003.
Minimum Error Rate Training in Sta-tistical Machine Translation.
In 41st Annual Meet-ing of the Association for Computational Linguistics,pages 160?167, Sapporo, Japan.Karolina Owczarzak, Josef van Genabith, and Andy Way.2007.
Labelled dependencies in machine translationevaluation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 104?111,Prague, Czech Republic.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of Machine Translation.
In 40th Annual Meet-ing of the Association for Computational Linguistics,pages 311?318, Philadelphia, PA.John C. Platt.
1999.
Probabilistic outputs for supportvector machines and comparisons to regularized likeli-hood methods.
Advances in Large Margin Classifiers,pages 61?74.Richard Sikes.
2007.
Fuzzy matching in theory and prac-tice.
Multilingual, 18(6):39?43.Michel Simard and Pierre Isabelle.
2009.
Phrase-basedmachine translation in a computer-assisted translationenvironment.
In Proceedings of the Twelfth MachineTranslation Summit (MT Summit XII), pages 120 ?127, Ottawa, Ontario, Canada.James Smith and Stephen Clark.
2009.
EBMT for SMT:A new EBMT-SMT hybrid.
In Proceedings of the 3rdInternational Workshop on Example-Based MachineTranslation, pages 3?10, Dublin, Ireland.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas (AMTA-2006), pages 223?231, Cam-bridge, MA, USA.Lucia Specia, Craig Saunders, Marco Turchi, ZhuoranWang, and John Shawe-Taylor.
2009.
Improving theconfidence of machine translation quality estimates.In Proceedings of the Twelfth Machine TranslationSummit (MT Summit XII), pages 136 ?
143, Ottawa,Ontario, Canada.Andreas Stolcke.
2002.
SRILM ?
An extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 901?904, Denver, CO.Ventsislav Zhechev and Josef van Genabith.
2010.Seeding statistical machine translation with translationmemory output through tree-based structural align-ment.
In Proceedings of the Fourth Workshop on Syn-tax and Structure in Statistical Translation, pages 43?51, Beijing, China.1248
