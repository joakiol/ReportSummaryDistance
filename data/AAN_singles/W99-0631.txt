An Iterative Approach to Estimating Frequencies over a SemanticHierarchyStephen Clark and David WeirSchool of Cognitive and Computing SciencesUniversity of SussexBrighton, BN1 9HQ, UK{stephecl, davidw)@cogs, susx.
ac.
ukAbstractThis paper is concerned with using a se-mantic hierarchy to estimate the frequencywith which a word sense appears as a givenargument of a verb, assuming the data isnot sense disambiguated.
The standard ap-proach is to split the count for any noun ap-pearing in the data equally among the al-ternative senses of the noun.
This can leadto inaccurate stimates.
We describe a re-estimation process which uses the accumu-lated counts of hypernyms of the alternativesenses in order to redistribute the count.
Inorder to choose a hypernym for each alter-native sense, we employ a novel techniquewhich uses a X 2 test to measure the homo-geneity of sets of concepts in the hierarchy.1 IntroductionKnowledge of the constraints a verb placeson the semantic types of its arguments (var-iously called selectional restrictions, selec-tional preferences, electional constraints) isof use in many areas of natural anguage pro-cessing, particularly structural disambigua-tion.
Recent treatments of selectional re-str ictions have been probabilistic in nature(Resnik, 1993), (Li and Abe, 1998), (Ribas,1995), (McCarthy, 1997), and estimationof the relevant probabilities has requiredcorpus-based counts of the number of timesword senses, or concepts, appear in the dif-ferent argument positions of verbs.
A dif-ficulty arises due to the absence of a largevolume of sense disambiguated data, as thecounts have to be estimated from the nounswhich appear in the corpus, most of whichwill have more than one sense.
The tech-niques in Resnik (1993), Li and Abe (1998)and Ribas (1995) simply distribute the countequally among the alternative senses of anoun.
Abney and Light (1998) have at-tempted to obtain selectional preferences us-ing the Expectation Maximization algorithmby encoding WordNet as a hidden Markovmodel and using a modified form of theforward-backward algorithm to estimate theparameters.The approach proposed in this paper is touse a re-estimation process which relies oncounts being passed up a semantic hierar-chy, from the senses of nouns appearing inthe data.
We make use of the semantic hier-archy in WordNet (Fellbaum, 1998), whichconsists of word senses, or concepts, 1 relatedby the 'is-a' or 'is-a-kind-of' relation.
If c' isa kind of c, then c is a hypernym of c', and c'a hyponym of c. Counts for any concept aretransmitted up the hierarchy to all of theconcept's hypernyms.
Thus if eat chickenappears in the corpus, the count is transmit-ted up to <meat >, < :food>, and all theother hypernyms of that sense of chicken?The problem is how to distinguish the cor-rect sense of chicken in this case from incor-rect senses such as <wimp>.
3 We utilise the1We use the words sense and concept interchange-ably to refer to a node in the semantic hierarchy.eWe use italics when referring to words, and an-gled brackets when referring to concepts or senses.This notation does not always pick out a conceptuniquely, but the particular concept being referredto should be clear from the context.3The example used here is adapted from Mc-Carthy (1997).
There are in fact four senses ofchicken in WordNet 1.6, but for ease of exposi-tion we consider only two.
The hypernyms of the258fact that whilst splitting the count equallycan lead to inaccurate stimates, counts dotend to accumulate in the right places.
Thuscounts will appear under <:food>, for theobject of eat, but not under <person>, in-dicating that the object position of eat ismore strongly associated with the set of con-cepts dominated by <:food> than with theset of concepts dominated by < person >.By choosing a hypernym for each alternativesense of chicken and comparing how stronglythe sets dominated by these hypernyms as-sociate with eat, we can give more count insubsequent i erations to the food sense ofchicken than to the wimp sense.A problem arises because these two sensesof chicken each have a number of hypernyms,so which two should be compared?
The cho-sen hypernyms have to be high enough inthe hierarchy for adequate counts to haveaccumulated, but not so high that the alter-native senses cannot be distinguished.
Forexample, a hypernym of the food sense ofchicken is <poul t ry>,  and a hypernym ofthe wimp sense is <weakl ing>.
However,these concepts may not be high enough inthe hierarchy for the accumulated counts toindicate that eat is much more strongly as-sociated with the set of concepts dominatedby <poultry> than with the set dominatedby <weakling>.
At the other extreme, ifwe were to choose <ent i ty>,  which is highin the hierarchy, as the hypernym of bothsenses, then clearly we would have no way ofdistinguishing between the two senses.We have developed a technique, using aX 2 test, for choosing a suitable hypernymfor each alternative sense.
The technique isbased on the observation that a chosen hy-pernym is too high in the hierarchy if the setconsisting of the children of the hypernym isnot sufficiently homogeneous with respect othe given verb and argument position.
Usingthe previous example, <ent i ty> is too highto represent either sense of chicken becausefood sense are <poultry>, <bird>, <meat>,< foodstuff >, < food >, < substance >, <object >, < entity >.
The hypernyms of thewimp sense are < weakling >, < person >, <life_form>, <entity>.the children of <ent i ty> are not all associ-ated in the same way with eat.
The set con-sisting of the children of <meat>, however, ishomogeneous with respect o the object po-sition of eat, and so <meat> is not too higha level of representation.
The measure of ho-mogeneity we use is detailed in Section 5.2 The Input Data andSemantic HierarchyThe input data used to estimate frequenciesand probabilities over the semantic hierarchyhas been obtained from the shallow parserdescribed in Briscoe and Carroll (1997).
Thedata consists of a multiset of 'co-occurrencetriples', each triple consisting of a nounlemma, verb lemma, and argument position.We refer to the data as follows: let the uni-verse of verbs, argument positions and nounsthat can appear in the input data be denoted= {Vl,.
.
.
,Vkv }, 1Z---- {r l , .
.
.
, rkn} andAf = {n l , .
.
.
, nk~?
}, respectively.
Note thatin our treatment of selectional restrictions,we do not attempt o distinguish betweenalternative senses of verbs.
We also assumethat each instance of a noun in the datarefers to one, and only one, concept.We use the noun hypernym taxonomy ofWordNet, version 1.6, as our semantic hier-archy.
4 Let C = {Cl,...,Ckc } be the setof concepts in WordNet.
There are approx-imately 66,000 different concepts.
A con-cept is represented in WordNet by a 'syn-onym set' (or 'synset'), which is a set ofsynonymous words which can be used to de-note that concept.
For example, the con-cept 'nut', as in a crazy person, is repre-sented by the following synset: {crackpot,crank, nut, nutcase, fruitcake, screwball}.Let syn(c) C Af be the synset for the con-cept c, and let an(n) = { c In 6 syn(c) } bethe set of concepts that can be denoted bythe noun n. The fact that some nouns areambiguous means that the synsets are notnecessarily disjoint.4There are other taxonomies in WordNet, but weonly use the noun taxonomy.
Hence, from now on,when we talk of concepts inWordNet, we mean con-cepts in the noun taxonomy only.259The hierarchy has the structure of a di-rected acyclic graph, 5with the isa C C xC re-lation connecting nodes in the graph, where(d,c) ?
isa implies d is a kind of c. Letisa* C C x C be the transitive, reflexive clo-sure of isa; and let~= { c' l (d,c ) ?
isa* }be the set consisting of the concept c and allof its hyponyms.
The set <:food> containsall the concepts which are kinds of food, in-eluding <food>.Note that words in our data can appearin synsets anywhere in the hierarchy.
Evenconcepts uch as <ent i ty>,  which appearnear the root of the hierarchy, have synsetscontaining words which may appear in thedata.
The synset for <ent i ty> is {entity,something}, and the words entity and some-thing may well appear in the argument po-sitions of verbs in the corpus.
Furthermore,for a concept c, we distinguish between theset of words that can be used to denote c(the synset of c), and the set of words thatcan be used to denote concepts in L 63 The Measure of Associat ionWe measure the association between argu-ment positions of verbs and sets of conceptsusing the assoc ia t ion  norm (Abe and Li,1996).
7 For C C C, v E Vandr  E 7~, theassociation orm is defined as follows:A(C, v, r) - p(CIv '  r)p(CI )For example, the association between the ob-ject position of eat and the set of conceptsdenoting kinds of food is expressed as fol-lows: A(<food>, eat, object).
Note that, for5The number of nodes in the graph with morethan one parent is only around one percent of thetotal.6Note that Resnik (1993) uses rather non-standard terminology by refering to this second setas the synsets of c.7This work restricts itself to verbs, but can be ex-tended to other kinds of predicates that take nounsas arguments, uch as adjectives.C c C, p(C\]v,r) is just the probability ofthe disjunction of the concepts in C; that is,= Zp(clv, r)cECIn order to see how p(clv ,r) relates to theinput data, note that given a concept c,verb v and argument position r, a noun canbe generated according to the distributionp(n\[c, v, r), wherep(nlc, v, r) = 1nEsyn(c)Now we have a model for the input data:p(n, v, r) = p(v,r)p(niv ,r)= p(v,r) p(clv, rlp(ntc, v,r)cecn(n)Note that for c ?
cn(n), p(nlc, v, r) = O.The association orm (and similar mea-sures such as the mutual information score)have been criticised (Dunning, 1993) becausethese scores can be greatly over-estimatedwhen frequency counts are low.
This prob-lem is overcome to some extent in the schemepresented below since, generally speaking,we only calculate the association orms forconcepts that have accumulated a significantcount.The association norm can be estimatedusing maximum likelihood estimates of theprobabilities as follows.?
(c ,v , r )  _ P(c I  v , r  )(Clr)4 Est imating FrequenciesLet freq(c, v,r), for a particular c, v and r,be the number of (n, v, r) triples in the datain which n is being used to denote c, andlet freq(v, r) be the number of times verb vappears with something in position r in thedata; then the relevant maximum likelihoodestimates, for c E C, v E 12, r E 7~, are as260follows.freq(~, v, r)freq(v, r)~eee  freq(g, v, r)freq(v, r)if(Fir) = Evevfreq(c' v'r)~,ev  freq(v, r)_ Ever~,ev  freq(v, r)Since we do not have sense disambiguateddata, we cannot obtain freq(c, v, r) by sim-ply counting senses.
The standard approachis to estimate freq(c, v, r) by distributingthe count for each noun n in syn(c) evenlyamong all senses of the noun as follows:freq(n, v, r)freq(c, v, r) = ~ I cn(n)lnEsyn(c)where freq(n, v, r) is the number times thetriple (n,v,r) appears in the data, and\[ cn(n)\] is the cardinality of an(n).Although this approach can give inaccu-rate estimates, the counts given to the incor-rect senses will disperse randomly through-out the hierarchy as noise, and by accu-mulating counts up the hierarchy we willtend to gather counts from the correct sensesof related words (Yarowsky, 1992; Resnik,1993).
To see why, consider two instancesof possible triples in the data, drink wineand drink water.
(This example is adaptedfrom Resnik (1993).)
The word water is amember of seven synsets in WordNet 1.6,and wine is a member of two synsets.
Thuseach sense of water will be incremented by0.14 counts, and each sense of wine will beincremented by 0.5 counts.
Now althoughthe incorrect senses of these words will re-ceive counts, those concepts in the hierarchywhich dominate more than one of the senses,such as <beverage>, will accumulate moresubstantial counts.However, although counts tend to accu-mulate in the right places, counts can begreatly underestimated.
In the previous ex-ample, freq(<beverage>,drink, object) is in-cremented by only 0.64 counts from the twodata instances, rather than the correct valueof 2.The approach explored here is to usethe accumulated counts in the following re-estimation procedure.
Given some verb vand position r, for each concept c we havethe following initial estimate, in which thecounts for a noun are distributed evenlyamong all of its senses:^ 0 freq(n,v,r)freq (c, v, r) ---- Icn(n) lnEsyn(c)Given the assumption that counts fromthe related senses of words that can fill po-sition r of verb v will accumulate at hyper-nyms of c, let top(c, v, r) be the hypernymof c (or possibly c itself) that most accu-rately represents his set of related senses.
Inother words, top(c, v, r) will be an approxi-mation of the set of concepts related to c thatfill position r of verb v. Rather than split-ting the counts for a noun n evenly amongeach of its senses c E cn(n), we distributethe counts for n on the basis of the accumu-lated counts at top(c, v, r) for each c E cn(n).In the next section we discuss a method forfinding top(c, v, r), but first we complete thedescription of how the re-estimation processuses the accumulated counts at top(c, v, r).Given a concept c, verb v and position r,in the following formula we use \[c, v, r\] to de-note the set of concepts top(c, v, r).
The re-_ ^  rn+l .estimated frequency treq (c, v, r) is givenas follows.f r  rn+l .
eq (c, v, r) =freq(n,v,r) Am(\[c'~%r\]'v'r)m(F, v, rl, v,r)decn(n)Note that only nouns n in syn(c) con-tribute to the count for c. The countfreq(n, v, r) is split among all concepts in261<milk><meal><course><dish><delicacy>^ 0freq (~, eat, obj)0.0 (0.6)8.5 (5.6)1.3 (1.7)5.3 (5.7)0.3 (1.8)15.4^ 0freq (~,obj)-^ 0freq (~, eat, obj)9.0 (8.4)78.0 (80.9)24.7 (24.3)82.3 (81.9)27.4 (25.9)221.4^ 0freq (~,obj)=^Ev~v freq?
(~, v,obj)9.086.526.087.627.7236.8Table 1: Contingency table for children of <nutriment>cn(n) according to the ratio?m(\[c,v,r\],v,r)5L~?.(.)
?
re(Iv, ~, r\], ~, r)For a set of concepts C,hm(C,v,r) =15m(CI v'r)~m(Clr)wherepm(Clv, r ) = freqm(c, v, r)fr~q(~, ~)ism(Clr) = ~vev freqm( C, v, r)~vev freq(v, r)freqm(C, v, r) = Z freqm(c, v, r)cEC5 Determining top(c,v,r)The technique for calculating top(c, v, r) isbased on the assumption that a hypernymd of c is too high in the hierarchy to betop(e, v, r ) i f  the children of e' are not suf-ficiently homogeneous with respect o v andr.
A set of concepts, C, is taken to be ho-mogeneous with respect to a given v Eand r 6 7~, ifp(vl~ , r) has a similar value foreach c 6 C. Note that this is equivalent tocomparing association orms sincep(vlC, r) _ p(Cv,  r) , , , p~ p(vr)= A(c ,v , r )p (v l r )262and, as we are considering homogeneity fora given verb and argument position, p(vlr )is a constant.To determine whether a set of conceptsis homogeneous, we apply a X 2 test to acontingency table of frequency counts.
Ta-ble 1 shows frequencies for the children of<nutr iment> in the object position of eat,and the figures in brackets are the expectedvalues, based on the marginal totals in thetable.Notice that we use the freq0 counts in thetable.
A more precise method, that we in-tend to explore, would involve creating a newtable for each freqm , m > 0, and recalculat-ing top(c, v, r) after each iteration.
A moresignificant problem of this approach is thatby considering p(v\]~, r), we are not takinginto account the possibility that some con-cepts are associated with more verbs thanothers.
In further work, we plan to consideralternative ways of comparing levels of asso-ciation.The null hypothesis of the test is thatp(vl~ , r) is the same for each c in the table.For example, in Table 1 the null hypothesisis that for every concept c that is a child of<nutriment>, the probability of some con-cept d 6 ~ being eaten, given that it is theobject of some verb, is the same.
For theexperiments described in Section 6, we used0.05 as the level of significance.
Further workwill investigate the effect that different lev-els of significance have on the estimated fre-quencies.The X 2 statistic orresponding to Table 1(v, c) Hypernyms of c( eat, <hotdog> )( drink, <coff ee> )( see, <movie> )( hear, <speaker> )(kiss, <Socrate s> )<sandwich> <snack_food> .
.
.<NUTRIMENT> <food> <substance> <entity><BEVEKAGE> <food> <substance> <entity><SHOW> <communication> <social_relation><relation> <abstraction><communicator> <person> <life_form> <ENTITY><philosopher> <intellect> <person> <LIFE_FOKM> <entity>Table 2: How log-likelihood X2 chooses top(c, v, r)is 4.8.
We use the log-likelihood X ~ statis-tic, rather than the Pearson's X 2 statistic,as this is thought to be more appropriatewhen the counts in the contingency tableare low (Dunning, 1993).
8 For a significancelevel of 0.05, with 4 degrees of freedom, thecritical value is 9.49 (Howell, 1997).
Thus inthis case, the null hypothesis (that the chil-dren of <nutr iment> are homogeneous withrespect o eat) would not be rejected.Given a verb v and position r, we com-pute top(c,v,r) for each c by determiningthe homogeneity of the children of the hy-pernyms of c. Initially, we let top(c, v, r) bethe concept c itself.
We work from c up thehierarchy reassigning top(c, v, r) to be suc-cessive hypernyms of c until we reach a hy-pernym whose children are not sufficientlyhomogeneous.
In situations where a concepthas more than one parent, we consider theparent which results in the lowest X 2 valueas this indicates the highest level of homo-geneity.6 Experimental ResultsIn order to evaluate the re-estimation pro-cedure, we took triples from approximatelytwo million words of parsed text from theSLow counts tend to occur in the table when thetest is being applied to a set of concepts near thefoot of the hierarchy.
A further extension of thiswork will be to use Fisher's exact est for the tableswith particularly low counts.BNC corpus using the shallow parser devel-oped by Briscoe and Carroll (1997).
For thiswork we only considered triples for which r =obj.
Table 2 shows some examples of howthe log-likelihood X2 test chooses top(c, v, r)for various v 6 V and c 6 C. 9 In givingthe list of hypernyms the selected concepttop(c, v, obj) is shown in upper case.Table 3 shows how frequency estimateschange, during the re-estimation process, forvarious v E ~, c E C, and r = obj.
The fig-ures in Table 3 show that the estimates ap-pear to be converging after around 10 itera-tions.
The first column gives the frequencyestimates using the technique of splitting thecount equally among alternative senses of anoun appearing in the data.
The figures foreat and drink suggest that these initial es-t imates can be greatly underestimated (andalso overestimated for cases where the argu-ment strongly violates the selectional prefer-ences of the verb, such as eat <locat ion>) .The final column gives an upper bound onthe re-estimated frequencies.
It shows howmany nouns in the data, in the object po-sition of the given verb, that could possiblybe denoting one of the concepts in ~, for eachv and ~ in the table.
For example, 95 is thenumber of times a noun which could possibly9Notice that < hotdog > is classified at the<nutr iment> level rather than <food>.
This ispresumably due to the fact that beverage is classedas a food, making the set of concepts <food> het-erogenous with respect to the object position of eat.263(v, ~)( eat, <f ood>)( drink,.<beverage>)( eat, <location>)(see, Gobj>)( hear, <person> )(enjoy, <amusement>)( measure, <abstract ion>)m=060.810.52.0237.190.82.919.1freq '~ (~, v, obj)m=l  I m=585.0 89.622.7 23.51.2 1.1235.7 240.285.5 85.53.1 3.321.7 23.3m=1089.823.41.1240.385.53.323.4Limit95266568130531Table 3: Example of re-estimated frequenciesbe denoting a concept dominated by ( food>appeared in the object position of eat.
Sinceeat selects so strongly for its object, wewould expect freq(<food>,eat, obj) (i.e., thetrue figure) to be close to 95.
Similarly, sincedrink selects so strongly for its object, wewould expect freq(< beverage >,drink, obj)to be close to 26.
We would also expectfreq(<location>,eat, obj) to be close to 0.As can be seen from Table 3, our estimatesconverge quite closely to these values.It is noticeable that the frequency countsfor weakly selecting verbs do not change asmuch as for strongly selecting verbs.
Thus,the benefit we achieve compared to the stan-dard approach of distributing counts evenlyis reduced in these cases.
In order to investi-gate the extent to which our technique maybe helping, for each triple in the data wecalculated how the distribution of the countchanged ue to our re-estimation technique.We estimated the extent o which the distri-bution had changed by calculating the per-centage increase in the count for the mostfavoured sense after 10 iterations.
Table 4shows the results we obtained.
The pro-portions given in the second column are ofthe triples in the data containing nouns withmore than one sense.
1?
We can see from the1?17% of the data involved nouns with only onesense in W0rdNet.table that for 43% of the triples our tech-nique is having little effect, but for 23% thecount is at least doubled.7 Conc lus ionsWe have shown that the standard techniquefor estimating frequencies over a semantichierarchy can lead to inaccurate stimates.We have described a re-estimation proce-dure which uses an existing measure of se-lectional preference and which employs anovel way of selecting a hypernym of a con-cept.
Our experiments indicate that the re-estimation procedure gives more accurate s-timates than the standard technique, par-ticularly for strongly selecting verbs.
Thiscould prove particularly useful when usingselectional restrictions, for example in struc-tural disambiguation.8 AcknowledgementsThis work is supported by UK EPSRCproject GR/K97400 and by an EPSRC Re-search Studentship to the first author.
Wewould like to thank John Carroll for supply-ing and providing help with the data, andDiana McCarthy, Gerald Gazdar, Bill Kellerand the anonymous reviewers for their help-ful comments.264Percentage Increase0-1010-5050-100100-Proportion of data43%18%16%23%Table 4: How the distribution of counts changeReferencesNaoki Abe and Hang Li.
1996.
LearningWord Association Norms using Tree CutPair Models.
In Proceedings of the Thir-teenth International Conference on Ma-chine Learning.Steve Abney and Marc Light.
1998.
Hid-ing a Semantic Class Hierarchy in aMarkov Model.
Unpublished.
Paper canbe obtained from http://www.ims.uni-stuttgart.de/,-~light/onlinepapers.html.Ted Briscoe and John Carroll.
1997.
Au-tomatic extraction of subcategorizationfrom corpora.
In Proceedings of the 5thA CL Conference on Applied Natural Lan-guage Processing, pages 356-363, Wash-ington, DC.Ted Dunning.
1993.
Accurate Methods forthe Statistics of Surprise and Coincidence.Computational Linguistics, 19(1):61-74.Christiane Fellbaum, editor.
1998.
WordNetAn Electronic Lexical Database.
The MITPress.D.
Howell.
1997.
Statistical Methods forPsychology: 4th ed.
Duxbury Press.Hang Li and Naoki Abe.
1998.
Generaliz-ing Case Frames using a Thesaurus andthe MDL Principle.
Computational Lin-guistics, 24(2):217-244.Diana McCarthy.
1997.
Word sense dis-ambiguation for acquisition of selectionalpreferences.
In Proceedings of the Pro-ceedings of the A CL/EACL 97 Work-shop Automatic Information Extractionand Building of Lexical Semantic Re-sources for NLP Applications, pages 52-61, Madrid, Spain.Philip Resnik.
1993.
Selection and Informa-tion: A Class-Based Approach to LexicalRelationships.
Ph.D. thesis, University ofPennsylvania.Francesc Ribas.
1995.
On Learning MoreAppropriate Selectional Restrictions.
InProceedings of the Seventh Conference ofthe European Chapter of the Associationfor Computational Linguistics, Dublin,Ireland.David Yarowsky.
1992.
Word-sense disam-biguation using statistical models of Ro-get's categories trained on large corpora.In Proceedings of COLING-92, pages 454-460.265
