NOUN CLASSIFICATION FROM PREDICATE.ARGUMENT STRUCTURESDonald HindleAT&T Bell Laboratories600 Mountain AvenueMurray Hill, NJ 07974ABSTRACTA method of determining the similarity of nounson the basis of a metric derived from the distributionof subject, verb and object in a large text corpus isdescribed.
The resulting quasi-semantic classificationof nouns demonstrates the plausibility of thedistributional hypothesis, and has potentialapplication to a variety of tasks, including automaticindexing, resolving nominal compounds, anddetermining the scope of modification.1.
INTRODUCTIONA variety of linguistic relations apply to sets ofsemantically similar words.
For example, modifiersselect semantically similar nouns, selecfionalrestrictions are expressed in terms of the semanticclass of objects, and semantic type restricts thepossibilities for noun compounding.
Therefore, it isuseful to have a classification of words intosemantically similar sets.
Standard approaches toclassifying nouns, in terms of an "is-a" hierarchy,have proven hard to apply to unrestricted language.Is-a hierarchies are expensive to acquire by hand foranything but highly restricted domains, whileattempts to automatically derive these hierarchiesfrom existing dictionaries have been only partiallysuccessful (Chodorow, Byrd, and Heidom 1985).This paper describes an approach to classifyingEnglish words according to the predicate-argumentstructures they show in a corpus of text.
The generalidea is straightforward: in any natural language thereate restrictions on what words can appear together inthe same construction, and in particular, on what canhe arguments of what predicates.
For nouns, there isa restricted set of verbs that it appears as subject ofor object of.
For example, wine may be drunk,produced, and sold but not pruned.
Each noun maytherefore he characterized according to the verbs thatit occurs with.
Nouns may then he groupedaccording to the extent to which they appear insimilar environments.This basic idea of the distributional foundation ofmeaning is not new.
Hams (1968) makes this"distributional hypothesis" central to his linguistictheory.
His claim is that: "the meaning of entities,and the meaning of grammatical relations amongthem, is related to the restriction of combinations ofthese entities relative to other entities."
(Harris1968:12).
Sparck Jones (1986) takes a similar view.It is however by no means obvious that thedistribution of words will directly provide a usefulsemantic classification, at least in the absence ofconsiderable human intervention.
The work that hasbeen done based on Harris' distributional hypothesis(most notably, the work of the associates of theLinguistic String Project (see for example,Hirschman, Grishman, and Sager 1975))unfortunately does not provide a direct answer, sincethe corpora used have been small (tens of thousandsof words rather than millions) and the analysis hastypically involved considerable intervention by theresearchers.
The stumbling block to any automaticuse of distributional patterns has been that nosufficiently robust syntactic analyzer has beenavailable.This paper reports an investigation of automaticdistributional classification of words in English,using a parser developed for extracting rammaticalstructures from unrestricted text (Hindle 1983).
Wepropose a particular measure of similarity that is afunction of mutual information estimated from text.On the basis of a six million word sample ofAssociated Press news stories, a classification ofnouns was developed according to the predicatesthey occur with.
This purely syntax-based similaritymeasure shows remarkably plausible semanticrelations.2682.
ANALYZING THE CORPUSA 6 million word sample of Associated Pressnews stories was analyzed, one sentence at a time,SBARI / ID N C PROTNS VS PROI I I I I I Ithe land  that  t * susta ins  usCONJNPi ? )
'?
CN Q p D NPLI I I I I Iand many of the products weS?A xvs 7AYx iPROTNS V PRO ThiS VS D NI I I I I I I I* use ?
* are  the resul tFigure 1.
Parser output for a fragment of sentence (1).by a deterministic parser (Fidditch) of the sortoriginated by Marcus (1980).
Fidditch providesa single syntactic analysis -- a tree or sequenceof trees -- for each sentence; Figure 1 shows partof the output for sentence (1).
(1) The clothes we wear, the food we eat, theair we breathe, the water we drink, the land thatsustains us, and many of  the products we use arethe result o f  agricultural research.
(March 221987)The parser aims to be non-committal when it isunsure of an analysis.
For example, it isperfectly willing to parse an embedded clauseand then leave it unattached.
If the object orsubject of a clause is not found, Fidditch leavesit empty, as in the last two clauses in Figure 1.This non-committal pproach simply reduces theeffective size of the sample.The aim of the parser is to produce anannotated surface structure, building constituentsas large as it can, and reconstructing theunderlying clause structure when it can.
Insentence (1), six clauses are found.
Theirpredicate-argument information may be coded asa table of 5-tuples, consisting of verb, surfacesubject, surface object, underlying subject,underlying object, as shown in Table 1.
In thesubject-verb-object table, the root form of thehead of phrases is recorded, and the deep subjectand object are used when available.
(Nounphrases of the form a nl of  n2 are coded as nln2; an example is the first entry in Table 2).269Table 1.
Predicate-argument relations foundin an AP news sentence (1).verb subject objectsurface deep surface deepwear weeat webreathe wedrink wesustain Otraceuse webe landlandOtrace foodOtrace airOtrace waterusresultThe parser's analysis of sentence (1) is farfrom perfect: the object of wear is not found, theobject of use is not found, and the single elementland rather than the conjunction of clothes, food,air, water, land, products is taken to be thesubject of be.
Despite these errors, the analysisis succeeds in discovering a number of thecorrect predicate-argument relations.
Theparsing errors that do occur seem to result, forthe current purposes, in the omission ofpredicate-argument relations, rather than theirmisidentification.
This makes the sample lesseffective than it might be, but it is not in generalmisleading.
(It may also skew the sample to theextent hat the parsing errors are consistent.
)The analysis of the 6 million word 1987 APsample yields 4789 verbs in 274613 clausalstructures, and 267zt2 head nouns.
This table ofpredicate-argument relations is the basis of oursimilarity metric.3.
TYP ICAL  ARGUMENTSFor any of verb in the sample, we can askwhat nouns it has as subjects or objects.
Table 2shows the objects of the verb drink that occur(more than once) in the sample, in effect givingthe answer to the question "what can you drink?
"Table 2.
Objects of the verb drink.OBJECT COUNT WEIGHTbunch beer 2 12.34tea 4 11.75Pepsi 2 11.75champagne 4 11.75liquid 2 10.53beer 5 10.20wine 2 9.34water 7 7.65anything 3 5.15much 3 2.54it 3 1.25<SOME AMOUNT> 2 1.22This list of drinkable things is intuitivelyquite good.
The objects in Table 2 are rankednot by raw frequency, but by a cooccurrencescore listed in the last column.
The idea is that,in ranking the importance of noun-verbassociations, we are interested not in the rawfrequency of cooccurrence of a predicate andargument, but in their frequency normalized bywhat we would expect.
More is to be learnedfrom the fact that you can drink wine than fromthe fact that you can drink it even though thereare more clauses in our sample with # as anobject of drink than with wine.
To capture thisintuition, we turn, following Church and Hanks(1989), to "mutual information" (see Fano 1961).The mutual information of two events l(x y)is defined as follows:P(x y) l (xy )  = log2 P(x) P(y)where P(x y) is the joint probability of events xand y, and P(x) and P(y) axe the respectiveindependent probabilities.
When the jointprobability P(x y) is high relative to the productof the independent probabilities, I is positive;when the joint probability is relatively low, I isnegative.
We use the observed frequencies toderive a cooccurrence score Cobj (an estimate ofmutual information) defined as follows.270/ ( .
v)NC~,j(n v) = log2 /(n) /(v)N Nwhere fin v) is the frequency of noun n occurringas object of verb v, f(n) is the frequency of thenoun n occurring as argument of any verb, f(v) isthe frequency of the verb v, and N is the countof clauses in the sample.
(C,,,bi(n v) is definedanalogously.
)Calculating the cooccurrence weight fordrink, shown in the third column of Table 2,gives us a reasonable tanking of terms, with itnear the bottom.Multiple RelationshipsFor any two nouns in the sample, we can askwhat verb contexts they share.
The distributionalhypothesis is that nouns axe similar to the extentthat they share contexts.
For example, Table 3shows all the verbs which wine and beer can beobjects of, highlighting the three verbs they havein common.
The verb drink is the key commonfactor.
There are of course many other objectsthat can be sold, but most of them are less alikethan wine or beer because they can't also bedrunk.
So for example, a car is an object thatyou can have and sell, like wine and beer, butyou do not -- in this sample (confirming what weknow from the meanings of the words) --typically drink a car.4.
NOUN S IMILARITYWe propose the following metric ofsimilarity, based on the mutual information ofverbs and arguments.
Each noun has a set ofverbs that it occurs with (either as subject orobject), and for each such relationship, there is amutual information value.
For each noun andverb pair, we get two mutual information values,for subject and object,Csubj(Vi nj) and Cobj(1Ji nj)We define the object similarity of two nounswith respect o a verb in terms of the minimumshared coocccurrence weights, as in (2).The subject similarity of two nouns, SIMs~j,is defined analogously.Now define the overall similarity of twonouns as the sum across all verbs of the objectsimilarity and the subject similarity, as in (3).
(2) Object similarity.SIMobj(vinjnt) =min(Cobj(vinj) Cobj(vln,)), ff Coni(vinj) > 0 andabs (m~x(Cobj(vinj) , Cobj(Vink))), if Cobj(vinj) < 0O, otherwiseCobj(vi,,) > 0and Cobj(vin,) < 0(3) Noun similarity.NSIM(ntn2) = ~'.i=0SIM~a,i(vinln2) + SIMobj(vinln2)The metric of similarity in (2) and (3) is butone of many that might be explored, but it hassome useful properties.
Unlike an inner productmeasure, it is guaranteed that a noun will bemost similar to itself.
And unlike cosinedistance, this metric is roughly proportional tothe number of different verb contexts that areshared by two nouns.Using the definition of similarity in (3), wecan begin to explore nouns that show thegreatest similarity.
Table 4 shows the ten nounsmost similar to boat, according to our similaritymetric.
The first column lists the noun which issimilar to boat.
The second column in eachtable shows the number of instances that thenoun appears in a predicate-argument pair(including verb environments not in the list inthe fifth column).
The third column is thenumber of distinct verb environments (eithersubject or object) that the noun occurs in whichare shared with the target noun of the table.Thus, boat is found in 79 verb environment.
Ofthese, ship shares 25 common environments(ship also occurs in many other unsharedenvironments).
The fourth column is themeasure of similarity of the noun with the targetnoun of the table, SIM(nln2), as defined above.The fifth column shows the common verbenvironments, ordered by cooccurrence score,C(vinj), as defined above.
An underscorebefore the verb indicates that it is a subjectenvironment; a following underscore indicates anobject environment.
In Table 4, we see that boatis a subject of cruise, and object of sink.
In thelist for boat, in column five, cruise appearsearlier in the list than carry because cruise has ahigher cooccurrence score.
A - before a verbmeans that the cooccurrence score is negative --i.e.
the noun is less likely to occur in thatargument context han expected.For many nouns, encouragingly appropriatesets of semantically similar nouns are found.Thus, of the ten nouns most similar to boat(Table 4), nine are words for vehicles; the mostTable 3.
Verbs taking wine and beer as objects.VERB wine beercount weight count weightdrug 2 12.26sit around l 10.29smell 1 10.07contaminate 1 9.75rest 2 9.56drink 2 9.34 5 10.20rescue 1 7.07purchase 1 6.79lift 1 6.72prohibit 1 6.69love l 6.33deliver 1 5.82buy 3 5.44name 1 5.42keep 2 4.86offer 1 4.13begin 1 4.09allow I 3.90be on 1 3.79sell I 4.21 1 3.75's 2 2.84make 1 1.27have 1 0.84 2 1.38similar noun is the near-synonym ship.
The tennouns most similar to treaty (agreement, plan,constitution, contract, proposal, accord,amendment, rule, law, legislation) seem to makeup a duster involving the notions of agreementand rule.
Table 5 shows the ten nouns mostsimilar to legislator, again a fairly coherent set.Of course, not all nouns fall into such neatclusters: Table 6 shows a quite heterogeneousgroup of nouns similar to table, though evenhere the most similar word (floor) is plausible.We need, in further work, to explore bothautomatic and supervised means ofdiscriminating the semantically relevantassociations from the spurious.271Table 4.
Nouns similar to boat.Noun ~n) verbs SIMboat 153 79 370.16ship 353 25 79.02plane 445 26 68.85bus 104 20 64.49jet 153 17 62.77vessel 172 18 57.14truck 146 21 56.71car 414 9_,4 52.22helicopter 151 14 50.66ferry 37 10 39.76man 1396 30 38.31Verbs_cruise, keel_, _plow, sink_, drift_, step off_, step from_, dock_,righ L, submerge , near, hoist , intercept, charter, stay on_,buzz_, stabilize_, _sit on, intercept, hijack_, park_, _be from,rock,  get off_, board,  miss_, stay with_, catch,  yield-, bring in_,seize_, pull_, grab , hit, exclude_, weigh_, _issue, demonstrate,_force, _cover, supply_, _name, attack, damage_, launch_,_provide, appear , carry, _go to, look a L, attack_, _reach, _be on,watch_, use_, return_, _ask, destroy_, f i re ,  be on_, describe_,charge_, include_, be in_, report_, identify_, expec L, cause , 's ,'s, take, _make, "be_,-say, "give_, see ," be, "have_, "get_near, charter, hijack_, get off_, buzz_, intercept, board_,damage, sink_, seize, _carry, attack_, "have_, _be on, _hit,destroy_, watch_, _go to, "give , ask, "be_, be on_, "say_,identify, see_hijack_, intercept_, charter, board_, get o f f ,  _near, _attack,_carry, seize_, -have_, _be on, _catch, destroy_, _hit, be on_,damage_, use_, -be_, _go to, _reach, "say_, identify_, _provide,expect,  cause-, see-step off_., hijack_, park_, get o f f ,  board , catch, seize-, _carry,attack_, _be on, be on_, charge_, expect_, "have , take, "say_,_make, include_, be in , " becharter, intercept, hijack_, park_, board , hit, seize-, _attack,_force, carry,  use_, describe_, include , be on, "_be, _make,-say_right-, dock ,  intercept, sink_, seize , catch, _attack, _carry,attack_, "have_, describe_, identify_, use_, report_, "be_, "say_,expec L, "give_park_, intercept-, stay with_, _be from, _hit, seize,  damage_,_carry, teach,  use_, return_, destroy_, attack , " be, be in , take,-have_, -say_, _make, include_, see_step from_, park_, board , hit, _catch, pull , carry, damage_,destroy_, watch_, miss_, return_, "give_, "be , - be, be in_, -have_,-say_, charge_, _'s, identify_, see , take, -get_hijack_, park_, board_, bring in , catch, _attack, watch_, use_,return_, fire_, _be on, include , make, -_bedock_, sink_, board-, pull_, _carry, use_, be on_, cause , take,"say_hoist_, bring in_, stay with_, _attack, grab,  exclude , catch,charge_, -have_, identify_, describe_, "give , be from, appear_,_go to, carry,  _reach, _take, pull_, h i t ,  -get , 's , attack_, cause_,_make, "_be, see , cover, _name, _ask272Table 5.
Nouns simliar to legislator.Noun fin) verbs SIMlegislator 45 35 165.85Senate 366 11 40.19commit~e 697 20 39.97organization 351 16 34.29commission 389 17 34.28legislature 86 12 34.12delega~ 132 13 33.65lawmaker 176 14 32.78panel 253 12 31.23Congress 827 15 31.20side 327 15 30.00Table 6.
Nouns similar to table.Noun f(n) verbs SIMtable 66 30 181.43floor 94 6 30.01farm 80 8 22.94scene 135 10 20.85America 156 7 19.68experience 129 5 19.04river 95 4 18.73town 195 6 18.68side 327 8 18.57hospital 190 7 18.10House 453 6 17.84Verbscajole , thump, _grasp, convince_, inform_, address , vote,_predict, _address, _withdraw, _adopt, _approve, criticize_,_criticize, represent, _reach, write , reject, _accuse, support_, goto_, _consider, _win, pay_, allow_, tell , hold, call__, _kill, _call,give_, _get, say , take, "__be_vote, address_, _approve, inform_, _reject, go to_, _consider,adopt, tell , - be, give__vote, _approve, go to_, inform_, _reject, tell , " be, convince_,_hold, address_, _consider, _address, _adopt, call_, criticize,allow_, support_, _accuse, give_, _calladopt, inform_, address, go to_, _predict, support_, _reject,represent_, call, _approve, -_be, allow , take, say_, _hold, tell__reject, _vote, criticize_, convince-, inform_, allow , accuse,_address, _adopt, "_be, _hold, _approve, give_, go to_, tell_,_consider, pay_convince_, approve, criticize_, _vote, _address, _hold, _consider,"_.be, call_, g ive,  say_, _take-vote, inform_, _approve, _adopt, allow_, _reject, _consider,_reach, tell_, give , " be, call, say_-criticize, _approve, _vote, _predict, tell , reject, _accuse, "__be,call_, give , consider, _win, _get, _take_vote, approve, convince_, tell , reject, _adopt, _criticize,_.consider, "__be, _hold, g ive,  _reachinform_, _approve, _vote, tell_, _consider, convince_, go to , " be,address_, give_, criticize_, address, _reach, _adopt, _holdreach,  _predict, criticize , withdraw, _consider, go to , hold,-_be, _accuse, support_, represent_, tell_, give_, allow , takeVerbshide beneath_, convolute_, memorize_, sit a t ,  sit across_, redo_,structure_, sit around_, fitter, _carry, lie on_, go from_, ho ld ,wait_, come to ,  return to ,  turn_, approach_, cover,  be on-,share, publish_, claim_, mean_, go to ,  raise_, leave_, "have_,do , belitter, lie on-, cover,  be on-, come to_, go to__carry, be on-, cover,  return to_, turn_, go to._, leave_, "have_approach_, retum to_, mean_, go to ,  be on-, turn_, come to_,leave_, do_, be_go from_, come to_, return to_, claim_, go to_, "have_, do_structure_, share_, claim_, publish_, be_sit across_, mean_, be on-, leave_litter,, approach_, go to_, return to_, come to_, leave_lie on_, be on-, go to_, _hold, "have_, cover,  leave._, come to_go from_, come to_, cover,  return to_, go to_, leave_, "have_return to_, claim_, come to_, go to_, cover_, leave_273Reciprocally most similar nounsWe can define "reciprocally most similar"nouns or "reciprocal nearest neighbors" (RNN)as two nouns which are each other's mostsimilar noun.
This is a rather stringentdefinition; under this definition, boat and ship donot qualify because, while ship is the mostsimilar to boat, the word most similar to ship isnot boat but plane (boat is second).
For asample of all the 319 nouns of frequency greaterthan 100 and less than 200, we asked whethereach has a reciprocally most similar noun in thesample.
For this sample, 36 had a reciprocalnearest neighbor.
These are shown in Table 7(duplicates are shown only once).Table 7.
A sample of reciprocally nearestneighbors.RNN word countsbomb device (192 101)ruling - decision (192 761)street road (188 145)protest strike (187 254)list fieM (184 104)debt deficit (183 351)guerrilla rebel (180 314)fear concern (176 355)higher lower (175 78)freedom right (164 609)battle fight (163 131)jet plane (153 445)shot bullet (152 35)truck car (146 414)researcher scientist (142 112)peace stability (133 64)property land (132 119)star editor (131 85)trend pattern (126 58)quake earthquake (126 120)economist analyst (120 318)remark comment (115 385)data information (115 505)explosion blast (115 52)tie relation (114 251)protester demonstrator (110 99)college school (109 380)radio IRNA (107 18)2 3 (105 90)The list in Table 7 shows quite a good set ofsubstitutable words, many of which axe neatsynonyms.
Some are not synonyms but are274nevertheless closely related: economist - analyst,2 - 3.
Some we recognize as synonyms in newsreporting style: explosion - blast, bomb - device,tie - relation.
And some are hard to interpret.
Isthe close relation between star and editor somereflection of news reporters' world view?
Is listmost like fieM because neither one has muchmeaning by itself?.5.
D ISCUSSIONUsing a similarity metric derived from thedistribution of subjects, verbs and objects in acorpus of English text, we have shown theplausibility of deriving semantic relatedness fromthe distribution of syntactic forms.
Thisdemonstration has depended on: 1) theavailability of relatively large text corpora; 2) theexistence of parsing technology that, despite alarge error rate, allows us to find the relevantsyntactic relations in unrestricted text; and 3)(most important) the fact that the lexicalrelations involved in the distribution of words insyntactic structures are an extremely stronglinguistic constraint.A number of issues will have to beconfronted to further exploit these structurally-mediated lexical constraints, including:Po/ysemy.
The analysis presented here doesnot distinguish among related senses of the(orthographically) same word.
Thus, in the tableof words similar to table, we find at least twodistinct senses of table conflated; the table onecan hide beneath is not the table that can becommuted or memorized.
Means of separatingsenses need to be developed.Empty words.
Not all nouns are equallycontentful.
For example, section is a generalword that can refer to sections of all sorts ofthings.
As a result, the ten words most similarto section (school, building, exchange, book,house, ship, some, headquarter, industry., office)are a semantically diverse list of words.
Thereason is clear: section is semantically a ratherempty word, and the selectional restrictions onits cooccurence depend primarily on itscomplement.
You might read a section of abook but not, typically, a section of a house.
Itwould be possible to predetermine a set of emptywords in advance of analysis, and thus avoidsome of the problem presented by empty words.But it is unlikely that the class is well-defined.Rather, we expect hat nouns could be ranked, onthe basis of their distribution, according to howempty they are; this is a matter for furtherexploration.Sample size.
The current sample is toosmall; many words occur too infrequently to beadequately sampled, and it is easy to think ofusages that are not represented in the sample.For example, it is quite expected to talk aboutbrewing beer, but the pair of brew and beer doesnot appear in this sample.
Part of the reason formissing selectional pairs is surely the restrictednature of the AP news sublanguage.Further analysis.
The similarity metricproposed here, based on subject-verb-objectrelations, represents a considerable reduction inthe information available in the subjec-verb-object table.
This reduction is useful in that itpermits, for example, a clustering analysis of thenouns in the sample, and for some purposes(such as demonstrating the plausibility of thedistribution-based metric) such clustering isuseful.
However, it is worth noting that theparticular information about, for example, whichnouns may be objects of a given verb, should notbe discarded, and is in itself useful for analysisof text.In this study, we have looked only at thelexical relationship between a verb and the headnouns of its subject and object.
Obviously, thereare many other relationships among words -- forexample, adjectival modification or thepossibility of particular prepositional djuncts --that can be extracted from a corpus and thatcontribute to our lexical knowledge.
It will beuseful to extend the analysis presented here toother kinds of relationships, including morecomplex kinds of verb complementation, nouncomplementation, and modification bothpreceding and following the head noun.
But inexpanding the number of different structuralrelations noted, it may become less useful tocompute a single-dimensional similarity score ofthe sort proposed in Section ,1.
Rather, thevarious lexical relations revealed by parsing acorpus, will be available to be combined in manydifferent ways yet to he explored.REFERENCESChodorow, Martin S., Roy J. Byrd, and GeorgeE.
Heidom.
1985.
Extracting semantichierarchies from a large on-line dictionary.Proceedings of the 23rd Annual Meetingof the ACL, 299-304.Church, Kenneth.
1988.
A stochastic partsprogram and noun phrase parser forunrestricted text.
Proceedings of the secondACL Conference on Applied NaturalLanguage Processing.Church, Kenneth and Patrick Hanks.
1989.
Wordassociation orms, mutual information andlexicography.
Proceedings of the 23rdAnnual Meeting of the ACL, 76-83.Fano, R. 1961.
Transmission of Information.Cambridge, Mass:MIT Press.Harris, Zelig S. 1968.
Mathematical Structures ofLanguage.
New York: Wiley.Hindle, Donald.
1983.
User manual for Fidditch.Naval Research Laboratory TechnicalMemorandum #7590-142.Hirschman, Lynette.
1985.
Discoveringsublanguage structures, in Grishman, Ralphand Richard Kittredge, eds.
AnalyzingLanguage in Restricted Domains, 211-234.Lawrence Erlbaum: Hillsdale, NJ.Hirschman, Lynette, Ralph Grishman, and NaomiSager.
1975.
Grammatically-basedautomatic word class formation.Information Processing and Management,11, 39-57.Marcus, Mitchell P. 1980.
A Theory of SyntacticRecognition for Natural Language.
MITPress.Sparck Jones, Karen.
1986.
Synomyny andSemantic Classification.
EdinburghUniversity Press.275
