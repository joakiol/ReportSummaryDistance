Proceedings of NAACL HLT 2009: Short Papers, pages 113?116,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsContrastive Summarization: An Experiment with Consumer ReviewsKevin LermanColumbia UniversityNew York, NYklerman@cs.columbia.eduRyan McDonaldGoogle Inc.New York, NYryanmcd@google.comAbstractContrastive summarization is the problem ofjointly generating summaries for two entitiesin order to highlight their differences.
In thispaper we present an investigation into con-trastive summarization through an implemen-tation and evaluation of a contrastive opinionsummarizer in the consumer reviews domain.1 IntroductionAutomatic summarization has historically focusedon summarizing events, a task embodied in theseries of Document Understanding Conferences1.However, there has also been work on entity-centricsummarization, which aims to produce summariesfrom text collections that are relevant to a particu-lar entity of interest, e.g., product, person, company,etc.
A well-known example of this is from the opin-ion mining community where there has been a num-ber of studies on summarizing the expressed senti-ment towards entities (cf.
Hu and Liu (2006)).
An-other recent example of entity-centric summariza-tion is the work of Filippova et al (2009) to producecompany-specific financial report summaries.In this study we investigate a variation of entity-centric summarization where the goal is not to sum-marize information about a single entity, but pairsof entities.
Specifically, our aim is to jointly gen-erate two summaries that highlight differences be-tween the entities ?
a task we call contrastive sum-marization.
An obvious application comes from theconsumer reviews domain, where a person consider-ing a purchase wishes to see the differences in opin-ion about the top candidates without reading all thereviews for each product.
Other applications include1http://duc.nist.gov/contrasting financial news about related companiesor comparing platforms of political candidates.Contrastive summarization has many points ofcomparison in the NLP, IR and Data-Mining liter-ature.
Jindal and Liu (2006) introduce techniquesto find and analyze explicit comparison sentences,but this assumes that such sentences exist.
In con-trastive summarization, there is no assumption thattwo entities have been explicitly compared.
Thegoal is to automatically generate the comparisonsbased on the data.
In the IR community, Sun etal.
(2006) explores retrieval systems that align queryresults to highlight points of commonality and dif-ference.
In contrast, we attempt to identify con-trasts from the data, and then generate summariesthat highlight them.
The novelty detection task ofdetermining whether a new text in a collection con-tains information distinct from that already gatheredis also related (Soboroff and Harman, 2005).
Theprimary difference here is that contrastive summa-rization aims to extract information from one col-lection not present in the other in addition to infor-mation present in both collections that highlights adifference between the entities.This paper describes a contrastive summarizationexperiment where the goal is to generate contrastingopinion summaries of two products based on con-sumer reviews of each.
We look at model designchoices, describe an implementation of a contrastivesummarizer, and provide an evaluation demonstrat-ing a significant improvement in the usefulness ofcontrastive summaries versus summaries generatedby single-product opinion summarizers.2 Single-Product Opinion SummarizationAs input we assume a set of relevant text excerpts(typically sentences), T = {t1, .
.
.
, tm}, which con-113tain opinions about some product of interest.
Thegoal of opinion summarization2 is to select somenumber of text excerpts to form a summary S ofthe product so that S is representative of the aver-age opinion and speaks to its important aspects (alsoproportional to opinion), which we can formalize as:S = argmaxS?TL(S) s.t.
LENGTH(S) ?
Kwhere L is some score over possible summaries thatembodies what a user might desire in an opinionsummary, LENGTH(S) is the length of the summaryand K is a pre-specified length constraint.We assume the existence of standard sentimentanalysis tools to provide the information used in thescoring function L. First, we assume the tools canassign a sentiment score from -1 (negative) to 1 (pos-itive) to an arbitrary span of text.
Second, we as-sume that we can extract a set of aspects that the textis discussing (e.g, ?The sound was crystal clear?
isabout the aspect sound quality).
We refer the readerto abundance of literature on sentiment analysis formore details on how such tools can be constructed(cf.
Pang and Lee (2008)).
For this study, we usethe tools described and evaluated in Lerman et al(2009).
We note however, that the subject of thisdiscussion is not the tools themselves, but their use.The single product opinion summarizer we con-sider is the Sentiment Aspect Match model (SAM)described and evaluated in (Lerman et al, 2009).Underlying SAM is the assumption that opinionscan be described by a bag-of-aspects generative pro-cess where each aspect is generated independentlyand the sentiment associated with the aspect is gen-erated conditioned on its identity,p(t) = ?a?Atp(a)p(SENT(at)|a)where At is a set of aspects that are mentioned intext excerpt t, p(a) is the probability of seeing aspecta, and SENT(at) ?
[?1, 1] is the sentiment associ-ated with aspect a in t. The SAM model sets p(a)through the maximum likelihood estimates over Tand assumes p(SENT(at)|a) is normally distributedwith a mean and variance also estimated from T .
We2We focus on text-only opinion summaries as opposed tothose based on numeric ratings (Hu and Liu, 2006).denote SAM(T ) as the model learned using the entireset of candidate text excerpts T .The SAM summarizer scores each potential sum-mary, S, by learning another model SAM(S) basedon the text excerpts used to construct S. We can thenmeasure the distance between a model learned overthe full set T and a summary S by summing the KL-divergence between their learned probability distri-butions.
In our case we have 1 + |AT | distributions?
p(a), and p(?|a) for all a ?
AT .
We then define L:L(S) = ?KL(SAM(T ), SAM(S))That is, the SAM summarizer prefers summarieswhose induced model is close to the model inducedfor all the opinions about the product of interest.Thus, a good summary should (1) mention aspects inroughly the same proportion that they are mentionedin the full set of opinions and (2) mention aspectswith sentiment also in proportion to what is observedin the full opinion set.
A high scoring summary isfound by initializing a summary with random sen-tences and hill-climbing by replacing sentences oneat a time until convergence.We chose to use the SAM model for our exper-iment for two reasons.
First, Lerman et al (2009)showed that among a set of different opinion sum-marizers, SAM was rated highest in a user study.Secondly, as we will show in the next section, theSAM summarization model can be naturally ex-tended to produce contrastive summaries.3 Constrastive SummarizationWhen jointly generating pairs of summaries, we at-tempt to highlight differences between two products.These differences can take multiple forms.
Clearly,two products can have different prevailing sentimentscores with respect to an aspect (e.g.
?Product X hasgreat image quality?
vs ?Product Y?s image qualityis terrible?).
Reviews of different products can alsoemphasize different aspects.
Perhaps one product?sscreen is particularly good or bad, but another?s isnot particularly noteworthy ?
or perhaps the otherproduct simply doesn?t have a screen.
Regardless ofsentiment, reviews of the first product will empha-size the screen quality aspect more than those of thesecond, indicating that our summary should as well.114Tx TySx SyTx TySx SyTx TySx Sy(a) (b) (c)Figure 1: (a) Non-joint model: Generates summaries fortwo products independently.
(b) Joint model: Summariesattempt to look like text they are drawn from, but contrasteach-other.
(c) Joint model: Like (b), except summariescontrast text that the other summary is drawn from.As input to our contrastive summarizer we assumetwo products, call them x and y as well as two corre-sponding candidate sets of opinions, Tx and Ty, re-spectively.
As output, a contrastive summarizer willproduce two summaries ?
Sx for product x and Syfor product y ?
so that the summaries highlight thedifferences in opinion between the two products.What might a contrastive summarizer look like ona high-level?
Figure 1 presents some options.
Thefirst example (1a) shows a system where each sum-mary is generated independently, i.e., running theSAM model on each product separately without re-gard to the other.
This procedure may provide someuseful contrastive information, but any such infor-mation will be present incidentally.
To make thesummaries specifically contrast each other, we canmodify our system by explicitly modeling the factthat we want summaries Sx and Sy to contrast.
Inthe SAM model this is trivial as we can simply add aterm to the scoring function L that attempts to maxi-mize the KL-divergence between the two summariesinduced models SAM(Sx) and SAM(Sy).This approach is graphically depicted in figure 1b,where the system attempts to produce summariesthat are maximally similar to the opinion set they aredrawn from and minimally similar from each other.However, some obvious degenerate solutions ariseif we chose to model our system this way.
Considertwo products, x and y, for which all opinions dis-cuss two aspects a and b with identical frequencyand sentiment polarity.
Furthermore, several opin-ions of x and y discuss an aspect c, but with oppo-site sentiment polarity.
Suppose we have to buildcontrastive summaries and only have enough spaceto cover a single aspect.
The highest scoring con-trastive pair of summaries would consist of one for xthat mentions a exclusively, and one for y that men-tions b exclusively ?
these summaries each mentiona promiment aspect of their product, and have nooverlap with each other.
However, they provide afalse contrast because they each attempt to contrastthe other summary, rather than the other product.Better would be for both to cover aspect c.To remedy this, we reward summaries that in-stead have a high KL-divergence with respect to theother product?s full model SAM(T ) as depicted inFigure 1c.
Under this setup, the degenerate solutiondescribed above is no longer appealing, as both sum-maries have the same KL-divergence with respect tothe other product as they do to their own product.The fact that the summaries themselves are dissim-ilar is irrelevant.
Comparing the summaries only tothe products?
full language models prevents us fromrewarding summaries that convey a false contrast be-tween the products under comparison.
Specifically,we now optimize the following joint summary score:L(Sx, Sy) = ?KL(SAM(Tx), SAM(Sx))?KL(SAM(Ty), SAM(Sy))+KL(SAM(Tx), SAM(Sy))+KL(SAM(Ty), SAM(Sx))Note that we could additionally model divergencebetween the two summaries (i.e., merging models infigures 1b and c), but such modeling is redundant.Furthermore, by not explicitly modeling divergencebetween the two summaries we simplify the searchspace as each summary can be constructed withoutknowledge of the content of the second summary.4 The ExperimentOur experiments focused on consumer electronics.In this setting an entity to be summarized is one spe-cific product and T is a set of segmented user re-views about that product.
We gathered reviews for56 electronics products from several sources such asCNet, Epinions, and PriceGrabber.
The productscovered 15 categories of electronics products, in-cluding MP3 players, digital cameras, laptops, GPSsystems, and more.
Each had at least four reviews,and the mean number of reviews per product was 70.We manually grouped the products into cate-gories (MP3 players, cameras, printers, GPS sys-115System As Received ConsolidatedSAM 1.85 ?
0.05 1.82 ?
0.05SAM + contrastive 1.76 ?
0.05 1.68 ?
0.05Table 1: Mean rater scores for contrastive summaries bysystem.
Scores range from 0-3 and lower is better.tems, headphones, computers, and others), and gen-erated contrastive summaries for each pair of prod-ucts in the same category using 2 different algo-rithms: (1) The SAM algorithm for each product in-dividually (figure 1a) and (2) The SAM algorithmwith our adaptation for contrastive summarization(figure 1c).
Summaries were generated using K =650, which typically consisted of 4 text excerpts ofroughly 160 characters.
This allowed us to comparedifferent summaries without worrying about the ef-fects of summary length on the ratings.
In all, wegathered 178 contrastive summaries (89 per system)to be evaluated by raters and each summary wasevaluated by 3 random raters resulting in 534 rat-ings.
The raters were 55 everyday internet usersthat signed-up for the experiment and were assignedroughly 10 random ratings each.
Raters were showntwo products and their contrastive summaries, andwere asked to list 1-3 differences between the prod-ucts as seen in the two summaries.
They were alsoasked to read the products?
reviews to help ensurethat the differences observed were not simply arti-facts of the summarizer but in fact are reflected inactual opinions.
Finally, raters were asked to ratethe helpfulness of the summaries in identifying thesedistinctions, rating each with an integer score from0 (?extremely useful?)
to 3 (?not useful?
).Upon examining the results, we found that ratershad a hard time finding a meaningful distinction be-tween the two middle ratings of 1 and 2 (?useful?and ?somewhat useful?).
We therefore present twosets of results: one with the scores as received fromraters, and another with all 1 and 2 votes consol-idated into a single class of votes with numericalscore 1.5.
Table 1 gives the average scores per sys-tem, lower scores indicating superior performance.5 Analysis and ConclusionsThe scores indicate that the addition of the con-trastive term to the SAM model improves helpful-ness, however both models roughly have averageSystem 2+ raters All 3 ratersSAM 0.8 0.2SAM + contrastive 2.0 0.6Table 2: Average number of points of contrast per com-parison observed by multiple raters, by system.
Raterswere asked to list up to 3.
Higher is better.scores in the somewhat-useful to useful range.
Thedifference becomes more pronounced when look-ing at the consolidated scores.
The natural questionarises: does the relatively small increase in helpful-ness reflect that the contrastive summarizer is doinga poor job?
Or does it indicate that users only findslightly more utility in contrastive information inthis domain?
We inspected comments left by ratersin an attempt to answer this.
Roughly 80% of raterswere able to find at least two points of contrast insummaries generated by the SAM+contrastive ver-sus 40% for summaries generated by the simpleSAM model.
We then examined the consistencyof rater comments, i.e., to what degree did differ-ent raters identify the same points of contrast from aspecific comparison?
We report the results in table 2.Note that by this metric in particular, the contrastivesummarizer outperforms its the single-product sum-marizer by significant margins and provides a strongargument that the contrastive model is doing its job.Acknowledgements: The Google sentiment analy-sis team for insightful discussions and suggestions.ReferencesK.
Filippova, M. Surdeanu, M. Ciaramita, andH.
Zaragoza.
2009.
Company-oriented extractivesummarization of financial news.
In Proc.
EACL.M.
Hu and B. Liu.
2006.
Opinion extraction and sum-marization on the web.
In Proc.
AAAI.N.
Jindal and B. Liu.
2006.
Mining comparative sen-tences and relations.
In Proc.
AAAI.Kevin Lerman, Sasha Blair-Goldensohn, and Ryan Mc-Donald.
2009.
Sentiment summarization: Evaluatingand learning user preferences.
In Proc.
EACL.B.
Pang and L. Lee.
2008.
Opinion mining and sentimentanalysis.
Now Publishers.I.
Soboroff and D. Harman.
2005.
Novelty detection:The TREC experience.
In Proc.
HLT/EMNLP.Sun, Wang, Shen, Zeng, and Chen.
2006.
CWS: A Com-parative Web search System.
In Proc.
WWW.116
