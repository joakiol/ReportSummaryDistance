Chinese Word Segmentation with Multiple Postprocessorsin HIT-IRLabHuipeng Zhang     Ting Liu    Jinshan Ma    Xiantao LiaoInformation Retrieval Lab, Harbin Institute of Technology, Harbin, 150001 CHINA{zhp,tliu,mjs,taozi}@ir.hit.edu.cnAbstractThis paper presents the results of thesystem IRLAS1 from HIT-IRLab in theSecond International Chinese WordSegmentation Bakeoff.
IRLAS consistsof several basic components and multi-ple postprocessors.
The basic compo-nents include basic segmentation,factoid recognition, and named entityrecognition.
These components main-tain a segment graph together.
Thepostprocessors include merging of ad-joining words, morphologically derivedword recognition, and new word identi-fication.
These postprocessors do somemodifications on the best word se-quence which is selected from the seg-ment graph.
Our system participated inthe open and closed tracks of PK cor-pus and ranked #4 and #3 respectively.Our scores were very close to the high-est level.
It proves that our system hasreached the current state of the art.1 IntroductionIRLAS participated in both the open and closedtracks of PK corpus.
The sections below descriptin detail the components of our system and thetracks we participated in.The structure of this paper is as follows.
Sec-tion 2 presents the system description.
Section 3describes in detail the tracks we participated in.Section 4 gives some experiments and discus-sions.
Section 5 enumerates some external fac-1 IRLAS is the abbreviation for ?Information RetrievalLab Lexical Analysis System?.tors that affect our performance.
Section 6 givesour conclusion.2 System Description2.1 Basic SegmentationWhen a line is input into the system, it is firstsplit into sentences separated by period.
Thereason to split a line into sentences is that innamed entity recognition, the processing of sev-eral shorter sentences can reach a higher namedentity recall rate than that of a long sentence.The reason to split the line only by period is forthe simplicity for programming, and the sen-tences separated by period are short enough toprocess.Then every sentence is segmented into singleatoms.
For example, a sentence like ?HIT-IRLab??????
SIGHAN ??????
will besegmented as ?HIT-IRLab/?/?/?/?/?/?/SIGHAN/?/?/?/?/?
?.After atom segmentation, a segment graph iscreated.
The number of nodes in the graph is thenumber of atoms plus 1, and every atom corre-sponds to an arc in the graph.Then all the words in the dictionary2 that ap-pear in the sentence will be added to the seg-ment graph.
The graph contains variousinformation such as the bigram possibility ofevery word.
Figure 1 shows the segment graphof the above sentence after basic segmentation.2.2 Factoid RecognitionAfter basic segmentation, a graph with all theatoms and all the words in the dictionary is setup.
On this basis, we find out all the factoids2 The dictionary is trained with training corpus.172Figure 1: The segment graphNote: the probability of each word is not shown in the graph.such as numbers, times and e-mails with a set ofrules.
Then, we also add all these factoids to thesegment graph.2.3 Named Entity RecognitionThen we will recognize the named entities suchas persons and locations.
First, we select N3 bestpaths from the segment graph with Dijkstra al-gorithm.
Then for every path of the N+1 paths4(N best paths and the atom path), we perform aprocess of Roles Tagging with HMM model(Zhang et al 2003).
The process of it is muchlike that of Part of Speech Tagging.
Then withthe best role sequence of every path, we can findout all the named entities and add them to thesegment graph as usual.
Take the sentence ???????????
for example.
After basicsegmentation and factoid recognition, the N+1paths are as follows:?/?/?/?/?/?/??/?
?/?/?/?/?/?/?/?/?Then for each path, the process of RolesTagging is performed and the following rolesequences are generated:X/S/W/N/O/O/O/O5X/S/W/N/O/O/O/O/OFrom these role sequences, we can find outthat ?XSW?
is a 3-character Chinese name.
Sothe word ?????
is recognized as a personname and be added to the segment graph.3 N is a constant which is 8 in our system.4 It may be smaller than N+1 if the sentence is shortenough; exactly, N+1 is the upper bound of the path num-ber.5 X, S, W, N and O are all roles for person name recogni-tion, X is surname, S is the first character of given name,W is the second character of given name, N is the wordfollowing a person name, and O is other remote context.We defined 17 roles for person name recognition and 10roles for location name recognition.2.4 Merging of Adjoining WordsAfter the steps above, the segment graph iscompleted and a best word sequence is gener-ated with Dijkstra algorithm.
This merging op-eration and all the following operations are doneto the best word sequence.There are many inconsistencies in the PKcorpus.
For example, in PK training corpus, theword ????
sometimes is considered as oneword, but sometimes is considered as two sepa-rate words as ??
??.
The inconsistencieslower the system?s performance to some extent.To solve this problem, we first train from thetraining corpus the probability of a word to beone word and the probability to be two separatewords.
Then we perform a process of merging:if two adjoining words in the best word se-quence are more likely to be one word, then wejust merge them together.2.5 Morphologically Derived Word Recog-nitionTo deal with the words with the postfix like??
?, ??
?, ???
and so on, we perform theprocess to merge the preceding word and thepostfix into one word.
We train a list of post-fixes from the training corpus.
Then we scan thebest word sequence, if there is a single characterword that appears in the postfix list, we mergethe preceding word and this postfix into oneword.
For example, a best word sequence like???
?
?
?
???
will be converted to????
?
?
???
after this operation.2.6 New Word IdentificationAs for the words that are not in the dictionaryand cannot be identified with the steps above,we perform a process of New Word Identifica-tion (NWI).
We train from the training corpusthe probability of a word to be independent andthe probability to be a special part of anotherword.
In our system, we only consider the wordsthat have one or two characters.
Then we scan173the best word sequence, if the product of theprobabilities of two adjoining words exceed athreshold, then we merge the two words into oneword.Take the word ????
for example.
It issegmented as ??
??
after all the above stepssince this word is not in the dictionary.
We findthat the word ???
has a probability of 0.83 tobe the first character of a two character word,and the word ???
has a probability of 0.94 to bethe last character of a two character word.
Theproduct of them is 0.78 which is larger than 0.65,which is the threshold in our system.
So theword ????
is recognized as a single word.3 Tracks3.1 Closed TrackAs for the PK closed track, we first extract allthe common words and tokens from the trainingcorpus and set up a dictionary of 55,335 entries.Then we extract every kind of named entity re-spectively.
With these named entities, we trainparameters for Roles Tagging.
We also train allthe other parameters mentioned in Section 2with the training corpus.3.2 Open TrackThe PK open track is similar to closed one.
Inopen track, we use all the 6 months corpus ofPeople?s Daily and set up a dictionary of107,749 entries.
Additionally, we find 101 newwords from the Web and add them to the dic-tionary.
We train the parameters of named entityrecognition with a person list and a location listin our laboratory.
The training of other parame-ters is the same with closed track.4 Experiments and DiscussionsWe do several experiments on PK test corpus tosee the contribution of each postprocessor.
Wecut off one postprocessor at a time from thecomplete system and record its F-score.
Theevaluation results are shown in Table 1.
In thetable, MDW represents Morphologically De-rived Word Recognition, and NWI representsNew Word Identification.PK open PK closedCompleteSystem 96.5% 94.9%WithoutMerging 96.3% 94.7%WithoutMDW 96.6% 94.4%WithoutNWI 96.5% 94.9%Table 1: Evaluation results of IRLAS with eachpostprocessor cut off at a timeFrom Table 1, we can come to some interest-ing facts:!
The Merging of Adjoining Words has goodeffect on both open and closed tracks.
Sowe can conclude that this module can solvethe problem of inconsistent training corpusto some extent.!
Morphologically Derived Word Recogni-tion does some harm in open track, but ithas a very good effect in closed track.Maybe it is because that in open track, wecan make a comparatively larger dictionarysince we can use any resource we have.
Somost MDWs6 are in the dictionary and theMDWs that are not in the dictionary aremostly difficult to recognize.
So it doesmore harm than good in many cases.
But inclosed track, we have a small dictionaryand many common MDWs are not in thedictionary.
So it does much more good inclosed track.!
New Word Identification is minimal in bothopen and closed tracks.
Maybe it is becausethat the above steps have recognized themost OOV words and it is hard to recognizeany more new words.5 External Factors That Affect OurPerformanceThe difference on the definition of words is themain factor that affects our performance.
Inmany cases such as ?????
?, ???
?, ?????
are all considered as one word in our systembut not so in the PK gold standard corpus.
An-other factor is the inconsistencies in trainingcorpus, although this problem has been solved tosome extent with the module of merging.
But6 It refers to Morphologically Derived Words.174because the inconsistencies also exist in test cor-pus and there are some instances that a word ismore likely to be a single word in training cor-pus but more likely to be separated into twowords in test corpus.
For example, the word ????
is more likely to be a single word in trainingcorpus but is more likely to be separated intotwo words in test corpus.
There is another factorthat affects MDW, many postfixes in our systemare not considered as postfixes in PK gold stan-dard corpus.
For example, the word ?????
isrecognized as a MDW in our system since ??
?is a postfix, however, it is segmented into twoseparate words as ???
??
in PK gold stan-dard corpus.6 ConclusionThrough the second SIGHAN bakeoff, we findthe segmentation model and the algorithm in oursystem is effective and the multiple postproces-sors we use can also enhance the performance ofour system.
At the same time, we also find someproblems of us.
It also has potential for us toimprove our system.
Take MDW for example,we can make use of more features such as thePOS and the length of the preceding word toenhance the recall and precision rate.
The bake-off points out the direction for us to improve oursystem.ReferencesHuaping Zhang, Qun Liu, Hongkui Yu, Xueqi Cheng,Shuo Bai, Chinese Named Entity Recognition Us-ing Role Model.
International Journal of Computa-tional Linguistics and Chinese LanguageProcessing, 2003, Vol.8(2)Andi Wu, Zixin Jiang, 2000.
Statistically-EnhancedNew Word Identification in a Rule-Based ChineseSystem.
In Proceedings of the Second ChineseLanguage Processing Workshop, pp.
46-51,HKUST, Hong Kong.Huaping Zhang, Hongkui Yu, Deyi Xiong, Qun Liu,HHMM-based Chinese Lexical Analyzer ICTCLAS.In Proceedings of the Second SIGHAN Workshopon Chinese Language Processing, July 11-12,2003, Sapporo, Japan.Aitao Chen, Chinese Word Segmentation UsingMinimal Linguistics Knowledge.
In Proceedings ofthe Second SIGHAN Workshop on Chinese Lan-guage Processing, July 11-12, 2003, Sapporo, Ja-pan.Andi Wu, Chinese Word Segmentation in MSR-NLP.In Proceedings of the Second SIGHAN Workshopon Chinese Language Processing, July 11-12,2003, Sapporo, Japan.175
