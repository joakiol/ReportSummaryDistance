Paying Heed to CollocationsMatthew Stone Christine Doran *Department of Computer Science Department of LinguisticsUniversity of PennsylvaniaPhiladelphia, PA 19104(matthew,cdoran) @ linc.cis.upenn.eduAbstractIn this paper, we introduce a system,Sentence Planning Using Description,which generates collocations within theparadigm of sentence planning.
SPUD si-multaneously constructs he semantics andsyntax of a sentence using a LexicalizedTree Adjoining Grammar (LTAG).
This ap-proach captures naturally and elegantly theinteraction between pragmatic and syntac-tic constraints on descriptions in a sen-tence, and the inferential and lexical in-teractions between multiple descriptions ina sentence, At the same time, it exploitslinguistically motivated, eclarative speci-fications of the discourse functions of syn-tactic constructions to make contextuallyappropriate syntactic hoices.1 Introduct ionWords come in a variety of conventional combi-nations; these units range from short expressionswith idiosyncratic meanings, like the call numberof a book, to full sentences with compositionally-derived, yet frozen, meanings, like You can'tteach an old dog new tricks.
Natural languagegeneration systems must adhere to these combi-nations, or risk that output will sound as if trans-lated, badly, from Lisp.Conventional combinations represent not justfamiliar words, but familiar meanings.
Noveldescriptions can be unintelligible ven if moreliterally accurate--imagine the key string of abook, instead of call number.
Alternatives tostock language can be even more absurd: 1"The authors thank Aravind Joshi, Mark Steed-man, Martha Palmer, Ellen Prince, Owen Rambow,Mike White, Joseph Rosenzweig, Betty Birner for theirhelpful comments on various tages of this work.
Thiswork has been supported by NSF and IRCS gradu-ate fellowships, NSF grant NSF-STC SBR 8920230,ARPA grant N00014-94 and ARO grant DAAH04-94-G0426.I We found this with some similar examples, athttp://149.28.3.6:1701/people/lensky/quotes.html.
(1) It is futile to attempt to indoctrinate asuperannuated canine with innovativemaneuvers.To naturally reuse familiar meanings, generationsystems hould exploit opportunities todo so asmea-ing is constructed, not just in transducingmeaning to a surface representation.
Followingthis line, the research presented here concernsgenerating idioms and collocations as part of SEN-TENCE PLANNING (Kittredge t al., 1991).Our approach uses Lexicalized Tree AdjoiningGrammar (LTAG) and takes DESCRIPTION as theparadigm for the final realization of content.
Webuild on the existing insights of linguists (includ-ing (Pustejovsky, 1991; Mel'Euk and Polgu~re,1987; Nunberg et al, 1994)) and implementations(including (Reiter and Dale, 1992; Viegas andBouillon, 1994; Smadja and McKeown, 1991)).However, our proposal introduces two key fea-tures.
First, the syntax AND SEMANTICS of collo-cations is planned incrementally and simultane-ously.
This simplifies the design of the procedureand the linguistic representations it requires; itgrounds the decision to select a particular col-location; and it helps integrate the different de-cisions that must be made in sentence planning.Second, we treat collocations and idioms not justas lexicographic entries, but with full semanticsand pragmatics.
This allows us to generate spe-cialized uses of words not just in certain lexicalor syntactic ontexts, but more generally in ap-propriate discourse contexts.
The use of theseconventional meanings is a consequence of thesystematic design of our planner to observe acomputational interpretation f Grice's Maxim ofManner (Grice, 1975): say the usual thing unlessyou mean something different.The organization of the paper is as follows.
Insection 2, we review treatments of collocation inlinguistic theory and natural language generation.In section 3 we describe the generation system,SPUD, within which the present analysis will be91developed.
Then, in section 4, we show howthe collocational information can be incorporatedinto SPUD.
Our work is set in the library domain,with the system having the role of a librariananswering patrons' queries.2 Conventional combinations of wordsThe different constructions that can be describedas collocations exhibit an enormous range of con-ventionalization.
On the one hand are arbitrary,fixed, undecomposable combinations like by andlarge; on the other are locutions like override aveto whose preferred co-occurrence derives fromthe specificity of the semantics of the compo-nents.
Between these extremes are three classesof constructions of particular concern for natu-ral language generation.
First, IDIOMATICALLYCOMBINING EXPRESSIONS (Nunberg et al, 1994)must be derived compositionally from special, id-iomatic meanings of their parts, as when strings =influence, pull = exert privately (from the OED):(2) The strings she pulled didn't get her thejob.Second, COLLOCATIONS PROPER involve con-stituents whose meaning is determined by ordi-nary principles, like copy area, but which must beregarded as conventional in light of the oddness ofnear synonyms (like duplication zone); such col-locations are the subject of the Lexical Functionsof the Meaning-Text Theory (MTT) (Mel'~uk andPolgu~re, 1987).
Finally, SEMANTIC COLLOCA-TIONS like long book derive their particular mean-ing from the recovery in context of parameters forevents and other entities (Pustejovsky, 1991).Researchers in generation rarely address allof these kinds of conventionality.
For exam-ple, (Viegas and Bouillon, 1994) handle semanticcollocations by implementing Pustejovsky's Gen-erative Lexicon Theory (GLT); modifiers takeon specialized meanings derived from salientprocesses and characteristics associated withthe heads they modify.
Thus, a long bookmeans a long book to read because of a lexi-cographic association between books and read-ing.
Similarly, implementations of MTY describethe conventional use of certain modifiers withheads (Mel'~uk and Polgu~re, 1987; Iordanskajaet al, 1991; Wanner, 1994) using Lexical Func-tions.
Thus, a function Magn determines the real-ization of a concept very, intense, intensely:(3) A Magn escape ~ a narrow escape;to Magn bleed ~ to bleed profusely.Copy area would be handled using the LexicalFunction SIoc, which returns the name of the lo-cation associated with an activity.
(Smadja andMcKeown, 1991) are an exception in treating awide range of conventionality, but they simplylist the idiomatic status and meaning of a varietyof forms in a way that collapses the diistinct he-oretical status, and to a large extent, the distinctmeanings, of different collocations.These various existing computational ap-proaches have three main deficiencies.
First, theyderive conventionality from relational exiconsthat describe only the properties of WORDS.
How-ever, the features that determine appropriatenessof conventional attributions are better modelledas properties of OBJECTS in an evolving model ofdiscourse.
Idiomatically combining expressionsintroduce ntities for subsequent reference:(4) Kim's family pulled some strings on herbehalf, but they weren't enough to get herthe job.
\[=(Nunberg et al, 1994) 10c\]Semantic collocations recover their parametersbased simply on the things described, regardlessof their syntactic proximity, as the examples in(5) show:(5) a I will not check out a long book.b I won't check out that book.
It's long.c I won't check that out.
It's a longmonstrosity.The modifications achieved by Lexical Functionsare parallel: as with narrow in (6):(6) a They made a narrow escape.b Their escape had been lucky; Bill found ituncomfortably narrow.c Whew!
\[after burrowing and swimmingout of Alcatraz, amid nearby shots andsearchlights\] That was narrow!Second, by treating different conventionalcombinations as mere paraphrases ofone another,researchers complicate the statement of when andwhy to use conventional forms.
No specifica-tion of idiomatic combination is complete with-out representing the pragmatic ircumstances inwhich its use is appropriate ( .g.
saying to some-one Your goose is cooked is not appropriate as aexpression of sympathy; the expression conveysa certain amount of disregard for their predica-ment).
Meanwhile, some representation of en-tities and their salience is required to determinewhether ellipsis is possible in context.
Whethera hard idea is hard to formalize, to communicate,or to understand depends on the topic; to be clear,a natural language system must model how itsaudience arrives at such understandings.Third, by recognizing collocations only whentransducing underlying semantic representations,researchers limit the extent o which knowledgeof collocations can be exploited in generating flu-92ent text.
In particular, transduction presupposesthat the content of referring expressions has al-ready been established.
This means that colloca-tions in definite descriptions either will arise onlyby accident (or by generate-and-test search) or bya secondary specificatioo that ensures the prefer-ence for semantics that can ultimately be realizedusing collocations.3 SPUDThis section provides a brief overview of therepresentations and algorithms that SentencePlanning Using Description (SPUD) uses to ad-dress the properties of collocations discussedabove.
SPUD extends the general procedure forbuilding referring expressions that is suggestedby the planning paradigm (Appelt, 1985; Kro-nfeld, 1986).
The procedure starts from a setof entities to describe and a set of intentions toachieve in describing them.
It then applies op-erators that enrich the content of the descriptionuntil all intentions are satisfied.
As in realiza-tions like (Dale and Haddock, 1991), we constrainthe inference required to generate and evaluatealternatives by limiting the kinds of intentionsconsidered.
However, whereas the planning pro-cedures on which we base our system are usedonly for noun phrases, we apply this procedureto the sentence as a whole using a rich semanticrepresentation; further, although these procedurestypically construct an abstract semantic represen-tation, we treat operators as entries with syntactic,semantic and pragmatic properties.
The lexical-ized tree adjoining grammar (LTAG) formalismprovides an abstraction of the combinatorial prop-erties of words.
The resulting system offers anumber of advantages.
By incorporating contentinto descriptions of a variety of entities until theaddressee can fill in the details, this procedureresults in short, natural and unambiguous sen-tences.
Moreover, by evaluating and selectingalternatives on the basis of their pragmatic, se-mantic and syntactic ontribution to the sentenceas a whole, the procedure uniformly handles ava-riety of interactions inside a sentence, includingcollocations.3.1 Linguistic SpecificationsThis algorithm requires a declarative specifica-tion of three kinds of information: first, whatoperators are available and how they may com-bine; second, how operators pecify the contentof a description; and third, how operators achievepragmatic effects.
We represent operators as el-ementary trees in an LTAG, and use TAG oper-ations to combine them; we give the meaning ofeach tree as a formula in an ontologically promis-cuous representation language; and, we model thepraghmtics of operators by associating with eachtree a set of discourse constraints describing whenthat operator can and should be used.TAG (Joshi et al, 1975) is a grammar for-malism built around two operations that combinepairs of trees: SUBSTITUTION and ADJOINING.
ATAG grammar consists of a finite set of ELEMEN-TARY trees, which can be combined by these op-erations to produce derived trees recognized bythe grammar.
In substitution, the root of the firsttree is identified with a leaf of the second tree,called the substitution site (.L).
Adjoining is amore complicated splicing operation, where thefirst tree replaces the subtree of the second treerooted at a node called the adjunction site; thatsubtree is then substituted back into the first treeat a distinguished leaf called the FOOT node ( , ) .Elementary trees without foot nodes are calledINITIAL trees and can only substitute; trees withfoot nodes are called AUXILIARY trees, and mustadjoin.
TAG elementary trees abstract the com-binatorial properties of words in a linguisticallyappealing way.
Figure 1 (a) shows an initial treerepresenting the book.
Figure 1 (b) shows an aux-iliary tree representing the modifier syntax, whichcould adjoin into the tree for the book to give thesyntax book.
All predicate-argument structuresare localized within a single elementary tree, evenin long-distance r lationships.
Figure l(c) showsthe topicalized tree anchored by have; both of itsarguments are substitution sites.Our grammar incorporates two additional prin-ciples.
First, the grammar is LEXICALIZED(Schabes, 1990): each elementary structure inthe grammar contains at least one lexical item.Second, our trees include FEATURES, follow-ing (Vijay-Shanker, 1987).We specify the semantics of trees by adaptingtwo principles of computational semantics to theLTAG formalism.
First, as originally advocatedby Hobbs (1985), we adopt an ONTOLOGICALLYPROMISCUOUS representation that includes awidevariety of types of entities.
In particular, abstractentities are introduced to represent the SCOPES ofOPERATORS.
A predicate is interpreted as if insidea scope when the predicate takes the correspond-ing abstract entity as an argument.
For this paper,we need EVENTUALITIES as abstract representa-tions of spatiotemporal scope and INFORMATIONSTATES to abstract he scope of modal operatorslike possibility and belief.
Nodes are labeled assupplying information about a particular entity or93NP \[about : <I> I, S, X\]DetP N\[about : <13\]IDet bookPthebook( I:INFO, S:STATE, X:IND)(unique-id(I, X))(a)S \[about : <1>I, R, H\]Nbbout : <1> L ?,X\].
I S Syr inX)  ~ NPJ.
\[about: <2> I,?,H-ee\] S \[about: <1~\]N\[about.
, , N* \[about:<lqNP.L \[about: ?,?,.-er\] VP\[about : <1~syntax V \[ \] NP \[about : <2~concerns(l: INFO, S:STATE, I IX:IND, syntax:INb) /have./ t\[always applicable\] have(l:lNFO, H:STATE, H-er:IND, H-ee:INg)(b) (in-poset(H-ee), in-op(have(I, H, H-er, H-ee)))(c)Figure 1: LTAG trees with semantic and pragmatic specificationscollection of entities (this is inspired by a similarhypothesis in (Jackendoff, 1990)).
To guaranteea coherent meaning for a derived structure, a nodeabout x can only substitute or adjoin into anothernode about x.
Here, we simply use an additionalfeature on the node to capture this.
Figure 1 alsoshows the semantics and about labels for eachtree; ?
indicates unspecified about values.To package information appropriately requiressensitivity to the knowledge of the hearer and thestate of the discourse.
Different constructionsmake different assumptions about the status ofentities and propositions.
We model these differ-ences by including in each tree a specification ofthe contextual conditions under which use of thetree is pragmatically icensed.
Our conditions de-rive from linguistic analysis, particularly (Gundelet al, 1993; Ward, 1985; Ward and Prince, 1991;Prince, 1993; Birner, 1992).The status of entities and propositions in dis-course varies along at least four dimensions thatare relevant to these specifications.
First, entitiesdiffer in NEWNESS (Prince, 1981).
At any point,an entity is either new or old to the HEARER, ac-cording to whether or not the hearer has at leastimplicit knowledge of the existence of the en-tity.
Analogously, an entity is either new or oldto the DISCOURSE, according to whether the dis-course contains an earlier reference to it.
Sec-ond, entities differ in SALIENCE (Grosz and Sid-ner, 1986; Grosz et al, 1995).
At any point,salience assigns each entity a position in a par-tial order that indicates how accessible it is forreference in the current context.
Third, entitiesare related by material PARTIALLY-ORDERED SET(POSET) RELATIONS to other entities in the con-text (Hirschberg, 1985).
These relations includepart and whole, subset and superset, and member-ship in a common class; a number of constructionsdepend on poset relations to signal their connec-tion with context.
Finally, the discourse may dis-tinguish some OPEN PROPOSITIONS, propositionscontaining free variables, as being under discus-sion (Halliday, 1967; Prince, 1986).
This priv-ileges subsequent information that provides trueinstantiations for the variables in a salient openproposition.
We assume that information of thesefour kinds is available in a model of the currentdiscourse state, and that the applicability condi-tions of constructions can freely make referenceto this information.
The pragmatic specificationfor the book, syntax, and topicalized have appearunder the semantics for each tree in figure 1.Our discourse model contains information onthe shared knowledge of the speaker and hearer,private knowledge of the speaker, and a specifi-cation of entities and their discourse status.
In thelibrary domain, shared knowledge includes suchthings as rules about how to check out books,while speaker knowledge includes uch informa-tion as the status of books in the library.
Thediscourse model can also include general proper-ties that describe the conversational situation as awhole; for example, it might specify the formal-ity of the register in which the communication isbeing conducted.3.2 The algorithmOur system takes two types of goals.
First, goalsof the form identify x as cat instruct the algo-rithm to construct a description of entity x usingthe syntactic ategory cat.
If x is uniquely iden-tifiable, then this goal is only satisfied when theoverall content planned so far distinguishes x forthe hearer.
Ifx is hearer new, this goal is satisfiedby including any constituent of type cat.
Sec-94ond, goals of the form communicate p instructthe algorithm to include the proposition p. Thisgoal is satisfied as.long as the overall content EN-TAILS p given the shared knowledge of speakerand hearer.In each iteration, our algorithm must determinethe appropriate elementary tree to incorporate intothe current description.
It performs this task intwo steps to take advantage of the regular asso-ciations between semantics and trees in the lex-icon.
Lexical entries pair a semantic onstraintwith a FAMILY of TREES that describe the com-binatory possibilities for realizing the semantics.For example, book is stored with a tree family thatincludes a book and the book.
We have chosento include the determiners in the basic NP treesbecause of their importance for the semantics andpragmatics of the NP.
Similarly, there are dif-ferent initial trees for each clause type anchoredby a particular verb.
Trees in the tree familyare shared among all lexical items that share aparticular structure.
This allows us to specifythe pragmatic onstraints associated with the treetype once and for all, regardless of which verb se-lects it.
Moreover, we can determine which treeto use by looking at each tree ONCE, even whenthe same tree is associated with multiple lexicalitems.Hence, the first step is to identify applicablelexical entries: these items must correctly de-scribe some entity; they must anchor trees thatcan substitute or adjoin into a node that describesthe entity; and they must contribute toward satis-fying current goals.
(We describe more preciselyhow this contribution isevaluated in section 4.1 .
)Then, the second step identifies which of the asso-ciated trees are applicable, by testing their prag-matic conditions against he current representa-tion of discourse.
We combine possible lexicalitems and possible trees, to give an evaluation ofall applicable options.
The algorithm identifiesthe entries that most contribute to current goals,and from these, selects the entry with the mostspecific semantic and pragmatic licensing condi-tions.
This means that the algorithm generatesthe most marked licensed form for the particularcontext.The entry is then substituted or adjoined intothe tree at the appropriate node.
The meaningof the derived tree is simply the CONJUNCTIONof the meanings of the elementary trees used toderive it.
The entry may specify additional goals,because it describes one entity in terms of a newone.
These new goals are added to the currentgoals, and then the algorithm repeats.3.3 DiscussionThe strength of the present work is that it capturesa number of phenomena discussed elsewhere sep-arately, and does so within the unified frameworkof description.
In particular, we treat many typesof content as contributing to expressions that re-fer to semantic objects.
The tenses of sentencesin discourse refer to times in much the same waypronouns and full NPs refer to individuals (Partee,1973; Partee, 1984).
The modality of sentencesmay refer to a salient possibility (Roberts, 1986)or provide the content of a salient psychologicalstate (Wiebe, 1994).
The rhetorical connectionbetween a sentence and surrounding discourseshould also be described with adjuncts (Huang,1994).
Adjuncts giving details about an eventshould be included only after reasoning that theseadjuncts are in fact necessary in context (McDon-ald, 1992).With its incremental choices and its emphasison the consequences of functional choices in thegrammar, our algorithm resembles the networksof systemic grammar (Mathiessen, 1983; Yang etal., 1991).
However, unlike systemic networks,our system derives its functional choices dynam-ically using a simple declarative specification offunction that correlates well with recent linguisticwork.
Further, like many sentence planners, weassume that there is a flexible association betweenthe content input to a sentence planner and themeaning that comes out.
Other researchers (Ni-colov et al, 1995; Rubinoff, 1992) have assumedthat this flexibility comes from a mismatch be-tween input content and grammatical options.
Inour system, such differences arise from the refer-ential requirements and inferential opportunitiesthat are encountered.Previous authors (McDonald and Pustejovsky,1985; Joshi, 1987) have noted that TAG has manyadvantages for generation as a syntactic formal-ism, because of its localization of argument struc-ture.
These aspects of TAGs are crucial for us.Lexicalization allows us to easily specify localsemantic and pragmatic onstraints imposed bythe lexical item in a particular syntactic frame.Various efforts at using TAG for generation (Mc-Donald and Pustejovsky, 1985; Joshi, 1987; Yanget al, 1991; Nicolov et al, 1995; Wahlster et al,1991) enjoy many of these advantages.
Further-more, (Shieber et al, 1990; Shieber and Schabes,1991; Prevost and Steedman, 1993; Hoffman,1994) exploit similar benefits of lexicalizationand localization.
What sets SPUD apart is its si-multaneous construction of syntax and semantics,and the tripartite, lexicalized, declarative gram-95matical specifications for constructions it uses.
(Shieber et al, 1990; Shieber and Schabes, 1991 )construct a simult .~eous derivation of syntax andsemantics--but they do not construct the seman-tics: it is an input to their system.
Moreover,they do not represent any pragmatic inforrnatiGn.
(Prevost and Steedman, 1993; Hoffman, 1994)do represent the division of sentences into themeand rheme, but because they do not model thepragmatics of particular constructions, they plandescriptions in a separate step.4 Convent ional  combinat ion  in SPUDBecause LTAG can associate multiple iexicalitems to a single tree, it is straightforward tolist frozen idioms, like call number, in the lex-icon (Abeille and Schabes, 1989).
These specifi-cations can include idiosyncratic semantic andpragmatic information; grammatical processeslike tense marking apply normally.In this section, we describe how SPUD can bemade to use words in other conventional combi-nations.
Our proposal involves three steps.
First,as in (Reiter and Dale, 1992), we stipulate thatsome attributes of entities are more important thanothers, and that some words more naturally de-scribe those attributes.
Second, in keeping withontological promiscuity (Hobbs, 1985), we repre-sent he importance of attributes by the salience ofevents and states in the discourse model--thesestates and events now have the same status in thediscourse model as any other entities.
Finally,we extend SPUD's evaluation of alternatives, othat it describes the most salient entities possible,and uses basic-level terms wherever possible.
Byassociating entities not just with salient attributesbut also with salient actions and salient figura-tions, we capture collocations, semantic ollo-cations and idiomatic compositionality using auniform mechanism.4.1 Collocations properAlthough primarily concerned with the interpre-tation of Gricean maxims, the work of (Reiter andDale, 1992; Dale and Reiter, 1995) underlines theconventionality ofdescription.
Based on a reviewof psychological experimentation a d their ownstudy of referring expressions in task-oriented di-alogue, they argue that some referring expres-sions can be constructed simply by selecting prop-erties from a prioritized list of attributes until theentity is distinguished.
To further conventional-ize descriptions, they privilege the selection ofproperties that provide basic-level characteriza-tions of the entity (Rosch, 1978; Reiter, 199l).Because any property is considered for only oneattribute, this algorithm offers a linear speedupover the greedy strategy used in (Dale and Had-dock, 1991) and described above for SPUD, whichconsiders every property at every stage.
How-ever, here we focus on how incorporating similarideas into SPUD gives a general framework forspecifying conventional uses of words, and re-main neutral about achieving similar speedups.Reiter and Dale suggest hat the prioritizedlist of attributes their algorithm uses is domain-dependent.
In fact, we find that these lists are bothdomain and object-dependent.
Obviously the at-tributes by which we describe abstractions likeevents and states--typically time, location, andmanner or quality--are quite distinct from thenatural attributes by which physical objects aredistinguished.
However, in the library, widelydifferent attributes can be appropriate ven forphysical objects of various types.
Books can bedescribed by author, by physical characteristics,or by content (e.g.
Chomsky ~ book; the yellowbook, a math book).
Periodicals, meanwhile, arebest described by date of issue (e.g.
the May is-sue of Language).
Parts of the library, as we shallsee below, are best distinguished by the specialservices they provide (e.g.
the reference desk).SPUD's ontologically promiscuous discoursemodel offers a natural dimension to representthese distinctions.
Since each property of an ob-ject is associated with an eventuality argument,we can assign a level of salience for that even-tuality.
We can use this ranking to indicate theconventional importance of the eventuality indis-tinguishing the object.
In other words, if we knowp(e, x), and it is natural to describe x in terms ofp, e will be salient.
For example, since period-icals are easily identified by their date of issue,we should make this state salient.
Note then thatsalience is determined for explicitly mentionedand inferable entities and depends not only onrecency of mention but also on facts about theconversational situation and real-world relation-ships between objects.Reiter and Dale also point out that which char-acterizations are basic-level must be adjusted toreflect the expertise of the addressee; however,we shall sidestep this issue here by assuming thatcertain lexical items are simply listed as basic-level terms.By itself, these additions are not enough: SPUDmust also take salience and basic-level seman-tics into account in the evaluation of its alterna-tives.
That is: other things being equal, SPUDshould choose to incorporate at each stage the96syntactic-semantic-pragmatic unit which refers tomaximally salient entities; and, other things be-ing equal, SPUD should incorporate a basic-levelpredicate.
Integrating Reiter and Dale's prioriti-zation of these considerations with SPUD's otherconsiderations leads to the following ranking ofcriteria for comparison:(7) RULES OUT A DISTRACTOR OR ENTAILSNEEDED INFORMATION > SALIENCE OFENTITIES MENTIONED > NUMBER OFDISTRACTORS RULED OUT > NUMBER OFINFORMATIONAL GOALS ACHIEVED >BASIC-LEVEL TERM > SPECIFICITY OFLICENSING CONDITIONSWith the right linguistic specification, this isall the machinery SPUD needs to generate con-ventionalized forms.
To see how we can generateordinary collocations, consider describing partsof a library.
Descriptions of these places are typ-ically collocations: e.g.
copy area, referencedesk, interlibrary loan office.
The names can beabbreviated in context, they can be interpretedcompositionally, but substituting synonyms gen-erally sounds odd.
Nevertheless, these descrip-tions share features, in that one always describesits type, sometimes the service it provides, andmost rarely its location.
This leads to the follow-ing axiomatization of the salience of states:(8) part-of(I, S1, Part, Lib) Alibrary(I, $2, Lib) D(has-type(I, S3, Part, Type) Aprovides-service(I, $4, Part, Service) Ahas-location(I, $5, Part, Loc) D$3 >s $4 >s $5)The first argument of each predicate is the in-formation state in which the various predica-tions hold; the second argument is the eventualitywhich witnesses the application of the predicate;>s indicates the salience ranking of the states.Thus, (8) considers a case where there is a partPart of a library Lib: suppose $3 witnesses thatPart has some type Type; $4, that Part providesservice Service; and $5, that Part has locationLoc.
Then, $4 is more salient han $5, and $3 ismore salient han both.
We must specify not onlythe salience of different states for the same copier,but also the salience of corresponding states fordifferent copiers.
Another axiom, similar to (8),ensures that states that specify a given attributeare equally salient across copiers when the copiersinvolved are equally salient.The vocabulary chosen, meanwhile, reflectsconventional names for the structures and ser-vices of the library.
Semantic declarations suchas the following represent this:(9) area  (I, S, A) : BASIChas-type(I, S, A, area)That is, area uses the specified semantics to pro-vide a basic-level description of A in terms ofstate 9 and information I.
Note that SPUD alwayschooses a maximally specific licensed form outof equally good alternatives.
Thus, we can haveany number of basic-level terms to describe anobject, and the appropriate one will be selectedon the basis of its specificity.
For example, evenif both room and area are basic, a room will bestill be described using room, because all roomsare areas but not all areas are rooms.Together, these assumptions suffice to gener-ate collocations for library parts.
For example,suppose  SPUD has the goal of describing the partof the library where copying takes place, loca-tion e30.
SPUD first selects the NP the area,eliminating alternatives like the room, the desk,the stack, because they do not truthfully describee30.
However, since many other parts of the li-brary are also areas, the current description doesnot rule out all possible distractors, and SPUD fur-ther elaborates the description.
The modifierscopy and service are both applicable to e30, butcopy eliminates all distractors while service doesnot, so the former is selected, yielding the finalNP the copy area.4.2 Semantic collocationsTo handle semantic collocations now requiresonly a representation f how certain lexical itemsdepend on hidden parameters for actions andevents.
For example, consider the lexical itemfast: it constrains the typical rate of some actionperformed by or with the entity it describes.
Thus,it has a meaning like this:(10) fast( I ,  S, Obj, Act): BASICparticipant(I, $2, Obj, Act) Atypical-rate(I, $3, Act, Rate) Ahigh(I, S, Rate)Corresponding tothe qualia structure of GLT, wehave axioms describing what actions are associ-ated with objects and how salient hey are.
For aphotocopier, this might be specified this way:(1 I) photocopier(I, S, X) 3(participant ( I, S 1 (X), X, copy-action) Aparticipant(I, S2(X), X, repair-action) Aparticipant(I, S3(X), X, fill-paper-action)ASI(X) >s S2(X) >s S3(X))That is, typically, with copiers, you not only makecopies, but also fill them with paper, and (sadly, alltoo often), have them repaired; however, copyingis the most salient hing to do with them.
Note thatwhile this axiom is expressed at the same level of97generality as GLT's qualia structures, this rule ispart of world knowledge and applies to all thingsthat are photocopiers, not to all occasions wherethings are described as photocopiers.To see how SPUD uses these specifications,let us say that we have a copier, c42, which isthe sole fast copier (at making copies) in thelibrary.
After planning a refemng expressionthe copier, SPUD has the goal of distinguishingc42 from the other copiers.
The KB entailsthe fact fast(i,s,c42,copy-action), which allowsus to incorporate the lexical item fast into thedescription.
SPUD then evaluates the distractorset; since copy-action is a new reference, SPUDchecks whether any distractor is also fast at anaction which is at least as salient as copy-action.None are, because copy-action isthe most salientaction of copiers.
Since the expression, the fastcopier, now refers uniquely both to c42 andto copy-action, the referring expression is ad-equate.
The need to rule out distractor actionscan cause information to be added to an expres-sion.
To describe another copier, c43, which isthe fastest copier to fill with paper, SPUD woulddescribe not only its rate but also the relevantaction in order to distinguish it from c42, i.e.the fast copier to fill.
Also, note SPUD can usethis same meaning of fast and the same reason-ing process even when fast does not modify anoun.
(For example, in a slightly different con-text it could describe the state s with this sentence:The copier is fasr)4.3 Idiomatic compositionAs (Nunberg et al, 1994) emphasize, idiomaticcomposition typically involves some distinctivefigurative or metaphorical view of the objects be-ing described.
Accordingly, to specify idiomaticcomposition, we adopt a representation f suchviews from (Ballim et al, 1991).
They outline amodel of reasoning in which facts are partitionedinto sets called ENVIRONMENTS.
Environmentscan collect information about particular topics,or, when nested, can represent the beliefs of par-ticular agents.
Moreover, they suggest that non-literal language can also be represented using anested environment, whose contents are deter-mined by treating topic-environments as com-peting sources of information analogous to dif-ferent agents' views.
We believe reasoning al-gorithms like those presented in (Ballim et al,1991) should be an important part of any nat-ural language generation system which aims atidiomatic language; however, for the present, hekey feature of this account is just its principled useof multiple informatiou-states, in which differentfacts hold.We combine this representation with two as-sumptions about how information states are rep-resented in the grammar.
We assume that in-formation states are recovered from the contextjust like other parameters of interpretation likestates and actions.
However, we use trees thatin some cases impose coreference requirementsbetween the information states in which differentconstituents are interpreted.
For the exampleswe have considered, what seems right is to coin-dex the information states of modifiers and theirheads, and to coindex the information state ofa verb with all its arguments except he subject.
(The trees of figure 1 respect this generalization.
)Consider the example from section 2: the com-bined convention strings = influence, pull = exertprivately.
The opportunity to use the expressionarises in any information state k where:(12) influence(k, Sl ,  C, X, F) Asubverts(k, $2, C, bureaucracy) Aexert(k, E, X, C) A private(k, $3, E)We can represent the idiom semantically using arule that introduces the associated stock figura-tion, that bureaucrats are puppets whose behavioris governed by such influence: bp(k, C).
(13) strings(bp(k, C), S4(k, C), C) Apull(bp(k, C), E, X, C)Now we just use the ordinary meanings of pulland strings to describe this situation.To constrain the situations in which this is anappropriate hing to say, we need to determine thecircumstances in which bp(k, C) is as salient ask.
(One might claim that the ready salience ofthe information state--naturally, different acrosslanguages--is what makes idioms different frommetaphors.)
Although such a specification isclearly open-ended, we approximate he full set ofconstraints in terms of two parameters of the dis-course context: a reasonable degree of intimacybetween speaker and hearer and an informal reg-ister of conversation.Consider how the noun phrase the strings shepulled is generated todescribe some exerted influ-ence c. Under appropriate discourse conditions,SPUD can choose to describe c in terms of theinformation state bp(k,c) and the lexical itemstrings.
To rule out c's additional distractors,the object relative clause anchored by pulled ischosen; the informational coindexation betweenthe foot N node and the verb in an object rela-tive clause ensures that exerted oes not apply--because c is NOT the object of an exerting eventaccording to information bp(k,c).
Finally, the98agent of the pulling is described with she.5 ConclusionSPUD uses a single body of syntactic, seman-tic, and pragmatic knowledge to generate bothproductive and conventional descriptive xpres-sions.
Hence, SPUD offers a natural frameworkfor dealing with the interactions between syntax,semantics and pragmatics which characterize thesentence planning problem, and ensuring contex-tually appropriate output.
This knowledge pro-duces good results; however, it is very expensiveto build.
The system requires rich descriptions oflanguage and of the world, which for now mustbe specified by hand.
Only SPUD's underlyingreasoning mechanisms are completely applica-tion independent, but others are at least partlyreusable.
Specifications ofworld knowledge canbe used for generation i many languages, whilelinguistic specifications apply across many do-mains.
For different languages, SPUD's modelmay vary along a number of dimensions, includ-ing the exact range of objects which roughlycorresponding lexical items can describe, andthe (default) salience rankings--both for typicalproperties and actions associated with objects andfor the information states licensing idioms.
Suchdifferences will allow SPUD to generate differentcollocations in different languages, even whendescribing the same entities.We have implemented a preliminary version ofSPUD, and realized the examples discussed in sec-tion 4.
Our future work includes refining this im-plementation a d enriching its linguistic knowl-edge.ReferencesAnne Abeille and Yves Schabes.
1989.
Parsing Idiomsin Lexicalized TAGs.
In Proceedings of EACL '89,pages 161-65.Douglas Appelt.
1985.
Planning English Sentences.Cambridge University Press, Cambridge England.Afzal Ballim, Yorick Wilks, and John Barnden.
1991.Belief ascription, metaphor, and intensional identi-fication.
Cognitive Science, 15:133-171.Betty Birner.
1992.
The Discourse Function of lnver-sion in English.
Ph.D. thesis, Northwestern Uni-versity.Robert Dale and Nicholas Haddock.
1991.
Contentdetermination in the generation ofreferring expres-sions.
Computational Intelligence, 7(4):252-265.Robert Dale and Ehud Reiter.
1995.
Computationalinterpretations of the Gricean maxims in the gener-ation of referring expressions.
Cognitive Science,18:233-263.H.
P. Grice.
1975.
Logic and conversation.
In P. Coleand J. Morgan, editors, Syntax and Semantics III.
"Speech Acts, pages 41-58.
Academic Press, NewYork.Barbara Grosz and Candace Sidner.
1986.
Attention,intentions, and the structure of discourse.
Compu-tational Linguistics, 12:175-204.Barbara Grosz, Aravind Joshi, and Scott Weinstein.1995.
Centering: A frameword for modeling thelocal coherence of discourse.
Computational Lin-guistics, 21 (2):203-225.Jeanette K. Gundel, Nancy Hedberg, and RonZacharski.
1993.
Cognitive status and the formof referring expressions~ in discourse.
Language,69(2):274-307.M.
A. K. Halliday.
1967.
Notes on transitivity andtheme in English.
Journal of Linguistics, 3:117-274.Julia Hirschberg.
1985.
A Theory of Scalar Implica-tu,e.
Ph.D. thesis, University of Pennsylvania.Jerry R. Hobbs.
1985.
Ontological promiscuity.
InProceedings of ACL, pages 61--69.Beryl Hoffman.
1994.
Generating context-appropriate word orders in Turkish.
In Proceedingsof the Seventh International Generation Workshop.Xiarong Huang.
1994.
Planning reference choicesfor argumentative texts.
In Seventh InternationalWorkshop on Natural Language Generation, pages145-152, June.Lidija Iordanskaja, Richard Kittredge, and AlainPolgu~re.
1991.
Lexical selection and paraphrasein a meaning-text generation model.
In Crcile L.Paris, William R. Swartout, and William C. Mann,editors, Natural Language Generation in ArtificialIntelligence and Computational Linguistics, pages293-312.
Kluwer, Dordrecht.Ray S. Jackendoff.
1990.
Semantic structures.
MITPress, Cambridge, MA.Aravind K. Joshi, L. Levy, and M. Takahashi.
1975.Tree adjunct grammars.
Journal of the Computerand System Sciences, 10:136--163.Aravind K. Joshi.
1987.
The relevance of tree adjoin-ing grammar to generation.
InGerard Kempen, edi-tor, Natural Language Generation, pages 233-252.Martinus NijhoffPress, Dordrect, The Netherlands.Richard Kittredge, Tanya Korelsky, and Owen Ram-bow.
1991.
On the need for domain communicationknowledge.
Computational Intelligence, 7(4):305-314.Amichai Kronfeld.
1986.
Donellan's distinction and acomputational model of reference.
In Proceedingsof ACL, pages 186-191.Christian M. I. M. Mathiessen.
1983.
Systemic gram-mar in computation: the Nigel case.
In Proceedingsof EACL, pages 155-164.David D. McDonald and James D. Pustejovsky.
1985.TAG's as a grammatical formalism for generation.In Proceedings of the 23 ra Annual Meeting of the99Association for Computational Linguistics, pages94-103, Chicago, IL.David McDonald.
1992.
Type-driven suppressionof redundancy in the generation of inference-richreports.
In Robert Dale, Eduard Hovy, DietmarRSsner, and Oiiviero Stock, editors, Aspects of Au-tomated Natural Language Generation: 6th Inter-national Workshop on Natural Language Genera-tion, Lecture Notes in Artificial Intelligence 587,pages 73-88.
Springer Verlag, Berlin.Igor A. Mel'~uk and Alaln Poigu~re.
1987.
A for-mal lexicon in the meaning-text theory (or how todo lexica with words).
Computational Linguistics,13(3-4):261-275.Nicolas Nicolov, Chris Mellish, and Graeme Ritchie.1995.
Sentence generation from conceptual graphs.In W. Rich G. Ellis, R. Levinson and F. Sowa, ed-itors, Conceptual Structures: Applications, Imple-mentation and Theory (Proceedings of Third In-ternational Conference on Conceptual Structures),pages 74-88.
Springer.Geoffrey Nunberg, Ivan A.
Sag, and Thomas Wasow.1994.
Idioms.
Language, 70(3):491-538.Barbara H. Partee.
1973.
Some structural analogiesbetween tenses and pronouns in English.
Journalof Philosophy, 70:601-609.Barbara H. Partee.
1984.
Nominal and temporalanaphora.
Linguistics and Philosophy, 7(3):243-286.Scott Prevost and Mark Steedman.
1993.
Generatingcontextually appropriate intonation.
In Proceedingsof the Sixth Conference of the European Chapter ofthe Association for Computational Linguistics.Ellen Prince.
1981.
Toward a taxonomy of given-newinformation.
In P. Cole, editor, Radical Pragmatics.Academic Press.Ellen Prince.
1986.
On the syntactic marking of pre-supposed open propositions.
In Proceedings of the22nd Annual Meeting of the Chicago Linguistic So-ciety, pages 208-222, Chicago.
CLS.Ellen Prince.
1993.
On the functions of left disloca-tion.
Manuscript, University of Pennsylvania.James Pustejovsky.
1991.
The generative l xicon.Computational Linguistics, 17(3):409-44 I.Ehud Reiter and Robert Dale.
1992.
A fast algorithmfor the generation of referring expressions.
In Pro-ceedings of COLING, pages 232-238.Ehud Reiter.
1991.
A new model oflexical choice fornouns.
Computational Intelligence, 7(4):240-251.Craige Roberts.
1986.
Modal Subordination,Anaphora and Distributivity.
Ph.D. thesis, Uni-versity of Massachusetts, Amherst.Eleanor Rosch.
1978.
Principles of categorization.
IEleanor Rosch and Barbara B. Lloyd, editors, Cog-nition and Categorization, pages 27-48.
Erlbaum,Hillsdale, NJ.Robert Rubinoff.
1992.
Integrating text planning andlinguistic hoice by annotating linguistic structures.In Robert Dale, Eduard Hovy, Dietmar R6sner, andOliviero Stock, editors, Aspects of Autornated Natu-ral Language Generation: 6th International Work-shop on Natural Language Generation, LectureNotes in Artificial Intelligence 587, pages 45-56.Springer Verlag, Berlin.Yves Schabes.
1990.
Mathematical nd Computa-tional Aspects of Lexicalized Grammars.
Ph.D.thesis, Computer Science Department, Universityof Pennsylvania.Stuart Shieber and Yves Schabes.
1991.
Generationand synchronous tree adjoining rammars.
Compu-tational Intelligence, 4(7):220-228.Stuart Shieber, Gertjan van Noord, Fernando Pereira,and Robert Moore.
1990.
Semantic-head-drivengeneration.
Computational Linguistics, 16:30-42.Frank Smadja and Kathleen McKeown.
1991.
Us-ing collocations for language generation.
Compu-tational Intelligence, 7(4):229-239.Evelyne Viegas and Pierrette Bouillon.
1994.
Seman-tic lexicons: the cornerstone for lexical choice innatural anguage generation.
In Seventh Interna-tional Workshop on Natural Language Generation,pages 91-98, June.K.
Vijay-Shanker.
1987.
A Study of Tree AdjoiningGrammars.
Ph.D. thesis, Department of Computerand Information Science, University of Pennsylva-nia.Wolfgang Wahlster, Elisabeth Andr6, Son Bandyopad-hyay, Winfried Graf, and Thomas Rist.
1991.WIP: The coordinated generation of multimodalpresentations from a common representation.
InOliviero Stock, John Slack, and Andrew Ortony,editors, Computational Theories of Communicationand their Applications.
Berlin: Springer Verlag.Leo Wanner.
1994.
Building another bridge over thegeneration gap.
In Seventh International Workshopon Natural Language Generation, pages 137-144,June.Gregory Ward and Ellen Prince.
1991.
On the topical-ization of indefinite NPs.
Journal of Pragmatics,15(8):338-351.Gregory Ward.
1985.
The Semantics and Pragmaticsof Preposing.
Ph.D. thesis, University of Pennsyl-vania.
Published 1988 by Garland.Janyce M. Wiebe.
1994.
Tracking point of view innarrative.
Computational Linguistics, 20(2):233-287.Gijoo Yang, Kathleen E McCoy, and K. Vijay-Shanker.
1991.
From functional specification tosyntactic structures: systemic grammar and tree-adjoining grammar.
Computational Intelligence,7(4):207-219.100
