Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 854?863,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsA Joint Model for Unsupervised Chinese Word SegmentationMiaohong Chen Baobao Chang Wenzhe PeiKey Laboratory of Computational Linguistics, Ministry of EducationSchool of Electronics Engineering and Computer Science, Peking UniversityBeijing, P.R.China, 100871miaohong-chen@foxmail.com,{chbb,peiwenzhe}@pku.edu.cnAbstractIn this paper, we propose a joint model forunsupervised Chinese word segmentation(CWS).
Inspired by the ?products of ex-perts?
idea, our joint model firstly com-bines two generative models, which areword-based hierarchical Dirichlet processmodel and character-based hidden Markovmodel, by simply multiplying their proba-bilities together.
Gibbs sampling is usedfor model inference.
In order to furthercombine the strength of goodness-basedmodel, we then integrated nVBE into ourjoint model by using it to initializing theGibbs sampler.
We conduct our experi-ments on PKU and MSRA datasets pro-vided by the second SIGHAN bakeoff.Test results on these two datasets showthat the joint model achieves much bet-ter results than all of its component mod-els.
Statistical significance tests also showthat it is significantly better than state-of-the-art systems, achieving the highestF-scores.
Finally, analysis indicates thatcompared with nVBE and HDP, the jointmodel has a stronger ability to solve bothcombinational and overlapping ambigui-ties in Chinese word segmentation.1 IntroductionUnlike English and many other western languages,there are no explicit word boundaries in Chinesesentences.
Therefore, word segmentation is a cru-cial first step for many Chinese language process-ing tasks such as syntactic parsing, information re-trieval and machine translation.
A great deal of su-pervised methods have been proposed for Chineseword segmentation.
While successful, they re-quire manually labeled resources and often sufferfrom issues like poor domain adaptability.
Thus,unsupervised word segmentation methods are stillattractive to researchers due to its independence ondomain and manually labeled corpora.Previous unsupervised approaches to word seg-mentation can be roughly classified into two types.The first type uses carefully designed goodnessmeasure to identify word candidates.
Populargoodness measures include description length gain(DLG) (Kit and Wilks, 1999), accessor variety(AV) (Feng et al., 2004), boundary entropy (BE)(Jin and Tanaka-Ishii, 2006) and normalized vari-ation of branching entropy (nVBE) (Magistry andSagot, 2012) etc.
Goodness measure based modelis not segmentation model in a very strict mean-ing and is actually strong in generating word listwithout supervision.
It inherently lacks capabil-ity to deal with ambiguous string, which is one ofmain sources of segmentation errors and has beenextensively explored in supervised Chinese wordsegmentation.The second type focuses on designing sophis-ticated statistical model, usually nonparametricBayesian models, to find the segmentation withhighest posterior probability, given the observedcharacter sequences.
Typical statistical mod-els includes Hierarchical Dirichlet process (HDP)model (Goldwater et al., 2009), Nested Pitman-Yor process (NPY) model (Mochihashi et al.,2009) etc, which are actually nonparametric lan-guage models and therefor can be categorized asword-based model.
Word-based model makes de-cision on wordhood of a candidate character se-quence mainly based on information outside thesequence, namely, the wordhood of character se-quences being adjacent to the concerned sequence.Inspired by the success of character-basedmodel in supervised word segmentation, we pro-pose a Bayesian HMM model for unsupervisedChinese word segmentation.
With the BayesianHMM model, we formulate the unsupervised seg-mentation tasks as procedure of tagging positional854tags to characters.
Different from word-basedmodel, character-based model like HMM-basedmodel as we propose make decisions on word-hood of a candidate character sequence based oninformation inside the sequence, namely, ability ofcharacters to form words.
Although the BayesianHMM model alone does not produce competi-tive results, it contributes substantially to the jointmodel as proposed in this paper.Our joint model takes advantage from three dif-ferent models: namely, a character-based model(HMM-based), a word-based model (HDP-based)and a goodness measure based model (nVBEmodel).
The combination of HDP-based modeland HMM-based model enables to utilize infor-mation of both word-level and character-level.
Wealso show that using nVBE model as initializationmodel could further improve the performance tooutperform the state-of-the-art systems and leadsto improvement in both wordhood judgment anddisambiguation ability.Word segmentation systems are usually eval-uated with metrics like precision, recall and F-Score, regardless of supervised or unsupervised.Following normal practice, we evaluate our modeland compare it with state-of-the-art systems us-ing F-Score.
However, we argue that the abilityto solve segmentation ambiguities is also impor-tant when evaluating different types of unsuper-vised word segmentation systems.This paper is organized as follows.
In Section2, we will introduce several related systems forunsupervised word segmentation.
Then our jointmodel is presented in Section 3.
Section 4 showsour experiment results on the benchmark datasetsand Section 5 concludes the paper.2 Related WorkUnsupervised Chinese word segmentation hasbeen explored in a number of previous works andby various methods.
Most of these methods canbe divided into two categories: goodness measurebased methods and nonparametric Bayesian meth-ods.There have been a plenty of work that is basedon a specific goodness measure.
Zhao and Kit(2008) compared several popular unsupervisedmodels within a unified framework.
They triedvarious types of goodness measures, such as De-scription Length Gain (DLG) proposed by Kit andWilks (1999), Accessor Variety (AV) proposed byFeng et al.
(2004) and Boundary Entropy (Jin andTanaka-Ishii, 2006).
A notable goodness-basedmethod is ESA: ?Evaluation, Selection, Adjust-ment?, which is proposed by Wang et al.
(2011)for unsupervised Mandarin Chinese word segmen-tation.
ESA is an iterative model based on a newgoodness algorithm that adopts a local maximumstrategy and avoids threshold setting.
One disad-vantage of ESA is that it needs to iterate the pro-cess several times on the corpus to get good perfor-mance.
Another disadvantage is the requirementfor a manually segmented training corpus to findbest value for parameters (they called it proper ex-ponent).
Another notable work is nVBE: Mag-istry and Sagot (2012) proposed a model basedon the Variation of Branching Entropy.
By addingnormalization and viterbi decoding, they improveperformance over Jin and Tanaka-Ishii (2006)and remove most of the parameters and thresholdsfrom the model.Nonparametric Bayesian models also achievedstate-of-the-art performance in unsupervised wordsegmentation.
Goldwater et al.
(2009) introduceda unigram and a bigram model for unsupervisedword segmentation, which are based on Dirichletprocess and hierarchical Dirichlet process (Teh etal., 2006) respectively.
The main drawback is thatit needs almost 20,000 iterations before the Gibbssampler converges.
Mochihashi et al.
(2009) ex-tended this method by introducing a nested charac-ter model and an efficient blocked Gibbs sampler.Their method is based on what they called nestedPitman-Yor language model.One disadvantage of goodness measure basedmethods is that they do not have any disambigua-tion ability in theory in spite of their competitiveperformances.
This is because once the goodnessmeasure is given, the decoding algorithm will seg-ment any ambiguous strings into the same wordsequences, no matter what their context is.
Incontrast, nonparametric Bayesian language mod-els aim to segment character string into a ?reason-able?
sentence according to the posterior probabil-ity.
Thus, theoretically, this method should havebetter ability to solve ambiguities over goodnessmeasure based methods.3 Joint ModelIn this section, we will discuss our joint model indetail.8553.1 Combining HDP and HMMIn supervised Chinese word segmentation lit-erature, word-based approaches and character-based approaches often have complementary ad-vantages (Wang et al., 2010).Since the two typesof model try to solve the problem from differentperspectives and by utilizing different levels of in-formation (word level and character level).
In un-supervised Chinese word segmentation literature,the HDP-base model can be viewed as a typi-cal word-based method.
And we can also builda character-based unsupervised model by using ahidden Markov model.
We believe that the HDP-based model and the HMM-based model are alsocomplementary with each other, and a combina-tion of them will take advantage of both and thuscapture different levels of information.Now the problem we are facing is how to com-bine these two models.
To keep the joint modelsimple and involve as little extra parameters aspossible, we combine the two baseline models byjust multiplying their probabilities together andthen renormalizing it.
Let C = c1c2?
?
?
c|C|be astring of characters andW = w1w2?
?
?w|W |is thecorresponding segmented words sequence.
Thenthe conditional probability of the segmentation Wgiven the character string C in our joint model isdefined as:PJ(W |C) =1Z(C)PD(W |C)PM(W |C) (1)where PD(W |C) is the probability from the HDPmodel as given in Equation 6 and PM(W |C)is the probability given by the Bayesian HMMmodel as given in Equation 2.
Z(C) is a nor-malization term to make sure that PJ(W |C) is aprobability distribution.
The combining method isinspired by Hinton (1999), which proved that it ispossible to combine many individual expert mod-els by multiplying the probabilities and then renor-malizing it.
They called it ?product of experts?.We can see that combining models in this waydoes not involve any extra parameters and Gibbssampling can be easily used for model inference.3.2 Bayesian HMMThe dominant method for supervised Chineseword segmentation is character-based modelwhich was first proposed by Xue (2003).
Thismethod treats word segmentation as a taggingproblem, each tag indicates the position of a char-acter within a word.
The most commonly usedtag set is {Single, Begin, Middle, End}.
Specifi-cally, S means the character forms a single word,B/E means the character is the begining/endingcharacter of the word, and M means the charac-ter is in the middle of the word.
Existing modelsare trained on manually annotated data in a super-vised way based on discriminative models such asConditional Random Fields (Peng et al., 2004;Tseng et al., 2005).
Supervised character-basedmethods make full use of character level informa-tion and thus have been very successful in the lastdecade.
However, no unsupervised model has uti-lized character level information in the way as su-pervised method does.We can also build a character-based model forChinese word segmentation using hidden Markovmodel(HMM) as formulated in the followingequation:PM(W |C) =|C|?i=1Pt(ti|ti?1)Pe(ci|ti) (2)where C and W have the same meaning as be-fore.
Pt(ti|ti?1) is the transition probability oftag tigiven its former tag ti?1and Pe(ci|ti) is theemission probability of character cigiven its tag ti.This model can be easily trained with MaximumLikelihood Estimation (MLE) on annotated dataor with Expectation Maximization (EM) on rawtexts.
But using any of this methods will make itdifficult to combine it with the HDP-based model.Instead, we propose a Bayesian HMM for unsu-pervised word segmentation.
The Bayesian HMMmodel is defined as follows:ti|ti?1= t, pt?
Mult(pt)ci|ti= t, et?
Mult(et)pt|?
?
Dirichlet(?)et|?
?
Dirichlet(?
)where ptand etare transition and emission dis-tributions, ?
and ?
are the symmetric parametersof Dirichlet distributions.
Now suppose we haveobserved tagged text h, then the conditional prob-ability PM(wi|wi?1= l, h) can be obtained:PM(wi|wi?1= l, h)=|wi|?j=1Pt(tj|tj?1, h)Pe(cj|tj, h) (3)where < wi?1, wi> is a word bigram, l is the in-dex of word wi?1, cjis the jth character in word856wiand tjis the corresponding tag.Pt(tj|tj?1, h)and Pe(cj|tj, h) are the posterior probabilities,they are given as:Pt(tj|tj?1, h) =n<tj?1,tj>+ ?n<tj?1,?>+ T?
(4)Pe(cj|tj, h) =n<tj,cj>+ ?n<tj,?>+ V ?
(5)where n<tj?1,tj>is the tag bigram count of <tj?1, tj> in h, n<tj,cj>denotes the number of oc-currences of tag tjand character cj, and ?
meansa sum operation.
T and V are the size of charactertag set (we follow the commonly used {SBME}tag set and thus T = 4 in this case) and charactervocabulary.3.3 HDP ModelGoldwater et al.
(2009) proposed a nonparametricBayesian model for unsupervised word segmenta-tion which is based on HDP (Teh et al., 2006).
Inthis model, the conditional probability of the seg-mentation W given the character string C is de-fined as:PD(W |C) =|W |?i=0PD(wi|wi?1) (6)where wiis the ith word in W .
This is actuallya nonparametric bigram language model.
This bi-gram model assumes that each different word hasa different distribution over words following it, butall these different distributions are linked througha HDP model:wi|wi?1= l ?
GlGl?
DP (?1, G0)G0?
DP (?,H)where DP denotes a Dirichlet process.Suppose we have observed segmentation re-sult h, then we can get the posterior probabilityPD(wi|wi?1= l, h) by integrating out Gl:PD(wi|wi?1= l, h)=n<wi?1,wi>+ ?1PD(wi|h)n<wi?1,?>+ ?1(7)where n<wi?1,wi>denotes the total number of oc-currences of the bigram < wi?1, wi> in the ob-servation h. And PD(wi|h) can be got by integrat-ing out G0:PD(wi|h) =twi+ ?H(wi)t+ ?
(8)where twidenotes the number of tables associ-ated with wiin the Chinese Restaurant Franchisemetaphor (Teh et al., 2006), t is the total numberof tables and H(wi) is the base measure of G0.
Infact, H(wi) is the prior distribution over words, soprior knowledge can be injected in this distributionto enhance the performance.In Goldwater et al.
(2009)?s work, the basemeasureH(wi) are defined as a character unigrammodel:H(wi) = (1?
ps)|wi|?1ps?jP (cij)where, psis the probability of generating a wordboundary.
P (cij) is the probability of the jth char-acter cijin word wi, this probability can be esti-mated from the training data using maximum like-lihood estimation.3.4 Initializing with nVBEAmong various goodness measure based models,we choose nVBE (Magistry and Sagot, 2012) toinitialize our Gibbs sampler with its segmentationresults.
nVBE achieved a relatively high perfo-mance over other goodness measure based meth-ods.
And it?s very simple as well as efficient.Theoretically, the Gibbs sampler may be initial-ized at random or using any other methods.
Initial-ization does not make a difference since the Gibbssampler will eventually converge to the posteriordistribution if it iterates as much as possible.
Thisis an essential attribute of Gibbs sampling.
How-ever, we believe that initializing the Gibbs sam-pler with the result of nVBE will benefit us intwo ways.
On one hand, in consideration of itscombination of nonparametric Bayesian methodand goodness-based method, it will improve theoverall performance as well as solve more seg-mentation ambiguities with the help of HDP-basedmodel.
On the other hand, it makes the conver-gence of Gibbs sampling faster.
In practice, ran-dom initialization often leads to extremely slowconvergence.3.5 Inference with Gibbs SamplingIn our proposed joint model, Gibbs sam-pling (Casella and George, 1992) can be easilyused to identify the highest probability segmen-tation from among all possibilities.
FollowingGoldwater et al.
(2009), we can repeatedly samplefrom potential word boundaries.
Each boundary857variable can only take on two possible values, cor-responding to a word boundary or not word bound-ary.For instance, suppose we have obtained a seg-mentation result ?|ci?2ci?1cici+1ci+2|?, where ?and ?
are the words sequences to the left andright and ci?2ci?1cici+1ci+2are characters be-tween them.
Now we are sampling at location ito decide whether there is a word boundary be-tween ciand ci+1.
Denote h1as the hypothesisthat it forms a word boundary (the correspond-ing result is ?w1w2?
where w1= ci?2ci?1ciandw2= ci+1ci+2), and h2as the opposite hypoth-esis (then the corresponding result is ?w?
wherew = ci?2ci?1cici+1ci+2).
The posterior probabil-ity for these two hypotheses would be:P (h1|h?)
?
PD(h1|h?)PM(h1|h?)
(9)P (h2|h?)
?
PD(h2|h?)PM(h2|h?)
(10)where PD(h|h?)
and PM(h|h?)
are the pos-terior probabilities in HDP-based model and inHMM-based model, and h?denotes the currentsegmentation results for all observed data exceptci?2ci?1cici+1ci+2.
Note that the normalizationterm Z(C) can be ignored during inference.
Theposterior probabilities for these two hypotheses inthe HDP-based model is given as:PD(h1|h?)
= PD(w1|wl, h?)?
PD(w2|w1, h?
)PD(wr|w2, h?)
(11)PD(h2|h?)
= PD(w|wl, h?)?
PD(wr|w, h?)
(12)where wl(wr) is the first word to the left (right) ofw.
And the posterior probabilities for the BayesianHMM model is given as:PM(h1|h?
)?i+2?j=i?2Pt(tj|tj?1, h?
)Pe(cj|tj, h?)
(13)PM(h2|h?
)?i+2?j=i?2Pt(tj|tj?1, h?
)Pe(cj|tj, h?)
(14)where Pt(tj|tj?1, h?)
and Pe(cj|tj, h?)
are givenin Equation 4 and 5.
The difference is that un-der hypothesis h1, ci?2ci?1cici+1ci+2are taggedas ?BMEBE?
and under hypothesis h2as ?BM-MME?.Once the Gibbs sampler is converged, a natu-ral way to is to treat the result of last iteration asthe final segmentation result, since each set of as-signments to the boundary variables uniquely de-termines a segmentation.4 ExperimentsIn this section, we test our joint model on PKUand MSRA datesets provided by the Second Seg-mentation Bake-off (SIGHAN 2005) (Emerson,2005).
Most previous works reported their resultson these two datasets, this will make it convenientto directly compare our joint model with theirs.4.1 SettingThe second SIGHAN Bakeoff provides severallarge-scale labeled data for evaluating the per-formance of Chinese word segmentation systems.Two of the four datasets are used in our exper-iments.
Both of the dataset contains only sim-plified Chinese.
Table 1 shows the statistics ofthe two selected corpus.
For development set, werandomly select a small subset (about 10%) ofthe training data.
Specifically, 2000 sentences areselected for PKU corpus and 8000 sentences forMSRA corpus.
The rest training data plus the testset is then combined for segmentation but only testdata is used for evaluation.
The development set isused to tune parameters of the HDP-based modeland HMM-based model separately.
Since our jointmodel does not involve any additional parameters,we reuse the parameters of the HDP-based modeland HMM-based model in the joint model.
Specif-ically, we set ?1= 1000.0, ?
= 10.0, ps= 0.5 forthe HDP-based model and set ?
= 1.0, ?
= 0.01for the HMM-based model.For evaluation, we use standard F-Score onwords for all following experiments.
F-Score isthe harmonic mean of the word precision and re-call.
Precision is given as:P =#correct words in result#total words in resultand recall is given as:R =#correct words in result#total words in gold corpusthen F-Score is calculated as:F =2?R ?
FR + F858Corpus TrainingSize (words) TestSize (words)PKU 1.1M 104KMSRA 2.37M 107KTable 1: Statistics of training and testing dataHuang and Zhao (2007) provided an empiricalmethod to estimate the consistency between thefour different segmentation standards involved inthe Bakeoff-3.
A lowest consistency rate 84.8%is found among the four standards.
Zhao and Kit(2008) considered this figure as the upper boundfor any unsupervised Chinese word segmentationsystems.
We also use it as the topline in our com-parison.4.2 Prior Knowledge UsedWhen it comes to the evaluation and compari-son for unsupervised word segmentation systems,an important issue is what kind of pre-processingsteps and prior knowledge are needed.
To be fullyunsupervised, any prior knowledge such as punc-tuation information, encoding scheme and wordlength could not be used in principle.
Neverthe-less, information like punctuation can be easily in-jected to most existing systems and significantlyenhance the performance.
The problem we arefaced with is that we don?t know for sure whatkind of prior information are used in other sys-tems.
One may use a small punctuation set tosegment a long sentence into shorter ones, whileanother may write simple regular expressions toidentify dates and numbers.
Lot of work we com-pare to don?t even mention this subject.Fortunately, we notice that Wang et al.
(2011)provided four kinds of preprocessings (they callsettings).
In their settings 1 and 2, punctuationand other encoding information are not used.
Insetting 3, punctuation is used to segment charac-ter sequences into sentences, and both punctuationand other encoding information are used in setting4.
Then the results reported in Magistry and Sagot(2012) relied on setting 3 and setting 4.
In orderto make the comparison as fair as possible, we usesetting 3 in our experiment, i.e., only a punctua-tion set for simplified Chinese is used in all ourexperiments.
We will compare our experiment re-sults to previous work on the same setting if theyare provided .4.3 Experiment ResultsTable 2 summarizes the F-Scores obtained by dif-ferent models on PKU and MSRA corpus, as wellas several state-of-the-art systems.
Detailed infor-mation about the presented models are listed asfollows:?
nVBE: the model based on Variation ofBranching Entropy in Magistry and Sagot(2012).
We re-implement their model on set-ting 31.?
HDP: the HDP-based model proposed byGoldwater et al.
(2009), initialized randomly.?
HDP+HMM: the model combining HDP-based model and HMM-based model as pro-posed in Section 3, initialized randomly.?
HDP+nVBE: the HDP-based model, initial-ized with the results of nVBE model.?
Joint: the ?HDP+HMM?
model initializedwith nVBE model.?
ESA: the model proposed in Wang et al.
(2011), as mentioned above, the conductedexperiments on four different settings, we re-port their results on setting 3.?
NPY(2): the 2-gram language model pre-sented by Mochihashi et al.
(2009).?
NPY(3): the 3-gram language model pre-sented by Mochihashi et al.
(2009).For all of our Gibbs samplers, we run 5 times toget the averaged F-Scores.
We also give the vari-ance of the F-Scores in Table 2.
For each run, wefind that random initialization takes around 1,000iterations to converge, while initialing with nVBEonly takes as few as 10 iterations.
This makes1The results we got with our implementation is slightlylower than what was reported in Magistry and Sagot (2012).According to Pei et al.
(2013), they had contacted the authorsand confirmed that the higher results was due to a bug in code.So we report the results with our bug free implementation asPei et al.
(2013) did.
Our reported results are identical tothose of Pei et al.
(2013)859SystemPKU MSRAR P F R P FnVBE 78.3 77.5 77.9 79.1 77.3 78.2HDP 69.0 68.4 68.7(0.012) 70.4 69.4 69.9(0.020)HDP+HMM 77.5 73.2 75.3(0.005) 79.9 73.0 76.3(0.013)HDP+nVBE 80.7 77.9 79.3(0.012) 81.8 77.3 79.5(0.005)Joint 83.1 79.2 81.1(0.002) 84.2 79.3 81.7(0.005)ESA N/A N/A 77.4 N/A N/A 78.4NPY(2) N/A N/A N/A N/A N/A 80.2NPY(3) N/A N/A N/A N/A N/A 80.7Topline N/A N/A 84.8 N/A N/A 84.8Table 2: Experiment results and comparison to state-of-the-art systems.
The figures in parentheses denotethe variance the of F-Scores.our joint model very efficient and possible to workin practical applications as well.
At last, a singlesample (the last one) is used for evaluation.From Table 2, we can see that the jointmodel (Joint) outperforms all the presented sys-tems in F-Score on all testing corpora.
Specifi-cally, comparing ?HDP+HMM?
with ?HDP?, theformer model increases the overall F-Score from68.7% to 75.3% (+6.6%) in PKU corpora andfrom 69.9% to 76.3% (+6.4%) in MSRA corpora,which proves that the character information inthe HMM-based model can actually enhance theperformance of the HDP-based model.
Compar-ing ?HDP+nVBE?
with ?HDP?, the former modelalso increases the overall F-Score by 10.6%/9.6%in PKU/MSRA corpora, which demonstrates thatinitializing the HDP-based model with nVBE willimprove the performance by a large margin.
Fi-nally, the joint model ?Joint?
take advantage fromboth from the character-based HMM model andthe nVBE model, it achieves a F-Score of 81.1%on PKU and 81.7% on MSRA.
This result outper-forms all its component baselines such as ?HDP?,?HDP+HMM?
and ?HDP+nVBE?.Our joint model also shows competitive advan-tages over several state-of-the-art systems.
Com-pared with nVBE,the F-Score increases by 3.2%on PKU corpora and by 3.5% on MSRA cor-pora.
Compared with ESA, the F-Score increasesby 3.7%/3.3% in PKU/MSRA corpora.
Lastly,compared to the nonparametric Bayesian models(NPY(n)), our joint model still increases the F-Score by 1.5% (NPY(2)) and 1.0% (NPY(3)) onMSRA corpora.
Moreover, compared with theempirical topline figure 84.8%, our joint modelachieves a pretty close F-Score.
The differencesare 3.7% on PKU corpora and 3.1% on MSRAcorpora.An phenomenon we should pay attention to isthe poor performance of the HMM-based model.With our implementation of the Bayesian HMM,we achieves a 34.3% F-Score on PKU corpora anda 34.9% F-Score on MSRA corpora, just slightlybetter than random segmentation.
The result showthat the hidden Markov Model alone is not suit-able for character-based Chinese word segmenta-tion problem.
However, it still substantially con-tributes to the joint model.We find that the variance of the results are rathersmall, this shows the stability of our Gibbs sam-plers.
From the segmentation results generatedby the joint model, we also found that quite alarge amount of errors it made are related to dates,numbers (both Chinese and English) and Englishwords.
This problem can be easily addressed dur-ing preprocessing by considering encoding infor-mation as previous work, and we believe this willbring us much better performance.4.4 Disambiguation AbilityPrevious unsupervised work usually evaluatedtheir models using F-score, regardless of goodnessmeasure based model or nonparametric Bayesianmodel.
However, segmentation ambiguity isa very important factor influencing accuracy ofChinese word segmentation systems (Huang andZhao, 2007).
We believe that the disambigua-tion ability of the models should also be consid-ered when evaluating different types of unsuper-vised segmentation systems, since different typeof models shows different disambiguation ability.We will compare the disambiguation ability of dif-860ferent systems in this section.In general, there are mainly two kinds of ambi-guity in Chinese word segmentation problem:?
Combinational Ambiguity: Given charac-ter strings ?A?
and ?B?, if ?A?, ?B?, ?AB?are all in the vocabulary, and ?AB?
or ?A-B?
(here ?-?
denotes a space) occurred in the realtext,then ?AB?
can be called a combinationalambiguous string.?
Overlapping Ambiguity: Given characterstrings ?A?, ?J?
and ?B?, if ?A?, ?B?, ?AJ?and ?JB?
are all in the vocabulary, and ?A-JB?
or ?AJ-B?
occurred in the real text, then?AJB?
can be called an overlapping ambigu-ous string.We count the total number of mistakes differ-ent systems made at ambiguous strings (the vo-cabulary is obtained from the gold standard an-swer of testing set).
As we have mentioned inSection 2, goodness measure based methods suchas nVBE do not have any disambiguation abilityin theory.
Our observation is identical to this ar-gument.
We find that nVBE always segments am-biguous strings into the same result.
Take a combi-national string ??k?
as an example, ??
(just)?,?k (have)?
and ?
?k (only)?
are all in the vo-cabulary.
In the PKU test set, this string occurs14 times as ?
?-k (just have)?
and 18 times as?
?k (only)?, 32 times in total.
nVBE segmentsall the 32 strings into ?
?k (only)?
(i.e.
18 ofthem are correct), while the joint model segmentsit 22 times as ?
?k (only)?
and 10 times as ?
?-k (just have)?
according to its context, and 24 ofthem are correct.Table 3 and 4 show the statistics of combi-national ambiguity and overlapping ambiguity re-spectively.
The numbers in parentheses denote thetotal number of ambiguous strings.
From thesetables, we can see that HDP+nVBE makes lessmistakes than nVBE in most circumstances, ex-cept that it solves less combinational ambigui-ties on MSRA corpora.
But our proposed jointmodel solves the most combinational and over-lapping ambiguities, on both PKU and MSRAcorpora.
Specifically, compared to nVBE, thejoint model correctly solves 171/871 more com-binational ambiguities on PKU/MSRA corpora,which is a 0.6%/13.8% relative error reduction.It also solves 28/45 more overlapping ambiguitieson PKU/MSRA corpora, which is a 11.5%/23.4%relative error reduction.
This indicates that thejoint model has a stronger ability of disambigua-tion over the compared systems.System PKU(35371) MSRA(38506)nVBE 8087 7236HDP+nVBE 7970 7500Joint 7916 6305Table 3: Statistics of combinational ambiguity.This table shows the total number of mistakesmade by different systems at combinational am-biguous strings.
The numbers in parentheses de-note the total number of combinational ambiguousstrings.System PKU(603) MSRA(467)nVBE 244 192HDP+nVBE 239 164Joint 216 157Table 4: Statistics of overlapping ambiguity.
Thistable shows the total number of mistakes madeby different systems at overlapping ambiguousstrings.
The numbers in parentheses denote the to-tal number of overlapping ambiguous strings.4.5 Statistical Significance TestThe main results presented in Table 2 has shownthat our proposed joint model outperforms thetwo baselines as well as state-of-the-art systems.But it is also important to know if the improve-ment is statistically significant over these sys-tems.
So we conduct statistical significance testsof F-scores among these various models.
Follow-ing Wang et al.
(2010), we use the bootstrappingmethod (Zhang et al., 2004).Here is how it works: suppose we have a testingset T0to test several word segmentation systems,there are N testing examples (sentences or line ofcharacters) in T0.
We create a new testing set T1with N examples by sampling with replacementfrom T0, then repeat these process M ?
1 times.And we will have a total M +1 testing sets.
In ourtest procedures, M is set to 2000.Since we just implement our joint model andits component models, we can not generate pairedsamples for other models (i.e.
ESA and NPY(n)).Instead, we follow Wang et al.
(2010)?s methodand first calculate the 95% confidence interval for861our proposed model.
Then other systems can becompared with the joint model in this way: if theF-score of system B doesn?t fall into the 95% con-fidence interval of system A, they are consideredas statistically significantly different from eachother.For all significant tests, we measure the 95%confidence interval for the difference betweentwo models.
First, the test results show that?HDP+nVBE?
and ?HDP+HMM?
are both sig-nificantly better than ?HDP?.
Second, the?Joint?
model significantly outperforms all itscomponent models, including ?HDP?, ?nVBE?,?HDP+nVBE?
and ?HDP+HMM?.
Finally, thecomparison also shows that the joint model signif-icantly outperforms state-of-the-art systems likeESA and NPY(n).5 ConclusionIn this paper, we proposed a joint model for un-supervised Chinese word segmentation.
Our jointmodel is a combination of the HDP-based model,which is a word-based model, and HMM-basedmodel, which is a character-based model.
Theway we combined these two component base-lines makes it natural and simple to inference withGibbs sampling.
Then the joint model take ad-vantage of a goodness-based method (nVBE) byusing it to initialize the sampler.
Experiment re-sults conducted on PKU and MSRA datasets pro-vided by the second SIGHAN Bakeoff show thatthe proposed joint model not only outperforms thebaseline systems but also achieves better perfor-mance (F-Score) over several state-of-the-art sys-tems.
Significance tests showed that the improve-ment is statistically significant.
Analysis also in-dicates that the joint model has a stronger abil-ity to solve ambiguities in Chinese word segmen-tation.
In summary, the joint model we pro-posed combines the strengths of character-basedmodel, nonparametric Bayesian language modeland goodness-based model.AcknowledgmentsThe contact author of this paper, according tothe meaning given to this role by Key Labora-tory of Computational Linguistics, Ministry of Ed-ucation, School of Electronics Engineering andComputer Science, Peking University, is BaobaoChang.
And this work is supported by NationalNatural Science Foundation of China under GrantNo.
61273318 and National Key Basic ResearchProgram of China 2014CB340504.ReferencesGeorge Casella, Edward I. George.
1992.
Explain-ing the Gibbs sampler.
The American Statistician,46(3): 167-174.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
Proceedings ofthe Fourth SIGHANWorkshop on Chinese LanguageProcessing, 133.
MLA.Haodi Feng, Kang Chen, Xiaotie Deng, et al.
2004.Accessor variety criteria for Chinese word extractionComputational Linguistics, 30(1): 75-93.Sharon Goldwater, Thomas L. Griffiths, Mark Johnson.2009.
A Bayesian framework for word segmenta-tion: Exploring the effects of context.
Cognition112(1): 21-54.Geoffrey E. Hinton.
1999.
Products of experts.
Arti-ficial Neural Networks.
Ninth International Confer-ence on Vol.
1.Changning Huang, Hai Zhao.
2007.
Chinese wordsegmentation: A decade review.
Journal of ChineseInformation Processing, 21(3): 8-20.Zhihui Jin, Kumiko Tanaka-Ishii.
2006.
Unsupervisedsegmentation of Chinese text by use of branchingentropy.
Proceedings of the COLING/ACL on Mainconference poster sessions, page 428-435.Chunyu Kit, Yorick Wilks.
1999.
Unsupervisedlearning of word boundary with description lengthgain.
Proceedings of the CoNLL99 ACL Workshop.Bergen, Norway: Association for ComputationalLinguis-tics, page 1-6.Pierre Magistry, Benoit Sagot.
2012.
Unsuper-vized word segmentation: the case for mandarinchinese.
Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics:Short Papers-Volume 2.
Association for Computa-tional Linguistics, page 383-387.Daichi Mochihashi, Takeshi Yamada, Naonori Ueda.2009.
Bayesian unsupervised word segmentationwith nested Pitman-Yor language modeling.
Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP: Volume 1-Volume 1.
Association for Com-putational Linguistics, page 100-108.Wenzhe Pei, Dongxu Han, Baobao Chang.
2013.
ARefined HDP-Based Model for Unsupervised Chi-nese Word Segmentation.
Chinese ComputationalLinguistics and Natural Language Processing Basedon Naturally Annotated Big Data.
Springer BerlinHeidelberg, page 44-51.862Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, etal.
2006.
Sharing Clusters among Related Groups:Hierarchical Dirichlet Processes.
NIPS.Fuchun Peng, Fangfang Feng, Andrew McCallum.2004.
Chinese segmentation and new word detec-tion using conditional random fields.
Proceedingsof COLING, page 562-568.Huihsin Tseng, Pichuan Chang, Galen Andrew, et al.2005.
A conditional random field word segmenterfor sighan bakeoff 2005.
Proceedings of the FourthSIGHAN Workshop on Chinese Language Process-ing, Vol.
171.Kun Wang, Chengqing Zong, Keh-Yih Su.
2010.
Acharacter-based joint model for Chinese word seg-mentation.
Proceedings of the 23rd InternationalConference on Computational Linguistics.
Associa-tion for Computational Linguistics, page 1173-1181.Hanshi Wang, Jian Zhu, Shiping Tang, et al.
2011.
Anew unsupervised approach to word segmentation.Computational Linguistics, 37(3): 421-454.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1): 29-48.Hai Zhao, Chunyu Kit.
2008.
An Empirical Compar-ison of Goodness Measures for Unsupervised Chi-nese Word Segmentation with a Unified Framework.IJCNLP, page 6-16.Ying Zhang, Stephan Vogel, Alex Waibel.
2004.
In-terpreting BLEU/NIST scores: How much improve-ment do we need to have a better system?
LREC.863
