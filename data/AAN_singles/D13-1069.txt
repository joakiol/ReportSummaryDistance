Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 736?746,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAutomatically Determining a Proper Length for Multi-documentSummarization: A Bayesian Nonparametric ApproachTengfei Ma and Hiroshi NakagawaThe University of Tokyo7-3-1 Hongo, Bunkyo-ku, Tokyo{matf@r., nakagawa@}dl.itc.u-tokyo.ac.jpAbstractDocument summarization is an important taskin the area of natural language processing,which aims to extract the most important in-formation from a single document or a clus-ter of documents.
In various summarizationtasks, the summary length is manually de-fined.
However, how to find the proper sum-mary length is quite a problem; and keepingall summaries restricted to the same lengthis not always a good choice.
It is obvi-ously improper to generate summaries withthe same length for two clusters of docu-ments which contain quite different quantityof information.
In this paper, we proposea Bayesian nonparametric model for multi-document summarization in order to automat-ically determine the proper lengths of sum-maries.
Assuming that an original documentcan be reconstructed from its summary, wedescribe the ?reconstruction?
by a Bayesianframework which selects sentences to forma good summary.
Experimental results onDUC2004 data sets and some expanded datademonstrate the good quality of our sum-maries and the rationality of the length deter-mination.1 IntroductionText summarization is the process of generating ashort version of a given text to indicate its main top-ics.
As the number of documents on the web expo-nentially increases, text summarization has attractedincreasing attention, because it can help people getthe most important information within a short time.In most of the existing summarization systems,people need to first define a constant length to re-strict all the output summaries.
However, in manycases it is improper to require all summaries are ofthe same length.
Take the multi-document summa-rization as an example, generating the summariesof the same length for a 5-document cluster and a50-document cluster is intuitively improper.
Morespecifically, consider two different clusters of doc-uments: one cluster contains very similar articleswhich all focus on the same event at the same time;the other contains different steps of the event buteach step has its own topics.
The former cluster mayneed only one or two sentences to explain its infor-mation, while the latter needs to include more.Research on summary length dates back in thelate 90s.
Goldstein et al(1999) studied the char-acteristics of a good summary (single-documentsummarization for news) and showed an empiri-cal distribution of summary length over documentsize.
However, the length problem has been grad-ually ignored later, since researchers need to fixthe length so as to estimate different summarizationmodels conveniently.
A typical instance is the Doc-ument Understanding Conferences (DUC)1, whichprovide authoritative evaluation for summarizationsystems.
The DUC conferences collect news arit-cles as the input data and define various summariza-tion tasks, such as generic multi-document summa-rization, query-focused summarization and updatesummarization.
In all the DUC tasks, the output isrestricted within a length.
Then human-generated1After 2007, the DUC tasks are incorporated into the TextAnalysis Conference (TAC).736summaries are provided to evaluate the results of dif-ferent summarization systems.
Limiting the lengthof summaries contributed a lot to the developmentof summarization techniques, but as we discussedbefore, in many cases keeping the summaries of thesame size is not a good choice.Moreover, even in constant-length summariza-tion, how to define a proper size of summaries forthe summarization tasks is quite a problem.
Whydoes DUC2007 main task require 250 words whileUpdate task require 100 words?
Is it reasonable?A short summary may sacrifice the coverage, whilea long summary may cause redundance.
Automati-cally determining the best size of summaries accord-ing to the input documents is valuable, and it maydeepen our understanding of summarization.In this work, we aim to find the proper lengthfor document summarization automatically and gen-erate varying-length summaries based on the doc-ument itself.
The varying-length summarization ismore robust for unbalanced clusters.
It can alsoprovide a recommended size as the predefined sum-mary length for general constant-length summariza-tion systems.
We advance a Bayesian nonparametricmodel of extractive multi-document summarizationto achieve this goal.
As far as we are concerned, it isthe first model that can learn appropriate lengths ofsummaries.Bayesian nonparametric (BNP) methods are pow-erful tools to determine the size of latent vari-ables (Gershman and Blei, 2011).
They let the data?speak for itself?
and allow the dimension of la-tent variables to grow with the data.
In order tointegrate the BNP methods into document summa-rization, we follow the assumption that the originaldocuments should be recovered from the reconstruc-tion of summaries (Ma and Wan, 2010; He et al2012).
We use the Beta process as a prior to gen-erate binary vectors for selecting active sentencesthat reconstruct the original documents.
Then weconstruct a Bayesian framework for summarizationand use the variational approximation for inference.Experimental results on DUC2004 dataset demon-strate the effectiveness of our model.
Besides, wereorganize the original documents to generate somenew datasets, and examine how the summary lengthchanges on the new data.
The results prove that oursummary length determination is rational and neces-sary on unbalanced data.2 Related Work2.1 Research on Summary LengthSummary length is an important aspect for gener-ating and evaluating summaries.
Early research onsummary length (Goldstein et al 1999) focused ondiscovering the properties of human-generated sum-maries and analyzing the effect of compression ratio.It demonstrated that an evaluation of summarizationsystems must take into account both the compres-sion ratios and the characteristics of the documents.Radev and Fan (2000) compared the readability andspeedup in reading time of 10% summaries and 20%summaries2 for topic sets with different number ofdocuments.
Sweeney et al(2008) developed an in-cremental summary containing additional sentencesthat provide context.
Kaisser et al(2008) studiedthe impact of query types on summary length ofsearch results.
Other than the content of originaldocuments, there are also some other factors affect-ing summary length especially in specific applica-tions.
For example, Sweeney and Crestani (2006)studied the relation between screen size and sum-mary length on mobile platforms.
The conclusion oftheir work is the optimal summary size always fallsinto the shorter one regardless of the screen size.In sum, the previous works on summary lengthmostly put their attention on the empirical study ofthe phenomenon, factors and impacts of summarylength.
None of them automatically find the bestlength, which is our main task in this paper.
Nev-ertheless, they demonstrated the importance of sum-mary length in summarization and the reasonabilityof determining summary length based on content ofnews documents (Goldstein et al 1999) or searchresults (Kaisser et al 2008).
As our model is mainlyapplied for generic summarization of news articles,we do not consider the factor of screen size in mo-bile applications.2.2 BNP Methods in Document SummarizationBayesian nonparametric methods provide aBayesian framework for model selection andadaptation using nonparametric models (Gershman210% and 20% are the compression rates, and the documentsare from search results in information retrieval systems.737and Blei, 2011).
A BNP model uses an infinite-dimensional parameter space, but invokes only afinite subset of the available parameters on anygiven finite data set.
This subset generally growswith the data set.
Thus BNP models address theproblem of choosing the number of mixture compo-nents or latent factors.
For example, the hierarchicalDirichlet process (HDP) can be used to infer thenumber of topics in topic models or the number ofstates in the infinite Hidden Markov model (Teh etal., 2006).Recently, some BNP models are also involved indocument summarization approaches (Celikyilmazand Hakkani-Tu?r, 2010; Chang et al 2011; Darlingand Song, 2011).
BNP priors such as the nested Chi-nese restaurant process (nCRP) are associated withtopic analysis in these models.
Then the topic dis-tributions are used to get the sentence scores andrank sentences.
BNP here only impacts the numberand the structure of the latent topics, but the sum-marization framework is still constant-length.
OurBNP summarization model differs from the previousmodels.
Besides using the HDP for topic analysis,our approach further integrates the beta process intosentence selection.
The BNP method in our modelare directly used to determine the number of sum-mary sentences but not latent topics.3 BNP SummarizationIn this section, we first introduce the BNP priorswhich will be used in our model.
Then we proposeour model called BNP summarization.3.1 The Beta Process and the Bernoulli processThe beta process(BP) (Thibaux and Jordan, 2007;Paisley and Carin, 2009) and the related Indian buf-fet process(IBP) (Griffiths and Ghahramani, 2005)are widely applied to factor/feature analysis.
Bydefining the infinite dimensional priors, these factoranalysis models need not to specify the number oflatent factors but automatically determine it.Definition of BP (Paisley et al 2010): Let B0bea continuous measure on a space ?
and B0(?)
= ?.If Bk is defined as follows,Bk =N?k=1?k?
?k,?k ?
Beta(??N,?(1?
?N))?k ?1?B0(1)(where ?
?kis the atom at the location ?k; and ?
is apositive scalar), then as N ?
?, Bk ?
B and B isa beta process: B ?
BP (?B0).Finite Approximation: The beta process is de-fined on an infinite parameter space, but sometimeswe can also use its finite approximation by sim-ply setting N to a large number (Paisley and Carin,2009).Bernoulli Process: The beta process is conju-gate to a class of Bernoulli processes, denoted byX ?
Bep(B).
If B is discrete, of the form in(1), then X =?k bk?
?k where the bk are indepen-dent Bernoulli variables with the probability p(bk =1) = ?k.
Due to the conjugation between thebeta process priors and Bernoulli process, the pos-terior of B given M samples X1, X2, ...XM whereXi ?
Bep(B)fori = 1, , ,M. is also a beta processwhich has updated parameters:B|X1, X2, ..., XM?
BP (?+M, ?
?+MB0 +1c+M?iXi) (2)Application of BP: Furthermore, marginalizingover the beta process measure B and taking ?
=1, provides a predictive distribution on indicatorsknown as the Indian buffet process (IBP) (Thibauxand Jordan, 2007).
The beta process or the IBP isoften used in a feature analysis model to generateinfinite vectors of binary indicator variables(Paisleyand Carin, 2009), which indicates whether a featureis used to represent a sample.
In this paper, we usethe beta process as the prior to select sentences.3.2 Framework of BNP SummarizationMost existing approaches for generic extractivesummarization are based on sentence ranking.
How-ever, these methods suffer from a severe problemthat they cannot make a good trade-off betweenthe coverage and minimum redundancy (He et al7382012).
Some global optimization algorithms are de-veloped, instead of greedy search, to select the bestoverall summaries (Nenkova and McKeown, 2012).One approach to global optimization of summariza-tion is to regard the summarization as a reconstruc-tion process (Ma and Wan, 2010; He et al 2012).
Considering a good summary must catch most ofthe important information in original documents, theoriginal documents are assumed able to be recov-ered from summaries with some information loss.Then the summarization problem is turned into find-ing the sentences that cause the least reconstructionerror (or information loss).
In this paper, we fol-low the assumption and formulate summarization asa Bayesian framework.First we review the models of (Ma and Wan,2010) and (He et al 2012).
Given a cluster ofM documents x1, x2, ..., xM and the sentence setcontained in the documents as S = [s1, s2, ..., sN ],we denote all corresponding summary sentences asV = [v1, ..., vn], where n is the number of summarysentences and N is the number of all sentences inthe cluster.
A document xi and a sentence vi or sihere are all represented by weighted term frequencyvectors in the space Rd, where d is the number oftotal terms (words).Following the reconstruction assumption, a can-didate sentence vi can be approximated by thelinear combination of summary sentences: si ?nj=1 w?jvj , where w?j is the weight for summarysentence vj .
Thus the document can also be ap-proximately represented by a linear combination ofsummary sentences (because it is the sum of the sen-tences).xi n?j=1wjvj .
(3)Then the work in (He et al 2012) aims to findthe summary sentence set that can minimize the re-construction error?Ni=1 ||si ?
?nj=1 w?jvj ||2; whilethe work in (Ma and Wan, 2010) defines the prob-lem as finding the sentences that minimize the dis-tortion between documents and its reconstructiondis(xi,?nj=1 wjvj) where this distortion functioncan also be a squared error function.Now we consider the reconstruction for each doc-ument, if we see the document xi as the dependentvariable, and the summary sentence set S as theindependent variable, the problem to minimize thereconstruction error can be seen as a linear regres-sion model.
The model can be easily changed to aBayesian regression model by adding a zero-meanGaussian noise  (Bishop, 2006), as follows.xi =n?j=1wjvj + i (4)where the weights wj are also assigned a Gaussianprior.The next step is sentence selection.
As our sys-tem is an extractive summarization model, all thesummary sentences are from the original documentcluster.
So we can use a binary vector zi =<zi1, ..., ziN >T to choose the active sentences V(i.e.
summary sentences) from the original sen-tence set S. The Equation (4) is turned into xi =?Nj=1 ?ij ?zijsj+i.
Using a beta process as a priorfor the binary vector zi, we can automatically inferthe number of active component associated with zi.As to the weights of the sentences, we use a randomvector ?i which has the multivariate normal distri-bution because of the conjugacy.
?i ?
RN is anextension to the weights {w1, ...wn} in (4).Integrating the linear reconstruction (4) and thebeta process3 (1), we get the complete process ofsummary sentence selection as follows.xi = S(?i ?
zi) + iS = [s1, s2, ..., sN ]zij ?
Bernoulli(?j)?j ?
Beta(??N,?(1?
?N))?i ?
N (0, ?2?I)i ?
N (0, ?2 I) (5)where N is the number of sentences in the wholedocument cluster.
The symbol ?
represents the ele-mentwise multiplication of two vectors.One problem of the reconstruction model is thatthe word vector representation of the sentences aresparse, which dramatically increase the reconstruc-tion error.
So we bring in topic models to reduce the3We use the finite approximation because the number of sen-tences is large but finite739dimension of the data.
We use a HDP-LDA (Teh etal., 2006) to get topic distributions for each sentence,and we represent the sentences and documents asthe topic weight vectors instead of word weight vec-tors.
Finally xi is a K-dimensional vector and S isa K ?N matrix, where K is the number of topics intopic models.4 Variational InferenceIn this section, we derive a variational Bayesian al-gorithm for fast inference of our sentence selec-tion model.
Variational inference (Bishop, 2006)is a framework for approximating the true posteriorwith the best from a set of distributions Q : q?
=argminq?QKL(q(Z)|p(Z|X)).
Suppose q(Z) canbe partitioned into disjoint groups denoted by Zj ,and the q distribution factorizes with respect to thesegroups: q(Z) =?Mj=1 q(Zj).
We can obtain a gen-eral expression for the optimal solution q?j (Zj) givenbyln q?j (Zj) = Ei =j [ln p(X,Z)] + const.
(6)where Ei =j [ln p(X,Z)] is the expectation of the log-arithm of the joint probability of the data and latentvariables, taken over all variables not in the parti-tion.
We will therefore seek a consistent solutionby first initializing all of the factors qj(Zj) appro-priately and then cycling through the factors and re-placing each in turn with a revised estimate given by(6) evaluated using the current estimates for all ofthe other factors.Update for Zp(zij |?j , xi, S, ?i) ?
p(xi|zij , sj , ?i)p(zij |?j)We use q(zij) to approximate the posterior:q(zij)?
exp{E[ln(p(xi|zij , z?ji , S, ?i)) + ln(p(zij |?))]}?
exp{E[ln(?j)]}?exp{E[?12?2(x?ji ?
sjzij?ij)T (x?ji ?
sjzij?ij)]}?
exp{ln(?j)}?exp{?
(?2ij ?
z2ij ?
sTj sj ?
2?ij ?
zij ?
sjT?
x?ji)2?2}(7)where x?ji = xi ?
S?j(?
?ji ?
z?ji ), and the symbol?
indicates the expectation value.
The ?2ij can beextended to this form:?2ij = ?ij2+?ji (8)where ?ji means the jth diagonal element of ?iwhich is defined by Equation 13.As zi is a binary vector, we only calculate theprobability of zij = 1 and zij = 0.q(zij = 1) ?
exp{ln(?j)} ?exp{?12?2(?2ij ?
sTj sj ?
2?ij ?
sjT?
x?ji)}q(zij = 0) ?
exp{ln(1?
?j)} (9)The expectations can be calculated asln(?j) = ?(?
?N+ nj)?
?
(?+M) (10)ln(1?
?j) = ?(?(1?
?N)+M ?nj)??
(?+M)(11)where nj =?Mi=1 zij .Update for ?p(?j |Z) ?
p(?j |?, ?,N)p(Z|?j)Because of the conjugacy of the beta to Bernoullidistribution, the posterior of ?
is still a beta distribu-tion:?j ?
Beta(?
?N+ nj , ?(1?
?N) +M ?
nj) (12)Update for ?p(?i|xi, Z, S) ?
p(xi|?i, Z, S)p(?i|?2?
)The posterior is also a normal distribution with mean?i and covariance ?i.
?i =(1?2S?iTS?i +1?2?I)?1(13)?i = ?i(1?2S?iTxi)(14)Here S?i ?
S ?
z?i and z?i ?
[zi, ..., zi]T is a K ?
Nmatrix with the vector zi repeated K(the number ofthe latent topics) times.S?i = S ?
z?i (15)740S?iTS?i = (STS) ?
(zi ?
ziT +Bcovi) (16)Bcovi = diag[zi1(1?
zi1), ..., ziN (1?
ziN )] (17)Update for ?2p(?2 |?, X, Z, S) ?
p(X|?, Z, S, ?2 )p(?2 )By using a conjugate prior, inverse gamma priorInvGamma(u, v), the posterior can be calculatedas a new inverse gamma distribution with parame-tersu?
= u+MK/2v?
= v +12M?i=1(||xi ?
S(zi ?
?i)||+ ?i)(18)where?i =?Nj=1(z2ij ?
?2ij ?
sTj sj ?
zij2?
?ij2?
sTj sj)+?j =l zij ?
zil ?
?i,jl ?
sTj slUpdate for ?2?p(?2?|?)
?
p(?|?2?)p(?2?
)By using a conjugate prior, inverse gamma priorInvGamma(e, f), the posterior can be calculatedas a new inverse gamma distribution with parame-terse?
= e+MN/2f ?
= f +12M?i=1((?
)T?+ trace(?
?i))(19)5 ExperimentsTo test the capability of our BNP summarization sys-tems, we design a series of experiments.
The aim ofthe experiments mainly includes three aspects:1.
To demonstrate the summaries extracted by ourmodel have good qualities and the summarylength determined by the model is reasonable.2.
To give examples where varying summarylength is necessary.3.
To observe the distribution of summary length.We evaluate the performance on the dataset ofDUC2004 task2.
The data contains 50 documentclusters, with 10 news articles in each cluster.
Be-sides, we construct three new datasets from theDUC2004 dataset to further prove the advantage ofvariable-length summarization.
We separate eachcluster in the original dataset into two parts whereeach has 5 documents, hence getting the SeparateDataset; Then we randomly combine two origi-nal clusters in the DUC2004 dataset, and get twodatasets called Combined1 and Combined2.
Thuseach of the clusters in the combined datasets include20 documents with two different themes.5.1 Evaluation of Summary QualitiesFirst, we implement our BNP summarization modelon the DUC2004 dataset, with summary length notlimited.
At the topic analysis step, we use the HDPmodel and follow the inference in (Teh et al 2006).For the sentence selection step, we use the varia-tional inference described in Section 4, where theparameters in the beta process (5) are set as ?
=1, ?
= 1.
The summaries that we finally generatehave an average length of 164 words.
We design sev-eral popular unsupervised summarization systemsand compare them with our model.?
The Random model selects sentences randomlyfor each document cluster.?
The MMR (Carbonell and Goldstein, 1998)strives to reduce redundancy while maintainingrelevance.
For generic summarization, we re-place the query relevance with the relevance todocuments.?
The Lexrank model (Erkan and Radev, 2004) isa graph-based method which choose sentencesbased on the concept of eigenvector centrality.?
The Linear Representation model (Ma andWan, 2010) has the same assumption as oursand it can be seen as an approximation of theconstant-length version of our model.741    	Figure 1: Rouge-1 values on DUC2004 dataset.    	Figure 2: Rouge-2 values on DUC2004 dataset.    	Figure 3: Rouge-L values on DUC2004 dataset.All the compared systems are implemented at dif-ferent predefined lengths from 50 to 300 words.Then we evaluate the summaries with ROUGE4tools (Lin and Hovy, 2003) in terms of the f-measure4we use ROUGE1.5.5 in this work.scores of Rouge-1 Rouge-2, and Rouge-L.
The met-ric of Rouge f-measure takes into consideration thesummary length in evaluation, so it is proper forour experiments.
From Fig.1, Fig.2 and Fig.3, wecan see that the result of BNP summarization (thedashed line) gets the second best value among allsystems.
It is only defeated by the Linear modelbut the result is comparable to the best in Fig.1 andFig.3; while it exceeds other systems at all lengths.This proves the good qualities of our BNP sum-maries.
The reason that the Linear system gets alittle better result may be its weights for linear com-bination of summary sentences are guaranteed non-negative while in our model the weights are zero-mean Gaussian variables.
This may lead to less re-dundance in sentence selection for the Linear Rep-resentation model.Turn to the length determination.
We take ad-vantage of the Linear Representation model to ap-proximate the constant-length version of our model.Comparing the summaries generated at differentpredefined lengths, Fig.4 shows the the model getsthe best performance (Rouge values) at the lengtharound 164 words, the length learned by our BNPmodel.
This result partly demonstrates our lengthdetermination is rational and it can be used as therecommended length for some constant-length sum-marization systems, such as the Linear . 	 ! ! ! !"#    Figure 4: Rate-dist value V.S.
summary word length.7425.2 A New Evaluation MetricThe Rouge evaluation requires golden standard sum-maries as the base.
However, in many cases wecannot get the reference summaries.
For example,when we implement experiments on our expandeddatasets (the separate and combined clusters of doc-uments), we do not have exact reference summaries.Louis and Nenkova (2009) advanced an automaticsummary evaluation without human models.
Theyused the Jensen-Shannon divergence(JSD) betweenthe input documents and the summaries as a fea-ture, and got high correlation with human evalua-tions and the rouge metric.
Unfortunately, it wasdesigned for comparison at a constant-length, whichcannot meet our needs.
To extend the JSD evaluationto compare varying-length summaries, we propose anew measure based on information theory, the rate-distortion (Cover and Thomas, 2006).Rate-Distortion: The distortion function d(x, x?
)is a measure of the cost of representing the symbolx to a new symbol x?
; and the rate can indicate howmuch compression can be achieved.
The problem offinding the minimum rate can be solved by minimiz-ing the functionalF [p(x?|x)] = I(X; X?)
+ ?E(d(x, x?)).
(20)where I(X; X?)
denotes the mutual information.The rate-distortion theory is a fundamental the-ory for lossy data compression.
Recently, it hasalso been successfully employed for text cluster-ing (Slonim, 2002) and document summarization(Ma and Wan, 2010).
Slonim (2002) claims thatthe mutual information I(X; X?)
measures the com-pactness of the new representation.
Thus the rate-distortion function is a trade-off between the com-pactness of new representation and the expected dis-tortion.
Specifically in summarization, the sum-maries can be seen as the new representation X?
oforiginal documents X .
A good summary balancesthe compression ratio and the information loss, thusminimizing the function (20).
So we use the func-tion (20)(we set ?
= 1) to compare which summaryis a better compression.
The JS-divergence (JSD),which has been proved to have high correlation withmanual evaluation (Louis and Nenkova, 2009) forconstant-length summary evaluation, is utilized asthe distortion in the function.
In the following sec-tions, we simply call the values of the function (20)rate-dist.
In fact, the rate-dist values can be seen asthe JSD measure with length regularization.To check the effectiveness of rate-dist measure,we evaluate all summaries generated in Section 5.1with the new measure (the lower the better).
Fig.
5shows that the results accord with the ones in Fig.
1and Fig.
3.
Moreover, in Fig.
4, the curve of rate-dist values has a inverse tendency of Rouge mea-sures (Rouge-1, Rouge-2, Rouge-L and Rouge-SU4are all listed here), and the best performance also oc-curs around the summary length of 164 words.
Thiseven more clearly reveals that the BNP summariza-tion achieves a perfect tradeoff between compact-ness and informativeness.
Due to the accordancewith rouge measures, it is promising to be regardedas an alternative to the rouge measures in case we donot have reference summaries.    	Figure 5: Comparison of BNP Summarization with othersystems using rate-dist measure.5.3 Necessity of Varying Summary LengthIn this section, we discuss the necessity of lengthdetermination and how summary length changes ac-cording to the input data.
As explained before,we generate three new datasets from the originalDUC2004 dataset.
Now we use them to indicatevarying summary length is necessary when the in-put data varies a lot.Table 1 shows the average summary length of dif-ferent data sets.
The results satisfy the intuitive ex-pectation of summary length change.
When we splita 10-document cluster into two 5-document parts,we expect the average summary length of the newclusters to be a little smaller than the original clus-ter but much larger than half of the original length,743because all the documents concentrate on the samethemes.
When we combine two clusters into one, thesummary length should be smaller than the sum ofthe summary lengths of two original clusters due tosome unavoidable common background informationbut much larger than the summary length of originalclusters.Original Separate Combined1 Combined2164 115 250 231Table 1: Average summary length (number of words) ondifferent datasetsWe also run the Linear Representation system atdifferent lengths on the new datasets and evaluatethe qualities.
As we do not have golden standardfor the new datasets, so we only use the rate-distmeasure here.
Results in Table 2,3,4 show the sum-maries which do not change the predefined length5 perform significantly worse than the BNP sum-marization.
All the comparison is statistically sig-nificant.
So varying summary length is necessarywhen the input changes a lot, and our model can justgive a good match to the new data.
This characteris-tic also can be used to give recommended summarylength for extractive summarization systems whengiven unknown data.Predefined Unchanged BNPLength 665 bytes 164 words 115 wordsRate-dist 0.4130 0.4404 0.4007Table 2: Comparison of summary lengths on SeparateDataset.Predefined Unchanged BNPLength 665 bytes 164 words 250 wordsRate-dist 0.3768 0.3450 0.3238Table 3: Comparison of summary lengths on Combined1Dataset.Then we observe the summary length distribu-tions and compression ratios according to documentsize(the length of the whole documents in a clus-ter).
The average summary length increases (Fig.
6),5665 bytes is the DUC2004 requirement and 164 words isthe best length on original dataPredefined Unchanged BNPLength 665 bytes 164 words 231 wordsRate-dist 0.3739 0.3464 0.3326Table 4: Comparison of summary lengths on Combined2Dataset.while the compression ratios decreases (Fig.
7) asdocument size grows.
The rule of the compres-sion ratio here agrees with the rule in (Goldsteinet al 1999), although that work is done for single-document summarization.$!%     &'&'"(		Figure 6: The distribution of summary word length.$!%     &'&'"(	Figure 7: Compression ratio versus document wordlength.7446 Conclusion and Future WorkIn this paper, we present a new problem of finding aproper summary length for multi-document summa-rization based on the document content.
A Bayesiannonparametric model is proposed to solve this prob-lem.
We use the beta process as the prior to constructa Bayesian framework for summary sentence selec-tion.
Experimental results are shown on DUC2004dataset, as well as some expanded datasets.
Wedemonstrate the summaries we extract have goodqualities and the length determination of our systemis rational.However, there is still much work to do forvariable-length summarization.
First, Our sys-tem is extractive-base summarization, which cannotachieve the perfect coherence and readability.
A sys-tem which can determine the best length even forabstractive summarization will be better.
Moreover,in this work we only consider the aspect of datacompression and evaluate the performance using aninformation-theoretic measure.
In future we mayconsider more human factors, and prove the sum-mary length determined by our system agrees withhuman preference.
In addition, in the experiments,we only use the imbalanced datasets as the examplethat intuitively needs varying the summary length.However, the data type is also important to impactthe summary length.
In future, we may extend thework by studying more cases that need varying sum-mary length.ReferencesChristopher M. Bishop.
2006.
Pattern recognition andmachine learning.
.
Vol.
4.
No.
4.
New York: springer.Jaime Carbonell, and Jade Goldstein.
1998.
The UseOf Mmr, Diversity-Based Reranking For ReorderingDocuments And Producing Summaries.
Proceedingsof the 21st annual international ACM SIGIR confer-ence on Research and development in information re-trieval.
ACM, 1998.Asli Celikyilmaz and Dilek Hakkani-Tu?r.
2010.
A Hy-brid Hierarchical Model for Multi-Document Summa-rization.
Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages815-824.Ying-Lan Chang, Jui-Jung Hung and Jen-Tzung Chien2011.
Bayesian Nonparametric Modeling Of Hier-archical Topics And Sentences.
IEEE InternationalWorkshop on Machine Learning for Signal Processing,September 18-21, 2011, Beijing, China.Thomas M. Cover, and Joy A. Thomas.
2006.
Elementsof information theory.
Wiley-interscience, 2006.William M. Darling and Fei Song.
2011.
PathSum:A Summarization Framework Based on HierarchicalTopics.
Canadian AI Workshop on Text Summariza-tion, St. John?s, Newfoundland.Samuel J. Gershman and David M. Blei.
2011.
A Tuto-rial On Bayesian Nonparametric Models.
Journal ofMathematical Psychology(2011).Thomas L. Griffiths and Zoubin Ghahramani.
2005.
Infi-nite Latent Feature Models and the Indian Buffet Pro-cess.
Advances in Neural Information Processing Sys-tems 18.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal andJaime Carbonelly.
1999.
Summarizing Text Doc-uments: Sentence Selection and Evaluation Metrics.Proceedings of SIGIR?99 , pages 121-128.Zhanying He, Chun Chen, Jiajun Bu, CanWang, LijunZhang, Deng Cai and Xiaofei He.
2012.
DocumentSummarization Based on Data Reconstruction.
Pro-ceedings of the Twenty-Sixth AAAI Conference on Ar-tificial Intelligence.Michael Kaisser, Marti A. Hearst, John B. Lowe.
2008.Improving Search Results Quality by CustomizingSummary Lengths.
Proceedings of ACL-08: HLT,pages 701-709.Chin-Yew Lin, Guihong Cao, Jianfeng Gao, and Jian-YunNie.
2006.
An Information-Theoretic Approach toAutomatic Evaluation of Summaries.
Proceedings ofNAACL2006, pages 463-470.Chin-Yew Lin, and Eduard Hovy.
2003.
Automaticevaluation of summaries using n-gram co-occurrencestatistics.
Proceedings of NAACL2003.Annie Louis and Ani Nenkova.
2009.
AutomaticallyEvaluating Content Selection in Summarization with-out Human Models.
Proceedings of the 2009 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 306-314.
Singapore, 6-7 August 2009.Tengfei Ma and Xiaojun Wan.
2010.
Multi-document Summarization Using Minimum Distortion.IEEE 10th International Conference on Data Mining(ICDM).Ani Nenkova and Kathleen McKeown.
2012.
A sur-vey of text summarization techniques.
Mining TextData, Chapter 3, Springer Science+Business Media,LLC (2012).John Paisley and Lawrence Carin.
2009.
NonparametricFactor Analysis with Beta Process Priors.
Proceed-ings of the 26th International Conference on MachineLearning, Montreal, Canada.745John Paisley, Aimee Zaas, Christopher W. Woods, Geof-frey S. Ginsburg and Lawrence Carin.
2010.
A Stick-Breaking Construction of the Beta Process.
Proceed-ings of the 27 th International Confer- ence on Ma-chine Learning, Haifa, Israel, 2010.Dragomir R. Radev and Weiguo Fan.
2000.
Effectivesearch results summary size and device screen size:Is there a relationship.
Proceedings of the ACL-2000workshop on Recent advances in natural languageprocessing and information retrievalGu?nes Erkan, and Dragomir R. Radev.
2004.
LexRank:Graph-based Lexical Centrality as Salience in TextSummarization.
Journal of Artificial Intelligence Re-search, 22 (2004) 457-479.Noam Slonim.
2002.
The Information Bottleneck: The-ory and Applications.
PHD Thesis of the Hebrew Uni-versity .Simon Sweeney and Fabio Crestani.
2006.
Effectivesearch results summary size and device screen size: Isthere a relationship.
Information Processing and Man-agement 42 (2006) 1056-1074.Simon Sweeney, Fabio Crestani and David E. Losada.2008.
?Show me more?
: Incremental length summari-sation using novelty detection.
Information Process-ing and Management 44 (2008) 663-686.Yee Whye Teh, Dilan Go?ru?r, and Zoubin Ghahramani.2007.
Stick-breaking Construction for the Indian Buf-fet Process.
Proceedings of the International Confer-ence on Artificial Intelligence and Statistics.Y.W.
Teh, M.I.
Jordan, M.J. Beal and D.M.
Blei.2006.
Hierarchical Dirichlet Processes.
JASA ,101(476):1566-1581.Romain Thibaux and Michael I. Jordan.
2009.
Hierar-chical Beta Processes and the Indian Buffet Process.AISTATS2007.746
