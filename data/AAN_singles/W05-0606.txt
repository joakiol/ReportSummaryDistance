Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 40?47, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsComputing Word Similarity and Identifying Cognateswith Pair Hidden Markov ModelsWesley Mackay and Grzegorz KondrakDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta, Canada T6G 2E8{wesleym,kondrak}@cs.ualberta.caAbstractWe present a system for computing sim-ilarity between pairs of words.
Our sys-tem is based on Pair Hidden Markov Mod-els, a variation on Hidden Markov Mod-els that has been used successfully for thealignment of biological sequences.
Theparameters of the model are automaticallylearned from training data that consistsof word pairs known to be similar.
Ourtests focus on the identification of cog-nates ?
words of common origin in re-lated languages.
The results show that oursystem outperforms previously proposedtechniques.1 IntroductionThe computation of surface similarity between pairsof words is an important task in many areas of nat-ural language processing.
In historical linguisticsphonetic similarity is one of the clues for identi-fying cognates, that is, words that share a com-mon origin (Oakes, 2000).
In statistical machinetranslation, cognates are helpful in inducing transla-tion lexicons (Koehn and Knight, 2001; Mann andYarowsky, 2001), sentence alignment (Melamed,1999), and word alignment (Tiedemann, 2003).
Indialectology, similarity is used for estimating dis-tance between dialects (Nerbonne, 2003).
Otherapplications include cross-lingual information re-trieval (Pirkola et al, 2003), detection of confusabledrug names (Kondrak and Dorr, 2004), and lexicog-raphy (Brew and McKelvie, 1996).Depending on the context, strong word similaritymay indicate either that words share a common ori-gin (cognates), a common meaning (synonyms), orare related in some way (e.g.
spelling variants).
Inthis paper, we focus on cognates.
Genetic cognatesare well-suited for testing measures of word simi-larity because they arise by evolving from a singleword in a proto-language.
Unlike rather indefiniteconcepts like synonymy or confusability, cognationis a binary notion, which in most cases can be reli-ably determined.Methods that are normally used for computingword similarity can be divided into orthographicand phonetic.
The former includes string edit dis-tance (Wagner and Fischer, 1974), longest commonsubsequence ratio (Melamed, 1999), and measuresbased on shared character n-grams (Brew and Mc-Kelvie, 1996).
These usually employ a binary iden-tity function on the level of character comparison.The phonetic approaches, such as Soundex (Halland Dowling, 1980) and Editex (Zobel and Dart,1996), attempt to take advantage of the phoneticcharacteristics of individual characters in order toestimate their similarity.
All of the above meth-ods are static, in the sense of having a fixed defi-nition that leaves little room for adaptation to a spe-cific context.
In contrast, the methods proposed byTiedemann (1999) automatically construct weightedstring similarity measures on the basis of string seg-mentation and bitext co-occurrence statistics.We have created a system for determining wordsimilarity based on a Pair Hidden Markov Model.The parameters of the model are automaticallylearned from training data that consists of word40pairs that are known to be similar.
The modelis trained using the Baum-Welch algorithm (Baumet al, 1970).
We examine several variants of themodel, which are characterized by different trainingtechniques, number of parameters, and word lengthcorrection method.
The models are tested on a cog-nate recognition task across word lists representingseveral Indo-European languages.
The experimentsindicate that our system substantially outperformsthe most commonly used approaches.The paper is organized as follows.
Section 2 givesa more detailed description of the problem of wordsimilarity.
Section 3 contains an introduction to PairHidden Markov Models, while section 4 describestheir adaptation to our domain.
Sections 5 and 6 re-port experimental set-up and results.2 Word SimilarityWord similarity is, at its core, an alignment task.
Inorder to determine similarity between two words, welook at the various alignments that can exist betweenthem.
Each component of the alignment is assigneda probability-based score by our trained model.
Thescores are then combined to produce the overall sim-ilarity score for any word pair, which can be used torank the word pairs against each other.
Alternatively,a discrete cut-off point can be selected in order toseparate pairs that show the required similarity fromthe ones that do not.Before we can align words, they must be sep-arated into symbols.
Typically, the symbols arecharacters in the orthographic representation, andphonemes in the phonetic representation.
We alsoneed to put some restrictions on the possible align-ments between these symbols.
By adopting the fol-lowing two assumptions, we are able to fully ex-ploit the simplicity and efficiency of the Pair HiddenMarkov Model.First, we assume that the basic ordering of sym-bols remains the same between languages.
This doesnot mean that every symbol has a corresponding onein the other language, but instead that word transfor-mation comes from three basic operations: substitu-tion, insertion and deletion.
Exceptions to this rulecertainly exist (e.g.
metathesis), but are sufficientlyinfrequent to make the benefits of this constraint faroutweigh the costs.Second, we assume that each symbol is alignedto at most one symbol in the other word.
This as-sumption is aimed at reducing the number of param-eters that have to be learned from limited-size train-ing data.
If there is a many-to-one correspondencethat is consistent between languages, it would bebeneficial to change the word representation so thatthe many symbols are considered as a single sym-bol instead.
For example, a group of characters inthe orthographic representation may correspond to asingle phoneme if the word is written phonetically.3 Pair Hidden Markov ModelsHidden Markov Models have been applied success-fully to a number of problems in natural languageprocessing, including speech recognition (Jelinek,1999) and statistical machine translation (Och andNey, 2000).
One of the more intangible aspects ofa Hidden Markov Model is the choice of the modelitself.
While algorithms exist to train the parametersof the model so that the model better describes itsdata, there is no formulaic way to create the model.We decided to adopt as a starting point a model de-veloped in a different field of study.Durbin et al (1998) created a new type of Hid-den Markov Model that has been used for the taskof aligning biological sequences (Figure 1).
Calleda Pair Hidden Markov Model, it uses two outputstreams in parallel, each corresponding to a se-quence that is being aligned.1 The alignment modelhas three states that represent the basic edit opera-tions: substitution (represented by state ?M?
), inser-tion (?Y?
), and deletion (?X?).
?M?, the match state,emits an aligned pair of symbols (not necessarilyidentical) with one symbol on the top and the otheron the bottom output stream.
?X?
and ?Y?, the gapstates, output a symbol on only one stream againsta gap on the other.
Each state has its own emissionprobabilities representing the likelihood of produc-ing a pairwise alignment of the type described bythe state.
The model has three transition parame-ters: ?, ?, and ?.
In order to reduce the number ofparameters, there is no explicit start state.
Rather,the probability of starting in a given state is equal to1Pair Hidden Markov Models have been used in the area ofnatural language processing once before: Clark (2001) appliedPHMMs to the task of learning stochastic finite-state transduc-ers for modeling morphological paradigms.41Figure 1: A Pair Hidden Markov Model for aligningbiological sequences.the probability of going from the match state to thegiven state.Durbin et al (1998) describe several different al-gorithms that can be used to score and rank pairedbiological sequences.
Two of them are based oncommon HMM algorithms.
The Viterbi algorithmuses the most probable path through the model toscore the pair.
The forward algorithm computes thetotal overall probability for a pair by summing up theprobabilities of every possible alignment betweenthe words.
A third algorithm (the log odds algo-rithm) was designed to take into account how likelythe pair would be to occur randomly within the twolanguages by considering a separately trained ran-dom model (Figure 2) in conjunction with the sim-ilarity model.
In the random model, the sequencesare assumed to have no relationship to each other, sothere is no match state.
The log odds algorithm cal-culates a score for a pair of symbols by dividing theprobability of a genuine correspondence between apair of symbols (the similarity model) by the proba-bility of them co-occurring by chance (the randommodel).
These individual scores are combined toproduce an overall score for the pair of sequencesin the same way as individual symbol probabilitiesare combined in other algorithms.4 PHMMs for Word SimilarityBecause of the differences between biological se-quence analysis and computing word similarity, thebioinformatics model has to be adapted to handle thelatter task.
In this section, we propose a number ofmodifications to the original model and the corre-Figure 2: The random Pair Hidden Markov Model.sponding algorithms.
The modified model is shownin Figure 3.First, the original model?s assumption that an in-sertion followed by a deletion is the same as a sub-stitution is problematic in the context of word simi-larity.
Covington (1998) illustrates the problem withan example of Italian ?due?
and the Spanish ?dos?,both of which mean ?two?.
While there is no doubtthat the first two pairs of symbols should be aligned,there is no historical connection between the Italian?e?
and the Spanish ?s?.
In this case, a sequence ofan insertion and a deletion is more appropriate thana substitution.
In order to remedy this problem, wedecided to a add a pair of transitions between states?X?
and ?Y?, which is denoted by ?
in Figure 3.The second modification involves splitting the pa-rameter ?
into two separate values: ?M for the matchstate, and ?XY for the gap states.
The original biolog-ical model keeps the probability for the transition tothe end state constant for all other states.
For cog-nates, and other word similarity tasks, it may be thatsimilar words are more or less likely to end in gapsor matches.
The modification preserves the symme-try of the model while allowing it to capture howlikely a given operation is to occur at the end of analignment.4.1 Algorithm VariationsWe have investigated several algorithms for thealignment and scoring of word pairs.
Apart fromthe standard Viterbi (abbreviated VIT) and forward(FOR) algorithms, we considered two variations ofthe log odds algorithm, The original log odds al-gorithm (LOG) functions much like a Viterbi algo-42Figure 3: A Pair Hidden Markov Model for aligningwords.rithm, looking at only the most probable sequenceof states.
We also created another variation, forwardlog odds (FLO), which uses a forward approach in-stead, considering the aggregate probability of allpossible paths through both models.4.2 Model VariationsApart from comparing the effectiveness of differentalgorithms, we are also interested in establishing theoptimal structure of the underlying model.
The sim-ilarity model can be broken up into three sets of pa-rameters: the match probabilities, the gap probabil-ities, and the transition probabilities.
Our goal is toexamine the relative contribution of various compo-nents of the model, and to find out whether simplify-ing the model affects the overall performance of thesystem.
Since the match probabilities constitute thecore of the model, we focus on the remaining emis-sion and transition probabilities.
We also investigatethe necessity of including an explicit end state in themodel.The first variation concerns the issue of gap emis-sion probabilities.
For the log odds algorithm,Durbin et al (1998) allow the gap emission prob-abilities of both the similarity and random models tobe equal.
While this greatly simplifies the calcula-tions and allows for the emphasis to be on matchedsymbols, it might be more in spirit with the wordsimilarity task to keep the emissions of the two mod-els separate.
If we adopt such an approach, the simi-larity model learns the gap emission probabilities us-ing the forward-backward algorithm, just as is donewith the match probabilities, but the random modeluses letter frequencies from the training data instead.A similar test of the effectiveness of trained gap pa-rameters can be performed for the Viterbi and for-ward algorithms by proceeding in the opposite direc-tion.
Instead of deriving the gap probabilities fromthe training data (as in the original model), we canset them to uniform values after training, thus mak-ing the final scores depend primarily on matches.The second variation removes the effect the tran-sition parameters have on the final calculation.
In theresulting model, a transition probability from anystate to any state (except the end state) is constant,effectively merging ?X?, ?Y?, and ?M?
into a sin-gle state.
One of the purposes of the separated stateswas to allow for affine gap penalties, which is whythere are different transition parameters for going toa gap state and for staying in that state.
By makingthe transitions constant, we are also taking away theaffine gap structure.
As a third variant, we try boththe first and second variation combined.The next variation concerns the effect of the endstate on the final score.
Unlike in the alignmentof biological sequences, word alignment boundariesare known beforehand, so an end state is not strictlynecessary.
It is simple enough to remove the endstate from our model after the training has been com-pleted.
The remaining transition probability mass isshifted to the transitions that lead to the match state.Once the end state is removed, it is possible toreduce the number of transition parameters to a sin-gle one, by taking advantage of the symmetry be-tween the insertion and deletion states.
In the result-ing model, the probability of entering a gap state isequal to 1?x2 , where x is the probability of a transi-tion to the match state.
Naturally, the log odds algo-rithms also have a separate parameter for the randommodel.4.3 Correcting for LengthAnother problem that needs to be addressed is thebias introduced by the length of the words.
The prin-cipal objective of the bioinformatics model is theoptimal alignment of two sequences.
In our case,the alignment is a means to computing word simi-larity.
In fact, some of the algorithms (e.g.
the for-ward algorithm) do not yield an explicit best align-ment.
While the log odds algorithms have a built-inlength correction, the Viterbi and the forward do not.43These algorithms continually multiply probabilitiestogether every time they process a symbol (or a sym-bol pair), which means that the overall probability ofan alignment strongly depends on word lengths.
Inorder to rectify this problem, we multiply the finalprobability by 1Cn , where n is the length of the longerword in the pair, and C is a constant.
The value of Ccan be established on a held-out data set.24.4 Levenshtein with Learned WeightsMann and Yarowsky (2001) investigated the induc-tion of translation lexicons via bridge languages.Their approach starts with a dictionary between twowell studied languages (e.g.
English-Spanish).
Theythen use cognate pairs to induce a bridge betweentwo strongly related languages (e.g.
Spanish andItalian), and from this create a smaller translationdictionary between the remaining two languages(e.g.
English and Italian).
They compared the per-formances of several different cognate similarity (ordistance) measures, including one based on the Lev-enshtein distance, one based on the stochastic trans-ducers of Ristad and Yianilos (1998), and a varia-tion of a Hidden Markov Model.
Somewhat surpris-ingly, the Hidden Markov Model falls well short ofthe baseline Levenshtein distance.3Mann and Yarowsky (2001) developed yet an-other model, which outperformed all other simi-larity measures.
In the approach, which they call?Levenshtein with learned weights?, the probabil-ities of their stochastic transducer are transformedinto substitution weights for computing Levenshteindistance: 0.5 for highly similar symbols, 0.75 forweakly similar symbols, etc.
We have endeavored toemulate this approach (abbreviated LLW) by con-verting the log odds substitution scores calculatedfrom the fully trained model into the substitutionweights used by the authors.2Another common method to correct for length is to takethe nth root of the final calculation, where n is the length of thelongest word.
However, our initial experiments indicated thatthis method does not perform well on the word similarity task.3The HMM model of (Mann and Yarowsky, 2001) is of dis-tinctly different design than our PHMM model.
For example,the emission probabilities corresponding to the atomic edit op-erations sum to one for each alphabet symbol.
In our model, theemission probabilities for different symbols are interdependent.5 Experimental SetupWe evaluated our word similarity system on the taskof the identification of cognates.
The input consistsof pairs of words that have the same meaning in dis-tinct languages.
For each pair, the system produces ascore representing the likelihood that the words arecognate.
Ideally, the scores for true cognate pairsshould always be higher than scores assigned to un-related pairs.
For binary classification, a specificscore threshold could be applied, but we defer thedecision on the precision-recall trade-off to down-stream applications.
Instead, we order the candidatepairs by their scores, and evaluate the ranking us-ing 11-point interpolated average precision (Man-ning and Schutze, 2001).Word similarity is not always a perfect indicatorof cognation because it can also result from lexicalborrowing and random chance.
It is also possiblethat two words are cognates and yet exhibit little sur-face similarity.
Therefore, the upper bound for aver-age precision is likely to be substantially lower than100%.5.1 DataTraining data for our cognate recognition modelcomes from the Comparative Indoeuropean DataCorpus (Dyen et al, 1992).
The data containsword lists of 200 basic meanings representing 95speech varieties from the Indoeuropean family oflanguages.
Each word is represented in an ortho-graphic form without diacritics using the 26 lettersof the Roman alphabet.
All cognate pairs are alsoidentified in the data.The development set4 consisted of two languagepairs: Italian and Serbo-Croatian, as well as Polishand Russian.
We chose these two language pairsbecause they represent very different levels of re-latedness: 25.3% and 73.5% of the word pairs arecognates, respectively.
The percentage of cognateswithin the data is important, as it provides a sim-ple baseline from which to compare the success ofour algorithms.
If our cognate identification process4Several parameters used in our experiments were deter-mined during the development of the word similarity model.These include the random model?s parameter ?, the constanttransition probabilities in the simplified model, and the constantC for correcting the length bias in the Viterbi and forward algo-rithms.
See (Mackay, 2004) for complete details.44were random, we would expect to get roughly thesepercentages for our recognition precision (on aver-age).The test set consisted of five 200-word lists repre-senting English, German, French, Latin, and Alba-nian, compiled by Kessler (2001).
The lists for theselanguages were removed from the training data (ex-cept Latin, which was not part of the training set), inorder to keep the testing and training data as sepa-rate as possible.5 We converted the test data to havethe same orthographic representation as the trainingdata.5.2 Significance testsWe performed pairwise statistical significance testsfor various model and algorithm combinations.
Fol-lowing the method proposed by Evert (2004), weapplied Fisher?s exact test to counts of word pairsthat are accepted by only one of the two tested al-gorithms.
For a given language pair, the cutoff levelwas set equal to the actual number of cognate pairsin the list.
For example, since 118 out of 200 wordpairs in the English/German list are cognate, we con-sidered the true and false positives among the set of118 top scoring pairs.
For the overall average ofa number of different language pairs, we took theunion of the individual sets.
For the results in Ta-bles 1 and 2, the pooled set contained 567 out of2000 pairs, which corresponds to the proportion ofcognates in the entire test data (28.35%).6 Experimental ResultsIn this section, we first report on the effect of modelvariations on the overall performance, and then wecompare the best results for each algorithm.6.1 Model VariationsTable 1 shows the average cognate recognition pre-cision on the test set for a number of model vari-ations combined with four basic algorithms, VIT,FOR, LOG, and FLO, which were introduced inSection 4.1.
The first row refers to the fully trained5The complete separation of training and testing data is diffi-cult to achieve in this case because of the similarity of cognatesacross languages in the same family.
For each of the removedlanguages, there are other closely related languages that are re-tained in the training set, which may exhibit similar or evenidentical correspondences.Model AlgorithmVariation VIT FOR LOG FLOfull model 0.630 0.621 0.656 0.631gaps const 0.633 0.631 0.684 0.624trans const 0.565 0.507 0.700 0.550both const 0.566 0.531 0.704 0.574no end state 0.626 0.620 0.637 0.601single param 0.647 0.650 0.703 0.596Table 1: Average cognate recognition precision foreach model and algorithm combination.model without changes.
The remaining rows con-tain the results for the model variations described inSection 4.2.
In all cases, the simplifications are ineffect during testing only, after the full model hadbeen trained.
We also performed experiments withthe model simplified prior to training but their resultswere consistently lower than the results presentedhere.With the exception of the forward log odds algo-rithm, the best results are obtained with simplifiedmodels.
The model with only a single transitionparameter performs particularly well.
On the otherhand, the removal of the end state invariably causesa decrease in performance with respect to the fullmodel.
If a non-essential part of the model is madeconstant, only the Viterbi-based log odds algorithmimproves significantly; the performance of the otherthree algorithms either deteriorates or shows no sig-nificant difference.Overall, the top four variations of the Viterbi-based log odds algorithm (shown in italics in Ta-ble 1) significantly outperform all other PHMMvariations and algorithms.
This is not entirely unex-pected as LOG is a more complex algorithms thanboth VIT and FOR.
It appears that the incorpora-tion of the random model allows LOG to better dis-tinguish true similarity from chance similarity.
Inaddition, the log odds algorithms automatically nor-malize the results based on the lengths of the wordsunder examination.
However, from the rather dis-appointing performance of FLO, we conclude thatconsidering all possible alignments does not help thelog odds approach.45Languages Proportion Methodof Cognates LCSR LLW ALINE VIT FOR LOG FLOEnglish German 0.590 0.895 0.917 0.916 0.932 0.932 0.930 0.929French Latin 0.560 0.902 0.893 0.863 0.916 0.914 0.934 0.904English Latin 0.290 0.634 0.713 0.725 0.789 0.792 0.803 0.755German Latin 0.290 0.539 0.647 0.706 0.673 0.666 0.730 0.644English French 0.275 0.673 0.725 0.615 0.751 0.757 0.812 0.725French German 0.245 0.568 0.591 0.504 0.556 0.559 0.734 0.588Albanian Latin 0.195 0.541 0.510 0.618 0.546 0.557 0.680 0.541Albanian French 0.165 0.486 0.444 0.612 0.505 0.530 0.653 0.545Albanian German 0.125 0.275 0.340 0.323 0.380 0.385 0.379 0.280Albanian English 0.100 0.245 0.322 0.277 0.416 0.406 0.382 0.403AVERAGE 0.2835 0.576 0.610 0.616 0.647 0.650 0.704 0.631Table 2: Average cognate recognition precision for various models and algorithms.6.2 ComparisonTable 2 contains the results of the best variants,which are shown in boldface in Table 1, along withother methods for comparison.
The results are sepa-rated into individual language pairs from the test set.For the baseline method, we selected the LongestCommon Subsequence Ratio (LCSR), a measure oforthographic word similarity often used for cognateidentification (Brew and McKelvie, 1996; Melamed,1999; Koehn and Knight, 2001).
The LCSR oftwo words is computed by dividing the length oftheir longest common subsequence by the lengthof the longer word.
LLW stands for ?Levenshteinwith learned weights?, which is described in Sec-tion 4.4.
We also include the results obtainedby the ALINE word aligner (Kondrak, 2000) onphonetically-transcribed word lists.Because of the relatively small size of the lists,the differences among results for individual lan-guage pairs are not statistically significant in manycases.
However, when the average over all languagepairs is considered, the Viterbi-based log odds al-gorithm (LOG) is significantly better than all otheralgorithms in Table 2.
The differences betweenthe remaining algorithms are not statistically signifi-cant, except that they all significantly outperform theLCSR baseline.The fact that LOG is significantly better thanALINE demonstrates that given a sufficiently largetraining set, an HMM-based algorithm can automat-ically learn the notion of phonetic similarity, whichis incorporated into ALINE.
ALINE does not in-volve extensive supervised training, but it requiresthe words to be in a phonetic, rather than ortho-graphic form.
We conjecture that the performanceof LOG would further improve if it could be trainedon phonetically-transcribed multilingual data.7 ConclusionWe created a system that learns to recognize wordpairs that are similar based on some criteria providedduring training, and separate such word pairs fromthose that do not exhibit such similarity or whosesimilarity exists solely by chance.
The system isbased on Pair Hidden Markov Models, a techniqueadapted from the field of bioinformatics.
We tested anumber of training algorithms and model variationson the task of identifying cognates.
However, sinceit does not rely on domain-specific knowledge, oursystem can be applied to any task that requires com-puting word similarity, as long as there are examplesof words that would be considered similar in a givencontext.In the future, we would like to extend our systemby removing the one-to-one constraint that requiresalignments to consist of single symbols.
It wouldalso be interesting to test the system in other ap-plications, such as the detection of confusable drugnames or word alignment in bitexts.46AcknowledgmentsThis research was funded in part by the Natural Sci-ences and Engineering Research Council of Canada(NSERC), and the Alberta Informatics Circle of Re-search Excellence (iCORE).ReferencesLeonard E. Baum, Ted Petrie, George Soules, and Nor-man Weiss.
1970.
A maximization technique occur-ring in the statistical analysis of probabilistic functionof Markov chains.
The Annals of Mathematical Statis-tics, 41(1):164?171.Chris Brew and David McKelvie.
1996.
Word-pair ex-traction for lexicography.
In Proceedings of the 2ndInternational Conference on New Methods in Lan-guage Processing, pages 45?55.Alexander Clark.
2001.
Learning morphology with PairHidden Markov Models.
In Proceedings of the StudentWorkshop at ACL 2001.Michael A. Covington.
1998.
Alignment of multiple lan-guages for historical comparison.
In Proceedings ofCOLING-ACL?98, pages 275?280.Richard Durbin, Sean R. Eddy, Anders Krogh, andGraeme Mitchison.
1998.
Biological sequence analy-sis.
Cambridge University Press.Isidore Dyen, Joseph B. Kruskal, and Paul Black.
1992.An Indoeuropean classification: A lexicostatistical ex-periment.
Transactions of the American PhilosophicalSociety, 82(5).Stefan Evert.
2004.
Significance tests for the evaluationof ranking methods.
In Proceedings of COLING 2004,pages 945?951.Patrick A. V. Hall and Geoff R. Dowling.
1980.
Approxi-mate string matching.
Computing Surveys, 12(4):381?402.Frederick Jelinek.
1999.
Statistical Methods for SpeechRecognition.
The Massachusetts Institute of Technol-ogy Press.Brett Kessler.
2001.
The Significance of Word Lists.Stanford: CSLI Publications.Philipp Koehn and Kevin Knight.
2001.
Knowledgesources for word-level translation models.
In Proceed-ings of the 2001 Conference on Empirical Methods inNatural Language Processing, pages 27?35.Grzegorz Kondrak and Bonnie Dorr.
2004.
Identificationof confusable drug names: A new approach and evalu-ation methodology.
In Proceedings of COLING 2004,pages 952?958.Grzegorz Kondrak.
2000.
A new algorithm for thealignment of phonetic sequences.
In Proceedings ofNAACL 2000, pages 288?295.Wesley Mackay.
2004.
Word similarity using Pair Hid-den Markov Models.
Master?s thesis, University ofAlberta.Gideon S. Mann and David Yarowsky.
2001.
Multipathtranslation lexicon induction via bridge languages.
InProceedings of NAACL 2001, pages 151?158.Christopher D. Manning and Hinrich Schutze.
2001.Foundations of Statistical Natural Language Process-ing.
The MIT Press.I.
Dan Melamed.
1999.
Bitext maps and alignmentvia pattern recognition.
Computational Linguistics,25(1):107?130.John Nerbonne.
2003.
Linguistic variation and compu-tation.
In Proceedings of EACL-03, pages 3?10.Michael P. Oakes.
2000.
Computer estimation of vocab-ulary in protolanguage from word lists in four daugh-ter languages.
Journal of Quantitative Linguistics,7(3):233?243.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of ACL-2000, pages 440?447.Ari Pirkola, Jarmo Toivonen, Heikki Keskustalo, KariVisala, and Kalervo Jarvelin.
2003.
Fuzzy transla-tion of cross-lingual spelling variants.
In Proceedingsof SIGIR?03, pages 345?352.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learn-ing string edit distance.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 20(2):522?532.Jo?rg Tiedemann.
1999.
Automatic construction ofweighted string similarity measures.
In Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora, College Park, Maryland.Jo?rg Tiedemann.
2003.
Combining clues for word align-ment.
In Proceedings of the 10th Conference of theEuropean Chapter of the ACL (EACL03).Robert A. Wagner and Michael J. Fischer.
1974.
Thestring-to-string correction problem.
Journal of theACM, 21(1):168?173.Justin Zobel and Philip Dart.
1996.
Phonetic stringmatching: Lessons from information retrieval.
In Pro-ceedings of SIGIR?96, pages 166?172.47
