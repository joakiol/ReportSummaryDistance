Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 752?760,Beijing, August 2010Automatic analysis of semantic similarity in comparable textthrough syntactic tree matchingErwin MarsiTiCC, Tilburg Universitye.c.marsi@uvt.nlEmiel KrahmerTiCC, Tilburg Universitye.j .krahmer@uvt.nlAbstractWe propose to analyse semantic similar-ity in comparable text by matching syn-tactic trees and labeling the alignmentsaccording to one of five semantic simi-larity relations.
We present a Memory-based Graph Matcher (MBGM) that per-forms both tasks simultaneously as a com-bination of exhaustive pairwise classifica-tion using a memory-based learner, fol-lowed by global optimization of the align-ments using a combinatorial optimizationalgorithm.
The method is evaluated on amonolingual treebank consisting of com-parable Dutch news texts.
Results showthat it performs substantially above thebaseline and close to the human reference.1 IntroductionNatural languages allow us to express essentiallythe same underlying meaning as many alterna-tive surface forms.
In other words, there are of-ten many similar ways to say the same thing.This characteristic poses a problem for many nat-ural language processing applications.
Automaticsummarizers, for example, typically rank sen-tences according to their informativity and thenextract the top n sentences, depending on the re-quired compression rate.
Although the sentencesare essentially treated as independent of eachother, they typically are not.
Extracted sentencesmay have substantial semantic overlap, result-ing in unintended redundancy in the summaries.This is particularly problematic in the case ofmulti-document summarization, where sentencesextracted from related documents are very likelyto express similar information in different ways(Radev and McKeown, 1998).
Therefore, if se-mantic similarity between sentences could be de-tected automatically, this would certainly help toavoid redundancy in summaries.Similar arguments can be made for many otherNLP applications.
Automatic duplicate and pla-giarism detection beyond obvious string overlaprequires recognition of semantic similarity.
Au-tomatic question-answering systems may benefitfrom clustering semantically similar candidate an-swers.
Intelligent document merging software,which supports a minimal but lossless merge ofseveral revisions of the same text, must handlecases of paraphrasing, restructuring, compression,etc.
Recognizing textual entailments (Dagan etal., 2005) could arguably be seen as a specific in-stance of detecting semantic similarity.In addition to merely detecting semantic simi-larity, we can ask to what extent two expressionsshare meaning.
For instance, the meaning of onesentence can be fully contained in that of another,the meaning of one sentence can overlap onlypartly with that of another, etc.
This requires ananalysis of the semantic similarity between a pairof expressions.
Like detection, automatic analy-sis of semantic similarity can play an importantrole in NLP applications.
To return to the caseof multi-document summarization, analysing thesemantic similarity between sentences extractedfrom different documents provides the basis forsentence fusion, a process where a new sentenceis generated that conveys all common informationfrom both sentences without introducing redun-dancy (Barzilay and McKeown, 2005; Marsi andKrahmer, 2005b).752Analysis of semantic similarity can be ap-proached from different angles.
A basic approachis to use string similarity measures such as theLevenshtein distance or the Jaccard similarity co-efficient.
Although cheap and fast, this fails toaccount for less obvious cases such as synonymsor syntactic paraphrasing.
At the other extreme,we can perform a deep semantic analysis of twoexpressions and rely on formal reasoning to de-rive a logical relation between them.
This ap-proach suffers from issues with coverage and ro-bustness commonly associated with deep linguis-tic processing.
We therefore think that the middleground between these two extremes offers the bestoption.
In this paper we present a new methodfor analysing semantic similarity in comparabletext.
It relies on a combination of morphologi-cal and syntactic analysis, lexical resources suchas word nets, and machine learning from exam-ples.
We propose to analyse semantic similaritybetween sentences by aligning their syntax trees,where each node is matched to the most similarnode in the other tree (if any).
In addition, welabel these alignments according to the type ofsimilarity relation that holds between the alignedphrases.
The labeling supports further processing.For instance, Marsi & Krahmer (2005b; 2008) de-scribe how to generate different types of sentencefusions on the basis of this relation labeling.In the next Section we provide a more formaldefinition of the task of matching syntactic treesand labeling alignments, followed by a discusionof related work in Section 3.
Section 4 describes aparallel, monolingual treebank used for develop-ing and testing our approach.
In Section 5 we pro-pose a new algorithm for simultaneous node align-ment and relation labeling.
The results of severalevaluation experiments are presented in Section 6.We finish with a conclusion.2 Problem statementAligning a pair of similar syntactic trees is the pro-cess of pairing those nodes that are most similar.More formally: let v be a node in the syntactictree T of sentence S and v?
a node in the syntactictree T ?
of sentence S?.
A labeled node alignmentis a tuple < v, v?, r > where r is a label from a setof relations.
A labeled tree alignment is a set oflabeled node alignments.
A labeled tree matchingis a tree alignment in which each node is alignedto at most one other node.For each node v, its terminal yield STR(v) is de-fined as the sequence of all terminal nodes reach-able from v (i.e., a substring of sentence S).Aligning node v to v?
with label r indicates thatrelation r holds between their yields STR(v) andSTR(v?).
We label alignments according to a smallset of semantic similarity relations.
As an exam-ple, consider the following Dutch sentences:(1) a. DagelijksDailykoffiecoffeevermindertdiminishesrisicoriskoponAlzheimerAlzheimerenandDementie.Dementia.b.
DrieThreekoppencupskoffiecoffeeperadagdayreduceertreduceskanschanceoponParkinsonParkinsonenandDementie.Dementia.The corresponding syntax trees and their (partial)alignment is shown in Figure 1.
We distinguishthe following five mutually exclusive similarityrelations:1. v equals v?
iff lower-cased STR(v) andlower-cased STR(v?)
are identical ?
example:Dementia equals Dementia;2. v restates v?
iff STR(v) is a proper para-phrase of STR(v?)
?
example: diminishes re-states reduces;3. v generalizes v?
iff STR(v) is more generalthan STR(v?)
?
example: daily coffee gener-alizes three cups of coffee a day;4. v specifies v?
iff STR(v) is more specific thanSTR(v?)
?
example: three cups of coffee a dayspecifies dailly coffee;5. v intersects v?
iff STR(v) and STR(v?)
sharemeaning, but each also contains unique infor-mation not expressed in the other ?
example:Alzheimer and Dementia intersects Parkin-son and Dementia.Our interpretation of these relations is one ofcommon sense rather than strict logic, akin tothe definition of entailment employed in the RTEchallenge (Dagan et al, 2005).
Note also that re-lations are prioritized: equals takes precedence753smainnp vermindert npDagelijks koffienpsmaminpvemrreduceertdmrtntmrrisico ppop conjAltzheimer en DementieconjDatmirmgtrDementiel jknprsmainnpDrie koppen koffie ppper dagkans ppopParkinson enFigure 1: Example of two aligned and labeled syntactic trees.
For expository reasons the alignment isnot exhaustive.over restates, etc.
Furthermore, equals, restatesand intersects are symmetrical, whereas general-izes is the inverse of specifies.
Finally, nodes con-taining unique information, such as Alzheimer andParkinson, remain unaligned.3 Related workMany syntax-based approaches to machine trans-lation rely on bilingual treebanks to extract trans-fer rules or train statistical translation models.
Inorder to build bilingual treebanks a number ofmethods for automatic tree alignment have beendeveloped, e.g., (Gildea, 2003; Groves et al,2004; Tinsley et al, 2007; Lavie et al, 2008).Most related to our approach is the work on dis-criminative tree alignment by Tiedemann & Kotze?(2009).
However, these algorithms assume thatsource and target sentences express the same in-formation (i.e.
parallel text) and cannot copewith comparable text where parts may remain un-aligned.
See (MacCartney et al, 2008) for furtherarguments and empirical evidence that MT align-ment algorithms are not suitable for aligning par-allel monolingual text.MacCartney, Galley, and Manning (2008) de-scribe a system for monolingual phrase alignmentbased on supervised learning which also exploitsexternal resources for knowledge of semantic re-latedness.
In contrast to our work, they do notuse syntactic trees or similarity relation labels.Partly similar semantic relations are used in (Mac-Cartney and Manning, 2008) for modeling seman-tic containment and exclusion in natural languageinference.
Marsi & Krahmer (2005a) is closelyrelated to our work, but follows a more com-plicated method: first a dynamic programming-based tree alignment algorithm is applied, fol-lowed by a classification of similarity relations us-ing a supervised-classifier.
Other differences arethat their data set is much smaller and consistsof parallel rather than comparable text.
A majordrawback of this algorithmic approach it that itcannot cope with crossing alignments.
We are notaware of other work that combines alignment withsemantic relation labeling, or algorithms whichperform both tasks simultaneously.4 Data collectionFor developing our alignment algorithm we usethe DAESO corpus1.
This is a Dutch parallelmonolingual treebank of 1 million words, halfof which were manually annotated.
The corpusconsists of pairs of sentences with different lev-els of semantic overlap, ranging from high (dif-ferent Dutch translations of books from Darwin,Montaigne and Saint-Exupe?ry) to low (differentpress releases from the two main news agenciesin The Netherlands, ANP and NOVUM).
For thispaper, we concentrate on the latter part of theDAESO corpus, where the proportion of Equalsand Restates is relatively low.
This corpus seg-ment consists of 8,248 pairs of sentences, contain-ing 162,361 tokens (ignoring punctuation).
Allsentences were tokenized and tagged, and subse-quently parsed by the Alpino dependency parserfor Dutch (Bouma et al, 2001).
Two annota-1http://daeso.uvt.nl754Alignment: Labeling:Eq: Re: Spec: Gen: Int: Macro: Micro:Words: F: 95.38 95.48 58.50 65.81 65.00 25.85 62.11 88.72SD: 2.16 2.69 7.63 13.05 11.25 18.74Full trees: F: 88.31 95.83 71.38 60.21 66.71 62.67 71.36 81.92SD: 1.15 2.27 3.77 7.63 8.17 6.14Table 1: Average F-scores (in percentages, with Standard Deviations) for the six human annotators onalignment and semantic relation labeling, for words and for full syntactic trees.tors determined which sentences in the compa-rable news reports contained semantic overlap.Six other annotators produced manual alignmentsof words and phrases in matched sentence pairs,which resulted in 86,227 aligned pairs of nodes.A small sample of 10 similar press releasescomprising a total of 48 sentence pairs was inde-pendently annotated by all six annotators to deter-mine inter-annotator agreement.
We used preci-sion, recall and F-score on alignment.
To calcu-late these scores for relation labeling, we simplyrestrict the set of alignments to those labeled witha particular relation, ignoring all others.
Likewise,we restrict these sets to terminal node alignmentsin order to get scores on word alignment.Given the six annotations A1, .
.
.
, A6, we re-peatedly took one as the True annotation againstwhich the five other annotations were evaluated.We then computed the average scores over these6 ?
5 = 30 scores (note that with this proce-dure, precision, recall and F score end up beingequal).
Table 1 summarizes the results, both forword alignments and for full syntactic tree align-ment.
It can be seen that for alignment of words anaverage F-score of over 95 % was obtained, whilealignment for full syntactic trees results in an F-score of 88%.
For relation labeling, the scores dif-fered per relation, as is to be expected: the averageF-score for Equals was over 95% for both wordand full tree alignment2, and for the other rela-tions average F-scores between 0.6 and 0.7 were2At first sight, it may seem that labeling Equals is a trivialand deterministic task, for which the F-score should alwaysbe close to 100%.
However, the same word may occur multi-ple times in the source or target sentences, which introducesambiguity.
This frequently occurs with function words suchas determiners and prepositions.
Moreover, choosing amongseveral equivalent Equals alignments may sometimes involvea somewhat arbitrary decision.
This situation arises, for in-stance, when a proper noun is mentioned just once in thesource sentence but twice in the target sentence.obtained.
The exception to note is Intersects onword level, which only occurred a few times ac-cording to a few of the annotators.
The macroand micro (weighted) F-score averages on labeledalignment are 62.11% and 88.72% for words, and71.36% and 81.92% for full syntactic trees.5 Memory-based Graph MatcherIn order to automatically perform the alignmentand labeling tasks described in Section 2, we castthese tasks simultaneously as a combination of ex-haustive pairwise classification using a supervisedmachine learning algorithm, followed by globaloptimization of the alignments using a combina-torial optimization algorithm.
Input to the treematching algorithm is a pair of syntactic trees con-sisting of a source tree Ts and a target tree Tt.Step 1: Feature extraction For each possiblepairing of a source node ns in tree Ts and a targetnode nt in tree Tt, create an instance consisting offeature values extracted from the input trees.
Fea-tures can represent properties of individual nodes,e.g.
the category of the source node is NP, or rela-tions between nodes, e.g.
source and target nodeshare the same part-of-speech.Step 2: Classification A generic supervisedclassifier is used to predict a class label for eachinstance.
The class is either one of the seman-tic similarity relations or the special class none,which is interpreted as no alignment.
Our im-plementation employs the memory-based learnerTiMBL (Daelemans et al, 2009), a freely avail-able, efficient and enhanced implementation of k-nearest neighbour classification.
The classifier istrained on instances derived according to Step 1from a parallel treebank of aligned and labeledsyntactic trees.755Step 3: Weighting Associate a cost with eachprediction so that high costs indicate low confi-dence in the predicted class and vice versa.
Weuse the normalized entropy of the class labels inthe set of nearest neighbours (H) defined asH = ?
?c?C p(c) log2 p(c)log2|C|(1)where C is the set of class labels encountered inthe set of nearest neighbours (i.e., a subset of thefive relations plus none), and p(c) is the probabil-ity of class c, which is simply the proportion ofinstances with class label c in the set of nearestneighbours.
Intuitively this means that the costis zero if all nearest neighbours are of the sameclass, whereas the cost goes to 1 if the nearestneighbours are equally distributed over all possi-ble classes.Step 4: Matching The classification step willusually give rise to one-to-many alignment ofnodes.
In order to reduce this to just one-to-onealignments, we search for a node matching whichminimizes the sum of costs over all alignments.This is a well-known problem in combinato-rial optimization known as the Assignment Prob-lem.
The equivalent in graph-theoretical termsis a minimum weighted bipartite graph match-ing.
This problem can be solved in polynomialtime (O(n3)) using e.g., the Hungarian algorithm(Kuhn, 1955).
The output of the algorithm is thelabeled tree matching obtained by removing allnode alignments labeled with the special none re-lation.6 Experiments6.1 Experimental setupWord alignment and full tree alignments are con-ceptually different tasks, which require partly dif-ferent features and may have different practicalapplications.
These are therefore addressed inseparate experiments.Table 2 summarizes the respective sizes of de-velopment and the held-out test set in terms ofnumber of aligned graph pairs, number of alignednode pairs and number of tokens.
The percentageof aligned nodes over all graphs is calculated rela-tive to the number of nodes over all graphs.
SinceData Graph Node Tokens Alignedpairs pairs nodes (%)word develop 2 664 13 027 45 149 15.71word test 547 2 858 10 005 14.96tree develop 2 664 22 741 45 149 47.20tree test 547 4 894 10 005 47.05Table 2: Properties of develop and test data setsData Eq Re Spec Gen Intword develop 84.92 6.15 2.10 1.77 5.07word test 85.62 6.09 2.17 1.99 4.13tree develop 56.61 6.57 7.52 6.38 22.91tree test 58.40 7.11 7.40 6.38 20.72Table 3: Distribution of semantic similarity rela-tions for word alignment and for full tree align-ments in both develop and test data setsalignments involving non-terminal nodes are ig-nored in the task of word alignment, the number ofaligned node pairs and the percentage of alignednodes is lower in the word develop and word testsets.
Table 3 gives the distribution of semantic re-lations in the development and test set, for wordand tree alignment.
It can be observed that thedistribution if fairly skewed with Equals being themajority class, even more so for word alignments.Another thing to notice is that Intersects are muchmore frequent at the level of non-terminal align-ments.Development was carried out using 10-foldcross validation on the development data and con-sequently reported scores on the development dataare averages over 10 folds.
Only two parameterswere coarsely optimized on the development set.First, the amount of downsampling of the noneclass varied between 0.1 or 0.5.
Second, the pa-rameter k of the memory-based classifier ?
thenumber of nearest neighbours taken into accountduring classification ?
ranged from 1 to 15.
Opti-mal settings were finally applied when testing onthe held-out data.A simple greedy alignment procedure served asbaseline.
For word alignment, identical words arealigned as Equals and identical roots as Restates.For full tree alignment, this is extended to the levelof phrases so that phrases with identical words arealigned as Equals and phrases with identical rootsas Restates.
The baseline does not predict Spec-756ifies, Generalizes or Intersects relations, as thatwould require a more involved, knowledge-basedapproach.All features used are described in Table 4.The word-based features rely on pure string pro-cessing and require no linguistic preprocessing.The morphology-based features exploit the lim-ited amount of morphological analysis providedby the Alpino parser (Bouma et al, 2001).
Forinstance, it provides word roots and decomposescompound words.
Likewise the part-of-speech-based features use the coarse-grained part-of-speech tags assigned by the Alpino parser.
Thelexical-semantic features rely on the Cornettodatabase (Vossen et al, 2008), a recent exten-sion to the Dutch WordNet, to look-up synonymand hypernym relations among source and tar-get lemmas.
Unfortunately there is no wordsense disambiguation module to identify the cor-rect senses.
In addition, a background corpusof over 500M words of (mainly) news text pro-vides the word counts required to calculate theLin similarity measure (Lin, 1998).
The syntax-based features use the syntactic structure, whichis a mix of phrase-based and dependency-basedanalysis.
The phrasal features express similar-ity between the terminal yields of source and tar-get nodes.
With the exception of same-parent-lc-phrase, these features are only used for full treealignment, not for word alignment.6.2 Results on word alignmentWe evaluate our alignment model in two steps:first focussing on word alignment and then on fulltree alignment.
Table 5 summarizes the results forMBGM on word alignment (50% downsamplingand k = 3), which we compare statistically to thebaseline performance, and informally with the hu-man scores reported in Table 1 in Section 4 (notethat the human scores are only for a subset of thedata used for automatic evaluation).The first thing to observe is that the MBGMscores on the development and tests sets arevery similar throughout.
For predicting wordalignments, the MBGM system performs signif-icantly better than the baseline system (t(18) =17.72, p < .0001).
On the test set, MBGM ob-tains an F-score of nearly 89%, which is almostexactly halfway between the scores of the base-line system and the human scores.
In a similarvein, the performance of the MBGM system onrelation labeling is considerably better than thatof the baseline system.
For all semantic rela-tions, MBGM performs significantly better thanthe baseline (t(18) > 9.4138, p < .0001 for eachrelation, trivially so for the Specifies, Generalizesand Intersects relations, which the baseline systemnever predicts).The macro scores are plain averages over the 5scores on each relation, whereas the micro scoresare weighted averages.
As the Equals is the major-ity class and at the same time easiest to predict, themicro scores are higher.
The macro scores, how-ever, better reflect performance on the real chal-lenge, that is, correctly predicting the relationsother than Equals.
The MBGM macro averageis 27.37% higher than the baseline (but still some10% below the human top line), while the microaverage is 5.83% higher and only 0.75% belowthe human top line.
Macro scores on the test setare overall lower than those on the develop set,presumably because of tuning on the developmentdata.6.3 Results on tree alignmentTable 6 contains the results of full tree alignment(50% downsampling and k = 5); here both termi-nal and non-terminal nodes are aligned and clas-sified in one pass.
Again scores on the develop-ment and test set are very similar, the latter beingslightly better.
For full tree alignment, MBGMonce again performs significantly better than thebaseline, t(18) = 25.68, p < .0001.
With an F-score on the test set of 86.65, MBGM scores al-most 20 percent higher than the baseline system.This F-score is less than 2% lower than the aver-age F-score obtained by our human annotators onfull tree alignment, albeit not on exactly the samesample.
The picture that emerges for semantic re-lation labeling is closely related to the one we sawfor word alignments.
MBGM significantly out-performs the baseline, for each semantic relation(t(18) > 12.6636, p < .0001).
MBGM scores amacro average F-score of 52.24% (an increase of30.05% over the baseline) and a micro average of80.03% (12.68% above the base score).
It is inter-757Feature Type DescriptionWordword-subsumption string indicate if source word equals, has as prefix, is a prefix of, has a suffix, is asuffix of, has as infix or is an infix of target wordshared-pre-/in-/suffix-len int length of shared prefix/infix/suffix in characterssource/target-stop-word bool test if source/target word is in a stop word list of frequent function wordssource/target-word-len int length of source/target word in charactersword-len-diff int word length difference in characterssource/target-word-uniq bool test if source/target word is unique in source/target sentencesame-words-lhs/rhs int no.
of identical preceding/following words in source and target word contextsMorphologyroot-subsumption string indicate if source root equals, has as prefix, is a prefix of, has a suffix, is a suffixof, has as infix or is an infix of target rootroots-share-pre-/in-/suffix bool source and target root share a prefix/infix/suffixPart-of-speechsource/target-pos string source/target part-of-speechsame-pos bool test if source and target have same part-of-speechsource/target-content-word bool test if source/target word is a content wordboth-content-word bool test if both source and target word are content wordsLexical-semantic using Cornettocornet-restates float 1.0 if source and target words are synonyms and 0.5 if they are near-synonyms,zero otherwisecornet-specifies float Lin similarity score if source word is a hyponym of target word, zero otherwisecornet-generalizes float Lin similarity score if source word is a hypernym of target word, zero otherwisecornet-intersects float Lin similarity score if source word share a common hypernym, zero otherwiseSyntaxsource/target-cat string source/target syntactic categorysame-cat bool test if source and target have same syntactic categorysource/target-parent-cat string source/target syntactic category of parent nodesource/target-deprel string source/target dependency relationsame-deprel bool test if source and target have same dependency relationsame-dephead-root bool test if the dependency heads of the source and target have same rootPhrasalword-prec/rec float precision/recall on the yields of source and target nodessame-lc-phrase bool test if lower-cased yields of source and target nodes are identicalsame-parent-lc-phrase bool test if lower-cased yields of parents of source and target nodes are identicalsource/target-phrase-len int length of source/target phrase in wordsphrase-len-diff int phrase length difference in wordsTable 4: Features (where slashes indicate multiple versions of the same feature, e.g.
source/target-posrepresents the two features source-pos and target-pos)esting to observe that MBGM obtains higher F-scores on Equals and on Intersects (the two mostfrequent relations) than the human annotators ob-tained.
As a result of this, the micro F-score ofthe automatic full tree alignment is less than 2%lower than the human reference score.Tree alignment can also be implemented as atwo-step procedure, where in the first step align-ments and semantic relation classifications at theword level are produced, while in the second stepthese are used to predict alignments and seman-tic relations for non-terminals.
We experimentedwith such a two-step procedure as well, in one ver-sion using the actual word alignments and in theother the predicted word alignments.
The scoresof the two-step prediction are only marginally dif-ferent from those of one step prediction, both foralignment and for relation classification, givingimprovements in the order of about 1% for bothsubtasks.
As is to be expected, the scores withtrue word alignments are much better than thosewith predicted word alignments.
They are inter-esting though, because they suggest that a fairlygood full tree alignment can be automatically ob-758Alignment: Labeling:Eq: Re: Spec: Gen: Int: Macro: Micro:Prec: 80.59 81.84 46.26 0.00 0.00 0.00 25.61 80.22Develop baseline: Rec: 81.58 93.10 34.71 0.00 0.00 0.00 25.56 82.20F: 81.08 87.11 39.66 0.00 0.00 0.00 25.35 80.70Prec: 91.72 94.54 61.26 74.60 67.82 45.80 68.80 90.82Develop MBGM: Rec: 87.82 95.91 46.19 40.87 43.22 27.27 50.61 86.96F: 89.73 95.02 52.67 52.81 52.80 34.19 57.50 88.85Prec: 82.45 83.83 43.12 0.00 0.00 0.00 25.39 82.17Test baseline: Rec: 82.19 93.87 27.01 0.00 0.00 0.00 24.18 82.02F: 82.32 88.57 33.22 0.00 0.00 0.00 24.36 82.14Prec: 90.92 94.20 53.33 59.87 54.21 42.47 60.84 89.90Test MBGM: Rec: 87.09 95.41 40.21 32.75 43.28 20.31 46.39 86.11F: 88.96 94.80 45.85 42.34 48.17 27.48 51.73 87.97Table 5: Scores (in percentages) on word alignment and semantic relation labelingAlignment: Labeling:Eq: Re: Spec: Gen: Int: Macro: Micro:Prec: 82.50 83.76 46.72 0.00 0.00 0.00 26.10 82.18Develop baseline: Rec: 54.54 93.66 20.01 0.00 0.00 0.00 22.74 54.34F: 65.67 88.43 28.02 0.00 0.00 0.00 23.29 65.42Prec: 92.23 96.15 55.90 54.40 56.15 70.33 66.59 84.99Develop MBGM: Rec: 81.04 94.03 26.64 21.71 29.34 70.27 48.40 74.68F: 86.27 95.08 36.08 31.03 38.54 70.30 54.21 79.50Prec: 84.23 85.68 42.24 0.00 0.00 0.00 25.58 84.14Test baseline: Rec: 56.21 94.44 14.08 0.00 0.00 0.00 21.70 56.15F: 67.43 89.85 21.12 0.00 0.00 0.00 22.19 67.35Prec: 92.27 96.67 60.25 46.92 56.85 68.64 65.87 85.23Test MBGM: Rec: 81.67 94.54 27.87 19.55 30.94 71.01 48.87 75.44F: 86.65 95.60 38.11 27.60 40.07 69.80 54.24 80.03Table 6: Scores (in percentages) on full tree alignment and semantic relation labelingtained given a manually checked word alignment.7 ConclusionsWe have proposed to analyse semantic similaritybetween comparable sentences by aligning theirsyntax trees, matching each node to the most sim-ilar node in the other tree (if any).
In addi-tion, alignments are labeled with a semantic sim-ilarity relation.
We have presented a Memory-based Graph Matcher (MBGM) that performsboth tasks simultaneously as a combination of ex-haustive pairwise classification using a memory-based learning algorithm, and global optimizationof alignments using a combinatorial optimizationalgorithm.
It relies on a combination of morpho-logical/syntactic analysis, lexical resources suchas word nets, and machine learning using a par-allel monolingual treebank.
Results on aligningcomparable news texts from a monolingual paral-lel treebank for Dutch show that MBGM consis-tently and significantly outperforms the baseline,both for alignment and labeling.
This holds bothfor word alignment and tree alignment.In future research we will test MBGM on otherdata, as the DAESO corpus contains sub-corporawith various degrees of semantic overlap.
In addi-tion, we intend to explore alternative features fromword space models.
Finally, we plan to evaluateMBGM in the context of NLP applications suchas multi-document summarization.
This includeswork on how to define similarity at the sentencelevel in terms of the proportion of aligned con-stituents.
Both MBGM and the annotated data setwill be publicly released.2759AcknowledgmentsThis work was conducted within the DAESOproject funded by the Stevin program (De Ned-erlandse Taalunie).ReferencesBarzilay, Regina and Kathleen R. McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Computational Linguistics, 31(3):297?328.Bouma, Gosse, Gertjan van Noord, and Robert Malouf.2001.
Alpino: Wide-coverage computational analysis ofDutch.
In Daelemans, Walter, Khalil Sima?an, Jorn Veen-stra, and Jakub Zavre, editors, Computational Linguisticsin the Netherlands 2000., pages 45?59.
Rodopi, Amster-dam, New York.Daelemans, W., J. Zavrel, K. Van der Sloot, andA.
Van den Bosch.
2009.
TiMBL: Tilburg MemoryBased Learner, version 6.2, reference manual.
TechnicalReport ILK 09-01, Induction of Linguistic Knowledge,Tilburg University.Dagan, I., O. Glickman, and B. Magnini.
2005.
The PAS-CAL Recognising Textual Entailment Challenge.
In Pro-ceedings of the PASCAL Challenges Workshop on Recog-nising Textual Entailment, Southampton, U.K.Gildea, Daniel.
2003.
Loosely tree-based alignment formachine translation.
In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics,pages 80?87, Sapporo, Japan.Groves, D., M. Hearne, and A.
Way.
2004.
Robust sub-sentential alignment of phrase-structure trees.
In Pro-ceedings of the 20th International Conference on Com-putational Linguistics (CoLing ?04), pages 1072?1078.Krahmer, Emiel, Erwin Marsi, and Paul van Pelt.
2008.Query-based sentence fusion is better defined and leadsto more preferred results than generic sentence fusion.
InMoore, J., S. Teufel, J. Allan, and S. Furui, editors, Pro-ceedings of the 46th Annual Meeting of the Associationfor Computational Linguistics: Human Language Tech-nologies, pages 193?196, Columbus, Ohio, USA.Kuhn, Harold W. 1955.
The Hungarian Method for the as-signment problem.
Naval Research Logistics Quarterly,2:83?97.Lavie, A., A. Parlikar, and V. Ambati.
2008.
Syntax-driven learning of sub-sentential translation equivalentsand translation rules from parsed parallel corpora.
In Pro-ceedings of the Second Workshop on Syntax and Structurein Statistical Translation, pages 87?95.Lin, D. 1998.
An information-theoretic definition of similar-ity.
In Proceedings of the 15th International Conferenceon Machine Learning, pages 296?304.MacCartney, B. and C.D.
Manning.
2008.
Modeling seman-tic containment and exclusion in natural language infer-ence.
In Proceedings of the 22nd International Confer-ence on Computational Linguistics-Volume 1, pages 521?528.MacCartney, Bill, Michel Galley, and Christopher D. Man-ning.
2008.
A phrase-based alignment model for naturallanguage inference.
In Proceedings of the 2008 Confer-ence on Empirical Methods in Natural Language Process-ing, pages 802?811, Honolulu, Hawaii, October.Marsi, Erwin and Emiel Krahmer.
2005a.
Classification ofsemantic relations by humans and machines.
In Proceed-ings of the ACL 2005 workshop on Empirical Modelingof Semantic Equivalence and Entailment, pages 1?6, AnnArbor, Michigan.Marsi, Erwin and Emiel Krahmer.
2005b.
Explorations insentence fusion.
In Proceedings of the 10th EuropeanWorkshop on Natural Language Generation, Aberdeen,GB.Radev, D.R.
and K.R.
McKeown.
1998.
Generating naturallanguage summaries from multiple on-line sources.
Com-putational Linguistics, 24(3):469?500.Tiedemann, J. and G. Kotze?.
2009.
Building a LargeMachine-Aligned Parallel Treebank.
In Eighth Interna-tional Workshop on Treebanks and Linguistic Theories,page 197.Tinsley, J., V. Zhechev, M. Hearne, and A.
Way.
2007.
Ro-bust language-pair independent sub-tree alignment.
Ma-chine Translation Summit XI, pages 467?474.Vossen, P., I. Maks, R. Segers, and H. van der Vliet.
2008.Integrating lexical units, synsets and ontology in the Cor-netto Database.
In Proceedings of LREC 2008, Mar-rakech, Morocco.760
