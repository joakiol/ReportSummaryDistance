Proceedings of NAACL-HLT 2013, pages 168?178,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsDrug Extraction from the Web:Summarizing Drug Experiences with Multi-Dimensional Topic ModelsMichael J. Paul and Mark DredzeHuman Language Technology Center of ExcellenceCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218{mpaul,mdredze}@cs.jhu.eduAbstractMulti-dimensional latent text models, such asfactorial LDA (f-LDA), capture multiple fac-tors of corpora, creating structured output forresearchers to better understand the contentsof a corpus.
We consider such models forclinical research of new recreational drugs andtrends, an important application for miningcurrent information for healthcare workers.We use a ?three-dimensional?
f-LDA variantto jointly model combinations of drug (mari-juana, salvia, etc.
), aspect (effects, chemistry,etc.)
and route of administration (smoking,oral, etc.)
Since a purely unsupervised topicmodel is unlikely to discover these specificfactors of interest, we develop a novel methodof incorporating prior knowledge by leverag-ing user generated tags as priors in our model.We demonstrate that this model can be usedas an exploratory tool for learning about thesedrugs from the Web by applying it to the taskof extractive summarization.
In addition toproviding useful output for this important pub-lic health task, our prior-enriched model pro-vides a framework for the application of f-LDA to other tasks.1 IntroductionTopic models aid exploration of the main thematicelements of large text corpora by revealing latentstructure and producing a high level semantic view(Blei et al 2003).
Topic models have been used forunderstanding the contents of a corpus and identify-ing interesting aspects of a collection for more in-depth analysis (Talley et al 2011; Mimno, 2011).While standard topic models assume a flat seman-tic structure, there are potentially many dimen-sions of a corpus that contribute to word choice,such as sentiment, perspective and ideology (Mei etal., 2007; Paul and Girju, 2010; Eisenstein et al2011).
Rather than studying these factors in isola-tion, multi-dimensional topic models can considermultiple factors jointly.Paul and Dredze (2012b) introduced factorialLDA (f-LDA), a general framework for multi-dimensional text models that capture an arbitrarynumber of factors (explained in ?3).
While a stan-dard topic model learns distributions over ?topics?in documents, f-LDA learns distributions over com-binations of multiple factors (e.g.
topic, perspec-tive) called tuples (e.g.
(HEALTHCARE,LIBERAL)).While f-LDA can model factors without supervision,it has not been used in situations where the user hasprior information about the factors.In this paper we consider a setting where the userhas prior knowledge about the end application: min-ing recreational drug trends from user forums, animportant clinical research problem (?2).
We showhow to incorporate available information from theseforums into f-LDA as a novel hierarchical prior overthe model parameters, guiding the model toward thedesired output (?3.1).We then demonstrate the model?s utility in ex-ploring a corpus in a targeted manner by using itto automatically extract interesting sentences fromthe text, a simple form of extractive multi-documentsummarization (Goldstein et al 2000).
In thesame way that topic models can be used for aspect-specific summarization (Titov and McDonald, 2008;Haghighi and Vanderwende, 2009), we use f-LDAto extract snippets corresponding to fine-grained in-formation patterns.
Our results demonstrate that ourmulti-dimensional modeling approach targets moreinformative text than a simpler model (?4).1682 Analyzing Drug Trends on the WebRecreational drug use imposes a significant burdenon the health infrastructure of the United States andother countries.
Accurate information on drugs, us-age profiles and side effects are necessary for sup-porting a range of healthcare activities, such as ad-diction treatment programs, toxin diagnosis, preven-tion and awareness campaigns, and public policy.These activities rely on up-to-date information ondrug trends, but it is increasingly difficult to keepup with current drug information, as distribution andinformation-sharing of novel drugs is easier thanever via the web (Wax, 2002).
For the third con-secutive year, a record number of new drugs (49)were detected in Europe in 2011 (EMCDDA, 2012).About two-thirds of these new drugs were syntheticcannabinoids (used as legal marijuana substitutes),which led to 11,000 hospitalizations in the U.S. in2010 (SAMHSA, 2012).
Treatment is complicatedby the fact that novel substances like these may haveunknown side effects and other properties.Accurate information on drug trends can be ob-tained by speaking directly with users, e.g.
focusgroups and interviews (Reyes et al 2012; Houtand Bingham, 2012), but such studies are slow andcostly, and can fail to identify the emergence ofnew drug classes, such as mephedrone (Dunn etal., 2011).
More recently, researchers have begunto recognize clinical value in information obtainedfrom the web (Corazza et al 2011).
By (manu-ally) analyzing YouTube videos, Drugs-Forum (dis-cussed below), and other social media websites andonline communities, researchers have uncovered de-tails about the use, effects, and popularity of a va-riety of new and emerging drugs (Morgan et al2010; Corazza et al 2012; Gallagher et al 2012),and comprehensive drug reviews now include non-standard sources such as web forums in addition tostandard sources (Hill and Thomas, 2011).Organizing and understanding forums requiressignificant effort.
We propose automated tools to aidin the exploration and analysis of these data.
Whiletopic models are a natural fit for corpus exploration(Eisenstein et al 2012; Chaney and Blei, 2012), andhave been used for similar public health applications(Paul and Dredze, 2011), online forums can be orga-nized in many ways beyond topic.
Guided by do-Factor ComponentsDrug ALCOHOL AMPHETAMINES BETA-KETONESCANNABINOIDS CANNABIS COCAINE DMT DOWN-ERS DXM ECSTASY GHB HERBAL ECSTASY KE-TAMINE KRATOM LSA LSD NOOTROPICS OPIATESPEYOTE PHENETHYLAMINES SALVIA TOBACCORoute INJECTION ORAL SMOKING SNORTINGAspect CHEMISTRY (Pharmacology, TEK)CULTURE (Culture, Setting, Social, Spiritual)EFFECTS (Effects)HEALTH (Health, Overdose, Side effects)USAGE (Dose, Storing, Weight)Table 1: The three factors of our model (details in ?3.1).The forum tags shown in parentheses are grouped to-gether to form aspects.main experts, we seek to model forums as a combi-nation of drug type, route of intake (oral, injection,etc.)
and aspect (cultural settings, drug chemistry,etc.)
A multi-dimensional topic model can jointlycapture these factors, providing a more informativeunderstanding of the data, and can be used to pro-duce fine-grained information such as the effects oftaking a particular drug orally.
Our hope is that mod-els such as f-LDA can lead to exploratory tools thataide researchers in learning about new drugs.2.1 Corpus: Drugs-ForumOur data set is taken from drugs-forum.com, asite active for more than 10 years with over 100,000members and more than 1 million monthly readers.The site is an information hub where people canfreely discuss recreational drugs with psychoactiveeffects, ranging from coffee to heroin, hosting in-formation and discussions on specific drugs, as wellas drug-related politics, law, news, recovery and ad-diction.
With current information on a variety ofdrugs and an extensive archive, Drugs-Forum pro-vides an ideal information source for public healthresearchers (Corazza et al 2012).Discussion threads are organized into numerousforums, including drugs, the law, addiction, etc.Since we are modeling drug use, we focus on thedrug forums.
Each thread is assigned to a specificforum or subforum (drug) and each thread has a userspecified tag, which can indicate categories like ?Ef-fects?
as well as routes of administration like ?Oral.
?We organized the tags and subforum categorizationsinto factors and components, as shown in Table 1.We make use of these tags in ?3.1.1693 Multi-Dimensional Text ModelsClinical researchers are interested in specific infor-mation about drug usage, including drug type, routeof administration, and other aspects of drug use(e.g.
dosage, side effects).
Rather than consideringthese factors independently, we would like to modelthese in a way that can capture interesting interac-tions between all three factors, because the effectsand other aspects of drugs can vary by route of ad-ministration.
Oral consumption of drugs often pro-duces longer lasting but milder effects than injec-tion or smoking, for example.
Many mephedroneusers report nose bleeds and nasal pain as a healtheffect of snorting the drug: this could be modeledas the triple (MEPHEDRONE,SNORTING,HEALTH), aparticular combination of all three factors.To this end, we utilize the multi-dimensional textmodel factorial LDA (f-LDA) (Paul and Dredze,2012b), which jointly models multiple semantic fac-tors or dimensions.
In this section we summarize f-LDA, then we describe an extension which incorpo-rates user-generated metadata into the model (?3.1).In a standard topic model such as LDA (Blei etal., 2003), each word token is associated with a la-tent ?topic?
variable.
f-LDA is conceptually similarto LDA except that rather than a single topic vari-able, each token is associated with a K-dimensionalvector of latent variables.
In a three-dimensional f-LDA model, each token has three latent variables?drug, route, and aspect in this case.In f-LDA, each document has a distributionover all possible K-tuples (rather than topics),and each K-tuple is associated with its own worddistribution.
Under this model, words are gen-erated by first sampling a tuple from the docu-ment?s tuple distribution, then sampling a wordfrom that tuple?s word distribution.
In our three-dimensional model, we will consider triples such as(CANNABIS,SMOKING,EFFECTS).Formally, each document has a distribution ?
(d)over triples, and each token is associated with a la-tent vector ~z of sizeK=3.
(We?ll describe the modelin terms of the three factors we are modeling in thispaper, but f-LDA generalizes toK dimensions.)
TheCartesian product of the three factors forms a setof triples and the vector ~z references three discretecomponents to form a triple ~t = (t1, t2, t3).
The car-??d?
??
?b?z wDNKKK?k Zk?k Zk?k ZkFigure 1: The graphical model for f-LDA augmentedwith priors ?
learned from labeled data (?3.1).
In thiswork, K = 3.dinality of each dimension (denoted Zk) is the num-ber of drugs, routes, and aspects, as shown in Table1.
Each triple has a corresponding word distribution?~t.
The graphical model is shown in Figure 1.One would expect that triples that have com-ponents in common should have similar worddistributions: (CANNABIS,SMOKING,EFFECTS)is expected to have some commonalities with(CANNABIS,ORAL,EFFECTS).
f-LDA models thisintuition by sharing parameters across priors fortriples which share components: all triples withCANNABIS as the drug include cannabis-specificparameters in the prior, and all triples with SMOK-ING as the route have smoking-specific parameters.Formally, ?~t (the word distribution for tuple ~t) has aDirichlet(??
(~t)) prior, where for each word w in thevector, ??
(~t)w is a log-linear function:??
(~t )w , exp(?(B)+?
(0)w +?
(drug)t1w +?
(route)t2w +?
(aspect)t3w)(1)where ?
(B) is a corpus-wide precision scalar (thebias), ?
(0)w is a corpus-specific bias for word w, and?
(k)tkw is a bias parameter for word w for componenttk of the kth factor.
That is, each drug, route, andaspect has a weight vector over the vocabulary, andthe prior for a particular triple is influenced by theweight vectors of each of the three factors.
The?
parameters are all independent and normally dis-tributed around 0 (effectively L2 regularization).The prior over each document?s distribution overtriples has a similar log-linear prior, where weightsfor each factor are combined to influence the dis-tribution.
Under our model, ?
(d) is drawn fromDirichlet(B ?
??
(d)), where ?
denotes an element-wiseproduct between B (described below) and ??
(d), with170??
(d)~t for each triple~t defined as:??
(d)~t , exp(?
(B) +?
(D,drug)t1 +?(d,drug)t1+?
(D,route)t2 +?(d,route)t2+?
(D,aspect)t3 +?
(d,aspect)t3)(2)Similar to the ?
formulation, ?
(B) is a globalbias parameter, while the ?D vectors are corpus-wide weight vectors and ?d are document-specificweight vectors over the components of each fac-tor.
Structuring the prior in this way models theintuition that if a triple with a particular compo-nent has high probability, other triples containingthat component are likely to also have high proba-bility.
For example, if a message discusses triplesof the form (CANNABIS,*,EFFECTS), it is morelikely to discuss (CANNABIS,*,HEALTH) than (CO-CAINE,*,HEALTH), because the message is aboutcannabis.Finally, B is a 3-dimensional array that encodesa sparsity pattern over the space of possible triples.This is used to accommodate triples that can be gen-erated by the model but are not supported by thedata.
For example, not all routes of administrationmay be applicable to certain drugs, or certain aspectsof a drug may happen to not be discussed in the fo-rum.
Each element b~t of the array is a real-valuedscalar in (0, 1) which is multiplied with ??
(d)~t to ad-just the prior for that triple.
If the b value is near0 for a particular triple, then it will have very lowprior probability.
The b values have Beta(?0,?1) pri-ors (?
< 1) which encourage them to be near 0 or 1,so that they function as binary variables.Posterior inference and parameter estimation con-sist of a Monte Carlo EM algorithm that alternatesbetween an iteration of collapsed Gibbs sampler onthe ~z variables (E-step), and an iteration of gradi-ent ascent on the ?
and ?
hyperparameters (M-step).See Paul and Dredze (2012b) for more details.3.1 Tags and Word PriorsIn an unsupervised setting, there is no reason f-LDAwould actually infer parameters corresponding tothe three factors we have been describing.
However,the forums include metadata that can help guide themodel: the messages are organized into forums cor-responding to drug type (factor 1), and some threadsCOCAINE SNORTING HEALTH?
(Prior over ?
)coke snort kidneycocaine snorting hcvcrack snorted painscola nose symptoms COCAINEblow nasal guidelines SNORTINGlines drip diet HEALTH?
(Prior over ?)
?
(Posterior)coke snort symptoms nosecocaine snorting long-term cocainecrack snorted depression cokecola passages disorder bloodrocks nostril schizophrenia watercoca insufflating severe painFigure 2: Example of parameters learned by f-LDA.
Thehighest weight words in the ?
and ?
vectors for threecomponents are shown on the left.
These are combinedto form the prior for the word distribution ?.
The triplingof (COCAINE,SNORTING,HEALTH) results in high proba-bility words about nose bleeds and nasal damage.are tagged with labels corresponding to routes of ad-ministration and other aspects (factors 2 and 3).
Tagsfor aspects are manually grouped into components:e.g.
USAGE (tags: Dose, Storing, Weight).
Table 1shows the factors and components in our model.One could simply use these tags as labels in a sim-ple supervised model?this will be our experimentalbaseline (?4.1).
However, this approach has limita-tions in that most documents are missing labels (lessthan a third of our corpus contains one of the labelsin Table 1) and many messages discuss several com-ponents, not just the one implied by the tag.
Forexample, a message tagged ?Side effects?
may talkabout both side effects and dosage.
While a super-vised classifier may attribute all words to a singletag, f-LDA learns per-token assignments.We will instead use the tags to inform the priorsover our f-LDA word distribution parameters.
Wedo this with a two-stage approach.
First, we use thetags to train parameters of a related but simplifiedmodel.
We then use the learned parameters as priorsover the corresponding f-LDA parameters.In particular, we will place priors on the ?
vectors,the Dirichlet hyperparameters which influence theword distributions.
Suppose that we are given a vec-tor ?
(0) which is believed to contain desirable valuesfor ?
(0), the weight vector over words in the corpus,and similarly we are given vectors ?
(f)i over the vo-cabulary for the ith component of factor f , whichare believed to be good values for ?
(f)i .
One option171is to fix ?
as ?, forcing the component weights tomatch the provided weights.
However, in our case ?will only be an approximation of the optimal com-ponent parameters since it is estimated from incom-plete data (only some messages have tags) and the ?vectors are learned using an approximate model (seebelow).
Instead, these weight vectors will merelyguide learning as prior knowledge over model pa-rameters ?.
While f-LDA assumes each ?
is drawnfrom a 0-mean Gaussian, we alter the means of theappropriate ?
parameters to use ?.?
(0)w ?
N (?
(0)w , ?2);?
(k)iw ?
N (?
(k)iw , ?2) (3)Recall that ?
(0)w are corpus-wide bias parameters foreach word and ?
(k)iw are component-specific param-eters for each word.
This yields a hierarchical priorin which ?
parameterizes the prior over ?, while ?parameterizes the prior over ?
(the word distribu-tions).
The resulting ?
parameters can vary from theprovided priors to adapt to the data.
An example oflearned parameters is shown in Figure 2, illustratingthe hierarchical process behind this model.Learning the Priors In various applications, pri-ors can come from many different sources, such aslabeled data (Jagarlamudi et al 2012).
We learnthe prior means ?
from tagged messages.
However,these parameters imply a latent division of responsi-bility for observed words: some are present becauseof the tag while others are general words in the cor-pus.
As a result, they must be estimated.We learn these parameters from the tagged mes-sages using SAGE, which model words in a docu-ment as combinations of background and topic worddistributions.
Eisenstein et al(2011) present SAGEmodels for Naive Bayes (one class per document),admixture models (one class per token), and admix-ture models where tokens come from multiple fac-tors.
We combine the first and third models, suchthat a document has multiple factors which are givenas labels across the entire document?the drug typeand the tag, which could correspond to a componentof either the route or aspect factors.
We posit thefollowing model of text generation per document:P (word w|drug = i, factorf = j) (4)=exp(?
(0)w + ?
(drug)iw + ?
(f)jw )?w?
exp(?(0)w?
+ ?(drug)iw?
+ ?(f)jw?
)This log-linear model has a similar form as Eq.1, but with two factors instead of three, and it isa distribution rather than a Dirichlet vector.
As inSAGE, we fix ?
(0) to be the observed vector of cor-pus log-frequencies over the vocabulary, which actsas an ?overall?
weight vector, while parameter esti-mation yields ?
(f)i , the logit parameters for the ithcomponent of factor f .1 These parameters are thenused as the mean of the Gaussian priors over ?.Standard optimization methods can be used to es-timate these parameters.
The partial derivative of thelikelihood with respect to the parameter ?
(drug)iw is:???
(drug)iw=?f?j?fc(i, j, w)?
pi(i, j, w)c(i, j, ?
)(5)where c(i, j, w) is the number of times word w ap-pears in documents labeled with i (drug) and j (tag),and pi(i, j, w) denotes the probability given by (4).The partial derivative of each ?
(f)j is similar.4 Experiments with Topic Modeling forExtractive SummarizationOur corpus consists of messages fromdrugs-forum.com (?2.1).
The site catego-rizes threads into many forums and subforums,including some on specific drugs, which are cat-egorized hierarchically.
We treated higher-levelcategories with pharmacologically similar drugs asa single drug type (e.g.
OPIOIDS, AMPHETAMINES);for others we took the finest-granularity subforumas the drug type.
We selected 22 popular drugs andfrom these forums we crawled 410K messages.
Weselected a subset of tags to form components forthe route and aspect factors.
(Some tags were toogeneral or infrequent to be useful.)
A list of thetags and drugs used appears in Table 1.
We alsoincluded a GENERAL component in the latter twofactors to model word usage which does not pertainto a particular route or aspect; the prior parameters?
for these components were simply set to 0.We wish to demonstrate that our modified f-LDAmodel can be used to discover useful information inthe text.
One way to demonstrate this is by using themodel to extract relevant snippets of text from the1SAGE models sparsity on the weights via a Laplacian prior.Such sparsity is not modeled in f-LDA, so we ignore this here.172forums, which will form the basis of our evaluationexperiments.
Our goal is not to build a completesummarization system, but rather to use the modelto direct researchers to interesting messages.While we model all 22 drugs, our summa-rization experiments will focus on five drugswhich have been studied only relatively recently:mephedrone and MDPV (?-ketones), Bromo-Dragonfly (synthetic phenethylamines), Spice/K2(synthetic cannabinoids), and salvia divinorum.
Wewill consider these drugs in particular because theseare the five drugs for which technical reports werecreated by the EU Psychonaut Project (Schifano etal., 2006), an online database of novel and emerg-ing drugs, whose information is collected by readingdrug websites, including Drugs-Forum.
Extensivetechnical reports were written about these five pop-ular drugs, and we can use these reports to producereference summaries for our experiments (?4.2).Of these five drugs, only salvia has its own sub-forum; the others belong to subforums representingthe broader categories shown in parentheses.
Wesimply model the drug type as a proxy for the spe-cific drug, as most of the drugs in each category havesimilar effects and properties.
The first two drugs areboth in the same subforum, so for the purpose of ourmodel we treat mephedrone and MDPV as the singledrug type, ?-ketones.
These two drugs are groupedtogether during summarization (?4.2), but the corre-sponding reference summaries incorporate exceptsfrom the technical reports on both drugs.4.1 Model SetupOf the four drug types being considered for summa-rization, our data set contains 12K messages withone of the tags in Table 1 and 30K without.
Ofthose without tags, we set aside 5K as developmentdata.
There are also over 300K messages (140Ktagged) from the remaining 18 drug types: someof these messages are utilized when training f-LDA.Even though we only consider four drug types in ourexperiments, our intuition is that it can be benefi-cial to model other drugs as well, because this willhelp to learn parameters for the various aspects androutes of administration.
Our model of the effects ofmephedrone can be informed by also modeling theeffects of other stimulants such as cocaine.Each message was treated as a document, and weonly used documents with at least five word tokensafter stop words, low-frequency words, and punc-tuation were removed.
The preprocessed data setscontained an average of 45 tokens per document.Below, we describe two f-LDA variants as well asthe baseline used in our experiments.Baseline Our baseline model is a unigram lan-guage model trained on the subset of messageswhich are tagged.
We treat the drug subforum asa label for the drug factor, and each message?s tagis used as a label for either the route or aspect fac-tor.
For example, the word distribution for the pair(SALVIA,EFFECTS) is estimated as the empirical dis-tribution from messages posted in the salvia forumand tagged with ?Effects.?
We use add-?
smooth-ing where ?
is chosen to optimize likelihood on theheld-out development set.This is a two-dimensional model, since we explic-itly model pairs such as (MEPHEDRONE,SNORTING)or (SALVIA,EFFECTS).
However, we also cre-ated word distributions for triples such as(SALVIA,ORAL,EFFECTS) by taking a mixtureof the corresponding pairs: in this example, weestimate the unigram distribution from salviadocuments tagged with either ?Oral?
or ?Effects.
?Factorial LDA Because f-LDA does not rely ontagged data (the tags are only used to create priors),we can run inference on larger sets of data.
Thedrawback is that despite these priors, it is still mostlyunsupervised and we want to be careful to ensurethe model will learn the patterns we care about.
Wethus add some reasonable constraints to the parame-ter space to guide the model further.First, we treat the drug type as an observed vari-able based on the subforum the message comesfrom, just as with the baseline.
For example, onlytuples of the form (SALVIA,?,?)
can be assigned totokens in the salvia forum.
Second, we restrict theset of possible routes of administration that can beassigned to tokens in particular drug forums, sincemost drugs can be taken through only a subset ofroutes.
For example, marijuana is typically smokedor eaten orally, but rarely injected.
We thereforerestrict each drug?s allowable set of administrationroutes to those which are tagged (e.g.
with ?Oral?
or?Snorting?)
in at least 1% of that drug?s data.
Sim-ilar ideas are used in Labeled LDA (Ramage et al173Reference Text System SnippetMephedrone (?-ketones/Bath salts)It is recommended by users that Mephedronebe taken on an empty stomach.
Doses usuallyvary between 100mg?1g.?
If it is SWIYs first time using Mephedrone SWIM recommendsa 100mg oral dose on an empty stomach.Reported negative side effects include:?
Loss of appetite.?
Dehydration and dry mouth?
Tense jaw, mild muscle clenching, stiff neck,and bruxia (teeth grinding)?
Anxiety and paranoia?
Increase in mean body temperature (sweat-ing/Mephedrone sweat and hot flushes)?
Elevated heart rate (tachycardia) and bloodpressure, and chest pains?
Dermatitis like symptoms (Itch and rash)?
Neutral side effects: Lack of appetite, occasional loss of visualfocus, [...] weight loss, possible diuretic.
Negative side effects:Grinding teeth, ?Cotton mouth?, unable to acheive orgasm?
Aside from his last session he has never experienced any neg-ative symptoms at all, no raised heart beat, vasoconstriction ,sweating, headaches, paranoia e.t.c nothing at all except some-times cold hands the next day.?
lot of people report that anxiety and paranoia are some of theside effects of taking mephedrone [...] is it also possible thatalot of the chest pains people are experiencing is due to anxiety??
moisturize the affected areas of skin twice daily with E45 or asimilar unperfumed dermatalogical lotion.Salvia divinorumSublingual ingestion of the leaf (quid): reducesintensity of effects and can taste disgusting.When Salvia is consumed as a smokeable for-mulation the duration of the trip lasts 30 min-utes or less, whereas if Salvia is consumed sub-lingually the effects lasts for 1 hour or more.?
The taste of sublingual salvia is foul and it is easy to have a dudtrip unless large amounts of it are used.?
SWIM has heard from many other users that chewing the freshleaves of the Salvia plant allow for a much longer and mellowertrip.
[...] SWIM has read that a trip this way can last anywherefrom a half on hour or longer.Dried leaves and/or salvia extract are smoked(using a butane lighter) either by pipe (consid-ered to be the most effective but is consideredto be quite painful) or water bong.?
2.
Use a water pipe.
Its harsh and needs to be smoked hot sothis should be self explanatory.
3.
Use a torch style lighter[...] Salvinorin A has a VERY high boiling point (around 700degrees F I believe) so a regular bic just wont do itSalvia is appealing to recreational users be-cause of intense, unique, hallucinatory effects.Brief hallucinations occur rapidly after admin-istration and are typically very vivid.
Users re-port weird thoughts, feelings of unreality, feel-ings of immersion in bizarre non-Euclidian di-mensions/geometries, feelings of floating.?
He noticed very clear [closed eye visuals], which looked similarto patterns on a persian rug, or ethnic oriental design.
SWIMfelt as if he was moving around, that he had got up and run andfallen, and that falling had shattered the space around his bodyas if I?d fallen through many glass framed pictures [...]?
I was aware of my body and my friends and my life below, butI was [...] standing outside of time and outside of space.Figure 3: Example snippets generated by f-LDA along with the corresponding reference text.
For space, the referencesand snippets shown have been shortened in some cases.
?SWIM?
and ?SWIY?
stand for ?someone who isn?t me/you?and are used to avoid self-incrimination on the web forum.2009), in which tags are used to restrict the space ofallowed topics in a document.We use f-LDA as a three-dimensional modelwhich explicitly models triples, but we also obtaindistributions for pairs such as (SALVIA,EFFECTS) bymarginalizing across all distributions of the form(SALVIA,?,EFFECTS).
We trained f-LDA on two dif-ferent data sets, yielding the following models:?
f-LDA-1: We use the 12K messages with tagsand fill the set out with 13K messages with tagsuniformly sampled from the 18 other drugs, fora total of 25K messages.?
f-LDA-2: We use all 37K messages (manywithout tags) and fill the set out with 63K mes-sages with tags uniformly sampled from the 18other drugs, for a total of 100K messages.All f-LDA instances are run with 5000 iterationsalternating between a sweep of Gibbs sampling fol-lowed by a step of gradient ascent on the hyperpa-rameters.
While we do not use the tags as strict la-bels during sampling, we initialize the Gibbs sam-pler so that each token in a document is assignedto its label given by the tag, when available.
In theabsence of tags (in f-LDA-2), we initialize tokens174to the GENERAL components.
We initialized ?
toits prior mean (Eq.
3), while the variance ?2 andthe initialization of bias ?
(B) are chosen to optimizelikelihood on the held-out development set.We optimized the hyperparameters and sparsityarray using gradient descent after each Gibbs sweep.We use a decreasing step size of a/(t+1000), wheret is the current iteration and a=10 for ?
and 1 for?
and the sparsity values.
To learn priors ?, weran our version of SAGE for 100 iterations of gra-dient ascent (fixed step size of 0.1).
See Paul andDredze (2012a) for examples of parameters (the topwords associated with various triples) learned bythis model on this corpus.4.2 Summary GenerationWe created twelve reference summaries by edit-ing together excerpts from the five PsychonautProject reports ((Psychonaut), 2009).
Each refer-ence is matched to drug-specific pairs and triples.For example, a paragraph describing the dif-ferences in effects of salvia between smokingand oral routes was matched to distributions for(SALVIA,EFFECTS), (SALVIA,SMOKING,EFFECTS),(SALVIA,ORAL,EFFECTS).
Descriptions of creat-ing tinctures and blotters for oral consumption werematched to (SALVIA,ORAL,CHEMISTRY).
We con-sider pairs in addition to triples because not all sum-maries correspond to particular routes or aspects.For each tuple-specific word distribution (a pair ora triple), we create a ?summary?
by extracting a setof five text snippets which minimize KL-divergenceto the target word distribution.
We consider all over-lapping text windows of widths {10,15,20} in thecorpus as candidate snippets.
Following Haghighiand Vanderwende (2009), we greedily add snippetsone by one with the lowest KL-divergence at eachstep until we have added five.We only considered candidate snippets within thesubforum for the particular drug, and snippets arebased on the preprocessed topic model input with nostop words.
Before presenting snippets to users, wethen map the snippets back to the raw text by takingall sentences which are at least partly spanned by thewindow of tokens.
Because each reference may bematched to more than one tuple, there may be morethan five snippets which correspond to a reference.1 2 3 4 5050100150200250CountHistogram of Annotator ScoresRandomBaselinef-LDA-1f-LDA-2Figure 4: The distribution of annotator scores (?4.3.1).The ?Random?
counts have been scaled to fit the samerange as the other systems, since fewer random snippetswere shown to annotators.4.3 Experimental ResultsRecall that the reports used as reference summarieswere themselves created by reading web forums.Our hypothesis is that f-LDA could be used as anexploratory tool to expedite the creation of these re-ports.
Thus in our evaluation we want to measurehow useful the extracted snippets would be in in-forming the writing of such reports.
We performedboth human and automatic evaluation on the sum-maries generated by f-LDA (variants 1 and 2) as wellas our baseline.
We also included randomly selectedsnippets as a control (five per reference).Example output is shown in Figure 3.4.3.1 Human Judgments of QualityThree annotators were presented snippets pooledfrom all four systems we are evaluating alongsidethe corresponding reference text.
Within each setcorresponding to a reference summary, the snippetswere shown in a random order.
Annotators wereasked to judge each snippet independently on a 5-point Likert scale as to how useful each snippetwould be in writing the reference text.The distribution of scores is shown in Figure 4 andsummarized in Table 2.
Annotators generally agreedon the relative quality of snippets: the average cor-relation of scores between each pair of annotatorswas 0.49.
Snippets produced by f-LDA were givenmore high scores and fewer low scores than the base-line, while the two f-LDA variants were rated com-parably.
The breakdown is more interesting whenwe compare scores for snippets that were matched175Rand.
Base.
f-LDA-1 f-LDA-2Annotator ScoresMean 1.67 2.55 2.79 2.81Pairs only n/a 2.58 2.79 2.72Triples only n/a 2.50 2.80 2.95ROUGE1-gram .112 .326 .355 .3272-gram .023 .072 .085 .084Table 2: Summary quality evaluation across four systems.to word distributions for pairs versus word distri-butions for triples.
The gap in scores between f-LDA and the baseline increases when we look at thescores for only triples: f-LDA beats the baseline bya margin of 0.45 for snippets matched to triples and0.21 for pairs.
This suggests that we produce bettertriples by modeling them jointly.
For triples, f-LDA-2 (which uses more data) beats f-LDA-1 (which usesonly tagged data), while the reverse is true for pairs.While some of the randomly selected controlsnippets happened to be useful, the scores for thesesnippets were much lower than those extractedthrough model-based systems.
This suggests thatexploring the forums in a targeted way (e.g.
throughour topic model approach) would be more efficientthan exploring the data in a non-targeted way (akinto the random approach).Finally, we asked two expert annotators (facultymembers in psychiatry and behavioral pharmacol-ogy, who have used drug forums in the past to studyemerging drugs) to rate the snippets correspondingto mephedrone/MDPV.
The best f-LDA system hadan average score of 2.57 compared to a baselinescore of 2.45 and random score of 1.63.4.3.2 Automatic Evaluation of RecallThe human judgments effectively measured aform of precision, as the quality of snippets werejudged by their correspondence to the reference text,without regard to how much of the reference textwas covered by all snippets.
We also used the au-tomatic evaluation metric ROUGE (Lin, 2004) as arough estimate of summary recall: this metric com-putes the percentage of n-grams in the reference textthat appeared in the generated summaries.We computed ROUGE for both 1-grams and 2-grams.
When computing n-gram counts, we appliedPorter?s stemmer to all tokens.
We excluded stopwords from 1-gram counts but included them in 2-gram counts where we care about longer phrases.2Results are shown in Table 2.
We find that f-LDA-1 has the highest score for both 1- and 2-grams, sug-gesting that it is extracting a more diverse set ofrelevant snippets.
When performing a paired t-testacross the 12 reference summaries, we find that f-LDA is better than the baseline with p-values 0.14and 0.10 for 1-gram and 2-gram recall, respectively.f-LDA?s recall advantage may come from the factthat it learns from a larger amount of data and itmay learn more diverse word distributions by di-rectly modeling triples.
f-LDA-1 had slightly betterrecall (under ROUGE), while f-LDA-2 was slightlybetter according to the human annotators.5 ConclusionWe have proposed exploratory tools for the analy-sis of online drug communities.
Such communi-ties are an emerging source of drug research, butmanually browsing through large corpora is imprac-tical and important information could be missed.We have demonstrated that topic models are capa-ble of modeling informative portions of text, and inparticular multi-dimensional topic models can tar-get desired structures such as the combination of as-pect and route of administration for each drug.
Wehave presented an extension to factorial LDA tai-lored to a particular application and data set whichwas demonstrated to induce desired properties.
Asa technical contribution, this study lays out practicalguidelines for customizing and incorporating priorknowledge into multi-dimensional text models.AcknowledgmentsWe are grateful to Dr. Margaret S. Chisolm andDr.
Ryan Vandrey from the Johns Hopkins Schoolof Medicine for providing the mephedrone/MDPVannotations, and Alex Lamb and Hieu Tran for as-sisting with the full annotations.
We also thank Dr.Matthew W. Johnson for additional advice, and theanonymous reviewers for helpful feedback and sug-gestions.
This research was partly supported by anNSF Graduate Research Fellowship.2In both cases, ROUGE scores were higher when stop wordswere included.
f-LDA beats the baseline by similar marginsregardless of whether we include stop words.176ReferencesD.
Blei, A. Ng, and M. Jordan.
2003.
Latent Dirichletallocation.
JMLR.A.
Chaney and D. Blei.
2012.
Visualizing topic models.In ICWSM.O.
Corazza, F. Schifano, M. Farre, P. Deluca, Z. Davey,C.
Drummond, M. Torrens, Z. Demetrovics, L. Di Fu-ria, L. Flesland, et al2011.
Designer drugs on theInternet: a phenomenon out-of-control?
The emer-gence of hallucinogenic drug Bromo-Dragonfly.
Cur-rent Clinical Pharmacology, 6(2):125?129.Ornella Corazza, Fabrizio Schifano, Pierluigi Simonato,Suzanne Fergus, Sulaf Assi, Jacqueline Stair, JohnCorkery, Giuseppina Trincas, Paolo Deluca, ZoeDavey, Ursula Blaszko, Zsolt Demetrovics, JacekMoskalewicz, Aurora Enea, Giuditta di Melchiorre,Barbara Mervo, Lucia di Furia, Magi Farre, Liv Fles-land, Manuela Pasinetti, Cinzia Pezzolesi, AgnieszkaPisarska, Harry Shapiro, Holger Siemann, ArvidSkutle, Aurora Enea, Giuditta di Melchiorre, EliasSferrazza, Marta Torrens, Peer van der Kreeft, DanielaZummo, and Norbert Scherbaum.
2012.
Phenomenonof new drugs on the Internet: the case of ketaminederivative methoxetamine.
Human Psychopharmacol-ogy: Clinical and Experimental, 27(2):145?149.Matthew Dunn, Raimondo Bruno, Lucinda Burns, andAmanda Roxburgh.
2011.
Effectiveness of and chal-lenges faced by surveillance systems.
Drug Testingand Analysis, 3(9):635?641.J.
Eisenstein, A. Ahmed, and E. P. Xing.
2011.
Sparseadditive generative models of text.
In ICML.Jacob Eisenstein, Duen Horng ?Polo?
Chau, Aniket Kit-tur, and Eric P. Xing.
2012.
Topicviz: Semanticnavigation of document collections.
In CHI Work-in-Progress Paper.EMCDDA.
2012.
2012 annual report on the state ofthe drugs problem in Europe.
European MonitoringCentre for Drugs and Drug Addiction, Lisbon.Cathal T. Gallagher, Sulaf Assi, Jacqueline L. Stair,Suzanne Fergus, Ornella Corazza, John M. Corkery,and Fabrizio Schifano.
2012.
5,6-methylenedioxy-2-aminoindane: from laboratory curiosity to ?legal high?.Human Psychopharmacology: Clinical and Experi-mental, 27(2):106?112.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, and MarkKantrowitz.
2000.
Multi-document summarizationby sentence extraction.
In Proceedings of the 2000NAACL-ANLP Workshop on Automatic summariza-tion, pages 40?48.Aria Haghighi and Lucy Vanderwende.
2009.
Exploringcontent models for multi-document summarization.
InNAACL ?09: Proceedings of Human Language Tech-nologies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 362?370.Simon L. Hill and Simon H. L. Thomas.
2011.
Clin-ical toxicology of newer recreational drugs.
ClinicalToxicology, 49(8):705?719.Marie Claire Van Hout and Tim Bingham.
2012.
Costlyturn on: Patterns of use and perceived consequences ofmephedrone based head shop products amongst Irishinjectors.
International Journal of Drug Policy.Jagadeesh Jagarlamudi, Hal Daume?
III, and RaghavendraUdupa.
2012.
Incorporating lexical priors into topicmodels.
In EACL.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop, pages 74?81, Barcelona, Spain, July.Q.
Mei, X. Ling, M. Wondra, H. Su, and C. Zhai.
2007.Topic sentiment mixture: modeling facets and opin-ions in weblogs.
In WWW.David Mimno.
2011.
Reconstructing Pompeian house-holds.
In UAI.Elizabeth M. Morgan, Chareen Snelson, and Patt Elison-Bowers.
2010.
Image and video disclosure of sub-stance use on social media websites.
Computers inHuman Behavior, 26(6):1405?1411.
Online Interac-tivity: Role of Technology in Behavior Change.Michael J. Paul and Mark Dredze.
2011.
You are whatyou Tweet: Analyzing Twitter for public health.
In 5thInternational AAAI Conference on Weblogs and SocialMedia (ICWSM).Michael J. Paul and Mark Dredze.
2012a.
Experiment-ing with drugs (and topic models): Multi-dimensionalexploration of recreational drug discussions.
In AAAI2012 Fall Symposium on Information Retrieval andKnowledge Discovery in Biomedical Text.Michael J. Paul and Mark Dredze.
2012b.
FactorialLDA: Sparse multi-dimensional text models.
In Neu-ral Information Processing Systems (NIPS).M.
Paul and R. Girju.
2010.
A two-dimensional topic-aspect model for discovering multi-faceted topics.
InAAAI.Psychonaut WebMapping Research Group (Psycho-naut).
2009.
Bromo-Dragonfly, MDPV, Spice,Mephodrone, and Salvia Divinorum reports.http://www.psychonautproject.eu/technical.php.Institute of Psychiatry, King?s College London.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled lda: a super-vised topic model for credit attribution in multi-labeledcorpora.
In EMNLP, pages 248?256.J.
Reyes, J. Negro?n, H. Colo?n, A. Padilla, M. Milla?n,T.
Matos, and R. Robles.
2012.
The emerging of177xylazine as a new drug of abuse and its health con-sequences among drug users in Puerto Rico.
Journalof Urban Health, pages 1?8.SAMHSA.
2012.
The DAWN report.http://www.samhsa.gov/data/2k12/DAWN105/SR105-synthetic-marijuana.pdf, December 4.Fabrizio Schifano, Paolo Deluca, Alex Baldacchino,Teuvo Peltoniemi, Norbert Scherbaum, Marta Tor-rens, Magi Farro?, Irene Flores, Mariangela Rossi,Dorte Eastwood, Claude Guionnet, Salman Rawaf,Lisa Agosti, Lucia Di Furia, Raffaella Brigada, AinoMajava, Holger Siemann, Mauro Leoni, AntonellaTomasin, Francesco Rovetto, and A. Hamid Ghodse.2006.
Drugs on the web: the Psychonaut 2002 EUproject.
Progress in Neuro-Psychopharmacology andBiological Psychiatry, 30(4):640 ?
646.Edmund Talley, David Newman, Bruce Herr II, HannaWallach, Gully Burns, Miriam Leenders, and AndrewMcCallum.
2011.
A database of National Institutesof Health (NIH) research using machine learned cate-gories and graphically clustered grant awards.
NatureMethods.Ivan Titov and Ryan McDonald.
2008.
Modeling on-line reviews with multi-grain topic models.
In Interna-tional World Wide Web Conference (WWW), Beijing.P.M.
Wax.
2002.
Just a click away: Recreational drugweb sites on the Internet.
Pediatrics, 109(6).178
