Proceedings of the Third Workshop on Statistical Machine Translation, pages 195?198,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Smorgasbord of Features for Automatic MT EvaluationJesu?s Gime?nez and Llu?
?s Ma`rquezTALP Research Center, LSI DepartmentUniversitat Polite`cnica de CatalunyaJordi Girona Salgado 1?3, E-08034, Barcelona{jgimenez,lluism}@lsi.upc.eduAbstractThis document describes the approach by theNLP Group at the Technical University of Cat-alonia (UPC-LSI), for the shared task on Au-tomatic Evaluation of Machine Translation atthe ACL 2008 Third SMT Workshop.1 IntroductionOur proposal is based on a rich set of individualmetrics operating at different linguistic levels: lex-ical (i.e., on word forms), shallow-syntactic (e.g., onword lemmas, part-of-speech tags, and base phrasechunks), syntactic (e.g., on dependency and con-stituency trees), shallow-semantic (e.g., on namedentities and semantic roles), and semantic (e.g., ondiscourse representations).
Although from differ-ent viewpoints, and based on different similarity as-sumptions, in all cases, translation quality is mea-sured by comparing automatic translations againsthuman references.
Extensive details on the met-ric set may be found in the IQMT technical manual(Gime?nez, 2007).Apart from individual metrics, we have alsoapplied a simple integration scheme based onuniformly-averaged linear metric combinations(Gime?nez and Ma`rquez, 2008a).2 What is new?The main novelty, with respect to the set of metricspresented last year (Gime?nez and Ma`rquez, 2007),is the incorporation of a novel family of metricsat the properly semantic level.
DR metrics ana-lyze similarities between automatic and referencetranslations by comparing their respective discourserepresentation structures (DRS), as provided by thethe C&C Tools (Clark and Curran, 2004).
DRS areessentially a variation of first-order predicate calcu-lus which can be seen as semantic trees.
We usethree different kinds of metrics:DR-STM Semantic Tree Matching, a la Liu andGildea (2005), but over DRS instead of overconstituency trees.DR-Or-?
Lexical overlapping over DRS.DR-Orp-?
Morphosyntactic overlapping on DRS.Further details on DR metrics can be found in(Gime?nez and Ma`rquez, 2008b).2.1 Improved Sentence Level BehaviorMetrics based on deep linguistic analysis rely onautomatic processors trained on out-domain data,which may be, thus, prone to error.
Indeed, we foundout that in many cases, metrics are unable to pro-duce a result due to the lack of linguistic analysis.For instance, in our experiments, for SR metrics, wefound that the semantic role labeler was unable toparse 14% of the sentences.
In order to improve therecall of these metrics, we have designed two simplevariants.
Given a linguistic metric x, we define:?
xb ?
by backing off to lexical overlapping,Ol, only when the linguistic processor is notable to produce a linguistic analysis.
Other-wise, x score is returned.
Lexical scores areconveniently scaled so that they are in a similarrange to scores of x.
Specifically, we multiply195them by the average x score attained over allother test cases for which the parser succeeded.?
xi ?
by linearly interpolating x and Ol scoresfor all test cases, via the arithmetic mean.In both cases, system scores are calculated by av-eraging over all sentence scores.
Currently, thesevariants are applied only to SR and DR metrics.2.2 Uniform Linear Metric CombinationsWe have simulated a non-parametric combinationscheme based on human acceptability by workingon uniformly averaged linear combinations (ULC)of metrics (Gime?nez and Ma`rquez, 2008a).
Our ap-proach is similar to that of Liu and Gildea (2007)except that in our case the contribution of each met-ric to the overall score is not adjusted.Optimal metric sets are determined by maximiz-ing the correlation with human assessments, eitherat the document or sentence level.
However, becauseexploring all possible combinations was not viable,we have used a simple algorithm which performs anapproximate search.
First, metrics are ranked ac-cording to their individual quality.
Then, followingthat order, metrics are added to the optimal set onlyif in doing so the global quality increases.3 Experimental WorkWe use all into-English test beds from the 2006and 2007 editions of the SMT workshop (Koehnand Monz, 2006; Callison-Burch et al, 2007).These include the translation of three differ-ent language-pairs: German-to-English (de-en),Spanish-to-English (es-en), and French-to-English(fr-en), over two different scenarios: in-domain (Eu-ropean Parliament Proceedings) and out-of-domain(News Commentary Corpus)1.
In all cases, a singlereference translation is available.
In addition, hu-man assessments on adequacy and fluency are avail-able for a subset of systems and sentences.
Eachsentence has been evaluated at least by two differentjudges.
A brief numerical description of these testbeds is available in Table 1.1We have not used the out-of-domain Czech-to-English testbed from the 2007 shared task because it includes only 4 sys-tems, and only 3 of them count on human assessments.WMT 2006in-domain out-of-domain2,000 cases 1,064 cases#snt #sys #snt #sysde-en 2,281 10/12 1,444 10/12es-en 1,852 11/15 1,008 11/15fr-en 2,268 11/14 1,281 11/14WMT 2007in-domain out-of-domain2,000 cases 2,007 cases#snt #sys #snt #sysde-en 956 7/8 947 5/6es-en 812 8/10 675 7/9fr-en 624 7/8 741 7/7Table 1: Test bed description.
?#snt?
columns show thenumber of sentences assessed (considering all systems).?#sys?
columns shows the number of systems countingon human assessments with respect to the total numberof systems which participated in each task.Metrics are evaluated in terms of human accept-ability, i.e., according to their ability to capturethe degree of acceptability to humans of automatictranslations.
We measure human acceptability bycomputing Pearson correlation coefficients betweenautomatic metric scores and human assessments oftranslation quality both at document and sentencelevel.
We use the sum of adequacy and fluency tosimulate a global assessment of quality.
Assess-ments from different judges over the same test caseare averaged into a single score.3.1 Individual PerformanceIn first place, we study the behavior of individualmetrics.
Table 2 shows meta-evaluation results, overinto-English WMT 2007 test beds, in-domain andout-of-domain, both at the system and sentence lev-els, for a set of selected representatives from severallinguistic levels.At the system level (columns 1-6), corroboratingprevious findings by Gime?nez and Ma`rquez (2007),highest levels of correlation are attained by met-rics based on deep linguistic analysis (either syn-tactic or semantic).
In particular, two kinds of met-rics, respectively based on head-word chain match-ing over grammatical categories and relations (?DP-196System Level Sentence Levelde-en es-en fr-en de-en es-en fr-enLevel Metric in out in out in out in out in out in out1-TER 0.64 0.41 0.83 0.58 0.72 0.47 0.43 0.29 0.23 0.23 0.29 0.20BLEU 0.87 0.76 0.88 0.70 0.74 0.54 0.46 0.27 0.33 0.20 0.20 0.12Lexical GTM (e = 2) 0.82 0.69 0.93 0.71 0.76 0.60 0.56 0.36 0.43 0.33 0.27 0.18ROUGEW 0.87 0.91 0.96 0.78 0.85 0.83 0.58 0.40 0.43 0.35 0.30 0.31METEORwn 0.83 0.92 0.96 0.74 0.91 0.86 0.53 0.41 0.35 0.28 0.33 0.32Ol 0.79 0.75 0.91 0.55 0.81 0.66 0.48 0.33 0.35 0.30 0.30 0.21CP-Oc-?
0.84 0.88 0.95 0.62 0.84 0.76 0.49 0.37 0.38 0.33 0.32 0.25DP-HWCw-4 0.85 0.93 0.96 0.68 0.84 0.80 0.31 0.26 0.33 0.07 0.10 0.14Syntactic DP-HWCc-4 0.91 0.98 0.96 0.90 0.98 0.95 0.30 0.25 0.23 0.06 0.13 0.12DP-HWCr-4 0.89 0.97 0.97 0.92 0.97 0.95 0.33 0.28 0.29 0.08 0.16 0.16DP-Or-?
0.88 0.96 0.97 0.84 0.89 0.89 0.57 0.41 0.44 0.36 0.33 0.30CP-STM-4 0.88 0.97 0.97 0.79 0.89 0.89 0.49 0.39 0.40 0.37 0.32 0.26NE-Me-?
-0.13 0.79 0.95 0.68 0.87 0.92 -0.03 0.07 0.07 -0.05 0.05 0.06NE-Oe-??
-0.18 0.78 0.95 0.58 0.81 0.71 0.32 0.26 0.37 0.26 0.31 0.20SR-Or-?
0.55 0.96 0.94 0.69 0.89 0.85 0.26 0.14 0.30 0.11 0.08 0.19SR-Or-?b 0.24 0.98 0.94 0.68 0.92 0.87 0.33 0.21 0.35 0.15 0.18 0.24Shallow SR-Or-?i 0.51 0.95 0.93 0.67 0.88 0.83 0.37 0.26 0.38 0.19 0.24 0.27Semantic SR-Mr-?
0.38 0.95 0.96 0.83 0.79 0.75 0.32 0.18 0.28 0.18 0.08 0.14SR-Mr-?b 0.14 0.98 0.97 0.82 0.84 0.79 0.37 0.23 0.32 0.21 0.15 0.17SR-Mr-?i 0.38 0.94 0.96 0.80 0.79 0.74 0.40 0.27 0.36 0.24 0.20 0.20SR-Or 0.73 0.99 0.94 0.66 0.97 0.93 0.12 0.09 0.16 0.07 -0.04 0.17SR-Ori 0.66 0.99 0.94 0.64 0.95 0.89 0.29 0.25 0.29 0.19 0.15 0.28DR-Or-?
0.87 0.89 0.96 0.71 0.78 0.75 0.50 0.40 0.37 0.35 0.27 0.28DR-Or-?b 0.91 0.93 0.97 0.72 0.83 0.80 0.52 0.41 0.38 0.34 0.28 0.27DR-Or-?i 0.87 0.87 0.96 0.68 0.79 0.74 0.53 0.42 0.39 0.35 0.30 0.28DR-Orp-?
0.92 0.98 0.99 0.81 0.91 0.89 0.42 0.32 0.29 0.25 0.21 0.30Semantic DR-Orp-?b 0.93 0.98 0.99 0.81 0.94 0.91 0.45 0.34 0.32 0.22 0.22 0.30DR-Orp-?i 0.91 0.95 0.98 0.75 0.89 0.85 0.50 0.38 0.36 0.28 0.27 0.33DR-STM-4 0.89 0.95 0.98 0.79 0.85 0.87 0.28 0.29 0.25 0.21 0.15 0.22DR-STM-4b 0.92 0.97 0.98 0.80 0.90 0.91 0.36 0.31 0.29 0.21 0.19 0.23DR-STM-4i 0.91 0.94 0.97 0.74 0.87 0.86 0.43 0.35 0.34 0.26 0.24 0.27Optimal07 0.93 1.00 0.99 0.92 0.98 0.95 0.60 0.46 0.47 0.42 0.36 0.39Optimal06 0.01 0.95 0.96 0.75 0.97 0.87 0.50 0.41 0.40 0.20 0.27 0.30ULC Optimal?07 0.93 0.98 0.99 0.81 0.94 0.91 0.58 0.45 0.46 0.39 0.35 0.34Optimal?06 0.34 0.96 0.98 0.82 0.92 0.93 0.54 0.41 0.42 0.32 0.32 0.34Optimalh 0.87 0.98 0.97 0.79 0.91 0.89 0.56 0.44 0.43 0.32 0.31 0.35Table 2: Meta-evaluation results based on human acceptability for the WMT 2007 into-English translation tasksHWCc-4?, ?DP-HWCr-4?
), and morphosyntactic over-lapping over discourse representations (?DR-Orp-??
),are consistently among the top-scoring in all testbeds.
At the lexical level, variants of ROUGE andMETEOR attain the best results, close to the perfor-mance of syntactic and semantic features.
It can alsobe observed that metrics based on semantic rolesand named entities have serious troubles with theGerman-to-English in-domain test bed (column 1).At the sentence level, the highest levels of corre-lation are attained by metrics based on lexical simi-larity alone, only rivaled by lexical overlapping overdependency relations (?DP-Or-??)
and discourse rep-resentations (?DR-Or-??).
We speculate the underly-ing cause might be on the side of parsing errors.
Inthat respect, lexical back-off strategies report in allcases a significant improvement.It can also be observed that, over these test beds,metrics based on named entities are completely use-less at the sentence level, at least in isolation.
Thereason is that they capture a very partial aspect ofquality which may be not relevant in many cases.This has been verified by computing the ?NE-Oe-???
variant which considers also lexical overlappingover regular items.
Observe how this metric attainsa much higher correlation with human assessments.1973.2 Metric CombinationsWe also study the behavior of metric combinationsunder the ULC scheme.
Last 5 rows in Table 2shows meta-evaluation results following 3 differentoptimization strategies:Optimal: the metric set is optimized for each testbed (language-pair and domain) individually.Optimal?
: the metric set is optimized over theunion of all test beds.Optimalh: the metric set is heuristically definedso as to include several of the top-scoringrepresentatives from each level: Optimalh ={ ROUGEW , METEORwnsyn, DP-HWCc-4, DP-HWCr-4, DP-Or-?, CP-STM-4, SR-Mr-?i, SR-Or-?i, SR-Ori, DR-Or-?i, DR-Orp-?b }.We present results optimizing over the 2006 and2007 data sets.
Let us provide, as an illustration,Optimal?07 sets.
For instance, at the system level,no combination improved the isolated global perfor-mance of the ?DR-Orp-?b?
metric (R=0.94).
In con-trast, at the sentence level, the optimal metric setcontains several metrics from each linguistic level:Optimal?07 = { ROUGEW , DP-Or-?, CP-STM-4, SR-Or-?i, SR-Mr-?i, DR-Or-?i }.
A similar pattern isobserved for all test beds, both at the system andsentence levels, although with different metrics.The behavior of optimal metric sets is in generalquite stable, except for the German-to-English in-domain test bed which presents an anomalous be-havior when meta-evaluating WMT 2006 optimalmetric sets at the system level.
The reason for thisanomaly is in the ?NE-Me-??
metric, which is in-cluded in the 2006 optimal set: { ?NE-Me-?
?, ?SR-Ori?
}.
?NE-Me-??
is based on lexical matching overnamed entities, and attains in the 2006 German-to-English in-domain test bed a very high correlationof 0.95 with human assessments.
This partial aspectof quality seems to be of marginal importance in the2007 test bed.
We have verified this hypothesis bycomputing optimal metrics sets without consideringNE variants.
Correlation increases to more reason-able values (e.g., from 0.01 to 0.66 and from 0.34to 0.91.
This result suggests that more robust metriccombination schemes should be pursued.For future work, we plan to apply parametriccombination schemes based on human likeness clas-sifiers, as suggested by Kulesza and Shieber (2004).We must also further investigate the impact of pars-ing errors on the performance of linguistic metrics.AcknowledgmentsThis research has been funded by the Spanish Min-istry of Education and Science (OpenMT, TIN2006-15307-C03-02).
Our group is recognized by DURSIas a Quality Research Group (2005 SGR-00130).ReferencesChris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(Meta-)Evaluation of Machine Translation.
In Proceedings ofthe ACL Second SMT Workshop, pages 136?158.Stephen Clark and James R. Curran.
2004.
Parsing theWSJ using CCG and Log-Linear Models.
In Proceed-ings of the 42nd Annual Meeting of the Association forComputational Linguistics (ACL), pages 104?111.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2007.
LinguisticFeatures for Automatic Evaluation of HeterogeneousMT Systems.
In Proceedings of the ACL Second SMTWorkshop, pages 256?264.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008a.
Hetero-geneous Automatic MT Evaluation Through Non-Parametric Metric Combinations.
In Proceedings ofIJCNLP, pages 319?326.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008b.
On theRobustness of Linguistic Features for Automatic MTEvaluation.
To be published.Jesu?s Gime?nez.
2007.
IQMT v 2.1.
Tech-nical Manual (LSI-07-29-R).
Technical re-port, TALP Research Center.
LSI Department.http://www.lsi.upc.edu/ nlp/IQMT/IQMT.v2.1.pdf.Philipp Koehn and Christof Monz.
2006.
Manual andAutomatic Evaluation of Machine Translation betweenEuropean Languages.
In Proceedings of the Workshopon Statistical Machine Translation, pages 102?121.Alex Kulesza and Stuart M. Shieber.
2004.
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the 10th TMI, pages 75?84.Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
In Proceed-ings of ACL Workshop on Intrinsic and Extrinsic Eval-uation Measures for MT and/or Summarization.Ding Liu and Daniel Gildea.
2007.
Source-LanguageFeatures and Maximum Correlation Training for Ma-chine Translation Evaluation.
In Proceedings ofNAACL, pages 41?48.198
