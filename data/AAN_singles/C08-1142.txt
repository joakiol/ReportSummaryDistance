Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1129?1136Manchester, August 2008Multi-Criteria-based Strategy to Stop Active Learning for Data An-notationJingbo Zhu   Huizhen WangNatural Language Processing LaboratoryNortheastern UniversityShenyang, Liaoning, P.R.China 110004zhujingbo@mail.neu.edu.cnwanghuizhen@mail.neu.edu.cnEduard HovyUniversity of Southern CaliforniaInformation Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695hovy@isi.eduAbstractIn this paper, we address the issue of de-ciding when to stop active learning forbuilding a labeled training corpus.
Firstly,this paper presents a new stopping crite-rion, classification-change, which con-siders the potential ability of each unla-beled example on changing decisionboundaries.
Secondly, a multi-criteria-based combination strategy is proposedto solve the problem of predefining anappropriate threshold for each confi-dence-based stopping criterion, such asmax-confidence, min-error, and overall-uncertainty.
Finally, we examine the ef-fectiveness of these stopping criteria onuncertainty sampling and heterogeneousuncertainty sampling for active learning.Experimental results show that thesestopping criteria work well on evaluationdata sets, and the combination strategiesoutperform individual criteria.1 IntroductionCreating a large labeled training corpus is veryexpensive and time-consuming in some real-world applications.
For example, it is a crucialissue for automated word sense disambiguationtask, because validations of sense definitions andsense-tagged data annotation have to be done byhuman experts, e.g.
OntoNotes project (Hovy etal., 2006).Active learning aims to minimize the amountof human labeling effort by automatically select-ing the most informative unlabeled example forhuman annotation.
In recent years active learning?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.has been widely studied in natural languageprocessing (NLP) applications, such as wordsense disambiguation (WSD) (Chen et al, 2006;Zhu and Hovy, 2007), text classification (TC)(Lewis and Gale, 1994; McCallum and Nigam,1998a), named entity recognition (Shen et al,2004), chunking (Ngai and Yarowsky, 2000),and statistical parsing (Tang et al, 2002).However, deciding when to stop active learn-ing is still an unsolved problem and seldom men-tioned issue in previous studies.
Actually it is avery important practical issue in real-world ap-plications, because it obviously makes no senseto continue the active learning procedure untilthe whole unlabeled corpus has been labeled.The active learning process can be ended whenthe current classifier reaches the maximum effec-tiveness.
In principle, how to learn a stoppingcriterion is a problem of estimation of classifier(i.e.
learner) effectiveness during active learning(Lewis and Gale, 1994).In this paper, we address the issue of a stop-ping criterion for pool-based active learning withuncertainty sampling (Lewis and Gale, 1994),and propose a multi-criteria-based approach todetermining when to stop active learning process.Firstly, this paper makes a comprehensive analy-sis on some confidence-based stopping criteria(Zhu and Hovy, 2007), including max-confidence, min-error and overall-uncertainty,then proposes a new stopping criterion, classifi-cation-change, which considers the potentialability of each unlabeled example on changingdecision boundaries.
Secondly, a combinationstrategy is proposed to solve the problem of pre-defining an appropriate threshold for each confi-dence-based stopping criterion in a specific task.In uncertainty sampling scheme, the most un-certain unlabeled example is considered as themost informative case selected by active learnerat each learning cycle.
However, an uncertainexample for one classifier may be not an uncer-1129tain example for other classifiers.
When usingactive learning for real-world applications suchas WSD, it is possible that a classifier of one typeselects samples for training a classifier of anothertype, called the heterogeneous approach (Lewisand Catlett, 1994).
For example, the final trainedclassifier for WSD is often different from theclassifier used in active learning for constructingthe training corpus.To date, no one has studied the stopping crite-rion issue for the heterogeneous approach.
In thispaper, we examine the effectiveness of eachstopping criterion on both traditional uncertaintysampling and heterogeneous uncertainty sam-pling for active learning.
Experimental results ofactive learning for WSD and TC tasks show thatthese proposed stopping criteria work well onevaluation data sets, and the combination strate-gies outperform individual criteria.2 Active Learning ProcessIn this paper, we are interested in uncertaintysampling for pool-based active learning (Lewisand Gale, 1994), in which an unlabeled examplex with maximum uncertainty is selected to aug-ment the training data at each learning cycle.
Themaximum uncertainty implies that the currentclassifier has the least confidence on its classifi-cation of this unlabeled example.Actually active learning is a two-stage processin which a small number of labeled samples anda large number of unlabeled examples are firstcollected in the initialization stage, and a closed-loop stage of query and retraining is adopted.Procedure: Active Learning ProcessInput: initial small training set L, and pool of unla-beled data set UUse L to train the initial classifier CRepeat1.
Use the current classifier C to label all unla-beled examples in U2.
Use uncertainty sampling technique to select mmost informative unlabeled examples, and askoracle H for labeling3.
Augment L with these m new examples, andremove them from U4.
Use L to retrain the current classifier CUntil the predefined stopping criterion SC is met.Figure 1.
Active learning with uncertainty sam-pling technique3 Stopping Criteria for Active LearningIn this section, we mainly address the problem ofgeneral stopping criteria for active leanring, andstudy how to define a reasonable and appropriatestopping criterion SC shown in Fig.
1.3.1 Effectiveness Estimation and Confi-dence EstimationTo examine whether the classifier has reachedthe maximum effectiveness during active learn-ing procedure, it seems an appealing solutionwhen repeated learning cycles show no signifi-cant performance improvement.
However, this isoften not feasible.
To investigate the impact ofperformance change on defining a stopping crite-rion for active learning, we first give an exampleof active learning for WSD shown in Fig.
2.0.50.550.60.650.70.750.80.850.90.9510  20  40  60  80  100  120  140  160  180  200  220  240  260  280  300AccuracyNumber of Learned ExamplesActive Learning for WSD taskinterestFigure 2.
An example of active learning for WSDon word ?interest?.Fig.
2 shows that the accuracy performancegenerally increases, but apparently degrades atiterations 30, 90 and 190, and does not changeanymore during iterations 220-260 in the activelearning process.
Actually the first time of thehighest performance of 91.5% is achieved at 900which is not shown in Fig.
2.
Although the accu-racy performance curve shows an increasingtrend, it is not monotonically increasing.
It is noteasy to automatically determine the point of nosignificant performance improvement on thevalidation set, because points such as 30 or 90would mislead a final judgment.Besides, there is a problem of performance es-timation of the current classifier during activelearning process, because a separate validationset should be prepared in advance, a procedurethat causes additional (high) cost since it is oftendone manually.
Besides, how many samples arerequired for the pregiven separate validation setis an open question.
Too few samples may not beadequate for a reasonable estimate and may re-sult in an incorrect result.
Too many sampleswould increase the building cost.To define a stopping criterion for active learn-ing, Zhu and hovy (2007) considered the estima-tion of the classifier?s effectiveness as the second1130task of confidence estimation of the classifier onits classification of all remaining unlabeled data.In the following section, we first introduce twoconfidence-based criteria, max-confidence andmin-error, proposed by Zhu and Hovy (2007).3.2 Max-ConfidenceIn uncertainty sampling scheme, if the uncer-tainty value of the most informative unlabeledexample is sufficiently small, we can assume thatthe current classifier has sufficient confidence onits classification of the remaining unlabeled data.So the active learning process can be ended.Based on such assumption, Zhu and Hovy (2007)proposed max-confidence criterion based on theuncertainty estimation of the most informativeunlabeled example.
Its strategy is to considerwhether the uncertainty value of the most infor-mative unlabeled example is less than a verysmall predefined threshold.3.3 Min-ErrorAs shown in Fig.
1, in uncertainty samplingscheme, the current classifier has the least confi-dence on its classification of these top-m selectedunlabeled examples.
If the current classifier cancorrectly classify these most informative exam-ples, we can assume that the current classifierhave sufficient confidence on its classification ofthe remaining unlabeled data.
Based on such as-sumption, Zhu and Hovy (2007) proposed min-error criterion based on feedback from the oracle.Its strategy is to consider whether the currentclassifier can correctly predict the labels on theseselected unlabeled examples, or the accuracyperformance of the current classifier on thesemost informative examples is larger than a prede-fined threshold.3.4 Overall-UncertaintyThe motivation behind the overall-uncertaintymethod is similar to that of the max-confidencemethod.
However, the max-confidence methodonly considers the most informative example ateach learning cycle.
The overall-uncertaintymethod considers the overall uncertainty on allunlabeled examples.
If the overall uncertainty ofall unlabeled examples becomes very small, wecan assume that the current classifier has suffi-cient confidence on its classification of the re-maining unlabeled data.
Based on such assump-tion, we propose overall-uncertainty methodwhich is to consider whether the average uncer-tainty value of all remaining unlabeled examplesis less than a very small predefined threshold.3.5 Classification-ChangeThere is another problem of estimating classifierperformance during active learning process.Cross-validation on the training set is almost im-practical during the active learning procedure,because the alternative of requiring a held-outvalidation set for active learning is counterpro-ductive.
Hence we should look for a self-contained method.Actually the motivation behind uncertaintysampling is to find some unlabeled examplesnear decision boundaries, and use them to clarifythe position of decision boundaries.
The currentclassifier considers such unlabeled examples neardecision boundaries as the most informative ex-amples in uncertainty sampling scheme for activelearning.
In other words, we assume that anunlabeled example with maximum uncertaintyhas the highest chance to change the decisionboundaries.Based on the above analysis, we think the ac-tive learning process can stop if there is no unla-beled example that can potentially change thedecision boundaries.
However, in practice, it isalmost impossible to exactly recognize whichunlabeled example can truly change the decisionboundaries in the next learning cycle, becausethe true label of each unlabeled example is un-known.To solve this problem, we make an assump-tion that labeling an unlabeled example may shiftthe decision boundaries if this example was pre-viously ?outside?
and is now ?inside?.
In otherwords, if an unlabeled example is automaticallyassigned to two different labels during two recentlearning cycles 2 , we think that the labeling ofthis unlabeled example has a good chance tochange the decision boundaries.Based on such assumption, we propose a newapproach based on classification change of eachunlabeled example during two recent consecutivelearning cycles (?previous?
and ?current?
), calledthe classification-change method.
Its strategy isto stop the active learning process by consideringwhether no classification change happens to theremaining unlabeled examples during two recentconsecutive learning cycles.
If true, we assumethat the current classifier has sufficient confi-dence on its classification of the remaining unla-2 For example, an unlabeled example x was classified intoclass A at ith iteration, and class B at i+1th iteration.1131beled data, because all unlabeled examples neardecision boundaries have been exhausted, and nofurther labeling will affect active learner.4 Combination StrategyAs for the above three confidence-based stoppingcriteria such as max-confidence, min-error andoverall-uncertainty, how to automatically deter-mine an appropriate threshold in a specific task isa crucial problem.
We think that different appro-priate thresholds are needed for various activelearning applications.To solve this problem, in this section we pro-pose a general combination strategy by consider-ing the best of both classification-change and aconfidence-based criterion, in which the prede-fined threshold of the confidence-based stoppingcriterion can be automatically updated duringactive learning.The motivation behind the general combina-tion strategy is to check whether the active learn-ing becomes stable (i.e.
check whether the classi-fication-change method is met) when the currentconfidence-based stopping criterion is satisfied.If not, we think there are some remaining unla-beled examples that can potentially shift the de-cision boundaries, even if they are considered ascertain cases from the current classifier?s view-points.
In this case, the threshold of the currentconfidence-based stopping criterion should beautomatically revised to keep continuing the ac-tive learning process.
The general combinationstrategy can be summarized as follows.Procedure: General combination strategyGiven:z stopping criterion 1: max-confidence or min-error or overall-uncertaintyz Stopping criterion 2: classification-changez The predefined threshold for stopping criterion 1is initially set to ?Steps(during active learning process):1.
First check whether stopping criterion 1 is satis-fied.
If yes, go to 2;2.
Then check whether stopping criterion 2 is satis-fied.
If yes, goto 4), otherwise goto 3;3.
Automatically update the current threshold to bea new smaller value for max-confidence andoverall-uncertainty, or to be a new larger valuefor min-error, and then goto 1.4.
Stop active learning process.Figure 3.
General combination strategy?
Strategy 1: This strategy combines the max-confidence and classification-change meth-ods simultaneously.?
Strategy 2: This strategy combines the min-error and classification-change methods si-multaneously.?
Strategy 3: This strategy combines the over-all-uncertainty and classification-changemethods simultaneously.5 Evaluation5.1 Experimental SettingsIn the following sections, we evaluate theeffectiveness of seven stopping criteria for activelearning for WSD and TC tasks, including max-confidence (MC), min-error (ME), overall-uncertainty (OU), classification-change (CC),strategy 1 (CC-MC), strategy 2 (CC-ME), andstrategy 3 (CC-OU).
Following previous studies(Zhu and Hovy, 2007), the predefined thresh-olds3 used for MC, ME and OU are set to 0.01,0.9 and 0.01, respectively.To evaluate the effectiveness of each stoppingcriterion, we first construct two types of baselinemethods called ?All?
and ?First?
methods.
?All?method is defined as when all unlabeled exam-ples in the pool are learned.
?First?
method isdefined as when the current classifier reaches thesame performance of the ?All?
method at thefirst time during the active learning process.A better stopping criterion can not onlyachieve almost the same performance given bythe ?All?
baseline method (i.e.
accuracyperformance), but also learn almost the samenumber of unlabeled examples by the ?First?baseline method (i.e.
percentage performance).In uncertainty sampling scheme, the well-known entropy-based uncertainty measurement(Chen et al, 2006; Schein and Ungar, 2007) isused in our active learning study as follows:( ) ( | ) log ( | )y YUM x P y x P y x?= ??
(1)where P(y|x) is the a posteriori probability.
Wedenote the output class y?Y={y1, y2, ?, yk}.
UMis the uncertainty measurement function based onthe entropy estimation of the classifier?sposterior distribution.We utilize maximum entropy (MaxEnt) model(Berger et al, 1996) to design the basic classifierused in active learning for WSD and TC tasks.The advantage of the MaxEnt model is the abilityto freely incorporate features from diversesources into a single, well-grounded statistical3 In the following experiments, these thresholds are alsoused as initial values of ?
for individual criteria in the gen-eral combination strategy shown in Fig.
3.1132model.
A publicly available MaxEnt toolkit4 wasused in our experiments.
To build the MaxEnt-based classifier for WSD, three knowledgesources are used to capture contextual informa-tion: unordered single words in topical context,POS of neighboring words with position infor-mation, and local collocations, which are thesame as the knowledge sources used in (Lee andNg, 2002).
In the design of text classifier, themaximum entropy model is also utilized, and nofeature selection technique is used.In the following active learning comparisonexperiments, the algorithm starts with arandomly chosen initial training set of 10 labeledexamples, and makes 10 queries after eachlearning iteration.
A 10 by 10-fold cross-validation was performed.
All results reportedare the average of 10 trials in each activelearning process.
In the following comparisonexperiments, the performance reported onOntonotes data set is the macro-average on tennouns, and the performance on TWA data set isthe macro-average on six words.5.2 Data SetsSix publicly available natural data sets have beenused in the following active learning comparisonexperiments.
Three data sets are used for TCtasks: WebKB, Comp2a and Comp2b.
The otherthree data sets are used for WSD tasks:OntoNotes, Interest and TWA.The WebKB dataset was widely used in TCresearch.
Following previous studies (McCallumand Nigam, 1998b), we use the four most popu-lous categories: student, faculty, course and pro-ject, altogether containing 4199 web pages.
Inthe preprocessing step, we only remove thosewords that occur merely once without usingstemming.
The resulting vocabulary has 23803words.The Comp2a data set consists of comp.os.ms-windows.misc and comp.sys.ibm.pc.hardwaresubset of NewsGroups.
The Comp2b data setconsists of comp.graphics and comp.windows.xcategories from NewsGroups.
Both two data setshave been previously used in active learning forTC (Roy and McCallum, 2001; Schein and Un-gar, 2007).The OntoNotes project (Hovy et al, 2006)uses the WSJ part of the Penn Treebank.
Thesenses of noun words occurring in OntoNotes arelinked to the Omega ontology.
Ontonotes has4See  http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htmlbeen used previously in active learning for WSDtasks (Zhu and Hovy, 2007).
In the followingcomparison experiments, we focus on 10 mostfrequent nouns 5  previously used in (Zhu andHovy, 2007): rate, president, people, part, point,director, revenue, bill, future, and order.The Interest data set developed by Bruce andWiebe (1994) has been previously used for WSD(Ng and Lee, 1996).
This data set consists of2369 sentences of the noun ?interest?
with itscorrect sense manually labeled.
The noun?interest?
has six different senses in this data set.TWA developed by Mihalcea and Yang on 2003,is sense tagged data for six words with two-wayambiguities, previously used in WSD research.These six words are bass, crane, motion, palm,plant and tank.
All instances were drawn fromthe British National Corpus.5.3 Stopping Criteria for Uncertainty Sam-plingIn order to evaluate the effectiveness of our stop-ping criteria, we first apply them to uncertaintysampling for active learning for WSD and TCtasks.
Table 1 shows that ?First?
method gener-ally achieves higher performance than that of the?All?
method.
We can see from the ?Average?row that stopping criteria MC, ME, CC-MC, CC-ME and CC-OU achieve close average accuracyperformance to the ?All?
method whereas OUand CC achieve lower average accuracyperformance.
OU method achieves the lowestaverage accuracy performance.
CC-ME achievesthe highest average accuracy of 89.6%, followedby CC-MC.Compared to the ?First?
method, CC-OUachieves the best average percentageperformance of 37.03% (i.e.
the closest one tothe ?First?
method), followed by ME method.
Onsix evaluation data sets, Table 1 shows that CC-ME method achieves 4 out of 6 highest accuracyperformances, followed by CC-MC and MCmethods.
And CC-ME method also achieves 3out of 6 best percentage performance, followedby CC, CC-OU and ME methods.Among these four individual stopping criteria,ME outperforms MC, OU and CC.
However, MEmethod can only be applied to batch-basedselection because ME criterion is based on thefeedback from Oracle.
Too few informativecandidates may not be adequate for obtaining areasonable feedback for ME criterion.5 See http://www.nlplab.com/ontonotes-10-nouns.rar1133Data set All First MC ME OU CC CC-MC CC-ME CC-OU0.910 0.911 0.910 0.910 0.837 0.912 0.912 0.913 0.912 WebKB100% 31.50% 27.11% 29.11% 8.42% 31.53% 32.37% 33.02% 31.53%0.880 0.884 0.877 0.879 0.868 0.876 0.879 0.880 0.876 Comp2a100% 35.12% 31.35% 31.28% 23.29% 27.35% 32.36% 36.80% 27.35%0.900 0.901 0.887 0.888 0.880 0.879 0.891 0.893 0.882 Comp2b100% 41.66% 37.52% 36.76% 28.36% 30.80% 37.95% 40.03% 31.81%0.939 0.942 0.929 0.934 0.928 0.936 0.940 0.939 0.939 Ontonotes100% 22.81% 30.19% 22.14% 21.81% 18.96% 34.77% 25.60% 24.75%0.908 0.910 0.910 0.906 0.906 0.901 0.910 0.906 0.906 Interest100% 29.83% 37.54% 28.25% 28.51% 25.55% 37.54% 28.67% 28.62%0.846 0.858 0.843 0.844 0.837 0.820 0.841 0.845 0.838 TWA100% 59.67% 80.34% 72.71% 70.47% 61.54% 86.99% 80.15% 78.12%0.897 0.901 0.892 0.893 0.876 0.887 0.895 0.896 0.892 Average100% 37.43% 40.67% 36.71% 30.14% 32.62% 43.66% 40.71% 37.03%Table 1.
Effectiveness of seven stopping criteria for uncertainty sampling for active learning.
For eachdata set, Table 1 shows the accuracy of the classifier and percentage of learned instances over allunlabeled data when each stopping criterion is met.
The boldface numbers indicate the best corre-sponding performances.Data set All MC ME OU CC CC-MC CC-ME CC-OU0.858 0.808 0.818 0.601 0.820 0.820 0.824 0.820 WebKB100% 27.11% 29.11% 8.42% 31.53% 32.37% 33.02% 31.53%0.894 0.838 0.839 0.825 0.837 0.838 0.846 0.837 Comp2a100% 31.35% 31.28% 23.29% 27.35% 32.36% 36.80% 27.35%0.922 0.884 0.882 0.878 0.874 0.885 0.883 0.879 Comp2b100% 37.52% 36.76% 28.36% 30.80% 37.95% 40.03% 31.81%0.925 0.923 0.924 0.921 0.921 0.932 0.927 0.929 Ontonotes100% 30.19% 22.14% 21.81% 18.96% 34.77% 25.60% 24.75%0.899 0.906 0.890 0.890 0.885 0.906 0.891 0.890 Interest100% 37.54% 28.25% 28.51% 25.55% 37.54% 28.67% 28.62%0.812 0.784 0.793 0.765 0.775 0.799 0.810 0.794 TWA100% 80.34% 72.71% 70.47% 61.54% 86.99% 80.15% 78.12%0.885 0.857 0.857 0.813 0.852 0.863 0.863 0.858 Average100% 40.67% 36.71% 30.14% 32.62% 43.66% 40.71% 37.03%Table 2.
Effectiveness of seven stopping criteria for heterogeneous uncertainty sampling for activelearning.
Table 2 shows the accuracy of the classifier and percentage of learned instances over allunlabeled data when each stopping criterion is met.
The boldface numbers indicate the best corre-sponding performances.Interestingly, our proposed CC methodacheves the best macro-average percentageperformance on the TWA data set, however,other criteria work poorly, compared to the?First?
method.
Actually the sense distribution ofeach noun in TWA set is very skewed.
FromWSD experimental results on TWA, we foundthat only few learned instances can train theMaxEnt-based classifier with the highestaccuracy performance.In Table 1, the boldface numbers indicate thebest performances.
Three combination strategiesachieve 12 out of 16 best performances 6 .
We6 CC and CC-OU methods achieve the same best percentageperformance of 31.53% on WebKB data set.
MC and CC-think the general combination strategyoutperform individual stopping criteria foruncertainty sampling for active learning, becausefour individual stopping criteria only totallyachieve 4 out of 16 best performances.5.4 Stopping Criteria for HeterogeneousUncertainty SamplingIn the following comparison experiments on het-erogeneous uncertainty sampling, a MaxEnt-based classifier is used to select the most infor-mative examples for training an another type ofclassifier based on multinomial na?ve Bayes (NB)model (McCallum and Nigam, 1998b).MC methods achieve the same highest accuracy perform-ance of 91% on Interest data set.1134Table 2 shows that the NB-based classifiertrained on all data (i.e.
?All method?)
achievesonly 1.2% lower average accuracy performancethan that of MaxEnt-based classifier.
However,we can see from Table 2 that accuracy perform-ances of each stopping criterion for heterogene-ous uncertainty sampling are apparently lowerthan that for uncertainty sampling shown in Ta-ble 1.
The main reason is that an uncertain ex-ample for one classifier (i.e.
MaxEnt) may not bean uncertain example for other classifiers (i.e.NB).
This comparison experiments aim to ana-lyze the accuracy effectiveness of stopping crite-ria for heterogeneous uncertainty sampling,compared to that for uncertainty sampling shownin Table 1.
Therefore we do not provide the re-sults of the ?First?
method for heterogeneousuncertainty sampling.
The ?Average?
row showsthat CC-MC and CC-ME achieve the highestaverage accuracy performance of 86.3%, fol-lowed by CC-OU.
On six data sets, CC-MEachieves 3 out of 6 highest accuracy perform-ances.Interestingly, these stopping criteria work verywell on the Ontonotes and Interest data sets.Three combination strategies achieve higher ac-curacy performance than the ?All?
method onOntonotes.
However, the accuracy performancesof these seven stopping criteria for heterogene-ous uncertainty sampling on WebKB, Comp2a,Comp2b, and TWA degrade, compared to the?All?
method.The general combination strategy achieves 7out of 9 boldface accuracy performances7.
Andonly MC method achieves other 2 boldface accu-racy performances.
Experimental results showthat the general combination strategy outper-forms individual stopping criteria in overall forheterogeneous uncertainty sampling.6 Related WorkZhu and Hovy (2007) proposed a confidence-based framework to predict the upper bound andthe lower bound for a stopping criterion in activelearning.
Actually this framework is a verycoarse solution that simply uses max-confidencemethod to predict the upper bound, and uses min-error method to predict the lower bound.
Zhu et.al.
(2008) proposed a minimum expected errorstrategy to learn a stopping criterion through es-7 MC and CC-MC methods achieve the same highest accu-racy performance of 90.6% on Interest data set.
CC-MC andCC-CA methods achieve the same highest average accuracyperformance of 86.3%.timation of the classifier?s expected error on fu-ture unlabeled examples.
However, both twostudies did not give an answer to the problem ofhow to define an appropriate threshold for thestopping criterion in a specific task.Vlachos (2008) also studied a stopping crite-rion of active learning based on the estimate ofthe classifier?s confidence, in which a separateand large dataset is prepared in advance to esti-mate the classifier?s confidence.
However, thereis a risk to be misleading because how manyexamples are required for this pregiven separatedataset is an open question in real-worldapplications, and it can not guarantee that theclassifier shows a rise-peak-drop confidencepattern during active learning process.Schohn and Cohn (2000) proposed a stoppingcriterion for active learning with support vectormachines based on an assumption that the dataused is linearly separable.
However, in most real-world cases this assumption seems to beunreasonable and difficult to satisfy.
And theirstopping criterion cannot be applied for activelearning with other type of classifier such as NB,MaxEnt models.7 DiscussionWe believe that a classifier?s performancechange is a good signal of stopping the activelearning process.
It is worth studying further howto combine the factor of performance changewith our proposed stopping criteria.Among these stopping criteria, ME, CC, CC-ME can be used directly for committee-basedsampling (Engelson and Dagan, 1999) for activelearning.
However, to use MC, OU, CC-MC andCC-OU for committee-based sampling, weshould adopt a new uncertainty measurementsuch as vote entropy to measure the uncertaintyof each unlabled example in the pool.In the above active learning comparisonexperiments, the confidence estimation for eachconfidence-based stopping criterion is donewithin the unlabeled pool U.
We think that forthese confidence-based stopping criteria exceptSA method, confidence estimation on a large-scale outside unlabeled data set is worth studyingin the future work.8 Conclusion and Future WorkIn this paper, we address the stopping criterionissue of active learning, and propose a new stop-ping criterion, classification-change, which con-siders the potential ability of each unlabeled ex-1135ample on changing decision boundaries.
To solvethe problem of predefining an appropriatethreshold for each confidence-based stoppingcriterion, a multi-criteria-based general combina-tion strategy is proposed.
Experimental results onuncertainty sampling and heterogeneous uncer-tainty sampling show that these stopping criteriawork well on evaluation data sets, and combina-tion strategies can achieve better performancethan individual criteria.
Some interesting futurework is to investigate further how to combine thebest of these criteria, and how to consider per-formance change to define an appropriate stop-ping criterion for active learning.AcknowledgmentsThis work was supported in part by the National863 High-tech Project (2006AA01Z154) and theProgram for New Century Excellent Talents inUniversity (NCET-05-0287).ReferencesBerger Adam L., Vincent J. Della Pietra, Stephen A.Della Pietra.
1996.
A maximum entropy approachto natural language processing.
ComputationalLinguistics 22(1):39?71.Bruce Rebecca and Janyce Wiebe.
1994.
Word sensedisambiguation using decomposable models.
Pro-ceedings of the 32nd annual meeting on Associa-tion for Computational Linguistics, pp.
139-146.Chen Jinying, Andrew Schein, Lyle Ungar and Mar-tha Palmer.
2006.
An empirical study of the behav-ior of active learning for word sense disambigua-tion.
Proceedings of the main conference on Hu-man Language Technology Conference of theNorth American Chapter of the Association ofComputational Linguistics, pp.
120-127Engelson S. Argamon and I. Dagan.
1999.
Commit-tee-based sample selection for probabilistic classi-fiers.
Journal of Artificial Intelligence Research(11):335-360.Hovy Eduard, Mitchell Marcus, Martha Palmer,Lance Ramshaw and Ralph Weischedel.
2006.Ontonotes: The 90% Solution.
In Proceedings ofthe Human Language Technology Conference ofthe NAACL, pp.
57-60.Lee Yoong Keok and Hwee Tou Ng.
2002.
An em-pirical evaluation of knowledge sources and learn-ing algorithm for word sense disambiguation.
InProceedings of the ACL conference on Empiricalmethods in natural language processing, pp.
41-48Lewis David D. and Jason Catlett.
1994.
Heterogene-ous uncertainty sampling for supervised learning.In Proceedings of 11th International Conference onMachine Learning, pp.
148-156Lewis David D. and William A. Gale.
1994.
A se-quential algorithm for training text classifiers.
InProceedings of the 17th annual international ACMSIGIR conference on Research and development ininformation retrieval, pp.
3-12McCallum Andrew and Kamal Nigam.
1998a.
Em-ploying EM in pool-based active learning for textclassification.
In Proceedings of the 15th Interna-tional Conference on Machine Learning, pp.350-358McCallum Andrew and Kamal Nigam.
1998b.
Acomparison of event models for na?ve bayes textclassification.
In AAAI-98 workshop on learningfor text categorization.Ng Hwee Tou and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsense: an exemplar-based approach.
In Proceed-ings of the 34th Annual Meeting of the Associationfor Computational Linguistics, pp.40-47Ngai Grace and David Yarowsky.
2000.
Rule writingor annotation: cost-efficient resource usage forbased noun phrase chunking.
In Proceedings of the38th Annual Meeting of the Association for Com-putational Linguistics, pp.
117-125Roy Nicholas and Andrew McCallum.
2001.
Towardoptimal active learning through sampling estima-tion of error reduction.
In Proceedings of theEighteenth International Conference on MachineLearning, pp.
441-448Schein Andrew I. and Lyle H. Ungar.
2007.
Activelearning for logistic regression: an evaluation.Machine Learning 68(3): 235-265Schohn Greg and David Cohn.
2000.
Less is more:Active learning with support vector machines.
InProceedings of the Seventeenth International Con-ference on Machine Learning, pp.
839-846Shen Dan, Jie Zhang, Jian Su, Guodong Zhou andChew-Lim Tan.
2004.
Multi-criteria-based activelearning for named entity recognition.
In Proceed-ings of the 42nd Annual Meeting on Associationfor Computational Linguistics.Tang Min, Xiaoqiang Luo and Salim Roukos.
2002.Active learning for statistical natural languageparsing.
In Proceedings of the 40th Annual Meet-ing on Association for Computational Linguistics,pp.
120-127Vlachos Andreas.
2008.
A stopping criterion for ac-tive learning.
Computer Speech and Language.22(3): 295-312Zhu Jingbo and Eduard Hovy.
2007.
Active learningfor word sense disambiguation with methods foraddressing the class imbalance problem.
In Pro-ceedings of the 2007 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pp.783-790Zhu Jingbo, Huizhen Wang and Eduard Hovy.
2008.Learning a stopping criterion for active learningfor word sense disambiguation and text classifica-tion.
In Proceedings of the Third International JointConference on Natural Language Processing, pp.366-3721136
