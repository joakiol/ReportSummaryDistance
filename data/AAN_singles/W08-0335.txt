Proceedings of the Third Workshop on Statistical Machine Translation, pages 216?223,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsImproved Statistical Machine Translation by Multiple Chinese WordSegmentationRuiqiang Zhang1,2 and Keiji Yasuda1,2 and Eiichiro Sumita1,21National Institute of Information and Communications Technology2ATR Spoken Language Communication Research Laboratories2-2-2 Hikaridai, Science City, Kyoto, 619-0288, Japan{ruiqiang.zhang,keiji.yasuda,eiichiro.sumita}@atr.jpAbstractChinese word segmentation (CWS) is anecessary step in Chinese-English statisti-cal machine translation (SMT) and its per-formance has an impact on the results ofSMT.
However, there are many settings in-volved in creating a CWS system such asvarious specifications and CWS methods.This paper investigates the effect of thesesettings to SMT.
We tested dictionary-based and CRF-based approaches andfound there was no significant differencebetween the two in the qualty of the result-ing translations.
We also found the corre-lation between the CWS F-score and SMTBLEU score was very weak.
This paperalso proposes two methods of combiningadvantages of different specifications: asimple concatenation of training data anda feature interpolation approach in whichthe same types of features of translationmodels from various CWS schemes arelinearly interpolated.
We found these ap-proaches were very effective in improvingquality of translations.1 IntroductionChinese word segmentation (CWS) is a necessarystep in Chinese-English statistical machine transla-tion (SMT).
The research on CWS independentlyfrom SMT has been conducted for decades.
As anevidence, the CWS evaluation campaign, the SighanBakeoff (Emerson, 2005),1, has been held four timessince 2004.
However, works on relations betweenCWS and SMT are scarce.Generally, two factors need to be considered inconstructing a CWS system.
The first one is thespecifications for CWS, i.e., the rules or guidelinesfor word segmentation, and the second one is theCWS methods.
There are many CWS specificationsused by different organizations.
Unfortunately, theseorganizations do not seem to have any intention ofreaching a unified specification.
More than five orsix specifications have been used in the four SighanBakeoffs.
There is also significant disagreement onthe specifications, although much of their contents isthe same.
One of the aims of this work was thereforeto establish whether inconsistencies in specificationssignificantly affect the quality of SMT.The second factor is CWS methods.
We groupedall of the CWS methods into two classes: the classwithout out-of-vocabulary (OOV) recognition andthe class with OOV recognition, represented by thedictionary-based CWS and the CRF-based CWS, re-spectively.
Out-of-vocabulary recognition may havetwo-sided effects on SMT performance.
The CRF-based CWS that supports OOV recognition producesword segmentations with a higher F-score, but ahuge number of new words recognized correctly andincorrectly that can incur data sparseness in trainingthe SMT models.
On the other hand, the dictionary-based approach that does not support OOV recogni-tion produced a lower F-score, but with a relativelyweak data spareness problem.
Which approach pro-1A CWS competition organized by the ACL special interestgroup on Chinese.216Table 1: Examples of disagreement in segmentation guidelinesChineseName EnglishName TimeAS DENGXIAOPING GEORGE BUSH 1997YEAR 7MONTH 1DAYCITYU DENGXIAOPING GEORGEBUSH 1997 YEAR 7 MONTH 1 DAYMSR DENGXIAOPING GEORGEBUSH 1997YEAR7MONTH1DAYPKU DENG XIAOPING GEORGEBUSH 1997YEAR 7MONTH 1DAYTable 2: A second example of disagreement in segmentation guidelinesComposite words Composite wordsAS FUJITSUCOMPANY EUROZONECITYU FUJITSU COMPANY EUROZONEMSR FUJITSUCOMPANY EURO ZONEPKU FUJITSU COMPANY EUROZONEduces a better SMT result is our research interest inthis work.The performance of CWS is usually measured bythe F-score, while that of SMT is measured usingthe BLEU score.
Does a CWS with a higher F-score produce a better translation?
In this paperwe answer this question by comparing F-scores withBLEU scores.In this work, we also propose approaches to makeuse of all the Sighan training data regardless of thespecifications.
Two methods are proposed: (1) asimple combination of all the training data, and (2)implementing linear interpolation of multiple trans-lation models.
Linear interpolation is widely used inlanguage modeling for speech recognition.
We in-terpolated multiple translation models generated bythe CWS schemes and found our approaches werevery effective in improving the translations.2 CWS specifications and corpora fromthe second Sighan BakeoffA Chinese word is composed of one or more char-acters.
There are no spaces between the words.Automatic word segmentation is required for ma-chine translation.
Usually a specification is neededto carry out word segmentation.
Unfortunately, thereare many different versions of specifications.
Differ-ent tasks give rise to different requirements and theCWS specifications must be adjusted accordingly.For example, shorter segmentation has been shownto be better for speech recognition.
A compositeword (numbers, dates, times, etc.)
is split into char-acters even if it is one word defined by linguists.
Incontrast, longer segmentation is preferred for namedentity recognition consisting of longer character se-quences, such as the name of people, places, and or-ganizations.This work investigated four well-known spec-ifications created by four different organizations:Academia Sinica (AS), City University of HongKong (CITYU), Microsoft Research (Beijing)(MSR), and Beijing University (PKU).
These specswere used in the second Sighan Bakeoff (Emerson,2005).
When we compared the four specificationsand the manual segmentations in the Sighan Bakeofftraining data, we found there were many inconsis-tencies among the four specifications.
Some exam-ples are shown in Table 1 and 2.
For instance, theAS and PKU specifications are distinct in splittingboth Chinese and English names.
We also found theMSR specification generated more composite wordsand grouped longer character sequences into a word.Using this specification could generate tens of thou-sands of new words, which can cause data sparse-ness for SMT.In addition to using the four specifications, wealso downloaded the training and test corpora of thesecond Sighan Bakeoff.
We used each of the train-ing corpora provided to create a CWS scheme andevaluated the performance of the schemes on our test217data.
This enabled us to examine the effect of CWSspecifications on SMT.We used a Chinese word segmentation tool,Achilles, to implement word segmentation.
Part ofthe work using this tool was described by (Zhanget al, 2006).
The approach was reported to achievethe highest word segmentation accuracy using thedata from the second Sighan Bakeoff.
Moreover,this tool meets our need to test the effect of the twokinds of CWS approaches for SMT.
We can easilytrain a dictionary-based and a CRF-based CWS byusing this tool.
By turning the program?s option forthe CRF model on and off, we can use the Achillesas a dictionary-based approach and as a CRF-basedCWS.
In fact, the dictionary-based approach is thedefault approach for Achilles.3 Experiments3.1 SMT resourcesWe followed the instructions for the 2005 NIST MTevaluation campaign.
Training the translation mod-els for our SMT system used the available LDC par-allel data except the UN corpus.
To train the lan-guage models for English, we used all the avail-able English parallel data plus Xinhua News of theLDC Gigaword English corpus, LDC2005T12.
Insummary, we used 2.4 million parallel sentences fortraining the translation model.
We used the test datadefined in the NIST MT05 evaluation which is de-fined in the LDC corpus as LDC2006E38.
We usedthe corpus, LDC2006E43, as the development datafor loglinear model optimization.We used a phrase-based SMT system that is basedon a log-linear model incorporating multiple fea-tures.
The training and decoding system of our SMTused the publicly available Pharaoh (Koehn et al,2003)2.
GIZA++ was used for word alignment.The Pharaoh decoder was used exclusively inall the experiments.
No additional features butthe defaults defined by Pharaoh were used.
Thefeature weights were optimized against the BLEUscores (Och, 2003).We chose automatic metrics to evaluate CWS andSMT.
We used the F-score for CWS and BLEU forSMT.
The BLEU is BLEU4, computed using theNIST-provided ?mt-eval?
script.2http://www.iccs.informatics.ed.ac.uk/?pkoehn3.2 Implementation of CWS schemesTo determine the effect of CWS on SMT, we cre-ated 14 CWS schemes which are shown in Ta-ble 3.
Schemes 1 to 12 were implemented usingthe in-house tool, Achilles, and schemes 13 and 14using off-the-shelf tools.
The CWS schemes arenamed according to the specifications (AS, CITYU,MSR, PKU), implementing methods (CRF-based ordictionary-based), and lexicon sources (Sighan orLDC corpus).
The table also shows the results ofsegmentation on the SMT training and test data, i.e.,number of total tokens, unique words, and OOVwords.We divided the schemes into two groups for sim-plicity.
The first group includes schemes 1 to 12,which were trained using a specific Sighan corpus.For example, schemes 1 to 3 were trained using theAS corpus, schemes 4 to 6 using the CITYU cor-pus, and so on.
The meaning of the name of theCWS scheme can be derived from the table ?
thename is defined by specifications, methods and lexi-con sources.
For example, the CRF-AS scheme per-forms CRF-based segmentation; and its lexicon isfrom the AS corpus provided by the Sighan.
TheCRF-AS segmenter can be easily trained, as de-scribed by Achilles.The second group contains two schemes 13 and14.
The ICTCLAS is a HHMM-based hierarchicalHMM segmenter (Zhang et al, 2003) that uses thespecifications of PKU.
This segmenter incorporatesparts-of-speech information in the probability mod-els and generates multiple HMM models for solvingsegmentation ambiguities.
The MSRSEG was de-veloped by Gao et al (Gao et al, 2004).
This seg-menter is based on the MSR specifications.
It uses alog-linear model that integrates multiple features.The segmenters of the first group, dict-ASand dict-LDC-AS, are two dictionary-based CWSschemes.
They differ in lexicon size and lexiconextracting source.
The former used a lexicon ex-tracted directly from the Sighan AS training datawhile the latter used a lexicon from LDC parallelcorpora.
It took some efforts to get the lexicon.
First,we used the CRF-AS to segment the LDC corpora.We extracted a unique word list from the segmenteddata and sorted it in decreasing order according toword frequency.
Because OOV was recognized by218Table 3: Analysis of results of segmentation on LDC training and test data for all CWS schemesNo.
CWS schemes Specifications Methods Lexicon Tokens Unique words OOVs1 CRF-AS AS CRF Sighan 47,934,088 413,588 1,1932 dict-AS AS Dict Sighan 51,664,675 89,346 2373 dict-LDC-AS AS Dict LDC 48,665,364 102,919 2734 CRF-CITYU CITYU CRF Sighan 47,963,541 426,273 1,1555 dict-CITYU CITYU Dict Sighan 51,251,729 56,996 3626 dict-LDC-CITYU CITYU Dict LDC 48,787,154 102,754 2177 CRF-MSR MSR CRF Sighan 46,483,923 523,788 1,2978 dict-MSR MSR Dict Sighan 51,302,509 60,247 2489 dict-LDC-MSR MSR Dict LDC 47,469,271 102,390 21710 CRF-PKU PKU CRF Sighan 48,022,697 440,114 1,13611 dict-PKU PKU Dict Sighan 52,721,809 47,176 21112 dict-LDC-PKU PKU Dict LDC 48,721,795 102,213 25613 ICTCLAS PKU HHMM - 50,751,402 162,222 83514 MSRSEG MSR - - ?48,734,113 274,411 1,443the CRF-AS, a huge word list was generated(see Ta-ble 3).
We chose the most frequent 100,000 wordsas the dictionary for the dict-LDC-AS 3.
The LM forthe dict-AS was trained using the AS corpus whilethe LM for the dict-LDC-AS was trained using thesegmented SMT training corpus.Therefore, the dict-LDC-AS used a larger lexiconthan the dict-AS.
This lexicon contained the mostfrequent OOV words recognized by the CRF-AS.Our aim was to investigate whether the dict-LDC-AS, whose lexicon consisted of the lexicon of dict-AS and new words recognized by CRF-AS, couldimprove SMT.As shown in Table 3, using CRF-AS generated ahuge number of unique words for the training dataand OOV words for the test data.
We found thatthe CRF-AS generated three times more OOVs forthe test data than the dictionary-based CWS,dict-AS(see OOVs in Table 3).Other schemes in the first group were imple-mented similarly to the ?AS?.Table 3 lists the segmentation statistics for thetraining and test data of all the tested CWS schemes,where ?Tokens?
indicates the total number of wordsin the training data.
?Unique words?
and ?OOVs?3Only those words that appeared at least five times in thelexicon were considered.Table 4: BLEU scores for CWS schemesCWS AS CITYU MSR PKUCRF 23.70 23.55 22.50 23.61dict 23.46 23.72 23.33 23.61dict-LDC 23.52 23.36 23.16 23.74ICTCLAS - - - 24.12MSRSEG - - 19.72 -BEST 23.70 23.72 23.33 23.74 (24.12)mean the lexicon size of the segmented training dataand the unknown words in the test data, respectively.3.3 Effect of CWS specifications on SMTOur first concern was the effect of CWS specifica-tions on SMT.
The results in Table 4 show the rela-tionships that were found.
The last row gives thebest BLEU scores obtained for each of the CWSspecifications.
The scores for AS, CITYU, MSR andPKU were 23.70 (CRF-AS), 23.72 (dict-CITYU),23.33 (dict-MSR) and 23.74 (dict-PKU-LDC), re-spectively.
We found there were no observable dif-ferences between AS, CITYU, and PKU.
However,the specification that produced the worst transla-tions was the MSR.
The MSR specification appears219to have been designed for recognizing named enti-ties (NE) (See the examples of segmentation in Ta-ble 1).
Many NEs are regarded as words by MSR,while they are more appropriately split into sepa-rate words by other specifications.
For example, thelong word, ?1997YEAR7MONTH1DAY?
(?July 1,1997?).
As a result, the CRF-MSR generated 20%more words in the vocabulary than the other CWSschemes in segmenting the SMT training data.
Thelarger vocabulary can trigger data sparseness prob-lems and result in SMT degradation.
The segmenter,MSRSEG, produced an even lower BLEU score(19.72) than the Achilles.The results were verified by significancetest (Zhang et al, 2004).
We found the systemswith the BLEU scores higher than 23.70 weresignificantly better than those lower than 23.70.3.4 Correlation between BLEU score andF-scoreThe values of the F-scores and BLEU scores arelisted in parallel in Table 5.
We tied the F-scoresand specifications together because comparing thevalue of the F-score across specs is meaningless.
Weseparated the F-score and BLEU score for differentcorpus.
The F-score was calculated using the Sighantest data.
The CRF-based approach usually gives ahigher F-score, but its corresponding BLEU scoreswere not always higher.
The F-score and BLEUscore correlated well for ICTCLAS and CRF-ASbut less well for CRF-CITYU, CRF-PKU and CRF-MSR.
Obviously, there is no strong correlation be-tween the F-score and BLEU score.4 Effect of combining multiple CWSschemesWe used the Sighan Bakeoff corpora of differentCWS specifications separately in the previous ex-periments.
Here, we propose two approaches to us-ing all the resources combined.
The first approachis to concatenate all the training data of the SighanBakeoff, regardless of the specifications and train-ing a new CWS for segmenting SMT training data.The second approach involves linear integration oftranslation models.
We found that both approachesproduced an improvement in translation quality.4.1 Effect of combining training data frommultiple CWS specificationsThe CWS specifications are very different and thecorresponding Sighan training data are segmentedin different ways.
We used these data separatelyin the previous work as if they were incompatible.However, creating data manually is laborious andcostly.
It would therefore be a significant advan-tage if all the data could be used, regardless of thedifferent specifications.
We therefore created a newCWS scheme, called ?dict-hybrid?.
This CWS wastrained by concatenating all the Sighan Bakeoff cor-pora regardless of the different specifications.
The?dict-hybrid?
was trained using Achilles.
It uses adictionary-based approach, and its lexicon and lan-guage model were obtained as follows.First, we created a hybrid corpus by combiningall the Sighan training corpora: AS, CITYU, MSR,PKU.
The hybrid corpus was used to train a CRF-based CWS.
This CWS was then used to segmentthe SMT training corpus and then we extracted alexicon of 100,000 from the top frequent words ofthe segmented SMT corpus.
This lexicon was usedas the lexicon of the ?dict-hybrid.?
The LM of ?dict-hybrid?
was also trained on the segmented corpus.Note a lexicon and a LM are the only needed re-sources for building a dictionary-based CWS, likethe ?dict-hybrid.?
(Zhang et al, 2006)We used the ?dict-hybrid?
to segment the SMTtraining corpus and test data.
This segmentationgenerated 49,546,231 tokens, 112,072 unique wordsfor the training data and 693 OOVs for the test data.The segmentation data were used for training anew SMT model.
We tested the model using thesame approach and found the BLEU score obtainedby this CWS scheme was 23.91.
This score wasbetter than those in Table 4 obtained by any of theAchilles CWS schemes except ICTCLAS.
There-fore, the CWS scheme ?dict-hybrid?
produced bettertranslations than other schemes implemented usingAchilles, indicating that using multiple CWS cor-pora can improve SMT even if their specificationsare different.Significance testing also showed that the resultsfor ICTCLAS and ?dict-hybrid?
were not signifi-cantly different.
The results of ?dict-hybrid?
are sig-nificantly better than those in the Table 4 which have220Table 5: Correlation between F-score and BLEUPKU MSRF-score BLEU F-score BLEUCRF 0.939 23.61 CRF 0.954 22.50dict 0.930 23.61 dict 0.947 23.22dict-LDC 0.931 23.74 dict-LDC 0.928 23.16ICTCLAS 0.948 24.12 MSRSEG 0.969 19.72CITYU ASF-score BLEU F-score BLEUCRF 0.920 23.55 CRF 0.922 23.70dict 0.873 23.72 dict 0.896 23.46dict-LDC 0.886 23.36 dict-LDC 0.878 23.52a BLEU score lower than 23.70.4.2 Effect of feature interpolation oftranslation modelsWe investigated the effect of linearly integratingmultiple features of the same type.
We generatedmultiple translation models by using different wordsegmenters.
Each translation model corresponded toa word segmenter.
The same type of features as inthe log-linear model were added linearly.
For exam-ple, the phrase translation model p(e| f ) can be lin-early interpolated as, p(e| f ) = ?Si=1 ?i pi(e| f ) wherepi(e| f ) is the phrase translation model correspond-ing to the i-th CWSs.
?i is the weight, and S is thetotal number of models.
?Si=1 ?i = 1.?s can be obtained by maximizing the likelihoodor BLEU scores of the development data.
Optimiz-ing the ?
has been described elsewhere (Foster andKuhn, 2007).
p(e| f ) is the phrase translation modelgenerated.In addition to the phrase translation model, weused the same approach to integrate three otherfeatures: phrase inverse probability p( f |e), lexicalprobability lex(e| f , a), and lexical inverse probabil-ity lex( f |e, a).We integrated the CWS schemes ranked in thetop five in Table 4: ICTCLAS, dict-hybrid, dict-LDC-PKU, dict-CITYU, and CRF-AS.
We labeledthe five schemes A, B, C, D, and E, respectively,as shown in Table 6.
The first line of Table 6 rep-resents the test data segmented by the five CWSschemes.
?tst-A?
means the test data was segmentedby ICTCLAS.
?tst-B?
means the test data segmentedby ?dict-hybrid?, and so on.
The second line givesbaseline results showing the original results with-out the use of feature integration.
For different testdata, the baseline is different.
The baseline of ICT-CLAS was tested on ?tst-A?
only.
The baseline of?dict-hybrid?
was tested on ?tst-B?
only.
From thethird line we gradually added a translation modelto the models used in the baseline.
For example,?A+B?
integrates models made using ICTCLAS and?dict-hybrid.?
Each integration models were testedonly on the test data participated in the integration.Hence, some slots in Table 6 are blank.We did not carry out parameter optimization withregards to the ?s.
Instead, we used equal ?s for allthe features.
For example, all ?s equal 0.5 for A+B,and 0.25 for A+B+C+D.
Each cell in Table 6 indi-cates the BLEU score of the integration in relationto the test data.
We found our approach improvedthe baseline results significantly.
The more modelsintegrated, the better the results.
The improvementwas positive for all of the test data.
With regards tothe integration, if a phrase pair exists in one modelonly, we suppose the values of probabilities are zeroin other models.To better understand the effects of feature inter-polation, we blended the features of the translationmodels, as shown in Table 7, by simply combiningthe phrase pairs without probability interpolation.When we merged two models, we defined one modelas the master model and the other as the supple-mentary model.
Only phrase pairs that were in the221supplementary models but not in the master modelwere appended to the master model.
Their featureprobabilities were not changed.
Hence, the com-bined model was a blend of phrase pairs from themaster model and supplementary model.
There wasno probability integration, that was significantly dif-ferent from the feature interpolation approach.
Foreach set of test data in Table 7, the master modelwas the model using the same CWS as the test data.While there was one row for each type of combina-tion, the cells in the row contained different models.For example, ?A+B?
for test data ?A?
uses ?A?
as themaster model and ?B?
as the supplementary model,while the opposite holds for test data ?B?.Comparing Table 6 and 7 showed that featureinterpolation outperformed feature blending.
Fea-ture interpolation yielded surprisingly good results.The performance consistently improved when moremodels were integrated, but this was not the casefor feature blending.
This shows that probabilityintegration is very effective.
Increasing the size ofphrase pairs, as feature blending does, is not as ef-fective.We used equal values for the ?s.
Optimal valuesmay be obtained using the optimization approachof maximizing BLEU or the likelihood of develop-ment data as has been reported previously (Fosterand Kuhn, 2007).
However, optimization is compu-tationally expensive and the effect was not satisfac-tory.
Therefore, we decided not optimizing the ?s inthis work.5 Related work and DiscussionsCWS has been the subject of intensive researchin recent years, as is evident from the lastfour international evaluations, the Sighan Bake-offs, and many approaches have been proposedover the past decade.
Segmentation performancehas been improved significantly, from the earli-est maximal match (dictionary-based) approaches toCRF (Peng and McCallum, 2004) approach.
Weused dictionary-based and CRF-based CWS ap-proaches to demonstrate the effect of CWS on SMT,both without and with OOV recognition.SMT is a very complicated system to study.
Itsresponse to CWS schemes is intractable and it isvery hard to use one or two measures to describethe relationship between CWS and SMT, in a similarway to describing the relationship between the align-ment error rate (AER) and SMT (Fraser and Marcu,2007).
The CWS and SMT are related by a series offactors such as the specifications, OOVs, lexicons,and F-scores.
None of these factors can be directlyrelated to the SMT.
While we have completed manyexperiments, based on changing the CWS specifica-tions and methods used, to determine the relation-ship between CWS and SMT, we have not estab-lished any overwhelming rules.
However, we be-lieve the following guidelines are appropriate in con-sidering a CWS system for SMT.
Firstly, the F-scoreis not a reliable guide to SMT quality.
A very highF-score may produce the lowest quality translations,as was found for the MSRSEG.
Secondly, it is betterto design a specification with smaller word units toreduce data sparseness.
Specifications like those forMSR will produce an inferior translation.
Thirdly,do not use a huge lexicon for word segmentation.A huge lexicon will result in data sparseness andsegmentation complexity.
And lastly, using multi-ple word segmentation results and approaches doeswork.
We used two approaches that combined mul-tiple word segmentation - dict-hybrid and feature in-tegration - and both improved the translations signif-icantly.The BLEU scores in our experiments were rela-tively low in comparison with current state-of-the artresults.
However, our system was very similar to thesystem (Koehn et al, 2005) that gave a BLEU scoreof 24.3, comparable to ours.
The BLEU score canbe raised if we do post-editing, use more data forlanguage modeling and other methods.6 ConclusionsWe investigated the effect of CWS on SMT fromtwo points of view.
Firstly, we analyzed multipleCWS specifications and built a CWS for each one toexamine how they affected translations.
Secondly,we investigated the advantages and disadvantages ofvarious CWS approaches, both dictionary-based andCRF-based, and built CWSs using these approachesto examine their effect on translations.We proposed a new approach to linear interpo-lation of translation features.
This approach pro-duced a significant improvement in translation and222Table 6: Feature interpolation of translation models: A=ICTCLAS, B=dict-hybrid, C=dict-PKU-LDC, D=dict-CITYU, E=CRF-ASModel tst-A tst-B tst-C tst-D tst-EBaseline 24.12 23.91 23.74 23.72 23.70A+B 24.25 24.20A+B+C 24.49 24.31 23.84A+B+C+D 24.60 24.43 24.05 24.27A+B+C+D+E 24.61 24.55 24.16 24.39 24.17Table 7: Feature blending of translation modelsModel tst-A tst-B tst-C tst-D tst-EBaseline 24.12 23.91 23.74 23.72 23.70A+B 24.20 24.24A+B+C 24.27 24.14 23.69A+B+C+D 23.92 24.29 23.61 24.00A+B+C+D+E 23.86 24.31 23.69 24.05 23.76achieved the best BLEU score of all the CWSschemes.We have published a much more detailed pa-per (Zhang et al, 2008) to describe the relations be-tween CWS and SMT.ReferencesThomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHAN Workshop on Chinese LanguageProcessing, Jeju, Korea.George Foster and Roland Kuhn.
2007.
Mixture-modeladaptation for SMT.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages128?135, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Alexander Fraser and Daniel Marcu.
2007.
Measuringword alignment quality for statistical machine transla-tion.
In Computational linguistics, Squibs Discussion,volume 33 of 3, pages 293?303, September.Jianfeng Gao, Andi Wu, Mu Li, Chang-Ning Huang,Hongqiao Li, Xinsong Xia, and Haowei Qin.
2004.Adaptive chinese word segmentation.
In ACL-2004,pages 462?469, Barcelona, July.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In HLT-NAACL2003: Main Proceedings, pages 127?133.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Miles Osborne Chris Callison-Burch, DavidTalbot, and Michael White.
2005.
Edinburgh sys-tem description for the 2005 nist mt evaluation.
InProceedings of Machine Translation Evaluation Work-shop.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
of the 41stAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 160?167.Fuchun Peng and Andrew McCallum.
2004.
Chinesesegmentation and new word detection using condi-tional random fields.
In Proc.
of Coling-2004, pages562?568, Geneva, Switzerland.Huaping Zhang, HongKui Yu, Deyi xiong, and Qun Liu.2003.
HHMM-based Chinese lexical analyzer ICT-CLAS.
In Proceedings of the Second SIGHAN Work-shop on Chinese Language Processing, pages 184?187.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.
In-terpreting BLEU/NIST scores: How much improve-ment do we need to have a better system?
In Proceed-ings of the LREC.Ruiqiang Zhang, Genichiro Kikui, and Eiichiro Sumita.2006.
Subword-based tagging by conditional randomfields for chinese word segmentation.
In Proceedingsof the HLT-NAACL, Companion Volume: Short Pa-pers, pages 193?196, New York City, USA, June.Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.2008.
Chinese word segmentation and statistical ma-chine translation.
ACM Trans.
Speech Lang.
Process.,5(2), May.223
