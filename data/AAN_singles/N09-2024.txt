Proceedings of NAACL HLT 2009: Short Papers, pages 93?96,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsA Simple Sentence-Level Extraction Algorithm for Comparable DataChristoph Tillmann and Jian-ming XuIBM T.J. Watson Research CenterYorktown Heights, N.Y. 10598{ctill,jianxu}@us.ibm.comAbstractThe paper presents a novel sentence pair ex-traction algorithm for comparable data, wherea large set of candidate sentence pairs is scoreddirectly at the sentence-level.
The sentence-level extraction relies on a very efficient im-plementation of a simple symmetric scoringfunction: a computation speed-up by a fac-tor of 30 is reported.
On Spanish-Englishdata, the extraction algorithm finds the highestscoring sentence pairs from close to 1 trillioncandidate pairs without search errors.
Sig-nificant improvements in BLEU are reportedby including the extracted sentence pairs intothe training of a phrase-based SMT (StatisticalMachine Translation) system.1 IntroductionThe paper presents a simple sentence-level trans-lation pair extraction algorithm from comparablemonolingual news data.
It differs from similaralgorithms that select translation correspondencesexplicitly at the document level (Fung and Che-ung, 2004; Resnik and Smith, 2003; Snover etal., 2008; Munteanu and Marcu, 2005; Quirk etal., 2007; Utiyama and Isahara, 2003).
In thesepublications, the authors use Information-Retrieval(IR) techniques to match document pairs that arelikely translations of each other.
More complexsentence-level models are then used to extract par-allel sentence pairs (or fragments).
From a com-putational perspective, the document-level filteringsteps are needed to reduce the number of candidatesentence pairs.
While IR techniques might be use-ful to improve the selection accuracy, the current pa-per demonstrates that they are not necessary to ob-tain parallel sentence pairs.
For some data, e.g.
thePortuguese-English Reuters data used in the experi-ments in Section 3, document-level information maynot even be available.In this paper, sentence pairs are extracted by a sim-ple model that is based on the so-called IBM Model-1 (Brown et al, 1993).
The Model-1 is trainedon some parallel data available for a language pair,i.e.
the data used to train the baseline systems inSection 3.
The scoring function used in this pa-per is inspired by phrase-based SMT.
Typically, aphrase-based SMT system includes a feature thatscores phrase pairs using lexical weights (Koehn etal., 2003) which are computed for two directions:source to target and target to source.
Here, a sen-tence pair is scored as a phrase pair that covers allthe source and target words.
The scoring function?
(S, T ) is defined as follows:?
(S, T ) = (1)=J?j=11J ?
log(p(sj |T )?
??
?1I ?I?i=1p(sj|ti) )?
??
??
(sj ,T )+I?i=11I ?
log(p(ti|S)?
??
?1J ?J?j=1p(ti|sj) )?
??
??
(ti,S)93Here, S = sJ1 is the source sentence of length J andT = tI1 is the target sentence of length I .
p(s|T )is the Model-1 probability assigned to the sourceword s given the target sentence T , p(t|S) is definedaccordingly.
p(s|t) and p(t|s) are word translationprobabilities obtained by two parallel Model-1 train-ing steps on the same data, but swapping the roleof source and target language.
They are smoothedto avoid 0.0 entries; there is no special NULL-wordmodel and stop words are kept.
The log(?)
is appliedto turn the sentence-level probabilities into scores.These log-probabilities are normalized with respectto the source and target sentence length: this waythe score ?
(S, T ) can be used across all sentencepairs considered, and a single manually set thresh-old ?
is used to select all those sentence pairs whosescore is above it.
For computational reasons, thesum ?
(S, T ) is computed over the following terms:?
(ti, S) where 1 ?
i ?
I and ?
(sj, T ), where1?
j?
J .
The ?
?s and ?
?s represent partial scorecontributions for a given source or target position.Note that ?
(S, T ) ?
0 since the terms ?
(?, S) ?
0and ?
(?, T ) ?
0.Section 2 presents an efficient implementation ofthe scoring function in Eq.
1.
Its effectiveness isdemonstrated in Section 3.
Finally, Section 4 dis-cusses future work and extensions of the current al-gorithm.2 Sentence-Level ProcessingWe process the comparable data at the sentence-level: for each language and all the documents inthe comparable data, we distribute sentences over alist of files : one file for each news feed f (for theSpanish Gigaword data, there are 3 news feeds) andpublication date d .
The Gigaword data comes anno-tated with sentence-level boundaries, and all docu-ment boundaries are discarded.
This way, the Span-ish data consists of about 24 thousand files and theEnglish data consists of about 53 thousand files (fordetails, see Table 2).
For a given source sentence S,the search algorithm computes the highest scoringsentence pair ?
(S, T ) over a set of candidate trans-lations T ?
?, where |?| can be in the hundredsof thousands of sentences .
?
consists of all targetsentences that have been published from the samenews feed f within a 7 day window from the pub-lication date of the current source sentence S. Theextraction algorithm is guaranteed to find the highestscoring sentence pairs (S, T ) among all T ?
?.
Inorder to make this processing pipeline feasible, thescoring function in Eq.
1 needs to be computed veryefficiently.
That efficiency is based on the decompo-sition of the scoring functions into I + J terms ( ?
?sand ?
?s) where source and target terms are treateddifferently.
While the scoring function computationis symmetric, the processing is organized accordingthe source language files: all the source sentencesare processed one-by-one with respect to their indi-vidual candidate sets ?:?
Caching for target term ?
(t, S): For each tar-get word t that occurs in a candidate translationT , the Model-1 based probability p(t|S) can becached: its value is independent of the otherwords in T .
The same word t in different tar-get sentences is processed with respect to thesame source sentence S and p(t|S) has to becomputed only once.?
Array access for source terms ?
(s, T ): For agiven source sentence S, we compute the scor-ing function ?
(S, T ) over a set of target sen-tences T ?
?.
The computation of the sourceterm ?
(s, T ) is based on translation probabil-ities p(s|t) .
For each source word s, we canretrieve all target words t for which p(s|t) > 0just once.
We store those words t along withtheir probabilities in an array the size of the tar-get vocabulary.
Words t that do not have anentry in the lexicon have a 0 entry in that ar-ray.
We keep a separate array for each sourceposition.
This way, we reduce the probabilityaccess to a simple array look-up.
Generatingthe full array presentation requires less than 50milliseconds per source sentence on average.?
Early-Stopping: Two loops compute the scor-ing function ?
(S, T ) exhaustively for each sen-tence pair (S, T ): 1) a loop over all the targetposition terms ?
(ti, S), and 2) a loop over allsource position terms ?
(sj , T ) .
Once the cur-rent partial sum is lower than the best score?
(S, Tbest) computed so far, the computationcan be safely discarded as ?
(ti, S), ?
(sj , T ) ?94Table 1: Effect of the implementation techniques on afull search that computes ?
(S, T ) exhaustively for all sen-tence pairs (S, T ) for a given S.Implementation Technique Speed[secs/sent]Baseline 33.95+ Array access source terms 19.66+ Cache for target terms 3.83+ Early stopping 1.53+ Frequency sorting 1.230 and adding additional terms can only lowerthat partial sum further.?
Frequency-Sorting: Here, we aim at makingthe early pruning step more efficient.
Sourceand target words are sorted according to thesource and target vocabulary frequency: lessfrequent words occur at the beginning of a sen-tence.
These words are likely to contributeterms with high partial scores.
As a result, theearly-stopping step fires earlier and becomesmore effective.?
Sentence-level filter: The word-overlap filterin (Munteanu and Marcu, 2005) has been im-plemented: for a sentence pair (S, T ) to be con-sidered parallel the ratio of the lengths of thetwo sentences has to be smaller than two.
Ad-ditionally, at least half of the words in each sen-tence have to have a translation in the other sen-tence based on the word-based lexicon.
Here,the implementation of the coverage restrictionis tightly integrated into the above implemen-tation: the decision whether a target word iscovered can be cached.
Likewise, source wordcoverage can be decided by a simple arraylook-up.3 ExperimentsThe parallel sentence extraction algorithm presentedin this paper is tested in detail on the large-scale Spanish-English Gigaword data (Graff, 2006;Graff, 2007).
The Spanish data comes from 3news feeds: Agence France-Presse (AFP), Associ-ated Press Worldstream (APW), and Xinhua NewsTable 2: Corpus statistics for comparable data.
Anydocument-level information is ignored.Spanish EnglishDate-Feed Files 24, 005 53, 373Sentences 19.4 million 47.9 millionWords 601.5 million 1.36 billionPortuguese EnglishDate-Feed Files 351 355Sentences 366.0 thousand 5.3 millionWords 11.6 million 171.1 millionAgency (XIN).
We do not use the additional newsfeed present in the English data.
Table 1 demon-strates the effectiveness of the implementation tech-niques in Section 2.
Here, the average extractiontime per source sentence is reported for one of the24, 000 source language files.
This file contains 913sentences.
Here, the size of the target candidate set?
is 61 736 sentences.
All the techniques presentedresult in some improvement.
The baseline uses onlythe length-based filtering and the coverage filteringwithout caching the coverage decisions (Munteanuand Marcu, 2005).
Caching the target word proba-bilities results in the biggest reduction.
The resultsare representative: finding the highest scoring targetsentence T for a given source sentence S takes about1 second on average.
Since 20 million source sen-tences are processed, and the workload is distributedover roughly 120 processors, overall processing timesums to less than 3 days.
Here, the total number oftranslation pairs considered is close to 1 trillion.The effect of including additional sentence pairsalong with selection statistics is presented in Ta-ble 3.
Translation results are presented for a standardphrase-based SMT system.
Here, both languagesuse a test set with a single reference.
Including about1.4 million sentence pairs extracted from the Giga-word data, we obtain a statistically significant im-provement from 42.3 to 45.6 in BLEU (Papineni etal., 2002).
The baseline system has been trainedon about 1.8 million sentence pairs from Europarland FBIS parallel data.
We also present results fora Portuguese-English system: the baseline has beentrained on Europarl and JRC data.
Parallel sentencepairs are extracted from comparable Reuters newsdata published in 2006.
The corpus statistics for95Table 3: Spanish-English and Portuguese-English extrac-tion results.Data Source # candidates #train pairs BleuSpanish-English: ?
= ?4.1Baseline - 1, 825, 709 42.3+ Gigaword 955.5 ?
109 1, 372, 124 45.6Portuguese-English: ?
= ?5.0Baseline - 2, 221, 891 45.3+ Reuters 06 32.8 ?
109 48, 500 48.5the Portuguese-English data are given in Table 2.The selection threshold ?
is determined with thehelp of bilingual annotators (it typically takes a fewhours).
Sentence pairs are selected with a conserva-tive threshold ??
first.
Then, all the sentence pairs aresorted by descending score.
The annotator descendsthis list to determine a score threshold cut-off.
Here,translation pairs are considered to be parallel if 75% of source and target words have a correspondingtranslation in the other sentence.
Using a threshold?
= ?4.1 for the Spanish-English data, results in aselection precision of around 80 % (most of the mis-qualified pairs are partial translations with less than75 % coverage or short sequences of high frequencywords).
This simple selection criterion proved suf-ficient to obtain the results presented in this paper.As can be seen from Table 3, the optimal thresholdis language specific.4 Future Work and DiscussionIn this paper, we have presented a novel sentence-level pair extraction algorithm for comparable data.We use a simple symmetrized scoring functionbased on the Model-1 translation probability.
Withthe help of an efficient implementation, it avoidsany translation candidate selection at the docu-ment level (Resnik and Smith, 2003; Smith, 2002;Snover et al, 2008; Utiyama and Isahara, 2003;Munteanu and Marcu, 2005; Fung and Cheung,2004).
In particular, the extraction algorithm workswhen no document-level information is available.Its usefulness for extracting parallel sentences isdemonstrated on news data for two language pairs.Currently, we are working on a feature-rich ap-proach (Munteanu and Marcu, 2005) to improvethe sentence-pair selection accuracy.
Feature func-tions will be ?light-weight?
such that they can becomputed efficiently in an incremental way at thesentence-level.
This way, we will be able to main-tain our search-driven extraction approach.
We arealso re-implementing IR-based techniques to pre-select translation pairs at the document-level, togauge the effect of this additional filtering step.
Wehope that a purely sentence-level processing mightresult in a more productive pair extraction in future.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
CL, 19(2):263?311.Pascale Fung and Percy Cheung.
2004.
Mining Very-Non-Parallel Corpora: Parallel Sentence and LexiconExtraction via Bootstrapping and EM.
In Proc, ofEMNLP 2004, pages 57?63, Barcelona, Spain, July.Dave Graff.
2006.
LDC2006T12: Spanish GigawordCorpus First Edition.
LDC.Dave Graff.
2007.
LDC2007T07: English GigawordCorpus Third Edition.
LDC.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proc.
ofHLT-NAACL?03, pages 127?133, Edmonton, Alberta,Canada, May 27 - June 1.Dragos S. Munteanu and Daniel Marcu.
2005.
Improv-ing Machine Translation Performance by ExploitingNon-Parallel Corpora.
CL, 31(4):477?504.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In In Proc.
ofACL?02, pages 311?318, Philadelphia, PA, July.Chris Quirk, Raghavendra Udupa, and Arul Menezes.2007.
Generative Models of Noisy Translations withApplications to Parallel Fragment Extraction.
InProc.
of the MT Summit XI, pages 321?327, Copen-hagen,Demark, September.Philip Resnik and Noah Smith.
2003.
The Web as Paral-lel Corpus.
CL, 29(3):349?380.Noah A. Smith.
2002.
From Words to Corpora: Rec-ognizing Translation.
In Proc.
of EMNLP02, pages95?102, Philadelphia, July.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and Translation Model Adaptationusing Comparable Corpora.
In Proc.
of EMNLP08,pages 856?865, Honolulu, Hawaii, October.Masao Utiyama and Hitoshi Isahara.
2003.
ReliableMeasures for Aligning Japanese-English News Arti-cles and Sentences.
In Proc.
of ACL03, pages 72?79,Sapporo, Japan, July.96
