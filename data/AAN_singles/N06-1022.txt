Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 168?175,New York, June 2006. c?2006 Association for Computational LinguisticsMultilevel Coarse-to-fine PCFG ParsingEugene Charniak, Mark Johnson, Micha Elsner, Joseph Austerweil,David Ellis, Isaac Haxton, Catherine Hill, R. Shrivaths,Jeremy Moore, Michael Pozar, and Theresa VuBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912ec@cs.brown.eduAbstractWe present a PCFG parsing algorithmthat uses a multilevel coarse-to-fine(mlctf) scheme to improve the effi-ciency of search for the best parse.Our approach requires the user to spec-ify a sequence of nested partitions orequivalence classes of the PCFG non-terminals.
We define a sequence ofPCFGs corresponding to each parti-tion, where the nonterminals of eachPCFG are clusters of nonterminals ofthe original source PCFG.
We use theresults of parsing at a coarser level(i.e., grammar defined in terms of acoarser partition) to prune the nextfiner level.
We present experimentsshowing that with our algorithm thework load (as measured by the totalnumber of constituents processed) isdecreased by a factor of ten with no de-crease in parsing accuracy compared tostandard CKY parsing with the origi-nal PCFG.
We suggest that the searchspace over mlctf algorithms is almosttotally unexplored so that future workshould be able to improve significantlyon these results.1 IntroductionReasonably accurate constituent-based parsingis fairly quick these days, if fairly quick meansabout a second per sentence.
Unfortunately, thisis still too slow for many applications.
In somecases researchers need large quantities of parseddata and do not have the hundreds of machinesnecessary to parse gigaword corpora in a weekor two.
More pressingly, in real-time applica-tions such as speech recognition, a parser wouldbe only a part of a much larger system, andthe system builders are not keen on giving theparser one of the ten seconds available to pro-cess, say, a thirty-word sentence.
Even worse,some applications require the parsing of multi-ple candidate strings per sentence (Johnson andCharniak, 2004) or parsing from a lattice (Halland Johnson, 2004), and in these applicationsparsing efficiency is even more important.We present here a multilevel coarse-to-fine(mlctf) PCFG parsing algorithm that reducesthe complexity of the search involved in find-ing the best parse.
It defines a sequence of in-creasingly more complex PCFGs, and uses theparse forest produced by one PCFG to prunethe search of the next more complex PCFG.We currently use four levels of grammars in ourmlctf algorithm.
The simplest PCFG, which wecall the level-0 grammar, contains only one non-trivial nonterminal and is so simple that min-imal time is needed to parse a sentence usingit.
Nonetheless, we demonstrate that it identi-fies the locations of correct constituents of theparse tree (the ?gold constituents?)
with highrecall.
Our level-1 grammar distinguishes onlyargument from modifier phrases (i.e., it has twonontrivial nonterminals), while our level-2 gram-mar distinguishes the four major phrasal cate-gories (verbal, nominal, adjectival and preposi-tional phrases), and level 3 distinguishes all ofthe standard categories of the Penn treebank.168The nonterminal categories in these grammarscan be regarded as clusters or equivalence classesof the original Penn treebank nonterminal cat-egories.
(In fact, we obtain these grammars byrelabeling the node labels in the treebank andextracting a PCFG from this relabelled treebankin the standard fashion, but we discuss other ap-proaches below.)
We require that the partitionof the nonterminals defined by the equivalenceclasses at level l + 1 be a refinement of the par-tition defined at level l. This means that eachnonterminal category at level l+1 is mapped to aunique nonterminal category at level l (althoughin general the mapping is many to one, i.e., eachnonterminal category at level l corresponds toseveral nonterminal categories at level l + 1).We use the correspondence between categoriesat different levels to prune possible constituents.A constituent is considered at level l + 1 onlyif the corresponding constituent at level l hasa probability exceeding some threshold.. Thusparsing a sentence proceeds as follows.
We firstparse the sentence with the level-0 grammar toproduce a parse forest using the CKY parsingalgorithm.
Then for each level l + 1 we reparsethe sentence with the level l + 1 grammar us-ing the level l parse forest to prune as describedabove.
As we demonstrate, this leads to consid-erable efficiency improvements.The paper proceeds as follows.
We next dis-cuss previous work (Section 2).
Section 3 out-lines the algorithm in more detail.
Section4 presents some experiments showing that thework load (as measured by the total number ofconstituents processed) is decreased by a fac-tor of ten over standard CKY parsing at thefinal level.
We also discuss some fine points ofthe results therein.
Finally in section 5 we sug-gest that because the search space of mlctf al-gorithms is, at this point, almost totally unex-plored, future work should be able to improvesignificantly on these results.2 Previous ResearchCoarse-to-fine search is an idea that has ap-peared several times in the literature of com-putational linguistics and related areas.
Thefirst appearance of this idea we are aware of isin Maxwell and Kaplan (1993), where a cover-ing CFG is automatically extracted from a moredetailed unification grammar and used to iden-tify the possible locations of constituents in themore detailed parses of the sentence.
Maxwelland Kaplan use their covering CFG to prune thesearch of their unification grammar parser in es-sentially the same manner as we do here, anddemonstrate significant performance improve-ments by using their coarse-to-fine approach.The basic theory of coarse-to-fine approxima-tions and dynamic programming in a stochasticframework is laid out in Geman and Kochanek(2001).
This paper describes the multileveldynamic programming algorithm needed forcoarse-to-fine analysis (which they apply to de-coding rather than parsing), and show howto perform exact coarse-to-fine computation,rather than the heuristic search we perform here.A paper closely related to ours is Goodman(1997).
In our terminology, Goodman?s parseris a two-stage ctf parser.
The second stage is astandard tree-bank parser while the first stage isa regular-expression approximation of the gram-mar.
Again, the second stage is constrained bythe parses found in the first stage.
Neither stageis smoothed.
The parser of Charniak (2000)is also a two-stage ctf model, where the firststage is a smoothed Markov grammar (it usesup to three previous constituents as context),and the second stage is a lexicalized Markovgrammar with extra annotations about parentsand grandparents.
The second stage exploresall of the constituents not pruned out after thefirst stage.
Related approaches are used in Hall(2004) and Charniak and Johnson (2005).A quite different approach to parsing effi-ciency is taken in Caraballo and Charniak (1998)(and refined in Charniak et al (1998)).
Hereefficiency is gained by using a standard chart-parsing algorithm and pulling constituents offthe agenda according to (an estimate of) theirprobability given the sentence.
This probabilityis computed by estimating Equation 1:p(nki,j | s) =?(nki,j)?
(nki,j)p(s) .
(1)169It must be estimated because during thebottom-up chart-parsing algorithm, the trueoutside probability cannot be computed.
Theresults cited in Caraballo and Charniak (1998)cannot be compared directly to ours, but areroughly in the same equivalence class.
Thosepresented in Charniak et al (1998) are superior,but in Section 5 below we suggest that a com-bination of the techniques could yield better re-sults still.Klein and Manning (2003a) describe efficientA?
for the most likely parse, where pruning isaccomplished by using Equation 1 and a trueupper bound on the outside probability.
Whiletheir maximum is a looser estimate of the out-side probability, it is an admissible heuristic andtogether with an A?
search is guaranteed to findthe best parse first.
One question is if the guar-antee is worth the extra search required by thelooser estimate of the true outside probability.Tsuruoka and Tsujii (2004) explore the frame-work developed in Klein and Manning (2003a),and seek ways to minimize the time requiredby the heap manipulations necessary in thisscheme.
They describe an iterative deepeningalgorithm that does not require a heap.
Theyalso speed computation by precomputing moreaccurate upper bounds on the outside proba-bilities of various kinds of constituents.
Theyare able to reduce by half the number of con-stituents required to find the best parse (com-pared to CKY).Most recently, McDonald et al (2005) haveimplemented a dependency parser with goodaccuracy (it is almost as good at dependencyparsing as Charniak (2000)) and very impres-sive speed (it is about ten times faster thanCollins (1997) and four times faster than Char-niak (2000)).
It achieves its speed in part be-cause it uses the Eisner and Satta (1999) algo-rithm for n3 bilexical parsing, but also becausedependency parsing has a much lower grammarconstant than does standard PCFG parsing ?after all, there are no phrasal constituents toconsider.
The current paper can be thought ofas a way to take the sting out of the grammarconstant for PCFGs by parsing first with veryfew phrasal constituents and adding them onlyLevel: 0 1 2 3S1{S1{S1{S1P???????????????????????????????????????????????????????????????????????????????????HP?????????????????????????????????????S?????????????SVPUCPSQSBARSBARQSINVN?????????????NPNACNXLSTXUCPFRAGMP?????????????????????????????????????A?????????????ADJPQPCONJPADVPINTJPRNPRTP????????????
?PPPRTRRCWHADJPWHADVPWHNPWHPPFigure 1: The levels of nonterminal labelsafter most constituents have been pruned away.3 Multilevel Course-to-fine ParsingWe use as the underlying parsing algorithm areasonably standard CKY parser, modified toallow unary branching rules.The complete nonterminal clustering is givenin Figure 1.
We do not cluster preterminals.These remain fixed at all levels to the standardPenn-tree-bank set Marcus et al (1993).Level-0 makes two distinctions, the root nodeand everybody else.
At level 1 we make onefurther distinction, between phrases that tendto be heads of constituents (NPs, VPs, and Ss)and those that tend to be modifiers (ADJPs,PPs, etc.).
Level-2 has a total of five categories:root, things that are typically headed by nouns,those headed by verbs, things headed by prepo-sitions, and things headed by classical modifiers(adjectives, adverbs, etc.).
Finally, level 3 is the170S1PPPRPHePVBDatePINatPDTtheNNmall..S1HPHPPRPHeHPVBDateMPINatHPDTtheNNmall..S1S_N_PRPHeS_VBDateP_INatN_DTtheNNmall..S1SNPPRPHeVPVBDatePPINatNPDTtheNNmall..Figure 2: A tree represented at levels 0 to 3classical tree-bank set.
As an example, Figure 2shows the parse for the sentence ?He ate at themall.?
at levels 0 to 3.During training we create four grammars, onefor each level of granularity.
So, for example, atlevel 1 the tree-bank ruleS ?NP VP .would be translated into the ruleHP ?HP HP .That is, each constituent type found in ?S ?NPVP .?
is mapped into its generalization at level 1.The probabilities of all rules are computed us-ing maximum likelihood for constituents at thatlevel.The grammar used by the parser can best bedescribed as being influenced by four compo-nents:1. the nonterminals defined at that level ofparsing,2.
the binarization scheme,3.
the generalizations defined over the bina-rization, and4.
extra annotation to improve parsing accu-racy.The first of these has already been covered.
Wediscuss the other three in turn.In anticipation of eventually lexicalizing thegrammar we binarize from the head out.
Forexample, consider the ruleA ?a b c d ewhere c is the head constituent.
We binarizethis as follows:A ?A1 eA1 ?A2 dA2 ?a A3A3 ?b cGrammars induced in this way tend to betoo specific, as the binarization introduce a verylarge number of very specialized phrasal cat-egories (the Ai).
Following common practiceJohnson (1998; Klein and Manning (2003b) weMarkovize by replacing these nonterminals withones that remember less of the immediate rulecontext.
In our version we keep track of only theparent, the head constituent and the constituentimmediately to the right or left, depending onwhich side of the constituent we are processing.With this scheme the above rules now look likethis:A ?Ad,c eAd,c ?Aa,c dAa,c ?a Ab,cAb,c ?b cSo, for example, the rule ?A ?Ad,c e?
wouldhave a high probability if constituents of typeA, with c as their head, often have d followedby e at their end.Lastly, we add parent annotation to phrasalcategories to improve parsing accuracy.
If weassume that in this case we are expanding a rulefor an A used as a child of Q, and a, b, c, d, ande are all phrasal categories, then the above rulesbecome:AQ ?Ad,c eAAd,c ?Aa,c dAAa,c ?aA Ab,cAb,c ?bA cA17110?8 10?7 10?6 10?5 10?4 10?30.00010.0010.010.1Level 0Level 1Level 2Level 3Figure 3: Probability of a gold constituent beingpruned as a function of pruning thresholds forthe first 100 sentences of the development corpusOnce we have parsed at a level, we evaluatethe probability of a constituent p(nki,j | s) ac-cording to the standard inside-outside formulaof Equation 1.
In this equation nki,j is a con-stituent of type k spanning the words i to j, and?(?)
and ?(?)
are the outside and inside proba-bilities of the constituent, respectively.
Becausewe prune at the end each granularity level, wecan evaluate the equation exactly; no approxi-mations are needed (as in, e.g., Charniak et al(1998)).During parsing, instead of building each con-stituent allowed by the grammar, we first testif the probability of the corresponding coarserconstituent (which we have from Equation 1 inthe previous round of parsing) is greater thana threshold.
(The threshold is set empiricallybased upon the development data.)
If it is belowthe threshold, we do not put the constituent inthe chart.
For example, before we can use a NPand a VP to create a S (using the rule above),we would first need to check that the probabilityin the coarser grammar of using the same spanHP and HP to create a HP is above the thresh-old.
We use the standard inside-outside for-mula to calculate this probability (Equation 1).The empirical results below justify our conjec-ture that there are thresholds that allow signifi-cant pruning while leaving the gold constituentsuntouched.10?8 10?7 10?6 10?5 10?4 10?30.0010.010.11Level 0Level 1Level 2Level 3Figure 4: Fraction of incorrect constituents keptas a function of pruning thresholds for the first100 sentences of the development corpus4 ResultsIn all experiments the system is trained on thePenn tree-bank sections 2-21.
Section 23 is usedfor testing and section 24 for development.
Theinput to the parser are the gold-standard partsof speech, not the words.The point of parsing at multiple levels of gran-ularity is to prune the results of rough levels be-fore going on to finer levels.
In particular, it isnecessary for any pruning scheme to retain thetrue (gold-standard WSJ) constituents in theface of the pruning.
To gain an idea of whatis possible, consider Figure 3.
According to thegraph, at the zeroth level of parsing and a thepruning level 10?4 the probability that a goldconstituent is deleted due to pruning is slightlymore than 0.001 (or 0.1%).
At level three it isslightly more that 0.01 (or 1.0%).The companion figure, Figure 4 shows theretention rate of the non-gold (incorrect) con-stituents.
Again, at pruning level 10?4 and pars-ing level 0 we retain about .3 (30%) of the badconstituents (so we pruned 70%), whereas atlevel 3 we retain about .004 (0.4%).
Note thatin the current paper we do not actually pruneat level 3, instead return the Viterbi parse.
Weinclude pruning results here in anticipation offuture work in which level 3 would be a precur-sor to still more fine-grained parsing.As noted in Section 2, there is some (implicit)172Level Constits Constits % PrunedProduced Pruned?106 ?1060 8.82 7.55 86.51 9.18 6.51 70.82 11.2 9.48 84.43 11,8 0 0.0total 40.4 ?
?3-only 392.0 0 0Figure 5: Total constituents pruned at all levelsfor WSJ section 23, sentences of length ?
100debate in the literature on using estimates ofthe outside probability in Equation 1, or insteadcomputing the exact upper bound.
The idea isthat an exact upper bound gives one an admis-sible search heuristic but at a cost, since it is aless accurate estimator of the true outside prob-ability.
(Note that even the upper bound doesnot, in general, keep all of the gold constituents,since a non-perfect model will assign some ofthem low probability.)
As is clear from Figure3, the estimate works very well indeed.On the basis of this graph, we set the lowestallowable constituent probability at ?
5 ?
10?4,?
10?5, and ?
10?4 for levels 0,1, and 2, re-spectively.
No pruning is done at level 3, sincethere is no level 4.
After setting the pruningparameters on the development set we proceedto parse the test set (WSJ section 23).
Figure 5shows the resulting pruning statistics.
The to-tal number of constituents created at level 0, forall sentences combined, is 8.82 ?
106.
Of those7.55 ?
106 (or 86.5%) are pruned before going onto level 1.
At level 1, the 1.3 million left overfrom level 0 expanded to a total of 9.18 ?
106.70.8% of these in turn are pruned, and so forth.The percent pruned at, e.g., level 1 in Figure 3is much higher than that shown here because itconsiders all of the possible level-1 constituents,not just those left unpruned after level 0.There is no pruning at level 3.
There we sim-ply return the Viterbi parse.
We also show thatwith pruning we generate a total of 40.4 ?
106constituents.
For comparison exhaustively pars-ing using the tree-bank grammar yields a totalof 392 ?
106 constituents.
This is the factor-of-10Level Time for Level Running Total0 1598 15981 2570 41682 4303 84713 1527 99983-only 114654 ?Figure 6: Running times in seconds on WSJ sec-tion 23, with and without pruningworkload reduction mentioned in Section 1.There are two points of interest.
The first isthat each level of pruning is worthwhile.
We donot get most of the effect from one or the otherlevel.
The second point is that we get signif-icant pruning at level 0.
The reader may re-member that level 0 distinguishes only betweenthe root node and the rest.
We initially ex-pected that it would be too coarse to distinguishgood from bad constituents at this level, but itproved as useful as the other levels.
The expla-nation is that this level does use the full tree-bank preterminal tags, and in many cases thesealone are sufficient to make certain constituentsvery unlikely.
For example, what is the proba-bility of any constituent of length two or greaterending in a preposition?
The answer is: verylow.
Similarly for constituents of length two orgreater ending in modal verbs, and determiners.Not quite so improbable, but nevertheless lesslikely than most, would be constituents endingin verbs, or ending just short of the end of thesentence.Figure 6 shows how much time is spent at eachlevel of the algorithm, along with a running to-tal of the time spent to that point.
(This is forall sentences in the test set, length ?
100.)
Thenumber for the unpruned parser is again aboutten times that for the pruned version, but thenumber for the standard CKY version is prob-ably too high.
Because our CKY implementa-tion is quite slow, we ran the unpruned versionon many machines and summed the results.
Inall likelihood at least some of these machineswere overloaded, a fact that our local job dis-tributer would not notice.
We suspect that thereal number is significantly lower, though still173No pruning 77.9With pruning 77.9Klein and Manning (2003b) 77.4Figure 7: Labeled precision/recall f-measure,WSJ section 23, all sentences of length ?
100much higher than the pruned version.Finally Figure 7 shows that our pruning is ac-complished without loss of accuracy.
The resultswith pruning include four sentences that did notreceive any parses at all.
These sentences re-ceived zeros for both precision and recall andpresumably lowered the results somewhat.
Weallowed ourselves to look at the first of these,which turned out to contain the phrase:(NP ... (INTJ (UH oh) (UH yes)) ...)The training data does not include interjectionsconsisting of two ?UH?s, and thus a gold parsecannot be constructed.
Note that a differentbinarization scheme (e.g.
the one used in Kleinand Manning (2003b) would have smoothed overthis problem.
In our case the unpruned versionis able to patch together a lot of very unlikelyconstituents to produce a parse, but not a verygood one.
Thus we attribute the problem not topruning, but to binarization.We also show the results for the most similarKlein and Manning (2003b) experiment.
Ourresults are slightly better.
We attribute the dif-ference to the fact that we have the gold tagsand they do not, but their binarization schemedoes not run into the problems that we encoun-tered.5 Conclusion and Future ResearchWe have presented a novel parsing algorithmbased upon the coarse-to-fine processing model.Several aspects of the method recommend it.First, unlike methods that depend on best-firstsearch, the method is ?holistic?
in its evalua-tion of constituents.
For example, consider theimpact of parent labeling.
It has been repeat-edly shown to improve parsing accuracy (John-son, 1998; Charniak, 2000; Klein and Manning,2003b), but it is difficult if not impossible tointegrate with best-first search in bottom-upchart-parsing (as in Charniak et al (1998)).
Thereason is that when working bottom up it is diffi-cult to determine if, say, ssbar is any more or lesslikely than ss, as the evidence, working bottomup, is negligible.
Since our method computesthe exact outside probability of constituents (al-beit at a coarser level) all of the top down in-formation is available to the system.
Or again,another very useful feature in English parsingis the knowledge that a constituent ends at theright boundary (minus punctuation) of a string.This can be included only in an ad-hoc way whenworking bottom up, but could be easily addedhere.Many aspects of the current implementationthat are far from optimal.
It seems clear tous that extracting the maximum benefit fromour pruning would involve taking the unprunedconstituents and specifying them in all possibleways allowed by the next level of granularity.What we actually did is to propose all possi-ble constituents at the next level, and immedi-ately rule out those lacking a corresponding con-stituent remaining at the previous level.
Thiswas dictated by ease of implementation.
Beforeusing mlctf parsing in a production parser, theother method should be evaluated to see if ourintuitions of greater efficiency are correct.It is also possible to combine mlctf parsingwith queue reordering methods.
The best-firstsearch method of Charniak et al (1998) esti-mates Equation 1.
Working bottom up, estimat-ing the inside probability is easy (we just sumthe probability of all the trees found to buildthis constituent).
All the cleverness goes intoestimating the outside probability.
Quite clearlythe current method could be used to provide amore accurate estimate of the outside probabil-ity, namely the outside probability at the coarserlevel of granularity.There is one more future-research topic to addbefore we stop, possibly the most interesting ofall.
The particular tree of coarser to finer con-stituents that governs our mlctf algorithm (Fig-ure 1) was created by hand after about 15 min-utes of reflection and survives, except for typos,with only two modifications.
There is no rea-174son to think it is anywhere close to optimal.
Itshould be possible to define ?optimal?
formallyand search for the best mlctf constituent tree.This would be a clustering problem, and, for-tunately, one thing statistical NLP researchersknow how to do is cluster.AcknowledgmentsThis paper is the class project for ComputerScience 241 at Brown University in fall 2005.The faculty involved were supported in partby DARPA GALE contract HR0011-06-2-0001.The graduate students were mostly supportedby Brown University fellowships.
The under-graduates were mostly supported by their par-ents.
Our thanks to all.ReferencesSharon Caraballo and Eugene Charniak.
1998.
Fig-ures of merit for best-first probabalistic parsing.Computational Linguistics, 24(2):275?298.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 2005 Meeting ofthe Association for Computational Linguistics.Eugene Charniak, Sharon Goldwater, and MarkJohnson.
1998.
Edge-based best-first chart pars-ing.
In Proceedings of the Sixth Workshop onVery Large Corpora, pages 127?133.
Morgan Kauf-mann.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the North Amer-ican Chapter of the Association for ComputationalLinguistics, pages 132?139.Michael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Proceedings ofthe 35th Annual Meeting of the Association forComputational Linguistics, San Francisco.
Mor-gan Kaufmann.Jason Eisner and Giorgio Satta.
1999.
Efficient pars-ing for bilexical context-free grammars and headautomaton grammars.
In Proceedings of the 37thAnnual Meeting of the Association for Computa-tional Linguistics, pages 457?464.Stuart Geman and Kevin Kochanek.
2001.
Dy-namic programming and the representation ofsoft-decodable codes.
IEEE Transactions on In-formation Theory, 47:549?568.Joshua Goodman.
1997.
Global thresholding andmultiple-pass parsing.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 1997).Keith Hall and Mark Johnson.
2004.
Attention shift-ing for parsing speech.
In The Proceedings of the42th Annual Meeting of the Association for Com-putational Linguistics, pages 40?46.Keith Hall.
2004.
Best-first Word-lattice Pars-ing: Techniques for Integrated Syntactic LanguageModeling.
Ph.D. thesis, Brown University.Mark Johnson and Eugene Charniak.
2004.
A TAG-based noisy-channel model of speech repairs.
InProceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics, pages 33?39.Mark Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguistics,24(4):613?632.Dan Klein and Chris Manning.
2003a.
A* parsing:Fast exact viterbi parse selection.
In Proceedingsof HLT-NAACL?03.Dan Klein and Christopher Manning.
2003b.
Accu-rate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics.Michell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313?330.John T. Maxwell and Ronald M. Kaplan.
1993.The interface between phrasal and functional con-straints.
Computational Linguistics, 19(4):571?590.Ryan McDonald, Toby Crammer, and FernandoPereira.
2005.
Online large margin training ofdependency parsers.
In Proceedings of the 43rdMeeting of the Association for Computational Lin-guistics.Yoshimasa Tsuruoka and Jun?ichi Tsujii.
2004.
It-erative cky parsing for probabilistic context-freegrammars.
In International Joint Conference onNatural-Language Processing.175
