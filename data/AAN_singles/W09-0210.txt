Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 74?82,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsUnsupervised and Constrained Dirichlet Process Mixture Models for VerbClusteringAndreas VlachosComputer LaboratoryUniversity of CambridgeCambridge CB3 0FD, UKav308l@cl.cam.ac.ukAnna KorhonenComputer LaboratoryUniversity of CambridgeCambridge CB3 0FD, UKalk23@cl.cam.ac.ukZoubin GhahramaniDepartment of EngineeringUniversity of CambridgeCambridge CB2 1PZ, UKzoubin@eng.cam.ac.ukAbstractIn this work, we apply Dirichlet ProcessMixture Models (DPMMs) to a learningtask in natural language processing (NLP):lexical-semantic verb clustering.
We thor-oughly evaluate a method of guiding DP-MMs towards a particular clustering so-lution using pairwise constraints.
Thequantitative and qualitative evaluation per-formed highlights the benefits of bothstandard and constrained DPMMs com-pared to previously used approaches.
Inaddition, it sheds light on the use of evalu-ation measures and their practical applica-tion.1 IntroductionBayesian non-parametric models have received alot of attention in the machine learning commu-nity.
These models have the attractive propertythat the number of components used to modelthe data is not fixed in advance but is actuallydetermined by the model and the data.
Thisproperty is particularly interesting for NLP wheremany tasks are aimed at discovering novel, pre-viously unknown information in corpora.
Recentwork has applied Bayesian non-parametric mod-els to anaphora resolution (Haghighi and Klein,2007), lexical acquisition (Goldwater, 2007) andlanguage modeling (Teh, 2006) with good results.Recently, Vlachos et al (2008) applied the ba-sic models of this class, Dirichlet Process Mix-ture Models (DPMMs) (Neal, 2000), to a typicallearning task in NLP: lexical-semantic verb clus-tering.
The task involves discovering classes ofverbs similar in terms of their syntactic-semanticproperties (e.g.
MOTION class for travel, walk,run, etc.).
Such classes can provide importantsupport for other NLP tasks, such as word sensedisambiguation, parsing and semantic role label-ing (Dang, 2004; Swier and Stevenson, 2004).Although some fixed classifications are available(e.g.
VerbNet (Kipper-Schuler, 2005)) these arenot comprehensive and are inadequate for specificdomains (Korhonen et al, 2006b).Unlike the clustering algorithms applied to thistask before, DPMMs do not require the number ofclusters as input.
This is important because evenif the number of classes in a particular task wasknown (e.g.
in the context of a carefully controlledexperiment), a particular dataset may not containinstances for all the classes.
Moreover, each classis not necessarily contained in one cluster exclu-sively, since the target classes are defined manu-ally without taking into account the feature rep-resentation used.
The fact that DPMMs do notrequire the number of target clusters in advance,renders them promising for the many NLP taskswhere clustering is used for learning purposes.While the results of Vlachos et al (2008) arepromising, the use of a clustering approach whichdiscovers the number of clusters in data presentsa new challenge to existing evaluation measures.In this work, we investigate optimal evaluationfor such approaches, using the dataset and the ba-sic method of Vlachos et al as a starting point.We review the applicability of existing evalua-tion measures and propose a modified version ofthe newly introduced V-measure (Rosenberg andHirschberg, 2007).
We complement the quanti-tative evaluation with thorough qualitative assess-ment, for which we introduce a method to summa-rize samples obtained from a clustering algorithm.In preliminary work by Vlachos et al (2008),a constrained version of DPMMs which takes ad-vantage of must-link and cannot-link pairwise con-straints was introduced.
It was demonstrated howsuch constraines can guide the clustering solutiontowards some prior intuition or considerations rel-evant to the specific NLP application in mind.
Weexplain the inference algorithm for the constrainedDPMM in greater detail and evaluate quantita-74tively the contribution of each constraint type ofindependently, complementing it with qualitativeanalysis.
The latter demonstrates how the pairwiseconstraints added affects instances beyond thoseinvolved directly.
Finally, we discuss how the un-supervised and the constrained version of DPMMscan be used in a real-world setup.The results from our comprehensive evaluationshow that both versions of DPMMs are capableof learning novel information not in the gold stan-dard, and that the constrained version is more ac-curate than a previous verb clustering approachwhich requires setting the number of clusters inadvance and is therefore less realistic.2 Unsupervised clustering with DPMMsWith DPMMs, as with other Bayesian non-parametric models, the number of mixture compo-nents is not fixed in advance, but is determined bythe model and the data.
The parameters of eachcomponent are generated by a Dirichlet Process(DP) which can be seen as a distribution over theparameters of other distributions.
In turn, each in-stance is generated by the chosen component giventhe parameters defined in the previous step:G|?,G0 ?
DP (?,G0)?i|G ?
G (1)xi|?i ?
F (?i)In Eq.
1, G0 and G are probability distributionsover the component parameters (?
), and ?
> 0 isthe concentration parameter which determines thevariance of the Dirichlet process.
We can thinkof G as a randomly drawn probability distributionwith meanG0.
Intuitively, the larger ?
is, the moresimilar G will be to G0.
Instance xi is generatedby distribution F , parameterized by ?i.
The graph-ical model is depicted in Figure 1.The prior probability of assigning an instanceto a particular component is proportionate to thenumber of instances already assigned to it (n?i,z).In other words, DPMMs exhibit the ?rich getricher?
property.
In addition, the probability thata new cluster is created is dependent on the con-centration parameter ?.
A popular metaphor to de-scribe DPMMs which exhibits an equivalent clus-tering property is the Chinese Restaurant Process(CRP).
Customers (instances) arrive at a Chineserestaurant which has an infinite number of tables(components).
Each customer sits at one of the ta-bles that is either occupied or vacant with populartables attracting more customers.Figure 1: Graphical representation of DPMMs.In this work, the distribution used to model thecomponents is the multinomial and the prior usedis the Dirichlet distribution (F and G0 in Eq.
1).The conjugacy between them allows for the ana-lytic integration over the component parameters.Following Neal (2000), the component assign-ments zi are sampled using the following scheme:P (zi = z|z?i, xi) ?p(zi = z|z?i)DirM(xi|zi = z, x?i,z, ?)
(2)In Eq.
2DirM is the Dirichlet-Multinomial distri-bution, ?
are the parameters of the Dirichlet priorG0 and x?i,z are the instances assigned already tocomponent z (none if we are sampling the prob-ability of assignment to a new component).
Thissampling scheme is possible due to the fact that theinstances in the model are exchangeable, i.e.
theorder in which they are generated is not relevant.In terms of the CRP metaphor, we consider eachinstance xi as the last customer to arrive and hechooses to sit together with other customers at anexisting table or to sit at a new table.
FollowingNavarro et al (2006) who used the same model toanalyze individual differences, we sample the con-centration parameter ?
using the inverse Gammadistribution as a prior.3 Evaluation measuresThe evaluation of unsupervised clustering againsta gold standard is not straightforward because theclusters found are not explicitly labelled.
Formallydefined, an unsupervised clustering algorithm par-titions a set of instances X = {xi|i = 1, ..., N}into a set of clusters K = {kj |j = 1, ..., |K|}.The standard approach to evaluate the quality ofthe clusters is to use an external gold standard inwhich the instances are partitioned into a set of75classes C = {cl|l = 1, ..., |C|}.
Given this, thegoal is to find a partitioning of the instances Kthat is as close as possible to the gold standard C.Most work on verb clustering has used the F-measure or the Rand Index (RI) (Rand, 1971)for evaluation, which rely on counting pairwiselinks between instances.
However, Rosenberg andHirschberg (2007) pointed out that F-measure as-sumes (the missing) mapping between cl and kj .In practice, RI values concentrate in a small inter-val near 100% (Meila?, 2007).Rosenberg & Hirschberg (2007) proposed aninformation-theoretic metric: V-measure.
V-measure is the harmonic mean of homogeneityand completeness which evaluate the quality of theclustering in a complementary way.
Homogeneityassesses the degree to which each cluster containsinstances from a single class of C. This is com-puted as the conditional entropy of the class dis-tribution of the gold standard given the clusteringdiscovered by the algorithm, H(C|K), normal-ized by the entropy of the class distribution in thegold standard, H(C).
Completeness assesses thedegree to which each class is contained in a singlecluster.
This is computed as the conditional en-tropy of the cluster distribution discovered by thealgorithm given the class, H(K|C), normalizedby the entropy of the cluster distribution, H(K).In both cases, we subtract the resulting ratios from1 to associate higher scores with better solutions:h = 1?H(C|K)H(C)c = 1?H(K|C)H(K)V?
=(1 + ?)
?
h ?
c(?
?
h) + c(3)The parameter ?
in Eq.
3 regulates the balancebetween homogeneity and completeness.
Rosen-berg & Hirschberg set it to 1 in order to obtain theharmonic mean of these qualities.
They also notethat V-measure favors clustering solutions with alarge number of clusters (large |K|), since such so-lutions can achieve very high homogeneity whilemaintaining reasonable completeness.
This ef-fect is more prominent when a dataset includes asmall number of instaces for gold standard classes.While increasing |K| does not guarantee an in-crease in V-measure (splitting homogeneous clus-ters would reduce completeness without improv-ing homogeneity), it is easier to achieve higherscores when more clusters are produced.Another relevant measure is the Variation of In-formation (VI) (Meila?, 2007).
Like V-measure,it assesses homogeneity and completeness usingthe quantitiesH(C|K) andH(K|C) respectively,however it simply adds them up to obtain a finalresult (higher scores are worse).
It is also a metric,i.e.
VI scores can be added, subtracted, etc, sincethe quantities involved are measured in bits.
How-ever, it can be observed that if |C| and |K| are verydifferent then the terms H(C|K) and H(K|C)will not necessarily be in the same range.
In par-ticular, if |K|  |C| then H(K|C) (and V I) willbe low.
In addition, VI scores are not normalizedand therefore their interpretation is difficult.Both V-measure and VI have important advan-tages over RI and F-measure: they do not assumea mapping between classes and clusters and theirscores depend only on the relative sizes of the clus-ters.
However, V-measure and VI can be mislead-ing if the number of clusters found (|K|) is sub-stantially different than the number of gold stan-dard classes (|C|).
In order to ameliorate this, wesuggest to take advantage of the ?
parameter inEq.
3 in order to balance homogeneity and com-pleteness.
More specifically, setting ?
= |K|/|C|assigns more weight to completeness than to ho-mogeneity in case |K| > |C| since the former isharder to achieve and the latter is easier when theclustering solution has more clusters than the goldstandard has classes.
The opposite occurs when|K| < |C|.
In case |K| = |C| the score is thesame as the original V-measure.
Achieving 100%score according to any of these measures requirescorrect prediction of the number of clusters.In this work, we evaluate our results using thethree measures described above (V-measure, VI,V-beta).
We complement this evaluation withqualitative evaluation which assesses the poten-tial of DPMMs to discover novel information thatmight not be included in the gold standard.4 ExperimentsTo perform lexical-semantic verb clustering weused the dataset of Sun et al (2008).
It contains204 verbs belonging to 17 fine-grained classes inLevin?s (1993) taxonomy so that each class con-tains 12 verbs.
The classes and their verbs wereselected randomly.
The features for each verb areits subcategorization frames (SCFs) and associ-ated frequencies in corpus data, which capture the76DPMM Sun et alno.
of clusters 37.79 17homogeneity 60.23% 57.57%completeness 55.82% 60.19%V-measure 57.94% 58.85%V-beta 57.11% 58.85%VI (bits) 3.5746 3.3598Table 1: Clustering performances.syntactic context in which the verb occurs.
SCFswere extracted from the publicly available VALEXlexicon (Korhonen et al, 2006a).
VALEX was ac-quired automatically using a domain-independentstatistical parsing toolkit, RASP (Briscoe and Car-roll, 2002), and a classifier which identifies verbalSCFs.
As a consequence, it includes some noisedue to standard text processing and parsing errorsand due to the subtlety of argument-adjunct dis-tinction.
In our experiments, we used the SCFsobtained from VALEX1, parameterized for theprepositional frame, which had the best perfor-mance in the experiments of Sun et al (2008).The feature sets based on verbal SCFs are verysparse and the counts vary over a large range ofvalues.
This can be problematic for generativemodels like DPMMs, since a few dominant fea-tures can mislead the model.
To reduce the spar-sity, we applied non-negative matrix factorization(NMF) (Lin, 2007) which decomposes the datasetin two dense matrices with non-negative values.
Ithas proven useful in a variety of tasks, e.g.
infor-mation retrieval (Xu et al, 2003) and image pro-cessing (Lee and Seung, 1999).We use a symmetric Dirichlet prior with param-eters of 1 (?
in Equation 2).
The number of di-mensions obtained using NMF was 35.
We runthe Gibbs sampler 5 times, using 100 iterations forburn-in and draw 20 samples from each run with5 iterations lag between samples.
Table 1 showsthe average performances.
The DPMM discov-ers 37.79 verb clusters on average with its perfor-mance ranging between 53% and 58% dependingon the evaluation measure used.
Homogeneity is4.5% higher than completeness, which is expectedsince the number of classes in the gold standard is17.
The fact that the DPMM discovers more thantwice the number of classes is reflected in the dif-ference between the V-measure and V-beta, the lat-ter being lower.
In the same table, we show the re-sults of Sun et al (2008), who used pairwise clus-tering (PC) (Puzicha et al, 2000) which involvesdetermining the number of clusters in advance.The performance of the DPMM is 1%-3% lowerthan that of Sun et al As expected, the differ-ence in V-measure is smaller since the DPMMdiscovers a larger number of clusters, while forVI it is larger.
The slightly better performanceof PC can be attributed to two factors.
First,the (correct) number of clusters is given as in-put to the PC algorithm and not discovered likeby the DPMM.
Secondly, PC uses the similaritiesbetween the instances to perform the clustering,while the DPMM attempts to find the parametersof the process that generated the data, which is adifferent and typically a harder task.
In addition,the DPMM has two clear advantages which we il-lustrate in the following sections: it can be used todiscover novel information and it can be modifiedto incorporate intuitive human supervision.5 Qualitative evaluationThe gold standard employed in this work (Sun etal., 2008) is not fully accurate or comprehensive.It classifies verbs according to their predominantsenses in the fairly small SemCor data.
Individ-ual classes are relatively coarse-grained in termsof syntactic-semantic analysis1 and they capturesome of the meaning components only.
In addi-tion, the gold standard does not capture the se-mantic relatedness of distinct classes.
In fact, themain goal of clustering is to improve such exist-ing classifications with novel information and tocreate classifications for new domains.
We per-formed qualitative analysis to investigate the ex-tent to which the DPMM meets this goal.We prepared the data for qualitative analysis asfollows: We represented each clustering sampleas a linking matrix between the instances of thedataset and measured the frequency of each pairof instances occurring in the same cluster.
Weconstructed a partial clustering of the instancesusing only those links that occur with frequencyhigher than a threshold prob link.
Singleton clus-ters were formed by considering instances thatare not linked with any other instances more fre-quently than a threshold prob single.
The lowerthe prob link threshold, the larger the clusters willbe, since more instances get linked.
Note that in-cluding more links in the solution can either in-1Many original Levin classes have been manually refinedin VerbNet.77crease the number of clusters when instances in-volved were not linked otherwise, or decrease itwhen linking instances that already belong to otherclusters.
The higher the prob single threshold,the more instances will end up as singletons.
Byadjusting these two thresholds we can affect thecoverage of the analysis.
This approach was cho-sen because it enables to conduct qualitative analy-sis of data relevant to most clustering samples andirrespective of individual samples.
It can also beuseful in order to use the output of the clusteringalgorithm as a component in a pipeline which re-quires a single result rather than multiple samples.Using this method, we generated data sets forqualitative analysis using 4 sets of values forprob link and prob single, respectively: (99%,1%), (95%, 5%), (90%, 10%) and (85%, 15%).Table 1 shows the number of a) verbs, b) clusters(2 or more instances) and c) singletons in eachresulting data set, along with the percentage andsize of the clusters which represent 1, 2, or mul-tiple gold standard classes.
As expected, higherthreshold values produce high precision clustersfor a smaller set of verbs (e.g.
(99%,1%) pro-duces 5 singletons and assigns 70 verbs to 20 clus-ters, 55% of which represent a single gold stan-dard class), while less extreme threshold valuesyield higher recall clusters for a larger set of verbs(e.g.
(85%,15%) produces 10 singletons and as-signs 140 verbs to 25 clusters, 20% of which con-tain verbs from several gold standard classes).We conducted the qualitative analysis by com-paring the four data sets against the gold standard,SCF distributions, and WordNet (Fellbaum, 1998)senses for each test verb.
We first analysed the5-10 singletons in data sets and discovered thatwhile 3 of the verbs resist classification becauseof syntactic idiosyncrasy (e.g.
unite takes intransi-tive SCFs with frequency higher than other mem-bers of class 22.2), the majority of them (7) endup in singletons for valid semantic reasons: takingseveral frequent WordNet senses they are ?too pol-ysemous?
to be realistically clustered according totheir predominant sense (e.g.
get and look).We then examined the clusters, and discoveredthat even in the data set created with the lowestprob link threshold of 85%, almost half of the?errors?
are in fact novel semantic patterns discov-ered by clustering.
Many of these could be newsub-classes of existing gold standard classes.
Forexample, looking at the 13 high accuracy clusterswhich correspond to a single gold standard classeach, they only represent 9 gold standard classesbecause as many as 4 classes been divided intotwo clusters, suggesting that the gold standard istoo coarse-grained.
Interestingly, each such sub-division seems semantically justified (e.g.
the 11.1PUT verbs bury and immerse appear in a differ-ent cluster than the semantically slightly differentplace and situate).In addition, the DPMM discovers semanticallysimilar gold standard classes.
For example, in thedata set created with the prob link threshold of99%, 6 of the clusters include members from 2different gold standard classes.
2 occur due tosyntactic idiosyncrasy, but the majority (4) oc-cur because of true semantic relatedness (e.g.
theclustering relates 22.2 AMALGAMATE and 36.1CORRESPOND classes which share similar mean-ing components).
Similarly, in the data set pro-duced by the prob link threshold of 85%, oneof the largest clusters includes 26 verbs from 5gold standard classes.
The majority of them be-long to 3 classes which are related by the meaningcomponent of ?motion?
: 43.1 LIGHT EMISSION,47.3 MODES OF BEING INVOLVING MOTION, and51.3.2 RUN verbs:?
class 22.2 AMALGAMATE: overlap?
class 36.1 CORRESPOND: banter, concur, dissent, hag-gle?
class 43.1 LIGHT EMISSION: flare, flicker, gleam, glis-ten, glow, shine, sparkle?
class 47.3 MODES OF BEING INVOLVING MOTION:falter, flutter, quiver, swirl, wobble?
class 51.3.2 RUN: fly, gallop, glide, jog, march, stroll,swim, travel, trotThus many of the singletons and the clustersin the different outputs capture finer or coarser-grained lexical-semantic differences than thosecaptured in the gold standard.
It is encouragingthat this happens despite us focussing on a rela-tively small set of 204 verbs and 17 classes only.6 Constrained DPMMsWhile the ability to discover novel information isattractive in NLP, in many cases it is also desir-able to influence the solution with respect to someprior intuition or consideration relevant to the ap-plication in mind.
For example, while discover-ing finer-grained classes than those included in thegold standard is useful for some applications, oth-ers may benefit from a coarser clustering or a clus-tering that reveals a specific aspect of the dataset.78% and size of clusters containingTHR verbs clusters singletons 1 class 2 classes multiple classes99%,1% 70 20 5 55% (3.0) 30% (2.8) 15% (4.5)95%,5% 104 25 9 40% (3.7) 44% (2.8) 16% (6.8)90%,10% 128 28 9 46% (3.4) 39% (2.5) 14% (11.0)85%,15% 140 25 10 44% (3.7) 28% (3.3) 20% (13.0)Table 2: An overview of the data sets generated for qualitative analysisPreliminary work by Vlachos et al (2008) intro-duced a constrained version of DPMMs that en-ables human supervision to guide the clusteringsolution when needed.
We model the human su-pervision as pairwise constraints over instances,following Wagstaff & Cardie (2000): given a pairof instances, they are either linked together (must-link) or not (cannot-link).
For example, chargeand run should form a must-link if the aim isto cluster 51.3 MOTION verbs together, but theyshould form a cannot-link if we are interested in54.5 BILL verbs.
In the discussion and the experi-ments that follow, we assume that all links are con-sistent with each other.
This information can beobtained by asking human experts to label links,or by extracting it from extant lexical resources.Specifying the relations between the instances re-sults in a partial labeling of the instances.
Suchlabeling is likely to be re-usable, since relationsbetween the instances are likely to be useful for awider range of tasks which might not have identi-cal labels but could still have similar relations.In order to incorporate the constraints in theDPMM, we modify the underlying generative pro-cess to take them into account.
In particular must-linked instances are generated by the same com-ponent and cannot-linked instances always by dif-ferent ones.
In terms of the CRP metaphor, cus-tomers connected with must-links arrive at therestaurant together and choose a table jointly, re-specting their cannot-links with other customers.They get seated at the same table successively oneafter the other.
Customers without must-links withothers choose tables avoiding their cannot-links.In order to sample the component assignmentsaccording to this model, we restrict the Gibbs sam-pler to take them into account using the samplingscheme of Fig.
2.
First we identify linked-groupsof instances, taking into account transitivity2.
Wethen sample the component assignments only fromdistributions that respect the links provided.
More2If A is linked to B and B to C, then A is linked to C.specifically, for each instance that does not belongto a linked-group, we restrict the sampler to choosecomponents that do not contain instances cannot-linked with it.
For instances in a linked-group, wesample their assignment jointly, again taking intoaccount their cannot-links.
This is performed byadding each instance of the linked-group succes-sively to the same component.
In Fig.
2, Ci are thecannot-links for instance(s) i, ` are the indices ofthe instances in a linked-group, and z<i and x<iare the assignments and the instances of a linked-group that have been assigned to a component be-fore instance i.Input: data X , must-linksM, cannot-links Clinked groups = find linked groups(X ,M)Initialize Z according toM, Cfor i not in linked groupsfor z = 1 to |Z|+ 1if x?i,z ?
Ci = ?P (zi = z|z?i, xi) (Eq.
2)elseP (zi = z|z?i, xi) = 0Sample from P (zi)for ` in linked groupsfor z = 1 to |Z|+ 1if x?`,z ?
C` = ?Set P (z` = z|z?`, x`) = 1for i in `P (z`= z|z?`, x`)?
=P (zi = z|z?`, x?`,z, z<i, x<i)elseP (z` = z|z?`, x`) = 0Sample from P (z`)Figure 2: Gibbs sampler incorporating must-linksand cannot-links.7 Experiments using constraintsTo investigate the impact of pairwise constraintson clustering by the DPMM, we conduct exper-79iments in which the links are sampled randomlyfrom the gold standard.
The number of links var-ied from 10 to 50 and the random choice was re-peated 5 times without checking for redundancydue to transitivity.
All the other experimental set-tings are identical to those in Section 4.
Follow-ing Wagstaff & Cardie (2000), in Table 3 we showthe impact of each link type independently (la-beled ?must?
and ?cannot?
accordingly), as wellas when mixed in equal proportions (?mix?
).Adding randomly selected pairwise links is ben-eficial.
In particular, must-links improve the clus-tering rapidly.
Incorporating 50 must-links im-proves the performance by 7-8% according to theevaluation measures.
In addition, it reduces theaverage number of clusters by approximately 4.The cannot-links are rather ineffective, which isexpected as the clustering discovered by the un-supervised DPMM is more fine-grained than thegold standard.
For the same reason, it is morelikely that the randomly selected cannot-links arealready discovered by the DPMM and are thus re-dundant.
Wagstaff & Cardie also noted that theimpact of the two types of links tends to varyacross data sets.
Nevertheless, a minor improve-ment is observed in terms of homogeneity.
Thebalanced mix improves the performance, but lessrapidly than the must-links.In order to assess how the links added help theDPMM learn other links we use the ConstrainedRand Index (CRI), which is a modification of theRand Index that takes into account only the pair-wise decisions that are not dictated by the con-straints added (Wagstaff and Cardie, 2000; Kleinet al, 2002).
We evaluate the constrained DPMMwith CRI (Table 3, bottom right graph) and our re-sults show that the improvements obtained usingpairwise constraints are due to learning links be-yond the ones enforced.In a real-world setting, obtaining the mixed setof links is equivalent to asking a human expert togive examples of verbs that should be clustered to-gether or not.
Such information could be extractedfrom a lexical resource (e.g.
ontology).
Alterna-tively, the DPMM could be run without any con-straints first and if a human expert judges the clus-tering too coarse (or fine) then cannot-links (ormust-links) could help, since they can adapt theclustering rapidly.
When 20 randomly selectedmust-links are integrated, the DPMM reaches orexceeds the performance of PC used by Sun etal.
(2008) according to all the evaluation mea-sures.
We also argue that it is more realistic toguide the clustering algorithm using pairwise con-straints than by defining the number of clusters inadvance.
Instead of using pairwise constraints toaffect the clustering solution, one could alter theparameters for the Dirichlet prior G0 (Eq.
1) orexperiment with varying concentration parametervalues.
However, it is difficult to predict in ad-vance the exact effect such changes would have inthe solution discovered.Finally, we conducted qualitative analysis of thesamples obtained constraining the DPMM with 10randomly selected must-links.
We first preparedthe data according to the method described in Sec-tion 5, using prob link and prob single thresh-olds of 99% and 1% respectively.
This resulted in26 clusters and one singleton for 79 verbs.
Recallthat without constraining the DPMM these thresh-olds produced 20 clusters and 5 singletons for 70verbs.
49 verbs are shared in both outputs, whilethe average cluster size is similar.The resulting clusters are highly accurate.
Asmany as 16 (i.e.
62%) of them represent a sin-gle gold standard class, 7 of which contain (only)the pairs of must-linked verbs.
Interestingly, only11 out of 17 gold standard classes are exempli-fied among the 16 clusters, with 5 classes sub-divided into finer-grained classes.
Each of thesesub-divisions seems semantically fully motivated(e.g.
30.3 PEER verbs were subdivided so thatpeep and peek were assigned to a different clusterthan the semantically different gaze, glance andstare) and 4 of them can be directly attributed tothe use of must-links.From the 6 clusters that contained membersfrom two different gold standard classes, the ma-jority (5) make sense as well.
3 of these containmembers of must-link pairs together with verbsfrom semantically related classes (e.g.
37.7 SAYand 40.2 NONVERBAL EXPRESSION classes).
3 ofthe clusters that contain members of several goldstandard classes include must-link pairs as well.In two cases must-links have helped to bring to-gether verbs which belong to the same class (e.g.the members of the must-link pair broaden-freezewhich represent 45.4 CHANGE OF STATE class ap-pear now in the same cluster with other class mem-bers dampen, soften and sharpen).
Thus, DP-MMs prove useful in learning novel informationtaking into account pairwise constraints.
Only 4806061626364656667680  10  20  30  40  50Homogeneitymixmustcannot5556575859606162630  10  20  30  40  50Completenessmixmustcannot575859606162636465660  10  20  30  40  50V-measuremixmustcannot565758596061626364650  10  20  30  40  50V-betamixmustcannot2.933.13.23.33.43.53.63.70  10  20  30  40  50VImixmustcannot9090.290.490.690.89191.291.491.691.80  10  20  30  40  50CRImixmustcannotTable 3: Performance of constrained DPMMs incorporating pairwise links.(i.e.
15%) of the clusters in the output examinedare not meaningful (mostly due to the mismatchbetween the syntax and semantics of verbs).8 Related workPrevious work on unsupervised verb clusteringused algorithms that require the number of clus-ters as input e.g.
PC, Information Bottleneck (Ko-rhonen et al, 2006b) and spectral clustering (Brewand Schulte im Walde, 2002).
In terms of apply-ing non-parametric Bayesian approaches to NLP,Haghighi and Klein (2007) evaluated the cluster-ing properties of DPMMs by performing anaphoraresolution with good results.There is a large body of work on semi-supervised learning (SSL), but relatively littlework has been done on incorporating some formof supervision in clustering.
It is important to notethat the pairwise links used in this work consti-tute a weak form of supervision since they cannotbe used to infer class labels which are required forSSL.
However, the opposite can be done.
Wagstaff& Cardie (2000) employed must-links and cannot-links to constrain the COBWEB algorithm, whileKlein et al (2002) applied them to complete-linkhierarchical agglomerative clustering.
The latteralso studied how the added links affect instancesnot directly involved in them.It can be argued that one could use clusteringalgorithms that require the number of clusters tobe known in advance to discover interesting sub-classes such as those discovered by the DPMMs.However, this would normally require multipleruns and manual inspection of the results, whileDPMMs discover them automatically.
Apart fromthe fact that fixing the number of clusters in ad-vance restricts the discovery of novel informationin the data, such algorithms cannot take full ad-vantage of the pairwise constraints, since the latterare likely to change the number of clusters.9 Conclusions - Future WorkIn this work, following Vlachos et al (2008) weexplored the application of DPMMs to the task ofverb clustering.
We modified V-measure (Rosen-berg and Hirschberg, 2007) to deal more appro-priately with the varying number of clusters dis-covered by DPMMs and presented a method ofagregating the generated samples which allows forqualitative evaluation.
The quantitative and qual-itative evaluation demonstrated that they achieveperformance comparable with that of previouswork and in addition discover novel information inthe data.
Furthermore, we evaluated the incorpo-ration of constraints to guide the DPMM obtainingpromising results and we discussed their applica-tion in a real-world setup.The results obtained encourage the applicationof DPMMs and non-parametric Bayesian methodsto other NLP tasks.
We plan to extend our ex-periments to larger datasets and further domains.While the improvements achieved using randomlyselected pairwise constraints were promising, anactive constraint selection scheme as in Klein etal.
(2002) could increase their impact.
Finally,an extrinsic evaluation of the clustering providedby DPMMs in the context of an NLP applicationwould be informative on their practical potential.81AcknowledgmentsWe are grateful to Diarmuid O?
Se?aghdha and Jur-gen Van Gael for helpful discussions.ReferencesChris Brew and Sabine Schulte im Walde.
2002.
Spec-tral Clustering for German Verbs.
In Proceedings ofthe 2002 Conference on Empirical Methods in Nat-ural Language Processing, pages 117?124.Ted Briscoe and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceedingsof the 3rd International Conference on LanguageResources and Evaluation, pages 1499?1504.Hoa Trang Dang.
2004.
Investigations into the roleof lexical semantics in word sense disambiguation.Ph.D.
thesis, University of Pennsylvania, Philadel-phia, PA, USA.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, andCommunication).
The MIT Press.Sharon J. Goldwater.
2007.
Nonparametric bayesianmodels of lexical acquisition.
Ph.D. thesis, BrownUniversity, Providence, RI, USA.Aria Haghighi and Dan Klein.
2007.
Unsupervisedcoreference resolution in a nonparametric bayesianmodel.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 848?855, Prague, Czech Republic.Karin Kipper-Schuler.
2005.
VerbNet: A broad-coverage, comprehensive verb lexicon.
Ph.D. thesis,University of Pennsylvania.Dan Klein, Sepandar Kamvar, and Chris Manning.2002.
From instance-level constraints to space-levelconstraints: Making the most of prior knowledge indata clustering.
In Proceedings of the Nineteenth In-ternational Conference on Machine Learning.Anna Korhonen, Yuval Krymolowski, and Ted Briscoe.2006a.
A large subcategorization lexicon for naturallanguage processing applications.
In Proceedings ofthe 5th International Conference on Language Re-sources and Evaluation.Anna Korhonen, Yuval Krymolowski, and Nigel Col-lier.
2006b.
Automatic classification of verbs inbiomedical texts.
In Proceedings of the COLING-ACL, pages 345?352.Daniel D. Lee and Sebastian H. Seung.
1999.
Learningthe parts of objects by non-negative matrix factoriza-tion.
Nature, 401(6755):788?791, October.Beth Levin.
1993.
English Verb Classes and Alter-nations: a preliminary investigation.
University ofChicago Press, Chicago.Chih-Jen Lin.
2007.
Projected gradient methods fornonnegative matrix factorization.
Neural Compua-tion, 19(10):2756?2779.Marina Meila?.
2007.
Comparing clusterings?an in-formation based distance.
Journal of MultivariateAnalysis, 98(5):873?895.Daniel J. Navarro, Thomas L. Griffiths, Mark Steyvers,and Michael D. Lee.
2006.
Modeling individual dif-ferences using dirichlet processes.
Journal of Math-ematical Psychology, 50(2):101?122, April.Radford M. Neal.
2000.
Markov Chain Sam-pling Methods for Dirichlet Process Mixture Mod-els.
Journal of Computational and Graphical Statis-tics, 9(2):249?265, June.Jan Puzicha, Thomas Hofmann, and Joachim Buh-mann.
2000.
A theory of proximity based clus-tering: Structure detection by optimization.
PatternRecognition, 33(4):617?634.William M. Rand.
1971.
Objective criteria for the eval-uation of clustering methods.
Journal of the Ameri-can Statistical Association, 66(336):846?850.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In Proceedings of EMNLP-CoNLL, pages 410?420, Prague, Czech Republic.Lin Sun, Anna Korhonen, and Yuval Krymolowski.2008.
Verb class discovery from rich syntactic data.In Proceedings of the 9th International Conferenceon Intelligent Text Processing and ComputationalLinguistics.Robert S. Swier and Suzanne Stevenson.
2004.
Unsu-pervised semantic role labelling.
In Proceedings ofthe 2004 Conference on Empirical Methods in Nat-ural Language Processing, pages 95?102.Yee Whye Teh.
2006.
A hierarchical Bayesian lan-guage model based on Pitman-Yor processes.
InProceedings of COLING-ACL, pages 985?992, Syd-ney, Australia.Andreas Vlachos, Zoubin Ghahramani, and Anna Ko-rhonen.
2008.
Dirichlet process mixture models forverb clustering.
In Proceedings of the ICML work-shop on Prior Knowledge for Text and Language.Kiri Wagstaff and Claire Cardie.
2000.
Clusteringwith instance-level constraints.
In Proceedings ofthe Seventeenth International Conference on Ma-chine Learning, pages 1103?1110, San Francisco,CA, USA.
Morgan Kaufmann Publishers Inc.Wei Xu, Xin Liu, and Yihong Gong.
2003.
Docu-ment clustering based on non-negative matrix factor-ization.
In Proceedings of the 26th annual interna-tional ACM SIGIR conference on Research and de-velopment in informaion retrieval, pages 267?273,New York, NY, USA.
ACM Press.82
