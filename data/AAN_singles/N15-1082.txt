Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 809?819,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSolving Hard Coreference ProblemsHaoruo Peng?and Daniel Khashabi?and Dan RothUniversity of Illinois, Urbana-ChampaignUrbana, IL, 61801{hpeng7,khashab2,danr}@illinois.eduAbstractCoreference resolution is a key problem innatural language understanding that still es-capes reliable solutions.
One fundamental dif-ficulty has been that of resolving instancesinvolving pronouns since they often requiredeep language understanding and use of back-ground knowledge.
In this paper we pro-pose an algorithmic solution that involves anew representation for the knowledge requiredto address hard coreference problems, alongwith a constrained optimization frameworkthat uses this knowledge in coreference de-cision making.
Our representation, PredicateSchemas, is instantiated with knowledge ac-quired in an unsupervised way, and is com-piled automatically into constraints that im-pact the coreference decision.
We presenta general coreference resolution system thatsignificantly improves state-of-the-art perfor-mance on hard, Winograd-style, pronoun reso-lution cases, while still performing at the state-of-the-art level on standard coreference reso-lution datasets.1 IntroductionCoreference resolution is one of the most impor-tant tasks in Natural Language Processing (NLP).Although there is a plethora of works on this task(Soon et al, 2001a; Ng and Cardie, 2002a; Ng,2004; Bengtson and Roth, 2008; Pradhan et al,2012; Kummerfeld and Klein, 2013; Chang et al,2013), it is still deemed an unsolved problem due tointricate and ambiguous nature of natural language?These authors contributed equally to this work.text.
Existing methods perform particularly poorlyon pronouns, specifically when gender or pluralityinformation cannot help.
In this paper, we aim toimprove coreference resolution by addressing thesehard problems.
Consider the following examples:Ex.1 [A bird]e1perched on the [limb]e2and[it]probent.Ex.2 [Robert]e1was robbed by [Kevin]e2, and[he]prois arrested by police.In both examples, one cannot resolve the pro-nouns based on only gender or plurality informa-tion.
Recently, Rahman and Ng (2012) gathered adataset containing 1886 sentences of such challeng-ing pronoun resolution problems (referred to lateras the Winograd dataset, following Winograd (1972)and Levesque et al (2011)).
As an indication to thedifficulty of these instances, we note that a state-of-the-art coreference resolution system (Chang et al,2013) achieves precision of 53.26% on it.
A specialpurpose classifier (Rahman and Ng, 2012) trainedon this data set achieves 73.05%.
The key contribu-tion of this paper is a general purpose, state-of-the-art coreference approach which, at the same time,achieves precision of 76.76% on these hard cases.Addressing these hard coreference problems re-quires significant amounts of background knowl-edge, along with an inference paradigm that canmake use of it in supporting the coreference deci-sion.
Specifically, in Ex.1 one needs to know that?a limb bends?
is more likely than ?a bird bends?.In Ex.2 one needs to know that the subject of theverb ?rob?
is more likely to be the object of ?ar-rest?
than the object of the verb ?rob?
is.
Theknowledge required is, naturally, centered around809the key predicates in the sentence, motivating thecentral notion proposed in this paper, that of Pred-icate Schemas.
In this paper, we develop the no-tion of Predicate Schemas, instantiate them with au-tomatically acquired knowledge, and show how tocompile it into constraints that are used to resolvecoreference within a general Integer Linear Pro-gramming (ILP) driven approach to coreference res-olution.
Specifically, we study two types of Predi-cate Schemas that, as we show, cover a large frac-tion of the challenging cases.
The first specifies onepredicate with its subject and object, thus providinginformation on the subject and object preferences ofa given predicate.
The second specifies two pred-icates with a semantically shared argument (eithersubject or object), thus specifies role preferences ofone predicate, among roles of the other.
We instanti-ate these schemas by acquiring statistics in an unsu-pervised way from multiple resources including theGigaword corpus, Wikipedia, Web Queries and po-larity information.A lot of recent work has attempted to utilize sim-ilar types of resources to improve coreference reso-lution (Rahman and Ng, 2011a; Ratinov and Roth,2012; Bansal and Klein, 2012; Rahman and Ng,2012).
The common approach has been to injectknowledge as features.
However, these pieces ofknowledge provide relatively strong evidence thatloses impact in standard training due to sparsity.
In-stead, we compile our Predicate Schemas knowl-edge automatically, at inference time, into con-straints, and make use of an ILP driven framework(Roth and Yih, 2004) to make decisions.
Using con-straints is also beneficial when the interaction be-tween multiple pronouns is taken into account whenmaking global decisions.
Consider the following ex-ample:Ex.3 [Jack]e1threw the bags of [John]e2into the water since [he]pro1mistakenly asked[him]pro2to carry [his]pro3bags.In order to correctly resolve the pronouns in Ex.3,one needs to have the knowledge that ?he asks him?indicates that he and him refer to different entities(because they are subject and object of the samepredicate; otherwise, himself should be used insteadof him).
This knowledge, which can be easily repre-sented as constraints during inference, then impactsother pronoun decisions in a global decision with re-spect to all pronouns: pro3is likely to be differentfrom pro2, and is likely to refer to e2.
This type ofinference can be easily represented as a constraintduring inference, but hard to inject as a feature.We then incorporate all constraints into a generalcoreference system (Chang et al, 2013) utilizing themention-pair model (Ng and Cardie, 2002b; Bengt-son and Roth, 2008; Stoyanov et al, 2010).
A classi-fier learns a pairwise metric between mentions, andduring inference, we follow the framework proposedin Chang et al (2011) using ILP.The main contributions of this paper can be sum-marized as follows:1.
We propose the Predicate Schemas representa-tion and study two specific schemas that are im-portant for coreference.2.
We show how, in a given context, PredicateSchemas can be automatically compiled intoconstraints and affect inference.3.
Consequently, we address hard pronoun resolu-tion problems as a standard coreference prob-lem and develop a system1which shows signif-icant improvement for hard coreference prob-lems while achieving the same state-of-the-artlevel of performance on standard coreferenceproblems.The rest of the paper is organized as follows.
Wedescribe our Predicate Schemas in Section 2 and ex-plain the inference framework and automatic con-straint generation in Section 3.
A summary of ourknowledge acquisition steps is given in Section 4.We report our experimental results and analysis inSection 5, and review related work in Section 6.2 Predicate SchemaIn this section we present multiple kinds of knowl-edge that are needed in order to improve hard coref-erence problems.
Table 1 provides two example sen-tences for each type of knowledge.
We use m torefer to a mention.
A mention can either be an en-tity e or a pronoun pro.
predmdenotes the pred-icate of m (similarly, predproand predefor pro-nouns and entities, respectively).
For instance, insentence 1.1 in Table 1, the predicate of e1and e21Available at http://cogcomp.cs.illinois.edu/page/softwareview/Winocoref810Category # Sentence11.1 [The bird]e1perched on the [limb]e2and [it]probent.1.2 [The bee]e1landed on [the flower]e2because [it]prohad pollen.22.1 [Bill]e1was robbed by [John]e2, so the officer arrested [him]pro.2.2 [Jimbo]e1was afraid of [Bobbert]e2because [he]progets scared around new people.33.1 [Lakshman]e1asked [Vivan]e2to get him some ice cream because [he]prowas hot.3.2 Paula liked [Ness]e1more than [Pokey]e2because [he]prowas mean to her.Table 1: Example sentences for each schema category.
The annotated entities and pronouns are hard coref-erence problems.Type Schema form Explanation of examples from Table 11 predm(m,a)Example 1.2: It is enough to know that:S (have (m = [the flower], a = [pollen])) >S (have (m = [the bee], a = [pollen]))2 predm(m,a) |?predm(m, a?)
, cnExample 2.2: It is enough to know that:S (be afraid of (m = ?, a = ?)
|get scared (m = ?, a?
= ?)
, because) >S (be afraid of (a = ?,m = ?)
|get scared (m = ?, a?
= ?)
, because)Table 2: Predicate Schemas and examples of the logic behind the schema design.
Here ?
indicates that theargument is dropped, and S(.)
denotes the scoring function defined in the text.Type1S (predm(m, a))S (predm(a,m))S (predm(m, ?
))S (predm(?,m))Type2S(predm(m, a) |?predm(m, a?)
, cn)S(predm(a,m) |?predm(m, a?)
, cn)S(predm(m, a) |?predm(a?,m) , cn)S(predm(a,m) |?predm(a?,m) , cn)S(predm(m, ?)
|?predm(m, ?)
, cn)...Table 3: Possible variations for scoring functionstatistics.
Here ?
indicates that the argument isdropped.is prede1= prede2=?perch on?.
cn refers to thediscourse connective (cn=?and?
in sentence 1.1).
adenotes an argument of predmother than m. Forexample, in sentence 1.1, assuming thatm = e1, thecorresponding argument is a = e2.We represent the knowledge needed with twotypes of Predicate Schemas (as depicted in Table 2).To solve the assignment of [it]proin sentence 1.1,as mentioned in Section 1, we need the knowledgethat ?a limb bends?
is more reasonable than ?a birdbends?.
Note that the predicate of the pronoun isplaying a key role here.
Also the entity mention it-self is essential.
Similarly, for sentence 1.2, to re-solve [it]pro, we need the knowledge that ?bee hadpollen?
is more reasonable than ?flower had pollen?.Here, in addition to entity mention and the predi-cate (of the pronoun), we need the argument whichshares the predicate with the pronoun.
To formallydefine the type of knowledge needed we denote itwith ?predm(m,a)?
where m and a are a mentionand an argument, respectively2.
We use S(.)
to de-note the score representing how likely the combina-tion of the predicate-mention-argument is.
For eachschema, we use several variations by either chang-ing the order of the arguments (subj.
vs obj.)
ordropping either of them.
We score the various Type1 and Type 2 schemas (shown in Table 3) differently.The first row of Table 2 shows how Type 1 schemais being used in the case of Sentence 1.2.For sentence 2.2, we need to have the knowledgethat the subject of the verb phrase ?be afraid of?
ismore likely than the object of the verb phrase ?beafraid of?
to be the subject of the verb phrase ?getscared?.
The structure here is more complicatedthan that of Type 1 schema.
To make it clearer, weanalyze sentence 2.1.
In this sentence, the objectof ?be robbed by?
is more likely than the subject2Note that the order of m and a relative to the predicate isa critical issue.
To keep things general in the schemas defini-tion, we do not show the ordering; however, when using scoresin practice the order between a mention and an argument is acritical issue.811of the verb phrase ?be robbed by?
to be the objectof ?the officer arrest?.
We can see in both exam-ples (and for the Type 2 schema in general), thatboth predicates (the entity predicate and the pronounpredicate) play a crucial role.
Consequently, we de-sign the Type 2 schema to capture the interactionbetween the entity predicate and the pronoun pred-icate.
In addition to the predicates, we may needmention-argument information.
Also, we stress theimportance of the discourse connective between en-tity mention and pronoun; if in either sentence 2.1or 2.2, we change the discourse connective to ?al-though?, the coreference resolution will completelychange.
Overall, we can represent the knowledgeas ?predm (m,a) |?predm (m, ?a) , cn?.
Just likefor Type 1 schema, we can represent Type 2 schemawith a score function for different variations of argu-ments (lower half of Table 3).
In Table 2, we exhibitthis for sentence 2.2.Type 3 contains the set of instances which can-not be solved using schemas of Type 1 or 2.
Twosuch examples are included in Table 1.
In sentence3.1 and 3.2, the context containing the necessary in-formation goes beyond our triple representation andtherefore this instance cannot be resolved with ei-ther of the two schema types.
It is important to notethat the notion of Predicate Schemas is more generalthan the Type 1 and Type 2 schemas introduced here.Designing more informative and structured schemaswill be essential to resolving additional types of hardcoreference instances.3 Constrained ILP InferenceInteger Linear Programming (ILP) based formula-tions of NLP problems (Roth and Yih, 2004) havebeen used in a board range of NLP problems and,particularly, in coreference problems (Chang et al,2011; Denis and Baldridge, 2007).
Our formulationis inspired by Chang et al (2013).
Let M be theset of all mentions in a given text snippet, and P theset of all pronouns, such that P ?
M. We train acoreference model by learning a pairwise mentionscoring function.
Specifically, given a mention-pair(u, v) ?
M (u is the antecedent of v), we learna left-linking scoring function fu,v= w>?
(u, v),where ?
(u, v) is a pairwise feature vector and w isthe weight vector.
We then follow the Best-Link ap-proach (Section 2.3 from Chang et al (2011)) for in-ference.
The ILP problem that we solve is formallydefined as follows:???????????????????argmaxy?u?M,v?Mfu,vyu,vs.t.
yu,v?
{0, 1}, ?u, v ?M?u<v,u?Myu,v?
1, ?v ?MConstraints from Predicate Schemas KnowledgeConstraints between pronouns.Here, u, v are mentions and yu,vis the decisionvariable to indicate whether or not mention u andmention v are coreferents.
As the first constraintshows, yu,vis a binary variable.
yu,vequals 1 if u, vare coreferents and 0 otherwise.
The second con-straint indicates that we only choose at most oneantecedent to be coreferent with each mention v.(u < v represents that u appears beore v, thus uis an antecedent of v.) In this work, we add con-straints from Predicate Schemas Knowledge and be-tween pronouns.The Predicate Schemas knowledge provides avector of score values S(u, v) for mention pairs{(u, v)|(u ?
M, v ?
P}, which concatenates allthe schemas involving u and v. Entries in the scorevector are designed so that the larger the value is, themore likely u and v are to be coreferents.
We havetwo ways to use the score values: 1) Augumentingthe feature vector ?
(u, v) with these scores.
2) Cast-ing the scores as constraints for the coreference res-olution ILP in one of the following forms:{if si(u, v) ?
?isi(w, v)?
yu,v?
yw,v,if si(u, v) ?
si(w, v) + ?i?
yu,v?
yw,v,(1)where si(.)
is the i-th dimension of the score vectorS(.)
corresponding to the i-th schema representedfor a given mention pair.
?iand ?iare thresholdvalues which we tune on a development set.3If aninequality holds for all relevant schemas (that is, allthe dimensions of the score vector), we add an in-equality between the corresponding indicator vari-ables inside the ILP.4As we increase the value of a3For the ithdimension of the score vector, we choose either?ior ?ias the threshold.4If the constraints dictated by any two dimensions of S arecontradictory, we ignore both of them.812threshold, the constraints in (1) become more con-servative, thus it leads to fewer but more reliableconstraints added into the ILP.
We tune the thresh-old values such that their corresponding scores at-tain high enough accuracy, either in the multiplica-tive form or the additive form.5Note that, given apair of mentions and context, we automatically in-stantiate a collection of relevant schemas, and thengenerate and evaluate a set of corresponding con-straints.
To the best of our knowledge, this is thefirst work to use such automatic constraint gener-ation and tuning method for coreference resolutionwith ILP inference.
In Section 4, we describe howwe acquire the score vectors S(u, v) for the Predi-cate Schemas in an unsupervised fashion.We now briefly explain the pre-processing step re-quired in order to extract the score vector S(u, v)from a pair of mentions.
Define a triple structuretm, predm(m, am) for anym ?M.
The subscriptm for pred and a, emphasizes that they are extractedas a function of the mention m. The extraction oftriples is done by utilizing the dependency parsetree from the Easy-first dependency parser (Gold-berg and Elhadad, 2010).
We start with a mentionm, and extract its related predicate and the other ar-gument based on the dependency parse tree and part-of-speech information.
To handle multiword predi-cates and arguments, we use a set of hand-designedrules.
We then get the score vector S(u, v) by con-catenating all scores of the Predicate Schemas giventwo triples tu, tv.
Thus, we can expand the scorerepresentation for each type of Predicate Schemasgiven in Table 2: 1) For Type 1 schema, S(u, v) ?S(predv(m = u, a = av))62) For Type 2 schema,S(u, v) ?
S(predu(m = u, a = au)|?predv(m =v, a = av), cn).In additional to schema-driven constraints, wealso apply constraints between pairs of pronounswithin a fixed distance7.
For two pronouns that aresemantically different (e.g.
he vs. it), they must referto different antecedents.
For two non-possesive pro-nouns that are related to the same predicate (e.g.
he5The choice is made based on the performance on the devel-opment set.6In predv(m = u, a = av) the argument and the predicateare extracted relative to v but the mention m is set to be u.7We set the distance to be 3 sentences.saw him), they must refer to different antecedents.84 Knowledge AcquisitionOne key point that remains to be explained ishow to acquire the knowledge scores S(u, v).
Inthis section, we propose multiple ways to ac-quire these scores.
In the current implementa-tion, we make use of four resources.
Each ofthem generates its own score vector.
Therefore,the overall score vector is the concatenation ofthe score vector from each resource: S(u, v) =[Sgiga(u, v) Swiki(u, v) Sweb(u, v) Spol(u, v)].4.1 Gigaword Co-occurenceWe extract triples tm, predm(m, am) (explainedin Section 3) from Gigaword data (4,111,240 docu-ments).
We start by extracting noun phrases usingthe Illinois-Chunker (Punyakanok and Roth, 2001).For each noun phrase, we extract its head noun andthen extract the associated predicate and argumentto form a triple.We gather the statistics for both schema types af-ter applying lemmatization on the predicates andarguments.
Using the extracted triples, we geta score vector from each schema type: Sgiga=[S(1)gigaS(2)giga].To extract scores for Type 1 Predicate Schemas,we create occurence counts for each schema in-stance.
After all scores are gathered, our goal is toquery S(1)giga(u, v) ?
S(predv(m = u, a = av))from our knowledge base.
The returned score is thelog(.)
of the number of occurences.For Type 2 Predicate Schemas, we gather thestatistics of triple co-occurence.
We count the co-occurrence of neighboring triples that share at leastone linked argument.
We consider two triples to beneighbors if they are within a distance of three sen-tences.
We use two heuristic rules to decide whethera pair of arguments between two neighboring triplesare coreferents or not: 1) If the head noun of two ar-guments can match, we consider them coreferents.2) If one argument in the first triple is a person nameand there is a compatible pronoun (based on its gen-der and plurality information) in the second triple,they are also labeled as coreferents.
We also extractthe discourse connectives between triples (because,8Three cases are considered: he-him, she-her, they-them813spol(u, v) =?
?1{Po(pu) = + AND Po(pv) = +} OR 1{Po(pu) = ?
AND Po(pv) = ?
}1{Po(pu) = + AND Po(pv) = +}1{Po(pu) = ?
AND Po(pv) = ?}?
?Table 4: Extrating the polarity score given polarity information of a mention-pair (u, v).
To be brief, we usethe shorthand notation pv, predvand pu, predu.
1{?}
is an indicator function.
spol(u, v) is a binaryvector of size three.therefore, etc.)
if there are any.
To avoid sparsity,we only keep the mention roles (only subj or obj; noexact strings are kept).
Two triple-pairs are consid-ered different if they have different predicates, dif-ferent roles, different coreferred argument-pairs, ordifferent discourse connectives.
The co-occurrencecounts extracted in this form correspond to Type 2schemas in Table 2.
During inference, we matcha Type 2 schema for S(2)giga(u, v) ?
S(predu(m =u, a = au)|?predv(m = u, a = av), cn).Our method is related, but different from the pro-posal in Balasubramanian et al (2012), who sug-gested to extract triples using an OpenIE system(Mausam et al, 2012).
We extracted triples by start-ing from a mention, then extract the predicate andthe other argument.
An OpenIE system does noteasily provide this ability.
Our Gigaword countsare gathered also in a way similar to what has beenproposed in Chambers and Jurafsky (2009), but wegather much larger amounts of data.4.2 Wikipedia Disambiguated Co-occurenceOne of the problems with blindly extracting triplecounts is that we may miss important semantic in-formation.
To address this issue, we use the publiclyavaiable Illinois Wikifier (Cheng and Roth, 2013;Ratinov et al, 2011), a system that disambiguatesmentions by mapping them into correct Wikipediapages, to process the Wikipedia data.
We then ex-tract from the Wikipedia text all entities, verbs andnouns, and gather co-occurrence statistics with thesesyntactic variations: 1) immediately after 2) imme-diately before 3) before 4) after.
For each of thesevariations, we get the probability and count9of apair of words (e.g.
probability10/count for ?bend?immediately following ?limb?)
as separate dimen-sions of the score vector.9We use the log(.)
of the counts here.10Conditional probability of ?limb?
immediately followingthe given verb ?bend?.Given the co-occurrence information, we geta score vector Swiki(u, v) corresponding to Type1 Predicate Schemas, and hence S(u, v)wiki?S(predv(m = u, a = av)).4.3 Web Search Query CountOur third source of score vectors is web queries thatwe implement using Google queries.
We extract ascore vector Sweb(u, v) ?
S(predv(m = u, a =av)) (Type 1 Predicate Schemas) by querying for 1)?u av?
2) ?u predv?
3) ?u predvav?
4) ?avu?11.For each variation of nouns (plural and singular) andverbs (different tenses) we create a different queryand average the counts over all queries.
Concatenat-ing the counts (each is a separate dimension) wouldgive us the score vector Sweb(u, v).4.4 Polarity of ContextAnother rich source of information is the polarity ofcontext, which has been previously used for Wino-grad schema problems (Rahman and Ng, 2012).Here we use a slightly modified version.
The polar-ity scores are used for Type 1 Predicate Schemas andtherefore we want to get Spol(u, v) ?
S(predv(m =u, a = av)).
We first extract polarity values forPo(predu) and Po(predv) by repeating the follow-ing procedures for each of them:?
We extract initial polarity information given thepredicate (using the data provided by Wilson etal.
(2005)).?
If the role of the mention is object, we negateits polarity.?
If there is a polarity-reversing discourse con-nective (such as ?but?)
preceding the predicate,we reverse the polarity.?
If there is a negative comparative adverb (suchas ?less?, ?lower?)
we reverse the polarity.11We query this only when avis an adjective and predvis ato-be verb.814# Doc # Train # Test # Mention # Pronoun # Predictions for PronounWinograd 1886 1212 674 5658 1886 1348WinoCoref 1886 1212 674 6404 2595 2118ACE 375 268 107 23247 3862 13836OntoNotes 3150 2802 348 175324 58952 37846Table 5: Statistics of Winograd, WinoCoref, ACE and OntoNotes.
We give the total number of mentions andpronouns, while the number of predictions for pronoun is specific for the test data.
We added 746 mentions(709 among them are pronouns) to WinoCoref compared to Winograd.Given the polarity values Po(predu) and Po(predv),we construct the score vector Spol(u, v) followingTable 4.5 ExperimentsIn this section, we evaluate our system for both hardcoreference problems and general coreference prob-lems, and provide detailed anaylsis on the impact ofour proposed Predicate Schemas.
Since we treat re-solving hard pronouns as part of the general corefer-ence problems, we extend the Winograd dataset witha more complete annotation to get a new dataset.We evaluate our system on both datasets, and showsignificant improvemnt over the baseline system andover the results reported in Rahman and Ng (2012).Moreover, we show that, at the same time, our sys-tem achieves the state-of-art performance on stan-dard coreference datasets.5.1 Experimental SetupDatasets: Since we aim to solve hard coreferenceproblems, we choose to test our system on the Wino-grad dataset12(Rahman and Ng, 2012).
It is a chal-lenging pronoun resolution dataset which consistsof sentence pairs based on Winograd schemas.
Theoriginal annotation only specifies one pronoun andtwo entites in each sentence, and it is considered as abinary decision for each pronoun.
As our target is tomodel and solve them as general coreference prob-lems, we expand the annotation to include all pro-nouns and their linked entities as mentions (We callthis new re-annotated dataset WinoCoref13).
Ex.3 inSection 1 is from the Winograd dataset.
It originallyonly specifies he as the pronoun in question, andwe added him and his as additional target pronouns.We also use two standard coreference resolution12Available at http://www.hlt.utdallas.edu/?vince/data/emnlp12/13Available at http://cogcomp.cs.illinois.edu/page/data/Systems Learning Method Inference MethodIllinois BLMP BLLIlliCons BLMP ILPKnowFeat BLMP+SF BLLKnowCons BLMP ILP+SCKnowComb BLMP+SF ILP+SCTable 6: Summary of learning and inference meth-ods for all systems.
SF stands for schema featureswhile SC represents constraints from schema knowl-edge.datasets ACE(2004) (NIST, 2004) and OntoNotes-5.0 (Pradhan et al, 2011) for evaluation.
Statisticsof the datasets are provided in Table 5.Baseline Systems: We use the state-of-art Illinoiscoreference system as our baseline system (Changet al, 2013).
It includes two different versions.
Oneemploys Best-Left-Link (BLL) inference method(Ng and Cardie, 2002b), and we name it Illinois14;while the other uses ILP with constraints for infer-ence, and we name it IlliCons.
Both systems useBest-Link Mention-Pair (BLMP) model for training.On Winograd dataset, we also treat the reported re-sult from Rahman and Ng (2012) as a baseline.Developed Systems: We present three variations ofthe Predicate Schemas based system developed here.We inject Predicate Schemas knowledge as mention-pair features and retrain the system (KnowFeat).We use the original coreference model and Predi-cate Schemas knowledge as constraints during infer-ence (KnowCons).
We also have a combined system(KnowComb), which uses the schema knowledge toadd features for learning as well as constraints forinference.
A summary of all systems is provided inTable 6.14In implementation, we use the L3M model proposed inChang et al (2013), which is slightly different.
It can be seenas an extension of BLL inference method.815Dataset Metric Illinois IlliCons Rahman and Ng (2012) KnowFeat KnowCons KnowCombWinograd Precision 51.48 53.26 73.05 71.81 74.93 76.41WinoCoref AntePre 68.37 74.32 ??
88.48 88.95 89.32Table 7: Performance results on Winograd and WinoCoref datasets.
All our three systems are trained onWinoCoref, and we evaluate the predictions on both datasets.
Our systems improve over the baselines byover than 20% on Winograd and over 15% on WinoCoref.Evaluation Metrics: When evaluating on the fulldatasets of ACE and OntoNotes, we use the widelyrecoginzed metrics MUC (Vilain et al, 1995),BCUB (Bagga and Baldwin, 1998), Entity-basedCEAF (CEAFe) (Luo, 2005) and their average.As Winograd is a pronoun resolution dataset, weuse precision as the evaluation metric.
AlthoughWinoCoref is more general, each coreferent clus-ter only contains 2-4 mentions and all are within thesame sentence.
Since traditional coreference metricscannot serve as good metrics, we extend the preci-sion metric and design a new one called AntePre.Suppose there are k pronouns in the dataset, andeach pronoun has n1, n2, ?
?
?
, nkantecedents, re-spectively.
We can view predicted coreference clus-ters as binary decisions on each antecedent-pronounpair (linked or not).
The total number of binary deci-sions is?ki=1ni.
We then meaure how many binarydecisions among them are correct; letm be the num-ber of correct decisions, then AnrePre is computedas:m?ki=1ni.5.2 Results for Hard Coreference ProblemsPerformance results on Winograd and WinoCorefdatasets are shown in Table 7.
The best perform-ing system is KnowComb.
It improves by over20% over a state-of-art general coreference systemon Winograd and also outperforms Rahman and Ng(2012) by a margin of 3.3%.
On the WinoCorefdataset, it improves by 15%.
These results show sig-nificant performance improvement by using Predi-cate Schemas knowledge on hard coreference prob-lems.
Note that the system developed in Rahmanand Ng (2012) cannot be used on the WinoCorefdataset.
The results also show that it is better to com-pile knowledge into constraints when the knowledgequality is high than add them as features.5.3 Results for Standard Coreference ProblemsPerformance results on standard ACE andOntoNotes datasets are shown in Table 8.
OurSystem MUC BCUB CEAFe AVGACEIlliCons 78.17 81.64 78.45 79.42KnowComb 77.51 81.97 77.44 78.97OntoNotesIlliCons 84.10 78.30 68.74 77.05KnowComb 84.33 78.02 67.95 76.76Table 8: Performance results on ACE and OntoNotesdatasets.
Our system gets the same level of per-formance compared to a state-of-art general coref-erence system.Category Cat1 Cat2 Cat3Size 317 1060 509Portion 16.8% 56.2% 27.0%Table 9: Distribution of instances in Winograddataset of each category.
Cat1/Cat2 is the subset ofinstances that require Type 1/Type 2 schema knowl-edge, respectively.
All other instances are put intoCat3.
Cat1 and Cat2 instances can be covered byour proposed Predicate Schemas.KnowComb system achieves the same level ofperformance as does the state-of-art generalcoreference system we base it on.
As hard coref-erence problems are rare in standard coreferencedatasets, we do not have significant performanceimprovement.
However, these results show thatour additional Predicate Schemas do not harm thepredictions for regular mentions.5.4 Detailed AnalysisTo study the coverage of our Predicate Schemasknowledge, we label the instances in Winograd(which also applies to WinoCoref ) with the type ofPredicate Schemas knowledge required.
The distri-bution of the instances is shown in Table 9.
Ourproposed Predicate Schemas cover 73% of the in-stances.We also provide an ablation study on the816Schema AntePre(Test) AntePre(Train)Type 1 76.67 86.79Type 2 79.55 88.86Type 1 (Cat1) 90.26 93.64Type 2 (Cat2) 83.38 92.49Table 10: Ablation Study of Knowledge Schemas onWinoCoref.
The first line specifies the preformancefor KnowComb with only Type 1 schema knowl-edge tested on all data while the third line speci-fies the preformance using the same model but testedon Cat1 data.
The second line specifies the prefor-mance results for KnowComb system with only Type2 schema knowledge on all data while the fourth linespecifies the preformance using the same model buttested on Cat2 data.WinoCoref dataset in Table 10.
These results usethe best performing KnowComb system.
They showthat both Type 1 and Type 2 schema knowledge havehigher precision on Category 1 and Category 2 datainstances, respectively, compared to that on full data.Type 1 and Type 2 knowledge have similiar perfor-mance on full data, but the results show that it isharder to solve instances in category 2 than thosein category 1.
Also, the performance drop betweenCat1/Cat2 and full data indicates that there is a needto design more complicated knowledge schemas andto refine the knowledge acquisition for further per-formance improvement.6 Related WorkWinograd Schema: Winograd (1972) showed thatsmall changes in context could completely changecoreference decisions.
Levesque et al (2011) pro-posed to assemble a set of sentences which com-ply with Winograd?s schema.
Specifically, there arepairs of sentences which are identical except for mi-nor differences which lead to different references ofthe same pronoun in both sentences.
These refer-ences can be easily solved by humans, but are hard,he claimed, for computer programs.Anaphora Resolution: There has been a lot ofwork on anaphora resolution in the past two decades.Many of the early rule-based systems like Hobbs(1978) and Lappin and Leass (1994) gained consid-erable popularity.
The early designs were easy tounderstand and the rules were designed manually.With the development of machine learning basedmodels (Connolly et al, 1994; Soon et al, 2001b;Ng and Cardie, 2002a), attention shifted to solvingstandard coreference resolution problems.
However,many hard coreference problems involve pronouns.As Winograd?s schema shows, there is still a needfor further investigation in this subarea.World Knowledge Acquisition: Many tasks inNLP (such as Textual Entailment, Question Answer-ing, etc.)
require World Knowledge.
Althoughthere are many existing works on acquiring them(Schwartz and Gomez, 2009; Balasubramanian etal., 2013; Tandon et al, 2014), there is still no con-sensus on how to represent, gather and utilize highquality World Knowledge.
When it comes to corefer-ence resolution, there are a handful of works whicheither use web query information or apply align-ment to an external knowledge base (Rahman andNg, 2011b; Kobdani et al, 2011; Ratinov and Roth,2012; Bansal and Klein, 2012; Zheng et al, 2013).With the introduction of Predicate Schema, our goalis to bring these different approaches together andprovide a coherent view.AcknowledgmentsThe authors would like to thank Kai-Wei Chang, Al-ice Lai, Eric Horn and Stephen Mayhew for com-ments that helped to improve this work.
This workis partly supported by NSF grant #SMA 12-09359and by DARPA under agreement number FA8750-13-2-0008.
The U.S. Government is authorized toreproduce and distribute reprints for Governmen-tal purposes notwithstanding any copyright nota-tion thereon.
The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the officialpolicies or endorsements, either expressed or im-plied, of DARPA or the U.S. Government.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In The First International Con-ference on Language Resources and Evaluation Work-shop on Linguistics Coreference, pages 563?566.N.
Balasubramanian, S. Soderland, O. Etzioni, et al2012.
Rel-grams: a probabilistic model of relationsin text.
In Proceedings of the Joint Workshop on Au-817tomatic Knowledge Base Construction and Web-scaleKnowledge Extraction, pages 101?105.
Associationfor Computational Linguistics.N.
Balasubramanian, S. Soderland, Mausam, and O. Et-zioni.
2013.
Generating coherent event schemas atscale.
In EMNLP, pages 1721?1731.M.
Bansal and D. Klein.
2012.
Coreference seman-tics from web features.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics (ACL), Jeju Island, South Korea, July.E.
Bengtson and D. Roth.
2008.
Understanding the valueof features for coreference resolution.
In Proceedingsof the Conference on Empirical Methods for NaturalLanguage Processing (EMNLP), pages 294?303, Oct.N.
Chambers and D. Jurafsky.
2009.
Unsupervisedlearning of narrative schemas and their participants.In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, volume 2, pages 602?610.
Association forComputational Linguistics.K.
Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,M.
Sammons, and D. Roth.
2011.
Inference protocolsfor coreference resolution.
In Proceedings of the An-nual Conference on Computational Natural LanguageLearning (CoNLL), pages 40?44, Portland, Oregon,USA.
Association for Computational Linguistics.K.
Chang, R. Samdani, and D. Roth.
2013.
A con-strained latent variable model for coreference reso-lution.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing,pages 601?612.
Association for Computational Lin-guistics.X.
Cheng and D. Roth.
2013.
Relational inference forwikification.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Process-ing, pages 1787?1796.
Association for ComputationalLinguistics.D.
Connolly, J. D. Burger, and D. S. Day.
1994.
A ma-chine learning approach to anaphoric reference.
InProceedings of the International Conference on NewMethods in Language Processing (NeMLaP).
ACL.P.
Denis and J. Baldridge.
2007.
Joint determination ofanaphoricity and coreference resolution using integerprogramming.
In Proceedings of the Annual Meetingof the North American Association of ComputationalLinguistics (NAACL).Y.
Goldberg and M. Elhadad.
2010.
An efficient algo-rithm for easy-first non-directional dependency pars-ing.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages742?750.
Association for Computational Linguistics.J.
R Hobbs.
1978.
Resolving pronoun references.
Lin-gua, 44(4):311?338.H.
Kobdani, H. Schuetze, M. Schiehlen, and H. Kamp.2011.
Bootstrapping coreference resolution usingword associations.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 783?792.Association for Computational Linguistics.K.
J. Kummerfeld and D. Klein.
2013.
Error-driven anal-ysis of challenges in coreference resolution.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 265?277.Association for Computational Linguistics.S.
Lappin and H. J. Leass.
1994.
An algorithm forpronominal anaphora resolution.
Computational lin-guistics, 20(4):535?561.H.
J. Levesque, E. Davis, and L. Morgenstern.
2011.
Thewinograd schema challenge.
In AAAI Spring Sympo-sium: Logical Formalizations of Commonsense Rea-soning.X.
Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of the Conference onEmpirical Methods for Natural Language Processing(EMNLP).Mausam, M. Schmitz, R. Bart, S. Soderland, and O. Et-zioni.
2012.
Open language learning for informa-tion extraction.
In Proceedings of Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL).V.
Ng and C. Cardie.
2002a.
Identifying anaphoricand non-anaphoric noun phrases to improve coref-erence resolution.
In Proceedings of the 19th in-ternational conference on Computational linguistics-Volume 1, pages 1?7.
Association for ComputationalLinguistics.V.
Ng and C. Cardie.
2002b.
Improving machine learn-ing approaches to coreference resolution.
In Proceed-ings of the 40th Annual Meeting of the Association forComputational Linguistics.V.
Ng.
2004.
Learning noun phrase anaphoricity to im-prove conference resolution: Issues in representationand optimization.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguis-tics (ACL-04).NIST.
2004.
The ACE evaluation plan.S.
Pradhan, L. Ramshaw, M. Marcus, M. Palmer,R.
Weischedel, and N. Xue.
2011.
Conll-2011 sharedtask: Modeling unrestricted coreference in ontonotes.In Proceedings of the Annual Conference on Compu-tational Natural Language Learning (CoNLL).S.
Pradhan, A. Moschitti, N. Xue, O. Uryupina, andY.
Zhang.
2012.
CoNLL-2012 shared task: Modeling818multilingual unrestricted coreference in OntoNotes.
InProceedings of the Annual Conference on Computa-tional Natural Language Learning (CoNLL).V.
Punyakanok and D. Roth.
2001.
The use of classi-fiers in sequential inference.
In The Conference onAdvances in Neural Information Processing Systems(NIPS), pages 995?1001.
MIT Press.A.
Rahman and V. Ng.
2011a.
Coreference resolu-tion with world knowledge.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 814?824.
Association for Computational Lin-guistics.A.
Rahman and V. Ng.
2011b.
Coreference resolutionwith world knowledge.
In ACL, pages 814?824.A.
Rahman and V. Ng.
2012.
Resolving complexcases of definite pronouns: the winograd schema chal-lenge.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 777?789.
Association for Computational Lin-guistics.L.
Ratinov and D. Roth.
2012.
Learning-based multi-sieve co-reference resolution with knowledge.
In Pro-ceedings of the Conference on Empirical Methods forNatural Language Processing (EMNLP).Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to wikipedia.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1375?1384, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InHwee Tou Ng and Ellen Riloff, editors, Proceedingsof the Annual Conference on Computational NaturalLanguage Learning (CoNLL), pages 1?8.
Associationfor Computational Linguistics.H.
A. Schwartz and F. Gomez.
2009.
Acquiring applica-ble common sense knowledge from the web.
In Pro-ceedings of the Workshop on Unsupervised and Mini-mally Supervised Learning of Lexical Semantics, UM-SLLS ?09, pages 1?9, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.W.
M. Soon, D. C. Y. Lim, and H. T. Ng.
2001a.
Amachine learning approach to coreference resolutionof noun phrases.
Computational Linguistics, Volume27, Number 4, December 2001.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001b.
A machine learning approach to coref-erence resolution of noun phrases.
Computational lin-guistics, 27(4):521?544.V.
Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,and D. Hysom.
2010.
Coreference resolution withreconcile.
In Proceedings of the ACL 2010 ConferenceShort Papers, pages 156?161.
Association for Compu-tational Linguistics.N.
Tandon, G. de Melo, and G. Weikum.
2014.
Acquir-ing comparative commonsense knowledge from theweb.
In Proceedings of AAAI Conference on ArtificialIntelligence.
AAAI.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In Proceedings of the 6th conferenceon Message understanding.T.
Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,J.
Wiebe, Y. Choi, C. Cardie, E. Riloff, and S. Pat-wardhan.
2005.
Opinionfinder: A system for sub-jectivity analysis.
In Proceedings of HLT/EMNLP oninteractive demonstrations, pages 34?35.
Associationfor Computational Linguistics.T.
Winograd.
1972.
Understanding natural language.Cognitive psychology, 3(1):1?191.J.
Zheng, L. Vilnis, S. Singh, J. D. Choi, and A. Mc-Callum.
2013.
Dynamic knowledge-base alignmentfor coreference resolution.
In Proceedings of the Sev-enteenth Conference on Computational Natural Lan-guage Learning (CoNLL), pages 153?162, Sofia, Bul-garia, August.
Association for Computational Linguis-tics.819
