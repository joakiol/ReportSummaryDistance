When is an Embedded MT System "Good Enough" for Filtering?Clare R. VossArmy Research LaboratoryAdelphi, MD 20783voss@arl.milCarol Van Ess-DykemaDepartment of DefenseFt.
Meade, MDcjvanes@ afterlife.ncsc.milAbstractThis paper proposes an end-to-end processanalysis template with replicable measuresto evaluate the filtering performance of aScan-OCR-MT system.
Preliminary results 1across three language-specific FALCon 2systems how that, with one exception, thederived measures consistently yield thesame performance ranking: Haitian Creoleat the low end, Arabic in the middle, andSpanish at the high end.1 The Filtering ProblemHow do people quickly determine whether aparticular foreign language text document isrelevant to their interest when they do notunderstand that foreign language?
FALCon, ourembedded MT system, has been designed toassist an English-speaking person in filtering,i.e., deciding which foreign language documentsare worth having an expert translator processfurther.
In this paper, we seek to determine whensuch systems are "good enough" for filtering.We define "filtering" to be a forced-choicedecision-making process on individualdocuments, where each document is assigned asingle value, either a "yes, relevant" or a "no,irrelevant" by the system user) The singl e -document relevance assessment is performedFor a more extensive report of our work, see Vossand Van Ess-Dykema (2000).2 FALCon (Forward Area Language CONverter) is alaptop-based embedded MT system integrated at theArmy Research Laboratory for field use.
(Fisher andVoss, 1997)3 See the report entitled "Mulfilingual InformationManagement: Current Levels and Future Abilities"for other definitions of filtering, available athttp://www.cs.cmu.edu/People/ref/mlim/.independent of the content of other documents inthe processing collection.When Church and Hovy (1993) introduced thenotion that "crummy" MT engines could be putto good use on tasks less-demanding thanpublication-quality translation, MT researchefforts did not typically evaluate systemperformance in the context of specific tasks.
(Sparck Jones and Galliers, 1996).
In the lastfew years, however, the Church and Hovyinsight has led to innovative xperiments, likethose reported by Resnik (1997), Pomarede t al.
(1998), and Taylor and White (1998), usingtask-based evaluation methods.
Most recently,research on task-based evaluation has been.proposed within TIDES, a recent DARPAinitiative whose goals include enabling English-speaking individuals to access, correlate, andinterpret multilingual sources of information(DARPA, 1999; Harmon, 1999).This paper introduces a method of assessingwhen an embedded MT system is "goodenough" for the filtering of hard-copy foreignlanguage (FL) documents by individuals with noknowledge of that language.
We describepreliminary work developing measures onsystem-internal components hat assess: (i) theflow of words relevant o the filtering task anddomain through the steps of documentprocessing in our embedded MT system, and (ii)the level of "noise," i.e., processing errors,passing through the system.
We present ananalysis template that displays the processingsteps, the sequence of document versions, andthe basic measures of our evaluation method.After tracing the processing of Spanish, Arabic,and Haitian Creole parallel texts that is recordedin the analysis templates, we discuss ourpreliminary results on the filtering performanceof the three language-specific embedded MTsystems from this process flow.Processes Document Versions1gords'~--~ image doe in S~__~ post-OCR doc !
)post-MT doe i~tagged TL dopen/dosed wtagged TL dosere.
related w~tagged TL dodomain woreFigure 1 Analysis TemplateMeasures2 An Embedded MT System Design 4Our three systems process documents using asequence of three software modules.
First, theScan software module creates an online bitmapimage in real-time as the user feeds thedocument into the page-feed scanner-.
5 Second,the optical character recognition (OCR) softwareconverts that image to character text and, third,the machine translation (MT) software convertsthe foreign language character text to English,where it may be stored to disk or displayed onscreen directly to the user.
The user interfaceonly requires that the user push one or twobuttons to carry out all of the system'sprocessing on an individual document.We tested three separate language-specificembedded MT systems for Spanish, Arabic andHaitian Creole.
These systems differ in their4 We use "embedded MT systems" as defined in Vossand Reeder (1998).5 We chose a small scanner for portability of thesystem.
Substituting in a flatbed scanner would notaffect performance.OCR and MT components, but otherwise theyshare the same software, Omnipage's Paperportfor scaning and Windows95 as the operatingsystem.
63 ApproachAs we sought to measure the performance ofeach component in the systems, it quicklybecame apparent that not all available measuresmay be equally applicable for our filtering task.For example, counting the number of sourcelanguage (SL) characters correctly OCR-ed maybe overly specific: as discussed below, we onlyneed to make use of the number of SL wordsthat are correctly OCR-ed.
In the sections tofollow, we describe those measures that havebeen most informative for the task of filtering.Analysis TemplateWe use three types of information in ourevaluation of the end-to-end embedded MTsystems that we have available to us:transformation processes, document versions,and basic count measures.
The transformationprocesses are listed vertically in the diamonds onthe left side of figure 1.
Starting with thehardcopy original document, each processtransforms its input text and creates a newversion.
These document versions are listedvertically in the boxes in the second column ofthe figure.
For each version, we compute one ormore basic count measures on the words in thatversion's text.
That is, for each process, there isan associated ocument version and for eachdocument version, there are associated basiccount measures.
These count measures hownas A. through M. are defined in figure 2 below.Two-Pass EvaluationFor each end-to-end system and language pair,we follow two separate passes in creatinganalysis files from scanned-in bitmap images.The first pass is for end-to-end Scan-OCR-MTevaluation: "OCR" the original document, thenMT the resulting OCR-output file.
The secondpass is for Ground Truth-MT evaluation:"ground-truth" (GT) the original document, henMT the resulting GT-ed output file.6 See Voss and Van Ess-Dykema (2000) for adescription of the products used.2A.B.Measuresimage doe"~ C.:# "words" lost/added~ \[t i,,OCR oo , J ISPANISHS CAN/ GT/OCR/ MTMTH.
_I.
.....................~ #  dosLed class i ?# incorrect iL .
.
.
.
.
.
.
.
.
.
.
.
.
M~ .....................t , -wor~-A-7~.
- - ) l  rol~,~it  iI *ords in TL  I .Figure2 Comparison ofThe two passes represent he "worst" and"best" cases respectively for filtering withineach of the three embedded MT systems.
By"ground truth" versions of the document, wemean online duplicated versions that match,character-for-character, the input text.We intentionally chose low-performanceOCR software (for each language) to simulate a"worst case" performance by our systems,enabling us to compare them with the ideal high-performance ground-truth input to simulate a"best case" performance.Texts from the Center for Disease ControlIn order to compare the three language-specificsystems, we had to fred a corpus in a domainwell-defined for filtering 7 that included paralleltexts in Spanish, Arabic, and Haitian Creole.
Wefound parallel corpora for these and many otherARABICSCAN/OCR/MTGT/MTHAITIAN C REOLES CAN/ OT/OCR/ MTMTLanguage-Spedfic System Resultslanguages at a website of the Center for DiseaseControl (CDC).
8We chose a paragraph from the chickenpox/varicella bulletin, page 2, for each of ourthree languages.
This passage contains narrativefull-length sentences and minimizes the OCRcomplications that arise with variable layouts.Our objective for selecting this input paragraphwas to illustrate our methodology in a tractableway for multiple languages.
Our next step willbe to increase the amount of data analyzed foreach language.4 AnalysesWe fill out one analysis template for eachdocument tested in a language-specific system.Example templates with the basic countII II 7 Filtering judgments are well-defined whenmultiple readers of a text in a domain agree on the"yes, relevant" status of the text.8 See http://www.irnmunize.org/vis/index.htm.
Thetexts are "Vaccine Information Statements"describing basic medical symptoms that individualsshould know about in advance of being vaccinated.3measures 9 are presented in figure 2 for each Ofthe three embedded MT systems that we tested.Notice that in figure 2 we distinguish validwords of a language from OCR-generatedstrings of characters that we identify as "words.
"The latter "words" may include any of thefollowing: wordstrings with OCR-inducedspelling changes (valid or invalid for the specificlanguage), wordstrings duplicating misspellingsin the source document, and words accuratelyOCR-ed.
"Words" may also be lost in the MTprocess (see F.).
1?The wide, block arrow in figure 2 connect,,; E .and G. because they are both based on the MToutput document.
(We do not compute a sum forthese counts because the E "words" are in the SLand the G words are in the TL.)
The open classwords (see H.) are nouns, verbs, adjectives, andadverbs.
Closed class words (see I.)
include: allparts of speech not listed as open classcategories.In this methodology, we track the conltentwords that ultimately contribute to the finalfiltering decision.
Clearly for other tasks, suchas summarization or information extraction,other measures may be more appropriate.
Thebasic count measures A. through M. arepreliminary and will require refinement as moredata sets are tested.
From these basic countmeasures, we define four derived percentagemeasures in section 5 and summarize these casesacross our three systems in figure 3 of thatsection.4.1 Embedded Spanish MT System Test"Worst"  case (Scan-OCR-MT pass)As can be seen in figure 2, not all of the original80 Spanish words in the source document retaintheir correct spelling after being OCR-ed.
Only26 OCR-ed "words" are found in the NITlexicon, i.e., recognized as valid Spanish words.Forty-nine of the OCR-ed "words" are treated as"not found words" (NFWs) by the MT engine,even though they may in fact be actual Spanishwords.
Five other OCR-ed "words" are lost in9 The following formulas ummarize the relationsamong the count measures: A ffi B+C; B ffi D+E+F;G ffi H+I; H = J+K;  J ffi L+M.10 For example, we found that the word la in theSpanish text was not present in the TL output, i.e.,the English equivalent the did not appear in theEnglish translation.the MT process.
Thus, the OCR process reducedthe number of Spanish words that the MT enginecould accept as input by more than 60%.Of the remaining 40% that generated 29English words, we found that 5 were "filter-relevant" as follows.
The MT engine ignored 49post-OCR Spanish "words" and working fromthe remaining 26 Spanish words, generated 29English words?
1 Seventeen were open classwords and 12 were closed class words.
Nearlyall of the open class words were translatedcorrectly or were semantically appropriate forthe domain (16 out of 17).
From this correct setof 16 open class words, 5 were domain-relevantand 9 were not.
That is, 5 of the 29 generatedEnglish words, or 17%, were semanticallyrelated and domain relevant words, i.e., triggersfor filtering judgments.
"Best" case (GT-MT pass)The MT engine generated 77 English wordsfrom the 80 original Spanish words.
Thirty-eight, or half of the 77, were open class words;39 were closed class words.
All of the 38 openclass words were correctly translated orsemantically related to the preferred translation.And half of those, 17, were domain-relevant.Thus, the 77 English words generated by the MTengine contained 17 "filter-relevant" words, or22%.Comparing the Two PassesSurprisingly the GT-MT pass only yields a 5%improvement in filtering judgments over theScan-OCR-MT pass, even though the OCR itselfreduced the number of Spanish words that theMT engine could accept as input by more than60%.
We must be cautious in interpreting thesignificance of this comparison, given the single,short paragraph used only for illustrating ourmethodology.4.2 Embedded Arabic MT System Test"Worst"  case  (Scan-OCR-MT pass)The OCR process converted the original 84Arabic words into 88 "words".
Of the original84 Arabic words in the source document, only11 This occurred because the MT engine was notusing a word-for-word scheme.
The Spanish verb fordebo is translated into 2 English words, I must.
As wewill note further on, different languages havedifferent expansion rates into English.4.55 retain their correct spelling after being OCR-ed and are found in the MT lexicon, i.e.,recognized as valid Arabic words.
Ten of theother OCR-ed "words" are treated as NFWs bythe MT engine.
The remaining 23 OCR-edmixture of original words and OCR-induced"words" are not found in the Arabic MT lexicon.Thus, the OCR process reduced the number oforiginal Arabic words that the MT engine couldaccept as input by slightly more than 65%.Of the remaining 35% that generated 70English words, we found that 7 were "filter-relevant" as follows.
The MT lexicon did notcontain 10 post-OCR Arabic "words" andworking from the remaining 55 Arabic words,the MT engine generated 70 English words.
12Thirty of the 70 were open class words and 40were closed class words.
Only one-third of theopen class words were translated correctly orwere semantically appropriate for the domain(10 out of 30).
From this correct set of 10 openclass words, 7 were domain-relevant and 3 werenot.
Thus, this pass yields 7 words for filteringjudgments from the 70 generated English words,or 10%, were semantically related and domainrelevant words.
"Best" case (GT-MT pass)Of the 84 original Arabic words, even with theGT as input, 28 were not found in the MTlexicon, reflecting the engine's emerging statusand the need for further development.
Twoothers were not found in the Arabic MT lexicon,leaving 54 remaining words as input to the MTengine.
The MT engine generated 68 Englishwords from these 54 words.
Thirty-one of the68 were open class words; 37 were closed classwords.
Of the open class words, 25 weretranslated correctly or semantically related.
And8 of those 25 were domain-relevant.
Thus, the68 English words generated by the MT enginecontained 8 "filter-relevant" words, or 12%.Comparing the Two PassesThe GT-MT pass yields a 2% improvement infiltering judgments over the Scan-OCR-MTpass, even though the OCR itself reduced the12 This expansion rate is consistent with the rule-of-thumb that Arabic linguists have for every oneArabic word yielding on average 1.3 words inEnglish.number of Arabic words that the MT enginecould accept as input by about 65%.One of the interesting findings about OCR-edArabic "words" was the presence of "falsepositives," inaccurately OCR-ed sourcedocument words that were nonetheless valid inArabic.
That is, we found instances of validArabic words in the OCR output that appearedas different words in the original document.
134.3 Embedded Haitian MT System Test"Worst" case (Scan-OCR-MT pass)In the template for the 76-word Haitian Creolesource document, we see that 27 words were lostin the OCR process, leaving only 49 in the post-OCR document.
Of those 49, only 20 exhibittheir correct spelling after being OCR-ed and arefound in the MT lexicon.
Twenty-nine of the 49OCR-ed "words" are not found (NFWs) by theMT engine.
The OCR process reduced thenumber of original Haitian Creole wordsacceptable by the MT engine from 76 to 20, or74%.Of the remaining 26% that generated 22English words, we found that none were "filter-relevant," i.e., 0%, as follows.
The MT engineignored 29 post-OCR "words" and working fromthe remaining 20 Haitian words, generated 22English words.
Ten were open class words and12 were closed class words.
Only 2 out of the 10open class words were translated correctly orwere semantically appropriate for the domain.From this correct set of 2 open class words,none were domain-relevant.
The human wouldbe unable to use this final document version tomake his or her f'dtering relevance judgments.
"Best" case (GT-MT pass)The MT engine generated 63 English wordsfrom the 76 original Haitian Creole words.Thirty of the 63 were open class words; 33 wereclosed class words.
Only 11 of the 30 open classwords were correctly translated or semanticallyrelated.
Of those 11 words, 3 were domain-relevant.
So, from the 63 generated Englishwords, only 3 were "filter-relevant", or 5%.13 As a result, the number of words in the two passescan differ.
As we see in figure 2 in the Scan-OCR-MT pass, there were 55 SL words translated but, inthe GT-MT pass, only 54 SL words in the originaltext.5Derived SpanishMeas.
OCR GTW.
40 95X.
55 49Y.
17 22Z.
94 100ArabicOCR GT35 6414 3710 1233 67Haitian CreoleOCR GT26 799 170 520 33Figure 3 Summary of Language-Specific Results(percentages)Comparing the Two PassesWith an OCR package not trained for thisspecific language and an MT engine from aresearch effort, the embedded MT system withthese components does not assist the human onthe filtering task.
And even with the ground-truth input, the MT engine is not sufficientlyrobust to produce useful translations of walidHaitian Creole words.5 Cross-System ResultsIn figure 3 we compare the three language-specific systems, we make use of four measuresderived from the basic counts, A. through M., asdefined in figure 2.W.
Original Doeument-MT Word Recall% of original SL document words translatableby the MT engine after being OCR-ed.
(D/A)This measure on the GT pass in all 3 systemsgives us the proportion of words in the originalSL document hat are in the individual lVITlexicons.
The Spanish lexicon is strong for thedomain of our document (W -- 95%).
Themeasures for Arabic and Haitian Creole reflectthe fact that their MT lexicons are still underdevelopment (W -- 64% and 79%, respectively).This measure on the OCR pass, given thecorresponding measure on the GT pass as abaseline, captures the degradation i troduced bythe Scan-OCR processing of the document.From figure 3 we see that the Spanish systemloses approximately 55% of its originaldocument words going into the MT engine (95%minus 40%), the Haitian Creole 53% (79%minus 26%), and the Arabic 29% (64% minus35%).
Recall that the Spanish and HaitianCreole systems included the same OCRsoftware, which may account for the similarlevel of performance h re.
This software was notavailable to us for Arabic.X.
MT Semantic Adequacy% of TL words generated by MT engine that areopen class & semantically adequate in theirtranslation (J/G )This measure is intended to assess whether asystem can be used for filtering broad-leveltopics (in contrast o domains with specializedvocabulary that we discuss below).
Here we seeevidence for two patterns that recur in the twomeasures below.
First, the GT pass---with oneexception---exhibits better performance than theOCR pass.
Second, there is a ranking of thesystems with Haitian Creole at the low end,Arabic in the middle, and Spanish at the highend.
We will need more data to determine thesignificance of the one exception (55% versus49%).Y.
MT Domain-Relevant Adequacy% of TL words generated by MT engine that areopen class, semantically adequate in theirtranslation, and domain-relevant (L/G)In all of the systems there was a slight gain indomain-relevant faltering performance from theOCR pass to the GT pass.
We can rank thesystems with the Haitian Creole at the low end,the Arabic in the middle, and the Spanish at thehigh end: the measures in both the OCR and GTpasses in Haitian Creole are lower than in theArabic, which are lower than in the Spanish.Only the Spanish documents, but not the Arabicor Haitian Creole ones, when machine translatedin either pass were judged domain-relevant byfive people dunng an informal test.
14 Thus, ourdata suggests that the Spanish system's lowerbound (OCR pass) of 17% on this measure isneeded for faltering.Z.
MT Open Class Semantic Adequacy% of open class TL words generated by MTengine that are semantically adequate in theirtranslation (J/H)14 We are in the process of running an experiment tovalidate the protocol for establishing domain-relevantjudgments as part of our research in measures ofeffectiveness (MOEs) for task-based valuation.6The same pattern emerges with this measure.
Ineach system there is an improvement inperformance stepping from the OCR pass to theGT pass.
Across systems we see the sameranking, with the OCR and GT passes of theHaitian Creole falling below the Arabic whichfalls below the Spanish.Conclusion and Future WorkOur main contribution has been the proposal ofan end-to-end process analysis template and areplicable valuation methodology.
We presentmeasures to evaluate filtering performance andpreliminary results on Spanish, Arabic andHaitian Creole FALCon systems.The cross-system comparisons using themeasures presented, with one exception, yieldedthe following expected rankings: (i) the GT-MTpass exhibits better performance than the Scan-OCR-MT pass and (ii) the Haitian Creolesystem is at the low end, Arabic is in the middle,and Spanish is at the high end.Our long-term objective is to compare theresults of the system-internal "measures ofperformance" (MOPs) presented here withresults we still need from system-external"measures of effectiveness" (MOEs)25 MOE-based methods evaluate (i) baseline unaidedhuman performance, (ii) human performanceusing a new system and (iii) human expertperformance.
From this comparison we will beable to determine whether these twoindependently derived sets of measures arereplicable and validate each other.
So far, wehave only addressed our original question,"when is an embedded MT system good enoughfor filtering?"
in terms of MOPs.
We found that,for our particular passage in the medical domain,documents need to reach at least 17% on ourderived measure Y., MT domain-relevantadequacy (recall discussion of derived measureY, in section 5).Given that all but one process tep ("ID wrongTL words" as shown in figure 1 where a humanstick figure appears) in filling the template canbe automated, the next phase of this work willbe to create a software tool to speed up andsystematize this process, improving our systemevaluation by increasing the number of15 See Roche and Watts (1991) for definitions ofthese terms.documents hat can be regularly Used to test eachnew system and reducing the burden on theoperational linguists who assist us for the onecritical step.
Currently available tools forparallel text processing, including text alignmentsoftware, may provide new user interfaceoptions as well, improving the interactiveassessment process and possibly extending theinput set to include transcribed speech.AcknowledgementsWe would like to acknowledge Lisa Decrozant(Army Research Laboratory) and BrianBranagan (Department of Defense) for languageexpertise and Francis Fisher (Army ResearchLaboratory) for systems engineering expertise.ReferencesChurch, K. and Hovy, E. 1993.
Good Applicationsfor Crummy Machine Translation.
MachineTranslation, Volume 8, pages 239 - 258.DARPA 1999.
Translingual Information Detection,Extraction, and Summarization (TIDES) Initiative.http://www.darpa.mil/ito/research/tides/index.htmlFisher, F. and Voss, C. R. 1997.
"FALCon, an MTSystem Support Tool for Non-linguists."
InProceedings of the Advanced InformationProcessing and Analysis Conference.
McLean,VA.Harmon, D. 1999.
"A Framework for Evaluation inTIDES."
Presentation at TIDES PlanningWorkshop, with link at http://www.dyncorp-is.com/darpa/meetings/fides99jul/agenda.html, July28-30, Leesburg, VA.Pomarede, J.-M., Taylor, K., and Van Ess-Dykema,C.
1998.
Sparse Training Data and EBMT.
InProceedings of the Workshop on Embedded MTSystems: Design, Construction, and Evaluation ofSystems with an MT Component held inconjunction with the Association for MachineTranslation in  the Americas (AMTA'98),Langhorne, PA, October.Resnik, P. 1997.
Evaluating Multilingual Gisting ofWeb Pages.
In Working Notes of the AAA1 SpringSymposium on Natural Language Processing forthe Worm Wide Web, Palo Alto, CA.Roche, J. G. and Watts, B. D. 1991.
ChoosingAnalytic Measures.
The Journal of StrategicStudies, Volume 14, pages 165-209, June.Sparck Jones, K. and Galliers, J.
1996.
EvaluatingNatural Language Processing Systems.
Springer-Verlag Publishers, Berlin, Germany.Taylor, K. and White, J.
1998.
Predicting What MTis Good for: User Judgments and TaskPerformance.
In Proceedings of the Third7Conference of the Association for MachineTranslation in the Americas (AMTA'98), pages364 -373, Langhome, PA, October.Voss, C. R. and Reeder, F.
(eds.).
1!998.Proceedings of the Workshop on Embedded MTSystems: Design, Construction, and Evaluation ofSystems with an MT Component held inconjunction with the Association for MachineTranslation in the Americas (AMTA'98),Langhorne, PA, October.Voss, C. R. and Van Ess-Dykema, C. 2000.Evaluating Scan-OCR-MT Processing for theFiltering Task.
Army Research LaboratoryTechnical Report, Adelphi, MD.8
