Proceedings of the 2014 ACL Workshop on Cognitive Modeling and Computational Linguistics, pages 10?18,Baltimore, Maryland USA, June 26 2014.c?2014 Association for Computational LinguisticsInvestigating the role of entropy in sentence processingTal LinzenDepartment of LinguisticsNew York Universitylinzen@nyu.eduT.
Florian JaegerBrain and Cognitive SciencesUniversity of Rochesterfjaeger@bcs.rochester.eduAbstractWe outline four ways in which uncertaintymight affect comprehension difficulty inhuman sentence processing.
These fourhypotheses motivate a self-paced readingexperiment, in which we used verb sub-categorization distributions to manipulatethe uncertainty over the next step in thesyntactic derivation (single step entropy)and the surprisal of the verb?s comple-ment.
We additionally estimate word-by-word surprisal and total entropy overparses of the sentence using a probabilisticcontext-free grammar (PCFG).
Surprisaland total entropy, but not single step en-tropy, were significant predictors of read-ing times in different parts of the sen-tence.
This suggests that a complete modelof sentence processing should incorporateboth entropy and surprisal.1 IntroductionPredictable linguistic elements are processedfaster than unpredictable ones.
Specifically, pro-cessing load on an element A in context C is lin-early correlated with its surprisal, ?
log2P (A|C)(Smith and Levy, 2013).
This suggests that read-ers maintain expectations as to the upcoming ele-ments: likely elements are accessed or constructedin advance of being read.
While there is substan-tial amount of work on the effect of predictabilityon processing difficulty, the role (if any) of the dis-tribution over expectations is less well understood.Surprisal predicts that the distribution overcompeting predicted elements should not affectreading times: if the conditional probability of aword A is P (A|C), reading times on the wordwill be proportional to ?
log2P (A|C), regardlessof whether the remaining probability mass is dis-tributed among two or a hundred options.The entropy reduction hypothesis (Hale,2003; Hale, 2006), on the other hand, accordsa central role to the distribution over predictedparses.
According to this hypothesis, an incom-ing element is costly to process when it entails achange from a state of high uncertainty (e.g., mul-tiple equiprobable parses) to a state of low uncer-tainty (e.g., one where a single parse is much morelikely than the others).
Uncertainty is quantifiedas the entropy of the distribution over completeparses of the sentence; that is, if Aiis the set ofall possible parses of the sentence after word wi,then the uncertainty following wiis given byHwi= ?
?a?AiP (a) log2P (a) (1)Processing load in this hypothesis is propor-tional to the entropy reduction caused by wn:1ER(wn) = max{Hwn?1?Hwn, 0} (2)A third hypothesis, which we term the com-petition hypothesis, predicts that higher compe-tition among potential outcomes should result inincreased processing load at the point at whichthe competing parses are still valid (McRae et al.,1998; Tabor and Tanenhaus, 1999).
This contrastswith the entropy reduction hypothesis, accordingto which processing cost arises when competitionis resolved.
Intuitively, the two hypotheses makeinversely correlated predictions: on average, therewill be less competition following words that re-duce entropy.
A recent study found that readingtimes on wicorrelated positively with entropy fol-lowing wi, providing support for this hypothesis(Roark et al., 2009).The fourth hypothesis we consider, which weterm the commitment hypothesis, is derived from1No processing load is predicted for words that increaseuncertainty.10startabcdef0.50.50.250.250.250.250.250.75Figure 1: Example language.
Output strings are indicatedinside the nodes, and transition probabilities are indicated onthe edges.
For example, the probability of the sentence bf is0.5?
0.75.the event-related potential (ERP) literature on con-textual constraint.
Studies in this tradition havecompared the responses to a low-predictabilityword across two types of context: high-constraintcontexts, in which there is a strong expectation fora (different) word, and low-constraint ones, whichare not strongly predictive of any individual word.There is increasing evidence for an ERP compo-nent that responds to violations of a strong pre-diction (Federmeier, 2007; Van Petten and Luka,2012).
This component can be interpreted as re-flecting disproportional commitment to high prob-ability predictions at the expense of lower proba-bility ones, a more extreme version of the proposalthat low-probability parses are pruned in the pres-ence of a high-probability parse (Jurafsky, 1996).Surprisal is therefore expected to have a larger ef-fect in high constraint contexts, in which entropywas low before the word being read.
Commitmentto a high probability prediction may also result inincreased processing load at the point at which thecommitment is made.We illustrate these four hypotheses using thesimple language sketched in Figure 1.
Considerthe predictions made by the four hypotheses forthe sentences ae and be.
Surprisal predicts no dif-ference in reading times between these sentences,since the conditional probabilities of the words inthe two sentences are identical (0.5 and 0.25 re-spectively).The competition hypothesis predicts increasedreading times on the first word in ae compared tobe, because the entropy following a is higher thanthe entropy following b (2 bits compared to 0.71).Since all sentences in the language are two wordlong, entropy goes down to 0 after the second wordin both sentences.
This hypothesis therefore doesnot predict a reading time difference on the secondword e.Moving on to the entropy reduction hypothesis,five of the six possible sentences in the languagehave probability 0.5?
0.25, and the sixth one (bf )has probability 0.5?
0.75.
The full entropy of thegrammar is therefore 2.4 bits.
The first word re-duces entropy in both ae and be (to 2 and 0.71 bitsrespectively), but entropy reduction is higher whenthe first word is b.
The entropy reduction hypoth-esis therefore predicts longer reading times on thefirst word in be than in ae.
Conversely, since en-tropy goes down to 0 in both cases, but from 2 bitsin ae compared to 0.71 bits in be, this hypothesispredicts longer reading times on e in ae than in be.Finally, the commitment hypothesis predictsthat after b the reader will become committed tothe prediction that the second word will be f .
Thiswill lead to longer reading times on e in be thanin ae, despite the fact that its conditional proba-bility is identical in both cases.
If commitment toa prediction entails additional work, this hypothe-sis predicts longer reading times on the first wordwhen it is b.This paper presents an reading time study thataims to test these hypotheses.
Empirical testsof computational theories of sentence processinghave employed either reading time corpora (Dem-berg and Keller, 2008) or controlled experimen-tal materials (Yun et al., 2010).
The current paperadopts the latter approach, trading off a decreasein lexical and syntactic heterogeneity for increasedcontrol.
This paper is divided into two parts.
Sec-tion 2 describes a reading time experiment, whichtested the predictions of the surprisal, competi-tion and commitment hypotheses, as applied to theentropy over the next single step in the syntacticderivation.2We then calculate the total entropy(up to an unbounded number of derivation steps)at each word using a PCFG; Section 3 describeshow this grammar was constructed, overviews thepredictions that it yielded in light of the four hy-potheses, and evaluates these predictions on the re-sults of the reading time experiment.2We do not test the predictions of the entropy reductionhypothesis in this part of the paper, since that theory explicitlyonly applies to total rather than single-step entropy.112 Reading time experiment2.1 DesignTo keep syntactic structure constant while ma-nipulating surprisal and entropy over the nextderivation step, we took advantage of the factthat verbs vary in the probability distributionof their syntactic complements (subcategorizationframes).
Several studies have demonstrated thatreaders are sensitive to subcategorization probabil-ities (Trueswell et al., 1993; Garnsey et al., 1997).The structure of the experimental materials isshown in Table 1.
In a 2x2x2 factorial design, wecrossed the surprisal of a sentential complement(SC) given the verb, the entropy of the verb?s sub-categorization distribution, and the presence or ab-sence of the complementizer that.
When the com-plementizer is absent, the region the island is am-biguous between a direct object and an embeddedsubject.Surprisal theory predicts an effect of SC sur-prisal on the disambiguating region in ambiguoussentences (sentences without that), as obtained inprevious studies (Garnsey et al., 1997), and an ef-fect of SC surprisal on the complementizer thatin unambiguous sentences.
Reading times shouldnot differ at the verb: in the minimal context weused (the men), the surprisal of the verb shouldbe closely approximated by its lexical frequency,which was matched across conditions.The competition hypothesis predicts a positivemain effect of subcategorization frame entropy(subcategorization frame entropy) at the verb:higher uncertainty over the syntactic category ofthe complement should result in slower readingtimes.The commitment hypothesis predicts that theeffect of surprisal in the disambiguating regionshould be amplified when subcategorization frameentropy is low, since the readers will have commit-ted to the competing high probability frame.
If thecommitment step in itself incurs a processing cost,there should be a negative main effect of subcate-gorization frame entropy at the verb.This experimental design varies the entropyover the single next derivation step: it assumesthat the parser only predicts the identity of the sub-categorization frame, but not its internal structure.Since the predictions of the entropy reduction hy-pothesis crucially depend on predicting the inter-nal structure as well, we defer the discussion ofthat hypothesis until Section 3.The men discovered (that) the islandmat.
subj.
verb that emb.
subj.had been invaded by the enemy.emb.
verb complex restTable 1: Structure of experimental materials (mat.
= matrix,emb.
= embedded, subj.
= subject).2.2 Methods2.2.1 Participants128 participants were recruited through AmazonMechanical Turk and were paid $1.75 for theirparticipation.2.2.2 Materials32 verbs were selected from the Gahl et al.
(2004)subcategorization frequency database, in 4 con-ditions: high vs. low SC surprisal and high vs.low subcategorization frame entropy (see Table 2).Verbs were matched across conditions for lengthin characters and for frequency in SUBTLEX-UScorpus (Brysbaert and New, 2009).
A sentencewas created for each verb, following the structurein Table 1.
Each sentence had two versions: onewith the complementizer that after the verb andone without it.
The matrix subjects were mini-mally informative two-word NPs (e.g.
the men).Following the complementizer (or the verb, if thecomplementizer was omitted) was a definite NP(the island), which was always a plausible directobject of the matrix verb.The embedded verb complex region consistedof three words: two auxiliary verbs (had been) oran auxiliary verb and negation (would not), fol-lowed by a past participle form (invaded).
Eachof the function words appeared the same num-ber of times in each condition.
The embeddedverb complex was followed by three more words.The nouns and verbs in the embedded clause werematched for frequency and length across condi-tions.In addition to the target sentences, the exper-iment contained 64 filler sentences, with variouscomplex syntactic structures.2.2.3 ProcedureThe sentences were presented word by word in aself-paced moving window paradigm.
The partic-ipants were presented with a Y/N comprehensionquestion after each trial.
The participants did not12NP Inf PP SC SC s. SFEforget 0.55 0.14 0.2 0.09 3.46 1.7hear 0.72 0 0.17 0.11 3.22 1.12claim 0.36 0.12 0 0.45 1.15 1.71sense 0.61 0 0.02 0.34 1.55 1.18Table 2: A example verb from each of the four conditions.On the left, probabilities of complement types: noun phrase(NP), infinitive (Inf), prepositional phrase (PP), sententialcomplement (SC); on the right, SC surprisal and subcatego-rization frame entropy.receive feedback on their responses.
The experi-ment was conducted online using a Flash applica-tion written by Harry Tily (now at Nuance Com-munications).2.2.4 Statistical analysisSubjects were excluded if their answer accuracywas lower than 75% (two subjects), or if theirmean reading time (RT) differed by more than2.5 standard deviations from the overall mean RTacross subjects (two subjects).
The results re-ported in what follows are based on the remaining124 subjects (97%).We followed standard preprocessing procedure.Individual words were excluded if their raw RTwas less than 100 ms or more than 2000 ms, or ifthe log-transformed RT was more than 3 standarddeviations away from the participant?s mean.
LogRTs were length-corrected by taking the residualsof a mixed-effects model (Bates et al., 2012) thathad log RT as the response variable, word lengthas a fixed effect, and a by-subject intercept andslope.The length-corrected reading times were re-gressed against the predictors of interest, sepa-rately for each region.
We used a maximal randomeffect structure.
All p values for fixed effects werecalculated using model comparison with a simplermodel with the same random effect structure thatdid not contain that fixed effect.2.3 ResultsReading times on the matrix subject (the men) ormatrix verb (discovered) did not vary significantlyacross conditions.The embedded subject the island was read fasterin unambiguous sentences (p < 0.001).
Read-ing times on this region were longer when SC sur-prisal was high (p = 0.04).
Models fitted to am-biguous and unambiguous sentences separately re-vealed that the simple effect of SC surprisal on theembedded subject was significant for unambigu-ous sentences (p = 0.02) but not for ambiguoussentences (p = 0.46), though the interaction be-tween SC surprisal and ambiguity did not reachsignificance (p = 0.22).The embedded verb complex (had been in-vaded) was read faster in unambiguous than in am-biguous sentences (p < 0.001).
Reading timesin this region were longer overall in the high SCsurprisal condition (p = 0.03).
As expected, thiseffect interacted with the presence of that (p =0.01): the simple effect of SC surprisal was notsignificant in unambiguous sentences (p = 0.28),but was highly significant in ambiguous ones (p =0.007).
We did not find an interaction between SCsurprisal and subcategorization frame entropy (ofthe sort predicted by the commitment hypothesis).Subcategorization frame entropy did not have asignificant effect in any of the regions of the sen-tence.
It was only strictly predicted to have an ef-fect on the matrix verb: longer reading times ac-cording to the competition hypothesis, and (possi-bly) shorter reading times according to the com-mitment hypothesis.
The absence of an subcat-egorization frame entropy effect provides weaksupport for the predictions of surprisal theory, ac-cording to which entropy should not affect readingtimes.3 Deriving predictions from a PCFG3.1 Calculating entropyAs mentioned above, the entropy of the nextderivation step following the current word (whichwe term single-step entropy) is calculated as fol-lows.
If aiis a nonterminal, ?iis the set of rulesrewriting ai, and pris the application probabilityof rule r, then the single-step entropy of aiis givenbyh(ai) = ??r?
?iprlog2pr(3)discoverNP (14 bits)SC (50 bits)0.50.5Figure 3: Entropy calculation example: the single step en-tropy after discover is 1 bit; the overall entropy is 1 + 0.5 ?14 + 0.5?
50 = 33 bits.13Ambiguous Unambiguous?0.10?0.050.000.05Themendiscovered theisland hadbeeninvaded by the Themendiscoveredthattheisland hadbeeninvaded by theSentential complementsurprisalLowHighSubcategorizationentropyLowHighFigure 2: Results of the self-paced reading experimentThe entropy of all derivations starting with ai(which we term total entropy) is then given by thefollowing recurrence:H(ai) = h(ai) +?r?
?iprkr?j=1H(ar,j) (4)where ar,1, .
.
.
, ar,krare the nonterminals onthe right-hand side of r. This recurrence hasa closed form solution (Wetherell, 1980; Hale,2006).
The expectation matrix A is a square ma-trix with N rows and columns, where N is the setof nonterminals.
Each element Aijindicates theexpected number of times nonterminal ajwill oc-cur when aiis rewritten using exactly one rule ofthe grammar.
If h = (h1, .
.
.
, hN) is the vector ofall single-step entropy values for the N nontermi-nal types in the grammar, andH = (H1, .
.
.
,HN)is the vector of all total entropy values, then theclosed form solution for the recurrence is given byH = (I ?A)?1h (5)where I is the identity matrix.
The entropy af-ter the first n words of the sentence, Hwn, can becalculated by applying Equation 5 to the grammarformed by intersecting the original grammar withthe prefix w1, .
.
.
, wn(i.e., considering only theparses that are compatible with the words encoun-tered so far) (Hale, 2006).Two points are worth noting about these equa-tions.
First, Equation 5 shows that calculating theentropy of a PCFG requires inverting the matrixI ?
A, which is the size of the number of non-terminal symbols in the grammar.
This makes itimpractical to use a lexicalized grammar, as advo-cated by Roark et al.
(2009), since those grammarshave a very large number of nonterminal types.Second, Equation 4 shows that the entropy of anonterminal is the sum of its single-step entropyand a weighted average of entropy of the nonter-minals it derives.
In the context of subcategoriza-tion decisions, the number of possible subcatego-rization frames is small, and the single-step en-tropy is on the order of magnitude of 1 or 2 bits.The entropy of a typical complement, on the otherhand, is much higher (consider all of the possibleinternal structures that an SC could have).
Thismeans that the total entropy H after processingthe verb is dominated by the entropy of its po-tential complements rather than the verb?s single-step entropy h (see Figure 3 for an illustration).
Alookahead of a single word (as used in Roark etal.
(2009)) may therefore be only weakly relatedto total entropy.3.2 Constructing the grammarWe used a PCFG induced from the Penn Treebank(Marcus et al., 1993).
As mentioned above, thegrammar was mostly unlexicalized; however, inorder for the predictions to depend on the identityof the verb, the grammar had to contain lexicallyspecific rules for each verb.
We discuss these rulesat end of this section.The Penn Treebank tag set is often expandedby adding to each node?s tag an annotation of the14node?s parent, e.g., marking an NP whose parentis a VP as NP VP (Klein and Manning, 2003).While systematic parent annotation would have in-creased the size of the grammar dramatically, wedid take the following minimal steps to improveparsing accuracy.
First, the word that is taggedin the Penn Treebank as a preposition (IN) whenit occurs as a subordinating conjunction.
This re-sulted in SCs being erroneously parsed as preposi-tional phrases.
To deal with this issue, we replacedthe generic IN with IN[that] whenever it referredto that.Second, the parser assigned high probabilityparses to reduced relative clauses in implausiblecontexts.
We made sure that cases that should notbe reduced relative clauses were not parsed as suchby splitting the VP category into sub-categoriesbased on the leftmost child of the VP (since onlyVP[VPN] should be able to be a reduced rela-tive), and by splitting SBAR into SBAR[overt]when the SBAR had an overt complementizer andSBAR[none] when it did not.Following standard practice, we removed gram-matical role information and filler-gap annota-tions, e.g., NP-SUBJ-2 was treated as NP.
To re-duce the number of rules in the grammar as muchas possible, we removed punctuation and the silentelement NONE (used to mark gaps, silent comple-mentizers, etc.
), rules that occurred less than 100times (out of the total 1320490 nonterminal pro-ductions), and rules that had a probability of lessthan 0.01.
These steps resulted in the removal of13%, 14% and 10% rule tokens respectively.
Wethen applied horizontal Markovization (Klein andManning, 2003).Finally, we added lexically specific rules tocapture the verbs?
subcategorization preferences,based on the Gahl et al.
(2004) subcategorizationdatabase.
The probability of frame fjfollowingverb viwas calculated as:P (VP[VBD]?
vifj) =12P (vi)P (fj|vi)?iP (vi)(6)In other words, half of the probability mass ofproduction rules deriving VP[VBD] (VP headedby past tense verbs) was taken away from the un-lexicalized rules and assigned to the verb-specificrules.
The same procedure was performed forVP[VBN] (VP headed by a past participle, withthe exception of the verbs forgot and wrote, whichare not ambiguous between the past and past par-ticiple forms.
The total probability of all rules de-riving VP as a specific verb (e.g., discovered) wasestimated as the corpus frequency of that verb di-vided by the total corpus frequency of all 32 verbsused in the experiment, yielding a normalized es-timate of the relative frequency of that verb.3.3 Surprisal, entropy and entropy reductionprofilesWord-by-word surprisal, entropy and entropy re-duction values for each item were derived from theequations in Section 3.1 using the Cornell Con-ditional Probability Calculator (provided by JohnHale).
Figure 4 shows the predictions averaged bythe conditions of the factorial design.
Surprisal onthe verb is always high because this is the only partof the grammar that encodes lexical identity; sur-prisal on the verb therefore conflates lexical andsyntactic surprisal.
Surprisal values on all otherwords are low, with the exception of the pointat which the reader gets the information that theverb?s complement is an SC: the embedded verbcomplex in ambiguous sentences, and the comple-mentizer in unambiguous sentence.The entropy profile is dominated by the fact thatSCs have much higher internal entropy than NPs.As a consequence, entropy after the verb is higherwhenever an SC is a more likely subcategorizationframe.
The entropy after high subcategorizationframe entropy verbs is higher than that after lowsubcategorization frame entropy verbs, though thedifference is small in comparison to the effect ofSC surprisal.
In ambiguous sentences, entropy re-mains higher for low SC surprisal verbs through-out the ambiguous region.
Somewhat counterin-tuitively, entropy increases when the parse is dis-ambiguated in favor of an SC.
This is again aconsequence of the higher internal entropy of aSC: the entropy of the ambiguity between SC andNP is dwarfed by the internal entropy of a SC.The entropy profile for unambiguous sentencesis straightforward: it increases sharply when thereader finds out that the complement is a SC, thendecreases gradually as more details are revealedabout the internal structure of the SC.The reading time predictions made by the en-tropy reduction hypothesis are therefore very dif-ferent than those made by surprisal theory.
Onthe verb, the entropy reduction hypothesis predictsthat high SC surprisal verbs will be read more15Ambiguous Unambiguouslll lllllllllllllllllllllllllll lllllllllllll lllllllllllllll lllllllll ll llllll010102001020SurprisalEntropyEnt.
Reductionmendiscovered theisland hadbeeninvadedmendiscoveredthattheisland hadbeeninvadedSentential complementsurprisalLowHighSubcategorizationentropyllLowHighFigure 4: Parser-derived surprisal, entropy and entropy reduction estimates for the stimuli in our experiments, averaged withineach condition of the factorial design (first word of sentence and rest region excluded).slowly than low SC surprisal verbs, whereas sur-prisal predicts no difference.
On the disambiguat-ing region in ambiguous sentences, the entropy re-duction hypothesis predicts no reading time dif-ferences at all, since an increase in entropy isnot predicted to affect reading times.
In fact, en-tropy reduction on the word had is positive only inunambiguous sentences, so the entropy reductionhypothesis predicts a slowdown in unambiguouscompared to ambiguous sentences.3.4 Evaluation on reading timesWe tested whether reading times could be pre-dicted by the word-by-word estimates derivedfrom the PCFG.
Since total entropy, entropy re-duction and surprisal values did not line up withthe factorial design, we used continuous regres-sion instead, again using lme4 with a maximal ran-dom effects structure.
We only analyzed wordsfor which the predictions depended on the prop-erties of the verb (as Figure 4 shows, this is onlythe case for a minority of the words).
As outcomevariables, we considered both reading times on theword wi, and a spillover variable computed as thesum of the reading times on wiand the next wordwi+1.
The predictors were standardized (sepa-rately for each word) to facilitate effect compar-ison.Parser-derived entropy reduction values variedthe most on the main verb.
Since the word follow-ing the verb differs between the ambiguous andunambiguous conditions, we added a categoricalcontrol variable for sentence ambiguity.
In theresulting model, lower entropy (or equivalently,higher entropy reduction values), caused an in-crease in reading times (no spillover:??
= 0.014,p = 0.05; one word spillover:??
= 0.022, p =0.04).
Our design does not enable us to determinewhether the effect of entropy on the verb is due toentropy reduction or simply entropy.
The commit-ment hypothesis is therefore equally supported bythis pattern as is the entropy reduction hypothesis.The only other word on which entropy reduc-tion values varied across verbs was the first wordthe of the ambiguous region.
Neither entropy re-duction nor surprisal were significant predictors ofreading times on this word.There was also some variation across verbs inentropy (though not entropy reduction) on the sec-ond word of the embedded subject (island) in am-biguous sentences; however, entropy was not asignificant predictor of reading times on that word.In general, entropy is much higher in the embed-16ded subject region in unambiguous than ambigu-ous sentences, since it is already known that thecomplement is an SC, and the entropy of an SCis higher.
Yet as mentioned above, reading timeson the embedded subject were higher when it wasambiguous (p < 0.001).Finally, PCFG-based surprisal was a significantpredictor of reading times on the disambiguatingword in ambiguous sentences (no spillover: n.s.
;one word spillover:??
= 0.037, p = 0.02; two-word spillover:??
= 0.058, p = 0.001).
In con-trast with simple SC surprisal (see Section 2.2.4),PCFG-based surprisal was not a significant predic-tor of reading times on the complementizer that inunambiguous sentences.4 DiscussionWe presented four hypotheses as to the role of en-tropy in syntactic processing, and evaluated themon the results of a reading time study.
We did notfind significant effects of subcategorization frameentropy, which is the entropy over the next deriva-tion step following the verb.
Entropy over com-plete derivations, on the other hand, was a signifi-cant predictor of reading time on the verb.
The ef-fect went in the direction predicted by the entropyreduction and commitment hypotheses, and oppo-site to that predicted by the competition hypothe-sis: reading times were higher when post-verb en-tropy was lower.Reading times on the embedded subject in am-biguous sentences were increased compared to un-ambiguous sentences.
This can be seen as sup-porting the competition hypothesis: the SC andNP parses both need to be maintained, which in-creases processing cost.
Yet the parser predic-tions showed that total entropy on the embeddedsubject was higher in unambiguous than ambigu-ous sentences, since the probability of the high-entropy sentential complement is 1 in unambigu-ous sentences.
In this case, then, total entropy,which entails searching enormous amounts of pre-dicted structure, may not be the right measure, andsingle-step (or n-step) entropy may be a better pre-dictor.In related work, Frank (2013) tested a version ofthe entropy reduction hypothesis whereby entropyreduction was not bounded by 0 (was allowed totake negative values).
A Simple Recurrent Net-work was used to predict the next four words inthe sentence; the uncertainty following the currentword was estimated as the entropy of this quadri-gram distribution.
Higher (modified) entropy re-duction resulted in increased reading times.
Theseresults are not directly comparable to the presentresults, however.
Frank (2013) tested a theory thattakes into account both positive and negative en-tropy changes.
In addition, a four-word lookaheadmay not capture the dramatic difference in internalentropy between SCs and NPs, which is responsi-ble for the differential reading times predicted onthe matrix.
This caveat applies even more stronglyto the one-word lookahead in Roark et al.
(2009).In contrast with much previous work, we cal-culated total entropy using a realistic PCFG ac-quired from a Treebank corpus.
In future work,this method can be used to investigate the ef-fect of entropy in a naturalistic reading time cor-pus.
It will be important to explore the extent towhich the reading time predictions derived fromthe grammar are affected by representational de-cisions (e.g., the parent annotations we used inSection 3.2).
This applies in particular to entropy,which is sensitive to the distribution over syntacticparses active at the word; surprisal depends onlythe conditional probability assigned to the wordby the grammar, irrespective of the number anddistribution over the parses that predict the currentword, and is therefore somewhat less sensitive torepresentational assumptions.5 ConclusionThis paper described four hypotheses regardingthe role of uncertainty in sentence processing.
Areading time study replicated a known effect ofsurprisal, and found a previously undocumentedeffect of entropy.
Entropy predicted reading timesonly when it was calculated over complete deriva-tions of the sentence, and not when it was calcu-lated over the single next derivation step.
Our re-sults suggest that a full theory of sentence process-ing would need to take both surprisal and uncer-tainty into account.AcknowledgmentsWe thank Alec Marantz for discussion and An-drew Watts for technical assistance.
This workwas supported by an Alfred P. Sloan Fellowshipto T. Florian Jaeger.17ReferencesD.
Bates, M. Maechler, and B. Bolker, 2012. lme4:Linear mixed-effects models using S4 classes.
Rpackage version 0.999999-0.M.
Brysbaert and B.
New.
2009.
Moving beyondKu?cera and Francis: A critical evaluation of cur-rent word frequency norms and the introduction ofa new and improved word frequency measure forAmerican English.
Behavior Research Methods,41(4):977?990.V.
Demberg and F. Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109(2):193?210.K.
D. Federmeier.
2007.
Thinking ahead: The roleand roots of prediction in language comprehension.Psychophysiology, 44(4):491?505.S.
L. Frank.
2013.
Uncertainty reduction as a measureof cognitive load in sentence comprehension.
Topicsin Cognitive Science, 5(3):475?494.S.
Gahl, D. Jurafsky, and D. Roland.
2004.
Verbsubcategorization frequencies: American Englishcorpus data, methodological studies, and cross-corpus comparisons.
Behavior Research Methods,36(3):432?443.S.
Garnsey, N. Pearlmutter, E. Myers, and M. Lotocky.1997.
The contributions of verb bias and plausi-bility to the comprehension of temporarily ambigu-ous sentences.
Journal of Memory and Language,37(1):58?93.J.
Hale.
2003.
The information conveyed by wordsin sentences.
Journal of Psycholinguistic Research,32(2):101?123.J.
Hale.
2006.
Uncertainty about the rest of the sen-tence.
Cognitive Science, 30(4):643?672.D.
Jurafsky.
1996.
A probabilistic model of lexicaland syntactic access and disambiguation.
CognitiveScience, 20(2):137?194.D.
Klein and C. D. Manning.
2003.
Accurate un-lexicalized parsing.
In Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics-Volume 1, pages 423?430.
Associationfor Computational Linguistics.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.K.
McRae, M. Spivey-Knowlton, and M. Tanenhaus.1998.
Modeling the influence of thematic fit (andother constraints) in on-line sentence comprehen-sion.
Journal of Memory and Language, 38(3):283?312.B.
Roark, A. Bachrach, C. Cardenas, and C. Pallier.2009.
Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling viaincremental top-down parsing.
In Proceedings ofthe 2009 Conference on Empirical Methods in Nat-ural Language Processing: Volume 1-Volume 1,pages 324?333.
Association for Computational Lin-guistics.N.
J. Smith and R. Levy.
2013.
The effect of wordpredictability on reading time is logarithmic.
Cog-nition, 128(3):302?319.W.
Tabor and M. K. Tanenhaus.
1999.
Dynamicalmodels of sentence processing.
Cognitive Science,23(4):491?515.J.
Trueswell, M. Tanenhaus, and C. Kello.
1993.
Verb-specific constraints in sentence processing: Sep-arating effects of lexical preference from garden-paths.
Journal of Experimental Psychology: Learn-ing, Memory, and Cognition, 19(3):528?553.C.
Van Petten and B. Luka.
2012.
Prediction duringlanguage comprehension: Benefits, costs, and ERPcomponents.
International Journal of Psychophysi-ology, 83(2):176?190.C.
S. Wetherell.
1980.
Probabilistic languages: A re-view and some open questions.
ACM ComputingSurveys (CSUR), 12(4):361?379.J.
Yun, J. Whitman, and J. Hale.
2010.
Subject-objectasymmetries in Korean sentence comprehension.
InProceedings of the 32nd Annual Meeting of the Cog-nitive Science Society.18
