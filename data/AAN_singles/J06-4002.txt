Automatic Evaluation of InformationOrdering: Kendall?s TauMirella Lapata?University of EdinburghThis article considers the automatic evaluation of information ordering, a task underlying manytext-based applications such as concept-to-text generation and multidocument summarization.We propose an evaluation method based on Kendall?s ?, a metric of rank correlation.
The methodis inexpensive, robust, and representation independent.
We show that Kendall?s ?
correlatesreliably with human ratings and reading times.1.
IntroductionThe systematic evaluation of natural language processing (NLP) systems is an impor-tant prerequisite for assessing their quality and improving their performance.
Tradition-ally, human involvement is called for in evaluating systems that generate textual output.Examples include text generation, summarization, and, notably, machine translation.Human evaluations consider many aspects of automatically generated texts rangingfrom grammaticality to content selection, fluency, and readability (Teufel and vanHalteren 2004; Nenkova 2005; Mani 2001; White and O?Connell 1994).The relatively high cost of producing human judgments, especially when evalua-tions must be performed quickly and frequently, has encouraged many researchers toseek ways of evaluating system output automatically.
Papineni et al (2002) proposedBLEU, a method for evaluating candidate translations by comparing them against ref-erence translations (using n-gram co-occurrence overlap).
Along the same lines, thecontent of a system summary can be assessed by measuring its similarity to one ormore manual summaries (Hovy and Lin 2003).
Bangalore, Rambow, and Whittaker(2000) introduce a variety of quantitative measures for evaluating the accuracy of anautomatically generated sentence against a reference corpus string.Despite differences in application and form, automatic evaluation methods usuallyinvolve the following desiderata.
First, they measure numeric similarity or closenessof system output to one or several gold standards.
Second, they are inexpensive,robust, and ideally language independent.
Third, correlation with human judgmentsis an important part of creating and testing an automated metric.
For instance,several studies have shown that BLEU correlates with human ratings on machinetranslation quality (Papineni et al 2002; Doddington 2002; Coughlin 2003).
Bangalore,Rambow, and Whittaker (2000) demonstrate that tree-based evaluation metrics for?
School of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.E-mail: mlap@inf.ed.ac.ukSubmission received: 28 December 2005; accepted for publication: 6 May 2006.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 32, Number 4surface generation correlate significantly with human judgments on sentence qualityand understandability.
Given their simplicity, automatic evaluation methods cannot beconsidered as a direct replacement for human evaluations (see Callison-Burch, Osborne,and Koehn [2006] for discussion on some problematic aspects of BLEU).
However, theycan be usefully employed during system development, for example, for quicklyassessing modeling ideas or for comparing across different system configurations(Papineni et al 2002; Bangalore, Rambow, and Whittaker 2000).Automatic methods have concentrated on evaluation aspects concerning lexicalchoice (e.g., words or phrases shared between reference and system translations), con-tent selection (e.g., document units shared between reference and system summaries),and grammaticality (e.g., how many insertions, substitutions, or deletions are requiredto transform a generated sentence to a reference string).
Another promising, but,less studied, avenue for automatic evaluation is information ordering.
The task con-cerns finding an acceptable ordering for a set of preselected information-bearing items(Lapata 2003; Barzilay and Lee 2004).
It is an essential step in concept-to-text genera-tion, multidocument summarization, and other text synthesis problems.
Depending onthe application and domain at hand, the items to be ordered may vary greatly frompropositions (Karamanis 2003; Dimitromanolaki and Androutsopoulos 2003) to trees(Mellish et al 1998) or sentences (Lapata 2003; Barzilay and Lee 2004).
It is thereforenot surprising that evaluation methods have concentrated primarily on the generatedorders, thus abstracting away from the items themselves.More concretely, Lapata (2003) proposed the use of Kendall?s ?, a measure of rankcorrelation, as a means of estimating the distance between a system-generated and ahuman-generated gold-standard order.
Rank correlation is an appealing way of eval-uating information ordering: It is a well-understood and widely used measure of thestrength of association between two variables; it is computed straightforwardly andcan operate over distinct linguistic units (e.g., sentences, trees, or propositions).
Indeed,several studies have adopted Kendall?s ?
as a performance measure for evaluatingthe output of information-ordering components both in the context of concept-to-textgeneration (Karamanis and Mellish 2005; Karamanis 2003) and summarization (Lapata2003; Barzilay and Lee 2004; Okazaki, Matsuo, and Ishizuka 2004).Despite its growing popularity, no study to date has investigated whetherKendall?s ?
correlates with human judgments on the information-ordering task.
Thisis in marked contrast with other automatic evaluation methods that have been shownto correlate with human assessments.
In this article, we aim to rectify this and un-dertake two studies that examine whether there is indeed a relationship between ?and behavioral data.
We first briefly introduce Kendall?s ?
and explain how it can beemployed for evaluating information ordering (Section 2).
Next, we present a controlledexperimental study that examines whether Kendall?s ?
is correlated with human ratings(Section 3).A commonly raised criticism of the judgment elicitation methodology is that it isnot fine-grained enough to rule out possible confounds.
In the information-orderingtask, for example, we cannot be certain that subjects rate a document low because it isgenuinely badly organized and, therefore, difficult to comprehend or because they areunfamiliar with its content or simply disinterested or distracted.
Similar confounds alsoarise in the evaluation of the output of MT systems, where it may be difficult to teaseapart whether subjects?
ratings reflect their assessment of the quality of the translatedtext or its subject matter and structure.
To eliminate such confounds, we follow ourjudgment elicitation study with an on-line reading experiment and demonstrate that ?is also correlated with processing time (Section 4).
Our second experiment provides472Lapata Automatic Evaluation of Information Orderingadditional evidence for the validity of ?
as a measure of text well-formedness.
Discus-sion of our results concludes the article.2.
Kendall?s MeasureIn common with other automatic evaluation methods, we assume that we have accessto a reference output that in most cases will be created by one or several humans.
Ourtask is to compare a system-produced ordering of items against a reference order.
Forease of exposition, let us assume that our information-ordering component is part of ageneration application whose ultimate goal is to generate coherent and understandabletext.
It is not crucially important how the items to be ordered are represented.
They canbe facts in a database (Duboue and McKeown 2001), propositions (Karamanis 2003),discourse trees (Mellish et al 1998), or sentences (Lapata 2003; Barzilay and Lee 2004).Now, we can think of the items as objects for which a ranking must be produced.Table 1 gives an example of a reference text containing 10 items (A?J) and the orders(i.e., rankings) produced by two hypothetical systems.
We can then calculate how muchthe system orders differ from the reference order, the underlying assumption being thatacceptable orders should be fairly similar to the reference.
A number of metrics can beemployed for this purpose, such as Spearman?s correlation coefficient (rs) for rankeddata, Cayley distance, or Kendall?s ?
(see Lebanon and Lafferty [2002] for an overview).Here we describe Kendall?s ?
(Kendall 1938) and explain why it is an appropriate choicefor information-ordering tasks.Let Y = y1 .
.
.
yn be a set of items to be ranked.
Let ?
and ?
denote two distinctorderings of Y, and S(?,?)
the minimum number of adjacent transpositions needed tobring ?
to ?.
Kendall?s ?
is defined as:?
= 1 ?
2S(?,?
)N(N ?
1)/2 (1)where N is the number of objects (i.e., items) being ranked.
As can be seen, Kendall?s?
is based on the number transpositions, that is, interchanges of consecutive elements,necessary to rearrange ?
into ?.
In Table 1 the number of transpositions can be calculatedby counting the number of intersections of the lines.
The ?
between the Reference andSystem 1 is 0.82, between the Reference and System 2 is 0.24, and between the twosystems is 0.15.
The metric ranges from ?1 (inverse ranks) to 1 (identical ranks).
Thecalculation of ?
must be appropriately modified when there are tied rankings (Hays1994; Siegel and Castellan 1988).Kendall?s ?
seems particularly appropriate for the information-ordering tasks con-sidered in this article.
The metric is sensitive to the fact that some items may be alwaysTable 1Example of reference order and system orders for a text consisting of 10 items.A B C D E F G H I JReference 1 2 3 4 5 6 7 8 9 10System 1 2 1 5 3 4 6 7 9 8 10System 2 10 2 3 4 5 6 7 8 9 1473Computational Linguistics Volume 32, Number 4ordered next to each other even though their absolute orders might differ.
It also penal-izes inverse rankings.
Comparison between the Reference and System 2 gives a ?
of 0.24even though the orders between the two models are identical modulo the beginningand the end.
This seems appropriate given that flipping the introduction in a documentwith the conclusions seriously disrupts coherence.Kendall?s ?
is less widely used than Spearman?s rank correlation coefficient (rs).Both coefficients use the same amount of information in the data, and thus both havethe same sensitivity to detect the existence of association.
This means that for a givendata set, both measures will lead to rejection of the null hypothesis at the same level ofsignificance.
However, the two measures have different underlying scales, and, numeri-cally, they are not directly comparable to each other.
Siegel and Castellan (1988) expressthe relationship of the two measures in terms of the inequality:?1 ?
3??
2rs ?
1 (2)More importantly, Kendall?s ?
and rs have different interpretations.
Kendall?s ?can be interpreted as a simple function of the probability of observing concordantand discordant pairs (Kerridge 1975).
In other words, it is the difference between theprobability that in the observed data two variables are in the same order versus theprobability that they are in different orders (the probability is rescaled to range from ?1to 1 as is customary for correlation; see equation (1)).
Unfortunately, no simple meaningcan be attributed to rs.
The latter is the same as a Pearson product?moment correlationcoefficient (rp) computed for values consisting of ranks.
Although r2 represents thepercent of variance shared by two variables in the case of rp, its interpretation is lessstraightforward for rs, where it refers to the percent of variance of two ranks.
It isdifficult to draw any meaningful conclusions with regard to information ordering basedon the variance of ranks.In practice, while both correlations frequently provide similar answers, there aresituations where they diverge.
For example, the statistical distribution of ?
approachesthe normal distribution faster than rs (Kendall and Gibbons 1990), thus offering anadvantage for small to moderate sample studies with 30 or fewer data points.
This iscrucial when experiments are conducted with a small number of subjects (a situationcommon in NLP) or test items.
Another related issue concerns sample size.
Spearman?srank correlation coefficient is a biased statistic (Kendall and Gibbons 1990).
The smallerthe sample the more rs diverges from the true population value, usually underestimat-ing it.
In contrast, Kendall?s ?
does not provide a biased estimate of the true correlation.Furthermore, ?
maintains good control of type I error rates (i.e., rejecting the nullhypothesis when it is actually true).
Arndt, Turvey, and Andreasen (1999) undertakean extensive empirical study and show that the number of times ?
incorrectly signalsa significant correlation when there is none is close to the nominal 5% using a p < 0.05significance criterion.
For a more detailed discussion of the advantages of ?
over rs,we refer the interested reader to Kendall and Gibbons (1990) and Arndt, Turvey, andAndreasen (1999).3.
Experiment 1: Judgment ElicitationTo assess whether Kendall?s ?
reliably correlates with human ratings, it is necessaryto have access to several different orderings of the same input.
In what follows we474Lapata Automatic Evaluation of Information Orderingdescribe our method for assembling a set of experimental materials and collectinghuman judgments.3.1 Method3.1.1 Design and Materials.
Our goal here is to establish whether ?
correlates withhuman judgments on overall text understandability and coherence.
A system thatrandomly pastes together sentences or facts from a database will ultimately producea badly organized document lacking coherence.
A good automatic evaluation methodshould assign low values to such documents and higher values to documents that areeasy for humans to read and understand.We could elicit judgments by asking humans to rate the output of an information-ordering component.
The ratings could be then correlated with ?
values representingthe difference between system and reference orders.
Such a comparison is, however,undesirable for a number of reasons.
First, the system may be biased toward verygood or very bad orders.
This means that our hypothetical study would only exam-ine a restricted and potentially skewed range of ?
values.
Furthermore, in concept-to-text generation applications, information ordering typically operates over symbolicrepresentations that will be unfamiliar to naive informants and could potentially distorttheir judgments.
A related issue arises in text-to-text generation applications where theproduced documents are not necessarily grammatical, for example, when a summary isthe output of an information fusion component (Barzilay 2003; Radev and McKeown1998).
Again, it is difficult to control whether informants judge the ordering or thegrammaticality of the texts.To make the judgment task easier, we concentrated on a document representa-tion familiar to our participants, namely, sentences.
We simulated the output of aninformation-ordering component by randomly generating different sentence ordersfor a reference text.
We elicited judgments for eight texts of the same length (eightsentences).
The texts were randomly sampled from a corpus collected by Barzilayand Lee (2004) (sampling took place over eight-sentence-length documents only).
Thecorpus consists of Associated Press articles on the topic of natural disasters, drug-relatedcriminal offenses, clashes between armies and rebel groups, and narratives from the U.S.National Transportation Safety Board database.1A document consisting of eight sentences can be sequenced in 8!
ways.
We exhaus-tively enumerated all possible orderings and calculated their ?
value against the refer-ence order found in the corpus.2 Figure 1 shows how many different orders correspondto a given ?
value.
For example, there is only one order with a ?
of 1 or ?1, whereasthere are 3,736 orders with ?
0.07 or ?0.07.Ideally, we should elicit judgments on orders corresponding to all 29 values fromFigure 1.
Unfortunately, this would render our experimental design unwieldy.
As-suming we randomly select one order for each value, our participants would have tojudge 29 ?
8 = 232 texts.
In order to strike a balance between a manageable designand a wide range of ?
values, we split the ?
range into eight bins (see Figure 2).
Foreach text, an order was randomly sampled from each bin.
Thus, our set of materialsconsisted of 8 ?
8 = 64 texts.
Pronouns that could not be resolved intra-sententiallywere substituted by their referents to avoid creating coherence violation artifacts.
For1 The corpus is available from http://people.csail.mit.edu/regina/struct/.2 Notice that the number of permutations and range of ?
values is the same for all our texts, since they allhave the same length.475Computational Linguistics Volume 32, Number 4Figure 1Range of ?
values for a document consisting of eight sentences.the same reason, we excluded from our materials texts containing discourse connectives(e.g., but, therefore).3.1.2 Procedure.
During the elicitation study, participants were presented with textsand asked to judge how comprehensible they were on a seven-point scale.
They weretold that some texts would be perfectly understandable, whereas others would be fairlyincoherent and the order of the sentences might seem scrambled.Figure 2Range of ?
values when collapsed across eight bins.476Lapata Automatic Evaluation of Information OrderingThe study was conducted remotely over the Internet.
Participants first saw a setof instructions that explained the task and provided several examples of well- andbadly organized texts, together with examples of numerical estimates.
From our setof materials we generated 8 lists (each consisting of 8 texts) following a Latin squaredesign.
Each subject was randomly assigned to one list.
The procedure ensured that notwo texts in a given list corresponded to the same reference text.
It was emphasized thatthere were no correct answers and that subjects should base their judgments on firstimpressions, not spending too much time on any one text.
Example stimuli are shownin Table 2.The subjects accessed the experiment using their Web browser.
Experimental in-structions and materials were administered via CGI scripts.
A number of safeguardswere put in place to ensure the authenticity of the subjects taking part.
Participantshad to provide their e-mail address and were asked to fill in a short questionnaireincluding basic demographic information (name, age, sex, handedness, and languagebackground).
Subjects?
e-mail addresses were automatically checked for plausibilityand subjects with fake addresses were removed.
The elicited responses were alsoscreened to identify (and eliminate) subjects taking part in the experiment morethan once.3.1.3 Subjects.
The experiment was completed by 189 unpaid volunteers, all self-reported native speakers of English.
Subjects were recruited by postings to local e-maillists; they had to be linguistically naive, neither linguists nor students of linguisticswere allowed to participate.
Four subjects were eliminated because they were non-native English speakers.
The data of six subjects were excluded after inspection of theirresponses revealed anomalies in their ratings.
For example, they either provided ratingsoutside the prespecified scale (1?7) or rated all documents uniformly.
This left 179subjects for analysis (approximately 22 per text).
Forty-nine of our participants wereTable 2Example stimuli representing a well- (top) and badly- (bottom) organized document.Police arrested 18 people Saturday in an alleged international ring that smuggled hashish infrom Morocco for distribution in Europe.
The group allegedly smuggled the hashish to Cadiz,on Spain?s southern coast, and then used trains to transport it to Barcelona and Italy.
The group,based in Seville with ties in Las Palmas, Barcelona, Morocco and Italy, was headed by the Rufosfamily, police said.
Police seized 100 kilograms (220 pounds) of hashish, 10 million pesetas (dlrs80,000), nine vehicles, rifles, computers, mobile phones, video cameras and false identificationpapers.
Arrests were made in Seville, Las Palmas and Barcelona.
Police did not provide names ofsuspects, or nationalities of those arrested.
Southern Spain is a main gateway for hashish beingsmuggled into Europe from northern Africa.
Hundreds of kilograms (pounds) are seized eachweek.The group allegedly smuggled the hashish to Cadiz, on Spain?s southern coast, and then usedtrains to transport it to Barcelona and Italy.
Hundreds of kilograms (pounds) are seized each week.Southern Spain is a main gateway for hashish being smuggled into Europe from northern Africa.Arrests were made in Seville, Las Palmas and Barcelona.
Police did not provide names of suspects,or nationalities of those arrested.
Police seized 100 kilograms (220 pounds) of hashish, 10 millionpesetas (dlrs 80,000), nine vehicles, rifles, computers, mobile phones, video cameras and falseidentification papers.
The group, based in Seville with ties in Las Palmas, Barcelona, Moroccoand Italy, was headed by the Rufos family, police said.
Police arrested 18 people Saturday in analleged international ring that smuggled hashish in from Morocco for distribution in Europe.477Computational Linguistics Volume 32, Number 4female and 42 male.
The age of the subjects ranged from 18 to 60 years.
The mean was28.5 years.3.2 ResultsThe judgments were averaged to provide a single rating per text.
We first analyzed thecorrespondence of human ratings and ?
values by performing an analysis of variance(ANOVA).
Recall that ?
represents the degree of similarity between a synthetically gen-erated text and a reference text.
In our case, the reference texts are the original human-authored documents from our corpus.
Our participants judge how well a document isorganized without having access to the original reference.Our ANOVA analysis had one factor (i.e., ?
value) with eight levels correspondingto the eight bins discussed in Section 3.1 (see Figure 2).
The ANOVA showed that thisfactor was significant in both by-subjects and by-items analyses: F1(7, 1239) = 42.60, p <0.01; F2(7, 56) = 2.77, p < 0.01.
Table 3 shows the average subject ratings and descriptivestatistics for each of the eight bins.
Post hoc Tukey tests indicated that the ratings fortexts with ?
values from Bin 1 were significantly different from the ratings assigned toall other bins (?
= 0.01).
Although ratings for Bins 2, 3, 4, and 5 did not significantlydiffer from each other, they all differed from Bins 6, 7, and 8.
The results of the ANOVAshow that our participants tended to give high scores to texts with high ?
values andlow scores to texts with low ?
values.We next used correlation analysis to explore the linear relationship between sub-jects?
ratings and Kendall?s ?.
The comparison yielded a Pearson correlation coefficientof r = 0.45 (p < 0.01, N = 64).
Figure 3 plots the relationship between judgments and?
values.
To get a better understanding of how this automatic evaluation methodcompares with human judgments, we examined how well our raters agreed in theirassessment.
To calculate intersubject agreement we used leave-one-out resampling.
Thetechnique is a special case of n-fold cross-validation (Weiss and Kulikowski 1991) andhas been previously used for measuring how well humans agree on judging seman-tic similarity (Resnik and Diab 2000; Resnik 1999), adjective plausibility (Lapata andLascarides 2003), and text coherence (Barzilay and Lapata 2005).The set of m subjects?
responses was divided into two sets: a set of size m ?
1(i.e., the response data of all but one subject) and a set of size one (i.e., the responsedata of a single subject).
We then correlated the mean ratings of the former set with theratings of the latter.
This was repeated m times.
Since we had 179 subjects, we performedTable 3Average subject ratings for binned ?
values and descriptive statistics.Bins Mean Min Max SD1 5.348 3.000 7.000 1.2362 4.916 1.000 7.000 1.6123 4.927 2.000 7.000 1.5804 4.470 1.000 7.000 1.4895 4.382 1.000 7.000 1.5596 4.208 1.000 7.000 1.6007 4.028 1.000 7.000 1.7028 3.966 1.000 7.000 1.558478Lapata Automatic Evaluation of Information OrderingFigure 3Correlation of elicited judgments and ?
values.178 correlation analyses and report their mean.3 The average intersubject agreementwas r = 0.56 (min = 0.001, max = 0.94, SD = 0.25), thus indicating that ?
?s agreementwith the human data is not far from the average human agreement.4.
Experiment 2: Kendall?s Tau and Processing EffortA potential criticism of our previous study is that it is based solely on ratings.
Theproblem with this off-line measure is that it indicates whether participants find a texteasy or difficult to comprehend, without, however, isolating the causes for this difficulty.For example, the ratings may reflect not only what subjects think about how a text isorganized but also their (un)familiarity with its genre or style, their lack of attention, ordisinterest in the subject matter.
To ascertain that this is not the case, we conducted afollow-up experiment whose aim was to explore the relationship between Kendall?s ?and processing effort.
Much work in psychology (McKoon and Ratcliff 1992; Britton1991) indicates that low-coherence texts require more inferences and therefore takelonger to read.
If Kendall?s ?
does indeed capture aspects of overall document organi-zation and coherence, then documents assigned a high ?
value should take less time toread than documents with low ?
values.
Unlike ratings, reading times are an immediatemeasure of processing effort that participants cannot consciously control or modulate.4.1 Method4.1.1 Design and Materials.
The experiment was designed to assess the relation ofKendall?s ?
with processing effort.
Our selection of materials was informed by theANOVA results presented in Experiment 1.
We used the same eight reference textsfrom the previous experiment.
For each text we randomly selected three syntheticallygenerated orders, each from Bin 1 (high ?
value), Bins 2?4 (medium ?
value), and Bins 5?8 (low ?
value).
In other words, we collapsed Bins 2?4 and Bins 5?8, since the ANOVA3 We cannot apply the commonly used kappa statistic for measuring intersubject agreement since it isappropriate for nominal scales, whereas our texts are rated on an ordinal scale.479Computational Linguistics Volume 32, Number 4Table 4Mean reading times (in milliseconds) for three experimental conditions.Mean Min Max SDHigh 5762.6 1963.3 9111.1 1429.0Medium 6499.0 1574.0 10344.5 2017.5Low 7250.4 1428.0 15000.0 3121.3revealed that ratings for these bins were not significantly different.
Our set of materialsconsisted of 8 ?
3 = 24 texts.4.1.2 Procedure.
The presentation of stimuli and collection of responses was controlledby E-Prime software4 (version 1.1) running on a Dell Optiplex GX270 with an IntelPentium 4 processor and 512 MB memory.
The experiment started with a practice ses-sion comprising two texts, each eight sentences long.
Then eight texts were presented;the presentation followed a Latin square design, thus ensuring that no subject saw thesame text twice.The texts were presented one sentence at a time.
The participant pressed the spacebar to proceed from one sentence to the next.
Participants were instructed to readthe texts at their own pace and to press the space bar after each sentence once theywere certain that they understood it.
Participants?
reading time was recorded for eachsentence.
After the final sentence was displayed, subjects were asked a comprehensionyes/no question to make sure that they were actually reading the texts rather thanpressing the space bar randomly.4.1.3 Subjects.
The experiment was completed by 32 volunteers, all self-reported nativespeakers of English.
The experiment was administered in the laboratory and subjectswere paid ?5 for their participation.
None of the subjects had previously participated inExperiment 1.4.2 ResultsSentence reading times were averaged to provide reading times for each text.
As a firststep, the reading time data were screened to remove errors and outliers.
Errors consistedof items where the subjects had incorrectly answered the comprehension question.
Thisaffected 12.3% of the data.
Reading times beyond 2.5 standard deviations above orbelow the mean for a particular participant were replaced with the mean plus this cut-off value.
This adjustment of outliers affected 9.7% of the data.
Mean reading times foreach experimental condition (high, medium, low) are shown in Table 4.ANOVA showed significant differences in reading times [F1(2, 62) = 6.39, p < 0.01;F2(2, 14) = 4.23, p < 0.05].
Post hoc Tukey tests revealed that high-?
texts were read sig-nificantly faster than medium- and low-?
texts (?
= 0.01).
Reading times for medium-?texts were not significantly different from low-?
texts.4 E-prime is a suite of tools for creating and running experiments while allowing for millisecond precisiondata collection.
For more information see http://www.pstnet.com/products/e-prime/.480Lapata Automatic Evaluation of Information OrderingWe next examine through correlation analysis whether there is a linear relationshipbetween reading times and ?
values.
We regressed ?
values and reading times followingthe procedure5 recommended in Lorch and Myers (1990).
The regression yielded aPearson correlation coefficient of r = ?0.48 (p < 0.01).
Expectedly, reading times arealso significantly correlated with human ratings: Pearson?s r = ?0.47 (p < 0.01).6To summarize, the results of our second experiment provide additional evidencefor the use of Kendall?s ?
as a measure of text well-formedness.
It correlates not onlywith human ratings but also with reading times.
The latter constitute much morefine-grained behavioral data, directly associated with processing effort: Less well-structured documents tend to have low ?
values and cause longer reading times,whereas documents with high ?
values tend to be better organized and cause shorterreading times.5.
DiscussionIn this article, we argue that Kendall?s ?
can be used as an automatic evaluationmethod for information-ordering tasks.
We have undertaken a judgment elicitationstudy demonstrating that ?
correlates reliably with human judgments.
We have alsoshown that ?
correlates with processing effort?texts with high ?
values take less timeto read than texts with low ?
values.
We have presented behavioral evidence collectedvia two distinct experimental paradigms suggesting that Kendall?s ?
is an ecologicallyvalid measure of document well-formedness and structure.An attractive feature of the ?
evaluation method is that it is representation inde-pendent.
It can therefore be used to evaluate both symbolic and statistical generationsystems.
We do not view ?
as an alternative to human evaluations; rather we consider itsrole complementary.
It can be used during system development for tracking incrementalprogress or as an easy way of assessing whether an idea is promising.
It can also beused to compare systems that employ comparable information-ordering strategies andoperate over the same input.
Furthermore, statistical generation systems (Lapata 2003;Barzilay and Lee 2004; Karamanis and Manurung 2002; Mellish et al 1998) could use ?as a means of directly optimizing information ordering, much in the same way MTsystems optimize model parameters using BLEU as a measure of translation quality(Och 2003).The ?
evaluations presented in this article used a single reference text.
Previouswork (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Karamanis and Mellish2005) has shown that there may be many acceptable orders for a set of information-bearing items, although topically related sentences seem to appear together (Barzilay,Elhadad, and McKeown 2002).
A straightforward way to incorporate multiple refer-ences in the evaluation paradigm discussed here is to compute the ?
statistic N timesfor every reference?system output pair and report the mean.
A more interesting futuredirection is to weight transpositions (see Section 2) according to agreements or disagree-ments in the set of multiple references.
A possible implementation of this idea would5 Lorch and Myers (1990) argue that it is not appropriate to average over subjects when dealing withrepeated measures designs.
Instead they propose three methods that effect regression analysis onreading times collected from individual subjects.
We refer the interested reader to Lorch and Myers(1990) and Baayen (2004) for further discussion.6 The correlation coefficients are negative since longer reading times correspond to lower ratings and?
values.481Computational Linguistics Volume 32, Number 4be to compute ?
against one (randomly selected) reference, but change the metric soas to give fractional counts (i.e., less than one) to transpositions that are not uniformlyattested in the reference set.Naturally, Kendall?s ?
is not the only automatic evaluation method that can beemployed to assess information ordering.
Barzilay and Lee (2004) and Barzilay andLapata (2005) measure accuracy as the percentage of test items for which the systemgives preference to the gold-standard reference order.
This measure allows us to com-pare the output of different systems; however, it only rewards orders identical to thegold standard, and considers all other orders deviating from it deficient.
Barzilay andLee (2004) propose an additional evaluation measure based on ranks.
Assuming that asystem can exhaustively generate all possible orders for a set of items (with a certainprobability), they report the rank given to the reference order when all possible ordersare sorted by their probability.
The best possible rank is 0 and the worst rank is N!
?
1.A system that gives a high rank to the reference order is considered worse than asystem that gives it a low rank.
However, not all systems are designed to exhaustivelyenumerate all possible permutations for a given document or have indeed a scoringmechanism that can rank alternative document renderings.
Duboue and McKeown(2002) employ an alignment algorithm that allows them to compare the output of theiralgorithm with a gold-standard order.
The alignment algorithm works by consideringthe similarity between system-generated and gold-standard facts.
The similarity func-tion is domain dependent (Duboue and McKeown [2002] generate postcardiac surgerymedical briefings) and would presumably have to be redefined for a different set of factsin another domain.Kendall?s ?
can be easily used to evaluate the output of automatic systems, irre-spectively of the domain or application at hand.
It requires no additional tuning andcorrelates reliably with behavioral data.
Since it is a similarity measure, it can be used toevaluate system output that is not necessarily identical to the gold standard.
Also notethat ?
could be used to compare across systems operating over similar input/outputeven if reference texts are not available.
For example, ?
could identify outlier systemswith output radically different from the mean.AcknowledgmentsThe author acknowledges the support ofEPSRC (grant GR/T04540/01).
Thanks toFrank Keller, Nikiforos Karamanis, ScottMcDonald, and two anonymous reviewersfor helpful comments and suggestions.ReferencesArndt, Stephan, Carolyn Turvey, andNancy C. Andreasen.
1999.
Correlatingand predicting psychiatric symptomratings: Spearman?s r versus Kendall?s taucorrelation.
Journal of Psychiatric Research,33:97?104.Baayen, Harald R. 2004.
Statistics inpsycholinguistics: A critique of somecurrent gold standards.
In Mental LexiconWorking Papers 1.
University of Alberta,Edmonton, pages 1?45.Bangalore, Srinivas, Owen Rambow, andSteven Whittaker.
2000.
Evaluation metricsfor generation.
In Proceedings of the INLG,pages 1?8, Mitzpe Ramon, Israel.Barzilay, Regina.
2003.
Information Fusion forMulti-Document Summarization:Paraphrasing and Generation.
Ph.D. thesis,Columbia University.Barzilay, Regina, Noemie Elhadad, andKathleen R. McKeown.
2002.
Inferringstrategies for sentence ordering inmultidocument news summarization.Journal of Artificial Intelligence Research,17:35?55.Barzilay, Regina and Mirella Lapata.
2005.Modeling local coherence: An entity-basedapproach.
In Proceedings of the 43rd AnnualMeeting of the Association for ComputationalLinguistics, pages 141?148, Ann Arbor.Barzilay, Regina and Lillian Lee.
2004.Catching the drift: Probabilistic contentmodels, with applications to generationand summarization.
In Proceedings of the2nd Human Language Technology Conference482Lapata Automatic Evaluation of Information Orderingand Annual Meeting of the North AmericanChapter of the Association for ComputationalLinguistics, pages 113?120, Boston, MA.Britton, Bruce K. 1991.
Using Kintsch?scomputational model to improveinstructional text: Effects of repairinginference calls on recall and cognitivestructures.
Journal of Educational Psychology,83(3):329?345.Callison-Burch, Chris, Miles Osborne, andPhilipp Koehn.
2006.
Re-evaluating therole of BLEU in machine translationresearch.
In Proceedings of the 11thConference of the European Chapter of theAssociation of Computational Linguistics,pages 249?256, Trento, Italy.Coughlin, Deborah.
2003.
Correlatingautomated and human assessments ofmachine translation quality.
In Proceedingsof MT Summit IX, pages 63?70,New Orleans.Dimitromanolaki, Aggeliki and IonAndroutsopoulos.
2003.
Learning to orderfacts for discourse planning in naturallanguage generation.
In Proceedings of the9th European Workshop on Natural LanguageGeneration, pages 113?120, Budapest,Hungary.Doddington, George.
2002.
Automaticevaluation of machine translation qualityusing n-gram cooccurrence statistics.
InHuman Language Technology: NotebookProceedings, pages 128?132, San Diego.Duboue, Pablo and Kathleen R. McKeown.2002.
Content planner construction viaevolutionary algorithms and a corpus-based fitness function.
In Proceedings ofINLG 2002, pages 89?96, New York.Duboue, Pablo A. and Kathleen R.McKeown.
2001.
Empirically estimatingorder constraints for content planning ingeneration.
In Proceedings of the 39thAnnual Meeting of the Association forComputational Linguistics, pages 172?179,Toulouse, France.Hays, William L. 1994.
Statistics.
HarcourtBrace College Publishers, New York,3rd edition.Hovy, Eduard and Chin-Yew Lin.
2003.Automatic evaluation of summaries usingN-gram co-occurrence statistics.
InProceedings of the 1st Human LanguageTechnology Conference and Annual Meeting ofthe North American Chapter of the Associationfor Computational Linguistics, pages 71?78,Edmonton, Canada.Karamanis, Nikiforos.
2003.
Entity Coherencefor Descriptive Text Structuring.
Ph.D.thesis, University of Edinburgh.Karamanis, Nikiforos and Hisar MaruliManurung.
2002.
Stochastic textstructuring using the principle ofcontinuity.
In Proceedings of the 2ndInternational Conference on Natural LanguageGeneration, pages 81?88, New York.Karamanis, Nikiforos and Chris Mellish.2005.
Using a corpus of sentence orderingsdefined by many experts to evaluatemetrics of coherence for text structuring.In Proceedings of the 10th EuropeanWorkshop on Natural Language Generation,pages 174?179, Aberdeen, Scotland.Kendall, Maurice G. 1938.
A new measure ofrank correlation.
Biometrika, 30:81?93.Kendall, Maurice G. and Jean DickinsonGibbons.
1990.
Rank Correlation Methods.Oxford University Press, New York.Kerridge, D. 1975.
The interpretation ofrank correlations.
Applied Statistics,24(2):257?258.Lapata, Maria and Alex Lascarides.
2003.
Aprobabilistic account of logical metonymy.Computational Linguistics, 29(2):263?317.Lapata, Mirella.
2003.
Probabilistic textstructuring: Experiments with sentenceordering.
In Proceedings of the 41st AnnualMeeting of the Association for ComputationalLinguistics, pages 545?552, Sapporo, Japan.Lebanon, Guy and John Lafferty.
2002.Combining rankings using conditionalprobability models on permutations.
InProceedings of the 19th InternationalConference on Machine Learning.
SanFrancisco, CA: Morgan KaufmannPublishers, pages 363?370.Lorch, Robert F. and Jerome L. Myers.
1990.Regression analyses of repeated measuresdata in cognitive research.
Journal ofExperimental Psychology: Learning, Memory,and Cognition, 16(1):149?157.Mani, Inderjeet.
2001.
AutomaticSummarization.
John Benjamins Pub Co.,Amsterdam; Philadelphia.McKoon, Gail and Roger Ratcliff.
1992.Inference during reading.
PsychologicalReview, 99(3):440?446.Mellish, Chris, Alistair Knott, JonOberlander, and Mick O?
Donnell.
1998.Experiments using stochastic search fortext planning.
In Proceedings of the 9thInternational Workshop on Natural LanguageGeneration, pages 98?107, Ontario, Canada.Nenkova, Ani.
2005.
Automatic textsummarization of newswire: Lessonslearned from the document understandingconference.
In Proceedings of the 20thNational Conference on Artificial Intelligence,pages 1436?1441, Pittsburgh, PA.483Computational Linguistics Volume 32, Number 4Och, Franz Joseph.
2003.
Minimum errorrate training in statistical machinetranslation.
In Proceedings of the 41stAnnual Meeting of the Association forComputational Linguistics, pages 160?167,Sapporo, Japan.Okazaki, Naoaki, Yutaka Matsuo, andMitsuru Ishizuka.
2004.
Improvingchronological sentence ordering byprecedence relation.
In Proceedingsof the 20th International Conference onComputational Linguistics, Geneva,Switzerland, pages 750?756.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
BLEU: Amethod for automatic evaluation ofmachine translation.
In Proceedings of the40th Annual Meeting of the Association forComputational Linguistics, pages 311?318,Philadelphia, PA.Radev, Dragomir and Kathleen R. McKeown.1998.
Generating natural languagesummaries from multiple on-line sources.Computational Linguistics, 24(3):469?500.Resnik, Philip.
1999.
Semantic similarityin a taxonomy: An information-basedmeasure and its application to problemsof ambiguity in natural language.Journal of Artificial Intelligence Research,pages 95?130.Resnik, Philip and Mona Diab.
2000.Measuring verb similarity.
In Proceedingsof the 22nd Annual Conference of theCognitive Science Society.
LawrenceErlbaum Associates, Mahwah, NJ,pages 399?404.Siegel, Sidney and N. John Castellan.
1988.Non Parametric Statistics for the BehavioralSciences.
McGraw-Hill, New York.Teufel, Simone and Hans van Halteren.2004.
Evaluating information contentby factoid analysis: Human annotationand stability.
In Dekang Lin andDekai Wu, editors, Proceedings of theConference on Empirical Methods in NaturalLanguage Processing, pages 419?426,Barcelona.Weiss, Sholom M. and Casimir A.Kulikowski.
1991.
Computer Systems thatLearn: Classification and Prediction Methodsfrom Statistics, Neural Nets, MachineLearning, and Expert Systems.
MorganKaufmann, San Mateo, CA.White, John S. and T. O?Connell.
1994.The ARPA MT evaluation methodologies:Evolution, lessons, and future approaches.In Proceedings of the First Conferenceof the Association for Machine Translationin the Americas, pages 193?205,Columbia, MD.484
