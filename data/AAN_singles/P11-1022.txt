Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 211?219,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsGoodness: A Method for Measuring Machine Translation ConfidenceNguyen Bach?Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAnbach@cs.cmu.eduFei Huang and Yaser Al-OnaizanIBM T.J. Watson Research Center1101 Kitchawan RdYorktown Heights, NY 10567, USA{huangfe, onaizan}@us.ibm.comAbstractState-of-the-art statistical machine translation(MT) systems have made significant progresstowards producing user-acceptable translationoutput.
However, there is still no efficientway for MT systems to inform users whichwords are likely translated correctly and howconfident it is about the whole sentence.
Wepropose a novel framework to predict word-level and sentence-level MT errors with a largenumber of novel features.
Experimental re-sults show that the MT error prediction accu-racy is increased from 69.1 to 72.2 in F-score.The Pearson correlation between the proposedconfidence measure and the human-targetedtranslation edit rate (HTER) is 0.6.
Improve-ments between 0.4 and 0.9 TER reduction areobtained with the n-best list reranking task us-ing the proposed confidence measure.
Also,we present a visualization prototype of MT er-rors at the word and sentence levels with theobjective to improve post-editor productivity.1 IntroductionState-of-the-art Machine Translation (MT) systems aremaking progress to generate more usable translationoutputs.
In particular, statistical machine translationsystems (Koehn et al, 2007; Bach et al, 2007; Shenet al, 2008) have advanced to a state that the transla-tion quality for certain language pairs (e.g.
Spanish-English, French-English, Iraqi-English) in certain do-mains (e.g.
broadcasting news, force-protection, travel)is acceptable to users.However, a remaining open question is how to pre-dict confidence scores for machine translated wordsand sentences.
An MT system typically returns thebest translation candidate from its search space, butstill has no reliable way to inform users which wordis likely to be correctly translated and how confident itis about the whole sentence.
Such information is vital?
Work done during an internship at IBM T.J. WatsonResearch Centerto realize the utility of machine translation in many ar-eas.
For example, a post-editor would like to quicklyidentify which sentences might be incorrectly trans-lated and in need of correction.
Other areas, such ascross-lingual question-answering, information extrac-tion and retrieval, can also benefit from the confidencescores of MT output.
Finally, even MT systems canleverage such information to do n-best list reranking,discriminative phrase table and rule filtering, and con-straint decoding (Hildebrand and Vogel, 2008).Numerous attempts have been made to tackle theconfidence estimation problem.
The work of Blatz etal.
(2004) is perhaps the best known study of sentenceand word level features and their impact on transla-tion error prediction.
Along this line of research, im-provements can be obtained by incorporating more fea-tures as shown in (Quirk, 2004; Sanchis et al, 2007;Raybaud et al, 2009; Specia et al, 2009).
Sori-cut and Echihabi (2010) developed regression modelswhich are used to predict the expected BLEU scoreof a given translation hypothesis.
Improvement alsocan be obtained by using target part-of-speech and nulldependency link in a MaxEnt classifier (Xiong et al,2010).
Ueffing and Ney (2007) introduced word pos-terior probabilities (WPP) features and applied them inthe n-best list reranking.
From the usability point ofview, back-translation is a tool to help users to assessthe accuracy level of MT output (Bach et al, 2007).Literally, it translates backward the MT output into thesource language to see whether the output of backwardtranslation matches the original source sentence.However, previous studies had a few shortcomings.First, source-side features were not extensively inves-tigated.
Blatz et al(2004) only investigated source n-gram frequency statistics and source language modelfeatures, while other work mainly focused on targetside features.
Second, previous work attempted to in-corporate more features but faced scalability issues,i.e., to train many features we need many training ex-amples and to train discriminatively we need to searchthrough all possible translations of each training exam-ple.
Another issue of previous work was that they areall trained with BLEU/TER score computing against211the translation references which is different from pre-dicting the human-targeted translation edit rate (HTER)which is crucial in post-editing applications (Snover etal., 2006; Papineni et al, 2002).
Finally, the back-translation approach faces a serious issue when forwardand backward translation models are symmetric.
In thiscase, back-translation will not be very informative toindicate forward translation quality.In this paper, we predict error types of each wordin the MT output with a confidence score, extend it tothe sentence level, then apply it to n-best list rerankingtask to improve MT quality, and finally design a vi-sualization prototype.
We try to answer the followingquestions:?
Can we use a rich feature set such as source-side information, alignment context, and depen-dency structures to improve error prediction per-formance??
Can we predict more translation error types i.esubstitution, insertion, deletion and shift??
How good do our prediction methods correlatewith human correction??
Do confidence measures help the MT system toselect a better translation??
How confidence score can be presented to im-prove end-user perception?In Section 2, we describe the models and trainingmethod for the classifier.
We describe novel featuresincluding source-side, alignment context, and depen-dency structures in Section 3.
Experimental results andanalysis are reported in Section 4.
Section 5 and 6present applications of confidence scores.2 Confidence Measure Model2.1 Problem settingConfidence estimation can be viewed as a sequen-tial labelling task in which the word sequence isMT output and word labels can be Bad/Good orInsertion/Substitution/Shift/Good.
We first esti-mate each individual word confidence and extend it tothe whole sentence.
Arabic text is fed into an Arabic-English SMT system and the English translation out-puts are corrected by humans in two phases.
In phaseone, a bilingual speaker corrects the MT system trans-lation output.
In phase two, another bilingual speakerdoes quality checking for the correction done in phaseone.
If bad corrections were spotted, they correct themagain.
In this paper we use the final correction datafrom phase two as the reference thus HTER can beused as an evaluation metric.
We have 75 thousand sen-tences with 2.4 million words in total from the humancorrection process described above.We obtain training labels for each word by perform-ing TER alignment between MT output and the phase-two human correction.
From TER alignments we ob-served that out of total errors are 48% substitution, 28%deletion, 13% shift, and 11% insertion errors.
Basedon the alignment, each word produced by the MT sys-tem has a label: good, insertion, substitution and shift.Since a deletion error occurs when it only appears in thereference translation, not in the MT output, our modelwill not predict deletion errors in the MT output.2.2 Word-level modelIn our problem, a training instance is a word from MToutput, and its label when the MT sentence is alignedwith the human correction.
Given a training instance x,y is the true label of x; f stands for its feature vectorf(x, y); and w is feature weight vector.
We define afeature-rich classifier score(x, y) as followscore(x, y) = w.f(x, y) (1)To obtain the label, we choose the class with the high-est score as the predicted label for that data instance.To learn optimized weights, we use the Margin InfusedRelaxed Algorithm or MIRA (Crammer and Singer,2003; McDonald et al, 2005) which is an online learnerclosely related to both the support vector machine andperceptron learning framework.
MIRA has been shownto provide state-of-the-art performance for sequentiallabelling task (Rozenfeld et al, 2006), and is also ableto provide an efficient mechanism to train and opti-mize MT systems with lots of features (Watanabe etal., 2007; Chiang et al, 2009).
In general, weights areupdated at each step time t according to the followingrule:wt+1 = argminwt+1 ||wt+1 ?
wt||s.t.
score(x, y) ?
score(x, y?)
+ L(y, y?
)(2)where L(y, y?)
is a measure of the loss of using y?
in-stead of the true label y.
In this problem L(y, y?)
is 0-1loss function.
More specifically, for each instance xi inthe training data at a time t we find the label with thehighest score:y?
= argmaxyscore(xi, y) (3)the weight vector is updated as followwt+1 = wt + ?
(f(xi, y)?
f(xi, y?))
(4)?
can be interpreted as a step size; when ?
is a largenumber we want to update our weights aggressively,otherwise weights are updated conservatively.?
= max(0, ?)?
= min{C, L(y,y?)?(score(xi,y)?score(xi,y?))||f(xi,y)?f(xi,y?
)||22}(5)where C is a positive constant used to cap the maxi-mum possible value of ?
.
In practice, a cut-off thresh-old n is the parameter which decides the number offeatures kept (whose occurrence is at least n) during212training.
Note that MIRA is sensitive to constant C,the cut-off feature threshold n, and the number of iter-ations.
The final weight is typically normalized by thenumber of training iterations and the number of train-ing instances.
These parameters are tuned on a devel-opment set.2.3 Sentence-level modelGiven the feature sets and optimized weights, we usethe Viterbi algorithm to find the best label sequence.To estimate the confidence of a sentence S we rely onthe information from the forward-backward inference.One approach is to directly use the conditional prob-abilities of the whole sequence.
However, this quan-tity is the confidence measure for the label sequencepredicted by the classifier and it does not represent thegoodness of the whole MT output.
Another more ap-propriated method is to use the marginal probability ofGood label which can be defined as follow:p(yi = Good|S) =?(yi|S)?
(yi|S)?j ?
(yj |S)?
(yj |S)(6)p(yi = Good|S) is the marginal probability of labelGood at position i given the MT output sentence S.?
(yi|S) and ?
(yi|S) are forward and backward values.Our confidence estimation for a sentence S of k wordsis defined as followgoodness(S) =?ki=1 p(yi = Good|S)k(7)goodness(S) is ranging between 0 and 1, where 0 isequivalent to an absolutely wrong translation and 1is a perfect translation.
Essentially, goodness(S) isthe arithmetic mean which represents the goodness oftranslation per word in the whole sentence.3 Confidence Measure FeaturesFeatures are generated from feature types: abstracttemplates from which specific features are instantiated.Features sets are often parameterized in various ways.In this section, we describe three new feature sets intro-duced on top of our baseline classifier which has WPPand target POS features (Ueffing and Ney, 2007; Xionget al, 2010).3.1 Source-side featuresFrom MT decoder log, we can track which sourcephrases generate target phrases.
Furthermore, one caninfer the alignment between source and target wordswithin the phrase pair using simple aligners such asIBM Model-1 alignment.Source phrase features: These features are designedto capture the likelihood that source phrase and targetword co-occur with a given error label.
The intuitionbehind them is that if a large percentage of the sourcephrase and target have often been seen together with theSource POS and PhrasesWPP: 1.00.671.0 1.01.00.67?Target POS: PRP  VBZINDTNNRBVBZTODTNNINDTJJ   JJNNSVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNS    DTJJMT outputSource POSSourceHeaddsthatthisprocessalsorefers  totheinabilityof  the  multinational  naval  forceswydyfanhdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhryt(a) Source phraseSource POS and PhrasesWPP: 1.00.671.0 1.01.00.67?Target POS: PRP  VBZINDTNNRBVBZTODTNNINDTJJ   JJNNS1 ifsource-POS-sequence =?DTDTNN?f 125(target-word = ?process?)
=0 otherwiseMT outputSource POSSourcewydyfanhdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhrytHeaddsthatthisprocessalsorefers  totheinabilityof  the  multinational  naval  forcesVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNS    DTJJ(b) Source POSSource POS and PhrasesWPP: 1.00.671.0 1.01.00.67?Target POS: PRP  VBZINDTNNRBVBZTODTNNINDTJJ   JJNNSMT outputSource POSSourceHeaddsthatthisprocessalsorefers  totheinabilityof  the  multinational  naval  forcesVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNSDTJJwydyfanhdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhryt(c) Source POS and phrase in right contextFigure 1: Source-side features.same label, then the produced target word should havethis label in the future.
Figure 1a illustrates this featuretemplate where the first line is source POS tags, thesecond line is the Buckwalter romanized source Arabicsequence, and the third line is MT output.
The sourcephrase feature is defined as followf102(process) ={1 if source-phrase=?hdhh alamlyt?0 otherwiseSource POS: Source phrase features might be suscep-tible to sparseness issues.
We can generalize sourcephrases based on their POS tags to reduce the numberof parameters.
For example, the example in Figure 1ais generalized as in Figure 1b and we have the follow-ing feature:f103(process) ={1 if source-POS=?
DT DTNN ?0 otherwiseSource POS and phrase context features: This fea-ture set alows us to look at the surrounding contextof the source phrase.
For example, in Figure 1c wehave ?hdhh alamlyt?
generates ?process?.
We alsohave other information such as on the right hand sidethe next two phrases are ?ayda?
and ?tshyr?
or the se-quence of source target POS on the right hand side is?RB VBP?.
An example of this type of feature isf104(process) ={1 if source-POS-context=?
RB VBP ?0 otherwise3.2 Alignment context featuresThe IBM Model-1 feature performed relatively well incomparison with the WPP feature as shown by Blatz etal.
(2004).
In our work, we incorporate not only the213Alignment ContextWPP: 1.0 0.67 1.0 1.01.00.67 ?PRPVBZINDTNNRBVBZTODTNNINDTJJJJNNSVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNSDTJJwydyfan hdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhrytHeadds  that  thisprocessalsorefersto  theinabilityofthemultinational  naval  forcesMToutputSourcePOSSourceTarget POS(a) Left sourceAlignment ContextWPP: 1.0 0.67 1.0 1.01.00.67 ?PRPVBZINDTNNRBVBZTODTNNINDTJJJJNNSVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNSDTJJwydyfanhdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhrytHeadds  that  thisprocessalsorefersto  theinabilityofthemultinational  naval  forcesMToutputSourcePOSSourceTarget POS(b) Rig t sourceAlignment ContextWPP: 1.0 0.67 1.0 1.01.00.67 ?PRPVBZINDTNNRBVBZTODTNNINDTJJJJNNSVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNSDTJJwydyfan hdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhrytHeadds  that thisprocessalsorefersto  theinabilityofthemultinational  naval  forcesMToutputSourcePOSSourceTarget POS(c) Left targetAlignment ContextWPP: 1.0 0.67 1.0 1.01.00.67 ?PRPVBZINDTNNRBVBZTODTNNINDTJJJJNNSwydyfan  hdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhrytMToutputSourcePOSSourceTarget POSHeadds  that  thisprocessalsoreferstotheinabilityofthemultinational  naval  forcesVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNSDTJJ(d) Source POS & right tar-getFigur 2: Alignment context features.IBM Model-1 feature but also the surr unding align-ment context.
The key intuition is that collocation is areliable indicator for judging if a target word is gener-ated by a particular source word (Huang, 2009).
More-over, the IBM Model-1 feature was already used in sev-eral steps of a translation system such as word align-ment, phrase extraction and scoring.
Also the impact ofthis feature alone might fade away when the MT sys-tem is scaled up.We obtain word-to-word alignments by applyingIBM Model-1 to bilingual phrase pairs that generatedthe MT output.
The IBM Model-1 assumes onetarget word can only be aligned to one source word.Therefore, given a target word we can always identifywhich source word it is aligned to.Source alignment context feature: We anchor thetarget word and derive context features surround-ing its source word.
For example, in Figure 2aand 2b we have an alignment between ?tshyr?
and?refers?
The source contexts ?tshyr?
with a windowof one word are ?ayda?
to the left and ?aly?
to the right.Target algnment context feature: Similar to sourcealignment context features, we anchor the source wordand derive context features surrounding the alignedtarget word.
Figure 2c shows a left target contextfeature of word ?refers?.
Our features are derived froma window of four words.Combining alignment context with POS tags: In-stead of using lexical context we have features to lookat source and target POS alignment context.
For in-stance, the feature in Figure 2d isf141(refers) ={1 if source-POS = ?VBP?and target-context = ?to?0 otherwiseSource &Target DependencyStructuresWPP: 1.00.671.0 1.01.00.67?PRPVBZINDTNNRBVBZTODTNNINDTJJJJNNSwydyfanhdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhrytHeaddsthatthisprocessalsorefers   totheinabilityof   the  multinational  naval  forcesVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNSDTJJnull(a) Source-Target dependencySource &Target DependencyStructuresPRPVBZINDTNNRBVBZTODTNNINDTJJJJNNSwydyfanhdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhrytHeaddsthatthisprocessalsorefersto  the  inability   of   the  multinational  naval  forcesVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNSDTJJWPP: 1.00.671.0 1.01.00.67?
(b) Child-Father agreementSource &Target DependencyStructuresPRPVBZINDTNNRBVBZTODTNNINDTJJJJNNSwydyfanhdhhalamlytaydatshyralyadmqdrtalmtaddtaljnsytalqwatalbhrytHeaddsthatthisprocessalsorefersto  the  inability   of   the  multinational  naval  forcesVBPINDTDTNNRBVBPINNNNNDTJJDTJJDTNNSDTJJChildrenAgreement:2WPP: 1.00.671.0 1.01.00.67?
(c) Children agreementFigure 3: Dependency structures features.3.3 Source and target dependency structurefeaturesThe contextual and source information in the previoussections only take into account surface structures ofsource and target sentences.
Meanwhile, dependencystructures have been extensively used in varioustranslation systems (Shen et al, 2008; Ma et al,2008; Bach et al, 2009).
The adoption of dependencystructures might enable the classifier to utilize deepstructures to predict translation errors.
Source and tar-get structures are unlikely to be isomorphic as shownin Figure 3a.
However, we expect some high-levellinguistic structures are likely to transfer across certainlanguage pairs.
For example, prepositional phrases(PP) in Arabic and English are similar in a sensethat PPs generally appear at the end of the sentence(after all the verbal arguments) and to a lesser extentat its beginning (Habash and Hu, 2009).
We use theStanford parser to obtain dependency trees and POStags (Marneffe et al, 2006).Child-Father agreement: The motivation is to takeadvantage of the long distance dependency relationsbetween source and target words.
Given an alignmentbetween a source word si and a target word tj .
A child-214father agreement exists when sk is aligned to tl, wheresk and tl are father of si and tj in source and targetdependency trees, respectively.
Figure 3b illustratesthat ?tshyr?
and ?refers?
have a child-father agreement.To verify our intuition, we analysed 243K words ofmanual aligned Arabic-English bitext.
We observed29.2% words having child-father agreements.
In termof structure types, we found 27.2% of copula verband 30.2% prepositional structures, including objectof a preposition, prepositional modifier, and preposi-tional complement, are having child-father agreements.Children agreement: In the child-father agreementfeature we look up in the dependency tree, however,we also can look down to the dependency tree with asimilar motivation.
Essentially, given an alignment be-tween a source word si and a target word tj , how manychildren of si and tj are aligned together?
For exam-ple, ?tshyr?
and ?refers?
have 2 aligned children whichare ?ayda-also?
and ?aly-to?
as shown in Figure 3c.4 Experiments4.1 Arabic-English translation systemThe SMT engine is a phrase-based system similar tothe description in (Tillmann, 2006), where variousfeatures are combined within a log-linear framework.These features include source-to-target phrase transla-tion score, source-to-target and target-to-source word-to-word translation scores, language model score, dis-tortion model scores and word count.
The trainingdata for these features are 7M Arabic-English sentencepairs, mostly newswire and UN corpora released byLDC.
The parallel sentences have word alignment au-tomatically generated with HMM and MaxEnt wordaligner (Ge, 2004; Ittycheriah and Roukos, 2005).Bilingual phrase translations are extracted from theseword-aligned parallel corpora.
The language model isa 5-gram model trained on roughly 3.5 billion Englishwords.Our training data contains 72k sentences Arabic-English machine translation with human correctionswhich include of 2.2M words in newswire and weblogdomains.
We have a development set of 2,707 sen-tences, 80K words (dev); an unseen test set of 2,707sentences, 79K words (test).
Feature selection and pa-rameter tuning has been done on the development set inwhich we experimented values of C, n and iterations inrange of [0.5:10], [1:5], and [50:200] respectively.
Thefinal MIRA classifier was trained by using pocket crftoolkit1 with 100 iterations, hyper-parameter C was 5and cut-off feature threshold n was 1.We use precision (P ), recall (R) and F-score (F ) toevaluate the classifier performance and they are com-1http://pocket-crf-1.sourceforge.net/puted as follow:P = the number of correctly tagged labelsthe number of tagged labelsR = the number of correctly tagged labelsthe number of reference labelsF = 2*P*RP+R(8)4.2 Contribution of feature setsWe designed our experiments to show the impactof each feature separately as well as their cumu-lative impact.
We trained two types of classifiersto predict the error type of each word in MT out-put, namely Good/Bad with a binary classifier andGood/Insertion/Substitution/Shift with a 4-class classi-fier.
Each classifier is trained with different feature setsas follow:?
WPP: we reimplemented WPP calculation basedon n-best lists as described in (Ueffing and Ney,2007).?
WPP + target POS: only WPP and target POS fea-tures are used.
This is a similar feature set used byXiong et al (2010).?
Our features: the classifier has source side, align-ment context, and dependency structure features;WPP and target POS features are excluded.?
WPP + our features: adding our features on top ofWPP.?
WPP + target POS + our features: using all fea-tures.binary 4-classdev test dev testWPP 69.3 68.7 64.4 63.7+ source side 72.1 71.6 66.2 65.7+ alignment context 71.4 70.9 65.7 65.3+ dependency structures 69.9 69.5 64.9 64.3WPP+ target POS 69.6 69.1 64.4 63.9+ source side 72.3 71.8 66.3 65.8+ alignment context 71.9 71.2 66 65.6+ dependency structures 70.4 70 65.1 64.4Table 1: Contribution of different feature sets measurein F-score.To evaluate the effectiveness of each feature set, weapply them on two different baseline systems: usingWPP and WPP+target POS, respectively.
We augmenteach baseline with our feature sets separately.
Ta-ble 1 shows the contribution in F-score of our proposedfeature sets.
Improvements are consistently obtainedwhen combining the proposed features with baselinefeatures.
Experimental results also indicate that source-side information, alignment context and dependency215Predicting Good/Bad words59.459.369.368.769.669.172.171.572.47272.672.2586062646668707274devtestTestsetsF-scoreWPP+targetPOS+OurfeaturesWPP+OurfeaturesOur featuresWPP+targetPOSWPPAll-Good(a) BinaryPredictingGood/Insertion/Substitution/Shift words59.459.364.463.764.463.966.265.666.665.966.866.15859606162636465666768devtestTestsetsF-scoreWPP+targetPOS+OurfeaturesWPP+OurfeaturesOur featuresWPP+targetPOSWPPAll-Good(b) 4-classFigure 4: Performance of binary and 4-class classifiers trained with different feature sets on the development andunseen test sets.structures have unique and effective levers to improvethe classifier performance.
Among the three proposedfeature sets, we observe the source side informationcontributes the most gain, which is followed by thealignment context and dependency structure features.4.3 Performance of classifiersWe trained several classifiers with our proposed featuresets as well as baseline features.
We compare their per-formances, including a naive baseline All-Good classi-fier, in which all words in the MT output are labelledas good translations.
Figure 4 shows the performanceof different classifiers trained with different feature setson development and unseen test sets.
On the unseen testset our proposed features outperform WPP and targetPOS features by 2.8 and 2.4 absolute F-score respec-tively.
Improvements of our features are consistent indevelopment and unseen sets as well as in binary and4-class classifiers.
We reach the best performance bycombining our proposed features with WPP and targetPOS features.
Experiments indicate that the gaps in F-score between our best system with the naive All-Goodsystem is 12.9 and 6.8 in binary and 4-class cases, re-spectively.
Table 2 presents precision, recall, and F-score of individual class of the best binary and 4-classclassifiers.
It shows that Good label is better predictedthan other labels, meanwhile, Substitution is gener-ally easier to predict than Insertion and Shift.4.4 Correlation between Goodness and HTERWe estimate sentence level confidence score basedon Equation 7.
Figure 5 illustrates the correla-tion between our proposed goodness sentence levelconfidence score and the human-targeted translationedit rate (HTER).
The Pearson correlation betweengoodness and HTER is 0.6, while the correlation ofWPP and HTER is 0.52.
This experiment shows thatgoodness has a large correlation with HTER.
Theblack bar is the linear regression line.
Blue and redLabel P R FBinaryGood 74.7 80.6 77.5Bad 68 60.1 63.84-classGood 70.8 87 78.1Insertion 37.5 16.9 23.3Substitution 57.8 44.9 50.5Shift 35.2 14.1 20.1Table 2: Detailed performance in precision, recalland F-score of binary and 4-class classifiers withWPP+target POS+Our features on the unseen test set.bars are thresholds used to visualize good and bad sen-tences respectively.
We also experimented goodnesscomputation in Equation 7 using geometric mean andharmonic mean; their Pearson correlation values are 0.5and 0.35 respectively.5 Improving MT quality with N-best listrerankingExperiments reporting in Section 4 indicate that theproposed confidence measure has a high correlationwith HTER.
However, it is not very clear if the core MTsystem can benefit from confidence measure by provid-ing better translations.
To investigate this question wepresent experimental results for the n-best list rerank-ing task.The MT system generates top n hypotheses and foreach hypothesis we compute sentence-level confidencescores.
The best candidate is the hypothesis with high-est confidence score.
Table 3 shows the performance ofreranking systems using goodness scores from our bestclassifier in various n-best sizes.
We obtained 0.7 TERreduction and 0.4 BLEU point improvement on the de-velopment set with a 5-best list.
On the unseen test, weobtained 0.6 TER reduction and 0.2 BLEU point im-provement.
Although, the improvement of BLEU score2160.91Good?Bad Linear?fit0.70.8 040.50.6Goodness0.20.30.4 00.10.2020406080100HTERFigure 5: Correlation between Goodness and HTER.Dev TestTER BLEU TER BLEUBaseline 49.9 31.0 50.2 30.62-best 49.5 31.4 49.9 30.85-best 49.2 31.4 49.6 30.810-best 49.2 31.2 49.5 30.820-best 49.1 31.0 49.3 30.730-best 49.0 31.0 49.3 30.640-best 49.0 31.0 49.4 30.550-best 49.1 30.9 49.4 30.5100-best 49.0 30.9 49.3 30.5Table 3: Reranking performance with goodness score.is not obvious, TER reductions are consistent in bothdevelopment and unseen sets.
Figure 6 shows the im-provement of reranking with goodness score.
Besides,the figure illustrates the upper and lower bound perfor-mances with TER metric in which the lower bound isour baseline system and the upper bound is the best hy-pothesis in a given n-best list.
Oracle scores of each n-best list are computed by choosing the translation can-didate with lowest TER score.6 Visualizing translation errorsBesides the application of confidence score in the n-best list reranking task, we propose a method to visual-ize translation error using confidence scores.
Our pur-pose is to visualize word and sentence-level confidencescores with the following objectives 1) easy for spottingtranslations errors; 2) simple and intuitive; and 3) help-ful for post-editing productivity.
We define three cate-gories of translation quality (good/bad/decent) on bothword and sentence level.
On word level, the marginalprobability of good label is used to visualize translationerrors as follow:Li =??
?good if p(yi = Good|S) ?
0.8bad if p(yi = Good|S) ?
0.45decent otherwise424344454647484950511 2 5 10 20 30 40 50 100TERN-best sizeOracleOur modelsBaselineFigure 6: A comparison between reranking and oraclescores with different n-best size in TER metric on thedevelopment set.On sentence level, the goodness score is used as follow:LS =??
?good if goodness(S) ?
0.7bad if goodness(S) ?
0.5decent otherwiseChoices IntentionFont sizebig badsmall goodmedium decentColorsred badblack goodorange decentTable 4: Choices of layoutDifferent font sizes and colors are used to catch theattention of post-editors whenever translation errors arelikely to appear as shown in Table 4.
Colors are ap-plied on word level, while font size is applied on bothword and sentence level.
The idea of using font sizeand colour to visualize translation confidence is simi-lar to the idea of using tag/word cloud to describe thecontent of websites2.
The reason we are using big fontsize and red color is to attract post-editors?
attentionand help them find translation errors quickly.
Figure 7shows an example of visualizing confidence scores byfont size and colours.
It shows that ?not to depriveyourself ?, displayed in big font and red color, is likelyto be bad translations.
Meanwhile, other words, suchas ?you?, ?different?, ?from?, and ?assimilation?, dis-played in small font and black color, are likely to begood translation.
Medium font and orange color wordsare decent translations.2http://en.wikipedia.org/wiki/Tag cloud217youtotally different fromzaidamr,and not to depriveyourself ina basement ofimitationand assimilation.?????
?    ???!"??
?%&?
???
MToutputSourceyoutotallydifferent fromzaidamr,andnottodeprive yourselfina basement of imitationand assimilation.Wepredictand visualizeHumancorrectionyouarequite different fromzaidandamr,sodonot cramyourself inthetunnel ofsimulation,imitationand assimilation.
(a)thepoll alsoshowed that most oftheparticipants in the developing countriesarereadyto introducequalitativechanges inthepattern of theirlives for the sakeof reducing theeffectsof climatechange.????
??
 ??
?
 ??
?
??
!"?
? !#$%&'())*& ?
?,# ?-?-?,??
?/0# 12- 34.MToutputSourcethepoll alsoshowedthat most oftheparticipants in the developing countriesarereadyto introducequalitativechangesinthepatternof theirlives forthe sakeofreducingtheeffectsof climatechange.Wepredictand visualizethesurvey alsoshowed that most oftheparticipants in developing countriesarereadyto introducechanges tothequalityof their lifestylein order toreduce the effects ofclimatechange.Humancorrection(b)Figure 7: MT errors visualization based on confidence scores.7 ConclusionsIn this paper we proposed a method to predict con-fidence scores for machine translated words and sen-tences based on a feature-rich classifier using linguisticand context features.
Our major contributions are threenovel feature sets including source side information,alignment context, and dependency structures.
Experi-mental results show that by combining the source sideinformation, alignment context, and dependency struc-ture features with word posterior probability and tar-get POS context (Ueffing & Ney 2007; Xiong et al,2010), the MT error prediction accuracy is increasedfrom 69.1 to 72.2 in F-score.
Our framework is able topredict error types namely insertion, substitution andshift.
The Pearson correlation with human judgementincreases from 0.52 to 0.6.
Furthermore, we show thatthe proposed confidence scores can help the MT sys-tem to select better translations and as a result improve-ments between 0.4 and 0.9 TER reduction are obtained.Finally, we demonstrate a prototype to visualize trans-lation errors.This work can be expanded in several directions.First, we plan to apply confidence estimation to per-form a second-pass constraint decoding.
After the firstpass decoding, our confidence estimation model can la-bel which word is likely to be correctly translated.
Thesecond-pass decoding utilizes the confidence informa-tion to constrain the search space and hopefully canfind a better hypothesis than in the first pass.
This ideais very similar to the multi-pass decoding strategy em-ployed by speech recognition engines.
Moreover, wealso intend to perform a user study on our visualiza-tion prototype to see if it increases the productivity ofpost-editors.AcknowledgementsWe would like to thank Christoph Tillmann and theIBM machine translation team for their supports.
Also,we would like to thank anonymous reviewers, Qin Gao,Joy Zhang, and Stephan Vogel for their helpful com-ments.ReferencesNguyen Bach, Matthias Eck, Paisarn Charoenpornsawat,Thilo Khler, Sebastian Stker, ThuyLinh Nguyen, RogerHsiao, Alex Waibel, Stephan Vogel, Tanja Schultz, andAlan Black.
2007.
The CMU TransTac 2007 Eyes-freeand Hands-free Two-way Speech-to-Speech TranslationSystem.
In Proceedings of the IWSLT?07, Trento, Italy.Nguyen Bach, Qin Gao, and Stephan Vogel.
2009.
Source-side dependency tree reordering models with subtreemovements and constraints.
In Proceedings of theMTSummit-XII, Ottawa, Canada, August.
InternationalAssociation for Machine Translation.218John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, andNicola Ueffing.
2004.
Confidence estimation for machinetranslation.
In The JHU Workshop Final Report, Balti-more, Maryland, USA, April.David Chiang, Kevin Knight, and Wei Wang.
2009.
11,001new features for statistical machine translation.
In Pro-ceedings of HLT-ACL, pages 218?226, Boulder, Colorado,June.
Association for Computational Linguistics.Koby Crammer and Yoram Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
Journal of Ma-chine Learning Research, 3:951?991.Niyu Ge.
2004.
Max-posterior HMM alignment for machinetranslation.
In Presentation given at DARPA/TIDES NISTMT Evaluation workshop.Nizar Habash and Jun Hu.
2009.
Improving arabic-chinesestatistical machine translation using english as pivot lan-guage.
In Proceedings of the 4th Workshop on Statisti-cal Machine Translation, pages 173?181, Morristown, NJ,USA.
Association for Computational Linguistics.Almut Silja Hildebrand and Stephan Vogel.
2008.
Combi-nation of machine translation systems via hypothesis se-lection from combined n-best lists.
In Proceedings of the8th Conference of the AMTA, pages 254?261, Waikiki,Hawaii, October.Fei Huang.
2009.
Confidence measure for word align-ment.
In Proceedings of the ACL-IJCNLP ?09, pages932?940, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Abraham Ittycheriah and Salim Roukos.
2005.
A maximumentropy word aligner for arabic-english machine transla-tion.
In Proceedings of the HTL-EMNLP?05, pages 89?96, Morristown, NJ, USA.
Association for ComputationalLinguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of ACL?07,pages 177?180, Prague, Czech Republic, June.Yanjun Ma, Sylwia Ozdowska, Yanli Sun, and Andy Way.2008.
Improving word alignment using syntactic depen-dencies.
In Proceedings of the ACL-08: HLT SSST-2,pages 69?77, Columbus, OH.Marie-Catherine Marneffe, Bill MacCartney, and ChristopherManning.
2006.
Generating typed dependency parsesfrom phrase structure parses.
In Proceedings of LREC?06,Genoa, Italy.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Flexible text segmentation with structured mul-tilabel classification.
In Proceedings of Human Lan-guage Technology Conference and Conference on Empiri-cal Methods in Natural Language Processing, pages 987?994, Vancouver, British Columbia, Canada, October.
As-sociation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: A method for automatic evaluationof machine translation.
In Proceedings of ACL?02, pages311?318, Philadelphia, PA, July.Chris Quirk.
2004.
Training a sentence-level machine trans-lation confidence measure.
In Proceedings of the 4thLREC.Sylvain Raybaud, Caroline Lavecchia, David Langlois, andKamel Smaili.
2009.
Error detection for statistical ma-chine translation using linguistic features.
In Proceedingsof the 13th EAMT, Barcelona, Spain, May.Binyamin Rozenfeld, Ronen Feldman, and Moshe Fresko.2006.
A systematic cross-comparison of sequence clas-sifiers.
In Proceedings of the SDM, pages 563?567,Bethesda, MD, USA, April.Alberto Sanchis, Alfons Juan, and Enrique Vidal.
2007.
Esti-mation of confidence measures for machine translation.
InProceedings of the MT Summit XI, Copenhagen, Denmark.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
A newstring-to-dependency machine translation algorithm witha target dependency language model.
In Proceedings ofACL-08: HLT, pages 577?585, Columbus, Ohio, June.
As-sociation for Computational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of trans-lation edit rate with targeted human annotation.
In Pro-ceedings of AMTA?06, pages 223?231, August.Radu Soricut and Abdessamad Echihabi.
2010.
Trustrank:Inducing trust in automatic translations via ranking.
InProceedings of the 48th ACL, pages 612?621, Uppsala,Sweden, July.
Association for Computational Linguistics.Lucia Specia, Zhuoran Wang, Marco Turchi, John Shawe-Taylor, and Craig Saunders.
2009.
Improving the con-fidence of machine translation quality estimates.
In Pro-ceedings of the MT Summit XII, Ottawa, Canada.Christoph Tillmann.
2006.
Efficient dynamic programmingsearch algorithms for phrase-based SMT.
In Proceedingsof the Workshop on Computationally Hard Problems andJoint Inference in Speech and Language Processing, pages9?16, Morristown, NJ, USA.
Association for Computa-tional Linguistics.Nicola Ueffing and Hermann Ney.
2007.
Word-level confi-dence estimation for machine translation.
ComputationalLinguistics, 33(1):9?40.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for statisti-cal machine translation.
In Proceedings of the EMNLP-CoNLL, pages 764?773, Prague, Czech Republic, June.Association for Computational Linguistics.Deyi Xiong, Min Zhang, and Haizhou Li.
2010.
Error de-tection for statistical machine translation using linguisticfeatures.
In Proceedings of the 48th ACL, pages 604?611, Uppsala, Sweden, July.
Association for Computa-tional Linguistics.219
