Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 1?10,Columbus, June 2008. c?2008 Association for Computational LinguisticsOptimizing Endpointing Thresholds using DialogueFeatures in a Spoken Dialogue SystemAntoine Raux and Maxine Eskenazi{antoine,max}@cs.cmu.eduLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAAbstractThis paper describes a novel algorithm to dy-namically set endpointing thresholds based ona rich set of dialogue features to detect the endof user utterances in a dialogue system.
Byanalyzing the relationship between silences inuser?s speech to a spoken dialogue system anda wide range of automatically extracted fea-tures from discourse, semantics, prosody, tim-ing and speaker characteristics, we found thatall features correlate with pause duration andwith whether a silence indicates the end of theturn, with semantics and timing being the mostinformative.
Based on these features, the pro-posed method reduces latency by up to 24%over a fixed threshold baseline.
Offline evalu-ation results were confirmed by implementingthe proposed algorithm in the Let?s Go system.1 Introduction1.1 Responsiveness in DialogueAlthough the quality of speech technologies has im-proved drastically and spoken interaction with ma-chines is becoming a part of the everyday life ofmany people, dialogues with artificial agents stillfall far short of their human counterpart in terms ofboth comfort and efficiency.
Besides lingering prob-lems in speech recognition and understanding, Wardet al(Ward et al, 2005) identified turn-taking is-sues, specifically responsiveness, as important short-comings.
Dialogues with artificial agents are typi-cally rigid, following a strict one-speaker-at-a-timestructure with significant latencies between turns.In a previous paper, we concurred with these find-ings when analyzing issues with the Let?s Go system(Raux et al, 2006).
In contrast, empirical studiesof conversation have shown that human-human dia-logues commonly feature swift exchanges with lit-tle or no gap between turns, or even non-disruptiveoverlap (Jaffe and Feldstein, 1970; Sacks et al,1974).
According to Conversation Analysis andpsycholinguistic studies, responsiveness in humanconversations is possible because participants in theconversation exchange cues indicating when a turnmight end, and are able to anticipate points at whichthey can take over the floor smoothly.
Much re-search has been devoted to finding these cues, lead-ing to the identification of many aspects of languageand dialogue that relate to turn-taking behavior, in-cluding syntax (Sacks et al, 1974; Ford and Thomp-son, 1996; Furo, 2001), prosody (Duncan, 1972;Orestro?m, 1983; Chafe, 1992; Ford and Thompson,1996; Koiso et al, 1998; Furo, 2001), and seman-tics (Orestro?m, 1983; Furo, 2001).
However, re-garding this last aspect, Orestrom notes about hiscorpus that ?there is no simple way to formaliz-ing a semantic analysis of this conversational mate-rial?.
This difficulty in formalizing higher levels ofconversation might explain the relatively low inter-est that conversational analysts have had in seman-tics and discourse.
Yet, as conversational analystsfocused on micro-levels of dialogue such as turn-taking, computational linguists uncovered and for-malized macro-level dialogue structure and devisedwell-defined representations of semantics for at leastsome forms of dialogues (Allen and Perrault, 1980;Grosz and Sidner, 1986; Clark, 1996), which have inturn been implemented in spoken dialogue systems(Rich and Sidner, 1998; Allen et al, 2005).11.2 Current Approaches to Turn-Taking inSpoken Dialogue SystemsUnfortunately, while socio- and psycho-linguists re-vealed the complexity of conversational turn-takingbehavior, designers of practical spoken dialogue sys-tems have stuck to a simplistic approach to end-of-turn detection (hereafter endpointing).
Typically, si-lences in user speech are detected using a low-levelVoice Activity Detector (VAD) and a turn is consid-ered finished once a silence lasts longer than a fixedthreshold.
This approach has the advantage of beingsimple, only relying on easily computable low-levelfeatures.
However, it leads to suboptimal behaviorin many instances.
First, False Alarms (FA) hap-pen when a pause lasts longer than the threshold andgets wrongly classified as a gap1.
Second, latencyoccurs at every gap, because the system must waitfor the duration of the threshold before classifying asilence as gap.
When setting the threshold, systemdesigners must consider the trade-off between thesetwo issues: setting a low threshold reduces latencybut increases FA rate, while setting a high thresholdreduces FA rate but increases latency.To help overcome the shortcomings of the single-threshold approach, several researchers have pro-posed to exploit various features.
Sato et al(Satoet al, 2002) used decision trees to classify pauseslonger than 750 ms as gap or pause.
By using fea-tures from semantics, syntax, dialogue state, andprosody, they were able to improve the classificationaccuracy from a baseline of 76.2% to 83.9%.
Whilethis important study shows encouraging results onthe value of using various sources of information ina dialogue system, the proposed approach (classify-ing long silences) is not completely realistic (whathappens when a gap is misclassified as a pause?)
anddoes not attempt to optimize latency.
An extensionto this approach was proposed in (Takeuchi et al,2004), in which a turn-taking decision is made every100 ms during pauses.
However, in this latter workthe features are limited to timing, prosody, and syn-tax (part-of-speech).
Also the reported classificationresults, with F-measures around 50% or below donot seem to be sufficient for practical use.1We use the terminology from (Sacks et al, 1974) where apause is a silence within a turn while a gap is a silence betweenturns.
We use the term silence to encompass both types.Similarly, Ferrer and her colleagues (Ferrer et al,2003) proposed the use of multiple decision trees,each triggered at a specific time in the pause, to de-cide to either endpoint or defer the decision to thenext tree, unless the user resumes speaking.
Usingfeatures like vowel duration or pitch for the regionimmediately preceding the silence, combined with alanguage model that predicts gaps based on the pre-ceding words, Ferrer et alare able shorten latencywhile keeping the FA rate constant.
On a corpusof recorded spoken dialogue-like utterances (ATIS),they report reductions of up to 81% for some FArates.
While very promising, this approach has sev-eral disadvantages.
First it relies on a small set ofpossible decision points for each pause, preventingfine optimization between them.
Second, the treesare trained on increasingly smaller datasets requir-ing smoothing of the tree scores to compensate forpoor training of the later trees (which are trainedon increasingly small subsets of pauses from thetraining set).
Finally, and perhaps most importantly,these authors have investigated prosodic and lexicalfeatures, but not other aspects of dialogue, such asdiscourse structure, timing, and semantics.In this paper, we propose a new approach to end-pointing that directly optimizes thresholds using au-tomatically extracted dialogue features ranging fromdiscourse to timing and prosody.
Section 2 out-lines the proposed algorithm.
Section 3 describesthe analysis of the relationship between silences anda wide range of features available to a standard spo-ken dialogue system (hereafter dialogue features).Evaluation results, both offline and in the deployedLet?s Go system are given in Section 4.2 Dynamic Endpointing ThresholdDecision Trees2.1 OverviewOne issue with current approaches to endpointingis that they rely on binary gap/pause classifiers andthe relationship between optimizing for classifica-tion accuracy vs optmizing to minimize latency isunclear.
Also, the performance we obtained whenapplying classification-based approaches to the Let?sGo data was disappointing.
The accuracy of the clas-sifiers was not sufficient for practical purposes, evenwith the improvements proposed by (Ferrer et al,22003).
We hypothesize that the discrepancy betweenthese results and the good performances reported byothers is due to the noisiness of the Let?s Go data(see Section 3.1.1).
To overcome these issues, wepropose a method that directly optimizes endpoint-ing thresholds using a two-stage process.
First, si-lences are clustered based on dialogue features soas to create groups of silences with similar proper-ties.
Second, a single threshold is set for each clus-ter, so as to minimize the overall latency at a givenfalse alarm rate.
The result of the training processis thus a decision tree on dialogue features that con-tains thresholds at its leaves.
At runtime, every timea silence is detected, the dialogue system runs thedecision tree and sets its endpointing threshold ac-cordingly.
The following sections describe the twotraining stages.2.2 Feature-based Silence ClusteringThe goal of the first stage of training is to clus-ter silences with a similar FA rate/latency trade-off.
The intuition is that we would like to generatelow-threshold clusters, which contain mostly gapsand short pauses, and clusters where long pauseswould be concentrated with no or very few gaps,allowing to set high thresholds that reduce cut-inrate without hurting overall latency.
We used astandard top-down clustering algorithm that exhaus-tively searches binary splits of the data based on fea-ture values.
The split that yields the minimal overallcost is kept, where the cost Cn of cluster Kn is de-fined by the following function:Cn = Gn ?
?1|K|?p?KDuration(p)2 (1)where Gn the number of gaps in Kn andDuration(p) the duration of a pause p, set to zerofor gaps.
While other cost functions are possible, theintuition behind this formula is that it captures boththe cluster?s gap ratio (first factor) and its pause du-ration distribution (second factor: root mean squareof pause duration).
The splitting process is repeatedrecursively until the reduction in cost between theoriginal cost and the sum of the costs of the two splitclusters falls below a certain threshold.
By minimiz-ing C(K), the clustering algorithm will find ques-tions that yield clusters with either a small Gn, i.e.mostly pauses, or a small root mean square pauseduration.
Ultimately, at the leaves of the tree are setsof silences that will share the same threshold.2.3 Cluster Threshold OptimizationGiven the clusters generated by the first phase, thegoal of the second phase is to find a threshold foreach cluster so that the overall latency is minimizedat a given FA rate.
Under the assumption that pausedurations follow an exponential distribution, whichis supported by previous work and our own data (seeSection 3.2), we show in Figure 3 in appendix thatthere is a unique set of thresholds that minimizes la-tency and that the threshold for any cluster n is givenby:?n =?n ?
log(?n ?E?
?nP?n)Gn(2)where ?n and ?n can be estimated from the data.3 Silences and Dialogue Features3.1 Overview of the Data3.1.1 The Let?s Go CorpusLet?s Go is a telephone-based spoken dialoguesystem that provides bus schedule information forthe Pittsburgh metropolitan area.
It is built on theOlympus architecture (Bohus et al, 2007), using theRavenClaw dialogue management framework, andthe Apollo interaction manager (Raux et al, 2007)as core components.
Outside of business hourscallers to the bus company?s customer service areoffered the option to use Let?s Go.
All calls arerecorded and extensively logged for further analy-sis.
The corpus used for this study was collectedbetween December 26, 2007 and January 25, 2008,with a total of 1326 dialogues, and 18013 user turns.Of the calls that had at least 4 user turns, 73% werecomplete, meaning that the system provided someschedule information to the user.While working on real user data has its advan-tages (large amounts of data, increased validity ofthe results), it also has its challenges.
In the case ofLet?s Go, users call from phones of varying quality(cell phones and landlines), often with backgroundnoises such as cars, infant cries, loud television sets,etc.
The wide variability of the acoustic conditionsmakes any sound processing more prone to error3than on carefully recorded corpora.
For example, asreported in (Raux et al, 2005), the original speechrecognizer had been found to yield a 17% word errorrate on a corpus of dialogues collected by recruit-ing subjects to call the system from an office.
Onthe live Let?s Go data, that same recognizer had a68% WER.
After acoustic and language model re-training/adaptation, that number was brought downto about 30% but it is still a testimony to the diffi-culty of obtaining robust features, particularly fromacoustics.3.1.2 Correcting Runtime Endpointing ErrorsLet?s Go uses a GMM-based VAD trained on pre-viously transcribed dialogues.
Endpointing deci-sions are based on a fixed 700 ms threshold on theduration of the detected silences.
One issue whenanalyzing pause distributions from the corpus is thatobserved user behavior was affected by system?s be-havior at runtime.
Most notably, because of the fixedthreshold, no recorded pause lasts more than 700 ms.To compensate for that, we used a simple heuristicto rule some online endpointing decisions as erro-neous.
If a user turn is followed within 1200 ms byanother user turn, we consider these two turns to bein fact a single turn, unless the first turn was a userbarge-in.
This heuristic was established by hand-labeling 200 dialogues from a previous corpus withendpointing errors (i.e.
each turn was annotated ascorrectly or incorrectly endpointed).
On this dataset,the heuristic has a precision of 70.6% and a recall of75.5% for endpointing errors.
Unless specified, allsubsequent results are based on this modified cor-pus.3.2 Turn-Internal Pause Duration DistributionOverall there were 9563 pauses in the corpus, whichamounts to 0.53 pauses per turn.
The latency / FArate trade-off for the corpus is plotted in Figure 1.This curve follows an exponential function (the R2on the linear regression of latency on Log(FA) is0.99).
This stems from the fact that pause durationapproximately follows an exponential distribution,which has been observed by others in the past (Jaffeand Feldstein, 1970; Lennes and Anttila, 2002).One consequence of the exponential-like distribu-tion is that short pauses strongly dominate the distri-bution.
We decided to exclude silences shorter thanFigure 1: Overall False Alarm / Latency trade-off in theLet?s Go corpus.
The dashed line represents a fitted curveof the form FA = e?+?
?Latency .200 ms from most of the following analysis for tworeasons: 1) they are more prone to voice activitydetection errors or short non-pause silences withinspeech (e.g.
unvoiced stop closure), and 2) in orderto apply the results found here to online endpointingby the system, some amount of time is required todetect the silence and compute necessary features,making endpointing decisions on such very short si-lences impractical.
Once short silences have beenexcluded, there are 3083 pauses in the corpus, 0.17per turn.3.3 Relationship Between Dialogue Featuresand Silence Distributions3.3.1 Statistical AnalysisIn order to get some insight into the interactionof the various aspects of dialogue and silence char-acteristics, we investigated a number of features au-tomatically extracted from the dialogue recordingsand system logs.
Each feature is used to split theset of silences into two subsets.
For nominal fea-tures, all possible splits of one value vs all the othersare tested, while for continuous and ordinal features,we tried a number of thresholds and report the onethat yielded the strongest results.
In order to avoidextreme cases that split the data into one very largeand one very small set, we excluded all splits whereeither of the two sets had fewer than 1000 silences.All the investigated splits are reported in Appendix,in Table 1 and 2.
We compare the two subsets gen-erated by each possible split in terms of two metrics:?
Gap Ratio (GR), defined as the proportion of4gaps among all silences of a given set.
We re-port the absolute difference in GR between thetwo sets, and use chi-square in a 2x2 design(pause vs gap and one subset vs the other) totest for statistical significance at the 0.01 level,using Bonferroni correction to compensate formultiple testings.?
Mean pause duration.
The strength of the in-teraction is shown by the difference in meanpause duration, and we use Mann Whitney?sRank Sum test for statistical significance, againat the 0.01 level, using Bonferroni correction.We group features into five categories: discourse,semantics, prosody, turn-taking, and speaker charac-teristics, described in the following sections.3.3.2 Discourse StructureDiscourse structure is captured by the system?s di-alogue act immediately preceding the current userturn.
In the Let?s Go dialogues, 97.9% of sys-tem dialogue acts directly preceding user turns arequestions2.
Of these, 13% are open questions (e.g.
?What can I do for you??
), 39% are closed ques-tions (e.g.
?Where are you leaving from??)
and 46%are confirmation requests (e.g.
?Leaving from theairport.
Is this correct??)3.
There are many morepauses in user responses to open questions than tothe other types (cf Table 1).
One explanation is thatuser answers to open questions tend to be longer(2046 ms on average, to be contrasted with 1268 msfor turns following closed questions and 819 ms forresponses to confirmation questions).
Conversely,confirmation questions lead to responses with sig-nificantly fewer pauses.
78% of such turns con-tained only one word, single YES and NO answersaccounting for 81% of these one-word responses,which obviously do not lend themselves to pauses.Discourse context also has an effect on pause dura-tions, albeit a weak one, with open questions leadingto turns with shorter pauses.
One possible explana-tion for this is that pauses after closed and confirma-tion questions tend to reflect more hesitations and/or2The remaining 2.1% belong to other cases such as the userbarging in right after the system utters a statement.3The high number of confirmations comes from the fact thatLet?s Go is designed to ask the user to explicitly confirm everyconcept.confusion on the user?s side, whereas responses toopen questions also have pauses in the normal flowof speech.3.3.3 SemanticsSemantic features are based on partial speechrecognition results and on their interpretation in thecurrent dialogue context.
We use the most recentrecognition hypothesis available at the time whenthe silence starts, parse it using the system?s standardparser and grammar, and match the parse against the?expectation agenda?
that RavenClaw (Bohus andRudnicky, 2003) maintains.
The expectation levelof a partial utterance indicates how well it fits in thecurrent dialogue context.
A level of 0 means thatthe utterance can be interpreted as a direct answerto the last system prompt (e.g.
a ?PLACE?
con-cept as an answer to ?Where are you leaving from?
?,a ?YES?
or a ?NO?
after a confirmation question).Higher levels correspond to utterances that fit in abroader dialogue context (e.g.
a place name afterthe system asks ?Leaving from the airport.
Is thiscorrect?
?, or ?HELP?
in any context).
Finally, non-understandings, which do not match any expecta-tion, are given a matching level of +?.Expectation level is strongly related to both fi-nality and pause duration.
Pauses following par-tial utterances of expectation level 0 are signifi-cantly more likely to be gaps than those matchingany higher level.
Also, very unexpected partial ut-terances (and non-understandings) contain shorterpauses than more expected ones.
Another indica-tive feature for finality is the presence of a posi-tive marker (i.e.
a word like ?YES?
or ?SURE?)
inthe partial utterance.
Utterances that contain such amarker are more likely to be finished than others.
Incontrast, the effect of negative markers is not signif-icant.
This can be explained by the fact that nega-tive responses to confirmation often lead to longercorrective utterances more prone to pauses.
Indeed,91% of complete utterances that contain a positivemarker are single-word, against 67% for negativemarkers.3.3.4 ProsodyWe extracted three types of prosodic features:acoustic energy of the last vowel, pitch of the lastvoiced region, and duration of the last vowel.
Vowel5location and duration were estimated by performingphoneme alignment with the speech recognizer.
Du-ration was normalized to account for both vowel andspeaker identity.
Energy was computed as the log-transformed signal intensity on 10ms frames.
Pitchwas extracted using the Snack toolkit (Sjolander,2004), also at 10ms intervals.
For both energy andpitch, the slope of the contour was computed by lin-ear regression, and the mean value was normalizedby Z-transformation using statistics of the dialogue-so-far.
As a consequence, all threshold values formeans are expressed in terms of standard deviationsfrom the current speaker?s mean value.Vowel energy, both slope and mean, yielded thehighest correlation with silence finality, although itdid not rank as high as features from other cate-gories.
As expected, vowels immediately preced-ing gaps tend to have lower and falling intensity,whereas rising intensity makes it more likely that theturn is not finished.
On the other hand, extremelyhigh pitch is a strong cue to longer pauses, but onlyhappen in 5.6% of the pauses.3.3.5 TimingTiming features, available from the InteractionManager, provide the strongest cue to finality.
Thelonger the on-going turn has been, the less likely it isthat the current silence is a gap.
This is true both interms of time elapsed since the beginning of the ut-terance and number of pauses observed so far.
Thislatter feature also correlates well with mean pauseduration, earlier pauses of a turn tending to be longerthan later ones.3.3.6 Speaker CharacteristicsThese features correspond to the observed pausalbehavior so far in the dialogue.
The idea is that dif-ferent speakers follow different patterns in the waythey speak (and pause), and that the system shouldbe able to learn these patterns to anticipate futurebehavior.
Specifically, we look at the mean num-ber of pauses per utterance observed so far, and themean pause duration observed so far for the currentdialogue.
Both features correlate reasonably wellwith silence finality: a higher mean duration indi-cates that upcoming silences are also less likely tobe final, so does a higher mean number of pausesper turn.3.4 DiscussionWhat emerges from the analysis above is that fea-tures from all aspects of dialogue provide informa-tion on silence characteristics.
While most previousresearch has focused on prosody as a cue to detectthe end of utterances, timing, discourse, semanticand previously observed silences appear to corre-late more strongly with silence finality in our corpus.This can be partly explained by the fact that prosodicfeatures are harder to reliably estimate on noisy dataand that prosodic features are in fact correlated tohigher levels of dialogue such as discourse and se-mantics.
However, we believe our results make astrong case in favor of a broader approach to turn-taking for conversational agents, making the mostof all the features that are readily available to suchsystems.
Indeed, particularly in constrained systemslike Let?s Go, higher level features like discourseand semantics might be more robust to poor acousticconditions than prosodic features.
Still, our findingson mean pause durations suggest that prosodic fea-tures might be best put to use when trying to pre-dict pause duration, or whether a pause will occuror not.
The key to more natural and responsive di-alogue systems lies in their ability to combine allthese features in order to make prompt and robustturn-taking decisions.4 Evaluation of Threshold Decision Trees4.1 Offline Evaluation Set-UpWe evaluated the approach introduced in Section 2on the Let?s Go corpus.
The set of features was ex-tended to contain a total of 4 discourse features, 6semantic features, 5 timing/turn-taking features, 43prosodic features, and 6 speaker characteristic fea-tures.
All evaluations were performed by 10-foldcross-validation on the corpus.
Based on the pro-posed algorithm, we built a decision tree and com-puted optimal cluster thresholds for different overallFA rates.
We report average latency as a functionof the proportion of turns for which any pause waserroneously endpointed, which is closer to real per-formance than silence FA rate since, once a turn hasbeen endpointed, all subsequent silences are irrele-vant.6Figure 2: Performance of the proposed approach usingdifferent feature sets.4.2 Performance of Different Feature SetsFirst we evaluated each feature set individually.
Theresults are shown in Figure 2.
We concentrate on the2-6% range of turn cut-in rate where any reasonableoperational value is likely to lie (the 700 ms thresh-old of the baseline Let?s Go system yields about 4%cut-in rate).
All feature sets improve over the base-line.
Statistical significance of the result was testedby performing a paired sign test on latencies for thewhole dataset, comparing, for each FA rate the pro-portion of gaps for which the proposed approachgives a shorter threshold than the single-thresholdbaseline.
Latencies produced by the decision treefor all feature sets were all found to be significantlyshorter (p < 0.0001) than the corresponding base-line threshold.The best performing feature set is semantics, fol-lowed by timing, prosody, speaker, and discourse.The maximum relative latency reductions for eachfeature set range from 12% to 22%.
When using allfeatures, the performance improves by a small butsignificant amount compared to any single set, up toa maximum latency reduction of 24%.
This confirmsthat the algorithm is able to combine features effec-tively, and that the features themselves are not com-pletely redundant.
However, while removing seman-tic or timing features from the complete set degradesthe performance, this is not the case for discourse,speaker, nor prosodic features.
This result, similarto what (Sato et al, 2002) reported in their own ex-periment, indicates that prosodic features might beredundant with semantic and timing features.4.3 Live EvaluationWe confirmed the offline evaluation?s findings byimplementing the proposed approach in Let?s Go?sInteraction Manager.
Since prosodic features werenot found to be helpful and since their online ex-traction is costly and error-prone, we did not includethem.
At the beginning of each dialogue, the sys-tem was randomly set as a baseline version, using a700 ms fixed threshold, or as an experimental ver-sion using the tree learned from the offline corpus.Results show that median latency (which includesboth the endpointing threshold and the time to pro-duce the system?s response) is significantly shorterin the experimental version (561 ms) than in thebaseline (957 ms).
Overall, the proposed approachreduced latency by 50% or more in about 48% of theturns.
However, global results like these might notreflect the actual improvement in user experience.Indeed, we know from human-human dialogues thatrelatively long latencies are normal in some circum-stances while very short or no latency is expectedin others.
The proposed algorithm reproduces someof these aspects.
For example, after open questions,where more uncertainty and variability is expected,the experimental version is in fact slightly slower(1047 ms vs 993 ms).
On the other hand, it is fasterafter closed question (800 ms vs 965 ms) and par-ticularly after confirmation requests (324 ms vs 965ms), which are more predictable parts of the dia-logue where high responsiveness is both achievableand natural.
This latter result indicates that our ap-proach has the potential to improve explicit confir-mations, which are often thought to be tedious andirritating to the user.5 ConclusionIn this paper, we described an algorithm to dynami-cally set endpointing threshold for each silence.
Weanalyzed the relationship between silence distribu-tion and a wide range of automatically extracted fea-tures from discourse, semantics, prosody, timing andspeaker characteristics.
When all features are used,the proposed method reduced latency by up to 24%for reasonable false alarm rates.
Prosodic featuresdid not help threshold optimization once other fea-ture were included.
The practicality of the approachand the offline evaluation results were confirmed by7implementing the proposed algorithm in the Let?sGo system.AcknowledgmentsThis work is supported by the US National ScienceFoundation under grant number 0208835.
Any opin-ions, findings, and conclusions or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect the views of the NSF.We would like to thank Alan Black for his manycomments and advice.ReferencesJ.
F. Allen and C. R. Perrault.
1980.
Analyzing intentionin utterances.
Artificial Intelligence, 15:143?178.J.
F. Allen, G. Ferguson, A. Stent, S. Stoness, M. Swift,L.
Galescu, N. Chambers, E. Campana, and G. S. Aist.2005.
Two diverse systems built using generic compo-nents for spoken dialogue (recent progress on trips).
InInteractive Demonstration Track, Association of Com-putational Linguistics Annual Meeting, Ann Arbor,MI.D.
Bohus and A. Rudnicky.
2003.
RavenClaw: Dia-log management using hierarchical task decomposi-tion and an expectation agenda.
In Eurospeech03,Geneva, Switzerland.D.
Bohus, A. Raux, T. Harris, M. Eskenazi, and A. Rud-nicky.
2007.
Olympus: an open-source frameworkfor conversational spoken language interface research.In HLT-NAACL 2007 workshop on Bridging the Gap:Academic and Industrial Research in Dialog Technol-ogy, Rochester, NY, USA.W.
L. Chafe, 1992.
Talking Data: Transcriptionand Coding Methods for Language Research, chapterProsodic and Functional Units of Language, pages 33?43.
Lawrence Erlbaum.H.H.
Clark.
1996.
Using language.
Cambridge Univer-sity Press.S.
Duncan.
1972.
Some signals and rules for takingspeaking turns in conversations.
Journal of Person-ality and Social Psychology, 23(2):283?292.L.
Ferrer, E. Shriberg, and A. Stolcke.
2003.
A prosody-based approach to end-of-utterance detection that doesnot require speech recognition.
In ICASSP, HongKong.C.
E. Ford and S. A. Thompson, 1996.
Interaction andGrammar, chapter Interactional Units in Conversation:Syntactic, Intonational, and Pragmatic Resources forthe Management of Turns, pages 134?184.
CambridgeUniversity Press.H.
Furo.
2001.
Turn-Taking in English and Japanese.Projectability in Grammar, Intonation, and Semantics.Routeledge.B.
J. Grosz and C. Sidner.
1986.
Attention, intentions,and the structure of discourse.
Computational Lin-guistics, 12(3):175?204.J.
Jaffe and S. Feldstein.
1970.
Rhythms of Dialogue.Academic Press.H.
Koiso, Y. Horiuchi, S. Tutiya, A. Ichikawa, andY.
Den.
1998.
An analysis of turn-taking andbackchannels based on prosodic and syntactic featuresin japanese map task dialogs.
Language and Speech,41(3-4):295?321.Mietta Lennes and Hanna Anttila.
2002.
Prosodic fea-tures associated with the distribution of turns in finnishinformal dialogues.
In Petri Korhonen, editor, ThePhonetics Symposium 2002, volume Report 67, pages149?158.
Laboratory of Acoustics and Audio SignalProcessing, Helsinki University of Technology.B.
Orestro?m.
1983.
Turn-Taking in English Conversa-tion.
CWK Gleerup, Lund.A.
Raux, B. Langner, D. Bohus, A. W. Black, and M. Es-kenazi.
2005.
Let?s Go Public!
taking a spoken dialogsystem to the real world.
In Proc.
Interspeech 2005,Lisbon, Portugal.A.
Raux, D. Bohus, B. Langner, A. W. Black, and M. Es-kenazi.
2006.
Doing research on a deployed spokendialogue system: One year of Let?s Go!
experience.In Proc.
Interspeech 2006, Pittsburgh, PA, USA.A.
Raux, , and M. Eskenazi.
2007.
A multi-layer ar-chitecture for semi-synchronous event-driven dialoguemanagement.
In Proc.
ASRU 2007, Kyoto, Japan.C.
Rich and C.L.
Sidner.
1998.
Collagen: A collabora-tion manager for software interface agents.
An Inter-national Journal: User Modeling and User-AdaptedInteraction, 8(3-4):315?350.H.
Sacks, E. A. Schegloff, and G. Jefferson.
1974.A simplest systematics for the organization of turn-taking for conversation.
Language, 50(4):696?735.R.
Sato, R. Higashinaka, M. Tamoto, M. Nakano, andK.
Aikawa.
2002.
Learning decision trees to deter-mine turn-taking by spoken dialogue systems.
In IC-SLP 2002, Denver, CO.Kare Sjolander.
2004.
The snack sound toolkit.http://www.speech.kth.se/snack/.M.
Takeuchi, N. Kitaoka, and S. Nakagawa.
2004.Timing detection for realtime dialog systems usingprosodic and linguistic information.
In Proc.
SpeechProsody 04, Nara, Japan.N.
Ward, A. Rivera, K. Ward, and D. Novick.
2005.
Rootcauses of lost time and user stress in a simple dialogsystem.
In Interspeech 2005, Lisbon, Portugal.8Category Feature test Number of Gap Ratio DifferenceSilencesTiming Pause start time ?
3000 ms 1836 / 19260 65% / 87% -23%Timing Pause number ?
2 3379 / 17717 69% / 88% -19%Discourse Previous question is open 3376 / 17720 70% / 88% -18%Semantics Utterance expectation level ?
1 10025 / 11071 78% / 92% -14%Individual Mean pause duration ?
500 ms 1336 / 19760 72% / 86% -14%Semantics Utterance contains a positive marker 4690 / 16406 96% / 82% 13%Prosody Mean energy of last vowel ?
5 1528 / 19568 74% / 86% -12%Prosody Slope of energy on last vowel ?
0 6922 / 14174 78% / 89% -10%Individual Mean number of pauses per utterance ?
3 1929 / 19267 76% / 86% -10%Semantic Utterance is a non-understanding 6023/15073 79% / 88% -9%Discourse Previous question is a confirmation 8893 / 12203 90% / 82% 8%Prosody Duration of last vowel ?
1 1319 / 19777 78% / 86% -8%Prosody Mean pitch on last voiced region ?
5 1136 / 19960 92% / 85% 7%Prosody Slope of pitch on last voiced region ?
0 6617 / 14479 82% / 87% -4%Semantics Utterance contains a negative marker 2667 / 18429 87% / 85% 2%*Discourse Previous question is closed 8451 / 12645 86% / 85% 1%*Table 1: Effect of Dialogue Features on Pause Finality.
In columns 3 and 4, the first number is for silences for whichthe condition in column 2 is true, while the second number is for those silences where the condition is false.
* indicatesthat the results are not statistically significant at the 0.01 level.Category Feature test Number of Mean pause DifferencePauses Duration (ms) (ms)Prosody Mean pitch on last voiced region ?
4 172 / 2911 608 / 482 126Semantics Utterance Expectation Level ?
4 2202 / 881 475 / 526 -51Prosody Slope of energy on last vowel ?
1 382 / 2701 446 / 495 -39Timing Pause number ?
2 1031 / 2052 459 / 504 -45Discourse Previous question is open 1015 / 2068 460 / 504 -43Individual Mean pause duration ?
500 ms 370 / 2713 455 / 494 -39*Prosody Mean energy of last vowel ?
4.5 404 / 2679 456 / 494 -38*Semantics Utterance contains a positive marker 211 / 2872 522 / 487 35*Discourse Previous question is closed 1178 / 1905 510 / 477 33*Timing Pause start time ?
3000 ms 650 / 2433 465 / 496 -31*Semantic Utterance is a non-understanding 1247 / 1836 472 / 502 -30*Prosody Duration of last vowel ?
0.4 1194 / 1889 507 / 478 29*Individual Mean number of pauses per utterance ?
2 461 / 2622 474 / 492 -19*Semantics Utterance contains a negative marker 344 / 2739 504 / 488 16*Prosody Slope of pitch on last voiced segment ?
0 1158 / 1925 482 / 494 -12*Discourse Previous question is a confirmation 867 / 2216 496 / 487 9*Table 2: Effect of Dialogue Features on Pause Duration.
In columns 3 and 4, the first number is for silences for whichthe condition in column 2 is true, while the second number is for those silences where the condition is false.
* indicatesthat the results are not statistically significant at the 0.01 level.9Let (Kn) be a set of n silence clusters, the goal is to set the thresholds (?n) that minimize overall meanlatency, while yielding a fixed, given number of false alarms E. let us define Gn the number of gaps amongthe silences of Kn.
For each cluster, let us define En(?n) the number of false alarms yielded by threshold?n in cluster n, and the total latency Ln by:Ln(?n) = Gn ?
?n (3)Assuming pause durations follow an exponential distribution, as shown in Section 3, the following relationholds between Ln and En:eLn(?n)?n = ?n ?
En(?n) (4)where ?K and ?K are cluster-specific coefficients estimated by linear regression in the log domain.
If wetake the log of both sides, we obtain:Ln(?n) = ?n ?
log(?n ?
En(?n)) (5)Theorem 1.
If (?n) is a set of thresholds that minimizes?n Ln such that?n En(?n) = E, then?As.t.
?n, dLndEn (?n) = AInformal proof.
The proof can be done by contradiction.
Let us assume (?n) is a set of thresholds thatminimizes?n Ln, and ?
(p, q)s.t.dLpdEp(?p) >dLqdEq(?q).
Then, there exists small neighborhoods of ?p and ?qwhere Lp(Ep) and Lq(Eq) can be approximated by their tangents.
Since their slopes differ, it is possible tofind a small  such that the decrease in FA yielded by ?p +  is exactly compensated by the increase yieldedby ?q ?
, but the reduction in latency in Kq is bigger than the increase in Kp, which contradicts the factthat (?n) minimizes L.From Theorem 1, we get ?As.t.
?n dLndEn = A.
Thus, by deriving Equation 5,?nEn= A which gives En = ?nA .Given that?En = E,P?nA = E. Hence, A =P?nE .
From 5, we can infer the values of Ln(?n) and,using 3, the optimal threshold ?n for each cluster:?n =?n ?
log(?n ?E?
?nP?n)Gn(6)where the values of ?n and ?n can be estimated by linear regression from the data based on 5.Figure 3: Derivation of the formula for optimal thresholds10
