Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 147?155,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsDesign Challenges and Misconceptions in Named Entity Recognition?
?
?Lev Ratinov Dan RothComputer Science DepartmentUniversity of IllinoisUrbana, IL 61801 USA{ratinov2,danr}@uiuc.eduAbstractWe analyze some of the fundamental designchallenges and misconceptions that underliethe development of an efficient and robustNER system.
In particular, we address issuessuch as the representation of text chunks, theinference approach needed to combine localNER decisions, the sources of prior knowl-edge and how to use them within an NERsystem.
In the process of comparing severalsolutions to these challenges we reach somesurprising conclusions, as well as develop anNER system that achieves 90.8 F1 score onthe CoNLL-2003 NER shared task, the bestreported result for this dataset.1 IntroductionNatural Language Processing applications are char-acterized by making complex interdependent deci-sions that require large amounts of prior knowledge.In this paper we investigate one such application?Named Entity Recognition (NER).
Figure 1 illus-trates the necessity of using prior knowledge andnon-local decisions in NER.
In the absence of mixedcase information it is difficult to understand that?
The system and the Webpages dataset are available at:http://l2r.cs.uiuc.edu/?cogcomp/software.php?
This work was supported by NSF grant NSF SoD-HCER-0613885, by MIAS, a DHS-IDS Center for Multimodal In-formation Access and Synthesis at UIUC and by an NDIIPPproject from the National Library of Congress.?
We thank Nicholas Rizzolo for the baseline LBJ NERsystem, Xavier Carreras for suggesting the word class models,and multiple reviewers for insightful comments.SOCCER - [PER BLINKER] BAN LIFTED .
[LOC LONDON] 1996-12-06 [MISC Dutch] forward[PER Reggie Blinker] had his indefinite suspensionlifted by [ORG FIFA] on Friday and was set to makehis [ORG Sheffield Wednesday] comeback against[ORG Liverpool] on Saturday .
[PER Blinker] missedhis club?s last two games after [ORG FIFA] slapped aworldwide ban on him for appearing to sign contracts forboth [ORG Wednesday] and [ORG Udinese] while he wasplaying for [ORG Feyenoord].Figure 1: Example illustrating challenges in NER.?BLINKER?
is a person.
Likewise, it is not obvi-ous that the last mention of ?Wednesday?
is an orga-nization (in fact, the first mention of ?Wednesday?can also be understood as a ?comeback?
which hap-pens on Wednesday).
An NER system could take ad-vantage of the fact that ?blinker?
is also mentionedlater in the text as the easily identifiable ?ReggieBlinker?.
It is also useful to know that Udineseis a soccer club (an entry about this club appearsin Wikipedia), and the expression ?both Wednesdayand Udinese?
implies that ?Wednesday?
and ?Udi-nese?
should be assigned the same label.The above discussion focuses on the need for ex-ternal knowledge resources (for example, that Udi-nese can be a soccer club) and the need for non-local features to leverage the multiple occurrencesof named entities in the text.
While these two needshave motivated some of the research in NER inthe last decade, several other fundamental decisionsmust be made.
These include: what model to use for147sequential inference, how to represent text chunksand what inference (decoding) algorithm to use.Despite the recent progress in NER, the effort hasbeen dispersed in several directions and there are nopublished attempts to compare or combine the re-cent advances, leading to some design misconcep-tions and less than optimal performance.
In thispaper we analyze some of the fundamental designchallenges and misconceptions that underlie the de-velopment of an efficient and robust NER system.We find that BILOU representation of text chunkssignificantly outperforms the widely adopted BIO.Surprisingly, naive greedy inference performs com-parably to beamsearch or Viterbi, while being con-siderably more computationally efficient.
We ana-lyze several approaches for modeling non-local de-pendencies proposed in the literature and find thatnone of them clearly outperforms the others acrossseveral datasets.
However, as we show, these contri-butions are, to a large extent, independent and, as weshow, the approaches can be used together to yieldbetter results.
Our experiments corroborate recentlypublished results indicating that word class modelslearned on unlabeled text can significantly improvethe performance of the system and can be an al-ternative to the traditional semi-supervised learningparadigm.
Combining recent advances, we developa publicly available NER system that achieves 90.8F1 score on the CoNLL-2003 NER shared task, thebest reported result for this dataset.
Our system is ro-bust ?
it consistently outperforms all publicly avail-able NER systems (e.g., the Stanford NER system)on all three datasets.2 Datasets and Evaluation MethodologyNER system should be robust across multiple do-mains, as it is expected to be applied on a diverse setof documents: historical texts, news articles, patentapplications, webpages etc.
Therefore, we have con-sidered three datasets: CoNLL03 shared task data,MUC7 data and a set of Webpages we have anno-tated manually.
In the experiments throughout thepaper, we test the ability of the tagger to adapt to newtest domains.
Throughout this work, we train on theCoNLL03 data and test on the other datasets withoutretraining.
The differences in annotation schemesacross datasets created evaluation challenges.
Wediscuss the datasets and the evaluation methods be-low.The CoNLL03 shared task data is a subset ofReuters 1996 news corpus annotated with 4 entitytypes: PER,ORG, LOC, MISC.
It is important tonotice that both the training and the developmentdatasets are news feeds from August 1996, while thetest set contains news feeds from December 1996.The named entities mentioned in the test dataset areconsiderably different from those that appear in thetraining or the development set.
As a result, the testdataset is considerably harder than the developmentset.
Evaluation: Following the convention, we re-port phrase-level F1 score.The MUC7 dataset is a subset of the NorthAmerican News Text Corpora annotated with a widevariety of entities including people, locations, or-ganizations, temporal events, monetary units, andso on.
Since there was no direct mapping fromtemporal events, monetary units, and other entitiesfrom MUC7 and the MISC label in the CoNLL03dataset, we measure performance only on PER,ORGand LOC.
Evaluation: There are several sourcesof inconsistency in annotation between MUC7 andCoNLL03.
For example, since the MUC7 datasetdoes not contain the MISC label, in the sentence?balloon, called the Virgin Global Challenger?
, theexpression Virgin Global Challenger should be la-beled as MISC according to CoNLL03 guidelines.However, the gold annotation in MUC7 is ?balloon,called the [ORG Virgin] Global Challenger?.
Theseand other annotation inconsistencies have promptedus to relax the requirements of finding the exactphrase boundaries and measure performance usingtoken-level F1.Webpages - we have assembled and manually an-notated a collection of 20 webpages, including per-sonal, academic and computer-science conferencehomepages.
The dataset contains 783 entities (96-loc, 223-org, 276-per, 188-misc).
Evaluation: Thenamed entities in the webpages were highly am-biguous and very different from the named entitiesseen in the training data.
For example, the data in-cluded sentences such as : ?Hear, O Israel, the Lordour God, the Lord is one.?
We could not agree onwhether ?O Israel?
should be labeled as ORG, LOC,or PER.
Similarly, we could not agree on whether?God?
and ?Lord?
is an ORG or PER.
These issues148led us to report token-level entity-identification F1score for this dataset.
That is, if a named entity to-ken was identified as such, we counted it as a correctprediction ignoring the named entity type.3 Design Challenges in NERIn this section we introduce the baseline NER sys-tem, and raise the fundamental questions underlyingrobust and efficient design.
These questions definethe outline of this paper.
NER is typically viewedas a sequential prediction problem, the typical mod-els include HMM (Rabiner, 1989), CRF (Laffertyet al, 2001), and sequential application of Per-ceptron or Winnow (Collins, 2002).
That is, letx = (x1, .
.
.
, xN ) be an input sequence and y =(y1, .
.
.
, yN ) be the output sequence.
The sequentialprediction problem is to estimate the probabilitiesP (yi|xi?k .
.
.
xi+l, yi?m .
.
.
yi?1),where k, l and m are small numbers to allowtractable inference and avoid overfitting.
This con-ditional probability distribution is estimated in NERusing the following baseline set of features (Zhangand Johnson, 2003): (1) previous two predictionsyi?1 and yi?2 (2) current word xi (3) xi word type(all-capitalized, is-capitalized, all-digits, alphanu-meric, etc.)
(4) prefixes and suffixes of xi (5) tokensin the window c = (xi?2, xi?1, xi, xi+1, xi+2) (6)capitalization pattern in the window c (7) conjunc-tion of c and yi?1.Most NER systems use additional features, suchas POS tags, shallow parsing information andgazetteers.
We discuss additional features in the fol-lowing sections.
We note that we normalize datesand numbers, that is 12/3/2008 becomes *Date*,1980 becomes *DDDD* and 212-325-4751 becomes*DDD*-*DDD*-*DDDD*.
This allows a degree of ab-straction to years, phone numbers, etc.Our baseline NER system uses a regularized aver-aged perceptron (Freund and Schapire, 1999).
Sys-tems based on perceptron have been shown to becompetitive in NER and text chunking (Kazama andTorisawa, 2007b; Punyakanok and Roth, 2001; Car-reras et al, 2003) We specify the model and the fea-tures with the LBJ (Rizzolo and Roth, 2007) mod-eling language.
We now state the four fundamentaldesign decisions in NER system which define thestructure of this paper.Algorithm Baseline system Final SystemGreedy 83.29 90.57Beam size=10 83.38 90.67Beam size=100 83.38 90.67Viterbi 83.71 N/ATable 1: Phrase-level F1 performance of different inferencemethods on CoNLL03 test data.
Viterbi cannot be used in theend system due to non-local features.Key design decisions in an NER system.1) How to represent text chunks in NER system?2) What inference algorithm to use?3) How to model non-local dependencies?4) How to use external knowledge resources in NER?4 Inference & Chunk RepresentationIn this section we compare the performance of sev-eral inference (decoding) algorithms: greedy left-to-right decoding, Viterbi and beamsearch.
It mayappear that beamsearch or Viterbi will performmuch better than naive greedy left-to-right decoding,which can be seen as beamsearch of size one.
TheViterbi algorithm has the limitation that it does notallow incorporating some of the non-local featureswhich will be discussed later, therefore, we cannotuse it in our end system.
However, it has the appeal-ing quality of finding the most likely assignment toa second-order model, and since the baseline fea-tures only have second order dependencies, we havetested it on the baseline configuration.Table 1 compares between the greedy decoding,beamsearch with varying beam size, and Viterbi,both for the system with baseline features and for theend system (to be presented later).
Surprisingly, thegreedy policy performs well, this phenmenon wasalso observed in the POS tagging task (Toutanovaet al, 2003; Roth and Zelenko, 1998).
The impli-cations are subtle.
First, due to the second-order ofthe model, the greedy decoding is over 100 timesfaster than Viterbi.
The reason is that with theBILOU encoding of four NE types, each token cantake 21 states (O, B-PER, I-PER , U-PER, etc.).
Totag a token, the greedy policy requires 21 compar-isons, while the Viterbi requires 213, and this analy-sis carries over to the number of classifier invoca-tions.
Furthermore, both beamsearch and Viterbirequire transforming the predictions of the classi-149Rep.
CoNLL03 MUC7Scheme Test Dev Dev TestBIO 89.15 93.61 86.76 85.15BILOU 90.57 93.28 88.09 85.62Table 2: End system performance with BILOU and BIOschemes.
BILOU outperforms the more widely used BIO.fiers to probabilities as discussed in (Niculescu-Mizil and Caruana, 2005), incurring additional timeoverhead.
Second, this result reinforces the intuitionthat global inference over the second-order HMMfeatures does not capture the non-local propertiesof the task.
The reason is that the NEs tend tobe short chunks separated by multiple ?outside?
to-kens.
This separation ?breaks?
the Viterbi decisionprocess to independent maximization of assignmentover short chunks, where the greedy policy performswell.
On the other hand, dependencies between iso-lated named entity chunks have longer-range depen-dencies and are not captured by second-order tran-sition features, therefore requiring separate mecha-nisms, which we discuss in Section 5.Another important question that has been stud-ied extensively in the context of shallow parsing andwas somewhat overlooked in the NER literature isthe representation of text segments (Veenstra, 1999).Related works include voting between several rep-resentation schemes (Shen and Sarkar, 2005), lex-icalizing the schemes (Molina and Pla, 2002) andautomatically searching for best encoding (Edward,2007).
However, we are not aware of similar workin the NER settings.
Due to space limitations, we donot discuss all the representation schemes and com-bining predictions by voting.
We focus instead ontwo most popular schemes?
BIO and BILOU.
TheBIO scheme suggests to learn classifiers that iden-tify the Beginning, the Inside and the Outside ofthe text segments.
The BILOU scheme suggeststo learn classifiers that identify the Beginning, theInside and the Last tokens of multi-token chunksas well as Unit-length chunks.
The BILOU schemeallows to learn a more expressive model with onlya small increase in the number of parameters to belearned.
Table 2 compares the end system?s perfor-mance with BIO and BILOU.
Examining the results,we reach two conclusions: (1) choice of encod-ing scheme has a big impact on the system perfor-mance and (2) the less used BILOU formalism sig-nificantly outperforms the widely adopted BIO tag-ging scheme.
We use the BILOU scheme throughoutthe paper.5 Non-Local FeaturesThe key intuition behind non-local features in NERhas been that identical tokens should have identi-cal label assignments.
The sample text discussedin the introduction shows one such example, whereall occurrences of ?blinker?
are assigned the PERlabel.
However, in general, this is not always thecase; for example we might see in the same doc-ument the word sequences ?Australia?
and ?Thebank of Australia?.
The first instance should be la-beled as LOC, and the second as ORG.
We considerthree approaches proposed in the literature in the fol-lowing sections.
Before continuing the discussion,we note that we found that adjacent documents inthe CoNLL03 and the MUC7 datasets often discussthe same entities.
Therefore, we ignore documentboundaries and analyze global dependencies in 200and 1000 token windows.
These constants were se-lected by hand after trying a small number of val-ues.
We believe that this approach will also makeour system more robust in cases when the documentboundaries are not given.5.1 Context aggregation(Chieu and Ng, 2003) used features that aggre-gate, for each document, the context tokens appearin.
Sample features are: the longest capitilized se-quence of words in the document which containsthe current token and the token appears before acompany marker such as ltd. elsewhere in text.In this work, we call this type of features con-text aggregation features.
Manually designed con-text aggregation features clearly have low coverage,therefore we used the following approach.
Recallthat for each token instance xi, we use as featuresthe tokens in the window of size two around it:ci = (xi?2, xi?1, xi, xi+1, xi+2).
When the sametoken type t appears in several locations in the text,say xi1 , xi2 , .
.
.
, xiN , for each instance xij , in ad-dition to the context features cij , we also aggregatethe context across all instances within 200 tokens:C = ?j=Nj=1 cij .150CoNLL03 CoNLL03 MUC7 MUC7 WebComponent Test data Dev data Dev Test pages1) Baseline 83.65 89.25 74.72 71.28 71.412) (1) + Context Aggregation 85.40 89.99 79.16 71.53 70.763) (1) + Extended Prediction History 85.57 90.97 78.56 74.27 72.194) (1)+ Two-stage Prediction Aggregation 85.01 89.97 75.48 72.16 72.725) All Non-local Features (1-4) 86.53 90.69 81.41 73.61 71.21Table 3: The utility of non-local features.
The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 andWebpages.
No single technique outperformed the rest on all domains.
The combination of all techniques is the most robust.5.2 Two-stage prediction aggregationContext aggregation as done above can lead to ex-cessive number of features.
(Krishnan and Manning,2006) used the intuition that some instances of a to-ken appear in easily-identifiable contexts.
Thereforethey apply a baseline NER system, and use the re-sulting predictions as features in a second level of in-ference.
We call the technique two-stage predictionaggregation.
We implemented the token-majorityand the entity-majority features discussed in (Krish-nan and Manning, 2006); however, instead of docu-ment and corpus majority tags, we used relative fre-quency of the tags in a 1000 token window.5.3 Extended prediction historyBoth context aggregation and two-stage predictionaggregation treat all tokens in the text similarly.However, we observed that the named entities in thebeginning of the documents tended to be more easilyidentifiable and matched gazetteers more often.
Thisis due to the fact that when a named entity is intro-duced for the first time in text, a canonical name isused, while in the following discussion abbreviatedmentions, pronouns, and other references are used.To break the symmetry, when using beamsearch orgreedy left-to-right decoding, we use the fact thatwhen we are making a prediction for token instancexi, we have already made predictions y1, .
.
.
, yi?1for token instances x1, .
.
.
, xi?1.
When making theprediction for token instance xi, we record the la-bel assignment distribution for all token instancesfor the same token type in the previous 1000 words.That is, if the token instance is ?Australia?, and inthe previous 1000 tokens, the token type ?Australia?was twice assigned the label L-ORG and three timesthe label U-LOC, then the prediction history featurewill be: (L?ORG : 25 ;U ?
LOC : 35).5.4 Utility of non-local featuresTable 3 summarizes the results.
Surprisingly, nosingle technique outperformed the others on alldatasets.
The extended prediction history methodwas the best on CoNLL03 data and MUC7 test set.Context aggregation was the best method for MUC7development set and two-stage prediction was thebest for Webpages.
Non-local features proved lesseffective for MUC7 test set and the Webpages.
Sincethe named entities in Webpages have less context,this result is expected for the Webpages.
However,we are unsure why MUC7 test set benefits from non-local features much less than MUC7 developmentset.
Our key conclusion is that no single approachis better than the rest and that the approaches arecomplimentary- their combination is the most stableand best performing.6 External KnowledgeAs we have illustrated in the introduction, NER isa knowledge-intensive task.
In this section, we dis-cuss two important knowledge resources?
gazetteersand unlabeled text.6.1 Unlabeled TextRecent successful semi-supervised systems (Andoand Zhang, 2005; Suzuki and Isozaki, 2008) haveillustrated that unlabeled text can be used to im-prove the performance of NER systems.
In thiswork, we analyze a simple technique of using wordclusters generated from unlabeled text, which hasbeen shown to improve performance of dependencyparsing (Koo et al, 2008), Chinese word segmen-tation (Liang, 2005) and NER (Miller et al, 2004).The technique is based on word class models, pio-neered by (Brown et al, 1992), which hierarchically151CoNLL03 CoNLL03 MUC7 MUC7 WebComponent Test data Dev data Dev Test pages1) Baseline 83.65 89.25 74.72 71.28 71.412) (1) + Gazetteer Match 87.22 91.61 85.83 80.43 74.463) (1) + Word Class Model 86.82 90.85 80.25 79.88 72.264) All External Knowledge 88.55 92.49 84.50 83.23 74.44Table 4: Utility of external knowledge.
The system was trained on CoNLL03 data and tested on CoNNL03, MUC7 and Webpages.clusters words, producing a binary tree as in Fig-ure 2.Figure 2: An extract from word cluster hierarchy.The approach is related, but not identical, to dis-tributional similarity (for details, see (Brown et al,1992) and (Liang, 2005)).
For example, since thewords Friday and Tuesday appear in similar con-texts, the Brown algorithm will assign them to thesame cluster.
Successful abstraction of both as aday of the week, addresses the data sparsity prob-lem common in NLP tasks.
In this work, we use theimplementation and the clusters obtained in (Liang,2005) from running the algorithm on the Reuters1996 dataset, a superset of the CoNLL03 NERdataset.
Within the binary tree produced by the al-gorithm, each word can be uniquely identified byits path from the root, and this path can be com-pactly represented with a bit string.
Paths of dif-ferent depths along the path from the root to theword provide different levels of word abstraction.For example, paths at depth 4 closely correspondto POS tags.
Since word class models use largeamounts of unlabeled data, they are essentially asemi-supervised technique, which we use to consid-erably improve the performance of our system.In this work, we used path prefixes of length4,6,10, and 20.
When Brown clusters are used asfeatures in the following sections, it implies that allfeatures in the system which contain a word formwill be duplicated and a new set of features con-taining the paths of varying length will be intro-duced.
For example, if the system contains the fea-ture concatenation of the current token and the sys-tem prediction on the previous word, four new fea-tures will be introduced which are concatenationsof the previous prediction and the 4,6,10,20 lengthpath-representations of the current word.6.2 GazetteersAn important question at the inception of the NERtask was whether machine learning techniques arenecessary at all, and whether simple dictionarylookup would be sufficient for good performance.Indeed, the baseline for the CoNLL03 shared taskwas essentially a dictionary lookup of the enti-ties which appeared in the training data, and itachieves 71.91 F1 score on the test set (Tjong andDe Meulder, 2003).
It turns out that while prob-lems of coverage and ambiguity prevent straightfor-ward lookup, injection of gazetteer matches as fea-tures in machine-learning based approaches is crit-ical for good performance (Cohen, 2004; Kazamaand Torisawa, 2007a; Toral and Munoz, 2006; Flo-rian et al, 2003).
Given these findings, several ap-proaches have been proposed to automatically ex-tract comprehensive gazetteers from the web andfrom large collections of unlabeled text (Etzioniet al, 2005; Riloff and Jones, 1999) with lim-ited impact on NER.
Recently, (Toral and Munoz,2006; Kazama and Torisawa, 2007a) have success-fully constructed high quality and high coveragegazetteers from Wikipedia.In this work, we use a collection of 14 high-precision, low-recall lists extracted from the webthat cover common names, countries, monetaryunits, temporal expressions, etc.
While thesegazetteers have excellent accuracy, they do not pro-vide sufficient coverage.
To further improve thecoverage, we have extracted 16 gazetteers fromWikipedia, which collectively contain over 1.5M en-tities.
Overall, we have 30 gazetteers (availablefor download with the system), and matches against152CoNLL03 CoNLL03 MUC7 MUC7 WebComponent Test data Dev data Dev Test pages1) Baseline 83.65 89.25 74.72 71.28 71.412) (1) + External Knowledge 88.55 92.49 84.50 83.23 74.443) (1) + Non-local 86.53 90.69 81.41 73.61 71.214) All Features 90.57 93.50 89.19 86.15 74.535) All Features (train with dev) 90.80 N/A 89.19 86.15 74.33Table 5: End system performance by component.
Results confirm that NER is a knowledge-intensive task.each one are weighted as a separate feature in thesystem (this allows us to trust each gazetteer to a dif-ferent degree).
We also note that we have developeda technique for injecting non-exact string matchingto gazetteers, which has marginally improved theperformance, but is not covered in the paper due tospace limitations.
In the rest of this section, we dis-cuss the construction of gazetteers from Wikipedia.Wikipedia is an open, collaborative encyclopediawith several attractive properties.
(1) It is kept up-dated manually by it collaborators, hence new enti-ties are constantly added to it.
(2) Wikipedia con-tains redirection pages, mapping several variationsof spelling of the same name to one canonical en-try.
For example, Suker is redirected to an entryabout Davor S?uker, the Croatian footballer (3) Theentries in Wikipedia are manually tagged with cate-gories.
For example, the entry about the Microsoftin Wikipedia has the following categories: Companieslisted on NASDAQ; Cloud computing vendors; etc.Both (Toral and Munoz, 2006) and (Kazama andTorisawa, 2007a) used the free-text description ofthe Wikipedia entity to reason about the entity type.We use a simpler method to extract high coverageand high quality gazetteers from Wikipedia.
Byinspection of the CoNLL03 shared task annotationguidelines and of the training set, we manually ag-gregated several categories into a higher-level con-cept (not necessarily NER type).
When a Wikipediaentry was tagged by one of the categories in the ta-ble, it was added to the corresponding gazetteer.6.3 Utility of External KnowledgeTable 4 summarizes the results of the techniquesfor injecting external knowledge.
It is importantto note that, although the world class model waslearned on the superset of CoNLL03 data, and al-though the Wikipedia gazetteers were constructedDataset Stanford-NER LBJ-NERMUC7 Test 80.62 85.71MUC7 Dev 84.67 87.99Webpages 72.50 74.89Reuters2003 test 87.04 90.74Reuters2003 dev 92.36 93.94Table 6: Comparison: token-based F1 score of LBJ-NER andStanford NER tagger across several domainsbased on CoNLL03 annotation guidelines, these fea-tures proved extremely good on all datasets.
Wordclass models discussed in Section 6.1 are computedoffline, are available online1, and provide an alter-native to traditional semi-supervised learning.
It isimportant to note that the word class models and thegazetteers and independednt and accumulative.
Fur-thermore, despite the number and the gigantic sizeof the extracted gazetteers, the gazeteers alone arenot sufficient for adequate performance.
When wemodified the CoNLL03 baseline to include gazetteermatches, the performance went up from 71.91 to82.3 on the CoNLL03 test set, below our baselinesystem?s result of 83.65.
When we have injected thegazetteers into our system, the performance went upto 87.22.
Word class model and nonlocal featuresfurther improve the performance to 90.57 (see Ta-ble 5), by more than 3 F1 points.7 Final System Performance AnalysisAs a final experiment, we have trained our systemboth on the training and on the development set,which gave us our best F1 score of 90.8 on theCoNLL03 data, yet it failed to improve the perfor-mance on other datasets.
Table 5 summarizes theperformance of the system.Next, we have compared the performance of our1http://people.csail.mit.edu/maestro/papers/bllip-clusters.gz153system to that of the Stanford NER tagger, across thedatasets discussed above.
We have chosen to com-pare against the Stanford tagger because to the bestof our knowledge, it is the best publicly availablesystem which is trained on the same data.
We havedownloaded the Stanford NER tagger and used thestrongest provided model trained on the CoNLL03data with distributional similarity features.
The re-sults we obtained on the CoNLL03 test set wereconsistent with what was reported in (Finkel et al,2005).
Our goal was to compare the performance ofthe taggers across several datasets.
For the most re-alistic comparison, we have presented each systemwith a raw text, and relied on the system?s sentencesplitter and tokenizer.
When evaluating the systems,we matched against the gold tokenization ignoringpunctuation marks.
Table 6 summarizes the results.Note that due to differences in sentence splitting, to-kenization and evaluation, these results are not iden-tical to those reported in Table 5.
Also note that inthis experiment we have used token-level accuracyon the CoNLL dataset as well.
Finally, to completethe comparison to other systems, in Table 7 we sum-marize the best results reported for the CoNLL03dataset in literature.8 ConclusionsWe have presented a simple model for NER thatuses expressive features to achieve new state of theart performance on the Named Entity recognitiontask.
We explored four fundamental design deci-sions: text chunks representation, inference algo-rithm, using non-local features and external knowl-edge.
We showed that BILOU encoding scheme sig-nificantly outperforms BIO and that, surprisingly, aconditional model that does not take into account in-teractions at the output level performs comparablyto beamsearch or Viterbi, while being considerablymore efficient computationally.
We analyzed sev-eral approaches for modeling non-local dependen-cies and found that none of them clearly outperformsthe others across several datasets.
Our experimentscorroborate recently published results indicating thatword class models learned on unlabeled text canbe an alternative to the traditional semi-supervisedlearning paradigm.
NER proves to be a knowledge-intensive task, and it was reassuring to observe thatSystem Resources Used F1+ LBJ-NER Wikipedia, Nonlocal Fea-tures, Word-class Model90.80- (Suzuki andIsozaki, 2008)Semi-supervised on 1G-word unlabeled data89.92- (Ando andZhang, 2005)Semi-supervised on 27M-word unlabeled data89.31- (Kazama andTorisawa, 2007a)Wikipedia 88.02- (Krishnan andManning, 2006)Non-local Features 87.24- (Kazama andTorisawa, 2007b)Non-local Features 87.17+ (Finkel et al,2005)Non-local Features 86.86Table 7: Results for CoNLL03 data reported in the literature.publicly available systems marked by +.knowledge-driven techniques adapt well across sev-eral domains.
We observed consistent performancegains across several domains, most interestingly inWebpages, where the named entities had less contextand were different in nature from the named entitiesin the training set.
Our system significantly outper-forms the current state of the art and is available todownload under a research license.Apendix?
wikipedia gazetters & categories1)People: people, births, deaths.
Extracts 494,699 Wikipediatitles and 382,336 redirect links.
2)Organizations: cooper-atives, federations, teams, clubs, departments, organizations,organisations, banks, legislatures, record labels, constructors,manufacturers, ministries, ministers, military units, militaryformations, universities, radio stations, newspapers, broad-casters, political parties, television networks, companies, busi-nesses, agencies.
Extracts 124,403 titles and 130,588 redi-rects.
3)Locations: airports, districts, regions, countries, ar-eas, lakes, seas, oceans, towns, villages, parks, bays, bases,cities, landmarks, rivers, valleys, deserts, locations, places,neighborhoods.
Extracts 211,872 titles and 194,049 redirects.4)Named Objects: aircraft, spacecraft, tanks, rifles, weapons,ships, firearms, automobiles, computers, boats.
Extracts 28,739titles and 31,389 redirects.
5)Art Work: novels, books, paint-ings, operas, plays.
Extracts 39,800 titles and 34037 redirects.6)Films: films, telenovelas, shows, musicals.
Extracts 50,454titles and 49,252 redirects.
7)Songs: songs, singles, albums.Extracts 109,645 titles and 67,473 redirects.
8)Events: playoffs,championships, races, competitions, battles.
Extracts 20,176 ti-tles and 15,182 redirects.154ReferencesR.
K. Ando and T. Zhang.
2005.
A high-performancesemi-supervised learning method for text chunking.
InACL.P.
F. Brown, P. V. deSouza, R. L. Mercer, V. J. D.Pietra, and J. C. Lai.
1992.
Class-based n-gram mod-els of natural language.
Computational Linguistics,18(4):467?479.X.
Carreras, L. Ma`rquez, and L. Padro?.
2003.
Learn-ing a perceptron-based named entity chunker via on-line recognition feedback.
In CoNLL.H.
Chieu and H. T. Ng.
2003.
Named entity recognitionwith a maximum entropy approach.
In Proceedings ofCoNLL.W.
W. Cohen.
2004.
Exploiting dictionaries in namedentity extraction: Combining semi-markov extractionprocesses and data integration methods.
In KDD.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In EMNLP.L.
Edward.
2007.
Finding good sequential model struc-tures using output transformations.
In EMNLP).O.
Etzioni, M. J. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.2005.
Unsupervised named-entity extraction from theweb: An experimental study.
Artificial Intelligence,165(1):91?134.J.
R. Finkel, T. Grenager, and C. D. Manning.
2005.
In-corporating non-local information into information ex-traction systems by gibbs sampling.
In ACL.R.
Florian, A. Ittycheriah, H. Jing, and T. Zhang.
2003.Named entity recognition through classifier combina-tion.
In CoNLL.Y.
Freund and R. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37(3):277?296.J.
Kazama and K. Torisawa.
2007a.
Exploiting wikipediaas external knowledge for named entity recognition.
InEMNLP.J.
Kazama and K. Torisawa.
2007b.
A new perceptron al-gorithm for sequence labeling with non-local features.In EMNLP-CoNLL.T.
Koo, X. Carreras, and M. Collins.
2008.
Simple semi-supervised dependency parsing.
In ACL.V.
Krishnan and C. D. Manning.
2006.
An effective two-stage model for exploiting non-local dependencies innamed entity recognition.
In ACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In ICML.
Mor-gan Kaufmann.P.
Liang.
2005.
Semi-supervised learning for naturallanguage.
Masters thesis, Massachusetts Institute ofTechnology.S.
Miller, J. Guinness, and A. Zamanian.
2004.
Nametagging with word clusters and discriminative training.In HLT-NAACL.A.
Molina and F. Pla.
2002.
Shallow parsing using spe-cialized hmms.
The Journal of Machine Learning Re-search, 2:595?613.A.
Niculescu-Mizil and R. Caruana.
2005.
Predictinggood probabilities with supervised learning.
In ICML.V.
Punyakanok and D. Roth.
2001.
The use of classifiersin sequential inference.
In NIPS.L.
R. Rabiner.
1989.
A tutorial on hidden markov mod-els and selected applications in speech recognition.
InIEEE.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In AAAI.N.
Rizzolo and D. Roth.
2007.
Modeling discriminativeglobal inference.
In ICSC.D.
Roth and D. Zelenko.
1998.
Part of speech tagging us-ing a network of linear separators.
In COLING-ACL.H.
Shen and A. Sarkar.
2005.
Voting between multipledata representations for text chunking.
Advances inArtificial Intelligence, pages 389?400.J.
Suzuki and H. Isozaki.
2008.
Semi-supervised sequen-tial labeling and segmentation using giga-word scaleunlabeled data.
In ACL.E.
Tjong, K. and F. De Meulder.
2003.
Introductionto the conll-2003 shared task: Language-independentnamed entity recognition.
In CoNLL.A.
Toral and R. Munoz.
2006.
A proposal to automat-ically build and maintain gazetteers for named entityrecognition by using wikipedia.
In EACL.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In NAACL.J.
Veenstra.
1999.
Representing text chunks.
In EACL.T.
Zhang and D. Johnson.
2003.
A robust risk mini-mization based named entity recognition system.
InCoNLL.155
