Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1335?1343,Beijing, August 2010Resolving Surface Forms to Wikipedia TopicsYiping Zhou   Lan Nie   Omid Rouhani-Kalleh   Flavian Vasile   Scott GaffneyYahoo!
Labs at Sunnyvale{zhouy,lannie,omid,flavian,gaffney}@yahoo-inc.comAbstractAmbiguity of entity mentions and con-cept references is a challenge to miningtext beyond surface-level keywords.
Wedescribe an effective method of disambi-guating surface forms and resolving themto Wikipedia entities and concepts.
Ourmethod employs an extensive set of fea-tures mined from Wikipedia and otherlarge data sources, and combines the fea-tures using a machine learning approachwith automatically generated training da-ta.
Based on a manually labeled evalua-tion set containing over 1000 news ar-ticles, our resolution model has 85% pre-cision and 87.8% recall.
The performanceis significantly better than three baselinesbased on traditional context similaritiesor sense commonness measurements.
Ourmethod can be applied to other languagesand scales well to new entities and con-cepts.1 IntroductionAmbiguity in natural language is prevalent and,as such, it can be a difficult challenge for infor-mation retrieval systems and other text miningapplications.
For example, a search for ?Ford?
inYahoo!
News retrieves about 40 thousand ar-ticles containing Ford referring to a company(Ford Motors), an athlete (Tommy Ford), a place(Ford City), etc.
Due to reference ambiguity,even if we knew the user was only interested inthe company, they would still have to contendwith articles referring to the other concepts aswell.In this paper we focus on the problem of re-solving references of named-entities and con-cepts in natural language through their textualsurface forms.
Specifically, we present a methodof resolving surface forms in general text docu-ments to Wikipedia entries.
The tasks of resolu-tion and disambiguation are nearly identical; wemake the distinction that resolution specificallyapplies when a known set of referent conceptsare given a priori.
Our approach differs from oth-ers in multiple aspects including the following.1) We employ a rich set of disambiguationfeatures leveraging mining results from large-scale data sources.
We calculate context-sensitive features by extensively mining the cat-egories, links and contents of the entire Wikipe-dia corpus.
Additionally we make use of context-independent data mined from various datasources including Web user-behavioral data andWikipedia.
Our features also capture the one-to-one relationship between a surface form and itsreferent.2) We use machine learning methods to trainresolution models with a large automatically la-beled training set.
Both ranking-based and classi-fication-based resolution approaches are ex-plored.3) Our method disambiguates both entities andword senses.
It scales well to new entities andconcepts, and it can be easily applied to otherlanguages.We propose an extensive set of metrics to eva-luate not only overall resolution performance butalso out-of-Wikipedia prediction.
Our systemsfor English language are evaluated using real-world test sets and compared with a number ofbaselines.
Evaluation results show that our sys-tems consistently and significantly outperformothers across all test sets.The paper is organized as follows.
We first de-scribe related research in Section 2, followed byan introduction of Wikipedia in Section 3.
Wethen introduce our learning method in Section 4and our features in Section 5.
We show our expe-rimental results in Section 6, and finally closewith a discussion of future work.13352 Related WorkNamed entity disambiguation research can bedivided into two categories: some works (Baggaand Baldwin, 1998; Mann and Yarowsky, 2003;Pedersen et al, 2005; Fleischman and Hovy,2004; Ravin and Kazi, 1999) aim to cluster am-biguous surface forms to different groups, witheach representing a unique entity; others (Cucer-zan, 2007; Bunescu and Pa?ca, 2006; Han andZhao, 2009; Milne and Witten, 2008a; Milne andWitten, 2008b) resolve a surface form to an enti-ty or concept extracted from existing knowledgebases.
Our work falls into the second category.Looking specifically at resolution, Bunescuand Pasca (2006) built a taxonomy SVM kernelto enrich a surface form?s representation withwords from Wikipedia articles in the same cate-gory.
Cucerzan (2007) employed context vectorsconsisting of phrases and categories extractedfrom Wikipedia.
The system also attempted todisambiguate all surface forms in a context si-multaneously, with the constraint that their re-solved entities should be globally consistent onthe category level as much as possible.
Milne andWitten (2008a, 2008b) proposed to use Wikipe-dia?s link structure to capture the relatednessbetween Wikipedia entities so that a surface formis resolved to an entity based on its relatedness tothe surface form?s surrounding entities.
Besidesrelatedness, they also define a commonness fea-ture that captures how common it is that a sur-face form links to a particular entity in general.Han and Zhao (2009) defined a novel alignmentstrategy to calculate similarity between surfaceforms based on semantic relatedness in the con-text.Milne and Witten?s work is most related towhat we propose here in that we also employfeatures similar to their relatedness and com-monness features.
However, we add to this amuch richer set of features which are extractedfrom Web-scale data sources beyond Wikipedia,and we develop a machine learning approach toautomatically blend our features using complete-ly automatically generated training data.3 WikipediaWikipedia has more than 200 language editions,and the English edition has more than 3 millionarticles as of March 2009.
Newsworthy eventsare often added to Wikipedia within days of oc-currence; Wikipedia has bi-weekly snapshotsavailable for download.Each article in Wikipedia is uniquely identi-fied by its title which is usually the most com-mon surface form of an entity or concept.
Eacharticle includes body text, outgoing links andcategories.
Here is a sample sentence in the ar-ticle titled ?Aristotle?
in wikitext format.
?To-gether with Plato and [[Socrates]] (Plato'steacher), Aristotle is one of the most importantfounding figures in [[Western philosophy]].
?Near the end of the article, there are categorylinks such as ?
[[Category:Ancient Greek mathe-maticians]]?.
The double brackets annotate out-going links to other Wikipedia articles with thespecified titles.
The category names are createdby authors.
Articles and category names havemany-to-many relationships.In addition to normal articles, Wikipedia alsohas special types of articles such as redirect ar-ticles and disambiguation articles.
A redirect ar-ticle?s title is an alternative surface form for aWikipedia entry.
A disambiguation article listslinks to similarly named articles, and usually itstitle is a commonly used surface form for mul-tiple entities and concepts.4 Method of LearningOur goal is to resolve surface forms to entities orconcepts described in Wikipedia.
To this end, wefirst need a recognizer to detect surface forms tobe resolved.
Then we need a resolver to map asurface form to the most probable entry in Wiki-pedia (or to out-of-wiki) based on the context.Recognizer: We first create a set of Wikipedia(article) entries E = {e1, e2, ?}
to which we wantto resolve surface forms.
Each entry?s surfaceforms are mined from multiple data sources.Then we use simple string match to recognizesurface forms from text documents.Among all Wikipedia entries, we excludethose with low importance.
In our experiments,we removed the entries that would not interestgeneral Web users, such as stop words and punc-tuations.
Second, we collect surface forms forentries in E using Wikipedia and Web searchquery click logs based on the following assump-tions:1336?
Each Wikipedia article title is a surface formfor the entry.
Redirect titles are taken as alter-native surface forms for the target entry.?
The anchor text of a link from one article toanother is taken as an alternative surface formfor the linked-to entry.?
Web search engine queries resulting in userclicks on a Wikipedia article are taken as alter-native surface forms for the entry.As a result, we get a number of surface formsfor each entry ei.
If we let sij denote the j-th sur-face form for entry i, then we can represent ourentry dictionary as EntSfDict = {<e1, (s11, s12,?
)>, <e2, (s21, s22, ?
)>, ?
}.Resolver: We first build a labeled training setautomatically, and then use supervised learningmethods to learn models to resolve among Wiki-pedia entries.
In the rest of this section we de-scribe the resolver in details.4.1 Automatically Labeled DataTo learn accurate models, supervised learningmethods require training data with both largequantity and high quality, which often takes lotsof human labeling effort.
However, in Wikipedia,links provide a supervised mapping from surfaceforms to article entries.
We use these links toautomatically generate training data.
If a link'sanchor text is a surface form in EntSfDict, weextract the anchor text as surface form s and thelink's destination article as Wikipedia entry e,then add the pair (s, e) with a positive judgmentto our labeled example set.
Continuing, we useEntSfDict to find other Wikipedia entries forwhich s is a surface form and create negativeexamples for these and add them to our labeledexample set.
If e does not exist in EntSfDict (forexample, if the link points to a Wikipedia articleabout a stop word), then a negative training ex-ample is created for every Wikipedia entry towhich s may resolve.
We use oow (out-of-wiki)to denote this case.Instead of article level coreference resolution,we only match partial names with full namesbased on the observation that surface forms fornamed entities are usually capitalized word se-quences in English language and a named entityis often mentioned by a long surface form fol-lowed by mentions of short forms in the samearticle.
For each pair (s, e) in the labeled exampleset, if s is a partial name of a full name s?
occur-ring earlier in the same document, we replace (s,e) with (s?, e) in the labeled example set.Using this methodology we created 2.4 millionlabeled examples from only 1% of English Wiki-pedia articles.
The abundance of data made itpossible for us to experiment on the impact oftraining set size on model accuracy.4.2 Learning AlgorithmsIn our experiments we explored both GradientBoosted Decision Trees (GBDT) and GradientBoosted Ranking (GBRank) to learn resolutionmodels.
They both can easily combine featuresof different scale and with missing values.
Othersupervised learning methods are to be exploredin the future.GBDT: We use the stochastic variant ofGBDTs (Friedman, 2001) to learn a binary logis-tic regression model with the judgments as thetarget.
GBDTs compute a function approxima-tion by performing a numerical optimization inthe function space.
It is done in multiple stages,with each stage modeling residuals from themodel of the last stage using a small decisiontree.
A brief summary is given in Algorithm 1.
Inthe stochastic version of GBDT, one sub-samplesthe training data instead of using the entire train-ing set to compute the loss function.Algorithm 1 GBDTsInput: training data Niii yx 1)},{( =  , loss functionL[y, f(x)] , the number of nodes for each tree J, the number of trees M .1: Initialize f(x)=f02: For m = 1 to M2.1:   For i = 1 to N, compute the negativegradient by taking the derivative of theloss with respect to f(x) and substitutewithiy and )(1 imi xf ?
.2.2:    Fit a J-node regression tree to thecomponents of the negative gradient.2.3:   Find the within-node updates mja  for j= 1 to J by performing J univariate op-timizations of the node contributions tothe estimated loss.2.4:   Do the update mjimiimi arxfxf ?+= ?
)()( 1 ,where j is the node that xi belongs to, ris learning rate.3: End for4: Return fM1337In our setting, the loss function is a negativebinomial log-likelihood, xi is the feature vectorfor a surface-form and Wikipedia-entry pair (si,ei), and yi is +1 for positive judgments and -1 isfor negative judgments.GBRank: From a given surface form?s judg-ments we can infer that the correct Wikipediaentry is preferred over other entries.
This allowsus to derive pair-wise preference judgments fromabsolute judgments and train a model to rank allthe Wikipedia candidate entries for each surfaceform.
Let },...,1),()(|),{( '' NixlxlxxS iiii =?=  be the set ofpreference judgments, where xi and xi' are thefeature vectors for two pairs of surface-forms andWikipedia-entry, l(xi) and l(xi') are their absolutejudgments respectively.
GBRank (Zheng et al,2007) tries to learn a function h such that)()( 'ii xhxh ?
for Sxx ii ?
),( ' .
A sketch of the algorithmis given in Algorithm 2.Algorithm 2 GBRank1: Initialize h=h02: For k=1 to K2.1:   Use hk-1 as an approximation of h andcompute})()(|),{( '11' ?+?
?= ?
?+ ikikii xhxhSxxS})()(|),{( '11' ?+<?= ???
ikikii xhxhSxxSwhere ))()(( 'ii xlxl ?=?
?2.2:   Fit a regression function gk usingGBDT and the incorrectly predictedexamples}),(|))(,(),)(,{( '1''1???
?
?+ Sxxxhxxhx iiikiiki ?
?2.3:   Do the update)1/())()(()( 1 ++= ?
kxgxkhxh kkk ?
, where ?
islearning rate.3: End for4: Return hKWe use a tuning set independent from thetraining set to select the optimal parameters forGBDT and GBRank.
This includes the numberof trees M, the number of nodes J, the learningrate r, and the sampling rate for GBDT; and forGBRank we select  K, ?
and ?.The feature importance measurement given byGBDT and GBRank is computed by keepingtrack of the reduction in the loss function at eachfeature variable split and then computing the to-tal reduction of loss along each explanatory fea-ture variable.
We use it to analyze feature effec-tiveness.4.3 PredictionAfter applying a resolution model on the giventest data, we obtain a score for each surface-formand Wikipedia-entry pair (s, e).
Among all thepairs containing s, we find the pair with the high-est score, denoted by (s, e~ ).It?s very common that a surface form refers toan entity or concept not defined in Wikipedia.
Soit?s important to correctly predict whether thegiven surface form cannot be mapped to any Wi-kipedia entry in EntSfDict.We apply a threshold to the scores from reso-lution models.
If the score for (s, e~ ) is lower thanthe threshold, then the prediction is oow (seeSection 4.1), otherwise e~  is predicted to be theentry referred by s. We select thresholds basedon F1 (see Section 6.2) on a tuning set that isindependent from our training set and test set.5 FeaturesFor each surface-form and Wikipedia-entry pair(s, e), we create a feature vector including fea-tures capturing the context surrounding s andfeatures independent of the context.
They arecontext-dependent and context-independent fea-tures respectively.
Various data sources aremined to extract these features, including Wiki-pedia articles, Web search query-click logs, andWeb-user browsing logs.
In addition, (s, e) iscompared to all pairs containing s based onabove features and the derived features are calleddifferentiation features.5.1 Context-dependent FeaturesThese features measure whether the given sur-face form s resolving to the given Wikipedia en-try e would make the given document more co-herent.
They are based on 1) the vector represen-tation of e, and 2) the vector representation of thecontext of s in a document d.Representation of e: By thoroughly miningWikipedia and other large data sources we ex-tract contextual clues for each Wikipedia entrye and formulate its representation in the follow-ing ways.1) Background representation.
The overallbackground description of e is given in the cor-responding Wikipedia article, denoted as Ae.
Na-turally, a bag of terms and surface forms in Aecan represent e. So we represent e by a back-1338ground word vector Ebw and a background sur-face form vector Ebs, in which each element isthe occurrence count of a word or a surface formin Ae?s first paragraph.2) Co-occurrence representation.
The termsand surface forms frequently co-occurring with ecapture its contextual characteristics.
We firstidentify all the Wikipedia articles linking to Ae.Then, for each link pointing to Ae we extract thesurrounding words and surface forms within awindow centered on the anchor text.
The windowsize is set to 10 words in our experiment.
Finally,we select the words and surface forms with thetop co-occurrence frequency, and represent e bya co-occurring word vector Ecw and a co-occurring surface form vector Ecs, in which eachelement is the co-occurrence frequency of a se-lected word or surface form.3) Relatedness representation.
We analyzedthe relatedness between Wikipedia entries fromdifferent data sources using various measure-ments, and we computed over 20 types of rela-tedness scores in our experiments.
In the follow-ing we discuss three types as examples.
The firsttype is computed based on the overlap betweentwo Wikipedia entries?
categories.
The secondtype is mined from Wikipedia inter-article links.
(In our experiments, two Wikipedia entries areconsidered to be related if the two articles aremutually linked to each other or co-cited bymany Wikipedia articles.)
The third type ismined from Web-user browsing data based onthe assumption that two Wikipedia articles co-occurring in the same browsing session are re-lated.
We used approximately one year of Yahoo!user data in our experiments.
A number of differ-ent metrics are used to measure the relatedness.For example, we apply the algorithm of Googledistance (Milne and Witten, 2008b) on Wikipe-dia links to calculate the Wikipedia link-basedrelatedness, and use mutual information for thebrowsing-session-based relatedness.
In summary,we represent e by a related entry vector Er foreach type of relatedness, in which each elementis the relatedness score between e and a relatedentry.Representation of s: We represent a surfaceform?s context as a vector, then calculate a con-text-dependent feature for a pair <s,e> by a simi-larity function Sim from two vectors.
Here areexamples of context representation.1) s is represented by a word vector Sw and asurface form vector Ss, in which each element isthe occurrence count of a word or a surface formsurrounding s. We calculate each vector?s simi-larity with the background and co-occurrencerepresentation of e, and it results in Sim(Sw, Ebw) ,Sim(Sw, Ecw) , Sim(Ss, Ebs) and Sim(Ss, Ecs) .2) s is represented by a Wikipedia entry vectorSe, in which each element is a Wikipedia entry towhich a surrounding surface form s could re-solve.
We calculate its similarity with the rela-tedness representation of e, and it results inSim(Se, Er).In the above description, similarity is calcu-lated by dot product or in a summation-of-maximum fashion.
In our experiments we ex-tracted surrounding words and surface forms fors from the whole document or from the text win-dow of 55 tokens centered on s, which resulted in2 sets of features.
We created around 50 context-dependent features in total.5.2 Context-independent FeaturesThese features are extracted from data beyondthe document containing s. Here are examples.?
During the process of building the dictionaryEntSfDict as described in Section 4, we counthow often s maps to e and estimate the proba-bility of s mapping to e for each data source.These are the commonness features.?
The number of Wikipedia entries that s couldmap to is a feature about the ambiguity of s.?
The string similarity between s and the title ofAe is used as a feature.
In our experimentsstring similarity was based on word overlap.5.3 Differentiation FeaturesAmong all surface-form and Wikipedia-entrypairs that contain s, at most one pair gets the pos-itive judgment.
Based on this observation wecreated differentiation features to represent how(s, e) is compared to other pairs for s.  They arederived from the context-dependent and context-independent features described above.
For exam-ple, we compute the difference between thestring similarity for (s, e) and the maximumstring similarity for all pairs containing s. Thederived feature value would be zero if (s, e) haslarger string similarity than other pairs contain-ing s.13396 Experimental ResultsIn our experiments we used the Wikipedia snap-shot for March 6th, 2009.
Our dictionaryEntSfDict contains 3.5 million Wikipedia entriesand 6.5 million surface forms.A training set was created from randomly se-lected Wikipedia articles using the process de-scribed in Section 4.1.
We varied the number ofWikipedia articles from 500 to 40,000, but theperformance did not increase much after 5000.The experimental results reported in this paperare based on the training set generated from 5000articles.
It contains around 1.4 million trainingexamples.
There are approximately 300,000 sur-face forms, out of which 28,000 are the oow case.Around 400 features were created in total, and200 of them were selected by GBDT andGBRank to be used in our resolution models.6.1 Evaluation DatasetsThree datasets from different data sources areused in evaluation.1) Wikipedia hold-out set.
Using the sameprocess for generating training data and exclud-ing the surface forms appearing in the trainingdata, we built the hold-out set from approximate-ly 15,000 Wikipedia articles, containing around600,000 labeled instances.
There are 400,000surface forms, out of which 46,000 do not re-solve to any Wikipedia entry.2) MSNBC News test set.
This entity disam-biguation data set was introduced by Cucerzan(2007).
It contains 200 news articles collectedfrom ten MSNBC news categories as of January2, 2007.
Surface forms were manually identifiedand mapped to Wikipedia entities.
The data setcontains 756 surface forms.
Only 589 of them arecontained in our dictionary EntSfDict, mainlybecause EntSfDict excludes surface forms of out-of-Wikipedia entities and concepts.
Since theevaluation task is focused on resolution perfor-mance rather than recognition, we exclude themissing surface forms from the labeled exampleset.
The final dataset contains 4,151 labeled in-stances.
There are 589 surface forms and 40 ofthem do not resolve to any Wikipedia entry.3) Yahoo!
News set.
One limitation of theMSNBC test set is the small size.
We built amuch larger data set by randomly samplingaround 1,000 news articles from Yahoo!
Newsover 2008 and had them manually annotated.
Theexperts first identified person, location and or-ganization names, then mapped each name to aWikipedia article if the article is about the entityreferred to by the name.
We didn?t include moregeneral concepts in this data set to make the ma-nual effort easier.
This data set contains around100,000 labeled instances.
The data set includes15,387 surface forms and 3,532 of them cannotbe resolved to any Wikipedia entity.
We random-ly split the data set to 2 parts of equal size.
Onepart is used to tune parameters of GBDT andGBRank and select thresholds based on F1 value.The evaluation results presented in this paper isbased on the remaining part of the Yahoo!
Newsset.6.2 MetricsThe possible outcomes from comparing a resolu-tion system?s prediction with ground truth can becategorized into the following types.?
True Positive (TP), the predicted e was correct-ly referred to by s.?
True Negative (TN), s was correctly predictedas resolving to oow.?
Mismatch (MM), the predicted e was not cor-rectly referred to by s and should have been e?from EntSfDict.?
False Positive (FP), the predicted e was notcorrectly referred to by s and should have beenoow.?
False Negative (FN), the predicted oow is notcorrect and should have been e?
fromEntSfDict.Similar to the widely used metrics for classifi-cation systems, we use following metrics to eva-luate disambiguation performance.MMFPTPTPprecision ++=MMFNTPTPrecall ++=recallprecisionrecallprecisionF +?
?= 21MMFNTNFPTPTNTPaccuracy +++++=In the Yahoo!
News test set, 23.5% of the sur-face forms do not resolve to any Wikipedia en-tries, and in the other two test sets the percentag-es of oow are between 10% and 20%.
This de-monstrates it is necessary in real-world applica-tions to explicitly measure oow prediction.
Wepropose following metrics.FNTNTNoowprecision +=_FPTNTNoowrecall +=_oowrecalloowprecisionoowrecalloowprecisionoowF____2_1 +?
?=13406.3 Evaluation ResultsWith our training set we trained one resolutionmodel using GBDT (named as WikiRes-c) andanother resolution model using GBRank (namedas WikiRes-r).
The models were evaluated alongwith the following systems.1) Baseline-r: each surface form s is randomlymapped to oow or a candidate entry for s inEntSfDict.2) Baseline-p: each surface form s is mappedto the candidate entry e for s with the highestcommonness score.
The commonness score islinear combination of the probability of s beingmapped to e estimated from different datasources.
The commonness score is among thefeatures used in WikiRes-c and WikiRes-r.3) Baseline-m: we implemented the approachbrought by Cucerzan (2007) based on our bestunderstanding.
Since we use a different versionof Wikipedia and a different entity recognitionapproach, the evaluation result differs from theresult presented in their paper.
But we believeour implementation follows the algorithm de-scribed in their paper.In Table 1 we present the performance foreach system on the Yahoo!
News test set and theMSNBC test set.
The performance of WikiRes-cand WikiRes-r are computed after we apply thethresholds selected on the tuning set described inSection 6.1.
In the upper half of Table 1, thethree baselines use the thresholds that lead to thebest F1 on the Yahoo!
News test set.
In the lowerhalf of Table 1, the three baselines use the thre-sholds that lead to the best F1 on the MSNBCtest set.Among the three baselines, Baseline-r has thelowest performance.
Baseline-m uses a few con-text-sensitive features and Baseline-p uses a con-text-independent feature.
These two types of fea-tures are both useful, but Baseline-p shows betterperformance, probably because the surface formsin our test sets are dominated by common senses.In our resolution models, these features are com-bined together with many other features calcu-lated from different large-scale data sources andon different granularity levels.
As shown in Ta-ble 1, both of our resolution solutions substan-tially outperform other systems.
Furthermore,WikiRes-c and WikiRes-r have similar perfor-mance.Precision Recall F1 Accuracy p-valueYahoo!
News Test SetBaseline-r 47.023 60.831 53.043 47.023 0Baseline-p 73.869 88.157 80.383 73.175 5.2e-78Baseline-m 62.240 80.517 70.208 62.240 1.3e-160WikiRes-r 83.406 88.858 86.046 80.717 0.012WikiRes-c 85.038 87.831 86.412 81.463 ---MSNBC Test SetBaseline-r 60.272 64.545 62.335 60.272 8.9e-19Baseline-p 82.292 86.182 84.192 82.003 0.306Baseline-m 78.947 84.545 81.651 78.947 0.05WikiRes-r 88.785 86.364 87.558 84.550 0.102WikiRes-c 88.658 85.273 86.932 83.192 ---Table 1.
Performance on the Yahoo!
News TestSet and the MSNBC Test setFigure 1.
Precision-recall on the Yahoo!
NewsTest Set and the MSNBC Test SetWe compared WikiRes-c with each competitorand from the statistical significance test results inthe last column of Table 1 we see that on the Ya-hoo!
News test set WikiRes-c significantly out-performs others.
The p-values for the MSNBCtest set are much higher than for the Yahoo!News test set because the MSNBC test set ismuch smaller.Attempting to address this point, we see thatthe F1 values of WikiRes on the MSNBC test setand on the Yahoo!
News test set only differs by acouple percentage points, although, these testsets were created independently.
This suggeststhe objectivity of our method for creating theYahoo!
News test set and provides a way tomeasure resolution model performance on what1341would occur in a general news corpus in a statis-tically significant manner.In Figure 1 we present the precision-recallcurves on the Yahoo!
News and the MSNBC testsets.
We see that our resolution models are sub-stantially better than the other two baselines atany particular precision or recall value on bothtest sets.
Baseline-r is not included in the com-parison since it does not have the tradeoff be-tween precision and recall.
We find the preci-sion-recall curve of WikiRes-r is very similar toWikiRes-c at the lower precision area, but its re-call is much lower than other systems after preci-sion reaches around 90%.
So, in Figure 1 thecurves of WikiRes-r are truncated at the high pre-cision area.In Table 2 we compare the performance ofout-of-Wikipedia prediction.
The comparison isdone on the Yahoo!
News test set only, sincethere are only 40 surface forms of oow case inthe MSNBC test set.
Each system?s threshold isthe same as that used for the upper half of Table1.
The results show our models have substantial-ly higher precision and recall than Baseline-p andBaseline-m. From the statistical significance testresults in the last column, we can see that Wi-kiRes-c significantly outperforms Baseline-p andBaseline-m. Also, our current approaches stillhave room to improve in the area of out-of-Wikipedia prediction.We also evaluated our models on a Wikipediahold-out set.
The model performance is greaterthan that obtained from the previous two test setsbecause the hold-out set is more similar to thetraining data source itself.
Again, our modelsperform better than others.From the feature importance lists of ourGBDT model and GBRank model, we find thatthe commonness features, the features based onWikipedia entries?
co-occurrence representationand the corresponding differentiation features arethe most important.Precision Recall F1 p-valueBaseline-p 64.907 22.152 33.03 1.6e-20Baseline-m 47.207 44.78 45.961 1.3e-34WikiRes-r 68.166 52.994 59.630 0.084WikiRes-c 67.303 59.777 63.317 ---Table 2.
Performance of Out-of-Wikipedia Pre-diction on the Yahoo!
News Test Set7 ConclusionsWe have described a method of learning to re-solve surface forms to Wikipedia entries.
Usingthis method we can enrich the unstructured doc-uments with structured knowledge from Wikipe-dia, the largest knowledge base in existence.
Theenrichment makes it possible to represent a doc-ument as a machine-readable network of sensesinstead of just a bag of words.
This can supplycritical semantic information useful for next-generation information retrieval systems and oth-er text mining applications.Our resolution models use an extensive set ofnovel features and are leveraged by a machinelearned approach that depends only on a purelyautomated training data generation facility.
Ourmethodology can be applied to any other lan-guage that has Wikipedia and Web data available(after modifying the simple capitalization rules inSection 4.1).
Our resolution models can be easilyand quickly retrained with updated data whenWikipedia and the relevant Web data arechanged.For future work, it will be important to inves-tigate other approaches to better predict oow.Adding global constraints on resolutions of thesame term at multiple locations in the same doc-ument may also be important.
Of course, devel-oping new features (such as part-of-speech,named entity type, etc) and improving trainingdata quality is always critical, especially for so-cial content sources such as those from Twitter.Finally, directly demonstrating the degree of ap-plicability to other languages is interesting whenaccounting for the fact that the quality of Wiki-pedia is variable across languages.ReferencesBagga, Amit and Breck Baldwin.
1998.
Entity-basedcross-document coreferencing using the VectorSpace Model.
Proceedings of the 17th interna-tional conference on Computational linguis-tics.Bunescu, Razvan and Marius Pa?ca.
2006.
Using En-cyclopedic Knowledge for Named Entity Disam-biguation.
Proceedings of the 11th Conferenceof the European Chapter of the Association ofComputational Linguistics (EACL-2006).Cucerzan, Silviu.
2007.
Large-Scale Named EntityDisambiguation Based on Wikipedia Data.
Pro-1342ceedings of the 2007 Joint Conference on Em-pirical Methods in Natural LanguageProcessing and Computational Natural Lan-guage Learning.Fleischman, Ben Michael and Eduard Hovy.
2004.Multi-Document Person Name Resolution.
Pro-ceesing of the Association for ComputationalLinguistics.Friedman, J. H. 2001.
Stochastic gradient boosting.Computational Statistics and Data Analysis,38:367?378.Han, Xianpei and Jun Zhao 2009.
Named Entity Dis-ambiguation by Leveraging Wikipedia SemanticKnowledge.
Proceedings of the 31st annual in-ternational ACM SIGIR conference on Re-search and development in information re-trieval.Mann, S. Gidon and David Yarowsky.
2003.
Unsu-pervised Personal Name Disambiguation.
Pro-ceedings of the seventh conference on Naturallanguage learning at HLT-NAACL 2003.Milne, David and Ian H. Witten.
2008a.
Learning toLink with Wikipedia.
In Proceedings of theACM Conference on Information and Know-ledge Management (CIKM'2008).Milne, David and Ian H. Witten.
2008b.
An effective,low-cost measure of semantic relatedness obtainedfrom Wikipedia links.
Proceedings of the firstAAAI Workshop on Wikipedia and ArtificialIntelligence.Pedersen, Ted, Amruta Purandare and Anagha Kul-karni.
2005.
Name Discrimination by ClusteringSimilar Contexts.
Proceedings of the Sixth In-ternational Conference on Intelligent TextProcessing and Computational Linguistics(2005).Ravin, Y. and Z. Kazi.
1999.
Is Hillary Rodham Clin-ton the President?
In Association for Computa-tional Linguistics Workshop on Coreferenceand its Applications.Yarowsky, David.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.
Pro-ceedings of the 33rd Annual Meeting of theAssociation for Computational Linguistics,pages 189-196.Zheng, Zhaohui, K. Chen, G. Sun, and H. Zha.
2007.A regression framework for learning ranking func-tions using relative relevance judgments.
Proceed-ings of the 30th annual international ACMSIGIR conference on Research and develop-ment in information retrieval, pages 287-294.1343
