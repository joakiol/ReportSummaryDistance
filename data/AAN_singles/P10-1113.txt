Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1108?1117,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsJoint Syntactic and Semantic Parsing of ChineseJunhui Li  and  Guodong ZhouSchool of Computer Science & TechnologySoochow UniversitySuzhou, China 215006{lijunhui, gdzhou}@suda.edu.cnHwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing Drive, Singapore 117417nght@comp.nus.edu.sgAbstractThis paper explores joint syntactic and seman-tic parsing of Chinese to further improve theperformance of both syntactic and semanticparsing, in particular the performance of se-mantic parsing (in this paper, semantic rolelabeling).
This is done from two levels.
Firstly,an integrated parsing approach is proposed tointegrate semantic parsing into the syntacticparsing process.
Secondly, semantic informa-tion generated by semantic parsing is incorpo-rated into the syntactic parsing model to bettercapture semantic information in syntacticparsing.
Evaluation on Chinese TreeBank,Chinese PropBank, and Chinese NomBankshows that our integrated parsing approachoutperforms the pipeline parsing approach onn-best parse trees, a natural extension of thewidely used pipeline parsing approach on thetop-best parse tree.
Moreover, it shows thatincorporating semantic role-related informa-tion into the syntactic parsing model signifi-cantly improves the performance of both syn-tactic parsing and semantic parsing.
To ourbest knowledge, this is the first research onexploring syntactic parsing and semantic rolelabeling for both verbal and nominal predi-cates in an integrated way.1 IntroductionSemantic parsing maps a natural language sen-tence into a formal representation of its meaning.Due to the difficulty in deep semantic parsing,most previous work focuses on shallow semanticparsing, which assigns a simple structure (suchas WHO did WHAT to WHOM, WHEN,WHERE, WHY, HOW) to each predicate in asentence.
In particular, the well-defined semanticrole labeling (SRL) task has been drawing in-creasing attention in recent years due to its im-portance in natural language processing (NLP)applications, such as question answering (Nara-yanan and Harabagiu, 2004), information extrac-tion (Surdeanu et al, 2003), and co-referenceresolution (Kong et al, 2009).
Given a sentenceand a predicate (either a verb or a noun) in thesentence, SRL recognizes and maps all the con-stituents in the sentence into their correspondingsemantic arguments (roles) of the predicate.
Inboth English and Chinese PropBank (Palmer etal., 2005; Xue and Palmer, 2003), and Englishand Chinese NomBank (Meyers et al, 2004; Xue,2006), these semantic arguments include corearguments (e.g., Arg0 for agent and Arg1 forrecipient) and adjunct arguments (e.g.,ArgM-LOC for locative argument andArgM-TMP for temporal argument).
Accordingto predicate type, SRL can be divided into SRLfor verbal predicates (verbal SRL, in short) andSRL for nominal predicates (nominal SRL, inshort).With the availability of large annotated cor-pora such as FrameNet (Baker et al, 1998),PropBank, and NomBank in English, data-driventechniques, including both feature-based andkernel-based methods, have been extensivelystudied for SRL (Carreras and M?rquez, 2004;Carreras and M?rquez, 2005; Pradhan et al,2005; Liu and Ng, 2007).
Nevertheless, for bothverbal and nominal SRL, state-of-the-art systemsdepend heavily on the top-best parse tree andthere exists a large performance gap betweenSRL based on the gold parse tree and thetop-best parse tree.
For example, Pradhan et al(2005) suffered a performance drop of 7.3 inF1-measure on English PropBank when using thetop-best parse tree returned from Charniak?sparser (Charniak, 2001).
Liu and Ng (2007) re-ported a performance drop of 4.21 in F1-measureon English NomBank.Compared with English SRL, Chinese SRLsuffers more seriously from syntactic parsing.Xue (2008) evaluated on Chinese PropBank andshowed that the performance of Chinese verbalSRL drops by about 25 in F1-measure when re-placing gold parse trees with automatic ones.Likewise, Xue (2008) and Li et al (2009) re-ported a performance drop of about 12 inF1-measure in Chinese NomBank SRL.1108While it may be difficult to further improvesyntactic parsing, a promising alternative is toperform both syntactic and semantic parsing inan integrated way.
Given the close interactionbetween the two tasks, joint learning not onlyallows uncertainty about syntactic parsing to becarried forward to semantic parsing but also al-lows useful information from semantic parsing tobe carried backward to syntactic parsing.This paper explores joint learning of syntacticand semantic parsing for Chinese texts from twolevels.
Firstly, an integrated parsing approach isproposed to benefit from the close interactionbetween syntactic and semantic parsing.
This isdone by integrating semantic parsing into thesyntactic parsing process.
Secondly, various se-mantic role-related features are directly incorpo-rated into the syntactic parsing model to bettercapture semantic role-related information in syn-tactic parsing.
Evaluation on Chinese TreeBank,Chinese PropBank, and Chinese NomBankshows that our method significantly improves theperformance of both syntactic and semanticparsing.
This is promising and encouraging.
Toour best knowledge, this is the first research onexploring syntactic parsing and SRL for verbaland nominal predicates in an integrated way.The rest of this paper is organized as follows.Section 2 reviews related work.
Section 3 pre-sents our baseline systems for syntactic and se-mantic parsing.
Section 4 presents our proposedmethod of joint syntactic and semantic parsingfor Chinese texts.
Section 5 presents the experi-mental results.
Finally, Section 6 concludes thepaper.2 Related WorkCompared to the large body of work on eithersyntactic parsing (Ratnaparkhi, 1999; Collins,1999; Charniak, 2001; Petrov and Klein, 2007),or SRL (Carreras and M?rquez, 2004; Carrerasand M?rquez, 2005; Jiang and Ng, 2006), there isrelatively less work on their joint learning.Koomen et al (2005) adopted the outputs ofmultiple SRL systems (each on a single parsetree) and combined them into a coherent predi-cate argument output by solving an optimizationproblem.
Sutton and McCallum (2005) adopted aprobabilistic SRL system to re-rank the N-bestresults of a probabilistic syntactic parser.
How-ever, they reported negative results, which theyblamed on the inaccurate probability estimatesfrom their locally trained SRL model.As an alternative to the above pseudo-jointlearning methods (strictly speaking, they are stillpipeline methods), one can augment the syntacticlabel of a constituent with semantic information,like what function parsing does (Merlo and Mu-sillo, 2005).
Yi and Palmer (2005) observed thatthe distributions of semantic labels could poten-tially interact with the distributions of syntacticlabels and redefined the boundaries of constitu-ents.
Based on this observation, they incorpo-rated semantic role information into syntacticparse trees by extending syntactic constituentlabels with their coarse-grained semantic roles(core argument or adjunct argument) in the sen-tence, and thus unified semantic parsing andsyntactic parsing.
The actual fine-grained seman-tic roles are assigned, as in other methods, by anensemble classifier.
However, the results ob-tained with this method were negative, and theyconcluded that semantic parsing on PropBankwas too difficult due to the differences betweenchunk annotation and tree structure.
Motivatedby Yi and Palmer (2005), Merlo and Musillo(2008) first extended a statistical parser to pro-duce a richly annotated tree that identifies andlabels nodes with semantic role labels as well assyntactic labels.
Then, they explored bothrule-based and machine learning techniques toextract predicate-argument structures from thisenriched output.
Their experiments showed thattheir method was biased against these roles ingeneral, thus lowering recall for them (e.g., pre-cision of 87.6 and recall of 65.8).There have been other efforts in NLP on jointlearning with various degrees of success.
In par-ticular, the recent shared tasks of CoNLL 2008and 2009 (Surdeanu et al, 2008; Hajic et al,2009) tackled joint parsing of syntactic and se-mantic dependencies.
However, all the top 5 re-ported systems decoupled the tasks, rather thanbuilding joint models.
Compared with the disap-pointing results of joint learning on syntactic andsemantic parsing, Miller et al (2000) and Finkeland Manning (2009) showed the effectiveness ofjoint learning on syntactic parsing and somesimple NLP tasks, such as information extractionand name entity recognition.
In addition, at-tempts on joint Chinese word segmentation andpart-of-speech (POS) tagging (Ng and Low,2004; Zhang and Clark, 2008) also illustrate thebenefits of joint learning.11093 Baseline: Pipeline Parsing onTop-Best Parse TreeIn this section, we briefly describe our approachto syntactic parsing and semantic role labeling,as well as the baseline system with pipelineparsing on the top-best parse tree.3.1 Syntactic ParsingOur syntactic parser re-implements Ratnaparkhi(1999), which adopts the maximum entropyprinciple.
The parser recasts a syntactic parsetree as a sequence of decisions similar to thoseof a standard shift-reduce parser and the parsingprocess is organized into three left-to-rightpasses via four procedures, called TAG,CHUNK, BUILD, and CHECK.First pass.
The first pass takes a tokenized sen-tence as input, and uses TAG to assign eachword a part-of-speech.Second pass.
The second pass takes the outputof the first pass as input, and uses CHUNK torecognize basic chunks in the sentence.Third pass.
The third pass takes the output ofthe second pass as input, and always alternatesbetween BUILD and CHECK in structural pars-ing in a recursive manner.
Here, BUILD decideswhether a subtree will start a new constituent orjoin the incomplete constituent immediately toits left.
CHECK finds the most recently pro-posed constituent, and decides if it is complete.3.2 Semantic Role LabelingFigure 1 demonstrates an annotation example ofChinese PropBank and NomBank.
In the figure,the verbal predicate ???/provide?
is annotatedwith three core arguments (i.e., ?NP (?
?/Chinese ??/govt.)?
as Arg0, ?PP (?/to ??/N.
Korean ??/govt.)?
as Arg2, and ?NP(??
?/RMB ??/loan)?
as Arg1), while thenominal predicate ???/loan?
is annotated withtwo core arguments (i.e., ?NP (?
?/Chinese ??/govt.)?
as Arg1 and ?PP (?/to ??/N.
Ko-rean ??/govt.)?
as Arg0), and an adjunct ar-gument (i.e., ?NN ( ?
?
?
/RMB)?
asArgM-MNR, denoting the manner of loan).
It isworth pointing out that there is a (Chinese)NomBank-specific label in Figure 1, Sup (sup-port verb) (Xue, 2006), to help introduce thearguments which occur outside the nominal pre-dicate-headed noun phrase.
In (Chinese) Nom-Bank, a verb is considered to be a support verbonly if it shares at least an argument with thenominal predicate.3.2.1 Automatic Predicate RecognitionAutomatic predicate recognition is a prerequisitefor the application of SRL systems.
For verbalpredicates, it is very easy.
For example, 99% ofverbs are annotated as predicates in ChinesePropBank.
Therefore, we can simply select anyword with a part-of-speech (POS) tag of VV,VA, VC, or VE as verbal predicate.Unlike verbal predicate recognition, nominalpredicate recognition is quite complicated.
ForFigure 1: Two predicates (Rel1 and Rel2) and their arguments in the style of Chinese PropBank and NomBank.
?to ??N.
Korean??govt.?
?providePNR NNVVNN NNNPPPArg0/Rel2Arg2/Rel1ArgM-MNR/Rel2 Rel2NPVPVP???RMB?
?loan?.NR NNPUNPArg1/Rel2Arg0/Rel1IP??Chinese?
?govt.Sup/Rel2Rel1Chinese government provides RMB loan to North Korean government.Arg1/Rel1TOP1110example, only 17.5% of nouns are annotated aspredicates in Chinese NomBank.
It is quitecommon that a noun is annotated as a predicatein some cases but not in others.
Therefore, au-tomatic predicate recognition is vital to nominalSRL.
In principle, automatic predicate recogni-tion can be cast as a binary classification (e.g.,Predicate vs. Non-Predicate) problem.
For no-minal predicates, a binary classifier is trained topredict whether a noun is a nominal predicate ornot.
In particular, any word POS-tagged as NNis considered as a predicate candidate in bothtraining and testing processes.
Let the nominalpredicate candidate be w0, and its left and rightneighboring words/POSs be w-1/p-1and w1/p1,respectively.
Table 1 lists the feature set used inour model.
In Table 1, local features present thecandidate?s contextual information while globalfeatures show its statistical information in thewhole training set.Type Descriptionw0, w-1, w1, p-1, p1 localfeatures The first and last characters of the candidateWhether w0 is ever tagged as a verb in thetraining data?
Yes/NoWhether w0 is ever annotated as a nominalpredicate in the training data?
Yes/NoThe most likely label for w0 when it occurstogether with w-1 and w1.The most likely label for w0 when it occurstogether with w-1.globalfeaturesThe most likely label for w0 when it occurstogether with w1.Table 1: Feature set for nominal predicate recognition3.2.2 SRL for Chinese PredicatesOur Chinese SRL models for both verbal andnominal predicates adopt the widely-used SRLframework, which divides the task into threesequential sub-tasks: argument pruning, argu-ment identification, and argument classification.In particular, we follow Xue (2008) and Li et al(2009) to develop verbal and nominal SRLmodels, respectively.
Moreover, we have furtherimproved the performance of Chinese verbalSRL by exploring additional features, e.g., voiceposition that indicates the voice maker (BA, BEI)is before or after the constituent in focus, therule that expands the parent of the constituent infocus, and the core arguments defined in thepredicate?s frame file.
For nominal SRL, wesimply use the final feature set of Li et al (2009).As a result, our Chinese verbal and nominal SRLsystems achieve performance of 92.38 and 72.67in F1-measure respectively (on golden parsetrees and golden predicates), which are compa-rable to Xue (2008) and Li et al (2009).
Formore details, please refer to Xue (2008) and Liet al (2009).3.3 Pipeline Parsing on Top-best ParseTreeSimilar to most of the state-of-the-art systems(Pradhan et al, 2005; Xue, 2008; Li et al, 2009),the top-best parse tree is first returned from oursyntactic parser and then fed into the SRL sys-tem.
Specifically, the verbal (nominal) SRL la-beler is in charge of verbal (nominal) predicates,respectively.
For each sentence, since SRL isonly performed on one parse tree, only con-stituents in it are candidates for semantic argu-ments.
Therefore, if no constituent in the parsetree can map the same text span to an argumentin the manual annotation, the system will not geta correct annotation.4 Joint Syntactic and Semantic ParsingIn this section, we first explore pipeline parsingon N-best parse trees, as a natural extension ofpipeline parsing on the top-best parse tree.
Then,joint syntactic and semantic parsing is exploredfor Chinese texts from two levels.
Firstly, anintegrated parsing approach to joint syntacticand semantic parsing is proposed.
Secondly,various semantic role-related features are di-rectly incorporated into the syntactic parsingmodel for better interaction between the twotasks.4.1 Pipeline Parsing on N-best Parse TreesThe pipeline parsing approach employed in thispaper is largely motivated by the generalframework of re-ranking, as proposed in Suttonand McCallum (2005).
The idea behind this ap-proach is that it allows uncertainty about syntac-tic parsing to be carried forward through anN-best list, and that a reliable SRL system, to acertain extent, can reflect qualities of syntacticparse trees.
Given a sentence x, a joint parsingmodel is defined over a semantic frame F and aparse tree t in a log-linear way:( )( ) ( ) ( ), |1 log | , log |Score F t xP F t x P t x?
?= ?
+    (1)where P(t|x) is returned by a probabilistic syn-tactic parsing model, e.g., our syntactic parser,and P(F|t, x) is returned by a probabilistic se-mantic parsing model, e.g.
our verbal & nominal1111SRL systems.
In our pipeline parsing approach,P(t|x) is calculated as the product of all involveddecisions?
probabilities in the syntactic parsingmodel, and P(F|t, x) is calculated as the productof all the semantic role labels?
probabilities in asentence (including both verbal and nominalSRL).
That is to say, we only consider thoseconstituents that are supposed to be arguments.Here, the parameter ?
is a balance factor in-dicating the importance of the semantic parsingmodel.In particular, (F*, t*) with maximal Score(F,t|x) is selected as the final syntactic and seman-tic parsing results.
Given a sentence, N-bestparse trees are generated first using the syntacticparser, and then for each parse tree, we predictthe best SRL frame using our verbal and nomi-nal SRL systems.4.2 Integrated ParsingAlthough pipeline parsing on N-best parse treescould relieve severe dependence on the qualityof the top-best parse tree, there is still a potentialdrawback: this method suffers from the limitedscope covered by the N-best parse trees since theitems in the parse tree list may be too similar,especially for long sentences.
For example,50-best parse trees can only represent a combi-nation of 5 to 6 binary ambiguities since 2^5 <50 < 2^6.Ideally, we should perform SRL on as manyparse trees as possible, so as to enlarge thesearch scope.
However, pipeline parsing on allpossible parse trees is time-consuming and thusunrealistic.
As an alternative, we turn to inte-grated parsing, which aims to perform syntacticand semantic parsing synchronously.
The keyidea is to construct a parse tree in a bottom-upway so that it is feasible to perform SRL at suit-able moments, instead of only when the wholeparse tree is built.
Integrated parsing is practica-ble, mostly due to the following two observa-tions: (1) Given a predicate in a parse tree, itssemantic arguments are usually siblings of thepredicate, or siblings of its ancestor.
Actually,this special observation has been widely em-ployed in SRL to prune non-arguments for averbal or nominal predicate (Xue, 2008; Li et al,2009).
(2) SRL feature spaces (both in fea-ture-based method and kernel-based method)mostly focus on the predicate-argument structureof a given (predicate, argument) pair.
That is tosay, once a predicate-argument structure isformed (i.e., an argument candidate is connectedwith the given predicate), there is enough con-textual information to predict their SRL relation.As far as our syntactic parser is concerned, weinvoke the SRL systems once a new constituentcovering a predicate is complete with a ?YES?decision from the CHECK procedure.
AlgorithmAlgorithm 1.
The algorithm integrating syntactic parsing and SRL.Assume:t: constituent which is complete with ?YES?
decision of CHECK procedureP: number of predicatesPi: ith predicateS: SRL result, set of predicates and its argumentsBEGINsrl_prob = 0.0;FOR i=1 to P DOIF t covers Pi THENT = number of children of t;FOR j=1 to T DOIF t?s jth child Chj does not cover Pi THENRun SRL given predicate Pi and constituent Chj to get their semantic rolelbl and its probability prob;IF lbl does not indicate non-argument THENsrl_prob += log( prob );S = S ?
{(Pi, Chj, lbl)};END IFEND IFEND FOREND IFEND FORreturn srl_prob;END11121 illustrates the integration of syntactic and se-mantic parsing.
For the example shown in Fig-ure 2, the CHECK procedure predicts a ?YES?decision, indicating the immediately proposedconstituent ?VP (??
/provide ???
/RMB??/loan)?
is complete.
So, at this moment, theverbal SRL system is invoked to predict the se-mantic label of the constituent ?NP (??
?/RMB ?
?/loan)?, given the verbal predicate?VV (??/provide)?.
Similarly, ?PP (?/to ??/N.
Korean ??/govt.)?
would also be se-mantically labeled as soon as ?PP (?/to ?
?/N.Korean ??/govt.)?
and ?VP (?
?/provide ??
?/RMB ??/loan)?
are merged into a big-ger VP.
In this way, both syntactic and semanticparsing are accomplished when the root nodeTOP is formed.
It is worth pointing out that allfeatures (Xue, 2008; Li et al, 2009) used in ourSRL model can be instantiated and their valuesare same as the ones when the whole tree isavailable.
In particular, the probability computedfrom the SRL model is interpolated with that ofthe syntactic parsing model in a log-linear way(with equal weights in our experiments).
This isdue to our hypothesis that the probability re-turned from SRL model is helpful to joint syn-tactic and semantic parsing, considering theclose interaction between the two tasks.4.3 Integrating Semantic Role-relatedFeatures into Syntactic Parsing ModelThe integrated parsing approach as shown inSection 4.2 performs syntactic and semanticparsing synchronously.
In contrast to traditionalsyntactic parsers where no semantic role-relatedinformation is used, it may be interesting to in-vestigate the contribution of such information inthe syntactic parsing model, due to the availabil-ity of such information in the syntactic parsingprocess.
In addition, it is found that 11% of pre-dicates in a sentence are speculatively attachedwith two or more core arguments with the samelabel due to semantic parsing errors (partlycaused by syntactic parsing errors in automaticparse trees).
This is abnormal since a predicatenormally only allows at most one argument ofeach core argument role (i.e., Arg0-Arg4).Therefore, such syntactic errors should beavoidable by considering those arguments al-ready obtained in the bottom-up parsing process.On the other hand, taking those expected seman-tic roles into account would help the syntacticparser.
In terms of our syntactic parsing model,this is done by directly incorporating varioussemantic role-related features into the syntacticparsing model (i.e., the BUILD procedure) whenthe newly-formed constituent covers one ormore predicates.For the example shown in Figure 2, once theconstituent ?VP (??
/provide ???
/RMB?
?/loan)?, which covers a verbal predicate?VV (?
?/provide)?, is complete, the verbalSRL model would be triggered first to markconstituent ?NP (??
?/RMB ??/loan)?
asARG1, given predicate ?VV (?
?/provide)?.Then, the BUILD procedure is called to makethe BUILD decision for the newly-formed con-stituent ?VP (?
?/provide ??
?/RMB ??/loan)?.
Table 2 lists various semanticrole-related features explored in our syntacticparsing model and their instantiations with re-gard to the example shown in Figure 2.
In Table2, feature sf4 gives the possible core semanticroles that the focus predicate may take, accord-ing to its frame file; feature sf5 presents the se-mantic roles that the focus predicate has alreadyoccupied; feature sf6 indicates the semanticroles that the focus predicate is expecting; andSF1-SF8 are combined features.
Specifically, ifthe current constituent covers n predicates, then14 * n features would be instantiated.
Moreover,we differentiate whether the focus predicate isverbal or nominal, and whether it is the headword of the current constituent.Feature Selection.
Some features proposedabove may not be effective in syntactic parsing.Here we adopt the greedy feature selection algo-rithm as described in Jiang and Ng (2006) toselect useful features empirically and incremen-tally according to their contributions on the de-velopment data.
The algorithm repeatedly se-lects one feature each time which contributes themost, and stops when adding any of the remain-Figure 2: An application of CHECK with YES as thedecision.
Thus, VV (?
?/provide) and NP (??
?/RMB ?
?/loan) reduce to a big VP.P NPPPStart_VP / NOVV NP???RMB?
?loanNN NN?
?provide?toNR NN??N.
Korean??govt.?
?VP YES?1113ing features fails to improve the syntactic pars-ing performance.Feat.
Descriptionsf1 Path: the syntactic path from C to P. (VP>VV)sf2 Predicate: the predicate itself.
(?
?/provide)sf3 Predicate class (Xue, 2008): the class that Pbelongs to.
(C3b)sf4 Possible roles: the core semantic roles P maytake.
(Arg0, Arg1, Arg2)sf5 Detected roles: the core semantic roles alreadyassigned to P. (Arg1)sf6 Expected roles:  possible semantic roles P isstill expecting.
(Arg0, Arg2)SF1 For each already detected argument, its rolelabel + its path from P. (Arg1+VV<VP>NP)SF2 sf1 + sf2.
(VP>VV+?
?/provide)SF3 sf1 + sf3.
(VP>VV+C3b)SF4 Combined possible argument roles.
(Arg0+Arg1+Arg2)SF5 Combined detected argument roles.
(Arg1)SF6 Combined expected argument roles.
(Arg0+Arg2)SF7 For each expected semantic role, sf1 + its rolelabel.
(VP>VV+Arg0, VP>VV+Arg2)SF8 For each expected semantic role, sf2 + its rolelabel.(?
?/provide+Arg0, ?
?/provide+Arg2)Table 2: SRL-related features and their instantiationsfor syntactic parsing, with ?VP (?
?/provide ??
?/RMB ??/loan)?
as the current constituent Cand ???/provide?
as the focus predicate P, basedon Figure 2.5 Experiments and ResultsWe have evaluated our integrated parsing ap-proach on Chinese TreeBank 5.1 and corre-sponding Chinese PropBank and NomBank.5.1 Experimental SettingsThis version of Chinese PropBank and ChineseNomBank consists of standoff annotations onthe file (chtb 001 to 1151.fid) of Chinese PennTreeBank 5.1.
Following the experimental set-tings in Xue (2008) and Li et al (2009), 648files (chtb 081 to 899.fid) are selected as thetraining data, 72 files (chtb 001 to 040.fid andchtb 900 to 931.fid) are held out as the test data,and 40 files (chtb 041 to 080.fid) are selected asthe development data.
In particular, the training,test, and development data contain 31,361(8,642), 3,599 (1,124), and 2,060 (731) verbal(nominal) propositions, respectively.For the evaluation measurement on syntacticparsing, we report labeled recall, labeled preci-sion, and their F1-measure.
Also, we report re-call, precision, and their F1-measure for evalua-tion of SRL on automatic predicates, combiningverbal SRL and nominal SRL.
An argument iscorrectly labeled if there is an argument in man-ual annotation with the same semantic label thatspans the same words.
Moreover, we also reportthe performance of predicate recognition.
To seewhether an improvement in F1-measure is statis-tically significant, we also conduct significancetests using a type of stratified shuffling which inturn is a type of compute-intensive randomizedtests.
In this paper, ?>>>?, ?>>?, and ?>?
denotep-values less than or equal to 0.01, in-between(0.01, 0.05], and bigger than 0.05, respectively.We are not aware of any SRL system comb-ing automatic predicate recognition, verbal SRLand nominal SRL on Chinese PropBank andNomBank.
Xue (2008) experimented independ-ently with verbal and nominal SRL and assumedcorrect predicates.
Li et al (2009) combinednominal predicate recognition and nominal SRLon Chinese NomBank.
The CoNLL-2009 sharedtask (Hajic et al, 2009) included both verbal andnominal SRL on dependency parsing, instead ofconstituent-based syntactic parsing.
Thus theSRL performances of their systems are not di-rectly comparable to ours.5.2 Results and DiscussionsResults of pipeline parsing on N-best parsetrees.
While performing pipeline parsing onN-best parse trees, 20-best (the same as the heapsize in our syntactic parsing) parse trees are ob-tained for each sentence using our syntacticparser as described in Section 3.1.
The balancefactor ?
is set to 0.5 indicating that the twocomponents in formula (1) are equally important.Table 3 compares the two pipeline parsing ap-proaches on the top-best parse tree and theN-best parse trees.
It shows that the approach onN-best parse trees outperforms the one on thetop-best parse tree by 0.42 (>>>) in F1-measureon SRL.
In addition, syntactic parsing also bene-fits from the N-best parse trees approach withimprovement of 0.17 (>>>) in F1-measure.
Thissuggests that pipeline parsing on N-best parsetrees can improve both syntactic and semanticparsing.It is worth noting that our experimental resultsin applying the re-ranking framework in Chinesepipeline parsing on N-best parse trees are veryencouraging, considering the pessimistic resultsof Sutton and McCallum (2005), in which there-ranking framework failed to improve the per-formance on English SRL.
It may be because,1114unlike Sutton and McCallum (2005), P(F, t|x)defined in this paper only considers those con-stituents which are identified as arguments.
Thiscan effectively avoid the noises caused by thepredominant non-argument constituents.
More-over, the huge performance gap between Chi-nese semantic parsing on the gold parse tree andthat on the top-best parse tree leaves much roomfor performance improvement.Method Task R (%) P (%) F1Syntactic 76.68 79.12 77.88SRL 62.96 65.04 63.98Predicate 94.18 92.28 93.22V-SRL 65.33 68.52 66.88V-Predicate 89.52 93.12 91.29N-SRL 49.58 48.19 48.88Pipeline on top-best parse treeN-Predicate 86.83 71.76 78.58Syntactic 76.89 79.25 78.05SRL 62.99 65.88 64.40Predicate 94.07 92.22 93.13V-SRL 65.41 69.09 67.20V-Predicate 89.66 93.02 91.31N-SRL 49.24 49.46 49.35Pipeline on 20-best parse treesN-Predicate 86.65 72.15 78.74Syntactic 77.14 79.01 78.07SRL 62.67 67.67 65.07Predicate 93.97 92.42 93.19V-SRL 65.37 70.27 67.74V-Predicate 90.08 92.87 91.45N-SRL 48.02 52.83 50.31IntegratedparsingN-Predicate 85.41 73.23 78.85Syntactic 77.47 79.58 78.51SRL 63.14 68.17 65.56Predicate 93.97 92.52 93.24V-SRL 65.74 70.98 68.26V-Predicate 89.86 93.17 91.49N-SRL 48.80 52.67 50.66Integratedparsing withsemanticrole-relatedfeaturesN-Predicate 85.85 72.78 78.78Table 3: Syntactic and semantic parsing performanceon test data (using gold standard word boundaries).?V-?
denotes ?verbal?
while ?N-?denotes ?nominal?.Results of integrated parsing.
Table 3 alsocompares the integrated parsing approach withthe two pipeline parsing approaches.
It showsthat the integrated parsing approach improvesthe performance of both syntactic and semanticparsing by 0.19 (>) and 1.09 (>>>) respectivelyin F1-measure over the pipeline parsing ap-proach on the top-best parse tree.
It is also notsurprising to find out that the integrated parsingapproach outperforms the pipeline parsing ap-proach on 20-best parse trees by 0.67 (>>>) inF1-measure on SRL, due to its exploring a largersearch space, although the integrated parsingapproach integrates the SRL probability and thesyntactic parsing probability in the same manneras the pipeline parsing approach on 20-bestparse trees.
However, the syntactic parsing per-formance gap between the integrated parsingapproach and the pipeline parsing approach on20-best parse trees is negligible.Results of integrated parsing with semanticrole-related features.
After performing thegreedy feature selection algorithm on the devel-opment data, features {SF3, SF2, sf5, sf6, SF4}as proposed in Section 4.3 are sequentially se-lected for syntactic parsing.
As what we haveassumed, knowledge about the detected seman-tic roles and expected semantic roles is helpfulfor syntactic parsing.
Table 3 also lists the per-formance achieved with those selected features.It shows that the integration of semanticrole-related features in integrated parsing sig-nificantly enhances both the performance of syn-tactic and semantic parsing by 0.44 (>>>) and0.49 (>>) respectively in F1-measure.
In addi-tion, it shows that it outperforms the wide-ly-used pipeline parsing approach on top-bestparse tree by 0.63 (>>>) and 1.58 (>>>) inF1-measure on syntactic and semantic parsing,respectively.
Finally, it shows that it outper-forms the widely-used pipeline parsing approachon 20-best parse trees by 0.46 (>>>) and 1.16(>>>) in F1-measure on syntactic and semanticparsing, respectively.
This is very encouraging,considering the notorious difficulty andcomplexity of both the syntactic and semanticparsing tasks.Table 3 also shows that our proposed methodworks well for both verbal SRL and nominalSRL.
In addition, it shows that the performanceof predicate recognition is very stable due to itshigh dependence on POS tagging results, ratherthan syntactic parsing results.
Finally, it is notsurprising to find out that the performance ofpredicate recognition when mixing verbal andnominal predicates is better than the perform-ance of either verbal predicates or nominalpredicates.5.3 Extending the Word-based SyntacticParser to a Character-based Syntactic ParserThe above experimental results on a word-basedsyntactic parser (assuming correct word seg-mentation) show that both syntactic and seman-tic parsing benefit from our integrated parsingapproach.
However, observing the great chal-lenge of word segmentation in Chinese informa-1115tion processing, it is still unclear whether andhow much joint learning benefits charac-ter-based syntactic and semantic parsing.
In thissection, we extended the Ratnaparkhi parser(1999) to a character-based parser (with auto-matic word segmentation), and then examinedthe effectiveness of joint learning.Given the three-pass process in theword-based syntactic parser, it is easy to extendit to a character-based parser for Chinese texts.This can be done by only replacing the TAGprocedure in the first pass with a POSCHUNKprocedure, which integrates Chinese word seg-mentation and POS tagging in one step, follow-ing the method described in (Ng and Low 2004).Here, each character is annotated with both aboundary tag and a POS tag.
The 4 possibleboundary tags include ?B?
for a character thatbegins a word and is followed by another char-acter, ?M?
for a character that occurs in themiddle of a word, ?E?
for a character that ends aword, and ?S?
for a character that occurs as asingle-character word.
For example, ???
?/Beijing city/NR?
would be decomposed intothree units: ?
?
/north/B_NR?, ?
?/capital/M_NR?, and ??/city/E_NR?.
Also, ??/is/VC?
would turn into ??/is/S_VC?.
ThroughPOSCHUNK, all characters in a sentence arefirst assigned with POS chunk labels which mustbe compatible with previous ones, and thenmerged into words with their POS tags.
For ex-ample, ?
?/north/B_NR?, ?
?/capital/M_NR?,and ??/city/E_NR?
will be merged as ???
?/Beijing/NR?, ??/is/S_VC?
will become ??/is/VC?.
Finally the merged results of the PO-SCHUNK are fed into the CHUNK procedure ofthe second pass.Using the same data split as the previous ex-periments, word segmentation achieves perfor-mance of 96.3 in F1-measure on the test data.Table 4 lists the syntactic and semantic parsingperformance by adopting the character-basedparser.Table 4 shows that integrated parsing benefitssyntactic and semantic parsing when automaticword segmentation is considered.
However, theimprovements are smaller due to the extra noisecaused by automatic word segmentation.
Forexample, our experiments show that the per-formance of predicate recognition drops from93.2 to 90.3 in F1-measure when replacing cor-rect word segmentations with automatic ones.Method Task R (%) P (%) F1Syntactic 82.23 84.28 83.24Pipeline on top-bestparse tree SRL 60.40 62.75 61.55Syntactic 82.25 84.29 83.26Pipeline on 20-bestparse trees SRL 60.17 63.63 61.85Syntactic 82.51 84.31 83.40Integrated parsingwith semanticrole-related featuresSRL 60.09 65.35 62.61Table 4: Performance with the character-based pars-er1 (using automatically recognized word bounda-ries).6 ConclusionIn this paper, we explore joint syntactic and se-mantic parsing to improve the performance ofboth syntactic and semantic parsing, in particularthat of semantic parsing.
Evaluation shows thatour integrated parsing approach outperforms thepipeline parsing approach on N-best parse trees,a natural extension of the widely-used pipelineparsing approach on the top-best parse tree.
Italso shows that incorporating semantic informa-tion into syntactic parsing significantly improvesthe performance of both syntactic and semanticparsing.
This is very promising and encouraging,considering the complexity of both syntactic andsemantic parsing.To our best knowledge, this is the first suc-cessful research on exploring syntactic parsingand semantic role labeling for verbal and nomi-nal predicates in an integrated way.AcknowledgmentsThe first two authors were financially supportedby Projects 60683150, 60970056, and 90920004under the National Natural Science Foundationof China.
This research was also partially sup-ported by a research grant R-252-000-225-112from National University of Singapore Aca-demic Research Fund.
We also want to thank thereviewers for insightful comments.ReferencesCollin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The Berkeley FrameNet Project.
InProceedings of COLING-ACL 1998.Xavier Carreras and Lluis M?rquez.
2004.
Introduc-tion to the CoNLL-2004 Shared Task: SemanticRole Labeling.
In Proceedings of CoNLL 2004.1 POS tags are included in evaluating the perform-ance of a character-based syntactic parser.
Thus itcannot be directly compared with the word-based onewhere correct word segmentation is assumed.1116Xavier Carreras and Lluis M?rquez.
2005.
Introduc-tion to the CoNLL-2005 Shared Task: SemanticRole Labeling.
In Proceedings of CoNLL 2005.Eugene Charniak.
2001.
Immediate-Head Parsing forLanguage Models.
In Proceedings of ACL 2001.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Jenny Rose Finkel and Christopher D. Manning.2009.
Joint Parsing and Named Entity Recognition.In Proceedings of NAACL 2009.Jan Hajic, Massimiliano Ciaramita, Richard Johans-son, et al 2009.
The CoNLL-2009 Shared Task:Syntactic and Semantic Dependencies in MultipleLanguages.
In Proceedings of CoNLL 2009.Zheng Ping Jiang and Hwee Tou Ng.
2006.
SemanticRole Labeling of NomBank: A Maximum EntropyApproach.
In Proceedings of EMNLP 2006.Fang Kong, Guodong Zhou, and Qiaoming Zhu.
2009.Employing the Centering Theory in PronounResolution from the Semantic Perspective.
InProceedings of EMNLP 2009.Peter Koomen, Vasin Punyakanok, Dan Roth,Wen-tau Yih.
2005.
Generalized Inference withMultiple Semantic Role Labeling Systems.
InProceedings of CoNLL 2005.Junhui Li, Guodong Zhou, Hai Zhao, Qiaoming Zhu,and Peide Qian.
2009.
Improving Nominal SRL inChinese Language with Verbal SRL informationand Automatic Predicate Recognition.
In Pro-ceedings of EMNLP 2009.Chang Liu and Hwee Tou Ng.
2007.
Learning Pre-dictive Structures for Semantic Role Labeling ofNomBank.
In Proceedings of ACL 2007.Paola Merlo and Gabriele Mussillo.
2005.
AccurateFunction Parsing.
In Proceedings of EMNLP 2005.Paola Merlo and Gabriele Musillo.
2008.
SemanticParsing for High-Precision Semantic Role Label-ling.
In Proceedings of CoNLL 2008.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
Annotating Noun Ar-gument Structure for NomBank.
In Proceedings ofLREC 2004.Scott Miller, Heidi Fox, Lance Ramshaw, and RalphWeischedel.
2000.
A Novel Use of StatisticalParsing to Extract Information from Text.
In Pro-ceedings of ANLP 2000.Srini Narayanan and Sanda Harabagiu.
2004.
Ques-tion Answering based on Semantic Structures.
InProceedings of COLING 2004.Hwee Tou Ng and Jin Kiat Low.
2004.
ChinesePart-of-Speech Tagging: One-at-a-Time orAll-at-Once?
Word-Based or Character-Based?
InProceedings of EMNLP 2004.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguistics,31, 71-106.Slav Petrov and Dan Klein.
2007.
Improved Infer-ence for Unlexicalized Parsing.
In Proceesings ofNAACL 2007.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Juraf-sky.
2005.
Support Vector Learning for SemanticArgument Classification.
Machine Learning, 2005,60:11-39.Adwait Ratnaparkhi.
1999.
Learning to Parse NaturalLanguage with Maximum Entropy Models.
Ma-chine Learning, 34, 151-175.Mihai Surdeanu, Sanda Harabagiu, John Williamsand Paul Aarseth.
2003.
Using Predi-cate-Argument Structures for Information Extrac-tion.
In Proceedings of ACL 2003.Mihai Surdeanu, Richard Johansson, Adam Meyers,Lluis M?rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 Shared Task on Joint Parsing ofSyntactic and Semantic Dependencies.
In Pro-ceedings of CoNLL 2008.Charles Sutton and Andrew McCallum.
2005.
JointParsing and Semantic Role Labeling.
In Proceed-ings of CoNLL2005.Nianwen Xue and Martha Palmer.
2003.
Annotatingthe Propositions in the Penn Chinese TreeBank.
InProceedings of the 2nd SIGHAN Workshop onChinese Language Processing.Nianwen Xue.
2006.
Annotating the Predi-cate-Argument Structure of Chinese Nominaliza-tions.
In Proceedings of LREC 2006.Nianwen Xue.
2008.
Labeling Chinese Predicateswith Semantic Roles.
Computational Linguistics,34(2):225-255.Szu-ting Yi and Martha Palmer.
2005.
The Integra-tion of Syntactic Parsing and Semantic Role La-beling.
In Proceedings of CoNLL 2005.Yue Zhang and Stephen Clark.
2008.
Joint WordSegmentation and POS Tagging Using a SinglePerceptron.
In Proceedings of ACL 2008.1117
