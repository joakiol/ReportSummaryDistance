Proceedings of NAACL-HLT 2013, pages 837?846,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsProbabilistic Frame InductionJackie Chi Kit Cheung?Department of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canadajcheung@cs.toronto.eduHoifung PoonOne Microsoft WayMicrosoft ResearchRedmond, WA 98052, USAhoifung@microsoft.comLucy VanderwendeOne Microsoft WayMicrosoft ResearchRedmond, WA 98052, USAlucyv@microsoft.comAbstractIn natural-language discourse, related eventstend to appear near each other to describe alarger scenario.
Such structures can be formal-ized by the notion of a frame (a.k.a.
template),which comprises a set of related events andprototypical participants and event transitions.Identifying frames is a prerequisite for infor-mation extraction and natural language gen-eration, and is usually done manually.
Meth-ods for inducing frames have been proposedrecently, but they typically use ad hoc proce-dures and are difficult to diagnose or extend.In this paper, we propose the first probabilisticapproach to frame induction, which incorpo-rates frames, events, and participants as latenttopics and learns those frame and event transi-tions that best explain the text.
The numberof frame components is inferred by a novelapplication of a split-merge method from syn-tactic parsing.
In end-to-end evaluations fromtext to induced frames and extracted facts, ourmethod produces state-of-the-art results whilesubstantially reducing engineering effort.1 IntroductionEvents with causal or temporal relations tend to oc-cur near each other in text.
For example, a BOMB-ING scenario in an article on terrorism might be-gin with a DETONATION event, in which terroristsset off a bomb.
Then, a DAMAGE event might en-sue to describe the resulting destruction and anycasualties, followed by an INVESTIGATION event?This research was undertaken during the author?s internshipat Microsoft Research.covering subsequent police investigations.
After-wards, the BOMBING scenario may transition intoa CRIMINAL-PROCESSING scenario, which beginswith police catching the terrorists, and proceeds toa trial, sentencing, etc.
A common set of partici-pants serves as the event arguments; e.g., the agent(or subject) of DETONATION is often the same asthe theme (or object) of INVESTIGATION and corre-sponds to a PERPETRATOR.Such structures can be formally captured by thenotion of a frame (a.k.a.
template, scenario), whichconsists of a set of events with prototypical transi-tions, as well as a set of slots representing the com-mon participants.
Identifying frames is an explicitor implicit prerequisite for many NLP tasks.
Infor-mation extraction, for example, stipulates the typesof events and slots that are extracted for a frame ortemplate.
Online applications such as dialogue sys-tems and personal-assistant applications also modelusers?
goals and subgoals using frame-like represen-tations.
In natural-language generation, frames areoften used to represent contents to be expressed aswell as to support surface realization.Until recently, frames and related representationshave been manually constructed, which has limitedtheir applicability to a relatively small number of do-mains and a few slots within a domain.
Furthermore,additional manual effort is needed after the framesare defined in order to extract frame componentsfrom text (e.g., in annotating examples and design-ing features to train a supervised learning model).This paradigm makes generalizing across tasks dif-ficult, and might suffer from annotator bias.Recently, there has been increasing interest in au-837tomatically inducing frames from text.
A notableexample is Chambers and Jurafsky (2011), whichfirst clusters related verbs to form frames, and thenclusters the verbs?
syntactic arguments to identifyslots.
While Chambers and Jurafsky (2011) repre-sents a major step forward in frame induction, it isalso limited in several aspects.
The clustering usedad hoc steps and customized similarity metrics, aswell as an additional retrieval step from a large ex-ternal text corpus for slot generation.
This makes ithard to replicate their approach or adapt it to newdomains.
Lacking a coherent model, it is also diffi-cult to incorporate additional linguistic insights andprior knowledge.In this paper, we present PROFINDER (PROba-bilistic Frame INDucER), the first probabilistic ap-proach to frame induction.
PROFINDER definesa joint distribution over the words in a documentand their frame assignments by modeling frameand event transitions, correlations among events andslots, and their surface realizations.
Given a set ofdocuments, PROFINDER outputs a set of inducedframes with learned parameters, as well as the mostprobable frame assignments that can be used forevent and entity extraction.
The numbers of eventsand slots are dynamically determined by a novelapplication of the split-merge approach from syn-tactic parsing (Petrov et al 2006).
In end-to-endevaluations from text to entity extraction using stan-dard MUC and TAC datasets, PROFINDER achievedstate-of-the-art results while significantly reducingengineering effort and requiring no external data.2 Related WorkIn information extraction and other semantic pro-cessing tasks, the dominant paradigm requires twostages of manual effort.
First, the target representa-tion is defined manually by domain experts.
Then,manual effort is required to construct an extractoror to annotate examples to train a machine-learningsystem.
Recently, there has been a burgeoning bodyof work in reducing such manual effort.
For exam-ple, a popular approach to reduce annotation effort isbootstrapping from seed examples (Patwardhan andRiloff, 2007; Huang and Riloff, 2012).
However,this still requires prespecified frames or templates,and selecting seed words is often a challenging task(Curran et al 2007).
Filatova et al(2006) constructsimple domain templates by mining verbs and thenamed entity type of verbal arguments that are topi-cal, whereas Shinyama and Sekine (2006) identifyquery-focused slots by clustering common namedentities and their syntactic contexts.
Open IE (Bankoand Etzioni, 2008) limits the manual effort to de-signing a few domain-independent relation patterns,which can then be applied to extract relational triplesfrom text.
While extremely scalable, this approachcan only extract atomic factoids within a sentence,and the resulting triples are noisy, non-canonicalizedtext fragments.More relevant to our approach is the recent workin unsupervised semantic induction, such as un-supervised semantic parsing (Poon and Domingos,2009), unsupervised semantical role labeling (Swierand Stevenson, 2004) and induction (Lang and Lap-ata, 2011, e.g.
), and slot induction from web searchlogs (Cheung and Li, 2012).
As in PROFINDER,they model distributional contexts for slots androles.
However, these approaches focus on the se-mantics of independent sentences or queries, and donot capture discourse-level dependencies.The modeling of frame and event transitions inPROFINDER is similar to a sequential topic model(Gruber et al 2007), and is inspired by the suc-cessful applications of such topic models in sum-marization (Barzilay and Lee, 2004; Daume?
III andMarcu, 2006; Haghighi and Vanderwende, 2009, in-ter alia).
There are, however, two main differences.First, PROFINDER contains not a single sequentialtopic model, but two (for frames and events, respec-tively).
In addition, it also models the interdepen-dencies among events, slots, and surface text, whichis analogous to the USP model (Poon and Domin-gos, 2009).
PROFINDER can thus be viewed as anovel combination of state-of-the-art models in un-supervised semantics and discourse modeling.In terms of aim and capability, PROFINDER ismost similar to Chambers and Jurafsky (2011),which culminated from a series of work for iden-tifying correlated events and arguments in narratives(Chambers and Jurafsky, 2008; Chambers and Ju-rafsky, 2009).
By adopting a probabilistic approach,PROFINDER has a sound theoretical underpinning,and is easy to modify or extend.
For example, inSection 3, we show how PROFINDER can easily be838augmented with additional linguistically-motivatedfeatures.
Likewise, PROFINDER can easily be usedas a semi-supervised system if some slot designa-tions and labeled examples are available.The idea of representing and capturing stereotyp-ical knowledge has a long history in artificial in-telligence and psychology, and has assumed vari-ous names such as frames (Minsky, 1974), schemata(Rumelhart, 1975), and scripts (Schank and Abel-son, 1977).
In the linguistics and computationallinguistics communities, frame semantics (Fillmore,1982) uses frames as the central representation ofword meaning, culminating in the development ofFrameNet (Baker et al 1998), which contains over1000 manually annotated frames.
A similarly richlexical resource is the MindNet project (Richard-son et al 1998).
Our notion of frame is related tothese representations, but there are also subtle differ-ences.
For example, Minsky?s frame emphasizes in-heritance, which we do not model in this paper1.
Asin semantic role labeling, FrameNet focuses on se-mantic roles and does not model event or frame tran-sitions, so the scope of its frames is often no morethan an event in our model.
Perhaps the most sim-ilar to our frame is Roger Schank?s scripts, whichcapture prototypical events and participants in a sce-nario such as restaurant dining.
In their approach,however, scripts are manually defined, making ithard to generalize.
In this regard, our work may beviewed as an attempt to revive a long tradition in AIand linguistics, by leveraging the recent advances incomputational power, NLP, and machine learning.3 Probabilistic Frame InductionIn this section, we present PROFINDER, a proba-bilistic model for frame induction.
Let F be a set offrames, where each frame F = (EF , SF ) comprisesa unique set of events EF and slots SF .
Given adocument D and a word w in D, Zw = (f, e) repre-sents an assignment of w to frame f ?
F and frameelement e ?
Ef ?
Sf .
At the heart of PROFINDERis a generative model P?
(D,Z) that defines a jointdistribution over document D and the frame assign-ment to its words Z.
Given a set of documents D,1This should be a straightforward extension ?
using thesplit-and-merge approach, PROFINDER already produces a hi-erarchy of events and slots in learning, although currently itmakes no use of the intermediate levels.frame induction in PROFINDER amounts to deter-mining the number of events and slots in each frame,as well as learning the parameters ?
by summing outthe latent assignments Z to maximize the likelihoodof the document set?D?DP?
(D).The induced frames identify the key event structuresin the document set.
Additionally, PROFINDER canconduct event and entity extraction by computingthe most probable frame assignment Z.
In the re-mainder of the section, we first present the basemodel for PROFINDER.
We then introduce sev-eral linguistically motivated refinements, as well asefficient algorithms for learning and inference inPROFINDER.3.1 Base ModelThe probabilistic formulation of PROFINDER makesit extremely flexible for incorporating linguistic in-tuition and prior knowledge.
In this paper, we designour PROFINDER model to capture three types of de-pendencies.Frame transitions between clauses A sentencecontains one or more clauses, each of which is aminimal unit expressing a proposition.
A clause isunlikely to straddle different frames, so we stipu-late that the words in a clause be assigned to thesame frame.
On the other hand, frame transitionscan happen between clauses, and we adopt the com-mon Markov assumption that the frame of a clauseonly depends on the previous clause in the docu-ment.
Clauses are automatically extracted from thedependency parse and further decomposed into anevent head and its syntactic arguments.Event transitions within a frame Events tend totransition into related events in the same frame, asdetermined by their causal or temporal relations.Each clause is assigned an event compatible withits frame assignment (i.e., the event is in the givenframe).
Like frame transitions, we assume that theevent assignment of a clause depends only on theevent of the previous clause.Emission of event heads and slot words Simi-lar to topics in topic models, each event determines839a multinomial from which the event head is gener-ated; e.g., a DETONATION event might use verbssuch as detonate, set off or nouns such as denota-tion, bombing as its event head.
Additionally, asin USP (Poon and Domingos, 2009), an event alsocontains a multinomial of slots for each of its argu-ment types2; e.g., the agent argument of a DETONA-TION event is generally the PERPETRATOR slot ofthe BOMBING frame.
Finally, each slot has its ownmultinomials for generating the argument head anddependency label, regardless of the event.Formally, let D be a document and C1, ?
?
?
, Cl beits clauses, the PROFINDER model is defined byP?
(D,Z) = PF?INIT(F1)??iPF?TRAN(Fi+1|Fi)?
PE?INIT(E1|F1)?
?iPE?TRAN(Ei+1|Ei, Fi+1, Fi)??iPE?HEAD(ei|Ei)?
?i,jPSLOT(Si,j |Ei,j , Ai,j)?
?i,jPA?HEAD(ai,j |Si,j)?
?i,jPA?DEP(depi,j |Si,j)Here, Fi, Ei denote the frame and event assign-ment to clause Ci, respectively, and ei denotes theevent head.
For the j-th argument of clause i,Si,j denotes the slot assignment, Ai,j the argumenttype, ai,j the head word, and depi,j the dependencyfrom the event head.
PE?TRAN(Ei+1|Ei, Fi+1, Fi) =PE?INIT(Ei+1|Fi+1) if Fi+1 6= Fi.Essentially, PROFINDER combines a frame HMMwith an event HMM, where the first models frametransition and emits events, and the second modelsevent transition within a frame and emits argumentslots.3.2 Model refinementsThe base model captures the main dependencies inevent narrative, but it can be easily extended to lever-2USP generates the argument types along with events fromclustering.
For simplicity, in PROFINDER we simply classifya syntactic argument into subject, object, and prepositional ob-ject, according to its Stanford dependency to the event head.age additional linguistic intuition.
PROFINDER in-corporates three such refinements.Background frame Event narratives often con-tain interjections of general content common to allframes.
For example, in newswire articles, ATTRI-BUTION is commonplace to describe who said orreported a particular quote or fact.
To avoid con-taminating frames with generic content, we intro-duce a background frame with its own events, slots,and emission distributions, and a binary switch vari-able Bi ?
{BKG,CNT} that determines whetherclause i is generated from the actual content frameFi (CNT ) or background (BKG).
We also stipu-late that if BKG is chosen, the nominal frame staysthe same as the previous clause.Stickiness in frame and event transitions Priorwork has demonstrated that promoting topic coher-ence in natural-language discourse helps discoursemodeling (Barzilay and Lee, 2004).
We extendPROFINDER to leverage this intuition by incorporat-ing a ?stickiness?
prior (Haghighi and Vanderwende,2009) to encourage neighboring clauses to stay inthe same frame.
Specifically, along with introducingthe background frame, the frame transition compo-nent now becomesPF?TRAN(Fi+1|Fi, Bi+1) = (1)????
?1(Fi+1 = Fi), if Bi+1 = BKG?1(Fi+1 = Fi)+(1?
?
)PF?TRAN(Fi+1|Fi),if Bi+1 = CNTwhere ?
is the stickiness parameter, and the eventtransition component correspondingly becomesPE?TRAN(Ei+1|Ei, Fi+1, Fi, Bi+1) = (2)????
?1(Ei+1 = Ei), if Bi+1 = BKGPE?TRAN(Ei+1|Ei), if Bi+1 = CNT,Fi = Fi+1PE?INIT(Ei+1), if Bi+1 = CNT,Fi 6= Fi+1Argument dependencies as caseframes As no-ticed in previous work such as Chambers and Juraf-sky (2011), the combination of an event head and adependency relation often gives a strong signal ofthe slot that is indicated.
For example, bomb >nsubj (subject argument of bomb) often indicatesa PERPETRATOR.
Thus, rather than simply emitting840FrameEventBackgroundEventhead?1?1?1?1?1 ?1??
?1 ?1?
??????????
??????
??.
.
.. .
.|?| |?| |?|???????Arguments???????
?????
?Figure 1: Graphical representation of our model.
Hyper-parameters, the stickiness factor, and the frame and eventinitial and transition distributions are not shown for clar-ity.the dependency from the event head to an event ar-gument depi,j , our model instead emits the pair ofevent head and dependency relation, which we calla caseframe following Bean and Riloff (2004).3.3 Full generative storyTo summarize, the distributions that are learned byour model are the default distributions PBKG(B),PF?INIT(F ), PE?INIT(E); the transition distri-butions PF?TRAN(Fi+1|Fi), PE?TRAN(Ei+1|Ei);and the emission distributions PSLOT(S|E,A,B),PE?HEAD(e|E,B), PA?HEAD(a|S), PA?DEP(dep|S).We used additive smoothing with uniform Dirich-let priors for all the multinomials.
The overallgenerative story of our model is as follows:1.
Draw a Bernoulli distribution for PBKG(B)2.
Draw the frame, event, and slot distributions3.
Draw an event head emission distributionPE?HEAD(e|E,B) for each frame including thebackground frame4.
Draw event argument lemma and caseframeemission distributions for each slot in eachframe including the background frame5.
For each clause in each document, generate theclause-internal structure.The clause-internal structure at clause i is gener-ated by the following steps:1.
Generate whether this clause is background(Bi ?
{CNT,BKG} ?
PBKG(B))2.
Generate the frame Fi and event Ei fromPF?INIT(F ), PE?INIT(E), or according toequations 1 and 23.
Generate the observed event head ei fromPE?HEAD(ei|Ei).4.
For each event argument:(a) Generate the slot Si,j fromPSLOT(S|E,A,B).
(b) Generate the dependency/caseframe emis-sion depi,j ?
PA?DEP(dep|S) and thelemma of the head word of the event ar-gument ai,j ?
PA?HEAD(a|S).3.4 Learning and InferenceOur generative model admits efficient inference bydynamic programming.
In particular, after collaps-ing the latent assignment of frame, event, and back-ground into a single hidden variable for each clause,the expectation and most probable assignment canbe computed using standard forward-backward andViterbi algorithms on fixed tree structures.Parameter learning can be done using EM by al-ternating the computation of expected counts and themaximization of multinomial parameters.
In par-ticular, PROFINDER uses incremental EM, whichhas been shown to have better and faster con-vergence properties than standard EM (Liang andKlein, 2009).Determining the optimal number of events andslots is challenging.
One solution is to adopt a non-parametric Bayesian method by incorporating a hi-erarchical prior over the parameters (e.g., a Dirich-let process).
However, this approach can imposeunrealistic restrictions on the model choice and re-sult in intractability which requires sampling or ap-proximate inference to overcome.
Additionally, EMlearning can suffer from local optima due to its non-convex learning objective, especially when dealingwith a large number hidden states without a goodinitialization.To address these issues, we adopt a novel appli-cation of the split-merge method previously used insyntactic parsing for inferring refined latent syntac-tic categories (Petrov et al 2006).
First, the modelis initialized with a number of frames, which is ahyperparameter, and each frame is associated with841one event and two slots.
Starting from this mini-mal structure, EM training begins.
After a numberof iterations, each event and slot state is ?split?
intwo; that is, each original state now becomes twonew states.
Each of the new states is generated withhalf of the probability of the original, and containsa duplicate of the associated emission distributions.Some perturbation is then added to the probabilitiesto break symmetry.
After splitting, we merge backa portion of the newly split events and slots that re-sult in the least improvement in the likelihood of thetraining data.
For more details on split-merge, seePetrov et al(2006)By adjusting the number of split-merge cycles andthe merge parameters, our model learns the numberof events and slots in a dynamical fashion that is tai-lored to the data.
Moreover, our model starts with asmall number of frame elements, which reduces thenumber of local optima and facilitates initial learn-ing.
After each split, the subsequent learning startswith (a perturbed version of) the previously learnedparameters, which makes a good initialization thatis crucial for EM.
Finally, it is also compatible withthe hierarchical nature of events and slots.
For ex-ample, slots can first be coarsely split into personsversus locations, and later refined into subcategoriessuch as perpetrators and victims.4 MUC-4 Entity Extraction ExperimentsWe first evaluate our model on a standard entityextraction task, using the evaluation settings fromChambers and Jurafsky (2011) (henceforth, C&J)to enable a head-to-head comparison.
Specifically,we use the MUC-4 data set (1992) , which contains1300 training and development documents on ter-rorism in South America, with 200 additional doc-uments for testing.
MUC-4 contains four templates:ATTACK, KIDNAPPING, BOMBING, and ARSON.3All templates share the same set of predefined slots,with the evaluation focusing on the following four:PERPETRATOR, PHYSICAL TARGET, HUMAN TAR-GET, and INSTRUMENT.For each slot in a MUC template, the systemfirst identifies an induced slot that best maps to itby F1 on the development set.
As in C&J, tem-3Two other templates have negligible counts and are ignoredas in C&J.plate is ignored in final evaluation, so all the clustersthat belong to the same slot are then merged acrossthe templates; e.g., the PERPETRATOR clusters forKIDNAPPING and BOMBING are merged.
The fi-nal precision, recall, and F1 are computed based onthese merged clusters.
Correctness is determined bymatching head words, and slots marked as optionalin MUC are ignored when computing recall.
All hy-perparameters are tuned on the development set (seeAppendix A for their values).Named entity type Named entity type is a usefulfeature to filter out entities for particular slots; e.g.
alocation cannot be an INSTRUMENT.
We thus divideeach induced cluster into four clusters by namedentity type before performing the mapping, follow-ing C&J?s heuristic and using a named entity recog-nizer and word lists derived from WordNet: PER-SON/ORGANIZATION, PHYSICAL OBJECT, LOCA-TION, and OTHER.Document classification The MUC-4 datasetcontains many documents that have words relatedto MUC slots (e.g., plane and aviation), but are notabout terrorism.
To reduce precision errors, C&Jfirst filtered irrelevant documents based on the speci-ficity of event heads to learned frames.
To estimatethe specificity, they used additional data retrievedfrom a large external corpus.
In PROFINDER, how-ever, specificity can be easily estimated using theprobability distributions learned during training.
Inparticular, we define the probability of an event headin a frame j as:PF (w) =?EF?FPE?HEAD(w|E)/|F |, (3)and the probability of a frame given an event headas:P (F |w) = PF (w)/?F ?
?FPF ?(w).
(4)We then follow the rest of C&J?s procedure toscore each learned frame with each MUC document.Specifically, a document is mapped to a frame if theaverage PF (w) in the document is above a thresholdand the document contains at least one trigger wordw?
with P (F |w?)
> 0.2.
The threshold and the in-duced frame were determined on the developmentset, and were used to filter irrelevant documents inthe test set.842Unsupervised methods P R F1PROFINDER (This work) 32 37 34Chambers and Jurafsky (2011) 48 25 33With additional informationPROFINDER +doc.
classification 41 44 43C&J 2011 +granularity 44 36 40Table 1: Results on MUC-4 entity extraction.
C&J 2011+granularity refers to their experiment in which theymapped one of their templates to five learned clustersrather than one.Results Compared to C&J, PROFINDER is con-ceptually much simpler, using a single probabilis-tic model and standard learning and inference algo-rithms, and not requiring multiple processing stepsor customized similarity metrics.
It only used thedata in MUC-4, whereas C&J required additionaltext to be retrieved from a large external corpus (Gi-gaword (Graff et al 2005)) for each event cluster.It currently does not make use of coreference infor-mation, whereas C&J did.
Remarkably, despite allthese, PROFINDER was still able to outperform C&Jon entity extraction, as shown in Table 1.
We alsoevaluated PROFINDER?s performance assuming per-fect document classification (+doc.
classification).This led to a substantially higher precision, suggest-ing that further improvement is possible from betterdocument classification.Figure 2 shows part of a frame learned byPROFINDER, which includes some slots and eventsannotated in MUC.
PROFINDER is also able to iden-tify events and slots not annotated in MUC, a de-sirable characteristic of unsupervised methods.
Forexample, it found a DISCUSSION event, an AR-REST event (call, arrest, express, meet, charge), aPEACE AGREEMENT slot (agreement, rights, law,proposal), and an AUTHORITIES slot (police, gov-ernment, force, command).
The background framewas able to capture many verbs related to attribu-tion, such as say, continue, add, believe, although itmissed report.5 Evaluating Frame Induction UsingGuided Summarization TemplatesThe MUC-4 dataset was originally designed forinformation extraction and focuses on a limitednumber of template and slot types.
To evalu-Event: Attack Event: Discussionreport, participate, kid-nap, kill, releasehold, meeting, talk, dis-cuss, investigateSlot: Perpetrator Slot: VictimPERSON/ORG PERSON/ORGWords: guerrilla, po-lice, source, person,groupWords: people, priest,leader, member, judgeCaseframes:report>nsubj,kidnap>nsubj,kill>nsubj,participate>nsubj,release>nsubjCaseframes:kill>dobj,murder>dobj,release>dobj,report>dobj,kidnap>dobjFigure 2: A partial frame learned by PROFINDER fromthe MUC-4 data set, with the most probable emissions foreach event and slot.
Labels are assigned by the authorsfor readability.ate PROFINDER?s capabilities in generalizing toa greater variety of text, we designed and con-ducted a novel evaluation based on the TAC guided-summarization dataset.
This evaluation was inspiredby the connection between summarization and infor-mation extraction (White et al 2001), and reflects aconceptualization of summarization as inducing andextracting structured information from source text.Essentially, we adapted the TAC summarization an-notation to create gold-standard slots, and used themto evaluate entity extraction as in MUC-4.Dataset We used the TAC 2010 guided-summarization dataset in our experiments(Owczarzak and Dang, 2010).
This data set con-sists of text from five domains (termed categoriesin TAC), each with a template defined by TACorganizers.
In total, there are 46 document clusters(termed topics in TAC), each of which contains 20documents and has eight human-written summaries.Each summary was manually segmented usingthe Pyramid method (Nenkova and Passonneau,2004) and each segment was annotated with a slot(termed aspect in TAC) from the correspondingtemplate.
Figure 3 shows an example and the fullset of templates is available at http://www.nist.gov/tac/2010/Summarization/Guided-Summ.2010.guidelines.html.
In843(a) Accidents and Natural Disasters:WHAT: what happenedWHEN: date, time, other temporal markersWHERE: physical locationWHY: reasons for accident/disasterWHO AFFECTED: casualties...DAMAGES: ... caused by the disasterCOUNTERMEASURES: rescue efforts...(b) (WHEN During the night of July 17,)(WHAT a 23-foot <WHAT tsunami) hit thenorth coast of Papua New Guinea (PNG)>,(WHY triggered by a 7.0 undersea earth-quake in the area).
(c) WHEN: night WHAT: tsunami, coastWHY: earthquakeFigure 3: (a) A frame from the TAC Guided Summariza-tion task with abbreviated slot descriptions.
(b) A TACtext span, segmented into several contributors with slotlabels.
Note that the two WHAT contributors overlap, andare demarcated by different bracket types.
(c) The entitiesthat are extracted for evaluation.TAC, each annotated segment (Figure 3b) is calleda contributor.Evaluation Method We converted the contribu-tors into a form that is more similar to the previ-ous MUC evaluation, so that we can fairly compareagainst previous work such as C&J that were de-signed to extract information into that form.
Specif-ically, we extracted the head lemma from all themaximal noun phrases found in the contributor (Fig-ure 3c) and treated them as gold-standard entity slotsto extract.
While this conversion may not be ideal insome cases, it simplifies the TAC slots and enablesautomatic evaluation.
We leave the refinement ofthis conversion to future work, and believe it couldbe done by crowdsourcing.For each TAC slot in a TAC category, we extractentities from the summaries that belong to the givenTAC category.
A system-induced entity is consid-ered a match to a TAC-derived entity from the samedocument if the head lemma in the former matchesone in the latter.
Based on this matching criterion,the system-induced slots are mapped to the TACslots in a way that achieves the best F1 for eachTAC slot.
We allow a system slot to map to mul-tiple TAC slots, due to potential overlaps in entities1-best 5-bestSystems P R F1 P R F1PROFINDER 24 25 24 21 38 27C&J 58 6.1 11 50 12 20Table 2: Results on TAC 2010 entity extraction with N -best mapping for N = 1 and N = 5.
Intermediate valuesof N produce intermediate results, and are not shown forbrevity.among TAC slots.
For example, in a document abouta tsunami, earthquake may appear both in the WHATslot as a disaster itself, and in the CAUSE slot as acause for the tsunami.One salient difference between TAC and MUCslots is that TAC slots are often more general thanMUC slots.
For example, TAC slots such as WHYand COUNTERMEASURES likely correspond to mul-tiple slots at the granularity of MUC.
As a result, wealso consider mapping the N -best system-inducedslots to each TAC slot, for N up to 5.Experiments We trained PROFINDER and a reim-plementation of C&J on the 920 full source texts ofTAC 2010, and tested them on the 368 model sum-maries.
We did not provide C&J?s model with accessto external data, in order to enable fair comparisonwith our model.
Since all of the summary sentencesare expected to be relevant, we did not conduct doc-ument or sentence relevance classification in C&J orPROFINDER.
We tuned all parameters by two-foldcross validation on the summaries.
We computed theoverall precision, recall, and F1 by taking a micro-average over the results for each TAC slot.Results The results are shown in Table 2.PROFINDER substantially outperformed C&J in F1,in both 1-best and N -best cases.
As in MUC-4, theprecision of C&J is higher, partly because C&J oftendid not do much in clustering and produced manysmall clusters.
For example, in the 1-best setting, theaverage number of entities mapped to each TAC slotby C&J is 21, whereas it is 208 for PROFINDER.
Forboth systems, the results are generally lower com-pared to that in MUC-4, which is expected since thistask is harder given the greater diversity in framesand slots to be induced.8446 ConclusionWe have presented PROFINDER, the first probabilis-tic approach to frame induction and shown that itachieves state-of-the-art results on end-to-end entityextraction in standard MUC and TAC data sets.
Ourmodel is inspired by recent advances in unsuper-vised semantic induction and content modeling insummarization.
Our probabilistic approach makesit easy to extend the model with additional linguisticinsights and prior knowledge.
While we have madea case for unsupervised methods and the importanceof robustness across domains, our method is alsoamenable to semi-supervised or supervised learn-ing if annotated data is available.
In future work,we would like to further investigate frame inductionevaluation, particularly in evaluating event cluster-ing.AcknowledgmentsWe would like to thank Nate Chambers for answer-ing questions about his system.
We would also liketo thank Chris Quirk for help with preprocessing theMUC corpus, and the members of the NLP group atMicrosoft Research for useful discussions.Appendix A. Hyperparameter SettingsWe document below the hyperparameter settings forPROFINDER that were used to generate the resultsin the paper.Hyperparameter MUC TACNumber of frames, |F| 9 8Frame stickiness, ?
0.125 0.5Smoothing (frames, events, slots) 0.5 2Smoothing (emissions) 0.05 0.2Number of split-merge cycles 4 2Iterations per cycle 10 10ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the 17th International Conference on Compu-tational linguistics.Michele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
Pro-ceedings of ACL-08: HLT, pages 28?36.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics: HLT-NAACL 2004.David Bean and Ellen Riloff.
2004.
Unsupervised learn-ing of contextual role knowledge for coreference reso-lution.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics: HLT-NAACL 2004.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised learning of narrative event chains.
In Pro-ceedings of ACL-08: HLT, pages 789?797, Columbus,Ohio, June.
Association for Computational Linguis-tics.Nathanael Chambers and Dan Jurafsky.
2009.
Unsuper-vised learning of narrative schemas and their partici-pants.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP.
Association for Computational Lin-guistics.Nathanael Chambers and Dan Jurafsky.
2011.
Template-based information extraction without the templates.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 976?986, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Jackie C. K. Cheung and Xiao Li.
2012.
Sequence clus-tering and labeling for unsupervised query intent dis-covery.
In Proceedings of the 5th ACM InternationalConference on Web Search and Data Mining, pages383?392.James R. Curran, Tara Murphy, and Bernhard Scholz.2007.
Minimising semantic drift with mutual exclu-sion bootstrapping.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics.Hal Daume?
III and Daniel Marcu.
2006.
BayesianQuery-Focused summarization.
In Proceedings of the21st International Conference on Computational Lin-guistics and 44th Annual Meeting of the Associationfor Computational Linguistics, pages 305?312, Syd-ney, Australia, July.
Association for ComputationalLinguistics.Elena Filatova, Vasileios Hatzivassiloglou, and Kath-leen McKeown.
2006.
Automatic creation of do-main templates.
In Proceedings of the COLING/ACL2006 Main Conference Poster Sessions, pages 207?214, Sydney, Australia, July.
Association for Compu-tational Linguistics.845Charles J. Fillmore.
1982.
Frame semantics.
Linguisticsin the Morning Calm, pages 111?137.David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.2005.
English gigaword second edition.
LinguisticData Consortium, Philadelphia.Amit Gruber, Michael Rosen-Zvi, and Yair Weiss.
2007.Hidden topic markov models.
Artificial Intelligenceand Statistics (AISTATS).Aria Haghighi and Lucy Vanderwende.
2009.
Exploringcontent models for multi-document summarization.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 362?370, Boulder, Colorado, June.
Associationfor Computational Linguistics.Ruihong Huang and Ellen Riloff.
2012.
Bootstrappedtraining of event extraction classifiers.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 286?295, Avignon, France, April.
Associationfor Computational Linguistics.Joel Lang and Mirella Lapata.
2011.
Unsupervised se-mantic role induction via split-merge clustering.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 1117?1126, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Percy Liang and Dan Klein.
2009.
Online EM for un-supervised models.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 611?619, Boulder,Colorado, June.
Association for Computational Lin-guistics.Marvin Minsky.
1974.
A framework for representingknowledge.
Technical report, Cambridge, MA, USA.1992.
Proceedings of the Fourth Message UnderstandingConference (MUC-4).
Morgan Kaufmann.Ani Nenkova and Rebecca Passonneau.
2004.
Evaluat-ing content selection in summarization: The pyramidmethod.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics: HLT-NAACL 2004, volume 2004, pages 145?152.Karolina Owczarzak and Hoa T. Dang.
2010.
TAC 2010guided summarization task guidelines.Siddharth Patwardhan and Ellen Riloff.
2007.
Effec-tive information extraction with semantic affinity pat-terns and relevant regions.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 717?727, Prague, Czech Republic, June.
Association forComputational Linguistics.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1?10.Stephen D. Richardson, William B. Dolan, and Lucy Van-derwende.
1998.
MindNet: Acquiring and structuringsemantic information from text.
In Proceedings of the36th Annual Meeting of the Association for Computa-tional Linguistics and 17th International Conferenceon Computational Linguistics, Volume 2, pages 1098?1102, Montreal, Quebec, Canada, August.
Associationfor Computational Linguistics.David Rumelhart, 1975.
Notes on a schema for stories,pages 211?236.
Academic Press, Inc.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,Plans, Goals, and Understanding: An Inquiry Into Hu-man Knowledge Structures.
Lawrence Erlbaum, July.Yusuke Shinyama and Satoshi Sekine.
2006.
Preemptiveinformation extraction using unrestricted relation dis-covery.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, Main Conference,New York City, USA, June.
Association for Computa-tional Linguistics.Robert S. Swier and Suzanne Stevenson.
2004.
Un-supervised semantic role labelling.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,pages 95?102, Barcelona, Spain, July.
Association forComputational Linguistics.Michael White, Tanya Korelsky, Claire Cardie, VincentNg, David Pierce, and Kiri Wagstaff.
2001.
Multidoc-ument summarization via information extraction.
InProceedings of the First International Conference onHuman Language Technology Research.
Associationfor Computational Linguistics.846
