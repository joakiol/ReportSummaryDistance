Exploring Asymmetric Clustering for Statistical Language ModelingJianfeng GaoMicrosoft Research, AsiaBeijing, 100080, P.R.Cjfgao@microsoft.comJoshua T. GoodmanMicrosoft Research, RedmondWashington 98052, USAjoshuago@microsoft.comGuihong Cao1Department of ComputerScience and Engineering ofTianjin University, ChinaHang LiMicrosoft Research, AsiaBeijing, 100080, P.R.Changli@microsoft.com1 This work was done while Cao was visiting Microsoft Research Asia.AbstractThe n-gram model is a stochastic model,which predicts the next word (predictedword) given the previous words(conditional words) in a word sequence.The cluster n-gram model is a variant ofthe n-gram model in which similar wordsare classified in the same cluster.
It hasbeen demonstrated that using differentclusters for predicted and conditionalwords leads to cluster models that aresuperior to classical cluster models whichuse the same clusters for both words.
Thisis the basis of the asymmetric clustermodel (ACM) discussed in our study.
Inthis paper, we first present a formaldefinition of the ACM.
We then describein detail the methodology of constructingthe ACM.
The effectiveness of the ACMis evaluated on a realistic application,namely Japanese Kana-Kanji conversion.Experimental results show substantialimprovements of the ACM in comparisonwith classical cluster models and wordn-gram models at the same model size.Our analysis shows that thehigh-performance of the ACM lies in theasymmetry of the model.1 IntroductionThe n-gram model has been widely applied in manyapplications such as speech recognition, machinetranslation, and Asian language text input [Jelinek,1990; Brown et al, 1990; Gao et al, 2002].
It is astochastic model, which predicts the next word(predicted word) given the previous n-1 words(conditional words) in a word sequence.The cluster n-gram model is a variant of the wordn-gram model in which similar words are classifiedin the same cluster.
This has been demonstrated asan effective way to deal with the data sparsenessproblem and to reduce the memory sizes for realisticapplications.
Recent research [Yamamoto et al,2001] shows that using different clusters forpredicted and conditional words can lead to clustermodels that are superior to classical cluster models,which use the same clusters for both words [Brownet al, 1992].
This is the basis of the asymmetriccluster model (ACM), which will be formallydefined and empirically studied in this paper.Although similar models have been used in previousstudies [Goodman and Gao, 2000; Yamamoto et al,2001], several issues have not been completelyinvestigated.
These include: (1) an effectivemethodology for constructing the ACM, (2) athorough comparative study of the ACM withclassical cluster models and word models when theyare applied to a realistic application, and (3) ananalysis of the reason why the ACM is superior.The goal of this study is to address the abovethree issues.
We first present a formal definition ofthe ACM; then we describe in detail themethodology of constructing the ACM including (1)an asymmetric clustering algorithm in whichdifferent metrics are used for clustering thepredicted and conditional words respectively; and(2) a method for model parameter optimization inwhich the optimal cluster numbers are found fordifferent clusters.
We evaluate the ACM on a realapplication, Japanese Kana-Kanji conversion, whichconverts phonetic Kana strings into proper Japaneseorthography.
The performance is measured in termsof character error rate (CER).
Our results showsubstantial improvements of the ACM incomparison with classical cluster models and wordn-gram models at the same model size.
Our analysisshows that the high-performance of the ACM comesComputational Linguistics (ACL), Philadelphia, July 2002, pp.
183-190.Proceedings of the 40th Annual Meeting of the Association forfrom better structure and better smoothing, both ofwhich lie in the asymmetry of the model.This paper is organized as follows: Section 1introduces our research topic, and then Section 2reviews related work.
Section 3 defines the ACMand describes in detail the method of modelconstruction.
Section 4 first introduces the JapaneseKana-Kanji conversion task; it then presents ourmain experiments and a discussion of our findings.Finally, conclusions are presented in Section 5.2 Related WorkA large amount of previous research on clusteringhas been focused on how to find the best clusters[Brown et al, 1992; Kneser and Ney, 1993;Yamamoto and Sagisaka, 1999; Ueberla, 1996;Pereira et al, 1993; Bellegarda et al, 1996; Bai etal., 1998].
Only small differences have beenobserved, however, in the performance of thedifferent techniques for constructing clusters.
In thisstudy, we focused our research on novel techniquesfor using clusters ?
the ACM, in which differentclusters are used for predicted and conditional wordsrespectively.The discussion of the ACM in this paper is anextension of several studies below.
The first similarcluster model was presented by Goodman and Gao[2000] in which the clustering techniques werecombined with Stolcke?s [1998] pruning to reducethe language model (LM) size effectively.
Goodman[2001] and Gao et al [2001] give detaileddescriptions of the asymmetric clustering algorithm.However, the impact of the asymmetric clusteringon the performance of the resulting cluster modelwas not empirically studied there.
Gao et al, [2001]presented a fairly thorough empirical study ofclustering techniques for Asian language modeling.Unfortunately, all of the above work studied theACM without applying it to an application; thusonly perplexity results were presented.
The first realapplication of the ACM was a simplified bigramACM used in a Chinese text input system [Gao et al2002].
However, quite a few techniques (includingclustering) were integrated to construct a Chineselanguage modeling system, and the contribution ofusing the ACM alone was by no means completelyinvestigated.Finally, there is one more point worthmentioning.
Most language modeling improvementsreported previously required significantly morespace than word trigram models [Rosenfeld, 2000].Their practical value is questionable since allrealistic applications have memory constraints.
Inthis paper, our goal is to achieve a better tradeoffbetween LM performance (perplexity and CER) andmodel size.
Thus, whenever we compare theperformance of different models (i.e.
ACM vs. wordtrigram model), Stolcke?s pruning is employed tobring the models compared to similar sizes.3 Asymmetric Cluster Model3.1 ModelThe LM predicts the next word wi given its history hby estimating the conditional probability P(wi|h).Using the trigram approximation, we haveP(wi|h)?P(wi|wi-2wi-1), assuming that the next worddepends only on the two preceding words.In the ACM, we will use different clusters forwords in different positions.
For the predicted word,wi, we will denote the cluster of the word by PWi,and we will refer to this as the predictive cluster.
.Forthe words wi-2 and wi-1 that we are conditioning on,we will denote their clusters by CWi-2 and CWi-1which we call conditional clusters.
When we whichto refer to a cluster of a word w in general we willuse the notation W.  The ACM estimates theprobability of wi given the two preceeding words wi-2and wi-1 as the product of the following twoprobabilities:(1) The probability of the predicted cluster PWigiven the preceding conditional clusters CWi-2and CWi-1, P(PWi|CWi-2CWi-1), and(2) The probability of the word given its cluster PWiand the preceding conditional clusters CWi-2 andCWi-1, P(wi|CWi-2CWi-1PWi).Thus, the ACM can be parameterized by)|()|()|( 1212 iiiiiiii PWCWCWwPCWCWPWPhwP ????
??
(1)The ACM consists of two sub-models: (1) thecluster sub-model P(PWi|CWi-2CWi-1), and (2) theword sub-model P(wi|CWi-2CWi-1PWi).
To deal withthe data sparseness problem, we used a backoffscheme (Katz, 1987) for the parameter estimation ofeach sub-model.
The backoff scheme recursivelyestimates the probability of an unseen n-gram byutilizing (n-1)-gram estimates.The basic idea underlying the ACM is the use ofdifferent clusters for predicted and conditionalwords respectively.
Classical cluster models aresymmetric in that the same clusters are employed forboth predicted and conditional words.
However, thesymmetric cluster model is suboptimal in practice.For example, consider a pair of words like ?a?
and?an?.
In general, ?a?
and ?an?
can follow the samewords, and thus, as predicted words, belong in thesame cluster.
But, there are very few words that canfollow both ?a?
and ?an?.
So as conditional words,they belong in different clusters.In generating clusters, two factors need to beconsidered: (1) clustering metrics, and (2) clusternumbers.
In what follows, we will investigate theimpact of each of the factors.3.2 Asymmetric clusteringThe basic criterion for statistical clustering is tomaximize the resulting probability (or minimize theresulting perplexity) of the training data.
Manytraditional clustering techniques [Brown et al,1992] attempt to maximize the average mutualinformation of adjacent clusters?=21 , 2122121 )()|(log)(),(WW WPWWPWWPWWI , (2)where the same clusters are used for both predictedand conditional words.
We will call these clusteringtechniques symmetric clustering, and the resultingclusters both clusters.
In constructing the ACM, weused asymmetric clustering, in which differentclusters are used for predicted and conditionalwords.
In particular, for clustering conditionalwords, we try to minimize the perplexity of trainingdata for a bigram of the form P(wi|Wi-1), which isequivalent to maximizing?=?Niii WwP11)|( .
(3)where N is the total number of words in the trainingdata.
We will call the resulting clusters conditionalclusters denoted by CW.
For clustering predictedwords, we try to minimize the perplexity of trainingdata of P(Wi|wi-1)?P(wi|Wi).
We will call theresulting clusters predicted clusters denoted by PW.We have2?
?= ??=?
?=?Ni iiiiiiNiiiii WPwWPwPWwPWwPwWP1 1111 )()()()()|()|(?=??
?= Ni iiiiiiWPWwPwPwWP111 )()()()(?= ??
?= Niiiii WwPwPwP111)|()()( .Now,)()(1?iiwPwP is independent of the clustering used.Therefore, for the selection of the best clusters, it issufficient to try to maximize?=?Niii WwP11 )|( .
(4)This is very convenient since it is exactly the op-posite of what was done for conditional clustering.
It2 Thanks to Lillian Lee for suggesting this justification ofpredictive clusters.means that we can use the same clustering tool forboth, and simply switch the order used by theprogram used to get the raw counts for clustering.The clustering technique we used creates a binarybranching tree with words at the leaves.
The ACMin this study is a hard cluster model, meaning thateach word belongs to only one cluster.
So in theclustering tree, each word occurs in a single leaf.
Inthe ACM, we actually use two different clusteringtrees.
One is optimized for predicted words, and theother for conditional words.The basic approach to clustering we used is atop-down, splitting clustering algorithm.
In eachiteration, a cluster is split into two clusters in theway that the splitting achieves the maximal entropydecrease (estimated by Equations (3) or (4)).
Finally,we can also perform iterations of swapping all wordsbetween all clusters until convergence i.e.
no moreentropy decrease can be found3.
We find that ouralgorithm is much more efficient than agglomerativeclustering algorithms ?
those which merge wordsbottom up.3.3 Parameter optimizationAsymmetric clustering results in two binaryclustering trees.
By cutting the trees at a certainlevel, it is possible to achieve a wide variety ofdifferent numbers of clusters.
For instance, if thetree is cut after the 8th level, there will be roughly28=256 clusters.
Since the tree is not balanced, theactual number of clusters may be somewhat smaller.We use Wl to represent the cluster of a word w usinga tree cut at level l.  In particular, if we set l to thevalue ?all?, it means that the tree is cut at infinitedepth, i.e.
each cluster contains a single word.
TheACM model of Equation (1) can be rewritten asP(PWil|CWi-2jCWi-1j)?P(wi|PWi-2kCWi-1kCWil).
(5)To optimally apply the ACM to realistic applicationswith memory constraints, we are always seeking thecorrect balance between model size andperformance.
We used Stolcke?s pruning method toproduce many ACMs with different model sizes.
Inour experiments, whenever we compare techniques,we do so by comparing the performance (perplexityand CER) of the LM techniques at the same modelsizes.
Stolcke?s pruning is an entropy-based cutoff3 Notice that for experiments reported in this paper, weused the basic top-down algorithm without swapping.Although the resulting clusters without swapping are noteven locally optimal, our experiments show that thequality of clusters (in terms of the perplexity of theresulting ACM) is not inferior to that of clusters withswapping.method, which can be described as follows: alln-grams that change perplexity by less than athreshold are removed from the model.
For pruningthe ACM, we have two thresholds: one for thecluster sub-model P(PWil|CWi-2jCWi-1j) and one forthe word sub-model P(wi|CWi-2kCWi-1kPWil)respectively, denoted by tc and  tw below.In this way, we have 5 different parameters thatneed to be simultaneously optimized: l, j, k, tc, andtw, where j, k, and l are the numbers of clusters, and tcand tw are the pruning thresholds.A brute-force approach to optimizing such a largenumber of parameters is prohibitively expensive.Rather than trying a large number of combinationsof all 5 parameters, we give an alternative techniquethat is significantly more efficient.
Simple mathshows that the perplexity of the overall modelP(PWil|CWi-2jCWi-1j)?
P(wi|CWi-2kCWi-1kPWil) isequal to the perplexity of the cluster sub-modelP(PWil|CWi-2jCWi-1j) times the perplexity of theword sub-model P(wi|CWi-2kCWi-1kPWil).
The size ofthe overall model is clearly the sum of the sizes ofthe two sub-models.
Thus, we try a large number ofvalues of j, l, and a pruning threshold tc forP(PWil|CWi-2jCWi-1j), computing sizes andperplexities of each, and a similarly large number ofvalues of l,  k, and a separate threshold tw forP(wi|CWi-2kCWi-1kPWil).
We can then look at allcompatible pairs of these models (those with thesame value of l) and quickly compute the perplexityand size of the overall models.
This allows us torelatively quickly search through what wouldotherwise be an overwhelmingly large search space.4 Experimental Results and Discussion4.1 Japanese Kana-Kanji Conversion TaskJapanese Kana-Kanji conversion is the standardmethod of inputting Japanese text by converting asyllabary-based Kana string into the appropriatecombination of ideographic Kanji and Kana.
This isa similar problem to speech recognition, except thatit does not include acoustic ambiguity.
Theperformance is generally measured in terms ofcharacter error rate (CER), which is the number ofcharacters wrongly converted from the phoneticstring divided by the number of characters in thecorrect transcript.
The role of the language model is,for all possible word strings that match the typedphonetic symbol string, to select the word stringwith the highest language model probability.Current products make about 5-10% errors in con-version of real data in a wide variety of domains.4.2 SettingsIn the experiments, we used two Japanesenewspaper corpora: the Nikkei Newspaper corpus,and the Yomiuri Newspaper corpus.
Both textcorpora have been word-segmented using a lexiconcontaining 167,107 entries.We performed two sets of experiments: (1) pilotexperiments, in which model performance ismeasured in terms of perplexity and (2) JapaneseKana-Kanji conversion experiments, in which theperformance of which is measured in terms of CER.In the pilot experiments, we used a subset of theNikkei newspaper corpus: ten million words of theNikkei corpus for language model training, 10,000words for held-out data, and 20,000 words fortesting data.
None of the three data sets overlapped.In the Japanese Kana-Kanji conversion experiments,we built language models on a subset of the NikkeiNewspaper corpus, which contains 36 millionwords.
We performed parameter optimization on asubset of held-out data from the Yomiuri Newspapercorpus, which contains 100,000 words.
Weperformed testing on another subset of the YomiuriNewspaper corpus, which contains 100,000 words.In both sets of experiments, word clusters werederived from bigram counts generated from thetraining corpora.
Out-of-vocabulary words were notincluded in perplexity and error rate computations.4.3 Impact of asymmetric clusteringAs described in Section 3.2, depending on theclustering metrics we chose for generating clusters,we obtained three types of clusters: both clusters(the metric of Equation (2)), conditional clusters(the metric of Equation (3)), and predicted clusters(the metric of Equation (4)).
We then performed aseries of experiments to investigate the impact ofdifferent types of clusters on the ACM.
We usedthree variants of the trigram ACM: (1) the predictivecluster model P(wi|wi-2wi-1Wi)?
P(Wi|wi-2wi-1) whereonly predicted words are clustered, (2) theconditional cluster model P(wi|Wi-2Wi-1) where onlyconditional words are clustered, and (3) the IBMmodel P(wi|Wi)?
P(Wi|Wi-2Wi-1) which can be treatedas a special case of the ACM of Equation (5) byusing the same type of cluster for both predicted andconditional words, and setting k = 0, and l = j. Foreach cluster trigram model, we compared theirperplexities and CER results on Japanese Kana-Kanji conversion using different types of clusters.For each cluster type, the number of clusters werefixed to the same value 2^6 just for comparison.
Theresults are shown in Table 1.
It turns out that thebenefit of using different clusters in differentpositions is obvious.
For each cluster trigrammodel, the best results were achieved by using the?matched?
clusters, e.g.
the predictive cluster modelP(wi|wi-2wi-1Wi)?
P(Wi|wi-2wi-1) has the bestperformance when the cluster Wi is the predictivecluster PWi generated by using the metric ofEquation (4).
In particular, the IBM model achievedthe best results when predicted and conditionalclusters were used for predicted and conditionalwords respectively.
That is, the IBM model is of theform P(wi|PWi)?
P(PWi|CWi-2CWi-1).Con Pre Both Con + PrePerplexity 287.7 414.5 377.6 --- Conmodel CER (%) 4.58 11.78 12.56 ---Perplexity 103.4 102.4 103.3 --- Premodel CER (%) 3.92 3.63 3.82 ---Perplexity 548.2 514.4 385.2 382.2 IBMmodel CER (%) 6.61 6.49 5.82 5.36Table 1: Comparison of different cluster typeswith cluster-based models4.4 Impact of parameter optimizationIn this section, we first present our pilot experimentsof finding the optimal parameter set of the ACM (l, j,k, tc, tw) described in Section 2.3.
Then, we comparethe ACM to the IBM model, showing that thesuperiority of the ACM results from its betterstructure.In this section, the performance of LMs wasmeasured in terms of perplexity, and the size wasmeasured as the total number of parameters of theLM: one parameter for each bigram and trigram, oneparameter for each normalization parameter ?
thatwas needed, and one parameter for each unigram.We first used the conditional cluster model of theform P(wi|CWi-2jCWi-1j).
Some sample settings ofparameters (j, tw) are shown in Figure 1.
Theperformance was consistently improved byincreasing the number of clusters j, except at thesmallest sizes.
The word trigram model wasconsistently the best model, except at the smallestsizes, and even then was only marginally worse thanthe conditional cluster models.
This is not surprisingbecause the conditional cluster model alwaysdiscards information for predicting words.We then used the predictive cluster model of theform P(PWil|wi-2wi-1)?P(wi|wi-2wi-1PWil), where onlypredicted words are clustered.
Some sample settingsof the parameters (l, tc, tw) are shown in Figure 2.
Forsimplicity, we assumed tc=tw, meaning that the samepruning threshold values were used for bothsub-models.
It turns out that predictive clustermodels achieve the best perplexity results at about2^6 or 2^8 clusters.
The models consistentlyoutperform the baseline word trigram models.We finally returned to the ACM of Equation (5),where both conditional words and the predictedword are clustered (with different numbers ofclusters), and which is referred to as the combinedcluster model below.
In addition, we allow differentvalues of the threshold for different sub-models.Therefore, we need to optimize the model parameterset l, j, k, tc, tw.Based on the pilot experiment results usingconditional and predictive cluster models, we triedcombined cluster models for values l?
[4, 10], j,k?
[8, 16].
We also allow j, k=all.
Rather than plotall points of all models together, we show only theouter envelope of the points.
That is, if for a givenmodel type and a given point there is some otherpoint of the same type with both lower perplexityand smaller size than the first point, then we do notplot the first, worse point.The results are shown in Figure 3, where thecluster number of IBM models is 2^14 whichachieves the best performance for IBM models inour experiments.
It turns out that when l?
[6, 8] andj, k>12, combined cluster models yield the bestresults.
We also found that the predictive clustermodels give as good performance as the bestcombined ones while combined modelsoutperformed very slightly only when model sizesare small.
This is not difficult to explain.
Recall thatthe predictive cluster model is a special case of thecombined model where words are used inconditional positions, i.e.
j=k=all.
Our experimentsshow that combined models achieved goodperformance when large numbers of clusters areused for conditional words, i.e.
large j, k>12, whichare similar to words.The most interesting analysis is to look at somesample settings of the parameters of the combinedcluster models in Figure 3.
In Table 2, we show thebest parameter settings at several levels of modelsize.
Notice that in larger model sizes, predictivecluster models (i.e.
j=k=all) perform the best insome cases.
The ?prune?
columns (i.e.
columns 6 and7) indicate the Stolcke pruning parameter we used.First, notice that the two pruning parameters (incolumns 6 and 7) tend to be very similar.
This isdesirable since applying the theory of relativeentropy pruning predicts that the two pruningparameters should actually have the same value.Next, let us compare the ACMP(PWil|CWi-2jCWi-1j)?P(wi|CWi-2kCWi-1kPWil) totraditional IBM clustering of the formP(Wil|Wi-2lWi-1l)?P(wi|Wil), which is equal toP(Wil|Wi-2lWi-1l)?P(wi|Wi-20Wi-10Wil) (assuming the1051101151201251301351401451500.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06sizeperplexity2^12 clusters2^14 clusters2^16 clustersword trigramFigure 1.
Comparison of conditional modelsapplied with different numbers of clusters1001051101151201251301351401451500.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06sizeperplexity2^4 clusters2^6 clusters2^8 clusters2^10 clustersword trigramFigure 2.
Comparison of predictive modelsapplied with different numbers of clusters1001101201301401501601700.0E+00 5.0E+05 1.0E+06 1.5E+06 2.0E+06 2.5E+06sizeperplexityACMIBMword trigrampredictive modelFigure 3.
Comparison of ACMs, predictivecluster model, IBM model, and word trigrammodelsame type of cluster is used for both predictive andconditional words).
Our results in Figure 3 show thatthe performance of IBM models is roughly an orderof magnitude worse than that of ACMs.
This isbecause in addition to the use of the symmetriccluster model, the traditional IBM model makes twomore assumptions that we consider suboptimal.First, it assumes that j=l.
We see that the best resultscome from unequal settings of j and l.  Second, moreimportantly, IBM clustering assumes that k=0.
Wesee that not only is the optimal setting for k not 0, butalso typically the exact opposite is the optimal: k=allin which case P(wi|CWi-2kCWi-1kPWil)=P(wi|wi-2wi-1PWil), or k=14, 16, which is verysimilar.
That is, we see that words depend on theprevious words and that an independenceassumption is a poor one.
Of course, many of theseword dependencies are pruned away ?
but when aword does depend on something, the previous wordsare better predictors than the previous clusters.Another important finding here is that for most ofthese settings, the unpruned model is actually largerthan a normal trigram model ?
whenever k=all or 14,16, the unpruned model P(PWil|CWi-2jCWi-1j) ?P(wi|CWi-2kCWi-1kPWil) is actually larger than anunpruned model P(wi|wi-2wi-1).This analysis of the data is very interesting ?
itimplies that the gains from clustering are not fromcompression, but rather from capturing structure.Factoring the model into two models, in which thecluster is predicted first, and then the word ispredicted given the cluster, allows the structure andregularities of the model to be found.
This larger,better structured model can be pruned moreeffectively, and it achieved better performance thana word trigram model at the same model size.Model size Perplexity l j k tc tw2.0E+05 141.1 8 12 14 24 242.5E+05 135.7 8 12 14 12 245.0E+05 118.8 6 14 16 6 127.5E+05 112.8 6 16 16 3 61.0E+06 109.0 6 16 16 3 31.3E+06 107.4 6 16 16 2 31.5E+06 106.0 6 All all 2 21.9E+06 104.9 6 All all 1 2Table 2: Sample parameter settings for the ACM4.5 CER resultsBefore we present CER results of the JapaneseKana-Kanji conversion system, we briefly describeour method for storing the ACM in practice.One of the most common methods for storingbackoff n-gram models is to store n-gramprobabilities (and backoff weights) in a treestructure, which begins with a hypothetical rootnode that branches out into unigram nodes at the firstlevel of the tree, and each of those unigram nodes inturn branches out into bigram nodes at the secondlevel and so on.
To save storage, n-gramprobabilities such as P(wi|wi-1) and backoff weightssuch as ?
(wi-2wi-1) are stored in a single (bigram)node array (Clarkson and Rosenfeld, 1997).Applying the above tree structure to storing theACM is a bit complicated ?
there are somerepresentation issues.
For example, consider thecluster sub-model P(PWil|CWi-2jCWi-1j).
N-gramprobabilities such as P(PWil|CWi-1j) and backoffweights such as ?
(CWi-2jCWi-1j) cannot be stored in asingle (bigram) node array, because l ?
j andPW?CW.
Therefore, we used two separate trees tostore probabilities and backoff weights,respectively.
As a result, we used four tree structuresto store ACMs in practice: two for the clustersub-model P(PWil|CWi-2jCWi-1j), and two for theword sub-model P(wi|CWi-2kCWi-1kPWil).
We foundthat the effect of the storage structure cannot beignored in a real application.In addition, we used several techniques tocompress model parameters (i.e.
word id, n-gramprobability, and backoff weight, etc.)
and reduce thestorage space of models significantly.
For example,rather than store 4-byte floating point values for alln-gram probabilities and backoff weights, the valuesare quantized to a small number of quantizationlevels.
Quantization is performed separately on eachof the n-gram probability and backoff weight lists,and separate quantization level look-up tables aregenerated for each of these sets of parameters.
Weused 8-bit quantization, which shows noperformance decline in our experiments.Our goal is to achieve the best tradeoff betweenperformance and model size.
Therefore, we wouldlike to compare the ACM with the word trigrammodel at the same model size.
Unfortunately, theACM contains four sub-models and this makes itdifficult to be pruned to a specific size.
Thus forcomparison, we always choose the ACM withsmaller size than its competing word trigram modelto guarantee that our evaluation is under-estimated.Experiments show that the ACMs achievestatistically significant improvements over wordtrigram models at even smaller model sizes (p-value=8.0E-9).
Some results are shown in Table 3.Word trigram model ACMSize(MB)CER Size(MB)CER  CERReduction1.8 4.56% 1.7 4.25% 6.8%5.8 4.08% 4.5 3.83% 6.1%11.7 4.04% 10.7 3.73% 7.7%23.5 4.00% 21.7 3.63% 9.3%42.4 3.98% 40.4 3.63% 8.8%Table 3:  CER results of ACMs and wordtrigram models at different model sizesNow we discuss why the ACM is superior tosimple word trigrams.
In addition to the betterstructure as shown in Section 3.3, we assume herethat the benefit of our model also comes from itsbetter smoothing.
Consider a probability such asP(Tuesday| party on).
If we put the word ?Tuesday?into the cluster WEEKDAY, we decompose theprobabilityWhen each word belongs to one class, simple mathshows that this decomposition is a strict equality.However, when smoothing is taken intoconsideration, using the clustered probability will bemore accurate than using the non-clusteredprobability.
For instance, even if we have never seenan example of ?party on Tuesday?, perhaps we haveseen examples of other phrases, such as ?party onWednesday?
; thus, the probability P(WEEKDAY |party on) will be relatively high.
Furthermore,although we may never have seen an example of?party on WEEKDAY Tuesday?, after we backoff orinterpolate with a lower order model, we may able toaccurately estimate P(Tuesday | on WEEKDAY).Thus, our smoothed clustered estimate may be agood one.Our assumption can be tested empirically byfollowing experiments.
We first constructed severaltest sets with different backoff rates4.
The backoffrate of a test set, when presented to a trigram model,is defined as the number of words whose trigramprobabilities are estimated by backoff bigramprobabilities divided by the number of words in thetest set.
Then for each test set, we obtained a pair ofCER results using the ACM and the word trigrammodel respectively.
As shown in Figure 4, in bothcases, CER increases as the backoff rate increasesfrom 28% to 40%.
But the curve of the word trigrammodel has a steeper upward trend.
The difference ofthe upward trends of the two curves can be shownmore clearly by plotting the CER difference betweenthem, as shown in Figure 5.
The results indicate thatbecause of its better smoothing, when the backoffrate increases, the CER using the ACM does notincrease as fast as that using the word trigram model.Therefore, we are reasonably confident that someportion of the benefit of the ACM comes from itsbetter smoothing.2.12.32.52.72.93.13.33.53.73.90.28 0.29 0.3 0.31 0.32 0.33 0.34 0.35 0.36 0.37 0.38 0.39 0.4 0.41backoff rateerror rateword trigram modelACMFigure 4: CER vs. backoff rate.4  The backoff rates are estimated using the baselinetrigram model, so the choice could be biased against theword trigram model.P(Tuesday | party on) = P(WEEKDAY | party on)?P(Tuesday | party on WEEKDAY).0.250.270.290.310.330.350.370.390.410.28 0.3 0.32 0.34 0.36 0.38 0.4 0.42backoff rateerror ratedifferenceFigure 5: CER difference vs. backoff rate.5 ConclusionThere are three main contributions of this paper.First, after presenting a formal definition of theACM, we described in detail the methodology ofconstructing the ACM effectively.
We showedempirically that both the asymmetric clustering andthe parameter optimization (i.e.
optimal clusternumbers) have positive impacts on the performanceof the resulting ACM.
The finding demonstratespartially the effectiveness of our research focus:techniques for using clusters (i.e.
the ACM) ratherthan techniques for finding clusters (i.e.
clusteringalgorithms).
Second, we explored the actualrepresentation of the ACM and evaluate it on arealistic application ?
Japanese Kana-Kanjiconversion.
Results show approximately 6-10%CER reduction of the ACMs in comparison with theword trigram models, even when the ACMs areslightly smaller.
Third, the reasons underlying thesuperiority of the ACM are analyzed.
For instance,our analysis suggests the benefit of the ACM comespartially from its better structure and its bettersmoothing.All cluster models discussed in this paper arebased on hard clustering, meaning that each wordbelongs to only one cluster.
One area we have notexplored is the use of soft clustering, where a word wcan be assigned to multiple clusters W with aprobability P(W|w) [Pereira et al, 1993].
Saul andPereira [1997] demonstrated the utility of softclustering and concluded that any method thatassigns each word to a single cluster would loseinformation.
It is an interesting question whether ourtechniques for hard clustering can be extended tosoft clustering.
On the other hand, soft clusteringmodels tend to be larger than hard clustering modelsbecause a given word can belong to multipleclusters, and thus a training instance P(wi|wi-2wi-1)can lead to multiple counts instead of just 1.ReferencesBai, S., Li, H., Lin, Z., and Yuan, B.
(1998).
Buildingclass-based language models with contextual statistics.
InICASSP-98, pp.
173-176.Bellegarda, J. R., Butzberger, J. W., Chow, Y. L., Coccaro, N.B., and Naik, D. (1996).
A novel word clustering algorithmbased on latent semantic analysis.
In ICASSP-96.Brown, P. F., Cocke, J., DellaPietra, S. A., DellaPietra, V. J.,Jelinek, F., Lafferty, J. D., Mercer, R. L., and Roossin, P. S.(1990).
A statistical approach to machine translation.Computational Linguistics, 16(2), pp.
79-85.Brown, P. F., DellaPietra V. J., deSouza, P. V., Lai, J. C., andMercer, R. L. (1992).
Class-based n-gram models of naturallanguage.
Computational Linguistics, 18(4), pp.
467-479.Clarkson, P. R., and Rosenfeld, R. (1997).
Statistical languagemodeling using the CMU-Cambridge toolkit.
In Eurospeech1997, Rhodes, Greece.Gao, J. Goodman, J. and Miao, J.
(2001).
The use of clusteringtechniques for language model ?
application to Asianlanguage.
Computational Linguistics and Chinese LanguageProcessing.
Vol.
6, No.
1, pp 27-60.Gao, J., Goodman, J., Li, M., and Lee, K. F. (2002).
Toward aunified approach to statistical language modeling for Chinese.ACM Transactions on Asian Language InformationProcessing.
Vol.
1, No.
1, pp 3-33.Goodman, J.
(2001).
A bit of progress in language modeling.
InComputer Speech and Language, October 2001, pp 403-434.Goodman, J., and Gao, J.
(2000).
Language model sizereduction by predictive clustering.
ICSLP-2000, Beijing.Jelinek, F. (1990).
Self-organized language modeling for speechrecognition.
In Readings in Speech Recognition, A. Waibeland K. F. Lee, eds., Morgan-Kaufmann, San Mateo, CA, pp.450-506.Katz, S. M. (1987).
Estimation of probabilities from sparse datafor the language model component of a speech recognizer.IEEE Transactions on Acoustics, Speech and SignalProcessing, ASSP-35(3):400-401, March.Kneser, R. and Ney, H. (1993).
Improved clustering techniquesfor class-based statistical language modeling.
In Eurospeech,Vol.
2, pp.
973-976, Berlin, Germany.Ney, H., Essen, U., and Kneser, R. (1994).
On structuringprobabilistic dependences in stochastic language modeling.Computer, Speech, and Language, 8:1-38.Pereira, F., Tishby, N., and Lee L. (1993).
Distributionalclustering of English words.
In Proceedings of the 31st AnnualMeeting of the ACL.Rosenfeld, R. (2000).
Two decades of statistical languagemodeling: where do we go from here.
In Proceeding of theIEEE, 88:1270-1278, August.Saul, L., and Pereira, F.C.N.
(1997).
Aggregate and mixed-orderMarkov models for statistical language processing.
InEMNLP-1997.Stolcke, A.
(1998).
Entropy-based Pruning of BackoffLanguage Models.
Proc.
DARPA News Transcription andUnderstanding Workshop, 1998, pp.
270-274.Ueberla, J. P. (1996).
An extended clustering algorithm forstatistical language models.
IEEE Transactions on Speechand Audio Processing, 4(4): 313-316.Yamamoto, H., Isogai, S., and Sagisaka, Y.
(2001).
Multi-ClassComposite N-gram Language Model for Spoken LanguageProcessing Using Multiple Word Clusters.
39th Annualmeetings of the Association for Computational Linguistics(ACL?01), Toulouse, 6-11 July 2001.Yamamoto, H., and Sagisaka, Y.
(1999).
Multi-class CompositeN-gram based on Connection Direction, In Proceedings of theIEEE International Conference on Acoustics, Speech andSignal Processing, May, Phoenix, Arizona.
