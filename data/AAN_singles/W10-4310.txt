Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 59?62,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsUsing entity features to classify implicit discourse relationsAnnie Louis, Aravind Joshi, Rashmi Prasad, Ani NenkovaUniversity of PennsylvaniaPhiladelphia, PA 19104, USA{lannie,joshi,rjprasad,nenkova}@seas.upenn.eduAbstractWe report results on predicting the senseof implicit discourse relations between ad-jacent sentences in text.
Our investigationconcentrates on the association betweendiscourse relations and properties of thereferring expressions that appear in the re-lated sentences.
The properties of inter-est include coreference information, gram-matical role, information status and syn-tactic form of referring expressions.
Pre-dicting the sense of implicit discourse re-lations based on these features is consid-erably better than a random baseline andseveral of the most discriminative featuresconform with linguistic intuitions.
How-ever, these features do not perform as wellas lexical features traditionally used forsense prediction.1 IntroductionCoherent text is described in terms of discourse re-lations such as ?cause?
and ?contrast?
between itsconstituent clauses.
It is also characterized by en-tity coherence, where the connectedness of the textis created by virtue of the mentioned entities andthe properties of referring expressions.
We aim toinvestigate the association between discourse rela-tions and the way in which references to entitiesare realized.
In our work, we employ features re-lated to entity realization to automatically identifydiscourse relations in text.We focus on implicit relations that hold be-tween adjacent sentences in the absence of dis-course connectives such as ?because?
or ?but?.Previous studies on this task have zeroed in onlexical indicators of relation sense: dependenciesbetween words (Marcu and Echihabi, 2001; Blair-Goldensohn et al, 2007) and the semantic orien-tation of words (Pitler et al, 2009), or on generalsyntactic regularities (Lin et al, 2009).The role of entities has also been hypothesizedas important for this task and entity-related fea-tures have been used alongside others (Corston-Oliver, 1998; Sporleder and Lascarides, 2008).Corpus studies and reading time experiments per-formed by Wolf and Gibson (2006) have in factdemonstrated that the type of discourse relationlinking two clauses influences the resolution ofpronouns in them.
However, the predictive powerof entity-related features has not been studied in-dependently of other factors.
Further motivationfor studying this type of features comes from newcorpus evidence (Prasad et al, 2008), that about aquarter of all adjacent sentences are linked purelyby entity coherence, solely because they talk aboutthe same entity.
Entity-related features would beexpected to better separate out such relations.We present the first comprehensive study of theconnection between entity features and discourserelations.
We show that there are notable differ-ences in properties of referring expressions acrossthe different relations.
Sense prediction can bedone with results better than random baseline us-ing only entity realization information.
Their per-formance, however, is lower than a knowledge-poor approach using only the words in the sen-tences as features.
The addition of entity featuresto these basic word features is also not beneficial.2 DataWe use 590 Wall Street Journal (WSJ) articleswith overlapping annotations for discourse, coref-erence and syntax from three corpora.The Penn Discourse Treebank (PDTB) (Prasadet al, 2008) is the largest available resource ofdiscourse relation annotations.
In the PDTB, im-plicit relations are annotated between adjacentsentences in the same paragraph.
They are as-signed senses from a hierarchy containing four toplevel categories?Comparison, Contingency, Tem-poral and Expansion.59An example ?Contingency?
relation is shownbelow.
Here, the second sentence provides thecause for the belief expressed in the first.Ex 1.
These rate indications aren?t directly comparable.Lending practices vary widely by location.Adjacent sentences can also become relatedsolely by talking about a common entity withoutany of the above discourse relation links betweentheir propositions.
Such pairs are annotated as En-tity Relations (EntRels) in the PDTB, for example:Ex 2.
Rolls-Royce Motor Cars Inc. said it expects its U.Ssales to remain steady at about 1,200 cars in 1990.
The luxuryauto maker last year sold 1,214 cars in the U.S.We use the coreference annotations from theOntonotes corpus (version 2.9) (Hovy et al, 2006)to compute our gold-standard entity features.
TheWSJ portion of this corpus contains 590 articles.Here, nominalizations and temporal expressionsare also annotated for coreference but we use thelinks between noun phrases only.
We expect thesefeatures computed on the gold-standard annota-tions to represent an upper bound on the perfor-mance of entity features.Finally, the Penn Treebank corpus (Marcus etal., 1994) is used to obtain gold-standard parse andgrammatical role information.Only adjacent sentences within the same para-graph are used in our experiments.3 Entity-related featuresWe associate each referring expression in a sen-tence with a set of attributes as described below.In Section 3.2, we detail how we combine theseattributes to compute features for a sentence pair.3.1 Referring expression attributesGrammatical role.
In exploratory analysis ofComparison relations, we often observed parallelsyntactic realizations for entities in the subject po-sition of the two sentences:Ex 3.
{Longer maturities}E1 are thought to indicate de-clining interest rates.
{Shorter maturities}E2 are considereda sign of rising rates because portfolio managers can capturehigher rates sooner.So, for each noun phrase, we record whetherit is the subject of a main clause (msubj), subjectof other clauses in the sentence (esubj) or a nounphrase not in subject position (other).Given vs. New.
When an entity is first intro-duced in the text, it is considered a new entity.Subsequent mentions of the same entity are given(Prince, 1992).
New-given distinction could helpto identify some of the Expansion and Entity re-lations.
When a sentence elaborates on another, itmight contain a greater number of new entities.We use the Ontonotes coreference annotationsto mark the information status for entities.
Foran entity, if an antecedent is found in the previ-ous sentences, it is marked as given, otherwise itis a new entity.Syntactic realization.
In Entity relations, the sec-ond sentence provides more information about aspecific entity in the first and a definite descriptionfor this second mention seems likely.
Also, giventhe importance of named entities in news, entitieswith proper names might be the ones frequentlydescribed using Entity relations.We use the part of speech (POS) tag associatedwith the head of the noun phrase to assign one ofthe following categories: pronoun, nominal, nameor expletive.
When the head does not belong tothe above classes, we simply record its POS tag.We also mark whether the noun phrase is a definitedescription using the presence of the article ?the?.Modification.
We expected modification proper-ties to be most useful for predicting Comparisonrelations.
Also, named or new entities in Entityrelations are very likely to have post modification.We record whether there are premodifiers orpostmodifiers in a given referring expression.
Inthe absence of pre- and postmodifiers, we indicatebare head realization.Topicalization.
Preposed prepositional or ad-verbial phrases before the subject of a sentenceindicate the topic under which the sentence isframed.
We observed that this property is frequentin Comparison and Temporal relations.
An exam-ple Comparison is shown below.Ex 4.
{Under British rules}T1, Blue Arrow was able towrite off at once $1.15 billion in goodwill arising from thepurchase.
{As a US-based company}T2, Blue Arrow wouldhave to amortize the good will over as many as 40 years, cre-ating a continuing drag on reported earnings.When the left sibling of a referring expression isa topicalized phrase, we mark the topic attribute.Number.
Using the POS tag of the head word, wenote whether the entity is singular or plural.3.2 Features for classificationNext, for each sentence pair, we associate two setsof features using the attributes described above.60Let S1 and S2 denote the two adjacent sentencesin a relation, where S1 occurs first in the text.Sentence level.
These features characterize S1and S2 individually.
For each sentence, we add afeature for each of the attributes described above.The value of the feature is the number of times thatattribute is observed in the sentence; i.e., the fea-ture S1given would have a value of 3 if there are 3given entities in the first sentence.Sentence pair.
These features capture the interac-tions between the entities present in S1 and S2.Firstly, for each pair of entities (a, b), such thata appears in S1 and b appears in S2, we assignone of the following classes: (i) SAME: a and bare coreferent, (ii) RELATED: their head words areidentical, (iii) DIFFERENT: neither coreferent norrelated.
The RELATED category was introduced tocapture the parallelism often present in Compari-son relations.
Even though the entities themselvesare not coreferent, they share the same head word(i.e.
longer maturities and shorter maturities).For features, we use the combination of theclass ((i), (ii) or (iii)) with the cross product ofthe attributes for a and b.
For example if a hasattributes {msubj, noun, ...} and b has attributes{esubj, defdesc, ...} and a and b are corefer-ent, we would increment the count for features?
{sameS1msubjS2esubj, sameS1msubjS2defdesc,sameS1nounS2esubj, sameS1nounS2defdesc ...}.Our total set of features observed for instancesin the training data is about 2000.We experimented with two variants of fea-tures: one using coreference annotations fromthe Ontonotes corpus (gold-standard) and an-other based on approximate coreference informa-tion where entities with identical head words aremarked as coreferent.4 Experimental setupWe define five classification tasks which disam-biguate if a specific PDTB relation holds betweenadjacent sentences.
In each task, we classify therelation of interest (positive) versus a categorywith a naturally occurring distribution of all of theother relations (negative).Sentence pairs from sections 0 to 22 of WSJ areused as training data and we test on sections 23and 24.
Given the skewed distribution of positiveand negative examples for each task, we randomlydownsample the negative instances in the trainingset to be equal to the positive examples.
The sizesof training sets for the tasks areExpansion vs other (4716)Contingency vs other (2466)Comparison vs other (1138)Temporal vs other (474)EntRel vs other (2378)Half of these examples are positive and theother negative in each case.The test set contains 1002 sentence pairs:Comp.
(133), Cont.
(230), Temp.
(34), Expn.
(369), EntRel (229), NoRel1 (7).
We do not down-sample our test set.
Instead, we evaluate our pre-dictions on the natural distribution present in thedata to get a realistic estimate of performance.We train a linear SVM classifier (LIBLIN-EAR2) for each task.3 The optimum regulariza-tion parameter was chosen using cross validationon the training data.5 Results5.1 Feature analysisWe ranked the features (based on gold-standardcoreference information) in the training sets bytheir information gain.
We then checked whichattributes are common among the top five featuresfor different classification tasks.As we had expected, the topicalization attributeand RELATED entities frequently appear amongthe top features for Comparison.Features with the name attribute were highlypredictive of Entity relations as hypothesized.However, while we had expected Entity relationsto have a high rate of coreference, we found coref-erent mentions to be very indicative of Temporalrelations: all the top features involve the SAME at-tribute.
A post-analysis showed that close to 70%of Temporal relations involve coreferent entitiescompared to around 50% for the other classes.The number of pronouns in the second sentencewas most characteristic of the Contingency rela-tion.
In the training set for Contingency task,about 45% of sentences pairs belonging to Contin-gency relation have a pronoun in the second sen-tence.
This is considerably larger than 32%, whichis the percentage of sentence pairs in the negativeexamples with a pronoun in second sentence.1PDTB relation for sentence pair when both entity anddiscourse relations are absent, very rare about 1% of our data.2http://www.csie.ntu.edu.tw/?cjlin/liblinear/3SVMs with linear kernel gave the best performance.
Wealso experimented with SVMs with radial basis kernel, NaiveBayes and MaxEnt classifiers.615.2 Performance on sense predictionThe classification results (fscores) are shown inTable 1.
The random baseline (Base.)
representsthe results if we predicted positive and negative re-lations according to their proportion in the test set.Entity features based on both gold-standard(EntGS) and approximate coreference (EntApp)outperform the random baseline for all the tasks.The drop in performance without gold-standardcoreference information is strongly noticable onlyfor Expansion relations.The best improvement from the baseline is seenfor predicting Contingency and Entity relations,with around 15% absolute improvement in fscorewith both EntGS and EntApp features.
The im-provements for Comparisons and Expansions arearound 11% in the approximate case.
Temporalrelations benefit least from these features.
Theserelations are rare, comprising 3% of the test setand harder to isolate from other relations.
Overall,our results indicate that discourse relations and en-tity realization have a strong association.5.3 Comparison with lexical featuresIn the context of using entity features for senseprediction, one would also like to test how theselinguistically rich features compare with simplerknowledge-lean approaches used in prior work.Specifically, we compare with word pairs, asimple yet powerful set of features introduced byMarcu and Echihabi (2001).
These features are thecross product of words in the first sentence withthose in the second.We trained classifiers on the word pairs from thesentences in the PDTB training sets.
In Table 1,we report the performance of word pairs (WP) aswell as their combination with gold-standard en-tity features (WP+EntGS).
Word pairs turn out asstronger predictors for all discourse relations com-pared to our entity features (except for Expansionprediction with EntGS features).
Further, no ben-efits over word pair results are obtained by com-bining entity realization information.6 ConclusionIn this work, we used a task-based approach toshow that the two components of coherence?discourse relations and entities?are related andinteract with each other.
Coreference, givenness,syntactic form and grammatical role of entities canpredict the implicit discourse relation between ad-Task Base.
EntGS EntApp WP WP+EntGSComp vs Oth.
13.27 24.18 24.14 27.30 26.19Cont vs Oth.
22.95 37.57 38.16 38.17 38.99Temp vs Oth.
3.39 7.58 5.61 11.09 10.04Expn vs Oth.
36.82 52.42 47.82 48.54 49.06Ent vs Oth.
22.85 38.03 36.73 38.48 38.14Table 1: Fscore resultsjacent sentences with results better than randombaseline.
However, with respect to developing au-tomatic discourse parsers, these entity features areless likely to be useful.
They do not outperformor complement simpler lexical features.
It wouldbe interesting to explore whether other aspects ofentity reference might be useful for this task, suchas bridging anaphora.
But currently, annotationsand tools for these phenomena are not available.ReferencesS.
Blair-Goldensohn, K. McKeown, and O. Rambow.2007.
Building and refining rhetorical-semantic re-lation models.
In HLT-NAACL.S.H.
Corston-Oliver.
1998.
Beyond string matchingand cue phrases: Improving efficiency and coveragein discourse analysis.
In The AAAI Spring Sympo-sium on Intelligent Text Summarization.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
Ontonotes: the 90% solution.In NAACL-HLT.Z.
Lin, M. Kan, and H.T.
Ng.
2009.
Recognizing im-plicit discourse relations in the Penn Discourse Tree-bank.
In EMNLP.D.
Marcu and A. Echihabi.
2001.
An unsupervised ap-proach to recognizing discourse relations.
In ACL.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1994.Building a large annotated corpus of english: Thepenn treebank.
Computational Linguistics.E.
Pitler, A. Louis, and A. Nenkova.
2009.
Automaticsense prediction for implicit discourse relations intext.
In ACL-IJCNLP.R.
Prasad, N. Dinesh, A. Lee, E. Miltsakaki,L.
Robaldo, A. Joshi, and B. Webber.
2008.
Thepenn discourse treebank 2.0.
In LREC.E.
Prince.
1992.
The zpg letter: subject, definiteness,and information status.
In Discourse description:diverse analyses of a fund raising text, pages 295?325.
John Benjamins.C.
Sporleder and A. Lascarides.
2008.
Using automat-ically labelled examples to classify rhetorical rela-tions: An assessment.
Natural Language Engineer-ing, 14:369?416.F.
Wolf and E. Gibson.
2006.
Coherence in naturallanguage: data structures and applications.
MITPress.62
